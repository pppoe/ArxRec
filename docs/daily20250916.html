<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250911.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity\n  3D Head Avatars", "author": "Tobias Kirschstein and Javier Romero and Artem Sevastopolsky and Matthias Nie\u00dfner and Shunsuke Saito", "abstract": "  Traditionally, creating photo-realistic 3D head avatars requires a\nstudio-level multi-view capture setup and expensive optimization during\ntest-time, limiting the use of digital human doubles to the VFX industry or\noffline renderings.\n  To address this shortcoming, we present Avat3r, which regresses a\nhigh-quality and animatable 3D head avatar from just a few input images, vastly\nreducing compute requirements during inference. More specifically, we make\nLarge Reconstruction Models animatable and learn a powerful prior over 3D human\nheads from a large multi-view video dataset. For better 3D head\nreconstructions, we employ position maps from DUSt3R and generalized feature\nmaps from the human foundation model Sapiens. To animate the 3D head, our key\ndiscovery is that simple cross-attention to an expression code is already\nsufficient. Finally, we increase robustness by feeding input images with\ndifferent expressions to our model during training, enabling the reconstruction\nof 3D head avatars from inconsistent inputs, e.g., an imperfect phone capture\nwith accidental movement, or frames from a monocular video.\n  We compare Avat3r with current state-of-the-art methods for few-input and\nsingle-input scenarios, and find that our method has a competitive advantage in\nboth tasks. Finally, we demonstrate the wide applicability of our proposed\nmodel, creating 3D head avatars from images of different sources, smartphone\ncaptures, single images, and even out-of-domain inputs like antique busts.\n  Project website: https://tobias-kirschstein.github.io/avat3r/\n", "link": "http://arxiv.org/abs/2502.20220v2", "date": "2025-09-15", "relevancy": 3.6227, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7495}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7495}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Avat3r%3A%20Large%20Animatable%20Gaussian%20Reconstruction%20Model%20for%20High-fidelity%0A%20%203D%20Head%20Avatars&body=Title%3A%20Avat3r%3A%20Large%20Animatable%20Gaussian%20Reconstruction%20Model%20for%20High-fidelity%0A%20%203D%20Head%20Avatars%0AAuthor%3A%20Tobias%20Kirschstein%20and%20Javier%20Romero%20and%20Artem%20Sevastopolsky%20and%20Matthias%20Nie%C3%9Fner%20and%20Shunsuke%20Saito%0AAbstract%3A%20%20%20Traditionally%2C%20creating%20photo-realistic%203D%20head%20avatars%20requires%20a%0Astudio-level%20multi-view%20capture%20setup%20and%20expensive%20optimization%20during%0Atest-time%2C%20limiting%20the%20use%20of%20digital%20human%20doubles%20to%20the%20VFX%20industry%20or%0Aoffline%20renderings.%0A%20%20To%20address%20this%20shortcoming%2C%20we%20present%20Avat3r%2C%20which%20regresses%20a%0Ahigh-quality%20and%20animatable%203D%20head%20avatar%20from%20just%20a%20few%20input%20images%2C%20vastly%0Areducing%20compute%20requirements%20during%20inference.%20More%20specifically%2C%20we%20make%0ALarge%20Reconstruction%20Models%20animatable%20and%20learn%20a%20powerful%20prior%20over%203D%20human%0Aheads%20from%20a%20large%20multi-view%20video%20dataset.%20For%20better%203D%20head%0Areconstructions%2C%20we%20employ%20position%20maps%20from%20DUSt3R%20and%20generalized%20feature%0Amaps%20from%20the%20human%20foundation%20model%20Sapiens.%20To%20animate%20the%203D%20head%2C%20our%20key%0Adiscovery%20is%20that%20simple%20cross-attention%20to%20an%20expression%20code%20is%20already%0Asufficient.%20Finally%2C%20we%20increase%20robustness%20by%20feeding%20input%20images%20with%0Adifferent%20expressions%20to%20our%20model%20during%20training%2C%20enabling%20the%20reconstruction%0Aof%203D%20head%20avatars%20from%20inconsistent%20inputs%2C%20e.g.%2C%20an%20imperfect%20phone%20capture%0Awith%20accidental%20movement%2C%20or%20frames%20from%20a%20monocular%20video.%0A%20%20We%20compare%20Avat3r%20with%20current%20state-of-the-art%20methods%20for%20few-input%20and%0Asingle-input%20scenarios%2C%20and%20find%20that%20our%20method%20has%20a%20competitive%20advantage%20in%0Aboth%20tasks.%20Finally%2C%20we%20demonstrate%20the%20wide%20applicability%20of%20our%20proposed%0Amodel%2C%20creating%203D%20head%20avatars%20from%20images%20of%20different%20sources%2C%20smartphone%0Acaptures%2C%20single%20images%2C%20and%20even%20out-of-domain%20inputs%20like%20antique%20busts.%0A%20%20Project%20website%3A%20https%3A//tobias-kirschstein.github.io/avat3r/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20220v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAvat3r%253A%2520Large%2520Animatable%2520Gaussian%2520Reconstruction%2520Model%2520for%2520High-fidelity%250A%2520%25203D%2520Head%2520Avatars%26entry.906535625%3DTobias%2520Kirschstein%2520and%2520Javier%2520Romero%2520and%2520Artem%2520Sevastopolsky%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Shunsuke%2520Saito%26entry.1292438233%3D%2520%2520Traditionally%252C%2520creating%2520photo-realistic%25203D%2520head%2520avatars%2520requires%2520a%250Astudio-level%2520multi-view%2520capture%2520setup%2520and%2520expensive%2520optimization%2520during%250Atest-time%252C%2520limiting%2520the%2520use%2520of%2520digital%2520human%2520doubles%2520to%2520the%2520VFX%2520industry%2520or%250Aoffline%2520renderings.%250A%2520%2520To%2520address%2520this%2520shortcoming%252C%2520we%2520present%2520Avat3r%252C%2520which%2520regresses%2520a%250Ahigh-quality%2520and%2520animatable%25203D%2520head%2520avatar%2520from%2520just%2520a%2520few%2520input%2520images%252C%2520vastly%250Areducing%2520compute%2520requirements%2520during%2520inference.%2520More%2520specifically%252C%2520we%2520make%250ALarge%2520Reconstruction%2520Models%2520animatable%2520and%2520learn%2520a%2520powerful%2520prior%2520over%25203D%2520human%250Aheads%2520from%2520a%2520large%2520multi-view%2520video%2520dataset.%2520For%2520better%25203D%2520head%250Areconstructions%252C%2520we%2520employ%2520position%2520maps%2520from%2520DUSt3R%2520and%2520generalized%2520feature%250Amaps%2520from%2520the%2520human%2520foundation%2520model%2520Sapiens.%2520To%2520animate%2520the%25203D%2520head%252C%2520our%2520key%250Adiscovery%2520is%2520that%2520simple%2520cross-attention%2520to%2520an%2520expression%2520code%2520is%2520already%250Asufficient.%2520Finally%252C%2520we%2520increase%2520robustness%2520by%2520feeding%2520input%2520images%2520with%250Adifferent%2520expressions%2520to%2520our%2520model%2520during%2520training%252C%2520enabling%2520the%2520reconstruction%250Aof%25203D%2520head%2520avatars%2520from%2520inconsistent%2520inputs%252C%2520e.g.%252C%2520an%2520imperfect%2520phone%2520capture%250Awith%2520accidental%2520movement%252C%2520or%2520frames%2520from%2520a%2520monocular%2520video.%250A%2520%2520We%2520compare%2520Avat3r%2520with%2520current%2520state-of-the-art%2520methods%2520for%2520few-input%2520and%250Asingle-input%2520scenarios%252C%2520and%2520find%2520that%2520our%2520method%2520has%2520a%2520competitive%2520advantage%2520in%250Aboth%2520tasks.%2520Finally%252C%2520we%2520demonstrate%2520the%2520wide%2520applicability%2520of%2520our%2520proposed%250Amodel%252C%2520creating%25203D%2520head%2520avatars%2520from%2520images%2520of%2520different%2520sources%252C%2520smartphone%250Acaptures%252C%2520single%2520images%252C%2520and%2520even%2520out-of-domain%2520inputs%2520like%2520antique%2520busts.%250A%2520%2520Project%2520website%253A%2520https%253A//tobias-kirschstein.github.io/avat3r/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20220v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Avat3r%3A%20Large%20Animatable%20Gaussian%20Reconstruction%20Model%20for%20High-fidelity%0A%20%203D%20Head%20Avatars&entry.906535625=Tobias%20Kirschstein%20and%20Javier%20Romero%20and%20Artem%20Sevastopolsky%20and%20Matthias%20Nie%C3%9Fner%20and%20Shunsuke%20Saito&entry.1292438233=%20%20Traditionally%2C%20creating%20photo-realistic%203D%20head%20avatars%20requires%20a%0Astudio-level%20multi-view%20capture%20setup%20and%20expensive%20optimization%20during%0Atest-time%2C%20limiting%20the%20use%20of%20digital%20human%20doubles%20to%20the%20VFX%20industry%20or%0Aoffline%20renderings.%0A%20%20To%20address%20this%20shortcoming%2C%20we%20present%20Avat3r%2C%20which%20regresses%20a%0Ahigh-quality%20and%20animatable%203D%20head%20avatar%20from%20just%20a%20few%20input%20images%2C%20vastly%0Areducing%20compute%20requirements%20during%20inference.%20More%20specifically%2C%20we%20make%0ALarge%20Reconstruction%20Models%20animatable%20and%20learn%20a%20powerful%20prior%20over%203D%20human%0Aheads%20from%20a%20large%20multi-view%20video%20dataset.%20For%20better%203D%20head%0Areconstructions%2C%20we%20employ%20position%20maps%20from%20DUSt3R%20and%20generalized%20feature%0Amaps%20from%20the%20human%20foundation%20model%20Sapiens.%20To%20animate%20the%203D%20head%2C%20our%20key%0Adiscovery%20is%20that%20simple%20cross-attention%20to%20an%20expression%20code%20is%20already%0Asufficient.%20Finally%2C%20we%20increase%20robustness%20by%20feeding%20input%20images%20with%0Adifferent%20expressions%20to%20our%20model%20during%20training%2C%20enabling%20the%20reconstruction%0Aof%203D%20head%20avatars%20from%20inconsistent%20inputs%2C%20e.g.%2C%20an%20imperfect%20phone%20capture%0Awith%20accidental%20movement%2C%20or%20frames%20from%20a%20monocular%20video.%0A%20%20We%20compare%20Avat3r%20with%20current%20state-of-the-art%20methods%20for%20few-input%20and%0Asingle-input%20scenarios%2C%20and%20find%20that%20our%20method%20has%20a%20competitive%20advantage%20in%0Aboth%20tasks.%20Finally%2C%20we%20demonstrate%20the%20wide%20applicability%20of%20our%20proposed%0Amodel%2C%20creating%203D%20head%20avatars%20from%20images%20of%20different%20sources%2C%20smartphone%0Acaptures%2C%20single%20images%2C%20and%20even%20out-of-domain%20inputs%20like%20antique%20busts.%0A%20%20Project%20website%3A%20https%3A//tobias-kirschstein.github.io/avat3r/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20220v2&entry.124074799=Read"},
{"title": "Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting", "author": "Yi-Hsin Li and Thomas Sikora and Sebastian Knorr and M\u00e5arten Sj\u00f6str\u00f6m", "abstract": "  Sparse-view synthesis remains a challenging problem due to the difficulty of\nrecovering accurate geometry and appearance from limited observations. While\nrecent advances in 3D Gaussian Splatting (3DGS) have enabled real-time\nrendering with competitive quality, existing pipelines often rely on\nStructure-from-Motion (SfM) for camera pose estimation, an approach that\nstruggles in genuinely sparse-view settings. Moreover, several SfM-free methods\nreplace SfM with multi-view stereo (MVS) models, but generate massive numbers\nof 3D Gaussians by back-projecting every pixel into 3D space, leading to high\nmemory costs. We propose Segmentation-Driven Initialization for Gaussian\nSplatting (SDI-GS), a method that mitigates inefficiency by leveraging\nregion-based segmentation to identify and retain only structurally significant\nregions. This enables selective downsampling of the dense point cloud,\npreserving scene fidelity while substantially reducing Gaussian count.\nExperiments across diverse benchmarks show that SDI-GS reduces Gaussian count\nby up to 50% and achieves comparable or superior rendering quality in PSNR and\nSSIM, with only marginal degradation in LPIPS. It further enables faster\ntraining and lower memory footprint, advancing the practicality of 3DGS for\nconstrained-view scenarios.\n", "link": "http://arxiv.org/abs/2509.11853v1", "date": "2025-09-15", "relevancy": 3.4718, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.729}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6832}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmentation-Driven%20Initialization%20for%20Sparse-view%203D%20Gaussian%20Splatting&body=Title%3A%20Segmentation-Driven%20Initialization%20for%20Sparse-view%203D%20Gaussian%20Splatting%0AAuthor%3A%20Yi-Hsin%20Li%20and%20Thomas%20Sikora%20and%20Sebastian%20Knorr%20and%20M%C3%A5arten%20Sj%C3%B6str%C3%B6m%0AAbstract%3A%20%20%20Sparse-view%20synthesis%20remains%20a%20challenging%20problem%20due%20to%20the%20difficulty%20of%0Arecovering%20accurate%20geometry%20and%20appearance%20from%20limited%20observations.%20While%0Arecent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20enabled%20real-time%0Arendering%20with%20competitive%20quality%2C%20existing%20pipelines%20often%20rely%20on%0AStructure-from-Motion%20%28SfM%29%20for%20camera%20pose%20estimation%2C%20an%20approach%20that%0Astruggles%20in%20genuinely%20sparse-view%20settings.%20Moreover%2C%20several%20SfM-free%20methods%0Areplace%20SfM%20with%20multi-view%20stereo%20%28MVS%29%20models%2C%20but%20generate%20massive%20numbers%0Aof%203D%20Gaussians%20by%20back-projecting%20every%20pixel%20into%203D%20space%2C%20leading%20to%20high%0Amemory%20costs.%20We%20propose%20Segmentation-Driven%20Initialization%20for%20Gaussian%0ASplatting%20%28SDI-GS%29%2C%20a%20method%20that%20mitigates%20inefficiency%20by%20leveraging%0Aregion-based%20segmentation%20to%20identify%20and%20retain%20only%20structurally%20significant%0Aregions.%20This%20enables%20selective%20downsampling%20of%20the%20dense%20point%20cloud%2C%0Apreserving%20scene%20fidelity%20while%20substantially%20reducing%20Gaussian%20count.%0AExperiments%20across%20diverse%20benchmarks%20show%20that%20SDI-GS%20reduces%20Gaussian%20count%0Aby%20up%20to%2050%25%20and%20achieves%20comparable%20or%20superior%20rendering%20quality%20in%20PSNR%20and%0ASSIM%2C%20with%20only%20marginal%20degradation%20in%20LPIPS.%20It%20further%20enables%20faster%0Atraining%20and%20lower%20memory%20footprint%2C%20advancing%20the%20practicality%20of%203DGS%20for%0Aconstrained-view%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentation-Driven%2520Initialization%2520for%2520Sparse-view%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DYi-Hsin%2520Li%2520and%2520Thomas%2520Sikora%2520and%2520Sebastian%2520Knorr%2520and%2520M%25C3%25A5arten%2520Sj%25C3%25B6str%25C3%25B6m%26entry.1292438233%3D%2520%2520Sparse-view%2520synthesis%2520remains%2520a%2520challenging%2520problem%2520due%2520to%2520the%2520difficulty%2520of%250Arecovering%2520accurate%2520geometry%2520and%2520appearance%2520from%2520limited%2520observations.%2520While%250Arecent%2520advances%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%2520enabled%2520real-time%250Arendering%2520with%2520competitive%2520quality%252C%2520existing%2520pipelines%2520often%2520rely%2520on%250AStructure-from-Motion%2520%2528SfM%2529%2520for%2520camera%2520pose%2520estimation%252C%2520an%2520approach%2520that%250Astruggles%2520in%2520genuinely%2520sparse-view%2520settings.%2520Moreover%252C%2520several%2520SfM-free%2520methods%250Areplace%2520SfM%2520with%2520multi-view%2520stereo%2520%2528MVS%2529%2520models%252C%2520but%2520generate%2520massive%2520numbers%250Aof%25203D%2520Gaussians%2520by%2520back-projecting%2520every%2520pixel%2520into%25203D%2520space%252C%2520leading%2520to%2520high%250Amemory%2520costs.%2520We%2520propose%2520Segmentation-Driven%2520Initialization%2520for%2520Gaussian%250ASplatting%2520%2528SDI-GS%2529%252C%2520a%2520method%2520that%2520mitigates%2520inefficiency%2520by%2520leveraging%250Aregion-based%2520segmentation%2520to%2520identify%2520and%2520retain%2520only%2520structurally%2520significant%250Aregions.%2520This%2520enables%2520selective%2520downsampling%2520of%2520the%2520dense%2520point%2520cloud%252C%250Apreserving%2520scene%2520fidelity%2520while%2520substantially%2520reducing%2520Gaussian%2520count.%250AExperiments%2520across%2520diverse%2520benchmarks%2520show%2520that%2520SDI-GS%2520reduces%2520Gaussian%2520count%250Aby%2520up%2520to%252050%2525%2520and%2520achieves%2520comparable%2520or%2520superior%2520rendering%2520quality%2520in%2520PSNR%2520and%250ASSIM%252C%2520with%2520only%2520marginal%2520degradation%2520in%2520LPIPS.%2520It%2520further%2520enables%2520faster%250Atraining%2520and%2520lower%2520memory%2520footprint%252C%2520advancing%2520the%2520practicality%2520of%25203DGS%2520for%250Aconstrained-view%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation-Driven%20Initialization%20for%20Sparse-view%203D%20Gaussian%20Splatting&entry.906535625=Yi-Hsin%20Li%20and%20Thomas%20Sikora%20and%20Sebastian%20Knorr%20and%20M%C3%A5arten%20Sj%C3%B6str%C3%B6m&entry.1292438233=%20%20Sparse-view%20synthesis%20remains%20a%20challenging%20problem%20due%20to%20the%20difficulty%20of%0Arecovering%20accurate%20geometry%20and%20appearance%20from%20limited%20observations.%20While%0Arecent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20enabled%20real-time%0Arendering%20with%20competitive%20quality%2C%20existing%20pipelines%20often%20rely%20on%0AStructure-from-Motion%20%28SfM%29%20for%20camera%20pose%20estimation%2C%20an%20approach%20that%0Astruggles%20in%20genuinely%20sparse-view%20settings.%20Moreover%2C%20several%20SfM-free%20methods%0Areplace%20SfM%20with%20multi-view%20stereo%20%28MVS%29%20models%2C%20but%20generate%20massive%20numbers%0Aof%203D%20Gaussians%20by%20back-projecting%20every%20pixel%20into%203D%20space%2C%20leading%20to%20high%0Amemory%20costs.%20We%20propose%20Segmentation-Driven%20Initialization%20for%20Gaussian%0ASplatting%20%28SDI-GS%29%2C%20a%20method%20that%20mitigates%20inefficiency%20by%20leveraging%0Aregion-based%20segmentation%20to%20identify%20and%20retain%20only%20structurally%20significant%0Aregions.%20This%20enables%20selective%20downsampling%20of%20the%20dense%20point%20cloud%2C%0Apreserving%20scene%20fidelity%20while%20substantially%20reducing%20Gaussian%20count.%0AExperiments%20across%20diverse%20benchmarks%20show%20that%20SDI-GS%20reduces%20Gaussian%20count%0Aby%20up%20to%2050%25%20and%20achieves%20comparable%20or%20superior%20rendering%20quality%20in%20PSNR%20and%0ASSIM%2C%20with%20only%20marginal%20degradation%20in%20LPIPS.%20It%20further%20enables%20faster%0Atraining%20and%20lower%20memory%20footprint%2C%20advancing%20the%20practicality%20of%203DGS%20for%0Aconstrained-view%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11853v1&entry.124074799=Read"},
{"title": "AvatarSync: Rethinking Talking-Head Animation through Autoregressive\n  Perspective", "author": "Yuchen Deng and Xiuyang Wu and Hai-Tao Zheng and Suiyang Zhang and Yi He and Yuxing Han", "abstract": "  Existing talking-head animation approaches based on Generative Adversarial\nNetworks (GANs) or diffusion models often suffer from inter-frame flicker,\nidentity drift, and slow inference. These limitations inherent to their video\ngeneration pipelines restrict their suitability for applications. To address\nthis, we introduce AvatarSync, an autoregressive framework on phoneme\nrepresentations that generates realistic and controllable talking-head\nanimations from a single reference image, driven directly text or audio input.\nIn addition, AvatarSync adopts a two-stage generation strategy, decoupling\nsemantic modeling from visual dynamics, which is a deliberate \"Divide and\nConquer\" design. The first stage, Facial Keyframe Generation (FKG), focuses on\nphoneme-level semantic representation by leveraging the many-to-one mapping\nfrom text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to\nanchor abstract phonemes to character-level units. Combined with a customized\nText-Frame Causal Attention Mask, the keyframes are generated. The second\nstage, inter-frame interpolation, emphasizes temporal coherence and visual\nsmoothness. We introduce a timestamp-aware adaptive strategy based on a\nselective state space model, enabling efficient bidirectional context\nreasoning. To support deployment, we optimize the inference pipeline to reduce\nlatency without compromising visual fidelity. Extensive experiments show that\nAvatarSync outperforms existing talking-head animation methods in visual\nfidelity, temporal consistency, and computational efficiency, providing a\nscalable and controllable solution.\n", "link": "http://arxiv.org/abs/2509.12052v1", "date": "2025-09-15", "relevancy": 3.1469, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6396}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6396}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AvatarSync%3A%20Rethinking%20Talking-Head%20Animation%20through%20Autoregressive%0A%20%20Perspective&body=Title%3A%20AvatarSync%3A%20Rethinking%20Talking-Head%20Animation%20through%20Autoregressive%0A%20%20Perspective%0AAuthor%3A%20Yuchen%20Deng%20and%20Xiuyang%20Wu%20and%20Hai-Tao%20Zheng%20and%20Suiyang%20Zhang%20and%20Yi%20He%20and%20Yuxing%20Han%0AAbstract%3A%20%20%20Existing%20talking-head%20animation%20approaches%20based%20on%20Generative%20Adversarial%0ANetworks%20%28GANs%29%20or%20diffusion%20models%20often%20suffer%20from%20inter-frame%20flicker%2C%0Aidentity%20drift%2C%20and%20slow%20inference.%20These%20limitations%20inherent%20to%20their%20video%0Ageneration%20pipelines%20restrict%20their%20suitability%20for%20applications.%20To%20address%0Athis%2C%20we%20introduce%20AvatarSync%2C%20an%20autoregressive%20framework%20on%20phoneme%0Arepresentations%20that%20generates%20realistic%20and%20controllable%20talking-head%0Aanimations%20from%20a%20single%20reference%20image%2C%20driven%20directly%20text%20or%20audio%20input.%0AIn%20addition%2C%20AvatarSync%20adopts%20a%20two-stage%20generation%20strategy%2C%20decoupling%0Asemantic%20modeling%20from%20visual%20dynamics%2C%20which%20is%20a%20deliberate%20%22Divide%20and%0AConquer%22%20design.%20The%20first%20stage%2C%20Facial%20Keyframe%20Generation%20%28FKG%29%2C%20focuses%20on%0Aphoneme-level%20semantic%20representation%20by%20leveraging%20the%20many-to-one%20mapping%0Afrom%20text%20or%20audio%20to%20phonemes.%20A%20Phoneme-to-Visual%20Mapping%20is%20constructed%20to%0Aanchor%20abstract%20phonemes%20to%20character-level%20units.%20Combined%20with%20a%20customized%0AText-Frame%20Causal%20Attention%20Mask%2C%20the%20keyframes%20are%20generated.%20The%20second%0Astage%2C%20inter-frame%20interpolation%2C%20emphasizes%20temporal%20coherence%20and%20visual%0Asmoothness.%20We%20introduce%20a%20timestamp-aware%20adaptive%20strategy%20based%20on%20a%0Aselective%20state%20space%20model%2C%20enabling%20efficient%20bidirectional%20context%0Areasoning.%20To%20support%20deployment%2C%20we%20optimize%20the%20inference%20pipeline%20to%20reduce%0Alatency%20without%20compromising%20visual%20fidelity.%20Extensive%20experiments%20show%20that%0AAvatarSync%20outperforms%20existing%20talking-head%20animation%20methods%20in%20visual%0Afidelity%2C%20temporal%20consistency%2C%20and%20computational%20efficiency%2C%20providing%20a%0Ascalable%20and%20controllable%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAvatarSync%253A%2520Rethinking%2520Talking-Head%2520Animation%2520through%2520Autoregressive%250A%2520%2520Perspective%26entry.906535625%3DYuchen%2520Deng%2520and%2520Xiuyang%2520Wu%2520and%2520Hai-Tao%2520Zheng%2520and%2520Suiyang%2520Zhang%2520and%2520Yi%2520He%2520and%2520Yuxing%2520Han%26entry.1292438233%3D%2520%2520Existing%2520talking-head%2520animation%2520approaches%2520based%2520on%2520Generative%2520Adversarial%250ANetworks%2520%2528GANs%2529%2520or%2520diffusion%2520models%2520often%2520suffer%2520from%2520inter-frame%2520flicker%252C%250Aidentity%2520drift%252C%2520and%2520slow%2520inference.%2520These%2520limitations%2520inherent%2520to%2520their%2520video%250Ageneration%2520pipelines%2520restrict%2520their%2520suitability%2520for%2520applications.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520AvatarSync%252C%2520an%2520autoregressive%2520framework%2520on%2520phoneme%250Arepresentations%2520that%2520generates%2520realistic%2520and%2520controllable%2520talking-head%250Aanimations%2520from%2520a%2520single%2520reference%2520image%252C%2520driven%2520directly%2520text%2520or%2520audio%2520input.%250AIn%2520addition%252C%2520AvatarSync%2520adopts%2520a%2520two-stage%2520generation%2520strategy%252C%2520decoupling%250Asemantic%2520modeling%2520from%2520visual%2520dynamics%252C%2520which%2520is%2520a%2520deliberate%2520%2522Divide%2520and%250AConquer%2522%2520design.%2520The%2520first%2520stage%252C%2520Facial%2520Keyframe%2520Generation%2520%2528FKG%2529%252C%2520focuses%2520on%250Aphoneme-level%2520semantic%2520representation%2520by%2520leveraging%2520the%2520many-to-one%2520mapping%250Afrom%2520text%2520or%2520audio%2520to%2520phonemes.%2520A%2520Phoneme-to-Visual%2520Mapping%2520is%2520constructed%2520to%250Aanchor%2520abstract%2520phonemes%2520to%2520character-level%2520units.%2520Combined%2520with%2520a%2520customized%250AText-Frame%2520Causal%2520Attention%2520Mask%252C%2520the%2520keyframes%2520are%2520generated.%2520The%2520second%250Astage%252C%2520inter-frame%2520interpolation%252C%2520emphasizes%2520temporal%2520coherence%2520and%2520visual%250Asmoothness.%2520We%2520introduce%2520a%2520timestamp-aware%2520adaptive%2520strategy%2520based%2520on%2520a%250Aselective%2520state%2520space%2520model%252C%2520enabling%2520efficient%2520bidirectional%2520context%250Areasoning.%2520To%2520support%2520deployment%252C%2520we%2520optimize%2520the%2520inference%2520pipeline%2520to%2520reduce%250Alatency%2520without%2520compromising%2520visual%2520fidelity.%2520Extensive%2520experiments%2520show%2520that%250AAvatarSync%2520outperforms%2520existing%2520talking-head%2520animation%2520methods%2520in%2520visual%250Afidelity%252C%2520temporal%2520consistency%252C%2520and%2520computational%2520efficiency%252C%2520providing%2520a%250Ascalable%2520and%2520controllable%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AvatarSync%3A%20Rethinking%20Talking-Head%20Animation%20through%20Autoregressive%0A%20%20Perspective&entry.906535625=Yuchen%20Deng%20and%20Xiuyang%20Wu%20and%20Hai-Tao%20Zheng%20and%20Suiyang%20Zhang%20and%20Yi%20He%20and%20Yuxing%20Han&entry.1292438233=%20%20Existing%20talking-head%20animation%20approaches%20based%20on%20Generative%20Adversarial%0ANetworks%20%28GANs%29%20or%20diffusion%20models%20often%20suffer%20from%20inter-frame%20flicker%2C%0Aidentity%20drift%2C%20and%20slow%20inference.%20These%20limitations%20inherent%20to%20their%20video%0Ageneration%20pipelines%20restrict%20their%20suitability%20for%20applications.%20To%20address%0Athis%2C%20we%20introduce%20AvatarSync%2C%20an%20autoregressive%20framework%20on%20phoneme%0Arepresentations%20that%20generates%20realistic%20and%20controllable%20talking-head%0Aanimations%20from%20a%20single%20reference%20image%2C%20driven%20directly%20text%20or%20audio%20input.%0AIn%20addition%2C%20AvatarSync%20adopts%20a%20two-stage%20generation%20strategy%2C%20decoupling%0Asemantic%20modeling%20from%20visual%20dynamics%2C%20which%20is%20a%20deliberate%20%22Divide%20and%0AConquer%22%20design.%20The%20first%20stage%2C%20Facial%20Keyframe%20Generation%20%28FKG%29%2C%20focuses%20on%0Aphoneme-level%20semantic%20representation%20by%20leveraging%20the%20many-to-one%20mapping%0Afrom%20text%20or%20audio%20to%20phonemes.%20A%20Phoneme-to-Visual%20Mapping%20is%20constructed%20to%0Aanchor%20abstract%20phonemes%20to%20character-level%20units.%20Combined%20with%20a%20customized%0AText-Frame%20Causal%20Attention%20Mask%2C%20the%20keyframes%20are%20generated.%20The%20second%0Astage%2C%20inter-frame%20interpolation%2C%20emphasizes%20temporal%20coherence%20and%20visual%0Asmoothness.%20We%20introduce%20a%20timestamp-aware%20adaptive%20strategy%20based%20on%20a%0Aselective%20state%20space%20model%2C%20enabling%20efficient%20bidirectional%20context%0Areasoning.%20To%20support%20deployment%2C%20we%20optimize%20the%20inference%20pipeline%20to%20reduce%0Alatency%20without%20compromising%20visual%20fidelity.%20Extensive%20experiments%20show%20that%0AAvatarSync%20outperforms%20existing%20talking-head%20animation%20methods%20in%20visual%0Afidelity%2C%20temporal%20consistency%2C%20and%20computational%20efficiency%2C%20providing%20a%0Ascalable%20and%20controllable%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12052v1&entry.124074799=Read"},
{"title": "HoloGarment: 360\u00b0 Novel View Synthesis of In-the-Wild Garments", "author": "Johanna Karras and Yingwei Li and Yasamin Jafarian and Ira Kemelmacher-Shlizerman", "abstract": "  Novel view synthesis (NVS) of in-the-wild garments is a challenging task due\nsignificant occlusions, complex human poses, and cloth deformations. Prior\nmethods rely on synthetic 3D training data consisting of mostly unoccluded and\nstatic objects, leading to poor generalization on real-world clothing. In this\npaper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3\nimages or a continuous video of a person wearing a garment and generates\n360{\\deg} novel views of the garment in a canonical pose. Our key insight is to\nbridge the domain gap between real and synthetic data with a novel implicit\ntraining paradigm leveraging a combination of large-scale real video data and\nsmall-scale synthetic 3D data to optimize a shared garment embedding space.\nDuring inference, the shared embedding space further enables dynamic\nvideo-to-360{\\deg} NVS through the construction of a garment \"atlas\"\nrepresentation by finetuning a garment embedding on a specific real-world\nvideo. The atlas captures garment-specific geometry and texture across all\nviewpoints, independent of body pose or motion. Extensive experiments show that\nHoloGarment achieves state-of-the-art performance on NVS of in-the-wild\ngarments from images and videos. Notably, our method robustly handles\nchallenging real-world artifacts -- such as wrinkling, pose variation, and\nocclusion -- while maintaining photorealism, view consistency, fine texture\ndetails, and accurate geometry. Visit our project page for additional results:\nhttps://johannakarras.github.io/HoloGarment\n", "link": "http://arxiv.org/abs/2509.12187v1", "date": "2025-09-15", "relevancy": 3.109, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6404}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6244}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoloGarment%3A%20360%C2%B0%20Novel%20View%20Synthesis%20of%20In-the-Wild%20Garments&body=Title%3A%20HoloGarment%3A%20360%C2%B0%20Novel%20View%20Synthesis%20of%20In-the-Wild%20Garments%0AAuthor%3A%20Johanna%20Karras%20and%20Yingwei%20Li%20and%20Yasamin%20Jafarian%20and%20Ira%20Kemelmacher-Shlizerman%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20%28NVS%29%20of%20in-the-wild%20garments%20is%20a%20challenging%20task%20due%0Asignificant%20occlusions%2C%20complex%20human%20poses%2C%20and%20cloth%20deformations.%20Prior%0Amethods%20rely%20on%20synthetic%203D%20training%20data%20consisting%20of%20mostly%20unoccluded%20and%0Astatic%20objects%2C%20leading%20to%20poor%20generalization%20on%20real-world%20clothing.%20In%20this%0Apaper%2C%20we%20propose%20HoloGarment%20%28Hologram-Garment%29%2C%20a%20method%20that%20takes%201-3%0Aimages%20or%20a%20continuous%20video%20of%20a%20person%20wearing%20a%20garment%20and%20generates%0A360%7B%5Cdeg%7D%20novel%20views%20of%20the%20garment%20in%20a%20canonical%20pose.%20Our%20key%20insight%20is%20to%0Abridge%20the%20domain%20gap%20between%20real%20and%20synthetic%20data%20with%20a%20novel%20implicit%0Atraining%20paradigm%20leveraging%20a%20combination%20of%20large-scale%20real%20video%20data%20and%0Asmall-scale%20synthetic%203D%20data%20to%20optimize%20a%20shared%20garment%20embedding%20space.%0ADuring%20inference%2C%20the%20shared%20embedding%20space%20further%20enables%20dynamic%0Avideo-to-360%7B%5Cdeg%7D%20NVS%20through%20the%20construction%20of%20a%20garment%20%22atlas%22%0Arepresentation%20by%20finetuning%20a%20garment%20embedding%20on%20a%20specific%20real-world%0Avideo.%20The%20atlas%20captures%20garment-specific%20geometry%20and%20texture%20across%20all%0Aviewpoints%2C%20independent%20of%20body%20pose%20or%20motion.%20Extensive%20experiments%20show%20that%0AHoloGarment%20achieves%20state-of-the-art%20performance%20on%20NVS%20of%20in-the-wild%0Agarments%20from%20images%20and%20videos.%20Notably%2C%20our%20method%20robustly%20handles%0Achallenging%20real-world%20artifacts%20--%20such%20as%20wrinkling%2C%20pose%20variation%2C%20and%0Aocclusion%20--%20while%20maintaining%20photorealism%2C%20view%20consistency%2C%20fine%20texture%0Adetails%2C%20and%20accurate%20geometry.%20Visit%20our%20project%20page%20for%20additional%20results%3A%0Ahttps%3A//johannakarras.github.io/HoloGarment%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoloGarment%253A%2520360%25C2%25B0%2520Novel%2520View%2520Synthesis%2520of%2520In-the-Wild%2520Garments%26entry.906535625%3DJohanna%2520Karras%2520and%2520Yingwei%2520Li%2520and%2520Yasamin%2520Jafarian%2520and%2520Ira%2520Kemelmacher-Shlizerman%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520%2528NVS%2529%2520of%2520in-the-wild%2520garments%2520is%2520a%2520challenging%2520task%2520due%250Asignificant%2520occlusions%252C%2520complex%2520human%2520poses%252C%2520and%2520cloth%2520deformations.%2520Prior%250Amethods%2520rely%2520on%2520synthetic%25203D%2520training%2520data%2520consisting%2520of%2520mostly%2520unoccluded%2520and%250Astatic%2520objects%252C%2520leading%2520to%2520poor%2520generalization%2520on%2520real-world%2520clothing.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520HoloGarment%2520%2528Hologram-Garment%2529%252C%2520a%2520method%2520that%2520takes%25201-3%250Aimages%2520or%2520a%2520continuous%2520video%2520of%2520a%2520person%2520wearing%2520a%2520garment%2520and%2520generates%250A360%257B%255Cdeg%257D%2520novel%2520views%2520of%2520the%2520garment%2520in%2520a%2520canonical%2520pose.%2520Our%2520key%2520insight%2520is%2520to%250Abridge%2520the%2520domain%2520gap%2520between%2520real%2520and%2520synthetic%2520data%2520with%2520a%2520novel%2520implicit%250Atraining%2520paradigm%2520leveraging%2520a%2520combination%2520of%2520large-scale%2520real%2520video%2520data%2520and%250Asmall-scale%2520synthetic%25203D%2520data%2520to%2520optimize%2520a%2520shared%2520garment%2520embedding%2520space.%250ADuring%2520inference%252C%2520the%2520shared%2520embedding%2520space%2520further%2520enables%2520dynamic%250Avideo-to-360%257B%255Cdeg%257D%2520NVS%2520through%2520the%2520construction%2520of%2520a%2520garment%2520%2522atlas%2522%250Arepresentation%2520by%2520finetuning%2520a%2520garment%2520embedding%2520on%2520a%2520specific%2520real-world%250Avideo.%2520The%2520atlas%2520captures%2520garment-specific%2520geometry%2520and%2520texture%2520across%2520all%250Aviewpoints%252C%2520independent%2520of%2520body%2520pose%2520or%2520motion.%2520Extensive%2520experiments%2520show%2520that%250AHoloGarment%2520achieves%2520state-of-the-art%2520performance%2520on%2520NVS%2520of%2520in-the-wild%250Agarments%2520from%2520images%2520and%2520videos.%2520Notably%252C%2520our%2520method%2520robustly%2520handles%250Achallenging%2520real-world%2520artifacts%2520--%2520such%2520as%2520wrinkling%252C%2520pose%2520variation%252C%2520and%250Aocclusion%2520--%2520while%2520maintaining%2520photorealism%252C%2520view%2520consistency%252C%2520fine%2520texture%250Adetails%252C%2520and%2520accurate%2520geometry.%2520Visit%2520our%2520project%2520page%2520for%2520additional%2520results%253A%250Ahttps%253A//johannakarras.github.io/HoloGarment%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoloGarment%3A%20360%C2%B0%20Novel%20View%20Synthesis%20of%20In-the-Wild%20Garments&entry.906535625=Johanna%20Karras%20and%20Yingwei%20Li%20and%20Yasamin%20Jafarian%20and%20Ira%20Kemelmacher-Shlizerman&entry.1292438233=%20%20Novel%20view%20synthesis%20%28NVS%29%20of%20in-the-wild%20garments%20is%20a%20challenging%20task%20due%0Asignificant%20occlusions%2C%20complex%20human%20poses%2C%20and%20cloth%20deformations.%20Prior%0Amethods%20rely%20on%20synthetic%203D%20training%20data%20consisting%20of%20mostly%20unoccluded%20and%0Astatic%20objects%2C%20leading%20to%20poor%20generalization%20on%20real-world%20clothing.%20In%20this%0Apaper%2C%20we%20propose%20HoloGarment%20%28Hologram-Garment%29%2C%20a%20method%20that%20takes%201-3%0Aimages%20or%20a%20continuous%20video%20of%20a%20person%20wearing%20a%20garment%20and%20generates%0A360%7B%5Cdeg%7D%20novel%20views%20of%20the%20garment%20in%20a%20canonical%20pose.%20Our%20key%20insight%20is%20to%0Abridge%20the%20domain%20gap%20between%20real%20and%20synthetic%20data%20with%20a%20novel%20implicit%0Atraining%20paradigm%20leveraging%20a%20combination%20of%20large-scale%20real%20video%20data%20and%0Asmall-scale%20synthetic%203D%20data%20to%20optimize%20a%20shared%20garment%20embedding%20space.%0ADuring%20inference%2C%20the%20shared%20embedding%20space%20further%20enables%20dynamic%0Avideo-to-360%7B%5Cdeg%7D%20NVS%20through%20the%20construction%20of%20a%20garment%20%22atlas%22%0Arepresentation%20by%20finetuning%20a%20garment%20embedding%20on%20a%20specific%20real-world%0Avideo.%20The%20atlas%20captures%20garment-specific%20geometry%20and%20texture%20across%20all%0Aviewpoints%2C%20independent%20of%20body%20pose%20or%20motion.%20Extensive%20experiments%20show%20that%0AHoloGarment%20achieves%20state-of-the-art%20performance%20on%20NVS%20of%20in-the-wild%0Agarments%20from%20images%20and%20videos.%20Notably%2C%20our%20method%20robustly%20handles%0Achallenging%20real-world%20artifacts%20--%20such%20as%20wrinkling%2C%20pose%20variation%2C%20and%0Aocclusion%20--%20while%20maintaining%20photorealism%2C%20view%20consistency%2C%20fine%20texture%0Adetails%2C%20and%20accurate%20geometry.%20Visit%20our%20project%20page%20for%20additional%20results%3A%0Ahttps%3A//johannakarras.github.io/HoloGarment%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12187v1&entry.124074799=Read"},
{"title": "Regist3R: Incremental Registration with Stereo Foundation Model", "author": "Sidun Liu and Wenyu Li and Peng Qiao and Yong Dou", "abstract": "  Multi-view 3D reconstruction has remained an essential yet challenging\nproblem in the field of computer vision. While DUSt3R and its successors have\nachieved breakthroughs in 3D reconstruction from unposed images, these methods\nexhibit significant limitations when scaling to multi-view scenarios, including\nhigh computational cost and cumulative error induced by global alignment. To\naddress these challenges, we propose Regist3R, a novel stereo foundation model\ntailored for efficient and scalable incremental reconstruction. Regist3R\nleverages an incremental reconstruction paradigm, enabling large-scale 3D\nreconstructions from unordered and many-view image collections. We evaluate\nRegist3R on public datasets for camera pose estimation and 3D reconstruction.\nOur experiments demonstrate that Regist3R achieves comparable performance with\noptimization-based methods while significantly improving computational\nefficiency, and outperforms existing multi-view reconstruction models.\nFurthermore, to assess its performance in real-world applications, we introduce\na challenging oblique aerial dataset which has long spatial spans and hundreds\nof views. The results highlight the effectiveness of Regist3R. We also\ndemonstrate the first attempt to reconstruct large-scale scenes encompassing\nover thousands of views through pointmap-based foundation models, showcasing\nits potential for practical applications in large-scale 3D reconstruction\ntasks, including urban modeling, aerial mapping, and beyond.\n", "link": "http://arxiv.org/abs/2504.12356v2", "date": "2025-09-15", "relevancy": 3.1045, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6236}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regist3R%3A%20Incremental%20Registration%20with%20Stereo%20Foundation%20Model&body=Title%3A%20Regist3R%3A%20Incremental%20Registration%20with%20Stereo%20Foundation%20Model%0AAuthor%3A%20Sidun%20Liu%20and%20Wenyu%20Li%20and%20Peng%20Qiao%20and%20Yong%20Dou%0AAbstract%3A%20%20%20Multi-view%203D%20reconstruction%20has%20remained%20an%20essential%20yet%20challenging%0Aproblem%20in%20the%20field%20of%20computer%20vision.%20While%20DUSt3R%20and%20its%20successors%20have%0Aachieved%20breakthroughs%20in%203D%20reconstruction%20from%20unposed%20images%2C%20these%20methods%0Aexhibit%20significant%20limitations%20when%20scaling%20to%20multi-view%20scenarios%2C%20including%0Ahigh%20computational%20cost%20and%20cumulative%20error%20induced%20by%20global%20alignment.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20Regist3R%2C%20a%20novel%20stereo%20foundation%20model%0Atailored%20for%20efficient%20and%20scalable%20incremental%20reconstruction.%20Regist3R%0Aleverages%20an%20incremental%20reconstruction%20paradigm%2C%20enabling%20large-scale%203D%0Areconstructions%20from%20unordered%20and%20many-view%20image%20collections.%20We%20evaluate%0ARegist3R%20on%20public%20datasets%20for%20camera%20pose%20estimation%20and%203D%20reconstruction.%0AOur%20experiments%20demonstrate%20that%20Regist3R%20achieves%20comparable%20performance%20with%0Aoptimization-based%20methods%20while%20significantly%20improving%20computational%0Aefficiency%2C%20and%20outperforms%20existing%20multi-view%20reconstruction%20models.%0AFurthermore%2C%20to%20assess%20its%20performance%20in%20real-world%20applications%2C%20we%20introduce%0Aa%20challenging%20oblique%20aerial%20dataset%20which%20has%20long%20spatial%20spans%20and%20hundreds%0Aof%20views.%20The%20results%20highlight%20the%20effectiveness%20of%20Regist3R.%20We%20also%0Ademonstrate%20the%20first%20attempt%20to%20reconstruct%20large-scale%20scenes%20encompassing%0Aover%20thousands%20of%20views%20through%20pointmap-based%20foundation%20models%2C%20showcasing%0Aits%20potential%20for%20practical%20applications%20in%20large-scale%203D%20reconstruction%0Atasks%2C%20including%20urban%20modeling%2C%20aerial%20mapping%2C%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12356v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegist3R%253A%2520Incremental%2520Registration%2520with%2520Stereo%2520Foundation%2520Model%26entry.906535625%3DSidun%2520Liu%2520and%2520Wenyu%2520Li%2520and%2520Peng%2520Qiao%2520and%2520Yong%2520Dou%26entry.1292438233%3D%2520%2520Multi-view%25203D%2520reconstruction%2520has%2520remained%2520an%2520essential%2520yet%2520challenging%250Aproblem%2520in%2520the%2520field%2520of%2520computer%2520vision.%2520While%2520DUSt3R%2520and%2520its%2520successors%2520have%250Aachieved%2520breakthroughs%2520in%25203D%2520reconstruction%2520from%2520unposed%2520images%252C%2520these%2520methods%250Aexhibit%2520significant%2520limitations%2520when%2520scaling%2520to%2520multi-view%2520scenarios%252C%2520including%250Ahigh%2520computational%2520cost%2520and%2520cumulative%2520error%2520induced%2520by%2520global%2520alignment.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520Regist3R%252C%2520a%2520novel%2520stereo%2520foundation%2520model%250Atailored%2520for%2520efficient%2520and%2520scalable%2520incremental%2520reconstruction.%2520Regist3R%250Aleverages%2520an%2520incremental%2520reconstruction%2520paradigm%252C%2520enabling%2520large-scale%25203D%250Areconstructions%2520from%2520unordered%2520and%2520many-view%2520image%2520collections.%2520We%2520evaluate%250ARegist3R%2520on%2520public%2520datasets%2520for%2520camera%2520pose%2520estimation%2520and%25203D%2520reconstruction.%250AOur%2520experiments%2520demonstrate%2520that%2520Regist3R%2520achieves%2520comparable%2520performance%2520with%250Aoptimization-based%2520methods%2520while%2520significantly%2520improving%2520computational%250Aefficiency%252C%2520and%2520outperforms%2520existing%2520multi-view%2520reconstruction%2520models.%250AFurthermore%252C%2520to%2520assess%2520its%2520performance%2520in%2520real-world%2520applications%252C%2520we%2520introduce%250Aa%2520challenging%2520oblique%2520aerial%2520dataset%2520which%2520has%2520long%2520spatial%2520spans%2520and%2520hundreds%250Aof%2520views.%2520The%2520results%2520highlight%2520the%2520effectiveness%2520of%2520Regist3R.%2520We%2520also%250Ademonstrate%2520the%2520first%2520attempt%2520to%2520reconstruct%2520large-scale%2520scenes%2520encompassing%250Aover%2520thousands%2520of%2520views%2520through%2520pointmap-based%2520foundation%2520models%252C%2520showcasing%250Aits%2520potential%2520for%2520practical%2520applications%2520in%2520large-scale%25203D%2520reconstruction%250Atasks%252C%2520including%2520urban%2520modeling%252C%2520aerial%2520mapping%252C%2520and%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12356v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regist3R%3A%20Incremental%20Registration%20with%20Stereo%20Foundation%20Model&entry.906535625=Sidun%20Liu%20and%20Wenyu%20Li%20and%20Peng%20Qiao%20and%20Yong%20Dou&entry.1292438233=%20%20Multi-view%203D%20reconstruction%20has%20remained%20an%20essential%20yet%20challenging%0Aproblem%20in%20the%20field%20of%20computer%20vision.%20While%20DUSt3R%20and%20its%20successors%20have%0Aachieved%20breakthroughs%20in%203D%20reconstruction%20from%20unposed%20images%2C%20these%20methods%0Aexhibit%20significant%20limitations%20when%20scaling%20to%20multi-view%20scenarios%2C%20including%0Ahigh%20computational%20cost%20and%20cumulative%20error%20induced%20by%20global%20alignment.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20Regist3R%2C%20a%20novel%20stereo%20foundation%20model%0Atailored%20for%20efficient%20and%20scalable%20incremental%20reconstruction.%20Regist3R%0Aleverages%20an%20incremental%20reconstruction%20paradigm%2C%20enabling%20large-scale%203D%0Areconstructions%20from%20unordered%20and%20many-view%20image%20collections.%20We%20evaluate%0ARegist3R%20on%20public%20datasets%20for%20camera%20pose%20estimation%20and%203D%20reconstruction.%0AOur%20experiments%20demonstrate%20that%20Regist3R%20achieves%20comparable%20performance%20with%0Aoptimization-based%20methods%20while%20significantly%20improving%20computational%0Aefficiency%2C%20and%20outperforms%20existing%20multi-view%20reconstruction%20models.%0AFurthermore%2C%20to%20assess%20its%20performance%20in%20real-world%20applications%2C%20we%20introduce%0Aa%20challenging%20oblique%20aerial%20dataset%20which%20has%20long%20spatial%20spans%20and%20hundreds%0Aof%20views.%20The%20results%20highlight%20the%20effectiveness%20of%20Regist3R.%20We%20also%0Ademonstrate%20the%20first%20attempt%20to%20reconstruct%20large-scale%20scenes%20encompassing%0Aover%20thousands%20of%20views%20through%20pointmap-based%20foundation%20models%2C%20showcasing%0Aits%20potential%20for%20practical%20applications%20in%20large-scale%203D%20reconstruction%0Atasks%2C%20including%20urban%20modeling%2C%20aerial%20mapping%2C%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12356v2&entry.124074799=Read"},
{"title": "Lost in Embeddings: Information Loss in Vision-Language Models", "author": "Wenyan Li and Raphael Tang and Chengzu Li and Caiqi Zhang and Ivan Vuli\u0107 and Anders S\u00f8gaard", "abstract": "  Vision--language models (VLMs) often process visual inputs through a\npretrained vision encoder, followed by a projection into the language model's\nembedding space via a connector component. While crucial for modality fusion,\nthe potential information loss induced by this projection step and its direct\nimpact on model capabilities remain understudied. We introduce two\ncomplementary approaches to examine and quantify this loss by analyzing the\nlatent representation space. First, we evaluate semantic information\npreservation by analyzing changes in k-nearest neighbor relationships between\nimage representations, before and after projection. Second, we directly measure\ninformation loss by reconstructing visual embeddings from the projected\nrepresentation, localizing loss at an image patch level. Experiments reveal\nthat connectors substantially distort the local geometry of visual\nrepresentations, with k-nearest neighbors diverging by 40--60\\%\npost-projection, correlating with degradation in retrieval performance. The\npatch-level embedding reconstruction provides interpretable insights for model\nbehavior on visually grounded question-answering tasks, finding that areas of\nhigh information loss reliably predict instances where models struggle.\n", "link": "http://arxiv.org/abs/2509.11986v1", "date": "2025-09-15", "relevancy": 2.9891, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6297}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lost%20in%20Embeddings%3A%20Information%20Loss%20in%20Vision-Language%20Models&body=Title%3A%20Lost%20in%20Embeddings%3A%20Information%20Loss%20in%20Vision-Language%20Models%0AAuthor%3A%20Wenyan%20Li%20and%20Raphael%20Tang%20and%20Chengzu%20Li%20and%20Caiqi%20Zhang%20and%20Ivan%20Vuli%C4%87%20and%20Anders%20S%C3%B8gaard%0AAbstract%3A%20%20%20Vision--language%20models%20%28VLMs%29%20often%20process%20visual%20inputs%20through%20a%0Apretrained%20vision%20encoder%2C%20followed%20by%20a%20projection%20into%20the%20language%20model%27s%0Aembedding%20space%20via%20a%20connector%20component.%20While%20crucial%20for%20modality%20fusion%2C%0Athe%20potential%20information%20loss%20induced%20by%20this%20projection%20step%20and%20its%20direct%0Aimpact%20on%20model%20capabilities%20remain%20understudied.%20We%20introduce%20two%0Acomplementary%20approaches%20to%20examine%20and%20quantify%20this%20loss%20by%20analyzing%20the%0Alatent%20representation%20space.%20First%2C%20we%20evaluate%20semantic%20information%0Apreservation%20by%20analyzing%20changes%20in%20k-nearest%20neighbor%20relationships%20between%0Aimage%20representations%2C%20before%20and%20after%20projection.%20Second%2C%20we%20directly%20measure%0Ainformation%20loss%20by%20reconstructing%20visual%20embeddings%20from%20the%20projected%0Arepresentation%2C%20localizing%20loss%20at%20an%20image%20patch%20level.%20Experiments%20reveal%0Athat%20connectors%20substantially%20distort%20the%20local%20geometry%20of%20visual%0Arepresentations%2C%20with%20k-nearest%20neighbors%20diverging%20by%2040--60%5C%25%0Apost-projection%2C%20correlating%20with%20degradation%20in%20retrieval%20performance.%20The%0Apatch-level%20embedding%20reconstruction%20provides%20interpretable%20insights%20for%20model%0Abehavior%20on%20visually%20grounded%20question-answering%20tasks%2C%20finding%20that%20areas%20of%0Ahigh%20information%20loss%20reliably%20predict%20instances%20where%20models%20struggle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11986v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLost%2520in%2520Embeddings%253A%2520Information%2520Loss%2520in%2520Vision-Language%2520Models%26entry.906535625%3DWenyan%2520Li%2520and%2520Raphael%2520Tang%2520and%2520Chengzu%2520Li%2520and%2520Caiqi%2520Zhang%2520and%2520Ivan%2520Vuli%25C4%2587%2520and%2520Anders%2520S%25C3%25B8gaard%26entry.1292438233%3D%2520%2520Vision--language%2520models%2520%2528VLMs%2529%2520often%2520process%2520visual%2520inputs%2520through%2520a%250Apretrained%2520vision%2520encoder%252C%2520followed%2520by%2520a%2520projection%2520into%2520the%2520language%2520model%2527s%250Aembedding%2520space%2520via%2520a%2520connector%2520component.%2520While%2520crucial%2520for%2520modality%2520fusion%252C%250Athe%2520potential%2520information%2520loss%2520induced%2520by%2520this%2520projection%2520step%2520and%2520its%2520direct%250Aimpact%2520on%2520model%2520capabilities%2520remain%2520understudied.%2520We%2520introduce%2520two%250Acomplementary%2520approaches%2520to%2520examine%2520and%2520quantify%2520this%2520loss%2520by%2520analyzing%2520the%250Alatent%2520representation%2520space.%2520First%252C%2520we%2520evaluate%2520semantic%2520information%250Apreservation%2520by%2520analyzing%2520changes%2520in%2520k-nearest%2520neighbor%2520relationships%2520between%250Aimage%2520representations%252C%2520before%2520and%2520after%2520projection.%2520Second%252C%2520we%2520directly%2520measure%250Ainformation%2520loss%2520by%2520reconstructing%2520visual%2520embeddings%2520from%2520the%2520projected%250Arepresentation%252C%2520localizing%2520loss%2520at%2520an%2520image%2520patch%2520level.%2520Experiments%2520reveal%250Athat%2520connectors%2520substantially%2520distort%2520the%2520local%2520geometry%2520of%2520visual%250Arepresentations%252C%2520with%2520k-nearest%2520neighbors%2520diverging%2520by%252040--60%255C%2525%250Apost-projection%252C%2520correlating%2520with%2520degradation%2520in%2520retrieval%2520performance.%2520The%250Apatch-level%2520embedding%2520reconstruction%2520provides%2520interpretable%2520insights%2520for%2520model%250Abehavior%2520on%2520visually%2520grounded%2520question-answering%2520tasks%252C%2520finding%2520that%2520areas%2520of%250Ahigh%2520information%2520loss%2520reliably%2520predict%2520instances%2520where%2520models%2520struggle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11986v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20in%20Embeddings%3A%20Information%20Loss%20in%20Vision-Language%20Models&entry.906535625=Wenyan%20Li%20and%20Raphael%20Tang%20and%20Chengzu%20Li%20and%20Caiqi%20Zhang%20and%20Ivan%20Vuli%C4%87%20and%20Anders%20S%C3%B8gaard&entry.1292438233=%20%20Vision--language%20models%20%28VLMs%29%20often%20process%20visual%20inputs%20through%20a%0Apretrained%20vision%20encoder%2C%20followed%20by%20a%20projection%20into%20the%20language%20model%27s%0Aembedding%20space%20via%20a%20connector%20component.%20While%20crucial%20for%20modality%20fusion%2C%0Athe%20potential%20information%20loss%20induced%20by%20this%20projection%20step%20and%20its%20direct%0Aimpact%20on%20model%20capabilities%20remain%20understudied.%20We%20introduce%20two%0Acomplementary%20approaches%20to%20examine%20and%20quantify%20this%20loss%20by%20analyzing%20the%0Alatent%20representation%20space.%20First%2C%20we%20evaluate%20semantic%20information%0Apreservation%20by%20analyzing%20changes%20in%20k-nearest%20neighbor%20relationships%20between%0Aimage%20representations%2C%20before%20and%20after%20projection.%20Second%2C%20we%20directly%20measure%0Ainformation%20loss%20by%20reconstructing%20visual%20embeddings%20from%20the%20projected%0Arepresentation%2C%20localizing%20loss%20at%20an%20image%20patch%20level.%20Experiments%20reveal%0Athat%20connectors%20substantially%20distort%20the%20local%20geometry%20of%20visual%0Arepresentations%2C%20with%20k-nearest%20neighbors%20diverging%20by%2040--60%5C%25%0Apost-projection%2C%20correlating%20with%20degradation%20in%20retrieval%20performance.%20The%0Apatch-level%20embedding%20reconstruction%20provides%20interpretable%20insights%20for%20model%0Abehavior%20on%20visually%20grounded%20question-answering%20tasks%2C%20finding%20that%20areas%20of%0Ahigh%20information%20loss%20reliably%20predict%20instances%20where%20models%20struggle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11986v1&entry.124074799=Read"},
{"title": "On the Geometric Accuracy of Implicit and Primitive-based\n  Representations Derived from View Rendering Constraints", "author": "Elias De Smijter and Renaud Detry and Christophe De Vleeschouwer", "abstract": "  We present the first systematic comparison of implicit and explicit Novel\nView Synthesis methods for space-based 3D object reconstruction, evaluating the\nrole of appearance embeddings. While embeddings improve photometric fidelity by\nmodeling lighting variation, we show they do not translate into meaningful\ngains in geometric accuracy - a critical requirement for space robotics\napplications. Using the SPEED+ dataset, we compare K-Planes, Gaussian\nSplatting, and Convex Splatting, and demonstrate that embeddings primarily\nreduce the number of primitives needed for explicit methods rather than\nenhancing geometric fidelity. Moreover, convex splatting achieves more compact\nand clutter-free representations than Gaussian splatting, offering advantages\nfor safety-critical applications such as interaction and collision avoidance.\nOur findings clarify the limits of appearance embeddings for geometry-centric\ntasks and highlight trade-offs between reconstruction quality and\nrepresentation efficiency in space scenarios.\n", "link": "http://arxiv.org/abs/2509.10241v2", "date": "2025-09-15", "relevancy": 2.9833, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6194}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5858}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Geometric%20Accuracy%20of%20Implicit%20and%20Primitive-based%0A%20%20Representations%20Derived%20from%20View%20Rendering%20Constraints&body=Title%3A%20On%20the%20Geometric%20Accuracy%20of%20Implicit%20and%20Primitive-based%0A%20%20Representations%20Derived%20from%20View%20Rendering%20Constraints%0AAuthor%3A%20Elias%20De%20Smijter%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer%0AAbstract%3A%20%20%20We%20present%20the%20first%20systematic%20comparison%20of%20implicit%20and%20explicit%20Novel%0AView%20Synthesis%20methods%20for%20space-based%203D%20object%20reconstruction%2C%20evaluating%20the%0Arole%20of%20appearance%20embeddings.%20While%20embeddings%20improve%20photometric%20fidelity%20by%0Amodeling%20lighting%20variation%2C%20we%20show%20they%20do%20not%20translate%20into%20meaningful%0Agains%20in%20geometric%20accuracy%20-%20a%20critical%20requirement%20for%20space%20robotics%0Aapplications.%20Using%20the%20SPEED%2B%20dataset%2C%20we%20compare%20K-Planes%2C%20Gaussian%0ASplatting%2C%20and%20Convex%20Splatting%2C%20and%20demonstrate%20that%20embeddings%20primarily%0Areduce%20the%20number%20of%20primitives%20needed%20for%20explicit%20methods%20rather%20than%0Aenhancing%20geometric%20fidelity.%20Moreover%2C%20convex%20splatting%20achieves%20more%20compact%0Aand%20clutter-free%20representations%20than%20Gaussian%20splatting%2C%20offering%20advantages%0Afor%20safety-critical%20applications%20such%20as%20interaction%20and%20collision%20avoidance.%0AOur%20findings%20clarify%20the%20limits%20of%20appearance%20embeddings%20for%20geometry-centric%0Atasks%20and%20highlight%20trade-offs%20between%20reconstruction%20quality%20and%0Arepresentation%20efficiency%20in%20space%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.10241v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Geometric%2520Accuracy%2520of%2520Implicit%2520and%2520Primitive-based%250A%2520%2520Representations%2520Derived%2520from%2520View%2520Rendering%2520Constraints%26entry.906535625%3DElias%2520De%2520Smijter%2520and%2520Renaud%2520Detry%2520and%2520Christophe%2520De%2520Vleeschouwer%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520first%2520systematic%2520comparison%2520of%2520implicit%2520and%2520explicit%2520Novel%250AView%2520Synthesis%2520methods%2520for%2520space-based%25203D%2520object%2520reconstruction%252C%2520evaluating%2520the%250Arole%2520of%2520appearance%2520embeddings.%2520While%2520embeddings%2520improve%2520photometric%2520fidelity%2520by%250Amodeling%2520lighting%2520variation%252C%2520we%2520show%2520they%2520do%2520not%2520translate%2520into%2520meaningful%250Agains%2520in%2520geometric%2520accuracy%2520-%2520a%2520critical%2520requirement%2520for%2520space%2520robotics%250Aapplications.%2520Using%2520the%2520SPEED%252B%2520dataset%252C%2520we%2520compare%2520K-Planes%252C%2520Gaussian%250ASplatting%252C%2520and%2520Convex%2520Splatting%252C%2520and%2520demonstrate%2520that%2520embeddings%2520primarily%250Areduce%2520the%2520number%2520of%2520primitives%2520needed%2520for%2520explicit%2520methods%2520rather%2520than%250Aenhancing%2520geometric%2520fidelity.%2520Moreover%252C%2520convex%2520splatting%2520achieves%2520more%2520compact%250Aand%2520clutter-free%2520representations%2520than%2520Gaussian%2520splatting%252C%2520offering%2520advantages%250Afor%2520safety-critical%2520applications%2520such%2520as%2520interaction%2520and%2520collision%2520avoidance.%250AOur%2520findings%2520clarify%2520the%2520limits%2520of%2520appearance%2520embeddings%2520for%2520geometry-centric%250Atasks%2520and%2520highlight%2520trade-offs%2520between%2520reconstruction%2520quality%2520and%250Arepresentation%2520efficiency%2520in%2520space%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10241v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Geometric%20Accuracy%20of%20Implicit%20and%20Primitive-based%0A%20%20Representations%20Derived%20from%20View%20Rendering%20Constraints&entry.906535625=Elias%20De%20Smijter%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer&entry.1292438233=%20%20We%20present%20the%20first%20systematic%20comparison%20of%20implicit%20and%20explicit%20Novel%0AView%20Synthesis%20methods%20for%20space-based%203D%20object%20reconstruction%2C%20evaluating%20the%0Arole%20of%20appearance%20embeddings.%20While%20embeddings%20improve%20photometric%20fidelity%20by%0Amodeling%20lighting%20variation%2C%20we%20show%20they%20do%20not%20translate%20into%20meaningful%0Agains%20in%20geometric%20accuracy%20-%20a%20critical%20requirement%20for%20space%20robotics%0Aapplications.%20Using%20the%20SPEED%2B%20dataset%2C%20we%20compare%20K-Planes%2C%20Gaussian%0ASplatting%2C%20and%20Convex%20Splatting%2C%20and%20demonstrate%20that%20embeddings%20primarily%0Areduce%20the%20number%20of%20primitives%20needed%20for%20explicit%20methods%20rather%20than%0Aenhancing%20geometric%20fidelity.%20Moreover%2C%20convex%20splatting%20achieves%20more%20compact%0Aand%20clutter-free%20representations%20than%20Gaussian%20splatting%2C%20offering%20advantages%0Afor%20safety-critical%20applications%20such%20as%20interaction%20and%20collision%20avoidance.%0AOur%20findings%20clarify%20the%20limits%20of%20appearance%20embeddings%20for%20geometry-centric%0Atasks%20and%20highlight%20trade-offs%20between%20reconstruction%20quality%20and%0Arepresentation%20efficiency%20in%20space%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.10241v2&entry.124074799=Read"},
{"title": "Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation", "author": "Tim Lebailly and Vijay Veerabadran and Satwik Kottur and Karl Ridgeway and Michael Louis Iuzzolino", "abstract": "  Generative vision-language models (VLMs) exhibit strong high-level image\nunderstanding but lack spatially dense alignment between vision and language\nmodalities, as our findings indicate. Orthogonal to advancements in generative\nVLMs, another line of research has focused on representation learning for\nvision-language alignment, targeting zero-shot inference for dense tasks like\nsegmentation. In this work, we bridge these two directions by densely aligning\nimages with synthetic descriptions generated by VLMs. Synthetic captions are\ninexpensive, scalable, and easy to generate, making them an excellent source of\nhigh-level semantic understanding for dense alignment methods. Empirically, our\napproach outperforms prior work on standard zero-shot open-vocabulary\nsegmentation benchmarks/datasets, while also being more data-efficient.\n", "link": "http://arxiv.org/abs/2509.11840v1", "date": "2025-09-15", "relevancy": 2.9799, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5968}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Captions%20for%20Open-Vocabulary%20Zero-Shot%20Segmentation&body=Title%3A%20Synthetic%20Captions%20for%20Open-Vocabulary%20Zero-Shot%20Segmentation%0AAuthor%3A%20Tim%20Lebailly%20and%20Vijay%20Veerabadran%20and%20Satwik%20Kottur%20and%20Karl%20Ridgeway%20and%20Michael%20Louis%20Iuzzolino%0AAbstract%3A%20%20%20Generative%20vision-language%20models%20%28VLMs%29%20exhibit%20strong%20high-level%20image%0Aunderstanding%20but%20lack%20spatially%20dense%20alignment%20between%20vision%20and%20language%0Amodalities%2C%20as%20our%20findings%20indicate.%20Orthogonal%20to%20advancements%20in%20generative%0AVLMs%2C%20another%20line%20of%20research%20has%20focused%20on%20representation%20learning%20for%0Avision-language%20alignment%2C%20targeting%20zero-shot%20inference%20for%20dense%20tasks%20like%0Asegmentation.%20In%20this%20work%2C%20we%20bridge%20these%20two%20directions%20by%20densely%20aligning%0Aimages%20with%20synthetic%20descriptions%20generated%20by%20VLMs.%20Synthetic%20captions%20are%0Ainexpensive%2C%20scalable%2C%20and%20easy%20to%20generate%2C%20making%20them%20an%20excellent%20source%20of%0Ahigh-level%20semantic%20understanding%20for%20dense%20alignment%20methods.%20Empirically%2C%20our%0Aapproach%20outperforms%20prior%20work%20on%20standard%20zero-shot%20open-vocabulary%0Asegmentation%20benchmarks/datasets%2C%20while%20also%20being%20more%20data-efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Captions%2520for%2520Open-Vocabulary%2520Zero-Shot%2520Segmentation%26entry.906535625%3DTim%2520Lebailly%2520and%2520Vijay%2520Veerabadran%2520and%2520Satwik%2520Kottur%2520and%2520Karl%2520Ridgeway%2520and%2520Michael%2520Louis%2520Iuzzolino%26entry.1292438233%3D%2520%2520Generative%2520vision-language%2520models%2520%2528VLMs%2529%2520exhibit%2520strong%2520high-level%2520image%250Aunderstanding%2520but%2520lack%2520spatially%2520dense%2520alignment%2520between%2520vision%2520and%2520language%250Amodalities%252C%2520as%2520our%2520findings%2520indicate.%2520Orthogonal%2520to%2520advancements%2520in%2520generative%250AVLMs%252C%2520another%2520line%2520of%2520research%2520has%2520focused%2520on%2520representation%2520learning%2520for%250Avision-language%2520alignment%252C%2520targeting%2520zero-shot%2520inference%2520for%2520dense%2520tasks%2520like%250Asegmentation.%2520In%2520this%2520work%252C%2520we%2520bridge%2520these%2520two%2520directions%2520by%2520densely%2520aligning%250Aimages%2520with%2520synthetic%2520descriptions%2520generated%2520by%2520VLMs.%2520Synthetic%2520captions%2520are%250Ainexpensive%252C%2520scalable%252C%2520and%2520easy%2520to%2520generate%252C%2520making%2520them%2520an%2520excellent%2520source%2520of%250Ahigh-level%2520semantic%2520understanding%2520for%2520dense%2520alignment%2520methods.%2520Empirically%252C%2520our%250Aapproach%2520outperforms%2520prior%2520work%2520on%2520standard%2520zero-shot%2520open-vocabulary%250Asegmentation%2520benchmarks/datasets%252C%2520while%2520also%2520being%2520more%2520data-efficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Captions%20for%20Open-Vocabulary%20Zero-Shot%20Segmentation&entry.906535625=Tim%20Lebailly%20and%20Vijay%20Veerabadran%20and%20Satwik%20Kottur%20and%20Karl%20Ridgeway%20and%20Michael%20Louis%20Iuzzolino&entry.1292438233=%20%20Generative%20vision-language%20models%20%28VLMs%29%20exhibit%20strong%20high-level%20image%0Aunderstanding%20but%20lack%20spatially%20dense%20alignment%20between%20vision%20and%20language%0Amodalities%2C%20as%20our%20findings%20indicate.%20Orthogonal%20to%20advancements%20in%20generative%0AVLMs%2C%20another%20line%20of%20research%20has%20focused%20on%20representation%20learning%20for%0Avision-language%20alignment%2C%20targeting%20zero-shot%20inference%20for%20dense%20tasks%20like%0Asegmentation.%20In%20this%20work%2C%20we%20bridge%20these%20two%20directions%20by%20densely%20aligning%0Aimages%20with%20synthetic%20descriptions%20generated%20by%20VLMs.%20Synthetic%20captions%20are%0Ainexpensive%2C%20scalable%2C%20and%20easy%20to%20generate%2C%20making%20them%20an%20excellent%20source%20of%0Ahigh-level%20semantic%20understanding%20for%20dense%20alignment%20methods.%20Empirically%2C%20our%0Aapproach%20outperforms%20prior%20work%20on%20standard%20zero-shot%20open-vocabulary%0Asegmentation%20benchmarks/datasets%2C%20while%20also%20being%20more%20data-efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11840v1&entry.124074799=Read"},
{"title": "Igniting VLMs toward the Embodied Space", "author": "Andy Zhai and Brae Liu and Bruno Fang and Chalse Cai and Ellie Ma and Ethan Yin and Hao Wang and Hugo Zhou and James Wang and Lights Shi and Lucy Liang and Make Wang and Qian Wang and Roy Gan and Ryan Yu and Shalfun Li and Starrick Liu and Sylas Chen and Vincent Chen and Zach Xu", "abstract": "  While foundation models show remarkable progress in language and vision,\nexisting vision-language models (VLMs) still have limited spatial and\nembodiment understanding. Transferring VLMs to embodied domains reveals\nfundamental mismatches between modalities, pretraining distributions, and\ntraining objectives, leaving action comprehension and generation as a central\nbottleneck on the path to AGI.\n  We introduce WALL-OSS, an end-to-end embodied foundation model that leverages\nlarge-scale multimodal pretraining to achieve (1) embodiment-aware\nvision-language understanding, (2) strong language-action association, and (3)\nrobust manipulation capability.\n  Our approach employs a tightly coupled architecture and multi-strategies\ntraining curriculum that enables Unified Cross-Level CoT-seamlessly unifying\ninstruction reasoning, subgoal decomposition, and fine-grained action synthesis\nwithin a single differentiable framework.\n  Our results show that WALL-OSS attains high success on complex long-horizon\nmanipulations, demonstrates strong instruction-following capabilities, complex\nunderstanding and reasoning, and outperforms strong baselines, thereby\nproviding a reliable and scalable path from VLMs to embodied foundation models.\n", "link": "http://arxiv.org/abs/2509.11766v1", "date": "2025-09-15", "relevancy": 2.9703, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6108}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Igniting%20VLMs%20toward%20the%20Embodied%20Space&body=Title%3A%20Igniting%20VLMs%20toward%20the%20Embodied%20Space%0AAuthor%3A%20Andy%20Zhai%20and%20Brae%20Liu%20and%20Bruno%20Fang%20and%20Chalse%20Cai%20and%20Ellie%20Ma%20and%20Ethan%20Yin%20and%20Hao%20Wang%20and%20Hugo%20Zhou%20and%20James%20Wang%20and%20Lights%20Shi%20and%20Lucy%20Liang%20and%20Make%20Wang%20and%20Qian%20Wang%20and%20Roy%20Gan%20and%20Ryan%20Yu%20and%20Shalfun%20Li%20and%20Starrick%20Liu%20and%20Sylas%20Chen%20and%20Vincent%20Chen%20and%20Zach%20Xu%0AAbstract%3A%20%20%20While%20foundation%20models%20show%20remarkable%20progress%20in%20language%20and%20vision%2C%0Aexisting%20vision-language%20models%20%28VLMs%29%20still%20have%20limited%20spatial%20and%0Aembodiment%20understanding.%20Transferring%20VLMs%20to%20embodied%20domains%20reveals%0Afundamental%20mismatches%20between%20modalities%2C%20pretraining%20distributions%2C%20and%0Atraining%20objectives%2C%20leaving%20action%20comprehension%20and%20generation%20as%20a%20central%0Abottleneck%20on%20the%20path%20to%20AGI.%0A%20%20We%20introduce%20WALL-OSS%2C%20an%20end-to-end%20embodied%20foundation%20model%20that%20leverages%0Alarge-scale%20multimodal%20pretraining%20to%20achieve%20%281%29%20embodiment-aware%0Avision-language%20understanding%2C%20%282%29%20strong%20language-action%20association%2C%20and%20%283%29%0Arobust%20manipulation%20capability.%0A%20%20Our%20approach%20employs%20a%20tightly%20coupled%20architecture%20and%20multi-strategies%0Atraining%20curriculum%20that%20enables%20Unified%20Cross-Level%20CoT-seamlessly%20unifying%0Ainstruction%20reasoning%2C%20subgoal%20decomposition%2C%20and%20fine-grained%20action%20synthesis%0Awithin%20a%20single%20differentiable%20framework.%0A%20%20Our%20results%20show%20that%20WALL-OSS%20attains%20high%20success%20on%20complex%20long-horizon%0Amanipulations%2C%20demonstrates%20strong%20instruction-following%20capabilities%2C%20complex%0Aunderstanding%20and%20reasoning%2C%20and%20outperforms%20strong%20baselines%2C%20thereby%0Aproviding%20a%20reliable%20and%20scalable%20path%20from%20VLMs%20to%20embodied%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIgniting%2520VLMs%2520toward%2520the%2520Embodied%2520Space%26entry.906535625%3DAndy%2520Zhai%2520and%2520Brae%2520Liu%2520and%2520Bruno%2520Fang%2520and%2520Chalse%2520Cai%2520and%2520Ellie%2520Ma%2520and%2520Ethan%2520Yin%2520and%2520Hao%2520Wang%2520and%2520Hugo%2520Zhou%2520and%2520James%2520Wang%2520and%2520Lights%2520Shi%2520and%2520Lucy%2520Liang%2520and%2520Make%2520Wang%2520and%2520Qian%2520Wang%2520and%2520Roy%2520Gan%2520and%2520Ryan%2520Yu%2520and%2520Shalfun%2520Li%2520and%2520Starrick%2520Liu%2520and%2520Sylas%2520Chen%2520and%2520Vincent%2520Chen%2520and%2520Zach%2520Xu%26entry.1292438233%3D%2520%2520While%2520foundation%2520models%2520show%2520remarkable%2520progress%2520in%2520language%2520and%2520vision%252C%250Aexisting%2520vision-language%2520models%2520%2528VLMs%2529%2520still%2520have%2520limited%2520spatial%2520and%250Aembodiment%2520understanding.%2520Transferring%2520VLMs%2520to%2520embodied%2520domains%2520reveals%250Afundamental%2520mismatches%2520between%2520modalities%252C%2520pretraining%2520distributions%252C%2520and%250Atraining%2520objectives%252C%2520leaving%2520action%2520comprehension%2520and%2520generation%2520as%2520a%2520central%250Abottleneck%2520on%2520the%2520path%2520to%2520AGI.%250A%2520%2520We%2520introduce%2520WALL-OSS%252C%2520an%2520end-to-end%2520embodied%2520foundation%2520model%2520that%2520leverages%250Alarge-scale%2520multimodal%2520pretraining%2520to%2520achieve%2520%25281%2529%2520embodiment-aware%250Avision-language%2520understanding%252C%2520%25282%2529%2520strong%2520language-action%2520association%252C%2520and%2520%25283%2529%250Arobust%2520manipulation%2520capability.%250A%2520%2520Our%2520approach%2520employs%2520a%2520tightly%2520coupled%2520architecture%2520and%2520multi-strategies%250Atraining%2520curriculum%2520that%2520enables%2520Unified%2520Cross-Level%2520CoT-seamlessly%2520unifying%250Ainstruction%2520reasoning%252C%2520subgoal%2520decomposition%252C%2520and%2520fine-grained%2520action%2520synthesis%250Awithin%2520a%2520single%2520differentiable%2520framework.%250A%2520%2520Our%2520results%2520show%2520that%2520WALL-OSS%2520attains%2520high%2520success%2520on%2520complex%2520long-horizon%250Amanipulations%252C%2520demonstrates%2520strong%2520instruction-following%2520capabilities%252C%2520complex%250Aunderstanding%2520and%2520reasoning%252C%2520and%2520outperforms%2520strong%2520baselines%252C%2520thereby%250Aproviding%2520a%2520reliable%2520and%2520scalable%2520path%2520from%2520VLMs%2520to%2520embodied%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Igniting%20VLMs%20toward%20the%20Embodied%20Space&entry.906535625=Andy%20Zhai%20and%20Brae%20Liu%20and%20Bruno%20Fang%20and%20Chalse%20Cai%20and%20Ellie%20Ma%20and%20Ethan%20Yin%20and%20Hao%20Wang%20and%20Hugo%20Zhou%20and%20James%20Wang%20and%20Lights%20Shi%20and%20Lucy%20Liang%20and%20Make%20Wang%20and%20Qian%20Wang%20and%20Roy%20Gan%20and%20Ryan%20Yu%20and%20Shalfun%20Li%20and%20Starrick%20Liu%20and%20Sylas%20Chen%20and%20Vincent%20Chen%20and%20Zach%20Xu&entry.1292438233=%20%20While%20foundation%20models%20show%20remarkable%20progress%20in%20language%20and%20vision%2C%0Aexisting%20vision-language%20models%20%28VLMs%29%20still%20have%20limited%20spatial%20and%0Aembodiment%20understanding.%20Transferring%20VLMs%20to%20embodied%20domains%20reveals%0Afundamental%20mismatches%20between%20modalities%2C%20pretraining%20distributions%2C%20and%0Atraining%20objectives%2C%20leaving%20action%20comprehension%20and%20generation%20as%20a%20central%0Abottleneck%20on%20the%20path%20to%20AGI.%0A%20%20We%20introduce%20WALL-OSS%2C%20an%20end-to-end%20embodied%20foundation%20model%20that%20leverages%0Alarge-scale%20multimodal%20pretraining%20to%20achieve%20%281%29%20embodiment-aware%0Avision-language%20understanding%2C%20%282%29%20strong%20language-action%20association%2C%20and%20%283%29%0Arobust%20manipulation%20capability.%0A%20%20Our%20approach%20employs%20a%20tightly%20coupled%20architecture%20and%20multi-strategies%0Atraining%20curriculum%20that%20enables%20Unified%20Cross-Level%20CoT-seamlessly%20unifying%0Ainstruction%20reasoning%2C%20subgoal%20decomposition%2C%20and%20fine-grained%20action%20synthesis%0Awithin%20a%20single%20differentiable%20framework.%0A%20%20Our%20results%20show%20that%20WALL-OSS%20attains%20high%20success%20on%20complex%20long-horizon%0Amanipulations%2C%20demonstrates%20strong%20instruction-following%20capabilities%2C%20complex%0Aunderstanding%20and%20reasoning%2C%20and%20outperforms%20strong%20baselines%2C%20thereby%0Aproviding%20a%20reliable%20and%20scalable%20path%20from%20VLMs%20to%20embodied%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11766v1&entry.124074799=Read"},
{"title": "OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling", "author": "Yang Zhou and Yifan Wang and Jianjun Zhou and Wenzheng Chang and Haoyu Guo and Zizun Li and Kaijing Ma and Xinyue Li and Yating Wang and Haoyi Zhu and Mingyu Liu and Dingning Liu and Jiange Yang and Zhoujie Fu and Junyi Chen and Chunhua Shen and Jiangmiao Pang and Kaipeng Zhang and Tong He", "abstract": "  The field of 4D world modeling - aiming to jointly capture spatial geometry\nand temporal dynamics - has witnessed remarkable progress in recent years,\ndriven by advances in large-scale generative models and multimodal learning.\nHowever, the development of truly general 4D world models remains fundamentally\nconstrained by the availability of high-quality data. Existing datasets and\nbenchmarks often lack the dynamic complexity, multi-domain diversity, and\nspatial-temporal annotations required to support key tasks such as 4D geometric\nreconstruction, future prediction, and camera-control video generation. To\naddress this gap, we introduce OmniWorld, a large-scale, multi-domain,\nmulti-modal dataset specifically designed for 4D world modeling. OmniWorld\nconsists of a newly collected OmniWorld-Game dataset and several curated public\ndatasets spanning diverse domains. Compared with existing synthetic datasets,\nOmniWorld-Game provides richer modality coverage, larger scale, and more\nrealistic dynamic interactions. Based on this dataset, we establish a\nchallenging benchmark that exposes the limitations of current state-of-the-art\n(SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning\nexisting SOTA methods on OmniWorld leads to significant performance gains\nacross 4D reconstruction and video generation tasks, strongly validating\nOmniWorld as a powerful resource for training and evaluation. We envision\nOmniWorld as a catalyst for accelerating the development of general-purpose 4D\nworld models, ultimately advancing machines' holistic understanding of the\nphysical world.\n", "link": "http://arxiv.org/abs/2509.12201v1", "date": "2025-09-15", "relevancy": 2.9218, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5864}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniWorld%3A%20A%20Multi-Domain%20and%20Multi-Modal%20Dataset%20for%204D%20World%20Modeling&body=Title%3A%20OmniWorld%3A%20A%20Multi-Domain%20and%20Multi-Modal%20Dataset%20for%204D%20World%20Modeling%0AAuthor%3A%20Yang%20Zhou%20and%20Yifan%20Wang%20and%20Jianjun%20Zhou%20and%20Wenzheng%20Chang%20and%20Haoyu%20Guo%20and%20Zizun%20Li%20and%20Kaijing%20Ma%20and%20Xinyue%20Li%20and%20Yating%20Wang%20and%20Haoyi%20Zhu%20and%20Mingyu%20Liu%20and%20Dingning%20Liu%20and%20Jiange%20Yang%20and%20Zhoujie%20Fu%20and%20Junyi%20Chen%20and%20Chunhua%20Shen%20and%20Jiangmiao%20Pang%20and%20Kaipeng%20Zhang%20and%20Tong%20He%0AAbstract%3A%20%20%20The%20field%20of%204D%20world%20modeling%20-%20aiming%20to%20jointly%20capture%20spatial%20geometry%0Aand%20temporal%20dynamics%20-%20has%20witnessed%20remarkable%20progress%20in%20recent%20years%2C%0Adriven%20by%20advances%20in%20large-scale%20generative%20models%20and%20multimodal%20learning.%0AHowever%2C%20the%20development%20of%20truly%20general%204D%20world%20models%20remains%20fundamentally%0Aconstrained%20by%20the%20availability%20of%20high-quality%20data.%20Existing%20datasets%20and%0Abenchmarks%20often%20lack%20the%20dynamic%20complexity%2C%20multi-domain%20diversity%2C%20and%0Aspatial-temporal%20annotations%20required%20to%20support%20key%20tasks%20such%20as%204D%20geometric%0Areconstruction%2C%20future%20prediction%2C%20and%20camera-control%20video%20generation.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20OmniWorld%2C%20a%20large-scale%2C%20multi-domain%2C%0Amulti-modal%20dataset%20specifically%20designed%20for%204D%20world%20modeling.%20OmniWorld%0Aconsists%20of%20a%20newly%20collected%20OmniWorld-Game%20dataset%20and%20several%20curated%20public%0Adatasets%20spanning%20diverse%20domains.%20Compared%20with%20existing%20synthetic%20datasets%2C%0AOmniWorld-Game%20provides%20richer%20modality%20coverage%2C%20larger%20scale%2C%20and%20more%0Arealistic%20dynamic%20interactions.%20Based%20on%20this%20dataset%2C%20we%20establish%20a%0Achallenging%20benchmark%20that%20exposes%20the%20limitations%20of%20current%20state-of-the-art%0A%28SOTA%29%20approaches%20in%20modeling%20complex%204D%20environments.%20Moreover%2C%20fine-tuning%0Aexisting%20SOTA%20methods%20on%20OmniWorld%20leads%20to%20significant%20performance%20gains%0Aacross%204D%20reconstruction%20and%20video%20generation%20tasks%2C%20strongly%20validating%0AOmniWorld%20as%20a%20powerful%20resource%20for%20training%20and%20evaluation.%20We%20envision%0AOmniWorld%20as%20a%20catalyst%20for%20accelerating%20the%20development%20of%20general-purpose%204D%0Aworld%20models%2C%20ultimately%20advancing%20machines%27%20holistic%20understanding%20of%20the%0Aphysical%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12201v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniWorld%253A%2520A%2520Multi-Domain%2520and%2520Multi-Modal%2520Dataset%2520for%25204D%2520World%2520Modeling%26entry.906535625%3DYang%2520Zhou%2520and%2520Yifan%2520Wang%2520and%2520Jianjun%2520Zhou%2520and%2520Wenzheng%2520Chang%2520and%2520Haoyu%2520Guo%2520and%2520Zizun%2520Li%2520and%2520Kaijing%2520Ma%2520and%2520Xinyue%2520Li%2520and%2520Yating%2520Wang%2520and%2520Haoyi%2520Zhu%2520and%2520Mingyu%2520Liu%2520and%2520Dingning%2520Liu%2520and%2520Jiange%2520Yang%2520and%2520Zhoujie%2520Fu%2520and%2520Junyi%2520Chen%2520and%2520Chunhua%2520Shen%2520and%2520Jiangmiao%2520Pang%2520and%2520Kaipeng%2520Zhang%2520and%2520Tong%2520He%26entry.1292438233%3D%2520%2520The%2520field%2520of%25204D%2520world%2520modeling%2520-%2520aiming%2520to%2520jointly%2520capture%2520spatial%2520geometry%250Aand%2520temporal%2520dynamics%2520-%2520has%2520witnessed%2520remarkable%2520progress%2520in%2520recent%2520years%252C%250Adriven%2520by%2520advances%2520in%2520large-scale%2520generative%2520models%2520and%2520multimodal%2520learning.%250AHowever%252C%2520the%2520development%2520of%2520truly%2520general%25204D%2520world%2520models%2520remains%2520fundamentally%250Aconstrained%2520by%2520the%2520availability%2520of%2520high-quality%2520data.%2520Existing%2520datasets%2520and%250Abenchmarks%2520often%2520lack%2520the%2520dynamic%2520complexity%252C%2520multi-domain%2520diversity%252C%2520and%250Aspatial-temporal%2520annotations%2520required%2520to%2520support%2520key%2520tasks%2520such%2520as%25204D%2520geometric%250Areconstruction%252C%2520future%2520prediction%252C%2520and%2520camera-control%2520video%2520generation.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520OmniWorld%252C%2520a%2520large-scale%252C%2520multi-domain%252C%250Amulti-modal%2520dataset%2520specifically%2520designed%2520for%25204D%2520world%2520modeling.%2520OmniWorld%250Aconsists%2520of%2520a%2520newly%2520collected%2520OmniWorld-Game%2520dataset%2520and%2520several%2520curated%2520public%250Adatasets%2520spanning%2520diverse%2520domains.%2520Compared%2520with%2520existing%2520synthetic%2520datasets%252C%250AOmniWorld-Game%2520provides%2520richer%2520modality%2520coverage%252C%2520larger%2520scale%252C%2520and%2520more%250Arealistic%2520dynamic%2520interactions.%2520Based%2520on%2520this%2520dataset%252C%2520we%2520establish%2520a%250Achallenging%2520benchmark%2520that%2520exposes%2520the%2520limitations%2520of%2520current%2520state-of-the-art%250A%2528SOTA%2529%2520approaches%2520in%2520modeling%2520complex%25204D%2520environments.%2520Moreover%252C%2520fine-tuning%250Aexisting%2520SOTA%2520methods%2520on%2520OmniWorld%2520leads%2520to%2520significant%2520performance%2520gains%250Aacross%25204D%2520reconstruction%2520and%2520video%2520generation%2520tasks%252C%2520strongly%2520validating%250AOmniWorld%2520as%2520a%2520powerful%2520resource%2520for%2520training%2520and%2520evaluation.%2520We%2520envision%250AOmniWorld%2520as%2520a%2520catalyst%2520for%2520accelerating%2520the%2520development%2520of%2520general-purpose%25204D%250Aworld%2520models%252C%2520ultimately%2520advancing%2520machines%2527%2520holistic%2520understanding%2520of%2520the%250Aphysical%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12201v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniWorld%3A%20A%20Multi-Domain%20and%20Multi-Modal%20Dataset%20for%204D%20World%20Modeling&entry.906535625=Yang%20Zhou%20and%20Yifan%20Wang%20and%20Jianjun%20Zhou%20and%20Wenzheng%20Chang%20and%20Haoyu%20Guo%20and%20Zizun%20Li%20and%20Kaijing%20Ma%20and%20Xinyue%20Li%20and%20Yating%20Wang%20and%20Haoyi%20Zhu%20and%20Mingyu%20Liu%20and%20Dingning%20Liu%20and%20Jiange%20Yang%20and%20Zhoujie%20Fu%20and%20Junyi%20Chen%20and%20Chunhua%20Shen%20and%20Jiangmiao%20Pang%20and%20Kaipeng%20Zhang%20and%20Tong%20He&entry.1292438233=%20%20The%20field%20of%204D%20world%20modeling%20-%20aiming%20to%20jointly%20capture%20spatial%20geometry%0Aand%20temporal%20dynamics%20-%20has%20witnessed%20remarkable%20progress%20in%20recent%20years%2C%0Adriven%20by%20advances%20in%20large-scale%20generative%20models%20and%20multimodal%20learning.%0AHowever%2C%20the%20development%20of%20truly%20general%204D%20world%20models%20remains%20fundamentally%0Aconstrained%20by%20the%20availability%20of%20high-quality%20data.%20Existing%20datasets%20and%0Abenchmarks%20often%20lack%20the%20dynamic%20complexity%2C%20multi-domain%20diversity%2C%20and%0Aspatial-temporal%20annotations%20required%20to%20support%20key%20tasks%20such%20as%204D%20geometric%0Areconstruction%2C%20future%20prediction%2C%20and%20camera-control%20video%20generation.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20OmniWorld%2C%20a%20large-scale%2C%20multi-domain%2C%0Amulti-modal%20dataset%20specifically%20designed%20for%204D%20world%20modeling.%20OmniWorld%0Aconsists%20of%20a%20newly%20collected%20OmniWorld-Game%20dataset%20and%20several%20curated%20public%0Adatasets%20spanning%20diverse%20domains.%20Compared%20with%20existing%20synthetic%20datasets%2C%0AOmniWorld-Game%20provides%20richer%20modality%20coverage%2C%20larger%20scale%2C%20and%20more%0Arealistic%20dynamic%20interactions.%20Based%20on%20this%20dataset%2C%20we%20establish%20a%0Achallenging%20benchmark%20that%20exposes%20the%20limitations%20of%20current%20state-of-the-art%0A%28SOTA%29%20approaches%20in%20modeling%20complex%204D%20environments.%20Moreover%2C%20fine-tuning%0Aexisting%20SOTA%20methods%20on%20OmniWorld%20leads%20to%20significant%20performance%20gains%0Aacross%204D%20reconstruction%20and%20video%20generation%20tasks%2C%20strongly%20validating%0AOmniWorld%20as%20a%20powerful%20resource%20for%20training%20and%20evaluation.%20We%20envision%0AOmniWorld%20as%20a%20catalyst%20for%20accelerating%20the%20development%20of%20general-purpose%204D%0Aworld%20models%2C%20ultimately%20advancing%20machines%27%20holistic%20understanding%20of%20the%0Aphysical%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12201v1&entry.124074799=Read"},
{"title": "MSMA: Multi-Scale Feature Fusion For Multi-Attribute 3D Face\n  Reconstruction From Unconstrained Images", "author": "Danling Cao", "abstract": "  Reconstructing 3D face from a single unconstrained image remains a\nchallenging problem due to diverse conditions in unconstrained environments.\nRecently, learning-based methods have achieved notable results by effectively\ncapturing complex facial structures and details across varying conditions.\nConsequently, many existing approaches employ projection-based losses between\ngenerated and input images to constrain model training. However, learning-based\nmethods for 3D face reconstruction typically require substantial amounts of 3D\nfacial data, which is difficult and costly to obtain. Consequently, to reduce\nreliance on labeled 3D face datasets, many existing approaches employ\nprojection-based losses between generated and input images to constrain model\ntraining. Nonetheless, despite these advancements, existing approaches\nfrequently struggle to capture detailed and multi-scale features under diverse\nfacial attributes and conditions, leading to incomplete or less accurate\nreconstructions. In this paper, we propose a Multi-Scale Feature Fusion with\nMulti-Attribute (MSMA) framework for 3D face reconstruction from unconstrained\nimages. Our method integrates multi-scale feature fusion with a focus on\nmulti-attribute learning and leverages a large-kernel attention module to\nenhance the precision of feature extraction across scales, enabling accurate 3D\nfacial parameter estimation from a single 2D image. Comprehensive experiments\non the MICC Florence, Facewarehouse and custom-collect datasets demonstrate\nthat our approach achieves results on par with current state-of-the-art\nmethods, and in some instances, surpasses SOTA performance across challenging\nconditions.\n", "link": "http://arxiv.org/abs/2509.11763v1", "date": "2025-09-15", "relevancy": 2.9213, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5965}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5781}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSMA%3A%20Multi-Scale%20Feature%20Fusion%20For%20Multi-Attribute%203D%20Face%0A%20%20Reconstruction%20From%20Unconstrained%20Images&body=Title%3A%20MSMA%3A%20Multi-Scale%20Feature%20Fusion%20For%20Multi-Attribute%203D%20Face%0A%20%20Reconstruction%20From%20Unconstrained%20Images%0AAuthor%3A%20Danling%20Cao%0AAbstract%3A%20%20%20Reconstructing%203D%20face%20from%20a%20single%20unconstrained%20image%20remains%20a%0Achallenging%20problem%20due%20to%20diverse%20conditions%20in%20unconstrained%20environments.%0ARecently%2C%20learning-based%20methods%20have%20achieved%20notable%20results%20by%20effectively%0Acapturing%20complex%20facial%20structures%20and%20details%20across%20varying%20conditions.%0AConsequently%2C%20many%20existing%20approaches%20employ%20projection-based%20losses%20between%0Agenerated%20and%20input%20images%20to%20constrain%20model%20training.%20However%2C%20learning-based%0Amethods%20for%203D%20face%20reconstruction%20typically%20require%20substantial%20amounts%20of%203D%0Afacial%20data%2C%20which%20is%20difficult%20and%20costly%20to%20obtain.%20Consequently%2C%20to%20reduce%0Areliance%20on%20labeled%203D%20face%20datasets%2C%20many%20existing%20approaches%20employ%0Aprojection-based%20losses%20between%20generated%20and%20input%20images%20to%20constrain%20model%0Atraining.%20Nonetheless%2C%20despite%20these%20advancements%2C%20existing%20approaches%0Afrequently%20struggle%20to%20capture%20detailed%20and%20multi-scale%20features%20under%20diverse%0Afacial%20attributes%20and%20conditions%2C%20leading%20to%20incomplete%20or%20less%20accurate%0Areconstructions.%20In%20this%20paper%2C%20we%20propose%20a%20Multi-Scale%20Feature%20Fusion%20with%0AMulti-Attribute%20%28MSMA%29%20framework%20for%203D%20face%20reconstruction%20from%20unconstrained%0Aimages.%20Our%20method%20integrates%20multi-scale%20feature%20fusion%20with%20a%20focus%20on%0Amulti-attribute%20learning%20and%20leverages%20a%20large-kernel%20attention%20module%20to%0Aenhance%20the%20precision%20of%20feature%20extraction%20across%20scales%2C%20enabling%20accurate%203D%0Afacial%20parameter%20estimation%20from%20a%20single%202D%20image.%20Comprehensive%20experiments%0Aon%20the%20MICC%20Florence%2C%20Facewarehouse%20and%20custom-collect%20datasets%20demonstrate%0Athat%20our%20approach%20achieves%20results%20on%20par%20with%20current%20state-of-the-art%0Amethods%2C%20and%20in%20some%20instances%2C%20surpasses%20SOTA%20performance%20across%20challenging%0Aconditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSMA%253A%2520Multi-Scale%2520Feature%2520Fusion%2520For%2520Multi-Attribute%25203D%2520Face%250A%2520%2520Reconstruction%2520From%2520Unconstrained%2520Images%26entry.906535625%3DDanling%2520Cao%26entry.1292438233%3D%2520%2520Reconstructing%25203D%2520face%2520from%2520a%2520single%2520unconstrained%2520image%2520remains%2520a%250Achallenging%2520problem%2520due%2520to%2520diverse%2520conditions%2520in%2520unconstrained%2520environments.%250ARecently%252C%2520learning-based%2520methods%2520have%2520achieved%2520notable%2520results%2520by%2520effectively%250Acapturing%2520complex%2520facial%2520structures%2520and%2520details%2520across%2520varying%2520conditions.%250AConsequently%252C%2520many%2520existing%2520approaches%2520employ%2520projection-based%2520losses%2520between%250Agenerated%2520and%2520input%2520images%2520to%2520constrain%2520model%2520training.%2520However%252C%2520learning-based%250Amethods%2520for%25203D%2520face%2520reconstruction%2520typically%2520require%2520substantial%2520amounts%2520of%25203D%250Afacial%2520data%252C%2520which%2520is%2520difficult%2520and%2520costly%2520to%2520obtain.%2520Consequently%252C%2520to%2520reduce%250Areliance%2520on%2520labeled%25203D%2520face%2520datasets%252C%2520many%2520existing%2520approaches%2520employ%250Aprojection-based%2520losses%2520between%2520generated%2520and%2520input%2520images%2520to%2520constrain%2520model%250Atraining.%2520Nonetheless%252C%2520despite%2520these%2520advancements%252C%2520existing%2520approaches%250Afrequently%2520struggle%2520to%2520capture%2520detailed%2520and%2520multi-scale%2520features%2520under%2520diverse%250Afacial%2520attributes%2520and%2520conditions%252C%2520leading%2520to%2520incomplete%2520or%2520less%2520accurate%250Areconstructions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Multi-Scale%2520Feature%2520Fusion%2520with%250AMulti-Attribute%2520%2528MSMA%2529%2520framework%2520for%25203D%2520face%2520reconstruction%2520from%2520unconstrained%250Aimages.%2520Our%2520method%2520integrates%2520multi-scale%2520feature%2520fusion%2520with%2520a%2520focus%2520on%250Amulti-attribute%2520learning%2520and%2520leverages%2520a%2520large-kernel%2520attention%2520module%2520to%250Aenhance%2520the%2520precision%2520of%2520feature%2520extraction%2520across%2520scales%252C%2520enabling%2520accurate%25203D%250Afacial%2520parameter%2520estimation%2520from%2520a%2520single%25202D%2520image.%2520Comprehensive%2520experiments%250Aon%2520the%2520MICC%2520Florence%252C%2520Facewarehouse%2520and%2520custom-collect%2520datasets%2520demonstrate%250Athat%2520our%2520approach%2520achieves%2520results%2520on%2520par%2520with%2520current%2520state-of-the-art%250Amethods%252C%2520and%2520in%2520some%2520instances%252C%2520surpasses%2520SOTA%2520performance%2520across%2520challenging%250Aconditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSMA%3A%20Multi-Scale%20Feature%20Fusion%20For%20Multi-Attribute%203D%20Face%0A%20%20Reconstruction%20From%20Unconstrained%20Images&entry.906535625=Danling%20Cao&entry.1292438233=%20%20Reconstructing%203D%20face%20from%20a%20single%20unconstrained%20image%20remains%20a%0Achallenging%20problem%20due%20to%20diverse%20conditions%20in%20unconstrained%20environments.%0ARecently%2C%20learning-based%20methods%20have%20achieved%20notable%20results%20by%20effectively%0Acapturing%20complex%20facial%20structures%20and%20details%20across%20varying%20conditions.%0AConsequently%2C%20many%20existing%20approaches%20employ%20projection-based%20losses%20between%0Agenerated%20and%20input%20images%20to%20constrain%20model%20training.%20However%2C%20learning-based%0Amethods%20for%203D%20face%20reconstruction%20typically%20require%20substantial%20amounts%20of%203D%0Afacial%20data%2C%20which%20is%20difficult%20and%20costly%20to%20obtain.%20Consequently%2C%20to%20reduce%0Areliance%20on%20labeled%203D%20face%20datasets%2C%20many%20existing%20approaches%20employ%0Aprojection-based%20losses%20between%20generated%20and%20input%20images%20to%20constrain%20model%0Atraining.%20Nonetheless%2C%20despite%20these%20advancements%2C%20existing%20approaches%0Afrequently%20struggle%20to%20capture%20detailed%20and%20multi-scale%20features%20under%20diverse%0Afacial%20attributes%20and%20conditions%2C%20leading%20to%20incomplete%20or%20less%20accurate%0Areconstructions.%20In%20this%20paper%2C%20we%20propose%20a%20Multi-Scale%20Feature%20Fusion%20with%0AMulti-Attribute%20%28MSMA%29%20framework%20for%203D%20face%20reconstruction%20from%20unconstrained%0Aimages.%20Our%20method%20integrates%20multi-scale%20feature%20fusion%20with%20a%20focus%20on%0Amulti-attribute%20learning%20and%20leverages%20a%20large-kernel%20attention%20module%20to%0Aenhance%20the%20precision%20of%20feature%20extraction%20across%20scales%2C%20enabling%20accurate%203D%0Afacial%20parameter%20estimation%20from%20a%20single%202D%20image.%20Comprehensive%20experiments%0Aon%20the%20MICC%20Florence%2C%20Facewarehouse%20and%20custom-collect%20datasets%20demonstrate%0Athat%20our%20approach%20achieves%20results%20on%20par%20with%20current%20state-of-the-art%0Amethods%2C%20and%20in%20some%20instances%2C%20surpasses%20SOTA%20performance%20across%20challenging%0Aconditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11763v1&entry.124074799=Read"},
{"title": "Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose\n  Video Hallucination by Fine-grained Spatial-Temporal Grounding", "author": "Meng Luo and Shengqiong Wu and Liqiang Jing and Tianjie Ju and Li Zheng and Jinxiang Lai and Tianlong Wu and Xinya Du and Jian Li and Siyuan Yan and Jiebo Luo and William Yang Wang and Hao Fei and Mong-Li Lee and Wynne Hsu", "abstract": "  Recent advancements in large video models (LVMs) have significantly enhance\nvideo understanding. However, these models continue to suffer from\nhallucinations, producing content that conflicts with input videos. To address\nthis issue, we propose Dr.V, a hierarchical framework covering perceptive,\ntemporal, and cognitive levels to diagnose video hallucination by fine-grained\nspatial-temporal grounding. Dr.V comprises of two key components: a benchmark\ndataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes\n10k instances drawn from 4,974 videos spanning diverse tasks, each enriched\nwith detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in\nLVMs by systematically applying fine-grained spatial-temporal grounding at the\nperceptive and temporal levels, followed by cognitive level reasoning. This\nstep-by-step pipeline mirrors human-like video comprehension and effectively\nidentifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is\neffective in diagnosing hallucination while enhancing interpretability and\nreliability, offering a practical blueprint for robust video understanding in\nreal-world scenarios. All our data and code are available at\nhttps://github.com/Eurekaleo/Dr.V.\n", "link": "http://arxiv.org/abs/2509.11866v1", "date": "2025-09-15", "relevancy": 2.9142, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5906}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5906}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dr.V%3A%20A%20Hierarchical%20Perception-Temporal-Cognition%20Framework%20to%20Diagnose%0A%20%20Video%20Hallucination%20by%20Fine-grained%20Spatial-Temporal%20Grounding&body=Title%3A%20Dr.V%3A%20A%20Hierarchical%20Perception-Temporal-Cognition%20Framework%20to%20Diagnose%0A%20%20Video%20Hallucination%20by%20Fine-grained%20Spatial-Temporal%20Grounding%0AAuthor%3A%20Meng%20Luo%20and%20Shengqiong%20Wu%20and%20Liqiang%20Jing%20and%20Tianjie%20Ju%20and%20Li%20Zheng%20and%20Jinxiang%20Lai%20and%20Tianlong%20Wu%20and%20Xinya%20Du%20and%20Jian%20Li%20and%20Siyuan%20Yan%20and%20Jiebo%20Luo%20and%20William%20Yang%20Wang%20and%20Hao%20Fei%20and%20Mong-Li%20Lee%20and%20Wynne%20Hsu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20video%20models%20%28LVMs%29%20have%20significantly%20enhance%0Avideo%20understanding.%20However%2C%20these%20models%20continue%20to%20suffer%20from%0Ahallucinations%2C%20producing%20content%20that%20conflicts%20with%20input%20videos.%20To%20address%0Athis%20issue%2C%20we%20propose%20Dr.V%2C%20a%20hierarchical%20framework%20covering%20perceptive%2C%0Atemporal%2C%20and%20cognitive%20levels%20to%20diagnose%20video%20hallucination%20by%20fine-grained%0Aspatial-temporal%20grounding.%20Dr.V%20comprises%20of%20two%20key%20components%3A%20a%20benchmark%0Adataset%20Dr.V-Bench%20and%20a%20satellite%20video%20agent%20Dr.V-Agent.%20Dr.V-Bench%20includes%0A10k%20instances%20drawn%20from%204%2C974%20videos%20spanning%20diverse%20tasks%2C%20each%20enriched%0Awith%20detailed%20spatial-temporal%20annotation.%20Dr.V-Agent%20detects%20hallucinations%20in%0ALVMs%20by%20systematically%20applying%20fine-grained%20spatial-temporal%20grounding%20at%20the%0Aperceptive%20and%20temporal%20levels%2C%20followed%20by%20cognitive%20level%20reasoning.%20This%0Astep-by-step%20pipeline%20mirrors%20human-like%20video%20comprehension%20and%20effectively%0Aidentifies%20hallucinations.%20Extensive%20experiments%20demonstrate%20that%20Dr.V-Agent%20is%0Aeffective%20in%20diagnosing%20hallucination%20while%20enhancing%20interpretability%20and%0Areliability%2C%20offering%20a%20practical%20blueprint%20for%20robust%20video%20understanding%20in%0Areal-world%20scenarios.%20All%20our%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/Eurekaleo/Dr.V.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11866v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDr.V%253A%2520A%2520Hierarchical%2520Perception-Temporal-Cognition%2520Framework%2520to%2520Diagnose%250A%2520%2520Video%2520Hallucination%2520by%2520Fine-grained%2520Spatial-Temporal%2520Grounding%26entry.906535625%3DMeng%2520Luo%2520and%2520Shengqiong%2520Wu%2520and%2520Liqiang%2520Jing%2520and%2520Tianjie%2520Ju%2520and%2520Li%2520Zheng%2520and%2520Jinxiang%2520Lai%2520and%2520Tianlong%2520Wu%2520and%2520Xinya%2520Du%2520and%2520Jian%2520Li%2520and%2520Siyuan%2520Yan%2520and%2520Jiebo%2520Luo%2520and%2520William%2520Yang%2520Wang%2520and%2520Hao%2520Fei%2520and%2520Mong-Li%2520Lee%2520and%2520Wynne%2520Hsu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520video%2520models%2520%2528LVMs%2529%2520have%2520significantly%2520enhance%250Avideo%2520understanding.%2520However%252C%2520these%2520models%2520continue%2520to%2520suffer%2520from%250Ahallucinations%252C%2520producing%2520content%2520that%2520conflicts%2520with%2520input%2520videos.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520Dr.V%252C%2520a%2520hierarchical%2520framework%2520covering%2520perceptive%252C%250Atemporal%252C%2520and%2520cognitive%2520levels%2520to%2520diagnose%2520video%2520hallucination%2520by%2520fine-grained%250Aspatial-temporal%2520grounding.%2520Dr.V%2520comprises%2520of%2520two%2520key%2520components%253A%2520a%2520benchmark%250Adataset%2520Dr.V-Bench%2520and%2520a%2520satellite%2520video%2520agent%2520Dr.V-Agent.%2520Dr.V-Bench%2520includes%250A10k%2520instances%2520drawn%2520from%25204%252C974%2520videos%2520spanning%2520diverse%2520tasks%252C%2520each%2520enriched%250Awith%2520detailed%2520spatial-temporal%2520annotation.%2520Dr.V-Agent%2520detects%2520hallucinations%2520in%250ALVMs%2520by%2520systematically%2520applying%2520fine-grained%2520spatial-temporal%2520grounding%2520at%2520the%250Aperceptive%2520and%2520temporal%2520levels%252C%2520followed%2520by%2520cognitive%2520level%2520reasoning.%2520This%250Astep-by-step%2520pipeline%2520mirrors%2520human-like%2520video%2520comprehension%2520and%2520effectively%250Aidentifies%2520hallucinations.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Dr.V-Agent%2520is%250Aeffective%2520in%2520diagnosing%2520hallucination%2520while%2520enhancing%2520interpretability%2520and%250Areliability%252C%2520offering%2520a%2520practical%2520blueprint%2520for%2520robust%2520video%2520understanding%2520in%250Areal-world%2520scenarios.%2520All%2520our%2520data%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/Eurekaleo/Dr.V.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11866v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dr.V%3A%20A%20Hierarchical%20Perception-Temporal-Cognition%20Framework%20to%20Diagnose%0A%20%20Video%20Hallucination%20by%20Fine-grained%20Spatial-Temporal%20Grounding&entry.906535625=Meng%20Luo%20and%20Shengqiong%20Wu%20and%20Liqiang%20Jing%20and%20Tianjie%20Ju%20and%20Li%20Zheng%20and%20Jinxiang%20Lai%20and%20Tianlong%20Wu%20and%20Xinya%20Du%20and%20Jian%20Li%20and%20Siyuan%20Yan%20and%20Jiebo%20Luo%20and%20William%20Yang%20Wang%20and%20Hao%20Fei%20and%20Mong-Li%20Lee%20and%20Wynne%20Hsu&entry.1292438233=%20%20Recent%20advancements%20in%20large%20video%20models%20%28LVMs%29%20have%20significantly%20enhance%0Avideo%20understanding.%20However%2C%20these%20models%20continue%20to%20suffer%20from%0Ahallucinations%2C%20producing%20content%20that%20conflicts%20with%20input%20videos.%20To%20address%0Athis%20issue%2C%20we%20propose%20Dr.V%2C%20a%20hierarchical%20framework%20covering%20perceptive%2C%0Atemporal%2C%20and%20cognitive%20levels%20to%20diagnose%20video%20hallucination%20by%20fine-grained%0Aspatial-temporal%20grounding.%20Dr.V%20comprises%20of%20two%20key%20components%3A%20a%20benchmark%0Adataset%20Dr.V-Bench%20and%20a%20satellite%20video%20agent%20Dr.V-Agent.%20Dr.V-Bench%20includes%0A10k%20instances%20drawn%20from%204%2C974%20videos%20spanning%20diverse%20tasks%2C%20each%20enriched%0Awith%20detailed%20spatial-temporal%20annotation.%20Dr.V-Agent%20detects%20hallucinations%20in%0ALVMs%20by%20systematically%20applying%20fine-grained%20spatial-temporal%20grounding%20at%20the%0Aperceptive%20and%20temporal%20levels%2C%20followed%20by%20cognitive%20level%20reasoning.%20This%0Astep-by-step%20pipeline%20mirrors%20human-like%20video%20comprehension%20and%20effectively%0Aidentifies%20hallucinations.%20Extensive%20experiments%20demonstrate%20that%20Dr.V-Agent%20is%0Aeffective%20in%20diagnosing%20hallucination%20while%20enhancing%20interpretability%20and%0Areliability%2C%20offering%20a%20practical%20blueprint%20for%20robust%20video%20understanding%20in%0Areal-world%20scenarios.%20All%20our%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/Eurekaleo/Dr.V.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11866v1&entry.124074799=Read"},
{"title": "Open-ended Hierarchical Streaming Video Understanding with Vision\n  Language Models", "author": "Hyolim Kang and Yunsu Park and Youngbeom Yoo and Yeeun Choi and Seon Joo Kim", "abstract": "  We introduce Hierarchical Streaming Video Understanding, a task that combines\nonline temporal action localization with free-form description generation.\nGiven the scarcity of datasets with hierarchical and fine-grained temporal\nannotations, we demonstrate that LLMs can effectively group atomic actions into\nhigher-level events, enriching existing datasets. We then propose OpenHOUSE\n(Open-ended Hierarchical Online Understanding System for Events), which extends\nstreaming action perception beyond action classification. OpenHOUSE features a\nspecialized streaming module that accurately detects boundaries between closely\nadjacent actions, nearly doubling the performance of direct extensions of\nexisting methods. We envision the future of streaming action perception in the\nintegration of powerful generative models, with OpenHOUSE representing a key\nstep in that direction.\n", "link": "http://arxiv.org/abs/2509.12145v1", "date": "2025-09-15", "relevancy": 2.8823, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5847}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5847}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.56}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-ended%20Hierarchical%20Streaming%20Video%20Understanding%20with%20Vision%0A%20%20Language%20Models&body=Title%3A%20Open-ended%20Hierarchical%20Streaming%20Video%20Understanding%20with%20Vision%0A%20%20Language%20Models%0AAuthor%3A%20Hyolim%20Kang%20and%20Yunsu%20Park%20and%20Youngbeom%20Yoo%20and%20Yeeun%20Choi%20and%20Seon%20Joo%20Kim%0AAbstract%3A%20%20%20We%20introduce%20Hierarchical%20Streaming%20Video%20Understanding%2C%20a%20task%20that%20combines%0Aonline%20temporal%20action%20localization%20with%20free-form%20description%20generation.%0AGiven%20the%20scarcity%20of%20datasets%20with%20hierarchical%20and%20fine-grained%20temporal%0Aannotations%2C%20we%20demonstrate%20that%20LLMs%20can%20effectively%20group%20atomic%20actions%20into%0Ahigher-level%20events%2C%20enriching%20existing%20datasets.%20We%20then%20propose%20OpenHOUSE%0A%28Open-ended%20Hierarchical%20Online%20Understanding%20System%20for%20Events%29%2C%20which%20extends%0Astreaming%20action%20perception%20beyond%20action%20classification.%20OpenHOUSE%20features%20a%0Aspecialized%20streaming%20module%20that%20accurately%20detects%20boundaries%20between%20closely%0Aadjacent%20actions%2C%20nearly%20doubling%20the%20performance%20of%20direct%20extensions%20of%0Aexisting%20methods.%20We%20envision%20the%20future%20of%20streaming%20action%20perception%20in%20the%0Aintegration%20of%20powerful%20generative%20models%2C%20with%20OpenHOUSE%20representing%20a%20key%0Astep%20in%20that%20direction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-ended%2520Hierarchical%2520Streaming%2520Video%2520Understanding%2520with%2520Vision%250A%2520%2520Language%2520Models%26entry.906535625%3DHyolim%2520Kang%2520and%2520Yunsu%2520Park%2520and%2520Youngbeom%2520Yoo%2520and%2520Yeeun%2520Choi%2520and%2520Seon%2520Joo%2520Kim%26entry.1292438233%3D%2520%2520We%2520introduce%2520Hierarchical%2520Streaming%2520Video%2520Understanding%252C%2520a%2520task%2520that%2520combines%250Aonline%2520temporal%2520action%2520localization%2520with%2520free-form%2520description%2520generation.%250AGiven%2520the%2520scarcity%2520of%2520datasets%2520with%2520hierarchical%2520and%2520fine-grained%2520temporal%250Aannotations%252C%2520we%2520demonstrate%2520that%2520LLMs%2520can%2520effectively%2520group%2520atomic%2520actions%2520into%250Ahigher-level%2520events%252C%2520enriching%2520existing%2520datasets.%2520We%2520then%2520propose%2520OpenHOUSE%250A%2528Open-ended%2520Hierarchical%2520Online%2520Understanding%2520System%2520for%2520Events%2529%252C%2520which%2520extends%250Astreaming%2520action%2520perception%2520beyond%2520action%2520classification.%2520OpenHOUSE%2520features%2520a%250Aspecialized%2520streaming%2520module%2520that%2520accurately%2520detects%2520boundaries%2520between%2520closely%250Aadjacent%2520actions%252C%2520nearly%2520doubling%2520the%2520performance%2520of%2520direct%2520extensions%2520of%250Aexisting%2520methods.%2520We%2520envision%2520the%2520future%2520of%2520streaming%2520action%2520perception%2520in%2520the%250Aintegration%2520of%2520powerful%2520generative%2520models%252C%2520with%2520OpenHOUSE%2520representing%2520a%2520key%250Astep%2520in%2520that%2520direction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-ended%20Hierarchical%20Streaming%20Video%20Understanding%20with%20Vision%0A%20%20Language%20Models&entry.906535625=Hyolim%20Kang%20and%20Yunsu%20Park%20and%20Youngbeom%20Yoo%20and%20Yeeun%20Choi%20and%20Seon%20Joo%20Kim&entry.1292438233=%20%20We%20introduce%20Hierarchical%20Streaming%20Video%20Understanding%2C%20a%20task%20that%20combines%0Aonline%20temporal%20action%20localization%20with%20free-form%20description%20generation.%0AGiven%20the%20scarcity%20of%20datasets%20with%20hierarchical%20and%20fine-grained%20temporal%0Aannotations%2C%20we%20demonstrate%20that%20LLMs%20can%20effectively%20group%20atomic%20actions%20into%0Ahigher-level%20events%2C%20enriching%20existing%20datasets.%20We%20then%20propose%20OpenHOUSE%0A%28Open-ended%20Hierarchical%20Online%20Understanding%20System%20for%20Events%29%2C%20which%20extends%0Astreaming%20action%20perception%20beyond%20action%20classification.%20OpenHOUSE%20features%20a%0Aspecialized%20streaming%20module%20that%20accurately%20detects%20boundaries%20between%20closely%0Aadjacent%20actions%2C%20nearly%20doubling%20the%20performance%20of%20direct%20extensions%20of%0Aexisting%20methods.%20We%20envision%20the%20future%20of%20streaming%20action%20perception%20in%20the%0Aintegration%20of%20powerful%20generative%20models%2C%20with%20OpenHOUSE%20representing%20a%20key%0Astep%20in%20that%20direction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12145v1&entry.124074799=Read"},
{"title": "Embodied Navigation Foundation Model", "author": "Jiazhao Zhang and Anqi Li and Yunpeng Qi and Minghan Li and Jiahang Liu and Shaoan Wang and Haoran Liu and Gengze Zhou and Yuze Wu and Xingxing Li and Yuxin Fan and Wenjun Li and Zhibo Chen and Fei Gao and Qi Wu and Zhizheng Zhang and He Wang", "abstract": "  Navigation is a fundamental capability in embodied AI, representing the\nintelligence required to perceive and interact within physical environments\nfollowing language instructions. Despite significant progress in large\nVision-Language Models (VLMs), which exhibit remarkable zero-shot performance\non general vision-language tasks, their generalization ability in embodied\nnavigation remains largely confined to narrow task settings and\nembodiment-specific architectures. In this work, we introduce a\ncross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained\non eight million navigation samples that encompass quadrupeds, drones, wheeled\nrobots, and vehicles, and spanning diverse tasks such as vision-and-language\nnavigation, object searching, target tracking, and autonomous driving. NavFoM\nemploys a unified architecture that processes multimodal navigation inputs from\nvarying camera configurations and navigation horizons. To accommodate diverse\ncamera setups and temporal horizons, NavFoM incorporates identifier tokens that\nembed camera view information of embodiments and the temporal context of tasks.\nFurthermore, to meet the demands of real-world deployment, NavFoM controls all\nobservation tokens using a dynamically adjusted sampling strategy under a\nlimited token length budget. Extensive evaluations on public benchmarks\ndemonstrate that our model achieves state-of-the-art or highly competitive\nperformance across multiple navigation tasks and embodiments without requiring\ntask-specific fine-tuning. Additional real-world experiments further confirm\nthe strong generalization capability and practical applicability of our\napproach.\n", "link": "http://arxiv.org/abs/2509.12129v1", "date": "2025-09-15", "relevancy": 2.8685, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.575}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embodied%20Navigation%20Foundation%20Model&body=Title%3A%20Embodied%20Navigation%20Foundation%20Model%0AAuthor%3A%20Jiazhao%20Zhang%20and%20Anqi%20Li%20and%20Yunpeng%20Qi%20and%20Minghan%20Li%20and%20Jiahang%20Liu%20and%20Shaoan%20Wang%20and%20Haoran%20Liu%20and%20Gengze%20Zhou%20and%20Yuze%20Wu%20and%20Xingxing%20Li%20and%20Yuxin%20Fan%20and%20Wenjun%20Li%20and%20Zhibo%20Chen%20and%20Fei%20Gao%20and%20Qi%20Wu%20and%20Zhizheng%20Zhang%20and%20He%20Wang%0AAbstract%3A%20%20%20Navigation%20is%20a%20fundamental%20capability%20in%20embodied%20AI%2C%20representing%20the%0Aintelligence%20required%20to%20perceive%20and%20interact%20within%20physical%20environments%0Afollowing%20language%20instructions.%20Despite%20significant%20progress%20in%20large%0AVision-Language%20Models%20%28VLMs%29%2C%20which%20exhibit%20remarkable%20zero-shot%20performance%0Aon%20general%20vision-language%20tasks%2C%20their%20generalization%20ability%20in%20embodied%0Anavigation%20remains%20largely%20confined%20to%20narrow%20task%20settings%20and%0Aembodiment-specific%20architectures.%20In%20this%20work%2C%20we%20introduce%20a%0Across-embodiment%20and%20cross-task%20Navigation%20Foundation%20Model%20%28NavFoM%29%2C%20trained%0Aon%20eight%20million%20navigation%20samples%20that%20encompass%20quadrupeds%2C%20drones%2C%20wheeled%0Arobots%2C%20and%20vehicles%2C%20and%20spanning%20diverse%20tasks%20such%20as%20vision-and-language%0Anavigation%2C%20object%20searching%2C%20target%20tracking%2C%20and%20autonomous%20driving.%20NavFoM%0Aemploys%20a%20unified%20architecture%20that%20processes%20multimodal%20navigation%20inputs%20from%0Avarying%20camera%20configurations%20and%20navigation%20horizons.%20To%20accommodate%20diverse%0Acamera%20setups%20and%20temporal%20horizons%2C%20NavFoM%20incorporates%20identifier%20tokens%20that%0Aembed%20camera%20view%20information%20of%20embodiments%20and%20the%20temporal%20context%20of%20tasks.%0AFurthermore%2C%20to%20meet%20the%20demands%20of%20real-world%20deployment%2C%20NavFoM%20controls%20all%0Aobservation%20tokens%20using%20a%20dynamically%20adjusted%20sampling%20strategy%20under%20a%0Alimited%20token%20length%20budget.%20Extensive%20evaluations%20on%20public%20benchmarks%0Ademonstrate%20that%20our%20model%20achieves%20state-of-the-art%20or%20highly%20competitive%0Aperformance%20across%20multiple%20navigation%20tasks%20and%20embodiments%20without%20requiring%0Atask-specific%20fine-tuning.%20Additional%20real-world%20experiments%20further%20confirm%0Athe%20strong%20generalization%20capability%20and%20practical%20applicability%20of%20our%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbodied%2520Navigation%2520Foundation%2520Model%26entry.906535625%3DJiazhao%2520Zhang%2520and%2520Anqi%2520Li%2520and%2520Yunpeng%2520Qi%2520and%2520Minghan%2520Li%2520and%2520Jiahang%2520Liu%2520and%2520Shaoan%2520Wang%2520and%2520Haoran%2520Liu%2520and%2520Gengze%2520Zhou%2520and%2520Yuze%2520Wu%2520and%2520Xingxing%2520Li%2520and%2520Yuxin%2520Fan%2520and%2520Wenjun%2520Li%2520and%2520Zhibo%2520Chen%2520and%2520Fei%2520Gao%2520and%2520Qi%2520Wu%2520and%2520Zhizheng%2520Zhang%2520and%2520He%2520Wang%26entry.1292438233%3D%2520%2520Navigation%2520is%2520a%2520fundamental%2520capability%2520in%2520embodied%2520AI%252C%2520representing%2520the%250Aintelligence%2520required%2520to%2520perceive%2520and%2520interact%2520within%2520physical%2520environments%250Afollowing%2520language%2520instructions.%2520Despite%2520significant%2520progress%2520in%2520large%250AVision-Language%2520Models%2520%2528VLMs%2529%252C%2520which%2520exhibit%2520remarkable%2520zero-shot%2520performance%250Aon%2520general%2520vision-language%2520tasks%252C%2520their%2520generalization%2520ability%2520in%2520embodied%250Anavigation%2520remains%2520largely%2520confined%2520to%2520narrow%2520task%2520settings%2520and%250Aembodiment-specific%2520architectures.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%250Across-embodiment%2520and%2520cross-task%2520Navigation%2520Foundation%2520Model%2520%2528NavFoM%2529%252C%2520trained%250Aon%2520eight%2520million%2520navigation%2520samples%2520that%2520encompass%2520quadrupeds%252C%2520drones%252C%2520wheeled%250Arobots%252C%2520and%2520vehicles%252C%2520and%2520spanning%2520diverse%2520tasks%2520such%2520as%2520vision-and-language%250Anavigation%252C%2520object%2520searching%252C%2520target%2520tracking%252C%2520and%2520autonomous%2520driving.%2520NavFoM%250Aemploys%2520a%2520unified%2520architecture%2520that%2520processes%2520multimodal%2520navigation%2520inputs%2520from%250Avarying%2520camera%2520configurations%2520and%2520navigation%2520horizons.%2520To%2520accommodate%2520diverse%250Acamera%2520setups%2520and%2520temporal%2520horizons%252C%2520NavFoM%2520incorporates%2520identifier%2520tokens%2520that%250Aembed%2520camera%2520view%2520information%2520of%2520embodiments%2520and%2520the%2520temporal%2520context%2520of%2520tasks.%250AFurthermore%252C%2520to%2520meet%2520the%2520demands%2520of%2520real-world%2520deployment%252C%2520NavFoM%2520controls%2520all%250Aobservation%2520tokens%2520using%2520a%2520dynamically%2520adjusted%2520sampling%2520strategy%2520under%2520a%250Alimited%2520token%2520length%2520budget.%2520Extensive%2520evaluations%2520on%2520public%2520benchmarks%250Ademonstrate%2520that%2520our%2520model%2520achieves%2520state-of-the-art%2520or%2520highly%2520competitive%250Aperformance%2520across%2520multiple%2520navigation%2520tasks%2520and%2520embodiments%2520without%2520requiring%250Atask-specific%2520fine-tuning.%2520Additional%2520real-world%2520experiments%2520further%2520confirm%250Athe%2520strong%2520generalization%2520capability%2520and%2520practical%2520applicability%2520of%2520our%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embodied%20Navigation%20Foundation%20Model&entry.906535625=Jiazhao%20Zhang%20and%20Anqi%20Li%20and%20Yunpeng%20Qi%20and%20Minghan%20Li%20and%20Jiahang%20Liu%20and%20Shaoan%20Wang%20and%20Haoran%20Liu%20and%20Gengze%20Zhou%20and%20Yuze%20Wu%20and%20Xingxing%20Li%20and%20Yuxin%20Fan%20and%20Wenjun%20Li%20and%20Zhibo%20Chen%20and%20Fei%20Gao%20and%20Qi%20Wu%20and%20Zhizheng%20Zhang%20and%20He%20Wang&entry.1292438233=%20%20Navigation%20is%20a%20fundamental%20capability%20in%20embodied%20AI%2C%20representing%20the%0Aintelligence%20required%20to%20perceive%20and%20interact%20within%20physical%20environments%0Afollowing%20language%20instructions.%20Despite%20significant%20progress%20in%20large%0AVision-Language%20Models%20%28VLMs%29%2C%20which%20exhibit%20remarkable%20zero-shot%20performance%0Aon%20general%20vision-language%20tasks%2C%20their%20generalization%20ability%20in%20embodied%0Anavigation%20remains%20largely%20confined%20to%20narrow%20task%20settings%20and%0Aembodiment-specific%20architectures.%20In%20this%20work%2C%20we%20introduce%20a%0Across-embodiment%20and%20cross-task%20Navigation%20Foundation%20Model%20%28NavFoM%29%2C%20trained%0Aon%20eight%20million%20navigation%20samples%20that%20encompass%20quadrupeds%2C%20drones%2C%20wheeled%0Arobots%2C%20and%20vehicles%2C%20and%20spanning%20diverse%20tasks%20such%20as%20vision-and-language%0Anavigation%2C%20object%20searching%2C%20target%20tracking%2C%20and%20autonomous%20driving.%20NavFoM%0Aemploys%20a%20unified%20architecture%20that%20processes%20multimodal%20navigation%20inputs%20from%0Avarying%20camera%20configurations%20and%20navigation%20horizons.%20To%20accommodate%20diverse%0Acamera%20setups%20and%20temporal%20horizons%2C%20NavFoM%20incorporates%20identifier%20tokens%20that%0Aembed%20camera%20view%20information%20of%20embodiments%20and%20the%20temporal%20context%20of%20tasks.%0AFurthermore%2C%20to%20meet%20the%20demands%20of%20real-world%20deployment%2C%20NavFoM%20controls%20all%0Aobservation%20tokens%20using%20a%20dynamically%20adjusted%20sampling%20strategy%20under%20a%0Alimited%20token%20length%20budget.%20Extensive%20evaluations%20on%20public%20benchmarks%0Ademonstrate%20that%20our%20model%20achieves%20state-of-the-art%20or%20highly%20competitive%0Aperformance%20across%20multiple%20navigation%20tasks%20and%20embodiments%20without%20requiring%0Atask-specific%20fine-tuning.%20Additional%20real-world%20experiments%20further%20confirm%0Athe%20strong%20generalization%20capability%20and%20practical%20applicability%20of%20our%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12129v1&entry.124074799=Read"},
{"title": "Character-Centric Understanding of Animated Movies", "author": "Zhongrui Gui and Junyu Xie and Tengda Han and Weidi Xie and Andrew Zisserman", "abstract": "  Animated movies are captivating for their unique character designs and\nimaginative storytelling, yet they pose significant challenges for existing\nrecognition systems. Unlike the consistent visual patterns detected by\nconventional face recognition methods, animated characters exhibit extreme\ndiversity in their appearance, motion, and deformation. In this work, we\npropose an audio-visual pipeline to enable automatic and robust animated\ncharacter recognition, and thereby enhance character-centric understanding of\nanimated movies. Central to our approach is the automatic construction of an\naudio-visual character bank from online sources. This bank contains both visual\nexemplars and voice (audio) samples for each character, enabling subsequent\nmulti-modal character recognition despite long-tailed appearance distributions.\nBuilding on accurate character recognition, we explore two downstream\napplications: Audio Description (AD) generation for visually impaired\naudiences, and character-aware subtitling for the hearing impaired. To support\nresearch in this domain, we introduce CMD-AM, a new dataset of 75 animated\nmovies with comprehensive annotations. Our character-centric pipeline\ndemonstrates significant improvements in both accessibility and narrative\ncomprehension for animated content over prior face-detection-based approaches.\nFor the code and dataset, visit\nhttps://www.robots.ox.ac.uk/~vgg/research/animated_ad/.\n", "link": "http://arxiv.org/abs/2509.12204v1", "date": "2025-09-15", "relevancy": 2.8604, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Character-Centric%20Understanding%20of%20Animated%20Movies&body=Title%3A%20Character-Centric%20Understanding%20of%20Animated%20Movies%0AAuthor%3A%20Zhongrui%20Gui%20and%20Junyu%20Xie%20and%20Tengda%20Han%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20Animated%20movies%20are%20captivating%20for%20their%20unique%20character%20designs%20and%0Aimaginative%20storytelling%2C%20yet%20they%20pose%20significant%20challenges%20for%20existing%0Arecognition%20systems.%20Unlike%20the%20consistent%20visual%20patterns%20detected%20by%0Aconventional%20face%20recognition%20methods%2C%20animated%20characters%20exhibit%20extreme%0Adiversity%20in%20their%20appearance%2C%20motion%2C%20and%20deformation.%20In%20this%20work%2C%20we%0Apropose%20an%20audio-visual%20pipeline%20to%20enable%20automatic%20and%20robust%20animated%0Acharacter%20recognition%2C%20and%20thereby%20enhance%20character-centric%20understanding%20of%0Aanimated%20movies.%20Central%20to%20our%20approach%20is%20the%20automatic%20construction%20of%20an%0Aaudio-visual%20character%20bank%20from%20online%20sources.%20This%20bank%20contains%20both%20visual%0Aexemplars%20and%20voice%20%28audio%29%20samples%20for%20each%20character%2C%20enabling%20subsequent%0Amulti-modal%20character%20recognition%20despite%20long-tailed%20appearance%20distributions.%0ABuilding%20on%20accurate%20character%20recognition%2C%20we%20explore%20two%20downstream%0Aapplications%3A%20Audio%20Description%20%28AD%29%20generation%20for%20visually%20impaired%0Aaudiences%2C%20and%20character-aware%20subtitling%20for%20the%20hearing%20impaired.%20To%20support%0Aresearch%20in%20this%20domain%2C%20we%20introduce%20CMD-AM%2C%20a%20new%20dataset%20of%2075%20animated%0Amovies%20with%20comprehensive%20annotations.%20Our%20character-centric%20pipeline%0Ademonstrates%20significant%20improvements%20in%20both%20accessibility%20and%20narrative%0Acomprehension%20for%20animated%20content%20over%20prior%20face-detection-based%20approaches.%0AFor%20the%20code%20and%20dataset%2C%20visit%0Ahttps%3A//www.robots.ox.ac.uk/~vgg/research/animated_ad/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacter-Centric%2520Understanding%2520of%2520Animated%2520Movies%26entry.906535625%3DZhongrui%2520Gui%2520and%2520Junyu%2520Xie%2520and%2520Tengda%2520Han%2520and%2520Weidi%2520Xie%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3D%2520%2520Animated%2520movies%2520are%2520captivating%2520for%2520their%2520unique%2520character%2520designs%2520and%250Aimaginative%2520storytelling%252C%2520yet%2520they%2520pose%2520significant%2520challenges%2520for%2520existing%250Arecognition%2520systems.%2520Unlike%2520the%2520consistent%2520visual%2520patterns%2520detected%2520by%250Aconventional%2520face%2520recognition%2520methods%252C%2520animated%2520characters%2520exhibit%2520extreme%250Adiversity%2520in%2520their%2520appearance%252C%2520motion%252C%2520and%2520deformation.%2520In%2520this%2520work%252C%2520we%250Apropose%2520an%2520audio-visual%2520pipeline%2520to%2520enable%2520automatic%2520and%2520robust%2520animated%250Acharacter%2520recognition%252C%2520and%2520thereby%2520enhance%2520character-centric%2520understanding%2520of%250Aanimated%2520movies.%2520Central%2520to%2520our%2520approach%2520is%2520the%2520automatic%2520construction%2520of%2520an%250Aaudio-visual%2520character%2520bank%2520from%2520online%2520sources.%2520This%2520bank%2520contains%2520both%2520visual%250Aexemplars%2520and%2520voice%2520%2528audio%2529%2520samples%2520for%2520each%2520character%252C%2520enabling%2520subsequent%250Amulti-modal%2520character%2520recognition%2520despite%2520long-tailed%2520appearance%2520distributions.%250ABuilding%2520on%2520accurate%2520character%2520recognition%252C%2520we%2520explore%2520two%2520downstream%250Aapplications%253A%2520Audio%2520Description%2520%2528AD%2529%2520generation%2520for%2520visually%2520impaired%250Aaudiences%252C%2520and%2520character-aware%2520subtitling%2520for%2520the%2520hearing%2520impaired.%2520To%2520support%250Aresearch%2520in%2520this%2520domain%252C%2520we%2520introduce%2520CMD-AM%252C%2520a%2520new%2520dataset%2520of%252075%2520animated%250Amovies%2520with%2520comprehensive%2520annotations.%2520Our%2520character-centric%2520pipeline%250Ademonstrates%2520significant%2520improvements%2520in%2520both%2520accessibility%2520and%2520narrative%250Acomprehension%2520for%2520animated%2520content%2520over%2520prior%2520face-detection-based%2520approaches.%250AFor%2520the%2520code%2520and%2520dataset%252C%2520visit%250Ahttps%253A//www.robots.ox.ac.uk/~vgg/research/animated_ad/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Character-Centric%20Understanding%20of%20Animated%20Movies&entry.906535625=Zhongrui%20Gui%20and%20Junyu%20Xie%20and%20Tengda%20Han%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman&entry.1292438233=%20%20Animated%20movies%20are%20captivating%20for%20their%20unique%20character%20designs%20and%0Aimaginative%20storytelling%2C%20yet%20they%20pose%20significant%20challenges%20for%20existing%0Arecognition%20systems.%20Unlike%20the%20consistent%20visual%20patterns%20detected%20by%0Aconventional%20face%20recognition%20methods%2C%20animated%20characters%20exhibit%20extreme%0Adiversity%20in%20their%20appearance%2C%20motion%2C%20and%20deformation.%20In%20this%20work%2C%20we%0Apropose%20an%20audio-visual%20pipeline%20to%20enable%20automatic%20and%20robust%20animated%0Acharacter%20recognition%2C%20and%20thereby%20enhance%20character-centric%20understanding%20of%0Aanimated%20movies.%20Central%20to%20our%20approach%20is%20the%20automatic%20construction%20of%20an%0Aaudio-visual%20character%20bank%20from%20online%20sources.%20This%20bank%20contains%20both%20visual%0Aexemplars%20and%20voice%20%28audio%29%20samples%20for%20each%20character%2C%20enabling%20subsequent%0Amulti-modal%20character%20recognition%20despite%20long-tailed%20appearance%20distributions.%0ABuilding%20on%20accurate%20character%20recognition%2C%20we%20explore%20two%20downstream%0Aapplications%3A%20Audio%20Description%20%28AD%29%20generation%20for%20visually%20impaired%0Aaudiences%2C%20and%20character-aware%20subtitling%20for%20the%20hearing%20impaired.%20To%20support%0Aresearch%20in%20this%20domain%2C%20we%20introduce%20CMD-AM%2C%20a%20new%20dataset%20of%2075%20animated%0Amovies%20with%20comprehensive%20annotations.%20Our%20character-centric%20pipeline%0Ademonstrates%20significant%20improvements%20in%20both%20accessibility%20and%20narrative%0Acomprehension%20for%20animated%20content%20over%20prior%20face-detection-based%20approaches.%0AFor%20the%20code%20and%20dataset%2C%20visit%0Ahttps%3A//www.robots.ox.ac.uk/~vgg/research/animated_ad/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12204v1&entry.124074799=Read"},
{"title": "3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review", "author": "Salma Galaaoui and Eduardo Valle and David Picard and Nermin Samet", "abstract": "  In this paper, we present a comprehensive review of 3D human pose estimation\nand human mesh recovery from in-the-wild LiDAR point clouds. We compare\nexisting approaches across several key dimensions, and propose a structured\ntaxonomy to classify these methods. Following this taxonomy, we analyze each\nmethod's strengths, limitations, and design choices. In addition, (i) we\nperform a quantitative comparison of the three most widely used datasets,\ndetailing their characteristics; (ii) we compile unified definitions of all\nevaluation metrics; and (iii) we establish benchmark tables for both tasks on\nthese datasets to enable fair comparisons and promote progress in the field. We\nalso outline open challenges and research directions critical for advancing\nLiDAR-based 3D human understanding. Moreover, we maintain an accompanying\nwebpage that organizes papers according to our taxonomy and continuously update\nit with new studies:\nhttps://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR\n", "link": "http://arxiv.org/abs/2509.12197v1", "date": "2025-09-15", "relevancy": 2.8353, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5805}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5753}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Human%20Pose%20and%20Shape%20Estimation%20from%20LiDAR%20Point%20Clouds%3A%20A%20Review&body=Title%3A%203D%20Human%20Pose%20and%20Shape%20Estimation%20from%20LiDAR%20Point%20Clouds%3A%20A%20Review%0AAuthor%3A%20Salma%20Galaaoui%20and%20Eduardo%20Valle%20and%20David%20Picard%20and%20Nermin%20Samet%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20review%20of%203D%20human%20pose%20estimation%0Aand%20human%20mesh%20recovery%20from%20in-the-wild%20LiDAR%20point%20clouds.%20We%20compare%0Aexisting%20approaches%20across%20several%20key%20dimensions%2C%20and%20propose%20a%20structured%0Ataxonomy%20to%20classify%20these%20methods.%20Following%20this%20taxonomy%2C%20we%20analyze%20each%0Amethod%27s%20strengths%2C%20limitations%2C%20and%20design%20choices.%20In%20addition%2C%20%28i%29%20we%0Aperform%20a%20quantitative%20comparison%20of%20the%20three%20most%20widely%20used%20datasets%2C%0Adetailing%20their%20characteristics%3B%20%28ii%29%20we%20compile%20unified%20definitions%20of%20all%0Aevaluation%20metrics%3B%20and%20%28iii%29%20we%20establish%20benchmark%20tables%20for%20both%20tasks%20on%0Athese%20datasets%20to%20enable%20fair%20comparisons%20and%20promote%20progress%20in%20the%20field.%20We%0Aalso%20outline%20open%20challenges%20and%20research%20directions%20critical%20for%20advancing%0ALiDAR-based%203D%20human%20understanding.%20Moreover%2C%20we%20maintain%20an%20accompanying%0Awebpage%20that%20organizes%20papers%20according%20to%20our%20taxonomy%20and%20continuously%20update%0Ait%20with%20new%20studies%3A%0Ahttps%3A//github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Human%2520Pose%2520and%2520Shape%2520Estimation%2520from%2520LiDAR%2520Point%2520Clouds%253A%2520A%2520Review%26entry.906535625%3DSalma%2520Galaaoui%2520and%2520Eduardo%2520Valle%2520and%2520David%2520Picard%2520and%2520Nermin%2520Samet%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520review%2520of%25203D%2520human%2520pose%2520estimation%250Aand%2520human%2520mesh%2520recovery%2520from%2520in-the-wild%2520LiDAR%2520point%2520clouds.%2520We%2520compare%250Aexisting%2520approaches%2520across%2520several%2520key%2520dimensions%252C%2520and%2520propose%2520a%2520structured%250Ataxonomy%2520to%2520classify%2520these%2520methods.%2520Following%2520this%2520taxonomy%252C%2520we%2520analyze%2520each%250Amethod%2527s%2520strengths%252C%2520limitations%252C%2520and%2520design%2520choices.%2520In%2520addition%252C%2520%2528i%2529%2520we%250Aperform%2520a%2520quantitative%2520comparison%2520of%2520the%2520three%2520most%2520widely%2520used%2520datasets%252C%250Adetailing%2520their%2520characteristics%253B%2520%2528ii%2529%2520we%2520compile%2520unified%2520definitions%2520of%2520all%250Aevaluation%2520metrics%253B%2520and%2520%2528iii%2529%2520we%2520establish%2520benchmark%2520tables%2520for%2520both%2520tasks%2520on%250Athese%2520datasets%2520to%2520enable%2520fair%2520comparisons%2520and%2520promote%2520progress%2520in%2520the%2520field.%2520We%250Aalso%2520outline%2520open%2520challenges%2520and%2520research%2520directions%2520critical%2520for%2520advancing%250ALiDAR-based%25203D%2520human%2520understanding.%2520Moreover%252C%2520we%2520maintain%2520an%2520accompanying%250Awebpage%2520that%2520organizes%2520papers%2520according%2520to%2520our%2520taxonomy%2520and%2520continuously%2520update%250Ait%2520with%2520new%2520studies%253A%250Ahttps%253A//github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Human%20Pose%20and%20Shape%20Estimation%20from%20LiDAR%20Point%20Clouds%3A%20A%20Review&entry.906535625=Salma%20Galaaoui%20and%20Eduardo%20Valle%20and%20David%20Picard%20and%20Nermin%20Samet&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20review%20of%203D%20human%20pose%20estimation%0Aand%20human%20mesh%20recovery%20from%20in-the-wild%20LiDAR%20point%20clouds.%20We%20compare%0Aexisting%20approaches%20across%20several%20key%20dimensions%2C%20and%20propose%20a%20structured%0Ataxonomy%20to%20classify%20these%20methods.%20Following%20this%20taxonomy%2C%20we%20analyze%20each%0Amethod%27s%20strengths%2C%20limitations%2C%20and%20design%20choices.%20In%20addition%2C%20%28i%29%20we%0Aperform%20a%20quantitative%20comparison%20of%20the%20three%20most%20widely%20used%20datasets%2C%0Adetailing%20their%20characteristics%3B%20%28ii%29%20we%20compile%20unified%20definitions%20of%20all%0Aevaluation%20metrics%3B%20and%20%28iii%29%20we%20establish%20benchmark%20tables%20for%20both%20tasks%20on%0Athese%20datasets%20to%20enable%20fair%20comparisons%20and%20promote%20progress%20in%20the%20field.%20We%0Aalso%20outline%20open%20challenges%20and%20research%20directions%20critical%20for%20advancing%0ALiDAR-based%203D%20human%20understanding.%20Moreover%2C%20we%20maintain%20an%20accompanying%0Awebpage%20that%20organizes%20papers%20according%20to%20our%20taxonomy%20and%20continuously%20update%0Ait%20with%20new%20studies%3A%0Ahttps%3A//github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12197v1&entry.124074799=Read"},
{"title": "SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and\n  Test-Time Training for Camouflaged Object Detection", "author": "Zhenni Yu and Li Zhao and Guobao Xiao and Xiaoqin Zhang", "abstract": "  This paper introduces a new Segment Anything Model (SAM) that leverages\nreverse parameter configuration and test-time training to enhance its\nperformance on Camouflaged Object Detection (COD), named SAM-TTT. While most\nexisting SAM-based COD models primarily focus on enhancing SAM by extracting\nfavorable features and amplifying its advantageous parameters, a crucial gap is\nidentified: insufficient attention to adverse parameters that impair SAM's\nsemantic understanding in downstream tasks. To tackle this issue, the Reverse\nSAM Parameter Configuration Module is proposed to effectively mitigate the\ninfluence of adverse parameters in a train-free manner by configuring SAM's\nparameters. Building on this foundation, the T-Visioner Module is unveiled to\nstrengthen advantageous parameters by integrating Test-Time Training layers,\noriginally developed for language tasks, into vision tasks. Test-Time Training\nlayers represent a new class of sequence modeling layers characterized by\nlinear complexity and an expressive hidden state. By integrating two modules,\nSAM-TTT simultaneously suppresses adverse parameters while reinforcing\nadvantageous ones, significantly improving SAM's semantic understanding in COD\ntask. Our experimental results on various COD benchmarks demonstrate that the\nproposed approach achieves state-of-the-art performance, setting a new\nbenchmark in the field. The code will be available at\nhttps://github.com/guobaoxiao/SAM-TTT.\n", "link": "http://arxiv.org/abs/2509.11884v1", "date": "2025-09-15", "relevancy": 2.8065, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5976}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5432}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM-TTT%3A%20Segment%20Anything%20Model%20via%20Reverse%20Parameter%20Configuration%20and%0A%20%20Test-Time%20Training%20for%20Camouflaged%20Object%20Detection&body=Title%3A%20SAM-TTT%3A%20Segment%20Anything%20Model%20via%20Reverse%20Parameter%20Configuration%20and%0A%20%20Test-Time%20Training%20for%20Camouflaged%20Object%20Detection%0AAuthor%3A%20Zhenni%20Yu%20and%20Li%20Zhao%20and%20Guobao%20Xiao%20and%20Xiaoqin%20Zhang%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20new%20Segment%20Anything%20Model%20%28SAM%29%20that%20leverages%0Areverse%20parameter%20configuration%20and%20test-time%20training%20to%20enhance%20its%0Aperformance%20on%20Camouflaged%20Object%20Detection%20%28COD%29%2C%20named%20SAM-TTT.%20While%20most%0Aexisting%20SAM-based%20COD%20models%20primarily%20focus%20on%20enhancing%20SAM%20by%20extracting%0Afavorable%20features%20and%20amplifying%20its%20advantageous%20parameters%2C%20a%20crucial%20gap%20is%0Aidentified%3A%20insufficient%20attention%20to%20adverse%20parameters%20that%20impair%20SAM%27s%0Asemantic%20understanding%20in%20downstream%20tasks.%20To%20tackle%20this%20issue%2C%20the%20Reverse%0ASAM%20Parameter%20Configuration%20Module%20is%20proposed%20to%20effectively%20mitigate%20the%0Ainfluence%20of%20adverse%20parameters%20in%20a%20train-free%20manner%20by%20configuring%20SAM%27s%0Aparameters.%20Building%20on%20this%20foundation%2C%20the%20T-Visioner%20Module%20is%20unveiled%20to%0Astrengthen%20advantageous%20parameters%20by%20integrating%20Test-Time%20Training%20layers%2C%0Aoriginally%20developed%20for%20language%20tasks%2C%20into%20vision%20tasks.%20Test-Time%20Training%0Alayers%20represent%20a%20new%20class%20of%20sequence%20modeling%20layers%20characterized%20by%0Alinear%20complexity%20and%20an%20expressive%20hidden%20state.%20By%20integrating%20two%20modules%2C%0ASAM-TTT%20simultaneously%20suppresses%20adverse%20parameters%20while%20reinforcing%0Aadvantageous%20ones%2C%20significantly%20improving%20SAM%27s%20semantic%20understanding%20in%20COD%0Atask.%20Our%20experimental%20results%20on%20various%20COD%20benchmarks%20demonstrate%20that%20the%0Aproposed%20approach%20achieves%20state-of-the-art%20performance%2C%20setting%20a%20new%0Abenchmark%20in%20the%20field.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/guobaoxiao/SAM-TTT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM-TTT%253A%2520Segment%2520Anything%2520Model%2520via%2520Reverse%2520Parameter%2520Configuration%2520and%250A%2520%2520Test-Time%2520Training%2520for%2520Camouflaged%2520Object%2520Detection%26entry.906535625%3DZhenni%2520Yu%2520and%2520Li%2520Zhao%2520and%2520Guobao%2520Xiao%2520and%2520Xiaoqin%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520new%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520that%2520leverages%250Areverse%2520parameter%2520configuration%2520and%2520test-time%2520training%2520to%2520enhance%2520its%250Aperformance%2520on%2520Camouflaged%2520Object%2520Detection%2520%2528COD%2529%252C%2520named%2520SAM-TTT.%2520While%2520most%250Aexisting%2520SAM-based%2520COD%2520models%2520primarily%2520focus%2520on%2520enhancing%2520SAM%2520by%2520extracting%250Afavorable%2520features%2520and%2520amplifying%2520its%2520advantageous%2520parameters%252C%2520a%2520crucial%2520gap%2520is%250Aidentified%253A%2520insufficient%2520attention%2520to%2520adverse%2520parameters%2520that%2520impair%2520SAM%2527s%250Asemantic%2520understanding%2520in%2520downstream%2520tasks.%2520To%2520tackle%2520this%2520issue%252C%2520the%2520Reverse%250ASAM%2520Parameter%2520Configuration%2520Module%2520is%2520proposed%2520to%2520effectively%2520mitigate%2520the%250Ainfluence%2520of%2520adverse%2520parameters%2520in%2520a%2520train-free%2520manner%2520by%2520configuring%2520SAM%2527s%250Aparameters.%2520Building%2520on%2520this%2520foundation%252C%2520the%2520T-Visioner%2520Module%2520is%2520unveiled%2520to%250Astrengthen%2520advantageous%2520parameters%2520by%2520integrating%2520Test-Time%2520Training%2520layers%252C%250Aoriginally%2520developed%2520for%2520language%2520tasks%252C%2520into%2520vision%2520tasks.%2520Test-Time%2520Training%250Alayers%2520represent%2520a%2520new%2520class%2520of%2520sequence%2520modeling%2520layers%2520characterized%2520by%250Alinear%2520complexity%2520and%2520an%2520expressive%2520hidden%2520state.%2520By%2520integrating%2520two%2520modules%252C%250ASAM-TTT%2520simultaneously%2520suppresses%2520adverse%2520parameters%2520while%2520reinforcing%250Aadvantageous%2520ones%252C%2520significantly%2520improving%2520SAM%2527s%2520semantic%2520understanding%2520in%2520COD%250Atask.%2520Our%2520experimental%2520results%2520on%2520various%2520COD%2520benchmarks%2520demonstrate%2520that%2520the%250Aproposed%2520approach%2520achieves%2520state-of-the-art%2520performance%252C%2520setting%2520a%2520new%250Abenchmark%2520in%2520the%2520field.%2520The%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/guobaoxiao/SAM-TTT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM-TTT%3A%20Segment%20Anything%20Model%20via%20Reverse%20Parameter%20Configuration%20and%0A%20%20Test-Time%20Training%20for%20Camouflaged%20Object%20Detection&entry.906535625=Zhenni%20Yu%20and%20Li%20Zhao%20and%20Guobao%20Xiao%20and%20Xiaoqin%20Zhang&entry.1292438233=%20%20This%20paper%20introduces%20a%20new%20Segment%20Anything%20Model%20%28SAM%29%20that%20leverages%0Areverse%20parameter%20configuration%20and%20test-time%20training%20to%20enhance%20its%0Aperformance%20on%20Camouflaged%20Object%20Detection%20%28COD%29%2C%20named%20SAM-TTT.%20While%20most%0Aexisting%20SAM-based%20COD%20models%20primarily%20focus%20on%20enhancing%20SAM%20by%20extracting%0Afavorable%20features%20and%20amplifying%20its%20advantageous%20parameters%2C%20a%20crucial%20gap%20is%0Aidentified%3A%20insufficient%20attention%20to%20adverse%20parameters%20that%20impair%20SAM%27s%0Asemantic%20understanding%20in%20downstream%20tasks.%20To%20tackle%20this%20issue%2C%20the%20Reverse%0ASAM%20Parameter%20Configuration%20Module%20is%20proposed%20to%20effectively%20mitigate%20the%0Ainfluence%20of%20adverse%20parameters%20in%20a%20train-free%20manner%20by%20configuring%20SAM%27s%0Aparameters.%20Building%20on%20this%20foundation%2C%20the%20T-Visioner%20Module%20is%20unveiled%20to%0Astrengthen%20advantageous%20parameters%20by%20integrating%20Test-Time%20Training%20layers%2C%0Aoriginally%20developed%20for%20language%20tasks%2C%20into%20vision%20tasks.%20Test-Time%20Training%0Alayers%20represent%20a%20new%20class%20of%20sequence%20modeling%20layers%20characterized%20by%0Alinear%20complexity%20and%20an%20expressive%20hidden%20state.%20By%20integrating%20two%20modules%2C%0ASAM-TTT%20simultaneously%20suppresses%20adverse%20parameters%20while%20reinforcing%0Aadvantageous%20ones%2C%20significantly%20improving%20SAM%27s%20semantic%20understanding%20in%20COD%0Atask.%20Our%20experimental%20results%20on%20various%20COD%20benchmarks%20demonstrate%20that%20the%0Aproposed%20approach%20achieves%20state-of-the-art%20performance%2C%20setting%20a%20new%0Abenchmark%20in%20the%20field.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/guobaoxiao/SAM-TTT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11884v1&entry.124074799=Read"},
{"title": "RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning", "author": "Suhang Hu and Wei Hu and Yuhang Su and Fan Zhang", "abstract": "  Vision-Language Models (VLMs) struggle with complex image annotation tasks,\nsuch as emotion classification and context-driven object detection, which\ndemand sophisticated reasoning. Standard Supervised Fine-Tuning (SFT) focuses\nsolely on annotation outcomes, ignoring underlying rationales, while Visual\nReinforcement Fine-Tuning (Visual-RFT) produces inconsistent Chains of Thought\n(CoTs) due to the absence of high-quality, verified CoTs during pre-training.\nWe introduce RISE (Reason-Inspire-Strengthen-Expertise), a two-stage framework\nto overcome these limitations. In the Reason stage (RISE-CoT), a reinforcement\nlearning-driven \"annotation-reasoning-annotation\" closed-loop generates\nvisually grounded, logically consistent CoTs by verifying their ability to\nreconstruct original annotations without direct leakage. The Inspire and\nStrengthen stage (RISE-R1) leverages a high-quality CoT subset, filtered by\nRISE-CoT rewards, for supervised fine-tuning, followed by reinforcement\nfine-tuning to produce interpretable reasoning and accurate annotations,\nachieving Expertise in complex visual tasks. Evaluated on complex and simple\nimage annotation tasks, RISE-trained Qwen2-VL-2B outperforms SFT and\nVisual-RFT, achieving robust performance and enhanced explainability. RISE\noffers a self-supervised solution for advancing VLM reasoning without requiring\nmanually annotated CoTs.Code and resources are available at:\nhttps://github.com/HSH55/RISE.\n", "link": "http://arxiv.org/abs/2508.13229v3", "date": "2025-09-15", "relevancy": 2.7917, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5638}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RISE%3A%20Enhancing%20VLM%20Image%20Annotation%20with%20Self-Supervised%20Reasoning&body=Title%3A%20RISE%3A%20Enhancing%20VLM%20Image%20Annotation%20with%20Self-Supervised%20Reasoning%0AAuthor%3A%20Suhang%20Hu%20and%20Wei%20Hu%20and%20Yuhang%20Su%20and%20Fan%20Zhang%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20struggle%20with%20complex%20image%20annotation%20tasks%2C%0Asuch%20as%20emotion%20classification%20and%20context-driven%20object%20detection%2C%20which%0Ademand%20sophisticated%20reasoning.%20Standard%20Supervised%20Fine-Tuning%20%28SFT%29%20focuses%0Asolely%20on%20annotation%20outcomes%2C%20ignoring%20underlying%20rationales%2C%20while%20Visual%0AReinforcement%20Fine-Tuning%20%28Visual-RFT%29%20produces%20inconsistent%20Chains%20of%20Thought%0A%28CoTs%29%20due%20to%20the%20absence%20of%20high-quality%2C%20verified%20CoTs%20during%20pre-training.%0AWe%20introduce%20RISE%20%28Reason-Inspire-Strengthen-Expertise%29%2C%20a%20two-stage%20framework%0Ato%20overcome%20these%20limitations.%20In%20the%20Reason%20stage%20%28RISE-CoT%29%2C%20a%20reinforcement%0Alearning-driven%20%22annotation-reasoning-annotation%22%20closed-loop%20generates%0Avisually%20grounded%2C%20logically%20consistent%20CoTs%20by%20verifying%20their%20ability%20to%0Areconstruct%20original%20annotations%20without%20direct%20leakage.%20The%20Inspire%20and%0AStrengthen%20stage%20%28RISE-R1%29%20leverages%20a%20high-quality%20CoT%20subset%2C%20filtered%20by%0ARISE-CoT%20rewards%2C%20for%20supervised%20fine-tuning%2C%20followed%20by%20reinforcement%0Afine-tuning%20to%20produce%20interpretable%20reasoning%20and%20accurate%20annotations%2C%0Aachieving%20Expertise%20in%20complex%20visual%20tasks.%20Evaluated%20on%20complex%20and%20simple%0Aimage%20annotation%20tasks%2C%20RISE-trained%20Qwen2-VL-2B%20outperforms%20SFT%20and%0AVisual-RFT%2C%20achieving%20robust%20performance%20and%20enhanced%20explainability.%20RISE%0Aoffers%20a%20self-supervised%20solution%20for%20advancing%20VLM%20reasoning%20without%20requiring%0Amanually%20annotated%20CoTs.Code%20and%20resources%20are%20available%20at%3A%0Ahttps%3A//github.com/HSH55/RISE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13229v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRISE%253A%2520Enhancing%2520VLM%2520Image%2520Annotation%2520with%2520Self-Supervised%2520Reasoning%26entry.906535625%3DSuhang%2520Hu%2520and%2520Wei%2520Hu%2520and%2520Yuhang%2520Su%2520and%2520Fan%2520Zhang%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520struggle%2520with%2520complex%2520image%2520annotation%2520tasks%252C%250Asuch%2520as%2520emotion%2520classification%2520and%2520context-driven%2520object%2520detection%252C%2520which%250Ademand%2520sophisticated%2520reasoning.%2520Standard%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520focuses%250Asolely%2520on%2520annotation%2520outcomes%252C%2520ignoring%2520underlying%2520rationales%252C%2520while%2520Visual%250AReinforcement%2520Fine-Tuning%2520%2528Visual-RFT%2529%2520produces%2520inconsistent%2520Chains%2520of%2520Thought%250A%2528CoTs%2529%2520due%2520to%2520the%2520absence%2520of%2520high-quality%252C%2520verified%2520CoTs%2520during%2520pre-training.%250AWe%2520introduce%2520RISE%2520%2528Reason-Inspire-Strengthen-Expertise%2529%252C%2520a%2520two-stage%2520framework%250Ato%2520overcome%2520these%2520limitations.%2520In%2520the%2520Reason%2520stage%2520%2528RISE-CoT%2529%252C%2520a%2520reinforcement%250Alearning-driven%2520%2522annotation-reasoning-annotation%2522%2520closed-loop%2520generates%250Avisually%2520grounded%252C%2520logically%2520consistent%2520CoTs%2520by%2520verifying%2520their%2520ability%2520to%250Areconstruct%2520original%2520annotations%2520without%2520direct%2520leakage.%2520The%2520Inspire%2520and%250AStrengthen%2520stage%2520%2528RISE-R1%2529%2520leverages%2520a%2520high-quality%2520CoT%2520subset%252C%2520filtered%2520by%250ARISE-CoT%2520rewards%252C%2520for%2520supervised%2520fine-tuning%252C%2520followed%2520by%2520reinforcement%250Afine-tuning%2520to%2520produce%2520interpretable%2520reasoning%2520and%2520accurate%2520annotations%252C%250Aachieving%2520Expertise%2520in%2520complex%2520visual%2520tasks.%2520Evaluated%2520on%2520complex%2520and%2520simple%250Aimage%2520annotation%2520tasks%252C%2520RISE-trained%2520Qwen2-VL-2B%2520outperforms%2520SFT%2520and%250AVisual-RFT%252C%2520achieving%2520robust%2520performance%2520and%2520enhanced%2520explainability.%2520RISE%250Aoffers%2520a%2520self-supervised%2520solution%2520for%2520advancing%2520VLM%2520reasoning%2520without%2520requiring%250Amanually%2520annotated%2520CoTs.Code%2520and%2520resources%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/HSH55/RISE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13229v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RISE%3A%20Enhancing%20VLM%20Image%20Annotation%20with%20Self-Supervised%20Reasoning&entry.906535625=Suhang%20Hu%20and%20Wei%20Hu%20and%20Yuhang%20Su%20and%20Fan%20Zhang&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20struggle%20with%20complex%20image%20annotation%20tasks%2C%0Asuch%20as%20emotion%20classification%20and%20context-driven%20object%20detection%2C%20which%0Ademand%20sophisticated%20reasoning.%20Standard%20Supervised%20Fine-Tuning%20%28SFT%29%20focuses%0Asolely%20on%20annotation%20outcomes%2C%20ignoring%20underlying%20rationales%2C%20while%20Visual%0AReinforcement%20Fine-Tuning%20%28Visual-RFT%29%20produces%20inconsistent%20Chains%20of%20Thought%0A%28CoTs%29%20due%20to%20the%20absence%20of%20high-quality%2C%20verified%20CoTs%20during%20pre-training.%0AWe%20introduce%20RISE%20%28Reason-Inspire-Strengthen-Expertise%29%2C%20a%20two-stage%20framework%0Ato%20overcome%20these%20limitations.%20In%20the%20Reason%20stage%20%28RISE-CoT%29%2C%20a%20reinforcement%0Alearning-driven%20%22annotation-reasoning-annotation%22%20closed-loop%20generates%0Avisually%20grounded%2C%20logically%20consistent%20CoTs%20by%20verifying%20their%20ability%20to%0Areconstruct%20original%20annotations%20without%20direct%20leakage.%20The%20Inspire%20and%0AStrengthen%20stage%20%28RISE-R1%29%20leverages%20a%20high-quality%20CoT%20subset%2C%20filtered%20by%0ARISE-CoT%20rewards%2C%20for%20supervised%20fine-tuning%2C%20followed%20by%20reinforcement%0Afine-tuning%20to%20produce%20interpretable%20reasoning%20and%20accurate%20annotations%2C%0Aachieving%20Expertise%20in%20complex%20visual%20tasks.%20Evaluated%20on%20complex%20and%20simple%0Aimage%20annotation%20tasks%2C%20RISE-trained%20Qwen2-VL-2B%20outperforms%20SFT%20and%0AVisual-RFT%2C%20achieving%20robust%20performance%20and%20enhanced%20explainability.%20RISE%0Aoffers%20a%20self-supervised%20solution%20for%20advancing%20VLM%20reasoning%20without%20requiring%0Amanually%20annotated%20CoTs.Code%20and%20resources%20are%20available%20at%3A%0Ahttps%3A//github.com/HSH55/RISE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13229v3&entry.124074799=Read"},
{"title": "3D Mesh Editing using Masked LRMs", "author": "Will Gao and Dilin Wang and Yuchen Fan and Aljaz Bozic and Tuur Stuyck and Zhengqin Li and Zhao Dong and Rakesh Ranjan and Nikolaos Sarafianos", "abstract": "  We present a novel approach to shape editing, building on recent progress in\n3D reconstruction from multi-view images. We formulate shape editing as a\nconditional reconstruction problem, where the model must reconstruct the input\nshape with the exception of a specified 3D region, in which the geometry should\nbe generated from the conditional signal. To this end, we train a conditional\nLarge Reconstruction Model (LRM) for masked reconstruction, using multi-view\nconsistent masks rendered from a randomly generated 3D occlusion, and using one\nclean viewpoint as the conditional signal. During inference, we manually define\na 3D region to edit and provide an edited image from a canonical viewpoint to\nfill that region. We demonstrate that, in just a single forward pass, our\nmethod not only preserves the input geometry in the unmasked region through\nreconstruction capabilities on par with SoTA, but is also expressive enough to\nperform a variety of mesh edits from a single image guidance that past works\nstruggle with, while being 2-10x faster than the top-performing prior work.\n", "link": "http://arxiv.org/abs/2412.08641v2", "date": "2025-09-15", "relevancy": 2.7907, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5675}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5541}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Mesh%20Editing%20using%20Masked%20LRMs&body=Title%3A%203D%20Mesh%20Editing%20using%20Masked%20LRMs%0AAuthor%3A%20Will%20Gao%20and%20Dilin%20Wang%20and%20Yuchen%20Fan%20and%20Aljaz%20Bozic%20and%20Tuur%20Stuyck%20and%20Zhengqin%20Li%20and%20Zhao%20Dong%20and%20Rakesh%20Ranjan%20and%20Nikolaos%20Sarafianos%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20to%20shape%20editing%2C%20building%20on%20recent%20progress%20in%0A3D%20reconstruction%20from%20multi-view%20images.%20We%20formulate%20shape%20editing%20as%20a%0Aconditional%20reconstruction%20problem%2C%20where%20the%20model%20must%20reconstruct%20the%20input%0Ashape%20with%20the%20exception%20of%20a%20specified%203D%20region%2C%20in%20which%20the%20geometry%20should%0Abe%20generated%20from%20the%20conditional%20signal.%20To%20this%20end%2C%20we%20train%20a%20conditional%0ALarge%20Reconstruction%20Model%20%28LRM%29%20for%20masked%20reconstruction%2C%20using%20multi-view%0Aconsistent%20masks%20rendered%20from%20a%20randomly%20generated%203D%20occlusion%2C%20and%20using%20one%0Aclean%20viewpoint%20as%20the%20conditional%20signal.%20During%20inference%2C%20we%20manually%20define%0Aa%203D%20region%20to%20edit%20and%20provide%20an%20edited%20image%20from%20a%20canonical%20viewpoint%20to%0Afill%20that%20region.%20We%20demonstrate%20that%2C%20in%20just%20a%20single%20forward%20pass%2C%20our%0Amethod%20not%20only%20preserves%20the%20input%20geometry%20in%20the%20unmasked%20region%20through%0Areconstruction%20capabilities%20on%20par%20with%20SoTA%2C%20but%20is%20also%20expressive%20enough%20to%0Aperform%20a%20variety%20of%20mesh%20edits%20from%20a%20single%20image%20guidance%20that%20past%20works%0Astruggle%20with%2C%20while%20being%202-10x%20faster%20than%20the%20top-performing%20prior%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08641v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Mesh%2520Editing%2520using%2520Masked%2520LRMs%26entry.906535625%3DWill%2520Gao%2520and%2520Dilin%2520Wang%2520and%2520Yuchen%2520Fan%2520and%2520Aljaz%2520Bozic%2520and%2520Tuur%2520Stuyck%2520and%2520Zhengqin%2520Li%2520and%2520Zhao%2520Dong%2520and%2520Rakesh%2520Ranjan%2520and%2520Nikolaos%2520Sarafianos%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520to%2520shape%2520editing%252C%2520building%2520on%2520recent%2520progress%2520in%250A3D%2520reconstruction%2520from%2520multi-view%2520images.%2520We%2520formulate%2520shape%2520editing%2520as%2520a%250Aconditional%2520reconstruction%2520problem%252C%2520where%2520the%2520model%2520must%2520reconstruct%2520the%2520input%250Ashape%2520with%2520the%2520exception%2520of%2520a%2520specified%25203D%2520region%252C%2520in%2520which%2520the%2520geometry%2520should%250Abe%2520generated%2520from%2520the%2520conditional%2520signal.%2520To%2520this%2520end%252C%2520we%2520train%2520a%2520conditional%250ALarge%2520Reconstruction%2520Model%2520%2528LRM%2529%2520for%2520masked%2520reconstruction%252C%2520using%2520multi-view%250Aconsistent%2520masks%2520rendered%2520from%2520a%2520randomly%2520generated%25203D%2520occlusion%252C%2520and%2520using%2520one%250Aclean%2520viewpoint%2520as%2520the%2520conditional%2520signal.%2520During%2520inference%252C%2520we%2520manually%2520define%250Aa%25203D%2520region%2520to%2520edit%2520and%2520provide%2520an%2520edited%2520image%2520from%2520a%2520canonical%2520viewpoint%2520to%250Afill%2520that%2520region.%2520We%2520demonstrate%2520that%252C%2520in%2520just%2520a%2520single%2520forward%2520pass%252C%2520our%250Amethod%2520not%2520only%2520preserves%2520the%2520input%2520geometry%2520in%2520the%2520unmasked%2520region%2520through%250Areconstruction%2520capabilities%2520on%2520par%2520with%2520SoTA%252C%2520but%2520is%2520also%2520expressive%2520enough%2520to%250Aperform%2520a%2520variety%2520of%2520mesh%2520edits%2520from%2520a%2520single%2520image%2520guidance%2520that%2520past%2520works%250Astruggle%2520with%252C%2520while%2520being%25202-10x%2520faster%2520than%2520the%2520top-performing%2520prior%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08641v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Mesh%20Editing%20using%20Masked%20LRMs&entry.906535625=Will%20Gao%20and%20Dilin%20Wang%20and%20Yuchen%20Fan%20and%20Aljaz%20Bozic%20and%20Tuur%20Stuyck%20and%20Zhengqin%20Li%20and%20Zhao%20Dong%20and%20Rakesh%20Ranjan%20and%20Nikolaos%20Sarafianos&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20to%20shape%20editing%2C%20building%20on%20recent%20progress%20in%0A3D%20reconstruction%20from%20multi-view%20images.%20We%20formulate%20shape%20editing%20as%20a%0Aconditional%20reconstruction%20problem%2C%20where%20the%20model%20must%20reconstruct%20the%20input%0Ashape%20with%20the%20exception%20of%20a%20specified%203D%20region%2C%20in%20which%20the%20geometry%20should%0Abe%20generated%20from%20the%20conditional%20signal.%20To%20this%20end%2C%20we%20train%20a%20conditional%0ALarge%20Reconstruction%20Model%20%28LRM%29%20for%20masked%20reconstruction%2C%20using%20multi-view%0Aconsistent%20masks%20rendered%20from%20a%20randomly%20generated%203D%20occlusion%2C%20and%20using%20one%0Aclean%20viewpoint%20as%20the%20conditional%20signal.%20During%20inference%2C%20we%20manually%20define%0Aa%203D%20region%20to%20edit%20and%20provide%20an%20edited%20image%20from%20a%20canonical%20viewpoint%20to%0Afill%20that%20region.%20We%20demonstrate%20that%2C%20in%20just%20a%20single%20forward%20pass%2C%20our%0Amethod%20not%20only%20preserves%20the%20input%20geometry%20in%20the%20unmasked%20region%20through%0Areconstruction%20capabilities%20on%20par%20with%20SoTA%2C%20but%20is%20also%20expressive%20enough%20to%0Aperform%20a%20variety%20of%20mesh%20edits%20from%20a%20single%20image%20guidance%20that%20past%20works%0Astruggle%20with%2C%20while%20being%202-10x%20faster%20than%20the%20top-performing%20prior%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08641v2&entry.124074799=Read"},
{"title": "RAM++: Robust Representation Learning via Adaptive Mask for All-in-One\n  Image Restoration", "author": "Zilong Zhang and Chujie Qin and Chunle Guo and Yong Zhang and Chao Xue and Ming-Ming Cheng and Chongyi Li", "abstract": "  This work presents Robust Representation Learning via Adaptive Mask (RAM++),\na two-stage framework for all-in-one image restoration. RAM++ integrates\nhigh-level semantic understanding with low-level texture generation to achieve\ncontent-oriented robust restoration. It addresses the limitations of existing\ndegradation-oriented methods in extreme scenarios (e.g., degradations strongly\ncoupled with image structures). RAM++ also mitigates common challenges such as\nunbalanced performance across tasks, overfitting to seen degradations, and weak\ngeneralization to unseen ones through three key designs: 1) Adaptive\nSemantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level\nmasks to semantically rich and textured regions. This design enables the\nnetwork to learn both generative priors and image content priors from various\ndegradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning\nstrategy that adjusts the layers with higher contributions to bridge the\nintegrity gap between masked pretraining and full-image fine-tuning while\nretaining learned priors. 3) Robust Feature Regularization (RFR): a strategy\nthat leverages DINOv2's semantically consistent and degradation-invariant\nrepresentations, together with efficient feature fusion, to achieve faithful\nand semantically coherent restoration. With these designs, RAM++ achieves\nrobust, well-balanced, and state-of-the-art performance across seen, unseen,\nextreme, and mixed degradations. Our code and model will be released at\nhttps://github.com/DragonisCV/RAM\n", "link": "http://arxiv.org/abs/2509.12039v1", "date": "2025-09-15", "relevancy": 2.7727, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5628}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5533}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAM%2B%2B%3A%20Robust%20Representation%20Learning%20via%20Adaptive%20Mask%20for%20All-in-One%0A%20%20Image%20Restoration&body=Title%3A%20RAM%2B%2B%3A%20Robust%20Representation%20Learning%20via%20Adaptive%20Mask%20for%20All-in-One%0A%20%20Image%20Restoration%0AAuthor%3A%20Zilong%20Zhang%20and%20Chujie%20Qin%20and%20Chunle%20Guo%20and%20Yong%20Zhang%20and%20Chao%20Xue%20and%20Ming-Ming%20Cheng%20and%20Chongyi%20Li%0AAbstract%3A%20%20%20This%20work%20presents%20Robust%20Representation%20Learning%20via%20Adaptive%20Mask%20%28RAM%2B%2B%29%2C%0Aa%20two-stage%20framework%20for%20all-in-one%20image%20restoration.%20RAM%2B%2B%20integrates%0Ahigh-level%20semantic%20understanding%20with%20low-level%20texture%20generation%20to%20achieve%0Acontent-oriented%20robust%20restoration.%20It%20addresses%20the%20limitations%20of%20existing%0Adegradation-oriented%20methods%20in%20extreme%20scenarios%20%28e.g.%2C%20degradations%20strongly%0Acoupled%20with%20image%20structures%29.%20RAM%2B%2B%20also%20mitigates%20common%20challenges%20such%20as%0Aunbalanced%20performance%20across%20tasks%2C%20overfitting%20to%20seen%20degradations%2C%20and%20weak%0Ageneralization%20to%20unseen%20ones%20through%20three%20key%20designs%3A%201%29%20Adaptive%0ASemantic-Aware%20Mask%20%28AdaSAM%29%3A%20a%20pretraining%20strategy%20that%20applies%20pixel-level%0Amasks%20to%20semantically%20rich%20and%20textured%20regions.%20This%20design%20enables%20the%0Anetwork%20to%20learn%20both%20generative%20priors%20and%20image%20content%20priors%20from%20various%0Adegradations.%202%29%20Mask%20Attribute%20Conductance%20%28MAC%29%3A%20a%20selective%20fine-tuning%0Astrategy%20that%20adjusts%20the%20layers%20with%20higher%20contributions%20to%20bridge%20the%0Aintegrity%20gap%20between%20masked%20pretraining%20and%20full-image%20fine-tuning%20while%0Aretaining%20learned%20priors.%203%29%20Robust%20Feature%20Regularization%20%28RFR%29%3A%20a%20strategy%0Athat%20leverages%20DINOv2%27s%20semantically%20consistent%20and%20degradation-invariant%0Arepresentations%2C%20together%20with%20efficient%20feature%20fusion%2C%20to%20achieve%20faithful%0Aand%20semantically%20coherent%20restoration.%20With%20these%20designs%2C%20RAM%2B%2B%20achieves%0Arobust%2C%20well-balanced%2C%20and%20state-of-the-art%20performance%20across%20seen%2C%20unseen%2C%0Aextreme%2C%20and%20mixed%20degradations.%20Our%20code%20and%20model%20will%20be%20released%20at%0Ahttps%3A//github.com/DragonisCV/RAM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAM%252B%252B%253A%2520Robust%2520Representation%2520Learning%2520via%2520Adaptive%2520Mask%2520for%2520All-in-One%250A%2520%2520Image%2520Restoration%26entry.906535625%3DZilong%2520Zhang%2520and%2520Chujie%2520Qin%2520and%2520Chunle%2520Guo%2520and%2520Yong%2520Zhang%2520and%2520Chao%2520Xue%2520and%2520Ming-Ming%2520Cheng%2520and%2520Chongyi%2520Li%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520Robust%2520Representation%2520Learning%2520via%2520Adaptive%2520Mask%2520%2528RAM%252B%252B%2529%252C%250Aa%2520two-stage%2520framework%2520for%2520all-in-one%2520image%2520restoration.%2520RAM%252B%252B%2520integrates%250Ahigh-level%2520semantic%2520understanding%2520with%2520low-level%2520texture%2520generation%2520to%2520achieve%250Acontent-oriented%2520robust%2520restoration.%2520It%2520addresses%2520the%2520limitations%2520of%2520existing%250Adegradation-oriented%2520methods%2520in%2520extreme%2520scenarios%2520%2528e.g.%252C%2520degradations%2520strongly%250Acoupled%2520with%2520image%2520structures%2529.%2520RAM%252B%252B%2520also%2520mitigates%2520common%2520challenges%2520such%2520as%250Aunbalanced%2520performance%2520across%2520tasks%252C%2520overfitting%2520to%2520seen%2520degradations%252C%2520and%2520weak%250Ageneralization%2520to%2520unseen%2520ones%2520through%2520three%2520key%2520designs%253A%25201%2529%2520Adaptive%250ASemantic-Aware%2520Mask%2520%2528AdaSAM%2529%253A%2520a%2520pretraining%2520strategy%2520that%2520applies%2520pixel-level%250Amasks%2520to%2520semantically%2520rich%2520and%2520textured%2520regions.%2520This%2520design%2520enables%2520the%250Anetwork%2520to%2520learn%2520both%2520generative%2520priors%2520and%2520image%2520content%2520priors%2520from%2520various%250Adegradations.%25202%2529%2520Mask%2520Attribute%2520Conductance%2520%2528MAC%2529%253A%2520a%2520selective%2520fine-tuning%250Astrategy%2520that%2520adjusts%2520the%2520layers%2520with%2520higher%2520contributions%2520to%2520bridge%2520the%250Aintegrity%2520gap%2520between%2520masked%2520pretraining%2520and%2520full-image%2520fine-tuning%2520while%250Aretaining%2520learned%2520priors.%25203%2529%2520Robust%2520Feature%2520Regularization%2520%2528RFR%2529%253A%2520a%2520strategy%250Athat%2520leverages%2520DINOv2%2527s%2520semantically%2520consistent%2520and%2520degradation-invariant%250Arepresentations%252C%2520together%2520with%2520efficient%2520feature%2520fusion%252C%2520to%2520achieve%2520faithful%250Aand%2520semantically%2520coherent%2520restoration.%2520With%2520these%2520designs%252C%2520RAM%252B%252B%2520achieves%250Arobust%252C%2520well-balanced%252C%2520and%2520state-of-the-art%2520performance%2520across%2520seen%252C%2520unseen%252C%250Aextreme%252C%2520and%2520mixed%2520degradations.%2520Our%2520code%2520and%2520model%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/DragonisCV/RAM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAM%2B%2B%3A%20Robust%20Representation%20Learning%20via%20Adaptive%20Mask%20for%20All-in-One%0A%20%20Image%20Restoration&entry.906535625=Zilong%20Zhang%20and%20Chujie%20Qin%20and%20Chunle%20Guo%20and%20Yong%20Zhang%20and%20Chao%20Xue%20and%20Ming-Ming%20Cheng%20and%20Chongyi%20Li&entry.1292438233=%20%20This%20work%20presents%20Robust%20Representation%20Learning%20via%20Adaptive%20Mask%20%28RAM%2B%2B%29%2C%0Aa%20two-stage%20framework%20for%20all-in-one%20image%20restoration.%20RAM%2B%2B%20integrates%0Ahigh-level%20semantic%20understanding%20with%20low-level%20texture%20generation%20to%20achieve%0Acontent-oriented%20robust%20restoration.%20It%20addresses%20the%20limitations%20of%20existing%0Adegradation-oriented%20methods%20in%20extreme%20scenarios%20%28e.g.%2C%20degradations%20strongly%0Acoupled%20with%20image%20structures%29.%20RAM%2B%2B%20also%20mitigates%20common%20challenges%20such%20as%0Aunbalanced%20performance%20across%20tasks%2C%20overfitting%20to%20seen%20degradations%2C%20and%20weak%0Ageneralization%20to%20unseen%20ones%20through%20three%20key%20designs%3A%201%29%20Adaptive%0ASemantic-Aware%20Mask%20%28AdaSAM%29%3A%20a%20pretraining%20strategy%20that%20applies%20pixel-level%0Amasks%20to%20semantically%20rich%20and%20textured%20regions.%20This%20design%20enables%20the%0Anetwork%20to%20learn%20both%20generative%20priors%20and%20image%20content%20priors%20from%20various%0Adegradations.%202%29%20Mask%20Attribute%20Conductance%20%28MAC%29%3A%20a%20selective%20fine-tuning%0Astrategy%20that%20adjusts%20the%20layers%20with%20higher%20contributions%20to%20bridge%20the%0Aintegrity%20gap%20between%20masked%20pretraining%20and%20full-image%20fine-tuning%20while%0Aretaining%20learned%20priors.%203%29%20Robust%20Feature%20Regularization%20%28RFR%29%3A%20a%20strategy%0Athat%20leverages%20DINOv2%27s%20semantically%20consistent%20and%20degradation-invariant%0Arepresentations%2C%20together%20with%20efficient%20feature%20fusion%2C%20to%20achieve%20faithful%0Aand%20semantically%20coherent%20restoration.%20With%20these%20designs%2C%20RAM%2B%2B%20achieves%0Arobust%2C%20well-balanced%2C%20and%20state-of-the-art%20performance%20across%20seen%2C%20unseen%2C%0Aextreme%2C%20and%20mixed%20degradations.%20Our%20code%20and%20model%20will%20be%20released%20at%0Ahttps%3A//github.com/DragonisCV/RAM%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12039v1&entry.124074799=Read"},
{"title": "Social Perception of Faces in a Vision-Language Model", "author": "Carina I. Hausladen and Manuel Knott and Colin F. Camerer and Pietro Perona", "abstract": "  We explore social perception of human faces in CLIP, a widely used\nopen-source vision-language model. To this end, we compare the similarity in\nCLIP embeddings between different textual prompts and a set of face images. Our\ntextual prompts are constructed from well-validated social psychology terms\ndenoting social perception. The face images are synthetic and are\nsystematically and independently varied along six dimensions: the legally\nprotected attributes of age, gender, and race, as well as facial expression,\nlighting, and pose. Independently and systematically manipulating face\nattributes allows us to study the effect of each on social perception and\navoids confounds that can occur in wild-collected data due to uncontrolled\nsystematic correlations between attributes. Thus, our findings are experimental\nrather than observational. Our main findings are three. First, while CLIP is\ntrained on the widest variety of images and texts, it is able to make\nfine-grained human-like social judgments on face images. Second, age, gender,\nand race do systematically impact CLIP's social perception of faces, suggesting\nan undesirable bias in CLIP vis-a-vis legally protected attributes. Most\nstrikingly, we find a strong pattern of bias concerning the faces of Black\nwomen, where CLIP produces extreme values of social perception across different\nages and facial expressions. Third, facial expression impacts social perception\nmore than age and lighting as much as age. The last finding predicts that\nstudies that do not control for unprotected visual attributes may reach the\nwrong conclusions on bias. Our novel method of investigation, which is founded\non the social psychology literature and on the experiments involving the\nmanipulation of individual attributes, yields sharper and more reliable\nobservations than previous observational methods and may be applied to study\nbiases in any vision-language model.\n", "link": "http://arxiv.org/abs/2408.14435v2", "date": "2025-09-15", "relevancy": 2.7393, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Social%20Perception%20of%20Faces%20in%20a%20Vision-Language%20Model&body=Title%3A%20Social%20Perception%20of%20Faces%20in%20a%20Vision-Language%20Model%0AAuthor%3A%20Carina%20I.%20Hausladen%20and%20Manuel%20Knott%20and%20Colin%20F.%20Camerer%20and%20Pietro%20Perona%0AAbstract%3A%20%20%20We%20explore%20social%20perception%20of%20human%20faces%20in%20CLIP%2C%20a%20widely%20used%0Aopen-source%20vision-language%20model.%20To%20this%20end%2C%20we%20compare%20the%20similarity%20in%0ACLIP%20embeddings%20between%20different%20textual%20prompts%20and%20a%20set%20of%20face%20images.%20Our%0Atextual%20prompts%20are%20constructed%20from%20well-validated%20social%20psychology%20terms%0Adenoting%20social%20perception.%20The%20face%20images%20are%20synthetic%20and%20are%0Asystematically%20and%20independently%20varied%20along%20six%20dimensions%3A%20the%20legally%0Aprotected%20attributes%20of%20age%2C%20gender%2C%20and%20race%2C%20as%20well%20as%20facial%20expression%2C%0Alighting%2C%20and%20pose.%20Independently%20and%20systematically%20manipulating%20face%0Aattributes%20allows%20us%20to%20study%20the%20effect%20of%20each%20on%20social%20perception%20and%0Aavoids%20confounds%20that%20can%20occur%20in%20wild-collected%20data%20due%20to%20uncontrolled%0Asystematic%20correlations%20between%20attributes.%20Thus%2C%20our%20findings%20are%20experimental%0Arather%20than%20observational.%20Our%20main%20findings%20are%20three.%20First%2C%20while%20CLIP%20is%0Atrained%20on%20the%20widest%20variety%20of%20images%20and%20texts%2C%20it%20is%20able%20to%20make%0Afine-grained%20human-like%20social%20judgments%20on%20face%20images.%20Second%2C%20age%2C%20gender%2C%0Aand%20race%20do%20systematically%20impact%20CLIP%27s%20social%20perception%20of%20faces%2C%20suggesting%0Aan%20undesirable%20bias%20in%20CLIP%20vis-a-vis%20legally%20protected%20attributes.%20Most%0Astrikingly%2C%20we%20find%20a%20strong%20pattern%20of%20bias%20concerning%20the%20faces%20of%20Black%0Awomen%2C%20where%20CLIP%20produces%20extreme%20values%20of%20social%20perception%20across%20different%0Aages%20and%20facial%20expressions.%20Third%2C%20facial%20expression%20impacts%20social%20perception%0Amore%20than%20age%20and%20lighting%20as%20much%20as%20age.%20The%20last%20finding%20predicts%20that%0Astudies%20that%20do%20not%20control%20for%20unprotected%20visual%20attributes%20may%20reach%20the%0Awrong%20conclusions%20on%20bias.%20Our%20novel%20method%20of%20investigation%2C%20which%20is%20founded%0Aon%20the%20social%20psychology%20literature%20and%20on%20the%20experiments%20involving%20the%0Amanipulation%20of%20individual%20attributes%2C%20yields%20sharper%20and%20more%20reliable%0Aobservations%20than%20previous%20observational%20methods%20and%20may%20be%20applied%20to%20study%0Abiases%20in%20any%20vision-language%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14435v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSocial%2520Perception%2520of%2520Faces%2520in%2520a%2520Vision-Language%2520Model%26entry.906535625%3DCarina%2520I.%2520Hausladen%2520and%2520Manuel%2520Knott%2520and%2520Colin%2520F.%2520Camerer%2520and%2520Pietro%2520Perona%26entry.1292438233%3D%2520%2520We%2520explore%2520social%2520perception%2520of%2520human%2520faces%2520in%2520CLIP%252C%2520a%2520widely%2520used%250Aopen-source%2520vision-language%2520model.%2520To%2520this%2520end%252C%2520we%2520compare%2520the%2520similarity%2520in%250ACLIP%2520embeddings%2520between%2520different%2520textual%2520prompts%2520and%2520a%2520set%2520of%2520face%2520images.%2520Our%250Atextual%2520prompts%2520are%2520constructed%2520from%2520well-validated%2520social%2520psychology%2520terms%250Adenoting%2520social%2520perception.%2520The%2520face%2520images%2520are%2520synthetic%2520and%2520are%250Asystematically%2520and%2520independently%2520varied%2520along%2520six%2520dimensions%253A%2520the%2520legally%250Aprotected%2520attributes%2520of%2520age%252C%2520gender%252C%2520and%2520race%252C%2520as%2520well%2520as%2520facial%2520expression%252C%250Alighting%252C%2520and%2520pose.%2520Independently%2520and%2520systematically%2520manipulating%2520face%250Aattributes%2520allows%2520us%2520to%2520study%2520the%2520effect%2520of%2520each%2520on%2520social%2520perception%2520and%250Aavoids%2520confounds%2520that%2520can%2520occur%2520in%2520wild-collected%2520data%2520due%2520to%2520uncontrolled%250Asystematic%2520correlations%2520between%2520attributes.%2520Thus%252C%2520our%2520findings%2520are%2520experimental%250Arather%2520than%2520observational.%2520Our%2520main%2520findings%2520are%2520three.%2520First%252C%2520while%2520CLIP%2520is%250Atrained%2520on%2520the%2520widest%2520variety%2520of%2520images%2520and%2520texts%252C%2520it%2520is%2520able%2520to%2520make%250Afine-grained%2520human-like%2520social%2520judgments%2520on%2520face%2520images.%2520Second%252C%2520age%252C%2520gender%252C%250Aand%2520race%2520do%2520systematically%2520impact%2520CLIP%2527s%2520social%2520perception%2520of%2520faces%252C%2520suggesting%250Aan%2520undesirable%2520bias%2520in%2520CLIP%2520vis-a-vis%2520legally%2520protected%2520attributes.%2520Most%250Astrikingly%252C%2520we%2520find%2520a%2520strong%2520pattern%2520of%2520bias%2520concerning%2520the%2520faces%2520of%2520Black%250Awomen%252C%2520where%2520CLIP%2520produces%2520extreme%2520values%2520of%2520social%2520perception%2520across%2520different%250Aages%2520and%2520facial%2520expressions.%2520Third%252C%2520facial%2520expression%2520impacts%2520social%2520perception%250Amore%2520than%2520age%2520and%2520lighting%2520as%2520much%2520as%2520age.%2520The%2520last%2520finding%2520predicts%2520that%250Astudies%2520that%2520do%2520not%2520control%2520for%2520unprotected%2520visual%2520attributes%2520may%2520reach%2520the%250Awrong%2520conclusions%2520on%2520bias.%2520Our%2520novel%2520method%2520of%2520investigation%252C%2520which%2520is%2520founded%250Aon%2520the%2520social%2520psychology%2520literature%2520and%2520on%2520the%2520experiments%2520involving%2520the%250Amanipulation%2520of%2520individual%2520attributes%252C%2520yields%2520sharper%2520and%2520more%2520reliable%250Aobservations%2520than%2520previous%2520observational%2520methods%2520and%2520may%2520be%2520applied%2520to%2520study%250Abiases%2520in%2520any%2520vision-language%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14435v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Social%20Perception%20of%20Faces%20in%20a%20Vision-Language%20Model&entry.906535625=Carina%20I.%20Hausladen%20and%20Manuel%20Knott%20and%20Colin%20F.%20Camerer%20and%20Pietro%20Perona&entry.1292438233=%20%20We%20explore%20social%20perception%20of%20human%20faces%20in%20CLIP%2C%20a%20widely%20used%0Aopen-source%20vision-language%20model.%20To%20this%20end%2C%20we%20compare%20the%20similarity%20in%0ACLIP%20embeddings%20between%20different%20textual%20prompts%20and%20a%20set%20of%20face%20images.%20Our%0Atextual%20prompts%20are%20constructed%20from%20well-validated%20social%20psychology%20terms%0Adenoting%20social%20perception.%20The%20face%20images%20are%20synthetic%20and%20are%0Asystematically%20and%20independently%20varied%20along%20six%20dimensions%3A%20the%20legally%0Aprotected%20attributes%20of%20age%2C%20gender%2C%20and%20race%2C%20as%20well%20as%20facial%20expression%2C%0Alighting%2C%20and%20pose.%20Independently%20and%20systematically%20manipulating%20face%0Aattributes%20allows%20us%20to%20study%20the%20effect%20of%20each%20on%20social%20perception%20and%0Aavoids%20confounds%20that%20can%20occur%20in%20wild-collected%20data%20due%20to%20uncontrolled%0Asystematic%20correlations%20between%20attributes.%20Thus%2C%20our%20findings%20are%20experimental%0Arather%20than%20observational.%20Our%20main%20findings%20are%20three.%20First%2C%20while%20CLIP%20is%0Atrained%20on%20the%20widest%20variety%20of%20images%20and%20texts%2C%20it%20is%20able%20to%20make%0Afine-grained%20human-like%20social%20judgments%20on%20face%20images.%20Second%2C%20age%2C%20gender%2C%0Aand%20race%20do%20systematically%20impact%20CLIP%27s%20social%20perception%20of%20faces%2C%20suggesting%0Aan%20undesirable%20bias%20in%20CLIP%20vis-a-vis%20legally%20protected%20attributes.%20Most%0Astrikingly%2C%20we%20find%20a%20strong%20pattern%20of%20bias%20concerning%20the%20faces%20of%20Black%0Awomen%2C%20where%20CLIP%20produces%20extreme%20values%20of%20social%20perception%20across%20different%0Aages%20and%20facial%20expressions.%20Third%2C%20facial%20expression%20impacts%20social%20perception%0Amore%20than%20age%20and%20lighting%20as%20much%20as%20age.%20The%20last%20finding%20predicts%20that%0Astudies%20that%20do%20not%20control%20for%20unprotected%20visual%20attributes%20may%20reach%20the%0Awrong%20conclusions%20on%20bias.%20Our%20novel%20method%20of%20investigation%2C%20which%20is%20founded%0Aon%20the%20social%20psychology%20literature%20and%20on%20the%20experiments%20involving%20the%0Amanipulation%20of%20individual%20attributes%2C%20yields%20sharper%20and%20more%20reliable%0Aobservations%20than%20previous%20observational%20methods%20and%20may%20be%20applied%20to%20study%0Abiases%20in%20any%20vision-language%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14435v2&entry.124074799=Read"},
{"title": "Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language\n  Models", "author": "Pu Jian and Junhong Wu and Wei Sun and Chen Wang and Shuo Ren and Jiajun Zhang", "abstract": "  Recent advances in text-only \"slow-thinking\" reasoning have prompted efforts\nto transfer this capability to vision-language models (VLMs), for training\nvisual reasoning models (\\textbf{VRMs}). owever, such transfer faces critical\nchallenges: Effective \"slow thinking\" in VRMs requires \\textbf{visual\nreflection}, the ability to check the reasoning process based on visual\ninformation. Through quantitative analysis, we observe that current VRMs\nexhibit limited visual reflection, as their attention to visual information\ndiminishes rapidly with longer generated responses. To address this challenge,\nwe propose a new VRM \\textbf{Reflection-V}, which enhances visual reflection\nbased on reasoning data construction for cold-start and reward design for\nreinforcement learning (RL). Firstly, we construct vision-centered reasoning\ndata by leveraging an agent that interacts between VLMs and reasoning LLMs,\nenabling cold-start learning of visual reflection patterns. Secondly, a visual\nattention based reward model is employed during RL to encourage reasoning based\non visual information. Therefore, \\textbf{Reflection-V} demonstrates\nsignificant improvements across multiple visual reasoning benchmarks.\nFurthermore, \\textbf{Reflection-V} maintains a stronger and more consistent\nreliance on visual information during visual reasoning, indicating effective\nenhancement in visual reflection capabilities.\n", "link": "http://arxiv.org/abs/2509.12132v1", "date": "2025-09-15", "relevancy": 2.7301, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5621}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Look%20Again%2C%20Think%20Slowly%3A%20Enhancing%20Visual%20Reflection%20in%20Vision-Language%0A%20%20Models&body=Title%3A%20Look%20Again%2C%20Think%20Slowly%3A%20Enhancing%20Visual%20Reflection%20in%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Pu%20Jian%20and%20Junhong%20Wu%20and%20Wei%20Sun%20and%20Chen%20Wang%20and%20Shuo%20Ren%20and%20Jiajun%20Zhang%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-only%20%22slow-thinking%22%20reasoning%20have%20prompted%20efforts%0Ato%20transfer%20this%20capability%20to%20vision-language%20models%20%28VLMs%29%2C%20for%20training%0Avisual%20reasoning%20models%20%28%5Ctextbf%7BVRMs%7D%29.%20owever%2C%20such%20transfer%20faces%20critical%0Achallenges%3A%20Effective%20%22slow%20thinking%22%20in%20VRMs%20requires%20%5Ctextbf%7Bvisual%0Areflection%7D%2C%20the%20ability%20to%20check%20the%20reasoning%20process%20based%20on%20visual%0Ainformation.%20Through%20quantitative%20analysis%2C%20we%20observe%20that%20current%20VRMs%0Aexhibit%20limited%20visual%20reflection%2C%20as%20their%20attention%20to%20visual%20information%0Adiminishes%20rapidly%20with%20longer%20generated%20responses.%20To%20address%20this%20challenge%2C%0Awe%20propose%20a%20new%20VRM%20%5Ctextbf%7BReflection-V%7D%2C%20which%20enhances%20visual%20reflection%0Abased%20on%20reasoning%20data%20construction%20for%20cold-start%20and%20reward%20design%20for%0Areinforcement%20learning%20%28RL%29.%20Firstly%2C%20we%20construct%20vision-centered%20reasoning%0Adata%20by%20leveraging%20an%20agent%20that%20interacts%20between%20VLMs%20and%20reasoning%20LLMs%2C%0Aenabling%20cold-start%20learning%20of%20visual%20reflection%20patterns.%20Secondly%2C%20a%20visual%0Aattention%20based%20reward%20model%20is%20employed%20during%20RL%20to%20encourage%20reasoning%20based%0Aon%20visual%20information.%20Therefore%2C%20%5Ctextbf%7BReflection-V%7D%20demonstrates%0Asignificant%20improvements%20across%20multiple%20visual%20reasoning%20benchmarks.%0AFurthermore%2C%20%5Ctextbf%7BReflection-V%7D%20maintains%20a%20stronger%20and%20more%20consistent%0Areliance%20on%20visual%20information%20during%20visual%20reasoning%2C%20indicating%20effective%0Aenhancement%20in%20visual%20reflection%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12132v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLook%2520Again%252C%2520Think%2520Slowly%253A%2520Enhancing%2520Visual%2520Reflection%2520in%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DPu%2520Jian%2520and%2520Junhong%2520Wu%2520and%2520Wei%2520Sun%2520and%2520Chen%2520Wang%2520and%2520Shuo%2520Ren%2520and%2520Jiajun%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-only%2520%2522slow-thinking%2522%2520reasoning%2520have%2520prompted%2520efforts%250Ato%2520transfer%2520this%2520capability%2520to%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520for%2520training%250Avisual%2520reasoning%2520models%2520%2528%255Ctextbf%257BVRMs%257D%2529.%2520owever%252C%2520such%2520transfer%2520faces%2520critical%250Achallenges%253A%2520Effective%2520%2522slow%2520thinking%2522%2520in%2520VRMs%2520requires%2520%255Ctextbf%257Bvisual%250Areflection%257D%252C%2520the%2520ability%2520to%2520check%2520the%2520reasoning%2520process%2520based%2520on%2520visual%250Ainformation.%2520Through%2520quantitative%2520analysis%252C%2520we%2520observe%2520that%2520current%2520VRMs%250Aexhibit%2520limited%2520visual%2520reflection%252C%2520as%2520their%2520attention%2520to%2520visual%2520information%250Adiminishes%2520rapidly%2520with%2520longer%2520generated%2520responses.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520propose%2520a%2520new%2520VRM%2520%255Ctextbf%257BReflection-V%257D%252C%2520which%2520enhances%2520visual%2520reflection%250Abased%2520on%2520reasoning%2520data%2520construction%2520for%2520cold-start%2520and%2520reward%2520design%2520for%250Areinforcement%2520learning%2520%2528RL%2529.%2520Firstly%252C%2520we%2520construct%2520vision-centered%2520reasoning%250Adata%2520by%2520leveraging%2520an%2520agent%2520that%2520interacts%2520between%2520VLMs%2520and%2520reasoning%2520LLMs%252C%250Aenabling%2520cold-start%2520learning%2520of%2520visual%2520reflection%2520patterns.%2520Secondly%252C%2520a%2520visual%250Aattention%2520based%2520reward%2520model%2520is%2520employed%2520during%2520RL%2520to%2520encourage%2520reasoning%2520based%250Aon%2520visual%2520information.%2520Therefore%252C%2520%255Ctextbf%257BReflection-V%257D%2520demonstrates%250Asignificant%2520improvements%2520across%2520multiple%2520visual%2520reasoning%2520benchmarks.%250AFurthermore%252C%2520%255Ctextbf%257BReflection-V%257D%2520maintains%2520a%2520stronger%2520and%2520more%2520consistent%250Areliance%2520on%2520visual%2520information%2520during%2520visual%2520reasoning%252C%2520indicating%2520effective%250Aenhancement%2520in%2520visual%2520reflection%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12132v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look%20Again%2C%20Think%20Slowly%3A%20Enhancing%20Visual%20Reflection%20in%20Vision-Language%0A%20%20Models&entry.906535625=Pu%20Jian%20and%20Junhong%20Wu%20and%20Wei%20Sun%20and%20Chen%20Wang%20and%20Shuo%20Ren%20and%20Jiajun%20Zhang&entry.1292438233=%20%20Recent%20advances%20in%20text-only%20%22slow-thinking%22%20reasoning%20have%20prompted%20efforts%0Ato%20transfer%20this%20capability%20to%20vision-language%20models%20%28VLMs%29%2C%20for%20training%0Avisual%20reasoning%20models%20%28%5Ctextbf%7BVRMs%7D%29.%20owever%2C%20such%20transfer%20faces%20critical%0Achallenges%3A%20Effective%20%22slow%20thinking%22%20in%20VRMs%20requires%20%5Ctextbf%7Bvisual%0Areflection%7D%2C%20the%20ability%20to%20check%20the%20reasoning%20process%20based%20on%20visual%0Ainformation.%20Through%20quantitative%20analysis%2C%20we%20observe%20that%20current%20VRMs%0Aexhibit%20limited%20visual%20reflection%2C%20as%20their%20attention%20to%20visual%20information%0Adiminishes%20rapidly%20with%20longer%20generated%20responses.%20To%20address%20this%20challenge%2C%0Awe%20propose%20a%20new%20VRM%20%5Ctextbf%7BReflection-V%7D%2C%20which%20enhances%20visual%20reflection%0Abased%20on%20reasoning%20data%20construction%20for%20cold-start%20and%20reward%20design%20for%0Areinforcement%20learning%20%28RL%29.%20Firstly%2C%20we%20construct%20vision-centered%20reasoning%0Adata%20by%20leveraging%20an%20agent%20that%20interacts%20between%20VLMs%20and%20reasoning%20LLMs%2C%0Aenabling%20cold-start%20learning%20of%20visual%20reflection%20patterns.%20Secondly%2C%20a%20visual%0Aattention%20based%20reward%20model%20is%20employed%20during%20RL%20to%20encourage%20reasoning%20based%0Aon%20visual%20information.%20Therefore%2C%20%5Ctextbf%7BReflection-V%7D%20demonstrates%0Asignificant%20improvements%20across%20multiple%20visual%20reasoning%20benchmarks.%0AFurthermore%2C%20%5Ctextbf%7BReflection-V%7D%20maintains%20a%20stronger%20and%20more%20consistent%0Areliance%20on%20visual%20information%20during%20visual%20reasoning%2C%20indicating%20effective%0Aenhancement%20in%20visual%20reflection%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12132v1&entry.124074799=Read"},
{"title": "FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic\n  Segmentation via Low-Rank Adaptation", "author": "Bernardo Forni and Gabriele Lombardi and Federico Pozzi and Mirco Planamente", "abstract": "  Few-shot semantic segmentation has recently attracted great attention. The\ngoal is to develop a model capable of segmenting unseen classes using only a\nfew annotated samples. Most existing approaches adapt a pre-trained model by\ntraining from scratch an additional module. Achieving optimal performance with\nthese approaches requires extensive training on large-scale datasets. The\nSegment Anything Model 2 (SAM2) is a foundational model for zero-shot image and\nvideo segmentation with a modular design. In this paper, we propose a Few-Shot\nsegmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities\nare directly repurposed for the few-shot task. Moreover, we apply a Low-Rank\nAdaptation (LoRA) to the original modules in order to handle the diverse images\ntypically found in standard datasets, unlike the temporally connected frames\nused in SAM2's pre-training. With this approach, only a small number of\nparameters is meta-trained, which effectively adapts SAM2 while benefiting from\nits impressive segmentation performance. Our method supports any K-shot\nconfiguration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and\nFSS-1000 datasets, achieving remarkable results and demonstrating excellent\ncomputational efficiency during inference. Code is available at\nhttps://github.com/fornib/FS-SAM2\n", "link": "http://arxiv.org/abs/2509.12105v1", "date": "2025-09-15", "relevancy": 2.7188, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.576}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5384}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FS-SAM2%3A%20Adapting%20Segment%20Anything%20Model%202%20for%20Few-Shot%20Semantic%0A%20%20Segmentation%20via%20Low-Rank%20Adaptation&body=Title%3A%20FS-SAM2%3A%20Adapting%20Segment%20Anything%20Model%202%20for%20Few-Shot%20Semantic%0A%20%20Segmentation%20via%20Low-Rank%20Adaptation%0AAuthor%3A%20Bernardo%20Forni%20and%20Gabriele%20Lombardi%20and%20Federico%20Pozzi%20and%20Mirco%20Planamente%0AAbstract%3A%20%20%20Few-shot%20semantic%20segmentation%20has%20recently%20attracted%20great%20attention.%20The%0Agoal%20is%20to%20develop%20a%20model%20capable%20of%20segmenting%20unseen%20classes%20using%20only%20a%0Afew%20annotated%20samples.%20Most%20existing%20approaches%20adapt%20a%20pre-trained%20model%20by%0Atraining%20from%20scratch%20an%20additional%20module.%20Achieving%20optimal%20performance%20with%0Athese%20approaches%20requires%20extensive%20training%20on%20large-scale%20datasets.%20The%0ASegment%20Anything%20Model%202%20%28SAM2%29%20is%20a%20foundational%20model%20for%20zero-shot%20image%20and%0Avideo%20segmentation%20with%20a%20modular%20design.%20In%20this%20paper%2C%20we%20propose%20a%20Few-Shot%0Asegmentation%20method%20based%20on%20SAM2%20%28FS-SAM2%29%2C%20where%20SAM2%27s%20video%20capabilities%0Aare%20directly%20repurposed%20for%20the%20few-shot%20task.%20Moreover%2C%20we%20apply%20a%20Low-Rank%0AAdaptation%20%28LoRA%29%20to%20the%20original%20modules%20in%20order%20to%20handle%20the%20diverse%20images%0Atypically%20found%20in%20standard%20datasets%2C%20unlike%20the%20temporally%20connected%20frames%0Aused%20in%20SAM2%27s%20pre-training.%20With%20this%20approach%2C%20only%20a%20small%20number%20of%0Aparameters%20is%20meta-trained%2C%20which%20effectively%20adapts%20SAM2%20while%20benefiting%20from%0Aits%20impressive%20segmentation%20performance.%20Our%20method%20supports%20any%20K-shot%0Aconfiguration.%20We%20evaluate%20FS-SAM2%20on%20the%20PASCAL-5%24%5Ei%24%2C%20COCO-20%24%5Ei%24%20and%0AFSS-1000%20datasets%2C%20achieving%20remarkable%20results%20and%20demonstrating%20excellent%0Acomputational%20efficiency%20during%20inference.%20Code%20is%20available%20at%0Ahttps%3A//github.com/fornib/FS-SAM2%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFS-SAM2%253A%2520Adapting%2520Segment%2520Anything%2520Model%25202%2520for%2520Few-Shot%2520Semantic%250A%2520%2520Segmentation%2520via%2520Low-Rank%2520Adaptation%26entry.906535625%3DBernardo%2520Forni%2520and%2520Gabriele%2520Lombardi%2520and%2520Federico%2520Pozzi%2520and%2520Mirco%2520Planamente%26entry.1292438233%3D%2520%2520Few-shot%2520semantic%2520segmentation%2520has%2520recently%2520attracted%2520great%2520attention.%2520The%250Agoal%2520is%2520to%2520develop%2520a%2520model%2520capable%2520of%2520segmenting%2520unseen%2520classes%2520using%2520only%2520a%250Afew%2520annotated%2520samples.%2520Most%2520existing%2520approaches%2520adapt%2520a%2520pre-trained%2520model%2520by%250Atraining%2520from%2520scratch%2520an%2520additional%2520module.%2520Achieving%2520optimal%2520performance%2520with%250Athese%2520approaches%2520requires%2520extensive%2520training%2520on%2520large-scale%2520datasets.%2520The%250ASegment%2520Anything%2520Model%25202%2520%2528SAM2%2529%2520is%2520a%2520foundational%2520model%2520for%2520zero-shot%2520image%2520and%250Avideo%2520segmentation%2520with%2520a%2520modular%2520design.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Few-Shot%250Asegmentation%2520method%2520based%2520on%2520SAM2%2520%2528FS-SAM2%2529%252C%2520where%2520SAM2%2527s%2520video%2520capabilities%250Aare%2520directly%2520repurposed%2520for%2520the%2520few-shot%2520task.%2520Moreover%252C%2520we%2520apply%2520a%2520Low-Rank%250AAdaptation%2520%2528LoRA%2529%2520to%2520the%2520original%2520modules%2520in%2520order%2520to%2520handle%2520the%2520diverse%2520images%250Atypically%2520found%2520in%2520standard%2520datasets%252C%2520unlike%2520the%2520temporally%2520connected%2520frames%250Aused%2520in%2520SAM2%2527s%2520pre-training.%2520With%2520this%2520approach%252C%2520only%2520a%2520small%2520number%2520of%250Aparameters%2520is%2520meta-trained%252C%2520which%2520effectively%2520adapts%2520SAM2%2520while%2520benefiting%2520from%250Aits%2520impressive%2520segmentation%2520performance.%2520Our%2520method%2520supports%2520any%2520K-shot%250Aconfiguration.%2520We%2520evaluate%2520FS-SAM2%2520on%2520the%2520PASCAL-5%2524%255Ei%2524%252C%2520COCO-20%2524%255Ei%2524%2520and%250AFSS-1000%2520datasets%252C%2520achieving%2520remarkable%2520results%2520and%2520demonstrating%2520excellent%250Acomputational%2520efficiency%2520during%2520inference.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/fornib/FS-SAM2%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FS-SAM2%3A%20Adapting%20Segment%20Anything%20Model%202%20for%20Few-Shot%20Semantic%0A%20%20Segmentation%20via%20Low-Rank%20Adaptation&entry.906535625=Bernardo%20Forni%20and%20Gabriele%20Lombardi%20and%20Federico%20Pozzi%20and%20Mirco%20Planamente&entry.1292438233=%20%20Few-shot%20semantic%20segmentation%20has%20recently%20attracted%20great%20attention.%20The%0Agoal%20is%20to%20develop%20a%20model%20capable%20of%20segmenting%20unseen%20classes%20using%20only%20a%0Afew%20annotated%20samples.%20Most%20existing%20approaches%20adapt%20a%20pre-trained%20model%20by%0Atraining%20from%20scratch%20an%20additional%20module.%20Achieving%20optimal%20performance%20with%0Athese%20approaches%20requires%20extensive%20training%20on%20large-scale%20datasets.%20The%0ASegment%20Anything%20Model%202%20%28SAM2%29%20is%20a%20foundational%20model%20for%20zero-shot%20image%20and%0Avideo%20segmentation%20with%20a%20modular%20design.%20In%20this%20paper%2C%20we%20propose%20a%20Few-Shot%0Asegmentation%20method%20based%20on%20SAM2%20%28FS-SAM2%29%2C%20where%20SAM2%27s%20video%20capabilities%0Aare%20directly%20repurposed%20for%20the%20few-shot%20task.%20Moreover%2C%20we%20apply%20a%20Low-Rank%0AAdaptation%20%28LoRA%29%20to%20the%20original%20modules%20in%20order%20to%20handle%20the%20diverse%20images%0Atypically%20found%20in%20standard%20datasets%2C%20unlike%20the%20temporally%20connected%20frames%0Aused%20in%20SAM2%27s%20pre-training.%20With%20this%20approach%2C%20only%20a%20small%20number%20of%0Aparameters%20is%20meta-trained%2C%20which%20effectively%20adapts%20SAM2%20while%20benefiting%20from%0Aits%20impressive%20segmentation%20performance.%20Our%20method%20supports%20any%20K-shot%0Aconfiguration.%20We%20evaluate%20FS-SAM2%20on%20the%20PASCAL-5%24%5Ei%24%2C%20COCO-20%24%5Ei%24%20and%0AFSS-1000%20datasets%2C%20achieving%20remarkable%20results%20and%20demonstrating%20excellent%0Acomputational%20efficiency%20during%20inference.%20Code%20is%20available%20at%0Ahttps%3A//github.com/fornib/FS-SAM2%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12105v1&entry.124074799=Read"},
{"title": "End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac\n  MRI", "author": "Yihong Chen and Jiancheng Yang and Deniz Sayin Mercadier and Hieu Le and Juerg Schwitter and Pascal Fua", "abstract": "  Reconstructing cardiac motion from cine CMR sequences is critical for\ndiagnosis, prediction, and intervention. Existing methods rely on complete CMR\nstacks to infer full heart motion, limiting their utility in intra-procedural\nscenarios where only sparse observations are available. We present TetHeart,\nthe first end-to-end framework that unifies full 4D multi-structure heart mesh\nrecovery from both offline full-stack acquisitions and intra-procedural\nsparse-slice observations. Our method leverages deep deformable tetrahedra, an\nexplicit-implicit hybrid representation, to capture shape and motion in a\ncoherent space shared across cardiac structures. It is initialized from\nhigh-quality pre-procedural or offline-acquired full stacks to build detailed,\npatient-specific heart meshes, which can then be updated using whatever slices\nare available, from full stacks down to a single slice. We further incorporate\nseveral key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D\nfeature assembly that dynamically integrates information from arbitrary numbers\nof slices at any position, combined with a distillation strategy from\nfull-slice to sparse-slice settings to ensure accurate reconstruction under\nextreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme\nrequiring only keyframe (e.g., ED and ES) annotations. Trained and validated on\nthree large public datasets and externally evaluated zero-shot on additional\nprivate interventional and public CMR datasets, TetHeart achieves\nstate-of-the-art accuracy and strong generalization in both pre- and\nintra-procedural settings.\n", "link": "http://arxiv.org/abs/2509.12090v1", "date": "2025-09-15", "relevancy": 2.7111, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5507}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.54}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%204D%20Heart%20Mesh%20Recovery%20Across%20Full-Stack%20and%20Sparse%20Cardiac%0A%20%20MRI&body=Title%3A%20End-to-End%204D%20Heart%20Mesh%20Recovery%20Across%20Full-Stack%20and%20Sparse%20Cardiac%0A%20%20MRI%0AAuthor%3A%20Yihong%20Chen%20and%20Jiancheng%20Yang%20and%20Deniz%20Sayin%20Mercadier%20and%20Hieu%20Le%20and%20Juerg%20Schwitter%20and%20Pascal%20Fua%0AAbstract%3A%20%20%20Reconstructing%20cardiac%20motion%20from%20cine%20CMR%20sequences%20is%20critical%20for%0Adiagnosis%2C%20prediction%2C%20and%20intervention.%20Existing%20methods%20rely%20on%20complete%20CMR%0Astacks%20to%20infer%20full%20heart%20motion%2C%20limiting%20their%20utility%20in%20intra-procedural%0Ascenarios%20where%20only%20sparse%20observations%20are%20available.%20We%20present%20TetHeart%2C%0Athe%20first%20end-to-end%20framework%20that%20unifies%20full%204D%20multi-structure%20heart%20mesh%0Arecovery%20from%20both%20offline%20full-stack%20acquisitions%20and%20intra-procedural%0Asparse-slice%20observations.%20Our%20method%20leverages%20deep%20deformable%20tetrahedra%2C%20an%0Aexplicit-implicit%20hybrid%20representation%2C%20to%20capture%20shape%20and%20motion%20in%20a%0Acoherent%20space%20shared%20across%20cardiac%20structures.%20It%20is%20initialized%20from%0Ahigh-quality%20pre-procedural%20or%20offline-acquired%20full%20stacks%20to%20build%20detailed%2C%0Apatient-specific%20heart%20meshes%2C%20which%20can%20then%20be%20updated%20using%20whatever%20slices%0Aare%20available%2C%20from%20full%20stacks%20down%20to%20a%20single%20slice.%20We%20further%20incorporate%0Aseveral%20key%20innovations%3A%20%28i%29%20an%20attentive%20mechanism%20for%20slice-adaptive%202D-3D%0Afeature%20assembly%20that%20dynamically%20integrates%20information%20from%20arbitrary%20numbers%0Aof%20slices%20at%20any%20position%2C%20combined%20with%20a%20distillation%20strategy%20from%0Afull-slice%20to%20sparse-slice%20settings%20to%20ensure%20accurate%20reconstruction%20under%0Aextreme%20sparsity%3B%20and%20%28ii%29%20a%20two-stage%20weakly%20supervised%20motion%20learning%20scheme%0Arequiring%20only%20keyframe%20%28e.g.%2C%20ED%20and%20ES%29%20annotations.%20Trained%20and%20validated%20on%0Athree%20large%20public%20datasets%20and%20externally%20evaluated%20zero-shot%20on%20additional%0Aprivate%20interventional%20and%20public%20CMR%20datasets%2C%20TetHeart%20achieves%0Astate-of-the-art%20accuracy%20and%20strong%20generalization%20in%20both%20pre-%20and%0Aintra-procedural%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%25204D%2520Heart%2520Mesh%2520Recovery%2520Across%2520Full-Stack%2520and%2520Sparse%2520Cardiac%250A%2520%2520MRI%26entry.906535625%3DYihong%2520Chen%2520and%2520Jiancheng%2520Yang%2520and%2520Deniz%2520Sayin%2520Mercadier%2520and%2520Hieu%2520Le%2520and%2520Juerg%2520Schwitter%2520and%2520Pascal%2520Fua%26entry.1292438233%3D%2520%2520Reconstructing%2520cardiac%2520motion%2520from%2520cine%2520CMR%2520sequences%2520is%2520critical%2520for%250Adiagnosis%252C%2520prediction%252C%2520and%2520intervention.%2520Existing%2520methods%2520rely%2520on%2520complete%2520CMR%250Astacks%2520to%2520infer%2520full%2520heart%2520motion%252C%2520limiting%2520their%2520utility%2520in%2520intra-procedural%250Ascenarios%2520where%2520only%2520sparse%2520observations%2520are%2520available.%2520We%2520present%2520TetHeart%252C%250Athe%2520first%2520end-to-end%2520framework%2520that%2520unifies%2520full%25204D%2520multi-structure%2520heart%2520mesh%250Arecovery%2520from%2520both%2520offline%2520full-stack%2520acquisitions%2520and%2520intra-procedural%250Asparse-slice%2520observations.%2520Our%2520method%2520leverages%2520deep%2520deformable%2520tetrahedra%252C%2520an%250Aexplicit-implicit%2520hybrid%2520representation%252C%2520to%2520capture%2520shape%2520and%2520motion%2520in%2520a%250Acoherent%2520space%2520shared%2520across%2520cardiac%2520structures.%2520It%2520is%2520initialized%2520from%250Ahigh-quality%2520pre-procedural%2520or%2520offline-acquired%2520full%2520stacks%2520to%2520build%2520detailed%252C%250Apatient-specific%2520heart%2520meshes%252C%2520which%2520can%2520then%2520be%2520updated%2520using%2520whatever%2520slices%250Aare%2520available%252C%2520from%2520full%2520stacks%2520down%2520to%2520a%2520single%2520slice.%2520We%2520further%2520incorporate%250Aseveral%2520key%2520innovations%253A%2520%2528i%2529%2520an%2520attentive%2520mechanism%2520for%2520slice-adaptive%25202D-3D%250Afeature%2520assembly%2520that%2520dynamically%2520integrates%2520information%2520from%2520arbitrary%2520numbers%250Aof%2520slices%2520at%2520any%2520position%252C%2520combined%2520with%2520a%2520distillation%2520strategy%2520from%250Afull-slice%2520to%2520sparse-slice%2520settings%2520to%2520ensure%2520accurate%2520reconstruction%2520under%250Aextreme%2520sparsity%253B%2520and%2520%2528ii%2529%2520a%2520two-stage%2520weakly%2520supervised%2520motion%2520learning%2520scheme%250Arequiring%2520only%2520keyframe%2520%2528e.g.%252C%2520ED%2520and%2520ES%2529%2520annotations.%2520Trained%2520and%2520validated%2520on%250Athree%2520large%2520public%2520datasets%2520and%2520externally%2520evaluated%2520zero-shot%2520on%2520additional%250Aprivate%2520interventional%2520and%2520public%2520CMR%2520datasets%252C%2520TetHeart%2520achieves%250Astate-of-the-art%2520accuracy%2520and%2520strong%2520generalization%2520in%2520both%2520pre-%2520and%250Aintra-procedural%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%204D%20Heart%20Mesh%20Recovery%20Across%20Full-Stack%20and%20Sparse%20Cardiac%0A%20%20MRI&entry.906535625=Yihong%20Chen%20and%20Jiancheng%20Yang%20and%20Deniz%20Sayin%20Mercadier%20and%20Hieu%20Le%20and%20Juerg%20Schwitter%20and%20Pascal%20Fua&entry.1292438233=%20%20Reconstructing%20cardiac%20motion%20from%20cine%20CMR%20sequences%20is%20critical%20for%0Adiagnosis%2C%20prediction%2C%20and%20intervention.%20Existing%20methods%20rely%20on%20complete%20CMR%0Astacks%20to%20infer%20full%20heart%20motion%2C%20limiting%20their%20utility%20in%20intra-procedural%0Ascenarios%20where%20only%20sparse%20observations%20are%20available.%20We%20present%20TetHeart%2C%0Athe%20first%20end-to-end%20framework%20that%20unifies%20full%204D%20multi-structure%20heart%20mesh%0Arecovery%20from%20both%20offline%20full-stack%20acquisitions%20and%20intra-procedural%0Asparse-slice%20observations.%20Our%20method%20leverages%20deep%20deformable%20tetrahedra%2C%20an%0Aexplicit-implicit%20hybrid%20representation%2C%20to%20capture%20shape%20and%20motion%20in%20a%0Acoherent%20space%20shared%20across%20cardiac%20structures.%20It%20is%20initialized%20from%0Ahigh-quality%20pre-procedural%20or%20offline-acquired%20full%20stacks%20to%20build%20detailed%2C%0Apatient-specific%20heart%20meshes%2C%20which%20can%20then%20be%20updated%20using%20whatever%20slices%0Aare%20available%2C%20from%20full%20stacks%20down%20to%20a%20single%20slice.%20We%20further%20incorporate%0Aseveral%20key%20innovations%3A%20%28i%29%20an%20attentive%20mechanism%20for%20slice-adaptive%202D-3D%0Afeature%20assembly%20that%20dynamically%20integrates%20information%20from%20arbitrary%20numbers%0Aof%20slices%20at%20any%20position%2C%20combined%20with%20a%20distillation%20strategy%20from%0Afull-slice%20to%20sparse-slice%20settings%20to%20ensure%20accurate%20reconstruction%20under%0Aextreme%20sparsity%3B%20and%20%28ii%29%20a%20two-stage%20weakly%20supervised%20motion%20learning%20scheme%0Arequiring%20only%20keyframe%20%28e.g.%2C%20ED%20and%20ES%29%20annotations.%20Trained%20and%20validated%20on%0Athree%20large%20public%20datasets%20and%20externally%20evaluated%20zero-shot%20on%20additional%0Aprivate%20interventional%20and%20public%20CMR%20datasets%2C%20TetHeart%20achieves%0Astate-of-the-art%20accuracy%20and%20strong%20generalization%20in%20both%20pre-%20and%0Aintra-procedural%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12090v1&entry.124074799=Read"},
{"title": "Robust Fetal Pose Estimation across Gestational Ages via\n  Cross-Population Augmentation", "author": "Sebastian Diaz and Benjamin Billot and Neel Dey and Molin Zhang and Esra Abaci Turk and P. Ellen Grant and Polina Golland and Elfar Adalsteinsson", "abstract": "  Fetal motion is a critical indicator of neurological development and\nintrauterine health, yet its quantification remains challenging, particularly\nat earlier gestational ages (GA). Current methods track fetal motion by\npredicting the location of annotated landmarks on 3D echo planar imaging (EPI)\ntime-series, primarily in third-trimester fetuses. The predicted landmarks\nenable simplification of the fetal body for downstream analysis. While these\nmethods perform well within their training age distribution, they consistently\nfail to generalize to early GAs due to significant anatomical changes in both\nmother and fetus across gestation, as well as the difficulty of obtaining\nannotated early GA EPI data. In this work, we develop a cross-population data\naugmentation framework that enables pose estimation models to robustly\ngeneralize to younger GA clinical cohorts using only annotated images from\nolder GA cohorts. Specifically, we introduce a fetal-specific augmentation\nstrategy that simulates the distinct intrauterine environment and fetal\npositioning of early GAs. Our experiments find that cross-population\naugmentation yields reduced variability and significant improvements across\nboth older GA and challenging early GA cases. By enabling more reliable pose\nestimation across gestation, our work potentially facilitates early clinical\ndetection and intervention in challenging 4D fetal imaging settings. Code is\navailable at https://github.com/sebodiaz/cross-population-pose.\n", "link": "http://arxiv.org/abs/2509.12062v1", "date": "2025-09-15", "relevancy": 2.6992, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5704}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5421}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Fetal%20Pose%20Estimation%20across%20Gestational%20Ages%20via%0A%20%20Cross-Population%20Augmentation&body=Title%3A%20Robust%20Fetal%20Pose%20Estimation%20across%20Gestational%20Ages%20via%0A%20%20Cross-Population%20Augmentation%0AAuthor%3A%20Sebastian%20Diaz%20and%20Benjamin%20Billot%20and%20Neel%20Dey%20and%20Molin%20Zhang%20and%20Esra%20Abaci%20Turk%20and%20P.%20Ellen%20Grant%20and%20Polina%20Golland%20and%20Elfar%20Adalsteinsson%0AAbstract%3A%20%20%20Fetal%20motion%20is%20a%20critical%20indicator%20of%20neurological%20development%20and%0Aintrauterine%20health%2C%20yet%20its%20quantification%20remains%20challenging%2C%20particularly%0Aat%20earlier%20gestational%20ages%20%28GA%29.%20Current%20methods%20track%20fetal%20motion%20by%0Apredicting%20the%20location%20of%20annotated%20landmarks%20on%203D%20echo%20planar%20imaging%20%28EPI%29%0Atime-series%2C%20primarily%20in%20third-trimester%20fetuses.%20The%20predicted%20landmarks%0Aenable%20simplification%20of%20the%20fetal%20body%20for%20downstream%20analysis.%20While%20these%0Amethods%20perform%20well%20within%20their%20training%20age%20distribution%2C%20they%20consistently%0Afail%20to%20generalize%20to%20early%20GAs%20due%20to%20significant%20anatomical%20changes%20in%20both%0Amother%20and%20fetus%20across%20gestation%2C%20as%20well%20as%20the%20difficulty%20of%20obtaining%0Aannotated%20early%20GA%20EPI%20data.%20In%20this%20work%2C%20we%20develop%20a%20cross-population%20data%0Aaugmentation%20framework%20that%20enables%20pose%20estimation%20models%20to%20robustly%0Ageneralize%20to%20younger%20GA%20clinical%20cohorts%20using%20only%20annotated%20images%20from%0Aolder%20GA%20cohorts.%20Specifically%2C%20we%20introduce%20a%20fetal-specific%20augmentation%0Astrategy%20that%20simulates%20the%20distinct%20intrauterine%20environment%20and%20fetal%0Apositioning%20of%20early%20GAs.%20Our%20experiments%20find%20that%20cross-population%0Aaugmentation%20yields%20reduced%20variability%20and%20significant%20improvements%20across%0Aboth%20older%20GA%20and%20challenging%20early%20GA%20cases.%20By%20enabling%20more%20reliable%20pose%0Aestimation%20across%20gestation%2C%20our%20work%20potentially%20facilitates%20early%20clinical%0Adetection%20and%20intervention%20in%20challenging%204D%20fetal%20imaging%20settings.%20Code%20is%0Aavailable%20at%20https%3A//github.com/sebodiaz/cross-population-pose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12062v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Fetal%2520Pose%2520Estimation%2520across%2520Gestational%2520Ages%2520via%250A%2520%2520Cross-Population%2520Augmentation%26entry.906535625%3DSebastian%2520Diaz%2520and%2520Benjamin%2520Billot%2520and%2520Neel%2520Dey%2520and%2520Molin%2520Zhang%2520and%2520Esra%2520Abaci%2520Turk%2520and%2520P.%2520Ellen%2520Grant%2520and%2520Polina%2520Golland%2520and%2520Elfar%2520Adalsteinsson%26entry.1292438233%3D%2520%2520Fetal%2520motion%2520is%2520a%2520critical%2520indicator%2520of%2520neurological%2520development%2520and%250Aintrauterine%2520health%252C%2520yet%2520its%2520quantification%2520remains%2520challenging%252C%2520particularly%250Aat%2520earlier%2520gestational%2520ages%2520%2528GA%2529.%2520Current%2520methods%2520track%2520fetal%2520motion%2520by%250Apredicting%2520the%2520location%2520of%2520annotated%2520landmarks%2520on%25203D%2520echo%2520planar%2520imaging%2520%2528EPI%2529%250Atime-series%252C%2520primarily%2520in%2520third-trimester%2520fetuses.%2520The%2520predicted%2520landmarks%250Aenable%2520simplification%2520of%2520the%2520fetal%2520body%2520for%2520downstream%2520analysis.%2520While%2520these%250Amethods%2520perform%2520well%2520within%2520their%2520training%2520age%2520distribution%252C%2520they%2520consistently%250Afail%2520to%2520generalize%2520to%2520early%2520GAs%2520due%2520to%2520significant%2520anatomical%2520changes%2520in%2520both%250Amother%2520and%2520fetus%2520across%2520gestation%252C%2520as%2520well%2520as%2520the%2520difficulty%2520of%2520obtaining%250Aannotated%2520early%2520GA%2520EPI%2520data.%2520In%2520this%2520work%252C%2520we%2520develop%2520a%2520cross-population%2520data%250Aaugmentation%2520framework%2520that%2520enables%2520pose%2520estimation%2520models%2520to%2520robustly%250Ageneralize%2520to%2520younger%2520GA%2520clinical%2520cohorts%2520using%2520only%2520annotated%2520images%2520from%250Aolder%2520GA%2520cohorts.%2520Specifically%252C%2520we%2520introduce%2520a%2520fetal-specific%2520augmentation%250Astrategy%2520that%2520simulates%2520the%2520distinct%2520intrauterine%2520environment%2520and%2520fetal%250Apositioning%2520of%2520early%2520GAs.%2520Our%2520experiments%2520find%2520that%2520cross-population%250Aaugmentation%2520yields%2520reduced%2520variability%2520and%2520significant%2520improvements%2520across%250Aboth%2520older%2520GA%2520and%2520challenging%2520early%2520GA%2520cases.%2520By%2520enabling%2520more%2520reliable%2520pose%250Aestimation%2520across%2520gestation%252C%2520our%2520work%2520potentially%2520facilitates%2520early%2520clinical%250Adetection%2520and%2520intervention%2520in%2520challenging%25204D%2520fetal%2520imaging%2520settings.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/sebodiaz/cross-population-pose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12062v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Fetal%20Pose%20Estimation%20across%20Gestational%20Ages%20via%0A%20%20Cross-Population%20Augmentation&entry.906535625=Sebastian%20Diaz%20and%20Benjamin%20Billot%20and%20Neel%20Dey%20and%20Molin%20Zhang%20and%20Esra%20Abaci%20Turk%20and%20P.%20Ellen%20Grant%20and%20Polina%20Golland%20and%20Elfar%20Adalsteinsson&entry.1292438233=%20%20Fetal%20motion%20is%20a%20critical%20indicator%20of%20neurological%20development%20and%0Aintrauterine%20health%2C%20yet%20its%20quantification%20remains%20challenging%2C%20particularly%0Aat%20earlier%20gestational%20ages%20%28GA%29.%20Current%20methods%20track%20fetal%20motion%20by%0Apredicting%20the%20location%20of%20annotated%20landmarks%20on%203D%20echo%20planar%20imaging%20%28EPI%29%0Atime-series%2C%20primarily%20in%20third-trimester%20fetuses.%20The%20predicted%20landmarks%0Aenable%20simplification%20of%20the%20fetal%20body%20for%20downstream%20analysis.%20While%20these%0Amethods%20perform%20well%20within%20their%20training%20age%20distribution%2C%20they%20consistently%0Afail%20to%20generalize%20to%20early%20GAs%20due%20to%20significant%20anatomical%20changes%20in%20both%0Amother%20and%20fetus%20across%20gestation%2C%20as%20well%20as%20the%20difficulty%20of%20obtaining%0Aannotated%20early%20GA%20EPI%20data.%20In%20this%20work%2C%20we%20develop%20a%20cross-population%20data%0Aaugmentation%20framework%20that%20enables%20pose%20estimation%20models%20to%20robustly%0Ageneralize%20to%20younger%20GA%20clinical%20cohorts%20using%20only%20annotated%20images%20from%0Aolder%20GA%20cohorts.%20Specifically%2C%20we%20introduce%20a%20fetal-specific%20augmentation%0Astrategy%20that%20simulates%20the%20distinct%20intrauterine%20environment%20and%20fetal%0Apositioning%20of%20early%20GAs.%20Our%20experiments%20find%20that%20cross-population%0Aaugmentation%20yields%20reduced%20variability%20and%20significant%20improvements%20across%0Aboth%20older%20GA%20and%20challenging%20early%20GA%20cases.%20By%20enabling%20more%20reliable%20pose%0Aestimation%20across%20gestation%2C%20our%20work%20potentially%20facilitates%20early%20clinical%0Adetection%20and%20intervention%20in%20challenging%204D%20fetal%20imaging%20settings.%20Code%20is%0Aavailable%20at%20https%3A//github.com/sebodiaz/cross-population-pose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12062v1&entry.124074799=Read"},
{"title": "Transformer-Based Multimodal Knowledge Graph Completion with Link-Aware\n  Contexts", "author": "Haodi Ma and Dzmitry Kasinets and Daisy Zhe Wang", "abstract": "  Multimodal knowledge graph completion (MMKGC) aims to predict missing links\nin multimodal knowledge graphs (MMKGs) by leveraging information from various\nmodalities alongside structural data. Existing MMKGC approaches primarily\nextend traditional knowledge graph embedding (KGE) models, which often require\ncreating an embedding for every entity. This results in large model sizes and\ninefficiencies in integrating multimodal information, particularly for\nreal-world graphs. Meanwhile, Transformer-based models have demonstrated\ncompetitive performance in knowledge graph completion (KGC). However, their\nfocus on single-modal knowledge limits their capacity to utilize cross-modal\ninformation. Recently, Large vision-language models (VLMs) have shown potential\nin cross-modal tasks but are constrained by the high cost of training. In this\nwork, we propose a novel approach that integrates Transformer-based KGE models\nwith cross-modal context generated by pre-trained VLMs, thereby extending their\napplicability to MMKGC. Specifically, we employ a pre-trained VLM to transform\nrelevant visual information from entities and their neighbors into textual\nsequences. We then frame KGC as a sequence-to-sequence task, fine-tuning the\nmodel with the generated cross-modal context. This simple yet effective method\nsignificantly reduces model size compared to traditional KGE approaches while\nachieving competitive performance across multiple large-scale datasets with\nminimal hyperparameter tuning.\n", "link": "http://arxiv.org/abs/2501.15688v2", "date": "2025-09-15", "relevancy": 2.6754, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5209}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer-Based%20Multimodal%20Knowledge%20Graph%20Completion%20with%20Link-Aware%0A%20%20Contexts&body=Title%3A%20Transformer-Based%20Multimodal%20Knowledge%20Graph%20Completion%20with%20Link-Aware%0A%20%20Contexts%0AAuthor%3A%20Haodi%20Ma%20and%20Dzmitry%20Kasinets%20and%20Daisy%20Zhe%20Wang%0AAbstract%3A%20%20%20Multimodal%20knowledge%20graph%20completion%20%28MMKGC%29%20aims%20to%20predict%20missing%20links%0Ain%20multimodal%20knowledge%20graphs%20%28MMKGs%29%20by%20leveraging%20information%20from%20various%0Amodalities%20alongside%20structural%20data.%20Existing%20MMKGC%20approaches%20primarily%0Aextend%20traditional%20knowledge%20graph%20embedding%20%28KGE%29%20models%2C%20which%20often%20require%0Acreating%20an%20embedding%20for%20every%20entity.%20This%20results%20in%20large%20model%20sizes%20and%0Ainefficiencies%20in%20integrating%20multimodal%20information%2C%20particularly%20for%0Areal-world%20graphs.%20Meanwhile%2C%20Transformer-based%20models%20have%20demonstrated%0Acompetitive%20performance%20in%20knowledge%20graph%20completion%20%28KGC%29.%20However%2C%20their%0Afocus%20on%20single-modal%20knowledge%20limits%20their%20capacity%20to%20utilize%20cross-modal%0Ainformation.%20Recently%2C%20Large%20vision-language%20models%20%28VLMs%29%20have%20shown%20potential%0Ain%20cross-modal%20tasks%20but%20are%20constrained%20by%20the%20high%20cost%20of%20training.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20approach%20that%20integrates%20Transformer-based%20KGE%20models%0Awith%20cross-modal%20context%20generated%20by%20pre-trained%20VLMs%2C%20thereby%20extending%20their%0Aapplicability%20to%20MMKGC.%20Specifically%2C%20we%20employ%20a%20pre-trained%20VLM%20to%20transform%0Arelevant%20visual%20information%20from%20entities%20and%20their%20neighbors%20into%20textual%0Asequences.%20We%20then%20frame%20KGC%20as%20a%20sequence-to-sequence%20task%2C%20fine-tuning%20the%0Amodel%20with%20the%20generated%20cross-modal%20context.%20This%20simple%20yet%20effective%20method%0Asignificantly%20reduces%20model%20size%20compared%20to%20traditional%20KGE%20approaches%20while%0Aachieving%20competitive%20performance%20across%20multiple%20large-scale%20datasets%20with%0Aminimal%20hyperparameter%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15688v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer-Based%2520Multimodal%2520Knowledge%2520Graph%2520Completion%2520with%2520Link-Aware%250A%2520%2520Contexts%26entry.906535625%3DHaodi%2520Ma%2520and%2520Dzmitry%2520Kasinets%2520and%2520Daisy%2520Zhe%2520Wang%26entry.1292438233%3D%2520%2520Multimodal%2520knowledge%2520graph%2520completion%2520%2528MMKGC%2529%2520aims%2520to%2520predict%2520missing%2520links%250Ain%2520multimodal%2520knowledge%2520graphs%2520%2528MMKGs%2529%2520by%2520leveraging%2520information%2520from%2520various%250Amodalities%2520alongside%2520structural%2520data.%2520Existing%2520MMKGC%2520approaches%2520primarily%250Aextend%2520traditional%2520knowledge%2520graph%2520embedding%2520%2528KGE%2529%2520models%252C%2520which%2520often%2520require%250Acreating%2520an%2520embedding%2520for%2520every%2520entity.%2520This%2520results%2520in%2520large%2520model%2520sizes%2520and%250Ainefficiencies%2520in%2520integrating%2520multimodal%2520information%252C%2520particularly%2520for%250Areal-world%2520graphs.%2520Meanwhile%252C%2520Transformer-based%2520models%2520have%2520demonstrated%250Acompetitive%2520performance%2520in%2520knowledge%2520graph%2520completion%2520%2528KGC%2529.%2520However%252C%2520their%250Afocus%2520on%2520single-modal%2520knowledge%2520limits%2520their%2520capacity%2520to%2520utilize%2520cross-modal%250Ainformation.%2520Recently%252C%2520Large%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520potential%250Ain%2520cross-modal%2520tasks%2520but%2520are%2520constrained%2520by%2520the%2520high%2520cost%2520of%2520training.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520integrates%2520Transformer-based%2520KGE%2520models%250Awith%2520cross-modal%2520context%2520generated%2520by%2520pre-trained%2520VLMs%252C%2520thereby%2520extending%2520their%250Aapplicability%2520to%2520MMKGC.%2520Specifically%252C%2520we%2520employ%2520a%2520pre-trained%2520VLM%2520to%2520transform%250Arelevant%2520visual%2520information%2520from%2520entities%2520and%2520their%2520neighbors%2520into%2520textual%250Asequences.%2520We%2520then%2520frame%2520KGC%2520as%2520a%2520sequence-to-sequence%2520task%252C%2520fine-tuning%2520the%250Amodel%2520with%2520the%2520generated%2520cross-modal%2520context.%2520This%2520simple%2520yet%2520effective%2520method%250Asignificantly%2520reduces%2520model%2520size%2520compared%2520to%2520traditional%2520KGE%2520approaches%2520while%250Aachieving%2520competitive%2520performance%2520across%2520multiple%2520large-scale%2520datasets%2520with%250Aminimal%2520hyperparameter%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15688v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer-Based%20Multimodal%20Knowledge%20Graph%20Completion%20with%20Link-Aware%0A%20%20Contexts&entry.906535625=Haodi%20Ma%20and%20Dzmitry%20Kasinets%20and%20Daisy%20Zhe%20Wang&entry.1292438233=%20%20Multimodal%20knowledge%20graph%20completion%20%28MMKGC%29%20aims%20to%20predict%20missing%20links%0Ain%20multimodal%20knowledge%20graphs%20%28MMKGs%29%20by%20leveraging%20information%20from%20various%0Amodalities%20alongside%20structural%20data.%20Existing%20MMKGC%20approaches%20primarily%0Aextend%20traditional%20knowledge%20graph%20embedding%20%28KGE%29%20models%2C%20which%20often%20require%0Acreating%20an%20embedding%20for%20every%20entity.%20This%20results%20in%20large%20model%20sizes%20and%0Ainefficiencies%20in%20integrating%20multimodal%20information%2C%20particularly%20for%0Areal-world%20graphs.%20Meanwhile%2C%20Transformer-based%20models%20have%20demonstrated%0Acompetitive%20performance%20in%20knowledge%20graph%20completion%20%28KGC%29.%20However%2C%20their%0Afocus%20on%20single-modal%20knowledge%20limits%20their%20capacity%20to%20utilize%20cross-modal%0Ainformation.%20Recently%2C%20Large%20vision-language%20models%20%28VLMs%29%20have%20shown%20potential%0Ain%20cross-modal%20tasks%20but%20are%20constrained%20by%20the%20high%20cost%20of%20training.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20approach%20that%20integrates%20Transformer-based%20KGE%20models%0Awith%20cross-modal%20context%20generated%20by%20pre-trained%20VLMs%2C%20thereby%20extending%20their%0Aapplicability%20to%20MMKGC.%20Specifically%2C%20we%20employ%20a%20pre-trained%20VLM%20to%20transform%0Arelevant%20visual%20information%20from%20entities%20and%20their%20neighbors%20into%20textual%0Asequences.%20We%20then%20frame%20KGC%20as%20a%20sequence-to-sequence%20task%2C%20fine-tuning%20the%0Amodel%20with%20the%20generated%20cross-modal%20context.%20This%20simple%20yet%20effective%20method%0Asignificantly%20reduces%20model%20size%20compared%20to%20traditional%20KGE%20approaches%20while%0Aachieving%20competitive%20performance%20across%20multiple%20large-scale%20datasets%20with%0Aminimal%20hyperparameter%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15688v2&entry.124074799=Read"},
{"title": "A Statistical 3D Stomach Shape Model for Anatomical Analysis", "author": "Erez Posner and Ore Shtalrid and Oded Erell and Daniel Noy and Moshe Bouhnik", "abstract": "  Realistic and parameterized 3D models of human anatomy have become invaluable\nin research, diagnostics, and surgical planning. However, the development of\ndetailed models for internal organs, such as the stomach, has been limited by\ndata availability and methodological challenges. In this paper, we propose a\nnovel pipeline for the generation of synthetic 3D stomach models, enabling the\ncreation of anatomically diverse morphologies informed by established studies\non stomach shape variability. Using this pipeline, we construct a dataset of\nsynthetic stomachs. Building on this dataset, we develop a 3D statistical shape\nmodel of the stomach, trained to capture natural anatomical variability in a\nlow-dimensional shape space. The model is further refined using CT meshes\nderived from publicly available datasets through a semi-supervised alignment\nprocess, enhancing its ability to generalize to unseen anatomical variations.\nWe evaluated the model on a held-out test set of real stomach CT scans,\ndemonstrating robust generalization and fit accuracy. We make the statistical\nshape model along with the synthetic dataset publicly available on GitLab:\nhttps://gitlab.com/Erez.Posner/stomach_pytorch to facilitate further research.\nThis work introduces the first statistical 3D shape model of the stomach, with\napplications ranging from surgical simulation and pre-operative planning to\nmedical education and computational modeling. By combining synthetic data\ngeneration, parametric modeling, and real-world validation, our approach\nrepresents a significant advancement in organ modeling and opens new\npossibilities for personalized healthcare solutions.\n", "link": "http://arxiv.org/abs/2509.06464v2", "date": "2025-09-15", "relevancy": 2.6749, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5824}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5203}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Statistical%203D%20Stomach%20Shape%20Model%20for%20Anatomical%20Analysis&body=Title%3A%20A%20Statistical%203D%20Stomach%20Shape%20Model%20for%20Anatomical%20Analysis%0AAuthor%3A%20Erez%20Posner%20and%20Ore%20Shtalrid%20and%20Oded%20Erell%20and%20Daniel%20Noy%20and%20Moshe%20Bouhnik%0AAbstract%3A%20%20%20Realistic%20and%20parameterized%203D%20models%20of%20human%20anatomy%20have%20become%20invaluable%0Ain%20research%2C%20diagnostics%2C%20and%20surgical%20planning.%20However%2C%20the%20development%20of%0Adetailed%20models%20for%20internal%20organs%2C%20such%20as%20the%20stomach%2C%20has%20been%20limited%20by%0Adata%20availability%20and%20methodological%20challenges.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20pipeline%20for%20the%20generation%20of%20synthetic%203D%20stomach%20models%2C%20enabling%20the%0Acreation%20of%20anatomically%20diverse%20morphologies%20informed%20by%20established%20studies%0Aon%20stomach%20shape%20variability.%20Using%20this%20pipeline%2C%20we%20construct%20a%20dataset%20of%0Asynthetic%20stomachs.%20Building%20on%20this%20dataset%2C%20we%20develop%20a%203D%20statistical%20shape%0Amodel%20of%20the%20stomach%2C%20trained%20to%20capture%20natural%20anatomical%20variability%20in%20a%0Alow-dimensional%20shape%20space.%20The%20model%20is%20further%20refined%20using%20CT%20meshes%0Aderived%20from%20publicly%20available%20datasets%20through%20a%20semi-supervised%20alignment%0Aprocess%2C%20enhancing%20its%20ability%20to%20generalize%20to%20unseen%20anatomical%20variations.%0AWe%20evaluated%20the%20model%20on%20a%20held-out%20test%20set%20of%20real%20stomach%20CT%20scans%2C%0Ademonstrating%20robust%20generalization%20and%20fit%20accuracy.%20We%20make%20the%20statistical%0Ashape%20model%20along%20with%20the%20synthetic%20dataset%20publicly%20available%20on%20GitLab%3A%0Ahttps%3A//gitlab.com/Erez.Posner/stomach_pytorch%20to%20facilitate%20further%20research.%0AThis%20work%20introduces%20the%20first%20statistical%203D%20shape%20model%20of%20the%20stomach%2C%20with%0Aapplications%20ranging%20from%20surgical%20simulation%20and%20pre-operative%20planning%20to%0Amedical%20education%20and%20computational%20modeling.%20By%20combining%20synthetic%20data%0Ageneration%2C%20parametric%20modeling%2C%20and%20real-world%20validation%2C%20our%20approach%0Arepresents%20a%20significant%20advancement%20in%20organ%20modeling%20and%20opens%20new%0Apossibilities%20for%20personalized%20healthcare%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06464v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Statistical%25203D%2520Stomach%2520Shape%2520Model%2520for%2520Anatomical%2520Analysis%26entry.906535625%3DErez%2520Posner%2520and%2520Ore%2520Shtalrid%2520and%2520Oded%2520Erell%2520and%2520Daniel%2520Noy%2520and%2520Moshe%2520Bouhnik%26entry.1292438233%3D%2520%2520Realistic%2520and%2520parameterized%25203D%2520models%2520of%2520human%2520anatomy%2520have%2520become%2520invaluable%250Ain%2520research%252C%2520diagnostics%252C%2520and%2520surgical%2520planning.%2520However%252C%2520the%2520development%2520of%250Adetailed%2520models%2520for%2520internal%2520organs%252C%2520such%2520as%2520the%2520stomach%252C%2520has%2520been%2520limited%2520by%250Adata%2520availability%2520and%2520methodological%2520challenges.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520pipeline%2520for%2520the%2520generation%2520of%2520synthetic%25203D%2520stomach%2520models%252C%2520enabling%2520the%250Acreation%2520of%2520anatomically%2520diverse%2520morphologies%2520informed%2520by%2520established%2520studies%250Aon%2520stomach%2520shape%2520variability.%2520Using%2520this%2520pipeline%252C%2520we%2520construct%2520a%2520dataset%2520of%250Asynthetic%2520stomachs.%2520Building%2520on%2520this%2520dataset%252C%2520we%2520develop%2520a%25203D%2520statistical%2520shape%250Amodel%2520of%2520the%2520stomach%252C%2520trained%2520to%2520capture%2520natural%2520anatomical%2520variability%2520in%2520a%250Alow-dimensional%2520shape%2520space.%2520The%2520model%2520is%2520further%2520refined%2520using%2520CT%2520meshes%250Aderived%2520from%2520publicly%2520available%2520datasets%2520through%2520a%2520semi-supervised%2520alignment%250Aprocess%252C%2520enhancing%2520its%2520ability%2520to%2520generalize%2520to%2520unseen%2520anatomical%2520variations.%250AWe%2520evaluated%2520the%2520model%2520on%2520a%2520held-out%2520test%2520set%2520of%2520real%2520stomach%2520CT%2520scans%252C%250Ademonstrating%2520robust%2520generalization%2520and%2520fit%2520accuracy.%2520We%2520make%2520the%2520statistical%250Ashape%2520model%2520along%2520with%2520the%2520synthetic%2520dataset%2520publicly%2520available%2520on%2520GitLab%253A%250Ahttps%253A//gitlab.com/Erez.Posner/stomach_pytorch%2520to%2520facilitate%2520further%2520research.%250AThis%2520work%2520introduces%2520the%2520first%2520statistical%25203D%2520shape%2520model%2520of%2520the%2520stomach%252C%2520with%250Aapplications%2520ranging%2520from%2520surgical%2520simulation%2520and%2520pre-operative%2520planning%2520to%250Amedical%2520education%2520and%2520computational%2520modeling.%2520By%2520combining%2520synthetic%2520data%250Ageneration%252C%2520parametric%2520modeling%252C%2520and%2520real-world%2520validation%252C%2520our%2520approach%250Arepresents%2520a%2520significant%2520advancement%2520in%2520organ%2520modeling%2520and%2520opens%2520new%250Apossibilities%2520for%2520personalized%2520healthcare%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06464v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Statistical%203D%20Stomach%20Shape%20Model%20for%20Anatomical%20Analysis&entry.906535625=Erez%20Posner%20and%20Ore%20Shtalrid%20and%20Oded%20Erell%20and%20Daniel%20Noy%20and%20Moshe%20Bouhnik&entry.1292438233=%20%20Realistic%20and%20parameterized%203D%20models%20of%20human%20anatomy%20have%20become%20invaluable%0Ain%20research%2C%20diagnostics%2C%20and%20surgical%20planning.%20However%2C%20the%20development%20of%0Adetailed%20models%20for%20internal%20organs%2C%20such%20as%20the%20stomach%2C%20has%20been%20limited%20by%0Adata%20availability%20and%20methodological%20challenges.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20pipeline%20for%20the%20generation%20of%20synthetic%203D%20stomach%20models%2C%20enabling%20the%0Acreation%20of%20anatomically%20diverse%20morphologies%20informed%20by%20established%20studies%0Aon%20stomach%20shape%20variability.%20Using%20this%20pipeline%2C%20we%20construct%20a%20dataset%20of%0Asynthetic%20stomachs.%20Building%20on%20this%20dataset%2C%20we%20develop%20a%203D%20statistical%20shape%0Amodel%20of%20the%20stomach%2C%20trained%20to%20capture%20natural%20anatomical%20variability%20in%20a%0Alow-dimensional%20shape%20space.%20The%20model%20is%20further%20refined%20using%20CT%20meshes%0Aderived%20from%20publicly%20available%20datasets%20through%20a%20semi-supervised%20alignment%0Aprocess%2C%20enhancing%20its%20ability%20to%20generalize%20to%20unseen%20anatomical%20variations.%0AWe%20evaluated%20the%20model%20on%20a%20held-out%20test%20set%20of%20real%20stomach%20CT%20scans%2C%0Ademonstrating%20robust%20generalization%20and%20fit%20accuracy.%20We%20make%20the%20statistical%0Ashape%20model%20along%20with%20the%20synthetic%20dataset%20publicly%20available%20on%20GitLab%3A%0Ahttps%3A//gitlab.com/Erez.Posner/stomach_pytorch%20to%20facilitate%20further%20research.%0AThis%20work%20introduces%20the%20first%20statistical%203D%20shape%20model%20of%20the%20stomach%2C%20with%0Aapplications%20ranging%20from%20surgical%20simulation%20and%20pre-operative%20planning%20to%0Amedical%20education%20and%20computational%20modeling.%20By%20combining%20synthetic%20data%0Ageneration%2C%20parametric%20modeling%2C%20and%20real-world%20validation%2C%20our%20approach%0Arepresents%20a%20significant%20advancement%20in%20organ%20modeling%20and%20opens%20new%0Apossibilities%20for%20personalized%20healthcare%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06464v2&entry.124074799=Read"},
{"title": "Visualization and Analysis of the Loss Landscape in Graph Neural\n  Networks", "author": "Samir Moustafa and Lorenz Kummer and Simon Fetzel and Nils M. Kriege and Wilfried N. Gansterer", "abstract": "  Graph Neural Networks (GNNs) are powerful models for graph-structured data,\nwith broad applications. However, the interplay between GNN parameter\noptimization, expressivity, and generalization remains poorly understood. We\naddress this by introducing an efficient learnable dimensionality reduction\nmethod for visualizing GNN loss landscapes, and by analyzing the effects of\nover-smoothing, jumping knowledge, quantization, sparsification, and\npreconditioner on GNN optimization. Our learnable projection method surpasses\nthe state-of-the-art PCA-based approach, enabling accurate reconstruction of\nhigh-dimensional parameters with lower memory usage. We further show that\narchitecture, sparsification, and optimizer's preconditioning significantly\nimpact the GNN optimization landscape and their training process and final\nprediction performance. These insights contribute to developing more efficient\ndesigns of GNN architectures and training strategies.\n", "link": "http://arxiv.org/abs/2509.11792v1", "date": "2025-09-15", "relevancy": 2.6568, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5599}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5205}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visualization%20and%20Analysis%20of%20the%20Loss%20Landscape%20in%20Graph%20Neural%0A%20%20Networks&body=Title%3A%20Visualization%20and%20Analysis%20of%20the%20Loss%20Landscape%20in%20Graph%20Neural%0A%20%20Networks%0AAuthor%3A%20Samir%20Moustafa%20and%20Lorenz%20Kummer%20and%20Simon%20Fetzel%20and%20Nils%20M.%20Kriege%20and%20Wilfried%20N.%20Gansterer%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20powerful%20models%20for%20graph-structured%20data%2C%0Awith%20broad%20applications.%20However%2C%20the%20interplay%20between%20GNN%20parameter%0Aoptimization%2C%20expressivity%2C%20and%20generalization%20remains%20poorly%20understood.%20We%0Aaddress%20this%20by%20introducing%20an%20efficient%20learnable%20dimensionality%20reduction%0Amethod%20for%20visualizing%20GNN%20loss%20landscapes%2C%20and%20by%20analyzing%20the%20effects%20of%0Aover-smoothing%2C%20jumping%20knowledge%2C%20quantization%2C%20sparsification%2C%20and%0Apreconditioner%20on%20GNN%20optimization.%20Our%20learnable%20projection%20method%20surpasses%0Athe%20state-of-the-art%20PCA-based%20approach%2C%20enabling%20accurate%20reconstruction%20of%0Ahigh-dimensional%20parameters%20with%20lower%20memory%20usage.%20We%20further%20show%20that%0Aarchitecture%2C%20sparsification%2C%20and%20optimizer%27s%20preconditioning%20significantly%0Aimpact%20the%20GNN%20optimization%20landscape%20and%20their%20training%20process%20and%20final%0Aprediction%20performance.%20These%20insights%20contribute%20to%20developing%20more%20efficient%0Adesigns%20of%20GNN%20architectures%20and%20training%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualization%2520and%2520Analysis%2520of%2520the%2520Loss%2520Landscape%2520in%2520Graph%2520Neural%250A%2520%2520Networks%26entry.906535625%3DSamir%2520Moustafa%2520and%2520Lorenz%2520Kummer%2520and%2520Simon%2520Fetzel%2520and%2520Nils%2520M.%2520Kriege%2520and%2520Wilfried%2520N.%2520Gansterer%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520powerful%2520models%2520for%2520graph-structured%2520data%252C%250Awith%2520broad%2520applications.%2520However%252C%2520the%2520interplay%2520between%2520GNN%2520parameter%250Aoptimization%252C%2520expressivity%252C%2520and%2520generalization%2520remains%2520poorly%2520understood.%2520We%250Aaddress%2520this%2520by%2520introducing%2520an%2520efficient%2520learnable%2520dimensionality%2520reduction%250Amethod%2520for%2520visualizing%2520GNN%2520loss%2520landscapes%252C%2520and%2520by%2520analyzing%2520the%2520effects%2520of%250Aover-smoothing%252C%2520jumping%2520knowledge%252C%2520quantization%252C%2520sparsification%252C%2520and%250Apreconditioner%2520on%2520GNN%2520optimization.%2520Our%2520learnable%2520projection%2520method%2520surpasses%250Athe%2520state-of-the-art%2520PCA-based%2520approach%252C%2520enabling%2520accurate%2520reconstruction%2520of%250Ahigh-dimensional%2520parameters%2520with%2520lower%2520memory%2520usage.%2520We%2520further%2520show%2520that%250Aarchitecture%252C%2520sparsification%252C%2520and%2520optimizer%2527s%2520preconditioning%2520significantly%250Aimpact%2520the%2520GNN%2520optimization%2520landscape%2520and%2520their%2520training%2520process%2520and%2520final%250Aprediction%2520performance.%2520These%2520insights%2520contribute%2520to%2520developing%2520more%2520efficient%250Adesigns%2520of%2520GNN%2520architectures%2520and%2520training%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visualization%20and%20Analysis%20of%20the%20Loss%20Landscape%20in%20Graph%20Neural%0A%20%20Networks&entry.906535625=Samir%20Moustafa%20and%20Lorenz%20Kummer%20and%20Simon%20Fetzel%20and%20Nils%20M.%20Kriege%20and%20Wilfried%20N.%20Gansterer&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20powerful%20models%20for%20graph-structured%20data%2C%0Awith%20broad%20applications.%20However%2C%20the%20interplay%20between%20GNN%20parameter%0Aoptimization%2C%20expressivity%2C%20and%20generalization%20remains%20poorly%20understood.%20We%0Aaddress%20this%20by%20introducing%20an%20efficient%20learnable%20dimensionality%20reduction%0Amethod%20for%20visualizing%20GNN%20loss%20landscapes%2C%20and%20by%20analyzing%20the%20effects%20of%0Aover-smoothing%2C%20jumping%20knowledge%2C%20quantization%2C%20sparsification%2C%20and%0Apreconditioner%20on%20GNN%20optimization.%20Our%20learnable%20projection%20method%20surpasses%0Athe%20state-of-the-art%20PCA-based%20approach%2C%20enabling%20accurate%20reconstruction%20of%0Ahigh-dimensional%20parameters%20with%20lower%20memory%20usage.%20We%20further%20show%20that%0Aarchitecture%2C%20sparsification%2C%20and%20optimizer%27s%20preconditioning%20significantly%0Aimpact%20the%20GNN%20optimization%20landscape%20and%20their%20training%20process%20and%20final%0Aprediction%20performance.%20These%20insights%20contribute%20to%20developing%20more%20efficient%0Adesigns%20of%20GNN%20architectures%20and%20training%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11792v1&entry.124074799=Read"},
{"title": "Preservation of Language Understanding Capabilities in Speech-aware\n  Large Language Models", "author": "Marek Kubis and Pawe\u0142 Sk\u00f3rzewski and Iwona Christop and Mateusz Czy\u017cnikiewicz and Jakub Kubiak and \u0141ukasz Bondaruk and Marcin Lewandowski", "abstract": "  The paper presents C3T (Cross-modal Capabilities Conservation Test), a new\nbenchmark for assessing the performance of speech-aware large language models.\nThe benchmark utilizes textual tasks and a voice cloning text-to-speech model\nto quantify the extent to which language understanding capabilities are\npreserved when the model is accessed via speech input. C3T quantifies the\nfairness of the model for different categories of speakers and its robustness\nacross text and speech modalities.\n", "link": "http://arxiv.org/abs/2509.12171v1", "date": "2025-09-15", "relevancy": 2.6563, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5389}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5389}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preservation%20of%20Language%20Understanding%20Capabilities%20in%20Speech-aware%0A%20%20Large%20Language%20Models&body=Title%3A%20Preservation%20of%20Language%20Understanding%20Capabilities%20in%20Speech-aware%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Marek%20Kubis%20and%20Pawe%C5%82%20Sk%C3%B3rzewski%20and%20Iwona%20Christop%20and%20Mateusz%20Czy%C5%BCnikiewicz%20and%20Jakub%20Kubiak%20and%20%C5%81ukasz%20Bondaruk%20and%20Marcin%20Lewandowski%0AAbstract%3A%20%20%20The%20paper%20presents%20C3T%20%28Cross-modal%20Capabilities%20Conservation%20Test%29%2C%20a%20new%0Abenchmark%20for%20assessing%20the%20performance%20of%20speech-aware%20large%20language%20models.%0AThe%20benchmark%20utilizes%20textual%20tasks%20and%20a%20voice%20cloning%20text-to-speech%20model%0Ato%20quantify%20the%20extent%20to%20which%20language%20understanding%20capabilities%20are%0Apreserved%20when%20the%20model%20is%20accessed%20via%20speech%20input.%20C3T%20quantifies%20the%0Afairness%20of%20the%20model%20for%20different%20categories%20of%20speakers%20and%20its%20robustness%0Aacross%20text%20and%20speech%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreservation%2520of%2520Language%2520Understanding%2520Capabilities%2520in%2520Speech-aware%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DMarek%2520Kubis%2520and%2520Pawe%25C5%2582%2520Sk%25C3%25B3rzewski%2520and%2520Iwona%2520Christop%2520and%2520Mateusz%2520Czy%25C5%25BCnikiewicz%2520and%2520Jakub%2520Kubiak%2520and%2520%25C5%2581ukasz%2520Bondaruk%2520and%2520Marcin%2520Lewandowski%26entry.1292438233%3D%2520%2520The%2520paper%2520presents%2520C3T%2520%2528Cross-modal%2520Capabilities%2520Conservation%2520Test%2529%252C%2520a%2520new%250Abenchmark%2520for%2520assessing%2520the%2520performance%2520of%2520speech-aware%2520large%2520language%2520models.%250AThe%2520benchmark%2520utilizes%2520textual%2520tasks%2520and%2520a%2520voice%2520cloning%2520text-to-speech%2520model%250Ato%2520quantify%2520the%2520extent%2520to%2520which%2520language%2520understanding%2520capabilities%2520are%250Apreserved%2520when%2520the%2520model%2520is%2520accessed%2520via%2520speech%2520input.%2520C3T%2520quantifies%2520the%250Afairness%2520of%2520the%2520model%2520for%2520different%2520categories%2520of%2520speakers%2520and%2520its%2520robustness%250Aacross%2520text%2520and%2520speech%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preservation%20of%20Language%20Understanding%20Capabilities%20in%20Speech-aware%0A%20%20Large%20Language%20Models&entry.906535625=Marek%20Kubis%20and%20Pawe%C5%82%20Sk%C3%B3rzewski%20and%20Iwona%20Christop%20and%20Mateusz%20Czy%C5%BCnikiewicz%20and%20Jakub%20Kubiak%20and%20%C5%81ukasz%20Bondaruk%20and%20Marcin%20Lewandowski&entry.1292438233=%20%20The%20paper%20presents%20C3T%20%28Cross-modal%20Capabilities%20Conservation%20Test%29%2C%20a%20new%0Abenchmark%20for%20assessing%20the%20performance%20of%20speech-aware%20large%20language%20models.%0AThe%20benchmark%20utilizes%20textual%20tasks%20and%20a%20voice%20cloning%20text-to-speech%20model%0Ato%20quantify%20the%20extent%20to%20which%20language%20understanding%20capabilities%20are%0Apreserved%20when%20the%20model%20is%20accessed%20via%20speech%20input.%20C3T%20quantifies%20the%0Afairness%20of%20the%20model%20for%20different%20categories%20of%20speakers%20and%20its%20robustness%0Aacross%20text%20and%20speech%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12171v1&entry.124074799=Read"},
{"title": "Multi Anatomy X-Ray Foundation Model", "author": "Nishank Singla and Krisztian Koos and Farzin Haddadpour and Amin Honarmandi Shandiz and Lovish Chum and Xiaojian Xu and Qing Jin and Erhan Bas", "abstract": "  X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation\nmodels are limited to chest anatomy and fail to generalize across broader\nclinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray\nfoundation model using self-supervised learning on a large, private dataset of\n1.15 million images spanning diverse anatomical regions and evaluated across 12\ndatasets and 20 downstream tasks, including classification, retrieval,\nsegmentation, localization, visual grounding, and report generation. XR-0\nachieves state-of-the-art performance on most multi-anatomy tasks and remains\ncompetitive on chest-specific benchmarks. Our results demonstrate that\nanatomical diversity and supervision are critical for building robust,\ngeneral-purpose medical vision models, paving the way for scalable and\nadaptable AI systems in radiology.\n", "link": "http://arxiv.org/abs/2509.12146v1", "date": "2025-09-15", "relevancy": 2.6536, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.543}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi%20Anatomy%20X-Ray%20Foundation%20Model&body=Title%3A%20Multi%20Anatomy%20X-Ray%20Foundation%20Model%0AAuthor%3A%20Nishank%20Singla%20and%20Krisztian%20Koos%20and%20Farzin%20Haddadpour%20and%20Amin%20Honarmandi%20Shandiz%20and%20Lovish%20Chum%20and%20Xiaojian%20Xu%20and%20Qing%20Jin%20and%20Erhan%20Bas%0AAbstract%3A%20%20%20X-ray%20imaging%20is%20a%20ubiquitous%20in%20radiology%2C%20yet%20most%20existing%20AI%20foundation%0Amodels%20are%20limited%20to%20chest%20anatomy%20and%20fail%20to%20generalize%20across%20broader%0Aclinical%20tasks.%20In%20this%20work%2C%20we%20introduce%20XR-0%2C%20the%20multi-anatomy%20X-ray%0Afoundation%20model%20using%20self-supervised%20learning%20on%20a%20large%2C%20private%20dataset%20of%0A1.15%20million%20images%20spanning%20diverse%20anatomical%20regions%20and%20evaluated%20across%2012%0Adatasets%20and%2020%20downstream%20tasks%2C%20including%20classification%2C%20retrieval%2C%0Asegmentation%2C%20localization%2C%20visual%20grounding%2C%20and%20report%20generation.%20XR-0%0Aachieves%20state-of-the-art%20performance%20on%20most%20multi-anatomy%20tasks%20and%20remains%0Acompetitive%20on%20chest-specific%20benchmarks.%20Our%20results%20demonstrate%20that%0Aanatomical%20diversity%20and%20supervision%20are%20critical%20for%20building%20robust%2C%0Ageneral-purpose%20medical%20vision%20models%2C%20paving%20the%20way%20for%20scalable%20and%0Aadaptable%20AI%20systems%20in%20radiology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti%2520Anatomy%2520X-Ray%2520Foundation%2520Model%26entry.906535625%3DNishank%2520Singla%2520and%2520Krisztian%2520Koos%2520and%2520Farzin%2520Haddadpour%2520and%2520Amin%2520Honarmandi%2520Shandiz%2520and%2520Lovish%2520Chum%2520and%2520Xiaojian%2520Xu%2520and%2520Qing%2520Jin%2520and%2520Erhan%2520Bas%26entry.1292438233%3D%2520%2520X-ray%2520imaging%2520is%2520a%2520ubiquitous%2520in%2520radiology%252C%2520yet%2520most%2520existing%2520AI%2520foundation%250Amodels%2520are%2520limited%2520to%2520chest%2520anatomy%2520and%2520fail%2520to%2520generalize%2520across%2520broader%250Aclinical%2520tasks.%2520In%2520this%2520work%252C%2520we%2520introduce%2520XR-0%252C%2520the%2520multi-anatomy%2520X-ray%250Afoundation%2520model%2520using%2520self-supervised%2520learning%2520on%2520a%2520large%252C%2520private%2520dataset%2520of%250A1.15%2520million%2520images%2520spanning%2520diverse%2520anatomical%2520regions%2520and%2520evaluated%2520across%252012%250Adatasets%2520and%252020%2520downstream%2520tasks%252C%2520including%2520classification%252C%2520retrieval%252C%250Asegmentation%252C%2520localization%252C%2520visual%2520grounding%252C%2520and%2520report%2520generation.%2520XR-0%250Aachieves%2520state-of-the-art%2520performance%2520on%2520most%2520multi-anatomy%2520tasks%2520and%2520remains%250Acompetitive%2520on%2520chest-specific%2520benchmarks.%2520Our%2520results%2520demonstrate%2520that%250Aanatomical%2520diversity%2520and%2520supervision%2520are%2520critical%2520for%2520building%2520robust%252C%250Ageneral-purpose%2520medical%2520vision%2520models%252C%2520paving%2520the%2520way%2520for%2520scalable%2520and%250Aadaptable%2520AI%2520systems%2520in%2520radiology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi%20Anatomy%20X-Ray%20Foundation%20Model&entry.906535625=Nishank%20Singla%20and%20Krisztian%20Koos%20and%20Farzin%20Haddadpour%20and%20Amin%20Honarmandi%20Shandiz%20and%20Lovish%20Chum%20and%20Xiaojian%20Xu%20and%20Qing%20Jin%20and%20Erhan%20Bas&entry.1292438233=%20%20X-ray%20imaging%20is%20a%20ubiquitous%20in%20radiology%2C%20yet%20most%20existing%20AI%20foundation%0Amodels%20are%20limited%20to%20chest%20anatomy%20and%20fail%20to%20generalize%20across%20broader%0Aclinical%20tasks.%20In%20this%20work%2C%20we%20introduce%20XR-0%2C%20the%20multi-anatomy%20X-ray%0Afoundation%20model%20using%20self-supervised%20learning%20on%20a%20large%2C%20private%20dataset%20of%0A1.15%20million%20images%20spanning%20diverse%20anatomical%20regions%20and%20evaluated%20across%2012%0Adatasets%20and%2020%20downstream%20tasks%2C%20including%20classification%2C%20retrieval%2C%0Asegmentation%2C%20localization%2C%20visual%20grounding%2C%20and%20report%20generation.%20XR-0%0Aachieves%20state-of-the-art%20performance%20on%20most%20multi-anatomy%20tasks%20and%20remains%0Acompetitive%20on%20chest-specific%20benchmarks.%20Our%20results%20demonstrate%20that%0Aanatomical%20diversity%20and%20supervision%20are%20critical%20for%20building%20robust%2C%0Ageneral-purpose%20medical%20vision%20models%2C%20paving%20the%20way%20for%20scalable%20and%0Aadaptable%20AI%20systems%20in%20radiology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12146v1&entry.124074799=Read"},
{"title": "Domain-Adaptive Pretraining Improves Primate Behavior Recognition", "author": "Felix B. Mueller and Timo Lueddecke and Richard Vogg and Alexander S. Ecker", "abstract": "  Computer vision for animal behavior offers promising tools to aid research in\necology, cognition, and to support conservation efforts. Video camera traps\nallow for large-scale data collection, but high labeling costs remain a\nbottleneck to creating large-scale datasets. We thus need data-efficient\nlearning approaches. In this work, we show that we can utilize self-supervised\nlearning to considerably improve action recognition on primate behavior. On two\ndatasets of great ape behavior (PanAf and ChimpACT), we outperform published\nstate-of-the-art action recognition models by 6.1 %pt. accuracy and 6.3 %pt.\nmAP, respectively. We achieve this by utilizing a pretrained V-JEPA model and\napplying domain-adaptive pretraining (DAP), i.e. continuing the pretraining\nwith in-domain data. We show that most of the performance gain stems from the\nDAP. Our method promises great potential for improving the recognition of\nanimal behavior, as DAP does not require labeled samples. Code is available at\nhttps://github.com/ecker-lab/dap-behavior\n", "link": "http://arxiv.org/abs/2509.12193v1", "date": "2025-09-15", "relevancy": 2.6487, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5646}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5368}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain-Adaptive%20Pretraining%20Improves%20Primate%20Behavior%20Recognition&body=Title%3A%20Domain-Adaptive%20Pretraining%20Improves%20Primate%20Behavior%20Recognition%0AAuthor%3A%20Felix%20B.%20Mueller%20and%20Timo%20Lueddecke%20and%20Richard%20Vogg%20and%20Alexander%20S.%20Ecker%0AAbstract%3A%20%20%20Computer%20vision%20for%20animal%20behavior%20offers%20promising%20tools%20to%20aid%20research%20in%0Aecology%2C%20cognition%2C%20and%20to%20support%20conservation%20efforts.%20Video%20camera%20traps%0Aallow%20for%20large-scale%20data%20collection%2C%20but%20high%20labeling%20costs%20remain%20a%0Abottleneck%20to%20creating%20large-scale%20datasets.%20We%20thus%20need%20data-efficient%0Alearning%20approaches.%20In%20this%20work%2C%20we%20show%20that%20we%20can%20utilize%20self-supervised%0Alearning%20to%20considerably%20improve%20action%20recognition%20on%20primate%20behavior.%20On%20two%0Adatasets%20of%20great%20ape%20behavior%20%28PanAf%20and%20ChimpACT%29%2C%20we%20outperform%20published%0Astate-of-the-art%20action%20recognition%20models%20by%206.1%20%25pt.%20accuracy%20and%206.3%20%25pt.%0AmAP%2C%20respectively.%20We%20achieve%20this%20by%20utilizing%20a%20pretrained%20V-JEPA%20model%20and%0Aapplying%20domain-adaptive%20pretraining%20%28DAP%29%2C%20i.e.%20continuing%20the%20pretraining%0Awith%20in-domain%20data.%20We%20show%20that%20most%20of%20the%20performance%20gain%20stems%20from%20the%0ADAP.%20Our%20method%20promises%20great%20potential%20for%20improving%20the%20recognition%20of%0Aanimal%20behavior%2C%20as%20DAP%20does%20not%20require%20labeled%20samples.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ecker-lab/dap-behavior%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12193v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain-Adaptive%2520Pretraining%2520Improves%2520Primate%2520Behavior%2520Recognition%26entry.906535625%3DFelix%2520B.%2520Mueller%2520and%2520Timo%2520Lueddecke%2520and%2520Richard%2520Vogg%2520and%2520Alexander%2520S.%2520Ecker%26entry.1292438233%3D%2520%2520Computer%2520vision%2520for%2520animal%2520behavior%2520offers%2520promising%2520tools%2520to%2520aid%2520research%2520in%250Aecology%252C%2520cognition%252C%2520and%2520to%2520support%2520conservation%2520efforts.%2520Video%2520camera%2520traps%250Aallow%2520for%2520large-scale%2520data%2520collection%252C%2520but%2520high%2520labeling%2520costs%2520remain%2520a%250Abottleneck%2520to%2520creating%2520large-scale%2520datasets.%2520We%2520thus%2520need%2520data-efficient%250Alearning%2520approaches.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520we%2520can%2520utilize%2520self-supervised%250Alearning%2520to%2520considerably%2520improve%2520action%2520recognition%2520on%2520primate%2520behavior.%2520On%2520two%250Adatasets%2520of%2520great%2520ape%2520behavior%2520%2528PanAf%2520and%2520ChimpACT%2529%252C%2520we%2520outperform%2520published%250Astate-of-the-art%2520action%2520recognition%2520models%2520by%25206.1%2520%2525pt.%2520accuracy%2520and%25206.3%2520%2525pt.%250AmAP%252C%2520respectively.%2520We%2520achieve%2520this%2520by%2520utilizing%2520a%2520pretrained%2520V-JEPA%2520model%2520and%250Aapplying%2520domain-adaptive%2520pretraining%2520%2528DAP%2529%252C%2520i.e.%2520continuing%2520the%2520pretraining%250Awith%2520in-domain%2520data.%2520We%2520show%2520that%2520most%2520of%2520the%2520performance%2520gain%2520stems%2520from%2520the%250ADAP.%2520Our%2520method%2520promises%2520great%2520potential%2520for%2520improving%2520the%2520recognition%2520of%250Aanimal%2520behavior%252C%2520as%2520DAP%2520does%2520not%2520require%2520labeled%2520samples.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/ecker-lab/dap-behavior%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12193v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain-Adaptive%20Pretraining%20Improves%20Primate%20Behavior%20Recognition&entry.906535625=Felix%20B.%20Mueller%20and%20Timo%20Lueddecke%20and%20Richard%20Vogg%20and%20Alexander%20S.%20Ecker&entry.1292438233=%20%20Computer%20vision%20for%20animal%20behavior%20offers%20promising%20tools%20to%20aid%20research%20in%0Aecology%2C%20cognition%2C%20and%20to%20support%20conservation%20efforts.%20Video%20camera%20traps%0Aallow%20for%20large-scale%20data%20collection%2C%20but%20high%20labeling%20costs%20remain%20a%0Abottleneck%20to%20creating%20large-scale%20datasets.%20We%20thus%20need%20data-efficient%0Alearning%20approaches.%20In%20this%20work%2C%20we%20show%20that%20we%20can%20utilize%20self-supervised%0Alearning%20to%20considerably%20improve%20action%20recognition%20on%20primate%20behavior.%20On%20two%0Adatasets%20of%20great%20ape%20behavior%20%28PanAf%20and%20ChimpACT%29%2C%20we%20outperform%20published%0Astate-of-the-art%20action%20recognition%20models%20by%206.1%20%25pt.%20accuracy%20and%206.3%20%25pt.%0AmAP%2C%20respectively.%20We%20achieve%20this%20by%20utilizing%20a%20pretrained%20V-JEPA%20model%20and%0Aapplying%20domain-adaptive%20pretraining%20%28DAP%29%2C%20i.e.%20continuing%20the%20pretraining%0Awith%20in-domain%20data.%20We%20show%20that%20most%20of%20the%20performance%20gain%20stems%20from%20the%0ADAP.%20Our%20method%20promises%20great%20potential%20for%20improving%20the%20recognition%20of%0Aanimal%20behavior%2C%20as%20DAP%20does%20not%20require%20labeled%20samples.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ecker-lab/dap-behavior%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12193v1&entry.124074799=Read"},
{"title": "Learning Representations in Video Game Agents with Supervised\n  Contrastive Imitation Learning", "author": "Carlos Celemin and Joseph Brennan and Pierluigi Vito Amadori and Tim Bradley", "abstract": "  This paper introduces a novel application of Supervised Contrastive Learning\n(SupCon) to Imitation Learning (IL), with a focus on learning more effective\nstate representations for agents in video game environments. The goal is to\nobtain latent representations of the observations that capture better the\naction-relevant factors, thereby modeling better the cause-effect relationship\nfrom the observations that are mapped to the actions performed by the\ndemonstrator, for example, the player jumps whenever an obstacle appears ahead.\nWe propose an approach to integrate the SupCon loss with continuous output\nspaces, enabling SupCon to operate without constraints regarding the type of\nactions of the environment. Experiments on the 3D games Astro Bot and Returnal,\nand multiple 2D Atari games show improved representation quality, faster\nlearning convergence, and better generalization compared to baseline models\ntrained only with supervised action prediction loss functions.\n", "link": "http://arxiv.org/abs/2509.11880v1", "date": "2025-09-15", "relevancy": 2.6378, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5385}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5325}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Representations%20in%20Video%20Game%20Agents%20with%20Supervised%0A%20%20Contrastive%20Imitation%20Learning&body=Title%3A%20Learning%20Representations%20in%20Video%20Game%20Agents%20with%20Supervised%0A%20%20Contrastive%20Imitation%20Learning%0AAuthor%3A%20Carlos%20Celemin%20and%20Joseph%20Brennan%20and%20Pierluigi%20Vito%20Amadori%20and%20Tim%20Bradley%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20application%20of%20Supervised%20Contrastive%20Learning%0A%28SupCon%29%20to%20Imitation%20Learning%20%28IL%29%2C%20with%20a%20focus%20on%20learning%20more%20effective%0Astate%20representations%20for%20agents%20in%20video%20game%20environments.%20The%20goal%20is%20to%0Aobtain%20latent%20representations%20of%20the%20observations%20that%20capture%20better%20the%0Aaction-relevant%20factors%2C%20thereby%20modeling%20better%20the%20cause-effect%20relationship%0Afrom%20the%20observations%20that%20are%20mapped%20to%20the%20actions%20performed%20by%20the%0Ademonstrator%2C%20for%20example%2C%20the%20player%20jumps%20whenever%20an%20obstacle%20appears%20ahead.%0AWe%20propose%20an%20approach%20to%20integrate%20the%20SupCon%20loss%20with%20continuous%20output%0Aspaces%2C%20enabling%20SupCon%20to%20operate%20without%20constraints%20regarding%20the%20type%20of%0Aactions%20of%20the%20environment.%20Experiments%20on%20the%203D%20games%20Astro%20Bot%20and%20Returnal%2C%0Aand%20multiple%202D%20Atari%20games%20show%20improved%20representation%20quality%2C%20faster%0Alearning%20convergence%2C%20and%20better%20generalization%20compared%20to%20baseline%20models%0Atrained%20only%20with%20supervised%20action%20prediction%20loss%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Representations%2520in%2520Video%2520Game%2520Agents%2520with%2520Supervised%250A%2520%2520Contrastive%2520Imitation%2520Learning%26entry.906535625%3DCarlos%2520Celemin%2520and%2520Joseph%2520Brennan%2520and%2520Pierluigi%2520Vito%2520Amadori%2520and%2520Tim%2520Bradley%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520application%2520of%2520Supervised%2520Contrastive%2520Learning%250A%2528SupCon%2529%2520to%2520Imitation%2520Learning%2520%2528IL%2529%252C%2520with%2520a%2520focus%2520on%2520learning%2520more%2520effective%250Astate%2520representations%2520for%2520agents%2520in%2520video%2520game%2520environments.%2520The%2520goal%2520is%2520to%250Aobtain%2520latent%2520representations%2520of%2520the%2520observations%2520that%2520capture%2520better%2520the%250Aaction-relevant%2520factors%252C%2520thereby%2520modeling%2520better%2520the%2520cause-effect%2520relationship%250Afrom%2520the%2520observations%2520that%2520are%2520mapped%2520to%2520the%2520actions%2520performed%2520by%2520the%250Ademonstrator%252C%2520for%2520example%252C%2520the%2520player%2520jumps%2520whenever%2520an%2520obstacle%2520appears%2520ahead.%250AWe%2520propose%2520an%2520approach%2520to%2520integrate%2520the%2520SupCon%2520loss%2520with%2520continuous%2520output%250Aspaces%252C%2520enabling%2520SupCon%2520to%2520operate%2520without%2520constraints%2520regarding%2520the%2520type%2520of%250Aactions%2520of%2520the%2520environment.%2520Experiments%2520on%2520the%25203D%2520games%2520Astro%2520Bot%2520and%2520Returnal%252C%250Aand%2520multiple%25202D%2520Atari%2520games%2520show%2520improved%2520representation%2520quality%252C%2520faster%250Alearning%2520convergence%252C%2520and%2520better%2520generalization%2520compared%2520to%2520baseline%2520models%250Atrained%2520only%2520with%2520supervised%2520action%2520prediction%2520loss%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Representations%20in%20Video%20Game%20Agents%20with%20Supervised%0A%20%20Contrastive%20Imitation%20Learning&entry.906535625=Carlos%20Celemin%20and%20Joseph%20Brennan%20and%20Pierluigi%20Vito%20Amadori%20and%20Tim%20Bradley&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20application%20of%20Supervised%20Contrastive%20Learning%0A%28SupCon%29%20to%20Imitation%20Learning%20%28IL%29%2C%20with%20a%20focus%20on%20learning%20more%20effective%0Astate%20representations%20for%20agents%20in%20video%20game%20environments.%20The%20goal%20is%20to%0Aobtain%20latent%20representations%20of%20the%20observations%20that%20capture%20better%20the%0Aaction-relevant%20factors%2C%20thereby%20modeling%20better%20the%20cause-effect%20relationship%0Afrom%20the%20observations%20that%20are%20mapped%20to%20the%20actions%20performed%20by%20the%0Ademonstrator%2C%20for%20example%2C%20the%20player%20jumps%20whenever%20an%20obstacle%20appears%20ahead.%0AWe%20propose%20an%20approach%20to%20integrate%20the%20SupCon%20loss%20with%20continuous%20output%0Aspaces%2C%20enabling%20SupCon%20to%20operate%20without%20constraints%20regarding%20the%20type%20of%0Aactions%20of%20the%20environment.%20Experiments%20on%20the%203D%20games%20Astro%20Bot%20and%20Returnal%2C%0Aand%20multiple%202D%20Atari%20games%20show%20improved%20representation%20quality%2C%20faster%0Alearning%20convergence%2C%20and%20better%20generalization%20compared%20to%20baseline%20models%0Atrained%20only%20with%20supervised%20action%20prediction%20loss%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11880v1&entry.124074799=Read"},
{"title": "When marine radar target detection meets pretrained large language\n  models", "author": "Qiying Hu and Linping Zhang and Xueqian Wang and Gang Li and Yu Liu and Xiao-Ping Zhang", "abstract": "  Deep learning (DL) methods are widely used to extract high-dimensional\npatterns from the sequence features of radar echo signals. However,\nconventional DL algorithms face challenges such as redundant feature segments,\nand constraints from restricted model sizes. To address these issues, we\npropose a framework that integrates feature preprocessing with large language\nmodels (LLMs). Our preprocessing module tokenizes radar sequence features,\napplies a patch selection algorithm to filter out uninformative segments, and\nprojects the selected patches into embeddings compatible with the feature space\nof pre-trained LLMs. Leveraging these refined embeddings, we incorporate a\npre-trained LLM, fine-tuning only the normalization layers to reduce training\nburdens while enhancing performance. Experiments on measured datasets\ndemonstrate that the proposed method significantly outperforms the\nstate-of-the-art baselines on supervised learning tests.\n", "link": "http://arxiv.org/abs/2509.12110v1", "date": "2025-09-15", "relevancy": 2.6342, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20marine%20radar%20target%20detection%20meets%20pretrained%20large%20language%0A%20%20models&body=Title%3A%20When%20marine%20radar%20target%20detection%20meets%20pretrained%20large%20language%0A%20%20models%0AAuthor%3A%20Qiying%20Hu%20and%20Linping%20Zhang%20and%20Xueqian%20Wang%20and%20Gang%20Li%20and%20Yu%20Liu%20and%20Xiao-Ping%20Zhang%0AAbstract%3A%20%20%20Deep%20learning%20%28DL%29%20methods%20are%20widely%20used%20to%20extract%20high-dimensional%0Apatterns%20from%20the%20sequence%20features%20of%20radar%20echo%20signals.%20However%2C%0Aconventional%20DL%20algorithms%20face%20challenges%20such%20as%20redundant%20feature%20segments%2C%0Aand%20constraints%20from%20restricted%20model%20sizes.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20framework%20that%20integrates%20feature%20preprocessing%20with%20large%20language%0Amodels%20%28LLMs%29.%20Our%20preprocessing%20module%20tokenizes%20radar%20sequence%20features%2C%0Aapplies%20a%20patch%20selection%20algorithm%20to%20filter%20out%20uninformative%20segments%2C%20and%0Aprojects%20the%20selected%20patches%20into%20embeddings%20compatible%20with%20the%20feature%20space%0Aof%20pre-trained%20LLMs.%20Leveraging%20these%20refined%20embeddings%2C%20we%20incorporate%20a%0Apre-trained%20LLM%2C%20fine-tuning%20only%20the%20normalization%20layers%20to%20reduce%20training%0Aburdens%20while%20enhancing%20performance.%20Experiments%20on%20measured%20datasets%0Ademonstrate%20that%20the%20proposed%20method%20significantly%20outperforms%20the%0Astate-of-the-art%20baselines%20on%20supervised%20learning%20tests.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520marine%2520radar%2520target%2520detection%2520meets%2520pretrained%2520large%2520language%250A%2520%2520models%26entry.906535625%3DQiying%2520Hu%2520and%2520Linping%2520Zhang%2520and%2520Xueqian%2520Wang%2520and%2520Gang%2520Li%2520and%2520Yu%2520Liu%2520and%2520Xiao-Ping%2520Zhang%26entry.1292438233%3D%2520%2520Deep%2520learning%2520%2528DL%2529%2520methods%2520are%2520widely%2520used%2520to%2520extract%2520high-dimensional%250Apatterns%2520from%2520the%2520sequence%2520features%2520of%2520radar%2520echo%2520signals.%2520However%252C%250Aconventional%2520DL%2520algorithms%2520face%2520challenges%2520such%2520as%2520redundant%2520feature%2520segments%252C%250Aand%2520constraints%2520from%2520restricted%2520model%2520sizes.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520a%2520framework%2520that%2520integrates%2520feature%2520preprocessing%2520with%2520large%2520language%250Amodels%2520%2528LLMs%2529.%2520Our%2520preprocessing%2520module%2520tokenizes%2520radar%2520sequence%2520features%252C%250Aapplies%2520a%2520patch%2520selection%2520algorithm%2520to%2520filter%2520out%2520uninformative%2520segments%252C%2520and%250Aprojects%2520the%2520selected%2520patches%2520into%2520embeddings%2520compatible%2520with%2520the%2520feature%2520space%250Aof%2520pre-trained%2520LLMs.%2520Leveraging%2520these%2520refined%2520embeddings%252C%2520we%2520incorporate%2520a%250Apre-trained%2520LLM%252C%2520fine-tuning%2520only%2520the%2520normalization%2520layers%2520to%2520reduce%2520training%250Aburdens%2520while%2520enhancing%2520performance.%2520Experiments%2520on%2520measured%2520datasets%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520significantly%2520outperforms%2520the%250Astate-of-the-art%2520baselines%2520on%2520supervised%2520learning%2520tests.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20marine%20radar%20target%20detection%20meets%20pretrained%20large%20language%0A%20%20models&entry.906535625=Qiying%20Hu%20and%20Linping%20Zhang%20and%20Xueqian%20Wang%20and%20Gang%20Li%20and%20Yu%20Liu%20and%20Xiao-Ping%20Zhang&entry.1292438233=%20%20Deep%20learning%20%28DL%29%20methods%20are%20widely%20used%20to%20extract%20high-dimensional%0Apatterns%20from%20the%20sequence%20features%20of%20radar%20echo%20signals.%20However%2C%0Aconventional%20DL%20algorithms%20face%20challenges%20such%20as%20redundant%20feature%20segments%2C%0Aand%20constraints%20from%20restricted%20model%20sizes.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20framework%20that%20integrates%20feature%20preprocessing%20with%20large%20language%0Amodels%20%28LLMs%29.%20Our%20preprocessing%20module%20tokenizes%20radar%20sequence%20features%2C%0Aapplies%20a%20patch%20selection%20algorithm%20to%20filter%20out%20uninformative%20segments%2C%20and%0Aprojects%20the%20selected%20patches%20into%20embeddings%20compatible%20with%20the%20feature%20space%0Aof%20pre-trained%20LLMs.%20Leveraging%20these%20refined%20embeddings%2C%20we%20incorporate%20a%0Apre-trained%20LLM%2C%20fine-tuning%20only%20the%20normalization%20layers%20to%20reduce%20training%0Aburdens%20while%20enhancing%20performance.%20Experiments%20on%20measured%20datasets%0Ademonstrate%20that%20the%20proposed%20method%20significantly%20outperforms%20the%0Astate-of-the-art%20baselines%20on%20supervised%20learning%20tests.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12110v1&entry.124074799=Read"},
{"title": "Video Signature: In-generation Watermarking for Latent Video Diffusion\n  Models", "author": "Yu Huang and Junhao Chen and Shuliang Liu and Hanqian Li and Qi Zheng and Yi R. Fung and Xuming Hu", "abstract": "  The rapid development of Artificial Intelligence Generated Content (AIGC) has\nled to significant progress in video generation but also raises serious\nconcerns about intellectual property protection and reliable content tracing.\nWatermarking is a widely adopted solution to this issue, but existing methods\nfor video generation mainly follow a post-generation paradigm, which introduces\nadditional computational overhead and often fails to effectively balance the\ntrade-off between video quality and watermark extraction. To address these\nissues, we propose Video Signature (VIDSIG), an in-generation watermarking\nmethod for latent video diffusion models, which enables implicit and adaptive\nwatermark integration during generation. Specifically, we achieve this by\npartially fine-tuning the latent decoder, where Perturbation-Aware Suppression\n(PAS) pre-identifies and freezes perceptually sensitive layers to preserve\nvisual quality. Beyond spatial fidelity, we further enhance temporal\nconsistency by introducing a lightweight Temporal Alignment module that guides\nthe decoder to generate coherent frame sequences during fine-tuning.\nExperimental results show that VIDSIG achieves the best overall performance in\nwatermark extraction, visual quality, and generation efficiency. It also\ndemonstrates strong robustness against both spatial and temporal tampering,\nhighlighting its practicality in real-world scenarios. Our code is available at\n\\href{https://github.com/hardenyu21/Video-Signature}{here}\n", "link": "http://arxiv.org/abs/2506.00652v3", "date": "2025-09-15", "relevancy": 2.6214, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6935}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.639}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Signature%3A%20In-generation%20Watermarking%20for%20Latent%20Video%20Diffusion%0A%20%20Models&body=Title%3A%20Video%20Signature%3A%20In-generation%20Watermarking%20for%20Latent%20Video%20Diffusion%0A%20%20Models%0AAuthor%3A%20Yu%20Huang%20and%20Junhao%20Chen%20and%20Shuliang%20Liu%20and%20Hanqian%20Li%20and%20Qi%20Zheng%20and%20Yi%20R.%20Fung%20and%20Xuming%20Hu%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20Artificial%20Intelligence%20Generated%20Content%20%28AIGC%29%20has%0Aled%20to%20significant%20progress%20in%20video%20generation%20but%20also%20raises%20serious%0Aconcerns%20about%20intellectual%20property%20protection%20and%20reliable%20content%20tracing.%0AWatermarking%20is%20a%20widely%20adopted%20solution%20to%20this%20issue%2C%20but%20existing%20methods%0Afor%20video%20generation%20mainly%20follow%20a%20post-generation%20paradigm%2C%20which%20introduces%0Aadditional%20computational%20overhead%20and%20often%20fails%20to%20effectively%20balance%20the%0Atrade-off%20between%20video%20quality%20and%20watermark%20extraction.%20To%20address%20these%0Aissues%2C%20we%20propose%20Video%20Signature%20%28VIDSIG%29%2C%20an%20in-generation%20watermarking%0Amethod%20for%20latent%20video%20diffusion%20models%2C%20which%20enables%20implicit%20and%20adaptive%0Awatermark%20integration%20during%20generation.%20Specifically%2C%20we%20achieve%20this%20by%0Apartially%20fine-tuning%20the%20latent%20decoder%2C%20where%20Perturbation-Aware%20Suppression%0A%28PAS%29%20pre-identifies%20and%20freezes%20perceptually%20sensitive%20layers%20to%20preserve%0Avisual%20quality.%20Beyond%20spatial%20fidelity%2C%20we%20further%20enhance%20temporal%0Aconsistency%20by%20introducing%20a%20lightweight%20Temporal%20Alignment%20module%20that%20guides%0Athe%20decoder%20to%20generate%20coherent%20frame%20sequences%20during%20fine-tuning.%0AExperimental%20results%20show%20that%20VIDSIG%20achieves%20the%20best%20overall%20performance%20in%0Awatermark%20extraction%2C%20visual%20quality%2C%20and%20generation%20efficiency.%20It%20also%0Ademonstrates%20strong%20robustness%20against%20both%20spatial%20and%20temporal%20tampering%2C%0Ahighlighting%20its%20practicality%20in%20real-world%20scenarios.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/hardenyu21/Video-Signature%7D%7Bhere%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00652v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Signature%253A%2520In-generation%2520Watermarking%2520for%2520Latent%2520Video%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DYu%2520Huang%2520and%2520Junhao%2520Chen%2520and%2520Shuliang%2520Liu%2520and%2520Hanqian%2520Li%2520and%2520Qi%2520Zheng%2520and%2520Yi%2520R.%2520Fung%2520and%2520Xuming%2520Hu%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520Artificial%2520Intelligence%2520Generated%2520Content%2520%2528AIGC%2529%2520has%250Aled%2520to%2520significant%2520progress%2520in%2520video%2520generation%2520but%2520also%2520raises%2520serious%250Aconcerns%2520about%2520intellectual%2520property%2520protection%2520and%2520reliable%2520content%2520tracing.%250AWatermarking%2520is%2520a%2520widely%2520adopted%2520solution%2520to%2520this%2520issue%252C%2520but%2520existing%2520methods%250Afor%2520video%2520generation%2520mainly%2520follow%2520a%2520post-generation%2520paradigm%252C%2520which%2520introduces%250Aadditional%2520computational%2520overhead%2520and%2520often%2520fails%2520to%2520effectively%2520balance%2520the%250Atrade-off%2520between%2520video%2520quality%2520and%2520watermark%2520extraction.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520Video%2520Signature%2520%2528VIDSIG%2529%252C%2520an%2520in-generation%2520watermarking%250Amethod%2520for%2520latent%2520video%2520diffusion%2520models%252C%2520which%2520enables%2520implicit%2520and%2520adaptive%250Awatermark%2520integration%2520during%2520generation.%2520Specifically%252C%2520we%2520achieve%2520this%2520by%250Apartially%2520fine-tuning%2520the%2520latent%2520decoder%252C%2520where%2520Perturbation-Aware%2520Suppression%250A%2528PAS%2529%2520pre-identifies%2520and%2520freezes%2520perceptually%2520sensitive%2520layers%2520to%2520preserve%250Avisual%2520quality.%2520Beyond%2520spatial%2520fidelity%252C%2520we%2520further%2520enhance%2520temporal%250Aconsistency%2520by%2520introducing%2520a%2520lightweight%2520Temporal%2520Alignment%2520module%2520that%2520guides%250Athe%2520decoder%2520to%2520generate%2520coherent%2520frame%2520sequences%2520during%2520fine-tuning.%250AExperimental%2520results%2520show%2520that%2520VIDSIG%2520achieves%2520the%2520best%2520overall%2520performance%2520in%250Awatermark%2520extraction%252C%2520visual%2520quality%252C%2520and%2520generation%2520efficiency.%2520It%2520also%250Ademonstrates%2520strong%2520robustness%2520against%2520both%2520spatial%2520and%2520temporal%2520tampering%252C%250Ahighlighting%2520its%2520practicality%2520in%2520real-world%2520scenarios.%2520Our%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/hardenyu21/Video-Signature%257D%257Bhere%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00652v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Signature%3A%20In-generation%20Watermarking%20for%20Latent%20Video%20Diffusion%0A%20%20Models&entry.906535625=Yu%20Huang%20and%20Junhao%20Chen%20and%20Shuliang%20Liu%20and%20Hanqian%20Li%20and%20Qi%20Zheng%20and%20Yi%20R.%20Fung%20and%20Xuming%20Hu&entry.1292438233=%20%20The%20rapid%20development%20of%20Artificial%20Intelligence%20Generated%20Content%20%28AIGC%29%20has%0Aled%20to%20significant%20progress%20in%20video%20generation%20but%20also%20raises%20serious%0Aconcerns%20about%20intellectual%20property%20protection%20and%20reliable%20content%20tracing.%0AWatermarking%20is%20a%20widely%20adopted%20solution%20to%20this%20issue%2C%20but%20existing%20methods%0Afor%20video%20generation%20mainly%20follow%20a%20post-generation%20paradigm%2C%20which%20introduces%0Aadditional%20computational%20overhead%20and%20often%20fails%20to%20effectively%20balance%20the%0Atrade-off%20between%20video%20quality%20and%20watermark%20extraction.%20To%20address%20these%0Aissues%2C%20we%20propose%20Video%20Signature%20%28VIDSIG%29%2C%20an%20in-generation%20watermarking%0Amethod%20for%20latent%20video%20diffusion%20models%2C%20which%20enables%20implicit%20and%20adaptive%0Awatermark%20integration%20during%20generation.%20Specifically%2C%20we%20achieve%20this%20by%0Apartially%20fine-tuning%20the%20latent%20decoder%2C%20where%20Perturbation-Aware%20Suppression%0A%28PAS%29%20pre-identifies%20and%20freezes%20perceptually%20sensitive%20layers%20to%20preserve%0Avisual%20quality.%20Beyond%20spatial%20fidelity%2C%20we%20further%20enhance%20temporal%0Aconsistency%20by%20introducing%20a%20lightweight%20Temporal%20Alignment%20module%20that%20guides%0Athe%20decoder%20to%20generate%20coherent%20frame%20sequences%20during%20fine-tuning.%0AExperimental%20results%20show%20that%20VIDSIG%20achieves%20the%20best%20overall%20performance%20in%0Awatermark%20extraction%2C%20visual%20quality%2C%20and%20generation%20efficiency.%20It%20also%0Ademonstrates%20strong%20robustness%20against%20both%20spatial%20and%20temporal%20tampering%2C%0Ahighlighting%20its%20practicality%20in%20real-world%20scenarios.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/hardenyu21/Video-Signature%7D%7Bhere%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00652v3&entry.124074799=Read"},
{"title": "3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph\n  Learning Framework for Major Depressive Disorder Detection Using Structural\n  MRI Data", "author": "Nojod M. Alotaibi and Areej M. Alhothali and Manar S. Ali", "abstract": "  Major depressive disorder (MDD) is a prevalent mental health condition that\nnegatively impacts both individual well-being and global public health.\nAutomated detection of MDD using structural magnetic resonance imaging (sMRI)\nand deep learning (DL) methods holds increasing promise for improving\ndiagnostic accuracy and enabling early intervention. Most existing methods\nemploy either voxel-level features or handcrafted regional representations\nbuilt from predefined brain atlases, limiting their ability to capture complex\nbrain patterns. This paper develops a unified pipeline that utilizes Vision\nTransformers (ViTs) for extracting 3D region embeddings from sMRI data and\nGraph Neural Network (GNN) for classification. We explore two strategies for\ndefining regions: (1) an atlas-based approach using predefined structural and\nfunctional brain atlases, and (2) an cube-based method by which ViTs are\ntrained directly to identify regions from uniformly extracted 3D patches.\nFurther, cosine similarity graphs are generated to model interregional\nrelationships, and guide GNN-based classification. Extensive experiments were\nconducted using the REST-meta-MDD dataset to demonstrate the effectiveness of\nour model. With stratified 10-fold cross-validation, the best model obtained\n78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and\n78.98% F1-score. Further, atlas-based models consistently outperformed the\ncube-based approach, highlighting the importance of using domain-specific\nanatomical priors for MDD detection.\n", "link": "http://arxiv.org/abs/2509.12143v1", "date": "2025-09-15", "relevancy": 2.612, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5301}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5193}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DViT-GAT%3A%20A%20Unified%20Atlas-Based%203D%20Vision%20Transformer%20and%20Graph%0A%20%20Learning%20Framework%20for%20Major%20Depressive%20Disorder%20Detection%20Using%20Structural%0A%20%20MRI%20Data&body=Title%3A%203DViT-GAT%3A%20A%20Unified%20Atlas-Based%203D%20Vision%20Transformer%20and%20Graph%0A%20%20Learning%20Framework%20for%20Major%20Depressive%20Disorder%20Detection%20Using%20Structural%0A%20%20MRI%20Data%0AAuthor%3A%20Nojod%20M.%20Alotaibi%20and%20Areej%20M.%20Alhothali%20and%20Manar%20S.%20Ali%0AAbstract%3A%20%20%20Major%20depressive%20disorder%20%28MDD%29%20is%20a%20prevalent%20mental%20health%20condition%20that%0Anegatively%20impacts%20both%20individual%20well-being%20and%20global%20public%20health.%0AAutomated%20detection%20of%20MDD%20using%20structural%20magnetic%20resonance%20imaging%20%28sMRI%29%0Aand%20deep%20learning%20%28DL%29%20methods%20holds%20increasing%20promise%20for%20improving%0Adiagnostic%20accuracy%20and%20enabling%20early%20intervention.%20Most%20existing%20methods%0Aemploy%20either%20voxel-level%20features%20or%20handcrafted%20regional%20representations%0Abuilt%20from%20predefined%20brain%20atlases%2C%20limiting%20their%20ability%20to%20capture%20complex%0Abrain%20patterns.%20This%20paper%20develops%20a%20unified%20pipeline%20that%20utilizes%20Vision%0ATransformers%20%28ViTs%29%20for%20extracting%203D%20region%20embeddings%20from%20sMRI%20data%20and%0AGraph%20Neural%20Network%20%28GNN%29%20for%20classification.%20We%20explore%20two%20strategies%20for%0Adefining%20regions%3A%20%281%29%20an%20atlas-based%20approach%20using%20predefined%20structural%20and%0Afunctional%20brain%20atlases%2C%20and%20%282%29%20an%20cube-based%20method%20by%20which%20ViTs%20are%0Atrained%20directly%20to%20identify%20regions%20from%20uniformly%20extracted%203D%20patches.%0AFurther%2C%20cosine%20similarity%20graphs%20are%20generated%20to%20model%20interregional%0Arelationships%2C%20and%20guide%20GNN-based%20classification.%20Extensive%20experiments%20were%0Aconducted%20using%20the%20REST-meta-MDD%20dataset%20to%20demonstrate%20the%20effectiveness%20of%0Aour%20model.%20With%20stratified%2010-fold%20cross-validation%2C%20the%20best%20model%20obtained%0A78.98%25%20accuracy%2C%2076.54%25%20sensitivity%2C%2081.58%25%20specificity%2C%2081.58%25%20precision%2C%20and%0A78.98%25%20F1-score.%20Further%2C%20atlas-based%20models%20consistently%20outperformed%20the%0Acube-based%20approach%2C%20highlighting%20the%20importance%20of%20using%20domain-specific%0Aanatomical%20priors%20for%20MDD%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DViT-GAT%253A%2520A%2520Unified%2520Atlas-Based%25203D%2520Vision%2520Transformer%2520and%2520Graph%250A%2520%2520Learning%2520Framework%2520for%2520Major%2520Depressive%2520Disorder%2520Detection%2520Using%2520Structural%250A%2520%2520MRI%2520Data%26entry.906535625%3DNojod%2520M.%2520Alotaibi%2520and%2520Areej%2520M.%2520Alhothali%2520and%2520Manar%2520S.%2520Ali%26entry.1292438233%3D%2520%2520Major%2520depressive%2520disorder%2520%2528MDD%2529%2520is%2520a%2520prevalent%2520mental%2520health%2520condition%2520that%250Anegatively%2520impacts%2520both%2520individual%2520well-being%2520and%2520global%2520public%2520health.%250AAutomated%2520detection%2520of%2520MDD%2520using%2520structural%2520magnetic%2520resonance%2520imaging%2520%2528sMRI%2529%250Aand%2520deep%2520learning%2520%2528DL%2529%2520methods%2520holds%2520increasing%2520promise%2520for%2520improving%250Adiagnostic%2520accuracy%2520and%2520enabling%2520early%2520intervention.%2520Most%2520existing%2520methods%250Aemploy%2520either%2520voxel-level%2520features%2520or%2520handcrafted%2520regional%2520representations%250Abuilt%2520from%2520predefined%2520brain%2520atlases%252C%2520limiting%2520their%2520ability%2520to%2520capture%2520complex%250Abrain%2520patterns.%2520This%2520paper%2520develops%2520a%2520unified%2520pipeline%2520that%2520utilizes%2520Vision%250ATransformers%2520%2528ViTs%2529%2520for%2520extracting%25203D%2520region%2520embeddings%2520from%2520sMRI%2520data%2520and%250AGraph%2520Neural%2520Network%2520%2528GNN%2529%2520for%2520classification.%2520We%2520explore%2520two%2520strategies%2520for%250Adefining%2520regions%253A%2520%25281%2529%2520an%2520atlas-based%2520approach%2520using%2520predefined%2520structural%2520and%250Afunctional%2520brain%2520atlases%252C%2520and%2520%25282%2529%2520an%2520cube-based%2520method%2520by%2520which%2520ViTs%2520are%250Atrained%2520directly%2520to%2520identify%2520regions%2520from%2520uniformly%2520extracted%25203D%2520patches.%250AFurther%252C%2520cosine%2520similarity%2520graphs%2520are%2520generated%2520to%2520model%2520interregional%250Arelationships%252C%2520and%2520guide%2520GNN-based%2520classification.%2520Extensive%2520experiments%2520were%250Aconducted%2520using%2520the%2520REST-meta-MDD%2520dataset%2520to%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520model.%2520With%2520stratified%252010-fold%2520cross-validation%252C%2520the%2520best%2520model%2520obtained%250A78.98%2525%2520accuracy%252C%252076.54%2525%2520sensitivity%252C%252081.58%2525%2520specificity%252C%252081.58%2525%2520precision%252C%2520and%250A78.98%2525%2520F1-score.%2520Further%252C%2520atlas-based%2520models%2520consistently%2520outperformed%2520the%250Acube-based%2520approach%252C%2520highlighting%2520the%2520importance%2520of%2520using%2520domain-specific%250Aanatomical%2520priors%2520for%2520MDD%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DViT-GAT%3A%20A%20Unified%20Atlas-Based%203D%20Vision%20Transformer%20and%20Graph%0A%20%20Learning%20Framework%20for%20Major%20Depressive%20Disorder%20Detection%20Using%20Structural%0A%20%20MRI%20Data&entry.906535625=Nojod%20M.%20Alotaibi%20and%20Areej%20M.%20Alhothali%20and%20Manar%20S.%20Ali&entry.1292438233=%20%20Major%20depressive%20disorder%20%28MDD%29%20is%20a%20prevalent%20mental%20health%20condition%20that%0Anegatively%20impacts%20both%20individual%20well-being%20and%20global%20public%20health.%0AAutomated%20detection%20of%20MDD%20using%20structural%20magnetic%20resonance%20imaging%20%28sMRI%29%0Aand%20deep%20learning%20%28DL%29%20methods%20holds%20increasing%20promise%20for%20improving%0Adiagnostic%20accuracy%20and%20enabling%20early%20intervention.%20Most%20existing%20methods%0Aemploy%20either%20voxel-level%20features%20or%20handcrafted%20regional%20representations%0Abuilt%20from%20predefined%20brain%20atlases%2C%20limiting%20their%20ability%20to%20capture%20complex%0Abrain%20patterns.%20This%20paper%20develops%20a%20unified%20pipeline%20that%20utilizes%20Vision%0ATransformers%20%28ViTs%29%20for%20extracting%203D%20region%20embeddings%20from%20sMRI%20data%20and%0AGraph%20Neural%20Network%20%28GNN%29%20for%20classification.%20We%20explore%20two%20strategies%20for%0Adefining%20regions%3A%20%281%29%20an%20atlas-based%20approach%20using%20predefined%20structural%20and%0Afunctional%20brain%20atlases%2C%20and%20%282%29%20an%20cube-based%20method%20by%20which%20ViTs%20are%0Atrained%20directly%20to%20identify%20regions%20from%20uniformly%20extracted%203D%20patches.%0AFurther%2C%20cosine%20similarity%20graphs%20are%20generated%20to%20model%20interregional%0Arelationships%2C%20and%20guide%20GNN-based%20classification.%20Extensive%20experiments%20were%0Aconducted%20using%20the%20REST-meta-MDD%20dataset%20to%20demonstrate%20the%20effectiveness%20of%0Aour%20model.%20With%20stratified%2010-fold%20cross-validation%2C%20the%20best%20model%20obtained%0A78.98%25%20accuracy%2C%2076.54%25%20sensitivity%2C%2081.58%25%20specificity%2C%2081.58%25%20precision%2C%20and%0A78.98%25%20F1-score.%20Further%2C%20atlas-based%20models%20consistently%20outperformed%20the%0Acube-based%20approach%2C%20highlighting%20the%20importance%20of%20using%20domain-specific%0Aanatomical%20priors%20for%20MDD%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12143v1&entry.124074799=Read"},
{"title": "Task-Focused Consolidation with Spaced Recall: Making Neural Networks\n  Learn like College Students", "author": "Prital Bamnodkar", "abstract": "  Deep neural networks often suffer from a critical limitation known as\ncatastrophic forgetting, where performance on past tasks degrades after\nlearning new ones. This paper introduces a novel continual learning approach\ninspired by human learning strategies like Active Recall, Deliberate Practice,\nand Spaced Repetition, named Task-Focused Consolidation with Spaced Recall\n(TFC-SR). TFC-SR enhances the standard experience replay framework with a\nmechanism we term the Active Recall Probe. It is a periodic, task-aware\nevaluation of the model's memory that stabilizes the representations of past\nknowledge. We test TFC-SR on the Split MNIST and the Split CIFAR-100 benchmarks\nagainst leading regularization-based and replay-based baselines. Our results\nshow that TFC-SR performs significantly better than these methods. For\ninstance, on the Split CIFAR-100, it achieves a final accuracy of 13.17%\ncompared to Standard Experience Replay's 7.40%. We demonstrate that this\nadvantage comes from the stabilizing effect of the probe itself, and not from\nthe difference in replay volume. Additionally, we analyze the trade-off between\nmemory size and performance and show that while TFC-SR performs better in\nmemory-constrained environments, higher replay volume is still more effective\nwhen available memory is abundant. We conclude that TFC-SR is a robust and\nefficient approach, highlighting the importance of integrating active memory\nretrieval mechanisms into continual learning systems.\n", "link": "http://arxiv.org/abs/2507.21109v2", "date": "2025-09-15", "relevancy": 2.5526, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5178}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5139}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Focused%20Consolidation%20with%20Spaced%20Recall%3A%20Making%20Neural%20Networks%0A%20%20Learn%20like%20College%20Students&body=Title%3A%20Task-Focused%20Consolidation%20with%20Spaced%20Recall%3A%20Making%20Neural%20Networks%0A%20%20Learn%20like%20College%20Students%0AAuthor%3A%20Prital%20Bamnodkar%0AAbstract%3A%20%20%20Deep%20neural%20networks%20often%20suffer%20from%20a%20critical%20limitation%20known%20as%0Acatastrophic%20forgetting%2C%20where%20performance%20on%20past%20tasks%20degrades%20after%0Alearning%20new%20ones.%20This%20paper%20introduces%20a%20novel%20continual%20learning%20approach%0Ainspired%20by%20human%20learning%20strategies%20like%20Active%20Recall%2C%20Deliberate%20Practice%2C%0Aand%20Spaced%20Repetition%2C%20named%20Task-Focused%20Consolidation%20with%20Spaced%20Recall%0A%28TFC-SR%29.%20TFC-SR%20enhances%20the%20standard%20experience%20replay%20framework%20with%20a%0Amechanism%20we%20term%20the%20Active%20Recall%20Probe.%20It%20is%20a%20periodic%2C%20task-aware%0Aevaluation%20of%20the%20model%27s%20memory%20that%20stabilizes%20the%20representations%20of%20past%0Aknowledge.%20We%20test%20TFC-SR%20on%20the%20Split%20MNIST%20and%20the%20Split%20CIFAR-100%20benchmarks%0Aagainst%20leading%20regularization-based%20and%20replay-based%20baselines.%20Our%20results%0Ashow%20that%20TFC-SR%20performs%20significantly%20better%20than%20these%20methods.%20For%0Ainstance%2C%20on%20the%20Split%20CIFAR-100%2C%20it%20achieves%20a%20final%20accuracy%20of%2013.17%25%0Acompared%20to%20Standard%20Experience%20Replay%27s%207.40%25.%20We%20demonstrate%20that%20this%0Aadvantage%20comes%20from%20the%20stabilizing%20effect%20of%20the%20probe%20itself%2C%20and%20not%20from%0Athe%20difference%20in%20replay%20volume.%20Additionally%2C%20we%20analyze%20the%20trade-off%20between%0Amemory%20size%20and%20performance%20and%20show%20that%20while%20TFC-SR%20performs%20better%20in%0Amemory-constrained%20environments%2C%20higher%20replay%20volume%20is%20still%20more%20effective%0Awhen%20available%20memory%20is%20abundant.%20We%20conclude%20that%20TFC-SR%20is%20a%20robust%20and%0Aefficient%20approach%2C%20highlighting%20the%20importance%20of%20integrating%20active%20memory%0Aretrieval%20mechanisms%20into%20continual%20learning%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21109v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Focused%2520Consolidation%2520with%2520Spaced%2520Recall%253A%2520Making%2520Neural%2520Networks%250A%2520%2520Learn%2520like%2520College%2520Students%26entry.906535625%3DPrital%2520Bamnodkar%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520often%2520suffer%2520from%2520a%2520critical%2520limitation%2520known%2520as%250Acatastrophic%2520forgetting%252C%2520where%2520performance%2520on%2520past%2520tasks%2520degrades%2520after%250Alearning%2520new%2520ones.%2520This%2520paper%2520introduces%2520a%2520novel%2520continual%2520learning%2520approach%250Ainspired%2520by%2520human%2520learning%2520strategies%2520like%2520Active%2520Recall%252C%2520Deliberate%2520Practice%252C%250Aand%2520Spaced%2520Repetition%252C%2520named%2520Task-Focused%2520Consolidation%2520with%2520Spaced%2520Recall%250A%2528TFC-SR%2529.%2520TFC-SR%2520enhances%2520the%2520standard%2520experience%2520replay%2520framework%2520with%2520a%250Amechanism%2520we%2520term%2520the%2520Active%2520Recall%2520Probe.%2520It%2520is%2520a%2520periodic%252C%2520task-aware%250Aevaluation%2520of%2520the%2520model%2527s%2520memory%2520that%2520stabilizes%2520the%2520representations%2520of%2520past%250Aknowledge.%2520We%2520test%2520TFC-SR%2520on%2520the%2520Split%2520MNIST%2520and%2520the%2520Split%2520CIFAR-100%2520benchmarks%250Aagainst%2520leading%2520regularization-based%2520and%2520replay-based%2520baselines.%2520Our%2520results%250Ashow%2520that%2520TFC-SR%2520performs%2520significantly%2520better%2520than%2520these%2520methods.%2520For%250Ainstance%252C%2520on%2520the%2520Split%2520CIFAR-100%252C%2520it%2520achieves%2520a%2520final%2520accuracy%2520of%252013.17%2525%250Acompared%2520to%2520Standard%2520Experience%2520Replay%2527s%25207.40%2525.%2520We%2520demonstrate%2520that%2520this%250Aadvantage%2520comes%2520from%2520the%2520stabilizing%2520effect%2520of%2520the%2520probe%2520itself%252C%2520and%2520not%2520from%250Athe%2520difference%2520in%2520replay%2520volume.%2520Additionally%252C%2520we%2520analyze%2520the%2520trade-off%2520between%250Amemory%2520size%2520and%2520performance%2520and%2520show%2520that%2520while%2520TFC-SR%2520performs%2520better%2520in%250Amemory-constrained%2520environments%252C%2520higher%2520replay%2520volume%2520is%2520still%2520more%2520effective%250Awhen%2520available%2520memory%2520is%2520abundant.%2520We%2520conclude%2520that%2520TFC-SR%2520is%2520a%2520robust%2520and%250Aefficient%2520approach%252C%2520highlighting%2520the%2520importance%2520of%2520integrating%2520active%2520memory%250Aretrieval%2520mechanisms%2520into%2520continual%2520learning%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21109v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Focused%20Consolidation%20with%20Spaced%20Recall%3A%20Making%20Neural%20Networks%0A%20%20Learn%20like%20College%20Students&entry.906535625=Prital%20Bamnodkar&entry.1292438233=%20%20Deep%20neural%20networks%20often%20suffer%20from%20a%20critical%20limitation%20known%20as%0Acatastrophic%20forgetting%2C%20where%20performance%20on%20past%20tasks%20degrades%20after%0Alearning%20new%20ones.%20This%20paper%20introduces%20a%20novel%20continual%20learning%20approach%0Ainspired%20by%20human%20learning%20strategies%20like%20Active%20Recall%2C%20Deliberate%20Practice%2C%0Aand%20Spaced%20Repetition%2C%20named%20Task-Focused%20Consolidation%20with%20Spaced%20Recall%0A%28TFC-SR%29.%20TFC-SR%20enhances%20the%20standard%20experience%20replay%20framework%20with%20a%0Amechanism%20we%20term%20the%20Active%20Recall%20Probe.%20It%20is%20a%20periodic%2C%20task-aware%0Aevaluation%20of%20the%20model%27s%20memory%20that%20stabilizes%20the%20representations%20of%20past%0Aknowledge.%20We%20test%20TFC-SR%20on%20the%20Split%20MNIST%20and%20the%20Split%20CIFAR-100%20benchmarks%0Aagainst%20leading%20regularization-based%20and%20replay-based%20baselines.%20Our%20results%0Ashow%20that%20TFC-SR%20performs%20significantly%20better%20than%20these%20methods.%20For%0Ainstance%2C%20on%20the%20Split%20CIFAR-100%2C%20it%20achieves%20a%20final%20accuracy%20of%2013.17%25%0Acompared%20to%20Standard%20Experience%20Replay%27s%207.40%25.%20We%20demonstrate%20that%20this%0Aadvantage%20comes%20from%20the%20stabilizing%20effect%20of%20the%20probe%20itself%2C%20and%20not%20from%0Athe%20difference%20in%20replay%20volume.%20Additionally%2C%20we%20analyze%20the%20trade-off%20between%0Amemory%20size%20and%20performance%20and%20show%20that%20while%20TFC-SR%20performs%20better%20in%0Amemory-constrained%20environments%2C%20higher%20replay%20volume%20is%20still%20more%20effective%0Awhen%20available%20memory%20is%20abundant.%20We%20conclude%20that%20TFC-SR%20is%20a%20robust%20and%0Aefficient%20approach%2C%20highlighting%20the%20importance%20of%20integrating%20active%20memory%0Aretrieval%20mechanisms%20into%20continual%20learning%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21109v2&entry.124074799=Read"},
{"title": "KB-DMGen: Knowledge-Based Global Guidance and Dynamic Pose Masking for\n  Human Image Generation", "author": "Shibang Liu and Xuemei Xie and Guangming Shi", "abstract": "  Recent methods using diffusion models have made significant progress in Human\nImage Generation (HIG) with various control signals such as pose priors. In\nHIG, both accurate human poses and coherent visual quality are crucial for\nimage generation. However, most existing methods mainly focus on pose accuracy\nwhile neglecting overall image quality, often improving pose alignment at the\ncost of image quality. To address this, we propose Knowledge-Based Global\nGuidance and Dynamic pose Masking for human image Generation (KB-DMGen). The\nKnowledge Base (KB), implemented as a visual codebook, provides coarse, global\nguidance based on input text-related visual features, improving pose accuracy\nwhile maintaining image quality, while the Dynamic pose Mask (DM) offers\nfine-grained local control to enhance precise pose accuracy. By injecting KB\nand DM at different stages of the diffusion process, our framework enhances\npose accuracy through both global and local control without compromising image\nquality. Experiments demonstrate the effectiveness of KB-DMGen, achieving new\nstate-of-the-art results in terms of AP and CAP on the HumanArt dataset. The\nproject page and code are available at https://lushbng.github.io/KBDMGen.\n", "link": "http://arxiv.org/abs/2507.20083v2", "date": "2025-09-15", "relevancy": 2.5512, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6421}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6375}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KB-DMGen%3A%20Knowledge-Based%20Global%20Guidance%20and%20Dynamic%20Pose%20Masking%20for%0A%20%20Human%20Image%20Generation&body=Title%3A%20KB-DMGen%3A%20Knowledge-Based%20Global%20Guidance%20and%20Dynamic%20Pose%20Masking%20for%0A%20%20Human%20Image%20Generation%0AAuthor%3A%20Shibang%20Liu%20and%20Xuemei%20Xie%20and%20Guangming%20Shi%0AAbstract%3A%20%20%20Recent%20methods%20using%20diffusion%20models%20have%20made%20significant%20progress%20in%20Human%0AImage%20Generation%20%28HIG%29%20with%20various%20control%20signals%20such%20as%20pose%20priors.%20In%0AHIG%2C%20both%20accurate%20human%20poses%20and%20coherent%20visual%20quality%20are%20crucial%20for%0Aimage%20generation.%20However%2C%20most%20existing%20methods%20mainly%20focus%20on%20pose%20accuracy%0Awhile%20neglecting%20overall%20image%20quality%2C%20often%20improving%20pose%20alignment%20at%20the%0Acost%20of%20image%20quality.%20To%20address%20this%2C%20we%20propose%20Knowledge-Based%20Global%0AGuidance%20and%20Dynamic%20pose%20Masking%20for%20human%20image%20Generation%20%28KB-DMGen%29.%20The%0AKnowledge%20Base%20%28KB%29%2C%20implemented%20as%20a%20visual%20codebook%2C%20provides%20coarse%2C%20global%0Aguidance%20based%20on%20input%20text-related%20visual%20features%2C%20improving%20pose%20accuracy%0Awhile%20maintaining%20image%20quality%2C%20while%20the%20Dynamic%20pose%20Mask%20%28DM%29%20offers%0Afine-grained%20local%20control%20to%20enhance%20precise%20pose%20accuracy.%20By%20injecting%20KB%0Aand%20DM%20at%20different%20stages%20of%20the%20diffusion%20process%2C%20our%20framework%20enhances%0Apose%20accuracy%20through%20both%20global%20and%20local%20control%20without%20compromising%20image%0Aquality.%20Experiments%20demonstrate%20the%20effectiveness%20of%20KB-DMGen%2C%20achieving%20new%0Astate-of-the-art%20results%20in%20terms%20of%20AP%20and%20CAP%20on%20the%20HumanArt%20dataset.%20The%0Aproject%20page%20and%20code%20are%20available%20at%20https%3A//lushbng.github.io/KBDMGen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKB-DMGen%253A%2520Knowledge-Based%2520Global%2520Guidance%2520and%2520Dynamic%2520Pose%2520Masking%2520for%250A%2520%2520Human%2520Image%2520Generation%26entry.906535625%3DShibang%2520Liu%2520and%2520Xuemei%2520Xie%2520and%2520Guangming%2520Shi%26entry.1292438233%3D%2520%2520Recent%2520methods%2520using%2520diffusion%2520models%2520have%2520made%2520significant%2520progress%2520in%2520Human%250AImage%2520Generation%2520%2528HIG%2529%2520with%2520various%2520control%2520signals%2520such%2520as%2520pose%2520priors.%2520In%250AHIG%252C%2520both%2520accurate%2520human%2520poses%2520and%2520coherent%2520visual%2520quality%2520are%2520crucial%2520for%250Aimage%2520generation.%2520However%252C%2520most%2520existing%2520methods%2520mainly%2520focus%2520on%2520pose%2520accuracy%250Awhile%2520neglecting%2520overall%2520image%2520quality%252C%2520often%2520improving%2520pose%2520alignment%2520at%2520the%250Acost%2520of%2520image%2520quality.%2520To%2520address%2520this%252C%2520we%2520propose%2520Knowledge-Based%2520Global%250AGuidance%2520and%2520Dynamic%2520pose%2520Masking%2520for%2520human%2520image%2520Generation%2520%2528KB-DMGen%2529.%2520The%250AKnowledge%2520Base%2520%2528KB%2529%252C%2520implemented%2520as%2520a%2520visual%2520codebook%252C%2520provides%2520coarse%252C%2520global%250Aguidance%2520based%2520on%2520input%2520text-related%2520visual%2520features%252C%2520improving%2520pose%2520accuracy%250Awhile%2520maintaining%2520image%2520quality%252C%2520while%2520the%2520Dynamic%2520pose%2520Mask%2520%2528DM%2529%2520offers%250Afine-grained%2520local%2520control%2520to%2520enhance%2520precise%2520pose%2520accuracy.%2520By%2520injecting%2520KB%250Aand%2520DM%2520at%2520different%2520stages%2520of%2520the%2520diffusion%2520process%252C%2520our%2520framework%2520enhances%250Apose%2520accuracy%2520through%2520both%2520global%2520and%2520local%2520control%2520without%2520compromising%2520image%250Aquality.%2520Experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520KB-DMGen%252C%2520achieving%2520new%250Astate-of-the-art%2520results%2520in%2520terms%2520of%2520AP%2520and%2520CAP%2520on%2520the%2520HumanArt%2520dataset.%2520The%250Aproject%2520page%2520and%2520code%2520are%2520available%2520at%2520https%253A//lushbng.github.io/KBDMGen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KB-DMGen%3A%20Knowledge-Based%20Global%20Guidance%20and%20Dynamic%20Pose%20Masking%20for%0A%20%20Human%20Image%20Generation&entry.906535625=Shibang%20Liu%20and%20Xuemei%20Xie%20and%20Guangming%20Shi&entry.1292438233=%20%20Recent%20methods%20using%20diffusion%20models%20have%20made%20significant%20progress%20in%20Human%0AImage%20Generation%20%28HIG%29%20with%20various%20control%20signals%20such%20as%20pose%20priors.%20In%0AHIG%2C%20both%20accurate%20human%20poses%20and%20coherent%20visual%20quality%20are%20crucial%20for%0Aimage%20generation.%20However%2C%20most%20existing%20methods%20mainly%20focus%20on%20pose%20accuracy%0Awhile%20neglecting%20overall%20image%20quality%2C%20often%20improving%20pose%20alignment%20at%20the%0Acost%20of%20image%20quality.%20To%20address%20this%2C%20we%20propose%20Knowledge-Based%20Global%0AGuidance%20and%20Dynamic%20pose%20Masking%20for%20human%20image%20Generation%20%28KB-DMGen%29.%20The%0AKnowledge%20Base%20%28KB%29%2C%20implemented%20as%20a%20visual%20codebook%2C%20provides%20coarse%2C%20global%0Aguidance%20based%20on%20input%20text-related%20visual%20features%2C%20improving%20pose%20accuracy%0Awhile%20maintaining%20image%20quality%2C%20while%20the%20Dynamic%20pose%20Mask%20%28DM%29%20offers%0Afine-grained%20local%20control%20to%20enhance%20precise%20pose%20accuracy.%20By%20injecting%20KB%0Aand%20DM%20at%20different%20stages%20of%20the%20diffusion%20process%2C%20our%20framework%20enhances%0Apose%20accuracy%20through%20both%20global%20and%20local%20control%20without%20compromising%20image%0Aquality.%20Experiments%20demonstrate%20the%20effectiveness%20of%20KB-DMGen%2C%20achieving%20new%0Astate-of-the-art%20results%20in%20terms%20of%20AP%20and%20CAP%20on%20the%20HumanArt%20dataset.%20The%0Aproject%20page%20and%20code%20are%20available%20at%20https%3A//lushbng.github.io/KBDMGen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20083v2&entry.124074799=Read"},
{"title": "Event2Vec: A Geometric Approach to Learning Composable Representations\n  of Event Sequences", "author": "Antonin Sulc", "abstract": "  The study of neural representations, both in biological and artificial\nsystems, is increasingly revealing the importance of geometric and topological\nstructures. Inspired by this, we introduce Event2Vec, a novel framework for\nlearning representations of discrete event sequences. Our model leverages a\nsimple, additive recurrent structure to learn composable, interpretable\nembeddings. We provide a theoretical analysis demonstrating that, under\nspecific training objectives, our model's learned representations in a\nEuclidean space converge to an ideal additive structure. This ensures that the\nrepresentation of a sequence is the vector sum of its constituent events, a\nproperty we term the linear additive hypothesis. To address the limitations of\nEuclidean geometry for hierarchical data, we also introduce a variant of our\nmodel in hyperbolic space, which is naturally suited to embedding tree-like\nstructures with low distortion. We present experiments to validate our\nhypothesis and demonstrate the benefits of each geometry, highlighting the\nimproved performance of the hyperbolic model on hierarchical event sequences.\n", "link": "http://arxiv.org/abs/2509.12188v1", "date": "2025-09-15", "relevancy": 2.5474, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5118}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event2Vec%3A%20A%20Geometric%20Approach%20to%20Learning%20Composable%20Representations%0A%20%20of%20Event%20Sequences&body=Title%3A%20Event2Vec%3A%20A%20Geometric%20Approach%20to%20Learning%20Composable%20Representations%0A%20%20of%20Event%20Sequences%0AAuthor%3A%20Antonin%20Sulc%0AAbstract%3A%20%20%20The%20study%20of%20neural%20representations%2C%20both%20in%20biological%20and%20artificial%0Asystems%2C%20is%20increasingly%20revealing%20the%20importance%20of%20geometric%20and%20topological%0Astructures.%20Inspired%20by%20this%2C%20we%20introduce%20Event2Vec%2C%20a%20novel%20framework%20for%0Alearning%20representations%20of%20discrete%20event%20sequences.%20Our%20model%20leverages%20a%0Asimple%2C%20additive%20recurrent%20structure%20to%20learn%20composable%2C%20interpretable%0Aembeddings.%20We%20provide%20a%20theoretical%20analysis%20demonstrating%20that%2C%20under%0Aspecific%20training%20objectives%2C%20our%20model%27s%20learned%20representations%20in%20a%0AEuclidean%20space%20converge%20to%20an%20ideal%20additive%20structure.%20This%20ensures%20that%20the%0Arepresentation%20of%20a%20sequence%20is%20the%20vector%20sum%20of%20its%20constituent%20events%2C%20a%0Aproperty%20we%20term%20the%20linear%20additive%20hypothesis.%20To%20address%20the%20limitations%20of%0AEuclidean%20geometry%20for%20hierarchical%20data%2C%20we%20also%20introduce%20a%20variant%20of%20our%0Amodel%20in%20hyperbolic%20space%2C%20which%20is%20naturally%20suited%20to%20embedding%20tree-like%0Astructures%20with%20low%20distortion.%20We%20present%20experiments%20to%20validate%20our%0Ahypothesis%20and%20demonstrate%20the%20benefits%20of%20each%20geometry%2C%20highlighting%20the%0Aimproved%20performance%20of%20the%20hyperbolic%20model%20on%20hierarchical%20event%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent2Vec%253A%2520A%2520Geometric%2520Approach%2520to%2520Learning%2520Composable%2520Representations%250A%2520%2520of%2520Event%2520Sequences%26entry.906535625%3DAntonin%2520Sulc%26entry.1292438233%3D%2520%2520The%2520study%2520of%2520neural%2520representations%252C%2520both%2520in%2520biological%2520and%2520artificial%250Asystems%252C%2520is%2520increasingly%2520revealing%2520the%2520importance%2520of%2520geometric%2520and%2520topological%250Astructures.%2520Inspired%2520by%2520this%252C%2520we%2520introduce%2520Event2Vec%252C%2520a%2520novel%2520framework%2520for%250Alearning%2520representations%2520of%2520discrete%2520event%2520sequences.%2520Our%2520model%2520leverages%2520a%250Asimple%252C%2520additive%2520recurrent%2520structure%2520to%2520learn%2520composable%252C%2520interpretable%250Aembeddings.%2520We%2520provide%2520a%2520theoretical%2520analysis%2520demonstrating%2520that%252C%2520under%250Aspecific%2520training%2520objectives%252C%2520our%2520model%2527s%2520learned%2520representations%2520in%2520a%250AEuclidean%2520space%2520converge%2520to%2520an%2520ideal%2520additive%2520structure.%2520This%2520ensures%2520that%2520the%250Arepresentation%2520of%2520a%2520sequence%2520is%2520the%2520vector%2520sum%2520of%2520its%2520constituent%2520events%252C%2520a%250Aproperty%2520we%2520term%2520the%2520linear%2520additive%2520hypothesis.%2520To%2520address%2520the%2520limitations%2520of%250AEuclidean%2520geometry%2520for%2520hierarchical%2520data%252C%2520we%2520also%2520introduce%2520a%2520variant%2520of%2520our%250Amodel%2520in%2520hyperbolic%2520space%252C%2520which%2520is%2520naturally%2520suited%2520to%2520embedding%2520tree-like%250Astructures%2520with%2520low%2520distortion.%2520We%2520present%2520experiments%2520to%2520validate%2520our%250Ahypothesis%2520and%2520demonstrate%2520the%2520benefits%2520of%2520each%2520geometry%252C%2520highlighting%2520the%250Aimproved%2520performance%2520of%2520the%2520hyperbolic%2520model%2520on%2520hierarchical%2520event%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event2Vec%3A%20A%20Geometric%20Approach%20to%20Learning%20Composable%20Representations%0A%20%20of%20Event%20Sequences&entry.906535625=Antonin%20Sulc&entry.1292438233=%20%20The%20study%20of%20neural%20representations%2C%20both%20in%20biological%20and%20artificial%0Asystems%2C%20is%20increasingly%20revealing%20the%20importance%20of%20geometric%20and%20topological%0Astructures.%20Inspired%20by%20this%2C%20we%20introduce%20Event2Vec%2C%20a%20novel%20framework%20for%0Alearning%20representations%20of%20discrete%20event%20sequences.%20Our%20model%20leverages%20a%0Asimple%2C%20additive%20recurrent%20structure%20to%20learn%20composable%2C%20interpretable%0Aembeddings.%20We%20provide%20a%20theoretical%20analysis%20demonstrating%20that%2C%20under%0Aspecific%20training%20objectives%2C%20our%20model%27s%20learned%20representations%20in%20a%0AEuclidean%20space%20converge%20to%20an%20ideal%20additive%20structure.%20This%20ensures%20that%20the%0Arepresentation%20of%20a%20sequence%20is%20the%20vector%20sum%20of%20its%20constituent%20events%2C%20a%0Aproperty%20we%20term%20the%20linear%20additive%20hypothesis.%20To%20address%20the%20limitations%20of%0AEuclidean%20geometry%20for%20hierarchical%20data%2C%20we%20also%20introduce%20a%20variant%20of%20our%0Amodel%20in%20hyperbolic%20space%2C%20which%20is%20naturally%20suited%20to%20embedding%20tree-like%0Astructures%20with%20low%20distortion.%20We%20present%20experiments%20to%20validate%20our%0Ahypothesis%20and%20demonstrate%20the%20benefits%20of%20each%20geometry%2C%20highlighting%20the%0Aimproved%20performance%20of%20the%20hyperbolic%20model%20on%20hierarchical%20event%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12188v1&entry.124074799=Read"},
{"title": "LEGO: Spatial Accelerator Generation and Optimization for Tensor\n  Applications", "author": "Yujun Lin and Zhekai Zhang and Song Han", "abstract": "  Modern tensor applications, especially foundation models and generative AI\napplications require multiple input modalities (both vision and language),\nwhich increases the demand for flexible accelerator architecture. Existing\nframeworks suffer from the trade-off between design flexibility and\nproductivity of RTL generation: either limited to very few hand-written\ntemplates or cannot automatically generate the RTL. To address this challenge,\nwe propose the LEGO framework, which targets tensor applications and\nautomatically generates spatial architecture design and outputs synthesizable\nRTL code without handwritten RTL design templates. Leveraging the\naffine-transformation-based architecture representation, LEGO front end finds\ninterconnections between function units, synthesizes the memory system, and\nfuses different spatial dataflow designs based on data reuse analysis. LEGO\nback end then translates the hardware in a primitive-level graph to perform\nlower-level optimizations, and applies a set of linear-programming algorithms\nto optimally insert pipeline registers and reduce the overhead of unused logic\nwhen switching spatial dataflows. Our evaluation demonstrates that LEGO can\nachieve 3.2x speedup and 2.4x energy efficiency compared to previous work\nGemmini, and can generate one architecture for diverse modern foundation models\nin generative AI applications.\n", "link": "http://arxiv.org/abs/2509.12053v1", "date": "2025-09-15", "relevancy": 2.544, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5141}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5075}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEGO%3A%20Spatial%20Accelerator%20Generation%20and%20Optimization%20for%20Tensor%0A%20%20Applications&body=Title%3A%20LEGO%3A%20Spatial%20Accelerator%20Generation%20and%20Optimization%20for%20Tensor%0A%20%20Applications%0AAuthor%3A%20Yujun%20Lin%20and%20Zhekai%20Zhang%20and%20Song%20Han%0AAbstract%3A%20%20%20Modern%20tensor%20applications%2C%20especially%20foundation%20models%20and%20generative%20AI%0Aapplications%20require%20multiple%20input%20modalities%20%28both%20vision%20and%20language%29%2C%0Awhich%20increases%20the%20demand%20for%20flexible%20accelerator%20architecture.%20Existing%0Aframeworks%20suffer%20from%20the%20trade-off%20between%20design%20flexibility%20and%0Aproductivity%20of%20RTL%20generation%3A%20either%20limited%20to%20very%20few%20hand-written%0Atemplates%20or%20cannot%20automatically%20generate%20the%20RTL.%20To%20address%20this%20challenge%2C%0Awe%20propose%20the%20LEGO%20framework%2C%20which%20targets%20tensor%20applications%20and%0Aautomatically%20generates%20spatial%20architecture%20design%20and%20outputs%20synthesizable%0ARTL%20code%20without%20handwritten%20RTL%20design%20templates.%20Leveraging%20the%0Aaffine-transformation-based%20architecture%20representation%2C%20LEGO%20front%20end%20finds%0Ainterconnections%20between%20function%20units%2C%20synthesizes%20the%20memory%20system%2C%20and%0Afuses%20different%20spatial%20dataflow%20designs%20based%20on%20data%20reuse%20analysis.%20LEGO%0Aback%20end%20then%20translates%20the%20hardware%20in%20a%20primitive-level%20graph%20to%20perform%0Alower-level%20optimizations%2C%20and%20applies%20a%20set%20of%20linear-programming%20algorithms%0Ato%20optimally%20insert%20pipeline%20registers%20and%20reduce%20the%20overhead%20of%20unused%20logic%0Awhen%20switching%20spatial%20dataflows.%20Our%20evaluation%20demonstrates%20that%20LEGO%20can%0Aachieve%203.2x%20speedup%20and%202.4x%20energy%20efficiency%20compared%20to%20previous%20work%0AGemmini%2C%20and%20can%20generate%20one%20architecture%20for%20diverse%20modern%20foundation%20models%0Ain%20generative%20AI%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEGO%253A%2520Spatial%2520Accelerator%2520Generation%2520and%2520Optimization%2520for%2520Tensor%250A%2520%2520Applications%26entry.906535625%3DYujun%2520Lin%2520and%2520Zhekai%2520Zhang%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520Modern%2520tensor%2520applications%252C%2520especially%2520foundation%2520models%2520and%2520generative%2520AI%250Aapplications%2520require%2520multiple%2520input%2520modalities%2520%2528both%2520vision%2520and%2520language%2529%252C%250Awhich%2520increases%2520the%2520demand%2520for%2520flexible%2520accelerator%2520architecture.%2520Existing%250Aframeworks%2520suffer%2520from%2520the%2520trade-off%2520between%2520design%2520flexibility%2520and%250Aproductivity%2520of%2520RTL%2520generation%253A%2520either%2520limited%2520to%2520very%2520few%2520hand-written%250Atemplates%2520or%2520cannot%2520automatically%2520generate%2520the%2520RTL.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520propose%2520the%2520LEGO%2520framework%252C%2520which%2520targets%2520tensor%2520applications%2520and%250Aautomatically%2520generates%2520spatial%2520architecture%2520design%2520and%2520outputs%2520synthesizable%250ARTL%2520code%2520without%2520handwritten%2520RTL%2520design%2520templates.%2520Leveraging%2520the%250Aaffine-transformation-based%2520architecture%2520representation%252C%2520LEGO%2520front%2520end%2520finds%250Ainterconnections%2520between%2520function%2520units%252C%2520synthesizes%2520the%2520memory%2520system%252C%2520and%250Afuses%2520different%2520spatial%2520dataflow%2520designs%2520based%2520on%2520data%2520reuse%2520analysis.%2520LEGO%250Aback%2520end%2520then%2520translates%2520the%2520hardware%2520in%2520a%2520primitive-level%2520graph%2520to%2520perform%250Alower-level%2520optimizations%252C%2520and%2520applies%2520a%2520set%2520of%2520linear-programming%2520algorithms%250Ato%2520optimally%2520insert%2520pipeline%2520registers%2520and%2520reduce%2520the%2520overhead%2520of%2520unused%2520logic%250Awhen%2520switching%2520spatial%2520dataflows.%2520Our%2520evaluation%2520demonstrates%2520that%2520LEGO%2520can%250Aachieve%25203.2x%2520speedup%2520and%25202.4x%2520energy%2520efficiency%2520compared%2520to%2520previous%2520work%250AGemmini%252C%2520and%2520can%2520generate%2520one%2520architecture%2520for%2520diverse%2520modern%2520foundation%2520models%250Ain%2520generative%2520AI%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEGO%3A%20Spatial%20Accelerator%20Generation%20and%20Optimization%20for%20Tensor%0A%20%20Applications&entry.906535625=Yujun%20Lin%20and%20Zhekai%20Zhang%20and%20Song%20Han&entry.1292438233=%20%20Modern%20tensor%20applications%2C%20especially%20foundation%20models%20and%20generative%20AI%0Aapplications%20require%20multiple%20input%20modalities%20%28both%20vision%20and%20language%29%2C%0Awhich%20increases%20the%20demand%20for%20flexible%20accelerator%20architecture.%20Existing%0Aframeworks%20suffer%20from%20the%20trade-off%20between%20design%20flexibility%20and%0Aproductivity%20of%20RTL%20generation%3A%20either%20limited%20to%20very%20few%20hand-written%0Atemplates%20or%20cannot%20automatically%20generate%20the%20RTL.%20To%20address%20this%20challenge%2C%0Awe%20propose%20the%20LEGO%20framework%2C%20which%20targets%20tensor%20applications%20and%0Aautomatically%20generates%20spatial%20architecture%20design%20and%20outputs%20synthesizable%0ARTL%20code%20without%20handwritten%20RTL%20design%20templates.%20Leveraging%20the%0Aaffine-transformation-based%20architecture%20representation%2C%20LEGO%20front%20end%20finds%0Ainterconnections%20between%20function%20units%2C%20synthesizes%20the%20memory%20system%2C%20and%0Afuses%20different%20spatial%20dataflow%20designs%20based%20on%20data%20reuse%20analysis.%20LEGO%0Aback%20end%20then%20translates%20the%20hardware%20in%20a%20primitive-level%20graph%20to%20perform%0Alower-level%20optimizations%2C%20and%20applies%20a%20set%20of%20linear-programming%20algorithms%0Ato%20optimally%20insert%20pipeline%20registers%20and%20reduce%20the%20overhead%20of%20unused%20logic%0Awhen%20switching%20spatial%20dataflows.%20Our%20evaluation%20demonstrates%20that%20LEGO%20can%0Aachieve%203.2x%20speedup%20and%202.4x%20energy%20efficiency%20compared%20to%20previous%20work%0AGemmini%2C%20and%20can%20generate%20one%20architecture%20for%20diverse%20modern%20foundation%20models%0Ain%20generative%20AI%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12053v1&entry.124074799=Read"},
{"title": "MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues\n  via Arena-style and Rubrics Protocols", "author": "Yuhao Du and Qianwei Huang and Guo Zhu and Zhanchen Dai and Shunian Chen and Qiming Zhu and Le Pan and Minghao Chen and Yuhao Zhang and Li Zhou and Benyou Wang and Haizhou Li", "abstract": "  The rapid advancement of speech-to-speech (S2S) large language models (LLMs)\nhas significantly improved real-time spoken interaction. However, current\nevaluation frameworks remain inadequate for assessing performance in complex,\nmulti-turn dialogues. To address this, we introduce MTalk-Bench, a multi-turn\nS2S benchmark covering three core dimensions: Semantic Information,\nParalinguistic Information, and Ambient Sound. Each dimension includes nine\nrealistic scenarios, along with targeted tasks to assess specific capabilities\nsuch as reasoning. Our dual-method evaluation framework combines Arena-style\nevaluation (pairwise comparison) and Rubrics-based evaluation (absolute\nscoring) for relative and absolute assessment. The benchmark includes both\nmodel and human outputs, evaluated by human evaluators and LLMs. Experimental\nresults reveal two sets of findings. Overall performance of S2S LLMs: (1)\nmodels excel at semantic information processing yet underperform on\nparalinguistic information and ambient sounds perception; (2) models typically\nregain coherence by increasing response length, sacrificing efficiency in\nmulti-turn dialogues; (3) modality-aware, task-specific designs outperform\nbrute scaling. Evaluation framework and reliability: (1) Arena and Rubrics\nyield consistent, complementary rankings, but reliable distinctions emerge only\nwhen performance gaps are large; (2) LLM-as-a-judge aligns with humans when\ngaps are clear or criteria explicit, but exhibits position and length biases\nand is reliable on nonverbal evaluation only with text annotations. These\nresults highlight current limitations in S2S evaluation and the need for more\nrobust, speech-aware assessment frameworks.\n", "link": "http://arxiv.org/abs/2508.18240v2", "date": "2025-09-15", "relevancy": 2.5392, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5203}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MTalk-Bench%3A%20Evaluating%20Speech-to-Speech%20Models%20in%20Multi-Turn%20Dialogues%0A%20%20via%20Arena-style%20and%20Rubrics%20Protocols&body=Title%3A%20MTalk-Bench%3A%20Evaluating%20Speech-to-Speech%20Models%20in%20Multi-Turn%20Dialogues%0A%20%20via%20Arena-style%20and%20Rubrics%20Protocols%0AAuthor%3A%20Yuhao%20Du%20and%20Qianwei%20Huang%20and%20Guo%20Zhu%20and%20Zhanchen%20Dai%20and%20Shunian%20Chen%20and%20Qiming%20Zhu%20and%20Le%20Pan%20and%20Minghao%20Chen%20and%20Yuhao%20Zhang%20and%20Li%20Zhou%20and%20Benyou%20Wang%20and%20Haizhou%20Li%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20speech-to-speech%20%28S2S%29%20large%20language%20models%20%28LLMs%29%0Ahas%20significantly%20improved%20real-time%20spoken%20interaction.%20However%2C%20current%0Aevaluation%20frameworks%20remain%20inadequate%20for%20assessing%20performance%20in%20complex%2C%0Amulti-turn%20dialogues.%20To%20address%20this%2C%20we%20introduce%20MTalk-Bench%2C%20a%20multi-turn%0AS2S%20benchmark%20covering%20three%20core%20dimensions%3A%20Semantic%20Information%2C%0AParalinguistic%20Information%2C%20and%20Ambient%20Sound.%20Each%20dimension%20includes%20nine%0Arealistic%20scenarios%2C%20along%20with%20targeted%20tasks%20to%20assess%20specific%20capabilities%0Asuch%20as%20reasoning.%20Our%20dual-method%20evaluation%20framework%20combines%20Arena-style%0Aevaluation%20%28pairwise%20comparison%29%20and%20Rubrics-based%20evaluation%20%28absolute%0Ascoring%29%20for%20relative%20and%20absolute%20assessment.%20The%20benchmark%20includes%20both%0Amodel%20and%20human%20outputs%2C%20evaluated%20by%20human%20evaluators%20and%20LLMs.%20Experimental%0Aresults%20reveal%20two%20sets%20of%20findings.%20Overall%20performance%20of%20S2S%20LLMs%3A%20%281%29%0Amodels%20excel%20at%20semantic%20information%20processing%20yet%20underperform%20on%0Aparalinguistic%20information%20and%20ambient%20sounds%20perception%3B%20%282%29%20models%20typically%0Aregain%20coherence%20by%20increasing%20response%20length%2C%20sacrificing%20efficiency%20in%0Amulti-turn%20dialogues%3B%20%283%29%20modality-aware%2C%20task-specific%20designs%20outperform%0Abrute%20scaling.%20Evaluation%20framework%20and%20reliability%3A%20%281%29%20Arena%20and%20Rubrics%0Ayield%20consistent%2C%20complementary%20rankings%2C%20but%20reliable%20distinctions%20emerge%20only%0Awhen%20performance%20gaps%20are%20large%3B%20%282%29%20LLM-as-a-judge%20aligns%20with%20humans%20when%0Agaps%20are%20clear%20or%20criteria%20explicit%2C%20but%20exhibits%20position%20and%20length%20biases%0Aand%20is%20reliable%20on%20nonverbal%20evaluation%20only%20with%20text%20annotations.%20These%0Aresults%20highlight%20current%20limitations%20in%20S2S%20evaluation%20and%20the%20need%20for%20more%0Arobust%2C%20speech-aware%20assessment%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18240v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMTalk-Bench%253A%2520Evaluating%2520Speech-to-Speech%2520Models%2520in%2520Multi-Turn%2520Dialogues%250A%2520%2520via%2520Arena-style%2520and%2520Rubrics%2520Protocols%26entry.906535625%3DYuhao%2520Du%2520and%2520Qianwei%2520Huang%2520and%2520Guo%2520Zhu%2520and%2520Zhanchen%2520Dai%2520and%2520Shunian%2520Chen%2520and%2520Qiming%2520Zhu%2520and%2520Le%2520Pan%2520and%2520Minghao%2520Chen%2520and%2520Yuhao%2520Zhang%2520and%2520Li%2520Zhou%2520and%2520Benyou%2520Wang%2520and%2520Haizhou%2520Li%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520speech-to-speech%2520%2528S2S%2529%2520large%2520language%2520models%2520%2528LLMs%2529%250Ahas%2520significantly%2520improved%2520real-time%2520spoken%2520interaction.%2520However%252C%2520current%250Aevaluation%2520frameworks%2520remain%2520inadequate%2520for%2520assessing%2520performance%2520in%2520complex%252C%250Amulti-turn%2520dialogues.%2520To%2520address%2520this%252C%2520we%2520introduce%2520MTalk-Bench%252C%2520a%2520multi-turn%250AS2S%2520benchmark%2520covering%2520three%2520core%2520dimensions%253A%2520Semantic%2520Information%252C%250AParalinguistic%2520Information%252C%2520and%2520Ambient%2520Sound.%2520Each%2520dimension%2520includes%2520nine%250Arealistic%2520scenarios%252C%2520along%2520with%2520targeted%2520tasks%2520to%2520assess%2520specific%2520capabilities%250Asuch%2520as%2520reasoning.%2520Our%2520dual-method%2520evaluation%2520framework%2520combines%2520Arena-style%250Aevaluation%2520%2528pairwise%2520comparison%2529%2520and%2520Rubrics-based%2520evaluation%2520%2528absolute%250Ascoring%2529%2520for%2520relative%2520and%2520absolute%2520assessment.%2520The%2520benchmark%2520includes%2520both%250Amodel%2520and%2520human%2520outputs%252C%2520evaluated%2520by%2520human%2520evaluators%2520and%2520LLMs.%2520Experimental%250Aresults%2520reveal%2520two%2520sets%2520of%2520findings.%2520Overall%2520performance%2520of%2520S2S%2520LLMs%253A%2520%25281%2529%250Amodels%2520excel%2520at%2520semantic%2520information%2520processing%2520yet%2520underperform%2520on%250Aparalinguistic%2520information%2520and%2520ambient%2520sounds%2520perception%253B%2520%25282%2529%2520models%2520typically%250Aregain%2520coherence%2520by%2520increasing%2520response%2520length%252C%2520sacrificing%2520efficiency%2520in%250Amulti-turn%2520dialogues%253B%2520%25283%2529%2520modality-aware%252C%2520task-specific%2520designs%2520outperform%250Abrute%2520scaling.%2520Evaluation%2520framework%2520and%2520reliability%253A%2520%25281%2529%2520Arena%2520and%2520Rubrics%250Ayield%2520consistent%252C%2520complementary%2520rankings%252C%2520but%2520reliable%2520distinctions%2520emerge%2520only%250Awhen%2520performance%2520gaps%2520are%2520large%253B%2520%25282%2529%2520LLM-as-a-judge%2520aligns%2520with%2520humans%2520when%250Agaps%2520are%2520clear%2520or%2520criteria%2520explicit%252C%2520but%2520exhibits%2520position%2520and%2520length%2520biases%250Aand%2520is%2520reliable%2520on%2520nonverbal%2520evaluation%2520only%2520with%2520text%2520annotations.%2520These%250Aresults%2520highlight%2520current%2520limitations%2520in%2520S2S%2520evaluation%2520and%2520the%2520need%2520for%2520more%250Arobust%252C%2520speech-aware%2520assessment%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18240v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTalk-Bench%3A%20Evaluating%20Speech-to-Speech%20Models%20in%20Multi-Turn%20Dialogues%0A%20%20via%20Arena-style%20and%20Rubrics%20Protocols&entry.906535625=Yuhao%20Du%20and%20Qianwei%20Huang%20and%20Guo%20Zhu%20and%20Zhanchen%20Dai%20and%20Shunian%20Chen%20and%20Qiming%20Zhu%20and%20Le%20Pan%20and%20Minghao%20Chen%20and%20Yuhao%20Zhang%20and%20Li%20Zhou%20and%20Benyou%20Wang%20and%20Haizhou%20Li&entry.1292438233=%20%20The%20rapid%20advancement%20of%20speech-to-speech%20%28S2S%29%20large%20language%20models%20%28LLMs%29%0Ahas%20significantly%20improved%20real-time%20spoken%20interaction.%20However%2C%20current%0Aevaluation%20frameworks%20remain%20inadequate%20for%20assessing%20performance%20in%20complex%2C%0Amulti-turn%20dialogues.%20To%20address%20this%2C%20we%20introduce%20MTalk-Bench%2C%20a%20multi-turn%0AS2S%20benchmark%20covering%20three%20core%20dimensions%3A%20Semantic%20Information%2C%0AParalinguistic%20Information%2C%20and%20Ambient%20Sound.%20Each%20dimension%20includes%20nine%0Arealistic%20scenarios%2C%20along%20with%20targeted%20tasks%20to%20assess%20specific%20capabilities%0Asuch%20as%20reasoning.%20Our%20dual-method%20evaluation%20framework%20combines%20Arena-style%0Aevaluation%20%28pairwise%20comparison%29%20and%20Rubrics-based%20evaluation%20%28absolute%0Ascoring%29%20for%20relative%20and%20absolute%20assessment.%20The%20benchmark%20includes%20both%0Amodel%20and%20human%20outputs%2C%20evaluated%20by%20human%20evaluators%20and%20LLMs.%20Experimental%0Aresults%20reveal%20two%20sets%20of%20findings.%20Overall%20performance%20of%20S2S%20LLMs%3A%20%281%29%0Amodels%20excel%20at%20semantic%20information%20processing%20yet%20underperform%20on%0Aparalinguistic%20information%20and%20ambient%20sounds%20perception%3B%20%282%29%20models%20typically%0Aregain%20coherence%20by%20increasing%20response%20length%2C%20sacrificing%20efficiency%20in%0Amulti-turn%20dialogues%3B%20%283%29%20modality-aware%2C%20task-specific%20designs%20outperform%0Abrute%20scaling.%20Evaluation%20framework%20and%20reliability%3A%20%281%29%20Arena%20and%20Rubrics%0Ayield%20consistent%2C%20complementary%20rankings%2C%20but%20reliable%20distinctions%20emerge%20only%0Awhen%20performance%20gaps%20are%20large%3B%20%282%29%20LLM-as-a-judge%20aligns%20with%20humans%20when%0Agaps%20are%20clear%20or%20criteria%20explicit%2C%20but%20exhibits%20position%20and%20length%20biases%0Aand%20is%20reliable%20on%20nonverbal%20evaluation%20only%20with%20text%20annotations.%20These%0Aresults%20highlight%20current%20limitations%20in%20S2S%20evaluation%20and%20the%20need%20for%20more%0Arobust%2C%20speech-aware%20assessment%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18240v2&entry.124074799=Read"},
{"title": "LoRA-fine-tuned Large Vision Models for Automated Assessment of\n  Post-SBRT Lung Injury", "author": "M. Bolhassani and B. Veasey and E. Daugherty and S. Keltner and N. Kumar and N. Dunlap and A. Amini", "abstract": "  This study investigates the efficacy of Low-Rank Adaptation (LoRA) for\nfine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose\nRadiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic\nBody Radiation Therapy (SBRT). To evaluate the robustness and efficiency of\nthis approach, we compare LoRA with traditional full fine-tuning and\ninference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3\nand 75 mm3), centered at the treatment isocenter, in addition to different\nadaptation techniques for adapting the 2D LVMs for 3D data were used to\ndetermine the sensitivity of the models to spatial context. Experimental\nresults show that LoRA achieves comparable or superior performance to\ntraditional fine-tuning while significantly reducing computational costs and\ntraining times by requiring fewer trainable parameters.\n", "link": "http://arxiv.org/abs/2509.12155v1", "date": "2025-09-15", "relevancy": 2.5369, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA-fine-tuned%20Large%20Vision%20Models%20for%20Automated%20Assessment%20of%0A%20%20Post-SBRT%20Lung%20Injury&body=Title%3A%20LoRA-fine-tuned%20Large%20Vision%20Models%20for%20Automated%20Assessment%20of%0A%20%20Post-SBRT%20Lung%20Injury%0AAuthor%3A%20M.%20Bolhassani%20and%20B.%20Veasey%20and%20E.%20Daugherty%20and%20S.%20Keltner%20and%20N.%20Kumar%20and%20N.%20Dunlap%20and%20A.%20Amini%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20efficacy%20of%20Low-Rank%20Adaptation%20%28LoRA%29%20for%0Afine-tuning%20large%20Vision%20Models%2C%20DinoV2%20and%20SwinV2%2C%20to%20diagnose%0ARadiation-Induced%20Lung%20Injury%20%28RILI%29%20from%20X-ray%20CT%20scans%20following%20Stereotactic%0ABody%20Radiation%20Therapy%20%28SBRT%29.%20To%20evaluate%20the%20robustness%20and%20efficiency%20of%0Athis%20approach%2C%20we%20compare%20LoRA%20with%20traditional%20full%20fine-tuning%20and%0Ainference-only%20%28no%20fine-tuning%29%20methods.%20Cropped%20images%20of%20two%20sizes%20%2850%20mm3%0Aand%2075%20mm3%29%2C%20centered%20at%20the%20treatment%20isocenter%2C%20in%20addition%20to%20different%0Aadaptation%20techniques%20for%20adapting%20the%202D%20LVMs%20for%203D%20data%20were%20used%20to%0Adetermine%20the%20sensitivity%20of%20the%20models%20to%20spatial%20context.%20Experimental%0Aresults%20show%20that%20LoRA%20achieves%20comparable%20or%20superior%20performance%20to%0Atraditional%20fine-tuning%20while%20significantly%20reducing%20computational%20costs%20and%0Atraining%20times%20by%20requiring%20fewer%20trainable%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA-fine-tuned%2520Large%2520Vision%2520Models%2520for%2520Automated%2520Assessment%2520of%250A%2520%2520Post-SBRT%2520Lung%2520Injury%26entry.906535625%3DM.%2520Bolhassani%2520and%2520B.%2520Veasey%2520and%2520E.%2520Daugherty%2520and%2520S.%2520Keltner%2520and%2520N.%2520Kumar%2520and%2520N.%2520Dunlap%2520and%2520A.%2520Amini%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520efficacy%2520of%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520for%250Afine-tuning%2520large%2520Vision%2520Models%252C%2520DinoV2%2520and%2520SwinV2%252C%2520to%2520diagnose%250ARadiation-Induced%2520Lung%2520Injury%2520%2528RILI%2529%2520from%2520X-ray%2520CT%2520scans%2520following%2520Stereotactic%250ABody%2520Radiation%2520Therapy%2520%2528SBRT%2529.%2520To%2520evaluate%2520the%2520robustness%2520and%2520efficiency%2520of%250Athis%2520approach%252C%2520we%2520compare%2520LoRA%2520with%2520traditional%2520full%2520fine-tuning%2520and%250Ainference-only%2520%2528no%2520fine-tuning%2529%2520methods.%2520Cropped%2520images%2520of%2520two%2520sizes%2520%252850%2520mm3%250Aand%252075%2520mm3%2529%252C%2520centered%2520at%2520the%2520treatment%2520isocenter%252C%2520in%2520addition%2520to%2520different%250Aadaptation%2520techniques%2520for%2520adapting%2520the%25202D%2520LVMs%2520for%25203D%2520data%2520were%2520used%2520to%250Adetermine%2520the%2520sensitivity%2520of%2520the%2520models%2520to%2520spatial%2520context.%2520Experimental%250Aresults%2520show%2520that%2520LoRA%2520achieves%2520comparable%2520or%2520superior%2520performance%2520to%250Atraditional%2520fine-tuning%2520while%2520significantly%2520reducing%2520computational%2520costs%2520and%250Atraining%2520times%2520by%2520requiring%2520fewer%2520trainable%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA-fine-tuned%20Large%20Vision%20Models%20for%20Automated%20Assessment%20of%0A%20%20Post-SBRT%20Lung%20Injury&entry.906535625=M.%20Bolhassani%20and%20B.%20Veasey%20and%20E.%20Daugherty%20and%20S.%20Keltner%20and%20N.%20Kumar%20and%20N.%20Dunlap%20and%20A.%20Amini&entry.1292438233=%20%20This%20study%20investigates%20the%20efficacy%20of%20Low-Rank%20Adaptation%20%28LoRA%29%20for%0Afine-tuning%20large%20Vision%20Models%2C%20DinoV2%20and%20SwinV2%2C%20to%20diagnose%0ARadiation-Induced%20Lung%20Injury%20%28RILI%29%20from%20X-ray%20CT%20scans%20following%20Stereotactic%0ABody%20Radiation%20Therapy%20%28SBRT%29.%20To%20evaluate%20the%20robustness%20and%20efficiency%20of%0Athis%20approach%2C%20we%20compare%20LoRA%20with%20traditional%20full%20fine-tuning%20and%0Ainference-only%20%28no%20fine-tuning%29%20methods.%20Cropped%20images%20of%20two%20sizes%20%2850%20mm3%0Aand%2075%20mm3%29%2C%20centered%20at%20the%20treatment%20isocenter%2C%20in%20addition%20to%20different%0Aadaptation%20techniques%20for%20adapting%20the%202D%20LVMs%20for%203D%20data%20were%20used%20to%0Adetermine%20the%20sensitivity%20of%20the%20models%20to%20spatial%20context.%20Experimental%0Aresults%20show%20that%20LoRA%20achieves%20comparable%20or%20superior%20performance%20to%0Atraditional%20fine-tuning%20while%20significantly%20reducing%20computational%20costs%20and%0Atraining%20times%20by%20requiring%20fewer%20trainable%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12155v1&entry.124074799=Read"},
{"title": "Long-Tailed 3D Detection via Multi-Modal Fusion", "author": "Yechi Ma and Neehar Peri and Achal Dave and Wei Hua and Deva Ramanan and Shu Kong", "abstract": "  Contemporary autonomous vehicle (AV) benchmarks have advanced techniques for\ntraining 3D detectors. While class labels naturally follow a long-tailed\ndistribution in the real world, existing benchmarks only focus on a few common\nclasses (e.g., pedestrian and car) and neglect many rare but crucial classes\n(e.g., emergency vehicle and stroller). However, AVs must reliably detect both\ncommon and rare classes for safe operation in the open world. We address this\nchallenge by formally studying the problem of Long-Tailed 3D Detection (LT3D),\nwhich evaluates all annotated classes, including those in-the-tail. We address\nLT3D with hierarchical losses that promote feature sharing across classes, and\nintroduce diagnostic metrics that award partial credit to \"reasonable\" mistakes\nwith respect to the semantic hierarchy. Further, we point out that rare-class\naccuracy is particularly improved via multi-modal late fusion (MMLF) of\nindependently trained uni-modal LiDAR and RGB detectors. Such an MMLF framework\nallows us to leverage large-scale uni-modal datasets (with more examples for\nrare classes) to train better uni-modal detectors. Finally, we examine three\ncritical components of our simple MMLF approach from first principles: whether\nto train 2D or 3D RGB detectors for fusion, whether to match RGB and LiDAR\ndetections in 3D or the projected 2D image plane, and how to fuse matched\ndetections. Extensive experiments reveal that 2D RGB detectors achieve better\nrecognition accuracy for rare classes than 3D RGB detectors, matching on the 2D\nimage plane mitigates depth estimation errors for better matching, and score\ncalibration and probabilistic fusion notably improves the final performance\nfurther. Our MMLF significantly outperforms prior work for LT3D, particularly\nimproving on the six rarest classes from 12.8 to 20.0 mAP! Our code and models\nare available on our project page.\n", "link": "http://arxiv.org/abs/2312.10986v5", "date": "2025-09-15", "relevancy": 2.5139, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6332}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6303}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Tailed%203D%20Detection%20via%20Multi-Modal%20Fusion&body=Title%3A%20Long-Tailed%203D%20Detection%20via%20Multi-Modal%20Fusion%0AAuthor%3A%20Yechi%20Ma%20and%20Neehar%20Peri%20and%20Achal%20Dave%20and%20Wei%20Hua%20and%20Deva%20Ramanan%20and%20Shu%20Kong%0AAbstract%3A%20%20%20Contemporary%20autonomous%20vehicle%20%28AV%29%20benchmarks%20have%20advanced%20techniques%20for%0Atraining%203D%20detectors.%20While%20class%20labels%20naturally%20follow%20a%20long-tailed%0Adistribution%20in%20the%20real%20world%2C%20existing%20benchmarks%20only%20focus%20on%20a%20few%20common%0Aclasses%20%28e.g.%2C%20pedestrian%20and%20car%29%20and%20neglect%20many%20rare%20but%20crucial%20classes%0A%28e.g.%2C%20emergency%20vehicle%20and%20stroller%29.%20However%2C%20AVs%20must%20reliably%20detect%20both%0Acommon%20and%20rare%20classes%20for%20safe%20operation%20in%20the%20open%20world.%20We%20address%20this%0Achallenge%20by%20formally%20studying%20the%20problem%20of%20Long-Tailed%203D%20Detection%20%28LT3D%29%2C%0Awhich%20evaluates%20all%20annotated%20classes%2C%20including%20those%20in-the-tail.%20We%20address%0ALT3D%20with%20hierarchical%20losses%20that%20promote%20feature%20sharing%20across%20classes%2C%20and%0Aintroduce%20diagnostic%20metrics%20that%20award%20partial%20credit%20to%20%22reasonable%22%20mistakes%0Awith%20respect%20to%20the%20semantic%20hierarchy.%20Further%2C%20we%20point%20out%20that%20rare-class%0Aaccuracy%20is%20particularly%20improved%20via%20multi-modal%20late%20fusion%20%28MMLF%29%20of%0Aindependently%20trained%20uni-modal%20LiDAR%20and%20RGB%20detectors.%20Such%20an%20MMLF%20framework%0Aallows%20us%20to%20leverage%20large-scale%20uni-modal%20datasets%20%28with%20more%20examples%20for%0Arare%20classes%29%20to%20train%20better%20uni-modal%20detectors.%20Finally%2C%20we%20examine%20three%0Acritical%20components%20of%20our%20simple%20MMLF%20approach%20from%20first%20principles%3A%20whether%0Ato%20train%202D%20or%203D%20RGB%20detectors%20for%20fusion%2C%20whether%20to%20match%20RGB%20and%20LiDAR%0Adetections%20in%203D%20or%20the%20projected%202D%20image%20plane%2C%20and%20how%20to%20fuse%20matched%0Adetections.%20Extensive%20experiments%20reveal%20that%202D%20RGB%20detectors%20achieve%20better%0Arecognition%20accuracy%20for%20rare%20classes%20than%203D%20RGB%20detectors%2C%20matching%20on%20the%202D%0Aimage%20plane%20mitigates%20depth%20estimation%20errors%20for%20better%20matching%2C%20and%20score%0Acalibration%20and%20probabilistic%20fusion%20notably%20improves%20the%20final%20performance%0Afurther.%20Our%20MMLF%20significantly%20outperforms%20prior%20work%20for%20LT3D%2C%20particularly%0Aimproving%20on%20the%20six%20rarest%20classes%20from%2012.8%20to%2020.0%20mAP%21%20Our%20code%20and%20models%0Aare%20available%20on%20our%20project%20page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10986v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Tailed%25203D%2520Detection%2520via%2520Multi-Modal%2520Fusion%26entry.906535625%3DYechi%2520Ma%2520and%2520Neehar%2520Peri%2520and%2520Achal%2520Dave%2520and%2520Wei%2520Hua%2520and%2520Deva%2520Ramanan%2520and%2520Shu%2520Kong%26entry.1292438233%3D%2520%2520Contemporary%2520autonomous%2520vehicle%2520%2528AV%2529%2520benchmarks%2520have%2520advanced%2520techniques%2520for%250Atraining%25203D%2520detectors.%2520While%2520class%2520labels%2520naturally%2520follow%2520a%2520long-tailed%250Adistribution%2520in%2520the%2520real%2520world%252C%2520existing%2520benchmarks%2520only%2520focus%2520on%2520a%2520few%2520common%250Aclasses%2520%2528e.g.%252C%2520pedestrian%2520and%2520car%2529%2520and%2520neglect%2520many%2520rare%2520but%2520crucial%2520classes%250A%2528e.g.%252C%2520emergency%2520vehicle%2520and%2520stroller%2529.%2520However%252C%2520AVs%2520must%2520reliably%2520detect%2520both%250Acommon%2520and%2520rare%2520classes%2520for%2520safe%2520operation%2520in%2520the%2520open%2520world.%2520We%2520address%2520this%250Achallenge%2520by%2520formally%2520studying%2520the%2520problem%2520of%2520Long-Tailed%25203D%2520Detection%2520%2528LT3D%2529%252C%250Awhich%2520evaluates%2520all%2520annotated%2520classes%252C%2520including%2520those%2520in-the-tail.%2520We%2520address%250ALT3D%2520with%2520hierarchical%2520losses%2520that%2520promote%2520feature%2520sharing%2520across%2520classes%252C%2520and%250Aintroduce%2520diagnostic%2520metrics%2520that%2520award%2520partial%2520credit%2520to%2520%2522reasonable%2522%2520mistakes%250Awith%2520respect%2520to%2520the%2520semantic%2520hierarchy.%2520Further%252C%2520we%2520point%2520out%2520that%2520rare-class%250Aaccuracy%2520is%2520particularly%2520improved%2520via%2520multi-modal%2520late%2520fusion%2520%2528MMLF%2529%2520of%250Aindependently%2520trained%2520uni-modal%2520LiDAR%2520and%2520RGB%2520detectors.%2520Such%2520an%2520MMLF%2520framework%250Aallows%2520us%2520to%2520leverage%2520large-scale%2520uni-modal%2520datasets%2520%2528with%2520more%2520examples%2520for%250Arare%2520classes%2529%2520to%2520train%2520better%2520uni-modal%2520detectors.%2520Finally%252C%2520we%2520examine%2520three%250Acritical%2520components%2520of%2520our%2520simple%2520MMLF%2520approach%2520from%2520first%2520principles%253A%2520whether%250Ato%2520train%25202D%2520or%25203D%2520RGB%2520detectors%2520for%2520fusion%252C%2520whether%2520to%2520match%2520RGB%2520and%2520LiDAR%250Adetections%2520in%25203D%2520or%2520the%2520projected%25202D%2520image%2520plane%252C%2520and%2520how%2520to%2520fuse%2520matched%250Adetections.%2520Extensive%2520experiments%2520reveal%2520that%25202D%2520RGB%2520detectors%2520achieve%2520better%250Arecognition%2520accuracy%2520for%2520rare%2520classes%2520than%25203D%2520RGB%2520detectors%252C%2520matching%2520on%2520the%25202D%250Aimage%2520plane%2520mitigates%2520depth%2520estimation%2520errors%2520for%2520better%2520matching%252C%2520and%2520score%250Acalibration%2520and%2520probabilistic%2520fusion%2520notably%2520improves%2520the%2520final%2520performance%250Afurther.%2520Our%2520MMLF%2520significantly%2520outperforms%2520prior%2520work%2520for%2520LT3D%252C%2520particularly%250Aimproving%2520on%2520the%2520six%2520rarest%2520classes%2520from%252012.8%2520to%252020.0%2520mAP%2521%2520Our%2520code%2520and%2520models%250Aare%2520available%2520on%2520our%2520project%2520page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.10986v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Tailed%203D%20Detection%20via%20Multi-Modal%20Fusion&entry.906535625=Yechi%20Ma%20and%20Neehar%20Peri%20and%20Achal%20Dave%20and%20Wei%20Hua%20and%20Deva%20Ramanan%20and%20Shu%20Kong&entry.1292438233=%20%20Contemporary%20autonomous%20vehicle%20%28AV%29%20benchmarks%20have%20advanced%20techniques%20for%0Atraining%203D%20detectors.%20While%20class%20labels%20naturally%20follow%20a%20long-tailed%0Adistribution%20in%20the%20real%20world%2C%20existing%20benchmarks%20only%20focus%20on%20a%20few%20common%0Aclasses%20%28e.g.%2C%20pedestrian%20and%20car%29%20and%20neglect%20many%20rare%20but%20crucial%20classes%0A%28e.g.%2C%20emergency%20vehicle%20and%20stroller%29.%20However%2C%20AVs%20must%20reliably%20detect%20both%0Acommon%20and%20rare%20classes%20for%20safe%20operation%20in%20the%20open%20world.%20We%20address%20this%0Achallenge%20by%20formally%20studying%20the%20problem%20of%20Long-Tailed%203D%20Detection%20%28LT3D%29%2C%0Awhich%20evaluates%20all%20annotated%20classes%2C%20including%20those%20in-the-tail.%20We%20address%0ALT3D%20with%20hierarchical%20losses%20that%20promote%20feature%20sharing%20across%20classes%2C%20and%0Aintroduce%20diagnostic%20metrics%20that%20award%20partial%20credit%20to%20%22reasonable%22%20mistakes%0Awith%20respect%20to%20the%20semantic%20hierarchy.%20Further%2C%20we%20point%20out%20that%20rare-class%0Aaccuracy%20is%20particularly%20improved%20via%20multi-modal%20late%20fusion%20%28MMLF%29%20of%0Aindependently%20trained%20uni-modal%20LiDAR%20and%20RGB%20detectors.%20Such%20an%20MMLF%20framework%0Aallows%20us%20to%20leverage%20large-scale%20uni-modal%20datasets%20%28with%20more%20examples%20for%0Arare%20classes%29%20to%20train%20better%20uni-modal%20detectors.%20Finally%2C%20we%20examine%20three%0Acritical%20components%20of%20our%20simple%20MMLF%20approach%20from%20first%20principles%3A%20whether%0Ato%20train%202D%20or%203D%20RGB%20detectors%20for%20fusion%2C%20whether%20to%20match%20RGB%20and%20LiDAR%0Adetections%20in%203D%20or%20the%20projected%202D%20image%20plane%2C%20and%20how%20to%20fuse%20matched%0Adetections.%20Extensive%20experiments%20reveal%20that%202D%20RGB%20detectors%20achieve%20better%0Arecognition%20accuracy%20for%20rare%20classes%20than%203D%20RGB%20detectors%2C%20matching%20on%20the%202D%0Aimage%20plane%20mitigates%20depth%20estimation%20errors%20for%20better%20matching%2C%20and%20score%0Acalibration%20and%20probabilistic%20fusion%20notably%20improves%20the%20final%20performance%0Afurther.%20Our%20MMLF%20significantly%20outperforms%20prior%20work%20for%20LT3D%2C%20particularly%0Aimproving%20on%20the%20six%20rarest%20classes%20from%2012.8%20to%2020.0%20mAP%21%20Our%20code%20and%20models%0Aare%20available%20on%20our%20project%20page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10986v5&entry.124074799=Read"},
{"title": "LFRA-Net: A Lightweight Focal and Region-Aware Attention Network for\n  Retinal Vessel Segmentatio", "author": "Mehwish Mehmood and Shahzaib Iqbal and Tariq Mahmood Khan and Ivor Spence and Muhammad Fahim", "abstract": "  Retinal vessel segmentation is critical for the early diagnosis of\nvision-threatening and systemic diseases, especially in real-world clinical\nsettings with limited computational resources. Although significant\nimprovements have been made in deep learning-based segmentation methods,\ncurrent models still face challenges in extracting tiny vessels and suffer from\nhigh computational costs. In this study, we present LFRA-Net by incorporating\nfocal modulation attention at the encoder-decoder bottleneck and region-aware\nattention in the selective skip connections. LFRA-Net is a lightweight network\noptimized for precise and effective retinal vascular segmentation. It enhances\nfeature representation and regional focus by efficiently capturing local and\nglobal dependencies. LFRA-Net outperformed many state-of-the-art models while\nmaintaining lightweight characteristics with only 0.17 million parameters, 0.66\nMB memory size, and 10.50 GFLOPs. We validated it on three publicly available\ndatasets: DRIVE, STARE, and CHASE\\_DB. It performed better in terms of Dice\nscore (84.28\\%, 88.44\\%, and 85.50\\%) and Jaccard index (72.86\\%, 79.31\\%, and\n74.70\\%) on the DRIVE, STARE, and CHASE\\_DB datasets, respectively. LFRA-Net\nprovides an ideal ratio between segmentation accuracy and computational cost\ncompared to existing deep learning methods, which makes it suitable for\nreal-time clinical applications in areas with limited resources. The code can\nbe found at https://github.com/Mehwish4593/LFRA-Net.\n", "link": "http://arxiv.org/abs/2509.11811v1", "date": "2025-09-15", "relevancy": 2.4921, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5028}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5022}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LFRA-Net%3A%20A%20Lightweight%20Focal%20and%20Region-Aware%20Attention%20Network%20for%0A%20%20Retinal%20Vessel%20Segmentatio&body=Title%3A%20LFRA-Net%3A%20A%20Lightweight%20Focal%20and%20Region-Aware%20Attention%20Network%20for%0A%20%20Retinal%20Vessel%20Segmentatio%0AAuthor%3A%20Mehwish%20Mehmood%20and%20Shahzaib%20Iqbal%20and%20Tariq%20Mahmood%20Khan%20and%20Ivor%20Spence%20and%20Muhammad%20Fahim%0AAbstract%3A%20%20%20Retinal%20vessel%20segmentation%20is%20critical%20for%20the%20early%20diagnosis%20of%0Avision-threatening%20and%20systemic%20diseases%2C%20especially%20in%20real-world%20clinical%0Asettings%20with%20limited%20computational%20resources.%20Although%20significant%0Aimprovements%20have%20been%20made%20in%20deep%20learning-based%20segmentation%20methods%2C%0Acurrent%20models%20still%20face%20challenges%20in%20extracting%20tiny%20vessels%20and%20suffer%20from%0Ahigh%20computational%20costs.%20In%20this%20study%2C%20we%20present%20LFRA-Net%20by%20incorporating%0Afocal%20modulation%20attention%20at%20the%20encoder-decoder%20bottleneck%20and%20region-aware%0Aattention%20in%20the%20selective%20skip%20connections.%20LFRA-Net%20is%20a%20lightweight%20network%0Aoptimized%20for%20precise%20and%20effective%20retinal%20vascular%20segmentation.%20It%20enhances%0Afeature%20representation%20and%20regional%20focus%20by%20efficiently%20capturing%20local%20and%0Aglobal%20dependencies.%20LFRA-Net%20outperformed%20many%20state-of-the-art%20models%20while%0Amaintaining%20lightweight%20characteristics%20with%20only%200.17%20million%20parameters%2C%200.66%0AMB%20memory%20size%2C%20and%2010.50%20GFLOPs.%20We%20validated%20it%20on%20three%20publicly%20available%0Adatasets%3A%20DRIVE%2C%20STARE%2C%20and%20CHASE%5C_DB.%20It%20performed%20better%20in%20terms%20of%20Dice%0Ascore%20%2884.28%5C%25%2C%2088.44%5C%25%2C%20and%2085.50%5C%25%29%20and%20Jaccard%20index%20%2872.86%5C%25%2C%2079.31%5C%25%2C%20and%0A74.70%5C%25%29%20on%20the%20DRIVE%2C%20STARE%2C%20and%20CHASE%5C_DB%20datasets%2C%20respectively.%20LFRA-Net%0Aprovides%20an%20ideal%20ratio%20between%20segmentation%20accuracy%20and%20computational%20cost%0Acompared%20to%20existing%20deep%20learning%20methods%2C%20which%20makes%20it%20suitable%20for%0Areal-time%20clinical%20applications%20in%20areas%20with%20limited%20resources.%20The%20code%20can%0Abe%20found%20at%20https%3A//github.com/Mehwish4593/LFRA-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLFRA-Net%253A%2520A%2520Lightweight%2520Focal%2520and%2520Region-Aware%2520Attention%2520Network%2520for%250A%2520%2520Retinal%2520Vessel%2520Segmentatio%26entry.906535625%3DMehwish%2520Mehmood%2520and%2520Shahzaib%2520Iqbal%2520and%2520Tariq%2520Mahmood%2520Khan%2520and%2520Ivor%2520Spence%2520and%2520Muhammad%2520Fahim%26entry.1292438233%3D%2520%2520Retinal%2520vessel%2520segmentation%2520is%2520critical%2520for%2520the%2520early%2520diagnosis%2520of%250Avision-threatening%2520and%2520systemic%2520diseases%252C%2520especially%2520in%2520real-world%2520clinical%250Asettings%2520with%2520limited%2520computational%2520resources.%2520Although%2520significant%250Aimprovements%2520have%2520been%2520made%2520in%2520deep%2520learning-based%2520segmentation%2520methods%252C%250Acurrent%2520models%2520still%2520face%2520challenges%2520in%2520extracting%2520tiny%2520vessels%2520and%2520suffer%2520from%250Ahigh%2520computational%2520costs.%2520In%2520this%2520study%252C%2520we%2520present%2520LFRA-Net%2520by%2520incorporating%250Afocal%2520modulation%2520attention%2520at%2520the%2520encoder-decoder%2520bottleneck%2520and%2520region-aware%250Aattention%2520in%2520the%2520selective%2520skip%2520connections.%2520LFRA-Net%2520is%2520a%2520lightweight%2520network%250Aoptimized%2520for%2520precise%2520and%2520effective%2520retinal%2520vascular%2520segmentation.%2520It%2520enhances%250Afeature%2520representation%2520and%2520regional%2520focus%2520by%2520efficiently%2520capturing%2520local%2520and%250Aglobal%2520dependencies.%2520LFRA-Net%2520outperformed%2520many%2520state-of-the-art%2520models%2520while%250Amaintaining%2520lightweight%2520characteristics%2520with%2520only%25200.17%2520million%2520parameters%252C%25200.66%250AMB%2520memory%2520size%252C%2520and%252010.50%2520GFLOPs.%2520We%2520validated%2520it%2520on%2520three%2520publicly%2520available%250Adatasets%253A%2520DRIVE%252C%2520STARE%252C%2520and%2520CHASE%255C_DB.%2520It%2520performed%2520better%2520in%2520terms%2520of%2520Dice%250Ascore%2520%252884.28%255C%2525%252C%252088.44%255C%2525%252C%2520and%252085.50%255C%2525%2529%2520and%2520Jaccard%2520index%2520%252872.86%255C%2525%252C%252079.31%255C%2525%252C%2520and%250A74.70%255C%2525%2529%2520on%2520the%2520DRIVE%252C%2520STARE%252C%2520and%2520CHASE%255C_DB%2520datasets%252C%2520respectively.%2520LFRA-Net%250Aprovides%2520an%2520ideal%2520ratio%2520between%2520segmentation%2520accuracy%2520and%2520computational%2520cost%250Acompared%2520to%2520existing%2520deep%2520learning%2520methods%252C%2520which%2520makes%2520it%2520suitable%2520for%250Areal-time%2520clinical%2520applications%2520in%2520areas%2520with%2520limited%2520resources.%2520The%2520code%2520can%250Abe%2520found%2520at%2520https%253A//github.com/Mehwish4593/LFRA-Net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LFRA-Net%3A%20A%20Lightweight%20Focal%20and%20Region-Aware%20Attention%20Network%20for%0A%20%20Retinal%20Vessel%20Segmentatio&entry.906535625=Mehwish%20Mehmood%20and%20Shahzaib%20Iqbal%20and%20Tariq%20Mahmood%20Khan%20and%20Ivor%20Spence%20and%20Muhammad%20Fahim&entry.1292438233=%20%20Retinal%20vessel%20segmentation%20is%20critical%20for%20the%20early%20diagnosis%20of%0Avision-threatening%20and%20systemic%20diseases%2C%20especially%20in%20real-world%20clinical%0Asettings%20with%20limited%20computational%20resources.%20Although%20significant%0Aimprovements%20have%20been%20made%20in%20deep%20learning-based%20segmentation%20methods%2C%0Acurrent%20models%20still%20face%20challenges%20in%20extracting%20tiny%20vessels%20and%20suffer%20from%0Ahigh%20computational%20costs.%20In%20this%20study%2C%20we%20present%20LFRA-Net%20by%20incorporating%0Afocal%20modulation%20attention%20at%20the%20encoder-decoder%20bottleneck%20and%20region-aware%0Aattention%20in%20the%20selective%20skip%20connections.%20LFRA-Net%20is%20a%20lightweight%20network%0Aoptimized%20for%20precise%20and%20effective%20retinal%20vascular%20segmentation.%20It%20enhances%0Afeature%20representation%20and%20regional%20focus%20by%20efficiently%20capturing%20local%20and%0Aglobal%20dependencies.%20LFRA-Net%20outperformed%20many%20state-of-the-art%20models%20while%0Amaintaining%20lightweight%20characteristics%20with%20only%200.17%20million%20parameters%2C%200.66%0AMB%20memory%20size%2C%20and%2010.50%20GFLOPs.%20We%20validated%20it%20on%20three%20publicly%20available%0Adatasets%3A%20DRIVE%2C%20STARE%2C%20and%20CHASE%5C_DB.%20It%20performed%20better%20in%20terms%20of%20Dice%0Ascore%20%2884.28%5C%25%2C%2088.44%5C%25%2C%20and%2085.50%5C%25%29%20and%20Jaccard%20index%20%2872.86%5C%25%2C%2079.31%5C%25%2C%20and%0A74.70%5C%25%29%20on%20the%20DRIVE%2C%20STARE%2C%20and%20CHASE%5C_DB%20datasets%2C%20respectively.%20LFRA-Net%0Aprovides%20an%20ideal%20ratio%20between%20segmentation%20accuracy%20and%20computational%20cost%0Acompared%20to%20existing%20deep%20learning%20methods%2C%20which%20makes%20it%20suitable%20for%0Areal-time%20clinical%20applications%20in%20areas%20with%20limited%20resources.%20The%20code%20can%0Abe%20found%20at%20https%3A//github.com/Mehwish4593/LFRA-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11811v1&entry.124074799=Read"},
{"title": "Intrinsic Training Signals for Federated Learning Aggregation", "author": "Cosimo Fiorini and Matteo Mosconi and Pietro Buzzega and Riccardo Salami and Simone Calderara", "abstract": "  Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy. While existing approaches\nfor aggregating client-specific classification heads and adapted backbone\nparameters require architectural modifications or loss function changes, our\nmethod uniquely leverages intrinsic training signals already available during\nstandard optimization. We present LIVAR (Layer Importance and VARiance-based\nmerging), which introduces: i) a variance-weighted classifier aggregation\nscheme using naturally emergent feature statistics, and ii) an\nexplainability-driven LoRA merging technique based on SHAP analysis of existing\nupdate parameter patterns. Without any architectural overhead, LIVAR achieves\nstate-of-the-art performance on multiple benchmarks while maintaining seamless\nintegration with existing FL methods. This work demonstrates that effective\nmodel merging can be achieved solely through existing training signals,\nestablishing a new paradigm for efficient federated model aggregation. The code\nis available at https://github.com/aimagelab/fed-mammoth.\n", "link": "http://arxiv.org/abs/2507.06813v2", "date": "2025-09-15", "relevancy": 2.4847, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5023}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4994}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intrinsic%20Training%20Signals%20for%20Federated%20Learning%20Aggregation&body=Title%3A%20Intrinsic%20Training%20Signals%20for%20Federated%20Learning%20Aggregation%0AAuthor%3A%20Cosimo%20Fiorini%20and%20Matteo%20Mosconi%20and%20Pietro%20Buzzega%20and%20Riccardo%20Salami%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20clients%20while%20preserving%20data%20privacy.%20While%20existing%20approaches%0Afor%20aggregating%20client-specific%20classification%20heads%20and%20adapted%20backbone%0Aparameters%20require%20architectural%20modifications%20or%20loss%20function%20changes%2C%20our%0Amethod%20uniquely%20leverages%20intrinsic%20training%20signals%20already%20available%20during%0Astandard%20optimization.%20We%20present%20LIVAR%20%28Layer%20Importance%20and%20VARiance-based%0Amerging%29%2C%20which%20introduces%3A%20i%29%20a%20variance-weighted%20classifier%20aggregation%0Ascheme%20using%20naturally%20emergent%20feature%20statistics%2C%20and%20ii%29%20an%0Aexplainability-driven%20LoRA%20merging%20technique%20based%20on%20SHAP%20analysis%20of%20existing%0Aupdate%20parameter%20patterns.%20Without%20any%20architectural%20overhead%2C%20LIVAR%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%20while%20maintaining%20seamless%0Aintegration%20with%20existing%20FL%20methods.%20This%20work%20demonstrates%20that%20effective%0Amodel%20merging%20can%20be%20achieved%20solely%20through%20existing%20training%20signals%2C%0Aestablishing%20a%20new%20paradigm%20for%20efficient%20federated%20model%20aggregation.%20The%20code%0Ais%20available%20at%20https%3A//github.com/aimagelab/fed-mammoth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06813v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntrinsic%2520Training%2520Signals%2520for%2520Federated%2520Learning%2520Aggregation%26entry.906535625%3DCosimo%2520Fiorini%2520and%2520Matteo%2520Mosconi%2520and%2520Pietro%2520Buzzega%2520and%2520Riccardo%2520Salami%2520and%2520Simone%2520Calderara%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520collaborative%2520model%2520training%2520across%250Adistributed%2520clients%2520while%2520preserving%2520data%2520privacy.%2520While%2520existing%2520approaches%250Afor%2520aggregating%2520client-specific%2520classification%2520heads%2520and%2520adapted%2520backbone%250Aparameters%2520require%2520architectural%2520modifications%2520or%2520loss%2520function%2520changes%252C%2520our%250Amethod%2520uniquely%2520leverages%2520intrinsic%2520training%2520signals%2520already%2520available%2520during%250Astandard%2520optimization.%2520We%2520present%2520LIVAR%2520%2528Layer%2520Importance%2520and%2520VARiance-based%250Amerging%2529%252C%2520which%2520introduces%253A%2520i%2529%2520a%2520variance-weighted%2520classifier%2520aggregation%250Ascheme%2520using%2520naturally%2520emergent%2520feature%2520statistics%252C%2520and%2520ii%2529%2520an%250Aexplainability-driven%2520LoRA%2520merging%2520technique%2520based%2520on%2520SHAP%2520analysis%2520of%2520existing%250Aupdate%2520parameter%2520patterns.%2520Without%2520any%2520architectural%2520overhead%252C%2520LIVAR%2520achieves%250Astate-of-the-art%2520performance%2520on%2520multiple%2520benchmarks%2520while%2520maintaining%2520seamless%250Aintegration%2520with%2520existing%2520FL%2520methods.%2520This%2520work%2520demonstrates%2520that%2520effective%250Amodel%2520merging%2520can%2520be%2520achieved%2520solely%2520through%2520existing%2520training%2520signals%252C%250Aestablishing%2520a%2520new%2520paradigm%2520for%2520efficient%2520federated%2520model%2520aggregation.%2520The%2520code%250Ais%2520available%2520at%2520https%253A//github.com/aimagelab/fed-mammoth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06813v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intrinsic%20Training%20Signals%20for%20Federated%20Learning%20Aggregation&entry.906535625=Cosimo%20Fiorini%20and%20Matteo%20Mosconi%20and%20Pietro%20Buzzega%20and%20Riccardo%20Salami%20and%20Simone%20Calderara&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20clients%20while%20preserving%20data%20privacy.%20While%20existing%20approaches%0Afor%20aggregating%20client-specific%20classification%20heads%20and%20adapted%20backbone%0Aparameters%20require%20architectural%20modifications%20or%20loss%20function%20changes%2C%20our%0Amethod%20uniquely%20leverages%20intrinsic%20training%20signals%20already%20available%20during%0Astandard%20optimization.%20We%20present%20LIVAR%20%28Layer%20Importance%20and%20VARiance-based%0Amerging%29%2C%20which%20introduces%3A%20i%29%20a%20variance-weighted%20classifier%20aggregation%0Ascheme%20using%20naturally%20emergent%20feature%20statistics%2C%20and%20ii%29%20an%0Aexplainability-driven%20LoRA%20merging%20technique%20based%20on%20SHAP%20analysis%20of%20existing%0Aupdate%20parameter%20patterns.%20Without%20any%20architectural%20overhead%2C%20LIVAR%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%20while%20maintaining%20seamless%0Aintegration%20with%20existing%20FL%20methods.%20This%20work%20demonstrates%20that%20effective%0Amodel%20merging%20can%20be%20achieved%20solely%20through%20existing%20training%20signals%2C%0Aestablishing%20a%20new%20paradigm%20for%20efficient%20federated%20model%20aggregation.%20The%20code%0Ais%20available%20at%20https%3A//github.com/aimagelab/fed-mammoth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06813v2&entry.124074799=Read"},
{"title": "Is In-Context Learning Learning?", "author": "Adrian de Wynter", "abstract": "  In-context learning (ICL) allows some autoregressive models to solve tasks\nvia next-token prediction and without needing further training. This has led to\nclaims about these model's ability to solve (learn) unseen tasks with only a\nfew shots (exemplars) in the prompt. However, deduction does not always imply\nlearning, as ICL does not explicitly encode a given observation. Instead, the\nmodels rely on their prior knowledge and the exemplars given, if any. We argue\nthat, mathematically, ICL does constitute learning, but its full\ncharacterisation requires empirical work. We then carry out a large-scale\nanalysis of ICL ablating out or accounting for memorisation, pretraining,\ndistributional shifts, and prompting style and phrasing. We find that ICL is an\neffective learning paradigm, but limited in its ability to learn and generalise\nto unseen tasks. We note that, in the limit where exemplars become more\nnumerous, accuracy is insensitive to exemplar distribution, model, prompt\nstyle, and the input's linguistic features. Instead, it deduces patterns from\nregularities in the prompt, which leads to distributional sensitivity,\nespecially in prompting styles such as chain-of-thought. Given the varied\naccuracies on formally similar tasks, we conclude that autoregression's ad-hoc\nencoding is not a robust mechanism, and suggests limited all-purpose\ngeneralisability.\n", "link": "http://arxiv.org/abs/2509.10414v2", "date": "2025-09-15", "relevancy": 2.4825, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4955}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20In-Context%20Learning%20Learning%3F&body=Title%3A%20Is%20In-Context%20Learning%20Learning%3F%0AAuthor%3A%20Adrian%20de%20Wynter%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20allows%20some%20autoregressive%20models%20to%20solve%20tasks%0Avia%20next-token%20prediction%20and%20without%20needing%20further%20training.%20This%20has%20led%20to%0Aclaims%20about%20these%20model%27s%20ability%20to%20solve%20%28learn%29%20unseen%20tasks%20with%20only%20a%0Afew%20shots%20%28exemplars%29%20in%20the%20prompt.%20However%2C%20deduction%20does%20not%20always%20imply%0Alearning%2C%20as%20ICL%20does%20not%20explicitly%20encode%20a%20given%20observation.%20Instead%2C%20the%0Amodels%20rely%20on%20their%20prior%20knowledge%20and%20the%20exemplars%20given%2C%20if%20any.%20We%20argue%0Athat%2C%20mathematically%2C%20ICL%20does%20constitute%20learning%2C%20but%20its%20full%0Acharacterisation%20requires%20empirical%20work.%20We%20then%20carry%20out%20a%20large-scale%0Aanalysis%20of%20ICL%20ablating%20out%20or%20accounting%20for%20memorisation%2C%20pretraining%2C%0Adistributional%20shifts%2C%20and%20prompting%20style%20and%20phrasing.%20We%20find%20that%20ICL%20is%20an%0Aeffective%20learning%20paradigm%2C%20but%20limited%20in%20its%20ability%20to%20learn%20and%20generalise%0Ato%20unseen%20tasks.%20We%20note%20that%2C%20in%20the%20limit%20where%20exemplars%20become%20more%0Anumerous%2C%20accuracy%20is%20insensitive%20to%20exemplar%20distribution%2C%20model%2C%20prompt%0Astyle%2C%20and%20the%20input%27s%20linguistic%20features.%20Instead%2C%20it%20deduces%20patterns%20from%0Aregularities%20in%20the%20prompt%2C%20which%20leads%20to%20distributional%20sensitivity%2C%0Aespecially%20in%20prompting%20styles%20such%20as%20chain-of-thought.%20Given%20the%20varied%0Aaccuracies%20on%20formally%20similar%20tasks%2C%20we%20conclude%20that%20autoregression%27s%20ad-hoc%0Aencoding%20is%20not%20a%20robust%20mechanism%2C%20and%20suggests%20limited%20all-purpose%0Ageneralisability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.10414v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520In-Context%2520Learning%2520Learning%253F%26entry.906535625%3DAdrian%2520de%2520Wynter%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520allows%2520some%2520autoregressive%2520models%2520to%2520solve%2520tasks%250Avia%2520next-token%2520prediction%2520and%2520without%2520needing%2520further%2520training.%2520This%2520has%2520led%2520to%250Aclaims%2520about%2520these%2520model%2527s%2520ability%2520to%2520solve%2520%2528learn%2529%2520unseen%2520tasks%2520with%2520only%2520a%250Afew%2520shots%2520%2528exemplars%2529%2520in%2520the%2520prompt.%2520However%252C%2520deduction%2520does%2520not%2520always%2520imply%250Alearning%252C%2520as%2520ICL%2520does%2520not%2520explicitly%2520encode%2520a%2520given%2520observation.%2520Instead%252C%2520the%250Amodels%2520rely%2520on%2520their%2520prior%2520knowledge%2520and%2520the%2520exemplars%2520given%252C%2520if%2520any.%2520We%2520argue%250Athat%252C%2520mathematically%252C%2520ICL%2520does%2520constitute%2520learning%252C%2520but%2520its%2520full%250Acharacterisation%2520requires%2520empirical%2520work.%2520We%2520then%2520carry%2520out%2520a%2520large-scale%250Aanalysis%2520of%2520ICL%2520ablating%2520out%2520or%2520accounting%2520for%2520memorisation%252C%2520pretraining%252C%250Adistributional%2520shifts%252C%2520and%2520prompting%2520style%2520and%2520phrasing.%2520We%2520find%2520that%2520ICL%2520is%2520an%250Aeffective%2520learning%2520paradigm%252C%2520but%2520limited%2520in%2520its%2520ability%2520to%2520learn%2520and%2520generalise%250Ato%2520unseen%2520tasks.%2520We%2520note%2520that%252C%2520in%2520the%2520limit%2520where%2520exemplars%2520become%2520more%250Anumerous%252C%2520accuracy%2520is%2520insensitive%2520to%2520exemplar%2520distribution%252C%2520model%252C%2520prompt%250Astyle%252C%2520and%2520the%2520input%2527s%2520linguistic%2520features.%2520Instead%252C%2520it%2520deduces%2520patterns%2520from%250Aregularities%2520in%2520the%2520prompt%252C%2520which%2520leads%2520to%2520distributional%2520sensitivity%252C%250Aespecially%2520in%2520prompting%2520styles%2520such%2520as%2520chain-of-thought.%2520Given%2520the%2520varied%250Aaccuracies%2520on%2520formally%2520similar%2520tasks%252C%2520we%2520conclude%2520that%2520autoregression%2527s%2520ad-hoc%250Aencoding%2520is%2520not%2520a%2520robust%2520mechanism%252C%2520and%2520suggests%2520limited%2520all-purpose%250Ageneralisability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10414v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20In-Context%20Learning%20Learning%3F&entry.906535625=Adrian%20de%20Wynter&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20allows%20some%20autoregressive%20models%20to%20solve%20tasks%0Avia%20next-token%20prediction%20and%20without%20needing%20further%20training.%20This%20has%20led%20to%0Aclaims%20about%20these%20model%27s%20ability%20to%20solve%20%28learn%29%20unseen%20tasks%20with%20only%20a%0Afew%20shots%20%28exemplars%29%20in%20the%20prompt.%20However%2C%20deduction%20does%20not%20always%20imply%0Alearning%2C%20as%20ICL%20does%20not%20explicitly%20encode%20a%20given%20observation.%20Instead%2C%20the%0Amodels%20rely%20on%20their%20prior%20knowledge%20and%20the%20exemplars%20given%2C%20if%20any.%20We%20argue%0Athat%2C%20mathematically%2C%20ICL%20does%20constitute%20learning%2C%20but%20its%20full%0Acharacterisation%20requires%20empirical%20work.%20We%20then%20carry%20out%20a%20large-scale%0Aanalysis%20of%20ICL%20ablating%20out%20or%20accounting%20for%20memorisation%2C%20pretraining%2C%0Adistributional%20shifts%2C%20and%20prompting%20style%20and%20phrasing.%20We%20find%20that%20ICL%20is%20an%0Aeffective%20learning%20paradigm%2C%20but%20limited%20in%20its%20ability%20to%20learn%20and%20generalise%0Ato%20unseen%20tasks.%20We%20note%20that%2C%20in%20the%20limit%20where%20exemplars%20become%20more%0Anumerous%2C%20accuracy%20is%20insensitive%20to%20exemplar%20distribution%2C%20model%2C%20prompt%0Astyle%2C%20and%20the%20input%27s%20linguistic%20features.%20Instead%2C%20it%20deduces%20patterns%20from%0Aregularities%20in%20the%20prompt%2C%20which%20leads%20to%20distributional%20sensitivity%2C%0Aespecially%20in%20prompting%20styles%20such%20as%20chain-of-thought.%20Given%20the%20varied%0Aaccuracies%20on%20formally%20similar%20tasks%2C%20we%20conclude%20that%20autoregression%27s%20ad-hoc%0Aencoding%20is%20not%20a%20robust%20mechanism%2C%20and%20suggests%20limited%20all-purpose%0Ageneralisability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.10414v2&entry.124074799=Read"},
{"title": "Bridging Vision Language Models and Symbolic Grounding for Video\n  Question Answering", "author": "Haodi Ma and Vyom Pathak and Daisy Zhe Wang", "abstract": "  Video Question Answering (VQA) requires models to reason over spatial,\ntemporal, and causal cues in videos. Recent vision language models (VLMs)\nachieve strong results but often rely on shallow correlations, leading to weak\ntemporal grounding and limited interpretability. We study symbolic scene graphs\n(SGs) as intermediate grounding signals for VQA. SGs provide structured\nobject-relation representations that complement VLMs holistic reasoning. We\nintroduce SG-VLM, a modular framework that integrates frozen VLMs with scene\ngraph grounding via prompting and visual localization. Across three benchmarks\n(NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLM\nimproves causal and temporal reasoning and outperforms prior baselines, though\ngains over strong VLMs are limited. These findings highlight both the promise\nand current limitations of symbolic grounding, and offer guidance for future\nhybrid VLM-symbolic approaches in video understanding.\n", "link": "http://arxiv.org/abs/2509.11862v1", "date": "2025-09-15", "relevancy": 2.4808, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6388}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6388}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Vision%20Language%20Models%20and%20Symbolic%20Grounding%20for%20Video%0A%20%20Question%20Answering&body=Title%3A%20Bridging%20Vision%20Language%20Models%20and%20Symbolic%20Grounding%20for%20Video%0A%20%20Question%20Answering%0AAuthor%3A%20Haodi%20Ma%20and%20Vyom%20Pathak%20and%20Daisy%20Zhe%20Wang%0AAbstract%3A%20%20%20Video%20Question%20Answering%20%28VQA%29%20requires%20models%20to%20reason%20over%20spatial%2C%0Atemporal%2C%20and%20causal%20cues%20in%20videos.%20Recent%20vision%20language%20models%20%28VLMs%29%0Aachieve%20strong%20results%20but%20often%20rely%20on%20shallow%20correlations%2C%20leading%20to%20weak%0Atemporal%20grounding%20and%20limited%20interpretability.%20We%20study%20symbolic%20scene%20graphs%0A%28SGs%29%20as%20intermediate%20grounding%20signals%20for%20VQA.%20SGs%20provide%20structured%0Aobject-relation%20representations%20that%20complement%20VLMs%20holistic%20reasoning.%20We%0Aintroduce%20SG-VLM%2C%20a%20modular%20framework%20that%20integrates%20frozen%20VLMs%20with%20scene%0Agraph%20grounding%20via%20prompting%20and%20visual%20localization.%20Across%20three%20benchmarks%0A%28NExT-QA%2C%20iVQA%2C%20ActivityNet-QA%29%20and%20multiple%20VLMs%20%28QwenVL%2C%20InternVL%29%2C%20SG-VLM%0Aimproves%20causal%20and%20temporal%20reasoning%20and%20outperforms%20prior%20baselines%2C%20though%0Agains%20over%20strong%20VLMs%20are%20limited.%20These%20findings%20highlight%20both%20the%20promise%0Aand%20current%20limitations%20of%20symbolic%20grounding%2C%20and%20offer%20guidance%20for%20future%0Ahybrid%20VLM-symbolic%20approaches%20in%20video%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Vision%2520Language%2520Models%2520and%2520Symbolic%2520Grounding%2520for%2520Video%250A%2520%2520Question%2520Answering%26entry.906535625%3DHaodi%2520Ma%2520and%2520Vyom%2520Pathak%2520and%2520Daisy%2520Zhe%2520Wang%26entry.1292438233%3D%2520%2520Video%2520Question%2520Answering%2520%2528VQA%2529%2520requires%2520models%2520to%2520reason%2520over%2520spatial%252C%250Atemporal%252C%2520and%2520causal%2520cues%2520in%2520videos.%2520Recent%2520vision%2520language%2520models%2520%2528VLMs%2529%250Aachieve%2520strong%2520results%2520but%2520often%2520rely%2520on%2520shallow%2520correlations%252C%2520leading%2520to%2520weak%250Atemporal%2520grounding%2520and%2520limited%2520interpretability.%2520We%2520study%2520symbolic%2520scene%2520graphs%250A%2528SGs%2529%2520as%2520intermediate%2520grounding%2520signals%2520for%2520VQA.%2520SGs%2520provide%2520structured%250Aobject-relation%2520representations%2520that%2520complement%2520VLMs%2520holistic%2520reasoning.%2520We%250Aintroduce%2520SG-VLM%252C%2520a%2520modular%2520framework%2520that%2520integrates%2520frozen%2520VLMs%2520with%2520scene%250Agraph%2520grounding%2520via%2520prompting%2520and%2520visual%2520localization.%2520Across%2520three%2520benchmarks%250A%2528NExT-QA%252C%2520iVQA%252C%2520ActivityNet-QA%2529%2520and%2520multiple%2520VLMs%2520%2528QwenVL%252C%2520InternVL%2529%252C%2520SG-VLM%250Aimproves%2520causal%2520and%2520temporal%2520reasoning%2520and%2520outperforms%2520prior%2520baselines%252C%2520though%250Agains%2520over%2520strong%2520VLMs%2520are%2520limited.%2520These%2520findings%2520highlight%2520both%2520the%2520promise%250Aand%2520current%2520limitations%2520of%2520symbolic%2520grounding%252C%2520and%2520offer%2520guidance%2520for%2520future%250Ahybrid%2520VLM-symbolic%2520approaches%2520in%2520video%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Vision%20Language%20Models%20and%20Symbolic%20Grounding%20for%20Video%0A%20%20Question%20Answering&entry.906535625=Haodi%20Ma%20and%20Vyom%20Pathak%20and%20Daisy%20Zhe%20Wang&entry.1292438233=%20%20Video%20Question%20Answering%20%28VQA%29%20requires%20models%20to%20reason%20over%20spatial%2C%0Atemporal%2C%20and%20causal%20cues%20in%20videos.%20Recent%20vision%20language%20models%20%28VLMs%29%0Aachieve%20strong%20results%20but%20often%20rely%20on%20shallow%20correlations%2C%20leading%20to%20weak%0Atemporal%20grounding%20and%20limited%20interpretability.%20We%20study%20symbolic%20scene%20graphs%0A%28SGs%29%20as%20intermediate%20grounding%20signals%20for%20VQA.%20SGs%20provide%20structured%0Aobject-relation%20representations%20that%20complement%20VLMs%20holistic%20reasoning.%20We%0Aintroduce%20SG-VLM%2C%20a%20modular%20framework%20that%20integrates%20frozen%20VLMs%20with%20scene%0Agraph%20grounding%20via%20prompting%20and%20visual%20localization.%20Across%20three%20benchmarks%0A%28NExT-QA%2C%20iVQA%2C%20ActivityNet-QA%29%20and%20multiple%20VLMs%20%28QwenVL%2C%20InternVL%29%2C%20SG-VLM%0Aimproves%20causal%20and%20temporal%20reasoning%20and%20outperforms%20prior%20baselines%2C%20though%0Agains%20over%20strong%20VLMs%20are%20limited.%20These%20findings%20highlight%20both%20the%20promise%0Aand%20current%20limitations%20of%20symbolic%20grounding%2C%20and%20offer%20guidance%20for%20future%0Ahybrid%20VLM-symbolic%20approaches%20in%20video%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11862v1&entry.124074799=Read"},
{"title": "A GPU-Accelerated RAG-Based Telegram Assistant for Supporting Parallel\n  Processing Students", "author": "Guy Tel-Zur", "abstract": "  This project addresses a critical pedagogical need: offering students\ncontinuous, on-demand academic assistance beyond conventional reception hours.\nI present a domain-specific Retrieval-Augmented Generation (RAG) system powered\nby a quantized Mistral-7B Instruct model and deployed as a Telegram bot. The\nassistant enhances learning by delivering real-time, personalized responses\naligned with the \"Introduction to Parallel Processing\" course materials. GPU\nacceleration significantly improves inference latency, enabling practical\ndeployment on consumer hardware. This approach demonstrates how consumer GPUs\ncan enable affordable, private, and effective AI tutoring for HPC education.\n", "link": "http://arxiv.org/abs/2509.11947v1", "date": "2025-09-15", "relevancy": 2.4551, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5139}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4804}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20GPU-Accelerated%20RAG-Based%20Telegram%20Assistant%20for%20Supporting%20Parallel%0A%20%20Processing%20Students&body=Title%3A%20A%20GPU-Accelerated%20RAG-Based%20Telegram%20Assistant%20for%20Supporting%20Parallel%0A%20%20Processing%20Students%0AAuthor%3A%20Guy%20Tel-Zur%0AAbstract%3A%20%20%20This%20project%20addresses%20a%20critical%20pedagogical%20need%3A%20offering%20students%0Acontinuous%2C%20on-demand%20academic%20assistance%20beyond%20conventional%20reception%20hours.%0AI%20present%20a%20domain-specific%20Retrieval-Augmented%20Generation%20%28RAG%29%20system%20powered%0Aby%20a%20quantized%20Mistral-7B%20Instruct%20model%20and%20deployed%20as%20a%20Telegram%20bot.%20The%0Aassistant%20enhances%20learning%20by%20delivering%20real-time%2C%20personalized%20responses%0Aaligned%20with%20the%20%22Introduction%20to%20Parallel%20Processing%22%20course%20materials.%20GPU%0Aacceleration%20significantly%20improves%20inference%20latency%2C%20enabling%20practical%0Adeployment%20on%20consumer%20hardware.%20This%20approach%20demonstrates%20how%20consumer%20GPUs%0Acan%20enable%20affordable%2C%20private%2C%20and%20effective%20AI%20tutoring%20for%20HPC%20education.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520GPU-Accelerated%2520RAG-Based%2520Telegram%2520Assistant%2520for%2520Supporting%2520Parallel%250A%2520%2520Processing%2520Students%26entry.906535625%3DGuy%2520Tel-Zur%26entry.1292438233%3D%2520%2520This%2520project%2520addresses%2520a%2520critical%2520pedagogical%2520need%253A%2520offering%2520students%250Acontinuous%252C%2520on-demand%2520academic%2520assistance%2520beyond%2520conventional%2520reception%2520hours.%250AI%2520present%2520a%2520domain-specific%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520system%2520powered%250Aby%2520a%2520quantized%2520Mistral-7B%2520Instruct%2520model%2520and%2520deployed%2520as%2520a%2520Telegram%2520bot.%2520The%250Aassistant%2520enhances%2520learning%2520by%2520delivering%2520real-time%252C%2520personalized%2520responses%250Aaligned%2520with%2520the%2520%2522Introduction%2520to%2520Parallel%2520Processing%2522%2520course%2520materials.%2520GPU%250Aacceleration%2520significantly%2520improves%2520inference%2520latency%252C%2520enabling%2520practical%250Adeployment%2520on%2520consumer%2520hardware.%2520This%2520approach%2520demonstrates%2520how%2520consumer%2520GPUs%250Acan%2520enable%2520affordable%252C%2520private%252C%2520and%2520effective%2520AI%2520tutoring%2520for%2520HPC%2520education.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20GPU-Accelerated%20RAG-Based%20Telegram%20Assistant%20for%20Supporting%20Parallel%0A%20%20Processing%20Students&entry.906535625=Guy%20Tel-Zur&entry.1292438233=%20%20This%20project%20addresses%20a%20critical%20pedagogical%20need%3A%20offering%20students%0Acontinuous%2C%20on-demand%20academic%20assistance%20beyond%20conventional%20reception%20hours.%0AI%20present%20a%20domain-specific%20Retrieval-Augmented%20Generation%20%28RAG%29%20system%20powered%0Aby%20a%20quantized%20Mistral-7B%20Instruct%20model%20and%20deployed%20as%20a%20Telegram%20bot.%20The%0Aassistant%20enhances%20learning%20by%20delivering%20real-time%2C%20personalized%20responses%0Aaligned%20with%20the%20%22Introduction%20to%20Parallel%20Processing%22%20course%20materials.%20GPU%0Aacceleration%20significantly%20improves%20inference%20latency%2C%20enabling%20practical%0Adeployment%20on%20consumer%20hardware.%20This%20approach%20demonstrates%20how%20consumer%20GPUs%0Acan%20enable%20affordable%2C%20private%2C%20and%20effective%20AI%20tutoring%20for%20HPC%20education.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11947v1&entry.124074799=Read"},
{"title": "Learning to Generate 4D LiDAR Sequences", "author": "Ao Liang and Youquan Liu and Yu Yang and Dongyue Lu and Linfeng Li and Lingdong Kong and Huaici Zhao and Wei Tsang Ooi", "abstract": "  While generative world models have advanced video and occupancy-based data\nsynthesis, LiDAR generation remains underexplored despite its importance for\naccurate 3D perception. Extending generation to 4D LiDAR data introduces\nchallenges in controllability, temporal stability, and evaluation. We present\nLiDARCrafter, a unified framework that converts free-form language into\neditable LiDAR sequences. Instructions are parsed into ego-centric scene\ngraphs, which a tri-branch diffusion model transforms into object layouts,\ntrajectories, and shapes. A range-image diffusion model generates the initial\nscan, and an autoregressive module extends it into a temporally coherent\nsequence. The explicit layout design further supports object-level editing,\nsuch as insertion or relocation. To enable fair assessment, we provide\nEvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On\nnuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and\ntemporal consistency, offering a foundation for LiDAR-based simulation and data\naugmentation.\n", "link": "http://arxiv.org/abs/2509.11959v1", "date": "2025-09-15", "relevancy": 2.4392, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6119}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6119}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Generate%204D%20LiDAR%20Sequences&body=Title%3A%20Learning%20to%20Generate%204D%20LiDAR%20Sequences%0AAuthor%3A%20Ao%20Liang%20and%20Youquan%20Liu%20and%20Yu%20Yang%20and%20Dongyue%20Lu%20and%20Linfeng%20Li%20and%20Lingdong%20Kong%20and%20Huaici%20Zhao%20and%20Wei%20Tsang%20Ooi%0AAbstract%3A%20%20%20While%20generative%20world%20models%20have%20advanced%20video%20and%20occupancy-based%20data%0Asynthesis%2C%20LiDAR%20generation%20remains%20underexplored%20despite%20its%20importance%20for%0Aaccurate%203D%20perception.%20Extending%20generation%20to%204D%20LiDAR%20data%20introduces%0Achallenges%20in%20controllability%2C%20temporal%20stability%2C%20and%20evaluation.%20We%20present%0ALiDARCrafter%2C%20a%20unified%20framework%20that%20converts%20free-form%20language%20into%0Aeditable%20LiDAR%20sequences.%20Instructions%20are%20parsed%20into%20ego-centric%20scene%0Agraphs%2C%20which%20a%20tri-branch%20diffusion%20model%20transforms%20into%20object%20layouts%2C%0Atrajectories%2C%20and%20shapes.%20A%20range-image%20diffusion%20model%20generates%20the%20initial%0Ascan%2C%20and%20an%20autoregressive%20module%20extends%20it%20into%20a%20temporally%20coherent%0Asequence.%20The%20explicit%20layout%20design%20further%20supports%20object-level%20editing%2C%0Asuch%20as%20insertion%20or%20relocation.%20To%20enable%20fair%20assessment%2C%20we%20provide%0AEvalSuite%2C%20a%20benchmark%20spanning%20scene-%2C%20object-%2C%20and%20sequence-level%20metrics.%20On%0AnuScenes%2C%20LiDARCrafter%20achieves%20state-of-the-art%20fidelity%2C%20controllability%2C%20and%0Atemporal%20consistency%2C%20offering%20a%20foundation%20for%20LiDAR-based%20simulation%20and%20data%0Aaugmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Generate%25204D%2520LiDAR%2520Sequences%26entry.906535625%3DAo%2520Liang%2520and%2520Youquan%2520Liu%2520and%2520Yu%2520Yang%2520and%2520Dongyue%2520Lu%2520and%2520Linfeng%2520Li%2520and%2520Lingdong%2520Kong%2520and%2520Huaici%2520Zhao%2520and%2520Wei%2520Tsang%2520Ooi%26entry.1292438233%3D%2520%2520While%2520generative%2520world%2520models%2520have%2520advanced%2520video%2520and%2520occupancy-based%2520data%250Asynthesis%252C%2520LiDAR%2520generation%2520remains%2520underexplored%2520despite%2520its%2520importance%2520for%250Aaccurate%25203D%2520perception.%2520Extending%2520generation%2520to%25204D%2520LiDAR%2520data%2520introduces%250Achallenges%2520in%2520controllability%252C%2520temporal%2520stability%252C%2520and%2520evaluation.%2520We%2520present%250ALiDARCrafter%252C%2520a%2520unified%2520framework%2520that%2520converts%2520free-form%2520language%2520into%250Aeditable%2520LiDAR%2520sequences.%2520Instructions%2520are%2520parsed%2520into%2520ego-centric%2520scene%250Agraphs%252C%2520which%2520a%2520tri-branch%2520diffusion%2520model%2520transforms%2520into%2520object%2520layouts%252C%250Atrajectories%252C%2520and%2520shapes.%2520A%2520range-image%2520diffusion%2520model%2520generates%2520the%2520initial%250Ascan%252C%2520and%2520an%2520autoregressive%2520module%2520extends%2520it%2520into%2520a%2520temporally%2520coherent%250Asequence.%2520The%2520explicit%2520layout%2520design%2520further%2520supports%2520object-level%2520editing%252C%250Asuch%2520as%2520insertion%2520or%2520relocation.%2520To%2520enable%2520fair%2520assessment%252C%2520we%2520provide%250AEvalSuite%252C%2520a%2520benchmark%2520spanning%2520scene-%252C%2520object-%252C%2520and%2520sequence-level%2520metrics.%2520On%250AnuScenes%252C%2520LiDARCrafter%2520achieves%2520state-of-the-art%2520fidelity%252C%2520controllability%252C%2520and%250Atemporal%2520consistency%252C%2520offering%2520a%2520foundation%2520for%2520LiDAR-based%2520simulation%2520and%2520data%250Aaugmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Generate%204D%20LiDAR%20Sequences&entry.906535625=Ao%20Liang%20and%20Youquan%20Liu%20and%20Yu%20Yang%20and%20Dongyue%20Lu%20and%20Linfeng%20Li%20and%20Lingdong%20Kong%20and%20Huaici%20Zhao%20and%20Wei%20Tsang%20Ooi&entry.1292438233=%20%20While%20generative%20world%20models%20have%20advanced%20video%20and%20occupancy-based%20data%0Asynthesis%2C%20LiDAR%20generation%20remains%20underexplored%20despite%20its%20importance%20for%0Aaccurate%203D%20perception.%20Extending%20generation%20to%204D%20LiDAR%20data%20introduces%0Achallenges%20in%20controllability%2C%20temporal%20stability%2C%20and%20evaluation.%20We%20present%0ALiDARCrafter%2C%20a%20unified%20framework%20that%20converts%20free-form%20language%20into%0Aeditable%20LiDAR%20sequences.%20Instructions%20are%20parsed%20into%20ego-centric%20scene%0Agraphs%2C%20which%20a%20tri-branch%20diffusion%20model%20transforms%20into%20object%20layouts%2C%0Atrajectories%2C%20and%20shapes.%20A%20range-image%20diffusion%20model%20generates%20the%20initial%0Ascan%2C%20and%20an%20autoregressive%20module%20extends%20it%20into%20a%20temporally%20coherent%0Asequence.%20The%20explicit%20layout%20design%20further%20supports%20object-level%20editing%2C%0Asuch%20as%20insertion%20or%20relocation.%20To%20enable%20fair%20assessment%2C%20we%20provide%0AEvalSuite%2C%20a%20benchmark%20spanning%20scene-%2C%20object-%2C%20and%20sequence-level%20metrics.%20On%0AnuScenes%2C%20LiDARCrafter%20achieves%20state-of-the-art%20fidelity%2C%20controllability%2C%20and%0Atemporal%20consistency%2C%20offering%20a%20foundation%20for%20LiDAR-based%20simulation%20and%20data%0Aaugmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11959v1&entry.124074799=Read"},
{"title": "InstructHumans: Editing Animated 3D Human Textures with Instructions", "author": "Jiayin Zhu and Linlin Yang and Angela Yao", "abstract": "  We present InstructHumans, a novel framework for instruction-driven\n{animatable} 3D human texture editing. Existing text-based 3D editing methods\noften directly apply Score Distillation Sampling (SDS). SDS, designed for\ngeneration tasks, cannot account for the defining requirement of editing --\nmaintaining consistency with the source avatar. This work shows that naively\nusing SDS harms editing, as it may destroy consistency. We propose a modified\nSDS for Editing (SDS-E) that selectively incorporates subterms of SDS across\ndiffusion timesteps. We further enhance SDS-E with spatial smoothness\nregularization and gradient-based viewpoint sampling for edits with sharp and\nhigh-fidelity detailing. Incorporating SDS-E into a 3D human texture editing\nframework allows us to outperform existing 3D editing methods. Our avatars\nfaithfully reflect the textual edits while remaining consistent with the\noriginal avatars. Project page: https://jyzhu.top/instruct-humans/.\n", "link": "http://arxiv.org/abs/2404.04037v2", "date": "2025-09-15", "relevancy": 2.4245, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6598}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5711}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstructHumans%3A%20Editing%20Animated%203D%20Human%20Textures%20with%20Instructions&body=Title%3A%20InstructHumans%3A%20Editing%20Animated%203D%20Human%20Textures%20with%20Instructions%0AAuthor%3A%20Jiayin%20Zhu%20and%20Linlin%20Yang%20and%20Angela%20Yao%0AAbstract%3A%20%20%20We%20present%20InstructHumans%2C%20a%20novel%20framework%20for%20instruction-driven%0A%7Banimatable%7D%203D%20human%20texture%20editing.%20Existing%20text-based%203D%20editing%20methods%0Aoften%20directly%20apply%20Score%20Distillation%20Sampling%20%28SDS%29.%20SDS%2C%20designed%20for%0Ageneration%20tasks%2C%20cannot%20account%20for%20the%20defining%20requirement%20of%20editing%20--%0Amaintaining%20consistency%20with%20the%20source%20avatar.%20This%20work%20shows%20that%20naively%0Ausing%20SDS%20harms%20editing%2C%20as%20it%20may%20destroy%20consistency.%20We%20propose%20a%20modified%0ASDS%20for%20Editing%20%28SDS-E%29%20that%20selectively%20incorporates%20subterms%20of%20SDS%20across%0Adiffusion%20timesteps.%20We%20further%20enhance%20SDS-E%20with%20spatial%20smoothness%0Aregularization%20and%20gradient-based%20viewpoint%20sampling%20for%20edits%20with%20sharp%20and%0Ahigh-fidelity%20detailing.%20Incorporating%20SDS-E%20into%20a%203D%20human%20texture%20editing%0Aframework%20allows%20us%20to%20outperform%20existing%203D%20editing%20methods.%20Our%20avatars%0Afaithfully%20reflect%20the%20textual%20edits%20while%20remaining%20consistent%20with%20the%0Aoriginal%20avatars.%20Project%20page%3A%20https%3A//jyzhu.top/instruct-humans/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04037v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstructHumans%253A%2520Editing%2520Animated%25203D%2520Human%2520Textures%2520with%2520Instructions%26entry.906535625%3DJiayin%2520Zhu%2520and%2520Linlin%2520Yang%2520and%2520Angela%2520Yao%26entry.1292438233%3D%2520%2520We%2520present%2520InstructHumans%252C%2520a%2520novel%2520framework%2520for%2520instruction-driven%250A%257Banimatable%257D%25203D%2520human%2520texture%2520editing.%2520Existing%2520text-based%25203D%2520editing%2520methods%250Aoften%2520directly%2520apply%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529.%2520SDS%252C%2520designed%2520for%250Ageneration%2520tasks%252C%2520cannot%2520account%2520for%2520the%2520defining%2520requirement%2520of%2520editing%2520--%250Amaintaining%2520consistency%2520with%2520the%2520source%2520avatar.%2520This%2520work%2520shows%2520that%2520naively%250Ausing%2520SDS%2520harms%2520editing%252C%2520as%2520it%2520may%2520destroy%2520consistency.%2520We%2520propose%2520a%2520modified%250ASDS%2520for%2520Editing%2520%2528SDS-E%2529%2520that%2520selectively%2520incorporates%2520subterms%2520of%2520SDS%2520across%250Adiffusion%2520timesteps.%2520We%2520further%2520enhance%2520SDS-E%2520with%2520spatial%2520smoothness%250Aregularization%2520and%2520gradient-based%2520viewpoint%2520sampling%2520for%2520edits%2520with%2520sharp%2520and%250Ahigh-fidelity%2520detailing.%2520Incorporating%2520SDS-E%2520into%2520a%25203D%2520human%2520texture%2520editing%250Aframework%2520allows%2520us%2520to%2520outperform%2520existing%25203D%2520editing%2520methods.%2520Our%2520avatars%250Afaithfully%2520reflect%2520the%2520textual%2520edits%2520while%2520remaining%2520consistent%2520with%2520the%250Aoriginal%2520avatars.%2520Project%2520page%253A%2520https%253A//jyzhu.top/instruct-humans/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04037v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructHumans%3A%20Editing%20Animated%203D%20Human%20Textures%20with%20Instructions&entry.906535625=Jiayin%20Zhu%20and%20Linlin%20Yang%20and%20Angela%20Yao&entry.1292438233=%20%20We%20present%20InstructHumans%2C%20a%20novel%20framework%20for%20instruction-driven%0A%7Banimatable%7D%203D%20human%20texture%20editing.%20Existing%20text-based%203D%20editing%20methods%0Aoften%20directly%20apply%20Score%20Distillation%20Sampling%20%28SDS%29.%20SDS%2C%20designed%20for%0Ageneration%20tasks%2C%20cannot%20account%20for%20the%20defining%20requirement%20of%20editing%20--%0Amaintaining%20consistency%20with%20the%20source%20avatar.%20This%20work%20shows%20that%20naively%0Ausing%20SDS%20harms%20editing%2C%20as%20it%20may%20destroy%20consistency.%20We%20propose%20a%20modified%0ASDS%20for%20Editing%20%28SDS-E%29%20that%20selectively%20incorporates%20subterms%20of%20SDS%20across%0Adiffusion%20timesteps.%20We%20further%20enhance%20SDS-E%20with%20spatial%20smoothness%0Aregularization%20and%20gradient-based%20viewpoint%20sampling%20for%20edits%20with%20sharp%20and%0Ahigh-fidelity%20detailing.%20Incorporating%20SDS-E%20into%20a%203D%20human%20texture%20editing%0Aframework%20allows%20us%20to%20outperform%20existing%203D%20editing%20methods.%20Our%20avatars%0Afaithfully%20reflect%20the%20textual%20edits%20while%20remaining%20consistent%20with%20the%0Aoriginal%20avatars.%20Project%20page%3A%20https%3A//jyzhu.top/instruct-humans/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04037v2&entry.124074799=Read"},
{"title": "Integrating Prior Observations for Incremental 3D Scene Graph Prediction", "author": "Marian Renz and Felix Igelbrink and Martin Atzmueller", "abstract": "  3D semantic scene graphs (3DSSG) provide compact structured representations\nof environments by explicitly modeling objects, attributes, and relationships.\nWhile 3DSSGs have shown promise in robotics and embodied AI, many existing\nmethods rely mainly on sensor data, not integrating further information from\nsemantically rich environments. Additionally, most methods assume access to\ncomplete scene reconstructions, limiting their applicability in real-world,\nincremental settings. This paper introduces a novel heterogeneous graph model\nfor incremental 3DSSG prediction that integrates additional, multi-modal\ninformation, such as prior observations, directly into the message-passing\nprocess. Utilizing multiple layers, the model flexibly incorporates global and\nlocal scene representations without requiring specialized modules or full scene\nreconstructions. We evaluate our approach on the 3DSSG dataset, showing that\nGNNs enriched with multi-modal information such as semantic embeddings (e.g.,\nCLIP) and prior observations offer a scalable and generalizable solution for\ncomplex, real-world environments. The full source code of the presented\narchitecture will be made available at\nhttps://github.com/m4renz/incremental-scene-graph-prediction.\n", "link": "http://arxiv.org/abs/2509.11895v1", "date": "2025-09-15", "relevancy": 2.4221, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6057}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6057}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Prior%20Observations%20for%20Incremental%203D%20Scene%20Graph%20Prediction&body=Title%3A%20Integrating%20Prior%20Observations%20for%20Incremental%203D%20Scene%20Graph%20Prediction%0AAuthor%3A%20Marian%20Renz%20and%20Felix%20Igelbrink%20and%20Martin%20Atzmueller%0AAbstract%3A%20%20%203D%20semantic%20scene%20graphs%20%283DSSG%29%20provide%20compact%20structured%20representations%0Aof%20environments%20by%20explicitly%20modeling%20objects%2C%20attributes%2C%20and%20relationships.%0AWhile%203DSSGs%20have%20shown%20promise%20in%20robotics%20and%20embodied%20AI%2C%20many%20existing%0Amethods%20rely%20mainly%20on%20sensor%20data%2C%20not%20integrating%20further%20information%20from%0Asemantically%20rich%20environments.%20Additionally%2C%20most%20methods%20assume%20access%20to%0Acomplete%20scene%20reconstructions%2C%20limiting%20their%20applicability%20in%20real-world%2C%0Aincremental%20settings.%20This%20paper%20introduces%20a%20novel%20heterogeneous%20graph%20model%0Afor%20incremental%203DSSG%20prediction%20that%20integrates%20additional%2C%20multi-modal%0Ainformation%2C%20such%20as%20prior%20observations%2C%20directly%20into%20the%20message-passing%0Aprocess.%20Utilizing%20multiple%20layers%2C%20the%20model%20flexibly%20incorporates%20global%20and%0Alocal%20scene%20representations%20without%20requiring%20specialized%20modules%20or%20full%20scene%0Areconstructions.%20We%20evaluate%20our%20approach%20on%20the%203DSSG%20dataset%2C%20showing%20that%0AGNNs%20enriched%20with%20multi-modal%20information%20such%20as%20semantic%20embeddings%20%28e.g.%2C%0ACLIP%29%20and%20prior%20observations%20offer%20a%20scalable%20and%20generalizable%20solution%20for%0Acomplex%2C%20real-world%20environments.%20The%20full%20source%20code%20of%20the%20presented%0Aarchitecture%20will%20be%20made%20available%20at%0Ahttps%3A//github.com/m4renz/incremental-scene-graph-prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Prior%2520Observations%2520for%2520Incremental%25203D%2520Scene%2520Graph%2520Prediction%26entry.906535625%3DMarian%2520Renz%2520and%2520Felix%2520Igelbrink%2520and%2520Martin%2520Atzmueller%26entry.1292438233%3D%2520%25203D%2520semantic%2520scene%2520graphs%2520%25283DSSG%2529%2520provide%2520compact%2520structured%2520representations%250Aof%2520environments%2520by%2520explicitly%2520modeling%2520objects%252C%2520attributes%252C%2520and%2520relationships.%250AWhile%25203DSSGs%2520have%2520shown%2520promise%2520in%2520robotics%2520and%2520embodied%2520AI%252C%2520many%2520existing%250Amethods%2520rely%2520mainly%2520on%2520sensor%2520data%252C%2520not%2520integrating%2520further%2520information%2520from%250Asemantically%2520rich%2520environments.%2520Additionally%252C%2520most%2520methods%2520assume%2520access%2520to%250Acomplete%2520scene%2520reconstructions%252C%2520limiting%2520their%2520applicability%2520in%2520real-world%252C%250Aincremental%2520settings.%2520This%2520paper%2520introduces%2520a%2520novel%2520heterogeneous%2520graph%2520model%250Afor%2520incremental%25203DSSG%2520prediction%2520that%2520integrates%2520additional%252C%2520multi-modal%250Ainformation%252C%2520such%2520as%2520prior%2520observations%252C%2520directly%2520into%2520the%2520message-passing%250Aprocess.%2520Utilizing%2520multiple%2520layers%252C%2520the%2520model%2520flexibly%2520incorporates%2520global%2520and%250Alocal%2520scene%2520representations%2520without%2520requiring%2520specialized%2520modules%2520or%2520full%2520scene%250Areconstructions.%2520We%2520evaluate%2520our%2520approach%2520on%2520the%25203DSSG%2520dataset%252C%2520showing%2520that%250AGNNs%2520enriched%2520with%2520multi-modal%2520information%2520such%2520as%2520semantic%2520embeddings%2520%2528e.g.%252C%250ACLIP%2529%2520and%2520prior%2520observations%2520offer%2520a%2520scalable%2520and%2520generalizable%2520solution%2520for%250Acomplex%252C%2520real-world%2520environments.%2520The%2520full%2520source%2520code%2520of%2520the%2520presented%250Aarchitecture%2520will%2520be%2520made%2520available%2520at%250Ahttps%253A//github.com/m4renz/incremental-scene-graph-prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Prior%20Observations%20for%20Incremental%203D%20Scene%20Graph%20Prediction&entry.906535625=Marian%20Renz%20and%20Felix%20Igelbrink%20and%20Martin%20Atzmueller&entry.1292438233=%20%203D%20semantic%20scene%20graphs%20%283DSSG%29%20provide%20compact%20structured%20representations%0Aof%20environments%20by%20explicitly%20modeling%20objects%2C%20attributes%2C%20and%20relationships.%0AWhile%203DSSGs%20have%20shown%20promise%20in%20robotics%20and%20embodied%20AI%2C%20many%20existing%0Amethods%20rely%20mainly%20on%20sensor%20data%2C%20not%20integrating%20further%20information%20from%0Asemantically%20rich%20environments.%20Additionally%2C%20most%20methods%20assume%20access%20to%0Acomplete%20scene%20reconstructions%2C%20limiting%20their%20applicability%20in%20real-world%2C%0Aincremental%20settings.%20This%20paper%20introduces%20a%20novel%20heterogeneous%20graph%20model%0Afor%20incremental%203DSSG%20prediction%20that%20integrates%20additional%2C%20multi-modal%0Ainformation%2C%20such%20as%20prior%20observations%2C%20directly%20into%20the%20message-passing%0Aprocess.%20Utilizing%20multiple%20layers%2C%20the%20model%20flexibly%20incorporates%20global%20and%0Alocal%20scene%20representations%20without%20requiring%20specialized%20modules%20or%20full%20scene%0Areconstructions.%20We%20evaluate%20our%20approach%20on%20the%203DSSG%20dataset%2C%20showing%20that%0AGNNs%20enriched%20with%20multi-modal%20information%20such%20as%20semantic%20embeddings%20%28e.g.%2C%0ACLIP%29%20and%20prior%20observations%20offer%20a%20scalable%20and%20generalizable%20solution%20for%0Acomplex%2C%20real-world%20environments.%20The%20full%20source%20code%20of%20the%20presented%0Aarchitecture%20will%20be%20made%20available%20at%0Ahttps%3A//github.com/m4renz/incremental-scene-graph-prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11895v1&entry.124074799=Read"},
{"title": "Learning Neural Networks by Neuron Pursuit", "author": "Akshay Kumar and Jarvis Haupt", "abstract": "  The first part of this paper studies the evolution of gradient flow for\nhomogeneous neural networks near a class of saddle points exhibiting a sparsity\nstructure. The choice of these saddle points is motivated from previous works\non homogeneous networks, which identified the first saddle point encountered by\ngradient flow after escaping the origin. It is shown here that, when\ninitialized sufficiently close to such saddle points, gradient flow remains\nnear the saddle point for a sufficiently long time, during which the set of\nweights with small norm remain small but converge in direction. Furthermore,\nimportant empirical observations are made on the behavior of gradient descent\nafter escaping these saddle points. The second part of the paper, motivated by\nthese results, introduces a greedy algorithm to train deep neural networks\ncalled Neuron Pursuit (NP). It is an iterative procedure which alternates\nbetween expanding the network by adding neuron(s) with carefully chosen\nweights, and minimizing the training loss using this augmented network. The\nefficacy of the proposed algorithm is validated using numerical experiments.\n", "link": "http://arxiv.org/abs/2509.12154v1", "date": "2025-09-15", "relevancy": 2.4147, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.543}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4656}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Neural%20Networks%20by%20Neuron%20Pursuit&body=Title%3A%20Learning%20Neural%20Networks%20by%20Neuron%20Pursuit%0AAuthor%3A%20Akshay%20Kumar%20and%20Jarvis%20Haupt%0AAbstract%3A%20%20%20The%20first%20part%20of%20this%20paper%20studies%20the%20evolution%20of%20gradient%20flow%20for%0Ahomogeneous%20neural%20networks%20near%20a%20class%20of%20saddle%20points%20exhibiting%20a%20sparsity%0Astructure.%20The%20choice%20of%20these%20saddle%20points%20is%20motivated%20from%20previous%20works%0Aon%20homogeneous%20networks%2C%20which%20identified%20the%20first%20saddle%20point%20encountered%20by%0Agradient%20flow%20after%20escaping%20the%20origin.%20It%20is%20shown%20here%20that%2C%20when%0Ainitialized%20sufficiently%20close%20to%20such%20saddle%20points%2C%20gradient%20flow%20remains%0Anear%20the%20saddle%20point%20for%20a%20sufficiently%20long%20time%2C%20during%20which%20the%20set%20of%0Aweights%20with%20small%20norm%20remain%20small%20but%20converge%20in%20direction.%20Furthermore%2C%0Aimportant%20empirical%20observations%20are%20made%20on%20the%20behavior%20of%20gradient%20descent%0Aafter%20escaping%20these%20saddle%20points.%20The%20second%20part%20of%20the%20paper%2C%20motivated%20by%0Athese%20results%2C%20introduces%20a%20greedy%20algorithm%20to%20train%20deep%20neural%20networks%0Acalled%20Neuron%20Pursuit%20%28NP%29.%20It%20is%20an%20iterative%20procedure%20which%20alternates%0Abetween%20expanding%20the%20network%20by%20adding%20neuron%28s%29%20with%20carefully%20chosen%0Aweights%2C%20and%20minimizing%20the%20training%20loss%20using%20this%20augmented%20network.%20The%0Aefficacy%20of%20the%20proposed%20algorithm%20is%20validated%20using%20numerical%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Neural%2520Networks%2520by%2520Neuron%2520Pursuit%26entry.906535625%3DAkshay%2520Kumar%2520and%2520Jarvis%2520Haupt%26entry.1292438233%3D%2520%2520The%2520first%2520part%2520of%2520this%2520paper%2520studies%2520the%2520evolution%2520of%2520gradient%2520flow%2520for%250Ahomogeneous%2520neural%2520networks%2520near%2520a%2520class%2520of%2520saddle%2520points%2520exhibiting%2520a%2520sparsity%250Astructure.%2520The%2520choice%2520of%2520these%2520saddle%2520points%2520is%2520motivated%2520from%2520previous%2520works%250Aon%2520homogeneous%2520networks%252C%2520which%2520identified%2520the%2520first%2520saddle%2520point%2520encountered%2520by%250Agradient%2520flow%2520after%2520escaping%2520the%2520origin.%2520It%2520is%2520shown%2520here%2520that%252C%2520when%250Ainitialized%2520sufficiently%2520close%2520to%2520such%2520saddle%2520points%252C%2520gradient%2520flow%2520remains%250Anear%2520the%2520saddle%2520point%2520for%2520a%2520sufficiently%2520long%2520time%252C%2520during%2520which%2520the%2520set%2520of%250Aweights%2520with%2520small%2520norm%2520remain%2520small%2520but%2520converge%2520in%2520direction.%2520Furthermore%252C%250Aimportant%2520empirical%2520observations%2520are%2520made%2520on%2520the%2520behavior%2520of%2520gradient%2520descent%250Aafter%2520escaping%2520these%2520saddle%2520points.%2520The%2520second%2520part%2520of%2520the%2520paper%252C%2520motivated%2520by%250Athese%2520results%252C%2520introduces%2520a%2520greedy%2520algorithm%2520to%2520train%2520deep%2520neural%2520networks%250Acalled%2520Neuron%2520Pursuit%2520%2528NP%2529.%2520It%2520is%2520an%2520iterative%2520procedure%2520which%2520alternates%250Abetween%2520expanding%2520the%2520network%2520by%2520adding%2520neuron%2528s%2529%2520with%2520carefully%2520chosen%250Aweights%252C%2520and%2520minimizing%2520the%2520training%2520loss%2520using%2520this%2520augmented%2520network.%2520The%250Aefficacy%2520of%2520the%2520proposed%2520algorithm%2520is%2520validated%2520using%2520numerical%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Neural%20Networks%20by%20Neuron%20Pursuit&entry.906535625=Akshay%20Kumar%20and%20Jarvis%20Haupt&entry.1292438233=%20%20The%20first%20part%20of%20this%20paper%20studies%20the%20evolution%20of%20gradient%20flow%20for%0Ahomogeneous%20neural%20networks%20near%20a%20class%20of%20saddle%20points%20exhibiting%20a%20sparsity%0Astructure.%20The%20choice%20of%20these%20saddle%20points%20is%20motivated%20from%20previous%20works%0Aon%20homogeneous%20networks%2C%20which%20identified%20the%20first%20saddle%20point%20encountered%20by%0Agradient%20flow%20after%20escaping%20the%20origin.%20It%20is%20shown%20here%20that%2C%20when%0Ainitialized%20sufficiently%20close%20to%20such%20saddle%20points%2C%20gradient%20flow%20remains%0Anear%20the%20saddle%20point%20for%20a%20sufficiently%20long%20time%2C%20during%20which%20the%20set%20of%0Aweights%20with%20small%20norm%20remain%20small%20but%20converge%20in%20direction.%20Furthermore%2C%0Aimportant%20empirical%20observations%20are%20made%20on%20the%20behavior%20of%20gradient%20descent%0Aafter%20escaping%20these%20saddle%20points.%20The%20second%20part%20of%20the%20paper%2C%20motivated%20by%0Athese%20results%2C%20introduces%20a%20greedy%20algorithm%20to%20train%20deep%20neural%20networks%0Acalled%20Neuron%20Pursuit%20%28NP%29.%20It%20is%20an%20iterative%20procedure%20which%20alternates%0Abetween%20expanding%20the%20network%20by%20adding%20neuron%28s%29%20with%20carefully%20chosen%0Aweights%2C%20and%20minimizing%20the%20training%20loss%20using%20this%20augmented%20network.%20The%0Aefficacy%20of%20the%20proposed%20algorithm%20is%20validated%20using%20numerical%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12154v1&entry.124074799=Read"},
{"title": "GATEAU: Selecting Influential Samples for Long Context Alignment", "author": "Shuzheng Si and Haozhe Zhao and Gang Chen and Yunshui Li and Kangyang Luo and Chuancheng Lv and Kaikai An and Fanchao Qi and Baobao Chang and Maosong Sun", "abstract": "  Aligning large language models to handle instructions with extremely long\ncontexts has yet to be fully investigated. Previous studies have attempted to\nscale up the available data volume by synthesizing long instruction-following\nsamples, as constructing such a dataset tends to be challenging for annotators.\nHowever, a lack of a well-defined strategy for ensuring data quality may\nintroduce low-quality samples and restrict the model's performance. Thus, we\npropose GATEAU, a novel framework to address the unique challenge of long\ncontext alignment by identifying the influential samples enriched with\nlong-range dependency relations. Specifically, GATEAU measures the long-range\ndependencies from two essential aspects: the difficulty of generating target\nresponses due to the long-range dependencies, and the difficulty of\nunderstanding long inputs due to such dependencies. Comprehensive experiments\nindicate that GATEAU effectively identifies influential samples, and the model\ntrained on these selected samples exhibits better instruction-following and\nlong-context understanding capabilities.\n", "link": "http://arxiv.org/abs/2410.15633v7", "date": "2025-09-15", "relevancy": 2.4034, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4998}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4815}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GATEAU%3A%20Selecting%20Influential%20Samples%20for%20Long%20Context%20Alignment&body=Title%3A%20GATEAU%3A%20Selecting%20Influential%20Samples%20for%20Long%20Context%20Alignment%0AAuthor%3A%20Shuzheng%20Si%20and%20Haozhe%20Zhao%20and%20Gang%20Chen%20and%20Yunshui%20Li%20and%20Kangyang%20Luo%20and%20Chuancheng%20Lv%20and%20Kaikai%20An%20and%20Fanchao%20Qi%20and%20Baobao%20Chang%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Aligning%20large%20language%20models%20to%20handle%20instructions%20with%20extremely%20long%0Acontexts%20has%20yet%20to%20be%20fully%20investigated.%20Previous%20studies%20have%20attempted%20to%0Ascale%20up%20the%20available%20data%20volume%20by%20synthesizing%20long%20instruction-following%0Asamples%2C%20as%20constructing%20such%20a%20dataset%20tends%20to%20be%20challenging%20for%20annotators.%0AHowever%2C%20a%20lack%20of%20a%20well-defined%20strategy%20for%20ensuring%20data%20quality%20may%0Aintroduce%20low-quality%20samples%20and%20restrict%20the%20model%27s%20performance.%20Thus%2C%20we%0Apropose%20GATEAU%2C%20a%20novel%20framework%20to%20address%20the%20unique%20challenge%20of%20long%0Acontext%20alignment%20by%20identifying%20the%20influential%20samples%20enriched%20with%0Along-range%20dependency%20relations.%20Specifically%2C%20GATEAU%20measures%20the%20long-range%0Adependencies%20from%20two%20essential%20aspects%3A%20the%20difficulty%20of%20generating%20target%0Aresponses%20due%20to%20the%20long-range%20dependencies%2C%20and%20the%20difficulty%20of%0Aunderstanding%20long%20inputs%20due%20to%20such%20dependencies.%20Comprehensive%20experiments%0Aindicate%20that%20GATEAU%20effectively%20identifies%20influential%20samples%2C%20and%20the%20model%0Atrained%20on%20these%20selected%20samples%20exhibits%20better%20instruction-following%20and%0Along-context%20understanding%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15633v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGATEAU%253A%2520Selecting%2520Influential%2520Samples%2520for%2520Long%2520Context%2520Alignment%26entry.906535625%3DShuzheng%2520Si%2520and%2520Haozhe%2520Zhao%2520and%2520Gang%2520Chen%2520and%2520Yunshui%2520Li%2520and%2520Kangyang%2520Luo%2520and%2520Chuancheng%2520Lv%2520and%2520Kaikai%2520An%2520and%2520Fanchao%2520Qi%2520and%2520Baobao%2520Chang%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Aligning%2520large%2520language%2520models%2520to%2520handle%2520instructions%2520with%2520extremely%2520long%250Acontexts%2520has%2520yet%2520to%2520be%2520fully%2520investigated.%2520Previous%2520studies%2520have%2520attempted%2520to%250Ascale%2520up%2520the%2520available%2520data%2520volume%2520by%2520synthesizing%2520long%2520instruction-following%250Asamples%252C%2520as%2520constructing%2520such%2520a%2520dataset%2520tends%2520to%2520be%2520challenging%2520for%2520annotators.%250AHowever%252C%2520a%2520lack%2520of%2520a%2520well-defined%2520strategy%2520for%2520ensuring%2520data%2520quality%2520may%250Aintroduce%2520low-quality%2520samples%2520and%2520restrict%2520the%2520model%2527s%2520performance.%2520Thus%252C%2520we%250Apropose%2520GATEAU%252C%2520a%2520novel%2520framework%2520to%2520address%2520the%2520unique%2520challenge%2520of%2520long%250Acontext%2520alignment%2520by%2520identifying%2520the%2520influential%2520samples%2520enriched%2520with%250Along-range%2520dependency%2520relations.%2520Specifically%252C%2520GATEAU%2520measures%2520the%2520long-range%250Adependencies%2520from%2520two%2520essential%2520aspects%253A%2520the%2520difficulty%2520of%2520generating%2520target%250Aresponses%2520due%2520to%2520the%2520long-range%2520dependencies%252C%2520and%2520the%2520difficulty%2520of%250Aunderstanding%2520long%2520inputs%2520due%2520to%2520such%2520dependencies.%2520Comprehensive%2520experiments%250Aindicate%2520that%2520GATEAU%2520effectively%2520identifies%2520influential%2520samples%252C%2520and%2520the%2520model%250Atrained%2520on%2520these%2520selected%2520samples%2520exhibits%2520better%2520instruction-following%2520and%250Along-context%2520understanding%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15633v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GATEAU%3A%20Selecting%20Influential%20Samples%20for%20Long%20Context%20Alignment&entry.906535625=Shuzheng%20Si%20and%20Haozhe%20Zhao%20and%20Gang%20Chen%20and%20Yunshui%20Li%20and%20Kangyang%20Luo%20and%20Chuancheng%20Lv%20and%20Kaikai%20An%20and%20Fanchao%20Qi%20and%20Baobao%20Chang%20and%20Maosong%20Sun&entry.1292438233=%20%20Aligning%20large%20language%20models%20to%20handle%20instructions%20with%20extremely%20long%0Acontexts%20has%20yet%20to%20be%20fully%20investigated.%20Previous%20studies%20have%20attempted%20to%0Ascale%20up%20the%20available%20data%20volume%20by%20synthesizing%20long%20instruction-following%0Asamples%2C%20as%20constructing%20such%20a%20dataset%20tends%20to%20be%20challenging%20for%20annotators.%0AHowever%2C%20a%20lack%20of%20a%20well-defined%20strategy%20for%20ensuring%20data%20quality%20may%0Aintroduce%20low-quality%20samples%20and%20restrict%20the%20model%27s%20performance.%20Thus%2C%20we%0Apropose%20GATEAU%2C%20a%20novel%20framework%20to%20address%20the%20unique%20challenge%20of%20long%0Acontext%20alignment%20by%20identifying%20the%20influential%20samples%20enriched%20with%0Along-range%20dependency%20relations.%20Specifically%2C%20GATEAU%20measures%20the%20long-range%0Adependencies%20from%20two%20essential%20aspects%3A%20the%20difficulty%20of%20generating%20target%0Aresponses%20due%20to%20the%20long-range%20dependencies%2C%20and%20the%20difficulty%20of%0Aunderstanding%20long%20inputs%20due%20to%20such%20dependencies.%20Comprehensive%20experiments%0Aindicate%20that%20GATEAU%20effectively%20identifies%20influential%20samples%2C%20and%20the%20model%0Atrained%20on%20these%20selected%20samples%20exhibits%20better%20instruction-following%20and%0Along-context%20understanding%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15633v7&entry.124074799=Read"},
{"title": "User eXperience Perception Insights Dataset (UXPID): Synthetic User\n  Feedback from Public Industrial Forums", "author": "Mikhail Kulyabin and Jan Joosten and Choro Ulan uulu and Nuno Miguel Martins Pacheco and Fabian Ries and Filippos Petridis and Jan Bosch and Helena Holmstr\u00f6m Olsson", "abstract": "  Customer feedback in industrial forums reflect a rich but underexplored\nsource of insight into real-world product experience. These publicly shared\ndiscussions offer an organic view of user expectations, frustrations, and\nsuccess stories shaped by the specific contexts of use. Yet, harnessing this\ninformation for systematic analysis remains challenging due to the unstructured\nand domain-specific nature of the content. The lack of structure and\nspecialized vocabulary makes it difficult for traditional data analysis\ntechniques to accurately interpret, categorize, and quantify the feedback,\nthereby limiting its potential to inform product development and support\nstrategies. To address these challenges, this paper presents the User\neXperience Perception Insights Dataset (UXPID), a collection of 7130\nartificially synthesized and anonymized user feedback branches extracted from a\npublic industrial automation forum. Each JavaScript object notation (JSON)\nrecord contains multi-post comments related to specific hardware and software\nproducts, enriched with metadata and contextual conversation data. Leveraging a\nlarge language model (LLM), each branch is systematically analyzed and\nannotated for UX insights, user expectations, severity and sentiment ratings,\nand topic classifications. The UXPID dataset is designed to facilitate research\nin user requirements, user experience (UX) analysis, and AI-driven feedback\nprocessing, particularly where privacy and licensing restrictions limit access\nto real-world data. UXPID supports the training and evaluation of\ntransformer-based models for tasks such as issue detection, sentiment analysis,\nand requirements extraction in the context of technical forums.\n", "link": "http://arxiv.org/abs/2509.11777v1", "date": "2025-09-15", "relevancy": 2.3997, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4712}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20User%20eXperience%20Perception%20Insights%20Dataset%20%28UXPID%29%3A%20Synthetic%20User%0A%20%20Feedback%20from%20Public%20Industrial%20Forums&body=Title%3A%20User%20eXperience%20Perception%20Insights%20Dataset%20%28UXPID%29%3A%20Synthetic%20User%0A%20%20Feedback%20from%20Public%20Industrial%20Forums%0AAuthor%3A%20Mikhail%20Kulyabin%20and%20Jan%20Joosten%20and%20Choro%20Ulan%20uulu%20and%20Nuno%20Miguel%20Martins%20Pacheco%20and%20Fabian%20Ries%20and%20Filippos%20Petridis%20and%20Jan%20Bosch%20and%20Helena%20Holmstr%C3%B6m%20Olsson%0AAbstract%3A%20%20%20Customer%20feedback%20in%20industrial%20forums%20reflect%20a%20rich%20but%20underexplored%0Asource%20of%20insight%20into%20real-world%20product%20experience.%20These%20publicly%20shared%0Adiscussions%20offer%20an%20organic%20view%20of%20user%20expectations%2C%20frustrations%2C%20and%0Asuccess%20stories%20shaped%20by%20the%20specific%20contexts%20of%20use.%20Yet%2C%20harnessing%20this%0Ainformation%20for%20systematic%20analysis%20remains%20challenging%20due%20to%20the%20unstructured%0Aand%20domain-specific%20nature%20of%20the%20content.%20The%20lack%20of%20structure%20and%0Aspecialized%20vocabulary%20makes%20it%20difficult%20for%20traditional%20data%20analysis%0Atechniques%20to%20accurately%20interpret%2C%20categorize%2C%20and%20quantify%20the%20feedback%2C%0Athereby%20limiting%20its%20potential%20to%20inform%20product%20development%20and%20support%0Astrategies.%20To%20address%20these%20challenges%2C%20this%20paper%20presents%20the%20User%0AeXperience%20Perception%20Insights%20Dataset%20%28UXPID%29%2C%20a%20collection%20of%207130%0Aartificially%20synthesized%20and%20anonymized%20user%20feedback%20branches%20extracted%20from%20a%0Apublic%20industrial%20automation%20forum.%20Each%20JavaScript%20object%20notation%20%28JSON%29%0Arecord%20contains%20multi-post%20comments%20related%20to%20specific%20hardware%20and%20software%0Aproducts%2C%20enriched%20with%20metadata%20and%20contextual%20conversation%20data.%20Leveraging%20a%0Alarge%20language%20model%20%28LLM%29%2C%20each%20branch%20is%20systematically%20analyzed%20and%0Aannotated%20for%20UX%20insights%2C%20user%20expectations%2C%20severity%20and%20sentiment%20ratings%2C%0Aand%20topic%20classifications.%20The%20UXPID%20dataset%20is%20designed%20to%20facilitate%20research%0Ain%20user%20requirements%2C%20user%20experience%20%28UX%29%20analysis%2C%20and%20AI-driven%20feedback%0Aprocessing%2C%20particularly%20where%20privacy%20and%20licensing%20restrictions%20limit%20access%0Ato%20real-world%20data.%20UXPID%20supports%20the%20training%20and%20evaluation%20of%0Atransformer-based%20models%20for%20tasks%20such%20as%20issue%20detection%2C%20sentiment%20analysis%2C%0Aand%20requirements%20extraction%20in%20the%20context%20of%20technical%20forums.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUser%2520eXperience%2520Perception%2520Insights%2520Dataset%2520%2528UXPID%2529%253A%2520Synthetic%2520User%250A%2520%2520Feedback%2520from%2520Public%2520Industrial%2520Forums%26entry.906535625%3DMikhail%2520Kulyabin%2520and%2520Jan%2520Joosten%2520and%2520Choro%2520Ulan%2520uulu%2520and%2520Nuno%2520Miguel%2520Martins%2520Pacheco%2520and%2520Fabian%2520Ries%2520and%2520Filippos%2520Petridis%2520and%2520Jan%2520Bosch%2520and%2520Helena%2520Holmstr%25C3%25B6m%2520Olsson%26entry.1292438233%3D%2520%2520Customer%2520feedback%2520in%2520industrial%2520forums%2520reflect%2520a%2520rich%2520but%2520underexplored%250Asource%2520of%2520insight%2520into%2520real-world%2520product%2520experience.%2520These%2520publicly%2520shared%250Adiscussions%2520offer%2520an%2520organic%2520view%2520of%2520user%2520expectations%252C%2520frustrations%252C%2520and%250Asuccess%2520stories%2520shaped%2520by%2520the%2520specific%2520contexts%2520of%2520use.%2520Yet%252C%2520harnessing%2520this%250Ainformation%2520for%2520systematic%2520analysis%2520remains%2520challenging%2520due%2520to%2520the%2520unstructured%250Aand%2520domain-specific%2520nature%2520of%2520the%2520content.%2520The%2520lack%2520of%2520structure%2520and%250Aspecialized%2520vocabulary%2520makes%2520it%2520difficult%2520for%2520traditional%2520data%2520analysis%250Atechniques%2520to%2520accurately%2520interpret%252C%2520categorize%252C%2520and%2520quantify%2520the%2520feedback%252C%250Athereby%2520limiting%2520its%2520potential%2520to%2520inform%2520product%2520development%2520and%2520support%250Astrategies.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520presents%2520the%2520User%250AeXperience%2520Perception%2520Insights%2520Dataset%2520%2528UXPID%2529%252C%2520a%2520collection%2520of%25207130%250Aartificially%2520synthesized%2520and%2520anonymized%2520user%2520feedback%2520branches%2520extracted%2520from%2520a%250Apublic%2520industrial%2520automation%2520forum.%2520Each%2520JavaScript%2520object%2520notation%2520%2528JSON%2529%250Arecord%2520contains%2520multi-post%2520comments%2520related%2520to%2520specific%2520hardware%2520and%2520software%250Aproducts%252C%2520enriched%2520with%2520metadata%2520and%2520contextual%2520conversation%2520data.%2520Leveraging%2520a%250Alarge%2520language%2520model%2520%2528LLM%2529%252C%2520each%2520branch%2520is%2520systematically%2520analyzed%2520and%250Aannotated%2520for%2520UX%2520insights%252C%2520user%2520expectations%252C%2520severity%2520and%2520sentiment%2520ratings%252C%250Aand%2520topic%2520classifications.%2520The%2520UXPID%2520dataset%2520is%2520designed%2520to%2520facilitate%2520research%250Ain%2520user%2520requirements%252C%2520user%2520experience%2520%2528UX%2529%2520analysis%252C%2520and%2520AI-driven%2520feedback%250Aprocessing%252C%2520particularly%2520where%2520privacy%2520and%2520licensing%2520restrictions%2520limit%2520access%250Ato%2520real-world%2520data.%2520UXPID%2520supports%2520the%2520training%2520and%2520evaluation%2520of%250Atransformer-based%2520models%2520for%2520tasks%2520such%2520as%2520issue%2520detection%252C%2520sentiment%2520analysis%252C%250Aand%2520requirements%2520extraction%2520in%2520the%2520context%2520of%2520technical%2520forums.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=User%20eXperience%20Perception%20Insights%20Dataset%20%28UXPID%29%3A%20Synthetic%20User%0A%20%20Feedback%20from%20Public%20Industrial%20Forums&entry.906535625=Mikhail%20Kulyabin%20and%20Jan%20Joosten%20and%20Choro%20Ulan%20uulu%20and%20Nuno%20Miguel%20Martins%20Pacheco%20and%20Fabian%20Ries%20and%20Filippos%20Petridis%20and%20Jan%20Bosch%20and%20Helena%20Holmstr%C3%B6m%20Olsson&entry.1292438233=%20%20Customer%20feedback%20in%20industrial%20forums%20reflect%20a%20rich%20but%20underexplored%0Asource%20of%20insight%20into%20real-world%20product%20experience.%20These%20publicly%20shared%0Adiscussions%20offer%20an%20organic%20view%20of%20user%20expectations%2C%20frustrations%2C%20and%0Asuccess%20stories%20shaped%20by%20the%20specific%20contexts%20of%20use.%20Yet%2C%20harnessing%20this%0Ainformation%20for%20systematic%20analysis%20remains%20challenging%20due%20to%20the%20unstructured%0Aand%20domain-specific%20nature%20of%20the%20content.%20The%20lack%20of%20structure%20and%0Aspecialized%20vocabulary%20makes%20it%20difficult%20for%20traditional%20data%20analysis%0Atechniques%20to%20accurately%20interpret%2C%20categorize%2C%20and%20quantify%20the%20feedback%2C%0Athereby%20limiting%20its%20potential%20to%20inform%20product%20development%20and%20support%0Astrategies.%20To%20address%20these%20challenges%2C%20this%20paper%20presents%20the%20User%0AeXperience%20Perception%20Insights%20Dataset%20%28UXPID%29%2C%20a%20collection%20of%207130%0Aartificially%20synthesized%20and%20anonymized%20user%20feedback%20branches%20extracted%20from%20a%0Apublic%20industrial%20automation%20forum.%20Each%20JavaScript%20object%20notation%20%28JSON%29%0Arecord%20contains%20multi-post%20comments%20related%20to%20specific%20hardware%20and%20software%0Aproducts%2C%20enriched%20with%20metadata%20and%20contextual%20conversation%20data.%20Leveraging%20a%0Alarge%20language%20model%20%28LLM%29%2C%20each%20branch%20is%20systematically%20analyzed%20and%0Aannotated%20for%20UX%20insights%2C%20user%20expectations%2C%20severity%20and%20sentiment%20ratings%2C%0Aand%20topic%20classifications.%20The%20UXPID%20dataset%20is%20designed%20to%20facilitate%20research%0Ain%20user%20requirements%2C%20user%20experience%20%28UX%29%20analysis%2C%20and%20AI-driven%20feedback%0Aprocessing%2C%20particularly%20where%20privacy%20and%20licensing%20restrictions%20limit%20access%0Ato%20real-world%20data.%20UXPID%20supports%20the%20training%20and%20evaluation%20of%0Atransformer-based%20models%20for%20tasks%20such%20as%20issue%20detection%2C%20sentiment%20analysis%2C%0Aand%20requirements%20extraction%20in%20the%20context%20of%20technical%20forums.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11777v1&entry.124074799=Read"},
{"title": "Early alignment in two-layer networks training is a two-edged sword", "author": "Etienne Boursier and Nicolas Flammarion", "abstract": "  Training neural networks with first order optimisation methods is at the core\nof the empirical success of deep learning. The scale of initialisation is a\ncrucial factor, as small initialisations are generally associated to a feature\nlearning regime, for which gradient descent is implicitly biased towards simple\nsolutions. This work provides a general and quantitative description of the\nearly alignment phase, originally introduced by Maennel et al. (2018). For\nsmall initialisation and one hidden ReLU layer networks, the early stage of the\ntraining dynamics leads to an alignment of the neurons towards key directions.\nThis alignment induces a sparse representation of the network, which is\ndirectly related to the implicit bias of gradient flow at convergence. This\nsparsity inducing alignment however comes at the expense of difficulties in\nminimising the training objective: we also provide a simple data example for\nwhich overparameterised networks fail to converge towards global minima and\nonly converge to a spurious stationary point instead.\n", "link": "http://arxiv.org/abs/2401.10791v3", "date": "2025-09-15", "relevancy": 2.3821, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5125}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.466}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Early%20alignment%20in%20two-layer%20networks%20training%20is%20a%20two-edged%20sword&body=Title%3A%20Early%20alignment%20in%20two-layer%20networks%20training%20is%20a%20two-edged%20sword%0AAuthor%3A%20Etienne%20Boursier%20and%20Nicolas%20Flammarion%0AAbstract%3A%20%20%20Training%20neural%20networks%20with%20first%20order%20optimisation%20methods%20is%20at%20the%20core%0Aof%20the%20empirical%20success%20of%20deep%20learning.%20The%20scale%20of%20initialisation%20is%20a%0Acrucial%20factor%2C%20as%20small%20initialisations%20are%20generally%20associated%20to%20a%20feature%0Alearning%20regime%2C%20for%20which%20gradient%20descent%20is%20implicitly%20biased%20towards%20simple%0Asolutions.%20This%20work%20provides%20a%20general%20and%20quantitative%20description%20of%20the%0Aearly%20alignment%20phase%2C%20originally%20introduced%20by%20Maennel%20et%20al.%20%282018%29.%20For%0Asmall%20initialisation%20and%20one%20hidden%20ReLU%20layer%20networks%2C%20the%20early%20stage%20of%20the%0Atraining%20dynamics%20leads%20to%20an%20alignment%20of%20the%20neurons%20towards%20key%20directions.%0AThis%20alignment%20induces%20a%20sparse%20representation%20of%20the%20network%2C%20which%20is%0Adirectly%20related%20to%20the%20implicit%20bias%20of%20gradient%20flow%20at%20convergence.%20This%0Asparsity%20inducing%20alignment%20however%20comes%20at%20the%20expense%20of%20difficulties%20in%0Aminimising%20the%20training%20objective%3A%20we%20also%20provide%20a%20simple%20data%20example%20for%0Awhich%20overparameterised%20networks%20fail%20to%20converge%20towards%20global%20minima%20and%0Aonly%20converge%20to%20a%20spurious%20stationary%20point%20instead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10791v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarly%2520alignment%2520in%2520two-layer%2520networks%2520training%2520is%2520a%2520two-edged%2520sword%26entry.906535625%3DEtienne%2520Boursier%2520and%2520Nicolas%2520Flammarion%26entry.1292438233%3D%2520%2520Training%2520neural%2520networks%2520with%2520first%2520order%2520optimisation%2520methods%2520is%2520at%2520the%2520core%250Aof%2520the%2520empirical%2520success%2520of%2520deep%2520learning.%2520The%2520scale%2520of%2520initialisation%2520is%2520a%250Acrucial%2520factor%252C%2520as%2520small%2520initialisations%2520are%2520generally%2520associated%2520to%2520a%2520feature%250Alearning%2520regime%252C%2520for%2520which%2520gradient%2520descent%2520is%2520implicitly%2520biased%2520towards%2520simple%250Asolutions.%2520This%2520work%2520provides%2520a%2520general%2520and%2520quantitative%2520description%2520of%2520the%250Aearly%2520alignment%2520phase%252C%2520originally%2520introduced%2520by%2520Maennel%2520et%2520al.%2520%25282018%2529.%2520For%250Asmall%2520initialisation%2520and%2520one%2520hidden%2520ReLU%2520layer%2520networks%252C%2520the%2520early%2520stage%2520of%2520the%250Atraining%2520dynamics%2520leads%2520to%2520an%2520alignment%2520of%2520the%2520neurons%2520towards%2520key%2520directions.%250AThis%2520alignment%2520induces%2520a%2520sparse%2520representation%2520of%2520the%2520network%252C%2520which%2520is%250Adirectly%2520related%2520to%2520the%2520implicit%2520bias%2520of%2520gradient%2520flow%2520at%2520convergence.%2520This%250Asparsity%2520inducing%2520alignment%2520however%2520comes%2520at%2520the%2520expense%2520of%2520difficulties%2520in%250Aminimising%2520the%2520training%2520objective%253A%2520we%2520also%2520provide%2520a%2520simple%2520data%2520example%2520for%250Awhich%2520overparameterised%2520networks%2520fail%2520to%2520converge%2520towards%2520global%2520minima%2520and%250Aonly%2520converge%2520to%2520a%2520spurious%2520stationary%2520point%2520instead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10791v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Early%20alignment%20in%20two-layer%20networks%20training%20is%20a%20two-edged%20sword&entry.906535625=Etienne%20Boursier%20and%20Nicolas%20Flammarion&entry.1292438233=%20%20Training%20neural%20networks%20with%20first%20order%20optimisation%20methods%20is%20at%20the%20core%0Aof%20the%20empirical%20success%20of%20deep%20learning.%20The%20scale%20of%20initialisation%20is%20a%0Acrucial%20factor%2C%20as%20small%20initialisations%20are%20generally%20associated%20to%20a%20feature%0Alearning%20regime%2C%20for%20which%20gradient%20descent%20is%20implicitly%20biased%20towards%20simple%0Asolutions.%20This%20work%20provides%20a%20general%20and%20quantitative%20description%20of%20the%0Aearly%20alignment%20phase%2C%20originally%20introduced%20by%20Maennel%20et%20al.%20%282018%29.%20For%0Asmall%20initialisation%20and%20one%20hidden%20ReLU%20layer%20networks%2C%20the%20early%20stage%20of%20the%0Atraining%20dynamics%20leads%20to%20an%20alignment%20of%20the%20neurons%20towards%20key%20directions.%0AThis%20alignment%20induces%20a%20sparse%20representation%20of%20the%20network%2C%20which%20is%0Adirectly%20related%20to%20the%20implicit%20bias%20of%20gradient%20flow%20at%20convergence.%20This%0Asparsity%20inducing%20alignment%20however%20comes%20at%20the%20expense%20of%20difficulties%20in%0Aminimising%20the%20training%20objective%3A%20we%20also%20provide%20a%20simple%20data%20example%20for%0Awhich%20overparameterised%20networks%20fail%20to%20converge%20towards%20global%20minima%20and%0Aonly%20converge%20to%20a%20spurious%20stationary%20point%20instead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10791v3&entry.124074799=Read"},
{"title": "Exploring Conversational Design Choices in LLMs for Pedagogical\n  Purposes: Socratic and Narrative Approaches for Improving Instructor's\n  Teaching Practice", "author": "Si Chen and Isabel R. Molnar and Peiyu Li and Adam Acunin and Ting Hua and Alex Ambrose and Nitesh V. Chawla and Ronald Metoyer", "abstract": "  Large language models (LLMs) typically generate direct answers, yet they are\nincreasingly used as learning tools. Studying instructors' usage is critical,\ngiven their role in teaching and guiding AI adoption in education. We designed\nand evaluated TeaPT, an LLM for pedagogical purposes that supports instructors'\nprofessional development through two conversational approaches: a Socratic\napproach that uses guided questioning to foster reflection, and a Narrative\napproach that offers elaborated suggestions to extend externalized cognition.\nIn a mixed-method study with 41 higher-education instructors, the Socratic\nversion elicited greater engagement, while the Narrative version was preferred\nfor actionable guidance. Subgroup analyses further revealed that\nless-experienced, AI-optimistic instructors favored the Socratic version,\nwhereas more-experienced, AI-cautious instructors preferred the Narrative\nversion. We contribute design implications for LLMs for pedagogical purposes,\nshowing how adaptive conversational approaches can support instructors with\nvaried profiles while highlighting how AI attitudes and experience shape\ninteraction and learning.\n", "link": "http://arxiv.org/abs/2509.12107v1", "date": "2025-09-15", "relevancy": 2.3659, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4781}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Conversational%20Design%20Choices%20in%20LLMs%20for%20Pedagogical%0A%20%20Purposes%3A%20Socratic%20and%20Narrative%20Approaches%20for%20Improving%20Instructor%27s%0A%20%20Teaching%20Practice&body=Title%3A%20Exploring%20Conversational%20Design%20Choices%20in%20LLMs%20for%20Pedagogical%0A%20%20Purposes%3A%20Socratic%20and%20Narrative%20Approaches%20for%20Improving%20Instructor%27s%0A%20%20Teaching%20Practice%0AAuthor%3A%20Si%20Chen%20and%20Isabel%20R.%20Molnar%20and%20Peiyu%20Li%20and%20Adam%20Acunin%20and%20Ting%20Hua%20and%20Alex%20Ambrose%20and%20Nitesh%20V.%20Chawla%20and%20Ronald%20Metoyer%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20typically%20generate%20direct%20answers%2C%20yet%20they%20are%0Aincreasingly%20used%20as%20learning%20tools.%20Studying%20instructors%27%20usage%20is%20critical%2C%0Agiven%20their%20role%20in%20teaching%20and%20guiding%20AI%20adoption%20in%20education.%20We%20designed%0Aand%20evaluated%20TeaPT%2C%20an%20LLM%20for%20pedagogical%20purposes%20that%20supports%20instructors%27%0Aprofessional%20development%20through%20two%20conversational%20approaches%3A%20a%20Socratic%0Aapproach%20that%20uses%20guided%20questioning%20to%20foster%20reflection%2C%20and%20a%20Narrative%0Aapproach%20that%20offers%20elaborated%20suggestions%20to%20extend%20externalized%20cognition.%0AIn%20a%20mixed-method%20study%20with%2041%20higher-education%20instructors%2C%20the%20Socratic%0Aversion%20elicited%20greater%20engagement%2C%20while%20the%20Narrative%20version%20was%20preferred%0Afor%20actionable%20guidance.%20Subgroup%20analyses%20further%20revealed%20that%0Aless-experienced%2C%20AI-optimistic%20instructors%20favored%20the%20Socratic%20version%2C%0Awhereas%20more-experienced%2C%20AI-cautious%20instructors%20preferred%20the%20Narrative%0Aversion.%20We%20contribute%20design%20implications%20for%20LLMs%20for%20pedagogical%20purposes%2C%0Ashowing%20how%20adaptive%20conversational%20approaches%20can%20support%20instructors%20with%0Avaried%20profiles%20while%20highlighting%20how%20AI%20attitudes%20and%20experience%20shape%0Ainteraction%20and%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Conversational%2520Design%2520Choices%2520in%2520LLMs%2520for%2520Pedagogical%250A%2520%2520Purposes%253A%2520Socratic%2520and%2520Narrative%2520Approaches%2520for%2520Improving%2520Instructor%2527s%250A%2520%2520Teaching%2520Practice%26entry.906535625%3DSi%2520Chen%2520and%2520Isabel%2520R.%2520Molnar%2520and%2520Peiyu%2520Li%2520and%2520Adam%2520Acunin%2520and%2520Ting%2520Hua%2520and%2520Alex%2520Ambrose%2520and%2520Nitesh%2520V.%2520Chawla%2520and%2520Ronald%2520Metoyer%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520typically%2520generate%2520direct%2520answers%252C%2520yet%2520they%2520are%250Aincreasingly%2520used%2520as%2520learning%2520tools.%2520Studying%2520instructors%2527%2520usage%2520is%2520critical%252C%250Agiven%2520their%2520role%2520in%2520teaching%2520and%2520guiding%2520AI%2520adoption%2520in%2520education.%2520We%2520designed%250Aand%2520evaluated%2520TeaPT%252C%2520an%2520LLM%2520for%2520pedagogical%2520purposes%2520that%2520supports%2520instructors%2527%250Aprofessional%2520development%2520through%2520two%2520conversational%2520approaches%253A%2520a%2520Socratic%250Aapproach%2520that%2520uses%2520guided%2520questioning%2520to%2520foster%2520reflection%252C%2520and%2520a%2520Narrative%250Aapproach%2520that%2520offers%2520elaborated%2520suggestions%2520to%2520extend%2520externalized%2520cognition.%250AIn%2520a%2520mixed-method%2520study%2520with%252041%2520higher-education%2520instructors%252C%2520the%2520Socratic%250Aversion%2520elicited%2520greater%2520engagement%252C%2520while%2520the%2520Narrative%2520version%2520was%2520preferred%250Afor%2520actionable%2520guidance.%2520Subgroup%2520analyses%2520further%2520revealed%2520that%250Aless-experienced%252C%2520AI-optimistic%2520instructors%2520favored%2520the%2520Socratic%2520version%252C%250Awhereas%2520more-experienced%252C%2520AI-cautious%2520instructors%2520preferred%2520the%2520Narrative%250Aversion.%2520We%2520contribute%2520design%2520implications%2520for%2520LLMs%2520for%2520pedagogical%2520purposes%252C%250Ashowing%2520how%2520adaptive%2520conversational%2520approaches%2520can%2520support%2520instructors%2520with%250Avaried%2520profiles%2520while%2520highlighting%2520how%2520AI%2520attitudes%2520and%2520experience%2520shape%250Ainteraction%2520and%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Conversational%20Design%20Choices%20in%20LLMs%20for%20Pedagogical%0A%20%20Purposes%3A%20Socratic%20and%20Narrative%20Approaches%20for%20Improving%20Instructor%27s%0A%20%20Teaching%20Practice&entry.906535625=Si%20Chen%20and%20Isabel%20R.%20Molnar%20and%20Peiyu%20Li%20and%20Adam%20Acunin%20and%20Ting%20Hua%20and%20Alex%20Ambrose%20and%20Nitesh%20V.%20Chawla%20and%20Ronald%20Metoyer&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20typically%20generate%20direct%20answers%2C%20yet%20they%20are%0Aincreasingly%20used%20as%20learning%20tools.%20Studying%20instructors%27%20usage%20is%20critical%2C%0Agiven%20their%20role%20in%20teaching%20and%20guiding%20AI%20adoption%20in%20education.%20We%20designed%0Aand%20evaluated%20TeaPT%2C%20an%20LLM%20for%20pedagogical%20purposes%20that%20supports%20instructors%27%0Aprofessional%20development%20through%20two%20conversational%20approaches%3A%20a%20Socratic%0Aapproach%20that%20uses%20guided%20questioning%20to%20foster%20reflection%2C%20and%20a%20Narrative%0Aapproach%20that%20offers%20elaborated%20suggestions%20to%20extend%20externalized%20cognition.%0AIn%20a%20mixed-method%20study%20with%2041%20higher-education%20instructors%2C%20the%20Socratic%0Aversion%20elicited%20greater%20engagement%2C%20while%20the%20Narrative%20version%20was%20preferred%0Afor%20actionable%20guidance.%20Subgroup%20analyses%20further%20revealed%20that%0Aless-experienced%2C%20AI-optimistic%20instructors%20favored%20the%20Socratic%20version%2C%0Awhereas%20more-experienced%2C%20AI-cautious%20instructors%20preferred%20the%20Narrative%0Aversion.%20We%20contribute%20design%20implications%20for%20LLMs%20for%20pedagogical%20purposes%2C%0Ashowing%20how%20adaptive%20conversational%20approaches%20can%20support%20instructors%20with%0Avaried%20profiles%20while%20highlighting%20how%20AI%20attitudes%20and%20experience%20shape%0Ainteraction%20and%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12107v1&entry.124074799=Read"},
{"title": "HD-OOD3D: Supervised and Unsupervised Out-of-Distribution object\n  detection in LiDAR data", "author": "Louis Soum-Fontez and Jean-Emmanuel Deschaud and Fran\u00e7ois Goulette", "abstract": "  Autonomous systems rely on accurate 3D object detection from LiDAR data, yet\nmost detectors are limited to a predefined set of known classes, making them\nvulnerable to unexpected out-of-distribution (OOD) objects. In this work, we\npresent HD-OOD3D, a novel two-stage method for detecting unknown objects. We\ndemonstrate the superiority of two-stage approaches over single-stage methods,\nachieving more robust detection of unknown objects while addressing key\nchallenges in the evaluation protocol. Furthermore, we conduct an in-depth\nanalysis of the standard evaluation protocol for OOD detection, revealing the\ncritical impact of hyperparameter choices. To address the challenge of scaling\nthe learning of unknown objects, we explore unsupervised training strategies to\ngenerate pseudo-labels for unknowns. Among the different approaches evaluated,\nour experiments show that top-5 auto-labelling offers more promising\nperformance compared to simple resizing techniques.\n", "link": "http://arxiv.org/abs/2410.23767v3", "date": "2025-09-15", "relevancy": 2.3413, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6069}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5815}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HD-OOD3D%3A%20Supervised%20and%20Unsupervised%20Out-of-Distribution%20object%0A%20%20detection%20in%20LiDAR%20data&body=Title%3A%20HD-OOD3D%3A%20Supervised%20and%20Unsupervised%20Out-of-Distribution%20object%0A%20%20detection%20in%20LiDAR%20data%0AAuthor%3A%20Louis%20Soum-Fontez%20and%20Jean-Emmanuel%20Deschaud%20and%20Fran%C3%A7ois%20Goulette%0AAbstract%3A%20%20%20Autonomous%20systems%20rely%20on%20accurate%203D%20object%20detection%20from%20LiDAR%20data%2C%20yet%0Amost%20detectors%20are%20limited%20to%20a%20predefined%20set%20of%20known%20classes%2C%20making%20them%0Avulnerable%20to%20unexpected%20out-of-distribution%20%28OOD%29%20objects.%20In%20this%20work%2C%20we%0Apresent%20HD-OOD3D%2C%20a%20novel%20two-stage%20method%20for%20detecting%20unknown%20objects.%20We%0Ademonstrate%20the%20superiority%20of%20two-stage%20approaches%20over%20single-stage%20methods%2C%0Aachieving%20more%20robust%20detection%20of%20unknown%20objects%20while%20addressing%20key%0Achallenges%20in%20the%20evaluation%20protocol.%20Furthermore%2C%20we%20conduct%20an%20in-depth%0Aanalysis%20of%20the%20standard%20evaluation%20protocol%20for%20OOD%20detection%2C%20revealing%20the%0Acritical%20impact%20of%20hyperparameter%20choices.%20To%20address%20the%20challenge%20of%20scaling%0Athe%20learning%20of%20unknown%20objects%2C%20we%20explore%20unsupervised%20training%20strategies%20to%0Agenerate%20pseudo-labels%20for%20unknowns.%20Among%20the%20different%20approaches%20evaluated%2C%0Aour%20experiments%20show%20that%20top-5%20auto-labelling%20offers%20more%20promising%0Aperformance%20compared%20to%20simple%20resizing%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23767v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHD-OOD3D%253A%2520Supervised%2520and%2520Unsupervised%2520Out-of-Distribution%2520object%250A%2520%2520detection%2520in%2520LiDAR%2520data%26entry.906535625%3DLouis%2520Soum-Fontez%2520and%2520Jean-Emmanuel%2520Deschaud%2520and%2520Fran%25C3%25A7ois%2520Goulette%26entry.1292438233%3D%2520%2520Autonomous%2520systems%2520rely%2520on%2520accurate%25203D%2520object%2520detection%2520from%2520LiDAR%2520data%252C%2520yet%250Amost%2520detectors%2520are%2520limited%2520to%2520a%2520predefined%2520set%2520of%2520known%2520classes%252C%2520making%2520them%250Avulnerable%2520to%2520unexpected%2520out-of-distribution%2520%2528OOD%2529%2520objects.%2520In%2520this%2520work%252C%2520we%250Apresent%2520HD-OOD3D%252C%2520a%2520novel%2520two-stage%2520method%2520for%2520detecting%2520unknown%2520objects.%2520We%250Ademonstrate%2520the%2520superiority%2520of%2520two-stage%2520approaches%2520over%2520single-stage%2520methods%252C%250Aachieving%2520more%2520robust%2520detection%2520of%2520unknown%2520objects%2520while%2520addressing%2520key%250Achallenges%2520in%2520the%2520evaluation%2520protocol.%2520Furthermore%252C%2520we%2520conduct%2520an%2520in-depth%250Aanalysis%2520of%2520the%2520standard%2520evaluation%2520protocol%2520for%2520OOD%2520detection%252C%2520revealing%2520the%250Acritical%2520impact%2520of%2520hyperparameter%2520choices.%2520To%2520address%2520the%2520challenge%2520of%2520scaling%250Athe%2520learning%2520of%2520unknown%2520objects%252C%2520we%2520explore%2520unsupervised%2520training%2520strategies%2520to%250Agenerate%2520pseudo-labels%2520for%2520unknowns.%2520Among%2520the%2520different%2520approaches%2520evaluated%252C%250Aour%2520experiments%2520show%2520that%2520top-5%2520auto-labelling%2520offers%2520more%2520promising%250Aperformance%2520compared%2520to%2520simple%2520resizing%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23767v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HD-OOD3D%3A%20Supervised%20and%20Unsupervised%20Out-of-Distribution%20object%0A%20%20detection%20in%20LiDAR%20data&entry.906535625=Louis%20Soum-Fontez%20and%20Jean-Emmanuel%20Deschaud%20and%20Fran%C3%A7ois%20Goulette&entry.1292438233=%20%20Autonomous%20systems%20rely%20on%20accurate%203D%20object%20detection%20from%20LiDAR%20data%2C%20yet%0Amost%20detectors%20are%20limited%20to%20a%20predefined%20set%20of%20known%20classes%2C%20making%20them%0Avulnerable%20to%20unexpected%20out-of-distribution%20%28OOD%29%20objects.%20In%20this%20work%2C%20we%0Apresent%20HD-OOD3D%2C%20a%20novel%20two-stage%20method%20for%20detecting%20unknown%20objects.%20We%0Ademonstrate%20the%20superiority%20of%20two-stage%20approaches%20over%20single-stage%20methods%2C%0Aachieving%20more%20robust%20detection%20of%20unknown%20objects%20while%20addressing%20key%0Achallenges%20in%20the%20evaluation%20protocol.%20Furthermore%2C%20we%20conduct%20an%20in-depth%0Aanalysis%20of%20the%20standard%20evaluation%20protocol%20for%20OOD%20detection%2C%20revealing%20the%0Acritical%20impact%20of%20hyperparameter%20choices.%20To%20address%20the%20challenge%20of%20scaling%0Athe%20learning%20of%20unknown%20objects%2C%20we%20explore%20unsupervised%20training%20strategies%20to%0Agenerate%20pseudo-labels%20for%20unknowns.%20Among%20the%20different%20approaches%20evaluated%2C%0Aour%20experiments%20show%20that%20top-5%20auto-labelling%20offers%20more%20promising%0Aperformance%20compared%20to%20simple%20resizing%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23767v3&entry.124074799=Read"},
{"title": "Draw a Portrait of Your Graph Data: An Instance-Level Profiling\n  Framework for Graph-Structured Data", "author": "Tianqi Zhao and Russa Biswas and Megha Khosla", "abstract": "  Graph machine learning models often achieve similar overall performance yet\nbehave differently at the node level, failing on different subsets of nodes\nwith varying reliability. Standard evaluation metrics such as accuracy obscure\nthese fine grained differences, making it difficult to diagnose when and where\nmodels fail. We introduce NodePro, a node profiling framework that enables\nfine-grained diagnosis of model behavior by assigning interpretable profile\nscores to individual nodes. These scores combine data-centric signals, such as\nfeature dissimilarity, label uncertainty, and structural ambiguity, with\nmodel-centric measures of prediction confidence and consistency during\ntraining. By aligning model behavior with these profiles, NodePro reveals\nsystematic differences between models, even when aggregate metrics are\nindistinguishable. We show that node profiles generalize to unseen nodes,\nsupporting prediction reliability without ground-truth labels. Finally, we\ndemonstrate the utility of NodePro in identifying semantically inconsistent or\ncorrupted nodes in a structured knowledge graph, illustrating its effectiveness\nin real-world settings.\n", "link": "http://arxiv.org/abs/2509.12094v1", "date": "2025-09-15", "relevancy": 2.3353, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4691}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4691}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Draw%20a%20Portrait%20of%20Your%20Graph%20Data%3A%20An%20Instance-Level%20Profiling%0A%20%20Framework%20for%20Graph-Structured%20Data&body=Title%3A%20Draw%20a%20Portrait%20of%20Your%20Graph%20Data%3A%20An%20Instance-Level%20Profiling%0A%20%20Framework%20for%20Graph-Structured%20Data%0AAuthor%3A%20Tianqi%20Zhao%20and%20Russa%20Biswas%20and%20Megha%20Khosla%0AAbstract%3A%20%20%20Graph%20machine%20learning%20models%20often%20achieve%20similar%20overall%20performance%20yet%0Abehave%20differently%20at%20the%20node%20level%2C%20failing%20on%20different%20subsets%20of%20nodes%0Awith%20varying%20reliability.%20Standard%20evaluation%20metrics%20such%20as%20accuracy%20obscure%0Athese%20fine%20grained%20differences%2C%20making%20it%20difficult%20to%20diagnose%20when%20and%20where%0Amodels%20fail.%20We%20introduce%20NodePro%2C%20a%20node%20profiling%20framework%20that%20enables%0Afine-grained%20diagnosis%20of%20model%20behavior%20by%20assigning%20interpretable%20profile%0Ascores%20to%20individual%20nodes.%20These%20scores%20combine%20data-centric%20signals%2C%20such%20as%0Afeature%20dissimilarity%2C%20label%20uncertainty%2C%20and%20structural%20ambiguity%2C%20with%0Amodel-centric%20measures%20of%20prediction%20confidence%20and%20consistency%20during%0Atraining.%20By%20aligning%20model%20behavior%20with%20these%20profiles%2C%20NodePro%20reveals%0Asystematic%20differences%20between%20models%2C%20even%20when%20aggregate%20metrics%20are%0Aindistinguishable.%20We%20show%20that%20node%20profiles%20generalize%20to%20unseen%20nodes%2C%0Asupporting%20prediction%20reliability%20without%20ground-truth%20labels.%20Finally%2C%20we%0Ademonstrate%20the%20utility%20of%20NodePro%20in%20identifying%20semantically%20inconsistent%20or%0Acorrupted%20nodes%20in%20a%20structured%20knowledge%20graph%2C%20illustrating%20its%20effectiveness%0Ain%20real-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDraw%2520a%2520Portrait%2520of%2520Your%2520Graph%2520Data%253A%2520An%2520Instance-Level%2520Profiling%250A%2520%2520Framework%2520for%2520Graph-Structured%2520Data%26entry.906535625%3DTianqi%2520Zhao%2520and%2520Russa%2520Biswas%2520and%2520Megha%2520Khosla%26entry.1292438233%3D%2520%2520Graph%2520machine%2520learning%2520models%2520often%2520achieve%2520similar%2520overall%2520performance%2520yet%250Abehave%2520differently%2520at%2520the%2520node%2520level%252C%2520failing%2520on%2520different%2520subsets%2520of%2520nodes%250Awith%2520varying%2520reliability.%2520Standard%2520evaluation%2520metrics%2520such%2520as%2520accuracy%2520obscure%250Athese%2520fine%2520grained%2520differences%252C%2520making%2520it%2520difficult%2520to%2520diagnose%2520when%2520and%2520where%250Amodels%2520fail.%2520We%2520introduce%2520NodePro%252C%2520a%2520node%2520profiling%2520framework%2520that%2520enables%250Afine-grained%2520diagnosis%2520of%2520model%2520behavior%2520by%2520assigning%2520interpretable%2520profile%250Ascores%2520to%2520individual%2520nodes.%2520These%2520scores%2520combine%2520data-centric%2520signals%252C%2520such%2520as%250Afeature%2520dissimilarity%252C%2520label%2520uncertainty%252C%2520and%2520structural%2520ambiguity%252C%2520with%250Amodel-centric%2520measures%2520of%2520prediction%2520confidence%2520and%2520consistency%2520during%250Atraining.%2520By%2520aligning%2520model%2520behavior%2520with%2520these%2520profiles%252C%2520NodePro%2520reveals%250Asystematic%2520differences%2520between%2520models%252C%2520even%2520when%2520aggregate%2520metrics%2520are%250Aindistinguishable.%2520We%2520show%2520that%2520node%2520profiles%2520generalize%2520to%2520unseen%2520nodes%252C%250Asupporting%2520prediction%2520reliability%2520without%2520ground-truth%2520labels.%2520Finally%252C%2520we%250Ademonstrate%2520the%2520utility%2520of%2520NodePro%2520in%2520identifying%2520semantically%2520inconsistent%2520or%250Acorrupted%2520nodes%2520in%2520a%2520structured%2520knowledge%2520graph%252C%2520illustrating%2520its%2520effectiveness%250Ain%2520real-world%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Draw%20a%20Portrait%20of%20Your%20Graph%20Data%3A%20An%20Instance-Level%20Profiling%0A%20%20Framework%20for%20Graph-Structured%20Data&entry.906535625=Tianqi%20Zhao%20and%20Russa%20Biswas%20and%20Megha%20Khosla&entry.1292438233=%20%20Graph%20machine%20learning%20models%20often%20achieve%20similar%20overall%20performance%20yet%0Abehave%20differently%20at%20the%20node%20level%2C%20failing%20on%20different%20subsets%20of%20nodes%0Awith%20varying%20reliability.%20Standard%20evaluation%20metrics%20such%20as%20accuracy%20obscure%0Athese%20fine%20grained%20differences%2C%20making%20it%20difficult%20to%20diagnose%20when%20and%20where%0Amodels%20fail.%20We%20introduce%20NodePro%2C%20a%20node%20profiling%20framework%20that%20enables%0Afine-grained%20diagnosis%20of%20model%20behavior%20by%20assigning%20interpretable%20profile%0Ascores%20to%20individual%20nodes.%20These%20scores%20combine%20data-centric%20signals%2C%20such%20as%0Afeature%20dissimilarity%2C%20label%20uncertainty%2C%20and%20structural%20ambiguity%2C%20with%0Amodel-centric%20measures%20of%20prediction%20confidence%20and%20consistency%20during%0Atraining.%20By%20aligning%20model%20behavior%20with%20these%20profiles%2C%20NodePro%20reveals%0Asystematic%20differences%20between%20models%2C%20even%20when%20aggregate%20metrics%20are%0Aindistinguishable.%20We%20show%20that%20node%20profiles%20generalize%20to%20unseen%20nodes%2C%0Asupporting%20prediction%20reliability%20without%20ground-truth%20labels.%20Finally%2C%20we%0Ademonstrate%20the%20utility%20of%20NodePro%20in%20identifying%20semantically%20inconsistent%20or%0Acorrupted%20nodes%20in%20a%20structured%20knowledge%20graph%2C%20illustrating%20its%20effectiveness%0Ain%20real-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12094v1&entry.124074799=Read"},
{"title": "UniPilot: Enabling GPS-Denied Autonomy Across Embodiments", "author": "Mihir Kulkarni and Mihir Dharmadhikari and Nikhil Khedekar and Morten Nissov and Mohit Singh and Philipp Weiss and Kostas Alexis", "abstract": "  This paper presents UniPilot, a compact hardware-software autonomy payload\nthat can be integrated across diverse robot embodiments to enable autonomous\noperation in GPS-denied environments. The system integrates a multi-modal\nsensing suite including LiDAR, radar, vision, and inertial sensing for robust\noperation in conditions where uni-modal approaches may fail. UniPilot runs a\ncomplete autonomy software comprising multi-modal perception, exploration and\ninspection path planning, and learning-based navigation policies. The payload\nprovides robust localization, mapping, planning, and safety and control\ncapabilities in a single unit that can be deployed across a wide range of\nplatforms. A large number of experiments are conducted across diverse\nenvironments and on a variety of robot platforms to validate the mapping,\nplanning, and safe navigation capabilities enabled by the payload.\n", "link": "http://arxiv.org/abs/2509.11793v1", "date": "2025-09-15", "relevancy": 2.3123, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6026}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5689}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniPilot%3A%20Enabling%20GPS-Denied%20Autonomy%20Across%20Embodiments&body=Title%3A%20UniPilot%3A%20Enabling%20GPS-Denied%20Autonomy%20Across%20Embodiments%0AAuthor%3A%20Mihir%20Kulkarni%20and%20Mihir%20Dharmadhikari%20and%20Nikhil%20Khedekar%20and%20Morten%20Nissov%20and%20Mohit%20Singh%20and%20Philipp%20Weiss%20and%20Kostas%20Alexis%0AAbstract%3A%20%20%20This%20paper%20presents%20UniPilot%2C%20a%20compact%20hardware-software%20autonomy%20payload%0Athat%20can%20be%20integrated%20across%20diverse%20robot%20embodiments%20to%20enable%20autonomous%0Aoperation%20in%20GPS-denied%20environments.%20The%20system%20integrates%20a%20multi-modal%0Asensing%20suite%20including%20LiDAR%2C%20radar%2C%20vision%2C%20and%20inertial%20sensing%20for%20robust%0Aoperation%20in%20conditions%20where%20uni-modal%20approaches%20may%20fail.%20UniPilot%20runs%20a%0Acomplete%20autonomy%20software%20comprising%20multi-modal%20perception%2C%20exploration%20and%0Ainspection%20path%20planning%2C%20and%20learning-based%20navigation%20policies.%20The%20payload%0Aprovides%20robust%20localization%2C%20mapping%2C%20planning%2C%20and%20safety%20and%20control%0Acapabilities%20in%20a%20single%20unit%20that%20can%20be%20deployed%20across%20a%20wide%20range%20of%0Aplatforms.%20A%20large%20number%20of%20experiments%20are%20conducted%20across%20diverse%0Aenvironments%20and%20on%20a%20variety%20of%20robot%20platforms%20to%20validate%20the%20mapping%2C%0Aplanning%2C%20and%20safe%20navigation%20capabilities%20enabled%20by%20the%20payload.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniPilot%253A%2520Enabling%2520GPS-Denied%2520Autonomy%2520Across%2520Embodiments%26entry.906535625%3DMihir%2520Kulkarni%2520and%2520Mihir%2520Dharmadhikari%2520and%2520Nikhil%2520Khedekar%2520and%2520Morten%2520Nissov%2520and%2520Mohit%2520Singh%2520and%2520Philipp%2520Weiss%2520and%2520Kostas%2520Alexis%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520UniPilot%252C%2520a%2520compact%2520hardware-software%2520autonomy%2520payload%250Athat%2520can%2520be%2520integrated%2520across%2520diverse%2520robot%2520embodiments%2520to%2520enable%2520autonomous%250Aoperation%2520in%2520GPS-denied%2520environments.%2520The%2520system%2520integrates%2520a%2520multi-modal%250Asensing%2520suite%2520including%2520LiDAR%252C%2520radar%252C%2520vision%252C%2520and%2520inertial%2520sensing%2520for%2520robust%250Aoperation%2520in%2520conditions%2520where%2520uni-modal%2520approaches%2520may%2520fail.%2520UniPilot%2520runs%2520a%250Acomplete%2520autonomy%2520software%2520comprising%2520multi-modal%2520perception%252C%2520exploration%2520and%250Ainspection%2520path%2520planning%252C%2520and%2520learning-based%2520navigation%2520policies.%2520The%2520payload%250Aprovides%2520robust%2520localization%252C%2520mapping%252C%2520planning%252C%2520and%2520safety%2520and%2520control%250Acapabilities%2520in%2520a%2520single%2520unit%2520that%2520can%2520be%2520deployed%2520across%2520a%2520wide%2520range%2520of%250Aplatforms.%2520A%2520large%2520number%2520of%2520experiments%2520are%2520conducted%2520across%2520diverse%250Aenvironments%2520and%2520on%2520a%2520variety%2520of%2520robot%2520platforms%2520to%2520validate%2520the%2520mapping%252C%250Aplanning%252C%2520and%2520safe%2520navigation%2520capabilities%2520enabled%2520by%2520the%2520payload.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniPilot%3A%20Enabling%20GPS-Denied%20Autonomy%20Across%20Embodiments&entry.906535625=Mihir%20Kulkarni%20and%20Mihir%20Dharmadhikari%20and%20Nikhil%20Khedekar%20and%20Morten%20Nissov%20and%20Mohit%20Singh%20and%20Philipp%20Weiss%20and%20Kostas%20Alexis&entry.1292438233=%20%20This%20paper%20presents%20UniPilot%2C%20a%20compact%20hardware-software%20autonomy%20payload%0Athat%20can%20be%20integrated%20across%20diverse%20robot%20embodiments%20to%20enable%20autonomous%0Aoperation%20in%20GPS-denied%20environments.%20The%20system%20integrates%20a%20multi-modal%0Asensing%20suite%20including%20LiDAR%2C%20radar%2C%20vision%2C%20and%20inertial%20sensing%20for%20robust%0Aoperation%20in%20conditions%20where%20uni-modal%20approaches%20may%20fail.%20UniPilot%20runs%20a%0Acomplete%20autonomy%20software%20comprising%20multi-modal%20perception%2C%20exploration%20and%0Ainspection%20path%20planning%2C%20and%20learning-based%20navigation%20policies.%20The%20payload%0Aprovides%20robust%20localization%2C%20mapping%2C%20planning%2C%20and%20safety%20and%20control%0Acapabilities%20in%20a%20single%20unit%20that%20can%20be%20deployed%20across%20a%20wide%20range%20of%0Aplatforms.%20A%20large%20number%20of%20experiments%20are%20conducted%20across%20diverse%0Aenvironments%20and%20on%20a%20variety%20of%20robot%20platforms%20to%20validate%20the%20mapping%2C%0Aplanning%2C%20and%20safe%20navigation%20capabilities%20enabled%20by%20the%20payload.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11793v1&entry.124074799=Read"},
{"title": "CLAIRE: A Dual Encoder Network with RIFT Loss and Phi-3 Small Language\n  Model Based Interpretability for Cross-Modality Synthetic Aperture Radar and\n  Optical Land Cover Segmentation", "author": "Debopom Sutradhar and Arefin Ittesafun Abian and Mohaimenul Azam Khan Raiaan and Reem E. Mohamed and Sheikh Izzal Azid and Sami Azam", "abstract": "  Accurate land cover classification from satellite imagery is crucial in\nenvironmental monitoring and sustainable resource management. However, it\nremains challenging due to the complexity of natural landscapes, the visual\nsimilarity between classes, and the significant class imbalance in the\navailable datasets. To address these issues, we propose a dual encoder\narchitecture that independently extracts modality-specific features from\noptical and Synthetic Aperture Radar (SAR) imagery, which are then fused using\na cross-modality attention-fusion module named Cross-modality Land cover\nsegmentation with Attention and Imbalance-aware Reasoning-Enhanced Explanations\n(CLAIRE). This fusion mechanism highlights complementary spatial and textural\nfeatures, enabling the network to better capture detailed and diverse land\ncover patterns. We incorporate a hybrid loss function that utilizes Weighted\nFocal Loss and Tversky Loss named RIFT (Rare-Instance Focal-Tversky) to address\nclass imbalance and improve segmentation performance across underrepresented\ncategories. Our model achieves competitive performance across multiple\nbenchmarks: a mean Intersection over Union (mIoU) of 56.02% and Overall\nAccuracy (OA) of 84.56% on the WHU-OPT-SAR dataset; strong generalization with\na mIoU of 59.89% and OA of 73.92% on the OpenEarthMap-SAR dataset; and\nremarkable robustness under cloud-obstructed conditions, achieving an mIoU of\n86.86% and OA of 94.58% on the PIE-RGB-SAR dataset. Additionally, we introduce\na metric-driven reasoning module generated by a Small Language Model (Phi-3),\nwhich generates expert-level, sample-specific justifications for model\npredictions, thereby enhancing transparency and interpretability.\n", "link": "http://arxiv.org/abs/2509.11952v1", "date": "2025-09-15", "relevancy": 2.3063, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5831}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.575}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLAIRE%3A%20A%20Dual%20Encoder%20Network%20with%20RIFT%20Loss%20and%20Phi-3%20Small%20Language%0A%20%20Model%20Based%20Interpretability%20for%20Cross-Modality%20Synthetic%20Aperture%20Radar%20and%0A%20%20Optical%20Land%20Cover%20Segmentation&body=Title%3A%20CLAIRE%3A%20A%20Dual%20Encoder%20Network%20with%20RIFT%20Loss%20and%20Phi-3%20Small%20Language%0A%20%20Model%20Based%20Interpretability%20for%20Cross-Modality%20Synthetic%20Aperture%20Radar%20and%0A%20%20Optical%20Land%20Cover%20Segmentation%0AAuthor%3A%20Debopom%20Sutradhar%20and%20Arefin%20Ittesafun%20Abian%20and%20Mohaimenul%20Azam%20Khan%20Raiaan%20and%20Reem%20E.%20Mohamed%20and%20Sheikh%20Izzal%20Azid%20and%20Sami%20Azam%0AAbstract%3A%20%20%20Accurate%20land%20cover%20classification%20from%20satellite%20imagery%20is%20crucial%20in%0Aenvironmental%20monitoring%20and%20sustainable%20resource%20management.%20However%2C%20it%0Aremains%20challenging%20due%20to%20the%20complexity%20of%20natural%20landscapes%2C%20the%20visual%0Asimilarity%20between%20classes%2C%20and%20the%20significant%20class%20imbalance%20in%20the%0Aavailable%20datasets.%20To%20address%20these%20issues%2C%20we%20propose%20a%20dual%20encoder%0Aarchitecture%20that%20independently%20extracts%20modality-specific%20features%20from%0Aoptical%20and%20Synthetic%20Aperture%20Radar%20%28SAR%29%20imagery%2C%20which%20are%20then%20fused%20using%0Aa%20cross-modality%20attention-fusion%20module%20named%20Cross-modality%20Land%20cover%0Asegmentation%20with%20Attention%20and%20Imbalance-aware%20Reasoning-Enhanced%20Explanations%0A%28CLAIRE%29.%20This%20fusion%20mechanism%20highlights%20complementary%20spatial%20and%20textural%0Afeatures%2C%20enabling%20the%20network%20to%20better%20capture%20detailed%20and%20diverse%20land%0Acover%20patterns.%20We%20incorporate%20a%20hybrid%20loss%20function%20that%20utilizes%20Weighted%0AFocal%20Loss%20and%20Tversky%20Loss%20named%20RIFT%20%28Rare-Instance%20Focal-Tversky%29%20to%20address%0Aclass%20imbalance%20and%20improve%20segmentation%20performance%20across%20underrepresented%0Acategories.%20Our%20model%20achieves%20competitive%20performance%20across%20multiple%0Abenchmarks%3A%20a%20mean%20Intersection%20over%20Union%20%28mIoU%29%20of%2056.02%25%20and%20Overall%0AAccuracy%20%28OA%29%20of%2084.56%25%20on%20the%20WHU-OPT-SAR%20dataset%3B%20strong%20generalization%20with%0Aa%20mIoU%20of%2059.89%25%20and%20OA%20of%2073.92%25%20on%20the%20OpenEarthMap-SAR%20dataset%3B%20and%0Aremarkable%20robustness%20under%20cloud-obstructed%20conditions%2C%20achieving%20an%20mIoU%20of%0A86.86%25%20and%20OA%20of%2094.58%25%20on%20the%20PIE-RGB-SAR%20dataset.%20Additionally%2C%20we%20introduce%0Aa%20metric-driven%20reasoning%20module%20generated%20by%20a%20Small%20Language%20Model%20%28Phi-3%29%2C%0Awhich%20generates%20expert-level%2C%20sample-specific%20justifications%20for%20model%0Apredictions%2C%20thereby%20enhancing%20transparency%20and%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLAIRE%253A%2520A%2520Dual%2520Encoder%2520Network%2520with%2520RIFT%2520Loss%2520and%2520Phi-3%2520Small%2520Language%250A%2520%2520Model%2520Based%2520Interpretability%2520for%2520Cross-Modality%2520Synthetic%2520Aperture%2520Radar%2520and%250A%2520%2520Optical%2520Land%2520Cover%2520Segmentation%26entry.906535625%3DDebopom%2520Sutradhar%2520and%2520Arefin%2520Ittesafun%2520Abian%2520and%2520Mohaimenul%2520Azam%2520Khan%2520Raiaan%2520and%2520Reem%2520E.%2520Mohamed%2520and%2520Sheikh%2520Izzal%2520Azid%2520and%2520Sami%2520Azam%26entry.1292438233%3D%2520%2520Accurate%2520land%2520cover%2520classification%2520from%2520satellite%2520imagery%2520is%2520crucial%2520in%250Aenvironmental%2520monitoring%2520and%2520sustainable%2520resource%2520management.%2520However%252C%2520it%250Aremains%2520challenging%2520due%2520to%2520the%2520complexity%2520of%2520natural%2520landscapes%252C%2520the%2520visual%250Asimilarity%2520between%2520classes%252C%2520and%2520the%2520significant%2520class%2520imbalance%2520in%2520the%250Aavailable%2520datasets.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520dual%2520encoder%250Aarchitecture%2520that%2520independently%2520extracts%2520modality-specific%2520features%2520from%250Aoptical%2520and%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520imagery%252C%2520which%2520are%2520then%2520fused%2520using%250Aa%2520cross-modality%2520attention-fusion%2520module%2520named%2520Cross-modality%2520Land%2520cover%250Asegmentation%2520with%2520Attention%2520and%2520Imbalance-aware%2520Reasoning-Enhanced%2520Explanations%250A%2528CLAIRE%2529.%2520This%2520fusion%2520mechanism%2520highlights%2520complementary%2520spatial%2520and%2520textural%250Afeatures%252C%2520enabling%2520the%2520network%2520to%2520better%2520capture%2520detailed%2520and%2520diverse%2520land%250Acover%2520patterns.%2520We%2520incorporate%2520a%2520hybrid%2520loss%2520function%2520that%2520utilizes%2520Weighted%250AFocal%2520Loss%2520and%2520Tversky%2520Loss%2520named%2520RIFT%2520%2528Rare-Instance%2520Focal-Tversky%2529%2520to%2520address%250Aclass%2520imbalance%2520and%2520improve%2520segmentation%2520performance%2520across%2520underrepresented%250Acategories.%2520Our%2520model%2520achieves%2520competitive%2520performance%2520across%2520multiple%250Abenchmarks%253A%2520a%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%2520of%252056.02%2525%2520and%2520Overall%250AAccuracy%2520%2528OA%2529%2520of%252084.56%2525%2520on%2520the%2520WHU-OPT-SAR%2520dataset%253B%2520strong%2520generalization%2520with%250Aa%2520mIoU%2520of%252059.89%2525%2520and%2520OA%2520of%252073.92%2525%2520on%2520the%2520OpenEarthMap-SAR%2520dataset%253B%2520and%250Aremarkable%2520robustness%2520under%2520cloud-obstructed%2520conditions%252C%2520achieving%2520an%2520mIoU%2520of%250A86.86%2525%2520and%2520OA%2520of%252094.58%2525%2520on%2520the%2520PIE-RGB-SAR%2520dataset.%2520Additionally%252C%2520we%2520introduce%250Aa%2520metric-driven%2520reasoning%2520module%2520generated%2520by%2520a%2520Small%2520Language%2520Model%2520%2528Phi-3%2529%252C%250Awhich%2520generates%2520expert-level%252C%2520sample-specific%2520justifications%2520for%2520model%250Apredictions%252C%2520thereby%2520enhancing%2520transparency%2520and%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLAIRE%3A%20A%20Dual%20Encoder%20Network%20with%20RIFT%20Loss%20and%20Phi-3%20Small%20Language%0A%20%20Model%20Based%20Interpretability%20for%20Cross-Modality%20Synthetic%20Aperture%20Radar%20and%0A%20%20Optical%20Land%20Cover%20Segmentation&entry.906535625=Debopom%20Sutradhar%20and%20Arefin%20Ittesafun%20Abian%20and%20Mohaimenul%20Azam%20Khan%20Raiaan%20and%20Reem%20E.%20Mohamed%20and%20Sheikh%20Izzal%20Azid%20and%20Sami%20Azam&entry.1292438233=%20%20Accurate%20land%20cover%20classification%20from%20satellite%20imagery%20is%20crucial%20in%0Aenvironmental%20monitoring%20and%20sustainable%20resource%20management.%20However%2C%20it%0Aremains%20challenging%20due%20to%20the%20complexity%20of%20natural%20landscapes%2C%20the%20visual%0Asimilarity%20between%20classes%2C%20and%20the%20significant%20class%20imbalance%20in%20the%0Aavailable%20datasets.%20To%20address%20these%20issues%2C%20we%20propose%20a%20dual%20encoder%0Aarchitecture%20that%20independently%20extracts%20modality-specific%20features%20from%0Aoptical%20and%20Synthetic%20Aperture%20Radar%20%28SAR%29%20imagery%2C%20which%20are%20then%20fused%20using%0Aa%20cross-modality%20attention-fusion%20module%20named%20Cross-modality%20Land%20cover%0Asegmentation%20with%20Attention%20and%20Imbalance-aware%20Reasoning-Enhanced%20Explanations%0A%28CLAIRE%29.%20This%20fusion%20mechanism%20highlights%20complementary%20spatial%20and%20textural%0Afeatures%2C%20enabling%20the%20network%20to%20better%20capture%20detailed%20and%20diverse%20land%0Acover%20patterns.%20We%20incorporate%20a%20hybrid%20loss%20function%20that%20utilizes%20Weighted%0AFocal%20Loss%20and%20Tversky%20Loss%20named%20RIFT%20%28Rare-Instance%20Focal-Tversky%29%20to%20address%0Aclass%20imbalance%20and%20improve%20segmentation%20performance%20across%20underrepresented%0Acategories.%20Our%20model%20achieves%20competitive%20performance%20across%20multiple%0Abenchmarks%3A%20a%20mean%20Intersection%20over%20Union%20%28mIoU%29%20of%2056.02%25%20and%20Overall%0AAccuracy%20%28OA%29%20of%2084.56%25%20on%20the%20WHU-OPT-SAR%20dataset%3B%20strong%20generalization%20with%0Aa%20mIoU%20of%2059.89%25%20and%20OA%20of%2073.92%25%20on%20the%20OpenEarthMap-SAR%20dataset%3B%20and%0Aremarkable%20robustness%20under%20cloud-obstructed%20conditions%2C%20achieving%20an%20mIoU%20of%0A86.86%25%20and%20OA%20of%2094.58%25%20on%20the%20PIE-RGB-SAR%20dataset.%20Additionally%2C%20we%20introduce%0Aa%20metric-driven%20reasoning%20module%20generated%20by%20a%20Small%20Language%20Model%20%28Phi-3%29%2C%0Awhich%20generates%20expert-level%2C%20sample-specific%20justifications%20for%20model%0Apredictions%2C%20thereby%20enhancing%20transparency%20and%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11952v1&entry.124074799=Read"},
{"title": "On the Generalization of Representation Uncertainty in Earth Observation", "author": "Spyros Kondylatos and Nikolaos Ioannis Bountos and Dimitrios Michail and Xiao Xiang Zhu and Gustau Camps-Valls and Ioannis Papoutsis", "abstract": "  Recent advances in Computer Vision have introduced the concept of pretrained\nrepresentation uncertainty, enabling zero-shot uncertainty estimation. This\nholds significant potential for Earth Observation (EO), where trustworthiness\nis critical, yet the complexity of EO data poses challenges to\nuncertainty-aware methods. In this work, we investigate the generalization of\nrepresentation uncertainty in EO, considering the domain's unique semantic\ncharacteristics. We pretrain uncertainties on large EO datasets and propose an\nevaluation framework to assess their zero-shot performance in multi-label\nclassification and segmentation EO tasks. Our findings reveal that, unlike\nuncertainties pretrained on natural images, EO-pretraining exhibits strong\ngeneralization across unseen EO domains, geographic locations, and target\ngranularities, while maintaining sensitivity to variations in ground sampling\ndistance. We demonstrate the practical utility of pretrained uncertainties\nshowcasing their alignment with task-specific uncertainties in downstream\ntasks, their sensitivity to real-world EO image noise, and their ability to\ngenerate spatial uncertainty estimates out-of-the-box. Initiating the\ndiscussion on representation uncertainty in EO, our study provides insights\ninto its strengths and limitations, paving the way for future research in the\nfield. Code and weights are available at:\nhttps://github.com/Orion-AI-Lab/EOUncertaintyGeneralization.\n", "link": "http://arxiv.org/abs/2503.07082v2", "date": "2025-09-15", "relevancy": 2.2847, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6072}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6006}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Generalization%20of%20Representation%20Uncertainty%20in%20Earth%20Observation&body=Title%3A%20On%20the%20Generalization%20of%20Representation%20Uncertainty%20in%20Earth%20Observation%0AAuthor%3A%20Spyros%20Kondylatos%20and%20Nikolaos%20Ioannis%20Bountos%20and%20Dimitrios%20Michail%20and%20Xiao%20Xiang%20Zhu%20and%20Gustau%20Camps-Valls%20and%20Ioannis%20Papoutsis%0AAbstract%3A%20%20%20Recent%20advances%20in%20Computer%20Vision%20have%20introduced%20the%20concept%20of%20pretrained%0Arepresentation%20uncertainty%2C%20enabling%20zero-shot%20uncertainty%20estimation.%20This%0Aholds%20significant%20potential%20for%20Earth%20Observation%20%28EO%29%2C%20where%20trustworthiness%0Ais%20critical%2C%20yet%20the%20complexity%20of%20EO%20data%20poses%20challenges%20to%0Auncertainty-aware%20methods.%20In%20this%20work%2C%20we%20investigate%20the%20generalization%20of%0Arepresentation%20uncertainty%20in%20EO%2C%20considering%20the%20domain%27s%20unique%20semantic%0Acharacteristics.%20We%20pretrain%20uncertainties%20on%20large%20EO%20datasets%20and%20propose%20an%0Aevaluation%20framework%20to%20assess%20their%20zero-shot%20performance%20in%20multi-label%0Aclassification%20and%20segmentation%20EO%20tasks.%20Our%20findings%20reveal%20that%2C%20unlike%0Auncertainties%20pretrained%20on%20natural%20images%2C%20EO-pretraining%20exhibits%20strong%0Ageneralization%20across%20unseen%20EO%20domains%2C%20geographic%20locations%2C%20and%20target%0Agranularities%2C%20while%20maintaining%20sensitivity%20to%20variations%20in%20ground%20sampling%0Adistance.%20We%20demonstrate%20the%20practical%20utility%20of%20pretrained%20uncertainties%0Ashowcasing%20their%20alignment%20with%20task-specific%20uncertainties%20in%20downstream%0Atasks%2C%20their%20sensitivity%20to%20real-world%20EO%20image%20noise%2C%20and%20their%20ability%20to%0Agenerate%20spatial%20uncertainty%20estimates%20out-of-the-box.%20Initiating%20the%0Adiscussion%20on%20representation%20uncertainty%20in%20EO%2C%20our%20study%20provides%20insights%0Ainto%20its%20strengths%20and%20limitations%2C%20paving%20the%20way%20for%20future%20research%20in%20the%0Afield.%20Code%20and%20weights%20are%20available%20at%3A%0Ahttps%3A//github.com/Orion-AI-Lab/EOUncertaintyGeneralization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07082v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Generalization%2520of%2520Representation%2520Uncertainty%2520in%2520Earth%2520Observation%26entry.906535625%3DSpyros%2520Kondylatos%2520and%2520Nikolaos%2520Ioannis%2520Bountos%2520and%2520Dimitrios%2520Michail%2520and%2520Xiao%2520Xiang%2520Zhu%2520and%2520Gustau%2520Camps-Valls%2520and%2520Ioannis%2520Papoutsis%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Computer%2520Vision%2520have%2520introduced%2520the%2520concept%2520of%2520pretrained%250Arepresentation%2520uncertainty%252C%2520enabling%2520zero-shot%2520uncertainty%2520estimation.%2520This%250Aholds%2520significant%2520potential%2520for%2520Earth%2520Observation%2520%2528EO%2529%252C%2520where%2520trustworthiness%250Ais%2520critical%252C%2520yet%2520the%2520complexity%2520of%2520EO%2520data%2520poses%2520challenges%2520to%250Auncertainty-aware%2520methods.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520generalization%2520of%250Arepresentation%2520uncertainty%2520in%2520EO%252C%2520considering%2520the%2520domain%2527s%2520unique%2520semantic%250Acharacteristics.%2520We%2520pretrain%2520uncertainties%2520on%2520large%2520EO%2520datasets%2520and%2520propose%2520an%250Aevaluation%2520framework%2520to%2520assess%2520their%2520zero-shot%2520performance%2520in%2520multi-label%250Aclassification%2520and%2520segmentation%2520EO%2520tasks.%2520Our%2520findings%2520reveal%2520that%252C%2520unlike%250Auncertainties%2520pretrained%2520on%2520natural%2520images%252C%2520EO-pretraining%2520exhibits%2520strong%250Ageneralization%2520across%2520unseen%2520EO%2520domains%252C%2520geographic%2520locations%252C%2520and%2520target%250Agranularities%252C%2520while%2520maintaining%2520sensitivity%2520to%2520variations%2520in%2520ground%2520sampling%250Adistance.%2520We%2520demonstrate%2520the%2520practical%2520utility%2520of%2520pretrained%2520uncertainties%250Ashowcasing%2520their%2520alignment%2520with%2520task-specific%2520uncertainties%2520in%2520downstream%250Atasks%252C%2520their%2520sensitivity%2520to%2520real-world%2520EO%2520image%2520noise%252C%2520and%2520their%2520ability%2520to%250Agenerate%2520spatial%2520uncertainty%2520estimates%2520out-of-the-box.%2520Initiating%2520the%250Adiscussion%2520on%2520representation%2520uncertainty%2520in%2520EO%252C%2520our%2520study%2520provides%2520insights%250Ainto%2520its%2520strengths%2520and%2520limitations%252C%2520paving%2520the%2520way%2520for%2520future%2520research%2520in%2520the%250Afield.%2520Code%2520and%2520weights%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/Orion-AI-Lab/EOUncertaintyGeneralization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07082v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Generalization%20of%20Representation%20Uncertainty%20in%20Earth%20Observation&entry.906535625=Spyros%20Kondylatos%20and%20Nikolaos%20Ioannis%20Bountos%20and%20Dimitrios%20Michail%20and%20Xiao%20Xiang%20Zhu%20and%20Gustau%20Camps-Valls%20and%20Ioannis%20Papoutsis&entry.1292438233=%20%20Recent%20advances%20in%20Computer%20Vision%20have%20introduced%20the%20concept%20of%20pretrained%0Arepresentation%20uncertainty%2C%20enabling%20zero-shot%20uncertainty%20estimation.%20This%0Aholds%20significant%20potential%20for%20Earth%20Observation%20%28EO%29%2C%20where%20trustworthiness%0Ais%20critical%2C%20yet%20the%20complexity%20of%20EO%20data%20poses%20challenges%20to%0Auncertainty-aware%20methods.%20In%20this%20work%2C%20we%20investigate%20the%20generalization%20of%0Arepresentation%20uncertainty%20in%20EO%2C%20considering%20the%20domain%27s%20unique%20semantic%0Acharacteristics.%20We%20pretrain%20uncertainties%20on%20large%20EO%20datasets%20and%20propose%20an%0Aevaluation%20framework%20to%20assess%20their%20zero-shot%20performance%20in%20multi-label%0Aclassification%20and%20segmentation%20EO%20tasks.%20Our%20findings%20reveal%20that%2C%20unlike%0Auncertainties%20pretrained%20on%20natural%20images%2C%20EO-pretraining%20exhibits%20strong%0Ageneralization%20across%20unseen%20EO%20domains%2C%20geographic%20locations%2C%20and%20target%0Agranularities%2C%20while%20maintaining%20sensitivity%20to%20variations%20in%20ground%20sampling%0Adistance.%20We%20demonstrate%20the%20practical%20utility%20of%20pretrained%20uncertainties%0Ashowcasing%20their%20alignment%20with%20task-specific%20uncertainties%20in%20downstream%0Atasks%2C%20their%20sensitivity%20to%20real-world%20EO%20image%20noise%2C%20and%20their%20ability%20to%0Agenerate%20spatial%20uncertainty%20estimates%20out-of-the-box.%20Initiating%20the%0Adiscussion%20on%20representation%20uncertainty%20in%20EO%2C%20our%20study%20provides%20insights%0Ainto%20its%20strengths%20and%20limitations%2C%20paving%20the%20way%20for%20future%20research%20in%20the%0Afield.%20Code%20and%20weights%20are%20available%20at%3A%0Ahttps%3A//github.com/Orion-AI-Lab/EOUncertaintyGeneralization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07082v2&entry.124074799=Read"},
{"title": "PartComposer: Learning and Composing Part-Level Concepts from\n  Single-Image Examples", "author": "Junyu Liu and R. Kenny Jones and Daniel Ritchie", "abstract": "  We present PartComposer: a framework for part-level concept learning from\nsingle-image examples that enables text-to-image diffusion models to compose\nnovel objects from meaningful components. Existing methods either struggle with\neffectively learning fine-grained concepts or require a large dataset as input.\nWe propose a dynamic data synthesis pipeline generating diverse part\ncompositions to address one-shot data scarcity. Most importantly, we propose to\nmaximize the mutual information between denoised latents and structured concept\ncodes via a concept predictor, enabling direct regulation on concept\ndisentanglement and re-composition supervision. Our method achieves strong\ndisentanglement and controllable composition, outperforming subject and\npart-level baselines when mixing concepts from the same, or different, object\ncategories.\n", "link": "http://arxiv.org/abs/2506.03004v2", "date": "2025-09-15", "relevancy": 2.2574, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PartComposer%3A%20Learning%20and%20Composing%20Part-Level%20Concepts%20from%0A%20%20Single-Image%20Examples&body=Title%3A%20PartComposer%3A%20Learning%20and%20Composing%20Part-Level%20Concepts%20from%0A%20%20Single-Image%20Examples%0AAuthor%3A%20Junyu%20Liu%20and%20R.%20Kenny%20Jones%20and%20Daniel%20Ritchie%0AAbstract%3A%20%20%20We%20present%20PartComposer%3A%20a%20framework%20for%20part-level%20concept%20learning%20from%0Asingle-image%20examples%20that%20enables%20text-to-image%20diffusion%20models%20to%20compose%0Anovel%20objects%20from%20meaningful%20components.%20Existing%20methods%20either%20struggle%20with%0Aeffectively%20learning%20fine-grained%20concepts%20or%20require%20a%20large%20dataset%20as%20input.%0AWe%20propose%20a%20dynamic%20data%20synthesis%20pipeline%20generating%20diverse%20part%0Acompositions%20to%20address%20one-shot%20data%20scarcity.%20Most%20importantly%2C%20we%20propose%20to%0Amaximize%20the%20mutual%20information%20between%20denoised%20latents%20and%20structured%20concept%0Acodes%20via%20a%20concept%20predictor%2C%20enabling%20direct%20regulation%20on%20concept%0Adisentanglement%20and%20re-composition%20supervision.%20Our%20method%20achieves%20strong%0Adisentanglement%20and%20controllable%20composition%2C%20outperforming%20subject%20and%0Apart-level%20baselines%20when%20mixing%20concepts%20from%20the%20same%2C%20or%20different%2C%20object%0Acategories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03004v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartComposer%253A%2520Learning%2520and%2520Composing%2520Part-Level%2520Concepts%2520from%250A%2520%2520Single-Image%2520Examples%26entry.906535625%3DJunyu%2520Liu%2520and%2520R.%2520Kenny%2520Jones%2520and%2520Daniel%2520Ritchie%26entry.1292438233%3D%2520%2520We%2520present%2520PartComposer%253A%2520a%2520framework%2520for%2520part-level%2520concept%2520learning%2520from%250Asingle-image%2520examples%2520that%2520enables%2520text-to-image%2520diffusion%2520models%2520to%2520compose%250Anovel%2520objects%2520from%2520meaningful%2520components.%2520Existing%2520methods%2520either%2520struggle%2520with%250Aeffectively%2520learning%2520fine-grained%2520concepts%2520or%2520require%2520a%2520large%2520dataset%2520as%2520input.%250AWe%2520propose%2520a%2520dynamic%2520data%2520synthesis%2520pipeline%2520generating%2520diverse%2520part%250Acompositions%2520to%2520address%2520one-shot%2520data%2520scarcity.%2520Most%2520importantly%252C%2520we%2520propose%2520to%250Amaximize%2520the%2520mutual%2520information%2520between%2520denoised%2520latents%2520and%2520structured%2520concept%250Acodes%2520via%2520a%2520concept%2520predictor%252C%2520enabling%2520direct%2520regulation%2520on%2520concept%250Adisentanglement%2520and%2520re-composition%2520supervision.%2520Our%2520method%2520achieves%2520strong%250Adisentanglement%2520and%2520controllable%2520composition%252C%2520outperforming%2520subject%2520and%250Apart-level%2520baselines%2520when%2520mixing%2520concepts%2520from%2520the%2520same%252C%2520or%2520different%252C%2520object%250Acategories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03004v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PartComposer%3A%20Learning%20and%20Composing%20Part-Level%20Concepts%20from%0A%20%20Single-Image%20Examples&entry.906535625=Junyu%20Liu%20and%20R.%20Kenny%20Jones%20and%20Daniel%20Ritchie&entry.1292438233=%20%20We%20present%20PartComposer%3A%20a%20framework%20for%20part-level%20concept%20learning%20from%0Asingle-image%20examples%20that%20enables%20text-to-image%20diffusion%20models%20to%20compose%0Anovel%20objects%20from%20meaningful%20components.%20Existing%20methods%20either%20struggle%20with%0Aeffectively%20learning%20fine-grained%20concepts%20or%20require%20a%20large%20dataset%20as%20input.%0AWe%20propose%20a%20dynamic%20data%20synthesis%20pipeline%20generating%20diverse%20part%0Acompositions%20to%20address%20one-shot%20data%20scarcity.%20Most%20importantly%2C%20we%20propose%20to%0Amaximize%20the%20mutual%20information%20between%20denoised%20latents%20and%20structured%20concept%0Acodes%20via%20a%20concept%20predictor%2C%20enabling%20direct%20regulation%20on%20concept%0Adisentanglement%20and%20re-composition%20supervision.%20Our%20method%20achieves%20strong%0Adisentanglement%20and%20controllable%20composition%2C%20outperforming%20subject%20and%0Apart-level%20baselines%20when%20mixing%20concepts%20from%20the%20same%2C%20or%20different%2C%20object%0Acategories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03004v2&entry.124074799=Read"},
{"title": "FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via\n  Agent-of-Thoughts Reasoning", "author": "Haodong Chen and Haojian Huang and XinXiang Yin and Dian Shao", "abstract": "  Video Question Answering (VideoQA) based on Large Language Models (LLMs) has\nshown potential in general video understanding but faces significant challenges\nwhen applied to the inherently complex domain of sports videos. In this work,\nwe propose FineQuest, the first training-free framework that leverages\ndual-mode reasoning inspired by cognitive science: i) Reactive Reasoning for\nstraightforward sports queries and ii) Deliberative Reasoning for more complex\nones. To bridge the knowledge gap between general-purpose models and\ndomain-specific sports understanding, FineQuest incorporates SSGraph, a\nmultimodal sports knowledge scene graph spanning nine sports, which encodes\nboth visual instances and domain-specific terminology to enhance reasoning\naccuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QA\nand Diving-QA, derived from the FineGym and FineDiving datasets, enabling\ndiverse and comprehensive evaluation. FineQuest achieves state-of-the-art\nperformance on these benchmarks as well as the existing SPORTU dataset, while\nmaintains strong general VideoQA capabilities.\n", "link": "http://arxiv.org/abs/2509.11796v1", "date": "2025-09-15", "relevancy": 2.2561, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5692}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5692}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FineQuest%3A%20Adaptive%20Knowledge-Assisted%20Sports%20Video%20Understanding%20via%0A%20%20Agent-of-Thoughts%20Reasoning&body=Title%3A%20FineQuest%3A%20Adaptive%20Knowledge-Assisted%20Sports%20Video%20Understanding%20via%0A%20%20Agent-of-Thoughts%20Reasoning%0AAuthor%3A%20Haodong%20Chen%20and%20Haojian%20Huang%20and%20XinXiang%20Yin%20and%20Dian%20Shao%0AAbstract%3A%20%20%20Video%20Question%20Answering%20%28VideoQA%29%20based%20on%20Large%20Language%20Models%20%28LLMs%29%20has%0Ashown%20potential%20in%20general%20video%20understanding%20but%20faces%20significant%20challenges%0Awhen%20applied%20to%20the%20inherently%20complex%20domain%20of%20sports%20videos.%20In%20this%20work%2C%0Awe%20propose%20FineQuest%2C%20the%20first%20training-free%20framework%20that%20leverages%0Adual-mode%20reasoning%20inspired%20by%20cognitive%20science%3A%20i%29%20Reactive%20Reasoning%20for%0Astraightforward%20sports%20queries%20and%20ii%29%20Deliberative%20Reasoning%20for%20more%20complex%0Aones.%20To%20bridge%20the%20knowledge%20gap%20between%20general-purpose%20models%20and%0Adomain-specific%20sports%20understanding%2C%20FineQuest%20incorporates%20SSGraph%2C%20a%0Amultimodal%20sports%20knowledge%20scene%20graph%20spanning%20nine%20sports%2C%20which%20encodes%0Aboth%20visual%20instances%20and%20domain-specific%20terminology%20to%20enhance%20reasoning%0Aaccuracy.%20Furthermore%2C%20we%20introduce%20two%20new%20sports%20VideoQA%20benchmarks%2C%20Gym-QA%0Aand%20Diving-QA%2C%20derived%20from%20the%20FineGym%20and%20FineDiving%20datasets%2C%20enabling%0Adiverse%20and%20comprehensive%20evaluation.%20FineQuest%20achieves%20state-of-the-art%0Aperformance%20on%20these%20benchmarks%20as%20well%20as%20the%20existing%20SPORTU%20dataset%2C%20while%0Amaintains%20strong%20general%20VideoQA%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFineQuest%253A%2520Adaptive%2520Knowledge-Assisted%2520Sports%2520Video%2520Understanding%2520via%250A%2520%2520Agent-of-Thoughts%2520Reasoning%26entry.906535625%3DHaodong%2520Chen%2520and%2520Haojian%2520Huang%2520and%2520XinXiang%2520Yin%2520and%2520Dian%2520Shao%26entry.1292438233%3D%2520%2520Video%2520Question%2520Answering%2520%2528VideoQA%2529%2520based%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%250Ashown%2520potential%2520in%2520general%2520video%2520understanding%2520but%2520faces%2520significant%2520challenges%250Awhen%2520applied%2520to%2520the%2520inherently%2520complex%2520domain%2520of%2520sports%2520videos.%2520In%2520this%2520work%252C%250Awe%2520propose%2520FineQuest%252C%2520the%2520first%2520training-free%2520framework%2520that%2520leverages%250Adual-mode%2520reasoning%2520inspired%2520by%2520cognitive%2520science%253A%2520i%2529%2520Reactive%2520Reasoning%2520for%250Astraightforward%2520sports%2520queries%2520and%2520ii%2529%2520Deliberative%2520Reasoning%2520for%2520more%2520complex%250Aones.%2520To%2520bridge%2520the%2520knowledge%2520gap%2520between%2520general-purpose%2520models%2520and%250Adomain-specific%2520sports%2520understanding%252C%2520FineQuest%2520incorporates%2520SSGraph%252C%2520a%250Amultimodal%2520sports%2520knowledge%2520scene%2520graph%2520spanning%2520nine%2520sports%252C%2520which%2520encodes%250Aboth%2520visual%2520instances%2520and%2520domain-specific%2520terminology%2520to%2520enhance%2520reasoning%250Aaccuracy.%2520Furthermore%252C%2520we%2520introduce%2520two%2520new%2520sports%2520VideoQA%2520benchmarks%252C%2520Gym-QA%250Aand%2520Diving-QA%252C%2520derived%2520from%2520the%2520FineGym%2520and%2520FineDiving%2520datasets%252C%2520enabling%250Adiverse%2520and%2520comprehensive%2520evaluation.%2520FineQuest%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520these%2520benchmarks%2520as%2520well%2520as%2520the%2520existing%2520SPORTU%2520dataset%252C%2520while%250Amaintains%2520strong%2520general%2520VideoQA%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FineQuest%3A%20Adaptive%20Knowledge-Assisted%20Sports%20Video%20Understanding%20via%0A%20%20Agent-of-Thoughts%20Reasoning&entry.906535625=Haodong%20Chen%20and%20Haojian%20Huang%20and%20XinXiang%20Yin%20and%20Dian%20Shao&entry.1292438233=%20%20Video%20Question%20Answering%20%28VideoQA%29%20based%20on%20Large%20Language%20Models%20%28LLMs%29%20has%0Ashown%20potential%20in%20general%20video%20understanding%20but%20faces%20significant%20challenges%0Awhen%20applied%20to%20the%20inherently%20complex%20domain%20of%20sports%20videos.%20In%20this%20work%2C%0Awe%20propose%20FineQuest%2C%20the%20first%20training-free%20framework%20that%20leverages%0Adual-mode%20reasoning%20inspired%20by%20cognitive%20science%3A%20i%29%20Reactive%20Reasoning%20for%0Astraightforward%20sports%20queries%20and%20ii%29%20Deliberative%20Reasoning%20for%20more%20complex%0Aones.%20To%20bridge%20the%20knowledge%20gap%20between%20general-purpose%20models%20and%0Adomain-specific%20sports%20understanding%2C%20FineQuest%20incorporates%20SSGraph%2C%20a%0Amultimodal%20sports%20knowledge%20scene%20graph%20spanning%20nine%20sports%2C%20which%20encodes%0Aboth%20visual%20instances%20and%20domain-specific%20terminology%20to%20enhance%20reasoning%0Aaccuracy.%20Furthermore%2C%20we%20introduce%20two%20new%20sports%20VideoQA%20benchmarks%2C%20Gym-QA%0Aand%20Diving-QA%2C%20derived%20from%20the%20FineGym%20and%20FineDiving%20datasets%2C%20enabling%0Adiverse%20and%20comprehensive%20evaluation.%20FineQuest%20achieves%20state-of-the-art%0Aperformance%20on%20these%20benchmarks%20as%20well%20as%20the%20existing%20SPORTU%20dataset%2C%20while%0Amaintains%20strong%20general%20VideoQA%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11796v1&entry.124074799=Read"},
{"title": "Hi-DARTS: Hierarchical Dynamically Adapting Reinforcement Trading System", "author": "Hoon Sagong and Heesu Kim and Hanbeen Hong", "abstract": "  Conventional autonomous trading systems struggle to balance computational\nefficiency and market responsiveness due to their fixed operating frequency. We\npropose Hi-DARTS, a hierarchical multi-agent reinforcement learning framework\nthat addresses this trade-off. Hi-DARTS utilizes a meta-agent to analyze market\nvolatility and dynamically activate specialized Time Frame Agents for\nhigh-frequency or low-frequency trading as needed. During back-testing on AAPL\nstock from January 2024 to May 2025, Hi-DARTS yielded a cumulative return of\n25.17% with a Sharpe Ratio of 0.75. This performance surpasses standard\nbenchmarks, including a passive buy-and-hold strategy on AAPL (12.19% return)\nand the S&P 500 ETF (SPY) (20.01% return). Our work demonstrates that dynamic,\nhierarchical agents can achieve superior risk-adjusted returns while\nmaintaining high computational efficiency.\n", "link": "http://arxiv.org/abs/2509.12048v1", "date": "2025-09-15", "relevancy": 2.2534, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4557}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4524}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hi-DARTS%3A%20Hierarchical%20Dynamically%20Adapting%20Reinforcement%20Trading%20System&body=Title%3A%20Hi-DARTS%3A%20Hierarchical%20Dynamically%20Adapting%20Reinforcement%20Trading%20System%0AAuthor%3A%20Hoon%20Sagong%20and%20Heesu%20Kim%20and%20Hanbeen%20Hong%0AAbstract%3A%20%20%20Conventional%20autonomous%20trading%20systems%20struggle%20to%20balance%20computational%0Aefficiency%20and%20market%20responsiveness%20due%20to%20their%20fixed%20operating%20frequency.%20We%0Apropose%20Hi-DARTS%2C%20a%20hierarchical%20multi-agent%20reinforcement%20learning%20framework%0Athat%20addresses%20this%20trade-off.%20Hi-DARTS%20utilizes%20a%20meta-agent%20to%20analyze%20market%0Avolatility%20and%20dynamically%20activate%20specialized%20Time%20Frame%20Agents%20for%0Ahigh-frequency%20or%20low-frequency%20trading%20as%20needed.%20During%20back-testing%20on%20AAPL%0Astock%20from%20January%202024%20to%20May%202025%2C%20Hi-DARTS%20yielded%20a%20cumulative%20return%20of%0A25.17%25%20with%20a%20Sharpe%20Ratio%20of%200.75.%20This%20performance%20surpasses%20standard%0Abenchmarks%2C%20including%20a%20passive%20buy-and-hold%20strategy%20on%20AAPL%20%2812.19%25%20return%29%0Aand%20the%20S%26P%20500%20ETF%20%28SPY%29%20%2820.01%25%20return%29.%20Our%20work%20demonstrates%20that%20dynamic%2C%0Ahierarchical%20agents%20can%20achieve%20superior%20risk-adjusted%20returns%20while%0Amaintaining%20high%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHi-DARTS%253A%2520Hierarchical%2520Dynamically%2520Adapting%2520Reinforcement%2520Trading%2520System%26entry.906535625%3DHoon%2520Sagong%2520and%2520Heesu%2520Kim%2520and%2520Hanbeen%2520Hong%26entry.1292438233%3D%2520%2520Conventional%2520autonomous%2520trading%2520systems%2520struggle%2520to%2520balance%2520computational%250Aefficiency%2520and%2520market%2520responsiveness%2520due%2520to%2520their%2520fixed%2520operating%2520frequency.%2520We%250Apropose%2520Hi-DARTS%252C%2520a%2520hierarchical%2520multi-agent%2520reinforcement%2520learning%2520framework%250Athat%2520addresses%2520this%2520trade-off.%2520Hi-DARTS%2520utilizes%2520a%2520meta-agent%2520to%2520analyze%2520market%250Avolatility%2520and%2520dynamically%2520activate%2520specialized%2520Time%2520Frame%2520Agents%2520for%250Ahigh-frequency%2520or%2520low-frequency%2520trading%2520as%2520needed.%2520During%2520back-testing%2520on%2520AAPL%250Astock%2520from%2520January%25202024%2520to%2520May%25202025%252C%2520Hi-DARTS%2520yielded%2520a%2520cumulative%2520return%2520of%250A25.17%2525%2520with%2520a%2520Sharpe%2520Ratio%2520of%25200.75.%2520This%2520performance%2520surpasses%2520standard%250Abenchmarks%252C%2520including%2520a%2520passive%2520buy-and-hold%2520strategy%2520on%2520AAPL%2520%252812.19%2525%2520return%2529%250Aand%2520the%2520S%2526P%2520500%2520ETF%2520%2528SPY%2529%2520%252820.01%2525%2520return%2529.%2520Our%2520work%2520demonstrates%2520that%2520dynamic%252C%250Ahierarchical%2520agents%2520can%2520achieve%2520superior%2520risk-adjusted%2520returns%2520while%250Amaintaining%2520high%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hi-DARTS%3A%20Hierarchical%20Dynamically%20Adapting%20Reinforcement%20Trading%20System&entry.906535625=Hoon%20Sagong%20and%20Heesu%20Kim%20and%20Hanbeen%20Hong&entry.1292438233=%20%20Conventional%20autonomous%20trading%20systems%20struggle%20to%20balance%20computational%0Aefficiency%20and%20market%20responsiveness%20due%20to%20their%20fixed%20operating%20frequency.%20We%0Apropose%20Hi-DARTS%2C%20a%20hierarchical%20multi-agent%20reinforcement%20learning%20framework%0Athat%20addresses%20this%20trade-off.%20Hi-DARTS%20utilizes%20a%20meta-agent%20to%20analyze%20market%0Avolatility%20and%20dynamically%20activate%20specialized%20Time%20Frame%20Agents%20for%0Ahigh-frequency%20or%20low-frequency%20trading%20as%20needed.%20During%20back-testing%20on%20AAPL%0Astock%20from%20January%202024%20to%20May%202025%2C%20Hi-DARTS%20yielded%20a%20cumulative%20return%20of%0A25.17%25%20with%20a%20Sharpe%20Ratio%20of%200.75.%20This%20performance%20surpasses%20standard%0Abenchmarks%2C%20including%20a%20passive%20buy-and-hold%20strategy%20on%20AAPL%20%2812.19%25%20return%29%0Aand%20the%20S%26P%20500%20ETF%20%28SPY%29%20%2820.01%25%20return%29.%20Our%20work%20demonstrates%20that%20dynamic%2C%0Ahierarchical%20agents%20can%20achieve%20superior%20risk-adjusted%20returns%20while%0Amaintaining%20high%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12048v1&entry.124074799=Read"},
{"title": "End-to-End Learning of Multi-Organ Implicit Surfaces from 3D Medical\n  Imaging Data", "author": "Farahdiba Zarin and Nicolas Padoy and J\u00e9r\u00e9my Dana and Vinkle Srivastav", "abstract": "  The fine-grained surface reconstruction of different organs from 3D medical\nimaging can provide advanced diagnostic support and improved surgical planning.\nHowever, the representation of the organs is often limited by the resolution,\nwith a detailed higher resolution requiring more memory and computing\nfootprint. Implicit representations of objects have been proposed to alleviate\nthis problem in general computer vision by providing compact and differentiable\nfunctions to represent the 3D object shapes. However, architectural and\ndata-related differences prevent the direct application of these methods to\nmedical images. This work introduces ImplMORe, an end-to-end deep learning\nmethod using implicit surface representations for multi-organ reconstruction\nfrom 3D medical images. ImplMORe incorporates local features using a 3D CNN\nencoder and performs multi-scale interpolation to learn the features in the\ncontinuous domain using occupancy functions. We apply our method for single and\nmultiple organ reconstructions using the totalsegmentator dataset. By\nleveraging the continuous nature of occupancy functions, our approach\noutperforms the discrete explicit representation based surface reconstruction\napproaches, providing fine-grained surface details of the organ at a resolution\nhigher than the given input image. The source code will be made publicly\navailable at: https://github.com/CAMMA-public/ImplMORe\n", "link": "http://arxiv.org/abs/2509.12068v1", "date": "2025-09-15", "relevancy": 2.2431, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5731}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5542}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Learning%20of%20Multi-Organ%20Implicit%20Surfaces%20from%203D%20Medical%0A%20%20Imaging%20Data&body=Title%3A%20End-to-End%20Learning%20of%20Multi-Organ%20Implicit%20Surfaces%20from%203D%20Medical%0A%20%20Imaging%20Data%0AAuthor%3A%20Farahdiba%20Zarin%20and%20Nicolas%20Padoy%20and%20J%C3%A9r%C3%A9my%20Dana%20and%20Vinkle%20Srivastav%0AAbstract%3A%20%20%20The%20fine-grained%20surface%20reconstruction%20of%20different%20organs%20from%203D%20medical%0Aimaging%20can%20provide%20advanced%20diagnostic%20support%20and%20improved%20surgical%20planning.%0AHowever%2C%20the%20representation%20of%20the%20organs%20is%20often%20limited%20by%20the%20resolution%2C%0Awith%20a%20detailed%20higher%20resolution%20requiring%20more%20memory%20and%20computing%0Afootprint.%20Implicit%20representations%20of%20objects%20have%20been%20proposed%20to%20alleviate%0Athis%20problem%20in%20general%20computer%20vision%20by%20providing%20compact%20and%20differentiable%0Afunctions%20to%20represent%20the%203D%20object%20shapes.%20However%2C%20architectural%20and%0Adata-related%20differences%20prevent%20the%20direct%20application%20of%20these%20methods%20to%0Amedical%20images.%20This%20work%20introduces%20ImplMORe%2C%20an%20end-to-end%20deep%20learning%0Amethod%20using%20implicit%20surface%20representations%20for%20multi-organ%20reconstruction%0Afrom%203D%20medical%20images.%20ImplMORe%20incorporates%20local%20features%20using%20a%203D%20CNN%0Aencoder%20and%20performs%20multi-scale%20interpolation%20to%20learn%20the%20features%20in%20the%0Acontinuous%20domain%20using%20occupancy%20functions.%20We%20apply%20our%20method%20for%20single%20and%0Amultiple%20organ%20reconstructions%20using%20the%20totalsegmentator%20dataset.%20By%0Aleveraging%20the%20continuous%20nature%20of%20occupancy%20functions%2C%20our%20approach%0Aoutperforms%20the%20discrete%20explicit%20representation%20based%20surface%20reconstruction%0Aapproaches%2C%20providing%20fine-grained%20surface%20details%20of%20the%20organ%20at%20a%20resolution%0Ahigher%20than%20the%20given%20input%20image.%20The%20source%20code%20will%20be%20made%20publicly%0Aavailable%20at%3A%20https%3A//github.com/CAMMA-public/ImplMORe%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Learning%2520of%2520Multi-Organ%2520Implicit%2520Surfaces%2520from%25203D%2520Medical%250A%2520%2520Imaging%2520Data%26entry.906535625%3DFarahdiba%2520Zarin%2520and%2520Nicolas%2520Padoy%2520and%2520J%25C3%25A9r%25C3%25A9my%2520Dana%2520and%2520Vinkle%2520Srivastav%26entry.1292438233%3D%2520%2520The%2520fine-grained%2520surface%2520reconstruction%2520of%2520different%2520organs%2520from%25203D%2520medical%250Aimaging%2520can%2520provide%2520advanced%2520diagnostic%2520support%2520and%2520improved%2520surgical%2520planning.%250AHowever%252C%2520the%2520representation%2520of%2520the%2520organs%2520is%2520often%2520limited%2520by%2520the%2520resolution%252C%250Awith%2520a%2520detailed%2520higher%2520resolution%2520requiring%2520more%2520memory%2520and%2520computing%250Afootprint.%2520Implicit%2520representations%2520of%2520objects%2520have%2520been%2520proposed%2520to%2520alleviate%250Athis%2520problem%2520in%2520general%2520computer%2520vision%2520by%2520providing%2520compact%2520and%2520differentiable%250Afunctions%2520to%2520represent%2520the%25203D%2520object%2520shapes.%2520However%252C%2520architectural%2520and%250Adata-related%2520differences%2520prevent%2520the%2520direct%2520application%2520of%2520these%2520methods%2520to%250Amedical%2520images.%2520This%2520work%2520introduces%2520ImplMORe%252C%2520an%2520end-to-end%2520deep%2520learning%250Amethod%2520using%2520implicit%2520surface%2520representations%2520for%2520multi-organ%2520reconstruction%250Afrom%25203D%2520medical%2520images.%2520ImplMORe%2520incorporates%2520local%2520features%2520using%2520a%25203D%2520CNN%250Aencoder%2520and%2520performs%2520multi-scale%2520interpolation%2520to%2520learn%2520the%2520features%2520in%2520the%250Acontinuous%2520domain%2520using%2520occupancy%2520functions.%2520We%2520apply%2520our%2520method%2520for%2520single%2520and%250Amultiple%2520organ%2520reconstructions%2520using%2520the%2520totalsegmentator%2520dataset.%2520By%250Aleveraging%2520the%2520continuous%2520nature%2520of%2520occupancy%2520functions%252C%2520our%2520approach%250Aoutperforms%2520the%2520discrete%2520explicit%2520representation%2520based%2520surface%2520reconstruction%250Aapproaches%252C%2520providing%2520fine-grained%2520surface%2520details%2520of%2520the%2520organ%2520at%2520a%2520resolution%250Ahigher%2520than%2520the%2520given%2520input%2520image.%2520The%2520source%2520code%2520will%2520be%2520made%2520publicly%250Aavailable%2520at%253A%2520https%253A//github.com/CAMMA-public/ImplMORe%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Learning%20of%20Multi-Organ%20Implicit%20Surfaces%20from%203D%20Medical%0A%20%20Imaging%20Data&entry.906535625=Farahdiba%20Zarin%20and%20Nicolas%20Padoy%20and%20J%C3%A9r%C3%A9my%20Dana%20and%20Vinkle%20Srivastav&entry.1292438233=%20%20The%20fine-grained%20surface%20reconstruction%20of%20different%20organs%20from%203D%20medical%0Aimaging%20can%20provide%20advanced%20diagnostic%20support%20and%20improved%20surgical%20planning.%0AHowever%2C%20the%20representation%20of%20the%20organs%20is%20often%20limited%20by%20the%20resolution%2C%0Awith%20a%20detailed%20higher%20resolution%20requiring%20more%20memory%20and%20computing%0Afootprint.%20Implicit%20representations%20of%20objects%20have%20been%20proposed%20to%20alleviate%0Athis%20problem%20in%20general%20computer%20vision%20by%20providing%20compact%20and%20differentiable%0Afunctions%20to%20represent%20the%203D%20object%20shapes.%20However%2C%20architectural%20and%0Adata-related%20differences%20prevent%20the%20direct%20application%20of%20these%20methods%20to%0Amedical%20images.%20This%20work%20introduces%20ImplMORe%2C%20an%20end-to-end%20deep%20learning%0Amethod%20using%20implicit%20surface%20representations%20for%20multi-organ%20reconstruction%0Afrom%203D%20medical%20images.%20ImplMORe%20incorporates%20local%20features%20using%20a%203D%20CNN%0Aencoder%20and%20performs%20multi-scale%20interpolation%20to%20learn%20the%20features%20in%20the%0Acontinuous%20domain%20using%20occupancy%20functions.%20We%20apply%20our%20method%20for%20single%20and%0Amultiple%20organ%20reconstructions%20using%20the%20totalsegmentator%20dataset.%20By%0Aleveraging%20the%20continuous%20nature%20of%20occupancy%20functions%2C%20our%20approach%0Aoutperforms%20the%20discrete%20explicit%20representation%20based%20surface%20reconstruction%0Aapproaches%2C%20providing%20fine-grained%20surface%20details%20of%20the%20organ%20at%20a%20resolution%0Ahigher%20than%20the%20given%20input%20image.%20The%20source%20code%20will%20be%20made%20publicly%0Aavailable%20at%3A%20https%3A//github.com/CAMMA-public/ImplMORe%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12068v1&entry.124074799=Read"},
{"title": "Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing", "author": "Bingyu Li and Haocheng Dong and Da Zhang and Zhiyuan Zhao and Junyu Gao and Xuelong Li", "abstract": "  Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task\nthat adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS)\ndomain, remains underexplored due to the absence of a unified evaluation\nbenchmark and the domain gap between natural and RS images. To bridge these\ngaps, we first establish a standardized OVRSIS benchmark (\\textbf{OVRSISBench})\nbased on widely-used RS segmentation datasets, enabling consistent evaluation\nacross methods. Using this benchmark, we comprehensively evaluate several\nrepresentative OVS/OVRSIS models and reveal their limitations when directly\napplied to remote sensing scenarios. Building on these insights, we propose\n\\textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for\nremote sensing. RSKT-Seg integrates three key components: (1) a\nMulti-Directional Cost Map Aggregation (RS-CMA) module that captures\nrotation-invariant visual cues by computing vision-language cosine similarities\nacross multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion)\ntransformer, which jointly models spatial and semantic dependencies with a\nlightweight dimensionality reduction strategy; and (3) a Remote Sensing\nKnowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and\nfacilitates domain adaptation via enhanced upsampling. Extensive experiments on\nthe benchmark show that RSKT-Seg consistently outperforms strong OVS baselines\nby +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through\nefficient aggregation. Our code is\n\\href{https://github.com/LiBingyu01/RSKT-Seg}{\\textcolor{blue}{here}}.\n", "link": "http://arxiv.org/abs/2509.12040v1", "date": "2025-09-15", "relevancy": 2.2395, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Efficient%20Open-Vocabulary%20Segmentation%20in%20the%20Remote%20Sensing&body=Title%3A%20Exploring%20Efficient%20Open-Vocabulary%20Segmentation%20in%20the%20Remote%20Sensing%0AAuthor%3A%20Bingyu%20Li%20and%20Haocheng%20Dong%20and%20Da%20Zhang%20and%20Zhiyuan%20Zhao%20and%20Junyu%20Gao%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Open-Vocabulary%20Remote%20Sensing%20Image%20Segmentation%20%28OVRSIS%29%2C%20an%20emerging%20task%0Athat%20adapts%20Open-Vocabulary%20Segmentation%20%28OVS%29%20to%20the%20remote%20sensing%20%28RS%29%0Adomain%2C%20remains%20underexplored%20due%20to%20the%20absence%20of%20a%20unified%20evaluation%0Abenchmark%20and%20the%20domain%20gap%20between%20natural%20and%20RS%20images.%20To%20bridge%20these%0Agaps%2C%20we%20first%20establish%20a%20standardized%20OVRSIS%20benchmark%20%28%5Ctextbf%7BOVRSISBench%7D%29%0Abased%20on%20widely-used%20RS%20segmentation%20datasets%2C%20enabling%20consistent%20evaluation%0Aacross%20methods.%20Using%20this%20benchmark%2C%20we%20comprehensively%20evaluate%20several%0Arepresentative%20OVS/OVRSIS%20models%20and%20reveal%20their%20limitations%20when%20directly%0Aapplied%20to%20remote%20sensing%20scenarios.%20Building%20on%20these%20insights%2C%20we%20propose%0A%5Ctextbf%7BRSKT-Seg%7D%2C%20a%20novel%20open-vocabulary%20segmentation%20framework%20tailored%20for%0Aremote%20sensing.%20RSKT-Seg%20integrates%20three%20key%20components%3A%20%281%29%20a%0AMulti-Directional%20Cost%20Map%20Aggregation%20%28RS-CMA%29%20module%20that%20captures%0Arotation-invariant%20visual%20cues%20by%20computing%20vision-language%20cosine%20similarities%0Aacross%20multiple%20directions%3B%20%282%29%20an%20Efficient%20Cost%20Map%20Fusion%20%28RS-Fusion%29%0Atransformer%2C%20which%20jointly%20models%20spatial%20and%20semantic%20dependencies%20with%20a%0Alightweight%20dimensionality%20reduction%20strategy%3B%20and%20%283%29%20a%20Remote%20Sensing%0AKnowledge%20Transfer%20%28RS-Transfer%29%20module%20that%20injects%20pre-trained%20knowledge%20and%0Afacilitates%20domain%20adaptation%20via%20enhanced%20upsampling.%20Extensive%20experiments%20on%0Athe%20benchmark%20show%20that%20RSKT-Seg%20consistently%20outperforms%20strong%20OVS%20baselines%0Aby%20%2B3.8%20mIoU%20and%20%2B5.9%20mACC%2C%20while%20achieving%202x%20faster%20inference%20through%0Aefficient%20aggregation.%20Our%20code%20is%0A%5Chref%7Bhttps%3A//github.com/LiBingyu01/RSKT-Seg%7D%7B%5Ctextcolor%7Bblue%7D%7Bhere%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Efficient%2520Open-Vocabulary%2520Segmentation%2520in%2520the%2520Remote%2520Sensing%26entry.906535625%3DBingyu%2520Li%2520and%2520Haocheng%2520Dong%2520and%2520Da%2520Zhang%2520and%2520Zhiyuan%2520Zhao%2520and%2520Junyu%2520Gao%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Open-Vocabulary%2520Remote%2520Sensing%2520Image%2520Segmentation%2520%2528OVRSIS%2529%252C%2520an%2520emerging%2520task%250Athat%2520adapts%2520Open-Vocabulary%2520Segmentation%2520%2528OVS%2529%2520to%2520the%2520remote%2520sensing%2520%2528RS%2529%250Adomain%252C%2520remains%2520underexplored%2520due%2520to%2520the%2520absence%2520of%2520a%2520unified%2520evaluation%250Abenchmark%2520and%2520the%2520domain%2520gap%2520between%2520natural%2520and%2520RS%2520images.%2520To%2520bridge%2520these%250Agaps%252C%2520we%2520first%2520establish%2520a%2520standardized%2520OVRSIS%2520benchmark%2520%2528%255Ctextbf%257BOVRSISBench%257D%2529%250Abased%2520on%2520widely-used%2520RS%2520segmentation%2520datasets%252C%2520enabling%2520consistent%2520evaluation%250Aacross%2520methods.%2520Using%2520this%2520benchmark%252C%2520we%2520comprehensively%2520evaluate%2520several%250Arepresentative%2520OVS/OVRSIS%2520models%2520and%2520reveal%2520their%2520limitations%2520when%2520directly%250Aapplied%2520to%2520remote%2520sensing%2520scenarios.%2520Building%2520on%2520these%2520insights%252C%2520we%2520propose%250A%255Ctextbf%257BRSKT-Seg%257D%252C%2520a%2520novel%2520open-vocabulary%2520segmentation%2520framework%2520tailored%2520for%250Aremote%2520sensing.%2520RSKT-Seg%2520integrates%2520three%2520key%2520components%253A%2520%25281%2529%2520a%250AMulti-Directional%2520Cost%2520Map%2520Aggregation%2520%2528RS-CMA%2529%2520module%2520that%2520captures%250Arotation-invariant%2520visual%2520cues%2520by%2520computing%2520vision-language%2520cosine%2520similarities%250Aacross%2520multiple%2520directions%253B%2520%25282%2529%2520an%2520Efficient%2520Cost%2520Map%2520Fusion%2520%2528RS-Fusion%2529%250Atransformer%252C%2520which%2520jointly%2520models%2520spatial%2520and%2520semantic%2520dependencies%2520with%2520a%250Alightweight%2520dimensionality%2520reduction%2520strategy%253B%2520and%2520%25283%2529%2520a%2520Remote%2520Sensing%250AKnowledge%2520Transfer%2520%2528RS-Transfer%2529%2520module%2520that%2520injects%2520pre-trained%2520knowledge%2520and%250Afacilitates%2520domain%2520adaptation%2520via%2520enhanced%2520upsampling.%2520Extensive%2520experiments%2520on%250Athe%2520benchmark%2520show%2520that%2520RSKT-Seg%2520consistently%2520outperforms%2520strong%2520OVS%2520baselines%250Aby%2520%252B3.8%2520mIoU%2520and%2520%252B5.9%2520mACC%252C%2520while%2520achieving%25202x%2520faster%2520inference%2520through%250Aefficient%2520aggregation.%2520Our%2520code%2520is%250A%255Chref%257Bhttps%253A//github.com/LiBingyu01/RSKT-Seg%257D%257B%255Ctextcolor%257Bblue%257D%257Bhere%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Efficient%20Open-Vocabulary%20Segmentation%20in%20the%20Remote%20Sensing&entry.906535625=Bingyu%20Li%20and%20Haocheng%20Dong%20and%20Da%20Zhang%20and%20Zhiyuan%20Zhao%20and%20Junyu%20Gao%20and%20Xuelong%20Li&entry.1292438233=%20%20Open-Vocabulary%20Remote%20Sensing%20Image%20Segmentation%20%28OVRSIS%29%2C%20an%20emerging%20task%0Athat%20adapts%20Open-Vocabulary%20Segmentation%20%28OVS%29%20to%20the%20remote%20sensing%20%28RS%29%0Adomain%2C%20remains%20underexplored%20due%20to%20the%20absence%20of%20a%20unified%20evaluation%0Abenchmark%20and%20the%20domain%20gap%20between%20natural%20and%20RS%20images.%20To%20bridge%20these%0Agaps%2C%20we%20first%20establish%20a%20standardized%20OVRSIS%20benchmark%20%28%5Ctextbf%7BOVRSISBench%7D%29%0Abased%20on%20widely-used%20RS%20segmentation%20datasets%2C%20enabling%20consistent%20evaluation%0Aacross%20methods.%20Using%20this%20benchmark%2C%20we%20comprehensively%20evaluate%20several%0Arepresentative%20OVS/OVRSIS%20models%20and%20reveal%20their%20limitations%20when%20directly%0Aapplied%20to%20remote%20sensing%20scenarios.%20Building%20on%20these%20insights%2C%20we%20propose%0A%5Ctextbf%7BRSKT-Seg%7D%2C%20a%20novel%20open-vocabulary%20segmentation%20framework%20tailored%20for%0Aremote%20sensing.%20RSKT-Seg%20integrates%20three%20key%20components%3A%20%281%29%20a%0AMulti-Directional%20Cost%20Map%20Aggregation%20%28RS-CMA%29%20module%20that%20captures%0Arotation-invariant%20visual%20cues%20by%20computing%20vision-language%20cosine%20similarities%0Aacross%20multiple%20directions%3B%20%282%29%20an%20Efficient%20Cost%20Map%20Fusion%20%28RS-Fusion%29%0Atransformer%2C%20which%20jointly%20models%20spatial%20and%20semantic%20dependencies%20with%20a%0Alightweight%20dimensionality%20reduction%20strategy%3B%20and%20%283%29%20a%20Remote%20Sensing%0AKnowledge%20Transfer%20%28RS-Transfer%29%20module%20that%20injects%20pre-trained%20knowledge%20and%0Afacilitates%20domain%20adaptation%20via%20enhanced%20upsampling.%20Extensive%20experiments%20on%0Athe%20benchmark%20show%20that%20RSKT-Seg%20consistently%20outperforms%20strong%20OVS%20baselines%0Aby%20%2B3.8%20mIoU%20and%20%2B5.9%20mACC%2C%20while%20achieving%202x%20faster%20inference%20through%0Aefficient%20aggregation.%20Our%20code%20is%0A%5Chref%7Bhttps%3A//github.com/LiBingyu01/RSKT-Seg%7D%7B%5Ctextcolor%7Bblue%7D%7Bhere%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12040v1&entry.124074799=Read"},
{"title": "Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360\u00b0\n  Videos", "author": "Mahmoud Z. A. Wahba and Sara Baldoni and Federica Battisti", "abstract": "  The recent success of immersive applications is pushing the research\ncommunity to define new approaches to process 360{\\deg} images and videos and\noptimize their transmission. Among these, saliency estimation provides a\npowerful tool that can be used to identify visually relevant areas and,\nconsequently, adapt processing algorithms. Although saliency estimation has\nbeen widely investigated for 2D content, very few algorithms have been proposed\nfor 360{\\deg} saliency estimation. Towards this goal, we introduce Sphere-GAN,\na saliency detection model for 360{\\deg} videos that leverages a Generative\nAdversarial Network with spherical convolutions. Extensive experiments were\nconducted using a public 360{\\deg} video saliency dataset, and the results\ndemonstrate that Sphere-GAN outperforms state-of-the-art models in accurately\npredicting saliency maps.\n", "link": "http://arxiv.org/abs/2509.11948v1", "date": "2025-09-15", "relevancy": 2.237, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5684}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5552}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sphere-GAN%3A%20a%20GAN-based%20Approach%20for%20Saliency%20Estimation%20in%20360%C2%B0%0A%20%20Videos&body=Title%3A%20Sphere-GAN%3A%20a%20GAN-based%20Approach%20for%20Saliency%20Estimation%20in%20360%C2%B0%0A%20%20Videos%0AAuthor%3A%20Mahmoud%20Z.%20A.%20Wahba%20and%20Sara%20Baldoni%20and%20Federica%20Battisti%0AAbstract%3A%20%20%20The%20recent%20success%20of%20immersive%20applications%20is%20pushing%20the%20research%0Acommunity%20to%20define%20new%20approaches%20to%20process%20360%7B%5Cdeg%7D%20images%20and%20videos%20and%0Aoptimize%20their%20transmission.%20Among%20these%2C%20saliency%20estimation%20provides%20a%0Apowerful%20tool%20that%20can%20be%20used%20to%20identify%20visually%20relevant%20areas%20and%2C%0Aconsequently%2C%20adapt%20processing%20algorithms.%20Although%20saliency%20estimation%20has%0Abeen%20widely%20investigated%20for%202D%20content%2C%20very%20few%20algorithms%20have%20been%20proposed%0Afor%20360%7B%5Cdeg%7D%20saliency%20estimation.%20Towards%20this%20goal%2C%20we%20introduce%20Sphere-GAN%2C%0Aa%20saliency%20detection%20model%20for%20360%7B%5Cdeg%7D%20videos%20that%20leverages%20a%20Generative%0AAdversarial%20Network%20with%20spherical%20convolutions.%20Extensive%20experiments%20were%0Aconducted%20using%20a%20public%20360%7B%5Cdeg%7D%20video%20saliency%20dataset%2C%20and%20the%20results%0Ademonstrate%20that%20Sphere-GAN%20outperforms%20state-of-the-art%20models%20in%20accurately%0Apredicting%20saliency%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSphere-GAN%253A%2520a%2520GAN-based%2520Approach%2520for%2520Saliency%2520Estimation%2520in%2520360%25C2%25B0%250A%2520%2520Videos%26entry.906535625%3DMahmoud%2520Z.%2520A.%2520Wahba%2520and%2520Sara%2520Baldoni%2520and%2520Federica%2520Battisti%26entry.1292438233%3D%2520%2520The%2520recent%2520success%2520of%2520immersive%2520applications%2520is%2520pushing%2520the%2520research%250Acommunity%2520to%2520define%2520new%2520approaches%2520to%2520process%2520360%257B%255Cdeg%257D%2520images%2520and%2520videos%2520and%250Aoptimize%2520their%2520transmission.%2520Among%2520these%252C%2520saliency%2520estimation%2520provides%2520a%250Apowerful%2520tool%2520that%2520can%2520be%2520used%2520to%2520identify%2520visually%2520relevant%2520areas%2520and%252C%250Aconsequently%252C%2520adapt%2520processing%2520algorithms.%2520Although%2520saliency%2520estimation%2520has%250Abeen%2520widely%2520investigated%2520for%25202D%2520content%252C%2520very%2520few%2520algorithms%2520have%2520been%2520proposed%250Afor%2520360%257B%255Cdeg%257D%2520saliency%2520estimation.%2520Towards%2520this%2520goal%252C%2520we%2520introduce%2520Sphere-GAN%252C%250Aa%2520saliency%2520detection%2520model%2520for%2520360%257B%255Cdeg%257D%2520videos%2520that%2520leverages%2520a%2520Generative%250AAdversarial%2520Network%2520with%2520spherical%2520convolutions.%2520Extensive%2520experiments%2520were%250Aconducted%2520using%2520a%2520public%2520360%257B%255Cdeg%257D%2520video%2520saliency%2520dataset%252C%2520and%2520the%2520results%250Ademonstrate%2520that%2520Sphere-GAN%2520outperforms%2520state-of-the-art%2520models%2520in%2520accurately%250Apredicting%2520saliency%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sphere-GAN%3A%20a%20GAN-based%20Approach%20for%20Saliency%20Estimation%20in%20360%C2%B0%0A%20%20Videos&entry.906535625=Mahmoud%20Z.%20A.%20Wahba%20and%20Sara%20Baldoni%20and%20Federica%20Battisti&entry.1292438233=%20%20The%20recent%20success%20of%20immersive%20applications%20is%20pushing%20the%20research%0Acommunity%20to%20define%20new%20approaches%20to%20process%20360%7B%5Cdeg%7D%20images%20and%20videos%20and%0Aoptimize%20their%20transmission.%20Among%20these%2C%20saliency%20estimation%20provides%20a%0Apowerful%20tool%20that%20can%20be%20used%20to%20identify%20visually%20relevant%20areas%20and%2C%0Aconsequently%2C%20adapt%20processing%20algorithms.%20Although%20saliency%20estimation%20has%0Abeen%20widely%20investigated%20for%202D%20content%2C%20very%20few%20algorithms%20have%20been%20proposed%0Afor%20360%7B%5Cdeg%7D%20saliency%20estimation.%20Towards%20this%20goal%2C%20we%20introduce%20Sphere-GAN%2C%0Aa%20saliency%20detection%20model%20for%20360%7B%5Cdeg%7D%20videos%20that%20leverages%20a%20Generative%0AAdversarial%20Network%20with%20spherical%20convolutions.%20Extensive%20experiments%20were%0Aconducted%20using%20a%20public%20360%7B%5Cdeg%7D%20video%20saliency%20dataset%2C%20and%20the%20results%0Ademonstrate%20that%20Sphere-GAN%20outperforms%20state-of-the-art%20models%20in%20accurately%0Apredicting%20saliency%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11948v1&entry.124074799=Read"},
{"title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models", "author": "Haiduo Huang and Fuwei Yang and Zhenhua Liu and Xuanwu Yin and Dong Li and Pengju Ren and Emad Barsoum", "abstract": "  Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.\n", "link": "http://arxiv.org/abs/2509.11815v1", "date": "2025-09-15", "relevancy": 2.2245, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecVLM%3A%20Fast%20Speculative%20Decoding%20in%20Vision-Language%20Models&body=Title%3A%20SpecVLM%3A%20Fast%20Speculative%20Decoding%20in%20Vision-Language%20Models%0AAuthor%3A%20Haiduo%20Huang%20and%20Fuwei%20Yang%20and%20Zhenhua%20Liu%20and%20Xuanwu%20Yin%20and%20Dong%20Li%20and%20Pengju%20Ren%20and%20Emad%20Barsoum%0AAbstract%3A%20%20%20Speculative%20decoding%20is%20a%20powerful%20way%20to%20accelerate%20autoregressive%20large%0Alanguage%20models%20%28LLMs%29%2C%20but%20directly%20porting%20it%20to%20vision-language%20models%0A%28VLMs%29%20faces%20unique%20systems%20constraints%3A%20the%20prefill%20stage%20is%20dominated%20by%0Avisual%20tokens%20whose%20count%20scales%20with%20image%20resolution%20and%20video%20length%2C%0Ainflating%20both%20compute%20and%20memory%2C%20especially%20the%20key-value%20%28KV%29%20cache.%20We%0Astudy%20speculative%20decoding%20for%20VLMs%20and%20introduce%20SpecVLM%2C%20a%20practical%20system%0Athat%20%281%29%20establishes%20a%20strong%20EAGLE-2-style%20baseline%2C%20EagleVLM%2C%20delivering%0A1.5--2.3x%20end-to-end%20speedups%20over%20full%20autoregressive%20inference%2C%20and%20%282%29%0Afurther%20accelerates%20VLM%20inference%20with%20an%20elastic%20visual%20compressor%20that%0Aadaptively%20selects%20among%20pruning%2C%20pooling%2C%20convolution%2C%20and%20resampler%0Aprimitives%20to%20balance%20FLOPs/parameters%20and%20accuracy%20per%20input.%20To%20avoid%20costly%0Aoffline%20distillation%20corpora%2C%20we%20propose%20an%20online-logit%20distillation%20protocol%0Athat%20trains%20the%20draft%20model%20with%20on-the-fly%20teacher%20logits%20and%20penultimate%0Afeatures%20using%20a%20combined%20cross-entropy%20and%20Smooth%20L1%20objective%2C%20eliminating%0Astorage%20and%20preprocessing%20while%20remaining%20compute-efficient.%20This%20protocol%0Areveals%20a%20training-time%20scaling%20effect%3A%20longer%20online%20training%20monotonically%0Aincreases%20the%20draft%20model%27s%20average%20accepted%20length%2C%20improving%20speculative%0Aefficiency.%20Empirically%2C%20SpecVLM%20achieves%20additional%20acceleration%2C%20culminating%0Ain%202.5--2.9x%20end-to-end%20speedups%20within%205%20epochs%20across%20LLaVA%20and%20MMMU%2C%0Aconsistently%20over%20resolutions%20and%20task%20difficulties%2C%20while%20preserving%20the%0Atarget%20model%27s%20output%20distribution%20%28lossless%20decoding%29.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/haiduo/SpecVLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecVLM%253A%2520Fast%2520Speculative%2520Decoding%2520in%2520Vision-Language%2520Models%26entry.906535625%3DHaiduo%2520Huang%2520and%2520Fuwei%2520Yang%2520and%2520Zhenhua%2520Liu%2520and%2520Xuanwu%2520Yin%2520and%2520Dong%2520Li%2520and%2520Pengju%2520Ren%2520and%2520Emad%2520Barsoum%26entry.1292438233%3D%2520%2520Speculative%2520decoding%2520is%2520a%2520powerful%2520way%2520to%2520accelerate%2520autoregressive%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520but%2520directly%2520porting%2520it%2520to%2520vision-language%2520models%250A%2528VLMs%2529%2520faces%2520unique%2520systems%2520constraints%253A%2520the%2520prefill%2520stage%2520is%2520dominated%2520by%250Avisual%2520tokens%2520whose%2520count%2520scales%2520with%2520image%2520resolution%2520and%2520video%2520length%252C%250Ainflating%2520both%2520compute%2520and%2520memory%252C%2520especially%2520the%2520key-value%2520%2528KV%2529%2520cache.%2520We%250Astudy%2520speculative%2520decoding%2520for%2520VLMs%2520and%2520introduce%2520SpecVLM%252C%2520a%2520practical%2520system%250Athat%2520%25281%2529%2520establishes%2520a%2520strong%2520EAGLE-2-style%2520baseline%252C%2520EagleVLM%252C%2520delivering%250A1.5--2.3x%2520end-to-end%2520speedups%2520over%2520full%2520autoregressive%2520inference%252C%2520and%2520%25282%2529%250Afurther%2520accelerates%2520VLM%2520inference%2520with%2520an%2520elastic%2520visual%2520compressor%2520that%250Aadaptively%2520selects%2520among%2520pruning%252C%2520pooling%252C%2520convolution%252C%2520and%2520resampler%250Aprimitives%2520to%2520balance%2520FLOPs/parameters%2520and%2520accuracy%2520per%2520input.%2520To%2520avoid%2520costly%250Aoffline%2520distillation%2520corpora%252C%2520we%2520propose%2520an%2520online-logit%2520distillation%2520protocol%250Athat%2520trains%2520the%2520draft%2520model%2520with%2520on-the-fly%2520teacher%2520logits%2520and%2520penultimate%250Afeatures%2520using%2520a%2520combined%2520cross-entropy%2520and%2520Smooth%2520L1%2520objective%252C%2520eliminating%250Astorage%2520and%2520preprocessing%2520while%2520remaining%2520compute-efficient.%2520This%2520protocol%250Areveals%2520a%2520training-time%2520scaling%2520effect%253A%2520longer%2520online%2520training%2520monotonically%250Aincreases%2520the%2520draft%2520model%2527s%2520average%2520accepted%2520length%252C%2520improving%2520speculative%250Aefficiency.%2520Empirically%252C%2520SpecVLM%2520achieves%2520additional%2520acceleration%252C%2520culminating%250Ain%25202.5--2.9x%2520end-to-end%2520speedups%2520within%25205%2520epochs%2520across%2520LLaVA%2520and%2520MMMU%252C%250Aconsistently%2520over%2520resolutions%2520and%2520task%2520difficulties%252C%2520while%2520preserving%2520the%250Atarget%2520model%2527s%2520output%2520distribution%2520%2528lossless%2520decoding%2529.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//github.com/haiduo/SpecVLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecVLM%3A%20Fast%20Speculative%20Decoding%20in%20Vision-Language%20Models&entry.906535625=Haiduo%20Huang%20and%20Fuwei%20Yang%20and%20Zhenhua%20Liu%20and%20Xuanwu%20Yin%20and%20Dong%20Li%20and%20Pengju%20Ren%20and%20Emad%20Barsoum&entry.1292438233=%20%20Speculative%20decoding%20is%20a%20powerful%20way%20to%20accelerate%20autoregressive%20large%0Alanguage%20models%20%28LLMs%29%2C%20but%20directly%20porting%20it%20to%20vision-language%20models%0A%28VLMs%29%20faces%20unique%20systems%20constraints%3A%20the%20prefill%20stage%20is%20dominated%20by%0Avisual%20tokens%20whose%20count%20scales%20with%20image%20resolution%20and%20video%20length%2C%0Ainflating%20both%20compute%20and%20memory%2C%20especially%20the%20key-value%20%28KV%29%20cache.%20We%0Astudy%20speculative%20decoding%20for%20VLMs%20and%20introduce%20SpecVLM%2C%20a%20practical%20system%0Athat%20%281%29%20establishes%20a%20strong%20EAGLE-2-style%20baseline%2C%20EagleVLM%2C%20delivering%0A1.5--2.3x%20end-to-end%20speedups%20over%20full%20autoregressive%20inference%2C%20and%20%282%29%0Afurther%20accelerates%20VLM%20inference%20with%20an%20elastic%20visual%20compressor%20that%0Aadaptively%20selects%20among%20pruning%2C%20pooling%2C%20convolution%2C%20and%20resampler%0Aprimitives%20to%20balance%20FLOPs/parameters%20and%20accuracy%20per%20input.%20To%20avoid%20costly%0Aoffline%20distillation%20corpora%2C%20we%20propose%20an%20online-logit%20distillation%20protocol%0Athat%20trains%20the%20draft%20model%20with%20on-the-fly%20teacher%20logits%20and%20penultimate%0Afeatures%20using%20a%20combined%20cross-entropy%20and%20Smooth%20L1%20objective%2C%20eliminating%0Astorage%20and%20preprocessing%20while%20remaining%20compute-efficient.%20This%20protocol%0Areveals%20a%20training-time%20scaling%20effect%3A%20longer%20online%20training%20monotonically%0Aincreases%20the%20draft%20model%27s%20average%20accepted%20length%2C%20improving%20speculative%0Aefficiency.%20Empirically%2C%20SpecVLM%20achieves%20additional%20acceleration%2C%20culminating%0Ain%202.5--2.9x%20end-to-end%20speedups%20within%205%20epochs%20across%20LLaVA%20and%20MMMU%2C%0Aconsistently%20over%20resolutions%20and%20task%20difficulties%2C%20while%20preserving%20the%0Atarget%20model%27s%20output%20distribution%20%28lossless%20decoding%29.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/haiduo/SpecVLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11815v1&entry.124074799=Read"},
{"title": "So-Fake: Benchmarking and Explaining Social Media Image Forgery\n  Detection", "author": "Zhenglin Huang and Tianxiao Li and Xiangtai Li and Haiquan Wen and Yiwei He and Jiangning Zhang and Hao Fei and Xi Yang and Xiaowei Huang and Bei Peng and Guangliang Cheng", "abstract": "  Recent advances in AI-powered generative models have enabled the creation of\nincreasingly realistic synthetic images, posing significant risks to\ninformation integrity and public trust on social media platforms. While robust\ndetection frameworks and diverse, large-scale datasets are essential to\nmitigate these risks, existing academic efforts remain limited in scope:\ncurrent datasets lack the diversity, scale, and realism required for social\nmedia contexts, while detection methods struggle with generalization to unseen\ngenerative technologies. To bridge this gap, we introduce So-Fake-Set, a\ncomprehensive social media-oriented dataset with over 2 million high-quality\nimages, diverse generative sources, and photorealistic imagery synthesized\nusing 35 state-of-the-art generative models. To rigorously evaluate\ncross-domain robustness, we establish a novel and large-scale (100K)\nout-of-domain benchmark (So-Fake-OOD) featuring synthetic imagery from\ncommercial models explicitly excluded from the training distribution, creating\na realistic testbed for evaluating real-world performance. Leveraging these\nresources, we present So-Fake-R1, an advanced vision-language framework that\nemploys reinforcement learning for highly accurate forgery detection, precise\nlocalization, and explainable inference through interpretable visual\nrationales. Extensive experiments show that So-Fake-R1 outperforms the\nsecond-best method, with a 1.3% gain in detection accuracy and a 4.5% increase\nin localization IoU. By integrating a scalable dataset, a challenging OOD\nbenchmark, and an advanced detection framework, this work establishes a new\nfoundation for social media-centric forgery detection research. The code,\nmodels, and datasets will be released publicly.\n", "link": "http://arxiv.org/abs/2505.18660v3", "date": "2025-09-15", "relevancy": 2.2243, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5618}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.56}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20So-Fake%3A%20Benchmarking%20and%20Explaining%20Social%20Media%20Image%20Forgery%0A%20%20Detection&body=Title%3A%20So-Fake%3A%20Benchmarking%20and%20Explaining%20Social%20Media%20Image%20Forgery%0A%20%20Detection%0AAuthor%3A%20Zhenglin%20Huang%20and%20Tianxiao%20Li%20and%20Xiangtai%20Li%20and%20Haiquan%20Wen%20and%20Yiwei%20He%20and%20Jiangning%20Zhang%20and%20Hao%20Fei%20and%20Xi%20Yang%20and%20Xiaowei%20Huang%20and%20Bei%20Peng%20and%20Guangliang%20Cheng%0AAbstract%3A%20%20%20Recent%20advances%20in%20AI-powered%20generative%20models%20have%20enabled%20the%20creation%20of%0Aincreasingly%20realistic%20synthetic%20images%2C%20posing%20significant%20risks%20to%0Ainformation%20integrity%20and%20public%20trust%20on%20social%20media%20platforms.%20While%20robust%0Adetection%20frameworks%20and%20diverse%2C%20large-scale%20datasets%20are%20essential%20to%0Amitigate%20these%20risks%2C%20existing%20academic%20efforts%20remain%20limited%20in%20scope%3A%0Acurrent%20datasets%20lack%20the%20diversity%2C%20scale%2C%20and%20realism%20required%20for%20social%0Amedia%20contexts%2C%20while%20detection%20methods%20struggle%20with%20generalization%20to%20unseen%0Agenerative%20technologies.%20To%20bridge%20this%20gap%2C%20we%20introduce%20So-Fake-Set%2C%20a%0Acomprehensive%20social%20media-oriented%20dataset%20with%20over%202%20million%20high-quality%0Aimages%2C%20diverse%20generative%20sources%2C%20and%20photorealistic%20imagery%20synthesized%0Ausing%2035%20state-of-the-art%20generative%20models.%20To%20rigorously%20evaluate%0Across-domain%20robustness%2C%20we%20establish%20a%20novel%20and%20large-scale%20%28100K%29%0Aout-of-domain%20benchmark%20%28So-Fake-OOD%29%20featuring%20synthetic%20imagery%20from%0Acommercial%20models%20explicitly%20excluded%20from%20the%20training%20distribution%2C%20creating%0Aa%20realistic%20testbed%20for%20evaluating%20real-world%20performance.%20Leveraging%20these%0Aresources%2C%20we%20present%20So-Fake-R1%2C%20an%20advanced%20vision-language%20framework%20that%0Aemploys%20reinforcement%20learning%20for%20highly%20accurate%20forgery%20detection%2C%20precise%0Alocalization%2C%20and%20explainable%20inference%20through%20interpretable%20visual%0Arationales.%20Extensive%20experiments%20show%20that%20So-Fake-R1%20outperforms%20the%0Asecond-best%20method%2C%20with%20a%201.3%25%20gain%20in%20detection%20accuracy%20and%20a%204.5%25%20increase%0Ain%20localization%20IoU.%20By%20integrating%20a%20scalable%20dataset%2C%20a%20challenging%20OOD%0Abenchmark%2C%20and%20an%20advanced%20detection%20framework%2C%20this%20work%20establishes%20a%20new%0Afoundation%20for%20social%20media-centric%20forgery%20detection%20research.%20The%20code%2C%0Amodels%2C%20and%20datasets%20will%20be%20released%20publicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18660v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSo-Fake%253A%2520Benchmarking%2520and%2520Explaining%2520Social%2520Media%2520Image%2520Forgery%250A%2520%2520Detection%26entry.906535625%3DZhenglin%2520Huang%2520and%2520Tianxiao%2520Li%2520and%2520Xiangtai%2520Li%2520and%2520Haiquan%2520Wen%2520and%2520Yiwei%2520He%2520and%2520Jiangning%2520Zhang%2520and%2520Hao%2520Fei%2520and%2520Xi%2520Yang%2520and%2520Xiaowei%2520Huang%2520and%2520Bei%2520Peng%2520and%2520Guangliang%2520Cheng%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520AI-powered%2520generative%2520models%2520have%2520enabled%2520the%2520creation%2520of%250Aincreasingly%2520realistic%2520synthetic%2520images%252C%2520posing%2520significant%2520risks%2520to%250Ainformation%2520integrity%2520and%2520public%2520trust%2520on%2520social%2520media%2520platforms.%2520While%2520robust%250Adetection%2520frameworks%2520and%2520diverse%252C%2520large-scale%2520datasets%2520are%2520essential%2520to%250Amitigate%2520these%2520risks%252C%2520existing%2520academic%2520efforts%2520remain%2520limited%2520in%2520scope%253A%250Acurrent%2520datasets%2520lack%2520the%2520diversity%252C%2520scale%252C%2520and%2520realism%2520required%2520for%2520social%250Amedia%2520contexts%252C%2520while%2520detection%2520methods%2520struggle%2520with%2520generalization%2520to%2520unseen%250Agenerative%2520technologies.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520So-Fake-Set%252C%2520a%250Acomprehensive%2520social%2520media-oriented%2520dataset%2520with%2520over%25202%2520million%2520high-quality%250Aimages%252C%2520diverse%2520generative%2520sources%252C%2520and%2520photorealistic%2520imagery%2520synthesized%250Ausing%252035%2520state-of-the-art%2520generative%2520models.%2520To%2520rigorously%2520evaluate%250Across-domain%2520robustness%252C%2520we%2520establish%2520a%2520novel%2520and%2520large-scale%2520%2528100K%2529%250Aout-of-domain%2520benchmark%2520%2528So-Fake-OOD%2529%2520featuring%2520synthetic%2520imagery%2520from%250Acommercial%2520models%2520explicitly%2520excluded%2520from%2520the%2520training%2520distribution%252C%2520creating%250Aa%2520realistic%2520testbed%2520for%2520evaluating%2520real-world%2520performance.%2520Leveraging%2520these%250Aresources%252C%2520we%2520present%2520So-Fake-R1%252C%2520an%2520advanced%2520vision-language%2520framework%2520that%250Aemploys%2520reinforcement%2520learning%2520for%2520highly%2520accurate%2520forgery%2520detection%252C%2520precise%250Alocalization%252C%2520and%2520explainable%2520inference%2520through%2520interpretable%2520visual%250Arationales.%2520Extensive%2520experiments%2520show%2520that%2520So-Fake-R1%2520outperforms%2520the%250Asecond-best%2520method%252C%2520with%2520a%25201.3%2525%2520gain%2520in%2520detection%2520accuracy%2520and%2520a%25204.5%2525%2520increase%250Ain%2520localization%2520IoU.%2520By%2520integrating%2520a%2520scalable%2520dataset%252C%2520a%2520challenging%2520OOD%250Abenchmark%252C%2520and%2520an%2520advanced%2520detection%2520framework%252C%2520this%2520work%2520establishes%2520a%2520new%250Afoundation%2520for%2520social%2520media-centric%2520forgery%2520detection%2520research.%2520The%2520code%252C%250Amodels%252C%2520and%2520datasets%2520will%2520be%2520released%2520publicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18660v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=So-Fake%3A%20Benchmarking%20and%20Explaining%20Social%20Media%20Image%20Forgery%0A%20%20Detection&entry.906535625=Zhenglin%20Huang%20and%20Tianxiao%20Li%20and%20Xiangtai%20Li%20and%20Haiquan%20Wen%20and%20Yiwei%20He%20and%20Jiangning%20Zhang%20and%20Hao%20Fei%20and%20Xi%20Yang%20and%20Xiaowei%20Huang%20and%20Bei%20Peng%20and%20Guangliang%20Cheng&entry.1292438233=%20%20Recent%20advances%20in%20AI-powered%20generative%20models%20have%20enabled%20the%20creation%20of%0Aincreasingly%20realistic%20synthetic%20images%2C%20posing%20significant%20risks%20to%0Ainformation%20integrity%20and%20public%20trust%20on%20social%20media%20platforms.%20While%20robust%0Adetection%20frameworks%20and%20diverse%2C%20large-scale%20datasets%20are%20essential%20to%0Amitigate%20these%20risks%2C%20existing%20academic%20efforts%20remain%20limited%20in%20scope%3A%0Acurrent%20datasets%20lack%20the%20diversity%2C%20scale%2C%20and%20realism%20required%20for%20social%0Amedia%20contexts%2C%20while%20detection%20methods%20struggle%20with%20generalization%20to%20unseen%0Agenerative%20technologies.%20To%20bridge%20this%20gap%2C%20we%20introduce%20So-Fake-Set%2C%20a%0Acomprehensive%20social%20media-oriented%20dataset%20with%20over%202%20million%20high-quality%0Aimages%2C%20diverse%20generative%20sources%2C%20and%20photorealistic%20imagery%20synthesized%0Ausing%2035%20state-of-the-art%20generative%20models.%20To%20rigorously%20evaluate%0Across-domain%20robustness%2C%20we%20establish%20a%20novel%20and%20large-scale%20%28100K%29%0Aout-of-domain%20benchmark%20%28So-Fake-OOD%29%20featuring%20synthetic%20imagery%20from%0Acommercial%20models%20explicitly%20excluded%20from%20the%20training%20distribution%2C%20creating%0Aa%20realistic%20testbed%20for%20evaluating%20real-world%20performance.%20Leveraging%20these%0Aresources%2C%20we%20present%20So-Fake-R1%2C%20an%20advanced%20vision-language%20framework%20that%0Aemploys%20reinforcement%20learning%20for%20highly%20accurate%20forgery%20detection%2C%20precise%0Alocalization%2C%20and%20explainable%20inference%20through%20interpretable%20visual%0Arationales.%20Extensive%20experiments%20show%20that%20So-Fake-R1%20outperforms%20the%0Asecond-best%20method%2C%20with%20a%201.3%25%20gain%20in%20detection%20accuracy%20and%20a%204.5%25%20increase%0Ain%20localization%20IoU.%20By%20integrating%20a%20scalable%20dataset%2C%20a%20challenging%20OOD%0Abenchmark%2C%20and%20an%20advanced%20detection%20framework%2C%20this%20work%20establishes%20a%20new%0Afoundation%20for%20social%20media-centric%20forgery%20detection%20research.%20The%20code%2C%0Amodels%2C%20and%20datasets%20will%20be%20released%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18660v3&entry.124074799=Read"},
{"title": "Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for\n  Zero-shot Generalization", "author": "Diogo Mendon\u00e7a and Tiago Barros and Cristiano Premebida and Urbano J. Nunes", "abstract": "  Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to\noperate reliably in dynamic environments. MOT ensures consistent object\nidentity assignment and precise spatial delineation. Recent advances in\nfoundation models, such as SAM2, have demonstrated strong zero-shot\ngeneralization for video segmentation, but their direct application to MOTS\n(MOT+Segmentation) remains limited by insufficient identity management and\nmemory efficiency. This work introduces Seg2Track-SAM2, a framework that\nintegrates pre-trained object detectors with SAM2 and a novel Seg2Track module\nto address track initialization, track management, and reinforcement. The\nproposed approach requires no fine-tuning and remains detector-agnostic.\nExperimental results on KITTI MOT and KITTI MOTS benchmarks show that\nSeg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth\noverall in both car and pedestrian classes on KITTI MOTS, while establishing a\nnew benchmark in association accuracy (AssA). Furthermore, a sliding-window\nmemory strategy reduces memory usage by up to 75% with negligible performance\ndegradation, supporting deployment under resource constraints. These results\nconfirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot\ntracking, enhanced identity preservation, and efficient memory utilization. The\ncode is available at https://github.com/hcmr-lab/Seg2Track-SAM2\n", "link": "http://arxiv.org/abs/2509.11772v1", "date": "2025-09-15", "relevancy": 2.2209, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5769}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5506}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seg2Track-SAM2%3A%20SAM2-based%20Multi-object%20Tracking%20and%20Segmentation%20for%0A%20%20Zero-shot%20Generalization&body=Title%3A%20Seg2Track-SAM2%3A%20SAM2-based%20Multi-object%20Tracking%20and%20Segmentation%20for%0A%20%20Zero-shot%20Generalization%0AAuthor%3A%20Diogo%20Mendon%C3%A7a%20and%20Tiago%20Barros%20and%20Cristiano%20Premebida%20and%20Urbano%20J.%20Nunes%0AAbstract%3A%20%20%20Autonomous%20systems%20require%20robust%20Multi-Object%20Tracking%20%28MOT%29%20capabilities%20to%0Aoperate%20reliably%20in%20dynamic%20environments.%20MOT%20ensures%20consistent%20object%0Aidentity%20assignment%20and%20precise%20spatial%20delineation.%20Recent%20advances%20in%0Afoundation%20models%2C%20such%20as%20SAM2%2C%20have%20demonstrated%20strong%20zero-shot%0Ageneralization%20for%20video%20segmentation%2C%20but%20their%20direct%20application%20to%20MOTS%0A%28MOT%2BSegmentation%29%20remains%20limited%20by%20insufficient%20identity%20management%20and%0Amemory%20efficiency.%20This%20work%20introduces%20Seg2Track-SAM2%2C%20a%20framework%20that%0Aintegrates%20pre-trained%20object%20detectors%20with%20SAM2%20and%20a%20novel%20Seg2Track%20module%0Ato%20address%20track%20initialization%2C%20track%20management%2C%20and%20reinforcement.%20The%0Aproposed%20approach%20requires%20no%20fine-tuning%20and%20remains%20detector-agnostic.%0AExperimental%20results%20on%20KITTI%20MOT%20and%20KITTI%20MOTS%20benchmarks%20show%20that%0ASeg2Track-SAM2%20achieves%20state-of-the-art%20%28SOTA%29%20performance%2C%20ranking%20fourth%0Aoverall%20in%20both%20car%20and%20pedestrian%20classes%20on%20KITTI%20MOTS%2C%20while%20establishing%20a%0Anew%20benchmark%20in%20association%20accuracy%20%28AssA%29.%20Furthermore%2C%20a%20sliding-window%0Amemory%20strategy%20reduces%20memory%20usage%20by%20up%20to%2075%25%20with%20negligible%20performance%0Adegradation%2C%20supporting%20deployment%20under%20resource%20constraints.%20These%20results%0Aconfirm%20that%20Seg2Track-SAM2%20advances%20MOTS%20by%20combining%20robust%20zero-shot%0Atracking%2C%20enhanced%20identity%20preservation%2C%20and%20efficient%20memory%20utilization.%20The%0Acode%20is%20available%20at%20https%3A//github.com/hcmr-lab/Seg2Track-SAM2%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeg2Track-SAM2%253A%2520SAM2-based%2520Multi-object%2520Tracking%2520and%2520Segmentation%2520for%250A%2520%2520Zero-shot%2520Generalization%26entry.906535625%3DDiogo%2520Mendon%25C3%25A7a%2520and%2520Tiago%2520Barros%2520and%2520Cristiano%2520Premebida%2520and%2520Urbano%2520J.%2520Nunes%26entry.1292438233%3D%2520%2520Autonomous%2520systems%2520require%2520robust%2520Multi-Object%2520Tracking%2520%2528MOT%2529%2520capabilities%2520to%250Aoperate%2520reliably%2520in%2520dynamic%2520environments.%2520MOT%2520ensures%2520consistent%2520object%250Aidentity%2520assignment%2520and%2520precise%2520spatial%2520delineation.%2520Recent%2520advances%2520in%250Afoundation%2520models%252C%2520such%2520as%2520SAM2%252C%2520have%2520demonstrated%2520strong%2520zero-shot%250Ageneralization%2520for%2520video%2520segmentation%252C%2520but%2520their%2520direct%2520application%2520to%2520MOTS%250A%2528MOT%252BSegmentation%2529%2520remains%2520limited%2520by%2520insufficient%2520identity%2520management%2520and%250Amemory%2520efficiency.%2520This%2520work%2520introduces%2520Seg2Track-SAM2%252C%2520a%2520framework%2520that%250Aintegrates%2520pre-trained%2520object%2520detectors%2520with%2520SAM2%2520and%2520a%2520novel%2520Seg2Track%2520module%250Ato%2520address%2520track%2520initialization%252C%2520track%2520management%252C%2520and%2520reinforcement.%2520The%250Aproposed%2520approach%2520requires%2520no%2520fine-tuning%2520and%2520remains%2520detector-agnostic.%250AExperimental%2520results%2520on%2520KITTI%2520MOT%2520and%2520KITTI%2520MOTS%2520benchmarks%2520show%2520that%250ASeg2Track-SAM2%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%252C%2520ranking%2520fourth%250Aoverall%2520in%2520both%2520car%2520and%2520pedestrian%2520classes%2520on%2520KITTI%2520MOTS%252C%2520while%2520establishing%2520a%250Anew%2520benchmark%2520in%2520association%2520accuracy%2520%2528AssA%2529.%2520Furthermore%252C%2520a%2520sliding-window%250Amemory%2520strategy%2520reduces%2520memory%2520usage%2520by%2520up%2520to%252075%2525%2520with%2520negligible%2520performance%250Adegradation%252C%2520supporting%2520deployment%2520under%2520resource%2520constraints.%2520These%2520results%250Aconfirm%2520that%2520Seg2Track-SAM2%2520advances%2520MOTS%2520by%2520combining%2520robust%2520zero-shot%250Atracking%252C%2520enhanced%2520identity%2520preservation%252C%2520and%2520efficient%2520memory%2520utilization.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/hcmr-lab/Seg2Track-SAM2%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seg2Track-SAM2%3A%20SAM2-based%20Multi-object%20Tracking%20and%20Segmentation%20for%0A%20%20Zero-shot%20Generalization&entry.906535625=Diogo%20Mendon%C3%A7a%20and%20Tiago%20Barros%20and%20Cristiano%20Premebida%20and%20Urbano%20J.%20Nunes&entry.1292438233=%20%20Autonomous%20systems%20require%20robust%20Multi-Object%20Tracking%20%28MOT%29%20capabilities%20to%0Aoperate%20reliably%20in%20dynamic%20environments.%20MOT%20ensures%20consistent%20object%0Aidentity%20assignment%20and%20precise%20spatial%20delineation.%20Recent%20advances%20in%0Afoundation%20models%2C%20such%20as%20SAM2%2C%20have%20demonstrated%20strong%20zero-shot%0Ageneralization%20for%20video%20segmentation%2C%20but%20their%20direct%20application%20to%20MOTS%0A%28MOT%2BSegmentation%29%20remains%20limited%20by%20insufficient%20identity%20management%20and%0Amemory%20efficiency.%20This%20work%20introduces%20Seg2Track-SAM2%2C%20a%20framework%20that%0Aintegrates%20pre-trained%20object%20detectors%20with%20SAM2%20and%20a%20novel%20Seg2Track%20module%0Ato%20address%20track%20initialization%2C%20track%20management%2C%20and%20reinforcement.%20The%0Aproposed%20approach%20requires%20no%20fine-tuning%20and%20remains%20detector-agnostic.%0AExperimental%20results%20on%20KITTI%20MOT%20and%20KITTI%20MOTS%20benchmarks%20show%20that%0ASeg2Track-SAM2%20achieves%20state-of-the-art%20%28SOTA%29%20performance%2C%20ranking%20fourth%0Aoverall%20in%20both%20car%20and%20pedestrian%20classes%20on%20KITTI%20MOTS%2C%20while%20establishing%20a%0Anew%20benchmark%20in%20association%20accuracy%20%28AssA%29.%20Furthermore%2C%20a%20sliding-window%0Amemory%20strategy%20reduces%20memory%20usage%20by%20up%20to%2075%25%20with%20negligible%20performance%0Adegradation%2C%20supporting%20deployment%20under%20resource%20constraints.%20These%20results%0Aconfirm%20that%20Seg2Track-SAM2%20advances%20MOTS%20by%20combining%20robust%20zero-shot%0Atracking%2C%20enhanced%20identity%20preservation%2C%20and%20efficient%20memory%20utilization.%20The%0Acode%20is%20available%20at%20https%3A//github.com/hcmr-lab/Seg2Track-SAM2%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11772v1&entry.124074799=Read"},
{"title": "CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning", "author": "Zhou-Peng Shou and Zhi-Qiang You and Fang Wang and Hai-Bo Liu", "abstract": "  Targeting the issues of \"shortcuts\" and insufficient contextual understanding\nin complex cross-modal reasoning of multimodal large models, this paper\nproposes a zero-shot multimodal reasoning component guided by human-like\ncognitive strategies centered on an \"intent sketch\". The component comprises a\nplug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and\nStrategy Selector-that explicitly constructs a \"understand-plan-select\"\ncognitive process. By generating and filtering \"intent sketch\" strategies to\nguide the final reasoning, it requires no parameter fine-tuning and achieves\ncross-model transfer solely through in-context engineering.\nInformation-theoretic analysis shows that this process can reduce conditional\nentropy and improve information utilization efficiency, thereby suppressing\nunintended shortcut reasoning. Experiments on IntentBench, WorldSense, and\nDaily-Omni validate the method's generality and robust gains; compared with\ntheir respective baselines, the complete \"three-module\" scheme yields\nconsistent improvements across different reasoning engines and pipeline\ncombinations, with gains up to approximately 9.51 percentage points,\ndemonstrating the practical value and portability of the \"intent sketch\"\nreasoning component in zero-shot scenarios.\n", "link": "http://arxiv.org/abs/2509.06641v3", "date": "2025-09-15", "relevancy": 2.1974, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CogGuide%3A%20Human-Like%20Guidance%20for%20Zero-Shot%20Omni-Modal%20Reasoning&body=Title%3A%20CogGuide%3A%20Human-Like%20Guidance%20for%20Zero-Shot%20Omni-Modal%20Reasoning%0AAuthor%3A%20Zhou-Peng%20Shou%20and%20Zhi-Qiang%20You%20and%20Fang%20Wang%20and%20Hai-Bo%20Liu%0AAbstract%3A%20%20%20Targeting%20the%20issues%20of%20%22shortcuts%22%20and%20insufficient%20contextual%20understanding%0Ain%20complex%20cross-modal%20reasoning%20of%20multimodal%20large%20models%2C%20this%20paper%0Aproposes%20a%20zero-shot%20multimodal%20reasoning%20component%20guided%20by%20human-like%0Acognitive%20strategies%20centered%20on%20an%20%22intent%20sketch%22.%20The%20component%20comprises%20a%0Aplug-and-play%20three-module%20pipeline-Intent%20Perceiver%2C%20Strategy%20Generator%2C%20and%0AStrategy%20Selector-that%20explicitly%20constructs%20a%20%22understand-plan-select%22%0Acognitive%20process.%20By%20generating%20and%20filtering%20%22intent%20sketch%22%20strategies%20to%0Aguide%20the%20final%20reasoning%2C%20it%20requires%20no%20parameter%20fine-tuning%20and%20achieves%0Across-model%20transfer%20solely%20through%20in-context%20engineering.%0AInformation-theoretic%20analysis%20shows%20that%20this%20process%20can%20reduce%20conditional%0Aentropy%20and%20improve%20information%20utilization%20efficiency%2C%20thereby%20suppressing%0Aunintended%20shortcut%20reasoning.%20Experiments%20on%20IntentBench%2C%20WorldSense%2C%20and%0ADaily-Omni%20validate%20the%20method%27s%20generality%20and%20robust%20gains%3B%20compared%20with%0Atheir%20respective%20baselines%2C%20the%20complete%20%22three-module%22%20scheme%20yields%0Aconsistent%20improvements%20across%20different%20reasoning%20engines%20and%20pipeline%0Acombinations%2C%20with%20gains%20up%20to%20approximately%209.51%20percentage%20points%2C%0Ademonstrating%20the%20practical%20value%20and%20portability%20of%20the%20%22intent%20sketch%22%0Areasoning%20component%20in%20zero-shot%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06641v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCogGuide%253A%2520Human-Like%2520Guidance%2520for%2520Zero-Shot%2520Omni-Modal%2520Reasoning%26entry.906535625%3DZhou-Peng%2520Shou%2520and%2520Zhi-Qiang%2520You%2520and%2520Fang%2520Wang%2520and%2520Hai-Bo%2520Liu%26entry.1292438233%3D%2520%2520Targeting%2520the%2520issues%2520of%2520%2522shortcuts%2522%2520and%2520insufficient%2520contextual%2520understanding%250Ain%2520complex%2520cross-modal%2520reasoning%2520of%2520multimodal%2520large%2520models%252C%2520this%2520paper%250Aproposes%2520a%2520zero-shot%2520multimodal%2520reasoning%2520component%2520guided%2520by%2520human-like%250Acognitive%2520strategies%2520centered%2520on%2520an%2520%2522intent%2520sketch%2522.%2520The%2520component%2520comprises%2520a%250Aplug-and-play%2520three-module%2520pipeline-Intent%2520Perceiver%252C%2520Strategy%2520Generator%252C%2520and%250AStrategy%2520Selector-that%2520explicitly%2520constructs%2520a%2520%2522understand-plan-select%2522%250Acognitive%2520process.%2520By%2520generating%2520and%2520filtering%2520%2522intent%2520sketch%2522%2520strategies%2520to%250Aguide%2520the%2520final%2520reasoning%252C%2520it%2520requires%2520no%2520parameter%2520fine-tuning%2520and%2520achieves%250Across-model%2520transfer%2520solely%2520through%2520in-context%2520engineering.%250AInformation-theoretic%2520analysis%2520shows%2520that%2520this%2520process%2520can%2520reduce%2520conditional%250Aentropy%2520and%2520improve%2520information%2520utilization%2520efficiency%252C%2520thereby%2520suppressing%250Aunintended%2520shortcut%2520reasoning.%2520Experiments%2520on%2520IntentBench%252C%2520WorldSense%252C%2520and%250ADaily-Omni%2520validate%2520the%2520method%2527s%2520generality%2520and%2520robust%2520gains%253B%2520compared%2520with%250Atheir%2520respective%2520baselines%252C%2520the%2520complete%2520%2522three-module%2522%2520scheme%2520yields%250Aconsistent%2520improvements%2520across%2520different%2520reasoning%2520engines%2520and%2520pipeline%250Acombinations%252C%2520with%2520gains%2520up%2520to%2520approximately%25209.51%2520percentage%2520points%252C%250Ademonstrating%2520the%2520practical%2520value%2520and%2520portability%2520of%2520the%2520%2522intent%2520sketch%2522%250Areasoning%2520component%2520in%2520zero-shot%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06641v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CogGuide%3A%20Human-Like%20Guidance%20for%20Zero-Shot%20Omni-Modal%20Reasoning&entry.906535625=Zhou-Peng%20Shou%20and%20Zhi-Qiang%20You%20and%20Fang%20Wang%20and%20Hai-Bo%20Liu&entry.1292438233=%20%20Targeting%20the%20issues%20of%20%22shortcuts%22%20and%20insufficient%20contextual%20understanding%0Ain%20complex%20cross-modal%20reasoning%20of%20multimodal%20large%20models%2C%20this%20paper%0Aproposes%20a%20zero-shot%20multimodal%20reasoning%20component%20guided%20by%20human-like%0Acognitive%20strategies%20centered%20on%20an%20%22intent%20sketch%22.%20The%20component%20comprises%20a%0Aplug-and-play%20three-module%20pipeline-Intent%20Perceiver%2C%20Strategy%20Generator%2C%20and%0AStrategy%20Selector-that%20explicitly%20constructs%20a%20%22understand-plan-select%22%0Acognitive%20process.%20By%20generating%20and%20filtering%20%22intent%20sketch%22%20strategies%20to%0Aguide%20the%20final%20reasoning%2C%20it%20requires%20no%20parameter%20fine-tuning%20and%20achieves%0Across-model%20transfer%20solely%20through%20in-context%20engineering.%0AInformation-theoretic%20analysis%20shows%20that%20this%20process%20can%20reduce%20conditional%0Aentropy%20and%20improve%20information%20utilization%20efficiency%2C%20thereby%20suppressing%0Aunintended%20shortcut%20reasoning.%20Experiments%20on%20IntentBench%2C%20WorldSense%2C%20and%0ADaily-Omni%20validate%20the%20method%27s%20generality%20and%20robust%20gains%3B%20compared%20with%0Atheir%20respective%20baselines%2C%20the%20complete%20%22three-module%22%20scheme%20yields%0Aconsistent%20improvements%20across%20different%20reasoning%20engines%20and%20pipeline%0Acombinations%2C%20with%20gains%20up%20to%20approximately%209.51%20percentage%20points%2C%0Ademonstrating%20the%20practical%20value%20and%20portability%20of%20the%20%22intent%20sketch%22%0Areasoning%20component%20in%20zero-shot%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06641v3&entry.124074799=Read"},
{"title": "Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and\n  Semantic Graph Integration", "author": "Shaoguang Wang and Ziyang Chen and Yijie Xu and Weiyu Guo and Hui Xiong", "abstract": "  The practical application of Multimodal Large Language Models (MLLMs) to\nVideo Question Answering (Video-QA) is severely hindered by the high token cost\nof processing numerous video frames. While increasing the number of sampled\nframes is a common strategy, we observe a \"less is more\" phenomenon where\nexcessive frames can paradoxically degrade performance due to context dilution.\nConcurrently, state-of-the-art keyframe selection methods, while effective,\nstill yield significant temporal redundancy, which we term 'visual echoes'. To\naddress these dual challenges, we propose Adaptive Frame-Pruning (AFP), a novel\npost-processing method that intelligently prunes the selected keyframes. AFP\nemploys an adaptive hierarchical clustering algorithm on a fused ResNet-50 and\nCLIP feature space to identify and merge these echoes into single\nrepresentatives. To compensate for information loss, we then introduce a\nlightweight, text-based semantic graph that provides critical context with\nminimal token overhead. Conducting extensive experiments on the LongVideoBench\nand VideoMME benchmarks across multiple leading MLLMs, our full approach\ndemonstrates a drastic reduction in required frames by up to 86.9% and total\ninput tokens by up to 83.2%. Crucially, by providing a concise, high-quality\nset of frames, our method not only enhances efficiency but often improves\naccuracy over baselines that use more frames. The code will be released upon\npublication.\n", "link": "http://arxiv.org/abs/2508.03337v6", "date": "2025-09-15", "relevancy": 2.1963, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.564}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5479}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20More%3A%20Token-Efficient%20Video-QA%20via%20Adaptive%20Frame-Pruning%20and%0A%20%20Semantic%20Graph%20Integration&body=Title%3A%20Less%20is%20More%3A%20Token-Efficient%20Video-QA%20via%20Adaptive%20Frame-Pruning%20and%0A%20%20Semantic%20Graph%20Integration%0AAuthor%3A%20Shaoguang%20Wang%20and%20Ziyang%20Chen%20and%20Yijie%20Xu%20and%20Weiyu%20Guo%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20The%20practical%20application%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%0AVideo%20Question%20Answering%20%28Video-QA%29%20is%20severely%20hindered%20by%20the%20high%20token%20cost%0Aof%20processing%20numerous%20video%20frames.%20While%20increasing%20the%20number%20of%20sampled%0Aframes%20is%20a%20common%20strategy%2C%20we%20observe%20a%20%22less%20is%20more%22%20phenomenon%20where%0Aexcessive%20frames%20can%20paradoxically%20degrade%20performance%20due%20to%20context%20dilution.%0AConcurrently%2C%20state-of-the-art%20keyframe%20selection%20methods%2C%20while%20effective%2C%0Astill%20yield%20significant%20temporal%20redundancy%2C%20which%20we%20term%20%27visual%20echoes%27.%20To%0Aaddress%20these%20dual%20challenges%2C%20we%20propose%20Adaptive%20Frame-Pruning%20%28AFP%29%2C%20a%20novel%0Apost-processing%20method%20that%20intelligently%20prunes%20the%20selected%20keyframes.%20AFP%0Aemploys%20an%20adaptive%20hierarchical%20clustering%20algorithm%20on%20a%20fused%20ResNet-50%20and%0ACLIP%20feature%20space%20to%20identify%20and%20merge%20these%20echoes%20into%20single%0Arepresentatives.%20To%20compensate%20for%20information%20loss%2C%20we%20then%20introduce%20a%0Alightweight%2C%20text-based%20semantic%20graph%20that%20provides%20critical%20context%20with%0Aminimal%20token%20overhead.%20Conducting%20extensive%20experiments%20on%20the%20LongVideoBench%0Aand%20VideoMME%20benchmarks%20across%20multiple%20leading%20MLLMs%2C%20our%20full%20approach%0Ademonstrates%20a%20drastic%20reduction%20in%20required%20frames%20by%20up%20to%2086.9%25%20and%20total%0Ainput%20tokens%20by%20up%20to%2083.2%25.%20Crucially%2C%20by%20providing%20a%20concise%2C%20high-quality%0Aset%20of%20frames%2C%20our%20method%20not%20only%20enhances%20efficiency%20but%20often%20improves%0Aaccuracy%20over%20baselines%20that%20use%20more%20frames.%20The%20code%20will%20be%20released%20upon%0Apublication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03337v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520More%253A%2520Token-Efficient%2520Video-QA%2520via%2520Adaptive%2520Frame-Pruning%2520and%250A%2520%2520Semantic%2520Graph%2520Integration%26entry.906535625%3DShaoguang%2520Wang%2520and%2520Ziyang%2520Chen%2520and%2520Yijie%2520Xu%2520and%2520Weiyu%2520Guo%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520The%2520practical%2520application%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520to%250AVideo%2520Question%2520Answering%2520%2528Video-QA%2529%2520is%2520severely%2520hindered%2520by%2520the%2520high%2520token%2520cost%250Aof%2520processing%2520numerous%2520video%2520frames.%2520While%2520increasing%2520the%2520number%2520of%2520sampled%250Aframes%2520is%2520a%2520common%2520strategy%252C%2520we%2520observe%2520a%2520%2522less%2520is%2520more%2522%2520phenomenon%2520where%250Aexcessive%2520frames%2520can%2520paradoxically%2520degrade%2520performance%2520due%2520to%2520context%2520dilution.%250AConcurrently%252C%2520state-of-the-art%2520keyframe%2520selection%2520methods%252C%2520while%2520effective%252C%250Astill%2520yield%2520significant%2520temporal%2520redundancy%252C%2520which%2520we%2520term%2520%2527visual%2520echoes%2527.%2520To%250Aaddress%2520these%2520dual%2520challenges%252C%2520we%2520propose%2520Adaptive%2520Frame-Pruning%2520%2528AFP%2529%252C%2520a%2520novel%250Apost-processing%2520method%2520that%2520intelligently%2520prunes%2520the%2520selected%2520keyframes.%2520AFP%250Aemploys%2520an%2520adaptive%2520hierarchical%2520clustering%2520algorithm%2520on%2520a%2520fused%2520ResNet-50%2520and%250ACLIP%2520feature%2520space%2520to%2520identify%2520and%2520merge%2520these%2520echoes%2520into%2520single%250Arepresentatives.%2520To%2520compensate%2520for%2520information%2520loss%252C%2520we%2520then%2520introduce%2520a%250Alightweight%252C%2520text-based%2520semantic%2520graph%2520that%2520provides%2520critical%2520context%2520with%250Aminimal%2520token%2520overhead.%2520Conducting%2520extensive%2520experiments%2520on%2520the%2520LongVideoBench%250Aand%2520VideoMME%2520benchmarks%2520across%2520multiple%2520leading%2520MLLMs%252C%2520our%2520full%2520approach%250Ademonstrates%2520a%2520drastic%2520reduction%2520in%2520required%2520frames%2520by%2520up%2520to%252086.9%2525%2520and%2520total%250Ainput%2520tokens%2520by%2520up%2520to%252083.2%2525.%2520Crucially%252C%2520by%2520providing%2520a%2520concise%252C%2520high-quality%250Aset%2520of%2520frames%252C%2520our%2520method%2520not%2520only%2520enhances%2520efficiency%2520but%2520often%2520improves%250Aaccuracy%2520over%2520baselines%2520that%2520use%2520more%2520frames.%2520The%2520code%2520will%2520be%2520released%2520upon%250Apublication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03337v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20More%3A%20Token-Efficient%20Video-QA%20via%20Adaptive%20Frame-Pruning%20and%0A%20%20Semantic%20Graph%20Integration&entry.906535625=Shaoguang%20Wang%20and%20Ziyang%20Chen%20and%20Yijie%20Xu%20and%20Weiyu%20Guo%20and%20Hui%20Xiong&entry.1292438233=%20%20The%20practical%20application%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%0AVideo%20Question%20Answering%20%28Video-QA%29%20is%20severely%20hindered%20by%20the%20high%20token%20cost%0Aof%20processing%20numerous%20video%20frames.%20While%20increasing%20the%20number%20of%20sampled%0Aframes%20is%20a%20common%20strategy%2C%20we%20observe%20a%20%22less%20is%20more%22%20phenomenon%20where%0Aexcessive%20frames%20can%20paradoxically%20degrade%20performance%20due%20to%20context%20dilution.%0AConcurrently%2C%20state-of-the-art%20keyframe%20selection%20methods%2C%20while%20effective%2C%0Astill%20yield%20significant%20temporal%20redundancy%2C%20which%20we%20term%20%27visual%20echoes%27.%20To%0Aaddress%20these%20dual%20challenges%2C%20we%20propose%20Adaptive%20Frame-Pruning%20%28AFP%29%2C%20a%20novel%0Apost-processing%20method%20that%20intelligently%20prunes%20the%20selected%20keyframes.%20AFP%0Aemploys%20an%20adaptive%20hierarchical%20clustering%20algorithm%20on%20a%20fused%20ResNet-50%20and%0ACLIP%20feature%20space%20to%20identify%20and%20merge%20these%20echoes%20into%20single%0Arepresentatives.%20To%20compensate%20for%20information%20loss%2C%20we%20then%20introduce%20a%0Alightweight%2C%20text-based%20semantic%20graph%20that%20provides%20critical%20context%20with%0Aminimal%20token%20overhead.%20Conducting%20extensive%20experiments%20on%20the%20LongVideoBench%0Aand%20VideoMME%20benchmarks%20across%20multiple%20leading%20MLLMs%2C%20our%20full%20approach%0Ademonstrates%20a%20drastic%20reduction%20in%20required%20frames%20by%20up%20to%2086.9%25%20and%20total%0Ainput%20tokens%20by%20up%20to%2083.2%25.%20Crucially%2C%20by%20providing%20a%20concise%2C%20high-quality%0Aset%20of%20frames%2C%20our%20method%20not%20only%20enhances%20efficiency%20but%20often%20improves%0Aaccuracy%20over%20baselines%20that%20use%20more%20frames.%20The%20code%20will%20be%20released%20upon%0Apublication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03337v6&entry.124074799=Read"},
{"title": "CVVNet: A Cross-Vertical-View Network for Gait Recognition", "author": "Xiangru Li and Wei Song and Yingda Huang and Wei Meng and Le Chang and Hongyang Li", "abstract": "  Gait recognition enables contact-free, long-range person identification that\nis robust to clothing variations and non-cooperative scenarios. While existing\nmethods perform well in controlled indoor environments, they struggle with\ncross-vertical view scenarios, where surveillance angles vary significantly in\nelevation. Our experiments show up to 60\\% accuracy degradation in low-to-high\nvertical view settings due to severe deformations and self-occlusions of key\nanatomical features. Current CNN and self-attention-based methods fail to\neffectively handle these challenges, due to their reliance on single-scale\nconvolutions or simplistic attention mechanisms that lack effective\nmulti-frequency feature integration. To tackle this challenge, we propose\nCVVNet (Cross-Vertical-View Network), a frequency aggregation architecture\nspecifically designed for robust cross-vertical-view gait recognition. CVVNet\nemploys a High-Low Frequency Extraction module (HLFE) that adopts parallel\nmulti-scale convolution/max-pooling path and self-attention path as high- and\nlow-frequency mixers for effective multi-frequency feature extraction from\ninput silhouettes. We also introduce the Dynamic Gated Aggregation (DGA)\nmechanism to adaptively adjust the fusion ratio of high- and low-frequency\nfeatures. The integration of our core Multi-Scale Attention Gated Aggregation\n(MSAGA) module, HLFE and DGA enables CVVNet to effectively handle distortions\nfrom view changes, significantly improving the recognition robustness across\ndifferent vertical views. Experimental results show that our CVVNet achieves\nstate-of-the-art performance, with $8.6\\%$ improvement on DroneGait and $2\\%$\non Gait3D compared with the best existing methods.\n", "link": "http://arxiv.org/abs/2505.01837v3", "date": "2025-09-15", "relevancy": 2.1858, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5589}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5484}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CVVNet%3A%20A%20Cross-Vertical-View%20Network%20for%20Gait%20Recognition&body=Title%3A%20CVVNet%3A%20A%20Cross-Vertical-View%20Network%20for%20Gait%20Recognition%0AAuthor%3A%20Xiangru%20Li%20and%20Wei%20Song%20and%20Yingda%20Huang%20and%20Wei%20Meng%20and%20Le%20Chang%20and%20Hongyang%20Li%0AAbstract%3A%20%20%20Gait%20recognition%20enables%20contact-free%2C%20long-range%20person%20identification%20that%0Ais%20robust%20to%20clothing%20variations%20and%20non-cooperative%20scenarios.%20While%20existing%0Amethods%20perform%20well%20in%20controlled%20indoor%20environments%2C%20they%20struggle%20with%0Across-vertical%20view%20scenarios%2C%20where%20surveillance%20angles%20vary%20significantly%20in%0Aelevation.%20Our%20experiments%20show%20up%20to%2060%5C%25%20accuracy%20degradation%20in%20low-to-high%0Avertical%20view%20settings%20due%20to%20severe%20deformations%20and%20self-occlusions%20of%20key%0Aanatomical%20features.%20Current%20CNN%20and%20self-attention-based%20methods%20fail%20to%0Aeffectively%20handle%20these%20challenges%2C%20due%20to%20their%20reliance%20on%20single-scale%0Aconvolutions%20or%20simplistic%20attention%20mechanisms%20that%20lack%20effective%0Amulti-frequency%20feature%20integration.%20To%20tackle%20this%20challenge%2C%20we%20propose%0ACVVNet%20%28Cross-Vertical-View%20Network%29%2C%20a%20frequency%20aggregation%20architecture%0Aspecifically%20designed%20for%20robust%20cross-vertical-view%20gait%20recognition.%20CVVNet%0Aemploys%20a%20High-Low%20Frequency%20Extraction%20module%20%28HLFE%29%20that%20adopts%20parallel%0Amulti-scale%20convolution/max-pooling%20path%20and%20self-attention%20path%20as%20high-%20and%0Alow-frequency%20mixers%20for%20effective%20multi-frequency%20feature%20extraction%20from%0Ainput%20silhouettes.%20We%20also%20introduce%20the%20Dynamic%20Gated%20Aggregation%20%28DGA%29%0Amechanism%20to%20adaptively%20adjust%20the%20fusion%20ratio%20of%20high-%20and%20low-frequency%0Afeatures.%20The%20integration%20of%20our%20core%20Multi-Scale%20Attention%20Gated%20Aggregation%0A%28MSAGA%29%20module%2C%20HLFE%20and%20DGA%20enables%20CVVNet%20to%20effectively%20handle%20distortions%0Afrom%20view%20changes%2C%20significantly%20improving%20the%20recognition%20robustness%20across%0Adifferent%20vertical%20views.%20Experimental%20results%20show%20that%20our%20CVVNet%20achieves%0Astate-of-the-art%20performance%2C%20with%20%248.6%5C%25%24%20improvement%20on%20DroneGait%20and%20%242%5C%25%24%0Aon%20Gait3D%20compared%20with%20the%20best%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01837v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCVVNet%253A%2520A%2520Cross-Vertical-View%2520Network%2520for%2520Gait%2520Recognition%26entry.906535625%3DXiangru%2520Li%2520and%2520Wei%2520Song%2520and%2520Yingda%2520Huang%2520and%2520Wei%2520Meng%2520and%2520Le%2520Chang%2520and%2520Hongyang%2520Li%26entry.1292438233%3D%2520%2520Gait%2520recognition%2520enables%2520contact-free%252C%2520long-range%2520person%2520identification%2520that%250Ais%2520robust%2520to%2520clothing%2520variations%2520and%2520non-cooperative%2520scenarios.%2520While%2520existing%250Amethods%2520perform%2520well%2520in%2520controlled%2520indoor%2520environments%252C%2520they%2520struggle%2520with%250Across-vertical%2520view%2520scenarios%252C%2520where%2520surveillance%2520angles%2520vary%2520significantly%2520in%250Aelevation.%2520Our%2520experiments%2520show%2520up%2520to%252060%255C%2525%2520accuracy%2520degradation%2520in%2520low-to-high%250Avertical%2520view%2520settings%2520due%2520to%2520severe%2520deformations%2520and%2520self-occlusions%2520of%2520key%250Aanatomical%2520features.%2520Current%2520CNN%2520and%2520self-attention-based%2520methods%2520fail%2520to%250Aeffectively%2520handle%2520these%2520challenges%252C%2520due%2520to%2520their%2520reliance%2520on%2520single-scale%250Aconvolutions%2520or%2520simplistic%2520attention%2520mechanisms%2520that%2520lack%2520effective%250Amulti-frequency%2520feature%2520integration.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%250ACVVNet%2520%2528Cross-Vertical-View%2520Network%2529%252C%2520a%2520frequency%2520aggregation%2520architecture%250Aspecifically%2520designed%2520for%2520robust%2520cross-vertical-view%2520gait%2520recognition.%2520CVVNet%250Aemploys%2520a%2520High-Low%2520Frequency%2520Extraction%2520module%2520%2528HLFE%2529%2520that%2520adopts%2520parallel%250Amulti-scale%2520convolution/max-pooling%2520path%2520and%2520self-attention%2520path%2520as%2520high-%2520and%250Alow-frequency%2520mixers%2520for%2520effective%2520multi-frequency%2520feature%2520extraction%2520from%250Ainput%2520silhouettes.%2520We%2520also%2520introduce%2520the%2520Dynamic%2520Gated%2520Aggregation%2520%2528DGA%2529%250Amechanism%2520to%2520adaptively%2520adjust%2520the%2520fusion%2520ratio%2520of%2520high-%2520and%2520low-frequency%250Afeatures.%2520The%2520integration%2520of%2520our%2520core%2520Multi-Scale%2520Attention%2520Gated%2520Aggregation%250A%2528MSAGA%2529%2520module%252C%2520HLFE%2520and%2520DGA%2520enables%2520CVVNet%2520to%2520effectively%2520handle%2520distortions%250Afrom%2520view%2520changes%252C%2520significantly%2520improving%2520the%2520recognition%2520robustness%2520across%250Adifferent%2520vertical%2520views.%2520Experimental%2520results%2520show%2520that%2520our%2520CVVNet%2520achieves%250Astate-of-the-art%2520performance%252C%2520with%2520%25248.6%255C%2525%2524%2520improvement%2520on%2520DroneGait%2520and%2520%25242%255C%2525%2524%250Aon%2520Gait3D%2520compared%2520with%2520the%2520best%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01837v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CVVNet%3A%20A%20Cross-Vertical-View%20Network%20for%20Gait%20Recognition&entry.906535625=Xiangru%20Li%20and%20Wei%20Song%20and%20Yingda%20Huang%20and%20Wei%20Meng%20and%20Le%20Chang%20and%20Hongyang%20Li&entry.1292438233=%20%20Gait%20recognition%20enables%20contact-free%2C%20long-range%20person%20identification%20that%0Ais%20robust%20to%20clothing%20variations%20and%20non-cooperative%20scenarios.%20While%20existing%0Amethods%20perform%20well%20in%20controlled%20indoor%20environments%2C%20they%20struggle%20with%0Across-vertical%20view%20scenarios%2C%20where%20surveillance%20angles%20vary%20significantly%20in%0Aelevation.%20Our%20experiments%20show%20up%20to%2060%5C%25%20accuracy%20degradation%20in%20low-to-high%0Avertical%20view%20settings%20due%20to%20severe%20deformations%20and%20self-occlusions%20of%20key%0Aanatomical%20features.%20Current%20CNN%20and%20self-attention-based%20methods%20fail%20to%0Aeffectively%20handle%20these%20challenges%2C%20due%20to%20their%20reliance%20on%20single-scale%0Aconvolutions%20or%20simplistic%20attention%20mechanisms%20that%20lack%20effective%0Amulti-frequency%20feature%20integration.%20To%20tackle%20this%20challenge%2C%20we%20propose%0ACVVNet%20%28Cross-Vertical-View%20Network%29%2C%20a%20frequency%20aggregation%20architecture%0Aspecifically%20designed%20for%20robust%20cross-vertical-view%20gait%20recognition.%20CVVNet%0Aemploys%20a%20High-Low%20Frequency%20Extraction%20module%20%28HLFE%29%20that%20adopts%20parallel%0Amulti-scale%20convolution/max-pooling%20path%20and%20self-attention%20path%20as%20high-%20and%0Alow-frequency%20mixers%20for%20effective%20multi-frequency%20feature%20extraction%20from%0Ainput%20silhouettes.%20We%20also%20introduce%20the%20Dynamic%20Gated%20Aggregation%20%28DGA%29%0Amechanism%20to%20adaptively%20adjust%20the%20fusion%20ratio%20of%20high-%20and%20low-frequency%0Afeatures.%20The%20integration%20of%20our%20core%20Multi-Scale%20Attention%20Gated%20Aggregation%0A%28MSAGA%29%20module%2C%20HLFE%20and%20DGA%20enables%20CVVNet%20to%20effectively%20handle%20distortions%0Afrom%20view%20changes%2C%20significantly%20improving%20the%20recognition%20robustness%20across%0Adifferent%20vertical%20views.%20Experimental%20results%20show%20that%20our%20CVVNet%20achieves%0Astate-of-the-art%20performance%2C%20with%20%248.6%5C%25%24%20improvement%20on%20DroneGait%20and%20%242%5C%25%24%0Aon%20Gait3D%20compared%20with%20the%20best%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01837v3&entry.124074799=Read"},
{"title": "Kolb-Based Experiential Learning for Generalist Agents with Human-Level\n  Kaggle Data Science Performance", "author": "Antoine Grosnit and Alexandre Maraval and Refinath S N and Zichao Zhao and James Doran and Giuseppe Paolo and Albert Thomas and Jonas Gonzalez and Abhineet Kumar and Khyati Khandelwal and Abdelhakim Benechehab and Hamza Cherkaoui and Youssef Attia El-Hili and Kun Shao and Jianye Hao and Jun Yao and Bal\u00e1zs K\u00e9gl and Haitham Bou-Ammar and Jun Wang", "abstract": "  Human expertise emerges through iterative cycles of interaction, reflection,\nand internal model updating, which are central to cognitive theories such as\nKolb's experiential learning and Vygotsky's zone of proximal development. In\ncontrast, current AI systems, particularly LLM agents, rely on static\npre-training or rigid workflows, lacking mechanisms for continual adaptation.\nRecent studies identified early cognitive traits in LLM agents (reflection,\nrevision, and self-correction) suggesting foundational elements of human-like\nexperiential learning. Thus the key question: Can we design LLM agents capable\nof structured, cognitively grounded learning similar to human processes? In\nresponse, we propose a computational framework of Kolb's learning cycle with\nVygotsky's ZPD for autonomous agents. Our architecture separates extrinsic\n(environment interaction) and intrinsic (internal reflection/abstraction)\nfunctions, enabling cognitively grounded scaffolded learning, where the agent\ninitially learns within structured environments, followed by open-ended\ngeneralisation. This approach empowers agents to master complex tasks ; domains\nthat traditional fine-tuning or simple reflective methods could not tackle\neffectively. Its potential is powerfully demonstrated via direct comparison\nwith humans in real-world Kaggle data science competitions. Learning fully\nautomated data science code generation across 81 tasks, our system, Agent K,\ndemonstrated the ability to perform the entire workflow autonomously, achieving\nan Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2%\namong 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals\nlevel performance - including 4 gold and 4 silver on prize-awarding\ncompetitions - Agent K is the 1st AI system to successfully integrate Kolb- and\nVygotsky-inspired human cognitive learning, marking a major step toward\ngeneralist AI.\n", "link": "http://arxiv.org/abs/2411.03562v3", "date": "2025-09-15", "relevancy": 2.1844, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5667}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5456}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kolb-Based%20Experiential%20Learning%20for%20Generalist%20Agents%20with%20Human-Level%0A%20%20Kaggle%20Data%20Science%20Performance&body=Title%3A%20Kolb-Based%20Experiential%20Learning%20for%20Generalist%20Agents%20with%20Human-Level%0A%20%20Kaggle%20Data%20Science%20Performance%0AAuthor%3A%20Antoine%20Grosnit%20and%20Alexandre%20Maraval%20and%20Refinath%20S%20N%20and%20Zichao%20Zhao%20and%20James%20Doran%20and%20Giuseppe%20Paolo%20and%20Albert%20Thomas%20and%20Jonas%20Gonzalez%20and%20Abhineet%20Kumar%20and%20Khyati%20Khandelwal%20and%20Abdelhakim%20Benechehab%20and%20Hamza%20Cherkaoui%20and%20Youssef%20Attia%20El-Hili%20and%20Kun%20Shao%20and%20Jianye%20Hao%20and%20Jun%20Yao%20and%20Bal%C3%A1zs%20K%C3%A9gl%20and%20Haitham%20Bou-Ammar%20and%20Jun%20Wang%0AAbstract%3A%20%20%20Human%20expertise%20emerges%20through%20iterative%20cycles%20of%20interaction%2C%20reflection%2C%0Aand%20internal%20model%20updating%2C%20which%20are%20central%20to%20cognitive%20theories%20such%20as%0AKolb%27s%20experiential%20learning%20and%20Vygotsky%27s%20zone%20of%20proximal%20development.%20In%0Acontrast%2C%20current%20AI%20systems%2C%20particularly%20LLM%20agents%2C%20rely%20on%20static%0Apre-training%20or%20rigid%20workflows%2C%20lacking%20mechanisms%20for%20continual%20adaptation.%0ARecent%20studies%20identified%20early%20cognitive%20traits%20in%20LLM%20agents%20%28reflection%2C%0Arevision%2C%20and%20self-correction%29%20suggesting%20foundational%20elements%20of%20human-like%0Aexperiential%20learning.%20Thus%20the%20key%20question%3A%20Can%20we%20design%20LLM%20agents%20capable%0Aof%20structured%2C%20cognitively%20grounded%20learning%20similar%20to%20human%20processes%3F%20In%0Aresponse%2C%20we%20propose%20a%20computational%20framework%20of%20Kolb%27s%20learning%20cycle%20with%0AVygotsky%27s%20ZPD%20for%20autonomous%20agents.%20Our%20architecture%20separates%20extrinsic%0A%28environment%20interaction%29%20and%20intrinsic%20%28internal%20reflection/abstraction%29%0Afunctions%2C%20enabling%20cognitively%20grounded%20scaffolded%20learning%2C%20where%20the%20agent%0Ainitially%20learns%20within%20structured%20environments%2C%20followed%20by%20open-ended%0Ageneralisation.%20This%20approach%20empowers%20agents%20to%20master%20complex%20tasks%20%3B%20domains%0Athat%20traditional%20fine-tuning%20or%20simple%20reflective%20methods%20could%20not%20tackle%0Aeffectively.%20Its%20potential%20is%20powerfully%20demonstrated%20via%20direct%20comparison%0Awith%20humans%20in%20real-world%20Kaggle%20data%20science%20competitions.%20Learning%20fully%0Aautomated%20data%20science%20code%20generation%20across%2081%20tasks%2C%20our%20system%2C%20Agent%20K%2C%0Ademonstrated%20the%20ability%20to%20perform%20the%20entire%20workflow%20autonomously%2C%20achieving%0Aan%20Elo-MMR%20score%20of%201694%2C%20beyond%20median%20score%20of%20the%20Kaggle%20Masters%20%28the%20top%202%25%0Aamong%20200%2C000%20users%29%20of%20our%20study.%20With%209%20gold%2C%208%20silver%2C%20and%2012%20bronze%20medals%0Alevel%20performance%20-%20including%204%20gold%20and%204%20silver%20on%20prize-awarding%0Acompetitions%20-%20Agent%20K%20is%20the%201st%20AI%20system%20to%20successfully%20integrate%20Kolb-%20and%0AVygotsky-inspired%20human%20cognitive%20learning%2C%20marking%20a%20major%20step%20toward%0Ageneralist%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03562v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKolb-Based%2520Experiential%2520Learning%2520for%2520Generalist%2520Agents%2520with%2520Human-Level%250A%2520%2520Kaggle%2520Data%2520Science%2520Performance%26entry.906535625%3DAntoine%2520Grosnit%2520and%2520Alexandre%2520Maraval%2520and%2520Refinath%2520S%2520N%2520and%2520Zichao%2520Zhao%2520and%2520James%2520Doran%2520and%2520Giuseppe%2520Paolo%2520and%2520Albert%2520Thomas%2520and%2520Jonas%2520Gonzalez%2520and%2520Abhineet%2520Kumar%2520and%2520Khyati%2520Khandelwal%2520and%2520Abdelhakim%2520Benechehab%2520and%2520Hamza%2520Cherkaoui%2520and%2520Youssef%2520Attia%2520El-Hili%2520and%2520Kun%2520Shao%2520and%2520Jianye%2520Hao%2520and%2520Jun%2520Yao%2520and%2520Bal%25C3%25A1zs%2520K%25C3%25A9gl%2520and%2520Haitham%2520Bou-Ammar%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520Human%2520expertise%2520emerges%2520through%2520iterative%2520cycles%2520of%2520interaction%252C%2520reflection%252C%250Aand%2520internal%2520model%2520updating%252C%2520which%2520are%2520central%2520to%2520cognitive%2520theories%2520such%2520as%250AKolb%2527s%2520experiential%2520learning%2520and%2520Vygotsky%2527s%2520zone%2520of%2520proximal%2520development.%2520In%250Acontrast%252C%2520current%2520AI%2520systems%252C%2520particularly%2520LLM%2520agents%252C%2520rely%2520on%2520static%250Apre-training%2520or%2520rigid%2520workflows%252C%2520lacking%2520mechanisms%2520for%2520continual%2520adaptation.%250ARecent%2520studies%2520identified%2520early%2520cognitive%2520traits%2520in%2520LLM%2520agents%2520%2528reflection%252C%250Arevision%252C%2520and%2520self-correction%2529%2520suggesting%2520foundational%2520elements%2520of%2520human-like%250Aexperiential%2520learning.%2520Thus%2520the%2520key%2520question%253A%2520Can%2520we%2520design%2520LLM%2520agents%2520capable%250Aof%2520structured%252C%2520cognitively%2520grounded%2520learning%2520similar%2520to%2520human%2520processes%253F%2520In%250Aresponse%252C%2520we%2520propose%2520a%2520computational%2520framework%2520of%2520Kolb%2527s%2520learning%2520cycle%2520with%250AVygotsky%2527s%2520ZPD%2520for%2520autonomous%2520agents.%2520Our%2520architecture%2520separates%2520extrinsic%250A%2528environment%2520interaction%2529%2520and%2520intrinsic%2520%2528internal%2520reflection/abstraction%2529%250Afunctions%252C%2520enabling%2520cognitively%2520grounded%2520scaffolded%2520learning%252C%2520where%2520the%2520agent%250Ainitially%2520learns%2520within%2520structured%2520environments%252C%2520followed%2520by%2520open-ended%250Ageneralisation.%2520This%2520approach%2520empowers%2520agents%2520to%2520master%2520complex%2520tasks%2520%253B%2520domains%250Athat%2520traditional%2520fine-tuning%2520or%2520simple%2520reflective%2520methods%2520could%2520not%2520tackle%250Aeffectively.%2520Its%2520potential%2520is%2520powerfully%2520demonstrated%2520via%2520direct%2520comparison%250Awith%2520humans%2520in%2520real-world%2520Kaggle%2520data%2520science%2520competitions.%2520Learning%2520fully%250Aautomated%2520data%2520science%2520code%2520generation%2520across%252081%2520tasks%252C%2520our%2520system%252C%2520Agent%2520K%252C%250Ademonstrated%2520the%2520ability%2520to%2520perform%2520the%2520entire%2520workflow%2520autonomously%252C%2520achieving%250Aan%2520Elo-MMR%2520score%2520of%25201694%252C%2520beyond%2520median%2520score%2520of%2520the%2520Kaggle%2520Masters%2520%2528the%2520top%25202%2525%250Aamong%2520200%252C000%2520users%2529%2520of%2520our%2520study.%2520With%25209%2520gold%252C%25208%2520silver%252C%2520and%252012%2520bronze%2520medals%250Alevel%2520performance%2520-%2520including%25204%2520gold%2520and%25204%2520silver%2520on%2520prize-awarding%250Acompetitions%2520-%2520Agent%2520K%2520is%2520the%25201st%2520AI%2520system%2520to%2520successfully%2520integrate%2520Kolb-%2520and%250AVygotsky-inspired%2520human%2520cognitive%2520learning%252C%2520marking%2520a%2520major%2520step%2520toward%250Ageneralist%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03562v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kolb-Based%20Experiential%20Learning%20for%20Generalist%20Agents%20with%20Human-Level%0A%20%20Kaggle%20Data%20Science%20Performance&entry.906535625=Antoine%20Grosnit%20and%20Alexandre%20Maraval%20and%20Refinath%20S%20N%20and%20Zichao%20Zhao%20and%20James%20Doran%20and%20Giuseppe%20Paolo%20and%20Albert%20Thomas%20and%20Jonas%20Gonzalez%20and%20Abhineet%20Kumar%20and%20Khyati%20Khandelwal%20and%20Abdelhakim%20Benechehab%20and%20Hamza%20Cherkaoui%20and%20Youssef%20Attia%20El-Hili%20and%20Kun%20Shao%20and%20Jianye%20Hao%20and%20Jun%20Yao%20and%20Bal%C3%A1zs%20K%C3%A9gl%20and%20Haitham%20Bou-Ammar%20and%20Jun%20Wang&entry.1292438233=%20%20Human%20expertise%20emerges%20through%20iterative%20cycles%20of%20interaction%2C%20reflection%2C%0Aand%20internal%20model%20updating%2C%20which%20are%20central%20to%20cognitive%20theories%20such%20as%0AKolb%27s%20experiential%20learning%20and%20Vygotsky%27s%20zone%20of%20proximal%20development.%20In%0Acontrast%2C%20current%20AI%20systems%2C%20particularly%20LLM%20agents%2C%20rely%20on%20static%0Apre-training%20or%20rigid%20workflows%2C%20lacking%20mechanisms%20for%20continual%20adaptation.%0ARecent%20studies%20identified%20early%20cognitive%20traits%20in%20LLM%20agents%20%28reflection%2C%0Arevision%2C%20and%20self-correction%29%20suggesting%20foundational%20elements%20of%20human-like%0Aexperiential%20learning.%20Thus%20the%20key%20question%3A%20Can%20we%20design%20LLM%20agents%20capable%0Aof%20structured%2C%20cognitively%20grounded%20learning%20similar%20to%20human%20processes%3F%20In%0Aresponse%2C%20we%20propose%20a%20computational%20framework%20of%20Kolb%27s%20learning%20cycle%20with%0AVygotsky%27s%20ZPD%20for%20autonomous%20agents.%20Our%20architecture%20separates%20extrinsic%0A%28environment%20interaction%29%20and%20intrinsic%20%28internal%20reflection/abstraction%29%0Afunctions%2C%20enabling%20cognitively%20grounded%20scaffolded%20learning%2C%20where%20the%20agent%0Ainitially%20learns%20within%20structured%20environments%2C%20followed%20by%20open-ended%0Ageneralisation.%20This%20approach%20empowers%20agents%20to%20master%20complex%20tasks%20%3B%20domains%0Athat%20traditional%20fine-tuning%20or%20simple%20reflective%20methods%20could%20not%20tackle%0Aeffectively.%20Its%20potential%20is%20powerfully%20demonstrated%20via%20direct%20comparison%0Awith%20humans%20in%20real-world%20Kaggle%20data%20science%20competitions.%20Learning%20fully%0Aautomated%20data%20science%20code%20generation%20across%2081%20tasks%2C%20our%20system%2C%20Agent%20K%2C%0Ademonstrated%20the%20ability%20to%20perform%20the%20entire%20workflow%20autonomously%2C%20achieving%0Aan%20Elo-MMR%20score%20of%201694%2C%20beyond%20median%20score%20of%20the%20Kaggle%20Masters%20%28the%20top%202%25%0Aamong%20200%2C000%20users%29%20of%20our%20study.%20With%209%20gold%2C%208%20silver%2C%20and%2012%20bronze%20medals%0Alevel%20performance%20-%20including%204%20gold%20and%204%20silver%20on%20prize-awarding%0Acompetitions%20-%20Agent%20K%20is%20the%201st%20AI%20system%20to%20successfully%20integrate%20Kolb-%20and%0AVygotsky-inspired%20human%20cognitive%20learning%2C%20marking%20a%20major%20step%20toward%0Ageneralist%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03562v3&entry.124074799=Read"},
{"title": "From Autoencoders to CycleGAN: Robust Unpaired Face Manipulation via\n  Adversarial Learning", "author": "Collin Guo", "abstract": "  Human face synthesis and manipulation are increasingly important in\nentertainment and AI, with a growing demand for highly realistic,\nidentity-preserving images even when only unpaired, unaligned datasets are\navailable. We study unpaired face manipulation via adversarial learning, moving\nfrom autoencoder baselines to a robust, guided CycleGAN framework. While\nautoencoders capture coarse identity, they often miss fine details. Our\napproach integrates spectral normalization for stable training, identity- and\nperceptual-guided losses to preserve subject identity and high-level structure,\nand landmark-weighted cycle constraints to maintain facial geometry across pose\nand illumination changes. Experiments show that our adversarial trained\nCycleGAN improves realism (FID), perceptual quality (LPIPS), and identity\npreservation (ID-Sim) over autoencoders, with competitive cycle-reconstruction\nSSIM and practical inference times, which achieved high quality without paired\ndatasets and approaching pix2pix on curated paired subsets. These results\ndemonstrate that guided, spectrally normalized CycleGANs provide a practical\npath from autoencoders to robust unpaired face manipulation.\n", "link": "http://arxiv.org/abs/2509.12176v1", "date": "2025-09-15", "relevancy": 2.1675, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5604}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5489}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Autoencoders%20to%20CycleGAN%3A%20Robust%20Unpaired%20Face%20Manipulation%20via%0A%20%20Adversarial%20Learning&body=Title%3A%20From%20Autoencoders%20to%20CycleGAN%3A%20Robust%20Unpaired%20Face%20Manipulation%20via%0A%20%20Adversarial%20Learning%0AAuthor%3A%20Collin%20Guo%0AAbstract%3A%20%20%20Human%20face%20synthesis%20and%20manipulation%20are%20increasingly%20important%20in%0Aentertainment%20and%20AI%2C%20with%20a%20growing%20demand%20for%20highly%20realistic%2C%0Aidentity-preserving%20images%20even%20when%20only%20unpaired%2C%20unaligned%20datasets%20are%0Aavailable.%20We%20study%20unpaired%20face%20manipulation%20via%20adversarial%20learning%2C%20moving%0Afrom%20autoencoder%20baselines%20to%20a%20robust%2C%20guided%20CycleGAN%20framework.%20While%0Aautoencoders%20capture%20coarse%20identity%2C%20they%20often%20miss%20fine%20details.%20Our%0Aapproach%20integrates%20spectral%20normalization%20for%20stable%20training%2C%20identity-%20and%0Aperceptual-guided%20losses%20to%20preserve%20subject%20identity%20and%20high-level%20structure%2C%0Aand%20landmark-weighted%20cycle%20constraints%20to%20maintain%20facial%20geometry%20across%20pose%0Aand%20illumination%20changes.%20Experiments%20show%20that%20our%20adversarial%20trained%0ACycleGAN%20improves%20realism%20%28FID%29%2C%20perceptual%20quality%20%28LPIPS%29%2C%20and%20identity%0Apreservation%20%28ID-Sim%29%20over%20autoencoders%2C%20with%20competitive%20cycle-reconstruction%0ASSIM%20and%20practical%20inference%20times%2C%20which%20achieved%20high%20quality%20without%20paired%0Adatasets%20and%20approaching%20pix2pix%20on%20curated%20paired%20subsets.%20These%20results%0Ademonstrate%20that%20guided%2C%20spectrally%20normalized%20CycleGANs%20provide%20a%20practical%0Apath%20from%20autoencoders%20to%20robust%20unpaired%20face%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Autoencoders%2520to%2520CycleGAN%253A%2520Robust%2520Unpaired%2520Face%2520Manipulation%2520via%250A%2520%2520Adversarial%2520Learning%26entry.906535625%3DCollin%2520Guo%26entry.1292438233%3D%2520%2520Human%2520face%2520synthesis%2520and%2520manipulation%2520are%2520increasingly%2520important%2520in%250Aentertainment%2520and%2520AI%252C%2520with%2520a%2520growing%2520demand%2520for%2520highly%2520realistic%252C%250Aidentity-preserving%2520images%2520even%2520when%2520only%2520unpaired%252C%2520unaligned%2520datasets%2520are%250Aavailable.%2520We%2520study%2520unpaired%2520face%2520manipulation%2520via%2520adversarial%2520learning%252C%2520moving%250Afrom%2520autoencoder%2520baselines%2520to%2520a%2520robust%252C%2520guided%2520CycleGAN%2520framework.%2520While%250Aautoencoders%2520capture%2520coarse%2520identity%252C%2520they%2520often%2520miss%2520fine%2520details.%2520Our%250Aapproach%2520integrates%2520spectral%2520normalization%2520for%2520stable%2520training%252C%2520identity-%2520and%250Aperceptual-guided%2520losses%2520to%2520preserve%2520subject%2520identity%2520and%2520high-level%2520structure%252C%250Aand%2520landmark-weighted%2520cycle%2520constraints%2520to%2520maintain%2520facial%2520geometry%2520across%2520pose%250Aand%2520illumination%2520changes.%2520Experiments%2520show%2520that%2520our%2520adversarial%2520trained%250ACycleGAN%2520improves%2520realism%2520%2528FID%2529%252C%2520perceptual%2520quality%2520%2528LPIPS%2529%252C%2520and%2520identity%250Apreservation%2520%2528ID-Sim%2529%2520over%2520autoencoders%252C%2520with%2520competitive%2520cycle-reconstruction%250ASSIM%2520and%2520practical%2520inference%2520times%252C%2520which%2520achieved%2520high%2520quality%2520without%2520paired%250Adatasets%2520and%2520approaching%2520pix2pix%2520on%2520curated%2520paired%2520subsets.%2520These%2520results%250Ademonstrate%2520that%2520guided%252C%2520spectrally%2520normalized%2520CycleGANs%2520provide%2520a%2520practical%250Apath%2520from%2520autoencoders%2520to%2520robust%2520unpaired%2520face%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Autoencoders%20to%20CycleGAN%3A%20Robust%20Unpaired%20Face%20Manipulation%20via%0A%20%20Adversarial%20Learning&entry.906535625=Collin%20Guo&entry.1292438233=%20%20Human%20face%20synthesis%20and%20manipulation%20are%20increasingly%20important%20in%0Aentertainment%20and%20AI%2C%20with%20a%20growing%20demand%20for%20highly%20realistic%2C%0Aidentity-preserving%20images%20even%20when%20only%20unpaired%2C%20unaligned%20datasets%20are%0Aavailable.%20We%20study%20unpaired%20face%20manipulation%20via%20adversarial%20learning%2C%20moving%0Afrom%20autoencoder%20baselines%20to%20a%20robust%2C%20guided%20CycleGAN%20framework.%20While%0Aautoencoders%20capture%20coarse%20identity%2C%20they%20often%20miss%20fine%20details.%20Our%0Aapproach%20integrates%20spectral%20normalization%20for%20stable%20training%2C%20identity-%20and%0Aperceptual-guided%20losses%20to%20preserve%20subject%20identity%20and%20high-level%20structure%2C%0Aand%20landmark-weighted%20cycle%20constraints%20to%20maintain%20facial%20geometry%20across%20pose%0Aand%20illumination%20changes.%20Experiments%20show%20that%20our%20adversarial%20trained%0ACycleGAN%20improves%20realism%20%28FID%29%2C%20perceptual%20quality%20%28LPIPS%29%2C%20and%20identity%0Apreservation%20%28ID-Sim%29%20over%20autoencoders%2C%20with%20competitive%20cycle-reconstruction%0ASSIM%20and%20practical%20inference%20times%2C%20which%20achieved%20high%20quality%20without%20paired%0Adatasets%20and%20approaching%20pix2pix%20on%20curated%20paired%20subsets.%20These%20results%0Ademonstrate%20that%20guided%2C%20spectrally%20normalized%20CycleGANs%20provide%20a%20practical%0Apath%20from%20autoencoders%20to%20robust%20unpaired%20face%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12176v1&entry.124074799=Read"},
{"title": "Growing Perspectives: Modelling Embodied Perspective Taking and Inner\n  Narrative Development Using Large Language Models", "author": "Sabrina Patania and Luca Annese and Anna Lambiase and Anita Pellegrini and Tom Foulsham and Azzurra Ruggeri and Silvia Rossi and Silvia Serino and Dimitri Ognibene", "abstract": "  Language and embodied perspective taking are essential for human\ncollaboration, yet few computational models address both simultaneously. This\nwork investigates the PerspAct system [1], which integrates the ReAct (Reason\nand Act) paradigm with Large Language Models (LLMs) to simulate developmental\nstages of perspective taking, grounded in Selman's theory [2]. Using an\nextended director task, we evaluate GPT's ability to generate internal\nnarratives aligned with specified developmental stages, and assess how these\ninfluence collaborative performance both qualitatively (action selection) and\nquantitatively (task efficiency). Results show that GPT reliably produces\ndevelopmentally-consistent narratives before task execution but often shifts\ntowards more advanced stages during interaction, suggesting that language\nexchanges help refine internal representations. Higher developmental stages\ngenerally enhance collaborative effectiveness, while earlier stages yield more\nvariable outcomes in complex contexts. These findings highlight the potential\nof integrating embodied perspective taking and language in LLMs to better model\ndevelopmental dynamics and stress the importance of evaluating internal speech\nduring combined linguistic and embodied tasks.\n", "link": "http://arxiv.org/abs/2509.11868v1", "date": "2025-09-15", "relevancy": 2.1659, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Growing%20Perspectives%3A%20Modelling%20Embodied%20Perspective%20Taking%20and%20Inner%0A%20%20Narrative%20Development%20Using%20Large%20Language%20Models&body=Title%3A%20Growing%20Perspectives%3A%20Modelling%20Embodied%20Perspective%20Taking%20and%20Inner%0A%20%20Narrative%20Development%20Using%20Large%20Language%20Models%0AAuthor%3A%20Sabrina%20Patania%20and%20Luca%20Annese%20and%20Anna%20Lambiase%20and%20Anita%20Pellegrini%20and%20Tom%20Foulsham%20and%20Azzurra%20Ruggeri%20and%20Silvia%20Rossi%20and%20Silvia%20Serino%20and%20Dimitri%20Ognibene%0AAbstract%3A%20%20%20Language%20and%20embodied%20perspective%20taking%20are%20essential%20for%20human%0Acollaboration%2C%20yet%20few%20computational%20models%20address%20both%20simultaneously.%20This%0Awork%20investigates%20the%20PerspAct%20system%20%5B1%5D%2C%20which%20integrates%20the%20ReAct%20%28Reason%0Aand%20Act%29%20paradigm%20with%20Large%20Language%20Models%20%28LLMs%29%20to%20simulate%20developmental%0Astages%20of%20perspective%20taking%2C%20grounded%20in%20Selman%27s%20theory%20%5B2%5D.%20Using%20an%0Aextended%20director%20task%2C%20we%20evaluate%20GPT%27s%20ability%20to%20generate%20internal%0Anarratives%20aligned%20with%20specified%20developmental%20stages%2C%20and%20assess%20how%20these%0Ainfluence%20collaborative%20performance%20both%20qualitatively%20%28action%20selection%29%20and%0Aquantitatively%20%28task%20efficiency%29.%20Results%20show%20that%20GPT%20reliably%20produces%0Adevelopmentally-consistent%20narratives%20before%20task%20execution%20but%20often%20shifts%0Atowards%20more%20advanced%20stages%20during%20interaction%2C%20suggesting%20that%20language%0Aexchanges%20help%20refine%20internal%20representations.%20Higher%20developmental%20stages%0Agenerally%20enhance%20collaborative%20effectiveness%2C%20while%20earlier%20stages%20yield%20more%0Avariable%20outcomes%20in%20complex%20contexts.%20These%20findings%20highlight%20the%20potential%0Aof%20integrating%20embodied%20perspective%20taking%20and%20language%20in%20LLMs%20to%20better%20model%0Adevelopmental%20dynamics%20and%20stress%20the%20importance%20of%20evaluating%20internal%20speech%0Aduring%20combined%20linguistic%20and%20embodied%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrowing%2520Perspectives%253A%2520Modelling%2520Embodied%2520Perspective%2520Taking%2520and%2520Inner%250A%2520%2520Narrative%2520Development%2520Using%2520Large%2520Language%2520Models%26entry.906535625%3DSabrina%2520Patania%2520and%2520Luca%2520Annese%2520and%2520Anna%2520Lambiase%2520and%2520Anita%2520Pellegrini%2520and%2520Tom%2520Foulsham%2520and%2520Azzurra%2520Ruggeri%2520and%2520Silvia%2520Rossi%2520and%2520Silvia%2520Serino%2520and%2520Dimitri%2520Ognibene%26entry.1292438233%3D%2520%2520Language%2520and%2520embodied%2520perspective%2520taking%2520are%2520essential%2520for%2520human%250Acollaboration%252C%2520yet%2520few%2520computational%2520models%2520address%2520both%2520simultaneously.%2520This%250Awork%2520investigates%2520the%2520PerspAct%2520system%2520%255B1%255D%252C%2520which%2520integrates%2520the%2520ReAct%2520%2528Reason%250Aand%2520Act%2529%2520paradigm%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520simulate%2520developmental%250Astages%2520of%2520perspective%2520taking%252C%2520grounded%2520in%2520Selman%2527s%2520theory%2520%255B2%255D.%2520Using%2520an%250Aextended%2520director%2520task%252C%2520we%2520evaluate%2520GPT%2527s%2520ability%2520to%2520generate%2520internal%250Anarratives%2520aligned%2520with%2520specified%2520developmental%2520stages%252C%2520and%2520assess%2520how%2520these%250Ainfluence%2520collaborative%2520performance%2520both%2520qualitatively%2520%2528action%2520selection%2529%2520and%250Aquantitatively%2520%2528task%2520efficiency%2529.%2520Results%2520show%2520that%2520GPT%2520reliably%2520produces%250Adevelopmentally-consistent%2520narratives%2520before%2520task%2520execution%2520but%2520often%2520shifts%250Atowards%2520more%2520advanced%2520stages%2520during%2520interaction%252C%2520suggesting%2520that%2520language%250Aexchanges%2520help%2520refine%2520internal%2520representations.%2520Higher%2520developmental%2520stages%250Agenerally%2520enhance%2520collaborative%2520effectiveness%252C%2520while%2520earlier%2520stages%2520yield%2520more%250Avariable%2520outcomes%2520in%2520complex%2520contexts.%2520These%2520findings%2520highlight%2520the%2520potential%250Aof%2520integrating%2520embodied%2520perspective%2520taking%2520and%2520language%2520in%2520LLMs%2520to%2520better%2520model%250Adevelopmental%2520dynamics%2520and%2520stress%2520the%2520importance%2520of%2520evaluating%2520internal%2520speech%250Aduring%2520combined%2520linguistic%2520and%2520embodied%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Growing%20Perspectives%3A%20Modelling%20Embodied%20Perspective%20Taking%20and%20Inner%0A%20%20Narrative%20Development%20Using%20Large%20Language%20Models&entry.906535625=Sabrina%20Patania%20and%20Luca%20Annese%20and%20Anna%20Lambiase%20and%20Anita%20Pellegrini%20and%20Tom%20Foulsham%20and%20Azzurra%20Ruggeri%20and%20Silvia%20Rossi%20and%20Silvia%20Serino%20and%20Dimitri%20Ognibene&entry.1292438233=%20%20Language%20and%20embodied%20perspective%20taking%20are%20essential%20for%20human%0Acollaboration%2C%20yet%20few%20computational%20models%20address%20both%20simultaneously.%20This%0Awork%20investigates%20the%20PerspAct%20system%20%5B1%5D%2C%20which%20integrates%20the%20ReAct%20%28Reason%0Aand%20Act%29%20paradigm%20with%20Large%20Language%20Models%20%28LLMs%29%20to%20simulate%20developmental%0Astages%20of%20perspective%20taking%2C%20grounded%20in%20Selman%27s%20theory%20%5B2%5D.%20Using%20an%0Aextended%20director%20task%2C%20we%20evaluate%20GPT%27s%20ability%20to%20generate%20internal%0Anarratives%20aligned%20with%20specified%20developmental%20stages%2C%20and%20assess%20how%20these%0Ainfluence%20collaborative%20performance%20both%20qualitatively%20%28action%20selection%29%20and%0Aquantitatively%20%28task%20efficiency%29.%20Results%20show%20that%20GPT%20reliably%20produces%0Adevelopmentally-consistent%20narratives%20before%20task%20execution%20but%20often%20shifts%0Atowards%20more%20advanced%20stages%20during%20interaction%2C%20suggesting%20that%20language%0Aexchanges%20help%20refine%20internal%20representations.%20Higher%20developmental%20stages%0Agenerally%20enhance%20collaborative%20effectiveness%2C%20while%20earlier%20stages%20yield%20more%0Avariable%20outcomes%20in%20complex%20contexts.%20These%20findings%20highlight%20the%20potential%0Aof%20integrating%20embodied%20perspective%20taking%20and%20language%20in%20LLMs%20to%20better%20model%0Adevelopmental%20dynamics%20and%20stress%20the%20importance%20of%20evaluating%20internal%20speech%0Aduring%20combined%20linguistic%20and%20embodied%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11868v1&entry.124074799=Read"},
{"title": "BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation", "author": "Francis Xiatian Zhang and Emile Mackute and Mohammadreza Kasaei and Kevin Dhaliwal and Robert Thomson and Mohsen Khadem", "abstract": "  Monocular depth estimation in bronchoscopy can significantly improve\nreal-time navigation accuracy and enhance the safety of interventions in\ncomplex, branching airways. Recent advances in depth foundation models have\nshown promise for endoscopic scenarios, yet these models often lack anatomical\nawareness in bronchoscopy, overfitting to local textures rather than capturing\nthe global airway structure, particularly under ambiguous depth cues and poor\nlighting. To address this, we propose Brea-Depth, a novel framework that\nintegrates airway-specific geometric priors into foundation model adaptation\nfor bronchoscopic depth estimation. Our method introduces a depth-aware\nCycleGAN, refining the translation between real bronchoscopic images and airway\ngeometries from anatomical data, effectively bridging the domain gap. In\naddition, we introduce an airway structure awareness loss to enforce depth\nconsistency within the airway lumen while preserving smooth transitions and\nstructural integrity. By incorporating anatomical priors, Brea-Depth enhances\nmodel generalization and yields more robust, accurate 3D airway\nreconstructions. To assess anatomical realism, we introduce Airway Depth\nStructure Evaluation, a new metric for structural consistency. We validate\nBREA-Depth on a collected ex vivo human lung dataset and an open bronchoscopic\ndataset, where it outperforms existing methods in anatomical depth\npreservation.\n", "link": "http://arxiv.org/abs/2509.11885v1", "date": "2025-09-15", "relevancy": 2.1538, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5439}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5439}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BREA-Depth%3A%20Bronchoscopy%20Realistic%20Airway-geometric%20Depth%20Estimation&body=Title%3A%20BREA-Depth%3A%20Bronchoscopy%20Realistic%20Airway-geometric%20Depth%20Estimation%0AAuthor%3A%20Francis%20Xiatian%20Zhang%20and%20Emile%20Mackute%20and%20Mohammadreza%20Kasaei%20and%20Kevin%20Dhaliwal%20and%20Robert%20Thomson%20and%20Mohsen%20Khadem%0AAbstract%3A%20%20%20Monocular%20depth%20estimation%20in%20bronchoscopy%20can%20significantly%20improve%0Areal-time%20navigation%20accuracy%20and%20enhance%20the%20safety%20of%20interventions%20in%0Acomplex%2C%20branching%20airways.%20Recent%20advances%20in%20depth%20foundation%20models%20have%0Ashown%20promise%20for%20endoscopic%20scenarios%2C%20yet%20these%20models%20often%20lack%20anatomical%0Aawareness%20in%20bronchoscopy%2C%20overfitting%20to%20local%20textures%20rather%20than%20capturing%0Athe%20global%20airway%20structure%2C%20particularly%20under%20ambiguous%20depth%20cues%20and%20poor%0Alighting.%20To%20address%20this%2C%20we%20propose%20Brea-Depth%2C%20a%20novel%20framework%20that%0Aintegrates%20airway-specific%20geometric%20priors%20into%20foundation%20model%20adaptation%0Afor%20bronchoscopic%20depth%20estimation.%20Our%20method%20introduces%20a%20depth-aware%0ACycleGAN%2C%20refining%20the%20translation%20between%20real%20bronchoscopic%20images%20and%20airway%0Ageometries%20from%20anatomical%20data%2C%20effectively%20bridging%20the%20domain%20gap.%20In%0Aaddition%2C%20we%20introduce%20an%20airway%20structure%20awareness%20loss%20to%20enforce%20depth%0Aconsistency%20within%20the%20airway%20lumen%20while%20preserving%20smooth%20transitions%20and%0Astructural%20integrity.%20By%20incorporating%20anatomical%20priors%2C%20Brea-Depth%20enhances%0Amodel%20generalization%20and%20yields%20more%20robust%2C%20accurate%203D%20airway%0Areconstructions.%20To%20assess%20anatomical%20realism%2C%20we%20introduce%20Airway%20Depth%0AStructure%20Evaluation%2C%20a%20new%20metric%20for%20structural%20consistency.%20We%20validate%0ABREA-Depth%20on%20a%20collected%20ex%20vivo%20human%20lung%20dataset%20and%20an%20open%20bronchoscopic%0Adataset%2C%20where%20it%20outperforms%20existing%20methods%20in%20anatomical%20depth%0Apreservation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBREA-Depth%253A%2520Bronchoscopy%2520Realistic%2520Airway-geometric%2520Depth%2520Estimation%26entry.906535625%3DFrancis%2520Xiatian%2520Zhang%2520and%2520Emile%2520Mackute%2520and%2520Mohammadreza%2520Kasaei%2520and%2520Kevin%2520Dhaliwal%2520and%2520Robert%2520Thomson%2520and%2520Mohsen%2520Khadem%26entry.1292438233%3D%2520%2520Monocular%2520depth%2520estimation%2520in%2520bronchoscopy%2520can%2520significantly%2520improve%250Areal-time%2520navigation%2520accuracy%2520and%2520enhance%2520the%2520safety%2520of%2520interventions%2520in%250Acomplex%252C%2520branching%2520airways.%2520Recent%2520advances%2520in%2520depth%2520foundation%2520models%2520have%250Ashown%2520promise%2520for%2520endoscopic%2520scenarios%252C%2520yet%2520these%2520models%2520often%2520lack%2520anatomical%250Aawareness%2520in%2520bronchoscopy%252C%2520overfitting%2520to%2520local%2520textures%2520rather%2520than%2520capturing%250Athe%2520global%2520airway%2520structure%252C%2520particularly%2520under%2520ambiguous%2520depth%2520cues%2520and%2520poor%250Alighting.%2520To%2520address%2520this%252C%2520we%2520propose%2520Brea-Depth%252C%2520a%2520novel%2520framework%2520that%250Aintegrates%2520airway-specific%2520geometric%2520priors%2520into%2520foundation%2520model%2520adaptation%250Afor%2520bronchoscopic%2520depth%2520estimation.%2520Our%2520method%2520introduces%2520a%2520depth-aware%250ACycleGAN%252C%2520refining%2520the%2520translation%2520between%2520real%2520bronchoscopic%2520images%2520and%2520airway%250Ageometries%2520from%2520anatomical%2520data%252C%2520effectively%2520bridging%2520the%2520domain%2520gap.%2520In%250Aaddition%252C%2520we%2520introduce%2520an%2520airway%2520structure%2520awareness%2520loss%2520to%2520enforce%2520depth%250Aconsistency%2520within%2520the%2520airway%2520lumen%2520while%2520preserving%2520smooth%2520transitions%2520and%250Astructural%2520integrity.%2520By%2520incorporating%2520anatomical%2520priors%252C%2520Brea-Depth%2520enhances%250Amodel%2520generalization%2520and%2520yields%2520more%2520robust%252C%2520accurate%25203D%2520airway%250Areconstructions.%2520To%2520assess%2520anatomical%2520realism%252C%2520we%2520introduce%2520Airway%2520Depth%250AStructure%2520Evaluation%252C%2520a%2520new%2520metric%2520for%2520structural%2520consistency.%2520We%2520validate%250ABREA-Depth%2520on%2520a%2520collected%2520ex%2520vivo%2520human%2520lung%2520dataset%2520and%2520an%2520open%2520bronchoscopic%250Adataset%252C%2520where%2520it%2520outperforms%2520existing%2520methods%2520in%2520anatomical%2520depth%250Apreservation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BREA-Depth%3A%20Bronchoscopy%20Realistic%20Airway-geometric%20Depth%20Estimation&entry.906535625=Francis%20Xiatian%20Zhang%20and%20Emile%20Mackute%20and%20Mohammadreza%20Kasaei%20and%20Kevin%20Dhaliwal%20and%20Robert%20Thomson%20and%20Mohsen%20Khadem&entry.1292438233=%20%20Monocular%20depth%20estimation%20in%20bronchoscopy%20can%20significantly%20improve%0Areal-time%20navigation%20accuracy%20and%20enhance%20the%20safety%20of%20interventions%20in%0Acomplex%2C%20branching%20airways.%20Recent%20advances%20in%20depth%20foundation%20models%20have%0Ashown%20promise%20for%20endoscopic%20scenarios%2C%20yet%20these%20models%20often%20lack%20anatomical%0Aawareness%20in%20bronchoscopy%2C%20overfitting%20to%20local%20textures%20rather%20than%20capturing%0Athe%20global%20airway%20structure%2C%20particularly%20under%20ambiguous%20depth%20cues%20and%20poor%0Alighting.%20To%20address%20this%2C%20we%20propose%20Brea-Depth%2C%20a%20novel%20framework%20that%0Aintegrates%20airway-specific%20geometric%20priors%20into%20foundation%20model%20adaptation%0Afor%20bronchoscopic%20depth%20estimation.%20Our%20method%20introduces%20a%20depth-aware%0ACycleGAN%2C%20refining%20the%20translation%20between%20real%20bronchoscopic%20images%20and%20airway%0Ageometries%20from%20anatomical%20data%2C%20effectively%20bridging%20the%20domain%20gap.%20In%0Aaddition%2C%20we%20introduce%20an%20airway%20structure%20awareness%20loss%20to%20enforce%20depth%0Aconsistency%20within%20the%20airway%20lumen%20while%20preserving%20smooth%20transitions%20and%0Astructural%20integrity.%20By%20incorporating%20anatomical%20priors%2C%20Brea-Depth%20enhances%0Amodel%20generalization%20and%20yields%20more%20robust%2C%20accurate%203D%20airway%0Areconstructions.%20To%20assess%20anatomical%20realism%2C%20we%20introduce%20Airway%20Depth%0AStructure%20Evaluation%2C%20a%20new%20metric%20for%20structural%20consistency.%20We%20validate%0ABREA-Depth%20on%20a%20collected%20ex%20vivo%20human%20lung%20dataset%20and%20an%20open%20bronchoscopic%0Adataset%2C%20where%20it%20outperforms%20existing%20methods%20in%20anatomical%20depth%0Apreservation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11885v1&entry.124074799=Read"},
{"title": "Learning from Uncertain Similarity and Unlabeled Data", "author": "Meng Wei and Zhongnian Li and Peng Ying and Xinzheng Xu", "abstract": "  Existing similarity-based weakly supervised learning approaches often rely on\nprecise similarity annotations between data pairs, which may inadvertently\nexpose sensitive label information and raise privacy risks. To mitigate this\nissue, we propose Uncertain Similarity and Unlabeled Learning (USimUL), a novel\nframework where each similarity pair is embedded with an uncertainty component\nto reduce label leakage. In this paper, we propose an unbiased risk estimator\nthat learns from uncertain similarity and unlabeled data. Additionally, we\ntheoretically prove that the estimator achieves statistically optimal\nparametric convergence rates. Extensive experiments on both benchmark and\nreal-world datasets show that our method achieves superior classification\nperformance compared to conventional similarity-based approaches.\n", "link": "http://arxiv.org/abs/2509.11984v1", "date": "2025-09-15", "relevancy": 2.1471, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5908}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5405}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Uncertain%20Similarity%20and%20Unlabeled%20Data&body=Title%3A%20Learning%20from%20Uncertain%20Similarity%20and%20Unlabeled%20Data%0AAuthor%3A%20Meng%20Wei%20and%20Zhongnian%20Li%20and%20Peng%20Ying%20and%20Xinzheng%20Xu%0AAbstract%3A%20%20%20Existing%20similarity-based%20weakly%20supervised%20learning%20approaches%20often%20rely%20on%0Aprecise%20similarity%20annotations%20between%20data%20pairs%2C%20which%20may%20inadvertently%0Aexpose%20sensitive%20label%20information%20and%20raise%20privacy%20risks.%20To%20mitigate%20this%0Aissue%2C%20we%20propose%20Uncertain%20Similarity%20and%20Unlabeled%20Learning%20%28USimUL%29%2C%20a%20novel%0Aframework%20where%20each%20similarity%20pair%20is%20embedded%20with%20an%20uncertainty%20component%0Ato%20reduce%20label%20leakage.%20In%20this%20paper%2C%20we%20propose%20an%20unbiased%20risk%20estimator%0Athat%20learns%20from%20uncertain%20similarity%20and%20unlabeled%20data.%20Additionally%2C%20we%0Atheoretically%20prove%20that%20the%20estimator%20achieves%20statistically%20optimal%0Aparametric%20convergence%20rates.%20Extensive%20experiments%20on%20both%20benchmark%20and%0Areal-world%20datasets%20show%20that%20our%20method%20achieves%20superior%20classification%0Aperformance%20compared%20to%20conventional%20similarity-based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Uncertain%2520Similarity%2520and%2520Unlabeled%2520Data%26entry.906535625%3DMeng%2520Wei%2520and%2520Zhongnian%2520Li%2520and%2520Peng%2520Ying%2520and%2520Xinzheng%2520Xu%26entry.1292438233%3D%2520%2520Existing%2520similarity-based%2520weakly%2520supervised%2520learning%2520approaches%2520often%2520rely%2520on%250Aprecise%2520similarity%2520annotations%2520between%2520data%2520pairs%252C%2520which%2520may%2520inadvertently%250Aexpose%2520sensitive%2520label%2520information%2520and%2520raise%2520privacy%2520risks.%2520To%2520mitigate%2520this%250Aissue%252C%2520we%2520propose%2520Uncertain%2520Similarity%2520and%2520Unlabeled%2520Learning%2520%2528USimUL%2529%252C%2520a%2520novel%250Aframework%2520where%2520each%2520similarity%2520pair%2520is%2520embedded%2520with%2520an%2520uncertainty%2520component%250Ato%2520reduce%2520label%2520leakage.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520unbiased%2520risk%2520estimator%250Athat%2520learns%2520from%2520uncertain%2520similarity%2520and%2520unlabeled%2520data.%2520Additionally%252C%2520we%250Atheoretically%2520prove%2520that%2520the%2520estimator%2520achieves%2520statistically%2520optimal%250Aparametric%2520convergence%2520rates.%2520Extensive%2520experiments%2520on%2520both%2520benchmark%2520and%250Areal-world%2520datasets%2520show%2520that%2520our%2520method%2520achieves%2520superior%2520classification%250Aperformance%2520compared%2520to%2520conventional%2520similarity-based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Uncertain%20Similarity%20and%20Unlabeled%20Data&entry.906535625=Meng%20Wei%20and%20Zhongnian%20Li%20and%20Peng%20Ying%20and%20Xinzheng%20Xu&entry.1292438233=%20%20Existing%20similarity-based%20weakly%20supervised%20learning%20approaches%20often%20rely%20on%0Aprecise%20similarity%20annotations%20between%20data%20pairs%2C%20which%20may%20inadvertently%0Aexpose%20sensitive%20label%20information%20and%20raise%20privacy%20risks.%20To%20mitigate%20this%0Aissue%2C%20we%20propose%20Uncertain%20Similarity%20and%20Unlabeled%20Learning%20%28USimUL%29%2C%20a%20novel%0Aframework%20where%20each%20similarity%20pair%20is%20embedded%20with%20an%20uncertainty%20component%0Ato%20reduce%20label%20leakage.%20In%20this%20paper%2C%20we%20propose%20an%20unbiased%20risk%20estimator%0Athat%20learns%20from%20uncertain%20similarity%20and%20unlabeled%20data.%20Additionally%2C%20we%0Atheoretically%20prove%20that%20the%20estimator%20achieves%20statistically%20optimal%0Aparametric%20convergence%20rates.%20Extensive%20experiments%20on%20both%20benchmark%20and%0Areal-world%20datasets%20show%20that%20our%20method%20achieves%20superior%20classification%0Aperformance%20compared%20to%20conventional%20similarity-based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11984v1&entry.124074799=Read"},
{"title": "LayerLock: Non-collapsing Representation Learning with Progressive\n  Freezing", "author": "Goker Erdogan and Nikhil Parthasarathy and Catalin Ionescu and Drew Hudson and Alexander Lerchner and Andrew Zisserman and Mehdi Sajjadi and Joao Carreira", "abstract": "  We introduce LayerLock, a simple yet effective approach for self-supervised\nvisual representation learning, that gradually transitions from pixel to latent\nprediction through progressive layer freezing. First, we make the observation\nthat during training of video masked-autoencoding (MAE) models, ViT layers\nconverge in the order of their depth: shallower layers converge early, deeper\nlayers converge late. We then show that this observation can be exploited to\naccelerate standard MAE by progressively freezing the model according to an\nexplicit schedule, throughout training. Furthermore, this same schedule can be\nused in a simple and scalable approach to latent prediction that does not\nsuffer from \"representation collapse\". We apply our proposed approach,\nLayerLock, to large models of up to 4B parameters with results surpassing those\nof non-latent masked prediction on the 4DS perception suite.\n", "link": "http://arxiv.org/abs/2509.10156v2", "date": "2025-09-15", "relevancy": 2.1397, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.561}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5177}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LayerLock%3A%20Non-collapsing%20Representation%20Learning%20with%20Progressive%0A%20%20Freezing&body=Title%3A%20LayerLock%3A%20Non-collapsing%20Representation%20Learning%20with%20Progressive%0A%20%20Freezing%0AAuthor%3A%20Goker%20Erdogan%20and%20Nikhil%20Parthasarathy%20and%20Catalin%20Ionescu%20and%20Drew%20Hudson%20and%20Alexander%20Lerchner%20and%20Andrew%20Zisserman%20and%20Mehdi%20Sajjadi%20and%20Joao%20Carreira%0AAbstract%3A%20%20%20We%20introduce%20LayerLock%2C%20a%20simple%20yet%20effective%20approach%20for%20self-supervised%0Avisual%20representation%20learning%2C%20that%20gradually%20transitions%20from%20pixel%20to%20latent%0Aprediction%20through%20progressive%20layer%20freezing.%20First%2C%20we%20make%20the%20observation%0Athat%20during%20training%20of%20video%20masked-autoencoding%20%28MAE%29%20models%2C%20ViT%20layers%0Aconverge%20in%20the%20order%20of%20their%20depth%3A%20shallower%20layers%20converge%20early%2C%20deeper%0Alayers%20converge%20late.%20We%20then%20show%20that%20this%20observation%20can%20be%20exploited%20to%0Aaccelerate%20standard%20MAE%20by%20progressively%20freezing%20the%20model%20according%20to%20an%0Aexplicit%20schedule%2C%20throughout%20training.%20Furthermore%2C%20this%20same%20schedule%20can%20be%0Aused%20in%20a%20simple%20and%20scalable%20approach%20to%20latent%20prediction%20that%20does%20not%0Asuffer%20from%20%22representation%20collapse%22.%20We%20apply%20our%20proposed%20approach%2C%0ALayerLock%2C%20to%20large%20models%20of%20up%20to%204B%20parameters%20with%20results%20surpassing%20those%0Aof%20non-latent%20masked%20prediction%20on%20the%204DS%20perception%20suite.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.10156v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayerLock%253A%2520Non-collapsing%2520Representation%2520Learning%2520with%2520Progressive%250A%2520%2520Freezing%26entry.906535625%3DGoker%2520Erdogan%2520and%2520Nikhil%2520Parthasarathy%2520and%2520Catalin%2520Ionescu%2520and%2520Drew%2520Hudson%2520and%2520Alexander%2520Lerchner%2520and%2520Andrew%2520Zisserman%2520and%2520Mehdi%2520Sajjadi%2520and%2520Joao%2520Carreira%26entry.1292438233%3D%2520%2520We%2520introduce%2520LayerLock%252C%2520a%2520simple%2520yet%2520effective%2520approach%2520for%2520self-supervised%250Avisual%2520representation%2520learning%252C%2520that%2520gradually%2520transitions%2520from%2520pixel%2520to%2520latent%250Aprediction%2520through%2520progressive%2520layer%2520freezing.%2520First%252C%2520we%2520make%2520the%2520observation%250Athat%2520during%2520training%2520of%2520video%2520masked-autoencoding%2520%2528MAE%2529%2520models%252C%2520ViT%2520layers%250Aconverge%2520in%2520the%2520order%2520of%2520their%2520depth%253A%2520shallower%2520layers%2520converge%2520early%252C%2520deeper%250Alayers%2520converge%2520late.%2520We%2520then%2520show%2520that%2520this%2520observation%2520can%2520be%2520exploited%2520to%250Aaccelerate%2520standard%2520MAE%2520by%2520progressively%2520freezing%2520the%2520model%2520according%2520to%2520an%250Aexplicit%2520schedule%252C%2520throughout%2520training.%2520Furthermore%252C%2520this%2520same%2520schedule%2520can%2520be%250Aused%2520in%2520a%2520simple%2520and%2520scalable%2520approach%2520to%2520latent%2520prediction%2520that%2520does%2520not%250Asuffer%2520from%2520%2522representation%2520collapse%2522.%2520We%2520apply%2520our%2520proposed%2520approach%252C%250ALayerLock%252C%2520to%2520large%2520models%2520of%2520up%2520to%25204B%2520parameters%2520with%2520results%2520surpassing%2520those%250Aof%2520non-latent%2520masked%2520prediction%2520on%2520the%25204DS%2520perception%2520suite.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10156v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LayerLock%3A%20Non-collapsing%20Representation%20Learning%20with%20Progressive%0A%20%20Freezing&entry.906535625=Goker%20Erdogan%20and%20Nikhil%20Parthasarathy%20and%20Catalin%20Ionescu%20and%20Drew%20Hudson%20and%20Alexander%20Lerchner%20and%20Andrew%20Zisserman%20and%20Mehdi%20Sajjadi%20and%20Joao%20Carreira&entry.1292438233=%20%20We%20introduce%20LayerLock%2C%20a%20simple%20yet%20effective%20approach%20for%20self-supervised%0Avisual%20representation%20learning%2C%20that%20gradually%20transitions%20from%20pixel%20to%20latent%0Aprediction%20through%20progressive%20layer%20freezing.%20First%2C%20we%20make%20the%20observation%0Athat%20during%20training%20of%20video%20masked-autoencoding%20%28MAE%29%20models%2C%20ViT%20layers%0Aconverge%20in%20the%20order%20of%20their%20depth%3A%20shallower%20layers%20converge%20early%2C%20deeper%0Alayers%20converge%20late.%20We%20then%20show%20that%20this%20observation%20can%20be%20exploited%20to%0Aaccelerate%20standard%20MAE%20by%20progressively%20freezing%20the%20model%20according%20to%20an%0Aexplicit%20schedule%2C%20throughout%20training.%20Furthermore%2C%20this%20same%20schedule%20can%20be%0Aused%20in%20a%20simple%20and%20scalable%20approach%20to%20latent%20prediction%20that%20does%20not%0Asuffer%20from%20%22representation%20collapse%22.%20We%20apply%20our%20proposed%20approach%2C%0ALayerLock%2C%20to%20large%20models%20of%20up%20to%204B%20parameters%20with%20results%20surpassing%20those%0Aof%20non-latent%20masked%20prediction%20on%20the%204DS%20perception%20suite.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.10156v2&entry.124074799=Read"},
{"title": "Earth Observation Foundation Model PhilEO: Pretraining on the MajorTOM\n  and FastTOM Datasets", "author": "Nikolaos Dionelis and Jente Bosmans and Riccardo Musto and Giancarlo Paoletti and Simone Sarti and Giacomo Cascarano and Casper Fibaek and Luke Camilleri and Bertrand Le Saux and Nicolas Long\u00e9p\u00e9", "abstract": "  Today, Earth Observation (EO) satellites generate massive volumes of data,\nwith the Copernicus Sentinel-2 constellation alone producing approximately\n1.6TB per day. To fully exploit this information, it is essential to pretrain\nEO Foundation Models (FMs) on large unlabeled datasets, enabling efficient\nfine-tuning for several different downstream tasks with minimal labeled data.\nIn this work, we present the scaling-up of our recently proposed EO Foundation\nModel, PhilEO Geo-Aware U-Net, on the unlabeled 23TB dataset MajorTOM, which\ncovers the vast majority of the Earth's surface, as well as on the specialized\nsubset FastTOM 2TB that does not include oceans and ice. We develop and study\nvarious PhilEO model variants with different numbers of parameters and\narchitectures.\n  We fine-tune the models on the PhilEO Bench for road density estimation,\nbuilding density pixel-wise regression, and land cover semantic segmentation,\nand we evaluate the performance. Our results demonstrate that for all n-shots\nfor road density regression, the PhilEO 44M MajorTOM 23TB model outperforms\nPhilEO Globe 0.5TB 44M. We also show that for most n-shots for road density\nestimation and building density regression, PhilEO 200M FastTOM outperforms all\nthe other models we examine. The effectiveness of both dataset and model\nscaling is validated using the PhilEO Bench. We also study the impact of\narchitecture scaling, transitioning from U-Net Convolutional Neural Networks\n(CNN) to Vision Transformers (ViT).\n", "link": "http://arxiv.org/abs/2506.14765v3", "date": "2025-09-15", "relevancy": 2.1371, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5419}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Earth%20Observation%20Foundation%20Model%20PhilEO%3A%20Pretraining%20on%20the%20MajorTOM%0A%20%20and%20FastTOM%20Datasets&body=Title%3A%20Earth%20Observation%20Foundation%20Model%20PhilEO%3A%20Pretraining%20on%20the%20MajorTOM%0A%20%20and%20FastTOM%20Datasets%0AAuthor%3A%20Nikolaos%20Dionelis%20and%20Jente%20Bosmans%20and%20Riccardo%20Musto%20and%20Giancarlo%20Paoletti%20and%20Simone%20Sarti%20and%20Giacomo%20Cascarano%20and%20Casper%20Fibaek%20and%20Luke%20Camilleri%20and%20Bertrand%20Le%20Saux%20and%20Nicolas%20Long%C3%A9p%C3%A9%0AAbstract%3A%20%20%20Today%2C%20Earth%20Observation%20%28EO%29%20satellites%20generate%20massive%20volumes%20of%20data%2C%0Awith%20the%20Copernicus%20Sentinel-2%20constellation%20alone%20producing%20approximately%0A1.6TB%20per%20day.%20To%20fully%20exploit%20this%20information%2C%20it%20is%20essential%20to%20pretrain%0AEO%20Foundation%20Models%20%28FMs%29%20on%20large%20unlabeled%20datasets%2C%20enabling%20efficient%0Afine-tuning%20for%20several%20different%20downstream%20tasks%20with%20minimal%20labeled%20data.%0AIn%20this%20work%2C%20we%20present%20the%20scaling-up%20of%20our%20recently%20proposed%20EO%20Foundation%0AModel%2C%20PhilEO%20Geo-Aware%20U-Net%2C%20on%20the%20unlabeled%2023TB%20dataset%20MajorTOM%2C%20which%0Acovers%20the%20vast%20majority%20of%20the%20Earth%27s%20surface%2C%20as%20well%20as%20on%20the%20specialized%0Asubset%20FastTOM%202TB%20that%20does%20not%20include%20oceans%20and%20ice.%20We%20develop%20and%20study%0Avarious%20PhilEO%20model%20variants%20with%20different%20numbers%20of%20parameters%20and%0Aarchitectures.%0A%20%20We%20fine-tune%20the%20models%20on%20the%20PhilEO%20Bench%20for%20road%20density%20estimation%2C%0Abuilding%20density%20pixel-wise%20regression%2C%20and%20land%20cover%20semantic%20segmentation%2C%0Aand%20we%20evaluate%20the%20performance.%20Our%20results%20demonstrate%20that%20for%20all%20n-shots%0Afor%20road%20density%20regression%2C%20the%20PhilEO%2044M%20MajorTOM%2023TB%20model%20outperforms%0APhilEO%20Globe%200.5TB%2044M.%20We%20also%20show%20that%20for%20most%20n-shots%20for%20road%20density%0Aestimation%20and%20building%20density%20regression%2C%20PhilEO%20200M%20FastTOM%20outperforms%20all%0Athe%20other%20models%20we%20examine.%20The%20effectiveness%20of%20both%20dataset%20and%20model%0Ascaling%20is%20validated%20using%20the%20PhilEO%20Bench.%20We%20also%20study%20the%20impact%20of%0Aarchitecture%20scaling%2C%20transitioning%20from%20U-Net%20Convolutional%20Neural%20Networks%0A%28CNN%29%20to%20Vision%20Transformers%20%28ViT%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.14765v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarth%2520Observation%2520Foundation%2520Model%2520PhilEO%253A%2520Pretraining%2520on%2520the%2520MajorTOM%250A%2520%2520and%2520FastTOM%2520Datasets%26entry.906535625%3DNikolaos%2520Dionelis%2520and%2520Jente%2520Bosmans%2520and%2520Riccardo%2520Musto%2520and%2520Giancarlo%2520Paoletti%2520and%2520Simone%2520Sarti%2520and%2520Giacomo%2520Cascarano%2520and%2520Casper%2520Fibaek%2520and%2520Luke%2520Camilleri%2520and%2520Bertrand%2520Le%2520Saux%2520and%2520Nicolas%2520Long%25C3%25A9p%25C3%25A9%26entry.1292438233%3D%2520%2520Today%252C%2520Earth%2520Observation%2520%2528EO%2529%2520satellites%2520generate%2520massive%2520volumes%2520of%2520data%252C%250Awith%2520the%2520Copernicus%2520Sentinel-2%2520constellation%2520alone%2520producing%2520approximately%250A1.6TB%2520per%2520day.%2520To%2520fully%2520exploit%2520this%2520information%252C%2520it%2520is%2520essential%2520to%2520pretrain%250AEO%2520Foundation%2520Models%2520%2528FMs%2529%2520on%2520large%2520unlabeled%2520datasets%252C%2520enabling%2520efficient%250Afine-tuning%2520for%2520several%2520different%2520downstream%2520tasks%2520with%2520minimal%2520labeled%2520data.%250AIn%2520this%2520work%252C%2520we%2520present%2520the%2520scaling-up%2520of%2520our%2520recently%2520proposed%2520EO%2520Foundation%250AModel%252C%2520PhilEO%2520Geo-Aware%2520U-Net%252C%2520on%2520the%2520unlabeled%252023TB%2520dataset%2520MajorTOM%252C%2520which%250Acovers%2520the%2520vast%2520majority%2520of%2520the%2520Earth%2527s%2520surface%252C%2520as%2520well%2520as%2520on%2520the%2520specialized%250Asubset%2520FastTOM%25202TB%2520that%2520does%2520not%2520include%2520oceans%2520and%2520ice.%2520We%2520develop%2520and%2520study%250Avarious%2520PhilEO%2520model%2520variants%2520with%2520different%2520numbers%2520of%2520parameters%2520and%250Aarchitectures.%250A%2520%2520We%2520fine-tune%2520the%2520models%2520on%2520the%2520PhilEO%2520Bench%2520for%2520road%2520density%2520estimation%252C%250Abuilding%2520density%2520pixel-wise%2520regression%252C%2520and%2520land%2520cover%2520semantic%2520segmentation%252C%250Aand%2520we%2520evaluate%2520the%2520performance.%2520Our%2520results%2520demonstrate%2520that%2520for%2520all%2520n-shots%250Afor%2520road%2520density%2520regression%252C%2520the%2520PhilEO%252044M%2520MajorTOM%252023TB%2520model%2520outperforms%250APhilEO%2520Globe%25200.5TB%252044M.%2520We%2520also%2520show%2520that%2520for%2520most%2520n-shots%2520for%2520road%2520density%250Aestimation%2520and%2520building%2520density%2520regression%252C%2520PhilEO%2520200M%2520FastTOM%2520outperforms%2520all%250Athe%2520other%2520models%2520we%2520examine.%2520The%2520effectiveness%2520of%2520both%2520dataset%2520and%2520model%250Ascaling%2520is%2520validated%2520using%2520the%2520PhilEO%2520Bench.%2520We%2520also%2520study%2520the%2520impact%2520of%250Aarchitecture%2520scaling%252C%2520transitioning%2520from%2520U-Net%2520Convolutional%2520Neural%2520Networks%250A%2528CNN%2529%2520to%2520Vision%2520Transformers%2520%2528ViT%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14765v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Earth%20Observation%20Foundation%20Model%20PhilEO%3A%20Pretraining%20on%20the%20MajorTOM%0A%20%20and%20FastTOM%20Datasets&entry.906535625=Nikolaos%20Dionelis%20and%20Jente%20Bosmans%20and%20Riccardo%20Musto%20and%20Giancarlo%20Paoletti%20and%20Simone%20Sarti%20and%20Giacomo%20Cascarano%20and%20Casper%20Fibaek%20and%20Luke%20Camilleri%20and%20Bertrand%20Le%20Saux%20and%20Nicolas%20Long%C3%A9p%C3%A9&entry.1292438233=%20%20Today%2C%20Earth%20Observation%20%28EO%29%20satellites%20generate%20massive%20volumes%20of%20data%2C%0Awith%20the%20Copernicus%20Sentinel-2%20constellation%20alone%20producing%20approximately%0A1.6TB%20per%20day.%20To%20fully%20exploit%20this%20information%2C%20it%20is%20essential%20to%20pretrain%0AEO%20Foundation%20Models%20%28FMs%29%20on%20large%20unlabeled%20datasets%2C%20enabling%20efficient%0Afine-tuning%20for%20several%20different%20downstream%20tasks%20with%20minimal%20labeled%20data.%0AIn%20this%20work%2C%20we%20present%20the%20scaling-up%20of%20our%20recently%20proposed%20EO%20Foundation%0AModel%2C%20PhilEO%20Geo-Aware%20U-Net%2C%20on%20the%20unlabeled%2023TB%20dataset%20MajorTOM%2C%20which%0Acovers%20the%20vast%20majority%20of%20the%20Earth%27s%20surface%2C%20as%20well%20as%20on%20the%20specialized%0Asubset%20FastTOM%202TB%20that%20does%20not%20include%20oceans%20and%20ice.%20We%20develop%20and%20study%0Avarious%20PhilEO%20model%20variants%20with%20different%20numbers%20of%20parameters%20and%0Aarchitectures.%0A%20%20We%20fine-tune%20the%20models%20on%20the%20PhilEO%20Bench%20for%20road%20density%20estimation%2C%0Abuilding%20density%20pixel-wise%20regression%2C%20and%20land%20cover%20semantic%20segmentation%2C%0Aand%20we%20evaluate%20the%20performance.%20Our%20results%20demonstrate%20that%20for%20all%20n-shots%0Afor%20road%20density%20regression%2C%20the%20PhilEO%2044M%20MajorTOM%2023TB%20model%20outperforms%0APhilEO%20Globe%200.5TB%2044M.%20We%20also%20show%20that%20for%20most%20n-shots%20for%20road%20density%0Aestimation%20and%20building%20density%20regression%2C%20PhilEO%20200M%20FastTOM%20outperforms%20all%0Athe%20other%20models%20we%20examine.%20The%20effectiveness%20of%20both%20dataset%20and%20model%0Ascaling%20is%20validated%20using%20the%20PhilEO%20Bench.%20We%20also%20study%20the%20impact%20of%0Aarchitecture%20scaling%2C%20transitioning%20from%20U-Net%20Convolutional%20Neural%20Networks%0A%28CNN%29%20to%20Vision%20Transformers%20%28ViT%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.14765v3&entry.124074799=Read"},
{"title": "MAFS: Masked Autoencoder for Infrared-Visible Image Fusion and Semantic\n  Segmentation", "author": "Liying Wang and Xiaoli Zhang and Chuanmin Jia and Siwei Ma", "abstract": "  Infrared-visible image fusion methods aim at generating fused images with\ngood visual quality and also facilitate the performance of high-level tasks.\nIndeed, existing semantic-driven methods have considered semantic information\ninjection for downstream applications. However, none of them investigates the\npotential for reciprocal promotion between pixel-wise image fusion and\ncross-modal feature fusion perception tasks from a macroscopic task-level\nperspective. To address this limitation, we propose a unified network for image\nfusion and semantic segmentation. MAFS is a parallel structure, containing a\nfusion sub-network and a segmentation sub-network. On the one hand, We devise a\nheterogeneous feature fusion strategy to enhance semantic-aware capabilities\nfor image fusion. On the other hand, by cascading the fusion sub-network and a\nsegmentation backbone, segmentation-related knowledge is transferred to promote\nfeature-level fusion-based segmentation. Within the framework, we design a\nnovel multi-stage Transformer decoder to aggregate fine-grained multi-scale\nfused features efficiently. Additionally, a dynamic factor based on the max-min\nfairness allocation principle is introduced to generate adaptive weights of two\ntasks and guarantee smooth training in a multi-task manner. Extensive\nexperiments demonstrate that our approach achieves competitive results compared\nwith state-of-the-art methods. The code is available at\nhttps://github.com/Abraham-Einstein/MAFS/.\n", "link": "http://arxiv.org/abs/2509.11817v1", "date": "2025-09-15", "relevancy": 2.1198, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5371}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5326}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAFS%3A%20Masked%20Autoencoder%20for%20Infrared-Visible%20Image%20Fusion%20and%20Semantic%0A%20%20Segmentation&body=Title%3A%20MAFS%3A%20Masked%20Autoencoder%20for%20Infrared-Visible%20Image%20Fusion%20and%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Liying%20Wang%20and%20Xiaoli%20Zhang%20and%20Chuanmin%20Jia%20and%20Siwei%20Ma%0AAbstract%3A%20%20%20Infrared-visible%20image%20fusion%20methods%20aim%20at%20generating%20fused%20images%20with%0Agood%20visual%20quality%20and%20also%20facilitate%20the%20performance%20of%20high-level%20tasks.%0AIndeed%2C%20existing%20semantic-driven%20methods%20have%20considered%20semantic%20information%0Ainjection%20for%20downstream%20applications.%20However%2C%20none%20of%20them%20investigates%20the%0Apotential%20for%20reciprocal%20promotion%20between%20pixel-wise%20image%20fusion%20and%0Across-modal%20feature%20fusion%20perception%20tasks%20from%20a%20macroscopic%20task-level%0Aperspective.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20unified%20network%20for%20image%0Afusion%20and%20semantic%20segmentation.%20MAFS%20is%20a%20parallel%20structure%2C%20containing%20a%0Afusion%20sub-network%20and%20a%20segmentation%20sub-network.%20On%20the%20one%20hand%2C%20We%20devise%20a%0Aheterogeneous%20feature%20fusion%20strategy%20to%20enhance%20semantic-aware%20capabilities%0Afor%20image%20fusion.%20On%20the%20other%20hand%2C%20by%20cascading%20the%20fusion%20sub-network%20and%20a%0Asegmentation%20backbone%2C%20segmentation-related%20knowledge%20is%20transferred%20to%20promote%0Afeature-level%20fusion-based%20segmentation.%20Within%20the%20framework%2C%20we%20design%20a%0Anovel%20multi-stage%20Transformer%20decoder%20to%20aggregate%20fine-grained%20multi-scale%0Afused%20features%20efficiently.%20Additionally%2C%20a%20dynamic%20factor%20based%20on%20the%20max-min%0Afairness%20allocation%20principle%20is%20introduced%20to%20generate%20adaptive%20weights%20of%20two%0Atasks%20and%20guarantee%20smooth%20training%20in%20a%20multi-task%20manner.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20achieves%20competitive%20results%20compared%0Awith%20state-of-the-art%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Abraham-Einstein/MAFS/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAFS%253A%2520Masked%2520Autoencoder%2520for%2520Infrared-Visible%2520Image%2520Fusion%2520and%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DLiying%2520Wang%2520and%2520Xiaoli%2520Zhang%2520and%2520Chuanmin%2520Jia%2520and%2520Siwei%2520Ma%26entry.1292438233%3D%2520%2520Infrared-visible%2520image%2520fusion%2520methods%2520aim%2520at%2520generating%2520fused%2520images%2520with%250Agood%2520visual%2520quality%2520and%2520also%2520facilitate%2520the%2520performance%2520of%2520high-level%2520tasks.%250AIndeed%252C%2520existing%2520semantic-driven%2520methods%2520have%2520considered%2520semantic%2520information%250Ainjection%2520for%2520downstream%2520applications.%2520However%252C%2520none%2520of%2520them%2520investigates%2520the%250Apotential%2520for%2520reciprocal%2520promotion%2520between%2520pixel-wise%2520image%2520fusion%2520and%250Across-modal%2520feature%2520fusion%2520perception%2520tasks%2520from%2520a%2520macroscopic%2520task-level%250Aperspective.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520unified%2520network%2520for%2520image%250Afusion%2520and%2520semantic%2520segmentation.%2520MAFS%2520is%2520a%2520parallel%2520structure%252C%2520containing%2520a%250Afusion%2520sub-network%2520and%2520a%2520segmentation%2520sub-network.%2520On%2520the%2520one%2520hand%252C%2520We%2520devise%2520a%250Aheterogeneous%2520feature%2520fusion%2520strategy%2520to%2520enhance%2520semantic-aware%2520capabilities%250Afor%2520image%2520fusion.%2520On%2520the%2520other%2520hand%252C%2520by%2520cascading%2520the%2520fusion%2520sub-network%2520and%2520a%250Asegmentation%2520backbone%252C%2520segmentation-related%2520knowledge%2520is%2520transferred%2520to%2520promote%250Afeature-level%2520fusion-based%2520segmentation.%2520Within%2520the%2520framework%252C%2520we%2520design%2520a%250Anovel%2520multi-stage%2520Transformer%2520decoder%2520to%2520aggregate%2520fine-grained%2520multi-scale%250Afused%2520features%2520efficiently.%2520Additionally%252C%2520a%2520dynamic%2520factor%2520based%2520on%2520the%2520max-min%250Afairness%2520allocation%2520principle%2520is%2520introduced%2520to%2520generate%2520adaptive%2520weights%2520of%2520two%250Atasks%2520and%2520guarantee%2520smooth%2520training%2520in%2520a%2520multi-task%2520manner.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520competitive%2520results%2520compared%250Awith%2520state-of-the-art%2520methods.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Abraham-Einstein/MAFS/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAFS%3A%20Masked%20Autoencoder%20for%20Infrared-Visible%20Image%20Fusion%20and%20Semantic%0A%20%20Segmentation&entry.906535625=Liying%20Wang%20and%20Xiaoli%20Zhang%20and%20Chuanmin%20Jia%20and%20Siwei%20Ma&entry.1292438233=%20%20Infrared-visible%20image%20fusion%20methods%20aim%20at%20generating%20fused%20images%20with%0Agood%20visual%20quality%20and%20also%20facilitate%20the%20performance%20of%20high-level%20tasks.%0AIndeed%2C%20existing%20semantic-driven%20methods%20have%20considered%20semantic%20information%0Ainjection%20for%20downstream%20applications.%20However%2C%20none%20of%20them%20investigates%20the%0Apotential%20for%20reciprocal%20promotion%20between%20pixel-wise%20image%20fusion%20and%0Across-modal%20feature%20fusion%20perception%20tasks%20from%20a%20macroscopic%20task-level%0Aperspective.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20unified%20network%20for%20image%0Afusion%20and%20semantic%20segmentation.%20MAFS%20is%20a%20parallel%20structure%2C%20containing%20a%0Afusion%20sub-network%20and%20a%20segmentation%20sub-network.%20On%20the%20one%20hand%2C%20We%20devise%20a%0Aheterogeneous%20feature%20fusion%20strategy%20to%20enhance%20semantic-aware%20capabilities%0Afor%20image%20fusion.%20On%20the%20other%20hand%2C%20by%20cascading%20the%20fusion%20sub-network%20and%20a%0Asegmentation%20backbone%2C%20segmentation-related%20knowledge%20is%20transferred%20to%20promote%0Afeature-level%20fusion-based%20segmentation.%20Within%20the%20framework%2C%20we%20design%20a%0Anovel%20multi-stage%20Transformer%20decoder%20to%20aggregate%20fine-grained%20multi-scale%0Afused%20features%20efficiently.%20Additionally%2C%20a%20dynamic%20factor%20based%20on%20the%20max-min%0Afairness%20allocation%20principle%20is%20introduced%20to%20generate%20adaptive%20weights%20of%20two%0Atasks%20and%20guarantee%20smooth%20training%20in%20a%20multi-task%20manner.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20achieves%20competitive%20results%20compared%0Awith%20state-of-the-art%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Abraham-Einstein/MAFS/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11817v1&entry.124074799=Read"},
{"title": "Safety Pretraining: Toward the Next Generation of Safe AI", "author": "Pratyush Maini and Sachin Goyal and Dylan Sam and Alex Robey and Yash Savani and Yiding Jiang and Andy Zou and Matt Fredrikson and Zacharcy C. Lipton and J. Zico Kolter", "abstract": "  As large language models (LLMs) are increasingly deployed in high-stakes\nsettings, the risk of generating harmful or toxic content remains a central\nchallenge. Post-hoc alignment methods are brittle: once unsafe patterns are\nlearned during pretraining, they are hard to remove. In this work, we present a\ndata-centric pretraining framework that builds safety into the model from the\nstart. Our framework consists of four key steps: (i) Safety Filtering: building\na safety classifier to classify webdata into safe and unsafe categories; (ii)\nSafety Rephrasing: we recontextualize unsafe webdata into safer narratives;\n(iii) Native Refusal: we develop RefuseWeb and Moral Education pretraining\ndatasets that actively teach model to refuse on unsafe content and the moral\nreasoning behind it, and (iv) Harmfulness-Tag annotated pretraining: we flag\nunsafe content during pretraining using a special token, and use it to steer\nmodel away from unsafe generations at inference. Our safety-pretrained models\nreduce attack success rates from 38.8\\% to 8.4\\% on standard LLM safety\nbenchmarks with no performance degradation on general tasks.\n", "link": "http://arxiv.org/abs/2504.16980v2", "date": "2025-09-15", "relevancy": 2.1101, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5317}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5275}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safety%20Pretraining%3A%20Toward%20the%20Next%20Generation%20of%20Safe%20AI&body=Title%3A%20Safety%20Pretraining%3A%20Toward%20the%20Next%20Generation%20of%20Safe%20AI%0AAuthor%3A%20Pratyush%20Maini%20and%20Sachin%20Goyal%20and%20Dylan%20Sam%20and%20Alex%20Robey%20and%20Yash%20Savani%20and%20Yiding%20Jiang%20and%20Andy%20Zou%20and%20Matt%20Fredrikson%20and%20Zacharcy%20C.%20Lipton%20and%20J.%20Zico%20Kolter%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20high-stakes%0Asettings%2C%20the%20risk%20of%20generating%20harmful%20or%20toxic%20content%20remains%20a%20central%0Achallenge.%20Post-hoc%20alignment%20methods%20are%20brittle%3A%20once%20unsafe%20patterns%20are%0Alearned%20during%20pretraining%2C%20they%20are%20hard%20to%20remove.%20In%20this%20work%2C%20we%20present%20a%0Adata-centric%20pretraining%20framework%20that%20builds%20safety%20into%20the%20model%20from%20the%0Astart.%20Our%20framework%20consists%20of%20four%20key%20steps%3A%20%28i%29%20Safety%20Filtering%3A%20building%0Aa%20safety%20classifier%20to%20classify%20webdata%20into%20safe%20and%20unsafe%20categories%3B%20%28ii%29%0ASafety%20Rephrasing%3A%20we%20recontextualize%20unsafe%20webdata%20into%20safer%20narratives%3B%0A%28iii%29%20Native%20Refusal%3A%20we%20develop%20RefuseWeb%20and%20Moral%20Education%20pretraining%0Adatasets%20that%20actively%20teach%20model%20to%20refuse%20on%20unsafe%20content%20and%20the%20moral%0Areasoning%20behind%20it%2C%20and%20%28iv%29%20Harmfulness-Tag%20annotated%20pretraining%3A%20we%20flag%0Aunsafe%20content%20during%20pretraining%20using%20a%20special%20token%2C%20and%20use%20it%20to%20steer%0Amodel%20away%20from%20unsafe%20generations%20at%20inference.%20Our%20safety-pretrained%20models%0Areduce%20attack%20success%20rates%20from%2038.8%5C%25%20to%208.4%5C%25%20on%20standard%20LLM%20safety%0Abenchmarks%20with%20no%20performance%20degradation%20on%20general%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16980v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafety%2520Pretraining%253A%2520Toward%2520the%2520Next%2520Generation%2520of%2520Safe%2520AI%26entry.906535625%3DPratyush%2520Maini%2520and%2520Sachin%2520Goyal%2520and%2520Dylan%2520Sam%2520and%2520Alex%2520Robey%2520and%2520Yash%2520Savani%2520and%2520Yiding%2520Jiang%2520and%2520Andy%2520Zou%2520and%2520Matt%2520Fredrikson%2520and%2520Zacharcy%2520C.%2520Lipton%2520and%2520J.%2520Zico%2520Kolter%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520high-stakes%250Asettings%252C%2520the%2520risk%2520of%2520generating%2520harmful%2520or%2520toxic%2520content%2520remains%2520a%2520central%250Achallenge.%2520Post-hoc%2520alignment%2520methods%2520are%2520brittle%253A%2520once%2520unsafe%2520patterns%2520are%250Alearned%2520during%2520pretraining%252C%2520they%2520are%2520hard%2520to%2520remove.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Adata-centric%2520pretraining%2520framework%2520that%2520builds%2520safety%2520into%2520the%2520model%2520from%2520the%250Astart.%2520Our%2520framework%2520consists%2520of%2520four%2520key%2520steps%253A%2520%2528i%2529%2520Safety%2520Filtering%253A%2520building%250Aa%2520safety%2520classifier%2520to%2520classify%2520webdata%2520into%2520safe%2520and%2520unsafe%2520categories%253B%2520%2528ii%2529%250ASafety%2520Rephrasing%253A%2520we%2520recontextualize%2520unsafe%2520webdata%2520into%2520safer%2520narratives%253B%250A%2528iii%2529%2520Native%2520Refusal%253A%2520we%2520develop%2520RefuseWeb%2520and%2520Moral%2520Education%2520pretraining%250Adatasets%2520that%2520actively%2520teach%2520model%2520to%2520refuse%2520on%2520unsafe%2520content%2520and%2520the%2520moral%250Areasoning%2520behind%2520it%252C%2520and%2520%2528iv%2529%2520Harmfulness-Tag%2520annotated%2520pretraining%253A%2520we%2520flag%250Aunsafe%2520content%2520during%2520pretraining%2520using%2520a%2520special%2520token%252C%2520and%2520use%2520it%2520to%2520steer%250Amodel%2520away%2520from%2520unsafe%2520generations%2520at%2520inference.%2520Our%2520safety-pretrained%2520models%250Areduce%2520attack%2520success%2520rates%2520from%252038.8%255C%2525%2520to%25208.4%255C%2525%2520on%2520standard%2520LLM%2520safety%250Abenchmarks%2520with%2520no%2520performance%2520degradation%2520on%2520general%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16980v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety%20Pretraining%3A%20Toward%20the%20Next%20Generation%20of%20Safe%20AI&entry.906535625=Pratyush%20Maini%20and%20Sachin%20Goyal%20and%20Dylan%20Sam%20and%20Alex%20Robey%20and%20Yash%20Savani%20and%20Yiding%20Jiang%20and%20Andy%20Zou%20and%20Matt%20Fredrikson%20and%20Zacharcy%20C.%20Lipton%20and%20J.%20Zico%20Kolter&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20high-stakes%0Asettings%2C%20the%20risk%20of%20generating%20harmful%20or%20toxic%20content%20remains%20a%20central%0Achallenge.%20Post-hoc%20alignment%20methods%20are%20brittle%3A%20once%20unsafe%20patterns%20are%0Alearned%20during%20pretraining%2C%20they%20are%20hard%20to%20remove.%20In%20this%20work%2C%20we%20present%20a%0Adata-centric%20pretraining%20framework%20that%20builds%20safety%20into%20the%20model%20from%20the%0Astart.%20Our%20framework%20consists%20of%20four%20key%20steps%3A%20%28i%29%20Safety%20Filtering%3A%20building%0Aa%20safety%20classifier%20to%20classify%20webdata%20into%20safe%20and%20unsafe%20categories%3B%20%28ii%29%0ASafety%20Rephrasing%3A%20we%20recontextualize%20unsafe%20webdata%20into%20safer%20narratives%3B%0A%28iii%29%20Native%20Refusal%3A%20we%20develop%20RefuseWeb%20and%20Moral%20Education%20pretraining%0Adatasets%20that%20actively%20teach%20model%20to%20refuse%20on%20unsafe%20content%20and%20the%20moral%0Areasoning%20behind%20it%2C%20and%20%28iv%29%20Harmfulness-Tag%20annotated%20pretraining%3A%20we%20flag%0Aunsafe%20content%20during%20pretraining%20using%20a%20special%20token%2C%20and%20use%20it%20to%20steer%0Amodel%20away%20from%20unsafe%20generations%20at%20inference.%20Our%20safety-pretrained%20models%0Areduce%20attack%20success%20rates%20from%2038.8%5C%25%20to%208.4%5C%25%20on%20standard%20LLM%20safety%0Abenchmarks%20with%20no%20performance%20degradation%20on%20general%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16980v2&entry.124074799=Read"},
{"title": "Foundational theory for optimal decision tree problems. II. Optimal\n  hypersurface decision tree algorithm", "author": "Xi He", "abstract": "  Decision trees are a ubiquitous model for classification and regression tasks\ndue to their interpretability and efficiency. However, solving the optimal\ndecision tree (ODT) problem remains a challenging combinatorial optimization\ntask. Even for the simplest splitting rules--axis-parallel hyperplanes--it is\nNP-hard to optimize. In Part I of this series, we rigorously defined the proper\ndecision tree model through four axioms and, based on these, introduced four\nformal definitions of the ODT problem. From these definitions, we derived four\ngeneric algorithms capable of solving ODT problems for arbitrary decision trees\nsatisfying the axioms. We also analyzed the combinatorial geometric properties\nof hypersurfaces, showing that decision trees defined by polynomial\nhypersurface splitting rules satisfy the proper axioms that we proposed.\n  In this second paper (Part II) of this two-part series, building on the\nalgorithmic and geometric foundations established in Part I, we introduce the\nfirst hypersurface decision tree (HODT) algorithm. To the best of our\nknowledge, existing optimal decision tree methods are, to date, limited to\nhyperplane splitting rules--a special case of hypersurfaces--and rely on\ngeneral-purpose solvers. In contrast, our HODT algorithm addresses the general\nhypersurface decision tree model without requiring external solvers.\n  Using synthetic datasets generated from ground-truth hyperplane decision\ntrees, we vary tree size, data size, dimensionality, and label and feature\nnoise. Results showing that our algorithm recovers the ground truth more\naccurately than axis-parallel trees and exhibits greater robustness to noise.\nWe also analyzed generalization performance across 30 real-world datasets,\nshowing that HODT can achieve up to 30% higher accuracy than the\nstate-of-the-art optimal axis-parallel decision tree algorithm when tree\ncomplexity is properly controlled.\n", "link": "http://arxiv.org/abs/2509.12057v1", "date": "2025-09-15", "relevancy": 2.1082, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4237}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4237}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundational%20theory%20for%20optimal%20decision%20tree%20problems.%20II.%20Optimal%0A%20%20hypersurface%20decision%20tree%20algorithm&body=Title%3A%20Foundational%20theory%20for%20optimal%20decision%20tree%20problems.%20II.%20Optimal%0A%20%20hypersurface%20decision%20tree%20algorithm%0AAuthor%3A%20Xi%20He%0AAbstract%3A%20%20%20Decision%20trees%20are%20a%20ubiquitous%20model%20for%20classification%20and%20regression%20tasks%0Adue%20to%20their%20interpretability%20and%20efficiency.%20However%2C%20solving%20the%20optimal%0Adecision%20tree%20%28ODT%29%20problem%20remains%20a%20challenging%20combinatorial%20optimization%0Atask.%20Even%20for%20the%20simplest%20splitting%20rules--axis-parallel%20hyperplanes--it%20is%0ANP-hard%20to%20optimize.%20In%20Part%20I%20of%20this%20series%2C%20we%20rigorously%20defined%20the%20proper%0Adecision%20tree%20model%20through%20four%20axioms%20and%2C%20based%20on%20these%2C%20introduced%20four%0Aformal%20definitions%20of%20the%20ODT%20problem.%20From%20these%20definitions%2C%20we%20derived%20four%0Ageneric%20algorithms%20capable%20of%20solving%20ODT%20problems%20for%20arbitrary%20decision%20trees%0Asatisfying%20the%20axioms.%20We%20also%20analyzed%20the%20combinatorial%20geometric%20properties%0Aof%20hypersurfaces%2C%20showing%20that%20decision%20trees%20defined%20by%20polynomial%0Ahypersurface%20splitting%20rules%20satisfy%20the%20proper%20axioms%20that%20we%20proposed.%0A%20%20In%20this%20second%20paper%20%28Part%20II%29%20of%20this%20two-part%20series%2C%20building%20on%20the%0Aalgorithmic%20and%20geometric%20foundations%20established%20in%20Part%20I%2C%20we%20introduce%20the%0Afirst%20hypersurface%20decision%20tree%20%28HODT%29%20algorithm.%20To%20the%20best%20of%20our%0Aknowledge%2C%20existing%20optimal%20decision%20tree%20methods%20are%2C%20to%20date%2C%20limited%20to%0Ahyperplane%20splitting%20rules--a%20special%20case%20of%20hypersurfaces--and%20rely%20on%0Ageneral-purpose%20solvers.%20In%20contrast%2C%20our%20HODT%20algorithm%20addresses%20the%20general%0Ahypersurface%20decision%20tree%20model%20without%20requiring%20external%20solvers.%0A%20%20Using%20synthetic%20datasets%20generated%20from%20ground-truth%20hyperplane%20decision%0Atrees%2C%20we%20vary%20tree%20size%2C%20data%20size%2C%20dimensionality%2C%20and%20label%20and%20feature%0Anoise.%20Results%20showing%20that%20our%20algorithm%20recovers%20the%20ground%20truth%20more%0Aaccurately%20than%20axis-parallel%20trees%20and%20exhibits%20greater%20robustness%20to%20noise.%0AWe%20also%20analyzed%20generalization%20performance%20across%2030%20real-world%20datasets%2C%0Ashowing%20that%20HODT%20can%20achieve%20up%20to%2030%25%20higher%20accuracy%20than%20the%0Astate-of-the-art%20optimal%20axis-parallel%20decision%20tree%20algorithm%20when%20tree%0Acomplexity%20is%20properly%20controlled.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundational%2520theory%2520for%2520optimal%2520decision%2520tree%2520problems.%2520II.%2520Optimal%250A%2520%2520hypersurface%2520decision%2520tree%2520algorithm%26entry.906535625%3DXi%2520He%26entry.1292438233%3D%2520%2520Decision%2520trees%2520are%2520a%2520ubiquitous%2520model%2520for%2520classification%2520and%2520regression%2520tasks%250Adue%2520to%2520their%2520interpretability%2520and%2520efficiency.%2520However%252C%2520solving%2520the%2520optimal%250Adecision%2520tree%2520%2528ODT%2529%2520problem%2520remains%2520a%2520challenging%2520combinatorial%2520optimization%250Atask.%2520Even%2520for%2520the%2520simplest%2520splitting%2520rules--axis-parallel%2520hyperplanes--it%2520is%250ANP-hard%2520to%2520optimize.%2520In%2520Part%2520I%2520of%2520this%2520series%252C%2520we%2520rigorously%2520defined%2520the%2520proper%250Adecision%2520tree%2520model%2520through%2520four%2520axioms%2520and%252C%2520based%2520on%2520these%252C%2520introduced%2520four%250Aformal%2520definitions%2520of%2520the%2520ODT%2520problem.%2520From%2520these%2520definitions%252C%2520we%2520derived%2520four%250Ageneric%2520algorithms%2520capable%2520of%2520solving%2520ODT%2520problems%2520for%2520arbitrary%2520decision%2520trees%250Asatisfying%2520the%2520axioms.%2520We%2520also%2520analyzed%2520the%2520combinatorial%2520geometric%2520properties%250Aof%2520hypersurfaces%252C%2520showing%2520that%2520decision%2520trees%2520defined%2520by%2520polynomial%250Ahypersurface%2520splitting%2520rules%2520satisfy%2520the%2520proper%2520axioms%2520that%2520we%2520proposed.%250A%2520%2520In%2520this%2520second%2520paper%2520%2528Part%2520II%2529%2520of%2520this%2520two-part%2520series%252C%2520building%2520on%2520the%250Aalgorithmic%2520and%2520geometric%2520foundations%2520established%2520in%2520Part%2520I%252C%2520we%2520introduce%2520the%250Afirst%2520hypersurface%2520decision%2520tree%2520%2528HODT%2529%2520algorithm.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520existing%2520optimal%2520decision%2520tree%2520methods%2520are%252C%2520to%2520date%252C%2520limited%2520to%250Ahyperplane%2520splitting%2520rules--a%2520special%2520case%2520of%2520hypersurfaces--and%2520rely%2520on%250Ageneral-purpose%2520solvers.%2520In%2520contrast%252C%2520our%2520HODT%2520algorithm%2520addresses%2520the%2520general%250Ahypersurface%2520decision%2520tree%2520model%2520without%2520requiring%2520external%2520solvers.%250A%2520%2520Using%2520synthetic%2520datasets%2520generated%2520from%2520ground-truth%2520hyperplane%2520decision%250Atrees%252C%2520we%2520vary%2520tree%2520size%252C%2520data%2520size%252C%2520dimensionality%252C%2520and%2520label%2520and%2520feature%250Anoise.%2520Results%2520showing%2520that%2520our%2520algorithm%2520recovers%2520the%2520ground%2520truth%2520more%250Aaccurately%2520than%2520axis-parallel%2520trees%2520and%2520exhibits%2520greater%2520robustness%2520to%2520noise.%250AWe%2520also%2520analyzed%2520generalization%2520performance%2520across%252030%2520real-world%2520datasets%252C%250Ashowing%2520that%2520HODT%2520can%2520achieve%2520up%2520to%252030%2525%2520higher%2520accuracy%2520than%2520the%250Astate-of-the-art%2520optimal%2520axis-parallel%2520decision%2520tree%2520algorithm%2520when%2520tree%250Acomplexity%2520is%2520properly%2520controlled.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundational%20theory%20for%20optimal%20decision%20tree%20problems.%20II.%20Optimal%0A%20%20hypersurface%20decision%20tree%20algorithm&entry.906535625=Xi%20He&entry.1292438233=%20%20Decision%20trees%20are%20a%20ubiquitous%20model%20for%20classification%20and%20regression%20tasks%0Adue%20to%20their%20interpretability%20and%20efficiency.%20However%2C%20solving%20the%20optimal%0Adecision%20tree%20%28ODT%29%20problem%20remains%20a%20challenging%20combinatorial%20optimization%0Atask.%20Even%20for%20the%20simplest%20splitting%20rules--axis-parallel%20hyperplanes--it%20is%0ANP-hard%20to%20optimize.%20In%20Part%20I%20of%20this%20series%2C%20we%20rigorously%20defined%20the%20proper%0Adecision%20tree%20model%20through%20four%20axioms%20and%2C%20based%20on%20these%2C%20introduced%20four%0Aformal%20definitions%20of%20the%20ODT%20problem.%20From%20these%20definitions%2C%20we%20derived%20four%0Ageneric%20algorithms%20capable%20of%20solving%20ODT%20problems%20for%20arbitrary%20decision%20trees%0Asatisfying%20the%20axioms.%20We%20also%20analyzed%20the%20combinatorial%20geometric%20properties%0Aof%20hypersurfaces%2C%20showing%20that%20decision%20trees%20defined%20by%20polynomial%0Ahypersurface%20splitting%20rules%20satisfy%20the%20proper%20axioms%20that%20we%20proposed.%0A%20%20In%20this%20second%20paper%20%28Part%20II%29%20of%20this%20two-part%20series%2C%20building%20on%20the%0Aalgorithmic%20and%20geometric%20foundations%20established%20in%20Part%20I%2C%20we%20introduce%20the%0Afirst%20hypersurface%20decision%20tree%20%28HODT%29%20algorithm.%20To%20the%20best%20of%20our%0Aknowledge%2C%20existing%20optimal%20decision%20tree%20methods%20are%2C%20to%20date%2C%20limited%20to%0Ahyperplane%20splitting%20rules--a%20special%20case%20of%20hypersurfaces--and%20rely%20on%0Ageneral-purpose%20solvers.%20In%20contrast%2C%20our%20HODT%20algorithm%20addresses%20the%20general%0Ahypersurface%20decision%20tree%20model%20without%20requiring%20external%20solvers.%0A%20%20Using%20synthetic%20datasets%20generated%20from%20ground-truth%20hyperplane%20decision%0Atrees%2C%20we%20vary%20tree%20size%2C%20data%20size%2C%20dimensionality%2C%20and%20label%20and%20feature%0Anoise.%20Results%20showing%20that%20our%20algorithm%20recovers%20the%20ground%20truth%20more%0Aaccurately%20than%20axis-parallel%20trees%20and%20exhibits%20greater%20robustness%20to%20noise.%0AWe%20also%20analyzed%20generalization%20performance%20across%2030%20real-world%20datasets%2C%0Ashowing%20that%20HODT%20can%20achieve%20up%20to%2030%25%20higher%20accuracy%20than%20the%0Astate-of-the-art%20optimal%20axis-parallel%20decision%20tree%20algorithm%20when%20tree%0Acomplexity%20is%20properly%20controlled.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12057v1&entry.124074799=Read"},
{"title": "Approaches to Analysis and Design of AI-Based Autonomous Vehicles", "author": "Tao Yan and Zheyu Zhang and Jingjing Jiang and Wen-Hua Chen", "abstract": "  Artificial intelligence (AI) models are becoming key components in an\nautonomous vehicle (AV), especially in handling complicated perception tasks.\nHowever, closing the loop through AI-based feedback may pose significant risks\non reliability of autonomous driving due to very limited understanding about\nthe mechanism of AI-driven perception processes. To overcome it, this paper\naims to develop tools for modeling, analysis, and synthesis for a class of\nAI-based AV; in particular, their closed-loop properties, e.g., stability,\nrobustness, and performance, are rigorously studied in the statistical sense.\nFirst, we provide a novel modeling means for the AI-driven perception processes\nby looking at their error characteristics. Specifically, three fundamental\nAI-induced perception uncertainties are recognized and modeled by Markov\nchains, Gaussian processes, and bounded disturbances, respectively. By means of\nthat, the closed-loop stochastic stability (SS) is established in the sense of\nmean square, and then, an SS control synthesis method is presented within the\nframework of linear matrix inequalities (LMIs). Besides the SS properties, the\nrobustness and performance of AI-based AVs are discussed in terms of a\nstochastic guaranteed cost, and criteria are given to test the robustness level\nof an AV when in the presence of AI-induced uncertainties. Furthermore, the\nstochastic optimal guaranteed cost control is investigated, and an efficient\ndesign procedure is developed innovatively based on LMI techniques and convex\noptimization. Finally, to illustrate the effectiveness, the developed results\nare applied to an example of car following control, along with extensive\nsimulation.\n", "link": "http://arxiv.org/abs/2509.12169v1", "date": "2025-09-15", "relevancy": 2.1049, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5415}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5312}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Approaches%20to%20Analysis%20and%20Design%20of%20AI-Based%20Autonomous%20Vehicles&body=Title%3A%20Approaches%20to%20Analysis%20and%20Design%20of%20AI-Based%20Autonomous%20Vehicles%0AAuthor%3A%20Tao%20Yan%20and%20Zheyu%20Zhang%20and%20Jingjing%20Jiang%20and%20Wen-Hua%20Chen%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20models%20are%20becoming%20key%20components%20in%20an%0Aautonomous%20vehicle%20%28AV%29%2C%20especially%20in%20handling%20complicated%20perception%20tasks.%0AHowever%2C%20closing%20the%20loop%20through%20AI-based%20feedback%20may%20pose%20significant%20risks%0Aon%20reliability%20of%20autonomous%20driving%20due%20to%20very%20limited%20understanding%20about%0Athe%20mechanism%20of%20AI-driven%20perception%20processes.%20To%20overcome%20it%2C%20this%20paper%0Aaims%20to%20develop%20tools%20for%20modeling%2C%20analysis%2C%20and%20synthesis%20for%20a%20class%20of%0AAI-based%20AV%3B%20in%20particular%2C%20their%20closed-loop%20properties%2C%20e.g.%2C%20stability%2C%0Arobustness%2C%20and%20performance%2C%20are%20rigorously%20studied%20in%20the%20statistical%20sense.%0AFirst%2C%20we%20provide%20a%20novel%20modeling%20means%20for%20the%20AI-driven%20perception%20processes%0Aby%20looking%20at%20their%20error%20characteristics.%20Specifically%2C%20three%20fundamental%0AAI-induced%20perception%20uncertainties%20are%20recognized%20and%20modeled%20by%20Markov%0Achains%2C%20Gaussian%20processes%2C%20and%20bounded%20disturbances%2C%20respectively.%20By%20means%20of%0Athat%2C%20the%20closed-loop%20stochastic%20stability%20%28SS%29%20is%20established%20in%20the%20sense%20of%0Amean%20square%2C%20and%20then%2C%20an%20SS%20control%20synthesis%20method%20is%20presented%20within%20the%0Aframework%20of%20linear%20matrix%20inequalities%20%28LMIs%29.%20Besides%20the%20SS%20properties%2C%20the%0Arobustness%20and%20performance%20of%20AI-based%20AVs%20are%20discussed%20in%20terms%20of%20a%0Astochastic%20guaranteed%20cost%2C%20and%20criteria%20are%20given%20to%20test%20the%20robustness%20level%0Aof%20an%20AV%20when%20in%20the%20presence%20of%20AI-induced%20uncertainties.%20Furthermore%2C%20the%0Astochastic%20optimal%20guaranteed%20cost%20control%20is%20investigated%2C%20and%20an%20efficient%0Adesign%20procedure%20is%20developed%20innovatively%20based%20on%20LMI%20techniques%20and%20convex%0Aoptimization.%20Finally%2C%20to%20illustrate%20the%20effectiveness%2C%20the%20developed%20results%0Aare%20applied%20to%20an%20example%20of%20car%20following%20control%2C%20along%20with%20extensive%0Asimulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApproaches%2520to%2520Analysis%2520and%2520Design%2520of%2520AI-Based%2520Autonomous%2520Vehicles%26entry.906535625%3DTao%2520Yan%2520and%2520Zheyu%2520Zhang%2520and%2520Jingjing%2520Jiang%2520and%2520Wen-Hua%2520Chen%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520models%2520are%2520becoming%2520key%2520components%2520in%2520an%250Aautonomous%2520vehicle%2520%2528AV%2529%252C%2520especially%2520in%2520handling%2520complicated%2520perception%2520tasks.%250AHowever%252C%2520closing%2520the%2520loop%2520through%2520AI-based%2520feedback%2520may%2520pose%2520significant%2520risks%250Aon%2520reliability%2520of%2520autonomous%2520driving%2520due%2520to%2520very%2520limited%2520understanding%2520about%250Athe%2520mechanism%2520of%2520AI-driven%2520perception%2520processes.%2520To%2520overcome%2520it%252C%2520this%2520paper%250Aaims%2520to%2520develop%2520tools%2520for%2520modeling%252C%2520analysis%252C%2520and%2520synthesis%2520for%2520a%2520class%2520of%250AAI-based%2520AV%253B%2520in%2520particular%252C%2520their%2520closed-loop%2520properties%252C%2520e.g.%252C%2520stability%252C%250Arobustness%252C%2520and%2520performance%252C%2520are%2520rigorously%2520studied%2520in%2520the%2520statistical%2520sense.%250AFirst%252C%2520we%2520provide%2520a%2520novel%2520modeling%2520means%2520for%2520the%2520AI-driven%2520perception%2520processes%250Aby%2520looking%2520at%2520their%2520error%2520characteristics.%2520Specifically%252C%2520three%2520fundamental%250AAI-induced%2520perception%2520uncertainties%2520are%2520recognized%2520and%2520modeled%2520by%2520Markov%250Achains%252C%2520Gaussian%2520processes%252C%2520and%2520bounded%2520disturbances%252C%2520respectively.%2520By%2520means%2520of%250Athat%252C%2520the%2520closed-loop%2520stochastic%2520stability%2520%2528SS%2529%2520is%2520established%2520in%2520the%2520sense%2520of%250Amean%2520square%252C%2520and%2520then%252C%2520an%2520SS%2520control%2520synthesis%2520method%2520is%2520presented%2520within%2520the%250Aframework%2520of%2520linear%2520matrix%2520inequalities%2520%2528LMIs%2529.%2520Besides%2520the%2520SS%2520properties%252C%2520the%250Arobustness%2520and%2520performance%2520of%2520AI-based%2520AVs%2520are%2520discussed%2520in%2520terms%2520of%2520a%250Astochastic%2520guaranteed%2520cost%252C%2520and%2520criteria%2520are%2520given%2520to%2520test%2520the%2520robustness%2520level%250Aof%2520an%2520AV%2520when%2520in%2520the%2520presence%2520of%2520AI-induced%2520uncertainties.%2520Furthermore%252C%2520the%250Astochastic%2520optimal%2520guaranteed%2520cost%2520control%2520is%2520investigated%252C%2520and%2520an%2520efficient%250Adesign%2520procedure%2520is%2520developed%2520innovatively%2520based%2520on%2520LMI%2520techniques%2520and%2520convex%250Aoptimization.%2520Finally%252C%2520to%2520illustrate%2520the%2520effectiveness%252C%2520the%2520developed%2520results%250Aare%2520applied%2520to%2520an%2520example%2520of%2520car%2520following%2520control%252C%2520along%2520with%2520extensive%250Asimulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approaches%20to%20Analysis%20and%20Design%20of%20AI-Based%20Autonomous%20Vehicles&entry.906535625=Tao%20Yan%20and%20Zheyu%20Zhang%20and%20Jingjing%20Jiang%20and%20Wen-Hua%20Chen&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20models%20are%20becoming%20key%20components%20in%20an%0Aautonomous%20vehicle%20%28AV%29%2C%20especially%20in%20handling%20complicated%20perception%20tasks.%0AHowever%2C%20closing%20the%20loop%20through%20AI-based%20feedback%20may%20pose%20significant%20risks%0Aon%20reliability%20of%20autonomous%20driving%20due%20to%20very%20limited%20understanding%20about%0Athe%20mechanism%20of%20AI-driven%20perception%20processes.%20To%20overcome%20it%2C%20this%20paper%0Aaims%20to%20develop%20tools%20for%20modeling%2C%20analysis%2C%20and%20synthesis%20for%20a%20class%20of%0AAI-based%20AV%3B%20in%20particular%2C%20their%20closed-loop%20properties%2C%20e.g.%2C%20stability%2C%0Arobustness%2C%20and%20performance%2C%20are%20rigorously%20studied%20in%20the%20statistical%20sense.%0AFirst%2C%20we%20provide%20a%20novel%20modeling%20means%20for%20the%20AI-driven%20perception%20processes%0Aby%20looking%20at%20their%20error%20characteristics.%20Specifically%2C%20three%20fundamental%0AAI-induced%20perception%20uncertainties%20are%20recognized%20and%20modeled%20by%20Markov%0Achains%2C%20Gaussian%20processes%2C%20and%20bounded%20disturbances%2C%20respectively.%20By%20means%20of%0Athat%2C%20the%20closed-loop%20stochastic%20stability%20%28SS%29%20is%20established%20in%20the%20sense%20of%0Amean%20square%2C%20and%20then%2C%20an%20SS%20control%20synthesis%20method%20is%20presented%20within%20the%0Aframework%20of%20linear%20matrix%20inequalities%20%28LMIs%29.%20Besides%20the%20SS%20properties%2C%20the%0Arobustness%20and%20performance%20of%20AI-based%20AVs%20are%20discussed%20in%20terms%20of%20a%0Astochastic%20guaranteed%20cost%2C%20and%20criteria%20are%20given%20to%20test%20the%20robustness%20level%0Aof%20an%20AV%20when%20in%20the%20presence%20of%20AI-induced%20uncertainties.%20Furthermore%2C%20the%0Astochastic%20optimal%20guaranteed%20cost%20control%20is%20investigated%2C%20and%20an%20efficient%0Adesign%20procedure%20is%20developed%20innovatively%20based%20on%20LMI%20techniques%20and%20convex%0Aoptimization.%20Finally%2C%20to%20illustrate%20the%20effectiveness%2C%20the%20developed%20results%0Aare%20applied%20to%20an%20example%20of%20car%20following%20control%2C%20along%20with%20extensive%0Asimulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12169v1&entry.124074799=Read"},
{"title": "Control Analysis and Design for Autonomous Vehicles Subject to Imperfect\n  AI-Based Perception", "author": "Tao Yan and Zheyu Zhang and Jingjing Jiang and Wen-Hua Chen", "abstract": "  Safety is a critical concern in autonomous vehicle (AV) systems, especially\nwhen AI-based sensing and perception modules are involved. However, due to the\nblack box nature of AI algorithms, it makes closed-loop analysis and synthesis\nparticularly challenging, for example, establishing closed-loop stability and\nensuring performance, while they are fundamental to AV safety. To approach this\ndifficulty, this paper aims to develop new modeling, analysis, and synthesis\ntools for AI-based AVs. Inspired by recent developments in perception error\nmodels (PEMs), the focus is shifted from directly modeling AI-based perception\nprocesses to characterizing the perception errors they produce. Two key classes\nof AI-induced perception errors are considered: misdetection and measurement\nnoise. These error patterns are modeled using continuous-time Markov chains and\nWiener processes, respectively. By means of that, a PEM-augmented driving model\nis proposed, with which we are able to establish the closed-loop stability for\na class of AI-driven AV systems via stochastic calculus. Furthermore, a\nperformance-guaranteed output feedback control synthesis method is presented,\nwhich ensures both stability and satisfactory performance. The method is\nformulated as a convex optimization problem, allowing for efficient numerical\nsolutions. The results are then applied to an adaptive cruise control (ACC)\nscenario, demonstrating their effectiveness and robustness despite the\ncorrupted and misleading perception.\n", "link": "http://arxiv.org/abs/2509.12137v1", "date": "2025-09-15", "relevancy": 2.0959, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5349}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.526}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Control%20Analysis%20and%20Design%20for%20Autonomous%20Vehicles%20Subject%20to%20Imperfect%0A%20%20AI-Based%20Perception&body=Title%3A%20Control%20Analysis%20and%20Design%20for%20Autonomous%20Vehicles%20Subject%20to%20Imperfect%0A%20%20AI-Based%20Perception%0AAuthor%3A%20Tao%20Yan%20and%20Zheyu%20Zhang%20and%20Jingjing%20Jiang%20and%20Wen-Hua%20Chen%0AAbstract%3A%20%20%20Safety%20is%20a%20critical%20concern%20in%20autonomous%20vehicle%20%28AV%29%20systems%2C%20especially%0Awhen%20AI-based%20sensing%20and%20perception%20modules%20are%20involved.%20However%2C%20due%20to%20the%0Ablack%20box%20nature%20of%20AI%20algorithms%2C%20it%20makes%20closed-loop%20analysis%20and%20synthesis%0Aparticularly%20challenging%2C%20for%20example%2C%20establishing%20closed-loop%20stability%20and%0Aensuring%20performance%2C%20while%20they%20are%20fundamental%20to%20AV%20safety.%20To%20approach%20this%0Adifficulty%2C%20this%20paper%20aims%20to%20develop%20new%20modeling%2C%20analysis%2C%20and%20synthesis%0Atools%20for%20AI-based%20AVs.%20Inspired%20by%20recent%20developments%20in%20perception%20error%0Amodels%20%28PEMs%29%2C%20the%20focus%20is%20shifted%20from%20directly%20modeling%20AI-based%20perception%0Aprocesses%20to%20characterizing%20the%20perception%20errors%20they%20produce.%20Two%20key%20classes%0Aof%20AI-induced%20perception%20errors%20are%20considered%3A%20misdetection%20and%20measurement%0Anoise.%20These%20error%20patterns%20are%20modeled%20using%20continuous-time%20Markov%20chains%20and%0AWiener%20processes%2C%20respectively.%20By%20means%20of%20that%2C%20a%20PEM-augmented%20driving%20model%0Ais%20proposed%2C%20with%20which%20we%20are%20able%20to%20establish%20the%20closed-loop%20stability%20for%0Aa%20class%20of%20AI-driven%20AV%20systems%20via%20stochastic%20calculus.%20Furthermore%2C%20a%0Aperformance-guaranteed%20output%20feedback%20control%20synthesis%20method%20is%20presented%2C%0Awhich%20ensures%20both%20stability%20and%20satisfactory%20performance.%20The%20method%20is%0Aformulated%20as%20a%20convex%20optimization%20problem%2C%20allowing%20for%20efficient%20numerical%0Asolutions.%20The%20results%20are%20then%20applied%20to%20an%20adaptive%20cruise%20control%20%28ACC%29%0Ascenario%2C%20demonstrating%20their%20effectiveness%20and%20robustness%20despite%20the%0Acorrupted%20and%20misleading%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControl%2520Analysis%2520and%2520Design%2520for%2520Autonomous%2520Vehicles%2520Subject%2520to%2520Imperfect%250A%2520%2520AI-Based%2520Perception%26entry.906535625%3DTao%2520Yan%2520and%2520Zheyu%2520Zhang%2520and%2520Jingjing%2520Jiang%2520and%2520Wen-Hua%2520Chen%26entry.1292438233%3D%2520%2520Safety%2520is%2520a%2520critical%2520concern%2520in%2520autonomous%2520vehicle%2520%2528AV%2529%2520systems%252C%2520especially%250Awhen%2520AI-based%2520sensing%2520and%2520perception%2520modules%2520are%2520involved.%2520However%252C%2520due%2520to%2520the%250Ablack%2520box%2520nature%2520of%2520AI%2520algorithms%252C%2520it%2520makes%2520closed-loop%2520analysis%2520and%2520synthesis%250Aparticularly%2520challenging%252C%2520for%2520example%252C%2520establishing%2520closed-loop%2520stability%2520and%250Aensuring%2520performance%252C%2520while%2520they%2520are%2520fundamental%2520to%2520AV%2520safety.%2520To%2520approach%2520this%250Adifficulty%252C%2520this%2520paper%2520aims%2520to%2520develop%2520new%2520modeling%252C%2520analysis%252C%2520and%2520synthesis%250Atools%2520for%2520AI-based%2520AVs.%2520Inspired%2520by%2520recent%2520developments%2520in%2520perception%2520error%250Amodels%2520%2528PEMs%2529%252C%2520the%2520focus%2520is%2520shifted%2520from%2520directly%2520modeling%2520AI-based%2520perception%250Aprocesses%2520to%2520characterizing%2520the%2520perception%2520errors%2520they%2520produce.%2520Two%2520key%2520classes%250Aof%2520AI-induced%2520perception%2520errors%2520are%2520considered%253A%2520misdetection%2520and%2520measurement%250Anoise.%2520These%2520error%2520patterns%2520are%2520modeled%2520using%2520continuous-time%2520Markov%2520chains%2520and%250AWiener%2520processes%252C%2520respectively.%2520By%2520means%2520of%2520that%252C%2520a%2520PEM-augmented%2520driving%2520model%250Ais%2520proposed%252C%2520with%2520which%2520we%2520are%2520able%2520to%2520establish%2520the%2520closed-loop%2520stability%2520for%250Aa%2520class%2520of%2520AI-driven%2520AV%2520systems%2520via%2520stochastic%2520calculus.%2520Furthermore%252C%2520a%250Aperformance-guaranteed%2520output%2520feedback%2520control%2520synthesis%2520method%2520is%2520presented%252C%250Awhich%2520ensures%2520both%2520stability%2520and%2520satisfactory%2520performance.%2520The%2520method%2520is%250Aformulated%2520as%2520a%2520convex%2520optimization%2520problem%252C%2520allowing%2520for%2520efficient%2520numerical%250Asolutions.%2520The%2520results%2520are%2520then%2520applied%2520to%2520an%2520adaptive%2520cruise%2520control%2520%2528ACC%2529%250Ascenario%252C%2520demonstrating%2520their%2520effectiveness%2520and%2520robustness%2520despite%2520the%250Acorrupted%2520and%2520misleading%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Control%20Analysis%20and%20Design%20for%20Autonomous%20Vehicles%20Subject%20to%20Imperfect%0A%20%20AI-Based%20Perception&entry.906535625=Tao%20Yan%20and%20Zheyu%20Zhang%20and%20Jingjing%20Jiang%20and%20Wen-Hua%20Chen&entry.1292438233=%20%20Safety%20is%20a%20critical%20concern%20in%20autonomous%20vehicle%20%28AV%29%20systems%2C%20especially%0Awhen%20AI-based%20sensing%20and%20perception%20modules%20are%20involved.%20However%2C%20due%20to%20the%0Ablack%20box%20nature%20of%20AI%20algorithms%2C%20it%20makes%20closed-loop%20analysis%20and%20synthesis%0Aparticularly%20challenging%2C%20for%20example%2C%20establishing%20closed-loop%20stability%20and%0Aensuring%20performance%2C%20while%20they%20are%20fundamental%20to%20AV%20safety.%20To%20approach%20this%0Adifficulty%2C%20this%20paper%20aims%20to%20develop%20new%20modeling%2C%20analysis%2C%20and%20synthesis%0Atools%20for%20AI-based%20AVs.%20Inspired%20by%20recent%20developments%20in%20perception%20error%0Amodels%20%28PEMs%29%2C%20the%20focus%20is%20shifted%20from%20directly%20modeling%20AI-based%20perception%0Aprocesses%20to%20characterizing%20the%20perception%20errors%20they%20produce.%20Two%20key%20classes%0Aof%20AI-induced%20perception%20errors%20are%20considered%3A%20misdetection%20and%20measurement%0Anoise.%20These%20error%20patterns%20are%20modeled%20using%20continuous-time%20Markov%20chains%20and%0AWiener%20processes%2C%20respectively.%20By%20means%20of%20that%2C%20a%20PEM-augmented%20driving%20model%0Ais%20proposed%2C%20with%20which%20we%20are%20able%20to%20establish%20the%20closed-loop%20stability%20for%0Aa%20class%20of%20AI-driven%20AV%20systems%20via%20stochastic%20calculus.%20Furthermore%2C%20a%0Aperformance-guaranteed%20output%20feedback%20control%20synthesis%20method%20is%20presented%2C%0Awhich%20ensures%20both%20stability%20and%20satisfactory%20performance.%20The%20method%20is%0Aformulated%20as%20a%20convex%20optimization%20problem%2C%20allowing%20for%20efficient%20numerical%0Asolutions.%20The%20results%20are%20then%20applied%20to%20an%20adaptive%20cruise%20control%20%28ACC%29%0Ascenario%2C%20demonstrating%20their%20effectiveness%20and%20robustness%20despite%20the%0Acorrupted%20and%20misleading%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12137v1&entry.124074799=Read"},
{"title": "SRSNetwork: Siamese Reconstruction-Segmentation Networks based on\n  Dynamic-Parameter Convolution", "author": "Bingkun Nian and Fenghe Tang and Jianrui Ding and Jie Yang and Zhonglong Zheng and Shaohua Kevin Zhou and Wei Liu", "abstract": "  Dynamic convolution demonstrates outstanding representation capabilities,\nwhich are crucial for natural image segmentation. However, it fails when\napplied to medical image segmentation (MIS) and infrared small target\nsegmentation (IRSTS) due to limited data and limited fitting capacity. In this\npaper, we propose a new type of dynamic convolution called dynamic parameter\nconvolution (DPConv) which shows superior fitting capacity, and it can\nefficiently leverage features from deep layers of encoder in reconstruction\ntasks to generate DPConv kernels that adapt to input variations.Moreover, we\nobserve that DPConv, built upon deep features derived from reconstruction\ntasks, significantly enhances downstream segmentation performance. We refer to\nthe segmentation network integrated with DPConv generated from reconstruction\nnetwork as the siamese reconstruction-segmentation network (SRS). We conduct\nextensive experiments on seven datasets including five medical datasets and two\ninfrared datasets, and the experimental results demonstrate that our method can\nshow superior performance over several recently proposed methods. Furthermore,\nthe zero-shot segmentation under unseen modality demonstrates the\ngeneralization of DPConv. The code is available at:\nhttps://github.com/fidshu/SRSNet.\n", "link": "http://arxiv.org/abs/2312.01741v2", "date": "2025-09-15", "relevancy": 2.0881, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5331}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5216}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SRSNetwork%3A%20Siamese%20Reconstruction-Segmentation%20Networks%20based%20on%0A%20%20Dynamic-Parameter%20Convolution&body=Title%3A%20SRSNetwork%3A%20Siamese%20Reconstruction-Segmentation%20Networks%20based%20on%0A%20%20Dynamic-Parameter%20Convolution%0AAuthor%3A%20Bingkun%20Nian%20and%20Fenghe%20Tang%20and%20Jianrui%20Ding%20and%20Jie%20Yang%20and%20Zhonglong%20Zheng%20and%20Shaohua%20Kevin%20Zhou%20and%20Wei%20Liu%0AAbstract%3A%20%20%20Dynamic%20convolution%20demonstrates%20outstanding%20representation%20capabilities%2C%0Awhich%20are%20crucial%20for%20natural%20image%20segmentation.%20However%2C%20it%20fails%20when%0Aapplied%20to%20medical%20image%20segmentation%20%28MIS%29%20and%20infrared%20small%20target%0Asegmentation%20%28IRSTS%29%20due%20to%20limited%20data%20and%20limited%20fitting%20capacity.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20type%20of%20dynamic%20convolution%20called%20dynamic%20parameter%0Aconvolution%20%28DPConv%29%20which%20shows%20superior%20fitting%20capacity%2C%20and%20it%20can%0Aefficiently%20leverage%20features%20from%20deep%20layers%20of%20encoder%20in%20reconstruction%0Atasks%20to%20generate%20DPConv%20kernels%20that%20adapt%20to%20input%20variations.Moreover%2C%20we%0Aobserve%20that%20DPConv%2C%20built%20upon%20deep%20features%20derived%20from%20reconstruction%0Atasks%2C%20significantly%20enhances%20downstream%20segmentation%20performance.%20We%20refer%20to%0Athe%20segmentation%20network%20integrated%20with%20DPConv%20generated%20from%20reconstruction%0Anetwork%20as%20the%20siamese%20reconstruction-segmentation%20network%20%28SRS%29.%20We%20conduct%0Aextensive%20experiments%20on%20seven%20datasets%20including%20five%20medical%20datasets%20and%20two%0Ainfrared%20datasets%2C%20and%20the%20experimental%20results%20demonstrate%20that%20our%20method%20can%0Ashow%20superior%20performance%20over%20several%20recently%20proposed%20methods.%20Furthermore%2C%0Athe%20zero-shot%20segmentation%20under%20unseen%20modality%20demonstrates%20the%0Ageneralization%20of%20DPConv.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/fidshu/SRSNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01741v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSRSNetwork%253A%2520Siamese%2520Reconstruction-Segmentation%2520Networks%2520based%2520on%250A%2520%2520Dynamic-Parameter%2520Convolution%26entry.906535625%3DBingkun%2520Nian%2520and%2520Fenghe%2520Tang%2520and%2520Jianrui%2520Ding%2520and%2520Jie%2520Yang%2520and%2520Zhonglong%2520Zheng%2520and%2520Shaohua%2520Kevin%2520Zhou%2520and%2520Wei%2520Liu%26entry.1292438233%3D%2520%2520Dynamic%2520convolution%2520demonstrates%2520outstanding%2520representation%2520capabilities%252C%250Awhich%2520are%2520crucial%2520for%2520natural%2520image%2520segmentation.%2520However%252C%2520it%2520fails%2520when%250Aapplied%2520to%2520medical%2520image%2520segmentation%2520%2528MIS%2529%2520and%2520infrared%2520small%2520target%250Asegmentation%2520%2528IRSTS%2529%2520due%2520to%2520limited%2520data%2520and%2520limited%2520fitting%2520capacity.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520new%2520type%2520of%2520dynamic%2520convolution%2520called%2520dynamic%2520parameter%250Aconvolution%2520%2528DPConv%2529%2520which%2520shows%2520superior%2520fitting%2520capacity%252C%2520and%2520it%2520can%250Aefficiently%2520leverage%2520features%2520from%2520deep%2520layers%2520of%2520encoder%2520in%2520reconstruction%250Atasks%2520to%2520generate%2520DPConv%2520kernels%2520that%2520adapt%2520to%2520input%2520variations.Moreover%252C%2520we%250Aobserve%2520that%2520DPConv%252C%2520built%2520upon%2520deep%2520features%2520derived%2520from%2520reconstruction%250Atasks%252C%2520significantly%2520enhances%2520downstream%2520segmentation%2520performance.%2520We%2520refer%2520to%250Athe%2520segmentation%2520network%2520integrated%2520with%2520DPConv%2520generated%2520from%2520reconstruction%250Anetwork%2520as%2520the%2520siamese%2520reconstruction-segmentation%2520network%2520%2528SRS%2529.%2520We%2520conduct%250Aextensive%2520experiments%2520on%2520seven%2520datasets%2520including%2520five%2520medical%2520datasets%2520and%2520two%250Ainfrared%2520datasets%252C%2520and%2520the%2520experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520can%250Ashow%2520superior%2520performance%2520over%2520several%2520recently%2520proposed%2520methods.%2520Furthermore%252C%250Athe%2520zero-shot%2520segmentation%2520under%2520unseen%2520modality%2520demonstrates%2520the%250Ageneralization%2520of%2520DPConv.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/fidshu/SRSNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.01741v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SRSNetwork%3A%20Siamese%20Reconstruction-Segmentation%20Networks%20based%20on%0A%20%20Dynamic-Parameter%20Convolution&entry.906535625=Bingkun%20Nian%20and%20Fenghe%20Tang%20and%20Jianrui%20Ding%20and%20Jie%20Yang%20and%20Zhonglong%20Zheng%20and%20Shaohua%20Kevin%20Zhou%20and%20Wei%20Liu&entry.1292438233=%20%20Dynamic%20convolution%20demonstrates%20outstanding%20representation%20capabilities%2C%0Awhich%20are%20crucial%20for%20natural%20image%20segmentation.%20However%2C%20it%20fails%20when%0Aapplied%20to%20medical%20image%20segmentation%20%28MIS%29%20and%20infrared%20small%20target%0Asegmentation%20%28IRSTS%29%20due%20to%20limited%20data%20and%20limited%20fitting%20capacity.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20type%20of%20dynamic%20convolution%20called%20dynamic%20parameter%0Aconvolution%20%28DPConv%29%20which%20shows%20superior%20fitting%20capacity%2C%20and%20it%20can%0Aefficiently%20leverage%20features%20from%20deep%20layers%20of%20encoder%20in%20reconstruction%0Atasks%20to%20generate%20DPConv%20kernels%20that%20adapt%20to%20input%20variations.Moreover%2C%20we%0Aobserve%20that%20DPConv%2C%20built%20upon%20deep%20features%20derived%20from%20reconstruction%0Atasks%2C%20significantly%20enhances%20downstream%20segmentation%20performance.%20We%20refer%20to%0Athe%20segmentation%20network%20integrated%20with%20DPConv%20generated%20from%20reconstruction%0Anetwork%20as%20the%20siamese%20reconstruction-segmentation%20network%20%28SRS%29.%20We%20conduct%0Aextensive%20experiments%20on%20seven%20datasets%20including%20five%20medical%20datasets%20and%20two%0Ainfrared%20datasets%2C%20and%20the%20experimental%20results%20demonstrate%20that%20our%20method%20can%0Ashow%20superior%20performance%20over%20several%20recently%20proposed%20methods.%20Furthermore%2C%0Athe%20zero-shot%20segmentation%20under%20unseen%20modality%20demonstrates%20the%0Ageneralization%20of%20DPConv.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/fidshu/SRSNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01741v2&entry.124074799=Read"},
{"title": "Unearthing Gems from Stones: Policy Optimization with Negative Sample\n  Augmentation for LLM Reasoning", "author": "Zhaohui Yang and Yuxiao Ye and Shilei Jiang and Chen Hu and Linjing Li and Shihong Deng and Daxin Jiang", "abstract": "  Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations.\n", "link": "http://arxiv.org/abs/2505.14403v4", "date": "2025-09-15", "relevancy": 2.0763, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5589}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4978}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unearthing%20Gems%20from%20Stones%3A%20Policy%20Optimization%20with%20Negative%20Sample%0A%20%20Augmentation%20for%20LLM%20Reasoning&body=Title%3A%20Unearthing%20Gems%20from%20Stones%3A%20Policy%20Optimization%20with%20Negative%20Sample%0A%20%20Augmentation%20for%20LLM%20Reasoning%0AAuthor%3A%20Zhaohui%20Yang%20and%20Yuxiao%20Ye%20and%20Shilei%20Jiang%20and%20Chen%20Hu%20and%20Linjing%20Li%20and%20Shihong%20Deng%20and%20Daxin%20Jiang%0AAbstract%3A%20%20%20Recent%20advances%20in%20reasoning%20language%20models%20have%20witnessed%20a%20paradigm%20shift%0Afrom%20short%20to%20long%20CoT%20pattern.%20Given%20the%20substantial%20computational%20cost%20of%0Arollouts%20in%20long%20CoT%20models%2C%20maximizing%20the%20utility%20of%20fixed%20training%20datasets%0Abecomes%20crucial.%20Our%20analysis%20reveals%20that%20negative%20responses%20contain%20valuable%0Acomponents%20such%20as%20self-reflection%20and%20error-correction%20steps%2C%20yet%20primary%0Aexisting%20methods%20either%20completely%20discard%20negative%20samples%20%28RFT%29%20or%20apply%0Aequal%20penalization%20across%20all%20tokens%20%28RL%29%2C%20failing%20to%20leverage%20these%20potential%0Alearning%20signals.%20In%20light%20of%20this%2C%20we%20propose%20Behavior%20Constrained%20Policy%0AGradient%20with%20Negative%20Sample%20Augmentation%20%28BCPG-NSA%29%2C%20a%20fine-grained%20offline%0ARL%20framework%20that%20encompasses%20three%20stages%3A%201%29%20sample%20segmentation%2C%202%29%0Aconsensus-based%20step%20correctness%20assessment%20combining%20LLM%20and%20PRM%20judgers%2C%20and%0A3%29%20policy%20optimization%20with%20NSA%20designed%20to%20effectively%20mine%20positive%20steps%0Awithin%20negative%20samples.%20Experimental%20results%20show%20that%20BCPG-NSA%20outperforms%0Abaselines%20on%20several%20challenging%20math/coding%20reasoning%20benchmarks%20using%20the%0Asame%20training%20dataset%2C%20achieving%20improved%20sample%20efficiency%20and%20demonstrating%0Arobustness%20and%20scalability%20when%20extended%20to%20multiple%20iterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14403v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnearthing%2520Gems%2520from%2520Stones%253A%2520Policy%2520Optimization%2520with%2520Negative%2520Sample%250A%2520%2520Augmentation%2520for%2520LLM%2520Reasoning%26entry.906535625%3DZhaohui%2520Yang%2520and%2520Yuxiao%2520Ye%2520and%2520Shilei%2520Jiang%2520and%2520Chen%2520Hu%2520and%2520Linjing%2520Li%2520and%2520Shihong%2520Deng%2520and%2520Daxin%2520Jiang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520reasoning%2520language%2520models%2520have%2520witnessed%2520a%2520paradigm%2520shift%250Afrom%2520short%2520to%2520long%2520CoT%2520pattern.%2520Given%2520the%2520substantial%2520computational%2520cost%2520of%250Arollouts%2520in%2520long%2520CoT%2520models%252C%2520maximizing%2520the%2520utility%2520of%2520fixed%2520training%2520datasets%250Abecomes%2520crucial.%2520Our%2520analysis%2520reveals%2520that%2520negative%2520responses%2520contain%2520valuable%250Acomponents%2520such%2520as%2520self-reflection%2520and%2520error-correction%2520steps%252C%2520yet%2520primary%250Aexisting%2520methods%2520either%2520completely%2520discard%2520negative%2520samples%2520%2528RFT%2529%2520or%2520apply%250Aequal%2520penalization%2520across%2520all%2520tokens%2520%2528RL%2529%252C%2520failing%2520to%2520leverage%2520these%2520potential%250Alearning%2520signals.%2520In%2520light%2520of%2520this%252C%2520we%2520propose%2520Behavior%2520Constrained%2520Policy%250AGradient%2520with%2520Negative%2520Sample%2520Augmentation%2520%2528BCPG-NSA%2529%252C%2520a%2520fine-grained%2520offline%250ARL%2520framework%2520that%2520encompasses%2520three%2520stages%253A%25201%2529%2520sample%2520segmentation%252C%25202%2529%250Aconsensus-based%2520step%2520correctness%2520assessment%2520combining%2520LLM%2520and%2520PRM%2520judgers%252C%2520and%250A3%2529%2520policy%2520optimization%2520with%2520NSA%2520designed%2520to%2520effectively%2520mine%2520positive%2520steps%250Awithin%2520negative%2520samples.%2520Experimental%2520results%2520show%2520that%2520BCPG-NSA%2520outperforms%250Abaselines%2520on%2520several%2520challenging%2520math/coding%2520reasoning%2520benchmarks%2520using%2520the%250Asame%2520training%2520dataset%252C%2520achieving%2520improved%2520sample%2520efficiency%2520and%2520demonstrating%250Arobustness%2520and%2520scalability%2520when%2520extended%2520to%2520multiple%2520iterations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14403v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unearthing%20Gems%20from%20Stones%3A%20Policy%20Optimization%20with%20Negative%20Sample%0A%20%20Augmentation%20for%20LLM%20Reasoning&entry.906535625=Zhaohui%20Yang%20and%20Yuxiao%20Ye%20and%20Shilei%20Jiang%20and%20Chen%20Hu%20and%20Linjing%20Li%20and%20Shihong%20Deng%20and%20Daxin%20Jiang&entry.1292438233=%20%20Recent%20advances%20in%20reasoning%20language%20models%20have%20witnessed%20a%20paradigm%20shift%0Afrom%20short%20to%20long%20CoT%20pattern.%20Given%20the%20substantial%20computational%20cost%20of%0Arollouts%20in%20long%20CoT%20models%2C%20maximizing%20the%20utility%20of%20fixed%20training%20datasets%0Abecomes%20crucial.%20Our%20analysis%20reveals%20that%20negative%20responses%20contain%20valuable%0Acomponents%20such%20as%20self-reflection%20and%20error-correction%20steps%2C%20yet%20primary%0Aexisting%20methods%20either%20completely%20discard%20negative%20samples%20%28RFT%29%20or%20apply%0Aequal%20penalization%20across%20all%20tokens%20%28RL%29%2C%20failing%20to%20leverage%20these%20potential%0Alearning%20signals.%20In%20light%20of%20this%2C%20we%20propose%20Behavior%20Constrained%20Policy%0AGradient%20with%20Negative%20Sample%20Augmentation%20%28BCPG-NSA%29%2C%20a%20fine-grained%20offline%0ARL%20framework%20that%20encompasses%20three%20stages%3A%201%29%20sample%20segmentation%2C%202%29%0Aconsensus-based%20step%20correctness%20assessment%20combining%20LLM%20and%20PRM%20judgers%2C%20and%0A3%29%20policy%20optimization%20with%20NSA%20designed%20to%20effectively%20mine%20positive%20steps%0Awithin%20negative%20samples.%20Experimental%20results%20show%20that%20BCPG-NSA%20outperforms%0Abaselines%20on%20several%20challenging%20math/coding%20reasoning%20benchmarks%20using%20the%0Asame%20training%20dataset%2C%20achieving%20improved%20sample%20efficiency%20and%20demonstrating%0Arobustness%20and%20scalability%20when%20extended%20to%20multiple%20iterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14403v4&entry.124074799=Read"},
{"title": "Quantum Noise Tomography with Physics-Informed Neural Networks", "author": "Antonin Sulc", "abstract": "  Characterizing the environmental interactions of quantum systems is a\ncritical bottleneck in the development of robust quantum technologies.\nTraditional tomographic methods are often data-intensive and struggle with\nscalability. In this work, we introduce a novel framework for performing\nLindblad tomography using Physics-Informed Neural Networks (PINNs). By\nembedding the Lindblad master equation directly into the neural network's loss\nfunction, our approach simultaneously learns the quantum state's evolution and\ninfers the underlying dissipation parameters from sparse, time-series\nmeasurement data. Our results show that PINNs can reconstruct both the system\ndynamics and the functional form of unknown noise parameters, presenting a\nsample-efficient and scalable solution for quantum device characterization.\nUltimately, our method produces a fully-differentiable digital twin of a noisy\nquantum system by learning its governing master equation.\n", "link": "http://arxiv.org/abs/2509.11911v1", "date": "2025-09-15", "relevancy": 2.0599, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5485}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4948}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Noise%20Tomography%20with%20Physics-Informed%20Neural%20Networks&body=Title%3A%20Quantum%20Noise%20Tomography%20with%20Physics-Informed%20Neural%20Networks%0AAuthor%3A%20Antonin%20Sulc%0AAbstract%3A%20%20%20Characterizing%20the%20environmental%20interactions%20of%20quantum%20systems%20is%20a%0Acritical%20bottleneck%20in%20the%20development%20of%20robust%20quantum%20technologies.%0ATraditional%20tomographic%20methods%20are%20often%20data-intensive%20and%20struggle%20with%0Ascalability.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20framework%20for%20performing%0ALindblad%20tomography%20using%20Physics-Informed%20Neural%20Networks%20%28PINNs%29.%20By%0Aembedding%20the%20Lindblad%20master%20equation%20directly%20into%20the%20neural%20network%27s%20loss%0Afunction%2C%20our%20approach%20simultaneously%20learns%20the%20quantum%20state%27s%20evolution%20and%0Ainfers%20the%20underlying%20dissipation%20parameters%20from%20sparse%2C%20time-series%0Ameasurement%20data.%20Our%20results%20show%20that%20PINNs%20can%20reconstruct%20both%20the%20system%0Adynamics%20and%20the%20functional%20form%20of%20unknown%20noise%20parameters%2C%20presenting%20a%0Asample-efficient%20and%20scalable%20solution%20for%20quantum%20device%20characterization.%0AUltimately%2C%20our%20method%20produces%20a%20fully-differentiable%20digital%20twin%20of%20a%20noisy%0Aquantum%20system%20by%20learning%20its%20governing%20master%20equation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11911v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Noise%2520Tomography%2520with%2520Physics-Informed%2520Neural%2520Networks%26entry.906535625%3DAntonin%2520Sulc%26entry.1292438233%3D%2520%2520Characterizing%2520the%2520environmental%2520interactions%2520of%2520quantum%2520systems%2520is%2520a%250Acritical%2520bottleneck%2520in%2520the%2520development%2520of%2520robust%2520quantum%2520technologies.%250ATraditional%2520tomographic%2520methods%2520are%2520often%2520data-intensive%2520and%2520struggle%2520with%250Ascalability.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520for%2520performing%250ALindblad%2520tomography%2520using%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529.%2520By%250Aembedding%2520the%2520Lindblad%2520master%2520equation%2520directly%2520into%2520the%2520neural%2520network%2527s%2520loss%250Afunction%252C%2520our%2520approach%2520simultaneously%2520learns%2520the%2520quantum%2520state%2527s%2520evolution%2520and%250Ainfers%2520the%2520underlying%2520dissipation%2520parameters%2520from%2520sparse%252C%2520time-series%250Ameasurement%2520data.%2520Our%2520results%2520show%2520that%2520PINNs%2520can%2520reconstruct%2520both%2520the%2520system%250Adynamics%2520and%2520the%2520functional%2520form%2520of%2520unknown%2520noise%2520parameters%252C%2520presenting%2520a%250Asample-efficient%2520and%2520scalable%2520solution%2520for%2520quantum%2520device%2520characterization.%250AUltimately%252C%2520our%2520method%2520produces%2520a%2520fully-differentiable%2520digital%2520twin%2520of%2520a%2520noisy%250Aquantum%2520system%2520by%2520learning%2520its%2520governing%2520master%2520equation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11911v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Noise%20Tomography%20with%20Physics-Informed%20Neural%20Networks&entry.906535625=Antonin%20Sulc&entry.1292438233=%20%20Characterizing%20the%20environmental%20interactions%20of%20quantum%20systems%20is%20a%0Acritical%20bottleneck%20in%20the%20development%20of%20robust%20quantum%20technologies.%0ATraditional%20tomographic%20methods%20are%20often%20data-intensive%20and%20struggle%20with%0Ascalability.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20framework%20for%20performing%0ALindblad%20tomography%20using%20Physics-Informed%20Neural%20Networks%20%28PINNs%29.%20By%0Aembedding%20the%20Lindblad%20master%20equation%20directly%20into%20the%20neural%20network%27s%20loss%0Afunction%2C%20our%20approach%20simultaneously%20learns%20the%20quantum%20state%27s%20evolution%20and%0Ainfers%20the%20underlying%20dissipation%20parameters%20from%20sparse%2C%20time-series%0Ameasurement%20data.%20Our%20results%20show%20that%20PINNs%20can%20reconstruct%20both%20the%20system%0Adynamics%20and%20the%20functional%20form%20of%20unknown%20noise%20parameters%2C%20presenting%20a%0Asample-efficient%20and%20scalable%20solution%20for%20quantum%20device%20characterization.%0AUltimately%2C%20our%20method%20produces%20a%20fully-differentiable%20digital%20twin%20of%20a%20noisy%0Aquantum%20system%20by%20learning%20its%20governing%20master%20equation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11911v1&entry.124074799=Read"},
{"title": "EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and\n  Output Token Compression", "author": "Jingyu Xiao and Zhongyi Zhang and Yuxuan Wan and Yintong Huo and Yang Liu and Michael R. Lyu", "abstract": "  Multimodal Large Language Models have demonstrated exceptional performance in\nUI2Code tasks, significantly enhancing website development efficiency. However,\nthese tasks incur substantially higher computational overhead than traditional\ncode generation due to the large number of input image tokens and extensive\noutput code tokens required. Our comprehensive study identifies significant\nredundancies in both image and code tokens that exacerbate computational\ncomplexity and hinder focus on key UI elements, resulting in excessively\nlengthy and often invalid HTML files. We propose EfficientUICoder, a\ncompression framework for efficient UI code generation with three key\ncomponents. First, Element and Layout-aware Token Compression preserves\nessential UI information by detecting element regions and constructing UI\nelement trees. Second, Region-aware Token Refinement leverages attention scores\nto discard low-attention tokens from selected regions while integrating\nhigh-attention tokens from unselected regions. Third, Adaptive Duplicate Token\nSuppression dynamically reduces repetitive generation by tracking HTML/CSS\nstructure frequencies and applying exponential penalties. Extensive experiments\nshow EfficientUICoderachieves a 55%-60% compression ratio without compromising\nwebpage quality and delivers superior efficiency improvements: reducing\ncomputational cost by 44.9%, generated tokens by 41.4%, prefill time by 46.6%,\nand inference time by 48.8% on 34B-level MLLMs. Code is available at\nhttps://github.com/WebPAI/EfficientUICoder.\n", "link": "http://arxiv.org/abs/2509.12159v1", "date": "2025-09-15", "relevancy": 2.058, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5328}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5028}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EfficientUICoder%3A%20Efficient%20MLLM-based%20UI%20Code%20Generation%20via%20Input%20and%0A%20%20Output%20Token%20Compression&body=Title%3A%20EfficientUICoder%3A%20Efficient%20MLLM-based%20UI%20Code%20Generation%20via%20Input%20and%0A%20%20Output%20Token%20Compression%0AAuthor%3A%20Jingyu%20Xiao%20and%20Zhongyi%20Zhang%20and%20Yuxuan%20Wan%20and%20Yintong%20Huo%20and%20Yang%20Liu%20and%20Michael%20R.%20Lyu%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20have%20demonstrated%20exceptional%20performance%20in%0AUI2Code%20tasks%2C%20significantly%20enhancing%20website%20development%20efficiency.%20However%2C%0Athese%20tasks%20incur%20substantially%20higher%20computational%20overhead%20than%20traditional%0Acode%20generation%20due%20to%20the%20large%20number%20of%20input%20image%20tokens%20and%20extensive%0Aoutput%20code%20tokens%20required.%20Our%20comprehensive%20study%20identifies%20significant%0Aredundancies%20in%20both%20image%20and%20code%20tokens%20that%20exacerbate%20computational%0Acomplexity%20and%20hinder%20focus%20on%20key%20UI%20elements%2C%20resulting%20in%20excessively%0Alengthy%20and%20often%20invalid%20HTML%20files.%20We%20propose%20EfficientUICoder%2C%20a%0Acompression%20framework%20for%20efficient%20UI%20code%20generation%20with%20three%20key%0Acomponents.%20First%2C%20Element%20and%20Layout-aware%20Token%20Compression%20preserves%0Aessential%20UI%20information%20by%20detecting%20element%20regions%20and%20constructing%20UI%0Aelement%20trees.%20Second%2C%20Region-aware%20Token%20Refinement%20leverages%20attention%20scores%0Ato%20discard%20low-attention%20tokens%20from%20selected%20regions%20while%20integrating%0Ahigh-attention%20tokens%20from%20unselected%20regions.%20Third%2C%20Adaptive%20Duplicate%20Token%0ASuppression%20dynamically%20reduces%20repetitive%20generation%20by%20tracking%20HTML/CSS%0Astructure%20frequencies%20and%20applying%20exponential%20penalties.%20Extensive%20experiments%0Ashow%20EfficientUICoderachieves%20a%2055%25-60%25%20compression%20ratio%20without%20compromising%0Awebpage%20quality%20and%20delivers%20superior%20efficiency%20improvements%3A%20reducing%0Acomputational%20cost%20by%2044.9%25%2C%20generated%20tokens%20by%2041.4%25%2C%20prefill%20time%20by%2046.6%25%2C%0Aand%20inference%20time%20by%2048.8%25%20on%2034B-level%20MLLMs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/WebPAI/EfficientUICoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficientUICoder%253A%2520Efficient%2520MLLM-based%2520UI%2520Code%2520Generation%2520via%2520Input%2520and%250A%2520%2520Output%2520Token%2520Compression%26entry.906535625%3DJingyu%2520Xiao%2520and%2520Zhongyi%2520Zhang%2520and%2520Yuxuan%2520Wan%2520and%2520Yintong%2520Huo%2520and%2520Yang%2520Liu%2520and%2520Michael%2520R.%2520Lyu%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520have%2520demonstrated%2520exceptional%2520performance%2520in%250AUI2Code%2520tasks%252C%2520significantly%2520enhancing%2520website%2520development%2520efficiency.%2520However%252C%250Athese%2520tasks%2520incur%2520substantially%2520higher%2520computational%2520overhead%2520than%2520traditional%250Acode%2520generation%2520due%2520to%2520the%2520large%2520number%2520of%2520input%2520image%2520tokens%2520and%2520extensive%250Aoutput%2520code%2520tokens%2520required.%2520Our%2520comprehensive%2520study%2520identifies%2520significant%250Aredundancies%2520in%2520both%2520image%2520and%2520code%2520tokens%2520that%2520exacerbate%2520computational%250Acomplexity%2520and%2520hinder%2520focus%2520on%2520key%2520UI%2520elements%252C%2520resulting%2520in%2520excessively%250Alengthy%2520and%2520often%2520invalid%2520HTML%2520files.%2520We%2520propose%2520EfficientUICoder%252C%2520a%250Acompression%2520framework%2520for%2520efficient%2520UI%2520code%2520generation%2520with%2520three%2520key%250Acomponents.%2520First%252C%2520Element%2520and%2520Layout-aware%2520Token%2520Compression%2520preserves%250Aessential%2520UI%2520information%2520by%2520detecting%2520element%2520regions%2520and%2520constructing%2520UI%250Aelement%2520trees.%2520Second%252C%2520Region-aware%2520Token%2520Refinement%2520leverages%2520attention%2520scores%250Ato%2520discard%2520low-attention%2520tokens%2520from%2520selected%2520regions%2520while%2520integrating%250Ahigh-attention%2520tokens%2520from%2520unselected%2520regions.%2520Third%252C%2520Adaptive%2520Duplicate%2520Token%250ASuppression%2520dynamically%2520reduces%2520repetitive%2520generation%2520by%2520tracking%2520HTML/CSS%250Astructure%2520frequencies%2520and%2520applying%2520exponential%2520penalties.%2520Extensive%2520experiments%250Ashow%2520EfficientUICoderachieves%2520a%252055%2525-60%2525%2520compression%2520ratio%2520without%2520compromising%250Awebpage%2520quality%2520and%2520delivers%2520superior%2520efficiency%2520improvements%253A%2520reducing%250Acomputational%2520cost%2520by%252044.9%2525%252C%2520generated%2520tokens%2520by%252041.4%2525%252C%2520prefill%2520time%2520by%252046.6%2525%252C%250Aand%2520inference%2520time%2520by%252048.8%2525%2520on%252034B-level%2520MLLMs.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/WebPAI/EfficientUICoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EfficientUICoder%3A%20Efficient%20MLLM-based%20UI%20Code%20Generation%20via%20Input%20and%0A%20%20Output%20Token%20Compression&entry.906535625=Jingyu%20Xiao%20and%20Zhongyi%20Zhang%20and%20Yuxuan%20Wan%20and%20Yintong%20Huo%20and%20Yang%20Liu%20and%20Michael%20R.%20Lyu&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20have%20demonstrated%20exceptional%20performance%20in%0AUI2Code%20tasks%2C%20significantly%20enhancing%20website%20development%20efficiency.%20However%2C%0Athese%20tasks%20incur%20substantially%20higher%20computational%20overhead%20than%20traditional%0Acode%20generation%20due%20to%20the%20large%20number%20of%20input%20image%20tokens%20and%20extensive%0Aoutput%20code%20tokens%20required.%20Our%20comprehensive%20study%20identifies%20significant%0Aredundancies%20in%20both%20image%20and%20code%20tokens%20that%20exacerbate%20computational%0Acomplexity%20and%20hinder%20focus%20on%20key%20UI%20elements%2C%20resulting%20in%20excessively%0Alengthy%20and%20often%20invalid%20HTML%20files.%20We%20propose%20EfficientUICoder%2C%20a%0Acompression%20framework%20for%20efficient%20UI%20code%20generation%20with%20three%20key%0Acomponents.%20First%2C%20Element%20and%20Layout-aware%20Token%20Compression%20preserves%0Aessential%20UI%20information%20by%20detecting%20element%20regions%20and%20constructing%20UI%0Aelement%20trees.%20Second%2C%20Region-aware%20Token%20Refinement%20leverages%20attention%20scores%0Ato%20discard%20low-attention%20tokens%20from%20selected%20regions%20while%20integrating%0Ahigh-attention%20tokens%20from%20unselected%20regions.%20Third%2C%20Adaptive%20Duplicate%20Token%0ASuppression%20dynamically%20reduces%20repetitive%20generation%20by%20tracking%20HTML/CSS%0Astructure%20frequencies%20and%20applying%20exponential%20penalties.%20Extensive%20experiments%0Ashow%20EfficientUICoderachieves%20a%2055%25-60%25%20compression%20ratio%20without%20compromising%0Awebpage%20quality%20and%20delivers%20superior%20efficiency%20improvements%3A%20reducing%0Acomputational%20cost%20by%2044.9%25%2C%20generated%20tokens%20by%2041.4%25%2C%20prefill%20time%20by%2046.6%25%2C%0Aand%20inference%20time%20by%2048.8%25%20on%2034B-level%20MLLMs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/WebPAI/EfficientUICoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12159v1&entry.124074799=Read"},
{"title": "Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive\n  Adaptation", "author": "Yubo Li and Weiyi Song", "abstract": "  Current AI alignment through RLHF follows a single directional paradigm that\nAI conforms to human preferences while treating human cognition as fixed. We\npropose a shift to co-alignment through Bidirectional Cognitive Alignment\n(BiCA), where humans and AI mutually adapt. BiCA uses learnable protocols,\nrepresentation mapping, and KL-budget constraints for controlled co-evolution.\nIn collaborative navigation, BiCA achieved 85.5% success versus 70.3% baseline,\nwith 230% better mutual adaptation and 332% better protocol convergence.\nEmergent protocols outperformed handcrafted ones by 84%, while bidirectional\nadaptation unexpectedly improved safety (+23% out-of-distribution robustness).\nThe 46% synergy improvement demonstrates optimal collaboration exists at the\nintersection, not union, of human and AI capabilities, validating the shift\nfrom single-directional to co-alignment paradigms.\n", "link": "http://arxiv.org/abs/2509.12179v1", "date": "2025-09-15", "relevancy": 2.0553, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5362}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5184}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Co-Alignment%3A%20Rethinking%20Alignment%20as%20Bidirectional%20Human-AI%20Cognitive%0A%20%20Adaptation&body=Title%3A%20Co-Alignment%3A%20Rethinking%20Alignment%20as%20Bidirectional%20Human-AI%20Cognitive%0A%20%20Adaptation%0AAuthor%3A%20Yubo%20Li%20and%20Weiyi%20Song%0AAbstract%3A%20%20%20Current%20AI%20alignment%20through%20RLHF%20follows%20a%20single%20directional%20paradigm%20that%0AAI%20conforms%20to%20human%20preferences%20while%20treating%20human%20cognition%20as%20fixed.%20We%0Apropose%20a%20shift%20to%20co-alignment%20through%20Bidirectional%20Cognitive%20Alignment%0A%28BiCA%29%2C%20where%20humans%20and%20AI%20mutually%20adapt.%20BiCA%20uses%20learnable%20protocols%2C%0Arepresentation%20mapping%2C%20and%20KL-budget%20constraints%20for%20controlled%20co-evolution.%0AIn%20collaborative%20navigation%2C%20BiCA%20achieved%2085.5%25%20success%20versus%2070.3%25%20baseline%2C%0Awith%20230%25%20better%20mutual%20adaptation%20and%20332%25%20better%20protocol%20convergence.%0AEmergent%20protocols%20outperformed%20handcrafted%20ones%20by%2084%25%2C%20while%20bidirectional%0Aadaptation%20unexpectedly%20improved%20safety%20%28%2B23%25%20out-of-distribution%20robustness%29.%0AThe%2046%25%20synergy%20improvement%20demonstrates%20optimal%20collaboration%20exists%20at%20the%0Aintersection%2C%20not%20union%2C%20of%20human%20and%20AI%20capabilities%2C%20validating%20the%20shift%0Afrom%20single-directional%20to%20co-alignment%20paradigms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCo-Alignment%253A%2520Rethinking%2520Alignment%2520as%2520Bidirectional%2520Human-AI%2520Cognitive%250A%2520%2520Adaptation%26entry.906535625%3DYubo%2520Li%2520and%2520Weiyi%2520Song%26entry.1292438233%3D%2520%2520Current%2520AI%2520alignment%2520through%2520RLHF%2520follows%2520a%2520single%2520directional%2520paradigm%2520that%250AAI%2520conforms%2520to%2520human%2520preferences%2520while%2520treating%2520human%2520cognition%2520as%2520fixed.%2520We%250Apropose%2520a%2520shift%2520to%2520co-alignment%2520through%2520Bidirectional%2520Cognitive%2520Alignment%250A%2528BiCA%2529%252C%2520where%2520humans%2520and%2520AI%2520mutually%2520adapt.%2520BiCA%2520uses%2520learnable%2520protocols%252C%250Arepresentation%2520mapping%252C%2520and%2520KL-budget%2520constraints%2520for%2520controlled%2520co-evolution.%250AIn%2520collaborative%2520navigation%252C%2520BiCA%2520achieved%252085.5%2525%2520success%2520versus%252070.3%2525%2520baseline%252C%250Awith%2520230%2525%2520better%2520mutual%2520adaptation%2520and%2520332%2525%2520better%2520protocol%2520convergence.%250AEmergent%2520protocols%2520outperformed%2520handcrafted%2520ones%2520by%252084%2525%252C%2520while%2520bidirectional%250Aadaptation%2520unexpectedly%2520improved%2520safety%2520%2528%252B23%2525%2520out-of-distribution%2520robustness%2529.%250AThe%252046%2525%2520synergy%2520improvement%2520demonstrates%2520optimal%2520collaboration%2520exists%2520at%2520the%250Aintersection%252C%2520not%2520union%252C%2520of%2520human%2520and%2520AI%2520capabilities%252C%2520validating%2520the%2520shift%250Afrom%2520single-directional%2520to%2520co-alignment%2520paradigms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-Alignment%3A%20Rethinking%20Alignment%20as%20Bidirectional%20Human-AI%20Cognitive%0A%20%20Adaptation&entry.906535625=Yubo%20Li%20and%20Weiyi%20Song&entry.1292438233=%20%20Current%20AI%20alignment%20through%20RLHF%20follows%20a%20single%20directional%20paradigm%20that%0AAI%20conforms%20to%20human%20preferences%20while%20treating%20human%20cognition%20as%20fixed.%20We%0Apropose%20a%20shift%20to%20co-alignment%20through%20Bidirectional%20Cognitive%20Alignment%0A%28BiCA%29%2C%20where%20humans%20and%20AI%20mutually%20adapt.%20BiCA%20uses%20learnable%20protocols%2C%0Arepresentation%20mapping%2C%20and%20KL-budget%20constraints%20for%20controlled%20co-evolution.%0AIn%20collaborative%20navigation%2C%20BiCA%20achieved%2085.5%25%20success%20versus%2070.3%25%20baseline%2C%0Awith%20230%25%20better%20mutual%20adaptation%20and%20332%25%20better%20protocol%20convergence.%0AEmergent%20protocols%20outperformed%20handcrafted%20ones%20by%2084%25%2C%20while%20bidirectional%0Aadaptation%20unexpectedly%20improved%20safety%20%28%2B23%25%20out-of-distribution%20robustness%29.%0AThe%2046%25%20synergy%20improvement%20demonstrates%20optimal%20collaboration%20exists%20at%20the%0Aintersection%2C%20not%20union%2C%20of%20human%20and%20AI%20capabilities%2C%20validating%20the%20shift%0Afrom%20single-directional%20to%20co-alignment%20paradigms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12179v1&entry.124074799=Read"},
{"title": "Automatic quality control in multi-centric fetal brain MRI\n  super-resolution reconstruction", "author": "Thomas Sanchez and Vladyslav Zalevskyi and Angeline Mihailov and Gerard Mart\u00ed-Juan and Elisenda Eixarch and Andras Jakab and Vincent Dunet and M\u00e9riam Koob and Guillaume Auzias and Meritxell Bach Cuadra", "abstract": "  Quality control (QC) has long been considered essential to guarantee the\nreliability of neuroimaging studies. It is particularly important for fetal\nbrain MRI, where acquisitions and image processing techniques are less\nstandardized than in adult imaging. In this work, we focus on automated quality\ncontrol of super-resolution reconstruction (SRR) volumes of fetal brain MRI, an\nimportant processing step where multiple stacks of thick 2D slices are\nregistered together and combined to build a single, isotropic and artifact-free\nT2 weighted volume. We propose FetMRQC$_{SR}$, a machine-learning method that\nextracts more than 100 image quality metrics to predict image quality scores\nusing a random forest model. This approach is well suited to a problem that is\nhigh dimensional, with highly heterogeneous data and small datasets. We\nvalidate FetMRQC$_{SR}$ in an out-of-domain (OOD) setting and report high\nperformance (ROC AUC = 0.89), even when faced with data from an unknown site or\nSRR method. We also investigate failure cases and show that they occur in\n$45\\%$ of the images due to ambiguous configurations for which the rating from\nthe expert is arguable. These results are encouraging and illustrate how a non\ndeep learning-based method like FetMRQC$_{SR}$ is well suited to this\nmultifaceted problem. Our tool, along with all the code used to generate, train\nand evaluate the model are available at\nhttps://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/ .\n", "link": "http://arxiv.org/abs/2503.10156v4", "date": "2025-09-15", "relevancy": 2.0497, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5206}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5123}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20quality%20control%20in%20multi-centric%20fetal%20brain%20MRI%0A%20%20super-resolution%20reconstruction&body=Title%3A%20Automatic%20quality%20control%20in%20multi-centric%20fetal%20brain%20MRI%0A%20%20super-resolution%20reconstruction%0AAuthor%3A%20Thomas%20Sanchez%20and%20Vladyslav%20Zalevskyi%20and%20Angeline%20Mihailov%20and%20Gerard%20Mart%C3%AD-Juan%20and%20Elisenda%20Eixarch%20and%20Andras%20Jakab%20and%20Vincent%20Dunet%20and%20M%C3%A9riam%20Koob%20and%20Guillaume%20Auzias%20and%20Meritxell%20Bach%20Cuadra%0AAbstract%3A%20%20%20Quality%20control%20%28QC%29%20has%20long%20been%20considered%20essential%20to%20guarantee%20the%0Areliability%20of%20neuroimaging%20studies.%20It%20is%20particularly%20important%20for%20fetal%0Abrain%20MRI%2C%20where%20acquisitions%20and%20image%20processing%20techniques%20are%20less%0Astandardized%20than%20in%20adult%20imaging.%20In%20this%20work%2C%20we%20focus%20on%20automated%20quality%0Acontrol%20of%20super-resolution%20reconstruction%20%28SRR%29%20volumes%20of%20fetal%20brain%20MRI%2C%20an%0Aimportant%20processing%20step%20where%20multiple%20stacks%20of%20thick%202D%20slices%20are%0Aregistered%20together%20and%20combined%20to%20build%20a%20single%2C%20isotropic%20and%20artifact-free%0AT2%20weighted%20volume.%20We%20propose%20FetMRQC%24_%7BSR%7D%24%2C%20a%20machine-learning%20method%20that%0Aextracts%20more%20than%20100%20image%20quality%20metrics%20to%20predict%20image%20quality%20scores%0Ausing%20a%20random%20forest%20model.%20This%20approach%20is%20well%20suited%20to%20a%20problem%20that%20is%0Ahigh%20dimensional%2C%20with%20highly%20heterogeneous%20data%20and%20small%20datasets.%20We%0Avalidate%20FetMRQC%24_%7BSR%7D%24%20in%20an%20out-of-domain%20%28OOD%29%20setting%20and%20report%20high%0Aperformance%20%28ROC%20AUC%20%3D%200.89%29%2C%20even%20when%20faced%20with%20data%20from%20an%20unknown%20site%20or%0ASRR%20method.%20We%20also%20investigate%20failure%20cases%20and%20show%20that%20they%20occur%20in%0A%2445%5C%25%24%20of%20the%20images%20due%20to%20ambiguous%20configurations%20for%20which%20the%20rating%20from%0Athe%20expert%20is%20arguable.%20These%20results%20are%20encouraging%20and%20illustrate%20how%20a%20non%0Adeep%20learning-based%20method%20like%20FetMRQC%24_%7BSR%7D%24%20is%20well%20suited%20to%20this%0Amultifaceted%20problem.%20Our%20tool%2C%20along%20with%20all%20the%20code%20used%20to%20generate%2C%20train%0Aand%20evaluate%20the%20model%20are%20available%20at%0Ahttps%3A//github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10156v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520quality%2520control%2520in%2520multi-centric%2520fetal%2520brain%2520MRI%250A%2520%2520super-resolution%2520reconstruction%26entry.906535625%3DThomas%2520Sanchez%2520and%2520Vladyslav%2520Zalevskyi%2520and%2520Angeline%2520Mihailov%2520and%2520Gerard%2520Mart%25C3%25AD-Juan%2520and%2520Elisenda%2520Eixarch%2520and%2520Andras%2520Jakab%2520and%2520Vincent%2520Dunet%2520and%2520M%25C3%25A9riam%2520Koob%2520and%2520Guillaume%2520Auzias%2520and%2520Meritxell%2520Bach%2520Cuadra%26entry.1292438233%3D%2520%2520Quality%2520control%2520%2528QC%2529%2520has%2520long%2520been%2520considered%2520essential%2520to%2520guarantee%2520the%250Areliability%2520of%2520neuroimaging%2520studies.%2520It%2520is%2520particularly%2520important%2520for%2520fetal%250Abrain%2520MRI%252C%2520where%2520acquisitions%2520and%2520image%2520processing%2520techniques%2520are%2520less%250Astandardized%2520than%2520in%2520adult%2520imaging.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520automated%2520quality%250Acontrol%2520of%2520super-resolution%2520reconstruction%2520%2528SRR%2529%2520volumes%2520of%2520fetal%2520brain%2520MRI%252C%2520an%250Aimportant%2520processing%2520step%2520where%2520multiple%2520stacks%2520of%2520thick%25202D%2520slices%2520are%250Aregistered%2520together%2520and%2520combined%2520to%2520build%2520a%2520single%252C%2520isotropic%2520and%2520artifact-free%250AT2%2520weighted%2520volume.%2520We%2520propose%2520FetMRQC%2524_%257BSR%257D%2524%252C%2520a%2520machine-learning%2520method%2520that%250Aextracts%2520more%2520than%2520100%2520image%2520quality%2520metrics%2520to%2520predict%2520image%2520quality%2520scores%250Ausing%2520a%2520random%2520forest%2520model.%2520This%2520approach%2520is%2520well%2520suited%2520to%2520a%2520problem%2520that%2520is%250Ahigh%2520dimensional%252C%2520with%2520highly%2520heterogeneous%2520data%2520and%2520small%2520datasets.%2520We%250Avalidate%2520FetMRQC%2524_%257BSR%257D%2524%2520in%2520an%2520out-of-domain%2520%2528OOD%2529%2520setting%2520and%2520report%2520high%250Aperformance%2520%2528ROC%2520AUC%2520%253D%25200.89%2529%252C%2520even%2520when%2520faced%2520with%2520data%2520from%2520an%2520unknown%2520site%2520or%250ASRR%2520method.%2520We%2520also%2520investigate%2520failure%2520cases%2520and%2520show%2520that%2520they%2520occur%2520in%250A%252445%255C%2525%2524%2520of%2520the%2520images%2520due%2520to%2520ambiguous%2520configurations%2520for%2520which%2520the%2520rating%2520from%250Athe%2520expert%2520is%2520arguable.%2520These%2520results%2520are%2520encouraging%2520and%2520illustrate%2520how%2520a%2520non%250Adeep%2520learning-based%2520method%2520like%2520FetMRQC%2524_%257BSR%257D%2524%2520is%2520well%2520suited%2520to%2520this%250Amultifaceted%2520problem.%2520Our%2520tool%252C%2520along%2520with%2520all%2520the%2520code%2520used%2520to%2520generate%252C%2520train%250Aand%2520evaluate%2520the%2520model%2520are%2520available%2520at%250Ahttps%253A//github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10156v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20quality%20control%20in%20multi-centric%20fetal%20brain%20MRI%0A%20%20super-resolution%20reconstruction&entry.906535625=Thomas%20Sanchez%20and%20Vladyslav%20Zalevskyi%20and%20Angeline%20Mihailov%20and%20Gerard%20Mart%C3%AD-Juan%20and%20Elisenda%20Eixarch%20and%20Andras%20Jakab%20and%20Vincent%20Dunet%20and%20M%C3%A9riam%20Koob%20and%20Guillaume%20Auzias%20and%20Meritxell%20Bach%20Cuadra&entry.1292438233=%20%20Quality%20control%20%28QC%29%20has%20long%20been%20considered%20essential%20to%20guarantee%20the%0Areliability%20of%20neuroimaging%20studies.%20It%20is%20particularly%20important%20for%20fetal%0Abrain%20MRI%2C%20where%20acquisitions%20and%20image%20processing%20techniques%20are%20less%0Astandardized%20than%20in%20adult%20imaging.%20In%20this%20work%2C%20we%20focus%20on%20automated%20quality%0Acontrol%20of%20super-resolution%20reconstruction%20%28SRR%29%20volumes%20of%20fetal%20brain%20MRI%2C%20an%0Aimportant%20processing%20step%20where%20multiple%20stacks%20of%20thick%202D%20slices%20are%0Aregistered%20together%20and%20combined%20to%20build%20a%20single%2C%20isotropic%20and%20artifact-free%0AT2%20weighted%20volume.%20We%20propose%20FetMRQC%24_%7BSR%7D%24%2C%20a%20machine-learning%20method%20that%0Aextracts%20more%20than%20100%20image%20quality%20metrics%20to%20predict%20image%20quality%20scores%0Ausing%20a%20random%20forest%20model.%20This%20approach%20is%20well%20suited%20to%20a%20problem%20that%20is%0Ahigh%20dimensional%2C%20with%20highly%20heterogeneous%20data%20and%20small%20datasets.%20We%0Avalidate%20FetMRQC%24_%7BSR%7D%24%20in%20an%20out-of-domain%20%28OOD%29%20setting%20and%20report%20high%0Aperformance%20%28ROC%20AUC%20%3D%200.89%29%2C%20even%20when%20faced%20with%20data%20from%20an%20unknown%20site%20or%0ASRR%20method.%20We%20also%20investigate%20failure%20cases%20and%20show%20that%20they%20occur%20in%0A%2445%5C%25%24%20of%20the%20images%20due%20to%20ambiguous%20configurations%20for%20which%20the%20rating%20from%0Athe%20expert%20is%20arguable.%20These%20results%20are%20encouraging%20and%20illustrate%20how%20a%20non%0Adeep%20learning-based%20method%20like%20FetMRQC%24_%7BSR%7D%24%20is%20well%20suited%20to%20this%0Amultifaceted%20problem.%20Our%20tool%2C%20along%20with%20all%20the%20code%20used%20to%20generate%2C%20train%0Aand%20evaluate%20the%20model%20are%20available%20at%0Ahttps%3A//github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10156v4&entry.124074799=Read"},
{"title": "Low-rank variational dropout: Uncertainty and rank selection in adapters", "author": "Cooper Doyle", "abstract": "  Parameter-efficient fine-tuning (PEFT) methods such as LoRA adapt large\nlanguage models by inserting low-rank adapters, but they leave open two key\nquestions: how to give the adapted model calibrated uncertainty, and how to\nchoose the adapter rank. Existing approaches to uncertainty are typically\npost-hoc, while rank selection is manual and task-specific. BayesLoRA revisits\nvariational dropout in the LoRA setting and shows that the natural unit of\nstochasticity is not individual weights but entire ranks of the adapter. By\nplacing rank-wise variational distributions over adapter components, BayesLoRA\ndefines a posterior that (i) yields calibrated predictions through adapter-only\nMonte Carlo sampling and (ii) prunes redundant ranks automatically via an\nARD-style KL term. Theoretical analysis shows that this rank-parameterized\nposterior localizes uncertainty to the adapted subspace and explains\namplification under distribution shift. Empirically, BayesLoRA improves\ncalibration while at the same time producing lighter, faster adapters, removing\nthe need to tune ranks by hand. This dual role of uncertainty estimation and\nuncertainty-driven pruning suggests BayesLoRA may offer a practical default for\nreliable and efficient PEFT.\n", "link": "http://arxiv.org/abs/2506.22809v2", "date": "2025-09-15", "relevancy": 1.4738, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5023}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4918}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-rank%20variational%20dropout%3A%20Uncertainty%20and%20rank%20selection%20in%20adapters&body=Title%3A%20Low-rank%20variational%20dropout%3A%20Uncertainty%20and%20rank%20selection%20in%20adapters%0AAuthor%3A%20Cooper%20Doyle%0AAbstract%3A%20%20%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20such%20as%20LoRA%20adapt%20large%0Alanguage%20models%20by%20inserting%20low-rank%20adapters%2C%20but%20they%20leave%20open%20two%20key%0Aquestions%3A%20how%20to%20give%20the%20adapted%20model%20calibrated%20uncertainty%2C%20and%20how%20to%0Achoose%20the%20adapter%20rank.%20Existing%20approaches%20to%20uncertainty%20are%20typically%0Apost-hoc%2C%20while%20rank%20selection%20is%20manual%20and%20task-specific.%20BayesLoRA%20revisits%0Avariational%20dropout%20in%20the%20LoRA%20setting%20and%20shows%20that%20the%20natural%20unit%20of%0Astochasticity%20is%20not%20individual%20weights%20but%20entire%20ranks%20of%20the%20adapter.%20By%0Aplacing%20rank-wise%20variational%20distributions%20over%20adapter%20components%2C%20BayesLoRA%0Adefines%20a%20posterior%20that%20%28i%29%20yields%20calibrated%20predictions%20through%20adapter-only%0AMonte%20Carlo%20sampling%20and%20%28ii%29%20prunes%20redundant%20ranks%20automatically%20via%20an%0AARD-style%20KL%20term.%20Theoretical%20analysis%20shows%20that%20this%20rank-parameterized%0Aposterior%20localizes%20uncertainty%20to%20the%20adapted%20subspace%20and%20explains%0Aamplification%20under%20distribution%20shift.%20Empirically%2C%20BayesLoRA%20improves%0Acalibration%20while%20at%20the%20same%20time%20producing%20lighter%2C%20faster%20adapters%2C%20removing%0Athe%20need%20to%20tune%20ranks%20by%20hand.%20This%20dual%20role%20of%20uncertainty%20estimation%20and%0Auncertainty-driven%20pruning%20suggests%20BayesLoRA%20may%20offer%20a%20practical%20default%20for%0Areliable%20and%20efficient%20PEFT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.22809v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-rank%2520variational%2520dropout%253A%2520Uncertainty%2520and%2520rank%2520selection%2520in%2520adapters%26entry.906535625%3DCooper%2520Doyle%26entry.1292438233%3D%2520%2520Parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520methods%2520such%2520as%2520LoRA%2520adapt%2520large%250Alanguage%2520models%2520by%2520inserting%2520low-rank%2520adapters%252C%2520but%2520they%2520leave%2520open%2520two%2520key%250Aquestions%253A%2520how%2520to%2520give%2520the%2520adapted%2520model%2520calibrated%2520uncertainty%252C%2520and%2520how%2520to%250Achoose%2520the%2520adapter%2520rank.%2520Existing%2520approaches%2520to%2520uncertainty%2520are%2520typically%250Apost-hoc%252C%2520while%2520rank%2520selection%2520is%2520manual%2520and%2520task-specific.%2520BayesLoRA%2520revisits%250Avariational%2520dropout%2520in%2520the%2520LoRA%2520setting%2520and%2520shows%2520that%2520the%2520natural%2520unit%2520of%250Astochasticity%2520is%2520not%2520individual%2520weights%2520but%2520entire%2520ranks%2520of%2520the%2520adapter.%2520By%250Aplacing%2520rank-wise%2520variational%2520distributions%2520over%2520adapter%2520components%252C%2520BayesLoRA%250Adefines%2520a%2520posterior%2520that%2520%2528i%2529%2520yields%2520calibrated%2520predictions%2520through%2520adapter-only%250AMonte%2520Carlo%2520sampling%2520and%2520%2528ii%2529%2520prunes%2520redundant%2520ranks%2520automatically%2520via%2520an%250AARD-style%2520KL%2520term.%2520Theoretical%2520analysis%2520shows%2520that%2520this%2520rank-parameterized%250Aposterior%2520localizes%2520uncertainty%2520to%2520the%2520adapted%2520subspace%2520and%2520explains%250Aamplification%2520under%2520distribution%2520shift.%2520Empirically%252C%2520BayesLoRA%2520improves%250Acalibration%2520while%2520at%2520the%2520same%2520time%2520producing%2520lighter%252C%2520faster%2520adapters%252C%2520removing%250Athe%2520need%2520to%2520tune%2520ranks%2520by%2520hand.%2520This%2520dual%2520role%2520of%2520uncertainty%2520estimation%2520and%250Auncertainty-driven%2520pruning%2520suggests%2520BayesLoRA%2520may%2520offer%2520a%2520practical%2520default%2520for%250Areliable%2520and%2520efficient%2520PEFT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.22809v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-rank%20variational%20dropout%3A%20Uncertainty%20and%20rank%20selection%20in%20adapters&entry.906535625=Cooper%20Doyle&entry.1292438233=%20%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20such%20as%20LoRA%20adapt%20large%0Alanguage%20models%20by%20inserting%20low-rank%20adapters%2C%20but%20they%20leave%20open%20two%20key%0Aquestions%3A%20how%20to%20give%20the%20adapted%20model%20calibrated%20uncertainty%2C%20and%20how%20to%0Achoose%20the%20adapter%20rank.%20Existing%20approaches%20to%20uncertainty%20are%20typically%0Apost-hoc%2C%20while%20rank%20selection%20is%20manual%20and%20task-specific.%20BayesLoRA%20revisits%0Avariational%20dropout%20in%20the%20LoRA%20setting%20and%20shows%20that%20the%20natural%20unit%20of%0Astochasticity%20is%20not%20individual%20weights%20but%20entire%20ranks%20of%20the%20adapter.%20By%0Aplacing%20rank-wise%20variational%20distributions%20over%20adapter%20components%2C%20BayesLoRA%0Adefines%20a%20posterior%20that%20%28i%29%20yields%20calibrated%20predictions%20through%20adapter-only%0AMonte%20Carlo%20sampling%20and%20%28ii%29%20prunes%20redundant%20ranks%20automatically%20via%20an%0AARD-style%20KL%20term.%20Theoretical%20analysis%20shows%20that%20this%20rank-parameterized%0Aposterior%20localizes%20uncertainty%20to%20the%20adapted%20subspace%20and%20explains%0Aamplification%20under%20distribution%20shift.%20Empirically%2C%20BayesLoRA%20improves%0Acalibration%20while%20at%20the%20same%20time%20producing%20lighter%2C%20faster%20adapters%2C%20removing%0Athe%20need%20to%20tune%20ranks%20by%20hand.%20This%20dual%20role%20of%20uncertainty%20estimation%20and%0Auncertainty-driven%20pruning%20suggests%20BayesLoRA%20may%20offer%20a%20practical%20default%20for%0Areliable%20and%20efficient%20PEFT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.22809v2&entry.124074799=Read"},
{"title": "Low-rank Orthogonalization for Large-scale Matrix Optimization with\n  Applications to Foundation Model Training", "author": "Chuan He and Zhanwang Deng and Zhaosong Lu", "abstract": "  Neural network (NN) training is inherently a large-scale matrix optimization\nproblem, yet the matrix structure of NN parameters has long been overlooked.\nRecently, the optimizer Muon \\cite{jordanmuon}, which explicitly exploits this\nstructure, has gained significant attention for its strong performance in\nfoundation model training. A key component contributing to Muon's success is\nmatrix orthogonalization. In this paper, we propose {\\it low-rank\northogonalization}, which explicitly leverages the low-rank nature of gradients\nduring NN training. Building on this, we propose low-rank matrix-signed\ngradient descent and a low-rank variant of Muon. Our numerical experiments\ndemonstrate the superior performance of low-rank orthogonalization, with the\nlow-rank Muon achieving promising results in GPT-2 and LLaMA pretraining --\nsurpassing the performance of the carefully tuned vanilla Muon. Theoretically,\nwe establish the iteration complexity of the low-rank matrix-signed gradient\ndescent for finding an approximate stationary solution, as well as that of\nlow-rank Muon for finding an approximate stochastic stationary solution under\nheavy-tailed noise.\n", "link": "http://arxiv.org/abs/2509.11983v1", "date": "2025-09-15", "relevancy": 1.9171, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4912}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.473}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-rank%20Orthogonalization%20for%20Large-scale%20Matrix%20Optimization%20with%0A%20%20Applications%20to%20Foundation%20Model%20Training&body=Title%3A%20Low-rank%20Orthogonalization%20for%20Large-scale%20Matrix%20Optimization%20with%0A%20%20Applications%20to%20Foundation%20Model%20Training%0AAuthor%3A%20Chuan%20He%20and%20Zhanwang%20Deng%20and%20Zhaosong%20Lu%0AAbstract%3A%20%20%20Neural%20network%20%28NN%29%20training%20is%20inherently%20a%20large-scale%20matrix%20optimization%0Aproblem%2C%20yet%20the%20matrix%20structure%20of%20NN%20parameters%20has%20long%20been%20overlooked.%0ARecently%2C%20the%20optimizer%20Muon%20%5Ccite%7Bjordanmuon%7D%2C%20which%20explicitly%20exploits%20this%0Astructure%2C%20has%20gained%20significant%20attention%20for%20its%20strong%20performance%20in%0Afoundation%20model%20training.%20A%20key%20component%20contributing%20to%20Muon%27s%20success%20is%0Amatrix%20orthogonalization.%20In%20this%20paper%2C%20we%20propose%20%7B%5Cit%20low-rank%0Aorthogonalization%7D%2C%20which%20explicitly%20leverages%20the%20low-rank%20nature%20of%20gradients%0Aduring%20NN%20training.%20Building%20on%20this%2C%20we%20propose%20low-rank%20matrix-signed%0Agradient%20descent%20and%20a%20low-rank%20variant%20of%20Muon.%20Our%20numerical%20experiments%0Ademonstrate%20the%20superior%20performance%20of%20low-rank%20orthogonalization%2C%20with%20the%0Alow-rank%20Muon%20achieving%20promising%20results%20in%20GPT-2%20and%20LLaMA%20pretraining%20--%0Asurpassing%20the%20performance%20of%20the%20carefully%20tuned%20vanilla%20Muon.%20Theoretically%2C%0Awe%20establish%20the%20iteration%20complexity%20of%20the%20low-rank%20matrix-signed%20gradient%0Adescent%20for%20finding%20an%20approximate%20stationary%20solution%2C%20as%20well%20as%20that%20of%0Alow-rank%20Muon%20for%20finding%20an%20approximate%20stochastic%20stationary%20solution%20under%0Aheavy-tailed%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-rank%2520Orthogonalization%2520for%2520Large-scale%2520Matrix%2520Optimization%2520with%250A%2520%2520Applications%2520to%2520Foundation%2520Model%2520Training%26entry.906535625%3DChuan%2520He%2520and%2520Zhanwang%2520Deng%2520and%2520Zhaosong%2520Lu%26entry.1292438233%3D%2520%2520Neural%2520network%2520%2528NN%2529%2520training%2520is%2520inherently%2520a%2520large-scale%2520matrix%2520optimization%250Aproblem%252C%2520yet%2520the%2520matrix%2520structure%2520of%2520NN%2520parameters%2520has%2520long%2520been%2520overlooked.%250ARecently%252C%2520the%2520optimizer%2520Muon%2520%255Ccite%257Bjordanmuon%257D%252C%2520which%2520explicitly%2520exploits%2520this%250Astructure%252C%2520has%2520gained%2520significant%2520attention%2520for%2520its%2520strong%2520performance%2520in%250Afoundation%2520model%2520training.%2520A%2520key%2520component%2520contributing%2520to%2520Muon%2527s%2520success%2520is%250Amatrix%2520orthogonalization.%2520In%2520this%2520paper%252C%2520we%2520propose%2520%257B%255Cit%2520low-rank%250Aorthogonalization%257D%252C%2520which%2520explicitly%2520leverages%2520the%2520low-rank%2520nature%2520of%2520gradients%250Aduring%2520NN%2520training.%2520Building%2520on%2520this%252C%2520we%2520propose%2520low-rank%2520matrix-signed%250Agradient%2520descent%2520and%2520a%2520low-rank%2520variant%2520of%2520Muon.%2520Our%2520numerical%2520experiments%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520low-rank%2520orthogonalization%252C%2520with%2520the%250Alow-rank%2520Muon%2520achieving%2520promising%2520results%2520in%2520GPT-2%2520and%2520LLaMA%2520pretraining%2520--%250Asurpassing%2520the%2520performance%2520of%2520the%2520carefully%2520tuned%2520vanilla%2520Muon.%2520Theoretically%252C%250Awe%2520establish%2520the%2520iteration%2520complexity%2520of%2520the%2520low-rank%2520matrix-signed%2520gradient%250Adescent%2520for%2520finding%2520an%2520approximate%2520stationary%2520solution%252C%2520as%2520well%2520as%2520that%2520of%250Alow-rank%2520Muon%2520for%2520finding%2520an%2520approximate%2520stochastic%2520stationary%2520solution%2520under%250Aheavy-tailed%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-rank%20Orthogonalization%20for%20Large-scale%20Matrix%20Optimization%20with%0A%20%20Applications%20to%20Foundation%20Model%20Training&entry.906535625=Chuan%20He%20and%20Zhanwang%20Deng%20and%20Zhaosong%20Lu&entry.1292438233=%20%20Neural%20network%20%28NN%29%20training%20is%20inherently%20a%20large-scale%20matrix%20optimization%0Aproblem%2C%20yet%20the%20matrix%20structure%20of%20NN%20parameters%20has%20long%20been%20overlooked.%0ARecently%2C%20the%20optimizer%20Muon%20%5Ccite%7Bjordanmuon%7D%2C%20which%20explicitly%20exploits%20this%0Astructure%2C%20has%20gained%20significant%20attention%20for%20its%20strong%20performance%20in%0Afoundation%20model%20training.%20A%20key%20component%20contributing%20to%20Muon%27s%20success%20is%0Amatrix%20orthogonalization.%20In%20this%20paper%2C%20we%20propose%20%7B%5Cit%20low-rank%0Aorthogonalization%7D%2C%20which%20explicitly%20leverages%20the%20low-rank%20nature%20of%20gradients%0Aduring%20NN%20training.%20Building%20on%20this%2C%20we%20propose%20low-rank%20matrix-signed%0Agradient%20descent%20and%20a%20low-rank%20variant%20of%20Muon.%20Our%20numerical%20experiments%0Ademonstrate%20the%20superior%20performance%20of%20low-rank%20orthogonalization%2C%20with%20the%0Alow-rank%20Muon%20achieving%20promising%20results%20in%20GPT-2%20and%20LLaMA%20pretraining%20--%0Asurpassing%20the%20performance%20of%20the%20carefully%20tuned%20vanilla%20Muon.%20Theoretically%2C%0Awe%20establish%20the%20iteration%20complexity%20of%20the%20low-rank%20matrix-signed%20gradient%0Adescent%20for%20finding%20an%20approximate%20stationary%20solution%2C%20as%20well%20as%20that%20of%0Alow-rank%20Muon%20for%20finding%20an%20approximate%20stochastic%20stationary%20solution%20under%0Aheavy-tailed%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11983v1&entry.124074799=Read"},
{"title": "RouteFinder: Towards Foundation Models for Vehicle Routing Problems", "author": "Federico Berto and Chuanbo Hua and Nayeli Gast Zepeda and Andr\u00e9 Hottung and Niels Wouda and Leon Lan and Junyoung Park and Kevin Tierney and Jinkyoo Park", "abstract": "  This paper introduces RouteFinder, a comprehensive foundation model framework\nto tackle different Vehicle Routing Problem (VRP) variants. Our core idea is\nthat a foundation model for VRPs should be able to represent variants by\ntreating each as a subset of a generalized problem equipped with different\nattributes. We propose a unified VRP environment capable of efficiently\nhandling any combination of these attributes. The RouteFinder model leverages a\nmodern transformer-based encoder and global attribute embeddings to improve\ntask representation. Additionally, we introduce two reinforcement learning\ntechniques to enhance multi-task performance: mixed batch training, which\nenables training on different variants at once, and multi-variant reward\nnormalization to balance different reward scales. Finally, we propose efficient\nadapter layers that enable fine-tuning for new variants with unseen attributes.\nExtensive experiments on 48 VRP variants show RouteFinder outperforms recent\nstate-of-the-art learning methods. Our code is publicly available at\nhttps://github.com/ai4co/routefinder.\n", "link": "http://arxiv.org/abs/2406.15007v4", "date": "2025-09-15", "relevancy": 2.0448, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RouteFinder%3A%20Towards%20Foundation%20Models%20for%20Vehicle%20Routing%20Problems&body=Title%3A%20RouteFinder%3A%20Towards%20Foundation%20Models%20for%20Vehicle%20Routing%20Problems%0AAuthor%3A%20Federico%20Berto%20and%20Chuanbo%20Hua%20and%20Nayeli%20Gast%20Zepeda%20and%20Andr%C3%A9%20Hottung%20and%20Niels%20Wouda%20and%20Leon%20Lan%20and%20Junyoung%20Park%20and%20Kevin%20Tierney%20and%20Jinkyoo%20Park%0AAbstract%3A%20%20%20This%20paper%20introduces%20RouteFinder%2C%20a%20comprehensive%20foundation%20model%20framework%0Ato%20tackle%20different%20Vehicle%20Routing%20Problem%20%28VRP%29%20variants.%20Our%20core%20idea%20is%0Athat%20a%20foundation%20model%20for%20VRPs%20should%20be%20able%20to%20represent%20variants%20by%0Atreating%20each%20as%20a%20subset%20of%20a%20generalized%20problem%20equipped%20with%20different%0Aattributes.%20We%20propose%20a%20unified%20VRP%20environment%20capable%20of%20efficiently%0Ahandling%20any%20combination%20of%20these%20attributes.%20The%20RouteFinder%20model%20leverages%20a%0Amodern%20transformer-based%20encoder%20and%20global%20attribute%20embeddings%20to%20improve%0Atask%20representation.%20Additionally%2C%20we%20introduce%20two%20reinforcement%20learning%0Atechniques%20to%20enhance%20multi-task%20performance%3A%20mixed%20batch%20training%2C%20which%0Aenables%20training%20on%20different%20variants%20at%20once%2C%20and%20multi-variant%20reward%0Anormalization%20to%20balance%20different%20reward%20scales.%20Finally%2C%20we%20propose%20efficient%0Aadapter%20layers%20that%20enable%20fine-tuning%20for%20new%20variants%20with%20unseen%20attributes.%0AExtensive%20experiments%20on%2048%20VRP%20variants%20show%20RouteFinder%20outperforms%20recent%0Astate-of-the-art%20learning%20methods.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/ai4co/routefinder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15007v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRouteFinder%253A%2520Towards%2520Foundation%2520Models%2520for%2520Vehicle%2520Routing%2520Problems%26entry.906535625%3DFederico%2520Berto%2520and%2520Chuanbo%2520Hua%2520and%2520Nayeli%2520Gast%2520Zepeda%2520and%2520Andr%25C3%25A9%2520Hottung%2520and%2520Niels%2520Wouda%2520and%2520Leon%2520Lan%2520and%2520Junyoung%2520Park%2520and%2520Kevin%2520Tierney%2520and%2520Jinkyoo%2520Park%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520RouteFinder%252C%2520a%2520comprehensive%2520foundation%2520model%2520framework%250Ato%2520tackle%2520different%2520Vehicle%2520Routing%2520Problem%2520%2528VRP%2529%2520variants.%2520Our%2520core%2520idea%2520is%250Athat%2520a%2520foundation%2520model%2520for%2520VRPs%2520should%2520be%2520able%2520to%2520represent%2520variants%2520by%250Atreating%2520each%2520as%2520a%2520subset%2520of%2520a%2520generalized%2520problem%2520equipped%2520with%2520different%250Aattributes.%2520We%2520propose%2520a%2520unified%2520VRP%2520environment%2520capable%2520of%2520efficiently%250Ahandling%2520any%2520combination%2520of%2520these%2520attributes.%2520The%2520RouteFinder%2520model%2520leverages%2520a%250Amodern%2520transformer-based%2520encoder%2520and%2520global%2520attribute%2520embeddings%2520to%2520improve%250Atask%2520representation.%2520Additionally%252C%2520we%2520introduce%2520two%2520reinforcement%2520learning%250Atechniques%2520to%2520enhance%2520multi-task%2520performance%253A%2520mixed%2520batch%2520training%252C%2520which%250Aenables%2520training%2520on%2520different%2520variants%2520at%2520once%252C%2520and%2520multi-variant%2520reward%250Anormalization%2520to%2520balance%2520different%2520reward%2520scales.%2520Finally%252C%2520we%2520propose%2520efficient%250Aadapter%2520layers%2520that%2520enable%2520fine-tuning%2520for%2520new%2520variants%2520with%2520unseen%2520attributes.%250AExtensive%2520experiments%2520on%252048%2520VRP%2520variants%2520show%2520RouteFinder%2520outperforms%2520recent%250Astate-of-the-art%2520learning%2520methods.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/ai4co/routefinder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15007v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RouteFinder%3A%20Towards%20Foundation%20Models%20for%20Vehicle%20Routing%20Problems&entry.906535625=Federico%20Berto%20and%20Chuanbo%20Hua%20and%20Nayeli%20Gast%20Zepeda%20and%20Andr%C3%A9%20Hottung%20and%20Niels%20Wouda%20and%20Leon%20Lan%20and%20Junyoung%20Park%20and%20Kevin%20Tierney%20and%20Jinkyoo%20Park&entry.1292438233=%20%20This%20paper%20introduces%20RouteFinder%2C%20a%20comprehensive%20foundation%20model%20framework%0Ato%20tackle%20different%20Vehicle%20Routing%20Problem%20%28VRP%29%20variants.%20Our%20core%20idea%20is%0Athat%20a%20foundation%20model%20for%20VRPs%20should%20be%20able%20to%20represent%20variants%20by%0Atreating%20each%20as%20a%20subset%20of%20a%20generalized%20problem%20equipped%20with%20different%0Aattributes.%20We%20propose%20a%20unified%20VRP%20environment%20capable%20of%20efficiently%0Ahandling%20any%20combination%20of%20these%20attributes.%20The%20RouteFinder%20model%20leverages%20a%0Amodern%20transformer-based%20encoder%20and%20global%20attribute%20embeddings%20to%20improve%0Atask%20representation.%20Additionally%2C%20we%20introduce%20two%20reinforcement%20learning%0Atechniques%20to%20enhance%20multi-task%20performance%3A%20mixed%20batch%20training%2C%20which%0Aenables%20training%20on%20different%20variants%20at%20once%2C%20and%20multi-variant%20reward%0Anormalization%20to%20balance%20different%20reward%20scales.%20Finally%2C%20we%20propose%20efficient%0Aadapter%20layers%20that%20enable%20fine-tuning%20for%20new%20variants%20with%20unseen%20attributes.%0AExtensive%20experiments%20on%2048%20VRP%20variants%20show%20RouteFinder%20outperforms%20recent%0Astate-of-the-art%20learning%20methods.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/ai4co/routefinder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15007v4&entry.124074799=Read"},
{"title": "Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic\n  Modeling in Social Recommendation", "author": "Yuqin Lan and Weihao Shen and Yuanze Hu and Qingchen Yu and Zhaoxin Fan and Faguo Wu and Laurence T. Yang", "abstract": "  In the era of rapid development of social media, social recommendation\nsystems as hybrid recommendation systems have been widely applied. Existing\nmethods capture interest similarity between users to filter out\ninterest-irrelevant relations in social networks that inevitably decrease\nrecommendation accuracy, however, limited research has a focus on the mutual\ninfluence of semantic information between the social network and the user-item\ninteraction network for further improving social recommendation. To address\nthese issues, we introduce a social \\underline{r}ecommendation model with\nro\\underline{bu}st g\\underline{r}aph denoisin\\underline{g}-augmentation fusion\nand multi-s\\underline{e}mantic Modeling(Burger). Specifically, we firstly\npropose to construct a social tensor in order to smooth the training process of\nthe model. Then, a graph convolutional network and a tensor convolutional\nnetwork are employed to capture user's item preference and social preference,\nrespectively. Considering the different semantic information in the user-item\ninteraction network and the social network, a bi-semantic coordination loss is\nproposed to model the mutual influence of semantic information. To alleviate\nthe interference of interest-irrelevant relations on multi-semantic modeling,\nwe further use Bayesian posterior probability to mine potential social\nrelations to replace social noise. Finally, the sliding window mechanism is\nutilized to update the social tensor as the input for the next iteration.\nExtensive experiments on three real datasets show Burger has a superior\nperformance compared with the state-of-the-art models.\n", "link": "http://arxiv.org/abs/2505.06612v3", "date": "2025-09-15", "relevancy": 1.5129, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5371}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5149}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Burger%3A%20Robust%20Graph%20Denoising-augmentation%20Fusion%20and%20Multi-semantic%0A%20%20Modeling%20in%20Social%20Recommendation&body=Title%3A%20Burger%3A%20Robust%20Graph%20Denoising-augmentation%20Fusion%20and%20Multi-semantic%0A%20%20Modeling%20in%20Social%20Recommendation%0AAuthor%3A%20Yuqin%20Lan%20and%20Weihao%20Shen%20and%20Yuanze%20Hu%20and%20Qingchen%20Yu%20and%20Zhaoxin%20Fan%20and%20Faguo%20Wu%20and%20Laurence%20T.%20Yang%0AAbstract%3A%20%20%20In%20the%20era%20of%20rapid%20development%20of%20social%20media%2C%20social%20recommendation%0Asystems%20as%20hybrid%20recommendation%20systems%20have%20been%20widely%20applied.%20Existing%0Amethods%20capture%20interest%20similarity%20between%20users%20to%20filter%20out%0Ainterest-irrelevant%20relations%20in%20social%20networks%20that%20inevitably%20decrease%0Arecommendation%20accuracy%2C%20however%2C%20limited%20research%20has%20a%20focus%20on%20the%20mutual%0Ainfluence%20of%20semantic%20information%20between%20the%20social%20network%20and%20the%20user-item%0Ainteraction%20network%20for%20further%20improving%20social%20recommendation.%20To%20address%0Athese%20issues%2C%20we%20introduce%20a%20social%20%5Cunderline%7Br%7Decommendation%20model%20with%0Aro%5Cunderline%7Bbu%7Dst%20g%5Cunderline%7Br%7Daph%20denoisin%5Cunderline%7Bg%7D-augmentation%20fusion%0Aand%20multi-s%5Cunderline%7Be%7Dmantic%20Modeling%28Burger%29.%20Specifically%2C%20we%20firstly%0Apropose%20to%20construct%20a%20social%20tensor%20in%20order%20to%20smooth%20the%20training%20process%20of%0Athe%20model.%20Then%2C%20a%20graph%20convolutional%20network%20and%20a%20tensor%20convolutional%0Anetwork%20are%20employed%20to%20capture%20user%27s%20item%20preference%20and%20social%20preference%2C%0Arespectively.%20Considering%20the%20different%20semantic%20information%20in%20the%20user-item%0Ainteraction%20network%20and%20the%20social%20network%2C%20a%20bi-semantic%20coordination%20loss%20is%0Aproposed%20to%20model%20the%20mutual%20influence%20of%20semantic%20information.%20To%20alleviate%0Athe%20interference%20of%20interest-irrelevant%20relations%20on%20multi-semantic%20modeling%2C%0Awe%20further%20use%20Bayesian%20posterior%20probability%20to%20mine%20potential%20social%0Arelations%20to%20replace%20social%20noise.%20Finally%2C%20the%20sliding%20window%20mechanism%20is%0Autilized%20to%20update%20the%20social%20tensor%20as%20the%20input%20for%20the%20next%20iteration.%0AExtensive%20experiments%20on%20three%20real%20datasets%20show%20Burger%20has%20a%20superior%0Aperformance%20compared%20with%20the%20state-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06612v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBurger%253A%2520Robust%2520Graph%2520Denoising-augmentation%2520Fusion%2520and%2520Multi-semantic%250A%2520%2520Modeling%2520in%2520Social%2520Recommendation%26entry.906535625%3DYuqin%2520Lan%2520and%2520Weihao%2520Shen%2520and%2520Yuanze%2520Hu%2520and%2520Qingchen%2520Yu%2520and%2520Zhaoxin%2520Fan%2520and%2520Faguo%2520Wu%2520and%2520Laurence%2520T.%2520Yang%26entry.1292438233%3D%2520%2520In%2520the%2520era%2520of%2520rapid%2520development%2520of%2520social%2520media%252C%2520social%2520recommendation%250Asystems%2520as%2520hybrid%2520recommendation%2520systems%2520have%2520been%2520widely%2520applied.%2520Existing%250Amethods%2520capture%2520interest%2520similarity%2520between%2520users%2520to%2520filter%2520out%250Ainterest-irrelevant%2520relations%2520in%2520social%2520networks%2520that%2520inevitably%2520decrease%250Arecommendation%2520accuracy%252C%2520however%252C%2520limited%2520research%2520has%2520a%2520focus%2520on%2520the%2520mutual%250Ainfluence%2520of%2520semantic%2520information%2520between%2520the%2520social%2520network%2520and%2520the%2520user-item%250Ainteraction%2520network%2520for%2520further%2520improving%2520social%2520recommendation.%2520To%2520address%250Athese%2520issues%252C%2520we%2520introduce%2520a%2520social%2520%255Cunderline%257Br%257Decommendation%2520model%2520with%250Aro%255Cunderline%257Bbu%257Dst%2520g%255Cunderline%257Br%257Daph%2520denoisin%255Cunderline%257Bg%257D-augmentation%2520fusion%250Aand%2520multi-s%255Cunderline%257Be%257Dmantic%2520Modeling%2528Burger%2529.%2520Specifically%252C%2520we%2520firstly%250Apropose%2520to%2520construct%2520a%2520social%2520tensor%2520in%2520order%2520to%2520smooth%2520the%2520training%2520process%2520of%250Athe%2520model.%2520Then%252C%2520a%2520graph%2520convolutional%2520network%2520and%2520a%2520tensor%2520convolutional%250Anetwork%2520are%2520employed%2520to%2520capture%2520user%2527s%2520item%2520preference%2520and%2520social%2520preference%252C%250Arespectively.%2520Considering%2520the%2520different%2520semantic%2520information%2520in%2520the%2520user-item%250Ainteraction%2520network%2520and%2520the%2520social%2520network%252C%2520a%2520bi-semantic%2520coordination%2520loss%2520is%250Aproposed%2520to%2520model%2520the%2520mutual%2520influence%2520of%2520semantic%2520information.%2520To%2520alleviate%250Athe%2520interference%2520of%2520interest-irrelevant%2520relations%2520on%2520multi-semantic%2520modeling%252C%250Awe%2520further%2520use%2520Bayesian%2520posterior%2520probability%2520to%2520mine%2520potential%2520social%250Arelations%2520to%2520replace%2520social%2520noise.%2520Finally%252C%2520the%2520sliding%2520window%2520mechanism%2520is%250Autilized%2520to%2520update%2520the%2520social%2520tensor%2520as%2520the%2520input%2520for%2520the%2520next%2520iteration.%250AExtensive%2520experiments%2520on%2520three%2520real%2520datasets%2520show%2520Burger%2520has%2520a%2520superior%250Aperformance%2520compared%2520with%2520the%2520state-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06612v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Burger%3A%20Robust%20Graph%20Denoising-augmentation%20Fusion%20and%20Multi-semantic%0A%20%20Modeling%20in%20Social%20Recommendation&entry.906535625=Yuqin%20Lan%20and%20Weihao%20Shen%20and%20Yuanze%20Hu%20and%20Qingchen%20Yu%20and%20Zhaoxin%20Fan%20and%20Faguo%20Wu%20and%20Laurence%20T.%20Yang&entry.1292438233=%20%20In%20the%20era%20of%20rapid%20development%20of%20social%20media%2C%20social%20recommendation%0Asystems%20as%20hybrid%20recommendation%20systems%20have%20been%20widely%20applied.%20Existing%0Amethods%20capture%20interest%20similarity%20between%20users%20to%20filter%20out%0Ainterest-irrelevant%20relations%20in%20social%20networks%20that%20inevitably%20decrease%0Arecommendation%20accuracy%2C%20however%2C%20limited%20research%20has%20a%20focus%20on%20the%20mutual%0Ainfluence%20of%20semantic%20information%20between%20the%20social%20network%20and%20the%20user-item%0Ainteraction%20network%20for%20further%20improving%20social%20recommendation.%20To%20address%0Athese%20issues%2C%20we%20introduce%20a%20social%20%5Cunderline%7Br%7Decommendation%20model%20with%0Aro%5Cunderline%7Bbu%7Dst%20g%5Cunderline%7Br%7Daph%20denoisin%5Cunderline%7Bg%7D-augmentation%20fusion%0Aand%20multi-s%5Cunderline%7Be%7Dmantic%20Modeling%28Burger%29.%20Specifically%2C%20we%20firstly%0Apropose%20to%20construct%20a%20social%20tensor%20in%20order%20to%20smooth%20the%20training%20process%20of%0Athe%20model.%20Then%2C%20a%20graph%20convolutional%20network%20and%20a%20tensor%20convolutional%0Anetwork%20are%20employed%20to%20capture%20user%27s%20item%20preference%20and%20social%20preference%2C%0Arespectively.%20Considering%20the%20different%20semantic%20information%20in%20the%20user-item%0Ainteraction%20network%20and%20the%20social%20network%2C%20a%20bi-semantic%20coordination%20loss%20is%0Aproposed%20to%20model%20the%20mutual%20influence%20of%20semantic%20information.%20To%20alleviate%0Athe%20interference%20of%20interest-irrelevant%20relations%20on%20multi-semantic%20modeling%2C%0Awe%20further%20use%20Bayesian%20posterior%20probability%20to%20mine%20potential%20social%0Arelations%20to%20replace%20social%20noise.%20Finally%2C%20the%20sliding%20window%20mechanism%20is%0Autilized%20to%20update%20the%20social%20tensor%20as%20the%20input%20for%20the%20next%20iteration.%0AExtensive%20experiments%20on%20three%20real%20datasets%20show%20Burger%20has%20a%20superior%0Aperformance%20compared%20with%20the%20state-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06612v3&entry.124074799=Read"},
{"title": "Pun Unintended: LLMs and the Illusion of Humor Understanding", "author": "Alessandro Zangari and Matteo Marcuzzo and Andrea Albarelli and Mohammad Taher Pilehvar and Jose Camacho-Collados", "abstract": "  Puns are a form of humorous wordplay that exploits polysemy and phonetic\nsimilarity. While LLMs have shown promise in detecting puns, we show in this\npaper that their understanding often remains shallow, lacking the nuanced grasp\ntypical of human interpretation. By systematically analyzing and reformulating\nexisting pun benchmarks, we demonstrate how subtle changes in puns are\nsufficient to mislead LLMs. Our contributions include comprehensive and nuanced\npun detection benchmarks, human evaluation across recent LLMs, and an analysis\nof the robustness challenges these models face in processing puns.\n", "link": "http://arxiv.org/abs/2509.12158v1", "date": "2025-09-15", "relevancy": 1.6546, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4306}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4103}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pun%20Unintended%3A%20LLMs%20and%20the%20Illusion%20of%20Humor%20Understanding&body=Title%3A%20Pun%20Unintended%3A%20LLMs%20and%20the%20Illusion%20of%20Humor%20Understanding%0AAuthor%3A%20Alessandro%20Zangari%20and%20Matteo%20Marcuzzo%20and%20Andrea%20Albarelli%20and%20Mohammad%20Taher%20Pilehvar%20and%20Jose%20Camacho-Collados%0AAbstract%3A%20%20%20Puns%20are%20a%20form%20of%20humorous%20wordplay%20that%20exploits%20polysemy%20and%20phonetic%0Asimilarity.%20While%20LLMs%20have%20shown%20promise%20in%20detecting%20puns%2C%20we%20show%20in%20this%0Apaper%20that%20their%20understanding%20often%20remains%20shallow%2C%20lacking%20the%20nuanced%20grasp%0Atypical%20of%20human%20interpretation.%20By%20systematically%20analyzing%20and%20reformulating%0Aexisting%20pun%20benchmarks%2C%20we%20demonstrate%20how%20subtle%20changes%20in%20puns%20are%0Asufficient%20to%20mislead%20LLMs.%20Our%20contributions%20include%20comprehensive%20and%20nuanced%0Apun%20detection%20benchmarks%2C%20human%20evaluation%20across%20recent%20LLMs%2C%20and%20an%20analysis%0Aof%20the%20robustness%20challenges%20these%20models%20face%20in%20processing%20puns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPun%2520Unintended%253A%2520LLMs%2520and%2520the%2520Illusion%2520of%2520Humor%2520Understanding%26entry.906535625%3DAlessandro%2520Zangari%2520and%2520Matteo%2520Marcuzzo%2520and%2520Andrea%2520Albarelli%2520and%2520Mohammad%2520Taher%2520Pilehvar%2520and%2520Jose%2520Camacho-Collados%26entry.1292438233%3D%2520%2520Puns%2520are%2520a%2520form%2520of%2520humorous%2520wordplay%2520that%2520exploits%2520polysemy%2520and%2520phonetic%250Asimilarity.%2520While%2520LLMs%2520have%2520shown%2520promise%2520in%2520detecting%2520puns%252C%2520we%2520show%2520in%2520this%250Apaper%2520that%2520their%2520understanding%2520often%2520remains%2520shallow%252C%2520lacking%2520the%2520nuanced%2520grasp%250Atypical%2520of%2520human%2520interpretation.%2520By%2520systematically%2520analyzing%2520and%2520reformulating%250Aexisting%2520pun%2520benchmarks%252C%2520we%2520demonstrate%2520how%2520subtle%2520changes%2520in%2520puns%2520are%250Asufficient%2520to%2520mislead%2520LLMs.%2520Our%2520contributions%2520include%2520comprehensive%2520and%2520nuanced%250Apun%2520detection%2520benchmarks%252C%2520human%2520evaluation%2520across%2520recent%2520LLMs%252C%2520and%2520an%2520analysis%250Aof%2520the%2520robustness%2520challenges%2520these%2520models%2520face%2520in%2520processing%2520puns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pun%20Unintended%3A%20LLMs%20and%20the%20Illusion%20of%20Humor%20Understanding&entry.906535625=Alessandro%20Zangari%20and%20Matteo%20Marcuzzo%20and%20Andrea%20Albarelli%20and%20Mohammad%20Taher%20Pilehvar%20and%20Jose%20Camacho-Collados&entry.1292438233=%20%20Puns%20are%20a%20form%20of%20humorous%20wordplay%20that%20exploits%20polysemy%20and%20phonetic%0Asimilarity.%20While%20LLMs%20have%20shown%20promise%20in%20detecting%20puns%2C%20we%20show%20in%20this%0Apaper%20that%20their%20understanding%20often%20remains%20shallow%2C%20lacking%20the%20nuanced%20grasp%0Atypical%20of%20human%20interpretation.%20By%20systematically%20analyzing%20and%20reformulating%0Aexisting%20pun%20benchmarks%2C%20we%20demonstrate%20how%20subtle%20changes%20in%20puns%20are%0Asufficient%20to%20mislead%20LLMs.%20Our%20contributions%20include%20comprehensive%20and%20nuanced%0Apun%20detection%20benchmarks%2C%20human%20evaluation%20across%20recent%20LLMs%2C%20and%20an%20analysis%0Aof%20the%20robustness%20challenges%20these%20models%20face%20in%20processing%20puns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12158v1&entry.124074799=Read"},
{"title": "MODIS: Multi-Omics Data Integration for Small and unpaired datasets", "author": "Daniel Lepe-Soltero and Thierry Arti\u00e8res and Ana\u00efs Baudot and Paul Villoutreix", "abstract": "  An important objective in computational biology is the efficient integration\nof multi-omics data. The task of integration comes with challenges: multi-omics\ndata are most often unpaired (requiring diagonal integration), partially\nlabeled with information about biological conditions, and in some situations\nsuch as rare diseases, only very small datasets are available. We present\nMODIS, a semi supervised framework designed to account for these particular\nchallenges. To address the challenge of very small datasets, we propose to\nexploit information contained in larger multi-omics databases by training our\nmodel on a large reference database and a small target dataset simultaneously,\neffectively turning the problem of transfer learning into a problem of learning\nwith class imbalance. MODIS performs diagonal integration on unpaired samples,\nleveraging class-labels to align modalities despite class imbalance and data\nscarcity. The architecture combines multiple variational auto-encoders, a class\nclassifier and an adversarially trained modality classifier. To ensure training\nstability, we adapted a regularized relativistic GAN loss to this setting. We\nfirst validate MODIS on a synthetic dataset to assess the level of supervision\nneeded for accurate alignment and to quantify the impact of class imbalance on\npredictive performance. We then apply our approach to the large public TCGA\ndatabase, considering between 10 and 34 classes (cancer types and normal\ntissue). MODIS demonstrates high prediction accuracy, robust performance with\nlimited supervision, and stability to class imbalance. These results position\nMODIS as a promising solution for challenging integration scenarios,\nparticularly diagonal integration with a small number of samples, typical of\nrare diseases studies. The code is available at\nhttps://github.com/VILLOUTREIXLab/MODIS.\n", "link": "http://arxiv.org/abs/2503.18856v2", "date": "2025-09-15", "relevancy": 1.6282, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5655}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5242}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MODIS%3A%20Multi-Omics%20Data%20Integration%20for%20Small%20and%20unpaired%20datasets&body=Title%3A%20MODIS%3A%20Multi-Omics%20Data%20Integration%20for%20Small%20and%20unpaired%20datasets%0AAuthor%3A%20Daniel%20Lepe-Soltero%20and%20Thierry%20Arti%C3%A8res%20and%20Ana%C3%AFs%20Baudot%20and%20Paul%20Villoutreix%0AAbstract%3A%20%20%20An%20important%20objective%20in%20computational%20biology%20is%20the%20efficient%20integration%0Aof%20multi-omics%20data.%20The%20task%20of%20integration%20comes%20with%20challenges%3A%20multi-omics%0Adata%20are%20most%20often%20unpaired%20%28requiring%20diagonal%20integration%29%2C%20partially%0Alabeled%20with%20information%20about%20biological%20conditions%2C%20and%20in%20some%20situations%0Asuch%20as%20rare%20diseases%2C%20only%20very%20small%20datasets%20are%20available.%20We%20present%0AMODIS%2C%20a%20semi%20supervised%20framework%20designed%20to%20account%20for%20these%20particular%0Achallenges.%20To%20address%20the%20challenge%20of%20very%20small%20datasets%2C%20we%20propose%20to%0Aexploit%20information%20contained%20in%20larger%20multi-omics%20databases%20by%20training%20our%0Amodel%20on%20a%20large%20reference%20database%20and%20a%20small%20target%20dataset%20simultaneously%2C%0Aeffectively%20turning%20the%20problem%20of%20transfer%20learning%20into%20a%20problem%20of%20learning%0Awith%20class%20imbalance.%20MODIS%20performs%20diagonal%20integration%20on%20unpaired%20samples%2C%0Aleveraging%20class-labels%20to%20align%20modalities%20despite%20class%20imbalance%20and%20data%0Ascarcity.%20The%20architecture%20combines%20multiple%20variational%20auto-encoders%2C%20a%20class%0Aclassifier%20and%20an%20adversarially%20trained%20modality%20classifier.%20To%20ensure%20training%0Astability%2C%20we%20adapted%20a%20regularized%20relativistic%20GAN%20loss%20to%20this%20setting.%20We%0Afirst%20validate%20MODIS%20on%20a%20synthetic%20dataset%20to%20assess%20the%20level%20of%20supervision%0Aneeded%20for%20accurate%20alignment%20and%20to%20quantify%20the%20impact%20of%20class%20imbalance%20on%0Apredictive%20performance.%20We%20then%20apply%20our%20approach%20to%20the%20large%20public%20TCGA%0Adatabase%2C%20considering%20between%2010%20and%2034%20classes%20%28cancer%20types%20and%20normal%0Atissue%29.%20MODIS%20demonstrates%20high%20prediction%20accuracy%2C%20robust%20performance%20with%0Alimited%20supervision%2C%20and%20stability%20to%20class%20imbalance.%20These%20results%20position%0AMODIS%20as%20a%20promising%20solution%20for%20challenging%20integration%20scenarios%2C%0Aparticularly%20diagonal%20integration%20with%20a%20small%20number%20of%20samples%2C%20typical%20of%0Arare%20diseases%20studies.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/VILLOUTREIXLab/MODIS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18856v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMODIS%253A%2520Multi-Omics%2520Data%2520Integration%2520for%2520Small%2520and%2520unpaired%2520datasets%26entry.906535625%3DDaniel%2520Lepe-Soltero%2520and%2520Thierry%2520Arti%25C3%25A8res%2520and%2520Ana%25C3%25AFs%2520Baudot%2520and%2520Paul%2520Villoutreix%26entry.1292438233%3D%2520%2520An%2520important%2520objective%2520in%2520computational%2520biology%2520is%2520the%2520efficient%2520integration%250Aof%2520multi-omics%2520data.%2520The%2520task%2520of%2520integration%2520comes%2520with%2520challenges%253A%2520multi-omics%250Adata%2520are%2520most%2520often%2520unpaired%2520%2528requiring%2520diagonal%2520integration%2529%252C%2520partially%250Alabeled%2520with%2520information%2520about%2520biological%2520conditions%252C%2520and%2520in%2520some%2520situations%250Asuch%2520as%2520rare%2520diseases%252C%2520only%2520very%2520small%2520datasets%2520are%2520available.%2520We%2520present%250AMODIS%252C%2520a%2520semi%2520supervised%2520framework%2520designed%2520to%2520account%2520for%2520these%2520particular%250Achallenges.%2520To%2520address%2520the%2520challenge%2520of%2520very%2520small%2520datasets%252C%2520we%2520propose%2520to%250Aexploit%2520information%2520contained%2520in%2520larger%2520multi-omics%2520databases%2520by%2520training%2520our%250Amodel%2520on%2520a%2520large%2520reference%2520database%2520and%2520a%2520small%2520target%2520dataset%2520simultaneously%252C%250Aeffectively%2520turning%2520the%2520problem%2520of%2520transfer%2520learning%2520into%2520a%2520problem%2520of%2520learning%250Awith%2520class%2520imbalance.%2520MODIS%2520performs%2520diagonal%2520integration%2520on%2520unpaired%2520samples%252C%250Aleveraging%2520class-labels%2520to%2520align%2520modalities%2520despite%2520class%2520imbalance%2520and%2520data%250Ascarcity.%2520The%2520architecture%2520combines%2520multiple%2520variational%2520auto-encoders%252C%2520a%2520class%250Aclassifier%2520and%2520an%2520adversarially%2520trained%2520modality%2520classifier.%2520To%2520ensure%2520training%250Astability%252C%2520we%2520adapted%2520a%2520regularized%2520relativistic%2520GAN%2520loss%2520to%2520this%2520setting.%2520We%250Afirst%2520validate%2520MODIS%2520on%2520a%2520synthetic%2520dataset%2520to%2520assess%2520the%2520level%2520of%2520supervision%250Aneeded%2520for%2520accurate%2520alignment%2520and%2520to%2520quantify%2520the%2520impact%2520of%2520class%2520imbalance%2520on%250Apredictive%2520performance.%2520We%2520then%2520apply%2520our%2520approach%2520to%2520the%2520large%2520public%2520TCGA%250Adatabase%252C%2520considering%2520between%252010%2520and%252034%2520classes%2520%2528cancer%2520types%2520and%2520normal%250Atissue%2529.%2520MODIS%2520demonstrates%2520high%2520prediction%2520accuracy%252C%2520robust%2520performance%2520with%250Alimited%2520supervision%252C%2520and%2520stability%2520to%2520class%2520imbalance.%2520These%2520results%2520position%250AMODIS%2520as%2520a%2520promising%2520solution%2520for%2520challenging%2520integration%2520scenarios%252C%250Aparticularly%2520diagonal%2520integration%2520with%2520a%2520small%2520number%2520of%2520samples%252C%2520typical%2520of%250Arare%2520diseases%2520studies.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/VILLOUTREIXLab/MODIS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18856v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MODIS%3A%20Multi-Omics%20Data%20Integration%20for%20Small%20and%20unpaired%20datasets&entry.906535625=Daniel%20Lepe-Soltero%20and%20Thierry%20Arti%C3%A8res%20and%20Ana%C3%AFs%20Baudot%20and%20Paul%20Villoutreix&entry.1292438233=%20%20An%20important%20objective%20in%20computational%20biology%20is%20the%20efficient%20integration%0Aof%20multi-omics%20data.%20The%20task%20of%20integration%20comes%20with%20challenges%3A%20multi-omics%0Adata%20are%20most%20often%20unpaired%20%28requiring%20diagonal%20integration%29%2C%20partially%0Alabeled%20with%20information%20about%20biological%20conditions%2C%20and%20in%20some%20situations%0Asuch%20as%20rare%20diseases%2C%20only%20very%20small%20datasets%20are%20available.%20We%20present%0AMODIS%2C%20a%20semi%20supervised%20framework%20designed%20to%20account%20for%20these%20particular%0Achallenges.%20To%20address%20the%20challenge%20of%20very%20small%20datasets%2C%20we%20propose%20to%0Aexploit%20information%20contained%20in%20larger%20multi-omics%20databases%20by%20training%20our%0Amodel%20on%20a%20large%20reference%20database%20and%20a%20small%20target%20dataset%20simultaneously%2C%0Aeffectively%20turning%20the%20problem%20of%20transfer%20learning%20into%20a%20problem%20of%20learning%0Awith%20class%20imbalance.%20MODIS%20performs%20diagonal%20integration%20on%20unpaired%20samples%2C%0Aleveraging%20class-labels%20to%20align%20modalities%20despite%20class%20imbalance%20and%20data%0Ascarcity.%20The%20architecture%20combines%20multiple%20variational%20auto-encoders%2C%20a%20class%0Aclassifier%20and%20an%20adversarially%20trained%20modality%20classifier.%20To%20ensure%20training%0Astability%2C%20we%20adapted%20a%20regularized%20relativistic%20GAN%20loss%20to%20this%20setting.%20We%0Afirst%20validate%20MODIS%20on%20a%20synthetic%20dataset%20to%20assess%20the%20level%20of%20supervision%0Aneeded%20for%20accurate%20alignment%20and%20to%20quantify%20the%20impact%20of%20class%20imbalance%20on%0Apredictive%20performance.%20We%20then%20apply%20our%20approach%20to%20the%20large%20public%20TCGA%0Adatabase%2C%20considering%20between%2010%20and%2034%20classes%20%28cancer%20types%20and%20normal%0Atissue%29.%20MODIS%20demonstrates%20high%20prediction%20accuracy%2C%20robust%20performance%20with%0Alimited%20supervision%2C%20and%20stability%20to%20class%20imbalance.%20These%20results%20position%0AMODIS%20as%20a%20promising%20solution%20for%20challenging%20integration%20scenarios%2C%0Aparticularly%20diagonal%20integration%20with%20a%20small%20number%20of%20samples%2C%20typical%20of%0Arare%20diseases%20studies.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/VILLOUTREIXLab/MODIS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18856v2&entry.124074799=Read"},
{"title": "ASP-FZN: A Translation-based Constraint Answer Set Solver", "author": "Thomas Eiter and Tobias Geibinger and Tobias Kaminski and Nysret Musliu and Johannes Oetsch", "abstract": "  We present the solver asp-fzn for Constraint Answer Set Programming (CASP),\nwhich extends ASP with linear constraints. Our approach is based on translating\nCASP programs into the solver-independent FlatZinc language that supports\nseveral Constraint Programming and Integer Programming backend solvers. Our\nsolver supports a rich language of linear constraints, including some common\nglobal constraints. As for evaluation, we show that asp-fzn is competitive with\nstate-of-the-art ASP solvers on benchmarks taken from past ASP competitions.\nFurthermore, we evaluate it on several CASP problems from the literature and\ncompare its performance with clingcon, which is a prominent CASP solver that\nsupports most of the asp-fzn language. The performance of asp-fzn is very\npromising as it is already competitive on plain ASP and even outperforms\nclingcon on some CASP benchmarks.\n", "link": "http://arxiv.org/abs/2507.22774v3", "date": "2025-09-15", "relevancy": 1.4272, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASP-FZN%3A%20A%20Translation-based%20Constraint%20Answer%20Set%20Solver&body=Title%3A%20ASP-FZN%3A%20A%20Translation-based%20Constraint%20Answer%20Set%20Solver%0AAuthor%3A%20Thomas%20Eiter%20and%20Tobias%20Geibinger%20and%20Tobias%20Kaminski%20and%20Nysret%20Musliu%20and%20Johannes%20Oetsch%0AAbstract%3A%20%20%20We%20present%20the%20solver%20asp-fzn%20for%20Constraint%20Answer%20Set%20Programming%20%28CASP%29%2C%0Awhich%20extends%20ASP%20with%20linear%20constraints.%20Our%20approach%20is%20based%20on%20translating%0ACASP%20programs%20into%20the%20solver-independent%20FlatZinc%20language%20that%20supports%0Aseveral%20Constraint%20Programming%20and%20Integer%20Programming%20backend%20solvers.%20Our%0Asolver%20supports%20a%20rich%20language%20of%20linear%20constraints%2C%20including%20some%20common%0Aglobal%20constraints.%20As%20for%20evaluation%2C%20we%20show%20that%20asp-fzn%20is%20competitive%20with%0Astate-of-the-art%20ASP%20solvers%20on%20benchmarks%20taken%20from%20past%20ASP%20competitions.%0AFurthermore%2C%20we%20evaluate%20it%20on%20several%20CASP%20problems%20from%20the%20literature%20and%0Acompare%20its%20performance%20with%20clingcon%2C%20which%20is%20a%20prominent%20CASP%20solver%20that%0Asupports%20most%20of%20the%20asp-fzn%20language.%20The%20performance%20of%20asp-fzn%20is%20very%0Apromising%20as%20it%20is%20already%20competitive%20on%20plain%20ASP%20and%20even%20outperforms%0Aclingcon%20on%20some%20CASP%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22774v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASP-FZN%253A%2520A%2520Translation-based%2520Constraint%2520Answer%2520Set%2520Solver%26entry.906535625%3DThomas%2520Eiter%2520and%2520Tobias%2520Geibinger%2520and%2520Tobias%2520Kaminski%2520and%2520Nysret%2520Musliu%2520and%2520Johannes%2520Oetsch%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520solver%2520asp-fzn%2520for%2520Constraint%2520Answer%2520Set%2520Programming%2520%2528CASP%2529%252C%250Awhich%2520extends%2520ASP%2520with%2520linear%2520constraints.%2520Our%2520approach%2520is%2520based%2520on%2520translating%250ACASP%2520programs%2520into%2520the%2520solver-independent%2520FlatZinc%2520language%2520that%2520supports%250Aseveral%2520Constraint%2520Programming%2520and%2520Integer%2520Programming%2520backend%2520solvers.%2520Our%250Asolver%2520supports%2520a%2520rich%2520language%2520of%2520linear%2520constraints%252C%2520including%2520some%2520common%250Aglobal%2520constraints.%2520As%2520for%2520evaluation%252C%2520we%2520show%2520that%2520asp-fzn%2520is%2520competitive%2520with%250Astate-of-the-art%2520ASP%2520solvers%2520on%2520benchmarks%2520taken%2520from%2520past%2520ASP%2520competitions.%250AFurthermore%252C%2520we%2520evaluate%2520it%2520on%2520several%2520CASP%2520problems%2520from%2520the%2520literature%2520and%250Acompare%2520its%2520performance%2520with%2520clingcon%252C%2520which%2520is%2520a%2520prominent%2520CASP%2520solver%2520that%250Asupports%2520most%2520of%2520the%2520asp-fzn%2520language.%2520The%2520performance%2520of%2520asp-fzn%2520is%2520very%250Apromising%2520as%2520it%2520is%2520already%2520competitive%2520on%2520plain%2520ASP%2520and%2520even%2520outperforms%250Aclingcon%2520on%2520some%2520CASP%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22774v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASP-FZN%3A%20A%20Translation-based%20Constraint%20Answer%20Set%20Solver&entry.906535625=Thomas%20Eiter%20and%20Tobias%20Geibinger%20and%20Tobias%20Kaminski%20and%20Nysret%20Musliu%20and%20Johannes%20Oetsch&entry.1292438233=%20%20We%20present%20the%20solver%20asp-fzn%20for%20Constraint%20Answer%20Set%20Programming%20%28CASP%29%2C%0Awhich%20extends%20ASP%20with%20linear%20constraints.%20Our%20approach%20is%20based%20on%20translating%0ACASP%20programs%20into%20the%20solver-independent%20FlatZinc%20language%20that%20supports%0Aseveral%20Constraint%20Programming%20and%20Integer%20Programming%20backend%20solvers.%20Our%0Asolver%20supports%20a%20rich%20language%20of%20linear%20constraints%2C%20including%20some%20common%0Aglobal%20constraints.%20As%20for%20evaluation%2C%20we%20show%20that%20asp-fzn%20is%20competitive%20with%0Astate-of-the-art%20ASP%20solvers%20on%20benchmarks%20taken%20from%20past%20ASP%20competitions.%0AFurthermore%2C%20we%20evaluate%20it%20on%20several%20CASP%20problems%20from%20the%20literature%20and%0Acompare%20its%20performance%20with%20clingcon%2C%20which%20is%20a%20prominent%20CASP%20solver%20that%0Asupports%20most%20of%20the%20asp-fzn%20language.%20The%20performance%20of%20asp-fzn%20is%20very%0Apromising%20as%20it%20is%20already%20competitive%20on%20plain%20ASP%20and%20even%20outperforms%0Aclingcon%20on%20some%20CASP%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22774v3&entry.124074799=Read"},
{"title": "Layout-Conditioned Autoregressive Text-to-Image Generation via\n  Structured Masking", "author": "Zirui Zheng and Takashi Isobe and Tong Shen and Xu Jia and Jianbin Zhao and Xiaomin Li and Mengmeng Ge and Baolu Li and Qinghe Wang and Dong Li and Dong Zhou and Yunzhi Zhuge and Huchuan Lu and Emad Barsoum", "abstract": "  While autoregressive (AR) models have demonstrated remarkable success in\nimage generation, extending them to layout-conditioned generation remains\nchallenging due to the sparse nature of layout conditions and the risk of\nfeature entanglement. We present Structured Masking for AR-based\nLayout-to-Image (SMARLI), a novel framework for layoutto-image generation that\neffectively integrates spatial layout constraints into AR-based image\ngeneration. To equip AR model with layout control, a specially designed\nstructured masking strategy is applied to attention computation to govern the\ninteraction among the global prompt, layout, and image tokens. This design\nprevents mis-association between different regions and their descriptions while\nenabling sufficient injection of layout constraints into the generation\nprocess. To further enhance generation quality and layout accuracy, we\nincorporate Group Relative Policy Optimization (GRPO) based post-training\nscheme with specially designed layout reward functions for next-set-based AR\nmodels. Experimental results demonstrate that SMARLI is able to seamlessly\nintegrate layout tokens with text and image tokens without compromising\ngeneration quality. It achieves superior layoutaware control while maintaining\nthe structural simplicity and generation efficiency of AR models.\n", "link": "http://arxiv.org/abs/2509.12046v1", "date": "2025-09-15", "relevancy": 1.7811, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6244}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5577}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Layout-Conditioned%20Autoregressive%20Text-to-Image%20Generation%20via%0A%20%20Structured%20Masking&body=Title%3A%20Layout-Conditioned%20Autoregressive%20Text-to-Image%20Generation%20via%0A%20%20Structured%20Masking%0AAuthor%3A%20Zirui%20Zheng%20and%20Takashi%20Isobe%20and%20Tong%20Shen%20and%20Xu%20Jia%20and%20Jianbin%20Zhao%20and%20Xiaomin%20Li%20and%20Mengmeng%20Ge%20and%20Baolu%20Li%20and%20Qinghe%20Wang%20and%20Dong%20Li%20and%20Dong%20Zhou%20and%20Yunzhi%20Zhuge%20and%20Huchuan%20Lu%20and%20Emad%20Barsoum%0AAbstract%3A%20%20%20While%20autoregressive%20%28AR%29%20models%20have%20demonstrated%20remarkable%20success%20in%0Aimage%20generation%2C%20extending%20them%20to%20layout-conditioned%20generation%20remains%0Achallenging%20due%20to%20the%20sparse%20nature%20of%20layout%20conditions%20and%20the%20risk%20of%0Afeature%20entanglement.%20We%20present%20Structured%20Masking%20for%20AR-based%0ALayout-to-Image%20%28SMARLI%29%2C%20a%20novel%20framework%20for%20layoutto-image%20generation%20that%0Aeffectively%20integrates%20spatial%20layout%20constraints%20into%20AR-based%20image%0Ageneration.%20To%20equip%20AR%20model%20with%20layout%20control%2C%20a%20specially%20designed%0Astructured%20masking%20strategy%20is%20applied%20to%20attention%20computation%20to%20govern%20the%0Ainteraction%20among%20the%20global%20prompt%2C%20layout%2C%20and%20image%20tokens.%20This%20design%0Aprevents%20mis-association%20between%20different%20regions%20and%20their%20descriptions%20while%0Aenabling%20sufficient%20injection%20of%20layout%20constraints%20into%20the%20generation%0Aprocess.%20To%20further%20enhance%20generation%20quality%20and%20layout%20accuracy%2C%20we%0Aincorporate%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20based%20post-training%0Ascheme%20with%20specially%20designed%20layout%20reward%20functions%20for%20next-set-based%20AR%0Amodels.%20Experimental%20results%20demonstrate%20that%20SMARLI%20is%20able%20to%20seamlessly%0Aintegrate%20layout%20tokens%20with%20text%20and%20image%20tokens%20without%20compromising%0Ageneration%20quality.%20It%20achieves%20superior%20layoutaware%20control%20while%20maintaining%0Athe%20structural%20simplicity%20and%20generation%20efficiency%20of%20AR%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayout-Conditioned%2520Autoregressive%2520Text-to-Image%2520Generation%2520via%250A%2520%2520Structured%2520Masking%26entry.906535625%3DZirui%2520Zheng%2520and%2520Takashi%2520Isobe%2520and%2520Tong%2520Shen%2520and%2520Xu%2520Jia%2520and%2520Jianbin%2520Zhao%2520and%2520Xiaomin%2520Li%2520and%2520Mengmeng%2520Ge%2520and%2520Baolu%2520Li%2520and%2520Qinghe%2520Wang%2520and%2520Dong%2520Li%2520and%2520Dong%2520Zhou%2520and%2520Yunzhi%2520Zhuge%2520and%2520Huchuan%2520Lu%2520and%2520Emad%2520Barsoum%26entry.1292438233%3D%2520%2520While%2520autoregressive%2520%2528AR%2529%2520models%2520have%2520demonstrated%2520remarkable%2520success%2520in%250Aimage%2520generation%252C%2520extending%2520them%2520to%2520layout-conditioned%2520generation%2520remains%250Achallenging%2520due%2520to%2520the%2520sparse%2520nature%2520of%2520layout%2520conditions%2520and%2520the%2520risk%2520of%250Afeature%2520entanglement.%2520We%2520present%2520Structured%2520Masking%2520for%2520AR-based%250ALayout-to-Image%2520%2528SMARLI%2529%252C%2520a%2520novel%2520framework%2520for%2520layoutto-image%2520generation%2520that%250Aeffectively%2520integrates%2520spatial%2520layout%2520constraints%2520into%2520AR-based%2520image%250Ageneration.%2520To%2520equip%2520AR%2520model%2520with%2520layout%2520control%252C%2520a%2520specially%2520designed%250Astructured%2520masking%2520strategy%2520is%2520applied%2520to%2520attention%2520computation%2520to%2520govern%2520the%250Ainteraction%2520among%2520the%2520global%2520prompt%252C%2520layout%252C%2520and%2520image%2520tokens.%2520This%2520design%250Aprevents%2520mis-association%2520between%2520different%2520regions%2520and%2520their%2520descriptions%2520while%250Aenabling%2520sufficient%2520injection%2520of%2520layout%2520constraints%2520into%2520the%2520generation%250Aprocess.%2520To%2520further%2520enhance%2520generation%2520quality%2520and%2520layout%2520accuracy%252C%2520we%250Aincorporate%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520based%2520post-training%250Ascheme%2520with%2520specially%2520designed%2520layout%2520reward%2520functions%2520for%2520next-set-based%2520AR%250Amodels.%2520Experimental%2520results%2520demonstrate%2520that%2520SMARLI%2520is%2520able%2520to%2520seamlessly%250Aintegrate%2520layout%2520tokens%2520with%2520text%2520and%2520image%2520tokens%2520without%2520compromising%250Ageneration%2520quality.%2520It%2520achieves%2520superior%2520layoutaware%2520control%2520while%2520maintaining%250Athe%2520structural%2520simplicity%2520and%2520generation%2520efficiency%2520of%2520AR%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Layout-Conditioned%20Autoregressive%20Text-to-Image%20Generation%20via%0A%20%20Structured%20Masking&entry.906535625=Zirui%20Zheng%20and%20Takashi%20Isobe%20and%20Tong%20Shen%20and%20Xu%20Jia%20and%20Jianbin%20Zhao%20and%20Xiaomin%20Li%20and%20Mengmeng%20Ge%20and%20Baolu%20Li%20and%20Qinghe%20Wang%20and%20Dong%20Li%20and%20Dong%20Zhou%20and%20Yunzhi%20Zhuge%20and%20Huchuan%20Lu%20and%20Emad%20Barsoum&entry.1292438233=%20%20While%20autoregressive%20%28AR%29%20models%20have%20demonstrated%20remarkable%20success%20in%0Aimage%20generation%2C%20extending%20them%20to%20layout-conditioned%20generation%20remains%0Achallenging%20due%20to%20the%20sparse%20nature%20of%20layout%20conditions%20and%20the%20risk%20of%0Afeature%20entanglement.%20We%20present%20Structured%20Masking%20for%20AR-based%0ALayout-to-Image%20%28SMARLI%29%2C%20a%20novel%20framework%20for%20layoutto-image%20generation%20that%0Aeffectively%20integrates%20spatial%20layout%20constraints%20into%20AR-based%20image%0Ageneration.%20To%20equip%20AR%20model%20with%20layout%20control%2C%20a%20specially%20designed%0Astructured%20masking%20strategy%20is%20applied%20to%20attention%20computation%20to%20govern%20the%0Ainteraction%20among%20the%20global%20prompt%2C%20layout%2C%20and%20image%20tokens.%20This%20design%0Aprevents%20mis-association%20between%20different%20regions%20and%20their%20descriptions%20while%0Aenabling%20sufficient%20injection%20of%20layout%20constraints%20into%20the%20generation%0Aprocess.%20To%20further%20enhance%20generation%20quality%20and%20layout%20accuracy%2C%20we%0Aincorporate%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20based%20post-training%0Ascheme%20with%20specially%20designed%20layout%20reward%20functions%20for%20next-set-based%20AR%0Amodels.%20Experimental%20results%20demonstrate%20that%20SMARLI%20is%20able%20to%20seamlessly%0Aintegrate%20layout%20tokens%20with%20text%20and%20image%20tokens%20without%20compromising%0Ageneration%20quality.%20It%20achieves%20superior%20layoutaware%20control%20while%20maintaining%0Athe%20structural%20simplicity%20and%20generation%20efficiency%20of%20AR%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12046v1&entry.124074799=Read"},
{"title": "Stabilizing PINNs: A regularization scheme for PINN training to avoid\n  unstable fixed points of dynamical systems", "author": "Milos Babic and Franz M. Rohrhofer and Bernhard C. Geiger", "abstract": "  It was recently shown that the loss function used for training\nphysics-informed neural networks (PINNs) exhibits local minima at solutions\ncorresponding to fixed points of dynamical systems. In the forward setting,\nwhere the PINN is trained to solve initial value problems, these local minima\ncan interfere with training and potentially leading to physically incorrect\nsolutions. Building on stability theory, this paper proposes a regularization\nscheme that penalizes solutions corresponding to unstable fixed points.\nExperimental results on four dynamical systems, including the Lotka-Volterra\nmodel and the van der Pol oscillator, show that our scheme helps avoiding\nphysically incorrect solutions and substantially improves the training success\nrate of PINNs.\n", "link": "http://arxiv.org/abs/2509.11768v1", "date": "2025-09-15", "relevancy": 1.8254, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4586}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4551}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stabilizing%20PINNs%3A%20A%20regularization%20scheme%20for%20PINN%20training%20to%20avoid%0A%20%20unstable%20fixed%20points%20of%20dynamical%20systems&body=Title%3A%20Stabilizing%20PINNs%3A%20A%20regularization%20scheme%20for%20PINN%20training%20to%20avoid%0A%20%20unstable%20fixed%20points%20of%20dynamical%20systems%0AAuthor%3A%20Milos%20Babic%20and%20Franz%20M.%20Rohrhofer%20and%20Bernhard%20C.%20Geiger%0AAbstract%3A%20%20%20It%20was%20recently%20shown%20that%20the%20loss%20function%20used%20for%20training%0Aphysics-informed%20neural%20networks%20%28PINNs%29%20exhibits%20local%20minima%20at%20solutions%0Acorresponding%20to%20fixed%20points%20of%20dynamical%20systems.%20In%20the%20forward%20setting%2C%0Awhere%20the%20PINN%20is%20trained%20to%20solve%20initial%20value%20problems%2C%20these%20local%20minima%0Acan%20interfere%20with%20training%20and%20potentially%20leading%20to%20physically%20incorrect%0Asolutions.%20Building%20on%20stability%20theory%2C%20this%20paper%20proposes%20a%20regularization%0Ascheme%20that%20penalizes%20solutions%20corresponding%20to%20unstable%20fixed%20points.%0AExperimental%20results%20on%20four%20dynamical%20systems%2C%20including%20the%20Lotka-Volterra%0Amodel%20and%20the%20van%20der%20Pol%20oscillator%2C%20show%20that%20our%20scheme%20helps%20avoiding%0Aphysically%20incorrect%20solutions%20and%20substantially%20improves%20the%20training%20success%0Arate%20of%20PINNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStabilizing%2520PINNs%253A%2520A%2520regularization%2520scheme%2520for%2520PINN%2520training%2520to%2520avoid%250A%2520%2520unstable%2520fixed%2520points%2520of%2520dynamical%2520systems%26entry.906535625%3DMilos%2520Babic%2520and%2520Franz%2520M.%2520Rohrhofer%2520and%2520Bernhard%2520C.%2520Geiger%26entry.1292438233%3D%2520%2520It%2520was%2520recently%2520shown%2520that%2520the%2520loss%2520function%2520used%2520for%2520training%250Aphysics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520exhibits%2520local%2520minima%2520at%2520solutions%250Acorresponding%2520to%2520fixed%2520points%2520of%2520dynamical%2520systems.%2520In%2520the%2520forward%2520setting%252C%250Awhere%2520the%2520PINN%2520is%2520trained%2520to%2520solve%2520initial%2520value%2520problems%252C%2520these%2520local%2520minima%250Acan%2520interfere%2520with%2520training%2520and%2520potentially%2520leading%2520to%2520physically%2520incorrect%250Asolutions.%2520Building%2520on%2520stability%2520theory%252C%2520this%2520paper%2520proposes%2520a%2520regularization%250Ascheme%2520that%2520penalizes%2520solutions%2520corresponding%2520to%2520unstable%2520fixed%2520points.%250AExperimental%2520results%2520on%2520four%2520dynamical%2520systems%252C%2520including%2520the%2520Lotka-Volterra%250Amodel%2520and%2520the%2520van%2520der%2520Pol%2520oscillator%252C%2520show%2520that%2520our%2520scheme%2520helps%2520avoiding%250Aphysically%2520incorrect%2520solutions%2520and%2520substantially%2520improves%2520the%2520training%2520success%250Arate%2520of%2520PINNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stabilizing%20PINNs%3A%20A%20regularization%20scheme%20for%20PINN%20training%20to%20avoid%0A%20%20unstable%20fixed%20points%20of%20dynamical%20systems&entry.906535625=Milos%20Babic%20and%20Franz%20M.%20Rohrhofer%20and%20Bernhard%20C.%20Geiger&entry.1292438233=%20%20It%20was%20recently%20shown%20that%20the%20loss%20function%20used%20for%20training%0Aphysics-informed%20neural%20networks%20%28PINNs%29%20exhibits%20local%20minima%20at%20solutions%0Acorresponding%20to%20fixed%20points%20of%20dynamical%20systems.%20In%20the%20forward%20setting%2C%0Awhere%20the%20PINN%20is%20trained%20to%20solve%20initial%20value%20problems%2C%20these%20local%20minima%0Acan%20interfere%20with%20training%20and%20potentially%20leading%20to%20physically%20incorrect%0Asolutions.%20Building%20on%20stability%20theory%2C%20this%20paper%20proposes%20a%20regularization%0Ascheme%20that%20penalizes%20solutions%20corresponding%20to%20unstable%20fixed%20points.%0AExperimental%20results%20on%20four%20dynamical%20systems%2C%20including%20the%20Lotka-Volterra%0Amodel%20and%20the%20van%20der%20Pol%20oscillator%2C%20show%20that%20our%20scheme%20helps%20avoiding%0Aphysically%20incorrect%20solutions%20and%20substantially%20improves%20the%20training%20success%0Arate%20of%20PINNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11768v1&entry.124074799=Read"},
{"title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation", "author": "Ruibin Yuan and Hanfeng Lin and Shuyue Guo and Ge Zhang and Jiahao Pan and Yongyi Zang and Haohe Liu and Yiming Liang and Wenye Ma and Xingjian Du and Xinrun Du and Zhen Ye and Tianyu Zheng and Zhengxuan Jiang and Yinghao Ma and Minghao Liu and Zeyue Tian and Ziya Zhou and Liumeng Xue and Xingwei Qu and Yizhi Li and Shangda Wu and Tianhao Shen and Ziyang Ma and Jun Zhan and Chunhui Wang and Yatian Wang and Xiaowei Chi and Xinyue Zhang and Zhenzhu Yang and Xiangzhou Wang and Shansong Liu and Lingrui Mei and Peng Li and Junjie Wang and Jianwei Yu and Guojian Pang and Xu Li and Zihao Wang and Xiaohuan Zhou and Lijun Yu and Emmanouil Benetos and Yong Chen and Chenghua Lin and Xie Chen and Gus Xia and Zhaoxiang Zhang and Chao Zhang and Wenhu Chen and Xinyu Zhou and Xipeng Qiu and Roger Dannenberg and Jiaheng Liu and Jian Yang and Wenhao Huang and Wei Xue and Xu Tan and Yike Guo", "abstract": "  We tackle the task of long-form music generation--particularly the\nchallenging \\textbf{lyrics-to-song} problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation\n", "link": "http://arxiv.org/abs/2503.08638v2", "date": "2025-09-15", "relevancy": 1.8742, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4761}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4676}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YuE%3A%20Scaling%20Open%20Foundation%20Models%20for%20Long-Form%20Music%20Generation&body=Title%3A%20YuE%3A%20Scaling%20Open%20Foundation%20Models%20for%20Long-Form%20Music%20Generation%0AAuthor%3A%20Ruibin%20Yuan%20and%20Hanfeng%20Lin%20and%20Shuyue%20Guo%20and%20Ge%20Zhang%20and%20Jiahao%20Pan%20and%20Yongyi%20Zang%20and%20Haohe%20Liu%20and%20Yiming%20Liang%20and%20Wenye%20Ma%20and%20Xingjian%20Du%20and%20Xinrun%20Du%20and%20Zhen%20Ye%20and%20Tianyu%20Zheng%20and%20Zhengxuan%20Jiang%20and%20Yinghao%20Ma%20and%20Minghao%20Liu%20and%20Zeyue%20Tian%20and%20Ziya%20Zhou%20and%20Liumeng%20Xue%20and%20Xingwei%20Qu%20and%20Yizhi%20Li%20and%20Shangda%20Wu%20and%20Tianhao%20Shen%20and%20Ziyang%20Ma%20and%20Jun%20Zhan%20and%20Chunhui%20Wang%20and%20Yatian%20Wang%20and%20Xiaowei%20Chi%20and%20Xinyue%20Zhang%20and%20Zhenzhu%20Yang%20and%20Xiangzhou%20Wang%20and%20Shansong%20Liu%20and%20Lingrui%20Mei%20and%20Peng%20Li%20and%20Junjie%20Wang%20and%20Jianwei%20Yu%20and%20Guojian%20Pang%20and%20Xu%20Li%20and%20Zihao%20Wang%20and%20Xiaohuan%20Zhou%20and%20Lijun%20Yu%20and%20Emmanouil%20Benetos%20and%20Yong%20Chen%20and%20Chenghua%20Lin%20and%20Xie%20Chen%20and%20Gus%20Xia%20and%20Zhaoxiang%20Zhang%20and%20Chao%20Zhang%20and%20Wenhu%20Chen%20and%20Xinyu%20Zhou%20and%20Xipeng%20Qiu%20and%20Roger%20Dannenberg%20and%20Jiaheng%20Liu%20and%20Jian%20Yang%20and%20Wenhao%20Huang%20and%20Wei%20Xue%20and%20Xu%20Tan%20and%20Yike%20Guo%0AAbstract%3A%20%20%20We%20tackle%20the%20task%20of%20long-form%20music%20generation--particularly%20the%0Achallenging%20%5Ctextbf%7Blyrics-to-song%7D%20problem--by%20introducing%20YuE%2C%20a%20family%20of%0Aopen%20foundation%20models%20based%20on%20the%20LLaMA2%20architecture.%20Specifically%2C%20YuE%0Ascales%20to%20trillions%20of%20tokens%20and%20generates%20up%20to%20five%20minutes%20of%20music%20while%0Amaintaining%20lyrical%20alignment%2C%20coherent%20musical%20structure%2C%20and%20engaging%20vocal%0Amelodies%20with%20appropriate%20accompaniment.%20It%20achieves%20this%20through%20%281%29%0Atrack-decoupled%20next-token%20prediction%20to%20overcome%20dense%20mixture%20signals%2C%20%282%29%0Astructural%20progressive%20conditioning%20for%20long-context%20lyrical%20alignment%2C%20and%20%283%29%0Aa%20multitask%2C%20multiphase%20pre-training%20recipe%20to%20converge%20and%20generalize.%20In%0Aaddition%2C%20we%20redesign%20the%20in-context%20learning%20technique%20for%20music%20generation%2C%0Aenabling%20versatile%20style%20transfer%20%28e.g.%2C%20converting%20Japanese%20city%20pop%20into%20an%0AEnglish%20rap%20while%20preserving%20the%20original%20accompaniment%29%20and%20bidirectional%0Ageneration.%20Through%20extensive%20evaluation%2C%20we%20demonstrate%20that%20YuE%20matches%20or%0Aeven%20surpasses%20some%20of%20the%20proprietary%20systems%20in%20musicality%20and%20vocal%20agility.%0AIn%20addition%2C%20fine-tuning%20YuE%20enables%20additional%20controls%20and%20enhanced%20support%0Afor%20tail%20languages.%20Furthermore%2C%20beyond%20generation%2C%20we%20show%20that%20YuE%27s%20learned%0Arepresentations%20can%20perform%20well%20on%20music%20understanding%20tasks%2C%20where%20the%0Aresults%20of%20YuE%20match%20or%20exceed%20state-of-the-art%20methods%20on%20the%20MARBLE%0Abenchmark.%20Keywords%3A%20lyrics2song%2C%20song%20generation%2C%20long-form%2C%20foundation%20model%2C%0Amusic%20generation%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08638v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYuE%253A%2520Scaling%2520Open%2520Foundation%2520Models%2520for%2520Long-Form%2520Music%2520Generation%26entry.906535625%3DRuibin%2520Yuan%2520and%2520Hanfeng%2520Lin%2520and%2520Shuyue%2520Guo%2520and%2520Ge%2520Zhang%2520and%2520Jiahao%2520Pan%2520and%2520Yongyi%2520Zang%2520and%2520Haohe%2520Liu%2520and%2520Yiming%2520Liang%2520and%2520Wenye%2520Ma%2520and%2520Xingjian%2520Du%2520and%2520Xinrun%2520Du%2520and%2520Zhen%2520Ye%2520and%2520Tianyu%2520Zheng%2520and%2520Zhengxuan%2520Jiang%2520and%2520Yinghao%2520Ma%2520and%2520Minghao%2520Liu%2520and%2520Zeyue%2520Tian%2520and%2520Ziya%2520Zhou%2520and%2520Liumeng%2520Xue%2520and%2520Xingwei%2520Qu%2520and%2520Yizhi%2520Li%2520and%2520Shangda%2520Wu%2520and%2520Tianhao%2520Shen%2520and%2520Ziyang%2520Ma%2520and%2520Jun%2520Zhan%2520and%2520Chunhui%2520Wang%2520and%2520Yatian%2520Wang%2520and%2520Xiaowei%2520Chi%2520and%2520Xinyue%2520Zhang%2520and%2520Zhenzhu%2520Yang%2520and%2520Xiangzhou%2520Wang%2520and%2520Shansong%2520Liu%2520and%2520Lingrui%2520Mei%2520and%2520Peng%2520Li%2520and%2520Junjie%2520Wang%2520and%2520Jianwei%2520Yu%2520and%2520Guojian%2520Pang%2520and%2520Xu%2520Li%2520and%2520Zihao%2520Wang%2520and%2520Xiaohuan%2520Zhou%2520and%2520Lijun%2520Yu%2520and%2520Emmanouil%2520Benetos%2520and%2520Yong%2520Chen%2520and%2520Chenghua%2520Lin%2520and%2520Xie%2520Chen%2520and%2520Gus%2520Xia%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Chao%2520Zhang%2520and%2520Wenhu%2520Chen%2520and%2520Xinyu%2520Zhou%2520and%2520Xipeng%2520Qiu%2520and%2520Roger%2520Dannenberg%2520and%2520Jiaheng%2520Liu%2520and%2520Jian%2520Yang%2520and%2520Wenhao%2520Huang%2520and%2520Wei%2520Xue%2520and%2520Xu%2520Tan%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520task%2520of%2520long-form%2520music%2520generation--particularly%2520the%250Achallenging%2520%255Ctextbf%257Blyrics-to-song%257D%2520problem--by%2520introducing%2520YuE%252C%2520a%2520family%2520of%250Aopen%2520foundation%2520models%2520based%2520on%2520the%2520LLaMA2%2520architecture.%2520Specifically%252C%2520YuE%250Ascales%2520to%2520trillions%2520of%2520tokens%2520and%2520generates%2520up%2520to%2520five%2520minutes%2520of%2520music%2520while%250Amaintaining%2520lyrical%2520alignment%252C%2520coherent%2520musical%2520structure%252C%2520and%2520engaging%2520vocal%250Amelodies%2520with%2520appropriate%2520accompaniment.%2520It%2520achieves%2520this%2520through%2520%25281%2529%250Atrack-decoupled%2520next-token%2520prediction%2520to%2520overcome%2520dense%2520mixture%2520signals%252C%2520%25282%2529%250Astructural%2520progressive%2520conditioning%2520for%2520long-context%2520lyrical%2520alignment%252C%2520and%2520%25283%2529%250Aa%2520multitask%252C%2520multiphase%2520pre-training%2520recipe%2520to%2520converge%2520and%2520generalize.%2520In%250Aaddition%252C%2520we%2520redesign%2520the%2520in-context%2520learning%2520technique%2520for%2520music%2520generation%252C%250Aenabling%2520versatile%2520style%2520transfer%2520%2528e.g.%252C%2520converting%2520Japanese%2520city%2520pop%2520into%2520an%250AEnglish%2520rap%2520while%2520preserving%2520the%2520original%2520accompaniment%2529%2520and%2520bidirectional%250Ageneration.%2520Through%2520extensive%2520evaluation%252C%2520we%2520demonstrate%2520that%2520YuE%2520matches%2520or%250Aeven%2520surpasses%2520some%2520of%2520the%2520proprietary%2520systems%2520in%2520musicality%2520and%2520vocal%2520agility.%250AIn%2520addition%252C%2520fine-tuning%2520YuE%2520enables%2520additional%2520controls%2520and%2520enhanced%2520support%250Afor%2520tail%2520languages.%2520Furthermore%252C%2520beyond%2520generation%252C%2520we%2520show%2520that%2520YuE%2527s%2520learned%250Arepresentations%2520can%2520perform%2520well%2520on%2520music%2520understanding%2520tasks%252C%2520where%2520the%250Aresults%2520of%2520YuE%2520match%2520or%2520exceed%2520state-of-the-art%2520methods%2520on%2520the%2520MARBLE%250Abenchmark.%2520Keywords%253A%2520lyrics2song%252C%2520song%2520generation%252C%2520long-form%252C%2520foundation%2520model%252C%250Amusic%2520generation%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08638v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YuE%3A%20Scaling%20Open%20Foundation%20Models%20for%20Long-Form%20Music%20Generation&entry.906535625=Ruibin%20Yuan%20and%20Hanfeng%20Lin%20and%20Shuyue%20Guo%20and%20Ge%20Zhang%20and%20Jiahao%20Pan%20and%20Yongyi%20Zang%20and%20Haohe%20Liu%20and%20Yiming%20Liang%20and%20Wenye%20Ma%20and%20Xingjian%20Du%20and%20Xinrun%20Du%20and%20Zhen%20Ye%20and%20Tianyu%20Zheng%20and%20Zhengxuan%20Jiang%20and%20Yinghao%20Ma%20and%20Minghao%20Liu%20and%20Zeyue%20Tian%20and%20Ziya%20Zhou%20and%20Liumeng%20Xue%20and%20Xingwei%20Qu%20and%20Yizhi%20Li%20and%20Shangda%20Wu%20and%20Tianhao%20Shen%20and%20Ziyang%20Ma%20and%20Jun%20Zhan%20and%20Chunhui%20Wang%20and%20Yatian%20Wang%20and%20Xiaowei%20Chi%20and%20Xinyue%20Zhang%20and%20Zhenzhu%20Yang%20and%20Xiangzhou%20Wang%20and%20Shansong%20Liu%20and%20Lingrui%20Mei%20and%20Peng%20Li%20and%20Junjie%20Wang%20and%20Jianwei%20Yu%20and%20Guojian%20Pang%20and%20Xu%20Li%20and%20Zihao%20Wang%20and%20Xiaohuan%20Zhou%20and%20Lijun%20Yu%20and%20Emmanouil%20Benetos%20and%20Yong%20Chen%20and%20Chenghua%20Lin%20and%20Xie%20Chen%20and%20Gus%20Xia%20and%20Zhaoxiang%20Zhang%20and%20Chao%20Zhang%20and%20Wenhu%20Chen%20and%20Xinyu%20Zhou%20and%20Xipeng%20Qiu%20and%20Roger%20Dannenberg%20and%20Jiaheng%20Liu%20and%20Jian%20Yang%20and%20Wenhao%20Huang%20and%20Wei%20Xue%20and%20Xu%20Tan%20and%20Yike%20Guo&entry.1292438233=%20%20We%20tackle%20the%20task%20of%20long-form%20music%20generation--particularly%20the%0Achallenging%20%5Ctextbf%7Blyrics-to-song%7D%20problem--by%20introducing%20YuE%2C%20a%20family%20of%0Aopen%20foundation%20models%20based%20on%20the%20LLaMA2%20architecture.%20Specifically%2C%20YuE%0Ascales%20to%20trillions%20of%20tokens%20and%20generates%20up%20to%20five%20minutes%20of%20music%20while%0Amaintaining%20lyrical%20alignment%2C%20coherent%20musical%20structure%2C%20and%20engaging%20vocal%0Amelodies%20with%20appropriate%20accompaniment.%20It%20achieves%20this%20through%20%281%29%0Atrack-decoupled%20next-token%20prediction%20to%20overcome%20dense%20mixture%20signals%2C%20%282%29%0Astructural%20progressive%20conditioning%20for%20long-context%20lyrical%20alignment%2C%20and%20%283%29%0Aa%20multitask%2C%20multiphase%20pre-training%20recipe%20to%20converge%20and%20generalize.%20In%0Aaddition%2C%20we%20redesign%20the%20in-context%20learning%20technique%20for%20music%20generation%2C%0Aenabling%20versatile%20style%20transfer%20%28e.g.%2C%20converting%20Japanese%20city%20pop%20into%20an%0AEnglish%20rap%20while%20preserving%20the%20original%20accompaniment%29%20and%20bidirectional%0Ageneration.%20Through%20extensive%20evaluation%2C%20we%20demonstrate%20that%20YuE%20matches%20or%0Aeven%20surpasses%20some%20of%20the%20proprietary%20systems%20in%20musicality%20and%20vocal%20agility.%0AIn%20addition%2C%20fine-tuning%20YuE%20enables%20additional%20controls%20and%20enhanced%20support%0Afor%20tail%20languages.%20Furthermore%2C%20beyond%20generation%2C%20we%20show%20that%20YuE%27s%20learned%0Arepresentations%20can%20perform%20well%20on%20music%20understanding%20tasks%2C%20where%20the%0Aresults%20of%20YuE%20match%20or%20exceed%20state-of-the-art%20methods%20on%20the%20MARBLE%0Abenchmark.%20Keywords%3A%20lyrics2song%2C%20song%20generation%2C%20long-form%2C%20foundation%20model%2C%0Amusic%20generation%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08638v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


