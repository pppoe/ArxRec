<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260112.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GP-GS: Gaussian Processes Densification for 3D Gaussian Splatting", "author": "Zhihao Guo and Jingxuan Su and Chenghao Qian and Shenglin Wang and Jinlong Fan and Jing Zhang and Wei Zhou and Hadi Amirpour and Yunlong Zhao and Liangxiu Han and Peng Wang", "abstract": "3D Gaussian Splatting (3DGS) enables photorealistic rendering but suffers from artefacts due to sparse Structure-from-Motion (SfM) initialisation. To address this limitation, we propose GP-GS, a Gaussian Process (GP) based densification framework for 3DGS optimisation. GP-GS formulates point cloud densification as a continuous regression problem, where a GP learns a local mapping from 2D pixel coordinates to 3D position and colour attributes. An adaptive neighbourhood-based sampling strategy generates candidate pixels for inference, while GP-predicted uncertainty is used to filter unreliable predictions, reducing noise and preserving geometric structure. Extensive experiments on synthetic and real-world benchmarks demonstrate that GP-GS consistently improves reconstruction quality and rendering fidelity, achieving up to 1.12 dB PSNR improvement over strong baselines.", "link": "http://arxiv.org/abs/2502.02283v6", "date": "2026-01-12", "relevancy": 3.4668, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7199}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6969}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GP-GS%3A%20Gaussian%20Processes%20Densification%20for%203D%20Gaussian%20Splatting&body=Title%3A%20GP-GS%3A%20Gaussian%20Processes%20Densification%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Zhihao%20Guo%20and%20Jingxuan%20Su%20and%20Chenghao%20Qian%20and%20Shenglin%20Wang%20and%20Jinlong%20Fan%20and%20Jing%20Zhang%20and%20Wei%20Zhou%20and%20Hadi%20Amirpour%20and%20Yunlong%20Zhao%20and%20Liangxiu%20Han%20and%20Peng%20Wang%0AAbstract%3A%203D%20Gaussian%20Splatting%20%283DGS%29%20enables%20photorealistic%20rendering%20but%20suffers%20from%20artefacts%20due%20to%20sparse%20Structure-from-Motion%20%28SfM%29%20initialisation.%20To%20address%20this%20limitation%2C%20we%20propose%20GP-GS%2C%20a%20Gaussian%20Process%20%28GP%29%20based%20densification%20framework%20for%203DGS%20optimisation.%20GP-GS%20formulates%20point%20cloud%20densification%20as%20a%20continuous%20regression%20problem%2C%20where%20a%20GP%20learns%20a%20local%20mapping%20from%202D%20pixel%20coordinates%20to%203D%20position%20and%20colour%20attributes.%20An%20adaptive%20neighbourhood-based%20sampling%20strategy%20generates%20candidate%20pixels%20for%20inference%2C%20while%20GP-predicted%20uncertainty%20is%20used%20to%20filter%20unreliable%20predictions%2C%20reducing%20noise%20and%20preserving%20geometric%20structure.%20Extensive%20experiments%20on%20synthetic%20and%20real-world%20benchmarks%20demonstrate%20that%20GP-GS%20consistently%20improves%20reconstruction%20quality%20and%20rendering%20fidelity%2C%20achieving%20up%20to%201.12%20dB%20PSNR%20improvement%20over%20strong%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2502.02283v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGP-GS%253A%2520Gaussian%2520Processes%2520Densification%2520for%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DZhihao%2520Guo%2520and%2520Jingxuan%2520Su%2520and%2520Chenghao%2520Qian%2520and%2520Shenglin%2520Wang%2520and%2520Jinlong%2520Fan%2520and%2520Jing%2520Zhang%2520and%2520Wei%2520Zhou%2520and%2520Hadi%2520Amirpour%2520and%2520Yunlong%2520Zhao%2520and%2520Liangxiu%2520Han%2520and%2520Peng%2520Wang%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520enables%2520photorealistic%2520rendering%2520but%2520suffers%2520from%2520artefacts%2520due%2520to%2520sparse%2520Structure-from-Motion%2520%2528SfM%2529%2520initialisation.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520GP-GS%252C%2520a%2520Gaussian%2520Process%2520%2528GP%2529%2520based%2520densification%2520framework%2520for%25203DGS%2520optimisation.%2520GP-GS%2520formulates%2520point%2520cloud%2520densification%2520as%2520a%2520continuous%2520regression%2520problem%252C%2520where%2520a%2520GP%2520learns%2520a%2520local%2520mapping%2520from%25202D%2520pixel%2520coordinates%2520to%25203D%2520position%2520and%2520colour%2520attributes.%2520An%2520adaptive%2520neighbourhood-based%2520sampling%2520strategy%2520generates%2520candidate%2520pixels%2520for%2520inference%252C%2520while%2520GP-predicted%2520uncertainty%2520is%2520used%2520to%2520filter%2520unreliable%2520predictions%252C%2520reducing%2520noise%2520and%2520preserving%2520geometric%2520structure.%2520Extensive%2520experiments%2520on%2520synthetic%2520and%2520real-world%2520benchmarks%2520demonstrate%2520that%2520GP-GS%2520consistently%2520improves%2520reconstruction%2520quality%2520and%2520rendering%2520fidelity%252C%2520achieving%2520up%2520to%25201.12%2520dB%2520PSNR%2520improvement%2520over%2520strong%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02283v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GP-GS%3A%20Gaussian%20Processes%20Densification%20for%203D%20Gaussian%20Splatting&entry.906535625=Zhihao%20Guo%20and%20Jingxuan%20Su%20and%20Chenghao%20Qian%20and%20Shenglin%20Wang%20and%20Jinlong%20Fan%20and%20Jing%20Zhang%20and%20Wei%20Zhou%20and%20Hadi%20Amirpour%20and%20Yunlong%20Zhao%20and%20Liangxiu%20Han%20and%20Peng%20Wang&entry.1292438233=3D%20Gaussian%20Splatting%20%283DGS%29%20enables%20photorealistic%20rendering%20but%20suffers%20from%20artefacts%20due%20to%20sparse%20Structure-from-Motion%20%28SfM%29%20initialisation.%20To%20address%20this%20limitation%2C%20we%20propose%20GP-GS%2C%20a%20Gaussian%20Process%20%28GP%29%20based%20densification%20framework%20for%203DGS%20optimisation.%20GP-GS%20formulates%20point%20cloud%20densification%20as%20a%20continuous%20regression%20problem%2C%20where%20a%20GP%20learns%20a%20local%20mapping%20from%202D%20pixel%20coordinates%20to%203D%20position%20and%20colour%20attributes.%20An%20adaptive%20neighbourhood-based%20sampling%20strategy%20generates%20candidate%20pixels%20for%20inference%2C%20while%20GP-predicted%20uncertainty%20is%20used%20to%20filter%20unreliable%20predictions%2C%20reducing%20noise%20and%20preserving%20geometric%20structure.%20Extensive%20experiments%20on%20synthetic%20and%20real-world%20benchmarks%20demonstrate%20that%20GP-GS%20consistently%20improves%20reconstruction%20quality%20and%20rendering%20fidelity%2C%20achieving%20up%20to%201.12%20dB%20PSNR%20improvement%20over%20strong%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2502.02283v6&entry.124074799=Read"},
{"title": "Mon3tr: Monocular 3D Telepresence with Pre-built Gaussian Avatars as Amortization", "author": "Fangyu Lin and Yingdong Hu and Zhening Liu and Yufan Zhuang and Zehong Lin and Jun Zhang", "abstract": "Immersive telepresence aims to transform human interaction in AR/VR applications by enabling lifelike full-body holographic representations for enhanced remote collaboration. However, existing systems rely on hardware-intensive multi-camera setups and demand high bandwidth for volumetric streaming, limiting their real-time performance on mobile devices. To overcome these challenges, we propose Mon3tr, a novel Monocular 3D telepresence framework that integrates 3D Gaussian splatting (3DGS) based parametric human modeling into telepresence for the first time. Mon3tr adopts an amortized computation strategy, dividing the process into a one-time offline multi-view reconstruction phase to build a user-specific avatar and a monocular online inference phase during live telepresence sessions. A single monocular RGB camera is used to capture body motions and facial expressions in real time to drive the 3DGS-based parametric human model, significantly reducing system complexity and cost. The extracted motion and appearance features are transmitted at < 0.2 Mbps over WebRTC's data channel, allowing robust adaptation to network fluctuations. On the receiver side, e.g., Meta Quest 3, we develop a lightweight 3DGS attribute deformation network to dynamically generate corrective 3DGS attribute adjustments on the pre-built avatar, synthesizing photorealistic motion and appearance at ~ 60 FPS. Extensive experiments demonstrate the state-of-the-art performance of our method, achieving a PSNR of > 28 dB for novel poses, an end-to-end latency of ~ 80 ms, and > 1000x bandwidth reduction compared to point-cloud streaming, while supporting real-time operation from monocular inputs across diverse scenarios. Our demos can be found at https://mon3tr3d.github.io.", "link": "http://arxiv.org/abs/2601.07518v1", "date": "2026-01-12", "relevancy": 3.2639, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6592}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6592}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.64}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mon3tr%3A%20Monocular%203D%20Telepresence%20with%20Pre-built%20Gaussian%20Avatars%20as%20Amortization&body=Title%3A%20Mon3tr%3A%20Monocular%203D%20Telepresence%20with%20Pre-built%20Gaussian%20Avatars%20as%20Amortization%0AAuthor%3A%20Fangyu%20Lin%20and%20Yingdong%20Hu%20and%20Zhening%20Liu%20and%20Yufan%20Zhuang%20and%20Zehong%20Lin%20and%20Jun%20Zhang%0AAbstract%3A%20Immersive%20telepresence%20aims%20to%20transform%20human%20interaction%20in%20AR/VR%20applications%20by%20enabling%20lifelike%20full-body%20holographic%20representations%20for%20enhanced%20remote%20collaboration.%20However%2C%20existing%20systems%20rely%20on%20hardware-intensive%20multi-camera%20setups%20and%20demand%20high%20bandwidth%20for%20volumetric%20streaming%2C%20limiting%20their%20real-time%20performance%20on%20mobile%20devices.%20To%20overcome%20these%20challenges%2C%20we%20propose%20Mon3tr%2C%20a%20novel%20Monocular%203D%20telepresence%20framework%20that%20integrates%203D%20Gaussian%20splatting%20%283DGS%29%20based%20parametric%20human%20modeling%20into%20telepresence%20for%20the%20first%20time.%20Mon3tr%20adopts%20an%20amortized%20computation%20strategy%2C%20dividing%20the%20process%20into%20a%20one-time%20offline%20multi-view%20reconstruction%20phase%20to%20build%20a%20user-specific%20avatar%20and%20a%20monocular%20online%20inference%20phase%20during%20live%20telepresence%20sessions.%20A%20single%20monocular%20RGB%20camera%20is%20used%20to%20capture%20body%20motions%20and%20facial%20expressions%20in%20real%20time%20to%20drive%20the%203DGS-based%20parametric%20human%20model%2C%20significantly%20reducing%20system%20complexity%20and%20cost.%20The%20extracted%20motion%20and%20appearance%20features%20are%20transmitted%20at%20%3C%200.2%20Mbps%20over%20WebRTC%27s%20data%20channel%2C%20allowing%20robust%20adaptation%20to%20network%20fluctuations.%20On%20the%20receiver%20side%2C%20e.g.%2C%20Meta%20Quest%203%2C%20we%20develop%20a%20lightweight%203DGS%20attribute%20deformation%20network%20to%20dynamically%20generate%20corrective%203DGS%20attribute%20adjustments%20on%20the%20pre-built%20avatar%2C%20synthesizing%20photorealistic%20motion%20and%20appearance%20at%20~%2060%20FPS.%20Extensive%20experiments%20demonstrate%20the%20state-of-the-art%20performance%20of%20our%20method%2C%20achieving%20a%20PSNR%20of%20%3E%2028%20dB%20for%20novel%20poses%2C%20an%20end-to-end%20latency%20of%20~%2080%20ms%2C%20and%20%3E%201000x%20bandwidth%20reduction%20compared%20to%20point-cloud%20streaming%2C%20while%20supporting%20real-time%20operation%20from%20monocular%20inputs%20across%20diverse%20scenarios.%20Our%20demos%20can%20be%20found%20at%20https%3A//mon3tr3d.github.io.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMon3tr%253A%2520Monocular%25203D%2520Telepresence%2520with%2520Pre-built%2520Gaussian%2520Avatars%2520as%2520Amortization%26entry.906535625%3DFangyu%2520Lin%2520and%2520Yingdong%2520Hu%2520and%2520Zhening%2520Liu%2520and%2520Yufan%2520Zhuang%2520and%2520Zehong%2520Lin%2520and%2520Jun%2520Zhang%26entry.1292438233%3DImmersive%2520telepresence%2520aims%2520to%2520transform%2520human%2520interaction%2520in%2520AR/VR%2520applications%2520by%2520enabling%2520lifelike%2520full-body%2520holographic%2520representations%2520for%2520enhanced%2520remote%2520collaboration.%2520However%252C%2520existing%2520systems%2520rely%2520on%2520hardware-intensive%2520multi-camera%2520setups%2520and%2520demand%2520high%2520bandwidth%2520for%2520volumetric%2520streaming%252C%2520limiting%2520their%2520real-time%2520performance%2520on%2520mobile%2520devices.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520Mon3tr%252C%2520a%2520novel%2520Monocular%25203D%2520telepresence%2520framework%2520that%2520integrates%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%2520based%2520parametric%2520human%2520modeling%2520into%2520telepresence%2520for%2520the%2520first%2520time.%2520Mon3tr%2520adopts%2520an%2520amortized%2520computation%2520strategy%252C%2520dividing%2520the%2520process%2520into%2520a%2520one-time%2520offline%2520multi-view%2520reconstruction%2520phase%2520to%2520build%2520a%2520user-specific%2520avatar%2520and%2520a%2520monocular%2520online%2520inference%2520phase%2520during%2520live%2520telepresence%2520sessions.%2520A%2520single%2520monocular%2520RGB%2520camera%2520is%2520used%2520to%2520capture%2520body%2520motions%2520and%2520facial%2520expressions%2520in%2520real%2520time%2520to%2520drive%2520the%25203DGS-based%2520parametric%2520human%2520model%252C%2520significantly%2520reducing%2520system%2520complexity%2520and%2520cost.%2520The%2520extracted%2520motion%2520and%2520appearance%2520features%2520are%2520transmitted%2520at%2520%253C%25200.2%2520Mbps%2520over%2520WebRTC%2527s%2520data%2520channel%252C%2520allowing%2520robust%2520adaptation%2520to%2520network%2520fluctuations.%2520On%2520the%2520receiver%2520side%252C%2520e.g.%252C%2520Meta%2520Quest%25203%252C%2520we%2520develop%2520a%2520lightweight%25203DGS%2520attribute%2520deformation%2520network%2520to%2520dynamically%2520generate%2520corrective%25203DGS%2520attribute%2520adjustments%2520on%2520the%2520pre-built%2520avatar%252C%2520synthesizing%2520photorealistic%2520motion%2520and%2520appearance%2520at%2520~%252060%2520FPS.%2520Extensive%2520experiments%2520demonstrate%2520the%2520state-of-the-art%2520performance%2520of%2520our%2520method%252C%2520achieving%2520a%2520PSNR%2520of%2520%253E%252028%2520dB%2520for%2520novel%2520poses%252C%2520an%2520end-to-end%2520latency%2520of%2520~%252080%2520ms%252C%2520and%2520%253E%25201000x%2520bandwidth%2520reduction%2520compared%2520to%2520point-cloud%2520streaming%252C%2520while%2520supporting%2520real-time%2520operation%2520from%2520monocular%2520inputs%2520across%2520diverse%2520scenarios.%2520Our%2520demos%2520can%2520be%2520found%2520at%2520https%253A//mon3tr3d.github.io.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mon3tr%3A%20Monocular%203D%20Telepresence%20with%20Pre-built%20Gaussian%20Avatars%20as%20Amortization&entry.906535625=Fangyu%20Lin%20and%20Yingdong%20Hu%20and%20Zhening%20Liu%20and%20Yufan%20Zhuang%20and%20Zehong%20Lin%20and%20Jun%20Zhang&entry.1292438233=Immersive%20telepresence%20aims%20to%20transform%20human%20interaction%20in%20AR/VR%20applications%20by%20enabling%20lifelike%20full-body%20holographic%20representations%20for%20enhanced%20remote%20collaboration.%20However%2C%20existing%20systems%20rely%20on%20hardware-intensive%20multi-camera%20setups%20and%20demand%20high%20bandwidth%20for%20volumetric%20streaming%2C%20limiting%20their%20real-time%20performance%20on%20mobile%20devices.%20To%20overcome%20these%20challenges%2C%20we%20propose%20Mon3tr%2C%20a%20novel%20Monocular%203D%20telepresence%20framework%20that%20integrates%203D%20Gaussian%20splatting%20%283DGS%29%20based%20parametric%20human%20modeling%20into%20telepresence%20for%20the%20first%20time.%20Mon3tr%20adopts%20an%20amortized%20computation%20strategy%2C%20dividing%20the%20process%20into%20a%20one-time%20offline%20multi-view%20reconstruction%20phase%20to%20build%20a%20user-specific%20avatar%20and%20a%20monocular%20online%20inference%20phase%20during%20live%20telepresence%20sessions.%20A%20single%20monocular%20RGB%20camera%20is%20used%20to%20capture%20body%20motions%20and%20facial%20expressions%20in%20real%20time%20to%20drive%20the%203DGS-based%20parametric%20human%20model%2C%20significantly%20reducing%20system%20complexity%20and%20cost.%20The%20extracted%20motion%20and%20appearance%20features%20are%20transmitted%20at%20%3C%200.2%20Mbps%20over%20WebRTC%27s%20data%20channel%2C%20allowing%20robust%20adaptation%20to%20network%20fluctuations.%20On%20the%20receiver%20side%2C%20e.g.%2C%20Meta%20Quest%203%2C%20we%20develop%20a%20lightweight%203DGS%20attribute%20deformation%20network%20to%20dynamically%20generate%20corrective%203DGS%20attribute%20adjustments%20on%20the%20pre-built%20avatar%2C%20synthesizing%20photorealistic%20motion%20and%20appearance%20at%20~%2060%20FPS.%20Extensive%20experiments%20demonstrate%20the%20state-of-the-art%20performance%20of%20our%20method%2C%20achieving%20a%20PSNR%20of%20%3E%2028%20dB%20for%20novel%20poses%2C%20an%20end-to-end%20latency%20of%20~%2080%20ms%2C%20and%20%3E%201000x%20bandwidth%20reduction%20compared%20to%20point-cloud%20streaming%2C%20while%20supporting%20real-time%20operation%20from%20monocular%20inputs%20across%20diverse%20scenarios.%20Our%20demos%20can%20be%20found%20at%20https%3A//mon3tr3d.github.io.&entry.1838667208=http%3A//arxiv.org/abs/2601.07518v1&entry.124074799=Read"},
{"title": "Sketch&Patch++: Efficient Structure-Aware 3D Gaussian Representation", "author": "Yuang Shi and G\u00e9raldine Morin and Simone Gasparini and Wei Tsang Ooi", "abstract": "We observe that Gaussians exhibit distinct roles and characteristics analogous to traditional artistic techniques -- like how artists first sketch outlines before filling in broader areas with color, some Gaussians capture high-frequency features such as edges and contours, while others represent broader, smoother regions analogous to brush strokes that add volume and depth. Based on this observation, we propose a hybrid representation that categorizes Gaussians into (i) Sketch Gaussians, which represent high-frequency, boundary-defining features, and (ii) Patch Gaussians, which cover low-frequency, smooth regions. This semantic separation naturally enables layered progressive streaming, where the compact Sketch Gaussians establish the structural skeleton before Patch Gaussians incrementally refine volumetric detail.\n  In this work, we extend our previous method to arbitrary 3D scenes by proposing a novel hierarchical adaptive categorization framework that operates directly on the 3DGS representation. Our approach employs multi-criteria density-based clustering, combined with adaptive quality-driven refinement. This method eliminates dependency on external 3D line primitives while ensuring optimal parametric encoding effectiveness. Our comprehensive evaluation across diverse scenes, including both man-made and natural environments, demonstrates that our method achieves up to 1.74 dB improvement in PSNR, 6.7% in SSIM, and 41.4% in LPIPS at equivalent model sizes compared to uniform pruning baselines. For indoor scenes, our method can maintain visual quality with only 0.5\\% of the original model size. This structure-aware representation enables efficient storage, adaptive streaming, and rendering of high-fidelity 3D content across bandwidth-constrained networks and resource-limited devices.", "link": "http://arxiv.org/abs/2601.05394v2", "date": "2026-01-12", "relevancy": 3.2464, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6805}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.647}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketch%26Patch%2B%2B%3A%20Efficient%20Structure-Aware%203D%20Gaussian%20Representation&body=Title%3A%20Sketch%26Patch%2B%2B%3A%20Efficient%20Structure-Aware%203D%20Gaussian%20Representation%0AAuthor%3A%20Yuang%20Shi%20and%20G%C3%A9raldine%20Morin%20and%20Simone%20Gasparini%20and%20Wei%20Tsang%20Ooi%0AAbstract%3A%20We%20observe%20that%20Gaussians%20exhibit%20distinct%20roles%20and%20characteristics%20analogous%20to%20traditional%20artistic%20techniques%20--%20like%20how%20artists%20first%20sketch%20outlines%20before%20filling%20in%20broader%20areas%20with%20color%2C%20some%20Gaussians%20capture%20high-frequency%20features%20such%20as%20edges%20and%20contours%2C%20while%20others%20represent%20broader%2C%20smoother%20regions%20analogous%20to%20brush%20strokes%20that%20add%20volume%20and%20depth.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20hybrid%20representation%20that%20categorizes%20Gaussians%20into%20%28i%29%20Sketch%20Gaussians%2C%20which%20represent%20high-frequency%2C%20boundary-defining%20features%2C%20and%20%28ii%29%20Patch%20Gaussians%2C%20which%20cover%20low-frequency%2C%20smooth%20regions.%20This%20semantic%20separation%20naturally%20enables%20layered%20progressive%20streaming%2C%20where%20the%20compact%20Sketch%20Gaussians%20establish%20the%20structural%20skeleton%20before%20Patch%20Gaussians%20incrementally%20refine%20volumetric%20detail.%0A%20%20In%20this%20work%2C%20we%20extend%20our%20previous%20method%20to%20arbitrary%203D%20scenes%20by%20proposing%20a%20novel%20hierarchical%20adaptive%20categorization%20framework%20that%20operates%20directly%20on%20the%203DGS%20representation.%20Our%20approach%20employs%20multi-criteria%20density-based%20clustering%2C%20combined%20with%20adaptive%20quality-driven%20refinement.%20This%20method%20eliminates%20dependency%20on%20external%203D%20line%20primitives%20while%20ensuring%20optimal%20parametric%20encoding%20effectiveness.%20Our%20comprehensive%20evaluation%20across%20diverse%20scenes%2C%20including%20both%20man-made%20and%20natural%20environments%2C%20demonstrates%20that%20our%20method%20achieves%20up%20to%201.74%20dB%20improvement%20in%20PSNR%2C%206.7%25%20in%20SSIM%2C%20and%2041.4%25%20in%20LPIPS%20at%20equivalent%20model%20sizes%20compared%20to%20uniform%20pruning%20baselines.%20For%20indoor%20scenes%2C%20our%20method%20can%20maintain%20visual%20quality%20with%20only%200.5%5C%25%20of%20the%20original%20model%20size.%20This%20structure-aware%20representation%20enables%20efficient%20storage%2C%20adaptive%20streaming%2C%20and%20rendering%20of%20high-fidelity%203D%20content%20across%20bandwidth-constrained%20networks%20and%20resource-limited%20devices.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05394v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketch%2526Patch%252B%252B%253A%2520Efficient%2520Structure-Aware%25203D%2520Gaussian%2520Representation%26entry.906535625%3DYuang%2520Shi%2520and%2520G%25C3%25A9raldine%2520Morin%2520and%2520Simone%2520Gasparini%2520and%2520Wei%2520Tsang%2520Ooi%26entry.1292438233%3DWe%2520observe%2520that%2520Gaussians%2520exhibit%2520distinct%2520roles%2520and%2520characteristics%2520analogous%2520to%2520traditional%2520artistic%2520techniques%2520--%2520like%2520how%2520artists%2520first%2520sketch%2520outlines%2520before%2520filling%2520in%2520broader%2520areas%2520with%2520color%252C%2520some%2520Gaussians%2520capture%2520high-frequency%2520features%2520such%2520as%2520edges%2520and%2520contours%252C%2520while%2520others%2520represent%2520broader%252C%2520smoother%2520regions%2520analogous%2520to%2520brush%2520strokes%2520that%2520add%2520volume%2520and%2520depth.%2520Based%2520on%2520this%2520observation%252C%2520we%2520propose%2520a%2520hybrid%2520representation%2520that%2520categorizes%2520Gaussians%2520into%2520%2528i%2529%2520Sketch%2520Gaussians%252C%2520which%2520represent%2520high-frequency%252C%2520boundary-defining%2520features%252C%2520and%2520%2528ii%2529%2520Patch%2520Gaussians%252C%2520which%2520cover%2520low-frequency%252C%2520smooth%2520regions.%2520This%2520semantic%2520separation%2520naturally%2520enables%2520layered%2520progressive%2520streaming%252C%2520where%2520the%2520compact%2520Sketch%2520Gaussians%2520establish%2520the%2520structural%2520skeleton%2520before%2520Patch%2520Gaussians%2520incrementally%2520refine%2520volumetric%2520detail.%250A%2520%2520In%2520this%2520work%252C%2520we%2520extend%2520our%2520previous%2520method%2520to%2520arbitrary%25203D%2520scenes%2520by%2520proposing%2520a%2520novel%2520hierarchical%2520adaptive%2520categorization%2520framework%2520that%2520operates%2520directly%2520on%2520the%25203DGS%2520representation.%2520Our%2520approach%2520employs%2520multi-criteria%2520density-based%2520clustering%252C%2520combined%2520with%2520adaptive%2520quality-driven%2520refinement.%2520This%2520method%2520eliminates%2520dependency%2520on%2520external%25203D%2520line%2520primitives%2520while%2520ensuring%2520optimal%2520parametric%2520encoding%2520effectiveness.%2520Our%2520comprehensive%2520evaluation%2520across%2520diverse%2520scenes%252C%2520including%2520both%2520man-made%2520and%2520natural%2520environments%252C%2520demonstrates%2520that%2520our%2520method%2520achieves%2520up%2520to%25201.74%2520dB%2520improvement%2520in%2520PSNR%252C%25206.7%2525%2520in%2520SSIM%252C%2520and%252041.4%2525%2520in%2520LPIPS%2520at%2520equivalent%2520model%2520sizes%2520compared%2520to%2520uniform%2520pruning%2520baselines.%2520For%2520indoor%2520scenes%252C%2520our%2520method%2520can%2520maintain%2520visual%2520quality%2520with%2520only%25200.5%255C%2525%2520of%2520the%2520original%2520model%2520size.%2520This%2520structure-aware%2520representation%2520enables%2520efficient%2520storage%252C%2520adaptive%2520streaming%252C%2520and%2520rendering%2520of%2520high-fidelity%25203D%2520content%2520across%2520bandwidth-constrained%2520networks%2520and%2520resource-limited%2520devices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05394v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketch%26Patch%2B%2B%3A%20Efficient%20Structure-Aware%203D%20Gaussian%20Representation&entry.906535625=Yuang%20Shi%20and%20G%C3%A9raldine%20Morin%20and%20Simone%20Gasparini%20and%20Wei%20Tsang%20Ooi&entry.1292438233=We%20observe%20that%20Gaussians%20exhibit%20distinct%20roles%20and%20characteristics%20analogous%20to%20traditional%20artistic%20techniques%20--%20like%20how%20artists%20first%20sketch%20outlines%20before%20filling%20in%20broader%20areas%20with%20color%2C%20some%20Gaussians%20capture%20high-frequency%20features%20such%20as%20edges%20and%20contours%2C%20while%20others%20represent%20broader%2C%20smoother%20regions%20analogous%20to%20brush%20strokes%20that%20add%20volume%20and%20depth.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20hybrid%20representation%20that%20categorizes%20Gaussians%20into%20%28i%29%20Sketch%20Gaussians%2C%20which%20represent%20high-frequency%2C%20boundary-defining%20features%2C%20and%20%28ii%29%20Patch%20Gaussians%2C%20which%20cover%20low-frequency%2C%20smooth%20regions.%20This%20semantic%20separation%20naturally%20enables%20layered%20progressive%20streaming%2C%20where%20the%20compact%20Sketch%20Gaussians%20establish%20the%20structural%20skeleton%20before%20Patch%20Gaussians%20incrementally%20refine%20volumetric%20detail.%0A%20%20In%20this%20work%2C%20we%20extend%20our%20previous%20method%20to%20arbitrary%203D%20scenes%20by%20proposing%20a%20novel%20hierarchical%20adaptive%20categorization%20framework%20that%20operates%20directly%20on%20the%203DGS%20representation.%20Our%20approach%20employs%20multi-criteria%20density-based%20clustering%2C%20combined%20with%20adaptive%20quality-driven%20refinement.%20This%20method%20eliminates%20dependency%20on%20external%203D%20line%20primitives%20while%20ensuring%20optimal%20parametric%20encoding%20effectiveness.%20Our%20comprehensive%20evaluation%20across%20diverse%20scenes%2C%20including%20both%20man-made%20and%20natural%20environments%2C%20demonstrates%20that%20our%20method%20achieves%20up%20to%201.74%20dB%20improvement%20in%20PSNR%2C%206.7%25%20in%20SSIM%2C%20and%2041.4%25%20in%20LPIPS%20at%20equivalent%20model%20sizes%20compared%20to%20uniform%20pruning%20baselines.%20For%20indoor%20scenes%2C%20our%20method%20can%20maintain%20visual%20quality%20with%20only%200.5%5C%25%20of%20the%20original%20model%20size.%20This%20structure-aware%20representation%20enables%20efficient%20storage%2C%20adaptive%20streaming%2C%20and%20rendering%20of%20high-fidelity%203D%20content%20across%20bandwidth-constrained%20networks%20and%20resource-limited%20devices.&entry.1838667208=http%3A//arxiv.org/abs/2601.05394v2&entry.124074799=Read"},
{"title": "SuperGSeg: Open-Vocabulary 3D Segmentation with Structured Super-Gaussians", "author": "Siyun Liang and Sen Wang and Kunyi Li and Michael Niemeyer and Stefano Gasperini and Nassir Navab and Federico Tombari", "abstract": "3D Gaussian Splatting has recently gained traction for its efficient training and real-time rendering. While the vanilla Gaussian Splatting representation is mainly designed for view synthesis, more recent works investigated how to extend it with scene understanding and language features. However, existing methods lack a detailed comprehension of scenes, limiting their ability to segment and interpret complex structures. To this end, We introduce SuperGSeg, a novel approach that fosters cohesive, context-aware scene representation by disentangling segmentation and language field distillation. SuperGSeg first employs neural Gaussians to learn instance and hierarchical segmentation features from multi-view images with the aid of off-the-shelf 2D masks. These features are then leveraged to create a sparse set of what we call Super-Gaussians. Super-Gaussians facilitate the distillation of 2D language features into 3D space. Through Super-Gaussians, our method enables high-dimensional language feature rendering without extreme increases in GPU memory. Extensive experiments demonstrate that SuperGSeg outperforms prior works on both open-vocabulary object localization and semantic segmentation tasks.", "link": "http://arxiv.org/abs/2412.10231v2", "date": "2026-01-12", "relevancy": 3.2267, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6622}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6573}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperGSeg%3A%20Open-Vocabulary%203D%20Segmentation%20with%20Structured%20Super-Gaussians&body=Title%3A%20SuperGSeg%3A%20Open-Vocabulary%203D%20Segmentation%20with%20Structured%20Super-Gaussians%0AAuthor%3A%20Siyun%20Liang%20and%20Sen%20Wang%20and%20Kunyi%20Li%20and%20Michael%20Niemeyer%20and%20Stefano%20Gasperini%20and%20Nassir%20Navab%20and%20Federico%20Tombari%0AAbstract%3A%203D%20Gaussian%20Splatting%20has%20recently%20gained%20traction%20for%20its%20efficient%20training%20and%20real-time%20rendering.%20While%20the%20vanilla%20Gaussian%20Splatting%20representation%20is%20mainly%20designed%20for%20view%20synthesis%2C%20more%20recent%20works%20investigated%20how%20to%20extend%20it%20with%20scene%20understanding%20and%20language%20features.%20However%2C%20existing%20methods%20lack%20a%20detailed%20comprehension%20of%20scenes%2C%20limiting%20their%20ability%20to%20segment%20and%20interpret%20complex%20structures.%20To%20this%20end%2C%20We%20introduce%20SuperGSeg%2C%20a%20novel%20approach%20that%20fosters%20cohesive%2C%20context-aware%20scene%20representation%20by%20disentangling%20segmentation%20and%20language%20field%20distillation.%20SuperGSeg%20first%20employs%20neural%20Gaussians%20to%20learn%20instance%20and%20hierarchical%20segmentation%20features%20from%20multi-view%20images%20with%20the%20aid%20of%20off-the-shelf%202D%20masks.%20These%20features%20are%20then%20leveraged%20to%20create%20a%20sparse%20set%20of%20what%20we%20call%20Super-Gaussians.%20Super-Gaussians%20facilitate%20the%20distillation%20of%202D%20language%20features%20into%203D%20space.%20Through%20Super-Gaussians%2C%20our%20method%20enables%20high-dimensional%20language%20feature%20rendering%20without%20extreme%20increases%20in%20GPU%20memory.%20Extensive%20experiments%20demonstrate%20that%20SuperGSeg%20outperforms%20prior%20works%20on%20both%20open-vocabulary%20object%20localization%20and%20semantic%20segmentation%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2412.10231v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperGSeg%253A%2520Open-Vocabulary%25203D%2520Segmentation%2520with%2520Structured%2520Super-Gaussians%26entry.906535625%3DSiyun%2520Liang%2520and%2520Sen%2520Wang%2520and%2520Kunyi%2520Li%2520and%2520Michael%2520Niemeyer%2520and%2520Stefano%2520Gasperini%2520and%2520Nassir%2520Navab%2520and%2520Federico%2520Tombari%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520has%2520recently%2520gained%2520traction%2520for%2520its%2520efficient%2520training%2520and%2520real-time%2520rendering.%2520While%2520the%2520vanilla%2520Gaussian%2520Splatting%2520representation%2520is%2520mainly%2520designed%2520for%2520view%2520synthesis%252C%2520more%2520recent%2520works%2520investigated%2520how%2520to%2520extend%2520it%2520with%2520scene%2520understanding%2520and%2520language%2520features.%2520However%252C%2520existing%2520methods%2520lack%2520a%2520detailed%2520comprehension%2520of%2520scenes%252C%2520limiting%2520their%2520ability%2520to%2520segment%2520and%2520interpret%2520complex%2520structures.%2520To%2520this%2520end%252C%2520We%2520introduce%2520SuperGSeg%252C%2520a%2520novel%2520approach%2520that%2520fosters%2520cohesive%252C%2520context-aware%2520scene%2520representation%2520by%2520disentangling%2520segmentation%2520and%2520language%2520field%2520distillation.%2520SuperGSeg%2520first%2520employs%2520neural%2520Gaussians%2520to%2520learn%2520instance%2520and%2520hierarchical%2520segmentation%2520features%2520from%2520multi-view%2520images%2520with%2520the%2520aid%2520of%2520off-the-shelf%25202D%2520masks.%2520These%2520features%2520are%2520then%2520leveraged%2520to%2520create%2520a%2520sparse%2520set%2520of%2520what%2520we%2520call%2520Super-Gaussians.%2520Super-Gaussians%2520facilitate%2520the%2520distillation%2520of%25202D%2520language%2520features%2520into%25203D%2520space.%2520Through%2520Super-Gaussians%252C%2520our%2520method%2520enables%2520high-dimensional%2520language%2520feature%2520rendering%2520without%2520extreme%2520increases%2520in%2520GPU%2520memory.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SuperGSeg%2520outperforms%2520prior%2520works%2520on%2520both%2520open-vocabulary%2520object%2520localization%2520and%2520semantic%2520segmentation%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10231v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperGSeg%3A%20Open-Vocabulary%203D%20Segmentation%20with%20Structured%20Super-Gaussians&entry.906535625=Siyun%20Liang%20and%20Sen%20Wang%20and%20Kunyi%20Li%20and%20Michael%20Niemeyer%20and%20Stefano%20Gasperini%20and%20Nassir%20Navab%20and%20Federico%20Tombari&entry.1292438233=3D%20Gaussian%20Splatting%20has%20recently%20gained%20traction%20for%20its%20efficient%20training%20and%20real-time%20rendering.%20While%20the%20vanilla%20Gaussian%20Splatting%20representation%20is%20mainly%20designed%20for%20view%20synthesis%2C%20more%20recent%20works%20investigated%20how%20to%20extend%20it%20with%20scene%20understanding%20and%20language%20features.%20However%2C%20existing%20methods%20lack%20a%20detailed%20comprehension%20of%20scenes%2C%20limiting%20their%20ability%20to%20segment%20and%20interpret%20complex%20structures.%20To%20this%20end%2C%20We%20introduce%20SuperGSeg%2C%20a%20novel%20approach%20that%20fosters%20cohesive%2C%20context-aware%20scene%20representation%20by%20disentangling%20segmentation%20and%20language%20field%20distillation.%20SuperGSeg%20first%20employs%20neural%20Gaussians%20to%20learn%20instance%20and%20hierarchical%20segmentation%20features%20from%20multi-view%20images%20with%20the%20aid%20of%20off-the-shelf%202D%20masks.%20These%20features%20are%20then%20leveraged%20to%20create%20a%20sparse%20set%20of%20what%20we%20call%20Super-Gaussians.%20Super-Gaussians%20facilitate%20the%20distillation%20of%202D%20language%20features%20into%203D%20space.%20Through%20Super-Gaussians%2C%20our%20method%20enables%20high-dimensional%20language%20feature%20rendering%20without%20extreme%20increases%20in%20GPU%20memory.%20Extensive%20experiments%20demonstrate%20that%20SuperGSeg%20outperforms%20prior%20works%20on%20both%20open-vocabulary%20object%20localization%20and%20semantic%20segmentation%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2412.10231v2&entry.124074799=Read"},
{"title": "CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian Splatting", "author": "Siyu Jiao and Haoye Dong and Yuyang Yin and Zequn Jie and Yinlong Qian and Yao Zhao and Humphrey Shi and Yunchao Wei", "abstract": "Recent works in 3D multimodal learning have made remarkable progress. However, typically 3D multimodal models are only capable of handling point clouds. Compared to the emerging 3D representation technique, 3D Gaussian Splatting (3DGS), the spatially sparse point cloud cannot depict the texture information of 3D objects, resulting in inferior reconstruction capabilities. This limitation constrains the potential of point cloud-based 3D multimodal representation learning. In this paper, we present CLIP-GS, a novel multimodal representation learning framework grounded in 3DGS. We introduce the GS Tokenizer to generate serialized gaussian tokens, which are then processed through transformer layers pre-initialized with weights from point cloud models, resulting in the 3DGS embeddings. CLIP-GS leverages contrastive loss between 3DGS and the visual-text embeddings of CLIP, and we introduce an image voting loss to guide the directionality and convergence of gradient optimization. Furthermore, we develop an efficient way to generate triplets of 3DGS, images, and text, facilitating CLIP-GS in learning unified multimodal representations. Leveraging the well-aligned multimodal representations, CLIP-GS demonstrates versatility and outperforms point cloud-based models on various 3D tasks, including multimodal retrieval, zero-shot, and few-shot classification.", "link": "http://arxiv.org/abs/2412.19142v2", "date": "2026-01-12", "relevancy": 3.2146, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6488}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP-GS%3A%20Unifying%20Vision-Language%20Representation%20with%203D%20Gaussian%20Splatting&body=Title%3A%20CLIP-GS%3A%20Unifying%20Vision-Language%20Representation%20with%203D%20Gaussian%20Splatting%0AAuthor%3A%20Siyu%20Jiao%20and%20Haoye%20Dong%20and%20Yuyang%20Yin%20and%20Zequn%20Jie%20and%20Yinlong%20Qian%20and%20Yao%20Zhao%20and%20Humphrey%20Shi%20and%20Yunchao%20Wei%0AAbstract%3A%20Recent%20works%20in%203D%20multimodal%20learning%20have%20made%20remarkable%20progress.%20However%2C%20typically%203D%20multimodal%20models%20are%20only%20capable%20of%20handling%20point%20clouds.%20Compared%20to%20the%20emerging%203D%20representation%20technique%2C%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20the%20spatially%20sparse%20point%20cloud%20cannot%20depict%20the%20texture%20information%20of%203D%20objects%2C%20resulting%20in%20inferior%20reconstruction%20capabilities.%20This%20limitation%20constrains%20the%20potential%20of%20point%20cloud-based%203D%20multimodal%20representation%20learning.%20In%20this%20paper%2C%20we%20present%20CLIP-GS%2C%20a%20novel%20multimodal%20representation%20learning%20framework%20grounded%20in%203DGS.%20We%20introduce%20the%20GS%20Tokenizer%20to%20generate%20serialized%20gaussian%20tokens%2C%20which%20are%20then%20processed%20through%20transformer%20layers%20pre-initialized%20with%20weights%20from%20point%20cloud%20models%2C%20resulting%20in%20the%203DGS%20embeddings.%20CLIP-GS%20leverages%20contrastive%20loss%20between%203DGS%20and%20the%20visual-text%20embeddings%20of%20CLIP%2C%20and%20we%20introduce%20an%20image%20voting%20loss%20to%20guide%20the%20directionality%20and%20convergence%20of%20gradient%20optimization.%20Furthermore%2C%20we%20develop%20an%20efficient%20way%20to%20generate%20triplets%20of%203DGS%2C%20images%2C%20and%20text%2C%20facilitating%20CLIP-GS%20in%20learning%20unified%20multimodal%20representations.%20Leveraging%20the%20well-aligned%20multimodal%20representations%2C%20CLIP-GS%20demonstrates%20versatility%20and%20outperforms%20point%20cloud-based%20models%20on%20various%203D%20tasks%2C%20including%20multimodal%20retrieval%2C%20zero-shot%2C%20and%20few-shot%20classification.%0ALink%3A%20http%3A//arxiv.org/abs/2412.19142v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP-GS%253A%2520Unifying%2520Vision-Language%2520Representation%2520with%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DSiyu%2520Jiao%2520and%2520Haoye%2520Dong%2520and%2520Yuyang%2520Yin%2520and%2520Zequn%2520Jie%2520and%2520Yinlong%2520Qian%2520and%2520Yao%2520Zhao%2520and%2520Humphrey%2520Shi%2520and%2520Yunchao%2520Wei%26entry.1292438233%3DRecent%2520works%2520in%25203D%2520multimodal%2520learning%2520have%2520made%2520remarkable%2520progress.%2520However%252C%2520typically%25203D%2520multimodal%2520models%2520are%2520only%2520capable%2520of%2520handling%2520point%2520clouds.%2520Compared%2520to%2520the%2520emerging%25203D%2520representation%2520technique%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520the%2520spatially%2520sparse%2520point%2520cloud%2520cannot%2520depict%2520the%2520texture%2520information%2520of%25203D%2520objects%252C%2520resulting%2520in%2520inferior%2520reconstruction%2520capabilities.%2520This%2520limitation%2520constrains%2520the%2520potential%2520of%2520point%2520cloud-based%25203D%2520multimodal%2520representation%2520learning.%2520In%2520this%2520paper%252C%2520we%2520present%2520CLIP-GS%252C%2520a%2520novel%2520multimodal%2520representation%2520learning%2520framework%2520grounded%2520in%25203DGS.%2520We%2520introduce%2520the%2520GS%2520Tokenizer%2520to%2520generate%2520serialized%2520gaussian%2520tokens%252C%2520which%2520are%2520then%2520processed%2520through%2520transformer%2520layers%2520pre-initialized%2520with%2520weights%2520from%2520point%2520cloud%2520models%252C%2520resulting%2520in%2520the%25203DGS%2520embeddings.%2520CLIP-GS%2520leverages%2520contrastive%2520loss%2520between%25203DGS%2520and%2520the%2520visual-text%2520embeddings%2520of%2520CLIP%252C%2520and%2520we%2520introduce%2520an%2520image%2520voting%2520loss%2520to%2520guide%2520the%2520directionality%2520and%2520convergence%2520of%2520gradient%2520optimization.%2520Furthermore%252C%2520we%2520develop%2520an%2520efficient%2520way%2520to%2520generate%2520triplets%2520of%25203DGS%252C%2520images%252C%2520and%2520text%252C%2520facilitating%2520CLIP-GS%2520in%2520learning%2520unified%2520multimodal%2520representations.%2520Leveraging%2520the%2520well-aligned%2520multimodal%2520representations%252C%2520CLIP-GS%2520demonstrates%2520versatility%2520and%2520outperforms%2520point%2520cloud-based%2520models%2520on%2520various%25203D%2520tasks%252C%2520including%2520multimodal%2520retrieval%252C%2520zero-shot%252C%2520and%2520few-shot%2520classification.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19142v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-GS%3A%20Unifying%20Vision-Language%20Representation%20with%203D%20Gaussian%20Splatting&entry.906535625=Siyu%20Jiao%20and%20Haoye%20Dong%20and%20Yuyang%20Yin%20and%20Zequn%20Jie%20and%20Yinlong%20Qian%20and%20Yao%20Zhao%20and%20Humphrey%20Shi%20and%20Yunchao%20Wei&entry.1292438233=Recent%20works%20in%203D%20multimodal%20learning%20have%20made%20remarkable%20progress.%20However%2C%20typically%203D%20multimodal%20models%20are%20only%20capable%20of%20handling%20point%20clouds.%20Compared%20to%20the%20emerging%203D%20representation%20technique%2C%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20the%20spatially%20sparse%20point%20cloud%20cannot%20depict%20the%20texture%20information%20of%203D%20objects%2C%20resulting%20in%20inferior%20reconstruction%20capabilities.%20This%20limitation%20constrains%20the%20potential%20of%20point%20cloud-based%203D%20multimodal%20representation%20learning.%20In%20this%20paper%2C%20we%20present%20CLIP-GS%2C%20a%20novel%20multimodal%20representation%20learning%20framework%20grounded%20in%203DGS.%20We%20introduce%20the%20GS%20Tokenizer%20to%20generate%20serialized%20gaussian%20tokens%2C%20which%20are%20then%20processed%20through%20transformer%20layers%20pre-initialized%20with%20weights%20from%20point%20cloud%20models%2C%20resulting%20in%20the%203DGS%20embeddings.%20CLIP-GS%20leverages%20contrastive%20loss%20between%203DGS%20and%20the%20visual-text%20embeddings%20of%20CLIP%2C%20and%20we%20introduce%20an%20image%20voting%20loss%20to%20guide%20the%20directionality%20and%20convergence%20of%20gradient%20optimization.%20Furthermore%2C%20we%20develop%20an%20efficient%20way%20to%20generate%20triplets%20of%203DGS%2C%20images%2C%20and%20text%2C%20facilitating%20CLIP-GS%20in%20learning%20unified%20multimodal%20representations.%20Leveraging%20the%20well-aligned%20multimodal%20representations%2C%20CLIP-GS%20demonstrates%20versatility%20and%20outperforms%20point%20cloud-based%20models%20on%20various%203D%20tasks%2C%20including%20multimodal%20retrieval%2C%20zero-shot%2C%20and%20few-shot%20classification.&entry.1838667208=http%3A//arxiv.org/abs/2412.19142v2&entry.124074799=Read"},
{"title": "UIKA: Fast Universal Head Avatar from Pose-Free Images", "author": "Zijian Wu and Boyao Zhou and Liangxiao Hu and Hongyu Liu and Yuan Sun and Xuan Wang and Xun Cao and Yujun Shen and Hao Zhu", "abstract": "We present UIKA, a feed-forward animatable Gaussian head model from an arbitrary number of unposed inputs, including a single image, multi-view captures, and smartphone-captured videos. Unlike the traditional avatar method, which requires a studio-level multi-view capture system and reconstructs a human-specific model through a long-time optimization process, we rethink the task through the lenses of model representation, network design, and data preparation. First, we introduce a UV-guided avatar modeling strategy, in which each input image is associated with a pixel-wise facial correspondence estimation. Such correspondence estimation allows us to reproject each valid pixel color from screen space to UV space, which is independent of camera pose and character expression. Furthermore, we design learnable UV tokens on which the attention mechanism can be applied at both the screen and UV levels. The learned UV tokens can be decoded into canonical Gaussian attributes using aggregated UV information from all input views. To train our large avatar model, we additionally prepare a large-scale, identity-rich synthetic training dataset. Our method significantly outperforms existing approaches in both monocular and multi-view settings. Project page: https://zijian-wu.github.io/uika-page/", "link": "http://arxiv.org/abs/2601.07603v1", "date": "2026-01-12", "relevancy": 3.2007, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6597}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6597}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UIKA%3A%20Fast%20Universal%20Head%20Avatar%20from%20Pose-Free%20Images&body=Title%3A%20UIKA%3A%20Fast%20Universal%20Head%20Avatar%20from%20Pose-Free%20Images%0AAuthor%3A%20Zijian%20Wu%20and%20Boyao%20Zhou%20and%20Liangxiao%20Hu%20and%20Hongyu%20Liu%20and%20Yuan%20Sun%20and%20Xuan%20Wang%20and%20Xun%20Cao%20and%20Yujun%20Shen%20and%20Hao%20Zhu%0AAbstract%3A%20We%20present%20UIKA%2C%20a%20feed-forward%20animatable%20Gaussian%20head%20model%20from%20an%20arbitrary%20number%20of%20unposed%20inputs%2C%20including%20a%20single%20image%2C%20multi-view%20captures%2C%20and%20smartphone-captured%20videos.%20Unlike%20the%20traditional%20avatar%20method%2C%20which%20requires%20a%20studio-level%20multi-view%20capture%20system%20and%20reconstructs%20a%20human-specific%20model%20through%20a%20long-time%20optimization%20process%2C%20we%20rethink%20the%20task%20through%20the%20lenses%20of%20model%20representation%2C%20network%20design%2C%20and%20data%20preparation.%20First%2C%20we%20introduce%20a%20UV-guided%20avatar%20modeling%20strategy%2C%20in%20which%20each%20input%20image%20is%20associated%20with%20a%20pixel-wise%20facial%20correspondence%20estimation.%20Such%20correspondence%20estimation%20allows%20us%20to%20reproject%20each%20valid%20pixel%20color%20from%20screen%20space%20to%20UV%20space%2C%20which%20is%20independent%20of%20camera%20pose%20and%20character%20expression.%20Furthermore%2C%20we%20design%20learnable%20UV%20tokens%20on%20which%20the%20attention%20mechanism%20can%20be%20applied%20at%20both%20the%20screen%20and%20UV%20levels.%20The%20learned%20UV%20tokens%20can%20be%20decoded%20into%20canonical%20Gaussian%20attributes%20using%20aggregated%20UV%20information%20from%20all%20input%20views.%20To%20train%20our%20large%20avatar%20model%2C%20we%20additionally%20prepare%20a%20large-scale%2C%20identity-rich%20synthetic%20training%20dataset.%20Our%20method%20significantly%20outperforms%20existing%20approaches%20in%20both%20monocular%20and%20multi-view%20settings.%20Project%20page%3A%20https%3A//zijian-wu.github.io/uika-page/%0ALink%3A%20http%3A//arxiv.org/abs/2601.07603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUIKA%253A%2520Fast%2520Universal%2520Head%2520Avatar%2520from%2520Pose-Free%2520Images%26entry.906535625%3DZijian%2520Wu%2520and%2520Boyao%2520Zhou%2520and%2520Liangxiao%2520Hu%2520and%2520Hongyu%2520Liu%2520and%2520Yuan%2520Sun%2520and%2520Xuan%2520Wang%2520and%2520Xun%2520Cao%2520and%2520Yujun%2520Shen%2520and%2520Hao%2520Zhu%26entry.1292438233%3DWe%2520present%2520UIKA%252C%2520a%2520feed-forward%2520animatable%2520Gaussian%2520head%2520model%2520from%2520an%2520arbitrary%2520number%2520of%2520unposed%2520inputs%252C%2520including%2520a%2520single%2520image%252C%2520multi-view%2520captures%252C%2520and%2520smartphone-captured%2520videos.%2520Unlike%2520the%2520traditional%2520avatar%2520method%252C%2520which%2520requires%2520a%2520studio-level%2520multi-view%2520capture%2520system%2520and%2520reconstructs%2520a%2520human-specific%2520model%2520through%2520a%2520long-time%2520optimization%2520process%252C%2520we%2520rethink%2520the%2520task%2520through%2520the%2520lenses%2520of%2520model%2520representation%252C%2520network%2520design%252C%2520and%2520data%2520preparation.%2520First%252C%2520we%2520introduce%2520a%2520UV-guided%2520avatar%2520modeling%2520strategy%252C%2520in%2520which%2520each%2520input%2520image%2520is%2520associated%2520with%2520a%2520pixel-wise%2520facial%2520correspondence%2520estimation.%2520Such%2520correspondence%2520estimation%2520allows%2520us%2520to%2520reproject%2520each%2520valid%2520pixel%2520color%2520from%2520screen%2520space%2520to%2520UV%2520space%252C%2520which%2520is%2520independent%2520of%2520camera%2520pose%2520and%2520character%2520expression.%2520Furthermore%252C%2520we%2520design%2520learnable%2520UV%2520tokens%2520on%2520which%2520the%2520attention%2520mechanism%2520can%2520be%2520applied%2520at%2520both%2520the%2520screen%2520and%2520UV%2520levels.%2520The%2520learned%2520UV%2520tokens%2520can%2520be%2520decoded%2520into%2520canonical%2520Gaussian%2520attributes%2520using%2520aggregated%2520UV%2520information%2520from%2520all%2520input%2520views.%2520To%2520train%2520our%2520large%2520avatar%2520model%252C%2520we%2520additionally%2520prepare%2520a%2520large-scale%252C%2520identity-rich%2520synthetic%2520training%2520dataset.%2520Our%2520method%2520significantly%2520outperforms%2520existing%2520approaches%2520in%2520both%2520monocular%2520and%2520multi-view%2520settings.%2520Project%2520page%253A%2520https%253A//zijian-wu.github.io/uika-page/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UIKA%3A%20Fast%20Universal%20Head%20Avatar%20from%20Pose-Free%20Images&entry.906535625=Zijian%20Wu%20and%20Boyao%20Zhou%20and%20Liangxiao%20Hu%20and%20Hongyu%20Liu%20and%20Yuan%20Sun%20and%20Xuan%20Wang%20and%20Xun%20Cao%20and%20Yujun%20Shen%20and%20Hao%20Zhu&entry.1292438233=We%20present%20UIKA%2C%20a%20feed-forward%20animatable%20Gaussian%20head%20model%20from%20an%20arbitrary%20number%20of%20unposed%20inputs%2C%20including%20a%20single%20image%2C%20multi-view%20captures%2C%20and%20smartphone-captured%20videos.%20Unlike%20the%20traditional%20avatar%20method%2C%20which%20requires%20a%20studio-level%20multi-view%20capture%20system%20and%20reconstructs%20a%20human-specific%20model%20through%20a%20long-time%20optimization%20process%2C%20we%20rethink%20the%20task%20through%20the%20lenses%20of%20model%20representation%2C%20network%20design%2C%20and%20data%20preparation.%20First%2C%20we%20introduce%20a%20UV-guided%20avatar%20modeling%20strategy%2C%20in%20which%20each%20input%20image%20is%20associated%20with%20a%20pixel-wise%20facial%20correspondence%20estimation.%20Such%20correspondence%20estimation%20allows%20us%20to%20reproject%20each%20valid%20pixel%20color%20from%20screen%20space%20to%20UV%20space%2C%20which%20is%20independent%20of%20camera%20pose%20and%20character%20expression.%20Furthermore%2C%20we%20design%20learnable%20UV%20tokens%20on%20which%20the%20attention%20mechanism%20can%20be%20applied%20at%20both%20the%20screen%20and%20UV%20levels.%20The%20learned%20UV%20tokens%20can%20be%20decoded%20into%20canonical%20Gaussian%20attributes%20using%20aggregated%20UV%20information%20from%20all%20input%20views.%20To%20train%20our%20large%20avatar%20model%2C%20we%20additionally%20prepare%20a%20large-scale%2C%20identity-rich%20synthetic%20training%20dataset.%20Our%20method%20significantly%20outperforms%20existing%20approaches%20in%20both%20monocular%20and%20multi-view%20settings.%20Project%20page%3A%20https%3A//zijian-wu.github.io/uika-page/&entry.1838667208=http%3A//arxiv.org/abs/2601.07603v1&entry.124074799=Read"},
{"title": "Video Evidence to Reasoning Efficient Video Understanding via Explicit Evidence Grounding", "author": "Yanxiang Huang and Guohua Gao and Zhaoyang Wei and Jianyuan Ni", "abstract": "Large Vision-Language Models (LVLMs) face a fundamental dilemma in video reasoning: they are caught between the prohibitive computational costs of verbose reasoning and the hallucination risks of efficient, ungrounded approaches. To resolve this, we introduce the Chain of Evidence (CoE), a novel framework that architecturally decouples and co-optimizes perceptual grounding and reasoning efficiency. CoE incorporates two core innovations: (1) A lightweight Evidence Grounding Module (EGM) that acts as a query-guided filter, dynamically identifying and extracting a compact set of high-fidelity visual evidence; and (2) An Evidence-Anchoring Protocol optimized via Reinforcement Learning. Crucially, we design a composite reward mechanism that enforces process alignment, compelling the model to strictly reference identified temporal anchors during deduction, thereby mitigating hallucinations. To enable this, we construct CoE-Instruct, a large-scale dataset (164k samples) featuring a novel dual-annotation schema for separate perception and reasoning supervision. Extensive experiments on five benchmarks, including Video-MME, MVBench, and VSI-Bench, demonstrate that CoE-enhanced models establish a new state-of-the-art. They significantly outperform existing methods in accuracy, proving CoE to be a powerful and practical paradigm for reliable video understanding.", "link": "http://arxiv.org/abs/2601.07761v1", "date": "2026-01-12", "relevancy": 3.0235, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6151}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6151}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Evidence%20to%20Reasoning%20Efficient%20Video%20Understanding%20via%20Explicit%20Evidence%20Grounding&body=Title%3A%20Video%20Evidence%20to%20Reasoning%20Efficient%20Video%20Understanding%20via%20Explicit%20Evidence%20Grounding%0AAuthor%3A%20Yanxiang%20Huang%20and%20Guohua%20Gao%20and%20Zhaoyang%20Wei%20and%20Jianyuan%20Ni%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28LVLMs%29%20face%20a%20fundamental%20dilemma%20in%20video%20reasoning%3A%20they%20are%20caught%20between%20the%20prohibitive%20computational%20costs%20of%20verbose%20reasoning%20and%20the%20hallucination%20risks%20of%20efficient%2C%20ungrounded%20approaches.%20To%20resolve%20this%2C%20we%20introduce%20the%20Chain%20of%20Evidence%20%28CoE%29%2C%20a%20novel%20framework%20that%20architecturally%20decouples%20and%20co-optimizes%20perceptual%20grounding%20and%20reasoning%20efficiency.%20CoE%20incorporates%20two%20core%20innovations%3A%20%281%29%20A%20lightweight%20Evidence%20Grounding%20Module%20%28EGM%29%20that%20acts%20as%20a%20query-guided%20filter%2C%20dynamically%20identifying%20and%20extracting%20a%20compact%20set%20of%20high-fidelity%20visual%20evidence%3B%20and%20%282%29%20An%20Evidence-Anchoring%20Protocol%20optimized%20via%20Reinforcement%20Learning.%20Crucially%2C%20we%20design%20a%20composite%20reward%20mechanism%20that%20enforces%20process%20alignment%2C%20compelling%20the%20model%20to%20strictly%20reference%20identified%20temporal%20anchors%20during%20deduction%2C%20thereby%20mitigating%20hallucinations.%20To%20enable%20this%2C%20we%20construct%20CoE-Instruct%2C%20a%20large-scale%20dataset%20%28164k%20samples%29%20featuring%20a%20novel%20dual-annotation%20schema%20for%20separate%20perception%20and%20reasoning%20supervision.%20Extensive%20experiments%20on%20five%20benchmarks%2C%20including%20Video-MME%2C%20MVBench%2C%20and%20VSI-Bench%2C%20demonstrate%20that%20CoE-enhanced%20models%20establish%20a%20new%20state-of-the-art.%20They%20significantly%20outperform%20existing%20methods%20in%20accuracy%2C%20proving%20CoE%20to%20be%20a%20powerful%20and%20practical%20paradigm%20for%20reliable%20video%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Evidence%2520to%2520Reasoning%2520Efficient%2520Video%2520Understanding%2520via%2520Explicit%2520Evidence%2520Grounding%26entry.906535625%3DYanxiang%2520Huang%2520and%2520Guohua%2520Gao%2520and%2520Zhaoyang%2520Wei%2520and%2520Jianyuan%2520Ni%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520face%2520a%2520fundamental%2520dilemma%2520in%2520video%2520reasoning%253A%2520they%2520are%2520caught%2520between%2520the%2520prohibitive%2520computational%2520costs%2520of%2520verbose%2520reasoning%2520and%2520the%2520hallucination%2520risks%2520of%2520efficient%252C%2520ungrounded%2520approaches.%2520To%2520resolve%2520this%252C%2520we%2520introduce%2520the%2520Chain%2520of%2520Evidence%2520%2528CoE%2529%252C%2520a%2520novel%2520framework%2520that%2520architecturally%2520decouples%2520and%2520co-optimizes%2520perceptual%2520grounding%2520and%2520reasoning%2520efficiency.%2520CoE%2520incorporates%2520two%2520core%2520innovations%253A%2520%25281%2529%2520A%2520lightweight%2520Evidence%2520Grounding%2520Module%2520%2528EGM%2529%2520that%2520acts%2520as%2520a%2520query-guided%2520filter%252C%2520dynamically%2520identifying%2520and%2520extracting%2520a%2520compact%2520set%2520of%2520high-fidelity%2520visual%2520evidence%253B%2520and%2520%25282%2529%2520An%2520Evidence-Anchoring%2520Protocol%2520optimized%2520via%2520Reinforcement%2520Learning.%2520Crucially%252C%2520we%2520design%2520a%2520composite%2520reward%2520mechanism%2520that%2520enforces%2520process%2520alignment%252C%2520compelling%2520the%2520model%2520to%2520strictly%2520reference%2520identified%2520temporal%2520anchors%2520during%2520deduction%252C%2520thereby%2520mitigating%2520hallucinations.%2520To%2520enable%2520this%252C%2520we%2520construct%2520CoE-Instruct%252C%2520a%2520large-scale%2520dataset%2520%2528164k%2520samples%2529%2520featuring%2520a%2520novel%2520dual-annotation%2520schema%2520for%2520separate%2520perception%2520and%2520reasoning%2520supervision.%2520Extensive%2520experiments%2520on%2520five%2520benchmarks%252C%2520including%2520Video-MME%252C%2520MVBench%252C%2520and%2520VSI-Bench%252C%2520demonstrate%2520that%2520CoE-enhanced%2520models%2520establish%2520a%2520new%2520state-of-the-art.%2520They%2520significantly%2520outperform%2520existing%2520methods%2520in%2520accuracy%252C%2520proving%2520CoE%2520to%2520be%2520a%2520powerful%2520and%2520practical%2520paradigm%2520for%2520reliable%2520video%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Evidence%20to%20Reasoning%20Efficient%20Video%20Understanding%20via%20Explicit%20Evidence%20Grounding&entry.906535625=Yanxiang%20Huang%20and%20Guohua%20Gao%20and%20Zhaoyang%20Wei%20and%20Jianyuan%20Ni&entry.1292438233=Large%20Vision-Language%20Models%20%28LVLMs%29%20face%20a%20fundamental%20dilemma%20in%20video%20reasoning%3A%20they%20are%20caught%20between%20the%20prohibitive%20computational%20costs%20of%20verbose%20reasoning%20and%20the%20hallucination%20risks%20of%20efficient%2C%20ungrounded%20approaches.%20To%20resolve%20this%2C%20we%20introduce%20the%20Chain%20of%20Evidence%20%28CoE%29%2C%20a%20novel%20framework%20that%20architecturally%20decouples%20and%20co-optimizes%20perceptual%20grounding%20and%20reasoning%20efficiency.%20CoE%20incorporates%20two%20core%20innovations%3A%20%281%29%20A%20lightweight%20Evidence%20Grounding%20Module%20%28EGM%29%20that%20acts%20as%20a%20query-guided%20filter%2C%20dynamically%20identifying%20and%20extracting%20a%20compact%20set%20of%20high-fidelity%20visual%20evidence%3B%20and%20%282%29%20An%20Evidence-Anchoring%20Protocol%20optimized%20via%20Reinforcement%20Learning.%20Crucially%2C%20we%20design%20a%20composite%20reward%20mechanism%20that%20enforces%20process%20alignment%2C%20compelling%20the%20model%20to%20strictly%20reference%20identified%20temporal%20anchors%20during%20deduction%2C%20thereby%20mitigating%20hallucinations.%20To%20enable%20this%2C%20we%20construct%20CoE-Instruct%2C%20a%20large-scale%20dataset%20%28164k%20samples%29%20featuring%20a%20novel%20dual-annotation%20schema%20for%20separate%20perception%20and%20reasoning%20supervision.%20Extensive%20experiments%20on%20five%20benchmarks%2C%20including%20Video-MME%2C%20MVBench%2C%20and%20VSI-Bench%2C%20demonstrate%20that%20CoE-enhanced%20models%20establish%20a%20new%20state-of-the-art.%20They%20significantly%20outperform%20existing%20methods%20in%20accuracy%2C%20proving%20CoE%20to%20be%20a%20powerful%20and%20practical%20paradigm%20for%20reliable%20video%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2601.07761v1&entry.124074799=Read"},
{"title": "StdGEN++: A Comprehensive System for Semantic-Decomposed 3D Character Generation", "author": "Yuze He and Yanning Zhou and Wang Zhao and Jingwen Ye and Zhongkai Wu and Ran Yi and Yong-Jin Liu", "abstract": "We present StdGEN++, a novel and comprehensive system for generating high-fidelity, semantically decomposed 3D characters from diverse inputs. Existing 3D generative methods often produce monolithic meshes that lack the structural flexibility required by industrial pipelines in gaming and animation. Addressing this gap, StdGEN++ is built upon a Dual-branch Semantic-aware Large Reconstruction Model (Dual-Branch S-LRM), which jointly reconstructs geometry, color, and per-component semantics in a feed-forward manner. To achieve production-level fidelity, we introduce a novel semantic surface extraction formalism compatible with hybrid implicit fields. This mechanism is accelerated by a coarse-to-fine proposal scheme, which significantly reduces memory footprint and enables high-resolution mesh generation. Furthermore, we propose a video-diffusion-based texture decomposition module that disentangles appearance into editable layers (e.g., separated iris and skin), resolving semantic confusion in facial regions. Experiments demonstrate that StdGEN++ achieves state-of-the-art performance, significantly outperforming existing methods in geometric accuracy and semantic disentanglement. Crucially, the resulting structural independence unlocks advanced downstream capabilities, including non-destructive editing, physics-compliant animation, and gaze tracking, making it a robust solution for automated character asset production.", "link": "http://arxiv.org/abs/2601.07660v1", "date": "2026-01-12", "relevancy": 3.0046, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6046}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.602}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StdGEN%2B%2B%3A%20A%20Comprehensive%20System%20for%20Semantic-Decomposed%203D%20Character%20Generation&body=Title%3A%20StdGEN%2B%2B%3A%20A%20Comprehensive%20System%20for%20Semantic-Decomposed%203D%20Character%20Generation%0AAuthor%3A%20Yuze%20He%20and%20Yanning%20Zhou%20and%20Wang%20Zhao%20and%20Jingwen%20Ye%20and%20Zhongkai%20Wu%20and%20Ran%20Yi%20and%20Yong-Jin%20Liu%0AAbstract%3A%20We%20present%20StdGEN%2B%2B%2C%20a%20novel%20and%20comprehensive%20system%20for%20generating%20high-fidelity%2C%20semantically%20decomposed%203D%20characters%20from%20diverse%20inputs.%20Existing%203D%20generative%20methods%20often%20produce%20monolithic%20meshes%20that%20lack%20the%20structural%20flexibility%20required%20by%20industrial%20pipelines%20in%20gaming%20and%20animation.%20Addressing%20this%20gap%2C%20StdGEN%2B%2B%20is%20built%20upon%20a%20Dual-branch%20Semantic-aware%20Large%20Reconstruction%20Model%20%28Dual-Branch%20S-LRM%29%2C%20which%20jointly%20reconstructs%20geometry%2C%20color%2C%20and%20per-component%20semantics%20in%20a%20feed-forward%20manner.%20To%20achieve%20production-level%20fidelity%2C%20we%20introduce%20a%20novel%20semantic%20surface%20extraction%20formalism%20compatible%20with%20hybrid%20implicit%20fields.%20This%20mechanism%20is%20accelerated%20by%20a%20coarse-to-fine%20proposal%20scheme%2C%20which%20significantly%20reduces%20memory%20footprint%20and%20enables%20high-resolution%20mesh%20generation.%20Furthermore%2C%20we%20propose%20a%20video-diffusion-based%20texture%20decomposition%20module%20that%20disentangles%20appearance%20into%20editable%20layers%20%28e.g.%2C%20separated%20iris%20and%20skin%29%2C%20resolving%20semantic%20confusion%20in%20facial%20regions.%20Experiments%20demonstrate%20that%20StdGEN%2B%2B%20achieves%20state-of-the-art%20performance%2C%20significantly%20outperforming%20existing%20methods%20in%20geometric%20accuracy%20and%20semantic%20disentanglement.%20Crucially%2C%20the%20resulting%20structural%20independence%20unlocks%20advanced%20downstream%20capabilities%2C%20including%20non-destructive%20editing%2C%20physics-compliant%20animation%2C%20and%20gaze%20tracking%2C%20making%20it%20a%20robust%20solution%20for%20automated%20character%20asset%20production.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStdGEN%252B%252B%253A%2520A%2520Comprehensive%2520System%2520for%2520Semantic-Decomposed%25203D%2520Character%2520Generation%26entry.906535625%3DYuze%2520He%2520and%2520Yanning%2520Zhou%2520and%2520Wang%2520Zhao%2520and%2520Jingwen%2520Ye%2520and%2520Zhongkai%2520Wu%2520and%2520Ran%2520Yi%2520and%2520Yong-Jin%2520Liu%26entry.1292438233%3DWe%2520present%2520StdGEN%252B%252B%252C%2520a%2520novel%2520and%2520comprehensive%2520system%2520for%2520generating%2520high-fidelity%252C%2520semantically%2520decomposed%25203D%2520characters%2520from%2520diverse%2520inputs.%2520Existing%25203D%2520generative%2520methods%2520often%2520produce%2520monolithic%2520meshes%2520that%2520lack%2520the%2520structural%2520flexibility%2520required%2520by%2520industrial%2520pipelines%2520in%2520gaming%2520and%2520animation.%2520Addressing%2520this%2520gap%252C%2520StdGEN%252B%252B%2520is%2520built%2520upon%2520a%2520Dual-branch%2520Semantic-aware%2520Large%2520Reconstruction%2520Model%2520%2528Dual-Branch%2520S-LRM%2529%252C%2520which%2520jointly%2520reconstructs%2520geometry%252C%2520color%252C%2520and%2520per-component%2520semantics%2520in%2520a%2520feed-forward%2520manner.%2520To%2520achieve%2520production-level%2520fidelity%252C%2520we%2520introduce%2520a%2520novel%2520semantic%2520surface%2520extraction%2520formalism%2520compatible%2520with%2520hybrid%2520implicit%2520fields.%2520This%2520mechanism%2520is%2520accelerated%2520by%2520a%2520coarse-to-fine%2520proposal%2520scheme%252C%2520which%2520significantly%2520reduces%2520memory%2520footprint%2520and%2520enables%2520high-resolution%2520mesh%2520generation.%2520Furthermore%252C%2520we%2520propose%2520a%2520video-diffusion-based%2520texture%2520decomposition%2520module%2520that%2520disentangles%2520appearance%2520into%2520editable%2520layers%2520%2528e.g.%252C%2520separated%2520iris%2520and%2520skin%2529%252C%2520resolving%2520semantic%2520confusion%2520in%2520facial%2520regions.%2520Experiments%2520demonstrate%2520that%2520StdGEN%252B%252B%2520achieves%2520state-of-the-art%2520performance%252C%2520significantly%2520outperforming%2520existing%2520methods%2520in%2520geometric%2520accuracy%2520and%2520semantic%2520disentanglement.%2520Crucially%252C%2520the%2520resulting%2520structural%2520independence%2520unlocks%2520advanced%2520downstream%2520capabilities%252C%2520including%2520non-destructive%2520editing%252C%2520physics-compliant%2520animation%252C%2520and%2520gaze%2520tracking%252C%2520making%2520it%2520a%2520robust%2520solution%2520for%2520automated%2520character%2520asset%2520production.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StdGEN%2B%2B%3A%20A%20Comprehensive%20System%20for%20Semantic-Decomposed%203D%20Character%20Generation&entry.906535625=Yuze%20He%20and%20Yanning%20Zhou%20and%20Wang%20Zhao%20and%20Jingwen%20Ye%20and%20Zhongkai%20Wu%20and%20Ran%20Yi%20and%20Yong-Jin%20Liu&entry.1292438233=We%20present%20StdGEN%2B%2B%2C%20a%20novel%20and%20comprehensive%20system%20for%20generating%20high-fidelity%2C%20semantically%20decomposed%203D%20characters%20from%20diverse%20inputs.%20Existing%203D%20generative%20methods%20often%20produce%20monolithic%20meshes%20that%20lack%20the%20structural%20flexibility%20required%20by%20industrial%20pipelines%20in%20gaming%20and%20animation.%20Addressing%20this%20gap%2C%20StdGEN%2B%2B%20is%20built%20upon%20a%20Dual-branch%20Semantic-aware%20Large%20Reconstruction%20Model%20%28Dual-Branch%20S-LRM%29%2C%20which%20jointly%20reconstructs%20geometry%2C%20color%2C%20and%20per-component%20semantics%20in%20a%20feed-forward%20manner.%20To%20achieve%20production-level%20fidelity%2C%20we%20introduce%20a%20novel%20semantic%20surface%20extraction%20formalism%20compatible%20with%20hybrid%20implicit%20fields.%20This%20mechanism%20is%20accelerated%20by%20a%20coarse-to-fine%20proposal%20scheme%2C%20which%20significantly%20reduces%20memory%20footprint%20and%20enables%20high-resolution%20mesh%20generation.%20Furthermore%2C%20we%20propose%20a%20video-diffusion-based%20texture%20decomposition%20module%20that%20disentangles%20appearance%20into%20editable%20layers%20%28e.g.%2C%20separated%20iris%20and%20skin%29%2C%20resolving%20semantic%20confusion%20in%20facial%20regions.%20Experiments%20demonstrate%20that%20StdGEN%2B%2B%20achieves%20state-of-the-art%20performance%2C%20significantly%20outperforming%20existing%20methods%20in%20geometric%20accuracy%20and%20semantic%20disentanglement.%20Crucially%2C%20the%20resulting%20structural%20independence%20unlocks%20advanced%20downstream%20capabilities%2C%20including%20non-destructive%20editing%2C%20physics-compliant%20animation%2C%20and%20gaze%20tracking%2C%20making%20it%20a%20robust%20solution%20for%20automated%20character%20asset%20production.&entry.1838667208=http%3A//arxiv.org/abs/2601.07660v1&entry.124074799=Read"},
{"title": "Evaluating the encoding competence of visual language models using uncommon actions", "author": "Chen Ling and Nai Ding", "abstract": "We propose UAIT (Uncommon-sense Action Image-Text) dataset, a new evaluation benchmark designed to test the semantic understanding ability of visual language models (VLMs) in uncommon-sense action scenes. Unlike previous datasets that focus on common visual scenes with statistical frequency advantages, UAIT challenges models with grammatically reasonable but semantically counter-common sense image-text pairs. Such tasks require models to go beyond superficial pattern recognition and demonstrate a deep understanding of agent-patient relationships and physical feasibility. To build UAIT, we designed a semi-automated process to synthesize high-quality uncommon-sense image-text samples using large language models, few-shot prompt engineering, and text-to-image generation. Each sample is accompanied by a carefully designed multiple-choice question to test the model's competence in fine-grained reasoning. We evaluate multiple state-of-the-art visual language models and compare them with models based on contrastive learning. Experiments show that all models perform significantly worse than humans in semantic judgment, especially in distinguishing grammatical correctness from semantic rationality. Further experiments show that even the lightweight model can improve its accuracy after fine-tuning, demonstrating the great potential of directional adaptation. This study not only reveals the key weaknesses of VLMs, but also provides diagnostic tools and research directions for the development of robust models with real visual semantic reasoning capabilities.", "link": "http://arxiv.org/abs/2601.07737v1", "date": "2026-01-12", "relevancy": 2.9887, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.614}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20encoding%20competence%20of%20visual%20language%20models%20using%20uncommon%20actions&body=Title%3A%20Evaluating%20the%20encoding%20competence%20of%20visual%20language%20models%20using%20uncommon%20actions%0AAuthor%3A%20Chen%20Ling%20and%20Nai%20Ding%0AAbstract%3A%20We%20propose%20UAIT%20%28Uncommon-sense%20Action%20Image-Text%29%20dataset%2C%20a%20new%20evaluation%20benchmark%20designed%20to%20test%20the%20semantic%20understanding%20ability%20of%20visual%20language%20models%20%28VLMs%29%20in%20uncommon-sense%20action%20scenes.%20Unlike%20previous%20datasets%20that%20focus%20on%20common%20visual%20scenes%20with%20statistical%20frequency%20advantages%2C%20UAIT%20challenges%20models%20with%20grammatically%20reasonable%20but%20semantically%20counter-common%20sense%20image-text%20pairs.%20Such%20tasks%20require%20models%20to%20go%20beyond%20superficial%20pattern%20recognition%20and%20demonstrate%20a%20deep%20understanding%20of%20agent-patient%20relationships%20and%20physical%20feasibility.%20To%20build%20UAIT%2C%20we%20designed%20a%20semi-automated%20process%20to%20synthesize%20high-quality%20uncommon-sense%20image-text%20samples%20using%20large%20language%20models%2C%20few-shot%20prompt%20engineering%2C%20and%20text-to-image%20generation.%20Each%20sample%20is%20accompanied%20by%20a%20carefully%20designed%20multiple-choice%20question%20to%20test%20the%20model%27s%20competence%20in%20fine-grained%20reasoning.%20We%20evaluate%20multiple%20state-of-the-art%20visual%20language%20models%20and%20compare%20them%20with%20models%20based%20on%20contrastive%20learning.%20Experiments%20show%20that%20all%20models%20perform%20significantly%20worse%20than%20humans%20in%20semantic%20judgment%2C%20especially%20in%20distinguishing%20grammatical%20correctness%20from%20semantic%20rationality.%20Further%20experiments%20show%20that%20even%20the%20lightweight%20model%20can%20improve%20its%20accuracy%20after%20fine-tuning%2C%20demonstrating%20the%20great%20potential%20of%20directional%20adaptation.%20This%20study%20not%20only%20reveals%20the%20key%20weaknesses%20of%20VLMs%2C%20but%20also%20provides%20diagnostic%20tools%20and%20research%20directions%20for%20the%20development%20of%20robust%20models%20with%20real%20visual%20semantic%20reasoning%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520encoding%2520competence%2520of%2520visual%2520language%2520models%2520using%2520uncommon%2520actions%26entry.906535625%3DChen%2520Ling%2520and%2520Nai%2520Ding%26entry.1292438233%3DWe%2520propose%2520UAIT%2520%2528Uncommon-sense%2520Action%2520Image-Text%2529%2520dataset%252C%2520a%2520new%2520evaluation%2520benchmark%2520designed%2520to%2520test%2520the%2520semantic%2520understanding%2520ability%2520of%2520visual%2520language%2520models%2520%2528VLMs%2529%2520in%2520uncommon-sense%2520action%2520scenes.%2520Unlike%2520previous%2520datasets%2520that%2520focus%2520on%2520common%2520visual%2520scenes%2520with%2520statistical%2520frequency%2520advantages%252C%2520UAIT%2520challenges%2520models%2520with%2520grammatically%2520reasonable%2520but%2520semantically%2520counter-common%2520sense%2520image-text%2520pairs.%2520Such%2520tasks%2520require%2520models%2520to%2520go%2520beyond%2520superficial%2520pattern%2520recognition%2520and%2520demonstrate%2520a%2520deep%2520understanding%2520of%2520agent-patient%2520relationships%2520and%2520physical%2520feasibility.%2520To%2520build%2520UAIT%252C%2520we%2520designed%2520a%2520semi-automated%2520process%2520to%2520synthesize%2520high-quality%2520uncommon-sense%2520image-text%2520samples%2520using%2520large%2520language%2520models%252C%2520few-shot%2520prompt%2520engineering%252C%2520and%2520text-to-image%2520generation.%2520Each%2520sample%2520is%2520accompanied%2520by%2520a%2520carefully%2520designed%2520multiple-choice%2520question%2520to%2520test%2520the%2520model%2527s%2520competence%2520in%2520fine-grained%2520reasoning.%2520We%2520evaluate%2520multiple%2520state-of-the-art%2520visual%2520language%2520models%2520and%2520compare%2520them%2520with%2520models%2520based%2520on%2520contrastive%2520learning.%2520Experiments%2520show%2520that%2520all%2520models%2520perform%2520significantly%2520worse%2520than%2520humans%2520in%2520semantic%2520judgment%252C%2520especially%2520in%2520distinguishing%2520grammatical%2520correctness%2520from%2520semantic%2520rationality.%2520Further%2520experiments%2520show%2520that%2520even%2520the%2520lightweight%2520model%2520can%2520improve%2520its%2520accuracy%2520after%2520fine-tuning%252C%2520demonstrating%2520the%2520great%2520potential%2520of%2520directional%2520adaptation.%2520This%2520study%2520not%2520only%2520reveals%2520the%2520key%2520weaknesses%2520of%2520VLMs%252C%2520but%2520also%2520provides%2520diagnostic%2520tools%2520and%2520research%2520directions%2520for%2520the%2520development%2520of%2520robust%2520models%2520with%2520real%2520visual%2520semantic%2520reasoning%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20encoding%20competence%20of%20visual%20language%20models%20using%20uncommon%20actions&entry.906535625=Chen%20Ling%20and%20Nai%20Ding&entry.1292438233=We%20propose%20UAIT%20%28Uncommon-sense%20Action%20Image-Text%29%20dataset%2C%20a%20new%20evaluation%20benchmark%20designed%20to%20test%20the%20semantic%20understanding%20ability%20of%20visual%20language%20models%20%28VLMs%29%20in%20uncommon-sense%20action%20scenes.%20Unlike%20previous%20datasets%20that%20focus%20on%20common%20visual%20scenes%20with%20statistical%20frequency%20advantages%2C%20UAIT%20challenges%20models%20with%20grammatically%20reasonable%20but%20semantically%20counter-common%20sense%20image-text%20pairs.%20Such%20tasks%20require%20models%20to%20go%20beyond%20superficial%20pattern%20recognition%20and%20demonstrate%20a%20deep%20understanding%20of%20agent-patient%20relationships%20and%20physical%20feasibility.%20To%20build%20UAIT%2C%20we%20designed%20a%20semi-automated%20process%20to%20synthesize%20high-quality%20uncommon-sense%20image-text%20samples%20using%20large%20language%20models%2C%20few-shot%20prompt%20engineering%2C%20and%20text-to-image%20generation.%20Each%20sample%20is%20accompanied%20by%20a%20carefully%20designed%20multiple-choice%20question%20to%20test%20the%20model%27s%20competence%20in%20fine-grained%20reasoning.%20We%20evaluate%20multiple%20state-of-the-art%20visual%20language%20models%20and%20compare%20them%20with%20models%20based%20on%20contrastive%20learning.%20Experiments%20show%20that%20all%20models%20perform%20significantly%20worse%20than%20humans%20in%20semantic%20judgment%2C%20especially%20in%20distinguishing%20grammatical%20correctness%20from%20semantic%20rationality.%20Further%20experiments%20show%20that%20even%20the%20lightweight%20model%20can%20improve%20its%20accuracy%20after%20fine-tuning%2C%20demonstrating%20the%20great%20potential%20of%20directional%20adaptation.%20This%20study%20not%20only%20reveals%20the%20key%20weaknesses%20of%20VLMs%2C%20but%20also%20provides%20diagnostic%20tools%20and%20research%20directions%20for%20the%20development%20of%20robust%20models%20with%20real%20visual%20semantic%20reasoning%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2601.07737v1&entry.124074799=Read"},
{"title": "Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey", "author": "Jinxuan Li and Chaolei Tan and Haoxuan Chen and Jianxin Ma and Jian-Fang Hu and Wei-Shi Zheng and Jianhuang Lai", "abstract": "Image-Language Foundation Models (ILFMs) have demonstrated remarkable success in vision-language understanding, providing transferable multimodal representations that generalize across diverse downstream image-based tasks. The advancement of video-text research has spurred growing interest in extending image-based models to the video domain. This paradigm, termed as image-to-video transfer learning, effectively mitigates the substantial data and computational demands compared to training video-language models from scratch while achieves comparable or even stronger model performance. This survey provides the first comprehensive review of this emerging field, which begins by summarizing the widely used ILFMs and their capabilities. We then systematically classify existing image-to-video transfer learning techniques into two broad root categories (frozen features and adapted features), along with numerous fine-grained subcategories, based on the paradigm for transferring image understanding capability to video tasks. Building upon the task-specific nature of image-to-video transfer, this survey methodically elaborates these strategies and details their applications across a spectrum of video-text learning tasks, ranging from fine-grained settings (e.g., spatio-temporal video grounding) to coarse-grained ones (e.g., video question answering). We further present a detailed experimental analysis to investigate the efficacy of different image-to-video transfer learning paradigms on a range of downstream video understanding tasks. Finally, we identify prevailing challenges and highlight promising directions for future research. By offering a comprehensive and structured overview, this survey aims to establish a structured roadmap for advancing video-text learning based on existing ILFM, and to inspire future research directions in this rapidly evolving domain. Github repository is available.", "link": "http://arxiv.org/abs/2510.10671v2", "date": "2026-01-12", "relevancy": 2.9737, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6117}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6117}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image-to-Video%20Transfer%20Learning%20based%20on%20Image-Language%20Foundation%20Models%3A%20A%20Comprehensive%20Survey&body=Title%3A%20Image-to-Video%20Transfer%20Learning%20based%20on%20Image-Language%20Foundation%20Models%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Jinxuan%20Li%20and%20Chaolei%20Tan%20and%20Haoxuan%20Chen%20and%20Jianxin%20Ma%20and%20Jian-Fang%20Hu%20and%20Wei-Shi%20Zheng%20and%20Jianhuang%20Lai%0AAbstract%3A%20Image-Language%20Foundation%20Models%20%28ILFMs%29%20have%20demonstrated%20remarkable%20success%20in%20vision-language%20understanding%2C%20providing%20transferable%20multimodal%20representations%20that%20generalize%20across%20diverse%20downstream%20image-based%20tasks.%20The%20advancement%20of%20video-text%20research%20has%20spurred%20growing%20interest%20in%20extending%20image-based%20models%20to%20the%20video%20domain.%20This%20paradigm%2C%20termed%20as%20image-to-video%20transfer%20learning%2C%20effectively%20mitigates%20the%20substantial%20data%20and%20computational%20demands%20compared%20to%20training%20video-language%20models%20from%20scratch%20while%20achieves%20comparable%20or%20even%20stronger%20model%20performance.%20This%20survey%20provides%20the%20first%20comprehensive%20review%20of%20this%20emerging%20field%2C%20which%20begins%20by%20summarizing%20the%20widely%20used%20ILFMs%20and%20their%20capabilities.%20We%20then%20systematically%20classify%20existing%20image-to-video%20transfer%20learning%20techniques%20into%20two%20broad%20root%20categories%20%28frozen%20features%20and%20adapted%20features%29%2C%20along%20with%20numerous%20fine-grained%20subcategories%2C%20based%20on%20the%20paradigm%20for%20transferring%20image%20understanding%20capability%20to%20video%20tasks.%20Building%20upon%20the%20task-specific%20nature%20of%20image-to-video%20transfer%2C%20this%20survey%20methodically%20elaborates%20these%20strategies%20and%20details%20their%20applications%20across%20a%20spectrum%20of%20video-text%20learning%20tasks%2C%20ranging%20from%20fine-grained%20settings%20%28e.g.%2C%20spatio-temporal%20video%20grounding%29%20to%20coarse-grained%20ones%20%28e.g.%2C%20video%20question%20answering%29.%20We%20further%20present%20a%20detailed%20experimental%20analysis%20to%20investigate%20the%20efficacy%20of%20different%20image-to-video%20transfer%20learning%20paradigms%20on%20a%20range%20of%20downstream%20video%20understanding%20tasks.%20Finally%2C%20we%20identify%20prevailing%20challenges%20and%20highlight%20promising%20directions%20for%20future%20research.%20By%20offering%20a%20comprehensive%20and%20structured%20overview%2C%20this%20survey%20aims%20to%20establish%20a%20structured%20roadmap%20for%20advancing%20video-text%20learning%20based%20on%20existing%20ILFM%2C%20and%20to%20inspire%20future%20research%20directions%20in%20this%20rapidly%20evolving%20domain.%20Github%20repository%20is%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2510.10671v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage-to-Video%2520Transfer%2520Learning%2520based%2520on%2520Image-Language%2520Foundation%2520Models%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DJinxuan%2520Li%2520and%2520Chaolei%2520Tan%2520and%2520Haoxuan%2520Chen%2520and%2520Jianxin%2520Ma%2520and%2520Jian-Fang%2520Hu%2520and%2520Wei-Shi%2520Zheng%2520and%2520Jianhuang%2520Lai%26entry.1292438233%3DImage-Language%2520Foundation%2520Models%2520%2528ILFMs%2529%2520have%2520demonstrated%2520remarkable%2520success%2520in%2520vision-language%2520understanding%252C%2520providing%2520transferable%2520multimodal%2520representations%2520that%2520generalize%2520across%2520diverse%2520downstream%2520image-based%2520tasks.%2520The%2520advancement%2520of%2520video-text%2520research%2520has%2520spurred%2520growing%2520interest%2520in%2520extending%2520image-based%2520models%2520to%2520the%2520video%2520domain.%2520This%2520paradigm%252C%2520termed%2520as%2520image-to-video%2520transfer%2520learning%252C%2520effectively%2520mitigates%2520the%2520substantial%2520data%2520and%2520computational%2520demands%2520compared%2520to%2520training%2520video-language%2520models%2520from%2520scratch%2520while%2520achieves%2520comparable%2520or%2520even%2520stronger%2520model%2520performance.%2520This%2520survey%2520provides%2520the%2520first%2520comprehensive%2520review%2520of%2520this%2520emerging%2520field%252C%2520which%2520begins%2520by%2520summarizing%2520the%2520widely%2520used%2520ILFMs%2520and%2520their%2520capabilities.%2520We%2520then%2520systematically%2520classify%2520existing%2520image-to-video%2520transfer%2520learning%2520techniques%2520into%2520two%2520broad%2520root%2520categories%2520%2528frozen%2520features%2520and%2520adapted%2520features%2529%252C%2520along%2520with%2520numerous%2520fine-grained%2520subcategories%252C%2520based%2520on%2520the%2520paradigm%2520for%2520transferring%2520image%2520understanding%2520capability%2520to%2520video%2520tasks.%2520Building%2520upon%2520the%2520task-specific%2520nature%2520of%2520image-to-video%2520transfer%252C%2520this%2520survey%2520methodically%2520elaborates%2520these%2520strategies%2520and%2520details%2520their%2520applications%2520across%2520a%2520spectrum%2520of%2520video-text%2520learning%2520tasks%252C%2520ranging%2520from%2520fine-grained%2520settings%2520%2528e.g.%252C%2520spatio-temporal%2520video%2520grounding%2529%2520to%2520coarse-grained%2520ones%2520%2528e.g.%252C%2520video%2520question%2520answering%2529.%2520We%2520further%2520present%2520a%2520detailed%2520experimental%2520analysis%2520to%2520investigate%2520the%2520efficacy%2520of%2520different%2520image-to-video%2520transfer%2520learning%2520paradigms%2520on%2520a%2520range%2520of%2520downstream%2520video%2520understanding%2520tasks.%2520Finally%252C%2520we%2520identify%2520prevailing%2520challenges%2520and%2520highlight%2520promising%2520directions%2520for%2520future%2520research.%2520By%2520offering%2520a%2520comprehensive%2520and%2520structured%2520overview%252C%2520this%2520survey%2520aims%2520to%2520establish%2520a%2520structured%2520roadmap%2520for%2520advancing%2520video-text%2520learning%2520based%2520on%2520existing%2520ILFM%252C%2520and%2520to%2520inspire%2520future%2520research%2520directions%2520in%2520this%2520rapidly%2520evolving%2520domain.%2520Github%2520repository%2520is%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10671v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image-to-Video%20Transfer%20Learning%20based%20on%20Image-Language%20Foundation%20Models%3A%20A%20Comprehensive%20Survey&entry.906535625=Jinxuan%20Li%20and%20Chaolei%20Tan%20and%20Haoxuan%20Chen%20and%20Jianxin%20Ma%20and%20Jian-Fang%20Hu%20and%20Wei-Shi%20Zheng%20and%20Jianhuang%20Lai&entry.1292438233=Image-Language%20Foundation%20Models%20%28ILFMs%29%20have%20demonstrated%20remarkable%20success%20in%20vision-language%20understanding%2C%20providing%20transferable%20multimodal%20representations%20that%20generalize%20across%20diverse%20downstream%20image-based%20tasks.%20The%20advancement%20of%20video-text%20research%20has%20spurred%20growing%20interest%20in%20extending%20image-based%20models%20to%20the%20video%20domain.%20This%20paradigm%2C%20termed%20as%20image-to-video%20transfer%20learning%2C%20effectively%20mitigates%20the%20substantial%20data%20and%20computational%20demands%20compared%20to%20training%20video-language%20models%20from%20scratch%20while%20achieves%20comparable%20or%20even%20stronger%20model%20performance.%20This%20survey%20provides%20the%20first%20comprehensive%20review%20of%20this%20emerging%20field%2C%20which%20begins%20by%20summarizing%20the%20widely%20used%20ILFMs%20and%20their%20capabilities.%20We%20then%20systematically%20classify%20existing%20image-to-video%20transfer%20learning%20techniques%20into%20two%20broad%20root%20categories%20%28frozen%20features%20and%20adapted%20features%29%2C%20along%20with%20numerous%20fine-grained%20subcategories%2C%20based%20on%20the%20paradigm%20for%20transferring%20image%20understanding%20capability%20to%20video%20tasks.%20Building%20upon%20the%20task-specific%20nature%20of%20image-to-video%20transfer%2C%20this%20survey%20methodically%20elaborates%20these%20strategies%20and%20details%20their%20applications%20across%20a%20spectrum%20of%20video-text%20learning%20tasks%2C%20ranging%20from%20fine-grained%20settings%20%28e.g.%2C%20spatio-temporal%20video%20grounding%29%20to%20coarse-grained%20ones%20%28e.g.%2C%20video%20question%20answering%29.%20We%20further%20present%20a%20detailed%20experimental%20analysis%20to%20investigate%20the%20efficacy%20of%20different%20image-to-video%20transfer%20learning%20paradigms%20on%20a%20range%20of%20downstream%20video%20understanding%20tasks.%20Finally%2C%20we%20identify%20prevailing%20challenges%20and%20highlight%20promising%20directions%20for%20future%20research.%20By%20offering%20a%20comprehensive%20and%20structured%20overview%2C%20this%20survey%20aims%20to%20establish%20a%20structured%20roadmap%20for%20advancing%20video-text%20learning%20based%20on%20existing%20ILFM%2C%20and%20to%20inspire%20future%20research%20directions%20in%20this%20rapidly%20evolving%20domain.%20Github%20repository%20is%20available.&entry.1838667208=http%3A//arxiv.org/abs/2510.10671v2&entry.124074799=Read"},
{"title": "Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model", "author": "Siwen Jiao and Tianxiong Lv and Kangan Qian and Chenxu Zhao and Xiuyuan Zhu and Tianlun Li and Xiaolong Cheng and Jinyu Li and Zhihao Liao and Yang Cai", "abstract": "Vision-Language Models (VLMs) face a critical bottleneck in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning (RL) approaches, primarily based on relative ranking, often suffer from severe reward sparsity and gradient instability, failing to effectively exploit the verifiable signals provided by 3D physical constraints. Notably, in standard GRPO frameworks, relative normalization causes \"near-miss\" samples (characterized by small but non-zero errors) to suffer from advantage collapse. This leads to a severe data utilization bottleneck where valuable boundary samples are discarded during optimization. To address this, we introduce the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA employs a dynamically parameterized Sigmoid function to transform raw feedback into a dense, continuous reward continuum. Concurrently, AP-GRPO integrates absolute scalar gradients to mitigate the numerical information loss inherent in conventional relative-ranking mechanisms. By leveraging this approach, we constructed Numerical3D-50k, a dataset comprising 50,000 verifiable 3D subtasks. Empirical results indicate that AP-GRPO achieves performance parity with large-scale supervised methods while maintaining higher data efficiency, effectively activating latent 3D reasoning in VLMs without requiring architectural modifications.", "link": "http://arxiv.org/abs/2601.07695v1", "date": "2026-01-12", "relevancy": 2.9735, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5975}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5975}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smooth%20Operator%3A%20Smooth%20Verifiable%20Reward%20Activates%20Spatial%20Reasoning%20Ability%20of%20Vision-Language%20Model&body=Title%3A%20Smooth%20Operator%3A%20Smooth%20Verifiable%20Reward%20Activates%20Spatial%20Reasoning%20Ability%20of%20Vision-Language%20Model%0AAuthor%3A%20Siwen%20Jiao%20and%20Tianxiong%20Lv%20and%20Kangan%20Qian%20and%20Chenxu%20Zhao%20and%20Xiuyuan%20Zhu%20and%20Tianlun%20Li%20and%20Xiaolong%20Cheng%20and%20Jinyu%20Li%20and%20Zhihao%20Liao%20and%20Yang%20Cai%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20face%20a%20critical%20bottleneck%20in%20achieving%20precise%20numerical%20prediction%20for%203D%20scene%20understanding.%20Traditional%20reinforcement%20learning%20%28RL%29%20approaches%2C%20primarily%20based%20on%20relative%20ranking%2C%20often%20suffer%20from%20severe%20reward%20sparsity%20and%20gradient%20instability%2C%20failing%20to%20effectively%20exploit%20the%20verifiable%20signals%20provided%20by%203D%20physical%20constraints.%20Notably%2C%20in%20standard%20GRPO%20frameworks%2C%20relative%20normalization%20causes%20%22near-miss%22%20samples%20%28characterized%20by%20small%20but%20non-zero%20errors%29%20to%20suffer%20from%20advantage%20collapse.%20This%20leads%20to%20a%20severe%20data%20utilization%20bottleneck%20where%20valuable%20boundary%20samples%20are%20discarded%20during%20optimization.%20To%20address%20this%2C%20we%20introduce%20the%20Smooth%20Numerical%20Reward%20Activation%20%28SNRA%29%20operator%20and%20the%20Absolute-Preserving%20GRPO%20%28AP-GRPO%29%20framework.%20SNRA%20employs%20a%20dynamically%20parameterized%20Sigmoid%20function%20to%20transform%20raw%20feedback%20into%20a%20dense%2C%20continuous%20reward%20continuum.%20Concurrently%2C%20AP-GRPO%20integrates%20absolute%20scalar%20gradients%20to%20mitigate%20the%20numerical%20information%20loss%20inherent%20in%20conventional%20relative-ranking%20mechanisms.%20By%20leveraging%20this%20approach%2C%20we%20constructed%20Numerical3D-50k%2C%20a%20dataset%20comprising%2050%2C000%20verifiable%203D%20subtasks.%20Empirical%20results%20indicate%20that%20AP-GRPO%20achieves%20performance%20parity%20with%20large-scale%20supervised%20methods%20while%20maintaining%20higher%20data%20efficiency%2C%20effectively%20activating%20latent%203D%20reasoning%20in%20VLMs%20without%20requiring%20architectural%20modifications.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmooth%2520Operator%253A%2520Smooth%2520Verifiable%2520Reward%2520Activates%2520Spatial%2520Reasoning%2520Ability%2520of%2520Vision-Language%2520Model%26entry.906535625%3DSiwen%2520Jiao%2520and%2520Tianxiong%2520Lv%2520and%2520Kangan%2520Qian%2520and%2520Chenxu%2520Zhao%2520and%2520Xiuyuan%2520Zhu%2520and%2520Tianlun%2520Li%2520and%2520Xiaolong%2520Cheng%2520and%2520Jinyu%2520Li%2520and%2520Zhihao%2520Liao%2520and%2520Yang%2520Cai%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520face%2520a%2520critical%2520bottleneck%2520in%2520achieving%2520precise%2520numerical%2520prediction%2520for%25203D%2520scene%2520understanding.%2520Traditional%2520reinforcement%2520learning%2520%2528RL%2529%2520approaches%252C%2520primarily%2520based%2520on%2520relative%2520ranking%252C%2520often%2520suffer%2520from%2520severe%2520reward%2520sparsity%2520and%2520gradient%2520instability%252C%2520failing%2520to%2520effectively%2520exploit%2520the%2520verifiable%2520signals%2520provided%2520by%25203D%2520physical%2520constraints.%2520Notably%252C%2520in%2520standard%2520GRPO%2520frameworks%252C%2520relative%2520normalization%2520causes%2520%2522near-miss%2522%2520samples%2520%2528characterized%2520by%2520small%2520but%2520non-zero%2520errors%2529%2520to%2520suffer%2520from%2520advantage%2520collapse.%2520This%2520leads%2520to%2520a%2520severe%2520data%2520utilization%2520bottleneck%2520where%2520valuable%2520boundary%2520samples%2520are%2520discarded%2520during%2520optimization.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520Smooth%2520Numerical%2520Reward%2520Activation%2520%2528SNRA%2529%2520operator%2520and%2520the%2520Absolute-Preserving%2520GRPO%2520%2528AP-GRPO%2529%2520framework.%2520SNRA%2520employs%2520a%2520dynamically%2520parameterized%2520Sigmoid%2520function%2520to%2520transform%2520raw%2520feedback%2520into%2520a%2520dense%252C%2520continuous%2520reward%2520continuum.%2520Concurrently%252C%2520AP-GRPO%2520integrates%2520absolute%2520scalar%2520gradients%2520to%2520mitigate%2520the%2520numerical%2520information%2520loss%2520inherent%2520in%2520conventional%2520relative-ranking%2520mechanisms.%2520By%2520leveraging%2520this%2520approach%252C%2520we%2520constructed%2520Numerical3D-50k%252C%2520a%2520dataset%2520comprising%252050%252C000%2520verifiable%25203D%2520subtasks.%2520Empirical%2520results%2520indicate%2520that%2520AP-GRPO%2520achieves%2520performance%2520parity%2520with%2520large-scale%2520supervised%2520methods%2520while%2520maintaining%2520higher%2520data%2520efficiency%252C%2520effectively%2520activating%2520latent%25203D%2520reasoning%2520in%2520VLMs%2520without%2520requiring%2520architectural%2520modifications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smooth%20Operator%3A%20Smooth%20Verifiable%20Reward%20Activates%20Spatial%20Reasoning%20Ability%20of%20Vision-Language%20Model&entry.906535625=Siwen%20Jiao%20and%20Tianxiong%20Lv%20and%20Kangan%20Qian%20and%20Chenxu%20Zhao%20and%20Xiuyuan%20Zhu%20and%20Tianlun%20Li%20and%20Xiaolong%20Cheng%20and%20Jinyu%20Li%20and%20Zhihao%20Liao%20and%20Yang%20Cai&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20face%20a%20critical%20bottleneck%20in%20achieving%20precise%20numerical%20prediction%20for%203D%20scene%20understanding.%20Traditional%20reinforcement%20learning%20%28RL%29%20approaches%2C%20primarily%20based%20on%20relative%20ranking%2C%20often%20suffer%20from%20severe%20reward%20sparsity%20and%20gradient%20instability%2C%20failing%20to%20effectively%20exploit%20the%20verifiable%20signals%20provided%20by%203D%20physical%20constraints.%20Notably%2C%20in%20standard%20GRPO%20frameworks%2C%20relative%20normalization%20causes%20%22near-miss%22%20samples%20%28characterized%20by%20small%20but%20non-zero%20errors%29%20to%20suffer%20from%20advantage%20collapse.%20This%20leads%20to%20a%20severe%20data%20utilization%20bottleneck%20where%20valuable%20boundary%20samples%20are%20discarded%20during%20optimization.%20To%20address%20this%2C%20we%20introduce%20the%20Smooth%20Numerical%20Reward%20Activation%20%28SNRA%29%20operator%20and%20the%20Absolute-Preserving%20GRPO%20%28AP-GRPO%29%20framework.%20SNRA%20employs%20a%20dynamically%20parameterized%20Sigmoid%20function%20to%20transform%20raw%20feedback%20into%20a%20dense%2C%20continuous%20reward%20continuum.%20Concurrently%2C%20AP-GRPO%20integrates%20absolute%20scalar%20gradients%20to%20mitigate%20the%20numerical%20information%20loss%20inherent%20in%20conventional%20relative-ranking%20mechanisms.%20By%20leveraging%20this%20approach%2C%20we%20constructed%20Numerical3D-50k%2C%20a%20dataset%20comprising%2050%2C000%20verifiable%203D%20subtasks.%20Empirical%20results%20indicate%20that%20AP-GRPO%20achieves%20performance%20parity%20with%20large-scale%20supervised%20methods%20while%20maintaining%20higher%20data%20efficiency%2C%20effectively%20activating%20latent%203D%20reasoning%20in%20VLMs%20without%20requiring%20architectural%20modifications.&entry.1838667208=http%3A//arxiv.org/abs/2601.07695v1&entry.124074799=Read"},
{"title": "SegDAC: Improving Visual Reinforcement Learning by Extracting Dynamic Object-Centric Representations from Pretrained Vision Models", "author": "Alexandre Brown and Glen Berseth", "abstract": "Visual reinforcement learning (RL) is challenging due to the need to extract useful representations from high-dimensional inputs while learning effective control from sparse and noisy rewards. Although large perception models exist, integrating them effectively into RL for visual generalization and improved sample efficiency remains difficult. We propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment Anything (SAM) for object-centric decomposition and YOLO-World to ground the image segmentation process via text inputs. It includes a novel transformer-based architecture that supports a dynamic number of segments at each time step and effectively learns which segments to focus on using online RL, without using human labels. By evaluating SegDAC over a challenging visual generalization benchmark using Maniskill3, which covers diverse manipulation tasks under strong visual perturbations, we demonstrate that SegDAC achieves significantly better visual generalization, doubling prior performance on the hardest setting and matching or surpassing prior methods in sample efficiency across all evaluated tasks. Project Page: https://segdac.github.io/", "link": "http://arxiv.org/abs/2508.09325v3", "date": "2026-01-12", "relevancy": 2.9633, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6022}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6022}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegDAC%3A%20Improving%20Visual%20Reinforcement%20Learning%20by%20Extracting%20Dynamic%20Object-Centric%20Representations%20from%20Pretrained%20Vision%20Models&body=Title%3A%20SegDAC%3A%20Improving%20Visual%20Reinforcement%20Learning%20by%20Extracting%20Dynamic%20Object-Centric%20Representations%20from%20Pretrained%20Vision%20Models%0AAuthor%3A%20Alexandre%20Brown%20and%20Glen%20Berseth%0AAbstract%3A%20Visual%20reinforcement%20learning%20%28RL%29%20is%20challenging%20due%20to%20the%20need%20to%20extract%20useful%20representations%20from%20high-dimensional%20inputs%20while%20learning%20effective%20control%20from%20sparse%20and%20noisy%20rewards.%20Although%20large%20perception%20models%20exist%2C%20integrating%20them%20effectively%20into%20RL%20for%20visual%20generalization%20and%20improved%20sample%20efficiency%20remains%20difficult.%20We%20propose%20SegDAC%2C%20a%20Segmentation-Driven%20Actor-Critic%20method.%20SegDAC%20uses%20Segment%20Anything%20%28SAM%29%20for%20object-centric%20decomposition%20and%20YOLO-World%20to%20ground%20the%20image%20segmentation%20process%20via%20text%20inputs.%20It%20includes%20a%20novel%20transformer-based%20architecture%20that%20supports%20a%20dynamic%20number%20of%20segments%20at%20each%20time%20step%20and%20effectively%20learns%20which%20segments%20to%20focus%20on%20using%20online%20RL%2C%20without%20using%20human%20labels.%20By%20evaluating%20SegDAC%20over%20a%20challenging%20visual%20generalization%20benchmark%20using%20Maniskill3%2C%20which%20covers%20diverse%20manipulation%20tasks%20under%20strong%20visual%20perturbations%2C%20we%20demonstrate%20that%20SegDAC%20achieves%20significantly%20better%20visual%20generalization%2C%20doubling%20prior%20performance%20on%20the%20hardest%20setting%20and%20matching%20or%20surpassing%20prior%20methods%20in%20sample%20efficiency%20across%20all%20evaluated%20tasks.%20Project%20Page%3A%20https%3A//segdac.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2508.09325v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegDAC%253A%2520Improving%2520Visual%2520Reinforcement%2520Learning%2520by%2520Extracting%2520Dynamic%2520Object-Centric%2520Representations%2520from%2520Pretrained%2520Vision%2520Models%26entry.906535625%3DAlexandre%2520Brown%2520and%2520Glen%2520Berseth%26entry.1292438233%3DVisual%2520reinforcement%2520learning%2520%2528RL%2529%2520is%2520challenging%2520due%2520to%2520the%2520need%2520to%2520extract%2520useful%2520representations%2520from%2520high-dimensional%2520inputs%2520while%2520learning%2520effective%2520control%2520from%2520sparse%2520and%2520noisy%2520rewards.%2520Although%2520large%2520perception%2520models%2520exist%252C%2520integrating%2520them%2520effectively%2520into%2520RL%2520for%2520visual%2520generalization%2520and%2520improved%2520sample%2520efficiency%2520remains%2520difficult.%2520We%2520propose%2520SegDAC%252C%2520a%2520Segmentation-Driven%2520Actor-Critic%2520method.%2520SegDAC%2520uses%2520Segment%2520Anything%2520%2528SAM%2529%2520for%2520object-centric%2520decomposition%2520and%2520YOLO-World%2520to%2520ground%2520the%2520image%2520segmentation%2520process%2520via%2520text%2520inputs.%2520It%2520includes%2520a%2520novel%2520transformer-based%2520architecture%2520that%2520supports%2520a%2520dynamic%2520number%2520of%2520segments%2520at%2520each%2520time%2520step%2520and%2520effectively%2520learns%2520which%2520segments%2520to%2520focus%2520on%2520using%2520online%2520RL%252C%2520without%2520using%2520human%2520labels.%2520By%2520evaluating%2520SegDAC%2520over%2520a%2520challenging%2520visual%2520generalization%2520benchmark%2520using%2520Maniskill3%252C%2520which%2520covers%2520diverse%2520manipulation%2520tasks%2520under%2520strong%2520visual%2520perturbations%252C%2520we%2520demonstrate%2520that%2520SegDAC%2520achieves%2520significantly%2520better%2520visual%2520generalization%252C%2520doubling%2520prior%2520performance%2520on%2520the%2520hardest%2520setting%2520and%2520matching%2520or%2520surpassing%2520prior%2520methods%2520in%2520sample%2520efficiency%2520across%2520all%2520evaluated%2520tasks.%2520Project%2520Page%253A%2520https%253A//segdac.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09325v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegDAC%3A%20Improving%20Visual%20Reinforcement%20Learning%20by%20Extracting%20Dynamic%20Object-Centric%20Representations%20from%20Pretrained%20Vision%20Models&entry.906535625=Alexandre%20Brown%20and%20Glen%20Berseth&entry.1292438233=Visual%20reinforcement%20learning%20%28RL%29%20is%20challenging%20due%20to%20the%20need%20to%20extract%20useful%20representations%20from%20high-dimensional%20inputs%20while%20learning%20effective%20control%20from%20sparse%20and%20noisy%20rewards.%20Although%20large%20perception%20models%20exist%2C%20integrating%20them%20effectively%20into%20RL%20for%20visual%20generalization%20and%20improved%20sample%20efficiency%20remains%20difficult.%20We%20propose%20SegDAC%2C%20a%20Segmentation-Driven%20Actor-Critic%20method.%20SegDAC%20uses%20Segment%20Anything%20%28SAM%29%20for%20object-centric%20decomposition%20and%20YOLO-World%20to%20ground%20the%20image%20segmentation%20process%20via%20text%20inputs.%20It%20includes%20a%20novel%20transformer-based%20architecture%20that%20supports%20a%20dynamic%20number%20of%20segments%20at%20each%20time%20step%20and%20effectively%20learns%20which%20segments%20to%20focus%20on%20using%20online%20RL%2C%20without%20using%20human%20labels.%20By%20evaluating%20SegDAC%20over%20a%20challenging%20visual%20generalization%20benchmark%20using%20Maniskill3%2C%20which%20covers%20diverse%20manipulation%20tasks%20under%20strong%20visual%20perturbations%2C%20we%20demonstrate%20that%20SegDAC%20achieves%20significantly%20better%20visual%20generalization%2C%20doubling%20prior%20performance%20on%20the%20hardest%20setting%20and%20matching%20or%20surpassing%20prior%20methods%20in%20sample%20efficiency%20across%20all%20evaluated%20tasks.%20Project%20Page%3A%20https%3A//segdac.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2508.09325v3&entry.124074799=Read"},
{"title": "More Images, More Problems? A Controlled Analysis of VLM Failure Modes", "author": "Anurag Das and Adrian Bulat and Alberto Baldrati and Ioannis Maniadis Metaxas and Bernt Schiele and Georgios Tzimiropoulos and Brais Martinez", "abstract": "Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.", "link": "http://arxiv.org/abs/2601.07812v1", "date": "2026-01-12", "relevancy": 2.9139, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5834}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5834}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20More%20Images%2C%20More%20Problems%3F%20A%20Controlled%20Analysis%20of%20VLM%20Failure%20Modes&body=Title%3A%20More%20Images%2C%20More%20Problems%3F%20A%20Controlled%20Analysis%20of%20VLM%20Failure%20Modes%0AAuthor%3A%20Anurag%20Das%20and%20Adrian%20Bulat%20and%20Alberto%20Baldrati%20and%20Ioannis%20Maniadis%20Metaxas%20and%20Bernt%20Schiele%20and%20Georgios%20Tzimiropoulos%20and%20Brais%20Martinez%0AAbstract%3A%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20have%20demonstrated%20remarkable%20capabilities%2C%20yet%20their%20proficiency%20in%20understanding%20and%20reasoning%20over%20multiple%20images%20remains%20largely%20unexplored.%20While%20existing%20benchmarks%20have%20initiated%20the%20evaluation%20of%20multi-image%20models%2C%20a%20comprehensive%20analysis%20of%20their%20core%20weaknesses%20and%20their%20causes%20is%20still%20lacking.%20In%20this%20work%2C%20we%20introduce%20MIMIC%20%28Multi-Image%20Model%20Insights%20and%20Challenges%29%2C%20a%20new%20benchmark%20designed%20to%20rigorously%20evaluate%20the%20multi-image%20capabilities%20of%20LVLMs.%20Using%20MIMIC%2C%20we%20conduct%20a%20series%20of%20diagnostic%20experiments%20that%20reveal%20pervasive%20issues%3A%20LVLMs%20often%20fail%20to%20aggregate%20information%20across%20images%20and%20struggle%20to%20track%20or%20attend%20to%20multiple%20concepts%20simultaneously.%20To%20address%20these%20failures%2C%20we%20propose%20two%20novel%20complementary%20remedies.%20On%20the%20data%20side%2C%20we%20present%20a%20procedural%20data-generation%20strategy%20that%20composes%20single-image%20annotations%20into%20rich%2C%20targeted%20multi-image%20training%20examples.%20On%20the%20optimization%20side%2C%20we%20analyze%20layer-wise%20attention%20patterns%20and%20derive%20an%20attention-masking%20scheme%20tailored%20for%20multi-image%20inputs.%20Experiments%20substantially%20improved%20cross-image%20aggregation%2C%20while%20also%20enhancing%20performance%20on%20existing%20multi-image%20benchmarks%2C%20outperforming%20prior%20state%20of%20the%20art%20across%20tasks.%20Data%20and%20code%20will%20be%20made%20available%20at%20https%3A//github.com/anurag-198/MIMIC.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMore%2520Images%252C%2520More%2520Problems%253F%2520A%2520Controlled%2520Analysis%2520of%2520VLM%2520Failure%2520Modes%26entry.906535625%3DAnurag%2520Das%2520and%2520Adrian%2520Bulat%2520and%2520Alberto%2520Baldrati%2520and%2520Ioannis%2520Maniadis%2520Metaxas%2520and%2520Bernt%2520Schiele%2520and%2520Georgios%2520Tzimiropoulos%2520and%2520Brais%2520Martinez%26entry.1292438233%3DLarge%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%252C%2520yet%2520their%2520proficiency%2520in%2520understanding%2520and%2520reasoning%2520over%2520multiple%2520images%2520remains%2520largely%2520unexplored.%2520While%2520existing%2520benchmarks%2520have%2520initiated%2520the%2520evaluation%2520of%2520multi-image%2520models%252C%2520a%2520comprehensive%2520analysis%2520of%2520their%2520core%2520weaknesses%2520and%2520their%2520causes%2520is%2520still%2520lacking.%2520In%2520this%2520work%252C%2520we%2520introduce%2520MIMIC%2520%2528Multi-Image%2520Model%2520Insights%2520and%2520Challenges%2529%252C%2520a%2520new%2520benchmark%2520designed%2520to%2520rigorously%2520evaluate%2520the%2520multi-image%2520capabilities%2520of%2520LVLMs.%2520Using%2520MIMIC%252C%2520we%2520conduct%2520a%2520series%2520of%2520diagnostic%2520experiments%2520that%2520reveal%2520pervasive%2520issues%253A%2520LVLMs%2520often%2520fail%2520to%2520aggregate%2520information%2520across%2520images%2520and%2520struggle%2520to%2520track%2520or%2520attend%2520to%2520multiple%2520concepts%2520simultaneously.%2520To%2520address%2520these%2520failures%252C%2520we%2520propose%2520two%2520novel%2520complementary%2520remedies.%2520On%2520the%2520data%2520side%252C%2520we%2520present%2520a%2520procedural%2520data-generation%2520strategy%2520that%2520composes%2520single-image%2520annotations%2520into%2520rich%252C%2520targeted%2520multi-image%2520training%2520examples.%2520On%2520the%2520optimization%2520side%252C%2520we%2520analyze%2520layer-wise%2520attention%2520patterns%2520and%2520derive%2520an%2520attention-masking%2520scheme%2520tailored%2520for%2520multi-image%2520inputs.%2520Experiments%2520substantially%2520improved%2520cross-image%2520aggregation%252C%2520while%2520also%2520enhancing%2520performance%2520on%2520existing%2520multi-image%2520benchmarks%252C%2520outperforming%2520prior%2520state%2520of%2520the%2520art%2520across%2520tasks.%2520Data%2520and%2520code%2520will%2520be%2520made%2520available%2520at%2520https%253A//github.com/anurag-198/MIMIC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=More%20Images%2C%20More%20Problems%3F%20A%20Controlled%20Analysis%20of%20VLM%20Failure%20Modes&entry.906535625=Anurag%20Das%20and%20Adrian%20Bulat%20and%20Alberto%20Baldrati%20and%20Ioannis%20Maniadis%20Metaxas%20and%20Bernt%20Schiele%20and%20Georgios%20Tzimiropoulos%20and%20Brais%20Martinez&entry.1292438233=Large%20Vision%20Language%20Models%20%28LVLMs%29%20have%20demonstrated%20remarkable%20capabilities%2C%20yet%20their%20proficiency%20in%20understanding%20and%20reasoning%20over%20multiple%20images%20remains%20largely%20unexplored.%20While%20existing%20benchmarks%20have%20initiated%20the%20evaluation%20of%20multi-image%20models%2C%20a%20comprehensive%20analysis%20of%20their%20core%20weaknesses%20and%20their%20causes%20is%20still%20lacking.%20In%20this%20work%2C%20we%20introduce%20MIMIC%20%28Multi-Image%20Model%20Insights%20and%20Challenges%29%2C%20a%20new%20benchmark%20designed%20to%20rigorously%20evaluate%20the%20multi-image%20capabilities%20of%20LVLMs.%20Using%20MIMIC%2C%20we%20conduct%20a%20series%20of%20diagnostic%20experiments%20that%20reveal%20pervasive%20issues%3A%20LVLMs%20often%20fail%20to%20aggregate%20information%20across%20images%20and%20struggle%20to%20track%20or%20attend%20to%20multiple%20concepts%20simultaneously.%20To%20address%20these%20failures%2C%20we%20propose%20two%20novel%20complementary%20remedies.%20On%20the%20data%20side%2C%20we%20present%20a%20procedural%20data-generation%20strategy%20that%20composes%20single-image%20annotations%20into%20rich%2C%20targeted%20multi-image%20training%20examples.%20On%20the%20optimization%20side%2C%20we%20analyze%20layer-wise%20attention%20patterns%20and%20derive%20an%20attention-masking%20scheme%20tailored%20for%20multi-image%20inputs.%20Experiments%20substantially%20improved%20cross-image%20aggregation%2C%20while%20also%20enhancing%20performance%20on%20existing%20multi-image%20benchmarks%2C%20outperforming%20prior%20state%20of%20the%20art%20across%20tasks.%20Data%20and%20code%20will%20be%20made%20available%20at%20https%3A//github.com/anurag-198/MIMIC.&entry.1838667208=http%3A//arxiv.org/abs/2601.07812v1&entry.124074799=Read"},
{"title": "PARL: Position-Aware Relation Learning Network for Document Layout Analysis", "author": "Fuyuan Liu and Dianyu Yu and He Ren and Nayu Liu and Xiaomian Kang and Delai Qiu and Fa Zhang and Genpeng Zhen and Shengping Liu and Jiaen Liang and Wei Huang and Yining Wang and Junnan Zhu", "abstract": "Document layout analysis aims to detect and categorize structural elements (e.g., titles, tables, figures) in scanned or digital documents. Popular methods often rely on high-quality Optical Character Recognition (OCR) to merge visual features with extracted text. This dependency introduces two major drawbacks: propagation of text recognition errors and substantial computational overhead, limiting the robustness and practical applicability of multimodal approaches. In contrast to the prevailing multimodal trend, we argue that effective layout analysis depends not on text-visual fusion, but on a deep understanding of documents' intrinsic visual structure. To this end, we propose PARL (Position-Aware Relation Learning Network), a novel OCR-free, vision-only framework that models layout through positional sensitivity and relational structure. Specifically, we first introduce a Bidirectional Spatial Position-Guided Deformable Attention module to embed explicit positional dependencies among layout elements directly into visual features. Second, we design a Graph Refinement Classifier (GRC) to refine predictions by modeling contextual relationships through a dynamically constructed layout graph. Extensive experiments show PARL achieves state-of-the-art results. It establishes a new benchmark for vision-only methods on DocLayNet and, notably, surpasses even strong multimodal models on M6Doc. Crucially, PARL (65M) is highly efficient, using roughly four times fewer parameters than large multimodal models (256M), demonstrating that sophisticated visual structure modeling can be both more efficient and robust than multimodal fusion.", "link": "http://arxiv.org/abs/2601.07620v1", "date": "2026-01-12", "relevancy": 2.885, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6645}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5397}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PARL%3A%20Position-Aware%20Relation%20Learning%20Network%20for%20Document%20Layout%20Analysis&body=Title%3A%20PARL%3A%20Position-Aware%20Relation%20Learning%20Network%20for%20Document%20Layout%20Analysis%0AAuthor%3A%20Fuyuan%20Liu%20and%20Dianyu%20Yu%20and%20He%20Ren%20and%20Nayu%20Liu%20and%20Xiaomian%20Kang%20and%20Delai%20Qiu%20and%20Fa%20Zhang%20and%20Genpeng%20Zhen%20and%20Shengping%20Liu%20and%20Jiaen%20Liang%20and%20Wei%20Huang%20and%20Yining%20Wang%20and%20Junnan%20Zhu%0AAbstract%3A%20Document%20layout%20analysis%20aims%20to%20detect%20and%20categorize%20structural%20elements%20%28e.g.%2C%20titles%2C%20tables%2C%20figures%29%20in%20scanned%20or%20digital%20documents.%20Popular%20methods%20often%20rely%20on%20high-quality%20Optical%20Character%20Recognition%20%28OCR%29%20to%20merge%20visual%20features%20with%20extracted%20text.%20This%20dependency%20introduces%20two%20major%20drawbacks%3A%20propagation%20of%20text%20recognition%20errors%20and%20substantial%20computational%20overhead%2C%20limiting%20the%20robustness%20and%20practical%20applicability%20of%20multimodal%20approaches.%20In%20contrast%20to%20the%20prevailing%20multimodal%20trend%2C%20we%20argue%20that%20effective%20layout%20analysis%20depends%20not%20on%20text-visual%20fusion%2C%20but%20on%20a%20deep%20understanding%20of%20documents%27%20intrinsic%20visual%20structure.%20To%20this%20end%2C%20we%20propose%20PARL%20%28Position-Aware%20Relation%20Learning%20Network%29%2C%20a%20novel%20OCR-free%2C%20vision-only%20framework%20that%20models%20layout%20through%20positional%20sensitivity%20and%20relational%20structure.%20Specifically%2C%20we%20first%20introduce%20a%20Bidirectional%20Spatial%20Position-Guided%20Deformable%20Attention%20module%20to%20embed%20explicit%20positional%20dependencies%20among%20layout%20elements%20directly%20into%20visual%20features.%20Second%2C%20we%20design%20a%20Graph%20Refinement%20Classifier%20%28GRC%29%20to%20refine%20predictions%20by%20modeling%20contextual%20relationships%20through%20a%20dynamically%20constructed%20layout%20graph.%20Extensive%20experiments%20show%20PARL%20achieves%20state-of-the-art%20results.%20It%20establishes%20a%20new%20benchmark%20for%20vision-only%20methods%20on%20DocLayNet%20and%2C%20notably%2C%20surpasses%20even%20strong%20multimodal%20models%20on%20M6Doc.%20Crucially%2C%20PARL%20%2865M%29%20is%20highly%20efficient%2C%20using%20roughly%20four%20times%20fewer%20parameters%20than%20large%20multimodal%20models%20%28256M%29%2C%20demonstrating%20that%20sophisticated%20visual%20structure%20modeling%20can%20be%20both%20more%20efficient%20and%20robust%20than%20multimodal%20fusion.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPARL%253A%2520Position-Aware%2520Relation%2520Learning%2520Network%2520for%2520Document%2520Layout%2520Analysis%26entry.906535625%3DFuyuan%2520Liu%2520and%2520Dianyu%2520Yu%2520and%2520He%2520Ren%2520and%2520Nayu%2520Liu%2520and%2520Xiaomian%2520Kang%2520and%2520Delai%2520Qiu%2520and%2520Fa%2520Zhang%2520and%2520Genpeng%2520Zhen%2520and%2520Shengping%2520Liu%2520and%2520Jiaen%2520Liang%2520and%2520Wei%2520Huang%2520and%2520Yining%2520Wang%2520and%2520Junnan%2520Zhu%26entry.1292438233%3DDocument%2520layout%2520analysis%2520aims%2520to%2520detect%2520and%2520categorize%2520structural%2520elements%2520%2528e.g.%252C%2520titles%252C%2520tables%252C%2520figures%2529%2520in%2520scanned%2520or%2520digital%2520documents.%2520Popular%2520methods%2520often%2520rely%2520on%2520high-quality%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%2520to%2520merge%2520visual%2520features%2520with%2520extracted%2520text.%2520This%2520dependency%2520introduces%2520two%2520major%2520drawbacks%253A%2520propagation%2520of%2520text%2520recognition%2520errors%2520and%2520substantial%2520computational%2520overhead%252C%2520limiting%2520the%2520robustness%2520and%2520practical%2520applicability%2520of%2520multimodal%2520approaches.%2520In%2520contrast%2520to%2520the%2520prevailing%2520multimodal%2520trend%252C%2520we%2520argue%2520that%2520effective%2520layout%2520analysis%2520depends%2520not%2520on%2520text-visual%2520fusion%252C%2520but%2520on%2520a%2520deep%2520understanding%2520of%2520documents%2527%2520intrinsic%2520visual%2520structure.%2520To%2520this%2520end%252C%2520we%2520propose%2520PARL%2520%2528Position-Aware%2520Relation%2520Learning%2520Network%2529%252C%2520a%2520novel%2520OCR-free%252C%2520vision-only%2520framework%2520that%2520models%2520layout%2520through%2520positional%2520sensitivity%2520and%2520relational%2520structure.%2520Specifically%252C%2520we%2520first%2520introduce%2520a%2520Bidirectional%2520Spatial%2520Position-Guided%2520Deformable%2520Attention%2520module%2520to%2520embed%2520explicit%2520positional%2520dependencies%2520among%2520layout%2520elements%2520directly%2520into%2520visual%2520features.%2520Second%252C%2520we%2520design%2520a%2520Graph%2520Refinement%2520Classifier%2520%2528GRC%2529%2520to%2520refine%2520predictions%2520by%2520modeling%2520contextual%2520relationships%2520through%2520a%2520dynamically%2520constructed%2520layout%2520graph.%2520Extensive%2520experiments%2520show%2520PARL%2520achieves%2520state-of-the-art%2520results.%2520It%2520establishes%2520a%2520new%2520benchmark%2520for%2520vision-only%2520methods%2520on%2520DocLayNet%2520and%252C%2520notably%252C%2520surpasses%2520even%2520strong%2520multimodal%2520models%2520on%2520M6Doc.%2520Crucially%252C%2520PARL%2520%252865M%2529%2520is%2520highly%2520efficient%252C%2520using%2520roughly%2520four%2520times%2520fewer%2520parameters%2520than%2520large%2520multimodal%2520models%2520%2528256M%2529%252C%2520demonstrating%2520that%2520sophisticated%2520visual%2520structure%2520modeling%2520can%2520be%2520both%2520more%2520efficient%2520and%2520robust%2520than%2520multimodal%2520fusion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PARL%3A%20Position-Aware%20Relation%20Learning%20Network%20for%20Document%20Layout%20Analysis&entry.906535625=Fuyuan%20Liu%20and%20Dianyu%20Yu%20and%20He%20Ren%20and%20Nayu%20Liu%20and%20Xiaomian%20Kang%20and%20Delai%20Qiu%20and%20Fa%20Zhang%20and%20Genpeng%20Zhen%20and%20Shengping%20Liu%20and%20Jiaen%20Liang%20and%20Wei%20Huang%20and%20Yining%20Wang%20and%20Junnan%20Zhu&entry.1292438233=Document%20layout%20analysis%20aims%20to%20detect%20and%20categorize%20structural%20elements%20%28e.g.%2C%20titles%2C%20tables%2C%20figures%29%20in%20scanned%20or%20digital%20documents.%20Popular%20methods%20often%20rely%20on%20high-quality%20Optical%20Character%20Recognition%20%28OCR%29%20to%20merge%20visual%20features%20with%20extracted%20text.%20This%20dependency%20introduces%20two%20major%20drawbacks%3A%20propagation%20of%20text%20recognition%20errors%20and%20substantial%20computational%20overhead%2C%20limiting%20the%20robustness%20and%20practical%20applicability%20of%20multimodal%20approaches.%20In%20contrast%20to%20the%20prevailing%20multimodal%20trend%2C%20we%20argue%20that%20effective%20layout%20analysis%20depends%20not%20on%20text-visual%20fusion%2C%20but%20on%20a%20deep%20understanding%20of%20documents%27%20intrinsic%20visual%20structure.%20To%20this%20end%2C%20we%20propose%20PARL%20%28Position-Aware%20Relation%20Learning%20Network%29%2C%20a%20novel%20OCR-free%2C%20vision-only%20framework%20that%20models%20layout%20through%20positional%20sensitivity%20and%20relational%20structure.%20Specifically%2C%20we%20first%20introduce%20a%20Bidirectional%20Spatial%20Position-Guided%20Deformable%20Attention%20module%20to%20embed%20explicit%20positional%20dependencies%20among%20layout%20elements%20directly%20into%20visual%20features.%20Second%2C%20we%20design%20a%20Graph%20Refinement%20Classifier%20%28GRC%29%20to%20refine%20predictions%20by%20modeling%20contextual%20relationships%20through%20a%20dynamically%20constructed%20layout%20graph.%20Extensive%20experiments%20show%20PARL%20achieves%20state-of-the-art%20results.%20It%20establishes%20a%20new%20benchmark%20for%20vision-only%20methods%20on%20DocLayNet%20and%2C%20notably%2C%20surpasses%20even%20strong%20multimodal%20models%20on%20M6Doc.%20Crucially%2C%20PARL%20%2865M%29%20is%20highly%20efficient%2C%20using%20roughly%20four%20times%20fewer%20parameters%20than%20large%20multimodal%20models%20%28256M%29%2C%20demonstrating%20that%20sophisticated%20visual%20structure%20modeling%20can%20be%20both%20more%20efficient%20and%20robust%20than%20multimodal%20fusion.&entry.1838667208=http%3A//arxiv.org/abs/2601.07620v1&entry.124074799=Read"},
{"title": "PanoSAMic: Panoramic Image Segmentation from SAM Feature Encoding and Dual View Fusion", "author": "Mahdi Chamseddine and Didier Stricker and Jason Rambach", "abstract": "Existing image foundation models are not optimized for spherical images having been trained primarily on perspective images. PanoSAMic integrates the pre-trained Segment Anything (SAM) encoder to make use of its extensive training and integrate it into a semantic segmentation model for panoramic images using multiple modalities. We modify the SAM encoder to output multi-stage features and introduce a novel spatio-modal fusion module that allows the model to select the relevant modalities and best features from each modality for different areas of the input. Furthermore, our semantic decoder uses spherical attention and dual view fusion to overcome the distortions and edge discontinuity often associated with panoramic images. PanoSAMic achieves state-of-the-art (SotA) results on Stanford2D3DS for RGB, RGB-D, and RGB-D-N modalities and on Matterport3D for RGB and RGB-D modalities. https://github.com/dfki-av/PanoSAMic", "link": "http://arxiv.org/abs/2601.07447v1", "date": "2026-01-12", "relevancy": 2.8431, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5774}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5676}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PanoSAMic%3A%20Panoramic%20Image%20Segmentation%20from%20SAM%20Feature%20Encoding%20and%20Dual%20View%20Fusion&body=Title%3A%20PanoSAMic%3A%20Panoramic%20Image%20Segmentation%20from%20SAM%20Feature%20Encoding%20and%20Dual%20View%20Fusion%0AAuthor%3A%20Mahdi%20Chamseddine%20and%20Didier%20Stricker%20and%20Jason%20Rambach%0AAbstract%3A%20Existing%20image%20foundation%20models%20are%20not%20optimized%20for%20spherical%20images%20having%20been%20trained%20primarily%20on%20perspective%20images.%20PanoSAMic%20integrates%20the%20pre-trained%20Segment%20Anything%20%28SAM%29%20encoder%20to%20make%20use%20of%20its%20extensive%20training%20and%20integrate%20it%20into%20a%20semantic%20segmentation%20model%20for%20panoramic%20images%20using%20multiple%20modalities.%20We%20modify%20the%20SAM%20encoder%20to%20output%20multi-stage%20features%20and%20introduce%20a%20novel%20spatio-modal%20fusion%20module%20that%20allows%20the%20model%20to%20select%20the%20relevant%20modalities%20and%20best%20features%20from%20each%20modality%20for%20different%20areas%20of%20the%20input.%20Furthermore%2C%20our%20semantic%20decoder%20uses%20spherical%20attention%20and%20dual%20view%20fusion%20to%20overcome%20the%20distortions%20and%20edge%20discontinuity%20often%20associated%20with%20panoramic%20images.%20PanoSAMic%20achieves%20state-of-the-art%20%28SotA%29%20results%20on%20Stanford2D3DS%20for%20RGB%2C%20RGB-D%2C%20and%20RGB-D-N%20modalities%20and%20on%20Matterport3D%20for%20RGB%20and%20RGB-D%20modalities.%20https%3A//github.com/dfki-av/PanoSAMic%0ALink%3A%20http%3A//arxiv.org/abs/2601.07447v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanoSAMic%253A%2520Panoramic%2520Image%2520Segmentation%2520from%2520SAM%2520Feature%2520Encoding%2520and%2520Dual%2520View%2520Fusion%26entry.906535625%3DMahdi%2520Chamseddine%2520and%2520Didier%2520Stricker%2520and%2520Jason%2520Rambach%26entry.1292438233%3DExisting%2520image%2520foundation%2520models%2520are%2520not%2520optimized%2520for%2520spherical%2520images%2520having%2520been%2520trained%2520primarily%2520on%2520perspective%2520images.%2520PanoSAMic%2520integrates%2520the%2520pre-trained%2520Segment%2520Anything%2520%2528SAM%2529%2520encoder%2520to%2520make%2520use%2520of%2520its%2520extensive%2520training%2520and%2520integrate%2520it%2520into%2520a%2520semantic%2520segmentation%2520model%2520for%2520panoramic%2520images%2520using%2520multiple%2520modalities.%2520We%2520modify%2520the%2520SAM%2520encoder%2520to%2520output%2520multi-stage%2520features%2520and%2520introduce%2520a%2520novel%2520spatio-modal%2520fusion%2520module%2520that%2520allows%2520the%2520model%2520to%2520select%2520the%2520relevant%2520modalities%2520and%2520best%2520features%2520from%2520each%2520modality%2520for%2520different%2520areas%2520of%2520the%2520input.%2520Furthermore%252C%2520our%2520semantic%2520decoder%2520uses%2520spherical%2520attention%2520and%2520dual%2520view%2520fusion%2520to%2520overcome%2520the%2520distortions%2520and%2520edge%2520discontinuity%2520often%2520associated%2520with%2520panoramic%2520images.%2520PanoSAMic%2520achieves%2520state-of-the-art%2520%2528SotA%2529%2520results%2520on%2520Stanford2D3DS%2520for%2520RGB%252C%2520RGB-D%252C%2520and%2520RGB-D-N%2520modalities%2520and%2520on%2520Matterport3D%2520for%2520RGB%2520and%2520RGB-D%2520modalities.%2520https%253A//github.com/dfki-av/PanoSAMic%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07447v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PanoSAMic%3A%20Panoramic%20Image%20Segmentation%20from%20SAM%20Feature%20Encoding%20and%20Dual%20View%20Fusion&entry.906535625=Mahdi%20Chamseddine%20and%20Didier%20Stricker%20and%20Jason%20Rambach&entry.1292438233=Existing%20image%20foundation%20models%20are%20not%20optimized%20for%20spherical%20images%20having%20been%20trained%20primarily%20on%20perspective%20images.%20PanoSAMic%20integrates%20the%20pre-trained%20Segment%20Anything%20%28SAM%29%20encoder%20to%20make%20use%20of%20its%20extensive%20training%20and%20integrate%20it%20into%20a%20semantic%20segmentation%20model%20for%20panoramic%20images%20using%20multiple%20modalities.%20We%20modify%20the%20SAM%20encoder%20to%20output%20multi-stage%20features%20and%20introduce%20a%20novel%20spatio-modal%20fusion%20module%20that%20allows%20the%20model%20to%20select%20the%20relevant%20modalities%20and%20best%20features%20from%20each%20modality%20for%20different%20areas%20of%20the%20input.%20Furthermore%2C%20our%20semantic%20decoder%20uses%20spherical%20attention%20and%20dual%20view%20fusion%20to%20overcome%20the%20distortions%20and%20edge%20discontinuity%20often%20associated%20with%20panoramic%20images.%20PanoSAMic%20achieves%20state-of-the-art%20%28SotA%29%20results%20on%20Stanford2D3DS%20for%20RGB%2C%20RGB-D%2C%20and%20RGB-D-N%20modalities%20and%20on%20Matterport3D%20for%20RGB%20and%20RGB-D%20modalities.%20https%3A//github.com/dfki-av/PanoSAMic&entry.1838667208=http%3A//arxiv.org/abs/2601.07447v1&entry.124074799=Read"},
{"title": "GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models", "author": "Zhankai Ye and Bofan Li and Yukai Jin and Shuoqiu Li and Wei Wang and Yanfu Zhang and Shangqian Gao and Xin Liu", "abstract": "Discrete motion tokenization has recently enabled Large Language Models (LLMs) to serve as versatile backbones for motion understanding and motion-language reasoning. However, existing pipelines typically decouple motion quantization from semantic embedding learning, linking them solely via token IDs. This approach fails to effectively align the intrinsic geometry of the motion space with the embedding space, thereby hindering the LLM's capacity for nuanced motion reasoning. We argue that alignment is most effective when both modalities share a unified geometric basis. Therefore, instead of forcing the LLM to reconstruct the complex geometry among motion tokens from scratch, we present a novel framework that explicitly enforces orthogonality on both the motion codebook and the LLM embedding space, ensuring that their relational structures naturally mirror each other. Specifically, we employ a decoder-only quantizer with Gumbel-Softmax for differentiable training and balanced codebook usage. To bridge the modalities, we use a sparse projection that maps motion codes into the LLM embedding space while preserving orthogonality. Finally, a two-stage orthonormal regularization schedule enforces soft constraints during tokenizer training and LLM fine-tuning to maintain geometric alignment without hindering semantic adaptation. Extensive experiments on HumanML3D demonstrate that our framework achieves a 20% performance improvement over current state-of-the-art methods, validating that a unified geometric basis effectively empowers the LLM for nuanced motion reasoning.", "link": "http://arxiv.org/abs/2601.07632v1", "date": "2026-01-12", "relevancy": 2.8144, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5701}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5631}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoMotionGPT%3A%20Geometry-Aligned%20Motion%20Understanding%20with%20Large%20Language%20Models&body=Title%3A%20GeoMotionGPT%3A%20Geometry-Aligned%20Motion%20Understanding%20with%20Large%20Language%20Models%0AAuthor%3A%20Zhankai%20Ye%20and%20Bofan%20Li%20and%20Yukai%20Jin%20and%20Shuoqiu%20Li%20and%20Wei%20Wang%20and%20Yanfu%20Zhang%20and%20Shangqian%20Gao%20and%20Xin%20Liu%0AAbstract%3A%20Discrete%20motion%20tokenization%20has%20recently%20enabled%20Large%20Language%20Models%20%28LLMs%29%20to%20serve%20as%20versatile%20backbones%20for%20motion%20understanding%20and%20motion-language%20reasoning.%20However%2C%20existing%20pipelines%20typically%20decouple%20motion%20quantization%20from%20semantic%20embedding%20learning%2C%20linking%20them%20solely%20via%20token%20IDs.%20This%20approach%20fails%20to%20effectively%20align%20the%20intrinsic%20geometry%20of%20the%20motion%20space%20with%20the%20embedding%20space%2C%20thereby%20hindering%20the%20LLM%27s%20capacity%20for%20nuanced%20motion%20reasoning.%20We%20argue%20that%20alignment%20is%20most%20effective%20when%20both%20modalities%20share%20a%20unified%20geometric%20basis.%20Therefore%2C%20instead%20of%20forcing%20the%20LLM%20to%20reconstruct%20the%20complex%20geometry%20among%20motion%20tokens%20from%20scratch%2C%20we%20present%20a%20novel%20framework%20that%20explicitly%20enforces%20orthogonality%20on%20both%20the%20motion%20codebook%20and%20the%20LLM%20embedding%20space%2C%20ensuring%20that%20their%20relational%20structures%20naturally%20mirror%20each%20other.%20Specifically%2C%20we%20employ%20a%20decoder-only%20quantizer%20with%20Gumbel-Softmax%20for%20differentiable%20training%20and%20balanced%20codebook%20usage.%20To%20bridge%20the%20modalities%2C%20we%20use%20a%20sparse%20projection%20that%20maps%20motion%20codes%20into%20the%20LLM%20embedding%20space%20while%20preserving%20orthogonality.%20Finally%2C%20a%20two-stage%20orthonormal%20regularization%20schedule%20enforces%20soft%20constraints%20during%20tokenizer%20training%20and%20LLM%20fine-tuning%20to%20maintain%20geometric%20alignment%20without%20hindering%20semantic%20adaptation.%20Extensive%20experiments%20on%20HumanML3D%20demonstrate%20that%20our%20framework%20achieves%20a%2020%25%20performance%20improvement%20over%20current%20state-of-the-art%20methods%2C%20validating%20that%20a%20unified%20geometric%20basis%20effectively%20empowers%20the%20LLM%20for%20nuanced%20motion%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoMotionGPT%253A%2520Geometry-Aligned%2520Motion%2520Understanding%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DZhankai%2520Ye%2520and%2520Bofan%2520Li%2520and%2520Yukai%2520Jin%2520and%2520Shuoqiu%2520Li%2520and%2520Wei%2520Wang%2520and%2520Yanfu%2520Zhang%2520and%2520Shangqian%2520Gao%2520and%2520Xin%2520Liu%26entry.1292438233%3DDiscrete%2520motion%2520tokenization%2520has%2520recently%2520enabled%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520serve%2520as%2520versatile%2520backbones%2520for%2520motion%2520understanding%2520and%2520motion-language%2520reasoning.%2520However%252C%2520existing%2520pipelines%2520typically%2520decouple%2520motion%2520quantization%2520from%2520semantic%2520embedding%2520learning%252C%2520linking%2520them%2520solely%2520via%2520token%2520IDs.%2520This%2520approach%2520fails%2520to%2520effectively%2520align%2520the%2520intrinsic%2520geometry%2520of%2520the%2520motion%2520space%2520with%2520the%2520embedding%2520space%252C%2520thereby%2520hindering%2520the%2520LLM%2527s%2520capacity%2520for%2520nuanced%2520motion%2520reasoning.%2520We%2520argue%2520that%2520alignment%2520is%2520most%2520effective%2520when%2520both%2520modalities%2520share%2520a%2520unified%2520geometric%2520basis.%2520Therefore%252C%2520instead%2520of%2520forcing%2520the%2520LLM%2520to%2520reconstruct%2520the%2520complex%2520geometry%2520among%2520motion%2520tokens%2520from%2520scratch%252C%2520we%2520present%2520a%2520novel%2520framework%2520that%2520explicitly%2520enforces%2520orthogonality%2520on%2520both%2520the%2520motion%2520codebook%2520and%2520the%2520LLM%2520embedding%2520space%252C%2520ensuring%2520that%2520their%2520relational%2520structures%2520naturally%2520mirror%2520each%2520other.%2520Specifically%252C%2520we%2520employ%2520a%2520decoder-only%2520quantizer%2520with%2520Gumbel-Softmax%2520for%2520differentiable%2520training%2520and%2520balanced%2520codebook%2520usage.%2520To%2520bridge%2520the%2520modalities%252C%2520we%2520use%2520a%2520sparse%2520projection%2520that%2520maps%2520motion%2520codes%2520into%2520the%2520LLM%2520embedding%2520space%2520while%2520preserving%2520orthogonality.%2520Finally%252C%2520a%2520two-stage%2520orthonormal%2520regularization%2520schedule%2520enforces%2520soft%2520constraints%2520during%2520tokenizer%2520training%2520and%2520LLM%2520fine-tuning%2520to%2520maintain%2520geometric%2520alignment%2520without%2520hindering%2520semantic%2520adaptation.%2520Extensive%2520experiments%2520on%2520HumanML3D%2520demonstrate%2520that%2520our%2520framework%2520achieves%2520a%252020%2525%2520performance%2520improvement%2520over%2520current%2520state-of-the-art%2520methods%252C%2520validating%2520that%2520a%2520unified%2520geometric%2520basis%2520effectively%2520empowers%2520the%2520LLM%2520for%2520nuanced%2520motion%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoMotionGPT%3A%20Geometry-Aligned%20Motion%20Understanding%20with%20Large%20Language%20Models&entry.906535625=Zhankai%20Ye%20and%20Bofan%20Li%20and%20Yukai%20Jin%20and%20Shuoqiu%20Li%20and%20Wei%20Wang%20and%20Yanfu%20Zhang%20and%20Shangqian%20Gao%20and%20Xin%20Liu&entry.1292438233=Discrete%20motion%20tokenization%20has%20recently%20enabled%20Large%20Language%20Models%20%28LLMs%29%20to%20serve%20as%20versatile%20backbones%20for%20motion%20understanding%20and%20motion-language%20reasoning.%20However%2C%20existing%20pipelines%20typically%20decouple%20motion%20quantization%20from%20semantic%20embedding%20learning%2C%20linking%20them%20solely%20via%20token%20IDs.%20This%20approach%20fails%20to%20effectively%20align%20the%20intrinsic%20geometry%20of%20the%20motion%20space%20with%20the%20embedding%20space%2C%20thereby%20hindering%20the%20LLM%27s%20capacity%20for%20nuanced%20motion%20reasoning.%20We%20argue%20that%20alignment%20is%20most%20effective%20when%20both%20modalities%20share%20a%20unified%20geometric%20basis.%20Therefore%2C%20instead%20of%20forcing%20the%20LLM%20to%20reconstruct%20the%20complex%20geometry%20among%20motion%20tokens%20from%20scratch%2C%20we%20present%20a%20novel%20framework%20that%20explicitly%20enforces%20orthogonality%20on%20both%20the%20motion%20codebook%20and%20the%20LLM%20embedding%20space%2C%20ensuring%20that%20their%20relational%20structures%20naturally%20mirror%20each%20other.%20Specifically%2C%20we%20employ%20a%20decoder-only%20quantizer%20with%20Gumbel-Softmax%20for%20differentiable%20training%20and%20balanced%20codebook%20usage.%20To%20bridge%20the%20modalities%2C%20we%20use%20a%20sparse%20projection%20that%20maps%20motion%20codes%20into%20the%20LLM%20embedding%20space%20while%20preserving%20orthogonality.%20Finally%2C%20a%20two-stage%20orthonormal%20regularization%20schedule%20enforces%20soft%20constraints%20during%20tokenizer%20training%20and%20LLM%20fine-tuning%20to%20maintain%20geometric%20alignment%20without%20hindering%20semantic%20adaptation.%20Extensive%20experiments%20on%20HumanML3D%20demonstrate%20that%20our%20framework%20achieves%20a%2020%25%20performance%20improvement%20over%20current%20state-of-the-art%20methods%2C%20validating%20that%20a%20unified%20geometric%20basis%20effectively%20empowers%20the%20LLM%20for%20nuanced%20motion%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2601.07632v1&entry.124074799=Read"},
{"title": "Fast Multi-Stack Slice-to-Volume Reconstruction via Multi-Scale Unrolled Optimization", "author": "Margherita Firenze and Sean I. Young and Clinton J. Wang and Hyuk Jin Yun and Elfar Adalsteinsson and Kiho Im and P. Ellen Grant and Polina Golland", "abstract": "Fully convolutional networks have become the backbone of modern medical imaging due to their ability to learn multi-scale representations and perform end-to-end inference. Yet their potential for slice-to-volume reconstruction (SVR), the task of jointly estimating 3D anatomy and slice poses from misaligned 2D acquisitions, remains underexplored. We introduce a fast convolutional framework that fuses multiple orthogonal 2D slice stacks to recover coherent 3D structure and refines slice alignment through lightweight model-based optimization. Applied to fetal brain MRI, our approach reconstructs high-quality 3D volumes in under 10s, with 1s slice registration and accuracy on par with state-of-the-art iterative SVR pipelines, offering more than speedup. The framework uses non-rigid displacement fields to represent transformations, generalizing to other SVR problems like fetal body and placental MRI. Additionally, the fast inference time paves the way for real-time, scanner-side volumetric feedback during MRI acquisition.", "link": "http://arxiv.org/abs/2601.07519v1", "date": "2026-01-12", "relevancy": 2.7999, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5835}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5494}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Multi-Stack%20Slice-to-Volume%20Reconstruction%20via%20Multi-Scale%20Unrolled%20Optimization&body=Title%3A%20Fast%20Multi-Stack%20Slice-to-Volume%20Reconstruction%20via%20Multi-Scale%20Unrolled%20Optimization%0AAuthor%3A%20Margherita%20Firenze%20and%20Sean%20I.%20Young%20and%20Clinton%20J.%20Wang%20and%20Hyuk%20Jin%20Yun%20and%20Elfar%20Adalsteinsson%20and%20Kiho%20Im%20and%20P.%20Ellen%20Grant%20and%20Polina%20Golland%0AAbstract%3A%20Fully%20convolutional%20networks%20have%20become%20the%20backbone%20of%20modern%20medical%20imaging%20due%20to%20their%20ability%20to%20learn%20multi-scale%20representations%20and%20perform%20end-to-end%20inference.%20Yet%20their%20potential%20for%20slice-to-volume%20reconstruction%20%28SVR%29%2C%20the%20task%20of%20jointly%20estimating%203D%20anatomy%20and%20slice%20poses%20from%20misaligned%202D%20acquisitions%2C%20remains%20underexplored.%20We%20introduce%20a%20fast%20convolutional%20framework%20that%20fuses%20multiple%20orthogonal%202D%20slice%20stacks%20to%20recover%20coherent%203D%20structure%20and%20refines%20slice%20alignment%20through%20lightweight%20model-based%20optimization.%20Applied%20to%20fetal%20brain%20MRI%2C%20our%20approach%20reconstructs%20high-quality%203D%20volumes%20in%20under%2010s%2C%20with%201s%20slice%20registration%20and%20accuracy%20on%20par%20with%20state-of-the-art%20iterative%20SVR%20pipelines%2C%20offering%20more%20than%20speedup.%20The%20framework%20uses%20non-rigid%20displacement%20fields%20to%20represent%20transformations%2C%20generalizing%20to%20other%20SVR%20problems%20like%20fetal%20body%20and%20placental%20MRI.%20Additionally%2C%20the%20fast%20inference%20time%20paves%20the%20way%20for%20real-time%2C%20scanner-side%20volumetric%20feedback%20during%20MRI%20acquisition.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Multi-Stack%2520Slice-to-Volume%2520Reconstruction%2520via%2520Multi-Scale%2520Unrolled%2520Optimization%26entry.906535625%3DMargherita%2520Firenze%2520and%2520Sean%2520I.%2520Young%2520and%2520Clinton%2520J.%2520Wang%2520and%2520Hyuk%2520Jin%2520Yun%2520and%2520Elfar%2520Adalsteinsson%2520and%2520Kiho%2520Im%2520and%2520P.%2520Ellen%2520Grant%2520and%2520Polina%2520Golland%26entry.1292438233%3DFully%2520convolutional%2520networks%2520have%2520become%2520the%2520backbone%2520of%2520modern%2520medical%2520imaging%2520due%2520to%2520their%2520ability%2520to%2520learn%2520multi-scale%2520representations%2520and%2520perform%2520end-to-end%2520inference.%2520Yet%2520their%2520potential%2520for%2520slice-to-volume%2520reconstruction%2520%2528SVR%2529%252C%2520the%2520task%2520of%2520jointly%2520estimating%25203D%2520anatomy%2520and%2520slice%2520poses%2520from%2520misaligned%25202D%2520acquisitions%252C%2520remains%2520underexplored.%2520We%2520introduce%2520a%2520fast%2520convolutional%2520framework%2520that%2520fuses%2520multiple%2520orthogonal%25202D%2520slice%2520stacks%2520to%2520recover%2520coherent%25203D%2520structure%2520and%2520refines%2520slice%2520alignment%2520through%2520lightweight%2520model-based%2520optimization.%2520Applied%2520to%2520fetal%2520brain%2520MRI%252C%2520our%2520approach%2520reconstructs%2520high-quality%25203D%2520volumes%2520in%2520under%252010s%252C%2520with%25201s%2520slice%2520registration%2520and%2520accuracy%2520on%2520par%2520with%2520state-of-the-art%2520iterative%2520SVR%2520pipelines%252C%2520offering%2520more%2520than%2520speedup.%2520The%2520framework%2520uses%2520non-rigid%2520displacement%2520fields%2520to%2520represent%2520transformations%252C%2520generalizing%2520to%2520other%2520SVR%2520problems%2520like%2520fetal%2520body%2520and%2520placental%2520MRI.%2520Additionally%252C%2520the%2520fast%2520inference%2520time%2520paves%2520the%2520way%2520for%2520real-time%252C%2520scanner-side%2520volumetric%2520feedback%2520during%2520MRI%2520acquisition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Multi-Stack%20Slice-to-Volume%20Reconstruction%20via%20Multi-Scale%20Unrolled%20Optimization&entry.906535625=Margherita%20Firenze%20and%20Sean%20I.%20Young%20and%20Clinton%20J.%20Wang%20and%20Hyuk%20Jin%20Yun%20and%20Elfar%20Adalsteinsson%20and%20Kiho%20Im%20and%20P.%20Ellen%20Grant%20and%20Polina%20Golland&entry.1292438233=Fully%20convolutional%20networks%20have%20become%20the%20backbone%20of%20modern%20medical%20imaging%20due%20to%20their%20ability%20to%20learn%20multi-scale%20representations%20and%20perform%20end-to-end%20inference.%20Yet%20their%20potential%20for%20slice-to-volume%20reconstruction%20%28SVR%29%2C%20the%20task%20of%20jointly%20estimating%203D%20anatomy%20and%20slice%20poses%20from%20misaligned%202D%20acquisitions%2C%20remains%20underexplored.%20We%20introduce%20a%20fast%20convolutional%20framework%20that%20fuses%20multiple%20orthogonal%202D%20slice%20stacks%20to%20recover%20coherent%203D%20structure%20and%20refines%20slice%20alignment%20through%20lightweight%20model-based%20optimization.%20Applied%20to%20fetal%20brain%20MRI%2C%20our%20approach%20reconstructs%20high-quality%203D%20volumes%20in%20under%2010s%2C%20with%201s%20slice%20registration%20and%20accuracy%20on%20par%20with%20state-of-the-art%20iterative%20SVR%20pipelines%2C%20offering%20more%20than%20speedup.%20The%20framework%20uses%20non-rigid%20displacement%20fields%20to%20represent%20transformations%2C%20generalizing%20to%20other%20SVR%20problems%20like%20fetal%20body%20and%20placental%20MRI.%20Additionally%2C%20the%20fast%20inference%20time%20paves%20the%20way%20for%20real-time%2C%20scanner-side%20volumetric%20feedback%20during%20MRI%20acquisition.&entry.1838667208=http%3A//arxiv.org/abs/2601.07519v1&entry.124074799=Read"},
{"title": "ViewMorpher3D: A 3D-aware Diffusion Framework for Multi-Camera Novel View Synthesis in Autonomous Driving", "author": "Farhad G. Zanjani and Hong Cai and Amirhossein Habibian", "abstract": "Autonomous driving systems rely heavily on multi-view images to ensure accurate perception and robust decision-making. To effectively develop and evaluate perception stacks and planning algorithms, realistic closed-loop simulators are indispensable. While 3D reconstruction techniques such as Gaussian Splatting offer promising avenues for simulator construction, the rendered novel views often exhibit artifacts, particularly in extrapolated perspectives or when available observations are sparse.\n  We introduce ViewMorpher3D, a multi-view image enhancement framework based on image diffusion models, designed to elevate photorealism and multi-view coherence in driving scenes. Unlike single-view approaches, ViewMorpher3D jointly processes a set of rendered views conditioned on camera poses, 3D geometric priors, and temporally adjacent or spatially overlapping reference views. This enables the model to infer missing details, suppress rendering artifacts, and enforce cross-view consistency.\n  Our framework accommodates variable numbers of cameras and flexible reference/target view configurations, making it adaptable to diverse sensor setups. Experiments on real-world driving datasets demonstrate substantial improvements in image quality metrics, effectively reducing artifacts while preserving geometric fidelity.", "link": "http://arxiv.org/abs/2601.07540v1", "date": "2026-01-12", "relevancy": 2.7338, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6934}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6934}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViewMorpher3D%3A%20A%203D-aware%20Diffusion%20Framework%20for%20Multi-Camera%20Novel%20View%20Synthesis%20in%20Autonomous%20Driving&body=Title%3A%20ViewMorpher3D%3A%20A%203D-aware%20Diffusion%20Framework%20for%20Multi-Camera%20Novel%20View%20Synthesis%20in%20Autonomous%20Driving%0AAuthor%3A%20Farhad%20G.%20Zanjani%20and%20Hong%20Cai%20and%20Amirhossein%20Habibian%0AAbstract%3A%20Autonomous%20driving%20systems%20rely%20heavily%20on%20multi-view%20images%20to%20ensure%20accurate%20perception%20and%20robust%20decision-making.%20To%20effectively%20develop%20and%20evaluate%20perception%20stacks%20and%20planning%20algorithms%2C%20realistic%20closed-loop%20simulators%20are%20indispensable.%20While%203D%20reconstruction%20techniques%20such%20as%20Gaussian%20Splatting%20offer%20promising%20avenues%20for%20simulator%20construction%2C%20the%20rendered%20novel%20views%20often%20exhibit%20artifacts%2C%20particularly%20in%20extrapolated%20perspectives%20or%20when%20available%20observations%20are%20sparse.%0A%20%20We%20introduce%20ViewMorpher3D%2C%20a%20multi-view%20image%20enhancement%20framework%20based%20on%20image%20diffusion%20models%2C%20designed%20to%20elevate%20photorealism%20and%20multi-view%20coherence%20in%20driving%20scenes.%20Unlike%20single-view%20approaches%2C%20ViewMorpher3D%20jointly%20processes%20a%20set%20of%20rendered%20views%20conditioned%20on%20camera%20poses%2C%203D%20geometric%20priors%2C%20and%20temporally%20adjacent%20or%20spatially%20overlapping%20reference%20views.%20This%20enables%20the%20model%20to%20infer%20missing%20details%2C%20suppress%20rendering%20artifacts%2C%20and%20enforce%20cross-view%20consistency.%0A%20%20Our%20framework%20accommodates%20variable%20numbers%20of%20cameras%20and%20flexible%20reference/target%20view%20configurations%2C%20making%20it%20adaptable%20to%20diverse%20sensor%20setups.%20Experiments%20on%20real-world%20driving%20datasets%20demonstrate%20substantial%20improvements%20in%20image%20quality%20metrics%2C%20effectively%20reducing%20artifacts%20while%20preserving%20geometric%20fidelity.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViewMorpher3D%253A%2520A%25203D-aware%2520Diffusion%2520Framework%2520for%2520Multi-Camera%2520Novel%2520View%2520Synthesis%2520in%2520Autonomous%2520Driving%26entry.906535625%3DFarhad%2520G.%2520Zanjani%2520and%2520Hong%2520Cai%2520and%2520Amirhossein%2520Habibian%26entry.1292438233%3DAutonomous%2520driving%2520systems%2520rely%2520heavily%2520on%2520multi-view%2520images%2520to%2520ensure%2520accurate%2520perception%2520and%2520robust%2520decision-making.%2520To%2520effectively%2520develop%2520and%2520evaluate%2520perception%2520stacks%2520and%2520planning%2520algorithms%252C%2520realistic%2520closed-loop%2520simulators%2520are%2520indispensable.%2520While%25203D%2520reconstruction%2520techniques%2520such%2520as%2520Gaussian%2520Splatting%2520offer%2520promising%2520avenues%2520for%2520simulator%2520construction%252C%2520the%2520rendered%2520novel%2520views%2520often%2520exhibit%2520artifacts%252C%2520particularly%2520in%2520extrapolated%2520perspectives%2520or%2520when%2520available%2520observations%2520are%2520sparse.%250A%2520%2520We%2520introduce%2520ViewMorpher3D%252C%2520a%2520multi-view%2520image%2520enhancement%2520framework%2520based%2520on%2520image%2520diffusion%2520models%252C%2520designed%2520to%2520elevate%2520photorealism%2520and%2520multi-view%2520coherence%2520in%2520driving%2520scenes.%2520Unlike%2520single-view%2520approaches%252C%2520ViewMorpher3D%2520jointly%2520processes%2520a%2520set%2520of%2520rendered%2520views%2520conditioned%2520on%2520camera%2520poses%252C%25203D%2520geometric%2520priors%252C%2520and%2520temporally%2520adjacent%2520or%2520spatially%2520overlapping%2520reference%2520views.%2520This%2520enables%2520the%2520model%2520to%2520infer%2520missing%2520details%252C%2520suppress%2520rendering%2520artifacts%252C%2520and%2520enforce%2520cross-view%2520consistency.%250A%2520%2520Our%2520framework%2520accommodates%2520variable%2520numbers%2520of%2520cameras%2520and%2520flexible%2520reference/target%2520view%2520configurations%252C%2520making%2520it%2520adaptable%2520to%2520diverse%2520sensor%2520setups.%2520Experiments%2520on%2520real-world%2520driving%2520datasets%2520demonstrate%2520substantial%2520improvements%2520in%2520image%2520quality%2520metrics%252C%2520effectively%2520reducing%2520artifacts%2520while%2520preserving%2520geometric%2520fidelity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViewMorpher3D%3A%20A%203D-aware%20Diffusion%20Framework%20for%20Multi-Camera%20Novel%20View%20Synthesis%20in%20Autonomous%20Driving&entry.906535625=Farhad%20G.%20Zanjani%20and%20Hong%20Cai%20and%20Amirhossein%20Habibian&entry.1292438233=Autonomous%20driving%20systems%20rely%20heavily%20on%20multi-view%20images%20to%20ensure%20accurate%20perception%20and%20robust%20decision-making.%20To%20effectively%20develop%20and%20evaluate%20perception%20stacks%20and%20planning%20algorithms%2C%20realistic%20closed-loop%20simulators%20are%20indispensable.%20While%203D%20reconstruction%20techniques%20such%20as%20Gaussian%20Splatting%20offer%20promising%20avenues%20for%20simulator%20construction%2C%20the%20rendered%20novel%20views%20often%20exhibit%20artifacts%2C%20particularly%20in%20extrapolated%20perspectives%20or%20when%20available%20observations%20are%20sparse.%0A%20%20We%20introduce%20ViewMorpher3D%2C%20a%20multi-view%20image%20enhancement%20framework%20based%20on%20image%20diffusion%20models%2C%20designed%20to%20elevate%20photorealism%20and%20multi-view%20coherence%20in%20driving%20scenes.%20Unlike%20single-view%20approaches%2C%20ViewMorpher3D%20jointly%20processes%20a%20set%20of%20rendered%20views%20conditioned%20on%20camera%20poses%2C%203D%20geometric%20priors%2C%20and%20temporally%20adjacent%20or%20spatially%20overlapping%20reference%20views.%20This%20enables%20the%20model%20to%20infer%20missing%20details%2C%20suppress%20rendering%20artifacts%2C%20and%20enforce%20cross-view%20consistency.%0A%20%20Our%20framework%20accommodates%20variable%20numbers%20of%20cameras%20and%20flexible%20reference/target%20view%20configurations%2C%20making%20it%20adaptable%20to%20diverse%20sensor%20setups.%20Experiments%20on%20real-world%20driving%20datasets%20demonstrate%20substantial%20improvements%20in%20image%20quality%20metrics%2C%20effectively%20reducing%20artifacts%20while%20preserving%20geometric%20fidelity.&entry.1838667208=http%3A//arxiv.org/abs/2601.07540v1&entry.124074799=Read"},
{"title": "REXO: Indoor Multi-View Radar Object Detection via 3D Bounding Box Diffusion", "author": "Ryoma Yataka and Pu Perry Wang and Petros Boufounos and Ryuhei Takahashi", "abstract": "Multi-view indoor radar perception has drawn attention due to its cost-effectiveness and low privacy risks. Existing methods often rely on {implicit} cross-view radar feature association, such as proposal pairing in RFMask or query-to-feature cross-attention in RETR, which can lead to ambiguous feature matches and degraded detection in complex indoor scenes. To address these limitations, we propose \\textbf{REXO} (multi-view Radar object dEtection with 3D bounding boX diffusiOn), which lifts the 2D bounding box (BBox) diffusion process of DiffusionDet into the 3D radar space. REXO utilizes these noisy 3D BBoxes to guide an {explicit} cross-view radar feature association, enhancing the cross-view radar-conditioned denoising process. By accounting for prior knowledge that the person is in contact with the ground, REXO reduces the number of diffusion parameters by determining them from this prior. Evaluated on two open indoor radar datasets, our approach surpasses state-of-the-art methods by a margin of +4.22 AP on the HIBER dataset and +11.02 AP on the MMVR dataset. The REXO implementation is available at https://github.com/merlresearch/radar-bbox-diffusion.", "link": "http://arxiv.org/abs/2511.17806v2", "date": "2026-01-12", "relevancy": 2.7334, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5505}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5448}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REXO%3A%20Indoor%20Multi-View%20Radar%20Object%20Detection%20via%203D%20Bounding%20Box%20Diffusion&body=Title%3A%20REXO%3A%20Indoor%20Multi-View%20Radar%20Object%20Detection%20via%203D%20Bounding%20Box%20Diffusion%0AAuthor%3A%20Ryoma%20Yataka%20and%20Pu%20Perry%20Wang%20and%20Petros%20Boufounos%20and%20Ryuhei%20Takahashi%0AAbstract%3A%20Multi-view%20indoor%20radar%20perception%20has%20drawn%20attention%20due%20to%20its%20cost-effectiveness%20and%20low%20privacy%20risks.%20Existing%20methods%20often%20rely%20on%20%7Bimplicit%7D%20cross-view%20radar%20feature%20association%2C%20such%20as%20proposal%20pairing%20in%20RFMask%20or%20query-to-feature%20cross-attention%20in%20RETR%2C%20which%20can%20lead%20to%20ambiguous%20feature%20matches%20and%20degraded%20detection%20in%20complex%20indoor%20scenes.%20To%20address%20these%20limitations%2C%20we%20propose%20%5Ctextbf%7BREXO%7D%20%28multi-view%20Radar%20object%20dEtection%20with%203D%20bounding%20boX%20diffusiOn%29%2C%20which%20lifts%20the%202D%20bounding%20box%20%28BBox%29%20diffusion%20process%20of%20DiffusionDet%20into%20the%203D%20radar%20space.%20REXO%20utilizes%20these%20noisy%203D%20BBoxes%20to%20guide%20an%20%7Bexplicit%7D%20cross-view%20radar%20feature%20association%2C%20enhancing%20the%20cross-view%20radar-conditioned%20denoising%20process.%20By%20accounting%20for%20prior%20knowledge%20that%20the%20person%20is%20in%20contact%20with%20the%20ground%2C%20REXO%20reduces%20the%20number%20of%20diffusion%20parameters%20by%20determining%20them%20from%20this%20prior.%20Evaluated%20on%20two%20open%20indoor%20radar%20datasets%2C%20our%20approach%20surpasses%20state-of-the-art%20methods%20by%20a%20margin%20of%20%2B4.22%20AP%20on%20the%20HIBER%20dataset%20and%20%2B11.02%20AP%20on%20the%20MMVR%20dataset.%20The%20REXO%20implementation%20is%20available%20at%20https%3A//github.com/merlresearch/radar-bbox-diffusion.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17806v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREXO%253A%2520Indoor%2520Multi-View%2520Radar%2520Object%2520Detection%2520via%25203D%2520Bounding%2520Box%2520Diffusion%26entry.906535625%3DRyoma%2520Yataka%2520and%2520Pu%2520Perry%2520Wang%2520and%2520Petros%2520Boufounos%2520and%2520Ryuhei%2520Takahashi%26entry.1292438233%3DMulti-view%2520indoor%2520radar%2520perception%2520has%2520drawn%2520attention%2520due%2520to%2520its%2520cost-effectiveness%2520and%2520low%2520privacy%2520risks.%2520Existing%2520methods%2520often%2520rely%2520on%2520%257Bimplicit%257D%2520cross-view%2520radar%2520feature%2520association%252C%2520such%2520as%2520proposal%2520pairing%2520in%2520RFMask%2520or%2520query-to-feature%2520cross-attention%2520in%2520RETR%252C%2520which%2520can%2520lead%2520to%2520ambiguous%2520feature%2520matches%2520and%2520degraded%2520detection%2520in%2520complex%2520indoor%2520scenes.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520%255Ctextbf%257BREXO%257D%2520%2528multi-view%2520Radar%2520object%2520dEtection%2520with%25203D%2520bounding%2520boX%2520diffusiOn%2529%252C%2520which%2520lifts%2520the%25202D%2520bounding%2520box%2520%2528BBox%2529%2520diffusion%2520process%2520of%2520DiffusionDet%2520into%2520the%25203D%2520radar%2520space.%2520REXO%2520utilizes%2520these%2520noisy%25203D%2520BBoxes%2520to%2520guide%2520an%2520%257Bexplicit%257D%2520cross-view%2520radar%2520feature%2520association%252C%2520enhancing%2520the%2520cross-view%2520radar-conditioned%2520denoising%2520process.%2520By%2520accounting%2520for%2520prior%2520knowledge%2520that%2520the%2520person%2520is%2520in%2520contact%2520with%2520the%2520ground%252C%2520REXO%2520reduces%2520the%2520number%2520of%2520diffusion%2520parameters%2520by%2520determining%2520them%2520from%2520this%2520prior.%2520Evaluated%2520on%2520two%2520open%2520indoor%2520radar%2520datasets%252C%2520our%2520approach%2520surpasses%2520state-of-the-art%2520methods%2520by%2520a%2520margin%2520of%2520%252B4.22%2520AP%2520on%2520the%2520HIBER%2520dataset%2520and%2520%252B11.02%2520AP%2520on%2520the%2520MMVR%2520dataset.%2520The%2520REXO%2520implementation%2520is%2520available%2520at%2520https%253A//github.com/merlresearch/radar-bbox-diffusion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17806v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REXO%3A%20Indoor%20Multi-View%20Radar%20Object%20Detection%20via%203D%20Bounding%20Box%20Diffusion&entry.906535625=Ryoma%20Yataka%20and%20Pu%20Perry%20Wang%20and%20Petros%20Boufounos%20and%20Ryuhei%20Takahashi&entry.1292438233=Multi-view%20indoor%20radar%20perception%20has%20drawn%20attention%20due%20to%20its%20cost-effectiveness%20and%20low%20privacy%20risks.%20Existing%20methods%20often%20rely%20on%20%7Bimplicit%7D%20cross-view%20radar%20feature%20association%2C%20such%20as%20proposal%20pairing%20in%20RFMask%20or%20query-to-feature%20cross-attention%20in%20RETR%2C%20which%20can%20lead%20to%20ambiguous%20feature%20matches%20and%20degraded%20detection%20in%20complex%20indoor%20scenes.%20To%20address%20these%20limitations%2C%20we%20propose%20%5Ctextbf%7BREXO%7D%20%28multi-view%20Radar%20object%20dEtection%20with%203D%20bounding%20boX%20diffusiOn%29%2C%20which%20lifts%20the%202D%20bounding%20box%20%28BBox%29%20diffusion%20process%20of%20DiffusionDet%20into%20the%203D%20radar%20space.%20REXO%20utilizes%20these%20noisy%203D%20BBoxes%20to%20guide%20an%20%7Bexplicit%7D%20cross-view%20radar%20feature%20association%2C%20enhancing%20the%20cross-view%20radar-conditioned%20denoising%20process.%20By%20accounting%20for%20prior%20knowledge%20that%20the%20person%20is%20in%20contact%20with%20the%20ground%2C%20REXO%20reduces%20the%20number%20of%20diffusion%20parameters%20by%20determining%20them%20from%20this%20prior.%20Evaluated%20on%20two%20open%20indoor%20radar%20datasets%2C%20our%20approach%20surpasses%20state-of-the-art%20methods%20by%20a%20margin%20of%20%2B4.22%20AP%20on%20the%20HIBER%20dataset%20and%20%2B11.02%20AP%20on%20the%20MMVR%20dataset.%20The%20REXO%20implementation%20is%20available%20at%20https%3A//github.com/merlresearch/radar-bbox-diffusion.&entry.1838667208=http%3A//arxiv.org/abs/2511.17806v2&entry.124074799=Read"},
{"title": "LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation", "author": "Yang Miao and Jan-Nico Zaech and Xi Wang and Fabien Despinoy and Danda Pani Paudel and Luc Van Gool", "abstract": "We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based framework for open-vocabulary object-part instance segmentation. Given an image, LangHOPS can jointly detect and segment hierarchical object and part instances from open-vocabulary candidate categories. Unlike prior approaches that rely on heuristic or learnable visual grouping, our approach grounds object-part hierarchies in language space. It integrates the MLLM into the object-part parsing pipeline to leverage its rich knowledge and reasoning capabilities, and link multi-granularity concepts within the hierarchies. We evaluate LangHOPS across multiple challenging scenarios, including in-domain and cross-dataset object-part instance segmentation, and zero-shot semantic segmentation. LangHOPS achieves state-of-the-art results, surpassing previous methods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on the PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K (zero-shot). Ablation studies further validate the effectiveness of the language-grounded hierarchy and MLLM driven part query refinement strategy. The code will be released here.", "link": "http://arxiv.org/abs/2510.25263v3", "date": "2026-01-12", "relevancy": 2.7078, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5352}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LangHOPS%3A%20Language%20Grounded%20Hierarchical%20Open-Vocabulary%20Part%20Segmentation&body=Title%3A%20LangHOPS%3A%20Language%20Grounded%20Hierarchical%20Open-Vocabulary%20Part%20Segmentation%0AAuthor%3A%20Yang%20Miao%20and%20Jan-Nico%20Zaech%20and%20Xi%20Wang%20and%20Fabien%20Despinoy%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool%0AAbstract%3A%20We%20propose%20LangHOPS%2C%20the%20first%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20based%20framework%20for%20open-vocabulary%20object-part%20instance%20segmentation.%20Given%20an%20image%2C%20LangHOPS%20can%20jointly%20detect%20and%20segment%20hierarchical%20object%20and%20part%20instances%20from%20open-vocabulary%20candidate%20categories.%20Unlike%20prior%20approaches%20that%20rely%20on%20heuristic%20or%20learnable%20visual%20grouping%2C%20our%20approach%20grounds%20object-part%20hierarchies%20in%20language%20space.%20It%20integrates%20the%20MLLM%20into%20the%20object-part%20parsing%20pipeline%20to%20leverage%20its%20rich%20knowledge%20and%20reasoning%20capabilities%2C%20and%20link%20multi-granularity%20concepts%20within%20the%20hierarchies.%20We%20evaluate%20LangHOPS%20across%20multiple%20challenging%20scenarios%2C%20including%20in-domain%20and%20cross-dataset%20object-part%20instance%20segmentation%2C%20and%20zero-shot%20semantic%20segmentation.%20LangHOPS%20achieves%20state-of-the-art%20results%2C%20surpassing%20previous%20methods%20by%205.5%25%20Average%20Precision%20%28AP%29%20%28in-domain%29%20and%204.8%25%20%28cross-dataset%29%20on%20the%20PartImageNet%20dataset%20and%20by%202.5%25%20mIOU%20on%20unseen%20object%20parts%20in%20ADE20K%20%28zero-shot%29.%20Ablation%20studies%20further%20validate%20the%20effectiveness%20of%20the%20language-grounded%20hierarchy%20and%20MLLM%20driven%20part%20query%20refinement%20strategy.%20The%20code%20will%20be%20released%20here.%0ALink%3A%20http%3A//arxiv.org/abs/2510.25263v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLangHOPS%253A%2520Language%2520Grounded%2520Hierarchical%2520Open-Vocabulary%2520Part%2520Segmentation%26entry.906535625%3DYang%2520Miao%2520and%2520Jan-Nico%2520Zaech%2520and%2520Xi%2520Wang%2520and%2520Fabien%2520Despinoy%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3DWe%2520propose%2520LangHOPS%252C%2520the%2520first%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520based%2520framework%2520for%2520open-vocabulary%2520object-part%2520instance%2520segmentation.%2520Given%2520an%2520image%252C%2520LangHOPS%2520can%2520jointly%2520detect%2520and%2520segment%2520hierarchical%2520object%2520and%2520part%2520instances%2520from%2520open-vocabulary%2520candidate%2520categories.%2520Unlike%2520prior%2520approaches%2520that%2520rely%2520on%2520heuristic%2520or%2520learnable%2520visual%2520grouping%252C%2520our%2520approach%2520grounds%2520object-part%2520hierarchies%2520in%2520language%2520space.%2520It%2520integrates%2520the%2520MLLM%2520into%2520the%2520object-part%2520parsing%2520pipeline%2520to%2520leverage%2520its%2520rich%2520knowledge%2520and%2520reasoning%2520capabilities%252C%2520and%2520link%2520multi-granularity%2520concepts%2520within%2520the%2520hierarchies.%2520We%2520evaluate%2520LangHOPS%2520across%2520multiple%2520challenging%2520scenarios%252C%2520including%2520in-domain%2520and%2520cross-dataset%2520object-part%2520instance%2520segmentation%252C%2520and%2520zero-shot%2520semantic%2520segmentation.%2520LangHOPS%2520achieves%2520state-of-the-art%2520results%252C%2520surpassing%2520previous%2520methods%2520by%25205.5%2525%2520Average%2520Precision%2520%2528AP%2529%2520%2528in-domain%2529%2520and%25204.8%2525%2520%2528cross-dataset%2529%2520on%2520the%2520PartImageNet%2520dataset%2520and%2520by%25202.5%2525%2520mIOU%2520on%2520unseen%2520object%2520parts%2520in%2520ADE20K%2520%2528zero-shot%2529.%2520Ablation%2520studies%2520further%2520validate%2520the%2520effectiveness%2520of%2520the%2520language-grounded%2520hierarchy%2520and%2520MLLM%2520driven%2520part%2520query%2520refinement%2520strategy.%2520The%2520code%2520will%2520be%2520released%2520here.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25263v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LangHOPS%3A%20Language%20Grounded%20Hierarchical%20Open-Vocabulary%20Part%20Segmentation&entry.906535625=Yang%20Miao%20and%20Jan-Nico%20Zaech%20and%20Xi%20Wang%20and%20Fabien%20Despinoy%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool&entry.1292438233=We%20propose%20LangHOPS%2C%20the%20first%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20based%20framework%20for%20open-vocabulary%20object-part%20instance%20segmentation.%20Given%20an%20image%2C%20LangHOPS%20can%20jointly%20detect%20and%20segment%20hierarchical%20object%20and%20part%20instances%20from%20open-vocabulary%20candidate%20categories.%20Unlike%20prior%20approaches%20that%20rely%20on%20heuristic%20or%20learnable%20visual%20grouping%2C%20our%20approach%20grounds%20object-part%20hierarchies%20in%20language%20space.%20It%20integrates%20the%20MLLM%20into%20the%20object-part%20parsing%20pipeline%20to%20leverage%20its%20rich%20knowledge%20and%20reasoning%20capabilities%2C%20and%20link%20multi-granularity%20concepts%20within%20the%20hierarchies.%20We%20evaluate%20LangHOPS%20across%20multiple%20challenging%20scenarios%2C%20including%20in-domain%20and%20cross-dataset%20object-part%20instance%20segmentation%2C%20and%20zero-shot%20semantic%20segmentation.%20LangHOPS%20achieves%20state-of-the-art%20results%2C%20surpassing%20previous%20methods%20by%205.5%25%20Average%20Precision%20%28AP%29%20%28in-domain%29%20and%204.8%25%20%28cross-dataset%29%20on%20the%20PartImageNet%20dataset%20and%20by%202.5%25%20mIOU%20on%20unseen%20object%20parts%20in%20ADE20K%20%28zero-shot%29.%20Ablation%20studies%20further%20validate%20the%20effectiveness%20of%20the%20language-grounded%20hierarchy%20and%20MLLM%20driven%20part%20query%20refinement%20strategy.%20The%20code%20will%20be%20released%20here.&entry.1838667208=http%3A//arxiv.org/abs/2510.25263v3&entry.124074799=Read"},
{"title": "SCALPEL: Selective Capability Ablation via Low-rank Parameter Editing for Large Language Model Interpretability Analysis", "author": "Zihao Fu and Xufeng Duan and Zhenguang G. Cai", "abstract": "Large language models excel across diverse domains, yet their deployment in healthcare, legal systems, and autonomous decision-making remains limited by incomplete understanding of their internal mechanisms. As these models integrate into high-stakes systems, understanding how they encode capabilities has become fundamental to interpretability research. Traditional approaches identify important modules through gradient attribution or activation analysis, assuming specific capabilities map to specific components. However, this oversimplifies neural computation: modules may contribute to multiple capabilities simultaneously, while single capabilities may distribute across multiple modules. These coarse-grained analyses fail to capture fine-grained, distributed capability encoding. We present SCALPEL (Selective Capability Ablation via Low-rank Parameter Editing for Large language models), a framework representing capabilities as low-rank parameter subspaces rather than discrete modules. Our key insight is that capabilities can be characterized by low-rank modifications distributed across layers and modules, enabling precise capability removal without affecting others. By training LoRA adapters to reduce distinguishing correct from incorrect answers while preserving general language modeling quality, SCALPEL identifies low-rank representations responsible for particular capabilities while remaining disentangled from others. Experiments across diverse capability and linguistic tasks from BLiMP demonstrate that SCALPEL successfully removes target capabilities while preserving general capabilities, providing fine-grained insights into capability distribution across parameter space. Results reveal that capabilities exhibit low-rank structure and can be selectively ablated through targeted parameter-space interventions, offering nuanced understanding of capability encoding in LLMs.", "link": "http://arxiv.org/abs/2601.07411v1", "date": "2026-01-12", "relevancy": 2.6899, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5469}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCALPEL%3A%20Selective%20Capability%20Ablation%20via%20Low-rank%20Parameter%20Editing%20for%20Large%20Language%20Model%20Interpretability%20Analysis&body=Title%3A%20SCALPEL%3A%20Selective%20Capability%20Ablation%20via%20Low-rank%20Parameter%20Editing%20for%20Large%20Language%20Model%20Interpretability%20Analysis%0AAuthor%3A%20Zihao%20Fu%20and%20Xufeng%20Duan%20and%20Zhenguang%20G.%20Cai%0AAbstract%3A%20Large%20language%20models%20excel%20across%20diverse%20domains%2C%20yet%20their%20deployment%20in%20healthcare%2C%20legal%20systems%2C%20and%20autonomous%20decision-making%20remains%20limited%20by%20incomplete%20understanding%20of%20their%20internal%20mechanisms.%20As%20these%20models%20integrate%20into%20high-stakes%20systems%2C%20understanding%20how%20they%20encode%20capabilities%20has%20become%20fundamental%20to%20interpretability%20research.%20Traditional%20approaches%20identify%20important%20modules%20through%20gradient%20attribution%20or%20activation%20analysis%2C%20assuming%20specific%20capabilities%20map%20to%20specific%20components.%20However%2C%20this%20oversimplifies%20neural%20computation%3A%20modules%20may%20contribute%20to%20multiple%20capabilities%20simultaneously%2C%20while%20single%20capabilities%20may%20distribute%20across%20multiple%20modules.%20These%20coarse-grained%20analyses%20fail%20to%20capture%20fine-grained%2C%20distributed%20capability%20encoding.%20We%20present%20SCALPEL%20%28Selective%20Capability%20Ablation%20via%20Low-rank%20Parameter%20Editing%20for%20Large%20language%20models%29%2C%20a%20framework%20representing%20capabilities%20as%20low-rank%20parameter%20subspaces%20rather%20than%20discrete%20modules.%20Our%20key%20insight%20is%20that%20capabilities%20can%20be%20characterized%20by%20low-rank%20modifications%20distributed%20across%20layers%20and%20modules%2C%20enabling%20precise%20capability%20removal%20without%20affecting%20others.%20By%20training%20LoRA%20adapters%20to%20reduce%20distinguishing%20correct%20from%20incorrect%20answers%20while%20preserving%20general%20language%20modeling%20quality%2C%20SCALPEL%20identifies%20low-rank%20representations%20responsible%20for%20particular%20capabilities%20while%20remaining%20disentangled%20from%20others.%20Experiments%20across%20diverse%20capability%20and%20linguistic%20tasks%20from%20BLiMP%20demonstrate%20that%20SCALPEL%20successfully%20removes%20target%20capabilities%20while%20preserving%20general%20capabilities%2C%20providing%20fine-grained%20insights%20into%20capability%20distribution%20across%20parameter%20space.%20Results%20reveal%20that%20capabilities%20exhibit%20low-rank%20structure%20and%20can%20be%20selectively%20ablated%20through%20targeted%20parameter-space%20interventions%2C%20offering%20nuanced%20understanding%20of%20capability%20encoding%20in%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCALPEL%253A%2520Selective%2520Capability%2520Ablation%2520via%2520Low-rank%2520Parameter%2520Editing%2520for%2520Large%2520Language%2520Model%2520Interpretability%2520Analysis%26entry.906535625%3DZihao%2520Fu%2520and%2520Xufeng%2520Duan%2520and%2520Zhenguang%2520G.%2520Cai%26entry.1292438233%3DLarge%2520language%2520models%2520excel%2520across%2520diverse%2520domains%252C%2520yet%2520their%2520deployment%2520in%2520healthcare%252C%2520legal%2520systems%252C%2520and%2520autonomous%2520decision-making%2520remains%2520limited%2520by%2520incomplete%2520understanding%2520of%2520their%2520internal%2520mechanisms.%2520As%2520these%2520models%2520integrate%2520into%2520high-stakes%2520systems%252C%2520understanding%2520how%2520they%2520encode%2520capabilities%2520has%2520become%2520fundamental%2520to%2520interpretability%2520research.%2520Traditional%2520approaches%2520identify%2520important%2520modules%2520through%2520gradient%2520attribution%2520or%2520activation%2520analysis%252C%2520assuming%2520specific%2520capabilities%2520map%2520to%2520specific%2520components.%2520However%252C%2520this%2520oversimplifies%2520neural%2520computation%253A%2520modules%2520may%2520contribute%2520to%2520multiple%2520capabilities%2520simultaneously%252C%2520while%2520single%2520capabilities%2520may%2520distribute%2520across%2520multiple%2520modules.%2520These%2520coarse-grained%2520analyses%2520fail%2520to%2520capture%2520fine-grained%252C%2520distributed%2520capability%2520encoding.%2520We%2520present%2520SCALPEL%2520%2528Selective%2520Capability%2520Ablation%2520via%2520Low-rank%2520Parameter%2520Editing%2520for%2520Large%2520language%2520models%2529%252C%2520a%2520framework%2520representing%2520capabilities%2520as%2520low-rank%2520parameter%2520subspaces%2520rather%2520than%2520discrete%2520modules.%2520Our%2520key%2520insight%2520is%2520that%2520capabilities%2520can%2520be%2520characterized%2520by%2520low-rank%2520modifications%2520distributed%2520across%2520layers%2520and%2520modules%252C%2520enabling%2520precise%2520capability%2520removal%2520without%2520affecting%2520others.%2520By%2520training%2520LoRA%2520adapters%2520to%2520reduce%2520distinguishing%2520correct%2520from%2520incorrect%2520answers%2520while%2520preserving%2520general%2520language%2520modeling%2520quality%252C%2520SCALPEL%2520identifies%2520low-rank%2520representations%2520responsible%2520for%2520particular%2520capabilities%2520while%2520remaining%2520disentangled%2520from%2520others.%2520Experiments%2520across%2520diverse%2520capability%2520and%2520linguistic%2520tasks%2520from%2520BLiMP%2520demonstrate%2520that%2520SCALPEL%2520successfully%2520removes%2520target%2520capabilities%2520while%2520preserving%2520general%2520capabilities%252C%2520providing%2520fine-grained%2520insights%2520into%2520capability%2520distribution%2520across%2520parameter%2520space.%2520Results%2520reveal%2520that%2520capabilities%2520exhibit%2520low-rank%2520structure%2520and%2520can%2520be%2520selectively%2520ablated%2520through%2520targeted%2520parameter-space%2520interventions%252C%2520offering%2520nuanced%2520understanding%2520of%2520capability%2520encoding%2520in%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCALPEL%3A%20Selective%20Capability%20Ablation%20via%20Low-rank%20Parameter%20Editing%20for%20Large%20Language%20Model%20Interpretability%20Analysis&entry.906535625=Zihao%20Fu%20and%20Xufeng%20Duan%20and%20Zhenguang%20G.%20Cai&entry.1292438233=Large%20language%20models%20excel%20across%20diverse%20domains%2C%20yet%20their%20deployment%20in%20healthcare%2C%20legal%20systems%2C%20and%20autonomous%20decision-making%20remains%20limited%20by%20incomplete%20understanding%20of%20their%20internal%20mechanisms.%20As%20these%20models%20integrate%20into%20high-stakes%20systems%2C%20understanding%20how%20they%20encode%20capabilities%20has%20become%20fundamental%20to%20interpretability%20research.%20Traditional%20approaches%20identify%20important%20modules%20through%20gradient%20attribution%20or%20activation%20analysis%2C%20assuming%20specific%20capabilities%20map%20to%20specific%20components.%20However%2C%20this%20oversimplifies%20neural%20computation%3A%20modules%20may%20contribute%20to%20multiple%20capabilities%20simultaneously%2C%20while%20single%20capabilities%20may%20distribute%20across%20multiple%20modules.%20These%20coarse-grained%20analyses%20fail%20to%20capture%20fine-grained%2C%20distributed%20capability%20encoding.%20We%20present%20SCALPEL%20%28Selective%20Capability%20Ablation%20via%20Low-rank%20Parameter%20Editing%20for%20Large%20language%20models%29%2C%20a%20framework%20representing%20capabilities%20as%20low-rank%20parameter%20subspaces%20rather%20than%20discrete%20modules.%20Our%20key%20insight%20is%20that%20capabilities%20can%20be%20characterized%20by%20low-rank%20modifications%20distributed%20across%20layers%20and%20modules%2C%20enabling%20precise%20capability%20removal%20without%20affecting%20others.%20By%20training%20LoRA%20adapters%20to%20reduce%20distinguishing%20correct%20from%20incorrect%20answers%20while%20preserving%20general%20language%20modeling%20quality%2C%20SCALPEL%20identifies%20low-rank%20representations%20responsible%20for%20particular%20capabilities%20while%20remaining%20disentangled%20from%20others.%20Experiments%20across%20diverse%20capability%20and%20linguistic%20tasks%20from%20BLiMP%20demonstrate%20that%20SCALPEL%20successfully%20removes%20target%20capabilities%20while%20preserving%20general%20capabilities%2C%20providing%20fine-grained%20insights%20into%20capability%20distribution%20across%20parameter%20space.%20Results%20reveal%20that%20capabilities%20exhibit%20low-rank%20structure%20and%20can%20be%20selectively%20ablated%20through%20targeted%20parameter-space%20interventions%2C%20offering%20nuanced%20understanding%20of%20capability%20encoding%20in%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2601.07411v1&entry.124074799=Read"},
{"title": "Vision-Language Model for Accurate Crater Detection", "author": "Patrick Bauer and Marius Schwinning and Florian Renk and Andreas Weinmann and Hichem Snoussi", "abstract": "The European Space Agency (ESA), driven by its ambitions on planned lunar missions with the Argonaut lander, has a profound interest in reliable crater detection, since craters pose a risk to safe lunar landings. This task is usually addressed with automated crater detection algorithms (CDA) based on deep learning techniques. It is non-trivial due to the vast amount of craters of various sizes and shapes, as well as challenging conditions such as varying illumination and rugged terrain. Therefore, we propose a deep-learning CDA based on the OWLv2 model, which is built on a Vision Transformer, that has proven highly effective in various computer vision tasks. For fine-tuning, we utilize a manually labeled dataset fom the IMPACT project, that provides crater annotations on high-resolution Lunar Reconnaissance Orbiter Camera Calibrated Data Record images. We insert trainable parameters using a parameter-efficient fine-tuning strategy with Low-Rank Adaptation, and optimize a combined loss function consisting of Complete Intersection over Union (CIoU) for localization and a contrastive loss for classification. We achieve satisfactory visual results, along with a maximum recall of 94.0% and a maximum precision of 73.1% on a test dataset from IMPACT. Our method achieves reliable crater detection across challenging lunar imaging conditions, paving the way for robust crater analysis in future lunar exploration.", "link": "http://arxiv.org/abs/2601.07795v1", "date": "2026-01-12", "relevancy": 2.6836, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5426}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5426}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Model%20for%20Accurate%20Crater%20Detection&body=Title%3A%20Vision-Language%20Model%20for%20Accurate%20Crater%20Detection%0AAuthor%3A%20Patrick%20Bauer%20and%20Marius%20Schwinning%20and%20Florian%20Renk%20and%20Andreas%20Weinmann%20and%20Hichem%20Snoussi%0AAbstract%3A%20The%20European%20Space%20Agency%20%28ESA%29%2C%20driven%20by%20its%20ambitions%20on%20planned%20lunar%20missions%20with%20the%20Argonaut%20lander%2C%20has%20a%20profound%20interest%20in%20reliable%20crater%20detection%2C%20since%20craters%20pose%20a%20risk%20to%20safe%20lunar%20landings.%20This%20task%20is%20usually%20addressed%20with%20automated%20crater%20detection%20algorithms%20%28CDA%29%20based%20on%20deep%20learning%20techniques.%20It%20is%20non-trivial%20due%20to%20the%20vast%20amount%20of%20craters%20of%20various%20sizes%20and%20shapes%2C%20as%20well%20as%20challenging%20conditions%20such%20as%20varying%20illumination%20and%20rugged%20terrain.%20Therefore%2C%20we%20propose%20a%20deep-learning%20CDA%20based%20on%20the%20OWLv2%20model%2C%20which%20is%20built%20on%20a%20Vision%20Transformer%2C%20that%20has%20proven%20highly%20effective%20in%20various%20computer%20vision%20tasks.%20For%20fine-tuning%2C%20we%20utilize%20a%20manually%20labeled%20dataset%20fom%20the%20IMPACT%20project%2C%20that%20provides%20crater%20annotations%20on%20high-resolution%20Lunar%20Reconnaissance%20Orbiter%20Camera%20Calibrated%20Data%20Record%20images.%20We%20insert%20trainable%20parameters%20using%20a%20parameter-efficient%20fine-tuning%20strategy%20with%20Low-Rank%20Adaptation%2C%20and%20optimize%20a%20combined%20loss%20function%20consisting%20of%20Complete%20Intersection%20over%20Union%20%28CIoU%29%20for%20localization%20and%20a%20contrastive%20loss%20for%20classification.%20We%20achieve%20satisfactory%20visual%20results%2C%20along%20with%20a%20maximum%20recall%20of%2094.0%25%20and%20a%20maximum%20precision%20of%2073.1%25%20on%20a%20test%20dataset%20from%20IMPACT.%20Our%20method%20achieves%20reliable%20crater%20detection%20across%20challenging%20lunar%20imaging%20conditions%2C%20paving%20the%20way%20for%20robust%20crater%20analysis%20in%20future%20lunar%20exploration.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Model%2520for%2520Accurate%2520Crater%2520Detection%26entry.906535625%3DPatrick%2520Bauer%2520and%2520Marius%2520Schwinning%2520and%2520Florian%2520Renk%2520and%2520Andreas%2520Weinmann%2520and%2520Hichem%2520Snoussi%26entry.1292438233%3DThe%2520European%2520Space%2520Agency%2520%2528ESA%2529%252C%2520driven%2520by%2520its%2520ambitions%2520on%2520planned%2520lunar%2520missions%2520with%2520the%2520Argonaut%2520lander%252C%2520has%2520a%2520profound%2520interest%2520in%2520reliable%2520crater%2520detection%252C%2520since%2520craters%2520pose%2520a%2520risk%2520to%2520safe%2520lunar%2520landings.%2520This%2520task%2520is%2520usually%2520addressed%2520with%2520automated%2520crater%2520detection%2520algorithms%2520%2528CDA%2529%2520based%2520on%2520deep%2520learning%2520techniques.%2520It%2520is%2520non-trivial%2520due%2520to%2520the%2520vast%2520amount%2520of%2520craters%2520of%2520various%2520sizes%2520and%2520shapes%252C%2520as%2520well%2520as%2520challenging%2520conditions%2520such%2520as%2520varying%2520illumination%2520and%2520rugged%2520terrain.%2520Therefore%252C%2520we%2520propose%2520a%2520deep-learning%2520CDA%2520based%2520on%2520the%2520OWLv2%2520model%252C%2520which%2520is%2520built%2520on%2520a%2520Vision%2520Transformer%252C%2520that%2520has%2520proven%2520highly%2520effective%2520in%2520various%2520computer%2520vision%2520tasks.%2520For%2520fine-tuning%252C%2520we%2520utilize%2520a%2520manually%2520labeled%2520dataset%2520fom%2520the%2520IMPACT%2520project%252C%2520that%2520provides%2520crater%2520annotations%2520on%2520high-resolution%2520Lunar%2520Reconnaissance%2520Orbiter%2520Camera%2520Calibrated%2520Data%2520Record%2520images.%2520We%2520insert%2520trainable%2520parameters%2520using%2520a%2520parameter-efficient%2520fine-tuning%2520strategy%2520with%2520Low-Rank%2520Adaptation%252C%2520and%2520optimize%2520a%2520combined%2520loss%2520function%2520consisting%2520of%2520Complete%2520Intersection%2520over%2520Union%2520%2528CIoU%2529%2520for%2520localization%2520and%2520a%2520contrastive%2520loss%2520for%2520classification.%2520We%2520achieve%2520satisfactory%2520visual%2520results%252C%2520along%2520with%2520a%2520maximum%2520recall%2520of%252094.0%2525%2520and%2520a%2520maximum%2520precision%2520of%252073.1%2525%2520on%2520a%2520test%2520dataset%2520from%2520IMPACT.%2520Our%2520method%2520achieves%2520reliable%2520crater%2520detection%2520across%2520challenging%2520lunar%2520imaging%2520conditions%252C%2520paving%2520the%2520way%2520for%2520robust%2520crater%2520analysis%2520in%2520future%2520lunar%2520exploration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Model%20for%20Accurate%20Crater%20Detection&entry.906535625=Patrick%20Bauer%20and%20Marius%20Schwinning%20and%20Florian%20Renk%20and%20Andreas%20Weinmann%20and%20Hichem%20Snoussi&entry.1292438233=The%20European%20Space%20Agency%20%28ESA%29%2C%20driven%20by%20its%20ambitions%20on%20planned%20lunar%20missions%20with%20the%20Argonaut%20lander%2C%20has%20a%20profound%20interest%20in%20reliable%20crater%20detection%2C%20since%20craters%20pose%20a%20risk%20to%20safe%20lunar%20landings.%20This%20task%20is%20usually%20addressed%20with%20automated%20crater%20detection%20algorithms%20%28CDA%29%20based%20on%20deep%20learning%20techniques.%20It%20is%20non-trivial%20due%20to%20the%20vast%20amount%20of%20craters%20of%20various%20sizes%20and%20shapes%2C%20as%20well%20as%20challenging%20conditions%20such%20as%20varying%20illumination%20and%20rugged%20terrain.%20Therefore%2C%20we%20propose%20a%20deep-learning%20CDA%20based%20on%20the%20OWLv2%20model%2C%20which%20is%20built%20on%20a%20Vision%20Transformer%2C%20that%20has%20proven%20highly%20effective%20in%20various%20computer%20vision%20tasks.%20For%20fine-tuning%2C%20we%20utilize%20a%20manually%20labeled%20dataset%20fom%20the%20IMPACT%20project%2C%20that%20provides%20crater%20annotations%20on%20high-resolution%20Lunar%20Reconnaissance%20Orbiter%20Camera%20Calibrated%20Data%20Record%20images.%20We%20insert%20trainable%20parameters%20using%20a%20parameter-efficient%20fine-tuning%20strategy%20with%20Low-Rank%20Adaptation%2C%20and%20optimize%20a%20combined%20loss%20function%20consisting%20of%20Complete%20Intersection%20over%20Union%20%28CIoU%29%20for%20localization%20and%20a%20contrastive%20loss%20for%20classification.%20We%20achieve%20satisfactory%20visual%20results%2C%20along%20with%20a%20maximum%20recall%20of%2094.0%25%20and%20a%20maximum%20precision%20of%2073.1%25%20on%20a%20test%20dataset%20from%20IMPACT.%20Our%20method%20achieves%20reliable%20crater%20detection%20across%20challenging%20lunar%20imaging%20conditions%2C%20paving%20the%20way%20for%20robust%20crater%20analysis%20in%20future%20lunar%20exploration.&entry.1838667208=http%3A//arxiv.org/abs/2601.07795v1&entry.124074799=Read"},
{"title": "Advancing Multinational License Plate Recognition Through Synthetic and Real Data Fusion: A Comprehensive Evaluation", "author": "Rayson Laroca and Valter Estevam and Gladston J. P. Moreira and Rodrigo Minetto and David Menotti", "abstract": "Automatic License Plate Recognition is a frequent research topic due to its wide-ranging practical applications. While recent studies use synthetic images to improve License Plate Recognition (LPR) results, there remain several limitations in these efforts. This work addresses these constraints by comprehensively exploring the integration of real and synthetic data to enhance LPR performance. We subject 16 Optical Character Recognition (OCR) models to a benchmarking process involving 12 public datasets acquired from various regions. Several key findings emerge from our investigation. Primarily, the massive incorporation of synthetic data substantially boosts model performance in both intra- and cross-dataset scenarios. We examine three distinct methodologies for generating synthetic data: template-based generation, character permutation, and utilizing a Generative Adversarial Network (GAN) model, each contributing significantly to performance enhancement. The combined use of these methodologies demonstrates a notable synergistic effect, leading to end-to-end results that surpass those reached by state-of-the-art methods and established commercial systems. Our experiments also underscore the efficacy of synthetic data in mitigating challenges posed by limited training data, enabling remarkable results to be achieved even with small fractions of the original training data. Finally, we investigate the trade-off between accuracy and speed among different models, identifying those that strike the optimal balance in each intra-dataset and cross-dataset settings.", "link": "http://arxiv.org/abs/2601.07671v1", "date": "2026-01-12", "relevancy": 2.6706, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5388}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5363}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Multinational%20License%20Plate%20Recognition%20Through%20Synthetic%20and%20Real%20Data%20Fusion%3A%20A%20Comprehensive%20Evaluation&body=Title%3A%20Advancing%20Multinational%20License%20Plate%20Recognition%20Through%20Synthetic%20and%20Real%20Data%20Fusion%3A%20A%20Comprehensive%20Evaluation%0AAuthor%3A%20Rayson%20Laroca%20and%20Valter%20Estevam%20and%20Gladston%20J.%20P.%20Moreira%20and%20Rodrigo%20Minetto%20and%20David%20Menotti%0AAbstract%3A%20Automatic%20License%20Plate%20Recognition%20is%20a%20frequent%20research%20topic%20due%20to%20its%20wide-ranging%20practical%20applications.%20While%20recent%20studies%20use%20synthetic%20images%20to%20improve%20License%20Plate%20Recognition%20%28LPR%29%20results%2C%20there%20remain%20several%20limitations%20in%20these%20efforts.%20This%20work%20addresses%20these%20constraints%20by%20comprehensively%20exploring%20the%20integration%20of%20real%20and%20synthetic%20data%20to%20enhance%20LPR%20performance.%20We%20subject%2016%20Optical%20Character%20Recognition%20%28OCR%29%20models%20to%20a%20benchmarking%20process%20involving%2012%20public%20datasets%20acquired%20from%20various%20regions.%20Several%20key%20findings%20emerge%20from%20our%20investigation.%20Primarily%2C%20the%20massive%20incorporation%20of%20synthetic%20data%20substantially%20boosts%20model%20performance%20in%20both%20intra-%20and%20cross-dataset%20scenarios.%20We%20examine%20three%20distinct%20methodologies%20for%20generating%20synthetic%20data%3A%20template-based%20generation%2C%20character%20permutation%2C%20and%20utilizing%20a%20Generative%20Adversarial%20Network%20%28GAN%29%20model%2C%20each%20contributing%20significantly%20to%20performance%20enhancement.%20The%20combined%20use%20of%20these%20methodologies%20demonstrates%20a%20notable%20synergistic%20effect%2C%20leading%20to%20end-to-end%20results%20that%20surpass%20those%20reached%20by%20state-of-the-art%20methods%20and%20established%20commercial%20systems.%20Our%20experiments%20also%20underscore%20the%20efficacy%20of%20synthetic%20data%20in%20mitigating%20challenges%20posed%20by%20limited%20training%20data%2C%20enabling%20remarkable%20results%20to%20be%20achieved%20even%20with%20small%20fractions%20of%20the%20original%20training%20data.%20Finally%2C%20we%20investigate%20the%20trade-off%20between%20accuracy%20and%20speed%20among%20different%20models%2C%20identifying%20those%20that%20strike%20the%20optimal%20balance%20in%20each%20intra-dataset%20and%20cross-dataset%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Multinational%2520License%2520Plate%2520Recognition%2520Through%2520Synthetic%2520and%2520Real%2520Data%2520Fusion%253A%2520A%2520Comprehensive%2520Evaluation%26entry.906535625%3DRayson%2520Laroca%2520and%2520Valter%2520Estevam%2520and%2520Gladston%2520J.%2520P.%2520Moreira%2520and%2520Rodrigo%2520Minetto%2520and%2520David%2520Menotti%26entry.1292438233%3DAutomatic%2520License%2520Plate%2520Recognition%2520is%2520a%2520frequent%2520research%2520topic%2520due%2520to%2520its%2520wide-ranging%2520practical%2520applications.%2520While%2520recent%2520studies%2520use%2520synthetic%2520images%2520to%2520improve%2520License%2520Plate%2520Recognition%2520%2528LPR%2529%2520results%252C%2520there%2520remain%2520several%2520limitations%2520in%2520these%2520efforts.%2520This%2520work%2520addresses%2520these%2520constraints%2520by%2520comprehensively%2520exploring%2520the%2520integration%2520of%2520real%2520and%2520synthetic%2520data%2520to%2520enhance%2520LPR%2520performance.%2520We%2520subject%252016%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%2520models%2520to%2520a%2520benchmarking%2520process%2520involving%252012%2520public%2520datasets%2520acquired%2520from%2520various%2520regions.%2520Several%2520key%2520findings%2520emerge%2520from%2520our%2520investigation.%2520Primarily%252C%2520the%2520massive%2520incorporation%2520of%2520synthetic%2520data%2520substantially%2520boosts%2520model%2520performance%2520in%2520both%2520intra-%2520and%2520cross-dataset%2520scenarios.%2520We%2520examine%2520three%2520distinct%2520methodologies%2520for%2520generating%2520synthetic%2520data%253A%2520template-based%2520generation%252C%2520character%2520permutation%252C%2520and%2520utilizing%2520a%2520Generative%2520Adversarial%2520Network%2520%2528GAN%2529%2520model%252C%2520each%2520contributing%2520significantly%2520to%2520performance%2520enhancement.%2520The%2520combined%2520use%2520of%2520these%2520methodologies%2520demonstrates%2520a%2520notable%2520synergistic%2520effect%252C%2520leading%2520to%2520end-to-end%2520results%2520that%2520surpass%2520those%2520reached%2520by%2520state-of-the-art%2520methods%2520and%2520established%2520commercial%2520systems.%2520Our%2520experiments%2520also%2520underscore%2520the%2520efficacy%2520of%2520synthetic%2520data%2520in%2520mitigating%2520challenges%2520posed%2520by%2520limited%2520training%2520data%252C%2520enabling%2520remarkable%2520results%2520to%2520be%2520achieved%2520even%2520with%2520small%2520fractions%2520of%2520the%2520original%2520training%2520data.%2520Finally%252C%2520we%2520investigate%2520the%2520trade-off%2520between%2520accuracy%2520and%2520speed%2520among%2520different%2520models%252C%2520identifying%2520those%2520that%2520strike%2520the%2520optimal%2520balance%2520in%2520each%2520intra-dataset%2520and%2520cross-dataset%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Multinational%20License%20Plate%20Recognition%20Through%20Synthetic%20and%20Real%20Data%20Fusion%3A%20A%20Comprehensive%20Evaluation&entry.906535625=Rayson%20Laroca%20and%20Valter%20Estevam%20and%20Gladston%20J.%20P.%20Moreira%20and%20Rodrigo%20Minetto%20and%20David%20Menotti&entry.1292438233=Automatic%20License%20Plate%20Recognition%20is%20a%20frequent%20research%20topic%20due%20to%20its%20wide-ranging%20practical%20applications.%20While%20recent%20studies%20use%20synthetic%20images%20to%20improve%20License%20Plate%20Recognition%20%28LPR%29%20results%2C%20there%20remain%20several%20limitations%20in%20these%20efforts.%20This%20work%20addresses%20these%20constraints%20by%20comprehensively%20exploring%20the%20integration%20of%20real%20and%20synthetic%20data%20to%20enhance%20LPR%20performance.%20We%20subject%2016%20Optical%20Character%20Recognition%20%28OCR%29%20models%20to%20a%20benchmarking%20process%20involving%2012%20public%20datasets%20acquired%20from%20various%20regions.%20Several%20key%20findings%20emerge%20from%20our%20investigation.%20Primarily%2C%20the%20massive%20incorporation%20of%20synthetic%20data%20substantially%20boosts%20model%20performance%20in%20both%20intra-%20and%20cross-dataset%20scenarios.%20We%20examine%20three%20distinct%20methodologies%20for%20generating%20synthetic%20data%3A%20template-based%20generation%2C%20character%20permutation%2C%20and%20utilizing%20a%20Generative%20Adversarial%20Network%20%28GAN%29%20model%2C%20each%20contributing%20significantly%20to%20performance%20enhancement.%20The%20combined%20use%20of%20these%20methodologies%20demonstrates%20a%20notable%20synergistic%20effect%2C%20leading%20to%20end-to-end%20results%20that%20surpass%20those%20reached%20by%20state-of-the-art%20methods%20and%20established%20commercial%20systems.%20Our%20experiments%20also%20underscore%20the%20efficacy%20of%20synthetic%20data%20in%20mitigating%20challenges%20posed%20by%20limited%20training%20data%2C%20enabling%20remarkable%20results%20to%20be%20achieved%20even%20with%20small%20fractions%20of%20the%20original%20training%20data.%20Finally%2C%20we%20investigate%20the%20trade-off%20between%20accuracy%20and%20speed%20among%20different%20models%2C%20identifying%20those%20that%20strike%20the%20optimal%20balance%20in%20each%20intra-dataset%20and%20cross-dataset%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2601.07671v1&entry.124074799=Read"},
{"title": "Studying Illustrations in Manuscripts: An Efficient Deep-Learning Approach", "author": "Yoav Evron and Michal Bar-Asher Siegal and Michael Fire", "abstract": "The recent Artificial Intelligence (AI) revolution has opened transformative possibilities for the humanities, particularly in unlocking the visual-artistic content embedded in historical illuminated manuscripts. While digital archives now offer unprecedented access to these materials, the ability to systematically locate, extract, and analyze illustrations at scale remains a major challenge. We present a general and scalable AI-based pipeline for large-scale visual analysis of illuminated manuscripts. The framework integrates modern deep-learning models for page-level illustration detection, illustration extraction, and multimodal description, enabling scholars to search, cluster, and study visual materials and artistic trends across entire corpora. We demonstrate the applicability of this approach on large heterogeneous collections, including the Vatican Library and richly illuminated manuscripts such as the Bible of Borso d'Este. The system reveals meaningful visual patterns and cross-manuscript relationships by embedding illustrations into a shared representation space and analyzing their similarity structure (see figure 4). By harnessing recent advances in computer vision and vision-language models, our framework enables new forms of large-scale visual scholarship in historical studies, art history, and cultural heritage making it possible to explore iconography, stylistic trends, and cultural connections in ways that were previously impractical.", "link": "http://arxiv.org/abs/2601.05269v2", "date": "2026-01-12", "relevancy": 2.6397, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Studying%20Illustrations%20in%20Manuscripts%3A%20An%20Efficient%20Deep-Learning%20Approach&body=Title%3A%20Studying%20Illustrations%20in%20Manuscripts%3A%20An%20Efficient%20Deep-Learning%20Approach%0AAuthor%3A%20Yoav%20Evron%20and%20Michal%20Bar-Asher%20Siegal%20and%20Michael%20Fire%0AAbstract%3A%20The%20recent%20Artificial%20Intelligence%20%28AI%29%20revolution%20has%20opened%20transformative%20possibilities%20for%20the%20humanities%2C%20particularly%20in%20unlocking%20the%20visual-artistic%20content%20embedded%20in%20historical%20illuminated%20manuscripts.%20While%20digital%20archives%20now%20offer%20unprecedented%20access%20to%20these%20materials%2C%20the%20ability%20to%20systematically%20locate%2C%20extract%2C%20and%20analyze%20illustrations%20at%20scale%20remains%20a%20major%20challenge.%20We%20present%20a%20general%20and%20scalable%20AI-based%20pipeline%20for%20large-scale%20visual%20analysis%20of%20illuminated%20manuscripts.%20The%20framework%20integrates%20modern%20deep-learning%20models%20for%20page-level%20illustration%20detection%2C%20illustration%20extraction%2C%20and%20multimodal%20description%2C%20enabling%20scholars%20to%20search%2C%20cluster%2C%20and%20study%20visual%20materials%20and%20artistic%20trends%20across%20entire%20corpora.%20We%20demonstrate%20the%20applicability%20of%20this%20approach%20on%20large%20heterogeneous%20collections%2C%20including%20the%20Vatican%20Library%20and%20richly%20illuminated%20manuscripts%20such%20as%20the%20Bible%20of%20Borso%20d%27Este.%20The%20system%20reveals%20meaningful%20visual%20patterns%20and%20cross-manuscript%20relationships%20by%20embedding%20illustrations%20into%20a%20shared%20representation%20space%20and%20analyzing%20their%20similarity%20structure%20%28see%20figure%204%29.%20By%20harnessing%20recent%20advances%20in%20computer%20vision%20and%20vision-language%20models%2C%20our%20framework%20enables%20new%20forms%20of%20large-scale%20visual%20scholarship%20in%20historical%20studies%2C%20art%20history%2C%20and%20cultural%20heritage%20making%20it%20possible%20to%20explore%20iconography%2C%20stylistic%20trends%2C%20and%20cultural%20connections%20in%20ways%20that%20were%20previously%20impractical.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05269v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStudying%2520Illustrations%2520in%2520Manuscripts%253A%2520An%2520Efficient%2520Deep-Learning%2520Approach%26entry.906535625%3DYoav%2520Evron%2520and%2520Michal%2520Bar-Asher%2520Siegal%2520and%2520Michael%2520Fire%26entry.1292438233%3DThe%2520recent%2520Artificial%2520Intelligence%2520%2528AI%2529%2520revolution%2520has%2520opened%2520transformative%2520possibilities%2520for%2520the%2520humanities%252C%2520particularly%2520in%2520unlocking%2520the%2520visual-artistic%2520content%2520embedded%2520in%2520historical%2520illuminated%2520manuscripts.%2520While%2520digital%2520archives%2520now%2520offer%2520unprecedented%2520access%2520to%2520these%2520materials%252C%2520the%2520ability%2520to%2520systematically%2520locate%252C%2520extract%252C%2520and%2520analyze%2520illustrations%2520at%2520scale%2520remains%2520a%2520major%2520challenge.%2520We%2520present%2520a%2520general%2520and%2520scalable%2520AI-based%2520pipeline%2520for%2520large-scale%2520visual%2520analysis%2520of%2520illuminated%2520manuscripts.%2520The%2520framework%2520integrates%2520modern%2520deep-learning%2520models%2520for%2520page-level%2520illustration%2520detection%252C%2520illustration%2520extraction%252C%2520and%2520multimodal%2520description%252C%2520enabling%2520scholars%2520to%2520search%252C%2520cluster%252C%2520and%2520study%2520visual%2520materials%2520and%2520artistic%2520trends%2520across%2520entire%2520corpora.%2520We%2520demonstrate%2520the%2520applicability%2520of%2520this%2520approach%2520on%2520large%2520heterogeneous%2520collections%252C%2520including%2520the%2520Vatican%2520Library%2520and%2520richly%2520illuminated%2520manuscripts%2520such%2520as%2520the%2520Bible%2520of%2520Borso%2520d%2527Este.%2520The%2520system%2520reveals%2520meaningful%2520visual%2520patterns%2520and%2520cross-manuscript%2520relationships%2520by%2520embedding%2520illustrations%2520into%2520a%2520shared%2520representation%2520space%2520and%2520analyzing%2520their%2520similarity%2520structure%2520%2528see%2520figure%25204%2529.%2520By%2520harnessing%2520recent%2520advances%2520in%2520computer%2520vision%2520and%2520vision-language%2520models%252C%2520our%2520framework%2520enables%2520new%2520forms%2520of%2520large-scale%2520visual%2520scholarship%2520in%2520historical%2520studies%252C%2520art%2520history%252C%2520and%2520cultural%2520heritage%2520making%2520it%2520possible%2520to%2520explore%2520iconography%252C%2520stylistic%2520trends%252C%2520and%2520cultural%2520connections%2520in%2520ways%2520that%2520were%2520previously%2520impractical.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05269v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Studying%20Illustrations%20in%20Manuscripts%3A%20An%20Efficient%20Deep-Learning%20Approach&entry.906535625=Yoav%20Evron%20and%20Michal%20Bar-Asher%20Siegal%20and%20Michael%20Fire&entry.1292438233=The%20recent%20Artificial%20Intelligence%20%28AI%29%20revolution%20has%20opened%20transformative%20possibilities%20for%20the%20humanities%2C%20particularly%20in%20unlocking%20the%20visual-artistic%20content%20embedded%20in%20historical%20illuminated%20manuscripts.%20While%20digital%20archives%20now%20offer%20unprecedented%20access%20to%20these%20materials%2C%20the%20ability%20to%20systematically%20locate%2C%20extract%2C%20and%20analyze%20illustrations%20at%20scale%20remains%20a%20major%20challenge.%20We%20present%20a%20general%20and%20scalable%20AI-based%20pipeline%20for%20large-scale%20visual%20analysis%20of%20illuminated%20manuscripts.%20The%20framework%20integrates%20modern%20deep-learning%20models%20for%20page-level%20illustration%20detection%2C%20illustration%20extraction%2C%20and%20multimodal%20description%2C%20enabling%20scholars%20to%20search%2C%20cluster%2C%20and%20study%20visual%20materials%20and%20artistic%20trends%20across%20entire%20corpora.%20We%20demonstrate%20the%20applicability%20of%20this%20approach%20on%20large%20heterogeneous%20collections%2C%20including%20the%20Vatican%20Library%20and%20richly%20illuminated%20manuscripts%20such%20as%20the%20Bible%20of%20Borso%20d%27Este.%20The%20system%20reveals%20meaningful%20visual%20patterns%20and%20cross-manuscript%20relationships%20by%20embedding%20illustrations%20into%20a%20shared%20representation%20space%20and%20analyzing%20their%20similarity%20structure%20%28see%20figure%204%29.%20By%20harnessing%20recent%20advances%20in%20computer%20vision%20and%20vision-language%20models%2C%20our%20framework%20enables%20new%20forms%20of%20large-scale%20visual%20scholarship%20in%20historical%20studies%2C%20art%20history%2C%20and%20cultural%20heritage%20making%20it%20possible%20to%20explore%20iconography%2C%20stylistic%20trends%2C%20and%20cultural%20connections%20in%20ways%20that%20were%20previously%20impractical.&entry.1838667208=http%3A//arxiv.org/abs/2601.05269v2&entry.124074799=Read"},
{"title": "Variational Contrastive Learning for Skeleton-based Action Recognition", "author": "Dang Dinh Nguyen and Decky Aspandi Latif and Titus Zaharia", "abstract": "In recent years, self-supervised representation learning for skeleton-based action recognition has advanced with the development of contrastive learning methods. However, most of contrastive paradigms are inherently discriminative and often struggle to capture the variability and uncertainty intrinsic to human motion. To address this issue, we propose a variational contrastive learning framework that integrates probabilistic latent modeling with contrastive self-supervised learning. This formulation enables the learning of structured and semantically meaningful representations that generalize across different datasets and supervision levels. Extensive experiments on three widely used skeleton-based action recognition benchmarks show that our proposed method consistently outperforms existing approaches, particularly in low-label regimes. Moreover, qualitative analyses show that the features provided by our method are more relevant given the motion and sample characteristics, with more focus on important skeleton joints, when compared to the other methods.", "link": "http://arxiv.org/abs/2601.07666v1", "date": "2026-01-12", "relevancy": 2.6382, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.546}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5282}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Contrastive%20Learning%20for%20Skeleton-based%20Action%20Recognition&body=Title%3A%20Variational%20Contrastive%20Learning%20for%20Skeleton-based%20Action%20Recognition%0AAuthor%3A%20Dang%20Dinh%20Nguyen%20and%20Decky%20Aspandi%20Latif%20and%20Titus%20Zaharia%0AAbstract%3A%20In%20recent%20years%2C%20self-supervised%20representation%20learning%20for%20skeleton-based%20action%20recognition%20has%20advanced%20with%20the%20development%20of%20contrastive%20learning%20methods.%20However%2C%20most%20of%20contrastive%20paradigms%20are%20inherently%20discriminative%20and%20often%20struggle%20to%20capture%20the%20variability%20and%20uncertainty%20intrinsic%20to%20human%20motion.%20To%20address%20this%20issue%2C%20we%20propose%20a%20variational%20contrastive%20learning%20framework%20that%20integrates%20probabilistic%20latent%20modeling%20with%20contrastive%20self-supervised%20learning.%20This%20formulation%20enables%20the%20learning%20of%20structured%20and%20semantically%20meaningful%20representations%20that%20generalize%20across%20different%20datasets%20and%20supervision%20levels.%20Extensive%20experiments%20on%20three%20widely%20used%20skeleton-based%20action%20recognition%20benchmarks%20show%20that%20our%20proposed%20method%20consistently%20outperforms%20existing%20approaches%2C%20particularly%20in%20low-label%20regimes.%20Moreover%2C%20qualitative%20analyses%20show%20that%20the%20features%20provided%20by%20our%20method%20are%20more%20relevant%20given%20the%20motion%20and%20sample%20characteristics%2C%20with%20more%20focus%20on%20important%20skeleton%20joints%2C%20when%20compared%20to%20the%20other%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Contrastive%2520Learning%2520for%2520Skeleton-based%2520Action%2520Recognition%26entry.906535625%3DDang%2520Dinh%2520Nguyen%2520and%2520Decky%2520Aspandi%2520Latif%2520and%2520Titus%2520Zaharia%26entry.1292438233%3DIn%2520recent%2520years%252C%2520self-supervised%2520representation%2520learning%2520for%2520skeleton-based%2520action%2520recognition%2520has%2520advanced%2520with%2520the%2520development%2520of%2520contrastive%2520learning%2520methods.%2520However%252C%2520most%2520of%2520contrastive%2520paradigms%2520are%2520inherently%2520discriminative%2520and%2520often%2520struggle%2520to%2520capture%2520the%2520variability%2520and%2520uncertainty%2520intrinsic%2520to%2520human%2520motion.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520variational%2520contrastive%2520learning%2520framework%2520that%2520integrates%2520probabilistic%2520latent%2520modeling%2520with%2520contrastive%2520self-supervised%2520learning.%2520This%2520formulation%2520enables%2520the%2520learning%2520of%2520structured%2520and%2520semantically%2520meaningful%2520representations%2520that%2520generalize%2520across%2520different%2520datasets%2520and%2520supervision%2520levels.%2520Extensive%2520experiments%2520on%2520three%2520widely%2520used%2520skeleton-based%2520action%2520recognition%2520benchmarks%2520show%2520that%2520our%2520proposed%2520method%2520consistently%2520outperforms%2520existing%2520approaches%252C%2520particularly%2520in%2520low-label%2520regimes.%2520Moreover%252C%2520qualitative%2520analyses%2520show%2520that%2520the%2520features%2520provided%2520by%2520our%2520method%2520are%2520more%2520relevant%2520given%2520the%2520motion%2520and%2520sample%2520characteristics%252C%2520with%2520more%2520focus%2520on%2520important%2520skeleton%2520joints%252C%2520when%2520compared%2520to%2520the%2520other%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Contrastive%20Learning%20for%20Skeleton-based%20Action%20Recognition&entry.906535625=Dang%20Dinh%20Nguyen%20and%20Decky%20Aspandi%20Latif%20and%20Titus%20Zaharia&entry.1292438233=In%20recent%20years%2C%20self-supervised%20representation%20learning%20for%20skeleton-based%20action%20recognition%20has%20advanced%20with%20the%20development%20of%20contrastive%20learning%20methods.%20However%2C%20most%20of%20contrastive%20paradigms%20are%20inherently%20discriminative%20and%20often%20struggle%20to%20capture%20the%20variability%20and%20uncertainty%20intrinsic%20to%20human%20motion.%20To%20address%20this%20issue%2C%20we%20propose%20a%20variational%20contrastive%20learning%20framework%20that%20integrates%20probabilistic%20latent%20modeling%20with%20contrastive%20self-supervised%20learning.%20This%20formulation%20enables%20the%20learning%20of%20structured%20and%20semantically%20meaningful%20representations%20that%20generalize%20across%20different%20datasets%20and%20supervision%20levels.%20Extensive%20experiments%20on%20three%20widely%20used%20skeleton-based%20action%20recognition%20benchmarks%20show%20that%20our%20proposed%20method%20consistently%20outperforms%20existing%20approaches%2C%20particularly%20in%20low-label%20regimes.%20Moreover%2C%20qualitative%20analyses%20show%20that%20the%20features%20provided%20by%20our%20method%20are%20more%20relevant%20given%20the%20motion%20and%20sample%20characteristics%2C%20with%20more%20focus%20on%20important%20skeleton%20joints%2C%20when%20compared%20to%20the%20other%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.07666v1&entry.124074799=Read"},
{"title": "FocalOrder: Focal Preference Optimization for Reading Order Detection", "author": "Fuyuan Liu and Dianyu Yu and He Ren and Nayu Liu and Xiaomian Kang and Delai Qiu and Fa Zhang and Genpeng Zhen and Shengping Liu and Jiaen Liang and Wei Huang and Yining Wang and Junnan Zhu", "abstract": "Reading order detection is the foundation of document understanding. Most existing methods rely on uniform supervision, implicitly assuming a constant difficulty distribution across layout regions. In this work, we challenge this assumption by revealing a critical flaw: \\textbf{Positional Disparity}, a phenomenon where models demonstrate mastery over the deterministic start and end regions but suffer a performance collapse in the complex intermediate sections. This degradation arises because standard training allows the massive volume of easy patterns to drown out the learning signals from difficult layouts. To address this, we propose \\textbf{FocalOrder}, a framework driven by \\textbf{Focal Preference Optimization (FPO)}. Specifically, FocalOrder employs adaptive difficulty discovery with exponential moving average mechanism to dynamically pinpoint hard-to-learn transitions, while introducing a difficulty-calibrated pairwise ranking objective to enforce global logical consistency. Extensive experiments demonstrate that FocalOrder establishes new state-of-the-art results on OmniDocBench v1.0 and Comp-HRDoc. Our compact model not only outperforms competitive specialized baselines but also significantly surpasses large-scale general VLMs. These results demonstrate that aligning the optimization with intrinsic structural ambiguity of documents is critical for mastering complex document structures.", "link": "http://arxiv.org/abs/2601.07483v1", "date": "2026-01-12", "relevancy": 2.6126, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5315}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5315}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FocalOrder%3A%20Focal%20Preference%20Optimization%20for%20Reading%20Order%20Detection&body=Title%3A%20FocalOrder%3A%20Focal%20Preference%20Optimization%20for%20Reading%20Order%20Detection%0AAuthor%3A%20Fuyuan%20Liu%20and%20Dianyu%20Yu%20and%20He%20Ren%20and%20Nayu%20Liu%20and%20Xiaomian%20Kang%20and%20Delai%20Qiu%20and%20Fa%20Zhang%20and%20Genpeng%20Zhen%20and%20Shengping%20Liu%20and%20Jiaen%20Liang%20and%20Wei%20Huang%20and%20Yining%20Wang%20and%20Junnan%20Zhu%0AAbstract%3A%20Reading%20order%20detection%20is%20the%20foundation%20of%20document%20understanding.%20Most%20existing%20methods%20rely%20on%20uniform%20supervision%2C%20implicitly%20assuming%20a%20constant%20difficulty%20distribution%20across%20layout%20regions.%20In%20this%20work%2C%20we%20challenge%20this%20assumption%20by%20revealing%20a%20critical%20flaw%3A%20%5Ctextbf%7BPositional%20Disparity%7D%2C%20a%20phenomenon%20where%20models%20demonstrate%20mastery%20over%20the%20deterministic%20start%20and%20end%20regions%20but%20suffer%20a%20performance%20collapse%20in%20the%20complex%20intermediate%20sections.%20This%20degradation%20arises%20because%20standard%20training%20allows%20the%20massive%20volume%20of%20easy%20patterns%20to%20drown%20out%20the%20learning%20signals%20from%20difficult%20layouts.%20To%20address%20this%2C%20we%20propose%20%5Ctextbf%7BFocalOrder%7D%2C%20a%20framework%20driven%20by%20%5Ctextbf%7BFocal%20Preference%20Optimization%20%28FPO%29%7D.%20Specifically%2C%20FocalOrder%20employs%20adaptive%20difficulty%20discovery%20with%20exponential%20moving%20average%20mechanism%20to%20dynamically%20pinpoint%20hard-to-learn%20transitions%2C%20while%20introducing%20a%20difficulty-calibrated%20pairwise%20ranking%20objective%20to%20enforce%20global%20logical%20consistency.%20Extensive%20experiments%20demonstrate%20that%20FocalOrder%20establishes%20new%20state-of-the-art%20results%20on%20OmniDocBench%20v1.0%20and%20Comp-HRDoc.%20Our%20compact%20model%20not%20only%20outperforms%20competitive%20specialized%20baselines%20but%20also%20significantly%20surpasses%20large-scale%20general%20VLMs.%20These%20results%20demonstrate%20that%20aligning%20the%20optimization%20with%20intrinsic%20structural%20ambiguity%20of%20documents%20is%20critical%20for%20mastering%20complex%20document%20structures.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocalOrder%253A%2520Focal%2520Preference%2520Optimization%2520for%2520Reading%2520Order%2520Detection%26entry.906535625%3DFuyuan%2520Liu%2520and%2520Dianyu%2520Yu%2520and%2520He%2520Ren%2520and%2520Nayu%2520Liu%2520and%2520Xiaomian%2520Kang%2520and%2520Delai%2520Qiu%2520and%2520Fa%2520Zhang%2520and%2520Genpeng%2520Zhen%2520and%2520Shengping%2520Liu%2520and%2520Jiaen%2520Liang%2520and%2520Wei%2520Huang%2520and%2520Yining%2520Wang%2520and%2520Junnan%2520Zhu%26entry.1292438233%3DReading%2520order%2520detection%2520is%2520the%2520foundation%2520of%2520document%2520understanding.%2520Most%2520existing%2520methods%2520rely%2520on%2520uniform%2520supervision%252C%2520implicitly%2520assuming%2520a%2520constant%2520difficulty%2520distribution%2520across%2520layout%2520regions.%2520In%2520this%2520work%252C%2520we%2520challenge%2520this%2520assumption%2520by%2520revealing%2520a%2520critical%2520flaw%253A%2520%255Ctextbf%257BPositional%2520Disparity%257D%252C%2520a%2520phenomenon%2520where%2520models%2520demonstrate%2520mastery%2520over%2520the%2520deterministic%2520start%2520and%2520end%2520regions%2520but%2520suffer%2520a%2520performance%2520collapse%2520in%2520the%2520complex%2520intermediate%2520sections.%2520This%2520degradation%2520arises%2520because%2520standard%2520training%2520allows%2520the%2520massive%2520volume%2520of%2520easy%2520patterns%2520to%2520drown%2520out%2520the%2520learning%2520signals%2520from%2520difficult%2520layouts.%2520To%2520address%2520this%252C%2520we%2520propose%2520%255Ctextbf%257BFocalOrder%257D%252C%2520a%2520framework%2520driven%2520by%2520%255Ctextbf%257BFocal%2520Preference%2520Optimization%2520%2528FPO%2529%257D.%2520Specifically%252C%2520FocalOrder%2520employs%2520adaptive%2520difficulty%2520discovery%2520with%2520exponential%2520moving%2520average%2520mechanism%2520to%2520dynamically%2520pinpoint%2520hard-to-learn%2520transitions%252C%2520while%2520introducing%2520a%2520difficulty-calibrated%2520pairwise%2520ranking%2520objective%2520to%2520enforce%2520global%2520logical%2520consistency.%2520Extensive%2520experiments%2520demonstrate%2520that%2520FocalOrder%2520establishes%2520new%2520state-of-the-art%2520results%2520on%2520OmniDocBench%2520v1.0%2520and%2520Comp-HRDoc.%2520Our%2520compact%2520model%2520not%2520only%2520outperforms%2520competitive%2520specialized%2520baselines%2520but%2520also%2520significantly%2520surpasses%2520large-scale%2520general%2520VLMs.%2520These%2520results%2520demonstrate%2520that%2520aligning%2520the%2520optimization%2520with%2520intrinsic%2520structural%2520ambiguity%2520of%2520documents%2520is%2520critical%2520for%2520mastering%2520complex%2520document%2520structures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FocalOrder%3A%20Focal%20Preference%20Optimization%20for%20Reading%20Order%20Detection&entry.906535625=Fuyuan%20Liu%20and%20Dianyu%20Yu%20and%20He%20Ren%20and%20Nayu%20Liu%20and%20Xiaomian%20Kang%20and%20Delai%20Qiu%20and%20Fa%20Zhang%20and%20Genpeng%20Zhen%20and%20Shengping%20Liu%20and%20Jiaen%20Liang%20and%20Wei%20Huang%20and%20Yining%20Wang%20and%20Junnan%20Zhu&entry.1292438233=Reading%20order%20detection%20is%20the%20foundation%20of%20document%20understanding.%20Most%20existing%20methods%20rely%20on%20uniform%20supervision%2C%20implicitly%20assuming%20a%20constant%20difficulty%20distribution%20across%20layout%20regions.%20In%20this%20work%2C%20we%20challenge%20this%20assumption%20by%20revealing%20a%20critical%20flaw%3A%20%5Ctextbf%7BPositional%20Disparity%7D%2C%20a%20phenomenon%20where%20models%20demonstrate%20mastery%20over%20the%20deterministic%20start%20and%20end%20regions%20but%20suffer%20a%20performance%20collapse%20in%20the%20complex%20intermediate%20sections.%20This%20degradation%20arises%20because%20standard%20training%20allows%20the%20massive%20volume%20of%20easy%20patterns%20to%20drown%20out%20the%20learning%20signals%20from%20difficult%20layouts.%20To%20address%20this%2C%20we%20propose%20%5Ctextbf%7BFocalOrder%7D%2C%20a%20framework%20driven%20by%20%5Ctextbf%7BFocal%20Preference%20Optimization%20%28FPO%29%7D.%20Specifically%2C%20FocalOrder%20employs%20adaptive%20difficulty%20discovery%20with%20exponential%20moving%20average%20mechanism%20to%20dynamically%20pinpoint%20hard-to-learn%20transitions%2C%20while%20introducing%20a%20difficulty-calibrated%20pairwise%20ranking%20objective%20to%20enforce%20global%20logical%20consistency.%20Extensive%20experiments%20demonstrate%20that%20FocalOrder%20establishes%20new%20state-of-the-art%20results%20on%20OmniDocBench%20v1.0%20and%20Comp-HRDoc.%20Our%20compact%20model%20not%20only%20outperforms%20competitive%20specialized%20baselines%20but%20also%20significantly%20surpasses%20large-scale%20general%20VLMs.%20These%20results%20demonstrate%20that%20aligning%20the%20optimization%20with%20intrinsic%20structural%20ambiguity%20of%20documents%20is%20critical%20for%20mastering%20complex%20document%20structures.&entry.1838667208=http%3A//arxiv.org/abs/2601.07483v1&entry.124074799=Read"},
{"title": "Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task", "author": "Kanata Suzuki and Shota Shimizu and Tetsuya Ogata", "abstract": "From the perspective of future developments in robotics, it is crucial to verify whether foundation models trained exclusively on offline data, such as images and language, can understand the robot motion. In particular, since Vision Language Models (VLMs) do not include low-level motion information from robots in their training datasets, video understanding including trajectory information remains a significant challenge. In this study, we assess two capabilities of VLMs through a video captioning task with low-level robot motion information: (1) automatic captioning of robot tasks and (2) segmentation of a series of tasks. Both capabilities are expected to enhance the efficiency of robot imitation learning by linking language and motion and serve as a measure of the foundation model's performance. The proposed method generates multiple \"scene\" captions using image captions and trajectory data from robot tasks. The full task caption is then generated by summarizing these individual captions. Additionally, the method performs subtask segmentation by comparing the similarity between text embeddings of image captions. In both captioning tasks, the proposed method aims to improve performance by providing the robot's motion data - joint and end-effector states - as input to the VLM. Simulator experiments were conducted to validate the effectiveness of the proposed method.", "link": "http://arxiv.org/abs/2512.20876v2", "date": "2026-01-12", "relevancy": 2.599, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6697}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6458}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proprioception%20Enhances%20Vision%20Language%20Model%20in%20Generating%20Captions%20and%20Subtask%20Segmentations%20for%20Robot%20Task&body=Title%3A%20Proprioception%20Enhances%20Vision%20Language%20Model%20in%20Generating%20Captions%20and%20Subtask%20Segmentations%20for%20Robot%20Task%0AAuthor%3A%20Kanata%20Suzuki%20and%20Shota%20Shimizu%20and%20Tetsuya%20Ogata%0AAbstract%3A%20From%20the%20perspective%20of%20future%20developments%20in%20robotics%2C%20it%20is%20crucial%20to%20verify%20whether%20foundation%20models%20trained%20exclusively%20on%20offline%20data%2C%20such%20as%20images%20and%20language%2C%20can%20understand%20the%20robot%20motion.%20In%20particular%2C%20since%20Vision%20Language%20Models%20%28VLMs%29%20do%20not%20include%20low-level%20motion%20information%20from%20robots%20in%20their%20training%20datasets%2C%20video%20understanding%20including%20trajectory%20information%20remains%20a%20significant%20challenge.%20In%20this%20study%2C%20we%20assess%20two%20capabilities%20of%20VLMs%20through%20a%20video%20captioning%20task%20with%20low-level%20robot%20motion%20information%3A%20%281%29%20automatic%20captioning%20of%20robot%20tasks%20and%20%282%29%20segmentation%20of%20a%20series%20of%20tasks.%20Both%20capabilities%20are%20expected%20to%20enhance%20the%20efficiency%20of%20robot%20imitation%20learning%20by%20linking%20language%20and%20motion%20and%20serve%20as%20a%20measure%20of%20the%20foundation%20model%27s%20performance.%20The%20proposed%20method%20generates%20multiple%20%22scene%22%20captions%20using%20image%20captions%20and%20trajectory%20data%20from%20robot%20tasks.%20The%20full%20task%20caption%20is%20then%20generated%20by%20summarizing%20these%20individual%20captions.%20Additionally%2C%20the%20method%20performs%20subtask%20segmentation%20by%20comparing%20the%20similarity%20between%20text%20embeddings%20of%20image%20captions.%20In%20both%20captioning%20tasks%2C%20the%20proposed%20method%20aims%20to%20improve%20performance%20by%20providing%20the%20robot%27s%20motion%20data%20-%20joint%20and%20end-effector%20states%20-%20as%20input%20to%20the%20VLM.%20Simulator%20experiments%20were%20conducted%20to%20validate%20the%20effectiveness%20of%20the%20proposed%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20876v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProprioception%2520Enhances%2520Vision%2520Language%2520Model%2520in%2520Generating%2520Captions%2520and%2520Subtask%2520Segmentations%2520for%2520Robot%2520Task%26entry.906535625%3DKanata%2520Suzuki%2520and%2520Shota%2520Shimizu%2520and%2520Tetsuya%2520Ogata%26entry.1292438233%3DFrom%2520the%2520perspective%2520of%2520future%2520developments%2520in%2520robotics%252C%2520it%2520is%2520crucial%2520to%2520verify%2520whether%2520foundation%2520models%2520trained%2520exclusively%2520on%2520offline%2520data%252C%2520such%2520as%2520images%2520and%2520language%252C%2520can%2520understand%2520the%2520robot%2520motion.%2520In%2520particular%252C%2520since%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520do%2520not%2520include%2520low-level%2520motion%2520information%2520from%2520robots%2520in%2520their%2520training%2520datasets%252C%2520video%2520understanding%2520including%2520trajectory%2520information%2520remains%2520a%2520significant%2520challenge.%2520In%2520this%2520study%252C%2520we%2520assess%2520two%2520capabilities%2520of%2520VLMs%2520through%2520a%2520video%2520captioning%2520task%2520with%2520low-level%2520robot%2520motion%2520information%253A%2520%25281%2529%2520automatic%2520captioning%2520of%2520robot%2520tasks%2520and%2520%25282%2529%2520segmentation%2520of%2520a%2520series%2520of%2520tasks.%2520Both%2520capabilities%2520are%2520expected%2520to%2520enhance%2520the%2520efficiency%2520of%2520robot%2520imitation%2520learning%2520by%2520linking%2520language%2520and%2520motion%2520and%2520serve%2520as%2520a%2520measure%2520of%2520the%2520foundation%2520model%2527s%2520performance.%2520The%2520proposed%2520method%2520generates%2520multiple%2520%2522scene%2522%2520captions%2520using%2520image%2520captions%2520and%2520trajectory%2520data%2520from%2520robot%2520tasks.%2520The%2520full%2520task%2520caption%2520is%2520then%2520generated%2520by%2520summarizing%2520these%2520individual%2520captions.%2520Additionally%252C%2520the%2520method%2520performs%2520subtask%2520segmentation%2520by%2520comparing%2520the%2520similarity%2520between%2520text%2520embeddings%2520of%2520image%2520captions.%2520In%2520both%2520captioning%2520tasks%252C%2520the%2520proposed%2520method%2520aims%2520to%2520improve%2520performance%2520by%2520providing%2520the%2520robot%2527s%2520motion%2520data%2520-%2520joint%2520and%2520end-effector%2520states%2520-%2520as%2520input%2520to%2520the%2520VLM.%2520Simulator%2520experiments%2520were%2520conducted%2520to%2520validate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20876v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proprioception%20Enhances%20Vision%20Language%20Model%20in%20Generating%20Captions%20and%20Subtask%20Segmentations%20for%20Robot%20Task&entry.906535625=Kanata%20Suzuki%20and%20Shota%20Shimizu%20and%20Tetsuya%20Ogata&entry.1292438233=From%20the%20perspective%20of%20future%20developments%20in%20robotics%2C%20it%20is%20crucial%20to%20verify%20whether%20foundation%20models%20trained%20exclusively%20on%20offline%20data%2C%20such%20as%20images%20and%20language%2C%20can%20understand%20the%20robot%20motion.%20In%20particular%2C%20since%20Vision%20Language%20Models%20%28VLMs%29%20do%20not%20include%20low-level%20motion%20information%20from%20robots%20in%20their%20training%20datasets%2C%20video%20understanding%20including%20trajectory%20information%20remains%20a%20significant%20challenge.%20In%20this%20study%2C%20we%20assess%20two%20capabilities%20of%20VLMs%20through%20a%20video%20captioning%20task%20with%20low-level%20robot%20motion%20information%3A%20%281%29%20automatic%20captioning%20of%20robot%20tasks%20and%20%282%29%20segmentation%20of%20a%20series%20of%20tasks.%20Both%20capabilities%20are%20expected%20to%20enhance%20the%20efficiency%20of%20robot%20imitation%20learning%20by%20linking%20language%20and%20motion%20and%20serve%20as%20a%20measure%20of%20the%20foundation%20model%27s%20performance.%20The%20proposed%20method%20generates%20multiple%20%22scene%22%20captions%20using%20image%20captions%20and%20trajectory%20data%20from%20robot%20tasks.%20The%20full%20task%20caption%20is%20then%20generated%20by%20summarizing%20these%20individual%20captions.%20Additionally%2C%20the%20method%20performs%20subtask%20segmentation%20by%20comparing%20the%20similarity%20between%20text%20embeddings%20of%20image%20captions.%20In%20both%20captioning%20tasks%2C%20the%20proposed%20method%20aims%20to%20improve%20performance%20by%20providing%20the%20robot%27s%20motion%20data%20-%20joint%20and%20end-effector%20states%20-%20as%20input%20to%20the%20VLM.%20Simulator%20experiments%20were%20conducted%20to%20validate%20the%20effectiveness%20of%20the%20proposed%20method.&entry.1838667208=http%3A//arxiv.org/abs/2512.20876v2&entry.124074799=Read"},
{"title": "Iterative Methods for Full-Scale Gaussian Process Approximations for Large Spatial Data", "author": "Tim Gyger and Reinhard Furrer and Fabio Sigrist", "abstract": "Gaussian processes are flexible probabilistic regression models which are widely used in statistics and machine learning. However, a drawback is their limited scalability to large data sets. To alleviate this, full-scale approximations (FSAs) combine predictive process methods and covariance tapering, thus approximating both global and local structures. We show how iterative methods can be used to reduce computational costs in calculating likelihoods, gradients, and predictive distributions with FSAs. In particular, we introduce a novel preconditioner and show theoretically and empirically that it accelerates the conjugate gradient method's convergence speed and mitigates its sensitivity with respect to the FSA parameters and the eigenvalue structure of the original covariance matrix, and we demonstrate empirically that it outperforms a state-of-the-art pivoted Cholesky preconditioner. Furthermore, we introduce an accurate and fast way to calculate predictive variances using stochastic simulation and iterative methods. In addition, we show how our newly proposed fully independent training conditional (FITC) preconditioner can also be used in iterative methods for Vecchia approximations. In our experiments, it outperforms existing state-of-the-art preconditioners for Vecchia approximations. All methods are implemented in a free C++ software library with high-level Python and R packages.", "link": "http://arxiv.org/abs/2405.14492v5", "date": "2026-01-12", "relevancy": 2.5935, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5355}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5114}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Methods%20for%20Full-Scale%20Gaussian%20Process%20Approximations%20for%20Large%20Spatial%20Data&body=Title%3A%20Iterative%20Methods%20for%20Full-Scale%20Gaussian%20Process%20Approximations%20for%20Large%20Spatial%20Data%0AAuthor%3A%20Tim%20Gyger%20and%20Reinhard%20Furrer%20and%20Fabio%20Sigrist%0AAbstract%3A%20Gaussian%20processes%20are%20flexible%20probabilistic%20regression%20models%20which%20are%20widely%20used%20in%20statistics%20and%20machine%20learning.%20However%2C%20a%20drawback%20is%20their%20limited%20scalability%20to%20large%20data%20sets.%20To%20alleviate%20this%2C%20full-scale%20approximations%20%28FSAs%29%20combine%20predictive%20process%20methods%20and%20covariance%20tapering%2C%20thus%20approximating%20both%20global%20and%20local%20structures.%20We%20show%20how%20iterative%20methods%20can%20be%20used%20to%20reduce%20computational%20costs%20in%20calculating%20likelihoods%2C%20gradients%2C%20and%20predictive%20distributions%20with%20FSAs.%20In%20particular%2C%20we%20introduce%20a%20novel%20preconditioner%20and%20show%20theoretically%20and%20empirically%20that%20it%20accelerates%20the%20conjugate%20gradient%20method%27s%20convergence%20speed%20and%20mitigates%20its%20sensitivity%20with%20respect%20to%20the%20FSA%20parameters%20and%20the%20eigenvalue%20structure%20of%20the%20original%20covariance%20matrix%2C%20and%20we%20demonstrate%20empirically%20that%20it%20outperforms%20a%20state-of-the-art%20pivoted%20Cholesky%20preconditioner.%20Furthermore%2C%20we%20introduce%20an%20accurate%20and%20fast%20way%20to%20calculate%20predictive%20variances%20using%20stochastic%20simulation%20and%20iterative%20methods.%20In%20addition%2C%20we%20show%20how%20our%20newly%20proposed%20fully%20independent%20training%20conditional%20%28FITC%29%20preconditioner%20can%20also%20be%20used%20in%20iterative%20methods%20for%20Vecchia%20approximations.%20In%20our%20experiments%2C%20it%20outperforms%20existing%20state-of-the-art%20preconditioners%20for%20Vecchia%20approximations.%20All%20methods%20are%20implemented%20in%20a%20free%20C%2B%2B%20software%20library%20with%20high-level%20Python%20and%20R%20packages.%0ALink%3A%20http%3A//arxiv.org/abs/2405.14492v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Methods%2520for%2520Full-Scale%2520Gaussian%2520Process%2520Approximations%2520for%2520Large%2520Spatial%2520Data%26entry.906535625%3DTim%2520Gyger%2520and%2520Reinhard%2520Furrer%2520and%2520Fabio%2520Sigrist%26entry.1292438233%3DGaussian%2520processes%2520are%2520flexible%2520probabilistic%2520regression%2520models%2520which%2520are%2520widely%2520used%2520in%2520statistics%2520and%2520machine%2520learning.%2520However%252C%2520a%2520drawback%2520is%2520their%2520limited%2520scalability%2520to%2520large%2520data%2520sets.%2520To%2520alleviate%2520this%252C%2520full-scale%2520approximations%2520%2528FSAs%2529%2520combine%2520predictive%2520process%2520methods%2520and%2520covariance%2520tapering%252C%2520thus%2520approximating%2520both%2520global%2520and%2520local%2520structures.%2520We%2520show%2520how%2520iterative%2520methods%2520can%2520be%2520used%2520to%2520reduce%2520computational%2520costs%2520in%2520calculating%2520likelihoods%252C%2520gradients%252C%2520and%2520predictive%2520distributions%2520with%2520FSAs.%2520In%2520particular%252C%2520we%2520introduce%2520a%2520novel%2520preconditioner%2520and%2520show%2520theoretically%2520and%2520empirically%2520that%2520it%2520accelerates%2520the%2520conjugate%2520gradient%2520method%2527s%2520convergence%2520speed%2520and%2520mitigates%2520its%2520sensitivity%2520with%2520respect%2520to%2520the%2520FSA%2520parameters%2520and%2520the%2520eigenvalue%2520structure%2520of%2520the%2520original%2520covariance%2520matrix%252C%2520and%2520we%2520demonstrate%2520empirically%2520that%2520it%2520outperforms%2520a%2520state-of-the-art%2520pivoted%2520Cholesky%2520preconditioner.%2520Furthermore%252C%2520we%2520introduce%2520an%2520accurate%2520and%2520fast%2520way%2520to%2520calculate%2520predictive%2520variances%2520using%2520stochastic%2520simulation%2520and%2520iterative%2520methods.%2520In%2520addition%252C%2520we%2520show%2520how%2520our%2520newly%2520proposed%2520fully%2520independent%2520training%2520conditional%2520%2528FITC%2529%2520preconditioner%2520can%2520also%2520be%2520used%2520in%2520iterative%2520methods%2520for%2520Vecchia%2520approximations.%2520In%2520our%2520experiments%252C%2520it%2520outperforms%2520existing%2520state-of-the-art%2520preconditioners%2520for%2520Vecchia%2520approximations.%2520All%2520methods%2520are%2520implemented%2520in%2520a%2520free%2520C%252B%252B%2520software%2520library%2520with%2520high-level%2520Python%2520and%2520R%2520packages.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14492v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Methods%20for%20Full-Scale%20Gaussian%20Process%20Approximations%20for%20Large%20Spatial%20Data&entry.906535625=Tim%20Gyger%20and%20Reinhard%20Furrer%20and%20Fabio%20Sigrist&entry.1292438233=Gaussian%20processes%20are%20flexible%20probabilistic%20regression%20models%20which%20are%20widely%20used%20in%20statistics%20and%20machine%20learning.%20However%2C%20a%20drawback%20is%20their%20limited%20scalability%20to%20large%20data%20sets.%20To%20alleviate%20this%2C%20full-scale%20approximations%20%28FSAs%29%20combine%20predictive%20process%20methods%20and%20covariance%20tapering%2C%20thus%20approximating%20both%20global%20and%20local%20structures.%20We%20show%20how%20iterative%20methods%20can%20be%20used%20to%20reduce%20computational%20costs%20in%20calculating%20likelihoods%2C%20gradients%2C%20and%20predictive%20distributions%20with%20FSAs.%20In%20particular%2C%20we%20introduce%20a%20novel%20preconditioner%20and%20show%20theoretically%20and%20empirically%20that%20it%20accelerates%20the%20conjugate%20gradient%20method%27s%20convergence%20speed%20and%20mitigates%20its%20sensitivity%20with%20respect%20to%20the%20FSA%20parameters%20and%20the%20eigenvalue%20structure%20of%20the%20original%20covariance%20matrix%2C%20and%20we%20demonstrate%20empirically%20that%20it%20outperforms%20a%20state-of-the-art%20pivoted%20Cholesky%20preconditioner.%20Furthermore%2C%20we%20introduce%20an%20accurate%20and%20fast%20way%20to%20calculate%20predictive%20variances%20using%20stochastic%20simulation%20and%20iterative%20methods.%20In%20addition%2C%20we%20show%20how%20our%20newly%20proposed%20fully%20independent%20training%20conditional%20%28FITC%29%20preconditioner%20can%20also%20be%20used%20in%20iterative%20methods%20for%20Vecchia%20approximations.%20In%20our%20experiments%2C%20it%20outperforms%20existing%20state-of-the-art%20preconditioners%20for%20Vecchia%20approximations.%20All%20methods%20are%20implemented%20in%20a%20free%20C%2B%2B%20software%20library%20with%20high-level%20Python%20and%20R%20packages.&entry.1838667208=http%3A//arxiv.org/abs/2405.14492v5&entry.124074799=Read"},
{"title": "TFEC: Multivariate Time-Series Clustering via Temporal-Frequency Enhanced Contrastive Learning", "author": "Zexi Tan and Tao Xie and Haoyi Xiao and Baoyao Yang and Yuzhu Ji and An Zeng and Xiang Zhang and Yiqun Zhang", "abstract": "Multivariate Time-Series (MTS) clustering is crucial for signal processing and data analysis. Although deep learning approaches, particularly those leveraging Contrastive Learning (CL), are prominent for MTS representation, existing CL-based models face two key limitations: 1) neglecting clustering information during positive/negative sample pair construction, and 2) introducing unreasonable inductive biases, e.g., destroying time dependence and periodicity through augmentation strategies, compromising representation quality. This paper, therefore, proposes a Temporal-Frequency Enhanced Contrastive (TFEC) learning framework. To preserve temporal structure while generating low-distortion representations, a temporal-frequency Co-EnHancement (CoEH) mechanism is introduced. Accordingly, a synergistic dual-path representation and cluster distribution learning framework is designed to jointly optimize cluster structure and representation fidelity. Experiments on six real-world benchmark datasets demonstrate TFEC's superiority, achieving 4.48% average NMI gains over SOTA methods, with ablation studies validating the design. The code of the paper is available at: https://github.com/yueliangy/TFEC.", "link": "http://arxiv.org/abs/2601.07550v1", "date": "2026-01-12", "relevancy": 2.5699, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5358}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5036}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TFEC%3A%20Multivariate%20Time-Series%20Clustering%20via%20Temporal-Frequency%20Enhanced%20Contrastive%20Learning&body=Title%3A%20TFEC%3A%20Multivariate%20Time-Series%20Clustering%20via%20Temporal-Frequency%20Enhanced%20Contrastive%20Learning%0AAuthor%3A%20Zexi%20Tan%20and%20Tao%20Xie%20and%20Haoyi%20Xiao%20and%20Baoyao%20Yang%20and%20Yuzhu%20Ji%20and%20An%20Zeng%20and%20Xiang%20Zhang%20and%20Yiqun%20Zhang%0AAbstract%3A%20Multivariate%20Time-Series%20%28MTS%29%20clustering%20is%20crucial%20for%20signal%20processing%20and%20data%20analysis.%20Although%20deep%20learning%20approaches%2C%20particularly%20those%20leveraging%20Contrastive%20Learning%20%28CL%29%2C%20are%20prominent%20for%20MTS%20representation%2C%20existing%20CL-based%20models%20face%20two%20key%20limitations%3A%201%29%20neglecting%20clustering%20information%20during%20positive/negative%20sample%20pair%20construction%2C%20and%202%29%20introducing%20unreasonable%20inductive%20biases%2C%20e.g.%2C%20destroying%20time%20dependence%20and%20periodicity%20through%20augmentation%20strategies%2C%20compromising%20representation%20quality.%20This%20paper%2C%20therefore%2C%20proposes%20a%20Temporal-Frequency%20Enhanced%20Contrastive%20%28TFEC%29%20learning%20framework.%20To%20preserve%20temporal%20structure%20while%20generating%20low-distortion%20representations%2C%20a%20temporal-frequency%20Co-EnHancement%20%28CoEH%29%20mechanism%20is%20introduced.%20Accordingly%2C%20a%20synergistic%20dual-path%20representation%20and%20cluster%20distribution%20learning%20framework%20is%20designed%20to%20jointly%20optimize%20cluster%20structure%20and%20representation%20fidelity.%20Experiments%20on%20six%20real-world%20benchmark%20datasets%20demonstrate%20TFEC%27s%20superiority%2C%20achieving%204.48%25%20average%20NMI%20gains%20over%20SOTA%20methods%2C%20with%20ablation%20studies%20validating%20the%20design.%20The%20code%20of%20the%20paper%20is%20available%20at%3A%20https%3A//github.com/yueliangy/TFEC.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTFEC%253A%2520Multivariate%2520Time-Series%2520Clustering%2520via%2520Temporal-Frequency%2520Enhanced%2520Contrastive%2520Learning%26entry.906535625%3DZexi%2520Tan%2520and%2520Tao%2520Xie%2520and%2520Haoyi%2520Xiao%2520and%2520Baoyao%2520Yang%2520and%2520Yuzhu%2520Ji%2520and%2520An%2520Zeng%2520and%2520Xiang%2520Zhang%2520and%2520Yiqun%2520Zhang%26entry.1292438233%3DMultivariate%2520Time-Series%2520%2528MTS%2529%2520clustering%2520is%2520crucial%2520for%2520signal%2520processing%2520and%2520data%2520analysis.%2520Although%2520deep%2520learning%2520approaches%252C%2520particularly%2520those%2520leveraging%2520Contrastive%2520Learning%2520%2528CL%2529%252C%2520are%2520prominent%2520for%2520MTS%2520representation%252C%2520existing%2520CL-based%2520models%2520face%2520two%2520key%2520limitations%253A%25201%2529%2520neglecting%2520clustering%2520information%2520during%2520positive/negative%2520sample%2520pair%2520construction%252C%2520and%25202%2529%2520introducing%2520unreasonable%2520inductive%2520biases%252C%2520e.g.%252C%2520destroying%2520time%2520dependence%2520and%2520periodicity%2520through%2520augmentation%2520strategies%252C%2520compromising%2520representation%2520quality.%2520This%2520paper%252C%2520therefore%252C%2520proposes%2520a%2520Temporal-Frequency%2520Enhanced%2520Contrastive%2520%2528TFEC%2529%2520learning%2520framework.%2520To%2520preserve%2520temporal%2520structure%2520while%2520generating%2520low-distortion%2520representations%252C%2520a%2520temporal-frequency%2520Co-EnHancement%2520%2528CoEH%2529%2520mechanism%2520is%2520introduced.%2520Accordingly%252C%2520a%2520synergistic%2520dual-path%2520representation%2520and%2520cluster%2520distribution%2520learning%2520framework%2520is%2520designed%2520to%2520jointly%2520optimize%2520cluster%2520structure%2520and%2520representation%2520fidelity.%2520Experiments%2520on%2520six%2520real-world%2520benchmark%2520datasets%2520demonstrate%2520TFEC%2527s%2520superiority%252C%2520achieving%25204.48%2525%2520average%2520NMI%2520gains%2520over%2520SOTA%2520methods%252C%2520with%2520ablation%2520studies%2520validating%2520the%2520design.%2520The%2520code%2520of%2520the%2520paper%2520is%2520available%2520at%253A%2520https%253A//github.com/yueliangy/TFEC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TFEC%3A%20Multivariate%20Time-Series%20Clustering%20via%20Temporal-Frequency%20Enhanced%20Contrastive%20Learning&entry.906535625=Zexi%20Tan%20and%20Tao%20Xie%20and%20Haoyi%20Xiao%20and%20Baoyao%20Yang%20and%20Yuzhu%20Ji%20and%20An%20Zeng%20and%20Xiang%20Zhang%20and%20Yiqun%20Zhang&entry.1292438233=Multivariate%20Time-Series%20%28MTS%29%20clustering%20is%20crucial%20for%20signal%20processing%20and%20data%20analysis.%20Although%20deep%20learning%20approaches%2C%20particularly%20those%20leveraging%20Contrastive%20Learning%20%28CL%29%2C%20are%20prominent%20for%20MTS%20representation%2C%20existing%20CL-based%20models%20face%20two%20key%20limitations%3A%201%29%20neglecting%20clustering%20information%20during%20positive/negative%20sample%20pair%20construction%2C%20and%202%29%20introducing%20unreasonable%20inductive%20biases%2C%20e.g.%2C%20destroying%20time%20dependence%20and%20periodicity%20through%20augmentation%20strategies%2C%20compromising%20representation%20quality.%20This%20paper%2C%20therefore%2C%20proposes%20a%20Temporal-Frequency%20Enhanced%20Contrastive%20%28TFEC%29%20learning%20framework.%20To%20preserve%20temporal%20structure%20while%20generating%20low-distortion%20representations%2C%20a%20temporal-frequency%20Co-EnHancement%20%28CoEH%29%20mechanism%20is%20introduced.%20Accordingly%2C%20a%20synergistic%20dual-path%20representation%20and%20cluster%20distribution%20learning%20framework%20is%20designed%20to%20jointly%20optimize%20cluster%20structure%20and%20representation%20fidelity.%20Experiments%20on%20six%20real-world%20benchmark%20datasets%20demonstrate%20TFEC%27s%20superiority%2C%20achieving%204.48%25%20average%20NMI%20gains%20over%20SOTA%20methods%2C%20with%20ablation%20studies%20validating%20the%20design.%20The%20code%20of%20the%20paper%20is%20available%20at%3A%20https%3A//github.com/yueliangy/TFEC.&entry.1838667208=http%3A//arxiv.org/abs/2601.07550v1&entry.124074799=Read"},
{"title": "Improving Domain Generalization in Contrastive Learning using Adaptive Temperature Control", "author": "Robert Lewis and Katie Matton and Rosalind W. Picard and John Guttag", "abstract": "Self-supervised pre-training with contrastive learning is a powerful method for learning from sparsely labeled data. However, performance can drop considerably when there is a shift in the distribution of data from training to test time. We study this phenomenon in a setting in which the training data come from multiple domains, and the test data come from a domain not seen at training that is subject to significant covariate shift. We present a new method for contrastive learning that incorporates domain labels to increase the domain invariance of learned representations, leading to improved out-of-distribution generalization. Our method adjusts the temperature parameter in the InfoNCE loss -- which controls the relative weighting of negative pairs -- using the probability that a negative sample comes from the same domain as the anchor. This upweights pairs from more similar domains, encouraging the model to discriminate samples based on domain-invariant attributes. Through experiments on a variant of the MNIST dataset, we demonstrate that our method yields better out-of-distribution performance than domain generalization baselines. Furthermore, our method maintains strong in-distribution task performance, substantially outperforming baselines on this measure.", "link": "http://arxiv.org/abs/2601.07748v1", "date": "2026-01-12", "relevancy": 2.5451, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5171}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5153}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Domain%20Generalization%20in%20Contrastive%20Learning%20using%20Adaptive%20Temperature%20Control&body=Title%3A%20Improving%20Domain%20Generalization%20in%20Contrastive%20Learning%20using%20Adaptive%20Temperature%20Control%0AAuthor%3A%20Robert%20Lewis%20and%20Katie%20Matton%20and%20Rosalind%20W.%20Picard%20and%20John%20Guttag%0AAbstract%3A%20Self-supervised%20pre-training%20with%20contrastive%20learning%20is%20a%20powerful%20method%20for%20learning%20from%20sparsely%20labeled%20data.%20However%2C%20performance%20can%20drop%20considerably%20when%20there%20is%20a%20shift%20in%20the%20distribution%20of%20data%20from%20training%20to%20test%20time.%20We%20study%20this%20phenomenon%20in%20a%20setting%20in%20which%20the%20training%20data%20come%20from%20multiple%20domains%2C%20and%20the%20test%20data%20come%20from%20a%20domain%20not%20seen%20at%20training%20that%20is%20subject%20to%20significant%20covariate%20shift.%20We%20present%20a%20new%20method%20for%20contrastive%20learning%20that%20incorporates%20domain%20labels%20to%20increase%20the%20domain%20invariance%20of%20learned%20representations%2C%20leading%20to%20improved%20out-of-distribution%20generalization.%20Our%20method%20adjusts%20the%20temperature%20parameter%20in%20the%20InfoNCE%20loss%20--%20which%20controls%20the%20relative%20weighting%20of%20negative%20pairs%20--%20using%20the%20probability%20that%20a%20negative%20sample%20comes%20from%20the%20same%20domain%20as%20the%20anchor.%20This%20upweights%20pairs%20from%20more%20similar%20domains%2C%20encouraging%20the%20model%20to%20discriminate%20samples%20based%20on%20domain-invariant%20attributes.%20Through%20experiments%20on%20a%20variant%20of%20the%20MNIST%20dataset%2C%20we%20demonstrate%20that%20our%20method%20yields%20better%20out-of-distribution%20performance%20than%20domain%20generalization%20baselines.%20Furthermore%2C%20our%20method%20maintains%20strong%20in-distribution%20task%20performance%2C%20substantially%20outperforming%20baselines%20on%20this%20measure.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Domain%2520Generalization%2520in%2520Contrastive%2520Learning%2520using%2520Adaptive%2520Temperature%2520Control%26entry.906535625%3DRobert%2520Lewis%2520and%2520Katie%2520Matton%2520and%2520Rosalind%2520W.%2520Picard%2520and%2520John%2520Guttag%26entry.1292438233%3DSelf-supervised%2520pre-training%2520with%2520contrastive%2520learning%2520is%2520a%2520powerful%2520method%2520for%2520learning%2520from%2520sparsely%2520labeled%2520data.%2520However%252C%2520performance%2520can%2520drop%2520considerably%2520when%2520there%2520is%2520a%2520shift%2520in%2520the%2520distribution%2520of%2520data%2520from%2520training%2520to%2520test%2520time.%2520We%2520study%2520this%2520phenomenon%2520in%2520a%2520setting%2520in%2520which%2520the%2520training%2520data%2520come%2520from%2520multiple%2520domains%252C%2520and%2520the%2520test%2520data%2520come%2520from%2520a%2520domain%2520not%2520seen%2520at%2520training%2520that%2520is%2520subject%2520to%2520significant%2520covariate%2520shift.%2520We%2520present%2520a%2520new%2520method%2520for%2520contrastive%2520learning%2520that%2520incorporates%2520domain%2520labels%2520to%2520increase%2520the%2520domain%2520invariance%2520of%2520learned%2520representations%252C%2520leading%2520to%2520improved%2520out-of-distribution%2520generalization.%2520Our%2520method%2520adjusts%2520the%2520temperature%2520parameter%2520in%2520the%2520InfoNCE%2520loss%2520--%2520which%2520controls%2520the%2520relative%2520weighting%2520of%2520negative%2520pairs%2520--%2520using%2520the%2520probability%2520that%2520a%2520negative%2520sample%2520comes%2520from%2520the%2520same%2520domain%2520as%2520the%2520anchor.%2520This%2520upweights%2520pairs%2520from%2520more%2520similar%2520domains%252C%2520encouraging%2520the%2520model%2520to%2520discriminate%2520samples%2520based%2520on%2520domain-invariant%2520attributes.%2520Through%2520experiments%2520on%2520a%2520variant%2520of%2520the%2520MNIST%2520dataset%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520yields%2520better%2520out-of-distribution%2520performance%2520than%2520domain%2520generalization%2520baselines.%2520Furthermore%252C%2520our%2520method%2520maintains%2520strong%2520in-distribution%2520task%2520performance%252C%2520substantially%2520outperforming%2520baselines%2520on%2520this%2520measure.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Domain%20Generalization%20in%20Contrastive%20Learning%20using%20Adaptive%20Temperature%20Control&entry.906535625=Robert%20Lewis%20and%20Katie%20Matton%20and%20Rosalind%20W.%20Picard%20and%20John%20Guttag&entry.1292438233=Self-supervised%20pre-training%20with%20contrastive%20learning%20is%20a%20powerful%20method%20for%20learning%20from%20sparsely%20labeled%20data.%20However%2C%20performance%20can%20drop%20considerably%20when%20there%20is%20a%20shift%20in%20the%20distribution%20of%20data%20from%20training%20to%20test%20time.%20We%20study%20this%20phenomenon%20in%20a%20setting%20in%20which%20the%20training%20data%20come%20from%20multiple%20domains%2C%20and%20the%20test%20data%20come%20from%20a%20domain%20not%20seen%20at%20training%20that%20is%20subject%20to%20significant%20covariate%20shift.%20We%20present%20a%20new%20method%20for%20contrastive%20learning%20that%20incorporates%20domain%20labels%20to%20increase%20the%20domain%20invariance%20of%20learned%20representations%2C%20leading%20to%20improved%20out-of-distribution%20generalization.%20Our%20method%20adjusts%20the%20temperature%20parameter%20in%20the%20InfoNCE%20loss%20--%20which%20controls%20the%20relative%20weighting%20of%20negative%20pairs%20--%20using%20the%20probability%20that%20a%20negative%20sample%20comes%20from%20the%20same%20domain%20as%20the%20anchor.%20This%20upweights%20pairs%20from%20more%20similar%20domains%2C%20encouraging%20the%20model%20to%20discriminate%20samples%20based%20on%20domain-invariant%20attributes.%20Through%20experiments%20on%20a%20variant%20of%20the%20MNIST%20dataset%2C%20we%20demonstrate%20that%20our%20method%20yields%20better%20out-of-distribution%20performance%20than%20domain%20generalization%20baselines.%20Furthermore%2C%20our%20method%20maintains%20strong%20in-distribution%20task%20performance%2C%20substantially%20outperforming%20baselines%20on%20this%20measure.&entry.1838667208=http%3A//arxiv.org/abs/2601.07748v1&entry.124074799=Read"},
{"title": "CaTS-Bench: Can Language Models Describe Time Series?", "author": "Luca Zhou and Pratham Yashwante and Marshall Fisher and Alessio Sampieri and Zihao Zhou and Fabio Galasso and Rose Yu", "abstract": "Time series captioning, the task of describing time series in natural language, requires numeric and temporal reasoning, trend interpretation, and contextual understanding. Existing benchmarks, however, often rely on fully synthetic or generic captions, and typically neglect metadata and visual representations. We introduce CaTS-Bench, a comprehensive benchmark for Context-aware Time Series reasoning across $11$ diverse domains, centered on a gold-standard evaluation set of $1746$ human-rewritten captions that measure how effectively models translate numeric trends into immediately interpretable narratives. To address the scarcity of human-annotated data, we also propose a scalable pipeline for generating high-fidelity synthetic captions, the quality of which we validate. We evaluate leading Vision-Language Models on our benchmark, revealing that even proprietary models struggle to capture numeric nuances in temporal descriptions, while finetuning open-source models on synthetic data yields substantial performance gains. Finally, we release a diagnostic suite of $910$ multiple-choice questions and tailored numeric metrics to gauge time-series-specific reasoning capabilities, establishing CaTS-Bench as a reliable foundation for grounded, multimodal language generation in numeric domains.", "link": "http://arxiv.org/abs/2509.20823v4", "date": "2026-01-12", "relevancy": 2.5402, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaTS-Bench%3A%20Can%20Language%20Models%20Describe%20Time%20Series%3F&body=Title%3A%20CaTS-Bench%3A%20Can%20Language%20Models%20Describe%20Time%20Series%3F%0AAuthor%3A%20Luca%20Zhou%20and%20Pratham%20Yashwante%20and%20Marshall%20Fisher%20and%20Alessio%20Sampieri%20and%20Zihao%20Zhou%20and%20Fabio%20Galasso%20and%20Rose%20Yu%0AAbstract%3A%20Time%20series%20captioning%2C%20the%20task%20of%20describing%20time%20series%20in%20natural%20language%2C%20requires%20numeric%20and%20temporal%20reasoning%2C%20trend%20interpretation%2C%20and%20contextual%20understanding.%20Existing%20benchmarks%2C%20however%2C%20often%20rely%20on%20fully%20synthetic%20or%20generic%20captions%2C%20and%20typically%20neglect%20metadata%20and%20visual%20representations.%20We%20introduce%20CaTS-Bench%2C%20a%20comprehensive%20benchmark%20for%20Context-aware%20Time%20Series%20reasoning%20across%20%2411%24%20diverse%20domains%2C%20centered%20on%20a%20gold-standard%20evaluation%20set%20of%20%241746%24%20human-rewritten%20captions%20that%20measure%20how%20effectively%20models%20translate%20numeric%20trends%20into%20immediately%20interpretable%20narratives.%20To%20address%20the%20scarcity%20of%20human-annotated%20data%2C%20we%20also%20propose%20a%20scalable%20pipeline%20for%20generating%20high-fidelity%20synthetic%20captions%2C%20the%20quality%20of%20which%20we%20validate.%20We%20evaluate%20leading%20Vision-Language%20Models%20on%20our%20benchmark%2C%20revealing%20that%20even%20proprietary%20models%20struggle%20to%20capture%20numeric%20nuances%20in%20temporal%20descriptions%2C%20while%20finetuning%20open-source%20models%20on%20synthetic%20data%20yields%20substantial%20performance%20gains.%20Finally%2C%20we%20release%20a%20diagnostic%20suite%20of%20%24910%24%20multiple-choice%20questions%20and%20tailored%20numeric%20metrics%20to%20gauge%20time-series-specific%20reasoning%20capabilities%2C%20establishing%20CaTS-Bench%20as%20a%20reliable%20foundation%20for%20grounded%2C%20multimodal%20language%20generation%20in%20numeric%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2509.20823v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaTS-Bench%253A%2520Can%2520Language%2520Models%2520Describe%2520Time%2520Series%253F%26entry.906535625%3DLuca%2520Zhou%2520and%2520Pratham%2520Yashwante%2520and%2520Marshall%2520Fisher%2520and%2520Alessio%2520Sampieri%2520and%2520Zihao%2520Zhou%2520and%2520Fabio%2520Galasso%2520and%2520Rose%2520Yu%26entry.1292438233%3DTime%2520series%2520captioning%252C%2520the%2520task%2520of%2520describing%2520time%2520series%2520in%2520natural%2520language%252C%2520requires%2520numeric%2520and%2520temporal%2520reasoning%252C%2520trend%2520interpretation%252C%2520and%2520contextual%2520understanding.%2520Existing%2520benchmarks%252C%2520however%252C%2520often%2520rely%2520on%2520fully%2520synthetic%2520or%2520generic%2520captions%252C%2520and%2520typically%2520neglect%2520metadata%2520and%2520visual%2520representations.%2520We%2520introduce%2520CaTS-Bench%252C%2520a%2520comprehensive%2520benchmark%2520for%2520Context-aware%2520Time%2520Series%2520reasoning%2520across%2520%252411%2524%2520diverse%2520domains%252C%2520centered%2520on%2520a%2520gold-standard%2520evaluation%2520set%2520of%2520%25241746%2524%2520human-rewritten%2520captions%2520that%2520measure%2520how%2520effectively%2520models%2520translate%2520numeric%2520trends%2520into%2520immediately%2520interpretable%2520narratives.%2520To%2520address%2520the%2520scarcity%2520of%2520human-annotated%2520data%252C%2520we%2520also%2520propose%2520a%2520scalable%2520pipeline%2520for%2520generating%2520high-fidelity%2520synthetic%2520captions%252C%2520the%2520quality%2520of%2520which%2520we%2520validate.%2520We%2520evaluate%2520leading%2520Vision-Language%2520Models%2520on%2520our%2520benchmark%252C%2520revealing%2520that%2520even%2520proprietary%2520models%2520struggle%2520to%2520capture%2520numeric%2520nuances%2520in%2520temporal%2520descriptions%252C%2520while%2520finetuning%2520open-source%2520models%2520on%2520synthetic%2520data%2520yields%2520substantial%2520performance%2520gains.%2520Finally%252C%2520we%2520release%2520a%2520diagnostic%2520suite%2520of%2520%2524910%2524%2520multiple-choice%2520questions%2520and%2520tailored%2520numeric%2520metrics%2520to%2520gauge%2520time-series-specific%2520reasoning%2520capabilities%252C%2520establishing%2520CaTS-Bench%2520as%2520a%2520reliable%2520foundation%2520for%2520grounded%252C%2520multimodal%2520language%2520generation%2520in%2520numeric%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20823v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaTS-Bench%3A%20Can%20Language%20Models%20Describe%20Time%20Series%3F&entry.906535625=Luca%20Zhou%20and%20Pratham%20Yashwante%20and%20Marshall%20Fisher%20and%20Alessio%20Sampieri%20and%20Zihao%20Zhou%20and%20Fabio%20Galasso%20and%20Rose%20Yu&entry.1292438233=Time%20series%20captioning%2C%20the%20task%20of%20describing%20time%20series%20in%20natural%20language%2C%20requires%20numeric%20and%20temporal%20reasoning%2C%20trend%20interpretation%2C%20and%20contextual%20understanding.%20Existing%20benchmarks%2C%20however%2C%20often%20rely%20on%20fully%20synthetic%20or%20generic%20captions%2C%20and%20typically%20neglect%20metadata%20and%20visual%20representations.%20We%20introduce%20CaTS-Bench%2C%20a%20comprehensive%20benchmark%20for%20Context-aware%20Time%20Series%20reasoning%20across%20%2411%24%20diverse%20domains%2C%20centered%20on%20a%20gold-standard%20evaluation%20set%20of%20%241746%24%20human-rewritten%20captions%20that%20measure%20how%20effectively%20models%20translate%20numeric%20trends%20into%20immediately%20interpretable%20narratives.%20To%20address%20the%20scarcity%20of%20human-annotated%20data%2C%20we%20also%20propose%20a%20scalable%20pipeline%20for%20generating%20high-fidelity%20synthetic%20captions%2C%20the%20quality%20of%20which%20we%20validate.%20We%20evaluate%20leading%20Vision-Language%20Models%20on%20our%20benchmark%2C%20revealing%20that%20even%20proprietary%20models%20struggle%20to%20capture%20numeric%20nuances%20in%20temporal%20descriptions%2C%20while%20finetuning%20open-source%20models%20on%20synthetic%20data%20yields%20substantial%20performance%20gains.%20Finally%2C%20we%20release%20a%20diagnostic%20suite%20of%20%24910%24%20multiple-choice%20questions%20and%20tailored%20numeric%20metrics%20to%20gauge%20time-series-specific%20reasoning%20capabilities%2C%20establishing%20CaTS-Bench%20as%20a%20reliable%20foundation%20for%20grounded%2C%20multimodal%20language%20generation%20in%20numeric%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2509.20823v4&entry.124074799=Read"},
{"title": "FMAC: a Fair Fiducial Marker Accuracy Comparison Software", "author": "Guillaume J. Laurent and Patrick Sandoz", "abstract": "This paper presents a method for carrying fair comparisons of the accuracy of pose estimation using fiducial markers. These comparisons rely on large sets of high-fidelity synthetic images enabling deep exploration of the 6 degrees of freedom. A low-discrepancy sampling of the space allows to check the correlations between each degree of freedom and the pose errors by plotting the 36 pairs of combinations. The images are rendered using a physically based ray tracing code that has been specifically developed to use the standard calibration coefficients of any camera directly. The software reproduces image distortions, defocus and diffraction blur. Furthermore, sub-pixel sampling is applied to sharp edges to enhance the fidelity of the rendered image. After introducing the rendering algorithm and its experimental validation, the paper proposes a method for evaluating the pose accuracy. This method is applied to well-known markers, revealing their strengths and weaknesses for pose estimation. The code is open source and available on GitHub.", "link": "http://arxiv.org/abs/2601.07723v1", "date": "2026-01-12", "relevancy": 2.5401, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5243}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5053}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FMAC%3A%20a%20Fair%20Fiducial%20Marker%20Accuracy%20Comparison%20Software&body=Title%3A%20FMAC%3A%20a%20Fair%20Fiducial%20Marker%20Accuracy%20Comparison%20Software%0AAuthor%3A%20Guillaume%20J.%20Laurent%20and%20Patrick%20Sandoz%0AAbstract%3A%20This%20paper%20presents%20a%20method%20for%20carrying%20fair%20comparisons%20of%20the%20accuracy%20of%20pose%20estimation%20using%20fiducial%20markers.%20These%20comparisons%20rely%20on%20large%20sets%20of%20high-fidelity%20synthetic%20images%20enabling%20deep%20exploration%20of%20the%206%20degrees%20of%20freedom.%20A%20low-discrepancy%20sampling%20of%20the%20space%20allows%20to%20check%20the%20correlations%20between%20each%20degree%20of%20freedom%20and%20the%20pose%20errors%20by%20plotting%20the%2036%20pairs%20of%20combinations.%20The%20images%20are%20rendered%20using%20a%20physically%20based%20ray%20tracing%20code%20that%20has%20been%20specifically%20developed%20to%20use%20the%20standard%20calibration%20coefficients%20of%20any%20camera%20directly.%20The%20software%20reproduces%20image%20distortions%2C%20defocus%20and%20diffraction%20blur.%20Furthermore%2C%20sub-pixel%20sampling%20is%20applied%20to%20sharp%20edges%20to%20enhance%20the%20fidelity%20of%20the%20rendered%20image.%20After%20introducing%20the%20rendering%20algorithm%20and%20its%20experimental%20validation%2C%20the%20paper%20proposes%20a%20method%20for%20evaluating%20the%20pose%20accuracy.%20This%20method%20is%20applied%20to%20well-known%20markers%2C%20revealing%20their%20strengths%20and%20weaknesses%20for%20pose%20estimation.%20The%20code%20is%20open%20source%20and%20available%20on%20GitHub.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFMAC%253A%2520a%2520Fair%2520Fiducial%2520Marker%2520Accuracy%2520Comparison%2520Software%26entry.906535625%3DGuillaume%2520J.%2520Laurent%2520and%2520Patrick%2520Sandoz%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520method%2520for%2520carrying%2520fair%2520comparisons%2520of%2520the%2520accuracy%2520of%2520pose%2520estimation%2520using%2520fiducial%2520markers.%2520These%2520comparisons%2520rely%2520on%2520large%2520sets%2520of%2520high-fidelity%2520synthetic%2520images%2520enabling%2520deep%2520exploration%2520of%2520the%25206%2520degrees%2520of%2520freedom.%2520A%2520low-discrepancy%2520sampling%2520of%2520the%2520space%2520allows%2520to%2520check%2520the%2520correlations%2520between%2520each%2520degree%2520of%2520freedom%2520and%2520the%2520pose%2520errors%2520by%2520plotting%2520the%252036%2520pairs%2520of%2520combinations.%2520The%2520images%2520are%2520rendered%2520using%2520a%2520physically%2520based%2520ray%2520tracing%2520code%2520that%2520has%2520been%2520specifically%2520developed%2520to%2520use%2520the%2520standard%2520calibration%2520coefficients%2520of%2520any%2520camera%2520directly.%2520The%2520software%2520reproduces%2520image%2520distortions%252C%2520defocus%2520and%2520diffraction%2520blur.%2520Furthermore%252C%2520sub-pixel%2520sampling%2520is%2520applied%2520to%2520sharp%2520edges%2520to%2520enhance%2520the%2520fidelity%2520of%2520the%2520rendered%2520image.%2520After%2520introducing%2520the%2520rendering%2520algorithm%2520and%2520its%2520experimental%2520validation%252C%2520the%2520paper%2520proposes%2520a%2520method%2520for%2520evaluating%2520the%2520pose%2520accuracy.%2520This%2520method%2520is%2520applied%2520to%2520well-known%2520markers%252C%2520revealing%2520their%2520strengths%2520and%2520weaknesses%2520for%2520pose%2520estimation.%2520The%2520code%2520is%2520open%2520source%2520and%2520available%2520on%2520GitHub.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FMAC%3A%20a%20Fair%20Fiducial%20Marker%20Accuracy%20Comparison%20Software&entry.906535625=Guillaume%20J.%20Laurent%20and%20Patrick%20Sandoz&entry.1292438233=This%20paper%20presents%20a%20method%20for%20carrying%20fair%20comparisons%20of%20the%20accuracy%20of%20pose%20estimation%20using%20fiducial%20markers.%20These%20comparisons%20rely%20on%20large%20sets%20of%20high-fidelity%20synthetic%20images%20enabling%20deep%20exploration%20of%20the%206%20degrees%20of%20freedom.%20A%20low-discrepancy%20sampling%20of%20the%20space%20allows%20to%20check%20the%20correlations%20between%20each%20degree%20of%20freedom%20and%20the%20pose%20errors%20by%20plotting%20the%2036%20pairs%20of%20combinations.%20The%20images%20are%20rendered%20using%20a%20physically%20based%20ray%20tracing%20code%20that%20has%20been%20specifically%20developed%20to%20use%20the%20standard%20calibration%20coefficients%20of%20any%20camera%20directly.%20The%20software%20reproduces%20image%20distortions%2C%20defocus%20and%20diffraction%20blur.%20Furthermore%2C%20sub-pixel%20sampling%20is%20applied%20to%20sharp%20edges%20to%20enhance%20the%20fidelity%20of%20the%20rendered%20image.%20After%20introducing%20the%20rendering%20algorithm%20and%20its%20experimental%20validation%2C%20the%20paper%20proposes%20a%20method%20for%20evaluating%20the%20pose%20accuracy.%20This%20method%20is%20applied%20to%20well-known%20markers%2C%20revealing%20their%20strengths%20and%20weaknesses%20for%20pose%20estimation.%20The%20code%20is%20open%20source%20and%20available%20on%20GitHub.&entry.1838667208=http%3A//arxiv.org/abs/2601.07723v1&entry.124074799=Read"},
{"title": "ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models", "author": "Nikhil Anand and Shwetha Somasundaram and Anirudh Phukan and Apoorv Saxena and Koyel Mukherjee", "abstract": "Large Language Models (LLMs) encode vast amounts of parametric knowledge during pre-training. As world knowledge evolves, effective deployment increasingly depends on their ability to faithfully follow externally retrieved context. When such evidence conflicts with the model's internal knowledge, LLMs often default to memorized facts, producing unfaithful outputs. In this work, we introduce ContextFocus, a lightweight activation steering approach that improves context faithfulness in such knowledge-conflict settings while preserving fluency and efficiency. Unlike prior approaches, our solution requires no model finetuning and incurs minimal inference-time overhead, making it highly efficient. We evaluate ContextFocus on the ConFiQA benchmark, comparing it against strong baselines including ContextDPO, COIECD, and prompting-based methods. Furthermore, we show that our method is complementary to prompting strategies and remains effective on larger models. Extensive experiments show that ContextFocus significantly improves contextual-faithfulness. Our results highlight the effectiveness, robustness, and efficiency of ContextFocus in improving contextual-faithfulness of LLM outputs.", "link": "http://arxiv.org/abs/2601.04131v2", "date": "2026-01-12", "relevancy": 2.5313, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContextFocus%3A%20Activation%20Steering%20for%20Contextual%20Faithfulness%20in%20Large%20Language%20Models&body=Title%3A%20ContextFocus%3A%20Activation%20Steering%20for%20Contextual%20Faithfulness%20in%20Large%20Language%20Models%0AAuthor%3A%20Nikhil%20Anand%20and%20Shwetha%20Somasundaram%20and%20Anirudh%20Phukan%20and%20Apoorv%20Saxena%20and%20Koyel%20Mukherjee%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20encode%20vast%20amounts%20of%20parametric%20knowledge%20during%20pre-training.%20As%20world%20knowledge%20evolves%2C%20effective%20deployment%20increasingly%20depends%20on%20their%20ability%20to%20faithfully%20follow%20externally%20retrieved%20context.%20When%20such%20evidence%20conflicts%20with%20the%20model%27s%20internal%20knowledge%2C%20LLMs%20often%20default%20to%20memorized%20facts%2C%20producing%20unfaithful%20outputs.%20In%20this%20work%2C%20we%20introduce%20ContextFocus%2C%20a%20lightweight%20activation%20steering%20approach%20that%20improves%20context%20faithfulness%20in%20such%20knowledge-conflict%20settings%20while%20preserving%20fluency%20and%20efficiency.%20Unlike%20prior%20approaches%2C%20our%20solution%20requires%20no%20model%20finetuning%20and%20incurs%20minimal%20inference-time%20overhead%2C%20making%20it%20highly%20efficient.%20We%20evaluate%20ContextFocus%20on%20the%20ConFiQA%20benchmark%2C%20comparing%20it%20against%20strong%20baselines%20including%20ContextDPO%2C%20COIECD%2C%20and%20prompting-based%20methods.%20Furthermore%2C%20we%20show%20that%20our%20method%20is%20complementary%20to%20prompting%20strategies%20and%20remains%20effective%20on%20larger%20models.%20Extensive%20experiments%20show%20that%20ContextFocus%20significantly%20improves%20contextual-faithfulness.%20Our%20results%20highlight%20the%20effectiveness%2C%20robustness%2C%20and%20efficiency%20of%20ContextFocus%20in%20improving%20contextual-faithfulness%20of%20LLM%20outputs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04131v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextFocus%253A%2520Activation%2520Steering%2520for%2520Contextual%2520Faithfulness%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DNikhil%2520Anand%2520and%2520Shwetha%2520Somasundaram%2520and%2520Anirudh%2520Phukan%2520and%2520Apoorv%2520Saxena%2520and%2520Koyel%2520Mukherjee%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520encode%2520vast%2520amounts%2520of%2520parametric%2520knowledge%2520during%2520pre-training.%2520As%2520world%2520knowledge%2520evolves%252C%2520effective%2520deployment%2520increasingly%2520depends%2520on%2520their%2520ability%2520to%2520faithfully%2520follow%2520externally%2520retrieved%2520context.%2520When%2520such%2520evidence%2520conflicts%2520with%2520the%2520model%2527s%2520internal%2520knowledge%252C%2520LLMs%2520often%2520default%2520to%2520memorized%2520facts%252C%2520producing%2520unfaithful%2520outputs.%2520In%2520this%2520work%252C%2520we%2520introduce%2520ContextFocus%252C%2520a%2520lightweight%2520activation%2520steering%2520approach%2520that%2520improves%2520context%2520faithfulness%2520in%2520such%2520knowledge-conflict%2520settings%2520while%2520preserving%2520fluency%2520and%2520efficiency.%2520Unlike%2520prior%2520approaches%252C%2520our%2520solution%2520requires%2520no%2520model%2520finetuning%2520and%2520incurs%2520minimal%2520inference-time%2520overhead%252C%2520making%2520it%2520highly%2520efficient.%2520We%2520evaluate%2520ContextFocus%2520on%2520the%2520ConFiQA%2520benchmark%252C%2520comparing%2520it%2520against%2520strong%2520baselines%2520including%2520ContextDPO%252C%2520COIECD%252C%2520and%2520prompting-based%2520methods.%2520Furthermore%252C%2520we%2520show%2520that%2520our%2520method%2520is%2520complementary%2520to%2520prompting%2520strategies%2520and%2520remains%2520effective%2520on%2520larger%2520models.%2520Extensive%2520experiments%2520show%2520that%2520ContextFocus%2520significantly%2520improves%2520contextual-faithfulness.%2520Our%2520results%2520highlight%2520the%2520effectiveness%252C%2520robustness%252C%2520and%2520efficiency%2520of%2520ContextFocus%2520in%2520improving%2520contextual-faithfulness%2520of%2520LLM%2520outputs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04131v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContextFocus%3A%20Activation%20Steering%20for%20Contextual%20Faithfulness%20in%20Large%20Language%20Models&entry.906535625=Nikhil%20Anand%20and%20Shwetha%20Somasundaram%20and%20Anirudh%20Phukan%20and%20Apoorv%20Saxena%20and%20Koyel%20Mukherjee&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20encode%20vast%20amounts%20of%20parametric%20knowledge%20during%20pre-training.%20As%20world%20knowledge%20evolves%2C%20effective%20deployment%20increasingly%20depends%20on%20their%20ability%20to%20faithfully%20follow%20externally%20retrieved%20context.%20When%20such%20evidence%20conflicts%20with%20the%20model%27s%20internal%20knowledge%2C%20LLMs%20often%20default%20to%20memorized%20facts%2C%20producing%20unfaithful%20outputs.%20In%20this%20work%2C%20we%20introduce%20ContextFocus%2C%20a%20lightweight%20activation%20steering%20approach%20that%20improves%20context%20faithfulness%20in%20such%20knowledge-conflict%20settings%20while%20preserving%20fluency%20and%20efficiency.%20Unlike%20prior%20approaches%2C%20our%20solution%20requires%20no%20model%20finetuning%20and%20incurs%20minimal%20inference-time%20overhead%2C%20making%20it%20highly%20efficient.%20We%20evaluate%20ContextFocus%20on%20the%20ConFiQA%20benchmark%2C%20comparing%20it%20against%20strong%20baselines%20including%20ContextDPO%2C%20COIECD%2C%20and%20prompting-based%20methods.%20Furthermore%2C%20we%20show%20that%20our%20method%20is%20complementary%20to%20prompting%20strategies%20and%20remains%20effective%20on%20larger%20models.%20Extensive%20experiments%20show%20that%20ContextFocus%20significantly%20improves%20contextual-faithfulness.%20Our%20results%20highlight%20the%20effectiveness%2C%20robustness%2C%20and%20efficiency%20of%20ContextFocus%20in%20improving%20contextual-faithfulness%20of%20LLM%20outputs.&entry.1838667208=http%3A//arxiv.org/abs/2601.04131v2&entry.124074799=Read"},
{"title": "Video Generation Models in Robotics -- Applications, Research Challenges, Future Directions", "author": "Zhiting Mei and Tenny Yin and Ola Shorinwa and Apurva Badithela and Zhonghe Zheng and Joseph Bruno and Madison Bland and Lihan Zha and Asher Hancock and Jaime Fern\u00e1ndez Fisac and Philip Dames and Anirudha Majumdar", "abstract": "Video generation models have emerged as high-fidelity models of the physical world, capable of synthesizing high-quality videos capturing fine-grained interactions between agents and their environments conditioned on multi-modal user inputs. Their impressive capabilities address many of the long-standing challenges faced by physics-based simulators, driving broad adoption in many problem domains, e.g., robotics. For example, video models enable photorealistic, physically consistent deformable-body simulation without making prohibitive simplifying assumptions, which is a major bottleneck in physics-based simulation. Moreover, video models can serve as foundation world models that capture the dynamics of the world in a fine-grained and expressive way. They thus overcome the limited expressiveness of language-only abstractions in describing intricate physical interactions. In this survey, we provide a review of video models and their applications as embodied world models in robotics, encompassing cost-effective data generation and action prediction in imitation learning, dynamics and rewards modeling in reinforcement learning, visual planning, and policy evaluation. Further, we highlight important challenges hindering the trustworthy integration of video models in robotics, which include poor instruction following, hallucinations such as violations of physics, and unsafe content generation, in addition to fundamental limitations such as significant data curation, training, and inference costs. We present potential future directions to address these open research challenges to motivate research and ultimately facilitate broader applications, especially in safety-critical settings.", "link": "http://arxiv.org/abs/2601.07823v1", "date": "2026-01-12", "relevancy": 2.5312, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6634}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6126}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Generation%20Models%20in%20Robotics%20--%20Applications%2C%20Research%20Challenges%2C%20Future%20Directions&body=Title%3A%20Video%20Generation%20Models%20in%20Robotics%20--%20Applications%2C%20Research%20Challenges%2C%20Future%20Directions%0AAuthor%3A%20Zhiting%20Mei%20and%20Tenny%20Yin%20and%20Ola%20Shorinwa%20and%20Apurva%20Badithela%20and%20Zhonghe%20Zheng%20and%20Joseph%20Bruno%20and%20Madison%20Bland%20and%20Lihan%20Zha%20and%20Asher%20Hancock%20and%20Jaime%20Fern%C3%A1ndez%20Fisac%20and%20Philip%20Dames%20and%20Anirudha%20Majumdar%0AAbstract%3A%20Video%20generation%20models%20have%20emerged%20as%20high-fidelity%20models%20of%20the%20physical%20world%2C%20capable%20of%20synthesizing%20high-quality%20videos%20capturing%20fine-grained%20interactions%20between%20agents%20and%20their%20environments%20conditioned%20on%20multi-modal%20user%20inputs.%20Their%20impressive%20capabilities%20address%20many%20of%20the%20long-standing%20challenges%20faced%20by%20physics-based%20simulators%2C%20driving%20broad%20adoption%20in%20many%20problem%20domains%2C%20e.g.%2C%20robotics.%20For%20example%2C%20video%20models%20enable%20photorealistic%2C%20physically%20consistent%20deformable-body%20simulation%20without%20making%20prohibitive%20simplifying%20assumptions%2C%20which%20is%20a%20major%20bottleneck%20in%20physics-based%20simulation.%20Moreover%2C%20video%20models%20can%20serve%20as%20foundation%20world%20models%20that%20capture%20the%20dynamics%20of%20the%20world%20in%20a%20fine-grained%20and%20expressive%20way.%20They%20thus%20overcome%20the%20limited%20expressiveness%20of%20language-only%20abstractions%20in%20describing%20intricate%20physical%20interactions.%20In%20this%20survey%2C%20we%20provide%20a%20review%20of%20video%20models%20and%20their%20applications%20as%20embodied%20world%20models%20in%20robotics%2C%20encompassing%20cost-effective%20data%20generation%20and%20action%20prediction%20in%20imitation%20learning%2C%20dynamics%20and%20rewards%20modeling%20in%20reinforcement%20learning%2C%20visual%20planning%2C%20and%20policy%20evaluation.%20Further%2C%20we%20highlight%20important%20challenges%20hindering%20the%20trustworthy%20integration%20of%20video%20models%20in%20robotics%2C%20which%20include%20poor%20instruction%20following%2C%20hallucinations%20such%20as%20violations%20of%20physics%2C%20and%20unsafe%20content%20generation%2C%20in%20addition%20to%20fundamental%20limitations%20such%20as%20significant%20data%20curation%2C%20training%2C%20and%20inference%20costs.%20We%20present%20potential%20future%20directions%20to%20address%20these%20open%20research%20challenges%20to%20motivate%20research%20and%20ultimately%20facilitate%20broader%20applications%2C%20especially%20in%20safety-critical%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Generation%2520Models%2520in%2520Robotics%2520--%2520Applications%252C%2520Research%2520Challenges%252C%2520Future%2520Directions%26entry.906535625%3DZhiting%2520Mei%2520and%2520Tenny%2520Yin%2520and%2520Ola%2520Shorinwa%2520and%2520Apurva%2520Badithela%2520and%2520Zhonghe%2520Zheng%2520and%2520Joseph%2520Bruno%2520and%2520Madison%2520Bland%2520and%2520Lihan%2520Zha%2520and%2520Asher%2520Hancock%2520and%2520Jaime%2520Fern%25C3%25A1ndez%2520Fisac%2520and%2520Philip%2520Dames%2520and%2520Anirudha%2520Majumdar%26entry.1292438233%3DVideo%2520generation%2520models%2520have%2520emerged%2520as%2520high-fidelity%2520models%2520of%2520the%2520physical%2520world%252C%2520capable%2520of%2520synthesizing%2520high-quality%2520videos%2520capturing%2520fine-grained%2520interactions%2520between%2520agents%2520and%2520their%2520environments%2520conditioned%2520on%2520multi-modal%2520user%2520inputs.%2520Their%2520impressive%2520capabilities%2520address%2520many%2520of%2520the%2520long-standing%2520challenges%2520faced%2520by%2520physics-based%2520simulators%252C%2520driving%2520broad%2520adoption%2520in%2520many%2520problem%2520domains%252C%2520e.g.%252C%2520robotics.%2520For%2520example%252C%2520video%2520models%2520enable%2520photorealistic%252C%2520physically%2520consistent%2520deformable-body%2520simulation%2520without%2520making%2520prohibitive%2520simplifying%2520assumptions%252C%2520which%2520is%2520a%2520major%2520bottleneck%2520in%2520physics-based%2520simulation.%2520Moreover%252C%2520video%2520models%2520can%2520serve%2520as%2520foundation%2520world%2520models%2520that%2520capture%2520the%2520dynamics%2520of%2520the%2520world%2520in%2520a%2520fine-grained%2520and%2520expressive%2520way.%2520They%2520thus%2520overcome%2520the%2520limited%2520expressiveness%2520of%2520language-only%2520abstractions%2520in%2520describing%2520intricate%2520physical%2520interactions.%2520In%2520this%2520survey%252C%2520we%2520provide%2520a%2520review%2520of%2520video%2520models%2520and%2520their%2520applications%2520as%2520embodied%2520world%2520models%2520in%2520robotics%252C%2520encompassing%2520cost-effective%2520data%2520generation%2520and%2520action%2520prediction%2520in%2520imitation%2520learning%252C%2520dynamics%2520and%2520rewards%2520modeling%2520in%2520reinforcement%2520learning%252C%2520visual%2520planning%252C%2520and%2520policy%2520evaluation.%2520Further%252C%2520we%2520highlight%2520important%2520challenges%2520hindering%2520the%2520trustworthy%2520integration%2520of%2520video%2520models%2520in%2520robotics%252C%2520which%2520include%2520poor%2520instruction%2520following%252C%2520hallucinations%2520such%2520as%2520violations%2520of%2520physics%252C%2520and%2520unsafe%2520content%2520generation%252C%2520in%2520addition%2520to%2520fundamental%2520limitations%2520such%2520as%2520significant%2520data%2520curation%252C%2520training%252C%2520and%2520inference%2520costs.%2520We%2520present%2520potential%2520future%2520directions%2520to%2520address%2520these%2520open%2520research%2520challenges%2520to%2520motivate%2520research%2520and%2520ultimately%2520facilitate%2520broader%2520applications%252C%2520especially%2520in%2520safety-critical%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Generation%20Models%20in%20Robotics%20--%20Applications%2C%20Research%20Challenges%2C%20Future%20Directions&entry.906535625=Zhiting%20Mei%20and%20Tenny%20Yin%20and%20Ola%20Shorinwa%20and%20Apurva%20Badithela%20and%20Zhonghe%20Zheng%20and%20Joseph%20Bruno%20and%20Madison%20Bland%20and%20Lihan%20Zha%20and%20Asher%20Hancock%20and%20Jaime%20Fern%C3%A1ndez%20Fisac%20and%20Philip%20Dames%20and%20Anirudha%20Majumdar&entry.1292438233=Video%20generation%20models%20have%20emerged%20as%20high-fidelity%20models%20of%20the%20physical%20world%2C%20capable%20of%20synthesizing%20high-quality%20videos%20capturing%20fine-grained%20interactions%20between%20agents%20and%20their%20environments%20conditioned%20on%20multi-modal%20user%20inputs.%20Their%20impressive%20capabilities%20address%20many%20of%20the%20long-standing%20challenges%20faced%20by%20physics-based%20simulators%2C%20driving%20broad%20adoption%20in%20many%20problem%20domains%2C%20e.g.%2C%20robotics.%20For%20example%2C%20video%20models%20enable%20photorealistic%2C%20physically%20consistent%20deformable-body%20simulation%20without%20making%20prohibitive%20simplifying%20assumptions%2C%20which%20is%20a%20major%20bottleneck%20in%20physics-based%20simulation.%20Moreover%2C%20video%20models%20can%20serve%20as%20foundation%20world%20models%20that%20capture%20the%20dynamics%20of%20the%20world%20in%20a%20fine-grained%20and%20expressive%20way.%20They%20thus%20overcome%20the%20limited%20expressiveness%20of%20language-only%20abstractions%20in%20describing%20intricate%20physical%20interactions.%20In%20this%20survey%2C%20we%20provide%20a%20review%20of%20video%20models%20and%20their%20applications%20as%20embodied%20world%20models%20in%20robotics%2C%20encompassing%20cost-effective%20data%20generation%20and%20action%20prediction%20in%20imitation%20learning%2C%20dynamics%20and%20rewards%20modeling%20in%20reinforcement%20learning%2C%20visual%20planning%2C%20and%20policy%20evaluation.%20Further%2C%20we%20highlight%20important%20challenges%20hindering%20the%20trustworthy%20integration%20of%20video%20models%20in%20robotics%2C%20which%20include%20poor%20instruction%20following%2C%20hallucinations%20such%20as%20violations%20of%20physics%2C%20and%20unsafe%20content%20generation%2C%20in%20addition%20to%20fundamental%20limitations%20such%20as%20significant%20data%20curation%2C%20training%2C%20and%20inference%20costs.%20We%20present%20potential%20future%20directions%20to%20address%20these%20open%20research%20challenges%20to%20motivate%20research%20and%20ultimately%20facilitate%20broader%20applications%2C%20especially%20in%20safety-critical%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2601.07823v1&entry.124074799=Read"},
{"title": "Tuning-free Visual Effect Transfer across Videos", "author": "Maxwell Jones and Rameen Abdal and Or Patashnik and Ruslan Salakhutdinov and Sergey Tulyakov and Jun-Yan Zhu and Kuan-Chieh Jackson Wang", "abstract": "We present RefVFX, a new framework that transfers complex temporal effects from a reference video onto a target video or image in a feed-forward manner. While existing methods excel at prompt-based or keyframe-conditioned editing, they struggle with dynamic temporal effects such as dynamic lighting changes or character transformations, which are difficult to describe via text or static conditions. Transferring a video effect is challenging, as the model must integrate the new temporal dynamics with the input video's existing motion and appearance. % To address this, we introduce a large-scale dataset of triplets, where each triplet consists of a reference effect video, an input image or video, and a corresponding output video depicting the transferred effect. Creating this data is non-trivial, especially the video-to-video effect triplets, which do not exist naturally. To generate these, we propose a scalable automated pipeline that creates high-quality paired videos designed to preserve the input's motion and structure while transforming it based on some fixed, repeatable effect. We then augment this data with image-to-video effects derived from LoRA adapters and code-based temporal effects generated through programmatic composition. Building on our new dataset, we train our reference-conditioned model using recent text-to-video backbones. Experimental results demonstrate that RefVFX produces visually consistent and temporally coherent edits, generalizes across unseen effect categories, and outperforms prompt-only baselines in both quantitative metrics and human preference. See our website $\\href{https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/}{at\\ this\\ URL}$.", "link": "http://arxiv.org/abs/2601.07833v1", "date": "2026-01-12", "relevancy": 2.5184, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6559}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6443}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tuning-free%20Visual%20Effect%20Transfer%20across%20Videos&body=Title%3A%20Tuning-free%20Visual%20Effect%20Transfer%20across%20Videos%0AAuthor%3A%20Maxwell%20Jones%20and%20Rameen%20Abdal%20and%20Or%20Patashnik%20and%20Ruslan%20Salakhutdinov%20and%20Sergey%20Tulyakov%20and%20Jun-Yan%20Zhu%20and%20Kuan-Chieh%20Jackson%20Wang%0AAbstract%3A%20We%20present%20RefVFX%2C%20a%20new%20framework%20that%20transfers%20complex%20temporal%20effects%20from%20a%20reference%20video%20onto%20a%20target%20video%20or%20image%20in%20a%20feed-forward%20manner.%20While%20existing%20methods%20excel%20at%20prompt-based%20or%20keyframe-conditioned%20editing%2C%20they%20struggle%20with%20dynamic%20temporal%20effects%20such%20as%20dynamic%20lighting%20changes%20or%20character%20transformations%2C%20which%20are%20difficult%20to%20describe%20via%20text%20or%20static%20conditions.%20Transferring%20a%20video%20effect%20is%20challenging%2C%20as%20the%20model%20must%20integrate%20the%20new%20temporal%20dynamics%20with%20the%20input%20video%27s%20existing%20motion%20and%20appearance.%20%25%20To%20address%20this%2C%20we%20introduce%20a%20large-scale%20dataset%20of%20triplets%2C%20where%20each%20triplet%20consists%20of%20a%20reference%20effect%20video%2C%20an%20input%20image%20or%20video%2C%20and%20a%20corresponding%20output%20video%20depicting%20the%20transferred%20effect.%20Creating%20this%20data%20is%20non-trivial%2C%20especially%20the%20video-to-video%20effect%20triplets%2C%20which%20do%20not%20exist%20naturally.%20To%20generate%20these%2C%20we%20propose%20a%20scalable%20automated%20pipeline%20that%20creates%20high-quality%20paired%20videos%20designed%20to%20preserve%20the%20input%27s%20motion%20and%20structure%20while%20transforming%20it%20based%20on%20some%20fixed%2C%20repeatable%20effect.%20We%20then%20augment%20this%20data%20with%20image-to-video%20effects%20derived%20from%20LoRA%20adapters%20and%20code-based%20temporal%20effects%20generated%20through%20programmatic%20composition.%20Building%20on%20our%20new%20dataset%2C%20we%20train%20our%20reference-conditioned%20model%20using%20recent%20text-to-video%20backbones.%20Experimental%20results%20demonstrate%20that%20RefVFX%20produces%20visually%20consistent%20and%20temporally%20coherent%20edits%2C%20generalizes%20across%20unseen%20effect%20categories%2C%20and%20outperforms%20prompt-only%20baselines%20in%20both%20quantitative%20metrics%20and%20human%20preference.%20See%20our%20website%20%24%5Chref%7Bhttps%3A//tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/%7D%7Bat%5C%20this%5C%20URL%7D%24.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTuning-free%2520Visual%2520Effect%2520Transfer%2520across%2520Videos%26entry.906535625%3DMaxwell%2520Jones%2520and%2520Rameen%2520Abdal%2520and%2520Or%2520Patashnik%2520and%2520Ruslan%2520Salakhutdinov%2520and%2520Sergey%2520Tulyakov%2520and%2520Jun-Yan%2520Zhu%2520and%2520Kuan-Chieh%2520Jackson%2520Wang%26entry.1292438233%3DWe%2520present%2520RefVFX%252C%2520a%2520new%2520framework%2520that%2520transfers%2520complex%2520temporal%2520effects%2520from%2520a%2520reference%2520video%2520onto%2520a%2520target%2520video%2520or%2520image%2520in%2520a%2520feed-forward%2520manner.%2520While%2520existing%2520methods%2520excel%2520at%2520prompt-based%2520or%2520keyframe-conditioned%2520editing%252C%2520they%2520struggle%2520with%2520dynamic%2520temporal%2520effects%2520such%2520as%2520dynamic%2520lighting%2520changes%2520or%2520character%2520transformations%252C%2520which%2520are%2520difficult%2520to%2520describe%2520via%2520text%2520or%2520static%2520conditions.%2520Transferring%2520a%2520video%2520effect%2520is%2520challenging%252C%2520as%2520the%2520model%2520must%2520integrate%2520the%2520new%2520temporal%2520dynamics%2520with%2520the%2520input%2520video%2527s%2520existing%2520motion%2520and%2520appearance.%2520%2525%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520large-scale%2520dataset%2520of%2520triplets%252C%2520where%2520each%2520triplet%2520consists%2520of%2520a%2520reference%2520effect%2520video%252C%2520an%2520input%2520image%2520or%2520video%252C%2520and%2520a%2520corresponding%2520output%2520video%2520depicting%2520the%2520transferred%2520effect.%2520Creating%2520this%2520data%2520is%2520non-trivial%252C%2520especially%2520the%2520video-to-video%2520effect%2520triplets%252C%2520which%2520do%2520not%2520exist%2520naturally.%2520To%2520generate%2520these%252C%2520we%2520propose%2520a%2520scalable%2520automated%2520pipeline%2520that%2520creates%2520high-quality%2520paired%2520videos%2520designed%2520to%2520preserve%2520the%2520input%2527s%2520motion%2520and%2520structure%2520while%2520transforming%2520it%2520based%2520on%2520some%2520fixed%252C%2520repeatable%2520effect.%2520We%2520then%2520augment%2520this%2520data%2520with%2520image-to-video%2520effects%2520derived%2520from%2520LoRA%2520adapters%2520and%2520code-based%2520temporal%2520effects%2520generated%2520through%2520programmatic%2520composition.%2520Building%2520on%2520our%2520new%2520dataset%252C%2520we%2520train%2520our%2520reference-conditioned%2520model%2520using%2520recent%2520text-to-video%2520backbones.%2520Experimental%2520results%2520demonstrate%2520that%2520RefVFX%2520produces%2520visually%2520consistent%2520and%2520temporally%2520coherent%2520edits%252C%2520generalizes%2520across%2520unseen%2520effect%2520categories%252C%2520and%2520outperforms%2520prompt-only%2520baselines%2520in%2520both%2520quantitative%2520metrics%2520and%2520human%2520preference.%2520See%2520our%2520website%2520%2524%255Chref%257Bhttps%253A//tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/%257D%257Bat%255C%2520this%255C%2520URL%257D%2524.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tuning-free%20Visual%20Effect%20Transfer%20across%20Videos&entry.906535625=Maxwell%20Jones%20and%20Rameen%20Abdal%20and%20Or%20Patashnik%20and%20Ruslan%20Salakhutdinov%20and%20Sergey%20Tulyakov%20and%20Jun-Yan%20Zhu%20and%20Kuan-Chieh%20Jackson%20Wang&entry.1292438233=We%20present%20RefVFX%2C%20a%20new%20framework%20that%20transfers%20complex%20temporal%20effects%20from%20a%20reference%20video%20onto%20a%20target%20video%20or%20image%20in%20a%20feed-forward%20manner.%20While%20existing%20methods%20excel%20at%20prompt-based%20or%20keyframe-conditioned%20editing%2C%20they%20struggle%20with%20dynamic%20temporal%20effects%20such%20as%20dynamic%20lighting%20changes%20or%20character%20transformations%2C%20which%20are%20difficult%20to%20describe%20via%20text%20or%20static%20conditions.%20Transferring%20a%20video%20effect%20is%20challenging%2C%20as%20the%20model%20must%20integrate%20the%20new%20temporal%20dynamics%20with%20the%20input%20video%27s%20existing%20motion%20and%20appearance.%20%25%20To%20address%20this%2C%20we%20introduce%20a%20large-scale%20dataset%20of%20triplets%2C%20where%20each%20triplet%20consists%20of%20a%20reference%20effect%20video%2C%20an%20input%20image%20or%20video%2C%20and%20a%20corresponding%20output%20video%20depicting%20the%20transferred%20effect.%20Creating%20this%20data%20is%20non-trivial%2C%20especially%20the%20video-to-video%20effect%20triplets%2C%20which%20do%20not%20exist%20naturally.%20To%20generate%20these%2C%20we%20propose%20a%20scalable%20automated%20pipeline%20that%20creates%20high-quality%20paired%20videos%20designed%20to%20preserve%20the%20input%27s%20motion%20and%20structure%20while%20transforming%20it%20based%20on%20some%20fixed%2C%20repeatable%20effect.%20We%20then%20augment%20this%20data%20with%20image-to-video%20effects%20derived%20from%20LoRA%20adapters%20and%20code-based%20temporal%20effects%20generated%20through%20programmatic%20composition.%20Building%20on%20our%20new%20dataset%2C%20we%20train%20our%20reference-conditioned%20model%20using%20recent%20text-to-video%20backbones.%20Experimental%20results%20demonstrate%20that%20RefVFX%20produces%20visually%20consistent%20and%20temporally%20coherent%20edits%2C%20generalizes%20across%20unseen%20effect%20categories%2C%20and%20outperforms%20prompt-only%20baselines%20in%20both%20quantitative%20metrics%20and%20human%20preference.%20See%20our%20website%20%24%5Chref%7Bhttps%3A//tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/%7D%7Bat%5C%20this%5C%20URL%7D%24.&entry.1838667208=http%3A//arxiv.org/abs/2601.07833v1&entry.124074799=Read"},
{"title": "A Model of Artificial Jagged Intelligence", "author": "Joshua Gans", "abstract": "Generative AI systems often display highly uneven performance across tasks that appear ``nearby'': they can be excellent on one prompt and confidently wrong on another with only small changes in wording or context. We call this phenomenon Artificial Jagged Intelligence (AJI). This paper develops a tractable economic model of AJI that treats adoption as an information problem: users care about \\emph{local} reliability, but typically observe only coarse, global quality signals. In a baseline one-dimensional landscape, truth is a rough Brownian process, and the model ``knows'' scattered points drawn from a Poisson process. The model interpolates optimally, and the local error is measured by posterior variance. We derive an adoption threshold for a blind user, show that experienced errors are amplified by the inspection paradox, and interpret scaling laws as denser coverage that improves average quality without eliminating jaggedness. We then study mastery and calibration: a calibrated user who can condition on local uncertainty enjoys positive expected value even in domains that fail the blind adoption test. Modelling mastery as learning a reliability map via Gaussian process regression yields a learning-rate bound driven by information gain, clarifying when discovering ``where the model works'' is slow. Finally, we study how scaling interacts with discoverability: when calibrated signals and user mastery accelerate the harvesting of scale improvements, and when opacity can make gains from scaling effectively invisible.", "link": "http://arxiv.org/abs/2601.07573v1", "date": "2026-01-12", "relevancy": 2.5124, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.513}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5022}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Model%20of%20Artificial%20Jagged%20Intelligence&body=Title%3A%20A%20Model%20of%20Artificial%20Jagged%20Intelligence%0AAuthor%3A%20Joshua%20Gans%0AAbstract%3A%20Generative%20AI%20systems%20often%20display%20highly%20uneven%20performance%20across%20tasks%20that%20appear%20%60%60nearby%27%27%3A%20they%20can%20be%20excellent%20on%20one%20prompt%20and%20confidently%20wrong%20on%20another%20with%20only%20small%20changes%20in%20wording%20or%20context.%20We%20call%20this%20phenomenon%20Artificial%20Jagged%20Intelligence%20%28AJI%29.%20This%20paper%20develops%20a%20tractable%20economic%20model%20of%20AJI%20that%20treats%20adoption%20as%20an%20information%20problem%3A%20users%20care%20about%20%5Cemph%7Blocal%7D%20reliability%2C%20but%20typically%20observe%20only%20coarse%2C%20global%20quality%20signals.%20In%20a%20baseline%20one-dimensional%20landscape%2C%20truth%20is%20a%20rough%20Brownian%20process%2C%20and%20the%20model%20%60%60knows%27%27%20scattered%20points%20drawn%20from%20a%20Poisson%20process.%20The%20model%20interpolates%20optimally%2C%20and%20the%20local%20error%20is%20measured%20by%20posterior%20variance.%20We%20derive%20an%20adoption%20threshold%20for%20a%20blind%20user%2C%20show%20that%20experienced%20errors%20are%20amplified%20by%20the%20inspection%20paradox%2C%20and%20interpret%20scaling%20laws%20as%20denser%20coverage%20that%20improves%20average%20quality%20without%20eliminating%20jaggedness.%20We%20then%20study%20mastery%20and%20calibration%3A%20a%20calibrated%20user%20who%20can%20condition%20on%20local%20uncertainty%20enjoys%20positive%20expected%20value%20even%20in%20domains%20that%20fail%20the%20blind%20adoption%20test.%20Modelling%20mastery%20as%20learning%20a%20reliability%20map%20via%20Gaussian%20process%20regression%20yields%20a%20learning-rate%20bound%20driven%20by%20information%20gain%2C%20clarifying%20when%20discovering%20%60%60where%20the%20model%20works%27%27%20is%20slow.%20Finally%2C%20we%20study%20how%20scaling%20interacts%20with%20discoverability%3A%20when%20calibrated%20signals%20and%20user%20mastery%20accelerate%20the%20harvesting%20of%20scale%20improvements%2C%20and%20when%20opacity%20can%20make%20gains%20from%20scaling%20effectively%20invisible.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Model%2520of%2520Artificial%2520Jagged%2520Intelligence%26entry.906535625%3DJoshua%2520Gans%26entry.1292438233%3DGenerative%2520AI%2520systems%2520often%2520display%2520highly%2520uneven%2520performance%2520across%2520tasks%2520that%2520appear%2520%2560%2560nearby%2527%2527%253A%2520they%2520can%2520be%2520excellent%2520on%2520one%2520prompt%2520and%2520confidently%2520wrong%2520on%2520another%2520with%2520only%2520small%2520changes%2520in%2520wording%2520or%2520context.%2520We%2520call%2520this%2520phenomenon%2520Artificial%2520Jagged%2520Intelligence%2520%2528AJI%2529.%2520This%2520paper%2520develops%2520a%2520tractable%2520economic%2520model%2520of%2520AJI%2520that%2520treats%2520adoption%2520as%2520an%2520information%2520problem%253A%2520users%2520care%2520about%2520%255Cemph%257Blocal%257D%2520reliability%252C%2520but%2520typically%2520observe%2520only%2520coarse%252C%2520global%2520quality%2520signals.%2520In%2520a%2520baseline%2520one-dimensional%2520landscape%252C%2520truth%2520is%2520a%2520rough%2520Brownian%2520process%252C%2520and%2520the%2520model%2520%2560%2560knows%2527%2527%2520scattered%2520points%2520drawn%2520from%2520a%2520Poisson%2520process.%2520The%2520model%2520interpolates%2520optimally%252C%2520and%2520the%2520local%2520error%2520is%2520measured%2520by%2520posterior%2520variance.%2520We%2520derive%2520an%2520adoption%2520threshold%2520for%2520a%2520blind%2520user%252C%2520show%2520that%2520experienced%2520errors%2520are%2520amplified%2520by%2520the%2520inspection%2520paradox%252C%2520and%2520interpret%2520scaling%2520laws%2520as%2520denser%2520coverage%2520that%2520improves%2520average%2520quality%2520without%2520eliminating%2520jaggedness.%2520We%2520then%2520study%2520mastery%2520and%2520calibration%253A%2520a%2520calibrated%2520user%2520who%2520can%2520condition%2520on%2520local%2520uncertainty%2520enjoys%2520positive%2520expected%2520value%2520even%2520in%2520domains%2520that%2520fail%2520the%2520blind%2520adoption%2520test.%2520Modelling%2520mastery%2520as%2520learning%2520a%2520reliability%2520map%2520via%2520Gaussian%2520process%2520regression%2520yields%2520a%2520learning-rate%2520bound%2520driven%2520by%2520information%2520gain%252C%2520clarifying%2520when%2520discovering%2520%2560%2560where%2520the%2520model%2520works%2527%2527%2520is%2520slow.%2520Finally%252C%2520we%2520study%2520how%2520scaling%2520interacts%2520with%2520discoverability%253A%2520when%2520calibrated%2520signals%2520and%2520user%2520mastery%2520accelerate%2520the%2520harvesting%2520of%2520scale%2520improvements%252C%2520and%2520when%2520opacity%2520can%2520make%2520gains%2520from%2520scaling%2520effectively%2520invisible.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Model%20of%20Artificial%20Jagged%20Intelligence&entry.906535625=Joshua%20Gans&entry.1292438233=Generative%20AI%20systems%20often%20display%20highly%20uneven%20performance%20across%20tasks%20that%20appear%20%60%60nearby%27%27%3A%20they%20can%20be%20excellent%20on%20one%20prompt%20and%20confidently%20wrong%20on%20another%20with%20only%20small%20changes%20in%20wording%20or%20context.%20We%20call%20this%20phenomenon%20Artificial%20Jagged%20Intelligence%20%28AJI%29.%20This%20paper%20develops%20a%20tractable%20economic%20model%20of%20AJI%20that%20treats%20adoption%20as%20an%20information%20problem%3A%20users%20care%20about%20%5Cemph%7Blocal%7D%20reliability%2C%20but%20typically%20observe%20only%20coarse%2C%20global%20quality%20signals.%20In%20a%20baseline%20one-dimensional%20landscape%2C%20truth%20is%20a%20rough%20Brownian%20process%2C%20and%20the%20model%20%60%60knows%27%27%20scattered%20points%20drawn%20from%20a%20Poisson%20process.%20The%20model%20interpolates%20optimally%2C%20and%20the%20local%20error%20is%20measured%20by%20posterior%20variance.%20We%20derive%20an%20adoption%20threshold%20for%20a%20blind%20user%2C%20show%20that%20experienced%20errors%20are%20amplified%20by%20the%20inspection%20paradox%2C%20and%20interpret%20scaling%20laws%20as%20denser%20coverage%20that%20improves%20average%20quality%20without%20eliminating%20jaggedness.%20We%20then%20study%20mastery%20and%20calibration%3A%20a%20calibrated%20user%20who%20can%20condition%20on%20local%20uncertainty%20enjoys%20positive%20expected%20value%20even%20in%20domains%20that%20fail%20the%20blind%20adoption%20test.%20Modelling%20mastery%20as%20learning%20a%20reliability%20map%20via%20Gaussian%20process%20regression%20yields%20a%20learning-rate%20bound%20driven%20by%20information%20gain%2C%20clarifying%20when%20discovering%20%60%60where%20the%20model%20works%27%27%20is%20slow.%20Finally%2C%20we%20study%20how%20scaling%20interacts%20with%20discoverability%3A%20when%20calibrated%20signals%20and%20user%20mastery%20accelerate%20the%20harvesting%20of%20scale%20improvements%2C%20and%20when%20opacity%20can%20make%20gains%20from%20scaling%20effectively%20invisible.&entry.1838667208=http%3A//arxiv.org/abs/2601.07573v1&entry.124074799=Read"},
{"title": "Two Pathways to Truthfulness: On the Intrinsic Encoding of LLM Hallucinations", "author": "Wen Luo and Guangyue Peng and Wei Li and Shaohang Wei and Feifan Song and Liang Wang and Nan Yang and Xingxing Zhang and Jing Jin and Furu Wei and Houfeng Wang", "abstract": "Despite their impressive capabilities, large language models (LLMs) frequently generate hallucinations. Previous work shows that their internal states encode rich signals of truthfulness, yet the origins and mechanisms of these signals remain unclear. In this paper, we demonstrate that truthfulness cues arise from two distinct information pathways: (1) a Question-Anchored pathway that depends on question-answer information flow, and (2) an Answer-Anchored pathway that derives self-contained evidence from the generated answer itself. First, we validate and disentangle these pathways through attention knockout and token patching. Afterwards, we uncover notable and intriguing properties of these two mechanisms. Further experiments reveal that (1) the two mechanisms are closely associated with LLM knowledge boundaries; and (2) internal representations are aware of their distinctions. Finally, building on these insightful findings, two applications are proposed to enhance hallucination detection performance. Overall, our work provides new insight into how LLMs internally encode truthfulness, offering directions for more reliable and self-aware generative systems.", "link": "http://arxiv.org/abs/2601.07422v1", "date": "2026-01-12", "relevancy": 2.5038, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5045}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5045}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two%20Pathways%20to%20Truthfulness%3A%20On%20the%20Intrinsic%20Encoding%20of%20LLM%20Hallucinations&body=Title%3A%20Two%20Pathways%20to%20Truthfulness%3A%20On%20the%20Intrinsic%20Encoding%20of%20LLM%20Hallucinations%0AAuthor%3A%20Wen%20Luo%20and%20Guangyue%20Peng%20and%20Wei%20Li%20and%20Shaohang%20Wei%20and%20Feifan%20Song%20and%20Liang%20Wang%20and%20Nan%20Yang%20and%20Xingxing%20Zhang%20and%20Jing%20Jin%20and%20Furu%20Wei%20and%20Houfeng%20Wang%0AAbstract%3A%20Despite%20their%20impressive%20capabilities%2C%20large%20language%20models%20%28LLMs%29%20frequently%20generate%20hallucinations.%20Previous%20work%20shows%20that%20their%20internal%20states%20encode%20rich%20signals%20of%20truthfulness%2C%20yet%20the%20origins%20and%20mechanisms%20of%20these%20signals%20remain%20unclear.%20In%20this%20paper%2C%20we%20demonstrate%20that%20truthfulness%20cues%20arise%20from%20two%20distinct%20information%20pathways%3A%20%281%29%20a%20Question-Anchored%20pathway%20that%20depends%20on%20question-answer%20information%20flow%2C%20and%20%282%29%20an%20Answer-Anchored%20pathway%20that%20derives%20self-contained%20evidence%20from%20the%20generated%20answer%20itself.%20First%2C%20we%20validate%20and%20disentangle%20these%20pathways%20through%20attention%20knockout%20and%20token%20patching.%20Afterwards%2C%20we%20uncover%20notable%20and%20intriguing%20properties%20of%20these%20two%20mechanisms.%20Further%20experiments%20reveal%20that%20%281%29%20the%20two%20mechanisms%20are%20closely%20associated%20with%20LLM%20knowledge%20boundaries%3B%20and%20%282%29%20internal%20representations%20are%20aware%20of%20their%20distinctions.%20Finally%2C%20building%20on%20these%20insightful%20findings%2C%20two%20applications%20are%20proposed%20to%20enhance%20hallucination%20detection%20performance.%20Overall%2C%20our%20work%20provides%20new%20insight%20into%20how%20LLMs%20internally%20encode%20truthfulness%2C%20offering%20directions%20for%20more%20reliable%20and%20self-aware%20generative%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo%2520Pathways%2520to%2520Truthfulness%253A%2520On%2520the%2520Intrinsic%2520Encoding%2520of%2520LLM%2520Hallucinations%26entry.906535625%3DWen%2520Luo%2520and%2520Guangyue%2520Peng%2520and%2520Wei%2520Li%2520and%2520Shaohang%2520Wei%2520and%2520Feifan%2520Song%2520and%2520Liang%2520Wang%2520and%2520Nan%2520Yang%2520and%2520Xingxing%2520Zhang%2520and%2520Jing%2520Jin%2520and%2520Furu%2520Wei%2520and%2520Houfeng%2520Wang%26entry.1292438233%3DDespite%2520their%2520impressive%2520capabilities%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520frequently%2520generate%2520hallucinations.%2520Previous%2520work%2520shows%2520that%2520their%2520internal%2520states%2520encode%2520rich%2520signals%2520of%2520truthfulness%252C%2520yet%2520the%2520origins%2520and%2520mechanisms%2520of%2520these%2520signals%2520remain%2520unclear.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520that%2520truthfulness%2520cues%2520arise%2520from%2520two%2520distinct%2520information%2520pathways%253A%2520%25281%2529%2520a%2520Question-Anchored%2520pathway%2520that%2520depends%2520on%2520question-answer%2520information%2520flow%252C%2520and%2520%25282%2529%2520an%2520Answer-Anchored%2520pathway%2520that%2520derives%2520self-contained%2520evidence%2520from%2520the%2520generated%2520answer%2520itself.%2520First%252C%2520we%2520validate%2520and%2520disentangle%2520these%2520pathways%2520through%2520attention%2520knockout%2520and%2520token%2520patching.%2520Afterwards%252C%2520we%2520uncover%2520notable%2520and%2520intriguing%2520properties%2520of%2520these%2520two%2520mechanisms.%2520Further%2520experiments%2520reveal%2520that%2520%25281%2529%2520the%2520two%2520mechanisms%2520are%2520closely%2520associated%2520with%2520LLM%2520knowledge%2520boundaries%253B%2520and%2520%25282%2529%2520internal%2520representations%2520are%2520aware%2520of%2520their%2520distinctions.%2520Finally%252C%2520building%2520on%2520these%2520insightful%2520findings%252C%2520two%2520applications%2520are%2520proposed%2520to%2520enhance%2520hallucination%2520detection%2520performance.%2520Overall%252C%2520our%2520work%2520provides%2520new%2520insight%2520into%2520how%2520LLMs%2520internally%2520encode%2520truthfulness%252C%2520offering%2520directions%2520for%2520more%2520reliable%2520and%2520self-aware%2520generative%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Pathways%20to%20Truthfulness%3A%20On%20the%20Intrinsic%20Encoding%20of%20LLM%20Hallucinations&entry.906535625=Wen%20Luo%20and%20Guangyue%20Peng%20and%20Wei%20Li%20and%20Shaohang%20Wei%20and%20Feifan%20Song%20and%20Liang%20Wang%20and%20Nan%20Yang%20and%20Xingxing%20Zhang%20and%20Jing%20Jin%20and%20Furu%20Wei%20and%20Houfeng%20Wang&entry.1292438233=Despite%20their%20impressive%20capabilities%2C%20large%20language%20models%20%28LLMs%29%20frequently%20generate%20hallucinations.%20Previous%20work%20shows%20that%20their%20internal%20states%20encode%20rich%20signals%20of%20truthfulness%2C%20yet%20the%20origins%20and%20mechanisms%20of%20these%20signals%20remain%20unclear.%20In%20this%20paper%2C%20we%20demonstrate%20that%20truthfulness%20cues%20arise%20from%20two%20distinct%20information%20pathways%3A%20%281%29%20a%20Question-Anchored%20pathway%20that%20depends%20on%20question-answer%20information%20flow%2C%20and%20%282%29%20an%20Answer-Anchored%20pathway%20that%20derives%20self-contained%20evidence%20from%20the%20generated%20answer%20itself.%20First%2C%20we%20validate%20and%20disentangle%20these%20pathways%20through%20attention%20knockout%20and%20token%20patching.%20Afterwards%2C%20we%20uncover%20notable%20and%20intriguing%20properties%20of%20these%20two%20mechanisms.%20Further%20experiments%20reveal%20that%20%281%29%20the%20two%20mechanisms%20are%20closely%20associated%20with%20LLM%20knowledge%20boundaries%3B%20and%20%282%29%20internal%20representations%20are%20aware%20of%20their%20distinctions.%20Finally%2C%20building%20on%20these%20insightful%20findings%2C%20two%20applications%20are%20proposed%20to%20enhance%20hallucination%20detection%20performance.%20Overall%2C%20our%20work%20provides%20new%20insight%20into%20how%20LLMs%20internally%20encode%20truthfulness%2C%20offering%20directions%20for%20more%20reliable%20and%20self-aware%20generative%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.07422v1&entry.124074799=Read"},
{"title": "EMP: Enhance Memory in Data Pruning", "author": "Jinying Xiao and Ping Li and Jie Nie and Bin Ji and Shasha Li and Xiaodong Liu and Jun Ma and Qingbo Wu and Jie Yu", "abstract": "Recently, large language and vision models have shown strong performance, but due to high pre-training and fine-tuning costs, research has shifted towards faster training via dataset pruning. Previous methods used sample loss as an evaluation criterion, aiming to select the most \"difficult\" samples for training. However, when the pruning rate increases, the number of times each sample is trained becomes more evenly distributed, which causes many critical or general samples to not be effectively fitted. We refer to this as Low-Frequency Learning (LFL). In other words, LFL prevents the model from remembering most samples. In our work, we decompose the scoring function of LFL, provide a theoretical explanation for the inefficiency of LFL, and propose adding a memory term to the scoring function to enhance the model's memory capability, along with an approximation of this memory term. Similarly, we explore memory in Self-Supervised Learning (SSL), marking the first discussion on SSL memory. Using contrastive learning, we derive the memory term both theoretically and experimentally. Finally, we propose Enhance Memory Pruning (EMP), which addresses the issue of insufficient memory under high pruning rates by enhancing the model's memory of data, thereby improving its performance. We evaluated the performance of EMP in tasks such as image classification, natural language understanding, and model pre-training. The results show that EMP can improve model performance under extreme pruning rates. For example, in the CIFAR100-ResNet50 pre-training task, with 70\\% pruning, EMP outperforms current methods by 2.2\\%.", "link": "http://arxiv.org/abs/2408.16031v2", "date": "2026-01-12", "relevancy": 2.4989, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5071}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5001}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMP%3A%20Enhance%20Memory%20in%20Data%20Pruning&body=Title%3A%20EMP%3A%20Enhance%20Memory%20in%20Data%20Pruning%0AAuthor%3A%20Jinying%20Xiao%20and%20Ping%20Li%20and%20Jie%20Nie%20and%20Bin%20Ji%20and%20Shasha%20Li%20and%20Xiaodong%20Liu%20and%20Jun%20Ma%20and%20Qingbo%20Wu%20and%20Jie%20Yu%0AAbstract%3A%20Recently%2C%20large%20language%20and%20vision%20models%20have%20shown%20strong%20performance%2C%20but%20due%20to%20high%20pre-training%20and%20fine-tuning%20costs%2C%20research%20has%20shifted%20towards%20faster%20training%20via%20dataset%20pruning.%20Previous%20methods%20used%20sample%20loss%20as%20an%20evaluation%20criterion%2C%20aiming%20to%20select%20the%20most%20%22difficult%22%20samples%20for%20training.%20However%2C%20when%20the%20pruning%20rate%20increases%2C%20the%20number%20of%20times%20each%20sample%20is%20trained%20becomes%20more%20evenly%20distributed%2C%20which%20causes%20many%20critical%20or%20general%20samples%20to%20not%20be%20effectively%20fitted.%20We%20refer%20to%20this%20as%20Low-Frequency%20Learning%20%28LFL%29.%20In%20other%20words%2C%20LFL%20prevents%20the%20model%20from%20remembering%20most%20samples.%20In%20our%20work%2C%20we%20decompose%20the%20scoring%20function%20of%20LFL%2C%20provide%20a%20theoretical%20explanation%20for%20the%20inefficiency%20of%20LFL%2C%20and%20propose%20adding%20a%20memory%20term%20to%20the%20scoring%20function%20to%20enhance%20the%20model%27s%20memory%20capability%2C%20along%20with%20an%20approximation%20of%20this%20memory%20term.%20Similarly%2C%20we%20explore%20memory%20in%20Self-Supervised%20Learning%20%28SSL%29%2C%20marking%20the%20first%20discussion%20on%20SSL%20memory.%20Using%20contrastive%20learning%2C%20we%20derive%20the%20memory%20term%20both%20theoretically%20and%20experimentally.%20Finally%2C%20we%20propose%20Enhance%20Memory%20Pruning%20%28EMP%29%2C%20which%20addresses%20the%20issue%20of%20insufficient%20memory%20under%20high%20pruning%20rates%20by%20enhancing%20the%20model%27s%20memory%20of%20data%2C%20thereby%20improving%20its%20performance.%20We%20evaluated%20the%20performance%20of%20EMP%20in%20tasks%20such%20as%20image%20classification%2C%20natural%20language%20understanding%2C%20and%20model%20pre-training.%20The%20results%20show%20that%20EMP%20can%20improve%20model%20performance%20under%20extreme%20pruning%20rates.%20For%20example%2C%20in%20the%20CIFAR100-ResNet50%20pre-training%20task%2C%20with%2070%5C%25%20pruning%2C%20EMP%20outperforms%20current%20methods%20by%202.2%5C%25.%0ALink%3A%20http%3A//arxiv.org/abs/2408.16031v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMP%253A%2520Enhance%2520Memory%2520in%2520Data%2520Pruning%26entry.906535625%3DJinying%2520Xiao%2520and%2520Ping%2520Li%2520and%2520Jie%2520Nie%2520and%2520Bin%2520Ji%2520and%2520Shasha%2520Li%2520and%2520Xiaodong%2520Liu%2520and%2520Jun%2520Ma%2520and%2520Qingbo%2520Wu%2520and%2520Jie%2520Yu%26entry.1292438233%3DRecently%252C%2520large%2520language%2520and%2520vision%2520models%2520have%2520shown%2520strong%2520performance%252C%2520but%2520due%2520to%2520high%2520pre-training%2520and%2520fine-tuning%2520costs%252C%2520research%2520has%2520shifted%2520towards%2520faster%2520training%2520via%2520dataset%2520pruning.%2520Previous%2520methods%2520used%2520sample%2520loss%2520as%2520an%2520evaluation%2520criterion%252C%2520aiming%2520to%2520select%2520the%2520most%2520%2522difficult%2522%2520samples%2520for%2520training.%2520However%252C%2520when%2520the%2520pruning%2520rate%2520increases%252C%2520the%2520number%2520of%2520times%2520each%2520sample%2520is%2520trained%2520becomes%2520more%2520evenly%2520distributed%252C%2520which%2520causes%2520many%2520critical%2520or%2520general%2520samples%2520to%2520not%2520be%2520effectively%2520fitted.%2520We%2520refer%2520to%2520this%2520as%2520Low-Frequency%2520Learning%2520%2528LFL%2529.%2520In%2520other%2520words%252C%2520LFL%2520prevents%2520the%2520model%2520from%2520remembering%2520most%2520samples.%2520In%2520our%2520work%252C%2520we%2520decompose%2520the%2520scoring%2520function%2520of%2520LFL%252C%2520provide%2520a%2520theoretical%2520explanation%2520for%2520the%2520inefficiency%2520of%2520LFL%252C%2520and%2520propose%2520adding%2520a%2520memory%2520term%2520to%2520the%2520scoring%2520function%2520to%2520enhance%2520the%2520model%2527s%2520memory%2520capability%252C%2520along%2520with%2520an%2520approximation%2520of%2520this%2520memory%2520term.%2520Similarly%252C%2520we%2520explore%2520memory%2520in%2520Self-Supervised%2520Learning%2520%2528SSL%2529%252C%2520marking%2520the%2520first%2520discussion%2520on%2520SSL%2520memory.%2520Using%2520contrastive%2520learning%252C%2520we%2520derive%2520the%2520memory%2520term%2520both%2520theoretically%2520and%2520experimentally.%2520Finally%252C%2520we%2520propose%2520Enhance%2520Memory%2520Pruning%2520%2528EMP%2529%252C%2520which%2520addresses%2520the%2520issue%2520of%2520insufficient%2520memory%2520under%2520high%2520pruning%2520rates%2520by%2520enhancing%2520the%2520model%2527s%2520memory%2520of%2520data%252C%2520thereby%2520improving%2520its%2520performance.%2520We%2520evaluated%2520the%2520performance%2520of%2520EMP%2520in%2520tasks%2520such%2520as%2520image%2520classification%252C%2520natural%2520language%2520understanding%252C%2520and%2520model%2520pre-training.%2520The%2520results%2520show%2520that%2520EMP%2520can%2520improve%2520model%2520performance%2520under%2520extreme%2520pruning%2520rates.%2520For%2520example%252C%2520in%2520the%2520CIFAR100-ResNet50%2520pre-training%2520task%252C%2520with%252070%255C%2525%2520pruning%252C%2520EMP%2520outperforms%2520current%2520methods%2520by%25202.2%255C%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16031v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMP%3A%20Enhance%20Memory%20in%20Data%20Pruning&entry.906535625=Jinying%20Xiao%20and%20Ping%20Li%20and%20Jie%20Nie%20and%20Bin%20Ji%20and%20Shasha%20Li%20and%20Xiaodong%20Liu%20and%20Jun%20Ma%20and%20Qingbo%20Wu%20and%20Jie%20Yu&entry.1292438233=Recently%2C%20large%20language%20and%20vision%20models%20have%20shown%20strong%20performance%2C%20but%20due%20to%20high%20pre-training%20and%20fine-tuning%20costs%2C%20research%20has%20shifted%20towards%20faster%20training%20via%20dataset%20pruning.%20Previous%20methods%20used%20sample%20loss%20as%20an%20evaluation%20criterion%2C%20aiming%20to%20select%20the%20most%20%22difficult%22%20samples%20for%20training.%20However%2C%20when%20the%20pruning%20rate%20increases%2C%20the%20number%20of%20times%20each%20sample%20is%20trained%20becomes%20more%20evenly%20distributed%2C%20which%20causes%20many%20critical%20or%20general%20samples%20to%20not%20be%20effectively%20fitted.%20We%20refer%20to%20this%20as%20Low-Frequency%20Learning%20%28LFL%29.%20In%20other%20words%2C%20LFL%20prevents%20the%20model%20from%20remembering%20most%20samples.%20In%20our%20work%2C%20we%20decompose%20the%20scoring%20function%20of%20LFL%2C%20provide%20a%20theoretical%20explanation%20for%20the%20inefficiency%20of%20LFL%2C%20and%20propose%20adding%20a%20memory%20term%20to%20the%20scoring%20function%20to%20enhance%20the%20model%27s%20memory%20capability%2C%20along%20with%20an%20approximation%20of%20this%20memory%20term.%20Similarly%2C%20we%20explore%20memory%20in%20Self-Supervised%20Learning%20%28SSL%29%2C%20marking%20the%20first%20discussion%20on%20SSL%20memory.%20Using%20contrastive%20learning%2C%20we%20derive%20the%20memory%20term%20both%20theoretically%20and%20experimentally.%20Finally%2C%20we%20propose%20Enhance%20Memory%20Pruning%20%28EMP%29%2C%20which%20addresses%20the%20issue%20of%20insufficient%20memory%20under%20high%20pruning%20rates%20by%20enhancing%20the%20model%27s%20memory%20of%20data%2C%20thereby%20improving%20its%20performance.%20We%20evaluated%20the%20performance%20of%20EMP%20in%20tasks%20such%20as%20image%20classification%2C%20natural%20language%20understanding%2C%20and%20model%20pre-training.%20The%20results%20show%20that%20EMP%20can%20improve%20model%20performance%20under%20extreme%20pruning%20rates.%20For%20example%2C%20in%20the%20CIFAR100-ResNet50%20pre-training%20task%2C%20with%2070%5C%25%20pruning%2C%20EMP%20outperforms%20current%20methods%20by%202.2%5C%25.&entry.1838667208=http%3A//arxiv.org/abs/2408.16031v2&entry.124074799=Read"},
{"title": "IFDNS: An Iterative Feedback-Driven Neuro-Symbolic Method for Faithful Logical Reasoning", "author": "Xiaoheng Wang and Tongxuan Liu and Zi Gong and Xianzhe Dong and Yuting Zeng and Minhan Hu and Weizhe Huang and Jing Li", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across a wide range of reasoning tasks, including logical and mathematical problem-solving. While prompt-based methods like Chain-of-Thought (CoT) can enhance LLM reasoning abilities to some extent, they often suffer from a lack of faithfulness, where the derived conclusions may not align with the generated reasoning chain. To address this issue, researchers have explored neuro-symbolic approaches to bolster LLM logical reasoning capabilities. However, existing neuro-symbolic methods still face challenges with information loss during the process. To overcome these limitations, we introduce Iterative Feedback-Driven Neuro-Symbolic (IFDNS), a novel prompt-based method that employs a multi-round feedback mechanism to address LLM limitations in handling complex logical relationships. IFDNS utilizes iterative feedback during the logic extraction phase to accurately extract causal relationship statements and translate them into propositional and logical implication expressions, effectively mitigating information loss issues. Furthermore, IFDNS is orthogonal to existing prompt methods, allowing for seamless integration with various prompting approaches. Empirical evaluations across six datasets demonstrate the effectiveness of IFDNS in significantly improving the performance of CoT and Chain-of-Thought with Self-Consistency (CoT-SC). Specifically, IFDNS achieves a +9.40% accuracy boost for CoT on the LogiQA dataset and a +11.70% improvement for CoT-SC on the PrOntoQA dataset.", "link": "http://arxiv.org/abs/2601.07464v1", "date": "2026-01-12", "relevancy": 2.487, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5001}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5001}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IFDNS%3A%20An%20Iterative%20Feedback-Driven%20Neuro-Symbolic%20Method%20for%20Faithful%20Logical%20Reasoning&body=Title%3A%20IFDNS%3A%20An%20Iterative%20Feedback-Driven%20Neuro-Symbolic%20Method%20for%20Faithful%20Logical%20Reasoning%0AAuthor%3A%20Xiaoheng%20Wang%20and%20Tongxuan%20Liu%20and%20Zi%20Gong%20and%20Xianzhe%20Dong%20and%20Yuting%20Zeng%20and%20Minhan%20Hu%20and%20Weizhe%20Huang%20and%20Jing%20Li%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20across%20a%20wide%20range%20of%20reasoning%20tasks%2C%20including%20logical%20and%20mathematical%20problem-solving.%20While%20prompt-based%20methods%20like%20Chain-of-Thought%20%28CoT%29%20can%20enhance%20LLM%20reasoning%20abilities%20to%20some%20extent%2C%20they%20often%20suffer%20from%20a%20lack%20of%20faithfulness%2C%20where%20the%20derived%20conclusions%20may%20not%20align%20with%20the%20generated%20reasoning%20chain.%20To%20address%20this%20issue%2C%20researchers%20have%20explored%20neuro-symbolic%20approaches%20to%20bolster%20LLM%20logical%20reasoning%20capabilities.%20However%2C%20existing%20neuro-symbolic%20methods%20still%20face%20challenges%20with%20information%20loss%20during%20the%20process.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20Iterative%20Feedback-Driven%20Neuro-Symbolic%20%28IFDNS%29%2C%20a%20novel%20prompt-based%20method%20that%20employs%20a%20multi-round%20feedback%20mechanism%20to%20address%20LLM%20limitations%20in%20handling%20complex%20logical%20relationships.%20IFDNS%20utilizes%20iterative%20feedback%20during%20the%20logic%20extraction%20phase%20to%20accurately%20extract%20causal%20relationship%20statements%20and%20translate%20them%20into%20propositional%20and%20logical%20implication%20expressions%2C%20effectively%20mitigating%20information%20loss%20issues.%20Furthermore%2C%20IFDNS%20is%20orthogonal%20to%20existing%20prompt%20methods%2C%20allowing%20for%20seamless%20integration%20with%20various%20prompting%20approaches.%20Empirical%20evaluations%20across%20six%20datasets%20demonstrate%20the%20effectiveness%20of%20IFDNS%20in%20significantly%20improving%20the%20performance%20of%20CoT%20and%20Chain-of-Thought%20with%20Self-Consistency%20%28CoT-SC%29.%20Specifically%2C%20IFDNS%20achieves%20a%20%2B9.40%25%20accuracy%20boost%20for%20CoT%20on%20the%20LogiQA%20dataset%20and%20a%20%2B11.70%25%20improvement%20for%20CoT-SC%20on%20the%20PrOntoQA%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIFDNS%253A%2520An%2520Iterative%2520Feedback-Driven%2520Neuro-Symbolic%2520Method%2520for%2520Faithful%2520Logical%2520Reasoning%26entry.906535625%3DXiaoheng%2520Wang%2520and%2520Tongxuan%2520Liu%2520and%2520Zi%2520Gong%2520and%2520Xianzhe%2520Dong%2520and%2520Yuting%2520Zeng%2520and%2520Minhan%2520Hu%2520and%2520Weizhe%2520Huang%2520and%2520Jing%2520Li%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520across%2520a%2520wide%2520range%2520of%2520reasoning%2520tasks%252C%2520including%2520logical%2520and%2520mathematical%2520problem-solving.%2520While%2520prompt-based%2520methods%2520like%2520Chain-of-Thought%2520%2528CoT%2529%2520can%2520enhance%2520LLM%2520reasoning%2520abilities%2520to%2520some%2520extent%252C%2520they%2520often%2520suffer%2520from%2520a%2520lack%2520of%2520faithfulness%252C%2520where%2520the%2520derived%2520conclusions%2520may%2520not%2520align%2520with%2520the%2520generated%2520reasoning%2520chain.%2520To%2520address%2520this%2520issue%252C%2520researchers%2520have%2520explored%2520neuro-symbolic%2520approaches%2520to%2520bolster%2520LLM%2520logical%2520reasoning%2520capabilities.%2520However%252C%2520existing%2520neuro-symbolic%2520methods%2520still%2520face%2520challenges%2520with%2520information%2520loss%2520during%2520the%2520process.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520Iterative%2520Feedback-Driven%2520Neuro-Symbolic%2520%2528IFDNS%2529%252C%2520a%2520novel%2520prompt-based%2520method%2520that%2520employs%2520a%2520multi-round%2520feedback%2520mechanism%2520to%2520address%2520LLM%2520limitations%2520in%2520handling%2520complex%2520logical%2520relationships.%2520IFDNS%2520utilizes%2520iterative%2520feedback%2520during%2520the%2520logic%2520extraction%2520phase%2520to%2520accurately%2520extract%2520causal%2520relationship%2520statements%2520and%2520translate%2520them%2520into%2520propositional%2520and%2520logical%2520implication%2520expressions%252C%2520effectively%2520mitigating%2520information%2520loss%2520issues.%2520Furthermore%252C%2520IFDNS%2520is%2520orthogonal%2520to%2520existing%2520prompt%2520methods%252C%2520allowing%2520for%2520seamless%2520integration%2520with%2520various%2520prompting%2520approaches.%2520Empirical%2520evaluations%2520across%2520six%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520IFDNS%2520in%2520significantly%2520improving%2520the%2520performance%2520of%2520CoT%2520and%2520Chain-of-Thought%2520with%2520Self-Consistency%2520%2528CoT-SC%2529.%2520Specifically%252C%2520IFDNS%2520achieves%2520a%2520%252B9.40%2525%2520accuracy%2520boost%2520for%2520CoT%2520on%2520the%2520LogiQA%2520dataset%2520and%2520a%2520%252B11.70%2525%2520improvement%2520for%2520CoT-SC%2520on%2520the%2520PrOntoQA%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IFDNS%3A%20An%20Iterative%20Feedback-Driven%20Neuro-Symbolic%20Method%20for%20Faithful%20Logical%20Reasoning&entry.906535625=Xiaoheng%20Wang%20and%20Tongxuan%20Liu%20and%20Zi%20Gong%20and%20Xianzhe%20Dong%20and%20Yuting%20Zeng%20and%20Minhan%20Hu%20and%20Weizhe%20Huang%20and%20Jing%20Li&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20across%20a%20wide%20range%20of%20reasoning%20tasks%2C%20including%20logical%20and%20mathematical%20problem-solving.%20While%20prompt-based%20methods%20like%20Chain-of-Thought%20%28CoT%29%20can%20enhance%20LLM%20reasoning%20abilities%20to%20some%20extent%2C%20they%20often%20suffer%20from%20a%20lack%20of%20faithfulness%2C%20where%20the%20derived%20conclusions%20may%20not%20align%20with%20the%20generated%20reasoning%20chain.%20To%20address%20this%20issue%2C%20researchers%20have%20explored%20neuro-symbolic%20approaches%20to%20bolster%20LLM%20logical%20reasoning%20capabilities.%20However%2C%20existing%20neuro-symbolic%20methods%20still%20face%20challenges%20with%20information%20loss%20during%20the%20process.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20Iterative%20Feedback-Driven%20Neuro-Symbolic%20%28IFDNS%29%2C%20a%20novel%20prompt-based%20method%20that%20employs%20a%20multi-round%20feedback%20mechanism%20to%20address%20LLM%20limitations%20in%20handling%20complex%20logical%20relationships.%20IFDNS%20utilizes%20iterative%20feedback%20during%20the%20logic%20extraction%20phase%20to%20accurately%20extract%20causal%20relationship%20statements%20and%20translate%20them%20into%20propositional%20and%20logical%20implication%20expressions%2C%20effectively%20mitigating%20information%20loss%20issues.%20Furthermore%2C%20IFDNS%20is%20orthogonal%20to%20existing%20prompt%20methods%2C%20allowing%20for%20seamless%20integration%20with%20various%20prompting%20approaches.%20Empirical%20evaluations%20across%20six%20datasets%20demonstrate%20the%20effectiveness%20of%20IFDNS%20in%20significantly%20improving%20the%20performance%20of%20CoT%20and%20Chain-of-Thought%20with%20Self-Consistency%20%28CoT-SC%29.%20Specifically%2C%20IFDNS%20achieves%20a%20%2B9.40%25%20accuracy%20boost%20for%20CoT%20on%20the%20LogiQA%20dataset%20and%20a%20%2B11.70%25%20improvement%20for%20CoT-SC%20on%20the%20PrOntoQA%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2601.07464v1&entry.124074799=Read"},
{"title": "Accelerating Targeted Hard-Label Adversarial Attacks in Low-Query Black-Box Settings", "author": "Arjhun Swaminathan and Mete Akg\u00fcn", "abstract": "Deep neural networks for image classification remain vulnerable to adversarial examples -- small, imperceptible perturbations that induce misclassifications. In black-box settings, where only the final prediction is accessible, crafting targeted attacks that aim to misclassify into a specific target class is particularly challenging due to narrow decision regions. Current state-of-the-art methods often exploit the geometric properties of the decision boundary separating a source image and a target image rather than incorporating information from the images themselves. In contrast, we propose Targeted Edge-informed Attack (TEA), a novel attack that utilizes edge information from the target image to carefully perturb it, thereby producing an adversarial image that is closer to the source image while still achieving the desired target classification. Our approach consistently outperforms current state-of-the-art methods across different models in low query settings (nearly 70% fewer queries are used), a scenario especially relevant in real-world applications with limited queries and black-box access. Furthermore, by efficiently generating a suitable adversarial example, TEA provides an improved target initialization for established geometry-based attacks.", "link": "http://arxiv.org/abs/2505.16313v3", "date": "2026-01-12", "relevancy": 2.4849, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.531}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4804}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Targeted%20Hard-Label%20Adversarial%20Attacks%20in%20Low-Query%20Black-Box%20Settings&body=Title%3A%20Accelerating%20Targeted%20Hard-Label%20Adversarial%20Attacks%20in%20Low-Query%20Black-Box%20Settings%0AAuthor%3A%20Arjhun%20Swaminathan%20and%20Mete%20Akg%C3%BCn%0AAbstract%3A%20Deep%20neural%20networks%20for%20image%20classification%20remain%20vulnerable%20to%20adversarial%20examples%20--%20small%2C%20imperceptible%20perturbations%20that%20induce%20misclassifications.%20In%20black-box%20settings%2C%20where%20only%20the%20final%20prediction%20is%20accessible%2C%20crafting%20targeted%20attacks%20that%20aim%20to%20misclassify%20into%20a%20specific%20target%20class%20is%20particularly%20challenging%20due%20to%20narrow%20decision%20regions.%20Current%20state-of-the-art%20methods%20often%20exploit%20the%20geometric%20properties%20of%20the%20decision%20boundary%20separating%20a%20source%20image%20and%20a%20target%20image%20rather%20than%20incorporating%20information%20from%20the%20images%20themselves.%20In%20contrast%2C%20we%20propose%20Targeted%20Edge-informed%20Attack%20%28TEA%29%2C%20a%20novel%20attack%20that%20utilizes%20edge%20information%20from%20the%20target%20image%20to%20carefully%20perturb%20it%2C%20thereby%20producing%20an%20adversarial%20image%20that%20is%20closer%20to%20the%20source%20image%20while%20still%20achieving%20the%20desired%20target%20classification.%20Our%20approach%20consistently%20outperforms%20current%20state-of-the-art%20methods%20across%20different%20models%20in%20low%20query%20settings%20%28nearly%2070%25%20fewer%20queries%20are%20used%29%2C%20a%20scenario%20especially%20relevant%20in%20real-world%20applications%20with%20limited%20queries%20and%20black-box%20access.%20Furthermore%2C%20by%20efficiently%20generating%20a%20suitable%20adversarial%20example%2C%20TEA%20provides%20an%20improved%20target%20initialization%20for%20established%20geometry-based%20attacks.%0ALink%3A%20http%3A//arxiv.org/abs/2505.16313v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Targeted%2520Hard-Label%2520Adversarial%2520Attacks%2520in%2520Low-Query%2520Black-Box%2520Settings%26entry.906535625%3DArjhun%2520Swaminathan%2520and%2520Mete%2520Akg%25C3%25BCn%26entry.1292438233%3DDeep%2520neural%2520networks%2520for%2520image%2520classification%2520remain%2520vulnerable%2520to%2520adversarial%2520examples%2520--%2520small%252C%2520imperceptible%2520perturbations%2520that%2520induce%2520misclassifications.%2520In%2520black-box%2520settings%252C%2520where%2520only%2520the%2520final%2520prediction%2520is%2520accessible%252C%2520crafting%2520targeted%2520attacks%2520that%2520aim%2520to%2520misclassify%2520into%2520a%2520specific%2520target%2520class%2520is%2520particularly%2520challenging%2520due%2520to%2520narrow%2520decision%2520regions.%2520Current%2520state-of-the-art%2520methods%2520often%2520exploit%2520the%2520geometric%2520properties%2520of%2520the%2520decision%2520boundary%2520separating%2520a%2520source%2520image%2520and%2520a%2520target%2520image%2520rather%2520than%2520incorporating%2520information%2520from%2520the%2520images%2520themselves.%2520In%2520contrast%252C%2520we%2520propose%2520Targeted%2520Edge-informed%2520Attack%2520%2528TEA%2529%252C%2520a%2520novel%2520attack%2520that%2520utilizes%2520edge%2520information%2520from%2520the%2520target%2520image%2520to%2520carefully%2520perturb%2520it%252C%2520thereby%2520producing%2520an%2520adversarial%2520image%2520that%2520is%2520closer%2520to%2520the%2520source%2520image%2520while%2520still%2520achieving%2520the%2520desired%2520target%2520classification.%2520Our%2520approach%2520consistently%2520outperforms%2520current%2520state-of-the-art%2520methods%2520across%2520different%2520models%2520in%2520low%2520query%2520settings%2520%2528nearly%252070%2525%2520fewer%2520queries%2520are%2520used%2529%252C%2520a%2520scenario%2520especially%2520relevant%2520in%2520real-world%2520applications%2520with%2520limited%2520queries%2520and%2520black-box%2520access.%2520Furthermore%252C%2520by%2520efficiently%2520generating%2520a%2520suitable%2520adversarial%2520example%252C%2520TEA%2520provides%2520an%2520improved%2520target%2520initialization%2520for%2520established%2520geometry-based%2520attacks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16313v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Targeted%20Hard-Label%20Adversarial%20Attacks%20in%20Low-Query%20Black-Box%20Settings&entry.906535625=Arjhun%20Swaminathan%20and%20Mete%20Akg%C3%BCn&entry.1292438233=Deep%20neural%20networks%20for%20image%20classification%20remain%20vulnerable%20to%20adversarial%20examples%20--%20small%2C%20imperceptible%20perturbations%20that%20induce%20misclassifications.%20In%20black-box%20settings%2C%20where%20only%20the%20final%20prediction%20is%20accessible%2C%20crafting%20targeted%20attacks%20that%20aim%20to%20misclassify%20into%20a%20specific%20target%20class%20is%20particularly%20challenging%20due%20to%20narrow%20decision%20regions.%20Current%20state-of-the-art%20methods%20often%20exploit%20the%20geometric%20properties%20of%20the%20decision%20boundary%20separating%20a%20source%20image%20and%20a%20target%20image%20rather%20than%20incorporating%20information%20from%20the%20images%20themselves.%20In%20contrast%2C%20we%20propose%20Targeted%20Edge-informed%20Attack%20%28TEA%29%2C%20a%20novel%20attack%20that%20utilizes%20edge%20information%20from%20the%20target%20image%20to%20carefully%20perturb%20it%2C%20thereby%20producing%20an%20adversarial%20image%20that%20is%20closer%20to%20the%20source%20image%20while%20still%20achieving%20the%20desired%20target%20classification.%20Our%20approach%20consistently%20outperforms%20current%20state-of-the-art%20methods%20across%20different%20models%20in%20low%20query%20settings%20%28nearly%2070%25%20fewer%20queries%20are%20used%29%2C%20a%20scenario%20especially%20relevant%20in%20real-world%20applications%20with%20limited%20queries%20and%20black-box%20access.%20Furthermore%2C%20by%20efficiently%20generating%20a%20suitable%20adversarial%20example%2C%20TEA%20provides%20an%20improved%20target%20initialization%20for%20established%20geometry-based%20attacks.&entry.1838667208=http%3A//arxiv.org/abs/2505.16313v3&entry.124074799=Read"},
{"title": "Geometry-Aware Edge Pooling for Graph Neural Networks", "author": "Katharina Limbeck and Lydia Mezrag and Guy Wolf and Bastian Rieck", "abstract": "Graph Neural Networks (GNNs) have shown significant success for graph-based tasks. Motivated by the prevalence of large datasets in real-world applications, pooling layers are crucial components of GNNs. By reducing the size of input graphs, pooling enables faster training and potentially better generalisation. However, existing pooling operations often optimise for the learning task at the expense of discarding fundamental graph structures, thus reducing interpretability. This leads to unreliable performance across dataset types, downstream tasks and pooling ratios. Addressing these concerns, we propose novel graph pooling layers for structure-aware pooling via edge collapses. Our methods leverage diffusion geometry and iteratively reduce a graph's size while preserving both its metric structure and its structural diversity. We guide pooling using magnitude, an isometry-invariant diversity measure, which permits us to control the fidelity of the pooling process. Further, we use the spread of a metric space as a faster and more stable alternative ensuring computational efficiency. Empirical results demonstrate that our methods (i) achieve top performance compared to alternative pooling layers across a range of diverse graph classification tasks, (ii) preserve key spectral properties of the input graphs, and (iii) retain high accuracy across varying pooling ratios.", "link": "http://arxiv.org/abs/2506.11700v3", "date": "2026-01-12", "relevancy": 2.4766, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5087}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4951}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Aware%20Edge%20Pooling%20for%20Graph%20Neural%20Networks&body=Title%3A%20Geometry-Aware%20Edge%20Pooling%20for%20Graph%20Neural%20Networks%0AAuthor%3A%20Katharina%20Limbeck%20and%20Lydia%20Mezrag%20and%20Guy%20Wolf%20and%20Bastian%20Rieck%0AAbstract%3A%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20significant%20success%20for%20graph-based%20tasks.%20Motivated%20by%20the%20prevalence%20of%20large%20datasets%20in%20real-world%20applications%2C%20pooling%20layers%20are%20crucial%20components%20of%20GNNs.%20By%20reducing%20the%20size%20of%20input%20graphs%2C%20pooling%20enables%20faster%20training%20and%20potentially%20better%20generalisation.%20However%2C%20existing%20pooling%20operations%20often%20optimise%20for%20the%20learning%20task%20at%20the%20expense%20of%20discarding%20fundamental%20graph%20structures%2C%20thus%20reducing%20interpretability.%20This%20leads%20to%20unreliable%20performance%20across%20dataset%20types%2C%20downstream%20tasks%20and%20pooling%20ratios.%20Addressing%20these%20concerns%2C%20we%20propose%20novel%20graph%20pooling%20layers%20for%20structure-aware%20pooling%20via%20edge%20collapses.%20Our%20methods%20leverage%20diffusion%20geometry%20and%20iteratively%20reduce%20a%20graph%27s%20size%20while%20preserving%20both%20its%20metric%20structure%20and%20its%20structural%20diversity.%20We%20guide%20pooling%20using%20magnitude%2C%20an%20isometry-invariant%20diversity%20measure%2C%20which%20permits%20us%20to%20control%20the%20fidelity%20of%20the%20pooling%20process.%20Further%2C%20we%20use%20the%20spread%20of%20a%20metric%20space%20as%20a%20faster%20and%20more%20stable%20alternative%20ensuring%20computational%20efficiency.%20Empirical%20results%20demonstrate%20that%20our%20methods%20%28i%29%20achieve%20top%20performance%20compared%20to%20alternative%20pooling%20layers%20across%20a%20range%20of%20diverse%20graph%20classification%20tasks%2C%20%28ii%29%20preserve%20key%20spectral%20properties%20of%20the%20input%20graphs%2C%20and%20%28iii%29%20retain%20high%20accuracy%20across%20varying%20pooling%20ratios.%0ALink%3A%20http%3A//arxiv.org/abs/2506.11700v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Aware%2520Edge%2520Pooling%2520for%2520Graph%2520Neural%2520Networks%26entry.906535625%3DKatharina%2520Limbeck%2520and%2520Lydia%2520Mezrag%2520and%2520Guy%2520Wolf%2520and%2520Bastian%2520Rieck%26entry.1292438233%3DGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520shown%2520significant%2520success%2520for%2520graph-based%2520tasks.%2520Motivated%2520by%2520the%2520prevalence%2520of%2520large%2520datasets%2520in%2520real-world%2520applications%252C%2520pooling%2520layers%2520are%2520crucial%2520components%2520of%2520GNNs.%2520By%2520reducing%2520the%2520size%2520of%2520input%2520graphs%252C%2520pooling%2520enables%2520faster%2520training%2520and%2520potentially%2520better%2520generalisation.%2520However%252C%2520existing%2520pooling%2520operations%2520often%2520optimise%2520for%2520the%2520learning%2520task%2520at%2520the%2520expense%2520of%2520discarding%2520fundamental%2520graph%2520structures%252C%2520thus%2520reducing%2520interpretability.%2520This%2520leads%2520to%2520unreliable%2520performance%2520across%2520dataset%2520types%252C%2520downstream%2520tasks%2520and%2520pooling%2520ratios.%2520Addressing%2520these%2520concerns%252C%2520we%2520propose%2520novel%2520graph%2520pooling%2520layers%2520for%2520structure-aware%2520pooling%2520via%2520edge%2520collapses.%2520Our%2520methods%2520leverage%2520diffusion%2520geometry%2520and%2520iteratively%2520reduce%2520a%2520graph%2527s%2520size%2520while%2520preserving%2520both%2520its%2520metric%2520structure%2520and%2520its%2520structural%2520diversity.%2520We%2520guide%2520pooling%2520using%2520magnitude%252C%2520an%2520isometry-invariant%2520diversity%2520measure%252C%2520which%2520permits%2520us%2520to%2520control%2520the%2520fidelity%2520of%2520the%2520pooling%2520process.%2520Further%252C%2520we%2520use%2520the%2520spread%2520of%2520a%2520metric%2520space%2520as%2520a%2520faster%2520and%2520more%2520stable%2520alternative%2520ensuring%2520computational%2520efficiency.%2520Empirical%2520results%2520demonstrate%2520that%2520our%2520methods%2520%2528i%2529%2520achieve%2520top%2520performance%2520compared%2520to%2520alternative%2520pooling%2520layers%2520across%2520a%2520range%2520of%2520diverse%2520graph%2520classification%2520tasks%252C%2520%2528ii%2529%2520preserve%2520key%2520spectral%2520properties%2520of%2520the%2520input%2520graphs%252C%2520and%2520%2528iii%2529%2520retain%2520high%2520accuracy%2520across%2520varying%2520pooling%2520ratios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11700v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Aware%20Edge%20Pooling%20for%20Graph%20Neural%20Networks&entry.906535625=Katharina%20Limbeck%20and%20Lydia%20Mezrag%20and%20Guy%20Wolf%20and%20Bastian%20Rieck&entry.1292438233=Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20significant%20success%20for%20graph-based%20tasks.%20Motivated%20by%20the%20prevalence%20of%20large%20datasets%20in%20real-world%20applications%2C%20pooling%20layers%20are%20crucial%20components%20of%20GNNs.%20By%20reducing%20the%20size%20of%20input%20graphs%2C%20pooling%20enables%20faster%20training%20and%20potentially%20better%20generalisation.%20However%2C%20existing%20pooling%20operations%20often%20optimise%20for%20the%20learning%20task%20at%20the%20expense%20of%20discarding%20fundamental%20graph%20structures%2C%20thus%20reducing%20interpretability.%20This%20leads%20to%20unreliable%20performance%20across%20dataset%20types%2C%20downstream%20tasks%20and%20pooling%20ratios.%20Addressing%20these%20concerns%2C%20we%20propose%20novel%20graph%20pooling%20layers%20for%20structure-aware%20pooling%20via%20edge%20collapses.%20Our%20methods%20leverage%20diffusion%20geometry%20and%20iteratively%20reduce%20a%20graph%27s%20size%20while%20preserving%20both%20its%20metric%20structure%20and%20its%20structural%20diversity.%20We%20guide%20pooling%20using%20magnitude%2C%20an%20isometry-invariant%20diversity%20measure%2C%20which%20permits%20us%20to%20control%20the%20fidelity%20of%20the%20pooling%20process.%20Further%2C%20we%20use%20the%20spread%20of%20a%20metric%20space%20as%20a%20faster%20and%20more%20stable%20alternative%20ensuring%20computational%20efficiency.%20Empirical%20results%20demonstrate%20that%20our%20methods%20%28i%29%20achieve%20top%20performance%20compared%20to%20alternative%20pooling%20layers%20across%20a%20range%20of%20diverse%20graph%20classification%20tasks%2C%20%28ii%29%20preserve%20key%20spectral%20properties%20of%20the%20input%20graphs%2C%20and%20%28iii%29%20retain%20high%20accuracy%20across%20varying%20pooling%20ratios.&entry.1838667208=http%3A//arxiv.org/abs/2506.11700v3&entry.124074799=Read"},
{"title": "Metaphors are a Source of Cross-Domain Misalignment of Large Reasoning Models", "author": "Zhibo Hu and Chen Wang and Yanfeng Shu and Hye-young Paik and Liming Zhu", "abstract": "Earlier research has shown that metaphors influence human's decision making, which raises the question of whether metaphors also influence large language models (LLMs)' reasoning pathways, considering their training data contain a large number of metaphors. In this work, we investigate the problem in the scope of the emergent misalignment problem where LLMs can generalize patterns learned from misaligned content in one domain to another domain. We discover a strong causal relationship between metaphors in training data and the misalignment degree of LLMs' reasoning contents. With interventions using metaphors in pre-training, fine-tuning and re-alignment phases, models' cross-domain misalignment degrees change significantly. As we delve deeper into the causes behind this phenomenon, we observe that there is a connection between metaphors and the activation of global and local latent features of large reasoning models. By monitoring these latent features, we design a detector that predict misaligned content with high accuracy.", "link": "http://arxiv.org/abs/2601.03388v2", "date": "2026-01-12", "relevancy": 2.4683, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5025}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4892}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metaphors%20are%20a%20Source%20of%20Cross-Domain%20Misalignment%20of%20Large%20Reasoning%20Models&body=Title%3A%20Metaphors%20are%20a%20Source%20of%20Cross-Domain%20Misalignment%20of%20Large%20Reasoning%20Models%0AAuthor%3A%20Zhibo%20Hu%20and%20Chen%20Wang%20and%20Yanfeng%20Shu%20and%20Hye-young%20Paik%20and%20Liming%20Zhu%0AAbstract%3A%20Earlier%20research%20has%20shown%20that%20metaphors%20influence%20human%27s%20decision%20making%2C%20which%20raises%20the%20question%20of%20whether%20metaphors%20also%20influence%20large%20language%20models%20%28LLMs%29%27%20reasoning%20pathways%2C%20considering%20their%20training%20data%20contain%20a%20large%20number%20of%20metaphors.%20In%20this%20work%2C%20we%20investigate%20the%20problem%20in%20the%20scope%20of%20the%20emergent%20misalignment%20problem%20where%20LLMs%20can%20generalize%20patterns%20learned%20from%20misaligned%20content%20in%20one%20domain%20to%20another%20domain.%20We%20discover%20a%20strong%20causal%20relationship%20between%20metaphors%20in%20training%20data%20and%20the%20misalignment%20degree%20of%20LLMs%27%20reasoning%20contents.%20With%20interventions%20using%20metaphors%20in%20pre-training%2C%20fine-tuning%20and%20re-alignment%20phases%2C%20models%27%20cross-domain%20misalignment%20degrees%20change%20significantly.%20As%20we%20delve%20deeper%20into%20the%20causes%20behind%20this%20phenomenon%2C%20we%20observe%20that%20there%20is%20a%20connection%20between%20metaphors%20and%20the%20activation%20of%20global%20and%20local%20latent%20features%20of%20large%20reasoning%20models.%20By%20monitoring%20these%20latent%20features%2C%20we%20design%20a%20detector%20that%20predict%20misaligned%20content%20with%20high%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03388v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaphors%2520are%2520a%2520Source%2520of%2520Cross-Domain%2520Misalignment%2520of%2520Large%2520Reasoning%2520Models%26entry.906535625%3DZhibo%2520Hu%2520and%2520Chen%2520Wang%2520and%2520Yanfeng%2520Shu%2520and%2520Hye-young%2520Paik%2520and%2520Liming%2520Zhu%26entry.1292438233%3DEarlier%2520research%2520has%2520shown%2520that%2520metaphors%2520influence%2520human%2527s%2520decision%2520making%252C%2520which%2520raises%2520the%2520question%2520of%2520whether%2520metaphors%2520also%2520influence%2520large%2520language%2520models%2520%2528LLMs%2529%2527%2520reasoning%2520pathways%252C%2520considering%2520their%2520training%2520data%2520contain%2520a%2520large%2520number%2520of%2520metaphors.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520problem%2520in%2520the%2520scope%2520of%2520the%2520emergent%2520misalignment%2520problem%2520where%2520LLMs%2520can%2520generalize%2520patterns%2520learned%2520from%2520misaligned%2520content%2520in%2520one%2520domain%2520to%2520another%2520domain.%2520We%2520discover%2520a%2520strong%2520causal%2520relationship%2520between%2520metaphors%2520in%2520training%2520data%2520and%2520the%2520misalignment%2520degree%2520of%2520LLMs%2527%2520reasoning%2520contents.%2520With%2520interventions%2520using%2520metaphors%2520in%2520pre-training%252C%2520fine-tuning%2520and%2520re-alignment%2520phases%252C%2520models%2527%2520cross-domain%2520misalignment%2520degrees%2520change%2520significantly.%2520As%2520we%2520delve%2520deeper%2520into%2520the%2520causes%2520behind%2520this%2520phenomenon%252C%2520we%2520observe%2520that%2520there%2520is%2520a%2520connection%2520between%2520metaphors%2520and%2520the%2520activation%2520of%2520global%2520and%2520local%2520latent%2520features%2520of%2520large%2520reasoning%2520models.%2520By%2520monitoring%2520these%2520latent%2520features%252C%2520we%2520design%2520a%2520detector%2520that%2520predict%2520misaligned%2520content%2520with%2520high%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03388v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metaphors%20are%20a%20Source%20of%20Cross-Domain%20Misalignment%20of%20Large%20Reasoning%20Models&entry.906535625=Zhibo%20Hu%20and%20Chen%20Wang%20and%20Yanfeng%20Shu%20and%20Hye-young%20Paik%20and%20Liming%20Zhu&entry.1292438233=Earlier%20research%20has%20shown%20that%20metaphors%20influence%20human%27s%20decision%20making%2C%20which%20raises%20the%20question%20of%20whether%20metaphors%20also%20influence%20large%20language%20models%20%28LLMs%29%27%20reasoning%20pathways%2C%20considering%20their%20training%20data%20contain%20a%20large%20number%20of%20metaphors.%20In%20this%20work%2C%20we%20investigate%20the%20problem%20in%20the%20scope%20of%20the%20emergent%20misalignment%20problem%20where%20LLMs%20can%20generalize%20patterns%20learned%20from%20misaligned%20content%20in%20one%20domain%20to%20another%20domain.%20We%20discover%20a%20strong%20causal%20relationship%20between%20metaphors%20in%20training%20data%20and%20the%20misalignment%20degree%20of%20LLMs%27%20reasoning%20contents.%20With%20interventions%20using%20metaphors%20in%20pre-training%2C%20fine-tuning%20and%20re-alignment%20phases%2C%20models%27%20cross-domain%20misalignment%20degrees%20change%20significantly.%20As%20we%20delve%20deeper%20into%20the%20causes%20behind%20this%20phenomenon%2C%20we%20observe%20that%20there%20is%20a%20connection%20between%20metaphors%20and%20the%20activation%20of%20global%20and%20local%20latent%20features%20of%20large%20reasoning%20models.%20By%20monitoring%20these%20latent%20features%2C%20we%20design%20a%20detector%20that%20predict%20misaligned%20content%20with%20high%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2601.03388v2&entry.124074799=Read"},
{"title": "Enhancing Binary Encoded Crime Linkage Analysis Using Siamese Network", "author": "Yicheng Zhan and Fahim Ahmed and Amy Burrell and Matthew J. Tonkin and Sarah Galambos and Jessica Woodhams and Dalal Alrajeh", "abstract": "Effective crime linkage analysis is crucial for identifying serial offenders and enhancing public safety. To address limitations of traditional crime linkage methods in handling high-dimensional, sparse, and heterogeneous data, we propose a Siamese Autoencoder framework that learns meaningful latent representations and uncovers correlations in complex crime data. Using data from the Violent Crime Linkage Analysis System (ViCLAS), maintained by the Serious Crime Analysis Section of the UK's National Crime Agency, our approach mitigates signal dilution in sparse feature spaces by integrating geographic-temporal features at the decoder stage. This design amplifies behavioral representations rather than allowing them to be overshadowed at the input level, yielding consistent improvements across multiple evaluation metrics. We further analyze how different domain-informed data reduction strategies influence model performance, providing practical guidance for preprocessing in crime linkage contexts. Our results show that advanced machine learning approaches can substantially enhance linkage accuracy, improving AUC by up to 9% over traditional methods while offering interpretable insights to support investigative decision-making.", "link": "http://arxiv.org/abs/2511.07651v2", "date": "2026-01-12", "relevancy": 2.4507, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Binary%20Encoded%20Crime%20Linkage%20Analysis%20Using%20Siamese%20Network&body=Title%3A%20Enhancing%20Binary%20Encoded%20Crime%20Linkage%20Analysis%20Using%20Siamese%20Network%0AAuthor%3A%20Yicheng%20Zhan%20and%20Fahim%20Ahmed%20and%20Amy%20Burrell%20and%20Matthew%20J.%20Tonkin%20and%20Sarah%20Galambos%20and%20Jessica%20Woodhams%20and%20Dalal%20Alrajeh%0AAbstract%3A%20Effective%20crime%20linkage%20analysis%20is%20crucial%20for%20identifying%20serial%20offenders%20and%20enhancing%20public%20safety.%20To%20address%20limitations%20of%20traditional%20crime%20linkage%20methods%20in%20handling%20high-dimensional%2C%20sparse%2C%20and%20heterogeneous%20data%2C%20we%20propose%20a%20Siamese%20Autoencoder%20framework%20that%20learns%20meaningful%20latent%20representations%20and%20uncovers%20correlations%20in%20complex%20crime%20data.%20Using%20data%20from%20the%20Violent%20Crime%20Linkage%20Analysis%20System%20%28ViCLAS%29%2C%20maintained%20by%20the%20Serious%20Crime%20Analysis%20Section%20of%20the%20UK%27s%20National%20Crime%20Agency%2C%20our%20approach%20mitigates%20signal%20dilution%20in%20sparse%20feature%20spaces%20by%20integrating%20geographic-temporal%20features%20at%20the%20decoder%20stage.%20This%20design%20amplifies%20behavioral%20representations%20rather%20than%20allowing%20them%20to%20be%20overshadowed%20at%20the%20input%20level%2C%20yielding%20consistent%20improvements%20across%20multiple%20evaluation%20metrics.%20We%20further%20analyze%20how%20different%20domain-informed%20data%20reduction%20strategies%20influence%20model%20performance%2C%20providing%20practical%20guidance%20for%20preprocessing%20in%20crime%20linkage%20contexts.%20Our%20results%20show%20that%20advanced%20machine%20learning%20approaches%20can%20substantially%20enhance%20linkage%20accuracy%2C%20improving%20AUC%20by%20up%20to%209%25%20over%20traditional%20methods%20while%20offering%20interpretable%20insights%20to%20support%20investigative%20decision-making.%0ALink%3A%20http%3A//arxiv.org/abs/2511.07651v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Binary%2520Encoded%2520Crime%2520Linkage%2520Analysis%2520Using%2520Siamese%2520Network%26entry.906535625%3DYicheng%2520Zhan%2520and%2520Fahim%2520Ahmed%2520and%2520Amy%2520Burrell%2520and%2520Matthew%2520J.%2520Tonkin%2520and%2520Sarah%2520Galambos%2520and%2520Jessica%2520Woodhams%2520and%2520Dalal%2520Alrajeh%26entry.1292438233%3DEffective%2520crime%2520linkage%2520analysis%2520is%2520crucial%2520for%2520identifying%2520serial%2520offenders%2520and%2520enhancing%2520public%2520safety.%2520To%2520address%2520limitations%2520of%2520traditional%2520crime%2520linkage%2520methods%2520in%2520handling%2520high-dimensional%252C%2520sparse%252C%2520and%2520heterogeneous%2520data%252C%2520we%2520propose%2520a%2520Siamese%2520Autoencoder%2520framework%2520that%2520learns%2520meaningful%2520latent%2520representations%2520and%2520uncovers%2520correlations%2520in%2520complex%2520crime%2520data.%2520Using%2520data%2520from%2520the%2520Violent%2520Crime%2520Linkage%2520Analysis%2520System%2520%2528ViCLAS%2529%252C%2520maintained%2520by%2520the%2520Serious%2520Crime%2520Analysis%2520Section%2520of%2520the%2520UK%2527s%2520National%2520Crime%2520Agency%252C%2520our%2520approach%2520mitigates%2520signal%2520dilution%2520in%2520sparse%2520feature%2520spaces%2520by%2520integrating%2520geographic-temporal%2520features%2520at%2520the%2520decoder%2520stage.%2520This%2520design%2520amplifies%2520behavioral%2520representations%2520rather%2520than%2520allowing%2520them%2520to%2520be%2520overshadowed%2520at%2520the%2520input%2520level%252C%2520yielding%2520consistent%2520improvements%2520across%2520multiple%2520evaluation%2520metrics.%2520We%2520further%2520analyze%2520how%2520different%2520domain-informed%2520data%2520reduction%2520strategies%2520influence%2520model%2520performance%252C%2520providing%2520practical%2520guidance%2520for%2520preprocessing%2520in%2520crime%2520linkage%2520contexts.%2520Our%2520results%2520show%2520that%2520advanced%2520machine%2520learning%2520approaches%2520can%2520substantially%2520enhance%2520linkage%2520accuracy%252C%2520improving%2520AUC%2520by%2520up%2520to%25209%2525%2520over%2520traditional%2520methods%2520while%2520offering%2520interpretable%2520insights%2520to%2520support%2520investigative%2520decision-making.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.07651v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Binary%20Encoded%20Crime%20Linkage%20Analysis%20Using%20Siamese%20Network&entry.906535625=Yicheng%20Zhan%20and%20Fahim%20Ahmed%20and%20Amy%20Burrell%20and%20Matthew%20J.%20Tonkin%20and%20Sarah%20Galambos%20and%20Jessica%20Woodhams%20and%20Dalal%20Alrajeh&entry.1292438233=Effective%20crime%20linkage%20analysis%20is%20crucial%20for%20identifying%20serial%20offenders%20and%20enhancing%20public%20safety.%20To%20address%20limitations%20of%20traditional%20crime%20linkage%20methods%20in%20handling%20high-dimensional%2C%20sparse%2C%20and%20heterogeneous%20data%2C%20we%20propose%20a%20Siamese%20Autoencoder%20framework%20that%20learns%20meaningful%20latent%20representations%20and%20uncovers%20correlations%20in%20complex%20crime%20data.%20Using%20data%20from%20the%20Violent%20Crime%20Linkage%20Analysis%20System%20%28ViCLAS%29%2C%20maintained%20by%20the%20Serious%20Crime%20Analysis%20Section%20of%20the%20UK%27s%20National%20Crime%20Agency%2C%20our%20approach%20mitigates%20signal%20dilution%20in%20sparse%20feature%20spaces%20by%20integrating%20geographic-temporal%20features%20at%20the%20decoder%20stage.%20This%20design%20amplifies%20behavioral%20representations%20rather%20than%20allowing%20them%20to%20be%20overshadowed%20at%20the%20input%20level%2C%20yielding%20consistent%20improvements%20across%20multiple%20evaluation%20metrics.%20We%20further%20analyze%20how%20different%20domain-informed%20data%20reduction%20strategies%20influence%20model%20performance%2C%20providing%20practical%20guidance%20for%20preprocessing%20in%20crime%20linkage%20contexts.%20Our%20results%20show%20that%20advanced%20machine%20learning%20approaches%20can%20substantially%20enhance%20linkage%20accuracy%2C%20improving%20AUC%20by%20up%20to%209%25%20over%20traditional%20methods%20while%20offering%20interpretable%20insights%20to%20support%20investigative%20decision-making.&entry.1838667208=http%3A//arxiv.org/abs/2511.07651v2&entry.124074799=Read"},
{"title": "KALE: Enhancing Knowledge Manipulation in Large Language Models via Knowledge-aware Learning", "author": "Qitan Lv and Tianyu Liu and Qiaosheng Zhang and Xingcheng Xu and Chaochao Lu", "abstract": "Despite the impressive performance of large language models (LLMs) pretrained on vast knowledge corpora, advancing their knowledge manipulation-the ability to effectively recall, reason, and transfer relevant knowledge-remains challenging. Existing methods mainly leverage Supervised Fine-Tuning (SFT) on labeled datasets to enhance LLMs' knowledge manipulation ability. However, we observe that SFT models still exhibit the known&incorrect phenomenon, where they explicitly possess relevant knowledge for a given question but fail to leverage it for correct answers. To address this challenge, we propose KALE (Knowledge-Aware LEarning)-a post-training framework that leverages knowledge graphs (KGs) to generate high-quality rationales and enhance LLMs' knowledge manipulation ability. Specifically, KALE first introduces a Knowledge-Induced (KI) data synthesis method that efficiently extracts multi-hop reasoning paths from KGs to generate high-quality rationales for question-answer pairs. Then, KALE employs a Knowledge-Aware (KA) fine-tuning paradigm that enhances knowledge manipulation by internalizing rationale-guided reasoning through minimizing the KL divergence between predictions with and without rationales. Extensive experiments on eight popular benchmarks across six different LLMs demonstrate the effectiveness of KALE, achieving accuracy improvements of up to 11.72% and an average of 4.18%.", "link": "http://arxiv.org/abs/2601.07430v1", "date": "2026-01-12", "relevancy": 2.446, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4903}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4903}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KALE%3A%20Enhancing%20Knowledge%20Manipulation%20in%20Large%20Language%20Models%20via%20Knowledge-aware%20Learning&body=Title%3A%20KALE%3A%20Enhancing%20Knowledge%20Manipulation%20in%20Large%20Language%20Models%20via%20Knowledge-aware%20Learning%0AAuthor%3A%20Qitan%20Lv%20and%20Tianyu%20Liu%20and%20Qiaosheng%20Zhang%20and%20Xingcheng%20Xu%20and%20Chaochao%20Lu%0AAbstract%3A%20Despite%20the%20impressive%20performance%20of%20large%20language%20models%20%28LLMs%29%20pretrained%20on%20vast%20knowledge%20corpora%2C%20advancing%20their%20knowledge%20manipulation-the%20ability%20to%20effectively%20recall%2C%20reason%2C%20and%20transfer%20relevant%20knowledge-remains%20challenging.%20Existing%20methods%20mainly%20leverage%20Supervised%20Fine-Tuning%20%28SFT%29%20on%20labeled%20datasets%20to%20enhance%20LLMs%27%20knowledge%20manipulation%20ability.%20However%2C%20we%20observe%20that%20SFT%20models%20still%20exhibit%20the%20known%26incorrect%20phenomenon%2C%20where%20they%20explicitly%20possess%20relevant%20knowledge%20for%20a%20given%20question%20but%20fail%20to%20leverage%20it%20for%20correct%20answers.%20To%20address%20this%20challenge%2C%20we%20propose%20KALE%20%28Knowledge-Aware%20LEarning%29-a%20post-training%20framework%20that%20leverages%20knowledge%20graphs%20%28KGs%29%20to%20generate%20high-quality%20rationales%20and%20enhance%20LLMs%27%20knowledge%20manipulation%20ability.%20Specifically%2C%20KALE%20first%20introduces%20a%20Knowledge-Induced%20%28KI%29%20data%20synthesis%20method%20that%20efficiently%20extracts%20multi-hop%20reasoning%20paths%20from%20KGs%20to%20generate%20high-quality%20rationales%20for%20question-answer%20pairs.%20Then%2C%20KALE%20employs%20a%20Knowledge-Aware%20%28KA%29%20fine-tuning%20paradigm%20that%20enhances%20knowledge%20manipulation%20by%20internalizing%20rationale-guided%20reasoning%20through%20minimizing%20the%20KL%20divergence%20between%20predictions%20with%20and%20without%20rationales.%20Extensive%20experiments%20on%20eight%20popular%20benchmarks%20across%20six%20different%20LLMs%20demonstrate%20the%20effectiveness%20of%20KALE%2C%20achieving%20accuracy%20improvements%20of%20up%20to%2011.72%25%20and%20an%20average%20of%204.18%25.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKALE%253A%2520Enhancing%2520Knowledge%2520Manipulation%2520in%2520Large%2520Language%2520Models%2520via%2520Knowledge-aware%2520Learning%26entry.906535625%3DQitan%2520Lv%2520and%2520Tianyu%2520Liu%2520and%2520Qiaosheng%2520Zhang%2520and%2520Xingcheng%2520Xu%2520and%2520Chaochao%2520Lu%26entry.1292438233%3DDespite%2520the%2520impressive%2520performance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520pretrained%2520on%2520vast%2520knowledge%2520corpora%252C%2520advancing%2520their%2520knowledge%2520manipulation-the%2520ability%2520to%2520effectively%2520recall%252C%2520reason%252C%2520and%2520transfer%2520relevant%2520knowledge-remains%2520challenging.%2520Existing%2520methods%2520mainly%2520leverage%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520on%2520labeled%2520datasets%2520to%2520enhance%2520LLMs%2527%2520knowledge%2520manipulation%2520ability.%2520However%252C%2520we%2520observe%2520that%2520SFT%2520models%2520still%2520exhibit%2520the%2520known%2526incorrect%2520phenomenon%252C%2520where%2520they%2520explicitly%2520possess%2520relevant%2520knowledge%2520for%2520a%2520given%2520question%2520but%2520fail%2520to%2520leverage%2520it%2520for%2520correct%2520answers.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520KALE%2520%2528Knowledge-Aware%2520LEarning%2529-a%2520post-training%2520framework%2520that%2520leverages%2520knowledge%2520graphs%2520%2528KGs%2529%2520to%2520generate%2520high-quality%2520rationales%2520and%2520enhance%2520LLMs%2527%2520knowledge%2520manipulation%2520ability.%2520Specifically%252C%2520KALE%2520first%2520introduces%2520a%2520Knowledge-Induced%2520%2528KI%2529%2520data%2520synthesis%2520method%2520that%2520efficiently%2520extracts%2520multi-hop%2520reasoning%2520paths%2520from%2520KGs%2520to%2520generate%2520high-quality%2520rationales%2520for%2520question-answer%2520pairs.%2520Then%252C%2520KALE%2520employs%2520a%2520Knowledge-Aware%2520%2528KA%2529%2520fine-tuning%2520paradigm%2520that%2520enhances%2520knowledge%2520manipulation%2520by%2520internalizing%2520rationale-guided%2520reasoning%2520through%2520minimizing%2520the%2520KL%2520divergence%2520between%2520predictions%2520with%2520and%2520without%2520rationales.%2520Extensive%2520experiments%2520on%2520eight%2520popular%2520benchmarks%2520across%2520six%2520different%2520LLMs%2520demonstrate%2520the%2520effectiveness%2520of%2520KALE%252C%2520achieving%2520accuracy%2520improvements%2520of%2520up%2520to%252011.72%2525%2520and%2520an%2520average%2520of%25204.18%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KALE%3A%20Enhancing%20Knowledge%20Manipulation%20in%20Large%20Language%20Models%20via%20Knowledge-aware%20Learning&entry.906535625=Qitan%20Lv%20and%20Tianyu%20Liu%20and%20Qiaosheng%20Zhang%20and%20Xingcheng%20Xu%20and%20Chaochao%20Lu&entry.1292438233=Despite%20the%20impressive%20performance%20of%20large%20language%20models%20%28LLMs%29%20pretrained%20on%20vast%20knowledge%20corpora%2C%20advancing%20their%20knowledge%20manipulation-the%20ability%20to%20effectively%20recall%2C%20reason%2C%20and%20transfer%20relevant%20knowledge-remains%20challenging.%20Existing%20methods%20mainly%20leverage%20Supervised%20Fine-Tuning%20%28SFT%29%20on%20labeled%20datasets%20to%20enhance%20LLMs%27%20knowledge%20manipulation%20ability.%20However%2C%20we%20observe%20that%20SFT%20models%20still%20exhibit%20the%20known%26incorrect%20phenomenon%2C%20where%20they%20explicitly%20possess%20relevant%20knowledge%20for%20a%20given%20question%20but%20fail%20to%20leverage%20it%20for%20correct%20answers.%20To%20address%20this%20challenge%2C%20we%20propose%20KALE%20%28Knowledge-Aware%20LEarning%29-a%20post-training%20framework%20that%20leverages%20knowledge%20graphs%20%28KGs%29%20to%20generate%20high-quality%20rationales%20and%20enhance%20LLMs%27%20knowledge%20manipulation%20ability.%20Specifically%2C%20KALE%20first%20introduces%20a%20Knowledge-Induced%20%28KI%29%20data%20synthesis%20method%20that%20efficiently%20extracts%20multi-hop%20reasoning%20paths%20from%20KGs%20to%20generate%20high-quality%20rationales%20for%20question-answer%20pairs.%20Then%2C%20KALE%20employs%20a%20Knowledge-Aware%20%28KA%29%20fine-tuning%20paradigm%20that%20enhances%20knowledge%20manipulation%20by%20internalizing%20rationale-guided%20reasoning%20through%20minimizing%20the%20KL%20divergence%20between%20predictions%20with%20and%20without%20rationales.%20Extensive%20experiments%20on%20eight%20popular%20benchmarks%20across%20six%20different%20LLMs%20demonstrate%20the%20effectiveness%20of%20KALE%2C%20achieving%20accuracy%20improvements%20of%20up%20to%2011.72%25%20and%20an%20average%20of%204.18%25.&entry.1838667208=http%3A//arxiv.org/abs/2601.07430v1&entry.124074799=Read"},
{"title": "FBFL: A Field-Based Coordination Approach for Data Heterogeneity in Federated Learning", "author": "Davide Domini and Gianluca Aguzzi and Lukas Esterle and Mirko Viroli", "abstract": "In the last years, Federated learning (FL) has become a popular solution to train machine learning models in domains with high privacy concerns. However, FL scalability and performance face significant challenges in real-world deployments where data across devices are non-independently and identically distributed (non-IID). The heterogeneity in data distribution frequently arises from spatial distribution of devices, leading to degraded model performance in the absence of proper handling. Additionally, FL typical reliance on centralized architectures introduces bottlenecks and single-point-of-failure risks, particularly problematic at scale or in dynamic environments. To close this gap, we propose Field-Based Federated Learning (FBFL), a novel approach leveraging macroprogramming and field coordination to address these limitations through: (i) distributed spatial-based leader election for personalization to mitigate non-IID data challenges; and (ii) construction of a self-organizing, hierarchical architecture using advanced macroprogramming patterns. Moreover, FBFL not only overcomes the aforementioned limitations, but also enables the development of more specialized models tailored to the specific data distribution in each subregion. This paper formalizes FBFL and evaluates it extensively using MNIST, FashionMNIST, and Extended MNIST datasets. We demonstrate that, when operating under IID data conditions, FBFL performs comparably to the widely-used FedAvg algorithm. Furthermore, in challenging non-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other state-of-the-art methods, namely FedProx and Scaffold, which have been specifically designed to address non-IID data distributions. Additionally, we showcase the resilience of FBFL's self-organizing hierarchical architecture against server failures.", "link": "http://arxiv.org/abs/2502.08577v3", "date": "2026-01-12", "relevancy": 2.4452, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4957}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4946}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FBFL%3A%20A%20Field-Based%20Coordination%20Approach%20for%20Data%20Heterogeneity%20in%20Federated%20Learning&body=Title%3A%20FBFL%3A%20A%20Field-Based%20Coordination%20Approach%20for%20Data%20Heterogeneity%20in%20Federated%20Learning%0AAuthor%3A%20Davide%20Domini%20and%20Gianluca%20Aguzzi%20and%20Lukas%20Esterle%20and%20Mirko%20Viroli%0AAbstract%3A%20In%20the%20last%20years%2C%20Federated%20learning%20%28FL%29%20has%20become%20a%20popular%20solution%20to%20train%20machine%20learning%20models%20in%20domains%20with%20high%20privacy%20concerns.%20However%2C%20FL%20scalability%20and%20performance%20face%20significant%20challenges%20in%20real-world%20deployments%20where%20data%20across%20devices%20are%20non-independently%20and%20identically%20distributed%20%28non-IID%29.%20The%20heterogeneity%20in%20data%20distribution%20frequently%20arises%20from%20spatial%20distribution%20of%20devices%2C%20leading%20to%20degraded%20model%20performance%20in%20the%20absence%20of%20proper%20handling.%20Additionally%2C%20FL%20typical%20reliance%20on%20centralized%20architectures%20introduces%20bottlenecks%20and%20single-point-of-failure%20risks%2C%20particularly%20problematic%20at%20scale%20or%20in%20dynamic%20environments.%20To%20close%20this%20gap%2C%20we%20propose%20Field-Based%20Federated%20Learning%20%28FBFL%29%2C%20a%20novel%20approach%20leveraging%20macroprogramming%20and%20field%20coordination%20to%20address%20these%20limitations%20through%3A%20%28i%29%20distributed%20spatial-based%20leader%20election%20for%20personalization%20to%20mitigate%20non-IID%20data%20challenges%3B%20and%20%28ii%29%20construction%20of%20a%20self-organizing%2C%20hierarchical%20architecture%20using%20advanced%20macroprogramming%20patterns.%20Moreover%2C%20FBFL%20not%20only%20overcomes%20the%20aforementioned%20limitations%2C%20but%20also%20enables%20the%20development%20of%20more%20specialized%20models%20tailored%20to%20the%20specific%20data%20distribution%20in%20each%20subregion.%20This%20paper%20formalizes%20FBFL%20and%20evaluates%20it%20extensively%20using%20MNIST%2C%20FashionMNIST%2C%20and%20Extended%20MNIST%20datasets.%20We%20demonstrate%20that%2C%20when%20operating%20under%20IID%20data%20conditions%2C%20FBFL%20performs%20comparably%20to%20the%20widely-used%20FedAvg%20algorithm.%20Furthermore%2C%20in%20challenging%20non-IID%20scenarios%2C%20FBFL%20not%20only%20outperforms%20FedAvg%20but%20also%20surpasses%20other%20state-of-the-art%20methods%2C%20namely%20FedProx%20and%20Scaffold%2C%20which%20have%20been%20specifically%20designed%20to%20address%20non-IID%20data%20distributions.%20Additionally%2C%20we%20showcase%20the%20resilience%20of%20FBFL%27s%20self-organizing%20hierarchical%20architecture%20against%20server%20failures.%0ALink%3A%20http%3A//arxiv.org/abs/2502.08577v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFBFL%253A%2520A%2520Field-Based%2520Coordination%2520Approach%2520for%2520Data%2520Heterogeneity%2520in%2520Federated%2520Learning%26entry.906535625%3DDavide%2520Domini%2520and%2520Gianluca%2520Aguzzi%2520and%2520Lukas%2520Esterle%2520and%2520Mirko%2520Viroli%26entry.1292438233%3DIn%2520the%2520last%2520years%252C%2520Federated%2520learning%2520%2528FL%2529%2520has%2520become%2520a%2520popular%2520solution%2520to%2520train%2520machine%2520learning%2520models%2520in%2520domains%2520with%2520high%2520privacy%2520concerns.%2520However%252C%2520FL%2520scalability%2520and%2520performance%2520face%2520significant%2520challenges%2520in%2520real-world%2520deployments%2520where%2520data%2520across%2520devices%2520are%2520non-independently%2520and%2520identically%2520distributed%2520%2528non-IID%2529.%2520The%2520heterogeneity%2520in%2520data%2520distribution%2520frequently%2520arises%2520from%2520spatial%2520distribution%2520of%2520devices%252C%2520leading%2520to%2520degraded%2520model%2520performance%2520in%2520the%2520absence%2520of%2520proper%2520handling.%2520Additionally%252C%2520FL%2520typical%2520reliance%2520on%2520centralized%2520architectures%2520introduces%2520bottlenecks%2520and%2520single-point-of-failure%2520risks%252C%2520particularly%2520problematic%2520at%2520scale%2520or%2520in%2520dynamic%2520environments.%2520To%2520close%2520this%2520gap%252C%2520we%2520propose%2520Field-Based%2520Federated%2520Learning%2520%2528FBFL%2529%252C%2520a%2520novel%2520approach%2520leveraging%2520macroprogramming%2520and%2520field%2520coordination%2520to%2520address%2520these%2520limitations%2520through%253A%2520%2528i%2529%2520distributed%2520spatial-based%2520leader%2520election%2520for%2520personalization%2520to%2520mitigate%2520non-IID%2520data%2520challenges%253B%2520and%2520%2528ii%2529%2520construction%2520of%2520a%2520self-organizing%252C%2520hierarchical%2520architecture%2520using%2520advanced%2520macroprogramming%2520patterns.%2520Moreover%252C%2520FBFL%2520not%2520only%2520overcomes%2520the%2520aforementioned%2520limitations%252C%2520but%2520also%2520enables%2520the%2520development%2520of%2520more%2520specialized%2520models%2520tailored%2520to%2520the%2520specific%2520data%2520distribution%2520in%2520each%2520subregion.%2520This%2520paper%2520formalizes%2520FBFL%2520and%2520evaluates%2520it%2520extensively%2520using%2520MNIST%252C%2520FashionMNIST%252C%2520and%2520Extended%2520MNIST%2520datasets.%2520We%2520demonstrate%2520that%252C%2520when%2520operating%2520under%2520IID%2520data%2520conditions%252C%2520FBFL%2520performs%2520comparably%2520to%2520the%2520widely-used%2520FedAvg%2520algorithm.%2520Furthermore%252C%2520in%2520challenging%2520non-IID%2520scenarios%252C%2520FBFL%2520not%2520only%2520outperforms%2520FedAvg%2520but%2520also%2520surpasses%2520other%2520state-of-the-art%2520methods%252C%2520namely%2520FedProx%2520and%2520Scaffold%252C%2520which%2520have%2520been%2520specifically%2520designed%2520to%2520address%2520non-IID%2520data%2520distributions.%2520Additionally%252C%2520we%2520showcase%2520the%2520resilience%2520of%2520FBFL%2527s%2520self-organizing%2520hierarchical%2520architecture%2520against%2520server%2520failures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08577v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FBFL%3A%20A%20Field-Based%20Coordination%20Approach%20for%20Data%20Heterogeneity%20in%20Federated%20Learning&entry.906535625=Davide%20Domini%20and%20Gianluca%20Aguzzi%20and%20Lukas%20Esterle%20and%20Mirko%20Viroli&entry.1292438233=In%20the%20last%20years%2C%20Federated%20learning%20%28FL%29%20has%20become%20a%20popular%20solution%20to%20train%20machine%20learning%20models%20in%20domains%20with%20high%20privacy%20concerns.%20However%2C%20FL%20scalability%20and%20performance%20face%20significant%20challenges%20in%20real-world%20deployments%20where%20data%20across%20devices%20are%20non-independently%20and%20identically%20distributed%20%28non-IID%29.%20The%20heterogeneity%20in%20data%20distribution%20frequently%20arises%20from%20spatial%20distribution%20of%20devices%2C%20leading%20to%20degraded%20model%20performance%20in%20the%20absence%20of%20proper%20handling.%20Additionally%2C%20FL%20typical%20reliance%20on%20centralized%20architectures%20introduces%20bottlenecks%20and%20single-point-of-failure%20risks%2C%20particularly%20problematic%20at%20scale%20or%20in%20dynamic%20environments.%20To%20close%20this%20gap%2C%20we%20propose%20Field-Based%20Federated%20Learning%20%28FBFL%29%2C%20a%20novel%20approach%20leveraging%20macroprogramming%20and%20field%20coordination%20to%20address%20these%20limitations%20through%3A%20%28i%29%20distributed%20spatial-based%20leader%20election%20for%20personalization%20to%20mitigate%20non-IID%20data%20challenges%3B%20and%20%28ii%29%20construction%20of%20a%20self-organizing%2C%20hierarchical%20architecture%20using%20advanced%20macroprogramming%20patterns.%20Moreover%2C%20FBFL%20not%20only%20overcomes%20the%20aforementioned%20limitations%2C%20but%20also%20enables%20the%20development%20of%20more%20specialized%20models%20tailored%20to%20the%20specific%20data%20distribution%20in%20each%20subregion.%20This%20paper%20formalizes%20FBFL%20and%20evaluates%20it%20extensively%20using%20MNIST%2C%20FashionMNIST%2C%20and%20Extended%20MNIST%20datasets.%20We%20demonstrate%20that%2C%20when%20operating%20under%20IID%20data%20conditions%2C%20FBFL%20performs%20comparably%20to%20the%20widely-used%20FedAvg%20algorithm.%20Furthermore%2C%20in%20challenging%20non-IID%20scenarios%2C%20FBFL%20not%20only%20outperforms%20FedAvg%20but%20also%20surpasses%20other%20state-of-the-art%20methods%2C%20namely%20FedProx%20and%20Scaffold%2C%20which%20have%20been%20specifically%20designed%20to%20address%20non-IID%20data%20distributions.%20Additionally%2C%20we%20showcase%20the%20resilience%20of%20FBFL%27s%20self-organizing%20hierarchical%20architecture%20against%20server%20failures.&entry.1838667208=http%3A//arxiv.org/abs/2502.08577v3&entry.124074799=Read"},
{"title": "Lightweight Neural Framework for Robust 3D Volume and Surface Estimation from Multi-View Images", "author": "Diego Eustachio Farchione and Ramzi Idoughi and Peter Wonka", "abstract": "Accurate estimation of object volume and surface area from visual data is an open challenge with broad implications across various domains. We propose a unified framework that predicts volumetric and surface metrics directly from a set of 2D multi-view images. Our approach first generates a point cloud from the captured multi-view images using recent 3D reconstruction techniques, while a parallel 2D encoder aggregates view-aligned features. A fusion module then aligns and merges 3D geometry with 2D visual embeddings, followed by a graph-based decoder that regresses volume, surface area, and their corresponding uncertainties. This proposed architecture maintains robustness against sparse or noisy data. We evaluate the framework across multiple application domains: corals, where precise geometric measurements support growth monitoring; food items, where volume prediction relates to dietary tracking and portion analysis; and human bodies, where volumetric cues are crucial for anthropometric and medical applications. Experimental results demonstrate the reliable performance of our framework across diverse scenarios, highlighting its versatility and adaptability. Furthermore, by coupling 3D reconstruction with neural regression and 2D features, our model provides a scalable and fast solution for quantitative shape analysis from visual data.", "link": "http://arxiv.org/abs/2509.11164v2", "date": "2026-01-12", "relevancy": 2.4229, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6083}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6083}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Neural%20Framework%20for%20Robust%203D%20Volume%20and%20Surface%20Estimation%20from%20Multi-View%20Images&body=Title%3A%20Lightweight%20Neural%20Framework%20for%20Robust%203D%20Volume%20and%20Surface%20Estimation%20from%20Multi-View%20Images%0AAuthor%3A%20Diego%20Eustachio%20Farchione%20and%20Ramzi%20Idoughi%20and%20Peter%20Wonka%0AAbstract%3A%20Accurate%20estimation%20of%20object%20volume%20and%20surface%20area%20from%20visual%20data%20is%20an%20open%20challenge%20with%20broad%20implications%20across%20various%20domains.%20We%20propose%20a%20unified%20framework%20that%20predicts%20volumetric%20and%20surface%20metrics%20directly%20from%20a%20set%20of%202D%20multi-view%20images.%20Our%20approach%20first%20generates%20a%20point%20cloud%20from%20the%20captured%20multi-view%20images%20using%20recent%203D%20reconstruction%20techniques%2C%20while%20a%20parallel%202D%20encoder%20aggregates%20view-aligned%20features.%20A%20fusion%20module%20then%20aligns%20and%20merges%203D%20geometry%20with%202D%20visual%20embeddings%2C%20followed%20by%20a%20graph-based%20decoder%20that%20regresses%20volume%2C%20surface%20area%2C%20and%20their%20corresponding%20uncertainties.%20This%20proposed%20architecture%20maintains%20robustness%20against%20sparse%20or%20noisy%20data.%20We%20evaluate%20the%20framework%20across%20multiple%20application%20domains%3A%20corals%2C%20where%20precise%20geometric%20measurements%20support%20growth%20monitoring%3B%20food%20items%2C%20where%20volume%20prediction%20relates%20to%20dietary%20tracking%20and%20portion%20analysis%3B%20and%20human%20bodies%2C%20where%20volumetric%20cues%20are%20crucial%20for%20anthropometric%20and%20medical%20applications.%20Experimental%20results%20demonstrate%20the%20reliable%20performance%20of%20our%20framework%20across%20diverse%20scenarios%2C%20highlighting%20its%20versatility%20and%20adaptability.%20Furthermore%2C%20by%20coupling%203D%20reconstruction%20with%20neural%20regression%20and%202D%20features%2C%20our%20model%20provides%20a%20scalable%20and%20fast%20solution%20for%20quantitative%20shape%20analysis%20from%20visual%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2509.11164v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Neural%2520Framework%2520for%2520Robust%25203D%2520Volume%2520and%2520Surface%2520Estimation%2520from%2520Multi-View%2520Images%26entry.906535625%3DDiego%2520Eustachio%2520Farchione%2520and%2520Ramzi%2520Idoughi%2520and%2520Peter%2520Wonka%26entry.1292438233%3DAccurate%2520estimation%2520of%2520object%2520volume%2520and%2520surface%2520area%2520from%2520visual%2520data%2520is%2520an%2520open%2520challenge%2520with%2520broad%2520implications%2520across%2520various%2520domains.%2520We%2520propose%2520a%2520unified%2520framework%2520that%2520predicts%2520volumetric%2520and%2520surface%2520metrics%2520directly%2520from%2520a%2520set%2520of%25202D%2520multi-view%2520images.%2520Our%2520approach%2520first%2520generates%2520a%2520point%2520cloud%2520from%2520the%2520captured%2520multi-view%2520images%2520using%2520recent%25203D%2520reconstruction%2520techniques%252C%2520while%2520a%2520parallel%25202D%2520encoder%2520aggregates%2520view-aligned%2520features.%2520A%2520fusion%2520module%2520then%2520aligns%2520and%2520merges%25203D%2520geometry%2520with%25202D%2520visual%2520embeddings%252C%2520followed%2520by%2520a%2520graph-based%2520decoder%2520that%2520regresses%2520volume%252C%2520surface%2520area%252C%2520and%2520their%2520corresponding%2520uncertainties.%2520This%2520proposed%2520architecture%2520maintains%2520robustness%2520against%2520sparse%2520or%2520noisy%2520data.%2520We%2520evaluate%2520the%2520framework%2520across%2520multiple%2520application%2520domains%253A%2520corals%252C%2520where%2520precise%2520geometric%2520measurements%2520support%2520growth%2520monitoring%253B%2520food%2520items%252C%2520where%2520volume%2520prediction%2520relates%2520to%2520dietary%2520tracking%2520and%2520portion%2520analysis%253B%2520and%2520human%2520bodies%252C%2520where%2520volumetric%2520cues%2520are%2520crucial%2520for%2520anthropometric%2520and%2520medical%2520applications.%2520Experimental%2520results%2520demonstrate%2520the%2520reliable%2520performance%2520of%2520our%2520framework%2520across%2520diverse%2520scenarios%252C%2520highlighting%2520its%2520versatility%2520and%2520adaptability.%2520Furthermore%252C%2520by%2520coupling%25203D%2520reconstruction%2520with%2520neural%2520regression%2520and%25202D%2520features%252C%2520our%2520model%2520provides%2520a%2520scalable%2520and%2520fast%2520solution%2520for%2520quantitative%2520shape%2520analysis%2520from%2520visual%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11164v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Neural%20Framework%20for%20Robust%203D%20Volume%20and%20Surface%20Estimation%20from%20Multi-View%20Images&entry.906535625=Diego%20Eustachio%20Farchione%20and%20Ramzi%20Idoughi%20and%20Peter%20Wonka&entry.1292438233=Accurate%20estimation%20of%20object%20volume%20and%20surface%20area%20from%20visual%20data%20is%20an%20open%20challenge%20with%20broad%20implications%20across%20various%20domains.%20We%20propose%20a%20unified%20framework%20that%20predicts%20volumetric%20and%20surface%20metrics%20directly%20from%20a%20set%20of%202D%20multi-view%20images.%20Our%20approach%20first%20generates%20a%20point%20cloud%20from%20the%20captured%20multi-view%20images%20using%20recent%203D%20reconstruction%20techniques%2C%20while%20a%20parallel%202D%20encoder%20aggregates%20view-aligned%20features.%20A%20fusion%20module%20then%20aligns%20and%20merges%203D%20geometry%20with%202D%20visual%20embeddings%2C%20followed%20by%20a%20graph-based%20decoder%20that%20regresses%20volume%2C%20surface%20area%2C%20and%20their%20corresponding%20uncertainties.%20This%20proposed%20architecture%20maintains%20robustness%20against%20sparse%20or%20noisy%20data.%20We%20evaluate%20the%20framework%20across%20multiple%20application%20domains%3A%20corals%2C%20where%20precise%20geometric%20measurements%20support%20growth%20monitoring%3B%20food%20items%2C%20where%20volume%20prediction%20relates%20to%20dietary%20tracking%20and%20portion%20analysis%3B%20and%20human%20bodies%2C%20where%20volumetric%20cues%20are%20crucial%20for%20anthropometric%20and%20medical%20applications.%20Experimental%20results%20demonstrate%20the%20reliable%20performance%20of%20our%20framework%20across%20diverse%20scenarios%2C%20highlighting%20its%20versatility%20and%20adaptability.%20Furthermore%2C%20by%20coupling%203D%20reconstruction%20with%20neural%20regression%20and%202D%20features%2C%20our%20model%20provides%20a%20scalable%20and%20fast%20solution%20for%20quantitative%20shape%20analysis%20from%20visual%20data.&entry.1838667208=http%3A//arxiv.org/abs/2509.11164v2&entry.124074799=Read"},
{"title": "LLMs Enable Bag-of-Texts Representations for Short-Text Clustering", "author": "I-Fan Lin and Faegheh Hasibi and Suzan Verberne", "abstract": "In this paper, we propose a training-free method for unsupervised short text clustering that relies less on careful selection of embedders than other methods. In customer-facing chatbots, companies are dealing with large amounts of user utterances that need to be clustered according to their intent. In these settings, no labeled data is typically available, and the number of clusters is not known. Recent approaches to short-text clustering in label-free settings incorporate LLM output to refine existing embeddings. While LLMs can identify similar texts effectively, the resulting similarities may not be directly represented by distances in the dense vector space, as they depend on the original embedding. We therefore propose a method for transforming LLM judgments directly into a bag-of-texts representation in which texts are initialized to be equidistant, without assuming any prior distance relationships. Our method achieves comparable or superior results to state-of-the-art methods, but without embeddings optimization or assuming prior knowledge of clusters or labels. Experiments on diverse datasets and smaller LLMs show that our method is model agnostic and can be applied to any embedder, with relatively small LLMs, and different clustering methods. We also show how our method scales to large datasets, reducing the computational cost of the LLM use. The flexibility and scalability of our method make it more aligned with real-world training-free scenarios than existing clustering methods.", "link": "http://arxiv.org/abs/2510.06747v2", "date": "2026-01-12", "relevancy": 2.394, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4873}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4784}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Enable%20Bag-of-Texts%20Representations%20for%20Short-Text%20Clustering&body=Title%3A%20LLMs%20Enable%20Bag-of-Texts%20Representations%20for%20Short-Text%20Clustering%0AAuthor%3A%20I-Fan%20Lin%20and%20Faegheh%20Hasibi%20and%20Suzan%20Verberne%0AAbstract%3A%20In%20this%20paper%2C%20we%20propose%20a%20training-free%20method%20for%20unsupervised%20short%20text%20clustering%20that%20relies%20less%20on%20careful%20selection%20of%20embedders%20than%20other%20methods.%20In%20customer-facing%20chatbots%2C%20companies%20are%20dealing%20with%20large%20amounts%20of%20user%20utterances%20that%20need%20to%20be%20clustered%20according%20to%20their%20intent.%20In%20these%20settings%2C%20no%20labeled%20data%20is%20typically%20available%2C%20and%20the%20number%20of%20clusters%20is%20not%20known.%20Recent%20approaches%20to%20short-text%20clustering%20in%20label-free%20settings%20incorporate%20LLM%20output%20to%20refine%20existing%20embeddings.%20While%20LLMs%20can%20identify%20similar%20texts%20effectively%2C%20the%20resulting%20similarities%20may%20not%20be%20directly%20represented%20by%20distances%20in%20the%20dense%20vector%20space%2C%20as%20they%20depend%20on%20the%20original%20embedding.%20We%20therefore%20propose%20a%20method%20for%20transforming%20LLM%20judgments%20directly%20into%20a%20bag-of-texts%20representation%20in%20which%20texts%20are%20initialized%20to%20be%20equidistant%2C%20without%20assuming%20any%20prior%20distance%20relationships.%20Our%20method%20achieves%20comparable%20or%20superior%20results%20to%20state-of-the-art%20methods%2C%20but%20without%20embeddings%20optimization%20or%20assuming%20prior%20knowledge%20of%20clusters%20or%20labels.%20Experiments%20on%20diverse%20datasets%20and%20smaller%20LLMs%20show%20that%20our%20method%20is%20model%20agnostic%20and%20can%20be%20applied%20to%20any%20embedder%2C%20with%20relatively%20small%20LLMs%2C%20and%20different%20clustering%20methods.%20We%20also%20show%20how%20our%20method%20scales%20to%20large%20datasets%2C%20reducing%20the%20computational%20cost%20of%20the%20LLM%20use.%20The%20flexibility%20and%20scalability%20of%20our%20method%20make%20it%20more%20aligned%20with%20real-world%20training-free%20scenarios%20than%20existing%20clustering%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2510.06747v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Enable%2520Bag-of-Texts%2520Representations%2520for%2520Short-Text%2520Clustering%26entry.906535625%3DI-Fan%2520Lin%2520and%2520Faegheh%2520Hasibi%2520and%2520Suzan%2520Verberne%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520training-free%2520method%2520for%2520unsupervised%2520short%2520text%2520clustering%2520that%2520relies%2520less%2520on%2520careful%2520selection%2520of%2520embedders%2520than%2520other%2520methods.%2520In%2520customer-facing%2520chatbots%252C%2520companies%2520are%2520dealing%2520with%2520large%2520amounts%2520of%2520user%2520utterances%2520that%2520need%2520to%2520be%2520clustered%2520according%2520to%2520their%2520intent.%2520In%2520these%2520settings%252C%2520no%2520labeled%2520data%2520is%2520typically%2520available%252C%2520and%2520the%2520number%2520of%2520clusters%2520is%2520not%2520known.%2520Recent%2520approaches%2520to%2520short-text%2520clustering%2520in%2520label-free%2520settings%2520incorporate%2520LLM%2520output%2520to%2520refine%2520existing%2520embeddings.%2520While%2520LLMs%2520can%2520identify%2520similar%2520texts%2520effectively%252C%2520the%2520resulting%2520similarities%2520may%2520not%2520be%2520directly%2520represented%2520by%2520distances%2520in%2520the%2520dense%2520vector%2520space%252C%2520as%2520they%2520depend%2520on%2520the%2520original%2520embedding.%2520We%2520therefore%2520propose%2520a%2520method%2520for%2520transforming%2520LLM%2520judgments%2520directly%2520into%2520a%2520bag-of-texts%2520representation%2520in%2520which%2520texts%2520are%2520initialized%2520to%2520be%2520equidistant%252C%2520without%2520assuming%2520any%2520prior%2520distance%2520relationships.%2520Our%2520method%2520achieves%2520comparable%2520or%2520superior%2520results%2520to%2520state-of-the-art%2520methods%252C%2520but%2520without%2520embeddings%2520optimization%2520or%2520assuming%2520prior%2520knowledge%2520of%2520clusters%2520or%2520labels.%2520Experiments%2520on%2520diverse%2520datasets%2520and%2520smaller%2520LLMs%2520show%2520that%2520our%2520method%2520is%2520model%2520agnostic%2520and%2520can%2520be%2520applied%2520to%2520any%2520embedder%252C%2520with%2520relatively%2520small%2520LLMs%252C%2520and%2520different%2520clustering%2520methods.%2520We%2520also%2520show%2520how%2520our%2520method%2520scales%2520to%2520large%2520datasets%252C%2520reducing%2520the%2520computational%2520cost%2520of%2520the%2520LLM%2520use.%2520The%2520flexibility%2520and%2520scalability%2520of%2520our%2520method%2520make%2520it%2520more%2520aligned%2520with%2520real-world%2520training-free%2520scenarios%2520than%2520existing%2520clustering%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06747v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Enable%20Bag-of-Texts%20Representations%20for%20Short-Text%20Clustering&entry.906535625=I-Fan%20Lin%20and%20Faegheh%20Hasibi%20and%20Suzan%20Verberne&entry.1292438233=In%20this%20paper%2C%20we%20propose%20a%20training-free%20method%20for%20unsupervised%20short%20text%20clustering%20that%20relies%20less%20on%20careful%20selection%20of%20embedders%20than%20other%20methods.%20In%20customer-facing%20chatbots%2C%20companies%20are%20dealing%20with%20large%20amounts%20of%20user%20utterances%20that%20need%20to%20be%20clustered%20according%20to%20their%20intent.%20In%20these%20settings%2C%20no%20labeled%20data%20is%20typically%20available%2C%20and%20the%20number%20of%20clusters%20is%20not%20known.%20Recent%20approaches%20to%20short-text%20clustering%20in%20label-free%20settings%20incorporate%20LLM%20output%20to%20refine%20existing%20embeddings.%20While%20LLMs%20can%20identify%20similar%20texts%20effectively%2C%20the%20resulting%20similarities%20may%20not%20be%20directly%20represented%20by%20distances%20in%20the%20dense%20vector%20space%2C%20as%20they%20depend%20on%20the%20original%20embedding.%20We%20therefore%20propose%20a%20method%20for%20transforming%20LLM%20judgments%20directly%20into%20a%20bag-of-texts%20representation%20in%20which%20texts%20are%20initialized%20to%20be%20equidistant%2C%20without%20assuming%20any%20prior%20distance%20relationships.%20Our%20method%20achieves%20comparable%20or%20superior%20results%20to%20state-of-the-art%20methods%2C%20but%20without%20embeddings%20optimization%20or%20assuming%20prior%20knowledge%20of%20clusters%20or%20labels.%20Experiments%20on%20diverse%20datasets%20and%20smaller%20LLMs%20show%20that%20our%20method%20is%20model%20agnostic%20and%20can%20be%20applied%20to%20any%20embedder%2C%20with%20relatively%20small%20LLMs%2C%20and%20different%20clustering%20methods.%20We%20also%20show%20how%20our%20method%20scales%20to%20large%20datasets%2C%20reducing%20the%20computational%20cost%20of%20the%20LLM%20use.%20The%20flexibility%20and%20scalability%20of%20our%20method%20make%20it%20more%20aligned%20with%20real-world%20training-free%20scenarios%20than%20existing%20clustering%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2510.06747v2&entry.124074799=Read"},
{"title": "Deep Whole-body Parkour", "author": "Ziwen Zhuang and Shaoting Zhu and Mengjie Zhao and Hang Zhao", "abstract": "Current approaches to humanoid control generally fall into two paradigms: perceptive locomotion, which handles terrain well but is limited to pedal gaits, and general motion tracking, which reproduces complex skills but ignores environmental capabilities. This work unites these paradigms to achieve perceptive general motion control. We present a framework where exteroceptive sensing is integrated into whole-body motion tracking, permitting a humanoid to perform highly dynamic, non-locomotion tasks on uneven terrain. By training a single policy to perform multiple distinct motions across varied terrestrial features, we demonstrate the non-trivial benefit of integrating perception into the control loop. Our results show that this framework enables robust, highly dynamic multi-contact motions, such as vaulting and dive-rolling, on unstructured terrain, significantly expanding the robot's traversability beyond simple walking or running. https://project-instinct.github.io/deep-whole-body-parkour", "link": "http://arxiv.org/abs/2601.07701v1", "date": "2026-01-12", "relevancy": 2.3865, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6058}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5935}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Whole-body%20Parkour&body=Title%3A%20Deep%20Whole-body%20Parkour%0AAuthor%3A%20Ziwen%20Zhuang%20and%20Shaoting%20Zhu%20and%20Mengjie%20Zhao%20and%20Hang%20Zhao%0AAbstract%3A%20Current%20approaches%20to%20humanoid%20control%20generally%20fall%20into%20two%20paradigms%3A%20perceptive%20locomotion%2C%20which%20handles%20terrain%20well%20but%20is%20limited%20to%20pedal%20gaits%2C%20and%20general%20motion%20tracking%2C%20which%20reproduces%20complex%20skills%20but%20ignores%20environmental%20capabilities.%20This%20work%20unites%20these%20paradigms%20to%20achieve%20perceptive%20general%20motion%20control.%20We%20present%20a%20framework%20where%20exteroceptive%20sensing%20is%20integrated%20into%20whole-body%20motion%20tracking%2C%20permitting%20a%20humanoid%20to%20perform%20highly%20dynamic%2C%20non-locomotion%20tasks%20on%20uneven%20terrain.%20By%20training%20a%20single%20policy%20to%20perform%20multiple%20distinct%20motions%20across%20varied%20terrestrial%20features%2C%20we%20demonstrate%20the%20non-trivial%20benefit%20of%20integrating%20perception%20into%20the%20control%20loop.%20Our%20results%20show%20that%20this%20framework%20enables%20robust%2C%20highly%20dynamic%20multi-contact%20motions%2C%20such%20as%20vaulting%20and%20dive-rolling%2C%20on%20unstructured%20terrain%2C%20significantly%20expanding%20the%20robot%27s%20traversability%20beyond%20simple%20walking%20or%20running.%20https%3A//project-instinct.github.io/deep-whole-body-parkour%0ALink%3A%20http%3A//arxiv.org/abs/2601.07701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Whole-body%2520Parkour%26entry.906535625%3DZiwen%2520Zhuang%2520and%2520Shaoting%2520Zhu%2520and%2520Mengjie%2520Zhao%2520and%2520Hang%2520Zhao%26entry.1292438233%3DCurrent%2520approaches%2520to%2520humanoid%2520control%2520generally%2520fall%2520into%2520two%2520paradigms%253A%2520perceptive%2520locomotion%252C%2520which%2520handles%2520terrain%2520well%2520but%2520is%2520limited%2520to%2520pedal%2520gaits%252C%2520and%2520general%2520motion%2520tracking%252C%2520which%2520reproduces%2520complex%2520skills%2520but%2520ignores%2520environmental%2520capabilities.%2520This%2520work%2520unites%2520these%2520paradigms%2520to%2520achieve%2520perceptive%2520general%2520motion%2520control.%2520We%2520present%2520a%2520framework%2520where%2520exteroceptive%2520sensing%2520is%2520integrated%2520into%2520whole-body%2520motion%2520tracking%252C%2520permitting%2520a%2520humanoid%2520to%2520perform%2520highly%2520dynamic%252C%2520non-locomotion%2520tasks%2520on%2520uneven%2520terrain.%2520By%2520training%2520a%2520single%2520policy%2520to%2520perform%2520multiple%2520distinct%2520motions%2520across%2520varied%2520terrestrial%2520features%252C%2520we%2520demonstrate%2520the%2520non-trivial%2520benefit%2520of%2520integrating%2520perception%2520into%2520the%2520control%2520loop.%2520Our%2520results%2520show%2520that%2520this%2520framework%2520enables%2520robust%252C%2520highly%2520dynamic%2520multi-contact%2520motions%252C%2520such%2520as%2520vaulting%2520and%2520dive-rolling%252C%2520on%2520unstructured%2520terrain%252C%2520significantly%2520expanding%2520the%2520robot%2527s%2520traversability%2520beyond%2520simple%2520walking%2520or%2520running.%2520https%253A//project-instinct.github.io/deep-whole-body-parkour%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Whole-body%20Parkour&entry.906535625=Ziwen%20Zhuang%20and%20Shaoting%20Zhu%20and%20Mengjie%20Zhao%20and%20Hang%20Zhao&entry.1292438233=Current%20approaches%20to%20humanoid%20control%20generally%20fall%20into%20two%20paradigms%3A%20perceptive%20locomotion%2C%20which%20handles%20terrain%20well%20but%20is%20limited%20to%20pedal%20gaits%2C%20and%20general%20motion%20tracking%2C%20which%20reproduces%20complex%20skills%20but%20ignores%20environmental%20capabilities.%20This%20work%20unites%20these%20paradigms%20to%20achieve%20perceptive%20general%20motion%20control.%20We%20present%20a%20framework%20where%20exteroceptive%20sensing%20is%20integrated%20into%20whole-body%20motion%20tracking%2C%20permitting%20a%20humanoid%20to%20perform%20highly%20dynamic%2C%20non-locomotion%20tasks%20on%20uneven%20terrain.%20By%20training%20a%20single%20policy%20to%20perform%20multiple%20distinct%20motions%20across%20varied%20terrestrial%20features%2C%20we%20demonstrate%20the%20non-trivial%20benefit%20of%20integrating%20perception%20into%20the%20control%20loop.%20Our%20results%20show%20that%20this%20framework%20enables%20robust%2C%20highly%20dynamic%20multi-contact%20motions%2C%20such%20as%20vaulting%20and%20dive-rolling%2C%20on%20unstructured%20terrain%2C%20significantly%20expanding%20the%20robot%27s%20traversability%20beyond%20simple%20walking%20or%20running.%20https%3A//project-instinct.github.io/deep-whole-body-parkour&entry.1838667208=http%3A//arxiv.org/abs/2601.07701v1&entry.124074799=Read"},
{"title": "Leveraging 3D Representation Alignment and RGB Pretrained Priors for LiDAR Scene Generation", "author": "Nicolas Sereyjol-Garros and Ellington Kirby and Victor Besnier and Nermin Samet", "abstract": "LiDAR scene synthesis is an emerging solution to scarcity in 3D data for robotic tasks such as autonomous driving. Recent approaches employ diffusion or flow matching models to generate realistic scenes, but 3D data remains limited compared to RGB datasets with millions of samples. We introduce R3DPA, the first LiDAR scene generation method to unlock image-pretrained priors for LiDAR point clouds, and leverage self-supervised 3D representations for state-of-the-art results. Specifically, we (i) align intermediate features of our generative model with self-supervised 3D features, which substantially improves generation quality; (ii) transfer knowledge from large-scale image-pretrained generative models to LiDAR generation, mitigating limited LiDAR datasets; and (iii) enable point cloud control at inference for object inpainting and scene mixing with solely an unconditional model. On the KITTI-360 benchmark R3DPA achieves state of the art performance. Code and pretrained models are available at https://github.com/valeoai/R3DPA.", "link": "http://arxiv.org/abs/2601.07692v1", "date": "2026-01-12", "relevancy": 2.3717, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5972}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.596}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%203D%20Representation%20Alignment%20and%20RGB%20Pretrained%20Priors%20for%20LiDAR%20Scene%20Generation&body=Title%3A%20Leveraging%203D%20Representation%20Alignment%20and%20RGB%20Pretrained%20Priors%20for%20LiDAR%20Scene%20Generation%0AAuthor%3A%20Nicolas%20Sereyjol-Garros%20and%20Ellington%20Kirby%20and%20Victor%20Besnier%20and%20Nermin%20Samet%0AAbstract%3A%20LiDAR%20scene%20synthesis%20is%20an%20emerging%20solution%20to%20scarcity%20in%203D%20data%20for%20robotic%20tasks%20such%20as%20autonomous%20driving.%20Recent%20approaches%20employ%20diffusion%20or%20flow%20matching%20models%20to%20generate%20realistic%20scenes%2C%20but%203D%20data%20remains%20limited%20compared%20to%20RGB%20datasets%20with%20millions%20of%20samples.%20We%20introduce%20R3DPA%2C%20the%20first%20LiDAR%20scene%20generation%20method%20to%20unlock%20image-pretrained%20priors%20for%20LiDAR%20point%20clouds%2C%20and%20leverage%20self-supervised%203D%20representations%20for%20state-of-the-art%20results.%20Specifically%2C%20we%20%28i%29%20align%20intermediate%20features%20of%20our%20generative%20model%20with%20self-supervised%203D%20features%2C%20which%20substantially%20improves%20generation%20quality%3B%20%28ii%29%20transfer%20knowledge%20from%20large-scale%20image-pretrained%20generative%20models%20to%20LiDAR%20generation%2C%20mitigating%20limited%20LiDAR%20datasets%3B%20and%20%28iii%29%20enable%20point%20cloud%20control%20at%20inference%20for%20object%20inpainting%20and%20scene%20mixing%20with%20solely%20an%20unconditional%20model.%20On%20the%20KITTI-360%20benchmark%20R3DPA%20achieves%20state%20of%20the%20art%20performance.%20Code%20and%20pretrained%20models%20are%20available%20at%20https%3A//github.com/valeoai/R3DPA.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%25203D%2520Representation%2520Alignment%2520and%2520RGB%2520Pretrained%2520Priors%2520for%2520LiDAR%2520Scene%2520Generation%26entry.906535625%3DNicolas%2520Sereyjol-Garros%2520and%2520Ellington%2520Kirby%2520and%2520Victor%2520Besnier%2520and%2520Nermin%2520Samet%26entry.1292438233%3DLiDAR%2520scene%2520synthesis%2520is%2520an%2520emerging%2520solution%2520to%2520scarcity%2520in%25203D%2520data%2520for%2520robotic%2520tasks%2520such%2520as%2520autonomous%2520driving.%2520Recent%2520approaches%2520employ%2520diffusion%2520or%2520flow%2520matching%2520models%2520to%2520generate%2520realistic%2520scenes%252C%2520but%25203D%2520data%2520remains%2520limited%2520compared%2520to%2520RGB%2520datasets%2520with%2520millions%2520of%2520samples.%2520We%2520introduce%2520R3DPA%252C%2520the%2520first%2520LiDAR%2520scene%2520generation%2520method%2520to%2520unlock%2520image-pretrained%2520priors%2520for%2520LiDAR%2520point%2520clouds%252C%2520and%2520leverage%2520self-supervised%25203D%2520representations%2520for%2520state-of-the-art%2520results.%2520Specifically%252C%2520we%2520%2528i%2529%2520align%2520intermediate%2520features%2520of%2520our%2520generative%2520model%2520with%2520self-supervised%25203D%2520features%252C%2520which%2520substantially%2520improves%2520generation%2520quality%253B%2520%2528ii%2529%2520transfer%2520knowledge%2520from%2520large-scale%2520image-pretrained%2520generative%2520models%2520to%2520LiDAR%2520generation%252C%2520mitigating%2520limited%2520LiDAR%2520datasets%253B%2520and%2520%2528iii%2529%2520enable%2520point%2520cloud%2520control%2520at%2520inference%2520for%2520object%2520inpainting%2520and%2520scene%2520mixing%2520with%2520solely%2520an%2520unconditional%2520model.%2520On%2520the%2520KITTI-360%2520benchmark%2520R3DPA%2520achieves%2520state%2520of%2520the%2520art%2520performance.%2520Code%2520and%2520pretrained%2520models%2520are%2520available%2520at%2520https%253A//github.com/valeoai/R3DPA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%203D%20Representation%20Alignment%20and%20RGB%20Pretrained%20Priors%20for%20LiDAR%20Scene%20Generation&entry.906535625=Nicolas%20Sereyjol-Garros%20and%20Ellington%20Kirby%20and%20Victor%20Besnier%20and%20Nermin%20Samet&entry.1292438233=LiDAR%20scene%20synthesis%20is%20an%20emerging%20solution%20to%20scarcity%20in%203D%20data%20for%20robotic%20tasks%20such%20as%20autonomous%20driving.%20Recent%20approaches%20employ%20diffusion%20or%20flow%20matching%20models%20to%20generate%20realistic%20scenes%2C%20but%203D%20data%20remains%20limited%20compared%20to%20RGB%20datasets%20with%20millions%20of%20samples.%20We%20introduce%20R3DPA%2C%20the%20first%20LiDAR%20scene%20generation%20method%20to%20unlock%20image-pretrained%20priors%20for%20LiDAR%20point%20clouds%2C%20and%20leverage%20self-supervised%203D%20representations%20for%20state-of-the-art%20results.%20Specifically%2C%20we%20%28i%29%20align%20intermediate%20features%20of%20our%20generative%20model%20with%20self-supervised%203D%20features%2C%20which%20substantially%20improves%20generation%20quality%3B%20%28ii%29%20transfer%20knowledge%20from%20large-scale%20image-pretrained%20generative%20models%20to%20LiDAR%20generation%2C%20mitigating%20limited%20LiDAR%20datasets%3B%20and%20%28iii%29%20enable%20point%20cloud%20control%20at%20inference%20for%20object%20inpainting%20and%20scene%20mixing%20with%20solely%20an%20unconditional%20model.%20On%20the%20KITTI-360%20benchmark%20R3DPA%20achieves%20state%20of%20the%20art%20performance.%20Code%20and%20pretrained%20models%20are%20available%20at%20https%3A//github.com/valeoai/R3DPA.&entry.1838667208=http%3A//arxiv.org/abs/2601.07692v1&entry.124074799=Read"},
{"title": "RLPO: Residual Listwise Preference Optimization for Long-Context Review Ranking", "author": "Hao Jiang and Zhi Yang and Annan Wang and Yichi Zhang and Weisi Lin", "abstract": "Review ranking is pivotal in e-commerce for prioritizing diagnostic and authentic feedback from the deluge of user-generated content. While large language models have improved semantic assessment, existing ranking paradigms face a persistent trade-off in long-context settings. Pointwise scoring is efficient but often fails to account for list-level interactions, leading to miscalibrated top-$k$ rankings. Listwise approaches can leverage global context, yet they are computationally expensive and become unstable as candidate lists grow. To address this, we propose Residual Listwise Preference Optimization (RLPO), which formulates ranking as listwise representation-level residual correction over a strong pointwise LLM scorer. RLPO first produces calibrated pointwise scores and item representations, then applies a lightweight encoder over the representations to predict listwise score residuals, avoiding full token-level listwise processing. We also introduce a large-scale benchmark for long-context review ranking with human verification. Experiments show RLPO improves NDCG@k over strong pointwise and listwise baselines and remains robust as list length increases.", "link": "http://arxiv.org/abs/2601.07449v1", "date": "2026-01-12", "relevancy": 2.3538, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RLPO%3A%20Residual%20Listwise%20Preference%20Optimization%20for%20Long-Context%20Review%20Ranking&body=Title%3A%20RLPO%3A%20Residual%20Listwise%20Preference%20Optimization%20for%20Long-Context%20Review%20Ranking%0AAuthor%3A%20Hao%20Jiang%20and%20Zhi%20Yang%20and%20Annan%20Wang%20and%20Yichi%20Zhang%20and%20Weisi%20Lin%0AAbstract%3A%20Review%20ranking%20is%20pivotal%20in%20e-commerce%20for%20prioritizing%20diagnostic%20and%20authentic%20feedback%20from%20the%20deluge%20of%20user-generated%20content.%20While%20large%20language%20models%20have%20improved%20semantic%20assessment%2C%20existing%20ranking%20paradigms%20face%20a%20persistent%20trade-off%20in%20long-context%20settings.%20Pointwise%20scoring%20is%20efficient%20but%20often%20fails%20to%20account%20for%20list-level%20interactions%2C%20leading%20to%20miscalibrated%20top-%24k%24%20rankings.%20Listwise%20approaches%20can%20leverage%20global%20context%2C%20yet%20they%20are%20computationally%20expensive%20and%20become%20unstable%20as%20candidate%20lists%20grow.%20To%20address%20this%2C%20we%20propose%20Residual%20Listwise%20Preference%20Optimization%20%28RLPO%29%2C%20which%20formulates%20ranking%20as%20listwise%20representation-level%20residual%20correction%20over%20a%20strong%20pointwise%20LLM%20scorer.%20RLPO%20first%20produces%20calibrated%20pointwise%20scores%20and%20item%20representations%2C%20then%20applies%20a%20lightweight%20encoder%20over%20the%20representations%20to%20predict%20listwise%20score%20residuals%2C%20avoiding%20full%20token-level%20listwise%20processing.%20We%20also%20introduce%20a%20large-scale%20benchmark%20for%20long-context%20review%20ranking%20with%20human%20verification.%20Experiments%20show%20RLPO%20improves%20NDCG%40k%20over%20strong%20pointwise%20and%20listwise%20baselines%20and%20remains%20robust%20as%20list%20length%20increases.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRLPO%253A%2520Residual%2520Listwise%2520Preference%2520Optimization%2520for%2520Long-Context%2520Review%2520Ranking%26entry.906535625%3DHao%2520Jiang%2520and%2520Zhi%2520Yang%2520and%2520Annan%2520Wang%2520and%2520Yichi%2520Zhang%2520and%2520Weisi%2520Lin%26entry.1292438233%3DReview%2520ranking%2520is%2520pivotal%2520in%2520e-commerce%2520for%2520prioritizing%2520diagnostic%2520and%2520authentic%2520feedback%2520from%2520the%2520deluge%2520of%2520user-generated%2520content.%2520While%2520large%2520language%2520models%2520have%2520improved%2520semantic%2520assessment%252C%2520existing%2520ranking%2520paradigms%2520face%2520a%2520persistent%2520trade-off%2520in%2520long-context%2520settings.%2520Pointwise%2520scoring%2520is%2520efficient%2520but%2520often%2520fails%2520to%2520account%2520for%2520list-level%2520interactions%252C%2520leading%2520to%2520miscalibrated%2520top-%2524k%2524%2520rankings.%2520Listwise%2520approaches%2520can%2520leverage%2520global%2520context%252C%2520yet%2520they%2520are%2520computationally%2520expensive%2520and%2520become%2520unstable%2520as%2520candidate%2520lists%2520grow.%2520To%2520address%2520this%252C%2520we%2520propose%2520Residual%2520Listwise%2520Preference%2520Optimization%2520%2528RLPO%2529%252C%2520which%2520formulates%2520ranking%2520as%2520listwise%2520representation-level%2520residual%2520correction%2520over%2520a%2520strong%2520pointwise%2520LLM%2520scorer.%2520RLPO%2520first%2520produces%2520calibrated%2520pointwise%2520scores%2520and%2520item%2520representations%252C%2520then%2520applies%2520a%2520lightweight%2520encoder%2520over%2520the%2520representations%2520to%2520predict%2520listwise%2520score%2520residuals%252C%2520avoiding%2520full%2520token-level%2520listwise%2520processing.%2520We%2520also%2520introduce%2520a%2520large-scale%2520benchmark%2520for%2520long-context%2520review%2520ranking%2520with%2520human%2520verification.%2520Experiments%2520show%2520RLPO%2520improves%2520NDCG%2540k%2520over%2520strong%2520pointwise%2520and%2520listwise%2520baselines%2520and%2520remains%2520robust%2520as%2520list%2520length%2520increases.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RLPO%3A%20Residual%20Listwise%20Preference%20Optimization%20for%20Long-Context%20Review%20Ranking&entry.906535625=Hao%20Jiang%20and%20Zhi%20Yang%20and%20Annan%20Wang%20and%20Yichi%20Zhang%20and%20Weisi%20Lin&entry.1292438233=Review%20ranking%20is%20pivotal%20in%20e-commerce%20for%20prioritizing%20diagnostic%20and%20authentic%20feedback%20from%20the%20deluge%20of%20user-generated%20content.%20While%20large%20language%20models%20have%20improved%20semantic%20assessment%2C%20existing%20ranking%20paradigms%20face%20a%20persistent%20trade-off%20in%20long-context%20settings.%20Pointwise%20scoring%20is%20efficient%20but%20often%20fails%20to%20account%20for%20list-level%20interactions%2C%20leading%20to%20miscalibrated%20top-%24k%24%20rankings.%20Listwise%20approaches%20can%20leverage%20global%20context%2C%20yet%20they%20are%20computationally%20expensive%20and%20become%20unstable%20as%20candidate%20lists%20grow.%20To%20address%20this%2C%20we%20propose%20Residual%20Listwise%20Preference%20Optimization%20%28RLPO%29%2C%20which%20formulates%20ranking%20as%20listwise%20representation-level%20residual%20correction%20over%20a%20strong%20pointwise%20LLM%20scorer.%20RLPO%20first%20produces%20calibrated%20pointwise%20scores%20and%20item%20representations%2C%20then%20applies%20a%20lightweight%20encoder%20over%20the%20representations%20to%20predict%20listwise%20score%20residuals%2C%20avoiding%20full%20token-level%20listwise%20processing.%20We%20also%20introduce%20a%20large-scale%20benchmark%20for%20long-context%20review%20ranking%20with%20human%20verification.%20Experiments%20show%20RLPO%20improves%20NDCG%40k%20over%20strong%20pointwise%20and%20listwise%20baselines%20and%20remains%20robust%20as%20list%20length%20increases.&entry.1838667208=http%3A//arxiv.org/abs/2601.07449v1&entry.124074799=Read"},
{"title": "Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation", "author": "Jiao Xu and Xin Chen and Lihe Zhang", "abstract": "In this paper, we present a new dynamic collaborative network for semi-supervised 3D vessel segmentation, termed DiCo. Conventional mean teacher (MT) methods typically employ a static approach, where the roles of the teacher and student models are fixed. However, due to the complexity of 3D vessel data, the teacher model may not always outperform the student model, leading to cognitive biases that can limit performance. To address this issue, we propose a dynamic collaborative network that allows the two models to dynamically switch their teacher-student roles. Additionally, we introduce a multi-view integration module to capture various perspectives of the inputs, mirroring the way doctors conduct medical analysis. We also incorporate adversarial supervision to constrain the shape of the segmented vessels in unlabeled data. In this process, the 3D volume is projected into 2D views to mitigate the impact of label inconsistencies. Experiments demonstrate that our DiCo method sets new state-of-the-art performance on three 3D vessel segmentation benchmarks. The code repository address is https://github.com/xujiaommcome/DiCo", "link": "http://arxiv.org/abs/2601.07377v1", "date": "2026-01-12", "relevancy": 2.3513, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6188}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5735}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Dynamic%20Collaborative%20Network%20for%20Semi-supervised%203D%20Vessel%20Segmentation&body=Title%3A%20Learning%20Dynamic%20Collaborative%20Network%20for%20Semi-supervised%203D%20Vessel%20Segmentation%0AAuthor%3A%20Jiao%20Xu%20and%20Xin%20Chen%20and%20Lihe%20Zhang%0AAbstract%3A%20In%20this%20paper%2C%20we%20present%20a%20new%20dynamic%20collaborative%20network%20for%20semi-supervised%203D%20vessel%20segmentation%2C%20termed%20DiCo.%20Conventional%20mean%20teacher%20%28MT%29%20methods%20typically%20employ%20a%20static%20approach%2C%20where%20the%20roles%20of%20the%20teacher%20and%20student%20models%20are%20fixed.%20However%2C%20due%20to%20the%20complexity%20of%203D%20vessel%20data%2C%20the%20teacher%20model%20may%20not%20always%20outperform%20the%20student%20model%2C%20leading%20to%20cognitive%20biases%20that%20can%20limit%20performance.%20To%20address%20this%20issue%2C%20we%20propose%20a%20dynamic%20collaborative%20network%20that%20allows%20the%20two%20models%20to%20dynamically%20switch%20their%20teacher-student%20roles.%20Additionally%2C%20we%20introduce%20a%20multi-view%20integration%20module%20to%20capture%20various%20perspectives%20of%20the%20inputs%2C%20mirroring%20the%20way%20doctors%20conduct%20medical%20analysis.%20We%20also%20incorporate%20adversarial%20supervision%20to%20constrain%20the%20shape%20of%20the%20segmented%20vessels%20in%20unlabeled%20data.%20In%20this%20process%2C%20the%203D%20volume%20is%20projected%20into%202D%20views%20to%20mitigate%20the%20impact%20of%20label%20inconsistencies.%20Experiments%20demonstrate%20that%20our%20DiCo%20method%20sets%20new%20state-of-the-art%20performance%20on%20three%203D%20vessel%20segmentation%20benchmarks.%20The%20code%20repository%20address%20is%20https%3A//github.com/xujiaommcome/DiCo%0ALink%3A%20http%3A//arxiv.org/abs/2601.07377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Dynamic%2520Collaborative%2520Network%2520for%2520Semi-supervised%25203D%2520Vessel%2520Segmentation%26entry.906535625%3DJiao%2520Xu%2520and%2520Xin%2520Chen%2520and%2520Lihe%2520Zhang%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520present%2520a%2520new%2520dynamic%2520collaborative%2520network%2520for%2520semi-supervised%25203D%2520vessel%2520segmentation%252C%2520termed%2520DiCo.%2520Conventional%2520mean%2520teacher%2520%2528MT%2529%2520methods%2520typically%2520employ%2520a%2520static%2520approach%252C%2520where%2520the%2520roles%2520of%2520the%2520teacher%2520and%2520student%2520models%2520are%2520fixed.%2520However%252C%2520due%2520to%2520the%2520complexity%2520of%25203D%2520vessel%2520data%252C%2520the%2520teacher%2520model%2520may%2520not%2520always%2520outperform%2520the%2520student%2520model%252C%2520leading%2520to%2520cognitive%2520biases%2520that%2520can%2520limit%2520performance.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520dynamic%2520collaborative%2520network%2520that%2520allows%2520the%2520two%2520models%2520to%2520dynamically%2520switch%2520their%2520teacher-student%2520roles.%2520Additionally%252C%2520we%2520introduce%2520a%2520multi-view%2520integration%2520module%2520to%2520capture%2520various%2520perspectives%2520of%2520the%2520inputs%252C%2520mirroring%2520the%2520way%2520doctors%2520conduct%2520medical%2520analysis.%2520We%2520also%2520incorporate%2520adversarial%2520supervision%2520to%2520constrain%2520the%2520shape%2520of%2520the%2520segmented%2520vessels%2520in%2520unlabeled%2520data.%2520In%2520this%2520process%252C%2520the%25203D%2520volume%2520is%2520projected%2520into%25202D%2520views%2520to%2520mitigate%2520the%2520impact%2520of%2520label%2520inconsistencies.%2520Experiments%2520demonstrate%2520that%2520our%2520DiCo%2520method%2520sets%2520new%2520state-of-the-art%2520performance%2520on%2520three%25203D%2520vessel%2520segmentation%2520benchmarks.%2520The%2520code%2520repository%2520address%2520is%2520https%253A//github.com/xujiaommcome/DiCo%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Dynamic%20Collaborative%20Network%20for%20Semi-supervised%203D%20Vessel%20Segmentation&entry.906535625=Jiao%20Xu%20and%20Xin%20Chen%20and%20Lihe%20Zhang&entry.1292438233=In%20this%20paper%2C%20we%20present%20a%20new%20dynamic%20collaborative%20network%20for%20semi-supervised%203D%20vessel%20segmentation%2C%20termed%20DiCo.%20Conventional%20mean%20teacher%20%28MT%29%20methods%20typically%20employ%20a%20static%20approach%2C%20where%20the%20roles%20of%20the%20teacher%20and%20student%20models%20are%20fixed.%20However%2C%20due%20to%20the%20complexity%20of%203D%20vessel%20data%2C%20the%20teacher%20model%20may%20not%20always%20outperform%20the%20student%20model%2C%20leading%20to%20cognitive%20biases%20that%20can%20limit%20performance.%20To%20address%20this%20issue%2C%20we%20propose%20a%20dynamic%20collaborative%20network%20that%20allows%20the%20two%20models%20to%20dynamically%20switch%20their%20teacher-student%20roles.%20Additionally%2C%20we%20introduce%20a%20multi-view%20integration%20module%20to%20capture%20various%20perspectives%20of%20the%20inputs%2C%20mirroring%20the%20way%20doctors%20conduct%20medical%20analysis.%20We%20also%20incorporate%20adversarial%20supervision%20to%20constrain%20the%20shape%20of%20the%20segmented%20vessels%20in%20unlabeled%20data.%20In%20this%20process%2C%20the%203D%20volume%20is%20projected%20into%202D%20views%20to%20mitigate%20the%20impact%20of%20label%20inconsistencies.%20Experiments%20demonstrate%20that%20our%20DiCo%20method%20sets%20new%20state-of-the-art%20performance%20on%20three%203D%20vessel%20segmentation%20benchmarks.%20The%20code%20repository%20address%20is%20https%3A//github.com/xujiaommcome/DiCo&entry.1838667208=http%3A//arxiv.org/abs/2601.07377v1&entry.124074799=Read"},
{"title": "Adaptive Layer Selection for Layer-Wise Token Pruning in LLM Inference", "author": "Rei Taniguchi and Yuyang Dong and Makoto Onizuka and Chuan Xiao", "abstract": "Due to the prevalence of large language models (LLMs), key-value (KV) cache reduction for LLM inference has received remarkable attention. Among numerous works that have been proposed in recent years, layer-wise token pruning approaches, which select a subset of tokens at particular layers to retain in KV cache and prune others, are one of the most popular schemes. They primarily adopt a set of pre-defined layers, at which tokens are selected. Such design is inflexible in the sense that the accuracy significantly varies across tasks and deteriorates in harder tasks such as KV retrieval. In this paper, we propose ASL, a training-free method that adaptively chooses the selection layer for KV cache reduction, exploiting the variance of token ranks ordered by attention score. The proposed method balances the performance across different tasks while meeting the user-specified KV budget requirement. ASL operates during the prefilling stage and can be jointly used with existing KV cache reduction methods such as SnapKV to optimize the decoding stage. By evaluations on the InfiniteBench, RULER, and NIAH benchmarks, we show that equipped with one-shot token selection, where tokens are selected at a layer and propagated to deeper layers, ASL outperforms state-of-the-art layer-wise token selection methods in accuracy while maintaining decoding speed and KV cache reduction.", "link": "http://arxiv.org/abs/2601.07667v1", "date": "2026-01-12", "relevancy": 2.3463, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5084}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4518}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Layer%20Selection%20for%20Layer-Wise%20Token%20Pruning%20in%20LLM%20Inference&body=Title%3A%20Adaptive%20Layer%20Selection%20for%20Layer-Wise%20Token%20Pruning%20in%20LLM%20Inference%0AAuthor%3A%20Rei%20Taniguchi%20and%20Yuyang%20Dong%20and%20Makoto%20Onizuka%20and%20Chuan%20Xiao%0AAbstract%3A%20Due%20to%20the%20prevalence%20of%20large%20language%20models%20%28LLMs%29%2C%20key-value%20%28KV%29%20cache%20reduction%20for%20LLM%20inference%20has%20received%20remarkable%20attention.%20Among%20numerous%20works%20that%20have%20been%20proposed%20in%20recent%20years%2C%20layer-wise%20token%20pruning%20approaches%2C%20which%20select%20a%20subset%20of%20tokens%20at%20particular%20layers%20to%20retain%20in%20KV%20cache%20and%20prune%20others%2C%20are%20one%20of%20the%20most%20popular%20schemes.%20They%20primarily%20adopt%20a%20set%20of%20pre-defined%20layers%2C%20at%20which%20tokens%20are%20selected.%20Such%20design%20is%20inflexible%20in%20the%20sense%20that%20the%20accuracy%20significantly%20varies%20across%20tasks%20and%20deteriorates%20in%20harder%20tasks%20such%20as%20KV%20retrieval.%20In%20this%20paper%2C%20we%20propose%20ASL%2C%20a%20training-free%20method%20that%20adaptively%20chooses%20the%20selection%20layer%20for%20KV%20cache%20reduction%2C%20exploiting%20the%20variance%20of%20token%20ranks%20ordered%20by%20attention%20score.%20The%20proposed%20method%20balances%20the%20performance%20across%20different%20tasks%20while%20meeting%20the%20user-specified%20KV%20budget%20requirement.%20ASL%20operates%20during%20the%20prefilling%20stage%20and%20can%20be%20jointly%20used%20with%20existing%20KV%20cache%20reduction%20methods%20such%20as%20SnapKV%20to%20optimize%20the%20decoding%20stage.%20By%20evaluations%20on%20the%20InfiniteBench%2C%20RULER%2C%20and%20NIAH%20benchmarks%2C%20we%20show%20that%20equipped%20with%20one-shot%20token%20selection%2C%20where%20tokens%20are%20selected%20at%20a%20layer%20and%20propagated%20to%20deeper%20layers%2C%20ASL%20outperforms%20state-of-the-art%20layer-wise%20token%20selection%20methods%20in%20accuracy%20while%20maintaining%20decoding%20speed%20and%20KV%20cache%20reduction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Layer%2520Selection%2520for%2520Layer-Wise%2520Token%2520Pruning%2520in%2520LLM%2520Inference%26entry.906535625%3DRei%2520Taniguchi%2520and%2520Yuyang%2520Dong%2520and%2520Makoto%2520Onizuka%2520and%2520Chuan%2520Xiao%26entry.1292438233%3DDue%2520to%2520the%2520prevalence%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520key-value%2520%2528KV%2529%2520cache%2520reduction%2520for%2520LLM%2520inference%2520has%2520received%2520remarkable%2520attention.%2520Among%2520numerous%2520works%2520that%2520have%2520been%2520proposed%2520in%2520recent%2520years%252C%2520layer-wise%2520token%2520pruning%2520approaches%252C%2520which%2520select%2520a%2520subset%2520of%2520tokens%2520at%2520particular%2520layers%2520to%2520retain%2520in%2520KV%2520cache%2520and%2520prune%2520others%252C%2520are%2520one%2520of%2520the%2520most%2520popular%2520schemes.%2520They%2520primarily%2520adopt%2520a%2520set%2520of%2520pre-defined%2520layers%252C%2520at%2520which%2520tokens%2520are%2520selected.%2520Such%2520design%2520is%2520inflexible%2520in%2520the%2520sense%2520that%2520the%2520accuracy%2520significantly%2520varies%2520across%2520tasks%2520and%2520deteriorates%2520in%2520harder%2520tasks%2520such%2520as%2520KV%2520retrieval.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ASL%252C%2520a%2520training-free%2520method%2520that%2520adaptively%2520chooses%2520the%2520selection%2520layer%2520for%2520KV%2520cache%2520reduction%252C%2520exploiting%2520the%2520variance%2520of%2520token%2520ranks%2520ordered%2520by%2520attention%2520score.%2520The%2520proposed%2520method%2520balances%2520the%2520performance%2520across%2520different%2520tasks%2520while%2520meeting%2520the%2520user-specified%2520KV%2520budget%2520requirement.%2520ASL%2520operates%2520during%2520the%2520prefilling%2520stage%2520and%2520can%2520be%2520jointly%2520used%2520with%2520existing%2520KV%2520cache%2520reduction%2520methods%2520such%2520as%2520SnapKV%2520to%2520optimize%2520the%2520decoding%2520stage.%2520By%2520evaluations%2520on%2520the%2520InfiniteBench%252C%2520RULER%252C%2520and%2520NIAH%2520benchmarks%252C%2520we%2520show%2520that%2520equipped%2520with%2520one-shot%2520token%2520selection%252C%2520where%2520tokens%2520are%2520selected%2520at%2520a%2520layer%2520and%2520propagated%2520to%2520deeper%2520layers%252C%2520ASL%2520outperforms%2520state-of-the-art%2520layer-wise%2520token%2520selection%2520methods%2520in%2520accuracy%2520while%2520maintaining%2520decoding%2520speed%2520and%2520KV%2520cache%2520reduction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Layer%20Selection%20for%20Layer-Wise%20Token%20Pruning%20in%20LLM%20Inference&entry.906535625=Rei%20Taniguchi%20and%20Yuyang%20Dong%20and%20Makoto%20Onizuka%20and%20Chuan%20Xiao&entry.1292438233=Due%20to%20the%20prevalence%20of%20large%20language%20models%20%28LLMs%29%2C%20key-value%20%28KV%29%20cache%20reduction%20for%20LLM%20inference%20has%20received%20remarkable%20attention.%20Among%20numerous%20works%20that%20have%20been%20proposed%20in%20recent%20years%2C%20layer-wise%20token%20pruning%20approaches%2C%20which%20select%20a%20subset%20of%20tokens%20at%20particular%20layers%20to%20retain%20in%20KV%20cache%20and%20prune%20others%2C%20are%20one%20of%20the%20most%20popular%20schemes.%20They%20primarily%20adopt%20a%20set%20of%20pre-defined%20layers%2C%20at%20which%20tokens%20are%20selected.%20Such%20design%20is%20inflexible%20in%20the%20sense%20that%20the%20accuracy%20significantly%20varies%20across%20tasks%20and%20deteriorates%20in%20harder%20tasks%20such%20as%20KV%20retrieval.%20In%20this%20paper%2C%20we%20propose%20ASL%2C%20a%20training-free%20method%20that%20adaptively%20chooses%20the%20selection%20layer%20for%20KV%20cache%20reduction%2C%20exploiting%20the%20variance%20of%20token%20ranks%20ordered%20by%20attention%20score.%20The%20proposed%20method%20balances%20the%20performance%20across%20different%20tasks%20while%20meeting%20the%20user-specified%20KV%20budget%20requirement.%20ASL%20operates%20during%20the%20prefilling%20stage%20and%20can%20be%20jointly%20used%20with%20existing%20KV%20cache%20reduction%20methods%20such%20as%20SnapKV%20to%20optimize%20the%20decoding%20stage.%20By%20evaluations%20on%20the%20InfiniteBench%2C%20RULER%2C%20and%20NIAH%20benchmarks%2C%20we%20show%20that%20equipped%20with%20one-shot%20token%20selection%2C%20where%20tokens%20are%20selected%20at%20a%20layer%20and%20propagated%20to%20deeper%20layers%2C%20ASL%20outperforms%20state-of-the-art%20layer-wise%20token%20selection%20methods%20in%20accuracy%20while%20maintaining%20decoding%20speed%20and%20KV%20cache%20reduction.&entry.1838667208=http%3A//arxiv.org/abs/2601.07667v1&entry.124074799=Read"},
{"title": "A Disproof of Large Language Model Consciousness: The Necessity of Continual Learning for Consciousness", "author": "Erik Hoel", "abstract": "Scientific theories of consciousness should be falsifiable and non-trivial. Recent research has given us formal tools to analyze these requirements of falsifiability and non-triviality for theories of consciousness. Surprisingly, many contemporary theories of consciousness fail to pass this bar, including theories based on causal structure but also (as I demonstrate) theories based on function. Herein I show these requirements of falsifiability and non-triviality especially constrain the potential consciousness of contemporary Large Language Models (LLMs) because of their proximity to systems that are equivalent to LLMs in terms of input/output function; yet, for these functionally equivalent systems, there cannot be any falsifiable and non-trivial theory of consciousness that judges them conscious. This forms the basis of a disproof of contemporary LLM consciousness. I then show a positive result, which is that theories of consciousness based on (or requiring) continual learning do satisfy the stringent formal constraints for a theory of consciousness in humans. Intriguingly, this work supports a hypothesis: If continual learning is linked to consciousness in humans, the current limitations of LLMs (which do not continually learn) are intimately tied to their lack of consciousness.", "link": "http://arxiv.org/abs/2512.12802v2", "date": "2026-01-12", "relevancy": 2.2889, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4688}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Disproof%20of%20Large%20Language%20Model%20Consciousness%3A%20The%20Necessity%20of%20Continual%20Learning%20for%20Consciousness&body=Title%3A%20A%20Disproof%20of%20Large%20Language%20Model%20Consciousness%3A%20The%20Necessity%20of%20Continual%20Learning%20for%20Consciousness%0AAuthor%3A%20Erik%20Hoel%0AAbstract%3A%20Scientific%20theories%20of%20consciousness%20should%20be%20falsifiable%20and%20non-trivial.%20Recent%20research%20has%20given%20us%20formal%20tools%20to%20analyze%20these%20requirements%20of%20falsifiability%20and%20non-triviality%20for%20theories%20of%20consciousness.%20Surprisingly%2C%20many%20contemporary%20theories%20of%20consciousness%20fail%20to%20pass%20this%20bar%2C%20including%20theories%20based%20on%20causal%20structure%20but%20also%20%28as%20I%20demonstrate%29%20theories%20based%20on%20function.%20Herein%20I%20show%20these%20requirements%20of%20falsifiability%20and%20non-triviality%20especially%20constrain%20the%20potential%20consciousness%20of%20contemporary%20Large%20Language%20Models%20%28LLMs%29%20because%20of%20their%20proximity%20to%20systems%20that%20are%20equivalent%20to%20LLMs%20in%20terms%20of%20input/output%20function%3B%20yet%2C%20for%20these%20functionally%20equivalent%20systems%2C%20there%20cannot%20be%20any%20falsifiable%20and%20non-trivial%20theory%20of%20consciousness%20that%20judges%20them%20conscious.%20This%20forms%20the%20basis%20of%20a%20disproof%20of%20contemporary%20LLM%20consciousness.%20I%20then%20show%20a%20positive%20result%2C%20which%20is%20that%20theories%20of%20consciousness%20based%20on%20%28or%20requiring%29%20continual%20learning%20do%20satisfy%20the%20stringent%20formal%20constraints%20for%20a%20theory%20of%20consciousness%20in%20humans.%20Intriguingly%2C%20this%20work%20supports%20a%20hypothesis%3A%20If%20continual%20learning%20is%20linked%20to%20consciousness%20in%20humans%2C%20the%20current%20limitations%20of%20LLMs%20%28which%20do%20not%20continually%20learn%29%20are%20intimately%20tied%20to%20their%20lack%20of%20consciousness.%0ALink%3A%20http%3A//arxiv.org/abs/2512.12802v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Disproof%2520of%2520Large%2520Language%2520Model%2520Consciousness%253A%2520The%2520Necessity%2520of%2520Continual%2520Learning%2520for%2520Consciousness%26entry.906535625%3DErik%2520Hoel%26entry.1292438233%3DScientific%2520theories%2520of%2520consciousness%2520should%2520be%2520falsifiable%2520and%2520non-trivial.%2520Recent%2520research%2520has%2520given%2520us%2520formal%2520tools%2520to%2520analyze%2520these%2520requirements%2520of%2520falsifiability%2520and%2520non-triviality%2520for%2520theories%2520of%2520consciousness.%2520Surprisingly%252C%2520many%2520contemporary%2520theories%2520of%2520consciousness%2520fail%2520to%2520pass%2520this%2520bar%252C%2520including%2520theories%2520based%2520on%2520causal%2520structure%2520but%2520also%2520%2528as%2520I%2520demonstrate%2529%2520theories%2520based%2520on%2520function.%2520Herein%2520I%2520show%2520these%2520requirements%2520of%2520falsifiability%2520and%2520non-triviality%2520especially%2520constrain%2520the%2520potential%2520consciousness%2520of%2520contemporary%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520because%2520of%2520their%2520proximity%2520to%2520systems%2520that%2520are%2520equivalent%2520to%2520LLMs%2520in%2520terms%2520of%2520input/output%2520function%253B%2520yet%252C%2520for%2520these%2520functionally%2520equivalent%2520systems%252C%2520there%2520cannot%2520be%2520any%2520falsifiable%2520and%2520non-trivial%2520theory%2520of%2520consciousness%2520that%2520judges%2520them%2520conscious.%2520This%2520forms%2520the%2520basis%2520of%2520a%2520disproof%2520of%2520contemporary%2520LLM%2520consciousness.%2520I%2520then%2520show%2520a%2520positive%2520result%252C%2520which%2520is%2520that%2520theories%2520of%2520consciousness%2520based%2520on%2520%2528or%2520requiring%2529%2520continual%2520learning%2520do%2520satisfy%2520the%2520stringent%2520formal%2520constraints%2520for%2520a%2520theory%2520of%2520consciousness%2520in%2520humans.%2520Intriguingly%252C%2520this%2520work%2520supports%2520a%2520hypothesis%253A%2520If%2520continual%2520learning%2520is%2520linked%2520to%2520consciousness%2520in%2520humans%252C%2520the%2520current%2520limitations%2520of%2520LLMs%2520%2528which%2520do%2520not%2520continually%2520learn%2529%2520are%2520intimately%2520tied%2520to%2520their%2520lack%2520of%2520consciousness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.12802v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Disproof%20of%20Large%20Language%20Model%20Consciousness%3A%20The%20Necessity%20of%20Continual%20Learning%20for%20Consciousness&entry.906535625=Erik%20Hoel&entry.1292438233=Scientific%20theories%20of%20consciousness%20should%20be%20falsifiable%20and%20non-trivial.%20Recent%20research%20has%20given%20us%20formal%20tools%20to%20analyze%20these%20requirements%20of%20falsifiability%20and%20non-triviality%20for%20theories%20of%20consciousness.%20Surprisingly%2C%20many%20contemporary%20theories%20of%20consciousness%20fail%20to%20pass%20this%20bar%2C%20including%20theories%20based%20on%20causal%20structure%20but%20also%20%28as%20I%20demonstrate%29%20theories%20based%20on%20function.%20Herein%20I%20show%20these%20requirements%20of%20falsifiability%20and%20non-triviality%20especially%20constrain%20the%20potential%20consciousness%20of%20contemporary%20Large%20Language%20Models%20%28LLMs%29%20because%20of%20their%20proximity%20to%20systems%20that%20are%20equivalent%20to%20LLMs%20in%20terms%20of%20input/output%20function%3B%20yet%2C%20for%20these%20functionally%20equivalent%20systems%2C%20there%20cannot%20be%20any%20falsifiable%20and%20non-trivial%20theory%20of%20consciousness%20that%20judges%20them%20conscious.%20This%20forms%20the%20basis%20of%20a%20disproof%20of%20contemporary%20LLM%20consciousness.%20I%20then%20show%20a%20positive%20result%2C%20which%20is%20that%20theories%20of%20consciousness%20based%20on%20%28or%20requiring%29%20continual%20learning%20do%20satisfy%20the%20stringent%20formal%20constraints%20for%20a%20theory%20of%20consciousness%20in%20humans.%20Intriguingly%2C%20this%20work%20supports%20a%20hypothesis%3A%20If%20continual%20learning%20is%20linked%20to%20consciousness%20in%20humans%2C%20the%20current%20limitations%20of%20LLMs%20%28which%20do%20not%20continually%20learn%29%20are%20intimately%20tied%20to%20their%20lack%20of%20consciousness.&entry.1838667208=http%3A//arxiv.org/abs/2512.12802v2&entry.124074799=Read"},
{"title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head", "author": "Kewei Zhang and Ye Huang and Yufan Deng and Jincheng Yu and Junsong Chen and Huan Ling and Enze Xie and Daquan Zhou", "abstract": "While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\\% improvement on ImageNet classification, a 6.3\\% gain on NLP, a 12.6\\% improvement on image generation, and a 41\\% enhancement on video generation under the same time complexity.", "link": "http://arxiv.org/abs/2601.07832v1", "date": "2026-01-12", "relevancy": 2.2861, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6101}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5443}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MHLA%3A%20Restoring%20Expressivity%20of%20Linear%20Attention%20via%20Token-Level%20Multi-Head&body=Title%3A%20MHLA%3A%20Restoring%20Expressivity%20of%20Linear%20Attention%20via%20Token-Level%20Multi-Head%0AAuthor%3A%20Kewei%20Zhang%20and%20Ye%20Huang%20and%20Yufan%20Deng%20and%20Jincheng%20Yu%20and%20Junsong%20Chen%20and%20Huan%20Ling%20and%20Enze%20Xie%20and%20Daquan%20Zhou%0AAbstract%3A%20While%20the%20Transformer%20architecture%20dominates%20many%20fields%2C%20its%20quadratic%20self-attention%20complexity%20hinders%20its%20use%20in%20large-scale%20applications.%20Linear%20attention%20offers%20an%20efficient%20alternative%2C%20but%20its%20direct%20application%20often%20degrades%20performance%2C%20with%20existing%20fixes%20typically%20re-introducing%20computational%20overhead%20through%20extra%20modules%20%28e.g.%2C%20depthwise%20separable%20convolution%29%20that%20defeat%20the%20original%20purpose.%20In%20this%20work%2C%20we%20identify%20a%20key%20failure%20mode%20in%20these%20methods%3A%20global%20context%20collapse%2C%20where%20the%20model%20loses%20representational%20diversity.%20To%20address%20this%2C%20we%20propose%20Multi-Head%20Linear%20Attention%20%28MHLA%29%2C%20which%20preserves%20this%20diversity%20by%20computing%20attention%20within%20divided%20heads%20along%20the%20token%20dimension.%20We%20prove%20that%20MHLA%20maintains%20linear%20complexity%20while%20recovering%20much%20of%20the%20expressive%20power%20of%20softmax%20attention%2C%20and%20verify%20its%20effectiveness%20across%20multiple%20domains%2C%20achieving%20a%203.6%5C%25%20improvement%20on%20ImageNet%20classification%2C%20a%206.3%5C%25%20gain%20on%20NLP%2C%20a%2012.6%5C%25%20improvement%20on%20image%20generation%2C%20and%20a%2041%5C%25%20enhancement%20on%20video%20generation%20under%20the%20same%20time%20complexity.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMHLA%253A%2520Restoring%2520Expressivity%2520of%2520Linear%2520Attention%2520via%2520Token-Level%2520Multi-Head%26entry.906535625%3DKewei%2520Zhang%2520and%2520Ye%2520Huang%2520and%2520Yufan%2520Deng%2520and%2520Jincheng%2520Yu%2520and%2520Junsong%2520Chen%2520and%2520Huan%2520Ling%2520and%2520Enze%2520Xie%2520and%2520Daquan%2520Zhou%26entry.1292438233%3DWhile%2520the%2520Transformer%2520architecture%2520dominates%2520many%2520fields%252C%2520its%2520quadratic%2520self-attention%2520complexity%2520hinders%2520its%2520use%2520in%2520large-scale%2520applications.%2520Linear%2520attention%2520offers%2520an%2520efficient%2520alternative%252C%2520but%2520its%2520direct%2520application%2520often%2520degrades%2520performance%252C%2520with%2520existing%2520fixes%2520typically%2520re-introducing%2520computational%2520overhead%2520through%2520extra%2520modules%2520%2528e.g.%252C%2520depthwise%2520separable%2520convolution%2529%2520that%2520defeat%2520the%2520original%2520purpose.%2520In%2520this%2520work%252C%2520we%2520identify%2520a%2520key%2520failure%2520mode%2520in%2520these%2520methods%253A%2520global%2520context%2520collapse%252C%2520where%2520the%2520model%2520loses%2520representational%2520diversity.%2520To%2520address%2520this%252C%2520we%2520propose%2520Multi-Head%2520Linear%2520Attention%2520%2528MHLA%2529%252C%2520which%2520preserves%2520this%2520diversity%2520by%2520computing%2520attention%2520within%2520divided%2520heads%2520along%2520the%2520token%2520dimension.%2520We%2520prove%2520that%2520MHLA%2520maintains%2520linear%2520complexity%2520while%2520recovering%2520much%2520of%2520the%2520expressive%2520power%2520of%2520softmax%2520attention%252C%2520and%2520verify%2520its%2520effectiveness%2520across%2520multiple%2520domains%252C%2520achieving%2520a%25203.6%255C%2525%2520improvement%2520on%2520ImageNet%2520classification%252C%2520a%25206.3%255C%2525%2520gain%2520on%2520NLP%252C%2520a%252012.6%255C%2525%2520improvement%2520on%2520image%2520generation%252C%2520and%2520a%252041%255C%2525%2520enhancement%2520on%2520video%2520generation%2520under%2520the%2520same%2520time%2520complexity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MHLA%3A%20Restoring%20Expressivity%20of%20Linear%20Attention%20via%20Token-Level%20Multi-Head&entry.906535625=Kewei%20Zhang%20and%20Ye%20Huang%20and%20Yufan%20Deng%20and%20Jincheng%20Yu%20and%20Junsong%20Chen%20and%20Huan%20Ling%20and%20Enze%20Xie%20and%20Daquan%20Zhou&entry.1292438233=While%20the%20Transformer%20architecture%20dominates%20many%20fields%2C%20its%20quadratic%20self-attention%20complexity%20hinders%20its%20use%20in%20large-scale%20applications.%20Linear%20attention%20offers%20an%20efficient%20alternative%2C%20but%20its%20direct%20application%20often%20degrades%20performance%2C%20with%20existing%20fixes%20typically%20re-introducing%20computational%20overhead%20through%20extra%20modules%20%28e.g.%2C%20depthwise%20separable%20convolution%29%20that%20defeat%20the%20original%20purpose.%20In%20this%20work%2C%20we%20identify%20a%20key%20failure%20mode%20in%20these%20methods%3A%20global%20context%20collapse%2C%20where%20the%20model%20loses%20representational%20diversity.%20To%20address%20this%2C%20we%20propose%20Multi-Head%20Linear%20Attention%20%28MHLA%29%2C%20which%20preserves%20this%20diversity%20by%20computing%20attention%20within%20divided%20heads%20along%20the%20token%20dimension.%20We%20prove%20that%20MHLA%20maintains%20linear%20complexity%20while%20recovering%20much%20of%20the%20expressive%20power%20of%20softmax%20attention%2C%20and%20verify%20its%20effectiveness%20across%20multiple%20domains%2C%20achieving%20a%203.6%5C%25%20improvement%20on%20ImageNet%20classification%2C%20a%206.3%5C%25%20gain%20on%20NLP%2C%20a%2012.6%5C%25%20improvement%20on%20image%20generation%2C%20and%20a%2041%5C%25%20enhancement%20on%20video%20generation%20under%20the%20same%20time%20complexity.&entry.1838667208=http%3A//arxiv.org/abs/2601.07832v1&entry.124074799=Read"},
{"title": "Near-Optimal Private Linear Regression via Iterative Hessian Mixing", "author": "Omri Lev and Moshe Shenfeld and Vishwak Srinivasan and Katrina Ligett and Ashia C. Wilson", "abstract": "We study differentially private ordinary least squares (DP-OLS) with bounded data. The dominant approach, adaptive sufficient-statistics perturbation (AdaSSP), adds an adaptively chosen perturbation to the sufficient statistics, namely, the matrix $X^{\\top}X$ and the vector $X^{\\top}Y$, and is known to achieve near-optimal accuracy and to have strong empirical performance. In contrast, methods that rely on Gaussian-sketching, which ensure differential privacy by pre-multiplying the data with a random Gaussian matrix, are widely used in federated and distributed regression, yet remain relatively uncommon for DP-OLS. In this work, we introduce the iterative Hessian mixing, a novel DP-OLS algorithm that relies on Gaussian sketches and is inspired by the iterative Hessian sketch algorithm. We provide utility analysis for the iterative Hessian mixing as well as a new analysis for the previous methods that rely on Gaussian sketches. Then, we show that our new approach circumvents the intrinsic limitations of the prior methods and provides non-trivial improvements over AdaSSP. We conclude by running an extensive set of experiments across standard benchmarks to demonstrate further that our approach consistently outperforms these prior baselines.", "link": "http://arxiv.org/abs/2601.07545v1", "date": "2026-01-12", "relevancy": 2.2801, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.463}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4537}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Near-Optimal%20Private%20Linear%20Regression%20via%20Iterative%20Hessian%20Mixing&body=Title%3A%20Near-Optimal%20Private%20Linear%20Regression%20via%20Iterative%20Hessian%20Mixing%0AAuthor%3A%20Omri%20Lev%20and%20Moshe%20Shenfeld%20and%20Vishwak%20Srinivasan%20and%20Katrina%20Ligett%20and%20Ashia%20C.%20Wilson%0AAbstract%3A%20We%20study%20differentially%20private%20ordinary%20least%20squares%20%28DP-OLS%29%20with%20bounded%20data.%20The%20dominant%20approach%2C%20adaptive%20sufficient-statistics%20perturbation%20%28AdaSSP%29%2C%20adds%20an%20adaptively%20chosen%20perturbation%20to%20the%20sufficient%20statistics%2C%20namely%2C%20the%20matrix%20%24X%5E%7B%5Ctop%7DX%24%20and%20the%20vector%20%24X%5E%7B%5Ctop%7DY%24%2C%20and%20is%20known%20to%20achieve%20near-optimal%20accuracy%20and%20to%20have%20strong%20empirical%20performance.%20In%20contrast%2C%20methods%20that%20rely%20on%20Gaussian-sketching%2C%20which%20ensure%20differential%20privacy%20by%20pre-multiplying%20the%20data%20with%20a%20random%20Gaussian%20matrix%2C%20are%20widely%20used%20in%20federated%20and%20distributed%20regression%2C%20yet%20remain%20relatively%20uncommon%20for%20DP-OLS.%20In%20this%20work%2C%20we%20introduce%20the%20iterative%20Hessian%20mixing%2C%20a%20novel%20DP-OLS%20algorithm%20that%20relies%20on%20Gaussian%20sketches%20and%20is%20inspired%20by%20the%20iterative%20Hessian%20sketch%20algorithm.%20We%20provide%20utility%20analysis%20for%20the%20iterative%20Hessian%20mixing%20as%20well%20as%20a%20new%20analysis%20for%20the%20previous%20methods%20that%20rely%20on%20Gaussian%20sketches.%20Then%2C%20we%20show%20that%20our%20new%20approach%20circumvents%20the%20intrinsic%20limitations%20of%20the%20prior%20methods%20and%20provides%20non-trivial%20improvements%20over%20AdaSSP.%20We%20conclude%20by%20running%20an%20extensive%20set%20of%20experiments%20across%20standard%20benchmarks%20to%20demonstrate%20further%20that%20our%20approach%20consistently%20outperforms%20these%20prior%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNear-Optimal%2520Private%2520Linear%2520Regression%2520via%2520Iterative%2520Hessian%2520Mixing%26entry.906535625%3DOmri%2520Lev%2520and%2520Moshe%2520Shenfeld%2520and%2520Vishwak%2520Srinivasan%2520and%2520Katrina%2520Ligett%2520and%2520Ashia%2520C.%2520Wilson%26entry.1292438233%3DWe%2520study%2520differentially%2520private%2520ordinary%2520least%2520squares%2520%2528DP-OLS%2529%2520with%2520bounded%2520data.%2520The%2520dominant%2520approach%252C%2520adaptive%2520sufficient-statistics%2520perturbation%2520%2528AdaSSP%2529%252C%2520adds%2520an%2520adaptively%2520chosen%2520perturbation%2520to%2520the%2520sufficient%2520statistics%252C%2520namely%252C%2520the%2520matrix%2520%2524X%255E%257B%255Ctop%257DX%2524%2520and%2520the%2520vector%2520%2524X%255E%257B%255Ctop%257DY%2524%252C%2520and%2520is%2520known%2520to%2520achieve%2520near-optimal%2520accuracy%2520and%2520to%2520have%2520strong%2520empirical%2520performance.%2520In%2520contrast%252C%2520methods%2520that%2520rely%2520on%2520Gaussian-sketching%252C%2520which%2520ensure%2520differential%2520privacy%2520by%2520pre-multiplying%2520the%2520data%2520with%2520a%2520random%2520Gaussian%2520matrix%252C%2520are%2520widely%2520used%2520in%2520federated%2520and%2520distributed%2520regression%252C%2520yet%2520remain%2520relatively%2520uncommon%2520for%2520DP-OLS.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520iterative%2520Hessian%2520mixing%252C%2520a%2520novel%2520DP-OLS%2520algorithm%2520that%2520relies%2520on%2520Gaussian%2520sketches%2520and%2520is%2520inspired%2520by%2520the%2520iterative%2520Hessian%2520sketch%2520algorithm.%2520We%2520provide%2520utility%2520analysis%2520for%2520the%2520iterative%2520Hessian%2520mixing%2520as%2520well%2520as%2520a%2520new%2520analysis%2520for%2520the%2520previous%2520methods%2520that%2520rely%2520on%2520Gaussian%2520sketches.%2520Then%252C%2520we%2520show%2520that%2520our%2520new%2520approach%2520circumvents%2520the%2520intrinsic%2520limitations%2520of%2520the%2520prior%2520methods%2520and%2520provides%2520non-trivial%2520improvements%2520over%2520AdaSSP.%2520We%2520conclude%2520by%2520running%2520an%2520extensive%2520set%2520of%2520experiments%2520across%2520standard%2520benchmarks%2520to%2520demonstrate%2520further%2520that%2520our%2520approach%2520consistently%2520outperforms%2520these%2520prior%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Near-Optimal%20Private%20Linear%20Regression%20via%20Iterative%20Hessian%20Mixing&entry.906535625=Omri%20Lev%20and%20Moshe%20Shenfeld%20and%20Vishwak%20Srinivasan%20and%20Katrina%20Ligett%20and%20Ashia%20C.%20Wilson&entry.1292438233=We%20study%20differentially%20private%20ordinary%20least%20squares%20%28DP-OLS%29%20with%20bounded%20data.%20The%20dominant%20approach%2C%20adaptive%20sufficient-statistics%20perturbation%20%28AdaSSP%29%2C%20adds%20an%20adaptively%20chosen%20perturbation%20to%20the%20sufficient%20statistics%2C%20namely%2C%20the%20matrix%20%24X%5E%7B%5Ctop%7DX%24%20and%20the%20vector%20%24X%5E%7B%5Ctop%7DY%24%2C%20and%20is%20known%20to%20achieve%20near-optimal%20accuracy%20and%20to%20have%20strong%20empirical%20performance.%20In%20contrast%2C%20methods%20that%20rely%20on%20Gaussian-sketching%2C%20which%20ensure%20differential%20privacy%20by%20pre-multiplying%20the%20data%20with%20a%20random%20Gaussian%20matrix%2C%20are%20widely%20used%20in%20federated%20and%20distributed%20regression%2C%20yet%20remain%20relatively%20uncommon%20for%20DP-OLS.%20In%20this%20work%2C%20we%20introduce%20the%20iterative%20Hessian%20mixing%2C%20a%20novel%20DP-OLS%20algorithm%20that%20relies%20on%20Gaussian%20sketches%20and%20is%20inspired%20by%20the%20iterative%20Hessian%20sketch%20algorithm.%20We%20provide%20utility%20analysis%20for%20the%20iterative%20Hessian%20mixing%20as%20well%20as%20a%20new%20analysis%20for%20the%20previous%20methods%20that%20rely%20on%20Gaussian%20sketches.%20Then%2C%20we%20show%20that%20our%20new%20approach%20circumvents%20the%20intrinsic%20limitations%20of%20the%20prior%20methods%20and%20provides%20non-trivial%20improvements%20over%20AdaSSP.%20We%20conclude%20by%20running%20an%20extensive%20set%20of%20experiments%20across%20standard%20benchmarks%20to%20demonstrate%20further%20that%20our%20approach%20consistently%20outperforms%20these%20prior%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.07545v1&entry.124074799=Read"},
{"title": "VirtualEnv: A Platform for Embodied AI Research", "author": "Kabir Swain and Sijie Han and Ayush Raina and Jin Zhang and Shuang Li and Michael Stopa and Antonio Torralba", "abstract": "As large language models (LLMs) continue to improve in reasoning and decision-making, there is a growing need for realistic and interactive environments where their abilities can be rigorously evaluated. We present VirtualEnv, a next-generation simulation platform built on Unreal Engine 5 that enables fine-grained benchmarking of LLMs in embodied and interactive scenarios. VirtualEnv supports rich agent-environment interactions, including object manipulation, navigation, and adaptive multi-agent collaboration, as well as game-inspired mechanics like escape rooms and procedurally generated environments. We provide a user-friendly API built on top of Unreal Engine, allowing researchers to deploy and control LLM-driven agents using natural language instructions. We integrate large-scale LLMs and vision-language models (VLMs), such as GPT-based models, to generate novel environments and structured tasks from multimodal inputs. Our experiments benchmark the performance of several popular LLMs across tasks of increasing complexity, analyzing differences in adaptability, planning, and multi-agent coordination. We also describe our methodology for procedural task generation, task validation, and real-time environment control. VirtualEnv is released as an open-source platform, we aim to advance research at the intersection of AI and gaming, enable standardized evaluation of LLMs in embodied AI settings, and pave the way for future developments in immersive simulations and interactive entertainment.", "link": "http://arxiv.org/abs/2601.07553v1", "date": "2026-01-12", "relevancy": 2.2747, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6017}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VirtualEnv%3A%20A%20Platform%20for%20Embodied%20AI%20Research&body=Title%3A%20VirtualEnv%3A%20A%20Platform%20for%20Embodied%20AI%20Research%0AAuthor%3A%20Kabir%20Swain%20and%20Sijie%20Han%20and%20Ayush%20Raina%20and%20Jin%20Zhang%20and%20Shuang%20Li%20and%20Michael%20Stopa%20and%20Antonio%20Torralba%0AAbstract%3A%20As%20large%20language%20models%20%28LLMs%29%20continue%20to%20improve%20in%20reasoning%20and%20decision-making%2C%20there%20is%20a%20growing%20need%20for%20realistic%20and%20interactive%20environments%20where%20their%20abilities%20can%20be%20rigorously%20evaluated.%20We%20present%20VirtualEnv%2C%20a%20next-generation%20simulation%20platform%20built%20on%20Unreal%20Engine%205%20that%20enables%20fine-grained%20benchmarking%20of%20LLMs%20in%20embodied%20and%20interactive%20scenarios.%20VirtualEnv%20supports%20rich%20agent-environment%20interactions%2C%20including%20object%20manipulation%2C%20navigation%2C%20and%20adaptive%20multi-agent%20collaboration%2C%20as%20well%20as%20game-inspired%20mechanics%20like%20escape%20rooms%20and%20procedurally%20generated%20environments.%20We%20provide%20a%20user-friendly%20API%20built%20on%20top%20of%20Unreal%20Engine%2C%20allowing%20researchers%20to%20deploy%20and%20control%20LLM-driven%20agents%20using%20natural%20language%20instructions.%20We%20integrate%20large-scale%20LLMs%20and%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20GPT-based%20models%2C%20to%20generate%20novel%20environments%20and%20structured%20tasks%20from%20multimodal%20inputs.%20Our%20experiments%20benchmark%20the%20performance%20of%20several%20popular%20LLMs%20across%20tasks%20of%20increasing%20complexity%2C%20analyzing%20differences%20in%20adaptability%2C%20planning%2C%20and%20multi-agent%20coordination.%20We%20also%20describe%20our%20methodology%20for%20procedural%20task%20generation%2C%20task%20validation%2C%20and%20real-time%20environment%20control.%20VirtualEnv%20is%20released%20as%20an%20open-source%20platform%2C%20we%20aim%20to%20advance%20research%20at%20the%20intersection%20of%20AI%20and%20gaming%2C%20enable%20standardized%20evaluation%20of%20LLMs%20in%20embodied%20AI%20settings%2C%20and%20pave%20the%20way%20for%20future%20developments%20in%20immersive%20simulations%20and%20interactive%20entertainment.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVirtualEnv%253A%2520A%2520Platform%2520for%2520Embodied%2520AI%2520Research%26entry.906535625%3DKabir%2520Swain%2520and%2520Sijie%2520Han%2520and%2520Ayush%2520Raina%2520and%2520Jin%2520Zhang%2520and%2520Shuang%2520Li%2520and%2520Michael%2520Stopa%2520and%2520Antonio%2520Torralba%26entry.1292438233%3DAs%2520large%2520language%2520models%2520%2528LLMs%2529%2520continue%2520to%2520improve%2520in%2520reasoning%2520and%2520decision-making%252C%2520there%2520is%2520a%2520growing%2520need%2520for%2520realistic%2520and%2520interactive%2520environments%2520where%2520their%2520abilities%2520can%2520be%2520rigorously%2520evaluated.%2520We%2520present%2520VirtualEnv%252C%2520a%2520next-generation%2520simulation%2520platform%2520built%2520on%2520Unreal%2520Engine%25205%2520that%2520enables%2520fine-grained%2520benchmarking%2520of%2520LLMs%2520in%2520embodied%2520and%2520interactive%2520scenarios.%2520VirtualEnv%2520supports%2520rich%2520agent-environment%2520interactions%252C%2520including%2520object%2520manipulation%252C%2520navigation%252C%2520and%2520adaptive%2520multi-agent%2520collaboration%252C%2520as%2520well%2520as%2520game-inspired%2520mechanics%2520like%2520escape%2520rooms%2520and%2520procedurally%2520generated%2520environments.%2520We%2520provide%2520a%2520user-friendly%2520API%2520built%2520on%2520top%2520of%2520Unreal%2520Engine%252C%2520allowing%2520researchers%2520to%2520deploy%2520and%2520control%2520LLM-driven%2520agents%2520using%2520natural%2520language%2520instructions.%2520We%2520integrate%2520large-scale%2520LLMs%2520and%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520GPT-based%2520models%252C%2520to%2520generate%2520novel%2520environments%2520and%2520structured%2520tasks%2520from%2520multimodal%2520inputs.%2520Our%2520experiments%2520benchmark%2520the%2520performance%2520of%2520several%2520popular%2520LLMs%2520across%2520tasks%2520of%2520increasing%2520complexity%252C%2520analyzing%2520differences%2520in%2520adaptability%252C%2520planning%252C%2520and%2520multi-agent%2520coordination.%2520We%2520also%2520describe%2520our%2520methodology%2520for%2520procedural%2520task%2520generation%252C%2520task%2520validation%252C%2520and%2520real-time%2520environment%2520control.%2520VirtualEnv%2520is%2520released%2520as%2520an%2520open-source%2520platform%252C%2520we%2520aim%2520to%2520advance%2520research%2520at%2520the%2520intersection%2520of%2520AI%2520and%2520gaming%252C%2520enable%2520standardized%2520evaluation%2520of%2520LLMs%2520in%2520embodied%2520AI%2520settings%252C%2520and%2520pave%2520the%2520way%2520for%2520future%2520developments%2520in%2520immersive%2520simulations%2520and%2520interactive%2520entertainment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VirtualEnv%3A%20A%20Platform%20for%20Embodied%20AI%20Research&entry.906535625=Kabir%20Swain%20and%20Sijie%20Han%20and%20Ayush%20Raina%20and%20Jin%20Zhang%20and%20Shuang%20Li%20and%20Michael%20Stopa%20and%20Antonio%20Torralba&entry.1292438233=As%20large%20language%20models%20%28LLMs%29%20continue%20to%20improve%20in%20reasoning%20and%20decision-making%2C%20there%20is%20a%20growing%20need%20for%20realistic%20and%20interactive%20environments%20where%20their%20abilities%20can%20be%20rigorously%20evaluated.%20We%20present%20VirtualEnv%2C%20a%20next-generation%20simulation%20platform%20built%20on%20Unreal%20Engine%205%20that%20enables%20fine-grained%20benchmarking%20of%20LLMs%20in%20embodied%20and%20interactive%20scenarios.%20VirtualEnv%20supports%20rich%20agent-environment%20interactions%2C%20including%20object%20manipulation%2C%20navigation%2C%20and%20adaptive%20multi-agent%20collaboration%2C%20as%20well%20as%20game-inspired%20mechanics%20like%20escape%20rooms%20and%20procedurally%20generated%20environments.%20We%20provide%20a%20user-friendly%20API%20built%20on%20top%20of%20Unreal%20Engine%2C%20allowing%20researchers%20to%20deploy%20and%20control%20LLM-driven%20agents%20using%20natural%20language%20instructions.%20We%20integrate%20large-scale%20LLMs%20and%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20GPT-based%20models%2C%20to%20generate%20novel%20environments%20and%20structured%20tasks%20from%20multimodal%20inputs.%20Our%20experiments%20benchmark%20the%20performance%20of%20several%20popular%20LLMs%20across%20tasks%20of%20increasing%20complexity%2C%20analyzing%20differences%20in%20adaptability%2C%20planning%2C%20and%20multi-agent%20coordination.%20We%20also%20describe%20our%20methodology%20for%20procedural%20task%20generation%2C%20task%20validation%2C%20and%20real-time%20environment%20control.%20VirtualEnv%20is%20released%20as%20an%20open-source%20platform%2C%20we%20aim%20to%20advance%20research%20at%20the%20intersection%20of%20AI%20and%20gaming%2C%20enable%20standardized%20evaluation%20of%20LLMs%20in%20embodied%20AI%20settings%2C%20and%20pave%20the%20way%20for%20future%20developments%20in%20immersive%20simulations%20and%20interactive%20entertainment.&entry.1838667208=http%3A//arxiv.org/abs/2601.07553v1&entry.124074799=Read"},
{"title": "Reasoning Models Will Blatantly Lie About Their Reasoning", "author": "William Walden", "abstract": "It has been shown that Large Reasoning Models (LRMs) may not *say what they think*: they do not always volunteer information about how certain parts of the input influence their reasoning. But it is one thing for a model to *omit* such information and another, worse thing to *lie* about it. Here, we extend the work of Chen et al. (2025) to show that LRMs will do just this: they will flatly deny relying on hints provided in the prompt in answering multiple choice questions -- even when directly asked to reflect on unusual (i.e. hinted) prompt content, even when allowed to use hints, and even though experiments *show* them to be using the hints. Our results thus have discouraging implications for CoT monitoring and interpretability.", "link": "http://arxiv.org/abs/2601.07663v1", "date": "2026-01-12", "relevancy": 2.2729, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4387}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20Models%20Will%20Blatantly%20Lie%20About%20Their%20Reasoning&body=Title%3A%20Reasoning%20Models%20Will%20Blatantly%20Lie%20About%20Their%20Reasoning%0AAuthor%3A%20William%20Walden%0AAbstract%3A%20It%20has%20been%20shown%20that%20Large%20Reasoning%20Models%20%28LRMs%29%20may%20not%20%2Asay%20what%20they%20think%2A%3A%20they%20do%20not%20always%20volunteer%20information%20about%20how%20certain%20parts%20of%20the%20input%20influence%20their%20reasoning.%20But%20it%20is%20one%20thing%20for%20a%20model%20to%20%2Aomit%2A%20such%20information%20and%20another%2C%20worse%20thing%20to%20%2Alie%2A%20about%20it.%20Here%2C%20we%20extend%20the%20work%20of%20Chen%20et%20al.%20%282025%29%20to%20show%20that%20LRMs%20will%20do%20just%20this%3A%20they%20will%20flatly%20deny%20relying%20on%20hints%20provided%20in%20the%20prompt%20in%20answering%20multiple%20choice%20questions%20--%20even%20when%20directly%20asked%20to%20reflect%20on%20unusual%20%28i.e.%20hinted%29%20prompt%20content%2C%20even%20when%20allowed%20to%20use%20hints%2C%20and%20even%20though%20experiments%20%2Ashow%2A%20them%20to%20be%20using%20the%20hints.%20Our%20results%20thus%20have%20discouraging%20implications%20for%20CoT%20monitoring%20and%20interpretability.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520Models%2520Will%2520Blatantly%2520Lie%2520About%2520Their%2520Reasoning%26entry.906535625%3DWilliam%2520Walden%26entry.1292438233%3DIt%2520has%2520been%2520shown%2520that%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529%2520may%2520not%2520%252Asay%2520what%2520they%2520think%252A%253A%2520they%2520do%2520not%2520always%2520volunteer%2520information%2520about%2520how%2520certain%2520parts%2520of%2520the%2520input%2520influence%2520their%2520reasoning.%2520But%2520it%2520is%2520one%2520thing%2520for%2520a%2520model%2520to%2520%252Aomit%252A%2520such%2520information%2520and%2520another%252C%2520worse%2520thing%2520to%2520%252Alie%252A%2520about%2520it.%2520Here%252C%2520we%2520extend%2520the%2520work%2520of%2520Chen%2520et%2520al.%2520%25282025%2529%2520to%2520show%2520that%2520LRMs%2520will%2520do%2520just%2520this%253A%2520they%2520will%2520flatly%2520deny%2520relying%2520on%2520hints%2520provided%2520in%2520the%2520prompt%2520in%2520answering%2520multiple%2520choice%2520questions%2520--%2520even%2520when%2520directly%2520asked%2520to%2520reflect%2520on%2520unusual%2520%2528i.e.%2520hinted%2529%2520prompt%2520content%252C%2520even%2520when%2520allowed%2520to%2520use%2520hints%252C%2520and%2520even%2520though%2520experiments%2520%252Ashow%252A%2520them%2520to%2520be%2520using%2520the%2520hints.%2520Our%2520results%2520thus%2520have%2520discouraging%2520implications%2520for%2520CoT%2520monitoring%2520and%2520interpretability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20Models%20Will%20Blatantly%20Lie%20About%20Their%20Reasoning&entry.906535625=William%20Walden&entry.1292438233=It%20has%20been%20shown%20that%20Large%20Reasoning%20Models%20%28LRMs%29%20may%20not%20%2Asay%20what%20they%20think%2A%3A%20they%20do%20not%20always%20volunteer%20information%20about%20how%20certain%20parts%20of%20the%20input%20influence%20their%20reasoning.%20But%20it%20is%20one%20thing%20for%20a%20model%20to%20%2Aomit%2A%20such%20information%20and%20another%2C%20worse%20thing%20to%20%2Alie%2A%20about%20it.%20Here%2C%20we%20extend%20the%20work%20of%20Chen%20et%20al.%20%282025%29%20to%20show%20that%20LRMs%20will%20do%20just%20this%3A%20they%20will%20flatly%20deny%20relying%20on%20hints%20provided%20in%20the%20prompt%20in%20answering%20multiple%20choice%20questions%20--%20even%20when%20directly%20asked%20to%20reflect%20on%20unusual%20%28i.e.%20hinted%29%20prompt%20content%2C%20even%20when%20allowed%20to%20use%20hints%2C%20and%20even%20though%20experiments%20%2Ashow%2A%20them%20to%20be%20using%20the%20hints.%20Our%20results%20thus%20have%20discouraging%20implications%20for%20CoT%20monitoring%20and%20interpretability.&entry.1838667208=http%3A//arxiv.org/abs/2601.07663v1&entry.124074799=Read"},
{"title": "MoE3D: A Mixture-of-Experts Module for 3D Reconstruction", "author": "Zichen Wang and Ang Cao and Liam J. Wang and Jeong Joon Park", "abstract": "We propose a simple yet effective approach to enhance the performance of feed-forward 3D reconstruction models. Existing methods often struggle near depth discontinuities, where standard regression losses encourage spatial averaging and thus blur sharp boundaries. To address this issue, we introduce a mixture-of-experts formulation that handles uncertainty at depth boundaries by combining multiple smooth depth predictions. A softmax weighting head dynamically selects among these hypotheses on a per-pixel basis. By integrating our mixture model into a pre-trained state-of-the-art 3D model, we achieve a substantial reduction of boundary artifacts and gains in overall reconstruction accuracy. Notably, our approach is highly compute efficient, delivering generalizable improvements even when fine-tuned on a small subset of training data while incurring only negligible additional inference computation, suggesting a promising direction for lightweight and accurate 3D reconstruction.", "link": "http://arxiv.org/abs/2601.05208v2", "date": "2026-01-12", "relevancy": 2.2717, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5878}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5659}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoE3D%3A%20A%20Mixture-of-Experts%20Module%20for%203D%20Reconstruction&body=Title%3A%20MoE3D%3A%20A%20Mixture-of-Experts%20Module%20for%203D%20Reconstruction%0AAuthor%3A%20Zichen%20Wang%20and%20Ang%20Cao%20and%20Liam%20J.%20Wang%20and%20Jeong%20Joon%20Park%0AAbstract%3A%20We%20propose%20a%20simple%20yet%20effective%20approach%20to%20enhance%20the%20performance%20of%20feed-forward%203D%20reconstruction%20models.%20Existing%20methods%20often%20struggle%20near%20depth%20discontinuities%2C%20where%20standard%20regression%20losses%20encourage%20spatial%20averaging%20and%20thus%20blur%20sharp%20boundaries.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20mixture-of-experts%20formulation%20that%20handles%20uncertainty%20at%20depth%20boundaries%20by%20combining%20multiple%20smooth%20depth%20predictions.%20A%20softmax%20weighting%20head%20dynamically%20selects%20among%20these%20hypotheses%20on%20a%20per-pixel%20basis.%20By%20integrating%20our%20mixture%20model%20into%20a%20pre-trained%20state-of-the-art%203D%20model%2C%20we%20achieve%20a%20substantial%20reduction%20of%20boundary%20artifacts%20and%20gains%20in%20overall%20reconstruction%20accuracy.%20Notably%2C%20our%20approach%20is%20highly%20compute%20efficient%2C%20delivering%20generalizable%20improvements%20even%20when%20fine-tuned%20on%20a%20small%20subset%20of%20training%20data%20while%20incurring%20only%20negligible%20additional%20inference%20computation%2C%20suggesting%20a%20promising%20direction%20for%20lightweight%20and%20accurate%203D%20reconstruction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05208v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoE3D%253A%2520A%2520Mixture-of-Experts%2520Module%2520for%25203D%2520Reconstruction%26entry.906535625%3DZichen%2520Wang%2520and%2520Ang%2520Cao%2520and%2520Liam%2520J.%2520Wang%2520and%2520Jeong%2520Joon%2520Park%26entry.1292438233%3DWe%2520propose%2520a%2520simple%2520yet%2520effective%2520approach%2520to%2520enhance%2520the%2520performance%2520of%2520feed-forward%25203D%2520reconstruction%2520models.%2520Existing%2520methods%2520often%2520struggle%2520near%2520depth%2520discontinuities%252C%2520where%2520standard%2520regression%2520losses%2520encourage%2520spatial%2520averaging%2520and%2520thus%2520blur%2520sharp%2520boundaries.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520mixture-of-experts%2520formulation%2520that%2520handles%2520uncertainty%2520at%2520depth%2520boundaries%2520by%2520combining%2520multiple%2520smooth%2520depth%2520predictions.%2520A%2520softmax%2520weighting%2520head%2520dynamically%2520selects%2520among%2520these%2520hypotheses%2520on%2520a%2520per-pixel%2520basis.%2520By%2520integrating%2520our%2520mixture%2520model%2520into%2520a%2520pre-trained%2520state-of-the-art%25203D%2520model%252C%2520we%2520achieve%2520a%2520substantial%2520reduction%2520of%2520boundary%2520artifacts%2520and%2520gains%2520in%2520overall%2520reconstruction%2520accuracy.%2520Notably%252C%2520our%2520approach%2520is%2520highly%2520compute%2520efficient%252C%2520delivering%2520generalizable%2520improvements%2520even%2520when%2520fine-tuned%2520on%2520a%2520small%2520subset%2520of%2520training%2520data%2520while%2520incurring%2520only%2520negligible%2520additional%2520inference%2520computation%252C%2520suggesting%2520a%2520promising%2520direction%2520for%2520lightweight%2520and%2520accurate%25203D%2520reconstruction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05208v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoE3D%3A%20A%20Mixture-of-Experts%20Module%20for%203D%20Reconstruction&entry.906535625=Zichen%20Wang%20and%20Ang%20Cao%20and%20Liam%20J.%20Wang%20and%20Jeong%20Joon%20Park&entry.1292438233=We%20propose%20a%20simple%20yet%20effective%20approach%20to%20enhance%20the%20performance%20of%20feed-forward%203D%20reconstruction%20models.%20Existing%20methods%20often%20struggle%20near%20depth%20discontinuities%2C%20where%20standard%20regression%20losses%20encourage%20spatial%20averaging%20and%20thus%20blur%20sharp%20boundaries.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20mixture-of-experts%20formulation%20that%20handles%20uncertainty%20at%20depth%20boundaries%20by%20combining%20multiple%20smooth%20depth%20predictions.%20A%20softmax%20weighting%20head%20dynamically%20selects%20among%20these%20hypotheses%20on%20a%20per-pixel%20basis.%20By%20integrating%20our%20mixture%20model%20into%20a%20pre-trained%20state-of-the-art%203D%20model%2C%20we%20achieve%20a%20substantial%20reduction%20of%20boundary%20artifacts%20and%20gains%20in%20overall%20reconstruction%20accuracy.%20Notably%2C%20our%20approach%20is%20highly%20compute%20efficient%2C%20delivering%20generalizable%20improvements%20even%20when%20fine-tuned%20on%20a%20small%20subset%20of%20training%20data%20while%20incurring%20only%20negligible%20additional%20inference%20computation%2C%20suggesting%20a%20promising%20direction%20for%20lightweight%20and%20accurate%203D%20reconstruction.&entry.1838667208=http%3A//arxiv.org/abs/2601.05208v2&entry.124074799=Read"},
{"title": "OceanSAR-2: A Universal Feature Extractor for SAR Ocean Observation", "author": "Alexandre Tuel and Thomas Kerdreux and Quentin Febvre and Alexis Mouche and Antoine Grouazel and Jean-Renaud Miadana and Antoine Audras and Chen Wang and Bertrand Chapron", "abstract": "We present OceanSAR-2, the second generation of our foundation model for SAR-based ocean observation. Building on our earlier release, which pioneered self-supervised learning on Sentinel-1 Wave Mode data, OceanSAR-2 relies on improved SSL training and dynamic data curation strategies, which enhances performance while reducing training cost. OceanSAR-2 demonstrates strong transfer performance across downstream tasks, including geophysical pattern classification, ocean surface wind vector and significant wave height estimation, and iceberg detection. We release standardized benchmark datasets, providing a foundation for systematic evaluation and advancement of SAR models for ocean applications.", "link": "http://arxiv.org/abs/2601.07392v1", "date": "2026-01-12", "relevancy": 2.2537, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4532}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OceanSAR-2%3A%20A%20Universal%20Feature%20Extractor%20for%20SAR%20Ocean%20Observation&body=Title%3A%20OceanSAR-2%3A%20A%20Universal%20Feature%20Extractor%20for%20SAR%20Ocean%20Observation%0AAuthor%3A%20Alexandre%20Tuel%20and%20Thomas%20Kerdreux%20and%20Quentin%20Febvre%20and%20Alexis%20Mouche%20and%20Antoine%20Grouazel%20and%20Jean-Renaud%20Miadana%20and%20Antoine%20Audras%20and%20Chen%20Wang%20and%20Bertrand%20Chapron%0AAbstract%3A%20We%20present%20OceanSAR-2%2C%20the%20second%20generation%20of%20our%20foundation%20model%20for%20SAR-based%20ocean%20observation.%20Building%20on%20our%20earlier%20release%2C%20which%20pioneered%20self-supervised%20learning%20on%20Sentinel-1%20Wave%20Mode%20data%2C%20OceanSAR-2%20relies%20on%20improved%20SSL%20training%20and%20dynamic%20data%20curation%20strategies%2C%20which%20enhances%20performance%20while%20reducing%20training%20cost.%20OceanSAR-2%20demonstrates%20strong%20transfer%20performance%20across%20downstream%20tasks%2C%20including%20geophysical%20pattern%20classification%2C%20ocean%20surface%20wind%20vector%20and%20significant%20wave%20height%20estimation%2C%20and%20iceberg%20detection.%20We%20release%20standardized%20benchmark%20datasets%2C%20providing%20a%20foundation%20for%20systematic%20evaluation%20and%20advancement%20of%20SAR%20models%20for%20ocean%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOceanSAR-2%253A%2520A%2520Universal%2520Feature%2520Extractor%2520for%2520SAR%2520Ocean%2520Observation%26entry.906535625%3DAlexandre%2520Tuel%2520and%2520Thomas%2520Kerdreux%2520and%2520Quentin%2520Febvre%2520and%2520Alexis%2520Mouche%2520and%2520Antoine%2520Grouazel%2520and%2520Jean-Renaud%2520Miadana%2520and%2520Antoine%2520Audras%2520and%2520Chen%2520Wang%2520and%2520Bertrand%2520Chapron%26entry.1292438233%3DWe%2520present%2520OceanSAR-2%252C%2520the%2520second%2520generation%2520of%2520our%2520foundation%2520model%2520for%2520SAR-based%2520ocean%2520observation.%2520Building%2520on%2520our%2520earlier%2520release%252C%2520which%2520pioneered%2520self-supervised%2520learning%2520on%2520Sentinel-1%2520Wave%2520Mode%2520data%252C%2520OceanSAR-2%2520relies%2520on%2520improved%2520SSL%2520training%2520and%2520dynamic%2520data%2520curation%2520strategies%252C%2520which%2520enhances%2520performance%2520while%2520reducing%2520training%2520cost.%2520OceanSAR-2%2520demonstrates%2520strong%2520transfer%2520performance%2520across%2520downstream%2520tasks%252C%2520including%2520geophysical%2520pattern%2520classification%252C%2520ocean%2520surface%2520wind%2520vector%2520and%2520significant%2520wave%2520height%2520estimation%252C%2520and%2520iceberg%2520detection.%2520We%2520release%2520standardized%2520benchmark%2520datasets%252C%2520providing%2520a%2520foundation%2520for%2520systematic%2520evaluation%2520and%2520advancement%2520of%2520SAR%2520models%2520for%2520ocean%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OceanSAR-2%3A%20A%20Universal%20Feature%20Extractor%20for%20SAR%20Ocean%20Observation&entry.906535625=Alexandre%20Tuel%20and%20Thomas%20Kerdreux%20and%20Quentin%20Febvre%20and%20Alexis%20Mouche%20and%20Antoine%20Grouazel%20and%20Jean-Renaud%20Miadana%20and%20Antoine%20Audras%20and%20Chen%20Wang%20and%20Bertrand%20Chapron&entry.1292438233=We%20present%20OceanSAR-2%2C%20the%20second%20generation%20of%20our%20foundation%20model%20for%20SAR-based%20ocean%20observation.%20Building%20on%20our%20earlier%20release%2C%20which%20pioneered%20self-supervised%20learning%20on%20Sentinel-1%20Wave%20Mode%20data%2C%20OceanSAR-2%20relies%20on%20improved%20SSL%20training%20and%20dynamic%20data%20curation%20strategies%2C%20which%20enhances%20performance%20while%20reducing%20training%20cost.%20OceanSAR-2%20demonstrates%20strong%20transfer%20performance%20across%20downstream%20tasks%2C%20including%20geophysical%20pattern%20classification%2C%20ocean%20surface%20wind%20vector%20and%20significant%20wave%20height%20estimation%2C%20and%20iceberg%20detection.%20We%20release%20standardized%20benchmark%20datasets%2C%20providing%20a%20foundation%20for%20systematic%20evaluation%20and%20advancement%20of%20SAR%20models%20for%20ocean%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2601.07392v1&entry.124074799=Read"},
{"title": "FlyCo: Foundation Model-Empowered Drones for Autonomous 3D Structure Scanning in Open-World Environments", "author": "Chen Feng and Guiyong Zheng and Tengkai Zhuang and Yongqian Wu and Fangzhan He and Haojia Li and Juepeng Zheng and Shaojie Shen and Boyu Zhou", "abstract": "Autonomous 3D scanning of open-world target structures via drones remains challenging despite broad applications. Existing paradigms rely on restrictive assumptions or effortful human priors, limiting practicality, efficiency, and adaptability. Recent foundation models (FMs) offer great potential to bridge this gap. This paper investigates a critical research problem: What system architecture can effectively integrate FM knowledge for this task? We answer it with FlyCo, a principled FM-empowered perception-prediction-planning loop enabling fully autonomous, prompt-driven 3D target scanning in diverse unknown open-world environments. FlyCo directly translates low-effort human prompts (text, visual annotations) into precise adaptive scanning flights via three coordinated stages: (1) perception fuses streaming sensor data with vision-language FMs for robust target grounding and tracking; (2) prediction distills FM knowledge and combines multi-modal cues to infer the partially observed target's complete geometry; (3) planning leverages predictive foresight to generate efficient and safe paths with comprehensive target coverage. Building on this, we further design key components to boost open-world target grounding efficiency and robustness, enhance prediction quality in terms of shape accuracy, zero-shot generalization, and temporal stability, and balance long-horizon flight efficiency with real-time computability and online collision avoidance. Extensive challenging real-world and simulation experiments show FlyCo delivers precise scene understanding, high efficiency, and real-time safety, outperforming existing paradigms with lower human effort and verifying the proposed architecture's practicality. Comprehensive ablations validate each component's contribution. FlyCo also serves as a flexible, extensible blueprint, readily leveraging future FM and robotics advances. Code will be released.", "link": "http://arxiv.org/abs/2601.07558v1", "date": "2026-01-12", "relevancy": 2.2476, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlyCo%3A%20Foundation%20Model-Empowered%20Drones%20for%20Autonomous%203D%20Structure%20Scanning%20in%20Open-World%20Environments&body=Title%3A%20FlyCo%3A%20Foundation%20Model-Empowered%20Drones%20for%20Autonomous%203D%20Structure%20Scanning%20in%20Open-World%20Environments%0AAuthor%3A%20Chen%20Feng%20and%20Guiyong%20Zheng%20and%20Tengkai%20Zhuang%20and%20Yongqian%20Wu%20and%20Fangzhan%20He%20and%20Haojia%20Li%20and%20Juepeng%20Zheng%20and%20Shaojie%20Shen%20and%20Boyu%20Zhou%0AAbstract%3A%20Autonomous%203D%20scanning%20of%20open-world%20target%20structures%20via%20drones%20remains%20challenging%20despite%20broad%20applications.%20Existing%20paradigms%20rely%20on%20restrictive%20assumptions%20or%20effortful%20human%20priors%2C%20limiting%20practicality%2C%20efficiency%2C%20and%20adaptability.%20Recent%20foundation%20models%20%28FMs%29%20offer%20great%20potential%20to%20bridge%20this%20gap.%20This%20paper%20investigates%20a%20critical%20research%20problem%3A%20What%20system%20architecture%20can%20effectively%20integrate%20FM%20knowledge%20for%20this%20task%3F%20We%20answer%20it%20with%20FlyCo%2C%20a%20principled%20FM-empowered%20perception-prediction-planning%20loop%20enabling%20fully%20autonomous%2C%20prompt-driven%203D%20target%20scanning%20in%20diverse%20unknown%20open-world%20environments.%20FlyCo%20directly%20translates%20low-effort%20human%20prompts%20%28text%2C%20visual%20annotations%29%20into%20precise%20adaptive%20scanning%20flights%20via%20three%20coordinated%20stages%3A%20%281%29%20perception%20fuses%20streaming%20sensor%20data%20with%20vision-language%20FMs%20for%20robust%20target%20grounding%20and%20tracking%3B%20%282%29%20prediction%20distills%20FM%20knowledge%20and%20combines%20multi-modal%20cues%20to%20infer%20the%20partially%20observed%20target%27s%20complete%20geometry%3B%20%283%29%20planning%20leverages%20predictive%20foresight%20to%20generate%20efficient%20and%20safe%20paths%20with%20comprehensive%20target%20coverage.%20Building%20on%20this%2C%20we%20further%20design%20key%20components%20to%20boost%20open-world%20target%20grounding%20efficiency%20and%20robustness%2C%20enhance%20prediction%20quality%20in%20terms%20of%20shape%20accuracy%2C%20zero-shot%20generalization%2C%20and%20temporal%20stability%2C%20and%20balance%20long-horizon%20flight%20efficiency%20with%20real-time%20computability%20and%20online%20collision%20avoidance.%20Extensive%20challenging%20real-world%20and%20simulation%20experiments%20show%20FlyCo%20delivers%20precise%20scene%20understanding%2C%20high%20efficiency%2C%20and%20real-time%20safety%2C%20outperforming%20existing%20paradigms%20with%20lower%20human%20effort%20and%20verifying%20the%20proposed%20architecture%27s%20practicality.%20Comprehensive%20ablations%20validate%20each%20component%27s%20contribution.%20FlyCo%20also%20serves%20as%20a%20flexible%2C%20extensible%20blueprint%2C%20readily%20leveraging%20future%20FM%20and%20robotics%20advances.%20Code%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlyCo%253A%2520Foundation%2520Model-Empowered%2520Drones%2520for%2520Autonomous%25203D%2520Structure%2520Scanning%2520in%2520Open-World%2520Environments%26entry.906535625%3DChen%2520Feng%2520and%2520Guiyong%2520Zheng%2520and%2520Tengkai%2520Zhuang%2520and%2520Yongqian%2520Wu%2520and%2520Fangzhan%2520He%2520and%2520Haojia%2520Li%2520and%2520Juepeng%2520Zheng%2520and%2520Shaojie%2520Shen%2520and%2520Boyu%2520Zhou%26entry.1292438233%3DAutonomous%25203D%2520scanning%2520of%2520open-world%2520target%2520structures%2520via%2520drones%2520remains%2520challenging%2520despite%2520broad%2520applications.%2520Existing%2520paradigms%2520rely%2520on%2520restrictive%2520assumptions%2520or%2520effortful%2520human%2520priors%252C%2520limiting%2520practicality%252C%2520efficiency%252C%2520and%2520adaptability.%2520Recent%2520foundation%2520models%2520%2528FMs%2529%2520offer%2520great%2520potential%2520to%2520bridge%2520this%2520gap.%2520This%2520paper%2520investigates%2520a%2520critical%2520research%2520problem%253A%2520What%2520system%2520architecture%2520can%2520effectively%2520integrate%2520FM%2520knowledge%2520for%2520this%2520task%253F%2520We%2520answer%2520it%2520with%2520FlyCo%252C%2520a%2520principled%2520FM-empowered%2520perception-prediction-planning%2520loop%2520enabling%2520fully%2520autonomous%252C%2520prompt-driven%25203D%2520target%2520scanning%2520in%2520diverse%2520unknown%2520open-world%2520environments.%2520FlyCo%2520directly%2520translates%2520low-effort%2520human%2520prompts%2520%2528text%252C%2520visual%2520annotations%2529%2520into%2520precise%2520adaptive%2520scanning%2520flights%2520via%2520three%2520coordinated%2520stages%253A%2520%25281%2529%2520perception%2520fuses%2520streaming%2520sensor%2520data%2520with%2520vision-language%2520FMs%2520for%2520robust%2520target%2520grounding%2520and%2520tracking%253B%2520%25282%2529%2520prediction%2520distills%2520FM%2520knowledge%2520and%2520combines%2520multi-modal%2520cues%2520to%2520infer%2520the%2520partially%2520observed%2520target%2527s%2520complete%2520geometry%253B%2520%25283%2529%2520planning%2520leverages%2520predictive%2520foresight%2520to%2520generate%2520efficient%2520and%2520safe%2520paths%2520with%2520comprehensive%2520target%2520coverage.%2520Building%2520on%2520this%252C%2520we%2520further%2520design%2520key%2520components%2520to%2520boost%2520open-world%2520target%2520grounding%2520efficiency%2520and%2520robustness%252C%2520enhance%2520prediction%2520quality%2520in%2520terms%2520of%2520shape%2520accuracy%252C%2520zero-shot%2520generalization%252C%2520and%2520temporal%2520stability%252C%2520and%2520balance%2520long-horizon%2520flight%2520efficiency%2520with%2520real-time%2520computability%2520and%2520online%2520collision%2520avoidance.%2520Extensive%2520challenging%2520real-world%2520and%2520simulation%2520experiments%2520show%2520FlyCo%2520delivers%2520precise%2520scene%2520understanding%252C%2520high%2520efficiency%252C%2520and%2520real-time%2520safety%252C%2520outperforming%2520existing%2520paradigms%2520with%2520lower%2520human%2520effort%2520and%2520verifying%2520the%2520proposed%2520architecture%2527s%2520practicality.%2520Comprehensive%2520ablations%2520validate%2520each%2520component%2527s%2520contribution.%2520FlyCo%2520also%2520serves%2520as%2520a%2520flexible%252C%2520extensible%2520blueprint%252C%2520readily%2520leveraging%2520future%2520FM%2520and%2520robotics%2520advances.%2520Code%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlyCo%3A%20Foundation%20Model-Empowered%20Drones%20for%20Autonomous%203D%20Structure%20Scanning%20in%20Open-World%20Environments&entry.906535625=Chen%20Feng%20and%20Guiyong%20Zheng%20and%20Tengkai%20Zhuang%20and%20Yongqian%20Wu%20and%20Fangzhan%20He%20and%20Haojia%20Li%20and%20Juepeng%20Zheng%20and%20Shaojie%20Shen%20and%20Boyu%20Zhou&entry.1292438233=Autonomous%203D%20scanning%20of%20open-world%20target%20structures%20via%20drones%20remains%20challenging%20despite%20broad%20applications.%20Existing%20paradigms%20rely%20on%20restrictive%20assumptions%20or%20effortful%20human%20priors%2C%20limiting%20practicality%2C%20efficiency%2C%20and%20adaptability.%20Recent%20foundation%20models%20%28FMs%29%20offer%20great%20potential%20to%20bridge%20this%20gap.%20This%20paper%20investigates%20a%20critical%20research%20problem%3A%20What%20system%20architecture%20can%20effectively%20integrate%20FM%20knowledge%20for%20this%20task%3F%20We%20answer%20it%20with%20FlyCo%2C%20a%20principled%20FM-empowered%20perception-prediction-planning%20loop%20enabling%20fully%20autonomous%2C%20prompt-driven%203D%20target%20scanning%20in%20diverse%20unknown%20open-world%20environments.%20FlyCo%20directly%20translates%20low-effort%20human%20prompts%20%28text%2C%20visual%20annotations%29%20into%20precise%20adaptive%20scanning%20flights%20via%20three%20coordinated%20stages%3A%20%281%29%20perception%20fuses%20streaming%20sensor%20data%20with%20vision-language%20FMs%20for%20robust%20target%20grounding%20and%20tracking%3B%20%282%29%20prediction%20distills%20FM%20knowledge%20and%20combines%20multi-modal%20cues%20to%20infer%20the%20partially%20observed%20target%27s%20complete%20geometry%3B%20%283%29%20planning%20leverages%20predictive%20foresight%20to%20generate%20efficient%20and%20safe%20paths%20with%20comprehensive%20target%20coverage.%20Building%20on%20this%2C%20we%20further%20design%20key%20components%20to%20boost%20open-world%20target%20grounding%20efficiency%20and%20robustness%2C%20enhance%20prediction%20quality%20in%20terms%20of%20shape%20accuracy%2C%20zero-shot%20generalization%2C%20and%20temporal%20stability%2C%20and%20balance%20long-horizon%20flight%20efficiency%20with%20real-time%20computability%20and%20online%20collision%20avoidance.%20Extensive%20challenging%20real-world%20and%20simulation%20experiments%20show%20FlyCo%20delivers%20precise%20scene%20understanding%2C%20high%20efficiency%2C%20and%20real-time%20safety%2C%20outperforming%20existing%20paradigms%20with%20lower%20human%20effort%20and%20verifying%20the%20proposed%20architecture%27s%20practicality.%20Comprehensive%20ablations%20validate%20each%20component%27s%20contribution.%20FlyCo%20also%20serves%20as%20a%20flexible%2C%20extensible%20blueprint%2C%20readily%20leveraging%20future%20FM%20and%20robotics%20advances.%20Code%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2601.07558v1&entry.124074799=Read"},
{"title": "Smooth regularization for efficient video recognition", "author": "Gil Goldman and Raja Giryes and Mahadev Satyanarayanan", "abstract": "We propose a smooth regularization technique that instills a strong temporal inductive bias in video recognition models, particularly benefiting lightweight architectures. Our method encourages smoothness in the intermediate-layer embeddings of consecutive frames by modeling their changes as a Gaussian Random Walk (GRW). This penalizes abrupt representational shifts, thereby promoting low-acceleration solutions that better align with the natural temporal coherence inherent in videos. By leveraging this enforced smoothness, lightweight models can more effectively capture complex temporal dynamics. Applied to such models, our technique yields a 3.8% to 6.4% accuracy improvement on Kinetics-600. Notably, the MoViNets model family trained with our smooth regularization improves the current state of the art by 3.8% to 6.1% within their respective FLOP constraints, while MobileNetV3 and the MoViNets-Stream family achieve gains of 4.9% to 6.4% over prior state-of-the-art models with comparable memory footprints. Our code and models are available at https://github.com/cmusatyalab/grw-smoothing.", "link": "http://arxiv.org/abs/2511.20928v2", "date": "2026-01-12", "relevancy": 2.2226, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5699}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5641}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smooth%20regularization%20for%20efficient%20video%20recognition&body=Title%3A%20Smooth%20regularization%20for%20efficient%20video%20recognition%0AAuthor%3A%20Gil%20Goldman%20and%20Raja%20Giryes%20and%20Mahadev%20Satyanarayanan%0AAbstract%3A%20We%20propose%20a%20smooth%20regularization%20technique%20that%20instills%20a%20strong%20temporal%20inductive%20bias%20in%20video%20recognition%20models%2C%20particularly%20benefiting%20lightweight%20architectures.%20Our%20method%20encourages%20smoothness%20in%20the%20intermediate-layer%20embeddings%20of%20consecutive%20frames%20by%20modeling%20their%20changes%20as%20a%20Gaussian%20Random%20Walk%20%28GRW%29.%20This%20penalizes%20abrupt%20representational%20shifts%2C%20thereby%20promoting%20low-acceleration%20solutions%20that%20better%20align%20with%20the%20natural%20temporal%20coherence%20inherent%20in%20videos.%20By%20leveraging%20this%20enforced%20smoothness%2C%20lightweight%20models%20can%20more%20effectively%20capture%20complex%20temporal%20dynamics.%20Applied%20to%20such%20models%2C%20our%20technique%20yields%20a%203.8%25%20to%206.4%25%20accuracy%20improvement%20on%20Kinetics-600.%20Notably%2C%20the%20MoViNets%20model%20family%20trained%20with%20our%20smooth%20regularization%20improves%20the%20current%20state%20of%20the%20art%20by%203.8%25%20to%206.1%25%20within%20their%20respective%20FLOP%20constraints%2C%20while%20MobileNetV3%20and%20the%20MoViNets-Stream%20family%20achieve%20gains%20of%204.9%25%20to%206.4%25%20over%20prior%20state-of-the-art%20models%20with%20comparable%20memory%20footprints.%20Our%20code%20and%20models%20are%20available%20at%20https%3A//github.com/cmusatyalab/grw-smoothing.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20928v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmooth%2520regularization%2520for%2520efficient%2520video%2520recognition%26entry.906535625%3DGil%2520Goldman%2520and%2520Raja%2520Giryes%2520and%2520Mahadev%2520Satyanarayanan%26entry.1292438233%3DWe%2520propose%2520a%2520smooth%2520regularization%2520technique%2520that%2520instills%2520a%2520strong%2520temporal%2520inductive%2520bias%2520in%2520video%2520recognition%2520models%252C%2520particularly%2520benefiting%2520lightweight%2520architectures.%2520Our%2520method%2520encourages%2520smoothness%2520in%2520the%2520intermediate-layer%2520embeddings%2520of%2520consecutive%2520frames%2520by%2520modeling%2520their%2520changes%2520as%2520a%2520Gaussian%2520Random%2520Walk%2520%2528GRW%2529.%2520This%2520penalizes%2520abrupt%2520representational%2520shifts%252C%2520thereby%2520promoting%2520low-acceleration%2520solutions%2520that%2520better%2520align%2520with%2520the%2520natural%2520temporal%2520coherence%2520inherent%2520in%2520videos.%2520By%2520leveraging%2520this%2520enforced%2520smoothness%252C%2520lightweight%2520models%2520can%2520more%2520effectively%2520capture%2520complex%2520temporal%2520dynamics.%2520Applied%2520to%2520such%2520models%252C%2520our%2520technique%2520yields%2520a%25203.8%2525%2520to%25206.4%2525%2520accuracy%2520improvement%2520on%2520Kinetics-600.%2520Notably%252C%2520the%2520MoViNets%2520model%2520family%2520trained%2520with%2520our%2520smooth%2520regularization%2520improves%2520the%2520current%2520state%2520of%2520the%2520art%2520by%25203.8%2525%2520to%25206.1%2525%2520within%2520their%2520respective%2520FLOP%2520constraints%252C%2520while%2520MobileNetV3%2520and%2520the%2520MoViNets-Stream%2520family%2520achieve%2520gains%2520of%25204.9%2525%2520to%25206.4%2525%2520over%2520prior%2520state-of-the-art%2520models%2520with%2520comparable%2520memory%2520footprints.%2520Our%2520code%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/cmusatyalab/grw-smoothing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20928v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smooth%20regularization%20for%20efficient%20video%20recognition&entry.906535625=Gil%20Goldman%20and%20Raja%20Giryes%20and%20Mahadev%20Satyanarayanan&entry.1292438233=We%20propose%20a%20smooth%20regularization%20technique%20that%20instills%20a%20strong%20temporal%20inductive%20bias%20in%20video%20recognition%20models%2C%20particularly%20benefiting%20lightweight%20architectures.%20Our%20method%20encourages%20smoothness%20in%20the%20intermediate-layer%20embeddings%20of%20consecutive%20frames%20by%20modeling%20their%20changes%20as%20a%20Gaussian%20Random%20Walk%20%28GRW%29.%20This%20penalizes%20abrupt%20representational%20shifts%2C%20thereby%20promoting%20low-acceleration%20solutions%20that%20better%20align%20with%20the%20natural%20temporal%20coherence%20inherent%20in%20videos.%20By%20leveraging%20this%20enforced%20smoothness%2C%20lightweight%20models%20can%20more%20effectively%20capture%20complex%20temporal%20dynamics.%20Applied%20to%20such%20models%2C%20our%20technique%20yields%20a%203.8%25%20to%206.4%25%20accuracy%20improvement%20on%20Kinetics-600.%20Notably%2C%20the%20MoViNets%20model%20family%20trained%20with%20our%20smooth%20regularization%20improves%20the%20current%20state%20of%20the%20art%20by%203.8%25%20to%206.1%25%20within%20their%20respective%20FLOP%20constraints%2C%20while%20MobileNetV3%20and%20the%20MoViNets-Stream%20family%20achieve%20gains%20of%204.9%25%20to%206.4%25%20over%20prior%20state-of-the-art%20models%20with%20comparable%20memory%20footprints.%20Our%20code%20and%20models%20are%20available%20at%20https%3A//github.com/cmusatyalab/grw-smoothing.&entry.1838667208=http%3A//arxiv.org/abs/2511.20928v2&entry.124074799=Read"},
{"title": "Self-Creating Random Walks for Decentralized Learning under Pac-Man Attacks", "author": "Xingran Chen and Parimal Parag and Rohit Bhagat and Salim El Rouayheb", "abstract": "Random walk (RW)-based algorithms have long been popular in distributed systems due to low overheads and scalability, with recent growing applications in decentralized learning. However, their reliance on local interactions makes them inherently vulnerable to malicious behavior. In this work, we investigate an adversarial threat that we term the ``Pac-Man'' attack, in which a malicious node probabilistically terminates any RW that visits it. This stealthy behavior gradually eliminates active RWs from the network, effectively halting the learning process without triggering failure alarms. To counter this threat, we propose the CREATE-IF-LATE (CIL) algorithm, which is a fully decentralized, resilient mechanism that enables self-creating RWs and prevents RW extinction in the presence of Pac-Man. Our theoretical analysis shows that the CIL algorithm guarantees several desirable properties, such as (i) non-extinction of the RW population, (ii) almost sure boundedness of the RW population, and (iii) convergence of RW-based stochastic gradient descent even in the presence of Pac-Man with a quantifiable deviation from the true optimum. Moreover, the learning process experiences at most a linear time delay due to Pac-Man interruptions and RW regeneration. Our extensive empirical results on both synthetic and public benchmark datasets validate our theoretical findings.", "link": "http://arxiv.org/abs/2601.07674v1", "date": "2026-01-12", "relevancy": 2.216, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4477}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4435}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Creating%20Random%20Walks%20for%20Decentralized%20Learning%20under%20Pac-Man%20Attacks&body=Title%3A%20Self-Creating%20Random%20Walks%20for%20Decentralized%20Learning%20under%20Pac-Man%20Attacks%0AAuthor%3A%20Xingran%20Chen%20and%20Parimal%20Parag%20and%20Rohit%20Bhagat%20and%20Salim%20El%20Rouayheb%0AAbstract%3A%20Random%20walk%20%28RW%29-based%20algorithms%20have%20long%20been%20popular%20in%20distributed%20systems%20due%20to%20low%20overheads%20and%20scalability%2C%20with%20recent%20growing%20applications%20in%20decentralized%20learning.%20However%2C%20their%20reliance%20on%20local%20interactions%20makes%20them%20inherently%20vulnerable%20to%20malicious%20behavior.%20In%20this%20work%2C%20we%20investigate%20an%20adversarial%20threat%20that%20we%20term%20the%20%60%60Pac-Man%27%27%20attack%2C%20in%20which%20a%20malicious%20node%20probabilistically%20terminates%20any%20RW%20that%20visits%20it.%20This%20stealthy%20behavior%20gradually%20eliminates%20active%20RWs%20from%20the%20network%2C%20effectively%20halting%20the%20learning%20process%20without%20triggering%20failure%20alarms.%20To%20counter%20this%20threat%2C%20we%20propose%20the%20CREATE-IF-LATE%20%28CIL%29%20algorithm%2C%20which%20is%20a%20fully%20decentralized%2C%20resilient%20mechanism%20that%20enables%20self-creating%20RWs%20and%20prevents%20RW%20extinction%20in%20the%20presence%20of%20Pac-Man.%20Our%20theoretical%20analysis%20shows%20that%20the%20CIL%20algorithm%20guarantees%20several%20desirable%20properties%2C%20such%20as%20%28i%29%20non-extinction%20of%20the%20RW%20population%2C%20%28ii%29%20almost%20sure%20boundedness%20of%20the%20RW%20population%2C%20and%20%28iii%29%20convergence%20of%20RW-based%20stochastic%20gradient%20descent%20even%20in%20the%20presence%20of%20Pac-Man%20with%20a%20quantifiable%20deviation%20from%20the%20true%20optimum.%20Moreover%2C%20the%20learning%20process%20experiences%20at%20most%20a%20linear%20time%20delay%20due%20to%20Pac-Man%20interruptions%20and%20RW%20regeneration.%20Our%20extensive%20empirical%20results%20on%20both%20synthetic%20and%20public%20benchmark%20datasets%20validate%20our%20theoretical%20findings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Creating%2520Random%2520Walks%2520for%2520Decentralized%2520Learning%2520under%2520Pac-Man%2520Attacks%26entry.906535625%3DXingran%2520Chen%2520and%2520Parimal%2520Parag%2520and%2520Rohit%2520Bhagat%2520and%2520Salim%2520El%2520Rouayheb%26entry.1292438233%3DRandom%2520walk%2520%2528RW%2529-based%2520algorithms%2520have%2520long%2520been%2520popular%2520in%2520distributed%2520systems%2520due%2520to%2520low%2520overheads%2520and%2520scalability%252C%2520with%2520recent%2520growing%2520applications%2520in%2520decentralized%2520learning.%2520However%252C%2520their%2520reliance%2520on%2520local%2520interactions%2520makes%2520them%2520inherently%2520vulnerable%2520to%2520malicious%2520behavior.%2520In%2520this%2520work%252C%2520we%2520investigate%2520an%2520adversarial%2520threat%2520that%2520we%2520term%2520the%2520%2560%2560Pac-Man%2527%2527%2520attack%252C%2520in%2520which%2520a%2520malicious%2520node%2520probabilistically%2520terminates%2520any%2520RW%2520that%2520visits%2520it.%2520This%2520stealthy%2520behavior%2520gradually%2520eliminates%2520active%2520RWs%2520from%2520the%2520network%252C%2520effectively%2520halting%2520the%2520learning%2520process%2520without%2520triggering%2520failure%2520alarms.%2520To%2520counter%2520this%2520threat%252C%2520we%2520propose%2520the%2520CREATE-IF-LATE%2520%2528CIL%2529%2520algorithm%252C%2520which%2520is%2520a%2520fully%2520decentralized%252C%2520resilient%2520mechanism%2520that%2520enables%2520self-creating%2520RWs%2520and%2520prevents%2520RW%2520extinction%2520in%2520the%2520presence%2520of%2520Pac-Man.%2520Our%2520theoretical%2520analysis%2520shows%2520that%2520the%2520CIL%2520algorithm%2520guarantees%2520several%2520desirable%2520properties%252C%2520such%2520as%2520%2528i%2529%2520non-extinction%2520of%2520the%2520RW%2520population%252C%2520%2528ii%2529%2520almost%2520sure%2520boundedness%2520of%2520the%2520RW%2520population%252C%2520and%2520%2528iii%2529%2520convergence%2520of%2520RW-based%2520stochastic%2520gradient%2520descent%2520even%2520in%2520the%2520presence%2520of%2520Pac-Man%2520with%2520a%2520quantifiable%2520deviation%2520from%2520the%2520true%2520optimum.%2520Moreover%252C%2520the%2520learning%2520process%2520experiences%2520at%2520most%2520a%2520linear%2520time%2520delay%2520due%2520to%2520Pac-Man%2520interruptions%2520and%2520RW%2520regeneration.%2520Our%2520extensive%2520empirical%2520results%2520on%2520both%2520synthetic%2520and%2520public%2520benchmark%2520datasets%2520validate%2520our%2520theoretical%2520findings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Creating%20Random%20Walks%20for%20Decentralized%20Learning%20under%20Pac-Man%20Attacks&entry.906535625=Xingran%20Chen%20and%20Parimal%20Parag%20and%20Rohit%20Bhagat%20and%20Salim%20El%20Rouayheb&entry.1292438233=Random%20walk%20%28RW%29-based%20algorithms%20have%20long%20been%20popular%20in%20distributed%20systems%20due%20to%20low%20overheads%20and%20scalability%2C%20with%20recent%20growing%20applications%20in%20decentralized%20learning.%20However%2C%20their%20reliance%20on%20local%20interactions%20makes%20them%20inherently%20vulnerable%20to%20malicious%20behavior.%20In%20this%20work%2C%20we%20investigate%20an%20adversarial%20threat%20that%20we%20term%20the%20%60%60Pac-Man%27%27%20attack%2C%20in%20which%20a%20malicious%20node%20probabilistically%20terminates%20any%20RW%20that%20visits%20it.%20This%20stealthy%20behavior%20gradually%20eliminates%20active%20RWs%20from%20the%20network%2C%20effectively%20halting%20the%20learning%20process%20without%20triggering%20failure%20alarms.%20To%20counter%20this%20threat%2C%20we%20propose%20the%20CREATE-IF-LATE%20%28CIL%29%20algorithm%2C%20which%20is%20a%20fully%20decentralized%2C%20resilient%20mechanism%20that%20enables%20self-creating%20RWs%20and%20prevents%20RW%20extinction%20in%20the%20presence%20of%20Pac-Man.%20Our%20theoretical%20analysis%20shows%20that%20the%20CIL%20algorithm%20guarantees%20several%20desirable%20properties%2C%20such%20as%20%28i%29%20non-extinction%20of%20the%20RW%20population%2C%20%28ii%29%20almost%20sure%20boundedness%20of%20the%20RW%20population%2C%20and%20%28iii%29%20convergence%20of%20RW-based%20stochastic%20gradient%20descent%20even%20in%20the%20presence%20of%20Pac-Man%20with%20a%20quantifiable%20deviation%20from%20the%20true%20optimum.%20Moreover%2C%20the%20learning%20process%20experiences%20at%20most%20a%20linear%20time%20delay%20due%20to%20Pac-Man%20interruptions%20and%20RW%20regeneration.%20Our%20extensive%20empirical%20results%20on%20both%20synthetic%20and%20public%20benchmark%20datasets%20validate%20our%20theoretical%20findings.&entry.1838667208=http%3A//arxiv.org/abs/2601.07674v1&entry.124074799=Read"},
{"title": "Thinking Before Constraining: A Unified Decoding Framework for Large Language Models", "author": "Ngoc Trinh Hung Nguyen and Alonso Silva and Laith Zumot and Liubov Tupikina and Armen Aghasaryan and Mehwish Alam", "abstract": "Natural generation allows Language Models (LMs) to produce free-form responses with rich reasoning, but the lack of guaranteed structure makes outputs difficult to parse or verify. Structured generation, or constrained decoding, addresses this drawback by producing content in standardized formats such as JSON, ensuring consistency and guaranteed-parsable outputs, but it can inadvertently restrict the model's reasoning capabilities. In this work, we propose a simple approach that combines the advantages of both natural and structured generation. By allowing LLMs to reason freely until specific trigger tokens are generated, and then switching to structured generation, our method preserves the expressive power of natural language reasoning while ensuring the reliability of structured outputs. We further evaluate our approach on several datasets, covering both classification and reasoning tasks, to demonstrate its effectiveness, achieving a substantial gain of up to 27% in accuracy compared to natural generation, while requiring only a small overhead of 10-20 extra tokens.", "link": "http://arxiv.org/abs/2601.07525v1", "date": "2026-01-12", "relevancy": 2.21, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.56}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.56}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20Before%20Constraining%3A%20A%20Unified%20Decoding%20Framework%20for%20Large%20Language%20Models&body=Title%3A%20Thinking%20Before%20Constraining%3A%20A%20Unified%20Decoding%20Framework%20for%20Large%20Language%20Models%0AAuthor%3A%20Ngoc%20Trinh%20Hung%20Nguyen%20and%20Alonso%20Silva%20and%20Laith%20Zumot%20and%20Liubov%20Tupikina%20and%20Armen%20Aghasaryan%20and%20Mehwish%20Alam%0AAbstract%3A%20Natural%20generation%20allows%20Language%20Models%20%28LMs%29%20to%20produce%20free-form%20responses%20with%20rich%20reasoning%2C%20but%20the%20lack%20of%20guaranteed%20structure%20makes%20outputs%20difficult%20to%20parse%20or%20verify.%20Structured%20generation%2C%20or%20constrained%20decoding%2C%20addresses%20this%20drawback%20by%20producing%20content%20in%20standardized%20formats%20such%20as%20JSON%2C%20ensuring%20consistency%20and%20guaranteed-parsable%20outputs%2C%20but%20it%20can%20inadvertently%20restrict%20the%20model%27s%20reasoning%20capabilities.%20In%20this%20work%2C%20we%20propose%20a%20simple%20approach%20that%20combines%20the%20advantages%20of%20both%20natural%20and%20structured%20generation.%20By%20allowing%20LLMs%20to%20reason%20freely%20until%20specific%20trigger%20tokens%20are%20generated%2C%20and%20then%20switching%20to%20structured%20generation%2C%20our%20method%20preserves%20the%20expressive%20power%20of%20natural%20language%20reasoning%20while%20ensuring%20the%20reliability%20of%20structured%20outputs.%20We%20further%20evaluate%20our%20approach%20on%20several%20datasets%2C%20covering%20both%20classification%20and%20reasoning%20tasks%2C%20to%20demonstrate%20its%20effectiveness%2C%20achieving%20a%20substantial%20gain%20of%20up%20to%2027%25%20in%20accuracy%20compared%20to%20natural%20generation%2C%20while%20requiring%20only%20a%20small%20overhead%20of%2010-20%20extra%20tokens.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520Before%2520Constraining%253A%2520A%2520Unified%2520Decoding%2520Framework%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DNgoc%2520Trinh%2520Hung%2520Nguyen%2520and%2520Alonso%2520Silva%2520and%2520Laith%2520Zumot%2520and%2520Liubov%2520Tupikina%2520and%2520Armen%2520Aghasaryan%2520and%2520Mehwish%2520Alam%26entry.1292438233%3DNatural%2520generation%2520allows%2520Language%2520Models%2520%2528LMs%2529%2520to%2520produce%2520free-form%2520responses%2520with%2520rich%2520reasoning%252C%2520but%2520the%2520lack%2520of%2520guaranteed%2520structure%2520makes%2520outputs%2520difficult%2520to%2520parse%2520or%2520verify.%2520Structured%2520generation%252C%2520or%2520constrained%2520decoding%252C%2520addresses%2520this%2520drawback%2520by%2520producing%2520content%2520in%2520standardized%2520formats%2520such%2520as%2520JSON%252C%2520ensuring%2520consistency%2520and%2520guaranteed-parsable%2520outputs%252C%2520but%2520it%2520can%2520inadvertently%2520restrict%2520the%2520model%2527s%2520reasoning%2520capabilities.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520simple%2520approach%2520that%2520combines%2520the%2520advantages%2520of%2520both%2520natural%2520and%2520structured%2520generation.%2520By%2520allowing%2520LLMs%2520to%2520reason%2520freely%2520until%2520specific%2520trigger%2520tokens%2520are%2520generated%252C%2520and%2520then%2520switching%2520to%2520structured%2520generation%252C%2520our%2520method%2520preserves%2520the%2520expressive%2520power%2520of%2520natural%2520language%2520reasoning%2520while%2520ensuring%2520the%2520reliability%2520of%2520structured%2520outputs.%2520We%2520further%2520evaluate%2520our%2520approach%2520on%2520several%2520datasets%252C%2520covering%2520both%2520classification%2520and%2520reasoning%2520tasks%252C%2520to%2520demonstrate%2520its%2520effectiveness%252C%2520achieving%2520a%2520substantial%2520gain%2520of%2520up%2520to%252027%2525%2520in%2520accuracy%2520compared%2520to%2520natural%2520generation%252C%2520while%2520requiring%2520only%2520a%2520small%2520overhead%2520of%252010-20%2520extra%2520tokens.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20Before%20Constraining%3A%20A%20Unified%20Decoding%20Framework%20for%20Large%20Language%20Models&entry.906535625=Ngoc%20Trinh%20Hung%20Nguyen%20and%20Alonso%20Silva%20and%20Laith%20Zumot%20and%20Liubov%20Tupikina%20and%20Armen%20Aghasaryan%20and%20Mehwish%20Alam&entry.1292438233=Natural%20generation%20allows%20Language%20Models%20%28LMs%29%20to%20produce%20free-form%20responses%20with%20rich%20reasoning%2C%20but%20the%20lack%20of%20guaranteed%20structure%20makes%20outputs%20difficult%20to%20parse%20or%20verify.%20Structured%20generation%2C%20or%20constrained%20decoding%2C%20addresses%20this%20drawback%20by%20producing%20content%20in%20standardized%20formats%20such%20as%20JSON%2C%20ensuring%20consistency%20and%20guaranteed-parsable%20outputs%2C%20but%20it%20can%20inadvertently%20restrict%20the%20model%27s%20reasoning%20capabilities.%20In%20this%20work%2C%20we%20propose%20a%20simple%20approach%20that%20combines%20the%20advantages%20of%20both%20natural%20and%20structured%20generation.%20By%20allowing%20LLMs%20to%20reason%20freely%20until%20specific%20trigger%20tokens%20are%20generated%2C%20and%20then%20switching%20to%20structured%20generation%2C%20our%20method%20preserves%20the%20expressive%20power%20of%20natural%20language%20reasoning%20while%20ensuring%20the%20reliability%20of%20structured%20outputs.%20We%20further%20evaluate%20our%20approach%20on%20several%20datasets%2C%20covering%20both%20classification%20and%20reasoning%20tasks%2C%20to%20demonstrate%20its%20effectiveness%2C%20achieving%20a%20substantial%20gain%20of%20up%20to%2027%25%20in%20accuracy%20compared%20to%20natural%20generation%2C%20while%20requiring%20only%20a%20small%20overhead%20of%2010-20%20extra%20tokens.&entry.1838667208=http%3A//arxiv.org/abs/2601.07525v1&entry.124074799=Read"},
{"title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent", "author": "Bowen Yang and Kaiming Jin and Zhenyu Wu and Zhaoyang Liu and Qiushi Sun and Zehao Li and JingJing Xie and Zhoumianze Liu and Fangzhi Xu and Kanzhi Cheng and Qingyun Li and Yian Wang and Yu Qiao and Zun Wang and Zichen Ding", "abstract": "While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.", "link": "http://arxiv.org/abs/2601.07779v1", "date": "2026-01-12", "relevancy": 2.1907, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OS-Symphony%3A%20A%20Holistic%20Framework%20for%20Robust%20and%20Generalist%20Computer-Using%20Agent&body=Title%3A%20OS-Symphony%3A%20A%20Holistic%20Framework%20for%20Robust%20and%20Generalist%20Computer-Using%20Agent%0AAuthor%3A%20Bowen%20Yang%20and%20Kaiming%20Jin%20and%20Zhenyu%20Wu%20and%20Zhaoyang%20Liu%20and%20Qiushi%20Sun%20and%20Zehao%20Li%20and%20JingJing%20Xie%20and%20Zhoumianze%20Liu%20and%20Fangzhi%20Xu%20and%20Kanzhi%20Cheng%20and%20Qingyun%20Li%20and%20Yian%20Wang%20and%20Yu%20Qiao%20and%20Zun%20Wang%20and%20Zichen%20Ding%0AAbstract%3A%20While%20Vision-Language%20Models%20%28VLMs%29%20have%20significantly%20advanced%20Computer-Using%20Agents%20%28CUAs%29%2C%20current%20frameworks%20struggle%20with%20robustness%20in%20long-horizon%20workflows%20and%20generalization%20in%20novel%20domains.%20These%20limitations%20stem%20from%20a%20lack%20of%20granular%20control%20over%20historical%20visual%20context%20curation%20and%20the%20absence%20of%20visual-aware%20tutorial%20retrieval.%20To%20bridge%20these%20gaps%2C%20we%20introduce%20OS-Symphony%2C%20a%20holistic%20framework%20that%20comprises%20an%20Orchestrator%20coordinating%20two%20key%20innovations%20for%20robust%20automation%3A%20%281%29%20a%20Reflection-Memory%20Agent%20that%20utilizes%20milestone-driven%20long-term%20memory%20to%20enable%20trajectory-level%20self-correction%2C%20effectively%20mitigating%20visual%20context%20loss%20in%20long-horizon%20tasks%3B%20%282%29%20Versatile%20Tool%20Agents%20featuring%20a%20Multimodal%20Searcher%20that%20adopts%20a%20SeeAct%20paradigm%20to%20navigate%20a%20browser-based%20sandbox%20to%20synthesize%20live%2C%20visually%20aligned%20tutorials%2C%20thereby%20resolving%20fidelity%20issues%20in%20unseen%20scenarios.%20Experimental%20results%20demonstrate%20that%20OS-Symphony%20delivers%20substantial%20performance%20gains%20across%20varying%20model%20scales%2C%20establishing%20new%20state-of-the-art%20results%20on%20three%20online%20benchmarks%2C%20notably%20achieving%2065.84%25%20on%20OSWorld.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOS-Symphony%253A%2520A%2520Holistic%2520Framework%2520for%2520Robust%2520and%2520Generalist%2520Computer-Using%2520Agent%26entry.906535625%3DBowen%2520Yang%2520and%2520Kaiming%2520Jin%2520and%2520Zhenyu%2520Wu%2520and%2520Zhaoyang%2520Liu%2520and%2520Qiushi%2520Sun%2520and%2520Zehao%2520Li%2520and%2520JingJing%2520Xie%2520and%2520Zhoumianze%2520Liu%2520and%2520Fangzhi%2520Xu%2520and%2520Kanzhi%2520Cheng%2520and%2520Qingyun%2520Li%2520and%2520Yian%2520Wang%2520and%2520Yu%2520Qiao%2520and%2520Zun%2520Wang%2520and%2520Zichen%2520Ding%26entry.1292438233%3DWhile%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520significantly%2520advanced%2520Computer-Using%2520Agents%2520%2528CUAs%2529%252C%2520current%2520frameworks%2520struggle%2520with%2520robustness%2520in%2520long-horizon%2520workflows%2520and%2520generalization%2520in%2520novel%2520domains.%2520These%2520limitations%2520stem%2520from%2520a%2520lack%2520of%2520granular%2520control%2520over%2520historical%2520visual%2520context%2520curation%2520and%2520the%2520absence%2520of%2520visual-aware%2520tutorial%2520retrieval.%2520To%2520bridge%2520these%2520gaps%252C%2520we%2520introduce%2520OS-Symphony%252C%2520a%2520holistic%2520framework%2520that%2520comprises%2520an%2520Orchestrator%2520coordinating%2520two%2520key%2520innovations%2520for%2520robust%2520automation%253A%2520%25281%2529%2520a%2520Reflection-Memory%2520Agent%2520that%2520utilizes%2520milestone-driven%2520long-term%2520memory%2520to%2520enable%2520trajectory-level%2520self-correction%252C%2520effectively%2520mitigating%2520visual%2520context%2520loss%2520in%2520long-horizon%2520tasks%253B%2520%25282%2529%2520Versatile%2520Tool%2520Agents%2520featuring%2520a%2520Multimodal%2520Searcher%2520that%2520adopts%2520a%2520SeeAct%2520paradigm%2520to%2520navigate%2520a%2520browser-based%2520sandbox%2520to%2520synthesize%2520live%252C%2520visually%2520aligned%2520tutorials%252C%2520thereby%2520resolving%2520fidelity%2520issues%2520in%2520unseen%2520scenarios.%2520Experimental%2520results%2520demonstrate%2520that%2520OS-Symphony%2520delivers%2520substantial%2520performance%2520gains%2520across%2520varying%2520model%2520scales%252C%2520establishing%2520new%2520state-of-the-art%2520results%2520on%2520three%2520online%2520benchmarks%252C%2520notably%2520achieving%252065.84%2525%2520on%2520OSWorld.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OS-Symphony%3A%20A%20Holistic%20Framework%20for%20Robust%20and%20Generalist%20Computer-Using%20Agent&entry.906535625=Bowen%20Yang%20and%20Kaiming%20Jin%20and%20Zhenyu%20Wu%20and%20Zhaoyang%20Liu%20and%20Qiushi%20Sun%20and%20Zehao%20Li%20and%20JingJing%20Xie%20and%20Zhoumianze%20Liu%20and%20Fangzhi%20Xu%20and%20Kanzhi%20Cheng%20and%20Qingyun%20Li%20and%20Yian%20Wang%20and%20Yu%20Qiao%20and%20Zun%20Wang%20and%20Zichen%20Ding&entry.1292438233=While%20Vision-Language%20Models%20%28VLMs%29%20have%20significantly%20advanced%20Computer-Using%20Agents%20%28CUAs%29%2C%20current%20frameworks%20struggle%20with%20robustness%20in%20long-horizon%20workflows%20and%20generalization%20in%20novel%20domains.%20These%20limitations%20stem%20from%20a%20lack%20of%20granular%20control%20over%20historical%20visual%20context%20curation%20and%20the%20absence%20of%20visual-aware%20tutorial%20retrieval.%20To%20bridge%20these%20gaps%2C%20we%20introduce%20OS-Symphony%2C%20a%20holistic%20framework%20that%20comprises%20an%20Orchestrator%20coordinating%20two%20key%20innovations%20for%20robust%20automation%3A%20%281%29%20a%20Reflection-Memory%20Agent%20that%20utilizes%20milestone-driven%20long-term%20memory%20to%20enable%20trajectory-level%20self-correction%2C%20effectively%20mitigating%20visual%20context%20loss%20in%20long-horizon%20tasks%3B%20%282%29%20Versatile%20Tool%20Agents%20featuring%20a%20Multimodal%20Searcher%20that%20adopts%20a%20SeeAct%20paradigm%20to%20navigate%20a%20browser-based%20sandbox%20to%20synthesize%20live%2C%20visually%20aligned%20tutorials%2C%20thereby%20resolving%20fidelity%20issues%20in%20unseen%20scenarios.%20Experimental%20results%20demonstrate%20that%20OS-Symphony%20delivers%20substantial%20performance%20gains%20across%20varying%20model%20scales%2C%20establishing%20new%20state-of-the-art%20results%20on%20three%20online%20benchmarks%2C%20notably%20achieving%2065.84%25%20on%20OSWorld.&entry.1838667208=http%3A//arxiv.org/abs/2601.07779v1&entry.124074799=Read"},
{"title": "Anatomy Aware Cascade Network: Bridging Epistemic Uncertainty and Geometric Manifold for 3D Tooth Segmentation", "author": "Bing Yu and Liu Shi and Haitao Wang and Deran Qi and Xiang Cai and Wei Zhong and Qiegen Liu", "abstract": "Accurate three-dimensional (3D) tooth segmentation from Cone-Beam Computed Tomography (CBCT) is a prerequisite for digital dental workflows. However, achieving high-fidelity segmentation remains challenging due to adhesion artifacts in naturally occluded scans, which are caused by low contrast and indistinct inter-arch boundaries. To address these limitations, we propose the Anatomy Aware Cascade Network (AACNet), a coarse-to-fine framework designed to resolve boundary ambiguity while maintaining global structural consistency. Specifically, we introduce two mechanisms: the Ambiguity Gated Boundary Refiner (AGBR) and the Signed Distance Map guided Anatomical Attention (SDMAA). The AGBR employs an entropy based gating mechanism to perform targeted feature rectification in high uncertainty transition zones. Meanwhile, the SDMAA integrates implicit geometric constraints via signed distance map to enforce topological consistency, preventing the loss of spatial details associated with standard pooling. Experimental results on a dataset of 125 CBCT volumes demonstrate that AACNet achieves a Dice Similarity Coefficient of 90.17 \\% and a 95\\% Hausdorff Distance of 3.63 mm, significantly outperforming state-of-the-art methods. Furthermore, the model exhibits strong generalization on an external dataset with an HD95 of 2.19 mm, validating its reliability for downstream clinical applications such as surgical planning. Code for AACNet is available at https://github.com/shiliu0114/AACNet.", "link": "http://arxiv.org/abs/2601.07499v1", "date": "2026-01-12", "relevancy": 2.1712, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5434}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5424}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anatomy%20Aware%20Cascade%20Network%3A%20Bridging%20Epistemic%20Uncertainty%20and%20Geometric%20Manifold%20for%203D%20Tooth%20Segmentation&body=Title%3A%20Anatomy%20Aware%20Cascade%20Network%3A%20Bridging%20Epistemic%20Uncertainty%20and%20Geometric%20Manifold%20for%203D%20Tooth%20Segmentation%0AAuthor%3A%20Bing%20Yu%20and%20Liu%20Shi%20and%20Haitao%20Wang%20and%20Deran%20Qi%20and%20Xiang%20Cai%20and%20Wei%20Zhong%20and%20Qiegen%20Liu%0AAbstract%3A%20Accurate%20three-dimensional%20%283D%29%20tooth%20segmentation%20from%20Cone-Beam%20Computed%20Tomography%20%28CBCT%29%20is%20a%20prerequisite%20for%20digital%20dental%20workflows.%20However%2C%20achieving%20high-fidelity%20segmentation%20remains%20challenging%20due%20to%20adhesion%20artifacts%20in%20naturally%20occluded%20scans%2C%20which%20are%20caused%20by%20low%20contrast%20and%20indistinct%20inter-arch%20boundaries.%20To%20address%20these%20limitations%2C%20we%20propose%20the%20Anatomy%20Aware%20Cascade%20Network%20%28AACNet%29%2C%20a%20coarse-to-fine%20framework%20designed%20to%20resolve%20boundary%20ambiguity%20while%20maintaining%20global%20structural%20consistency.%20Specifically%2C%20we%20introduce%20two%20mechanisms%3A%20the%20Ambiguity%20Gated%20Boundary%20Refiner%20%28AGBR%29%20and%20the%20Signed%20Distance%20Map%20guided%20Anatomical%20Attention%20%28SDMAA%29.%20The%20AGBR%20employs%20an%20entropy%20based%20gating%20mechanism%20to%20perform%20targeted%20feature%20rectification%20in%20high%20uncertainty%20transition%20zones.%20Meanwhile%2C%20the%20SDMAA%20integrates%20implicit%20geometric%20constraints%20via%20signed%20distance%20map%20to%20enforce%20topological%20consistency%2C%20preventing%20the%20loss%20of%20spatial%20details%20associated%20with%20standard%20pooling.%20Experimental%20results%20on%20a%20dataset%20of%20125%20CBCT%20volumes%20demonstrate%20that%20AACNet%20achieves%20a%20Dice%20Similarity%20Coefficient%20of%2090.17%20%5C%25%20and%20a%2095%5C%25%20Hausdorff%20Distance%20of%203.63%20mm%2C%20significantly%20outperforming%20state-of-the-art%20methods.%20Furthermore%2C%20the%20model%20exhibits%20strong%20generalization%20on%20an%20external%20dataset%20with%20an%20HD95%20of%202.19%20mm%2C%20validating%20its%20reliability%20for%20downstream%20clinical%20applications%20such%20as%20surgical%20planning.%20Code%20for%20AACNet%20is%20available%20at%20https%3A//github.com/shiliu0114/AACNet.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnatomy%2520Aware%2520Cascade%2520Network%253A%2520Bridging%2520Epistemic%2520Uncertainty%2520and%2520Geometric%2520Manifold%2520for%25203D%2520Tooth%2520Segmentation%26entry.906535625%3DBing%2520Yu%2520and%2520Liu%2520Shi%2520and%2520Haitao%2520Wang%2520and%2520Deran%2520Qi%2520and%2520Xiang%2520Cai%2520and%2520Wei%2520Zhong%2520and%2520Qiegen%2520Liu%26entry.1292438233%3DAccurate%2520three-dimensional%2520%25283D%2529%2520tooth%2520segmentation%2520from%2520Cone-Beam%2520Computed%2520Tomography%2520%2528CBCT%2529%2520is%2520a%2520prerequisite%2520for%2520digital%2520dental%2520workflows.%2520However%252C%2520achieving%2520high-fidelity%2520segmentation%2520remains%2520challenging%2520due%2520to%2520adhesion%2520artifacts%2520in%2520naturally%2520occluded%2520scans%252C%2520which%2520are%2520caused%2520by%2520low%2520contrast%2520and%2520indistinct%2520inter-arch%2520boundaries.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520the%2520Anatomy%2520Aware%2520Cascade%2520Network%2520%2528AACNet%2529%252C%2520a%2520coarse-to-fine%2520framework%2520designed%2520to%2520resolve%2520boundary%2520ambiguity%2520while%2520maintaining%2520global%2520structural%2520consistency.%2520Specifically%252C%2520we%2520introduce%2520two%2520mechanisms%253A%2520the%2520Ambiguity%2520Gated%2520Boundary%2520Refiner%2520%2528AGBR%2529%2520and%2520the%2520Signed%2520Distance%2520Map%2520guided%2520Anatomical%2520Attention%2520%2528SDMAA%2529.%2520The%2520AGBR%2520employs%2520an%2520entropy%2520based%2520gating%2520mechanism%2520to%2520perform%2520targeted%2520feature%2520rectification%2520in%2520high%2520uncertainty%2520transition%2520zones.%2520Meanwhile%252C%2520the%2520SDMAA%2520integrates%2520implicit%2520geometric%2520constraints%2520via%2520signed%2520distance%2520map%2520to%2520enforce%2520topological%2520consistency%252C%2520preventing%2520the%2520loss%2520of%2520spatial%2520details%2520associated%2520with%2520standard%2520pooling.%2520Experimental%2520results%2520on%2520a%2520dataset%2520of%2520125%2520CBCT%2520volumes%2520demonstrate%2520that%2520AACNet%2520achieves%2520a%2520Dice%2520Similarity%2520Coefficient%2520of%252090.17%2520%255C%2525%2520and%2520a%252095%255C%2525%2520Hausdorff%2520Distance%2520of%25203.63%2520mm%252C%2520significantly%2520outperforming%2520state-of-the-art%2520methods.%2520Furthermore%252C%2520the%2520model%2520exhibits%2520strong%2520generalization%2520on%2520an%2520external%2520dataset%2520with%2520an%2520HD95%2520of%25202.19%2520mm%252C%2520validating%2520its%2520reliability%2520for%2520downstream%2520clinical%2520applications%2520such%2520as%2520surgical%2520planning.%2520Code%2520for%2520AACNet%2520is%2520available%2520at%2520https%253A//github.com/shiliu0114/AACNet.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anatomy%20Aware%20Cascade%20Network%3A%20Bridging%20Epistemic%20Uncertainty%20and%20Geometric%20Manifold%20for%203D%20Tooth%20Segmentation&entry.906535625=Bing%20Yu%20and%20Liu%20Shi%20and%20Haitao%20Wang%20and%20Deran%20Qi%20and%20Xiang%20Cai%20and%20Wei%20Zhong%20and%20Qiegen%20Liu&entry.1292438233=Accurate%20three-dimensional%20%283D%29%20tooth%20segmentation%20from%20Cone-Beam%20Computed%20Tomography%20%28CBCT%29%20is%20a%20prerequisite%20for%20digital%20dental%20workflows.%20However%2C%20achieving%20high-fidelity%20segmentation%20remains%20challenging%20due%20to%20adhesion%20artifacts%20in%20naturally%20occluded%20scans%2C%20which%20are%20caused%20by%20low%20contrast%20and%20indistinct%20inter-arch%20boundaries.%20To%20address%20these%20limitations%2C%20we%20propose%20the%20Anatomy%20Aware%20Cascade%20Network%20%28AACNet%29%2C%20a%20coarse-to-fine%20framework%20designed%20to%20resolve%20boundary%20ambiguity%20while%20maintaining%20global%20structural%20consistency.%20Specifically%2C%20we%20introduce%20two%20mechanisms%3A%20the%20Ambiguity%20Gated%20Boundary%20Refiner%20%28AGBR%29%20and%20the%20Signed%20Distance%20Map%20guided%20Anatomical%20Attention%20%28SDMAA%29.%20The%20AGBR%20employs%20an%20entropy%20based%20gating%20mechanism%20to%20perform%20targeted%20feature%20rectification%20in%20high%20uncertainty%20transition%20zones.%20Meanwhile%2C%20the%20SDMAA%20integrates%20implicit%20geometric%20constraints%20via%20signed%20distance%20map%20to%20enforce%20topological%20consistency%2C%20preventing%20the%20loss%20of%20spatial%20details%20associated%20with%20standard%20pooling.%20Experimental%20results%20on%20a%20dataset%20of%20125%20CBCT%20volumes%20demonstrate%20that%20AACNet%20achieves%20a%20Dice%20Similarity%20Coefficient%20of%2090.17%20%5C%25%20and%20a%2095%5C%25%20Hausdorff%20Distance%20of%203.63%20mm%2C%20significantly%20outperforming%20state-of-the-art%20methods.%20Furthermore%2C%20the%20model%20exhibits%20strong%20generalization%20on%20an%20external%20dataset%20with%20an%20HD95%20of%202.19%20mm%2C%20validating%20its%20reliability%20for%20downstream%20clinical%20applications%20such%20as%20surgical%20planning.%20Code%20for%20AACNet%20is%20available%20at%20https%3A//github.com/shiliu0114/AACNet.&entry.1838667208=http%3A//arxiv.org/abs/2601.07499v1&entry.124074799=Read"},
{"title": "Multi-scale Graph Autoregressive Modeling: Molecular Property Prediction via Next Token Prediction", "author": "Zhuoyang Jiang and Yaosen Min and Peiran Jin and Lei Chen", "abstract": "We present Connection-Aware Motif Sequencing (CamS), a graph-to-sequence representation that enables decoder-only Transformers to learn molecular graphs via standard next-token prediction (NTP). For molecular property prediction, SMILES-based NTP scales well but lacks explicit topology, whereas graph-native masked modeling captures connectivity but risks disrupting the pivotal chemical details (e.g., activity cliffs). CamS bridges this gap by serializing molecular graphs into structure-rich causal sequences. CamS first mines data-driven connection-aware motifs. It then serializes motifs via scaffold-rooted breadth-first search (BFS) to establish a stable core-to-periphery order. Crucially, CamS enables hierarchical modeling by concatenating sequences from fine to coarse motif scales, allowing the model to condition global scaffolds on dense, uncorrupted local structural evidence. We instantiate CamS-LLaMA by pre-training a vanilla LLaMA backbone on CamS sequences. It achieves state-of-the-art performance on MoleculeNet and the activity-cliff benchmark MoleculeACE, outperforming both SMILES-based language models and strong graph baselines. Interpretability analysis confirms that our multi-scale causal serialization effectively drives attention toward cliff-determining differences.", "link": "http://arxiv.org/abs/2601.02530v2", "date": "2026-01-12", "relevancy": 2.1533, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5701}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-scale%20Graph%20Autoregressive%20Modeling%3A%20Molecular%20Property%20Prediction%20via%20Next%20Token%20Prediction&body=Title%3A%20Multi-scale%20Graph%20Autoregressive%20Modeling%3A%20Molecular%20Property%20Prediction%20via%20Next%20Token%20Prediction%0AAuthor%3A%20Zhuoyang%20Jiang%20and%20Yaosen%20Min%20and%20Peiran%20Jin%20and%20Lei%20Chen%0AAbstract%3A%20We%20present%20Connection-Aware%20Motif%20Sequencing%20%28CamS%29%2C%20a%20graph-to-sequence%20representation%20that%20enables%20decoder-only%20Transformers%20to%20learn%20molecular%20graphs%20via%20standard%20next-token%20prediction%20%28NTP%29.%20For%20molecular%20property%20prediction%2C%20SMILES-based%20NTP%20scales%20well%20but%20lacks%20explicit%20topology%2C%20whereas%20graph-native%20masked%20modeling%20captures%20connectivity%20but%20risks%20disrupting%20the%20pivotal%20chemical%20details%20%28e.g.%2C%20activity%20cliffs%29.%20CamS%20bridges%20this%20gap%20by%20serializing%20molecular%20graphs%20into%20structure-rich%20causal%20sequences.%20CamS%20first%20mines%20data-driven%20connection-aware%20motifs.%20It%20then%20serializes%20motifs%20via%20scaffold-rooted%20breadth-first%20search%20%28BFS%29%20to%20establish%20a%20stable%20core-to-periphery%20order.%20Crucially%2C%20CamS%20enables%20hierarchical%20modeling%20by%20concatenating%20sequences%20from%20fine%20to%20coarse%20motif%20scales%2C%20allowing%20the%20model%20to%20condition%20global%20scaffolds%20on%20dense%2C%20uncorrupted%20local%20structural%20evidence.%20We%20instantiate%20CamS-LLaMA%20by%20pre-training%20a%20vanilla%20LLaMA%20backbone%20on%20CamS%20sequences.%20It%20achieves%20state-of-the-art%20performance%20on%20MoleculeNet%20and%20the%20activity-cliff%20benchmark%20MoleculeACE%2C%20outperforming%20both%20SMILES-based%20language%20models%20and%20strong%20graph%20baselines.%20Interpretability%20analysis%20confirms%20that%20our%20multi-scale%20causal%20serialization%20effectively%20drives%20attention%20toward%20cliff-determining%20differences.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02530v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-scale%2520Graph%2520Autoregressive%2520Modeling%253A%2520Molecular%2520Property%2520Prediction%2520via%2520Next%2520Token%2520Prediction%26entry.906535625%3DZhuoyang%2520Jiang%2520and%2520Yaosen%2520Min%2520and%2520Peiran%2520Jin%2520and%2520Lei%2520Chen%26entry.1292438233%3DWe%2520present%2520Connection-Aware%2520Motif%2520Sequencing%2520%2528CamS%2529%252C%2520a%2520graph-to-sequence%2520representation%2520that%2520enables%2520decoder-only%2520Transformers%2520to%2520learn%2520molecular%2520graphs%2520via%2520standard%2520next-token%2520prediction%2520%2528NTP%2529.%2520For%2520molecular%2520property%2520prediction%252C%2520SMILES-based%2520NTP%2520scales%2520well%2520but%2520lacks%2520explicit%2520topology%252C%2520whereas%2520graph-native%2520masked%2520modeling%2520captures%2520connectivity%2520but%2520risks%2520disrupting%2520the%2520pivotal%2520chemical%2520details%2520%2528e.g.%252C%2520activity%2520cliffs%2529.%2520CamS%2520bridges%2520this%2520gap%2520by%2520serializing%2520molecular%2520graphs%2520into%2520structure-rich%2520causal%2520sequences.%2520CamS%2520first%2520mines%2520data-driven%2520connection-aware%2520motifs.%2520It%2520then%2520serializes%2520motifs%2520via%2520scaffold-rooted%2520breadth-first%2520search%2520%2528BFS%2529%2520to%2520establish%2520a%2520stable%2520core-to-periphery%2520order.%2520Crucially%252C%2520CamS%2520enables%2520hierarchical%2520modeling%2520by%2520concatenating%2520sequences%2520from%2520fine%2520to%2520coarse%2520motif%2520scales%252C%2520allowing%2520the%2520model%2520to%2520condition%2520global%2520scaffolds%2520on%2520dense%252C%2520uncorrupted%2520local%2520structural%2520evidence.%2520We%2520instantiate%2520CamS-LLaMA%2520by%2520pre-training%2520a%2520vanilla%2520LLaMA%2520backbone%2520on%2520CamS%2520sequences.%2520It%2520achieves%2520state-of-the-art%2520performance%2520on%2520MoleculeNet%2520and%2520the%2520activity-cliff%2520benchmark%2520MoleculeACE%252C%2520outperforming%2520both%2520SMILES-based%2520language%2520models%2520and%2520strong%2520graph%2520baselines.%2520Interpretability%2520analysis%2520confirms%2520that%2520our%2520multi-scale%2520causal%2520serialization%2520effectively%2520drives%2520attention%2520toward%2520cliff-determining%2520differences.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02530v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-scale%20Graph%20Autoregressive%20Modeling%3A%20Molecular%20Property%20Prediction%20via%20Next%20Token%20Prediction&entry.906535625=Zhuoyang%20Jiang%20and%20Yaosen%20Min%20and%20Peiran%20Jin%20and%20Lei%20Chen&entry.1292438233=We%20present%20Connection-Aware%20Motif%20Sequencing%20%28CamS%29%2C%20a%20graph-to-sequence%20representation%20that%20enables%20decoder-only%20Transformers%20to%20learn%20molecular%20graphs%20via%20standard%20next-token%20prediction%20%28NTP%29.%20For%20molecular%20property%20prediction%2C%20SMILES-based%20NTP%20scales%20well%20but%20lacks%20explicit%20topology%2C%20whereas%20graph-native%20masked%20modeling%20captures%20connectivity%20but%20risks%20disrupting%20the%20pivotal%20chemical%20details%20%28e.g.%2C%20activity%20cliffs%29.%20CamS%20bridges%20this%20gap%20by%20serializing%20molecular%20graphs%20into%20structure-rich%20causal%20sequences.%20CamS%20first%20mines%20data-driven%20connection-aware%20motifs.%20It%20then%20serializes%20motifs%20via%20scaffold-rooted%20breadth-first%20search%20%28BFS%29%20to%20establish%20a%20stable%20core-to-periphery%20order.%20Crucially%2C%20CamS%20enables%20hierarchical%20modeling%20by%20concatenating%20sequences%20from%20fine%20to%20coarse%20motif%20scales%2C%20allowing%20the%20model%20to%20condition%20global%20scaffolds%20on%20dense%2C%20uncorrupted%20local%20structural%20evidence.%20We%20instantiate%20CamS-LLaMA%20by%20pre-training%20a%20vanilla%20LLaMA%20backbone%20on%20CamS%20sequences.%20It%20achieves%20state-of-the-art%20performance%20on%20MoleculeNet%20and%20the%20activity-cliff%20benchmark%20MoleculeACE%2C%20outperforming%20both%20SMILES-based%20language%20models%20and%20strong%20graph%20baselines.%20Interpretability%20analysis%20confirms%20that%20our%20multi-scale%20causal%20serialization%20effectively%20drives%20attention%20toward%20cliff-determining%20differences.&entry.1838667208=http%3A//arxiv.org/abs/2601.02530v2&entry.124074799=Read"},
{"title": "Improving Video Question Answering through query-based frame selection", "author": "Himanshu Patil and Geo Jolly and Ramana Raja Buddala and Ganesh Ramakrishnan and Rohit Saluja", "abstract": "Video Question Answering (VideoQA) models enhance understanding and interaction with audiovisual content, making it more accessible, searchable, and useful for a wide range of fields such as education, surveillance, entertainment, and content creation. Due to heavy compute requirements, most large visual language models (VLMs) for VideoQA rely on a fixed number of frames by uniformly sampling the video. However, this process does not pick important frames or capture the context of the video. We present a novel query-based selection of frames relevant to the questions based on the submodular mutual Information (SMI) functions. By replacing uniform frame sampling with query-based selection, our method ensures that the chosen frames provide complementary and essential visual information for accurate VideoQA. We evaluate our approach on the MVBench dataset, which spans a diverse set of multi-action video tasks. VideoQA accuracy on this dataset was assessed using two VLMs, namely Video-LLaVA and LLaVA-NeXT, both of which originally employed uniform frame sampling. Experiments were conducted using both uniform and query-based sampling strategies. An accuracy improvement of up to \\textbf{4\\%} was observed when using query-based frame selection over uniform sampling. Qualitative analysis further highlights that query-based selection, using SMI functions, consistently picks frames better aligned with the question. We opine that such query-based frame selection can enhance accuracy in a wide range of tasks that rely on only a subset of video frames.", "link": "http://arxiv.org/abs/2601.07459v1", "date": "2026-01-12", "relevancy": 2.1478, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5362}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Video%20Question%20Answering%20through%20query-based%20frame%20selection&body=Title%3A%20Improving%20Video%20Question%20Answering%20through%20query-based%20frame%20selection%0AAuthor%3A%20Himanshu%20Patil%20and%20Geo%20Jolly%20and%20Ramana%20Raja%20Buddala%20and%20Ganesh%20Ramakrishnan%20and%20Rohit%20Saluja%0AAbstract%3A%20Video%20Question%20Answering%20%28VideoQA%29%20models%20enhance%20understanding%20and%20interaction%20with%20audiovisual%20content%2C%20making%20it%20more%20accessible%2C%20searchable%2C%20and%20useful%20for%20a%20wide%20range%20of%20fields%20such%20as%20education%2C%20surveillance%2C%20entertainment%2C%20and%20content%20creation.%20Due%20to%20heavy%20compute%20requirements%2C%20most%20large%20visual%20language%20models%20%28VLMs%29%20for%20VideoQA%20rely%20on%20a%20fixed%20number%20of%20frames%20by%20uniformly%20sampling%20the%20video.%20However%2C%20this%20process%20does%20not%20pick%20important%20frames%20or%20capture%20the%20context%20of%20the%20video.%20We%20present%20a%20novel%20query-based%20selection%20of%20frames%20relevant%20to%20the%20questions%20based%20on%20the%20submodular%20mutual%20Information%20%28SMI%29%20functions.%20By%20replacing%20uniform%20frame%20sampling%20with%20query-based%20selection%2C%20our%20method%20ensures%20that%20the%20chosen%20frames%20provide%20complementary%20and%20essential%20visual%20information%20for%20accurate%20VideoQA.%20We%20evaluate%20our%20approach%20on%20the%20MVBench%20dataset%2C%20which%20spans%20a%20diverse%20set%20of%20multi-action%20video%20tasks.%20VideoQA%20accuracy%20on%20this%20dataset%20was%20assessed%20using%20two%20VLMs%2C%20namely%20Video-LLaVA%20and%20LLaVA-NeXT%2C%20both%20of%20which%20originally%20employed%20uniform%20frame%20sampling.%20Experiments%20were%20conducted%20using%20both%20uniform%20and%20query-based%20sampling%20strategies.%20An%20accuracy%20improvement%20of%20up%20to%20%5Ctextbf%7B4%5C%25%7D%20was%20observed%20when%20using%20query-based%20frame%20selection%20over%20uniform%20sampling.%20Qualitative%20analysis%20further%20highlights%20that%20query-based%20selection%2C%20using%20SMI%20functions%2C%20consistently%20picks%20frames%20better%20aligned%20with%20the%20question.%20We%20opine%20that%20such%20query-based%20frame%20selection%20can%20enhance%20accuracy%20in%20a%20wide%20range%20of%20tasks%20that%20rely%20on%20only%20a%20subset%20of%20video%20frames.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Video%2520Question%2520Answering%2520through%2520query-based%2520frame%2520selection%26entry.906535625%3DHimanshu%2520Patil%2520and%2520Geo%2520Jolly%2520and%2520Ramana%2520Raja%2520Buddala%2520and%2520Ganesh%2520Ramakrishnan%2520and%2520Rohit%2520Saluja%26entry.1292438233%3DVideo%2520Question%2520Answering%2520%2528VideoQA%2529%2520models%2520enhance%2520understanding%2520and%2520interaction%2520with%2520audiovisual%2520content%252C%2520making%2520it%2520more%2520accessible%252C%2520searchable%252C%2520and%2520useful%2520for%2520a%2520wide%2520range%2520of%2520fields%2520such%2520as%2520education%252C%2520surveillance%252C%2520entertainment%252C%2520and%2520content%2520creation.%2520Due%2520to%2520heavy%2520compute%2520requirements%252C%2520most%2520large%2520visual%2520language%2520models%2520%2528VLMs%2529%2520for%2520VideoQA%2520rely%2520on%2520a%2520fixed%2520number%2520of%2520frames%2520by%2520uniformly%2520sampling%2520the%2520video.%2520However%252C%2520this%2520process%2520does%2520not%2520pick%2520important%2520frames%2520or%2520capture%2520the%2520context%2520of%2520the%2520video.%2520We%2520present%2520a%2520novel%2520query-based%2520selection%2520of%2520frames%2520relevant%2520to%2520the%2520questions%2520based%2520on%2520the%2520submodular%2520mutual%2520Information%2520%2528SMI%2529%2520functions.%2520By%2520replacing%2520uniform%2520frame%2520sampling%2520with%2520query-based%2520selection%252C%2520our%2520method%2520ensures%2520that%2520the%2520chosen%2520frames%2520provide%2520complementary%2520and%2520essential%2520visual%2520information%2520for%2520accurate%2520VideoQA.%2520We%2520evaluate%2520our%2520approach%2520on%2520the%2520MVBench%2520dataset%252C%2520which%2520spans%2520a%2520diverse%2520set%2520of%2520multi-action%2520video%2520tasks.%2520VideoQA%2520accuracy%2520on%2520this%2520dataset%2520was%2520assessed%2520using%2520two%2520VLMs%252C%2520namely%2520Video-LLaVA%2520and%2520LLaVA-NeXT%252C%2520both%2520of%2520which%2520originally%2520employed%2520uniform%2520frame%2520sampling.%2520Experiments%2520were%2520conducted%2520using%2520both%2520uniform%2520and%2520query-based%2520sampling%2520strategies.%2520An%2520accuracy%2520improvement%2520of%2520up%2520to%2520%255Ctextbf%257B4%255C%2525%257D%2520was%2520observed%2520when%2520using%2520query-based%2520frame%2520selection%2520over%2520uniform%2520sampling.%2520Qualitative%2520analysis%2520further%2520highlights%2520that%2520query-based%2520selection%252C%2520using%2520SMI%2520functions%252C%2520consistently%2520picks%2520frames%2520better%2520aligned%2520with%2520the%2520question.%2520We%2520opine%2520that%2520such%2520query-based%2520frame%2520selection%2520can%2520enhance%2520accuracy%2520in%2520a%2520wide%2520range%2520of%2520tasks%2520that%2520rely%2520on%2520only%2520a%2520subset%2520of%2520video%2520frames.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Video%20Question%20Answering%20through%20query-based%20frame%20selection&entry.906535625=Himanshu%20Patil%20and%20Geo%20Jolly%20and%20Ramana%20Raja%20Buddala%20and%20Ganesh%20Ramakrishnan%20and%20Rohit%20Saluja&entry.1292438233=Video%20Question%20Answering%20%28VideoQA%29%20models%20enhance%20understanding%20and%20interaction%20with%20audiovisual%20content%2C%20making%20it%20more%20accessible%2C%20searchable%2C%20and%20useful%20for%20a%20wide%20range%20of%20fields%20such%20as%20education%2C%20surveillance%2C%20entertainment%2C%20and%20content%20creation.%20Due%20to%20heavy%20compute%20requirements%2C%20most%20large%20visual%20language%20models%20%28VLMs%29%20for%20VideoQA%20rely%20on%20a%20fixed%20number%20of%20frames%20by%20uniformly%20sampling%20the%20video.%20However%2C%20this%20process%20does%20not%20pick%20important%20frames%20or%20capture%20the%20context%20of%20the%20video.%20We%20present%20a%20novel%20query-based%20selection%20of%20frames%20relevant%20to%20the%20questions%20based%20on%20the%20submodular%20mutual%20Information%20%28SMI%29%20functions.%20By%20replacing%20uniform%20frame%20sampling%20with%20query-based%20selection%2C%20our%20method%20ensures%20that%20the%20chosen%20frames%20provide%20complementary%20and%20essential%20visual%20information%20for%20accurate%20VideoQA.%20We%20evaluate%20our%20approach%20on%20the%20MVBench%20dataset%2C%20which%20spans%20a%20diverse%20set%20of%20multi-action%20video%20tasks.%20VideoQA%20accuracy%20on%20this%20dataset%20was%20assessed%20using%20two%20VLMs%2C%20namely%20Video-LLaVA%20and%20LLaVA-NeXT%2C%20both%20of%20which%20originally%20employed%20uniform%20frame%20sampling.%20Experiments%20were%20conducted%20using%20both%20uniform%20and%20query-based%20sampling%20strategies.%20An%20accuracy%20improvement%20of%20up%20to%20%5Ctextbf%7B4%5C%25%7D%20was%20observed%20when%20using%20query-based%20frame%20selection%20over%20uniform%20sampling.%20Qualitative%20analysis%20further%20highlights%20that%20query-based%20selection%2C%20using%20SMI%20functions%2C%20consistently%20picks%20frames%20better%20aligned%20with%20the%20question.%20We%20opine%20that%20such%20query-based%20frame%20selection%20can%20enhance%20accuracy%20in%20a%20wide%20range%20of%20tasks%20that%20rely%20on%20only%20a%20subset%20of%20video%20frames.&entry.1838667208=http%3A//arxiv.org/abs/2601.07459v1&entry.124074799=Read"},
{"title": "Optimizing the Design of a Simple Three-Sphere Magnetic Microswimmer", "author": "Theo Lequy and Andreas M. Menzel", "abstract": "When swimming at low Reynolds numbers, inertial effects are negligible and reciprocal movements cannot induce net motion. Instead, symmetry breaking is necessary to achieve net propulsion. Directed swimming can be supported by magnetic fields, which simultaneously provide a versatile means of remote actuation. Thus, we analyze the motion of a straight microswimmer composed of three magnetizable beads connected by two elastic links. The swimming mechanism is based on oriented external magnetic fields that oscillate in magnitude. Through induced reversible hysteretic collapse of the two segments of the swimmer, the two pairs of beads jump into contact and separate nonreciprocally. Due to higher-order hydrodynamic interactions, net displacement results after each cycle. Different microswimmers can be tuned to different driving amplitudes and frequencies, allowing for simultaneous independent control by just one external magnetic field. The swimmer geometry and magnetic field shape are optimized for maximum swimming speed using an evolutionary optimization strategy. Thanks to the simple working principle, an experimental realization of such a microrobot seems feasible and may open new approaches for microinvasive medical interventions such as targeted drug delivery.", "link": "http://arxiv.org/abs/2601.07370v1", "date": "2026-01-12", "relevancy": 2.1467, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4679}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4106}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20the%20Design%20of%20a%20Simple%20Three-Sphere%20Magnetic%20Microswimmer&body=Title%3A%20Optimizing%20the%20Design%20of%20a%20Simple%20Three-Sphere%20Magnetic%20Microswimmer%0AAuthor%3A%20Theo%20Lequy%20and%20Andreas%20M.%20Menzel%0AAbstract%3A%20When%20swimming%20at%20low%20Reynolds%20numbers%2C%20inertial%20effects%20are%20negligible%20and%20reciprocal%20movements%20cannot%20induce%20net%20motion.%20Instead%2C%20symmetry%20breaking%20is%20necessary%20to%20achieve%20net%20propulsion.%20Directed%20swimming%20can%20be%20supported%20by%20magnetic%20fields%2C%20which%20simultaneously%20provide%20a%20versatile%20means%20of%20remote%20actuation.%20Thus%2C%20we%20analyze%20the%20motion%20of%20a%20straight%20microswimmer%20composed%20of%20three%20magnetizable%20beads%20connected%20by%20two%20elastic%20links.%20The%20swimming%20mechanism%20is%20based%20on%20oriented%20external%20magnetic%20fields%20that%20oscillate%20in%20magnitude.%20Through%20induced%20reversible%20hysteretic%20collapse%20of%20the%20two%20segments%20of%20the%20swimmer%2C%20the%20two%20pairs%20of%20beads%20jump%20into%20contact%20and%20separate%20nonreciprocally.%20Due%20to%20higher-order%20hydrodynamic%20interactions%2C%20net%20displacement%20results%20after%20each%20cycle.%20Different%20microswimmers%20can%20be%20tuned%20to%20different%20driving%20amplitudes%20and%20frequencies%2C%20allowing%20for%20simultaneous%20independent%20control%20by%20just%20one%20external%20magnetic%20field.%20The%20swimmer%20geometry%20and%20magnetic%20field%20shape%20are%20optimized%20for%20maximum%20swimming%20speed%20using%20an%20evolutionary%20optimization%20strategy.%20Thanks%20to%20the%20simple%20working%20principle%2C%20an%20experimental%20realization%20of%20such%20a%20microrobot%20seems%20feasible%20and%20may%20open%20new%20approaches%20for%20microinvasive%20medical%20interventions%20such%20as%20targeted%20drug%20delivery.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520the%2520Design%2520of%2520a%2520Simple%2520Three-Sphere%2520Magnetic%2520Microswimmer%26entry.906535625%3DTheo%2520Lequy%2520and%2520Andreas%2520M.%2520Menzel%26entry.1292438233%3DWhen%2520swimming%2520at%2520low%2520Reynolds%2520numbers%252C%2520inertial%2520effects%2520are%2520negligible%2520and%2520reciprocal%2520movements%2520cannot%2520induce%2520net%2520motion.%2520Instead%252C%2520symmetry%2520breaking%2520is%2520necessary%2520to%2520achieve%2520net%2520propulsion.%2520Directed%2520swimming%2520can%2520be%2520supported%2520by%2520magnetic%2520fields%252C%2520which%2520simultaneously%2520provide%2520a%2520versatile%2520means%2520of%2520remote%2520actuation.%2520Thus%252C%2520we%2520analyze%2520the%2520motion%2520of%2520a%2520straight%2520microswimmer%2520composed%2520of%2520three%2520magnetizable%2520beads%2520connected%2520by%2520two%2520elastic%2520links.%2520The%2520swimming%2520mechanism%2520is%2520based%2520on%2520oriented%2520external%2520magnetic%2520fields%2520that%2520oscillate%2520in%2520magnitude.%2520Through%2520induced%2520reversible%2520hysteretic%2520collapse%2520of%2520the%2520two%2520segments%2520of%2520the%2520swimmer%252C%2520the%2520two%2520pairs%2520of%2520beads%2520jump%2520into%2520contact%2520and%2520separate%2520nonreciprocally.%2520Due%2520to%2520higher-order%2520hydrodynamic%2520interactions%252C%2520net%2520displacement%2520results%2520after%2520each%2520cycle.%2520Different%2520microswimmers%2520can%2520be%2520tuned%2520to%2520different%2520driving%2520amplitudes%2520and%2520frequencies%252C%2520allowing%2520for%2520simultaneous%2520independent%2520control%2520by%2520just%2520one%2520external%2520magnetic%2520field.%2520The%2520swimmer%2520geometry%2520and%2520magnetic%2520field%2520shape%2520are%2520optimized%2520for%2520maximum%2520swimming%2520speed%2520using%2520an%2520evolutionary%2520optimization%2520strategy.%2520Thanks%2520to%2520the%2520simple%2520working%2520principle%252C%2520an%2520experimental%2520realization%2520of%2520such%2520a%2520microrobot%2520seems%2520feasible%2520and%2520may%2520open%2520new%2520approaches%2520for%2520microinvasive%2520medical%2520interventions%2520such%2520as%2520targeted%2520drug%2520delivery.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20the%20Design%20of%20a%20Simple%20Three-Sphere%20Magnetic%20Microswimmer&entry.906535625=Theo%20Lequy%20and%20Andreas%20M.%20Menzel&entry.1292438233=When%20swimming%20at%20low%20Reynolds%20numbers%2C%20inertial%20effects%20are%20negligible%20and%20reciprocal%20movements%20cannot%20induce%20net%20motion.%20Instead%2C%20symmetry%20breaking%20is%20necessary%20to%20achieve%20net%20propulsion.%20Directed%20swimming%20can%20be%20supported%20by%20magnetic%20fields%2C%20which%20simultaneously%20provide%20a%20versatile%20means%20of%20remote%20actuation.%20Thus%2C%20we%20analyze%20the%20motion%20of%20a%20straight%20microswimmer%20composed%20of%20three%20magnetizable%20beads%20connected%20by%20two%20elastic%20links.%20The%20swimming%20mechanism%20is%20based%20on%20oriented%20external%20magnetic%20fields%20that%20oscillate%20in%20magnitude.%20Through%20induced%20reversible%20hysteretic%20collapse%20of%20the%20two%20segments%20of%20the%20swimmer%2C%20the%20two%20pairs%20of%20beads%20jump%20into%20contact%20and%20separate%20nonreciprocally.%20Due%20to%20higher-order%20hydrodynamic%20interactions%2C%20net%20displacement%20results%20after%20each%20cycle.%20Different%20microswimmers%20can%20be%20tuned%20to%20different%20driving%20amplitudes%20and%20frequencies%2C%20allowing%20for%20simultaneous%20independent%20control%20by%20just%20one%20external%20magnetic%20field.%20The%20swimmer%20geometry%20and%20magnetic%20field%20shape%20are%20optimized%20for%20maximum%20swimming%20speed%20using%20an%20evolutionary%20optimization%20strategy.%20Thanks%20to%20the%20simple%20working%20principle%2C%20an%20experimental%20realization%20of%20such%20a%20microrobot%20seems%20feasible%20and%20may%20open%20new%20approaches%20for%20microinvasive%20medical%20interventions%20such%20as%20targeted%20drug%20delivery.&entry.1838667208=http%3A//arxiv.org/abs/2601.07370v1&entry.124074799=Read"},
{"title": "Learning from Reasoning Failures via Synthetic Data Generation", "author": "Gabriela Ben Melech Stan and Estelle Aflalo and Avinash Madasu and Vasudev Lal and Phillip Howard", "abstract": "Training models on synthetic data has emerged as an increasingly important strategy for improving the performance of generative AI. This approach is particularly helpful for large multimodal models (LMMs) due to the relative scarcity of high-quality paired image-text data compared to language-only data. While a variety of methods have been proposed for generating large multimodal datasets, they do not tailor the synthetic data to address specific deficiencies in the reasoning abilities of LMMs which will be trained with the generated dataset. In contrast, humans often learn in a more efficient manner by seeking out examples related to the types of reasoning where they have failed previously. Inspired by this observation, we propose a new approach for synthetic data generation which is grounded in the analysis of an existing LMM's reasoning failures. Our methodology leverages frontier models to automatically analyze errors produced by a weaker LMM and propose new examples which can be used to correct the reasoning failure via additional training, which are then further filtered to ensure high quality. We generate a large multimodal instruction tuning dataset containing over 553k examples using our approach and conduct extensive experiments demonstrating its utility for improving the performance of LMMs on multiple downstream tasks. Our results show that models trained on our synthetic data can even exceed the performance of LMMs trained on an equivalent amount of additional real data, demonstrating the high value of generating synthetic data targeted to specific reasoning failure modes in LMMs. We will make our dataset and code publicly available.", "link": "http://arxiv.org/abs/2504.14523v2", "date": "2026-01-12", "relevancy": 2.1364, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5504}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5292}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Reasoning%20Failures%20via%20Synthetic%20Data%20Generation&body=Title%3A%20Learning%20from%20Reasoning%20Failures%20via%20Synthetic%20Data%20Generation%0AAuthor%3A%20Gabriela%20Ben%20Melech%20Stan%20and%20Estelle%20Aflalo%20and%20Avinash%20Madasu%20and%20Vasudev%20Lal%20and%20Phillip%20Howard%0AAbstract%3A%20Training%20models%20on%20synthetic%20data%20has%20emerged%20as%20an%20increasingly%20important%20strategy%20for%20improving%20the%20performance%20of%20generative%20AI.%20This%20approach%20is%20particularly%20helpful%20for%20large%20multimodal%20models%20%28LMMs%29%20due%20to%20the%20relative%20scarcity%20of%20high-quality%20paired%20image-text%20data%20compared%20to%20language-only%20data.%20While%20a%20variety%20of%20methods%20have%20been%20proposed%20for%20generating%20large%20multimodal%20datasets%2C%20they%20do%20not%20tailor%20the%20synthetic%20data%20to%20address%20specific%20deficiencies%20in%20the%20reasoning%20abilities%20of%20LMMs%20which%20will%20be%20trained%20with%20the%20generated%20dataset.%20In%20contrast%2C%20humans%20often%20learn%20in%20a%20more%20efficient%20manner%20by%20seeking%20out%20examples%20related%20to%20the%20types%20of%20reasoning%20where%20they%20have%20failed%20previously.%20Inspired%20by%20this%20observation%2C%20we%20propose%20a%20new%20approach%20for%20synthetic%20data%20generation%20which%20is%20grounded%20in%20the%20analysis%20of%20an%20existing%20LMM%27s%20reasoning%20failures.%20Our%20methodology%20leverages%20frontier%20models%20to%20automatically%20analyze%20errors%20produced%20by%20a%20weaker%20LMM%20and%20propose%20new%20examples%20which%20can%20be%20used%20to%20correct%20the%20reasoning%20failure%20via%20additional%20training%2C%20which%20are%20then%20further%20filtered%20to%20ensure%20high%20quality.%20We%20generate%20a%20large%20multimodal%20instruction%20tuning%20dataset%20containing%20over%20553k%20examples%20using%20our%20approach%20and%20conduct%20extensive%20experiments%20demonstrating%20its%20utility%20for%20improving%20the%20performance%20of%20LMMs%20on%20multiple%20downstream%20tasks.%20Our%20results%20show%20that%20models%20trained%20on%20our%20synthetic%20data%20can%20even%20exceed%20the%20performance%20of%20LMMs%20trained%20on%20an%20equivalent%20amount%20of%20additional%20real%20data%2C%20demonstrating%20the%20high%20value%20of%20generating%20synthetic%20data%20targeted%20to%20specific%20reasoning%20failure%20modes%20in%20LMMs.%20We%20will%20make%20our%20dataset%20and%20code%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2504.14523v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Reasoning%2520Failures%2520via%2520Synthetic%2520Data%2520Generation%26entry.906535625%3DGabriela%2520Ben%2520Melech%2520Stan%2520and%2520Estelle%2520Aflalo%2520and%2520Avinash%2520Madasu%2520and%2520Vasudev%2520Lal%2520and%2520Phillip%2520Howard%26entry.1292438233%3DTraining%2520models%2520on%2520synthetic%2520data%2520has%2520emerged%2520as%2520an%2520increasingly%2520important%2520strategy%2520for%2520improving%2520the%2520performance%2520of%2520generative%2520AI.%2520This%2520approach%2520is%2520particularly%2520helpful%2520for%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520due%2520to%2520the%2520relative%2520scarcity%2520of%2520high-quality%2520paired%2520image-text%2520data%2520compared%2520to%2520language-only%2520data.%2520While%2520a%2520variety%2520of%2520methods%2520have%2520been%2520proposed%2520for%2520generating%2520large%2520multimodal%2520datasets%252C%2520they%2520do%2520not%2520tailor%2520the%2520synthetic%2520data%2520to%2520address%2520specific%2520deficiencies%2520in%2520the%2520reasoning%2520abilities%2520of%2520LMMs%2520which%2520will%2520be%2520trained%2520with%2520the%2520generated%2520dataset.%2520In%2520contrast%252C%2520humans%2520often%2520learn%2520in%2520a%2520more%2520efficient%2520manner%2520by%2520seeking%2520out%2520examples%2520related%2520to%2520the%2520types%2520of%2520reasoning%2520where%2520they%2520have%2520failed%2520previously.%2520Inspired%2520by%2520this%2520observation%252C%2520we%2520propose%2520a%2520new%2520approach%2520for%2520synthetic%2520data%2520generation%2520which%2520is%2520grounded%2520in%2520the%2520analysis%2520of%2520an%2520existing%2520LMM%2527s%2520reasoning%2520failures.%2520Our%2520methodology%2520leverages%2520frontier%2520models%2520to%2520automatically%2520analyze%2520errors%2520produced%2520by%2520a%2520weaker%2520LMM%2520and%2520propose%2520new%2520examples%2520which%2520can%2520be%2520used%2520to%2520correct%2520the%2520reasoning%2520failure%2520via%2520additional%2520training%252C%2520which%2520are%2520then%2520further%2520filtered%2520to%2520ensure%2520high%2520quality.%2520We%2520generate%2520a%2520large%2520multimodal%2520instruction%2520tuning%2520dataset%2520containing%2520over%2520553k%2520examples%2520using%2520our%2520approach%2520and%2520conduct%2520extensive%2520experiments%2520demonstrating%2520its%2520utility%2520for%2520improving%2520the%2520performance%2520of%2520LMMs%2520on%2520multiple%2520downstream%2520tasks.%2520Our%2520results%2520show%2520that%2520models%2520trained%2520on%2520our%2520synthetic%2520data%2520can%2520even%2520exceed%2520the%2520performance%2520of%2520LMMs%2520trained%2520on%2520an%2520equivalent%2520amount%2520of%2520additional%2520real%2520data%252C%2520demonstrating%2520the%2520high%2520value%2520of%2520generating%2520synthetic%2520data%2520targeted%2520to%2520specific%2520reasoning%2520failure%2520modes%2520in%2520LMMs.%2520We%2520will%2520make%2520our%2520dataset%2520and%2520code%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14523v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Reasoning%20Failures%20via%20Synthetic%20Data%20Generation&entry.906535625=Gabriela%20Ben%20Melech%20Stan%20and%20Estelle%20Aflalo%20and%20Avinash%20Madasu%20and%20Vasudev%20Lal%20and%20Phillip%20Howard&entry.1292438233=Training%20models%20on%20synthetic%20data%20has%20emerged%20as%20an%20increasingly%20important%20strategy%20for%20improving%20the%20performance%20of%20generative%20AI.%20This%20approach%20is%20particularly%20helpful%20for%20large%20multimodal%20models%20%28LMMs%29%20due%20to%20the%20relative%20scarcity%20of%20high-quality%20paired%20image-text%20data%20compared%20to%20language-only%20data.%20While%20a%20variety%20of%20methods%20have%20been%20proposed%20for%20generating%20large%20multimodal%20datasets%2C%20they%20do%20not%20tailor%20the%20synthetic%20data%20to%20address%20specific%20deficiencies%20in%20the%20reasoning%20abilities%20of%20LMMs%20which%20will%20be%20trained%20with%20the%20generated%20dataset.%20In%20contrast%2C%20humans%20often%20learn%20in%20a%20more%20efficient%20manner%20by%20seeking%20out%20examples%20related%20to%20the%20types%20of%20reasoning%20where%20they%20have%20failed%20previously.%20Inspired%20by%20this%20observation%2C%20we%20propose%20a%20new%20approach%20for%20synthetic%20data%20generation%20which%20is%20grounded%20in%20the%20analysis%20of%20an%20existing%20LMM%27s%20reasoning%20failures.%20Our%20methodology%20leverages%20frontier%20models%20to%20automatically%20analyze%20errors%20produced%20by%20a%20weaker%20LMM%20and%20propose%20new%20examples%20which%20can%20be%20used%20to%20correct%20the%20reasoning%20failure%20via%20additional%20training%2C%20which%20are%20then%20further%20filtered%20to%20ensure%20high%20quality.%20We%20generate%20a%20large%20multimodal%20instruction%20tuning%20dataset%20containing%20over%20553k%20examples%20using%20our%20approach%20and%20conduct%20extensive%20experiments%20demonstrating%20its%20utility%20for%20improving%20the%20performance%20of%20LMMs%20on%20multiple%20downstream%20tasks.%20Our%20results%20show%20that%20models%20trained%20on%20our%20synthetic%20data%20can%20even%20exceed%20the%20performance%20of%20LMMs%20trained%20on%20an%20equivalent%20amount%20of%20additional%20real%20data%2C%20demonstrating%20the%20high%20value%20of%20generating%20synthetic%20data%20targeted%20to%20specific%20reasoning%20failure%20modes%20in%20LMMs.%20We%20will%20make%20our%20dataset%20and%20code%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2504.14523v2&entry.124074799=Read"},
{"title": "A Unified Framework for Emotion Recognition and Sentiment Analysis via Expert-Guided Multimodal Fusion with Large Language Models", "author": "Jiaqi Qiao and Xiujuan Xu and Xinran Li and Yu Liu", "abstract": "Multimodal emotion understanding requires effective integration of text, audio, and visual modalities for both discrete emotion recognition and continuous sentiment analysis. We present EGMF, a unified framework combining expert-guided multimodal fusion with large language models. Our approach features three specialized expert networks--a fine-grained local expert for subtle emotional nuances, a semantic correlation expert for cross-modal relationships, and a global context expert for long-range dependencies--adaptively integrated through hierarchical dynamic gating for context-aware feature selection. Enhanced multimodal representations are integrated with LLMs via pseudo token injection and prompt-based conditioning, enabling a single generative framework to handle both classification and regression through natural language generation. We employ LoRA fine-tuning for computational efficiency. Experiments on bilingual benchmarks (MELD, CHERMA, MOSEI, SIMS-V2) demonstrate consistent improvements over state-of-the-art methods, with superior cross-lingual robustness revealing universal patterns in multimodal emotional expressions across English and Chinese. We will release the source code publicly.", "link": "http://arxiv.org/abs/2601.07565v1", "date": "2026-01-12", "relevancy": 2.1295, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.54}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5273}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Framework%20for%20Emotion%20Recognition%20and%20Sentiment%20Analysis%20via%20Expert-Guided%20Multimodal%20Fusion%20with%20Large%20Language%20Models&body=Title%3A%20A%20Unified%20Framework%20for%20Emotion%20Recognition%20and%20Sentiment%20Analysis%20via%20Expert-Guided%20Multimodal%20Fusion%20with%20Large%20Language%20Models%0AAuthor%3A%20Jiaqi%20Qiao%20and%20Xiujuan%20Xu%20and%20Xinran%20Li%20and%20Yu%20Liu%0AAbstract%3A%20Multimodal%20emotion%20understanding%20requires%20effective%20integration%20of%20text%2C%20audio%2C%20and%20visual%20modalities%20for%20both%20discrete%20emotion%20recognition%20and%20continuous%20sentiment%20analysis.%20We%20present%20EGMF%2C%20a%20unified%20framework%20combining%20expert-guided%20multimodal%20fusion%20with%20large%20language%20models.%20Our%20approach%20features%20three%20specialized%20expert%20networks--a%20fine-grained%20local%20expert%20for%20subtle%20emotional%20nuances%2C%20a%20semantic%20correlation%20expert%20for%20cross-modal%20relationships%2C%20and%20a%20global%20context%20expert%20for%20long-range%20dependencies--adaptively%20integrated%20through%20hierarchical%20dynamic%20gating%20for%20context-aware%20feature%20selection.%20Enhanced%20multimodal%20representations%20are%20integrated%20with%20LLMs%20via%20pseudo%20token%20injection%20and%20prompt-based%20conditioning%2C%20enabling%20a%20single%20generative%20framework%20to%20handle%20both%20classification%20and%20regression%20through%20natural%20language%20generation.%20We%20employ%20LoRA%20fine-tuning%20for%20computational%20efficiency.%20Experiments%20on%20bilingual%20benchmarks%20%28MELD%2C%20CHERMA%2C%20MOSEI%2C%20SIMS-V2%29%20demonstrate%20consistent%20improvements%20over%20state-of-the-art%20methods%2C%20with%20superior%20cross-lingual%20robustness%20revealing%20universal%20patterns%20in%20multimodal%20emotional%20expressions%20across%20English%20and%20Chinese.%20We%20will%20release%20the%20source%20code%20publicly.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Framework%2520for%2520Emotion%2520Recognition%2520and%2520Sentiment%2520Analysis%2520via%2520Expert-Guided%2520Multimodal%2520Fusion%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DJiaqi%2520Qiao%2520and%2520Xiujuan%2520Xu%2520and%2520Xinran%2520Li%2520and%2520Yu%2520Liu%26entry.1292438233%3DMultimodal%2520emotion%2520understanding%2520requires%2520effective%2520integration%2520of%2520text%252C%2520audio%252C%2520and%2520visual%2520modalities%2520for%2520both%2520discrete%2520emotion%2520recognition%2520and%2520continuous%2520sentiment%2520analysis.%2520We%2520present%2520EGMF%252C%2520a%2520unified%2520framework%2520combining%2520expert-guided%2520multimodal%2520fusion%2520with%2520large%2520language%2520models.%2520Our%2520approach%2520features%2520three%2520specialized%2520expert%2520networks--a%2520fine-grained%2520local%2520expert%2520for%2520subtle%2520emotional%2520nuances%252C%2520a%2520semantic%2520correlation%2520expert%2520for%2520cross-modal%2520relationships%252C%2520and%2520a%2520global%2520context%2520expert%2520for%2520long-range%2520dependencies--adaptively%2520integrated%2520through%2520hierarchical%2520dynamic%2520gating%2520for%2520context-aware%2520feature%2520selection.%2520Enhanced%2520multimodal%2520representations%2520are%2520integrated%2520with%2520LLMs%2520via%2520pseudo%2520token%2520injection%2520and%2520prompt-based%2520conditioning%252C%2520enabling%2520a%2520single%2520generative%2520framework%2520to%2520handle%2520both%2520classification%2520and%2520regression%2520through%2520natural%2520language%2520generation.%2520We%2520employ%2520LoRA%2520fine-tuning%2520for%2520computational%2520efficiency.%2520Experiments%2520on%2520bilingual%2520benchmarks%2520%2528MELD%252C%2520CHERMA%252C%2520MOSEI%252C%2520SIMS-V2%2529%2520demonstrate%2520consistent%2520improvements%2520over%2520state-of-the-art%2520methods%252C%2520with%2520superior%2520cross-lingual%2520robustness%2520revealing%2520universal%2520patterns%2520in%2520multimodal%2520emotional%2520expressions%2520across%2520English%2520and%2520Chinese.%2520We%2520will%2520release%2520the%2520source%2520code%2520publicly.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Framework%20for%20Emotion%20Recognition%20and%20Sentiment%20Analysis%20via%20Expert-Guided%20Multimodal%20Fusion%20with%20Large%20Language%20Models&entry.906535625=Jiaqi%20Qiao%20and%20Xiujuan%20Xu%20and%20Xinran%20Li%20and%20Yu%20Liu&entry.1292438233=Multimodal%20emotion%20understanding%20requires%20effective%20integration%20of%20text%2C%20audio%2C%20and%20visual%20modalities%20for%20both%20discrete%20emotion%20recognition%20and%20continuous%20sentiment%20analysis.%20We%20present%20EGMF%2C%20a%20unified%20framework%20combining%20expert-guided%20multimodal%20fusion%20with%20large%20language%20models.%20Our%20approach%20features%20three%20specialized%20expert%20networks--a%20fine-grained%20local%20expert%20for%20subtle%20emotional%20nuances%2C%20a%20semantic%20correlation%20expert%20for%20cross-modal%20relationships%2C%20and%20a%20global%20context%20expert%20for%20long-range%20dependencies--adaptively%20integrated%20through%20hierarchical%20dynamic%20gating%20for%20context-aware%20feature%20selection.%20Enhanced%20multimodal%20representations%20are%20integrated%20with%20LLMs%20via%20pseudo%20token%20injection%20and%20prompt-based%20conditioning%2C%20enabling%20a%20single%20generative%20framework%20to%20handle%20both%20classification%20and%20regression%20through%20natural%20language%20generation.%20We%20employ%20LoRA%20fine-tuning%20for%20computational%20efficiency.%20Experiments%20on%20bilingual%20benchmarks%20%28MELD%2C%20CHERMA%2C%20MOSEI%2C%20SIMS-V2%29%20demonstrate%20consistent%20improvements%20over%20state-of-the-art%20methods%2C%20with%20superior%20cross-lingual%20robustness%20revealing%20universal%20patterns%20in%20multimodal%20emotional%20expressions%20across%20English%20and%20Chinese.%20We%20will%20release%20the%20source%20code%20publicly.&entry.1838667208=http%3A//arxiv.org/abs/2601.07565v1&entry.124074799=Read"},
{"title": "LOONG: Online Time-Optimal Autonomous Flight for MAVs in Cluttered Environments", "author": "Xin Guan and Fangguo Zhao and Qianyi Wang and Chengcheng Zhao and Jiming Chen and Shuo Li", "abstract": "Autonomous flight of micro air vehicles (MAVs) in unknown, cluttered environments remains challenging for time-critical missions due to conservative maneuvering strategies. This article presents an integrated planning and control framework for high-speed, time-optimal autonomous flight of MAVs in cluttered environments. In each replanning cycle (100 Hz), a time-optimal trajectory under polynomial presentation is generated as a reference, with the time-allocation process accelerated by imitation learning. Subsequently, a time-optimal model predictive contouring control (MPCC) incorporates safe flight corridor (SFC) constraints at variable horizon steps to enable aggressive yet safe maneuvering, while fully exploiting the MAV's dynamics. We validate the proposed framework extensively on a custom-built LiDAR-based MAV platform. Simulation results demonstrate superior aggressiveness compared to the state of the art, while real-world experiments achieve a peak speed of 18 m/s in a cluttered environment and succeed in 10 consecutive trials from diverse start points. The video is available at the following link: https://youtu.be/vexXXhv99oQ.", "link": "http://arxiv.org/abs/2601.07434v1", "date": "2026-01-12", "relevancy": 2.1255, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5389}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5276}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOONG%3A%20Online%20Time-Optimal%20Autonomous%20Flight%20for%20MAVs%20in%20Cluttered%20Environments&body=Title%3A%20LOONG%3A%20Online%20Time-Optimal%20Autonomous%20Flight%20for%20MAVs%20in%20Cluttered%20Environments%0AAuthor%3A%20Xin%20Guan%20and%20Fangguo%20Zhao%20and%20Qianyi%20Wang%20and%20Chengcheng%20Zhao%20and%20Jiming%20Chen%20and%20Shuo%20Li%0AAbstract%3A%20Autonomous%20flight%20of%20micro%20air%20vehicles%20%28MAVs%29%20in%20unknown%2C%20cluttered%20environments%20remains%20challenging%20for%20time-critical%20missions%20due%20to%20conservative%20maneuvering%20strategies.%20This%20article%20presents%20an%20integrated%20planning%20and%20control%20framework%20for%20high-speed%2C%20time-optimal%20autonomous%20flight%20of%20MAVs%20in%20cluttered%20environments.%20In%20each%20replanning%20cycle%20%28100%20Hz%29%2C%20a%20time-optimal%20trajectory%20under%20polynomial%20presentation%20is%20generated%20as%20a%20reference%2C%20with%20the%20time-allocation%20process%20accelerated%20by%20imitation%20learning.%20Subsequently%2C%20a%20time-optimal%20model%20predictive%20contouring%20control%20%28MPCC%29%20incorporates%20safe%20flight%20corridor%20%28SFC%29%20constraints%20at%20variable%20horizon%20steps%20to%20enable%20aggressive%20yet%20safe%20maneuvering%2C%20while%20fully%20exploiting%20the%20MAV%27s%20dynamics.%20We%20validate%20the%20proposed%20framework%20extensively%20on%20a%20custom-built%20LiDAR-based%20MAV%20platform.%20Simulation%20results%20demonstrate%20superior%20aggressiveness%20compared%20to%20the%20state%20of%20the%20art%2C%20while%20real-world%20experiments%20achieve%20a%20peak%20speed%20of%2018%20m/s%20in%20a%20cluttered%20environment%20and%20succeed%20in%2010%20consecutive%20trials%20from%20diverse%20start%20points.%20The%20video%20is%20available%20at%20the%20following%20link%3A%20https%3A//youtu.be/vexXXhv99oQ.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOONG%253A%2520Online%2520Time-Optimal%2520Autonomous%2520Flight%2520for%2520MAVs%2520in%2520Cluttered%2520Environments%26entry.906535625%3DXin%2520Guan%2520and%2520Fangguo%2520Zhao%2520and%2520Qianyi%2520Wang%2520and%2520Chengcheng%2520Zhao%2520and%2520Jiming%2520Chen%2520and%2520Shuo%2520Li%26entry.1292438233%3DAutonomous%2520flight%2520of%2520micro%2520air%2520vehicles%2520%2528MAVs%2529%2520in%2520unknown%252C%2520cluttered%2520environments%2520remains%2520challenging%2520for%2520time-critical%2520missions%2520due%2520to%2520conservative%2520maneuvering%2520strategies.%2520This%2520article%2520presents%2520an%2520integrated%2520planning%2520and%2520control%2520framework%2520for%2520high-speed%252C%2520time-optimal%2520autonomous%2520flight%2520of%2520MAVs%2520in%2520cluttered%2520environments.%2520In%2520each%2520replanning%2520cycle%2520%2528100%2520Hz%2529%252C%2520a%2520time-optimal%2520trajectory%2520under%2520polynomial%2520presentation%2520is%2520generated%2520as%2520a%2520reference%252C%2520with%2520the%2520time-allocation%2520process%2520accelerated%2520by%2520imitation%2520learning.%2520Subsequently%252C%2520a%2520time-optimal%2520model%2520predictive%2520contouring%2520control%2520%2528MPCC%2529%2520incorporates%2520safe%2520flight%2520corridor%2520%2528SFC%2529%2520constraints%2520at%2520variable%2520horizon%2520steps%2520to%2520enable%2520aggressive%2520yet%2520safe%2520maneuvering%252C%2520while%2520fully%2520exploiting%2520the%2520MAV%2527s%2520dynamics.%2520We%2520validate%2520the%2520proposed%2520framework%2520extensively%2520on%2520a%2520custom-built%2520LiDAR-based%2520MAV%2520platform.%2520Simulation%2520results%2520demonstrate%2520superior%2520aggressiveness%2520compared%2520to%2520the%2520state%2520of%2520the%2520art%252C%2520while%2520real-world%2520experiments%2520achieve%2520a%2520peak%2520speed%2520of%252018%2520m/s%2520in%2520a%2520cluttered%2520environment%2520and%2520succeed%2520in%252010%2520consecutive%2520trials%2520from%2520diverse%2520start%2520points.%2520The%2520video%2520is%2520available%2520at%2520the%2520following%2520link%253A%2520https%253A//youtu.be/vexXXhv99oQ.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOONG%3A%20Online%20Time-Optimal%20Autonomous%20Flight%20for%20MAVs%20in%20Cluttered%20Environments&entry.906535625=Xin%20Guan%20and%20Fangguo%20Zhao%20and%20Qianyi%20Wang%20and%20Chengcheng%20Zhao%20and%20Jiming%20Chen%20and%20Shuo%20Li&entry.1292438233=Autonomous%20flight%20of%20micro%20air%20vehicles%20%28MAVs%29%20in%20unknown%2C%20cluttered%20environments%20remains%20challenging%20for%20time-critical%20missions%20due%20to%20conservative%20maneuvering%20strategies.%20This%20article%20presents%20an%20integrated%20planning%20and%20control%20framework%20for%20high-speed%2C%20time-optimal%20autonomous%20flight%20of%20MAVs%20in%20cluttered%20environments.%20In%20each%20replanning%20cycle%20%28100%20Hz%29%2C%20a%20time-optimal%20trajectory%20under%20polynomial%20presentation%20is%20generated%20as%20a%20reference%2C%20with%20the%20time-allocation%20process%20accelerated%20by%20imitation%20learning.%20Subsequently%2C%20a%20time-optimal%20model%20predictive%20contouring%20control%20%28MPCC%29%20incorporates%20safe%20flight%20corridor%20%28SFC%29%20constraints%20at%20variable%20horizon%20steps%20to%20enable%20aggressive%20yet%20safe%20maneuvering%2C%20while%20fully%20exploiting%20the%20MAV%27s%20dynamics.%20We%20validate%20the%20proposed%20framework%20extensively%20on%20a%20custom-built%20LiDAR-based%20MAV%20platform.%20Simulation%20results%20demonstrate%20superior%20aggressiveness%20compared%20to%20the%20state%20of%20the%20art%2C%20while%20real-world%20experiments%20achieve%20a%20peak%20speed%20of%2018%20m/s%20in%20a%20cluttered%20environment%20and%20succeed%20in%2010%20consecutive%20trials%20from%20diverse%20start%20points.%20The%20video%20is%20available%20at%20the%20following%20link%3A%20https%3A//youtu.be/vexXXhv99oQ.&entry.1838667208=http%3A//arxiv.org/abs/2601.07434v1&entry.124074799=Read"},
{"title": "Learning to accelerate Krasnosel'skii-Mann fixed-point iterations with guarantees", "author": "Andrea Martin and Giuseppe Belgioioso", "abstract": "We introduce a principled learning to optimize (L2O) framework for solving fixed-point problems involving general nonexpansive mappings. Our idea is to deliberately inject summable perturbations into a standard Krasnosel'skii-Mann iteration to improve its average-case performance over a specific distribution of problems while retaining its convergence guarantees. Under a metric sub-regularity assumption, we prove that the proposed parametrization includes only iterations that locally achieve linear convergence-up to a vanishing bias term-and that it encompasses all iterations that do so at a sufficiently fast rate. We then demonstrate how our framework can be used to augment several widely-used operator splitting methods to accelerate the solution of structured monotone inclusion problems, and validate our approach on a best approximation problem using an L2O-augmented Douglas-Rachford splitting algorithm.", "link": "http://arxiv.org/abs/2601.07665v1", "date": "2026-01-12", "relevancy": 2.1158, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4303}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4223}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20accelerate%20Krasnosel%27skii-Mann%20fixed-point%20iterations%20with%20guarantees&body=Title%3A%20Learning%20to%20accelerate%20Krasnosel%27skii-Mann%20fixed-point%20iterations%20with%20guarantees%0AAuthor%3A%20Andrea%20Martin%20and%20Giuseppe%20Belgioioso%0AAbstract%3A%20We%20introduce%20a%20principled%20learning%20to%20optimize%20%28L2O%29%20framework%20for%20solving%20fixed-point%20problems%20involving%20general%20nonexpansive%20mappings.%20Our%20idea%20is%20to%20deliberately%20inject%20summable%20perturbations%20into%20a%20standard%20Krasnosel%27skii-Mann%20iteration%20to%20improve%20its%20average-case%20performance%20over%20a%20specific%20distribution%20of%20problems%20while%20retaining%20its%20convergence%20guarantees.%20Under%20a%20metric%20sub-regularity%20assumption%2C%20we%20prove%20that%20the%20proposed%20parametrization%20includes%20only%20iterations%20that%20locally%20achieve%20linear%20convergence-up%20to%20a%20vanishing%20bias%20term-and%20that%20it%20encompasses%20all%20iterations%20that%20do%20so%20at%20a%20sufficiently%20fast%20rate.%20We%20then%20demonstrate%20how%20our%20framework%20can%20be%20used%20to%20augment%20several%20widely-used%20operator%20splitting%20methods%20to%20accelerate%20the%20solution%20of%20structured%20monotone%20inclusion%20problems%2C%20and%20validate%20our%20approach%20on%20a%20best%20approximation%20problem%20using%20an%20L2O-augmented%20Douglas-Rachford%20splitting%20algorithm.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520accelerate%2520Krasnosel%2527skii-Mann%2520fixed-point%2520iterations%2520with%2520guarantees%26entry.906535625%3DAndrea%2520Martin%2520and%2520Giuseppe%2520Belgioioso%26entry.1292438233%3DWe%2520introduce%2520a%2520principled%2520learning%2520to%2520optimize%2520%2528L2O%2529%2520framework%2520for%2520solving%2520fixed-point%2520problems%2520involving%2520general%2520nonexpansive%2520mappings.%2520Our%2520idea%2520is%2520to%2520deliberately%2520inject%2520summable%2520perturbations%2520into%2520a%2520standard%2520Krasnosel%2527skii-Mann%2520iteration%2520to%2520improve%2520its%2520average-case%2520performance%2520over%2520a%2520specific%2520distribution%2520of%2520problems%2520while%2520retaining%2520its%2520convergence%2520guarantees.%2520Under%2520a%2520metric%2520sub-regularity%2520assumption%252C%2520we%2520prove%2520that%2520the%2520proposed%2520parametrization%2520includes%2520only%2520iterations%2520that%2520locally%2520achieve%2520linear%2520convergence-up%2520to%2520a%2520vanishing%2520bias%2520term-and%2520that%2520it%2520encompasses%2520all%2520iterations%2520that%2520do%2520so%2520at%2520a%2520sufficiently%2520fast%2520rate.%2520We%2520then%2520demonstrate%2520how%2520our%2520framework%2520can%2520be%2520used%2520to%2520augment%2520several%2520widely-used%2520operator%2520splitting%2520methods%2520to%2520accelerate%2520the%2520solution%2520of%2520structured%2520monotone%2520inclusion%2520problems%252C%2520and%2520validate%2520our%2520approach%2520on%2520a%2520best%2520approximation%2520problem%2520using%2520an%2520L2O-augmented%2520Douglas-Rachford%2520splitting%2520algorithm.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20accelerate%20Krasnosel%27skii-Mann%20fixed-point%20iterations%20with%20guarantees&entry.906535625=Andrea%20Martin%20and%20Giuseppe%20Belgioioso&entry.1292438233=We%20introduce%20a%20principled%20learning%20to%20optimize%20%28L2O%29%20framework%20for%20solving%20fixed-point%20problems%20involving%20general%20nonexpansive%20mappings.%20Our%20idea%20is%20to%20deliberately%20inject%20summable%20perturbations%20into%20a%20standard%20Krasnosel%27skii-Mann%20iteration%20to%20improve%20its%20average-case%20performance%20over%20a%20specific%20distribution%20of%20problems%20while%20retaining%20its%20convergence%20guarantees.%20Under%20a%20metric%20sub-regularity%20assumption%2C%20we%20prove%20that%20the%20proposed%20parametrization%20includes%20only%20iterations%20that%20locally%20achieve%20linear%20convergence-up%20to%20a%20vanishing%20bias%20term-and%20that%20it%20encompasses%20all%20iterations%20that%20do%20so%20at%20a%20sufficiently%20fast%20rate.%20We%20then%20demonstrate%20how%20our%20framework%20can%20be%20used%20to%20augment%20several%20widely-used%20operator%20splitting%20methods%20to%20accelerate%20the%20solution%20of%20structured%20monotone%20inclusion%20problems%2C%20and%20validate%20our%20approach%20on%20a%20best%20approximation%20problem%20using%20an%20L2O-augmented%20Douglas-Rachford%20splitting%20algorithm.&entry.1838667208=http%3A//arxiv.org/abs/2601.07665v1&entry.124074799=Read"},
{"title": "SDHSI-Net: Learning Better Representations for Hyperspectral Images via Self-Distillation", "author": "Prachet Dev Singh and Shyamsundar Paramasivam and Sneha Barman and Mainak Singha and Ankit Jha and Girish Mishra and Biplab Banerjee", "abstract": "Hyperspectral image (HSI) classification presents unique challenges due to its high spectral dimensionality and limited labeled data. Traditional deep learning models often suffer from overfitting and high computational costs. Self-distillation (SD), a variant of knowledge distillation where a network learns from its own predictions, has recently emerged as a promising strategy to enhance model performance without requiring external teacher networks. In this work, we explore the application of SD to HSI by treating earlier outputs as soft targets, thereby enforcing consistency between intermediate and final predictions. This process improves intra-class compactness and inter-class separability in the learned feature space. Our approach is validated on two benchmark HSI datasets and demonstrates significant improvements in classification accuracy and robustness, highlighting the effectiveness of SD for spectral-spatial learning. Codes are available at https://github.com/Prachet-Dev-Singh/SDHSI.", "link": "http://arxiv.org/abs/2601.07416v1", "date": "2026-01-12", "relevancy": 2.1145, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5497}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5287}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDHSI-Net%3A%20Learning%20Better%20Representations%20for%20Hyperspectral%20Images%20via%20Self-Distillation&body=Title%3A%20SDHSI-Net%3A%20Learning%20Better%20Representations%20for%20Hyperspectral%20Images%20via%20Self-Distillation%0AAuthor%3A%20Prachet%20Dev%20Singh%20and%20Shyamsundar%20Paramasivam%20and%20Sneha%20Barman%20and%20Mainak%20Singha%20and%20Ankit%20Jha%20and%20Girish%20Mishra%20and%20Biplab%20Banerjee%0AAbstract%3A%20Hyperspectral%20image%20%28HSI%29%20classification%20presents%20unique%20challenges%20due%20to%20its%20high%20spectral%20dimensionality%20and%20limited%20labeled%20data.%20Traditional%20deep%20learning%20models%20often%20suffer%20from%20overfitting%20and%20high%20computational%20costs.%20Self-distillation%20%28SD%29%2C%20a%20variant%20of%20knowledge%20distillation%20where%20a%20network%20learns%20from%20its%20own%20predictions%2C%20has%20recently%20emerged%20as%20a%20promising%20strategy%20to%20enhance%20model%20performance%20without%20requiring%20external%20teacher%20networks.%20In%20this%20work%2C%20we%20explore%20the%20application%20of%20SD%20to%20HSI%20by%20treating%20earlier%20outputs%20as%20soft%20targets%2C%20thereby%20enforcing%20consistency%20between%20intermediate%20and%20final%20predictions.%20This%20process%20improves%20intra-class%20compactness%20and%20inter-class%20separability%20in%20the%20learned%20feature%20space.%20Our%20approach%20is%20validated%20on%20two%20benchmark%20HSI%20datasets%20and%20demonstrates%20significant%20improvements%20in%20classification%20accuracy%20and%20robustness%2C%20highlighting%20the%20effectiveness%20of%20SD%20for%20spectral-spatial%20learning.%20Codes%20are%20available%20at%20https%3A//github.com/Prachet-Dev-Singh/SDHSI.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07416v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDHSI-Net%253A%2520Learning%2520Better%2520Representations%2520for%2520Hyperspectral%2520Images%2520via%2520Self-Distillation%26entry.906535625%3DPrachet%2520Dev%2520Singh%2520and%2520Shyamsundar%2520Paramasivam%2520and%2520Sneha%2520Barman%2520and%2520Mainak%2520Singha%2520and%2520Ankit%2520Jha%2520and%2520Girish%2520Mishra%2520and%2520Biplab%2520Banerjee%26entry.1292438233%3DHyperspectral%2520image%2520%2528HSI%2529%2520classification%2520presents%2520unique%2520challenges%2520due%2520to%2520its%2520high%2520spectral%2520dimensionality%2520and%2520limited%2520labeled%2520data.%2520Traditional%2520deep%2520learning%2520models%2520often%2520suffer%2520from%2520overfitting%2520and%2520high%2520computational%2520costs.%2520Self-distillation%2520%2528SD%2529%252C%2520a%2520variant%2520of%2520knowledge%2520distillation%2520where%2520a%2520network%2520learns%2520from%2520its%2520own%2520predictions%252C%2520has%2520recently%2520emerged%2520as%2520a%2520promising%2520strategy%2520to%2520enhance%2520model%2520performance%2520without%2520requiring%2520external%2520teacher%2520networks.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520application%2520of%2520SD%2520to%2520HSI%2520by%2520treating%2520earlier%2520outputs%2520as%2520soft%2520targets%252C%2520thereby%2520enforcing%2520consistency%2520between%2520intermediate%2520and%2520final%2520predictions.%2520This%2520process%2520improves%2520intra-class%2520compactness%2520and%2520inter-class%2520separability%2520in%2520the%2520learned%2520feature%2520space.%2520Our%2520approach%2520is%2520validated%2520on%2520two%2520benchmark%2520HSI%2520datasets%2520and%2520demonstrates%2520significant%2520improvements%2520in%2520classification%2520accuracy%2520and%2520robustness%252C%2520highlighting%2520the%2520effectiveness%2520of%2520SD%2520for%2520spectral-spatial%2520learning.%2520Codes%2520are%2520available%2520at%2520https%253A//github.com/Prachet-Dev-Singh/SDHSI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07416v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDHSI-Net%3A%20Learning%20Better%20Representations%20for%20Hyperspectral%20Images%20via%20Self-Distillation&entry.906535625=Prachet%20Dev%20Singh%20and%20Shyamsundar%20Paramasivam%20and%20Sneha%20Barman%20and%20Mainak%20Singha%20and%20Ankit%20Jha%20and%20Girish%20Mishra%20and%20Biplab%20Banerjee&entry.1292438233=Hyperspectral%20image%20%28HSI%29%20classification%20presents%20unique%20challenges%20due%20to%20its%20high%20spectral%20dimensionality%20and%20limited%20labeled%20data.%20Traditional%20deep%20learning%20models%20often%20suffer%20from%20overfitting%20and%20high%20computational%20costs.%20Self-distillation%20%28SD%29%2C%20a%20variant%20of%20knowledge%20distillation%20where%20a%20network%20learns%20from%20its%20own%20predictions%2C%20has%20recently%20emerged%20as%20a%20promising%20strategy%20to%20enhance%20model%20performance%20without%20requiring%20external%20teacher%20networks.%20In%20this%20work%2C%20we%20explore%20the%20application%20of%20SD%20to%20HSI%20by%20treating%20earlier%20outputs%20as%20soft%20targets%2C%20thereby%20enforcing%20consistency%20between%20intermediate%20and%20final%20predictions.%20This%20process%20improves%20intra-class%20compactness%20and%20inter-class%20separability%20in%20the%20learned%20feature%20space.%20Our%20approach%20is%20validated%20on%20two%20benchmark%20HSI%20datasets%20and%20demonstrates%20significant%20improvements%20in%20classification%20accuracy%20and%20robustness%2C%20highlighting%20the%20effectiveness%20of%20SD%20for%20spectral-spatial%20learning.%20Codes%20are%20available%20at%20https%3A//github.com/Prachet-Dev-Singh/SDHSI.&entry.1838667208=http%3A//arxiv.org/abs/2601.07416v1&entry.124074799=Read"},
{"title": "Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification", "author": "Mustafa Demetgul and Sanja Lazarova Molnar", "abstract": "Monitoring states of road surfaces provides valuable information for the planning and controlling vehicles and active vehicle control systems. Classical road monitoring methods are expensive and unsystematic because they require time for measurements. This article proposes an real time system based on weather conditional data and road surface condition data. For this purpose, we collected data with a mobile phone camera on the roads around the campus of the Karlsruhe Institute of Technology. We tested a large number of different image-based deep learning algorithms for road classification. In addition, we used road acceleration data along with road image data for training by using them as images. We compared the performances of acceleration-based and camera image-based approaches. The performances of the simple Alexnet, LeNet, VGG, and Resnet algorithms were compared as deep learning algorithms. For road condition classification, 5 classes were considered: asphalt, damaged asphalt, gravel road, damaged gravel road, pavement road and over 95% accuracy performance was achieved. It is also proposed to use the acceleration or the camera image to classify the road surface according to the weather and the time of day using fuzzy logic.", "link": "http://arxiv.org/abs/2512.23436v2", "date": "2026-01-12", "relevancy": 2.113, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5618}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5303}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fuzzy-Logic%20and%20Deep%20Learning%20for%20Environmental%20Condition-Aware%20Road%20Surface%20Classification&body=Title%3A%20Fuzzy-Logic%20and%20Deep%20Learning%20for%20Environmental%20Condition-Aware%20Road%20Surface%20Classification%0AAuthor%3A%20Mustafa%20Demetgul%20and%20Sanja%20Lazarova%20Molnar%0AAbstract%3A%20Monitoring%20states%20of%20road%20surfaces%20provides%20valuable%20information%20for%20the%20planning%20and%20controlling%20vehicles%20and%20active%20vehicle%20control%20systems.%20Classical%20road%20monitoring%20methods%20are%20expensive%20and%20unsystematic%20because%20they%20require%20time%20for%20measurements.%20This%20article%20proposes%20an%20real%20time%20system%20based%20on%20weather%20conditional%20data%20and%20road%20surface%20condition%20data.%20For%20this%20purpose%2C%20we%20collected%20data%20with%20a%20mobile%20phone%20camera%20on%20the%20roads%20around%20the%20campus%20of%20the%20Karlsruhe%20Institute%20of%20Technology.%20We%20tested%20a%20large%20number%20of%20different%20image-based%20deep%20learning%20algorithms%20for%20road%20classification.%20In%20addition%2C%20we%20used%20road%20acceleration%20data%20along%20with%20road%20image%20data%20for%20training%20by%20using%20them%20as%20images.%20We%20compared%20the%20performances%20of%20acceleration-based%20and%20camera%20image-based%20approaches.%20The%20performances%20of%20the%20simple%20Alexnet%2C%20LeNet%2C%20VGG%2C%20and%20Resnet%20algorithms%20were%20compared%20as%20deep%20learning%20algorithms.%20For%20road%20condition%20classification%2C%205%20classes%20were%20considered%3A%20asphalt%2C%20damaged%20asphalt%2C%20gravel%20road%2C%20damaged%20gravel%20road%2C%20pavement%20road%20and%20over%2095%25%20accuracy%20performance%20was%20achieved.%20It%20is%20also%20proposed%20to%20use%20the%20acceleration%20or%20the%20camera%20image%20to%20classify%20the%20road%20surface%20according%20to%20the%20weather%20and%20the%20time%20of%20day%20using%20fuzzy%20logic.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23436v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFuzzy-Logic%2520and%2520Deep%2520Learning%2520for%2520Environmental%2520Condition-Aware%2520Road%2520Surface%2520Classification%26entry.906535625%3DMustafa%2520Demetgul%2520and%2520Sanja%2520Lazarova%2520Molnar%26entry.1292438233%3DMonitoring%2520states%2520of%2520road%2520surfaces%2520provides%2520valuable%2520information%2520for%2520the%2520planning%2520and%2520controlling%2520vehicles%2520and%2520active%2520vehicle%2520control%2520systems.%2520Classical%2520road%2520monitoring%2520methods%2520are%2520expensive%2520and%2520unsystematic%2520because%2520they%2520require%2520time%2520for%2520measurements.%2520This%2520article%2520proposes%2520an%2520real%2520time%2520system%2520based%2520on%2520weather%2520conditional%2520data%2520and%2520road%2520surface%2520condition%2520data.%2520For%2520this%2520purpose%252C%2520we%2520collected%2520data%2520with%2520a%2520mobile%2520phone%2520camera%2520on%2520the%2520roads%2520around%2520the%2520campus%2520of%2520the%2520Karlsruhe%2520Institute%2520of%2520Technology.%2520We%2520tested%2520a%2520large%2520number%2520of%2520different%2520image-based%2520deep%2520learning%2520algorithms%2520for%2520road%2520classification.%2520In%2520addition%252C%2520we%2520used%2520road%2520acceleration%2520data%2520along%2520with%2520road%2520image%2520data%2520for%2520training%2520by%2520using%2520them%2520as%2520images.%2520We%2520compared%2520the%2520performances%2520of%2520acceleration-based%2520and%2520camera%2520image-based%2520approaches.%2520The%2520performances%2520of%2520the%2520simple%2520Alexnet%252C%2520LeNet%252C%2520VGG%252C%2520and%2520Resnet%2520algorithms%2520were%2520compared%2520as%2520deep%2520learning%2520algorithms.%2520For%2520road%2520condition%2520classification%252C%25205%2520classes%2520were%2520considered%253A%2520asphalt%252C%2520damaged%2520asphalt%252C%2520gravel%2520road%252C%2520damaged%2520gravel%2520road%252C%2520pavement%2520road%2520and%2520over%252095%2525%2520accuracy%2520performance%2520was%2520achieved.%2520It%2520is%2520also%2520proposed%2520to%2520use%2520the%2520acceleration%2520or%2520the%2520camera%2520image%2520to%2520classify%2520the%2520road%2520surface%2520according%2520to%2520the%2520weather%2520and%2520the%2520time%2520of%2520day%2520using%2520fuzzy%2520logic.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23436v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fuzzy-Logic%20and%20Deep%20Learning%20for%20Environmental%20Condition-Aware%20Road%20Surface%20Classification&entry.906535625=Mustafa%20Demetgul%20and%20Sanja%20Lazarova%20Molnar&entry.1292438233=Monitoring%20states%20of%20road%20surfaces%20provides%20valuable%20information%20for%20the%20planning%20and%20controlling%20vehicles%20and%20active%20vehicle%20control%20systems.%20Classical%20road%20monitoring%20methods%20are%20expensive%20and%20unsystematic%20because%20they%20require%20time%20for%20measurements.%20This%20article%20proposes%20an%20real%20time%20system%20based%20on%20weather%20conditional%20data%20and%20road%20surface%20condition%20data.%20For%20this%20purpose%2C%20we%20collected%20data%20with%20a%20mobile%20phone%20camera%20on%20the%20roads%20around%20the%20campus%20of%20the%20Karlsruhe%20Institute%20of%20Technology.%20We%20tested%20a%20large%20number%20of%20different%20image-based%20deep%20learning%20algorithms%20for%20road%20classification.%20In%20addition%2C%20we%20used%20road%20acceleration%20data%20along%20with%20road%20image%20data%20for%20training%20by%20using%20them%20as%20images.%20We%20compared%20the%20performances%20of%20acceleration-based%20and%20camera%20image-based%20approaches.%20The%20performances%20of%20the%20simple%20Alexnet%2C%20LeNet%2C%20VGG%2C%20and%20Resnet%20algorithms%20were%20compared%20as%20deep%20learning%20algorithms.%20For%20road%20condition%20classification%2C%205%20classes%20were%20considered%3A%20asphalt%2C%20damaged%20asphalt%2C%20gravel%20road%2C%20damaged%20gravel%20road%2C%20pavement%20road%20and%20over%2095%25%20accuracy%20performance%20was%20achieved.%20It%20is%20also%20proposed%20to%20use%20the%20acceleration%20or%20the%20camera%20image%20to%20classify%20the%20road%20surface%20according%20to%20the%20weather%20and%20the%20time%20of%20day%20using%20fuzzy%20logic.&entry.1838667208=http%3A//arxiv.org/abs/2512.23436v2&entry.124074799=Read"},
{"title": "WaveMan: mmWave-Based Room-Scale Human Interaction Perception for Humanoid Robots", "author": "Yuxuan Hu and Kuangji Zuo and Boyu Ma and Shihao Li and Zhaoyang Xia and Feng Xu and Jianfei Yang", "abstract": "Reliable humanoid-robot interaction (HRI) in household environments is constrained by two fundamental requirements, namely robustness to unconstrained user positions and preservation of user privacy. Millimeter-wave (mmWave) sensing inherently supports privacy-preserving interaction, making it a promising modality for room-scale HRI. However, existing mmWave-based interaction-sensing systems exhibit poor spatial generalization at unseen distances or viewpoints. To address this challenge, we introduce WaveMan, a spatially adaptive room-scale perception system that restores reliable human interaction sensing across arbitrary user positions. WaveMan integrates viewpoint alignment and spectrogram enhancement for spatial consistency, with dual-channel attention for robust feature extraction. Experiments across five participants show that, under fixed-position evaluation, WaveMan achieves the same cross-position accuracy as the baseline with five times fewer training positions. In random free-position testing, accuracy increases from 33.00% to 94.33%, enabled by the proposed method. These results demonstrate the feasibility of reliable, privacy-preserving interaction for household humanoid robots across unconstrained user positions.", "link": "http://arxiv.org/abs/2601.07454v1", "date": "2026-01-12", "relevancy": 2.1107, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5505}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5194}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WaveMan%3A%20mmWave-Based%20Room-Scale%20Human%20Interaction%20Perception%20for%20Humanoid%20Robots&body=Title%3A%20WaveMan%3A%20mmWave-Based%20Room-Scale%20Human%20Interaction%20Perception%20for%20Humanoid%20Robots%0AAuthor%3A%20Yuxuan%20Hu%20and%20Kuangji%20Zuo%20and%20Boyu%20Ma%20and%20Shihao%20Li%20and%20Zhaoyang%20Xia%20and%20Feng%20Xu%20and%20Jianfei%20Yang%0AAbstract%3A%20Reliable%20humanoid-robot%20interaction%20%28HRI%29%20in%20household%20environments%20is%20constrained%20by%20two%20fundamental%20requirements%2C%20namely%20robustness%20to%20unconstrained%20user%20positions%20and%20preservation%20of%20user%20privacy.%20Millimeter-wave%20%28mmWave%29%20sensing%20inherently%20supports%20privacy-preserving%20interaction%2C%20making%20it%20a%20promising%20modality%20for%20room-scale%20HRI.%20However%2C%20existing%20mmWave-based%20interaction-sensing%20systems%20exhibit%20poor%20spatial%20generalization%20at%20unseen%20distances%20or%20viewpoints.%20To%20address%20this%20challenge%2C%20we%20introduce%20WaveMan%2C%20a%20spatially%20adaptive%20room-scale%20perception%20system%20that%20restores%20reliable%20human%20interaction%20sensing%20across%20arbitrary%20user%20positions.%20WaveMan%20integrates%20viewpoint%20alignment%20and%20spectrogram%20enhancement%20for%20spatial%20consistency%2C%20with%20dual-channel%20attention%20for%20robust%20feature%20extraction.%20Experiments%20across%20five%20participants%20show%20that%2C%20under%20fixed-position%20evaluation%2C%20WaveMan%20achieves%20the%20same%20cross-position%20accuracy%20as%20the%20baseline%20with%20five%20times%20fewer%20training%20positions.%20In%20random%20free-position%20testing%2C%20accuracy%20increases%20from%2033.00%25%20to%2094.33%25%2C%20enabled%20by%20the%20proposed%20method.%20These%20results%20demonstrate%20the%20feasibility%20of%20reliable%2C%20privacy-preserving%20interaction%20for%20household%20humanoid%20robots%20across%20unconstrained%20user%20positions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWaveMan%253A%2520mmWave-Based%2520Room-Scale%2520Human%2520Interaction%2520Perception%2520for%2520Humanoid%2520Robots%26entry.906535625%3DYuxuan%2520Hu%2520and%2520Kuangji%2520Zuo%2520and%2520Boyu%2520Ma%2520and%2520Shihao%2520Li%2520and%2520Zhaoyang%2520Xia%2520and%2520Feng%2520Xu%2520and%2520Jianfei%2520Yang%26entry.1292438233%3DReliable%2520humanoid-robot%2520interaction%2520%2528HRI%2529%2520in%2520household%2520environments%2520is%2520constrained%2520by%2520two%2520fundamental%2520requirements%252C%2520namely%2520robustness%2520to%2520unconstrained%2520user%2520positions%2520and%2520preservation%2520of%2520user%2520privacy.%2520Millimeter-wave%2520%2528mmWave%2529%2520sensing%2520inherently%2520supports%2520privacy-preserving%2520interaction%252C%2520making%2520it%2520a%2520promising%2520modality%2520for%2520room-scale%2520HRI.%2520However%252C%2520existing%2520mmWave-based%2520interaction-sensing%2520systems%2520exhibit%2520poor%2520spatial%2520generalization%2520at%2520unseen%2520distances%2520or%2520viewpoints.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520WaveMan%252C%2520a%2520spatially%2520adaptive%2520room-scale%2520perception%2520system%2520that%2520restores%2520reliable%2520human%2520interaction%2520sensing%2520across%2520arbitrary%2520user%2520positions.%2520WaveMan%2520integrates%2520viewpoint%2520alignment%2520and%2520spectrogram%2520enhancement%2520for%2520spatial%2520consistency%252C%2520with%2520dual-channel%2520attention%2520for%2520robust%2520feature%2520extraction.%2520Experiments%2520across%2520five%2520participants%2520show%2520that%252C%2520under%2520fixed-position%2520evaluation%252C%2520WaveMan%2520achieves%2520the%2520same%2520cross-position%2520accuracy%2520as%2520the%2520baseline%2520with%2520five%2520times%2520fewer%2520training%2520positions.%2520In%2520random%2520free-position%2520testing%252C%2520accuracy%2520increases%2520from%252033.00%2525%2520to%252094.33%2525%252C%2520enabled%2520by%2520the%2520proposed%2520method.%2520These%2520results%2520demonstrate%2520the%2520feasibility%2520of%2520reliable%252C%2520privacy-preserving%2520interaction%2520for%2520household%2520humanoid%2520robots%2520across%2520unconstrained%2520user%2520positions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WaveMan%3A%20mmWave-Based%20Room-Scale%20Human%20Interaction%20Perception%20for%20Humanoid%20Robots&entry.906535625=Yuxuan%20Hu%20and%20Kuangji%20Zuo%20and%20Boyu%20Ma%20and%20Shihao%20Li%20and%20Zhaoyang%20Xia%20and%20Feng%20Xu%20and%20Jianfei%20Yang&entry.1292438233=Reliable%20humanoid-robot%20interaction%20%28HRI%29%20in%20household%20environments%20is%20constrained%20by%20two%20fundamental%20requirements%2C%20namely%20robustness%20to%20unconstrained%20user%20positions%20and%20preservation%20of%20user%20privacy.%20Millimeter-wave%20%28mmWave%29%20sensing%20inherently%20supports%20privacy-preserving%20interaction%2C%20making%20it%20a%20promising%20modality%20for%20room-scale%20HRI.%20However%2C%20existing%20mmWave-based%20interaction-sensing%20systems%20exhibit%20poor%20spatial%20generalization%20at%20unseen%20distances%20or%20viewpoints.%20To%20address%20this%20challenge%2C%20we%20introduce%20WaveMan%2C%20a%20spatially%20adaptive%20room-scale%20perception%20system%20that%20restores%20reliable%20human%20interaction%20sensing%20across%20arbitrary%20user%20positions.%20WaveMan%20integrates%20viewpoint%20alignment%20and%20spectrogram%20enhancement%20for%20spatial%20consistency%2C%20with%20dual-channel%20attention%20for%20robust%20feature%20extraction.%20Experiments%20across%20five%20participants%20show%20that%2C%20under%20fixed-position%20evaluation%2C%20WaveMan%20achieves%20the%20same%20cross-position%20accuracy%20as%20the%20baseline%20with%20five%20times%20fewer%20training%20positions.%20In%20random%20free-position%20testing%2C%20accuracy%20increases%20from%2033.00%25%20to%2094.33%25%2C%20enabled%20by%20the%20proposed%20method.%20These%20results%20demonstrate%20the%20feasibility%20of%20reliable%2C%20privacy-preserving%20interaction%20for%20household%20humanoid%20robots%20across%20unconstrained%20user%20positions.&entry.1838667208=http%3A//arxiv.org/abs/2601.07454v1&entry.124074799=Read"},
{"title": "Learning to bin: differentiable and Bayesian optimization for multi-dimensional discriminants in high-energy physics", "author": "Johannes Erdmann and Nitish Kumar Kasaraguppe and Florian Mausolf", "abstract": "Categorizing events using discriminant observables is central to many high-energy physics analyses. Yet, bin boundaries are often chosen by hand. A simple, popular choice is to apply argmax projections of multi-class scores and equidistant binning of one-dimensional discriminants. We propose a binning optimization for signal significance directly in multi-dimensional discriminants. We use a Gaussian Mixture Model (GMM) to define flexible bin boundary shapes for multi-class scores, while in one dimension (binary classification) we move bin boundaries directly. On this binning model, we study two optimization strategies: a differentiable and a Bayesian optimization approach. We study two toy setups: a binary classification and a three-class problem with two signals and backgrounds. In the one-dimensional case, both approaches achieve similar gains in signal sensitivity compared to equidistant binnings for a given number of bins. In the multi-dimensional case, the GMM-based binning defines sensitive categories as well, with the differentiable approach performing best. We show that, in particular for limited separability of the signal processes, our approach outperforms argmax classification even with optimized binning in the one-dimensional projections. Both methods are released as lightweight Python plugins intended for straightforward integration into existing analyses.", "link": "http://arxiv.org/abs/2601.07756v1", "date": "2026-01-12", "relevancy": 2.0775, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5308}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5175}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20bin%3A%20differentiable%20and%20Bayesian%20optimization%20for%20multi-dimensional%20discriminants%20in%20high-energy%20physics&body=Title%3A%20Learning%20to%20bin%3A%20differentiable%20and%20Bayesian%20optimization%20for%20multi-dimensional%20discriminants%20in%20high-energy%20physics%0AAuthor%3A%20Johannes%20Erdmann%20and%20Nitish%20Kumar%20Kasaraguppe%20and%20Florian%20Mausolf%0AAbstract%3A%20Categorizing%20events%20using%20discriminant%20observables%20is%20central%20to%20many%20high-energy%20physics%20analyses.%20Yet%2C%20bin%20boundaries%20are%20often%20chosen%20by%20hand.%20A%20simple%2C%20popular%20choice%20is%20to%20apply%20argmax%20projections%20of%20multi-class%20scores%20and%20equidistant%20binning%20of%20one-dimensional%20discriminants.%20We%20propose%20a%20binning%20optimization%20for%20signal%20significance%20directly%20in%20multi-dimensional%20discriminants.%20We%20use%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20to%20define%20flexible%20bin%20boundary%20shapes%20for%20multi-class%20scores%2C%20while%20in%20one%20dimension%20%28binary%20classification%29%20we%20move%20bin%20boundaries%20directly.%20On%20this%20binning%20model%2C%20we%20study%20two%20optimization%20strategies%3A%20a%20differentiable%20and%20a%20Bayesian%20optimization%20approach.%20We%20study%20two%20toy%20setups%3A%20a%20binary%20classification%20and%20a%20three-class%20problem%20with%20two%20signals%20and%20backgrounds.%20In%20the%20one-dimensional%20case%2C%20both%20approaches%20achieve%20similar%20gains%20in%20signal%20sensitivity%20compared%20to%20equidistant%20binnings%20for%20a%20given%20number%20of%20bins.%20In%20the%20multi-dimensional%20case%2C%20the%20GMM-based%20binning%20defines%20sensitive%20categories%20as%20well%2C%20with%20the%20differentiable%20approach%20performing%20best.%20We%20show%20that%2C%20in%20particular%20for%20limited%20separability%20of%20the%20signal%20processes%2C%20our%20approach%20outperforms%20argmax%20classification%20even%20with%20optimized%20binning%20in%20the%20one-dimensional%20projections.%20Both%20methods%20are%20released%20as%20lightweight%20Python%20plugins%20intended%20for%20straightforward%20integration%20into%20existing%20analyses.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520bin%253A%2520differentiable%2520and%2520Bayesian%2520optimization%2520for%2520multi-dimensional%2520discriminants%2520in%2520high-energy%2520physics%26entry.906535625%3DJohannes%2520Erdmann%2520and%2520Nitish%2520Kumar%2520Kasaraguppe%2520and%2520Florian%2520Mausolf%26entry.1292438233%3DCategorizing%2520events%2520using%2520discriminant%2520observables%2520is%2520central%2520to%2520many%2520high-energy%2520physics%2520analyses.%2520Yet%252C%2520bin%2520boundaries%2520are%2520often%2520chosen%2520by%2520hand.%2520A%2520simple%252C%2520popular%2520choice%2520is%2520to%2520apply%2520argmax%2520projections%2520of%2520multi-class%2520scores%2520and%2520equidistant%2520binning%2520of%2520one-dimensional%2520discriminants.%2520We%2520propose%2520a%2520binning%2520optimization%2520for%2520signal%2520significance%2520directly%2520in%2520multi-dimensional%2520discriminants.%2520We%2520use%2520a%2520Gaussian%2520Mixture%2520Model%2520%2528GMM%2529%2520to%2520define%2520flexible%2520bin%2520boundary%2520shapes%2520for%2520multi-class%2520scores%252C%2520while%2520in%2520one%2520dimension%2520%2528binary%2520classification%2529%2520we%2520move%2520bin%2520boundaries%2520directly.%2520On%2520this%2520binning%2520model%252C%2520we%2520study%2520two%2520optimization%2520strategies%253A%2520a%2520differentiable%2520and%2520a%2520Bayesian%2520optimization%2520approach.%2520We%2520study%2520two%2520toy%2520setups%253A%2520a%2520binary%2520classification%2520and%2520a%2520three-class%2520problem%2520with%2520two%2520signals%2520and%2520backgrounds.%2520In%2520the%2520one-dimensional%2520case%252C%2520both%2520approaches%2520achieve%2520similar%2520gains%2520in%2520signal%2520sensitivity%2520compared%2520to%2520equidistant%2520binnings%2520for%2520a%2520given%2520number%2520of%2520bins.%2520In%2520the%2520multi-dimensional%2520case%252C%2520the%2520GMM-based%2520binning%2520defines%2520sensitive%2520categories%2520as%2520well%252C%2520with%2520the%2520differentiable%2520approach%2520performing%2520best.%2520We%2520show%2520that%252C%2520in%2520particular%2520for%2520limited%2520separability%2520of%2520the%2520signal%2520processes%252C%2520our%2520approach%2520outperforms%2520argmax%2520classification%2520even%2520with%2520optimized%2520binning%2520in%2520the%2520one-dimensional%2520projections.%2520Both%2520methods%2520are%2520released%2520as%2520lightweight%2520Python%2520plugins%2520intended%2520for%2520straightforward%2520integration%2520into%2520existing%2520analyses.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20bin%3A%20differentiable%20and%20Bayesian%20optimization%20for%20multi-dimensional%20discriminants%20in%20high-energy%20physics&entry.906535625=Johannes%20Erdmann%20and%20Nitish%20Kumar%20Kasaraguppe%20and%20Florian%20Mausolf&entry.1292438233=Categorizing%20events%20using%20discriminant%20observables%20is%20central%20to%20many%20high-energy%20physics%20analyses.%20Yet%2C%20bin%20boundaries%20are%20often%20chosen%20by%20hand.%20A%20simple%2C%20popular%20choice%20is%20to%20apply%20argmax%20projections%20of%20multi-class%20scores%20and%20equidistant%20binning%20of%20one-dimensional%20discriminants.%20We%20propose%20a%20binning%20optimization%20for%20signal%20significance%20directly%20in%20multi-dimensional%20discriminants.%20We%20use%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20to%20define%20flexible%20bin%20boundary%20shapes%20for%20multi-class%20scores%2C%20while%20in%20one%20dimension%20%28binary%20classification%29%20we%20move%20bin%20boundaries%20directly.%20On%20this%20binning%20model%2C%20we%20study%20two%20optimization%20strategies%3A%20a%20differentiable%20and%20a%20Bayesian%20optimization%20approach.%20We%20study%20two%20toy%20setups%3A%20a%20binary%20classification%20and%20a%20three-class%20problem%20with%20two%20signals%20and%20backgrounds.%20In%20the%20one-dimensional%20case%2C%20both%20approaches%20achieve%20similar%20gains%20in%20signal%20sensitivity%20compared%20to%20equidistant%20binnings%20for%20a%20given%20number%20of%20bins.%20In%20the%20multi-dimensional%20case%2C%20the%20GMM-based%20binning%20defines%20sensitive%20categories%20as%20well%2C%20with%20the%20differentiable%20approach%20performing%20best.%20We%20show%20that%2C%20in%20particular%20for%20limited%20separability%20of%20the%20signal%20processes%2C%20our%20approach%20outperforms%20argmax%20classification%20even%20with%20optimized%20binning%20in%20the%20one-dimensional%20projections.%20Both%20methods%20are%20released%20as%20lightweight%20Python%20plugins%20intended%20for%20straightforward%20integration%20into%20existing%20analyses.&entry.1838667208=http%3A//arxiv.org/abs/2601.07756v1&entry.124074799=Read"},
{"title": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models", "author": "Xin Cheng and Wangding Zeng and Damai Dai and Qinyu Chen and Bingxuan Wang and Zhenda Xie and Kezhao Huang and Xingkai Yu and Zhewen Hao and Yukun Li and Han Zhang and Huishuai Zhang and Dongyan Zhao and Wenfeng Liang", "abstract": "While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup, forcing them to inefficiently simulate retrieval through computation. To address this, we introduce conditional memory as a complementary sparsity axis, instantiated via Engram, a module that modernizes classic $N$-gram embedding for O(1) lookup. By formulating the Sparsity Allocation problem, we uncover a U-shaped scaling law that optimizes the trade-off between neural computation (MoE) and static memory (Engram). Guided by this law, we scale Engram to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. Most notably, while the memory module is expected to aid knowledge retrieval (e.g., MMLU +3.4; CMMLU +4.0), we observe even larger gains in general reasoning (e.g., BBH +5.0; ARC-Challenge +3.7) and code/math domains~(HumanEval +3.0; MATH +2.4). Mechanistic analyses reveal that Engram relieves the backbone's early layers from static reconstruction, effectively deepening the network for complex reasoning. Furthermore, by delegating local dependencies to lookups, it frees up attention capacity for global context, substantially boosting long-context retrieval (e.g., Multi-Query NIAH: 84.2 to 97.0). Finally, Engram establishes infrastructure-aware efficiency: its deterministic addressing enables runtime prefetching from host memory, incurring negligible overhead. We envision conditional memory as an indispensable modeling primitive for next-generation sparse models.", "link": "http://arxiv.org/abs/2601.07372v1", "date": "2026-01-12", "relevancy": 2.0735, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5221}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5221}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditional%20Memory%20via%20Scalable%20Lookup%3A%20A%20New%20Axis%20of%20Sparsity%20for%20Large%20Language%20Models&body=Title%3A%20Conditional%20Memory%20via%20Scalable%20Lookup%3A%20A%20New%20Axis%20of%20Sparsity%20for%20Large%20Language%20Models%0AAuthor%3A%20Xin%20Cheng%20and%20Wangding%20Zeng%20and%20Damai%20Dai%20and%20Qinyu%20Chen%20and%20Bingxuan%20Wang%20and%20Zhenda%20Xie%20and%20Kezhao%20Huang%20and%20Xingkai%20Yu%20and%20Zhewen%20Hao%20and%20Yukun%20Li%20and%20Han%20Zhang%20and%20Huishuai%20Zhang%20and%20Dongyan%20Zhao%20and%20Wenfeng%20Liang%0AAbstract%3A%20While%20Mixture-of-Experts%20%28MoE%29%20scales%20capacity%20via%20conditional%20computation%2C%20Transformers%20lack%20a%20native%20primitive%20for%20knowledge%20lookup%2C%20forcing%20them%20to%20inefficiently%20simulate%20retrieval%20through%20computation.%20To%20address%20this%2C%20we%20introduce%20conditional%20memory%20as%20a%20complementary%20sparsity%20axis%2C%20instantiated%20via%20Engram%2C%20a%20module%20that%20modernizes%20classic%20%24N%24-gram%20embedding%20for%20O%281%29%20lookup.%20By%20formulating%20the%20Sparsity%20Allocation%20problem%2C%20we%20uncover%20a%20U-shaped%20scaling%20law%20that%20optimizes%20the%20trade-off%20between%20neural%20computation%20%28MoE%29%20and%20static%20memory%20%28Engram%29.%20Guided%20by%20this%20law%2C%20we%20scale%20Engram%20to%2027B%20parameters%2C%20achieving%20superior%20performance%20over%20a%20strictly%20iso-parameter%20and%20iso-FLOPs%20MoE%20baseline.%20Most%20notably%2C%20while%20the%20memory%20module%20is%20expected%20to%20aid%20knowledge%20retrieval%20%28e.g.%2C%20MMLU%20%2B3.4%3B%20CMMLU%20%2B4.0%29%2C%20we%20observe%20even%20larger%20gains%20in%20general%20reasoning%20%28e.g.%2C%20BBH%20%2B5.0%3B%20ARC-Challenge%20%2B3.7%29%20and%20code/math%20domains~%28HumanEval%20%2B3.0%3B%20MATH%20%2B2.4%29.%20Mechanistic%20analyses%20reveal%20that%20Engram%20relieves%20the%20backbone%27s%20early%20layers%20from%20static%20reconstruction%2C%20effectively%20deepening%20the%20network%20for%20complex%20reasoning.%20Furthermore%2C%20by%20delegating%20local%20dependencies%20to%20lookups%2C%20it%20frees%20up%20attention%20capacity%20for%20global%20context%2C%20substantially%20boosting%20long-context%20retrieval%20%28e.g.%2C%20Multi-Query%20NIAH%3A%2084.2%20to%2097.0%29.%20Finally%2C%20Engram%20establishes%20infrastructure-aware%20efficiency%3A%20its%20deterministic%20addressing%20enables%20runtime%20prefetching%20from%20host%20memory%2C%20incurring%20negligible%20overhead.%20We%20envision%20conditional%20memory%20as%20an%20indispensable%20modeling%20primitive%20for%20next-generation%20sparse%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditional%2520Memory%2520via%2520Scalable%2520Lookup%253A%2520A%2520New%2520Axis%2520of%2520Sparsity%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DXin%2520Cheng%2520and%2520Wangding%2520Zeng%2520and%2520Damai%2520Dai%2520and%2520Qinyu%2520Chen%2520and%2520Bingxuan%2520Wang%2520and%2520Zhenda%2520Xie%2520and%2520Kezhao%2520Huang%2520and%2520Xingkai%2520Yu%2520and%2520Zhewen%2520Hao%2520and%2520Yukun%2520Li%2520and%2520Han%2520Zhang%2520and%2520Huishuai%2520Zhang%2520and%2520Dongyan%2520Zhao%2520and%2520Wenfeng%2520Liang%26entry.1292438233%3DWhile%2520Mixture-of-Experts%2520%2528MoE%2529%2520scales%2520capacity%2520via%2520conditional%2520computation%252C%2520Transformers%2520lack%2520a%2520native%2520primitive%2520for%2520knowledge%2520lookup%252C%2520forcing%2520them%2520to%2520inefficiently%2520simulate%2520retrieval%2520through%2520computation.%2520To%2520address%2520this%252C%2520we%2520introduce%2520conditional%2520memory%2520as%2520a%2520complementary%2520sparsity%2520axis%252C%2520instantiated%2520via%2520Engram%252C%2520a%2520module%2520that%2520modernizes%2520classic%2520%2524N%2524-gram%2520embedding%2520for%2520O%25281%2529%2520lookup.%2520By%2520formulating%2520the%2520Sparsity%2520Allocation%2520problem%252C%2520we%2520uncover%2520a%2520U-shaped%2520scaling%2520law%2520that%2520optimizes%2520the%2520trade-off%2520between%2520neural%2520computation%2520%2528MoE%2529%2520and%2520static%2520memory%2520%2528Engram%2529.%2520Guided%2520by%2520this%2520law%252C%2520we%2520scale%2520Engram%2520to%252027B%2520parameters%252C%2520achieving%2520superior%2520performance%2520over%2520a%2520strictly%2520iso-parameter%2520and%2520iso-FLOPs%2520MoE%2520baseline.%2520Most%2520notably%252C%2520while%2520the%2520memory%2520module%2520is%2520expected%2520to%2520aid%2520knowledge%2520retrieval%2520%2528e.g.%252C%2520MMLU%2520%252B3.4%253B%2520CMMLU%2520%252B4.0%2529%252C%2520we%2520observe%2520even%2520larger%2520gains%2520in%2520general%2520reasoning%2520%2528e.g.%252C%2520BBH%2520%252B5.0%253B%2520ARC-Challenge%2520%252B3.7%2529%2520and%2520code/math%2520domains~%2528HumanEval%2520%252B3.0%253B%2520MATH%2520%252B2.4%2529.%2520Mechanistic%2520analyses%2520reveal%2520that%2520Engram%2520relieves%2520the%2520backbone%2527s%2520early%2520layers%2520from%2520static%2520reconstruction%252C%2520effectively%2520deepening%2520the%2520network%2520for%2520complex%2520reasoning.%2520Furthermore%252C%2520by%2520delegating%2520local%2520dependencies%2520to%2520lookups%252C%2520it%2520frees%2520up%2520attention%2520capacity%2520for%2520global%2520context%252C%2520substantially%2520boosting%2520long-context%2520retrieval%2520%2528e.g.%252C%2520Multi-Query%2520NIAH%253A%252084.2%2520to%252097.0%2529.%2520Finally%252C%2520Engram%2520establishes%2520infrastructure-aware%2520efficiency%253A%2520its%2520deterministic%2520addressing%2520enables%2520runtime%2520prefetching%2520from%2520host%2520memory%252C%2520incurring%2520negligible%2520overhead.%2520We%2520envision%2520conditional%2520memory%2520as%2520an%2520indispensable%2520modeling%2520primitive%2520for%2520next-generation%2520sparse%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Memory%20via%20Scalable%20Lookup%3A%20A%20New%20Axis%20of%20Sparsity%20for%20Large%20Language%20Models&entry.906535625=Xin%20Cheng%20and%20Wangding%20Zeng%20and%20Damai%20Dai%20and%20Qinyu%20Chen%20and%20Bingxuan%20Wang%20and%20Zhenda%20Xie%20and%20Kezhao%20Huang%20and%20Xingkai%20Yu%20and%20Zhewen%20Hao%20and%20Yukun%20Li%20and%20Han%20Zhang%20and%20Huishuai%20Zhang%20and%20Dongyan%20Zhao%20and%20Wenfeng%20Liang&entry.1292438233=While%20Mixture-of-Experts%20%28MoE%29%20scales%20capacity%20via%20conditional%20computation%2C%20Transformers%20lack%20a%20native%20primitive%20for%20knowledge%20lookup%2C%20forcing%20them%20to%20inefficiently%20simulate%20retrieval%20through%20computation.%20To%20address%20this%2C%20we%20introduce%20conditional%20memory%20as%20a%20complementary%20sparsity%20axis%2C%20instantiated%20via%20Engram%2C%20a%20module%20that%20modernizes%20classic%20%24N%24-gram%20embedding%20for%20O%281%29%20lookup.%20By%20formulating%20the%20Sparsity%20Allocation%20problem%2C%20we%20uncover%20a%20U-shaped%20scaling%20law%20that%20optimizes%20the%20trade-off%20between%20neural%20computation%20%28MoE%29%20and%20static%20memory%20%28Engram%29.%20Guided%20by%20this%20law%2C%20we%20scale%20Engram%20to%2027B%20parameters%2C%20achieving%20superior%20performance%20over%20a%20strictly%20iso-parameter%20and%20iso-FLOPs%20MoE%20baseline.%20Most%20notably%2C%20while%20the%20memory%20module%20is%20expected%20to%20aid%20knowledge%20retrieval%20%28e.g.%2C%20MMLU%20%2B3.4%3B%20CMMLU%20%2B4.0%29%2C%20we%20observe%20even%20larger%20gains%20in%20general%20reasoning%20%28e.g.%2C%20BBH%20%2B5.0%3B%20ARC-Challenge%20%2B3.7%29%20and%20code/math%20domains~%28HumanEval%20%2B3.0%3B%20MATH%20%2B2.4%29.%20Mechanistic%20analyses%20reveal%20that%20Engram%20relieves%20the%20backbone%27s%20early%20layers%20from%20static%20reconstruction%2C%20effectively%20deepening%20the%20network%20for%20complex%20reasoning.%20Furthermore%2C%20by%20delegating%20local%20dependencies%20to%20lookups%2C%20it%20frees%20up%20attention%20capacity%20for%20global%20context%2C%20substantially%20boosting%20long-context%20retrieval%20%28e.g.%2C%20Multi-Query%20NIAH%3A%2084.2%20to%2097.0%29.%20Finally%2C%20Engram%20establishes%20infrastructure-aware%20efficiency%3A%20its%20deterministic%20addressing%20enables%20runtime%20prefetching%20from%20host%20memory%2C%20incurring%20negligible%20overhead.%20We%20envision%20conditional%20memory%20as%20an%20indispensable%20modeling%20primitive%20for%20next-generation%20sparse%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.07372v1&entry.124074799=Read"},
{"title": "Towards Mitigating Excessive Forgetting in LLM Unlearning via Entanglement-Guidance with Proxy Constraint", "author": "Zhihao Liu and Jian Lou and Yuke Hu and Xiaochen Li and Yitian Chen and Tailun Chen and Zhizhen Qin and Kui Ren and Zhan Qin", "abstract": "Large language models (LLMs) are trained on massive datasets that may include private or copyrighted content. Due to growing privacy and ownership concerns, data owners may request the removal of their data from trained models. Machine unlearning provides a practical solution by removing the influence of specific data without full retraining. However, most existing methods still suffer from over-unlearning due to the lack of a principled mechanism to regulate the forgetting boundary, leading to unnecessary utility degradation and heightened privacy and robustness risks. In this work, we propose EGUP (Entanglement-Guided Unlearning with Proxy Constraint), a novel framework that leverages entanglement and proxy constraint to guide the unlearning process while mitigating over-unlearning. Within each iteration, EGUP employs inter-sample entanglement to adaptively reweight the unlearning strength, assigning greater unlearning efforts to forget samples that are semantically closer to retained knowledge. Across iterations, EGUP leverages intra-sample entanglement to track the representation shift of each forget sample and dynamically adjust its unlearning effort. In addition, we incorporate a proxy constraint that approximates the model's expected outputs after unlearning, forming a reference boundary that softly regularizes the unlearning process. EGUP is compatible with existing gradient-based objectives and serves as a plug-and-play enhancement. We evaluate EGUP on the TOFU and MUSE benchmarks, demonstrating consistent improvements in the unlearning-utility trade-off across multiple LLMs. Moreover, EGUP achieves performance close to the retrained model while remaining scalable and robust.", "link": "http://arxiv.org/abs/2508.20443v2", "date": "2026-01-12", "relevancy": 2.0692, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.569}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5243}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Mitigating%20Excessive%20Forgetting%20in%20LLM%20Unlearning%20via%20Entanglement-Guidance%20with%20Proxy%20Constraint&body=Title%3A%20Towards%20Mitigating%20Excessive%20Forgetting%20in%20LLM%20Unlearning%20via%20Entanglement-Guidance%20with%20Proxy%20Constraint%0AAuthor%3A%20Zhihao%20Liu%20and%20Jian%20Lou%20and%20Yuke%20Hu%20and%20Xiaochen%20Li%20and%20Yitian%20Chen%20and%20Tailun%20Chen%20and%20Zhizhen%20Qin%20and%20Kui%20Ren%20and%20Zhan%20Qin%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20trained%20on%20massive%20datasets%20that%20may%20include%20private%20or%20copyrighted%20content.%20Due%20to%20growing%20privacy%20and%20ownership%20concerns%2C%20data%20owners%20may%20request%20the%20removal%20of%20their%20data%20from%20trained%20models.%20Machine%20unlearning%20provides%20a%20practical%20solution%20by%20removing%20the%20influence%20of%20specific%20data%20without%20full%20retraining.%20However%2C%20most%20existing%20methods%20still%20suffer%20from%20over-unlearning%20due%20to%20the%20lack%20of%20a%20principled%20mechanism%20to%20regulate%20the%20forgetting%20boundary%2C%20leading%20to%20unnecessary%20utility%20degradation%20and%20heightened%20privacy%20and%20robustness%20risks.%20In%20this%20work%2C%20we%20propose%20EGUP%20%28Entanglement-Guided%20Unlearning%20with%20Proxy%20Constraint%29%2C%20a%20novel%20framework%20that%20leverages%20entanglement%20and%20proxy%20constraint%20to%20guide%20the%20unlearning%20process%20while%20mitigating%20over-unlearning.%20Within%20each%20iteration%2C%20EGUP%20employs%20inter-sample%20entanglement%20to%20adaptively%20reweight%20the%20unlearning%20strength%2C%20assigning%20greater%20unlearning%20efforts%20to%20forget%20samples%20that%20are%20semantically%20closer%20to%20retained%20knowledge.%20Across%20iterations%2C%20EGUP%20leverages%20intra-sample%20entanglement%20to%20track%20the%20representation%20shift%20of%20each%20forget%20sample%20and%20dynamically%20adjust%20its%20unlearning%20effort.%20In%20addition%2C%20we%20incorporate%20a%20proxy%20constraint%20that%20approximates%20the%20model%27s%20expected%20outputs%20after%20unlearning%2C%20forming%20a%20reference%20boundary%20that%20softly%20regularizes%20the%20unlearning%20process.%20EGUP%20is%20compatible%20with%20existing%20gradient-based%20objectives%20and%20serves%20as%20a%20plug-and-play%20enhancement.%20We%20evaluate%20EGUP%20on%20the%20TOFU%20and%20MUSE%20benchmarks%2C%20demonstrating%20consistent%20improvements%20in%20the%20unlearning-utility%20trade-off%20across%20multiple%20LLMs.%20Moreover%2C%20EGUP%20achieves%20performance%20close%20to%20the%20retrained%20model%20while%20remaining%20scalable%20and%20robust.%0ALink%3A%20http%3A//arxiv.org/abs/2508.20443v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Mitigating%2520Excessive%2520Forgetting%2520in%2520LLM%2520Unlearning%2520via%2520Entanglement-Guidance%2520with%2520Proxy%2520Constraint%26entry.906535625%3DZhihao%2520Liu%2520and%2520Jian%2520Lou%2520and%2520Yuke%2520Hu%2520and%2520Xiaochen%2520Li%2520and%2520Yitian%2520Chen%2520and%2520Tailun%2520Chen%2520and%2520Zhizhen%2520Qin%2520and%2520Kui%2520Ren%2520and%2520Zhan%2520Qin%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520trained%2520on%2520massive%2520datasets%2520that%2520may%2520include%2520private%2520or%2520copyrighted%2520content.%2520Due%2520to%2520growing%2520privacy%2520and%2520ownership%2520concerns%252C%2520data%2520owners%2520may%2520request%2520the%2520removal%2520of%2520their%2520data%2520from%2520trained%2520models.%2520Machine%2520unlearning%2520provides%2520a%2520practical%2520solution%2520by%2520removing%2520the%2520influence%2520of%2520specific%2520data%2520without%2520full%2520retraining.%2520However%252C%2520most%2520existing%2520methods%2520still%2520suffer%2520from%2520over-unlearning%2520due%2520to%2520the%2520lack%2520of%2520a%2520principled%2520mechanism%2520to%2520regulate%2520the%2520forgetting%2520boundary%252C%2520leading%2520to%2520unnecessary%2520utility%2520degradation%2520and%2520heightened%2520privacy%2520and%2520robustness%2520risks.%2520In%2520this%2520work%252C%2520we%2520propose%2520EGUP%2520%2528Entanglement-Guided%2520Unlearning%2520with%2520Proxy%2520Constraint%2529%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520entanglement%2520and%2520proxy%2520constraint%2520to%2520guide%2520the%2520unlearning%2520process%2520while%2520mitigating%2520over-unlearning.%2520Within%2520each%2520iteration%252C%2520EGUP%2520employs%2520inter-sample%2520entanglement%2520to%2520adaptively%2520reweight%2520the%2520unlearning%2520strength%252C%2520assigning%2520greater%2520unlearning%2520efforts%2520to%2520forget%2520samples%2520that%2520are%2520semantically%2520closer%2520to%2520retained%2520knowledge.%2520Across%2520iterations%252C%2520EGUP%2520leverages%2520intra-sample%2520entanglement%2520to%2520track%2520the%2520representation%2520shift%2520of%2520each%2520forget%2520sample%2520and%2520dynamically%2520adjust%2520its%2520unlearning%2520effort.%2520In%2520addition%252C%2520we%2520incorporate%2520a%2520proxy%2520constraint%2520that%2520approximates%2520the%2520model%2527s%2520expected%2520outputs%2520after%2520unlearning%252C%2520forming%2520a%2520reference%2520boundary%2520that%2520softly%2520regularizes%2520the%2520unlearning%2520process.%2520EGUP%2520is%2520compatible%2520with%2520existing%2520gradient-based%2520objectives%2520and%2520serves%2520as%2520a%2520plug-and-play%2520enhancement.%2520We%2520evaluate%2520EGUP%2520on%2520the%2520TOFU%2520and%2520MUSE%2520benchmarks%252C%2520demonstrating%2520consistent%2520improvements%2520in%2520the%2520unlearning-utility%2520trade-off%2520across%2520multiple%2520LLMs.%2520Moreover%252C%2520EGUP%2520achieves%2520performance%2520close%2520to%2520the%2520retrained%2520model%2520while%2520remaining%2520scalable%2520and%2520robust.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20443v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Mitigating%20Excessive%20Forgetting%20in%20LLM%20Unlearning%20via%20Entanglement-Guidance%20with%20Proxy%20Constraint&entry.906535625=Zhihao%20Liu%20and%20Jian%20Lou%20and%20Yuke%20Hu%20and%20Xiaochen%20Li%20and%20Yitian%20Chen%20and%20Tailun%20Chen%20and%20Zhizhen%20Qin%20and%20Kui%20Ren%20and%20Zhan%20Qin&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20trained%20on%20massive%20datasets%20that%20may%20include%20private%20or%20copyrighted%20content.%20Due%20to%20growing%20privacy%20and%20ownership%20concerns%2C%20data%20owners%20may%20request%20the%20removal%20of%20their%20data%20from%20trained%20models.%20Machine%20unlearning%20provides%20a%20practical%20solution%20by%20removing%20the%20influence%20of%20specific%20data%20without%20full%20retraining.%20However%2C%20most%20existing%20methods%20still%20suffer%20from%20over-unlearning%20due%20to%20the%20lack%20of%20a%20principled%20mechanism%20to%20regulate%20the%20forgetting%20boundary%2C%20leading%20to%20unnecessary%20utility%20degradation%20and%20heightened%20privacy%20and%20robustness%20risks.%20In%20this%20work%2C%20we%20propose%20EGUP%20%28Entanglement-Guided%20Unlearning%20with%20Proxy%20Constraint%29%2C%20a%20novel%20framework%20that%20leverages%20entanglement%20and%20proxy%20constraint%20to%20guide%20the%20unlearning%20process%20while%20mitigating%20over-unlearning.%20Within%20each%20iteration%2C%20EGUP%20employs%20inter-sample%20entanglement%20to%20adaptively%20reweight%20the%20unlearning%20strength%2C%20assigning%20greater%20unlearning%20efforts%20to%20forget%20samples%20that%20are%20semantically%20closer%20to%20retained%20knowledge.%20Across%20iterations%2C%20EGUP%20leverages%20intra-sample%20entanglement%20to%20track%20the%20representation%20shift%20of%20each%20forget%20sample%20and%20dynamically%20adjust%20its%20unlearning%20effort.%20In%20addition%2C%20we%20incorporate%20a%20proxy%20constraint%20that%20approximates%20the%20model%27s%20expected%20outputs%20after%20unlearning%2C%20forming%20a%20reference%20boundary%20that%20softly%20regularizes%20the%20unlearning%20process.%20EGUP%20is%20compatible%20with%20existing%20gradient-based%20objectives%20and%20serves%20as%20a%20plug-and-play%20enhancement.%20We%20evaluate%20EGUP%20on%20the%20TOFU%20and%20MUSE%20benchmarks%2C%20demonstrating%20consistent%20improvements%20in%20the%20unlearning-utility%20trade-off%20across%20multiple%20LLMs.%20Moreover%2C%20EGUP%20achieves%20performance%20close%20to%20the%20retrained%20model%20while%20remaining%20scalable%20and%20robust.&entry.1838667208=http%3A//arxiv.org/abs/2508.20443v2&entry.124074799=Read"},
{"title": "MiCo: Multiple Instance Learning with Context-Aware Clustering for Whole Slide Image Analysis", "author": "Junjian Li and Jin Liu and Hulin Kuang and Hailin Yue and Mengshen He and Jianxin Wang", "abstract": "Multiple instance learning (MIL) has shown significant promise in histopathology whole slide image (WSI) analysis for cancer diagnosis and prognosis. However, the inherent spatial heterogeneity of WSIs presents critical challenges, as morphologically similar tissue types are often dispersed across distant anatomical regions. Conventional MIL methods struggle to model these scattered tissue distributions and capture cross-regional spatial interactions effectively. To address these limitations, we propose a novel Multiple instance learning framework with Context-Aware Clustering (MiCo), designed to enhance cross-regional intra-tissue correlations and strengthen inter-tissue semantic associations in WSIs. MiCo begins by clustering instances to distill discriminative morphological patterns, with cluster centroids serving as semantic anchors. To enhance cross-regional intra-tissue correlations, MiCo employs a Cluster Route module, which dynamically links instances of the same tissue type across distant regions via feature similarity. These semantic anchors act as contextual hubs, propagating semantic relationships to refine instance-level representations. To eliminate semantic fragmentation and strengthen inter-tissue semantic associations, MiCo integrates a Cluster Reducer module, which consolidates redundant anchors while enhancing information exchange between distinct semantic groups. Extensive experiments on two challenging tasks across nine large-scale public cancer datasets demonstrate the effectiveness of MiCo, showcasing its superiority over state-of-the-art methods. The code is available at https://github.com/junjianli106/MiCo.", "link": "http://arxiv.org/abs/2506.18028v3", "date": "2026-01-12", "relevancy": 2.0682, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5363}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5115}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MiCo%3A%20Multiple%20Instance%20Learning%20with%20Context-Aware%20Clustering%20for%20Whole%20Slide%20Image%20Analysis&body=Title%3A%20MiCo%3A%20Multiple%20Instance%20Learning%20with%20Context-Aware%20Clustering%20for%20Whole%20Slide%20Image%20Analysis%0AAuthor%3A%20Junjian%20Li%20and%20Jin%20Liu%20and%20Hulin%20Kuang%20and%20Hailin%20Yue%20and%20Mengshen%20He%20and%20Jianxin%20Wang%0AAbstract%3A%20Multiple%20instance%20learning%20%28MIL%29%20has%20shown%20significant%20promise%20in%20histopathology%20whole%20slide%20image%20%28WSI%29%20analysis%20for%20cancer%20diagnosis%20and%20prognosis.%20However%2C%20the%20inherent%20spatial%20heterogeneity%20of%20WSIs%20presents%20critical%20challenges%2C%20as%20morphologically%20similar%20tissue%20types%20are%20often%20dispersed%20across%20distant%20anatomical%20regions.%20Conventional%20MIL%20methods%20struggle%20to%20model%20these%20scattered%20tissue%20distributions%20and%20capture%20cross-regional%20spatial%20interactions%20effectively.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20Multiple%20instance%20learning%20framework%20with%20Context-Aware%20Clustering%20%28MiCo%29%2C%20designed%20to%20enhance%20cross-regional%20intra-tissue%20correlations%20and%20strengthen%20inter-tissue%20semantic%20associations%20in%20WSIs.%20MiCo%20begins%20by%20clustering%20instances%20to%20distill%20discriminative%20morphological%20patterns%2C%20with%20cluster%20centroids%20serving%20as%20semantic%20anchors.%20To%20enhance%20cross-regional%20intra-tissue%20correlations%2C%20MiCo%20employs%20a%20Cluster%20Route%20module%2C%20which%20dynamically%20links%20instances%20of%20the%20same%20tissue%20type%20across%20distant%20regions%20via%20feature%20similarity.%20These%20semantic%20anchors%20act%20as%20contextual%20hubs%2C%20propagating%20semantic%20relationships%20to%20refine%20instance-level%20representations.%20To%20eliminate%20semantic%20fragmentation%20and%20strengthen%20inter-tissue%20semantic%20associations%2C%20MiCo%20integrates%20a%20Cluster%20Reducer%20module%2C%20which%20consolidates%20redundant%20anchors%20while%20enhancing%20information%20exchange%20between%20distinct%20semantic%20groups.%20Extensive%20experiments%20on%20two%20challenging%20tasks%20across%20nine%20large-scale%20public%20cancer%20datasets%20demonstrate%20the%20effectiveness%20of%20MiCo%2C%20showcasing%20its%20superiority%20over%20state-of-the-art%20methods.%20The%20code%20is%20available%20at%20https%3A//github.com/junjianli106/MiCo.%0ALink%3A%20http%3A//arxiv.org/abs/2506.18028v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiCo%253A%2520Multiple%2520Instance%2520Learning%2520with%2520Context-Aware%2520Clustering%2520for%2520Whole%2520Slide%2520Image%2520Analysis%26entry.906535625%3DJunjian%2520Li%2520and%2520Jin%2520Liu%2520and%2520Hulin%2520Kuang%2520and%2520Hailin%2520Yue%2520and%2520Mengshen%2520He%2520and%2520Jianxin%2520Wang%26entry.1292438233%3DMultiple%2520instance%2520learning%2520%2528MIL%2529%2520has%2520shown%2520significant%2520promise%2520in%2520histopathology%2520whole%2520slide%2520image%2520%2528WSI%2529%2520analysis%2520for%2520cancer%2520diagnosis%2520and%2520prognosis.%2520However%252C%2520the%2520inherent%2520spatial%2520heterogeneity%2520of%2520WSIs%2520presents%2520critical%2520challenges%252C%2520as%2520morphologically%2520similar%2520tissue%2520types%2520are%2520often%2520dispersed%2520across%2520distant%2520anatomical%2520regions.%2520Conventional%2520MIL%2520methods%2520struggle%2520to%2520model%2520these%2520scattered%2520tissue%2520distributions%2520and%2520capture%2520cross-regional%2520spatial%2520interactions%2520effectively.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520Multiple%2520instance%2520learning%2520framework%2520with%2520Context-Aware%2520Clustering%2520%2528MiCo%2529%252C%2520designed%2520to%2520enhance%2520cross-regional%2520intra-tissue%2520correlations%2520and%2520strengthen%2520inter-tissue%2520semantic%2520associations%2520in%2520WSIs.%2520MiCo%2520begins%2520by%2520clustering%2520instances%2520to%2520distill%2520discriminative%2520morphological%2520patterns%252C%2520with%2520cluster%2520centroids%2520serving%2520as%2520semantic%2520anchors.%2520To%2520enhance%2520cross-regional%2520intra-tissue%2520correlations%252C%2520MiCo%2520employs%2520a%2520Cluster%2520Route%2520module%252C%2520which%2520dynamically%2520links%2520instances%2520of%2520the%2520same%2520tissue%2520type%2520across%2520distant%2520regions%2520via%2520feature%2520similarity.%2520These%2520semantic%2520anchors%2520act%2520as%2520contextual%2520hubs%252C%2520propagating%2520semantic%2520relationships%2520to%2520refine%2520instance-level%2520representations.%2520To%2520eliminate%2520semantic%2520fragmentation%2520and%2520strengthen%2520inter-tissue%2520semantic%2520associations%252C%2520MiCo%2520integrates%2520a%2520Cluster%2520Reducer%2520module%252C%2520which%2520consolidates%2520redundant%2520anchors%2520while%2520enhancing%2520information%2520exchange%2520between%2520distinct%2520semantic%2520groups.%2520Extensive%2520experiments%2520on%2520two%2520challenging%2520tasks%2520across%2520nine%2520large-scale%2520public%2520cancer%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520MiCo%252C%2520showcasing%2520its%2520superiority%2520over%2520state-of-the-art%2520methods.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/junjianli106/MiCo.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18028v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiCo%3A%20Multiple%20Instance%20Learning%20with%20Context-Aware%20Clustering%20for%20Whole%20Slide%20Image%20Analysis&entry.906535625=Junjian%20Li%20and%20Jin%20Liu%20and%20Hulin%20Kuang%20and%20Hailin%20Yue%20and%20Mengshen%20He%20and%20Jianxin%20Wang&entry.1292438233=Multiple%20instance%20learning%20%28MIL%29%20has%20shown%20significant%20promise%20in%20histopathology%20whole%20slide%20image%20%28WSI%29%20analysis%20for%20cancer%20diagnosis%20and%20prognosis.%20However%2C%20the%20inherent%20spatial%20heterogeneity%20of%20WSIs%20presents%20critical%20challenges%2C%20as%20morphologically%20similar%20tissue%20types%20are%20often%20dispersed%20across%20distant%20anatomical%20regions.%20Conventional%20MIL%20methods%20struggle%20to%20model%20these%20scattered%20tissue%20distributions%20and%20capture%20cross-regional%20spatial%20interactions%20effectively.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20Multiple%20instance%20learning%20framework%20with%20Context-Aware%20Clustering%20%28MiCo%29%2C%20designed%20to%20enhance%20cross-regional%20intra-tissue%20correlations%20and%20strengthen%20inter-tissue%20semantic%20associations%20in%20WSIs.%20MiCo%20begins%20by%20clustering%20instances%20to%20distill%20discriminative%20morphological%20patterns%2C%20with%20cluster%20centroids%20serving%20as%20semantic%20anchors.%20To%20enhance%20cross-regional%20intra-tissue%20correlations%2C%20MiCo%20employs%20a%20Cluster%20Route%20module%2C%20which%20dynamically%20links%20instances%20of%20the%20same%20tissue%20type%20across%20distant%20regions%20via%20feature%20similarity.%20These%20semantic%20anchors%20act%20as%20contextual%20hubs%2C%20propagating%20semantic%20relationships%20to%20refine%20instance-level%20representations.%20To%20eliminate%20semantic%20fragmentation%20and%20strengthen%20inter-tissue%20semantic%20associations%2C%20MiCo%20integrates%20a%20Cluster%20Reducer%20module%2C%20which%20consolidates%20redundant%20anchors%20while%20enhancing%20information%20exchange%20between%20distinct%20semantic%20groups.%20Extensive%20experiments%20on%20two%20challenging%20tasks%20across%20nine%20large-scale%20public%20cancer%20datasets%20demonstrate%20the%20effectiveness%20of%20MiCo%2C%20showcasing%20its%20superiority%20over%20state-of-the-art%20methods.%20The%20code%20is%20available%20at%20https%3A//github.com/junjianli106/MiCo.&entry.1838667208=http%3A//arxiv.org/abs/2506.18028v3&entry.124074799=Read"},
{"title": "BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation", "author": "Ahmad AlMughrabi and Guillermo Rivo and Carlos Jim\u00e9nez-Farf\u00e1n and Umair Haroon and Farid Al-Areqi and Hyunjun Jung and Benjamin Busam and Ricardo Marques and Petia Radeva", "abstract": "Food image segmentation is a critical task for dietary analysis, enabling accurate estimation of food volume and nutrients. However, current methods suffer from limited multi-view data and poor generalization to new viewpoints. We introduce BenchSeg, a novel multi-view food video segmentation dataset and benchmark. BenchSeg aggregates 55 dish scenes (from Nutrition5k, Vegetables & Fruits, MetaFood3D, and FoodKit) with 25,284 meticulously annotated frames, capturing each dish under free 360\u00b0 camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, transformer, CNN, and large multimodal) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on BenchSeg. Quantitative and qualitative results demonstrate that while standard image segmenters degrade sharply under novel viewpoints, memory-augmented methods maintain temporal consistency across frames. Our best model based on a combination of SeTR-MLA+XMem2 outperforms prior work (e.g., improving over FoodMem by ~2.63% mAP), offering new insights into food segmentation and tracking for dietary analysis. We release BenchSeg to foster future research. The project page including the dataset annotations and the food segmentation models can be found at https://amughrabi.github.io/benchseg.", "link": "http://arxiv.org/abs/2601.07581v1", "date": "2026-01-12", "relevancy": 2.066, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5278}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BenchSeg%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Multi-View%20Food%20Video%20Segmentation&body=Title%3A%20BenchSeg%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Multi-View%20Food%20Video%20Segmentation%0AAuthor%3A%20Ahmad%20AlMughrabi%20and%20Guillermo%20Rivo%20and%20Carlos%20Jim%C3%A9nez-Farf%C3%A1n%20and%20Umair%20Haroon%20and%20Farid%20Al-Areqi%20and%20Hyunjun%20Jung%20and%20Benjamin%20Busam%20and%20Ricardo%20Marques%20and%20Petia%20Radeva%0AAbstract%3A%20Food%20image%20segmentation%20is%20a%20critical%20task%20for%20dietary%20analysis%2C%20enabling%20accurate%20estimation%20of%20food%20volume%20and%20nutrients.%20However%2C%20current%20methods%20suffer%20from%20limited%20multi-view%20data%20and%20poor%20generalization%20to%20new%20viewpoints.%20We%20introduce%20BenchSeg%2C%20a%20novel%20multi-view%20food%20video%20segmentation%20dataset%20and%20benchmark.%20BenchSeg%20aggregates%2055%20dish%20scenes%20%28from%20Nutrition5k%2C%20Vegetables%20%26%20Fruits%2C%20MetaFood3D%2C%20and%20FoodKit%29%20with%2025%2C284%20meticulously%20annotated%20frames%2C%20capturing%20each%20dish%20under%20free%20360%C2%B0%20camera%20motion.%20We%20evaluate%20a%20diverse%20set%20of%2020%20state-of-the-art%20segmentation%20models%20%28e.g.%2C%20SAM-based%2C%20transformer%2C%20CNN%2C%20and%20large%20multimodal%29%20on%20the%20existing%20FoodSeg103%20dataset%20and%20evaluate%20them%20%28alone%20and%20combined%20with%20video-memory%20modules%29%20on%20BenchSeg.%20Quantitative%20and%20qualitative%20results%20demonstrate%20that%20while%20standard%20image%20segmenters%20degrade%20sharply%20under%20novel%20viewpoints%2C%20memory-augmented%20methods%20maintain%20temporal%20consistency%20across%20frames.%20Our%20best%20model%20based%20on%20a%20combination%20of%20SeTR-MLA%2BXMem2%20outperforms%20prior%20work%20%28e.g.%2C%20improving%20over%20FoodMem%20by%20~2.63%25%20mAP%29%2C%20offering%20new%20insights%20into%20food%20segmentation%20and%20tracking%20for%20dietary%20analysis.%20We%20release%20BenchSeg%20to%20foster%20future%20research.%20The%20project%20page%20including%20the%20dataset%20annotations%20and%20the%20food%20segmentation%20models%20can%20be%20found%20at%20https%3A//amughrabi.github.io/benchseg.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchSeg%253A%2520A%2520Large-Scale%2520Dataset%2520and%2520Benchmark%2520for%2520Multi-View%2520Food%2520Video%2520Segmentation%26entry.906535625%3DAhmad%2520AlMughrabi%2520and%2520Guillermo%2520Rivo%2520and%2520Carlos%2520Jim%25C3%25A9nez-Farf%25C3%25A1n%2520and%2520Umair%2520Haroon%2520and%2520Farid%2520Al-Areqi%2520and%2520Hyunjun%2520Jung%2520and%2520Benjamin%2520Busam%2520and%2520Ricardo%2520Marques%2520and%2520Petia%2520Radeva%26entry.1292438233%3DFood%2520image%2520segmentation%2520is%2520a%2520critical%2520task%2520for%2520dietary%2520analysis%252C%2520enabling%2520accurate%2520estimation%2520of%2520food%2520volume%2520and%2520nutrients.%2520However%252C%2520current%2520methods%2520suffer%2520from%2520limited%2520multi-view%2520data%2520and%2520poor%2520generalization%2520to%2520new%2520viewpoints.%2520We%2520introduce%2520BenchSeg%252C%2520a%2520novel%2520multi-view%2520food%2520video%2520segmentation%2520dataset%2520and%2520benchmark.%2520BenchSeg%2520aggregates%252055%2520dish%2520scenes%2520%2528from%2520Nutrition5k%252C%2520Vegetables%2520%2526%2520Fruits%252C%2520MetaFood3D%252C%2520and%2520FoodKit%2529%2520with%252025%252C284%2520meticulously%2520annotated%2520frames%252C%2520capturing%2520each%2520dish%2520under%2520free%2520360%25C2%25B0%2520camera%2520motion.%2520We%2520evaluate%2520a%2520diverse%2520set%2520of%252020%2520state-of-the-art%2520segmentation%2520models%2520%2528e.g.%252C%2520SAM-based%252C%2520transformer%252C%2520CNN%252C%2520and%2520large%2520multimodal%2529%2520on%2520the%2520existing%2520FoodSeg103%2520dataset%2520and%2520evaluate%2520them%2520%2528alone%2520and%2520combined%2520with%2520video-memory%2520modules%2529%2520on%2520BenchSeg.%2520Quantitative%2520and%2520qualitative%2520results%2520demonstrate%2520that%2520while%2520standard%2520image%2520segmenters%2520degrade%2520sharply%2520under%2520novel%2520viewpoints%252C%2520memory-augmented%2520methods%2520maintain%2520temporal%2520consistency%2520across%2520frames.%2520Our%2520best%2520model%2520based%2520on%2520a%2520combination%2520of%2520SeTR-MLA%252BXMem2%2520outperforms%2520prior%2520work%2520%2528e.g.%252C%2520improving%2520over%2520FoodMem%2520by%2520~2.63%2525%2520mAP%2529%252C%2520offering%2520new%2520insights%2520into%2520food%2520segmentation%2520and%2520tracking%2520for%2520dietary%2520analysis.%2520We%2520release%2520BenchSeg%2520to%2520foster%2520future%2520research.%2520The%2520project%2520page%2520including%2520the%2520dataset%2520annotations%2520and%2520the%2520food%2520segmentation%2520models%2520can%2520be%2520found%2520at%2520https%253A//amughrabi.github.io/benchseg.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BenchSeg%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Multi-View%20Food%20Video%20Segmentation&entry.906535625=Ahmad%20AlMughrabi%20and%20Guillermo%20Rivo%20and%20Carlos%20Jim%C3%A9nez-Farf%C3%A1n%20and%20Umair%20Haroon%20and%20Farid%20Al-Areqi%20and%20Hyunjun%20Jung%20and%20Benjamin%20Busam%20and%20Ricardo%20Marques%20and%20Petia%20Radeva&entry.1292438233=Food%20image%20segmentation%20is%20a%20critical%20task%20for%20dietary%20analysis%2C%20enabling%20accurate%20estimation%20of%20food%20volume%20and%20nutrients.%20However%2C%20current%20methods%20suffer%20from%20limited%20multi-view%20data%20and%20poor%20generalization%20to%20new%20viewpoints.%20We%20introduce%20BenchSeg%2C%20a%20novel%20multi-view%20food%20video%20segmentation%20dataset%20and%20benchmark.%20BenchSeg%20aggregates%2055%20dish%20scenes%20%28from%20Nutrition5k%2C%20Vegetables%20%26%20Fruits%2C%20MetaFood3D%2C%20and%20FoodKit%29%20with%2025%2C284%20meticulously%20annotated%20frames%2C%20capturing%20each%20dish%20under%20free%20360%C2%B0%20camera%20motion.%20We%20evaluate%20a%20diverse%20set%20of%2020%20state-of-the-art%20segmentation%20models%20%28e.g.%2C%20SAM-based%2C%20transformer%2C%20CNN%2C%20and%20large%20multimodal%29%20on%20the%20existing%20FoodSeg103%20dataset%20and%20evaluate%20them%20%28alone%20and%20combined%20with%20video-memory%20modules%29%20on%20BenchSeg.%20Quantitative%20and%20qualitative%20results%20demonstrate%20that%20while%20standard%20image%20segmenters%20degrade%20sharply%20under%20novel%20viewpoints%2C%20memory-augmented%20methods%20maintain%20temporal%20consistency%20across%20frames.%20Our%20best%20model%20based%20on%20a%20combination%20of%20SeTR-MLA%2BXMem2%20outperforms%20prior%20work%20%28e.g.%2C%20improving%20over%20FoodMem%20by%20~2.63%25%20mAP%29%2C%20offering%20new%20insights%20into%20food%20segmentation%20and%20tracking%20for%20dietary%20analysis.%20We%20release%20BenchSeg%20to%20foster%20future%20research.%20The%20project%20page%20including%20the%20dataset%20annotations%20and%20the%20food%20segmentation%20models%20can%20be%20found%20at%20https%3A//amughrabi.github.io/benchseg.&entry.1838667208=http%3A//arxiv.org/abs/2601.07581v1&entry.124074799=Read"},
{"title": "Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory", "author": "Sirui Liang and Pengfei Cao and Jian Zhao and Wenhao Teng and Xiangwen Liao and Jun Zhao and Kang Liu", "abstract": "Large language model (LLM) agents increasingly rely on accumulated memory to solve long-horizon decision-making tasks. However, most existing approaches store memory in fixed representations and reuse it at a single or implicit level of abstraction, which limits generalization and often leads to negative transfer when distribution shift. This paper proposes the Meta-Cognitive Memory Abstraction method (MCMA), which treats memory abstraction as a learnable cognitive skill rather than a fixed design choice. MCMA decouples task execution from memory management by combining a frozen task model with a learned memory copilot. The memory copilot is trained using direct preference optimization, it determines how memories should be structured, abstracted, and reused. Memories are further organized into a hierarchy of abstraction levels, enabling selective reuse based on task similarity. When no memory is transferable, MCMA transfers the ability to abstract and manage memory by transferring the memory copilot. Experiments on ALFWorld, ScienceWorld, and BabyAI demonstrate substantial improvements in performance, out-of-distribution generalization, and cross-task transfer over several baselines.", "link": "http://arxiv.org/abs/2601.07470v1", "date": "2026-01-12", "relevancy": 2.0622, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.525}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5096}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20How%20to%20Remember%3A%20A%20Meta-Cognitive%20Management%20Method%20for%20Structured%20and%20Transferable%20Agent%20Memory&body=Title%3A%20Learning%20How%20to%20Remember%3A%20A%20Meta-Cognitive%20Management%20Method%20for%20Structured%20and%20Transferable%20Agent%20Memory%0AAuthor%3A%20Sirui%20Liang%20and%20Pengfei%20Cao%20and%20Jian%20Zhao%20and%20Wenhao%20Teng%20and%20Xiangwen%20Liao%20and%20Jun%20Zhao%20and%20Kang%20Liu%0AAbstract%3A%20Large%20language%20model%20%28LLM%29%20agents%20increasingly%20rely%20on%20accumulated%20memory%20to%20solve%20long-horizon%20decision-making%20tasks.%20However%2C%20most%20existing%20approaches%20store%20memory%20in%20fixed%20representations%20and%20reuse%20it%20at%20a%20single%20or%20implicit%20level%20of%20abstraction%2C%20which%20limits%20generalization%20and%20often%20leads%20to%20negative%20transfer%20when%20distribution%20shift.%20This%20paper%20proposes%20the%20Meta-Cognitive%20Memory%20Abstraction%20method%20%28MCMA%29%2C%20which%20treats%20memory%20abstraction%20as%20a%20learnable%20cognitive%20skill%20rather%20than%20a%20fixed%20design%20choice.%20MCMA%20decouples%20task%20execution%20from%20memory%20management%20by%20combining%20a%20frozen%20task%20model%20with%20a%20learned%20memory%20copilot.%20The%20memory%20copilot%20is%20trained%20using%20direct%20preference%20optimization%2C%20it%20determines%20how%20memories%20should%20be%20structured%2C%20abstracted%2C%20and%20reused.%20Memories%20are%20further%20organized%20into%20a%20hierarchy%20of%20abstraction%20levels%2C%20enabling%20selective%20reuse%20based%20on%20task%20similarity.%20When%20no%20memory%20is%20transferable%2C%20MCMA%20transfers%20the%20ability%20to%20abstract%20and%20manage%20memory%20by%20transferring%20the%20memory%20copilot.%20Experiments%20on%20ALFWorld%2C%20ScienceWorld%2C%20and%20BabyAI%20demonstrate%20substantial%20improvements%20in%20performance%2C%20out-of-distribution%20generalization%2C%20and%20cross-task%20transfer%20over%20several%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520How%2520to%2520Remember%253A%2520A%2520Meta-Cognitive%2520Management%2520Method%2520for%2520Structured%2520and%2520Transferable%2520Agent%2520Memory%26entry.906535625%3DSirui%2520Liang%2520and%2520Pengfei%2520Cao%2520and%2520Jian%2520Zhao%2520and%2520Wenhao%2520Teng%2520and%2520Xiangwen%2520Liao%2520and%2520Jun%2520Zhao%2520and%2520Kang%2520Liu%26entry.1292438233%3DLarge%2520language%2520model%2520%2528LLM%2529%2520agents%2520increasingly%2520rely%2520on%2520accumulated%2520memory%2520to%2520solve%2520long-horizon%2520decision-making%2520tasks.%2520However%252C%2520most%2520existing%2520approaches%2520store%2520memory%2520in%2520fixed%2520representations%2520and%2520reuse%2520it%2520at%2520a%2520single%2520or%2520implicit%2520level%2520of%2520abstraction%252C%2520which%2520limits%2520generalization%2520and%2520often%2520leads%2520to%2520negative%2520transfer%2520when%2520distribution%2520shift.%2520This%2520paper%2520proposes%2520the%2520Meta-Cognitive%2520Memory%2520Abstraction%2520method%2520%2528MCMA%2529%252C%2520which%2520treats%2520memory%2520abstraction%2520as%2520a%2520learnable%2520cognitive%2520skill%2520rather%2520than%2520a%2520fixed%2520design%2520choice.%2520MCMA%2520decouples%2520task%2520execution%2520from%2520memory%2520management%2520by%2520combining%2520a%2520frozen%2520task%2520model%2520with%2520a%2520learned%2520memory%2520copilot.%2520The%2520memory%2520copilot%2520is%2520trained%2520using%2520direct%2520preference%2520optimization%252C%2520it%2520determines%2520how%2520memories%2520should%2520be%2520structured%252C%2520abstracted%252C%2520and%2520reused.%2520Memories%2520are%2520further%2520organized%2520into%2520a%2520hierarchy%2520of%2520abstraction%2520levels%252C%2520enabling%2520selective%2520reuse%2520based%2520on%2520task%2520similarity.%2520When%2520no%2520memory%2520is%2520transferable%252C%2520MCMA%2520transfers%2520the%2520ability%2520to%2520abstract%2520and%2520manage%2520memory%2520by%2520transferring%2520the%2520memory%2520copilot.%2520Experiments%2520on%2520ALFWorld%252C%2520ScienceWorld%252C%2520and%2520BabyAI%2520demonstrate%2520substantial%2520improvements%2520in%2520performance%252C%2520out-of-distribution%2520generalization%252C%2520and%2520cross-task%2520transfer%2520over%2520several%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20How%20to%20Remember%3A%20A%20Meta-Cognitive%20Management%20Method%20for%20Structured%20and%20Transferable%20Agent%20Memory&entry.906535625=Sirui%20Liang%20and%20Pengfei%20Cao%20and%20Jian%20Zhao%20and%20Wenhao%20Teng%20and%20Xiangwen%20Liao%20and%20Jun%20Zhao%20and%20Kang%20Liu&entry.1292438233=Large%20language%20model%20%28LLM%29%20agents%20increasingly%20rely%20on%20accumulated%20memory%20to%20solve%20long-horizon%20decision-making%20tasks.%20However%2C%20most%20existing%20approaches%20store%20memory%20in%20fixed%20representations%20and%20reuse%20it%20at%20a%20single%20or%20implicit%20level%20of%20abstraction%2C%20which%20limits%20generalization%20and%20often%20leads%20to%20negative%20transfer%20when%20distribution%20shift.%20This%20paper%20proposes%20the%20Meta-Cognitive%20Memory%20Abstraction%20method%20%28MCMA%29%2C%20which%20treats%20memory%20abstraction%20as%20a%20learnable%20cognitive%20skill%20rather%20than%20a%20fixed%20design%20choice.%20MCMA%20decouples%20task%20execution%20from%20memory%20management%20by%20combining%20a%20frozen%20task%20model%20with%20a%20learned%20memory%20copilot.%20The%20memory%20copilot%20is%20trained%20using%20direct%20preference%20optimization%2C%20it%20determines%20how%20memories%20should%20be%20structured%2C%20abstracted%2C%20and%20reused.%20Memories%20are%20further%20organized%20into%20a%20hierarchy%20of%20abstraction%20levels%2C%20enabling%20selective%20reuse%20based%20on%20task%20similarity.%20When%20no%20memory%20is%20transferable%2C%20MCMA%20transfers%20the%20ability%20to%20abstract%20and%20manage%20memory%20by%20transferring%20the%20memory%20copilot.%20Experiments%20on%20ALFWorld%2C%20ScienceWorld%2C%20and%20BabyAI%20demonstrate%20substantial%20improvements%20in%20performance%2C%20out-of-distribution%20generalization%2C%20and%20cross-task%20transfer%20over%20several%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.07470v1&entry.124074799=Read"},
{"title": "Beyond topography: Topographic regularization improves robustness and reshapes representations in convolutional neural networks", "author": "Nhut Truong and Uri Hasson", "abstract": "Topographic convolutional neural networks (TCNNs) are computational models that can simulate aspects of the brain's spatial and functional organization. However, it is unclear whether and how different types of topographic regularization shape robustness, representational structure, and functional organization during end-to-end training. We address this question by comparing TCNNs trained with two local spatial losses applied to a penultimate-layer topographic grid: i) Weight Similarity (WS), whose objective penalizes differences between neighboring units' incoming weight vectors, and ii) Activation Similarity (AS), whose objective penalizes differences between neighboring units' activation patterns over stimuli. We evaluate the trained models on classification accuracy, robustness to weight perturbations and input degradation, the spatial organization of learned representations, and development of category-selective \"expert units\" in the penultimate layer. Both losses changed inter-unit correlation structure, but in qualitatively different ways. WS produced smooth topographies, with correlated neighborhoods. In contrast, AS produced a bimodal inter-unit correlation structure that lacked spatial smoothness. AS and WS training increased robustness relative to control (non-topographic) models: AS improved robustness to image degradation on CIFAR-10, WS did so on MNIST, and both improved robustness to weight perturbations. WS was also associated with greater input sensitivity at the unit level and stronger functional localization. In addition, as compared to control models, both AS and WS produced differences in orientation tuning, symmetry sensitivity, and eccentricity profiles of units. Together, these results show that local topographic regularization can improve robustness during end-to-end training while systematically reshaping representational structure.", "link": "http://arxiv.org/abs/2508.00043v2", "date": "2026-01-12", "relevancy": 2.0308, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5198}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5099}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20topography%3A%20Topographic%20regularization%20improves%20robustness%20and%20reshapes%20representations%20in%20convolutional%20neural%20networks&body=Title%3A%20Beyond%20topography%3A%20Topographic%20regularization%20improves%20robustness%20and%20reshapes%20representations%20in%20convolutional%20neural%20networks%0AAuthor%3A%20Nhut%20Truong%20and%20Uri%20Hasson%0AAbstract%3A%20Topographic%20convolutional%20neural%20networks%20%28TCNNs%29%20are%20computational%20models%20that%20can%20simulate%20aspects%20of%20the%20brain%27s%20spatial%20and%20functional%20organization.%20However%2C%20it%20is%20unclear%20whether%20and%20how%20different%20types%20of%20topographic%20regularization%20shape%20robustness%2C%20representational%20structure%2C%20and%20functional%20organization%20during%20end-to-end%20training.%20We%20address%20this%20question%20by%20comparing%20TCNNs%20trained%20with%20two%20local%20spatial%20losses%20applied%20to%20a%20penultimate-layer%20topographic%20grid%3A%20i%29%20Weight%20Similarity%20%28WS%29%2C%20whose%20objective%20penalizes%20differences%20between%20neighboring%20units%27%20incoming%20weight%20vectors%2C%20and%20ii%29%20Activation%20Similarity%20%28AS%29%2C%20whose%20objective%20penalizes%20differences%20between%20neighboring%20units%27%20activation%20patterns%20over%20stimuli.%20We%20evaluate%20the%20trained%20models%20on%20classification%20accuracy%2C%20robustness%20to%20weight%20perturbations%20and%20input%20degradation%2C%20the%20spatial%20organization%20of%20learned%20representations%2C%20and%20development%20of%20category-selective%20%22expert%20units%22%20in%20the%20penultimate%20layer.%20Both%20losses%20changed%20inter-unit%20correlation%20structure%2C%20but%20in%20qualitatively%20different%20ways.%20WS%20produced%20smooth%20topographies%2C%20with%20correlated%20neighborhoods.%20In%20contrast%2C%20AS%20produced%20a%20bimodal%20inter-unit%20correlation%20structure%20that%20lacked%20spatial%20smoothness.%20AS%20and%20WS%20training%20increased%20robustness%20relative%20to%20control%20%28non-topographic%29%20models%3A%20AS%20improved%20robustness%20to%20image%20degradation%20on%20CIFAR-10%2C%20WS%20did%20so%20on%20MNIST%2C%20and%20both%20improved%20robustness%20to%20weight%20perturbations.%20WS%20was%20also%20associated%20with%20greater%20input%20sensitivity%20at%20the%20unit%20level%20and%20stronger%20functional%20localization.%20In%20addition%2C%20as%20compared%20to%20control%20models%2C%20both%20AS%20and%20WS%20produced%20differences%20in%20orientation%20tuning%2C%20symmetry%20sensitivity%2C%20and%20eccentricity%20profiles%20of%20units.%20Together%2C%20these%20results%20show%20that%20local%20topographic%20regularization%20can%20improve%20robustness%20during%20end-to-end%20training%20while%20systematically%20reshaping%20representational%20structure.%0ALink%3A%20http%3A//arxiv.org/abs/2508.00043v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520topography%253A%2520Topographic%2520regularization%2520improves%2520robustness%2520and%2520reshapes%2520representations%2520in%2520convolutional%2520neural%2520networks%26entry.906535625%3DNhut%2520Truong%2520and%2520Uri%2520Hasson%26entry.1292438233%3DTopographic%2520convolutional%2520neural%2520networks%2520%2528TCNNs%2529%2520are%2520computational%2520models%2520that%2520can%2520simulate%2520aspects%2520of%2520the%2520brain%2527s%2520spatial%2520and%2520functional%2520organization.%2520However%252C%2520it%2520is%2520unclear%2520whether%2520and%2520how%2520different%2520types%2520of%2520topographic%2520regularization%2520shape%2520robustness%252C%2520representational%2520structure%252C%2520and%2520functional%2520organization%2520during%2520end-to-end%2520training.%2520We%2520address%2520this%2520question%2520by%2520comparing%2520TCNNs%2520trained%2520with%2520two%2520local%2520spatial%2520losses%2520applied%2520to%2520a%2520penultimate-layer%2520topographic%2520grid%253A%2520i%2529%2520Weight%2520Similarity%2520%2528WS%2529%252C%2520whose%2520objective%2520penalizes%2520differences%2520between%2520neighboring%2520units%2527%2520incoming%2520weight%2520vectors%252C%2520and%2520ii%2529%2520Activation%2520Similarity%2520%2528AS%2529%252C%2520whose%2520objective%2520penalizes%2520differences%2520between%2520neighboring%2520units%2527%2520activation%2520patterns%2520over%2520stimuli.%2520We%2520evaluate%2520the%2520trained%2520models%2520on%2520classification%2520accuracy%252C%2520robustness%2520to%2520weight%2520perturbations%2520and%2520input%2520degradation%252C%2520the%2520spatial%2520organization%2520of%2520learned%2520representations%252C%2520and%2520development%2520of%2520category-selective%2520%2522expert%2520units%2522%2520in%2520the%2520penultimate%2520layer.%2520Both%2520losses%2520changed%2520inter-unit%2520correlation%2520structure%252C%2520but%2520in%2520qualitatively%2520different%2520ways.%2520WS%2520produced%2520smooth%2520topographies%252C%2520with%2520correlated%2520neighborhoods.%2520In%2520contrast%252C%2520AS%2520produced%2520a%2520bimodal%2520inter-unit%2520correlation%2520structure%2520that%2520lacked%2520spatial%2520smoothness.%2520AS%2520and%2520WS%2520training%2520increased%2520robustness%2520relative%2520to%2520control%2520%2528non-topographic%2529%2520models%253A%2520AS%2520improved%2520robustness%2520to%2520image%2520degradation%2520on%2520CIFAR-10%252C%2520WS%2520did%2520so%2520on%2520MNIST%252C%2520and%2520both%2520improved%2520robustness%2520to%2520weight%2520perturbations.%2520WS%2520was%2520also%2520associated%2520with%2520greater%2520input%2520sensitivity%2520at%2520the%2520unit%2520level%2520and%2520stronger%2520functional%2520localization.%2520In%2520addition%252C%2520as%2520compared%2520to%2520control%2520models%252C%2520both%2520AS%2520and%2520WS%2520produced%2520differences%2520in%2520orientation%2520tuning%252C%2520symmetry%2520sensitivity%252C%2520and%2520eccentricity%2520profiles%2520of%2520units.%2520Together%252C%2520these%2520results%2520show%2520that%2520local%2520topographic%2520regularization%2520can%2520improve%2520robustness%2520during%2520end-to-end%2520training%2520while%2520systematically%2520reshaping%2520representational%2520structure.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00043v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20topography%3A%20Topographic%20regularization%20improves%20robustness%20and%20reshapes%20representations%20in%20convolutional%20neural%20networks&entry.906535625=Nhut%20Truong%20and%20Uri%20Hasson&entry.1292438233=Topographic%20convolutional%20neural%20networks%20%28TCNNs%29%20are%20computational%20models%20that%20can%20simulate%20aspects%20of%20the%20brain%27s%20spatial%20and%20functional%20organization.%20However%2C%20it%20is%20unclear%20whether%20and%20how%20different%20types%20of%20topographic%20regularization%20shape%20robustness%2C%20representational%20structure%2C%20and%20functional%20organization%20during%20end-to-end%20training.%20We%20address%20this%20question%20by%20comparing%20TCNNs%20trained%20with%20two%20local%20spatial%20losses%20applied%20to%20a%20penultimate-layer%20topographic%20grid%3A%20i%29%20Weight%20Similarity%20%28WS%29%2C%20whose%20objective%20penalizes%20differences%20between%20neighboring%20units%27%20incoming%20weight%20vectors%2C%20and%20ii%29%20Activation%20Similarity%20%28AS%29%2C%20whose%20objective%20penalizes%20differences%20between%20neighboring%20units%27%20activation%20patterns%20over%20stimuli.%20We%20evaluate%20the%20trained%20models%20on%20classification%20accuracy%2C%20robustness%20to%20weight%20perturbations%20and%20input%20degradation%2C%20the%20spatial%20organization%20of%20learned%20representations%2C%20and%20development%20of%20category-selective%20%22expert%20units%22%20in%20the%20penultimate%20layer.%20Both%20losses%20changed%20inter-unit%20correlation%20structure%2C%20but%20in%20qualitatively%20different%20ways.%20WS%20produced%20smooth%20topographies%2C%20with%20correlated%20neighborhoods.%20In%20contrast%2C%20AS%20produced%20a%20bimodal%20inter-unit%20correlation%20structure%20that%20lacked%20spatial%20smoothness.%20AS%20and%20WS%20training%20increased%20robustness%20relative%20to%20control%20%28non-topographic%29%20models%3A%20AS%20improved%20robustness%20to%20image%20degradation%20on%20CIFAR-10%2C%20WS%20did%20so%20on%20MNIST%2C%20and%20both%20improved%20robustness%20to%20weight%20perturbations.%20WS%20was%20also%20associated%20with%20greater%20input%20sensitivity%20at%20the%20unit%20level%20and%20stronger%20functional%20localization.%20In%20addition%2C%20as%20compared%20to%20control%20models%2C%20both%20AS%20and%20WS%20produced%20differences%20in%20orientation%20tuning%2C%20symmetry%20sensitivity%2C%20and%20eccentricity%20profiles%20of%20units.%20Together%2C%20these%20results%20show%20that%20local%20topographic%20regularization%20can%20improve%20robustness%20during%20end-to-end%20training%20while%20systematically%20reshaping%20representational%20structure.&entry.1838667208=http%3A//arxiv.org/abs/2508.00043v2&entry.124074799=Read"},
{"title": "Exchange Is All You Need for Remote Sensing Change Detection", "author": "Sijun Dong and Siming Fu and Kaiyu Li and Xiangyong Cao and Xiaoliang Meng and Bo Du", "abstract": "Remote sensing change detection fundamentally relies on the effective fusion and discrimination of bi-temporal features. Prevailing paradigms typically utilize Siamese encoders bridged by explicit difference computation modules, such as subtraction or concatenation, to identify changes. In this work, we challenge this complexity with SEED (Siamese Encoder-Exchange-Decoder), a streamlined paradigm that replaces explicit differencing with parameter-free feature exchange. By sharing weights across both Siamese encoders and decoders, SEED effectively operates as a single parameter set model. Theoretically, we formalize feature exchange as an orthogonal permutation operator and prove that, under pixel consistency, this mechanism preserves mutual information and Bayes optimal risk, whereas common arithmetic fusion methods often introduce information loss. Extensive experiments across five benchmarks, including SYSU-CD, LEVIR-CD, PX-CLCD, WaterCD, and CDD, and three backbones, namely SwinT, EfficientNet, and ResNet, demonstrate that SEED matches or surpasses state of the art methods despite its simplicity. Furthermore, we reveal that standard semantic segmentation models can be transformed into competitive change detectors solely by inserting this exchange mechanism, referred to as SEG2CD. The proposed paradigm offers a robust, unified, and interpretable framework for change detection, demonstrating that simple feature exchange is sufficient for high performance information fusion. Code and full training and evaluation protocols will be released at https://github.com/dyzy41/open-rscd.", "link": "http://arxiv.org/abs/2601.07805v1", "date": "2026-01-12", "relevancy": 2.029, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5221}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exchange%20Is%20All%20You%20Need%20for%20Remote%20Sensing%20Change%20Detection&body=Title%3A%20Exchange%20Is%20All%20You%20Need%20for%20Remote%20Sensing%20Change%20Detection%0AAuthor%3A%20Sijun%20Dong%20and%20Siming%20Fu%20and%20Kaiyu%20Li%20and%20Xiangyong%20Cao%20and%20Xiaoliang%20Meng%20and%20Bo%20Du%0AAbstract%3A%20Remote%20sensing%20change%20detection%20fundamentally%20relies%20on%20the%20effective%20fusion%20and%20discrimination%20of%20bi-temporal%20features.%20Prevailing%20paradigms%20typically%20utilize%20Siamese%20encoders%20bridged%20by%20explicit%20difference%20computation%20modules%2C%20such%20as%20subtraction%20or%20concatenation%2C%20to%20identify%20changes.%20In%20this%20work%2C%20we%20challenge%20this%20complexity%20with%20SEED%20%28Siamese%20Encoder-Exchange-Decoder%29%2C%20a%20streamlined%20paradigm%20that%20replaces%20explicit%20differencing%20with%20parameter-free%20feature%20exchange.%20By%20sharing%20weights%20across%20both%20Siamese%20encoders%20and%20decoders%2C%20SEED%20effectively%20operates%20as%20a%20single%20parameter%20set%20model.%20Theoretically%2C%20we%20formalize%20feature%20exchange%20as%20an%20orthogonal%20permutation%20operator%20and%20prove%20that%2C%20under%20pixel%20consistency%2C%20this%20mechanism%20preserves%20mutual%20information%20and%20Bayes%20optimal%20risk%2C%20whereas%20common%20arithmetic%20fusion%20methods%20often%20introduce%20information%20loss.%20Extensive%20experiments%20across%20five%20benchmarks%2C%20including%20SYSU-CD%2C%20LEVIR-CD%2C%20PX-CLCD%2C%20WaterCD%2C%20and%20CDD%2C%20and%20three%20backbones%2C%20namely%20SwinT%2C%20EfficientNet%2C%20and%20ResNet%2C%20demonstrate%20that%20SEED%20matches%20or%20surpasses%20state%20of%20the%20art%20methods%20despite%20its%20simplicity.%20Furthermore%2C%20we%20reveal%20that%20standard%20semantic%20segmentation%20models%20can%20be%20transformed%20into%20competitive%20change%20detectors%20solely%20by%20inserting%20this%20exchange%20mechanism%2C%20referred%20to%20as%20SEG2CD.%20The%20proposed%20paradigm%20offers%20a%20robust%2C%20unified%2C%20and%20interpretable%20framework%20for%20change%20detection%2C%20demonstrating%20that%20simple%20feature%20exchange%20is%20sufficient%20for%20high%20performance%20information%20fusion.%20Code%20and%20full%20training%20and%20evaluation%20protocols%20will%20be%20released%20at%20https%3A//github.com/dyzy41/open-rscd.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExchange%2520Is%2520All%2520You%2520Need%2520for%2520Remote%2520Sensing%2520Change%2520Detection%26entry.906535625%3DSijun%2520Dong%2520and%2520Siming%2520Fu%2520and%2520Kaiyu%2520Li%2520and%2520Xiangyong%2520Cao%2520and%2520Xiaoliang%2520Meng%2520and%2520Bo%2520Du%26entry.1292438233%3DRemote%2520sensing%2520change%2520detection%2520fundamentally%2520relies%2520on%2520the%2520effective%2520fusion%2520and%2520discrimination%2520of%2520bi-temporal%2520features.%2520Prevailing%2520paradigms%2520typically%2520utilize%2520Siamese%2520encoders%2520bridged%2520by%2520explicit%2520difference%2520computation%2520modules%252C%2520such%2520as%2520subtraction%2520or%2520concatenation%252C%2520to%2520identify%2520changes.%2520In%2520this%2520work%252C%2520we%2520challenge%2520this%2520complexity%2520with%2520SEED%2520%2528Siamese%2520Encoder-Exchange-Decoder%2529%252C%2520a%2520streamlined%2520paradigm%2520that%2520replaces%2520explicit%2520differencing%2520with%2520parameter-free%2520feature%2520exchange.%2520By%2520sharing%2520weights%2520across%2520both%2520Siamese%2520encoders%2520and%2520decoders%252C%2520SEED%2520effectively%2520operates%2520as%2520a%2520single%2520parameter%2520set%2520model.%2520Theoretically%252C%2520we%2520formalize%2520feature%2520exchange%2520as%2520an%2520orthogonal%2520permutation%2520operator%2520and%2520prove%2520that%252C%2520under%2520pixel%2520consistency%252C%2520this%2520mechanism%2520preserves%2520mutual%2520information%2520and%2520Bayes%2520optimal%2520risk%252C%2520whereas%2520common%2520arithmetic%2520fusion%2520methods%2520often%2520introduce%2520information%2520loss.%2520Extensive%2520experiments%2520across%2520five%2520benchmarks%252C%2520including%2520SYSU-CD%252C%2520LEVIR-CD%252C%2520PX-CLCD%252C%2520WaterCD%252C%2520and%2520CDD%252C%2520and%2520three%2520backbones%252C%2520namely%2520SwinT%252C%2520EfficientNet%252C%2520and%2520ResNet%252C%2520demonstrate%2520that%2520SEED%2520matches%2520or%2520surpasses%2520state%2520of%2520the%2520art%2520methods%2520despite%2520its%2520simplicity.%2520Furthermore%252C%2520we%2520reveal%2520that%2520standard%2520semantic%2520segmentation%2520models%2520can%2520be%2520transformed%2520into%2520competitive%2520change%2520detectors%2520solely%2520by%2520inserting%2520this%2520exchange%2520mechanism%252C%2520referred%2520to%2520as%2520SEG2CD.%2520The%2520proposed%2520paradigm%2520offers%2520a%2520robust%252C%2520unified%252C%2520and%2520interpretable%2520framework%2520for%2520change%2520detection%252C%2520demonstrating%2520that%2520simple%2520feature%2520exchange%2520is%2520sufficient%2520for%2520high%2520performance%2520information%2520fusion.%2520Code%2520and%2520full%2520training%2520and%2520evaluation%2520protocols%2520will%2520be%2520released%2520at%2520https%253A//github.com/dyzy41/open-rscd.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exchange%20Is%20All%20You%20Need%20for%20Remote%20Sensing%20Change%20Detection&entry.906535625=Sijun%20Dong%20and%20Siming%20Fu%20and%20Kaiyu%20Li%20and%20Xiangyong%20Cao%20and%20Xiaoliang%20Meng%20and%20Bo%20Du&entry.1292438233=Remote%20sensing%20change%20detection%20fundamentally%20relies%20on%20the%20effective%20fusion%20and%20discrimination%20of%20bi-temporal%20features.%20Prevailing%20paradigms%20typically%20utilize%20Siamese%20encoders%20bridged%20by%20explicit%20difference%20computation%20modules%2C%20such%20as%20subtraction%20or%20concatenation%2C%20to%20identify%20changes.%20In%20this%20work%2C%20we%20challenge%20this%20complexity%20with%20SEED%20%28Siamese%20Encoder-Exchange-Decoder%29%2C%20a%20streamlined%20paradigm%20that%20replaces%20explicit%20differencing%20with%20parameter-free%20feature%20exchange.%20By%20sharing%20weights%20across%20both%20Siamese%20encoders%20and%20decoders%2C%20SEED%20effectively%20operates%20as%20a%20single%20parameter%20set%20model.%20Theoretically%2C%20we%20formalize%20feature%20exchange%20as%20an%20orthogonal%20permutation%20operator%20and%20prove%20that%2C%20under%20pixel%20consistency%2C%20this%20mechanism%20preserves%20mutual%20information%20and%20Bayes%20optimal%20risk%2C%20whereas%20common%20arithmetic%20fusion%20methods%20often%20introduce%20information%20loss.%20Extensive%20experiments%20across%20five%20benchmarks%2C%20including%20SYSU-CD%2C%20LEVIR-CD%2C%20PX-CLCD%2C%20WaterCD%2C%20and%20CDD%2C%20and%20three%20backbones%2C%20namely%20SwinT%2C%20EfficientNet%2C%20and%20ResNet%2C%20demonstrate%20that%20SEED%20matches%20or%20surpasses%20state%20of%20the%20art%20methods%20despite%20its%20simplicity.%20Furthermore%2C%20we%20reveal%20that%20standard%20semantic%20segmentation%20models%20can%20be%20transformed%20into%20competitive%20change%20detectors%20solely%20by%20inserting%20this%20exchange%20mechanism%2C%20referred%20to%20as%20SEG2CD.%20The%20proposed%20paradigm%20offers%20a%20robust%2C%20unified%2C%20and%20interpretable%20framework%20for%20change%20detection%2C%20demonstrating%20that%20simple%20feature%20exchange%20is%20sufficient%20for%20high%20performance%20information%20fusion.%20Code%20and%20full%20training%20and%20evaluation%20protocols%20will%20be%20released%20at%20https%3A//github.com/dyzy41/open-rscd.&entry.1838667208=http%3A//arxiv.org/abs/2601.07805v1&entry.124074799=Read"},
{"title": "Stable In-hand Manipulation for a Lightweight Four-motor Prosthetic Hand", "author": "Yuki Kuroda and Tomoya Takahashi and Cristian C. Beltran-Hernandez and Kazutoshi Tanaka and Masashi Hamaya", "abstract": "Electric prosthetic hands should be lightweight to decrease the burden on the user, shaped like human hands for cosmetic purposes, and designed with motors enclosed inside to protect them from damage and dirt. Additionally, in-hand manipulation is necessary to perform daily activities such as transitioning between different postures, particularly through rotational movements, such as reorienting a pen into a writing posture after picking it up from a desk. We previously developed PLEXUS hand (Precision-Lateral dEXteroUS manipulation hand), a lightweight (311 g) prosthetic hand driven by four motors. This prosthetic performed reorientation between precision and lateral grasps with various objects. However, its controller required predefined object widths and was limited to handling lightweight objects (of weight up to 34 g). This study addresses these limitations by employing motor current feedback. Combined with the hand's previously optimized single-axis thumb, this approach achieves more stable manipulation by estimating the object's width and adjusting the index finger position to maintain stable object holding during the reorientation. Experimental validation using primitive objects of various widths (5-30 mm) and shapes (cylinders and prisms) resulted in a 100% success rate with lightweight objects and maintained a high success rate (>=80) even with heavy aluminum prisms (of weight up to 289 g). By contrast, the performance without index finger coordination dropped to just 40% on the heaviest 289 g prism. The hand also successfully executed several daily tasks, including closing bottle caps and orienting a pen for writing.", "link": "http://arxiv.org/abs/2601.07559v1", "date": "2026-01-12", "relevancy": 2.0278, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5393}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4953}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20In-hand%20Manipulation%20for%20a%20Lightweight%20Four-motor%20Prosthetic%20Hand&body=Title%3A%20Stable%20In-hand%20Manipulation%20for%20a%20Lightweight%20Four-motor%20Prosthetic%20Hand%0AAuthor%3A%20Yuki%20Kuroda%20and%20Tomoya%20Takahashi%20and%20Cristian%20C.%20Beltran-Hernandez%20and%20Kazutoshi%20Tanaka%20and%20Masashi%20Hamaya%0AAbstract%3A%20Electric%20prosthetic%20hands%20should%20be%20lightweight%20to%20decrease%20the%20burden%20on%20the%20user%2C%20shaped%20like%20human%20hands%20for%20cosmetic%20purposes%2C%20and%20designed%20with%20motors%20enclosed%20inside%20to%20protect%20them%20from%20damage%20and%20dirt.%20Additionally%2C%20in-hand%20manipulation%20is%20necessary%20to%20perform%20daily%20activities%20such%20as%20transitioning%20between%20different%20postures%2C%20particularly%20through%20rotational%20movements%2C%20such%20as%20reorienting%20a%20pen%20into%20a%20writing%20posture%20after%20picking%20it%20up%20from%20a%20desk.%20We%20previously%20developed%20PLEXUS%20hand%20%28Precision-Lateral%20dEXteroUS%20manipulation%20hand%29%2C%20a%20lightweight%20%28311%20g%29%20prosthetic%20hand%20driven%20by%20four%20motors.%20This%20prosthetic%20performed%20reorientation%20between%20precision%20and%20lateral%20grasps%20with%20various%20objects.%20However%2C%20its%20controller%20required%20predefined%20object%20widths%20and%20was%20limited%20to%20handling%20lightweight%20objects%20%28of%20weight%20up%20to%2034%20g%29.%20This%20study%20addresses%20these%20limitations%20by%20employing%20motor%20current%20feedback.%20Combined%20with%20the%20hand%27s%20previously%20optimized%20single-axis%20thumb%2C%20this%20approach%20achieves%20more%20stable%20manipulation%20by%20estimating%20the%20object%27s%20width%20and%20adjusting%20the%20index%20finger%20position%20to%20maintain%20stable%20object%20holding%20during%20the%20reorientation.%20Experimental%20validation%20using%20primitive%20objects%20of%20various%20widths%20%285-30%20mm%29%20and%20shapes%20%28cylinders%20and%20prisms%29%20resulted%20in%20a%20100%25%20success%20rate%20with%20lightweight%20objects%20and%20maintained%20a%20high%20success%20rate%20%28%3E%3D80%29%20even%20with%20heavy%20aluminum%20prisms%20%28of%20weight%20up%20to%20289%20g%29.%20By%20contrast%2C%20the%20performance%20without%20index%20finger%20coordination%20dropped%20to%20just%2040%25%20on%20the%20heaviest%20289%20g%20prism.%20The%20hand%20also%20successfully%20executed%20several%20daily%20tasks%2C%20including%20closing%20bottle%20caps%20and%20orienting%20a%20pen%20for%20writing.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07559v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520In-hand%2520Manipulation%2520for%2520a%2520Lightweight%2520Four-motor%2520Prosthetic%2520Hand%26entry.906535625%3DYuki%2520Kuroda%2520and%2520Tomoya%2520Takahashi%2520and%2520Cristian%2520C.%2520Beltran-Hernandez%2520and%2520Kazutoshi%2520Tanaka%2520and%2520Masashi%2520Hamaya%26entry.1292438233%3DElectric%2520prosthetic%2520hands%2520should%2520be%2520lightweight%2520to%2520decrease%2520the%2520burden%2520on%2520the%2520user%252C%2520shaped%2520like%2520human%2520hands%2520for%2520cosmetic%2520purposes%252C%2520and%2520designed%2520with%2520motors%2520enclosed%2520inside%2520to%2520protect%2520them%2520from%2520damage%2520and%2520dirt.%2520Additionally%252C%2520in-hand%2520manipulation%2520is%2520necessary%2520to%2520perform%2520daily%2520activities%2520such%2520as%2520transitioning%2520between%2520different%2520postures%252C%2520particularly%2520through%2520rotational%2520movements%252C%2520such%2520as%2520reorienting%2520a%2520pen%2520into%2520a%2520writing%2520posture%2520after%2520picking%2520it%2520up%2520from%2520a%2520desk.%2520We%2520previously%2520developed%2520PLEXUS%2520hand%2520%2528Precision-Lateral%2520dEXteroUS%2520manipulation%2520hand%2529%252C%2520a%2520lightweight%2520%2528311%2520g%2529%2520prosthetic%2520hand%2520driven%2520by%2520four%2520motors.%2520This%2520prosthetic%2520performed%2520reorientation%2520between%2520precision%2520and%2520lateral%2520grasps%2520with%2520various%2520objects.%2520However%252C%2520its%2520controller%2520required%2520predefined%2520object%2520widths%2520and%2520was%2520limited%2520to%2520handling%2520lightweight%2520objects%2520%2528of%2520weight%2520up%2520to%252034%2520g%2529.%2520This%2520study%2520addresses%2520these%2520limitations%2520by%2520employing%2520motor%2520current%2520feedback.%2520Combined%2520with%2520the%2520hand%2527s%2520previously%2520optimized%2520single-axis%2520thumb%252C%2520this%2520approach%2520achieves%2520more%2520stable%2520manipulation%2520by%2520estimating%2520the%2520object%2527s%2520width%2520and%2520adjusting%2520the%2520index%2520finger%2520position%2520to%2520maintain%2520stable%2520object%2520holding%2520during%2520the%2520reorientation.%2520Experimental%2520validation%2520using%2520primitive%2520objects%2520of%2520various%2520widths%2520%25285-30%2520mm%2529%2520and%2520shapes%2520%2528cylinders%2520and%2520prisms%2529%2520resulted%2520in%2520a%2520100%2525%2520success%2520rate%2520with%2520lightweight%2520objects%2520and%2520maintained%2520a%2520high%2520success%2520rate%2520%2528%253E%253D80%2529%2520even%2520with%2520heavy%2520aluminum%2520prisms%2520%2528of%2520weight%2520up%2520to%2520289%2520g%2529.%2520By%2520contrast%252C%2520the%2520performance%2520without%2520index%2520finger%2520coordination%2520dropped%2520to%2520just%252040%2525%2520on%2520the%2520heaviest%2520289%2520g%2520prism.%2520The%2520hand%2520also%2520successfully%2520executed%2520several%2520daily%2520tasks%252C%2520including%2520closing%2520bottle%2520caps%2520and%2520orienting%2520a%2520pen%2520for%2520writing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07559v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20In-hand%20Manipulation%20for%20a%20Lightweight%20Four-motor%20Prosthetic%20Hand&entry.906535625=Yuki%20Kuroda%20and%20Tomoya%20Takahashi%20and%20Cristian%20C.%20Beltran-Hernandez%20and%20Kazutoshi%20Tanaka%20and%20Masashi%20Hamaya&entry.1292438233=Electric%20prosthetic%20hands%20should%20be%20lightweight%20to%20decrease%20the%20burden%20on%20the%20user%2C%20shaped%20like%20human%20hands%20for%20cosmetic%20purposes%2C%20and%20designed%20with%20motors%20enclosed%20inside%20to%20protect%20them%20from%20damage%20and%20dirt.%20Additionally%2C%20in-hand%20manipulation%20is%20necessary%20to%20perform%20daily%20activities%20such%20as%20transitioning%20between%20different%20postures%2C%20particularly%20through%20rotational%20movements%2C%20such%20as%20reorienting%20a%20pen%20into%20a%20writing%20posture%20after%20picking%20it%20up%20from%20a%20desk.%20We%20previously%20developed%20PLEXUS%20hand%20%28Precision-Lateral%20dEXteroUS%20manipulation%20hand%29%2C%20a%20lightweight%20%28311%20g%29%20prosthetic%20hand%20driven%20by%20four%20motors.%20This%20prosthetic%20performed%20reorientation%20between%20precision%20and%20lateral%20grasps%20with%20various%20objects.%20However%2C%20its%20controller%20required%20predefined%20object%20widths%20and%20was%20limited%20to%20handling%20lightweight%20objects%20%28of%20weight%20up%20to%2034%20g%29.%20This%20study%20addresses%20these%20limitations%20by%20employing%20motor%20current%20feedback.%20Combined%20with%20the%20hand%27s%20previously%20optimized%20single-axis%20thumb%2C%20this%20approach%20achieves%20more%20stable%20manipulation%20by%20estimating%20the%20object%27s%20width%20and%20adjusting%20the%20index%20finger%20position%20to%20maintain%20stable%20object%20holding%20during%20the%20reorientation.%20Experimental%20validation%20using%20primitive%20objects%20of%20various%20widths%20%285-30%20mm%29%20and%20shapes%20%28cylinders%20and%20prisms%29%20resulted%20in%20a%20100%25%20success%20rate%20with%20lightweight%20objects%20and%20maintained%20a%20high%20success%20rate%20%28%3E%3D80%29%20even%20with%20heavy%20aluminum%20prisms%20%28of%20weight%20up%20to%20289%20g%29.%20By%20contrast%2C%20the%20performance%20without%20index%20finger%20coordination%20dropped%20to%20just%2040%25%20on%20the%20heaviest%20289%20g%20prism.%20The%20hand%20also%20successfully%20executed%20several%20daily%20tasks%2C%20including%20closing%20bottle%20caps%20and%20orienting%20a%20pen%20for%20writing.&entry.1838667208=http%3A//arxiv.org/abs/2601.07559v1&entry.124074799=Read"},
{"title": "Kinship Data Benchmark for Multi-hop Reasoning", "author": "Tianda Sun and Dimitar Kazakov", "abstract": "Large language models (LLMs) are increasingly evaluated on their ability to perform multi-hop reasoning, i.e., to combine multiple pieces of information into a coherent inference. We introduce KinshipQA, a benchmark designed to probe this capability through reasoning over kinship relations. The central contribution of our work is a generative pipeline that produces, on demand, large-scale, realistic, and culture-specific genealogical data: collections of interconnected family trees that satisfy explicit marriage constraints associated with different kinship systems. This allows task difficulty, cultural assumptions, and relational depth to be systematically controlled and varied. From these genealogies, we derive textual inference tasks that require reasoning over implicit relational chains. We evaluate the resulting benchmark using six state-of-the-art LLMs, spanning both open-source and closed-source models, under a uniform zero-shot protocol with deterministic decoding. Performance is measured using exact-match and set-based metrics. Our results demonstrate that KinshipQA yields a wide spread of outcomes and exposes systematic differences in multi-hop reasoning across models and cultural settings.", "link": "http://arxiv.org/abs/2601.07794v1", "date": "2026-01-12", "relevancy": 2.0255, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5085}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5085}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kinship%20Data%20Benchmark%20for%20Multi-hop%20Reasoning&body=Title%3A%20Kinship%20Data%20Benchmark%20for%20Multi-hop%20Reasoning%0AAuthor%3A%20Tianda%20Sun%20and%20Dimitar%20Kazakov%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20evaluated%20on%20their%20ability%20to%20perform%20multi-hop%20reasoning%2C%20i.e.%2C%20to%20combine%20multiple%20pieces%20of%20information%20into%20a%20coherent%20inference.%20We%20introduce%20KinshipQA%2C%20a%20benchmark%20designed%20to%20probe%20this%20capability%20through%20reasoning%20over%20kinship%20relations.%20The%20central%20contribution%20of%20our%20work%20is%20a%20generative%20pipeline%20that%20produces%2C%20on%20demand%2C%20large-scale%2C%20realistic%2C%20and%20culture-specific%20genealogical%20data%3A%20collections%20of%20interconnected%20family%20trees%20that%20satisfy%20explicit%20marriage%20constraints%20associated%20with%20different%20kinship%20systems.%20This%20allows%20task%20difficulty%2C%20cultural%20assumptions%2C%20and%20relational%20depth%20to%20be%20systematically%20controlled%20and%20varied.%20From%20these%20genealogies%2C%20we%20derive%20textual%20inference%20tasks%20that%20require%20reasoning%20over%20implicit%20relational%20chains.%20We%20evaluate%20the%20resulting%20benchmark%20using%20six%20state-of-the-art%20LLMs%2C%20spanning%20both%20open-source%20and%20closed-source%20models%2C%20under%20a%20uniform%20zero-shot%20protocol%20with%20deterministic%20decoding.%20Performance%20is%20measured%20using%20exact-match%20and%20set-based%20metrics.%20Our%20results%20demonstrate%20that%20KinshipQA%20yields%20a%20wide%20spread%20of%20outcomes%20and%20exposes%20systematic%20differences%20in%20multi-hop%20reasoning%20across%20models%20and%20cultural%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKinship%2520Data%2520Benchmark%2520for%2520Multi-hop%2520Reasoning%26entry.906535625%3DTianda%2520Sun%2520and%2520Dimitar%2520Kazakov%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520evaluated%2520on%2520their%2520ability%2520to%2520perform%2520multi-hop%2520reasoning%252C%2520i.e.%252C%2520to%2520combine%2520multiple%2520pieces%2520of%2520information%2520into%2520a%2520coherent%2520inference.%2520We%2520introduce%2520KinshipQA%252C%2520a%2520benchmark%2520designed%2520to%2520probe%2520this%2520capability%2520through%2520reasoning%2520over%2520kinship%2520relations.%2520The%2520central%2520contribution%2520of%2520our%2520work%2520is%2520a%2520generative%2520pipeline%2520that%2520produces%252C%2520on%2520demand%252C%2520large-scale%252C%2520realistic%252C%2520and%2520culture-specific%2520genealogical%2520data%253A%2520collections%2520of%2520interconnected%2520family%2520trees%2520that%2520satisfy%2520explicit%2520marriage%2520constraints%2520associated%2520with%2520different%2520kinship%2520systems.%2520This%2520allows%2520task%2520difficulty%252C%2520cultural%2520assumptions%252C%2520and%2520relational%2520depth%2520to%2520be%2520systematically%2520controlled%2520and%2520varied.%2520From%2520these%2520genealogies%252C%2520we%2520derive%2520textual%2520inference%2520tasks%2520that%2520require%2520reasoning%2520over%2520implicit%2520relational%2520chains.%2520We%2520evaluate%2520the%2520resulting%2520benchmark%2520using%2520six%2520state-of-the-art%2520LLMs%252C%2520spanning%2520both%2520open-source%2520and%2520closed-source%2520models%252C%2520under%2520a%2520uniform%2520zero-shot%2520protocol%2520with%2520deterministic%2520decoding.%2520Performance%2520is%2520measured%2520using%2520exact-match%2520and%2520set-based%2520metrics.%2520Our%2520results%2520demonstrate%2520that%2520KinshipQA%2520yields%2520a%2520wide%2520spread%2520of%2520outcomes%2520and%2520exposes%2520systematic%2520differences%2520in%2520multi-hop%2520reasoning%2520across%2520models%2520and%2520cultural%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kinship%20Data%20Benchmark%20for%20Multi-hop%20Reasoning&entry.906535625=Tianda%20Sun%20and%20Dimitar%20Kazakov&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20increasingly%20evaluated%20on%20their%20ability%20to%20perform%20multi-hop%20reasoning%2C%20i.e.%2C%20to%20combine%20multiple%20pieces%20of%20information%20into%20a%20coherent%20inference.%20We%20introduce%20KinshipQA%2C%20a%20benchmark%20designed%20to%20probe%20this%20capability%20through%20reasoning%20over%20kinship%20relations.%20The%20central%20contribution%20of%20our%20work%20is%20a%20generative%20pipeline%20that%20produces%2C%20on%20demand%2C%20large-scale%2C%20realistic%2C%20and%20culture-specific%20genealogical%20data%3A%20collections%20of%20interconnected%20family%20trees%20that%20satisfy%20explicit%20marriage%20constraints%20associated%20with%20different%20kinship%20systems.%20This%20allows%20task%20difficulty%2C%20cultural%20assumptions%2C%20and%20relational%20depth%20to%20be%20systematically%20controlled%20and%20varied.%20From%20these%20genealogies%2C%20we%20derive%20textual%20inference%20tasks%20that%20require%20reasoning%20over%20implicit%20relational%20chains.%20We%20evaluate%20the%20resulting%20benchmark%20using%20six%20state-of-the-art%20LLMs%2C%20spanning%20both%20open-source%20and%20closed-source%20models%2C%20under%20a%20uniform%20zero-shot%20protocol%20with%20deterministic%20decoding.%20Performance%20is%20measured%20using%20exact-match%20and%20set-based%20metrics.%20Our%20results%20demonstrate%20that%20KinshipQA%20yields%20a%20wide%20spread%20of%20outcomes%20and%20exposes%20systematic%20differences%20in%20multi-hop%20reasoning%20across%20models%20and%20cultural%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2601.07794v1&entry.124074799=Read"},
{"title": "Learning Generalizable and Efficient Image Watermarking via Hierarchical Two-Stage Optimization", "author": "Ke Liu and Xuanhan Wang and Qilong Zhang and Lianli Gao and Jingkuan Song", "abstract": "Deep image watermarking, which refers to enabling imperceptible watermark embedding and reliable extraction in cover images, has been shown to be effective for copyright protection of image assets. However, existing methods face limitations in simultaneously satisfying three essential criteria for generalizable watermarking: (1) invisibility (imperceptible hiding of watermarks), (2) robustness (reliable watermark recovery under diverse conditions), and (3) broad applicability (low latency in the watermarking process). To address these limitations, we propose a Hierarchical Watermark Learning (HiWL) framework, a two-stage optimization that enables a watermarking model to simultaneously achieve all three criteria. In the first stage, distribution alignment learning is designed to establish a common latent space with two constraints: (1) visual consistency between watermarked and non-watermarked images, and (2) information invariance across watermark latent representations. In this way, multimodal inputs -- including watermark messages (binary codes) and cover images (RGB pixels) -- can be effectively represented, ensuring both the invisibility of watermarks and robustness in the watermarking process. In the second stage, we employ generalized watermark representation learning to separate a unique representation of the watermark from the marked image in RGB space. Once trained, the HiWL model effectively learns generalizable watermark representations while maintaining broad applicability. Extensive experiments demonstrate the effectiveness of the proposed method. Specifically, it achieves 7.6% higher accuracy in watermark extraction compared to existing methods, while maintaining extremely low latency (processing 1000 images in 1 second).", "link": "http://arxiv.org/abs/2508.08667v2", "date": "2026-01-12", "relevancy": 2.0178, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5201}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4956}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Generalizable%20and%20Efficient%20Image%20Watermarking%20via%20Hierarchical%20Two-Stage%20Optimization&body=Title%3A%20Learning%20Generalizable%20and%20Efficient%20Image%20Watermarking%20via%20Hierarchical%20Two-Stage%20Optimization%0AAuthor%3A%20Ke%20Liu%20and%20Xuanhan%20Wang%20and%20Qilong%20Zhang%20and%20Lianli%20Gao%20and%20Jingkuan%20Song%0AAbstract%3A%20Deep%20image%20watermarking%2C%20which%20refers%20to%20enabling%20imperceptible%20watermark%20embedding%20and%20reliable%20extraction%20in%20cover%20images%2C%20has%20been%20shown%20to%20be%20effective%20for%20copyright%20protection%20of%20image%20assets.%20However%2C%20existing%20methods%20face%20limitations%20in%20simultaneously%20satisfying%20three%20essential%20criteria%20for%20generalizable%20watermarking%3A%20%281%29%20invisibility%20%28imperceptible%20hiding%20of%20watermarks%29%2C%20%282%29%20robustness%20%28reliable%20watermark%20recovery%20under%20diverse%20conditions%29%2C%20and%20%283%29%20broad%20applicability%20%28low%20latency%20in%20the%20watermarking%20process%29.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20Hierarchical%20Watermark%20Learning%20%28HiWL%29%20framework%2C%20a%20two-stage%20optimization%20that%20enables%20a%20watermarking%20model%20to%20simultaneously%20achieve%20all%20three%20criteria.%20In%20the%20first%20stage%2C%20distribution%20alignment%20learning%20is%20designed%20to%20establish%20a%20common%20latent%20space%20with%20two%20constraints%3A%20%281%29%20visual%20consistency%20between%20watermarked%20and%20non-watermarked%20images%2C%20and%20%282%29%20information%20invariance%20across%20watermark%20latent%20representations.%20In%20this%20way%2C%20multimodal%20inputs%20--%20including%20watermark%20messages%20%28binary%20codes%29%20and%20cover%20images%20%28RGB%20pixels%29%20--%20can%20be%20effectively%20represented%2C%20ensuring%20both%20the%20invisibility%20of%20watermarks%20and%20robustness%20in%20the%20watermarking%20process.%20In%20the%20second%20stage%2C%20we%20employ%20generalized%20watermark%20representation%20learning%20to%20separate%20a%20unique%20representation%20of%20the%20watermark%20from%20the%20marked%20image%20in%20RGB%20space.%20Once%20trained%2C%20the%20HiWL%20model%20effectively%20learns%20generalizable%20watermark%20representations%20while%20maintaining%20broad%20applicability.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%20Specifically%2C%20it%20achieves%207.6%25%20higher%20accuracy%20in%20watermark%20extraction%20compared%20to%20existing%20methods%2C%20while%20maintaining%20extremely%20low%20latency%20%28processing%201000%20images%20in%201%20second%29.%0ALink%3A%20http%3A//arxiv.org/abs/2508.08667v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Generalizable%2520and%2520Efficient%2520Image%2520Watermarking%2520via%2520Hierarchical%2520Two-Stage%2520Optimization%26entry.906535625%3DKe%2520Liu%2520and%2520Xuanhan%2520Wang%2520and%2520Qilong%2520Zhang%2520and%2520Lianli%2520Gao%2520and%2520Jingkuan%2520Song%26entry.1292438233%3DDeep%2520image%2520watermarking%252C%2520which%2520refers%2520to%2520enabling%2520imperceptible%2520watermark%2520embedding%2520and%2520reliable%2520extraction%2520in%2520cover%2520images%252C%2520has%2520been%2520shown%2520to%2520be%2520effective%2520for%2520copyright%2520protection%2520of%2520image%2520assets.%2520However%252C%2520existing%2520methods%2520face%2520limitations%2520in%2520simultaneously%2520satisfying%2520three%2520essential%2520criteria%2520for%2520generalizable%2520watermarking%253A%2520%25281%2529%2520invisibility%2520%2528imperceptible%2520hiding%2520of%2520watermarks%2529%252C%2520%25282%2529%2520robustness%2520%2528reliable%2520watermark%2520recovery%2520under%2520diverse%2520conditions%2529%252C%2520and%2520%25283%2529%2520broad%2520applicability%2520%2528low%2520latency%2520in%2520the%2520watermarking%2520process%2529.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520Hierarchical%2520Watermark%2520Learning%2520%2528HiWL%2529%2520framework%252C%2520a%2520two-stage%2520optimization%2520that%2520enables%2520a%2520watermarking%2520model%2520to%2520simultaneously%2520achieve%2520all%2520three%2520criteria.%2520In%2520the%2520first%2520stage%252C%2520distribution%2520alignment%2520learning%2520is%2520designed%2520to%2520establish%2520a%2520common%2520latent%2520space%2520with%2520two%2520constraints%253A%2520%25281%2529%2520visual%2520consistency%2520between%2520watermarked%2520and%2520non-watermarked%2520images%252C%2520and%2520%25282%2529%2520information%2520invariance%2520across%2520watermark%2520latent%2520representations.%2520In%2520this%2520way%252C%2520multimodal%2520inputs%2520--%2520including%2520watermark%2520messages%2520%2528binary%2520codes%2529%2520and%2520cover%2520images%2520%2528RGB%2520pixels%2529%2520--%2520can%2520be%2520effectively%2520represented%252C%2520ensuring%2520both%2520the%2520invisibility%2520of%2520watermarks%2520and%2520robustness%2520in%2520the%2520watermarking%2520process.%2520In%2520the%2520second%2520stage%252C%2520we%2520employ%2520generalized%2520watermark%2520representation%2520learning%2520to%2520separate%2520a%2520unique%2520representation%2520of%2520the%2520watermark%2520from%2520the%2520marked%2520image%2520in%2520RGB%2520space.%2520Once%2520trained%252C%2520the%2520HiWL%2520model%2520effectively%2520learns%2520generalizable%2520watermark%2520representations%2520while%2520maintaining%2520broad%2520applicability.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%2520Specifically%252C%2520it%2520achieves%25207.6%2525%2520higher%2520accuracy%2520in%2520watermark%2520extraction%2520compared%2520to%2520existing%2520methods%252C%2520while%2520maintaining%2520extremely%2520low%2520latency%2520%2528processing%25201000%2520images%2520in%25201%2520second%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08667v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Generalizable%20and%20Efficient%20Image%20Watermarking%20via%20Hierarchical%20Two-Stage%20Optimization&entry.906535625=Ke%20Liu%20and%20Xuanhan%20Wang%20and%20Qilong%20Zhang%20and%20Lianli%20Gao%20and%20Jingkuan%20Song&entry.1292438233=Deep%20image%20watermarking%2C%20which%20refers%20to%20enabling%20imperceptible%20watermark%20embedding%20and%20reliable%20extraction%20in%20cover%20images%2C%20has%20been%20shown%20to%20be%20effective%20for%20copyright%20protection%20of%20image%20assets.%20However%2C%20existing%20methods%20face%20limitations%20in%20simultaneously%20satisfying%20three%20essential%20criteria%20for%20generalizable%20watermarking%3A%20%281%29%20invisibility%20%28imperceptible%20hiding%20of%20watermarks%29%2C%20%282%29%20robustness%20%28reliable%20watermark%20recovery%20under%20diverse%20conditions%29%2C%20and%20%283%29%20broad%20applicability%20%28low%20latency%20in%20the%20watermarking%20process%29.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20Hierarchical%20Watermark%20Learning%20%28HiWL%29%20framework%2C%20a%20two-stage%20optimization%20that%20enables%20a%20watermarking%20model%20to%20simultaneously%20achieve%20all%20three%20criteria.%20In%20the%20first%20stage%2C%20distribution%20alignment%20learning%20is%20designed%20to%20establish%20a%20common%20latent%20space%20with%20two%20constraints%3A%20%281%29%20visual%20consistency%20between%20watermarked%20and%20non-watermarked%20images%2C%20and%20%282%29%20information%20invariance%20across%20watermark%20latent%20representations.%20In%20this%20way%2C%20multimodal%20inputs%20--%20including%20watermark%20messages%20%28binary%20codes%29%20and%20cover%20images%20%28RGB%20pixels%29%20--%20can%20be%20effectively%20represented%2C%20ensuring%20both%20the%20invisibility%20of%20watermarks%20and%20robustness%20in%20the%20watermarking%20process.%20In%20the%20second%20stage%2C%20we%20employ%20generalized%20watermark%20representation%20learning%20to%20separate%20a%20unique%20representation%20of%20the%20watermark%20from%20the%20marked%20image%20in%20RGB%20space.%20Once%20trained%2C%20the%20HiWL%20model%20effectively%20learns%20generalizable%20watermark%20representations%20while%20maintaining%20broad%20applicability.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%20Specifically%2C%20it%20achieves%207.6%25%20higher%20accuracy%20in%20watermark%20extraction%20compared%20to%20existing%20methods%2C%20while%20maintaining%20extremely%20low%20latency%20%28processing%201000%20images%20in%201%20second%29.&entry.1838667208=http%3A//arxiv.org/abs/2508.08667v2&entry.124074799=Read"},
{"title": "Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification", "author": "Yahya Masri and Emily Ma and Zifu Wang and Joseph Rogers and Chaowei Yang", "abstract": "System logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results reveal strong stratification. Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. In contrast, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements further separate models: most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving <10% accuracy. These findings suggest that (1) architectural design, (2) training objectives, and (3) the ability to integrate retrieved context under strict output constraints jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability, with implications for root cause analysis (RCA) and broader DT integration.", "link": "http://arxiv.org/abs/2601.07790v1", "date": "2026-01-12", "relevancy": 2.0155, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Small%20Language%20Models%20and%20Small%20Reasoning%20Language%20Models%20on%20System%20Log%20Severity%20Classification&body=Title%3A%20Benchmarking%20Small%20Language%20Models%20and%20Small%20Reasoning%20Language%20Models%20on%20System%20Log%20Severity%20Classification%0AAuthor%3A%20Yahya%20Masri%20and%20Emily%20Ma%20and%20Zifu%20Wang%20and%20Joseph%20Rogers%20and%20Chaowei%20Yang%0AAbstract%3A%20System%20logs%20are%20crucial%20for%20monitoring%20and%20diagnosing%20modern%20computing%20infrastructure%2C%20but%20their%20scale%20and%20complexity%20require%20reliable%20and%20efficient%20automated%20interpretation.%20Since%20severity%20levels%20are%20predefined%20metadata%20in%20system%20log%20messages%2C%20having%20a%20model%20merely%20classify%20them%20offers%20limited%20standalone%20practical%20value%2C%20revealing%20little%20about%20its%20underlying%20ability%20to%20interpret%20system%20logs.%20We%20argue%20that%20severity%20classification%20is%20more%20informative%20when%20treated%20as%20a%20benchmark%20for%20probing%20runtime%20log%20comprehension%20rather%20than%20as%20an%20end%20task.%20Using%20real-world%20journalctl%20data%20from%20Linux%20production%20servers%2C%20we%20evaluate%20nine%20small%20language%20models%20%28SLMs%29%20and%20small%20reasoning%20language%20models%20%28SRLMs%29%20under%20zero-shot%2C%20few-shot%2C%20and%20retrieval-augmented%20generation%20%28RAG%29%20prompting.%20The%20results%20reveal%20strong%20stratification.%20Qwen3-4B%20achieves%20the%20highest%20accuracy%20at%2095.64%25%20with%20RAG%2C%20while%20Gemma3-1B%20improves%20from%2020.25%25%20under%20few-shot%20prompting%20to%2085.28%25%20with%20RAG.%20Notably%2C%20the%20tiny%20Qwen3-0.6B%20reaches%2088.12%25%20accuracy%20despite%20weak%20performance%20without%20retrieval.%20In%20contrast%2C%20several%20SRLMs%2C%20including%20Qwen3-1.7B%20and%20DeepSeek-R1-Distill-Qwen-1.5B%2C%20degrade%20substantially%20when%20paired%20with%20RAG.%20Efficiency%20measurements%20further%20separate%20models%3A%20most%20Gemma%20and%20Llama%20variants%20complete%20inference%20in%20under%201.2%20seconds%20per%20log%2C%20whereas%20Phi-4-Mini-Reasoning%20exceeds%20228%20seconds%20per%20log%20while%20achieving%20%3C10%25%20accuracy.%20These%20findings%20suggest%20that%20%281%29%20architectural%20design%2C%20%282%29%20training%20objectives%2C%20and%20%283%29%20the%20ability%20to%20integrate%20retrieved%20context%20under%20strict%20output%20constraints%20jointly%20determine%20performance.%20By%20emphasizing%20small%2C%20deployable%20models%2C%20this%20benchmark%20aligns%20with%20real-time%20requirements%20of%20digital%20twin%20%28DT%29%20systems%20and%20shows%20that%20severity%20classification%20serves%20as%20a%20lens%20for%20evaluating%20model%20competence%20and%20real-time%20deployability%2C%20with%20implications%20for%20root%20cause%20analysis%20%28RCA%29%20and%20broader%20DT%20integration.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Small%2520Language%2520Models%2520and%2520Small%2520Reasoning%2520Language%2520Models%2520on%2520System%2520Log%2520Severity%2520Classification%26entry.906535625%3DYahya%2520Masri%2520and%2520Emily%2520Ma%2520and%2520Zifu%2520Wang%2520and%2520Joseph%2520Rogers%2520and%2520Chaowei%2520Yang%26entry.1292438233%3DSystem%2520logs%2520are%2520crucial%2520for%2520monitoring%2520and%2520diagnosing%2520modern%2520computing%2520infrastructure%252C%2520but%2520their%2520scale%2520and%2520complexity%2520require%2520reliable%2520and%2520efficient%2520automated%2520interpretation.%2520Since%2520severity%2520levels%2520are%2520predefined%2520metadata%2520in%2520system%2520log%2520messages%252C%2520having%2520a%2520model%2520merely%2520classify%2520them%2520offers%2520limited%2520standalone%2520practical%2520value%252C%2520revealing%2520little%2520about%2520its%2520underlying%2520ability%2520to%2520interpret%2520system%2520logs.%2520We%2520argue%2520that%2520severity%2520classification%2520is%2520more%2520informative%2520when%2520treated%2520as%2520a%2520benchmark%2520for%2520probing%2520runtime%2520log%2520comprehension%2520rather%2520than%2520as%2520an%2520end%2520task.%2520Using%2520real-world%2520journalctl%2520data%2520from%2520Linux%2520production%2520servers%252C%2520we%2520evaluate%2520nine%2520small%2520language%2520models%2520%2528SLMs%2529%2520and%2520small%2520reasoning%2520language%2520models%2520%2528SRLMs%2529%2520under%2520zero-shot%252C%2520few-shot%252C%2520and%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520prompting.%2520The%2520results%2520reveal%2520strong%2520stratification.%2520Qwen3-4B%2520achieves%2520the%2520highest%2520accuracy%2520at%252095.64%2525%2520with%2520RAG%252C%2520while%2520Gemma3-1B%2520improves%2520from%252020.25%2525%2520under%2520few-shot%2520prompting%2520to%252085.28%2525%2520with%2520RAG.%2520Notably%252C%2520the%2520tiny%2520Qwen3-0.6B%2520reaches%252088.12%2525%2520accuracy%2520despite%2520weak%2520performance%2520without%2520retrieval.%2520In%2520contrast%252C%2520several%2520SRLMs%252C%2520including%2520Qwen3-1.7B%2520and%2520DeepSeek-R1-Distill-Qwen-1.5B%252C%2520degrade%2520substantially%2520when%2520paired%2520with%2520RAG.%2520Efficiency%2520measurements%2520further%2520separate%2520models%253A%2520most%2520Gemma%2520and%2520Llama%2520variants%2520complete%2520inference%2520in%2520under%25201.2%2520seconds%2520per%2520log%252C%2520whereas%2520Phi-4-Mini-Reasoning%2520exceeds%2520228%2520seconds%2520per%2520log%2520while%2520achieving%2520%253C10%2525%2520accuracy.%2520These%2520findings%2520suggest%2520that%2520%25281%2529%2520architectural%2520design%252C%2520%25282%2529%2520training%2520objectives%252C%2520and%2520%25283%2529%2520the%2520ability%2520to%2520integrate%2520retrieved%2520context%2520under%2520strict%2520output%2520constraints%2520jointly%2520determine%2520performance.%2520By%2520emphasizing%2520small%252C%2520deployable%2520models%252C%2520this%2520benchmark%2520aligns%2520with%2520real-time%2520requirements%2520of%2520digital%2520twin%2520%2528DT%2529%2520systems%2520and%2520shows%2520that%2520severity%2520classification%2520serves%2520as%2520a%2520lens%2520for%2520evaluating%2520model%2520competence%2520and%2520real-time%2520deployability%252C%2520with%2520implications%2520for%2520root%2520cause%2520analysis%2520%2528RCA%2529%2520and%2520broader%2520DT%2520integration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Small%20Language%20Models%20and%20Small%20Reasoning%20Language%20Models%20on%20System%20Log%20Severity%20Classification&entry.906535625=Yahya%20Masri%20and%20Emily%20Ma%20and%20Zifu%20Wang%20and%20Joseph%20Rogers%20and%20Chaowei%20Yang&entry.1292438233=System%20logs%20are%20crucial%20for%20monitoring%20and%20diagnosing%20modern%20computing%20infrastructure%2C%20but%20their%20scale%20and%20complexity%20require%20reliable%20and%20efficient%20automated%20interpretation.%20Since%20severity%20levels%20are%20predefined%20metadata%20in%20system%20log%20messages%2C%20having%20a%20model%20merely%20classify%20them%20offers%20limited%20standalone%20practical%20value%2C%20revealing%20little%20about%20its%20underlying%20ability%20to%20interpret%20system%20logs.%20We%20argue%20that%20severity%20classification%20is%20more%20informative%20when%20treated%20as%20a%20benchmark%20for%20probing%20runtime%20log%20comprehension%20rather%20than%20as%20an%20end%20task.%20Using%20real-world%20journalctl%20data%20from%20Linux%20production%20servers%2C%20we%20evaluate%20nine%20small%20language%20models%20%28SLMs%29%20and%20small%20reasoning%20language%20models%20%28SRLMs%29%20under%20zero-shot%2C%20few-shot%2C%20and%20retrieval-augmented%20generation%20%28RAG%29%20prompting.%20The%20results%20reveal%20strong%20stratification.%20Qwen3-4B%20achieves%20the%20highest%20accuracy%20at%2095.64%25%20with%20RAG%2C%20while%20Gemma3-1B%20improves%20from%2020.25%25%20under%20few-shot%20prompting%20to%2085.28%25%20with%20RAG.%20Notably%2C%20the%20tiny%20Qwen3-0.6B%20reaches%2088.12%25%20accuracy%20despite%20weak%20performance%20without%20retrieval.%20In%20contrast%2C%20several%20SRLMs%2C%20including%20Qwen3-1.7B%20and%20DeepSeek-R1-Distill-Qwen-1.5B%2C%20degrade%20substantially%20when%20paired%20with%20RAG.%20Efficiency%20measurements%20further%20separate%20models%3A%20most%20Gemma%20and%20Llama%20variants%20complete%20inference%20in%20under%201.2%20seconds%20per%20log%2C%20whereas%20Phi-4-Mini-Reasoning%20exceeds%20228%20seconds%20per%20log%20while%20achieving%20%3C10%25%20accuracy.%20These%20findings%20suggest%20that%20%281%29%20architectural%20design%2C%20%282%29%20training%20objectives%2C%20and%20%283%29%20the%20ability%20to%20integrate%20retrieved%20context%20under%20strict%20output%20constraints%20jointly%20determine%20performance.%20By%20emphasizing%20small%2C%20deployable%20models%2C%20this%20benchmark%20aligns%20with%20real-time%20requirements%20of%20digital%20twin%20%28DT%29%20systems%20and%20shows%20that%20severity%20classification%20serves%20as%20a%20lens%20for%20evaluating%20model%20competence%20and%20real-time%20deployability%2C%20with%20implications%20for%20root%20cause%20analysis%20%28RCA%29%20and%20broader%20DT%20integration.&entry.1838667208=http%3A//arxiv.org/abs/2601.07790v1&entry.124074799=Read"},
{"title": "Backpropagation through space, time, and the brain", "author": "Benjamin Ellenberger and Paul Haider and Jakob Jordan and Kevin Max and Ismael Jaras and Laura Kriener and Federico Benitez and Mihai A. Petrovici", "abstract": "How physical networks of neurons, bound by spatio-temporal locality constraints, can perform efficient credit assignment, remains, to a large extent, an open question. In machine learning, the answer is almost universally given by the error backpropagation algorithm, through both space and time. However, this algorithm is well-known to rely on biologically implausible assumptions, in particular with respect to spatio-temporal (non-)locality. Alternative forward-propagation models such as real-time recurrent learning only partially solve the locality problem, but only at the cost of scaling, due to prohibitive storage requirements. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by defining an energy based on neuron-local mismatches, from which we derive both neuronal dynamics via stationarity and parameter dynamics via gradient descent. The resulting dynamics can be interpreted as a real-time, biologically plausible approximation of backpropagation through space and time in deep cortical networks with continuous-time neuronal dynamics and continuously active, local synaptic plasticity. In particular, GLE exploits the morphology of dendritic trees to enable more complex information storage and processing in single neurons, as well as the ability of biological neurons to phase-shift their output rate with respect to their membrane potential, which is essential in both directions of information propagation. For the forward computation, it enables the mapping of time-continuous inputs to neuronal space, effectively performing a spatio-temporal convolution. For the backward computation, it permits the temporal inversion of feedback signals, which consequently approximate the adjoint variables necessary for useful parameter updates.", "link": "http://arxiv.org/abs/2403.16933v4", "date": "2026-01-12", "relevancy": 2.0126, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5243}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5217}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Backpropagation%20through%20space%2C%20time%2C%20and%20the%20brain&body=Title%3A%20Backpropagation%20through%20space%2C%20time%2C%20and%20the%20brain%0AAuthor%3A%20Benjamin%20Ellenberger%20and%20Paul%20Haider%20and%20Jakob%20Jordan%20and%20Kevin%20Max%20and%20Ismael%20Jaras%20and%20Laura%20Kriener%20and%20Federico%20Benitez%20and%20Mihai%20A.%20Petrovici%0AAbstract%3A%20How%20physical%20networks%20of%20neurons%2C%20bound%20by%20spatio-temporal%20locality%20constraints%2C%20can%20perform%20efficient%20credit%20assignment%2C%20remains%2C%20to%20a%20large%20extent%2C%20an%20open%20question.%20In%20machine%20learning%2C%20the%20answer%20is%20almost%20universally%20given%20by%20the%20error%20backpropagation%20algorithm%2C%20through%20both%20space%20and%20time.%20However%2C%20this%20algorithm%20is%20well-known%20to%20rely%20on%20biologically%20implausible%20assumptions%2C%20in%20particular%20with%20respect%20to%20spatio-temporal%20%28non-%29locality.%20Alternative%20forward-propagation%20models%20such%20as%20real-time%20recurrent%20learning%20only%20partially%20solve%20the%20locality%20problem%2C%20but%20only%20at%20the%20cost%20of%20scaling%2C%20due%20to%20prohibitive%20storage%20requirements.%20We%20introduce%20Generalized%20Latent%20Equilibrium%20%28GLE%29%2C%20a%20computational%20framework%20for%20fully%20local%20spatio-temporal%20credit%20assignment%20in%20physical%2C%20dynamical%20networks%20of%20neurons.%20We%20start%20by%20defining%20an%20energy%20based%20on%20neuron-local%20mismatches%2C%20from%20which%20we%20derive%20both%20neuronal%20dynamics%20via%20stationarity%20and%20parameter%20dynamics%20via%20gradient%20descent.%20The%20resulting%20dynamics%20can%20be%20interpreted%20as%20a%20real-time%2C%20biologically%20plausible%20approximation%20of%20backpropagation%20through%20space%20and%20time%20in%20deep%20cortical%20networks%20with%20continuous-time%20neuronal%20dynamics%20and%20continuously%20active%2C%20local%20synaptic%20plasticity.%20In%20particular%2C%20GLE%20exploits%20the%20morphology%20of%20dendritic%20trees%20to%20enable%20more%20complex%20information%20storage%20and%20processing%20in%20single%20neurons%2C%20as%20well%20as%20the%20ability%20of%20biological%20neurons%20to%20phase-shift%20their%20output%20rate%20with%20respect%20to%20their%20membrane%20potential%2C%20which%20is%20essential%20in%20both%20directions%20of%20information%20propagation.%20For%20the%20forward%20computation%2C%20it%20enables%20the%20mapping%20of%20time-continuous%20inputs%20to%20neuronal%20space%2C%20effectively%20performing%20a%20spatio-temporal%20convolution.%20For%20the%20backward%20computation%2C%20it%20permits%20the%20temporal%20inversion%20of%20feedback%20signals%2C%20which%20consequently%20approximate%20the%20adjoint%20variables%20necessary%20for%20useful%20parameter%20updates.%0ALink%3A%20http%3A//arxiv.org/abs/2403.16933v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackpropagation%2520through%2520space%252C%2520time%252C%2520and%2520the%2520brain%26entry.906535625%3DBenjamin%2520Ellenberger%2520and%2520Paul%2520Haider%2520and%2520Jakob%2520Jordan%2520and%2520Kevin%2520Max%2520and%2520Ismael%2520Jaras%2520and%2520Laura%2520Kriener%2520and%2520Federico%2520Benitez%2520and%2520Mihai%2520A.%2520Petrovici%26entry.1292438233%3DHow%2520physical%2520networks%2520of%2520neurons%252C%2520bound%2520by%2520spatio-temporal%2520locality%2520constraints%252C%2520can%2520perform%2520efficient%2520credit%2520assignment%252C%2520remains%252C%2520to%2520a%2520large%2520extent%252C%2520an%2520open%2520question.%2520In%2520machine%2520learning%252C%2520the%2520answer%2520is%2520almost%2520universally%2520given%2520by%2520the%2520error%2520backpropagation%2520algorithm%252C%2520through%2520both%2520space%2520and%2520time.%2520However%252C%2520this%2520algorithm%2520is%2520well-known%2520to%2520rely%2520on%2520biologically%2520implausible%2520assumptions%252C%2520in%2520particular%2520with%2520respect%2520to%2520spatio-temporal%2520%2528non-%2529locality.%2520Alternative%2520forward-propagation%2520models%2520such%2520as%2520real-time%2520recurrent%2520learning%2520only%2520partially%2520solve%2520the%2520locality%2520problem%252C%2520but%2520only%2520at%2520the%2520cost%2520of%2520scaling%252C%2520due%2520to%2520prohibitive%2520storage%2520requirements.%2520We%2520introduce%2520Generalized%2520Latent%2520Equilibrium%2520%2528GLE%2529%252C%2520a%2520computational%2520framework%2520for%2520fully%2520local%2520spatio-temporal%2520credit%2520assignment%2520in%2520physical%252C%2520dynamical%2520networks%2520of%2520neurons.%2520We%2520start%2520by%2520defining%2520an%2520energy%2520based%2520on%2520neuron-local%2520mismatches%252C%2520from%2520which%2520we%2520derive%2520both%2520neuronal%2520dynamics%2520via%2520stationarity%2520and%2520parameter%2520dynamics%2520via%2520gradient%2520descent.%2520The%2520resulting%2520dynamics%2520can%2520be%2520interpreted%2520as%2520a%2520real-time%252C%2520biologically%2520plausible%2520approximation%2520of%2520backpropagation%2520through%2520space%2520and%2520time%2520in%2520deep%2520cortical%2520networks%2520with%2520continuous-time%2520neuronal%2520dynamics%2520and%2520continuously%2520active%252C%2520local%2520synaptic%2520plasticity.%2520In%2520particular%252C%2520GLE%2520exploits%2520the%2520morphology%2520of%2520dendritic%2520trees%2520to%2520enable%2520more%2520complex%2520information%2520storage%2520and%2520processing%2520in%2520single%2520neurons%252C%2520as%2520well%2520as%2520the%2520ability%2520of%2520biological%2520neurons%2520to%2520phase-shift%2520their%2520output%2520rate%2520with%2520respect%2520to%2520their%2520membrane%2520potential%252C%2520which%2520is%2520essential%2520in%2520both%2520directions%2520of%2520information%2520propagation.%2520For%2520the%2520forward%2520computation%252C%2520it%2520enables%2520the%2520mapping%2520of%2520time-continuous%2520inputs%2520to%2520neuronal%2520space%252C%2520effectively%2520performing%2520a%2520spatio-temporal%2520convolution.%2520For%2520the%2520backward%2520computation%252C%2520it%2520permits%2520the%2520temporal%2520inversion%2520of%2520feedback%2520signals%252C%2520which%2520consequently%2520approximate%2520the%2520adjoint%2520variables%2520necessary%2520for%2520useful%2520parameter%2520updates.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16933v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Backpropagation%20through%20space%2C%20time%2C%20and%20the%20brain&entry.906535625=Benjamin%20Ellenberger%20and%20Paul%20Haider%20and%20Jakob%20Jordan%20and%20Kevin%20Max%20and%20Ismael%20Jaras%20and%20Laura%20Kriener%20and%20Federico%20Benitez%20and%20Mihai%20A.%20Petrovici&entry.1292438233=How%20physical%20networks%20of%20neurons%2C%20bound%20by%20spatio-temporal%20locality%20constraints%2C%20can%20perform%20efficient%20credit%20assignment%2C%20remains%2C%20to%20a%20large%20extent%2C%20an%20open%20question.%20In%20machine%20learning%2C%20the%20answer%20is%20almost%20universally%20given%20by%20the%20error%20backpropagation%20algorithm%2C%20through%20both%20space%20and%20time.%20However%2C%20this%20algorithm%20is%20well-known%20to%20rely%20on%20biologically%20implausible%20assumptions%2C%20in%20particular%20with%20respect%20to%20spatio-temporal%20%28non-%29locality.%20Alternative%20forward-propagation%20models%20such%20as%20real-time%20recurrent%20learning%20only%20partially%20solve%20the%20locality%20problem%2C%20but%20only%20at%20the%20cost%20of%20scaling%2C%20due%20to%20prohibitive%20storage%20requirements.%20We%20introduce%20Generalized%20Latent%20Equilibrium%20%28GLE%29%2C%20a%20computational%20framework%20for%20fully%20local%20spatio-temporal%20credit%20assignment%20in%20physical%2C%20dynamical%20networks%20of%20neurons.%20We%20start%20by%20defining%20an%20energy%20based%20on%20neuron-local%20mismatches%2C%20from%20which%20we%20derive%20both%20neuronal%20dynamics%20via%20stationarity%20and%20parameter%20dynamics%20via%20gradient%20descent.%20The%20resulting%20dynamics%20can%20be%20interpreted%20as%20a%20real-time%2C%20biologically%20plausible%20approximation%20of%20backpropagation%20through%20space%20and%20time%20in%20deep%20cortical%20networks%20with%20continuous-time%20neuronal%20dynamics%20and%20continuously%20active%2C%20local%20synaptic%20plasticity.%20In%20particular%2C%20GLE%20exploits%20the%20morphology%20of%20dendritic%20trees%20to%20enable%20more%20complex%20information%20storage%20and%20processing%20in%20single%20neurons%2C%20as%20well%20as%20the%20ability%20of%20biological%20neurons%20to%20phase-shift%20their%20output%20rate%20with%20respect%20to%20their%20membrane%20potential%2C%20which%20is%20essential%20in%20both%20directions%20of%20information%20propagation.%20For%20the%20forward%20computation%2C%20it%20enables%20the%20mapping%20of%20time-continuous%20inputs%20to%20neuronal%20space%2C%20effectively%20performing%20a%20spatio-temporal%20convolution.%20For%20the%20backward%20computation%2C%20it%20permits%20the%20temporal%20inversion%20of%20feedback%20signals%2C%20which%20consequently%20approximate%20the%20adjoint%20variables%20necessary%20for%20useful%20parameter%20updates.&entry.1838667208=http%3A//arxiv.org/abs/2403.16933v4&entry.124074799=Read"},
{"title": "OpenTinker: Separating Concerns in Agentic Reinforcement Learning", "author": "Siqi Zhu and Jiaxuan You", "abstract": "We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around a separation of concerns across algorithm design, execution, and agent-environment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to a managed execution runtime. OpenTinker introduces a centralized scheduler for managing training and inference workloads, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present a set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios.", "link": "http://arxiv.org/abs/2601.07376v1", "date": "2026-01-12", "relevancy": 2.0114, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5417}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4768}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenTinker%3A%20Separating%20Concerns%20in%20Agentic%20Reinforcement%20Learning&body=Title%3A%20OpenTinker%3A%20Separating%20Concerns%20in%20Agentic%20Reinforcement%20Learning%0AAuthor%3A%20Siqi%20Zhu%20and%20Jiaxuan%20You%0AAbstract%3A%20We%20introduce%20OpenTinker%2C%20an%20infrastructure%20for%20reinforcement%20learning%20%28RL%29%20of%20large%20language%20model%20%28LLM%29%20agents%20built%20around%20a%20separation%20of%20concerns%20across%20algorithm%20design%2C%20execution%2C%20and%20agent-environment%20interaction.%20Rather%20than%20relying%20on%20monolithic%2C%20end-to-end%20RL%20pipelines%2C%20OpenTinker%20decomposes%20agentic%20learning%20systems%20into%20lightweight%2C%20composable%20components%20with%20clearly%20defined%20abstraction%20boundaries.%20Users%20specify%20agents%2C%20environments%2C%20and%20interaction%20protocols%2C%20while%20inference%20and%20training%20are%20delegated%20to%20a%20managed%20execution%20runtime.%20OpenTinker%20introduces%20a%20centralized%20scheduler%20for%20managing%20training%20and%20inference%20workloads%2C%20including%20LoRA-based%20and%20full-parameter%20RL%2C%20supervised%20fine-tuning%2C%20and%20inference%2C%20over%20shared%20resources.%20We%20further%20discuss%20design%20principles%20for%20extending%20OpenTinker%20to%20multi-agent%20training.%20Finally%2C%20we%20present%20a%20set%20of%20RL%20use%20cases%20that%20demonstrate%20the%20effectiveness%20of%20the%20framework%20in%20practical%20agentic%20learning%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07376v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenTinker%253A%2520Separating%2520Concerns%2520in%2520Agentic%2520Reinforcement%2520Learning%26entry.906535625%3DSiqi%2520Zhu%2520and%2520Jiaxuan%2520You%26entry.1292438233%3DWe%2520introduce%2520OpenTinker%252C%2520an%2520infrastructure%2520for%2520reinforcement%2520learning%2520%2528RL%2529%2520of%2520large%2520language%2520model%2520%2528LLM%2529%2520agents%2520built%2520around%2520a%2520separation%2520of%2520concerns%2520across%2520algorithm%2520design%252C%2520execution%252C%2520and%2520agent-environment%2520interaction.%2520Rather%2520than%2520relying%2520on%2520monolithic%252C%2520end-to-end%2520RL%2520pipelines%252C%2520OpenTinker%2520decomposes%2520agentic%2520learning%2520systems%2520into%2520lightweight%252C%2520composable%2520components%2520with%2520clearly%2520defined%2520abstraction%2520boundaries.%2520Users%2520specify%2520agents%252C%2520environments%252C%2520and%2520interaction%2520protocols%252C%2520while%2520inference%2520and%2520training%2520are%2520delegated%2520to%2520a%2520managed%2520execution%2520runtime.%2520OpenTinker%2520introduces%2520a%2520centralized%2520scheduler%2520for%2520managing%2520training%2520and%2520inference%2520workloads%252C%2520including%2520LoRA-based%2520and%2520full-parameter%2520RL%252C%2520supervised%2520fine-tuning%252C%2520and%2520inference%252C%2520over%2520shared%2520resources.%2520We%2520further%2520discuss%2520design%2520principles%2520for%2520extending%2520OpenTinker%2520to%2520multi-agent%2520training.%2520Finally%252C%2520we%2520present%2520a%2520set%2520of%2520RL%2520use%2520cases%2520that%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520framework%2520in%2520practical%2520agentic%2520learning%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07376v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenTinker%3A%20Separating%20Concerns%20in%20Agentic%20Reinforcement%20Learning&entry.906535625=Siqi%20Zhu%20and%20Jiaxuan%20You&entry.1292438233=We%20introduce%20OpenTinker%2C%20an%20infrastructure%20for%20reinforcement%20learning%20%28RL%29%20of%20large%20language%20model%20%28LLM%29%20agents%20built%20around%20a%20separation%20of%20concerns%20across%20algorithm%20design%2C%20execution%2C%20and%20agent-environment%20interaction.%20Rather%20than%20relying%20on%20monolithic%2C%20end-to-end%20RL%20pipelines%2C%20OpenTinker%20decomposes%20agentic%20learning%20systems%20into%20lightweight%2C%20composable%20components%20with%20clearly%20defined%20abstraction%20boundaries.%20Users%20specify%20agents%2C%20environments%2C%20and%20interaction%20protocols%2C%20while%20inference%20and%20training%20are%20delegated%20to%20a%20managed%20execution%20runtime.%20OpenTinker%20introduces%20a%20centralized%20scheduler%20for%20managing%20training%20and%20inference%20workloads%2C%20including%20LoRA-based%20and%20full-parameter%20RL%2C%20supervised%20fine-tuning%2C%20and%20inference%2C%20over%20shared%20resources.%20We%20further%20discuss%20design%20principles%20for%20extending%20OpenTinker%20to%20multi-agent%20training.%20Finally%2C%20we%20present%20a%20set%20of%20RL%20use%20cases%20that%20demonstrate%20the%20effectiveness%20of%20the%20framework%20in%20practical%20agentic%20learning%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2601.07376v1&entry.124074799=Read"},
{"title": "Can Reasoning Help Large Language Models Capture Human Annotator Disagreement?", "author": "Jingwei Ni and Yu Fan and Vil\u00e9m Zouhar and Donya Rooein and Alexander Hoyle and Mrinmaya Sachan and Markus Leippold and Dirk Hovy and Elliott Ash", "abstract": "Variation in human annotation (i.e., disagreements) is common in NLP, often reflecting important information like task subjectivity and sample ambiguity. Modeling this variation is important for applications that are sensitive to such information. Although RLVR-style reasoning (Reinforcement Learning with Verifiable Rewards) has improved Large Language Model (LLM) performance on many tasks, it remains unclear whether such reasoning enables LLMs to capture informative variation in human annotation. In this work, we evaluate the influence of different reasoning settings on LLM disagreement modeling. We systematically evaluate each reasoning setting across model sizes, distribution expression methods, and steering methods, resulting in 60 experimental setups across 3 tasks. Surprisingly, our results show that RLVR-style reasoning degrades performance in disagreement modeling, while naive Chain-of-Thought (CoT) reasoning improves the performance of RLHF LLMs (RL from human feedback). These findings underscore the potential risk of replacing human annotators with reasoning LLMs, especially when disagreements are important.", "link": "http://arxiv.org/abs/2506.19467v3", "date": "2026-01-12", "relevancy": 2.0084, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5061}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Reasoning%20Help%20Large%20Language%20Models%20Capture%20Human%20Annotator%20Disagreement%3F&body=Title%3A%20Can%20Reasoning%20Help%20Large%20Language%20Models%20Capture%20Human%20Annotator%20Disagreement%3F%0AAuthor%3A%20Jingwei%20Ni%20and%20Yu%20Fan%20and%20Vil%C3%A9m%20Zouhar%20and%20Donya%20Rooein%20and%20Alexander%20Hoyle%20and%20Mrinmaya%20Sachan%20and%20Markus%20Leippold%20and%20Dirk%20Hovy%20and%20Elliott%20Ash%0AAbstract%3A%20Variation%20in%20human%20annotation%20%28i.e.%2C%20disagreements%29%20is%20common%20in%20NLP%2C%20often%20reflecting%20important%20information%20like%20task%20subjectivity%20and%20sample%20ambiguity.%20Modeling%20this%20variation%20is%20important%20for%20applications%20that%20are%20sensitive%20to%20such%20information.%20Although%20RLVR-style%20reasoning%20%28Reinforcement%20Learning%20with%20Verifiable%20Rewards%29%20has%20improved%20Large%20Language%20Model%20%28LLM%29%20performance%20on%20many%20tasks%2C%20it%20remains%20unclear%20whether%20such%20reasoning%20enables%20LLMs%20to%20capture%20informative%20variation%20in%20human%20annotation.%20In%20this%20work%2C%20we%20evaluate%20the%20influence%20of%20different%20reasoning%20settings%20on%20LLM%20disagreement%20modeling.%20We%20systematically%20evaluate%20each%20reasoning%20setting%20across%20model%20sizes%2C%20distribution%20expression%20methods%2C%20and%20steering%20methods%2C%20resulting%20in%2060%20experimental%20setups%20across%203%20tasks.%20Surprisingly%2C%20our%20results%20show%20that%20RLVR-style%20reasoning%20degrades%20performance%20in%20disagreement%20modeling%2C%20while%20naive%20Chain-of-Thought%20%28CoT%29%20reasoning%20improves%20the%20performance%20of%20RLHF%20LLMs%20%28RL%20from%20human%20feedback%29.%20These%20findings%20underscore%20the%20potential%20risk%20of%20replacing%20human%20annotators%20with%20reasoning%20LLMs%2C%20especially%20when%20disagreements%20are%20important.%0ALink%3A%20http%3A//arxiv.org/abs/2506.19467v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Reasoning%2520Help%2520Large%2520Language%2520Models%2520Capture%2520Human%2520Annotator%2520Disagreement%253F%26entry.906535625%3DJingwei%2520Ni%2520and%2520Yu%2520Fan%2520and%2520Vil%25C3%25A9m%2520Zouhar%2520and%2520Donya%2520Rooein%2520and%2520Alexander%2520Hoyle%2520and%2520Mrinmaya%2520Sachan%2520and%2520Markus%2520Leippold%2520and%2520Dirk%2520Hovy%2520and%2520Elliott%2520Ash%26entry.1292438233%3DVariation%2520in%2520human%2520annotation%2520%2528i.e.%252C%2520disagreements%2529%2520is%2520common%2520in%2520NLP%252C%2520often%2520reflecting%2520important%2520information%2520like%2520task%2520subjectivity%2520and%2520sample%2520ambiguity.%2520Modeling%2520this%2520variation%2520is%2520important%2520for%2520applications%2520that%2520are%2520sensitive%2520to%2520such%2520information.%2520Although%2520RLVR-style%2520reasoning%2520%2528Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2529%2520has%2520improved%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520performance%2520on%2520many%2520tasks%252C%2520it%2520remains%2520unclear%2520whether%2520such%2520reasoning%2520enables%2520LLMs%2520to%2520capture%2520informative%2520variation%2520in%2520human%2520annotation.%2520In%2520this%2520work%252C%2520we%2520evaluate%2520the%2520influence%2520of%2520different%2520reasoning%2520settings%2520on%2520LLM%2520disagreement%2520modeling.%2520We%2520systematically%2520evaluate%2520each%2520reasoning%2520setting%2520across%2520model%2520sizes%252C%2520distribution%2520expression%2520methods%252C%2520and%2520steering%2520methods%252C%2520resulting%2520in%252060%2520experimental%2520setups%2520across%25203%2520tasks.%2520Surprisingly%252C%2520our%2520results%2520show%2520that%2520RLVR-style%2520reasoning%2520degrades%2520performance%2520in%2520disagreement%2520modeling%252C%2520while%2520naive%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520improves%2520the%2520performance%2520of%2520RLHF%2520LLMs%2520%2528RL%2520from%2520human%2520feedback%2529.%2520These%2520findings%2520underscore%2520the%2520potential%2520risk%2520of%2520replacing%2520human%2520annotators%2520with%2520reasoning%2520LLMs%252C%2520especially%2520when%2520disagreements%2520are%2520important.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19467v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Reasoning%20Help%20Large%20Language%20Models%20Capture%20Human%20Annotator%20Disagreement%3F&entry.906535625=Jingwei%20Ni%20and%20Yu%20Fan%20and%20Vil%C3%A9m%20Zouhar%20and%20Donya%20Rooein%20and%20Alexander%20Hoyle%20and%20Mrinmaya%20Sachan%20and%20Markus%20Leippold%20and%20Dirk%20Hovy%20and%20Elliott%20Ash&entry.1292438233=Variation%20in%20human%20annotation%20%28i.e.%2C%20disagreements%29%20is%20common%20in%20NLP%2C%20often%20reflecting%20important%20information%20like%20task%20subjectivity%20and%20sample%20ambiguity.%20Modeling%20this%20variation%20is%20important%20for%20applications%20that%20are%20sensitive%20to%20such%20information.%20Although%20RLVR-style%20reasoning%20%28Reinforcement%20Learning%20with%20Verifiable%20Rewards%29%20has%20improved%20Large%20Language%20Model%20%28LLM%29%20performance%20on%20many%20tasks%2C%20it%20remains%20unclear%20whether%20such%20reasoning%20enables%20LLMs%20to%20capture%20informative%20variation%20in%20human%20annotation.%20In%20this%20work%2C%20we%20evaluate%20the%20influence%20of%20different%20reasoning%20settings%20on%20LLM%20disagreement%20modeling.%20We%20systematically%20evaluate%20each%20reasoning%20setting%20across%20model%20sizes%2C%20distribution%20expression%20methods%2C%20and%20steering%20methods%2C%20resulting%20in%2060%20experimental%20setups%20across%203%20tasks.%20Surprisingly%2C%20our%20results%20show%20that%20RLVR-style%20reasoning%20degrades%20performance%20in%20disagreement%20modeling%2C%20while%20naive%20Chain-of-Thought%20%28CoT%29%20reasoning%20improves%20the%20performance%20of%20RLHF%20LLMs%20%28RL%20from%20human%20feedback%29.%20These%20findings%20underscore%20the%20potential%20risk%20of%20replacing%20human%20annotators%20with%20reasoning%20LLMs%2C%20especially%20when%20disagreements%20are%20important.&entry.1838667208=http%3A//arxiv.org/abs/2506.19467v3&entry.124074799=Read"},
{"title": "Contextual Discrepancy-Aware Contrastive Learning for Robust Medical Time Series Diagnosis in Small-Sample Scenarios", "author": "Kaito Tanaka and Aya Nakayama and Masato Ito and Yuji Nishimura and Keisuke Matsuda", "abstract": "Medical time series data, such as EEG and ECG, are vital for diagnosing neurological and cardiovascular diseases. However, their precise interpretation faces significant challenges due to high annotation costs, leading to data scarcity, and the limitations of traditional contrastive learning in capturing complex temporal patterns. To address these issues, we propose CoDAC (Contextual Discrepancy-Aware Contrastive learning), a novel framework that enhances diagnostic accuracy and generalization, particularly in small-sample settings. CoDAC leverages external healthy data and introduces a Contextual Discrepancy Estimator (CDE), built upon a Transformer-based Autoencoder, to precisely quantify abnormal signals through context-aware anomaly scores. These scores dynamically inform a Dynamic Multi-views Contrastive Framework (DMCF), which adaptively weights different temporal views to focus contrastive learning on diagnostically relevant, discrepant regions. Our encoder combines dilated convolutions with multi-head attention for robust feature extraction. Comprehensive experiments on Alzheimer's Disease EEG, Parkinson's Disease EEG, and Myocardial Infarction ECG datasets demonstrate CoDAC's superior performance across all metrics, consistently outperforming state-of-the-art baselines, especially under low label availability. Ablation studies further validate the critical contributions of CDE and DMCF. CoDAC offers a robust and interpretable solution for medical time series diagnosis, effectively mitigating data scarcity challenges.", "link": "http://arxiv.org/abs/2601.07548v1", "date": "2026-01-12", "relevancy": 1.5179, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5177}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5058}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Discrepancy-Aware%20Contrastive%20Learning%20for%20Robust%20Medical%20Time%20Series%20Diagnosis%20in%20Small-Sample%20Scenarios&body=Title%3A%20Contextual%20Discrepancy-Aware%20Contrastive%20Learning%20for%20Robust%20Medical%20Time%20Series%20Diagnosis%20in%20Small-Sample%20Scenarios%0AAuthor%3A%20Kaito%20Tanaka%20and%20Aya%20Nakayama%20and%20Masato%20Ito%20and%20Yuji%20Nishimura%20and%20Keisuke%20Matsuda%0AAbstract%3A%20Medical%20time%20series%20data%2C%20such%20as%20EEG%20and%20ECG%2C%20are%20vital%20for%20diagnosing%20neurological%20and%20cardiovascular%20diseases.%20However%2C%20their%20precise%20interpretation%20faces%20significant%20challenges%20due%20to%20high%20annotation%20costs%2C%20leading%20to%20data%20scarcity%2C%20and%20the%20limitations%20of%20traditional%20contrastive%20learning%20in%20capturing%20complex%20temporal%20patterns.%20To%20address%20these%20issues%2C%20we%20propose%20CoDAC%20%28Contextual%20Discrepancy-Aware%20Contrastive%20learning%29%2C%20a%20novel%20framework%20that%20enhances%20diagnostic%20accuracy%20and%20generalization%2C%20particularly%20in%20small-sample%20settings.%20CoDAC%20leverages%20external%20healthy%20data%20and%20introduces%20a%20Contextual%20Discrepancy%20Estimator%20%28CDE%29%2C%20built%20upon%20a%20Transformer-based%20Autoencoder%2C%20to%20precisely%20quantify%20abnormal%20signals%20through%20context-aware%20anomaly%20scores.%20These%20scores%20dynamically%20inform%20a%20Dynamic%20Multi-views%20Contrastive%20Framework%20%28DMCF%29%2C%20which%20adaptively%20weights%20different%20temporal%20views%20to%20focus%20contrastive%20learning%20on%20diagnostically%20relevant%2C%20discrepant%20regions.%20Our%20encoder%20combines%20dilated%20convolutions%20with%20multi-head%20attention%20for%20robust%20feature%20extraction.%20Comprehensive%20experiments%20on%20Alzheimer%27s%20Disease%20EEG%2C%20Parkinson%27s%20Disease%20EEG%2C%20and%20Myocardial%20Infarction%20ECG%20datasets%20demonstrate%20CoDAC%27s%20superior%20performance%20across%20all%20metrics%2C%20consistently%20outperforming%20state-of-the-art%20baselines%2C%20especially%20under%20low%20label%20availability.%20Ablation%20studies%20further%20validate%20the%20critical%20contributions%20of%20CDE%20and%20DMCF.%20CoDAC%20offers%20a%20robust%20and%20interpretable%20solution%20for%20medical%20time%20series%20diagnosis%2C%20effectively%20mitigating%20data%20scarcity%20challenges.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Discrepancy-Aware%2520Contrastive%2520Learning%2520for%2520Robust%2520Medical%2520Time%2520Series%2520Diagnosis%2520in%2520Small-Sample%2520Scenarios%26entry.906535625%3DKaito%2520Tanaka%2520and%2520Aya%2520Nakayama%2520and%2520Masato%2520Ito%2520and%2520Yuji%2520Nishimura%2520and%2520Keisuke%2520Matsuda%26entry.1292438233%3DMedical%2520time%2520series%2520data%252C%2520such%2520as%2520EEG%2520and%2520ECG%252C%2520are%2520vital%2520for%2520diagnosing%2520neurological%2520and%2520cardiovascular%2520diseases.%2520However%252C%2520their%2520precise%2520interpretation%2520faces%2520significant%2520challenges%2520due%2520to%2520high%2520annotation%2520costs%252C%2520leading%2520to%2520data%2520scarcity%252C%2520and%2520the%2520limitations%2520of%2520traditional%2520contrastive%2520learning%2520in%2520capturing%2520complex%2520temporal%2520patterns.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520CoDAC%2520%2528Contextual%2520Discrepancy-Aware%2520Contrastive%2520learning%2529%252C%2520a%2520novel%2520framework%2520that%2520enhances%2520diagnostic%2520accuracy%2520and%2520generalization%252C%2520particularly%2520in%2520small-sample%2520settings.%2520CoDAC%2520leverages%2520external%2520healthy%2520data%2520and%2520introduces%2520a%2520Contextual%2520Discrepancy%2520Estimator%2520%2528CDE%2529%252C%2520built%2520upon%2520a%2520Transformer-based%2520Autoencoder%252C%2520to%2520precisely%2520quantify%2520abnormal%2520signals%2520through%2520context-aware%2520anomaly%2520scores.%2520These%2520scores%2520dynamically%2520inform%2520a%2520Dynamic%2520Multi-views%2520Contrastive%2520Framework%2520%2528DMCF%2529%252C%2520which%2520adaptively%2520weights%2520different%2520temporal%2520views%2520to%2520focus%2520contrastive%2520learning%2520on%2520diagnostically%2520relevant%252C%2520discrepant%2520regions.%2520Our%2520encoder%2520combines%2520dilated%2520convolutions%2520with%2520multi-head%2520attention%2520for%2520robust%2520feature%2520extraction.%2520Comprehensive%2520experiments%2520on%2520Alzheimer%2527s%2520Disease%2520EEG%252C%2520Parkinson%2527s%2520Disease%2520EEG%252C%2520and%2520Myocardial%2520Infarction%2520ECG%2520datasets%2520demonstrate%2520CoDAC%2527s%2520superior%2520performance%2520across%2520all%2520metrics%252C%2520consistently%2520outperforming%2520state-of-the-art%2520baselines%252C%2520especially%2520under%2520low%2520label%2520availability.%2520Ablation%2520studies%2520further%2520validate%2520the%2520critical%2520contributions%2520of%2520CDE%2520and%2520DMCF.%2520CoDAC%2520offers%2520a%2520robust%2520and%2520interpretable%2520solution%2520for%2520medical%2520time%2520series%2520diagnosis%252C%2520effectively%2520mitigating%2520data%2520scarcity%2520challenges.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Discrepancy-Aware%20Contrastive%20Learning%20for%20Robust%20Medical%20Time%20Series%20Diagnosis%20in%20Small-Sample%20Scenarios&entry.906535625=Kaito%20Tanaka%20and%20Aya%20Nakayama%20and%20Masato%20Ito%20and%20Yuji%20Nishimura%20and%20Keisuke%20Matsuda&entry.1292438233=Medical%20time%20series%20data%2C%20such%20as%20EEG%20and%20ECG%2C%20are%20vital%20for%20diagnosing%20neurological%20and%20cardiovascular%20diseases.%20However%2C%20their%20precise%20interpretation%20faces%20significant%20challenges%20due%20to%20high%20annotation%20costs%2C%20leading%20to%20data%20scarcity%2C%20and%20the%20limitations%20of%20traditional%20contrastive%20learning%20in%20capturing%20complex%20temporal%20patterns.%20To%20address%20these%20issues%2C%20we%20propose%20CoDAC%20%28Contextual%20Discrepancy-Aware%20Contrastive%20learning%29%2C%20a%20novel%20framework%20that%20enhances%20diagnostic%20accuracy%20and%20generalization%2C%20particularly%20in%20small-sample%20settings.%20CoDAC%20leverages%20external%20healthy%20data%20and%20introduces%20a%20Contextual%20Discrepancy%20Estimator%20%28CDE%29%2C%20built%20upon%20a%20Transformer-based%20Autoencoder%2C%20to%20precisely%20quantify%20abnormal%20signals%20through%20context-aware%20anomaly%20scores.%20These%20scores%20dynamically%20inform%20a%20Dynamic%20Multi-views%20Contrastive%20Framework%20%28DMCF%29%2C%20which%20adaptively%20weights%20different%20temporal%20views%20to%20focus%20contrastive%20learning%20on%20diagnostically%20relevant%2C%20discrepant%20regions.%20Our%20encoder%20combines%20dilated%20convolutions%20with%20multi-head%20attention%20for%20robust%20feature%20extraction.%20Comprehensive%20experiments%20on%20Alzheimer%27s%20Disease%20EEG%2C%20Parkinson%27s%20Disease%20EEG%2C%20and%20Myocardial%20Infarction%20ECG%20datasets%20demonstrate%20CoDAC%27s%20superior%20performance%20across%20all%20metrics%2C%20consistently%20outperforming%20state-of-the-art%20baselines%2C%20especially%20under%20low%20label%20availability.%20Ablation%20studies%20further%20validate%20the%20critical%20contributions%20of%20CDE%20and%20DMCF.%20CoDAC%20offers%20a%20robust%20and%20interpretable%20solution%20for%20medical%20time%20series%20diagnosis%2C%20effectively%20mitigating%20data%20scarcity%20challenges.&entry.1838667208=http%3A//arxiv.org/abs/2601.07548v1&entry.124074799=Read"},
{"title": "Rep3Net: An Approach Exploiting Multimodal Representation for Molecular Bioactivity Prediction", "author": "Sabrina Islam and Md. Atiqur Rahman and Md. Bakhtiar Hasan and Md. Hasanul Kabir", "abstract": "Accurate prediction of compound potency accelerates early-stage drug discovery by prioritizing candidates for experimental testing. However, many Quantitative Structure-Activity Relationship (QSAR) approaches for this prediction are constrained by their choice of molecular representation: handcrafted descriptors capture global properties but miss local topology, graph neural networks encode structure but often lack broader chemical context, and SMILES-based language models provide contextual patterns learned from large corpora but are seldom combined with structural features. To exploit these complementary signals, we introduce Rep3Net, a unified multimodal architecture that fuses RDKit molecular descriptors, graph-derived features from a residual graph-convolutional backbone, and ChemBERTa SMILES embeddings. We evaluate Rep3Net on a curated ChEMBL subset for Human PARP1 using fivefold cross validation. Rep3Net attains an MSE of $0.83\\pm0.06$, RMSE of $0.91\\pm0.03$, $R^{2}=0.43\\pm0.01$, and yields Pearson and Spearman correlations of $0.66\\pm0.01$ and $0.67\\pm0.01$, respectively, substantially improving over several strong GNN baselines. In addition, Rep3Net achieves a favorable latency-to-parameter trade-off thanks to a single-layer GCN backbone and parallel frozen encoders. Ablations show that graph topology, ChemBERTa semantics, and handcrafted descriptors each contribute complementary information, with full fusion providing the largest error reduction. These results demonstrate that multimodal representation fusion can improve potency prediction for PARP1 and provide a scalable framework for virtual screening in early-stage drug discovery.", "link": "http://arxiv.org/abs/2512.00521v2", "date": "2026-01-12", "relevancy": 1.9302, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5078}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4843}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rep3Net%3A%20An%20Approach%20Exploiting%20Multimodal%20Representation%20for%20Molecular%20Bioactivity%20Prediction&body=Title%3A%20Rep3Net%3A%20An%20Approach%20Exploiting%20Multimodal%20Representation%20for%20Molecular%20Bioactivity%20Prediction%0AAuthor%3A%20Sabrina%20Islam%20and%20Md.%20Atiqur%20Rahman%20and%20Md.%20Bakhtiar%20Hasan%20and%20Md.%20Hasanul%20Kabir%0AAbstract%3A%20Accurate%20prediction%20of%20compound%20potency%20accelerates%20early-stage%20drug%20discovery%20by%20prioritizing%20candidates%20for%20experimental%20testing.%20However%2C%20many%20Quantitative%20Structure-Activity%20Relationship%20%28QSAR%29%20approaches%20for%20this%20prediction%20are%20constrained%20by%20their%20choice%20of%20molecular%20representation%3A%20handcrafted%20descriptors%20capture%20global%20properties%20but%20miss%20local%20topology%2C%20graph%20neural%20networks%20encode%20structure%20but%20often%20lack%20broader%20chemical%20context%2C%20and%20SMILES-based%20language%20models%20provide%20contextual%20patterns%20learned%20from%20large%20corpora%20but%20are%20seldom%20combined%20with%20structural%20features.%20To%20exploit%20these%20complementary%20signals%2C%20we%20introduce%20Rep3Net%2C%20a%20unified%20multimodal%20architecture%20that%20fuses%20RDKit%20molecular%20descriptors%2C%20graph-derived%20features%20from%20a%20residual%20graph-convolutional%20backbone%2C%20and%20ChemBERTa%20SMILES%20embeddings.%20We%20evaluate%20Rep3Net%20on%20a%20curated%20ChEMBL%20subset%20for%20Human%20PARP1%20using%20fivefold%20cross%20validation.%20Rep3Net%20attains%20an%20MSE%20of%20%240.83%5Cpm0.06%24%2C%20RMSE%20of%20%240.91%5Cpm0.03%24%2C%20%24R%5E%7B2%7D%3D0.43%5Cpm0.01%24%2C%20and%20yields%20Pearson%20and%20Spearman%20correlations%20of%20%240.66%5Cpm0.01%24%20and%20%240.67%5Cpm0.01%24%2C%20respectively%2C%20substantially%20improving%20over%20several%20strong%20GNN%20baselines.%20In%20addition%2C%20Rep3Net%20achieves%20a%20favorable%20latency-to-parameter%20trade-off%20thanks%20to%20a%20single-layer%20GCN%20backbone%20and%20parallel%20frozen%20encoders.%20Ablations%20show%20that%20graph%20topology%2C%20ChemBERTa%20semantics%2C%20and%20handcrafted%20descriptors%20each%20contribute%20complementary%20information%2C%20with%20full%20fusion%20providing%20the%20largest%20error%20reduction.%20These%20results%20demonstrate%20that%20multimodal%20representation%20fusion%20can%20improve%20potency%20prediction%20for%20PARP1%20and%20provide%20a%20scalable%20framework%20for%20virtual%20screening%20in%20early-stage%20drug%20discovery.%0ALink%3A%20http%3A//arxiv.org/abs/2512.00521v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRep3Net%253A%2520An%2520Approach%2520Exploiting%2520Multimodal%2520Representation%2520for%2520Molecular%2520Bioactivity%2520Prediction%26entry.906535625%3DSabrina%2520Islam%2520and%2520Md.%2520Atiqur%2520Rahman%2520and%2520Md.%2520Bakhtiar%2520Hasan%2520and%2520Md.%2520Hasanul%2520Kabir%26entry.1292438233%3DAccurate%2520prediction%2520of%2520compound%2520potency%2520accelerates%2520early-stage%2520drug%2520discovery%2520by%2520prioritizing%2520candidates%2520for%2520experimental%2520testing.%2520However%252C%2520many%2520Quantitative%2520Structure-Activity%2520Relationship%2520%2528QSAR%2529%2520approaches%2520for%2520this%2520prediction%2520are%2520constrained%2520by%2520their%2520choice%2520of%2520molecular%2520representation%253A%2520handcrafted%2520descriptors%2520capture%2520global%2520properties%2520but%2520miss%2520local%2520topology%252C%2520graph%2520neural%2520networks%2520encode%2520structure%2520but%2520often%2520lack%2520broader%2520chemical%2520context%252C%2520and%2520SMILES-based%2520language%2520models%2520provide%2520contextual%2520patterns%2520learned%2520from%2520large%2520corpora%2520but%2520are%2520seldom%2520combined%2520with%2520structural%2520features.%2520To%2520exploit%2520these%2520complementary%2520signals%252C%2520we%2520introduce%2520Rep3Net%252C%2520a%2520unified%2520multimodal%2520architecture%2520that%2520fuses%2520RDKit%2520molecular%2520descriptors%252C%2520graph-derived%2520features%2520from%2520a%2520residual%2520graph-convolutional%2520backbone%252C%2520and%2520ChemBERTa%2520SMILES%2520embeddings.%2520We%2520evaluate%2520Rep3Net%2520on%2520a%2520curated%2520ChEMBL%2520subset%2520for%2520Human%2520PARP1%2520using%2520fivefold%2520cross%2520validation.%2520Rep3Net%2520attains%2520an%2520MSE%2520of%2520%25240.83%255Cpm0.06%2524%252C%2520RMSE%2520of%2520%25240.91%255Cpm0.03%2524%252C%2520%2524R%255E%257B2%257D%253D0.43%255Cpm0.01%2524%252C%2520and%2520yields%2520Pearson%2520and%2520Spearman%2520correlations%2520of%2520%25240.66%255Cpm0.01%2524%2520and%2520%25240.67%255Cpm0.01%2524%252C%2520respectively%252C%2520substantially%2520improving%2520over%2520several%2520strong%2520GNN%2520baselines.%2520In%2520addition%252C%2520Rep3Net%2520achieves%2520a%2520favorable%2520latency-to-parameter%2520trade-off%2520thanks%2520to%2520a%2520single-layer%2520GCN%2520backbone%2520and%2520parallel%2520frozen%2520encoders.%2520Ablations%2520show%2520that%2520graph%2520topology%252C%2520ChemBERTa%2520semantics%252C%2520and%2520handcrafted%2520descriptors%2520each%2520contribute%2520complementary%2520information%252C%2520with%2520full%2520fusion%2520providing%2520the%2520largest%2520error%2520reduction.%2520These%2520results%2520demonstrate%2520that%2520multimodal%2520representation%2520fusion%2520can%2520improve%2520potency%2520prediction%2520for%2520PARP1%2520and%2520provide%2520a%2520scalable%2520framework%2520for%2520virtual%2520screening%2520in%2520early-stage%2520drug%2520discovery.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00521v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rep3Net%3A%20An%20Approach%20Exploiting%20Multimodal%20Representation%20for%20Molecular%20Bioactivity%20Prediction&entry.906535625=Sabrina%20Islam%20and%20Md.%20Atiqur%20Rahman%20and%20Md.%20Bakhtiar%20Hasan%20and%20Md.%20Hasanul%20Kabir&entry.1292438233=Accurate%20prediction%20of%20compound%20potency%20accelerates%20early-stage%20drug%20discovery%20by%20prioritizing%20candidates%20for%20experimental%20testing.%20However%2C%20many%20Quantitative%20Structure-Activity%20Relationship%20%28QSAR%29%20approaches%20for%20this%20prediction%20are%20constrained%20by%20their%20choice%20of%20molecular%20representation%3A%20handcrafted%20descriptors%20capture%20global%20properties%20but%20miss%20local%20topology%2C%20graph%20neural%20networks%20encode%20structure%20but%20often%20lack%20broader%20chemical%20context%2C%20and%20SMILES-based%20language%20models%20provide%20contextual%20patterns%20learned%20from%20large%20corpora%20but%20are%20seldom%20combined%20with%20structural%20features.%20To%20exploit%20these%20complementary%20signals%2C%20we%20introduce%20Rep3Net%2C%20a%20unified%20multimodal%20architecture%20that%20fuses%20RDKit%20molecular%20descriptors%2C%20graph-derived%20features%20from%20a%20residual%20graph-convolutional%20backbone%2C%20and%20ChemBERTa%20SMILES%20embeddings.%20We%20evaluate%20Rep3Net%20on%20a%20curated%20ChEMBL%20subset%20for%20Human%20PARP1%20using%20fivefold%20cross%20validation.%20Rep3Net%20attains%20an%20MSE%20of%20%240.83%5Cpm0.06%24%2C%20RMSE%20of%20%240.91%5Cpm0.03%24%2C%20%24R%5E%7B2%7D%3D0.43%5Cpm0.01%24%2C%20and%20yields%20Pearson%20and%20Spearman%20correlations%20of%20%240.66%5Cpm0.01%24%20and%20%240.67%5Cpm0.01%24%2C%20respectively%2C%20substantially%20improving%20over%20several%20strong%20GNN%20baselines.%20In%20addition%2C%20Rep3Net%20achieves%20a%20favorable%20latency-to-parameter%20trade-off%20thanks%20to%20a%20single-layer%20GCN%20backbone%20and%20parallel%20frozen%20encoders.%20Ablations%20show%20that%20graph%20topology%2C%20ChemBERTa%20semantics%2C%20and%20handcrafted%20descriptors%20each%20contribute%20complementary%20information%2C%20with%20full%20fusion%20providing%20the%20largest%20error%20reduction.%20These%20results%20demonstrate%20that%20multimodal%20representation%20fusion%20can%20improve%20potency%20prediction%20for%20PARP1%20and%20provide%20a%20scalable%20framework%20for%20virtual%20screening%20in%20early-stage%20drug%20discovery.&entry.1838667208=http%3A//arxiv.org/abs/2512.00521v2&entry.124074799=Read"},
{"title": "MCP-ITP: An Automated Framework for Implicit Tool Poisoning in MCP", "author": "Ruiqi Li and Zhiqiang Wang and Yunhao Yao and Xiang-Yang Li", "abstract": "To standardize interactions between LLM-based agents and their environments, the Model Context Protocol (MCP) was proposed and has since been widely adopted. However, integrating external tools expands the attack surface, exposing agents to tool poisoning attacks. In such attacks, malicious instructions embedded in tool metadata are injected into the agent context during MCP registration phase, thereby manipulating agent behavior. Prior work primarily focuses on explicit tool poisoning or relied on manually crafted poisoned tools. In contrast, we focus on a particularly stealthy variant: implicit tool poisoning, where the poisoned tool itself remains uninvoked. Instead, the instructions embedded in the tool metadata induce the agent to invoke a legitimate but high-privilege tool to perform malicious operations. We propose MCP-ITP, the first automated and adaptive framework for implicit tool poisoning within the MCP ecosystem. MCP-ITP formulates poisoned tool generation as a black-box optimization problem and employs an iterative optimization strategy that leverages feedback from both an evaluation LLM and a detection LLM to maximize Attack Success Rate (ASR) while evading current detection mechanisms. Experimental results on the MCPTox dataset across 12 LLM agents demonstrate that MCP-ITP consistently outperforms the manually crafted baseline, achieving up to 84.2% ASR while suppressing the Malicious Tool Detection Rate (MDR) to as low as 0.3%.", "link": "http://arxiv.org/abs/2601.07395v1", "date": "2026-01-12", "relevancy": 1.2117, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.409}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.398}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCP-ITP%3A%20An%20Automated%20Framework%20for%20Implicit%20Tool%20Poisoning%20in%20MCP&body=Title%3A%20MCP-ITP%3A%20An%20Automated%20Framework%20for%20Implicit%20Tool%20Poisoning%20in%20MCP%0AAuthor%3A%20Ruiqi%20Li%20and%20Zhiqiang%20Wang%20and%20Yunhao%20Yao%20and%20Xiang-Yang%20Li%0AAbstract%3A%20To%20standardize%20interactions%20between%20LLM-based%20agents%20and%20their%20environments%2C%20the%20Model%20Context%20Protocol%20%28MCP%29%20was%20proposed%20and%20has%20since%20been%20widely%20adopted.%20However%2C%20integrating%20external%20tools%20expands%20the%20attack%20surface%2C%20exposing%20agents%20to%20tool%20poisoning%20attacks.%20In%20such%20attacks%2C%20malicious%20instructions%20embedded%20in%20tool%20metadata%20are%20injected%20into%20the%20agent%20context%20during%20MCP%20registration%20phase%2C%20thereby%20manipulating%20agent%20behavior.%20Prior%20work%20primarily%20focuses%20on%20explicit%20tool%20poisoning%20or%20relied%20on%20manually%20crafted%20poisoned%20tools.%20In%20contrast%2C%20we%20focus%20on%20a%20particularly%20stealthy%20variant%3A%20implicit%20tool%20poisoning%2C%20where%20the%20poisoned%20tool%20itself%20remains%20uninvoked.%20Instead%2C%20the%20instructions%20embedded%20in%20the%20tool%20metadata%20induce%20the%20agent%20to%20invoke%20a%20legitimate%20but%20high-privilege%20tool%20to%20perform%20malicious%20operations.%20We%20propose%20MCP-ITP%2C%20the%20first%20automated%20and%20adaptive%20framework%20for%20implicit%20tool%20poisoning%20within%20the%20MCP%20ecosystem.%20MCP-ITP%20formulates%20poisoned%20tool%20generation%20as%20a%20black-box%20optimization%20problem%20and%20employs%20an%20iterative%20optimization%20strategy%20that%20leverages%20feedback%20from%20both%20an%20evaluation%20LLM%20and%20a%20detection%20LLM%20to%20maximize%20Attack%20Success%20Rate%20%28ASR%29%20while%20evading%20current%20detection%20mechanisms.%20Experimental%20results%20on%20the%20MCPTox%20dataset%20across%2012%20LLM%20agents%20demonstrate%20that%20MCP-ITP%20consistently%20outperforms%20the%20manually%20crafted%20baseline%2C%20achieving%20up%20to%2084.2%25%20ASR%20while%20suppressing%20the%20Malicious%20Tool%20Detection%20Rate%20%28MDR%29%20to%20as%20low%20as%200.3%25.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07395v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCP-ITP%253A%2520An%2520Automated%2520Framework%2520for%2520Implicit%2520Tool%2520Poisoning%2520in%2520MCP%26entry.906535625%3DRuiqi%2520Li%2520and%2520Zhiqiang%2520Wang%2520and%2520Yunhao%2520Yao%2520and%2520Xiang-Yang%2520Li%26entry.1292438233%3DTo%2520standardize%2520interactions%2520between%2520LLM-based%2520agents%2520and%2520their%2520environments%252C%2520the%2520Model%2520Context%2520Protocol%2520%2528MCP%2529%2520was%2520proposed%2520and%2520has%2520since%2520been%2520widely%2520adopted.%2520However%252C%2520integrating%2520external%2520tools%2520expands%2520the%2520attack%2520surface%252C%2520exposing%2520agents%2520to%2520tool%2520poisoning%2520attacks.%2520In%2520such%2520attacks%252C%2520malicious%2520instructions%2520embedded%2520in%2520tool%2520metadata%2520are%2520injected%2520into%2520the%2520agent%2520context%2520during%2520MCP%2520registration%2520phase%252C%2520thereby%2520manipulating%2520agent%2520behavior.%2520Prior%2520work%2520primarily%2520focuses%2520on%2520explicit%2520tool%2520poisoning%2520or%2520relied%2520on%2520manually%2520crafted%2520poisoned%2520tools.%2520In%2520contrast%252C%2520we%2520focus%2520on%2520a%2520particularly%2520stealthy%2520variant%253A%2520implicit%2520tool%2520poisoning%252C%2520where%2520the%2520poisoned%2520tool%2520itself%2520remains%2520uninvoked.%2520Instead%252C%2520the%2520instructions%2520embedded%2520in%2520the%2520tool%2520metadata%2520induce%2520the%2520agent%2520to%2520invoke%2520a%2520legitimate%2520but%2520high-privilege%2520tool%2520to%2520perform%2520malicious%2520operations.%2520We%2520propose%2520MCP-ITP%252C%2520the%2520first%2520automated%2520and%2520adaptive%2520framework%2520for%2520implicit%2520tool%2520poisoning%2520within%2520the%2520MCP%2520ecosystem.%2520MCP-ITP%2520formulates%2520poisoned%2520tool%2520generation%2520as%2520a%2520black-box%2520optimization%2520problem%2520and%2520employs%2520an%2520iterative%2520optimization%2520strategy%2520that%2520leverages%2520feedback%2520from%2520both%2520an%2520evaluation%2520LLM%2520and%2520a%2520detection%2520LLM%2520to%2520maximize%2520Attack%2520Success%2520Rate%2520%2528ASR%2529%2520while%2520evading%2520current%2520detection%2520mechanisms.%2520Experimental%2520results%2520on%2520the%2520MCPTox%2520dataset%2520across%252012%2520LLM%2520agents%2520demonstrate%2520that%2520MCP-ITP%2520consistently%2520outperforms%2520the%2520manually%2520crafted%2520baseline%252C%2520achieving%2520up%2520to%252084.2%2525%2520ASR%2520while%2520suppressing%2520the%2520Malicious%2520Tool%2520Detection%2520Rate%2520%2528MDR%2529%2520to%2520as%2520low%2520as%25200.3%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07395v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCP-ITP%3A%20An%20Automated%20Framework%20for%20Implicit%20Tool%20Poisoning%20in%20MCP&entry.906535625=Ruiqi%20Li%20and%20Zhiqiang%20Wang%20and%20Yunhao%20Yao%20and%20Xiang-Yang%20Li&entry.1292438233=To%20standardize%20interactions%20between%20LLM-based%20agents%20and%20their%20environments%2C%20the%20Model%20Context%20Protocol%20%28MCP%29%20was%20proposed%20and%20has%20since%20been%20widely%20adopted.%20However%2C%20integrating%20external%20tools%20expands%20the%20attack%20surface%2C%20exposing%20agents%20to%20tool%20poisoning%20attacks.%20In%20such%20attacks%2C%20malicious%20instructions%20embedded%20in%20tool%20metadata%20are%20injected%20into%20the%20agent%20context%20during%20MCP%20registration%20phase%2C%20thereby%20manipulating%20agent%20behavior.%20Prior%20work%20primarily%20focuses%20on%20explicit%20tool%20poisoning%20or%20relied%20on%20manually%20crafted%20poisoned%20tools.%20In%20contrast%2C%20we%20focus%20on%20a%20particularly%20stealthy%20variant%3A%20implicit%20tool%20poisoning%2C%20where%20the%20poisoned%20tool%20itself%20remains%20uninvoked.%20Instead%2C%20the%20instructions%20embedded%20in%20the%20tool%20metadata%20induce%20the%20agent%20to%20invoke%20a%20legitimate%20but%20high-privilege%20tool%20to%20perform%20malicious%20operations.%20We%20propose%20MCP-ITP%2C%20the%20first%20automated%20and%20adaptive%20framework%20for%20implicit%20tool%20poisoning%20within%20the%20MCP%20ecosystem.%20MCP-ITP%20formulates%20poisoned%20tool%20generation%20as%20a%20black-box%20optimization%20problem%20and%20employs%20an%20iterative%20optimization%20strategy%20that%20leverages%20feedback%20from%20both%20an%20evaluation%20LLM%20and%20a%20detection%20LLM%20to%20maximize%20Attack%20Success%20Rate%20%28ASR%29%20while%20evading%20current%20detection%20mechanisms.%20Experimental%20results%20on%20the%20MCPTox%20dataset%20across%2012%20LLM%20agents%20demonstrate%20that%20MCP-ITP%20consistently%20outperforms%20the%20manually%20crafted%20baseline%2C%20achieving%20up%20to%2084.2%25%20ASR%20while%20suppressing%20the%20Malicious%20Tool%20Detection%20Rate%20%28MDR%29%20to%20as%20low%20as%200.3%25.&entry.1838667208=http%3A//arxiv.org/abs/2601.07395v1&entry.124074799=Read"},
{"title": "Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents", "author": "Yunfan Li and Bingbing Xu and Xueyun Tian and Xiucheng Xu and Huawei Shen", "abstract": "Recent advances in large language models (LLMs) have enabled agents to autonomously execute complex, long-horizon tasks, yet planning remains a primary bottleneck for reliable task execution. Existing methods typically fall into two paradigms: step-wise planning, which is reactive but often short-sighted; and one-shot planning, which generates a complete plan upfront yet is brittle to execution errors. Crucially, both paradigms suffer from entangled contexts, where the agent must reason over a monolithic history spanning multiple sub-tasks. This entanglement increases cognitive load and lets local errors propagate across otherwise independent decisions, making recovery computationally expensive. To address this, we propose Task-Decoupled Planning (TDP), a training-free framework that replaces entangled reasoning with task decoupling. TDP decomposes tasks into a directed acyclic graph (DAG) of sub-goals via a Supervisor. Using a Planner and Executor with scoped contexts, TDP confines reasoning and replanning to the active sub-task. This isolation prevents error propagation and corrects deviations locally without disrupting the workflow. Results on TravelPlanner, ScienceWorld, and HotpotQA show that TDP outperforms strong baselines while reducing token consumption by up to 82%, demonstrating that sub-task decoupling improves both robustness and efficiency for long-horizon agents.", "link": "http://arxiv.org/abs/2601.07577v1", "date": "2026-01-12", "relevancy": 2.0075, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5348}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5209}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Entangled%20Planning%3A%20Task-Decoupled%20Planning%20for%20Long-Horizon%20Agents&body=Title%3A%20Beyond%20Entangled%20Planning%3A%20Task-Decoupled%20Planning%20for%20Long-Horizon%20Agents%0AAuthor%3A%20Yunfan%20Li%20and%20Bingbing%20Xu%20and%20Xueyun%20Tian%20and%20Xiucheng%20Xu%20and%20Huawei%20Shen%0AAbstract%3A%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20enabled%20agents%20to%20autonomously%20execute%20complex%2C%20long-horizon%20tasks%2C%20yet%20planning%20remains%20a%20primary%20bottleneck%20for%20reliable%20task%20execution.%20Existing%20methods%20typically%20fall%20into%20two%20paradigms%3A%20step-wise%20planning%2C%20which%20is%20reactive%20but%20often%20short-sighted%3B%20and%20one-shot%20planning%2C%20which%20generates%20a%20complete%20plan%20upfront%20yet%20is%20brittle%20to%20execution%20errors.%20Crucially%2C%20both%20paradigms%20suffer%20from%20entangled%20contexts%2C%20where%20the%20agent%20must%20reason%20over%20a%20monolithic%20history%20spanning%20multiple%20sub-tasks.%20This%20entanglement%20increases%20cognitive%20load%20and%20lets%20local%20errors%20propagate%20across%20otherwise%20independent%20decisions%2C%20making%20recovery%20computationally%20expensive.%20To%20address%20this%2C%20we%20propose%20Task-Decoupled%20Planning%20%28TDP%29%2C%20a%20training-free%20framework%20that%20replaces%20entangled%20reasoning%20with%20task%20decoupling.%20TDP%20decomposes%20tasks%20into%20a%20directed%20acyclic%20graph%20%28DAG%29%20of%20sub-goals%20via%20a%20Supervisor.%20Using%20a%20Planner%20and%20Executor%20with%20scoped%20contexts%2C%20TDP%20confines%20reasoning%20and%20replanning%20to%20the%20active%20sub-task.%20This%20isolation%20prevents%20error%20propagation%20and%20corrects%20deviations%20locally%20without%20disrupting%20the%20workflow.%20Results%20on%20TravelPlanner%2C%20ScienceWorld%2C%20and%20HotpotQA%20show%20that%20TDP%20outperforms%20strong%20baselines%20while%20reducing%20token%20consumption%20by%20up%20to%2082%25%2C%20demonstrating%20that%20sub-task%20decoupling%20improves%20both%20robustness%20and%20efficiency%20for%20long-horizon%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Entangled%2520Planning%253A%2520Task-Decoupled%2520Planning%2520for%2520Long-Horizon%2520Agents%26entry.906535625%3DYunfan%2520Li%2520and%2520Bingbing%2520Xu%2520and%2520Xueyun%2520Tian%2520and%2520Xiucheng%2520Xu%2520and%2520Huawei%2520Shen%26entry.1292438233%3DRecent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520enabled%2520agents%2520to%2520autonomously%2520execute%2520complex%252C%2520long-horizon%2520tasks%252C%2520yet%2520planning%2520remains%2520a%2520primary%2520bottleneck%2520for%2520reliable%2520task%2520execution.%2520Existing%2520methods%2520typically%2520fall%2520into%2520two%2520paradigms%253A%2520step-wise%2520planning%252C%2520which%2520is%2520reactive%2520but%2520often%2520short-sighted%253B%2520and%2520one-shot%2520planning%252C%2520which%2520generates%2520a%2520complete%2520plan%2520upfront%2520yet%2520is%2520brittle%2520to%2520execution%2520errors.%2520Crucially%252C%2520both%2520paradigms%2520suffer%2520from%2520entangled%2520contexts%252C%2520where%2520the%2520agent%2520must%2520reason%2520over%2520a%2520monolithic%2520history%2520spanning%2520multiple%2520sub-tasks.%2520This%2520entanglement%2520increases%2520cognitive%2520load%2520and%2520lets%2520local%2520errors%2520propagate%2520across%2520otherwise%2520independent%2520decisions%252C%2520making%2520recovery%2520computationally%2520expensive.%2520To%2520address%2520this%252C%2520we%2520propose%2520Task-Decoupled%2520Planning%2520%2528TDP%2529%252C%2520a%2520training-free%2520framework%2520that%2520replaces%2520entangled%2520reasoning%2520with%2520task%2520decoupling.%2520TDP%2520decomposes%2520tasks%2520into%2520a%2520directed%2520acyclic%2520graph%2520%2528DAG%2529%2520of%2520sub-goals%2520via%2520a%2520Supervisor.%2520Using%2520a%2520Planner%2520and%2520Executor%2520with%2520scoped%2520contexts%252C%2520TDP%2520confines%2520reasoning%2520and%2520replanning%2520to%2520the%2520active%2520sub-task.%2520This%2520isolation%2520prevents%2520error%2520propagation%2520and%2520corrects%2520deviations%2520locally%2520without%2520disrupting%2520the%2520workflow.%2520Results%2520on%2520TravelPlanner%252C%2520ScienceWorld%252C%2520and%2520HotpotQA%2520show%2520that%2520TDP%2520outperforms%2520strong%2520baselines%2520while%2520reducing%2520token%2520consumption%2520by%2520up%2520to%252082%2525%252C%2520demonstrating%2520that%2520sub-task%2520decoupling%2520improves%2520both%2520robustness%2520and%2520efficiency%2520for%2520long-horizon%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Entangled%20Planning%3A%20Task-Decoupled%20Planning%20for%20Long-Horizon%20Agents&entry.906535625=Yunfan%20Li%20and%20Bingbing%20Xu%20and%20Xueyun%20Tian%20and%20Xiucheng%20Xu%20and%20Huawei%20Shen&entry.1292438233=Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20enabled%20agents%20to%20autonomously%20execute%20complex%2C%20long-horizon%20tasks%2C%20yet%20planning%20remains%20a%20primary%20bottleneck%20for%20reliable%20task%20execution.%20Existing%20methods%20typically%20fall%20into%20two%20paradigms%3A%20step-wise%20planning%2C%20which%20is%20reactive%20but%20often%20short-sighted%3B%20and%20one-shot%20planning%2C%20which%20generates%20a%20complete%20plan%20upfront%20yet%20is%20brittle%20to%20execution%20errors.%20Crucially%2C%20both%20paradigms%20suffer%20from%20entangled%20contexts%2C%20where%20the%20agent%20must%20reason%20over%20a%20monolithic%20history%20spanning%20multiple%20sub-tasks.%20This%20entanglement%20increases%20cognitive%20load%20and%20lets%20local%20errors%20propagate%20across%20otherwise%20independent%20decisions%2C%20making%20recovery%20computationally%20expensive.%20To%20address%20this%2C%20we%20propose%20Task-Decoupled%20Planning%20%28TDP%29%2C%20a%20training-free%20framework%20that%20replaces%20entangled%20reasoning%20with%20task%20decoupling.%20TDP%20decomposes%20tasks%20into%20a%20directed%20acyclic%20graph%20%28DAG%29%20of%20sub-goals%20via%20a%20Supervisor.%20Using%20a%20Planner%20and%20Executor%20with%20scoped%20contexts%2C%20TDP%20confines%20reasoning%20and%20replanning%20to%20the%20active%20sub-task.%20This%20isolation%20prevents%20error%20propagation%20and%20corrects%20deviations%20locally%20without%20disrupting%20the%20workflow.%20Results%20on%20TravelPlanner%2C%20ScienceWorld%2C%20and%20HotpotQA%20show%20that%20TDP%20outperforms%20strong%20baselines%20while%20reducing%20token%20consumption%20by%20up%20to%2082%25%2C%20demonstrating%20that%20sub-task%20decoupling%20improves%20both%20robustness%20and%20efficiency%20for%20long-horizon%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2601.07577v1&entry.124074799=Read"},
{"title": "NeoAMT: Neologism-Aware Agentic Machine Translation with Reinforcement Learning", "author": "Zhongtao Miao and Kaiyan Zhao and Masaaki Nagata and Yoshimasa Tsuruoka", "abstract": "Neologism-aware machine translation aims to translate source sentences containing neologisms into target languages. This field remains underexplored compared with general machine translation (MT). In this paper, we propose an agentic framework, NeoAMT, for neologism-aware machine translation using a Wiktionary search tool. Specifically, we first create a new dataset for neologism-aware machine translation and develop a search tool based on Wiktionary. The new dataset covers 16 languages and 75 translation directions and is derived from approximately 10 million records of an English Wiktionary dump. The retrieval corpus of the search tool is also constructed from around 3 million cleaned records of the Wiktionary dump. We then use it for training the translation agent with reinforcement learning (RL) and evaluating the accuracy of neologism-aware machine translation. Based on this, we also propose an RL training framework that contains a novel reward design and an adaptive rollout generation approach by leveraging \"translation difficulty\" to further improve the translation quality of translation agents using our search tool.", "link": "http://arxiv.org/abs/2601.03790v2", "date": "2026-01-12", "relevancy": 1.3546, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4567}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4465}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeoAMT%3A%20Neologism-Aware%20Agentic%20Machine%20Translation%20with%20Reinforcement%20Learning&body=Title%3A%20NeoAMT%3A%20Neologism-Aware%20Agentic%20Machine%20Translation%20with%20Reinforcement%20Learning%0AAuthor%3A%20Zhongtao%20Miao%20and%20Kaiyan%20Zhao%20and%20Masaaki%20Nagata%20and%20Yoshimasa%20Tsuruoka%0AAbstract%3A%20Neologism-aware%20machine%20translation%20aims%20to%20translate%20source%20sentences%20containing%20neologisms%20into%20target%20languages.%20This%20field%20remains%20underexplored%20compared%20with%20general%20machine%20translation%20%28MT%29.%20In%20this%20paper%2C%20we%20propose%20an%20agentic%20framework%2C%20NeoAMT%2C%20for%20neologism-aware%20machine%20translation%20using%20a%20Wiktionary%20search%20tool.%20Specifically%2C%20we%20first%20create%20a%20new%20dataset%20for%20neologism-aware%20machine%20translation%20and%20develop%20a%20search%20tool%20based%20on%20Wiktionary.%20The%20new%20dataset%20covers%2016%20languages%20and%2075%20translation%20directions%20and%20is%20derived%20from%20approximately%2010%20million%20records%20of%20an%20English%20Wiktionary%20dump.%20The%20retrieval%20corpus%20of%20the%20search%20tool%20is%20also%20constructed%20from%20around%203%20million%20cleaned%20records%20of%20the%20Wiktionary%20dump.%20We%20then%20use%20it%20for%20training%20the%20translation%20agent%20with%20reinforcement%20learning%20%28RL%29%20and%20evaluating%20the%20accuracy%20of%20neologism-aware%20machine%20translation.%20Based%20on%20this%2C%20we%20also%20propose%20an%20RL%20training%20framework%20that%20contains%20a%20novel%20reward%20design%20and%20an%20adaptive%20rollout%20generation%20approach%20by%20leveraging%20%22translation%20difficulty%22%20to%20further%20improve%20the%20translation%20quality%20of%20translation%20agents%20using%20our%20search%20tool.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03790v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeoAMT%253A%2520Neologism-Aware%2520Agentic%2520Machine%2520Translation%2520with%2520Reinforcement%2520Learning%26entry.906535625%3DZhongtao%2520Miao%2520and%2520Kaiyan%2520Zhao%2520and%2520Masaaki%2520Nagata%2520and%2520Yoshimasa%2520Tsuruoka%26entry.1292438233%3DNeologism-aware%2520machine%2520translation%2520aims%2520to%2520translate%2520source%2520sentences%2520containing%2520neologisms%2520into%2520target%2520languages.%2520This%2520field%2520remains%2520underexplored%2520compared%2520with%2520general%2520machine%2520translation%2520%2528MT%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520agentic%2520framework%252C%2520NeoAMT%252C%2520for%2520neologism-aware%2520machine%2520translation%2520using%2520a%2520Wiktionary%2520search%2520tool.%2520Specifically%252C%2520we%2520first%2520create%2520a%2520new%2520dataset%2520for%2520neologism-aware%2520machine%2520translation%2520and%2520develop%2520a%2520search%2520tool%2520based%2520on%2520Wiktionary.%2520The%2520new%2520dataset%2520covers%252016%2520languages%2520and%252075%2520translation%2520directions%2520and%2520is%2520derived%2520from%2520approximately%252010%2520million%2520records%2520of%2520an%2520English%2520Wiktionary%2520dump.%2520The%2520retrieval%2520corpus%2520of%2520the%2520search%2520tool%2520is%2520also%2520constructed%2520from%2520around%25203%2520million%2520cleaned%2520records%2520of%2520the%2520Wiktionary%2520dump.%2520We%2520then%2520use%2520it%2520for%2520training%2520the%2520translation%2520agent%2520with%2520reinforcement%2520learning%2520%2528RL%2529%2520and%2520evaluating%2520the%2520accuracy%2520of%2520neologism-aware%2520machine%2520translation.%2520Based%2520on%2520this%252C%2520we%2520also%2520propose%2520an%2520RL%2520training%2520framework%2520that%2520contains%2520a%2520novel%2520reward%2520design%2520and%2520an%2520adaptive%2520rollout%2520generation%2520approach%2520by%2520leveraging%2520%2522translation%2520difficulty%2522%2520to%2520further%2520improve%2520the%2520translation%2520quality%2520of%2520translation%2520agents%2520using%2520our%2520search%2520tool.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03790v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeoAMT%3A%20Neologism-Aware%20Agentic%20Machine%20Translation%20with%20Reinforcement%20Learning&entry.906535625=Zhongtao%20Miao%20and%20Kaiyan%20Zhao%20and%20Masaaki%20Nagata%20and%20Yoshimasa%20Tsuruoka&entry.1292438233=Neologism-aware%20machine%20translation%20aims%20to%20translate%20source%20sentences%20containing%20neologisms%20into%20target%20languages.%20This%20field%20remains%20underexplored%20compared%20with%20general%20machine%20translation%20%28MT%29.%20In%20this%20paper%2C%20we%20propose%20an%20agentic%20framework%2C%20NeoAMT%2C%20for%20neologism-aware%20machine%20translation%20using%20a%20Wiktionary%20search%20tool.%20Specifically%2C%20we%20first%20create%20a%20new%20dataset%20for%20neologism-aware%20machine%20translation%20and%20develop%20a%20search%20tool%20based%20on%20Wiktionary.%20The%20new%20dataset%20covers%2016%20languages%20and%2075%20translation%20directions%20and%20is%20derived%20from%20approximately%2010%20million%20records%20of%20an%20English%20Wiktionary%20dump.%20The%20retrieval%20corpus%20of%20the%20search%20tool%20is%20also%20constructed%20from%20around%203%20million%20cleaned%20records%20of%20the%20Wiktionary%20dump.%20We%20then%20use%20it%20for%20training%20the%20translation%20agent%20with%20reinforcement%20learning%20%28RL%29%20and%20evaluating%20the%20accuracy%20of%20neologism-aware%20machine%20translation.%20Based%20on%20this%2C%20we%20also%20propose%20an%20RL%20training%20framework%20that%20contains%20a%20novel%20reward%20design%20and%20an%20adaptive%20rollout%20generation%20approach%20by%20leveraging%20%22translation%20difficulty%22%20to%20further%20improve%20the%20translation%20quality%20of%20translation%20agents%20using%20our%20search%20tool.&entry.1838667208=http%3A//arxiv.org/abs/2601.03790v2&entry.124074799=Read"},
{"title": "An adjoint method for training data-driven reduced-order models", "author": "Donglin Liu and Francisco Garc\u00eda Atienza and Mengwu Guo", "abstract": "Reduced-order modeling lies at the interface of numerical analysis and data-driven scientific computing, providing principled ways to compress high-fidelity simulations in science and engineering. We propose a training framework that couples a continuous-time form of operator inference with the adjoint-state method to obtain robust data-driven reduced-order models. This method minimizes a trajectory-based loss between reduced-order solutions and projected snapshot data, which removes the need to estimate time derivatives from noisy measurements and provides intrinsic temporal regularization through time integration. We derive the corresponding continuous adjoint equations to compute gradients efficiently and implement a gradient based optimizer to update the reduced model parameters. Each iteration only requires one forward reduced order solve and one adjoint solve, followed by inexpensive gradient assembly, making the method attractive for large-scale simulations. We validate the proposed method on three partial differential equations: viscous Burgers' equation, the two-dimensional Fisher-KPP equation, and an advection-diffusion equation. We perform systematic comparisons against standard operator inference under two perturbation regimes, namely reduced temporal snapshot density and additive Gaussian noise. For clean data, both approaches deliver similar accuracy, but in situations with sparse sampling and noise, the proposed adjoint-based training provides better accuracy and enhanced roll-out stability.", "link": "http://arxiv.org/abs/2601.07579v1", "date": "2026-01-12", "relevancy": 1.4445, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4839}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4819}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20adjoint%20method%20for%20training%20data-driven%20reduced-order%20models&body=Title%3A%20An%20adjoint%20method%20for%20training%20data-driven%20reduced-order%20models%0AAuthor%3A%20Donglin%20Liu%20and%20Francisco%20Garc%C3%ADa%20Atienza%20and%20Mengwu%20Guo%0AAbstract%3A%20Reduced-order%20modeling%20lies%20at%20the%20interface%20of%20numerical%20analysis%20and%20data-driven%20scientific%20computing%2C%20providing%20principled%20ways%20to%20compress%20high-fidelity%20simulations%20in%20science%20and%20engineering.%20We%20propose%20a%20training%20framework%20that%20couples%20a%20continuous-time%20form%20of%20operator%20inference%20with%20the%20adjoint-state%20method%20to%20obtain%20robust%20data-driven%20reduced-order%20models.%20This%20method%20minimizes%20a%20trajectory-based%20loss%20between%20reduced-order%20solutions%20and%20projected%20snapshot%20data%2C%20which%20removes%20the%20need%20to%20estimate%20time%20derivatives%20from%20noisy%20measurements%20and%20provides%20intrinsic%20temporal%20regularization%20through%20time%20integration.%20We%20derive%20the%20corresponding%20continuous%20adjoint%20equations%20to%20compute%20gradients%20efficiently%20and%20implement%20a%20gradient%20based%20optimizer%20to%20update%20the%20reduced%20model%20parameters.%20Each%20iteration%20only%20requires%20one%20forward%20reduced%20order%20solve%20and%20one%20adjoint%20solve%2C%20followed%20by%20inexpensive%20gradient%20assembly%2C%20making%20the%20method%20attractive%20for%20large-scale%20simulations.%20We%20validate%20the%20proposed%20method%20on%20three%20partial%20differential%20equations%3A%20viscous%20Burgers%27%20equation%2C%20the%20two-dimensional%20Fisher-KPP%20equation%2C%20and%20an%20advection-diffusion%20equation.%20We%20perform%20systematic%20comparisons%20against%20standard%20operator%20inference%20under%20two%20perturbation%20regimes%2C%20namely%20reduced%20temporal%20snapshot%20density%20and%20additive%20Gaussian%20noise.%20For%20clean%20data%2C%20both%20approaches%20deliver%20similar%20accuracy%2C%20but%20in%20situations%20with%20sparse%20sampling%20and%20noise%2C%20the%20proposed%20adjoint-based%20training%20provides%20better%20accuracy%20and%20enhanced%20roll-out%20stability.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520adjoint%2520method%2520for%2520training%2520data-driven%2520reduced-order%2520models%26entry.906535625%3DDonglin%2520Liu%2520and%2520Francisco%2520Garc%25C3%25ADa%2520Atienza%2520and%2520Mengwu%2520Guo%26entry.1292438233%3DReduced-order%2520modeling%2520lies%2520at%2520the%2520interface%2520of%2520numerical%2520analysis%2520and%2520data-driven%2520scientific%2520computing%252C%2520providing%2520principled%2520ways%2520to%2520compress%2520high-fidelity%2520simulations%2520in%2520science%2520and%2520engineering.%2520We%2520propose%2520a%2520training%2520framework%2520that%2520couples%2520a%2520continuous-time%2520form%2520of%2520operator%2520inference%2520with%2520the%2520adjoint-state%2520method%2520to%2520obtain%2520robust%2520data-driven%2520reduced-order%2520models.%2520This%2520method%2520minimizes%2520a%2520trajectory-based%2520loss%2520between%2520reduced-order%2520solutions%2520and%2520projected%2520snapshot%2520data%252C%2520which%2520removes%2520the%2520need%2520to%2520estimate%2520time%2520derivatives%2520from%2520noisy%2520measurements%2520and%2520provides%2520intrinsic%2520temporal%2520regularization%2520through%2520time%2520integration.%2520We%2520derive%2520the%2520corresponding%2520continuous%2520adjoint%2520equations%2520to%2520compute%2520gradients%2520efficiently%2520and%2520implement%2520a%2520gradient%2520based%2520optimizer%2520to%2520update%2520the%2520reduced%2520model%2520parameters.%2520Each%2520iteration%2520only%2520requires%2520one%2520forward%2520reduced%2520order%2520solve%2520and%2520one%2520adjoint%2520solve%252C%2520followed%2520by%2520inexpensive%2520gradient%2520assembly%252C%2520making%2520the%2520method%2520attractive%2520for%2520large-scale%2520simulations.%2520We%2520validate%2520the%2520proposed%2520method%2520on%2520three%2520partial%2520differential%2520equations%253A%2520viscous%2520Burgers%2527%2520equation%252C%2520the%2520two-dimensional%2520Fisher-KPP%2520equation%252C%2520and%2520an%2520advection-diffusion%2520equation.%2520We%2520perform%2520systematic%2520comparisons%2520against%2520standard%2520operator%2520inference%2520under%2520two%2520perturbation%2520regimes%252C%2520namely%2520reduced%2520temporal%2520snapshot%2520density%2520and%2520additive%2520Gaussian%2520noise.%2520For%2520clean%2520data%252C%2520both%2520approaches%2520deliver%2520similar%2520accuracy%252C%2520but%2520in%2520situations%2520with%2520sparse%2520sampling%2520and%2520noise%252C%2520the%2520proposed%2520adjoint-based%2520training%2520provides%2520better%2520accuracy%2520and%2520enhanced%2520roll-out%2520stability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20adjoint%20method%20for%20training%20data-driven%20reduced-order%20models&entry.906535625=Donglin%20Liu%20and%20Francisco%20Garc%C3%ADa%20Atienza%20and%20Mengwu%20Guo&entry.1292438233=Reduced-order%20modeling%20lies%20at%20the%20interface%20of%20numerical%20analysis%20and%20data-driven%20scientific%20computing%2C%20providing%20principled%20ways%20to%20compress%20high-fidelity%20simulations%20in%20science%20and%20engineering.%20We%20propose%20a%20training%20framework%20that%20couples%20a%20continuous-time%20form%20of%20operator%20inference%20with%20the%20adjoint-state%20method%20to%20obtain%20robust%20data-driven%20reduced-order%20models.%20This%20method%20minimizes%20a%20trajectory-based%20loss%20between%20reduced-order%20solutions%20and%20projected%20snapshot%20data%2C%20which%20removes%20the%20need%20to%20estimate%20time%20derivatives%20from%20noisy%20measurements%20and%20provides%20intrinsic%20temporal%20regularization%20through%20time%20integration.%20We%20derive%20the%20corresponding%20continuous%20adjoint%20equations%20to%20compute%20gradients%20efficiently%20and%20implement%20a%20gradient%20based%20optimizer%20to%20update%20the%20reduced%20model%20parameters.%20Each%20iteration%20only%20requires%20one%20forward%20reduced%20order%20solve%20and%20one%20adjoint%20solve%2C%20followed%20by%20inexpensive%20gradient%20assembly%2C%20making%20the%20method%20attractive%20for%20large-scale%20simulations.%20We%20validate%20the%20proposed%20method%20on%20three%20partial%20differential%20equations%3A%20viscous%20Burgers%27%20equation%2C%20the%20two-dimensional%20Fisher-KPP%20equation%2C%20and%20an%20advection-diffusion%20equation.%20We%20perform%20systematic%20comparisons%20against%20standard%20operator%20inference%20under%20two%20perturbation%20regimes%2C%20namely%20reduced%20temporal%20snapshot%20density%20and%20additive%20Gaussian%20noise.%20For%20clean%20data%2C%20both%20approaches%20deliver%20similar%20accuracy%2C%20but%20in%20situations%20with%20sparse%20sampling%20and%20noise%2C%20the%20proposed%20adjoint-based%20training%20provides%20better%20accuracy%20and%20enhanced%20roll-out%20stability.&entry.1838667208=http%3A//arxiv.org/abs/2601.07579v1&entry.124074799=Read"},
{"title": "Symbolic regression for defect interactions in 2D materials", "author": "Mikhail Lazarev and Andrey Ustyuzhanin", "abstract": "Machine learning models have become firmly established across all scientific fields. Extracting features from data and making inferences based on them with neural network models often yields high accuracy; however, this approach has several drawbacks. Symbolic regression is a powerful technique for discovering analytical equations that describe data, providing interpretable and generalizable models capable of predicting unseen data. Symbolic regression methods have gained new momentum with the advancement of neural network technologies and offer several advantages, the main one being the interpretability of results. In this work, we examined the application of the deep symbolic regression algorithm SEGVAE to determine the properties of two-dimensional materials with defects. Comparing the results with state-of-the-art graph neural network-based methods shows comparable or, in some cases, even identical outcomes. We also discuss the applicability of this class of methods in natural sciences.", "link": "http://arxiv.org/abs/2512.20785v2", "date": "2026-01-12", "relevancy": 1.4223, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5184}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4631}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symbolic%20regression%20for%20defect%20interactions%20in%202D%20materials&body=Title%3A%20Symbolic%20regression%20for%20defect%20interactions%20in%202D%20materials%0AAuthor%3A%20Mikhail%20Lazarev%20and%20Andrey%20Ustyuzhanin%0AAbstract%3A%20Machine%20learning%20models%20have%20become%20firmly%20established%20across%20all%20scientific%20fields.%20Extracting%20features%20from%20data%20and%20making%20inferences%20based%20on%20them%20with%20neural%20network%20models%20often%20yields%20high%20accuracy%3B%20however%2C%20this%20approach%20has%20several%20drawbacks.%20Symbolic%20regression%20is%20a%20powerful%20technique%20for%20discovering%20analytical%20equations%20that%20describe%20data%2C%20providing%20interpretable%20and%20generalizable%20models%20capable%20of%20predicting%20unseen%20data.%20Symbolic%20regression%20methods%20have%20gained%20new%20momentum%20with%20the%20advancement%20of%20neural%20network%20technologies%20and%20offer%20several%20advantages%2C%20the%20main%20one%20being%20the%20interpretability%20of%20results.%20In%20this%20work%2C%20we%20examined%20the%20application%20of%20the%20deep%20symbolic%20regression%20algorithm%20SEGVAE%20to%20determine%20the%20properties%20of%20two-dimensional%20materials%20with%20defects.%20Comparing%20the%20results%20with%20state-of-the-art%20graph%20neural%20network-based%20methods%20shows%20comparable%20or%2C%20in%20some%20cases%2C%20even%20identical%20outcomes.%20We%20also%20discuss%20the%20applicability%20of%20this%20class%20of%20methods%20in%20natural%20sciences.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20785v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymbolic%2520regression%2520for%2520defect%2520interactions%2520in%25202D%2520materials%26entry.906535625%3DMikhail%2520Lazarev%2520and%2520Andrey%2520Ustyuzhanin%26entry.1292438233%3DMachine%2520learning%2520models%2520have%2520become%2520firmly%2520established%2520across%2520all%2520scientific%2520fields.%2520Extracting%2520features%2520from%2520data%2520and%2520making%2520inferences%2520based%2520on%2520them%2520with%2520neural%2520network%2520models%2520often%2520yields%2520high%2520accuracy%253B%2520however%252C%2520this%2520approach%2520has%2520several%2520drawbacks.%2520Symbolic%2520regression%2520is%2520a%2520powerful%2520technique%2520for%2520discovering%2520analytical%2520equations%2520that%2520describe%2520data%252C%2520providing%2520interpretable%2520and%2520generalizable%2520models%2520capable%2520of%2520predicting%2520unseen%2520data.%2520Symbolic%2520regression%2520methods%2520have%2520gained%2520new%2520momentum%2520with%2520the%2520advancement%2520of%2520neural%2520network%2520technologies%2520and%2520offer%2520several%2520advantages%252C%2520the%2520main%2520one%2520being%2520the%2520interpretability%2520of%2520results.%2520In%2520this%2520work%252C%2520we%2520examined%2520the%2520application%2520of%2520the%2520deep%2520symbolic%2520regression%2520algorithm%2520SEGVAE%2520to%2520determine%2520the%2520properties%2520of%2520two-dimensional%2520materials%2520with%2520defects.%2520Comparing%2520the%2520results%2520with%2520state-of-the-art%2520graph%2520neural%2520network-based%2520methods%2520shows%2520comparable%2520or%252C%2520in%2520some%2520cases%252C%2520even%2520identical%2520outcomes.%2520We%2520also%2520discuss%2520the%2520applicability%2520of%2520this%2520class%2520of%2520methods%2520in%2520natural%2520sciences.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20785v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symbolic%20regression%20for%20defect%20interactions%20in%202D%20materials&entry.906535625=Mikhail%20Lazarev%20and%20Andrey%20Ustyuzhanin&entry.1292438233=Machine%20learning%20models%20have%20become%20firmly%20established%20across%20all%20scientific%20fields.%20Extracting%20features%20from%20data%20and%20making%20inferences%20based%20on%20them%20with%20neural%20network%20models%20often%20yields%20high%20accuracy%3B%20however%2C%20this%20approach%20has%20several%20drawbacks.%20Symbolic%20regression%20is%20a%20powerful%20technique%20for%20discovering%20analytical%20equations%20that%20describe%20data%2C%20providing%20interpretable%20and%20generalizable%20models%20capable%20of%20predicting%20unseen%20data.%20Symbolic%20regression%20methods%20have%20gained%20new%20momentum%20with%20the%20advancement%20of%20neural%20network%20technologies%20and%20offer%20several%20advantages%2C%20the%20main%20one%20being%20the%20interpretability%20of%20results.%20In%20this%20work%2C%20we%20examined%20the%20application%20of%20the%20deep%20symbolic%20regression%20algorithm%20SEGVAE%20to%20determine%20the%20properties%20of%20two-dimensional%20materials%20with%20defects.%20Comparing%20the%20results%20with%20state-of-the-art%20graph%20neural%20network-based%20methods%20shows%20comparable%20or%2C%20in%20some%20cases%2C%20even%20identical%20outcomes.%20We%20also%20discuss%20the%20applicability%20of%20this%20class%20of%20methods%20in%20natural%20sciences.&entry.1838667208=http%3A//arxiv.org/abs/2512.20785v2&entry.124074799=Read"},
{"title": "Membership Inference Attacks on Tokenizers of Large Language Models", "author": "Meng Tong and Yuntao Du and Kejiang Chen and Weiming Zhang", "abstract": "Membership inference attacks (MIAs) are widely used to assess the privacy risks associated with machine learning models. However, when these attacks are applied to pre-trained large language models (LLMs), they encounter significant challenges, including mislabeled samples, distribution shifts, and discrepancies in model size between experimental and real-world settings. To address these limitations, we introduce tokenizers as a new attack vector for membership inference. Specifically, a tokenizer converts raw text into tokens for LLMs. Unlike full models, tokenizers can be efficiently trained from scratch, thereby avoiding the aforementioned challenges. In addition, the tokenizer's training data is typically representative of the data used to pre-train LLMs. Despite these advantages, the potential of tokenizers as an attack vector remains unexplored. To this end, we present the first study on membership leakage through tokenizers and explore five attack methods to infer dataset membership. Extensive experiments on millions of Internet samples reveal the vulnerabilities in the tokenizers of state-of-the-art LLMs. To mitigate this emerging risk, we further propose an adaptive defense. Our findings highlight tokenizers as an overlooked yet critical privacy threat, underscoring the urgent need for privacy-preserving mechanisms specifically designed for them.", "link": "http://arxiv.org/abs/2510.05699v2", "date": "2026-01-12", "relevancy": 1.6827, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4247}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4215}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Membership%20Inference%20Attacks%20on%20Tokenizers%20of%20Large%20Language%20Models&body=Title%3A%20Membership%20Inference%20Attacks%20on%20Tokenizers%20of%20Large%20Language%20Models%0AAuthor%3A%20Meng%20Tong%20and%20Yuntao%20Du%20and%20Kejiang%20Chen%20and%20Weiming%20Zhang%0AAbstract%3A%20Membership%20inference%20attacks%20%28MIAs%29%20are%20widely%20used%20to%20assess%20the%20privacy%20risks%20associated%20with%20machine%20learning%20models.%20However%2C%20when%20these%20attacks%20are%20applied%20to%20pre-trained%20large%20language%20models%20%28LLMs%29%2C%20they%20encounter%20significant%20challenges%2C%20including%20mislabeled%20samples%2C%20distribution%20shifts%2C%20and%20discrepancies%20in%20model%20size%20between%20experimental%20and%20real-world%20settings.%20To%20address%20these%20limitations%2C%20we%20introduce%20tokenizers%20as%20a%20new%20attack%20vector%20for%20membership%20inference.%20Specifically%2C%20a%20tokenizer%20converts%20raw%20text%20into%20tokens%20for%20LLMs.%20Unlike%20full%20models%2C%20tokenizers%20can%20be%20efficiently%20trained%20from%20scratch%2C%20thereby%20avoiding%20the%20aforementioned%20challenges.%20In%20addition%2C%20the%20tokenizer%27s%20training%20data%20is%20typically%20representative%20of%20the%20data%20used%20to%20pre-train%20LLMs.%20Despite%20these%20advantages%2C%20the%20potential%20of%20tokenizers%20as%20an%20attack%20vector%20remains%20unexplored.%20To%20this%20end%2C%20we%20present%20the%20first%20study%20on%20membership%20leakage%20through%20tokenizers%20and%20explore%20five%20attack%20methods%20to%20infer%20dataset%20membership.%20Extensive%20experiments%20on%20millions%20of%20Internet%20samples%20reveal%20the%20vulnerabilities%20in%20the%20tokenizers%20of%20state-of-the-art%20LLMs.%20To%20mitigate%20this%20emerging%20risk%2C%20we%20further%20propose%20an%20adaptive%20defense.%20Our%20findings%20highlight%20tokenizers%20as%20an%20overlooked%20yet%20critical%20privacy%20threat%2C%20underscoring%20the%20urgent%20need%20for%20privacy-preserving%20mechanisms%20specifically%20designed%20for%20them.%0ALink%3A%20http%3A//arxiv.org/abs/2510.05699v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMembership%2520Inference%2520Attacks%2520on%2520Tokenizers%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DMeng%2520Tong%2520and%2520Yuntao%2520Du%2520and%2520Kejiang%2520Chen%2520and%2520Weiming%2520Zhang%26entry.1292438233%3DMembership%2520inference%2520attacks%2520%2528MIAs%2529%2520are%2520widely%2520used%2520to%2520assess%2520the%2520privacy%2520risks%2520associated%2520with%2520machine%2520learning%2520models.%2520However%252C%2520when%2520these%2520attacks%2520are%2520applied%2520to%2520pre-trained%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520they%2520encounter%2520significant%2520challenges%252C%2520including%2520mislabeled%2520samples%252C%2520distribution%2520shifts%252C%2520and%2520discrepancies%2520in%2520model%2520size%2520between%2520experimental%2520and%2520real-world%2520settings.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520tokenizers%2520as%2520a%2520new%2520attack%2520vector%2520for%2520membership%2520inference.%2520Specifically%252C%2520a%2520tokenizer%2520converts%2520raw%2520text%2520into%2520tokens%2520for%2520LLMs.%2520Unlike%2520full%2520models%252C%2520tokenizers%2520can%2520be%2520efficiently%2520trained%2520from%2520scratch%252C%2520thereby%2520avoiding%2520the%2520aforementioned%2520challenges.%2520In%2520addition%252C%2520the%2520tokenizer%2527s%2520training%2520data%2520is%2520typically%2520representative%2520of%2520the%2520data%2520used%2520to%2520pre-train%2520LLMs.%2520Despite%2520these%2520advantages%252C%2520the%2520potential%2520of%2520tokenizers%2520as%2520an%2520attack%2520vector%2520remains%2520unexplored.%2520To%2520this%2520end%252C%2520we%2520present%2520the%2520first%2520study%2520on%2520membership%2520leakage%2520through%2520tokenizers%2520and%2520explore%2520five%2520attack%2520methods%2520to%2520infer%2520dataset%2520membership.%2520Extensive%2520experiments%2520on%2520millions%2520of%2520Internet%2520samples%2520reveal%2520the%2520vulnerabilities%2520in%2520the%2520tokenizers%2520of%2520state-of-the-art%2520LLMs.%2520To%2520mitigate%2520this%2520emerging%2520risk%252C%2520we%2520further%2520propose%2520an%2520adaptive%2520defense.%2520Our%2520findings%2520highlight%2520tokenizers%2520as%2520an%2520overlooked%2520yet%2520critical%2520privacy%2520threat%252C%2520underscoring%2520the%2520urgent%2520need%2520for%2520privacy-preserving%2520mechanisms%2520specifically%2520designed%2520for%2520them.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05699v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Membership%20Inference%20Attacks%20on%20Tokenizers%20of%20Large%20Language%20Models&entry.906535625=Meng%20Tong%20and%20Yuntao%20Du%20and%20Kejiang%20Chen%20and%20Weiming%20Zhang&entry.1292438233=Membership%20inference%20attacks%20%28MIAs%29%20are%20widely%20used%20to%20assess%20the%20privacy%20risks%20associated%20with%20machine%20learning%20models.%20However%2C%20when%20these%20attacks%20are%20applied%20to%20pre-trained%20large%20language%20models%20%28LLMs%29%2C%20they%20encounter%20significant%20challenges%2C%20including%20mislabeled%20samples%2C%20distribution%20shifts%2C%20and%20discrepancies%20in%20model%20size%20between%20experimental%20and%20real-world%20settings.%20To%20address%20these%20limitations%2C%20we%20introduce%20tokenizers%20as%20a%20new%20attack%20vector%20for%20membership%20inference.%20Specifically%2C%20a%20tokenizer%20converts%20raw%20text%20into%20tokens%20for%20LLMs.%20Unlike%20full%20models%2C%20tokenizers%20can%20be%20efficiently%20trained%20from%20scratch%2C%20thereby%20avoiding%20the%20aforementioned%20challenges.%20In%20addition%2C%20the%20tokenizer%27s%20training%20data%20is%20typically%20representative%20of%20the%20data%20used%20to%20pre-train%20LLMs.%20Despite%20these%20advantages%2C%20the%20potential%20of%20tokenizers%20as%20an%20attack%20vector%20remains%20unexplored.%20To%20this%20end%2C%20we%20present%20the%20first%20study%20on%20membership%20leakage%20through%20tokenizers%20and%20explore%20five%20attack%20methods%20to%20infer%20dataset%20membership.%20Extensive%20experiments%20on%20millions%20of%20Internet%20samples%20reveal%20the%20vulnerabilities%20in%20the%20tokenizers%20of%20state-of-the-art%20LLMs.%20To%20mitigate%20this%20emerging%20risk%2C%20we%20further%20propose%20an%20adaptive%20defense.%20Our%20findings%20highlight%20tokenizers%20as%20an%20overlooked%20yet%20critical%20privacy%20threat%2C%20underscoring%20the%20urgent%20need%20for%20privacy-preserving%20mechanisms%20specifically%20designed%20for%20them.&entry.1838667208=http%3A//arxiv.org/abs/2510.05699v2&entry.124074799=Read"},
{"title": "Free-RBF-KAN: Kolmogorov-Arnold Networks with Adaptive Radial Basis Functions for Efficient Function Learning", "author": "Shao-Ting Chiu and Siu Wun Cheung and Ulisses Braga-Neto and Chak Shing Lee and Rui Peng Li", "abstract": "Kolmogorov-Arnold Networks (KANs) have shown strong potential for efficiently approximating complex nonlinear functions. However, the original KAN formulation relies on B-spline basis functions, which incur substantial computational overhead due to De Boor's algorithm. To address this limitation, recent work has explored alternative basis functions such as radial basis functions (RBFs) that can improve computational efficiency and flexibility. Yet, standard RBF-KANs often sacrifice accuracy relative to the original KAN design. In this work, we propose Free-RBF-KAN, a RBF-based KAN architecture that incorporates adaptive learning grids and trainable smoothness to close this performance gap. Our method employs freely learnable RBF shapes that dynamically align grid representations with activation patterns, enabling expressive and adaptive function approximation. Additionally, we treat smoothness as a kernel parameter optimized jointly with network weights, without increasing computational complexity. We provide a general universality proof for RBF-KANs, which encompasses our Free-RBF-KAN formulation. Through a broad set of experiments, including multiscale function approximation, physics-informed machine learning, and PDE solution operator learning, Free-RBF-KAN achieves accuracy comparable to the original B-spline-based KAN while delivering faster training and inference. These results highlight Free-RBF-KAN as a compelling balance between computational efficiency and adaptive resolution, particularly for high-dimensional structured modeling tasks.", "link": "http://arxiv.org/abs/2601.07760v1", "date": "2026-01-12", "relevancy": 1.8888, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5074}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4707}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free-RBF-KAN%3A%20Kolmogorov-Arnold%20Networks%20with%20Adaptive%20Radial%20Basis%20Functions%20for%20Efficient%20Function%20Learning&body=Title%3A%20Free-RBF-KAN%3A%20Kolmogorov-Arnold%20Networks%20with%20Adaptive%20Radial%20Basis%20Functions%20for%20Efficient%20Function%20Learning%0AAuthor%3A%20Shao-Ting%20Chiu%20and%20Siu%20Wun%20Cheung%20and%20Ulisses%20Braga-Neto%20and%20Chak%20Shing%20Lee%20and%20Rui%20Peng%20Li%0AAbstract%3A%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20have%20shown%20strong%20potential%20for%20efficiently%20approximating%20complex%20nonlinear%20functions.%20However%2C%20the%20original%20KAN%20formulation%20relies%20on%20B-spline%20basis%20functions%2C%20which%20incur%20substantial%20computational%20overhead%20due%20to%20De%20Boor%27s%20algorithm.%20To%20address%20this%20limitation%2C%20recent%20work%20has%20explored%20alternative%20basis%20functions%20such%20as%20radial%20basis%20functions%20%28RBFs%29%20that%20can%20improve%20computational%20efficiency%20and%20flexibility.%20Yet%2C%20standard%20RBF-KANs%20often%20sacrifice%20accuracy%20relative%20to%20the%20original%20KAN%20design.%20In%20this%20work%2C%20we%20propose%20Free-RBF-KAN%2C%20a%20RBF-based%20KAN%20architecture%20that%20incorporates%20adaptive%20learning%20grids%20and%20trainable%20smoothness%20to%20close%20this%20performance%20gap.%20Our%20method%20employs%20freely%20learnable%20RBF%20shapes%20that%20dynamically%20align%20grid%20representations%20with%20activation%20patterns%2C%20enabling%20expressive%20and%20adaptive%20function%20approximation.%20Additionally%2C%20we%20treat%20smoothness%20as%20a%20kernel%20parameter%20optimized%20jointly%20with%20network%20weights%2C%20without%20increasing%20computational%20complexity.%20We%20provide%20a%20general%20universality%20proof%20for%20RBF-KANs%2C%20which%20encompasses%20our%20Free-RBF-KAN%20formulation.%20Through%20a%20broad%20set%20of%20experiments%2C%20including%20multiscale%20function%20approximation%2C%20physics-informed%20machine%20learning%2C%20and%20PDE%20solution%20operator%20learning%2C%20Free-RBF-KAN%20achieves%20accuracy%20comparable%20to%20the%20original%20B-spline-based%20KAN%20while%20delivering%20faster%20training%20and%20inference.%20These%20results%20highlight%20Free-RBF-KAN%20as%20a%20compelling%20balance%20between%20computational%20efficiency%20and%20adaptive%20resolution%2C%20particularly%20for%20high-dimensional%20structured%20modeling%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree-RBF-KAN%253A%2520Kolmogorov-Arnold%2520Networks%2520with%2520Adaptive%2520Radial%2520Basis%2520Functions%2520for%2520Efficient%2520Function%2520Learning%26entry.906535625%3DShao-Ting%2520Chiu%2520and%2520Siu%2520Wun%2520Cheung%2520and%2520Ulisses%2520Braga-Neto%2520and%2520Chak%2520Shing%2520Lee%2520and%2520Rui%2520Peng%2520Li%26entry.1292438233%3DKolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520have%2520shown%2520strong%2520potential%2520for%2520efficiently%2520approximating%2520complex%2520nonlinear%2520functions.%2520However%252C%2520the%2520original%2520KAN%2520formulation%2520relies%2520on%2520B-spline%2520basis%2520functions%252C%2520which%2520incur%2520substantial%2520computational%2520overhead%2520due%2520to%2520De%2520Boor%2527s%2520algorithm.%2520To%2520address%2520this%2520limitation%252C%2520recent%2520work%2520has%2520explored%2520alternative%2520basis%2520functions%2520such%2520as%2520radial%2520basis%2520functions%2520%2528RBFs%2529%2520that%2520can%2520improve%2520computational%2520efficiency%2520and%2520flexibility.%2520Yet%252C%2520standard%2520RBF-KANs%2520often%2520sacrifice%2520accuracy%2520relative%2520to%2520the%2520original%2520KAN%2520design.%2520In%2520this%2520work%252C%2520we%2520propose%2520Free-RBF-KAN%252C%2520a%2520RBF-based%2520KAN%2520architecture%2520that%2520incorporates%2520adaptive%2520learning%2520grids%2520and%2520trainable%2520smoothness%2520to%2520close%2520this%2520performance%2520gap.%2520Our%2520method%2520employs%2520freely%2520learnable%2520RBF%2520shapes%2520that%2520dynamically%2520align%2520grid%2520representations%2520with%2520activation%2520patterns%252C%2520enabling%2520expressive%2520and%2520adaptive%2520function%2520approximation.%2520Additionally%252C%2520we%2520treat%2520smoothness%2520as%2520a%2520kernel%2520parameter%2520optimized%2520jointly%2520with%2520network%2520weights%252C%2520without%2520increasing%2520computational%2520complexity.%2520We%2520provide%2520a%2520general%2520universality%2520proof%2520for%2520RBF-KANs%252C%2520which%2520encompasses%2520our%2520Free-RBF-KAN%2520formulation.%2520Through%2520a%2520broad%2520set%2520of%2520experiments%252C%2520including%2520multiscale%2520function%2520approximation%252C%2520physics-informed%2520machine%2520learning%252C%2520and%2520PDE%2520solution%2520operator%2520learning%252C%2520Free-RBF-KAN%2520achieves%2520accuracy%2520comparable%2520to%2520the%2520original%2520B-spline-based%2520KAN%2520while%2520delivering%2520faster%2520training%2520and%2520inference.%2520These%2520results%2520highlight%2520Free-RBF-KAN%2520as%2520a%2520compelling%2520balance%2520between%2520computational%2520efficiency%2520and%2520adaptive%2520resolution%252C%2520particularly%2520for%2520high-dimensional%2520structured%2520modeling%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free-RBF-KAN%3A%20Kolmogorov-Arnold%20Networks%20with%20Adaptive%20Radial%20Basis%20Functions%20for%20Efficient%20Function%20Learning&entry.906535625=Shao-Ting%20Chiu%20and%20Siu%20Wun%20Cheung%20and%20Ulisses%20Braga-Neto%20and%20Chak%20Shing%20Lee%20and%20Rui%20Peng%20Li&entry.1292438233=Kolmogorov-Arnold%20Networks%20%28KANs%29%20have%20shown%20strong%20potential%20for%20efficiently%20approximating%20complex%20nonlinear%20functions.%20However%2C%20the%20original%20KAN%20formulation%20relies%20on%20B-spline%20basis%20functions%2C%20which%20incur%20substantial%20computational%20overhead%20due%20to%20De%20Boor%27s%20algorithm.%20To%20address%20this%20limitation%2C%20recent%20work%20has%20explored%20alternative%20basis%20functions%20such%20as%20radial%20basis%20functions%20%28RBFs%29%20that%20can%20improve%20computational%20efficiency%20and%20flexibility.%20Yet%2C%20standard%20RBF-KANs%20often%20sacrifice%20accuracy%20relative%20to%20the%20original%20KAN%20design.%20In%20this%20work%2C%20we%20propose%20Free-RBF-KAN%2C%20a%20RBF-based%20KAN%20architecture%20that%20incorporates%20adaptive%20learning%20grids%20and%20trainable%20smoothness%20to%20close%20this%20performance%20gap.%20Our%20method%20employs%20freely%20learnable%20RBF%20shapes%20that%20dynamically%20align%20grid%20representations%20with%20activation%20patterns%2C%20enabling%20expressive%20and%20adaptive%20function%20approximation.%20Additionally%2C%20we%20treat%20smoothness%20as%20a%20kernel%20parameter%20optimized%20jointly%20with%20network%20weights%2C%20without%20increasing%20computational%20complexity.%20We%20provide%20a%20general%20universality%20proof%20for%20RBF-KANs%2C%20which%20encompasses%20our%20Free-RBF-KAN%20formulation.%20Through%20a%20broad%20set%20of%20experiments%2C%20including%20multiscale%20function%20approximation%2C%20physics-informed%20machine%20learning%2C%20and%20PDE%20solution%20operator%20learning%2C%20Free-RBF-KAN%20achieves%20accuracy%20comparable%20to%20the%20original%20B-spline-based%20KAN%20while%20delivering%20faster%20training%20and%20inference.%20These%20results%20highlight%20Free-RBF-KAN%20as%20a%20compelling%20balance%20between%20computational%20efficiency%20and%20adaptive%20resolution%2C%20particularly%20for%20high-dimensional%20structured%20modeling%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.07760v1&entry.124074799=Read"},
{"title": "A Framework for Feature Discovery in Intracranial Pressure Monitoring Data Using Neural Network Attention", "author": "Jonathan D. Socha and Seyed F. Maroufi and Dipankar Biswas and Richard Um and Aruna S. Rao and Mark G. Luciano", "abstract": "We present a novel framework for analyzing intracranial pressure monitoring data by applying interpretability principles. Intracranial pressure monitoring data was collected from 60 patients at Johns Hopkins. The data was segmented into individual cardiac cycles. A convolutional neural network was trained to classify each cardiac cycle into one of seven body positions. Neural network attention was extracted and was used to identify regions of interest in the waveform. Further directions for exploration are identified. This framework provides an extensible method to further understand the physiological and clinical underpinnings of the intracranial pressure waveform, which could lead to better diagnostic capabilities for intracranial pressure monitoring.", "link": "http://arxiv.org/abs/2601.07691v1", "date": "2026-01-12", "relevancy": 1.8158, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4575}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4546}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20for%20Feature%20Discovery%20in%20Intracranial%20Pressure%20Monitoring%20Data%20Using%20Neural%20Network%20Attention&body=Title%3A%20A%20Framework%20for%20Feature%20Discovery%20in%20Intracranial%20Pressure%20Monitoring%20Data%20Using%20Neural%20Network%20Attention%0AAuthor%3A%20Jonathan%20D.%20Socha%20and%20Seyed%20F.%20Maroufi%20and%20Dipankar%20Biswas%20and%20Richard%20Um%20and%20Aruna%20S.%20Rao%20and%20Mark%20G.%20Luciano%0AAbstract%3A%20We%20present%20a%20novel%20framework%20for%20analyzing%20intracranial%20pressure%20monitoring%20data%20by%20applying%20interpretability%20principles.%20Intracranial%20pressure%20monitoring%20data%20was%20collected%20from%2060%20patients%20at%20Johns%20Hopkins.%20The%20data%20was%20segmented%20into%20individual%20cardiac%20cycles.%20A%20convolutional%20neural%20network%20was%20trained%20to%20classify%20each%20cardiac%20cycle%20into%20one%20of%20seven%20body%20positions.%20Neural%20network%20attention%20was%20extracted%20and%20was%20used%20to%20identify%20regions%20of%20interest%20in%20the%20waveform.%20Further%20directions%20for%20exploration%20are%20identified.%20This%20framework%20provides%20an%20extensible%20method%20to%20further%20understand%20the%20physiological%20and%20clinical%20underpinnings%20of%20the%20intracranial%20pressure%20waveform%2C%20which%20could%20lead%20to%20better%20diagnostic%20capabilities%20for%20intracranial%20pressure%20monitoring.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Framework%2520for%2520Feature%2520Discovery%2520in%2520Intracranial%2520Pressure%2520Monitoring%2520Data%2520Using%2520Neural%2520Network%2520Attention%26entry.906535625%3DJonathan%2520D.%2520Socha%2520and%2520Seyed%2520F.%2520Maroufi%2520and%2520Dipankar%2520Biswas%2520and%2520Richard%2520Um%2520and%2520Aruna%2520S.%2520Rao%2520and%2520Mark%2520G.%2520Luciano%26entry.1292438233%3DWe%2520present%2520a%2520novel%2520framework%2520for%2520analyzing%2520intracranial%2520pressure%2520monitoring%2520data%2520by%2520applying%2520interpretability%2520principles.%2520Intracranial%2520pressure%2520monitoring%2520data%2520was%2520collected%2520from%252060%2520patients%2520at%2520Johns%2520Hopkins.%2520The%2520data%2520was%2520segmented%2520into%2520individual%2520cardiac%2520cycles.%2520A%2520convolutional%2520neural%2520network%2520was%2520trained%2520to%2520classify%2520each%2520cardiac%2520cycle%2520into%2520one%2520of%2520seven%2520body%2520positions.%2520Neural%2520network%2520attention%2520was%2520extracted%2520and%2520was%2520used%2520to%2520identify%2520regions%2520of%2520interest%2520in%2520the%2520waveform.%2520Further%2520directions%2520for%2520exploration%2520are%2520identified.%2520This%2520framework%2520provides%2520an%2520extensible%2520method%2520to%2520further%2520understand%2520the%2520physiological%2520and%2520clinical%2520underpinnings%2520of%2520the%2520intracranial%2520pressure%2520waveform%252C%2520which%2520could%2520lead%2520to%2520better%2520diagnostic%2520capabilities%2520for%2520intracranial%2520pressure%2520monitoring.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Feature%20Discovery%20in%20Intracranial%20Pressure%20Monitoring%20Data%20Using%20Neural%20Network%20Attention&entry.906535625=Jonathan%20D.%20Socha%20and%20Seyed%20F.%20Maroufi%20and%20Dipankar%20Biswas%20and%20Richard%20Um%20and%20Aruna%20S.%20Rao%20and%20Mark%20G.%20Luciano&entry.1292438233=We%20present%20a%20novel%20framework%20for%20analyzing%20intracranial%20pressure%20monitoring%20data%20by%20applying%20interpretability%20principles.%20Intracranial%20pressure%20monitoring%20data%20was%20collected%20from%2060%20patients%20at%20Johns%20Hopkins.%20The%20data%20was%20segmented%20into%20individual%20cardiac%20cycles.%20A%20convolutional%20neural%20network%20was%20trained%20to%20classify%20each%20cardiac%20cycle%20into%20one%20of%20seven%20body%20positions.%20Neural%20network%20attention%20was%20extracted%20and%20was%20used%20to%20identify%20regions%20of%20interest%20in%20the%20waveform.%20Further%20directions%20for%20exploration%20are%20identified.%20This%20framework%20provides%20an%20extensible%20method%20to%20further%20understand%20the%20physiological%20and%20clinical%20underpinnings%20of%20the%20intracranial%20pressure%20waveform%2C%20which%20could%20lead%20to%20better%20diagnostic%20capabilities%20for%20intracranial%20pressure%20monitoring.&entry.1838667208=http%3A//arxiv.org/abs/2601.07691v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


