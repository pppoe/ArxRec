<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250819.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Online 3D Gaussian Splatting Modeling with Novel View Selection", "author": "Byeonggwon Lee and Junkyu Park and Khang Truong Giang and Soohwan Song", "abstract": "  This study addresses the challenge of generating online 3D Gaussian Splatting\n(3DGS) models from RGB-only frames. Previous studies have employed dense SLAM\ntechniques to estimate 3D scenes from keyframes for 3DGS model construction.\nHowever, these methods are limited by their reliance solely on keyframes, which\nare insufficient to capture an entire scene, resulting in incomplete\nreconstructions. Moreover, building a generalizable model requires\nincorporating frames from diverse viewpoints to achieve broader scene coverage.\nHowever, online processing restricts the use of many frames or extensive\ntraining iterations. Therefore, we propose a novel method for high-quality 3DGS\nmodeling that improves model completeness through adaptive view selection. By\nanalyzing reconstruction quality online, our approach selects optimal\nnon-keyframes for additional training. By integrating both keyframes and\nselected non-keyframes, the method refines incomplete regions from diverse\nviewpoints, significantly enhancing completeness. We also present a framework\nthat incorporates an online multi-view stereo approach, ensuring consistency in\n3D information throughout the 3DGS modeling process. Experimental results\ndemonstrate that our method outperforms state-of-the-art methods, delivering\nexceptional performance in complex outdoor scenes.\n", "link": "http://arxiv.org/abs/2508.14014v1", "date": "2025-08-19", "relevancy": 3.5049, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.745}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6966}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%203D%20Gaussian%20Splatting%20Modeling%20with%20Novel%20View%20Selection&body=Title%3A%20Online%203D%20Gaussian%20Splatting%20Modeling%20with%20Novel%20View%20Selection%0AAuthor%3A%20Byeonggwon%20Lee%20and%20Junkyu%20Park%20and%20Khang%20Truong%20Giang%20and%20Soohwan%20Song%0AAbstract%3A%20%20%20This%20study%20addresses%20the%20challenge%20of%20generating%20online%203D%20Gaussian%20Splatting%0A%283DGS%29%20models%20from%20RGB-only%20frames.%20Previous%20studies%20have%20employed%20dense%20SLAM%0Atechniques%20to%20estimate%203D%20scenes%20from%20keyframes%20for%203DGS%20model%20construction.%0AHowever%2C%20these%20methods%20are%20limited%20by%20their%20reliance%20solely%20on%20keyframes%2C%20which%0Aare%20insufficient%20to%20capture%20an%20entire%20scene%2C%20resulting%20in%20incomplete%0Areconstructions.%20Moreover%2C%20building%20a%20generalizable%20model%20requires%0Aincorporating%20frames%20from%20diverse%20viewpoints%20to%20achieve%20broader%20scene%20coverage.%0AHowever%2C%20online%20processing%20restricts%20the%20use%20of%20many%20frames%20or%20extensive%0Atraining%20iterations.%20Therefore%2C%20we%20propose%20a%20novel%20method%20for%20high-quality%203DGS%0Amodeling%20that%20improves%20model%20completeness%20through%20adaptive%20view%20selection.%20By%0Aanalyzing%20reconstruction%20quality%20online%2C%20our%20approach%20selects%20optimal%0Anon-keyframes%20for%20additional%20training.%20By%20integrating%20both%20keyframes%20and%0Aselected%20non-keyframes%2C%20the%20method%20refines%20incomplete%20regions%20from%20diverse%0Aviewpoints%2C%20significantly%20enhancing%20completeness.%20We%20also%20present%20a%20framework%0Athat%20incorporates%20an%20online%20multi-view%20stereo%20approach%2C%20ensuring%20consistency%20in%0A3D%20information%20throughout%20the%203DGS%20modeling%20process.%20Experimental%20results%0Ademonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20methods%2C%20delivering%0Aexceptional%20performance%20in%20complex%20outdoor%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14014v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%25203D%2520Gaussian%2520Splatting%2520Modeling%2520with%2520Novel%2520View%2520Selection%26entry.906535625%3DByeonggwon%2520Lee%2520and%2520Junkyu%2520Park%2520and%2520Khang%2520Truong%2520Giang%2520and%2520Soohwan%2520Song%26entry.1292438233%3D%2520%2520This%2520study%2520addresses%2520the%2520challenge%2520of%2520generating%2520online%25203D%2520Gaussian%2520Splatting%250A%25283DGS%2529%2520models%2520from%2520RGB-only%2520frames.%2520Previous%2520studies%2520have%2520employed%2520dense%2520SLAM%250Atechniques%2520to%2520estimate%25203D%2520scenes%2520from%2520keyframes%2520for%25203DGS%2520model%2520construction.%250AHowever%252C%2520these%2520methods%2520are%2520limited%2520by%2520their%2520reliance%2520solely%2520on%2520keyframes%252C%2520which%250Aare%2520insufficient%2520to%2520capture%2520an%2520entire%2520scene%252C%2520resulting%2520in%2520incomplete%250Areconstructions.%2520Moreover%252C%2520building%2520a%2520generalizable%2520model%2520requires%250Aincorporating%2520frames%2520from%2520diverse%2520viewpoints%2520to%2520achieve%2520broader%2520scene%2520coverage.%250AHowever%252C%2520online%2520processing%2520restricts%2520the%2520use%2520of%2520many%2520frames%2520or%2520extensive%250Atraining%2520iterations.%2520Therefore%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%2520high-quality%25203DGS%250Amodeling%2520that%2520improves%2520model%2520completeness%2520through%2520adaptive%2520view%2520selection.%2520By%250Aanalyzing%2520reconstruction%2520quality%2520online%252C%2520our%2520approach%2520selects%2520optimal%250Anon-keyframes%2520for%2520additional%2520training.%2520By%2520integrating%2520both%2520keyframes%2520and%250Aselected%2520non-keyframes%252C%2520the%2520method%2520refines%2520incomplete%2520regions%2520from%2520diverse%250Aviewpoints%252C%2520significantly%2520enhancing%2520completeness.%2520We%2520also%2520present%2520a%2520framework%250Athat%2520incorporates%2520an%2520online%2520multi-view%2520stereo%2520approach%252C%2520ensuring%2520consistency%2520in%250A3D%2520information%2520throughout%2520the%25203DGS%2520modeling%2520process.%2520Experimental%2520results%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520methods%252C%2520delivering%250Aexceptional%2520performance%2520in%2520complex%2520outdoor%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14014v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%203D%20Gaussian%20Splatting%20Modeling%20with%20Novel%20View%20Selection&entry.906535625=Byeonggwon%20Lee%20and%20Junkyu%20Park%20and%20Khang%20Truong%20Giang%20and%20Soohwan%20Song&entry.1292438233=%20%20This%20study%20addresses%20the%20challenge%20of%20generating%20online%203D%20Gaussian%20Splatting%0A%283DGS%29%20models%20from%20RGB-only%20frames.%20Previous%20studies%20have%20employed%20dense%20SLAM%0Atechniques%20to%20estimate%203D%20scenes%20from%20keyframes%20for%203DGS%20model%20construction.%0AHowever%2C%20these%20methods%20are%20limited%20by%20their%20reliance%20solely%20on%20keyframes%2C%20which%0Aare%20insufficient%20to%20capture%20an%20entire%20scene%2C%20resulting%20in%20incomplete%0Areconstructions.%20Moreover%2C%20building%20a%20generalizable%20model%20requires%0Aincorporating%20frames%20from%20diverse%20viewpoints%20to%20achieve%20broader%20scene%20coverage.%0AHowever%2C%20online%20processing%20restricts%20the%20use%20of%20many%20frames%20or%20extensive%0Atraining%20iterations.%20Therefore%2C%20we%20propose%20a%20novel%20method%20for%20high-quality%203DGS%0Amodeling%20that%20improves%20model%20completeness%20through%20adaptive%20view%20selection.%20By%0Aanalyzing%20reconstruction%20quality%20online%2C%20our%20approach%20selects%20optimal%0Anon-keyframes%20for%20additional%20training.%20By%20integrating%20both%20keyframes%20and%0Aselected%20non-keyframes%2C%20the%20method%20refines%20incomplete%20regions%20from%20diverse%0Aviewpoints%2C%20significantly%20enhancing%20completeness.%20We%20also%20present%20a%20framework%0Athat%20incorporates%20an%20online%20multi-view%20stereo%20approach%2C%20ensuring%20consistency%20in%0A3D%20information%20throughout%20the%203DGS%20modeling%20process.%20Experimental%20results%0Ademonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20methods%2C%20delivering%0Aexceptional%20performance%20in%20complex%20outdoor%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14014v1&entry.124074799=Read"},
{"title": "PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis", "author": "Chunji Lv and Zequn Chen and Donglin Di and Weinan Zhang and Hao Li and Wei Chen and Changsheng Li", "abstract": "  While physics-grounded 3D motion synthesis has seen significant progress,\ncurrent methods face critical limitations. They typically rely on\npre-reconstructed 3D Gaussian Splatting (3DGS) representations, while physics\nintegration depends on either inflexible, manually defined physical attributes\nor unstable, optimization-heavy guidance from video models. To overcome these\nchallenges, we introduce PhysGM, a feed-forward framework that jointly predicts\na 3D Gaussian representation and its physical properties from a single image,\nenabling immediate, physical simulation and high-fidelity 4D rendering. We\nfirst establish a base model by jointly optimizing for Gaussian reconstruction\nand probabilistic physics prediction. The model is then refined with physically\nplausible reference videos to enhance both rendering fidelity and physics\nprediction accuracy. We adopt the Direct Preference Optimization (DPO) to align\nits simulations with reference videos, circumventing Score Distillation\nSampling (SDS) optimization which needs back-propagating gradients through the\ncomplex differentiable simulation and rasterization. To facilitate the\ntraining, we introduce a new dataset PhysAssets of over 24,000 3D assets,\nannotated with physical properties and corresponding guiding videos.\nExperimental results demonstrate that our method effectively generates\nhigh-fidelity 4D simulations from a single image in one minute. This represents\na significant speedup over prior works while delivering realistic rendering\nresults. Our project page is at:https://hihixiaolv.github.io/PhysGM.github.io/\n", "link": "http://arxiv.org/abs/2508.13911v1", "date": "2025-08-19", "relevancy": 3.4612, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7413}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6717}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysGM%3A%20Large%20Physical%20Gaussian%20Model%20for%20Feed-Forward%204D%20Synthesis&body=Title%3A%20PhysGM%3A%20Large%20Physical%20Gaussian%20Model%20for%20Feed-Forward%204D%20Synthesis%0AAuthor%3A%20Chunji%20Lv%20and%20Zequn%20Chen%20and%20Donglin%20Di%20and%20Weinan%20Zhang%20and%20Hao%20Li%20and%20Wei%20Chen%20and%20Changsheng%20Li%0AAbstract%3A%20%20%20While%20physics-grounded%203D%20motion%20synthesis%20has%20seen%20significant%20progress%2C%0Acurrent%20methods%20face%20critical%20limitations.%20They%20typically%20rely%20on%0Apre-reconstructed%203D%20Gaussian%20Splatting%20%283DGS%29%20representations%2C%20while%20physics%0Aintegration%20depends%20on%20either%20inflexible%2C%20manually%20defined%20physical%20attributes%0Aor%20unstable%2C%20optimization-heavy%20guidance%20from%20video%20models.%20To%20overcome%20these%0Achallenges%2C%20we%20introduce%20PhysGM%2C%20a%20feed-forward%20framework%20that%20jointly%20predicts%0Aa%203D%20Gaussian%20representation%20and%20its%20physical%20properties%20from%20a%20single%20image%2C%0Aenabling%20immediate%2C%20physical%20simulation%20and%20high-fidelity%204D%20rendering.%20We%0Afirst%20establish%20a%20base%20model%20by%20jointly%20optimizing%20for%20Gaussian%20reconstruction%0Aand%20probabilistic%20physics%20prediction.%20The%20model%20is%20then%20refined%20with%20physically%0Aplausible%20reference%20videos%20to%20enhance%20both%20rendering%20fidelity%20and%20physics%0Aprediction%20accuracy.%20We%20adopt%20the%20Direct%20Preference%20Optimization%20%28DPO%29%20to%20align%0Aits%20simulations%20with%20reference%20videos%2C%20circumventing%20Score%20Distillation%0ASampling%20%28SDS%29%20optimization%20which%20needs%20back-propagating%20gradients%20through%20the%0Acomplex%20differentiable%20simulation%20and%20rasterization.%20To%20facilitate%20the%0Atraining%2C%20we%20introduce%20a%20new%20dataset%20PhysAssets%20of%20over%2024%2C000%203D%20assets%2C%0Aannotated%20with%20physical%20properties%20and%20corresponding%20guiding%20videos.%0AExperimental%20results%20demonstrate%20that%20our%20method%20effectively%20generates%0Ahigh-fidelity%204D%20simulations%20from%20a%20single%20image%20in%20one%20minute.%20This%20represents%0Aa%20significant%20speedup%20over%20prior%20works%20while%20delivering%20realistic%20rendering%0Aresults.%20Our%20project%20page%20is%20at%3Ahttps%3A//hihixiaolv.github.io/PhysGM.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13911v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysGM%253A%2520Large%2520Physical%2520Gaussian%2520Model%2520for%2520Feed-Forward%25204D%2520Synthesis%26entry.906535625%3DChunji%2520Lv%2520and%2520Zequn%2520Chen%2520and%2520Donglin%2520Di%2520and%2520Weinan%2520Zhang%2520and%2520Hao%2520Li%2520and%2520Wei%2520Chen%2520and%2520Changsheng%2520Li%26entry.1292438233%3D%2520%2520While%2520physics-grounded%25203D%2520motion%2520synthesis%2520has%2520seen%2520significant%2520progress%252C%250Acurrent%2520methods%2520face%2520critical%2520limitations.%2520They%2520typically%2520rely%2520on%250Apre-reconstructed%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520representations%252C%2520while%2520physics%250Aintegration%2520depends%2520on%2520either%2520inflexible%252C%2520manually%2520defined%2520physical%2520attributes%250Aor%2520unstable%252C%2520optimization-heavy%2520guidance%2520from%2520video%2520models.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520introduce%2520PhysGM%252C%2520a%2520feed-forward%2520framework%2520that%2520jointly%2520predicts%250Aa%25203D%2520Gaussian%2520representation%2520and%2520its%2520physical%2520properties%2520from%2520a%2520single%2520image%252C%250Aenabling%2520immediate%252C%2520physical%2520simulation%2520and%2520high-fidelity%25204D%2520rendering.%2520We%250Afirst%2520establish%2520a%2520base%2520model%2520by%2520jointly%2520optimizing%2520for%2520Gaussian%2520reconstruction%250Aand%2520probabilistic%2520physics%2520prediction.%2520The%2520model%2520is%2520then%2520refined%2520with%2520physically%250Aplausible%2520reference%2520videos%2520to%2520enhance%2520both%2520rendering%2520fidelity%2520and%2520physics%250Aprediction%2520accuracy.%2520We%2520adopt%2520the%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520to%2520align%250Aits%2520simulations%2520with%2520reference%2520videos%252C%2520circumventing%2520Score%2520Distillation%250ASampling%2520%2528SDS%2529%2520optimization%2520which%2520needs%2520back-propagating%2520gradients%2520through%2520the%250Acomplex%2520differentiable%2520simulation%2520and%2520rasterization.%2520To%2520facilitate%2520the%250Atraining%252C%2520we%2520introduce%2520a%2520new%2520dataset%2520PhysAssets%2520of%2520over%252024%252C000%25203D%2520assets%252C%250Aannotated%2520with%2520physical%2520properties%2520and%2520corresponding%2520guiding%2520videos.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520effectively%2520generates%250Ahigh-fidelity%25204D%2520simulations%2520from%2520a%2520single%2520image%2520in%2520one%2520minute.%2520This%2520represents%250Aa%2520significant%2520speedup%2520over%2520prior%2520works%2520while%2520delivering%2520realistic%2520rendering%250Aresults.%2520Our%2520project%2520page%2520is%2520at%253Ahttps%253A//hihixiaolv.github.io/PhysGM.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13911v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysGM%3A%20Large%20Physical%20Gaussian%20Model%20for%20Feed-Forward%204D%20Synthesis&entry.906535625=Chunji%20Lv%20and%20Zequn%20Chen%20and%20Donglin%20Di%20and%20Weinan%20Zhang%20and%20Hao%20Li%20and%20Wei%20Chen%20and%20Changsheng%20Li&entry.1292438233=%20%20While%20physics-grounded%203D%20motion%20synthesis%20has%20seen%20significant%20progress%2C%0Acurrent%20methods%20face%20critical%20limitations.%20They%20typically%20rely%20on%0Apre-reconstructed%203D%20Gaussian%20Splatting%20%283DGS%29%20representations%2C%20while%20physics%0Aintegration%20depends%20on%20either%20inflexible%2C%20manually%20defined%20physical%20attributes%0Aor%20unstable%2C%20optimization-heavy%20guidance%20from%20video%20models.%20To%20overcome%20these%0Achallenges%2C%20we%20introduce%20PhysGM%2C%20a%20feed-forward%20framework%20that%20jointly%20predicts%0Aa%203D%20Gaussian%20representation%20and%20its%20physical%20properties%20from%20a%20single%20image%2C%0Aenabling%20immediate%2C%20physical%20simulation%20and%20high-fidelity%204D%20rendering.%20We%0Afirst%20establish%20a%20base%20model%20by%20jointly%20optimizing%20for%20Gaussian%20reconstruction%0Aand%20probabilistic%20physics%20prediction.%20The%20model%20is%20then%20refined%20with%20physically%0Aplausible%20reference%20videos%20to%20enhance%20both%20rendering%20fidelity%20and%20physics%0Aprediction%20accuracy.%20We%20adopt%20the%20Direct%20Preference%20Optimization%20%28DPO%29%20to%20align%0Aits%20simulations%20with%20reference%20videos%2C%20circumventing%20Score%20Distillation%0ASampling%20%28SDS%29%20optimization%20which%20needs%20back-propagating%20gradients%20through%20the%0Acomplex%20differentiable%20simulation%20and%20rasterization.%20To%20facilitate%20the%0Atraining%2C%20we%20introduce%20a%20new%20dataset%20PhysAssets%20of%20over%2024%2C000%203D%20assets%2C%0Aannotated%20with%20physical%20properties%20and%20corresponding%20guiding%20videos.%0AExperimental%20results%20demonstrate%20that%20our%20method%20effectively%20generates%0Ahigh-fidelity%204D%20simulations%20from%20a%20single%20image%20in%20one%20minute.%20This%20represents%0Aa%20significant%20speedup%20over%20prior%20works%20while%20delivering%20realistic%20rendering%0Aresults.%20Our%20project%20page%20is%20at%3Ahttps%3A//hihixiaolv.github.io/PhysGM.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13911v1&entry.124074799=Read"},
{"title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos", "author": "Chin-Yang Lin and Cheng Sun and Fu-En Yang and Min-Hung Chen and Yen-Yu Lin and Yu-Lun Liu", "abstract": "  LongSplat addresses critical challenges in novel view synthesis (NVS) from\ncasually captured long videos characterized by irregular camera motion, unknown\ncamera poses, and expansive scenes. Current methods often suffer from pose\ndrift, inaccurate geometry initialization, and severe memory limitations. To\naddress these issues, we introduce LongSplat, a robust unposed 3D Gaussian\nSplatting framework featuring: (1) Incremental Joint Optimization that\nconcurrently optimizes camera poses and 3D Gaussians to avoid local minima and\nensure global consistency; (2) a robust Pose Estimation Module leveraging\nlearned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that\nconverts dense point clouds into anchors based on spatial density. Extensive\nexperiments on challenging benchmarks demonstrate that LongSplat achieves\nstate-of-the-art results, substantially improving rendering quality, pose\naccuracy, and computational efficiency compared to prior approaches. Project\npage: https://linjohnss.github.io/longsplat/\n", "link": "http://arxiv.org/abs/2508.14041v1", "date": "2025-08-19", "relevancy": 3.4353, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7311}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6854}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongSplat%3A%20Robust%20Unposed%203D%20Gaussian%20Splatting%20for%20Casual%20Long%20Videos&body=Title%3A%20LongSplat%3A%20Robust%20Unposed%203D%20Gaussian%20Splatting%20for%20Casual%20Long%20Videos%0AAuthor%3A%20Chin-Yang%20Lin%20and%20Cheng%20Sun%20and%20Fu-En%20Yang%20and%20Min-Hung%20Chen%20and%20Yen-Yu%20Lin%20and%20Yu-Lun%20Liu%0AAbstract%3A%20%20%20LongSplat%20addresses%20critical%20challenges%20in%20novel%20view%20synthesis%20%28NVS%29%20from%0Acasually%20captured%20long%20videos%20characterized%20by%20irregular%20camera%20motion%2C%20unknown%0Acamera%20poses%2C%20and%20expansive%20scenes.%20Current%20methods%20often%20suffer%20from%20pose%0Adrift%2C%20inaccurate%20geometry%20initialization%2C%20and%20severe%20memory%20limitations.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20LongSplat%2C%20a%20robust%20unposed%203D%20Gaussian%0ASplatting%20framework%20featuring%3A%20%281%29%20Incremental%20Joint%20Optimization%20that%0Aconcurrently%20optimizes%20camera%20poses%20and%203D%20Gaussians%20to%20avoid%20local%20minima%20and%0Aensure%20global%20consistency%3B%20%282%29%20a%20robust%20Pose%20Estimation%20Module%20leveraging%0Alearned%203D%20priors%3B%20and%20%283%29%20an%20efficient%20Octree%20Anchor%20Formation%20mechanism%20that%0Aconverts%20dense%20point%20clouds%20into%20anchors%20based%20on%20spatial%20density.%20Extensive%0Aexperiments%20on%20challenging%20benchmarks%20demonstrate%20that%20LongSplat%20achieves%0Astate-of-the-art%20results%2C%20substantially%20improving%20rendering%20quality%2C%20pose%0Aaccuracy%2C%20and%20computational%20efficiency%20compared%20to%20prior%20approaches.%20Project%0Apage%3A%20https%3A//linjohnss.github.io/longsplat/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongSplat%253A%2520Robust%2520Unposed%25203D%2520Gaussian%2520Splatting%2520for%2520Casual%2520Long%2520Videos%26entry.906535625%3DChin-Yang%2520Lin%2520and%2520Cheng%2520Sun%2520and%2520Fu-En%2520Yang%2520and%2520Min-Hung%2520Chen%2520and%2520Yen-Yu%2520Lin%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3D%2520%2520LongSplat%2520addresses%2520critical%2520challenges%2520in%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520from%250Acasually%2520captured%2520long%2520videos%2520characterized%2520by%2520irregular%2520camera%2520motion%252C%2520unknown%250Acamera%2520poses%252C%2520and%2520expansive%2520scenes.%2520Current%2520methods%2520often%2520suffer%2520from%2520pose%250Adrift%252C%2520inaccurate%2520geometry%2520initialization%252C%2520and%2520severe%2520memory%2520limitations.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520introduce%2520LongSplat%252C%2520a%2520robust%2520unposed%25203D%2520Gaussian%250ASplatting%2520framework%2520featuring%253A%2520%25281%2529%2520Incremental%2520Joint%2520Optimization%2520that%250Aconcurrently%2520optimizes%2520camera%2520poses%2520and%25203D%2520Gaussians%2520to%2520avoid%2520local%2520minima%2520and%250Aensure%2520global%2520consistency%253B%2520%25282%2529%2520a%2520robust%2520Pose%2520Estimation%2520Module%2520leveraging%250Alearned%25203D%2520priors%253B%2520and%2520%25283%2529%2520an%2520efficient%2520Octree%2520Anchor%2520Formation%2520mechanism%2520that%250Aconverts%2520dense%2520point%2520clouds%2520into%2520anchors%2520based%2520on%2520spatial%2520density.%2520Extensive%250Aexperiments%2520on%2520challenging%2520benchmarks%2520demonstrate%2520that%2520LongSplat%2520achieves%250Astate-of-the-art%2520results%252C%2520substantially%2520improving%2520rendering%2520quality%252C%2520pose%250Aaccuracy%252C%2520and%2520computational%2520efficiency%2520compared%2520to%2520prior%2520approaches.%2520Project%250Apage%253A%2520https%253A//linjohnss.github.io/longsplat/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongSplat%3A%20Robust%20Unposed%203D%20Gaussian%20Splatting%20for%20Casual%20Long%20Videos&entry.906535625=Chin-Yang%20Lin%20and%20Cheng%20Sun%20and%20Fu-En%20Yang%20and%20Min-Hung%20Chen%20and%20Yen-Yu%20Lin%20and%20Yu-Lun%20Liu&entry.1292438233=%20%20LongSplat%20addresses%20critical%20challenges%20in%20novel%20view%20synthesis%20%28NVS%29%20from%0Acasually%20captured%20long%20videos%20characterized%20by%20irregular%20camera%20motion%2C%20unknown%0Acamera%20poses%2C%20and%20expansive%20scenes.%20Current%20methods%20often%20suffer%20from%20pose%0Adrift%2C%20inaccurate%20geometry%20initialization%2C%20and%20severe%20memory%20limitations.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20LongSplat%2C%20a%20robust%20unposed%203D%20Gaussian%0ASplatting%20framework%20featuring%3A%20%281%29%20Incremental%20Joint%20Optimization%20that%0Aconcurrently%20optimizes%20camera%20poses%20and%203D%20Gaussians%20to%20avoid%20local%20minima%20and%0Aensure%20global%20consistency%3B%20%282%29%20a%20robust%20Pose%20Estimation%20Module%20leveraging%0Alearned%203D%20priors%3B%20and%20%283%29%20an%20efficient%20Octree%20Anchor%20Formation%20mechanism%20that%0Aconverts%20dense%20point%20clouds%20into%20anchors%20based%20on%20spatial%20density.%20Extensive%0Aexperiments%20on%20challenging%20benchmarks%20demonstrate%20that%20LongSplat%20achieves%0Astate-of-the-art%20results%2C%20substantially%20improving%20rendering%20quality%2C%20pose%0Aaccuracy%2C%20and%20computational%20efficiency%20compared%20to%20prior%20approaches.%20Project%0Apage%3A%20https%3A//linjohnss.github.io/longsplat/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14041v1&entry.124074799=Read"},
{"title": "HouseCrafter: Lifting Floorplans to 3D Scenes with 2D Diffusion Model", "author": "Hieu T. Nguyen and Yiwen Chen and Vikram Voleti and Varun Jampani and Huaizu Jiang", "abstract": "  We introduce HouseCrafter, a novel approach that can lift a floorplan into a\ncomplete large 3D indoor scene (e.g., a house). Our key insight is to adapt a\n2D diffusion model, which is trained on web-scale images, to generate\nconsistent multi-view color (RGB) and depth (D) images across different\nlocations of the scene. Specifically, the RGB-D images are generated\nautoregressively in a batch-wise manner along sampled locations based on the\nfloorplan, where previously generated images are used as condition to the\ndiffusion model to produce images at nearby locations. The global floorplan and\nattention design in the diffusion model ensures the consistency of the\ngenerated images, from which a 3D scene can be reconstructed. Through extensive\nevaluation on the 3D-Front dataset, we demonstrate that HouseCraft can generate\nhigh-quality house-scale 3D scenes. Ablation studies also validate the\neffectiveness of different design choices. We will release our code and model\nweights. Project page: https://neu-vi.github.io/houseCrafter/\n", "link": "http://arxiv.org/abs/2406.20077v2", "date": "2025-08-19", "relevancy": 3.3473, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6819}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6819}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HouseCrafter%3A%20Lifting%20Floorplans%20to%203D%20Scenes%20with%202D%20Diffusion%20Model&body=Title%3A%20HouseCrafter%3A%20Lifting%20Floorplans%20to%203D%20Scenes%20with%202D%20Diffusion%20Model%0AAuthor%3A%20Hieu%20T.%20Nguyen%20and%20Yiwen%20Chen%20and%20Vikram%20Voleti%20and%20Varun%20Jampani%20and%20Huaizu%20Jiang%0AAbstract%3A%20%20%20We%20introduce%20HouseCrafter%2C%20a%20novel%20approach%20that%20can%20lift%20a%20floorplan%20into%20a%0Acomplete%20large%203D%20indoor%20scene%20%28e.g.%2C%20a%20house%29.%20Our%20key%20insight%20is%20to%20adapt%20a%0A2D%20diffusion%20model%2C%20which%20is%20trained%20on%20web-scale%20images%2C%20to%20generate%0Aconsistent%20multi-view%20color%20%28RGB%29%20and%20depth%20%28D%29%20images%20across%20different%0Alocations%20of%20the%20scene.%20Specifically%2C%20the%20RGB-D%20images%20are%20generated%0Aautoregressively%20in%20a%20batch-wise%20manner%20along%20sampled%20locations%20based%20on%20the%0Afloorplan%2C%20where%20previously%20generated%20images%20are%20used%20as%20condition%20to%20the%0Adiffusion%20model%20to%20produce%20images%20at%20nearby%20locations.%20The%20global%20floorplan%20and%0Aattention%20design%20in%20the%20diffusion%20model%20ensures%20the%20consistency%20of%20the%0Agenerated%20images%2C%20from%20which%20a%203D%20scene%20can%20be%20reconstructed.%20Through%20extensive%0Aevaluation%20on%20the%203D-Front%20dataset%2C%20we%20demonstrate%20that%20HouseCraft%20can%20generate%0Ahigh-quality%20house-scale%203D%20scenes.%20Ablation%20studies%20also%20validate%20the%0Aeffectiveness%20of%20different%20design%20choices.%20We%20will%20release%20our%20code%20and%20model%0Aweights.%20Project%20page%3A%20https%3A//neu-vi.github.io/houseCrafter/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20077v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHouseCrafter%253A%2520Lifting%2520Floorplans%2520to%25203D%2520Scenes%2520with%25202D%2520Diffusion%2520Model%26entry.906535625%3DHieu%2520T.%2520Nguyen%2520and%2520Yiwen%2520Chen%2520and%2520Vikram%2520Voleti%2520and%2520Varun%2520Jampani%2520and%2520Huaizu%2520Jiang%26entry.1292438233%3D%2520%2520We%2520introduce%2520HouseCrafter%252C%2520a%2520novel%2520approach%2520that%2520can%2520lift%2520a%2520floorplan%2520into%2520a%250Acomplete%2520large%25203D%2520indoor%2520scene%2520%2528e.g.%252C%2520a%2520house%2529.%2520Our%2520key%2520insight%2520is%2520to%2520adapt%2520a%250A2D%2520diffusion%2520model%252C%2520which%2520is%2520trained%2520on%2520web-scale%2520images%252C%2520to%2520generate%250Aconsistent%2520multi-view%2520color%2520%2528RGB%2529%2520and%2520depth%2520%2528D%2529%2520images%2520across%2520different%250Alocations%2520of%2520the%2520scene.%2520Specifically%252C%2520the%2520RGB-D%2520images%2520are%2520generated%250Aautoregressively%2520in%2520a%2520batch-wise%2520manner%2520along%2520sampled%2520locations%2520based%2520on%2520the%250Afloorplan%252C%2520where%2520previously%2520generated%2520images%2520are%2520used%2520as%2520condition%2520to%2520the%250Adiffusion%2520model%2520to%2520produce%2520images%2520at%2520nearby%2520locations.%2520The%2520global%2520floorplan%2520and%250Aattention%2520design%2520in%2520the%2520diffusion%2520model%2520ensures%2520the%2520consistency%2520of%2520the%250Agenerated%2520images%252C%2520from%2520which%2520a%25203D%2520scene%2520can%2520be%2520reconstructed.%2520Through%2520extensive%250Aevaluation%2520on%2520the%25203D-Front%2520dataset%252C%2520we%2520demonstrate%2520that%2520HouseCraft%2520can%2520generate%250Ahigh-quality%2520house-scale%25203D%2520scenes.%2520Ablation%2520studies%2520also%2520validate%2520the%250Aeffectiveness%2520of%2520different%2520design%2520choices.%2520We%2520will%2520release%2520our%2520code%2520and%2520model%250Aweights.%2520Project%2520page%253A%2520https%253A//neu-vi.github.io/houseCrafter/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20077v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HouseCrafter%3A%20Lifting%20Floorplans%20to%203D%20Scenes%20with%202D%20Diffusion%20Model&entry.906535625=Hieu%20T.%20Nguyen%20and%20Yiwen%20Chen%20and%20Vikram%20Voleti%20and%20Varun%20Jampani%20and%20Huaizu%20Jiang&entry.1292438233=%20%20We%20introduce%20HouseCrafter%2C%20a%20novel%20approach%20that%20can%20lift%20a%20floorplan%20into%20a%0Acomplete%20large%203D%20indoor%20scene%20%28e.g.%2C%20a%20house%29.%20Our%20key%20insight%20is%20to%20adapt%20a%0A2D%20diffusion%20model%2C%20which%20is%20trained%20on%20web-scale%20images%2C%20to%20generate%0Aconsistent%20multi-view%20color%20%28RGB%29%20and%20depth%20%28D%29%20images%20across%20different%0Alocations%20of%20the%20scene.%20Specifically%2C%20the%20RGB-D%20images%20are%20generated%0Aautoregressively%20in%20a%20batch-wise%20manner%20along%20sampled%20locations%20based%20on%20the%0Afloorplan%2C%20where%20previously%20generated%20images%20are%20used%20as%20condition%20to%20the%0Adiffusion%20model%20to%20produce%20images%20at%20nearby%20locations.%20The%20global%20floorplan%20and%0Aattention%20design%20in%20the%20diffusion%20model%20ensures%20the%20consistency%20of%20the%0Agenerated%20images%2C%20from%20which%20a%203D%20scene%20can%20be%20reconstructed.%20Through%20extensive%0Aevaluation%20on%20the%203D-Front%20dataset%2C%20we%20demonstrate%20that%20HouseCraft%20can%20generate%0Ahigh-quality%20house-scale%203D%20scenes.%20Ablation%20studies%20also%20validate%20the%0Aeffectiveness%20of%20different%20design%20choices.%20We%20will%20release%20our%20code%20and%20model%0Aweights.%20Project%20page%3A%20https%3A//neu-vi.github.io/houseCrafter/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20077v2&entry.124074799=Read"},
{"title": "Distilled-3DGS:Distilled 3D Gaussian Splatting", "author": "Lintao Xiang and Xinkai Chen and Jianhuang Lai and Guangcong Wang", "abstract": "  3D Gaussian Splatting (3DGS) has exhibited remarkable efficacy in novel view\nsynthesis (NVS). However, it suffers from a significant drawback: achieving\nhigh-fidelity rendering typically necessitates a large number of 3D Gaussians,\nresulting in substantial memory consumption and storage requirements. To\naddress this challenge, we propose the first knowledge distillation framework\nfor 3DGS, featuring various teacher models, including vanilla 3DGS,\nnoise-augmented variants, and dropout-regularized versions. The outputs of\nthese teachers are aggregated to guide the optimization of a lightweight\nstudent model. To distill the hidden geometric structure, we propose a\nstructural similarity loss to boost the consistency of spatial geometric\ndistributions between the student and teacher model. Through comprehensive\nquantitative and qualitative evaluations across diverse datasets, the proposed\nDistilled-3DGS, a simple yet effective framework without bells and whistles,\nachieves promising rendering results in both rendering quality and storage\nefficiency compared to state-of-the-art methods. Project page:\nhttps://distilled3dgs.github.io . Code:\nhttps://github.com/lt-xiang/Distilled-3DGS .\n", "link": "http://arxiv.org/abs/2508.14037v1", "date": "2025-08-19", "relevancy": 3.3019, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6782}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6607}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distilled-3DGS%3ADistilled%203D%20Gaussian%20Splatting&body=Title%3A%20Distilled-3DGS%3ADistilled%203D%20Gaussian%20Splatting%0AAuthor%3A%20Lintao%20Xiang%20and%20Xinkai%20Chen%20and%20Jianhuang%20Lai%20and%20Guangcong%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20exhibited%20remarkable%20efficacy%20in%20novel%20view%0Asynthesis%20%28NVS%29.%20However%2C%20it%20suffers%20from%20a%20significant%20drawback%3A%20achieving%0Ahigh-fidelity%20rendering%20typically%20necessitates%20a%20large%20number%20of%203D%20Gaussians%2C%0Aresulting%20in%20substantial%20memory%20consumption%20and%20storage%20requirements.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20the%20first%20knowledge%20distillation%20framework%0Afor%203DGS%2C%20featuring%20various%20teacher%20models%2C%20including%20vanilla%203DGS%2C%0Anoise-augmented%20variants%2C%20and%20dropout-regularized%20versions.%20The%20outputs%20of%0Athese%20teachers%20are%20aggregated%20to%20guide%20the%20optimization%20of%20a%20lightweight%0Astudent%20model.%20To%20distill%20the%20hidden%20geometric%20structure%2C%20we%20propose%20a%0Astructural%20similarity%20loss%20to%20boost%20the%20consistency%20of%20spatial%20geometric%0Adistributions%20between%20the%20student%20and%20teacher%20model.%20Through%20comprehensive%0Aquantitative%20and%20qualitative%20evaluations%20across%20diverse%20datasets%2C%20the%20proposed%0ADistilled-3DGS%2C%20a%20simple%20yet%20effective%20framework%20without%20bells%20and%20whistles%2C%0Aachieves%20promising%20rendering%20results%20in%20both%20rendering%20quality%20and%20storage%0Aefficiency%20compared%20to%20state-of-the-art%20methods.%20Project%20page%3A%0Ahttps%3A//distilled3dgs.github.io%20.%20Code%3A%0Ahttps%3A//github.com/lt-xiang/Distilled-3DGS%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilled-3DGS%253ADistilled%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DLintao%2520Xiang%2520and%2520Xinkai%2520Chen%2520and%2520Jianhuang%2520Lai%2520and%2520Guangcong%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520exhibited%2520remarkable%2520efficacy%2520in%2520novel%2520view%250Asynthesis%2520%2528NVS%2529.%2520However%252C%2520it%2520suffers%2520from%2520a%2520significant%2520drawback%253A%2520achieving%250Ahigh-fidelity%2520rendering%2520typically%2520necessitates%2520a%2520large%2520number%2520of%25203D%2520Gaussians%252C%250Aresulting%2520in%2520substantial%2520memory%2520consumption%2520and%2520storage%2520requirements.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520the%2520first%2520knowledge%2520distillation%2520framework%250Afor%25203DGS%252C%2520featuring%2520various%2520teacher%2520models%252C%2520including%2520vanilla%25203DGS%252C%250Anoise-augmented%2520variants%252C%2520and%2520dropout-regularized%2520versions.%2520The%2520outputs%2520of%250Athese%2520teachers%2520are%2520aggregated%2520to%2520guide%2520the%2520optimization%2520of%2520a%2520lightweight%250Astudent%2520model.%2520To%2520distill%2520the%2520hidden%2520geometric%2520structure%252C%2520we%2520propose%2520a%250Astructural%2520similarity%2520loss%2520to%2520boost%2520the%2520consistency%2520of%2520spatial%2520geometric%250Adistributions%2520between%2520the%2520student%2520and%2520teacher%2520model.%2520Through%2520comprehensive%250Aquantitative%2520and%2520qualitative%2520evaluations%2520across%2520diverse%2520datasets%252C%2520the%2520proposed%250ADistilled-3DGS%252C%2520a%2520simple%2520yet%2520effective%2520framework%2520without%2520bells%2520and%2520whistles%252C%250Aachieves%2520promising%2520rendering%2520results%2520in%2520both%2520rendering%2520quality%2520and%2520storage%250Aefficiency%2520compared%2520to%2520state-of-the-art%2520methods.%2520Project%2520page%253A%250Ahttps%253A//distilled3dgs.github.io%2520.%2520Code%253A%250Ahttps%253A//github.com/lt-xiang/Distilled-3DGS%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilled-3DGS%3ADistilled%203D%20Gaussian%20Splatting&entry.906535625=Lintao%20Xiang%20and%20Xinkai%20Chen%20and%20Jianhuang%20Lai%20and%20Guangcong%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20exhibited%20remarkable%20efficacy%20in%20novel%20view%0Asynthesis%20%28NVS%29.%20However%2C%20it%20suffers%20from%20a%20significant%20drawback%3A%20achieving%0Ahigh-fidelity%20rendering%20typically%20necessitates%20a%20large%20number%20of%203D%20Gaussians%2C%0Aresulting%20in%20substantial%20memory%20consumption%20and%20storage%20requirements.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20the%20first%20knowledge%20distillation%20framework%0Afor%203DGS%2C%20featuring%20various%20teacher%20models%2C%20including%20vanilla%203DGS%2C%0Anoise-augmented%20variants%2C%20and%20dropout-regularized%20versions.%20The%20outputs%20of%0Athese%20teachers%20are%20aggregated%20to%20guide%20the%20optimization%20of%20a%20lightweight%0Astudent%20model.%20To%20distill%20the%20hidden%20geometric%20structure%2C%20we%20propose%20a%0Astructural%20similarity%20loss%20to%20boost%20the%20consistency%20of%20spatial%20geometric%0Adistributions%20between%20the%20student%20and%20teacher%20model.%20Through%20comprehensive%0Aquantitative%20and%20qualitative%20evaluations%20across%20diverse%20datasets%2C%20the%20proposed%0ADistilled-3DGS%2C%20a%20simple%20yet%20effective%20framework%20without%20bells%20and%20whistles%2C%0Aachieves%20promising%20rendering%20results%20in%20both%20rendering%20quality%20and%20storage%0Aefficiency%20compared%20to%20state-of-the-art%20methods.%20Project%20page%3A%0Ahttps%3A//distilled3dgs.github.io%20.%20Code%3A%0Ahttps%3A//github.com/lt-xiang/Distilled-3DGS%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14037v1&entry.124074799=Read"},
{"title": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar\n  Relighting", "author": "Zeren Jiang and Shaofei Wang and Siyu Tang", "abstract": "  Creating relightable and animatable human avatars from monocular videos is a\nrising research topic with a range of applications, e.g. virtual reality,\nsports, and video games. Previous works utilize neural fields together with\nphysically based rendering (PBR), to estimate geometry and disentangle\nappearance properties of human avatars. However, one drawback of these methods\nis the slow rendering speed due to the expensive Monte Carlo ray tracing. To\ntackle this problem, we proposed to distill the knowledge from implicit neural\nfields (teacher) to explicit 2D Gaussian splatting (student) representation to\ntake advantage of the fast rasterization property of Gaussian splatting. To\navoid ray-tracing, we employ the split-sum approximation for PBR appearance. We\nalso propose novel part-wise ambient occlusion probes for shadow computation.\nShadow prediction is achieved by querying these probes only once per pixel,\nwhich paves the way for real-time relighting of avatars. These techniques\ncombined give high-quality relighting results with realistic shadow effects.\nOur experiments demonstrate that the proposed student model achieves comparable\nor even better relighting results with our teacher model while being 370 times\nfaster at inference time, achieving a 67 FPS rendering speed.\n", "link": "http://arxiv.org/abs/2504.10486v2", "date": "2025-08-19", "relevancy": 3.1294, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6359}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6359}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DNF-Avatar%3A%20Distilling%20Neural%20Fields%20for%20Real-time%20Animatable%20Avatar%0A%20%20Relighting&body=Title%3A%20DNF-Avatar%3A%20Distilling%20Neural%20Fields%20for%20Real-time%20Animatable%20Avatar%0A%20%20Relighting%0AAuthor%3A%20Zeren%20Jiang%20and%20Shaofei%20Wang%20and%20Siyu%20Tang%0AAbstract%3A%20%20%20Creating%20relightable%20and%20animatable%20human%20avatars%20from%20monocular%20videos%20is%20a%0Arising%20research%20topic%20with%20a%20range%20of%20applications%2C%20e.g.%20virtual%20reality%2C%0Asports%2C%20and%20video%20games.%20Previous%20works%20utilize%20neural%20fields%20together%20with%0Aphysically%20based%20rendering%20%28PBR%29%2C%20to%20estimate%20geometry%20and%20disentangle%0Aappearance%20properties%20of%20human%20avatars.%20However%2C%20one%20drawback%20of%20these%20methods%0Ais%20the%20slow%20rendering%20speed%20due%20to%20the%20expensive%20Monte%20Carlo%20ray%20tracing.%20To%0Atackle%20this%20problem%2C%20we%20proposed%20to%20distill%20the%20knowledge%20from%20implicit%20neural%0Afields%20%28teacher%29%20to%20explicit%202D%20Gaussian%20splatting%20%28student%29%20representation%20to%0Atake%20advantage%20of%20the%20fast%20rasterization%20property%20of%20Gaussian%20splatting.%20To%0Aavoid%20ray-tracing%2C%20we%20employ%20the%20split-sum%20approximation%20for%20PBR%20appearance.%20We%0Aalso%20propose%20novel%20part-wise%20ambient%20occlusion%20probes%20for%20shadow%20computation.%0AShadow%20prediction%20is%20achieved%20by%20querying%20these%20probes%20only%20once%20per%20pixel%2C%0Awhich%20paves%20the%20way%20for%20real-time%20relighting%20of%20avatars.%20These%20techniques%0Acombined%20give%20high-quality%20relighting%20results%20with%20realistic%20shadow%20effects.%0AOur%20experiments%20demonstrate%20that%20the%20proposed%20student%20model%20achieves%20comparable%0Aor%20even%20better%20relighting%20results%20with%20our%20teacher%20model%20while%20being%20370%20times%0Afaster%20at%20inference%20time%2C%20achieving%20a%2067%20FPS%20rendering%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10486v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDNF-Avatar%253A%2520Distilling%2520Neural%2520Fields%2520for%2520Real-time%2520Animatable%2520Avatar%250A%2520%2520Relighting%26entry.906535625%3DZeren%2520Jiang%2520and%2520Shaofei%2520Wang%2520and%2520Siyu%2520Tang%26entry.1292438233%3D%2520%2520Creating%2520relightable%2520and%2520animatable%2520human%2520avatars%2520from%2520monocular%2520videos%2520is%2520a%250Arising%2520research%2520topic%2520with%2520a%2520range%2520of%2520applications%252C%2520e.g.%2520virtual%2520reality%252C%250Asports%252C%2520and%2520video%2520games.%2520Previous%2520works%2520utilize%2520neural%2520fields%2520together%2520with%250Aphysically%2520based%2520rendering%2520%2528PBR%2529%252C%2520to%2520estimate%2520geometry%2520and%2520disentangle%250Aappearance%2520properties%2520of%2520human%2520avatars.%2520However%252C%2520one%2520drawback%2520of%2520these%2520methods%250Ais%2520the%2520slow%2520rendering%2520speed%2520due%2520to%2520the%2520expensive%2520Monte%2520Carlo%2520ray%2520tracing.%2520To%250Atackle%2520this%2520problem%252C%2520we%2520proposed%2520to%2520distill%2520the%2520knowledge%2520from%2520implicit%2520neural%250Afields%2520%2528teacher%2529%2520to%2520explicit%25202D%2520Gaussian%2520splatting%2520%2528student%2529%2520representation%2520to%250Atake%2520advantage%2520of%2520the%2520fast%2520rasterization%2520property%2520of%2520Gaussian%2520splatting.%2520To%250Aavoid%2520ray-tracing%252C%2520we%2520employ%2520the%2520split-sum%2520approximation%2520for%2520PBR%2520appearance.%2520We%250Aalso%2520propose%2520novel%2520part-wise%2520ambient%2520occlusion%2520probes%2520for%2520shadow%2520computation.%250AShadow%2520prediction%2520is%2520achieved%2520by%2520querying%2520these%2520probes%2520only%2520once%2520per%2520pixel%252C%250Awhich%2520paves%2520the%2520way%2520for%2520real-time%2520relighting%2520of%2520avatars.%2520These%2520techniques%250Acombined%2520give%2520high-quality%2520relighting%2520results%2520with%2520realistic%2520shadow%2520effects.%250AOur%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520student%2520model%2520achieves%2520comparable%250Aor%2520even%2520better%2520relighting%2520results%2520with%2520our%2520teacher%2520model%2520while%2520being%2520370%2520times%250Afaster%2520at%2520inference%2520time%252C%2520achieving%2520a%252067%2520FPS%2520rendering%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10486v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DNF-Avatar%3A%20Distilling%20Neural%20Fields%20for%20Real-time%20Animatable%20Avatar%0A%20%20Relighting&entry.906535625=Zeren%20Jiang%20and%20Shaofei%20Wang%20and%20Siyu%20Tang&entry.1292438233=%20%20Creating%20relightable%20and%20animatable%20human%20avatars%20from%20monocular%20videos%20is%20a%0Arising%20research%20topic%20with%20a%20range%20of%20applications%2C%20e.g.%20virtual%20reality%2C%0Asports%2C%20and%20video%20games.%20Previous%20works%20utilize%20neural%20fields%20together%20with%0Aphysically%20based%20rendering%20%28PBR%29%2C%20to%20estimate%20geometry%20and%20disentangle%0Aappearance%20properties%20of%20human%20avatars.%20However%2C%20one%20drawback%20of%20these%20methods%0Ais%20the%20slow%20rendering%20speed%20due%20to%20the%20expensive%20Monte%20Carlo%20ray%20tracing.%20To%0Atackle%20this%20problem%2C%20we%20proposed%20to%20distill%20the%20knowledge%20from%20implicit%20neural%0Afields%20%28teacher%29%20to%20explicit%202D%20Gaussian%20splatting%20%28student%29%20representation%20to%0Atake%20advantage%20of%20the%20fast%20rasterization%20property%20of%20Gaussian%20splatting.%20To%0Aavoid%20ray-tracing%2C%20we%20employ%20the%20split-sum%20approximation%20for%20PBR%20appearance.%20We%0Aalso%20propose%20novel%20part-wise%20ambient%20occlusion%20probes%20for%20shadow%20computation.%0AShadow%20prediction%20is%20achieved%20by%20querying%20these%20probes%20only%20once%20per%20pixel%2C%0Awhich%20paves%20the%20way%20for%20real-time%20relighting%20of%20avatars.%20These%20techniques%0Acombined%20give%20high-quality%20relighting%20results%20with%20realistic%20shadow%20effects.%0AOur%20experiments%20demonstrate%20that%20the%20proposed%20student%20model%20achieves%20comparable%0Aor%20even%20better%20relighting%20results%20with%20our%20teacher%20model%20while%20being%20370%20times%0Afaster%20at%20inference%20time%2C%20achieving%20a%2067%20FPS%20rendering%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10486v2&entry.124074799=Read"},
{"title": "Vector-Quantized Vision Foundation Models for Object-Centric Learning", "author": "Rongzhen Zhao and Vivienne Wang and Juho Kannala and Joni Pajarinen", "abstract": "  Object-Centric Learning (OCL) aggregates image or video feature maps into\nobject-level feature vectors, termed \\textit{slots}. It's self-supervision of\nreconstructing the input from slots struggles with complex object textures,\nthus Vision Foundation Model (VFM) representations are used as the aggregation\ninput and reconstruction target. Existing methods leverage VFM representations\nin diverse ways yet fail to fully exploit their potential. In response, we\npropose a unified architecture, Vector-Quantized VFMs for OCL (VQ-VFM-OCL, or\nVVO). The key to our unification is simply shared quantizing VFM\nrepresentations in OCL aggregation and decoding. Experiments show that across\ndifferent VFMs, aggregators and decoders, our VVO consistently outperforms\nbaselines in object discovery and recognition, as well as downstream visual\nprediction and reasoning. We also mathematically analyze why VFM\nrepresentations facilitate OCL aggregation and why their shared quantization as\nreconstruction targets strengthens OCL supervision. Our source code and model\ncheckpoints are available on https://github.com/Genera1Z/VQ-VFM-OCL.\n", "link": "http://arxiv.org/abs/2502.20263v5", "date": "2025-08-19", "relevancy": 3.0711, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6378}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6378}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vector-Quantized%20Vision%20Foundation%20Models%20for%20Object-Centric%20Learning&body=Title%3A%20Vector-Quantized%20Vision%20Foundation%20Models%20for%20Object-Centric%20Learning%0AAuthor%3A%20Rongzhen%20Zhao%20and%20Vivienne%20Wang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Object-Centric%20Learning%20%28OCL%29%20aggregates%20image%20or%20video%20feature%20maps%20into%0Aobject-level%20feature%20vectors%2C%20termed%20%5Ctextit%7Bslots%7D.%20It%27s%20self-supervision%20of%0Areconstructing%20the%20input%20from%20slots%20struggles%20with%20complex%20object%20textures%2C%0Athus%20Vision%20Foundation%20Model%20%28VFM%29%20representations%20are%20used%20as%20the%20aggregation%0Ainput%20and%20reconstruction%20target.%20Existing%20methods%20leverage%20VFM%20representations%0Ain%20diverse%20ways%20yet%20fail%20to%20fully%20exploit%20their%20potential.%20In%20response%2C%20we%0Apropose%20a%20unified%20architecture%2C%20Vector-Quantized%20VFMs%20for%20OCL%20%28VQ-VFM-OCL%2C%20or%0AVVO%29.%20The%20key%20to%20our%20unification%20is%20simply%20shared%20quantizing%20VFM%0Arepresentations%20in%20OCL%20aggregation%20and%20decoding.%20Experiments%20show%20that%20across%0Adifferent%20VFMs%2C%20aggregators%20and%20decoders%2C%20our%20VVO%20consistently%20outperforms%0Abaselines%20in%20object%20discovery%20and%20recognition%2C%20as%20well%20as%20downstream%20visual%0Aprediction%20and%20reasoning.%20We%20also%20mathematically%20analyze%20why%20VFM%0Arepresentations%20facilitate%20OCL%20aggregation%20and%20why%20their%20shared%20quantization%20as%0Areconstruction%20targets%20strengthens%20OCL%20supervision.%20Our%20source%20code%20and%20model%0Acheckpoints%20are%20available%20on%20https%3A//github.com/Genera1Z/VQ-VFM-OCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20263v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVector-Quantized%2520Vision%2520Foundation%2520Models%2520for%2520Object-Centric%2520Learning%26entry.906535625%3DRongzhen%2520Zhao%2520and%2520Vivienne%2520Wang%2520and%2520Juho%2520Kannala%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Object-Centric%2520Learning%2520%2528OCL%2529%2520aggregates%2520image%2520or%2520video%2520feature%2520maps%2520into%250Aobject-level%2520feature%2520vectors%252C%2520termed%2520%255Ctextit%257Bslots%257D.%2520It%2527s%2520self-supervision%2520of%250Areconstructing%2520the%2520input%2520from%2520slots%2520struggles%2520with%2520complex%2520object%2520textures%252C%250Athus%2520Vision%2520Foundation%2520Model%2520%2528VFM%2529%2520representations%2520are%2520used%2520as%2520the%2520aggregation%250Ainput%2520and%2520reconstruction%2520target.%2520Existing%2520methods%2520leverage%2520VFM%2520representations%250Ain%2520diverse%2520ways%2520yet%2520fail%2520to%2520fully%2520exploit%2520their%2520potential.%2520In%2520response%252C%2520we%250Apropose%2520a%2520unified%2520architecture%252C%2520Vector-Quantized%2520VFMs%2520for%2520OCL%2520%2528VQ-VFM-OCL%252C%2520or%250AVVO%2529.%2520The%2520key%2520to%2520our%2520unification%2520is%2520simply%2520shared%2520quantizing%2520VFM%250Arepresentations%2520in%2520OCL%2520aggregation%2520and%2520decoding.%2520Experiments%2520show%2520that%2520across%250Adifferent%2520VFMs%252C%2520aggregators%2520and%2520decoders%252C%2520our%2520VVO%2520consistently%2520outperforms%250Abaselines%2520in%2520object%2520discovery%2520and%2520recognition%252C%2520as%2520well%2520as%2520downstream%2520visual%250Aprediction%2520and%2520reasoning.%2520We%2520also%2520mathematically%2520analyze%2520why%2520VFM%250Arepresentations%2520facilitate%2520OCL%2520aggregation%2520and%2520why%2520their%2520shared%2520quantization%2520as%250Areconstruction%2520targets%2520strengthens%2520OCL%2520supervision.%2520Our%2520source%2520code%2520and%2520model%250Acheckpoints%2520are%2520available%2520on%2520https%253A//github.com/Genera1Z/VQ-VFM-OCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20263v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vector-Quantized%20Vision%20Foundation%20Models%20for%20Object-Centric%20Learning&entry.906535625=Rongzhen%20Zhao%20and%20Vivienne%20Wang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Object-Centric%20Learning%20%28OCL%29%20aggregates%20image%20or%20video%20feature%20maps%20into%0Aobject-level%20feature%20vectors%2C%20termed%20%5Ctextit%7Bslots%7D.%20It%27s%20self-supervision%20of%0Areconstructing%20the%20input%20from%20slots%20struggles%20with%20complex%20object%20textures%2C%0Athus%20Vision%20Foundation%20Model%20%28VFM%29%20representations%20are%20used%20as%20the%20aggregation%0Ainput%20and%20reconstruction%20target.%20Existing%20methods%20leverage%20VFM%20representations%0Ain%20diverse%20ways%20yet%20fail%20to%20fully%20exploit%20their%20potential.%20In%20response%2C%20we%0Apropose%20a%20unified%20architecture%2C%20Vector-Quantized%20VFMs%20for%20OCL%20%28VQ-VFM-OCL%2C%20or%0AVVO%29.%20The%20key%20to%20our%20unification%20is%20simply%20shared%20quantizing%20VFM%0Arepresentations%20in%20OCL%20aggregation%20and%20decoding.%20Experiments%20show%20that%20across%0Adifferent%20VFMs%2C%20aggregators%20and%20decoders%2C%20our%20VVO%20consistently%20outperforms%0Abaselines%20in%20object%20discovery%20and%20recognition%2C%20as%20well%20as%20downstream%20visual%0Aprediction%20and%20reasoning.%20We%20also%20mathematically%20analyze%20why%20VFM%0Arepresentations%20facilitate%20OCL%20aggregation%20and%20why%20their%20shared%20quantization%20as%0Areconstruction%20targets%20strengthens%20OCL%20supervision.%20Our%20source%20code%20and%20model%0Acheckpoints%20are%20available%20on%20https%3A//github.com/Genera1Z/VQ-VFM-OCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20263v5&entry.124074799=Read"},
{"title": "Blending 3D Geometry and Machine Learning for Multi-View Stereopsis", "author": "Vibhas Vats and Md. Alimoor Reza and David Crandall and Soon-heung Jung", "abstract": "  Traditional multi-view stereo (MVS) methods primarily depend on photometric\nand geometric consistency constraints. In contrast, modern learning-based\nalgorithms often rely on the plane sweep algorithm to infer 3D geometry,\napplying explicit geometric consistency (GC) checks only as a post-processing\nstep, with no impact on the learning process itself. In this work, we introduce\nGC MVSNet plus plus, a novel approach that actively enforces geometric\nconsistency of reference view depth maps across multiple source views (multi\nview) and at various scales (multi scale) during the learning phase (see Fig.\n1). This integrated GC check significantly accelerates the learning process by\ndirectly penalizing geometrically inconsistent pixels, effectively halving the\nnumber of training iterations compared to other MVS methods. Furthermore, we\nintroduce a densely connected cost regularization network with two distinct\nblock designs simple and feature dense optimized to harness dense feature\nconnections for enhanced regularization. Extensive experiments demonstrate that\nour approach achieves a new state of the art on the DTU and BlendedMVS datasets\nand secures second place on the Tanks and Temples benchmark. To our knowledge,\nGC MVSNet plus plus is the first method to enforce multi-view, multi-scale\nsupervised geometric consistency during learning. Our code is available.\n", "link": "http://arxiv.org/abs/2505.03470v3", "date": "2025-08-19", "relevancy": 3.004, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6152}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5965}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blending%203D%20Geometry%20and%20Machine%20Learning%20for%20Multi-View%20Stereopsis&body=Title%3A%20Blending%203D%20Geometry%20and%20Machine%20Learning%20for%20Multi-View%20Stereopsis%0AAuthor%3A%20Vibhas%20Vats%20and%20Md.%20Alimoor%20Reza%20and%20David%20Crandall%20and%20Soon-heung%20Jung%0AAbstract%3A%20%20%20Traditional%20multi-view%20stereo%20%28MVS%29%20methods%20primarily%20depend%20on%20photometric%0Aand%20geometric%20consistency%20constraints.%20In%20contrast%2C%20modern%20learning-based%0Aalgorithms%20often%20rely%20on%20the%20plane%20sweep%20algorithm%20to%20infer%203D%20geometry%2C%0Aapplying%20explicit%20geometric%20consistency%20%28GC%29%20checks%20only%20as%20a%20post-processing%0Astep%2C%20with%20no%20impact%20on%20the%20learning%20process%20itself.%20In%20this%20work%2C%20we%20introduce%0AGC%20MVSNet%20plus%20plus%2C%20a%20novel%20approach%20that%20actively%20enforces%20geometric%0Aconsistency%20of%20reference%20view%20depth%20maps%20across%20multiple%20source%20views%20%28multi%0Aview%29%20and%20at%20various%20scales%20%28multi%20scale%29%20during%20the%20learning%20phase%20%28see%20Fig.%0A1%29.%20This%20integrated%20GC%20check%20significantly%20accelerates%20the%20learning%20process%20by%0Adirectly%20penalizing%20geometrically%20inconsistent%20pixels%2C%20effectively%20halving%20the%0Anumber%20of%20training%20iterations%20compared%20to%20other%20MVS%20methods.%20Furthermore%2C%20we%0Aintroduce%20a%20densely%20connected%20cost%20regularization%20network%20with%20two%20distinct%0Ablock%20designs%20simple%20and%20feature%20dense%20optimized%20to%20harness%20dense%20feature%0Aconnections%20for%20enhanced%20regularization.%20Extensive%20experiments%20demonstrate%20that%0Aour%20approach%20achieves%20a%20new%20state%20of%20the%20art%20on%20the%20DTU%20and%20BlendedMVS%20datasets%0Aand%20secures%20second%20place%20on%20the%20Tanks%20and%20Temples%20benchmark.%20To%20our%20knowledge%2C%0AGC%20MVSNet%20plus%20plus%20is%20the%20first%20method%20to%20enforce%20multi-view%2C%20multi-scale%0Asupervised%20geometric%20consistency%20during%20learning.%20Our%20code%20is%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03470v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlending%25203D%2520Geometry%2520and%2520Machine%2520Learning%2520for%2520Multi-View%2520Stereopsis%26entry.906535625%3DVibhas%2520Vats%2520and%2520Md.%2520Alimoor%2520Reza%2520and%2520David%2520Crandall%2520and%2520Soon-heung%2520Jung%26entry.1292438233%3D%2520%2520Traditional%2520multi-view%2520stereo%2520%2528MVS%2529%2520methods%2520primarily%2520depend%2520on%2520photometric%250Aand%2520geometric%2520consistency%2520constraints.%2520In%2520contrast%252C%2520modern%2520learning-based%250Aalgorithms%2520often%2520rely%2520on%2520the%2520plane%2520sweep%2520algorithm%2520to%2520infer%25203D%2520geometry%252C%250Aapplying%2520explicit%2520geometric%2520consistency%2520%2528GC%2529%2520checks%2520only%2520as%2520a%2520post-processing%250Astep%252C%2520with%2520no%2520impact%2520on%2520the%2520learning%2520process%2520itself.%2520In%2520this%2520work%252C%2520we%2520introduce%250AGC%2520MVSNet%2520plus%2520plus%252C%2520a%2520novel%2520approach%2520that%2520actively%2520enforces%2520geometric%250Aconsistency%2520of%2520reference%2520view%2520depth%2520maps%2520across%2520multiple%2520source%2520views%2520%2528multi%250Aview%2529%2520and%2520at%2520various%2520scales%2520%2528multi%2520scale%2529%2520during%2520the%2520learning%2520phase%2520%2528see%2520Fig.%250A1%2529.%2520This%2520integrated%2520GC%2520check%2520significantly%2520accelerates%2520the%2520learning%2520process%2520by%250Adirectly%2520penalizing%2520geometrically%2520inconsistent%2520pixels%252C%2520effectively%2520halving%2520the%250Anumber%2520of%2520training%2520iterations%2520compared%2520to%2520other%2520MVS%2520methods.%2520Furthermore%252C%2520we%250Aintroduce%2520a%2520densely%2520connected%2520cost%2520regularization%2520network%2520with%2520two%2520distinct%250Ablock%2520designs%2520simple%2520and%2520feature%2520dense%2520optimized%2520to%2520harness%2520dense%2520feature%250Aconnections%2520for%2520enhanced%2520regularization.%2520Extensive%2520experiments%2520demonstrate%2520that%250Aour%2520approach%2520achieves%2520a%2520new%2520state%2520of%2520the%2520art%2520on%2520the%2520DTU%2520and%2520BlendedMVS%2520datasets%250Aand%2520secures%2520second%2520place%2520on%2520the%2520Tanks%2520and%2520Temples%2520benchmark.%2520To%2520our%2520knowledge%252C%250AGC%2520MVSNet%2520plus%2520plus%2520is%2520the%2520first%2520method%2520to%2520enforce%2520multi-view%252C%2520multi-scale%250Asupervised%2520geometric%2520consistency%2520during%2520learning.%2520Our%2520code%2520is%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03470v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blending%203D%20Geometry%20and%20Machine%20Learning%20for%20Multi-View%20Stereopsis&entry.906535625=Vibhas%20Vats%20and%20Md.%20Alimoor%20Reza%20and%20David%20Crandall%20and%20Soon-heung%20Jung&entry.1292438233=%20%20Traditional%20multi-view%20stereo%20%28MVS%29%20methods%20primarily%20depend%20on%20photometric%0Aand%20geometric%20consistency%20constraints.%20In%20contrast%2C%20modern%20learning-based%0Aalgorithms%20often%20rely%20on%20the%20plane%20sweep%20algorithm%20to%20infer%203D%20geometry%2C%0Aapplying%20explicit%20geometric%20consistency%20%28GC%29%20checks%20only%20as%20a%20post-processing%0Astep%2C%20with%20no%20impact%20on%20the%20learning%20process%20itself.%20In%20this%20work%2C%20we%20introduce%0AGC%20MVSNet%20plus%20plus%2C%20a%20novel%20approach%20that%20actively%20enforces%20geometric%0Aconsistency%20of%20reference%20view%20depth%20maps%20across%20multiple%20source%20views%20%28multi%0Aview%29%20and%20at%20various%20scales%20%28multi%20scale%29%20during%20the%20learning%20phase%20%28see%20Fig.%0A1%29.%20This%20integrated%20GC%20check%20significantly%20accelerates%20the%20learning%20process%20by%0Adirectly%20penalizing%20geometrically%20inconsistent%20pixels%2C%20effectively%20halving%20the%0Anumber%20of%20training%20iterations%20compared%20to%20other%20MVS%20methods.%20Furthermore%2C%20we%0Aintroduce%20a%20densely%20connected%20cost%20regularization%20network%20with%20two%20distinct%0Ablock%20designs%20simple%20and%20feature%20dense%20optimized%20to%20harness%20dense%20feature%0Aconnections%20for%20enhanced%20regularization.%20Extensive%20experiments%20demonstrate%20that%0Aour%20approach%20achieves%20a%20new%20state%20of%20the%20art%20on%20the%20DTU%20and%20BlendedMVS%20datasets%0Aand%20secures%20second%20place%20on%20the%20Tanks%20and%20Temples%20benchmark.%20To%20our%20knowledge%2C%0AGC%20MVSNet%20plus%20plus%20is%20the%20first%20method%20to%20enforce%20multi-view%2C%20multi-scale%0Asupervised%20geometric%20consistency%20during%20learning.%20Our%20code%20is%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03470v3&entry.124074799=Read"},
{"title": "Real-Time, Population-Based Reconstruction of 3D Bone Models via\n  Very-Low-Dose Protocols", "author": "Yiqun Lin and Haoran Sun and Yongqing Li and Rabia Aslam and Lung Fung Tse and Tiange Cheng and Chun Sing Chui and Wing Fung Yau and Victorine R. Le Meur and Meruyert Amangeldy and Kiho Cho and Yinyu Ye and James Zou and Wei Zhao and Xiaomeng Li", "abstract": "  Patient-specific bone models are essential for designing surgical guides and\npreoperative planning, as they enable the visualization of intricate anatomical\nstructures. However, traditional CT-based approaches for creating bone models\nare limited to preoperative use due to the low flexibility and high radiation\nexposure of CT and time-consuming manual delineation. Here, we introduce\nSemi-Supervised Reconstruction with Knowledge Distillation (SSR-KD), a fast and\naccurate AI framework to reconstruct high-quality bone models from biplanar\nX-rays in 30 seconds, with an average error under 1.0 mm, eliminating the\ndependence on CT and manual work. Additionally, high tibial osteotomy\nsimulation was performed by experts on reconstructed bone models, demonstrating\nthat bone models reconstructed from biplanar X-rays have comparable clinical\napplicability to those annotated from CT. Overall, our approach accelerates the\nprocess, reduces radiation exposure, enables intraoperative guidance, and\nsignificantly improves the practicality of bone models, offering transformative\napplications in orthopedics.\n", "link": "http://arxiv.org/abs/2508.13947v1", "date": "2025-08-19", "relevancy": 2.9268, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6006}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6006}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%2C%20Population-Based%20Reconstruction%20of%203D%20Bone%20Models%20via%0A%20%20Very-Low-Dose%20Protocols&body=Title%3A%20Real-Time%2C%20Population-Based%20Reconstruction%20of%203D%20Bone%20Models%20via%0A%20%20Very-Low-Dose%20Protocols%0AAuthor%3A%20Yiqun%20Lin%20and%20Haoran%20Sun%20and%20Yongqing%20Li%20and%20Rabia%20Aslam%20and%20Lung%20Fung%20Tse%20and%20Tiange%20Cheng%20and%20Chun%20Sing%20Chui%20and%20Wing%20Fung%20Yau%20and%20Victorine%20R.%20Le%20Meur%20and%20Meruyert%20Amangeldy%20and%20Kiho%20Cho%20and%20Yinyu%20Ye%20and%20James%20Zou%20and%20Wei%20Zhao%20and%20Xiaomeng%20Li%0AAbstract%3A%20%20%20Patient-specific%20bone%20models%20are%20essential%20for%20designing%20surgical%20guides%20and%0Apreoperative%20planning%2C%20as%20they%20enable%20the%20visualization%20of%20intricate%20anatomical%0Astructures.%20However%2C%20traditional%20CT-based%20approaches%20for%20creating%20bone%20models%0Aare%20limited%20to%20preoperative%20use%20due%20to%20the%20low%20flexibility%20and%20high%20radiation%0Aexposure%20of%20CT%20and%20time-consuming%20manual%20delineation.%20Here%2C%20we%20introduce%0ASemi-Supervised%20Reconstruction%20with%20Knowledge%20Distillation%20%28SSR-KD%29%2C%20a%20fast%20and%0Aaccurate%20AI%20framework%20to%20reconstruct%20high-quality%20bone%20models%20from%20biplanar%0AX-rays%20in%2030%20seconds%2C%20with%20an%20average%20error%20under%201.0%20mm%2C%20eliminating%20the%0Adependence%20on%20CT%20and%20manual%20work.%20Additionally%2C%20high%20tibial%20osteotomy%0Asimulation%20was%20performed%20by%20experts%20on%20reconstructed%20bone%20models%2C%20demonstrating%0Athat%20bone%20models%20reconstructed%20from%20biplanar%20X-rays%20have%20comparable%20clinical%0Aapplicability%20to%20those%20annotated%20from%20CT.%20Overall%2C%20our%20approach%20accelerates%20the%0Aprocess%2C%20reduces%20radiation%20exposure%2C%20enables%20intraoperative%20guidance%2C%20and%0Asignificantly%20improves%20the%20practicality%20of%20bone%20models%2C%20offering%20transformative%0Aapplications%20in%20orthopedics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%252C%2520Population-Based%2520Reconstruction%2520of%25203D%2520Bone%2520Models%2520via%250A%2520%2520Very-Low-Dose%2520Protocols%26entry.906535625%3DYiqun%2520Lin%2520and%2520Haoran%2520Sun%2520and%2520Yongqing%2520Li%2520and%2520Rabia%2520Aslam%2520and%2520Lung%2520Fung%2520Tse%2520and%2520Tiange%2520Cheng%2520and%2520Chun%2520Sing%2520Chui%2520and%2520Wing%2520Fung%2520Yau%2520and%2520Victorine%2520R.%2520Le%2520Meur%2520and%2520Meruyert%2520Amangeldy%2520and%2520Kiho%2520Cho%2520and%2520Yinyu%2520Ye%2520and%2520James%2520Zou%2520and%2520Wei%2520Zhao%2520and%2520Xiaomeng%2520Li%26entry.1292438233%3D%2520%2520Patient-specific%2520bone%2520models%2520are%2520essential%2520for%2520designing%2520surgical%2520guides%2520and%250Apreoperative%2520planning%252C%2520as%2520they%2520enable%2520the%2520visualization%2520of%2520intricate%2520anatomical%250Astructures.%2520However%252C%2520traditional%2520CT-based%2520approaches%2520for%2520creating%2520bone%2520models%250Aare%2520limited%2520to%2520preoperative%2520use%2520due%2520to%2520the%2520low%2520flexibility%2520and%2520high%2520radiation%250Aexposure%2520of%2520CT%2520and%2520time-consuming%2520manual%2520delineation.%2520Here%252C%2520we%2520introduce%250ASemi-Supervised%2520Reconstruction%2520with%2520Knowledge%2520Distillation%2520%2528SSR-KD%2529%252C%2520a%2520fast%2520and%250Aaccurate%2520AI%2520framework%2520to%2520reconstruct%2520high-quality%2520bone%2520models%2520from%2520biplanar%250AX-rays%2520in%252030%2520seconds%252C%2520with%2520an%2520average%2520error%2520under%25201.0%2520mm%252C%2520eliminating%2520the%250Adependence%2520on%2520CT%2520and%2520manual%2520work.%2520Additionally%252C%2520high%2520tibial%2520osteotomy%250Asimulation%2520was%2520performed%2520by%2520experts%2520on%2520reconstructed%2520bone%2520models%252C%2520demonstrating%250Athat%2520bone%2520models%2520reconstructed%2520from%2520biplanar%2520X-rays%2520have%2520comparable%2520clinical%250Aapplicability%2520to%2520those%2520annotated%2520from%2520CT.%2520Overall%252C%2520our%2520approach%2520accelerates%2520the%250Aprocess%252C%2520reduces%2520radiation%2520exposure%252C%2520enables%2520intraoperative%2520guidance%252C%2520and%250Asignificantly%2520improves%2520the%2520practicality%2520of%2520bone%2520models%252C%2520offering%2520transformative%250Aapplications%2520in%2520orthopedics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%2C%20Population-Based%20Reconstruction%20of%203D%20Bone%20Models%20via%0A%20%20Very-Low-Dose%20Protocols&entry.906535625=Yiqun%20Lin%20and%20Haoran%20Sun%20and%20Yongqing%20Li%20and%20Rabia%20Aslam%20and%20Lung%20Fung%20Tse%20and%20Tiange%20Cheng%20and%20Chun%20Sing%20Chui%20and%20Wing%20Fung%20Yau%20and%20Victorine%20R.%20Le%20Meur%20and%20Meruyert%20Amangeldy%20and%20Kiho%20Cho%20and%20Yinyu%20Ye%20and%20James%20Zou%20and%20Wei%20Zhao%20and%20Xiaomeng%20Li&entry.1292438233=%20%20Patient-specific%20bone%20models%20are%20essential%20for%20designing%20surgical%20guides%20and%0Apreoperative%20planning%2C%20as%20they%20enable%20the%20visualization%20of%20intricate%20anatomical%0Astructures.%20However%2C%20traditional%20CT-based%20approaches%20for%20creating%20bone%20models%0Aare%20limited%20to%20preoperative%20use%20due%20to%20the%20low%20flexibility%20and%20high%20radiation%0Aexposure%20of%20CT%20and%20time-consuming%20manual%20delineation.%20Here%2C%20we%20introduce%0ASemi-Supervised%20Reconstruction%20with%20Knowledge%20Distillation%20%28SSR-KD%29%2C%20a%20fast%20and%0Aaccurate%20AI%20framework%20to%20reconstruct%20high-quality%20bone%20models%20from%20biplanar%0AX-rays%20in%2030%20seconds%2C%20with%20an%20average%20error%20under%201.0%20mm%2C%20eliminating%20the%0Adependence%20on%20CT%20and%20manual%20work.%20Additionally%2C%20high%20tibial%20osteotomy%0Asimulation%20was%20performed%20by%20experts%20on%20reconstructed%20bone%20models%2C%20demonstrating%0Athat%20bone%20models%20reconstructed%20from%20biplanar%20X-rays%20have%20comparable%20clinical%0Aapplicability%20to%20those%20annotated%20from%20CT.%20Overall%2C%20our%20approach%20accelerates%20the%0Aprocess%2C%20reduces%20radiation%20exposure%2C%20enables%20intraoperative%20guidance%2C%20and%0Asignificantly%20improves%20the%20practicality%20of%20bone%20models%2C%20offering%20transformative%0Aapplications%20in%20orthopedics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13947v1&entry.124074799=Read"},
{"title": "Self-Aware Adaptive Alignment: Enabling Accurate Perception for\n  Intelligent Transportation Systems", "author": "Tong Xiang and Hongxia Zhao and Fenghua Zhu and Yuanyuan Chen and Yisheng Lv", "abstract": "  Achieving top-notch performance in Intelligent Transportation detection is a\ncritical research area. However, many challenges still need to be addressed\nwhen it comes to detecting in a cross-domain scenario. In this paper, we\npropose a Self-Aware Adaptive Alignment (SA3), by leveraging an efficient\nalignment mechanism and recognition strategy. Our proposed method employs a\nspecified attention-based alignment module trained on source and target domain\ndatasets to guide the image-level features alignment process, enabling the\nlocal-global adaptive alignment between the source domain and target domain.\nFeatures from both domains, whose channel importance is re-weighted, are fed\ninto the region proposal network, which facilitates the acquisition of salient\nregion features. Also, we introduce an instance-to-image level alignment module\nspecific to the target domain to adaptively mitigate the domain gap. To\nevaluate the proposed method, extensive experiments have been conducted on\npopular cross-domain object detection benchmarks. Experimental results show\nthat SA3 achieves superior results to the previous state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2508.13823v1", "date": "2025-08-19", "relevancy": 2.8633, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5869}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5848}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Aware%20Adaptive%20Alignment%3A%20Enabling%20Accurate%20Perception%20for%0A%20%20Intelligent%20Transportation%20Systems&body=Title%3A%20Self-Aware%20Adaptive%20Alignment%3A%20Enabling%20Accurate%20Perception%20for%0A%20%20Intelligent%20Transportation%20Systems%0AAuthor%3A%20Tong%20Xiang%20and%20Hongxia%20Zhao%20and%20Fenghua%20Zhu%20and%20Yuanyuan%20Chen%20and%20Yisheng%20Lv%0AAbstract%3A%20%20%20Achieving%20top-notch%20performance%20in%20Intelligent%20Transportation%20detection%20is%20a%0Acritical%20research%20area.%20However%2C%20many%20challenges%20still%20need%20to%20be%20addressed%0Awhen%20it%20comes%20to%20detecting%20in%20a%20cross-domain%20scenario.%20In%20this%20paper%2C%20we%0Apropose%20a%20Self-Aware%20Adaptive%20Alignment%20%28SA3%29%2C%20by%20leveraging%20an%20efficient%0Aalignment%20mechanism%20and%20recognition%20strategy.%20Our%20proposed%20method%20employs%20a%0Aspecified%20attention-based%20alignment%20module%20trained%20on%20source%20and%20target%20domain%0Adatasets%20to%20guide%20the%20image-level%20features%20alignment%20process%2C%20enabling%20the%0Alocal-global%20adaptive%20alignment%20between%20the%20source%20domain%20and%20target%20domain.%0AFeatures%20from%20both%20domains%2C%20whose%20channel%20importance%20is%20re-weighted%2C%20are%20fed%0Ainto%20the%20region%20proposal%20network%2C%20which%20facilitates%20the%20acquisition%20of%20salient%0Aregion%20features.%20Also%2C%20we%20introduce%20an%20instance-to-image%20level%20alignment%20module%0Aspecific%20to%20the%20target%20domain%20to%20adaptively%20mitigate%20the%20domain%20gap.%20To%0Aevaluate%20the%20proposed%20method%2C%20extensive%20experiments%20have%20been%20conducted%20on%0Apopular%20cross-domain%20object%20detection%20benchmarks.%20Experimental%20results%20show%0Athat%20SA3%20achieves%20superior%20results%20to%20the%20previous%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Aware%2520Adaptive%2520Alignment%253A%2520Enabling%2520Accurate%2520Perception%2520for%250A%2520%2520Intelligent%2520Transportation%2520Systems%26entry.906535625%3DTong%2520Xiang%2520and%2520Hongxia%2520Zhao%2520and%2520Fenghua%2520Zhu%2520and%2520Yuanyuan%2520Chen%2520and%2520Yisheng%2520Lv%26entry.1292438233%3D%2520%2520Achieving%2520top-notch%2520performance%2520in%2520Intelligent%2520Transportation%2520detection%2520is%2520a%250Acritical%2520research%2520area.%2520However%252C%2520many%2520challenges%2520still%2520need%2520to%2520be%2520addressed%250Awhen%2520it%2520comes%2520to%2520detecting%2520in%2520a%2520cross-domain%2520scenario.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520Self-Aware%2520Adaptive%2520Alignment%2520%2528SA3%2529%252C%2520by%2520leveraging%2520an%2520efficient%250Aalignment%2520mechanism%2520and%2520recognition%2520strategy.%2520Our%2520proposed%2520method%2520employs%2520a%250Aspecified%2520attention-based%2520alignment%2520module%2520trained%2520on%2520source%2520and%2520target%2520domain%250Adatasets%2520to%2520guide%2520the%2520image-level%2520features%2520alignment%2520process%252C%2520enabling%2520the%250Alocal-global%2520adaptive%2520alignment%2520between%2520the%2520source%2520domain%2520and%2520target%2520domain.%250AFeatures%2520from%2520both%2520domains%252C%2520whose%2520channel%2520importance%2520is%2520re-weighted%252C%2520are%2520fed%250Ainto%2520the%2520region%2520proposal%2520network%252C%2520which%2520facilitates%2520the%2520acquisition%2520of%2520salient%250Aregion%2520features.%2520Also%252C%2520we%2520introduce%2520an%2520instance-to-image%2520level%2520alignment%2520module%250Aspecific%2520to%2520the%2520target%2520domain%2520to%2520adaptively%2520mitigate%2520the%2520domain%2520gap.%2520To%250Aevaluate%2520the%2520proposed%2520method%252C%2520extensive%2520experiments%2520have%2520been%2520conducted%2520on%250Apopular%2520cross-domain%2520object%2520detection%2520benchmarks.%2520Experimental%2520results%2520show%250Athat%2520SA3%2520achieves%2520superior%2520results%2520to%2520the%2520previous%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Aware%20Adaptive%20Alignment%3A%20Enabling%20Accurate%20Perception%20for%0A%20%20Intelligent%20Transportation%20Systems&entry.906535625=Tong%20Xiang%20and%20Hongxia%20Zhao%20and%20Fenghua%20Zhu%20and%20Yuanyuan%20Chen%20and%20Yisheng%20Lv&entry.1292438233=%20%20Achieving%20top-notch%20performance%20in%20Intelligent%20Transportation%20detection%20is%20a%0Acritical%20research%20area.%20However%2C%20many%20challenges%20still%20need%20to%20be%20addressed%0Awhen%20it%20comes%20to%20detecting%20in%20a%20cross-domain%20scenario.%20In%20this%20paper%2C%20we%0Apropose%20a%20Self-Aware%20Adaptive%20Alignment%20%28SA3%29%2C%20by%20leveraging%20an%20efficient%0Aalignment%20mechanism%20and%20recognition%20strategy.%20Our%20proposed%20method%20employs%20a%0Aspecified%20attention-based%20alignment%20module%20trained%20on%20source%20and%20target%20domain%0Adatasets%20to%20guide%20the%20image-level%20features%20alignment%20process%2C%20enabling%20the%0Alocal-global%20adaptive%20alignment%20between%20the%20source%20domain%20and%20target%20domain.%0AFeatures%20from%20both%20domains%2C%20whose%20channel%20importance%20is%20re-weighted%2C%20are%20fed%0Ainto%20the%20region%20proposal%20network%2C%20which%20facilitates%20the%20acquisition%20of%20salient%0Aregion%20features.%20Also%2C%20we%20introduce%20an%20instance-to-image%20level%20alignment%20module%0Aspecific%20to%20the%20target%20domain%20to%20adaptively%20mitigate%20the%20domain%20gap.%20To%0Aevaluate%20the%20proposed%20method%2C%20extensive%20experiments%20have%20been%20conducted%20on%0Apopular%20cross-domain%20object%20detection%20benchmarks.%20Experimental%20results%20show%0Athat%20SA3%20achieves%20superior%20results%20to%20the%20previous%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13823v1&entry.124074799=Read"},
{"title": "GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation", "author": "Ken Deng and Yunhan Yang and Jingxiang Sun and Xihui Liu and Yebin Liu and Ding Liang and Yan-Pei Cao", "abstract": "  Modern 3D generation methods can rapidly create shapes from sparse or single\nviews, but their outputs often lack geometric detail due to computational\nconstraints. We present DetailGen3D, a generative approach specifically\ndesigned to enhance these generated 3D shapes. Our key insight is to model the\ncoarse-to-fine transformation directly through data-dependent flows in latent\nspace, avoiding the computational overhead of large-scale 3D generative models.\nWe introduce a token matching strategy that ensures accurate spatial\ncorrespondence during refinement, enabling local detail synthesis while\npreserving global structure. By carefully designing our training data to match\nthe characteristics of synthesized coarse shapes, our method can effectively\nenhance shapes produced by various 3D generation and reconstruction approaches,\nfrom single-view to sparse multi-view inputs. Extensive experiments demonstrate\nthat DetailGen3D achieves high-fidelity geometric detail synthesis while\nmaintaining efficiency in training.\n", "link": "http://arxiv.org/abs/2508.14036v1", "date": "2025-08-19", "relevancy": 2.8269, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5727}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5658}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoSAM2%3A%20Unleashing%20the%20Power%20of%20SAM2%20for%203D%20Part%20Segmentation&body=Title%3A%20GeoSAM2%3A%20Unleashing%20the%20Power%20of%20SAM2%20for%203D%20Part%20Segmentation%0AAuthor%3A%20Ken%20Deng%20and%20Yunhan%20Yang%20and%20Jingxiang%20Sun%20and%20Xihui%20Liu%20and%20Yebin%20Liu%20and%20Ding%20Liang%20and%20Yan-Pei%20Cao%0AAbstract%3A%20%20%20Modern%203D%20generation%20methods%20can%20rapidly%20create%20shapes%20from%20sparse%20or%20single%0Aviews%2C%20but%20their%20outputs%20often%20lack%20geometric%20detail%20due%20to%20computational%0Aconstraints.%20We%20present%20DetailGen3D%2C%20a%20generative%20approach%20specifically%0Adesigned%20to%20enhance%20these%20generated%203D%20shapes.%20Our%20key%20insight%20is%20to%20model%20the%0Acoarse-to-fine%20transformation%20directly%20through%20data-dependent%20flows%20in%20latent%0Aspace%2C%20avoiding%20the%20computational%20overhead%20of%20large-scale%203D%20generative%20models.%0AWe%20introduce%20a%20token%20matching%20strategy%20that%20ensures%20accurate%20spatial%0Acorrespondence%20during%20refinement%2C%20enabling%20local%20detail%20synthesis%20while%0Apreserving%20global%20structure.%20By%20carefully%20designing%20our%20training%20data%20to%20match%0Athe%20characteristics%20of%20synthesized%20coarse%20shapes%2C%20our%20method%20can%20effectively%0Aenhance%20shapes%20produced%20by%20various%203D%20generation%20and%20reconstruction%20approaches%2C%0Afrom%20single-view%20to%20sparse%20multi-view%20inputs.%20Extensive%20experiments%20demonstrate%0Athat%20DetailGen3D%20achieves%20high-fidelity%20geometric%20detail%20synthesis%20while%0Amaintaining%20efficiency%20in%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14036v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoSAM2%253A%2520Unleashing%2520the%2520Power%2520of%2520SAM2%2520for%25203D%2520Part%2520Segmentation%26entry.906535625%3DKen%2520Deng%2520and%2520Yunhan%2520Yang%2520and%2520Jingxiang%2520Sun%2520and%2520Xihui%2520Liu%2520and%2520Yebin%2520Liu%2520and%2520Ding%2520Liang%2520and%2520Yan-Pei%2520Cao%26entry.1292438233%3D%2520%2520Modern%25203D%2520generation%2520methods%2520can%2520rapidly%2520create%2520shapes%2520from%2520sparse%2520or%2520single%250Aviews%252C%2520but%2520their%2520outputs%2520often%2520lack%2520geometric%2520detail%2520due%2520to%2520computational%250Aconstraints.%2520We%2520present%2520DetailGen3D%252C%2520a%2520generative%2520approach%2520specifically%250Adesigned%2520to%2520enhance%2520these%2520generated%25203D%2520shapes.%2520Our%2520key%2520insight%2520is%2520to%2520model%2520the%250Acoarse-to-fine%2520transformation%2520directly%2520through%2520data-dependent%2520flows%2520in%2520latent%250Aspace%252C%2520avoiding%2520the%2520computational%2520overhead%2520of%2520large-scale%25203D%2520generative%2520models.%250AWe%2520introduce%2520a%2520token%2520matching%2520strategy%2520that%2520ensures%2520accurate%2520spatial%250Acorrespondence%2520during%2520refinement%252C%2520enabling%2520local%2520detail%2520synthesis%2520while%250Apreserving%2520global%2520structure.%2520By%2520carefully%2520designing%2520our%2520training%2520data%2520to%2520match%250Athe%2520characteristics%2520of%2520synthesized%2520coarse%2520shapes%252C%2520our%2520method%2520can%2520effectively%250Aenhance%2520shapes%2520produced%2520by%2520various%25203D%2520generation%2520and%2520reconstruction%2520approaches%252C%250Afrom%2520single-view%2520to%2520sparse%2520multi-view%2520inputs.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520DetailGen3D%2520achieves%2520high-fidelity%2520geometric%2520detail%2520synthesis%2520while%250Amaintaining%2520efficiency%2520in%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14036v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoSAM2%3A%20Unleashing%20the%20Power%20of%20SAM2%20for%203D%20Part%20Segmentation&entry.906535625=Ken%20Deng%20and%20Yunhan%20Yang%20and%20Jingxiang%20Sun%20and%20Xihui%20Liu%20and%20Yebin%20Liu%20and%20Ding%20Liang%20and%20Yan-Pei%20Cao&entry.1292438233=%20%20Modern%203D%20generation%20methods%20can%20rapidly%20create%20shapes%20from%20sparse%20or%20single%0Aviews%2C%20but%20their%20outputs%20often%20lack%20geometric%20detail%20due%20to%20computational%0Aconstraints.%20We%20present%20DetailGen3D%2C%20a%20generative%20approach%20specifically%0Adesigned%20to%20enhance%20these%20generated%203D%20shapes.%20Our%20key%20insight%20is%20to%20model%20the%0Acoarse-to-fine%20transformation%20directly%20through%20data-dependent%20flows%20in%20latent%0Aspace%2C%20avoiding%20the%20computational%20overhead%20of%20large-scale%203D%20generative%20models.%0AWe%20introduce%20a%20token%20matching%20strategy%20that%20ensures%20accurate%20spatial%0Acorrespondence%20during%20refinement%2C%20enabling%20local%20detail%20synthesis%20while%0Apreserving%20global%20structure.%20By%20carefully%20designing%20our%20training%20data%20to%20match%0Athe%20characteristics%20of%20synthesized%20coarse%20shapes%2C%20our%20method%20can%20effectively%0Aenhance%20shapes%20produced%20by%20various%203D%20generation%20and%20reconstruction%20approaches%2C%0Afrom%20single-view%20to%20sparse%20multi-view%20inputs.%20Extensive%20experiments%20demonstrate%0Athat%20DetailGen3D%20achieves%20high-fidelity%20geometric%20detail%20synthesis%20while%0Amaintaining%20efficiency%20in%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14036v1&entry.124074799=Read"},
{"title": "Hierarchical Vision-Language Retrieval of Educational Metaverse Content\n  in Agriculture", "author": "Ali Abdari and Alex Falcon and Giuseppe Serra", "abstract": "  Every day, a large amount of educational content is uploaded online across\ndifferent areas, including agriculture and gardening. When these videos or\nmaterials are grouped meaningfully, they can make learning easier and more\neffective. One promising way to organize and enrich such content is through the\nMetaverse, which allows users to explore educational experiences in an\ninteractive and immersive environment. However, searching for relevant\nMetaverse scenarios and finding those matching users' interests remains a\nchallenging task. A first step in this direction has been done recently, but\nexisting datasets are small and not sufficient for training advanced models. In\nthis work, we make two main contributions: first, we introduce a new dataset\ncontaining 457 agricultural-themed virtual museums (AgriMuseums), each enriched\nwith textual descriptions; and second, we propose a hierarchical\nvision-language model to represent and retrieve relevant AgriMuseums using\nnatural language queries. In our experimental setting, the proposed method\nachieves up to about 62\\% R@1 and 78\\% MRR, confirming its effectiveness, and\nit also leads to improvements on existing benchmarks by up to 6\\% R@1 and 11\\%\nMRR. Moreover, an extensive evaluation validates our design choices. Code and\ndataset are available at\nhttps://github.com/aliabdari/Agricultural_Metaverse_Retrieval .\n", "link": "http://arxiv.org/abs/2508.13713v1", "date": "2025-08-19", "relevancy": 2.786, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5701}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5701}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Vision-Language%20Retrieval%20of%20Educational%20Metaverse%20Content%0A%20%20in%20Agriculture&body=Title%3A%20Hierarchical%20Vision-Language%20Retrieval%20of%20Educational%20Metaverse%20Content%0A%20%20in%20Agriculture%0AAuthor%3A%20Ali%20Abdari%20and%20Alex%20Falcon%20and%20Giuseppe%20Serra%0AAbstract%3A%20%20%20Every%20day%2C%20a%20large%20amount%20of%20educational%20content%20is%20uploaded%20online%20across%0Adifferent%20areas%2C%20including%20agriculture%20and%20gardening.%20When%20these%20videos%20or%0Amaterials%20are%20grouped%20meaningfully%2C%20they%20can%20make%20learning%20easier%20and%20more%0Aeffective.%20One%20promising%20way%20to%20organize%20and%20enrich%20such%20content%20is%20through%20the%0AMetaverse%2C%20which%20allows%20users%20to%20explore%20educational%20experiences%20in%20an%0Ainteractive%20and%20immersive%20environment.%20However%2C%20searching%20for%20relevant%0AMetaverse%20scenarios%20and%20finding%20those%20matching%20users%27%20interests%20remains%20a%0Achallenging%20task.%20A%20first%20step%20in%20this%20direction%20has%20been%20done%20recently%2C%20but%0Aexisting%20datasets%20are%20small%20and%20not%20sufficient%20for%20training%20advanced%20models.%20In%0Athis%20work%2C%20we%20make%20two%20main%20contributions%3A%20first%2C%20we%20introduce%20a%20new%20dataset%0Acontaining%20457%20agricultural-themed%20virtual%20museums%20%28AgriMuseums%29%2C%20each%20enriched%0Awith%20textual%20descriptions%3B%20and%20second%2C%20we%20propose%20a%20hierarchical%0Avision-language%20model%20to%20represent%20and%20retrieve%20relevant%20AgriMuseums%20using%0Anatural%20language%20queries.%20In%20our%20experimental%20setting%2C%20the%20proposed%20method%0Aachieves%20up%20to%20about%2062%5C%25%20R%401%20and%2078%5C%25%20MRR%2C%20confirming%20its%20effectiveness%2C%20and%0Ait%20also%20leads%20to%20improvements%20on%20existing%20benchmarks%20by%20up%20to%206%5C%25%20R%401%20and%2011%5C%25%0AMRR.%20Moreover%2C%20an%20extensive%20evaluation%20validates%20our%20design%20choices.%20Code%20and%0Adataset%20are%20available%20at%0Ahttps%3A//github.com/aliabdari/Agricultural_Metaverse_Retrieval%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Vision-Language%2520Retrieval%2520of%2520Educational%2520Metaverse%2520Content%250A%2520%2520in%2520Agriculture%26entry.906535625%3DAli%2520Abdari%2520and%2520Alex%2520Falcon%2520and%2520Giuseppe%2520Serra%26entry.1292438233%3D%2520%2520Every%2520day%252C%2520a%2520large%2520amount%2520of%2520educational%2520content%2520is%2520uploaded%2520online%2520across%250Adifferent%2520areas%252C%2520including%2520agriculture%2520and%2520gardening.%2520When%2520these%2520videos%2520or%250Amaterials%2520are%2520grouped%2520meaningfully%252C%2520they%2520can%2520make%2520learning%2520easier%2520and%2520more%250Aeffective.%2520One%2520promising%2520way%2520to%2520organize%2520and%2520enrich%2520such%2520content%2520is%2520through%2520the%250AMetaverse%252C%2520which%2520allows%2520users%2520to%2520explore%2520educational%2520experiences%2520in%2520an%250Ainteractive%2520and%2520immersive%2520environment.%2520However%252C%2520searching%2520for%2520relevant%250AMetaverse%2520scenarios%2520and%2520finding%2520those%2520matching%2520users%2527%2520interests%2520remains%2520a%250Achallenging%2520task.%2520A%2520first%2520step%2520in%2520this%2520direction%2520has%2520been%2520done%2520recently%252C%2520but%250Aexisting%2520datasets%2520are%2520small%2520and%2520not%2520sufficient%2520for%2520training%2520advanced%2520models.%2520In%250Athis%2520work%252C%2520we%2520make%2520two%2520main%2520contributions%253A%2520first%252C%2520we%2520introduce%2520a%2520new%2520dataset%250Acontaining%2520457%2520agricultural-themed%2520virtual%2520museums%2520%2528AgriMuseums%2529%252C%2520each%2520enriched%250Awith%2520textual%2520descriptions%253B%2520and%2520second%252C%2520we%2520propose%2520a%2520hierarchical%250Avision-language%2520model%2520to%2520represent%2520and%2520retrieve%2520relevant%2520AgriMuseums%2520using%250Anatural%2520language%2520queries.%2520In%2520our%2520experimental%2520setting%252C%2520the%2520proposed%2520method%250Aachieves%2520up%2520to%2520about%252062%255C%2525%2520R%25401%2520and%252078%255C%2525%2520MRR%252C%2520confirming%2520its%2520effectiveness%252C%2520and%250Ait%2520also%2520leads%2520to%2520improvements%2520on%2520existing%2520benchmarks%2520by%2520up%2520to%25206%255C%2525%2520R%25401%2520and%252011%255C%2525%250AMRR.%2520Moreover%252C%2520an%2520extensive%2520evaluation%2520validates%2520our%2520design%2520choices.%2520Code%2520and%250Adataset%2520are%2520available%2520at%250Ahttps%253A//github.com/aliabdari/Agricultural_Metaverse_Retrieval%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Vision-Language%20Retrieval%20of%20Educational%20Metaverse%20Content%0A%20%20in%20Agriculture&entry.906535625=Ali%20Abdari%20and%20Alex%20Falcon%20and%20Giuseppe%20Serra&entry.1292438233=%20%20Every%20day%2C%20a%20large%20amount%20of%20educational%20content%20is%20uploaded%20online%20across%0Adifferent%20areas%2C%20including%20agriculture%20and%20gardening.%20When%20these%20videos%20or%0Amaterials%20are%20grouped%20meaningfully%2C%20they%20can%20make%20learning%20easier%20and%20more%0Aeffective.%20One%20promising%20way%20to%20organize%20and%20enrich%20such%20content%20is%20through%20the%0AMetaverse%2C%20which%20allows%20users%20to%20explore%20educational%20experiences%20in%20an%0Ainteractive%20and%20immersive%20environment.%20However%2C%20searching%20for%20relevant%0AMetaverse%20scenarios%20and%20finding%20those%20matching%20users%27%20interests%20remains%20a%0Achallenging%20task.%20A%20first%20step%20in%20this%20direction%20has%20been%20done%20recently%2C%20but%0Aexisting%20datasets%20are%20small%20and%20not%20sufficient%20for%20training%20advanced%20models.%20In%0Athis%20work%2C%20we%20make%20two%20main%20contributions%3A%20first%2C%20we%20introduce%20a%20new%20dataset%0Acontaining%20457%20agricultural-themed%20virtual%20museums%20%28AgriMuseums%29%2C%20each%20enriched%0Awith%20textual%20descriptions%3B%20and%20second%2C%20we%20propose%20a%20hierarchical%0Avision-language%20model%20to%20represent%20and%20retrieve%20relevant%20AgriMuseums%20using%0Anatural%20language%20queries.%20In%20our%20experimental%20setting%2C%20the%20proposed%20method%0Aachieves%20up%20to%20about%2062%5C%25%20R%401%20and%2078%5C%25%20MRR%2C%20confirming%20its%20effectiveness%2C%20and%0Ait%20also%20leads%20to%20improvements%20on%20existing%20benchmarks%20by%20up%20to%206%5C%25%20R%401%20and%2011%5C%25%0AMRR.%20Moreover%2C%20an%20extensive%20evaluation%20validates%20our%20design%20choices.%20Code%20and%0Adataset%20are%20available%20at%0Ahttps%3A//github.com/aliabdari/Agricultural_Metaverse_Retrieval%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13713v1&entry.124074799=Read"},
{"title": "UNICON: UNIfied CONtinual Learning for Medical Foundational Models", "author": "Mohammad Areeb Qazi and Munachiso S Nwadike and Ibrahim Almakky and Mohammad Yaqub and Numan Saeed", "abstract": "  Foundational models are trained on extensive datasets to capture the general\ntrends of a domain. However, in medical imaging, the scarcity of data makes\npre-training for every domain, modality, or task challenging. Continual\nlearning offers a solution by fine-tuning a model sequentially on different\ndomains or tasks, enabling it to integrate new knowledge without requiring\nlarge datasets for each training phase. In this paper, we propose UNIfied\nCONtinual Learning for Medical Foundational Models (UNICON), a framework that\nenables the seamless adaptation of foundation models to diverse domains, tasks,\nand modalities. Unlike conventional adaptation methods that treat these changes\nin isolation, UNICON provides a unified, perpetually expandable framework.\nThrough careful integration, we show that foundation models can dynamically\nexpand across imaging modalities, anatomical regions, and clinical objectives\nwithout catastrophic forgetting or task interference. Empirically, we validate\nour approach by adapting a chest CT foundation model initially trained for\nclassification to a prognosis and segmentation task. Our results show improved\nperformance across both additional tasks. Furthermore, we continually\nincorporated PET scans and achieved a 5\\% improvement in Dice score compared to\nrespective baselines. These findings establish that foundation models are not\ninherently constrained to their initial training scope but can evolve, paving\nthe way toward generalist AI models for medical imaging.\n", "link": "http://arxiv.org/abs/2508.14024v1", "date": "2025-08-19", "relevancy": 2.7722, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5604}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5604}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UNICON%3A%20UNIfied%20CONtinual%20Learning%20for%20Medical%20Foundational%20Models&body=Title%3A%20UNICON%3A%20UNIfied%20CONtinual%20Learning%20for%20Medical%20Foundational%20Models%0AAuthor%3A%20Mohammad%20Areeb%20Qazi%20and%20Munachiso%20S%20Nwadike%20and%20Ibrahim%20Almakky%20and%20Mohammad%20Yaqub%20and%20Numan%20Saeed%0AAbstract%3A%20%20%20Foundational%20models%20are%20trained%20on%20extensive%20datasets%20to%20capture%20the%20general%0Atrends%20of%20a%20domain.%20However%2C%20in%20medical%20imaging%2C%20the%20scarcity%20of%20data%20makes%0Apre-training%20for%20every%20domain%2C%20modality%2C%20or%20task%20challenging.%20Continual%0Alearning%20offers%20a%20solution%20by%20fine-tuning%20a%20model%20sequentially%20on%20different%0Adomains%20or%20tasks%2C%20enabling%20it%20to%20integrate%20new%20knowledge%20without%20requiring%0Alarge%20datasets%20for%20each%20training%20phase.%20In%20this%20paper%2C%20we%20propose%20UNIfied%0ACONtinual%20Learning%20for%20Medical%20Foundational%20Models%20%28UNICON%29%2C%20a%20framework%20that%0Aenables%20the%20seamless%20adaptation%20of%20foundation%20models%20to%20diverse%20domains%2C%20tasks%2C%0Aand%20modalities.%20Unlike%20conventional%20adaptation%20methods%20that%20treat%20these%20changes%0Ain%20isolation%2C%20UNICON%20provides%20a%20unified%2C%20perpetually%20expandable%20framework.%0AThrough%20careful%20integration%2C%20we%20show%20that%20foundation%20models%20can%20dynamically%0Aexpand%20across%20imaging%20modalities%2C%20anatomical%20regions%2C%20and%20clinical%20objectives%0Awithout%20catastrophic%20forgetting%20or%20task%20interference.%20Empirically%2C%20we%20validate%0Aour%20approach%20by%20adapting%20a%20chest%20CT%20foundation%20model%20initially%20trained%20for%0Aclassification%20to%20a%20prognosis%20and%20segmentation%20task.%20Our%20results%20show%20improved%0Aperformance%20across%20both%20additional%20tasks.%20Furthermore%2C%20we%20continually%0Aincorporated%20PET%20scans%20and%20achieved%20a%205%5C%25%20improvement%20in%20Dice%20score%20compared%20to%0Arespective%20baselines.%20These%20findings%20establish%20that%20foundation%20models%20are%20not%0Ainherently%20constrained%20to%20their%20initial%20training%20scope%20but%20can%20evolve%2C%20paving%0Athe%20way%20toward%20generalist%20AI%20models%20for%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUNICON%253A%2520UNIfied%2520CONtinual%2520Learning%2520for%2520Medical%2520Foundational%2520Models%26entry.906535625%3DMohammad%2520Areeb%2520Qazi%2520and%2520Munachiso%2520S%2520Nwadike%2520and%2520Ibrahim%2520Almakky%2520and%2520Mohammad%2520Yaqub%2520and%2520Numan%2520Saeed%26entry.1292438233%3D%2520%2520Foundational%2520models%2520are%2520trained%2520on%2520extensive%2520datasets%2520to%2520capture%2520the%2520general%250Atrends%2520of%2520a%2520domain.%2520However%252C%2520in%2520medical%2520imaging%252C%2520the%2520scarcity%2520of%2520data%2520makes%250Apre-training%2520for%2520every%2520domain%252C%2520modality%252C%2520or%2520task%2520challenging.%2520Continual%250Alearning%2520offers%2520a%2520solution%2520by%2520fine-tuning%2520a%2520model%2520sequentially%2520on%2520different%250Adomains%2520or%2520tasks%252C%2520enabling%2520it%2520to%2520integrate%2520new%2520knowledge%2520without%2520requiring%250Alarge%2520datasets%2520for%2520each%2520training%2520phase.%2520In%2520this%2520paper%252C%2520we%2520propose%2520UNIfied%250ACONtinual%2520Learning%2520for%2520Medical%2520Foundational%2520Models%2520%2528UNICON%2529%252C%2520a%2520framework%2520that%250Aenables%2520the%2520seamless%2520adaptation%2520of%2520foundation%2520models%2520to%2520diverse%2520domains%252C%2520tasks%252C%250Aand%2520modalities.%2520Unlike%2520conventional%2520adaptation%2520methods%2520that%2520treat%2520these%2520changes%250Ain%2520isolation%252C%2520UNICON%2520provides%2520a%2520unified%252C%2520perpetually%2520expandable%2520framework.%250AThrough%2520careful%2520integration%252C%2520we%2520show%2520that%2520foundation%2520models%2520can%2520dynamically%250Aexpand%2520across%2520imaging%2520modalities%252C%2520anatomical%2520regions%252C%2520and%2520clinical%2520objectives%250Awithout%2520catastrophic%2520forgetting%2520or%2520task%2520interference.%2520Empirically%252C%2520we%2520validate%250Aour%2520approach%2520by%2520adapting%2520a%2520chest%2520CT%2520foundation%2520model%2520initially%2520trained%2520for%250Aclassification%2520to%2520a%2520prognosis%2520and%2520segmentation%2520task.%2520Our%2520results%2520show%2520improved%250Aperformance%2520across%2520both%2520additional%2520tasks.%2520Furthermore%252C%2520we%2520continually%250Aincorporated%2520PET%2520scans%2520and%2520achieved%2520a%25205%255C%2525%2520improvement%2520in%2520Dice%2520score%2520compared%2520to%250Arespective%2520baselines.%2520These%2520findings%2520establish%2520that%2520foundation%2520models%2520are%2520not%250Ainherently%2520constrained%2520to%2520their%2520initial%2520training%2520scope%2520but%2520can%2520evolve%252C%2520paving%250Athe%2520way%2520toward%2520generalist%2520AI%2520models%2520for%2520medical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UNICON%3A%20UNIfied%20CONtinual%20Learning%20for%20Medical%20Foundational%20Models&entry.906535625=Mohammad%20Areeb%20Qazi%20and%20Munachiso%20S%20Nwadike%20and%20Ibrahim%20Almakky%20and%20Mohammad%20Yaqub%20and%20Numan%20Saeed&entry.1292438233=%20%20Foundational%20models%20are%20trained%20on%20extensive%20datasets%20to%20capture%20the%20general%0Atrends%20of%20a%20domain.%20However%2C%20in%20medical%20imaging%2C%20the%20scarcity%20of%20data%20makes%0Apre-training%20for%20every%20domain%2C%20modality%2C%20or%20task%20challenging.%20Continual%0Alearning%20offers%20a%20solution%20by%20fine-tuning%20a%20model%20sequentially%20on%20different%0Adomains%20or%20tasks%2C%20enabling%20it%20to%20integrate%20new%20knowledge%20without%20requiring%0Alarge%20datasets%20for%20each%20training%20phase.%20In%20this%20paper%2C%20we%20propose%20UNIfied%0ACONtinual%20Learning%20for%20Medical%20Foundational%20Models%20%28UNICON%29%2C%20a%20framework%20that%0Aenables%20the%20seamless%20adaptation%20of%20foundation%20models%20to%20diverse%20domains%2C%20tasks%2C%0Aand%20modalities.%20Unlike%20conventional%20adaptation%20methods%20that%20treat%20these%20changes%0Ain%20isolation%2C%20UNICON%20provides%20a%20unified%2C%20perpetually%20expandable%20framework.%0AThrough%20careful%20integration%2C%20we%20show%20that%20foundation%20models%20can%20dynamically%0Aexpand%20across%20imaging%20modalities%2C%20anatomical%20regions%2C%20and%20clinical%20objectives%0Awithout%20catastrophic%20forgetting%20or%20task%20interference.%20Empirically%2C%20we%20validate%0Aour%20approach%20by%20adapting%20a%20chest%20CT%20foundation%20model%20initially%20trained%20for%0Aclassification%20to%20a%20prognosis%20and%20segmentation%20task.%20Our%20results%20show%20improved%0Aperformance%20across%20both%20additional%20tasks.%20Furthermore%2C%20we%20continually%0Aincorporated%20PET%20scans%20and%20achieved%20a%205%5C%25%20improvement%20in%20Dice%20score%20compared%20to%0Arespective%20baselines.%20These%20findings%20establish%20that%20foundation%20models%20are%20not%0Ainherently%20constrained%20to%20their%20initial%20training%20scope%20but%20can%20evolve%2C%20paving%0Athe%20way%20toward%20generalist%20AI%20models%20for%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14024v1&entry.124074799=Read"},
{"title": "Slot Attention with Re-Initialization and Self-Distillation", "author": "Rongzhen Zhao and Yi Zhao and Juho Kannala and Joni Pajarinen", "abstract": "  Unlike popular solutions based on dense feature maps, Object-Centric Learning\n(OCL) represents visual scenes as sub-symbolic object-level feature vectors,\ntermed slots, which are highly versatile for tasks involving visual modalities.\nOCL typically aggregates object superpixels into slots by iteratively applying\ncompetitive cross attention, known as Slot Attention, with the slots as the\nquery. However, once initialized, these slots are reused naively, causing\nredundant slots to compete with informative ones for representing objects. This\noften results in objects being erroneously segmented into parts. Additionally,\nmainstream methods derive supervision signals solely from decoding slots into\nthe input's reconstruction, overlooking potential supervision based on internal\ninformation. To address these issues, we propose Slot Attention with\nre-Initialization and self-Distillation (DIAS): $\\emph{i)}$ We reduce\nredundancy in the aggregated slots and re-initialize extra aggregation to\nupdate the remaining slots; $\\emph{ii)}$ We drive the bad attention map at the\nfirst aggregation iteration to approximate the good at the last iteration to\nenable self-distillation. Experiments demonstrate that DIAS achieves\nstate-of-the-art on OCL tasks like object discovery and recognition, while also\nimproving advanced visual prediction and reasoning. Our source code and model\ncheckpoints are available on https://github.com/Genera1Z/DIAS.\n", "link": "http://arxiv.org/abs/2507.23755v2", "date": "2025-08-19", "relevancy": 2.7669, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5702}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5522}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Slot%20Attention%20with%20Re-Initialization%20and%20Self-Distillation&body=Title%3A%20Slot%20Attention%20with%20Re-Initialization%20and%20Self-Distillation%0AAuthor%3A%20Rongzhen%20Zhao%20and%20Yi%20Zhao%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Unlike%20popular%20solutions%20based%20on%20dense%20feature%20maps%2C%20Object-Centric%20Learning%0A%28OCL%29%20represents%20visual%20scenes%20as%20sub-symbolic%20object-level%20feature%20vectors%2C%0Atermed%20slots%2C%20which%20are%20highly%20versatile%20for%20tasks%20involving%20visual%20modalities.%0AOCL%20typically%20aggregates%20object%20superpixels%20into%20slots%20by%20iteratively%20applying%0Acompetitive%20cross%20attention%2C%20known%20as%20Slot%20Attention%2C%20with%20the%20slots%20as%20the%0Aquery.%20However%2C%20once%20initialized%2C%20these%20slots%20are%20reused%20naively%2C%20causing%0Aredundant%20slots%20to%20compete%20with%20informative%20ones%20for%20representing%20objects.%20This%0Aoften%20results%20in%20objects%20being%20erroneously%20segmented%20into%20parts.%20Additionally%2C%0Amainstream%20methods%20derive%20supervision%20signals%20solely%20from%20decoding%20slots%20into%0Athe%20input%27s%20reconstruction%2C%20overlooking%20potential%20supervision%20based%20on%20internal%0Ainformation.%20To%20address%20these%20issues%2C%20we%20propose%20Slot%20Attention%20with%0Are-Initialization%20and%20self-Distillation%20%28DIAS%29%3A%20%24%5Cemph%7Bi%29%7D%24%20We%20reduce%0Aredundancy%20in%20the%20aggregated%20slots%20and%20re-initialize%20extra%20aggregation%20to%0Aupdate%20the%20remaining%20slots%3B%20%24%5Cemph%7Bii%29%7D%24%20We%20drive%20the%20bad%20attention%20map%20at%20the%0Afirst%20aggregation%20iteration%20to%20approximate%20the%20good%20at%20the%20last%20iteration%20to%0Aenable%20self-distillation.%20Experiments%20demonstrate%20that%20DIAS%20achieves%0Astate-of-the-art%20on%20OCL%20tasks%20like%20object%20discovery%20and%20recognition%2C%20while%20also%0Aimproving%20advanced%20visual%20prediction%20and%20reasoning.%20Our%20source%20code%20and%20model%0Acheckpoints%20are%20available%20on%20https%3A//github.com/Genera1Z/DIAS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23755v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlot%2520Attention%2520with%2520Re-Initialization%2520and%2520Self-Distillation%26entry.906535625%3DRongzhen%2520Zhao%2520and%2520Yi%2520Zhao%2520and%2520Juho%2520Kannala%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Unlike%2520popular%2520solutions%2520based%2520on%2520dense%2520feature%2520maps%252C%2520Object-Centric%2520Learning%250A%2528OCL%2529%2520represents%2520visual%2520scenes%2520as%2520sub-symbolic%2520object-level%2520feature%2520vectors%252C%250Atermed%2520slots%252C%2520which%2520are%2520highly%2520versatile%2520for%2520tasks%2520involving%2520visual%2520modalities.%250AOCL%2520typically%2520aggregates%2520object%2520superpixels%2520into%2520slots%2520by%2520iteratively%2520applying%250Acompetitive%2520cross%2520attention%252C%2520known%2520as%2520Slot%2520Attention%252C%2520with%2520the%2520slots%2520as%2520the%250Aquery.%2520However%252C%2520once%2520initialized%252C%2520these%2520slots%2520are%2520reused%2520naively%252C%2520causing%250Aredundant%2520slots%2520to%2520compete%2520with%2520informative%2520ones%2520for%2520representing%2520objects.%2520This%250Aoften%2520results%2520in%2520objects%2520being%2520erroneously%2520segmented%2520into%2520parts.%2520Additionally%252C%250Amainstream%2520methods%2520derive%2520supervision%2520signals%2520solely%2520from%2520decoding%2520slots%2520into%250Athe%2520input%2527s%2520reconstruction%252C%2520overlooking%2520potential%2520supervision%2520based%2520on%2520internal%250Ainformation.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Slot%2520Attention%2520with%250Are-Initialization%2520and%2520self-Distillation%2520%2528DIAS%2529%253A%2520%2524%255Cemph%257Bi%2529%257D%2524%2520We%2520reduce%250Aredundancy%2520in%2520the%2520aggregated%2520slots%2520and%2520re-initialize%2520extra%2520aggregation%2520to%250Aupdate%2520the%2520remaining%2520slots%253B%2520%2524%255Cemph%257Bii%2529%257D%2524%2520We%2520drive%2520the%2520bad%2520attention%2520map%2520at%2520the%250Afirst%2520aggregation%2520iteration%2520to%2520approximate%2520the%2520good%2520at%2520the%2520last%2520iteration%2520to%250Aenable%2520self-distillation.%2520Experiments%2520demonstrate%2520that%2520DIAS%2520achieves%250Astate-of-the-art%2520on%2520OCL%2520tasks%2520like%2520object%2520discovery%2520and%2520recognition%252C%2520while%2520also%250Aimproving%2520advanced%2520visual%2520prediction%2520and%2520reasoning.%2520Our%2520source%2520code%2520and%2520model%250Acheckpoints%2520are%2520available%2520on%2520https%253A//github.com/Genera1Z/DIAS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23755v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Slot%20Attention%20with%20Re-Initialization%20and%20Self-Distillation&entry.906535625=Rongzhen%20Zhao%20and%20Yi%20Zhao%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Unlike%20popular%20solutions%20based%20on%20dense%20feature%20maps%2C%20Object-Centric%20Learning%0A%28OCL%29%20represents%20visual%20scenes%20as%20sub-symbolic%20object-level%20feature%20vectors%2C%0Atermed%20slots%2C%20which%20are%20highly%20versatile%20for%20tasks%20involving%20visual%20modalities.%0AOCL%20typically%20aggregates%20object%20superpixels%20into%20slots%20by%20iteratively%20applying%0Acompetitive%20cross%20attention%2C%20known%20as%20Slot%20Attention%2C%20with%20the%20slots%20as%20the%0Aquery.%20However%2C%20once%20initialized%2C%20these%20slots%20are%20reused%20naively%2C%20causing%0Aredundant%20slots%20to%20compete%20with%20informative%20ones%20for%20representing%20objects.%20This%0Aoften%20results%20in%20objects%20being%20erroneously%20segmented%20into%20parts.%20Additionally%2C%0Amainstream%20methods%20derive%20supervision%20signals%20solely%20from%20decoding%20slots%20into%0Athe%20input%27s%20reconstruction%2C%20overlooking%20potential%20supervision%20based%20on%20internal%0Ainformation.%20To%20address%20these%20issues%2C%20we%20propose%20Slot%20Attention%20with%0Are-Initialization%20and%20self-Distillation%20%28DIAS%29%3A%20%24%5Cemph%7Bi%29%7D%24%20We%20reduce%0Aredundancy%20in%20the%20aggregated%20slots%20and%20re-initialize%20extra%20aggregation%20to%0Aupdate%20the%20remaining%20slots%3B%20%24%5Cemph%7Bii%29%7D%24%20We%20drive%20the%20bad%20attention%20map%20at%20the%0Afirst%20aggregation%20iteration%20to%20approximate%20the%20good%20at%20the%20last%20iteration%20to%0Aenable%20self-distillation.%20Experiments%20demonstrate%20that%20DIAS%20achieves%0Astate-of-the-art%20on%20OCL%20tasks%20like%20object%20discovery%20and%20recognition%2C%20while%20also%0Aimproving%20advanced%20visual%20prediction%20and%20reasoning.%20Our%20source%20code%20and%20model%0Acheckpoints%20are%20available%20on%20https%3A//github.com/Genera1Z/DIAS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23755v2&entry.124074799=Read"},
{"title": "RotBench: Evaluating Multimodal Large Language Models on Identifying\n  Image Rotation", "author": "Tianyi Niu and Jaemin Cho and Elias Stengel-Eskin and Mohit Bansal", "abstract": "  We investigate to what extent Multimodal Large Language Models (MLLMs) can\naccurately identify the orientation of input images rotated 0{\\deg}, 90{\\deg},\n180{\\deg}, and 270{\\deg}. This task demands robust visual reasoning\ncapabilities to detect rotational cues and contextualize spatial relationships\nwithin images, regardless of their orientation. To evaluate MLLMs on these\nabilities, we introduce RotBench -- a 350-image manually-filtered benchmark\ncomprising lifestyle, portrait, and landscape images. Despite the relatively\nsimple nature of this task, we show that several state-of-the-art open and\nproprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably\nidentify rotation in input images. Providing models with auxiliary information\n-- including captions, depth maps, and more -- or using chain-of-thought\nprompting offers only small and inconsistent improvements. Our results indicate\nthat most models are able to reliably identify right-side-up (0{\\deg}) images,\nwhile certain models are able to identify upside-down (180{\\deg}) images. None\ncan reliably distinguish between 90{\\deg} and 270{\\deg}. Simultaneously showing\nthe image rotated in different orientations leads to moderate performance gains\nfor reasoning models, while a modified setup using voting improves the\nperformance of weaker models. We further show that fine-tuning does not improve\nmodels' ability to distinguish 90{\\deg} and 270{\\deg} rotations, despite\nsubstantially improving the identification of 180{\\deg} images. Together, these\nresults reveal a significant gap between MLLMs' spatial reasoning capabilities\nand human perception in identifying rotation.\n", "link": "http://arxiv.org/abs/2508.13968v1", "date": "2025-08-19", "relevancy": 2.7282, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5433}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RotBench%3A%20Evaluating%20Multimodal%20Large%20Language%20Models%20on%20Identifying%0A%20%20Image%20Rotation&body=Title%3A%20RotBench%3A%20Evaluating%20Multimodal%20Large%20Language%20Models%20on%20Identifying%0A%20%20Image%20Rotation%0AAuthor%3A%20Tianyi%20Niu%20and%20Jaemin%20Cho%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20We%20investigate%20to%20what%20extent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20can%0Aaccurately%20identify%20the%20orientation%20of%20input%20images%20rotated%200%7B%5Cdeg%7D%2C%2090%7B%5Cdeg%7D%2C%0A180%7B%5Cdeg%7D%2C%20and%20270%7B%5Cdeg%7D.%20This%20task%20demands%20robust%20visual%20reasoning%0Acapabilities%20to%20detect%20rotational%20cues%20and%20contextualize%20spatial%20relationships%0Awithin%20images%2C%20regardless%20of%20their%20orientation.%20To%20evaluate%20MLLMs%20on%20these%0Aabilities%2C%20we%20introduce%20RotBench%20--%20a%20350-image%20manually-filtered%20benchmark%0Acomprising%20lifestyle%2C%20portrait%2C%20and%20landscape%20images.%20Despite%20the%20relatively%0Asimple%20nature%20of%20this%20task%2C%20we%20show%20that%20several%20state-of-the-art%20open%20and%0Aproprietary%20MLLMs%2C%20including%20GPT-5%2C%20o3%2C%20and%20Gemini-2.5-Pro%2C%20do%20not%20reliably%0Aidentify%20rotation%20in%20input%20images.%20Providing%20models%20with%20auxiliary%20information%0A--%20including%20captions%2C%20depth%20maps%2C%20and%20more%20--%20or%20using%20chain-of-thought%0Aprompting%20offers%20only%20small%20and%20inconsistent%20improvements.%20Our%20results%20indicate%0Athat%20most%20models%20are%20able%20to%20reliably%20identify%20right-side-up%20%280%7B%5Cdeg%7D%29%20images%2C%0Awhile%20certain%20models%20are%20able%20to%20identify%20upside-down%20%28180%7B%5Cdeg%7D%29%20images.%20None%0Acan%20reliably%20distinguish%20between%2090%7B%5Cdeg%7D%20and%20270%7B%5Cdeg%7D.%20Simultaneously%20showing%0Athe%20image%20rotated%20in%20different%20orientations%20leads%20to%20moderate%20performance%20gains%0Afor%20reasoning%20models%2C%20while%20a%20modified%20setup%20using%20voting%20improves%20the%0Aperformance%20of%20weaker%20models.%20We%20further%20show%20that%20fine-tuning%20does%20not%20improve%0Amodels%27%20ability%20to%20distinguish%2090%7B%5Cdeg%7D%20and%20270%7B%5Cdeg%7D%20rotations%2C%20despite%0Asubstantially%20improving%20the%20identification%20of%20180%7B%5Cdeg%7D%20images.%20Together%2C%20these%0Aresults%20reveal%20a%20significant%20gap%20between%20MLLMs%27%20spatial%20reasoning%20capabilities%0Aand%20human%20perception%20in%20identifying%20rotation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRotBench%253A%2520Evaluating%2520Multimodal%2520Large%2520Language%2520Models%2520on%2520Identifying%250A%2520%2520Image%2520Rotation%26entry.906535625%3DTianyi%2520Niu%2520and%2520Jaemin%2520Cho%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520We%2520investigate%2520to%2520what%2520extent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520can%250Aaccurately%2520identify%2520the%2520orientation%2520of%2520input%2520images%2520rotated%25200%257B%255Cdeg%257D%252C%252090%257B%255Cdeg%257D%252C%250A180%257B%255Cdeg%257D%252C%2520and%2520270%257B%255Cdeg%257D.%2520This%2520task%2520demands%2520robust%2520visual%2520reasoning%250Acapabilities%2520to%2520detect%2520rotational%2520cues%2520and%2520contextualize%2520spatial%2520relationships%250Awithin%2520images%252C%2520regardless%2520of%2520their%2520orientation.%2520To%2520evaluate%2520MLLMs%2520on%2520these%250Aabilities%252C%2520we%2520introduce%2520RotBench%2520--%2520a%2520350-image%2520manually-filtered%2520benchmark%250Acomprising%2520lifestyle%252C%2520portrait%252C%2520and%2520landscape%2520images.%2520Despite%2520the%2520relatively%250Asimple%2520nature%2520of%2520this%2520task%252C%2520we%2520show%2520that%2520several%2520state-of-the-art%2520open%2520and%250Aproprietary%2520MLLMs%252C%2520including%2520GPT-5%252C%2520o3%252C%2520and%2520Gemini-2.5-Pro%252C%2520do%2520not%2520reliably%250Aidentify%2520rotation%2520in%2520input%2520images.%2520Providing%2520models%2520with%2520auxiliary%2520information%250A--%2520including%2520captions%252C%2520depth%2520maps%252C%2520and%2520more%2520--%2520or%2520using%2520chain-of-thought%250Aprompting%2520offers%2520only%2520small%2520and%2520inconsistent%2520improvements.%2520Our%2520results%2520indicate%250Athat%2520most%2520models%2520are%2520able%2520to%2520reliably%2520identify%2520right-side-up%2520%25280%257B%255Cdeg%257D%2529%2520images%252C%250Awhile%2520certain%2520models%2520are%2520able%2520to%2520identify%2520upside-down%2520%2528180%257B%255Cdeg%257D%2529%2520images.%2520None%250Acan%2520reliably%2520distinguish%2520between%252090%257B%255Cdeg%257D%2520and%2520270%257B%255Cdeg%257D.%2520Simultaneously%2520showing%250Athe%2520image%2520rotated%2520in%2520different%2520orientations%2520leads%2520to%2520moderate%2520performance%2520gains%250Afor%2520reasoning%2520models%252C%2520while%2520a%2520modified%2520setup%2520using%2520voting%2520improves%2520the%250Aperformance%2520of%2520weaker%2520models.%2520We%2520further%2520show%2520that%2520fine-tuning%2520does%2520not%2520improve%250Amodels%2527%2520ability%2520to%2520distinguish%252090%257B%255Cdeg%257D%2520and%2520270%257B%255Cdeg%257D%2520rotations%252C%2520despite%250Asubstantially%2520improving%2520the%2520identification%2520of%2520180%257B%255Cdeg%257D%2520images.%2520Together%252C%2520these%250Aresults%2520reveal%2520a%2520significant%2520gap%2520between%2520MLLMs%2527%2520spatial%2520reasoning%2520capabilities%250Aand%2520human%2520perception%2520in%2520identifying%2520rotation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RotBench%3A%20Evaluating%20Multimodal%20Large%20Language%20Models%20on%20Identifying%0A%20%20Image%20Rotation&entry.906535625=Tianyi%20Niu%20and%20Jaemin%20Cho%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal&entry.1292438233=%20%20We%20investigate%20to%20what%20extent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20can%0Aaccurately%20identify%20the%20orientation%20of%20input%20images%20rotated%200%7B%5Cdeg%7D%2C%2090%7B%5Cdeg%7D%2C%0A180%7B%5Cdeg%7D%2C%20and%20270%7B%5Cdeg%7D.%20This%20task%20demands%20robust%20visual%20reasoning%0Acapabilities%20to%20detect%20rotational%20cues%20and%20contextualize%20spatial%20relationships%0Awithin%20images%2C%20regardless%20of%20their%20orientation.%20To%20evaluate%20MLLMs%20on%20these%0Aabilities%2C%20we%20introduce%20RotBench%20--%20a%20350-image%20manually-filtered%20benchmark%0Acomprising%20lifestyle%2C%20portrait%2C%20and%20landscape%20images.%20Despite%20the%20relatively%0Asimple%20nature%20of%20this%20task%2C%20we%20show%20that%20several%20state-of-the-art%20open%20and%0Aproprietary%20MLLMs%2C%20including%20GPT-5%2C%20o3%2C%20and%20Gemini-2.5-Pro%2C%20do%20not%20reliably%0Aidentify%20rotation%20in%20input%20images.%20Providing%20models%20with%20auxiliary%20information%0A--%20including%20captions%2C%20depth%20maps%2C%20and%20more%20--%20or%20using%20chain-of-thought%0Aprompting%20offers%20only%20small%20and%20inconsistent%20improvements.%20Our%20results%20indicate%0Athat%20most%20models%20are%20able%20to%20reliably%20identify%20right-side-up%20%280%7B%5Cdeg%7D%29%20images%2C%0Awhile%20certain%20models%20are%20able%20to%20identify%20upside-down%20%28180%7B%5Cdeg%7D%29%20images.%20None%0Acan%20reliably%20distinguish%20between%2090%7B%5Cdeg%7D%20and%20270%7B%5Cdeg%7D.%20Simultaneously%20showing%0Athe%20image%20rotated%20in%20different%20orientations%20leads%20to%20moderate%20performance%20gains%0Afor%20reasoning%20models%2C%20while%20a%20modified%20setup%20using%20voting%20improves%20the%0Aperformance%20of%20weaker%20models.%20We%20further%20show%20that%20fine-tuning%20does%20not%20improve%0Amodels%27%20ability%20to%20distinguish%2090%7B%5Cdeg%7D%20and%20270%7B%5Cdeg%7D%20rotations%2C%20despite%0Asubstantially%20improving%20the%20identification%20of%20180%7B%5Cdeg%7D%20images.%20Together%2C%20these%0Aresults%20reveal%20a%20significant%20gap%20between%20MLLMs%27%20spatial%20reasoning%20capabilities%0Aand%20human%20perception%20in%20identifying%20rotation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13968v1&entry.124074799=Read"},
{"title": "Enhancing Targeted Adversarial Attacks on Large Vision-Language Models\n  through Intermediate Projector Guidance", "author": "Yiming Cao and Yanjie Li and Kaisheng Liang and Yuni Lai and Bin Xiao", "abstract": "  Targeted adversarial attacks are essential for proactively identifying\nsecurity flaws in Vision-Language Models before real-world deployment. However,\ncurrent methods perturb images to maximize global similarity with the target\ntext or reference image at the encoder level, collapsing rich visual semantics\ninto a single global vector. This limits attack granularity, hindering\nfine-grained manipulations such as modifying a car while preserving its\nbackground. Furthermore, these methods largely overlook the projector module, a\ncritical semantic bridge between the visual encoder and the language model in\nVLMs, thereby failing to disrupt the full vision-language alignment pipeline\nwithin VLMs and limiting attack effectiveness. To address these issues, we\npropose the Intermediate Projector Guided Attack (IPGA), the first method to\nattack using the intermediate stage of the projector module, specifically the\nwidely adopted Q-Former, which transforms global image embeddings into\nfine-grained visual features. This enables more precise control over\nadversarial perturbations by operating on semantically meaningful visual tokens\nrather than a single global representation. Specifically, IPGA leverages the\nQ-Former pretrained solely on the first vision-language alignment stage,\nwithout LLM fine-tuning, which improves both attack effectiveness and\ntransferability across diverse VLMs. Furthermore, we propose Residual Query\nAlignment (RQA) to preserve unrelated visual content, thereby yielding more\ncontrolled and precise adversarial manipulations. Extensive experiments show\nthat our attack method consistently outperforms existing methods in both\nstandard global image captioning tasks and fine-grained visual\nquestion-answering tasks in black-box environment. Additionally, IPGA\nsuccessfully transfers to multiple commercial VLMs, including Google Gemini and\nOpenAI GPT.\n", "link": "http://arxiv.org/abs/2508.13739v1", "date": "2025-08-19", "relevancy": 2.7222, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5418}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Targeted%20Adversarial%20Attacks%20on%20Large%20Vision-Language%20Models%0A%20%20through%20Intermediate%20Projector%20Guidance&body=Title%3A%20Enhancing%20Targeted%20Adversarial%20Attacks%20on%20Large%20Vision-Language%20Models%0A%20%20through%20Intermediate%20Projector%20Guidance%0AAuthor%3A%20Yiming%20Cao%20and%20Yanjie%20Li%20and%20Kaisheng%20Liang%20and%20Yuni%20Lai%20and%20Bin%20Xiao%0AAbstract%3A%20%20%20Targeted%20adversarial%20attacks%20are%20essential%20for%20proactively%20identifying%0Asecurity%20flaws%20in%20Vision-Language%20Models%20before%20real-world%20deployment.%20However%2C%0Acurrent%20methods%20perturb%20images%20to%20maximize%20global%20similarity%20with%20the%20target%0Atext%20or%20reference%20image%20at%20the%20encoder%20level%2C%20collapsing%20rich%20visual%20semantics%0Ainto%20a%20single%20global%20vector.%20This%20limits%20attack%20granularity%2C%20hindering%0Afine-grained%20manipulations%20such%20as%20modifying%20a%20car%20while%20preserving%20its%0Abackground.%20Furthermore%2C%20these%20methods%20largely%20overlook%20the%20projector%20module%2C%20a%0Acritical%20semantic%20bridge%20between%20the%20visual%20encoder%20and%20the%20language%20model%20in%0AVLMs%2C%20thereby%20failing%20to%20disrupt%20the%20full%20vision-language%20alignment%20pipeline%0Awithin%20VLMs%20and%20limiting%20attack%20effectiveness.%20To%20address%20these%20issues%2C%20we%0Apropose%20the%20Intermediate%20Projector%20Guided%20Attack%20%28IPGA%29%2C%20the%20first%20method%20to%0Aattack%20using%20the%20intermediate%20stage%20of%20the%20projector%20module%2C%20specifically%20the%0Awidely%20adopted%20Q-Former%2C%20which%20transforms%20global%20image%20embeddings%20into%0Afine-grained%20visual%20features.%20This%20enables%20more%20precise%20control%20over%0Aadversarial%20perturbations%20by%20operating%20on%20semantically%20meaningful%20visual%20tokens%0Arather%20than%20a%20single%20global%20representation.%20Specifically%2C%20IPGA%20leverages%20the%0AQ-Former%20pretrained%20solely%20on%20the%20first%20vision-language%20alignment%20stage%2C%0Awithout%20LLM%20fine-tuning%2C%20which%20improves%20both%20attack%20effectiveness%20and%0Atransferability%20across%20diverse%20VLMs.%20Furthermore%2C%20we%20propose%20Residual%20Query%0AAlignment%20%28RQA%29%20to%20preserve%20unrelated%20visual%20content%2C%20thereby%20yielding%20more%0Acontrolled%20and%20precise%20adversarial%20manipulations.%20Extensive%20experiments%20show%0Athat%20our%20attack%20method%20consistently%20outperforms%20existing%20methods%20in%20both%0Astandard%20global%20image%20captioning%20tasks%20and%20fine-grained%20visual%0Aquestion-answering%20tasks%20in%20black-box%20environment.%20Additionally%2C%20IPGA%0Asuccessfully%20transfers%20to%20multiple%20commercial%20VLMs%2C%20including%20Google%20Gemini%20and%0AOpenAI%20GPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Targeted%2520Adversarial%2520Attacks%2520on%2520Large%2520Vision-Language%2520Models%250A%2520%2520through%2520Intermediate%2520Projector%2520Guidance%26entry.906535625%3DYiming%2520Cao%2520and%2520Yanjie%2520Li%2520and%2520Kaisheng%2520Liang%2520and%2520Yuni%2520Lai%2520and%2520Bin%2520Xiao%26entry.1292438233%3D%2520%2520Targeted%2520adversarial%2520attacks%2520are%2520essential%2520for%2520proactively%2520identifying%250Asecurity%2520flaws%2520in%2520Vision-Language%2520Models%2520before%2520real-world%2520deployment.%2520However%252C%250Acurrent%2520methods%2520perturb%2520images%2520to%2520maximize%2520global%2520similarity%2520with%2520the%2520target%250Atext%2520or%2520reference%2520image%2520at%2520the%2520encoder%2520level%252C%2520collapsing%2520rich%2520visual%2520semantics%250Ainto%2520a%2520single%2520global%2520vector.%2520This%2520limits%2520attack%2520granularity%252C%2520hindering%250Afine-grained%2520manipulations%2520such%2520as%2520modifying%2520a%2520car%2520while%2520preserving%2520its%250Abackground.%2520Furthermore%252C%2520these%2520methods%2520largely%2520overlook%2520the%2520projector%2520module%252C%2520a%250Acritical%2520semantic%2520bridge%2520between%2520the%2520visual%2520encoder%2520and%2520the%2520language%2520model%2520in%250AVLMs%252C%2520thereby%2520failing%2520to%2520disrupt%2520the%2520full%2520vision-language%2520alignment%2520pipeline%250Awithin%2520VLMs%2520and%2520limiting%2520attack%2520effectiveness.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520the%2520Intermediate%2520Projector%2520Guided%2520Attack%2520%2528IPGA%2529%252C%2520the%2520first%2520method%2520to%250Aattack%2520using%2520the%2520intermediate%2520stage%2520of%2520the%2520projector%2520module%252C%2520specifically%2520the%250Awidely%2520adopted%2520Q-Former%252C%2520which%2520transforms%2520global%2520image%2520embeddings%2520into%250Afine-grained%2520visual%2520features.%2520This%2520enables%2520more%2520precise%2520control%2520over%250Aadversarial%2520perturbations%2520by%2520operating%2520on%2520semantically%2520meaningful%2520visual%2520tokens%250Arather%2520than%2520a%2520single%2520global%2520representation.%2520Specifically%252C%2520IPGA%2520leverages%2520the%250AQ-Former%2520pretrained%2520solely%2520on%2520the%2520first%2520vision-language%2520alignment%2520stage%252C%250Awithout%2520LLM%2520fine-tuning%252C%2520which%2520improves%2520both%2520attack%2520effectiveness%2520and%250Atransferability%2520across%2520diverse%2520VLMs.%2520Furthermore%252C%2520we%2520propose%2520Residual%2520Query%250AAlignment%2520%2528RQA%2529%2520to%2520preserve%2520unrelated%2520visual%2520content%252C%2520thereby%2520yielding%2520more%250Acontrolled%2520and%2520precise%2520adversarial%2520manipulations.%2520Extensive%2520experiments%2520show%250Athat%2520our%2520attack%2520method%2520consistently%2520outperforms%2520existing%2520methods%2520in%2520both%250Astandard%2520global%2520image%2520captioning%2520tasks%2520and%2520fine-grained%2520visual%250Aquestion-answering%2520tasks%2520in%2520black-box%2520environment.%2520Additionally%252C%2520IPGA%250Asuccessfully%2520transfers%2520to%2520multiple%2520commercial%2520VLMs%252C%2520including%2520Google%2520Gemini%2520and%250AOpenAI%2520GPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Targeted%20Adversarial%20Attacks%20on%20Large%20Vision-Language%20Models%0A%20%20through%20Intermediate%20Projector%20Guidance&entry.906535625=Yiming%20Cao%20and%20Yanjie%20Li%20and%20Kaisheng%20Liang%20and%20Yuni%20Lai%20and%20Bin%20Xiao&entry.1292438233=%20%20Targeted%20adversarial%20attacks%20are%20essential%20for%20proactively%20identifying%0Asecurity%20flaws%20in%20Vision-Language%20Models%20before%20real-world%20deployment.%20However%2C%0Acurrent%20methods%20perturb%20images%20to%20maximize%20global%20similarity%20with%20the%20target%0Atext%20or%20reference%20image%20at%20the%20encoder%20level%2C%20collapsing%20rich%20visual%20semantics%0Ainto%20a%20single%20global%20vector.%20This%20limits%20attack%20granularity%2C%20hindering%0Afine-grained%20manipulations%20such%20as%20modifying%20a%20car%20while%20preserving%20its%0Abackground.%20Furthermore%2C%20these%20methods%20largely%20overlook%20the%20projector%20module%2C%20a%0Acritical%20semantic%20bridge%20between%20the%20visual%20encoder%20and%20the%20language%20model%20in%0AVLMs%2C%20thereby%20failing%20to%20disrupt%20the%20full%20vision-language%20alignment%20pipeline%0Awithin%20VLMs%20and%20limiting%20attack%20effectiveness.%20To%20address%20these%20issues%2C%20we%0Apropose%20the%20Intermediate%20Projector%20Guided%20Attack%20%28IPGA%29%2C%20the%20first%20method%20to%0Aattack%20using%20the%20intermediate%20stage%20of%20the%20projector%20module%2C%20specifically%20the%0Awidely%20adopted%20Q-Former%2C%20which%20transforms%20global%20image%20embeddings%20into%0Afine-grained%20visual%20features.%20This%20enables%20more%20precise%20control%20over%0Aadversarial%20perturbations%20by%20operating%20on%20semantically%20meaningful%20visual%20tokens%0Arather%20than%20a%20single%20global%20representation.%20Specifically%2C%20IPGA%20leverages%20the%0AQ-Former%20pretrained%20solely%20on%20the%20first%20vision-language%20alignment%20stage%2C%0Awithout%20LLM%20fine-tuning%2C%20which%20improves%20both%20attack%20effectiveness%20and%0Atransferability%20across%20diverse%20VLMs.%20Furthermore%2C%20we%20propose%20Residual%20Query%0AAlignment%20%28RQA%29%20to%20preserve%20unrelated%20visual%20content%2C%20thereby%20yielding%20more%0Acontrolled%20and%20precise%20adversarial%20manipulations.%20Extensive%20experiments%20show%0Athat%20our%20attack%20method%20consistently%20outperforms%20existing%20methods%20in%20both%0Astandard%20global%20image%20captioning%20tasks%20and%20fine-grained%20visual%0Aquestion-answering%20tasks%20in%20black-box%20environment.%20Additionally%2C%20IPGA%0Asuccessfully%20transfers%20to%20multiple%20commercial%20VLMs%2C%20including%20Google%20Gemini%20and%0AOpenAI%20GPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13739v1&entry.124074799=Read"},
{"title": "What Matters for Bioacoustic Encoding", "author": "Marius Miron and David Robinson and Milad Alizadeh and Ellen Gilsenan-McMahon and Gagan Narula and Emmanuel Chemla and Maddie Cusimano and Felix Effenberger and Masato Hagiwara and Benjamin Hoffman and Sara Keen and Diane Kim and Jane Lawton and Jen-Yu Liu and Aza Raskin and Olivier Pietquin and Matthieu Geist", "abstract": "  Bioacoustics, the study of sounds produced by living organisms, plays a vital\nrole in conservation, biodiversity monitoring, and behavioral studies. Many\ntasks in this field, such as species, individual, and behavior classification\nand detection, are well-suited to machine learning. However, they often suffer\nfrom limited annotated data, highlighting the need for a general-purpose\nbioacoustic encoder capable of extracting useful representations for diverse\ndownstream tasks. Such encoders have been proposed before, but are often\nlimited in scope due to a focus on a narrow range of species (typically birds),\nand a reliance on a single model architecture or training paradigm. Moreover,\nthey are usually evaluated on a small set of tasks and datasets. In this work,\nwe present a large-scale empirical study that covers aspects of bioacoustics\nthat are relevant to research but have previously been scarcely considered:\ntraining data diversity and scale, model architectures and training recipes,\nand the breadth of evaluation tasks and datasets. We obtain encoders that are\nstate-of-the-art on the existing and proposed benchmarks. We also identify what\nmatters for training these encoders, such that this work can be extended when\nmore data are available or better architectures are proposed. Specifically,\nacross 26 datasets with tasks including species classification, detection,\nindividual ID, and vocal repertoire discovery, we find self-supervised\npre-training followed by supervised post-training on a mixed bioacoustics +\ngeneral-audio corpus yields the strongest in- and out-of-distribution\nperformance. We show the importance of data diversity in both stages. To\nsupport ongoing research and application, we will release the model\ncheckpoints.\n", "link": "http://arxiv.org/abs/2508.11845v2", "date": "2025-08-19", "relevancy": 2.7008, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5653}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5653}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Matters%20for%20Bioacoustic%20Encoding&body=Title%3A%20What%20Matters%20for%20Bioacoustic%20Encoding%0AAuthor%3A%20Marius%20Miron%20and%20David%20Robinson%20and%20Milad%20Alizadeh%20and%20Ellen%20Gilsenan-McMahon%20and%20Gagan%20Narula%20and%20Emmanuel%20Chemla%20and%20Maddie%20Cusimano%20and%20Felix%20Effenberger%20and%20Masato%20Hagiwara%20and%20Benjamin%20Hoffman%20and%20Sara%20Keen%20and%20Diane%20Kim%20and%20Jane%20Lawton%20and%20Jen-Yu%20Liu%20and%20Aza%20Raskin%20and%20Olivier%20Pietquin%20and%20Matthieu%20Geist%0AAbstract%3A%20%20%20Bioacoustics%2C%20the%20study%20of%20sounds%20produced%20by%20living%20organisms%2C%20plays%20a%20vital%0Arole%20in%20conservation%2C%20biodiversity%20monitoring%2C%20and%20behavioral%20studies.%20Many%0Atasks%20in%20this%20field%2C%20such%20as%20species%2C%20individual%2C%20and%20behavior%20classification%0Aand%20detection%2C%20are%20well-suited%20to%20machine%20learning.%20However%2C%20they%20often%20suffer%0Afrom%20limited%20annotated%20data%2C%20highlighting%20the%20need%20for%20a%20general-purpose%0Abioacoustic%20encoder%20capable%20of%20extracting%20useful%20representations%20for%20diverse%0Adownstream%20tasks.%20Such%20encoders%20have%20been%20proposed%20before%2C%20but%20are%20often%0Alimited%20in%20scope%20due%20to%20a%20focus%20on%20a%20narrow%20range%20of%20species%20%28typically%20birds%29%2C%0Aand%20a%20reliance%20on%20a%20single%20model%20architecture%20or%20training%20paradigm.%20Moreover%2C%0Athey%20are%20usually%20evaluated%20on%20a%20small%20set%20of%20tasks%20and%20datasets.%20In%20this%20work%2C%0Awe%20present%20a%20large-scale%20empirical%20study%20that%20covers%20aspects%20of%20bioacoustics%0Athat%20are%20relevant%20to%20research%20but%20have%20previously%20been%20scarcely%20considered%3A%0Atraining%20data%20diversity%20and%20scale%2C%20model%20architectures%20and%20training%20recipes%2C%0Aand%20the%20breadth%20of%20evaluation%20tasks%20and%20datasets.%20We%20obtain%20encoders%20that%20are%0Astate-of-the-art%20on%20the%20existing%20and%20proposed%20benchmarks.%20We%20also%20identify%20what%0Amatters%20for%20training%20these%20encoders%2C%20such%20that%20this%20work%20can%20be%20extended%20when%0Amore%20data%20are%20available%20or%20better%20architectures%20are%20proposed.%20Specifically%2C%0Aacross%2026%20datasets%20with%20tasks%20including%20species%20classification%2C%20detection%2C%0Aindividual%20ID%2C%20and%20vocal%20repertoire%20discovery%2C%20we%20find%20self-supervised%0Apre-training%20followed%20by%20supervised%20post-training%20on%20a%20mixed%20bioacoustics%20%2B%0Ageneral-audio%20corpus%20yields%20the%20strongest%20in-%20and%20out-of-distribution%0Aperformance.%20We%20show%20the%20importance%20of%20data%20diversity%20in%20both%20stages.%20To%0Asupport%20ongoing%20research%20and%20application%2C%20we%20will%20release%20the%20model%0Acheckpoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11845v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Matters%2520for%2520Bioacoustic%2520Encoding%26entry.906535625%3DMarius%2520Miron%2520and%2520David%2520Robinson%2520and%2520Milad%2520Alizadeh%2520and%2520Ellen%2520Gilsenan-McMahon%2520and%2520Gagan%2520Narula%2520and%2520Emmanuel%2520Chemla%2520and%2520Maddie%2520Cusimano%2520and%2520Felix%2520Effenberger%2520and%2520Masato%2520Hagiwara%2520and%2520Benjamin%2520Hoffman%2520and%2520Sara%2520Keen%2520and%2520Diane%2520Kim%2520and%2520Jane%2520Lawton%2520and%2520Jen-Yu%2520Liu%2520and%2520Aza%2520Raskin%2520and%2520Olivier%2520Pietquin%2520and%2520Matthieu%2520Geist%26entry.1292438233%3D%2520%2520Bioacoustics%252C%2520the%2520study%2520of%2520sounds%2520produced%2520by%2520living%2520organisms%252C%2520plays%2520a%2520vital%250Arole%2520in%2520conservation%252C%2520biodiversity%2520monitoring%252C%2520and%2520behavioral%2520studies.%2520Many%250Atasks%2520in%2520this%2520field%252C%2520such%2520as%2520species%252C%2520individual%252C%2520and%2520behavior%2520classification%250Aand%2520detection%252C%2520are%2520well-suited%2520to%2520machine%2520learning.%2520However%252C%2520they%2520often%2520suffer%250Afrom%2520limited%2520annotated%2520data%252C%2520highlighting%2520the%2520need%2520for%2520a%2520general-purpose%250Abioacoustic%2520encoder%2520capable%2520of%2520extracting%2520useful%2520representations%2520for%2520diverse%250Adownstream%2520tasks.%2520Such%2520encoders%2520have%2520been%2520proposed%2520before%252C%2520but%2520are%2520often%250Alimited%2520in%2520scope%2520due%2520to%2520a%2520focus%2520on%2520a%2520narrow%2520range%2520of%2520species%2520%2528typically%2520birds%2529%252C%250Aand%2520a%2520reliance%2520on%2520a%2520single%2520model%2520architecture%2520or%2520training%2520paradigm.%2520Moreover%252C%250Athey%2520are%2520usually%2520evaluated%2520on%2520a%2520small%2520set%2520of%2520tasks%2520and%2520datasets.%2520In%2520this%2520work%252C%250Awe%2520present%2520a%2520large-scale%2520empirical%2520study%2520that%2520covers%2520aspects%2520of%2520bioacoustics%250Athat%2520are%2520relevant%2520to%2520research%2520but%2520have%2520previously%2520been%2520scarcely%2520considered%253A%250Atraining%2520data%2520diversity%2520and%2520scale%252C%2520model%2520architectures%2520and%2520training%2520recipes%252C%250Aand%2520the%2520breadth%2520of%2520evaluation%2520tasks%2520and%2520datasets.%2520We%2520obtain%2520encoders%2520that%2520are%250Astate-of-the-art%2520on%2520the%2520existing%2520and%2520proposed%2520benchmarks.%2520We%2520also%2520identify%2520what%250Amatters%2520for%2520training%2520these%2520encoders%252C%2520such%2520that%2520this%2520work%2520can%2520be%2520extended%2520when%250Amore%2520data%2520are%2520available%2520or%2520better%2520architectures%2520are%2520proposed.%2520Specifically%252C%250Aacross%252026%2520datasets%2520with%2520tasks%2520including%2520species%2520classification%252C%2520detection%252C%250Aindividual%2520ID%252C%2520and%2520vocal%2520repertoire%2520discovery%252C%2520we%2520find%2520self-supervised%250Apre-training%2520followed%2520by%2520supervised%2520post-training%2520on%2520a%2520mixed%2520bioacoustics%2520%252B%250Ageneral-audio%2520corpus%2520yields%2520the%2520strongest%2520in-%2520and%2520out-of-distribution%250Aperformance.%2520We%2520show%2520the%2520importance%2520of%2520data%2520diversity%2520in%2520both%2520stages.%2520To%250Asupport%2520ongoing%2520research%2520and%2520application%252C%2520we%2520will%2520release%2520the%2520model%250Acheckpoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11845v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Matters%20for%20Bioacoustic%20Encoding&entry.906535625=Marius%20Miron%20and%20David%20Robinson%20and%20Milad%20Alizadeh%20and%20Ellen%20Gilsenan-McMahon%20and%20Gagan%20Narula%20and%20Emmanuel%20Chemla%20and%20Maddie%20Cusimano%20and%20Felix%20Effenberger%20and%20Masato%20Hagiwara%20and%20Benjamin%20Hoffman%20and%20Sara%20Keen%20and%20Diane%20Kim%20and%20Jane%20Lawton%20and%20Jen-Yu%20Liu%20and%20Aza%20Raskin%20and%20Olivier%20Pietquin%20and%20Matthieu%20Geist&entry.1292438233=%20%20Bioacoustics%2C%20the%20study%20of%20sounds%20produced%20by%20living%20organisms%2C%20plays%20a%20vital%0Arole%20in%20conservation%2C%20biodiversity%20monitoring%2C%20and%20behavioral%20studies.%20Many%0Atasks%20in%20this%20field%2C%20such%20as%20species%2C%20individual%2C%20and%20behavior%20classification%0Aand%20detection%2C%20are%20well-suited%20to%20machine%20learning.%20However%2C%20they%20often%20suffer%0Afrom%20limited%20annotated%20data%2C%20highlighting%20the%20need%20for%20a%20general-purpose%0Abioacoustic%20encoder%20capable%20of%20extracting%20useful%20representations%20for%20diverse%0Adownstream%20tasks.%20Such%20encoders%20have%20been%20proposed%20before%2C%20but%20are%20often%0Alimited%20in%20scope%20due%20to%20a%20focus%20on%20a%20narrow%20range%20of%20species%20%28typically%20birds%29%2C%0Aand%20a%20reliance%20on%20a%20single%20model%20architecture%20or%20training%20paradigm.%20Moreover%2C%0Athey%20are%20usually%20evaluated%20on%20a%20small%20set%20of%20tasks%20and%20datasets.%20In%20this%20work%2C%0Awe%20present%20a%20large-scale%20empirical%20study%20that%20covers%20aspects%20of%20bioacoustics%0Athat%20are%20relevant%20to%20research%20but%20have%20previously%20been%20scarcely%20considered%3A%0Atraining%20data%20diversity%20and%20scale%2C%20model%20architectures%20and%20training%20recipes%2C%0Aand%20the%20breadth%20of%20evaluation%20tasks%20and%20datasets.%20We%20obtain%20encoders%20that%20are%0Astate-of-the-art%20on%20the%20existing%20and%20proposed%20benchmarks.%20We%20also%20identify%20what%0Amatters%20for%20training%20these%20encoders%2C%20such%20that%20this%20work%20can%20be%20extended%20when%0Amore%20data%20are%20available%20or%20better%20architectures%20are%20proposed.%20Specifically%2C%0Aacross%2026%20datasets%20with%20tasks%20including%20species%20classification%2C%20detection%2C%0Aindividual%20ID%2C%20and%20vocal%20repertoire%20discovery%2C%20we%20find%20self-supervised%0Apre-training%20followed%20by%20supervised%20post-training%20on%20a%20mixed%20bioacoustics%20%2B%0Ageneral-audio%20corpus%20yields%20the%20strongest%20in-%20and%20out-of-distribution%0Aperformance.%20We%20show%20the%20importance%20of%20data%20diversity%20in%20both%20stages.%20To%0Asupport%20ongoing%20research%20and%20application%2C%20we%20will%20release%20the%20model%0Acheckpoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11845v2&entry.124074799=Read"},
{"title": "Disentangled Representation Learning with the Gromov-Monge Gap", "author": "Th\u00e9o Uscidda and Luca Eyring and Karsten Roth and Fabian Theis and Zeynep Akata and Marco Cuturi", "abstract": "  Learning disentangled representations from unlabelled data is a fundamental\nchallenge in machine learning. Solving it may unlock other problems, such as\ngeneralization, interpretability, or fairness. Although remarkably challenging\nto solve in theory, disentanglement is often achieved in practice through prior\nmatching. Furthermore, recent works have shown that prior matching approaches\ncan be enhanced by leveraging geometrical considerations, e.g., by learning\nrepresentations that preserve geometric features of the data, such as distances\nor angles between points. However, matching the prior while preserving\ngeometric features is challenging, as a mapping that fully preserves these\nfeatures while aligning the data distribution with the prior does not exist in\ngeneral. To address these challenges, we introduce a novel approach to\ndisentangled representation learning based on quadratic optimal transport. We\nformulate the problem using Gromov-Monge maps that transport one distribution\nonto another with minimal distortion of predefined geometric features,\npreserving them as much as can be achieved. To compute such maps, we propose\nthe Gromov-Monge-Gap (GMG), a regularizer quantifying whether a map moves a\nreference distribution with minimal geometry distortion. We demonstrate the\neffectiveness of our approach for disentanglement across four standard\nbenchmarks, outperforming other methods leveraging geometric considerations.\n", "link": "http://arxiv.org/abs/2407.07829v3", "date": "2025-08-19", "relevancy": 2.6921, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5463}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5392}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Representation%20Learning%20with%20the%20Gromov-Monge%20Gap&body=Title%3A%20Disentangled%20Representation%20Learning%20with%20the%20Gromov-Monge%20Gap%0AAuthor%3A%20Th%C3%A9o%20Uscidda%20and%20Luca%20Eyring%20and%20Karsten%20Roth%20and%20Fabian%20Theis%20and%20Zeynep%20Akata%20and%20Marco%20Cuturi%0AAbstract%3A%20%20%20Learning%20disentangled%20representations%20from%20unlabelled%20data%20is%20a%20fundamental%0Achallenge%20in%20machine%20learning.%20Solving%20it%20may%20unlock%20other%20problems%2C%20such%20as%0Ageneralization%2C%20interpretability%2C%20or%20fairness.%20Although%20remarkably%20challenging%0Ato%20solve%20in%20theory%2C%20disentanglement%20is%20often%20achieved%20in%20practice%20through%20prior%0Amatching.%20Furthermore%2C%20recent%20works%20have%20shown%20that%20prior%20matching%20approaches%0Acan%20be%20enhanced%20by%20leveraging%20geometrical%20considerations%2C%20e.g.%2C%20by%20learning%0Arepresentations%20that%20preserve%20geometric%20features%20of%20the%20data%2C%20such%20as%20distances%0Aor%20angles%20between%20points.%20However%2C%20matching%20the%20prior%20while%20preserving%0Ageometric%20features%20is%20challenging%2C%20as%20a%20mapping%20that%20fully%20preserves%20these%0Afeatures%20while%20aligning%20the%20data%20distribution%20with%20the%20prior%20does%20not%20exist%20in%0Ageneral.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20approach%20to%0Adisentangled%20representation%20learning%20based%20on%20quadratic%20optimal%20transport.%20We%0Aformulate%20the%20problem%20using%20Gromov-Monge%20maps%20that%20transport%20one%20distribution%0Aonto%20another%20with%20minimal%20distortion%20of%20predefined%20geometric%20features%2C%0Apreserving%20them%20as%20much%20as%20can%20be%20achieved.%20To%20compute%20such%20maps%2C%20we%20propose%0Athe%20Gromov-Monge-Gap%20%28GMG%29%2C%20a%20regularizer%20quantifying%20whether%20a%20map%20moves%20a%0Areference%20distribution%20with%20minimal%20geometry%20distortion.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20for%20disentanglement%20across%20four%20standard%0Abenchmarks%2C%20outperforming%20other%20methods%20leveraging%20geometric%20considerations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07829v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Representation%2520Learning%2520with%2520the%2520Gromov-Monge%2520Gap%26entry.906535625%3DTh%25C3%25A9o%2520Uscidda%2520and%2520Luca%2520Eyring%2520and%2520Karsten%2520Roth%2520and%2520Fabian%2520Theis%2520and%2520Zeynep%2520Akata%2520and%2520Marco%2520Cuturi%26entry.1292438233%3D%2520%2520Learning%2520disentangled%2520representations%2520from%2520unlabelled%2520data%2520is%2520a%2520fundamental%250Achallenge%2520in%2520machine%2520learning.%2520Solving%2520it%2520may%2520unlock%2520other%2520problems%252C%2520such%2520as%250Ageneralization%252C%2520interpretability%252C%2520or%2520fairness.%2520Although%2520remarkably%2520challenging%250Ato%2520solve%2520in%2520theory%252C%2520disentanglement%2520is%2520often%2520achieved%2520in%2520practice%2520through%2520prior%250Amatching.%2520Furthermore%252C%2520recent%2520works%2520have%2520shown%2520that%2520prior%2520matching%2520approaches%250Acan%2520be%2520enhanced%2520by%2520leveraging%2520geometrical%2520considerations%252C%2520e.g.%252C%2520by%2520learning%250Arepresentations%2520that%2520preserve%2520geometric%2520features%2520of%2520the%2520data%252C%2520such%2520as%2520distances%250Aor%2520angles%2520between%2520points.%2520However%252C%2520matching%2520the%2520prior%2520while%2520preserving%250Ageometric%2520features%2520is%2520challenging%252C%2520as%2520a%2520mapping%2520that%2520fully%2520preserves%2520these%250Afeatures%2520while%2520aligning%2520the%2520data%2520distribution%2520with%2520the%2520prior%2520does%2520not%2520exist%2520in%250Ageneral.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520to%250Adisentangled%2520representation%2520learning%2520based%2520on%2520quadratic%2520optimal%2520transport.%2520We%250Aformulate%2520the%2520problem%2520using%2520Gromov-Monge%2520maps%2520that%2520transport%2520one%2520distribution%250Aonto%2520another%2520with%2520minimal%2520distortion%2520of%2520predefined%2520geometric%2520features%252C%250Apreserving%2520them%2520as%2520much%2520as%2520can%2520be%2520achieved.%2520To%2520compute%2520such%2520maps%252C%2520we%2520propose%250Athe%2520Gromov-Monge-Gap%2520%2528GMG%2529%252C%2520a%2520regularizer%2520quantifying%2520whether%2520a%2520map%2520moves%2520a%250Areference%2520distribution%2520with%2520minimal%2520geometry%2520distortion.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520for%2520disentanglement%2520across%2520four%2520standard%250Abenchmarks%252C%2520outperforming%2520other%2520methods%2520leveraging%2520geometric%2520considerations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07829v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Representation%20Learning%20with%20the%20Gromov-Monge%20Gap&entry.906535625=Th%C3%A9o%20Uscidda%20and%20Luca%20Eyring%20and%20Karsten%20Roth%20and%20Fabian%20Theis%20and%20Zeynep%20Akata%20and%20Marco%20Cuturi&entry.1292438233=%20%20Learning%20disentangled%20representations%20from%20unlabelled%20data%20is%20a%20fundamental%0Achallenge%20in%20machine%20learning.%20Solving%20it%20may%20unlock%20other%20problems%2C%20such%20as%0Ageneralization%2C%20interpretability%2C%20or%20fairness.%20Although%20remarkably%20challenging%0Ato%20solve%20in%20theory%2C%20disentanglement%20is%20often%20achieved%20in%20practice%20through%20prior%0Amatching.%20Furthermore%2C%20recent%20works%20have%20shown%20that%20prior%20matching%20approaches%0Acan%20be%20enhanced%20by%20leveraging%20geometrical%20considerations%2C%20e.g.%2C%20by%20learning%0Arepresentations%20that%20preserve%20geometric%20features%20of%20the%20data%2C%20such%20as%20distances%0Aor%20angles%20between%20points.%20However%2C%20matching%20the%20prior%20while%20preserving%0Ageometric%20features%20is%20challenging%2C%20as%20a%20mapping%20that%20fully%20preserves%20these%0Afeatures%20while%20aligning%20the%20data%20distribution%20with%20the%20prior%20does%20not%20exist%20in%0Ageneral.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20approach%20to%0Adisentangled%20representation%20learning%20based%20on%20quadratic%20optimal%20transport.%20We%0Aformulate%20the%20problem%20using%20Gromov-Monge%20maps%20that%20transport%20one%20distribution%0Aonto%20another%20with%20minimal%20distortion%20of%20predefined%20geometric%20features%2C%0Apreserving%20them%20as%20much%20as%20can%20be%20achieved.%20To%20compute%20such%20maps%2C%20we%20propose%0Athe%20Gromov-Monge-Gap%20%28GMG%29%2C%20a%20regularizer%20quantifying%20whether%20a%20map%20moves%20a%0Areference%20distribution%20with%20minimal%20geometry%20distortion.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20for%20disentanglement%20across%20four%20standard%0Abenchmarks%2C%20outperforming%20other%20methods%20leveraging%20geometric%20considerations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07829v3&entry.124074799=Read"},
{"title": "RadGPT: Constructing 3D Image-Text Tumor Datasets", "author": "Pedro R. A. S. Bassi and Mehmet Can Yavuz and Kang Wang and Xiaoxi Chen and Wenxuan Li and Sergio Decherchi and Andrea Cavalli and Yang Yang and Alan Yuille and Zongwei Zhou", "abstract": "  Cancers identified in CT scans are usually accompanied by detailed radiology\nreports, but publicly available CT datasets often lack these essential reports.\nThis absence limits their usefulness for developing accurate report generation\nAI. To address this gap, we present AbdomenAtlas 3.0, the first public,\nhigh-quality abdominal CT dataset with detailed, expert-reviewed radiology\nreports. All reports are paired with per-voxel masks and they describe liver,\nkidney and pancreatic tumors. AbdomenAtlas 3.0 has 9,262 triplets of CT, mask\nand report--3,955 with tumors. These CT scans come from 17 public datasets.\nBesides creating the reports for these datasets, we expanded their number of\ntumor masks by 4.2x, identifying 3,011 new tumor cases. Notably, the reports in\nAbdomenAtlas 3.0 are more standardized, and generated faster than traditional\nhuman-made reports. They provide details like tumor size, location, attenuation\nand surgical resectability. These reports were created by 12 board-certified\nradiologists using our proposed RadGPT, a novel framework that converted\nradiologist-revised tumor segmentation masks into structured and narrative\nreports. Besides being a dataset creation tool, RadGPT can also become a\nfully-automatic, segmentation-assisted report generation method. We benchmarked\nthis method and 5 state-of-the-art report generation vision-language models.\nOur results show that segmentation strongly improves tumor detection in AI-made\nreports.\n", "link": "http://arxiv.org/abs/2501.04678v2", "date": "2025-08-19", "relevancy": 2.6813, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5376}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5356}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RadGPT%3A%20Constructing%203D%20Image-Text%20Tumor%20Datasets&body=Title%3A%20RadGPT%3A%20Constructing%203D%20Image-Text%20Tumor%20Datasets%0AAuthor%3A%20Pedro%20R.%20A.%20S.%20Bassi%20and%20Mehmet%20Can%20Yavuz%20and%20Kang%20Wang%20and%20Xiaoxi%20Chen%20and%20Wenxuan%20Li%20and%20Sergio%20Decherchi%20and%20Andrea%20Cavalli%20and%20Yang%20Yang%20and%20Alan%20Yuille%20and%20Zongwei%20Zhou%0AAbstract%3A%20%20%20Cancers%20identified%20in%20CT%20scans%20are%20usually%20accompanied%20by%20detailed%20radiology%0Areports%2C%20but%20publicly%20available%20CT%20datasets%20often%20lack%20these%20essential%20reports.%0AThis%20absence%20limits%20their%20usefulness%20for%20developing%20accurate%20report%20generation%0AAI.%20To%20address%20this%20gap%2C%20we%20present%20AbdomenAtlas%203.0%2C%20the%20first%20public%2C%0Ahigh-quality%20abdominal%20CT%20dataset%20with%20detailed%2C%20expert-reviewed%20radiology%0Areports.%20All%20reports%20are%20paired%20with%20per-voxel%20masks%20and%20they%20describe%20liver%2C%0Akidney%20and%20pancreatic%20tumors.%20AbdomenAtlas%203.0%20has%209%2C262%20triplets%20of%20CT%2C%20mask%0Aand%20report--3%2C955%20with%20tumors.%20These%20CT%20scans%20come%20from%2017%20public%20datasets.%0ABesides%20creating%20the%20reports%20for%20these%20datasets%2C%20we%20expanded%20their%20number%20of%0Atumor%20masks%20by%204.2x%2C%20identifying%203%2C011%20new%20tumor%20cases.%20Notably%2C%20the%20reports%20in%0AAbdomenAtlas%203.0%20are%20more%20standardized%2C%20and%20generated%20faster%20than%20traditional%0Ahuman-made%20reports.%20They%20provide%20details%20like%20tumor%20size%2C%20location%2C%20attenuation%0Aand%20surgical%20resectability.%20These%20reports%20were%20created%20by%2012%20board-certified%0Aradiologists%20using%20our%20proposed%20RadGPT%2C%20a%20novel%20framework%20that%20converted%0Aradiologist-revised%20tumor%20segmentation%20masks%20into%20structured%20and%20narrative%0Areports.%20Besides%20being%20a%20dataset%20creation%20tool%2C%20RadGPT%20can%20also%20become%20a%0Afully-automatic%2C%20segmentation-assisted%20report%20generation%20method.%20We%20benchmarked%0Athis%20method%20and%205%20state-of-the-art%20report%20generation%20vision-language%20models.%0AOur%20results%20show%20that%20segmentation%20strongly%20improves%20tumor%20detection%20in%20AI-made%0Areports.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04678v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadGPT%253A%2520Constructing%25203D%2520Image-Text%2520Tumor%2520Datasets%26entry.906535625%3DPedro%2520R.%2520A.%2520S.%2520Bassi%2520and%2520Mehmet%2520Can%2520Yavuz%2520and%2520Kang%2520Wang%2520and%2520Xiaoxi%2520Chen%2520and%2520Wenxuan%2520Li%2520and%2520Sergio%2520Decherchi%2520and%2520Andrea%2520Cavalli%2520and%2520Yang%2520Yang%2520and%2520Alan%2520Yuille%2520and%2520Zongwei%2520Zhou%26entry.1292438233%3D%2520%2520Cancers%2520identified%2520in%2520CT%2520scans%2520are%2520usually%2520accompanied%2520by%2520detailed%2520radiology%250Areports%252C%2520but%2520publicly%2520available%2520CT%2520datasets%2520often%2520lack%2520these%2520essential%2520reports.%250AThis%2520absence%2520limits%2520their%2520usefulness%2520for%2520developing%2520accurate%2520report%2520generation%250AAI.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520AbdomenAtlas%25203.0%252C%2520the%2520first%2520public%252C%250Ahigh-quality%2520abdominal%2520CT%2520dataset%2520with%2520detailed%252C%2520expert-reviewed%2520radiology%250Areports.%2520All%2520reports%2520are%2520paired%2520with%2520per-voxel%2520masks%2520and%2520they%2520describe%2520liver%252C%250Akidney%2520and%2520pancreatic%2520tumors.%2520AbdomenAtlas%25203.0%2520has%25209%252C262%2520triplets%2520of%2520CT%252C%2520mask%250Aand%2520report--3%252C955%2520with%2520tumors.%2520These%2520CT%2520scans%2520come%2520from%252017%2520public%2520datasets.%250ABesides%2520creating%2520the%2520reports%2520for%2520these%2520datasets%252C%2520we%2520expanded%2520their%2520number%2520of%250Atumor%2520masks%2520by%25204.2x%252C%2520identifying%25203%252C011%2520new%2520tumor%2520cases.%2520Notably%252C%2520the%2520reports%2520in%250AAbdomenAtlas%25203.0%2520are%2520more%2520standardized%252C%2520and%2520generated%2520faster%2520than%2520traditional%250Ahuman-made%2520reports.%2520They%2520provide%2520details%2520like%2520tumor%2520size%252C%2520location%252C%2520attenuation%250Aand%2520surgical%2520resectability.%2520These%2520reports%2520were%2520created%2520by%252012%2520board-certified%250Aradiologists%2520using%2520our%2520proposed%2520RadGPT%252C%2520a%2520novel%2520framework%2520that%2520converted%250Aradiologist-revised%2520tumor%2520segmentation%2520masks%2520into%2520structured%2520and%2520narrative%250Areports.%2520Besides%2520being%2520a%2520dataset%2520creation%2520tool%252C%2520RadGPT%2520can%2520also%2520become%2520a%250Afully-automatic%252C%2520segmentation-assisted%2520report%2520generation%2520method.%2520We%2520benchmarked%250Athis%2520method%2520and%25205%2520state-of-the-art%2520report%2520generation%2520vision-language%2520models.%250AOur%2520results%2520show%2520that%2520segmentation%2520strongly%2520improves%2520tumor%2520detection%2520in%2520AI-made%250Areports.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04678v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadGPT%3A%20Constructing%203D%20Image-Text%20Tumor%20Datasets&entry.906535625=Pedro%20R.%20A.%20S.%20Bassi%20and%20Mehmet%20Can%20Yavuz%20and%20Kang%20Wang%20and%20Xiaoxi%20Chen%20and%20Wenxuan%20Li%20and%20Sergio%20Decherchi%20and%20Andrea%20Cavalli%20and%20Yang%20Yang%20and%20Alan%20Yuille%20and%20Zongwei%20Zhou&entry.1292438233=%20%20Cancers%20identified%20in%20CT%20scans%20are%20usually%20accompanied%20by%20detailed%20radiology%0Areports%2C%20but%20publicly%20available%20CT%20datasets%20often%20lack%20these%20essential%20reports.%0AThis%20absence%20limits%20their%20usefulness%20for%20developing%20accurate%20report%20generation%0AAI.%20To%20address%20this%20gap%2C%20we%20present%20AbdomenAtlas%203.0%2C%20the%20first%20public%2C%0Ahigh-quality%20abdominal%20CT%20dataset%20with%20detailed%2C%20expert-reviewed%20radiology%0Areports.%20All%20reports%20are%20paired%20with%20per-voxel%20masks%20and%20they%20describe%20liver%2C%0Akidney%20and%20pancreatic%20tumors.%20AbdomenAtlas%203.0%20has%209%2C262%20triplets%20of%20CT%2C%20mask%0Aand%20report--3%2C955%20with%20tumors.%20These%20CT%20scans%20come%20from%2017%20public%20datasets.%0ABesides%20creating%20the%20reports%20for%20these%20datasets%2C%20we%20expanded%20their%20number%20of%0Atumor%20masks%20by%204.2x%2C%20identifying%203%2C011%20new%20tumor%20cases.%20Notably%2C%20the%20reports%20in%0AAbdomenAtlas%203.0%20are%20more%20standardized%2C%20and%20generated%20faster%20than%20traditional%0Ahuman-made%20reports.%20They%20provide%20details%20like%20tumor%20size%2C%20location%2C%20attenuation%0Aand%20surgical%20resectability.%20These%20reports%20were%20created%20by%2012%20board-certified%0Aradiologists%20using%20our%20proposed%20RadGPT%2C%20a%20novel%20framework%20that%20converted%0Aradiologist-revised%20tumor%20segmentation%20masks%20into%20structured%20and%20narrative%0Areports.%20Besides%20being%20a%20dataset%20creation%20tool%2C%20RadGPT%20can%20also%20become%20a%0Afully-automatic%2C%20segmentation-assisted%20report%20generation%20method.%20We%20benchmarked%0Athis%20method%20and%205%20state-of-the-art%20report%20generation%20vision-language%20models.%0AOur%20results%20show%20that%20segmentation%20strongly%20improves%20tumor%20detection%20in%20AI-made%0Areports.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04678v2&entry.124074799=Read"},
{"title": "Incorporating Attributes and Multi-Scale Structures for Heterogeneous\n  Graph Contrastive Learning", "author": "Ruobing Jiang and Yacong Li and Haobing Liu and Yanwei Yu", "abstract": "  Heterogeneous graphs (HGs) are composed of multiple types of nodes and edges,\nmaking it more effective in capturing the complex relational structures\ninherent in the real world. However, in real-world scenarios, labeled data is\noften difficult to obtain, which limits the applicability of semi-supervised\napproaches. Self-supervised learning aims to enable models to automatically\nlearn useful features from data, effectively addressing the challenge of\nlimited labeling data. In this paper, we propose a novel contrastive learning\nframework for heterogeneous graphs (ASHGCL), which incorporates three distinct\nviews, each focusing on node attributes, high-order and low-order structural\ninformation, respectively, to effectively capture attribute information,\nhigh-order structures, and low-order structures for node representation\nlearning. Furthermore, we introduce an attribute-enhanced positive sample\nselection strategy that combines both structural information and attribute\ninformation, effectively addressing the issue of sampling bias. Extensive\nexperiments on four real-world datasets show that ASHGCL outperforms\nstate-of-the-art unsupervised baselines and even surpasses some supervised\nbenchmarks.\n", "link": "http://arxiv.org/abs/2503.13911v3", "date": "2025-08-19", "relevancy": 2.6354, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5625}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5137}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incorporating%20Attributes%20and%20Multi-Scale%20Structures%20for%20Heterogeneous%0A%20%20Graph%20Contrastive%20Learning&body=Title%3A%20Incorporating%20Attributes%20and%20Multi-Scale%20Structures%20for%20Heterogeneous%0A%20%20Graph%20Contrastive%20Learning%0AAuthor%3A%20Ruobing%20Jiang%20and%20Yacong%20Li%20and%20Haobing%20Liu%20and%20Yanwei%20Yu%0AAbstract%3A%20%20%20Heterogeneous%20graphs%20%28HGs%29%20are%20composed%20of%20multiple%20types%20of%20nodes%20and%20edges%2C%0Amaking%20it%20more%20effective%20in%20capturing%20the%20complex%20relational%20structures%0Ainherent%20in%20the%20real%20world.%20However%2C%20in%20real-world%20scenarios%2C%20labeled%20data%20is%0Aoften%20difficult%20to%20obtain%2C%20which%20limits%20the%20applicability%20of%20semi-supervised%0Aapproaches.%20Self-supervised%20learning%20aims%20to%20enable%20models%20to%20automatically%0Alearn%20useful%20features%20from%20data%2C%20effectively%20addressing%20the%20challenge%20of%0Alimited%20labeling%20data.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20contrastive%20learning%0Aframework%20for%20heterogeneous%20graphs%20%28ASHGCL%29%2C%20which%20incorporates%20three%20distinct%0Aviews%2C%20each%20focusing%20on%20node%20attributes%2C%20high-order%20and%20low-order%20structural%0Ainformation%2C%20respectively%2C%20to%20effectively%20capture%20attribute%20information%2C%0Ahigh-order%20structures%2C%20and%20low-order%20structures%20for%20node%20representation%0Alearning.%20Furthermore%2C%20we%20introduce%20an%20attribute-enhanced%20positive%20sample%0Aselection%20strategy%20that%20combines%20both%20structural%20information%20and%20attribute%0Ainformation%2C%20effectively%20addressing%20the%20issue%20of%20sampling%20bias.%20Extensive%0Aexperiments%20on%20four%20real-world%20datasets%20show%20that%20ASHGCL%20outperforms%0Astate-of-the-art%20unsupervised%20baselines%20and%20even%20surpasses%20some%20supervised%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.13911v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncorporating%2520Attributes%2520and%2520Multi-Scale%2520Structures%2520for%2520Heterogeneous%250A%2520%2520Graph%2520Contrastive%2520Learning%26entry.906535625%3DRuobing%2520Jiang%2520and%2520Yacong%2520Li%2520and%2520Haobing%2520Liu%2520and%2520Yanwei%2520Yu%26entry.1292438233%3D%2520%2520Heterogeneous%2520graphs%2520%2528HGs%2529%2520are%2520composed%2520of%2520multiple%2520types%2520of%2520nodes%2520and%2520edges%252C%250Amaking%2520it%2520more%2520effective%2520in%2520capturing%2520the%2520complex%2520relational%2520structures%250Ainherent%2520in%2520the%2520real%2520world.%2520However%252C%2520in%2520real-world%2520scenarios%252C%2520labeled%2520data%2520is%250Aoften%2520difficult%2520to%2520obtain%252C%2520which%2520limits%2520the%2520applicability%2520of%2520semi-supervised%250Aapproaches.%2520Self-supervised%2520learning%2520aims%2520to%2520enable%2520models%2520to%2520automatically%250Alearn%2520useful%2520features%2520from%2520data%252C%2520effectively%2520addressing%2520the%2520challenge%2520of%250Alimited%2520labeling%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520contrastive%2520learning%250Aframework%2520for%2520heterogeneous%2520graphs%2520%2528ASHGCL%2529%252C%2520which%2520incorporates%2520three%2520distinct%250Aviews%252C%2520each%2520focusing%2520on%2520node%2520attributes%252C%2520high-order%2520and%2520low-order%2520structural%250Ainformation%252C%2520respectively%252C%2520to%2520effectively%2520capture%2520attribute%2520information%252C%250Ahigh-order%2520structures%252C%2520and%2520low-order%2520structures%2520for%2520node%2520representation%250Alearning.%2520Furthermore%252C%2520we%2520introduce%2520an%2520attribute-enhanced%2520positive%2520sample%250Aselection%2520strategy%2520that%2520combines%2520both%2520structural%2520information%2520and%2520attribute%250Ainformation%252C%2520effectively%2520addressing%2520the%2520issue%2520of%2520sampling%2520bias.%2520Extensive%250Aexperiments%2520on%2520four%2520real-world%2520datasets%2520show%2520that%2520ASHGCL%2520outperforms%250Astate-of-the-art%2520unsupervised%2520baselines%2520and%2520even%2520surpasses%2520some%2520supervised%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.13911v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Attributes%20and%20Multi-Scale%20Structures%20for%20Heterogeneous%0A%20%20Graph%20Contrastive%20Learning&entry.906535625=Ruobing%20Jiang%20and%20Yacong%20Li%20and%20Haobing%20Liu%20and%20Yanwei%20Yu&entry.1292438233=%20%20Heterogeneous%20graphs%20%28HGs%29%20are%20composed%20of%20multiple%20types%20of%20nodes%20and%20edges%2C%0Amaking%20it%20more%20effective%20in%20capturing%20the%20complex%20relational%20structures%0Ainherent%20in%20the%20real%20world.%20However%2C%20in%20real-world%20scenarios%2C%20labeled%20data%20is%0Aoften%20difficult%20to%20obtain%2C%20which%20limits%20the%20applicability%20of%20semi-supervised%0Aapproaches.%20Self-supervised%20learning%20aims%20to%20enable%20models%20to%20automatically%0Alearn%20useful%20features%20from%20data%2C%20effectively%20addressing%20the%20challenge%20of%0Alimited%20labeling%20data.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20contrastive%20learning%0Aframework%20for%20heterogeneous%20graphs%20%28ASHGCL%29%2C%20which%20incorporates%20three%20distinct%0Aviews%2C%20each%20focusing%20on%20node%20attributes%2C%20high-order%20and%20low-order%20structural%0Ainformation%2C%20respectively%2C%20to%20effectively%20capture%20attribute%20information%2C%0Ahigh-order%20structures%2C%20and%20low-order%20structures%20for%20node%20representation%0Alearning.%20Furthermore%2C%20we%20introduce%20an%20attribute-enhanced%20positive%20sample%0Aselection%20strategy%20that%20combines%20both%20structural%20information%20and%20attribute%0Ainformation%2C%20effectively%20addressing%20the%20issue%20of%20sampling%20bias.%20Extensive%0Aexperiments%20on%20four%20real-world%20datasets%20show%20that%20ASHGCL%20outperforms%0Astate-of-the-art%20unsupervised%20baselines%20and%20even%20surpasses%20some%20supervised%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.13911v3&entry.124074799=Read"},
{"title": "Backdooring Self-Supervised Contrastive Learning by Noisy Alignment", "author": "Tuo Chen and Jie Gui and Minjing Dong and Ju Jia and Lanting Fang and Jian Liu", "abstract": "  Self-supervised contrastive learning (CL) effectively learns transferable\nrepresentations from unlabeled data containing images or image-text pairs but\nsuffers vulnerability to data poisoning backdoor attacks (DPCLs). An adversary\ncan inject poisoned images into pretraining datasets, causing compromised CL\nencoders to exhibit targeted misbehavior in downstream tasks. Existing DPCLs,\nhowever, achieve limited efficacy due to their dependence on fragile implicit\nco-occurrence between backdoor and target object and inadequate suppression of\ndiscriminative features in backdoored images. We propose Noisy Alignment (NA),\na DPCL method that explicitly suppresses noise components in poisoned images.\nInspired by powerful training-controllable CL attacks, we identify and extract\nthe critical objective of noisy alignment, adapting it effectively into\ndata-poisoning scenarios. Our method implements noisy alignment by\nstrategically manipulating contrastive learning's random cropping mechanism,\nformulating this process as an image layout optimization problem with\ntheoretically derived optimal parameters. The resulting method is simple yet\neffective, achieving state-of-the-art performance compared to existing DPCLs,\nwhile maintaining clean-data accuracy. Furthermore, Noisy Alignment\ndemonstrates robustness against common backdoor defenses. Codes can be found at\nhttps://github.com/jsrdcht/Noisy-Alignment.\n", "link": "http://arxiv.org/abs/2508.14015v1", "date": "2025-08-19", "relevancy": 2.6057, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5414}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5158}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Backdooring%20Self-Supervised%20Contrastive%20Learning%20by%20Noisy%20Alignment&body=Title%3A%20Backdooring%20Self-Supervised%20Contrastive%20Learning%20by%20Noisy%20Alignment%0AAuthor%3A%20Tuo%20Chen%20and%20Jie%20Gui%20and%20Minjing%20Dong%20and%20Ju%20Jia%20and%20Lanting%20Fang%20and%20Jian%20Liu%0AAbstract%3A%20%20%20Self-supervised%20contrastive%20learning%20%28CL%29%20effectively%20learns%20transferable%0Arepresentations%20from%20unlabeled%20data%20containing%20images%20or%20image-text%20pairs%20but%0Asuffers%20vulnerability%20to%20data%20poisoning%20backdoor%20attacks%20%28DPCLs%29.%20An%20adversary%0Acan%20inject%20poisoned%20images%20into%20pretraining%20datasets%2C%20causing%20compromised%20CL%0Aencoders%20to%20exhibit%20targeted%20misbehavior%20in%20downstream%20tasks.%20Existing%20DPCLs%2C%0Ahowever%2C%20achieve%20limited%20efficacy%20due%20to%20their%20dependence%20on%20fragile%20implicit%0Aco-occurrence%20between%20backdoor%20and%20target%20object%20and%20inadequate%20suppression%20of%0Adiscriminative%20features%20in%20backdoored%20images.%20We%20propose%20Noisy%20Alignment%20%28NA%29%2C%0Aa%20DPCL%20method%20that%20explicitly%20suppresses%20noise%20components%20in%20poisoned%20images.%0AInspired%20by%20powerful%20training-controllable%20CL%20attacks%2C%20we%20identify%20and%20extract%0Athe%20critical%20objective%20of%20noisy%20alignment%2C%20adapting%20it%20effectively%20into%0Adata-poisoning%20scenarios.%20Our%20method%20implements%20noisy%20alignment%20by%0Astrategically%20manipulating%20contrastive%20learning%27s%20random%20cropping%20mechanism%2C%0Aformulating%20this%20process%20as%20an%20image%20layout%20optimization%20problem%20with%0Atheoretically%20derived%20optimal%20parameters.%20The%20resulting%20method%20is%20simple%20yet%0Aeffective%2C%20achieving%20state-of-the-art%20performance%20compared%20to%20existing%20DPCLs%2C%0Awhile%20maintaining%20clean-data%20accuracy.%20Furthermore%2C%20Noisy%20Alignment%0Ademonstrates%20robustness%20against%20common%20backdoor%20defenses.%20Codes%20can%20be%20found%20at%0Ahttps%3A//github.com/jsrdcht/Noisy-Alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackdooring%2520Self-Supervised%2520Contrastive%2520Learning%2520by%2520Noisy%2520Alignment%26entry.906535625%3DTuo%2520Chen%2520and%2520Jie%2520Gui%2520and%2520Minjing%2520Dong%2520and%2520Ju%2520Jia%2520and%2520Lanting%2520Fang%2520and%2520Jian%2520Liu%26entry.1292438233%3D%2520%2520Self-supervised%2520contrastive%2520learning%2520%2528CL%2529%2520effectively%2520learns%2520transferable%250Arepresentations%2520from%2520unlabeled%2520data%2520containing%2520images%2520or%2520image-text%2520pairs%2520but%250Asuffers%2520vulnerability%2520to%2520data%2520poisoning%2520backdoor%2520attacks%2520%2528DPCLs%2529.%2520An%2520adversary%250Acan%2520inject%2520poisoned%2520images%2520into%2520pretraining%2520datasets%252C%2520causing%2520compromised%2520CL%250Aencoders%2520to%2520exhibit%2520targeted%2520misbehavior%2520in%2520downstream%2520tasks.%2520Existing%2520DPCLs%252C%250Ahowever%252C%2520achieve%2520limited%2520efficacy%2520due%2520to%2520their%2520dependence%2520on%2520fragile%2520implicit%250Aco-occurrence%2520between%2520backdoor%2520and%2520target%2520object%2520and%2520inadequate%2520suppression%2520of%250Adiscriminative%2520features%2520in%2520backdoored%2520images.%2520We%2520propose%2520Noisy%2520Alignment%2520%2528NA%2529%252C%250Aa%2520DPCL%2520method%2520that%2520explicitly%2520suppresses%2520noise%2520components%2520in%2520poisoned%2520images.%250AInspired%2520by%2520powerful%2520training-controllable%2520CL%2520attacks%252C%2520we%2520identify%2520and%2520extract%250Athe%2520critical%2520objective%2520of%2520noisy%2520alignment%252C%2520adapting%2520it%2520effectively%2520into%250Adata-poisoning%2520scenarios.%2520Our%2520method%2520implements%2520noisy%2520alignment%2520by%250Astrategically%2520manipulating%2520contrastive%2520learning%2527s%2520random%2520cropping%2520mechanism%252C%250Aformulating%2520this%2520process%2520as%2520an%2520image%2520layout%2520optimization%2520problem%2520with%250Atheoretically%2520derived%2520optimal%2520parameters.%2520The%2520resulting%2520method%2520is%2520simple%2520yet%250Aeffective%252C%2520achieving%2520state-of-the-art%2520performance%2520compared%2520to%2520existing%2520DPCLs%252C%250Awhile%2520maintaining%2520clean-data%2520accuracy.%2520Furthermore%252C%2520Noisy%2520Alignment%250Ademonstrates%2520robustness%2520against%2520common%2520backdoor%2520defenses.%2520Codes%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/jsrdcht/Noisy-Alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Backdooring%20Self-Supervised%20Contrastive%20Learning%20by%20Noisy%20Alignment&entry.906535625=Tuo%20Chen%20and%20Jie%20Gui%20and%20Minjing%20Dong%20and%20Ju%20Jia%20and%20Lanting%20Fang%20and%20Jian%20Liu&entry.1292438233=%20%20Self-supervised%20contrastive%20learning%20%28CL%29%20effectively%20learns%20transferable%0Arepresentations%20from%20unlabeled%20data%20containing%20images%20or%20image-text%20pairs%20but%0Asuffers%20vulnerability%20to%20data%20poisoning%20backdoor%20attacks%20%28DPCLs%29.%20An%20adversary%0Acan%20inject%20poisoned%20images%20into%20pretraining%20datasets%2C%20causing%20compromised%20CL%0Aencoders%20to%20exhibit%20targeted%20misbehavior%20in%20downstream%20tasks.%20Existing%20DPCLs%2C%0Ahowever%2C%20achieve%20limited%20efficacy%20due%20to%20their%20dependence%20on%20fragile%20implicit%0Aco-occurrence%20between%20backdoor%20and%20target%20object%20and%20inadequate%20suppression%20of%0Adiscriminative%20features%20in%20backdoored%20images.%20We%20propose%20Noisy%20Alignment%20%28NA%29%2C%0Aa%20DPCL%20method%20that%20explicitly%20suppresses%20noise%20components%20in%20poisoned%20images.%0AInspired%20by%20powerful%20training-controllable%20CL%20attacks%2C%20we%20identify%20and%20extract%0Athe%20critical%20objective%20of%20noisy%20alignment%2C%20adapting%20it%20effectively%20into%0Adata-poisoning%20scenarios.%20Our%20method%20implements%20noisy%20alignment%20by%0Astrategically%20manipulating%20contrastive%20learning%27s%20random%20cropping%20mechanism%2C%0Aformulating%20this%20process%20as%20an%20image%20layout%20optimization%20problem%20with%0Atheoretically%20derived%20optimal%20parameters.%20The%20resulting%20method%20is%20simple%20yet%0Aeffective%2C%20achieving%20state-of-the-art%20performance%20compared%20to%20existing%20DPCLs%2C%0Awhile%20maintaining%20clean-data%20accuracy.%20Furthermore%2C%20Noisy%20Alignment%0Ademonstrates%20robustness%20against%20common%20backdoor%20defenses.%20Codes%20can%20be%20found%20at%0Ahttps%3A//github.com/jsrdcht/Noisy-Alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14015v1&entry.124074799=Read"},
{"title": "Vision Backbone Efficient Selection for Image Classification in Low-Data\n  Regimes", "author": "Joris Guerin and Shray Bansal and Amirreza Shaban and Paulo Mann and Harshvardhan Gazula", "abstract": "  Transfer learning has become an essential tool in modern computer vision,\nallowing practitioners to leverage backbones, pretrained on large datasets, to\ntrain successful models from limited annotated data. Choosing the right\nbackbone is crucial, especially for small datasets, since final performance\ndepends heavily on the quality of the initial feature representations. While\nprior work has conducted benchmarks across various datasets to identify\nuniversal top-performing backbones, we demonstrate that backbone effectiveness\nis highly dataset-dependent, especially in low-data scenarios where no single\nbackbone consistently excels. To overcome this limitation, we introduce\ndataset-specific backbone selection as a new research direction and investigate\nits practical viability in low-data regimes. Since exhaustive evaluation is\ncomputationally impractical for large backbone pools, we formalize Vision\nBackbone Efficient Selection (VIBES) as the problem of searching for\nhigh-performing backbones under computational constraints. We define the\nsolution space, propose several heuristics, and demonstrate VIBES feasibility\nfor low-data image classification by performing experiments on four diverse\ndatasets. Our results show that even simple search strategies can find\nwell-suited backbones within a pool of over $1300$ pretrained models,\noutperforming generic benchmark recommendations within just ten minutes of\nsearch time on a single GPU (NVIDIA RTX A5000).\n", "link": "http://arxiv.org/abs/2410.08592v2", "date": "2025-08-19", "relevancy": 2.5964, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Backbone%20Efficient%20Selection%20for%20Image%20Classification%20in%20Low-Data%0A%20%20Regimes&body=Title%3A%20Vision%20Backbone%20Efficient%20Selection%20for%20Image%20Classification%20in%20Low-Data%0A%20%20Regimes%0AAuthor%3A%20Joris%20Guerin%20and%20Shray%20Bansal%20and%20Amirreza%20Shaban%20and%20Paulo%20Mann%20and%20Harshvardhan%20Gazula%0AAbstract%3A%20%20%20Transfer%20learning%20has%20become%20an%20essential%20tool%20in%20modern%20computer%20vision%2C%0Aallowing%20practitioners%20to%20leverage%20backbones%2C%20pretrained%20on%20large%20datasets%2C%20to%0Atrain%20successful%20models%20from%20limited%20annotated%20data.%20Choosing%20the%20right%0Abackbone%20is%20crucial%2C%20especially%20for%20small%20datasets%2C%20since%20final%20performance%0Adepends%20heavily%20on%20the%20quality%20of%20the%20initial%20feature%20representations.%20While%0Aprior%20work%20has%20conducted%20benchmarks%20across%20various%20datasets%20to%20identify%0Auniversal%20top-performing%20backbones%2C%20we%20demonstrate%20that%20backbone%20effectiveness%0Ais%20highly%20dataset-dependent%2C%20especially%20in%20low-data%20scenarios%20where%20no%20single%0Abackbone%20consistently%20excels.%20To%20overcome%20this%20limitation%2C%20we%20introduce%0Adataset-specific%20backbone%20selection%20as%20a%20new%20research%20direction%20and%20investigate%0Aits%20practical%20viability%20in%20low-data%20regimes.%20Since%20exhaustive%20evaluation%20is%0Acomputationally%20impractical%20for%20large%20backbone%20pools%2C%20we%20formalize%20Vision%0ABackbone%20Efficient%20Selection%20%28VIBES%29%20as%20the%20problem%20of%20searching%20for%0Ahigh-performing%20backbones%20under%20computational%20constraints.%20We%20define%20the%0Asolution%20space%2C%20propose%20several%20heuristics%2C%20and%20demonstrate%20VIBES%20feasibility%0Afor%20low-data%20image%20classification%20by%20performing%20experiments%20on%20four%20diverse%0Adatasets.%20Our%20results%20show%20that%20even%20simple%20search%20strategies%20can%20find%0Awell-suited%20backbones%20within%20a%20pool%20of%20over%20%241300%24%20pretrained%20models%2C%0Aoutperforming%20generic%20benchmark%20recommendations%20within%20just%20ten%20minutes%20of%0Asearch%20time%20on%20a%20single%20GPU%20%28NVIDIA%20RTX%20A5000%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08592v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Backbone%2520Efficient%2520Selection%2520for%2520Image%2520Classification%2520in%2520Low-Data%250A%2520%2520Regimes%26entry.906535625%3DJoris%2520Guerin%2520and%2520Shray%2520Bansal%2520and%2520Amirreza%2520Shaban%2520and%2520Paulo%2520Mann%2520and%2520Harshvardhan%2520Gazula%26entry.1292438233%3D%2520%2520Transfer%2520learning%2520has%2520become%2520an%2520essential%2520tool%2520in%2520modern%2520computer%2520vision%252C%250Aallowing%2520practitioners%2520to%2520leverage%2520backbones%252C%2520pretrained%2520on%2520large%2520datasets%252C%2520to%250Atrain%2520successful%2520models%2520from%2520limited%2520annotated%2520data.%2520Choosing%2520the%2520right%250Abackbone%2520is%2520crucial%252C%2520especially%2520for%2520small%2520datasets%252C%2520since%2520final%2520performance%250Adepends%2520heavily%2520on%2520the%2520quality%2520of%2520the%2520initial%2520feature%2520representations.%2520While%250Aprior%2520work%2520has%2520conducted%2520benchmarks%2520across%2520various%2520datasets%2520to%2520identify%250Auniversal%2520top-performing%2520backbones%252C%2520we%2520demonstrate%2520that%2520backbone%2520effectiveness%250Ais%2520highly%2520dataset-dependent%252C%2520especially%2520in%2520low-data%2520scenarios%2520where%2520no%2520single%250Abackbone%2520consistently%2520excels.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%250Adataset-specific%2520backbone%2520selection%2520as%2520a%2520new%2520research%2520direction%2520and%2520investigate%250Aits%2520practical%2520viability%2520in%2520low-data%2520regimes.%2520Since%2520exhaustive%2520evaluation%2520is%250Acomputationally%2520impractical%2520for%2520large%2520backbone%2520pools%252C%2520we%2520formalize%2520Vision%250ABackbone%2520Efficient%2520Selection%2520%2528VIBES%2529%2520as%2520the%2520problem%2520of%2520searching%2520for%250Ahigh-performing%2520backbones%2520under%2520computational%2520constraints.%2520We%2520define%2520the%250Asolution%2520space%252C%2520propose%2520several%2520heuristics%252C%2520and%2520demonstrate%2520VIBES%2520feasibility%250Afor%2520low-data%2520image%2520classification%2520by%2520performing%2520experiments%2520on%2520four%2520diverse%250Adatasets.%2520Our%2520results%2520show%2520that%2520even%2520simple%2520search%2520strategies%2520can%2520find%250Awell-suited%2520backbones%2520within%2520a%2520pool%2520of%2520over%2520%25241300%2524%2520pretrained%2520models%252C%250Aoutperforming%2520generic%2520benchmark%2520recommendations%2520within%2520just%2520ten%2520minutes%2520of%250Asearch%2520time%2520on%2520a%2520single%2520GPU%2520%2528NVIDIA%2520RTX%2520A5000%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08592v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Backbone%20Efficient%20Selection%20for%20Image%20Classification%20in%20Low-Data%0A%20%20Regimes&entry.906535625=Joris%20Guerin%20and%20Shray%20Bansal%20and%20Amirreza%20Shaban%20and%20Paulo%20Mann%20and%20Harshvardhan%20Gazula&entry.1292438233=%20%20Transfer%20learning%20has%20become%20an%20essential%20tool%20in%20modern%20computer%20vision%2C%0Aallowing%20practitioners%20to%20leverage%20backbones%2C%20pretrained%20on%20large%20datasets%2C%20to%0Atrain%20successful%20models%20from%20limited%20annotated%20data.%20Choosing%20the%20right%0Abackbone%20is%20crucial%2C%20especially%20for%20small%20datasets%2C%20since%20final%20performance%0Adepends%20heavily%20on%20the%20quality%20of%20the%20initial%20feature%20representations.%20While%0Aprior%20work%20has%20conducted%20benchmarks%20across%20various%20datasets%20to%20identify%0Auniversal%20top-performing%20backbones%2C%20we%20demonstrate%20that%20backbone%20effectiveness%0Ais%20highly%20dataset-dependent%2C%20especially%20in%20low-data%20scenarios%20where%20no%20single%0Abackbone%20consistently%20excels.%20To%20overcome%20this%20limitation%2C%20we%20introduce%0Adataset-specific%20backbone%20selection%20as%20a%20new%20research%20direction%20and%20investigate%0Aits%20practical%20viability%20in%20low-data%20regimes.%20Since%20exhaustive%20evaluation%20is%0Acomputationally%20impractical%20for%20large%20backbone%20pools%2C%20we%20formalize%20Vision%0ABackbone%20Efficient%20Selection%20%28VIBES%29%20as%20the%20problem%20of%20searching%20for%0Ahigh-performing%20backbones%20under%20computational%20constraints.%20We%20define%20the%0Asolution%20space%2C%20propose%20several%20heuristics%2C%20and%20demonstrate%20VIBES%20feasibility%0Afor%20low-data%20image%20classification%20by%20performing%20experiments%20on%20four%20diverse%0Adatasets.%20Our%20results%20show%20that%20even%20simple%20search%20strategies%20can%20find%0Awell-suited%20backbones%20within%20a%20pool%20of%20over%20%241300%24%20pretrained%20models%2C%0Aoutperforming%20generic%20benchmark%20recommendations%20within%20just%20ten%20minutes%20of%0Asearch%20time%20on%20a%20single%20GPU%20%28NVIDIA%20RTX%20A5000%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08592v2&entry.124074799=Read"},
{"title": "RICO: Two Realistic Benchmarks and an In-Depth Analysis for Incremental\n  Learning in Object Detection", "author": "Matthias Neuwirth-Trapp and Maarten Bieshaar and Danda Pani Paudel and Luc Van Gool", "abstract": "  Incremental Learning (IL) trains models sequentially on new data without full\nretraining, offering privacy, efficiency, and scalability. IL must balance\nadaptability to new data with retention of old knowledge. However, evaluations\noften rely on synthetic, simplified benchmarks, obscuring real-world IL\nperformance. To address this, we introduce two Realistic Incremental Object\nDetection Benchmarks (RICO): Domain RICO (D-RICO) features domain shifts with a\nfixed class set, and Expanding-Classes RICO (EC-RICO) integrates new domains\nand classes per IL step. Built from 14 diverse datasets covering real and\nsynthetic domains, varying conditions (e.g., weather, time of day), camera\nsensors, perspectives, and labeling policies, both benchmarks capture\nchallenges absent in existing evaluations. Our experiments show that all IL\nmethods underperform in adaptability and retention, while replaying a small\namount of previous data already outperforms all methods. However, individual\ntraining on the data remains superior. We heuristically attribute this gap to\nweak teachers in distillation, single models' inability to manage diverse\ntasks, and insufficient plasticity. Our code will be made publicly available.\n", "link": "http://arxiv.org/abs/2508.13878v1", "date": "2025-08-19", "relevancy": 2.5917, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RICO%3A%20Two%20Realistic%20Benchmarks%20and%20an%20In-Depth%20Analysis%20for%20Incremental%0A%20%20Learning%20in%20Object%20Detection&body=Title%3A%20RICO%3A%20Two%20Realistic%20Benchmarks%20and%20an%20In-Depth%20Analysis%20for%20Incremental%0A%20%20Learning%20in%20Object%20Detection%0AAuthor%3A%20Matthias%20Neuwirth-Trapp%20and%20Maarten%20Bieshaar%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Incremental%20Learning%20%28IL%29%20trains%20models%20sequentially%20on%20new%20data%20without%20full%0Aretraining%2C%20offering%20privacy%2C%20efficiency%2C%20and%20scalability.%20IL%20must%20balance%0Aadaptability%20to%20new%20data%20with%20retention%20of%20old%20knowledge.%20However%2C%20evaluations%0Aoften%20rely%20on%20synthetic%2C%20simplified%20benchmarks%2C%20obscuring%20real-world%20IL%0Aperformance.%20To%20address%20this%2C%20we%20introduce%20two%20Realistic%20Incremental%20Object%0ADetection%20Benchmarks%20%28RICO%29%3A%20Domain%20RICO%20%28D-RICO%29%20features%20domain%20shifts%20with%20a%0Afixed%20class%20set%2C%20and%20Expanding-Classes%20RICO%20%28EC-RICO%29%20integrates%20new%20domains%0Aand%20classes%20per%20IL%20step.%20Built%20from%2014%20diverse%20datasets%20covering%20real%20and%0Asynthetic%20domains%2C%20varying%20conditions%20%28e.g.%2C%20weather%2C%20time%20of%20day%29%2C%20camera%0Asensors%2C%20perspectives%2C%20and%20labeling%20policies%2C%20both%20benchmarks%20capture%0Achallenges%20absent%20in%20existing%20evaluations.%20Our%20experiments%20show%20that%20all%20IL%0Amethods%20underperform%20in%20adaptability%20and%20retention%2C%20while%20replaying%20a%20small%0Aamount%20of%20previous%20data%20already%20outperforms%20all%20methods.%20However%2C%20individual%0Atraining%20on%20the%20data%20remains%20superior.%20We%20heuristically%20attribute%20this%20gap%20to%0Aweak%20teachers%20in%20distillation%2C%20single%20models%27%20inability%20to%20manage%20diverse%0Atasks%2C%20and%20insufficient%20plasticity.%20Our%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRICO%253A%2520Two%2520Realistic%2520Benchmarks%2520and%2520an%2520In-Depth%2520Analysis%2520for%2520Incremental%250A%2520%2520Learning%2520in%2520Object%2520Detection%26entry.906535625%3DMatthias%2520Neuwirth-Trapp%2520and%2520Maarten%2520Bieshaar%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520Incremental%2520Learning%2520%2528IL%2529%2520trains%2520models%2520sequentially%2520on%2520new%2520data%2520without%2520full%250Aretraining%252C%2520offering%2520privacy%252C%2520efficiency%252C%2520and%2520scalability.%2520IL%2520must%2520balance%250Aadaptability%2520to%2520new%2520data%2520with%2520retention%2520of%2520old%2520knowledge.%2520However%252C%2520evaluations%250Aoften%2520rely%2520on%2520synthetic%252C%2520simplified%2520benchmarks%252C%2520obscuring%2520real-world%2520IL%250Aperformance.%2520To%2520address%2520this%252C%2520we%2520introduce%2520two%2520Realistic%2520Incremental%2520Object%250ADetection%2520Benchmarks%2520%2528RICO%2529%253A%2520Domain%2520RICO%2520%2528D-RICO%2529%2520features%2520domain%2520shifts%2520with%2520a%250Afixed%2520class%2520set%252C%2520and%2520Expanding-Classes%2520RICO%2520%2528EC-RICO%2529%2520integrates%2520new%2520domains%250Aand%2520classes%2520per%2520IL%2520step.%2520Built%2520from%252014%2520diverse%2520datasets%2520covering%2520real%2520and%250Asynthetic%2520domains%252C%2520varying%2520conditions%2520%2528e.g.%252C%2520weather%252C%2520time%2520of%2520day%2529%252C%2520camera%250Asensors%252C%2520perspectives%252C%2520and%2520labeling%2520policies%252C%2520both%2520benchmarks%2520capture%250Achallenges%2520absent%2520in%2520existing%2520evaluations.%2520Our%2520experiments%2520show%2520that%2520all%2520IL%250Amethods%2520underperform%2520in%2520adaptability%2520and%2520retention%252C%2520while%2520replaying%2520a%2520small%250Aamount%2520of%2520previous%2520data%2520already%2520outperforms%2520all%2520methods.%2520However%252C%2520individual%250Atraining%2520on%2520the%2520data%2520remains%2520superior.%2520We%2520heuristically%2520attribute%2520this%2520gap%2520to%250Aweak%2520teachers%2520in%2520distillation%252C%2520single%2520models%2527%2520inability%2520to%2520manage%2520diverse%250Atasks%252C%2520and%2520insufficient%2520plasticity.%2520Our%2520code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RICO%3A%20Two%20Realistic%20Benchmarks%20and%20an%20In-Depth%20Analysis%20for%20Incremental%0A%20%20Learning%20in%20Object%20Detection&entry.906535625=Matthias%20Neuwirth-Trapp%20and%20Maarten%20Bieshaar%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Incremental%20Learning%20%28IL%29%20trains%20models%20sequentially%20on%20new%20data%20without%20full%0Aretraining%2C%20offering%20privacy%2C%20efficiency%2C%20and%20scalability.%20IL%20must%20balance%0Aadaptability%20to%20new%20data%20with%20retention%20of%20old%20knowledge.%20However%2C%20evaluations%0Aoften%20rely%20on%20synthetic%2C%20simplified%20benchmarks%2C%20obscuring%20real-world%20IL%0Aperformance.%20To%20address%20this%2C%20we%20introduce%20two%20Realistic%20Incremental%20Object%0ADetection%20Benchmarks%20%28RICO%29%3A%20Domain%20RICO%20%28D-RICO%29%20features%20domain%20shifts%20with%20a%0Afixed%20class%20set%2C%20and%20Expanding-Classes%20RICO%20%28EC-RICO%29%20integrates%20new%20domains%0Aand%20classes%20per%20IL%20step.%20Built%20from%2014%20diverse%20datasets%20covering%20real%20and%0Asynthetic%20domains%2C%20varying%20conditions%20%28e.g.%2C%20weather%2C%20time%20of%20day%29%2C%20camera%0Asensors%2C%20perspectives%2C%20and%20labeling%20policies%2C%20both%20benchmarks%20capture%0Achallenges%20absent%20in%20existing%20evaluations.%20Our%20experiments%20show%20that%20all%20IL%0Amethods%20underperform%20in%20adaptability%20and%20retention%2C%20while%20replaying%20a%20small%0Aamount%20of%20previous%20data%20already%20outperforms%20all%20methods.%20However%2C%20individual%0Atraining%20on%20the%20data%20remains%20superior.%20We%20heuristically%20attribute%20this%20gap%20to%0Aweak%20teachers%20in%20distillation%2C%20single%20models%27%20inability%20to%20manage%20diverse%0Atasks%2C%20and%20insufficient%20plasticity.%20Our%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13878v1&entry.124074799=Read"},
{"title": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction", "author": "Zeren Jiang and Chuanxia Zheng and Iro Laina and Diane Larlus and Andrea Vedaldi", "abstract": "  We introduce Geo4D, a method to repurpose video diffusion models for\nmonocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic\npriors captured by large-scale pre-trained video models, Geo4D can be trained\nusing only synthetic data while generalizing well to real data in a zero-shot\nmanner. Geo4D predicts several complementary geometric modalities, namely\npoint, disparity, and ray maps. We propose a new multi-modal alignment\nalgorithm to align and fuse these modalities, as well as a sliding window\napproach at inference time, thus enabling robust and accurate 4D reconstruction\nof long videos. Extensive experiments across multiple benchmarks show that\nGeo4D significantly surpasses state-of-the-art video depth estimation methods.\n", "link": "http://arxiv.org/abs/2504.07961v2", "date": "2025-08-19", "relevancy": 2.5717, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6643}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6416}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geo4D%3A%20Leveraging%20Video%20Generators%20for%20Geometric%204D%20Scene%20Reconstruction&body=Title%3A%20Geo4D%3A%20Leveraging%20Video%20Generators%20for%20Geometric%204D%20Scene%20Reconstruction%0AAuthor%3A%20Zeren%20Jiang%20and%20Chuanxia%20Zheng%20and%20Iro%20Laina%20and%20Diane%20Larlus%20and%20Andrea%20Vedaldi%0AAbstract%3A%20%20%20We%20introduce%20Geo4D%2C%20a%20method%20to%20repurpose%20video%20diffusion%20models%20for%0Amonocular%203D%20reconstruction%20of%20dynamic%20scenes.%20By%20leveraging%20the%20strong%20dynamic%0Apriors%20captured%20by%20large-scale%20pre-trained%20video%20models%2C%20Geo4D%20can%20be%20trained%0Ausing%20only%20synthetic%20data%20while%20generalizing%20well%20to%20real%20data%20in%20a%20zero-shot%0Amanner.%20Geo4D%20predicts%20several%20complementary%20geometric%20modalities%2C%20namely%0Apoint%2C%20disparity%2C%20and%20ray%20maps.%20We%20propose%20a%20new%20multi-modal%20alignment%0Aalgorithm%20to%20align%20and%20fuse%20these%20modalities%2C%20as%20well%20as%20a%20sliding%20window%0Aapproach%20at%20inference%20time%2C%20thus%20enabling%20robust%20and%20accurate%204D%20reconstruction%0Aof%20long%20videos.%20Extensive%20experiments%20across%20multiple%20benchmarks%20show%20that%0AGeo4D%20significantly%20surpasses%20state-of-the-art%20video%20depth%20estimation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07961v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeo4D%253A%2520Leveraging%2520Video%2520Generators%2520for%2520Geometric%25204D%2520Scene%2520Reconstruction%26entry.906535625%3DZeren%2520Jiang%2520and%2520Chuanxia%2520Zheng%2520and%2520Iro%2520Laina%2520and%2520Diane%2520Larlus%2520and%2520Andrea%2520Vedaldi%26entry.1292438233%3D%2520%2520We%2520introduce%2520Geo4D%252C%2520a%2520method%2520to%2520repurpose%2520video%2520diffusion%2520models%2520for%250Amonocular%25203D%2520reconstruction%2520of%2520dynamic%2520scenes.%2520By%2520leveraging%2520the%2520strong%2520dynamic%250Apriors%2520captured%2520by%2520large-scale%2520pre-trained%2520video%2520models%252C%2520Geo4D%2520can%2520be%2520trained%250Ausing%2520only%2520synthetic%2520data%2520while%2520generalizing%2520well%2520to%2520real%2520data%2520in%2520a%2520zero-shot%250Amanner.%2520Geo4D%2520predicts%2520several%2520complementary%2520geometric%2520modalities%252C%2520namely%250Apoint%252C%2520disparity%252C%2520and%2520ray%2520maps.%2520We%2520propose%2520a%2520new%2520multi-modal%2520alignment%250Aalgorithm%2520to%2520align%2520and%2520fuse%2520these%2520modalities%252C%2520as%2520well%2520as%2520a%2520sliding%2520window%250Aapproach%2520at%2520inference%2520time%252C%2520thus%2520enabling%2520robust%2520and%2520accurate%25204D%2520reconstruction%250Aof%2520long%2520videos.%2520Extensive%2520experiments%2520across%2520multiple%2520benchmarks%2520show%2520that%250AGeo4D%2520significantly%2520surpasses%2520state-of-the-art%2520video%2520depth%2520estimation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07961v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geo4D%3A%20Leveraging%20Video%20Generators%20for%20Geometric%204D%20Scene%20Reconstruction&entry.906535625=Zeren%20Jiang%20and%20Chuanxia%20Zheng%20and%20Iro%20Laina%20and%20Diane%20Larlus%20and%20Andrea%20Vedaldi&entry.1292438233=%20%20We%20introduce%20Geo4D%2C%20a%20method%20to%20repurpose%20video%20diffusion%20models%20for%0Amonocular%203D%20reconstruction%20of%20dynamic%20scenes.%20By%20leveraging%20the%20strong%20dynamic%0Apriors%20captured%20by%20large-scale%20pre-trained%20video%20models%2C%20Geo4D%20can%20be%20trained%0Ausing%20only%20synthetic%20data%20while%20generalizing%20well%20to%20real%20data%20in%20a%20zero-shot%0Amanner.%20Geo4D%20predicts%20several%20complementary%20geometric%20modalities%2C%20namely%0Apoint%2C%20disparity%2C%20and%20ray%20maps.%20We%20propose%20a%20new%20multi-modal%20alignment%0Aalgorithm%20to%20align%20and%20fuse%20these%20modalities%2C%20as%20well%20as%20a%20sliding%20window%0Aapproach%20at%20inference%20time%2C%20thus%20enabling%20robust%20and%20accurate%204D%20reconstruction%0Aof%20long%20videos.%20Extensive%20experiments%20across%20multiple%20benchmarks%20show%20that%0AGeo4D%20significantly%20surpasses%20state-of-the-art%20video%20depth%20estimation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07961v2&entry.124074799=Read"},
{"title": "Can Masked Autoencoders Also Listen to Birds?", "author": "Lukas Rauch and Ren\u00e9 Heinrich and Ilyass Moummad and Alexis Joly and Bernhard Sick and Christoph Scholz", "abstract": "  Masked Autoencoders (MAEs) learn rich semantic representations in audio\nclassification through an efficient self-supervised reconstruction task.\nHowever, general-purpose models fail to generalize well when applied directly\nto fine-grained audio domains. Specifically, bird-sound classification requires\ndistinguishing subtle inter-species differences and managing high intra-species\nacoustic variability, revealing the performance limitations of general-domain\nAudio-MAEs. This work demonstrates that bridging this domain gap domain gap\nrequires full-pipeline adaptation, not just domain-specific pretraining data.\nWe systematically revisit and adapt the pretraining recipe, fine-tuning\nmethods, and frozen feature utilization to bird sounds using BirdSet, a\nlarge-scale bioacoustic dataset comparable to AudioSet. Our resulting Bird-MAE\nachieves new state-of-the-art results in BirdSet's multi-label classification\nbenchmark. Additionally, we introduce the parameter-efficient prototypical\nprobing, enhancing the utility of frozen MAE representations and closely\napproaching fine-tuning performance in low-resource settings. Bird-MAE's\nprototypical probes outperform linear probing by up to 37 percentage points in\nmean average precision and narrow the gap to fine-tuning across BirdSet\ndownstream tasks. Bird-MAE also demonstrates robust few-shot capabilities with\nprototypical probing in our newly established few-shot benchmark on BirdSet,\nhighlighting the potential of tailored self-supervised learning pipelines for\nfine-grained audio domains.\n", "link": "http://arxiv.org/abs/2504.12880v4", "date": "2025-08-19", "relevancy": 2.5462, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5497}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Masked%20Autoencoders%20Also%20Listen%20to%20Birds%3F&body=Title%3A%20Can%20Masked%20Autoencoders%20Also%20Listen%20to%20Birds%3F%0AAuthor%3A%20Lukas%20Rauch%20and%20Ren%C3%A9%20Heinrich%20and%20Ilyass%20Moummad%20and%20Alexis%20Joly%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz%0AAbstract%3A%20%20%20Masked%20Autoencoders%20%28MAEs%29%20learn%20rich%20semantic%20representations%20in%20audio%0Aclassification%20through%20an%20efficient%20self-supervised%20reconstruction%20task.%0AHowever%2C%20general-purpose%20models%20fail%20to%20generalize%20well%20when%20applied%20directly%0Ato%20fine-grained%20audio%20domains.%20Specifically%2C%20bird-sound%20classification%20requires%0Adistinguishing%20subtle%20inter-species%20differences%20and%20managing%20high%20intra-species%0Aacoustic%20variability%2C%20revealing%20the%20performance%20limitations%20of%20general-domain%0AAudio-MAEs.%20This%20work%20demonstrates%20that%20bridging%20this%20domain%20gap%20domain%20gap%0Arequires%20full-pipeline%20adaptation%2C%20not%20just%20domain-specific%20pretraining%20data.%0AWe%20systematically%20revisit%20and%20adapt%20the%20pretraining%20recipe%2C%20fine-tuning%0Amethods%2C%20and%20frozen%20feature%20utilization%20to%20bird%20sounds%20using%20BirdSet%2C%20a%0Alarge-scale%20bioacoustic%20dataset%20comparable%20to%20AudioSet.%20Our%20resulting%20Bird-MAE%0Aachieves%20new%20state-of-the-art%20results%20in%20BirdSet%27s%20multi-label%20classification%0Abenchmark.%20Additionally%2C%20we%20introduce%20the%20parameter-efficient%20prototypical%0Aprobing%2C%20enhancing%20the%20utility%20of%20frozen%20MAE%20representations%20and%20closely%0Aapproaching%20fine-tuning%20performance%20in%20low-resource%20settings.%20Bird-MAE%27s%0Aprototypical%20probes%20outperform%20linear%20probing%20by%20up%20to%2037%20percentage%20points%20in%0Amean%20average%20precision%20and%20narrow%20the%20gap%20to%20fine-tuning%20across%20BirdSet%0Adownstream%20tasks.%20Bird-MAE%20also%20demonstrates%20robust%20few-shot%20capabilities%20with%0Aprototypical%20probing%20in%20our%20newly%20established%20few-shot%20benchmark%20on%20BirdSet%2C%0Ahighlighting%20the%20potential%20of%20tailored%20self-supervised%20learning%20pipelines%20for%0Afine-grained%20audio%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12880v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Masked%2520Autoencoders%2520Also%2520Listen%2520to%2520Birds%253F%26entry.906535625%3DLukas%2520Rauch%2520and%2520Ren%25C3%25A9%2520Heinrich%2520and%2520Ilyass%2520Moummad%2520and%2520Alexis%2520Joly%2520and%2520Bernhard%2520Sick%2520and%2520Christoph%2520Scholz%26entry.1292438233%3D%2520%2520Masked%2520Autoencoders%2520%2528MAEs%2529%2520learn%2520rich%2520semantic%2520representations%2520in%2520audio%250Aclassification%2520through%2520an%2520efficient%2520self-supervised%2520reconstruction%2520task.%250AHowever%252C%2520general-purpose%2520models%2520fail%2520to%2520generalize%2520well%2520when%2520applied%2520directly%250Ato%2520fine-grained%2520audio%2520domains.%2520Specifically%252C%2520bird-sound%2520classification%2520requires%250Adistinguishing%2520subtle%2520inter-species%2520differences%2520and%2520managing%2520high%2520intra-species%250Aacoustic%2520variability%252C%2520revealing%2520the%2520performance%2520limitations%2520of%2520general-domain%250AAudio-MAEs.%2520This%2520work%2520demonstrates%2520that%2520bridging%2520this%2520domain%2520gap%2520domain%2520gap%250Arequires%2520full-pipeline%2520adaptation%252C%2520not%2520just%2520domain-specific%2520pretraining%2520data.%250AWe%2520systematically%2520revisit%2520and%2520adapt%2520the%2520pretraining%2520recipe%252C%2520fine-tuning%250Amethods%252C%2520and%2520frozen%2520feature%2520utilization%2520to%2520bird%2520sounds%2520using%2520BirdSet%252C%2520a%250Alarge-scale%2520bioacoustic%2520dataset%2520comparable%2520to%2520AudioSet.%2520Our%2520resulting%2520Bird-MAE%250Aachieves%2520new%2520state-of-the-art%2520results%2520in%2520BirdSet%2527s%2520multi-label%2520classification%250Abenchmark.%2520Additionally%252C%2520we%2520introduce%2520the%2520parameter-efficient%2520prototypical%250Aprobing%252C%2520enhancing%2520the%2520utility%2520of%2520frozen%2520MAE%2520representations%2520and%2520closely%250Aapproaching%2520fine-tuning%2520performance%2520in%2520low-resource%2520settings.%2520Bird-MAE%2527s%250Aprototypical%2520probes%2520outperform%2520linear%2520probing%2520by%2520up%2520to%252037%2520percentage%2520points%2520in%250Amean%2520average%2520precision%2520and%2520narrow%2520the%2520gap%2520to%2520fine-tuning%2520across%2520BirdSet%250Adownstream%2520tasks.%2520Bird-MAE%2520also%2520demonstrates%2520robust%2520few-shot%2520capabilities%2520with%250Aprototypical%2520probing%2520in%2520our%2520newly%2520established%2520few-shot%2520benchmark%2520on%2520BirdSet%252C%250Ahighlighting%2520the%2520potential%2520of%2520tailored%2520self-supervised%2520learning%2520pipelines%2520for%250Afine-grained%2520audio%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12880v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Masked%20Autoencoders%20Also%20Listen%20to%20Birds%3F&entry.906535625=Lukas%20Rauch%20and%20Ren%C3%A9%20Heinrich%20and%20Ilyass%20Moummad%20and%20Alexis%20Joly%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz&entry.1292438233=%20%20Masked%20Autoencoders%20%28MAEs%29%20learn%20rich%20semantic%20representations%20in%20audio%0Aclassification%20through%20an%20efficient%20self-supervised%20reconstruction%20task.%0AHowever%2C%20general-purpose%20models%20fail%20to%20generalize%20well%20when%20applied%20directly%0Ato%20fine-grained%20audio%20domains.%20Specifically%2C%20bird-sound%20classification%20requires%0Adistinguishing%20subtle%20inter-species%20differences%20and%20managing%20high%20intra-species%0Aacoustic%20variability%2C%20revealing%20the%20performance%20limitations%20of%20general-domain%0AAudio-MAEs.%20This%20work%20demonstrates%20that%20bridging%20this%20domain%20gap%20domain%20gap%0Arequires%20full-pipeline%20adaptation%2C%20not%20just%20domain-specific%20pretraining%20data.%0AWe%20systematically%20revisit%20and%20adapt%20the%20pretraining%20recipe%2C%20fine-tuning%0Amethods%2C%20and%20frozen%20feature%20utilization%20to%20bird%20sounds%20using%20BirdSet%2C%20a%0Alarge-scale%20bioacoustic%20dataset%20comparable%20to%20AudioSet.%20Our%20resulting%20Bird-MAE%0Aachieves%20new%20state-of-the-art%20results%20in%20BirdSet%27s%20multi-label%20classification%0Abenchmark.%20Additionally%2C%20we%20introduce%20the%20parameter-efficient%20prototypical%0Aprobing%2C%20enhancing%20the%20utility%20of%20frozen%20MAE%20representations%20and%20closely%0Aapproaching%20fine-tuning%20performance%20in%20low-resource%20settings.%20Bird-MAE%27s%0Aprototypical%20probes%20outperform%20linear%20probing%20by%20up%20to%2037%20percentage%20points%20in%0Amean%20average%20precision%20and%20narrow%20the%20gap%20to%20fine-tuning%20across%20BirdSet%0Adownstream%20tasks.%20Bird-MAE%20also%20demonstrates%20robust%20few-shot%20capabilities%20with%0Aprototypical%20probing%20in%20our%20newly%20established%20few-shot%20benchmark%20on%20BirdSet%2C%0Ahighlighting%20the%20potential%20of%20tailored%20self-supervised%20learning%20pipelines%20for%0Afine-grained%20audio%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12880v4&entry.124074799=Read"},
{"title": "DREAMS: Preserving both Local and Global Structure in Dimensionality\n  Reduction", "author": "No\u00ebl Kury and Dmitry Kobak and Sebastian Damrich", "abstract": "  Dimensionality reduction techniques are widely used for visualizing\nhigh-dimensional data in two dimensions. Existing methods are typically\ndesigned to preserve either local (e.g. $t$-SNE, UMAP) or global (e.g. MDS,\nPCA) structure of the data, but none of the established methods can represent\nboth aspects well. In this paper, we present DREAMS (Dimensionality Reduction\nEnhanced Across Multiple Scales), a method that combines the local structure\npreservation of $t$-SNE with the global structure preservation of PCA via a\nsimple regularization term. Our approach generates a spectrum of embeddings\nbetween the locally well-structured $t$-SNE embedding and the globally\nwell-structured PCA embedding, efficiently balancing both local and global\nstructure preservation. We benchmark DREAMS across seven real-world datasets,\nincluding five from single-cell transcriptomics and one from population\ngenetics, showcasing qualitatively and quantitatively its superior ability to\npreserve structure across multiple scales compared to previous approaches.\n", "link": "http://arxiv.org/abs/2508.13747v1", "date": "2025-08-19", "relevancy": 2.5257, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5243}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.498}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DREAMS%3A%20Preserving%20both%20Local%20and%20Global%20Structure%20in%20Dimensionality%0A%20%20Reduction&body=Title%3A%20DREAMS%3A%20Preserving%20both%20Local%20and%20Global%20Structure%20in%20Dimensionality%0A%20%20Reduction%0AAuthor%3A%20No%C3%ABl%20Kury%20and%20Dmitry%20Kobak%20and%20Sebastian%20Damrich%0AAbstract%3A%20%20%20Dimensionality%20reduction%20techniques%20are%20widely%20used%20for%20visualizing%0Ahigh-dimensional%20data%20in%20two%20dimensions.%20Existing%20methods%20are%20typically%0Adesigned%20to%20preserve%20either%20local%20%28e.g.%20%24t%24-SNE%2C%20UMAP%29%20or%20global%20%28e.g.%20MDS%2C%0APCA%29%20structure%20of%20the%20data%2C%20but%20none%20of%20the%20established%20methods%20can%20represent%0Aboth%20aspects%20well.%20In%20this%20paper%2C%20we%20present%20DREAMS%20%28Dimensionality%20Reduction%0AEnhanced%20Across%20Multiple%20Scales%29%2C%20a%20method%20that%20combines%20the%20local%20structure%0Apreservation%20of%20%24t%24-SNE%20with%20the%20global%20structure%20preservation%20of%20PCA%20via%20a%0Asimple%20regularization%20term.%20Our%20approach%20generates%20a%20spectrum%20of%20embeddings%0Abetween%20the%20locally%20well-structured%20%24t%24-SNE%20embedding%20and%20the%20globally%0Awell-structured%20PCA%20embedding%2C%20efficiently%20balancing%20both%20local%20and%20global%0Astructure%20preservation.%20We%20benchmark%20DREAMS%20across%20seven%20real-world%20datasets%2C%0Aincluding%20five%20from%20single-cell%20transcriptomics%20and%20one%20from%20population%0Agenetics%2C%20showcasing%20qualitatively%20and%20quantitatively%20its%20superior%20ability%20to%0Apreserve%20structure%20across%20multiple%20scales%20compared%20to%20previous%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDREAMS%253A%2520Preserving%2520both%2520Local%2520and%2520Global%2520Structure%2520in%2520Dimensionality%250A%2520%2520Reduction%26entry.906535625%3DNo%25C3%25ABl%2520Kury%2520and%2520Dmitry%2520Kobak%2520and%2520Sebastian%2520Damrich%26entry.1292438233%3D%2520%2520Dimensionality%2520reduction%2520techniques%2520are%2520widely%2520used%2520for%2520visualizing%250Ahigh-dimensional%2520data%2520in%2520two%2520dimensions.%2520Existing%2520methods%2520are%2520typically%250Adesigned%2520to%2520preserve%2520either%2520local%2520%2528e.g.%2520%2524t%2524-SNE%252C%2520UMAP%2529%2520or%2520global%2520%2528e.g.%2520MDS%252C%250APCA%2529%2520structure%2520of%2520the%2520data%252C%2520but%2520none%2520of%2520the%2520established%2520methods%2520can%2520represent%250Aboth%2520aspects%2520well.%2520In%2520this%2520paper%252C%2520we%2520present%2520DREAMS%2520%2528Dimensionality%2520Reduction%250AEnhanced%2520Across%2520Multiple%2520Scales%2529%252C%2520a%2520method%2520that%2520combines%2520the%2520local%2520structure%250Apreservation%2520of%2520%2524t%2524-SNE%2520with%2520the%2520global%2520structure%2520preservation%2520of%2520PCA%2520via%2520a%250Asimple%2520regularization%2520term.%2520Our%2520approach%2520generates%2520a%2520spectrum%2520of%2520embeddings%250Abetween%2520the%2520locally%2520well-structured%2520%2524t%2524-SNE%2520embedding%2520and%2520the%2520globally%250Awell-structured%2520PCA%2520embedding%252C%2520efficiently%2520balancing%2520both%2520local%2520and%2520global%250Astructure%2520preservation.%2520We%2520benchmark%2520DREAMS%2520across%2520seven%2520real-world%2520datasets%252C%250Aincluding%2520five%2520from%2520single-cell%2520transcriptomics%2520and%2520one%2520from%2520population%250Agenetics%252C%2520showcasing%2520qualitatively%2520and%2520quantitatively%2520its%2520superior%2520ability%2520to%250Apreserve%2520structure%2520across%2520multiple%2520scales%2520compared%2520to%2520previous%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DREAMS%3A%20Preserving%20both%20Local%20and%20Global%20Structure%20in%20Dimensionality%0A%20%20Reduction&entry.906535625=No%C3%ABl%20Kury%20and%20Dmitry%20Kobak%20and%20Sebastian%20Damrich&entry.1292438233=%20%20Dimensionality%20reduction%20techniques%20are%20widely%20used%20for%20visualizing%0Ahigh-dimensional%20data%20in%20two%20dimensions.%20Existing%20methods%20are%20typically%0Adesigned%20to%20preserve%20either%20local%20%28e.g.%20%24t%24-SNE%2C%20UMAP%29%20or%20global%20%28e.g.%20MDS%2C%0APCA%29%20structure%20of%20the%20data%2C%20but%20none%20of%20the%20established%20methods%20can%20represent%0Aboth%20aspects%20well.%20In%20this%20paper%2C%20we%20present%20DREAMS%20%28Dimensionality%20Reduction%0AEnhanced%20Across%20Multiple%20Scales%29%2C%20a%20method%20that%20combines%20the%20local%20structure%0Apreservation%20of%20%24t%24-SNE%20with%20the%20global%20structure%20preservation%20of%20PCA%20via%20a%0Asimple%20regularization%20term.%20Our%20approach%20generates%20a%20spectrum%20of%20embeddings%0Abetween%20the%20locally%20well-structured%20%24t%24-SNE%20embedding%20and%20the%20globally%0Awell-structured%20PCA%20embedding%2C%20efficiently%20balancing%20both%20local%20and%20global%0Astructure%20preservation.%20We%20benchmark%20DREAMS%20across%20seven%20real-world%20datasets%2C%0Aincluding%20five%20from%20single-cell%20transcriptomics%20and%20one%20from%20population%0Agenetics%2C%20showcasing%20qualitatively%20and%20quantitatively%20its%20superior%20ability%20to%0Apreserve%20structure%20across%20multiple%20scales%20compared%20to%20previous%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13747v1&entry.124074799=Read"},
{"title": "Optimizing Region of Interest Selection for Effective Embedding in Video\n  Steganography Based on Genetic Algorithms", "author": "Nizheen A. Ali and Ramadhan J. Mstafa", "abstract": "  With the widespread use of the internet, there is an increasing need to\nensure the security and privacy of transmitted data. This has led to an\nintensified focus on the study of video steganography, which is a technique\nthat hides data within a video cover to avoid detection. The effectiveness of\nany steganography method depends on its ability to embed data without altering\nthe original video quality while maintaining high efficiency. This paper\nproposes a new method to video steganography, which involves utilizing a\nGenetic Algorithm (GA) for identifying the Region of Interest (ROI) in the\ncover video. The ROI is the area in the video that is the most suitable for\ndata embedding. The secret data is encrypted using the Advanced Encryption\nStandard (AES), which is a widely accepted encryption standard, before being\nembedded into the cover video, utilizing up to 10% of the cover video. This\nprocess ensures the security and confidentiality of the embedded data. The\nperformance metrics for assessing the proposed method are the Peak Signal to\nNoise Ratio (PSNR) and the encoding and decoding time. The results show that\nthe proposed method has a high embedding capacity and efficiency, with a PSNR\nranging between 64 and 75 dBs, which indicates that the embedded data is almost\nindistinguishable from the original video. Additionally, the method can encode\nand decode data quickly, making it efficient for real time applications.\n", "link": "http://arxiv.org/abs/2508.13710v1", "date": "2025-08-19", "relevancy": 2.5241, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5269}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5023}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Region%20of%20Interest%20Selection%20for%20Effective%20Embedding%20in%20Video%0A%20%20Steganography%20Based%20on%20Genetic%20Algorithms&body=Title%3A%20Optimizing%20Region%20of%20Interest%20Selection%20for%20Effective%20Embedding%20in%20Video%0A%20%20Steganography%20Based%20on%20Genetic%20Algorithms%0AAuthor%3A%20Nizheen%20A.%20Ali%20and%20Ramadhan%20J.%20Mstafa%0AAbstract%3A%20%20%20With%20the%20widespread%20use%20of%20the%20internet%2C%20there%20is%20an%20increasing%20need%20to%0Aensure%20the%20security%20and%20privacy%20of%20transmitted%20data.%20This%20has%20led%20to%20an%0Aintensified%20focus%20on%20the%20study%20of%20video%20steganography%2C%20which%20is%20a%20technique%0Athat%20hides%20data%20within%20a%20video%20cover%20to%20avoid%20detection.%20The%20effectiveness%20of%0Aany%20steganography%20method%20depends%20on%20its%20ability%20to%20embed%20data%20without%20altering%0Athe%20original%20video%20quality%20while%20maintaining%20high%20efficiency.%20This%20paper%0Aproposes%20a%20new%20method%20to%20video%20steganography%2C%20which%20involves%20utilizing%20a%0AGenetic%20Algorithm%20%28GA%29%20for%20identifying%20the%20Region%20of%20Interest%20%28ROI%29%20in%20the%0Acover%20video.%20The%20ROI%20is%20the%20area%20in%20the%20video%20that%20is%20the%20most%20suitable%20for%0Adata%20embedding.%20The%20secret%20data%20is%20encrypted%20using%20the%20Advanced%20Encryption%0AStandard%20%28AES%29%2C%20which%20is%20a%20widely%20accepted%20encryption%20standard%2C%20before%20being%0Aembedded%20into%20the%20cover%20video%2C%20utilizing%20up%20to%2010%25%20of%20the%20cover%20video.%20This%0Aprocess%20ensures%20the%20security%20and%20confidentiality%20of%20the%20embedded%20data.%20The%0Aperformance%20metrics%20for%20assessing%20the%20proposed%20method%20are%20the%20Peak%20Signal%20to%0ANoise%20Ratio%20%28PSNR%29%20and%20the%20encoding%20and%20decoding%20time.%20The%20results%20show%20that%0Athe%20proposed%20method%20has%20a%20high%20embedding%20capacity%20and%20efficiency%2C%20with%20a%20PSNR%0Aranging%20between%2064%20and%2075%20dBs%2C%20which%20indicates%20that%20the%20embedded%20data%20is%20almost%0Aindistinguishable%20from%20the%20original%20video.%20Additionally%2C%20the%20method%20can%20encode%0Aand%20decode%20data%20quickly%2C%20making%20it%20efficient%20for%20real%20time%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Region%2520of%2520Interest%2520Selection%2520for%2520Effective%2520Embedding%2520in%2520Video%250A%2520%2520Steganography%2520Based%2520on%2520Genetic%2520Algorithms%26entry.906535625%3DNizheen%2520A.%2520Ali%2520and%2520Ramadhan%2520J.%2520Mstafa%26entry.1292438233%3D%2520%2520With%2520the%2520widespread%2520use%2520of%2520the%2520internet%252C%2520there%2520is%2520an%2520increasing%2520need%2520to%250Aensure%2520the%2520security%2520and%2520privacy%2520of%2520transmitted%2520data.%2520This%2520has%2520led%2520to%2520an%250Aintensified%2520focus%2520on%2520the%2520study%2520of%2520video%2520steganography%252C%2520which%2520is%2520a%2520technique%250Athat%2520hides%2520data%2520within%2520a%2520video%2520cover%2520to%2520avoid%2520detection.%2520The%2520effectiveness%2520of%250Aany%2520steganography%2520method%2520depends%2520on%2520its%2520ability%2520to%2520embed%2520data%2520without%2520altering%250Athe%2520original%2520video%2520quality%2520while%2520maintaining%2520high%2520efficiency.%2520This%2520paper%250Aproposes%2520a%2520new%2520method%2520to%2520video%2520steganography%252C%2520which%2520involves%2520utilizing%2520a%250AGenetic%2520Algorithm%2520%2528GA%2529%2520for%2520identifying%2520the%2520Region%2520of%2520Interest%2520%2528ROI%2529%2520in%2520the%250Acover%2520video.%2520The%2520ROI%2520is%2520the%2520area%2520in%2520the%2520video%2520that%2520is%2520the%2520most%2520suitable%2520for%250Adata%2520embedding.%2520The%2520secret%2520data%2520is%2520encrypted%2520using%2520the%2520Advanced%2520Encryption%250AStandard%2520%2528AES%2529%252C%2520which%2520is%2520a%2520widely%2520accepted%2520encryption%2520standard%252C%2520before%2520being%250Aembedded%2520into%2520the%2520cover%2520video%252C%2520utilizing%2520up%2520to%252010%2525%2520of%2520the%2520cover%2520video.%2520This%250Aprocess%2520ensures%2520the%2520security%2520and%2520confidentiality%2520of%2520the%2520embedded%2520data.%2520The%250Aperformance%2520metrics%2520for%2520assessing%2520the%2520proposed%2520method%2520are%2520the%2520Peak%2520Signal%2520to%250ANoise%2520Ratio%2520%2528PSNR%2529%2520and%2520the%2520encoding%2520and%2520decoding%2520time.%2520The%2520results%2520show%2520that%250Athe%2520proposed%2520method%2520has%2520a%2520high%2520embedding%2520capacity%2520and%2520efficiency%252C%2520with%2520a%2520PSNR%250Aranging%2520between%252064%2520and%252075%2520dBs%252C%2520which%2520indicates%2520that%2520the%2520embedded%2520data%2520is%2520almost%250Aindistinguishable%2520from%2520the%2520original%2520video.%2520Additionally%252C%2520the%2520method%2520can%2520encode%250Aand%2520decode%2520data%2520quickly%252C%2520making%2520it%2520efficient%2520for%2520real%2520time%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Region%20of%20Interest%20Selection%20for%20Effective%20Embedding%20in%20Video%0A%20%20Steganography%20Based%20on%20Genetic%20Algorithms&entry.906535625=Nizheen%20A.%20Ali%20and%20Ramadhan%20J.%20Mstafa&entry.1292438233=%20%20With%20the%20widespread%20use%20of%20the%20internet%2C%20there%20is%20an%20increasing%20need%20to%0Aensure%20the%20security%20and%20privacy%20of%20transmitted%20data.%20This%20has%20led%20to%20an%0Aintensified%20focus%20on%20the%20study%20of%20video%20steganography%2C%20which%20is%20a%20technique%0Athat%20hides%20data%20within%20a%20video%20cover%20to%20avoid%20detection.%20The%20effectiveness%20of%0Aany%20steganography%20method%20depends%20on%20its%20ability%20to%20embed%20data%20without%20altering%0Athe%20original%20video%20quality%20while%20maintaining%20high%20efficiency.%20This%20paper%0Aproposes%20a%20new%20method%20to%20video%20steganography%2C%20which%20involves%20utilizing%20a%0AGenetic%20Algorithm%20%28GA%29%20for%20identifying%20the%20Region%20of%20Interest%20%28ROI%29%20in%20the%0Acover%20video.%20The%20ROI%20is%20the%20area%20in%20the%20video%20that%20is%20the%20most%20suitable%20for%0Adata%20embedding.%20The%20secret%20data%20is%20encrypted%20using%20the%20Advanced%20Encryption%0AStandard%20%28AES%29%2C%20which%20is%20a%20widely%20accepted%20encryption%20standard%2C%20before%20being%0Aembedded%20into%20the%20cover%20video%2C%20utilizing%20up%20to%2010%25%20of%20the%20cover%20video.%20This%0Aprocess%20ensures%20the%20security%20and%20confidentiality%20of%20the%20embedded%20data.%20The%0Aperformance%20metrics%20for%20assessing%20the%20proposed%20method%20are%20the%20Peak%20Signal%20to%0ANoise%20Ratio%20%28PSNR%29%20and%20the%20encoding%20and%20decoding%20time.%20The%20results%20show%20that%0Athe%20proposed%20method%20has%20a%20high%20embedding%20capacity%20and%20efficiency%2C%20with%20a%20PSNR%0Aranging%20between%2064%20and%2075%20dBs%2C%20which%20indicates%20that%20the%20embedded%20data%20is%20almost%0Aindistinguishable%20from%20the%20original%20video.%20Additionally%2C%20the%20method%20can%20encode%0Aand%20decode%20data%20quickly%2C%20making%20it%20efficient%20for%20real%20time%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13710v1&entry.124074799=Read"},
{"title": "HumanPCR: Probing MLLM Capabilities in Diverse Human-Centric Scenes", "author": "Keliang Li and Hongze Shen and Hao Shi and Ruibing Hou and Hong Chang and Jie Huang and Chenghao Jia and Wen Wang and Yiling Wu and Dongmei Jiang and Shiguang Shan and Xilin Chen", "abstract": "  The aspiration for artificial general intelligence, fueled by the rapid\nprogress of multimodal models, demands human-comparable performance across\ndiverse environments. We propose HumanPCR, an evaluation suite for probing\nMLLMs' capacity about human-related visual contexts across three hierarchical\nlevels: Perception, Comprehension, and Reasoning (denoted by Human-P, Human-C,\nand Human-R, respectively). Human-P and Human-C feature over 6,000\nhuman-verified multiple choice questions, assessing massive tasks of 9\ndimensions, including but not limited to essential skills frequently overlooked\nby existing benchmarks. Human-R offers a challenging manually curated video\nreasoning test that requires integrating multiple visual evidences, proactively\nextracting context beyond question cues, and applying human-like expertise.\nEach question includes human-annotated Chain-of-Thought (CoT) rationales with\nkey visual evidence to support further research. Extensive evaluations on over\n30 state-of-the-art models exhibit significant challenges in human-centric\nvisual understanding, particularly in tasks involving detailed space\nperception, temporal understanding, and mind modeling. Moreover, analysis of\nHuman-R reveals the struggle of models in extracting essential proactive visual\nevidence from diverse human scenes and their faulty reliance on query-guided\nretrieval. Even with advanced techniques like scaling visual contexts and\ntest-time thinking yield only limited benefits. We hope HumanPCR and our\nfindings will advance the development, evaluation, and human-centric\napplication of multimodal models.\n", "link": "http://arxiv.org/abs/2508.13692v1", "date": "2025-08-19", "relevancy": 2.5212, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6412}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6412}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanPCR%3A%20Probing%20MLLM%20Capabilities%20in%20Diverse%20Human-Centric%20Scenes&body=Title%3A%20HumanPCR%3A%20Probing%20MLLM%20Capabilities%20in%20Diverse%20Human-Centric%20Scenes%0AAuthor%3A%20Keliang%20Li%20and%20Hongze%20Shen%20and%20Hao%20Shi%20and%20Ruibing%20Hou%20and%20Hong%20Chang%20and%20Jie%20Huang%20and%20Chenghao%20Jia%20and%20Wen%20Wang%20and%20Yiling%20Wu%20and%20Dongmei%20Jiang%20and%20Shiguang%20Shan%20and%20Xilin%20Chen%0AAbstract%3A%20%20%20The%20aspiration%20for%20artificial%20general%20intelligence%2C%20fueled%20by%20the%20rapid%0Aprogress%20of%20multimodal%20models%2C%20demands%20human-comparable%20performance%20across%0Adiverse%20environments.%20We%20propose%20HumanPCR%2C%20an%20evaluation%20suite%20for%20probing%0AMLLMs%27%20capacity%20about%20human-related%20visual%20contexts%20across%20three%20hierarchical%0Alevels%3A%20Perception%2C%20Comprehension%2C%20and%20Reasoning%20%28denoted%20by%20Human-P%2C%20Human-C%2C%0Aand%20Human-R%2C%20respectively%29.%20Human-P%20and%20Human-C%20feature%20over%206%2C000%0Ahuman-verified%20multiple%20choice%20questions%2C%20assessing%20massive%20tasks%20of%209%0Adimensions%2C%20including%20but%20not%20limited%20to%20essential%20skills%20frequently%20overlooked%0Aby%20existing%20benchmarks.%20Human-R%20offers%20a%20challenging%20manually%20curated%20video%0Areasoning%20test%20that%20requires%20integrating%20multiple%20visual%20evidences%2C%20proactively%0Aextracting%20context%20beyond%20question%20cues%2C%20and%20applying%20human-like%20expertise.%0AEach%20question%20includes%20human-annotated%20Chain-of-Thought%20%28CoT%29%20rationales%20with%0Akey%20visual%20evidence%20to%20support%20further%20research.%20Extensive%20evaluations%20on%20over%0A30%20state-of-the-art%20models%20exhibit%20significant%20challenges%20in%20human-centric%0Avisual%20understanding%2C%20particularly%20in%20tasks%20involving%20detailed%20space%0Aperception%2C%20temporal%20understanding%2C%20and%20mind%20modeling.%20Moreover%2C%20analysis%20of%0AHuman-R%20reveals%20the%20struggle%20of%20models%20in%20extracting%20essential%20proactive%20visual%0Aevidence%20from%20diverse%20human%20scenes%20and%20their%20faulty%20reliance%20on%20query-guided%0Aretrieval.%20Even%20with%20advanced%20techniques%20like%20scaling%20visual%20contexts%20and%0Atest-time%20thinking%20yield%20only%20limited%20benefits.%20We%20hope%20HumanPCR%20and%20our%0Afindings%20will%20advance%20the%20development%2C%20evaluation%2C%20and%20human-centric%0Aapplication%20of%20multimodal%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanPCR%253A%2520Probing%2520MLLM%2520Capabilities%2520in%2520Diverse%2520Human-Centric%2520Scenes%26entry.906535625%3DKeliang%2520Li%2520and%2520Hongze%2520Shen%2520and%2520Hao%2520Shi%2520and%2520Ruibing%2520Hou%2520and%2520Hong%2520Chang%2520and%2520Jie%2520Huang%2520and%2520Chenghao%2520Jia%2520and%2520Wen%2520Wang%2520and%2520Yiling%2520Wu%2520and%2520Dongmei%2520Jiang%2520and%2520Shiguang%2520Shan%2520and%2520Xilin%2520Chen%26entry.1292438233%3D%2520%2520The%2520aspiration%2520for%2520artificial%2520general%2520intelligence%252C%2520fueled%2520by%2520the%2520rapid%250Aprogress%2520of%2520multimodal%2520models%252C%2520demands%2520human-comparable%2520performance%2520across%250Adiverse%2520environments.%2520We%2520propose%2520HumanPCR%252C%2520an%2520evaluation%2520suite%2520for%2520probing%250AMLLMs%2527%2520capacity%2520about%2520human-related%2520visual%2520contexts%2520across%2520three%2520hierarchical%250Alevels%253A%2520Perception%252C%2520Comprehension%252C%2520and%2520Reasoning%2520%2528denoted%2520by%2520Human-P%252C%2520Human-C%252C%250Aand%2520Human-R%252C%2520respectively%2529.%2520Human-P%2520and%2520Human-C%2520feature%2520over%25206%252C000%250Ahuman-verified%2520multiple%2520choice%2520questions%252C%2520assessing%2520massive%2520tasks%2520of%25209%250Adimensions%252C%2520including%2520but%2520not%2520limited%2520to%2520essential%2520skills%2520frequently%2520overlooked%250Aby%2520existing%2520benchmarks.%2520Human-R%2520offers%2520a%2520challenging%2520manually%2520curated%2520video%250Areasoning%2520test%2520that%2520requires%2520integrating%2520multiple%2520visual%2520evidences%252C%2520proactively%250Aextracting%2520context%2520beyond%2520question%2520cues%252C%2520and%2520applying%2520human-like%2520expertise.%250AEach%2520question%2520includes%2520human-annotated%2520Chain-of-Thought%2520%2528CoT%2529%2520rationales%2520with%250Akey%2520visual%2520evidence%2520to%2520support%2520further%2520research.%2520Extensive%2520evaluations%2520on%2520over%250A30%2520state-of-the-art%2520models%2520exhibit%2520significant%2520challenges%2520in%2520human-centric%250Avisual%2520understanding%252C%2520particularly%2520in%2520tasks%2520involving%2520detailed%2520space%250Aperception%252C%2520temporal%2520understanding%252C%2520and%2520mind%2520modeling.%2520Moreover%252C%2520analysis%2520of%250AHuman-R%2520reveals%2520the%2520struggle%2520of%2520models%2520in%2520extracting%2520essential%2520proactive%2520visual%250Aevidence%2520from%2520diverse%2520human%2520scenes%2520and%2520their%2520faulty%2520reliance%2520on%2520query-guided%250Aretrieval.%2520Even%2520with%2520advanced%2520techniques%2520like%2520scaling%2520visual%2520contexts%2520and%250Atest-time%2520thinking%2520yield%2520only%2520limited%2520benefits.%2520We%2520hope%2520HumanPCR%2520and%2520our%250Afindings%2520will%2520advance%2520the%2520development%252C%2520evaluation%252C%2520and%2520human-centric%250Aapplication%2520of%2520multimodal%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanPCR%3A%20Probing%20MLLM%20Capabilities%20in%20Diverse%20Human-Centric%20Scenes&entry.906535625=Keliang%20Li%20and%20Hongze%20Shen%20and%20Hao%20Shi%20and%20Ruibing%20Hou%20and%20Hong%20Chang%20and%20Jie%20Huang%20and%20Chenghao%20Jia%20and%20Wen%20Wang%20and%20Yiling%20Wu%20and%20Dongmei%20Jiang%20and%20Shiguang%20Shan%20and%20Xilin%20Chen&entry.1292438233=%20%20The%20aspiration%20for%20artificial%20general%20intelligence%2C%20fueled%20by%20the%20rapid%0Aprogress%20of%20multimodal%20models%2C%20demands%20human-comparable%20performance%20across%0Adiverse%20environments.%20We%20propose%20HumanPCR%2C%20an%20evaluation%20suite%20for%20probing%0AMLLMs%27%20capacity%20about%20human-related%20visual%20contexts%20across%20three%20hierarchical%0Alevels%3A%20Perception%2C%20Comprehension%2C%20and%20Reasoning%20%28denoted%20by%20Human-P%2C%20Human-C%2C%0Aand%20Human-R%2C%20respectively%29.%20Human-P%20and%20Human-C%20feature%20over%206%2C000%0Ahuman-verified%20multiple%20choice%20questions%2C%20assessing%20massive%20tasks%20of%209%0Adimensions%2C%20including%20but%20not%20limited%20to%20essential%20skills%20frequently%20overlooked%0Aby%20existing%20benchmarks.%20Human-R%20offers%20a%20challenging%20manually%20curated%20video%0Areasoning%20test%20that%20requires%20integrating%20multiple%20visual%20evidences%2C%20proactively%0Aextracting%20context%20beyond%20question%20cues%2C%20and%20applying%20human-like%20expertise.%0AEach%20question%20includes%20human-annotated%20Chain-of-Thought%20%28CoT%29%20rationales%20with%0Akey%20visual%20evidence%20to%20support%20further%20research.%20Extensive%20evaluations%20on%20over%0A30%20state-of-the-art%20models%20exhibit%20significant%20challenges%20in%20human-centric%0Avisual%20understanding%2C%20particularly%20in%20tasks%20involving%20detailed%20space%0Aperception%2C%20temporal%20understanding%2C%20and%20mind%20modeling.%20Moreover%2C%20analysis%20of%0AHuman-R%20reveals%20the%20struggle%20of%20models%20in%20extracting%20essential%20proactive%20visual%0Aevidence%20from%20diverse%20human%20scenes%20and%20their%20faulty%20reliance%20on%20query-guided%0Aretrieval.%20Even%20with%20advanced%20techniques%20like%20scaling%20visual%20contexts%20and%0Atest-time%20thinking%20yield%20only%20limited%20benefits.%20We%20hope%20HumanPCR%20and%20our%0Afindings%20will%20advance%20the%20development%2C%20evaluation%2C%20and%20human-centric%0Aapplication%20of%20multimodal%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13692v1&entry.124074799=Read"},
{"title": "InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video\n  Dubbing", "author": "Shaoshu Yang and Zhe Kong and Feng Gao and Meng Cheng and Xiangyu Liu and Yong Zhang and Zhuoliang Kang and Wenhan Luo and Xunliang Cai and Ran He and Xiaoming Wei", "abstract": "  Recent breakthroughs in video AIGC have ushered in a transformative era for\naudio-driven human animation. However, conventional video dubbing techniques\nremain constrained to mouth region editing, resulting in discordant facial\nexpressions and body gestures that compromise viewer immersion. To overcome\nthis limitation, we introduce sparse-frame video dubbing, a novel paradigm that\nstrategically preserves reference keyframes to maintain identity, iconic\ngestures, and camera trajectories while enabling holistic, audio-synchronized\nfull-body motion editing. Through critical analysis, we identify why naive\nimage-to-video models fail in this task, particularly their inability to\nachieve adaptive conditioning. Addressing this, we propose InfiniteTalk, a\nstreaming audio-driven generator designed for infinite-length long sequence\ndubbing. This architecture leverages temporal context frames for seamless\ninter-chunk transitions and incorporates a simple yet effective sampling\nstrategy that optimizes control strength via fine-grained reference frame\npositioning. Comprehensive evaluations on HDTF, CelebV-HQ, and EMTD datasets\ndemonstrate state-of-the-art performance. Quantitative metrics confirm superior\nvisual realism, emotional coherence, and full-body motion synchronization.\n", "link": "http://arxiv.org/abs/2508.14033v1", "date": "2025-08-19", "relevancy": 2.4953, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6352}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6265}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfiniteTalk%3A%20Audio-driven%20Video%20Generation%20for%20Sparse-Frame%20Video%0A%20%20Dubbing&body=Title%3A%20InfiniteTalk%3A%20Audio-driven%20Video%20Generation%20for%20Sparse-Frame%20Video%0A%20%20Dubbing%0AAuthor%3A%20Shaoshu%20Yang%20and%20Zhe%20Kong%20and%20Feng%20Gao%20and%20Meng%20Cheng%20and%20Xiangyu%20Liu%20and%20Yong%20Zhang%20and%20Zhuoliang%20Kang%20and%20Wenhan%20Luo%20and%20Xunliang%20Cai%20and%20Ran%20He%20and%20Xiaoming%20Wei%0AAbstract%3A%20%20%20Recent%20breakthroughs%20in%20video%20AIGC%20have%20ushered%20in%20a%20transformative%20era%20for%0Aaudio-driven%20human%20animation.%20However%2C%20conventional%20video%20dubbing%20techniques%0Aremain%20constrained%20to%20mouth%20region%20editing%2C%20resulting%20in%20discordant%20facial%0Aexpressions%20and%20body%20gestures%20that%20compromise%20viewer%20immersion.%20To%20overcome%0Athis%20limitation%2C%20we%20introduce%20sparse-frame%20video%20dubbing%2C%20a%20novel%20paradigm%20that%0Astrategically%20preserves%20reference%20keyframes%20to%20maintain%20identity%2C%20iconic%0Agestures%2C%20and%20camera%20trajectories%20while%20enabling%20holistic%2C%20audio-synchronized%0Afull-body%20motion%20editing.%20Through%20critical%20analysis%2C%20we%20identify%20why%20naive%0Aimage-to-video%20models%20fail%20in%20this%20task%2C%20particularly%20their%20inability%20to%0Aachieve%20adaptive%20conditioning.%20Addressing%20this%2C%20we%20propose%20InfiniteTalk%2C%20a%0Astreaming%20audio-driven%20generator%20designed%20for%20infinite-length%20long%20sequence%0Adubbing.%20This%20architecture%20leverages%20temporal%20context%20frames%20for%20seamless%0Ainter-chunk%20transitions%20and%20incorporates%20a%20simple%20yet%20effective%20sampling%0Astrategy%20that%20optimizes%20control%20strength%20via%20fine-grained%20reference%20frame%0Apositioning.%20Comprehensive%20evaluations%20on%20HDTF%2C%20CelebV-HQ%2C%20and%20EMTD%20datasets%0Ademonstrate%20state-of-the-art%20performance.%20Quantitative%20metrics%20confirm%20superior%0Avisual%20realism%2C%20emotional%20coherence%2C%20and%20full-body%20motion%20synchronization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfiniteTalk%253A%2520Audio-driven%2520Video%2520Generation%2520for%2520Sparse-Frame%2520Video%250A%2520%2520Dubbing%26entry.906535625%3DShaoshu%2520Yang%2520and%2520Zhe%2520Kong%2520and%2520Feng%2520Gao%2520and%2520Meng%2520Cheng%2520and%2520Xiangyu%2520Liu%2520and%2520Yong%2520Zhang%2520and%2520Zhuoliang%2520Kang%2520and%2520Wenhan%2520Luo%2520and%2520Xunliang%2520Cai%2520and%2520Ran%2520He%2520and%2520Xiaoming%2520Wei%26entry.1292438233%3D%2520%2520Recent%2520breakthroughs%2520in%2520video%2520AIGC%2520have%2520ushered%2520in%2520a%2520transformative%2520era%2520for%250Aaudio-driven%2520human%2520animation.%2520However%252C%2520conventional%2520video%2520dubbing%2520techniques%250Aremain%2520constrained%2520to%2520mouth%2520region%2520editing%252C%2520resulting%2520in%2520discordant%2520facial%250Aexpressions%2520and%2520body%2520gestures%2520that%2520compromise%2520viewer%2520immersion.%2520To%2520overcome%250Athis%2520limitation%252C%2520we%2520introduce%2520sparse-frame%2520video%2520dubbing%252C%2520a%2520novel%2520paradigm%2520that%250Astrategically%2520preserves%2520reference%2520keyframes%2520to%2520maintain%2520identity%252C%2520iconic%250Agestures%252C%2520and%2520camera%2520trajectories%2520while%2520enabling%2520holistic%252C%2520audio-synchronized%250Afull-body%2520motion%2520editing.%2520Through%2520critical%2520analysis%252C%2520we%2520identify%2520why%2520naive%250Aimage-to-video%2520models%2520fail%2520in%2520this%2520task%252C%2520particularly%2520their%2520inability%2520to%250Aachieve%2520adaptive%2520conditioning.%2520Addressing%2520this%252C%2520we%2520propose%2520InfiniteTalk%252C%2520a%250Astreaming%2520audio-driven%2520generator%2520designed%2520for%2520infinite-length%2520long%2520sequence%250Adubbing.%2520This%2520architecture%2520leverages%2520temporal%2520context%2520frames%2520for%2520seamless%250Ainter-chunk%2520transitions%2520and%2520incorporates%2520a%2520simple%2520yet%2520effective%2520sampling%250Astrategy%2520that%2520optimizes%2520control%2520strength%2520via%2520fine-grained%2520reference%2520frame%250Apositioning.%2520Comprehensive%2520evaluations%2520on%2520HDTF%252C%2520CelebV-HQ%252C%2520and%2520EMTD%2520datasets%250Ademonstrate%2520state-of-the-art%2520performance.%2520Quantitative%2520metrics%2520confirm%2520superior%250Avisual%2520realism%252C%2520emotional%2520coherence%252C%2520and%2520full-body%2520motion%2520synchronization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfiniteTalk%3A%20Audio-driven%20Video%20Generation%20for%20Sparse-Frame%20Video%0A%20%20Dubbing&entry.906535625=Shaoshu%20Yang%20and%20Zhe%20Kong%20and%20Feng%20Gao%20and%20Meng%20Cheng%20and%20Xiangyu%20Liu%20and%20Yong%20Zhang%20and%20Zhuoliang%20Kang%20and%20Wenhan%20Luo%20and%20Xunliang%20Cai%20and%20Ran%20He%20and%20Xiaoming%20Wei&entry.1292438233=%20%20Recent%20breakthroughs%20in%20video%20AIGC%20have%20ushered%20in%20a%20transformative%20era%20for%0Aaudio-driven%20human%20animation.%20However%2C%20conventional%20video%20dubbing%20techniques%0Aremain%20constrained%20to%20mouth%20region%20editing%2C%20resulting%20in%20discordant%20facial%0Aexpressions%20and%20body%20gestures%20that%20compromise%20viewer%20immersion.%20To%20overcome%0Athis%20limitation%2C%20we%20introduce%20sparse-frame%20video%20dubbing%2C%20a%20novel%20paradigm%20that%0Astrategically%20preserves%20reference%20keyframes%20to%20maintain%20identity%2C%20iconic%0Agestures%2C%20and%20camera%20trajectories%20while%20enabling%20holistic%2C%20audio-synchronized%0Afull-body%20motion%20editing.%20Through%20critical%20analysis%2C%20we%20identify%20why%20naive%0Aimage-to-video%20models%20fail%20in%20this%20task%2C%20particularly%20their%20inability%20to%0Aachieve%20adaptive%20conditioning.%20Addressing%20this%2C%20we%20propose%20InfiniteTalk%2C%20a%0Astreaming%20audio-driven%20generator%20designed%20for%20infinite-length%20long%20sequence%0Adubbing.%20This%20architecture%20leverages%20temporal%20context%20frames%20for%20seamless%0Ainter-chunk%20transitions%20and%20incorporates%20a%20simple%20yet%20effective%20sampling%0Astrategy%20that%20optimizes%20control%20strength%20via%20fine-grained%20reference%20frame%0Apositioning.%20Comprehensive%20evaluations%20on%20HDTF%2C%20CelebV-HQ%2C%20and%20EMTD%20datasets%0Ademonstrate%20state-of-the-art%20performance.%20Quantitative%20metrics%20confirm%20superior%0Avisual%20realism%2C%20emotional%20coherence%2C%20and%20full-body%20motion%20synchronization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14033v1&entry.124074799=Read"},
{"title": "Disentangled Deep Smoothed Bootstrap for Fair Imbalanced Regression", "author": "Samuel Stocksieker and Denys pommeret and Arthur Charpentier", "abstract": "  Imbalanced distribution learning is a common and significant challenge in\npredictive modeling, often reducing the performance of standard algorithms.\nAlthough various approaches address this issue, most are tailored to\nclassification problems, with a limited focus on regression. This paper\nintroduces a novel method to improve learning on tabular data within the\nImbalanced Regression (IR) framework, which is a critical problem. We propose\nusing Variational Autoencoders (VAEs) to model and define a latent\nrepresentation of data distributions. However, VAEs can be inefficient with\nimbalanced data like other standard approaches. To address this, we develop an\ninnovative data generation method that combines a disentangled VAE with a\nSmoothed Bootstrap applied in the latent space. We evaluate the efficiency of\nthis method through numerical comparisons with competitors on benchmark\ndatasets for IR.\n", "link": "http://arxiv.org/abs/2508.13829v1", "date": "2025-08-19", "relevancy": 2.4921, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.523}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4867}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Deep%20Smoothed%20Bootstrap%20for%20Fair%20Imbalanced%20Regression&body=Title%3A%20Disentangled%20Deep%20Smoothed%20Bootstrap%20for%20Fair%20Imbalanced%20Regression%0AAuthor%3A%20Samuel%20Stocksieker%20and%20Denys%20pommeret%20and%20Arthur%20Charpentier%0AAbstract%3A%20%20%20Imbalanced%20distribution%20learning%20is%20a%20common%20and%20significant%20challenge%20in%0Apredictive%20modeling%2C%20often%20reducing%20the%20performance%20of%20standard%20algorithms.%0AAlthough%20various%20approaches%20address%20this%20issue%2C%20most%20are%20tailored%20to%0Aclassification%20problems%2C%20with%20a%20limited%20focus%20on%20regression.%20This%20paper%0Aintroduces%20a%20novel%20method%20to%20improve%20learning%20on%20tabular%20data%20within%20the%0AImbalanced%20Regression%20%28IR%29%20framework%2C%20which%20is%20a%20critical%20problem.%20We%20propose%0Ausing%20Variational%20Autoencoders%20%28VAEs%29%20to%20model%20and%20define%20a%20latent%0Arepresentation%20of%20data%20distributions.%20However%2C%20VAEs%20can%20be%20inefficient%20with%0Aimbalanced%20data%20like%20other%20standard%20approaches.%20To%20address%20this%2C%20we%20develop%20an%0Ainnovative%20data%20generation%20method%20that%20combines%20a%20disentangled%20VAE%20with%20a%0ASmoothed%20Bootstrap%20applied%20in%20the%20latent%20space.%20We%20evaluate%20the%20efficiency%20of%0Athis%20method%20through%20numerical%20comparisons%20with%20competitors%20on%20benchmark%0Adatasets%20for%20IR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Deep%2520Smoothed%2520Bootstrap%2520for%2520Fair%2520Imbalanced%2520Regression%26entry.906535625%3DSamuel%2520Stocksieker%2520and%2520Denys%2520pommeret%2520and%2520Arthur%2520Charpentier%26entry.1292438233%3D%2520%2520Imbalanced%2520distribution%2520learning%2520is%2520a%2520common%2520and%2520significant%2520challenge%2520in%250Apredictive%2520modeling%252C%2520often%2520reducing%2520the%2520performance%2520of%2520standard%2520algorithms.%250AAlthough%2520various%2520approaches%2520address%2520this%2520issue%252C%2520most%2520are%2520tailored%2520to%250Aclassification%2520problems%252C%2520with%2520a%2520limited%2520focus%2520on%2520regression.%2520This%2520paper%250Aintroduces%2520a%2520novel%2520method%2520to%2520improve%2520learning%2520on%2520tabular%2520data%2520within%2520the%250AImbalanced%2520Regression%2520%2528IR%2529%2520framework%252C%2520which%2520is%2520a%2520critical%2520problem.%2520We%2520propose%250Ausing%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520to%2520model%2520and%2520define%2520a%2520latent%250Arepresentation%2520of%2520data%2520distributions.%2520However%252C%2520VAEs%2520can%2520be%2520inefficient%2520with%250Aimbalanced%2520data%2520like%2520other%2520standard%2520approaches.%2520To%2520address%2520this%252C%2520we%2520develop%2520an%250Ainnovative%2520data%2520generation%2520method%2520that%2520combines%2520a%2520disentangled%2520VAE%2520with%2520a%250ASmoothed%2520Bootstrap%2520applied%2520in%2520the%2520latent%2520space.%2520We%2520evaluate%2520the%2520efficiency%2520of%250Athis%2520method%2520through%2520numerical%2520comparisons%2520with%2520competitors%2520on%2520benchmark%250Adatasets%2520for%2520IR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Deep%20Smoothed%20Bootstrap%20for%20Fair%20Imbalanced%20Regression&entry.906535625=Samuel%20Stocksieker%20and%20Denys%20pommeret%20and%20Arthur%20Charpentier&entry.1292438233=%20%20Imbalanced%20distribution%20learning%20is%20a%20common%20and%20significant%20challenge%20in%0Apredictive%20modeling%2C%20often%20reducing%20the%20performance%20of%20standard%20algorithms.%0AAlthough%20various%20approaches%20address%20this%20issue%2C%20most%20are%20tailored%20to%0Aclassification%20problems%2C%20with%20a%20limited%20focus%20on%20regression.%20This%20paper%0Aintroduces%20a%20novel%20method%20to%20improve%20learning%20on%20tabular%20data%20within%20the%0AImbalanced%20Regression%20%28IR%29%20framework%2C%20which%20is%20a%20critical%20problem.%20We%20propose%0Ausing%20Variational%20Autoencoders%20%28VAEs%29%20to%20model%20and%20define%20a%20latent%0Arepresentation%20of%20data%20distributions.%20However%2C%20VAEs%20can%20be%20inefficient%20with%0Aimbalanced%20data%20like%20other%20standard%20approaches.%20To%20address%20this%2C%20we%20develop%20an%0Ainnovative%20data%20generation%20method%20that%20combines%20a%20disentangled%20VAE%20with%20a%0ASmoothed%20Bootstrap%20applied%20in%20the%20latent%20space.%20We%20evaluate%20the%20efficiency%20of%0Athis%20method%20through%20numerical%20comparisons%20with%20competitors%20on%20benchmark%0Adatasets%20for%20IR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13829v1&entry.124074799=Read"},
{"title": "Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing", "author": "Feng-Lin Liu and Shi-Yang Li and Yan-Pei Cao and Hongbo Fu and Lin Gao", "abstract": "  Recent video editing methods achieve attractive results in style transfer or\nappearance modification. However, editing the structural content of 3D scenes\nin videos remains challenging, particularly when dealing with significant\nviewpoint changes, such as large camera rotations or zooms. Key challenges\ninclude generating novel view content that remains consistent with the original\nvideo, preserving unedited regions, and translating sparse 2D inputs into\nrealistic 3D video outputs. To address these issues, we propose Sketch3DVE, a\nsketch-based 3D-aware video editing method to enable detailed local\nmanipulation of videos with significant viewpoint changes. To solve the\nchallenge posed by sparse inputs, we employ image editing methods to generate\nedited results for the first frame, which are then propagated to the remaining\nframes of the video. We utilize sketching as an interaction tool for precise\ngeometry control, while other mask-based image editing methods are also\nsupported. To handle viewpoint changes, we perform a detailed analysis and\nmanipulation of the 3D information in the video. Specifically, we utilize a\ndense stereo method to estimate a point cloud and the camera parameters of the\ninput video. We then propose a point cloud editing approach that uses depth\nmaps to represent the 3D geometry of newly edited components, aligning them\neffectively with the original 3D scene. To seamlessly merge the newly edited\ncontent with the original video while preserving the features of unedited\nregions, we introduce a 3D-aware mask propagation strategy and employ a video\ndiffusion model to produce realistic edited videos. Extensive experiments\ndemonstrate the superiority of Sketch3DVE in video editing. Homepage and code:\nhttp://http://geometrylearning.com/Sketch3DVE/\n", "link": "http://arxiv.org/abs/2508.13797v1", "date": "2025-08-19", "relevancy": 2.4437, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6152}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6101}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketch3DVE%3A%20Sketch-based%203D-Aware%20Scene%20Video%20Editing&body=Title%3A%20Sketch3DVE%3A%20Sketch-based%203D-Aware%20Scene%20Video%20Editing%0AAuthor%3A%20Feng-Lin%20Liu%20and%20Shi-Yang%20Li%20and%20Yan-Pei%20Cao%20and%20Hongbo%20Fu%20and%20Lin%20Gao%0AAbstract%3A%20%20%20Recent%20video%20editing%20methods%20achieve%20attractive%20results%20in%20style%20transfer%20or%0Aappearance%20modification.%20However%2C%20editing%20the%20structural%20content%20of%203D%20scenes%0Ain%20videos%20remains%20challenging%2C%20particularly%20when%20dealing%20with%20significant%0Aviewpoint%20changes%2C%20such%20as%20large%20camera%20rotations%20or%20zooms.%20Key%20challenges%0Ainclude%20generating%20novel%20view%20content%20that%20remains%20consistent%20with%20the%20original%0Avideo%2C%20preserving%20unedited%20regions%2C%20and%20translating%20sparse%202D%20inputs%20into%0Arealistic%203D%20video%20outputs.%20To%20address%20these%20issues%2C%20we%20propose%20Sketch3DVE%2C%20a%0Asketch-based%203D-aware%20video%20editing%20method%20to%20enable%20detailed%20local%0Amanipulation%20of%20videos%20with%20significant%20viewpoint%20changes.%20To%20solve%20the%0Achallenge%20posed%20by%20sparse%20inputs%2C%20we%20employ%20image%20editing%20methods%20to%20generate%0Aedited%20results%20for%20the%20first%20frame%2C%20which%20are%20then%20propagated%20to%20the%20remaining%0Aframes%20of%20the%20video.%20We%20utilize%20sketching%20as%20an%20interaction%20tool%20for%20precise%0Ageometry%20control%2C%20while%20other%20mask-based%20image%20editing%20methods%20are%20also%0Asupported.%20To%20handle%20viewpoint%20changes%2C%20we%20perform%20a%20detailed%20analysis%20and%0Amanipulation%20of%20the%203D%20information%20in%20the%20video.%20Specifically%2C%20we%20utilize%20a%0Adense%20stereo%20method%20to%20estimate%20a%20point%20cloud%20and%20the%20camera%20parameters%20of%20the%0Ainput%20video.%20We%20then%20propose%20a%20point%20cloud%20editing%20approach%20that%20uses%20depth%0Amaps%20to%20represent%20the%203D%20geometry%20of%20newly%20edited%20components%2C%20aligning%20them%0Aeffectively%20with%20the%20original%203D%20scene.%20To%20seamlessly%20merge%20the%20newly%20edited%0Acontent%20with%20the%20original%20video%20while%20preserving%20the%20features%20of%20unedited%0Aregions%2C%20we%20introduce%20a%203D-aware%20mask%20propagation%20strategy%20and%20employ%20a%20video%0Adiffusion%20model%20to%20produce%20realistic%20edited%20videos.%20Extensive%20experiments%0Ademonstrate%20the%20superiority%20of%20Sketch3DVE%20in%20video%20editing.%20Homepage%20and%20code%3A%0Ahttp%3A//http%3A//geometrylearning.com/Sketch3DVE/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketch3DVE%253A%2520Sketch-based%25203D-Aware%2520Scene%2520Video%2520Editing%26entry.906535625%3DFeng-Lin%2520Liu%2520and%2520Shi-Yang%2520Li%2520and%2520Yan-Pei%2520Cao%2520and%2520Hongbo%2520Fu%2520and%2520Lin%2520Gao%26entry.1292438233%3D%2520%2520Recent%2520video%2520editing%2520methods%2520achieve%2520attractive%2520results%2520in%2520style%2520transfer%2520or%250Aappearance%2520modification.%2520However%252C%2520editing%2520the%2520structural%2520content%2520of%25203D%2520scenes%250Ain%2520videos%2520remains%2520challenging%252C%2520particularly%2520when%2520dealing%2520with%2520significant%250Aviewpoint%2520changes%252C%2520such%2520as%2520large%2520camera%2520rotations%2520or%2520zooms.%2520Key%2520challenges%250Ainclude%2520generating%2520novel%2520view%2520content%2520that%2520remains%2520consistent%2520with%2520the%2520original%250Avideo%252C%2520preserving%2520unedited%2520regions%252C%2520and%2520translating%2520sparse%25202D%2520inputs%2520into%250Arealistic%25203D%2520video%2520outputs.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Sketch3DVE%252C%2520a%250Asketch-based%25203D-aware%2520video%2520editing%2520method%2520to%2520enable%2520detailed%2520local%250Amanipulation%2520of%2520videos%2520with%2520significant%2520viewpoint%2520changes.%2520To%2520solve%2520the%250Achallenge%2520posed%2520by%2520sparse%2520inputs%252C%2520we%2520employ%2520image%2520editing%2520methods%2520to%2520generate%250Aedited%2520results%2520for%2520the%2520first%2520frame%252C%2520which%2520are%2520then%2520propagated%2520to%2520the%2520remaining%250Aframes%2520of%2520the%2520video.%2520We%2520utilize%2520sketching%2520as%2520an%2520interaction%2520tool%2520for%2520precise%250Ageometry%2520control%252C%2520while%2520other%2520mask-based%2520image%2520editing%2520methods%2520are%2520also%250Asupported.%2520To%2520handle%2520viewpoint%2520changes%252C%2520we%2520perform%2520a%2520detailed%2520analysis%2520and%250Amanipulation%2520of%2520the%25203D%2520information%2520in%2520the%2520video.%2520Specifically%252C%2520we%2520utilize%2520a%250Adense%2520stereo%2520method%2520to%2520estimate%2520a%2520point%2520cloud%2520and%2520the%2520camera%2520parameters%2520of%2520the%250Ainput%2520video.%2520We%2520then%2520propose%2520a%2520point%2520cloud%2520editing%2520approach%2520that%2520uses%2520depth%250Amaps%2520to%2520represent%2520the%25203D%2520geometry%2520of%2520newly%2520edited%2520components%252C%2520aligning%2520them%250Aeffectively%2520with%2520the%2520original%25203D%2520scene.%2520To%2520seamlessly%2520merge%2520the%2520newly%2520edited%250Acontent%2520with%2520the%2520original%2520video%2520while%2520preserving%2520the%2520features%2520of%2520unedited%250Aregions%252C%2520we%2520introduce%2520a%25203D-aware%2520mask%2520propagation%2520strategy%2520and%2520employ%2520a%2520video%250Adiffusion%2520model%2520to%2520produce%2520realistic%2520edited%2520videos.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520superiority%2520of%2520Sketch3DVE%2520in%2520video%2520editing.%2520Homepage%2520and%2520code%253A%250Ahttp%253A//http%253A//geometrylearning.com/Sketch3DVE/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketch3DVE%3A%20Sketch-based%203D-Aware%20Scene%20Video%20Editing&entry.906535625=Feng-Lin%20Liu%20and%20Shi-Yang%20Li%20and%20Yan-Pei%20Cao%20and%20Hongbo%20Fu%20and%20Lin%20Gao&entry.1292438233=%20%20Recent%20video%20editing%20methods%20achieve%20attractive%20results%20in%20style%20transfer%20or%0Aappearance%20modification.%20However%2C%20editing%20the%20structural%20content%20of%203D%20scenes%0Ain%20videos%20remains%20challenging%2C%20particularly%20when%20dealing%20with%20significant%0Aviewpoint%20changes%2C%20such%20as%20large%20camera%20rotations%20or%20zooms.%20Key%20challenges%0Ainclude%20generating%20novel%20view%20content%20that%20remains%20consistent%20with%20the%20original%0Avideo%2C%20preserving%20unedited%20regions%2C%20and%20translating%20sparse%202D%20inputs%20into%0Arealistic%203D%20video%20outputs.%20To%20address%20these%20issues%2C%20we%20propose%20Sketch3DVE%2C%20a%0Asketch-based%203D-aware%20video%20editing%20method%20to%20enable%20detailed%20local%0Amanipulation%20of%20videos%20with%20significant%20viewpoint%20changes.%20To%20solve%20the%0Achallenge%20posed%20by%20sparse%20inputs%2C%20we%20employ%20image%20editing%20methods%20to%20generate%0Aedited%20results%20for%20the%20first%20frame%2C%20which%20are%20then%20propagated%20to%20the%20remaining%0Aframes%20of%20the%20video.%20We%20utilize%20sketching%20as%20an%20interaction%20tool%20for%20precise%0Ageometry%20control%2C%20while%20other%20mask-based%20image%20editing%20methods%20are%20also%0Asupported.%20To%20handle%20viewpoint%20changes%2C%20we%20perform%20a%20detailed%20analysis%20and%0Amanipulation%20of%20the%203D%20information%20in%20the%20video.%20Specifically%2C%20we%20utilize%20a%0Adense%20stereo%20method%20to%20estimate%20a%20point%20cloud%20and%20the%20camera%20parameters%20of%20the%0Ainput%20video.%20We%20then%20propose%20a%20point%20cloud%20editing%20approach%20that%20uses%20depth%0Amaps%20to%20represent%20the%203D%20geometry%20of%20newly%20edited%20components%2C%20aligning%20them%0Aeffectively%20with%20the%20original%203D%20scene.%20To%20seamlessly%20merge%20the%20newly%20edited%0Acontent%20with%20the%20original%20video%20while%20preserving%20the%20features%20of%20unedited%0Aregions%2C%20we%20introduce%20a%203D-aware%20mask%20propagation%20strategy%20and%20employ%20a%20video%0Adiffusion%20model%20to%20produce%20realistic%20edited%20videos.%20Extensive%20experiments%0Ademonstrate%20the%20superiority%20of%20Sketch3DVE%20in%20video%20editing.%20Homepage%20and%20code%3A%0Ahttp%3A//http%3A//geometrylearning.com/Sketch3DVE/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13797v1&entry.124074799=Read"},
{"title": "MEGA: Second-Order Gradient Alignment for Catastrophic Forgetting\n  Mitigation in GFSCIL", "author": "Jinhui Pang and Changqing Lin and Hao Lin and Zhihui Zhang and Long Chen and Weiping Ding and Yu Liu and Xiaoshuai Hao", "abstract": "  Graph Few-Shot Class-Incremental Learning (GFSCIL) enables models to\ncontinually learn from limited samples of novel tasks after initial training on\na large base dataset. Existing GFSCIL approaches typically utilize Prototypical\nNetworks (PNs) for metric-based class representations and fine-tune the model\nduring the incremental learning stage. However, these PN-based methods\noversimplify learning via novel query set fine-tuning and fail to integrate\nGraph Continual Learning (GCL) techniques due to architectural constraints. To\naddress these challenges, we propose a more rigorous and practical setting for\nGFSCIL that excludes query sets during the incremental training phase. Building\non this foundation, we introduce Model-Agnostic Meta Graph Continual Learning\n(MEGA), aimed at effectively alleviating catastrophic forgetting for GFSCIL.\nSpecifically, by calculating the incremental second-order gradient during the\nmeta-training stage, we endow the model to learn high-quality priors that\nenhance incremental learning by aligning its behaviors across both the\nmeta-training and incremental learning stages. Extensive experiments on four\nmainstream graph datasets demonstrate that MEGA achieves state-of-the-art\nresults and enhances the effectiveness of various GCL methods in GFSCIL. We\nbelieve that our proposed MEGA serves as a model-agnostic GFSCIL paradigm,\npaving the way for future research.\n", "link": "http://arxiv.org/abs/2504.13691v2", "date": "2025-08-19", "relevancy": 2.4398, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4956}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4889}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEGA%3A%20Second-Order%20Gradient%20Alignment%20for%20Catastrophic%20Forgetting%0A%20%20Mitigation%20in%20GFSCIL&body=Title%3A%20MEGA%3A%20Second-Order%20Gradient%20Alignment%20for%20Catastrophic%20Forgetting%0A%20%20Mitigation%20in%20GFSCIL%0AAuthor%3A%20Jinhui%20Pang%20and%20Changqing%20Lin%20and%20Hao%20Lin%20and%20Zhihui%20Zhang%20and%20Long%20Chen%20and%20Weiping%20Ding%20and%20Yu%20Liu%20and%20Xiaoshuai%20Hao%0AAbstract%3A%20%20%20Graph%20Few-Shot%20Class-Incremental%20Learning%20%28GFSCIL%29%20enables%20models%20to%0Acontinually%20learn%20from%20limited%20samples%20of%20novel%20tasks%20after%20initial%20training%20on%0Aa%20large%20base%20dataset.%20Existing%20GFSCIL%20approaches%20typically%20utilize%20Prototypical%0ANetworks%20%28PNs%29%20for%20metric-based%20class%20representations%20and%20fine-tune%20the%20model%0Aduring%20the%20incremental%20learning%20stage.%20However%2C%20these%20PN-based%20methods%0Aoversimplify%20learning%20via%20novel%20query%20set%20fine-tuning%20and%20fail%20to%20integrate%0AGraph%20Continual%20Learning%20%28GCL%29%20techniques%20due%20to%20architectural%20constraints.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20more%20rigorous%20and%20practical%20setting%20for%0AGFSCIL%20that%20excludes%20query%20sets%20during%20the%20incremental%20training%20phase.%20Building%0Aon%20this%20foundation%2C%20we%20introduce%20Model-Agnostic%20Meta%20Graph%20Continual%20Learning%0A%28MEGA%29%2C%20aimed%20at%20effectively%20alleviating%20catastrophic%20forgetting%20for%20GFSCIL.%0ASpecifically%2C%20by%20calculating%20the%20incremental%20second-order%20gradient%20during%20the%0Ameta-training%20stage%2C%20we%20endow%20the%20model%20to%20learn%20high-quality%20priors%20that%0Aenhance%20incremental%20learning%20by%20aligning%20its%20behaviors%20across%20both%20the%0Ameta-training%20and%20incremental%20learning%20stages.%20Extensive%20experiments%20on%20four%0Amainstream%20graph%20datasets%20demonstrate%20that%20MEGA%20achieves%20state-of-the-art%0Aresults%20and%20enhances%20the%20effectiveness%20of%20various%20GCL%20methods%20in%20GFSCIL.%20We%0Abelieve%20that%20our%20proposed%20MEGA%20serves%20as%20a%20model-agnostic%20GFSCIL%20paradigm%2C%0Apaving%20the%20way%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13691v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEGA%253A%2520Second-Order%2520Gradient%2520Alignment%2520for%2520Catastrophic%2520Forgetting%250A%2520%2520Mitigation%2520in%2520GFSCIL%26entry.906535625%3DJinhui%2520Pang%2520and%2520Changqing%2520Lin%2520and%2520Hao%2520Lin%2520and%2520Zhihui%2520Zhang%2520and%2520Long%2520Chen%2520and%2520Weiping%2520Ding%2520and%2520Yu%2520Liu%2520and%2520Xiaoshuai%2520Hao%26entry.1292438233%3D%2520%2520Graph%2520Few-Shot%2520Class-Incremental%2520Learning%2520%2528GFSCIL%2529%2520enables%2520models%2520to%250Acontinually%2520learn%2520from%2520limited%2520samples%2520of%2520novel%2520tasks%2520after%2520initial%2520training%2520on%250Aa%2520large%2520base%2520dataset.%2520Existing%2520GFSCIL%2520approaches%2520typically%2520utilize%2520Prototypical%250ANetworks%2520%2528PNs%2529%2520for%2520metric-based%2520class%2520representations%2520and%2520fine-tune%2520the%2520model%250Aduring%2520the%2520incremental%2520learning%2520stage.%2520However%252C%2520these%2520PN-based%2520methods%250Aoversimplify%2520learning%2520via%2520novel%2520query%2520set%2520fine-tuning%2520and%2520fail%2520to%2520integrate%250AGraph%2520Continual%2520Learning%2520%2528GCL%2529%2520techniques%2520due%2520to%2520architectural%2520constraints.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520more%2520rigorous%2520and%2520practical%2520setting%2520for%250AGFSCIL%2520that%2520excludes%2520query%2520sets%2520during%2520the%2520incremental%2520training%2520phase.%2520Building%250Aon%2520this%2520foundation%252C%2520we%2520introduce%2520Model-Agnostic%2520Meta%2520Graph%2520Continual%2520Learning%250A%2528MEGA%2529%252C%2520aimed%2520at%2520effectively%2520alleviating%2520catastrophic%2520forgetting%2520for%2520GFSCIL.%250ASpecifically%252C%2520by%2520calculating%2520the%2520incremental%2520second-order%2520gradient%2520during%2520the%250Ameta-training%2520stage%252C%2520we%2520endow%2520the%2520model%2520to%2520learn%2520high-quality%2520priors%2520that%250Aenhance%2520incremental%2520learning%2520by%2520aligning%2520its%2520behaviors%2520across%2520both%2520the%250Ameta-training%2520and%2520incremental%2520learning%2520stages.%2520Extensive%2520experiments%2520on%2520four%250Amainstream%2520graph%2520datasets%2520demonstrate%2520that%2520MEGA%2520achieves%2520state-of-the-art%250Aresults%2520and%2520enhances%2520the%2520effectiveness%2520of%2520various%2520GCL%2520methods%2520in%2520GFSCIL.%2520We%250Abelieve%2520that%2520our%2520proposed%2520MEGA%2520serves%2520as%2520a%2520model-agnostic%2520GFSCIL%2520paradigm%252C%250Apaving%2520the%2520way%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13691v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEGA%3A%20Second-Order%20Gradient%20Alignment%20for%20Catastrophic%20Forgetting%0A%20%20Mitigation%20in%20GFSCIL&entry.906535625=Jinhui%20Pang%20and%20Changqing%20Lin%20and%20Hao%20Lin%20and%20Zhihui%20Zhang%20and%20Long%20Chen%20and%20Weiping%20Ding%20and%20Yu%20Liu%20and%20Xiaoshuai%20Hao&entry.1292438233=%20%20Graph%20Few-Shot%20Class-Incremental%20Learning%20%28GFSCIL%29%20enables%20models%20to%0Acontinually%20learn%20from%20limited%20samples%20of%20novel%20tasks%20after%20initial%20training%20on%0Aa%20large%20base%20dataset.%20Existing%20GFSCIL%20approaches%20typically%20utilize%20Prototypical%0ANetworks%20%28PNs%29%20for%20metric-based%20class%20representations%20and%20fine-tune%20the%20model%0Aduring%20the%20incremental%20learning%20stage.%20However%2C%20these%20PN-based%20methods%0Aoversimplify%20learning%20via%20novel%20query%20set%20fine-tuning%20and%20fail%20to%20integrate%0AGraph%20Continual%20Learning%20%28GCL%29%20techniques%20due%20to%20architectural%20constraints.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20more%20rigorous%20and%20practical%20setting%20for%0AGFSCIL%20that%20excludes%20query%20sets%20during%20the%20incremental%20training%20phase.%20Building%0Aon%20this%20foundation%2C%20we%20introduce%20Model-Agnostic%20Meta%20Graph%20Continual%20Learning%0A%28MEGA%29%2C%20aimed%20at%20effectively%20alleviating%20catastrophic%20forgetting%20for%20GFSCIL.%0ASpecifically%2C%20by%20calculating%20the%20incremental%20second-order%20gradient%20during%20the%0Ameta-training%20stage%2C%20we%20endow%20the%20model%20to%20learn%20high-quality%20priors%20that%0Aenhance%20incremental%20learning%20by%20aligning%20its%20behaviors%20across%20both%20the%0Ameta-training%20and%20incremental%20learning%20stages.%20Extensive%20experiments%20on%20four%0Amainstream%20graph%20datasets%20demonstrate%20that%20MEGA%20achieves%20state-of-the-art%0Aresults%20and%20enhances%20the%20effectiveness%20of%20various%20GCL%20methods%20in%20GFSCIL.%20We%0Abelieve%20that%20our%20proposed%20MEGA%20serves%20as%20a%20model-agnostic%20GFSCIL%20paradigm%2C%0Apaving%20the%20way%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13691v2&entry.124074799=Read"},
{"title": "ConTextTab: A Semantics-Aware Tabular In-Context Learner", "author": "Marco Spinaci and Marek Polewczyk and Maximilian Schambach and Sam Thelin", "abstract": "  Tabular in-context learning (ICL) has recently achieved state-of-the-art\n(SOTA) performance on several tabular prediction tasks. Previously restricted\nto classification problems on small tables, recent advances such as TabPFN and\nTabICL have extended its use to larger datasets. Although current table-native\nICL architectures are architecturally efficient and well-adapted to tabular\ndata structures, their exclusive training on synthetic data limits their\nability to fully leverage the rich semantics and world knowledge contained in\nreal-world tabular data. At the other end of the spectrum, tabular ICL models\nbased on pretrained large language models such as TabuLa-8B integrate deep\nsemantic understanding and world knowledge but are only able to make use of a\nsmall amount of context due to inherent architectural limitations. With the aim\nto combine the best of both these worlds, we introduce ConTextTab, integrating\nsemantic understanding and alignment into a table-native ICL framework. By\nemploying specialized embeddings for different data modalities and by training\non large-scale real-world tabular data, our model is competitive with SOTA\nacross a broad set of benchmarks while setting a new standard on the\nsemantically rich CARTE benchmark. Code and model checkpoints are available at:\nhttps://github.com/SAP-samples/contexttab\n", "link": "http://arxiv.org/abs/2506.10707v3", "date": "2025-08-19", "relevancy": 2.4162, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConTextTab%3A%20A%20Semantics-Aware%20Tabular%20In-Context%20Learner&body=Title%3A%20ConTextTab%3A%20A%20Semantics-Aware%20Tabular%20In-Context%20Learner%0AAuthor%3A%20Marco%20Spinaci%20and%20Marek%20Polewczyk%20and%20Maximilian%20Schambach%20and%20Sam%20Thelin%0AAbstract%3A%20%20%20Tabular%20in-context%20learning%20%28ICL%29%20has%20recently%20achieved%20state-of-the-art%0A%28SOTA%29%20performance%20on%20several%20tabular%20prediction%20tasks.%20Previously%20restricted%0Ato%20classification%20problems%20on%20small%20tables%2C%20recent%20advances%20such%20as%20TabPFN%20and%0ATabICL%20have%20extended%20its%20use%20to%20larger%20datasets.%20Although%20current%20table-native%0AICL%20architectures%20are%20architecturally%20efficient%20and%20well-adapted%20to%20tabular%0Adata%20structures%2C%20their%20exclusive%20training%20on%20synthetic%20data%20limits%20their%0Aability%20to%20fully%20leverage%20the%20rich%20semantics%20and%20world%20knowledge%20contained%20in%0Areal-world%20tabular%20data.%20At%20the%20other%20end%20of%20the%20spectrum%2C%20tabular%20ICL%20models%0Abased%20on%20pretrained%20large%20language%20models%20such%20as%20TabuLa-8B%20integrate%20deep%0Asemantic%20understanding%20and%20world%20knowledge%20but%20are%20only%20able%20to%20make%20use%20of%20a%0Asmall%20amount%20of%20context%20due%20to%20inherent%20architectural%20limitations.%20With%20the%20aim%0Ato%20combine%20the%20best%20of%20both%20these%20worlds%2C%20we%20introduce%20ConTextTab%2C%20integrating%0Asemantic%20understanding%20and%20alignment%20into%20a%20table-native%20ICL%20framework.%20By%0Aemploying%20specialized%20embeddings%20for%20different%20data%20modalities%20and%20by%20training%0Aon%20large-scale%20real-world%20tabular%20data%2C%20our%20model%20is%20competitive%20with%20SOTA%0Aacross%20a%20broad%20set%20of%20benchmarks%20while%20setting%20a%20new%20standard%20on%20the%0Asemantically%20rich%20CARTE%20benchmark.%20Code%20and%20model%20checkpoints%20are%20available%20at%3A%0Ahttps%3A//github.com/SAP-samples/contexttab%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10707v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConTextTab%253A%2520A%2520Semantics-Aware%2520Tabular%2520In-Context%2520Learner%26entry.906535625%3DMarco%2520Spinaci%2520and%2520Marek%2520Polewczyk%2520and%2520Maximilian%2520Schambach%2520and%2520Sam%2520Thelin%26entry.1292438233%3D%2520%2520Tabular%2520in-context%2520learning%2520%2528ICL%2529%2520has%2520recently%2520achieved%2520state-of-the-art%250A%2528SOTA%2529%2520performance%2520on%2520several%2520tabular%2520prediction%2520tasks.%2520Previously%2520restricted%250Ato%2520classification%2520problems%2520on%2520small%2520tables%252C%2520recent%2520advances%2520such%2520as%2520TabPFN%2520and%250ATabICL%2520have%2520extended%2520its%2520use%2520to%2520larger%2520datasets.%2520Although%2520current%2520table-native%250AICL%2520architectures%2520are%2520architecturally%2520efficient%2520and%2520well-adapted%2520to%2520tabular%250Adata%2520structures%252C%2520their%2520exclusive%2520training%2520on%2520synthetic%2520data%2520limits%2520their%250Aability%2520to%2520fully%2520leverage%2520the%2520rich%2520semantics%2520and%2520world%2520knowledge%2520contained%2520in%250Areal-world%2520tabular%2520data.%2520At%2520the%2520other%2520end%2520of%2520the%2520spectrum%252C%2520tabular%2520ICL%2520models%250Abased%2520on%2520pretrained%2520large%2520language%2520models%2520such%2520as%2520TabuLa-8B%2520integrate%2520deep%250Asemantic%2520understanding%2520and%2520world%2520knowledge%2520but%2520are%2520only%2520able%2520to%2520make%2520use%2520of%2520a%250Asmall%2520amount%2520of%2520context%2520due%2520to%2520inherent%2520architectural%2520limitations.%2520With%2520the%2520aim%250Ato%2520combine%2520the%2520best%2520of%2520both%2520these%2520worlds%252C%2520we%2520introduce%2520ConTextTab%252C%2520integrating%250Asemantic%2520understanding%2520and%2520alignment%2520into%2520a%2520table-native%2520ICL%2520framework.%2520By%250Aemploying%2520specialized%2520embeddings%2520for%2520different%2520data%2520modalities%2520and%2520by%2520training%250Aon%2520large-scale%2520real-world%2520tabular%2520data%252C%2520our%2520model%2520is%2520competitive%2520with%2520SOTA%250Aacross%2520a%2520broad%2520set%2520of%2520benchmarks%2520while%2520setting%2520a%2520new%2520standard%2520on%2520the%250Asemantically%2520rich%2520CARTE%2520benchmark.%2520Code%2520and%2520model%2520checkpoints%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/SAP-samples/contexttab%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10707v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConTextTab%3A%20A%20Semantics-Aware%20Tabular%20In-Context%20Learner&entry.906535625=Marco%20Spinaci%20and%20Marek%20Polewczyk%20and%20Maximilian%20Schambach%20and%20Sam%20Thelin&entry.1292438233=%20%20Tabular%20in-context%20learning%20%28ICL%29%20has%20recently%20achieved%20state-of-the-art%0A%28SOTA%29%20performance%20on%20several%20tabular%20prediction%20tasks.%20Previously%20restricted%0Ato%20classification%20problems%20on%20small%20tables%2C%20recent%20advances%20such%20as%20TabPFN%20and%0ATabICL%20have%20extended%20its%20use%20to%20larger%20datasets.%20Although%20current%20table-native%0AICL%20architectures%20are%20architecturally%20efficient%20and%20well-adapted%20to%20tabular%0Adata%20structures%2C%20their%20exclusive%20training%20on%20synthetic%20data%20limits%20their%0Aability%20to%20fully%20leverage%20the%20rich%20semantics%20and%20world%20knowledge%20contained%20in%0Areal-world%20tabular%20data.%20At%20the%20other%20end%20of%20the%20spectrum%2C%20tabular%20ICL%20models%0Abased%20on%20pretrained%20large%20language%20models%20such%20as%20TabuLa-8B%20integrate%20deep%0Asemantic%20understanding%20and%20world%20knowledge%20but%20are%20only%20able%20to%20make%20use%20of%20a%0Asmall%20amount%20of%20context%20due%20to%20inherent%20architectural%20limitations.%20With%20the%20aim%0Ato%20combine%20the%20best%20of%20both%20these%20worlds%2C%20we%20introduce%20ConTextTab%2C%20integrating%0Asemantic%20understanding%20and%20alignment%20into%20a%20table-native%20ICL%20framework.%20By%0Aemploying%20specialized%20embeddings%20for%20different%20data%20modalities%20and%20by%20training%0Aon%20large-scale%20real-world%20tabular%20data%2C%20our%20model%20is%20competitive%20with%20SOTA%0Aacross%20a%20broad%20set%20of%20benchmarks%20while%20setting%20a%20new%20standard%20on%20the%0Asemantically%20rich%20CARTE%20benchmark.%20Code%20and%20model%20checkpoints%20are%20available%20at%3A%0Ahttps%3A//github.com/SAP-samples/contexttab%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10707v3&entry.124074799=Read"},
{"title": "Generics and Default Reasoning in Large Language Models", "author": "James Ravi Kirkpatrick and Rachel Katharine Sterken", "abstract": "  This paper evaluates the capabilities of 28 large language models (LLMs) to\nreason with 20 defeasible reasoning patterns involving generic generalizations\n(e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic.\nGenerics are of special interest to linguists, philosophers, logicians, and\ncognitive scientists because of their complex exception-permitting behaviour\nand their centrality to default reasoning, cognition, and concept acquisition.\nWe find that while several frontier models handle many default reasoning\nproblems well, performance varies widely across models and prompting styles.\nFew-shot prompting modestly improves performance for some models, but\nchain-of-thought (CoT) prompting often leads to serious performance degradation\n(mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy\nin zero-shot condition, temperature 0). Most models either struggle to\ndistinguish between defeasible and deductive inference or misinterpret generics\nas universal statements. These findings underscore both the promise and limits\nof current LLMs for default reasoning.\n", "link": "http://arxiv.org/abs/2508.13718v1", "date": "2025-08-19", "relevancy": 2.4111, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.503}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generics%20and%20Default%20Reasoning%20in%20Large%20Language%20Models&body=Title%3A%20Generics%20and%20Default%20Reasoning%20in%20Large%20Language%20Models%0AAuthor%3A%20James%20Ravi%20Kirkpatrick%20and%20Rachel%20Katharine%20Sterken%0AAbstract%3A%20%20%20This%20paper%20evaluates%20the%20capabilities%20of%2028%20large%20language%20models%20%28LLMs%29%20to%0Areason%20with%2020%20defeasible%20reasoning%20patterns%20involving%20generic%20generalizations%0A%28e.g.%2C%20%27Birds%20fly%27%2C%20%27Ravens%20are%20black%27%29%20central%20to%20non-monotonic%20logic.%0AGenerics%20are%20of%20special%20interest%20to%20linguists%2C%20philosophers%2C%20logicians%2C%20and%0Acognitive%20scientists%20because%20of%20their%20complex%20exception-permitting%20behaviour%0Aand%20their%20centrality%20to%20default%20reasoning%2C%20cognition%2C%20and%20concept%20acquisition.%0AWe%20find%20that%20while%20several%20frontier%20models%20handle%20many%20default%20reasoning%0Aproblems%20well%2C%20performance%20varies%20widely%20across%20models%20and%20prompting%20styles.%0AFew-shot%20prompting%20modestly%20improves%20performance%20for%20some%20models%2C%20but%0Achain-of-thought%20%28CoT%29%20prompting%20often%20leads%20to%20serious%20performance%20degradation%0A%28mean%20accuracy%20drop%20-11.14%25%2C%20SD%2015.74%25%20in%20models%20performing%20above%2075%25%20accuracy%0Ain%20zero-shot%20condition%2C%20temperature%200%29.%20Most%20models%20either%20struggle%20to%0Adistinguish%20between%20defeasible%20and%20deductive%20inference%20or%20misinterpret%20generics%0Aas%20universal%20statements.%20These%20findings%20underscore%20both%20the%20promise%20and%20limits%0Aof%20current%20LLMs%20for%20default%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerics%2520and%2520Default%2520Reasoning%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DJames%2520Ravi%2520Kirkpatrick%2520and%2520Rachel%2520Katharine%2520Sterken%26entry.1292438233%3D%2520%2520This%2520paper%2520evaluates%2520the%2520capabilities%2520of%252028%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%250Areason%2520with%252020%2520defeasible%2520reasoning%2520patterns%2520involving%2520generic%2520generalizations%250A%2528e.g.%252C%2520%2527Birds%2520fly%2527%252C%2520%2527Ravens%2520are%2520black%2527%2529%2520central%2520to%2520non-monotonic%2520logic.%250AGenerics%2520are%2520of%2520special%2520interest%2520to%2520linguists%252C%2520philosophers%252C%2520logicians%252C%2520and%250Acognitive%2520scientists%2520because%2520of%2520their%2520complex%2520exception-permitting%2520behaviour%250Aand%2520their%2520centrality%2520to%2520default%2520reasoning%252C%2520cognition%252C%2520and%2520concept%2520acquisition.%250AWe%2520find%2520that%2520while%2520several%2520frontier%2520models%2520handle%2520many%2520default%2520reasoning%250Aproblems%2520well%252C%2520performance%2520varies%2520widely%2520across%2520models%2520and%2520prompting%2520styles.%250AFew-shot%2520prompting%2520modestly%2520improves%2520performance%2520for%2520some%2520models%252C%2520but%250Achain-of-thought%2520%2528CoT%2529%2520prompting%2520often%2520leads%2520to%2520serious%2520performance%2520degradation%250A%2528mean%2520accuracy%2520drop%2520-11.14%2525%252C%2520SD%252015.74%2525%2520in%2520models%2520performing%2520above%252075%2525%2520accuracy%250Ain%2520zero-shot%2520condition%252C%2520temperature%25200%2529.%2520Most%2520models%2520either%2520struggle%2520to%250Adistinguish%2520between%2520defeasible%2520and%2520deductive%2520inference%2520or%2520misinterpret%2520generics%250Aas%2520universal%2520statements.%2520These%2520findings%2520underscore%2520both%2520the%2520promise%2520and%2520limits%250Aof%2520current%2520LLMs%2520for%2520default%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generics%20and%20Default%20Reasoning%20in%20Large%20Language%20Models&entry.906535625=James%20Ravi%20Kirkpatrick%20and%20Rachel%20Katharine%20Sterken&entry.1292438233=%20%20This%20paper%20evaluates%20the%20capabilities%20of%2028%20large%20language%20models%20%28LLMs%29%20to%0Areason%20with%2020%20defeasible%20reasoning%20patterns%20involving%20generic%20generalizations%0A%28e.g.%2C%20%27Birds%20fly%27%2C%20%27Ravens%20are%20black%27%29%20central%20to%20non-monotonic%20logic.%0AGenerics%20are%20of%20special%20interest%20to%20linguists%2C%20philosophers%2C%20logicians%2C%20and%0Acognitive%20scientists%20because%20of%20their%20complex%20exception-permitting%20behaviour%0Aand%20their%20centrality%20to%20default%20reasoning%2C%20cognition%2C%20and%20concept%20acquisition.%0AWe%20find%20that%20while%20several%20frontier%20models%20handle%20many%20default%20reasoning%0Aproblems%20well%2C%20performance%20varies%20widely%20across%20models%20and%20prompting%20styles.%0AFew-shot%20prompting%20modestly%20improves%20performance%20for%20some%20models%2C%20but%0Achain-of-thought%20%28CoT%29%20prompting%20often%20leads%20to%20serious%20performance%20degradation%0A%28mean%20accuracy%20drop%20-11.14%25%2C%20SD%2015.74%25%20in%20models%20performing%20above%2075%25%20accuracy%0Ain%20zero-shot%20condition%2C%20temperature%200%29.%20Most%20models%20either%20struggle%20to%0Adistinguish%20between%20defeasible%20and%20deductive%20inference%20or%20misinterpret%20generics%0Aas%20universal%20statements.%20These%20findings%20underscore%20both%20the%20promise%20and%20limits%0Aof%20current%20LLMs%20for%20default%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13718v1&entry.124074799=Read"},
{"title": "InPars+: Supercharging Synthetic Data Generation for Information\n  Retrieval Systems", "author": "Matey Krastev and Miklos Hamar and Danilo Toapanta and Jesse Brouwers and Yibin Lei", "abstract": "  This work revisits and extends synthetic query generation pipelines for\nNeural Information Retrieval (NIR) by leveraging the InPars Toolkit, a\nreproducible, end-to-end framework for generating training data using large\nlanguage models (LLMs). We first assess the reproducibility of the original\nInPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and\nvalidate their effectiveness using open-source reranker and generator models.\nBuilding on this foundation, we introduce two key extensions to the pipeline:\n(1) fine-tuning a query generator LLM via Contrastive Preference Optimization\n(CPO) to improve the signal quality in generated queries, and (2) replacing\nstatic prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts\nusing the DSPy framework. Our results show that both extensions reduce the need\nfor aggressive filtering while improving retrieval performance. All code,\nmodels, and synthetic datasets are publicly released to support further\nresearch at: \\href{https://github.com/danilotpnta/IR2-project}{this https URL}.\n", "link": "http://arxiv.org/abs/2508.13930v1", "date": "2025-08-19", "relevancy": 2.4036, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4885}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4829}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InPars%2B%3A%20Supercharging%20Synthetic%20Data%20Generation%20for%20Information%0A%20%20Retrieval%20Systems&body=Title%3A%20InPars%2B%3A%20Supercharging%20Synthetic%20Data%20Generation%20for%20Information%0A%20%20Retrieval%20Systems%0AAuthor%3A%20Matey%20Krastev%20and%20Miklos%20Hamar%20and%20Danilo%20Toapanta%20and%20Jesse%20Brouwers%20and%20Yibin%20Lei%0AAbstract%3A%20%20%20This%20work%20revisits%20and%20extends%20synthetic%20query%20generation%20pipelines%20for%0ANeural%20Information%20Retrieval%20%28NIR%29%20by%20leveraging%20the%20InPars%20Toolkit%2C%20a%0Areproducible%2C%20end-to-end%20framework%20for%20generating%20training%20data%20using%20large%0Alanguage%20models%20%28LLMs%29.%20We%20first%20assess%20the%20reproducibility%20of%20the%20original%0AInPars%2C%20InPars-V2%2C%20and%20Promptagator%20pipelines%20on%20the%20SciFact%20benchmark%20and%0Avalidate%20their%20effectiveness%20using%20open-source%20reranker%20and%20generator%20models.%0ABuilding%20on%20this%20foundation%2C%20we%20introduce%20two%20key%20extensions%20to%20the%20pipeline%3A%0A%281%29%20fine-tuning%20a%20query%20generator%20LLM%20via%20Contrastive%20Preference%20Optimization%0A%28CPO%29%20to%20improve%20the%20signal%20quality%20in%20generated%20queries%2C%20and%20%282%29%20replacing%0Astatic%20prompt%20templates%20with%20dynamic%2C%20Chain-of-Thought%20%28CoT%29%20optimized%20prompts%0Ausing%20the%20DSPy%20framework.%20Our%20results%20show%20that%20both%20extensions%20reduce%20the%20need%0Afor%20aggressive%20filtering%20while%20improving%20retrieval%20performance.%20All%20code%2C%0Amodels%2C%20and%20synthetic%20datasets%20are%20publicly%20released%20to%20support%20further%0Aresearch%20at%3A%20%5Chref%7Bhttps%3A//github.com/danilotpnta/IR2-project%7D%7Bthis%20https%20URL%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInPars%252B%253A%2520Supercharging%2520Synthetic%2520Data%2520Generation%2520for%2520Information%250A%2520%2520Retrieval%2520Systems%26entry.906535625%3DMatey%2520Krastev%2520and%2520Miklos%2520Hamar%2520and%2520Danilo%2520Toapanta%2520and%2520Jesse%2520Brouwers%2520and%2520Yibin%2520Lei%26entry.1292438233%3D%2520%2520This%2520work%2520revisits%2520and%2520extends%2520synthetic%2520query%2520generation%2520pipelines%2520for%250ANeural%2520Information%2520Retrieval%2520%2528NIR%2529%2520by%2520leveraging%2520the%2520InPars%2520Toolkit%252C%2520a%250Areproducible%252C%2520end-to-end%2520framework%2520for%2520generating%2520training%2520data%2520using%2520large%250Alanguage%2520models%2520%2528LLMs%2529.%2520We%2520first%2520assess%2520the%2520reproducibility%2520of%2520the%2520original%250AInPars%252C%2520InPars-V2%252C%2520and%2520Promptagator%2520pipelines%2520on%2520the%2520SciFact%2520benchmark%2520and%250Avalidate%2520their%2520effectiveness%2520using%2520open-source%2520reranker%2520and%2520generator%2520models.%250ABuilding%2520on%2520this%2520foundation%252C%2520we%2520introduce%2520two%2520key%2520extensions%2520to%2520the%2520pipeline%253A%250A%25281%2529%2520fine-tuning%2520a%2520query%2520generator%2520LLM%2520via%2520Contrastive%2520Preference%2520Optimization%250A%2528CPO%2529%2520to%2520improve%2520the%2520signal%2520quality%2520in%2520generated%2520queries%252C%2520and%2520%25282%2529%2520replacing%250Astatic%2520prompt%2520templates%2520with%2520dynamic%252C%2520Chain-of-Thought%2520%2528CoT%2529%2520optimized%2520prompts%250Ausing%2520the%2520DSPy%2520framework.%2520Our%2520results%2520show%2520that%2520both%2520extensions%2520reduce%2520the%2520need%250Afor%2520aggressive%2520filtering%2520while%2520improving%2520retrieval%2520performance.%2520All%2520code%252C%250Amodels%252C%2520and%2520synthetic%2520datasets%2520are%2520publicly%2520released%2520to%2520support%2520further%250Aresearch%2520at%253A%2520%255Chref%257Bhttps%253A//github.com/danilotpnta/IR2-project%257D%257Bthis%2520https%2520URL%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InPars%2B%3A%20Supercharging%20Synthetic%20Data%20Generation%20for%20Information%0A%20%20Retrieval%20Systems&entry.906535625=Matey%20Krastev%20and%20Miklos%20Hamar%20and%20Danilo%20Toapanta%20and%20Jesse%20Brouwers%20and%20Yibin%20Lei&entry.1292438233=%20%20This%20work%20revisits%20and%20extends%20synthetic%20query%20generation%20pipelines%20for%0ANeural%20Information%20Retrieval%20%28NIR%29%20by%20leveraging%20the%20InPars%20Toolkit%2C%20a%0Areproducible%2C%20end-to-end%20framework%20for%20generating%20training%20data%20using%20large%0Alanguage%20models%20%28LLMs%29.%20We%20first%20assess%20the%20reproducibility%20of%20the%20original%0AInPars%2C%20InPars-V2%2C%20and%20Promptagator%20pipelines%20on%20the%20SciFact%20benchmark%20and%0Avalidate%20their%20effectiveness%20using%20open-source%20reranker%20and%20generator%20models.%0ABuilding%20on%20this%20foundation%2C%20we%20introduce%20two%20key%20extensions%20to%20the%20pipeline%3A%0A%281%29%20fine-tuning%20a%20query%20generator%20LLM%20via%20Contrastive%20Preference%20Optimization%0A%28CPO%29%20to%20improve%20the%20signal%20quality%20in%20generated%20queries%2C%20and%20%282%29%20replacing%0Astatic%20prompt%20templates%20with%20dynamic%2C%20Chain-of-Thought%20%28CoT%29%20optimized%20prompts%0Ausing%20the%20DSPy%20framework.%20Our%20results%20show%20that%20both%20extensions%20reduce%20the%20need%0Afor%20aggressive%20filtering%20while%20improving%20retrieval%20performance.%20All%20code%2C%0Amodels%2C%20and%20synthetic%20datasets%20are%20publicly%20released%20to%20support%20further%0Aresearch%20at%3A%20%5Chref%7Bhttps%3A//github.com/danilotpnta/IR2-project%7D%7Bthis%20https%20URL%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13930v1&entry.124074799=Read"},
{"title": "Latent Interpolation Learning Using Diffusion Models for Cardiac Volume\n  Reconstruction", "author": "Niklas Bubeck and Suprosanna Shit and Chen Chen and Can Zhao and Pengfei Guo and Dong Yang and Georg Zitzlsberger and Daguang Xu and Bernhard Kainz and Daniel Rueckert and Jiazhen Pan", "abstract": "  Cardiac Magnetic Resonance (CMR) imaging is a critical tool for diagnosing\nand managing cardiovascular disease, yet its utility is often limited by the\nsparse acquisition of 2D short-axis slices, resulting in incomplete volumetric\ninformation. Accurate 3D reconstruction from these sparse slices is essential\nfor comprehensive cardiac assessment, but existing methods face challenges,\nincluding reliance on predefined interpolation schemes (e.g., linear or\nspherical), computational inefficiency, and dependence on additional semantic\ninputs such as segmentation labels or motion data. To address these\nlimitations, we propose a novel \\textbf{Ca}rdiac \\textbf{L}atent\n\\textbf{I}nterpolation \\textbf{D}iffusion (CaLID) framework that introduces\nthree key innovations. First, we present a data-driven interpolation scheme\nbased on diffusion models, which can capture complex, non-linear relationships\nbetween sparse slices and improves reconstruction accuracy. Second, we design a\ncomputationally efficient method that operates in the latent space and speeds\nup 3D whole-heart upsampling time by a factor of 24, reducing computational\noverhead compared to previous methods. Third, with only sparse 2D CMR images as\ninput, our method achieves SOTA performance against baseline methods,\neliminating the need for auxiliary input such as morphological guidance, thus\nsimplifying workflows. We further extend our method to 2D+T data, enabling the\neffective modeling of spatiotemporal dynamics and ensuring temporal coherence.\nExtensive volumetric evaluations and downstream segmentation tasks demonstrate\nthat CaLID achieves superior reconstruction quality and efficiency. By\naddressing the fundamental limitations of existing approaches, our framework\nadvances the state of the art for spatio and spatiotemporal whole-heart\nreconstruction, offering a robust and clinically practical solution for\ncardiovascular imaging.\n", "link": "http://arxiv.org/abs/2508.13826v1", "date": "2025-08-19", "relevancy": 2.4015, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6008}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6008}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Interpolation%20Learning%20Using%20Diffusion%20Models%20for%20Cardiac%20Volume%0A%20%20Reconstruction&body=Title%3A%20Latent%20Interpolation%20Learning%20Using%20Diffusion%20Models%20for%20Cardiac%20Volume%0A%20%20Reconstruction%0AAuthor%3A%20Niklas%20Bubeck%20and%20Suprosanna%20Shit%20and%20Chen%20Chen%20and%20Can%20Zhao%20and%20Pengfei%20Guo%20and%20Dong%20Yang%20and%20Georg%20Zitzlsberger%20and%20Daguang%20Xu%20and%20Bernhard%20Kainz%20and%20Daniel%20Rueckert%20and%20Jiazhen%20Pan%0AAbstract%3A%20%20%20Cardiac%20Magnetic%20Resonance%20%28CMR%29%20imaging%20is%20a%20critical%20tool%20for%20diagnosing%0Aand%20managing%20cardiovascular%20disease%2C%20yet%20its%20utility%20is%20often%20limited%20by%20the%0Asparse%20acquisition%20of%202D%20short-axis%20slices%2C%20resulting%20in%20incomplete%20volumetric%0Ainformation.%20Accurate%203D%20reconstruction%20from%20these%20sparse%20slices%20is%20essential%0Afor%20comprehensive%20cardiac%20assessment%2C%20but%20existing%20methods%20face%20challenges%2C%0Aincluding%20reliance%20on%20predefined%20interpolation%20schemes%20%28e.g.%2C%20linear%20or%0Aspherical%29%2C%20computational%20inefficiency%2C%20and%20dependence%20on%20additional%20semantic%0Ainputs%20such%20as%20segmentation%20labels%20or%20motion%20data.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20novel%20%5Ctextbf%7BCa%7Drdiac%20%5Ctextbf%7BL%7Datent%0A%5Ctextbf%7BI%7Dnterpolation%20%5Ctextbf%7BD%7Diffusion%20%28CaLID%29%20framework%20that%20introduces%0Athree%20key%20innovations.%20First%2C%20we%20present%20a%20data-driven%20interpolation%20scheme%0Abased%20on%20diffusion%20models%2C%20which%20can%20capture%20complex%2C%20non-linear%20relationships%0Abetween%20sparse%20slices%20and%20improves%20reconstruction%20accuracy.%20Second%2C%20we%20design%20a%0Acomputationally%20efficient%20method%20that%20operates%20in%20the%20latent%20space%20and%20speeds%0Aup%203D%20whole-heart%20upsampling%20time%20by%20a%20factor%20of%2024%2C%20reducing%20computational%0Aoverhead%20compared%20to%20previous%20methods.%20Third%2C%20with%20only%20sparse%202D%20CMR%20images%20as%0Ainput%2C%20our%20method%20achieves%20SOTA%20performance%20against%20baseline%20methods%2C%0Aeliminating%20the%20need%20for%20auxiliary%20input%20such%20as%20morphological%20guidance%2C%20thus%0Asimplifying%20workflows.%20We%20further%20extend%20our%20method%20to%202D%2BT%20data%2C%20enabling%20the%0Aeffective%20modeling%20of%20spatiotemporal%20dynamics%20and%20ensuring%20temporal%20coherence.%0AExtensive%20volumetric%20evaluations%20and%20downstream%20segmentation%20tasks%20demonstrate%0Athat%20CaLID%20achieves%20superior%20reconstruction%20quality%20and%20efficiency.%20By%0Aaddressing%20the%20fundamental%20limitations%20of%20existing%20approaches%2C%20our%20framework%0Aadvances%20the%20state%20of%20the%20art%20for%20spatio%20and%20spatiotemporal%20whole-heart%0Areconstruction%2C%20offering%20a%20robust%20and%20clinically%20practical%20solution%20for%0Acardiovascular%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13826v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Interpolation%2520Learning%2520Using%2520Diffusion%2520Models%2520for%2520Cardiac%2520Volume%250A%2520%2520Reconstruction%26entry.906535625%3DNiklas%2520Bubeck%2520and%2520Suprosanna%2520Shit%2520and%2520Chen%2520Chen%2520and%2520Can%2520Zhao%2520and%2520Pengfei%2520Guo%2520and%2520Dong%2520Yang%2520and%2520Georg%2520Zitzlsberger%2520and%2520Daguang%2520Xu%2520and%2520Bernhard%2520Kainz%2520and%2520Daniel%2520Rueckert%2520and%2520Jiazhen%2520Pan%26entry.1292438233%3D%2520%2520Cardiac%2520Magnetic%2520Resonance%2520%2528CMR%2529%2520imaging%2520is%2520a%2520critical%2520tool%2520for%2520diagnosing%250Aand%2520managing%2520cardiovascular%2520disease%252C%2520yet%2520its%2520utility%2520is%2520often%2520limited%2520by%2520the%250Asparse%2520acquisition%2520of%25202D%2520short-axis%2520slices%252C%2520resulting%2520in%2520incomplete%2520volumetric%250Ainformation.%2520Accurate%25203D%2520reconstruction%2520from%2520these%2520sparse%2520slices%2520is%2520essential%250Afor%2520comprehensive%2520cardiac%2520assessment%252C%2520but%2520existing%2520methods%2520face%2520challenges%252C%250Aincluding%2520reliance%2520on%2520predefined%2520interpolation%2520schemes%2520%2528e.g.%252C%2520linear%2520or%250Aspherical%2529%252C%2520computational%2520inefficiency%252C%2520and%2520dependence%2520on%2520additional%2520semantic%250Ainputs%2520such%2520as%2520segmentation%2520labels%2520or%2520motion%2520data.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520novel%2520%255Ctextbf%257BCa%257Drdiac%2520%255Ctextbf%257BL%257Datent%250A%255Ctextbf%257BI%257Dnterpolation%2520%255Ctextbf%257BD%257Diffusion%2520%2528CaLID%2529%2520framework%2520that%2520introduces%250Athree%2520key%2520innovations.%2520First%252C%2520we%2520present%2520a%2520data-driven%2520interpolation%2520scheme%250Abased%2520on%2520diffusion%2520models%252C%2520which%2520can%2520capture%2520complex%252C%2520non-linear%2520relationships%250Abetween%2520sparse%2520slices%2520and%2520improves%2520reconstruction%2520accuracy.%2520Second%252C%2520we%2520design%2520a%250Acomputationally%2520efficient%2520method%2520that%2520operates%2520in%2520the%2520latent%2520space%2520and%2520speeds%250Aup%25203D%2520whole-heart%2520upsampling%2520time%2520by%2520a%2520factor%2520of%252024%252C%2520reducing%2520computational%250Aoverhead%2520compared%2520to%2520previous%2520methods.%2520Third%252C%2520with%2520only%2520sparse%25202D%2520CMR%2520images%2520as%250Ainput%252C%2520our%2520method%2520achieves%2520SOTA%2520performance%2520against%2520baseline%2520methods%252C%250Aeliminating%2520the%2520need%2520for%2520auxiliary%2520input%2520such%2520as%2520morphological%2520guidance%252C%2520thus%250Asimplifying%2520workflows.%2520We%2520further%2520extend%2520our%2520method%2520to%25202D%252BT%2520data%252C%2520enabling%2520the%250Aeffective%2520modeling%2520of%2520spatiotemporal%2520dynamics%2520and%2520ensuring%2520temporal%2520coherence.%250AExtensive%2520volumetric%2520evaluations%2520and%2520downstream%2520segmentation%2520tasks%2520demonstrate%250Athat%2520CaLID%2520achieves%2520superior%2520reconstruction%2520quality%2520and%2520efficiency.%2520By%250Aaddressing%2520the%2520fundamental%2520limitations%2520of%2520existing%2520approaches%252C%2520our%2520framework%250Aadvances%2520the%2520state%2520of%2520the%2520art%2520for%2520spatio%2520and%2520spatiotemporal%2520whole-heart%250Areconstruction%252C%2520offering%2520a%2520robust%2520and%2520clinically%2520practical%2520solution%2520for%250Acardiovascular%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13826v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Interpolation%20Learning%20Using%20Diffusion%20Models%20for%20Cardiac%20Volume%0A%20%20Reconstruction&entry.906535625=Niklas%20Bubeck%20and%20Suprosanna%20Shit%20and%20Chen%20Chen%20and%20Can%20Zhao%20and%20Pengfei%20Guo%20and%20Dong%20Yang%20and%20Georg%20Zitzlsberger%20and%20Daguang%20Xu%20and%20Bernhard%20Kainz%20and%20Daniel%20Rueckert%20and%20Jiazhen%20Pan&entry.1292438233=%20%20Cardiac%20Magnetic%20Resonance%20%28CMR%29%20imaging%20is%20a%20critical%20tool%20for%20diagnosing%0Aand%20managing%20cardiovascular%20disease%2C%20yet%20its%20utility%20is%20often%20limited%20by%20the%0Asparse%20acquisition%20of%202D%20short-axis%20slices%2C%20resulting%20in%20incomplete%20volumetric%0Ainformation.%20Accurate%203D%20reconstruction%20from%20these%20sparse%20slices%20is%20essential%0Afor%20comprehensive%20cardiac%20assessment%2C%20but%20existing%20methods%20face%20challenges%2C%0Aincluding%20reliance%20on%20predefined%20interpolation%20schemes%20%28e.g.%2C%20linear%20or%0Aspherical%29%2C%20computational%20inefficiency%2C%20and%20dependence%20on%20additional%20semantic%0Ainputs%20such%20as%20segmentation%20labels%20or%20motion%20data.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20novel%20%5Ctextbf%7BCa%7Drdiac%20%5Ctextbf%7BL%7Datent%0A%5Ctextbf%7BI%7Dnterpolation%20%5Ctextbf%7BD%7Diffusion%20%28CaLID%29%20framework%20that%20introduces%0Athree%20key%20innovations.%20First%2C%20we%20present%20a%20data-driven%20interpolation%20scheme%0Abased%20on%20diffusion%20models%2C%20which%20can%20capture%20complex%2C%20non-linear%20relationships%0Abetween%20sparse%20slices%20and%20improves%20reconstruction%20accuracy.%20Second%2C%20we%20design%20a%0Acomputationally%20efficient%20method%20that%20operates%20in%20the%20latent%20space%20and%20speeds%0Aup%203D%20whole-heart%20upsampling%20time%20by%20a%20factor%20of%2024%2C%20reducing%20computational%0Aoverhead%20compared%20to%20previous%20methods.%20Third%2C%20with%20only%20sparse%202D%20CMR%20images%20as%0Ainput%2C%20our%20method%20achieves%20SOTA%20performance%20against%20baseline%20methods%2C%0Aeliminating%20the%20need%20for%20auxiliary%20input%20such%20as%20morphological%20guidance%2C%20thus%0Asimplifying%20workflows.%20We%20further%20extend%20our%20method%20to%202D%2BT%20data%2C%20enabling%20the%0Aeffective%20modeling%20of%20spatiotemporal%20dynamics%20and%20ensuring%20temporal%20coherence.%0AExtensive%20volumetric%20evaluations%20and%20downstream%20segmentation%20tasks%20demonstrate%0Athat%20CaLID%20achieves%20superior%20reconstruction%20quality%20and%20efficiency.%20By%0Aaddressing%20the%20fundamental%20limitations%20of%20existing%20approaches%2C%20our%20framework%0Aadvances%20the%20state%20of%20the%20art%20for%20spatio%20and%20spatiotemporal%20whole-heart%0Areconstruction%2C%20offering%20a%20robust%20and%20clinically%20practical%20solution%20for%0Acardiovascular%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13826v1&entry.124074799=Read"},
{"title": "BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification\n  with Swin-HAFNet", "author": "Amirreza Fateh and Yasin Rezvani and Sara Moayedi and Sadjad Rezvani and Fatemeh Fateh and Mansoor Fateh", "abstract": "  Accurate segmentation and classification of brain tumors from Magnetic\nResonance Imaging (MRI) remain key challenges in medical image analysis. This\nis primarily due to the lack of high-quality, balanced, and diverse datasets.\nIn this work, we present a newly developed MRI dataset named BRISC designed\nspecifically for brain tumor segmentation and classification tasks. The dataset\ncomprises 6,000 contrast-enhanced T1-weighted MRI scans annotated by certified\nradiologists and physicians. It includes three major tumor types, namely\nglioma, meningioma, and pituitary, as well as non-tumorous cases. Each sample\nincludes high-resolution labels and is categorized across axial, sagittal, and\ncoronal imaging planes to facilitate robust model development and cross-view\ngeneralization. To demonstrate the utility of the dataset, we propose a\ntransformer-based segmentation model and benchmark it against established\nbaselines. In this work, we propose a transformer-based model designed for both\nsegmentation and classification of brain tumors, leveraging multi-scale feature\nrepresentations from a Swin Transformer backbone. The model is benchmarked\nagainst established baselines to demonstrate the utility of the dataset,\nenabling accurate segmentation and robust classification across four diagnostic\ncategories: glioma, meningioma, pituitary, and non-tumorous cases. In this\nwork, our proposed transformer-based model demonstrates superior performance in\nboth segmentation and classification tasks for brain tumor analysis. For the\nsegmentation task, the method achieves the highest weighted mean\nIntersection-over-Union (IoU) of 82.3\\%, with improvements observed across all\ntumor categories. For the classification task, the model attains an accuracy of\n99.63\\%, effectively distinguishing between glioma, meningioma, pituitary, and\nnon-tumorous cases. https://www.kaggle.com/datasets/briscdataset/brisc2025/\n", "link": "http://arxiv.org/abs/2506.14318v2", "date": "2025-08-19", "relevancy": 2.3871, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5113}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4728}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BRISC%3A%20Annotated%20Dataset%20for%20Brain%20Tumor%20Segmentation%20and%20Classification%0A%20%20with%20Swin-HAFNet&body=Title%3A%20BRISC%3A%20Annotated%20Dataset%20for%20Brain%20Tumor%20Segmentation%20and%20Classification%0A%20%20with%20Swin-HAFNet%0AAuthor%3A%20Amirreza%20Fateh%20and%20Yasin%20Rezvani%20and%20Sara%20Moayedi%20and%20Sadjad%20Rezvani%20and%20Fatemeh%20Fateh%20and%20Mansoor%20Fateh%0AAbstract%3A%20%20%20Accurate%20segmentation%20and%20classification%20of%20brain%20tumors%20from%20Magnetic%0AResonance%20Imaging%20%28MRI%29%20remain%20key%20challenges%20in%20medical%20image%20analysis.%20This%0Ais%20primarily%20due%20to%20the%20lack%20of%20high-quality%2C%20balanced%2C%20and%20diverse%20datasets.%0AIn%20this%20work%2C%20we%20present%20a%20newly%20developed%20MRI%20dataset%20named%20BRISC%20designed%0Aspecifically%20for%20brain%20tumor%20segmentation%20and%20classification%20tasks.%20The%20dataset%0Acomprises%206%2C000%20contrast-enhanced%20T1-weighted%20MRI%20scans%20annotated%20by%20certified%0Aradiologists%20and%20physicians.%20It%20includes%20three%20major%20tumor%20types%2C%20namely%0Aglioma%2C%20meningioma%2C%20and%20pituitary%2C%20as%20well%20as%20non-tumorous%20cases.%20Each%20sample%0Aincludes%20high-resolution%20labels%20and%20is%20categorized%20across%20axial%2C%20sagittal%2C%20and%0Acoronal%20imaging%20planes%20to%20facilitate%20robust%20model%20development%20and%20cross-view%0Ageneralization.%20To%20demonstrate%20the%20utility%20of%20the%20dataset%2C%20we%20propose%20a%0Atransformer-based%20segmentation%20model%20and%20benchmark%20it%20against%20established%0Abaselines.%20In%20this%20work%2C%20we%20propose%20a%20transformer-based%20model%20designed%20for%20both%0Asegmentation%20and%20classification%20of%20brain%20tumors%2C%20leveraging%20multi-scale%20feature%0Arepresentations%20from%20a%20Swin%20Transformer%20backbone.%20The%20model%20is%20benchmarked%0Aagainst%20established%20baselines%20to%20demonstrate%20the%20utility%20of%20the%20dataset%2C%0Aenabling%20accurate%20segmentation%20and%20robust%20classification%20across%20four%20diagnostic%0Acategories%3A%20glioma%2C%20meningioma%2C%20pituitary%2C%20and%20non-tumorous%20cases.%20In%20this%0Awork%2C%20our%20proposed%20transformer-based%20model%20demonstrates%20superior%20performance%20in%0Aboth%20segmentation%20and%20classification%20tasks%20for%20brain%20tumor%20analysis.%20For%20the%0Asegmentation%20task%2C%20the%20method%20achieves%20the%20highest%20weighted%20mean%0AIntersection-over-Union%20%28IoU%29%20of%2082.3%5C%25%2C%20with%20improvements%20observed%20across%20all%0Atumor%20categories.%20For%20the%20classification%20task%2C%20the%20model%20attains%20an%20accuracy%20of%0A99.63%5C%25%2C%20effectively%20distinguishing%20between%20glioma%2C%20meningioma%2C%20pituitary%2C%20and%0Anon-tumorous%20cases.%20https%3A//www.kaggle.com/datasets/briscdataset/brisc2025/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.14318v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBRISC%253A%2520Annotated%2520Dataset%2520for%2520Brain%2520Tumor%2520Segmentation%2520and%2520Classification%250A%2520%2520with%2520Swin-HAFNet%26entry.906535625%3DAmirreza%2520Fateh%2520and%2520Yasin%2520Rezvani%2520and%2520Sara%2520Moayedi%2520and%2520Sadjad%2520Rezvani%2520and%2520Fatemeh%2520Fateh%2520and%2520Mansoor%2520Fateh%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520and%2520classification%2520of%2520brain%2520tumors%2520from%2520Magnetic%250AResonance%2520Imaging%2520%2528MRI%2529%2520remain%2520key%2520challenges%2520in%2520medical%2520image%2520analysis.%2520This%250Ais%2520primarily%2520due%2520to%2520the%2520lack%2520of%2520high-quality%252C%2520balanced%252C%2520and%2520diverse%2520datasets.%250AIn%2520this%2520work%252C%2520we%2520present%2520a%2520newly%2520developed%2520MRI%2520dataset%2520named%2520BRISC%2520designed%250Aspecifically%2520for%2520brain%2520tumor%2520segmentation%2520and%2520classification%2520tasks.%2520The%2520dataset%250Acomprises%25206%252C000%2520contrast-enhanced%2520T1-weighted%2520MRI%2520scans%2520annotated%2520by%2520certified%250Aradiologists%2520and%2520physicians.%2520It%2520includes%2520three%2520major%2520tumor%2520types%252C%2520namely%250Aglioma%252C%2520meningioma%252C%2520and%2520pituitary%252C%2520as%2520well%2520as%2520non-tumorous%2520cases.%2520Each%2520sample%250Aincludes%2520high-resolution%2520labels%2520and%2520is%2520categorized%2520across%2520axial%252C%2520sagittal%252C%2520and%250Acoronal%2520imaging%2520planes%2520to%2520facilitate%2520robust%2520model%2520development%2520and%2520cross-view%250Ageneralization.%2520To%2520demonstrate%2520the%2520utility%2520of%2520the%2520dataset%252C%2520we%2520propose%2520a%250Atransformer-based%2520segmentation%2520model%2520and%2520benchmark%2520it%2520against%2520established%250Abaselines.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520transformer-based%2520model%2520designed%2520for%2520both%250Asegmentation%2520and%2520classification%2520of%2520brain%2520tumors%252C%2520leveraging%2520multi-scale%2520feature%250Arepresentations%2520from%2520a%2520Swin%2520Transformer%2520backbone.%2520The%2520model%2520is%2520benchmarked%250Aagainst%2520established%2520baselines%2520to%2520demonstrate%2520the%2520utility%2520of%2520the%2520dataset%252C%250Aenabling%2520accurate%2520segmentation%2520and%2520robust%2520classification%2520across%2520four%2520diagnostic%250Acategories%253A%2520glioma%252C%2520meningioma%252C%2520pituitary%252C%2520and%2520non-tumorous%2520cases.%2520In%2520this%250Awork%252C%2520our%2520proposed%2520transformer-based%2520model%2520demonstrates%2520superior%2520performance%2520in%250Aboth%2520segmentation%2520and%2520classification%2520tasks%2520for%2520brain%2520tumor%2520analysis.%2520For%2520the%250Asegmentation%2520task%252C%2520the%2520method%2520achieves%2520the%2520highest%2520weighted%2520mean%250AIntersection-over-Union%2520%2528IoU%2529%2520of%252082.3%255C%2525%252C%2520with%2520improvements%2520observed%2520across%2520all%250Atumor%2520categories.%2520For%2520the%2520classification%2520task%252C%2520the%2520model%2520attains%2520an%2520accuracy%2520of%250A99.63%255C%2525%252C%2520effectively%2520distinguishing%2520between%2520glioma%252C%2520meningioma%252C%2520pituitary%252C%2520and%250Anon-tumorous%2520cases.%2520https%253A//www.kaggle.com/datasets/briscdataset/brisc2025/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14318v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRISC%3A%20Annotated%20Dataset%20for%20Brain%20Tumor%20Segmentation%20and%20Classification%0A%20%20with%20Swin-HAFNet&entry.906535625=Amirreza%20Fateh%20and%20Yasin%20Rezvani%20and%20Sara%20Moayedi%20and%20Sadjad%20Rezvani%20and%20Fatemeh%20Fateh%20and%20Mansoor%20Fateh&entry.1292438233=%20%20Accurate%20segmentation%20and%20classification%20of%20brain%20tumors%20from%20Magnetic%0AResonance%20Imaging%20%28MRI%29%20remain%20key%20challenges%20in%20medical%20image%20analysis.%20This%0Ais%20primarily%20due%20to%20the%20lack%20of%20high-quality%2C%20balanced%2C%20and%20diverse%20datasets.%0AIn%20this%20work%2C%20we%20present%20a%20newly%20developed%20MRI%20dataset%20named%20BRISC%20designed%0Aspecifically%20for%20brain%20tumor%20segmentation%20and%20classification%20tasks.%20The%20dataset%0Acomprises%206%2C000%20contrast-enhanced%20T1-weighted%20MRI%20scans%20annotated%20by%20certified%0Aradiologists%20and%20physicians.%20It%20includes%20three%20major%20tumor%20types%2C%20namely%0Aglioma%2C%20meningioma%2C%20and%20pituitary%2C%20as%20well%20as%20non-tumorous%20cases.%20Each%20sample%0Aincludes%20high-resolution%20labels%20and%20is%20categorized%20across%20axial%2C%20sagittal%2C%20and%0Acoronal%20imaging%20planes%20to%20facilitate%20robust%20model%20development%20and%20cross-view%0Ageneralization.%20To%20demonstrate%20the%20utility%20of%20the%20dataset%2C%20we%20propose%20a%0Atransformer-based%20segmentation%20model%20and%20benchmark%20it%20against%20established%0Abaselines.%20In%20this%20work%2C%20we%20propose%20a%20transformer-based%20model%20designed%20for%20both%0Asegmentation%20and%20classification%20of%20brain%20tumors%2C%20leveraging%20multi-scale%20feature%0Arepresentations%20from%20a%20Swin%20Transformer%20backbone.%20The%20model%20is%20benchmarked%0Aagainst%20established%20baselines%20to%20demonstrate%20the%20utility%20of%20the%20dataset%2C%0Aenabling%20accurate%20segmentation%20and%20robust%20classification%20across%20four%20diagnostic%0Acategories%3A%20glioma%2C%20meningioma%2C%20pituitary%2C%20and%20non-tumorous%20cases.%20In%20this%0Awork%2C%20our%20proposed%20transformer-based%20model%20demonstrates%20superior%20performance%20in%0Aboth%20segmentation%20and%20classification%20tasks%20for%20brain%20tumor%20analysis.%20For%20the%0Asegmentation%20task%2C%20the%20method%20achieves%20the%20highest%20weighted%20mean%0AIntersection-over-Union%20%28IoU%29%20of%2082.3%5C%25%2C%20with%20improvements%20observed%20across%20all%0Atumor%20categories.%20For%20the%20classification%20task%2C%20the%20model%20attains%20an%20accuracy%20of%0A99.63%5C%25%2C%20effectively%20distinguishing%20between%20glioma%2C%20meningioma%2C%20pituitary%2C%20and%0Anon-tumorous%20cases.%20https%3A//www.kaggle.com/datasets/briscdataset/brisc2025/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.14318v2&entry.124074799=Read"},
{"title": "Self-Supervised Sparse Sensor Fusion for Long Range Perception", "author": "Edoardo Palladin and Samuel Brucker and Filippo Ghilotti and Praveen Narayanan and Mario Bijelic and Felix Heide", "abstract": "  Outside of urban hubs, autonomous cars and trucks have to master driving on\nintercity highways. Safe, long-distance highway travel at speeds exceeding 100\nkm/h demands perception distances of at least 250 m, which is about five times\nthe 50-100m typically addressed in city driving, to allow sufficient planning\nand braking margins. Increasing the perception ranges also allows to extend\nautonomy from light two-ton passenger vehicles to large-scale forty-ton trucks,\nwhich need a longer planning horizon due to their high inertia. However, most\nexisting perception approaches focus on shorter ranges and rely on Bird's Eye\nView (BEV) representations, which incur quadratic increases in memory and\ncompute costs as distance grows. To overcome this limitation, we built on top\nof a sparse representation and introduced an efficient 3D encoding of\nmulti-modal and temporal features, along with a novel self-supervised\npre-training scheme that enables large-scale learning from unlabeled\ncamera-LiDAR data. Our approach extends perception distances to 250 meters and\nachieves an 26.6% improvement in mAP in object detection and a decrease of\n30.5% in Chamfer Distance in LiDAR forecasting compared to existing methods,\nreaching distances up to 250 meters. Project Page:\nhttps://light.princeton.edu/lrs4fusion/\n", "link": "http://arxiv.org/abs/2508.13995v1", "date": "2025-08-19", "relevancy": 2.3632, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6226}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5759}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Sparse%20Sensor%20Fusion%20for%20Long%20Range%20Perception&body=Title%3A%20Self-Supervised%20Sparse%20Sensor%20Fusion%20for%20Long%20Range%20Perception%0AAuthor%3A%20Edoardo%20Palladin%20and%20Samuel%20Brucker%20and%20Filippo%20Ghilotti%20and%20Praveen%20Narayanan%20and%20Mario%20Bijelic%20and%20Felix%20Heide%0AAbstract%3A%20%20%20Outside%20of%20urban%20hubs%2C%20autonomous%20cars%20and%20trucks%20have%20to%20master%20driving%20on%0Aintercity%20highways.%20Safe%2C%20long-distance%20highway%20travel%20at%20speeds%20exceeding%20100%0Akm/h%20demands%20perception%20distances%20of%20at%20least%20250%20m%2C%20which%20is%20about%20five%20times%0Athe%2050-100m%20typically%20addressed%20in%20city%20driving%2C%20to%20allow%20sufficient%20planning%0Aand%20braking%20margins.%20Increasing%20the%20perception%20ranges%20also%20allows%20to%20extend%0Aautonomy%20from%20light%20two-ton%20passenger%20vehicles%20to%20large-scale%20forty-ton%20trucks%2C%0Awhich%20need%20a%20longer%20planning%20horizon%20due%20to%20their%20high%20inertia.%20However%2C%20most%0Aexisting%20perception%20approaches%20focus%20on%20shorter%20ranges%20and%20rely%20on%20Bird%27s%20Eye%0AView%20%28BEV%29%20representations%2C%20which%20incur%20quadratic%20increases%20in%20memory%20and%0Acompute%20costs%20as%20distance%20grows.%20To%20overcome%20this%20limitation%2C%20we%20built%20on%20top%0Aof%20a%20sparse%20representation%20and%20introduced%20an%20efficient%203D%20encoding%20of%0Amulti-modal%20and%20temporal%20features%2C%20along%20with%20a%20novel%20self-supervised%0Apre-training%20scheme%20that%20enables%20large-scale%20learning%20from%20unlabeled%0Acamera-LiDAR%20data.%20Our%20approach%20extends%20perception%20distances%20to%20250%20meters%20and%0Aachieves%20an%2026.6%25%20improvement%20in%20mAP%20in%20object%20detection%20and%20a%20decrease%20of%0A30.5%25%20in%20Chamfer%20Distance%20in%20LiDAR%20forecasting%20compared%20to%20existing%20methods%2C%0Areaching%20distances%20up%20to%20250%20meters.%20Project%20Page%3A%0Ahttps%3A//light.princeton.edu/lrs4fusion/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Sparse%2520Sensor%2520Fusion%2520for%2520Long%2520Range%2520Perception%26entry.906535625%3DEdoardo%2520Palladin%2520and%2520Samuel%2520Brucker%2520and%2520Filippo%2520Ghilotti%2520and%2520Praveen%2520Narayanan%2520and%2520Mario%2520Bijelic%2520and%2520Felix%2520Heide%26entry.1292438233%3D%2520%2520Outside%2520of%2520urban%2520hubs%252C%2520autonomous%2520cars%2520and%2520trucks%2520have%2520to%2520master%2520driving%2520on%250Aintercity%2520highways.%2520Safe%252C%2520long-distance%2520highway%2520travel%2520at%2520speeds%2520exceeding%2520100%250Akm/h%2520demands%2520perception%2520distances%2520of%2520at%2520least%2520250%2520m%252C%2520which%2520is%2520about%2520five%2520times%250Athe%252050-100m%2520typically%2520addressed%2520in%2520city%2520driving%252C%2520to%2520allow%2520sufficient%2520planning%250Aand%2520braking%2520margins.%2520Increasing%2520the%2520perception%2520ranges%2520also%2520allows%2520to%2520extend%250Aautonomy%2520from%2520light%2520two-ton%2520passenger%2520vehicles%2520to%2520large-scale%2520forty-ton%2520trucks%252C%250Awhich%2520need%2520a%2520longer%2520planning%2520horizon%2520due%2520to%2520their%2520high%2520inertia.%2520However%252C%2520most%250Aexisting%2520perception%2520approaches%2520focus%2520on%2520shorter%2520ranges%2520and%2520rely%2520on%2520Bird%2527s%2520Eye%250AView%2520%2528BEV%2529%2520representations%252C%2520which%2520incur%2520quadratic%2520increases%2520in%2520memory%2520and%250Acompute%2520costs%2520as%2520distance%2520grows.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520built%2520on%2520top%250Aof%2520a%2520sparse%2520representation%2520and%2520introduced%2520an%2520efficient%25203D%2520encoding%2520of%250Amulti-modal%2520and%2520temporal%2520features%252C%2520along%2520with%2520a%2520novel%2520self-supervised%250Apre-training%2520scheme%2520that%2520enables%2520large-scale%2520learning%2520from%2520unlabeled%250Acamera-LiDAR%2520data.%2520Our%2520approach%2520extends%2520perception%2520distances%2520to%2520250%2520meters%2520and%250Aachieves%2520an%252026.6%2525%2520improvement%2520in%2520mAP%2520in%2520object%2520detection%2520and%2520a%2520decrease%2520of%250A30.5%2525%2520in%2520Chamfer%2520Distance%2520in%2520LiDAR%2520forecasting%2520compared%2520to%2520existing%2520methods%252C%250Areaching%2520distances%2520up%2520to%2520250%2520meters.%2520Project%2520Page%253A%250Ahttps%253A//light.princeton.edu/lrs4fusion/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Sparse%20Sensor%20Fusion%20for%20Long%20Range%20Perception&entry.906535625=Edoardo%20Palladin%20and%20Samuel%20Brucker%20and%20Filippo%20Ghilotti%20and%20Praveen%20Narayanan%20and%20Mario%20Bijelic%20and%20Felix%20Heide&entry.1292438233=%20%20Outside%20of%20urban%20hubs%2C%20autonomous%20cars%20and%20trucks%20have%20to%20master%20driving%20on%0Aintercity%20highways.%20Safe%2C%20long-distance%20highway%20travel%20at%20speeds%20exceeding%20100%0Akm/h%20demands%20perception%20distances%20of%20at%20least%20250%20m%2C%20which%20is%20about%20five%20times%0Athe%2050-100m%20typically%20addressed%20in%20city%20driving%2C%20to%20allow%20sufficient%20planning%0Aand%20braking%20margins.%20Increasing%20the%20perception%20ranges%20also%20allows%20to%20extend%0Aautonomy%20from%20light%20two-ton%20passenger%20vehicles%20to%20large-scale%20forty-ton%20trucks%2C%0Awhich%20need%20a%20longer%20planning%20horizon%20due%20to%20their%20high%20inertia.%20However%2C%20most%0Aexisting%20perception%20approaches%20focus%20on%20shorter%20ranges%20and%20rely%20on%20Bird%27s%20Eye%0AView%20%28BEV%29%20representations%2C%20which%20incur%20quadratic%20increases%20in%20memory%20and%0Acompute%20costs%20as%20distance%20grows.%20To%20overcome%20this%20limitation%2C%20we%20built%20on%20top%0Aof%20a%20sparse%20representation%20and%20introduced%20an%20efficient%203D%20encoding%20of%0Amulti-modal%20and%20temporal%20features%2C%20along%20with%20a%20novel%20self-supervised%0Apre-training%20scheme%20that%20enables%20large-scale%20learning%20from%20unlabeled%0Acamera-LiDAR%20data.%20Our%20approach%20extends%20perception%20distances%20to%20250%20meters%20and%0Aachieves%20an%2026.6%25%20improvement%20in%20mAP%20in%20object%20detection%20and%20a%20decrease%20of%0A30.5%25%20in%20Chamfer%20Distance%20in%20LiDAR%20forecasting%20compared%20to%20existing%20methods%2C%0Areaching%20distances%20up%20to%20250%20meters.%20Project%20Page%3A%0Ahttps%3A//light.princeton.edu/lrs4fusion/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13995v1&entry.124074799=Read"},
{"title": "VisionLaw: Inferring Interpretable Intrinsic Dynamics from Visual\n  Observations via Bilevel Optimization", "author": "Jiajing Lin and Shu Jiang and Qingyuan Zeng and Zhenzhong Wang and Min Jiang", "abstract": "  The intrinsic dynamics of an object governs its physical behavior in the real\nworld, playing a critical role in enabling physically plausible interactive\nsimulation with 3D assets. Existing methods have attempted to infer the\nintrinsic dynamics of objects from visual observations, but generally face two\nmajor challenges: one line of work relies on manually defined constitutive\npriors, making it difficult to generalize to complex scenarios; the other\nmodels intrinsic dynamics using neural networks, resulting in limited\ninterpretability and poor generalization. To address these challenges, we\npropose VisionLaw, a bilevel optimization framework that infers interpretable\nexpressions of intrinsic dynamics from visual observations. At the upper level,\nwe introduce an LLMs-driven decoupled constitutive evolution strategy, where\nLLMs are prompted as a knowledgeable physics expert to generate and revise\nconstitutive laws, with a built-in decoupling mechanism that substantially\nreduces the search complexity of LLMs. At the lower level, we introduce a\nvision-guided constitutive evaluation mechanism, which utilizes visual\nsimulation to evaluate the consistency between the generated constitutive law\nand the underlying intrinsic dynamics, thereby guiding the upper-level\nevolution. Experiments on both synthetic and real-world datasets demonstrate\nthat VisionLaw can effectively infer interpretable intrinsic dynamics from\nvisual observations. It significantly outperforms existing state-of-the-art\nmethods and exhibits strong generalization for interactive simulation in novel\nscenarios.\n", "link": "http://arxiv.org/abs/2508.13792v1", "date": "2025-08-19", "relevancy": 2.3292, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5967}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisionLaw%3A%20Inferring%20Interpretable%20Intrinsic%20Dynamics%20from%20Visual%0A%20%20Observations%20via%20Bilevel%20Optimization&body=Title%3A%20VisionLaw%3A%20Inferring%20Interpretable%20Intrinsic%20Dynamics%20from%20Visual%0A%20%20Observations%20via%20Bilevel%20Optimization%0AAuthor%3A%20Jiajing%20Lin%20and%20Shu%20Jiang%20and%20Qingyuan%20Zeng%20and%20Zhenzhong%20Wang%20and%20Min%20Jiang%0AAbstract%3A%20%20%20The%20intrinsic%20dynamics%20of%20an%20object%20governs%20its%20physical%20behavior%20in%20the%20real%0Aworld%2C%20playing%20a%20critical%20role%20in%20enabling%20physically%20plausible%20interactive%0Asimulation%20with%203D%20assets.%20Existing%20methods%20have%20attempted%20to%20infer%20the%0Aintrinsic%20dynamics%20of%20objects%20from%20visual%20observations%2C%20but%20generally%20face%20two%0Amajor%20challenges%3A%20one%20line%20of%20work%20relies%20on%20manually%20defined%20constitutive%0Apriors%2C%20making%20it%20difficult%20to%20generalize%20to%20complex%20scenarios%3B%20the%20other%0Amodels%20intrinsic%20dynamics%20using%20neural%20networks%2C%20resulting%20in%20limited%0Ainterpretability%20and%20poor%20generalization.%20To%20address%20these%20challenges%2C%20we%0Apropose%20VisionLaw%2C%20a%20bilevel%20optimization%20framework%20that%20infers%20interpretable%0Aexpressions%20of%20intrinsic%20dynamics%20from%20visual%20observations.%20At%20the%20upper%20level%2C%0Awe%20introduce%20an%20LLMs-driven%20decoupled%20constitutive%20evolution%20strategy%2C%20where%0ALLMs%20are%20prompted%20as%20a%20knowledgeable%20physics%20expert%20to%20generate%20and%20revise%0Aconstitutive%20laws%2C%20with%20a%20built-in%20decoupling%20mechanism%20that%20substantially%0Areduces%20the%20search%20complexity%20of%20LLMs.%20At%20the%20lower%20level%2C%20we%20introduce%20a%0Avision-guided%20constitutive%20evaluation%20mechanism%2C%20which%20utilizes%20visual%0Asimulation%20to%20evaluate%20the%20consistency%20between%20the%20generated%20constitutive%20law%0Aand%20the%20underlying%20intrinsic%20dynamics%2C%20thereby%20guiding%20the%20upper-level%0Aevolution.%20Experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%0Athat%20VisionLaw%20can%20effectively%20infer%20interpretable%20intrinsic%20dynamics%20from%0Avisual%20observations.%20It%20significantly%20outperforms%20existing%20state-of-the-art%0Amethods%20and%20exhibits%20strong%20generalization%20for%20interactive%20simulation%20in%20novel%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisionLaw%253A%2520Inferring%2520Interpretable%2520Intrinsic%2520Dynamics%2520from%2520Visual%250A%2520%2520Observations%2520via%2520Bilevel%2520Optimization%26entry.906535625%3DJiajing%2520Lin%2520and%2520Shu%2520Jiang%2520and%2520Qingyuan%2520Zeng%2520and%2520Zhenzhong%2520Wang%2520and%2520Min%2520Jiang%26entry.1292438233%3D%2520%2520The%2520intrinsic%2520dynamics%2520of%2520an%2520object%2520governs%2520its%2520physical%2520behavior%2520in%2520the%2520real%250Aworld%252C%2520playing%2520a%2520critical%2520role%2520in%2520enabling%2520physically%2520plausible%2520interactive%250Asimulation%2520with%25203D%2520assets.%2520Existing%2520methods%2520have%2520attempted%2520to%2520infer%2520the%250Aintrinsic%2520dynamics%2520of%2520objects%2520from%2520visual%2520observations%252C%2520but%2520generally%2520face%2520two%250Amajor%2520challenges%253A%2520one%2520line%2520of%2520work%2520relies%2520on%2520manually%2520defined%2520constitutive%250Apriors%252C%2520making%2520it%2520difficult%2520to%2520generalize%2520to%2520complex%2520scenarios%253B%2520the%2520other%250Amodels%2520intrinsic%2520dynamics%2520using%2520neural%2520networks%252C%2520resulting%2520in%2520limited%250Ainterpretability%2520and%2520poor%2520generalization.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520VisionLaw%252C%2520a%2520bilevel%2520optimization%2520framework%2520that%2520infers%2520interpretable%250Aexpressions%2520of%2520intrinsic%2520dynamics%2520from%2520visual%2520observations.%2520At%2520the%2520upper%2520level%252C%250Awe%2520introduce%2520an%2520LLMs-driven%2520decoupled%2520constitutive%2520evolution%2520strategy%252C%2520where%250ALLMs%2520are%2520prompted%2520as%2520a%2520knowledgeable%2520physics%2520expert%2520to%2520generate%2520and%2520revise%250Aconstitutive%2520laws%252C%2520with%2520a%2520built-in%2520decoupling%2520mechanism%2520that%2520substantially%250Areduces%2520the%2520search%2520complexity%2520of%2520LLMs.%2520At%2520the%2520lower%2520level%252C%2520we%2520introduce%2520a%250Avision-guided%2520constitutive%2520evaluation%2520mechanism%252C%2520which%2520utilizes%2520visual%250Asimulation%2520to%2520evaluate%2520the%2520consistency%2520between%2520the%2520generated%2520constitutive%2520law%250Aand%2520the%2520underlying%2520intrinsic%2520dynamics%252C%2520thereby%2520guiding%2520the%2520upper-level%250Aevolution.%2520Experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%250Athat%2520VisionLaw%2520can%2520effectively%2520infer%2520interpretable%2520intrinsic%2520dynamics%2520from%250Avisual%2520observations.%2520It%2520significantly%2520outperforms%2520existing%2520state-of-the-art%250Amethods%2520and%2520exhibits%2520strong%2520generalization%2520for%2520interactive%2520simulation%2520in%2520novel%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisionLaw%3A%20Inferring%20Interpretable%20Intrinsic%20Dynamics%20from%20Visual%0A%20%20Observations%20via%20Bilevel%20Optimization&entry.906535625=Jiajing%20Lin%20and%20Shu%20Jiang%20and%20Qingyuan%20Zeng%20and%20Zhenzhong%20Wang%20and%20Min%20Jiang&entry.1292438233=%20%20The%20intrinsic%20dynamics%20of%20an%20object%20governs%20its%20physical%20behavior%20in%20the%20real%0Aworld%2C%20playing%20a%20critical%20role%20in%20enabling%20physically%20plausible%20interactive%0Asimulation%20with%203D%20assets.%20Existing%20methods%20have%20attempted%20to%20infer%20the%0Aintrinsic%20dynamics%20of%20objects%20from%20visual%20observations%2C%20but%20generally%20face%20two%0Amajor%20challenges%3A%20one%20line%20of%20work%20relies%20on%20manually%20defined%20constitutive%0Apriors%2C%20making%20it%20difficult%20to%20generalize%20to%20complex%20scenarios%3B%20the%20other%0Amodels%20intrinsic%20dynamics%20using%20neural%20networks%2C%20resulting%20in%20limited%0Ainterpretability%20and%20poor%20generalization.%20To%20address%20these%20challenges%2C%20we%0Apropose%20VisionLaw%2C%20a%20bilevel%20optimization%20framework%20that%20infers%20interpretable%0Aexpressions%20of%20intrinsic%20dynamics%20from%20visual%20observations.%20At%20the%20upper%20level%2C%0Awe%20introduce%20an%20LLMs-driven%20decoupled%20constitutive%20evolution%20strategy%2C%20where%0ALLMs%20are%20prompted%20as%20a%20knowledgeable%20physics%20expert%20to%20generate%20and%20revise%0Aconstitutive%20laws%2C%20with%20a%20built-in%20decoupling%20mechanism%20that%20substantially%0Areduces%20the%20search%20complexity%20of%20LLMs.%20At%20the%20lower%20level%2C%20we%20introduce%20a%0Avision-guided%20constitutive%20evaluation%20mechanism%2C%20which%20utilizes%20visual%0Asimulation%20to%20evaluate%20the%20consistency%20between%20the%20generated%20constitutive%20law%0Aand%20the%20underlying%20intrinsic%20dynamics%2C%20thereby%20guiding%20the%20upper-level%0Aevolution.%20Experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%0Athat%20VisionLaw%20can%20effectively%20infer%20interpretable%20intrinsic%20dynamics%20from%0Avisual%20observations.%20It%20significantly%20outperforms%20existing%20state-of-the-art%0Amethods%20and%20exhibits%20strong%20generalization%20for%20interactive%20simulation%20in%20novel%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13792v1&entry.124074799=Read"},
{"title": "Is-NeRF: In-scattering Neural Radiance Field for Blurred Images", "author": "Nan Luo and Chenglin Ye and Jiaxu Li and Gang Liu and Bo Wan and Di Wang and Lupeng Liu and Jun Xiao", "abstract": "  Neural Radiance Fields (NeRF) has gained significant attention for its\nprominent implicit 3D representation and realistic novel view synthesis\ncapabilities. Available works unexceptionally employ straight-line volume\nrendering, which struggles to handle sophisticated lightpath scenarios and\nintroduces geometric ambiguities during training, particularly evident when\nprocessing motion-blurred images. To address these challenges, this work\nproposes a novel deblur neural radiance field, Is-NeRF, featuring explicit\nlightpath modeling in real-world environments. By unifying six common light\npropagation phenomena through an in-scattering representation, we establish a\nnew scattering-aware volume rendering pipeline adaptable to complex lightpaths.\nAdditionally, we introduce an adaptive learning strategy that enables\nautonomous determining of scattering directions and sampling intervals to\ncapture finer object details. The proposed network jointly optimizes NeRF\nparameters, scattering parameters, and camera motions to recover fine-grained\nscene representations from blurry images. Comprehensive evaluations demonstrate\nthat it effectively handles complex real-world scenarios, outperforming\nstate-of-the-art approaches in generating high-fidelity images with accurate\ngeometric details.\n", "link": "http://arxiv.org/abs/2508.13808v1", "date": "2025-08-19", "relevancy": 2.3239, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6025}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5657}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is-NeRF%3A%20In-scattering%20Neural%20Radiance%20Field%20for%20Blurred%20Images&body=Title%3A%20Is-NeRF%3A%20In-scattering%20Neural%20Radiance%20Field%20for%20Blurred%20Images%0AAuthor%3A%20Nan%20Luo%20and%20Chenglin%20Ye%20and%20Jiaxu%20Li%20and%20Gang%20Liu%20and%20Bo%20Wan%20and%20Di%20Wang%20and%20Lupeng%20Liu%20and%20Jun%20Xiao%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20has%20gained%20significant%20attention%20for%20its%0Aprominent%20implicit%203D%20representation%20and%20realistic%20novel%20view%20synthesis%0Acapabilities.%20Available%20works%20unexceptionally%20employ%20straight-line%20volume%0Arendering%2C%20which%20struggles%20to%20handle%20sophisticated%20lightpath%20scenarios%20and%0Aintroduces%20geometric%20ambiguities%20during%20training%2C%20particularly%20evident%20when%0Aprocessing%20motion-blurred%20images.%20To%20address%20these%20challenges%2C%20this%20work%0Aproposes%20a%20novel%20deblur%20neural%20radiance%20field%2C%20Is-NeRF%2C%20featuring%20explicit%0Alightpath%20modeling%20in%20real-world%20environments.%20By%20unifying%20six%20common%20light%0Apropagation%20phenomena%20through%20an%20in-scattering%20representation%2C%20we%20establish%20a%0Anew%20scattering-aware%20volume%20rendering%20pipeline%20adaptable%20to%20complex%20lightpaths.%0AAdditionally%2C%20we%20introduce%20an%20adaptive%20learning%20strategy%20that%20enables%0Aautonomous%20determining%20of%20scattering%20directions%20and%20sampling%20intervals%20to%0Acapture%20finer%20object%20details.%20The%20proposed%20network%20jointly%20optimizes%20NeRF%0Aparameters%2C%20scattering%20parameters%2C%20and%20camera%20motions%20to%20recover%20fine-grained%0Ascene%20representations%20from%20blurry%20images.%20Comprehensive%20evaluations%20demonstrate%0Athat%20it%20effectively%20handles%20complex%20real-world%20scenarios%2C%20outperforming%0Astate-of-the-art%20approaches%20in%20generating%20high-fidelity%20images%20with%20accurate%0Ageometric%20details.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs-NeRF%253A%2520In-scattering%2520Neural%2520Radiance%2520Field%2520for%2520Blurred%2520Images%26entry.906535625%3DNan%2520Luo%2520and%2520Chenglin%2520Ye%2520and%2520Jiaxu%2520Li%2520and%2520Gang%2520Liu%2520and%2520Bo%2520Wan%2520and%2520Di%2520Wang%2520and%2520Lupeng%2520Liu%2520and%2520Jun%2520Xiao%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520has%2520gained%2520significant%2520attention%2520for%2520its%250Aprominent%2520implicit%25203D%2520representation%2520and%2520realistic%2520novel%2520view%2520synthesis%250Acapabilities.%2520Available%2520works%2520unexceptionally%2520employ%2520straight-line%2520volume%250Arendering%252C%2520which%2520struggles%2520to%2520handle%2520sophisticated%2520lightpath%2520scenarios%2520and%250Aintroduces%2520geometric%2520ambiguities%2520during%2520training%252C%2520particularly%2520evident%2520when%250Aprocessing%2520motion-blurred%2520images.%2520To%2520address%2520these%2520challenges%252C%2520this%2520work%250Aproposes%2520a%2520novel%2520deblur%2520neural%2520radiance%2520field%252C%2520Is-NeRF%252C%2520featuring%2520explicit%250Alightpath%2520modeling%2520in%2520real-world%2520environments.%2520By%2520unifying%2520six%2520common%2520light%250Apropagation%2520phenomena%2520through%2520an%2520in-scattering%2520representation%252C%2520we%2520establish%2520a%250Anew%2520scattering-aware%2520volume%2520rendering%2520pipeline%2520adaptable%2520to%2520complex%2520lightpaths.%250AAdditionally%252C%2520we%2520introduce%2520an%2520adaptive%2520learning%2520strategy%2520that%2520enables%250Aautonomous%2520determining%2520of%2520scattering%2520directions%2520and%2520sampling%2520intervals%2520to%250Acapture%2520finer%2520object%2520details.%2520The%2520proposed%2520network%2520jointly%2520optimizes%2520NeRF%250Aparameters%252C%2520scattering%2520parameters%252C%2520and%2520camera%2520motions%2520to%2520recover%2520fine-grained%250Ascene%2520representations%2520from%2520blurry%2520images.%2520Comprehensive%2520evaluations%2520demonstrate%250Athat%2520it%2520effectively%2520handles%2520complex%2520real-world%2520scenarios%252C%2520outperforming%250Astate-of-the-art%2520approaches%2520in%2520generating%2520high-fidelity%2520images%2520with%2520accurate%250Ageometric%2520details.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is-NeRF%3A%20In-scattering%20Neural%20Radiance%20Field%20for%20Blurred%20Images&entry.906535625=Nan%20Luo%20and%20Chenglin%20Ye%20and%20Jiaxu%20Li%20and%20Gang%20Liu%20and%20Bo%20Wan%20and%20Di%20Wang%20and%20Lupeng%20Liu%20and%20Jun%20Xiao&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20has%20gained%20significant%20attention%20for%20its%0Aprominent%20implicit%203D%20representation%20and%20realistic%20novel%20view%20synthesis%0Acapabilities.%20Available%20works%20unexceptionally%20employ%20straight-line%20volume%0Arendering%2C%20which%20struggles%20to%20handle%20sophisticated%20lightpath%20scenarios%20and%0Aintroduces%20geometric%20ambiguities%20during%20training%2C%20particularly%20evident%20when%0Aprocessing%20motion-blurred%20images.%20To%20address%20these%20challenges%2C%20this%20work%0Aproposes%20a%20novel%20deblur%20neural%20radiance%20field%2C%20Is-NeRF%2C%20featuring%20explicit%0Alightpath%20modeling%20in%20real-world%20environments.%20By%20unifying%20six%20common%20light%0Apropagation%20phenomena%20through%20an%20in-scattering%20representation%2C%20we%20establish%20a%0Anew%20scattering-aware%20volume%20rendering%20pipeline%20adaptable%20to%20complex%20lightpaths.%0AAdditionally%2C%20we%20introduce%20an%20adaptive%20learning%20strategy%20that%20enables%0Aautonomous%20determining%20of%20scattering%20directions%20and%20sampling%20intervals%20to%0Acapture%20finer%20object%20details.%20The%20proposed%20network%20jointly%20optimizes%20NeRF%0Aparameters%2C%20scattering%20parameters%2C%20and%20camera%20motions%20to%20recover%20fine-grained%0Ascene%20representations%20from%20blurry%20images.%20Comprehensive%20evaluations%20demonstrate%0Athat%20it%20effectively%20handles%20complex%20real-world%20scenarios%2C%20outperforming%0Astate-of-the-art%20approaches%20in%20generating%20high-fidelity%20images%20with%20accurate%0Ageometric%20details.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13808v1&entry.124074799=Read"},
{"title": "Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient\n  Descent", "author": "Marina Sheshukova and Sergey Samsonov and Denis Belomestny and Eric Moulines and Qi-Man Shao and Zhuo-Song Zhang and Alexey Naumov", "abstract": "  In this paper, we establish the non-asymptotic validity of the multiplier\nbootstrap procedure for constructing the confidence sets using the Stochastic\nGradient Descent (SGD) algorithm. Under appropriate regularity conditions, our\napproach avoids the need to approximate the limiting covariance of\nPolyak-Ruppert SGD iterates, which allows us to derive approximation rates in\nconvex distance of order up to $1/\\sqrt{n}$. Notably, this rate can be faster\nthan the one that can be proven in the Polyak-Juditsky central limit theorem.\nTo our knowledge, this provides the first fully non-asymptotic bound on the\naccuracy of bootstrap approximations in SGD algorithms. Our analysis builds on\nthe Gaussian approximation results for nonlinear statistics of independent\nrandom variables.\n", "link": "http://arxiv.org/abs/2502.06719v2", "date": "2025-08-19", "relevancy": 2.3237, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4818}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4673}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Approximation%20and%20Multiplier%20Bootstrap%20for%20Stochastic%20Gradient%0A%20%20Descent&body=Title%3A%20Gaussian%20Approximation%20and%20Multiplier%20Bootstrap%20for%20Stochastic%20Gradient%0A%20%20Descent%0AAuthor%3A%20Marina%20Sheshukova%20and%20Sergey%20Samsonov%20and%20Denis%20Belomestny%20and%20Eric%20Moulines%20and%20Qi-Man%20Shao%20and%20Zhuo-Song%20Zhang%20and%20Alexey%20Naumov%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20establish%20the%20non-asymptotic%20validity%20of%20the%20multiplier%0Abootstrap%20procedure%20for%20constructing%20the%20confidence%20sets%20using%20the%20Stochastic%0AGradient%20Descent%20%28SGD%29%20algorithm.%20Under%20appropriate%20regularity%20conditions%2C%20our%0Aapproach%20avoids%20the%20need%20to%20approximate%20the%20limiting%20covariance%20of%0APolyak-Ruppert%20SGD%20iterates%2C%20which%20allows%20us%20to%20derive%20approximation%20rates%20in%0Aconvex%20distance%20of%20order%20up%20to%20%241/%5Csqrt%7Bn%7D%24.%20Notably%2C%20this%20rate%20can%20be%20faster%0Athan%20the%20one%20that%20can%20be%20proven%20in%20the%20Polyak-Juditsky%20central%20limit%20theorem.%0ATo%20our%20knowledge%2C%20this%20provides%20the%20first%20fully%20non-asymptotic%20bound%20on%20the%0Aaccuracy%20of%20bootstrap%20approximations%20in%20SGD%20algorithms.%20Our%20analysis%20builds%20on%0Athe%20Gaussian%20approximation%20results%20for%20nonlinear%20statistics%20of%20independent%0Arandom%20variables.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06719v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Approximation%2520and%2520Multiplier%2520Bootstrap%2520for%2520Stochastic%2520Gradient%250A%2520%2520Descent%26entry.906535625%3DMarina%2520Sheshukova%2520and%2520Sergey%2520Samsonov%2520and%2520Denis%2520Belomestny%2520and%2520Eric%2520Moulines%2520and%2520Qi-Man%2520Shao%2520and%2520Zhuo-Song%2520Zhang%2520and%2520Alexey%2520Naumov%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520establish%2520the%2520non-asymptotic%2520validity%2520of%2520the%2520multiplier%250Abootstrap%2520procedure%2520for%2520constructing%2520the%2520confidence%2520sets%2520using%2520the%2520Stochastic%250AGradient%2520Descent%2520%2528SGD%2529%2520algorithm.%2520Under%2520appropriate%2520regularity%2520conditions%252C%2520our%250Aapproach%2520avoids%2520the%2520need%2520to%2520approximate%2520the%2520limiting%2520covariance%2520of%250APolyak-Ruppert%2520SGD%2520iterates%252C%2520which%2520allows%2520us%2520to%2520derive%2520approximation%2520rates%2520in%250Aconvex%2520distance%2520of%2520order%2520up%2520to%2520%25241/%255Csqrt%257Bn%257D%2524.%2520Notably%252C%2520this%2520rate%2520can%2520be%2520faster%250Athan%2520the%2520one%2520that%2520can%2520be%2520proven%2520in%2520the%2520Polyak-Juditsky%2520central%2520limit%2520theorem.%250ATo%2520our%2520knowledge%252C%2520this%2520provides%2520the%2520first%2520fully%2520non-asymptotic%2520bound%2520on%2520the%250Aaccuracy%2520of%2520bootstrap%2520approximations%2520in%2520SGD%2520algorithms.%2520Our%2520analysis%2520builds%2520on%250Athe%2520Gaussian%2520approximation%2520results%2520for%2520nonlinear%2520statistics%2520of%2520independent%250Arandom%2520variables.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06719v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Approximation%20and%20Multiplier%20Bootstrap%20for%20Stochastic%20Gradient%0A%20%20Descent&entry.906535625=Marina%20Sheshukova%20and%20Sergey%20Samsonov%20and%20Denis%20Belomestny%20and%20Eric%20Moulines%20and%20Qi-Man%20Shao%20and%20Zhuo-Song%20Zhang%20and%20Alexey%20Naumov&entry.1292438233=%20%20In%20this%20paper%2C%20we%20establish%20the%20non-asymptotic%20validity%20of%20the%20multiplier%0Abootstrap%20procedure%20for%20constructing%20the%20confidence%20sets%20using%20the%20Stochastic%0AGradient%20Descent%20%28SGD%29%20algorithm.%20Under%20appropriate%20regularity%20conditions%2C%20our%0Aapproach%20avoids%20the%20need%20to%20approximate%20the%20limiting%20covariance%20of%0APolyak-Ruppert%20SGD%20iterates%2C%20which%20allows%20us%20to%20derive%20approximation%20rates%20in%0Aconvex%20distance%20of%20order%20up%20to%20%241/%5Csqrt%7Bn%7D%24.%20Notably%2C%20this%20rate%20can%20be%20faster%0Athan%20the%20one%20that%20can%20be%20proven%20in%20the%20Polyak-Juditsky%20central%20limit%20theorem.%0ATo%20our%20knowledge%2C%20this%20provides%20the%20first%20fully%20non-asymptotic%20bound%20on%20the%0Aaccuracy%20of%20bootstrap%20approximations%20in%20SGD%20algorithms.%20Our%20analysis%20builds%20on%0Athe%20Gaussian%20approximation%20results%20for%20nonlinear%20statistics%20of%20independent%0Arandom%20variables.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06719v2&entry.124074799=Read"},
{"title": "Shape-from-Template with Generalised Camera", "author": "Agniva Sengupta and Stefan Zachow", "abstract": "  This article presents a new method for non-rigidly registering a 3D shape to\n2D keypoints observed by a constellation of multiple cameras. Non-rigid\nregistration of a 3D shape to observed 2D keypoints, i.e., Shape-from-Template\n(SfT), has been widely studied using single images, but SfT with information\nfrom multiple-cameras jointly opens new directions for extending the scope of\nknown use-cases such as 3D shape registration in medical imaging and\nregistration from hand-held cameras, to name a few. We represent such\nmulti-camera setup with the generalised camera model; therefore any collection\nof perspective or orthographic cameras observing any deforming object can be\nregistered. We propose multiple approaches for such SfT: the first approach\nwhere the corresponded keypoints lie on a direction vector from a known 3D\npoint in space, the second approach where the corresponded keypoints lie on a\ndirection vector from an unknown 3D point in space but with known orientation\nw.r.t some local reference frame, and a third approach where, apart from\ncorrespondences, the silhouette of the imaged object is also known. Together,\nthese form the first set of solutions to the SfT problem with generalised\ncameras. The key idea behind SfT with generalised camera is the improved\nreconstruction accuracy from estimating deformed shape while utilising the\nadditional information from the mutual constraints between multiple views of a\ndeformed object. The correspondence-based approaches are solved with convex\nprogramming while the silhouette-based approach is an iterative refinement of\nthe results from the convex solutions. We demonstrate the accuracy of our\nproposed methods on many synthetic and real data\n", "link": "http://arxiv.org/abs/2508.13791v1", "date": "2025-08-19", "relevancy": 2.3146, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5963}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5725}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shape-from-Template%20with%20Generalised%20Camera&body=Title%3A%20Shape-from-Template%20with%20Generalised%20Camera%0AAuthor%3A%20Agniva%20Sengupta%20and%20Stefan%20Zachow%0AAbstract%3A%20%20%20This%20article%20presents%20a%20new%20method%20for%20non-rigidly%20registering%20a%203D%20shape%20to%0A2D%20keypoints%20observed%20by%20a%20constellation%20of%20multiple%20cameras.%20Non-rigid%0Aregistration%20of%20a%203D%20shape%20to%20observed%202D%20keypoints%2C%20i.e.%2C%20Shape-from-Template%0A%28SfT%29%2C%20has%20been%20widely%20studied%20using%20single%20images%2C%20but%20SfT%20with%20information%0Afrom%20multiple-cameras%20jointly%20opens%20new%20directions%20for%20extending%20the%20scope%20of%0Aknown%20use-cases%20such%20as%203D%20shape%20registration%20in%20medical%20imaging%20and%0Aregistration%20from%20hand-held%20cameras%2C%20to%20name%20a%20few.%20We%20represent%20such%0Amulti-camera%20setup%20with%20the%20generalised%20camera%20model%3B%20therefore%20any%20collection%0Aof%20perspective%20or%20orthographic%20cameras%20observing%20any%20deforming%20object%20can%20be%0Aregistered.%20We%20propose%20multiple%20approaches%20for%20such%20SfT%3A%20the%20first%20approach%0Awhere%20the%20corresponded%20keypoints%20lie%20on%20a%20direction%20vector%20from%20a%20known%203D%0Apoint%20in%20space%2C%20the%20second%20approach%20where%20the%20corresponded%20keypoints%20lie%20on%20a%0Adirection%20vector%20from%20an%20unknown%203D%20point%20in%20space%20but%20with%20known%20orientation%0Aw.r.t%20some%20local%20reference%20frame%2C%20and%20a%20third%20approach%20where%2C%20apart%20from%0Acorrespondences%2C%20the%20silhouette%20of%20the%20imaged%20object%20is%20also%20known.%20Together%2C%0Athese%20form%20the%20first%20set%20of%20solutions%20to%20the%20SfT%20problem%20with%20generalised%0Acameras.%20The%20key%20idea%20behind%20SfT%20with%20generalised%20camera%20is%20the%20improved%0Areconstruction%20accuracy%20from%20estimating%20deformed%20shape%20while%20utilising%20the%0Aadditional%20information%20from%20the%20mutual%20constraints%20between%20multiple%20views%20of%20a%0Adeformed%20object.%20The%20correspondence-based%20approaches%20are%20solved%20with%20convex%0Aprogramming%20while%20the%20silhouette-based%20approach%20is%20an%20iterative%20refinement%20of%0Athe%20results%20from%20the%20convex%20solutions.%20We%20demonstrate%20the%20accuracy%20of%20our%0Aproposed%20methods%20on%20many%20synthetic%20and%20real%20data%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShape-from-Template%2520with%2520Generalised%2520Camera%26entry.906535625%3DAgniva%2520Sengupta%2520and%2520Stefan%2520Zachow%26entry.1292438233%3D%2520%2520This%2520article%2520presents%2520a%2520new%2520method%2520for%2520non-rigidly%2520registering%2520a%25203D%2520shape%2520to%250A2D%2520keypoints%2520observed%2520by%2520a%2520constellation%2520of%2520multiple%2520cameras.%2520Non-rigid%250Aregistration%2520of%2520a%25203D%2520shape%2520to%2520observed%25202D%2520keypoints%252C%2520i.e.%252C%2520Shape-from-Template%250A%2528SfT%2529%252C%2520has%2520been%2520widely%2520studied%2520using%2520single%2520images%252C%2520but%2520SfT%2520with%2520information%250Afrom%2520multiple-cameras%2520jointly%2520opens%2520new%2520directions%2520for%2520extending%2520the%2520scope%2520of%250Aknown%2520use-cases%2520such%2520as%25203D%2520shape%2520registration%2520in%2520medical%2520imaging%2520and%250Aregistration%2520from%2520hand-held%2520cameras%252C%2520to%2520name%2520a%2520few.%2520We%2520represent%2520such%250Amulti-camera%2520setup%2520with%2520the%2520generalised%2520camera%2520model%253B%2520therefore%2520any%2520collection%250Aof%2520perspective%2520or%2520orthographic%2520cameras%2520observing%2520any%2520deforming%2520object%2520can%2520be%250Aregistered.%2520We%2520propose%2520multiple%2520approaches%2520for%2520such%2520SfT%253A%2520the%2520first%2520approach%250Awhere%2520the%2520corresponded%2520keypoints%2520lie%2520on%2520a%2520direction%2520vector%2520from%2520a%2520known%25203D%250Apoint%2520in%2520space%252C%2520the%2520second%2520approach%2520where%2520the%2520corresponded%2520keypoints%2520lie%2520on%2520a%250Adirection%2520vector%2520from%2520an%2520unknown%25203D%2520point%2520in%2520space%2520but%2520with%2520known%2520orientation%250Aw.r.t%2520some%2520local%2520reference%2520frame%252C%2520and%2520a%2520third%2520approach%2520where%252C%2520apart%2520from%250Acorrespondences%252C%2520the%2520silhouette%2520of%2520the%2520imaged%2520object%2520is%2520also%2520known.%2520Together%252C%250Athese%2520form%2520the%2520first%2520set%2520of%2520solutions%2520to%2520the%2520SfT%2520problem%2520with%2520generalised%250Acameras.%2520The%2520key%2520idea%2520behind%2520SfT%2520with%2520generalised%2520camera%2520is%2520the%2520improved%250Areconstruction%2520accuracy%2520from%2520estimating%2520deformed%2520shape%2520while%2520utilising%2520the%250Aadditional%2520information%2520from%2520the%2520mutual%2520constraints%2520between%2520multiple%2520views%2520of%2520a%250Adeformed%2520object.%2520The%2520correspondence-based%2520approaches%2520are%2520solved%2520with%2520convex%250Aprogramming%2520while%2520the%2520silhouette-based%2520approach%2520is%2520an%2520iterative%2520refinement%2520of%250Athe%2520results%2520from%2520the%2520convex%2520solutions.%2520We%2520demonstrate%2520the%2520accuracy%2520of%2520our%250Aproposed%2520methods%2520on%2520many%2520synthetic%2520and%2520real%2520data%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shape-from-Template%20with%20Generalised%20Camera&entry.906535625=Agniva%20Sengupta%20and%20Stefan%20Zachow&entry.1292438233=%20%20This%20article%20presents%20a%20new%20method%20for%20non-rigidly%20registering%20a%203D%20shape%20to%0A2D%20keypoints%20observed%20by%20a%20constellation%20of%20multiple%20cameras.%20Non-rigid%0Aregistration%20of%20a%203D%20shape%20to%20observed%202D%20keypoints%2C%20i.e.%2C%20Shape-from-Template%0A%28SfT%29%2C%20has%20been%20widely%20studied%20using%20single%20images%2C%20but%20SfT%20with%20information%0Afrom%20multiple-cameras%20jointly%20opens%20new%20directions%20for%20extending%20the%20scope%20of%0Aknown%20use-cases%20such%20as%203D%20shape%20registration%20in%20medical%20imaging%20and%0Aregistration%20from%20hand-held%20cameras%2C%20to%20name%20a%20few.%20We%20represent%20such%0Amulti-camera%20setup%20with%20the%20generalised%20camera%20model%3B%20therefore%20any%20collection%0Aof%20perspective%20or%20orthographic%20cameras%20observing%20any%20deforming%20object%20can%20be%0Aregistered.%20We%20propose%20multiple%20approaches%20for%20such%20SfT%3A%20the%20first%20approach%0Awhere%20the%20corresponded%20keypoints%20lie%20on%20a%20direction%20vector%20from%20a%20known%203D%0Apoint%20in%20space%2C%20the%20second%20approach%20where%20the%20corresponded%20keypoints%20lie%20on%20a%0Adirection%20vector%20from%20an%20unknown%203D%20point%20in%20space%20but%20with%20known%20orientation%0Aw.r.t%20some%20local%20reference%20frame%2C%20and%20a%20third%20approach%20where%2C%20apart%20from%0Acorrespondences%2C%20the%20silhouette%20of%20the%20imaged%20object%20is%20also%20known.%20Together%2C%0Athese%20form%20the%20first%20set%20of%20solutions%20to%20the%20SfT%20problem%20with%20generalised%0Acameras.%20The%20key%20idea%20behind%20SfT%20with%20generalised%20camera%20is%20the%20improved%0Areconstruction%20accuracy%20from%20estimating%20deformed%20shape%20while%20utilising%20the%0Aadditional%20information%20from%20the%20mutual%20constraints%20between%20multiple%20views%20of%20a%0Adeformed%20object.%20The%20correspondence-based%20approaches%20are%20solved%20with%20convex%0Aprogramming%20while%20the%20silhouette-based%20approach%20is%20an%20iterative%20refinement%20of%0Athe%20results%20from%20the%20convex%20solutions.%20We%20demonstrate%20the%20accuracy%20of%20our%0Aproposed%20methods%20on%20many%20synthetic%20and%20real%20data%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13791v1&entry.124074799=Read"},
{"title": "Enhancing Cost Efficiency in Active Learning with Candidate Set Query", "author": "Yeho Gwon and Sehyun Hwang and Hoyoung Kim and Jungseul Ok and Suha Kwak", "abstract": "  This paper introduces a cost-efficient active learning (AL) framework for\nclassification, featuring a novel query design called candidate set query.\nUnlike traditional AL queries requiring the oracle to examine all possible\nclasses, our method narrows down the set of candidate classes likely to include\nthe ground-truth class, significantly reducing the search space and labeling\ncost. Moreover, we leverage conformal prediction to dynamically generate small\nyet reliable candidate sets, adapting to model enhancement over successive AL\nrounds. To this end, we introduce an acquisition function designed to\nprioritize data points that offer high information gain at lower cost.\nEmpirical evaluations on CIFAR-10, CIFAR-100, and ImageNet64x64 demonstrate the\neffectiveness and scalability of our framework. Notably, it reduces labeling\ncost by 48% on ImageNet64x64. The project page can be found at\nhttps://yehogwon.github.io/csq-al.\n", "link": "http://arxiv.org/abs/2502.06209v2", "date": "2025-08-19", "relevancy": 2.3087, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4871}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4497}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Cost%20Efficiency%20in%20Active%20Learning%20with%20Candidate%20Set%20Query&body=Title%3A%20Enhancing%20Cost%20Efficiency%20in%20Active%20Learning%20with%20Candidate%20Set%20Query%0AAuthor%3A%20Yeho%20Gwon%20and%20Sehyun%20Hwang%20and%20Hoyoung%20Kim%20and%20Jungseul%20Ok%20and%20Suha%20Kwak%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20cost-efficient%20active%20learning%20%28AL%29%20framework%20for%0Aclassification%2C%20featuring%20a%20novel%20query%20design%20called%20candidate%20set%20query.%0AUnlike%20traditional%20AL%20queries%20requiring%20the%20oracle%20to%20examine%20all%20possible%0Aclasses%2C%20our%20method%20narrows%20down%20the%20set%20of%20candidate%20classes%20likely%20to%20include%0Athe%20ground-truth%20class%2C%20significantly%20reducing%20the%20search%20space%20and%20labeling%0Acost.%20Moreover%2C%20we%20leverage%20conformal%20prediction%20to%20dynamically%20generate%20small%0Ayet%20reliable%20candidate%20sets%2C%20adapting%20to%20model%20enhancement%20over%20successive%20AL%0Arounds.%20To%20this%20end%2C%20we%20introduce%20an%20acquisition%20function%20designed%20to%0Aprioritize%20data%20points%20that%20offer%20high%20information%20gain%20at%20lower%20cost.%0AEmpirical%20evaluations%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet64x64%20demonstrate%20the%0Aeffectiveness%20and%20scalability%20of%20our%20framework.%20Notably%2C%20it%20reduces%20labeling%0Acost%20by%2048%25%20on%20ImageNet64x64.%20The%20project%20page%20can%20be%20found%20at%0Ahttps%3A//yehogwon.github.io/csq-al.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06209v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Cost%2520Efficiency%2520in%2520Active%2520Learning%2520with%2520Candidate%2520Set%2520Query%26entry.906535625%3DYeho%2520Gwon%2520and%2520Sehyun%2520Hwang%2520and%2520Hoyoung%2520Kim%2520and%2520Jungseul%2520Ok%2520and%2520Suha%2520Kwak%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520cost-efficient%2520active%2520learning%2520%2528AL%2529%2520framework%2520for%250Aclassification%252C%2520featuring%2520a%2520novel%2520query%2520design%2520called%2520candidate%2520set%2520query.%250AUnlike%2520traditional%2520AL%2520queries%2520requiring%2520the%2520oracle%2520to%2520examine%2520all%2520possible%250Aclasses%252C%2520our%2520method%2520narrows%2520down%2520the%2520set%2520of%2520candidate%2520classes%2520likely%2520to%2520include%250Athe%2520ground-truth%2520class%252C%2520significantly%2520reducing%2520the%2520search%2520space%2520and%2520labeling%250Acost.%2520Moreover%252C%2520we%2520leverage%2520conformal%2520prediction%2520to%2520dynamically%2520generate%2520small%250Ayet%2520reliable%2520candidate%2520sets%252C%2520adapting%2520to%2520model%2520enhancement%2520over%2520successive%2520AL%250Arounds.%2520To%2520this%2520end%252C%2520we%2520introduce%2520an%2520acquisition%2520function%2520designed%2520to%250Aprioritize%2520data%2520points%2520that%2520offer%2520high%2520information%2520gain%2520at%2520lower%2520cost.%250AEmpirical%2520evaluations%2520on%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%2520ImageNet64x64%2520demonstrate%2520the%250Aeffectiveness%2520and%2520scalability%2520of%2520our%2520framework.%2520Notably%252C%2520it%2520reduces%2520labeling%250Acost%2520by%252048%2525%2520on%2520ImageNet64x64.%2520The%2520project%2520page%2520can%2520be%2520found%2520at%250Ahttps%253A//yehogwon.github.io/csq-al.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06209v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Cost%20Efficiency%20in%20Active%20Learning%20with%20Candidate%20Set%20Query&entry.906535625=Yeho%20Gwon%20and%20Sehyun%20Hwang%20and%20Hoyoung%20Kim%20and%20Jungseul%20Ok%20and%20Suha%20Kwak&entry.1292438233=%20%20This%20paper%20introduces%20a%20cost-efficient%20active%20learning%20%28AL%29%20framework%20for%0Aclassification%2C%20featuring%20a%20novel%20query%20design%20called%20candidate%20set%20query.%0AUnlike%20traditional%20AL%20queries%20requiring%20the%20oracle%20to%20examine%20all%20possible%0Aclasses%2C%20our%20method%20narrows%20down%20the%20set%20of%20candidate%20classes%20likely%20to%20include%0Athe%20ground-truth%20class%2C%20significantly%20reducing%20the%20search%20space%20and%20labeling%0Acost.%20Moreover%2C%20we%20leverage%20conformal%20prediction%20to%20dynamically%20generate%20small%0Ayet%20reliable%20candidate%20sets%2C%20adapting%20to%20model%20enhancement%20over%20successive%20AL%0Arounds.%20To%20this%20end%2C%20we%20introduce%20an%20acquisition%20function%20designed%20to%0Aprioritize%20data%20points%20that%20offer%20high%20information%20gain%20at%20lower%20cost.%0AEmpirical%20evaluations%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20ImageNet64x64%20demonstrate%20the%0Aeffectiveness%20and%20scalability%20of%20our%20framework.%20Notably%2C%20it%20reduces%20labeling%0Acost%20by%2048%25%20on%20ImageNet64x64.%20The%20project%20page%20can%20be%20found%20at%0Ahttps%3A//yehogwon.github.io/csq-al.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06209v2&entry.124074799=Read"},
{"title": "Mitigating Cross-Image Information Leakage in LVLMs for Multi-Image\n  Tasks", "author": "Yeji Park and Minyoung Lee and Sanghyuk Chun and Junsuk Choe", "abstract": "  Large Vision-Language Models (LVLMs) demonstrate strong performance on\nsingle-image tasks. However, we observe that their performance degrades\nsignificantly when handling multi-image inputs. This occurs because visual cues\nfrom different images become entangled in the model's output. We refer to this\nphenomenon as cross-image information leakage. To address this issue, we\npropose FOCUS, a training-free and architecture-agnostic decoding strategy that\nmitigates cross-image information leakage during inference. FOCUS sequentially\nmasks all but one image with random noise, guiding the model to focus on the\nsingle clean image. We repeat this process across all target images to obtain\nlogits under partially masked contexts. These logits are aggregated and then\ncontrastively refined using a noise-only reference input, which suppresses the\nleakage and yields more accurate outputs. FOCUS consistently improves\nperformance across four multi-image benchmarks and diverse LVLM families. This\ndemonstrates that FOCUS offers a general and practical solution for enhancing\nmulti-image reasoning without additional training or architectural\nmodifications.\n", "link": "http://arxiv.org/abs/2508.13744v1", "date": "2025-08-19", "relevancy": 2.2806, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5716}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5716}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Cross-Image%20Information%20Leakage%20in%20LVLMs%20for%20Multi-Image%0A%20%20Tasks&body=Title%3A%20Mitigating%20Cross-Image%20Information%20Leakage%20in%20LVLMs%20for%20Multi-Image%0A%20%20Tasks%0AAuthor%3A%20Yeji%20Park%20and%20Minyoung%20Lee%20and%20Sanghyuk%20Chun%20and%20Junsuk%20Choe%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20demonstrate%20strong%20performance%20on%0Asingle-image%20tasks.%20However%2C%20we%20observe%20that%20their%20performance%20degrades%0Asignificantly%20when%20handling%20multi-image%20inputs.%20This%20occurs%20because%20visual%20cues%0Afrom%20different%20images%20become%20entangled%20in%20the%20model%27s%20output.%20We%20refer%20to%20this%0Aphenomenon%20as%20cross-image%20information%20leakage.%20To%20address%20this%20issue%2C%20we%0Apropose%20FOCUS%2C%20a%20training-free%20and%20architecture-agnostic%20decoding%20strategy%20that%0Amitigates%20cross-image%20information%20leakage%20during%20inference.%20FOCUS%20sequentially%0Amasks%20all%20but%20one%20image%20with%20random%20noise%2C%20guiding%20the%20model%20to%20focus%20on%20the%0Asingle%20clean%20image.%20We%20repeat%20this%20process%20across%20all%20target%20images%20to%20obtain%0Alogits%20under%20partially%20masked%20contexts.%20These%20logits%20are%20aggregated%20and%20then%0Acontrastively%20refined%20using%20a%20noise-only%20reference%20input%2C%20which%20suppresses%20the%0Aleakage%20and%20yields%20more%20accurate%20outputs.%20FOCUS%20consistently%20improves%0Aperformance%20across%20four%20multi-image%20benchmarks%20and%20diverse%20LVLM%20families.%20This%0Ademonstrates%20that%20FOCUS%20offers%20a%20general%20and%20practical%20solution%20for%20enhancing%0Amulti-image%20reasoning%20without%20additional%20training%20or%20architectural%0Amodifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13744v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Cross-Image%2520Information%2520Leakage%2520in%2520LVLMs%2520for%2520Multi-Image%250A%2520%2520Tasks%26entry.906535625%3DYeji%2520Park%2520and%2520Minyoung%2520Lee%2520and%2520Sanghyuk%2520Chun%2520and%2520Junsuk%2520Choe%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520demonstrate%2520strong%2520performance%2520on%250Asingle-image%2520tasks.%2520However%252C%2520we%2520observe%2520that%2520their%2520performance%2520degrades%250Asignificantly%2520when%2520handling%2520multi-image%2520inputs.%2520This%2520occurs%2520because%2520visual%2520cues%250Afrom%2520different%2520images%2520become%2520entangled%2520in%2520the%2520model%2527s%2520output.%2520We%2520refer%2520to%2520this%250Aphenomenon%2520as%2520cross-image%2520information%2520leakage.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520FOCUS%252C%2520a%2520training-free%2520and%2520architecture-agnostic%2520decoding%2520strategy%2520that%250Amitigates%2520cross-image%2520information%2520leakage%2520during%2520inference.%2520FOCUS%2520sequentially%250Amasks%2520all%2520but%2520one%2520image%2520with%2520random%2520noise%252C%2520guiding%2520the%2520model%2520to%2520focus%2520on%2520the%250Asingle%2520clean%2520image.%2520We%2520repeat%2520this%2520process%2520across%2520all%2520target%2520images%2520to%2520obtain%250Alogits%2520under%2520partially%2520masked%2520contexts.%2520These%2520logits%2520are%2520aggregated%2520and%2520then%250Acontrastively%2520refined%2520using%2520a%2520noise-only%2520reference%2520input%252C%2520which%2520suppresses%2520the%250Aleakage%2520and%2520yields%2520more%2520accurate%2520outputs.%2520FOCUS%2520consistently%2520improves%250Aperformance%2520across%2520four%2520multi-image%2520benchmarks%2520and%2520diverse%2520LVLM%2520families.%2520This%250Ademonstrates%2520that%2520FOCUS%2520offers%2520a%2520general%2520and%2520practical%2520solution%2520for%2520enhancing%250Amulti-image%2520reasoning%2520without%2520additional%2520training%2520or%2520architectural%250Amodifications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13744v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Cross-Image%20Information%20Leakage%20in%20LVLMs%20for%20Multi-Image%0A%20%20Tasks&entry.906535625=Yeji%20Park%20and%20Minyoung%20Lee%20and%20Sanghyuk%20Chun%20and%20Junsuk%20Choe&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20demonstrate%20strong%20performance%20on%0Asingle-image%20tasks.%20However%2C%20we%20observe%20that%20their%20performance%20degrades%0Asignificantly%20when%20handling%20multi-image%20inputs.%20This%20occurs%20because%20visual%20cues%0Afrom%20different%20images%20become%20entangled%20in%20the%20model%27s%20output.%20We%20refer%20to%20this%0Aphenomenon%20as%20cross-image%20information%20leakage.%20To%20address%20this%20issue%2C%20we%0Apropose%20FOCUS%2C%20a%20training-free%20and%20architecture-agnostic%20decoding%20strategy%20that%0Amitigates%20cross-image%20information%20leakage%20during%20inference.%20FOCUS%20sequentially%0Amasks%20all%20but%20one%20image%20with%20random%20noise%2C%20guiding%20the%20model%20to%20focus%20on%20the%0Asingle%20clean%20image.%20We%20repeat%20this%20process%20across%20all%20target%20images%20to%20obtain%0Alogits%20under%20partially%20masked%20contexts.%20These%20logits%20are%20aggregated%20and%20then%0Acontrastively%20refined%20using%20a%20noise-only%20reference%20input%2C%20which%20suppresses%20the%0Aleakage%20and%20yields%20more%20accurate%20outputs.%20FOCUS%20consistently%20improves%0Aperformance%20across%20four%20multi-image%20benchmarks%20and%20diverse%20LVLM%20families.%20This%0Ademonstrates%20that%20FOCUS%20offers%20a%20general%20and%20practical%20solution%20for%20enhancing%0Amulti-image%20reasoning%20without%20additional%20training%20or%20architectural%0Amodifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13744v1&entry.124074799=Read"},
{"title": "LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware\n  LoRA Fine-Tuning", "author": "Chenjian Gao and Lihe Ding and Xin Cai and Zhanpeng Huang and Zibin Wang and Tianfan Xue", "abstract": "  Video editing using diffusion models has achieved remarkable results in\ngenerating high-quality edits for videos. However, current methods often rely\non large-scale pretraining, limiting flexibility for specific edits.\nFirst-frame-guided editing provides control over the first frame, but lacks\nflexibility over subsequent frames. To address this, we propose a mask-based\nLoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video\n(I2V) models for flexible video editing. Our key innovation is using a\nspatiotemporal mask to strategically guide the LoRA fine-tuning process. This\nteaches the model two distinct skills: first, to interpret the mask as a\ncommand to either preserve content from the source video or generate new\ncontent in designated regions. Second, for these generated regions, LoRA learns\nto synthesize either temporally consistent motion inherited from the video or\nnovel appearances guided by user-provided reference frames. This\ndual-capability LoRA grants users control over the edit's entire temporal\nevolution, allowing complex transformations like an object rotating or a flower\nblooming. Experimental results show our method achieves superior video editing\nperformance compared to baseline methods. Project Page:\nhttps://cjeen.github.io/LoRAEdit\n", "link": "http://arxiv.org/abs/2506.10082v5", "date": "2025-08-19", "relevancy": 2.2801, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6345}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5765}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA-Edit%3A%20Controllable%20First-Frame-Guided%20Video%20Editing%20via%20Mask-Aware%0A%20%20LoRA%20Fine-Tuning&body=Title%3A%20LoRA-Edit%3A%20Controllable%20First-Frame-Guided%20Video%20Editing%20via%20Mask-Aware%0A%20%20LoRA%20Fine-Tuning%0AAuthor%3A%20Chenjian%20Gao%20and%20Lihe%20Ding%20and%20Xin%20Cai%20and%20Zhanpeng%20Huang%20and%20Zibin%20Wang%20and%20Tianfan%20Xue%0AAbstract%3A%20%20%20Video%20editing%20using%20diffusion%20models%20has%20achieved%20remarkable%20results%20in%0Agenerating%20high-quality%20edits%20for%20videos.%20However%2C%20current%20methods%20often%20rely%0Aon%20large-scale%20pretraining%2C%20limiting%20flexibility%20for%20specific%20edits.%0AFirst-frame-guided%20editing%20provides%20control%20over%20the%20first%20frame%2C%20but%20lacks%0Aflexibility%20over%20subsequent%20frames.%20To%20address%20this%2C%20we%20propose%20a%20mask-based%0ALoRA%20%28Low-Rank%20Adaptation%29%20tuning%20method%20that%20adapts%20pretrained%20Image-to-Video%0A%28I2V%29%20models%20for%20flexible%20video%20editing.%20Our%20key%20innovation%20is%20using%20a%0Aspatiotemporal%20mask%20to%20strategically%20guide%20the%20LoRA%20fine-tuning%20process.%20This%0Ateaches%20the%20model%20two%20distinct%20skills%3A%20first%2C%20to%20interpret%20the%20mask%20as%20a%0Acommand%20to%20either%20preserve%20content%20from%20the%20source%20video%20or%20generate%20new%0Acontent%20in%20designated%20regions.%20Second%2C%20for%20these%20generated%20regions%2C%20LoRA%20learns%0Ato%20synthesize%20either%20temporally%20consistent%20motion%20inherited%20from%20the%20video%20or%0Anovel%20appearances%20guided%20by%20user-provided%20reference%20frames.%20This%0Adual-capability%20LoRA%20grants%20users%20control%20over%20the%20edit%27s%20entire%20temporal%0Aevolution%2C%20allowing%20complex%20transformations%20like%20an%20object%20rotating%20or%20a%20flower%0Ablooming.%20Experimental%20results%20show%20our%20method%20achieves%20superior%20video%20editing%0Aperformance%20compared%20to%20baseline%20methods.%20Project%20Page%3A%0Ahttps%3A//cjeen.github.io/LoRAEdit%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10082v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA-Edit%253A%2520Controllable%2520First-Frame-Guided%2520Video%2520Editing%2520via%2520Mask-Aware%250A%2520%2520LoRA%2520Fine-Tuning%26entry.906535625%3DChenjian%2520Gao%2520and%2520Lihe%2520Ding%2520and%2520Xin%2520Cai%2520and%2520Zhanpeng%2520Huang%2520and%2520Zibin%2520Wang%2520and%2520Tianfan%2520Xue%26entry.1292438233%3D%2520%2520Video%2520editing%2520using%2520diffusion%2520models%2520has%2520achieved%2520remarkable%2520results%2520in%250Agenerating%2520high-quality%2520edits%2520for%2520videos.%2520However%252C%2520current%2520methods%2520often%2520rely%250Aon%2520large-scale%2520pretraining%252C%2520limiting%2520flexibility%2520for%2520specific%2520edits.%250AFirst-frame-guided%2520editing%2520provides%2520control%2520over%2520the%2520first%2520frame%252C%2520but%2520lacks%250Aflexibility%2520over%2520subsequent%2520frames.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520mask-based%250ALoRA%2520%2528Low-Rank%2520Adaptation%2529%2520tuning%2520method%2520that%2520adapts%2520pretrained%2520Image-to-Video%250A%2528I2V%2529%2520models%2520for%2520flexible%2520video%2520editing.%2520Our%2520key%2520innovation%2520is%2520using%2520a%250Aspatiotemporal%2520mask%2520to%2520strategically%2520guide%2520the%2520LoRA%2520fine-tuning%2520process.%2520This%250Ateaches%2520the%2520model%2520two%2520distinct%2520skills%253A%2520first%252C%2520to%2520interpret%2520the%2520mask%2520as%2520a%250Acommand%2520to%2520either%2520preserve%2520content%2520from%2520the%2520source%2520video%2520or%2520generate%2520new%250Acontent%2520in%2520designated%2520regions.%2520Second%252C%2520for%2520these%2520generated%2520regions%252C%2520LoRA%2520learns%250Ato%2520synthesize%2520either%2520temporally%2520consistent%2520motion%2520inherited%2520from%2520the%2520video%2520or%250Anovel%2520appearances%2520guided%2520by%2520user-provided%2520reference%2520frames.%2520This%250Adual-capability%2520LoRA%2520grants%2520users%2520control%2520over%2520the%2520edit%2527s%2520entire%2520temporal%250Aevolution%252C%2520allowing%2520complex%2520transformations%2520like%2520an%2520object%2520rotating%2520or%2520a%2520flower%250Ablooming.%2520Experimental%2520results%2520show%2520our%2520method%2520achieves%2520superior%2520video%2520editing%250Aperformance%2520compared%2520to%2520baseline%2520methods.%2520Project%2520Page%253A%250Ahttps%253A//cjeen.github.io/LoRAEdit%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10082v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA-Edit%3A%20Controllable%20First-Frame-Guided%20Video%20Editing%20via%20Mask-Aware%0A%20%20LoRA%20Fine-Tuning&entry.906535625=Chenjian%20Gao%20and%20Lihe%20Ding%20and%20Xin%20Cai%20and%20Zhanpeng%20Huang%20and%20Zibin%20Wang%20and%20Tianfan%20Xue&entry.1292438233=%20%20Video%20editing%20using%20diffusion%20models%20has%20achieved%20remarkable%20results%20in%0Agenerating%20high-quality%20edits%20for%20videos.%20However%2C%20current%20methods%20often%20rely%0Aon%20large-scale%20pretraining%2C%20limiting%20flexibility%20for%20specific%20edits.%0AFirst-frame-guided%20editing%20provides%20control%20over%20the%20first%20frame%2C%20but%20lacks%0Aflexibility%20over%20subsequent%20frames.%20To%20address%20this%2C%20we%20propose%20a%20mask-based%0ALoRA%20%28Low-Rank%20Adaptation%29%20tuning%20method%20that%20adapts%20pretrained%20Image-to-Video%0A%28I2V%29%20models%20for%20flexible%20video%20editing.%20Our%20key%20innovation%20is%20using%20a%0Aspatiotemporal%20mask%20to%20strategically%20guide%20the%20LoRA%20fine-tuning%20process.%20This%0Ateaches%20the%20model%20two%20distinct%20skills%3A%20first%2C%20to%20interpret%20the%20mask%20as%20a%0Acommand%20to%20either%20preserve%20content%20from%20the%20source%20video%20or%20generate%20new%0Acontent%20in%20designated%20regions.%20Second%2C%20for%20these%20generated%20regions%2C%20LoRA%20learns%0Ato%20synthesize%20either%20temporally%20consistent%20motion%20inherited%20from%20the%20video%20or%0Anovel%20appearances%20guided%20by%20user-provided%20reference%20frames.%20This%0Adual-capability%20LoRA%20grants%20users%20control%20over%20the%20edit%27s%20entire%20temporal%0Aevolution%2C%20allowing%20complex%20transformations%20like%20an%20object%20rotating%20or%20a%20flower%0Ablooming.%20Experimental%20results%20show%20our%20method%20achieves%20superior%20video%20editing%0Aperformance%20compared%20to%20baseline%20methods.%20Project%20Page%3A%0Ahttps%3A//cjeen.github.io/LoRAEdit%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10082v5&entry.124074799=Read"},
{"title": "AutoComPose: Automatic Generation of Pose Transition Descriptions for\n  Composed Pose Retrieval Using Multimodal LLMs", "author": "Yi-Ting Shen and Sungmin Eum and Doheon Lee and Rohit Shete and Chiao-Yi Wang and Heesung Kwon and Shuvra S. Bhattacharyya", "abstract": "  Composed pose retrieval (CPR) enables users to search for human poses by\nspecifying a reference pose and a transition description, but progress in this\nfield is hindered by the scarcity and inconsistency of annotated pose\ntransitions. Existing CPR datasets rely on costly human annotations or\nheuristic-based rule generation, both of which limit scalability and diversity.\nIn this work, we introduce AutoComPose, the first framework that leverages\nmultimodal large language models (MLLMs) to automatically generate rich and\nstructured pose transition descriptions. Our method enhances annotation quality\nby structuring transitions into fine-grained body part movements and\nintroducing mirrored/swapped variations, while a cyclic consistency constraint\nensures logical coherence between forward and reverse transitions. To advance\nCPR research, we construct and release two dedicated benchmarks, AIST-CPR and\nPoseFixCPR, supplementing prior datasets with enhanced attributes. Extensive\nexperiments demonstrate that training retrieval models with AutoComPose yields\nsuperior performance over human-annotated and heuristic-based methods,\nsignificantly reducing annotation costs while improving retrieval quality. Our\nwork pioneers the automatic annotation of pose transitions, establishing a\nscalable foundation for future CPR research.\n", "link": "http://arxiv.org/abs/2503.22884v2", "date": "2025-08-19", "relevancy": 2.2574, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5683}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5634}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoComPose%3A%20Automatic%20Generation%20of%20Pose%20Transition%20Descriptions%20for%0A%20%20Composed%20Pose%20Retrieval%20Using%20Multimodal%20LLMs&body=Title%3A%20AutoComPose%3A%20Automatic%20Generation%20of%20Pose%20Transition%20Descriptions%20for%0A%20%20Composed%20Pose%20Retrieval%20Using%20Multimodal%20LLMs%0AAuthor%3A%20Yi-Ting%20Shen%20and%20Sungmin%20Eum%20and%20Doheon%20Lee%20and%20Rohit%20Shete%20and%20Chiao-Yi%20Wang%20and%20Heesung%20Kwon%20and%20Shuvra%20S.%20Bhattacharyya%0AAbstract%3A%20%20%20Composed%20pose%20retrieval%20%28CPR%29%20enables%20users%20to%20search%20for%20human%20poses%20by%0Aspecifying%20a%20reference%20pose%20and%20a%20transition%20description%2C%20but%20progress%20in%20this%0Afield%20is%20hindered%20by%20the%20scarcity%20and%20inconsistency%20of%20annotated%20pose%0Atransitions.%20Existing%20CPR%20datasets%20rely%20on%20costly%20human%20annotations%20or%0Aheuristic-based%20rule%20generation%2C%20both%20of%20which%20limit%20scalability%20and%20diversity.%0AIn%20this%20work%2C%20we%20introduce%20AutoComPose%2C%20the%20first%20framework%20that%20leverages%0Amultimodal%20large%20language%20models%20%28MLLMs%29%20to%20automatically%20generate%20rich%20and%0Astructured%20pose%20transition%20descriptions.%20Our%20method%20enhances%20annotation%20quality%0Aby%20structuring%20transitions%20into%20fine-grained%20body%20part%20movements%20and%0Aintroducing%20mirrored/swapped%20variations%2C%20while%20a%20cyclic%20consistency%20constraint%0Aensures%20logical%20coherence%20between%20forward%20and%20reverse%20transitions.%20To%20advance%0ACPR%20research%2C%20we%20construct%20and%20release%20two%20dedicated%20benchmarks%2C%20AIST-CPR%20and%0APoseFixCPR%2C%20supplementing%20prior%20datasets%20with%20enhanced%20attributes.%20Extensive%0Aexperiments%20demonstrate%20that%20training%20retrieval%20models%20with%20AutoComPose%20yields%0Asuperior%20performance%20over%20human-annotated%20and%20heuristic-based%20methods%2C%0Asignificantly%20reducing%20annotation%20costs%20while%20improving%20retrieval%20quality.%20Our%0Awork%20pioneers%20the%20automatic%20annotation%20of%20pose%20transitions%2C%20establishing%20a%0Ascalable%20foundation%20for%20future%20CPR%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.22884v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoComPose%253A%2520Automatic%2520Generation%2520of%2520Pose%2520Transition%2520Descriptions%2520for%250A%2520%2520Composed%2520Pose%2520Retrieval%2520Using%2520Multimodal%2520LLMs%26entry.906535625%3DYi-Ting%2520Shen%2520and%2520Sungmin%2520Eum%2520and%2520Doheon%2520Lee%2520and%2520Rohit%2520Shete%2520and%2520Chiao-Yi%2520Wang%2520and%2520Heesung%2520Kwon%2520and%2520Shuvra%2520S.%2520Bhattacharyya%26entry.1292438233%3D%2520%2520Composed%2520pose%2520retrieval%2520%2528CPR%2529%2520enables%2520users%2520to%2520search%2520for%2520human%2520poses%2520by%250Aspecifying%2520a%2520reference%2520pose%2520and%2520a%2520transition%2520description%252C%2520but%2520progress%2520in%2520this%250Afield%2520is%2520hindered%2520by%2520the%2520scarcity%2520and%2520inconsistency%2520of%2520annotated%2520pose%250Atransitions.%2520Existing%2520CPR%2520datasets%2520rely%2520on%2520costly%2520human%2520annotations%2520or%250Aheuristic-based%2520rule%2520generation%252C%2520both%2520of%2520which%2520limit%2520scalability%2520and%2520diversity.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520AutoComPose%252C%2520the%2520first%2520framework%2520that%2520leverages%250Amultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520to%2520automatically%2520generate%2520rich%2520and%250Astructured%2520pose%2520transition%2520descriptions.%2520Our%2520method%2520enhances%2520annotation%2520quality%250Aby%2520structuring%2520transitions%2520into%2520fine-grained%2520body%2520part%2520movements%2520and%250Aintroducing%2520mirrored/swapped%2520variations%252C%2520while%2520a%2520cyclic%2520consistency%2520constraint%250Aensures%2520logical%2520coherence%2520between%2520forward%2520and%2520reverse%2520transitions.%2520To%2520advance%250ACPR%2520research%252C%2520we%2520construct%2520and%2520release%2520two%2520dedicated%2520benchmarks%252C%2520AIST-CPR%2520and%250APoseFixCPR%252C%2520supplementing%2520prior%2520datasets%2520with%2520enhanced%2520attributes.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520training%2520retrieval%2520models%2520with%2520AutoComPose%2520yields%250Asuperior%2520performance%2520over%2520human-annotated%2520and%2520heuristic-based%2520methods%252C%250Asignificantly%2520reducing%2520annotation%2520costs%2520while%2520improving%2520retrieval%2520quality.%2520Our%250Awork%2520pioneers%2520the%2520automatic%2520annotation%2520of%2520pose%2520transitions%252C%2520establishing%2520a%250Ascalable%2520foundation%2520for%2520future%2520CPR%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22884v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoComPose%3A%20Automatic%20Generation%20of%20Pose%20Transition%20Descriptions%20for%0A%20%20Composed%20Pose%20Retrieval%20Using%20Multimodal%20LLMs&entry.906535625=Yi-Ting%20Shen%20and%20Sungmin%20Eum%20and%20Doheon%20Lee%20and%20Rohit%20Shete%20and%20Chiao-Yi%20Wang%20and%20Heesung%20Kwon%20and%20Shuvra%20S.%20Bhattacharyya&entry.1292438233=%20%20Composed%20pose%20retrieval%20%28CPR%29%20enables%20users%20to%20search%20for%20human%20poses%20by%0Aspecifying%20a%20reference%20pose%20and%20a%20transition%20description%2C%20but%20progress%20in%20this%0Afield%20is%20hindered%20by%20the%20scarcity%20and%20inconsistency%20of%20annotated%20pose%0Atransitions.%20Existing%20CPR%20datasets%20rely%20on%20costly%20human%20annotations%20or%0Aheuristic-based%20rule%20generation%2C%20both%20of%20which%20limit%20scalability%20and%20diversity.%0AIn%20this%20work%2C%20we%20introduce%20AutoComPose%2C%20the%20first%20framework%20that%20leverages%0Amultimodal%20large%20language%20models%20%28MLLMs%29%20to%20automatically%20generate%20rich%20and%0Astructured%20pose%20transition%20descriptions.%20Our%20method%20enhances%20annotation%20quality%0Aby%20structuring%20transitions%20into%20fine-grained%20body%20part%20movements%20and%0Aintroducing%20mirrored/swapped%20variations%2C%20while%20a%20cyclic%20consistency%20constraint%0Aensures%20logical%20coherence%20between%20forward%20and%20reverse%20transitions.%20To%20advance%0ACPR%20research%2C%20we%20construct%20and%20release%20two%20dedicated%20benchmarks%2C%20AIST-CPR%20and%0APoseFixCPR%2C%20supplementing%20prior%20datasets%20with%20enhanced%20attributes.%20Extensive%0Aexperiments%20demonstrate%20that%20training%20retrieval%20models%20with%20AutoComPose%20yields%0Asuperior%20performance%20over%20human-annotated%20and%20heuristic-based%20methods%2C%0Asignificantly%20reducing%20annotation%20costs%20while%20improving%20retrieval%20quality.%20Our%0Awork%20pioneers%20the%20automatic%20annotation%20of%20pose%20transitions%2C%20establishing%20a%0Ascalable%20foundation%20for%20future%20CPR%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.22884v2&entry.124074799=Read"},
{"title": "Deep Biomechanically-Guided Interpolation for Keypoint-Based Brain Shift\n  Registration", "author": "Tiago Assis and Ines P. Machado and Benjamin Zwick and Nuno C. Garcia and Reuben Dorent", "abstract": "  Accurate compensation of brain shift is critical for maintaining the\nreliability of neuronavigation during neurosurgery. While keypoint-based\nregistration methods offer robustness to large deformations and topological\nchanges, they typically rely on simple geometric interpolators that ignore\ntissue biomechanics to create dense displacement fields. In this work, we\npropose a novel deep learning framework that estimates dense, physically\nplausible brain deformations from sparse matched keypoints. We first generate a\nlarge dataset of synthetic brain deformations using biomechanical simulations.\nThen, a residual 3D U-Net is trained to refine standard interpolation estimates\ninto biomechanically guided deformations. Experiments on a large set of\nsimulated displacement fields demonstrate that our method significantly\noutperforms classical interpolators, reducing by half the mean square error\nwhile introducing negligible computational overhead at inference time. Code\navailable at:\n\\href{https://github.com/tiago-assis/Deep-Biomechanical-Interpolator}{https://github.com/tiago-assis/Deep-Biomechanical-Interpolator}.\n", "link": "http://arxiv.org/abs/2508.13762v1", "date": "2025-08-19", "relevancy": 2.2521, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5666}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5651}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Biomechanically-Guided%20Interpolation%20for%20Keypoint-Based%20Brain%20Shift%0A%20%20Registration&body=Title%3A%20Deep%20Biomechanically-Guided%20Interpolation%20for%20Keypoint-Based%20Brain%20Shift%0A%20%20Registration%0AAuthor%3A%20Tiago%20Assis%20and%20Ines%20P.%20Machado%20and%20Benjamin%20Zwick%20and%20Nuno%20C.%20Garcia%20and%20Reuben%20Dorent%0AAbstract%3A%20%20%20Accurate%20compensation%20of%20brain%20shift%20is%20critical%20for%20maintaining%20the%0Areliability%20of%20neuronavigation%20during%20neurosurgery.%20While%20keypoint-based%0Aregistration%20methods%20offer%20robustness%20to%20large%20deformations%20and%20topological%0Achanges%2C%20they%20typically%20rely%20on%20simple%20geometric%20interpolators%20that%20ignore%0Atissue%20biomechanics%20to%20create%20dense%20displacement%20fields.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20deep%20learning%20framework%20that%20estimates%20dense%2C%20physically%0Aplausible%20brain%20deformations%20from%20sparse%20matched%20keypoints.%20We%20first%20generate%20a%0Alarge%20dataset%20of%20synthetic%20brain%20deformations%20using%20biomechanical%20simulations.%0AThen%2C%20a%20residual%203D%20U-Net%20is%20trained%20to%20refine%20standard%20interpolation%20estimates%0Ainto%20biomechanically%20guided%20deformations.%20Experiments%20on%20a%20large%20set%20of%0Asimulated%20displacement%20fields%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20classical%20interpolators%2C%20reducing%20by%20half%20the%20mean%20square%20error%0Awhile%20introducing%20negligible%20computational%20overhead%20at%20inference%20time.%20Code%0Aavailable%20at%3A%0A%5Chref%7Bhttps%3A//github.com/tiago-assis/Deep-Biomechanical-Interpolator%7D%7Bhttps%3A//github.com/tiago-assis/Deep-Biomechanical-Interpolator%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Biomechanically-Guided%2520Interpolation%2520for%2520Keypoint-Based%2520Brain%2520Shift%250A%2520%2520Registration%26entry.906535625%3DTiago%2520Assis%2520and%2520Ines%2520P.%2520Machado%2520and%2520Benjamin%2520Zwick%2520and%2520Nuno%2520C.%2520Garcia%2520and%2520Reuben%2520Dorent%26entry.1292438233%3D%2520%2520Accurate%2520compensation%2520of%2520brain%2520shift%2520is%2520critical%2520for%2520maintaining%2520the%250Areliability%2520of%2520neuronavigation%2520during%2520neurosurgery.%2520While%2520keypoint-based%250Aregistration%2520methods%2520offer%2520robustness%2520to%2520large%2520deformations%2520and%2520topological%250Achanges%252C%2520they%2520typically%2520rely%2520on%2520simple%2520geometric%2520interpolators%2520that%2520ignore%250Atissue%2520biomechanics%2520to%2520create%2520dense%2520displacement%2520fields.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520deep%2520learning%2520framework%2520that%2520estimates%2520dense%252C%2520physically%250Aplausible%2520brain%2520deformations%2520from%2520sparse%2520matched%2520keypoints.%2520We%2520first%2520generate%2520a%250Alarge%2520dataset%2520of%2520synthetic%2520brain%2520deformations%2520using%2520biomechanical%2520simulations.%250AThen%252C%2520a%2520residual%25203D%2520U-Net%2520is%2520trained%2520to%2520refine%2520standard%2520interpolation%2520estimates%250Ainto%2520biomechanically%2520guided%2520deformations.%2520Experiments%2520on%2520a%2520large%2520set%2520of%250Asimulated%2520displacement%2520fields%2520demonstrate%2520that%2520our%2520method%2520significantly%250Aoutperforms%2520classical%2520interpolators%252C%2520reducing%2520by%2520half%2520the%2520mean%2520square%2520error%250Awhile%2520introducing%2520negligible%2520computational%2520overhead%2520at%2520inference%2520time.%2520Code%250Aavailable%2520at%253A%250A%255Chref%257Bhttps%253A//github.com/tiago-assis/Deep-Biomechanical-Interpolator%257D%257Bhttps%253A//github.com/tiago-assis/Deep-Biomechanical-Interpolator%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Biomechanically-Guided%20Interpolation%20for%20Keypoint-Based%20Brain%20Shift%0A%20%20Registration&entry.906535625=Tiago%20Assis%20and%20Ines%20P.%20Machado%20and%20Benjamin%20Zwick%20and%20Nuno%20C.%20Garcia%20and%20Reuben%20Dorent&entry.1292438233=%20%20Accurate%20compensation%20of%20brain%20shift%20is%20critical%20for%20maintaining%20the%0Areliability%20of%20neuronavigation%20during%20neurosurgery.%20While%20keypoint-based%0Aregistration%20methods%20offer%20robustness%20to%20large%20deformations%20and%20topological%0Achanges%2C%20they%20typically%20rely%20on%20simple%20geometric%20interpolators%20that%20ignore%0Atissue%20biomechanics%20to%20create%20dense%20displacement%20fields.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20deep%20learning%20framework%20that%20estimates%20dense%2C%20physically%0Aplausible%20brain%20deformations%20from%20sparse%20matched%20keypoints.%20We%20first%20generate%20a%0Alarge%20dataset%20of%20synthetic%20brain%20deformations%20using%20biomechanical%20simulations.%0AThen%2C%20a%20residual%203D%20U-Net%20is%20trained%20to%20refine%20standard%20interpolation%20estimates%0Ainto%20biomechanically%20guided%20deformations.%20Experiments%20on%20a%20large%20set%20of%0Asimulated%20displacement%20fields%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20classical%20interpolators%2C%20reducing%20by%20half%20the%20mean%20square%20error%0Awhile%20introducing%20negligible%20computational%20overhead%20at%20inference%20time.%20Code%0Aavailable%20at%3A%0A%5Chref%7Bhttps%3A//github.com/tiago-assis/Deep-Biomechanical-Interpolator%7D%7Bhttps%3A//github.com/tiago-assis/Deep-Biomechanical-Interpolator%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13762v1&entry.124074799=Read"},
{"title": "DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided\n  Diffusion Transformer", "author": "Yisu Liu and Chenxing Li and Wanqian Zhang and Wenfu Wang and Meng Yu and Ruibo Fu and Zheng Lin and Weiping Wang and Dong Yu", "abstract": "  Controllable text-to-audio generation aims to synthesize audio from textual\ndescriptions while satisfying user-specified constraints, including event\ntypes, temporal sequences, and onset and offset timestamps. This enables\nprecise control over both the content and temporal structure of the generated\naudio. Despite recent progress, existing methods still face inherent trade-offs\namong accurate temporal localization, open-vocabulary scalability, and\npractical efficiency. To address these challenges, we propose DegDiT, a novel\ndynamic event graph-guided diffusion transformer framework for open-vocabulary\ncontrollable audio generation. DegDiT encodes the events in the description as\nstructured dynamic graphs. The nodes in each graph are designed to represent\nthree aspects: semantic features, temporal attributes, and inter-event\nconnections. A graph transformer is employed to integrate these nodes and\nproduce contextualized event embeddings that serve as guidance for the\ndiffusion model. To ensure high-quality and diverse training data, we introduce\na quality-balanced data selection pipeline that combines hierarchical event\nannotation with multi-criteria quality scoring, resulting in a curated dataset\nwith semantic diversity. Furthermore, we present consensus preference\noptimization, facilitating audio generation through consensus among multiple\nreward signals. Extensive experiments on AudioCondition, DESED, and AudioTime\ndatasets demonstrate that DegDiT achieves state-of-the-art performances across\na variety of objective and subjective evaluation metrics.\n", "link": "http://arxiv.org/abs/2508.13786v1", "date": "2025-08-19", "relevancy": 2.2492, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6072}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5603}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DegDiT%3A%20Controllable%20Audio%20Generation%20with%20Dynamic%20Event%20Graph%20Guided%0A%20%20Diffusion%20Transformer&body=Title%3A%20DegDiT%3A%20Controllable%20Audio%20Generation%20with%20Dynamic%20Event%20Graph%20Guided%0A%20%20Diffusion%20Transformer%0AAuthor%3A%20Yisu%20Liu%20and%20Chenxing%20Li%20and%20Wanqian%20Zhang%20and%20Wenfu%20Wang%20and%20Meng%20Yu%20and%20Ruibo%20Fu%20and%20Zheng%20Lin%20and%20Weiping%20Wang%20and%20Dong%20Yu%0AAbstract%3A%20%20%20Controllable%20text-to-audio%20generation%20aims%20to%20synthesize%20audio%20from%20textual%0Adescriptions%20while%20satisfying%20user-specified%20constraints%2C%20including%20event%0Atypes%2C%20temporal%20sequences%2C%20and%20onset%20and%20offset%20timestamps.%20This%20enables%0Aprecise%20control%20over%20both%20the%20content%20and%20temporal%20structure%20of%20the%20generated%0Aaudio.%20Despite%20recent%20progress%2C%20existing%20methods%20still%20face%20inherent%20trade-offs%0Aamong%20accurate%20temporal%20localization%2C%20open-vocabulary%20scalability%2C%20and%0Apractical%20efficiency.%20To%20address%20these%20challenges%2C%20we%20propose%20DegDiT%2C%20a%20novel%0Adynamic%20event%20graph-guided%20diffusion%20transformer%20framework%20for%20open-vocabulary%0Acontrollable%20audio%20generation.%20DegDiT%20encodes%20the%20events%20in%20the%20description%20as%0Astructured%20dynamic%20graphs.%20The%20nodes%20in%20each%20graph%20are%20designed%20to%20represent%0Athree%20aspects%3A%20semantic%20features%2C%20temporal%20attributes%2C%20and%20inter-event%0Aconnections.%20A%20graph%20transformer%20is%20employed%20to%20integrate%20these%20nodes%20and%0Aproduce%20contextualized%20event%20embeddings%20that%20serve%20as%20guidance%20for%20the%0Adiffusion%20model.%20To%20ensure%20high-quality%20and%20diverse%20training%20data%2C%20we%20introduce%0Aa%20quality-balanced%20data%20selection%20pipeline%20that%20combines%20hierarchical%20event%0Aannotation%20with%20multi-criteria%20quality%20scoring%2C%20resulting%20in%20a%20curated%20dataset%0Awith%20semantic%20diversity.%20Furthermore%2C%20we%20present%20consensus%20preference%0Aoptimization%2C%20facilitating%20audio%20generation%20through%20consensus%20among%20multiple%0Areward%20signals.%20Extensive%20experiments%20on%20AudioCondition%2C%20DESED%2C%20and%20AudioTime%0Adatasets%20demonstrate%20that%20DegDiT%20achieves%20state-of-the-art%20performances%20across%0Aa%20variety%20of%20objective%20and%20subjective%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDegDiT%253A%2520Controllable%2520Audio%2520Generation%2520with%2520Dynamic%2520Event%2520Graph%2520Guided%250A%2520%2520Diffusion%2520Transformer%26entry.906535625%3DYisu%2520Liu%2520and%2520Chenxing%2520Li%2520and%2520Wanqian%2520Zhang%2520and%2520Wenfu%2520Wang%2520and%2520Meng%2520Yu%2520and%2520Ruibo%2520Fu%2520and%2520Zheng%2520Lin%2520and%2520Weiping%2520Wang%2520and%2520Dong%2520Yu%26entry.1292438233%3D%2520%2520Controllable%2520text-to-audio%2520generation%2520aims%2520to%2520synthesize%2520audio%2520from%2520textual%250Adescriptions%2520while%2520satisfying%2520user-specified%2520constraints%252C%2520including%2520event%250Atypes%252C%2520temporal%2520sequences%252C%2520and%2520onset%2520and%2520offset%2520timestamps.%2520This%2520enables%250Aprecise%2520control%2520over%2520both%2520the%2520content%2520and%2520temporal%2520structure%2520of%2520the%2520generated%250Aaudio.%2520Despite%2520recent%2520progress%252C%2520existing%2520methods%2520still%2520face%2520inherent%2520trade-offs%250Aamong%2520accurate%2520temporal%2520localization%252C%2520open-vocabulary%2520scalability%252C%2520and%250Apractical%2520efficiency.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520DegDiT%252C%2520a%2520novel%250Adynamic%2520event%2520graph-guided%2520diffusion%2520transformer%2520framework%2520for%2520open-vocabulary%250Acontrollable%2520audio%2520generation.%2520DegDiT%2520encodes%2520the%2520events%2520in%2520the%2520description%2520as%250Astructured%2520dynamic%2520graphs.%2520The%2520nodes%2520in%2520each%2520graph%2520are%2520designed%2520to%2520represent%250Athree%2520aspects%253A%2520semantic%2520features%252C%2520temporal%2520attributes%252C%2520and%2520inter-event%250Aconnections.%2520A%2520graph%2520transformer%2520is%2520employed%2520to%2520integrate%2520these%2520nodes%2520and%250Aproduce%2520contextualized%2520event%2520embeddings%2520that%2520serve%2520as%2520guidance%2520for%2520the%250Adiffusion%2520model.%2520To%2520ensure%2520high-quality%2520and%2520diverse%2520training%2520data%252C%2520we%2520introduce%250Aa%2520quality-balanced%2520data%2520selection%2520pipeline%2520that%2520combines%2520hierarchical%2520event%250Aannotation%2520with%2520multi-criteria%2520quality%2520scoring%252C%2520resulting%2520in%2520a%2520curated%2520dataset%250Awith%2520semantic%2520diversity.%2520Furthermore%252C%2520we%2520present%2520consensus%2520preference%250Aoptimization%252C%2520facilitating%2520audio%2520generation%2520through%2520consensus%2520among%2520multiple%250Areward%2520signals.%2520Extensive%2520experiments%2520on%2520AudioCondition%252C%2520DESED%252C%2520and%2520AudioTime%250Adatasets%2520demonstrate%2520that%2520DegDiT%2520achieves%2520state-of-the-art%2520performances%2520across%250Aa%2520variety%2520of%2520objective%2520and%2520subjective%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DegDiT%3A%20Controllable%20Audio%20Generation%20with%20Dynamic%20Event%20Graph%20Guided%0A%20%20Diffusion%20Transformer&entry.906535625=Yisu%20Liu%20and%20Chenxing%20Li%20and%20Wanqian%20Zhang%20and%20Wenfu%20Wang%20and%20Meng%20Yu%20and%20Ruibo%20Fu%20and%20Zheng%20Lin%20and%20Weiping%20Wang%20and%20Dong%20Yu&entry.1292438233=%20%20Controllable%20text-to-audio%20generation%20aims%20to%20synthesize%20audio%20from%20textual%0Adescriptions%20while%20satisfying%20user-specified%20constraints%2C%20including%20event%0Atypes%2C%20temporal%20sequences%2C%20and%20onset%20and%20offset%20timestamps.%20This%20enables%0Aprecise%20control%20over%20both%20the%20content%20and%20temporal%20structure%20of%20the%20generated%0Aaudio.%20Despite%20recent%20progress%2C%20existing%20methods%20still%20face%20inherent%20trade-offs%0Aamong%20accurate%20temporal%20localization%2C%20open-vocabulary%20scalability%2C%20and%0Apractical%20efficiency.%20To%20address%20these%20challenges%2C%20we%20propose%20DegDiT%2C%20a%20novel%0Adynamic%20event%20graph-guided%20diffusion%20transformer%20framework%20for%20open-vocabulary%0Acontrollable%20audio%20generation.%20DegDiT%20encodes%20the%20events%20in%20the%20description%20as%0Astructured%20dynamic%20graphs.%20The%20nodes%20in%20each%20graph%20are%20designed%20to%20represent%0Athree%20aspects%3A%20semantic%20features%2C%20temporal%20attributes%2C%20and%20inter-event%0Aconnections.%20A%20graph%20transformer%20is%20employed%20to%20integrate%20these%20nodes%20and%0Aproduce%20contextualized%20event%20embeddings%20that%20serve%20as%20guidance%20for%20the%0Adiffusion%20model.%20To%20ensure%20high-quality%20and%20diverse%20training%20data%2C%20we%20introduce%0Aa%20quality-balanced%20data%20selection%20pipeline%20that%20combines%20hierarchical%20event%0Aannotation%20with%20multi-criteria%20quality%20scoring%2C%20resulting%20in%20a%20curated%20dataset%0Awith%20semantic%20diversity.%20Furthermore%2C%20we%20present%20consensus%20preference%0Aoptimization%2C%20facilitating%20audio%20generation%20through%20consensus%20among%20multiple%0Areward%20signals.%20Extensive%20experiments%20on%20AudioCondition%2C%20DESED%2C%20and%20AudioTime%0Adatasets%20demonstrate%20that%20DegDiT%20achieves%20state-of-the-art%20performances%20across%0Aa%20variety%20of%20objective%20and%20subjective%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13786v1&entry.124074799=Read"},
{"title": "Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object\n  Manipulation", "author": "Zhuoling Li and Xiaoyang Wu and Zhenhua Xu and Hengshuang Zhao", "abstract": "  Realizing generalizable dynamic object manipulation is important for\nenhancing manufacturing efficiency, as it eliminates specialized engineering\nfor various scenarios. To this end, imitation learning emerges as a promising\nparadigm, leveraging expert demonstrations to teach a policy manipulation\nskills. Although the generalization of an imitation learning policy can be\nimproved by increasing demonstrations, demonstration collection is\nlabor-intensive. To address this problem, this paper investigates whether\nstrong generalization in dynamic object manipulation is achievable with only a\nfew demonstrations. Specifically, we develop an entropy-based theoretical\nframework to quantify the optimization of imitation learning. Based on this\nframework, we propose a system named Generalizable Entropy-based Manipulation\n(GEM). Extensive experiments in simulated and real tasks demonstrate that GEM\ncan generalize across diverse environment backgrounds, robot embodiments,\nmotion dynamics, and object geometries. Notably, GEM has been deployed in a\nreal canteen for tableware collection. Without any in-scene demonstration, it\nachieves a success rate of over 97% across more than 10,000 operations.\n", "link": "http://arxiv.org/abs/2508.14042v1", "date": "2025-08-19", "relevancy": 2.2453, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5903}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5592}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Train%20Once%2C%20Deploy%20Anywhere%3A%20Realize%20Data-Efficient%20Dynamic%20Object%0A%20%20Manipulation&body=Title%3A%20Train%20Once%2C%20Deploy%20Anywhere%3A%20Realize%20Data-Efficient%20Dynamic%20Object%0A%20%20Manipulation%0AAuthor%3A%20Zhuoling%20Li%20and%20Xiaoyang%20Wu%20and%20Zhenhua%20Xu%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20Realizing%20generalizable%20dynamic%20object%20manipulation%20is%20important%20for%0Aenhancing%20manufacturing%20efficiency%2C%20as%20it%20eliminates%20specialized%20engineering%0Afor%20various%20scenarios.%20To%20this%20end%2C%20imitation%20learning%20emerges%20as%20a%20promising%0Aparadigm%2C%20leveraging%20expert%20demonstrations%20to%20teach%20a%20policy%20manipulation%0Askills.%20Although%20the%20generalization%20of%20an%20imitation%20learning%20policy%20can%20be%0Aimproved%20by%20increasing%20demonstrations%2C%20demonstration%20collection%20is%0Alabor-intensive.%20To%20address%20this%20problem%2C%20this%20paper%20investigates%20whether%0Astrong%20generalization%20in%20dynamic%20object%20manipulation%20is%20achievable%20with%20only%20a%0Afew%20demonstrations.%20Specifically%2C%20we%20develop%20an%20entropy-based%20theoretical%0Aframework%20to%20quantify%20the%20optimization%20of%20imitation%20learning.%20Based%20on%20this%0Aframework%2C%20we%20propose%20a%20system%20named%20Generalizable%20Entropy-based%20Manipulation%0A%28GEM%29.%20Extensive%20experiments%20in%20simulated%20and%20real%20tasks%20demonstrate%20that%20GEM%0Acan%20generalize%20across%20diverse%20environment%20backgrounds%2C%20robot%20embodiments%2C%0Amotion%20dynamics%2C%20and%20object%20geometries.%20Notably%2C%20GEM%20has%20been%20deployed%20in%20a%0Areal%20canteen%20for%20tableware%20collection.%20Without%20any%20in-scene%20demonstration%2C%20it%0Aachieves%20a%20success%20rate%20of%20over%2097%25%20across%20more%20than%2010%2C000%20operations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrain%2520Once%252C%2520Deploy%2520Anywhere%253A%2520Realize%2520Data-Efficient%2520Dynamic%2520Object%250A%2520%2520Manipulation%26entry.906535625%3DZhuoling%2520Li%2520and%2520Xiaoyang%2520Wu%2520and%2520Zhenhua%2520Xu%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520Realizing%2520generalizable%2520dynamic%2520object%2520manipulation%2520is%2520important%2520for%250Aenhancing%2520manufacturing%2520efficiency%252C%2520as%2520it%2520eliminates%2520specialized%2520engineering%250Afor%2520various%2520scenarios.%2520To%2520this%2520end%252C%2520imitation%2520learning%2520emerges%2520as%2520a%2520promising%250Aparadigm%252C%2520leveraging%2520expert%2520demonstrations%2520to%2520teach%2520a%2520policy%2520manipulation%250Askills.%2520Although%2520the%2520generalization%2520of%2520an%2520imitation%2520learning%2520policy%2520can%2520be%250Aimproved%2520by%2520increasing%2520demonstrations%252C%2520demonstration%2520collection%2520is%250Alabor-intensive.%2520To%2520address%2520this%2520problem%252C%2520this%2520paper%2520investigates%2520whether%250Astrong%2520generalization%2520in%2520dynamic%2520object%2520manipulation%2520is%2520achievable%2520with%2520only%2520a%250Afew%2520demonstrations.%2520Specifically%252C%2520we%2520develop%2520an%2520entropy-based%2520theoretical%250Aframework%2520to%2520quantify%2520the%2520optimization%2520of%2520imitation%2520learning.%2520Based%2520on%2520this%250Aframework%252C%2520we%2520propose%2520a%2520system%2520named%2520Generalizable%2520Entropy-based%2520Manipulation%250A%2528GEM%2529.%2520Extensive%2520experiments%2520in%2520simulated%2520and%2520real%2520tasks%2520demonstrate%2520that%2520GEM%250Acan%2520generalize%2520across%2520diverse%2520environment%2520backgrounds%252C%2520robot%2520embodiments%252C%250Amotion%2520dynamics%252C%2520and%2520object%2520geometries.%2520Notably%252C%2520GEM%2520has%2520been%2520deployed%2520in%2520a%250Areal%2520canteen%2520for%2520tableware%2520collection.%2520Without%2520any%2520in-scene%2520demonstration%252C%2520it%250Aachieves%2520a%2520success%2520rate%2520of%2520over%252097%2525%2520across%2520more%2520than%252010%252C000%2520operations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Train%20Once%2C%20Deploy%20Anywhere%3A%20Realize%20Data-Efficient%20Dynamic%20Object%0A%20%20Manipulation&entry.906535625=Zhuoling%20Li%20and%20Xiaoyang%20Wu%20and%20Zhenhua%20Xu%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20Realizing%20generalizable%20dynamic%20object%20manipulation%20is%20important%20for%0Aenhancing%20manufacturing%20efficiency%2C%20as%20it%20eliminates%20specialized%20engineering%0Afor%20various%20scenarios.%20To%20this%20end%2C%20imitation%20learning%20emerges%20as%20a%20promising%0Aparadigm%2C%20leveraging%20expert%20demonstrations%20to%20teach%20a%20policy%20manipulation%0Askills.%20Although%20the%20generalization%20of%20an%20imitation%20learning%20policy%20can%20be%0Aimproved%20by%20increasing%20demonstrations%2C%20demonstration%20collection%20is%0Alabor-intensive.%20To%20address%20this%20problem%2C%20this%20paper%20investigates%20whether%0Astrong%20generalization%20in%20dynamic%20object%20manipulation%20is%20achievable%20with%20only%20a%0Afew%20demonstrations.%20Specifically%2C%20we%20develop%20an%20entropy-based%20theoretical%0Aframework%20to%20quantify%20the%20optimization%20of%20imitation%20learning.%20Based%20on%20this%0Aframework%2C%20we%20propose%20a%20system%20named%20Generalizable%20Entropy-based%20Manipulation%0A%28GEM%29.%20Extensive%20experiments%20in%20simulated%20and%20real%20tasks%20demonstrate%20that%20GEM%0Acan%20generalize%20across%20diverse%20environment%20backgrounds%2C%20robot%20embodiments%2C%0Amotion%20dynamics%2C%20and%20object%20geometries.%20Notably%2C%20GEM%20has%20been%20deployed%20in%20a%0Areal%20canteen%20for%20tableware%20collection.%20Without%20any%20in-scene%20demonstration%2C%20it%0Aachieves%20a%20success%20rate%20of%20over%2097%25%20across%20more%20than%2010%2C000%20operations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14042v1&entry.124074799=Read"},
{"title": "SlotMatch: Distilling Temporally Consistent Object-Centric\n  Representations for Unsupervised Video Segmentation", "author": "Diana-Nicoleta Grigore and Neelu Madan and Andreas Mogelmose and Thomas B. Moeslund and Radu Tudor Ionescu", "abstract": "  Unsupervised video segmentation is a challenging computer vision task,\nespecially due to the lack of supervisory signals coupled with the complexity\nof visual scenes. To overcome this challenge, state-of-the-art models based on\nslot attention often have to rely on large and computationally expensive neural\narchitectures. To this end, we propose a simple knowledge distillation\nframework that effectively transfers object-centric representations to a\nlightweight student. The proposed framework, called SlotMatch, aligns\ncorresponding teacher and student slots via the cosine similarity, requiring no\nadditional distillation objectives or auxiliary supervision. The simplicity of\nSlotMatch is confirmed via theoretical and empirical evidence, both indicating\nthat integrating additional losses is redundant. We conduct experiments on two\ndatasets to compare the state-of-the-art teacher model, SlotContrast, with our\ndistilled student. The results show that our student based on SlotMatch matches\nand even outperforms its teacher, while using 3.6x less parameters and running\n1.9x faster. Moreover, our student surpasses previous unsupervised video\nsegmentation models.\n", "link": "http://arxiv.org/abs/2508.03411v2", "date": "2025-08-19", "relevancy": 2.2393, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5747}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5536}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlotMatch%3A%20Distilling%20Temporally%20Consistent%20Object-Centric%0A%20%20Representations%20for%20Unsupervised%20Video%20Segmentation&body=Title%3A%20SlotMatch%3A%20Distilling%20Temporally%20Consistent%20Object-Centric%0A%20%20Representations%20for%20Unsupervised%20Video%20Segmentation%0AAuthor%3A%20Diana-Nicoleta%20Grigore%20and%20Neelu%20Madan%20and%20Andreas%20Mogelmose%20and%20Thomas%20B.%20Moeslund%20and%20Radu%20Tudor%20Ionescu%0AAbstract%3A%20%20%20Unsupervised%20video%20segmentation%20is%20a%20challenging%20computer%20vision%20task%2C%0Aespecially%20due%20to%20the%20lack%20of%20supervisory%20signals%20coupled%20with%20the%20complexity%0Aof%20visual%20scenes.%20To%20overcome%20this%20challenge%2C%20state-of-the-art%20models%20based%20on%0Aslot%20attention%20often%20have%20to%20rely%20on%20large%20and%20computationally%20expensive%20neural%0Aarchitectures.%20To%20this%20end%2C%20we%20propose%20a%20simple%20knowledge%20distillation%0Aframework%20that%20effectively%20transfers%20object-centric%20representations%20to%20a%0Alightweight%20student.%20The%20proposed%20framework%2C%20called%20SlotMatch%2C%20aligns%0Acorresponding%20teacher%20and%20student%20slots%20via%20the%20cosine%20similarity%2C%20requiring%20no%0Aadditional%20distillation%20objectives%20or%20auxiliary%20supervision.%20The%20simplicity%20of%0ASlotMatch%20is%20confirmed%20via%20theoretical%20and%20empirical%20evidence%2C%20both%20indicating%0Athat%20integrating%20additional%20losses%20is%20redundant.%20We%20conduct%20experiments%20on%20two%0Adatasets%20to%20compare%20the%20state-of-the-art%20teacher%20model%2C%20SlotContrast%2C%20with%20our%0Adistilled%20student.%20The%20results%20show%20that%20our%20student%20based%20on%20SlotMatch%20matches%0Aand%20even%20outperforms%20its%20teacher%2C%20while%20using%203.6x%20less%20parameters%20and%20running%0A1.9x%20faster.%20Moreover%2C%20our%20student%20surpasses%20previous%20unsupervised%20video%0Asegmentation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03411v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlotMatch%253A%2520Distilling%2520Temporally%2520Consistent%2520Object-Centric%250A%2520%2520Representations%2520for%2520Unsupervised%2520Video%2520Segmentation%26entry.906535625%3DDiana-Nicoleta%2520Grigore%2520and%2520Neelu%2520Madan%2520and%2520Andreas%2520Mogelmose%2520and%2520Thomas%2520B.%2520Moeslund%2520and%2520Radu%2520Tudor%2520Ionescu%26entry.1292438233%3D%2520%2520Unsupervised%2520video%2520segmentation%2520is%2520a%2520challenging%2520computer%2520vision%2520task%252C%250Aespecially%2520due%2520to%2520the%2520lack%2520of%2520supervisory%2520signals%2520coupled%2520with%2520the%2520complexity%250Aof%2520visual%2520scenes.%2520To%2520overcome%2520this%2520challenge%252C%2520state-of-the-art%2520models%2520based%2520on%250Aslot%2520attention%2520often%2520have%2520to%2520rely%2520on%2520large%2520and%2520computationally%2520expensive%2520neural%250Aarchitectures.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520simple%2520knowledge%2520distillation%250Aframework%2520that%2520effectively%2520transfers%2520object-centric%2520representations%2520to%2520a%250Alightweight%2520student.%2520The%2520proposed%2520framework%252C%2520called%2520SlotMatch%252C%2520aligns%250Acorresponding%2520teacher%2520and%2520student%2520slots%2520via%2520the%2520cosine%2520similarity%252C%2520requiring%2520no%250Aadditional%2520distillation%2520objectives%2520or%2520auxiliary%2520supervision.%2520The%2520simplicity%2520of%250ASlotMatch%2520is%2520confirmed%2520via%2520theoretical%2520and%2520empirical%2520evidence%252C%2520both%2520indicating%250Athat%2520integrating%2520additional%2520losses%2520is%2520redundant.%2520We%2520conduct%2520experiments%2520on%2520two%250Adatasets%2520to%2520compare%2520the%2520state-of-the-art%2520teacher%2520model%252C%2520SlotContrast%252C%2520with%2520our%250Adistilled%2520student.%2520The%2520results%2520show%2520that%2520our%2520student%2520based%2520on%2520SlotMatch%2520matches%250Aand%2520even%2520outperforms%2520its%2520teacher%252C%2520while%2520using%25203.6x%2520less%2520parameters%2520and%2520running%250A1.9x%2520faster.%2520Moreover%252C%2520our%2520student%2520surpasses%2520previous%2520unsupervised%2520video%250Asegmentation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03411v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlotMatch%3A%20Distilling%20Temporally%20Consistent%20Object-Centric%0A%20%20Representations%20for%20Unsupervised%20Video%20Segmentation&entry.906535625=Diana-Nicoleta%20Grigore%20and%20Neelu%20Madan%20and%20Andreas%20Mogelmose%20and%20Thomas%20B.%20Moeslund%20and%20Radu%20Tudor%20Ionescu&entry.1292438233=%20%20Unsupervised%20video%20segmentation%20is%20a%20challenging%20computer%20vision%20task%2C%0Aespecially%20due%20to%20the%20lack%20of%20supervisory%20signals%20coupled%20with%20the%20complexity%0Aof%20visual%20scenes.%20To%20overcome%20this%20challenge%2C%20state-of-the-art%20models%20based%20on%0Aslot%20attention%20often%20have%20to%20rely%20on%20large%20and%20computationally%20expensive%20neural%0Aarchitectures.%20To%20this%20end%2C%20we%20propose%20a%20simple%20knowledge%20distillation%0Aframework%20that%20effectively%20transfers%20object-centric%20representations%20to%20a%0Alightweight%20student.%20The%20proposed%20framework%2C%20called%20SlotMatch%2C%20aligns%0Acorresponding%20teacher%20and%20student%20slots%20via%20the%20cosine%20similarity%2C%20requiring%20no%0Aadditional%20distillation%20objectives%20or%20auxiliary%20supervision.%20The%20simplicity%20of%0ASlotMatch%20is%20confirmed%20via%20theoretical%20and%20empirical%20evidence%2C%20both%20indicating%0Athat%20integrating%20additional%20losses%20is%20redundant.%20We%20conduct%20experiments%20on%20two%0Adatasets%20to%20compare%20the%20state-of-the-art%20teacher%20model%2C%20SlotContrast%2C%20with%20our%0Adistilled%20student.%20The%20results%20show%20that%20our%20student%20based%20on%20SlotMatch%20matches%0Aand%20even%20outperforms%20its%20teacher%2C%20while%20using%203.6x%20less%20parameters%20and%20running%0A1.9x%20faster.%20Moreover%2C%20our%20student%20surpasses%20previous%20unsupervised%20video%0Asegmentation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03411v2&entry.124074799=Read"},
{"title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with\n  Adaptive Exploration", "author": "Zhicheng Yang and Zhijiang Guo and Yinya Huang and Yongxin Wang and Dongchun Xie and Yiwei Wang and Xiaodan Liang and Jing Tang", "abstract": "  Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR.\n", "link": "http://arxiv.org/abs/2508.13755v1", "date": "2025-08-19", "relevancy": 2.2387, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5606}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth-Breadth%20Synergy%20in%20RLVR%3A%20Unlocking%20LLM%20Reasoning%20Gains%20with%0A%20%20Adaptive%20Exploration&body=Title%3A%20Depth-Breadth%20Synergy%20in%20RLVR%3A%20Unlocking%20LLM%20Reasoning%20Gains%20with%0A%20%20Adaptive%20Exploration%0AAuthor%3A%20Zhicheng%20Yang%20and%20Zhijiang%20Guo%20and%20Yinya%20Huang%20and%20Yongxin%20Wang%20and%20Dongchun%20Xie%20and%20Yiwei%20Wang%20and%20Xiaodan%20Liang%20and%20Jing%20Tang%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Verifiable%20Reward%20%28RLVR%29%20has%20emerged%20as%20a%0Apowerful%20paradigm%20for%20unlocking%20reasoning%20capabilities%20in%20large%20language%0Amodels%2C%20yet%20its%20full%20potential%20is%20hindered%20by%20two%20under-explored%20dimensions%3A%0ADepth-the%20hardest%20problem%20a%20model%20can%20sample%3B%20Breadth-the%20number%20of%20instances%0Aconsumed%20in%20a%20single%20iteration.%20We%20dissect%20the%20popular%20GRPO%20algorithm%20and%0Areveal%20a%20systematic%20bias%3A%20the%20cumulative-advantage%20disproportionately%20weights%0Asamples%20with%20medium%20accuracy%2C%20while%20down-weighting%20the%20low-accuracy%20instances%0Athat%20are%20crucial%20for%20pushing%20reasoning%20boundaries.%20To%20rectify%20the%20depth%0Aneglect%2C%20we%20introduce%20Difficulty%20Adaptive%20Rollout%20Sampling%20%28DARS%29%2C%20which%0Are-weights%20hard%20problems%20through%20targeted%20multi-stage%20rollouts%2C%20thereby%0Aincreasing%20the%20number%20of%20positive%20rollouts%20for%20hard%20problems.%20Empirically%2C%0Anaively%20enlarging%20rollout%20size%20only%20accelerates%20convergence%20and%20even%20hurts%0APass%40K.%20Our%20DARS%2C%20in%20contrast%2C%20delivers%20consistent%20Pass%40K%20gains%20without%20extra%0Ainference%20cost%20at%20convergence.%20Just%20as%20we%20adaptively%20expanded%20the%20depth%20of%0Aexploration%2C%20we%20now%20ask%20whether%20aggressively%20scaling%20the%20breadth%20of%20training%0Adata%20can%20further%20amplify%20reasoning%20gains.%20To%20this%20end%2C%20we%20intensely%20scale%20batch%0Asize%20and%20replace%20PPO%27s%20mini-batch%20iterations%20with%20full-batch%20updates%20over%0Amultiple%20epochs.%20Increasing%20breadth%20significantly%20enhances%20Pass%401%20performance.%0ALarge-breadth%20training%20sustains%20high%20token-level%20entropy%2C%20indicating%20continued%0Aexploration%20and%20reduced%20gradient%20noise.%20We%20further%20present%20DARS-B%2C%20which%0Aaugments%20DARS%20with%20large%20breadth%2C%20and%20demonstrate%20simultaneous%20gains%20in%20Pass%40K%0Aand%20Pass%401.%20The%20results%20confirm%20that%20breadth%20and%20adaptive%20exploration%20across%0Adepth%20operate%20as%20orthogonal%20dimensions%20in%20RLVR%2C%20which%20are%20key%20to%20unleashing%20the%0Areasoning%20power%20of%20RLVR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth-Breadth%2520Synergy%2520in%2520RLVR%253A%2520Unlocking%2520LLM%2520Reasoning%2520Gains%2520with%250A%2520%2520Adaptive%2520Exploration%26entry.906535625%3DZhicheng%2520Yang%2520and%2520Zhijiang%2520Guo%2520and%2520Yinya%2520Huang%2520and%2520Yongxin%2520Wang%2520and%2520Dongchun%2520Xie%2520and%2520Yiwei%2520Wang%2520and%2520Xiaodan%2520Liang%2520and%2520Jing%2520Tang%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Reward%2520%2528RLVR%2529%2520has%2520emerged%2520as%2520a%250Apowerful%2520paradigm%2520for%2520unlocking%2520reasoning%2520capabilities%2520in%2520large%2520language%250Amodels%252C%2520yet%2520its%2520full%2520potential%2520is%2520hindered%2520by%2520two%2520under-explored%2520dimensions%253A%250ADepth-the%2520hardest%2520problem%2520a%2520model%2520can%2520sample%253B%2520Breadth-the%2520number%2520of%2520instances%250Aconsumed%2520in%2520a%2520single%2520iteration.%2520We%2520dissect%2520the%2520popular%2520GRPO%2520algorithm%2520and%250Areveal%2520a%2520systematic%2520bias%253A%2520the%2520cumulative-advantage%2520disproportionately%2520weights%250Asamples%2520with%2520medium%2520accuracy%252C%2520while%2520down-weighting%2520the%2520low-accuracy%2520instances%250Athat%2520are%2520crucial%2520for%2520pushing%2520reasoning%2520boundaries.%2520To%2520rectify%2520the%2520depth%250Aneglect%252C%2520we%2520introduce%2520Difficulty%2520Adaptive%2520Rollout%2520Sampling%2520%2528DARS%2529%252C%2520which%250Are-weights%2520hard%2520problems%2520through%2520targeted%2520multi-stage%2520rollouts%252C%2520thereby%250Aincreasing%2520the%2520number%2520of%2520positive%2520rollouts%2520for%2520hard%2520problems.%2520Empirically%252C%250Anaively%2520enlarging%2520rollout%2520size%2520only%2520accelerates%2520convergence%2520and%2520even%2520hurts%250APass%2540K.%2520Our%2520DARS%252C%2520in%2520contrast%252C%2520delivers%2520consistent%2520Pass%2540K%2520gains%2520without%2520extra%250Ainference%2520cost%2520at%2520convergence.%2520Just%2520as%2520we%2520adaptively%2520expanded%2520the%2520depth%2520of%250Aexploration%252C%2520we%2520now%2520ask%2520whether%2520aggressively%2520scaling%2520the%2520breadth%2520of%2520training%250Adata%2520can%2520further%2520amplify%2520reasoning%2520gains.%2520To%2520this%2520end%252C%2520we%2520intensely%2520scale%2520batch%250Asize%2520and%2520replace%2520PPO%2527s%2520mini-batch%2520iterations%2520with%2520full-batch%2520updates%2520over%250Amultiple%2520epochs.%2520Increasing%2520breadth%2520significantly%2520enhances%2520Pass%25401%2520performance.%250ALarge-breadth%2520training%2520sustains%2520high%2520token-level%2520entropy%252C%2520indicating%2520continued%250Aexploration%2520and%2520reduced%2520gradient%2520noise.%2520We%2520further%2520present%2520DARS-B%252C%2520which%250Aaugments%2520DARS%2520with%2520large%2520breadth%252C%2520and%2520demonstrate%2520simultaneous%2520gains%2520in%2520Pass%2540K%250Aand%2520Pass%25401.%2520The%2520results%2520confirm%2520that%2520breadth%2520and%2520adaptive%2520exploration%2520across%250Adepth%2520operate%2520as%2520orthogonal%2520dimensions%2520in%2520RLVR%252C%2520which%2520are%2520key%2520to%2520unleashing%2520the%250Areasoning%2520power%2520of%2520RLVR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-Breadth%20Synergy%20in%20RLVR%3A%20Unlocking%20LLM%20Reasoning%20Gains%20with%0A%20%20Adaptive%20Exploration&entry.906535625=Zhicheng%20Yang%20and%20Zhijiang%20Guo%20and%20Yinya%20Huang%20and%20Yongxin%20Wang%20and%20Dongchun%20Xie%20and%20Yiwei%20Wang%20and%20Xiaodan%20Liang%20and%20Jing%20Tang&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Verifiable%20Reward%20%28RLVR%29%20has%20emerged%20as%20a%0Apowerful%20paradigm%20for%20unlocking%20reasoning%20capabilities%20in%20large%20language%0Amodels%2C%20yet%20its%20full%20potential%20is%20hindered%20by%20two%20under-explored%20dimensions%3A%0ADepth-the%20hardest%20problem%20a%20model%20can%20sample%3B%20Breadth-the%20number%20of%20instances%0Aconsumed%20in%20a%20single%20iteration.%20We%20dissect%20the%20popular%20GRPO%20algorithm%20and%0Areveal%20a%20systematic%20bias%3A%20the%20cumulative-advantage%20disproportionately%20weights%0Asamples%20with%20medium%20accuracy%2C%20while%20down-weighting%20the%20low-accuracy%20instances%0Athat%20are%20crucial%20for%20pushing%20reasoning%20boundaries.%20To%20rectify%20the%20depth%0Aneglect%2C%20we%20introduce%20Difficulty%20Adaptive%20Rollout%20Sampling%20%28DARS%29%2C%20which%0Are-weights%20hard%20problems%20through%20targeted%20multi-stage%20rollouts%2C%20thereby%0Aincreasing%20the%20number%20of%20positive%20rollouts%20for%20hard%20problems.%20Empirically%2C%0Anaively%20enlarging%20rollout%20size%20only%20accelerates%20convergence%20and%20even%20hurts%0APass%40K.%20Our%20DARS%2C%20in%20contrast%2C%20delivers%20consistent%20Pass%40K%20gains%20without%20extra%0Ainference%20cost%20at%20convergence.%20Just%20as%20we%20adaptively%20expanded%20the%20depth%20of%0Aexploration%2C%20we%20now%20ask%20whether%20aggressively%20scaling%20the%20breadth%20of%20training%0Adata%20can%20further%20amplify%20reasoning%20gains.%20To%20this%20end%2C%20we%20intensely%20scale%20batch%0Asize%20and%20replace%20PPO%27s%20mini-batch%20iterations%20with%20full-batch%20updates%20over%0Amultiple%20epochs.%20Increasing%20breadth%20significantly%20enhances%20Pass%401%20performance.%0ALarge-breadth%20training%20sustains%20high%20token-level%20entropy%2C%20indicating%20continued%0Aexploration%20and%20reduced%20gradient%20noise.%20We%20further%20present%20DARS-B%2C%20which%0Aaugments%20DARS%20with%20large%20breadth%2C%20and%20demonstrate%20simultaneous%20gains%20in%20Pass%40K%0Aand%20Pass%401.%20The%20results%20confirm%20that%20breadth%20and%20adaptive%20exploration%20across%0Adepth%20operate%20as%20orthogonal%20dimensions%20in%20RLVR%2C%20which%20are%20key%20to%20unleashing%20the%0Areasoning%20power%20of%20RLVR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13755v1&entry.124074799=Read"},
{"title": "UltraDfeGAN: Detail-Enhancing Generative Adversarial Networks for\n  High-Fidelity Functional Ultrasound Synthesis", "author": "Zhuo Li and Xuhang Chen and Shuqiang Wang and Bin Yuan and Nou Sotheany and Ngeth Rithea", "abstract": "  Functional ultrasound (fUS) is a neuroimaging technique known for its high\nspatiotemporal resolution, enabling non-invasive observation of brain activity\nthrough neurovascular coupling. Despite its potential in clinical applications\nsuch as neonatal monitoring and intraoperative guidance, the development of fUS\nfaces challenges related to data scarcity and limitations in generating\nrealistic fUS images. This paper explores the use of a generative adversarial\nnetwork (GAN) framework tailored for fUS image synthesis. The proposed method\nincorporates architectural enhancements, including feature enhancement modules\nand normalization techniques, aiming to improve the fidelity and physiological\nplausibility of generated images. The study evaluates the performance of the\nframework against existing generative models, demonstrating its capability to\nproduce high-quality fUS images under various experimental conditions.\nAdditionally, the synthesized images are assessed for their utility in\ndownstream tasks, showing improvements in classification accuracy when used for\ndata augmentation. Experimental results are based on publicly available fUS\ndatasets, highlighting the framework's effectiveness in addressing data\nlimitations.\n", "link": "http://arxiv.org/abs/2507.03341v2", "date": "2025-08-19", "relevancy": 2.2361, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5738}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5495}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UltraDfeGAN%3A%20Detail-Enhancing%20Generative%20Adversarial%20Networks%20for%0A%20%20High-Fidelity%20Functional%20Ultrasound%20Synthesis&body=Title%3A%20UltraDfeGAN%3A%20Detail-Enhancing%20Generative%20Adversarial%20Networks%20for%0A%20%20High-Fidelity%20Functional%20Ultrasound%20Synthesis%0AAuthor%3A%20Zhuo%20Li%20and%20Xuhang%20Chen%20and%20Shuqiang%20Wang%20and%20Bin%20Yuan%20and%20Nou%20Sotheany%20and%20Ngeth%20Rithea%0AAbstract%3A%20%20%20Functional%20ultrasound%20%28fUS%29%20is%20a%20neuroimaging%20technique%20known%20for%20its%20high%0Aspatiotemporal%20resolution%2C%20enabling%20non-invasive%20observation%20of%20brain%20activity%0Athrough%20neurovascular%20coupling.%20Despite%20its%20potential%20in%20clinical%20applications%0Asuch%20as%20neonatal%20monitoring%20and%20intraoperative%20guidance%2C%20the%20development%20of%20fUS%0Afaces%20challenges%20related%20to%20data%20scarcity%20and%20limitations%20in%20generating%0Arealistic%20fUS%20images.%20This%20paper%20explores%20the%20use%20of%20a%20generative%20adversarial%0Anetwork%20%28GAN%29%20framework%20tailored%20for%20fUS%20image%20synthesis.%20The%20proposed%20method%0Aincorporates%20architectural%20enhancements%2C%20including%20feature%20enhancement%20modules%0Aand%20normalization%20techniques%2C%20aiming%20to%20improve%20the%20fidelity%20and%20physiological%0Aplausibility%20of%20generated%20images.%20The%20study%20evaluates%20the%20performance%20of%20the%0Aframework%20against%20existing%20generative%20models%2C%20demonstrating%20its%20capability%20to%0Aproduce%20high-quality%20fUS%20images%20under%20various%20experimental%20conditions.%0AAdditionally%2C%20the%20synthesized%20images%20are%20assessed%20for%20their%20utility%20in%0Adownstream%20tasks%2C%20showing%20improvements%20in%20classification%20accuracy%20when%20used%20for%0Adata%20augmentation.%20Experimental%20results%20are%20based%20on%20publicly%20available%20fUS%0Adatasets%2C%20highlighting%20the%20framework%27s%20effectiveness%20in%20addressing%20data%0Alimitations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03341v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltraDfeGAN%253A%2520Detail-Enhancing%2520Generative%2520Adversarial%2520Networks%2520for%250A%2520%2520High-Fidelity%2520Functional%2520Ultrasound%2520Synthesis%26entry.906535625%3DZhuo%2520Li%2520and%2520Xuhang%2520Chen%2520and%2520Shuqiang%2520Wang%2520and%2520Bin%2520Yuan%2520and%2520Nou%2520Sotheany%2520and%2520Ngeth%2520Rithea%26entry.1292438233%3D%2520%2520Functional%2520ultrasound%2520%2528fUS%2529%2520is%2520a%2520neuroimaging%2520technique%2520known%2520for%2520its%2520high%250Aspatiotemporal%2520resolution%252C%2520enabling%2520non-invasive%2520observation%2520of%2520brain%2520activity%250Athrough%2520neurovascular%2520coupling.%2520Despite%2520its%2520potential%2520in%2520clinical%2520applications%250Asuch%2520as%2520neonatal%2520monitoring%2520and%2520intraoperative%2520guidance%252C%2520the%2520development%2520of%2520fUS%250Afaces%2520challenges%2520related%2520to%2520data%2520scarcity%2520and%2520limitations%2520in%2520generating%250Arealistic%2520fUS%2520images.%2520This%2520paper%2520explores%2520the%2520use%2520of%2520a%2520generative%2520adversarial%250Anetwork%2520%2528GAN%2529%2520framework%2520tailored%2520for%2520fUS%2520image%2520synthesis.%2520The%2520proposed%2520method%250Aincorporates%2520architectural%2520enhancements%252C%2520including%2520feature%2520enhancement%2520modules%250Aand%2520normalization%2520techniques%252C%2520aiming%2520to%2520improve%2520the%2520fidelity%2520and%2520physiological%250Aplausibility%2520of%2520generated%2520images.%2520The%2520study%2520evaluates%2520the%2520performance%2520of%2520the%250Aframework%2520against%2520existing%2520generative%2520models%252C%2520demonstrating%2520its%2520capability%2520to%250Aproduce%2520high-quality%2520fUS%2520images%2520under%2520various%2520experimental%2520conditions.%250AAdditionally%252C%2520the%2520synthesized%2520images%2520are%2520assessed%2520for%2520their%2520utility%2520in%250Adownstream%2520tasks%252C%2520showing%2520improvements%2520in%2520classification%2520accuracy%2520when%2520used%2520for%250Adata%2520augmentation.%2520Experimental%2520results%2520are%2520based%2520on%2520publicly%2520available%2520fUS%250Adatasets%252C%2520highlighting%2520the%2520framework%2527s%2520effectiveness%2520in%2520addressing%2520data%250Alimitations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03341v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UltraDfeGAN%3A%20Detail-Enhancing%20Generative%20Adversarial%20Networks%20for%0A%20%20High-Fidelity%20Functional%20Ultrasound%20Synthesis&entry.906535625=Zhuo%20Li%20and%20Xuhang%20Chen%20and%20Shuqiang%20Wang%20and%20Bin%20Yuan%20and%20Nou%20Sotheany%20and%20Ngeth%20Rithea&entry.1292438233=%20%20Functional%20ultrasound%20%28fUS%29%20is%20a%20neuroimaging%20technique%20known%20for%20its%20high%0Aspatiotemporal%20resolution%2C%20enabling%20non-invasive%20observation%20of%20brain%20activity%0Athrough%20neurovascular%20coupling.%20Despite%20its%20potential%20in%20clinical%20applications%0Asuch%20as%20neonatal%20monitoring%20and%20intraoperative%20guidance%2C%20the%20development%20of%20fUS%0Afaces%20challenges%20related%20to%20data%20scarcity%20and%20limitations%20in%20generating%0Arealistic%20fUS%20images.%20This%20paper%20explores%20the%20use%20of%20a%20generative%20adversarial%0Anetwork%20%28GAN%29%20framework%20tailored%20for%20fUS%20image%20synthesis.%20The%20proposed%20method%0Aincorporates%20architectural%20enhancements%2C%20including%20feature%20enhancement%20modules%0Aand%20normalization%20techniques%2C%20aiming%20to%20improve%20the%20fidelity%20and%20physiological%0Aplausibility%20of%20generated%20images.%20The%20study%20evaluates%20the%20performance%20of%20the%0Aframework%20against%20existing%20generative%20models%2C%20demonstrating%20its%20capability%20to%0Aproduce%20high-quality%20fUS%20images%20under%20various%20experimental%20conditions.%0AAdditionally%2C%20the%20synthesized%20images%20are%20assessed%20for%20their%20utility%20in%0Adownstream%20tasks%2C%20showing%20improvements%20in%20classification%20accuracy%20when%20used%20for%0Adata%20augmentation.%20Experimental%20results%20are%20based%20on%20publicly%20available%20fUS%0Adatasets%2C%20highlighting%20the%20framework%27s%20effectiveness%20in%20addressing%20data%0Alimitations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03341v2&entry.124074799=Read"},
{"title": "ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving", "author": "Xianda Guo and Ruijun Zhang and Yiqun Duan and Ruilin Wang and Keyuan Zhou and Wenzhao Zheng and Wenke Huang and Gangwei Xu and Mike Horton and Yuan Si and Hao Zhao and Long Chen", "abstract": "  Depth estimation is a fundamental task for 3D scene understanding in\nautonomous driving, robotics, and augmented reality. Existing depth datasets,\nsuch as KITTI, nuScenes, and DDAD, have advanced the field but suffer from\nlimitations in diversity and scalability. As benchmark performance on these\ndatasets approaches saturation, there is an increasing need for a new\ngeneration of large-scale, diverse, and cost-efficient datasets to support the\nera of foundation models and multi-modal learning. To address these challenges,\nwe introduce a large-scale, diverse, frame-wise continuous dataset for depth\nestimation in dynamic outdoor driving environments, comprising 20K video frames\nto evaluate existing methods. Our lightweight acquisition pipeline ensures\nbroad scene coverage at low cost, while sparse yet statistically sufficient\nground truth enables robust training. Compared to existing datasets, ours\npresents greater diversity in driving scenarios and lower depth density,\ncreating new challenges for generalization. Benchmark experiments with standard\nmonocular depth estimation models validate the dataset's utility and highlight\nsubstantial performance gaps in challenging conditions, establishing a new\nplatform for advancing depth estimation research.\n", "link": "http://arxiv.org/abs/2508.13977v1", "date": "2025-08-19", "relevancy": 2.2341, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5758}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ROVR-Open-Dataset%3A%20A%20Large-Scale%20Depth%20Dataset%20for%20Autonomous%20Driving&body=Title%3A%20ROVR-Open-Dataset%3A%20A%20Large-Scale%20Depth%20Dataset%20for%20Autonomous%20Driving%0AAuthor%3A%20Xianda%20Guo%20and%20Ruijun%20Zhang%20and%20Yiqun%20Duan%20and%20Ruilin%20Wang%20and%20Keyuan%20Zhou%20and%20Wenzhao%20Zheng%20and%20Wenke%20Huang%20and%20Gangwei%20Xu%20and%20Mike%20Horton%20and%20Yuan%20Si%20and%20Hao%20Zhao%20and%20Long%20Chen%0AAbstract%3A%20%20%20Depth%20estimation%20is%20a%20fundamental%20task%20for%203D%20scene%20understanding%20in%0Aautonomous%20driving%2C%20robotics%2C%20and%20augmented%20reality.%20Existing%20depth%20datasets%2C%0Asuch%20as%20KITTI%2C%20nuScenes%2C%20and%20DDAD%2C%20have%20advanced%20the%20field%20but%20suffer%20from%0Alimitations%20in%20diversity%20and%20scalability.%20As%20benchmark%20performance%20on%20these%0Adatasets%20approaches%20saturation%2C%20there%20is%20an%20increasing%20need%20for%20a%20new%0Ageneration%20of%20large-scale%2C%20diverse%2C%20and%20cost-efficient%20datasets%20to%20support%20the%0Aera%20of%20foundation%20models%20and%20multi-modal%20learning.%20To%20address%20these%20challenges%2C%0Awe%20introduce%20a%20large-scale%2C%20diverse%2C%20frame-wise%20continuous%20dataset%20for%20depth%0Aestimation%20in%20dynamic%20outdoor%20driving%20environments%2C%20comprising%2020K%20video%20frames%0Ato%20evaluate%20existing%20methods.%20Our%20lightweight%20acquisition%20pipeline%20ensures%0Abroad%20scene%20coverage%20at%20low%20cost%2C%20while%20sparse%20yet%20statistically%20sufficient%0Aground%20truth%20enables%20robust%20training.%20Compared%20to%20existing%20datasets%2C%20ours%0Apresents%20greater%20diversity%20in%20driving%20scenarios%20and%20lower%20depth%20density%2C%0Acreating%20new%20challenges%20for%20generalization.%20Benchmark%20experiments%20with%20standard%0Amonocular%20depth%20estimation%20models%20validate%20the%20dataset%27s%20utility%20and%20highlight%0Asubstantial%20performance%20gaps%20in%20challenging%20conditions%2C%20establishing%20a%20new%0Aplatform%20for%20advancing%20depth%20estimation%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DROVR-Open-Dataset%253A%2520A%2520Large-Scale%2520Depth%2520Dataset%2520for%2520Autonomous%2520Driving%26entry.906535625%3DXianda%2520Guo%2520and%2520Ruijun%2520Zhang%2520and%2520Yiqun%2520Duan%2520and%2520Ruilin%2520Wang%2520and%2520Keyuan%2520Zhou%2520and%2520Wenzhao%2520Zheng%2520and%2520Wenke%2520Huang%2520and%2520Gangwei%2520Xu%2520and%2520Mike%2520Horton%2520and%2520Yuan%2520Si%2520and%2520Hao%2520Zhao%2520and%2520Long%2520Chen%26entry.1292438233%3D%2520%2520Depth%2520estimation%2520is%2520a%2520fundamental%2520task%2520for%25203D%2520scene%2520understanding%2520in%250Aautonomous%2520driving%252C%2520robotics%252C%2520and%2520augmented%2520reality.%2520Existing%2520depth%2520datasets%252C%250Asuch%2520as%2520KITTI%252C%2520nuScenes%252C%2520and%2520DDAD%252C%2520have%2520advanced%2520the%2520field%2520but%2520suffer%2520from%250Alimitations%2520in%2520diversity%2520and%2520scalability.%2520As%2520benchmark%2520performance%2520on%2520these%250Adatasets%2520approaches%2520saturation%252C%2520there%2520is%2520an%2520increasing%2520need%2520for%2520a%2520new%250Ageneration%2520of%2520large-scale%252C%2520diverse%252C%2520and%2520cost-efficient%2520datasets%2520to%2520support%2520the%250Aera%2520of%2520foundation%2520models%2520and%2520multi-modal%2520learning.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520introduce%2520a%2520large-scale%252C%2520diverse%252C%2520frame-wise%2520continuous%2520dataset%2520for%2520depth%250Aestimation%2520in%2520dynamic%2520outdoor%2520driving%2520environments%252C%2520comprising%252020K%2520video%2520frames%250Ato%2520evaluate%2520existing%2520methods.%2520Our%2520lightweight%2520acquisition%2520pipeline%2520ensures%250Abroad%2520scene%2520coverage%2520at%2520low%2520cost%252C%2520while%2520sparse%2520yet%2520statistically%2520sufficient%250Aground%2520truth%2520enables%2520robust%2520training.%2520Compared%2520to%2520existing%2520datasets%252C%2520ours%250Apresents%2520greater%2520diversity%2520in%2520driving%2520scenarios%2520and%2520lower%2520depth%2520density%252C%250Acreating%2520new%2520challenges%2520for%2520generalization.%2520Benchmark%2520experiments%2520with%2520standard%250Amonocular%2520depth%2520estimation%2520models%2520validate%2520the%2520dataset%2527s%2520utility%2520and%2520highlight%250Asubstantial%2520performance%2520gaps%2520in%2520challenging%2520conditions%252C%2520establishing%2520a%2520new%250Aplatform%2520for%2520advancing%2520depth%2520estimation%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROVR-Open-Dataset%3A%20A%20Large-Scale%20Depth%20Dataset%20for%20Autonomous%20Driving&entry.906535625=Xianda%20Guo%20and%20Ruijun%20Zhang%20and%20Yiqun%20Duan%20and%20Ruilin%20Wang%20and%20Keyuan%20Zhou%20and%20Wenzhao%20Zheng%20and%20Wenke%20Huang%20and%20Gangwei%20Xu%20and%20Mike%20Horton%20and%20Yuan%20Si%20and%20Hao%20Zhao%20and%20Long%20Chen&entry.1292438233=%20%20Depth%20estimation%20is%20a%20fundamental%20task%20for%203D%20scene%20understanding%20in%0Aautonomous%20driving%2C%20robotics%2C%20and%20augmented%20reality.%20Existing%20depth%20datasets%2C%0Asuch%20as%20KITTI%2C%20nuScenes%2C%20and%20DDAD%2C%20have%20advanced%20the%20field%20but%20suffer%20from%0Alimitations%20in%20diversity%20and%20scalability.%20As%20benchmark%20performance%20on%20these%0Adatasets%20approaches%20saturation%2C%20there%20is%20an%20increasing%20need%20for%20a%20new%0Ageneration%20of%20large-scale%2C%20diverse%2C%20and%20cost-efficient%20datasets%20to%20support%20the%0Aera%20of%20foundation%20models%20and%20multi-modal%20learning.%20To%20address%20these%20challenges%2C%0Awe%20introduce%20a%20large-scale%2C%20diverse%2C%20frame-wise%20continuous%20dataset%20for%20depth%0Aestimation%20in%20dynamic%20outdoor%20driving%20environments%2C%20comprising%2020K%20video%20frames%0Ato%20evaluate%20existing%20methods.%20Our%20lightweight%20acquisition%20pipeline%20ensures%0Abroad%20scene%20coverage%20at%20low%20cost%2C%20while%20sparse%20yet%20statistically%20sufficient%0Aground%20truth%20enables%20robust%20training.%20Compared%20to%20existing%20datasets%2C%20ours%0Apresents%20greater%20diversity%20in%20driving%20scenarios%20and%20lower%20depth%20density%2C%0Acreating%20new%20challenges%20for%20generalization.%20Benchmark%20experiments%20with%20standard%0Amonocular%20depth%20estimation%20models%20validate%20the%20dataset%27s%20utility%20and%20highlight%0Asubstantial%20performance%20gaps%20in%20challenging%20conditions%2C%20establishing%20a%20new%0Aplatform%20for%20advancing%20depth%20estimation%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13977v1&entry.124074799=Read"},
{"title": "Prompt Orchestration Markup Language", "author": "Yuge Zhang and Nan Chen and Jiahang Xu and Yuqing Yang", "abstract": "  Large Language Models (LLMs) require sophisticated prompting, yet current\npractices face challenges in structure, data integration, format sensitivity,\nand tooling. Existing methods lack comprehensive solutions for organizing\ncomplex prompts involving diverse data types (documents, tables, images) or\nmanaging presentation variations systematically. To address these gaps, we\nintroduce POML (Prompt Orchestration Markup Language). POML employs\ncomponent-based markup for logical structure (roles, tasks, examples),\nspecialized tags for seamless data integration, and a CSS-like styling system\nto decouple content from presentation, reducing formatting sensitivity. It\nincludes templating for dynamic prompts and a comprehensive developer toolkit\n(IDE support, SDKs) to improve version control and collaboration. We validate\nPOML through two case studies demonstrating its impact on complex application\nintegration (PomLink) and accuracy performance (TableQA), as well as a user\nstudy assessing its effectiveness in real-world development scenarios.\n", "link": "http://arxiv.org/abs/2508.13948v1", "date": "2025-08-19", "relevancy": 2.2298, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4476}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt%20Orchestration%20Markup%20Language&body=Title%3A%20Prompt%20Orchestration%20Markup%20Language%0AAuthor%3A%20Yuge%20Zhang%20and%20Nan%20Chen%20and%20Jiahang%20Xu%20and%20Yuqing%20Yang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20require%20sophisticated%20prompting%2C%20yet%20current%0Apractices%20face%20challenges%20in%20structure%2C%20data%20integration%2C%20format%20sensitivity%2C%0Aand%20tooling.%20Existing%20methods%20lack%20comprehensive%20solutions%20for%20organizing%0Acomplex%20prompts%20involving%20diverse%20data%20types%20%28documents%2C%20tables%2C%20images%29%20or%0Amanaging%20presentation%20variations%20systematically.%20To%20address%20these%20gaps%2C%20we%0Aintroduce%20POML%20%28Prompt%20Orchestration%20Markup%20Language%29.%20POML%20employs%0Acomponent-based%20markup%20for%20logical%20structure%20%28roles%2C%20tasks%2C%20examples%29%2C%0Aspecialized%20tags%20for%20seamless%20data%20integration%2C%20and%20a%20CSS-like%20styling%20system%0Ato%20decouple%20content%20from%20presentation%2C%20reducing%20formatting%20sensitivity.%20It%0Aincludes%20templating%20for%20dynamic%20prompts%20and%20a%20comprehensive%20developer%20toolkit%0A%28IDE%20support%2C%20SDKs%29%20to%20improve%20version%20control%20and%20collaboration.%20We%20validate%0APOML%20through%20two%20case%20studies%20demonstrating%20its%20impact%20on%20complex%20application%0Aintegration%20%28PomLink%29%20and%20accuracy%20performance%20%28TableQA%29%2C%20as%20well%20as%20a%20user%0Astudy%20assessing%20its%20effectiveness%20in%20real-world%20development%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt%2520Orchestration%2520Markup%2520Language%26entry.906535625%3DYuge%2520Zhang%2520and%2520Nan%2520Chen%2520and%2520Jiahang%2520Xu%2520and%2520Yuqing%2520Yang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520require%2520sophisticated%2520prompting%252C%2520yet%2520current%250Apractices%2520face%2520challenges%2520in%2520structure%252C%2520data%2520integration%252C%2520format%2520sensitivity%252C%250Aand%2520tooling.%2520Existing%2520methods%2520lack%2520comprehensive%2520solutions%2520for%2520organizing%250Acomplex%2520prompts%2520involving%2520diverse%2520data%2520types%2520%2528documents%252C%2520tables%252C%2520images%2529%2520or%250Amanaging%2520presentation%2520variations%2520systematically.%2520To%2520address%2520these%2520gaps%252C%2520we%250Aintroduce%2520POML%2520%2528Prompt%2520Orchestration%2520Markup%2520Language%2529.%2520POML%2520employs%250Acomponent-based%2520markup%2520for%2520logical%2520structure%2520%2528roles%252C%2520tasks%252C%2520examples%2529%252C%250Aspecialized%2520tags%2520for%2520seamless%2520data%2520integration%252C%2520and%2520a%2520CSS-like%2520styling%2520system%250Ato%2520decouple%2520content%2520from%2520presentation%252C%2520reducing%2520formatting%2520sensitivity.%2520It%250Aincludes%2520templating%2520for%2520dynamic%2520prompts%2520and%2520a%2520comprehensive%2520developer%2520toolkit%250A%2528IDE%2520support%252C%2520SDKs%2529%2520to%2520improve%2520version%2520control%2520and%2520collaboration.%2520We%2520validate%250APOML%2520through%2520two%2520case%2520studies%2520demonstrating%2520its%2520impact%2520on%2520complex%2520application%250Aintegration%2520%2528PomLink%2529%2520and%2520accuracy%2520performance%2520%2528TableQA%2529%252C%2520as%2520well%2520as%2520a%2520user%250Astudy%2520assessing%2520its%2520effectiveness%2520in%2520real-world%2520development%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt%20Orchestration%20Markup%20Language&entry.906535625=Yuge%20Zhang%20and%20Nan%20Chen%20and%20Jiahang%20Xu%20and%20Yuqing%20Yang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20require%20sophisticated%20prompting%2C%20yet%20current%0Apractices%20face%20challenges%20in%20structure%2C%20data%20integration%2C%20format%20sensitivity%2C%0Aand%20tooling.%20Existing%20methods%20lack%20comprehensive%20solutions%20for%20organizing%0Acomplex%20prompts%20involving%20diverse%20data%20types%20%28documents%2C%20tables%2C%20images%29%20or%0Amanaging%20presentation%20variations%20systematically.%20To%20address%20these%20gaps%2C%20we%0Aintroduce%20POML%20%28Prompt%20Orchestration%20Markup%20Language%29.%20POML%20employs%0Acomponent-based%20markup%20for%20logical%20structure%20%28roles%2C%20tasks%2C%20examples%29%2C%0Aspecialized%20tags%20for%20seamless%20data%20integration%2C%20and%20a%20CSS-like%20styling%20system%0Ato%20decouple%20content%20from%20presentation%2C%20reducing%20formatting%20sensitivity.%20It%0Aincludes%20templating%20for%20dynamic%20prompts%20and%20a%20comprehensive%20developer%20toolkit%0A%28IDE%20support%2C%20SDKs%29%20to%20improve%20version%20control%20and%20collaboration.%20We%20validate%0APOML%20through%20two%20case%20studies%20demonstrating%20its%20impact%20on%20complex%20application%0Aintegration%20%28PomLink%29%20and%20accuracy%20performance%20%28TableQA%29%2C%20as%20well%20as%20a%20user%0Astudy%20assessing%20its%20effectiveness%20in%20real-world%20development%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13948v1&entry.124074799=Read"},
{"title": "A Fully Transformer Based Multimodal Framework for Explainable Cancer\n  Image Segmentation Using Radiology Reports", "author": "Enobong Adahada and Isabel Sassoon and Kate Hone and Yongmin Li", "abstract": "  We introduce Med-CTX, a fully transformer based multimodal framework for\nexplainable breast cancer ultrasound segmentation. We integrate clinical\nradiology reports to boost both performance and interpretability. Med-CTX\nachieves exact lesion delineation by using a dual-branch visual encoder that\ncombines ViT and Swin transformers, as well as uncertainty aware fusion.\nClinical language structured with BI-RADS semantics is encoded by\nBioClinicalBERT and combined with visual features utilising cross-modal\nattention, allowing the model to provide clinically grounded, model generated\nexplanations. Our methodology generates segmentation masks, uncertainty maps,\nand diagnostic rationales all at once, increasing confidence and transparency\nin computer assisted diagnosis. On the BUS-BRA dataset, Med-CTX achieves a Dice\nscore of 99% and an IoU of 95%, beating existing baselines U-Net, ViT, and\nSwin. Clinical text plays a key role in segmentation accuracy and explanation\nquality, as evidenced by ablation studies that show a -5.4% decline in Dice\nscore and -31% in CIDEr. Med-CTX achieves good multimodal alignment (CLIP\nscore: 85%) and increased confi dence calibration (ECE: 3.2%), setting a new\nbar for trustworthy, multimodal medical architecture.\n", "link": "http://arxiv.org/abs/2508.13796v1", "date": "2025-08-19", "relevancy": 2.2139, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.572}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Fully%20Transformer%20Based%20Multimodal%20Framework%20for%20Explainable%20Cancer%0A%20%20Image%20Segmentation%20Using%20Radiology%20Reports&body=Title%3A%20A%20Fully%20Transformer%20Based%20Multimodal%20Framework%20for%20Explainable%20Cancer%0A%20%20Image%20Segmentation%20Using%20Radiology%20Reports%0AAuthor%3A%20Enobong%20Adahada%20and%20Isabel%20Sassoon%20and%20Kate%20Hone%20and%20Yongmin%20Li%0AAbstract%3A%20%20%20We%20introduce%20Med-CTX%2C%20a%20fully%20transformer%20based%20multimodal%20framework%20for%0Aexplainable%20breast%20cancer%20ultrasound%20segmentation.%20We%20integrate%20clinical%0Aradiology%20reports%20to%20boost%20both%20performance%20and%20interpretability.%20Med-CTX%0Aachieves%20exact%20lesion%20delineation%20by%20using%20a%20dual-branch%20visual%20encoder%20that%0Acombines%20ViT%20and%20Swin%20transformers%2C%20as%20well%20as%20uncertainty%20aware%20fusion.%0AClinical%20language%20structured%20with%20BI-RADS%20semantics%20is%20encoded%20by%0ABioClinicalBERT%20and%20combined%20with%20visual%20features%20utilising%20cross-modal%0Aattention%2C%20allowing%20the%20model%20to%20provide%20clinically%20grounded%2C%20model%20generated%0Aexplanations.%20Our%20methodology%20generates%20segmentation%20masks%2C%20uncertainty%20maps%2C%0Aand%20diagnostic%20rationales%20all%20at%20once%2C%20increasing%20confidence%20and%20transparency%0Ain%20computer%20assisted%20diagnosis.%20On%20the%20BUS-BRA%20dataset%2C%20Med-CTX%20achieves%20a%20Dice%0Ascore%20of%2099%25%20and%20an%20IoU%20of%2095%25%2C%20beating%20existing%20baselines%20U-Net%2C%20ViT%2C%20and%0ASwin.%20Clinical%20text%20plays%20a%20key%20role%20in%20segmentation%20accuracy%20and%20explanation%0Aquality%2C%20as%20evidenced%20by%20ablation%20studies%20that%20show%20a%20-5.4%25%20decline%20in%20Dice%0Ascore%20and%20-31%25%20in%20CIDEr.%20Med-CTX%20achieves%20good%20multimodal%20alignment%20%28CLIP%0Ascore%3A%2085%25%29%20and%20increased%20confi%20dence%20calibration%20%28ECE%3A%203.2%25%29%2C%20setting%20a%20new%0Abar%20for%20trustworthy%2C%20multimodal%20medical%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Fully%2520Transformer%2520Based%2520Multimodal%2520Framework%2520for%2520Explainable%2520Cancer%250A%2520%2520Image%2520Segmentation%2520Using%2520Radiology%2520Reports%26entry.906535625%3DEnobong%2520Adahada%2520and%2520Isabel%2520Sassoon%2520and%2520Kate%2520Hone%2520and%2520Yongmin%2520Li%26entry.1292438233%3D%2520%2520We%2520introduce%2520Med-CTX%252C%2520a%2520fully%2520transformer%2520based%2520multimodal%2520framework%2520for%250Aexplainable%2520breast%2520cancer%2520ultrasound%2520segmentation.%2520We%2520integrate%2520clinical%250Aradiology%2520reports%2520to%2520boost%2520both%2520performance%2520and%2520interpretability.%2520Med-CTX%250Aachieves%2520exact%2520lesion%2520delineation%2520by%2520using%2520a%2520dual-branch%2520visual%2520encoder%2520that%250Acombines%2520ViT%2520and%2520Swin%2520transformers%252C%2520as%2520well%2520as%2520uncertainty%2520aware%2520fusion.%250AClinical%2520language%2520structured%2520with%2520BI-RADS%2520semantics%2520is%2520encoded%2520by%250ABioClinicalBERT%2520and%2520combined%2520with%2520visual%2520features%2520utilising%2520cross-modal%250Aattention%252C%2520allowing%2520the%2520model%2520to%2520provide%2520clinically%2520grounded%252C%2520model%2520generated%250Aexplanations.%2520Our%2520methodology%2520generates%2520segmentation%2520masks%252C%2520uncertainty%2520maps%252C%250Aand%2520diagnostic%2520rationales%2520all%2520at%2520once%252C%2520increasing%2520confidence%2520and%2520transparency%250Ain%2520computer%2520assisted%2520diagnosis.%2520On%2520the%2520BUS-BRA%2520dataset%252C%2520Med-CTX%2520achieves%2520a%2520Dice%250Ascore%2520of%252099%2525%2520and%2520an%2520IoU%2520of%252095%2525%252C%2520beating%2520existing%2520baselines%2520U-Net%252C%2520ViT%252C%2520and%250ASwin.%2520Clinical%2520text%2520plays%2520a%2520key%2520role%2520in%2520segmentation%2520accuracy%2520and%2520explanation%250Aquality%252C%2520as%2520evidenced%2520by%2520ablation%2520studies%2520that%2520show%2520a%2520-5.4%2525%2520decline%2520in%2520Dice%250Ascore%2520and%2520-31%2525%2520in%2520CIDEr.%2520Med-CTX%2520achieves%2520good%2520multimodal%2520alignment%2520%2528CLIP%250Ascore%253A%252085%2525%2529%2520and%2520increased%2520confi%2520dence%2520calibration%2520%2528ECE%253A%25203.2%2525%2529%252C%2520setting%2520a%2520new%250Abar%2520for%2520trustworthy%252C%2520multimodal%2520medical%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Fully%20Transformer%20Based%20Multimodal%20Framework%20for%20Explainable%20Cancer%0A%20%20Image%20Segmentation%20Using%20Radiology%20Reports&entry.906535625=Enobong%20Adahada%20and%20Isabel%20Sassoon%20and%20Kate%20Hone%20and%20Yongmin%20Li&entry.1292438233=%20%20We%20introduce%20Med-CTX%2C%20a%20fully%20transformer%20based%20multimodal%20framework%20for%0Aexplainable%20breast%20cancer%20ultrasound%20segmentation.%20We%20integrate%20clinical%0Aradiology%20reports%20to%20boost%20both%20performance%20and%20interpretability.%20Med-CTX%0Aachieves%20exact%20lesion%20delineation%20by%20using%20a%20dual-branch%20visual%20encoder%20that%0Acombines%20ViT%20and%20Swin%20transformers%2C%20as%20well%20as%20uncertainty%20aware%20fusion.%0AClinical%20language%20structured%20with%20BI-RADS%20semantics%20is%20encoded%20by%0ABioClinicalBERT%20and%20combined%20with%20visual%20features%20utilising%20cross-modal%0Aattention%2C%20allowing%20the%20model%20to%20provide%20clinically%20grounded%2C%20model%20generated%0Aexplanations.%20Our%20methodology%20generates%20segmentation%20masks%2C%20uncertainty%20maps%2C%0Aand%20diagnostic%20rationales%20all%20at%20once%2C%20increasing%20confidence%20and%20transparency%0Ain%20computer%20assisted%20diagnosis.%20On%20the%20BUS-BRA%20dataset%2C%20Med-CTX%20achieves%20a%20Dice%0Ascore%20of%2099%25%20and%20an%20IoU%20of%2095%25%2C%20beating%20existing%20baselines%20U-Net%2C%20ViT%2C%20and%0ASwin.%20Clinical%20text%20plays%20a%20key%20role%20in%20segmentation%20accuracy%20and%20explanation%0Aquality%2C%20as%20evidenced%20by%20ablation%20studies%20that%20show%20a%20-5.4%25%20decline%20in%20Dice%0Ascore%20and%20-31%25%20in%20CIDEr.%20Med-CTX%20achieves%20good%20multimodal%20alignment%20%28CLIP%0Ascore%3A%2085%25%29%20and%20increased%20confi%20dence%20calibration%20%28ECE%3A%203.2%25%29%2C%20setting%20a%20new%0Abar%20for%20trustworthy%2C%20multimodal%20medical%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13796v1&entry.124074799=Read"},
{"title": "Beyond Simple Edits: Composed Video Retrieval with Dense Modifications", "author": "Omkar Thawakar and Dmitry Demidov and Ritesh Thawkar and Rao Muhammad Anwer and Mubarak Shah and Fahad Shahbaz Khan and Salman Khan", "abstract": "  Composed video retrieval is a challenging task that strives to retrieve a\ntarget video based on a query video and a textual description detailing\nspecific modifications. Standard retrieval frameworks typically struggle to\nhandle the complexity of fine-grained compositional queries and variations in\ntemporal understanding limiting their retrieval ability in the fine-grained\nsetting. To address this issue, we introduce a novel dataset that captures both\nfine-grained and composed actions across diverse video segments, enabling more\ndetailed compositional changes in retrieved video content. The proposed\ndataset, named Dense-WebVid-CoVR, consists of 1.6 million samples with dense\nmodification text that is around seven times more than its existing\ncounterpart. We further develop a new model that integrates visual and textual\ninformation through Cross-Attention (CA) fusion using grounded text encoder,\nenabling precise alignment between dense query modifications and target videos.\nThe proposed model achieves state-of-the-art results surpassing existing\nmethods on all metrics. Notably, it achieves 71.3\\% Recall@1 in visual+text\nsetting and outperforms the state-of-the-art by 3.4\\%, highlighting its\nefficacy in terms of leveraging detailed video descriptions and dense\nmodification texts. Our proposed dataset, code, and model are available at\n:https://github.com/OmkarThawakar/BSE-CoVR\n", "link": "http://arxiv.org/abs/2508.14039v1", "date": "2025-08-19", "relevancy": 2.1903, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5601}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Simple%20Edits%3A%20Composed%20Video%20Retrieval%20with%20Dense%20Modifications&body=Title%3A%20Beyond%20Simple%20Edits%3A%20Composed%20Video%20Retrieval%20with%20Dense%20Modifications%0AAuthor%3A%20Omkar%20Thawakar%20and%20Dmitry%20Demidov%20and%20Ritesh%20Thawkar%20and%20Rao%20Muhammad%20Anwer%20and%20Mubarak%20Shah%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan%0AAbstract%3A%20%20%20Composed%20video%20retrieval%20is%20a%20challenging%20task%20that%20strives%20to%20retrieve%20a%0Atarget%20video%20based%20on%20a%20query%20video%20and%20a%20textual%20description%20detailing%0Aspecific%20modifications.%20Standard%20retrieval%20frameworks%20typically%20struggle%20to%0Ahandle%20the%20complexity%20of%20fine-grained%20compositional%20queries%20and%20variations%20in%0Atemporal%20understanding%20limiting%20their%20retrieval%20ability%20in%20the%20fine-grained%0Asetting.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20novel%20dataset%20that%20captures%20both%0Afine-grained%20and%20composed%20actions%20across%20diverse%20video%20segments%2C%20enabling%20more%0Adetailed%20compositional%20changes%20in%20retrieved%20video%20content.%20The%20proposed%0Adataset%2C%20named%20Dense-WebVid-CoVR%2C%20consists%20of%201.6%20million%20samples%20with%20dense%0Amodification%20text%20that%20is%20around%20seven%20times%20more%20than%20its%20existing%0Acounterpart.%20We%20further%20develop%20a%20new%20model%20that%20integrates%20visual%20and%20textual%0Ainformation%20through%20Cross-Attention%20%28CA%29%20fusion%20using%20grounded%20text%20encoder%2C%0Aenabling%20precise%20alignment%20between%20dense%20query%20modifications%20and%20target%20videos.%0AThe%20proposed%20model%20achieves%20state-of-the-art%20results%20surpassing%20existing%0Amethods%20on%20all%20metrics.%20Notably%2C%20it%20achieves%2071.3%5C%25%20Recall%401%20in%20visual%2Btext%0Asetting%20and%20outperforms%20the%20state-of-the-art%20by%203.4%5C%25%2C%20highlighting%20its%0Aefficacy%20in%20terms%20of%20leveraging%20detailed%20video%20descriptions%20and%20dense%0Amodification%20texts.%20Our%20proposed%20dataset%2C%20code%2C%20and%20model%20are%20available%20at%0A%3Ahttps%3A//github.com/OmkarThawakar/BSE-CoVR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Simple%2520Edits%253A%2520Composed%2520Video%2520Retrieval%2520with%2520Dense%2520Modifications%26entry.906535625%3DOmkar%2520Thawakar%2520and%2520Dmitry%2520Demidov%2520and%2520Ritesh%2520Thawkar%2520and%2520Rao%2520Muhammad%2520Anwer%2520and%2520Mubarak%2520Shah%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Salman%2520Khan%26entry.1292438233%3D%2520%2520Composed%2520video%2520retrieval%2520is%2520a%2520challenging%2520task%2520that%2520strives%2520to%2520retrieve%2520a%250Atarget%2520video%2520based%2520on%2520a%2520query%2520video%2520and%2520a%2520textual%2520description%2520detailing%250Aspecific%2520modifications.%2520Standard%2520retrieval%2520frameworks%2520typically%2520struggle%2520to%250Ahandle%2520the%2520complexity%2520of%2520fine-grained%2520compositional%2520queries%2520and%2520variations%2520in%250Atemporal%2520understanding%2520limiting%2520their%2520retrieval%2520ability%2520in%2520the%2520fine-grained%250Asetting.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520novel%2520dataset%2520that%2520captures%2520both%250Afine-grained%2520and%2520composed%2520actions%2520across%2520diverse%2520video%2520segments%252C%2520enabling%2520more%250Adetailed%2520compositional%2520changes%2520in%2520retrieved%2520video%2520content.%2520The%2520proposed%250Adataset%252C%2520named%2520Dense-WebVid-CoVR%252C%2520consists%2520of%25201.6%2520million%2520samples%2520with%2520dense%250Amodification%2520text%2520that%2520is%2520around%2520seven%2520times%2520more%2520than%2520its%2520existing%250Acounterpart.%2520We%2520further%2520develop%2520a%2520new%2520model%2520that%2520integrates%2520visual%2520and%2520textual%250Ainformation%2520through%2520Cross-Attention%2520%2528CA%2529%2520fusion%2520using%2520grounded%2520text%2520encoder%252C%250Aenabling%2520precise%2520alignment%2520between%2520dense%2520query%2520modifications%2520and%2520target%2520videos.%250AThe%2520proposed%2520model%2520achieves%2520state-of-the-art%2520results%2520surpassing%2520existing%250Amethods%2520on%2520all%2520metrics.%2520Notably%252C%2520it%2520achieves%252071.3%255C%2525%2520Recall%25401%2520in%2520visual%252Btext%250Asetting%2520and%2520outperforms%2520the%2520state-of-the-art%2520by%25203.4%255C%2525%252C%2520highlighting%2520its%250Aefficacy%2520in%2520terms%2520of%2520leveraging%2520detailed%2520video%2520descriptions%2520and%2520dense%250Amodification%2520texts.%2520Our%2520proposed%2520dataset%252C%2520code%252C%2520and%2520model%2520are%2520available%2520at%250A%253Ahttps%253A//github.com/OmkarThawakar/BSE-CoVR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Simple%20Edits%3A%20Composed%20Video%20Retrieval%20with%20Dense%20Modifications&entry.906535625=Omkar%20Thawakar%20and%20Dmitry%20Demidov%20and%20Ritesh%20Thawkar%20and%20Rao%20Muhammad%20Anwer%20and%20Mubarak%20Shah%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan&entry.1292438233=%20%20Composed%20video%20retrieval%20is%20a%20challenging%20task%20that%20strives%20to%20retrieve%20a%0Atarget%20video%20based%20on%20a%20query%20video%20and%20a%20textual%20description%20detailing%0Aspecific%20modifications.%20Standard%20retrieval%20frameworks%20typically%20struggle%20to%0Ahandle%20the%20complexity%20of%20fine-grained%20compositional%20queries%20and%20variations%20in%0Atemporal%20understanding%20limiting%20their%20retrieval%20ability%20in%20the%20fine-grained%0Asetting.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20novel%20dataset%20that%20captures%20both%0Afine-grained%20and%20composed%20actions%20across%20diverse%20video%20segments%2C%20enabling%20more%0Adetailed%20compositional%20changes%20in%20retrieved%20video%20content.%20The%20proposed%0Adataset%2C%20named%20Dense-WebVid-CoVR%2C%20consists%20of%201.6%20million%20samples%20with%20dense%0Amodification%20text%20that%20is%20around%20seven%20times%20more%20than%20its%20existing%0Acounterpart.%20We%20further%20develop%20a%20new%20model%20that%20integrates%20visual%20and%20textual%0Ainformation%20through%20Cross-Attention%20%28CA%29%20fusion%20using%20grounded%20text%20encoder%2C%0Aenabling%20precise%20alignment%20between%20dense%20query%20modifications%20and%20target%20videos.%0AThe%20proposed%20model%20achieves%20state-of-the-art%20results%20surpassing%20existing%0Amethods%20on%20all%20metrics.%20Notably%2C%20it%20achieves%2071.3%5C%25%20Recall%401%20in%20visual%2Btext%0Asetting%20and%20outperforms%20the%20state-of-the-art%20by%203.4%5C%25%2C%20highlighting%20its%0Aefficacy%20in%20terms%20of%20leveraging%20detailed%20video%20descriptions%20and%20dense%0Amodification%20texts.%20Our%20proposed%20dataset%2C%20code%2C%20and%20model%20are%20available%20at%0A%3Ahttps%3A//github.com/OmkarThawakar/BSE-CoVR%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14039v1&entry.124074799=Read"},
{"title": "Prediction is not Explanation: Revisiting the Explanatory Capacity of\n  Mapping Embeddings", "author": "Hanna Herasimchyk and Alhassan Abdelhalim and S\u00f6ren Laue and Michaela Regneri", "abstract": "  Understanding what knowledge is implicitly encoded in deep learning models is\nessential for improving the interpretability of AI systems. This paper examines\ncommon methods to explain the knowledge encoded in word embeddings, which are\ncore elements of large language models (LLMs). These methods typically involve\nmapping embeddings onto collections of human-interpretable semantic features,\nknown as feature norms. Prior work assumes that accurately predicting these\nsemantic features from the word embeddings implies that the embeddings contain\nthe corresponding knowledge. We challenge this assumption by demonstrating that\nprediction accuracy alone does not reliably indicate genuine feature-based\ninterpretability.\n  We show that these methods can successfully predict even random information,\nconcluding that the results are predominantly determined by an algorithmic\nupper bound rather than meaningful semantic representation in the word\nembeddings. Consequently, comparisons between datasets based solely on\nprediction performance do not reliably indicate which dataset is better\ncaptured by the word embeddings. Our analysis illustrates that such mappings\nprimarily reflect geometric similarity within vector spaces rather than\nindicating the genuine emergence of semantic properties.\n", "link": "http://arxiv.org/abs/2508.13729v1", "date": "2025-08-19", "relevancy": 2.1878, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prediction%20is%20not%20Explanation%3A%20Revisiting%20the%20Explanatory%20Capacity%20of%0A%20%20Mapping%20Embeddings&body=Title%3A%20Prediction%20is%20not%20Explanation%3A%20Revisiting%20the%20Explanatory%20Capacity%20of%0A%20%20Mapping%20Embeddings%0AAuthor%3A%20Hanna%20Herasimchyk%20and%20Alhassan%20Abdelhalim%20and%20S%C3%B6ren%20Laue%20and%20Michaela%20Regneri%0AAbstract%3A%20%20%20Understanding%20what%20knowledge%20is%20implicitly%20encoded%20in%20deep%20learning%20models%20is%0Aessential%20for%20improving%20the%20interpretability%20of%20AI%20systems.%20This%20paper%20examines%0Acommon%20methods%20to%20explain%20the%20knowledge%20encoded%20in%20word%20embeddings%2C%20which%20are%0Acore%20elements%20of%20large%20language%20models%20%28LLMs%29.%20These%20methods%20typically%20involve%0Amapping%20embeddings%20onto%20collections%20of%20human-interpretable%20semantic%20features%2C%0Aknown%20as%20feature%20norms.%20Prior%20work%20assumes%20that%20accurately%20predicting%20these%0Asemantic%20features%20from%20the%20word%20embeddings%20implies%20that%20the%20embeddings%20contain%0Athe%20corresponding%20knowledge.%20We%20challenge%20this%20assumption%20by%20demonstrating%20that%0Aprediction%20accuracy%20alone%20does%20not%20reliably%20indicate%20genuine%20feature-based%0Ainterpretability.%0A%20%20We%20show%20that%20these%20methods%20can%20successfully%20predict%20even%20random%20information%2C%0Aconcluding%20that%20the%20results%20are%20predominantly%20determined%20by%20an%20algorithmic%0Aupper%20bound%20rather%20than%20meaningful%20semantic%20representation%20in%20the%20word%0Aembeddings.%20Consequently%2C%20comparisons%20between%20datasets%20based%20solely%20on%0Aprediction%20performance%20do%20not%20reliably%20indicate%20which%20dataset%20is%20better%0Acaptured%20by%20the%20word%20embeddings.%20Our%20analysis%20illustrates%20that%20such%20mappings%0Aprimarily%20reflect%20geometric%20similarity%20within%20vector%20spaces%20rather%20than%0Aindicating%20the%20genuine%20emergence%20of%20semantic%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrediction%2520is%2520not%2520Explanation%253A%2520Revisiting%2520the%2520Explanatory%2520Capacity%2520of%250A%2520%2520Mapping%2520Embeddings%26entry.906535625%3DHanna%2520Herasimchyk%2520and%2520Alhassan%2520Abdelhalim%2520and%2520S%25C3%25B6ren%2520Laue%2520and%2520Michaela%2520Regneri%26entry.1292438233%3D%2520%2520Understanding%2520what%2520knowledge%2520is%2520implicitly%2520encoded%2520in%2520deep%2520learning%2520models%2520is%250Aessential%2520for%2520improving%2520the%2520interpretability%2520of%2520AI%2520systems.%2520This%2520paper%2520examines%250Acommon%2520methods%2520to%2520explain%2520the%2520knowledge%2520encoded%2520in%2520word%2520embeddings%252C%2520which%2520are%250Acore%2520elements%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520These%2520methods%2520typically%2520involve%250Amapping%2520embeddings%2520onto%2520collections%2520of%2520human-interpretable%2520semantic%2520features%252C%250Aknown%2520as%2520feature%2520norms.%2520Prior%2520work%2520assumes%2520that%2520accurately%2520predicting%2520these%250Asemantic%2520features%2520from%2520the%2520word%2520embeddings%2520implies%2520that%2520the%2520embeddings%2520contain%250Athe%2520corresponding%2520knowledge.%2520We%2520challenge%2520this%2520assumption%2520by%2520demonstrating%2520that%250Aprediction%2520accuracy%2520alone%2520does%2520not%2520reliably%2520indicate%2520genuine%2520feature-based%250Ainterpretability.%250A%2520%2520We%2520show%2520that%2520these%2520methods%2520can%2520successfully%2520predict%2520even%2520random%2520information%252C%250Aconcluding%2520that%2520the%2520results%2520are%2520predominantly%2520determined%2520by%2520an%2520algorithmic%250Aupper%2520bound%2520rather%2520than%2520meaningful%2520semantic%2520representation%2520in%2520the%2520word%250Aembeddings.%2520Consequently%252C%2520comparisons%2520between%2520datasets%2520based%2520solely%2520on%250Aprediction%2520performance%2520do%2520not%2520reliably%2520indicate%2520which%2520dataset%2520is%2520better%250Acaptured%2520by%2520the%2520word%2520embeddings.%2520Our%2520analysis%2520illustrates%2520that%2520such%2520mappings%250Aprimarily%2520reflect%2520geometric%2520similarity%2520within%2520vector%2520spaces%2520rather%2520than%250Aindicating%2520the%2520genuine%2520emergence%2520of%2520semantic%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction%20is%20not%20Explanation%3A%20Revisiting%20the%20Explanatory%20Capacity%20of%0A%20%20Mapping%20Embeddings&entry.906535625=Hanna%20Herasimchyk%20and%20Alhassan%20Abdelhalim%20and%20S%C3%B6ren%20Laue%20and%20Michaela%20Regneri&entry.1292438233=%20%20Understanding%20what%20knowledge%20is%20implicitly%20encoded%20in%20deep%20learning%20models%20is%0Aessential%20for%20improving%20the%20interpretability%20of%20AI%20systems.%20This%20paper%20examines%0Acommon%20methods%20to%20explain%20the%20knowledge%20encoded%20in%20word%20embeddings%2C%20which%20are%0Acore%20elements%20of%20large%20language%20models%20%28LLMs%29.%20These%20methods%20typically%20involve%0Amapping%20embeddings%20onto%20collections%20of%20human-interpretable%20semantic%20features%2C%0Aknown%20as%20feature%20norms.%20Prior%20work%20assumes%20that%20accurately%20predicting%20these%0Asemantic%20features%20from%20the%20word%20embeddings%20implies%20that%20the%20embeddings%20contain%0Athe%20corresponding%20knowledge.%20We%20challenge%20this%20assumption%20by%20demonstrating%20that%0Aprediction%20accuracy%20alone%20does%20not%20reliably%20indicate%20genuine%20feature-based%0Ainterpretability.%0A%20%20We%20show%20that%20these%20methods%20can%20successfully%20predict%20even%20random%20information%2C%0Aconcluding%20that%20the%20results%20are%20predominantly%20determined%20by%20an%20algorithmic%0Aupper%20bound%20rather%20than%20meaningful%20semantic%20representation%20in%20the%20word%0Aembeddings.%20Consequently%2C%20comparisons%20between%20datasets%20based%20solely%20on%0Aprediction%20performance%20do%20not%20reliably%20indicate%20which%20dataset%20is%20better%0Acaptured%20by%20the%20word%20embeddings.%20Our%20analysis%20illustrates%20that%20such%20mappings%0Aprimarily%20reflect%20geometric%20similarity%20within%20vector%20spaces%20rather%20than%0Aindicating%20the%20genuine%20emergence%20of%20semantic%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13729v1&entry.124074799=Read"},
{"title": "Efficient Knowledge Graph Unlearning with Zeroth-order Information", "author": "Yang Xiao and Ruimeng Ye and Bohan Liu and Xiaolong Ma and Bo Hui", "abstract": "  Due to regulations like the Right to be Forgotten, there is growing demand\nfor removing training data and its influence from models. Since full retraining\nis costly, various machine unlearning methods have been proposed. In this\npaper, we firstly present an efficient knowledge graph (KG) unlearning\nalgorithm. We remark that KG unlearning is nontrivial due to the distinctive\nstructure of KG and the semantic relations between entities. Also, unlearning\nby estimating the influence of removed components incurs significant\ncomputational overhead when applied to large-scale knowledge graphs. To this\nend, we define an influence function for KG unlearning and propose to\napproximate the model's sensitivity without expensive computation of\nfirst-order and second-order derivatives for parameter updates. Specifically,\nwe use Taylor expansion to estimate the parameter changes caused by data\nremoval. Given that the first-order gradients and second-order derivatives\ndominate the computational load, we use the Fisher matrices and zeroth-order\noptimization to approximate the inverse-Hessian vector product without\nconstructing the computational graphs. Our experimental results demonstrate\nthat the proposed method outperforms other state-of-the-art graph unlearning\nbaselines significantly in terms of unlearning efficiency and unlearning\nquality. Our code is released at https://github.com/NKUShaw/ZOWFKGIF.\n", "link": "http://arxiv.org/abs/2508.14013v1", "date": "2025-08-19", "relevancy": 2.1875, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4451}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4406}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Knowledge%20Graph%20Unlearning%20with%20Zeroth-order%20Information&body=Title%3A%20Efficient%20Knowledge%20Graph%20Unlearning%20with%20Zeroth-order%20Information%0AAuthor%3A%20Yang%20Xiao%20and%20Ruimeng%20Ye%20and%20Bohan%20Liu%20and%20Xiaolong%20Ma%20and%20Bo%20Hui%0AAbstract%3A%20%20%20Due%20to%20regulations%20like%20the%20Right%20to%20be%20Forgotten%2C%20there%20is%20growing%20demand%0Afor%20removing%20training%20data%20and%20its%20influence%20from%20models.%20Since%20full%20retraining%0Ais%20costly%2C%20various%20machine%20unlearning%20methods%20have%20been%20proposed.%20In%20this%0Apaper%2C%20we%20firstly%20present%20an%20efficient%20knowledge%20graph%20%28KG%29%20unlearning%0Aalgorithm.%20We%20remark%20that%20KG%20unlearning%20is%20nontrivial%20due%20to%20the%20distinctive%0Astructure%20of%20KG%20and%20the%20semantic%20relations%20between%20entities.%20Also%2C%20unlearning%0Aby%20estimating%20the%20influence%20of%20removed%20components%20incurs%20significant%0Acomputational%20overhead%20when%20applied%20to%20large-scale%20knowledge%20graphs.%20To%20this%0Aend%2C%20we%20define%20an%20influence%20function%20for%20KG%20unlearning%20and%20propose%20to%0Aapproximate%20the%20model%27s%20sensitivity%20without%20expensive%20computation%20of%0Afirst-order%20and%20second-order%20derivatives%20for%20parameter%20updates.%20Specifically%2C%0Awe%20use%20Taylor%20expansion%20to%20estimate%20the%20parameter%20changes%20caused%20by%20data%0Aremoval.%20Given%20that%20the%20first-order%20gradients%20and%20second-order%20derivatives%0Adominate%20the%20computational%20load%2C%20we%20use%20the%20Fisher%20matrices%20and%20zeroth-order%0Aoptimization%20to%20approximate%20the%20inverse-Hessian%20vector%20product%20without%0Aconstructing%20the%20computational%20graphs.%20Our%20experimental%20results%20demonstrate%0Athat%20the%20proposed%20method%20outperforms%20other%20state-of-the-art%20graph%20unlearning%0Abaselines%20significantly%20in%20terms%20of%20unlearning%20efficiency%20and%20unlearning%0Aquality.%20Our%20code%20is%20released%20at%20https%3A//github.com/NKUShaw/ZOWFKGIF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Knowledge%2520Graph%2520Unlearning%2520with%2520Zeroth-order%2520Information%26entry.906535625%3DYang%2520Xiao%2520and%2520Ruimeng%2520Ye%2520and%2520Bohan%2520Liu%2520and%2520Xiaolong%2520Ma%2520and%2520Bo%2520Hui%26entry.1292438233%3D%2520%2520Due%2520to%2520regulations%2520like%2520the%2520Right%2520to%2520be%2520Forgotten%252C%2520there%2520is%2520growing%2520demand%250Afor%2520removing%2520training%2520data%2520and%2520its%2520influence%2520from%2520models.%2520Since%2520full%2520retraining%250Ais%2520costly%252C%2520various%2520machine%2520unlearning%2520methods%2520have%2520been%2520proposed.%2520In%2520this%250Apaper%252C%2520we%2520firstly%2520present%2520an%2520efficient%2520knowledge%2520graph%2520%2528KG%2529%2520unlearning%250Aalgorithm.%2520We%2520remark%2520that%2520KG%2520unlearning%2520is%2520nontrivial%2520due%2520to%2520the%2520distinctive%250Astructure%2520of%2520KG%2520and%2520the%2520semantic%2520relations%2520between%2520entities.%2520Also%252C%2520unlearning%250Aby%2520estimating%2520the%2520influence%2520of%2520removed%2520components%2520incurs%2520significant%250Acomputational%2520overhead%2520when%2520applied%2520to%2520large-scale%2520knowledge%2520graphs.%2520To%2520this%250Aend%252C%2520we%2520define%2520an%2520influence%2520function%2520for%2520KG%2520unlearning%2520and%2520propose%2520to%250Aapproximate%2520the%2520model%2527s%2520sensitivity%2520without%2520expensive%2520computation%2520of%250Afirst-order%2520and%2520second-order%2520derivatives%2520for%2520parameter%2520updates.%2520Specifically%252C%250Awe%2520use%2520Taylor%2520expansion%2520to%2520estimate%2520the%2520parameter%2520changes%2520caused%2520by%2520data%250Aremoval.%2520Given%2520that%2520the%2520first-order%2520gradients%2520and%2520second-order%2520derivatives%250Adominate%2520the%2520computational%2520load%252C%2520we%2520use%2520the%2520Fisher%2520matrices%2520and%2520zeroth-order%250Aoptimization%2520to%2520approximate%2520the%2520inverse-Hessian%2520vector%2520product%2520without%250Aconstructing%2520the%2520computational%2520graphs.%2520Our%2520experimental%2520results%2520demonstrate%250Athat%2520the%2520proposed%2520method%2520outperforms%2520other%2520state-of-the-art%2520graph%2520unlearning%250Abaselines%2520significantly%2520in%2520terms%2520of%2520unlearning%2520efficiency%2520and%2520unlearning%250Aquality.%2520Our%2520code%2520is%2520released%2520at%2520https%253A//github.com/NKUShaw/ZOWFKGIF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Knowledge%20Graph%20Unlearning%20with%20Zeroth-order%20Information&entry.906535625=Yang%20Xiao%20and%20Ruimeng%20Ye%20and%20Bohan%20Liu%20and%20Xiaolong%20Ma%20and%20Bo%20Hui&entry.1292438233=%20%20Due%20to%20regulations%20like%20the%20Right%20to%20be%20Forgotten%2C%20there%20is%20growing%20demand%0Afor%20removing%20training%20data%20and%20its%20influence%20from%20models.%20Since%20full%20retraining%0Ais%20costly%2C%20various%20machine%20unlearning%20methods%20have%20been%20proposed.%20In%20this%0Apaper%2C%20we%20firstly%20present%20an%20efficient%20knowledge%20graph%20%28KG%29%20unlearning%0Aalgorithm.%20We%20remark%20that%20KG%20unlearning%20is%20nontrivial%20due%20to%20the%20distinctive%0Astructure%20of%20KG%20and%20the%20semantic%20relations%20between%20entities.%20Also%2C%20unlearning%0Aby%20estimating%20the%20influence%20of%20removed%20components%20incurs%20significant%0Acomputational%20overhead%20when%20applied%20to%20large-scale%20knowledge%20graphs.%20To%20this%0Aend%2C%20we%20define%20an%20influence%20function%20for%20KG%20unlearning%20and%20propose%20to%0Aapproximate%20the%20model%27s%20sensitivity%20without%20expensive%20computation%20of%0Afirst-order%20and%20second-order%20derivatives%20for%20parameter%20updates.%20Specifically%2C%0Awe%20use%20Taylor%20expansion%20to%20estimate%20the%20parameter%20changes%20caused%20by%20data%0Aremoval.%20Given%20that%20the%20first-order%20gradients%20and%20second-order%20derivatives%0Adominate%20the%20computational%20load%2C%20we%20use%20the%20Fisher%20matrices%20and%20zeroth-order%0Aoptimization%20to%20approximate%20the%20inverse-Hessian%20vector%20product%20without%0Aconstructing%20the%20computational%20graphs.%20Our%20experimental%20results%20demonstrate%0Athat%20the%20proposed%20method%20outperforms%20other%20state-of-the-art%20graph%20unlearning%0Abaselines%20significantly%20in%20terms%20of%20unlearning%20efficiency%20and%20unlearning%0Aquality.%20Our%20code%20is%20released%20at%20https%3A//github.com/NKUShaw/ZOWFKGIF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14013v1&entry.124074799=Read"},
{"title": "LEGO: Learning and Graph-Optimized Modular Tracker for Online\n  Multi-Object Tracking with Point Clouds", "author": "Zhenrong Zhang and Jianan Liu and Yuxuan Xia and Tao Huang and Qing-Long Han and Hongbin Liu", "abstract": "  Online multi-object tracking (MOT) plays a pivotal role in autonomous\nsystems. The state-of-the-art approaches usually employ a tracking-by-detection\nmethod, and data association plays a critical role. This paper proposes a\nlearning and graph-optimized (LEGO) modular tracker to improve data association\nperformance in the existing literature. The proposed LEGO tracker integrates\ngraph optimization and self-attention mechanisms, which efficiently formulate\nthe association score map, facilitating the accurate and efficient matching of\nobjects across time frames. To further enhance the state update process, the\nKalman filter is added to ensure consistent tracking by incorporating temporal\ncoherence in the object states. Our proposed method utilizing LiDAR alone has\nshown exceptional performance compared to other online tracking approaches,\nincluding LiDAR-based and LiDAR-camera fusion-based methods. LEGO ranked 1st at\nthe time of submitting results to KITTI object tracking evaluation ranking\nboard and remains 2nd at the time of submitting this paper, among all online\ntrackers in the KITTI MOT benchmark for cars1\n", "link": "http://arxiv.org/abs/2308.09908v5", "date": "2025-08-19", "relevancy": 2.1862, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5752}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5452}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEGO%3A%20Learning%20and%20Graph-Optimized%20Modular%20Tracker%20for%20Online%0A%20%20Multi-Object%20Tracking%20with%20Point%20Clouds&body=Title%3A%20LEGO%3A%20Learning%20and%20Graph-Optimized%20Modular%20Tracker%20for%20Online%0A%20%20Multi-Object%20Tracking%20with%20Point%20Clouds%0AAuthor%3A%20Zhenrong%20Zhang%20and%20Jianan%20Liu%20and%20Yuxuan%20Xia%20and%20Tao%20Huang%20and%20Qing-Long%20Han%20and%20Hongbin%20Liu%0AAbstract%3A%20%20%20Online%20multi-object%20tracking%20%28MOT%29%20plays%20a%20pivotal%20role%20in%20autonomous%0Asystems.%20The%20state-of-the-art%20approaches%20usually%20employ%20a%20tracking-by-detection%0Amethod%2C%20and%20data%20association%20plays%20a%20critical%20role.%20This%20paper%20proposes%20a%0Alearning%20and%20graph-optimized%20%28LEGO%29%20modular%20tracker%20to%20improve%20data%20association%0Aperformance%20in%20the%20existing%20literature.%20The%20proposed%20LEGO%20tracker%20integrates%0Agraph%20optimization%20and%20self-attention%20mechanisms%2C%20which%20efficiently%20formulate%0Athe%20association%20score%20map%2C%20facilitating%20the%20accurate%20and%20efficient%20matching%20of%0Aobjects%20across%20time%20frames.%20To%20further%20enhance%20the%20state%20update%20process%2C%20the%0AKalman%20filter%20is%20added%20to%20ensure%20consistent%20tracking%20by%20incorporating%20temporal%0Acoherence%20in%20the%20object%20states.%20Our%20proposed%20method%20utilizing%20LiDAR%20alone%20has%0Ashown%20exceptional%20performance%20compared%20to%20other%20online%20tracking%20approaches%2C%0Aincluding%20LiDAR-based%20and%20LiDAR-camera%20fusion-based%20methods.%20LEGO%20ranked%201st%20at%0Athe%20time%20of%20submitting%20results%20to%20KITTI%20object%20tracking%20evaluation%20ranking%0Aboard%20and%20remains%202nd%20at%20the%20time%20of%20submitting%20this%20paper%2C%20among%20all%20online%0Atrackers%20in%20the%20KITTI%20MOT%20benchmark%20for%20cars1%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.09908v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEGO%253A%2520Learning%2520and%2520Graph-Optimized%2520Modular%2520Tracker%2520for%2520Online%250A%2520%2520Multi-Object%2520Tracking%2520with%2520Point%2520Clouds%26entry.906535625%3DZhenrong%2520Zhang%2520and%2520Jianan%2520Liu%2520and%2520Yuxuan%2520Xia%2520and%2520Tao%2520Huang%2520and%2520Qing-Long%2520Han%2520and%2520Hongbin%2520Liu%26entry.1292438233%3D%2520%2520Online%2520multi-object%2520tracking%2520%2528MOT%2529%2520plays%2520a%2520pivotal%2520role%2520in%2520autonomous%250Asystems.%2520The%2520state-of-the-art%2520approaches%2520usually%2520employ%2520a%2520tracking-by-detection%250Amethod%252C%2520and%2520data%2520association%2520plays%2520a%2520critical%2520role.%2520This%2520paper%2520proposes%2520a%250Alearning%2520and%2520graph-optimized%2520%2528LEGO%2529%2520modular%2520tracker%2520to%2520improve%2520data%2520association%250Aperformance%2520in%2520the%2520existing%2520literature.%2520The%2520proposed%2520LEGO%2520tracker%2520integrates%250Agraph%2520optimization%2520and%2520self-attention%2520mechanisms%252C%2520which%2520efficiently%2520formulate%250Athe%2520association%2520score%2520map%252C%2520facilitating%2520the%2520accurate%2520and%2520efficient%2520matching%2520of%250Aobjects%2520across%2520time%2520frames.%2520To%2520further%2520enhance%2520the%2520state%2520update%2520process%252C%2520the%250AKalman%2520filter%2520is%2520added%2520to%2520ensure%2520consistent%2520tracking%2520by%2520incorporating%2520temporal%250Acoherence%2520in%2520the%2520object%2520states.%2520Our%2520proposed%2520method%2520utilizing%2520LiDAR%2520alone%2520has%250Ashown%2520exceptional%2520performance%2520compared%2520to%2520other%2520online%2520tracking%2520approaches%252C%250Aincluding%2520LiDAR-based%2520and%2520LiDAR-camera%2520fusion-based%2520methods.%2520LEGO%2520ranked%25201st%2520at%250Athe%2520time%2520of%2520submitting%2520results%2520to%2520KITTI%2520object%2520tracking%2520evaluation%2520ranking%250Aboard%2520and%2520remains%25202nd%2520at%2520the%2520time%2520of%2520submitting%2520this%2520paper%252C%2520among%2520all%2520online%250Atrackers%2520in%2520the%2520KITTI%2520MOT%2520benchmark%2520for%2520cars1%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.09908v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEGO%3A%20Learning%20and%20Graph-Optimized%20Modular%20Tracker%20for%20Online%0A%20%20Multi-Object%20Tracking%20with%20Point%20Clouds&entry.906535625=Zhenrong%20Zhang%20and%20Jianan%20Liu%20and%20Yuxuan%20Xia%20and%20Tao%20Huang%20and%20Qing-Long%20Han%20and%20Hongbin%20Liu&entry.1292438233=%20%20Online%20multi-object%20tracking%20%28MOT%29%20plays%20a%20pivotal%20role%20in%20autonomous%0Asystems.%20The%20state-of-the-art%20approaches%20usually%20employ%20a%20tracking-by-detection%0Amethod%2C%20and%20data%20association%20plays%20a%20critical%20role.%20This%20paper%20proposes%20a%0Alearning%20and%20graph-optimized%20%28LEGO%29%20modular%20tracker%20to%20improve%20data%20association%0Aperformance%20in%20the%20existing%20literature.%20The%20proposed%20LEGO%20tracker%20integrates%0Agraph%20optimization%20and%20self-attention%20mechanisms%2C%20which%20efficiently%20formulate%0Athe%20association%20score%20map%2C%20facilitating%20the%20accurate%20and%20efficient%20matching%20of%0Aobjects%20across%20time%20frames.%20To%20further%20enhance%20the%20state%20update%20process%2C%20the%0AKalman%20filter%20is%20added%20to%20ensure%20consistent%20tracking%20by%20incorporating%20temporal%0Acoherence%20in%20the%20object%20states.%20Our%20proposed%20method%20utilizing%20LiDAR%20alone%20has%0Ashown%20exceptional%20performance%20compared%20to%20other%20online%20tracking%20approaches%2C%0Aincluding%20LiDAR-based%20and%20LiDAR-camera%20fusion-based%20methods.%20LEGO%20ranked%201st%20at%0Athe%20time%20of%20submitting%20results%20to%20KITTI%20object%20tracking%20evaluation%20ranking%0Aboard%20and%20remains%202nd%20at%20the%20time%20of%20submitting%20this%20paper%2C%20among%20all%20online%0Atrackers%20in%20the%20KITTI%20MOT%20benchmark%20for%20cars1%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.09908v5&entry.124074799=Read"},
{"title": "Physics-Based 3D Simulation for Synthetic Data Generation and Failure\n  Analysis in Packaging Stability Assessment", "author": "Samuel Seligardi and Pietro Musoni and Eleonora Iotti and Gianluca Contesso and Alessandro Dal Pal\u00f9", "abstract": "  The design and analysis of pallet setups are essential for ensuring safety of\npackages transportation. With rising demands in the logistics sector, the\ndevelopment of automated systems utilizing advanced technologies has become\nincreasingly crucial. Moreover, the widespread use of plastic wrapping has\nmotivated researchers to investigate eco-friendly alternatives that still\nadhere to safety standards. We present a fully controllable and accurate\nphysical simulation system capable of replicating the behavior of moving\npallets. It features a 3D graphics-based virtual environment that supports a\nwide range of configurations, including variable package layouts, different\nwrapping materials, and diverse dynamic conditions. This innovative approach\nreduces the need for physical testing, cutting costs and environmental impact\nwhile improving measurement accuracy for analyzing pallet dynamics.\nAdditionally, we train a deep neural network to evaluate the rendered videos\ngenerated by our simulator, as a crash-test predictor for pallet\nconfigurations, further enhancing the system's utility in safety analysis.\n", "link": "http://arxiv.org/abs/2508.13989v1", "date": "2025-08-19", "relevancy": 2.1775, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5591}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5359}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Based%203D%20Simulation%20for%20Synthetic%20Data%20Generation%20and%20Failure%0A%20%20Analysis%20in%20Packaging%20Stability%20Assessment&body=Title%3A%20Physics-Based%203D%20Simulation%20for%20Synthetic%20Data%20Generation%20and%20Failure%0A%20%20Analysis%20in%20Packaging%20Stability%20Assessment%0AAuthor%3A%20Samuel%20Seligardi%20and%20Pietro%20Musoni%20and%20Eleonora%20Iotti%20and%20Gianluca%20Contesso%20and%20Alessandro%20Dal%20Pal%C3%B9%0AAbstract%3A%20%20%20The%20design%20and%20analysis%20of%20pallet%20setups%20are%20essential%20for%20ensuring%20safety%20of%0Apackages%20transportation.%20With%20rising%20demands%20in%20the%20logistics%20sector%2C%20the%0Adevelopment%20of%20automated%20systems%20utilizing%20advanced%20technologies%20has%20become%0Aincreasingly%20crucial.%20Moreover%2C%20the%20widespread%20use%20of%20plastic%20wrapping%20has%0Amotivated%20researchers%20to%20investigate%20eco-friendly%20alternatives%20that%20still%0Aadhere%20to%20safety%20standards.%20We%20present%20a%20fully%20controllable%20and%20accurate%0Aphysical%20simulation%20system%20capable%20of%20replicating%20the%20behavior%20of%20moving%0Apallets.%20It%20features%20a%203D%20graphics-based%20virtual%20environment%20that%20supports%20a%0Awide%20range%20of%20configurations%2C%20including%20variable%20package%20layouts%2C%20different%0Awrapping%20materials%2C%20and%20diverse%20dynamic%20conditions.%20This%20innovative%20approach%0Areduces%20the%20need%20for%20physical%20testing%2C%20cutting%20costs%20and%20environmental%20impact%0Awhile%20improving%20measurement%20accuracy%20for%20analyzing%20pallet%20dynamics.%0AAdditionally%2C%20we%20train%20a%20deep%20neural%20network%20to%20evaluate%20the%20rendered%20videos%0Agenerated%20by%20our%20simulator%2C%20as%20a%20crash-test%20predictor%20for%20pallet%0Aconfigurations%2C%20further%20enhancing%20the%20system%27s%20utility%20in%20safety%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Based%25203D%2520Simulation%2520for%2520Synthetic%2520Data%2520Generation%2520and%2520Failure%250A%2520%2520Analysis%2520in%2520Packaging%2520Stability%2520Assessment%26entry.906535625%3DSamuel%2520Seligardi%2520and%2520Pietro%2520Musoni%2520and%2520Eleonora%2520Iotti%2520and%2520Gianluca%2520Contesso%2520and%2520Alessandro%2520Dal%2520Pal%25C3%25B9%26entry.1292438233%3D%2520%2520The%2520design%2520and%2520analysis%2520of%2520pallet%2520setups%2520are%2520essential%2520for%2520ensuring%2520safety%2520of%250Apackages%2520transportation.%2520With%2520rising%2520demands%2520in%2520the%2520logistics%2520sector%252C%2520the%250Adevelopment%2520of%2520automated%2520systems%2520utilizing%2520advanced%2520technologies%2520has%2520become%250Aincreasingly%2520crucial.%2520Moreover%252C%2520the%2520widespread%2520use%2520of%2520plastic%2520wrapping%2520has%250Amotivated%2520researchers%2520to%2520investigate%2520eco-friendly%2520alternatives%2520that%2520still%250Aadhere%2520to%2520safety%2520standards.%2520We%2520present%2520a%2520fully%2520controllable%2520and%2520accurate%250Aphysical%2520simulation%2520system%2520capable%2520of%2520replicating%2520the%2520behavior%2520of%2520moving%250Apallets.%2520It%2520features%2520a%25203D%2520graphics-based%2520virtual%2520environment%2520that%2520supports%2520a%250Awide%2520range%2520of%2520configurations%252C%2520including%2520variable%2520package%2520layouts%252C%2520different%250Awrapping%2520materials%252C%2520and%2520diverse%2520dynamic%2520conditions.%2520This%2520innovative%2520approach%250Areduces%2520the%2520need%2520for%2520physical%2520testing%252C%2520cutting%2520costs%2520and%2520environmental%2520impact%250Awhile%2520improving%2520measurement%2520accuracy%2520for%2520analyzing%2520pallet%2520dynamics.%250AAdditionally%252C%2520we%2520train%2520a%2520deep%2520neural%2520network%2520to%2520evaluate%2520the%2520rendered%2520videos%250Agenerated%2520by%2520our%2520simulator%252C%2520as%2520a%2520crash-test%2520predictor%2520for%2520pallet%250Aconfigurations%252C%2520further%2520enhancing%2520the%2520system%2527s%2520utility%2520in%2520safety%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Based%203D%20Simulation%20for%20Synthetic%20Data%20Generation%20and%20Failure%0A%20%20Analysis%20in%20Packaging%20Stability%20Assessment&entry.906535625=Samuel%20Seligardi%20and%20Pietro%20Musoni%20and%20Eleonora%20Iotti%20and%20Gianluca%20Contesso%20and%20Alessandro%20Dal%20Pal%C3%B9&entry.1292438233=%20%20The%20design%20and%20analysis%20of%20pallet%20setups%20are%20essential%20for%20ensuring%20safety%20of%0Apackages%20transportation.%20With%20rising%20demands%20in%20the%20logistics%20sector%2C%20the%0Adevelopment%20of%20automated%20systems%20utilizing%20advanced%20technologies%20has%20become%0Aincreasingly%20crucial.%20Moreover%2C%20the%20widespread%20use%20of%20plastic%20wrapping%20has%0Amotivated%20researchers%20to%20investigate%20eco-friendly%20alternatives%20that%20still%0Aadhere%20to%20safety%20standards.%20We%20present%20a%20fully%20controllable%20and%20accurate%0Aphysical%20simulation%20system%20capable%20of%20replicating%20the%20behavior%20of%20moving%0Apallets.%20It%20features%20a%203D%20graphics-based%20virtual%20environment%20that%20supports%20a%0Awide%20range%20of%20configurations%2C%20including%20variable%20package%20layouts%2C%20different%0Awrapping%20materials%2C%20and%20diverse%20dynamic%20conditions.%20This%20innovative%20approach%0Areduces%20the%20need%20for%20physical%20testing%2C%20cutting%20costs%20and%20environmental%20impact%0Awhile%20improving%20measurement%20accuracy%20for%20analyzing%20pallet%20dynamics.%0AAdditionally%2C%20we%20train%20a%20deep%20neural%20network%20to%20evaluate%20the%20rendered%20videos%0Agenerated%20by%20our%20simulator%2C%20as%20a%20crash-test%20predictor%20for%20pallet%0Aconfigurations%2C%20further%20enhancing%20the%20system%27s%20utility%20in%20safety%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13989v1&entry.124074799=Read"},
{"title": "SCRNet: Spatial-Channel Regulation Network for Medical Ultrasound Image\n  Segmentation", "author": "Weixin Xu and Ziliang Wang", "abstract": "  Medical ultrasound image segmentation presents a formidable challenge in the\nrealm of computer vision. Traditional approaches rely on Convolutional Neural\nNetworks (CNNs) and Transformer-based methods to address the intricacies of\nmedical image segmentation. Nevertheless, inherent limitations persist, as\nCNN-based methods tend to disregard long-range dependencies, while\nTransformer-based methods may overlook local contextual information. To address\nthese deficiencies, we propose a novel Feature Aggregation Module (FAM)\ndesigned to process two input features from the preceding layer. These features\nare seamlessly directed into two branches of the Convolution and\nCross-Attention Parallel Module (CCAPM) to endow them with different roles in\neach of the two branches to help establish a strong connection between the two\ninput features. This strategy enables our module to focus concurrently on both\nlong-range dependencies and local contextual information by judiciously merging\nconvolution operations with cross-attention mechanisms. Moreover, by\nintegrating FAM within our proposed Spatial-Channel Regulation Module (SCRM),\nthe ability to discern salient regions and informative features warranting\nincreased attention is enhanced. Furthermore, by incorporating the SCRM into\nthe encoder block of the UNet architecture, we introduce a novel framework\ndubbed Spatial-Channel Regulation Network (SCRNet). The results of our\nextensive experiments demonstrate the superiority of SCRNet, which consistently\nachieves state-of-the-art (SOTA) performance compared to existing methods.\n", "link": "http://arxiv.org/abs/2508.13899v1", "date": "2025-08-19", "relevancy": 2.1674, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5498}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5481}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCRNet%3A%20Spatial-Channel%20Regulation%20Network%20for%20Medical%20Ultrasound%20Image%0A%20%20Segmentation&body=Title%3A%20SCRNet%3A%20Spatial-Channel%20Regulation%20Network%20for%20Medical%20Ultrasound%20Image%0A%20%20Segmentation%0AAuthor%3A%20Weixin%20Xu%20and%20Ziliang%20Wang%0AAbstract%3A%20%20%20Medical%20ultrasound%20image%20segmentation%20presents%20a%20formidable%20challenge%20in%20the%0Arealm%20of%20computer%20vision.%20Traditional%20approaches%20rely%20on%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20and%20Transformer-based%20methods%20to%20address%20the%20intricacies%20of%0Amedical%20image%20segmentation.%20Nevertheless%2C%20inherent%20limitations%20persist%2C%20as%0ACNN-based%20methods%20tend%20to%20disregard%20long-range%20dependencies%2C%20while%0ATransformer-based%20methods%20may%20overlook%20local%20contextual%20information.%20To%20address%0Athese%20deficiencies%2C%20we%20propose%20a%20novel%20Feature%20Aggregation%20Module%20%28FAM%29%0Adesigned%20to%20process%20two%20input%20features%20from%20the%20preceding%20layer.%20These%20features%0Aare%20seamlessly%20directed%20into%20two%20branches%20of%20the%20Convolution%20and%0ACross-Attention%20Parallel%20Module%20%28CCAPM%29%20to%20endow%20them%20with%20different%20roles%20in%0Aeach%20of%20the%20two%20branches%20to%20help%20establish%20a%20strong%20connection%20between%20the%20two%0Ainput%20features.%20This%20strategy%20enables%20our%20module%20to%20focus%20concurrently%20on%20both%0Along-range%20dependencies%20and%20local%20contextual%20information%20by%20judiciously%20merging%0Aconvolution%20operations%20with%20cross-attention%20mechanisms.%20Moreover%2C%20by%0Aintegrating%20FAM%20within%20our%20proposed%20Spatial-Channel%20Regulation%20Module%20%28SCRM%29%2C%0Athe%20ability%20to%20discern%20salient%20regions%20and%20informative%20features%20warranting%0Aincreased%20attention%20is%20enhanced.%20Furthermore%2C%20by%20incorporating%20the%20SCRM%20into%0Athe%20encoder%20block%20of%20the%20UNet%20architecture%2C%20we%20introduce%20a%20novel%20framework%0Adubbed%20Spatial-Channel%20Regulation%20Network%20%28SCRNet%29.%20The%20results%20of%20our%0Aextensive%20experiments%20demonstrate%20the%20superiority%20of%20SCRNet%2C%20which%20consistently%0Aachieves%20state-of-the-art%20%28SOTA%29%20performance%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCRNet%253A%2520Spatial-Channel%2520Regulation%2520Network%2520for%2520Medical%2520Ultrasound%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DWeixin%2520Xu%2520and%2520Ziliang%2520Wang%26entry.1292438233%3D%2520%2520Medical%2520ultrasound%2520image%2520segmentation%2520presents%2520a%2520formidable%2520challenge%2520in%2520the%250Arealm%2520of%2520computer%2520vision.%2520Traditional%2520approaches%2520rely%2520on%2520Convolutional%2520Neural%250ANetworks%2520%2528CNNs%2529%2520and%2520Transformer-based%2520methods%2520to%2520address%2520the%2520intricacies%2520of%250Amedical%2520image%2520segmentation.%2520Nevertheless%252C%2520inherent%2520limitations%2520persist%252C%2520as%250ACNN-based%2520methods%2520tend%2520to%2520disregard%2520long-range%2520dependencies%252C%2520while%250ATransformer-based%2520methods%2520may%2520overlook%2520local%2520contextual%2520information.%2520To%2520address%250Athese%2520deficiencies%252C%2520we%2520propose%2520a%2520novel%2520Feature%2520Aggregation%2520Module%2520%2528FAM%2529%250Adesigned%2520to%2520process%2520two%2520input%2520features%2520from%2520the%2520preceding%2520layer.%2520These%2520features%250Aare%2520seamlessly%2520directed%2520into%2520two%2520branches%2520of%2520the%2520Convolution%2520and%250ACross-Attention%2520Parallel%2520Module%2520%2528CCAPM%2529%2520to%2520endow%2520them%2520with%2520different%2520roles%2520in%250Aeach%2520of%2520the%2520two%2520branches%2520to%2520help%2520establish%2520a%2520strong%2520connection%2520between%2520the%2520two%250Ainput%2520features.%2520This%2520strategy%2520enables%2520our%2520module%2520to%2520focus%2520concurrently%2520on%2520both%250Along-range%2520dependencies%2520and%2520local%2520contextual%2520information%2520by%2520judiciously%2520merging%250Aconvolution%2520operations%2520with%2520cross-attention%2520mechanisms.%2520Moreover%252C%2520by%250Aintegrating%2520FAM%2520within%2520our%2520proposed%2520Spatial-Channel%2520Regulation%2520Module%2520%2528SCRM%2529%252C%250Athe%2520ability%2520to%2520discern%2520salient%2520regions%2520and%2520informative%2520features%2520warranting%250Aincreased%2520attention%2520is%2520enhanced.%2520Furthermore%252C%2520by%2520incorporating%2520the%2520SCRM%2520into%250Athe%2520encoder%2520block%2520of%2520the%2520UNet%2520architecture%252C%2520we%2520introduce%2520a%2520novel%2520framework%250Adubbed%2520Spatial-Channel%2520Regulation%2520Network%2520%2528SCRNet%2529.%2520The%2520results%2520of%2520our%250Aextensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520SCRNet%252C%2520which%2520consistently%250Aachieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCRNet%3A%20Spatial-Channel%20Regulation%20Network%20for%20Medical%20Ultrasound%20Image%0A%20%20Segmentation&entry.906535625=Weixin%20Xu%20and%20Ziliang%20Wang&entry.1292438233=%20%20Medical%20ultrasound%20image%20segmentation%20presents%20a%20formidable%20challenge%20in%20the%0Arealm%20of%20computer%20vision.%20Traditional%20approaches%20rely%20on%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20and%20Transformer-based%20methods%20to%20address%20the%20intricacies%20of%0Amedical%20image%20segmentation.%20Nevertheless%2C%20inherent%20limitations%20persist%2C%20as%0ACNN-based%20methods%20tend%20to%20disregard%20long-range%20dependencies%2C%20while%0ATransformer-based%20methods%20may%20overlook%20local%20contextual%20information.%20To%20address%0Athese%20deficiencies%2C%20we%20propose%20a%20novel%20Feature%20Aggregation%20Module%20%28FAM%29%0Adesigned%20to%20process%20two%20input%20features%20from%20the%20preceding%20layer.%20These%20features%0Aare%20seamlessly%20directed%20into%20two%20branches%20of%20the%20Convolution%20and%0ACross-Attention%20Parallel%20Module%20%28CCAPM%29%20to%20endow%20them%20with%20different%20roles%20in%0Aeach%20of%20the%20two%20branches%20to%20help%20establish%20a%20strong%20connection%20between%20the%20two%0Ainput%20features.%20This%20strategy%20enables%20our%20module%20to%20focus%20concurrently%20on%20both%0Along-range%20dependencies%20and%20local%20contextual%20information%20by%20judiciously%20merging%0Aconvolution%20operations%20with%20cross-attention%20mechanisms.%20Moreover%2C%20by%0Aintegrating%20FAM%20within%20our%20proposed%20Spatial-Channel%20Regulation%20Module%20%28SCRM%29%2C%0Athe%20ability%20to%20discern%20salient%20regions%20and%20informative%20features%20warranting%0Aincreased%20attention%20is%20enhanced.%20Furthermore%2C%20by%20incorporating%20the%20SCRM%20into%0Athe%20encoder%20block%20of%20the%20UNet%20architecture%2C%20we%20introduce%20a%20novel%20framework%0Adubbed%20Spatial-Channel%20Regulation%20Network%20%28SCRNet%29.%20The%20results%20of%20our%0Aextensive%20experiments%20demonstrate%20the%20superiority%20of%20SCRNet%2C%20which%20consistently%0Aachieves%20state-of-the-art%20%28SOTA%29%20performance%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13899v1&entry.124074799=Read"},
{"title": "EmoSEM: Segment and Explain Emotion Stimuli in Visual Art", "author": "Jing Zhang and Dan Guo and Zhangbin Li and Meng Wang", "abstract": "  This paper focuses on a key challenge in visual emotion understanding: given\nan art image, the model pinpoints pixel regions that trigger a specific human\nemotion, and generates linguistic explanations for it. Despite advances in\ngeneral segmentation, pixel-level emotion understanding still faces a dual\nchallenge: first, the subjectivity of emotion limits general segmentation\nmodels like SAM to adapt to emotion-oriented segmentation tasks; and second,\nthe abstract nature of art expression makes it hard for captioning models to\nbalance pixel-level semantics and emotion reasoning. To solve the above\nproblems, this paper proposes the Emotion stimuli Segmentation and Explanation\nModel (EmoSEM) model to endow the segmentation framework with emotion\ncomprehension capability. First, to enable the model to perform segmentation\nunder the guidance of emotional intent well, we introduce an emotional prompt\nwith a learnable mask token as the conditional input for segmentation decoding.\nThen, we design an emotion projector to establish the association between\nemotion and visual features. Next, more importantly, to address emotion-visual\nstimuli alignment, we develop a lightweight prefix adapter, a module that fuses\nthe learned emotional mask with the corresponding emotion into a unified\nrepresentation compatible with the language model. Finally, we input the joint\nvisual, mask, and emotional tokens into the language model and output the\nemotional explanations. It ensures that the generated interpretations remain\nsemantically and emotionally coherent with the visual stimuli. Our method\nrealizes end-to-end modeling from low-level pixel features to high-level\nemotion interpretation, delivering the first interpretable fine-grained\nframework for visual emotion analysis. Extensive experiments validate the\neffectiveness of our model. Code will be made publicly available.\n", "link": "http://arxiv.org/abs/2504.14658v3", "date": "2025-08-19", "relevancy": 2.1626, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5464}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5464}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmoSEM%3A%20Segment%20and%20Explain%20Emotion%20Stimuli%20in%20Visual%20Art&body=Title%3A%20EmoSEM%3A%20Segment%20and%20Explain%20Emotion%20Stimuli%20in%20Visual%20Art%0AAuthor%3A%20Jing%20Zhang%20and%20Dan%20Guo%20and%20Zhangbin%20Li%20and%20Meng%20Wang%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20a%20key%20challenge%20in%20visual%20emotion%20understanding%3A%20given%0Aan%20art%20image%2C%20the%20model%20pinpoints%20pixel%20regions%20that%20trigger%20a%20specific%20human%0Aemotion%2C%20and%20generates%20linguistic%20explanations%20for%20it.%20Despite%20advances%20in%0Ageneral%20segmentation%2C%20pixel-level%20emotion%20understanding%20still%20faces%20a%20dual%0Achallenge%3A%20first%2C%20the%20subjectivity%20of%20emotion%20limits%20general%20segmentation%0Amodels%20like%20SAM%20to%20adapt%20to%20emotion-oriented%20segmentation%20tasks%3B%20and%20second%2C%0Athe%20abstract%20nature%20of%20art%20expression%20makes%20it%20hard%20for%20captioning%20models%20to%0Abalance%20pixel-level%20semantics%20and%20emotion%20reasoning.%20To%20solve%20the%20above%0Aproblems%2C%20this%20paper%20proposes%20the%20Emotion%20stimuli%20Segmentation%20and%20Explanation%0AModel%20%28EmoSEM%29%20model%20to%20endow%20the%20segmentation%20framework%20with%20emotion%0Acomprehension%20capability.%20First%2C%20to%20enable%20the%20model%20to%20perform%20segmentation%0Aunder%20the%20guidance%20of%20emotional%20intent%20well%2C%20we%20introduce%20an%20emotional%20prompt%0Awith%20a%20learnable%20mask%20token%20as%20the%20conditional%20input%20for%20segmentation%20decoding.%0AThen%2C%20we%20design%20an%20emotion%20projector%20to%20establish%20the%20association%20between%0Aemotion%20and%20visual%20features.%20Next%2C%20more%20importantly%2C%20to%20address%20emotion-visual%0Astimuli%20alignment%2C%20we%20develop%20a%20lightweight%20prefix%20adapter%2C%20a%20module%20that%20fuses%0Athe%20learned%20emotional%20mask%20with%20the%20corresponding%20emotion%20into%20a%20unified%0Arepresentation%20compatible%20with%20the%20language%20model.%20Finally%2C%20we%20input%20the%20joint%0Avisual%2C%20mask%2C%20and%20emotional%20tokens%20into%20the%20language%20model%20and%20output%20the%0Aemotional%20explanations.%20It%20ensures%20that%20the%20generated%20interpretations%20remain%0Asemantically%20and%20emotionally%20coherent%20with%20the%20visual%20stimuli.%20Our%20method%0Arealizes%20end-to-end%20modeling%20from%20low-level%20pixel%20features%20to%20high-level%0Aemotion%20interpretation%2C%20delivering%20the%20first%20interpretable%20fine-grained%0Aframework%20for%20visual%20emotion%20analysis.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20our%20model.%20Code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14658v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmoSEM%253A%2520Segment%2520and%2520Explain%2520Emotion%2520Stimuli%2520in%2520Visual%2520Art%26entry.906535625%3DJing%2520Zhang%2520and%2520Dan%2520Guo%2520and%2520Zhangbin%2520Li%2520and%2520Meng%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520a%2520key%2520challenge%2520in%2520visual%2520emotion%2520understanding%253A%2520given%250Aan%2520art%2520image%252C%2520the%2520model%2520pinpoints%2520pixel%2520regions%2520that%2520trigger%2520a%2520specific%2520human%250Aemotion%252C%2520and%2520generates%2520linguistic%2520explanations%2520for%2520it.%2520Despite%2520advances%2520in%250Ageneral%2520segmentation%252C%2520pixel-level%2520emotion%2520understanding%2520still%2520faces%2520a%2520dual%250Achallenge%253A%2520first%252C%2520the%2520subjectivity%2520of%2520emotion%2520limits%2520general%2520segmentation%250Amodels%2520like%2520SAM%2520to%2520adapt%2520to%2520emotion-oriented%2520segmentation%2520tasks%253B%2520and%2520second%252C%250Athe%2520abstract%2520nature%2520of%2520art%2520expression%2520makes%2520it%2520hard%2520for%2520captioning%2520models%2520to%250Abalance%2520pixel-level%2520semantics%2520and%2520emotion%2520reasoning.%2520To%2520solve%2520the%2520above%250Aproblems%252C%2520this%2520paper%2520proposes%2520the%2520Emotion%2520stimuli%2520Segmentation%2520and%2520Explanation%250AModel%2520%2528EmoSEM%2529%2520model%2520to%2520endow%2520the%2520segmentation%2520framework%2520with%2520emotion%250Acomprehension%2520capability.%2520First%252C%2520to%2520enable%2520the%2520model%2520to%2520perform%2520segmentation%250Aunder%2520the%2520guidance%2520of%2520emotional%2520intent%2520well%252C%2520we%2520introduce%2520an%2520emotional%2520prompt%250Awith%2520a%2520learnable%2520mask%2520token%2520as%2520the%2520conditional%2520input%2520for%2520segmentation%2520decoding.%250AThen%252C%2520we%2520design%2520an%2520emotion%2520projector%2520to%2520establish%2520the%2520association%2520between%250Aemotion%2520and%2520visual%2520features.%2520Next%252C%2520more%2520importantly%252C%2520to%2520address%2520emotion-visual%250Astimuli%2520alignment%252C%2520we%2520develop%2520a%2520lightweight%2520prefix%2520adapter%252C%2520a%2520module%2520that%2520fuses%250Athe%2520learned%2520emotional%2520mask%2520with%2520the%2520corresponding%2520emotion%2520into%2520a%2520unified%250Arepresentation%2520compatible%2520with%2520the%2520language%2520model.%2520Finally%252C%2520we%2520input%2520the%2520joint%250Avisual%252C%2520mask%252C%2520and%2520emotional%2520tokens%2520into%2520the%2520language%2520model%2520and%2520output%2520the%250Aemotional%2520explanations.%2520It%2520ensures%2520that%2520the%2520generated%2520interpretations%2520remain%250Asemantically%2520and%2520emotionally%2520coherent%2520with%2520the%2520visual%2520stimuli.%2520Our%2520method%250Arealizes%2520end-to-end%2520modeling%2520from%2520low-level%2520pixel%2520features%2520to%2520high-level%250Aemotion%2520interpretation%252C%2520delivering%2520the%2520first%2520interpretable%2520fine-grained%250Aframework%2520for%2520visual%2520emotion%2520analysis.%2520Extensive%2520experiments%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520model.%2520Code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14658v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmoSEM%3A%20Segment%20and%20Explain%20Emotion%20Stimuli%20in%20Visual%20Art&entry.906535625=Jing%20Zhang%20and%20Dan%20Guo%20and%20Zhangbin%20Li%20and%20Meng%20Wang&entry.1292438233=%20%20This%20paper%20focuses%20on%20a%20key%20challenge%20in%20visual%20emotion%20understanding%3A%20given%0Aan%20art%20image%2C%20the%20model%20pinpoints%20pixel%20regions%20that%20trigger%20a%20specific%20human%0Aemotion%2C%20and%20generates%20linguistic%20explanations%20for%20it.%20Despite%20advances%20in%0Ageneral%20segmentation%2C%20pixel-level%20emotion%20understanding%20still%20faces%20a%20dual%0Achallenge%3A%20first%2C%20the%20subjectivity%20of%20emotion%20limits%20general%20segmentation%0Amodels%20like%20SAM%20to%20adapt%20to%20emotion-oriented%20segmentation%20tasks%3B%20and%20second%2C%0Athe%20abstract%20nature%20of%20art%20expression%20makes%20it%20hard%20for%20captioning%20models%20to%0Abalance%20pixel-level%20semantics%20and%20emotion%20reasoning.%20To%20solve%20the%20above%0Aproblems%2C%20this%20paper%20proposes%20the%20Emotion%20stimuli%20Segmentation%20and%20Explanation%0AModel%20%28EmoSEM%29%20model%20to%20endow%20the%20segmentation%20framework%20with%20emotion%0Acomprehension%20capability.%20First%2C%20to%20enable%20the%20model%20to%20perform%20segmentation%0Aunder%20the%20guidance%20of%20emotional%20intent%20well%2C%20we%20introduce%20an%20emotional%20prompt%0Awith%20a%20learnable%20mask%20token%20as%20the%20conditional%20input%20for%20segmentation%20decoding.%0AThen%2C%20we%20design%20an%20emotion%20projector%20to%20establish%20the%20association%20between%0Aemotion%20and%20visual%20features.%20Next%2C%20more%20importantly%2C%20to%20address%20emotion-visual%0Astimuli%20alignment%2C%20we%20develop%20a%20lightweight%20prefix%20adapter%2C%20a%20module%20that%20fuses%0Athe%20learned%20emotional%20mask%20with%20the%20corresponding%20emotion%20into%20a%20unified%0Arepresentation%20compatible%20with%20the%20language%20model.%20Finally%2C%20we%20input%20the%20joint%0Avisual%2C%20mask%2C%20and%20emotional%20tokens%20into%20the%20language%20model%20and%20output%20the%0Aemotional%20explanations.%20It%20ensures%20that%20the%20generated%20interpretations%20remain%0Asemantically%20and%20emotionally%20coherent%20with%20the%20visual%20stimuli.%20Our%20method%0Arealizes%20end-to-end%20modeling%20from%20low-level%20pixel%20features%20to%20high-level%0Aemotion%20interpretation%2C%20delivering%20the%20first%20interpretable%20fine-grained%0Aframework%20for%20visual%20emotion%20analysis.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20our%20model.%20Code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14658v3&entry.124074799=Read"},
{"title": "MME-SCI: A Comprehensive and Challenging Science Benchmark for\n  Multimodal Large Language Models", "author": "Jiacheng Ruan and Dan Jiang and Xian Gao and Ting Liu and Yuzhuo Fu and Yangyang Kang", "abstract": "  Recently, multimodal large language models (MLLMs) have achieved significant\nadvancements across various domains, and corresponding evaluation benchmarks\nhave been continuously refined and improved. In this process, benchmarks in the\nscientific domain have played an important role in assessing the reasoning\ncapabilities of MLLMs. However, existing benchmarks still face three key\nchallenges: 1) Insufficient evaluation of models' reasoning abilities in\nmultilingual scenarios; 2) Inadequate assessment of MLLMs' comprehensive\nmodality coverage; 3) Lack of fine-grained annotation of scientific knowledge\npoints. To address these gaps, we propose MME-SCI, a comprehensive and\nchallenging benchmark. We carefully collected 1,019 high-quality\nquestion-answer pairs, which involve 3 distinct evaluation modes. These pairs\ncover four subjects, namely mathematics, physics, chemistry, and biology, and\nsupport five languages: Chinese, English, French, Spanish, and Japanese. We\nconducted extensive experiments on 16 open-source models and 4 closed-source\nmodels, and the results demonstrate that MME-SCI is widely challenging for\nexisting MLLMs. For instance, under the Image-only evaluation mode, o4-mini\nachieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics,\nphysics, chemistry, and biology, respectively, indicating a significantly\nhigher difficulty level compared to existing benchmarks. More importantly,\nusing MME-SCI's multilingual and fine-grained knowledge attributes, we analyzed\nexisting models' performance in depth and identified their weaknesses in\nspecific domains. The Data and Evaluation Code are available at\nhttps://github.com/JCruan519/MME-SCI.\n", "link": "http://arxiv.org/abs/2508.13938v1", "date": "2025-08-19", "relevancy": 2.1583, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MME-SCI%3A%20A%20Comprehensive%20and%20Challenging%20Science%20Benchmark%20for%0A%20%20Multimodal%20Large%20Language%20Models&body=Title%3A%20MME-SCI%3A%20A%20Comprehensive%20and%20Challenging%20Science%20Benchmark%20for%0A%20%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Jiacheng%20Ruan%20and%20Dan%20Jiang%20and%20Xian%20Gao%20and%20Ting%20Liu%20and%20Yuzhuo%20Fu%20and%20Yangyang%20Kang%0AAbstract%3A%20%20%20Recently%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20achieved%20significant%0Aadvancements%20across%20various%20domains%2C%20and%20corresponding%20evaluation%20benchmarks%0Ahave%20been%20continuously%20refined%20and%20improved.%20In%20this%20process%2C%20benchmarks%20in%20the%0Ascientific%20domain%20have%20played%20an%20important%20role%20in%20assessing%20the%20reasoning%0Acapabilities%20of%20MLLMs.%20However%2C%20existing%20benchmarks%20still%20face%20three%20key%0Achallenges%3A%201%29%20Insufficient%20evaluation%20of%20models%27%20reasoning%20abilities%20in%0Amultilingual%20scenarios%3B%202%29%20Inadequate%20assessment%20of%20MLLMs%27%20comprehensive%0Amodality%20coverage%3B%203%29%20Lack%20of%20fine-grained%20annotation%20of%20scientific%20knowledge%0Apoints.%20To%20address%20these%20gaps%2C%20we%20propose%20MME-SCI%2C%20a%20comprehensive%20and%0Achallenging%20benchmark.%20We%20carefully%20collected%201%2C019%20high-quality%0Aquestion-answer%20pairs%2C%20which%20involve%203%20distinct%20evaluation%20modes.%20These%20pairs%0Acover%20four%20subjects%2C%20namely%20mathematics%2C%20physics%2C%20chemistry%2C%20and%20biology%2C%20and%0Asupport%20five%20languages%3A%20Chinese%2C%20English%2C%20French%2C%20Spanish%2C%20and%20Japanese.%20We%0Aconducted%20extensive%20experiments%20on%2016%20open-source%20models%20and%204%20closed-source%0Amodels%2C%20and%20the%20results%20demonstrate%20that%20MME-SCI%20is%20widely%20challenging%20for%0Aexisting%20MLLMs.%20For%20instance%2C%20under%20the%20Image-only%20evaluation%20mode%2C%20o4-mini%0Aachieved%20accuracy%20of%20only%2052.11%25%2C%2024.73%25%2C%2036.57%25%2C%20and%2029.80%25%20in%20mathematics%2C%0Aphysics%2C%20chemistry%2C%20and%20biology%2C%20respectively%2C%20indicating%20a%20significantly%0Ahigher%20difficulty%20level%20compared%20to%20existing%20benchmarks.%20More%20importantly%2C%0Ausing%20MME-SCI%27s%20multilingual%20and%20fine-grained%20knowledge%20attributes%2C%20we%20analyzed%0Aexisting%20models%27%20performance%20in%20depth%20and%20identified%20their%20weaknesses%20in%0Aspecific%20domains.%20The%20Data%20and%20Evaluation%20Code%20are%20available%20at%0Ahttps%3A//github.com/JCruan519/MME-SCI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMME-SCI%253A%2520A%2520Comprehensive%2520and%2520Challenging%2520Science%2520Benchmark%2520for%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DJiacheng%2520Ruan%2520and%2520Dan%2520Jiang%2520and%2520Xian%2520Gao%2520and%2520Ting%2520Liu%2520and%2520Yuzhuo%2520Fu%2520and%2520Yangyang%2520Kang%26entry.1292438233%3D%2520%2520Recently%252C%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520achieved%2520significant%250Aadvancements%2520across%2520various%2520domains%252C%2520and%2520corresponding%2520evaluation%2520benchmarks%250Ahave%2520been%2520continuously%2520refined%2520and%2520improved.%2520In%2520this%2520process%252C%2520benchmarks%2520in%2520the%250Ascientific%2520domain%2520have%2520played%2520an%2520important%2520role%2520in%2520assessing%2520the%2520reasoning%250Acapabilities%2520of%2520MLLMs.%2520However%252C%2520existing%2520benchmarks%2520still%2520face%2520three%2520key%250Achallenges%253A%25201%2529%2520Insufficient%2520evaluation%2520of%2520models%2527%2520reasoning%2520abilities%2520in%250Amultilingual%2520scenarios%253B%25202%2529%2520Inadequate%2520assessment%2520of%2520MLLMs%2527%2520comprehensive%250Amodality%2520coverage%253B%25203%2529%2520Lack%2520of%2520fine-grained%2520annotation%2520of%2520scientific%2520knowledge%250Apoints.%2520To%2520address%2520these%2520gaps%252C%2520we%2520propose%2520MME-SCI%252C%2520a%2520comprehensive%2520and%250Achallenging%2520benchmark.%2520We%2520carefully%2520collected%25201%252C019%2520high-quality%250Aquestion-answer%2520pairs%252C%2520which%2520involve%25203%2520distinct%2520evaluation%2520modes.%2520These%2520pairs%250Acover%2520four%2520subjects%252C%2520namely%2520mathematics%252C%2520physics%252C%2520chemistry%252C%2520and%2520biology%252C%2520and%250Asupport%2520five%2520languages%253A%2520Chinese%252C%2520English%252C%2520French%252C%2520Spanish%252C%2520and%2520Japanese.%2520We%250Aconducted%2520extensive%2520experiments%2520on%252016%2520open-source%2520models%2520and%25204%2520closed-source%250Amodels%252C%2520and%2520the%2520results%2520demonstrate%2520that%2520MME-SCI%2520is%2520widely%2520challenging%2520for%250Aexisting%2520MLLMs.%2520For%2520instance%252C%2520under%2520the%2520Image-only%2520evaluation%2520mode%252C%2520o4-mini%250Aachieved%2520accuracy%2520of%2520only%252052.11%2525%252C%252024.73%2525%252C%252036.57%2525%252C%2520and%252029.80%2525%2520in%2520mathematics%252C%250Aphysics%252C%2520chemistry%252C%2520and%2520biology%252C%2520respectively%252C%2520indicating%2520a%2520significantly%250Ahigher%2520difficulty%2520level%2520compared%2520to%2520existing%2520benchmarks.%2520More%2520importantly%252C%250Ausing%2520MME-SCI%2527s%2520multilingual%2520and%2520fine-grained%2520knowledge%2520attributes%252C%2520we%2520analyzed%250Aexisting%2520models%2527%2520performance%2520in%2520depth%2520and%2520identified%2520their%2520weaknesses%2520in%250Aspecific%2520domains.%2520The%2520Data%2520and%2520Evaluation%2520Code%2520are%2520available%2520at%250Ahttps%253A//github.com/JCruan519/MME-SCI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MME-SCI%3A%20A%20Comprehensive%20and%20Challenging%20Science%20Benchmark%20for%0A%20%20Multimodal%20Large%20Language%20Models&entry.906535625=Jiacheng%20Ruan%20and%20Dan%20Jiang%20and%20Xian%20Gao%20and%20Ting%20Liu%20and%20Yuzhuo%20Fu%20and%20Yangyang%20Kang&entry.1292438233=%20%20Recently%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20achieved%20significant%0Aadvancements%20across%20various%20domains%2C%20and%20corresponding%20evaluation%20benchmarks%0Ahave%20been%20continuously%20refined%20and%20improved.%20In%20this%20process%2C%20benchmarks%20in%20the%0Ascientific%20domain%20have%20played%20an%20important%20role%20in%20assessing%20the%20reasoning%0Acapabilities%20of%20MLLMs.%20However%2C%20existing%20benchmarks%20still%20face%20three%20key%0Achallenges%3A%201%29%20Insufficient%20evaluation%20of%20models%27%20reasoning%20abilities%20in%0Amultilingual%20scenarios%3B%202%29%20Inadequate%20assessment%20of%20MLLMs%27%20comprehensive%0Amodality%20coverage%3B%203%29%20Lack%20of%20fine-grained%20annotation%20of%20scientific%20knowledge%0Apoints.%20To%20address%20these%20gaps%2C%20we%20propose%20MME-SCI%2C%20a%20comprehensive%20and%0Achallenging%20benchmark.%20We%20carefully%20collected%201%2C019%20high-quality%0Aquestion-answer%20pairs%2C%20which%20involve%203%20distinct%20evaluation%20modes.%20These%20pairs%0Acover%20four%20subjects%2C%20namely%20mathematics%2C%20physics%2C%20chemistry%2C%20and%20biology%2C%20and%0Asupport%20five%20languages%3A%20Chinese%2C%20English%2C%20French%2C%20Spanish%2C%20and%20Japanese.%20We%0Aconducted%20extensive%20experiments%20on%2016%20open-source%20models%20and%204%20closed-source%0Amodels%2C%20and%20the%20results%20demonstrate%20that%20MME-SCI%20is%20widely%20challenging%20for%0Aexisting%20MLLMs.%20For%20instance%2C%20under%20the%20Image-only%20evaluation%20mode%2C%20o4-mini%0Aachieved%20accuracy%20of%20only%2052.11%25%2C%2024.73%25%2C%2036.57%25%2C%20and%2029.80%25%20in%20mathematics%2C%0Aphysics%2C%20chemistry%2C%20and%20biology%2C%20respectively%2C%20indicating%20a%20significantly%0Ahigher%20difficulty%20level%20compared%20to%20existing%20benchmarks.%20More%20importantly%2C%0Ausing%20MME-SCI%27s%20multilingual%20and%20fine-grained%20knowledge%20attributes%2C%20we%20analyzed%0Aexisting%20models%27%20performance%20in%20depth%20and%20identified%20their%20weaknesses%20in%0Aspecific%20domains.%20The%20Data%20and%20Evaluation%20Code%20are%20available%20at%0Ahttps%3A//github.com/JCruan519/MME-SCI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13938v1&entry.124074799=Read"},
{"title": "MR6D: Benchmarking 6D Pose Estimation for Mobile Robots", "author": "Anas Gouda and Shrutarv Awasthi and Christian Blesing and Lokeshwaran Manohar and Frank Hoffmann and Alice Kirchheim", "abstract": "  Existing 6D pose estimation datasets primarily focus on small household\nobjects typically handled by robot arm manipulators, limiting their relevance\nto mobile robotics. Mobile platforms often operate without manipulators,\ninteract with larger objects, and face challenges such as long-range\nperception, heavy self-occlusion, and diverse camera perspectives. While recent\nmodels generalize well to unseen objects, evaluations remain confined to\nhousehold-like settings that overlook these factors. We introduce MR6D, a\ndataset designed for 6D pose estimation for mobile robots in industrial\nenvironments. It includes 92 real-world scenes featuring 16 unique objects\nacross static and dynamic interactions. MR6D captures the challenges specific\nto mobile platforms, including distant viewpoints, varied object\nconfigurations, larger object sizes, and complex occlusion/self-occlusion\npatterns. Initial experiments reveal that current 6D pipelines underperform in\nthese settings, with 2D segmentation being another hurdle. MR6D establishes a\nfoundation for developing and evaluating pose estimation methods tailored to\nthe demands of mobile robotics. The dataset is available at\nhttps://huggingface.co/datasets/anas-gouda/mr6d.\n", "link": "http://arxiv.org/abs/2508.13775v1", "date": "2025-08-19", "relevancy": 2.1522, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5427}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5373}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MR6D%3A%20Benchmarking%206D%20Pose%20Estimation%20for%20Mobile%20Robots&body=Title%3A%20MR6D%3A%20Benchmarking%206D%20Pose%20Estimation%20for%20Mobile%20Robots%0AAuthor%3A%20Anas%20Gouda%20and%20Shrutarv%20Awasthi%20and%20Christian%20Blesing%20and%20Lokeshwaran%20Manohar%20and%20Frank%20Hoffmann%20and%20Alice%20Kirchheim%0AAbstract%3A%20%20%20Existing%206D%20pose%20estimation%20datasets%20primarily%20focus%20on%20small%20household%0Aobjects%20typically%20handled%20by%20robot%20arm%20manipulators%2C%20limiting%20their%20relevance%0Ato%20mobile%20robotics.%20Mobile%20platforms%20often%20operate%20without%20manipulators%2C%0Ainteract%20with%20larger%20objects%2C%20and%20face%20challenges%20such%20as%20long-range%0Aperception%2C%20heavy%20self-occlusion%2C%20and%20diverse%20camera%20perspectives.%20While%20recent%0Amodels%20generalize%20well%20to%20unseen%20objects%2C%20evaluations%20remain%20confined%20to%0Ahousehold-like%20settings%20that%20overlook%20these%20factors.%20We%20introduce%20MR6D%2C%20a%0Adataset%20designed%20for%206D%20pose%20estimation%20for%20mobile%20robots%20in%20industrial%0Aenvironments.%20It%20includes%2092%20real-world%20scenes%20featuring%2016%20unique%20objects%0Aacross%20static%20and%20dynamic%20interactions.%20MR6D%20captures%20the%20challenges%20specific%0Ato%20mobile%20platforms%2C%20including%20distant%20viewpoints%2C%20varied%20object%0Aconfigurations%2C%20larger%20object%20sizes%2C%20and%20complex%20occlusion/self-occlusion%0Apatterns.%20Initial%20experiments%20reveal%20that%20current%206D%20pipelines%20underperform%20in%0Athese%20settings%2C%20with%202D%20segmentation%20being%20another%20hurdle.%20MR6D%20establishes%20a%0Afoundation%20for%20developing%20and%20evaluating%20pose%20estimation%20methods%20tailored%20to%0Athe%20demands%20of%20mobile%20robotics.%20The%20dataset%20is%20available%20at%0Ahttps%3A//huggingface.co/datasets/anas-gouda/mr6d.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMR6D%253A%2520Benchmarking%25206D%2520Pose%2520Estimation%2520for%2520Mobile%2520Robots%26entry.906535625%3DAnas%2520Gouda%2520and%2520Shrutarv%2520Awasthi%2520and%2520Christian%2520Blesing%2520and%2520Lokeshwaran%2520Manohar%2520and%2520Frank%2520Hoffmann%2520and%2520Alice%2520Kirchheim%26entry.1292438233%3D%2520%2520Existing%25206D%2520pose%2520estimation%2520datasets%2520primarily%2520focus%2520on%2520small%2520household%250Aobjects%2520typically%2520handled%2520by%2520robot%2520arm%2520manipulators%252C%2520limiting%2520their%2520relevance%250Ato%2520mobile%2520robotics.%2520Mobile%2520platforms%2520often%2520operate%2520without%2520manipulators%252C%250Ainteract%2520with%2520larger%2520objects%252C%2520and%2520face%2520challenges%2520such%2520as%2520long-range%250Aperception%252C%2520heavy%2520self-occlusion%252C%2520and%2520diverse%2520camera%2520perspectives.%2520While%2520recent%250Amodels%2520generalize%2520well%2520to%2520unseen%2520objects%252C%2520evaluations%2520remain%2520confined%2520to%250Ahousehold-like%2520settings%2520that%2520overlook%2520these%2520factors.%2520We%2520introduce%2520MR6D%252C%2520a%250Adataset%2520designed%2520for%25206D%2520pose%2520estimation%2520for%2520mobile%2520robots%2520in%2520industrial%250Aenvironments.%2520It%2520includes%252092%2520real-world%2520scenes%2520featuring%252016%2520unique%2520objects%250Aacross%2520static%2520and%2520dynamic%2520interactions.%2520MR6D%2520captures%2520the%2520challenges%2520specific%250Ato%2520mobile%2520platforms%252C%2520including%2520distant%2520viewpoints%252C%2520varied%2520object%250Aconfigurations%252C%2520larger%2520object%2520sizes%252C%2520and%2520complex%2520occlusion/self-occlusion%250Apatterns.%2520Initial%2520experiments%2520reveal%2520that%2520current%25206D%2520pipelines%2520underperform%2520in%250Athese%2520settings%252C%2520with%25202D%2520segmentation%2520being%2520another%2520hurdle.%2520MR6D%2520establishes%2520a%250Afoundation%2520for%2520developing%2520and%2520evaluating%2520pose%2520estimation%2520methods%2520tailored%2520to%250Athe%2520demands%2520of%2520mobile%2520robotics.%2520The%2520dataset%2520is%2520available%2520at%250Ahttps%253A//huggingface.co/datasets/anas-gouda/mr6d.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MR6D%3A%20Benchmarking%206D%20Pose%20Estimation%20for%20Mobile%20Robots&entry.906535625=Anas%20Gouda%20and%20Shrutarv%20Awasthi%20and%20Christian%20Blesing%20and%20Lokeshwaran%20Manohar%20and%20Frank%20Hoffmann%20and%20Alice%20Kirchheim&entry.1292438233=%20%20Existing%206D%20pose%20estimation%20datasets%20primarily%20focus%20on%20small%20household%0Aobjects%20typically%20handled%20by%20robot%20arm%20manipulators%2C%20limiting%20their%20relevance%0Ato%20mobile%20robotics.%20Mobile%20platforms%20often%20operate%20without%20manipulators%2C%0Ainteract%20with%20larger%20objects%2C%20and%20face%20challenges%20such%20as%20long-range%0Aperception%2C%20heavy%20self-occlusion%2C%20and%20diverse%20camera%20perspectives.%20While%20recent%0Amodels%20generalize%20well%20to%20unseen%20objects%2C%20evaluations%20remain%20confined%20to%0Ahousehold-like%20settings%20that%20overlook%20these%20factors.%20We%20introduce%20MR6D%2C%20a%0Adataset%20designed%20for%206D%20pose%20estimation%20for%20mobile%20robots%20in%20industrial%0Aenvironments.%20It%20includes%2092%20real-world%20scenes%20featuring%2016%20unique%20objects%0Aacross%20static%20and%20dynamic%20interactions.%20MR6D%20captures%20the%20challenges%20specific%0Ato%20mobile%20platforms%2C%20including%20distant%20viewpoints%2C%20varied%20object%0Aconfigurations%2C%20larger%20object%20sizes%2C%20and%20complex%20occlusion/self-occlusion%0Apatterns.%20Initial%20experiments%20reveal%20that%20current%206D%20pipelines%20underperform%20in%0Athese%20settings%2C%20with%202D%20segmentation%20being%20another%20hurdle.%20MR6D%20establishes%20a%0Afoundation%20for%20developing%20and%20evaluating%20pose%20estimation%20methods%20tailored%20to%0Athe%20demands%20of%20mobile%20robotics.%20The%20dataset%20is%20available%20at%0Ahttps%3A//huggingface.co/datasets/anas-gouda/mr6d.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13775v1&entry.124074799=Read"},
{"title": "GDNSQ: Gradual Differentiable Noise Scale Quantization for Low-bit\n  Neural Networks", "author": "Sergey Salishev and Ian Akhremchik", "abstract": "  Quantized neural networks can be viewed as a chain of noisy channels, where\nrounding in each layer reduces capacity as bit-width shrinks; the\nfloating-point (FP) checkpoint sets the maximum input rate. We track capacity\ndynamics as the average bit-width decreases and identify resulting quantization\nbottlenecks by casting fine-tuning as a smooth, constrained optimization\nproblem. Our approach employs a fully differentiable Straight-Through Estimator\n(STE) with learnable bit-width, noise scale and clamp bounds, and enforces a\ntarget bit-width via an exterior-point penalty; mild metric smoothing (via\ndistillation) stabilizes training. Despite its simplicity, the method attains\ncompetitive accuracy down to the extreme W1A1 setting while retaining the\nefficiency of STE.\n", "link": "http://arxiv.org/abs/2508.14004v1", "date": "2025-08-19", "relevancy": 2.1513, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5419}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.535}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GDNSQ%3A%20Gradual%20Differentiable%20Noise%20Scale%20Quantization%20for%20Low-bit%0A%20%20Neural%20Networks&body=Title%3A%20GDNSQ%3A%20Gradual%20Differentiable%20Noise%20Scale%20Quantization%20for%20Low-bit%0A%20%20Neural%20Networks%0AAuthor%3A%20Sergey%20Salishev%20and%20Ian%20Akhremchik%0AAbstract%3A%20%20%20Quantized%20neural%20networks%20can%20be%20viewed%20as%20a%20chain%20of%20noisy%20channels%2C%20where%0Arounding%20in%20each%20layer%20reduces%20capacity%20as%20bit-width%20shrinks%3B%20the%0Afloating-point%20%28FP%29%20checkpoint%20sets%20the%20maximum%20input%20rate.%20We%20track%20capacity%0Adynamics%20as%20the%20average%20bit-width%20decreases%20and%20identify%20resulting%20quantization%0Abottlenecks%20by%20casting%20fine-tuning%20as%20a%20smooth%2C%20constrained%20optimization%0Aproblem.%20Our%20approach%20employs%20a%20fully%20differentiable%20Straight-Through%20Estimator%0A%28STE%29%20with%20learnable%20bit-width%2C%20noise%20scale%20and%20clamp%20bounds%2C%20and%20enforces%20a%0Atarget%20bit-width%20via%20an%20exterior-point%20penalty%3B%20mild%20metric%20smoothing%20%28via%0Adistillation%29%20stabilizes%20training.%20Despite%20its%20simplicity%2C%20the%20method%20attains%0Acompetitive%20accuracy%20down%20to%20the%20extreme%20W1A1%20setting%20while%20retaining%20the%0Aefficiency%20of%20STE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGDNSQ%253A%2520Gradual%2520Differentiable%2520Noise%2520Scale%2520Quantization%2520for%2520Low-bit%250A%2520%2520Neural%2520Networks%26entry.906535625%3DSergey%2520Salishev%2520and%2520Ian%2520Akhremchik%26entry.1292438233%3D%2520%2520Quantized%2520neural%2520networks%2520can%2520be%2520viewed%2520as%2520a%2520chain%2520of%2520noisy%2520channels%252C%2520where%250Arounding%2520in%2520each%2520layer%2520reduces%2520capacity%2520as%2520bit-width%2520shrinks%253B%2520the%250Afloating-point%2520%2528FP%2529%2520checkpoint%2520sets%2520the%2520maximum%2520input%2520rate.%2520We%2520track%2520capacity%250Adynamics%2520as%2520the%2520average%2520bit-width%2520decreases%2520and%2520identify%2520resulting%2520quantization%250Abottlenecks%2520by%2520casting%2520fine-tuning%2520as%2520a%2520smooth%252C%2520constrained%2520optimization%250Aproblem.%2520Our%2520approach%2520employs%2520a%2520fully%2520differentiable%2520Straight-Through%2520Estimator%250A%2528STE%2529%2520with%2520learnable%2520bit-width%252C%2520noise%2520scale%2520and%2520clamp%2520bounds%252C%2520and%2520enforces%2520a%250Atarget%2520bit-width%2520via%2520an%2520exterior-point%2520penalty%253B%2520mild%2520metric%2520smoothing%2520%2528via%250Adistillation%2529%2520stabilizes%2520training.%2520Despite%2520its%2520simplicity%252C%2520the%2520method%2520attains%250Acompetitive%2520accuracy%2520down%2520to%2520the%2520extreme%2520W1A1%2520setting%2520while%2520retaining%2520the%250Aefficiency%2520of%2520STE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GDNSQ%3A%20Gradual%20Differentiable%20Noise%20Scale%20Quantization%20for%20Low-bit%0A%20%20Neural%20Networks&entry.906535625=Sergey%20Salishev%20and%20Ian%20Akhremchik&entry.1292438233=%20%20Quantized%20neural%20networks%20can%20be%20viewed%20as%20a%20chain%20of%20noisy%20channels%2C%20where%0Arounding%20in%20each%20layer%20reduces%20capacity%20as%20bit-width%20shrinks%3B%20the%0Afloating-point%20%28FP%29%20checkpoint%20sets%20the%20maximum%20input%20rate.%20We%20track%20capacity%0Adynamics%20as%20the%20average%20bit-width%20decreases%20and%20identify%20resulting%20quantization%0Abottlenecks%20by%20casting%20fine-tuning%20as%20a%20smooth%2C%20constrained%20optimization%0Aproblem.%20Our%20approach%20employs%20a%20fully%20differentiable%20Straight-Through%20Estimator%0A%28STE%29%20with%20learnable%20bit-width%2C%20noise%20scale%20and%20clamp%20bounds%2C%20and%20enforces%20a%0Atarget%20bit-width%20via%20an%20exterior-point%20penalty%3B%20mild%20metric%20smoothing%20%28via%0Adistillation%29%20stabilizes%20training.%20Despite%20its%20simplicity%2C%20the%20method%20attains%0Acompetitive%20accuracy%20down%20to%20the%20extreme%20W1A1%20setting%20while%20retaining%20the%0Aefficiency%20of%20STE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14004v1&entry.124074799=Read"},
{"title": "Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based\n  Generalized Planning", "author": "Sangwoo Jeon and Juchul Shin and Gyeong-Tae Kim and YeonJe Cho and Seongwoo Kim", "abstract": "  Generalized planning using deep reinforcement learning (RL) combined with\ngraph neural networks (GNNs) has shown promising results in various symbolic\nplanning domains described by PDDL. However, existing approaches typically\nrepresent planning states as fully connected graphs, leading to a combinatorial\nexplosion in edge information and substantial sparsity as problem scales grow,\nespecially evident in large grid-based environments. This dense representation\nresults in diluted node-level information, exponentially increases memory\nrequirements, and ultimately makes learning infeasible for larger-scale\nproblems. To address these challenges, we propose a sparse, goal-aware GNN\nrepresentation that selectively encodes relevant local relationships and\nexplicitly integrates spatial features related to the goal. We validate our\napproach by designing novel drone mission scenarios based on PDDL within a grid\nworld, effectively simulating realistic mission execution environments. Our\nexperimental results demonstrate that our method scales effectively to larger\ngrid sizes previously infeasible with dense graph representations and\nsubstantially improves policy generalization and success rates. Our findings\nprovide a practical foundation for addressing realistic, large-scale\ngeneralized planning tasks.\n", "link": "http://arxiv.org/abs/2508.10747v2", "date": "2025-08-19", "relevancy": 2.1483, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5509}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5458}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Up%20without%20Fading%20Out%3A%20Goal-Aware%20Sparse%20GNN%20for%20RL-based%0A%20%20Generalized%20Planning&body=Title%3A%20Scaling%20Up%20without%20Fading%20Out%3A%20Goal-Aware%20Sparse%20GNN%20for%20RL-based%0A%20%20Generalized%20Planning%0AAuthor%3A%20Sangwoo%20Jeon%20and%20Juchul%20Shin%20and%20Gyeong-Tae%20Kim%20and%20YeonJe%20Cho%20and%20Seongwoo%20Kim%0AAbstract%3A%20%20%20Generalized%20planning%20using%20deep%20reinforcement%20learning%20%28RL%29%20combined%20with%0Agraph%20neural%20networks%20%28GNNs%29%20has%20shown%20promising%20results%20in%20various%20symbolic%0Aplanning%20domains%20described%20by%20PDDL.%20However%2C%20existing%20approaches%20typically%0Arepresent%20planning%20states%20as%20fully%20connected%20graphs%2C%20leading%20to%20a%20combinatorial%0Aexplosion%20in%20edge%20information%20and%20substantial%20sparsity%20as%20problem%20scales%20grow%2C%0Aespecially%20evident%20in%20large%20grid-based%20environments.%20This%20dense%20representation%0Aresults%20in%20diluted%20node-level%20information%2C%20exponentially%20increases%20memory%0Arequirements%2C%20and%20ultimately%20makes%20learning%20infeasible%20for%20larger-scale%0Aproblems.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20sparse%2C%20goal-aware%20GNN%0Arepresentation%20that%20selectively%20encodes%20relevant%20local%20relationships%20and%0Aexplicitly%20integrates%20spatial%20features%20related%20to%20the%20goal.%20We%20validate%20our%0Aapproach%20by%20designing%20novel%20drone%20mission%20scenarios%20based%20on%20PDDL%20within%20a%20grid%0Aworld%2C%20effectively%20simulating%20realistic%20mission%20execution%20environments.%20Our%0Aexperimental%20results%20demonstrate%20that%20our%20method%20scales%20effectively%20to%20larger%0Agrid%20sizes%20previously%20infeasible%20with%20dense%20graph%20representations%20and%0Asubstantially%20improves%20policy%20generalization%20and%20success%20rates.%20Our%20findings%0Aprovide%20a%20practical%20foundation%20for%20addressing%20realistic%2C%20large-scale%0Ageneralized%20planning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10747v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Up%2520without%2520Fading%2520Out%253A%2520Goal-Aware%2520Sparse%2520GNN%2520for%2520RL-based%250A%2520%2520Generalized%2520Planning%26entry.906535625%3DSangwoo%2520Jeon%2520and%2520Juchul%2520Shin%2520and%2520Gyeong-Tae%2520Kim%2520and%2520YeonJe%2520Cho%2520and%2520Seongwoo%2520Kim%26entry.1292438233%3D%2520%2520Generalized%2520planning%2520using%2520deep%2520reinforcement%2520learning%2520%2528RL%2529%2520combined%2520with%250Agraph%2520neural%2520networks%2520%2528GNNs%2529%2520has%2520shown%2520promising%2520results%2520in%2520various%2520symbolic%250Aplanning%2520domains%2520described%2520by%2520PDDL.%2520However%252C%2520existing%2520approaches%2520typically%250Arepresent%2520planning%2520states%2520as%2520fully%2520connected%2520graphs%252C%2520leading%2520to%2520a%2520combinatorial%250Aexplosion%2520in%2520edge%2520information%2520and%2520substantial%2520sparsity%2520as%2520problem%2520scales%2520grow%252C%250Aespecially%2520evident%2520in%2520large%2520grid-based%2520environments.%2520This%2520dense%2520representation%250Aresults%2520in%2520diluted%2520node-level%2520information%252C%2520exponentially%2520increases%2520memory%250Arequirements%252C%2520and%2520ultimately%2520makes%2520learning%2520infeasible%2520for%2520larger-scale%250Aproblems.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520sparse%252C%2520goal-aware%2520GNN%250Arepresentation%2520that%2520selectively%2520encodes%2520relevant%2520local%2520relationships%2520and%250Aexplicitly%2520integrates%2520spatial%2520features%2520related%2520to%2520the%2520goal.%2520We%2520validate%2520our%250Aapproach%2520by%2520designing%2520novel%2520drone%2520mission%2520scenarios%2520based%2520on%2520PDDL%2520within%2520a%2520grid%250Aworld%252C%2520effectively%2520simulating%2520realistic%2520mission%2520execution%2520environments.%2520Our%250Aexperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520scales%2520effectively%2520to%2520larger%250Agrid%2520sizes%2520previously%2520infeasible%2520with%2520dense%2520graph%2520representations%2520and%250Asubstantially%2520improves%2520policy%2520generalization%2520and%2520success%2520rates.%2520Our%2520findings%250Aprovide%2520a%2520practical%2520foundation%2520for%2520addressing%2520realistic%252C%2520large-scale%250Ageneralized%2520planning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10747v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Up%20without%20Fading%20Out%3A%20Goal-Aware%20Sparse%20GNN%20for%20RL-based%0A%20%20Generalized%20Planning&entry.906535625=Sangwoo%20Jeon%20and%20Juchul%20Shin%20and%20Gyeong-Tae%20Kim%20and%20YeonJe%20Cho%20and%20Seongwoo%20Kim&entry.1292438233=%20%20Generalized%20planning%20using%20deep%20reinforcement%20learning%20%28RL%29%20combined%20with%0Agraph%20neural%20networks%20%28GNNs%29%20has%20shown%20promising%20results%20in%20various%20symbolic%0Aplanning%20domains%20described%20by%20PDDL.%20However%2C%20existing%20approaches%20typically%0Arepresent%20planning%20states%20as%20fully%20connected%20graphs%2C%20leading%20to%20a%20combinatorial%0Aexplosion%20in%20edge%20information%20and%20substantial%20sparsity%20as%20problem%20scales%20grow%2C%0Aespecially%20evident%20in%20large%20grid-based%20environments.%20This%20dense%20representation%0Aresults%20in%20diluted%20node-level%20information%2C%20exponentially%20increases%20memory%0Arequirements%2C%20and%20ultimately%20makes%20learning%20infeasible%20for%20larger-scale%0Aproblems.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20sparse%2C%20goal-aware%20GNN%0Arepresentation%20that%20selectively%20encodes%20relevant%20local%20relationships%20and%0Aexplicitly%20integrates%20spatial%20features%20related%20to%20the%20goal.%20We%20validate%20our%0Aapproach%20by%20designing%20novel%20drone%20mission%20scenarios%20based%20on%20PDDL%20within%20a%20grid%0Aworld%2C%20effectively%20simulating%20realistic%20mission%20execution%20environments.%20Our%0Aexperimental%20results%20demonstrate%20that%20our%20method%20scales%20effectively%20to%20larger%0Agrid%20sizes%20previously%20infeasible%20with%20dense%20graph%20representations%20and%0Asubstantially%20improves%20policy%20generalization%20and%20success%20rates.%20Our%20findings%0Aprovide%20a%20practical%20foundation%20for%20addressing%20realistic%2C%20large-scale%0Ageneralized%20planning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10747v2&entry.124074799=Read"},
{"title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for\n  Pansharpening", "author": "Tao Tang and Chengxu Yang", "abstract": "  Pansharpening refers to the process of integrating a high resolution\npanchromatic (PAN) image with a lower resolution multispectral (MS) image to\ngenerate a fused product, which is pivotal in remote sensing. Despite the\neffectiveness of CNNs in addressing this challenge, they are inherently\nconstrained by the uniform application of convolutional kernels across all\nspatial positions, overlooking local content variations. To overcome this\nissue, we introduce RAPNet, a new architecture that leverages content-adaptive\nconvolution. At its core, RAPNet employs the Receptive-field Adaptive\nPansharpening Convolution (RAPConv), designed to produce spatially adaptive\nkernels responsive to local feature context, thereby enhancing the precision of\nspatial detail extraction. Additionally, the network integrates the\nPansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an\nattention mechanism to achieve an optimal balance between spatial detail\nenhancement and spectral fidelity. Comprehensive evaluations on publicly\navailable datasets confirm that RAPNet delivers superior performance compared\nto existing approaches, as demonstrated by both quantitative metrics and\nqualitative assessments. Ablation analyses further substantiate the\neffectiveness of the proposed adaptive components.\n", "link": "http://arxiv.org/abs/2507.10461v3", "date": "2025-08-19", "relevancy": 2.1411, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5848}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5066}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAPNet%3A%20A%20Receptive-Field%20Adaptive%20Convolutional%20Neural%20Network%20for%0A%20%20Pansharpening&body=Title%3A%20RAPNet%3A%20A%20Receptive-Field%20Adaptive%20Convolutional%20Neural%20Network%20for%0A%20%20Pansharpening%0AAuthor%3A%20Tao%20Tang%20and%20Chengxu%20Yang%0AAbstract%3A%20%20%20Pansharpening%20refers%20to%20the%20process%20of%20integrating%20a%20high%20resolution%0Apanchromatic%20%28PAN%29%20image%20with%20a%20lower%20resolution%20multispectral%20%28MS%29%20image%20to%0Agenerate%20a%20fused%20product%2C%20which%20is%20pivotal%20in%20remote%20sensing.%20Despite%20the%0Aeffectiveness%20of%20CNNs%20in%20addressing%20this%20challenge%2C%20they%20are%20inherently%0Aconstrained%20by%20the%20uniform%20application%20of%20convolutional%20kernels%20across%20all%0Aspatial%20positions%2C%20overlooking%20local%20content%20variations.%20To%20overcome%20this%0Aissue%2C%20we%20introduce%20RAPNet%2C%20a%20new%20architecture%20that%20leverages%20content-adaptive%0Aconvolution.%20At%20its%20core%2C%20RAPNet%20employs%20the%20Receptive-field%20Adaptive%0APansharpening%20Convolution%20%28RAPConv%29%2C%20designed%20to%20produce%20spatially%20adaptive%0Akernels%20responsive%20to%20local%20feature%20context%2C%20thereby%20enhancing%20the%20precision%20of%0Aspatial%20detail%20extraction.%20Additionally%2C%20the%20network%20integrates%20the%0APansharpening%20Dynamic%20Feature%20Fusion%20%28PAN-DFF%29%20module%2C%20which%20incorporates%20an%0Aattention%20mechanism%20to%20achieve%20an%20optimal%20balance%20between%20spatial%20detail%0Aenhancement%20and%20spectral%20fidelity.%20Comprehensive%20evaluations%20on%20publicly%0Aavailable%20datasets%20confirm%20that%20RAPNet%20delivers%20superior%20performance%20compared%0Ato%20existing%20approaches%2C%20as%20demonstrated%20by%20both%20quantitative%20metrics%20and%0Aqualitative%20assessments.%20Ablation%20analyses%20further%20substantiate%20the%0Aeffectiveness%20of%20the%20proposed%20adaptive%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10461v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAPNet%253A%2520A%2520Receptive-Field%2520Adaptive%2520Convolutional%2520Neural%2520Network%2520for%250A%2520%2520Pansharpening%26entry.906535625%3DTao%2520Tang%2520and%2520Chengxu%2520Yang%26entry.1292438233%3D%2520%2520Pansharpening%2520refers%2520to%2520the%2520process%2520of%2520integrating%2520a%2520high%2520resolution%250Apanchromatic%2520%2528PAN%2529%2520image%2520with%2520a%2520lower%2520resolution%2520multispectral%2520%2528MS%2529%2520image%2520to%250Agenerate%2520a%2520fused%2520product%252C%2520which%2520is%2520pivotal%2520in%2520remote%2520sensing.%2520Despite%2520the%250Aeffectiveness%2520of%2520CNNs%2520in%2520addressing%2520this%2520challenge%252C%2520they%2520are%2520inherently%250Aconstrained%2520by%2520the%2520uniform%2520application%2520of%2520convolutional%2520kernels%2520across%2520all%250Aspatial%2520positions%252C%2520overlooking%2520local%2520content%2520variations.%2520To%2520overcome%2520this%250Aissue%252C%2520we%2520introduce%2520RAPNet%252C%2520a%2520new%2520architecture%2520that%2520leverages%2520content-adaptive%250Aconvolution.%2520At%2520its%2520core%252C%2520RAPNet%2520employs%2520the%2520Receptive-field%2520Adaptive%250APansharpening%2520Convolution%2520%2528RAPConv%2529%252C%2520designed%2520to%2520produce%2520spatially%2520adaptive%250Akernels%2520responsive%2520to%2520local%2520feature%2520context%252C%2520thereby%2520enhancing%2520the%2520precision%2520of%250Aspatial%2520detail%2520extraction.%2520Additionally%252C%2520the%2520network%2520integrates%2520the%250APansharpening%2520Dynamic%2520Feature%2520Fusion%2520%2528PAN-DFF%2529%2520module%252C%2520which%2520incorporates%2520an%250Aattention%2520mechanism%2520to%2520achieve%2520an%2520optimal%2520balance%2520between%2520spatial%2520detail%250Aenhancement%2520and%2520spectral%2520fidelity.%2520Comprehensive%2520evaluations%2520on%2520publicly%250Aavailable%2520datasets%2520confirm%2520that%2520RAPNet%2520delivers%2520superior%2520performance%2520compared%250Ato%2520existing%2520approaches%252C%2520as%2520demonstrated%2520by%2520both%2520quantitative%2520metrics%2520and%250Aqualitative%2520assessments.%2520Ablation%2520analyses%2520further%2520substantiate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520adaptive%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10461v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAPNet%3A%20A%20Receptive-Field%20Adaptive%20Convolutional%20Neural%20Network%20for%0A%20%20Pansharpening&entry.906535625=Tao%20Tang%20and%20Chengxu%20Yang&entry.1292438233=%20%20Pansharpening%20refers%20to%20the%20process%20of%20integrating%20a%20high%20resolution%0Apanchromatic%20%28PAN%29%20image%20with%20a%20lower%20resolution%20multispectral%20%28MS%29%20image%20to%0Agenerate%20a%20fused%20product%2C%20which%20is%20pivotal%20in%20remote%20sensing.%20Despite%20the%0Aeffectiveness%20of%20CNNs%20in%20addressing%20this%20challenge%2C%20they%20are%20inherently%0Aconstrained%20by%20the%20uniform%20application%20of%20convolutional%20kernels%20across%20all%0Aspatial%20positions%2C%20overlooking%20local%20content%20variations.%20To%20overcome%20this%0Aissue%2C%20we%20introduce%20RAPNet%2C%20a%20new%20architecture%20that%20leverages%20content-adaptive%0Aconvolution.%20At%20its%20core%2C%20RAPNet%20employs%20the%20Receptive-field%20Adaptive%0APansharpening%20Convolution%20%28RAPConv%29%2C%20designed%20to%20produce%20spatially%20adaptive%0Akernels%20responsive%20to%20local%20feature%20context%2C%20thereby%20enhancing%20the%20precision%20of%0Aspatial%20detail%20extraction.%20Additionally%2C%20the%20network%20integrates%20the%0APansharpening%20Dynamic%20Feature%20Fusion%20%28PAN-DFF%29%20module%2C%20which%20incorporates%20an%0Aattention%20mechanism%20to%20achieve%20an%20optimal%20balance%20between%20spatial%20detail%0Aenhancement%20and%20spectral%20fidelity.%20Comprehensive%20evaluations%20on%20publicly%0Aavailable%20datasets%20confirm%20that%20RAPNet%20delivers%20superior%20performance%20compared%0Ato%20existing%20approaches%2C%20as%20demonstrated%20by%20both%20quantitative%20metrics%20and%0Aqualitative%20assessments.%20Ablation%20analyses%20further%20substantiate%20the%0Aeffectiveness%20of%20the%20proposed%20adaptive%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10461v3&entry.124074799=Read"},
{"title": "Spatial-Temporal Transformer with Curriculum Learning for EEG-Based\n  Emotion Recognition", "author": "Xuetao Lin and Tianhao Peng and Peihong Dai and Yu Liang and Wenjun Wu", "abstract": "  EEG-based emotion recognition plays an important role in developing adaptive\nbrain-computer communication systems, yet faces two fundamental challenges in\npractical implementations: (1) effective integration of non-stationary\nspatial-temporal neural patterns, (2) robust adaptation to dynamic emotional\nintensity variations in real-world scenarios. This paper proposes SST-CL, a\nnovel framework integrating spatial-temporal transformers with curriculum\nlearning. Our method introduces two core components: a spatial encoder that\nmodels inter-channel relationships and a temporal encoder that captures\nmulti-scale dependencies through windowed attention mechanisms, enabling\nsimultaneous extraction of spatial correlations and temporal dynamics from EEG\nsignals. Complementing this architecture, an intensity-aware curriculum\nlearning strategy progressively guides training from high-intensity to\nlow-intensity emotional states through dynamic sample scheduling based on a\ndual difficulty assessment. Comprehensive experiments on three benchmark\ndatasets demonstrate state-of-the-art performance across various emotional\nintensity levels, with ablation studies confirming the necessity of both\narchitectural components and the curriculum learning mechanism.\n", "link": "http://arxiv.org/abs/2507.14698v2", "date": "2025-08-19", "relevancy": 2.1384, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5549}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5225}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-Temporal%20Transformer%20with%20Curriculum%20Learning%20for%20EEG-Based%0A%20%20Emotion%20Recognition&body=Title%3A%20Spatial-Temporal%20Transformer%20with%20Curriculum%20Learning%20for%20EEG-Based%0A%20%20Emotion%20Recognition%0AAuthor%3A%20Xuetao%20Lin%20and%20Tianhao%20Peng%20and%20Peihong%20Dai%20and%20Yu%20Liang%20and%20Wenjun%20Wu%0AAbstract%3A%20%20%20EEG-based%20emotion%20recognition%20plays%20an%20important%20role%20in%20developing%20adaptive%0Abrain-computer%20communication%20systems%2C%20yet%20faces%20two%20fundamental%20challenges%20in%0Apractical%20implementations%3A%20%281%29%20effective%20integration%20of%20non-stationary%0Aspatial-temporal%20neural%20patterns%2C%20%282%29%20robust%20adaptation%20to%20dynamic%20emotional%0Aintensity%20variations%20in%20real-world%20scenarios.%20This%20paper%20proposes%20SST-CL%2C%20a%0Anovel%20framework%20integrating%20spatial-temporal%20transformers%20with%20curriculum%0Alearning.%20Our%20method%20introduces%20two%20core%20components%3A%20a%20spatial%20encoder%20that%0Amodels%20inter-channel%20relationships%20and%20a%20temporal%20encoder%20that%20captures%0Amulti-scale%20dependencies%20through%20windowed%20attention%20mechanisms%2C%20enabling%0Asimultaneous%20extraction%20of%20spatial%20correlations%20and%20temporal%20dynamics%20from%20EEG%0Asignals.%20Complementing%20this%20architecture%2C%20an%20intensity-aware%20curriculum%0Alearning%20strategy%20progressively%20guides%20training%20from%20high-intensity%20to%0Alow-intensity%20emotional%20states%20through%20dynamic%20sample%20scheduling%20based%20on%20a%0Adual%20difficulty%20assessment.%20Comprehensive%20experiments%20on%20three%20benchmark%0Adatasets%20demonstrate%20state-of-the-art%20performance%20across%20various%20emotional%0Aintensity%20levels%2C%20with%20ablation%20studies%20confirming%20the%20necessity%20of%20both%0Aarchitectural%20components%20and%20the%20curriculum%20learning%20mechanism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14698v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-Temporal%2520Transformer%2520with%2520Curriculum%2520Learning%2520for%2520EEG-Based%250A%2520%2520Emotion%2520Recognition%26entry.906535625%3DXuetao%2520Lin%2520and%2520Tianhao%2520Peng%2520and%2520Peihong%2520Dai%2520and%2520Yu%2520Liang%2520and%2520Wenjun%2520Wu%26entry.1292438233%3D%2520%2520EEG-based%2520emotion%2520recognition%2520plays%2520an%2520important%2520role%2520in%2520developing%2520adaptive%250Abrain-computer%2520communication%2520systems%252C%2520yet%2520faces%2520two%2520fundamental%2520challenges%2520in%250Apractical%2520implementations%253A%2520%25281%2529%2520effective%2520integration%2520of%2520non-stationary%250Aspatial-temporal%2520neural%2520patterns%252C%2520%25282%2529%2520robust%2520adaptation%2520to%2520dynamic%2520emotional%250Aintensity%2520variations%2520in%2520real-world%2520scenarios.%2520This%2520paper%2520proposes%2520SST-CL%252C%2520a%250Anovel%2520framework%2520integrating%2520spatial-temporal%2520transformers%2520with%2520curriculum%250Alearning.%2520Our%2520method%2520introduces%2520two%2520core%2520components%253A%2520a%2520spatial%2520encoder%2520that%250Amodels%2520inter-channel%2520relationships%2520and%2520a%2520temporal%2520encoder%2520that%2520captures%250Amulti-scale%2520dependencies%2520through%2520windowed%2520attention%2520mechanisms%252C%2520enabling%250Asimultaneous%2520extraction%2520of%2520spatial%2520correlations%2520and%2520temporal%2520dynamics%2520from%2520EEG%250Asignals.%2520Complementing%2520this%2520architecture%252C%2520an%2520intensity-aware%2520curriculum%250Alearning%2520strategy%2520progressively%2520guides%2520training%2520from%2520high-intensity%2520to%250Alow-intensity%2520emotional%2520states%2520through%2520dynamic%2520sample%2520scheduling%2520based%2520on%2520a%250Adual%2520difficulty%2520assessment.%2520Comprehensive%2520experiments%2520on%2520three%2520benchmark%250Adatasets%2520demonstrate%2520state-of-the-art%2520performance%2520across%2520various%2520emotional%250Aintensity%2520levels%252C%2520with%2520ablation%2520studies%2520confirming%2520the%2520necessity%2520of%2520both%250Aarchitectural%2520components%2520and%2520the%2520curriculum%2520learning%2520mechanism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14698v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-Temporal%20Transformer%20with%20Curriculum%20Learning%20for%20EEG-Based%0A%20%20Emotion%20Recognition&entry.906535625=Xuetao%20Lin%20and%20Tianhao%20Peng%20and%20Peihong%20Dai%20and%20Yu%20Liang%20and%20Wenjun%20Wu&entry.1292438233=%20%20EEG-based%20emotion%20recognition%20plays%20an%20important%20role%20in%20developing%20adaptive%0Abrain-computer%20communication%20systems%2C%20yet%20faces%20two%20fundamental%20challenges%20in%0Apractical%20implementations%3A%20%281%29%20effective%20integration%20of%20non-stationary%0Aspatial-temporal%20neural%20patterns%2C%20%282%29%20robust%20adaptation%20to%20dynamic%20emotional%0Aintensity%20variations%20in%20real-world%20scenarios.%20This%20paper%20proposes%20SST-CL%2C%20a%0Anovel%20framework%20integrating%20spatial-temporal%20transformers%20with%20curriculum%0Alearning.%20Our%20method%20introduces%20two%20core%20components%3A%20a%20spatial%20encoder%20that%0Amodels%20inter-channel%20relationships%20and%20a%20temporal%20encoder%20that%20captures%0Amulti-scale%20dependencies%20through%20windowed%20attention%20mechanisms%2C%20enabling%0Asimultaneous%20extraction%20of%20spatial%20correlations%20and%20temporal%20dynamics%20from%20EEG%0Asignals.%20Complementing%20this%20architecture%2C%20an%20intensity-aware%20curriculum%0Alearning%20strategy%20progressively%20guides%20training%20from%20high-intensity%20to%0Alow-intensity%20emotional%20states%20through%20dynamic%20sample%20scheduling%20based%20on%20a%0Adual%20difficulty%20assessment.%20Comprehensive%20experiments%20on%20three%20benchmark%0Adatasets%20demonstrate%20state-of-the-art%20performance%20across%20various%20emotional%0Aintensity%20levels%2C%20with%20ablation%20studies%20confirming%20the%20necessity%20of%20both%0Aarchitectural%20components%20and%20the%20curriculum%20learning%20mechanism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14698v2&entry.124074799=Read"},
{"title": "Driving Style Recognition Like an Expert Using Semantic Privileged\n  Information from Large Language Models", "author": "Zhaokun Chen and Chaopeng Zhang and Xiaohan Li and Wenshuo Wang and Gentiane Venture and Junqiang Xi", "abstract": "  Existing driving style recognition systems largely depend on low-level\nsensor-derived features for training, neglecting the rich semantic reasoning\ncapability inherent to human experts. This discrepancy results in a fundamental\nmisalignment between algorithmic classifications and expert judgments. To\nbridge this gap, we propose a novel framework that integrates Semantic\nPrivileged Information (SPI) derived from large language models (LLMs) to align\nrecognition outcomes with human-interpretable reasoning. First, we introduce\nDriBehavGPT, an interactive LLM-based module that generates natural-language\ndescriptions of driving behaviors. These descriptions are then encoded into\nmachine learning-compatible representations via text embedding and\ndimensionality reduction. Finally, we incorporate them as privileged\ninformation into Support Vector Machine Plus (SVM+) for training, enabling the\nmodel to approximate human-like interpretation patterns. Experiments across\ndiverse real-world driving scenarios demonstrate that our SPI-enhanced\nframework outperforms conventional methods, achieving F1-score improvements of\n7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively\nused during training, while inference relies solely on sensor data, ensuring\ncomputational efficiency without sacrificing performance. These results\nhighlight the pivotal role of semantic behavioral representations in improving\nrecognition accuracy while advancing interpretable, human-centric driving\nsystems.\n", "link": "http://arxiv.org/abs/2508.13881v1", "date": "2025-08-19", "relevancy": 2.1363, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5621}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5292}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Driving%20Style%20Recognition%20Like%20an%20Expert%20Using%20Semantic%20Privileged%0A%20%20Information%20from%20Large%20Language%20Models&body=Title%3A%20Driving%20Style%20Recognition%20Like%20an%20Expert%20Using%20Semantic%20Privileged%0A%20%20Information%20from%20Large%20Language%20Models%0AAuthor%3A%20Zhaokun%20Chen%20and%20Chaopeng%20Zhang%20and%20Xiaohan%20Li%20and%20Wenshuo%20Wang%20and%20Gentiane%20Venture%20and%20Junqiang%20Xi%0AAbstract%3A%20%20%20Existing%20driving%20style%20recognition%20systems%20largely%20depend%20on%20low-level%0Asensor-derived%20features%20for%20training%2C%20neglecting%20the%20rich%20semantic%20reasoning%0Acapability%20inherent%20to%20human%20experts.%20This%20discrepancy%20results%20in%20a%20fundamental%0Amisalignment%20between%20algorithmic%20classifications%20and%20expert%20judgments.%20To%0Abridge%20this%20gap%2C%20we%20propose%20a%20novel%20framework%20that%20integrates%20Semantic%0APrivileged%20Information%20%28SPI%29%20derived%20from%20large%20language%20models%20%28LLMs%29%20to%20align%0Arecognition%20outcomes%20with%20human-interpretable%20reasoning.%20First%2C%20we%20introduce%0ADriBehavGPT%2C%20an%20interactive%20LLM-based%20module%20that%20generates%20natural-language%0Adescriptions%20of%20driving%20behaviors.%20These%20descriptions%20are%20then%20encoded%20into%0Amachine%20learning-compatible%20representations%20via%20text%20embedding%20and%0Adimensionality%20reduction.%20Finally%2C%20we%20incorporate%20them%20as%20privileged%0Ainformation%20into%20Support%20Vector%20Machine%20Plus%20%28SVM%2B%29%20for%20training%2C%20enabling%20the%0Amodel%20to%20approximate%20human-like%20interpretation%20patterns.%20Experiments%20across%0Adiverse%20real-world%20driving%20scenarios%20demonstrate%20that%20our%20SPI-enhanced%0Aframework%20outperforms%20conventional%20methods%2C%20achieving%20F1-score%20improvements%20of%0A7.6%25%20%28car-following%29%20and%207.9%25%20%28lane-changing%29.%20Importantly%2C%20SPI%20is%20exclusively%0Aused%20during%20training%2C%20while%20inference%20relies%20solely%20on%20sensor%20data%2C%20ensuring%0Acomputational%20efficiency%20without%20sacrificing%20performance.%20These%20results%0Ahighlight%20the%20pivotal%20role%20of%20semantic%20behavioral%20representations%20in%20improving%0Arecognition%20accuracy%20while%20advancing%20interpretable%2C%20human-centric%20driving%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriving%2520Style%2520Recognition%2520Like%2520an%2520Expert%2520Using%2520Semantic%2520Privileged%250A%2520%2520Information%2520from%2520Large%2520Language%2520Models%26entry.906535625%3DZhaokun%2520Chen%2520and%2520Chaopeng%2520Zhang%2520and%2520Xiaohan%2520Li%2520and%2520Wenshuo%2520Wang%2520and%2520Gentiane%2520Venture%2520and%2520Junqiang%2520Xi%26entry.1292438233%3D%2520%2520Existing%2520driving%2520style%2520recognition%2520systems%2520largely%2520depend%2520on%2520low-level%250Asensor-derived%2520features%2520for%2520training%252C%2520neglecting%2520the%2520rich%2520semantic%2520reasoning%250Acapability%2520inherent%2520to%2520human%2520experts.%2520This%2520discrepancy%2520results%2520in%2520a%2520fundamental%250Amisalignment%2520between%2520algorithmic%2520classifications%2520and%2520expert%2520judgments.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520integrates%2520Semantic%250APrivileged%2520Information%2520%2528SPI%2529%2520derived%2520from%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520align%250Arecognition%2520outcomes%2520with%2520human-interpretable%2520reasoning.%2520First%252C%2520we%2520introduce%250ADriBehavGPT%252C%2520an%2520interactive%2520LLM-based%2520module%2520that%2520generates%2520natural-language%250Adescriptions%2520of%2520driving%2520behaviors.%2520These%2520descriptions%2520are%2520then%2520encoded%2520into%250Amachine%2520learning-compatible%2520representations%2520via%2520text%2520embedding%2520and%250Adimensionality%2520reduction.%2520Finally%252C%2520we%2520incorporate%2520them%2520as%2520privileged%250Ainformation%2520into%2520Support%2520Vector%2520Machine%2520Plus%2520%2528SVM%252B%2529%2520for%2520training%252C%2520enabling%2520the%250Amodel%2520to%2520approximate%2520human-like%2520interpretation%2520patterns.%2520Experiments%2520across%250Adiverse%2520real-world%2520driving%2520scenarios%2520demonstrate%2520that%2520our%2520SPI-enhanced%250Aframework%2520outperforms%2520conventional%2520methods%252C%2520achieving%2520F1-score%2520improvements%2520of%250A7.6%2525%2520%2528car-following%2529%2520and%25207.9%2525%2520%2528lane-changing%2529.%2520Importantly%252C%2520SPI%2520is%2520exclusively%250Aused%2520during%2520training%252C%2520while%2520inference%2520relies%2520solely%2520on%2520sensor%2520data%252C%2520ensuring%250Acomputational%2520efficiency%2520without%2520sacrificing%2520performance.%2520These%2520results%250Ahighlight%2520the%2520pivotal%2520role%2520of%2520semantic%2520behavioral%2520representations%2520in%2520improving%250Arecognition%2520accuracy%2520while%2520advancing%2520interpretable%252C%2520human-centric%2520driving%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Driving%20Style%20Recognition%20Like%20an%20Expert%20Using%20Semantic%20Privileged%0A%20%20Information%20from%20Large%20Language%20Models&entry.906535625=Zhaokun%20Chen%20and%20Chaopeng%20Zhang%20and%20Xiaohan%20Li%20and%20Wenshuo%20Wang%20and%20Gentiane%20Venture%20and%20Junqiang%20Xi&entry.1292438233=%20%20Existing%20driving%20style%20recognition%20systems%20largely%20depend%20on%20low-level%0Asensor-derived%20features%20for%20training%2C%20neglecting%20the%20rich%20semantic%20reasoning%0Acapability%20inherent%20to%20human%20experts.%20This%20discrepancy%20results%20in%20a%20fundamental%0Amisalignment%20between%20algorithmic%20classifications%20and%20expert%20judgments.%20To%0Abridge%20this%20gap%2C%20we%20propose%20a%20novel%20framework%20that%20integrates%20Semantic%0APrivileged%20Information%20%28SPI%29%20derived%20from%20large%20language%20models%20%28LLMs%29%20to%20align%0Arecognition%20outcomes%20with%20human-interpretable%20reasoning.%20First%2C%20we%20introduce%0ADriBehavGPT%2C%20an%20interactive%20LLM-based%20module%20that%20generates%20natural-language%0Adescriptions%20of%20driving%20behaviors.%20These%20descriptions%20are%20then%20encoded%20into%0Amachine%20learning-compatible%20representations%20via%20text%20embedding%20and%0Adimensionality%20reduction.%20Finally%2C%20we%20incorporate%20them%20as%20privileged%0Ainformation%20into%20Support%20Vector%20Machine%20Plus%20%28SVM%2B%29%20for%20training%2C%20enabling%20the%0Amodel%20to%20approximate%20human-like%20interpretation%20patterns.%20Experiments%20across%0Adiverse%20real-world%20driving%20scenarios%20demonstrate%20that%20our%20SPI-enhanced%0Aframework%20outperforms%20conventional%20methods%2C%20achieving%20F1-score%20improvements%20of%0A7.6%25%20%28car-following%29%20and%207.9%25%20%28lane-changing%29.%20Importantly%2C%20SPI%20is%20exclusively%0Aused%20during%20training%2C%20while%20inference%20relies%20solely%20on%20sensor%20data%2C%20ensuring%0Acomputational%20efficiency%20without%20sacrificing%20performance.%20These%20results%0Ahighlight%20the%20pivotal%20role%20of%20semantic%20behavioral%20representations%20in%20improving%0Arecognition%20accuracy%20while%20advancing%20interpretable%2C%20human-centric%20driving%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13881v1&entry.124074799=Read"},
{"title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM\n  Preference Optimization", "author": "Shaohua Duan and Xinze Li and Zhenghao Liu and Xiaoyuan Yi and Yukun Yan and Shuo Wang and Yu Gu and Ge Yu and Maosong Sun", "abstract": "  Long-context modeling is critical for a wide range of real-world tasks,\nincluding long-context question answering, summarization, and complex reasoning\ntasks. Recent studies have explored fine-tuning Large Language Models (LLMs)\nwith synthetic data to enhance their long-context capabilities. However, the\neffectiveness of such approaches is often limited by the low diversity and\nfactual inconsistencies in the generated data. To address these challenges, we\npropose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB)\nrollout strategy to identify the most informative chunks from the given long\ncontext for sampling high-quality and diverse responses and constructing\npreference data pairs for Direct Preference Optimization (DPO) training.\nSpecifically, we treat context chunks as arms of MAB, select chunks based on\ntheir expected reward scores to input into LLMs to generate responses, and\niteratively update these scores based on reward feedback. This exploration and\nexploitation process enables the model to focus on the most relevant context\nsegments, thereby generating and collecting high-quality and diverse responses.\nFinally, we collect these generated responses from the rollout process and\napply the DPO method to further optimize the LLM. Experimental results show\nthat LongMab-PO significantly improves the diversity and quality of preference\ndata pairs, achieving state-of-the-art performance on long-context reasoning\nbenchmarks. All code and data will be released on\nhttps://github.com/NEUIR/LongMab-PO.\n", "link": "http://arxiv.org/abs/2508.13993v1", "date": "2025-08-19", "relevancy": 2.1341, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5517}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.525}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chunks%20as%20Arms%3A%20Multi-Armed%20Bandit-Guided%20Sampling%20for%20Long-Context%20LLM%0A%20%20Preference%20Optimization&body=Title%3A%20Chunks%20as%20Arms%3A%20Multi-Armed%20Bandit-Guided%20Sampling%20for%20Long-Context%20LLM%0A%20%20Preference%20Optimization%0AAuthor%3A%20Shaohua%20Duan%20and%20Xinze%20Li%20and%20Zhenghao%20Liu%20and%20Xiaoyuan%20Yi%20and%20Yukun%20Yan%20and%20Shuo%20Wang%20and%20Yu%20Gu%20and%20Ge%20Yu%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Long-context%20modeling%20is%20critical%20for%20a%20wide%20range%20of%20real-world%20tasks%2C%0Aincluding%20long-context%20question%20answering%2C%20summarization%2C%20and%20complex%20reasoning%0Atasks.%20Recent%20studies%20have%20explored%20fine-tuning%20Large%20Language%20Models%20%28LLMs%29%0Awith%20synthetic%20data%20to%20enhance%20their%20long-context%20capabilities.%20However%2C%20the%0Aeffectiveness%20of%20such%20approaches%20is%20often%20limited%20by%20the%20low%20diversity%20and%0Afactual%20inconsistencies%20in%20the%20generated%20data.%20To%20address%20these%20challenges%2C%20we%0Apropose%20LongMab-PO%2C%20a%20novel%20framework%20that%20leverages%20a%20Multi-Armed%20Bandit%20%28MAB%29%0Arollout%20strategy%20to%20identify%20the%20most%20informative%20chunks%20from%20the%20given%20long%0Acontext%20for%20sampling%20high-quality%20and%20diverse%20responses%20and%20constructing%0Apreference%20data%20pairs%20for%20Direct%20Preference%20Optimization%20%28DPO%29%20training.%0ASpecifically%2C%20we%20treat%20context%20chunks%20as%20arms%20of%20MAB%2C%20select%20chunks%20based%20on%0Atheir%20expected%20reward%20scores%20to%20input%20into%20LLMs%20to%20generate%20responses%2C%20and%0Aiteratively%20update%20these%20scores%20based%20on%20reward%20feedback.%20This%20exploration%20and%0Aexploitation%20process%20enables%20the%20model%20to%20focus%20on%20the%20most%20relevant%20context%0Asegments%2C%20thereby%20generating%20and%20collecting%20high-quality%20and%20diverse%20responses.%0AFinally%2C%20we%20collect%20these%20generated%20responses%20from%20the%20rollout%20process%20and%0Aapply%20the%20DPO%20method%20to%20further%20optimize%20the%20LLM.%20Experimental%20results%20show%0Athat%20LongMab-PO%20significantly%20improves%20the%20diversity%20and%20quality%20of%20preference%0Adata%20pairs%2C%20achieving%20state-of-the-art%20performance%20on%20long-context%20reasoning%0Abenchmarks.%20All%20code%20and%20data%20will%20be%20released%20on%0Ahttps%3A//github.com/NEUIR/LongMab-PO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13993v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChunks%2520as%2520Arms%253A%2520Multi-Armed%2520Bandit-Guided%2520Sampling%2520for%2520Long-Context%2520LLM%250A%2520%2520Preference%2520Optimization%26entry.906535625%3DShaohua%2520Duan%2520and%2520Xinze%2520Li%2520and%2520Zhenghao%2520Liu%2520and%2520Xiaoyuan%2520Yi%2520and%2520Yukun%2520Yan%2520and%2520Shuo%2520Wang%2520and%2520Yu%2520Gu%2520and%2520Ge%2520Yu%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Long-context%2520modeling%2520is%2520critical%2520for%2520a%2520wide%2520range%2520of%2520real-world%2520tasks%252C%250Aincluding%2520long-context%2520question%2520answering%252C%2520summarization%252C%2520and%2520complex%2520reasoning%250Atasks.%2520Recent%2520studies%2520have%2520explored%2520fine-tuning%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Awith%2520synthetic%2520data%2520to%2520enhance%2520their%2520long-context%2520capabilities.%2520However%252C%2520the%250Aeffectiveness%2520of%2520such%2520approaches%2520is%2520often%2520limited%2520by%2520the%2520low%2520diversity%2520and%250Afactual%2520inconsistencies%2520in%2520the%2520generated%2520data.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520LongMab-PO%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520a%2520Multi-Armed%2520Bandit%2520%2528MAB%2529%250Arollout%2520strategy%2520to%2520identify%2520the%2520most%2520informative%2520chunks%2520from%2520the%2520given%2520long%250Acontext%2520for%2520sampling%2520high-quality%2520and%2520diverse%2520responses%2520and%2520constructing%250Apreference%2520data%2520pairs%2520for%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520training.%250ASpecifically%252C%2520we%2520treat%2520context%2520chunks%2520as%2520arms%2520of%2520MAB%252C%2520select%2520chunks%2520based%2520on%250Atheir%2520expected%2520reward%2520scores%2520to%2520input%2520into%2520LLMs%2520to%2520generate%2520responses%252C%2520and%250Aiteratively%2520update%2520these%2520scores%2520based%2520on%2520reward%2520feedback.%2520This%2520exploration%2520and%250Aexploitation%2520process%2520enables%2520the%2520model%2520to%2520focus%2520on%2520the%2520most%2520relevant%2520context%250Asegments%252C%2520thereby%2520generating%2520and%2520collecting%2520high-quality%2520and%2520diverse%2520responses.%250AFinally%252C%2520we%2520collect%2520these%2520generated%2520responses%2520from%2520the%2520rollout%2520process%2520and%250Aapply%2520the%2520DPO%2520method%2520to%2520further%2520optimize%2520the%2520LLM.%2520Experimental%2520results%2520show%250Athat%2520LongMab-PO%2520significantly%2520improves%2520the%2520diversity%2520and%2520quality%2520of%2520preference%250Adata%2520pairs%252C%2520achieving%2520state-of-the-art%2520performance%2520on%2520long-context%2520reasoning%250Abenchmarks.%2520All%2520code%2520and%2520data%2520will%2520be%2520released%2520on%250Ahttps%253A//github.com/NEUIR/LongMab-PO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13993v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chunks%20as%20Arms%3A%20Multi-Armed%20Bandit-Guided%20Sampling%20for%20Long-Context%20LLM%0A%20%20Preference%20Optimization&entry.906535625=Shaohua%20Duan%20and%20Xinze%20Li%20and%20Zhenghao%20Liu%20and%20Xiaoyuan%20Yi%20and%20Yukun%20Yan%20and%20Shuo%20Wang%20and%20Yu%20Gu%20and%20Ge%20Yu%20and%20Maosong%20Sun&entry.1292438233=%20%20Long-context%20modeling%20is%20critical%20for%20a%20wide%20range%20of%20real-world%20tasks%2C%0Aincluding%20long-context%20question%20answering%2C%20summarization%2C%20and%20complex%20reasoning%0Atasks.%20Recent%20studies%20have%20explored%20fine-tuning%20Large%20Language%20Models%20%28LLMs%29%0Awith%20synthetic%20data%20to%20enhance%20their%20long-context%20capabilities.%20However%2C%20the%0Aeffectiveness%20of%20such%20approaches%20is%20often%20limited%20by%20the%20low%20diversity%20and%0Afactual%20inconsistencies%20in%20the%20generated%20data.%20To%20address%20these%20challenges%2C%20we%0Apropose%20LongMab-PO%2C%20a%20novel%20framework%20that%20leverages%20a%20Multi-Armed%20Bandit%20%28MAB%29%0Arollout%20strategy%20to%20identify%20the%20most%20informative%20chunks%20from%20the%20given%20long%0Acontext%20for%20sampling%20high-quality%20and%20diverse%20responses%20and%20constructing%0Apreference%20data%20pairs%20for%20Direct%20Preference%20Optimization%20%28DPO%29%20training.%0ASpecifically%2C%20we%20treat%20context%20chunks%20as%20arms%20of%20MAB%2C%20select%20chunks%20based%20on%0Atheir%20expected%20reward%20scores%20to%20input%20into%20LLMs%20to%20generate%20responses%2C%20and%0Aiteratively%20update%20these%20scores%20based%20on%20reward%20feedback.%20This%20exploration%20and%0Aexploitation%20process%20enables%20the%20model%20to%20focus%20on%20the%20most%20relevant%20context%0Asegments%2C%20thereby%20generating%20and%20collecting%20high-quality%20and%20diverse%20responses.%0AFinally%2C%20we%20collect%20these%20generated%20responses%20from%20the%20rollout%20process%20and%0Aapply%20the%20DPO%20method%20to%20further%20optimize%20the%20LLM.%20Experimental%20results%20show%0Athat%20LongMab-PO%20significantly%20improves%20the%20diversity%20and%20quality%20of%20preference%0Adata%20pairs%2C%20achieving%20state-of-the-art%20performance%20on%20long-context%20reasoning%0Abenchmarks.%20All%20code%20and%20data%20will%20be%20released%20on%0Ahttps%3A//github.com/NEUIR/LongMab-PO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13993v1&entry.124074799=Read"},
{"title": "ComputerRL: Scaling End-to-End Online Reinforcement Learning for\n  Computer Use Agents", "author": "Hanyu Lai and Xiao Liu and Yanxiao Zhao and Han Xu and Hanchen Zhang and Bohao Jing and Yanyu Ren and Shuntian Yao and Yuxiao Dong and Jie Tang", "abstract": "  We introduce ComputerRL, a framework for autonomous desktop intelligence that\nenables agents to operate complex digital workspaces skillfully. ComputerRL\nfeatures the API-GUI paradigm, which unifies programmatic API calls and direct\nGUI interaction to address the inherent mismatch between machine agents and\nhuman-centric desktop environments. Scaling end-to-end RL training is crucial\nfor improvement and generalization across diverse desktop tasks, yet remains\nchallenging due to environmental inefficiency and instability in extended\ntraining. To support scalable and robust training, we develop a distributed RL\ninfrastructure capable of orchestrating thousands of parallel virtual desktop\nenvironments to accelerate large-scale online RL. Furthermore, we propose\nEntropulse, a training strategy that alternates reinforcement learning with\nsupervised fine-tuning, effectively mitigating entropy collapse during extended\ntraining runs. We employ ComputerRL on open models GLM-4-9B-0414 and\nQwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B\nbased on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%,\ndemonstrating significant improvements for general agents in desktop\nautomation. The algorithm and framework are adopted in building AutoGLM (Liu et\nal., 2024a)\n", "link": "http://arxiv.org/abs/2508.14040v1", "date": "2025-08-19", "relevancy": 2.1325, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5482}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5306}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ComputerRL%3A%20Scaling%20End-to-End%20Online%20Reinforcement%20Learning%20for%0A%20%20Computer%20Use%20Agents&body=Title%3A%20ComputerRL%3A%20Scaling%20End-to-End%20Online%20Reinforcement%20Learning%20for%0A%20%20Computer%20Use%20Agents%0AAuthor%3A%20Hanyu%20Lai%20and%20Xiao%20Liu%20and%20Yanxiao%20Zhao%20and%20Han%20Xu%20and%20Hanchen%20Zhang%20and%20Bohao%20Jing%20and%20Yanyu%20Ren%20and%20Shuntian%20Yao%20and%20Yuxiao%20Dong%20and%20Jie%20Tang%0AAbstract%3A%20%20%20We%20introduce%20ComputerRL%2C%20a%20framework%20for%20autonomous%20desktop%20intelligence%20that%0Aenables%20agents%20to%20operate%20complex%20digital%20workspaces%20skillfully.%20ComputerRL%0Afeatures%20the%20API-GUI%20paradigm%2C%20which%20unifies%20programmatic%20API%20calls%20and%20direct%0AGUI%20interaction%20to%20address%20the%20inherent%20mismatch%20between%20machine%20agents%20and%0Ahuman-centric%20desktop%20environments.%20Scaling%20end-to-end%20RL%20training%20is%20crucial%0Afor%20improvement%20and%20generalization%20across%20diverse%20desktop%20tasks%2C%20yet%20remains%0Achallenging%20due%20to%20environmental%20inefficiency%20and%20instability%20in%20extended%0Atraining.%20To%20support%20scalable%20and%20robust%20training%2C%20we%20develop%20a%20distributed%20RL%0Ainfrastructure%20capable%20of%20orchestrating%20thousands%20of%20parallel%20virtual%20desktop%0Aenvironments%20to%20accelerate%20large-scale%20online%20RL.%20Furthermore%2C%20we%20propose%0AEntropulse%2C%20a%20training%20strategy%20that%20alternates%20reinforcement%20learning%20with%0Asupervised%20fine-tuning%2C%20effectively%20mitigating%20entropy%20collapse%20during%20extended%0Atraining%20runs.%20We%20employ%20ComputerRL%20on%20open%20models%20GLM-4-9B-0414%20and%0AQwen2.5-14B%2C%20and%20evaluate%20them%20on%20the%20OSWorld%20benchmark.%20The%20AutoGLM-OS-9B%0Abased%20on%20GLM-4-9B-0414%20achieves%20a%20new%20state-of-the-art%20accuracy%20of%2048.1%25%2C%0Ademonstrating%20significant%20improvements%20for%20general%20agents%20in%20desktop%0Aautomation.%20The%20algorithm%20and%20framework%20are%20adopted%20in%20building%20AutoGLM%20%28Liu%20et%0Aal.%2C%202024a%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputerRL%253A%2520Scaling%2520End-to-End%2520Online%2520Reinforcement%2520Learning%2520for%250A%2520%2520Computer%2520Use%2520Agents%26entry.906535625%3DHanyu%2520Lai%2520and%2520Xiao%2520Liu%2520and%2520Yanxiao%2520Zhao%2520and%2520Han%2520Xu%2520and%2520Hanchen%2520Zhang%2520and%2520Bohao%2520Jing%2520and%2520Yanyu%2520Ren%2520and%2520Shuntian%2520Yao%2520and%2520Yuxiao%2520Dong%2520and%2520Jie%2520Tang%26entry.1292438233%3D%2520%2520We%2520introduce%2520ComputerRL%252C%2520a%2520framework%2520for%2520autonomous%2520desktop%2520intelligence%2520that%250Aenables%2520agents%2520to%2520operate%2520complex%2520digital%2520workspaces%2520skillfully.%2520ComputerRL%250Afeatures%2520the%2520API-GUI%2520paradigm%252C%2520which%2520unifies%2520programmatic%2520API%2520calls%2520and%2520direct%250AGUI%2520interaction%2520to%2520address%2520the%2520inherent%2520mismatch%2520between%2520machine%2520agents%2520and%250Ahuman-centric%2520desktop%2520environments.%2520Scaling%2520end-to-end%2520RL%2520training%2520is%2520crucial%250Afor%2520improvement%2520and%2520generalization%2520across%2520diverse%2520desktop%2520tasks%252C%2520yet%2520remains%250Achallenging%2520due%2520to%2520environmental%2520inefficiency%2520and%2520instability%2520in%2520extended%250Atraining.%2520To%2520support%2520scalable%2520and%2520robust%2520training%252C%2520we%2520develop%2520a%2520distributed%2520RL%250Ainfrastructure%2520capable%2520of%2520orchestrating%2520thousands%2520of%2520parallel%2520virtual%2520desktop%250Aenvironments%2520to%2520accelerate%2520large-scale%2520online%2520RL.%2520Furthermore%252C%2520we%2520propose%250AEntropulse%252C%2520a%2520training%2520strategy%2520that%2520alternates%2520reinforcement%2520learning%2520with%250Asupervised%2520fine-tuning%252C%2520effectively%2520mitigating%2520entropy%2520collapse%2520during%2520extended%250Atraining%2520runs.%2520We%2520employ%2520ComputerRL%2520on%2520open%2520models%2520GLM-4-9B-0414%2520and%250AQwen2.5-14B%252C%2520and%2520evaluate%2520them%2520on%2520the%2520OSWorld%2520benchmark.%2520The%2520AutoGLM-OS-9B%250Abased%2520on%2520GLM-4-9B-0414%2520achieves%2520a%2520new%2520state-of-the-art%2520accuracy%2520of%252048.1%2525%252C%250Ademonstrating%2520significant%2520improvements%2520for%2520general%2520agents%2520in%2520desktop%250Aautomation.%2520The%2520algorithm%2520and%2520framework%2520are%2520adopted%2520in%2520building%2520AutoGLM%2520%2528Liu%2520et%250Aal.%252C%25202024a%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ComputerRL%3A%20Scaling%20End-to-End%20Online%20Reinforcement%20Learning%20for%0A%20%20Computer%20Use%20Agents&entry.906535625=Hanyu%20Lai%20and%20Xiao%20Liu%20and%20Yanxiao%20Zhao%20and%20Han%20Xu%20and%20Hanchen%20Zhang%20and%20Bohao%20Jing%20and%20Yanyu%20Ren%20and%20Shuntian%20Yao%20and%20Yuxiao%20Dong%20and%20Jie%20Tang&entry.1292438233=%20%20We%20introduce%20ComputerRL%2C%20a%20framework%20for%20autonomous%20desktop%20intelligence%20that%0Aenables%20agents%20to%20operate%20complex%20digital%20workspaces%20skillfully.%20ComputerRL%0Afeatures%20the%20API-GUI%20paradigm%2C%20which%20unifies%20programmatic%20API%20calls%20and%20direct%0AGUI%20interaction%20to%20address%20the%20inherent%20mismatch%20between%20machine%20agents%20and%0Ahuman-centric%20desktop%20environments.%20Scaling%20end-to-end%20RL%20training%20is%20crucial%0Afor%20improvement%20and%20generalization%20across%20diverse%20desktop%20tasks%2C%20yet%20remains%0Achallenging%20due%20to%20environmental%20inefficiency%20and%20instability%20in%20extended%0Atraining.%20To%20support%20scalable%20and%20robust%20training%2C%20we%20develop%20a%20distributed%20RL%0Ainfrastructure%20capable%20of%20orchestrating%20thousands%20of%20parallel%20virtual%20desktop%0Aenvironments%20to%20accelerate%20large-scale%20online%20RL.%20Furthermore%2C%20we%20propose%0AEntropulse%2C%20a%20training%20strategy%20that%20alternates%20reinforcement%20learning%20with%0Asupervised%20fine-tuning%2C%20effectively%20mitigating%20entropy%20collapse%20during%20extended%0Atraining%20runs.%20We%20employ%20ComputerRL%20on%20open%20models%20GLM-4-9B-0414%20and%0AQwen2.5-14B%2C%20and%20evaluate%20them%20on%20the%20OSWorld%20benchmark.%20The%20AutoGLM-OS-9B%0Abased%20on%20GLM-4-9B-0414%20achieves%20a%20new%20state-of-the-art%20accuracy%20of%2048.1%25%2C%0Ademonstrating%20significant%20improvements%20for%20general%20agents%20in%20desktop%0Aautomation.%20The%20algorithm%20and%20framework%20are%20adopted%20in%20building%20AutoGLM%20%28Liu%20et%0Aal.%2C%202024a%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14040v1&entry.124074799=Read"},
{"title": "SBP-YOLO:A Lightweight Real-Time Model for Detecting Speed Bumps and\n  Potholes", "author": "Chuanqi Liang and Jie Fu and Miao Yu and Lei Luo", "abstract": "  Reliable and real-time detection of road speed bumps and potholes is crucial\nfor anticipatory perception in advanced suspension systems, enabling timely and\nadaptive damping control. Achieving high accuracy and efficiency on embedded\nplatforms remains challenging due to limited computational resources and the\nsmall scale of distant targets. This paper presents SBP-YOLO, a lightweight and\nhigh-speed detection framework tailored for bump and pothole recognition. Based\non YOLOv11n, the model integrates GhostConv and VoVGSCSPC modules into the\nbackbone and neck to reduce computation while enhancing multi-scale semantic\nfeatures. To improve small-object detection, a P2-level branch is introduced\nwith a lightweight and efficient detection head LEDH mitigating the added\ncomputational overhead without compromising accuracy. A hybrid training\nstrategy combining NWD loss, backbone-level knowledge distillation, and\nAlbumentations-driven augmentation further enhances localization precision and\nrobustness. Experiments show that SBP-YOLO achieves 87.0 percent mAP,\noutperforming the YOLOv11n baseline by 5.8 percent. After TensorRT FP16\nquantization, it runs at 139.5 FPS on Jetson AGX Xavier, delivering a 12.4\npercent speedup over the P2-enhanced YOLOv11. These results validate the\neffectiveness of the proposed method for fast and low-latency road condition\nperception in embedded suspension control systems.\n", "link": "http://arxiv.org/abs/2508.01339v2", "date": "2025-08-19", "relevancy": 2.1263, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5521}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5365}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SBP-YOLO%3AA%20Lightweight%20Real-Time%20Model%20for%20Detecting%20Speed%20Bumps%20and%0A%20%20Potholes&body=Title%3A%20SBP-YOLO%3AA%20Lightweight%20Real-Time%20Model%20for%20Detecting%20Speed%20Bumps%20and%0A%20%20Potholes%0AAuthor%3A%20Chuanqi%20Liang%20and%20Jie%20Fu%20and%20Miao%20Yu%20and%20Lei%20Luo%0AAbstract%3A%20%20%20Reliable%20and%20real-time%20detection%20of%20road%20speed%20bumps%20and%20potholes%20is%20crucial%0Afor%20anticipatory%20perception%20in%20advanced%20suspension%20systems%2C%20enabling%20timely%20and%0Aadaptive%20damping%20control.%20Achieving%20high%20accuracy%20and%20efficiency%20on%20embedded%0Aplatforms%20remains%20challenging%20due%20to%20limited%20computational%20resources%20and%20the%0Asmall%20scale%20of%20distant%20targets.%20This%20paper%20presents%20SBP-YOLO%2C%20a%20lightweight%20and%0Ahigh-speed%20detection%20framework%20tailored%20for%20bump%20and%20pothole%20recognition.%20Based%0Aon%20YOLOv11n%2C%20the%20model%20integrates%20GhostConv%20and%20VoVGSCSPC%20modules%20into%20the%0Abackbone%20and%20neck%20to%20reduce%20computation%20while%20enhancing%20multi-scale%20semantic%0Afeatures.%20To%20improve%20small-object%20detection%2C%20a%20P2-level%20branch%20is%20introduced%0Awith%20a%20lightweight%20and%20efficient%20detection%20head%20LEDH%20mitigating%20the%20added%0Acomputational%20overhead%20without%20compromising%20accuracy.%20A%20hybrid%20training%0Astrategy%20combining%20NWD%20loss%2C%20backbone-level%20knowledge%20distillation%2C%20and%0AAlbumentations-driven%20augmentation%20further%20enhances%20localization%20precision%20and%0Arobustness.%20Experiments%20show%20that%20SBP-YOLO%20achieves%2087.0%20percent%20mAP%2C%0Aoutperforming%20the%20YOLOv11n%20baseline%20by%205.8%20percent.%20After%20TensorRT%20FP16%0Aquantization%2C%20it%20runs%20at%20139.5%20FPS%20on%20Jetson%20AGX%20Xavier%2C%20delivering%20a%2012.4%0Apercent%20speedup%20over%20the%20P2-enhanced%20YOLOv11.%20These%20results%20validate%20the%0Aeffectiveness%20of%20the%20proposed%20method%20for%20fast%20and%20low-latency%20road%20condition%0Aperception%20in%20embedded%20suspension%20control%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01339v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSBP-YOLO%253AA%2520Lightweight%2520Real-Time%2520Model%2520for%2520Detecting%2520Speed%2520Bumps%2520and%250A%2520%2520Potholes%26entry.906535625%3DChuanqi%2520Liang%2520and%2520Jie%2520Fu%2520and%2520Miao%2520Yu%2520and%2520Lei%2520Luo%26entry.1292438233%3D%2520%2520Reliable%2520and%2520real-time%2520detection%2520of%2520road%2520speed%2520bumps%2520and%2520potholes%2520is%2520crucial%250Afor%2520anticipatory%2520perception%2520in%2520advanced%2520suspension%2520systems%252C%2520enabling%2520timely%2520and%250Aadaptive%2520damping%2520control.%2520Achieving%2520high%2520accuracy%2520and%2520efficiency%2520on%2520embedded%250Aplatforms%2520remains%2520challenging%2520due%2520to%2520limited%2520computational%2520resources%2520and%2520the%250Asmall%2520scale%2520of%2520distant%2520targets.%2520This%2520paper%2520presents%2520SBP-YOLO%252C%2520a%2520lightweight%2520and%250Ahigh-speed%2520detection%2520framework%2520tailored%2520for%2520bump%2520and%2520pothole%2520recognition.%2520Based%250Aon%2520YOLOv11n%252C%2520the%2520model%2520integrates%2520GhostConv%2520and%2520VoVGSCSPC%2520modules%2520into%2520the%250Abackbone%2520and%2520neck%2520to%2520reduce%2520computation%2520while%2520enhancing%2520multi-scale%2520semantic%250Afeatures.%2520To%2520improve%2520small-object%2520detection%252C%2520a%2520P2-level%2520branch%2520is%2520introduced%250Awith%2520a%2520lightweight%2520and%2520efficient%2520detection%2520head%2520LEDH%2520mitigating%2520the%2520added%250Acomputational%2520overhead%2520without%2520compromising%2520accuracy.%2520A%2520hybrid%2520training%250Astrategy%2520combining%2520NWD%2520loss%252C%2520backbone-level%2520knowledge%2520distillation%252C%2520and%250AAlbumentations-driven%2520augmentation%2520further%2520enhances%2520localization%2520precision%2520and%250Arobustness.%2520Experiments%2520show%2520that%2520SBP-YOLO%2520achieves%252087.0%2520percent%2520mAP%252C%250Aoutperforming%2520the%2520YOLOv11n%2520baseline%2520by%25205.8%2520percent.%2520After%2520TensorRT%2520FP16%250Aquantization%252C%2520it%2520runs%2520at%2520139.5%2520FPS%2520on%2520Jetson%2520AGX%2520Xavier%252C%2520delivering%2520a%252012.4%250Apercent%2520speedup%2520over%2520the%2520P2-enhanced%2520YOLOv11.%2520These%2520results%2520validate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520method%2520for%2520fast%2520and%2520low-latency%2520road%2520condition%250Aperception%2520in%2520embedded%2520suspension%2520control%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01339v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SBP-YOLO%3AA%20Lightweight%20Real-Time%20Model%20for%20Detecting%20Speed%20Bumps%20and%0A%20%20Potholes&entry.906535625=Chuanqi%20Liang%20and%20Jie%20Fu%20and%20Miao%20Yu%20and%20Lei%20Luo&entry.1292438233=%20%20Reliable%20and%20real-time%20detection%20of%20road%20speed%20bumps%20and%20potholes%20is%20crucial%0Afor%20anticipatory%20perception%20in%20advanced%20suspension%20systems%2C%20enabling%20timely%20and%0Aadaptive%20damping%20control.%20Achieving%20high%20accuracy%20and%20efficiency%20on%20embedded%0Aplatforms%20remains%20challenging%20due%20to%20limited%20computational%20resources%20and%20the%0Asmall%20scale%20of%20distant%20targets.%20This%20paper%20presents%20SBP-YOLO%2C%20a%20lightweight%20and%0Ahigh-speed%20detection%20framework%20tailored%20for%20bump%20and%20pothole%20recognition.%20Based%0Aon%20YOLOv11n%2C%20the%20model%20integrates%20GhostConv%20and%20VoVGSCSPC%20modules%20into%20the%0Abackbone%20and%20neck%20to%20reduce%20computation%20while%20enhancing%20multi-scale%20semantic%0Afeatures.%20To%20improve%20small-object%20detection%2C%20a%20P2-level%20branch%20is%20introduced%0Awith%20a%20lightweight%20and%20efficient%20detection%20head%20LEDH%20mitigating%20the%20added%0Acomputational%20overhead%20without%20compromising%20accuracy.%20A%20hybrid%20training%0Astrategy%20combining%20NWD%20loss%2C%20backbone-level%20knowledge%20distillation%2C%20and%0AAlbumentations-driven%20augmentation%20further%20enhances%20localization%20precision%20and%0Arobustness.%20Experiments%20show%20that%20SBP-YOLO%20achieves%2087.0%20percent%20mAP%2C%0Aoutperforming%20the%20YOLOv11n%20baseline%20by%205.8%20percent.%20After%20TensorRT%20FP16%0Aquantization%2C%20it%20runs%20at%20139.5%20FPS%20on%20Jetson%20AGX%20Xavier%2C%20delivering%20a%2012.4%0Apercent%20speedup%20over%20the%20P2-enhanced%20YOLOv11.%20These%20results%20validate%20the%0Aeffectiveness%20of%20the%20proposed%20method%20for%20fast%20and%20low-latency%20road%20condition%0Aperception%20in%20embedded%20suspension%20control%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01339v2&entry.124074799=Read"},
{"title": "Hallucinations and Key Information Extraction in Medical Texts: A\n  Comprehensive Assessment of Open-Source Large Language Models", "author": "Anindya Bijoy Das and Shibbir Ahmed and Shahnewaz Karim Sakib", "abstract": "  Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, including admission reasons, major in-hospital events, and\ncritical follow-up actions. In addition, we also assess the prevalence of\nvarious types of hallucinations in the summaries produced by these models.\nDetecting hallucinations is vital as it directly influences the reliability of\nthe information, potentially affecting patient care and treatment outcomes. We\nconduct comprehensive simulations to rigorously evaluate the performance of\nthese models, further probing the accuracy and fidelity of the extracted\ncontent in clinical summarization. Our results reveal that while the LLMs\n(e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admission\nreasons and hospitalization events, they are generally less consistent when it\ncomes to identifying follow-up recommendations, highlighting broader challenges\nin leveraging LLMs for comprehensive summarization.\n", "link": "http://arxiv.org/abs/2504.19061v2", "date": "2025-08-19", "relevancy": 2.1121, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hallucinations%20and%20Key%20Information%20Extraction%20in%20Medical%20Texts%3A%20A%0A%20%20Comprehensive%20Assessment%20of%20Open-Source%20Large%20Language%20Models&body=Title%3A%20Hallucinations%20and%20Key%20Information%20Extraction%20in%20Medical%20Texts%3A%20A%0A%20%20Comprehensive%20Assessment%20of%20Open-Source%20Large%20Language%20Models%0AAuthor%3A%20Anindya%20Bijoy%20Das%20and%20Shibbir%20Ahmed%20and%20Shahnewaz%20Karim%20Sakib%0AAbstract%3A%20%20%20Clinical%20summarization%20is%20crucial%20in%20healthcare%20as%20it%20distills%20complex%0Amedical%20data%20into%20digestible%20information%2C%20enhancing%20patient%20understanding%20and%0Acare%20management.%20Large%20language%20models%20%28LLMs%29%20have%20shown%20significant%20potential%0Ain%20automating%20and%20improving%20the%20accuracy%20of%20such%20summarizations%20due%20to%20their%0Aadvanced%20natural%20language%20understanding%20capabilities.%20These%20models%20are%0Aparticularly%20applicable%20in%20the%20context%20of%20summarizing%20medical/clinical%20texts%2C%0Awhere%20precise%20and%20concise%20information%20transfer%20is%20essential.%20In%20this%20paper%2C%20we%0Ainvestigate%20the%20effectiveness%20of%20open-source%20LLMs%20in%20extracting%20key%20events%20from%0Adischarge%20reports%2C%20including%20admission%20reasons%2C%20major%20in-hospital%20events%2C%20and%0Acritical%20follow-up%20actions.%20In%20addition%2C%20we%20also%20assess%20the%20prevalence%20of%0Avarious%20types%20of%20hallucinations%20in%20the%20summaries%20produced%20by%20these%20models.%0ADetecting%20hallucinations%20is%20vital%20as%20it%20directly%20influences%20the%20reliability%20of%0Athe%20information%2C%20potentially%20affecting%20patient%20care%20and%20treatment%20outcomes.%20We%0Aconduct%20comprehensive%20simulations%20to%20rigorously%20evaluate%20the%20performance%20of%0Athese%20models%2C%20further%20probing%20the%20accuracy%20and%20fidelity%20of%20the%20extracted%0Acontent%20in%20clinical%20summarization.%20Our%20results%20reveal%20that%20while%20the%20LLMs%0A%28e.g.%2C%20Qwen2.5%20and%20DeepSeek-v2%29%20perform%20quite%20well%20in%20capturing%20admission%0Areasons%20and%20hospitalization%20events%2C%20they%20are%20generally%20less%20consistent%20when%20it%0Acomes%20to%20identifying%20follow-up%20recommendations%2C%20highlighting%20broader%20challenges%0Ain%20leveraging%20LLMs%20for%20comprehensive%20summarization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19061v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHallucinations%2520and%2520Key%2520Information%2520Extraction%2520in%2520Medical%2520Texts%253A%2520A%250A%2520%2520Comprehensive%2520Assessment%2520of%2520Open-Source%2520Large%2520Language%2520Models%26entry.906535625%3DAnindya%2520Bijoy%2520Das%2520and%2520Shibbir%2520Ahmed%2520and%2520Shahnewaz%2520Karim%2520Sakib%26entry.1292438233%3D%2520%2520Clinical%2520summarization%2520is%2520crucial%2520in%2520healthcare%2520as%2520it%2520distills%2520complex%250Amedical%2520data%2520into%2520digestible%2520information%252C%2520enhancing%2520patient%2520understanding%2520and%250Acare%2520management.%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520significant%2520potential%250Ain%2520automating%2520and%2520improving%2520the%2520accuracy%2520of%2520such%2520summarizations%2520due%2520to%2520their%250Aadvanced%2520natural%2520language%2520understanding%2520capabilities.%2520These%2520models%2520are%250Aparticularly%2520applicable%2520in%2520the%2520context%2520of%2520summarizing%2520medical/clinical%2520texts%252C%250Awhere%2520precise%2520and%2520concise%2520information%2520transfer%2520is%2520essential.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520the%2520effectiveness%2520of%2520open-source%2520LLMs%2520in%2520extracting%2520key%2520events%2520from%250Adischarge%2520reports%252C%2520including%2520admission%2520reasons%252C%2520major%2520in-hospital%2520events%252C%2520and%250Acritical%2520follow-up%2520actions.%2520In%2520addition%252C%2520we%2520also%2520assess%2520the%2520prevalence%2520of%250Avarious%2520types%2520of%2520hallucinations%2520in%2520the%2520summaries%2520produced%2520by%2520these%2520models.%250ADetecting%2520hallucinations%2520is%2520vital%2520as%2520it%2520directly%2520influences%2520the%2520reliability%2520of%250Athe%2520information%252C%2520potentially%2520affecting%2520patient%2520care%2520and%2520treatment%2520outcomes.%2520We%250Aconduct%2520comprehensive%2520simulations%2520to%2520rigorously%2520evaluate%2520the%2520performance%2520of%250Athese%2520models%252C%2520further%2520probing%2520the%2520accuracy%2520and%2520fidelity%2520of%2520the%2520extracted%250Acontent%2520in%2520clinical%2520summarization.%2520Our%2520results%2520reveal%2520that%2520while%2520the%2520LLMs%250A%2528e.g.%252C%2520Qwen2.5%2520and%2520DeepSeek-v2%2529%2520perform%2520quite%2520well%2520in%2520capturing%2520admission%250Areasons%2520and%2520hospitalization%2520events%252C%2520they%2520are%2520generally%2520less%2520consistent%2520when%2520it%250Acomes%2520to%2520identifying%2520follow-up%2520recommendations%252C%2520highlighting%2520broader%2520challenges%250Ain%2520leveraging%2520LLMs%2520for%2520comprehensive%2520summarization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19061v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hallucinations%20and%20Key%20Information%20Extraction%20in%20Medical%20Texts%3A%20A%0A%20%20Comprehensive%20Assessment%20of%20Open-Source%20Large%20Language%20Models&entry.906535625=Anindya%20Bijoy%20Das%20and%20Shibbir%20Ahmed%20and%20Shahnewaz%20Karim%20Sakib&entry.1292438233=%20%20Clinical%20summarization%20is%20crucial%20in%20healthcare%20as%20it%20distills%20complex%0Amedical%20data%20into%20digestible%20information%2C%20enhancing%20patient%20understanding%20and%0Acare%20management.%20Large%20language%20models%20%28LLMs%29%20have%20shown%20significant%20potential%0Ain%20automating%20and%20improving%20the%20accuracy%20of%20such%20summarizations%20due%20to%20their%0Aadvanced%20natural%20language%20understanding%20capabilities.%20These%20models%20are%0Aparticularly%20applicable%20in%20the%20context%20of%20summarizing%20medical/clinical%20texts%2C%0Awhere%20precise%20and%20concise%20information%20transfer%20is%20essential.%20In%20this%20paper%2C%20we%0Ainvestigate%20the%20effectiveness%20of%20open-source%20LLMs%20in%20extracting%20key%20events%20from%0Adischarge%20reports%2C%20including%20admission%20reasons%2C%20major%20in-hospital%20events%2C%20and%0Acritical%20follow-up%20actions.%20In%20addition%2C%20we%20also%20assess%20the%20prevalence%20of%0Avarious%20types%20of%20hallucinations%20in%20the%20summaries%20produced%20by%20these%20models.%0ADetecting%20hallucinations%20is%20vital%20as%20it%20directly%20influences%20the%20reliability%20of%0Athe%20information%2C%20potentially%20affecting%20patient%20care%20and%20treatment%20outcomes.%20We%0Aconduct%20comprehensive%20simulations%20to%20rigorously%20evaluate%20the%20performance%20of%0Athese%20models%2C%20further%20probing%20the%20accuracy%20and%20fidelity%20of%20the%20extracted%0Acontent%20in%20clinical%20summarization.%20Our%20results%20reveal%20that%20while%20the%20LLMs%0A%28e.g.%2C%20Qwen2.5%20and%20DeepSeek-v2%29%20perform%20quite%20well%20in%20capturing%20admission%0Areasons%20and%20hospitalization%20events%2C%20they%20are%20generally%20less%20consistent%20when%20it%0Acomes%20to%20identifying%20follow-up%20recommendations%2C%20highlighting%20broader%20challenges%0Ain%20leveraging%20LLMs%20for%20comprehensive%20summarization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19061v2&entry.124074799=Read"},
{"title": "ViT-FIQA: Assessing Face Image Quality using Vision Transformers", "author": "Andrea Atzori and Fadi Boutros and Naser Damer", "abstract": "  Face Image Quality Assessment (FIQA) aims to predict the utility of a face\nimage for face recognition (FR) systems. State-of-the-art FIQA methods mainly\nrely on convolutional neural networks (CNNs), leaving the potential of Vision\nTransformer (ViT) architectures underexplored. This work proposes ViT-FIQA, a\nnovel approach that extends standard ViT backbones, originally optimized for\nFR, through a learnable quality token designed to predict a scalar utility\nscore for any given face image. The learnable quality token is concatenated\nwith the standard image patch tokens, and the whole sequence is processed via\nglobal self-attention by the ViT encoders to aggregate contextual information\nacross all patches. At the output of the backbone, ViT-FIQA branches into two\nheads: (1) the patch tokens are passed through a fully connected layer to learn\ndiscriminative face representations via a margin-penalty softmax loss, and (2)\nthe quality token is fed into a regression head to learn to predict the face\nsample's utility. Extensive experiments on challenging benchmarks and several\nFR models, including both CNN- and ViT-based architectures, demonstrate that\nViT-FIQA consistently achieves top-tier performance. These results underscore\nthe effectiveness of transformer-based architectures in modeling face image\nutility and highlight the potential of ViTs as a scalable foundation for future\nFIQA research https://cutt.ly/irHlzXUC.\n", "link": "http://arxiv.org/abs/2508.13957v1", "date": "2025-08-19", "relevancy": 2.1035, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5392}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5284}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViT-FIQA%3A%20Assessing%20Face%20Image%20Quality%20using%20Vision%20Transformers&body=Title%3A%20ViT-FIQA%3A%20Assessing%20Face%20Image%20Quality%20using%20Vision%20Transformers%0AAuthor%3A%20Andrea%20Atzori%20and%20Fadi%20Boutros%20and%20Naser%20Damer%0AAbstract%3A%20%20%20Face%20Image%20Quality%20Assessment%20%28FIQA%29%20aims%20to%20predict%20the%20utility%20of%20a%20face%0Aimage%20for%20face%20recognition%20%28FR%29%20systems.%20State-of-the-art%20FIQA%20methods%20mainly%0Arely%20on%20convolutional%20neural%20networks%20%28CNNs%29%2C%20leaving%20the%20potential%20of%20Vision%0ATransformer%20%28ViT%29%20architectures%20underexplored.%20This%20work%20proposes%20ViT-FIQA%2C%20a%0Anovel%20approach%20that%20extends%20standard%20ViT%20backbones%2C%20originally%20optimized%20for%0AFR%2C%20through%20a%20learnable%20quality%20token%20designed%20to%20predict%20a%20scalar%20utility%0Ascore%20for%20any%20given%20face%20image.%20The%20learnable%20quality%20token%20is%20concatenated%0Awith%20the%20standard%20image%20patch%20tokens%2C%20and%20the%20whole%20sequence%20is%20processed%20via%0Aglobal%20self-attention%20by%20the%20ViT%20encoders%20to%20aggregate%20contextual%20information%0Aacross%20all%20patches.%20At%20the%20output%20of%20the%20backbone%2C%20ViT-FIQA%20branches%20into%20two%0Aheads%3A%20%281%29%20the%20patch%20tokens%20are%20passed%20through%20a%20fully%20connected%20layer%20to%20learn%0Adiscriminative%20face%20representations%20via%20a%20margin-penalty%20softmax%20loss%2C%20and%20%282%29%0Athe%20quality%20token%20is%20fed%20into%20a%20regression%20head%20to%20learn%20to%20predict%20the%20face%0Asample%27s%20utility.%20Extensive%20experiments%20on%20challenging%20benchmarks%20and%20several%0AFR%20models%2C%20including%20both%20CNN-%20and%20ViT-based%20architectures%2C%20demonstrate%20that%0AViT-FIQA%20consistently%20achieves%20top-tier%20performance.%20These%20results%20underscore%0Athe%20effectiveness%20of%20transformer-based%20architectures%20in%20modeling%20face%20image%0Autility%20and%20highlight%20the%20potential%20of%20ViTs%20as%20a%20scalable%20foundation%20for%20future%0AFIQA%20research%20https%3A//cutt.ly/irHlzXUC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViT-FIQA%253A%2520Assessing%2520Face%2520Image%2520Quality%2520using%2520Vision%2520Transformers%26entry.906535625%3DAndrea%2520Atzori%2520and%2520Fadi%2520Boutros%2520and%2520Naser%2520Damer%26entry.1292438233%3D%2520%2520Face%2520Image%2520Quality%2520Assessment%2520%2528FIQA%2529%2520aims%2520to%2520predict%2520the%2520utility%2520of%2520a%2520face%250Aimage%2520for%2520face%2520recognition%2520%2528FR%2529%2520systems.%2520State-of-the-art%2520FIQA%2520methods%2520mainly%250Arely%2520on%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%2520leaving%2520the%2520potential%2520of%2520Vision%250ATransformer%2520%2528ViT%2529%2520architectures%2520underexplored.%2520This%2520work%2520proposes%2520ViT-FIQA%252C%2520a%250Anovel%2520approach%2520that%2520extends%2520standard%2520ViT%2520backbones%252C%2520originally%2520optimized%2520for%250AFR%252C%2520through%2520a%2520learnable%2520quality%2520token%2520designed%2520to%2520predict%2520a%2520scalar%2520utility%250Ascore%2520for%2520any%2520given%2520face%2520image.%2520The%2520learnable%2520quality%2520token%2520is%2520concatenated%250Awith%2520the%2520standard%2520image%2520patch%2520tokens%252C%2520and%2520the%2520whole%2520sequence%2520is%2520processed%2520via%250Aglobal%2520self-attention%2520by%2520the%2520ViT%2520encoders%2520to%2520aggregate%2520contextual%2520information%250Aacross%2520all%2520patches.%2520At%2520the%2520output%2520of%2520the%2520backbone%252C%2520ViT-FIQA%2520branches%2520into%2520two%250Aheads%253A%2520%25281%2529%2520the%2520patch%2520tokens%2520are%2520passed%2520through%2520a%2520fully%2520connected%2520layer%2520to%2520learn%250Adiscriminative%2520face%2520representations%2520via%2520a%2520margin-penalty%2520softmax%2520loss%252C%2520and%2520%25282%2529%250Athe%2520quality%2520token%2520is%2520fed%2520into%2520a%2520regression%2520head%2520to%2520learn%2520to%2520predict%2520the%2520face%250Asample%2527s%2520utility.%2520Extensive%2520experiments%2520on%2520challenging%2520benchmarks%2520and%2520several%250AFR%2520models%252C%2520including%2520both%2520CNN-%2520and%2520ViT-based%2520architectures%252C%2520demonstrate%2520that%250AViT-FIQA%2520consistently%2520achieves%2520top-tier%2520performance.%2520These%2520results%2520underscore%250Athe%2520effectiveness%2520of%2520transformer-based%2520architectures%2520in%2520modeling%2520face%2520image%250Autility%2520and%2520highlight%2520the%2520potential%2520of%2520ViTs%2520as%2520a%2520scalable%2520foundation%2520for%2520future%250AFIQA%2520research%2520https%253A//cutt.ly/irHlzXUC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViT-FIQA%3A%20Assessing%20Face%20Image%20Quality%20using%20Vision%20Transformers&entry.906535625=Andrea%20Atzori%20and%20Fadi%20Boutros%20and%20Naser%20Damer&entry.1292438233=%20%20Face%20Image%20Quality%20Assessment%20%28FIQA%29%20aims%20to%20predict%20the%20utility%20of%20a%20face%0Aimage%20for%20face%20recognition%20%28FR%29%20systems.%20State-of-the-art%20FIQA%20methods%20mainly%0Arely%20on%20convolutional%20neural%20networks%20%28CNNs%29%2C%20leaving%20the%20potential%20of%20Vision%0ATransformer%20%28ViT%29%20architectures%20underexplored.%20This%20work%20proposes%20ViT-FIQA%2C%20a%0Anovel%20approach%20that%20extends%20standard%20ViT%20backbones%2C%20originally%20optimized%20for%0AFR%2C%20through%20a%20learnable%20quality%20token%20designed%20to%20predict%20a%20scalar%20utility%0Ascore%20for%20any%20given%20face%20image.%20The%20learnable%20quality%20token%20is%20concatenated%0Awith%20the%20standard%20image%20patch%20tokens%2C%20and%20the%20whole%20sequence%20is%20processed%20via%0Aglobal%20self-attention%20by%20the%20ViT%20encoders%20to%20aggregate%20contextual%20information%0Aacross%20all%20patches.%20At%20the%20output%20of%20the%20backbone%2C%20ViT-FIQA%20branches%20into%20two%0Aheads%3A%20%281%29%20the%20patch%20tokens%20are%20passed%20through%20a%20fully%20connected%20layer%20to%20learn%0Adiscriminative%20face%20representations%20via%20a%20margin-penalty%20softmax%20loss%2C%20and%20%282%29%0Athe%20quality%20token%20is%20fed%20into%20a%20regression%20head%20to%20learn%20to%20predict%20the%20face%0Asample%27s%20utility.%20Extensive%20experiments%20on%20challenging%20benchmarks%20and%20several%0AFR%20models%2C%20including%20both%20CNN-%20and%20ViT-based%20architectures%2C%20demonstrate%20that%0AViT-FIQA%20consistently%20achieves%20top-tier%20performance.%20These%20results%20underscore%0Athe%20effectiveness%20of%20transformer-based%20architectures%20in%20modeling%20face%20image%0Autility%20and%20highlight%20the%20potential%20of%20ViTs%20as%20a%20scalable%20foundation%20for%20future%0AFIQA%20research%20https%3A//cutt.ly/irHlzXUC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13957v1&entry.124074799=Read"},
{"title": "Diversity-enhanced Collaborative Mamba for Semi-supervised Medical Image\n  Segmentation", "author": "Shumeng Li and Jian Zhang and Lei Qi and Luping Zhou and Yinghuan Shi and Yang Gao", "abstract": "  Acquiring high-quality annotated data for medical image segmentation is\ntedious and costly. Semi-supervised segmentation techniques alleviate this\nburden by leveraging unlabeled data to generate pseudo labels. Recently,\nadvanced state space models, represented by Mamba, have shown efficient\nhandling of long-range dependencies. This drives us to explore their potential\nin semi-supervised medical image segmentation. In this paper, we propose a\nnovel Diversity-enhanced Collaborative Mamba framework (namely DCMamba) for\nsemi-supervised medical image segmentation, which explores and utilizes the\ndiversity from data, network, and feature perspectives. Firstly, from the data\nperspective, we develop patch-level weak-strong mixing augmentation with\nMamba's scanning modeling characteristics. Moreover, from the network\nperspective, we introduce a diverse-scan collaboration module, which could\nbenefit from the prediction discrepancies arising from different scanning\ndirections. Furthermore, from the feature perspective, we adopt an\nuncertainty-weighted contrastive learning mechanism to enhance the diversity of\nfeature representation. Experiments demonstrate that our DCMamba significantly\noutperforms other semi-supervised medical image segmentation methods, e.g.,\nyielding the latest SSM-based method by 6.69% on the Synapse dataset with 20%\nlabeled data.\n", "link": "http://arxiv.org/abs/2508.13712v1", "date": "2025-08-19", "relevancy": 2.1017, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5523}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5256}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diversity-enhanced%20Collaborative%20Mamba%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20Diversity-enhanced%20Collaborative%20Mamba%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Shumeng%20Li%20and%20Jian%20Zhang%20and%20Lei%20Qi%20and%20Luping%20Zhou%20and%20Yinghuan%20Shi%20and%20Yang%20Gao%0AAbstract%3A%20%20%20Acquiring%20high-quality%20annotated%20data%20for%20medical%20image%20segmentation%20is%0Atedious%20and%20costly.%20Semi-supervised%20segmentation%20techniques%20alleviate%20this%0Aburden%20by%20leveraging%20unlabeled%20data%20to%20generate%20pseudo%20labels.%20Recently%2C%0Aadvanced%20state%20space%20models%2C%20represented%20by%20Mamba%2C%20have%20shown%20efficient%0Ahandling%20of%20long-range%20dependencies.%20This%20drives%20us%20to%20explore%20their%20potential%0Ain%20semi-supervised%20medical%20image%20segmentation.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20Diversity-enhanced%20Collaborative%20Mamba%20framework%20%28namely%20DCMamba%29%20for%0Asemi-supervised%20medical%20image%20segmentation%2C%20which%20explores%20and%20utilizes%20the%0Adiversity%20from%20data%2C%20network%2C%20and%20feature%20perspectives.%20Firstly%2C%20from%20the%20data%0Aperspective%2C%20we%20develop%20patch-level%20weak-strong%20mixing%20augmentation%20with%0AMamba%27s%20scanning%20modeling%20characteristics.%20Moreover%2C%20from%20the%20network%0Aperspective%2C%20we%20introduce%20a%20diverse-scan%20collaboration%20module%2C%20which%20could%0Abenefit%20from%20the%20prediction%20discrepancies%20arising%20from%20different%20scanning%0Adirections.%20Furthermore%2C%20from%20the%20feature%20perspective%2C%20we%20adopt%20an%0Auncertainty-weighted%20contrastive%20learning%20mechanism%20to%20enhance%20the%20diversity%20of%0Afeature%20representation.%20Experiments%20demonstrate%20that%20our%20DCMamba%20significantly%0Aoutperforms%20other%20semi-supervised%20medical%20image%20segmentation%20methods%2C%20e.g.%2C%0Ayielding%20the%20latest%20SSM-based%20method%20by%206.69%25%20on%20the%20Synapse%20dataset%20with%2020%25%0Alabeled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiversity-enhanced%2520Collaborative%2520Mamba%2520for%2520Semi-supervised%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DShumeng%2520Li%2520and%2520Jian%2520Zhang%2520and%2520Lei%2520Qi%2520and%2520Luping%2520Zhou%2520and%2520Yinghuan%2520Shi%2520and%2520Yang%2520Gao%26entry.1292438233%3D%2520%2520Acquiring%2520high-quality%2520annotated%2520data%2520for%2520medical%2520image%2520segmentation%2520is%250Atedious%2520and%2520costly.%2520Semi-supervised%2520segmentation%2520techniques%2520alleviate%2520this%250Aburden%2520by%2520leveraging%2520unlabeled%2520data%2520to%2520generate%2520pseudo%2520labels.%2520Recently%252C%250Aadvanced%2520state%2520space%2520models%252C%2520represented%2520by%2520Mamba%252C%2520have%2520shown%2520efficient%250Ahandling%2520of%2520long-range%2520dependencies.%2520This%2520drives%2520us%2520to%2520explore%2520their%2520potential%250Ain%2520semi-supervised%2520medical%2520image%2520segmentation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520Diversity-enhanced%2520Collaborative%2520Mamba%2520framework%2520%2528namely%2520DCMamba%2529%2520for%250Asemi-supervised%2520medical%2520image%2520segmentation%252C%2520which%2520explores%2520and%2520utilizes%2520the%250Adiversity%2520from%2520data%252C%2520network%252C%2520and%2520feature%2520perspectives.%2520Firstly%252C%2520from%2520the%2520data%250Aperspective%252C%2520we%2520develop%2520patch-level%2520weak-strong%2520mixing%2520augmentation%2520with%250AMamba%2527s%2520scanning%2520modeling%2520characteristics.%2520Moreover%252C%2520from%2520the%2520network%250Aperspective%252C%2520we%2520introduce%2520a%2520diverse-scan%2520collaboration%2520module%252C%2520which%2520could%250Abenefit%2520from%2520the%2520prediction%2520discrepancies%2520arising%2520from%2520different%2520scanning%250Adirections.%2520Furthermore%252C%2520from%2520the%2520feature%2520perspective%252C%2520we%2520adopt%2520an%250Auncertainty-weighted%2520contrastive%2520learning%2520mechanism%2520to%2520enhance%2520the%2520diversity%2520of%250Afeature%2520representation.%2520Experiments%2520demonstrate%2520that%2520our%2520DCMamba%2520significantly%250Aoutperforms%2520other%2520semi-supervised%2520medical%2520image%2520segmentation%2520methods%252C%2520e.g.%252C%250Ayielding%2520the%2520latest%2520SSM-based%2520method%2520by%25206.69%2525%2520on%2520the%2520Synapse%2520dataset%2520with%252020%2525%250Alabeled%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diversity-enhanced%20Collaborative%20Mamba%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Shumeng%20Li%20and%20Jian%20Zhang%20and%20Lei%20Qi%20and%20Luping%20Zhou%20and%20Yinghuan%20Shi%20and%20Yang%20Gao&entry.1292438233=%20%20Acquiring%20high-quality%20annotated%20data%20for%20medical%20image%20segmentation%20is%0Atedious%20and%20costly.%20Semi-supervised%20segmentation%20techniques%20alleviate%20this%0Aburden%20by%20leveraging%20unlabeled%20data%20to%20generate%20pseudo%20labels.%20Recently%2C%0Aadvanced%20state%20space%20models%2C%20represented%20by%20Mamba%2C%20have%20shown%20efficient%0Ahandling%20of%20long-range%20dependencies.%20This%20drives%20us%20to%20explore%20their%20potential%0Ain%20semi-supervised%20medical%20image%20segmentation.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20Diversity-enhanced%20Collaborative%20Mamba%20framework%20%28namely%20DCMamba%29%20for%0Asemi-supervised%20medical%20image%20segmentation%2C%20which%20explores%20and%20utilizes%20the%0Adiversity%20from%20data%2C%20network%2C%20and%20feature%20perspectives.%20Firstly%2C%20from%20the%20data%0Aperspective%2C%20we%20develop%20patch-level%20weak-strong%20mixing%20augmentation%20with%0AMamba%27s%20scanning%20modeling%20characteristics.%20Moreover%2C%20from%20the%20network%0Aperspective%2C%20we%20introduce%20a%20diverse-scan%20collaboration%20module%2C%20which%20could%0Abenefit%20from%20the%20prediction%20discrepancies%20arising%20from%20different%20scanning%0Adirections.%20Furthermore%2C%20from%20the%20feature%20perspective%2C%20we%20adopt%20an%0Auncertainty-weighted%20contrastive%20learning%20mechanism%20to%20enhance%20the%20diversity%20of%0Afeature%20representation.%20Experiments%20demonstrate%20that%20our%20DCMamba%20significantly%0Aoutperforms%20other%20semi-supervised%20medical%20image%20segmentation%20methods%2C%20e.g.%2C%0Ayielding%20the%20latest%20SSM-based%20method%20by%206.69%25%20on%20the%20Synapse%20dataset%20with%2020%25%0Alabeled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13712v1&entry.124074799=Read"},
{"title": "In-hoc Concept Representations to Regularise Deep Learning in Medical\n  Imaging", "author": "Valentina Corbetta and Floris Six Dijkstra and Regina Beets-Tan and Hoel Kervadec and Kristoffer Wickstr\u00f8m and Wilson Silva", "abstract": "  Deep learning models in medical imaging often achieve strong in-distribution\nperformance but struggle to generalise under distribution shifts, frequently\nrelying on spurious correlations instead of clinically meaningful features. We\nintroduce LCRReg, a novel regularisation approach that leverages Latent Concept\nRepresentations (LCRs) (e.g., Concept Activation Vectors (CAVs)) to guide\nmodels toward semantically grounded representations. LCRReg requires no concept\nlabels in the main training set and instead uses a small auxiliary dataset to\nsynthesise high-quality, disentangled concept examples. We extract LCRs for\npredefined relevant features, and incorporate a regularisation term that guides\na Convolutional Neural Network (CNN) to activate within latent subspaces\nassociated with those concepts. We evaluate LCRReg across synthetic and\nreal-world medical tasks. On a controlled toy dataset, it significantly\nimproves robustness to injected spurious correlations and remains effective\neven in multi-concept and multiclass settings. On the diabetic retinopathy\nbinary classification task, LCRReg enhances performance under both synthetic\nspurious perturbations and out-of-distribution (OOD) generalisation. Compared\nto baselines, including multitask learning, linear probing, and post-hoc\nconcept-based models, LCRReg offers a lightweight, architecture-agnostic\nstrategy for improving model robustness without requiring dense concept\nsupervision. Code is available at the following link:\nhttps://github.com/Trustworthy-AI-UU-NKI/lcr\\_regularization\n", "link": "http://arxiv.org/abs/2508.13880v1", "date": "2025-08-19", "relevancy": 2.0948, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.531}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5198}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-hoc%20Concept%20Representations%20to%20Regularise%20Deep%20Learning%20in%20Medical%0A%20%20Imaging&body=Title%3A%20In-hoc%20Concept%20Representations%20to%20Regularise%20Deep%20Learning%20in%20Medical%0A%20%20Imaging%0AAuthor%3A%20Valentina%20Corbetta%20and%20Floris%20Six%20Dijkstra%20and%20Regina%20Beets-Tan%20and%20Hoel%20Kervadec%20and%20Kristoffer%20Wickstr%C3%B8m%20and%20Wilson%20Silva%0AAbstract%3A%20%20%20Deep%20learning%20models%20in%20medical%20imaging%20often%20achieve%20strong%20in-distribution%0Aperformance%20but%20struggle%20to%20generalise%20under%20distribution%20shifts%2C%20frequently%0Arelying%20on%20spurious%20correlations%20instead%20of%20clinically%20meaningful%20features.%20We%0Aintroduce%20LCRReg%2C%20a%20novel%20regularisation%20approach%20that%20leverages%20Latent%20Concept%0ARepresentations%20%28LCRs%29%20%28e.g.%2C%20Concept%20Activation%20Vectors%20%28CAVs%29%29%20to%20guide%0Amodels%20toward%20semantically%20grounded%20representations.%20LCRReg%20requires%20no%20concept%0Alabels%20in%20the%20main%20training%20set%20and%20instead%20uses%20a%20small%20auxiliary%20dataset%20to%0Asynthesise%20high-quality%2C%20disentangled%20concept%20examples.%20We%20extract%20LCRs%20for%0Apredefined%20relevant%20features%2C%20and%20incorporate%20a%20regularisation%20term%20that%20guides%0Aa%20Convolutional%20Neural%20Network%20%28CNN%29%20to%20activate%20within%20latent%20subspaces%0Aassociated%20with%20those%20concepts.%20We%20evaluate%20LCRReg%20across%20synthetic%20and%0Areal-world%20medical%20tasks.%20On%20a%20controlled%20toy%20dataset%2C%20it%20significantly%0Aimproves%20robustness%20to%20injected%20spurious%20correlations%20and%20remains%20effective%0Aeven%20in%20multi-concept%20and%20multiclass%20settings.%20On%20the%20diabetic%20retinopathy%0Abinary%20classification%20task%2C%20LCRReg%20enhances%20performance%20under%20both%20synthetic%0Aspurious%20perturbations%20and%20out-of-distribution%20%28OOD%29%20generalisation.%20Compared%0Ato%20baselines%2C%20including%20multitask%20learning%2C%20linear%20probing%2C%20and%20post-hoc%0Aconcept-based%20models%2C%20LCRReg%20offers%20a%20lightweight%2C%20architecture-agnostic%0Astrategy%20for%20improving%20model%20robustness%20without%20requiring%20dense%20concept%0Asupervision.%20Code%20is%20available%20at%20the%20following%20link%3A%0Ahttps%3A//github.com/Trustworthy-AI-UU-NKI/lcr%5C_regularization%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-hoc%2520Concept%2520Representations%2520to%2520Regularise%2520Deep%2520Learning%2520in%2520Medical%250A%2520%2520Imaging%26entry.906535625%3DValentina%2520Corbetta%2520and%2520Floris%2520Six%2520Dijkstra%2520and%2520Regina%2520Beets-Tan%2520and%2520Hoel%2520Kervadec%2520and%2520Kristoffer%2520Wickstr%25C3%25B8m%2520and%2520Wilson%2520Silva%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520in%2520medical%2520imaging%2520often%2520achieve%2520strong%2520in-distribution%250Aperformance%2520but%2520struggle%2520to%2520generalise%2520under%2520distribution%2520shifts%252C%2520frequently%250Arelying%2520on%2520spurious%2520correlations%2520instead%2520of%2520clinically%2520meaningful%2520features.%2520We%250Aintroduce%2520LCRReg%252C%2520a%2520novel%2520regularisation%2520approach%2520that%2520leverages%2520Latent%2520Concept%250ARepresentations%2520%2528LCRs%2529%2520%2528e.g.%252C%2520Concept%2520Activation%2520Vectors%2520%2528CAVs%2529%2529%2520to%2520guide%250Amodels%2520toward%2520semantically%2520grounded%2520representations.%2520LCRReg%2520requires%2520no%2520concept%250Alabels%2520in%2520the%2520main%2520training%2520set%2520and%2520instead%2520uses%2520a%2520small%2520auxiliary%2520dataset%2520to%250Asynthesise%2520high-quality%252C%2520disentangled%2520concept%2520examples.%2520We%2520extract%2520LCRs%2520for%250Apredefined%2520relevant%2520features%252C%2520and%2520incorporate%2520a%2520regularisation%2520term%2520that%2520guides%250Aa%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%2520to%2520activate%2520within%2520latent%2520subspaces%250Aassociated%2520with%2520those%2520concepts.%2520We%2520evaluate%2520LCRReg%2520across%2520synthetic%2520and%250Areal-world%2520medical%2520tasks.%2520On%2520a%2520controlled%2520toy%2520dataset%252C%2520it%2520significantly%250Aimproves%2520robustness%2520to%2520injected%2520spurious%2520correlations%2520and%2520remains%2520effective%250Aeven%2520in%2520multi-concept%2520and%2520multiclass%2520settings.%2520On%2520the%2520diabetic%2520retinopathy%250Abinary%2520classification%2520task%252C%2520LCRReg%2520enhances%2520performance%2520under%2520both%2520synthetic%250Aspurious%2520perturbations%2520and%2520out-of-distribution%2520%2528OOD%2529%2520generalisation.%2520Compared%250Ato%2520baselines%252C%2520including%2520multitask%2520learning%252C%2520linear%2520probing%252C%2520and%2520post-hoc%250Aconcept-based%2520models%252C%2520LCRReg%2520offers%2520a%2520lightweight%252C%2520architecture-agnostic%250Astrategy%2520for%2520improving%2520model%2520robustness%2520without%2520requiring%2520dense%2520concept%250Asupervision.%2520Code%2520is%2520available%2520at%2520the%2520following%2520link%253A%250Ahttps%253A//github.com/Trustworthy-AI-UU-NKI/lcr%255C_regularization%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-hoc%20Concept%20Representations%20to%20Regularise%20Deep%20Learning%20in%20Medical%0A%20%20Imaging&entry.906535625=Valentina%20Corbetta%20and%20Floris%20Six%20Dijkstra%20and%20Regina%20Beets-Tan%20and%20Hoel%20Kervadec%20and%20Kristoffer%20Wickstr%C3%B8m%20and%20Wilson%20Silva&entry.1292438233=%20%20Deep%20learning%20models%20in%20medical%20imaging%20often%20achieve%20strong%20in-distribution%0Aperformance%20but%20struggle%20to%20generalise%20under%20distribution%20shifts%2C%20frequently%0Arelying%20on%20spurious%20correlations%20instead%20of%20clinically%20meaningful%20features.%20We%0Aintroduce%20LCRReg%2C%20a%20novel%20regularisation%20approach%20that%20leverages%20Latent%20Concept%0ARepresentations%20%28LCRs%29%20%28e.g.%2C%20Concept%20Activation%20Vectors%20%28CAVs%29%29%20to%20guide%0Amodels%20toward%20semantically%20grounded%20representations.%20LCRReg%20requires%20no%20concept%0Alabels%20in%20the%20main%20training%20set%20and%20instead%20uses%20a%20small%20auxiliary%20dataset%20to%0Asynthesise%20high-quality%2C%20disentangled%20concept%20examples.%20We%20extract%20LCRs%20for%0Apredefined%20relevant%20features%2C%20and%20incorporate%20a%20regularisation%20term%20that%20guides%0Aa%20Convolutional%20Neural%20Network%20%28CNN%29%20to%20activate%20within%20latent%20subspaces%0Aassociated%20with%20those%20concepts.%20We%20evaluate%20LCRReg%20across%20synthetic%20and%0Areal-world%20medical%20tasks.%20On%20a%20controlled%20toy%20dataset%2C%20it%20significantly%0Aimproves%20robustness%20to%20injected%20spurious%20correlations%20and%20remains%20effective%0Aeven%20in%20multi-concept%20and%20multiclass%20settings.%20On%20the%20diabetic%20retinopathy%0Abinary%20classification%20task%2C%20LCRReg%20enhances%20performance%20under%20both%20synthetic%0Aspurious%20perturbations%20and%20out-of-distribution%20%28OOD%29%20generalisation.%20Compared%0Ato%20baselines%2C%20including%20multitask%20learning%2C%20linear%20probing%2C%20and%20post-hoc%0Aconcept-based%20models%2C%20LCRReg%20offers%20a%20lightweight%2C%20architecture-agnostic%0Astrategy%20for%20improving%20model%20robustness%20without%20requiring%20dense%20concept%0Asupervision.%20Code%20is%20available%20at%20the%20following%20link%3A%0Ahttps%3A//github.com/Trustworthy-AI-UU-NKI/lcr%5C_regularization%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13880v1&entry.124074799=Read"},
{"title": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing\n  through Cluster Retrieval and Execution Description", "author": "Shaoming Duan and Zirui Wang and Chuanyi Liu and Zhibin Zhu and Yuhao Zhang and Peiyi Han and Liang Yan and Zewu Penge", "abstract": "  Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git\n", "link": "http://arxiv.org/abs/2508.12769v2", "date": "2025-08-19", "relevancy": 2.0946, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CRED-SQL%3A%20Enhancing%20Real-world%20Large%20Scale%20Database%20Text-to-SQL%20Parsing%0A%20%20through%20Cluster%20Retrieval%20and%20Execution%20Description&body=Title%3A%20CRED-SQL%3A%20Enhancing%20Real-world%20Large%20Scale%20Database%20Text-to-SQL%20Parsing%0A%20%20through%20Cluster%20Retrieval%20and%20Execution%20Description%0AAuthor%3A%20Shaoming%20Duan%20and%20Zirui%20Wang%20and%20Chuanyi%20Liu%20and%20Zhibin%20Zhu%20and%20Yuhao%20Zhang%20and%20Peiyi%20Han%20and%20Liang%20Yan%20and%20Zewu%20Penge%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20significantly%20improved%0Athe%20accuracy%20of%20Text-to-SQL%20systems.%20However%2C%20a%20critical%20challenge%20remains%3A%20the%0Asemantic%20mismatch%20between%20natural%20language%20questions%20%28NLQs%29%20and%20their%0Acorresponding%20SQL%20queries.%20This%20issue%20is%20exacerbated%20in%20large-scale%20databases%2C%0Awhere%20semantically%20similar%20attributes%20hinder%20schema%20linking%20and%20semantic%20drift%0Aduring%20SQL%20generation%2C%20ultimately%20reducing%20model%20accuracy.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20CRED-SQL%2C%20a%20framework%20designed%20for%20large-scale%0Adatabases%20that%20integrates%20Cluster%20Retrieval%20and%20Execution%20Description.%20CRED-SQL%0Afirst%20performs%20cluster-based%20large-scale%20schema%20retrieval%20to%20pinpoint%20the%0Atables%20and%20columns%20most%20relevant%20to%20a%20given%20NLQ%2C%20alleviating%20schema%20mismatch.%0AIt%20then%20introduces%20an%20intermediate%20natural%20language%20representation-Execution%0ADescription%20Language%20%28EDL%29-to%20bridge%20the%20gap%20between%20NLQs%20and%20SQL.%20This%0Areformulation%20decomposes%20the%20task%20into%20two%20stages%3A%20Text-to-EDL%20and%20EDL-to-SQL%2C%0Aleveraging%20LLMs%27%20strong%20general%20reasoning%20capabilities%20while%20reducing%20semantic%0Adeviation.%20Extensive%20experiments%20on%20two%20large-scale%2C%20cross-domain%0Abenchmarks-SpiderUnion%20and%20BirdUnion-demonstrate%20that%20CRED-SQL%20achieves%20new%0Astate-of-the-art%20%28SOTA%29%20performance%2C%20validating%20its%20effectiveness%20and%0Ascalability.%20Our%20code%20is%20available%20at%20https%3A//github.com/smduan/CRED-SQL.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12769v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCRED-SQL%253A%2520Enhancing%2520Real-world%2520Large%2520Scale%2520Database%2520Text-to-SQL%2520Parsing%250A%2520%2520through%2520Cluster%2520Retrieval%2520and%2520Execution%2520Description%26entry.906535625%3DShaoming%2520Duan%2520and%2520Zirui%2520Wang%2520and%2520Chuanyi%2520Liu%2520and%2520Zhibin%2520Zhu%2520and%2520Yuhao%2520Zhang%2520and%2520Peiyi%2520Han%2520and%2520Liang%2520Yan%2520and%2520Zewu%2520Penge%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520significantly%2520improved%250Athe%2520accuracy%2520of%2520Text-to-SQL%2520systems.%2520However%252C%2520a%2520critical%2520challenge%2520remains%253A%2520the%250Asemantic%2520mismatch%2520between%2520natural%2520language%2520questions%2520%2528NLQs%2529%2520and%2520their%250Acorresponding%2520SQL%2520queries.%2520This%2520issue%2520is%2520exacerbated%2520in%2520large-scale%2520databases%252C%250Awhere%2520semantically%2520similar%2520attributes%2520hinder%2520schema%2520linking%2520and%2520semantic%2520drift%250Aduring%2520SQL%2520generation%252C%2520ultimately%2520reducing%2520model%2520accuracy.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520CRED-SQL%252C%2520a%2520framework%2520designed%2520for%2520large-scale%250Adatabases%2520that%2520integrates%2520Cluster%2520Retrieval%2520and%2520Execution%2520Description.%2520CRED-SQL%250Afirst%2520performs%2520cluster-based%2520large-scale%2520schema%2520retrieval%2520to%2520pinpoint%2520the%250Atables%2520and%2520columns%2520most%2520relevant%2520to%2520a%2520given%2520NLQ%252C%2520alleviating%2520schema%2520mismatch.%250AIt%2520then%2520introduces%2520an%2520intermediate%2520natural%2520language%2520representation-Execution%250ADescription%2520Language%2520%2528EDL%2529-to%2520bridge%2520the%2520gap%2520between%2520NLQs%2520and%2520SQL.%2520This%250Areformulation%2520decomposes%2520the%2520task%2520into%2520two%2520stages%253A%2520Text-to-EDL%2520and%2520EDL-to-SQL%252C%250Aleveraging%2520LLMs%2527%2520strong%2520general%2520reasoning%2520capabilities%2520while%2520reducing%2520semantic%250Adeviation.%2520Extensive%2520experiments%2520on%2520two%2520large-scale%252C%2520cross-domain%250Abenchmarks-SpiderUnion%2520and%2520BirdUnion-demonstrate%2520that%2520CRED-SQL%2520achieves%2520new%250Astate-of-the-art%2520%2528SOTA%2529%2520performance%252C%2520validating%2520its%2520effectiveness%2520and%250Ascalability.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/smduan/CRED-SQL.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12769v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CRED-SQL%3A%20Enhancing%20Real-world%20Large%20Scale%20Database%20Text-to-SQL%20Parsing%0A%20%20through%20Cluster%20Retrieval%20and%20Execution%20Description&entry.906535625=Shaoming%20Duan%20and%20Zirui%20Wang%20and%20Chuanyi%20Liu%20and%20Zhibin%20Zhu%20and%20Yuhao%20Zhang%20and%20Peiyi%20Han%20and%20Liang%20Yan%20and%20Zewu%20Penge&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20significantly%20improved%0Athe%20accuracy%20of%20Text-to-SQL%20systems.%20However%2C%20a%20critical%20challenge%20remains%3A%20the%0Asemantic%20mismatch%20between%20natural%20language%20questions%20%28NLQs%29%20and%20their%0Acorresponding%20SQL%20queries.%20This%20issue%20is%20exacerbated%20in%20large-scale%20databases%2C%0Awhere%20semantically%20similar%20attributes%20hinder%20schema%20linking%20and%20semantic%20drift%0Aduring%20SQL%20generation%2C%20ultimately%20reducing%20model%20accuracy.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20CRED-SQL%2C%20a%20framework%20designed%20for%20large-scale%0Adatabases%20that%20integrates%20Cluster%20Retrieval%20and%20Execution%20Description.%20CRED-SQL%0Afirst%20performs%20cluster-based%20large-scale%20schema%20retrieval%20to%20pinpoint%20the%0Atables%20and%20columns%20most%20relevant%20to%20a%20given%20NLQ%2C%20alleviating%20schema%20mismatch.%0AIt%20then%20introduces%20an%20intermediate%20natural%20language%20representation-Execution%0ADescription%20Language%20%28EDL%29-to%20bridge%20the%20gap%20between%20NLQs%20and%20SQL.%20This%0Areformulation%20decomposes%20the%20task%20into%20two%20stages%3A%20Text-to-EDL%20and%20EDL-to-SQL%2C%0Aleveraging%20LLMs%27%20strong%20general%20reasoning%20capabilities%20while%20reducing%20semantic%0Adeviation.%20Extensive%20experiments%20on%20two%20large-scale%2C%20cross-domain%0Abenchmarks-SpiderUnion%20and%20BirdUnion-demonstrate%20that%20CRED-SQL%20achieves%20new%0Astate-of-the-art%20%28SOTA%29%20performance%2C%20validating%20its%20effectiveness%20and%0Ascalability.%20Our%20code%20is%20available%20at%20https%3A//github.com/smduan/CRED-SQL.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12769v2&entry.124074799=Read"},
{"title": "MMHMER:Multi-viewer and Multi-task for Handwritten Mathematical\n  Expression Recognition", "author": "Kehua Chen and Haoyang Shen and Lifan Zhong and Mingyi Chen", "abstract": "  Handwritten Mathematical Expression Recognition (HMER) methods have made\nremarkable progress, with most existing HMER approaches based on either a\nhybrid CNN/RNN-based with GRU architecture or Transformer architectures. Each\nof these has its strengths and weaknesses. Leveraging different model\nstructures as viewers and effectively integrating their diverse capabilities\npresents an intriguing avenue for exploration. This involves addressing two key\nchallenges: 1) How to fuse these two methods effectively, and 2) How to achieve\nhigher performance under an appropriate level of complexity. This paper\nproposes an efficient CNN-Transformer multi-viewer, multi-task approach to\nenhance the model's recognition performance. Our MMHMER model achieves 63.96%,\n62.51%, and 65.46% ExpRate on CROHME14, CROHME16, and CROHME19, outperforming\nPosformer with an absolute gain of 1.28%, 1.48%, and 0.58%. The main\ncontribution of our approach is that we propose a new multi-view, multi-task\nframework that can effectively integrate the strengths of CNN and Transformer.\nBy leveraging the feature extraction capabilities of CNN and the sequence\nmodeling capabilities of Transformer, our model can better handle the\ncomplexity of handwritten mathematical expressions.\n", "link": "http://arxiv.org/abs/2502.05557v2", "date": "2025-08-19", "relevancy": 2.0872, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5633}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5133}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMHMER%3AMulti-viewer%20and%20Multi-task%20for%20Handwritten%20Mathematical%0A%20%20Expression%20Recognition&body=Title%3A%20MMHMER%3AMulti-viewer%20and%20Multi-task%20for%20Handwritten%20Mathematical%0A%20%20Expression%20Recognition%0AAuthor%3A%20Kehua%20Chen%20and%20Haoyang%20Shen%20and%20Lifan%20Zhong%20and%20Mingyi%20Chen%0AAbstract%3A%20%20%20Handwritten%20Mathematical%20Expression%20Recognition%20%28HMER%29%20methods%20have%20made%0Aremarkable%20progress%2C%20with%20most%20existing%20HMER%20approaches%20based%20on%20either%20a%0Ahybrid%20CNN/RNN-based%20with%20GRU%20architecture%20or%20Transformer%20architectures.%20Each%0Aof%20these%20has%20its%20strengths%20and%20weaknesses.%20Leveraging%20different%20model%0Astructures%20as%20viewers%20and%20effectively%20integrating%20their%20diverse%20capabilities%0Apresents%20an%20intriguing%20avenue%20for%20exploration.%20This%20involves%20addressing%20two%20key%0Achallenges%3A%201%29%20How%20to%20fuse%20these%20two%20methods%20effectively%2C%20and%202%29%20How%20to%20achieve%0Ahigher%20performance%20under%20an%20appropriate%20level%20of%20complexity.%20This%20paper%0Aproposes%20an%20efficient%20CNN-Transformer%20multi-viewer%2C%20multi-task%20approach%20to%0Aenhance%20the%20model%27s%20recognition%20performance.%20Our%20MMHMER%20model%20achieves%2063.96%25%2C%0A62.51%25%2C%20and%2065.46%25%20ExpRate%20on%20CROHME14%2C%20CROHME16%2C%20and%20CROHME19%2C%20outperforming%0APosformer%20with%20an%20absolute%20gain%20of%201.28%25%2C%201.48%25%2C%20and%200.58%25.%20The%20main%0Acontribution%20of%20our%20approach%20is%20that%20we%20propose%20a%20new%20multi-view%2C%20multi-task%0Aframework%20that%20can%20effectively%20integrate%20the%20strengths%20of%20CNN%20and%20Transformer.%0ABy%20leveraging%20the%20feature%20extraction%20capabilities%20of%20CNN%20and%20the%20sequence%0Amodeling%20capabilities%20of%20Transformer%2C%20our%20model%20can%20better%20handle%20the%0Acomplexity%20of%20handwritten%20mathematical%20expressions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05557v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMHMER%253AMulti-viewer%2520and%2520Multi-task%2520for%2520Handwritten%2520Mathematical%250A%2520%2520Expression%2520Recognition%26entry.906535625%3DKehua%2520Chen%2520and%2520Haoyang%2520Shen%2520and%2520Lifan%2520Zhong%2520and%2520Mingyi%2520Chen%26entry.1292438233%3D%2520%2520Handwritten%2520Mathematical%2520Expression%2520Recognition%2520%2528HMER%2529%2520methods%2520have%2520made%250Aremarkable%2520progress%252C%2520with%2520most%2520existing%2520HMER%2520approaches%2520based%2520on%2520either%2520a%250Ahybrid%2520CNN/RNN-based%2520with%2520GRU%2520architecture%2520or%2520Transformer%2520architectures.%2520Each%250Aof%2520these%2520has%2520its%2520strengths%2520and%2520weaknesses.%2520Leveraging%2520different%2520model%250Astructures%2520as%2520viewers%2520and%2520effectively%2520integrating%2520their%2520diverse%2520capabilities%250Apresents%2520an%2520intriguing%2520avenue%2520for%2520exploration.%2520This%2520involves%2520addressing%2520two%2520key%250Achallenges%253A%25201%2529%2520How%2520to%2520fuse%2520these%2520two%2520methods%2520effectively%252C%2520and%25202%2529%2520How%2520to%2520achieve%250Ahigher%2520performance%2520under%2520an%2520appropriate%2520level%2520of%2520complexity.%2520This%2520paper%250Aproposes%2520an%2520efficient%2520CNN-Transformer%2520multi-viewer%252C%2520multi-task%2520approach%2520to%250Aenhance%2520the%2520model%2527s%2520recognition%2520performance.%2520Our%2520MMHMER%2520model%2520achieves%252063.96%2525%252C%250A62.51%2525%252C%2520and%252065.46%2525%2520ExpRate%2520on%2520CROHME14%252C%2520CROHME16%252C%2520and%2520CROHME19%252C%2520outperforming%250APosformer%2520with%2520an%2520absolute%2520gain%2520of%25201.28%2525%252C%25201.48%2525%252C%2520and%25200.58%2525.%2520The%2520main%250Acontribution%2520of%2520our%2520approach%2520is%2520that%2520we%2520propose%2520a%2520new%2520multi-view%252C%2520multi-task%250Aframework%2520that%2520can%2520effectively%2520integrate%2520the%2520strengths%2520of%2520CNN%2520and%2520Transformer.%250ABy%2520leveraging%2520the%2520feature%2520extraction%2520capabilities%2520of%2520CNN%2520and%2520the%2520sequence%250Amodeling%2520capabilities%2520of%2520Transformer%252C%2520our%2520model%2520can%2520better%2520handle%2520the%250Acomplexity%2520of%2520handwritten%2520mathematical%2520expressions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05557v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMHMER%3AMulti-viewer%20and%20Multi-task%20for%20Handwritten%20Mathematical%0A%20%20Expression%20Recognition&entry.906535625=Kehua%20Chen%20and%20Haoyang%20Shen%20and%20Lifan%20Zhong%20and%20Mingyi%20Chen&entry.1292438233=%20%20Handwritten%20Mathematical%20Expression%20Recognition%20%28HMER%29%20methods%20have%20made%0Aremarkable%20progress%2C%20with%20most%20existing%20HMER%20approaches%20based%20on%20either%20a%0Ahybrid%20CNN/RNN-based%20with%20GRU%20architecture%20or%20Transformer%20architectures.%20Each%0Aof%20these%20has%20its%20strengths%20and%20weaknesses.%20Leveraging%20different%20model%0Astructures%20as%20viewers%20and%20effectively%20integrating%20their%20diverse%20capabilities%0Apresents%20an%20intriguing%20avenue%20for%20exploration.%20This%20involves%20addressing%20two%20key%0Achallenges%3A%201%29%20How%20to%20fuse%20these%20two%20methods%20effectively%2C%20and%202%29%20How%20to%20achieve%0Ahigher%20performance%20under%20an%20appropriate%20level%20of%20complexity.%20This%20paper%0Aproposes%20an%20efficient%20CNN-Transformer%20multi-viewer%2C%20multi-task%20approach%20to%0Aenhance%20the%20model%27s%20recognition%20performance.%20Our%20MMHMER%20model%20achieves%2063.96%25%2C%0A62.51%25%2C%20and%2065.46%25%20ExpRate%20on%20CROHME14%2C%20CROHME16%2C%20and%20CROHME19%2C%20outperforming%0APosformer%20with%20an%20absolute%20gain%20of%201.28%25%2C%201.48%25%2C%20and%200.58%25.%20The%20main%0Acontribution%20of%20our%20approach%20is%20that%20we%20propose%20a%20new%20multi-view%2C%20multi-task%0Aframework%20that%20can%20effectively%20integrate%20the%20strengths%20of%20CNN%20and%20Transformer.%0ABy%20leveraging%20the%20feature%20extraction%20capabilities%20of%20CNN%20and%20the%20sequence%0Amodeling%20capabilities%20of%20Transformer%2C%20our%20model%20can%20better%20handle%20the%0Acomplexity%20of%20handwritten%20mathematical%20expressions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05557v2&entry.124074799=Read"},
{"title": "Scaling Intelligence: Designing Data Centers for Next-Gen Language\n  Models", "author": "Jesmin Jahan Tithi and Hanjiang Wu and Avishaii Abuhatzera and Fabrizio Petrini", "abstract": "  The explosive growth of Large Language Models (LLMs), such as GPT-4 with 1.8\ntrillion parameters, demands a fundamental rethinking of data center\narchitecture to ensure scalability, efficiency, and cost-effectiveness. Our\nwork provides a comprehensive co-design framework that jointly explores FLOPS,\nHBM bandwidth and capacity, multiple network topologies (two-tier vs. FullFlat\noptical), the size of the scale-out domain, and popular\nparallelism/optimization strategies used in LLMs. We introduce and evaluate\nFullFlat network architectures, which provide uniform high-bandwidth,\nlow-latency connectivity between all nodes, and demonstrate their\ntransformative impact on performance and scalability. Through detailed\nsensitivity analyses, we quantify the benefits of overlapping compute and\ncommunication, leveraging hardware-accelerated collectives, widening the\nscale-out domain, and increasing memory capacity. Our study spans both sparse\n(mixture of experts) and dense transformer-based LLMs, revealing how system\ndesign choices affect Model FLOPS Utilization (MFU = Model FLOPS per token *\nObserved tokens per second / Peak FLOPS of the hardware) and overall\nthroughput. For the co-design study, we utilized an analytical performance\nmodeling tool capable of predicting LLM runtime within 10% of real-world\nmeasurements. Our findings offer actionable insights and a practical roadmap\nfor designing AI data centers that can efficiently support trillion-parameter\nmodels, reduce optimization complexity, and sustain the rapid evolution of AI\ncapabilities.\n", "link": "http://arxiv.org/abs/2506.15006v2", "date": "2025-08-19", "relevancy": 2.0708, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5818}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Intelligence%3A%20Designing%20Data%20Centers%20for%20Next-Gen%20Language%0A%20%20Models&body=Title%3A%20Scaling%20Intelligence%3A%20Designing%20Data%20Centers%20for%20Next-Gen%20Language%0A%20%20Models%0AAuthor%3A%20Jesmin%20Jahan%20Tithi%20and%20Hanjiang%20Wu%20and%20Avishaii%20Abuhatzera%20and%20Fabrizio%20Petrini%0AAbstract%3A%20%20%20The%20explosive%20growth%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20such%20as%20GPT-4%20with%201.8%0Atrillion%20parameters%2C%20demands%20a%20fundamental%20rethinking%20of%20data%20center%0Aarchitecture%20to%20ensure%20scalability%2C%20efficiency%2C%20and%20cost-effectiveness.%20Our%0Awork%20provides%20a%20comprehensive%20co-design%20framework%20that%20jointly%20explores%20FLOPS%2C%0AHBM%20bandwidth%20and%20capacity%2C%20multiple%20network%20topologies%20%28two-tier%20vs.%20FullFlat%0Aoptical%29%2C%20the%20size%20of%20the%20scale-out%20domain%2C%20and%20popular%0Aparallelism/optimization%20strategies%20used%20in%20LLMs.%20We%20introduce%20and%20evaluate%0AFullFlat%20network%20architectures%2C%20which%20provide%20uniform%20high-bandwidth%2C%0Alow-latency%20connectivity%20between%20all%20nodes%2C%20and%20demonstrate%20their%0Atransformative%20impact%20on%20performance%20and%20scalability.%20Through%20detailed%0Asensitivity%20analyses%2C%20we%20quantify%20the%20benefits%20of%20overlapping%20compute%20and%0Acommunication%2C%20leveraging%20hardware-accelerated%20collectives%2C%20widening%20the%0Ascale-out%20domain%2C%20and%20increasing%20memory%20capacity.%20Our%20study%20spans%20both%20sparse%0A%28mixture%20of%20experts%29%20and%20dense%20transformer-based%20LLMs%2C%20revealing%20how%20system%0Adesign%20choices%20affect%20Model%20FLOPS%20Utilization%20%28MFU%20%3D%20Model%20FLOPS%20per%20token%20%2A%0AObserved%20tokens%20per%20second%20/%20Peak%20FLOPS%20of%20the%20hardware%29%20and%20overall%0Athroughput.%20For%20the%20co-design%20study%2C%20we%20utilized%20an%20analytical%20performance%0Amodeling%20tool%20capable%20of%20predicting%20LLM%20runtime%20within%2010%25%20of%20real-world%0Ameasurements.%20Our%20findings%20offer%20actionable%20insights%20and%20a%20practical%20roadmap%0Afor%20designing%20AI%20data%20centers%20that%20can%20efficiently%20support%20trillion-parameter%0Amodels%2C%20reduce%20optimization%20complexity%2C%20and%20sustain%20the%20rapid%20evolution%20of%20AI%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15006v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Intelligence%253A%2520Designing%2520Data%2520Centers%2520for%2520Next-Gen%2520Language%250A%2520%2520Models%26entry.906535625%3DJesmin%2520Jahan%2520Tithi%2520and%2520Hanjiang%2520Wu%2520and%2520Avishaii%2520Abuhatzera%2520and%2520Fabrizio%2520Petrini%26entry.1292438233%3D%2520%2520The%2520explosive%2520growth%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520such%2520as%2520GPT-4%2520with%25201.8%250Atrillion%2520parameters%252C%2520demands%2520a%2520fundamental%2520rethinking%2520of%2520data%2520center%250Aarchitecture%2520to%2520ensure%2520scalability%252C%2520efficiency%252C%2520and%2520cost-effectiveness.%2520Our%250Awork%2520provides%2520a%2520comprehensive%2520co-design%2520framework%2520that%2520jointly%2520explores%2520FLOPS%252C%250AHBM%2520bandwidth%2520and%2520capacity%252C%2520multiple%2520network%2520topologies%2520%2528two-tier%2520vs.%2520FullFlat%250Aoptical%2529%252C%2520the%2520size%2520of%2520the%2520scale-out%2520domain%252C%2520and%2520popular%250Aparallelism/optimization%2520strategies%2520used%2520in%2520LLMs.%2520We%2520introduce%2520and%2520evaluate%250AFullFlat%2520network%2520architectures%252C%2520which%2520provide%2520uniform%2520high-bandwidth%252C%250Alow-latency%2520connectivity%2520between%2520all%2520nodes%252C%2520and%2520demonstrate%2520their%250Atransformative%2520impact%2520on%2520performance%2520and%2520scalability.%2520Through%2520detailed%250Asensitivity%2520analyses%252C%2520we%2520quantify%2520the%2520benefits%2520of%2520overlapping%2520compute%2520and%250Acommunication%252C%2520leveraging%2520hardware-accelerated%2520collectives%252C%2520widening%2520the%250Ascale-out%2520domain%252C%2520and%2520increasing%2520memory%2520capacity.%2520Our%2520study%2520spans%2520both%2520sparse%250A%2528mixture%2520of%2520experts%2529%2520and%2520dense%2520transformer-based%2520LLMs%252C%2520revealing%2520how%2520system%250Adesign%2520choices%2520affect%2520Model%2520FLOPS%2520Utilization%2520%2528MFU%2520%253D%2520Model%2520FLOPS%2520per%2520token%2520%252A%250AObserved%2520tokens%2520per%2520second%2520/%2520Peak%2520FLOPS%2520of%2520the%2520hardware%2529%2520and%2520overall%250Athroughput.%2520For%2520the%2520co-design%2520study%252C%2520we%2520utilized%2520an%2520analytical%2520performance%250Amodeling%2520tool%2520capable%2520of%2520predicting%2520LLM%2520runtime%2520within%252010%2525%2520of%2520real-world%250Ameasurements.%2520Our%2520findings%2520offer%2520actionable%2520insights%2520and%2520a%2520practical%2520roadmap%250Afor%2520designing%2520AI%2520data%2520centers%2520that%2520can%2520efficiently%2520support%2520trillion-parameter%250Amodels%252C%2520reduce%2520optimization%2520complexity%252C%2520and%2520sustain%2520the%2520rapid%2520evolution%2520of%2520AI%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15006v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Intelligence%3A%20Designing%20Data%20Centers%20for%20Next-Gen%20Language%0A%20%20Models&entry.906535625=Jesmin%20Jahan%20Tithi%20and%20Hanjiang%20Wu%20and%20Avishaii%20Abuhatzera%20and%20Fabrizio%20Petrini&entry.1292438233=%20%20The%20explosive%20growth%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20such%20as%20GPT-4%20with%201.8%0Atrillion%20parameters%2C%20demands%20a%20fundamental%20rethinking%20of%20data%20center%0Aarchitecture%20to%20ensure%20scalability%2C%20efficiency%2C%20and%20cost-effectiveness.%20Our%0Awork%20provides%20a%20comprehensive%20co-design%20framework%20that%20jointly%20explores%20FLOPS%2C%0AHBM%20bandwidth%20and%20capacity%2C%20multiple%20network%20topologies%20%28two-tier%20vs.%20FullFlat%0Aoptical%29%2C%20the%20size%20of%20the%20scale-out%20domain%2C%20and%20popular%0Aparallelism/optimization%20strategies%20used%20in%20LLMs.%20We%20introduce%20and%20evaluate%0AFullFlat%20network%20architectures%2C%20which%20provide%20uniform%20high-bandwidth%2C%0Alow-latency%20connectivity%20between%20all%20nodes%2C%20and%20demonstrate%20their%0Atransformative%20impact%20on%20performance%20and%20scalability.%20Through%20detailed%0Asensitivity%20analyses%2C%20we%20quantify%20the%20benefits%20of%20overlapping%20compute%20and%0Acommunication%2C%20leveraging%20hardware-accelerated%20collectives%2C%20widening%20the%0Ascale-out%20domain%2C%20and%20increasing%20memory%20capacity.%20Our%20study%20spans%20both%20sparse%0A%28mixture%20of%20experts%29%20and%20dense%20transformer-based%20LLMs%2C%20revealing%20how%20system%0Adesign%20choices%20affect%20Model%20FLOPS%20Utilization%20%28MFU%20%3D%20Model%20FLOPS%20per%20token%20%2A%0AObserved%20tokens%20per%20second%20/%20Peak%20FLOPS%20of%20the%20hardware%29%20and%20overall%0Athroughput.%20For%20the%20co-design%20study%2C%20we%20utilized%20an%20analytical%20performance%0Amodeling%20tool%20capable%20of%20predicting%20LLM%20runtime%20within%2010%25%20of%20real-world%0Ameasurements.%20Our%20findings%20offer%20actionable%20insights%20and%20a%20practical%20roadmap%0Afor%20designing%20AI%20data%20centers%20that%20can%20efficiently%20support%20trillion-parameter%0Amodels%2C%20reduce%20optimization%20complexity%2C%20and%20sustain%20the%20rapid%20evolution%20of%20AI%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15006v2&entry.124074799=Read"},
{"title": "ResPlan: A Large-Scale Vector-Graph Dataset of 17,000 Residential Floor\n  Plans", "author": "Mohamed Abouagour and Eleftherios Garyfallidis", "abstract": "  We introduce ResPlan, a large-scale dataset of 17,000 detailed, structurally\nrich, and realistic residential floor plans, created to advance spatial AI\nresearch. Each plan includes precise annotations of architectural elements\n(walls, doors, windows, balconies) and functional spaces (such as kitchens,\nbedrooms, and bathrooms). ResPlan addresses key limitations of existing\ndatasets such as RPLAN (Wu et al., 2019) and MSD (van Engelenburg et al., 2024)\nby offering enhanced visual fidelity and greater structural diversity,\nreflecting realistic and non-idealized residential layouts. Designed as a\nversatile, general-purpose resource, ResPlan supports a wide range of\napplications including robotics, reinforcement learning, generative AI, virtual\nand augmented reality, simulations, and game development. Plans are provided in\nboth geometric and graph-based formats, enabling direct integration into\nsimulation engines and fast 3D conversion. A key contribution is an open-source\npipeline for geometry cleaning, alignment, and annotation refinement.\nAdditionally, ResPlan includes structured representations of room connectivity,\nsupporting graph-based spatial reasoning tasks. Finally, we present comparative\nanalyses with existing benchmarks and outline several open benchmark tasks\nenabled by ResPlan. Ultimately, ResPlan offers a significant advance in scale,\nrealism, and usability, providing a robust foundation for developing and\nbenchmarking next-generation spatial intelligence systems.\n", "link": "http://arxiv.org/abs/2508.14006v1", "date": "2025-08-19", "relevancy": 2.0642, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5293}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5108}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ResPlan%3A%20A%20Large-Scale%20Vector-Graph%20Dataset%20of%2017%2C000%20Residential%20Floor%0A%20%20Plans&body=Title%3A%20ResPlan%3A%20A%20Large-Scale%20Vector-Graph%20Dataset%20of%2017%2C000%20Residential%20Floor%0A%20%20Plans%0AAuthor%3A%20Mohamed%20Abouagour%20and%20Eleftherios%20Garyfallidis%0AAbstract%3A%20%20%20We%20introduce%20ResPlan%2C%20a%20large-scale%20dataset%20of%2017%2C000%20detailed%2C%20structurally%0Arich%2C%20and%20realistic%20residential%20floor%20plans%2C%20created%20to%20advance%20spatial%20AI%0Aresearch.%20Each%20plan%20includes%20precise%20annotations%20of%20architectural%20elements%0A%28walls%2C%20doors%2C%20windows%2C%20balconies%29%20and%20functional%20spaces%20%28such%20as%20kitchens%2C%0Abedrooms%2C%20and%20bathrooms%29.%20ResPlan%20addresses%20key%20limitations%20of%20existing%0Adatasets%20such%20as%20RPLAN%20%28Wu%20et%20al.%2C%202019%29%20and%20MSD%20%28van%20Engelenburg%20et%20al.%2C%202024%29%0Aby%20offering%20enhanced%20visual%20fidelity%20and%20greater%20structural%20diversity%2C%0Areflecting%20realistic%20and%20non-idealized%20residential%20layouts.%20Designed%20as%20a%0Aversatile%2C%20general-purpose%20resource%2C%20ResPlan%20supports%20a%20wide%20range%20of%0Aapplications%20including%20robotics%2C%20reinforcement%20learning%2C%20generative%20AI%2C%20virtual%0Aand%20augmented%20reality%2C%20simulations%2C%20and%20game%20development.%20Plans%20are%20provided%20in%0Aboth%20geometric%20and%20graph-based%20formats%2C%20enabling%20direct%20integration%20into%0Asimulation%20engines%20and%20fast%203D%20conversion.%20A%20key%20contribution%20is%20an%20open-source%0Apipeline%20for%20geometry%20cleaning%2C%20alignment%2C%20and%20annotation%20refinement.%0AAdditionally%2C%20ResPlan%20includes%20structured%20representations%20of%20room%20connectivity%2C%0Asupporting%20graph-based%20spatial%20reasoning%20tasks.%20Finally%2C%20we%20present%20comparative%0Aanalyses%20with%20existing%20benchmarks%20and%20outline%20several%20open%20benchmark%20tasks%0Aenabled%20by%20ResPlan.%20Ultimately%2C%20ResPlan%20offers%20a%20significant%20advance%20in%20scale%2C%0Arealism%2C%20and%20usability%2C%20providing%20a%20robust%20foundation%20for%20developing%20and%0Abenchmarking%20next-generation%20spatial%20intelligence%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResPlan%253A%2520A%2520Large-Scale%2520Vector-Graph%2520Dataset%2520of%252017%252C000%2520Residential%2520Floor%250A%2520%2520Plans%26entry.906535625%3DMohamed%2520Abouagour%2520and%2520Eleftherios%2520Garyfallidis%26entry.1292438233%3D%2520%2520We%2520introduce%2520ResPlan%252C%2520a%2520large-scale%2520dataset%2520of%252017%252C000%2520detailed%252C%2520structurally%250Arich%252C%2520and%2520realistic%2520residential%2520floor%2520plans%252C%2520created%2520to%2520advance%2520spatial%2520AI%250Aresearch.%2520Each%2520plan%2520includes%2520precise%2520annotations%2520of%2520architectural%2520elements%250A%2528walls%252C%2520doors%252C%2520windows%252C%2520balconies%2529%2520and%2520functional%2520spaces%2520%2528such%2520as%2520kitchens%252C%250Abedrooms%252C%2520and%2520bathrooms%2529.%2520ResPlan%2520addresses%2520key%2520limitations%2520of%2520existing%250Adatasets%2520such%2520as%2520RPLAN%2520%2528Wu%2520et%2520al.%252C%25202019%2529%2520and%2520MSD%2520%2528van%2520Engelenburg%2520et%2520al.%252C%25202024%2529%250Aby%2520offering%2520enhanced%2520visual%2520fidelity%2520and%2520greater%2520structural%2520diversity%252C%250Areflecting%2520realistic%2520and%2520non-idealized%2520residential%2520layouts.%2520Designed%2520as%2520a%250Aversatile%252C%2520general-purpose%2520resource%252C%2520ResPlan%2520supports%2520a%2520wide%2520range%2520of%250Aapplications%2520including%2520robotics%252C%2520reinforcement%2520learning%252C%2520generative%2520AI%252C%2520virtual%250Aand%2520augmented%2520reality%252C%2520simulations%252C%2520and%2520game%2520development.%2520Plans%2520are%2520provided%2520in%250Aboth%2520geometric%2520and%2520graph-based%2520formats%252C%2520enabling%2520direct%2520integration%2520into%250Asimulation%2520engines%2520and%2520fast%25203D%2520conversion.%2520A%2520key%2520contribution%2520is%2520an%2520open-source%250Apipeline%2520for%2520geometry%2520cleaning%252C%2520alignment%252C%2520and%2520annotation%2520refinement.%250AAdditionally%252C%2520ResPlan%2520includes%2520structured%2520representations%2520of%2520room%2520connectivity%252C%250Asupporting%2520graph-based%2520spatial%2520reasoning%2520tasks.%2520Finally%252C%2520we%2520present%2520comparative%250Aanalyses%2520with%2520existing%2520benchmarks%2520and%2520outline%2520several%2520open%2520benchmark%2520tasks%250Aenabled%2520by%2520ResPlan.%2520Ultimately%252C%2520ResPlan%2520offers%2520a%2520significant%2520advance%2520in%2520scale%252C%250Arealism%252C%2520and%2520usability%252C%2520providing%2520a%2520robust%2520foundation%2520for%2520developing%2520and%250Abenchmarking%2520next-generation%2520spatial%2520intelligence%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ResPlan%3A%20A%20Large-Scale%20Vector-Graph%20Dataset%20of%2017%2C000%20Residential%20Floor%0A%20%20Plans&entry.906535625=Mohamed%20Abouagour%20and%20Eleftherios%20Garyfallidis&entry.1292438233=%20%20We%20introduce%20ResPlan%2C%20a%20large-scale%20dataset%20of%2017%2C000%20detailed%2C%20structurally%0Arich%2C%20and%20realistic%20residential%20floor%20plans%2C%20created%20to%20advance%20spatial%20AI%0Aresearch.%20Each%20plan%20includes%20precise%20annotations%20of%20architectural%20elements%0A%28walls%2C%20doors%2C%20windows%2C%20balconies%29%20and%20functional%20spaces%20%28such%20as%20kitchens%2C%0Abedrooms%2C%20and%20bathrooms%29.%20ResPlan%20addresses%20key%20limitations%20of%20existing%0Adatasets%20such%20as%20RPLAN%20%28Wu%20et%20al.%2C%202019%29%20and%20MSD%20%28van%20Engelenburg%20et%20al.%2C%202024%29%0Aby%20offering%20enhanced%20visual%20fidelity%20and%20greater%20structural%20diversity%2C%0Areflecting%20realistic%20and%20non-idealized%20residential%20layouts.%20Designed%20as%20a%0Aversatile%2C%20general-purpose%20resource%2C%20ResPlan%20supports%20a%20wide%20range%20of%0Aapplications%20including%20robotics%2C%20reinforcement%20learning%2C%20generative%20AI%2C%20virtual%0Aand%20augmented%20reality%2C%20simulations%2C%20and%20game%20development.%20Plans%20are%20provided%20in%0Aboth%20geometric%20and%20graph-based%20formats%2C%20enabling%20direct%20integration%20into%0Asimulation%20engines%20and%20fast%203D%20conversion.%20A%20key%20contribution%20is%20an%20open-source%0Apipeline%20for%20geometry%20cleaning%2C%20alignment%2C%20and%20annotation%20refinement.%0AAdditionally%2C%20ResPlan%20includes%20structured%20representations%20of%20room%20connectivity%2C%0Asupporting%20graph-based%20spatial%20reasoning%20tasks.%20Finally%2C%20we%20present%20comparative%0Aanalyses%20with%20existing%20benchmarks%20and%20outline%20several%20open%20benchmark%20tasks%0Aenabled%20by%20ResPlan.%20Ultimately%2C%20ResPlan%20offers%20a%20significant%20advance%20in%20scale%2C%0Arealism%2C%20and%20usability%2C%20providing%20a%20robust%20foundation%20for%20developing%20and%0Abenchmarking%20next-generation%20spatial%20intelligence%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14006v1&entry.124074799=Read"},
{"title": "Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman\n  Model Predictive Control", "author": "Haitham El-Hussieny", "abstract": "  This paper presents a data-driven control framework for quadrotor systems\nthat integrates a deep Koopman operator with model predictive control (DK-MPC).\nThe deep Koopman operator is trained on sampled flight data to construct a\nhigh-dimensional latent representation in which the nonlinear quadrotor\ndynamics are approximated by linear models. This linearization enables the\napplication of MPC to efficiently optimize control actions over a finite\nprediction horizon, ensuring accurate trajectory tracking and stabilization.\nThe proposed DK-MPC approach is validated through a series of\ntrajectory-following and point-stabilization numerical experiments, where it\ndemonstrates superior tracking accuracy and significantly lower computation\ntime compared to conventional nonlinear MPC. These results highlight the\npotential of Koopman-based learning methods to handle complex quadrotor\ndynamics while meeting the real-time requirements of embedded flight control.\nFuture work will focus on extending the framework to more agile flight\nscenarios and improving robustness against external disturbances.\n", "link": "http://arxiv.org/abs/2508.13795v1", "date": "2025-08-19", "relevancy": 2.0612, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5525}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.508}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trajectory%20Tracking%20and%20Stabilization%20of%20Quadrotors%20Using%20Deep%20Koopman%0A%20%20Model%20Predictive%20Control&body=Title%3A%20Trajectory%20Tracking%20and%20Stabilization%20of%20Quadrotors%20Using%20Deep%20Koopman%0A%20%20Model%20Predictive%20Control%0AAuthor%3A%20Haitham%20El-Hussieny%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20data-driven%20control%20framework%20for%20quadrotor%20systems%0Athat%20integrates%20a%20deep%20Koopman%20operator%20with%20model%20predictive%20control%20%28DK-MPC%29.%0AThe%20deep%20Koopman%20operator%20is%20trained%20on%20sampled%20flight%20data%20to%20construct%20a%0Ahigh-dimensional%20latent%20representation%20in%20which%20the%20nonlinear%20quadrotor%0Adynamics%20are%20approximated%20by%20linear%20models.%20This%20linearization%20enables%20the%0Aapplication%20of%20MPC%20to%20efficiently%20optimize%20control%20actions%20over%20a%20finite%0Aprediction%20horizon%2C%20ensuring%20accurate%20trajectory%20tracking%20and%20stabilization.%0AThe%20proposed%20DK-MPC%20approach%20is%20validated%20through%20a%20series%20of%0Atrajectory-following%20and%20point-stabilization%20numerical%20experiments%2C%20where%20it%0Ademonstrates%20superior%20tracking%20accuracy%20and%20significantly%20lower%20computation%0Atime%20compared%20to%20conventional%20nonlinear%20MPC.%20These%20results%20highlight%20the%0Apotential%20of%20Koopman-based%20learning%20methods%20to%20handle%20complex%20quadrotor%0Adynamics%20while%20meeting%20the%20real-time%20requirements%20of%20embedded%20flight%20control.%0AFuture%20work%20will%20focus%20on%20extending%20the%20framework%20to%20more%20agile%20flight%0Ascenarios%20and%20improving%20robustness%20against%20external%20disturbances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrajectory%2520Tracking%2520and%2520Stabilization%2520of%2520Quadrotors%2520Using%2520Deep%2520Koopman%250A%2520%2520Model%2520Predictive%2520Control%26entry.906535625%3DHaitham%2520El-Hussieny%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520data-driven%2520control%2520framework%2520for%2520quadrotor%2520systems%250Athat%2520integrates%2520a%2520deep%2520Koopman%2520operator%2520with%2520model%2520predictive%2520control%2520%2528DK-MPC%2529.%250AThe%2520deep%2520Koopman%2520operator%2520is%2520trained%2520on%2520sampled%2520flight%2520data%2520to%2520construct%2520a%250Ahigh-dimensional%2520latent%2520representation%2520in%2520which%2520the%2520nonlinear%2520quadrotor%250Adynamics%2520are%2520approximated%2520by%2520linear%2520models.%2520This%2520linearization%2520enables%2520the%250Aapplication%2520of%2520MPC%2520to%2520efficiently%2520optimize%2520control%2520actions%2520over%2520a%2520finite%250Aprediction%2520horizon%252C%2520ensuring%2520accurate%2520trajectory%2520tracking%2520and%2520stabilization.%250AThe%2520proposed%2520DK-MPC%2520approach%2520is%2520validated%2520through%2520a%2520series%2520of%250Atrajectory-following%2520and%2520point-stabilization%2520numerical%2520experiments%252C%2520where%2520it%250Ademonstrates%2520superior%2520tracking%2520accuracy%2520and%2520significantly%2520lower%2520computation%250Atime%2520compared%2520to%2520conventional%2520nonlinear%2520MPC.%2520These%2520results%2520highlight%2520the%250Apotential%2520of%2520Koopman-based%2520learning%2520methods%2520to%2520handle%2520complex%2520quadrotor%250Adynamics%2520while%2520meeting%2520the%2520real-time%2520requirements%2520of%2520embedded%2520flight%2520control.%250AFuture%2520work%2520will%2520focus%2520on%2520extending%2520the%2520framework%2520to%2520more%2520agile%2520flight%250Ascenarios%2520and%2520improving%2520robustness%2520against%2520external%2520disturbances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trajectory%20Tracking%20and%20Stabilization%20of%20Quadrotors%20Using%20Deep%20Koopman%0A%20%20Model%20Predictive%20Control&entry.906535625=Haitham%20El-Hussieny&entry.1292438233=%20%20This%20paper%20presents%20a%20data-driven%20control%20framework%20for%20quadrotor%20systems%0Athat%20integrates%20a%20deep%20Koopman%20operator%20with%20model%20predictive%20control%20%28DK-MPC%29.%0AThe%20deep%20Koopman%20operator%20is%20trained%20on%20sampled%20flight%20data%20to%20construct%20a%0Ahigh-dimensional%20latent%20representation%20in%20which%20the%20nonlinear%20quadrotor%0Adynamics%20are%20approximated%20by%20linear%20models.%20This%20linearization%20enables%20the%0Aapplication%20of%20MPC%20to%20efficiently%20optimize%20control%20actions%20over%20a%20finite%0Aprediction%20horizon%2C%20ensuring%20accurate%20trajectory%20tracking%20and%20stabilization.%0AThe%20proposed%20DK-MPC%20approach%20is%20validated%20through%20a%20series%20of%0Atrajectory-following%20and%20point-stabilization%20numerical%20experiments%2C%20where%20it%0Ademonstrates%20superior%20tracking%20accuracy%20and%20significantly%20lower%20computation%0Atime%20compared%20to%20conventional%20nonlinear%20MPC.%20These%20results%20highlight%20the%0Apotential%20of%20Koopman-based%20learning%20methods%20to%20handle%20complex%20quadrotor%0Adynamics%20while%20meeting%20the%20real-time%20requirements%20of%20embedded%20flight%20control.%0AFuture%20work%20will%20focus%20on%20extending%20the%20framework%20to%20more%20agile%20flight%0Ascenarios%20and%20improving%20robustness%20against%20external%20disturbances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13795v1&entry.124074799=Read"},
{"title": "COMPASS: A Multi-Dimensional Benchmark for Evaluating Code Generation in\n  Large Language Models", "author": "James Meaden and Micha\u0142 Jarosz and Piotr Jod\u0142owski and Grigori Melnik", "abstract": "  Current code generation benchmarks focus primarily on functional correctness\nwhile overlooking two critical aspects of real-world programming: algorithmic\nefficiency and code quality. We introduce COMPASS (COdility's Multi-dimensional\nProgramming ASSessment), a comprehensive evaluation framework that assesses\ncode generation across three dimensions: correctness, efficiency, and quality.\nCOMPASS consists of 50 competitive programming problems from real Codility\ncompetitions, providing authentic human baselines from 393,150 submissions.\nUnlike existing benchmarks that treat algorithmically inefficient solutions\nidentically to optimal ones provided they pass test cases, COMPASS\nsystematically evaluates runtime efficiency and code quality using\nindustry-standard analysis tools. Our evaluation of three leading\nreasoning-enhanced models, Anthropic Claude Opus 4, Google Gemini 2.5 Pro, and\nOpenAI O4-Mini-High, reveals that models achieving high correctness scores do\nnot necessarily produce efficient algorithms or maintainable code. These\nfindings highlight the importance of evaluating more than just correctness to\ntruly understand the real-world capabilities of code generation models. COMPASS\nserves as a guiding framework, charting a path for future research toward AI\nsystems that are robust, reliable, and ready for production use.\n", "link": "http://arxiv.org/abs/2508.13757v1", "date": "2025-08-19", "relevancy": 2.0529, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5395}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COMPASS%3A%20A%20Multi-Dimensional%20Benchmark%20for%20Evaluating%20Code%20Generation%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20COMPASS%3A%20A%20Multi-Dimensional%20Benchmark%20for%20Evaluating%20Code%20Generation%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20James%20Meaden%20and%20Micha%C5%82%20Jarosz%20and%20Piotr%20Jod%C5%82owski%20and%20Grigori%20Melnik%0AAbstract%3A%20%20%20Current%20code%20generation%20benchmarks%20focus%20primarily%20on%20functional%20correctness%0Awhile%20overlooking%20two%20critical%20aspects%20of%20real-world%20programming%3A%20algorithmic%0Aefficiency%20and%20code%20quality.%20We%20introduce%20COMPASS%20%28COdility%27s%20Multi-dimensional%0AProgramming%20ASSessment%29%2C%20a%20comprehensive%20evaluation%20framework%20that%20assesses%0Acode%20generation%20across%20three%20dimensions%3A%20correctness%2C%20efficiency%2C%20and%20quality.%0ACOMPASS%20consists%20of%2050%20competitive%20programming%20problems%20from%20real%20Codility%0Acompetitions%2C%20providing%20authentic%20human%20baselines%20from%20393%2C150%20submissions.%0AUnlike%20existing%20benchmarks%20that%20treat%20algorithmically%20inefficient%20solutions%0Aidentically%20to%20optimal%20ones%20provided%20they%20pass%20test%20cases%2C%20COMPASS%0Asystematically%20evaluates%20runtime%20efficiency%20and%20code%20quality%20using%0Aindustry-standard%20analysis%20tools.%20Our%20evaluation%20of%20three%20leading%0Areasoning-enhanced%20models%2C%20Anthropic%20Claude%20Opus%204%2C%20Google%20Gemini%202.5%20Pro%2C%20and%0AOpenAI%20O4-Mini-High%2C%20reveals%20that%20models%20achieving%20high%20correctness%20scores%20do%0Anot%20necessarily%20produce%20efficient%20algorithms%20or%20maintainable%20code.%20These%0Afindings%20highlight%20the%20importance%20of%20evaluating%20more%20than%20just%20correctness%20to%0Atruly%20understand%20the%20real-world%20capabilities%20of%20code%20generation%20models.%20COMPASS%0Aserves%20as%20a%20guiding%20framework%2C%20charting%20a%20path%20for%20future%20research%20toward%20AI%0Asystems%20that%20are%20robust%2C%20reliable%2C%20and%20ready%20for%20production%20use.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOMPASS%253A%2520A%2520Multi-Dimensional%2520Benchmark%2520for%2520Evaluating%2520Code%2520Generation%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DJames%2520Meaden%2520and%2520Micha%25C5%2582%2520Jarosz%2520and%2520Piotr%2520Jod%25C5%2582owski%2520and%2520Grigori%2520Melnik%26entry.1292438233%3D%2520%2520Current%2520code%2520generation%2520benchmarks%2520focus%2520primarily%2520on%2520functional%2520correctness%250Awhile%2520overlooking%2520two%2520critical%2520aspects%2520of%2520real-world%2520programming%253A%2520algorithmic%250Aefficiency%2520and%2520code%2520quality.%2520We%2520introduce%2520COMPASS%2520%2528COdility%2527s%2520Multi-dimensional%250AProgramming%2520ASSessment%2529%252C%2520a%2520comprehensive%2520evaluation%2520framework%2520that%2520assesses%250Acode%2520generation%2520across%2520three%2520dimensions%253A%2520correctness%252C%2520efficiency%252C%2520and%2520quality.%250ACOMPASS%2520consists%2520of%252050%2520competitive%2520programming%2520problems%2520from%2520real%2520Codility%250Acompetitions%252C%2520providing%2520authentic%2520human%2520baselines%2520from%2520393%252C150%2520submissions.%250AUnlike%2520existing%2520benchmarks%2520that%2520treat%2520algorithmically%2520inefficient%2520solutions%250Aidentically%2520to%2520optimal%2520ones%2520provided%2520they%2520pass%2520test%2520cases%252C%2520COMPASS%250Asystematically%2520evaluates%2520runtime%2520efficiency%2520and%2520code%2520quality%2520using%250Aindustry-standard%2520analysis%2520tools.%2520Our%2520evaluation%2520of%2520three%2520leading%250Areasoning-enhanced%2520models%252C%2520Anthropic%2520Claude%2520Opus%25204%252C%2520Google%2520Gemini%25202.5%2520Pro%252C%2520and%250AOpenAI%2520O4-Mini-High%252C%2520reveals%2520that%2520models%2520achieving%2520high%2520correctness%2520scores%2520do%250Anot%2520necessarily%2520produce%2520efficient%2520algorithms%2520or%2520maintainable%2520code.%2520These%250Afindings%2520highlight%2520the%2520importance%2520of%2520evaluating%2520more%2520than%2520just%2520correctness%2520to%250Atruly%2520understand%2520the%2520real-world%2520capabilities%2520of%2520code%2520generation%2520models.%2520COMPASS%250Aserves%2520as%2520a%2520guiding%2520framework%252C%2520charting%2520a%2520path%2520for%2520future%2520research%2520toward%2520AI%250Asystems%2520that%2520are%2520robust%252C%2520reliable%252C%2520and%2520ready%2520for%2520production%2520use.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COMPASS%3A%20A%20Multi-Dimensional%20Benchmark%20for%20Evaluating%20Code%20Generation%20in%0A%20%20Large%20Language%20Models&entry.906535625=James%20Meaden%20and%20Micha%C5%82%20Jarosz%20and%20Piotr%20Jod%C5%82owski%20and%20Grigori%20Melnik&entry.1292438233=%20%20Current%20code%20generation%20benchmarks%20focus%20primarily%20on%20functional%20correctness%0Awhile%20overlooking%20two%20critical%20aspects%20of%20real-world%20programming%3A%20algorithmic%0Aefficiency%20and%20code%20quality.%20We%20introduce%20COMPASS%20%28COdility%27s%20Multi-dimensional%0AProgramming%20ASSessment%29%2C%20a%20comprehensive%20evaluation%20framework%20that%20assesses%0Acode%20generation%20across%20three%20dimensions%3A%20correctness%2C%20efficiency%2C%20and%20quality.%0ACOMPASS%20consists%20of%2050%20competitive%20programming%20problems%20from%20real%20Codility%0Acompetitions%2C%20providing%20authentic%20human%20baselines%20from%20393%2C150%20submissions.%0AUnlike%20existing%20benchmarks%20that%20treat%20algorithmically%20inefficient%20solutions%0Aidentically%20to%20optimal%20ones%20provided%20they%20pass%20test%20cases%2C%20COMPASS%0Asystematically%20evaluates%20runtime%20efficiency%20and%20code%20quality%20using%0Aindustry-standard%20analysis%20tools.%20Our%20evaluation%20of%20three%20leading%0Areasoning-enhanced%20models%2C%20Anthropic%20Claude%20Opus%204%2C%20Google%20Gemini%202.5%20Pro%2C%20and%0AOpenAI%20O4-Mini-High%2C%20reveals%20that%20models%20achieving%20high%20correctness%20scores%20do%0Anot%20necessarily%20produce%20efficient%20algorithms%20or%20maintainable%20code.%20These%0Afindings%20highlight%20the%20importance%20of%20evaluating%20more%20than%20just%20correctness%20to%0Atruly%20understand%20the%20real-world%20capabilities%20of%20code%20generation%20models.%20COMPASS%0Aserves%20as%20a%20guiding%20framework%2C%20charting%20a%20path%20for%20future%20research%20toward%20AI%0Asystems%20that%20are%20robust%2C%20reliable%2C%20and%20ready%20for%20production%20use.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13757v1&entry.124074799=Read"},
{"title": "Extracting Structured Requirements from Unstructured Building Technical\n  Specifications for Building Information Modeling", "author": "Insaf Nahri and Romain Pinqui\u00e9 and Philippe V\u00e9ron and Nicolas Bus and Mathieu Thorel", "abstract": "  This study explores the integration of Building Information Modeling (BIM)\nwith Natural Language Processing (NLP) to automate the extraction of\nrequirements from unstructured French Building Technical Specification (BTS)\ndocuments within the construction industry. Employing Named Entity Recognition\n(NER) and Relation Extraction (RE) techniques, the study leverages the\ntransformer-based model CamemBERT and applies transfer learning with the French\nlanguage model Fr\\_core\\_news\\_lg, both pre-trained on a large French corpus in\nthe general domain. To benchmark these models, additional approaches ranging\nfrom rule-based to deep learning-based methods are developed. For RE, four\ndifferent supervised models, including Random Forest, are implemented using a\ncustom feature vector. A hand-crafted annotated dataset is used to compare the\neffectiveness of NER approaches and RE models. Results indicate that CamemBERT\nand Fr\\_core\\_news\\_lg exhibited superior performance in NER, achieving\nF1-scores over 90\\%, while Random Forest proved most effective in RE, with an\nF1 score above 80\\%. The outcomes are intended to be represented as a knowledge\ngraph in future work to further enhance automatic verification systems.\n", "link": "http://arxiv.org/abs/2508.13833v1", "date": "2025-08-19", "relevancy": 2.0416, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5175}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5175}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extracting%20Structured%20Requirements%20from%20Unstructured%20Building%20Technical%0A%20%20Specifications%20for%20Building%20Information%20Modeling&body=Title%3A%20Extracting%20Structured%20Requirements%20from%20Unstructured%20Building%20Technical%0A%20%20Specifications%20for%20Building%20Information%20Modeling%0AAuthor%3A%20Insaf%20Nahri%20and%20Romain%20Pinqui%C3%A9%20and%20Philippe%20V%C3%A9ron%20and%20Nicolas%20Bus%20and%20Mathieu%20Thorel%0AAbstract%3A%20%20%20This%20study%20explores%20the%20integration%20of%20Building%20Information%20Modeling%20%28BIM%29%0Awith%20Natural%20Language%20Processing%20%28NLP%29%20to%20automate%20the%20extraction%20of%0Arequirements%20from%20unstructured%20French%20Building%20Technical%20Specification%20%28BTS%29%0Adocuments%20within%20the%20construction%20industry.%20Employing%20Named%20Entity%20Recognition%0A%28NER%29%20and%20Relation%20Extraction%20%28RE%29%20techniques%2C%20the%20study%20leverages%20the%0Atransformer-based%20model%20CamemBERT%20and%20applies%20transfer%20learning%20with%20the%20French%0Alanguage%20model%20Fr%5C_core%5C_news%5C_lg%2C%20both%20pre-trained%20on%20a%20large%20French%20corpus%20in%0Athe%20general%20domain.%20To%20benchmark%20these%20models%2C%20additional%20approaches%20ranging%0Afrom%20rule-based%20to%20deep%20learning-based%20methods%20are%20developed.%20For%20RE%2C%20four%0Adifferent%20supervised%20models%2C%20including%20Random%20Forest%2C%20are%20implemented%20using%20a%0Acustom%20feature%20vector.%20A%20hand-crafted%20annotated%20dataset%20is%20used%20to%20compare%20the%0Aeffectiveness%20of%20NER%20approaches%20and%20RE%20models.%20Results%20indicate%20that%20CamemBERT%0Aand%20Fr%5C_core%5C_news%5C_lg%20exhibited%20superior%20performance%20in%20NER%2C%20achieving%0AF1-scores%20over%2090%5C%25%2C%20while%20Random%20Forest%20proved%20most%20effective%20in%20RE%2C%20with%20an%0AF1%20score%20above%2080%5C%25.%20The%20outcomes%20are%20intended%20to%20be%20represented%20as%20a%20knowledge%0Agraph%20in%20future%20work%20to%20further%20enhance%20automatic%20verification%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtracting%2520Structured%2520Requirements%2520from%2520Unstructured%2520Building%2520Technical%250A%2520%2520Specifications%2520for%2520Building%2520Information%2520Modeling%26entry.906535625%3DInsaf%2520Nahri%2520and%2520Romain%2520Pinqui%25C3%25A9%2520and%2520Philippe%2520V%25C3%25A9ron%2520and%2520Nicolas%2520Bus%2520and%2520Mathieu%2520Thorel%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520the%2520integration%2520of%2520Building%2520Information%2520Modeling%2520%2528BIM%2529%250Awith%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520to%2520automate%2520the%2520extraction%2520of%250Arequirements%2520from%2520unstructured%2520French%2520Building%2520Technical%2520Specification%2520%2528BTS%2529%250Adocuments%2520within%2520the%2520construction%2520industry.%2520Employing%2520Named%2520Entity%2520Recognition%250A%2528NER%2529%2520and%2520Relation%2520Extraction%2520%2528RE%2529%2520techniques%252C%2520the%2520study%2520leverages%2520the%250Atransformer-based%2520model%2520CamemBERT%2520and%2520applies%2520transfer%2520learning%2520with%2520the%2520French%250Alanguage%2520model%2520Fr%255C_core%255C_news%255C_lg%252C%2520both%2520pre-trained%2520on%2520a%2520large%2520French%2520corpus%2520in%250Athe%2520general%2520domain.%2520To%2520benchmark%2520these%2520models%252C%2520additional%2520approaches%2520ranging%250Afrom%2520rule-based%2520to%2520deep%2520learning-based%2520methods%2520are%2520developed.%2520For%2520RE%252C%2520four%250Adifferent%2520supervised%2520models%252C%2520including%2520Random%2520Forest%252C%2520are%2520implemented%2520using%2520a%250Acustom%2520feature%2520vector.%2520A%2520hand-crafted%2520annotated%2520dataset%2520is%2520used%2520to%2520compare%2520the%250Aeffectiveness%2520of%2520NER%2520approaches%2520and%2520RE%2520models.%2520Results%2520indicate%2520that%2520CamemBERT%250Aand%2520Fr%255C_core%255C_news%255C_lg%2520exhibited%2520superior%2520performance%2520in%2520NER%252C%2520achieving%250AF1-scores%2520over%252090%255C%2525%252C%2520while%2520Random%2520Forest%2520proved%2520most%2520effective%2520in%2520RE%252C%2520with%2520an%250AF1%2520score%2520above%252080%255C%2525.%2520The%2520outcomes%2520are%2520intended%2520to%2520be%2520represented%2520as%2520a%2520knowledge%250Agraph%2520in%2520future%2520work%2520to%2520further%2520enhance%2520automatic%2520verification%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extracting%20Structured%20Requirements%20from%20Unstructured%20Building%20Technical%0A%20%20Specifications%20for%20Building%20Information%20Modeling&entry.906535625=Insaf%20Nahri%20and%20Romain%20Pinqui%C3%A9%20and%20Philippe%20V%C3%A9ron%20and%20Nicolas%20Bus%20and%20Mathieu%20Thorel&entry.1292438233=%20%20This%20study%20explores%20the%20integration%20of%20Building%20Information%20Modeling%20%28BIM%29%0Awith%20Natural%20Language%20Processing%20%28NLP%29%20to%20automate%20the%20extraction%20of%0Arequirements%20from%20unstructured%20French%20Building%20Technical%20Specification%20%28BTS%29%0Adocuments%20within%20the%20construction%20industry.%20Employing%20Named%20Entity%20Recognition%0A%28NER%29%20and%20Relation%20Extraction%20%28RE%29%20techniques%2C%20the%20study%20leverages%20the%0Atransformer-based%20model%20CamemBERT%20and%20applies%20transfer%20learning%20with%20the%20French%0Alanguage%20model%20Fr%5C_core%5C_news%5C_lg%2C%20both%20pre-trained%20on%20a%20large%20French%20corpus%20in%0Athe%20general%20domain.%20To%20benchmark%20these%20models%2C%20additional%20approaches%20ranging%0Afrom%20rule-based%20to%20deep%20learning-based%20methods%20are%20developed.%20For%20RE%2C%20four%0Adifferent%20supervised%20models%2C%20including%20Random%20Forest%2C%20are%20implemented%20using%20a%0Acustom%20feature%20vector.%20A%20hand-crafted%20annotated%20dataset%20is%20used%20to%20compare%20the%0Aeffectiveness%20of%20NER%20approaches%20and%20RE%20models.%20Results%20indicate%20that%20CamemBERT%0Aand%20Fr%5C_core%5C_news%5C_lg%20exhibited%20superior%20performance%20in%20NER%2C%20achieving%0AF1-scores%20over%2090%5C%25%2C%20while%20Random%20Forest%20proved%20most%20effective%20in%20RE%2C%20with%20an%0AF1%20score%20above%2080%5C%25.%20The%20outcomes%20are%20intended%20to%20be%20represented%20as%20a%20knowledge%0Agraph%20in%20future%20work%20to%20further%20enhance%20automatic%20verification%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13833v1&entry.124074799=Read"},
{"title": "Structured Agentic Workflows for Financial Time-Series Modeling with\n  LLMs and Reflective Feedback", "author": "Yihao Ang and Yifan Bao and Lei Jiang and Jiajie Tao and Anthony K. H. Tung and Lukasz Szpruch and Hao Ni", "abstract": "  Time-series data is central to decision-making in financial markets, yet\nbuilding high-performing, interpretable, and auditable models remains a major\nchallenge. While Automated Machine Learning (AutoML) frameworks streamline\nmodel development, they often lack adaptability and responsiveness to\ndomain-specific needs and evolving objectives. Concurrently, Large Language\nModels (LLMs) have enabled agentic systems capable of reasoning, memory\nmanagement, and dynamic code generation, offering a path toward more flexible\nworkflow automation. In this paper, we introduce \\textsf{TS-Agent}, a modular\nagentic framework designed to automate and enhance time-series modeling\nworkflows for financial applications. The agent formalizes the pipeline as a\nstructured, iterative decision process across three stages: model selection,\ncode refinement, and fine-tuning, guided by contextual reasoning and\nexperimental feedback. Central to our architecture is a planner agent equipped\nwith structured knowledge banks, curated libraries of models and refinement\nstrategies, which guide exploration, while improving interpretability and\nreducing error propagation. \\textsf{TS-Agent} supports adaptive learning,\nrobust debugging, and transparent auditing, key requirements for high-stakes\nenvironments such as financial services. Empirical evaluations on diverse\nfinancial forecasting and synthetic data generation tasks demonstrate that\n\\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic\nbaselines, achieving superior accuracy, robustness, and decision traceability.\n", "link": "http://arxiv.org/abs/2508.13915v1", "date": "2025-08-19", "relevancy": 2.0338, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5196}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5119}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Agentic%20Workflows%20for%20Financial%20Time-Series%20Modeling%20with%0A%20%20LLMs%20and%20Reflective%20Feedback&body=Title%3A%20Structured%20Agentic%20Workflows%20for%20Financial%20Time-Series%20Modeling%20with%0A%20%20LLMs%20and%20Reflective%20Feedback%0AAuthor%3A%20Yihao%20Ang%20and%20Yifan%20Bao%20and%20Lei%20Jiang%20and%20Jiajie%20Tao%20and%20Anthony%20K.%20H.%20Tung%20and%20Lukasz%20Szpruch%20and%20Hao%20Ni%0AAbstract%3A%20%20%20Time-series%20data%20is%20central%20to%20decision-making%20in%20financial%20markets%2C%20yet%0Abuilding%20high-performing%2C%20interpretable%2C%20and%20auditable%20models%20remains%20a%20major%0Achallenge.%20While%20Automated%20Machine%20Learning%20%28AutoML%29%20frameworks%20streamline%0Amodel%20development%2C%20they%20often%20lack%20adaptability%20and%20responsiveness%20to%0Adomain-specific%20needs%20and%20evolving%20objectives.%20Concurrently%2C%20Large%20Language%0AModels%20%28LLMs%29%20have%20enabled%20agentic%20systems%20capable%20of%20reasoning%2C%20memory%0Amanagement%2C%20and%20dynamic%20code%20generation%2C%20offering%20a%20path%20toward%20more%20flexible%0Aworkflow%20automation.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextsf%7BTS-Agent%7D%2C%20a%20modular%0Aagentic%20framework%20designed%20to%20automate%20and%20enhance%20time-series%20modeling%0Aworkflows%20for%20financial%20applications.%20The%20agent%20formalizes%20the%20pipeline%20as%20a%0Astructured%2C%20iterative%20decision%20process%20across%20three%20stages%3A%20model%20selection%2C%0Acode%20refinement%2C%20and%20fine-tuning%2C%20guided%20by%20contextual%20reasoning%20and%0Aexperimental%20feedback.%20Central%20to%20our%20architecture%20is%20a%20planner%20agent%20equipped%0Awith%20structured%20knowledge%20banks%2C%20curated%20libraries%20of%20models%20and%20refinement%0Astrategies%2C%20which%20guide%20exploration%2C%20while%20improving%20interpretability%20and%0Areducing%20error%20propagation.%20%5Ctextsf%7BTS-Agent%7D%20supports%20adaptive%20learning%2C%0Arobust%20debugging%2C%20and%20transparent%20auditing%2C%20key%20requirements%20for%20high-stakes%0Aenvironments%20such%20as%20financial%20services.%20Empirical%20evaluations%20on%20diverse%0Afinancial%20forecasting%20and%20synthetic%20data%20generation%20tasks%20demonstrate%20that%0A%5Ctextsf%7BTS-Agent%7D%20consistently%20outperforms%20state-of-the-art%20AutoML%20and%20agentic%0Abaselines%2C%20achieving%20superior%20accuracy%2C%20robustness%2C%20and%20decision%20traceability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Agentic%2520Workflows%2520for%2520Financial%2520Time-Series%2520Modeling%2520with%250A%2520%2520LLMs%2520and%2520Reflective%2520Feedback%26entry.906535625%3DYihao%2520Ang%2520and%2520Yifan%2520Bao%2520and%2520Lei%2520Jiang%2520and%2520Jiajie%2520Tao%2520and%2520Anthony%2520K.%2520H.%2520Tung%2520and%2520Lukasz%2520Szpruch%2520and%2520Hao%2520Ni%26entry.1292438233%3D%2520%2520Time-series%2520data%2520is%2520central%2520to%2520decision-making%2520in%2520financial%2520markets%252C%2520yet%250Abuilding%2520high-performing%252C%2520interpretable%252C%2520and%2520auditable%2520models%2520remains%2520a%2520major%250Achallenge.%2520While%2520Automated%2520Machine%2520Learning%2520%2528AutoML%2529%2520frameworks%2520streamline%250Amodel%2520development%252C%2520they%2520often%2520lack%2520adaptability%2520and%2520responsiveness%2520to%250Adomain-specific%2520needs%2520and%2520evolving%2520objectives.%2520Concurrently%252C%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520have%2520enabled%2520agentic%2520systems%2520capable%2520of%2520reasoning%252C%2520memory%250Amanagement%252C%2520and%2520dynamic%2520code%2520generation%252C%2520offering%2520a%2520path%2520toward%2520more%2520flexible%250Aworkflow%2520automation.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520%255Ctextsf%257BTS-Agent%257D%252C%2520a%2520modular%250Aagentic%2520framework%2520designed%2520to%2520automate%2520and%2520enhance%2520time-series%2520modeling%250Aworkflows%2520for%2520financial%2520applications.%2520The%2520agent%2520formalizes%2520the%2520pipeline%2520as%2520a%250Astructured%252C%2520iterative%2520decision%2520process%2520across%2520three%2520stages%253A%2520model%2520selection%252C%250Acode%2520refinement%252C%2520and%2520fine-tuning%252C%2520guided%2520by%2520contextual%2520reasoning%2520and%250Aexperimental%2520feedback.%2520Central%2520to%2520our%2520architecture%2520is%2520a%2520planner%2520agent%2520equipped%250Awith%2520structured%2520knowledge%2520banks%252C%2520curated%2520libraries%2520of%2520models%2520and%2520refinement%250Astrategies%252C%2520which%2520guide%2520exploration%252C%2520while%2520improving%2520interpretability%2520and%250Areducing%2520error%2520propagation.%2520%255Ctextsf%257BTS-Agent%257D%2520supports%2520adaptive%2520learning%252C%250Arobust%2520debugging%252C%2520and%2520transparent%2520auditing%252C%2520key%2520requirements%2520for%2520high-stakes%250Aenvironments%2520such%2520as%2520financial%2520services.%2520Empirical%2520evaluations%2520on%2520diverse%250Afinancial%2520forecasting%2520and%2520synthetic%2520data%2520generation%2520tasks%2520demonstrate%2520that%250A%255Ctextsf%257BTS-Agent%257D%2520consistently%2520outperforms%2520state-of-the-art%2520AutoML%2520and%2520agentic%250Abaselines%252C%2520achieving%2520superior%2520accuracy%252C%2520robustness%252C%2520and%2520decision%2520traceability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Agentic%20Workflows%20for%20Financial%20Time-Series%20Modeling%20with%0A%20%20LLMs%20and%20Reflective%20Feedback&entry.906535625=Yihao%20Ang%20and%20Yifan%20Bao%20and%20Lei%20Jiang%20and%20Jiajie%20Tao%20and%20Anthony%20K.%20H.%20Tung%20and%20Lukasz%20Szpruch%20and%20Hao%20Ni&entry.1292438233=%20%20Time-series%20data%20is%20central%20to%20decision-making%20in%20financial%20markets%2C%20yet%0Abuilding%20high-performing%2C%20interpretable%2C%20and%20auditable%20models%20remains%20a%20major%0Achallenge.%20While%20Automated%20Machine%20Learning%20%28AutoML%29%20frameworks%20streamline%0Amodel%20development%2C%20they%20often%20lack%20adaptability%20and%20responsiveness%20to%0Adomain-specific%20needs%20and%20evolving%20objectives.%20Concurrently%2C%20Large%20Language%0AModels%20%28LLMs%29%20have%20enabled%20agentic%20systems%20capable%20of%20reasoning%2C%20memory%0Amanagement%2C%20and%20dynamic%20code%20generation%2C%20offering%20a%20path%20toward%20more%20flexible%0Aworkflow%20automation.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextsf%7BTS-Agent%7D%2C%20a%20modular%0Aagentic%20framework%20designed%20to%20automate%20and%20enhance%20time-series%20modeling%0Aworkflows%20for%20financial%20applications.%20The%20agent%20formalizes%20the%20pipeline%20as%20a%0Astructured%2C%20iterative%20decision%20process%20across%20three%20stages%3A%20model%20selection%2C%0Acode%20refinement%2C%20and%20fine-tuning%2C%20guided%20by%20contextual%20reasoning%20and%0Aexperimental%20feedback.%20Central%20to%20our%20architecture%20is%20a%20planner%20agent%20equipped%0Awith%20structured%20knowledge%20banks%2C%20curated%20libraries%20of%20models%20and%20refinement%0Astrategies%2C%20which%20guide%20exploration%2C%20while%20improving%20interpretability%20and%0Areducing%20error%20propagation.%20%5Ctextsf%7BTS-Agent%7D%20supports%20adaptive%20learning%2C%0Arobust%20debugging%2C%20and%20transparent%20auditing%2C%20key%20requirements%20for%20high-stakes%0Aenvironments%20such%20as%20financial%20services.%20Empirical%20evaluations%20on%20diverse%0Afinancial%20forecasting%20and%20synthetic%20data%20generation%20tasks%20demonstrate%20that%0A%5Ctextsf%7BTS-Agent%7D%20consistently%20outperforms%20state-of-the-art%20AutoML%20and%20agentic%0Abaselines%2C%20achieving%20superior%20accuracy%2C%20robustness%2C%20and%20decision%20traceability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13915v1&entry.124074799=Read"},
{"title": "Timestep-Compressed Attack on Spiking Neural Networks through\n  Timestep-Level Backpropagation", "author": "Donghwa Kang and Doohyun Kim and Sang-Ki Ko and Jinkyu Lee and Hyeongboo Baek and Brent ByungHoon Kang", "abstract": "  State-of-the-art (SOTA) gradient-based adversarial attacks on spiking neural\nnetworks (SNNs), which largely rely on extending FGSM and PGD frameworks, face\na critical limitation: substantial attack latency from multi-timestep\nprocessing, rendering them infeasible for practical real-time applications.\nThis inefficiency stems from their design as direct extensions of ANN\nparadigms, which fail to exploit key SNN properties. In this paper, we propose\nthe timestep-compressed attack (TCA), a novel framework that significantly\nreduces attack latency. TCA introduces two components founded on key insights\ninto SNN behavior. First, timestep-level backpropagation (TLBP) is based on our\nfinding that global temporal information in backpropagation to generate\nperturbations is not critical for an attack's success, enabling per-timestep\nevaluation for early stopping. Second, adversarial membrane potential reuse\n(A-MPR) is motivated by the observation that initial timesteps are\ninefficiently spent accumulating membrane potential, a warm-up phase that can\nbe pre-calculated and reused. Our experiments on VGG-11 and ResNet-17 with the\nCIFAR-10/100 and CIFAR10-DVS datasets show that TCA significantly reduces the\nrequired attack latency by up to 56.6% and 57.1% compared to SOTA methods in\nwhite-box and black-box settings, respectively, while maintaining a comparable\nattack success rate.\n", "link": "http://arxiv.org/abs/2508.13812v1", "date": "2025-08-19", "relevancy": 2.0307, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5189}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5172}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Timestep-Compressed%20Attack%20on%20Spiking%20Neural%20Networks%20through%0A%20%20Timestep-Level%20Backpropagation&body=Title%3A%20Timestep-Compressed%20Attack%20on%20Spiking%20Neural%20Networks%20through%0A%20%20Timestep-Level%20Backpropagation%0AAuthor%3A%20Donghwa%20Kang%20and%20Doohyun%20Kim%20and%20Sang-Ki%20Ko%20and%20Jinkyu%20Lee%20and%20Hyeongboo%20Baek%20and%20Brent%20ByungHoon%20Kang%0AAbstract%3A%20%20%20State-of-the-art%20%28SOTA%29%20gradient-based%20adversarial%20attacks%20on%20spiking%20neural%0Anetworks%20%28SNNs%29%2C%20which%20largely%20rely%20on%20extending%20FGSM%20and%20PGD%20frameworks%2C%20face%0Aa%20critical%20limitation%3A%20substantial%20attack%20latency%20from%20multi-timestep%0Aprocessing%2C%20rendering%20them%20infeasible%20for%20practical%20real-time%20applications.%0AThis%20inefficiency%20stems%20from%20their%20design%20as%20direct%20extensions%20of%20ANN%0Aparadigms%2C%20which%20fail%20to%20exploit%20key%20SNN%20properties.%20In%20this%20paper%2C%20we%20propose%0Athe%20timestep-compressed%20attack%20%28TCA%29%2C%20a%20novel%20framework%20that%20significantly%0Areduces%20attack%20latency.%20TCA%20introduces%20two%20components%20founded%20on%20key%20insights%0Ainto%20SNN%20behavior.%20First%2C%20timestep-level%20backpropagation%20%28TLBP%29%20is%20based%20on%20our%0Afinding%20that%20global%20temporal%20information%20in%20backpropagation%20to%20generate%0Aperturbations%20is%20not%20critical%20for%20an%20attack%27s%20success%2C%20enabling%20per-timestep%0Aevaluation%20for%20early%20stopping.%20Second%2C%20adversarial%20membrane%20potential%20reuse%0A%28A-MPR%29%20is%20motivated%20by%20the%20observation%20that%20initial%20timesteps%20are%0Ainefficiently%20spent%20accumulating%20membrane%20potential%2C%20a%20warm-up%20phase%20that%20can%0Abe%20pre-calculated%20and%20reused.%20Our%20experiments%20on%20VGG-11%20and%20ResNet-17%20with%20the%0ACIFAR-10/100%20and%20CIFAR10-DVS%20datasets%20show%20that%20TCA%20significantly%20reduces%20the%0Arequired%20attack%20latency%20by%20up%20to%2056.6%25%20and%2057.1%25%20compared%20to%20SOTA%20methods%20in%0Awhite-box%20and%20black-box%20settings%2C%20respectively%2C%20while%20maintaining%20a%20comparable%0Aattack%20success%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimestep-Compressed%2520Attack%2520on%2520Spiking%2520Neural%2520Networks%2520through%250A%2520%2520Timestep-Level%2520Backpropagation%26entry.906535625%3DDonghwa%2520Kang%2520and%2520Doohyun%2520Kim%2520and%2520Sang-Ki%2520Ko%2520and%2520Jinkyu%2520Lee%2520and%2520Hyeongboo%2520Baek%2520and%2520Brent%2520ByungHoon%2520Kang%26entry.1292438233%3D%2520%2520State-of-the-art%2520%2528SOTA%2529%2520gradient-based%2520adversarial%2520attacks%2520on%2520spiking%2520neural%250Anetworks%2520%2528SNNs%2529%252C%2520which%2520largely%2520rely%2520on%2520extending%2520FGSM%2520and%2520PGD%2520frameworks%252C%2520face%250Aa%2520critical%2520limitation%253A%2520substantial%2520attack%2520latency%2520from%2520multi-timestep%250Aprocessing%252C%2520rendering%2520them%2520infeasible%2520for%2520practical%2520real-time%2520applications.%250AThis%2520inefficiency%2520stems%2520from%2520their%2520design%2520as%2520direct%2520extensions%2520of%2520ANN%250Aparadigms%252C%2520which%2520fail%2520to%2520exploit%2520key%2520SNN%2520properties.%2520In%2520this%2520paper%252C%2520we%2520propose%250Athe%2520timestep-compressed%2520attack%2520%2528TCA%2529%252C%2520a%2520novel%2520framework%2520that%2520significantly%250Areduces%2520attack%2520latency.%2520TCA%2520introduces%2520two%2520components%2520founded%2520on%2520key%2520insights%250Ainto%2520SNN%2520behavior.%2520First%252C%2520timestep-level%2520backpropagation%2520%2528TLBP%2529%2520is%2520based%2520on%2520our%250Afinding%2520that%2520global%2520temporal%2520information%2520in%2520backpropagation%2520to%2520generate%250Aperturbations%2520is%2520not%2520critical%2520for%2520an%2520attack%2527s%2520success%252C%2520enabling%2520per-timestep%250Aevaluation%2520for%2520early%2520stopping.%2520Second%252C%2520adversarial%2520membrane%2520potential%2520reuse%250A%2528A-MPR%2529%2520is%2520motivated%2520by%2520the%2520observation%2520that%2520initial%2520timesteps%2520are%250Ainefficiently%2520spent%2520accumulating%2520membrane%2520potential%252C%2520a%2520warm-up%2520phase%2520that%2520can%250Abe%2520pre-calculated%2520and%2520reused.%2520Our%2520experiments%2520on%2520VGG-11%2520and%2520ResNet-17%2520with%2520the%250ACIFAR-10/100%2520and%2520CIFAR10-DVS%2520datasets%2520show%2520that%2520TCA%2520significantly%2520reduces%2520the%250Arequired%2520attack%2520latency%2520by%2520up%2520to%252056.6%2525%2520and%252057.1%2525%2520compared%2520to%2520SOTA%2520methods%2520in%250Awhite-box%2520and%2520black-box%2520settings%252C%2520respectively%252C%2520while%2520maintaining%2520a%2520comparable%250Aattack%2520success%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Timestep-Compressed%20Attack%20on%20Spiking%20Neural%20Networks%20through%0A%20%20Timestep-Level%20Backpropagation&entry.906535625=Donghwa%20Kang%20and%20Doohyun%20Kim%20and%20Sang-Ki%20Ko%20and%20Jinkyu%20Lee%20and%20Hyeongboo%20Baek%20and%20Brent%20ByungHoon%20Kang&entry.1292438233=%20%20State-of-the-art%20%28SOTA%29%20gradient-based%20adversarial%20attacks%20on%20spiking%20neural%0Anetworks%20%28SNNs%29%2C%20which%20largely%20rely%20on%20extending%20FGSM%20and%20PGD%20frameworks%2C%20face%0Aa%20critical%20limitation%3A%20substantial%20attack%20latency%20from%20multi-timestep%0Aprocessing%2C%20rendering%20them%20infeasible%20for%20practical%20real-time%20applications.%0AThis%20inefficiency%20stems%20from%20their%20design%20as%20direct%20extensions%20of%20ANN%0Aparadigms%2C%20which%20fail%20to%20exploit%20key%20SNN%20properties.%20In%20this%20paper%2C%20we%20propose%0Athe%20timestep-compressed%20attack%20%28TCA%29%2C%20a%20novel%20framework%20that%20significantly%0Areduces%20attack%20latency.%20TCA%20introduces%20two%20components%20founded%20on%20key%20insights%0Ainto%20SNN%20behavior.%20First%2C%20timestep-level%20backpropagation%20%28TLBP%29%20is%20based%20on%20our%0Afinding%20that%20global%20temporal%20information%20in%20backpropagation%20to%20generate%0Aperturbations%20is%20not%20critical%20for%20an%20attack%27s%20success%2C%20enabling%20per-timestep%0Aevaluation%20for%20early%20stopping.%20Second%2C%20adversarial%20membrane%20potential%20reuse%0A%28A-MPR%29%20is%20motivated%20by%20the%20observation%20that%20initial%20timesteps%20are%0Ainefficiently%20spent%20accumulating%20membrane%20potential%2C%20a%20warm-up%20phase%20that%20can%0Abe%20pre-calculated%20and%20reused.%20Our%20experiments%20on%20VGG-11%20and%20ResNet-17%20with%20the%0ACIFAR-10/100%20and%20CIFAR10-DVS%20datasets%20show%20that%20TCA%20significantly%20reduces%20the%0Arequired%20attack%20latency%20by%20up%20to%2056.6%25%20and%2057.1%25%20compared%20to%20SOTA%20methods%20in%0Awhite-box%20and%20black-box%20settings%2C%20respectively%2C%20while%20maintaining%20a%20comparable%0Aattack%20success%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13812v1&entry.124074799=Read"},
{"title": "Typed Topological Structures Of Datasets", "author": "Wanjun Hu", "abstract": "  A datatset $X$ on $R^2$ is a finite topological space. Current research of a\ndataset focuses on statistical methods and the algebraic topological method\n\\cite{carlsson}. In \\cite{hu}, the concept of typed topological space was\nintroduced and showed to have the potential for studying finite topological\nspaces, such as a dataset. It is a new method from the general topology\nperspective. A typed topological space is a topological space whose open sets\nare assigned types. Topological concepts and methods can be redefined using\nopen sets of certain types. In this article, we develop a special set of types\nand its related typed topology on a dataset $X$. Using it, we can investigate\nthe inner structure of $X$. In particular, $R^2$ has a natural quotient space,\nin which $X$ is organized into tracks, and each track is split into components.\nThose components are in a order. Further, they can be represented by an integer\nsequence. Components crossing tracks form branches, and the relationship can be\nwell represented by a type of pseudotree (called typed-II pseudotree). Such\nstructures provide a platform for new algorithms for problems such as\ncalculating convex hull, holes, clustering and anomaly detection.\n", "link": "http://arxiv.org/abs/2508.14008v1", "date": "2025-08-19", "relevancy": 2.0275, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4169}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4048}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Typed%20Topological%20Structures%20Of%20Datasets&body=Title%3A%20Typed%20Topological%20Structures%20Of%20Datasets%0AAuthor%3A%20Wanjun%20Hu%0AAbstract%3A%20%20%20A%20datatset%20%24X%24%20on%20%24R%5E2%24%20is%20a%20finite%20topological%20space.%20Current%20research%20of%20a%0Adataset%20focuses%20on%20statistical%20methods%20and%20the%20algebraic%20topological%20method%0A%5Ccite%7Bcarlsson%7D.%20In%20%5Ccite%7Bhu%7D%2C%20the%20concept%20of%20typed%20topological%20space%20was%0Aintroduced%20and%20showed%20to%20have%20the%20potential%20for%20studying%20finite%20topological%0Aspaces%2C%20such%20as%20a%20dataset.%20It%20is%20a%20new%20method%20from%20the%20general%20topology%0Aperspective.%20A%20typed%20topological%20space%20is%20a%20topological%20space%20whose%20open%20sets%0Aare%20assigned%20types.%20Topological%20concepts%20and%20methods%20can%20be%20redefined%20using%0Aopen%20sets%20of%20certain%20types.%20In%20this%20article%2C%20we%20develop%20a%20special%20set%20of%20types%0Aand%20its%20related%20typed%20topology%20on%20a%20dataset%20%24X%24.%20Using%20it%2C%20we%20can%20investigate%0Athe%20inner%20structure%20of%20%24X%24.%20In%20particular%2C%20%24R%5E2%24%20has%20a%20natural%20quotient%20space%2C%0Ain%20which%20%24X%24%20is%20organized%20into%20tracks%2C%20and%20each%20track%20is%20split%20into%20components.%0AThose%20components%20are%20in%20a%20order.%20Further%2C%20they%20can%20be%20represented%20by%20an%20integer%0Asequence.%20Components%20crossing%20tracks%20form%20branches%2C%20and%20the%20relationship%20can%20be%0Awell%20represented%20by%20a%20type%20of%20pseudotree%20%28called%20typed-II%20pseudotree%29.%20Such%0Astructures%20provide%20a%20platform%20for%20new%20algorithms%20for%20problems%20such%20as%0Acalculating%20convex%20hull%2C%20holes%2C%20clustering%20and%20anomaly%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTyped%2520Topological%2520Structures%2520Of%2520Datasets%26entry.906535625%3DWanjun%2520Hu%26entry.1292438233%3D%2520%2520A%2520datatset%2520%2524X%2524%2520on%2520%2524R%255E2%2524%2520is%2520a%2520finite%2520topological%2520space.%2520Current%2520research%2520of%2520a%250Adataset%2520focuses%2520on%2520statistical%2520methods%2520and%2520the%2520algebraic%2520topological%2520method%250A%255Ccite%257Bcarlsson%257D.%2520In%2520%255Ccite%257Bhu%257D%252C%2520the%2520concept%2520of%2520typed%2520topological%2520space%2520was%250Aintroduced%2520and%2520showed%2520to%2520have%2520the%2520potential%2520for%2520studying%2520finite%2520topological%250Aspaces%252C%2520such%2520as%2520a%2520dataset.%2520It%2520is%2520a%2520new%2520method%2520from%2520the%2520general%2520topology%250Aperspective.%2520A%2520typed%2520topological%2520space%2520is%2520a%2520topological%2520space%2520whose%2520open%2520sets%250Aare%2520assigned%2520types.%2520Topological%2520concepts%2520and%2520methods%2520can%2520be%2520redefined%2520using%250Aopen%2520sets%2520of%2520certain%2520types.%2520In%2520this%2520article%252C%2520we%2520develop%2520a%2520special%2520set%2520of%2520types%250Aand%2520its%2520related%2520typed%2520topology%2520on%2520a%2520dataset%2520%2524X%2524.%2520Using%2520it%252C%2520we%2520can%2520investigate%250Athe%2520inner%2520structure%2520of%2520%2524X%2524.%2520In%2520particular%252C%2520%2524R%255E2%2524%2520has%2520a%2520natural%2520quotient%2520space%252C%250Ain%2520which%2520%2524X%2524%2520is%2520organized%2520into%2520tracks%252C%2520and%2520each%2520track%2520is%2520split%2520into%2520components.%250AThose%2520components%2520are%2520in%2520a%2520order.%2520Further%252C%2520they%2520can%2520be%2520represented%2520by%2520an%2520integer%250Asequence.%2520Components%2520crossing%2520tracks%2520form%2520branches%252C%2520and%2520the%2520relationship%2520can%2520be%250Awell%2520represented%2520by%2520a%2520type%2520of%2520pseudotree%2520%2528called%2520typed-II%2520pseudotree%2529.%2520Such%250Astructures%2520provide%2520a%2520platform%2520for%2520new%2520algorithms%2520for%2520problems%2520such%2520as%250Acalculating%2520convex%2520hull%252C%2520holes%252C%2520clustering%2520and%2520anomaly%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Typed%20Topological%20Structures%20Of%20Datasets&entry.906535625=Wanjun%20Hu&entry.1292438233=%20%20A%20datatset%20%24X%24%20on%20%24R%5E2%24%20is%20a%20finite%20topological%20space.%20Current%20research%20of%20a%0Adataset%20focuses%20on%20statistical%20methods%20and%20the%20algebraic%20topological%20method%0A%5Ccite%7Bcarlsson%7D.%20In%20%5Ccite%7Bhu%7D%2C%20the%20concept%20of%20typed%20topological%20space%20was%0Aintroduced%20and%20showed%20to%20have%20the%20potential%20for%20studying%20finite%20topological%0Aspaces%2C%20such%20as%20a%20dataset.%20It%20is%20a%20new%20method%20from%20the%20general%20topology%0Aperspective.%20A%20typed%20topological%20space%20is%20a%20topological%20space%20whose%20open%20sets%0Aare%20assigned%20types.%20Topological%20concepts%20and%20methods%20can%20be%20redefined%20using%0Aopen%20sets%20of%20certain%20types.%20In%20this%20article%2C%20we%20develop%20a%20special%20set%20of%20types%0Aand%20its%20related%20typed%20topology%20on%20a%20dataset%20%24X%24.%20Using%20it%2C%20we%20can%20investigate%0Athe%20inner%20structure%20of%20%24X%24.%20In%20particular%2C%20%24R%5E2%24%20has%20a%20natural%20quotient%20space%2C%0Ain%20which%20%24X%24%20is%20organized%20into%20tracks%2C%20and%20each%20track%20is%20split%20into%20components.%0AThose%20components%20are%20in%20a%20order.%20Further%2C%20they%20can%20be%20represented%20by%20an%20integer%0Asequence.%20Components%20crossing%20tracks%20form%20branches%2C%20and%20the%20relationship%20can%20be%0Awell%20represented%20by%20a%20type%20of%20pseudotree%20%28called%20typed-II%20pseudotree%29.%20Such%0Astructures%20provide%20a%20platform%20for%20new%20algorithms%20for%20problems%20such%20as%0Acalculating%20convex%20hull%2C%20holes%2C%20clustering%20and%20anomaly%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14008v1&entry.124074799=Read"},
{"title": "Unsupervised Urban Tree Biodiversity Mapping from Street-Level Imagery\n  Using Spatially-Aware Visual Clustering", "author": "Diaa Addeen Abuhani and Marco Seccaroni and Martina Mazzarello and Imran Zualkernan and Fabio Duarte and Carlo Ratti", "abstract": "  Urban tree biodiversity is critical for climate resilience, ecological\nstability, and livability in cities, yet most municipalities lack detailed\nknowledge of their canopies. Field-based inventories provide reliable estimates\nof Shannon and Simpson diversity but are costly and time-consuming, while\nsupervised AI methods require labeled data that often fail to generalize across\nregions. We introduce an unsupervised clustering framework that integrates\nvisual embeddings from street-level imagery with spatial planting patterns to\nestimate biodiversity without labels. Applied to eight North American cities,\nthe method recovers genus-level diversity patterns with high fidelity,\nachieving low Wasserstein distances to ground truth for Shannon and Simpson\nindices and preserving spatial autocorrelation. This scalable, fine-grained\napproach enables biodiversity mapping in cities lacking detailed inventories\nand offers a pathway for continuous, low-cost monitoring to support equitable\naccess to greenery and adaptive management of urban ecosystems.\n", "link": "http://arxiv.org/abs/2508.13814v1", "date": "2025-08-19", "relevancy": 2.0205, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5264}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4914}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Urban%20Tree%20Biodiversity%20Mapping%20from%20Street-Level%20Imagery%0A%20%20Using%20Spatially-Aware%20Visual%20Clustering&body=Title%3A%20Unsupervised%20Urban%20Tree%20Biodiversity%20Mapping%20from%20Street-Level%20Imagery%0A%20%20Using%20Spatially-Aware%20Visual%20Clustering%0AAuthor%3A%20Diaa%20Addeen%20Abuhani%20and%20Marco%20Seccaroni%20and%20Martina%20Mazzarello%20and%20Imran%20Zualkernan%20and%20Fabio%20Duarte%20and%20Carlo%20Ratti%0AAbstract%3A%20%20%20Urban%20tree%20biodiversity%20is%20critical%20for%20climate%20resilience%2C%20ecological%0Astability%2C%20and%20livability%20in%20cities%2C%20yet%20most%20municipalities%20lack%20detailed%0Aknowledge%20of%20their%20canopies.%20Field-based%20inventories%20provide%20reliable%20estimates%0Aof%20Shannon%20and%20Simpson%20diversity%20but%20are%20costly%20and%20time-consuming%2C%20while%0Asupervised%20AI%20methods%20require%20labeled%20data%20that%20often%20fail%20to%20generalize%20across%0Aregions.%20We%20introduce%20an%20unsupervised%20clustering%20framework%20that%20integrates%0Avisual%20embeddings%20from%20street-level%20imagery%20with%20spatial%20planting%20patterns%20to%0Aestimate%20biodiversity%20without%20labels.%20Applied%20to%20eight%20North%20American%20cities%2C%0Athe%20method%20recovers%20genus-level%20diversity%20patterns%20with%20high%20fidelity%2C%0Aachieving%20low%20Wasserstein%20distances%20to%20ground%20truth%20for%20Shannon%20and%20Simpson%0Aindices%20and%20preserving%20spatial%20autocorrelation.%20This%20scalable%2C%20fine-grained%0Aapproach%20enables%20biodiversity%20mapping%20in%20cities%20lacking%20detailed%20inventories%0Aand%20offers%20a%20pathway%20for%20continuous%2C%20low-cost%20monitoring%20to%20support%20equitable%0Aaccess%20to%20greenery%20and%20adaptive%20management%20of%20urban%20ecosystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Urban%2520Tree%2520Biodiversity%2520Mapping%2520from%2520Street-Level%2520Imagery%250A%2520%2520Using%2520Spatially-Aware%2520Visual%2520Clustering%26entry.906535625%3DDiaa%2520Addeen%2520Abuhani%2520and%2520Marco%2520Seccaroni%2520and%2520Martina%2520Mazzarello%2520and%2520Imran%2520Zualkernan%2520and%2520Fabio%2520Duarte%2520and%2520Carlo%2520Ratti%26entry.1292438233%3D%2520%2520Urban%2520tree%2520biodiversity%2520is%2520critical%2520for%2520climate%2520resilience%252C%2520ecological%250Astability%252C%2520and%2520livability%2520in%2520cities%252C%2520yet%2520most%2520municipalities%2520lack%2520detailed%250Aknowledge%2520of%2520their%2520canopies.%2520Field-based%2520inventories%2520provide%2520reliable%2520estimates%250Aof%2520Shannon%2520and%2520Simpson%2520diversity%2520but%2520are%2520costly%2520and%2520time-consuming%252C%2520while%250Asupervised%2520AI%2520methods%2520require%2520labeled%2520data%2520that%2520often%2520fail%2520to%2520generalize%2520across%250Aregions.%2520We%2520introduce%2520an%2520unsupervised%2520clustering%2520framework%2520that%2520integrates%250Avisual%2520embeddings%2520from%2520street-level%2520imagery%2520with%2520spatial%2520planting%2520patterns%2520to%250Aestimate%2520biodiversity%2520without%2520labels.%2520Applied%2520to%2520eight%2520North%2520American%2520cities%252C%250Athe%2520method%2520recovers%2520genus-level%2520diversity%2520patterns%2520with%2520high%2520fidelity%252C%250Aachieving%2520low%2520Wasserstein%2520distances%2520to%2520ground%2520truth%2520for%2520Shannon%2520and%2520Simpson%250Aindices%2520and%2520preserving%2520spatial%2520autocorrelation.%2520This%2520scalable%252C%2520fine-grained%250Aapproach%2520enables%2520biodiversity%2520mapping%2520in%2520cities%2520lacking%2520detailed%2520inventories%250Aand%2520offers%2520a%2520pathway%2520for%2520continuous%252C%2520low-cost%2520monitoring%2520to%2520support%2520equitable%250Aaccess%2520to%2520greenery%2520and%2520adaptive%2520management%2520of%2520urban%2520ecosystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Urban%20Tree%20Biodiversity%20Mapping%20from%20Street-Level%20Imagery%0A%20%20Using%20Spatially-Aware%20Visual%20Clustering&entry.906535625=Diaa%20Addeen%20Abuhani%20and%20Marco%20Seccaroni%20and%20Martina%20Mazzarello%20and%20Imran%20Zualkernan%20and%20Fabio%20Duarte%20and%20Carlo%20Ratti&entry.1292438233=%20%20Urban%20tree%20biodiversity%20is%20critical%20for%20climate%20resilience%2C%20ecological%0Astability%2C%20and%20livability%20in%20cities%2C%20yet%20most%20municipalities%20lack%20detailed%0Aknowledge%20of%20their%20canopies.%20Field-based%20inventories%20provide%20reliable%20estimates%0Aof%20Shannon%20and%20Simpson%20diversity%20but%20are%20costly%20and%20time-consuming%2C%20while%0Asupervised%20AI%20methods%20require%20labeled%20data%20that%20often%20fail%20to%20generalize%20across%0Aregions.%20We%20introduce%20an%20unsupervised%20clustering%20framework%20that%20integrates%0Avisual%20embeddings%20from%20street-level%20imagery%20with%20spatial%20planting%20patterns%20to%0Aestimate%20biodiversity%20without%20labels.%20Applied%20to%20eight%20North%20American%20cities%2C%0Athe%20method%20recovers%20genus-level%20diversity%20patterns%20with%20high%20fidelity%2C%0Aachieving%20low%20Wasserstein%20distances%20to%20ground%20truth%20for%20Shannon%20and%20Simpson%0Aindices%20and%20preserving%20spatial%20autocorrelation.%20This%20scalable%2C%20fine-grained%0Aapproach%20enables%20biodiversity%20mapping%20in%20cities%20lacking%20detailed%20inventories%0Aand%20offers%20a%20pathway%20for%20continuous%2C%20low-cost%20monitoring%20to%20support%20equitable%0Aaccess%20to%20greenery%20and%20adaptive%20management%20of%20urban%20ecosystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13814v1&entry.124074799=Read"},
{"title": "UniECS: Unified Multimodal E-Commerce Search Framework with Gated\n  Cross-modal Fusion", "author": "Zihan Liang and Yufei Ma and ZhiPeng Qian and Huangyu Dai and Zihan Wang and Ben Chen and Chenyi Lei and Yuqing Ding and Han Li", "abstract": "  Current e-commerce multimodal retrieval systems face two key limitations:\nthey optimize for specific tasks with fixed modality pairings, and lack\ncomprehensive benchmarks for evaluating unified retrieval approaches. To\naddress these challenges, we introduce UniECS, a unified multimodal e-commerce\nsearch framework that handles all retrieval scenarios across image, text, and\ntheir combinations. Our work makes three key contributions. First, we propose a\nflexible architecture with a novel gated multimodal encoder that uses adaptive\nfusion mechanisms. This encoder integrates different modality representations\nwhile handling missing modalities. Second, we develop a comprehensive training\nstrategy to optimize learning. It combines cross-modal alignment loss (CMAL),\ncohesive local alignment loss (CLAL), intra-modal contrastive loss (IMCL), and\nadaptive loss weighting. Third, we create M-BEER, a carefully curated\nmultimodal benchmark containing 50K product pairs for e-commerce search\nevaluation. Extensive experiments demonstrate that UniECS consistently\noutperforms existing methods across four e-commerce benchmarks with fine-tuning\nor zero-shot evaluation. On our M-BEER bench, UniECS achieves substantial\nimprovements in cross-modal tasks (up to 28\\% gain in R@10 for text-to-image\nretrieval) while maintaining parameter efficiency (0.2B parameters) compared to\nlarger models like GME-Qwen2VL (2B) and MM-Embed (8B). Furthermore, we deploy\nUniECS in the e-commerce search platform of Kuaishou Inc. across two search\nscenarios, achieving notable improvements in Click-Through Rate (+2.74\\%) and\nRevenue (+8.33\\%). The comprehensive evaluation demonstrates the effectiveness\nof our approach in both experimental and real-world settings. Corresponding\ncodes, models and datasets will be made publicly available at\nhttps://github.com/qzp2018/UniECS.\n", "link": "http://arxiv.org/abs/2508.13843v1", "date": "2025-08-19", "relevancy": 2.0188, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5224}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniECS%3A%20Unified%20Multimodal%20E-Commerce%20Search%20Framework%20with%20Gated%0A%20%20Cross-modal%20Fusion&body=Title%3A%20UniECS%3A%20Unified%20Multimodal%20E-Commerce%20Search%20Framework%20with%20Gated%0A%20%20Cross-modal%20Fusion%0AAuthor%3A%20Zihan%20Liang%20and%20Yufei%20Ma%20and%20ZhiPeng%20Qian%20and%20Huangyu%20Dai%20and%20Zihan%20Wang%20and%20Ben%20Chen%20and%20Chenyi%20Lei%20and%20Yuqing%20Ding%20and%20Han%20Li%0AAbstract%3A%20%20%20Current%20e-commerce%20multimodal%20retrieval%20systems%20face%20two%20key%20limitations%3A%0Athey%20optimize%20for%20specific%20tasks%20with%20fixed%20modality%20pairings%2C%20and%20lack%0Acomprehensive%20benchmarks%20for%20evaluating%20unified%20retrieval%20approaches.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20UniECS%2C%20a%20unified%20multimodal%20e-commerce%0Asearch%20framework%20that%20handles%20all%20retrieval%20scenarios%20across%20image%2C%20text%2C%20and%0Atheir%20combinations.%20Our%20work%20makes%20three%20key%20contributions.%20First%2C%20we%20propose%20a%0Aflexible%20architecture%20with%20a%20novel%20gated%20multimodal%20encoder%20that%20uses%20adaptive%0Afusion%20mechanisms.%20This%20encoder%20integrates%20different%20modality%20representations%0Awhile%20handling%20missing%20modalities.%20Second%2C%20we%20develop%20a%20comprehensive%20training%0Astrategy%20to%20optimize%20learning.%20It%20combines%20cross-modal%20alignment%20loss%20%28CMAL%29%2C%0Acohesive%20local%20alignment%20loss%20%28CLAL%29%2C%20intra-modal%20contrastive%20loss%20%28IMCL%29%2C%20and%0Aadaptive%20loss%20weighting.%20Third%2C%20we%20create%20M-BEER%2C%20a%20carefully%20curated%0Amultimodal%20benchmark%20containing%2050K%20product%20pairs%20for%20e-commerce%20search%0Aevaluation.%20Extensive%20experiments%20demonstrate%20that%20UniECS%20consistently%0Aoutperforms%20existing%20methods%20across%20four%20e-commerce%20benchmarks%20with%20fine-tuning%0Aor%20zero-shot%20evaluation.%20On%20our%20M-BEER%20bench%2C%20UniECS%20achieves%20substantial%0Aimprovements%20in%20cross-modal%20tasks%20%28up%20to%2028%5C%25%20gain%20in%20R%4010%20for%20text-to-image%0Aretrieval%29%20while%20maintaining%20parameter%20efficiency%20%280.2B%20parameters%29%20compared%20to%0Alarger%20models%20like%20GME-Qwen2VL%20%282B%29%20and%20MM-Embed%20%288B%29.%20Furthermore%2C%20we%20deploy%0AUniECS%20in%20the%20e-commerce%20search%20platform%20of%20Kuaishou%20Inc.%20across%20two%20search%0Ascenarios%2C%20achieving%20notable%20improvements%20in%20Click-Through%20Rate%20%28%2B2.74%5C%25%29%20and%0ARevenue%20%28%2B8.33%5C%25%29.%20The%20comprehensive%20evaluation%20demonstrates%20the%20effectiveness%0Aof%20our%20approach%20in%20both%20experimental%20and%20real-world%20settings.%20Corresponding%0Acodes%2C%20models%20and%20datasets%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/qzp2018/UniECS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniECS%253A%2520Unified%2520Multimodal%2520E-Commerce%2520Search%2520Framework%2520with%2520Gated%250A%2520%2520Cross-modal%2520Fusion%26entry.906535625%3DZihan%2520Liang%2520and%2520Yufei%2520Ma%2520and%2520ZhiPeng%2520Qian%2520and%2520Huangyu%2520Dai%2520and%2520Zihan%2520Wang%2520and%2520Ben%2520Chen%2520and%2520Chenyi%2520Lei%2520and%2520Yuqing%2520Ding%2520and%2520Han%2520Li%26entry.1292438233%3D%2520%2520Current%2520e-commerce%2520multimodal%2520retrieval%2520systems%2520face%2520two%2520key%2520limitations%253A%250Athey%2520optimize%2520for%2520specific%2520tasks%2520with%2520fixed%2520modality%2520pairings%252C%2520and%2520lack%250Acomprehensive%2520benchmarks%2520for%2520evaluating%2520unified%2520retrieval%2520approaches.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520UniECS%252C%2520a%2520unified%2520multimodal%2520e-commerce%250Asearch%2520framework%2520that%2520handles%2520all%2520retrieval%2520scenarios%2520across%2520image%252C%2520text%252C%2520and%250Atheir%2520combinations.%2520Our%2520work%2520makes%2520three%2520key%2520contributions.%2520First%252C%2520we%2520propose%2520a%250Aflexible%2520architecture%2520with%2520a%2520novel%2520gated%2520multimodal%2520encoder%2520that%2520uses%2520adaptive%250Afusion%2520mechanisms.%2520This%2520encoder%2520integrates%2520different%2520modality%2520representations%250Awhile%2520handling%2520missing%2520modalities.%2520Second%252C%2520we%2520develop%2520a%2520comprehensive%2520training%250Astrategy%2520to%2520optimize%2520learning.%2520It%2520combines%2520cross-modal%2520alignment%2520loss%2520%2528CMAL%2529%252C%250Acohesive%2520local%2520alignment%2520loss%2520%2528CLAL%2529%252C%2520intra-modal%2520contrastive%2520loss%2520%2528IMCL%2529%252C%2520and%250Aadaptive%2520loss%2520weighting.%2520Third%252C%2520we%2520create%2520M-BEER%252C%2520a%2520carefully%2520curated%250Amultimodal%2520benchmark%2520containing%252050K%2520product%2520pairs%2520for%2520e-commerce%2520search%250Aevaluation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520UniECS%2520consistently%250Aoutperforms%2520existing%2520methods%2520across%2520four%2520e-commerce%2520benchmarks%2520with%2520fine-tuning%250Aor%2520zero-shot%2520evaluation.%2520On%2520our%2520M-BEER%2520bench%252C%2520UniECS%2520achieves%2520substantial%250Aimprovements%2520in%2520cross-modal%2520tasks%2520%2528up%2520to%252028%255C%2525%2520gain%2520in%2520R%254010%2520for%2520text-to-image%250Aretrieval%2529%2520while%2520maintaining%2520parameter%2520efficiency%2520%25280.2B%2520parameters%2529%2520compared%2520to%250Alarger%2520models%2520like%2520GME-Qwen2VL%2520%25282B%2529%2520and%2520MM-Embed%2520%25288B%2529.%2520Furthermore%252C%2520we%2520deploy%250AUniECS%2520in%2520the%2520e-commerce%2520search%2520platform%2520of%2520Kuaishou%2520Inc.%2520across%2520two%2520search%250Ascenarios%252C%2520achieving%2520notable%2520improvements%2520in%2520Click-Through%2520Rate%2520%2528%252B2.74%255C%2525%2529%2520and%250ARevenue%2520%2528%252B8.33%255C%2525%2529.%2520The%2520comprehensive%2520evaluation%2520demonstrates%2520the%2520effectiveness%250Aof%2520our%2520approach%2520in%2520both%2520experimental%2520and%2520real-world%2520settings.%2520Corresponding%250Acodes%252C%2520models%2520and%2520datasets%2520will%2520be%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/qzp2018/UniECS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniECS%3A%20Unified%20Multimodal%20E-Commerce%20Search%20Framework%20with%20Gated%0A%20%20Cross-modal%20Fusion&entry.906535625=Zihan%20Liang%20and%20Yufei%20Ma%20and%20ZhiPeng%20Qian%20and%20Huangyu%20Dai%20and%20Zihan%20Wang%20and%20Ben%20Chen%20and%20Chenyi%20Lei%20and%20Yuqing%20Ding%20and%20Han%20Li&entry.1292438233=%20%20Current%20e-commerce%20multimodal%20retrieval%20systems%20face%20two%20key%20limitations%3A%0Athey%20optimize%20for%20specific%20tasks%20with%20fixed%20modality%20pairings%2C%20and%20lack%0Acomprehensive%20benchmarks%20for%20evaluating%20unified%20retrieval%20approaches.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20UniECS%2C%20a%20unified%20multimodal%20e-commerce%0Asearch%20framework%20that%20handles%20all%20retrieval%20scenarios%20across%20image%2C%20text%2C%20and%0Atheir%20combinations.%20Our%20work%20makes%20three%20key%20contributions.%20First%2C%20we%20propose%20a%0Aflexible%20architecture%20with%20a%20novel%20gated%20multimodal%20encoder%20that%20uses%20adaptive%0Afusion%20mechanisms.%20This%20encoder%20integrates%20different%20modality%20representations%0Awhile%20handling%20missing%20modalities.%20Second%2C%20we%20develop%20a%20comprehensive%20training%0Astrategy%20to%20optimize%20learning.%20It%20combines%20cross-modal%20alignment%20loss%20%28CMAL%29%2C%0Acohesive%20local%20alignment%20loss%20%28CLAL%29%2C%20intra-modal%20contrastive%20loss%20%28IMCL%29%2C%20and%0Aadaptive%20loss%20weighting.%20Third%2C%20we%20create%20M-BEER%2C%20a%20carefully%20curated%0Amultimodal%20benchmark%20containing%2050K%20product%20pairs%20for%20e-commerce%20search%0Aevaluation.%20Extensive%20experiments%20demonstrate%20that%20UniECS%20consistently%0Aoutperforms%20existing%20methods%20across%20four%20e-commerce%20benchmarks%20with%20fine-tuning%0Aor%20zero-shot%20evaluation.%20On%20our%20M-BEER%20bench%2C%20UniECS%20achieves%20substantial%0Aimprovements%20in%20cross-modal%20tasks%20%28up%20to%2028%5C%25%20gain%20in%20R%4010%20for%20text-to-image%0Aretrieval%29%20while%20maintaining%20parameter%20efficiency%20%280.2B%20parameters%29%20compared%20to%0Alarger%20models%20like%20GME-Qwen2VL%20%282B%29%20and%20MM-Embed%20%288B%29.%20Furthermore%2C%20we%20deploy%0AUniECS%20in%20the%20e-commerce%20search%20platform%20of%20Kuaishou%20Inc.%20across%20two%20search%0Ascenarios%2C%20achieving%20notable%20improvements%20in%20Click-Through%20Rate%20%28%2B2.74%5C%25%29%20and%0ARevenue%20%28%2B8.33%5C%25%29.%20The%20comprehensive%20evaluation%20demonstrates%20the%20effectiveness%0Aof%20our%20approach%20in%20both%20experimental%20and%20real-world%20settings.%20Corresponding%0Acodes%2C%20models%20and%20datasets%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/qzp2018/UniECS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13843v1&entry.124074799=Read"},
{"title": "Expertise-aware Multi-LLM Recruitment and Collaboration for Medical\n  Decision-Making", "author": "Liuxin Bao and Zhihao Peng and Xiaofei Zhou and Runmin Cong and Jiyong Zhang and Yixuan Yuan", "abstract": "  Medical Decision-Making (MDM) is a complex process requiring substantial\ndomain-specific expertise to effectively synthesize heterogeneous and\ncomplicated clinical information. While recent advancements in Large Language\nModels (LLMs) show promise in supporting MDM, single-LLM approaches are limited\nby their parametric knowledge constraints and static training corpora, failing\nto robustly integrate the clinical information. To address this challenge, we\npropose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)\nframework to enhance the accuracy and reliability of MDM systems. It operates\nin two stages: (i) expertise-aware agent recruitment and (ii) confidence- and\nadversarial-driven multi-agent collaboration. Specifically, in the first stage,\nwe use a publicly available corpus to construct an LLM expertise table for\ncapturing expertise-specific strengths of multiple LLMs across medical\ndepartment categories and query difficulty levels. This table enables the\nsubsequent dynamic selection of the optimal LLMs to act as medical expert\nagents for each medical query during the inference phase. In the second stage,\nwe employ selected agents to generate responses with self-assessed confidence\nscores, which are then integrated through the confidence fusion and adversarial\nvalidation to improve diagnostic reliability. We evaluate our EMRC framework on\nthree public MDM datasets, where the results demonstrate that our EMRC\noutperforms state-of-the-art single- and multi-LLM methods, achieving superior\ndiagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC\nachieves 74.45% accuracy, representing a 2.69% improvement over the\nbest-performing closed-source model GPT- 4-0613, which demonstrates the\neffectiveness of our expertise-aware agent recruitment strategy and the agent\ncomplementarity in leveraging each LLM's specialized capabilities.\n", "link": "http://arxiv.org/abs/2508.13754v1", "date": "2025-08-19", "relevancy": 2.0147, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5892}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expertise-aware%20Multi-LLM%20Recruitment%20and%20Collaboration%20for%20Medical%0A%20%20Decision-Making&body=Title%3A%20Expertise-aware%20Multi-LLM%20Recruitment%20and%20Collaboration%20for%20Medical%0A%20%20Decision-Making%0AAuthor%3A%20Liuxin%20Bao%20and%20Zhihao%20Peng%20and%20Xiaofei%20Zhou%20and%20Runmin%20Cong%20and%20Jiyong%20Zhang%20and%20Yixuan%20Yuan%0AAbstract%3A%20%20%20Medical%20Decision-Making%20%28MDM%29%20is%20a%20complex%20process%20requiring%20substantial%0Adomain-specific%20expertise%20to%20effectively%20synthesize%20heterogeneous%20and%0Acomplicated%20clinical%20information.%20While%20recent%20advancements%20in%20Large%20Language%0AModels%20%28LLMs%29%20show%20promise%20in%20supporting%20MDM%2C%20single-LLM%20approaches%20are%20limited%0Aby%20their%20parametric%20knowledge%20constraints%20and%20static%20training%20corpora%2C%20failing%0Ato%20robustly%20integrate%20the%20clinical%20information.%20To%20address%20this%20challenge%2C%20we%0Apropose%20the%20Expertise-aware%20Multi-LLM%20Recruitment%20and%20Collaboration%20%28EMRC%29%0Aframework%20to%20enhance%20the%20accuracy%20and%20reliability%20of%20MDM%20systems.%20It%20operates%0Ain%20two%20stages%3A%20%28i%29%20expertise-aware%20agent%20recruitment%20and%20%28ii%29%20confidence-%20and%0Aadversarial-driven%20multi-agent%20collaboration.%20Specifically%2C%20in%20the%20first%20stage%2C%0Awe%20use%20a%20publicly%20available%20corpus%20to%20construct%20an%20LLM%20expertise%20table%20for%0Acapturing%20expertise-specific%20strengths%20of%20multiple%20LLMs%20across%20medical%0Adepartment%20categories%20and%20query%20difficulty%20levels.%20This%20table%20enables%20the%0Asubsequent%20dynamic%20selection%20of%20the%20optimal%20LLMs%20to%20act%20as%20medical%20expert%0Aagents%20for%20each%20medical%20query%20during%20the%20inference%20phase.%20In%20the%20second%20stage%2C%0Awe%20employ%20selected%20agents%20to%20generate%20responses%20with%20self-assessed%20confidence%0Ascores%2C%20which%20are%20then%20integrated%20through%20the%20confidence%20fusion%20and%20adversarial%0Avalidation%20to%20improve%20diagnostic%20reliability.%20We%20evaluate%20our%20EMRC%20framework%20on%0Athree%20public%20MDM%20datasets%2C%20where%20the%20results%20demonstrate%20that%20our%20EMRC%0Aoutperforms%20state-of-the-art%20single-%20and%20multi-LLM%20methods%2C%20achieving%20superior%0Adiagnostic%20performance.%20For%20instance%2C%20on%20the%20MMLU-Pro-Health%20dataset%2C%20our%20EMRC%0Aachieves%2074.45%25%20accuracy%2C%20representing%20a%202.69%25%20improvement%20over%20the%0Abest-performing%20closed-source%20model%20GPT-%204-0613%2C%20which%20demonstrates%20the%0Aeffectiveness%20of%20our%20expertise-aware%20agent%20recruitment%20strategy%20and%20the%20agent%0Acomplementarity%20in%20leveraging%20each%20LLM%27s%20specialized%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpertise-aware%2520Multi-LLM%2520Recruitment%2520and%2520Collaboration%2520for%2520Medical%250A%2520%2520Decision-Making%26entry.906535625%3DLiuxin%2520Bao%2520and%2520Zhihao%2520Peng%2520and%2520Xiaofei%2520Zhou%2520and%2520Runmin%2520Cong%2520and%2520Jiyong%2520Zhang%2520and%2520Yixuan%2520Yuan%26entry.1292438233%3D%2520%2520Medical%2520Decision-Making%2520%2528MDM%2529%2520is%2520a%2520complex%2520process%2520requiring%2520substantial%250Adomain-specific%2520expertise%2520to%2520effectively%2520synthesize%2520heterogeneous%2520and%250Acomplicated%2520clinical%2520information.%2520While%2520recent%2520advancements%2520in%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520show%2520promise%2520in%2520supporting%2520MDM%252C%2520single-LLM%2520approaches%2520are%2520limited%250Aby%2520their%2520parametric%2520knowledge%2520constraints%2520and%2520static%2520training%2520corpora%252C%2520failing%250Ato%2520robustly%2520integrate%2520the%2520clinical%2520information.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apropose%2520the%2520Expertise-aware%2520Multi-LLM%2520Recruitment%2520and%2520Collaboration%2520%2528EMRC%2529%250Aframework%2520to%2520enhance%2520the%2520accuracy%2520and%2520reliability%2520of%2520MDM%2520systems.%2520It%2520operates%250Ain%2520two%2520stages%253A%2520%2528i%2529%2520expertise-aware%2520agent%2520recruitment%2520and%2520%2528ii%2529%2520confidence-%2520and%250Aadversarial-driven%2520multi-agent%2520collaboration.%2520Specifically%252C%2520in%2520the%2520first%2520stage%252C%250Awe%2520use%2520a%2520publicly%2520available%2520corpus%2520to%2520construct%2520an%2520LLM%2520expertise%2520table%2520for%250Acapturing%2520expertise-specific%2520strengths%2520of%2520multiple%2520LLMs%2520across%2520medical%250Adepartment%2520categories%2520and%2520query%2520difficulty%2520levels.%2520This%2520table%2520enables%2520the%250Asubsequent%2520dynamic%2520selection%2520of%2520the%2520optimal%2520LLMs%2520to%2520act%2520as%2520medical%2520expert%250Aagents%2520for%2520each%2520medical%2520query%2520during%2520the%2520inference%2520phase.%2520In%2520the%2520second%2520stage%252C%250Awe%2520employ%2520selected%2520agents%2520to%2520generate%2520responses%2520with%2520self-assessed%2520confidence%250Ascores%252C%2520which%2520are%2520then%2520integrated%2520through%2520the%2520confidence%2520fusion%2520and%2520adversarial%250Avalidation%2520to%2520improve%2520diagnostic%2520reliability.%2520We%2520evaluate%2520our%2520EMRC%2520framework%2520on%250Athree%2520public%2520MDM%2520datasets%252C%2520where%2520the%2520results%2520demonstrate%2520that%2520our%2520EMRC%250Aoutperforms%2520state-of-the-art%2520single-%2520and%2520multi-LLM%2520methods%252C%2520achieving%2520superior%250Adiagnostic%2520performance.%2520For%2520instance%252C%2520on%2520the%2520MMLU-Pro-Health%2520dataset%252C%2520our%2520EMRC%250Aachieves%252074.45%2525%2520accuracy%252C%2520representing%2520a%25202.69%2525%2520improvement%2520over%2520the%250Abest-performing%2520closed-source%2520model%2520GPT-%25204-0613%252C%2520which%2520demonstrates%2520the%250Aeffectiveness%2520of%2520our%2520expertise-aware%2520agent%2520recruitment%2520strategy%2520and%2520the%2520agent%250Acomplementarity%2520in%2520leveraging%2520each%2520LLM%2527s%2520specialized%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expertise-aware%20Multi-LLM%20Recruitment%20and%20Collaboration%20for%20Medical%0A%20%20Decision-Making&entry.906535625=Liuxin%20Bao%20and%20Zhihao%20Peng%20and%20Xiaofei%20Zhou%20and%20Runmin%20Cong%20and%20Jiyong%20Zhang%20and%20Yixuan%20Yuan&entry.1292438233=%20%20Medical%20Decision-Making%20%28MDM%29%20is%20a%20complex%20process%20requiring%20substantial%0Adomain-specific%20expertise%20to%20effectively%20synthesize%20heterogeneous%20and%0Acomplicated%20clinical%20information.%20While%20recent%20advancements%20in%20Large%20Language%0AModels%20%28LLMs%29%20show%20promise%20in%20supporting%20MDM%2C%20single-LLM%20approaches%20are%20limited%0Aby%20their%20parametric%20knowledge%20constraints%20and%20static%20training%20corpora%2C%20failing%0Ato%20robustly%20integrate%20the%20clinical%20information.%20To%20address%20this%20challenge%2C%20we%0Apropose%20the%20Expertise-aware%20Multi-LLM%20Recruitment%20and%20Collaboration%20%28EMRC%29%0Aframework%20to%20enhance%20the%20accuracy%20and%20reliability%20of%20MDM%20systems.%20It%20operates%0Ain%20two%20stages%3A%20%28i%29%20expertise-aware%20agent%20recruitment%20and%20%28ii%29%20confidence-%20and%0Aadversarial-driven%20multi-agent%20collaboration.%20Specifically%2C%20in%20the%20first%20stage%2C%0Awe%20use%20a%20publicly%20available%20corpus%20to%20construct%20an%20LLM%20expertise%20table%20for%0Acapturing%20expertise-specific%20strengths%20of%20multiple%20LLMs%20across%20medical%0Adepartment%20categories%20and%20query%20difficulty%20levels.%20This%20table%20enables%20the%0Asubsequent%20dynamic%20selection%20of%20the%20optimal%20LLMs%20to%20act%20as%20medical%20expert%0Aagents%20for%20each%20medical%20query%20during%20the%20inference%20phase.%20In%20the%20second%20stage%2C%0Awe%20employ%20selected%20agents%20to%20generate%20responses%20with%20self-assessed%20confidence%0Ascores%2C%20which%20are%20then%20integrated%20through%20the%20confidence%20fusion%20and%20adversarial%0Avalidation%20to%20improve%20diagnostic%20reliability.%20We%20evaluate%20our%20EMRC%20framework%20on%0Athree%20public%20MDM%20datasets%2C%20where%20the%20results%20demonstrate%20that%20our%20EMRC%0Aoutperforms%20state-of-the-art%20single-%20and%20multi-LLM%20methods%2C%20achieving%20superior%0Adiagnostic%20performance.%20For%20instance%2C%20on%20the%20MMLU-Pro-Health%20dataset%2C%20our%20EMRC%0Aachieves%2074.45%25%20accuracy%2C%20representing%20a%202.69%25%20improvement%20over%20the%0Abest-performing%20closed-source%20model%20GPT-%204-0613%2C%20which%20demonstrates%20the%0Aeffectiveness%20of%20our%20expertise-aware%20agent%20recruitment%20strategy%20and%20the%20agent%0Acomplementarity%20in%20leveraging%20each%20LLM%27s%20specialized%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13754v1&entry.124074799=Read"},
{"title": "Active Learning of Mealy Machines with Timers", "author": "V\u00e9ronique Bruy\u00e8re and Bharat Garhewal and Guillermo A. P\u00e9rez and Ga\u00ebtan Staquet and Frits W. Vaandrager", "abstract": "  We present the first algorithm for query learning Mealy machines with timers\nin a black-box context. Our algorithm is an extension of the L# algorithm of\nVaandrager et al. to a timed setting. We rely on symbolic queries which empower\nus to reason on untimed executions while learning. Similarly to the algorithm\nfor learning timed automata of Waga, these symbolic queries can be realized\nusing finitely many concrete queries. Experiments with a prototype\nimplementation show that our algorithm is able to efficiently learn realistic\nbenchmarks.\n", "link": "http://arxiv.org/abs/2403.02019v3", "date": "2025-08-19", "relevancy": 2.0125, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4047}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4032}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.3996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Learning%20of%20Mealy%20Machines%20with%20Timers&body=Title%3A%20Active%20Learning%20of%20Mealy%20Machines%20with%20Timers%0AAuthor%3A%20V%C3%A9ronique%20Bruy%C3%A8re%20and%20Bharat%20Garhewal%20and%20Guillermo%20A.%20P%C3%A9rez%20and%20Ga%C3%ABtan%20Staquet%20and%20Frits%20W.%20Vaandrager%0AAbstract%3A%20%20%20We%20present%20the%20first%20algorithm%20for%20query%20learning%20Mealy%20machines%20with%20timers%0Ain%20a%20black-box%20context.%20Our%20algorithm%20is%20an%20extension%20of%20the%20L%23%20algorithm%20of%0AVaandrager%20et%20al.%20to%20a%20timed%20setting.%20We%20rely%20on%20symbolic%20queries%20which%20empower%0Aus%20to%20reason%20on%20untimed%20executions%20while%20learning.%20Similarly%20to%20the%20algorithm%0Afor%20learning%20timed%20automata%20of%20Waga%2C%20these%20symbolic%20queries%20can%20be%20realized%0Ausing%20finitely%20many%20concrete%20queries.%20Experiments%20with%20a%20prototype%0Aimplementation%20show%20that%20our%20algorithm%20is%20able%20to%20efficiently%20learn%20realistic%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02019v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Learning%2520of%2520Mealy%2520Machines%2520with%2520Timers%26entry.906535625%3DV%25C3%25A9ronique%2520Bruy%25C3%25A8re%2520and%2520Bharat%2520Garhewal%2520and%2520Guillermo%2520A.%2520P%25C3%25A9rez%2520and%2520Ga%25C3%25ABtan%2520Staquet%2520and%2520Frits%2520W.%2520Vaandrager%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520first%2520algorithm%2520for%2520query%2520learning%2520Mealy%2520machines%2520with%2520timers%250Ain%2520a%2520black-box%2520context.%2520Our%2520algorithm%2520is%2520an%2520extension%2520of%2520the%2520L%2523%2520algorithm%2520of%250AVaandrager%2520et%2520al.%2520to%2520a%2520timed%2520setting.%2520We%2520rely%2520on%2520symbolic%2520queries%2520which%2520empower%250Aus%2520to%2520reason%2520on%2520untimed%2520executions%2520while%2520learning.%2520Similarly%2520to%2520the%2520algorithm%250Afor%2520learning%2520timed%2520automata%2520of%2520Waga%252C%2520these%2520symbolic%2520queries%2520can%2520be%2520realized%250Ausing%2520finitely%2520many%2520concrete%2520queries.%2520Experiments%2520with%2520a%2520prototype%250Aimplementation%2520show%2520that%2520our%2520algorithm%2520is%2520able%2520to%2520efficiently%2520learn%2520realistic%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02019v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Learning%20of%20Mealy%20Machines%20with%20Timers&entry.906535625=V%C3%A9ronique%20Bruy%C3%A8re%20and%20Bharat%20Garhewal%20and%20Guillermo%20A.%20P%C3%A9rez%20and%20Ga%C3%ABtan%20Staquet%20and%20Frits%20W.%20Vaandrager&entry.1292438233=%20%20We%20present%20the%20first%20algorithm%20for%20query%20learning%20Mealy%20machines%20with%20timers%0Ain%20a%20black-box%20context.%20Our%20algorithm%20is%20an%20extension%20of%20the%20L%23%20algorithm%20of%0AVaandrager%20et%20al.%20to%20a%20timed%20setting.%20We%20rely%20on%20symbolic%20queries%20which%20empower%0Aus%20to%20reason%20on%20untimed%20executions%20while%20learning.%20Similarly%20to%20the%20algorithm%0Afor%20learning%20timed%20automata%20of%20Waga%2C%20these%20symbolic%20queries%20can%20be%20realized%0Ausing%20finitely%20many%20concrete%20queries.%20Experiments%20with%20a%20prototype%0Aimplementation%20show%20that%20our%20algorithm%20is%20able%20to%20efficiently%20learn%20realistic%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02019v3&entry.124074799=Read"},
{"title": "OmViD: Omni-supervised active learning for video action detection", "author": "Aayush Rana and Akash Kumar and Vibhav Vineet and Yogesh S Rawat", "abstract": "  Video action detection requires dense spatio-temporal annotations, which are\nboth challenging and expensive to obtain. However, real-world videos often vary\nin difficulty and may not require the same level of annotation. This paper\nanalyzes the appropriate annotation types for each sample and their impact on\nspatio-temporal video action detection. It focuses on two key aspects: 1) how\nto obtain varying levels of annotation for videos, and 2) how to learn action\ndetection from different annotation types. The study explores video-level tags,\npoints, scribbles, bounding boxes, and pixel-level masks. First, a simple\nactive learning strategy is proposed to estimate the necessary annotation type\nfor each video. Then, a novel spatio-temporal 3D-superpixel approach is\nintroduced to generate pseudo-labels from these annotations, enabling effective\ntraining. The approach is validated on UCF101-24 and JHMDB-21 datasets,\nsignificantly cutting annotation costs with minimal performance loss.\n", "link": "http://arxiv.org/abs/2508.13983v1", "date": "2025-08-19", "relevancy": 1.7839, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6258}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5629}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmViD%3A%20Omni-supervised%20active%20learning%20for%20video%20action%20detection&body=Title%3A%20OmViD%3A%20Omni-supervised%20active%20learning%20for%20video%20action%20detection%0AAuthor%3A%20Aayush%20Rana%20and%20Akash%20Kumar%20and%20Vibhav%20Vineet%20and%20Yogesh%20S%20Rawat%0AAbstract%3A%20%20%20Video%20action%20detection%20requires%20dense%20spatio-temporal%20annotations%2C%20which%20are%0Aboth%20challenging%20and%20expensive%20to%20obtain.%20However%2C%20real-world%20videos%20often%20vary%0Ain%20difficulty%20and%20may%20not%20require%20the%20same%20level%20of%20annotation.%20This%20paper%0Aanalyzes%20the%20appropriate%20annotation%20types%20for%20each%20sample%20and%20their%20impact%20on%0Aspatio-temporal%20video%20action%20detection.%20It%20focuses%20on%20two%20key%20aspects%3A%201%29%20how%0Ato%20obtain%20varying%20levels%20of%20annotation%20for%20videos%2C%20and%202%29%20how%20to%20learn%20action%0Adetection%20from%20different%20annotation%20types.%20The%20study%20explores%20video-level%20tags%2C%0Apoints%2C%20scribbles%2C%20bounding%20boxes%2C%20and%20pixel-level%20masks.%20First%2C%20a%20simple%0Aactive%20learning%20strategy%20is%20proposed%20to%20estimate%20the%20necessary%20annotation%20type%0Afor%20each%20video.%20Then%2C%20a%20novel%20spatio-temporal%203D-superpixel%20approach%20is%0Aintroduced%20to%20generate%20pseudo-labels%20from%20these%20annotations%2C%20enabling%20effective%0Atraining.%20The%20approach%20is%20validated%20on%20UCF101-24%20and%20JHMDB-21%20datasets%2C%0Asignificantly%20cutting%20annotation%20costs%20with%20minimal%20performance%20loss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmViD%253A%2520Omni-supervised%2520active%2520learning%2520for%2520video%2520action%2520detection%26entry.906535625%3DAayush%2520Rana%2520and%2520Akash%2520Kumar%2520and%2520Vibhav%2520Vineet%2520and%2520Yogesh%2520S%2520Rawat%26entry.1292438233%3D%2520%2520Video%2520action%2520detection%2520requires%2520dense%2520spatio-temporal%2520annotations%252C%2520which%2520are%250Aboth%2520challenging%2520and%2520expensive%2520to%2520obtain.%2520However%252C%2520real-world%2520videos%2520often%2520vary%250Ain%2520difficulty%2520and%2520may%2520not%2520require%2520the%2520same%2520level%2520of%2520annotation.%2520This%2520paper%250Aanalyzes%2520the%2520appropriate%2520annotation%2520types%2520for%2520each%2520sample%2520and%2520their%2520impact%2520on%250Aspatio-temporal%2520video%2520action%2520detection.%2520It%2520focuses%2520on%2520two%2520key%2520aspects%253A%25201%2529%2520how%250Ato%2520obtain%2520varying%2520levels%2520of%2520annotation%2520for%2520videos%252C%2520and%25202%2529%2520how%2520to%2520learn%2520action%250Adetection%2520from%2520different%2520annotation%2520types.%2520The%2520study%2520explores%2520video-level%2520tags%252C%250Apoints%252C%2520scribbles%252C%2520bounding%2520boxes%252C%2520and%2520pixel-level%2520masks.%2520First%252C%2520a%2520simple%250Aactive%2520learning%2520strategy%2520is%2520proposed%2520to%2520estimate%2520the%2520necessary%2520annotation%2520type%250Afor%2520each%2520video.%2520Then%252C%2520a%2520novel%2520spatio-temporal%25203D-superpixel%2520approach%2520is%250Aintroduced%2520to%2520generate%2520pseudo-labels%2520from%2520these%2520annotations%252C%2520enabling%2520effective%250Atraining.%2520The%2520approach%2520is%2520validated%2520on%2520UCF101-24%2520and%2520JHMDB-21%2520datasets%252C%250Asignificantly%2520cutting%2520annotation%2520costs%2520with%2520minimal%2520performance%2520loss.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmViD%3A%20Omni-supervised%20active%20learning%20for%20video%20action%20detection&entry.906535625=Aayush%20Rana%20and%20Akash%20Kumar%20and%20Vibhav%20Vineet%20and%20Yogesh%20S%20Rawat&entry.1292438233=%20%20Video%20action%20detection%20requires%20dense%20spatio-temporal%20annotations%2C%20which%20are%0Aboth%20challenging%20and%20expensive%20to%20obtain.%20However%2C%20real-world%20videos%20often%20vary%0Ain%20difficulty%20and%20may%20not%20require%20the%20same%20level%20of%20annotation.%20This%20paper%0Aanalyzes%20the%20appropriate%20annotation%20types%20for%20each%20sample%20and%20their%20impact%20on%0Aspatio-temporal%20video%20action%20detection.%20It%20focuses%20on%20two%20key%20aspects%3A%201%29%20how%0Ato%20obtain%20varying%20levels%20of%20annotation%20for%20videos%2C%20and%202%29%20how%20to%20learn%20action%0Adetection%20from%20different%20annotation%20types.%20The%20study%20explores%20video-level%20tags%2C%0Apoints%2C%20scribbles%2C%20bounding%20boxes%2C%20and%20pixel-level%20masks.%20First%2C%20a%20simple%0Aactive%20learning%20strategy%20is%20proposed%20to%20estimate%20the%20necessary%20annotation%20type%0Afor%20each%20video.%20Then%2C%20a%20novel%20spatio-temporal%203D-superpixel%20approach%20is%0Aintroduced%20to%20generate%20pseudo-labels%20from%20these%20annotations%2C%20enabling%20effective%0Atraining.%20The%20approach%20is%20validated%20on%20UCF101-24%20and%20JHMDB-21%20datasets%2C%0Asignificantly%20cutting%20annotation%20costs%20with%20minimal%20performance%20loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13983v1&entry.124074799=Read"},
{"title": "Penalizing Infeasible Actions and Reward Scaling in Reinforcement\n  Learning with Offline Data", "author": "Jeonghye Kim and Yongjae Shin and Whiyoung Jung and Sunghoon Hong and Deunsol Yoon and Youngchul Sung and Kanghoon Lee and Woohyung Lim", "abstract": "  Reinforcement learning with offline data suffers from Q-value extrapolation\nerrors. To address this issue, we first demonstrate that linear extrapolation\nof the Q-function beyond the data range is particularly problematic. To\nmitigate this, we propose guiding the gradual decrease of Q-values outside the\ndata range, which is achieved through reward scaling with layer normalization\n(RS-LN) and a penalization mechanism for infeasible actions (PA). By combining\nRS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a\nrange of tasks, demonstrating superior performance compared to state-of-the-art\nalgorithms in both offline training and online fine-tuning on the D4RL\nbenchmark, with notable success in the challenging AntMaze Ultra task.\n", "link": "http://arxiv.org/abs/2507.08761v2", "date": "2025-08-19", "relevancy": 1.384, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4802}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4564}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Penalizing%20Infeasible%20Actions%20and%20Reward%20Scaling%20in%20Reinforcement%0A%20%20Learning%20with%20Offline%20Data&body=Title%3A%20Penalizing%20Infeasible%20Actions%20and%20Reward%20Scaling%20in%20Reinforcement%0A%20%20Learning%20with%20Offline%20Data%0AAuthor%3A%20Jeonghye%20Kim%20and%20Yongjae%20Shin%20and%20Whiyoung%20Jung%20and%20Sunghoon%20Hong%20and%20Deunsol%20Yoon%20and%20Youngchul%20Sung%20and%20Kanghoon%20Lee%20and%20Woohyung%20Lim%0AAbstract%3A%20%20%20Reinforcement%20learning%20with%20offline%20data%20suffers%20from%20Q-value%20extrapolation%0Aerrors.%20To%20address%20this%20issue%2C%20we%20first%20demonstrate%20that%20linear%20extrapolation%0Aof%20the%20Q-function%20beyond%20the%20data%20range%20is%20particularly%20problematic.%20To%0Amitigate%20this%2C%20we%20propose%20guiding%20the%20gradual%20decrease%20of%20Q-values%20outside%20the%0Adata%20range%2C%20which%20is%20achieved%20through%20reward%20scaling%20with%20layer%20normalization%0A%28RS-LN%29%20and%20a%20penalization%20mechanism%20for%20infeasible%20actions%20%28PA%29.%20By%20combining%0ARS-LN%20and%20PA%2C%20we%20develop%20a%20new%20algorithm%20called%20PARS.%20We%20evaluate%20PARS%20across%20a%0Arange%20of%20tasks%2C%20demonstrating%20superior%20performance%20compared%20to%20state-of-the-art%0Aalgorithms%20in%20both%20offline%20training%20and%20online%20fine-tuning%20on%20the%20D4RL%0Abenchmark%2C%20with%20notable%20success%20in%20the%20challenging%20AntMaze%20Ultra%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08761v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPenalizing%2520Infeasible%2520Actions%2520and%2520Reward%2520Scaling%2520in%2520Reinforcement%250A%2520%2520Learning%2520with%2520Offline%2520Data%26entry.906535625%3DJeonghye%2520Kim%2520and%2520Yongjae%2520Shin%2520and%2520Whiyoung%2520Jung%2520and%2520Sunghoon%2520Hong%2520and%2520Deunsol%2520Yoon%2520and%2520Youngchul%2520Sung%2520and%2520Kanghoon%2520Lee%2520and%2520Woohyung%2520Lim%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520with%2520offline%2520data%2520suffers%2520from%2520Q-value%2520extrapolation%250Aerrors.%2520To%2520address%2520this%2520issue%252C%2520we%2520first%2520demonstrate%2520that%2520linear%2520extrapolation%250Aof%2520the%2520Q-function%2520beyond%2520the%2520data%2520range%2520is%2520particularly%2520problematic.%2520To%250Amitigate%2520this%252C%2520we%2520propose%2520guiding%2520the%2520gradual%2520decrease%2520of%2520Q-values%2520outside%2520the%250Adata%2520range%252C%2520which%2520is%2520achieved%2520through%2520reward%2520scaling%2520with%2520layer%2520normalization%250A%2528RS-LN%2529%2520and%2520a%2520penalization%2520mechanism%2520for%2520infeasible%2520actions%2520%2528PA%2529.%2520By%2520combining%250ARS-LN%2520and%2520PA%252C%2520we%2520develop%2520a%2520new%2520algorithm%2520called%2520PARS.%2520We%2520evaluate%2520PARS%2520across%2520a%250Arange%2520of%2520tasks%252C%2520demonstrating%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%250Aalgorithms%2520in%2520both%2520offline%2520training%2520and%2520online%2520fine-tuning%2520on%2520the%2520D4RL%250Abenchmark%252C%2520with%2520notable%2520success%2520in%2520the%2520challenging%2520AntMaze%2520Ultra%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08761v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Penalizing%20Infeasible%20Actions%20and%20Reward%20Scaling%20in%20Reinforcement%0A%20%20Learning%20with%20Offline%20Data&entry.906535625=Jeonghye%20Kim%20and%20Yongjae%20Shin%20and%20Whiyoung%20Jung%20and%20Sunghoon%20Hong%20and%20Deunsol%20Yoon%20and%20Youngchul%20Sung%20and%20Kanghoon%20Lee%20and%20Woohyung%20Lim&entry.1292438233=%20%20Reinforcement%20learning%20with%20offline%20data%20suffers%20from%20Q-value%20extrapolation%0Aerrors.%20To%20address%20this%20issue%2C%20we%20first%20demonstrate%20that%20linear%20extrapolation%0Aof%20the%20Q-function%20beyond%20the%20data%20range%20is%20particularly%20problematic.%20To%0Amitigate%20this%2C%20we%20propose%20guiding%20the%20gradual%20decrease%20of%20Q-values%20outside%20the%0Adata%20range%2C%20which%20is%20achieved%20through%20reward%20scaling%20with%20layer%20normalization%0A%28RS-LN%29%20and%20a%20penalization%20mechanism%20for%20infeasible%20actions%20%28PA%29.%20By%20combining%0ARS-LN%20and%20PA%2C%20we%20develop%20a%20new%20algorithm%20called%20PARS.%20We%20evaluate%20PARS%20across%20a%0Arange%20of%20tasks%2C%20demonstrating%20superior%20performance%20compared%20to%20state-of-the-art%0Aalgorithms%20in%20both%20offline%20training%20and%20online%20fine-tuning%20on%20the%20D4RL%0Abenchmark%2C%20with%20notable%20success%20in%20the%20challenging%20AntMaze%20Ultra%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08761v2&entry.124074799=Read"},
{"title": "Recommendations with Sparse Comparison Data: Provably Fast Convergence\n  for Nonconvex Matrix Factorization", "author": "Suryanarayana Sankagiri and Jalal Etesami and Matthias Grossglauser", "abstract": "  This paper provides a theoretical analysis of a new learning problem for\nrecommender systems where users provide feedback by comparing pairs of items\ninstead of rating them individually. We assume that comparisons stem from\nlatent user and item features, which reduces the task of predicting preferences\nto learning these features from comparison data. Similar to the classical\nmatrix factorization problem, the main challenge in this learning task is that\nthe resulting loss function is nonconvex. Our analysis shows that the loss\nfunction exhibits (restricted) strong convexity near the true solution, which\nensures gradient-based methods converge exponentially, given an appropriate\nwarm start. Importantly, this result holds in a sparse data regime, where each\nuser compares only a few pairs of items. Our main technical contribution is to\nextend certain concentration inequalities commonly used in matrix completion to\nour model. Our work demonstrates that learning personalized recommendations\nfrom comparison data is computationally and statistically efficient.\n", "link": "http://arxiv.org/abs/2502.20033v2", "date": "2025-08-19", "relevancy": 1.7976, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4691}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.449}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recommendations%20with%20Sparse%20Comparison%20Data%3A%20Provably%20Fast%20Convergence%0A%20%20for%20Nonconvex%20Matrix%20Factorization&body=Title%3A%20Recommendations%20with%20Sparse%20Comparison%20Data%3A%20Provably%20Fast%20Convergence%0A%20%20for%20Nonconvex%20Matrix%20Factorization%0AAuthor%3A%20Suryanarayana%20Sankagiri%20and%20Jalal%20Etesami%20and%20Matthias%20Grossglauser%0AAbstract%3A%20%20%20This%20paper%20provides%20a%20theoretical%20analysis%20of%20a%20new%20learning%20problem%20for%0Arecommender%20systems%20where%20users%20provide%20feedback%20by%20comparing%20pairs%20of%20items%0Ainstead%20of%20rating%20them%20individually.%20We%20assume%20that%20comparisons%20stem%20from%0Alatent%20user%20and%20item%20features%2C%20which%20reduces%20the%20task%20of%20predicting%20preferences%0Ato%20learning%20these%20features%20from%20comparison%20data.%20Similar%20to%20the%20classical%0Amatrix%20factorization%20problem%2C%20the%20main%20challenge%20in%20this%20learning%20task%20is%20that%0Athe%20resulting%20loss%20function%20is%20nonconvex.%20Our%20analysis%20shows%20that%20the%20loss%0Afunction%20exhibits%20%28restricted%29%20strong%20convexity%20near%20the%20true%20solution%2C%20which%0Aensures%20gradient-based%20methods%20converge%20exponentially%2C%20given%20an%20appropriate%0Awarm%20start.%20Importantly%2C%20this%20result%20holds%20in%20a%20sparse%20data%20regime%2C%20where%20each%0Auser%20compares%20only%20a%20few%20pairs%20of%20items.%20Our%20main%20technical%20contribution%20is%20to%0Aextend%20certain%20concentration%20inequalities%20commonly%20used%20in%20matrix%20completion%20to%0Aour%20model.%20Our%20work%20demonstrates%20that%20learning%20personalized%20recommendations%0Afrom%20comparison%20data%20is%20computationally%20and%20statistically%20efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20033v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecommendations%2520with%2520Sparse%2520Comparison%2520Data%253A%2520Provably%2520Fast%2520Convergence%250A%2520%2520for%2520Nonconvex%2520Matrix%2520Factorization%26entry.906535625%3DSuryanarayana%2520Sankagiri%2520and%2520Jalal%2520Etesami%2520and%2520Matthias%2520Grossglauser%26entry.1292438233%3D%2520%2520This%2520paper%2520provides%2520a%2520theoretical%2520analysis%2520of%2520a%2520new%2520learning%2520problem%2520for%250Arecommender%2520systems%2520where%2520users%2520provide%2520feedback%2520by%2520comparing%2520pairs%2520of%2520items%250Ainstead%2520of%2520rating%2520them%2520individually.%2520We%2520assume%2520that%2520comparisons%2520stem%2520from%250Alatent%2520user%2520and%2520item%2520features%252C%2520which%2520reduces%2520the%2520task%2520of%2520predicting%2520preferences%250Ato%2520learning%2520these%2520features%2520from%2520comparison%2520data.%2520Similar%2520to%2520the%2520classical%250Amatrix%2520factorization%2520problem%252C%2520the%2520main%2520challenge%2520in%2520this%2520learning%2520task%2520is%2520that%250Athe%2520resulting%2520loss%2520function%2520is%2520nonconvex.%2520Our%2520analysis%2520shows%2520that%2520the%2520loss%250Afunction%2520exhibits%2520%2528restricted%2529%2520strong%2520convexity%2520near%2520the%2520true%2520solution%252C%2520which%250Aensures%2520gradient-based%2520methods%2520converge%2520exponentially%252C%2520given%2520an%2520appropriate%250Awarm%2520start.%2520Importantly%252C%2520this%2520result%2520holds%2520in%2520a%2520sparse%2520data%2520regime%252C%2520where%2520each%250Auser%2520compares%2520only%2520a%2520few%2520pairs%2520of%2520items.%2520Our%2520main%2520technical%2520contribution%2520is%2520to%250Aextend%2520certain%2520concentration%2520inequalities%2520commonly%2520used%2520in%2520matrix%2520completion%2520to%250Aour%2520model.%2520Our%2520work%2520demonstrates%2520that%2520learning%2520personalized%2520recommendations%250Afrom%2520comparison%2520data%2520is%2520computationally%2520and%2520statistically%2520efficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20033v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recommendations%20with%20Sparse%20Comparison%20Data%3A%20Provably%20Fast%20Convergence%0A%20%20for%20Nonconvex%20Matrix%20Factorization&entry.906535625=Suryanarayana%20Sankagiri%20and%20Jalal%20Etesami%20and%20Matthias%20Grossglauser&entry.1292438233=%20%20This%20paper%20provides%20a%20theoretical%20analysis%20of%20a%20new%20learning%20problem%20for%0Arecommender%20systems%20where%20users%20provide%20feedback%20by%20comparing%20pairs%20of%20items%0Ainstead%20of%20rating%20them%20individually.%20We%20assume%20that%20comparisons%20stem%20from%0Alatent%20user%20and%20item%20features%2C%20which%20reduces%20the%20task%20of%20predicting%20preferences%0Ato%20learning%20these%20features%20from%20comparison%20data.%20Similar%20to%20the%20classical%0Amatrix%20factorization%20problem%2C%20the%20main%20challenge%20in%20this%20learning%20task%20is%20that%0Athe%20resulting%20loss%20function%20is%20nonconvex.%20Our%20analysis%20shows%20that%20the%20loss%0Afunction%20exhibits%20%28restricted%29%20strong%20convexity%20near%20the%20true%20solution%2C%20which%0Aensures%20gradient-based%20methods%20converge%20exponentially%2C%20given%20an%20appropriate%0Awarm%20start.%20Importantly%2C%20this%20result%20holds%20in%20a%20sparse%20data%20regime%2C%20where%20each%0Auser%20compares%20only%20a%20few%20pairs%20of%20items.%20Our%20main%20technical%20contribution%20is%20to%0Aextend%20certain%20concentration%20inequalities%20commonly%20used%20in%20matrix%20completion%20to%0Aour%20model.%20Our%20work%20demonstrates%20that%20learning%20personalized%20recommendations%0Afrom%20comparison%20data%20is%20computationally%20and%20statistically%20efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20033v2&entry.124074799=Read"},
{"title": "Modeling Uncertainty: Constraint-Based Belief States in\n  Imperfect-Information Games", "author": "Achille Morenville and \u00c9ric Piette", "abstract": "  In imperfect-information games, agents must make decisions based on partial\nknowledge of the game state. The Belief Stochastic Game model addresses this\nchallenge by delegating state estimation to the game model itself. This allows\nagents to operate on externally provided belief states, thereby reducing the\nneed for game-specific inference logic. This paper investigates two approaches\nto represent beliefs in games with hidden piece identities: a constraint-based\nmodel using Constraint Satisfaction Problems and a probabilistic extension\nusing Belief Propagation to estimate marginal probabilities. We evaluated the\nimpact of both representations using general-purpose agents across two\ndifferent games. Our findings indicate that constraint-based beliefs yield\nresults comparable to those of probabilistic inference, with minimal\ndifferences in agent performance. This suggests that constraint-based belief\nstates alone may suffice for effective decision-making in many settings.\n", "link": "http://arxiv.org/abs/2507.19263v2", "date": "2025-08-19", "relevancy": 1.8883, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5098}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4899}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Uncertainty%3A%20Constraint-Based%20Belief%20States%20in%0A%20%20Imperfect-Information%20Games&body=Title%3A%20Modeling%20Uncertainty%3A%20Constraint-Based%20Belief%20States%20in%0A%20%20Imperfect-Information%20Games%0AAuthor%3A%20Achille%20Morenville%20and%20%C3%89ric%20Piette%0AAbstract%3A%20%20%20In%20imperfect-information%20games%2C%20agents%20must%20make%20decisions%20based%20on%20partial%0Aknowledge%20of%20the%20game%20state.%20The%20Belief%20Stochastic%20Game%20model%20addresses%20this%0Achallenge%20by%20delegating%20state%20estimation%20to%20the%20game%20model%20itself.%20This%20allows%0Aagents%20to%20operate%20on%20externally%20provided%20belief%20states%2C%20thereby%20reducing%20the%0Aneed%20for%20game-specific%20inference%20logic.%20This%20paper%20investigates%20two%20approaches%0Ato%20represent%20beliefs%20in%20games%20with%20hidden%20piece%20identities%3A%20a%20constraint-based%0Amodel%20using%20Constraint%20Satisfaction%20Problems%20and%20a%20probabilistic%20extension%0Ausing%20Belief%20Propagation%20to%20estimate%20marginal%20probabilities.%20We%20evaluated%20the%0Aimpact%20of%20both%20representations%20using%20general-purpose%20agents%20across%20two%0Adifferent%20games.%20Our%20findings%20indicate%20that%20constraint-based%20beliefs%20yield%0Aresults%20comparable%20to%20those%20of%20probabilistic%20inference%2C%20with%20minimal%0Adifferences%20in%20agent%20performance.%20This%20suggests%20that%20constraint-based%20belief%0Astates%20alone%20may%20suffice%20for%20effective%20decision-making%20in%20many%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19263v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Uncertainty%253A%2520Constraint-Based%2520Belief%2520States%2520in%250A%2520%2520Imperfect-Information%2520Games%26entry.906535625%3DAchille%2520Morenville%2520and%2520%25C3%2589ric%2520Piette%26entry.1292438233%3D%2520%2520In%2520imperfect-information%2520games%252C%2520agents%2520must%2520make%2520decisions%2520based%2520on%2520partial%250Aknowledge%2520of%2520the%2520game%2520state.%2520The%2520Belief%2520Stochastic%2520Game%2520model%2520addresses%2520this%250Achallenge%2520by%2520delegating%2520state%2520estimation%2520to%2520the%2520game%2520model%2520itself.%2520This%2520allows%250Aagents%2520to%2520operate%2520on%2520externally%2520provided%2520belief%2520states%252C%2520thereby%2520reducing%2520the%250Aneed%2520for%2520game-specific%2520inference%2520logic.%2520This%2520paper%2520investigates%2520two%2520approaches%250Ato%2520represent%2520beliefs%2520in%2520games%2520with%2520hidden%2520piece%2520identities%253A%2520a%2520constraint-based%250Amodel%2520using%2520Constraint%2520Satisfaction%2520Problems%2520and%2520a%2520probabilistic%2520extension%250Ausing%2520Belief%2520Propagation%2520to%2520estimate%2520marginal%2520probabilities.%2520We%2520evaluated%2520the%250Aimpact%2520of%2520both%2520representations%2520using%2520general-purpose%2520agents%2520across%2520two%250Adifferent%2520games.%2520Our%2520findings%2520indicate%2520that%2520constraint-based%2520beliefs%2520yield%250Aresults%2520comparable%2520to%2520those%2520of%2520probabilistic%2520inference%252C%2520with%2520minimal%250Adifferences%2520in%2520agent%2520performance.%2520This%2520suggests%2520that%2520constraint-based%2520belief%250Astates%2520alone%2520may%2520suffice%2520for%2520effective%2520decision-making%2520in%2520many%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19263v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Uncertainty%3A%20Constraint-Based%20Belief%20States%20in%0A%20%20Imperfect-Information%20Games&entry.906535625=Achille%20Morenville%20and%20%C3%89ric%20Piette&entry.1292438233=%20%20In%20imperfect-information%20games%2C%20agents%20must%20make%20decisions%20based%20on%20partial%0Aknowledge%20of%20the%20game%20state.%20The%20Belief%20Stochastic%20Game%20model%20addresses%20this%0Achallenge%20by%20delegating%20state%20estimation%20to%20the%20game%20model%20itself.%20This%20allows%0Aagents%20to%20operate%20on%20externally%20provided%20belief%20states%2C%20thereby%20reducing%20the%0Aneed%20for%20game-specific%20inference%20logic.%20This%20paper%20investigates%20two%20approaches%0Ato%20represent%20beliefs%20in%20games%20with%20hidden%20piece%20identities%3A%20a%20constraint-based%0Amodel%20using%20Constraint%20Satisfaction%20Problems%20and%20a%20probabilistic%20extension%0Ausing%20Belief%20Propagation%20to%20estimate%20marginal%20probabilities.%20We%20evaluated%20the%0Aimpact%20of%20both%20representations%20using%20general-purpose%20agents%20across%20two%0Adifferent%20games.%20Our%20findings%20indicate%20that%20constraint-based%20beliefs%20yield%0Aresults%20comparable%20to%20those%20of%20probabilistic%20inference%2C%20with%20minimal%0Adifferences%20in%20agent%20performance.%20This%20suggests%20that%20constraint-based%20belief%0Astates%20alone%20may%20suffice%20for%20effective%20decision-making%20in%20many%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19263v2&entry.124074799=Read"},
{"title": "A PC Algorithm for Max-Linear Bayesian Networks", "author": "Carlos Am\u00e9ndola and Benjamin Hollering and Francesco Nowell", "abstract": "  Max-linear Bayesian networks (MLBNs) are a relatively recent class of\nstructural equation models which arise when the random variables involved have\nheavy-tailed distributions. Unlike most directed graphical models, MLBNs are\ntypically not faithful to d-separation and thus classical causal discovery\nalgorithms such as the PC algorithm or greedy equivalence search can not be\nused to accurately recover the true graph structure. In this paper, we begin\nthe study of constraint-based discovery algorithms for MLBNs given an oracle\nfor testing conditional independence in the true, unknown graph. We show that\nif the oracle is given by the $\\ast$-separation criteria in the true graph,\nthen the PC algorithm remains consistent despite the presence of additional CI\nstatements implied by $\\ast$-separation. We also introduce a new causal\ndiscovery algorithm named \"PCstar\" which assumes faithfulness to\n$C^\\ast$-separation and is able to orient additional edges which cannot be\noriented with only d- or $\\ast$-separation.\n", "link": "http://arxiv.org/abs/2508.13967v1", "date": "2025-08-19", "relevancy": 1.3091, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4817}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4299}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20PC%20Algorithm%20for%20Max-Linear%20Bayesian%20Networks&body=Title%3A%20A%20PC%20Algorithm%20for%20Max-Linear%20Bayesian%20Networks%0AAuthor%3A%20Carlos%20Am%C3%A9ndola%20and%20Benjamin%20Hollering%20and%20Francesco%20Nowell%0AAbstract%3A%20%20%20Max-linear%20Bayesian%20networks%20%28MLBNs%29%20are%20a%20relatively%20recent%20class%20of%0Astructural%20equation%20models%20which%20arise%20when%20the%20random%20variables%20involved%20have%0Aheavy-tailed%20distributions.%20Unlike%20most%20directed%20graphical%20models%2C%20MLBNs%20are%0Atypically%20not%20faithful%20to%20d-separation%20and%20thus%20classical%20causal%20discovery%0Aalgorithms%20such%20as%20the%20PC%20algorithm%20or%20greedy%20equivalence%20search%20can%20not%20be%0Aused%20to%20accurately%20recover%20the%20true%20graph%20structure.%20In%20this%20paper%2C%20we%20begin%0Athe%20study%20of%20constraint-based%20discovery%20algorithms%20for%20MLBNs%20given%20an%20oracle%0Afor%20testing%20conditional%20independence%20in%20the%20true%2C%20unknown%20graph.%20We%20show%20that%0Aif%20the%20oracle%20is%20given%20by%20the%20%24%5Cast%24-separation%20criteria%20in%20the%20true%20graph%2C%0Athen%20the%20PC%20algorithm%20remains%20consistent%20despite%20the%20presence%20of%20additional%20CI%0Astatements%20implied%20by%20%24%5Cast%24-separation.%20We%20also%20introduce%20a%20new%20causal%0Adiscovery%20algorithm%20named%20%22PCstar%22%20which%20assumes%20faithfulness%20to%0A%24C%5E%5Cast%24-separation%20and%20is%20able%20to%20orient%20additional%20edges%20which%20cannot%20be%0Aoriented%20with%20only%20d-%20or%20%24%5Cast%24-separation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520PC%2520Algorithm%2520for%2520Max-Linear%2520Bayesian%2520Networks%26entry.906535625%3DCarlos%2520Am%25C3%25A9ndola%2520and%2520Benjamin%2520Hollering%2520and%2520Francesco%2520Nowell%26entry.1292438233%3D%2520%2520Max-linear%2520Bayesian%2520networks%2520%2528MLBNs%2529%2520are%2520a%2520relatively%2520recent%2520class%2520of%250Astructural%2520equation%2520models%2520which%2520arise%2520when%2520the%2520random%2520variables%2520involved%2520have%250Aheavy-tailed%2520distributions.%2520Unlike%2520most%2520directed%2520graphical%2520models%252C%2520MLBNs%2520are%250Atypically%2520not%2520faithful%2520to%2520d-separation%2520and%2520thus%2520classical%2520causal%2520discovery%250Aalgorithms%2520such%2520as%2520the%2520PC%2520algorithm%2520or%2520greedy%2520equivalence%2520search%2520can%2520not%2520be%250Aused%2520to%2520accurately%2520recover%2520the%2520true%2520graph%2520structure.%2520In%2520this%2520paper%252C%2520we%2520begin%250Athe%2520study%2520of%2520constraint-based%2520discovery%2520algorithms%2520for%2520MLBNs%2520given%2520an%2520oracle%250Afor%2520testing%2520conditional%2520independence%2520in%2520the%2520true%252C%2520unknown%2520graph.%2520We%2520show%2520that%250Aif%2520the%2520oracle%2520is%2520given%2520by%2520the%2520%2524%255Cast%2524-separation%2520criteria%2520in%2520the%2520true%2520graph%252C%250Athen%2520the%2520PC%2520algorithm%2520remains%2520consistent%2520despite%2520the%2520presence%2520of%2520additional%2520CI%250Astatements%2520implied%2520by%2520%2524%255Cast%2524-separation.%2520We%2520also%2520introduce%2520a%2520new%2520causal%250Adiscovery%2520algorithm%2520named%2520%2522PCstar%2522%2520which%2520assumes%2520faithfulness%2520to%250A%2524C%255E%255Cast%2524-separation%2520and%2520is%2520able%2520to%2520orient%2520additional%2520edges%2520which%2520cannot%2520be%250Aoriented%2520with%2520only%2520d-%2520or%2520%2524%255Cast%2524-separation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20PC%20Algorithm%20for%20Max-Linear%20Bayesian%20Networks&entry.906535625=Carlos%20Am%C3%A9ndola%20and%20Benjamin%20Hollering%20and%20Francesco%20Nowell&entry.1292438233=%20%20Max-linear%20Bayesian%20networks%20%28MLBNs%29%20are%20a%20relatively%20recent%20class%20of%0Astructural%20equation%20models%20which%20arise%20when%20the%20random%20variables%20involved%20have%0Aheavy-tailed%20distributions.%20Unlike%20most%20directed%20graphical%20models%2C%20MLBNs%20are%0Atypically%20not%20faithful%20to%20d-separation%20and%20thus%20classical%20causal%20discovery%0Aalgorithms%20such%20as%20the%20PC%20algorithm%20or%20greedy%20equivalence%20search%20can%20not%20be%0Aused%20to%20accurately%20recover%20the%20true%20graph%20structure.%20In%20this%20paper%2C%20we%20begin%0Athe%20study%20of%20constraint-based%20discovery%20algorithms%20for%20MLBNs%20given%20an%20oracle%0Afor%20testing%20conditional%20independence%20in%20the%20true%2C%20unknown%20graph.%20We%20show%20that%0Aif%20the%20oracle%20is%20given%20by%20the%20%24%5Cast%24-separation%20criteria%20in%20the%20true%20graph%2C%0Athen%20the%20PC%20algorithm%20remains%20consistent%20despite%20the%20presence%20of%20additional%20CI%0Astatements%20implied%20by%20%24%5Cast%24-separation.%20We%20also%20introduce%20a%20new%20causal%0Adiscovery%20algorithm%20named%20%22PCstar%22%20which%20assumes%20faithfulness%20to%0A%24C%5E%5Cast%24-separation%20and%20is%20able%20to%20orient%20additional%20edges%20which%20cannot%20be%0Aoriented%20with%20only%20d-%20or%20%24%5Cast%24-separation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13967v1&entry.124074799=Read"},
{"title": "Closed-Form Feedback-Free Learning with Forward Projection", "author": "Robert O'Shea and Bipin Rajendran", "abstract": "  State-of-the-art methods for backpropagation-free learning employ local error\nfeedback to direct iterative optimisation via gradient descent. In this study,\nwe examine the more restrictive setting where retrograde communication from\nneuronal outputs is unavailable for pre-synaptic weight optimisation. To\naddress this challenge, we propose Forward Projection (FP). This novel\nrandomised closed-form training method requires only a single forward pass over\nthe entire dataset for model fitting, without retrograde communication. Target\nvalues for pre-activation membrane potentials are generated layer-wise via\nnonlinear projections of pre-synaptic inputs and the labels. Local loss\nfunctions are optimised over pre-synaptic inputs using closed-form regression,\nwithout feedback from neuronal outputs or downstream layers. Interpretability\nis a key advantage of FP training; membrane potentials of hidden neurons in\nFP-trained networks encode information which is interpretable layer-wise as\nlabel predictions. We demonstrate the effectiveness of FP across four\nbiomedical datasets. In few-shot learning tasks, FP yielded more generalisable\nmodels than those optimised via backpropagation. In large-sample tasks,\nFP-based models achieve generalisation comparable to gradient descent-based\nlocal learning methods while requiring only a single forward propagation step,\nachieving significant speed up for training. Interpretation functions defined\non local neuronal activity in FP-based models successfully identified\nclinically salient features for diagnosis in two biomedical datasets. Forward\nProjection is a computationally efficient machine learning approach that yields\ninterpretable neural network models without retrograde communication of\nneuronal activity during training.\n", "link": "http://arxiv.org/abs/2501.16476v2", "date": "2025-08-19", "relevancy": 1.4119, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.473}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4695}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closed-Form%20Feedback-Free%20Learning%20with%20Forward%20Projection&body=Title%3A%20Closed-Form%20Feedback-Free%20Learning%20with%20Forward%20Projection%0AAuthor%3A%20Robert%20O%27Shea%20and%20Bipin%20Rajendran%0AAbstract%3A%20%20%20State-of-the-art%20methods%20for%20backpropagation-free%20learning%20employ%20local%20error%0Afeedback%20to%20direct%20iterative%20optimisation%20via%20gradient%20descent.%20In%20this%20study%2C%0Awe%20examine%20the%20more%20restrictive%20setting%20where%20retrograde%20communication%20from%0Aneuronal%20outputs%20is%20unavailable%20for%20pre-synaptic%20weight%20optimisation.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20Forward%20Projection%20%28FP%29.%20This%20novel%0Arandomised%20closed-form%20training%20method%20requires%20only%20a%20single%20forward%20pass%20over%0Athe%20entire%20dataset%20for%20model%20fitting%2C%20without%20retrograde%20communication.%20Target%0Avalues%20for%20pre-activation%20membrane%20potentials%20are%20generated%20layer-wise%20via%0Anonlinear%20projections%20of%20pre-synaptic%20inputs%20and%20the%20labels.%20Local%20loss%0Afunctions%20are%20optimised%20over%20pre-synaptic%20inputs%20using%20closed-form%20regression%2C%0Awithout%20feedback%20from%20neuronal%20outputs%20or%20downstream%20layers.%20Interpretability%0Ais%20a%20key%20advantage%20of%20FP%20training%3B%20membrane%20potentials%20of%20hidden%20neurons%20in%0AFP-trained%20networks%20encode%20information%20which%20is%20interpretable%20layer-wise%20as%0Alabel%20predictions.%20We%20demonstrate%20the%20effectiveness%20of%20FP%20across%20four%0Abiomedical%20datasets.%20In%20few-shot%20learning%20tasks%2C%20FP%20yielded%20more%20generalisable%0Amodels%20than%20those%20optimised%20via%20backpropagation.%20In%20large-sample%20tasks%2C%0AFP-based%20models%20achieve%20generalisation%20comparable%20to%20gradient%20descent-based%0Alocal%20learning%20methods%20while%20requiring%20only%20a%20single%20forward%20propagation%20step%2C%0Aachieving%20significant%20speed%20up%20for%20training.%20Interpretation%20functions%20defined%0Aon%20local%20neuronal%20activity%20in%20FP-based%20models%20successfully%20identified%0Aclinically%20salient%20features%20for%20diagnosis%20in%20two%20biomedical%20datasets.%20Forward%0AProjection%20is%20a%20computationally%20efficient%20machine%20learning%20approach%20that%20yields%0Ainterpretable%20neural%20network%20models%20without%20retrograde%20communication%20of%0Aneuronal%20activity%20during%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16476v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosed-Form%2520Feedback-Free%2520Learning%2520with%2520Forward%2520Projection%26entry.906535625%3DRobert%2520O%2527Shea%2520and%2520Bipin%2520Rajendran%26entry.1292438233%3D%2520%2520State-of-the-art%2520methods%2520for%2520backpropagation-free%2520learning%2520employ%2520local%2520error%250Afeedback%2520to%2520direct%2520iterative%2520optimisation%2520via%2520gradient%2520descent.%2520In%2520this%2520study%252C%250Awe%2520examine%2520the%2520more%2520restrictive%2520setting%2520where%2520retrograde%2520communication%2520from%250Aneuronal%2520outputs%2520is%2520unavailable%2520for%2520pre-synaptic%2520weight%2520optimisation.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520Forward%2520Projection%2520%2528FP%2529.%2520This%2520novel%250Arandomised%2520closed-form%2520training%2520method%2520requires%2520only%2520a%2520single%2520forward%2520pass%2520over%250Athe%2520entire%2520dataset%2520for%2520model%2520fitting%252C%2520without%2520retrograde%2520communication.%2520Target%250Avalues%2520for%2520pre-activation%2520membrane%2520potentials%2520are%2520generated%2520layer-wise%2520via%250Anonlinear%2520projections%2520of%2520pre-synaptic%2520inputs%2520and%2520the%2520labels.%2520Local%2520loss%250Afunctions%2520are%2520optimised%2520over%2520pre-synaptic%2520inputs%2520using%2520closed-form%2520regression%252C%250Awithout%2520feedback%2520from%2520neuronal%2520outputs%2520or%2520downstream%2520layers.%2520Interpretability%250Ais%2520a%2520key%2520advantage%2520of%2520FP%2520training%253B%2520membrane%2520potentials%2520of%2520hidden%2520neurons%2520in%250AFP-trained%2520networks%2520encode%2520information%2520which%2520is%2520interpretable%2520layer-wise%2520as%250Alabel%2520predictions.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520FP%2520across%2520four%250Abiomedical%2520datasets.%2520In%2520few-shot%2520learning%2520tasks%252C%2520FP%2520yielded%2520more%2520generalisable%250Amodels%2520than%2520those%2520optimised%2520via%2520backpropagation.%2520In%2520large-sample%2520tasks%252C%250AFP-based%2520models%2520achieve%2520generalisation%2520comparable%2520to%2520gradient%2520descent-based%250Alocal%2520learning%2520methods%2520while%2520requiring%2520only%2520a%2520single%2520forward%2520propagation%2520step%252C%250Aachieving%2520significant%2520speed%2520up%2520for%2520training.%2520Interpretation%2520functions%2520defined%250Aon%2520local%2520neuronal%2520activity%2520in%2520FP-based%2520models%2520successfully%2520identified%250Aclinically%2520salient%2520features%2520for%2520diagnosis%2520in%2520two%2520biomedical%2520datasets.%2520Forward%250AProjection%2520is%2520a%2520computationally%2520efficient%2520machine%2520learning%2520approach%2520that%2520yields%250Ainterpretable%2520neural%2520network%2520models%2520without%2520retrograde%2520communication%2520of%250Aneuronal%2520activity%2520during%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16476v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closed-Form%20Feedback-Free%20Learning%20with%20Forward%20Projection&entry.906535625=Robert%20O%27Shea%20and%20Bipin%20Rajendran&entry.1292438233=%20%20State-of-the-art%20methods%20for%20backpropagation-free%20learning%20employ%20local%20error%0Afeedback%20to%20direct%20iterative%20optimisation%20via%20gradient%20descent.%20In%20this%20study%2C%0Awe%20examine%20the%20more%20restrictive%20setting%20where%20retrograde%20communication%20from%0Aneuronal%20outputs%20is%20unavailable%20for%20pre-synaptic%20weight%20optimisation.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20Forward%20Projection%20%28FP%29.%20This%20novel%0Arandomised%20closed-form%20training%20method%20requires%20only%20a%20single%20forward%20pass%20over%0Athe%20entire%20dataset%20for%20model%20fitting%2C%20without%20retrograde%20communication.%20Target%0Avalues%20for%20pre-activation%20membrane%20potentials%20are%20generated%20layer-wise%20via%0Anonlinear%20projections%20of%20pre-synaptic%20inputs%20and%20the%20labels.%20Local%20loss%0Afunctions%20are%20optimised%20over%20pre-synaptic%20inputs%20using%20closed-form%20regression%2C%0Awithout%20feedback%20from%20neuronal%20outputs%20or%20downstream%20layers.%20Interpretability%0Ais%20a%20key%20advantage%20of%20FP%20training%3B%20membrane%20potentials%20of%20hidden%20neurons%20in%0AFP-trained%20networks%20encode%20information%20which%20is%20interpretable%20layer-wise%20as%0Alabel%20predictions.%20We%20demonstrate%20the%20effectiveness%20of%20FP%20across%20four%0Abiomedical%20datasets.%20In%20few-shot%20learning%20tasks%2C%20FP%20yielded%20more%20generalisable%0Amodels%20than%20those%20optimised%20via%20backpropagation.%20In%20large-sample%20tasks%2C%0AFP-based%20models%20achieve%20generalisation%20comparable%20to%20gradient%20descent-based%0Alocal%20learning%20methods%20while%20requiring%20only%20a%20single%20forward%20propagation%20step%2C%0Aachieving%20significant%20speed%20up%20for%20training.%20Interpretation%20functions%20defined%0Aon%20local%20neuronal%20activity%20in%20FP-based%20models%20successfully%20identified%0Aclinically%20salient%20features%20for%20diagnosis%20in%20two%20biomedical%20datasets.%20Forward%0AProjection%20is%20a%20computationally%20efficient%20machine%20learning%20approach%20that%20yields%0Ainterpretable%20neural%20network%20models%20without%20retrograde%20communication%20of%0Aneuronal%20activity%20during%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16476v2&entry.124074799=Read"},
{"title": "Communication-Efficient Federated Learning with Adaptive Number of\n  Participants", "author": "Sergey Skorik and Vladislav Dorofeev and Gleb Molodtsov and Aram Avetisyan and Dmitry Bylinkin and Daniil Medyakov and Aleksandr Beznosikov", "abstract": "  Rapid scaling of deep learning models has enabled performance gains across\ndomains, yet it introduced several challenges. Federated Learning (FL) has\nemerged as a promising framework to address these concerns by enabling\ndecentralized training. Nevertheless, communication efficiency remains a key\nbottleneck in FL, particularly under heterogeneous and dynamic client\nparticipation. Existing methods, such as FedAvg and FedProx, or other\napproaches, including client selection strategies, attempt to mitigate\ncommunication costs. However, the problem of choosing the number of clients in\na training round remains extremely underexplored. We introduce Intelligent\nSelection of Participants (ISP), an adaptive mechanism that dynamically\ndetermines the optimal number of clients per round to enhance communication\nefficiency without compromising model accuracy. We validate the effectiveness\nof ISP across diverse setups, including vision transformers, real-world ECG\nclassification, and training with gradient compression. Our results show\nconsistent communication savings of up to 30\\% without losing the final\nquality. Applying ISP to different real-world ECG classification setups\nhighlighted the selection of the number of clients as a separate task of\nfederated learning.\n", "link": "http://arxiv.org/abs/2508.13803v1", "date": "2025-08-19", "relevancy": 1.458, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4947}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4851}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication-Efficient%20Federated%20Learning%20with%20Adaptive%20Number%20of%0A%20%20Participants&body=Title%3A%20Communication-Efficient%20Federated%20Learning%20with%20Adaptive%20Number%20of%0A%20%20Participants%0AAuthor%3A%20Sergey%20Skorik%20and%20Vladislav%20Dorofeev%20and%20Gleb%20Molodtsov%20and%20Aram%20Avetisyan%20and%20Dmitry%20Bylinkin%20and%20Daniil%20Medyakov%20and%20Aleksandr%20Beznosikov%0AAbstract%3A%20%20%20Rapid%20scaling%20of%20deep%20learning%20models%20has%20enabled%20performance%20gains%20across%0Adomains%2C%20yet%20it%20introduced%20several%20challenges.%20Federated%20Learning%20%28FL%29%20has%0Aemerged%20as%20a%20promising%20framework%20to%20address%20these%20concerns%20by%20enabling%0Adecentralized%20training.%20Nevertheless%2C%20communication%20efficiency%20remains%20a%20key%0Abottleneck%20in%20FL%2C%20particularly%20under%20heterogeneous%20and%20dynamic%20client%0Aparticipation.%20Existing%20methods%2C%20such%20as%20FedAvg%20and%20FedProx%2C%20or%20other%0Aapproaches%2C%20including%20client%20selection%20strategies%2C%20attempt%20to%20mitigate%0Acommunication%20costs.%20However%2C%20the%20problem%20of%20choosing%20the%20number%20of%20clients%20in%0Aa%20training%20round%20remains%20extremely%20underexplored.%20We%20introduce%20Intelligent%0ASelection%20of%20Participants%20%28ISP%29%2C%20an%20adaptive%20mechanism%20that%20dynamically%0Adetermines%20the%20optimal%20number%20of%20clients%20per%20round%20to%20enhance%20communication%0Aefficiency%20without%20compromising%20model%20accuracy.%20We%20validate%20the%20effectiveness%0Aof%20ISP%20across%20diverse%20setups%2C%20including%20vision%20transformers%2C%20real-world%20ECG%0Aclassification%2C%20and%20training%20with%20gradient%20compression.%20Our%20results%20show%0Aconsistent%20communication%20savings%20of%20up%20to%2030%5C%25%20without%20losing%20the%20final%0Aquality.%20Applying%20ISP%20to%20different%20real-world%20ECG%20classification%20setups%0Ahighlighted%20the%20selection%20of%20the%20number%20of%20clients%20as%20a%20separate%20task%20of%0Afederated%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication-Efficient%2520Federated%2520Learning%2520with%2520Adaptive%2520Number%2520of%250A%2520%2520Participants%26entry.906535625%3DSergey%2520Skorik%2520and%2520Vladislav%2520Dorofeev%2520and%2520Gleb%2520Molodtsov%2520and%2520Aram%2520Avetisyan%2520and%2520Dmitry%2520Bylinkin%2520and%2520Daniil%2520Medyakov%2520and%2520Aleksandr%2520Beznosikov%26entry.1292438233%3D%2520%2520Rapid%2520scaling%2520of%2520deep%2520learning%2520models%2520has%2520enabled%2520performance%2520gains%2520across%250Adomains%252C%2520yet%2520it%2520introduced%2520several%2520challenges.%2520Federated%2520Learning%2520%2528FL%2529%2520has%250Aemerged%2520as%2520a%2520promising%2520framework%2520to%2520address%2520these%2520concerns%2520by%2520enabling%250Adecentralized%2520training.%2520Nevertheless%252C%2520communication%2520efficiency%2520remains%2520a%2520key%250Abottleneck%2520in%2520FL%252C%2520particularly%2520under%2520heterogeneous%2520and%2520dynamic%2520client%250Aparticipation.%2520Existing%2520methods%252C%2520such%2520as%2520FedAvg%2520and%2520FedProx%252C%2520or%2520other%250Aapproaches%252C%2520including%2520client%2520selection%2520strategies%252C%2520attempt%2520to%2520mitigate%250Acommunication%2520costs.%2520However%252C%2520the%2520problem%2520of%2520choosing%2520the%2520number%2520of%2520clients%2520in%250Aa%2520training%2520round%2520remains%2520extremely%2520underexplored.%2520We%2520introduce%2520Intelligent%250ASelection%2520of%2520Participants%2520%2528ISP%2529%252C%2520an%2520adaptive%2520mechanism%2520that%2520dynamically%250Adetermines%2520the%2520optimal%2520number%2520of%2520clients%2520per%2520round%2520to%2520enhance%2520communication%250Aefficiency%2520without%2520compromising%2520model%2520accuracy.%2520We%2520validate%2520the%2520effectiveness%250Aof%2520ISP%2520across%2520diverse%2520setups%252C%2520including%2520vision%2520transformers%252C%2520real-world%2520ECG%250Aclassification%252C%2520and%2520training%2520with%2520gradient%2520compression.%2520Our%2520results%2520show%250Aconsistent%2520communication%2520savings%2520of%2520up%2520to%252030%255C%2525%2520without%2520losing%2520the%2520final%250Aquality.%2520Applying%2520ISP%2520to%2520different%2520real-world%2520ECG%2520classification%2520setups%250Ahighlighted%2520the%2520selection%2520of%2520the%2520number%2520of%2520clients%2520as%2520a%2520separate%2520task%2520of%250Afederated%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication-Efficient%20Federated%20Learning%20with%20Adaptive%20Number%20of%0A%20%20Participants&entry.906535625=Sergey%20Skorik%20and%20Vladislav%20Dorofeev%20and%20Gleb%20Molodtsov%20and%20Aram%20Avetisyan%20and%20Dmitry%20Bylinkin%20and%20Daniil%20Medyakov%20and%20Aleksandr%20Beznosikov&entry.1292438233=%20%20Rapid%20scaling%20of%20deep%20learning%20models%20has%20enabled%20performance%20gains%20across%0Adomains%2C%20yet%20it%20introduced%20several%20challenges.%20Federated%20Learning%20%28FL%29%20has%0Aemerged%20as%20a%20promising%20framework%20to%20address%20these%20concerns%20by%20enabling%0Adecentralized%20training.%20Nevertheless%2C%20communication%20efficiency%20remains%20a%20key%0Abottleneck%20in%20FL%2C%20particularly%20under%20heterogeneous%20and%20dynamic%20client%0Aparticipation.%20Existing%20methods%2C%20such%20as%20FedAvg%20and%20FedProx%2C%20or%20other%0Aapproaches%2C%20including%20client%20selection%20strategies%2C%20attempt%20to%20mitigate%0Acommunication%20costs.%20However%2C%20the%20problem%20of%20choosing%20the%20number%20of%20clients%20in%0Aa%20training%20round%20remains%20extremely%20underexplored.%20We%20introduce%20Intelligent%0ASelection%20of%20Participants%20%28ISP%29%2C%20an%20adaptive%20mechanism%20that%20dynamically%0Adetermines%20the%20optimal%20number%20of%20clients%20per%20round%20to%20enhance%20communication%0Aefficiency%20without%20compromising%20model%20accuracy.%20We%20validate%20the%20effectiveness%0Aof%20ISP%20across%20diverse%20setups%2C%20including%20vision%20transformers%2C%20real-world%20ECG%0Aclassification%2C%20and%20training%20with%20gradient%20compression.%20Our%20results%20show%0Aconsistent%20communication%20savings%20of%20up%20to%2030%5C%25%20without%20losing%20the%20final%0Aquality.%20Applying%20ISP%20to%20different%20real-world%20ECG%20classification%20setups%0Ahighlighted%20the%20selection%20of%20the%20number%20of%20clients%20as%20a%20separate%20task%20of%0Afederated%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13803v1&entry.124074799=Read"},
{"title": "Convergent Reinforcement Learning Algorithms for Stochastic Shortest\n  Path Problem", "author": "Soumyajit Guin and Shalabh Bhatnagar", "abstract": "  In this paper we propose two algorithms in the tabular setting and an\nalgorithm for the function approximation setting for the Stochastic Shortest\nPath (SSP) problem. SSP problems form an important class of problems in\nReinforcement Learning (RL), as other types of cost-criteria in RL can be\nformulated in the setting of SSP. We show asymptotic almost-sure convergence\nfor all our algorithms. We observe superior performance of our tabular\nalgorithms compared to other well-known convergent RL algorithms. We further\nobserve reliable performance of our function approximation algorithm compared\nto other algorithms in the function approximation setting.\n", "link": "http://arxiv.org/abs/2508.13963v1", "date": "2025-08-19", "relevancy": 1.6886, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4497}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4198}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergent%20Reinforcement%20Learning%20Algorithms%20for%20Stochastic%20Shortest%0A%20%20Path%20Problem&body=Title%3A%20Convergent%20Reinforcement%20Learning%20Algorithms%20for%20Stochastic%20Shortest%0A%20%20Path%20Problem%0AAuthor%3A%20Soumyajit%20Guin%20and%20Shalabh%20Bhatnagar%0AAbstract%3A%20%20%20In%20this%20paper%20we%20propose%20two%20algorithms%20in%20the%20tabular%20setting%20and%20an%0Aalgorithm%20for%20the%20function%20approximation%20setting%20for%20the%20Stochastic%20Shortest%0APath%20%28SSP%29%20problem.%20SSP%20problems%20form%20an%20important%20class%20of%20problems%20in%0AReinforcement%20Learning%20%28RL%29%2C%20as%20other%20types%20of%20cost-criteria%20in%20RL%20can%20be%0Aformulated%20in%20the%20setting%20of%20SSP.%20We%20show%20asymptotic%20almost-sure%20convergence%0Afor%20all%20our%20algorithms.%20We%20observe%20superior%20performance%20of%20our%20tabular%0Aalgorithms%20compared%20to%20other%20well-known%20convergent%20RL%20algorithms.%20We%20further%0Aobserve%20reliable%20performance%20of%20our%20function%20approximation%20algorithm%20compared%0Ato%20other%20algorithms%20in%20the%20function%20approximation%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergent%2520Reinforcement%2520Learning%2520Algorithms%2520for%2520Stochastic%2520Shortest%250A%2520%2520Path%2520Problem%26entry.906535625%3DSoumyajit%2520Guin%2520and%2520Shalabh%2520Bhatnagar%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520propose%2520two%2520algorithms%2520in%2520the%2520tabular%2520setting%2520and%2520an%250Aalgorithm%2520for%2520the%2520function%2520approximation%2520setting%2520for%2520the%2520Stochastic%2520Shortest%250APath%2520%2528SSP%2529%2520problem.%2520SSP%2520problems%2520form%2520an%2520important%2520class%2520of%2520problems%2520in%250AReinforcement%2520Learning%2520%2528RL%2529%252C%2520as%2520other%2520types%2520of%2520cost-criteria%2520in%2520RL%2520can%2520be%250Aformulated%2520in%2520the%2520setting%2520of%2520SSP.%2520We%2520show%2520asymptotic%2520almost-sure%2520convergence%250Afor%2520all%2520our%2520algorithms.%2520We%2520observe%2520superior%2520performance%2520of%2520our%2520tabular%250Aalgorithms%2520compared%2520to%2520other%2520well-known%2520convergent%2520RL%2520algorithms.%2520We%2520further%250Aobserve%2520reliable%2520performance%2520of%2520our%2520function%2520approximation%2520algorithm%2520compared%250Ato%2520other%2520algorithms%2520in%2520the%2520function%2520approximation%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergent%20Reinforcement%20Learning%20Algorithms%20for%20Stochastic%20Shortest%0A%20%20Path%20Problem&entry.906535625=Soumyajit%20Guin%20and%20Shalabh%20Bhatnagar&entry.1292438233=%20%20In%20this%20paper%20we%20propose%20two%20algorithms%20in%20the%20tabular%20setting%20and%20an%0Aalgorithm%20for%20the%20function%20approximation%20setting%20for%20the%20Stochastic%20Shortest%0APath%20%28SSP%29%20problem.%20SSP%20problems%20form%20an%20important%20class%20of%20problems%20in%0AReinforcement%20Learning%20%28RL%29%2C%20as%20other%20types%20of%20cost-criteria%20in%20RL%20can%20be%0Aformulated%20in%20the%20setting%20of%20SSP.%20We%20show%20asymptotic%20almost-sure%20convergence%0Afor%20all%20our%20algorithms.%20We%20observe%20superior%20performance%20of%20our%20tabular%0Aalgorithms%20compared%20to%20other%20well-known%20convergent%20RL%20algorithms.%20We%20further%0Aobserve%20reliable%20performance%20of%20our%20function%20approximation%20algorithm%20compared%0Ato%20other%20algorithms%20in%20the%20function%20approximation%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13963v1&entry.124074799=Read"},
{"title": "Augmenting cobots for sheet-metal SMEs with 3D object recognition and\n  localisation", "author": "Martijn Cramer and Yanming Wu and David De Schepper and Eric Demeester", "abstract": "  Due to high-mix-low-volume production, sheet-metal workshops today are\nchallenged by small series and varying orders. As standard automation solutions\ntend to fall short, SMEs resort to repetitive manual labour impacting\nproduction costs and leading to tech-skilled workforces not being used to their\nfull potential. The COOCK+ ROBUST project aims to transform cobots into mobile\nand reconfigurable production assistants by integrating existing technologies,\nincluding 3D object recognition and localisation. This article explores both\nthe opportunities and challenges of enhancing cobotic systems with these\ntechnologies in an industrial setting, outlining the key steps involved in the\nprocess. Additionally, insights from a past project, carried out by the ACRO\nresearch unit in collaboration with an industrial partner, serves as a concrete\nimplementation example throughout.\n", "link": "http://arxiv.org/abs/2508.13964v1", "date": "2025-08-19", "relevancy": 1.4605, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5568}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4696}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Augmenting%20cobots%20for%20sheet-metal%20SMEs%20with%203D%20object%20recognition%20and%0A%20%20localisation&body=Title%3A%20Augmenting%20cobots%20for%20sheet-metal%20SMEs%20with%203D%20object%20recognition%20and%0A%20%20localisation%0AAuthor%3A%20Martijn%20Cramer%20and%20Yanming%20Wu%20and%20David%20De%20Schepper%20and%20Eric%20Demeester%0AAbstract%3A%20%20%20Due%20to%20high-mix-low-volume%20production%2C%20sheet-metal%20workshops%20today%20are%0Achallenged%20by%20small%20series%20and%20varying%20orders.%20As%20standard%20automation%20solutions%0Atend%20to%20fall%20short%2C%20SMEs%20resort%20to%20repetitive%20manual%20labour%20impacting%0Aproduction%20costs%20and%20leading%20to%20tech-skilled%20workforces%20not%20being%20used%20to%20their%0Afull%20potential.%20The%20COOCK%2B%20ROBUST%20project%20aims%20to%20transform%20cobots%20into%20mobile%0Aand%20reconfigurable%20production%20assistants%20by%20integrating%20existing%20technologies%2C%0Aincluding%203D%20object%20recognition%20and%20localisation.%20This%20article%20explores%20both%0Athe%20opportunities%20and%20challenges%20of%20enhancing%20cobotic%20systems%20with%20these%0Atechnologies%20in%20an%20industrial%20setting%2C%20outlining%20the%20key%20steps%20involved%20in%20the%0Aprocess.%20Additionally%2C%20insights%20from%20a%20past%20project%2C%20carried%20out%20by%20the%20ACRO%0Aresearch%20unit%20in%20collaboration%20with%20an%20industrial%20partner%2C%20serves%20as%20a%20concrete%0Aimplementation%20example%20throughout.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugmenting%2520cobots%2520for%2520sheet-metal%2520SMEs%2520with%25203D%2520object%2520recognition%2520and%250A%2520%2520localisation%26entry.906535625%3DMartijn%2520Cramer%2520and%2520Yanming%2520Wu%2520and%2520David%2520De%2520Schepper%2520and%2520Eric%2520Demeester%26entry.1292438233%3D%2520%2520Due%2520to%2520high-mix-low-volume%2520production%252C%2520sheet-metal%2520workshops%2520today%2520are%250Achallenged%2520by%2520small%2520series%2520and%2520varying%2520orders.%2520As%2520standard%2520automation%2520solutions%250Atend%2520to%2520fall%2520short%252C%2520SMEs%2520resort%2520to%2520repetitive%2520manual%2520labour%2520impacting%250Aproduction%2520costs%2520and%2520leading%2520to%2520tech-skilled%2520workforces%2520not%2520being%2520used%2520to%2520their%250Afull%2520potential.%2520The%2520COOCK%252B%2520ROBUST%2520project%2520aims%2520to%2520transform%2520cobots%2520into%2520mobile%250Aand%2520reconfigurable%2520production%2520assistants%2520by%2520integrating%2520existing%2520technologies%252C%250Aincluding%25203D%2520object%2520recognition%2520and%2520localisation.%2520This%2520article%2520explores%2520both%250Athe%2520opportunities%2520and%2520challenges%2520of%2520enhancing%2520cobotic%2520systems%2520with%2520these%250Atechnologies%2520in%2520an%2520industrial%2520setting%252C%2520outlining%2520the%2520key%2520steps%2520involved%2520in%2520the%250Aprocess.%2520Additionally%252C%2520insights%2520from%2520a%2520past%2520project%252C%2520carried%2520out%2520by%2520the%2520ACRO%250Aresearch%2520unit%2520in%2520collaboration%2520with%2520an%2520industrial%2520partner%252C%2520serves%2520as%2520a%2520concrete%250Aimplementation%2520example%2520throughout.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmenting%20cobots%20for%20sheet-metal%20SMEs%20with%203D%20object%20recognition%20and%0A%20%20localisation&entry.906535625=Martijn%20Cramer%20and%20Yanming%20Wu%20and%20David%20De%20Schepper%20and%20Eric%20Demeester&entry.1292438233=%20%20Due%20to%20high-mix-low-volume%20production%2C%20sheet-metal%20workshops%20today%20are%0Achallenged%20by%20small%20series%20and%20varying%20orders.%20As%20standard%20automation%20solutions%0Atend%20to%20fall%20short%2C%20SMEs%20resort%20to%20repetitive%20manual%20labour%20impacting%0Aproduction%20costs%20and%20leading%20to%20tech-skilled%20workforces%20not%20being%20used%20to%20their%0Afull%20potential.%20The%20COOCK%2B%20ROBUST%20project%20aims%20to%20transform%20cobots%20into%20mobile%0Aand%20reconfigurable%20production%20assistants%20by%20integrating%20existing%20technologies%2C%0Aincluding%203D%20object%20recognition%20and%20localisation.%20This%20article%20explores%20both%0Athe%20opportunities%20and%20challenges%20of%20enhancing%20cobotic%20systems%20with%20these%0Atechnologies%20in%20an%20industrial%20setting%2C%20outlining%20the%20key%20steps%20involved%20in%20the%0Aprocess.%20Additionally%2C%20insights%20from%20a%20past%20project%2C%20carried%20out%20by%20the%20ACRO%0Aresearch%20unit%20in%20collaboration%20with%20an%20industrial%20partner%2C%20serves%20as%20a%20concrete%0Aimplementation%20example%20throughout.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13964v1&entry.124074799=Read"},
{"title": "Assessing Trustworthiness of AI Training Dataset using Subjective Logic\n  -- A Use Case on Bias", "author": "Koffi Ismael Ouattara and Ioannis Krontiris and Theo Dimitrakos and Frank Kargl", "abstract": "  As AI systems increasingly rely on training data, assessing dataset\ntrustworthiness has become critical, particularly for properties like fairness\nor bias that emerge at the dataset level. Prior work has used Subjective Logic\nto assess trustworthiness of individual data, but not to evaluate\ntrustworthiness properties that emerge only at the level of the dataset as a\nwhole. This paper introduces the first formal framework for assessing the\ntrustworthiness of AI training datasets, enabling uncertainty-aware evaluations\nof global properties such as bias. Built on Subjective Logic, our approach\nsupports trust propositions and quantifies uncertainty in scenarios where\nevidence is incomplete, distributed, and/or conflicting. We instantiate this\nframework on the trustworthiness property of bias, and we experimentally\nevaluate it based on a traffic sign recognition dataset. The results\ndemonstrate that our method captures class imbalance and remains interpretable\nand robust in both centralized and federated contexts.\n", "link": "http://arxiv.org/abs/2508.13813v1", "date": "2025-08-19", "relevancy": 1.5567, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5326}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.503}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Trustworthiness%20of%20AI%20Training%20Dataset%20using%20Subjective%20Logic%0A%20%20--%20A%20Use%20Case%20on%20Bias&body=Title%3A%20Assessing%20Trustworthiness%20of%20AI%20Training%20Dataset%20using%20Subjective%20Logic%0A%20%20--%20A%20Use%20Case%20on%20Bias%0AAuthor%3A%20Koffi%20Ismael%20Ouattara%20and%20Ioannis%20Krontiris%20and%20Theo%20Dimitrakos%20and%20Frank%20Kargl%0AAbstract%3A%20%20%20As%20AI%20systems%20increasingly%20rely%20on%20training%20data%2C%20assessing%20dataset%0Atrustworthiness%20has%20become%20critical%2C%20particularly%20for%20properties%20like%20fairness%0Aor%20bias%20that%20emerge%20at%20the%20dataset%20level.%20Prior%20work%20has%20used%20Subjective%20Logic%0Ato%20assess%20trustworthiness%20of%20individual%20data%2C%20but%20not%20to%20evaluate%0Atrustworthiness%20properties%20that%20emerge%20only%20at%20the%20level%20of%20the%20dataset%20as%20a%0Awhole.%20This%20paper%20introduces%20the%20first%20formal%20framework%20for%20assessing%20the%0Atrustworthiness%20of%20AI%20training%20datasets%2C%20enabling%20uncertainty-aware%20evaluations%0Aof%20global%20properties%20such%20as%20bias.%20Built%20on%20Subjective%20Logic%2C%20our%20approach%0Asupports%20trust%20propositions%20and%20quantifies%20uncertainty%20in%20scenarios%20where%0Aevidence%20is%20incomplete%2C%20distributed%2C%20and/or%20conflicting.%20We%20instantiate%20this%0Aframework%20on%20the%20trustworthiness%20property%20of%20bias%2C%20and%20we%20experimentally%0Aevaluate%20it%20based%20on%20a%20traffic%20sign%20recognition%20dataset.%20The%20results%0Ademonstrate%20that%20our%20method%20captures%20class%20imbalance%20and%20remains%20interpretable%0Aand%20robust%20in%20both%20centralized%20and%20federated%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Trustworthiness%2520of%2520AI%2520Training%2520Dataset%2520using%2520Subjective%2520Logic%250A%2520%2520--%2520A%2520Use%2520Case%2520on%2520Bias%26entry.906535625%3DKoffi%2520Ismael%2520Ouattara%2520and%2520Ioannis%2520Krontiris%2520and%2520Theo%2520Dimitrakos%2520and%2520Frank%2520Kargl%26entry.1292438233%3D%2520%2520As%2520AI%2520systems%2520increasingly%2520rely%2520on%2520training%2520data%252C%2520assessing%2520dataset%250Atrustworthiness%2520has%2520become%2520critical%252C%2520particularly%2520for%2520properties%2520like%2520fairness%250Aor%2520bias%2520that%2520emerge%2520at%2520the%2520dataset%2520level.%2520Prior%2520work%2520has%2520used%2520Subjective%2520Logic%250Ato%2520assess%2520trustworthiness%2520of%2520individual%2520data%252C%2520but%2520not%2520to%2520evaluate%250Atrustworthiness%2520properties%2520that%2520emerge%2520only%2520at%2520the%2520level%2520of%2520the%2520dataset%2520as%2520a%250Awhole.%2520This%2520paper%2520introduces%2520the%2520first%2520formal%2520framework%2520for%2520assessing%2520the%250Atrustworthiness%2520of%2520AI%2520training%2520datasets%252C%2520enabling%2520uncertainty-aware%2520evaluations%250Aof%2520global%2520properties%2520such%2520as%2520bias.%2520Built%2520on%2520Subjective%2520Logic%252C%2520our%2520approach%250Asupports%2520trust%2520propositions%2520and%2520quantifies%2520uncertainty%2520in%2520scenarios%2520where%250Aevidence%2520is%2520incomplete%252C%2520distributed%252C%2520and/or%2520conflicting.%2520We%2520instantiate%2520this%250Aframework%2520on%2520the%2520trustworthiness%2520property%2520of%2520bias%252C%2520and%2520we%2520experimentally%250Aevaluate%2520it%2520based%2520on%2520a%2520traffic%2520sign%2520recognition%2520dataset.%2520The%2520results%250Ademonstrate%2520that%2520our%2520method%2520captures%2520class%2520imbalance%2520and%2520remains%2520interpretable%250Aand%2520robust%2520in%2520both%2520centralized%2520and%2520federated%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Trustworthiness%20of%20AI%20Training%20Dataset%20using%20Subjective%20Logic%0A%20%20--%20A%20Use%20Case%20on%20Bias&entry.906535625=Koffi%20Ismael%20Ouattara%20and%20Ioannis%20Krontiris%20and%20Theo%20Dimitrakos%20and%20Frank%20Kargl&entry.1292438233=%20%20As%20AI%20systems%20increasingly%20rely%20on%20training%20data%2C%20assessing%20dataset%0Atrustworthiness%20has%20become%20critical%2C%20particularly%20for%20properties%20like%20fairness%0Aor%20bias%20that%20emerge%20at%20the%20dataset%20level.%20Prior%20work%20has%20used%20Subjective%20Logic%0Ato%20assess%20trustworthiness%20of%20individual%20data%2C%20but%20not%20to%20evaluate%0Atrustworthiness%20properties%20that%20emerge%20only%20at%20the%20level%20of%20the%20dataset%20as%20a%0Awhole.%20This%20paper%20introduces%20the%20first%20formal%20framework%20for%20assessing%20the%0Atrustworthiness%20of%20AI%20training%20datasets%2C%20enabling%20uncertainty-aware%20evaluations%0Aof%20global%20properties%20such%20as%20bias.%20Built%20on%20Subjective%20Logic%2C%20our%20approach%0Asupports%20trust%20propositions%20and%20quantifies%20uncertainty%20in%20scenarios%20where%0Aevidence%20is%20incomplete%2C%20distributed%2C%20and/or%20conflicting.%20We%20instantiate%20this%0Aframework%20on%20the%20trustworthiness%20property%20of%20bias%2C%20and%20we%20experimentally%0Aevaluate%20it%20based%20on%20a%20traffic%20sign%20recognition%20dataset.%20The%20results%0Ademonstrate%20that%20our%20method%20captures%20class%20imbalance%20and%20remains%20interpretable%0Aand%20robust%20in%20both%20centralized%20and%20federated%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13813v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


