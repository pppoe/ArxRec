<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251118.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting", "author": "Yi-Hsin Li and Thomas Sikora and Sebastian Knorr and M\u00e5rten Sj\u00f6str\u00f6m", "abstract": "Sparse-view synthesis remains a challenging problem due to the difficulty of recovering accurate geometry and appearance from limited observations. While recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time rendering with competitive quality, existing pipelines often rely on Structure-from-Motion (SfM) for camera pose estimation, an approach that struggles in genuinely sparse-view settings. Moreover, several SfM-free methods replace SfM with multi-view stereo (MVS) models, but generate massive numbers of 3D Gaussians by back-projecting every pixel into 3D space, leading to high memory costs. We propose Segmentation-Driven Initialization for Gaussian Splatting (SDI-GS), a method that mitigates inefficiency by leveraging region-based segmentation to identify and retain only structurally significant regions. This enables selective downsampling of the dense point cloud, preserving scene fidelity while substantially reducing Gaussian count. Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count by up to 50% and achieves comparable or superior rendering quality in PSNR and SSIM, with only marginal degradation in LPIPS. It further enables faster training and lower memory footprint, advancing the practicality of 3DGS for constrained-view scenarios.", "link": "http://arxiv.org/abs/2509.11853v2", "date": "2025-11-18", "relevancy": 3.4878, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.731}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6843}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmentation-Driven%20Initialization%20for%20Sparse-view%203D%20Gaussian%20Splatting&body=Title%3A%20Segmentation-Driven%20Initialization%20for%20Sparse-view%203D%20Gaussian%20Splatting%0AAuthor%3A%20Yi-Hsin%20Li%20and%20Thomas%20Sikora%20and%20Sebastian%20Knorr%20and%20M%C3%A5rten%20Sj%C3%B6str%C3%B6m%0AAbstract%3A%20Sparse-view%20synthesis%20remains%20a%20challenging%20problem%20due%20to%20the%20difficulty%20of%20recovering%20accurate%20geometry%20and%20appearance%20from%20limited%20observations.%20While%20recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20enabled%20real-time%20rendering%20with%20competitive%20quality%2C%20existing%20pipelines%20often%20rely%20on%20Structure-from-Motion%20%28SfM%29%20for%20camera%20pose%20estimation%2C%20an%20approach%20that%20struggles%20in%20genuinely%20sparse-view%20settings.%20Moreover%2C%20several%20SfM-free%20methods%20replace%20SfM%20with%20multi-view%20stereo%20%28MVS%29%20models%2C%20but%20generate%20massive%20numbers%20of%203D%20Gaussians%20by%20back-projecting%20every%20pixel%20into%203D%20space%2C%20leading%20to%20high%20memory%20costs.%20We%20propose%20Segmentation-Driven%20Initialization%20for%20Gaussian%20Splatting%20%28SDI-GS%29%2C%20a%20method%20that%20mitigates%20inefficiency%20by%20leveraging%20region-based%20segmentation%20to%20identify%20and%20retain%20only%20structurally%20significant%20regions.%20This%20enables%20selective%20downsampling%20of%20the%20dense%20point%20cloud%2C%20preserving%20scene%20fidelity%20while%20substantially%20reducing%20Gaussian%20count.%20Experiments%20across%20diverse%20benchmarks%20show%20that%20SDI-GS%20reduces%20Gaussian%20count%20by%20up%20to%2050%25%20and%20achieves%20comparable%20or%20superior%20rendering%20quality%20in%20PSNR%20and%20SSIM%2C%20with%20only%20marginal%20degradation%20in%20LPIPS.%20It%20further%20enables%20faster%20training%20and%20lower%20memory%20footprint%2C%20advancing%20the%20practicality%20of%203DGS%20for%20constrained-view%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2509.11853v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentation-Driven%2520Initialization%2520for%2520Sparse-view%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DYi-Hsin%2520Li%2520and%2520Thomas%2520Sikora%2520and%2520Sebastian%2520Knorr%2520and%2520M%25C3%25A5rten%2520Sj%25C3%25B6str%25C3%25B6m%26entry.1292438233%3DSparse-view%2520synthesis%2520remains%2520a%2520challenging%2520problem%2520due%2520to%2520the%2520difficulty%2520of%2520recovering%2520accurate%2520geometry%2520and%2520appearance%2520from%2520limited%2520observations.%2520While%2520recent%2520advances%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%2520enabled%2520real-time%2520rendering%2520with%2520competitive%2520quality%252C%2520existing%2520pipelines%2520often%2520rely%2520on%2520Structure-from-Motion%2520%2528SfM%2529%2520for%2520camera%2520pose%2520estimation%252C%2520an%2520approach%2520that%2520struggles%2520in%2520genuinely%2520sparse-view%2520settings.%2520Moreover%252C%2520several%2520SfM-free%2520methods%2520replace%2520SfM%2520with%2520multi-view%2520stereo%2520%2528MVS%2529%2520models%252C%2520but%2520generate%2520massive%2520numbers%2520of%25203D%2520Gaussians%2520by%2520back-projecting%2520every%2520pixel%2520into%25203D%2520space%252C%2520leading%2520to%2520high%2520memory%2520costs.%2520We%2520propose%2520Segmentation-Driven%2520Initialization%2520for%2520Gaussian%2520Splatting%2520%2528SDI-GS%2529%252C%2520a%2520method%2520that%2520mitigates%2520inefficiency%2520by%2520leveraging%2520region-based%2520segmentation%2520to%2520identify%2520and%2520retain%2520only%2520structurally%2520significant%2520regions.%2520This%2520enables%2520selective%2520downsampling%2520of%2520the%2520dense%2520point%2520cloud%252C%2520preserving%2520scene%2520fidelity%2520while%2520substantially%2520reducing%2520Gaussian%2520count.%2520Experiments%2520across%2520diverse%2520benchmarks%2520show%2520that%2520SDI-GS%2520reduces%2520Gaussian%2520count%2520by%2520up%2520to%252050%2525%2520and%2520achieves%2520comparable%2520or%2520superior%2520rendering%2520quality%2520in%2520PSNR%2520and%2520SSIM%252C%2520with%2520only%2520marginal%2520degradation%2520in%2520LPIPS.%2520It%2520further%2520enables%2520faster%2520training%2520and%2520lower%2520memory%2520footprint%252C%2520advancing%2520the%2520practicality%2520of%25203DGS%2520for%2520constrained-view%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11853v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation-Driven%20Initialization%20for%20Sparse-view%203D%20Gaussian%20Splatting&entry.906535625=Yi-Hsin%20Li%20and%20Thomas%20Sikora%20and%20Sebastian%20Knorr%20and%20M%C3%A5rten%20Sj%C3%B6str%C3%B6m&entry.1292438233=Sparse-view%20synthesis%20remains%20a%20challenging%20problem%20due%20to%20the%20difficulty%20of%20recovering%20accurate%20geometry%20and%20appearance%20from%20limited%20observations.%20While%20recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20enabled%20real-time%20rendering%20with%20competitive%20quality%2C%20existing%20pipelines%20often%20rely%20on%20Structure-from-Motion%20%28SfM%29%20for%20camera%20pose%20estimation%2C%20an%20approach%20that%20struggles%20in%20genuinely%20sparse-view%20settings.%20Moreover%2C%20several%20SfM-free%20methods%20replace%20SfM%20with%20multi-view%20stereo%20%28MVS%29%20models%2C%20but%20generate%20massive%20numbers%20of%203D%20Gaussians%20by%20back-projecting%20every%20pixel%20into%203D%20space%2C%20leading%20to%20high%20memory%20costs.%20We%20propose%20Segmentation-Driven%20Initialization%20for%20Gaussian%20Splatting%20%28SDI-GS%29%2C%20a%20method%20that%20mitigates%20inefficiency%20by%20leveraging%20region-based%20segmentation%20to%20identify%20and%20retain%20only%20structurally%20significant%20regions.%20This%20enables%20selective%20downsampling%20of%20the%20dense%20point%20cloud%2C%20preserving%20scene%20fidelity%20while%20substantially%20reducing%20Gaussian%20count.%20Experiments%20across%20diverse%20benchmarks%20show%20that%20SDI-GS%20reduces%20Gaussian%20count%20by%20up%20to%2050%25%20and%20achieves%20comparable%20or%20superior%20rendering%20quality%20in%20PSNR%20and%20SSIM%2C%20with%20only%20marginal%20degradation%20in%20LPIPS.%20It%20further%20enables%20faster%20training%20and%20lower%20memory%20footprint%2C%20advancing%20the%20practicality%20of%203DGS%20for%20constrained-view%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2509.11853v2&entry.124074799=Read"},
{"title": "IBGS: Image-Based Gaussian Splatting", "author": "Hoang Chuong Nguyen and Wei Mao and Jose M. Alvarez and Miaomiao Liu", "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a fast, high-quality method for novel view synthesis (NVS). However, its use of low-degree spherical harmonics limits its ability to capture spatially varying color and view-dependent effects such as specular highlights. Existing works augment Gaussians with either a global texture map, which struggles with complex scenes, or per-Gaussian texture maps, which introduces high storage overhead. We propose Image-Based Gaussian Splatting, an efficient alternative that leverages high-resolution source images for fine details and view-specific color modeling. Specifically, we model each pixel color as a combination of a base color from standard 3DGS rendering and a learned residual inferred from neighboring training images. This promotes accurate surface alignment and enables rendering images of high-frequency details and accurate view-dependent effects. Experiments on standard NVS benchmarks show that our method significantly outperforms prior Gaussian Splatting approaches in rendering quality, without increasing the storage footprint.", "link": "http://arxiv.org/abs/2511.14357v1", "date": "2025-11-18", "relevancy": 3.4567, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7077}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6884}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IBGS%3A%20Image-Based%20Gaussian%20Splatting&body=Title%3A%20IBGS%3A%20Image-Based%20Gaussian%20Splatting%0AAuthor%3A%20Hoang%20Chuong%20Nguyen%20and%20Wei%20Mao%20and%20Jose%20M.%20Alvarez%20and%20Miaomiao%20Liu%0AAbstract%3A%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20emerged%20as%20a%20fast%2C%20high-quality%20method%20for%20novel%20view%20synthesis%20%28NVS%29.%20However%2C%20its%20use%20of%20low-degree%20spherical%20harmonics%20limits%20its%20ability%20to%20capture%20spatially%20varying%20color%20and%20view-dependent%20effects%20such%20as%20specular%20highlights.%20Existing%20works%20augment%20Gaussians%20with%20either%20a%20global%20texture%20map%2C%20which%20struggles%20with%20complex%20scenes%2C%20or%20per-Gaussian%20texture%20maps%2C%20which%20introduces%20high%20storage%20overhead.%20We%20propose%20Image-Based%20Gaussian%20Splatting%2C%20an%20efficient%20alternative%20that%20leverages%20high-resolution%20source%20images%20for%20fine%20details%20and%20view-specific%20color%20modeling.%20Specifically%2C%20we%20model%20each%20pixel%20color%20as%20a%20combination%20of%20a%20base%20color%20from%20standard%203DGS%20rendering%20and%20a%20learned%20residual%20inferred%20from%20neighboring%20training%20images.%20This%20promotes%20accurate%20surface%20alignment%20and%20enables%20rendering%20images%20of%20high-frequency%20details%20and%20accurate%20view-dependent%20effects.%20Experiments%20on%20standard%20NVS%20benchmarks%20show%20that%20our%20method%20significantly%20outperforms%20prior%20Gaussian%20Splatting%20approaches%20in%20rendering%20quality%2C%20without%20increasing%20the%20storage%20footprint.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIBGS%253A%2520Image-Based%2520Gaussian%2520Splatting%26entry.906535625%3DHoang%2520Chuong%2520Nguyen%2520and%2520Wei%2520Mao%2520and%2520Jose%2520M.%2520Alvarez%2520and%2520Miaomiao%2520Liu%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520recently%2520emerged%2520as%2520a%2520fast%252C%2520high-quality%2520method%2520for%2520novel%2520view%2520synthesis%2520%2528NVS%2529.%2520However%252C%2520its%2520use%2520of%2520low-degree%2520spherical%2520harmonics%2520limits%2520its%2520ability%2520to%2520capture%2520spatially%2520varying%2520color%2520and%2520view-dependent%2520effects%2520such%2520as%2520specular%2520highlights.%2520Existing%2520works%2520augment%2520Gaussians%2520with%2520either%2520a%2520global%2520texture%2520map%252C%2520which%2520struggles%2520with%2520complex%2520scenes%252C%2520or%2520per-Gaussian%2520texture%2520maps%252C%2520which%2520introduces%2520high%2520storage%2520overhead.%2520We%2520propose%2520Image-Based%2520Gaussian%2520Splatting%252C%2520an%2520efficient%2520alternative%2520that%2520leverages%2520high-resolution%2520source%2520images%2520for%2520fine%2520details%2520and%2520view-specific%2520color%2520modeling.%2520Specifically%252C%2520we%2520model%2520each%2520pixel%2520color%2520as%2520a%2520combination%2520of%2520a%2520base%2520color%2520from%2520standard%25203DGS%2520rendering%2520and%2520a%2520learned%2520residual%2520inferred%2520from%2520neighboring%2520training%2520images.%2520This%2520promotes%2520accurate%2520surface%2520alignment%2520and%2520enables%2520rendering%2520images%2520of%2520high-frequency%2520details%2520and%2520accurate%2520view-dependent%2520effects.%2520Experiments%2520on%2520standard%2520NVS%2520benchmarks%2520show%2520that%2520our%2520method%2520significantly%2520outperforms%2520prior%2520Gaussian%2520Splatting%2520approaches%2520in%2520rendering%2520quality%252C%2520without%2520increasing%2520the%2520storage%2520footprint.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IBGS%3A%20Image-Based%20Gaussian%20Splatting&entry.906535625=Hoang%20Chuong%20Nguyen%20and%20Wei%20Mao%20and%20Jose%20M.%20Alvarez%20and%20Miaomiao%20Liu&entry.1292438233=3D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20emerged%20as%20a%20fast%2C%20high-quality%20method%20for%20novel%20view%20synthesis%20%28NVS%29.%20However%2C%20its%20use%20of%20low-degree%20spherical%20harmonics%20limits%20its%20ability%20to%20capture%20spatially%20varying%20color%20and%20view-dependent%20effects%20such%20as%20specular%20highlights.%20Existing%20works%20augment%20Gaussians%20with%20either%20a%20global%20texture%20map%2C%20which%20struggles%20with%20complex%20scenes%2C%20or%20per-Gaussian%20texture%20maps%2C%20which%20introduces%20high%20storage%20overhead.%20We%20propose%20Image-Based%20Gaussian%20Splatting%2C%20an%20efficient%20alternative%20that%20leverages%20high-resolution%20source%20images%20for%20fine%20details%20and%20view-specific%20color%20modeling.%20Specifically%2C%20we%20model%20each%20pixel%20color%20as%20a%20combination%20of%20a%20base%20color%20from%20standard%203DGS%20rendering%20and%20a%20learned%20residual%20inferred%20from%20neighboring%20training%20images.%20This%20promotes%20accurate%20surface%20alignment%20and%20enables%20rendering%20images%20of%20high-frequency%20details%20and%20accurate%20view-dependent%20effects.%20Experiments%20on%20standard%20NVS%20benchmarks%20show%20that%20our%20method%20significantly%20outperforms%20prior%20Gaussian%20Splatting%20approaches%20in%20rendering%20quality%2C%20without%20increasing%20the%20storage%20footprint.&entry.1838667208=http%3A//arxiv.org/abs/2511.14357v1&entry.124074799=Read"},
{"title": "SpeeDe3DGS: Speedy Deformable 3D Gaussian Splatting with Temporal Pruning and Motion Grouping", "author": "Allen Tu and Haiyang Ying and Alex Hanson and Yonghan Lee and Tom Goldstein and Matthias Zwicker", "abstract": "Dynamic extensions of 3D Gaussian Splatting (3DGS) achieve high-quality reconstructions through neural motion fields, but per-Gaussian neural inference makes these models computationally expensive. Building on DeformableGS, we introduce Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), which bridges this efficiency-fidelity gap through three complementary modules: Temporal Sensitivity Pruning (TSP) removes low-impact Gaussians via temporally aggregated sensitivity analysis, Temporal Sensitivity Sampling (TSS) perturbs timestamps to suppress floaters and improve temporal coherence, and GroupFlow distills the learned deformation field into shared SE(3) transformations for efficient groupwise motion. On the 50 dynamic scenes in MonoDyGauBench, integrating TSP and TSS into DeformableGS accelerates rendering by 6.78$\\times$ on average while maintaining neural-field fidelity and using 10$\\times$ fewer primitives. Adding GroupFlow culminates in 13.71$\\times$ faster rendering and 2.53$\\times$ shorter training, surpassing all baselines in speed while preserving superior image quality.", "link": "http://arxiv.org/abs/2506.07917v2", "date": "2025-11-18", "relevancy": 3.3115, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6649}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6634}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpeeDe3DGS%3A%20Speedy%20Deformable%203D%20Gaussian%20Splatting%20with%20Temporal%20Pruning%20and%20Motion%20Grouping&body=Title%3A%20SpeeDe3DGS%3A%20Speedy%20Deformable%203D%20Gaussian%20Splatting%20with%20Temporal%20Pruning%20and%20Motion%20Grouping%0AAuthor%3A%20Allen%20Tu%20and%20Haiyang%20Ying%20and%20Alex%20Hanson%20and%20Yonghan%20Lee%20and%20Tom%20Goldstein%20and%20Matthias%20Zwicker%0AAbstract%3A%20Dynamic%20extensions%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20achieve%20high-quality%20reconstructions%20through%20neural%20motion%20fields%2C%20but%20per-Gaussian%20neural%20inference%20makes%20these%20models%20computationally%20expensive.%20Building%20on%20DeformableGS%2C%20we%20introduce%20Speedy%20Deformable%203D%20Gaussian%20Splatting%20%28SpeeDe3DGS%29%2C%20which%20bridges%20this%20efficiency-fidelity%20gap%20through%20three%20complementary%20modules%3A%20Temporal%20Sensitivity%20Pruning%20%28TSP%29%20removes%20low-impact%20Gaussians%20via%20temporally%20aggregated%20sensitivity%20analysis%2C%20Temporal%20Sensitivity%20Sampling%20%28TSS%29%20perturbs%20timestamps%20to%20suppress%20floaters%20and%20improve%20temporal%20coherence%2C%20and%20GroupFlow%20distills%20the%20learned%20deformation%20field%20into%20shared%20SE%283%29%20transformations%20for%20efficient%20groupwise%20motion.%20On%20the%2050%20dynamic%20scenes%20in%20MonoDyGauBench%2C%20integrating%20TSP%20and%20TSS%20into%20DeformableGS%20accelerates%20rendering%20by%206.78%24%5Ctimes%24%20on%20average%20while%20maintaining%20neural-field%20fidelity%20and%20using%2010%24%5Ctimes%24%20fewer%20primitives.%20Adding%20GroupFlow%20culminates%20in%2013.71%24%5Ctimes%24%20faster%20rendering%20and%202.53%24%5Ctimes%24%20shorter%20training%2C%20surpassing%20all%20baselines%20in%20speed%20while%20preserving%20superior%20image%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2506.07917v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeeDe3DGS%253A%2520Speedy%2520Deformable%25203D%2520Gaussian%2520Splatting%2520with%2520Temporal%2520Pruning%2520and%2520Motion%2520Grouping%26entry.906535625%3DAllen%2520Tu%2520and%2520Haiyang%2520Ying%2520and%2520Alex%2520Hanson%2520and%2520Yonghan%2520Lee%2520and%2520Tom%2520Goldstein%2520and%2520Matthias%2520Zwicker%26entry.1292438233%3DDynamic%2520extensions%2520of%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520achieve%2520high-quality%2520reconstructions%2520through%2520neural%2520motion%2520fields%252C%2520but%2520per-Gaussian%2520neural%2520inference%2520makes%2520these%2520models%2520computationally%2520expensive.%2520Building%2520on%2520DeformableGS%252C%2520we%2520introduce%2520Speedy%2520Deformable%25203D%2520Gaussian%2520Splatting%2520%2528SpeeDe3DGS%2529%252C%2520which%2520bridges%2520this%2520efficiency-fidelity%2520gap%2520through%2520three%2520complementary%2520modules%253A%2520Temporal%2520Sensitivity%2520Pruning%2520%2528TSP%2529%2520removes%2520low-impact%2520Gaussians%2520via%2520temporally%2520aggregated%2520sensitivity%2520analysis%252C%2520Temporal%2520Sensitivity%2520Sampling%2520%2528TSS%2529%2520perturbs%2520timestamps%2520to%2520suppress%2520floaters%2520and%2520improve%2520temporal%2520coherence%252C%2520and%2520GroupFlow%2520distills%2520the%2520learned%2520deformation%2520field%2520into%2520shared%2520SE%25283%2529%2520transformations%2520for%2520efficient%2520groupwise%2520motion.%2520On%2520the%252050%2520dynamic%2520scenes%2520in%2520MonoDyGauBench%252C%2520integrating%2520TSP%2520and%2520TSS%2520into%2520DeformableGS%2520accelerates%2520rendering%2520by%25206.78%2524%255Ctimes%2524%2520on%2520average%2520while%2520maintaining%2520neural-field%2520fidelity%2520and%2520using%252010%2524%255Ctimes%2524%2520fewer%2520primitives.%2520Adding%2520GroupFlow%2520culminates%2520in%252013.71%2524%255Ctimes%2524%2520faster%2520rendering%2520and%25202.53%2524%255Ctimes%2524%2520shorter%2520training%252C%2520surpassing%2520all%2520baselines%2520in%2520speed%2520while%2520preserving%2520superior%2520image%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07917v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpeeDe3DGS%3A%20Speedy%20Deformable%203D%20Gaussian%20Splatting%20with%20Temporal%20Pruning%20and%20Motion%20Grouping&entry.906535625=Allen%20Tu%20and%20Haiyang%20Ying%20and%20Alex%20Hanson%20and%20Yonghan%20Lee%20and%20Tom%20Goldstein%20and%20Matthias%20Zwicker&entry.1292438233=Dynamic%20extensions%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20achieve%20high-quality%20reconstructions%20through%20neural%20motion%20fields%2C%20but%20per-Gaussian%20neural%20inference%20makes%20these%20models%20computationally%20expensive.%20Building%20on%20DeformableGS%2C%20we%20introduce%20Speedy%20Deformable%203D%20Gaussian%20Splatting%20%28SpeeDe3DGS%29%2C%20which%20bridges%20this%20efficiency-fidelity%20gap%20through%20three%20complementary%20modules%3A%20Temporal%20Sensitivity%20Pruning%20%28TSP%29%20removes%20low-impact%20Gaussians%20via%20temporally%20aggregated%20sensitivity%20analysis%2C%20Temporal%20Sensitivity%20Sampling%20%28TSS%29%20perturbs%20timestamps%20to%20suppress%20floaters%20and%20improve%20temporal%20coherence%2C%20and%20GroupFlow%20distills%20the%20learned%20deformation%20field%20into%20shared%20SE%283%29%20transformations%20for%20efficient%20groupwise%20motion.%20On%20the%2050%20dynamic%20scenes%20in%20MonoDyGauBench%2C%20integrating%20TSP%20and%20TSS%20into%20DeformableGS%20accelerates%20rendering%20by%206.78%24%5Ctimes%24%20on%20average%20while%20maintaining%20neural-field%20fidelity%20and%20using%2010%24%5Ctimes%24%20fewer%20primitives.%20Adding%20GroupFlow%20culminates%20in%2013.71%24%5Ctimes%24%20faster%20rendering%20and%202.53%24%5Ctimes%24%20shorter%20training%2C%20surpassing%20all%20baselines%20in%20speed%20while%20preserving%20superior%20image%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2506.07917v2&entry.124074799=Read"},
{"title": "SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction", "author": "Meiying Gu and Jiawei Zhang and Jiahe Li and Xiaohan Yu and Haonan Luo and Jin Zheng and Xiao Bai", "abstract": "Recent advances in optimizing Gaussian Splatting for scene geometry have enabled efficient reconstruction of detailed surfaces from images. However, when input views are sparse, such optimization is prone to overfitting, leading to suboptimal reconstruction quality. Existing approaches address this challenge by employing flattened Gaussian primitives to better fit surface geometry, combined with depth regularization to alleviate geometric ambiguities under limited viewpoints. Nevertheless, the increased anisotropy inherent in flattened Gaussians exacerbates overfitting in sparse-view scenarios, hindering accurate surface fitting and degrading novel view synthesis performance. In this paper, we propose \\net{}, a method that reconstructs more accurate and detailed surfaces while preserving high-quality novel view rendering. Our key insight is to introduce Stereo Geometry-Texture Alignment, which bridges rendering quality and geometry estimation, thereby jointly enhancing both surface reconstruction and view synthesis. In addition, we present a Pseudo-Feature Enhanced Geometry Consistency that enforces multi-view geometric consistency by incorporating both training and unseen views, effectively mitigating overfitting caused by sparse supervision. Extensive experiments on the DTU, BlendedMVS, and Mip-NeRF360 datasets demonstrate that our method achieves the state-of-the-art performance.", "link": "http://arxiv.org/abs/2511.14633v1", "date": "2025-11-18", "relevancy": 3.3015, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6882}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6471}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparseSurf%3A%20Sparse-View%203D%20Gaussian%20Splatting%20for%20Surface%20Reconstruction&body=Title%3A%20SparseSurf%3A%20Sparse-View%203D%20Gaussian%20Splatting%20for%20Surface%20Reconstruction%0AAuthor%3A%20Meiying%20Gu%20and%20Jiawei%20Zhang%20and%20Jiahe%20Li%20and%20Xiaohan%20Yu%20and%20Haonan%20Luo%20and%20Jin%20Zheng%20and%20Xiao%20Bai%0AAbstract%3A%20Recent%20advances%20in%20optimizing%20Gaussian%20Splatting%20for%20scene%20geometry%20have%20enabled%20efficient%20reconstruction%20of%20detailed%20surfaces%20from%20images.%20However%2C%20when%20input%20views%20are%20sparse%2C%20such%20optimization%20is%20prone%20to%20overfitting%2C%20leading%20to%20suboptimal%20reconstruction%20quality.%20Existing%20approaches%20address%20this%20challenge%20by%20employing%20flattened%20Gaussian%20primitives%20to%20better%20fit%20surface%20geometry%2C%20combined%20with%20depth%20regularization%20to%20alleviate%20geometric%20ambiguities%20under%20limited%20viewpoints.%20Nevertheless%2C%20the%20increased%20anisotropy%20inherent%20in%20flattened%20Gaussians%20exacerbates%20overfitting%20in%20sparse-view%20scenarios%2C%20hindering%20accurate%20surface%20fitting%20and%20degrading%20novel%20view%20synthesis%20performance.%20In%20this%20paper%2C%20we%20propose%20%5Cnet%7B%7D%2C%20a%20method%20that%20reconstructs%20more%20accurate%20and%20detailed%20surfaces%20while%20preserving%20high-quality%20novel%20view%20rendering.%20Our%20key%20insight%20is%20to%20introduce%20Stereo%20Geometry-Texture%20Alignment%2C%20which%20bridges%20rendering%20quality%20and%20geometry%20estimation%2C%20thereby%20jointly%20enhancing%20both%20surface%20reconstruction%20and%20view%20synthesis.%20In%20addition%2C%20we%20present%20a%20Pseudo-Feature%20Enhanced%20Geometry%20Consistency%20that%20enforces%20multi-view%20geometric%20consistency%20by%20incorporating%20both%20training%20and%20unseen%20views%2C%20effectively%20mitigating%20overfitting%20caused%20by%20sparse%20supervision.%20Extensive%20experiments%20on%20the%20DTU%2C%20BlendedMVS%2C%20and%20Mip-NeRF360%20datasets%20demonstrate%20that%20our%20method%20achieves%20the%20state-of-the-art%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparseSurf%253A%2520Sparse-View%25203D%2520Gaussian%2520Splatting%2520for%2520Surface%2520Reconstruction%26entry.906535625%3DMeiying%2520Gu%2520and%2520Jiawei%2520Zhang%2520and%2520Jiahe%2520Li%2520and%2520Xiaohan%2520Yu%2520and%2520Haonan%2520Luo%2520and%2520Jin%2520Zheng%2520and%2520Xiao%2520Bai%26entry.1292438233%3DRecent%2520advances%2520in%2520optimizing%2520Gaussian%2520Splatting%2520for%2520scene%2520geometry%2520have%2520enabled%2520efficient%2520reconstruction%2520of%2520detailed%2520surfaces%2520from%2520images.%2520However%252C%2520when%2520input%2520views%2520are%2520sparse%252C%2520such%2520optimization%2520is%2520prone%2520to%2520overfitting%252C%2520leading%2520to%2520suboptimal%2520reconstruction%2520quality.%2520Existing%2520approaches%2520address%2520this%2520challenge%2520by%2520employing%2520flattened%2520Gaussian%2520primitives%2520to%2520better%2520fit%2520surface%2520geometry%252C%2520combined%2520with%2520depth%2520regularization%2520to%2520alleviate%2520geometric%2520ambiguities%2520under%2520limited%2520viewpoints.%2520Nevertheless%252C%2520the%2520increased%2520anisotropy%2520inherent%2520in%2520flattened%2520Gaussians%2520exacerbates%2520overfitting%2520in%2520sparse-view%2520scenarios%252C%2520hindering%2520accurate%2520surface%2520fitting%2520and%2520degrading%2520novel%2520view%2520synthesis%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520%255Cnet%257B%257D%252C%2520a%2520method%2520that%2520reconstructs%2520more%2520accurate%2520and%2520detailed%2520surfaces%2520while%2520preserving%2520high-quality%2520novel%2520view%2520rendering.%2520Our%2520key%2520insight%2520is%2520to%2520introduce%2520Stereo%2520Geometry-Texture%2520Alignment%252C%2520which%2520bridges%2520rendering%2520quality%2520and%2520geometry%2520estimation%252C%2520thereby%2520jointly%2520enhancing%2520both%2520surface%2520reconstruction%2520and%2520view%2520synthesis.%2520In%2520addition%252C%2520we%2520present%2520a%2520Pseudo-Feature%2520Enhanced%2520Geometry%2520Consistency%2520that%2520enforces%2520multi-view%2520geometric%2520consistency%2520by%2520incorporating%2520both%2520training%2520and%2520unseen%2520views%252C%2520effectively%2520mitigating%2520overfitting%2520caused%2520by%2520sparse%2520supervision.%2520Extensive%2520experiments%2520on%2520the%2520DTU%252C%2520BlendedMVS%252C%2520and%2520Mip-NeRF360%2520datasets%2520demonstrate%2520that%2520our%2520method%2520achieves%2520the%2520state-of-the-art%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparseSurf%3A%20Sparse-View%203D%20Gaussian%20Splatting%20for%20Surface%20Reconstruction&entry.906535625=Meiying%20Gu%20and%20Jiawei%20Zhang%20and%20Jiahe%20Li%20and%20Xiaohan%20Yu%20and%20Haonan%20Luo%20and%20Jin%20Zheng%20and%20Xiao%20Bai&entry.1292438233=Recent%20advances%20in%20optimizing%20Gaussian%20Splatting%20for%20scene%20geometry%20have%20enabled%20efficient%20reconstruction%20of%20detailed%20surfaces%20from%20images.%20However%2C%20when%20input%20views%20are%20sparse%2C%20such%20optimization%20is%20prone%20to%20overfitting%2C%20leading%20to%20suboptimal%20reconstruction%20quality.%20Existing%20approaches%20address%20this%20challenge%20by%20employing%20flattened%20Gaussian%20primitives%20to%20better%20fit%20surface%20geometry%2C%20combined%20with%20depth%20regularization%20to%20alleviate%20geometric%20ambiguities%20under%20limited%20viewpoints.%20Nevertheless%2C%20the%20increased%20anisotropy%20inherent%20in%20flattened%20Gaussians%20exacerbates%20overfitting%20in%20sparse-view%20scenarios%2C%20hindering%20accurate%20surface%20fitting%20and%20degrading%20novel%20view%20synthesis%20performance.%20In%20this%20paper%2C%20we%20propose%20%5Cnet%7B%7D%2C%20a%20method%20that%20reconstructs%20more%20accurate%20and%20detailed%20surfaces%20while%20preserving%20high-quality%20novel%20view%20rendering.%20Our%20key%20insight%20is%20to%20introduce%20Stereo%20Geometry-Texture%20Alignment%2C%20which%20bridges%20rendering%20quality%20and%20geometry%20estimation%2C%20thereby%20jointly%20enhancing%20both%20surface%20reconstruction%20and%20view%20synthesis.%20In%20addition%2C%20we%20present%20a%20Pseudo-Feature%20Enhanced%20Geometry%20Consistency%20that%20enforces%20multi-view%20geometric%20consistency%20by%20incorporating%20both%20training%20and%20unseen%20views%2C%20effectively%20mitigating%20overfitting%20caused%20by%20sparse%20supervision.%20Extensive%20experiments%20on%20the%20DTU%2C%20BlendedMVS%2C%20and%20Mip-NeRF360%20datasets%20demonstrate%20that%20our%20method%20achieves%20the%20state-of-the-art%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2511.14633v1&entry.124074799=Read"},
{"title": "GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction", "author": "Jiaqi Wu and Yaosen Chen and Shuyuan Zhu", "abstract": "Multi-view image generation holds significant application value in computer vision, particularly in domains like 3D reconstruction, virtual reality, and augmented reality. Most existing methods, which rely on extending single images, face notable computational challenges in maintaining cross-view consistency and generating high-resolution outputs. To address these issues, we propose the Geometry-guided Multi-View Diffusion Model, which incorporates mechanisms for extracting multi-view geometric information and adjusting the intensity of geometric features to generate images that are both consistent across views and rich in detail. Specifically, we design a multi-view geometry information extraction module that leverages depth maps, normal maps, and foreground segmentation masks to construct a shared geometric structure, ensuring shape and structural consistency across different views. To enhance consistency and detail restoration during generation, we develop a decoupled geometry-enhanced attention mechanism that strengthens feature focus on key geometric details, thereby improving overall image quality and detail preservation. Furthermore, we apply an adaptive learning strategy that fine-tunes the model to better capture spatial relationships and visual coherence between the generated views, ensuring realistic results. Our model also incorporates an iterative refinement process that progressively improves the output quality through multiple stages of image generation. Finally, a dynamic geometry information intensity adjustment mechanism is proposed to adaptively regulate the influence of geometric data, optimizing overall quality while ensuring the naturalness of generated images. More details can be found on the project page: https://sobeymil.github.io/GeoMVD.com.", "link": "http://arxiv.org/abs/2511.12204v2", "date": "2025-11-18", "relevancy": 3.2643, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.666}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.666}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoMVD%3A%20Geometry-Enhanced%20Multi-View%20Generation%20Model%20Based%20on%20Geometric%20Information%20Extraction&body=Title%3A%20GeoMVD%3A%20Geometry-Enhanced%20Multi-View%20Generation%20Model%20Based%20on%20Geometric%20Information%20Extraction%0AAuthor%3A%20Jiaqi%20Wu%20and%20Yaosen%20Chen%20and%20Shuyuan%20Zhu%0AAbstract%3A%20Multi-view%20image%20generation%20holds%20significant%20application%20value%20in%20computer%20vision%2C%20particularly%20in%20domains%20like%203D%20reconstruction%2C%20virtual%20reality%2C%20and%20augmented%20reality.%20Most%20existing%20methods%2C%20which%20rely%20on%20extending%20single%20images%2C%20face%20notable%20computational%20challenges%20in%20maintaining%20cross-view%20consistency%20and%20generating%20high-resolution%20outputs.%20To%20address%20these%20issues%2C%20we%20propose%20the%20Geometry-guided%20Multi-View%20Diffusion%20Model%2C%20which%20incorporates%20mechanisms%20for%20extracting%20multi-view%20geometric%20information%20and%20adjusting%20the%20intensity%20of%20geometric%20features%20to%20generate%20images%20that%20are%20both%20consistent%20across%20views%20and%20rich%20in%20detail.%20Specifically%2C%20we%20design%20a%20multi-view%20geometry%20information%20extraction%20module%20that%20leverages%20depth%20maps%2C%20normal%20maps%2C%20and%20foreground%20segmentation%20masks%20to%20construct%20a%20shared%20geometric%20structure%2C%20ensuring%20shape%20and%20structural%20consistency%20across%20different%20views.%20To%20enhance%20consistency%20and%20detail%20restoration%20during%20generation%2C%20we%20develop%20a%20decoupled%20geometry-enhanced%20attention%20mechanism%20that%20strengthens%20feature%20focus%20on%20key%20geometric%20details%2C%20thereby%20improving%20overall%20image%20quality%20and%20detail%20preservation.%20Furthermore%2C%20we%20apply%20an%20adaptive%20learning%20strategy%20that%20fine-tunes%20the%20model%20to%20better%20capture%20spatial%20relationships%20and%20visual%20coherence%20between%20the%20generated%20views%2C%20ensuring%20realistic%20results.%20Our%20model%20also%20incorporates%20an%20iterative%20refinement%20process%20that%20progressively%20improves%20the%20output%20quality%20through%20multiple%20stages%20of%20image%20generation.%20Finally%2C%20a%20dynamic%20geometry%20information%20intensity%20adjustment%20mechanism%20is%20proposed%20to%20adaptively%20regulate%20the%20influence%20of%20geometric%20data%2C%20optimizing%20overall%20quality%20while%20ensuring%20the%20naturalness%20of%20generated%20images.%20More%20details%20can%20be%20found%20on%20the%20project%20page%3A%20https%3A//sobeymil.github.io/GeoMVD.com.%0ALink%3A%20http%3A//arxiv.org/abs/2511.12204v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoMVD%253A%2520Geometry-Enhanced%2520Multi-View%2520Generation%2520Model%2520Based%2520on%2520Geometric%2520Information%2520Extraction%26entry.906535625%3DJiaqi%2520Wu%2520and%2520Yaosen%2520Chen%2520and%2520Shuyuan%2520Zhu%26entry.1292438233%3DMulti-view%2520image%2520generation%2520holds%2520significant%2520application%2520value%2520in%2520computer%2520vision%252C%2520particularly%2520in%2520domains%2520like%25203D%2520reconstruction%252C%2520virtual%2520reality%252C%2520and%2520augmented%2520reality.%2520Most%2520existing%2520methods%252C%2520which%2520rely%2520on%2520extending%2520single%2520images%252C%2520face%2520notable%2520computational%2520challenges%2520in%2520maintaining%2520cross-view%2520consistency%2520and%2520generating%2520high-resolution%2520outputs.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520the%2520Geometry-guided%2520Multi-View%2520Diffusion%2520Model%252C%2520which%2520incorporates%2520mechanisms%2520for%2520extracting%2520multi-view%2520geometric%2520information%2520and%2520adjusting%2520the%2520intensity%2520of%2520geometric%2520features%2520to%2520generate%2520images%2520that%2520are%2520both%2520consistent%2520across%2520views%2520and%2520rich%2520in%2520detail.%2520Specifically%252C%2520we%2520design%2520a%2520multi-view%2520geometry%2520information%2520extraction%2520module%2520that%2520leverages%2520depth%2520maps%252C%2520normal%2520maps%252C%2520and%2520foreground%2520segmentation%2520masks%2520to%2520construct%2520a%2520shared%2520geometric%2520structure%252C%2520ensuring%2520shape%2520and%2520structural%2520consistency%2520across%2520different%2520views.%2520To%2520enhance%2520consistency%2520and%2520detail%2520restoration%2520during%2520generation%252C%2520we%2520develop%2520a%2520decoupled%2520geometry-enhanced%2520attention%2520mechanism%2520that%2520strengthens%2520feature%2520focus%2520on%2520key%2520geometric%2520details%252C%2520thereby%2520improving%2520overall%2520image%2520quality%2520and%2520detail%2520preservation.%2520Furthermore%252C%2520we%2520apply%2520an%2520adaptive%2520learning%2520strategy%2520that%2520fine-tunes%2520the%2520model%2520to%2520better%2520capture%2520spatial%2520relationships%2520and%2520visual%2520coherence%2520between%2520the%2520generated%2520views%252C%2520ensuring%2520realistic%2520results.%2520Our%2520model%2520also%2520incorporates%2520an%2520iterative%2520refinement%2520process%2520that%2520progressively%2520improves%2520the%2520output%2520quality%2520through%2520multiple%2520stages%2520of%2520image%2520generation.%2520Finally%252C%2520a%2520dynamic%2520geometry%2520information%2520intensity%2520adjustment%2520mechanism%2520is%2520proposed%2520to%2520adaptively%2520regulate%2520the%2520influence%2520of%2520geometric%2520data%252C%2520optimizing%2520overall%2520quality%2520while%2520ensuring%2520the%2520naturalness%2520of%2520generated%2520images.%2520More%2520details%2520can%2520be%2520found%2520on%2520the%2520project%2520page%253A%2520https%253A//sobeymil.github.io/GeoMVD.com.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.12204v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoMVD%3A%20Geometry-Enhanced%20Multi-View%20Generation%20Model%20Based%20on%20Geometric%20Information%20Extraction&entry.906535625=Jiaqi%20Wu%20and%20Yaosen%20Chen%20and%20Shuyuan%20Zhu&entry.1292438233=Multi-view%20image%20generation%20holds%20significant%20application%20value%20in%20computer%20vision%2C%20particularly%20in%20domains%20like%203D%20reconstruction%2C%20virtual%20reality%2C%20and%20augmented%20reality.%20Most%20existing%20methods%2C%20which%20rely%20on%20extending%20single%20images%2C%20face%20notable%20computational%20challenges%20in%20maintaining%20cross-view%20consistency%20and%20generating%20high-resolution%20outputs.%20To%20address%20these%20issues%2C%20we%20propose%20the%20Geometry-guided%20Multi-View%20Diffusion%20Model%2C%20which%20incorporates%20mechanisms%20for%20extracting%20multi-view%20geometric%20information%20and%20adjusting%20the%20intensity%20of%20geometric%20features%20to%20generate%20images%20that%20are%20both%20consistent%20across%20views%20and%20rich%20in%20detail.%20Specifically%2C%20we%20design%20a%20multi-view%20geometry%20information%20extraction%20module%20that%20leverages%20depth%20maps%2C%20normal%20maps%2C%20and%20foreground%20segmentation%20masks%20to%20construct%20a%20shared%20geometric%20structure%2C%20ensuring%20shape%20and%20structural%20consistency%20across%20different%20views.%20To%20enhance%20consistency%20and%20detail%20restoration%20during%20generation%2C%20we%20develop%20a%20decoupled%20geometry-enhanced%20attention%20mechanism%20that%20strengthens%20feature%20focus%20on%20key%20geometric%20details%2C%20thereby%20improving%20overall%20image%20quality%20and%20detail%20preservation.%20Furthermore%2C%20we%20apply%20an%20adaptive%20learning%20strategy%20that%20fine-tunes%20the%20model%20to%20better%20capture%20spatial%20relationships%20and%20visual%20coherence%20between%20the%20generated%20views%2C%20ensuring%20realistic%20results.%20Our%20model%20also%20incorporates%20an%20iterative%20refinement%20process%20that%20progressively%20improves%20the%20output%20quality%20through%20multiple%20stages%20of%20image%20generation.%20Finally%2C%20a%20dynamic%20geometry%20information%20intensity%20adjustment%20mechanism%20is%20proposed%20to%20adaptively%20regulate%20the%20influence%20of%20geometric%20data%2C%20optimizing%20overall%20quality%20while%20ensuring%20the%20naturalness%20of%20generated%20images.%20More%20details%20can%20be%20found%20on%20the%20project%20page%3A%20https%3A//sobeymil.github.io/GeoMVD.com.&entry.1838667208=http%3A//arxiv.org/abs/2511.12204v2&entry.124074799=Read"},
{"title": "Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback", "author": "Xingpei Ma and Shenneng Huang and Jiaran Cai and Yuansheng Guan and Shen Zheng and Hanfeng Zhao and Qiang Zhang and Shunsi Zhang", "abstract": "Recent advances in diffusion models have significantly improved audio-driven human video generation, surpassing traditional methods in both quality and controllability. However, existing approaches still face challenges in lip-sync accuracy, temporal coherence for long video generation, and multi-character animation. In this work, we propose a diffusion transformer (DiT)-based framework for generating lifelike talking videos of arbitrary length, and introduce a training-free method for multi-character audio-driven animation. First, we employ a LoRA-based training strategy combined with a position shift inference approach, which enables efficient long video generation while preserving the capabilities of the foundation model. Moreover, we combine partial parameter updates with reward feedback to enhance both lip synchronization and natural body motion. Finally, we propose a training-free approach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character animation, which requires no specialized datasets or model modifications and supports audio-driven animation for three or more characters. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving high-quality, temporally coherent, and multi-character audio-driven video generation in a simple, efficient, and cost-effective manner.", "link": "http://arxiv.org/abs/2510.12089v2", "date": "2025-11-18", "relevancy": 3.2257, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6674}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6346}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Playmate2%3A%20Training-Free%20Multi-Character%20Audio-Driven%20Animation%20via%20Diffusion%20Transformer%20with%20Reward%20Feedback&body=Title%3A%20Playmate2%3A%20Training-Free%20Multi-Character%20Audio-Driven%20Animation%20via%20Diffusion%20Transformer%20with%20Reward%20Feedback%0AAuthor%3A%20Xingpei%20Ma%20and%20Shenneng%20Huang%20and%20Jiaran%20Cai%20and%20Yuansheng%20Guan%20and%20Shen%20Zheng%20and%20Hanfeng%20Zhao%20and%20Qiang%20Zhang%20and%20Shunsi%20Zhang%0AAbstract%3A%20Recent%20advances%20in%20diffusion%20models%20have%20significantly%20improved%20audio-driven%20human%20video%20generation%2C%20surpassing%20traditional%20methods%20in%20both%20quality%20and%20controllability.%20However%2C%20existing%20approaches%20still%20face%20challenges%20in%20lip-sync%20accuracy%2C%20temporal%20coherence%20for%20long%20video%20generation%2C%20and%20multi-character%20animation.%20In%20this%20work%2C%20we%20propose%20a%20diffusion%20transformer%20%28DiT%29-based%20framework%20for%20generating%20lifelike%20talking%20videos%20of%20arbitrary%20length%2C%20and%20introduce%20a%20training-free%20method%20for%20multi-character%20audio-driven%20animation.%20First%2C%20we%20employ%20a%20LoRA-based%20training%20strategy%20combined%20with%20a%20position%20shift%20inference%20approach%2C%20which%20enables%20efficient%20long%20video%20generation%20while%20preserving%20the%20capabilities%20of%20the%20foundation%20model.%20Moreover%2C%20we%20combine%20partial%20parameter%20updates%20with%20reward%20feedback%20to%20enhance%20both%20lip%20synchronization%20and%20natural%20body%20motion.%20Finally%2C%20we%20propose%20a%20training-free%20approach%2C%20Mask%20Classifier-Free%20Guidance%20%28Mask-CFG%29%2C%20for%20multi-character%20animation%2C%20which%20requires%20no%20specialized%20datasets%20or%20model%20modifications%20and%20supports%20audio-driven%20animation%20for%20three%20or%20more%20characters.%20Experimental%20results%20demonstrate%20that%20our%20method%20outperforms%20existing%20state-of-the-art%20approaches%2C%20achieving%20high-quality%2C%20temporally%20coherent%2C%20and%20multi-character%20audio-driven%20video%20generation%20in%20a%20simple%2C%20efficient%2C%20and%20cost-effective%20manner.%0ALink%3A%20http%3A//arxiv.org/abs/2510.12089v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlaymate2%253A%2520Training-Free%2520Multi-Character%2520Audio-Driven%2520Animation%2520via%2520Diffusion%2520Transformer%2520with%2520Reward%2520Feedback%26entry.906535625%3DXingpei%2520Ma%2520and%2520Shenneng%2520Huang%2520and%2520Jiaran%2520Cai%2520and%2520Yuansheng%2520Guan%2520and%2520Shen%2520Zheng%2520and%2520Hanfeng%2520Zhao%2520and%2520Qiang%2520Zhang%2520and%2520Shunsi%2520Zhang%26entry.1292438233%3DRecent%2520advances%2520in%2520diffusion%2520models%2520have%2520significantly%2520improved%2520audio-driven%2520human%2520video%2520generation%252C%2520surpassing%2520traditional%2520methods%2520in%2520both%2520quality%2520and%2520controllability.%2520However%252C%2520existing%2520approaches%2520still%2520face%2520challenges%2520in%2520lip-sync%2520accuracy%252C%2520temporal%2520coherence%2520for%2520long%2520video%2520generation%252C%2520and%2520multi-character%2520animation.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520diffusion%2520transformer%2520%2528DiT%2529-based%2520framework%2520for%2520generating%2520lifelike%2520talking%2520videos%2520of%2520arbitrary%2520length%252C%2520and%2520introduce%2520a%2520training-free%2520method%2520for%2520multi-character%2520audio-driven%2520animation.%2520First%252C%2520we%2520employ%2520a%2520LoRA-based%2520training%2520strategy%2520combined%2520with%2520a%2520position%2520shift%2520inference%2520approach%252C%2520which%2520enables%2520efficient%2520long%2520video%2520generation%2520while%2520preserving%2520the%2520capabilities%2520of%2520the%2520foundation%2520model.%2520Moreover%252C%2520we%2520combine%2520partial%2520parameter%2520updates%2520with%2520reward%2520feedback%2520to%2520enhance%2520both%2520lip%2520synchronization%2520and%2520natural%2520body%2520motion.%2520Finally%252C%2520we%2520propose%2520a%2520training-free%2520approach%252C%2520Mask%2520Classifier-Free%2520Guidance%2520%2528Mask-CFG%2529%252C%2520for%2520multi-character%2520animation%252C%2520which%2520requires%2520no%2520specialized%2520datasets%2520or%2520model%2520modifications%2520and%2520supports%2520audio-driven%2520animation%2520for%2520three%2520or%2520more%2520characters.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520state-of-the-art%2520approaches%252C%2520achieving%2520high-quality%252C%2520temporally%2520coherent%252C%2520and%2520multi-character%2520audio-driven%2520video%2520generation%2520in%2520a%2520simple%252C%2520efficient%252C%2520and%2520cost-effective%2520manner.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12089v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Playmate2%3A%20Training-Free%20Multi-Character%20Audio-Driven%20Animation%20via%20Diffusion%20Transformer%20with%20Reward%20Feedback&entry.906535625=Xingpei%20Ma%20and%20Shenneng%20Huang%20and%20Jiaran%20Cai%20and%20Yuansheng%20Guan%20and%20Shen%20Zheng%20and%20Hanfeng%20Zhao%20and%20Qiang%20Zhang%20and%20Shunsi%20Zhang&entry.1292438233=Recent%20advances%20in%20diffusion%20models%20have%20significantly%20improved%20audio-driven%20human%20video%20generation%2C%20surpassing%20traditional%20methods%20in%20both%20quality%20and%20controllability.%20However%2C%20existing%20approaches%20still%20face%20challenges%20in%20lip-sync%20accuracy%2C%20temporal%20coherence%20for%20long%20video%20generation%2C%20and%20multi-character%20animation.%20In%20this%20work%2C%20we%20propose%20a%20diffusion%20transformer%20%28DiT%29-based%20framework%20for%20generating%20lifelike%20talking%20videos%20of%20arbitrary%20length%2C%20and%20introduce%20a%20training-free%20method%20for%20multi-character%20audio-driven%20animation.%20First%2C%20we%20employ%20a%20LoRA-based%20training%20strategy%20combined%20with%20a%20position%20shift%20inference%20approach%2C%20which%20enables%20efficient%20long%20video%20generation%20while%20preserving%20the%20capabilities%20of%20the%20foundation%20model.%20Moreover%2C%20we%20combine%20partial%20parameter%20updates%20with%20reward%20feedback%20to%20enhance%20both%20lip%20synchronization%20and%20natural%20body%20motion.%20Finally%2C%20we%20propose%20a%20training-free%20approach%2C%20Mask%20Classifier-Free%20Guidance%20%28Mask-CFG%29%2C%20for%20multi-character%20animation%2C%20which%20requires%20no%20specialized%20datasets%20or%20model%20modifications%20and%20supports%20audio-driven%20animation%20for%20three%20or%20more%20characters.%20Experimental%20results%20demonstrate%20that%20our%20method%20outperforms%20existing%20state-of-the-art%20approaches%2C%20achieving%20high-quality%2C%20temporally%20coherent%2C%20and%20multi-character%20audio-driven%20video%20generation%20in%20a%20simple%2C%20efficient%2C%20and%20cost-effective%20manner.&entry.1838667208=http%3A//arxiv.org/abs/2510.12089v2&entry.124074799=Read"},
{"title": "Interaction-Aware 4D Gaussian Splatting for Dynamic Hand-Object Interaction Reconstruction", "author": "Hao Tian and Chenyangguang Zhang and Rui Liu and Wen Shen and Xiaolin Qin", "abstract": "This paper focuses on a challenging setting of simultaneously modeling geometry and appearance of hand-object interaction scenes without any object priors. We follow the trend of dynamic 3D Gaussian Splatting based methods, and address several significant challenges. To model complex hand-object interaction with mutual occlusion and edge blur, we present interaction-aware hand-object Gaussians with newly introduced optimizable parameters aiming to adopt piecewise linear hypothesis for clearer structural representation. Moreover, considering the complementarity and tightness of hand shape and object shape during interaction dynamics, we incorporate hand information into object deformation field, constructing interaction-aware dynamic fields to model flexible motions. To further address difficulties in the optimization process, we propose a progressive strategy that handles dynamic regions and static background step by step. Correspondingly, explicit regularizations are designed to stabilize the hand-object representations for smooth motion transition, physical interaction reality, and coherent lighting. Experiments show that our approach surpasses existing dynamic 3D-GS-based methods and achieves state-of-the-art performance in reconstructing dynamic hand-object interaction.", "link": "http://arxiv.org/abs/2511.14540v1", "date": "2025-11-18", "relevancy": 3.1827, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6438}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6374}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interaction-Aware%204D%20Gaussian%20Splatting%20for%20Dynamic%20Hand-Object%20Interaction%20Reconstruction&body=Title%3A%20Interaction-Aware%204D%20Gaussian%20Splatting%20for%20Dynamic%20Hand-Object%20Interaction%20Reconstruction%0AAuthor%3A%20Hao%20Tian%20and%20Chenyangguang%20Zhang%20and%20Rui%20Liu%20and%20Wen%20Shen%20and%20Xiaolin%20Qin%0AAbstract%3A%20This%20paper%20focuses%20on%20a%20challenging%20setting%20of%20simultaneously%20modeling%20geometry%20and%20appearance%20of%20hand-object%20interaction%20scenes%20without%20any%20object%20priors.%20We%20follow%20the%20trend%20of%20dynamic%203D%20Gaussian%20Splatting%20based%20methods%2C%20and%20address%20several%20significant%20challenges.%20To%20model%20complex%20hand-object%20interaction%20with%20mutual%20occlusion%20and%20edge%20blur%2C%20we%20present%20interaction-aware%20hand-object%20Gaussians%20with%20newly%20introduced%20optimizable%20parameters%20aiming%20to%20adopt%20piecewise%20linear%20hypothesis%20for%20clearer%20structural%20representation.%20Moreover%2C%20considering%20the%20complementarity%20and%20tightness%20of%20hand%20shape%20and%20object%20shape%20during%20interaction%20dynamics%2C%20we%20incorporate%20hand%20information%20into%20object%20deformation%20field%2C%20constructing%20interaction-aware%20dynamic%20fields%20to%20model%20flexible%20motions.%20To%20further%20address%20difficulties%20in%20the%20optimization%20process%2C%20we%20propose%20a%20progressive%20strategy%20that%20handles%20dynamic%20regions%20and%20static%20background%20step%20by%20step.%20Correspondingly%2C%20explicit%20regularizations%20are%20designed%20to%20stabilize%20the%20hand-object%20representations%20for%20smooth%20motion%20transition%2C%20physical%20interaction%20reality%2C%20and%20coherent%20lighting.%20Experiments%20show%20that%20our%20approach%20surpasses%20existing%20dynamic%203D-GS-based%20methods%20and%20achieves%20state-of-the-art%20performance%20in%20reconstructing%20dynamic%20hand-object%20interaction.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteraction-Aware%25204D%2520Gaussian%2520Splatting%2520for%2520Dynamic%2520Hand-Object%2520Interaction%2520Reconstruction%26entry.906535625%3DHao%2520Tian%2520and%2520Chenyangguang%2520Zhang%2520and%2520Rui%2520Liu%2520and%2520Wen%2520Shen%2520and%2520Xiaolin%2520Qin%26entry.1292438233%3DThis%2520paper%2520focuses%2520on%2520a%2520challenging%2520setting%2520of%2520simultaneously%2520modeling%2520geometry%2520and%2520appearance%2520of%2520hand-object%2520interaction%2520scenes%2520without%2520any%2520object%2520priors.%2520We%2520follow%2520the%2520trend%2520of%2520dynamic%25203D%2520Gaussian%2520Splatting%2520based%2520methods%252C%2520and%2520address%2520several%2520significant%2520challenges.%2520To%2520model%2520complex%2520hand-object%2520interaction%2520with%2520mutual%2520occlusion%2520and%2520edge%2520blur%252C%2520we%2520present%2520interaction-aware%2520hand-object%2520Gaussians%2520with%2520newly%2520introduced%2520optimizable%2520parameters%2520aiming%2520to%2520adopt%2520piecewise%2520linear%2520hypothesis%2520for%2520clearer%2520structural%2520representation.%2520Moreover%252C%2520considering%2520the%2520complementarity%2520and%2520tightness%2520of%2520hand%2520shape%2520and%2520object%2520shape%2520during%2520interaction%2520dynamics%252C%2520we%2520incorporate%2520hand%2520information%2520into%2520object%2520deformation%2520field%252C%2520constructing%2520interaction-aware%2520dynamic%2520fields%2520to%2520model%2520flexible%2520motions.%2520To%2520further%2520address%2520difficulties%2520in%2520the%2520optimization%2520process%252C%2520we%2520propose%2520a%2520progressive%2520strategy%2520that%2520handles%2520dynamic%2520regions%2520and%2520static%2520background%2520step%2520by%2520step.%2520Correspondingly%252C%2520explicit%2520regularizations%2520are%2520designed%2520to%2520stabilize%2520the%2520hand-object%2520representations%2520for%2520smooth%2520motion%2520transition%252C%2520physical%2520interaction%2520reality%252C%2520and%2520coherent%2520lighting.%2520Experiments%2520show%2520that%2520our%2520approach%2520surpasses%2520existing%2520dynamic%25203D-GS-based%2520methods%2520and%2520achieves%2520state-of-the-art%2520performance%2520in%2520reconstructing%2520dynamic%2520hand-object%2520interaction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interaction-Aware%204D%20Gaussian%20Splatting%20for%20Dynamic%20Hand-Object%20Interaction%20Reconstruction&entry.906535625=Hao%20Tian%20and%20Chenyangguang%20Zhang%20and%20Rui%20Liu%20and%20Wen%20Shen%20and%20Xiaolin%20Qin&entry.1292438233=This%20paper%20focuses%20on%20a%20challenging%20setting%20of%20simultaneously%20modeling%20geometry%20and%20appearance%20of%20hand-object%20interaction%20scenes%20without%20any%20object%20priors.%20We%20follow%20the%20trend%20of%20dynamic%203D%20Gaussian%20Splatting%20based%20methods%2C%20and%20address%20several%20significant%20challenges.%20To%20model%20complex%20hand-object%20interaction%20with%20mutual%20occlusion%20and%20edge%20blur%2C%20we%20present%20interaction-aware%20hand-object%20Gaussians%20with%20newly%20introduced%20optimizable%20parameters%20aiming%20to%20adopt%20piecewise%20linear%20hypothesis%20for%20clearer%20structural%20representation.%20Moreover%2C%20considering%20the%20complementarity%20and%20tightness%20of%20hand%20shape%20and%20object%20shape%20during%20interaction%20dynamics%2C%20we%20incorporate%20hand%20information%20into%20object%20deformation%20field%2C%20constructing%20interaction-aware%20dynamic%20fields%20to%20model%20flexible%20motions.%20To%20further%20address%20difficulties%20in%20the%20optimization%20process%2C%20we%20propose%20a%20progressive%20strategy%20that%20handles%20dynamic%20regions%20and%20static%20background%20step%20by%20step.%20Correspondingly%2C%20explicit%20regularizations%20are%20designed%20to%20stabilize%20the%20hand-object%20representations%20for%20smooth%20motion%20transition%2C%20physical%20interaction%20reality%2C%20and%20coherent%20lighting.%20Experiments%20show%20that%20our%20approach%20surpasses%20existing%20dynamic%203D-GS-based%20methods%20and%20achieves%20state-of-the-art%20performance%20in%20reconstructing%20dynamic%20hand-object%20interaction.&entry.1838667208=http%3A//arxiv.org/abs/2511.14540v1&entry.124074799=Read"},
{"title": "Dental3R: Geometry-Aware Pairing for Intraoral 3D Reconstruction from Sparse-View Photographs", "author": "Yiyi Miao and Taoyu Wu and Tong Chen and Ji Jiang and Zhe Tang and Zhengyong Jiang and Angelos Stefanidis and Limin Yu and Jionglong Su", "abstract": "Intraoral 3D reconstruction is fundamental to digital orthodontics, yet conventional methods like intraoral scanning are inaccessible for remote tele-orthodontics, which typically relies on sparse smartphone imagery. While 3D Gaussian Splatting (3DGS) shows promise for novel view synthesis, its application to the standard clinical triad of unposed anterior and bilateral buccal photographs is challenging. The large view baselines, inconsistent illumination, and specular surfaces common in intraoral settings can destabilize simultaneous pose and geometry estimation. Furthermore, sparse-view photometric supervision often induces a frequency bias, leading to over-smoothed reconstructions that lose critical diagnostic details. To address these limitations, we propose \\textbf{Dental3R}, a pose-free, graph-guided pipeline for robust, high-fidelity reconstruction from sparse intraoral photographs. Our method first constructs a Geometry-Aware Pairing Strategy (GAPS) to intelligently select a compact subgraph of high-value image pairs. The GAPS focuses on correspondence matching, thereby improving the stability of the geometry initialization and reducing memory usage. Building on the recovered poses and point cloud, we train the 3DGS model with a wavelet-regularized objective. By enforcing band-limited fidelity using a discrete wavelet transform, our approach preserves fine enamel boundaries and interproximal edges while suppressing high-frequency artifacts. We validate our approach on a large-scale dataset of 950 clinical cases and an additional video-based test set of 195 cases. Experimental results demonstrate that Dental3R effectively handles sparse, unposed inputs and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art methods.", "link": "http://arxiv.org/abs/2511.14315v1", "date": "2025-11-18", "relevancy": 3.1643, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6423}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6293}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dental3R%3A%20Geometry-Aware%20Pairing%20for%20Intraoral%203D%20Reconstruction%20from%20Sparse-View%20Photographs&body=Title%3A%20Dental3R%3A%20Geometry-Aware%20Pairing%20for%20Intraoral%203D%20Reconstruction%20from%20Sparse-View%20Photographs%0AAuthor%3A%20Yiyi%20Miao%20and%20Taoyu%20Wu%20and%20Tong%20Chen%20and%20Ji%20Jiang%20and%20Zhe%20Tang%20and%20Zhengyong%20Jiang%20and%20Angelos%20Stefanidis%20and%20Limin%20Yu%20and%20Jionglong%20Su%0AAbstract%3A%20Intraoral%203D%20reconstruction%20is%20fundamental%20to%20digital%20orthodontics%2C%20yet%20conventional%20methods%20like%20intraoral%20scanning%20are%20inaccessible%20for%20remote%20tele-orthodontics%2C%20which%20typically%20relies%20on%20sparse%20smartphone%20imagery.%20While%203D%20Gaussian%20Splatting%20%283DGS%29%20shows%20promise%20for%20novel%20view%20synthesis%2C%20its%20application%20to%20the%20standard%20clinical%20triad%20of%20unposed%20anterior%20and%20bilateral%20buccal%20photographs%20is%20challenging.%20The%20large%20view%20baselines%2C%20inconsistent%20illumination%2C%20and%20specular%20surfaces%20common%20in%20intraoral%20settings%20can%20destabilize%20simultaneous%20pose%20and%20geometry%20estimation.%20Furthermore%2C%20sparse-view%20photometric%20supervision%20often%20induces%20a%20frequency%20bias%2C%20leading%20to%20over-smoothed%20reconstructions%20that%20lose%20critical%20diagnostic%20details.%20To%20address%20these%20limitations%2C%20we%20propose%20%5Ctextbf%7BDental3R%7D%2C%20a%20pose-free%2C%20graph-guided%20pipeline%20for%20robust%2C%20high-fidelity%20reconstruction%20from%20sparse%20intraoral%20photographs.%20Our%20method%20first%20constructs%20a%20Geometry-Aware%20Pairing%20Strategy%20%28GAPS%29%20to%20intelligently%20select%20a%20compact%20subgraph%20of%20high-value%20image%20pairs.%20The%20GAPS%20focuses%20on%20correspondence%20matching%2C%20thereby%20improving%20the%20stability%20of%20the%20geometry%20initialization%20and%20reducing%20memory%20usage.%20Building%20on%20the%20recovered%20poses%20and%20point%20cloud%2C%20we%20train%20the%203DGS%20model%20with%20a%20wavelet-regularized%20objective.%20By%20enforcing%20band-limited%20fidelity%20using%20a%20discrete%20wavelet%20transform%2C%20our%20approach%20preserves%20fine%20enamel%20boundaries%20and%20interproximal%20edges%20while%20suppressing%20high-frequency%20artifacts.%20We%20validate%20our%20approach%20on%20a%20large-scale%20dataset%20of%20950%20clinical%20cases%20and%20an%20additional%20video-based%20test%20set%20of%20195%20cases.%20Experimental%20results%20demonstrate%20that%20Dental3R%20effectively%20handles%20sparse%2C%20unposed%20inputs%20and%20achieves%20superior%20novel%20view%20synthesis%20quality%20for%20dental%20occlusion%20visualization%2C%20outperforming%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDental3R%253A%2520Geometry-Aware%2520Pairing%2520for%2520Intraoral%25203D%2520Reconstruction%2520from%2520Sparse-View%2520Photographs%26entry.906535625%3DYiyi%2520Miao%2520and%2520Taoyu%2520Wu%2520and%2520Tong%2520Chen%2520and%2520Ji%2520Jiang%2520and%2520Zhe%2520Tang%2520and%2520Zhengyong%2520Jiang%2520and%2520Angelos%2520Stefanidis%2520and%2520Limin%2520Yu%2520and%2520Jionglong%2520Su%26entry.1292438233%3DIntraoral%25203D%2520reconstruction%2520is%2520fundamental%2520to%2520digital%2520orthodontics%252C%2520yet%2520conventional%2520methods%2520like%2520intraoral%2520scanning%2520are%2520inaccessible%2520for%2520remote%2520tele-orthodontics%252C%2520which%2520typically%2520relies%2520on%2520sparse%2520smartphone%2520imagery.%2520While%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520shows%2520promise%2520for%2520novel%2520view%2520synthesis%252C%2520its%2520application%2520to%2520the%2520standard%2520clinical%2520triad%2520of%2520unposed%2520anterior%2520and%2520bilateral%2520buccal%2520photographs%2520is%2520challenging.%2520The%2520large%2520view%2520baselines%252C%2520inconsistent%2520illumination%252C%2520and%2520specular%2520surfaces%2520common%2520in%2520intraoral%2520settings%2520can%2520destabilize%2520simultaneous%2520pose%2520and%2520geometry%2520estimation.%2520Furthermore%252C%2520sparse-view%2520photometric%2520supervision%2520often%2520induces%2520a%2520frequency%2520bias%252C%2520leading%2520to%2520over-smoothed%2520reconstructions%2520that%2520lose%2520critical%2520diagnostic%2520details.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520%255Ctextbf%257BDental3R%257D%252C%2520a%2520pose-free%252C%2520graph-guided%2520pipeline%2520for%2520robust%252C%2520high-fidelity%2520reconstruction%2520from%2520sparse%2520intraoral%2520photographs.%2520Our%2520method%2520first%2520constructs%2520a%2520Geometry-Aware%2520Pairing%2520Strategy%2520%2528GAPS%2529%2520to%2520intelligently%2520select%2520a%2520compact%2520subgraph%2520of%2520high-value%2520image%2520pairs.%2520The%2520GAPS%2520focuses%2520on%2520correspondence%2520matching%252C%2520thereby%2520improving%2520the%2520stability%2520of%2520the%2520geometry%2520initialization%2520and%2520reducing%2520memory%2520usage.%2520Building%2520on%2520the%2520recovered%2520poses%2520and%2520point%2520cloud%252C%2520we%2520train%2520the%25203DGS%2520model%2520with%2520a%2520wavelet-regularized%2520objective.%2520By%2520enforcing%2520band-limited%2520fidelity%2520using%2520a%2520discrete%2520wavelet%2520transform%252C%2520our%2520approach%2520preserves%2520fine%2520enamel%2520boundaries%2520and%2520interproximal%2520edges%2520while%2520suppressing%2520high-frequency%2520artifacts.%2520We%2520validate%2520our%2520approach%2520on%2520a%2520large-scale%2520dataset%2520of%2520950%2520clinical%2520cases%2520and%2520an%2520additional%2520video-based%2520test%2520set%2520of%2520195%2520cases.%2520Experimental%2520results%2520demonstrate%2520that%2520Dental3R%2520effectively%2520handles%2520sparse%252C%2520unposed%2520inputs%2520and%2520achieves%2520superior%2520novel%2520view%2520synthesis%2520quality%2520for%2520dental%2520occlusion%2520visualization%252C%2520outperforming%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dental3R%3A%20Geometry-Aware%20Pairing%20for%20Intraoral%203D%20Reconstruction%20from%20Sparse-View%20Photographs&entry.906535625=Yiyi%20Miao%20and%20Taoyu%20Wu%20and%20Tong%20Chen%20and%20Ji%20Jiang%20and%20Zhe%20Tang%20and%20Zhengyong%20Jiang%20and%20Angelos%20Stefanidis%20and%20Limin%20Yu%20and%20Jionglong%20Su&entry.1292438233=Intraoral%203D%20reconstruction%20is%20fundamental%20to%20digital%20orthodontics%2C%20yet%20conventional%20methods%20like%20intraoral%20scanning%20are%20inaccessible%20for%20remote%20tele-orthodontics%2C%20which%20typically%20relies%20on%20sparse%20smartphone%20imagery.%20While%203D%20Gaussian%20Splatting%20%283DGS%29%20shows%20promise%20for%20novel%20view%20synthesis%2C%20its%20application%20to%20the%20standard%20clinical%20triad%20of%20unposed%20anterior%20and%20bilateral%20buccal%20photographs%20is%20challenging.%20The%20large%20view%20baselines%2C%20inconsistent%20illumination%2C%20and%20specular%20surfaces%20common%20in%20intraoral%20settings%20can%20destabilize%20simultaneous%20pose%20and%20geometry%20estimation.%20Furthermore%2C%20sparse-view%20photometric%20supervision%20often%20induces%20a%20frequency%20bias%2C%20leading%20to%20over-smoothed%20reconstructions%20that%20lose%20critical%20diagnostic%20details.%20To%20address%20these%20limitations%2C%20we%20propose%20%5Ctextbf%7BDental3R%7D%2C%20a%20pose-free%2C%20graph-guided%20pipeline%20for%20robust%2C%20high-fidelity%20reconstruction%20from%20sparse%20intraoral%20photographs.%20Our%20method%20first%20constructs%20a%20Geometry-Aware%20Pairing%20Strategy%20%28GAPS%29%20to%20intelligently%20select%20a%20compact%20subgraph%20of%20high-value%20image%20pairs.%20The%20GAPS%20focuses%20on%20correspondence%20matching%2C%20thereby%20improving%20the%20stability%20of%20the%20geometry%20initialization%20and%20reducing%20memory%20usage.%20Building%20on%20the%20recovered%20poses%20and%20point%20cloud%2C%20we%20train%20the%203DGS%20model%20with%20a%20wavelet-regularized%20objective.%20By%20enforcing%20band-limited%20fidelity%20using%20a%20discrete%20wavelet%20transform%2C%20our%20approach%20preserves%20fine%20enamel%20boundaries%20and%20interproximal%20edges%20while%20suppressing%20high-frequency%20artifacts.%20We%20validate%20our%20approach%20on%20a%20large-scale%20dataset%20of%20950%20clinical%20cases%20and%20an%20additional%20video-based%20test%20set%20of%20195%20cases.%20Experimental%20results%20demonstrate%20that%20Dental3R%20effectively%20handles%20sparse%2C%20unposed%20inputs%20and%20achieves%20superior%20novel%20view%20synthesis%20quality%20for%20dental%20occlusion%20visualization%2C%20outperforming%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.14315v1&entry.124074799=Read"},
{"title": "From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D", "author": "Jiahui Zhang and Yurui Chen and Yanpeng Zhou and Yueming Xu and Ze Huang and Jilin Mei and Junhui Chen and Yu-Jie Yuan and Xinyue Cai and Guowei Huang and Xingyue Quan and Hang Xu and Li Zhang", "abstract": "Recent advances in LVLMs have improved vision-language understanding, but they still struggle with spatial perception, limiting their ability to reason about complex 3D scenes. Unlike previous approaches that incorporate 3D representations into models to improve spatial understanding, we aim to unlock the potential of VLMs by leveraging spatially relevant image data. To this end, we introduce a novel 2D spatial data generation and annotation pipeline built upon scene data with 3D ground-truth. This pipeline enables the creation of a diverse set of spatial tasks, ranging from basic perception tasks to more complex reasoning tasks. Leveraging this pipeline, we construct SPAR-7M, a large-scale dataset generated from thousands of scenes across multiple public datasets. In addition, we introduce SPAR-Bench, a benchmark designed to offer a more comprehensive evaluation of spatial capabilities compared to existing spatial benchmarks, supporting both single-view and multi-view inputs. Training on both SPAR-7M and large-scale 2D datasets enables our models to achieve state-of-the-art performance on 2D spatial benchmarks. Further fine-tuning on 3D task-specific datasets yields competitive results, underscoring the effectiveness of our dataset in enhancing spatial reasoning.", "link": "http://arxiv.org/abs/2503.22976v6", "date": "2025-11-18", "relevancy": 3.1433, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6538}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Flatland%20to%20Space%3A%20Teaching%20Vision-Language%20Models%20to%20Perceive%20and%20Reason%20in%203D&body=Title%3A%20From%20Flatland%20to%20Space%3A%20Teaching%20Vision-Language%20Models%20to%20Perceive%20and%20Reason%20in%203D%0AAuthor%3A%20Jiahui%20Zhang%20and%20Yurui%20Chen%20and%20Yanpeng%20Zhou%20and%20Yueming%20Xu%20and%20Ze%20Huang%20and%20Jilin%20Mei%20and%20Junhui%20Chen%20and%20Yu-Jie%20Yuan%20and%20Xinyue%20Cai%20and%20Guowei%20Huang%20and%20Xingyue%20Quan%20and%20Hang%20Xu%20and%20Li%20Zhang%0AAbstract%3A%20Recent%20advances%20in%20LVLMs%20have%20improved%20vision-language%20understanding%2C%20but%20they%20still%20struggle%20with%20spatial%20perception%2C%20limiting%20their%20ability%20to%20reason%20about%20complex%203D%20scenes.%20Unlike%20previous%20approaches%20that%20incorporate%203D%20representations%20into%20models%20to%20improve%20spatial%20understanding%2C%20we%20aim%20to%20unlock%20the%20potential%20of%20VLMs%20by%20leveraging%20spatially%20relevant%20image%20data.%20To%20this%20end%2C%20we%20introduce%20a%20novel%202D%20spatial%20data%20generation%20and%20annotation%20pipeline%20built%20upon%20scene%20data%20with%203D%20ground-truth.%20This%20pipeline%20enables%20the%20creation%20of%20a%20diverse%20set%20of%20spatial%20tasks%2C%20ranging%20from%20basic%20perception%20tasks%20to%20more%20complex%20reasoning%20tasks.%20Leveraging%20this%20pipeline%2C%20we%20construct%20SPAR-7M%2C%20a%20large-scale%20dataset%20generated%20from%20thousands%20of%20scenes%20across%20multiple%20public%20datasets.%20In%20addition%2C%20we%20introduce%20SPAR-Bench%2C%20a%20benchmark%20designed%20to%20offer%20a%20more%20comprehensive%20evaluation%20of%20spatial%20capabilities%20compared%20to%20existing%20spatial%20benchmarks%2C%20supporting%20both%20single-view%20and%20multi-view%20inputs.%20Training%20on%20both%20SPAR-7M%20and%20large-scale%202D%20datasets%20enables%20our%20models%20to%20achieve%20state-of-the-art%20performance%20on%202D%20spatial%20benchmarks.%20Further%20fine-tuning%20on%203D%20task-specific%20datasets%20yields%20competitive%20results%2C%20underscoring%20the%20effectiveness%20of%20our%20dataset%20in%20enhancing%20spatial%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2503.22976v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Flatland%2520to%2520Space%253A%2520Teaching%2520Vision-Language%2520Models%2520to%2520Perceive%2520and%2520Reason%2520in%25203D%26entry.906535625%3DJiahui%2520Zhang%2520and%2520Yurui%2520Chen%2520and%2520Yanpeng%2520Zhou%2520and%2520Yueming%2520Xu%2520and%2520Ze%2520Huang%2520and%2520Jilin%2520Mei%2520and%2520Junhui%2520Chen%2520and%2520Yu-Jie%2520Yuan%2520and%2520Xinyue%2520Cai%2520and%2520Guowei%2520Huang%2520and%2520Xingyue%2520Quan%2520and%2520Hang%2520Xu%2520and%2520Li%2520Zhang%26entry.1292438233%3DRecent%2520advances%2520in%2520LVLMs%2520have%2520improved%2520vision-language%2520understanding%252C%2520but%2520they%2520still%2520struggle%2520with%2520spatial%2520perception%252C%2520limiting%2520their%2520ability%2520to%2520reason%2520about%2520complex%25203D%2520scenes.%2520Unlike%2520previous%2520approaches%2520that%2520incorporate%25203D%2520representations%2520into%2520models%2520to%2520improve%2520spatial%2520understanding%252C%2520we%2520aim%2520to%2520unlock%2520the%2520potential%2520of%2520VLMs%2520by%2520leveraging%2520spatially%2520relevant%2520image%2520data.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520novel%25202D%2520spatial%2520data%2520generation%2520and%2520annotation%2520pipeline%2520built%2520upon%2520scene%2520data%2520with%25203D%2520ground-truth.%2520This%2520pipeline%2520enables%2520the%2520creation%2520of%2520a%2520diverse%2520set%2520of%2520spatial%2520tasks%252C%2520ranging%2520from%2520basic%2520perception%2520tasks%2520to%2520more%2520complex%2520reasoning%2520tasks.%2520Leveraging%2520this%2520pipeline%252C%2520we%2520construct%2520SPAR-7M%252C%2520a%2520large-scale%2520dataset%2520generated%2520from%2520thousands%2520of%2520scenes%2520across%2520multiple%2520public%2520datasets.%2520In%2520addition%252C%2520we%2520introduce%2520SPAR-Bench%252C%2520a%2520benchmark%2520designed%2520to%2520offer%2520a%2520more%2520comprehensive%2520evaluation%2520of%2520spatial%2520capabilities%2520compared%2520to%2520existing%2520spatial%2520benchmarks%252C%2520supporting%2520both%2520single-view%2520and%2520multi-view%2520inputs.%2520Training%2520on%2520both%2520SPAR-7M%2520and%2520large-scale%25202D%2520datasets%2520enables%2520our%2520models%2520to%2520achieve%2520state-of-the-art%2520performance%2520on%25202D%2520spatial%2520benchmarks.%2520Further%2520fine-tuning%2520on%25203D%2520task-specific%2520datasets%2520yields%2520competitive%2520results%252C%2520underscoring%2520the%2520effectiveness%2520of%2520our%2520dataset%2520in%2520enhancing%2520spatial%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22976v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Flatland%20to%20Space%3A%20Teaching%20Vision-Language%20Models%20to%20Perceive%20and%20Reason%20in%203D&entry.906535625=Jiahui%20Zhang%20and%20Yurui%20Chen%20and%20Yanpeng%20Zhou%20and%20Yueming%20Xu%20and%20Ze%20Huang%20and%20Jilin%20Mei%20and%20Junhui%20Chen%20and%20Yu-Jie%20Yuan%20and%20Xinyue%20Cai%20and%20Guowei%20Huang%20and%20Xingyue%20Quan%20and%20Hang%20Xu%20and%20Li%20Zhang&entry.1292438233=Recent%20advances%20in%20LVLMs%20have%20improved%20vision-language%20understanding%2C%20but%20they%20still%20struggle%20with%20spatial%20perception%2C%20limiting%20their%20ability%20to%20reason%20about%20complex%203D%20scenes.%20Unlike%20previous%20approaches%20that%20incorporate%203D%20representations%20into%20models%20to%20improve%20spatial%20understanding%2C%20we%20aim%20to%20unlock%20the%20potential%20of%20VLMs%20by%20leveraging%20spatially%20relevant%20image%20data.%20To%20this%20end%2C%20we%20introduce%20a%20novel%202D%20spatial%20data%20generation%20and%20annotation%20pipeline%20built%20upon%20scene%20data%20with%203D%20ground-truth.%20This%20pipeline%20enables%20the%20creation%20of%20a%20diverse%20set%20of%20spatial%20tasks%2C%20ranging%20from%20basic%20perception%20tasks%20to%20more%20complex%20reasoning%20tasks.%20Leveraging%20this%20pipeline%2C%20we%20construct%20SPAR-7M%2C%20a%20large-scale%20dataset%20generated%20from%20thousands%20of%20scenes%20across%20multiple%20public%20datasets.%20In%20addition%2C%20we%20introduce%20SPAR-Bench%2C%20a%20benchmark%20designed%20to%20offer%20a%20more%20comprehensive%20evaluation%20of%20spatial%20capabilities%20compared%20to%20existing%20spatial%20benchmarks%2C%20supporting%20both%20single-view%20and%20multi-view%20inputs.%20Training%20on%20both%20SPAR-7M%20and%20large-scale%202D%20datasets%20enables%20our%20models%20to%20achieve%20state-of-the-art%20performance%20on%202D%20spatial%20benchmarks.%20Further%20fine-tuning%20on%203D%20task-specific%20datasets%20yields%20competitive%20results%2C%20underscoring%20the%20effectiveness%20of%20our%20dataset%20in%20enhancing%20spatial%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2503.22976v6&entry.124074799=Read"},
{"title": "Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors", "author": "Jeryes Danial and Yosi Ben Asher and Itzik Klein", "abstract": "Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera. Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive. Monocular SLAM also faces scale ambiguities, which affect its accuracy. To address these challenges, we propose an edge-aware lightweight monocular SLAM system combining sparse keypoint-based pose estimation with dense edge reconstruction. Our method employs deep learning-based depth prediction and edge detection, followed by optimization to refine keypoints and edges for geometric consistency, without relying on global loop closure or heavy neural computations. We fuse inertial data with vision by using an extended Kalman filter to resolve scale ambiguity and improve accuracy. The system operates in real time on low-power platforms, as demonstrated on a DJI Tello drone with a monocular camera and inertial sensors. In addition, we demonstrate robust autonomous navigation and obstacle avoidance in indoor corridors and on the TUM RGBD dataset. Our approach offers an effective, practical solution to real-time mapping and navigation in resource-constrained environments.", "link": "http://arxiv.org/abs/2511.14335v1", "date": "2025-11-18", "relevancy": 3.0898, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6253}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6183}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simultaneous%20Localization%20and%203D-Semi%20Dense%20Mapping%20for%20Micro%20Drones%20Using%20Monocular%20Camera%20and%20Inertial%20Sensors&body=Title%3A%20Simultaneous%20Localization%20and%203D-Semi%20Dense%20Mapping%20for%20Micro%20Drones%20Using%20Monocular%20Camera%20and%20Inertial%20Sensors%0AAuthor%3A%20Jeryes%20Danial%20and%20Yosi%20Ben%20Asher%20and%20Itzik%20Klein%0AAbstract%3A%20Monocular%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20algorithms%20estimate%20drone%20poses%20and%20build%20a%203D%20map%20using%20a%20single%20camera.%20Current%20algorithms%20include%20sparse%20methods%20that%20lack%20detailed%20geometry%2C%20while%20learning-driven%20approaches%20produce%20dense%20maps%20but%20are%20computationally%20intensive.%20Monocular%20SLAM%20also%20faces%20scale%20ambiguities%2C%20which%20affect%20its%20accuracy.%20To%20address%20these%20challenges%2C%20we%20propose%20an%20edge-aware%20lightweight%20monocular%20SLAM%20system%20combining%20sparse%20keypoint-based%20pose%20estimation%20with%20dense%20edge%20reconstruction.%20Our%20method%20employs%20deep%20learning-based%20depth%20prediction%20and%20edge%20detection%2C%20followed%20by%20optimization%20to%20refine%20keypoints%20and%20edges%20for%20geometric%20consistency%2C%20without%20relying%20on%20global%20loop%20closure%20or%20heavy%20neural%20computations.%20We%20fuse%20inertial%20data%20with%20vision%20by%20using%20an%20extended%20Kalman%20filter%20to%20resolve%20scale%20ambiguity%20and%20improve%20accuracy.%20The%20system%20operates%20in%20real%20time%20on%20low-power%20platforms%2C%20as%20demonstrated%20on%20a%20DJI%20Tello%20drone%20with%20a%20monocular%20camera%20and%20inertial%20sensors.%20In%20addition%2C%20we%20demonstrate%20robust%20autonomous%20navigation%20and%20obstacle%20avoidance%20in%20indoor%20corridors%20and%20on%20the%20TUM%20RGBD%20dataset.%20Our%20approach%20offers%20an%20effective%2C%20practical%20solution%20to%20real-time%20mapping%20and%20navigation%20in%20resource-constrained%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimultaneous%2520Localization%2520and%25203D-Semi%2520Dense%2520Mapping%2520for%2520Micro%2520Drones%2520Using%2520Monocular%2520Camera%2520and%2520Inertial%2520Sensors%26entry.906535625%3DJeryes%2520Danial%2520and%2520Yosi%2520Ben%2520Asher%2520and%2520Itzik%2520Klein%26entry.1292438233%3DMonocular%2520simultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%2520algorithms%2520estimate%2520drone%2520poses%2520and%2520build%2520a%25203D%2520map%2520using%2520a%2520single%2520camera.%2520Current%2520algorithms%2520include%2520sparse%2520methods%2520that%2520lack%2520detailed%2520geometry%252C%2520while%2520learning-driven%2520approaches%2520produce%2520dense%2520maps%2520but%2520are%2520computationally%2520intensive.%2520Monocular%2520SLAM%2520also%2520faces%2520scale%2520ambiguities%252C%2520which%2520affect%2520its%2520accuracy.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520an%2520edge-aware%2520lightweight%2520monocular%2520SLAM%2520system%2520combining%2520sparse%2520keypoint-based%2520pose%2520estimation%2520with%2520dense%2520edge%2520reconstruction.%2520Our%2520method%2520employs%2520deep%2520learning-based%2520depth%2520prediction%2520and%2520edge%2520detection%252C%2520followed%2520by%2520optimization%2520to%2520refine%2520keypoints%2520and%2520edges%2520for%2520geometric%2520consistency%252C%2520without%2520relying%2520on%2520global%2520loop%2520closure%2520or%2520heavy%2520neural%2520computations.%2520We%2520fuse%2520inertial%2520data%2520with%2520vision%2520by%2520using%2520an%2520extended%2520Kalman%2520filter%2520to%2520resolve%2520scale%2520ambiguity%2520and%2520improve%2520accuracy.%2520The%2520system%2520operates%2520in%2520real%2520time%2520on%2520low-power%2520platforms%252C%2520as%2520demonstrated%2520on%2520a%2520DJI%2520Tello%2520drone%2520with%2520a%2520monocular%2520camera%2520and%2520inertial%2520sensors.%2520In%2520addition%252C%2520we%2520demonstrate%2520robust%2520autonomous%2520navigation%2520and%2520obstacle%2520avoidance%2520in%2520indoor%2520corridors%2520and%2520on%2520the%2520TUM%2520RGBD%2520dataset.%2520Our%2520approach%2520offers%2520an%2520effective%252C%2520practical%2520solution%2520to%2520real-time%2520mapping%2520and%2520navigation%2520in%2520resource-constrained%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simultaneous%20Localization%20and%203D-Semi%20Dense%20Mapping%20for%20Micro%20Drones%20Using%20Monocular%20Camera%20and%20Inertial%20Sensors&entry.906535625=Jeryes%20Danial%20and%20Yosi%20Ben%20Asher%20and%20Itzik%20Klein&entry.1292438233=Monocular%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20algorithms%20estimate%20drone%20poses%20and%20build%20a%203D%20map%20using%20a%20single%20camera.%20Current%20algorithms%20include%20sparse%20methods%20that%20lack%20detailed%20geometry%2C%20while%20learning-driven%20approaches%20produce%20dense%20maps%20but%20are%20computationally%20intensive.%20Monocular%20SLAM%20also%20faces%20scale%20ambiguities%2C%20which%20affect%20its%20accuracy.%20To%20address%20these%20challenges%2C%20we%20propose%20an%20edge-aware%20lightweight%20monocular%20SLAM%20system%20combining%20sparse%20keypoint-based%20pose%20estimation%20with%20dense%20edge%20reconstruction.%20Our%20method%20employs%20deep%20learning-based%20depth%20prediction%20and%20edge%20detection%2C%20followed%20by%20optimization%20to%20refine%20keypoints%20and%20edges%20for%20geometric%20consistency%2C%20without%20relying%20on%20global%20loop%20closure%20or%20heavy%20neural%20computations.%20We%20fuse%20inertial%20data%20with%20vision%20by%20using%20an%20extended%20Kalman%20filter%20to%20resolve%20scale%20ambiguity%20and%20improve%20accuracy.%20The%20system%20operates%20in%20real%20time%20on%20low-power%20platforms%2C%20as%20demonstrated%20on%20a%20DJI%20Tello%20drone%20with%20a%20monocular%20camera%20and%20inertial%20sensors.%20In%20addition%2C%20we%20demonstrate%20robust%20autonomous%20navigation%20and%20obstacle%20avoidance%20in%20indoor%20corridors%20and%20on%20the%20TUM%20RGBD%20dataset.%20Our%20approach%20offers%20an%20effective%2C%20practical%20solution%20to%20real-time%20mapping%20and%20navigation%20in%20resource-constrained%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2511.14335v1&entry.124074799=Read"},
{"title": "O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model", "author": "Rishi Gupta and Mukilan Karuppasamy and Shyam Marjit and Aditay Tripathi and Anirban Chakraborty", "abstract": "While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.", "link": "http://arxiv.org/abs/2511.14368v1", "date": "2025-11-18", "relevancy": 3.0765, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6287}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20O3SLM%3A%20Open%20Weight%2C%20Open%20Data%2C%20and%20Open%20Vocabulary%20Sketch-Language%20Model&body=Title%3A%20O3SLM%3A%20Open%20Weight%2C%20Open%20Data%2C%20and%20Open%20Vocabulary%20Sketch-Language%20Model%0AAuthor%3A%20Rishi%20Gupta%20and%20Mukilan%20Karuppasamy%20and%20Shyam%20Marjit%20and%20Aditay%20Tripathi%20and%20Anirban%20Chakraborty%0AAbstract%3A%20While%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20are%20increasingly%20deployed%20in%20real-world%20applications%2C%20their%20ability%20to%20interpret%20abstract%20visual%20inputs%20remains%20limited.%20Specifically%2C%20they%20struggle%20to%20comprehend%20hand-drawn%20sketches%2C%20a%20modality%20that%20offers%20an%20intuitive%20means%20of%20expressing%20concepts%20that%20are%20difficult%20to%20describe%20textually.%20We%20identify%20the%20primary%20bottleneck%20as%20the%20absence%20of%20a%20large-scale%20dataset%20that%20jointly%20models%20sketches%2C%20photorealistic%20images%2C%20and%20corresponding%20natural%20language%20instructions.%20To%20address%20this%2C%20we%20present%20two%20key%20contributions%3A%20%281%29%20a%20new%2C%20large-scale%20dataset%20of%20image-sketch-instruction%20triplets%20designed%20to%20facilitate%20both%20pretraining%20and%20instruction%20tuning%2C%20and%20%282%29%20O3SLM%2C%20an%20LVLM%20trained%20on%20this%20dataset.%20Comprehensive%20evaluations%20on%20multiple%20sketch-based%20tasks%3A%20%28a%29%20object%20localization%2C%20%28b%29%20counting%2C%20%28c%29%20image%20retrieval%20i.e.%2C%20%28SBIR%20and%20fine-grained%20SBIR%29%2C%20and%20%28d%29%20visual%20question%20answering%20%28VQA%29%3B%20while%20incorporating%20the%20three%20existing%20sketch%20datasets%2C%20namely%20QuickDraw%21%2C%20Sketchy%2C%20and%20Tu%20Berlin%2C%20along%20with%20our%20generated%20SketchVCL%20dataset%2C%20show%20that%20O3SLM%20achieves%20state-of-the-art%20performance%2C%20substantially%20outperforming%20existing%20LVLMs%20in%20sketch%20comprehension%20and%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DO3SLM%253A%2520Open%2520Weight%252C%2520Open%2520Data%252C%2520and%2520Open%2520Vocabulary%2520Sketch-Language%2520Model%26entry.906535625%3DRishi%2520Gupta%2520and%2520Mukilan%2520Karuppasamy%2520and%2520Shyam%2520Marjit%2520and%2520Aditay%2520Tripathi%2520and%2520Anirban%2520Chakraborty%26entry.1292438233%3DWhile%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520real-world%2520applications%252C%2520their%2520ability%2520to%2520interpret%2520abstract%2520visual%2520inputs%2520remains%2520limited.%2520Specifically%252C%2520they%2520struggle%2520to%2520comprehend%2520hand-drawn%2520sketches%252C%2520a%2520modality%2520that%2520offers%2520an%2520intuitive%2520means%2520of%2520expressing%2520concepts%2520that%2520are%2520difficult%2520to%2520describe%2520textually.%2520We%2520identify%2520the%2520primary%2520bottleneck%2520as%2520the%2520absence%2520of%2520a%2520large-scale%2520dataset%2520that%2520jointly%2520models%2520sketches%252C%2520photorealistic%2520images%252C%2520and%2520corresponding%2520natural%2520language%2520instructions.%2520To%2520address%2520this%252C%2520we%2520present%2520two%2520key%2520contributions%253A%2520%25281%2529%2520a%2520new%252C%2520large-scale%2520dataset%2520of%2520image-sketch-instruction%2520triplets%2520designed%2520to%2520facilitate%2520both%2520pretraining%2520and%2520instruction%2520tuning%252C%2520and%2520%25282%2529%2520O3SLM%252C%2520an%2520LVLM%2520trained%2520on%2520this%2520dataset.%2520Comprehensive%2520evaluations%2520on%2520multiple%2520sketch-based%2520tasks%253A%2520%2528a%2529%2520object%2520localization%252C%2520%2528b%2529%2520counting%252C%2520%2528c%2529%2520image%2520retrieval%2520i.e.%252C%2520%2528SBIR%2520and%2520fine-grained%2520SBIR%2529%252C%2520and%2520%2528d%2529%2520visual%2520question%2520answering%2520%2528VQA%2529%253B%2520while%2520incorporating%2520the%2520three%2520existing%2520sketch%2520datasets%252C%2520namely%2520QuickDraw%2521%252C%2520Sketchy%252C%2520and%2520Tu%2520Berlin%252C%2520along%2520with%2520our%2520generated%2520SketchVCL%2520dataset%252C%2520show%2520that%2520O3SLM%2520achieves%2520state-of-the-art%2520performance%252C%2520substantially%2520outperforming%2520existing%2520LVLMs%2520in%2520sketch%2520comprehension%2520and%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=O3SLM%3A%20Open%20Weight%2C%20Open%20Data%2C%20and%20Open%20Vocabulary%20Sketch-Language%20Model&entry.906535625=Rishi%20Gupta%20and%20Mukilan%20Karuppasamy%20and%20Shyam%20Marjit%20and%20Aditay%20Tripathi%20and%20Anirban%20Chakraborty&entry.1292438233=While%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20are%20increasingly%20deployed%20in%20real-world%20applications%2C%20their%20ability%20to%20interpret%20abstract%20visual%20inputs%20remains%20limited.%20Specifically%2C%20they%20struggle%20to%20comprehend%20hand-drawn%20sketches%2C%20a%20modality%20that%20offers%20an%20intuitive%20means%20of%20expressing%20concepts%20that%20are%20difficult%20to%20describe%20textually.%20We%20identify%20the%20primary%20bottleneck%20as%20the%20absence%20of%20a%20large-scale%20dataset%20that%20jointly%20models%20sketches%2C%20photorealistic%20images%2C%20and%20corresponding%20natural%20language%20instructions.%20To%20address%20this%2C%20we%20present%20two%20key%20contributions%3A%20%281%29%20a%20new%2C%20large-scale%20dataset%20of%20image-sketch-instruction%20triplets%20designed%20to%20facilitate%20both%20pretraining%20and%20instruction%20tuning%2C%20and%20%282%29%20O3SLM%2C%20an%20LVLM%20trained%20on%20this%20dataset.%20Comprehensive%20evaluations%20on%20multiple%20sketch-based%20tasks%3A%20%28a%29%20object%20localization%2C%20%28b%29%20counting%2C%20%28c%29%20image%20retrieval%20i.e.%2C%20%28SBIR%20and%20fine-grained%20SBIR%29%2C%20and%20%28d%29%20visual%20question%20answering%20%28VQA%29%3B%20while%20incorporating%20the%20three%20existing%20sketch%20datasets%2C%20namely%20QuickDraw%21%2C%20Sketchy%2C%20and%20Tu%20Berlin%2C%20along%20with%20our%20generated%20SketchVCL%20dataset%2C%20show%20that%20O3SLM%20achieves%20state-of-the-art%20performance%2C%20substantially%20outperforming%20existing%20LVLMs%20in%20sketch%20comprehension%20and%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2511.14368v1&entry.124074799=Read"},
{"title": "Agentic Video Intelligence: A Flexible Framework for Advanced Video Exploration and Understanding", "author": "Hong Gao and Yiming Bao and Xuezhen Tu and Yutong Xu and Yue Jin and Yiyang Mu and Bin Zhong and Linan Yue and Min-Ling Zhang", "abstract": "Video understanding requires not only visual recognition but also complex reasoning. While Vision-Language Models (VLMs) demonstrate impressive capabilities, they typically process videos largely in a single-pass manner with limited support for evidence revisit and iterative refinement. While recently emerging agent-based methods enable long-horizon reasoning, they either depend heavily on expensive proprietary models or require extensive agentic RL training. To overcome these limitations, we propose Agentic Video Intelligence (AVI), a flexible and training-free framework that can mirror human video comprehension through system-level design and optimization. AVI introduces three key innovations: (1) a human-inspired three-phase reasoning process (Retrieve-Perceive-Review) that ensures both sufficient global exploration and focused local analysis, (2) a structured video knowledge base organized through entity graphs, along with multi-granularity integrated tools, constituting the agent's interaction environment, and (3) an open-source model ensemble combining reasoning LLMs with lightweight base CV models and VLM, eliminating dependence on proprietary APIs or RL training. Experiments on LVBench, VideoMME-Long, LongVideoBench, and Charades-STA demonstrate that AVI achieves competitive performance while offering superior interpretability.", "link": "http://arxiv.org/abs/2511.14446v1", "date": "2025-11-18", "relevancy": 3.0379, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6149}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6149}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%20Video%20Intelligence%3A%20A%20Flexible%20Framework%20for%20Advanced%20Video%20Exploration%20and%20Understanding&body=Title%3A%20Agentic%20Video%20Intelligence%3A%20A%20Flexible%20Framework%20for%20Advanced%20Video%20Exploration%20and%20Understanding%0AAuthor%3A%20Hong%20Gao%20and%20Yiming%20Bao%20and%20Xuezhen%20Tu%20and%20Yutong%20Xu%20and%20Yue%20Jin%20and%20Yiyang%20Mu%20and%20Bin%20Zhong%20and%20Linan%20Yue%20and%20Min-Ling%20Zhang%0AAbstract%3A%20Video%20understanding%20requires%20not%20only%20visual%20recognition%20but%20also%20complex%20reasoning.%20While%20Vision-Language%20Models%20%28VLMs%29%20demonstrate%20impressive%20capabilities%2C%20they%20typically%20process%20videos%20largely%20in%20a%20single-pass%20manner%20with%20limited%20support%20for%20evidence%20revisit%20and%20iterative%20refinement.%20While%20recently%20emerging%20agent-based%20methods%20enable%20long-horizon%20reasoning%2C%20they%20either%20depend%20heavily%20on%20expensive%20proprietary%20models%20or%20require%20extensive%20agentic%20RL%20training.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Agentic%20Video%20Intelligence%20%28AVI%29%2C%20a%20flexible%20and%20training-free%20framework%20that%20can%20mirror%20human%20video%20comprehension%20through%20system-level%20design%20and%20optimization.%20AVI%20introduces%20three%20key%20innovations%3A%20%281%29%20a%20human-inspired%20three-phase%20reasoning%20process%20%28Retrieve-Perceive-Review%29%20that%20ensures%20both%20sufficient%20global%20exploration%20and%20focused%20local%20analysis%2C%20%282%29%20a%20structured%20video%20knowledge%20base%20organized%20through%20entity%20graphs%2C%20along%20with%20multi-granularity%20integrated%20tools%2C%20constituting%20the%20agent%27s%20interaction%20environment%2C%20and%20%283%29%20an%20open-source%20model%20ensemble%20combining%20reasoning%20LLMs%20with%20lightweight%20base%20CV%20models%20and%20VLM%2C%20eliminating%20dependence%20on%20proprietary%20APIs%20or%20RL%20training.%20Experiments%20on%20LVBench%2C%20VideoMME-Long%2C%20LongVideoBench%2C%20and%20Charades-STA%20demonstrate%20that%20AVI%20achieves%20competitive%20performance%20while%20offering%20superior%20interpretability.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%2520Video%2520Intelligence%253A%2520A%2520Flexible%2520Framework%2520for%2520Advanced%2520Video%2520Exploration%2520and%2520Understanding%26entry.906535625%3DHong%2520Gao%2520and%2520Yiming%2520Bao%2520and%2520Xuezhen%2520Tu%2520and%2520Yutong%2520Xu%2520and%2520Yue%2520Jin%2520and%2520Yiyang%2520Mu%2520and%2520Bin%2520Zhong%2520and%2520Linan%2520Yue%2520and%2520Min-Ling%2520Zhang%26entry.1292438233%3DVideo%2520understanding%2520requires%2520not%2520only%2520visual%2520recognition%2520but%2520also%2520complex%2520reasoning.%2520While%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520demonstrate%2520impressive%2520capabilities%252C%2520they%2520typically%2520process%2520videos%2520largely%2520in%2520a%2520single-pass%2520manner%2520with%2520limited%2520support%2520for%2520evidence%2520revisit%2520and%2520iterative%2520refinement.%2520While%2520recently%2520emerging%2520agent-based%2520methods%2520enable%2520long-horizon%2520reasoning%252C%2520they%2520either%2520depend%2520heavily%2520on%2520expensive%2520proprietary%2520models%2520or%2520require%2520extensive%2520agentic%2520RL%2520training.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520Agentic%2520Video%2520Intelligence%2520%2528AVI%2529%252C%2520a%2520flexible%2520and%2520training-free%2520framework%2520that%2520can%2520mirror%2520human%2520video%2520comprehension%2520through%2520system-level%2520design%2520and%2520optimization.%2520AVI%2520introduces%2520three%2520key%2520innovations%253A%2520%25281%2529%2520a%2520human-inspired%2520three-phase%2520reasoning%2520process%2520%2528Retrieve-Perceive-Review%2529%2520that%2520ensures%2520both%2520sufficient%2520global%2520exploration%2520and%2520focused%2520local%2520analysis%252C%2520%25282%2529%2520a%2520structured%2520video%2520knowledge%2520base%2520organized%2520through%2520entity%2520graphs%252C%2520along%2520with%2520multi-granularity%2520integrated%2520tools%252C%2520constituting%2520the%2520agent%2527s%2520interaction%2520environment%252C%2520and%2520%25283%2529%2520an%2520open-source%2520model%2520ensemble%2520combining%2520reasoning%2520LLMs%2520with%2520lightweight%2520base%2520CV%2520models%2520and%2520VLM%252C%2520eliminating%2520dependence%2520on%2520proprietary%2520APIs%2520or%2520RL%2520training.%2520Experiments%2520on%2520LVBench%252C%2520VideoMME-Long%252C%2520LongVideoBench%252C%2520and%2520Charades-STA%2520demonstrate%2520that%2520AVI%2520achieves%2520competitive%2520performance%2520while%2520offering%2520superior%2520interpretability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%20Video%20Intelligence%3A%20A%20Flexible%20Framework%20for%20Advanced%20Video%20Exploration%20and%20Understanding&entry.906535625=Hong%20Gao%20and%20Yiming%20Bao%20and%20Xuezhen%20Tu%20and%20Yutong%20Xu%20and%20Yue%20Jin%20and%20Yiyang%20Mu%20and%20Bin%20Zhong%20and%20Linan%20Yue%20and%20Min-Ling%20Zhang&entry.1292438233=Video%20understanding%20requires%20not%20only%20visual%20recognition%20but%20also%20complex%20reasoning.%20While%20Vision-Language%20Models%20%28VLMs%29%20demonstrate%20impressive%20capabilities%2C%20they%20typically%20process%20videos%20largely%20in%20a%20single-pass%20manner%20with%20limited%20support%20for%20evidence%20revisit%20and%20iterative%20refinement.%20While%20recently%20emerging%20agent-based%20methods%20enable%20long-horizon%20reasoning%2C%20they%20either%20depend%20heavily%20on%20expensive%20proprietary%20models%20or%20require%20extensive%20agentic%20RL%20training.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Agentic%20Video%20Intelligence%20%28AVI%29%2C%20a%20flexible%20and%20training-free%20framework%20that%20can%20mirror%20human%20video%20comprehension%20through%20system-level%20design%20and%20optimization.%20AVI%20introduces%20three%20key%20innovations%3A%20%281%29%20a%20human-inspired%20three-phase%20reasoning%20process%20%28Retrieve-Perceive-Review%29%20that%20ensures%20both%20sufficient%20global%20exploration%20and%20focused%20local%20analysis%2C%20%282%29%20a%20structured%20video%20knowledge%20base%20organized%20through%20entity%20graphs%2C%20along%20with%20multi-granularity%20integrated%20tools%2C%20constituting%20the%20agent%27s%20interaction%20environment%2C%20and%20%283%29%20an%20open-source%20model%20ensemble%20combining%20reasoning%20LLMs%20with%20lightweight%20base%20CV%20models%20and%20VLM%2C%20eliminating%20dependence%20on%20proprietary%20APIs%20or%20RL%20training.%20Experiments%20on%20LVBench%2C%20VideoMME-Long%2C%20LongVideoBench%2C%20and%20Charades-STA%20demonstrate%20that%20AVI%20achieves%20competitive%20performance%20while%20offering%20superior%20interpretability.&entry.1838667208=http%3A//arxiv.org/abs/2511.14446v1&entry.124074799=Read"},
{"title": "2D Gaussians Spatial Transport for Point-supervised Density Regression", "author": "Miao Shang and Xiaopeng Hong", "abstract": "This paper introduces Gaussian Spatial Transport (GST), a novel framework that leverages Gaussian splatting to facilitate transport from the probability measure in the image coordinate space to the annotation map. We propose a Gaussian splatting-based method to estimate pixel-annotation correspondence, which is then used to compute a transport plan derived from Bayesian probability. To integrate the resulting transport plan into standard network optimization in typical computer vision tasks, we derive a loss function that measures discrepancy after transport. Extensive experiments on representative computer vision tasks, including crowd counting and landmark detection, validate the effectiveness of our approach. Compared to conventional optimal transport schemes, GST eliminates iterative transport plan computation during training, significantly improving efficiency. Code is available at https://github.com/infinite0522/GST.", "link": "http://arxiv.org/abs/2511.14477v1", "date": "2025-11-18", "relevancy": 3.0353, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6445}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6024}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%202D%20Gaussians%20Spatial%20Transport%20for%20Point-supervised%20Density%20Regression&body=Title%3A%202D%20Gaussians%20Spatial%20Transport%20for%20Point-supervised%20Density%20Regression%0AAuthor%3A%20Miao%20Shang%20and%20Xiaopeng%20Hong%0AAbstract%3A%20This%20paper%20introduces%20Gaussian%20Spatial%20Transport%20%28GST%29%2C%20a%20novel%20framework%20that%20leverages%20Gaussian%20splatting%20to%20facilitate%20transport%20from%20the%20probability%20measure%20in%20the%20image%20coordinate%20space%20to%20the%20annotation%20map.%20We%20propose%20a%20Gaussian%20splatting-based%20method%20to%20estimate%20pixel-annotation%20correspondence%2C%20which%20is%20then%20used%20to%20compute%20a%20transport%20plan%20derived%20from%20Bayesian%20probability.%20To%20integrate%20the%20resulting%20transport%20plan%20into%20standard%20network%20optimization%20in%20typical%20computer%20vision%20tasks%2C%20we%20derive%20a%20loss%20function%20that%20measures%20discrepancy%20after%20transport.%20Extensive%20experiments%20on%20representative%20computer%20vision%20tasks%2C%20including%20crowd%20counting%20and%20landmark%20detection%2C%20validate%20the%20effectiveness%20of%20our%20approach.%20Compared%20to%20conventional%20optimal%20transport%20schemes%2C%20GST%20eliminates%20iterative%20transport%20plan%20computation%20during%20training%2C%20significantly%20improving%20efficiency.%20Code%20is%20available%20at%20https%3A//github.com/infinite0522/GST.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D2D%2520Gaussians%2520Spatial%2520Transport%2520for%2520Point-supervised%2520Density%2520Regression%26entry.906535625%3DMiao%2520Shang%2520and%2520Xiaopeng%2520Hong%26entry.1292438233%3DThis%2520paper%2520introduces%2520Gaussian%2520Spatial%2520Transport%2520%2528GST%2529%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520Gaussian%2520splatting%2520to%2520facilitate%2520transport%2520from%2520the%2520probability%2520measure%2520in%2520the%2520image%2520coordinate%2520space%2520to%2520the%2520annotation%2520map.%2520We%2520propose%2520a%2520Gaussian%2520splatting-based%2520method%2520to%2520estimate%2520pixel-annotation%2520correspondence%252C%2520which%2520is%2520then%2520used%2520to%2520compute%2520a%2520transport%2520plan%2520derived%2520from%2520Bayesian%2520probability.%2520To%2520integrate%2520the%2520resulting%2520transport%2520plan%2520into%2520standard%2520network%2520optimization%2520in%2520typical%2520computer%2520vision%2520tasks%252C%2520we%2520derive%2520a%2520loss%2520function%2520that%2520measures%2520discrepancy%2520after%2520transport.%2520Extensive%2520experiments%2520on%2520representative%2520computer%2520vision%2520tasks%252C%2520including%2520crowd%2520counting%2520and%2520landmark%2520detection%252C%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach.%2520Compared%2520to%2520conventional%2520optimal%2520transport%2520schemes%252C%2520GST%2520eliminates%2520iterative%2520transport%2520plan%2520computation%2520during%2520training%252C%2520significantly%2520improving%2520efficiency.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/infinite0522/GST.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=2D%20Gaussians%20Spatial%20Transport%20for%20Point-supervised%20Density%20Regression&entry.906535625=Miao%20Shang%20and%20Xiaopeng%20Hong&entry.1292438233=This%20paper%20introduces%20Gaussian%20Spatial%20Transport%20%28GST%29%2C%20a%20novel%20framework%20that%20leverages%20Gaussian%20splatting%20to%20facilitate%20transport%20from%20the%20probability%20measure%20in%20the%20image%20coordinate%20space%20to%20the%20annotation%20map.%20We%20propose%20a%20Gaussian%20splatting-based%20method%20to%20estimate%20pixel-annotation%20correspondence%2C%20which%20is%20then%20used%20to%20compute%20a%20transport%20plan%20derived%20from%20Bayesian%20probability.%20To%20integrate%20the%20resulting%20transport%20plan%20into%20standard%20network%20optimization%20in%20typical%20computer%20vision%20tasks%2C%20we%20derive%20a%20loss%20function%20that%20measures%20discrepancy%20after%20transport.%20Extensive%20experiments%20on%20representative%20computer%20vision%20tasks%2C%20including%20crowd%20counting%20and%20landmark%20detection%2C%20validate%20the%20effectiveness%20of%20our%20approach.%20Compared%20to%20conventional%20optimal%20transport%20schemes%2C%20GST%20eliminates%20iterative%20transport%20plan%20computation%20during%20training%2C%20significantly%20improving%20efficiency.%20Code%20is%20available%20at%20https%3A//github.com/infinite0522/GST.&entry.1838667208=http%3A//arxiv.org/abs/2511.14477v1&entry.124074799=Read"},
{"title": "LoG3D: Ultra-High-Resolution 3D Shape Modeling via Local-to-Global Partitioning", "author": "Xinran Yang and Shuichang Lai and Jiangjing Lyu and Hongjie Li and Bowen Pan and Yuanqi Li and Jie Guo and Zhengkang Zhou and Yanwen Guo", "abstract": "Generating high-fidelity 3D contents remains a fundamental challenge due to the complexity of representing arbitrary topologies-such as open surfaces and intricate internal structures-while preserving geometric details. Prevailing methods based on signed distance fields (SDFs) are hampered by costly watertight preprocessing and struggle with non-manifold geometries, while point-cloud representations often suffer from sampling artifacts and surface discontinuities. To overcome these limitations, we propose a novel 3D variational autoencoder (VAE) framework built upon unsigned distance fields (UDFs)-a more robust and computationally efficient representation that naturally handles complex and incomplete shapes. Our core innovation is a local-to-global (LoG) architecture that processes the UDF by partitioning it into uniform subvolumes, termed UBlocks. This architecture couples 3D convolutions for capturing local detail with sparse transformers for enforcing global coherence. A Pad-Average strategy further ensures smooth transitions at subvolume boundaries during reconstruction. This modular design enables seamless scaling to ultra-high resolutions up to $2048^3$-a regime previously unattainable for 3D VAEs. Experiments demonstrate state-of-the-art performance in both reconstruction accuracy and generative quality, yielding superior surface smoothness and geometric flexibility.", "link": "http://arxiv.org/abs/2511.10040v2", "date": "2025-11-18", "relevancy": 2.9536, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5948}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5948}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoG3D%3A%20Ultra-High-Resolution%203D%20Shape%20Modeling%20via%20Local-to-Global%20Partitioning&body=Title%3A%20LoG3D%3A%20Ultra-High-Resolution%203D%20Shape%20Modeling%20via%20Local-to-Global%20Partitioning%0AAuthor%3A%20Xinran%20Yang%20and%20Shuichang%20Lai%20and%20Jiangjing%20Lyu%20and%20Hongjie%20Li%20and%20Bowen%20Pan%20and%20Yuanqi%20Li%20and%20Jie%20Guo%20and%20Zhengkang%20Zhou%20and%20Yanwen%20Guo%0AAbstract%3A%20Generating%20high-fidelity%203D%20contents%20remains%20a%20fundamental%20challenge%20due%20to%20the%20complexity%20of%20representing%20arbitrary%20topologies-such%20as%20open%20surfaces%20and%20intricate%20internal%20structures-while%20preserving%20geometric%20details.%20Prevailing%20methods%20based%20on%20signed%20distance%20fields%20%28SDFs%29%20are%20hampered%20by%20costly%20watertight%20preprocessing%20and%20struggle%20with%20non-manifold%20geometries%2C%20while%20point-cloud%20representations%20often%20suffer%20from%20sampling%20artifacts%20and%20surface%20discontinuities.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%203D%20variational%20autoencoder%20%28VAE%29%20framework%20built%20upon%20unsigned%20distance%20fields%20%28UDFs%29-a%20more%20robust%20and%20computationally%20efficient%20representation%20that%20naturally%20handles%20complex%20and%20incomplete%20shapes.%20Our%20core%20innovation%20is%20a%20local-to-global%20%28LoG%29%20architecture%20that%20processes%20the%20UDF%20by%20partitioning%20it%20into%20uniform%20subvolumes%2C%20termed%20UBlocks.%20This%20architecture%20couples%203D%20convolutions%20for%20capturing%20local%20detail%20with%20sparse%20transformers%20for%20enforcing%20global%20coherence.%20A%20Pad-Average%20strategy%20further%20ensures%20smooth%20transitions%20at%20subvolume%20boundaries%20during%20reconstruction.%20This%20modular%20design%20enables%20seamless%20scaling%20to%20ultra-high%20resolutions%20up%20to%20%242048%5E3%24-a%20regime%20previously%20unattainable%20for%203D%20VAEs.%20Experiments%20demonstrate%20state-of-the-art%20performance%20in%20both%20reconstruction%20accuracy%20and%20generative%20quality%2C%20yielding%20superior%20surface%20smoothness%20and%20geometric%20flexibility.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10040v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoG3D%253A%2520Ultra-High-Resolution%25203D%2520Shape%2520Modeling%2520via%2520Local-to-Global%2520Partitioning%26entry.906535625%3DXinran%2520Yang%2520and%2520Shuichang%2520Lai%2520and%2520Jiangjing%2520Lyu%2520and%2520Hongjie%2520Li%2520and%2520Bowen%2520Pan%2520and%2520Yuanqi%2520Li%2520and%2520Jie%2520Guo%2520and%2520Zhengkang%2520Zhou%2520and%2520Yanwen%2520Guo%26entry.1292438233%3DGenerating%2520high-fidelity%25203D%2520contents%2520remains%2520a%2520fundamental%2520challenge%2520due%2520to%2520the%2520complexity%2520of%2520representing%2520arbitrary%2520topologies-such%2520as%2520open%2520surfaces%2520and%2520intricate%2520internal%2520structures-while%2520preserving%2520geometric%2520details.%2520Prevailing%2520methods%2520based%2520on%2520signed%2520distance%2520fields%2520%2528SDFs%2529%2520are%2520hampered%2520by%2520costly%2520watertight%2520preprocessing%2520and%2520struggle%2520with%2520non-manifold%2520geometries%252C%2520while%2520point-cloud%2520representations%2520often%2520suffer%2520from%2520sampling%2520artifacts%2520and%2520surface%2520discontinuities.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%25203D%2520variational%2520autoencoder%2520%2528VAE%2529%2520framework%2520built%2520upon%2520unsigned%2520distance%2520fields%2520%2528UDFs%2529-a%2520more%2520robust%2520and%2520computationally%2520efficient%2520representation%2520that%2520naturally%2520handles%2520complex%2520and%2520incomplete%2520shapes.%2520Our%2520core%2520innovation%2520is%2520a%2520local-to-global%2520%2528LoG%2529%2520architecture%2520that%2520processes%2520the%2520UDF%2520by%2520partitioning%2520it%2520into%2520uniform%2520subvolumes%252C%2520termed%2520UBlocks.%2520This%2520architecture%2520couples%25203D%2520convolutions%2520for%2520capturing%2520local%2520detail%2520with%2520sparse%2520transformers%2520for%2520enforcing%2520global%2520coherence.%2520A%2520Pad-Average%2520strategy%2520further%2520ensures%2520smooth%2520transitions%2520at%2520subvolume%2520boundaries%2520during%2520reconstruction.%2520This%2520modular%2520design%2520enables%2520seamless%2520scaling%2520to%2520ultra-high%2520resolutions%2520up%2520to%2520%25242048%255E3%2524-a%2520regime%2520previously%2520unattainable%2520for%25203D%2520VAEs.%2520Experiments%2520demonstrate%2520state-of-the-art%2520performance%2520in%2520both%2520reconstruction%2520accuracy%2520and%2520generative%2520quality%252C%2520yielding%2520superior%2520surface%2520smoothness%2520and%2520geometric%2520flexibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10040v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoG3D%3A%20Ultra-High-Resolution%203D%20Shape%20Modeling%20via%20Local-to-Global%20Partitioning&entry.906535625=Xinran%20Yang%20and%20Shuichang%20Lai%20and%20Jiangjing%20Lyu%20and%20Hongjie%20Li%20and%20Bowen%20Pan%20and%20Yuanqi%20Li%20and%20Jie%20Guo%20and%20Zhengkang%20Zhou%20and%20Yanwen%20Guo&entry.1292438233=Generating%20high-fidelity%203D%20contents%20remains%20a%20fundamental%20challenge%20due%20to%20the%20complexity%20of%20representing%20arbitrary%20topologies-such%20as%20open%20surfaces%20and%20intricate%20internal%20structures-while%20preserving%20geometric%20details.%20Prevailing%20methods%20based%20on%20signed%20distance%20fields%20%28SDFs%29%20are%20hampered%20by%20costly%20watertight%20preprocessing%20and%20struggle%20with%20non-manifold%20geometries%2C%20while%20point-cloud%20representations%20often%20suffer%20from%20sampling%20artifacts%20and%20surface%20discontinuities.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%203D%20variational%20autoencoder%20%28VAE%29%20framework%20built%20upon%20unsigned%20distance%20fields%20%28UDFs%29-a%20more%20robust%20and%20computationally%20efficient%20representation%20that%20naturally%20handles%20complex%20and%20incomplete%20shapes.%20Our%20core%20innovation%20is%20a%20local-to-global%20%28LoG%29%20architecture%20that%20processes%20the%20UDF%20by%20partitioning%20it%20into%20uniform%20subvolumes%2C%20termed%20UBlocks.%20This%20architecture%20couples%203D%20convolutions%20for%20capturing%20local%20detail%20with%20sparse%20transformers%20for%20enforcing%20global%20coherence.%20A%20Pad-Average%20strategy%20further%20ensures%20smooth%20transitions%20at%20subvolume%20boundaries%20during%20reconstruction.%20This%20modular%20design%20enables%20seamless%20scaling%20to%20ultra-high%20resolutions%20up%20to%20%242048%5E3%24-a%20regime%20previously%20unattainable%20for%203D%20VAEs.%20Experiments%20demonstrate%20state-of-the-art%20performance%20in%20both%20reconstruction%20accuracy%20and%20generative%20quality%2C%20yielding%20superior%20surface%20smoothness%20and%20geometric%20flexibility.&entry.1838667208=http%3A//arxiv.org/abs/2511.10040v2&entry.124074799=Read"},
{"title": "Vision Large Language Models Are Good Noise Handlers in Engagement Analysis", "author": "Alexander Vedernikov and Puneet Kumar and Haoyu Chen and Tapio Sepp\u00e4nen and Xiaobai Li", "abstract": "Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS / PAFE with F1 gains of +0.22 / +0.06.", "link": "http://arxiv.org/abs/2511.14749v1", "date": "2025-11-18", "relevancy": 2.9518, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5993}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5859}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Large%20Language%20Models%20Are%20Good%20Noise%20Handlers%20in%20Engagement%20Analysis&body=Title%3A%20Vision%20Large%20Language%20Models%20Are%20Good%20Noise%20Handlers%20in%20Engagement%20Analysis%0AAuthor%3A%20Alexander%20Vedernikov%20and%20Puneet%20Kumar%20and%20Haoyu%20Chen%20and%20Tapio%20Sepp%C3%A4nen%20and%20Xiaobai%20Li%0AAbstract%3A%20Engagement%20recognition%20in%20video%20datasets%2C%20unlike%20traditional%20image%20classification%20tasks%2C%20is%20particularly%20challenged%20by%20subjective%20labels%20and%20noise%20limiting%20model%20performance.%20To%20overcome%20the%20challenges%20of%20subjective%20and%20noisy%20engagement%20labels%2C%20we%20propose%20a%20framework%20leveraging%20Vision%20Large%20Language%20Models%20%28VLMs%29%20to%20refine%20annotations%20and%20guide%20the%20training%20process.%20Our%20framework%20uses%20a%20questionnaire%20to%20extract%20behavioral%20cues%20and%20split%20data%20into%20high-%20and%20low-reliability%20subsets.%20We%20also%20introduce%20a%20training%20strategy%20combining%20curriculum%20learning%20with%20soft%20label%20refinement%2C%20gradually%20incorporating%20ambiguous%20samples%20while%20adjusting%20supervision%20to%20reflect%20uncertainty.%20We%20demonstrate%20that%20classical%20computer%20vision%20models%20trained%20on%20refined%20high-reliability%20subsets%20and%20enhanced%20with%20our%20curriculum%20strategy%20show%20improvements%2C%20highlighting%20benefits%20of%20addressing%20label%20subjectivity%20with%20VLMs.%20This%20method%20surpasses%20prior%20state%20of%20the%20art%20across%20engagement%20benchmarks%20such%20as%20EngageNet%20%28three%20of%20six%20feature%20settings%2C%20maximum%20improvement%20of%20%2B1.21%25%29%2C%20and%20DREAMS%20/%20PAFE%20with%20F1%20gains%20of%20%2B0.22%20/%20%2B0.06.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Large%2520Language%2520Models%2520Are%2520Good%2520Noise%2520Handlers%2520in%2520Engagement%2520Analysis%26entry.906535625%3DAlexander%2520Vedernikov%2520and%2520Puneet%2520Kumar%2520and%2520Haoyu%2520Chen%2520and%2520Tapio%2520Sepp%25C3%25A4nen%2520and%2520Xiaobai%2520Li%26entry.1292438233%3DEngagement%2520recognition%2520in%2520video%2520datasets%252C%2520unlike%2520traditional%2520image%2520classification%2520tasks%252C%2520is%2520particularly%2520challenged%2520by%2520subjective%2520labels%2520and%2520noise%2520limiting%2520model%2520performance.%2520To%2520overcome%2520the%2520challenges%2520of%2520subjective%2520and%2520noisy%2520engagement%2520labels%252C%2520we%2520propose%2520a%2520framework%2520leveraging%2520Vision%2520Large%2520Language%2520Models%2520%2528VLMs%2529%2520to%2520refine%2520annotations%2520and%2520guide%2520the%2520training%2520process.%2520Our%2520framework%2520uses%2520a%2520questionnaire%2520to%2520extract%2520behavioral%2520cues%2520and%2520split%2520data%2520into%2520high-%2520and%2520low-reliability%2520subsets.%2520We%2520also%2520introduce%2520a%2520training%2520strategy%2520combining%2520curriculum%2520learning%2520with%2520soft%2520label%2520refinement%252C%2520gradually%2520incorporating%2520ambiguous%2520samples%2520while%2520adjusting%2520supervision%2520to%2520reflect%2520uncertainty.%2520We%2520demonstrate%2520that%2520classical%2520computer%2520vision%2520models%2520trained%2520on%2520refined%2520high-reliability%2520subsets%2520and%2520enhanced%2520with%2520our%2520curriculum%2520strategy%2520show%2520improvements%252C%2520highlighting%2520benefits%2520of%2520addressing%2520label%2520subjectivity%2520with%2520VLMs.%2520This%2520method%2520surpasses%2520prior%2520state%2520of%2520the%2520art%2520across%2520engagement%2520benchmarks%2520such%2520as%2520EngageNet%2520%2528three%2520of%2520six%2520feature%2520settings%252C%2520maximum%2520improvement%2520of%2520%252B1.21%2525%2529%252C%2520and%2520DREAMS%2520/%2520PAFE%2520with%2520F1%2520gains%2520of%2520%252B0.22%2520/%2520%252B0.06.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Large%20Language%20Models%20Are%20Good%20Noise%20Handlers%20in%20Engagement%20Analysis&entry.906535625=Alexander%20Vedernikov%20and%20Puneet%20Kumar%20and%20Haoyu%20Chen%20and%20Tapio%20Sepp%C3%A4nen%20and%20Xiaobai%20Li&entry.1292438233=Engagement%20recognition%20in%20video%20datasets%2C%20unlike%20traditional%20image%20classification%20tasks%2C%20is%20particularly%20challenged%20by%20subjective%20labels%20and%20noise%20limiting%20model%20performance.%20To%20overcome%20the%20challenges%20of%20subjective%20and%20noisy%20engagement%20labels%2C%20we%20propose%20a%20framework%20leveraging%20Vision%20Large%20Language%20Models%20%28VLMs%29%20to%20refine%20annotations%20and%20guide%20the%20training%20process.%20Our%20framework%20uses%20a%20questionnaire%20to%20extract%20behavioral%20cues%20and%20split%20data%20into%20high-%20and%20low-reliability%20subsets.%20We%20also%20introduce%20a%20training%20strategy%20combining%20curriculum%20learning%20with%20soft%20label%20refinement%2C%20gradually%20incorporating%20ambiguous%20samples%20while%20adjusting%20supervision%20to%20reflect%20uncertainty.%20We%20demonstrate%20that%20classical%20computer%20vision%20models%20trained%20on%20refined%20high-reliability%20subsets%20and%20enhanced%20with%20our%20curriculum%20strategy%20show%20improvements%2C%20highlighting%20benefits%20of%20addressing%20label%20subjectivity%20with%20VLMs.%20This%20method%20surpasses%20prior%20state%20of%20the%20art%20across%20engagement%20benchmarks%20such%20as%20EngageNet%20%28three%20of%20six%20feature%20settings%2C%20maximum%20improvement%20of%20%2B1.21%25%29%2C%20and%20DREAMS%20/%20PAFE%20with%20F1%20gains%20of%20%2B0.22%20/%20%2B0.06.&entry.1838667208=http%3A//arxiv.org/abs/2511.14749v1&entry.124074799=Read"},
{"title": "DepthVision: Enabling Robust Vision-Language Models with GAN-Based LiDAR-to-RGB Synthesis for Autonomous Driving", "author": "Sven Kirchner and Nils Purschke and Ross Greer and Alois C. Knoll", "abstract": "Ensuring reliable autonomous operation when visual input is degraded remains a key challenge in intelligent vehicles and robotics. We present DepthVision, a multimodal framework that enables Vision--Language Models (VLMs) to exploit LiDAR data without any architectural changes or retraining. DepthVision synthesizes dense, RGB-like images from sparse LiDAR point clouds using a conditional GAN with an integrated refiner, and feeds these into off-the-shelf VLMs through their standard visual interface. A Luminance-Aware Modality Adaptation (LAMA) module fuses synthesized and real camera images by dynamically weighting each modality based on ambient lighting, compensating for degradation such as darkness or motion blur. This design turns LiDAR into a drop-in visual surrogate when RGB becomes unreliable, effectively extending the operational envelope of existing VLMs. We evaluate DepthVision on real and simulated datasets across multiple VLMs and safety-critical tasks, including vehicle-in-the-loop experiments. The results show substantial improvements in low-light scene understanding over RGB-only baselines while preserving full compatibility with frozen VLM architectures. These findings demonstrate that LiDAR-guided RGB synthesis is a practical pathway for integrating range sensing into modern vision-language systems for autonomous driving.", "link": "http://arxiv.org/abs/2509.07463v2", "date": "2025-11-18", "relevancy": 2.928, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5942}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5942}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DepthVision%3A%20Enabling%20Robust%20Vision-Language%20Models%20with%20GAN-Based%20LiDAR-to-RGB%20Synthesis%20for%20Autonomous%20Driving&body=Title%3A%20DepthVision%3A%20Enabling%20Robust%20Vision-Language%20Models%20with%20GAN-Based%20LiDAR-to-RGB%20Synthesis%20for%20Autonomous%20Driving%0AAuthor%3A%20Sven%20Kirchner%20and%20Nils%20Purschke%20and%20Ross%20Greer%20and%20Alois%20C.%20Knoll%0AAbstract%3A%20Ensuring%20reliable%20autonomous%20operation%20when%20visual%20input%20is%20degraded%20remains%20a%20key%20challenge%20in%20intelligent%20vehicles%20and%20robotics.%20We%20present%20DepthVision%2C%20a%20multimodal%20framework%20that%20enables%20Vision--Language%20Models%20%28VLMs%29%20to%20exploit%20LiDAR%20data%20without%20any%20architectural%20changes%20or%20retraining.%20DepthVision%20synthesizes%20dense%2C%20RGB-like%20images%20from%20sparse%20LiDAR%20point%20clouds%20using%20a%20conditional%20GAN%20with%20an%20integrated%20refiner%2C%20and%20feeds%20these%20into%20off-the-shelf%20VLMs%20through%20their%20standard%20visual%20interface.%20A%20Luminance-Aware%20Modality%20Adaptation%20%28LAMA%29%20module%20fuses%20synthesized%20and%20real%20camera%20images%20by%20dynamically%20weighting%20each%20modality%20based%20on%20ambient%20lighting%2C%20compensating%20for%20degradation%20such%20as%20darkness%20or%20motion%20blur.%20This%20design%20turns%20LiDAR%20into%20a%20drop-in%20visual%20surrogate%20when%20RGB%20becomes%20unreliable%2C%20effectively%20extending%20the%20operational%20envelope%20of%20existing%20VLMs.%20We%20evaluate%20DepthVision%20on%20real%20and%20simulated%20datasets%20across%20multiple%20VLMs%20and%20safety-critical%20tasks%2C%20including%20vehicle-in-the-loop%20experiments.%20The%20results%20show%20substantial%20improvements%20in%20low-light%20scene%20understanding%20over%20RGB-only%20baselines%20while%20preserving%20full%20compatibility%20with%20frozen%20VLM%20architectures.%20These%20findings%20demonstrate%20that%20LiDAR-guided%20RGB%20synthesis%20is%20a%20practical%20pathway%20for%20integrating%20range%20sensing%20into%20modern%20vision-language%20systems%20for%20autonomous%20driving.%0ALink%3A%20http%3A//arxiv.org/abs/2509.07463v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepthVision%253A%2520Enabling%2520Robust%2520Vision-Language%2520Models%2520with%2520GAN-Based%2520LiDAR-to-RGB%2520Synthesis%2520for%2520Autonomous%2520Driving%26entry.906535625%3DSven%2520Kirchner%2520and%2520Nils%2520Purschke%2520and%2520Ross%2520Greer%2520and%2520Alois%2520C.%2520Knoll%26entry.1292438233%3DEnsuring%2520reliable%2520autonomous%2520operation%2520when%2520visual%2520input%2520is%2520degraded%2520remains%2520a%2520key%2520challenge%2520in%2520intelligent%2520vehicles%2520and%2520robotics.%2520We%2520present%2520DepthVision%252C%2520a%2520multimodal%2520framework%2520that%2520enables%2520Vision--Language%2520Models%2520%2528VLMs%2529%2520to%2520exploit%2520LiDAR%2520data%2520without%2520any%2520architectural%2520changes%2520or%2520retraining.%2520DepthVision%2520synthesizes%2520dense%252C%2520RGB-like%2520images%2520from%2520sparse%2520LiDAR%2520point%2520clouds%2520using%2520a%2520conditional%2520GAN%2520with%2520an%2520integrated%2520refiner%252C%2520and%2520feeds%2520these%2520into%2520off-the-shelf%2520VLMs%2520through%2520their%2520standard%2520visual%2520interface.%2520A%2520Luminance-Aware%2520Modality%2520Adaptation%2520%2528LAMA%2529%2520module%2520fuses%2520synthesized%2520and%2520real%2520camera%2520images%2520by%2520dynamically%2520weighting%2520each%2520modality%2520based%2520on%2520ambient%2520lighting%252C%2520compensating%2520for%2520degradation%2520such%2520as%2520darkness%2520or%2520motion%2520blur.%2520This%2520design%2520turns%2520LiDAR%2520into%2520a%2520drop-in%2520visual%2520surrogate%2520when%2520RGB%2520becomes%2520unreliable%252C%2520effectively%2520extending%2520the%2520operational%2520envelope%2520of%2520existing%2520VLMs.%2520We%2520evaluate%2520DepthVision%2520on%2520real%2520and%2520simulated%2520datasets%2520across%2520multiple%2520VLMs%2520and%2520safety-critical%2520tasks%252C%2520including%2520vehicle-in-the-loop%2520experiments.%2520The%2520results%2520show%2520substantial%2520improvements%2520in%2520low-light%2520scene%2520understanding%2520over%2520RGB-only%2520baselines%2520while%2520preserving%2520full%2520compatibility%2520with%2520frozen%2520VLM%2520architectures.%2520These%2520findings%2520demonstrate%2520that%2520LiDAR-guided%2520RGB%2520synthesis%2520is%2520a%2520practical%2520pathway%2520for%2520integrating%2520range%2520sensing%2520into%2520modern%2520vision-language%2520systems%2520for%2520autonomous%2520driving.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07463v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DepthVision%3A%20Enabling%20Robust%20Vision-Language%20Models%20with%20GAN-Based%20LiDAR-to-RGB%20Synthesis%20for%20Autonomous%20Driving&entry.906535625=Sven%20Kirchner%20and%20Nils%20Purschke%20and%20Ross%20Greer%20and%20Alois%20C.%20Knoll&entry.1292438233=Ensuring%20reliable%20autonomous%20operation%20when%20visual%20input%20is%20degraded%20remains%20a%20key%20challenge%20in%20intelligent%20vehicles%20and%20robotics.%20We%20present%20DepthVision%2C%20a%20multimodal%20framework%20that%20enables%20Vision--Language%20Models%20%28VLMs%29%20to%20exploit%20LiDAR%20data%20without%20any%20architectural%20changes%20or%20retraining.%20DepthVision%20synthesizes%20dense%2C%20RGB-like%20images%20from%20sparse%20LiDAR%20point%20clouds%20using%20a%20conditional%20GAN%20with%20an%20integrated%20refiner%2C%20and%20feeds%20these%20into%20off-the-shelf%20VLMs%20through%20their%20standard%20visual%20interface.%20A%20Luminance-Aware%20Modality%20Adaptation%20%28LAMA%29%20module%20fuses%20synthesized%20and%20real%20camera%20images%20by%20dynamically%20weighting%20each%20modality%20based%20on%20ambient%20lighting%2C%20compensating%20for%20degradation%20such%20as%20darkness%20or%20motion%20blur.%20This%20design%20turns%20LiDAR%20into%20a%20drop-in%20visual%20surrogate%20when%20RGB%20becomes%20unreliable%2C%20effectively%20extending%20the%20operational%20envelope%20of%20existing%20VLMs.%20We%20evaluate%20DepthVision%20on%20real%20and%20simulated%20datasets%20across%20multiple%20VLMs%20and%20safety-critical%20tasks%2C%20including%20vehicle-in-the-loop%20experiments.%20The%20results%20show%20substantial%20improvements%20in%20low-light%20scene%20understanding%20over%20RGB-only%20baselines%20while%20preserving%20full%20compatibility%20with%20frozen%20VLM%20architectures.%20These%20findings%20demonstrate%20that%20LiDAR-guided%20RGB%20synthesis%20is%20a%20practical%20pathway%20for%20integrating%20range%20sensing%20into%20modern%20vision-language%20systems%20for%20autonomous%20driving.&entry.1838667208=http%3A//arxiv.org/abs/2509.07463v2&entry.124074799=Read"},
{"title": "Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising", "author": "Yifan Wang and Liya Ji and Zhanghan Ke and Harry Yang and Ser-Nam Lim and Qifeng Chen", "abstract": "We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion. Our realism enhancement approach is a zero-shot framework that focuses on preserving the multi-level structures from synthetic videos into the enhanced one in both spatial and temporal domains, built upon a diffusion video foundational model without further fine-tuning. Specifically, we incorporate an effective modification to have the generation/denoising process conditioned on estimated structure-aware information from the synthetic video, such as depth maps, semantic maps, and edge maps, by an auxiliary model, rather than extracting the information from a simulator. This guidance ensures that the enhanced videos are consistent with the original synthetic video at both the structural and semantic levels. Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.", "link": "http://arxiv.org/abs/2511.14719v1", "date": "2025-11-18", "relevancy": 2.9182, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6268}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5623}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20Synthetic%20Video%20Realism%20Enhancement%20via%20Structure-aware%20Denoising&body=Title%3A%20Zero-shot%20Synthetic%20Video%20Realism%20Enhancement%20via%20Structure-aware%20Denoising%0AAuthor%3A%20Yifan%20Wang%20and%20Liya%20Ji%20and%20Zhanghan%20Ke%20and%20Harry%20Yang%20and%20Ser-Nam%20Lim%20and%20Qifeng%20Chen%0AAbstract%3A%20We%20propose%20an%20approach%20to%20enhancing%20synthetic%20video%20realism%2C%20which%20can%20re-render%20synthetic%20videos%20from%20a%20simulator%20in%20photorealistic%20fashion.%20Our%20realism%20enhancement%20approach%20is%20a%20zero-shot%20framework%20that%20focuses%20on%20preserving%20the%20multi-level%20structures%20from%20synthetic%20videos%20into%20the%20enhanced%20one%20in%20both%20spatial%20and%20temporal%20domains%2C%20built%20upon%20a%20diffusion%20video%20foundational%20model%20without%20further%20fine-tuning.%20Specifically%2C%20we%20incorporate%20an%20effective%20modification%20to%20have%20the%20generation/denoising%20process%20conditioned%20on%20estimated%20structure-aware%20information%20from%20the%20synthetic%20video%2C%20such%20as%20depth%20maps%2C%20semantic%20maps%2C%20and%20edge%20maps%2C%20by%20an%20auxiliary%20model%2C%20rather%20than%20extracting%20the%20information%20from%20a%20simulator.%20This%20guidance%20ensures%20that%20the%20enhanced%20videos%20are%20consistent%20with%20the%20original%20synthetic%20video%20at%20both%20the%20structural%20and%20semantic%20levels.%20Our%20approach%20is%20a%20simple%20yet%20general%20and%20powerful%20approach%20to%20enhancing%20synthetic%20video%20realism%3A%20we%20show%20that%20our%20approach%20outperforms%20existing%20baselines%20in%20structural%20consistency%20with%20the%20original%20video%20while%20maintaining%20state-of-the-art%20photorealism%20quality%20in%20our%20experiments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520Synthetic%2520Video%2520Realism%2520Enhancement%2520via%2520Structure-aware%2520Denoising%26entry.906535625%3DYifan%2520Wang%2520and%2520Liya%2520Ji%2520and%2520Zhanghan%2520Ke%2520and%2520Harry%2520Yang%2520and%2520Ser-Nam%2520Lim%2520and%2520Qifeng%2520Chen%26entry.1292438233%3DWe%2520propose%2520an%2520approach%2520to%2520enhancing%2520synthetic%2520video%2520realism%252C%2520which%2520can%2520re-render%2520synthetic%2520videos%2520from%2520a%2520simulator%2520in%2520photorealistic%2520fashion.%2520Our%2520realism%2520enhancement%2520approach%2520is%2520a%2520zero-shot%2520framework%2520that%2520focuses%2520on%2520preserving%2520the%2520multi-level%2520structures%2520from%2520synthetic%2520videos%2520into%2520the%2520enhanced%2520one%2520in%2520both%2520spatial%2520and%2520temporal%2520domains%252C%2520built%2520upon%2520a%2520diffusion%2520video%2520foundational%2520model%2520without%2520further%2520fine-tuning.%2520Specifically%252C%2520we%2520incorporate%2520an%2520effective%2520modification%2520to%2520have%2520the%2520generation/denoising%2520process%2520conditioned%2520on%2520estimated%2520structure-aware%2520information%2520from%2520the%2520synthetic%2520video%252C%2520such%2520as%2520depth%2520maps%252C%2520semantic%2520maps%252C%2520and%2520edge%2520maps%252C%2520by%2520an%2520auxiliary%2520model%252C%2520rather%2520than%2520extracting%2520the%2520information%2520from%2520a%2520simulator.%2520This%2520guidance%2520ensures%2520that%2520the%2520enhanced%2520videos%2520are%2520consistent%2520with%2520the%2520original%2520synthetic%2520video%2520at%2520both%2520the%2520structural%2520and%2520semantic%2520levels.%2520Our%2520approach%2520is%2520a%2520simple%2520yet%2520general%2520and%2520powerful%2520approach%2520to%2520enhancing%2520synthetic%2520video%2520realism%253A%2520we%2520show%2520that%2520our%2520approach%2520outperforms%2520existing%2520baselines%2520in%2520structural%2520consistency%2520with%2520the%2520original%2520video%2520while%2520maintaining%2520state-of-the-art%2520photorealism%2520quality%2520in%2520our%2520experiments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20Synthetic%20Video%20Realism%20Enhancement%20via%20Structure-aware%20Denoising&entry.906535625=Yifan%20Wang%20and%20Liya%20Ji%20and%20Zhanghan%20Ke%20and%20Harry%20Yang%20and%20Ser-Nam%20Lim%20and%20Qifeng%20Chen&entry.1292438233=We%20propose%20an%20approach%20to%20enhancing%20synthetic%20video%20realism%2C%20which%20can%20re-render%20synthetic%20videos%20from%20a%20simulator%20in%20photorealistic%20fashion.%20Our%20realism%20enhancement%20approach%20is%20a%20zero-shot%20framework%20that%20focuses%20on%20preserving%20the%20multi-level%20structures%20from%20synthetic%20videos%20into%20the%20enhanced%20one%20in%20both%20spatial%20and%20temporal%20domains%2C%20built%20upon%20a%20diffusion%20video%20foundational%20model%20without%20further%20fine-tuning.%20Specifically%2C%20we%20incorporate%20an%20effective%20modification%20to%20have%20the%20generation/denoising%20process%20conditioned%20on%20estimated%20structure-aware%20information%20from%20the%20synthetic%20video%2C%20such%20as%20depth%20maps%2C%20semantic%20maps%2C%20and%20edge%20maps%2C%20by%20an%20auxiliary%20model%2C%20rather%20than%20extracting%20the%20information%20from%20a%20simulator.%20This%20guidance%20ensures%20that%20the%20enhanced%20videos%20are%20consistent%20with%20the%20original%20synthetic%20video%20at%20both%20the%20structural%20and%20semantic%20levels.%20Our%20approach%20is%20a%20simple%20yet%20general%20and%20powerful%20approach%20to%20enhancing%20synthetic%20video%20realism%3A%20we%20show%20that%20our%20approach%20outperforms%20existing%20baselines%20in%20structural%20consistency%20with%20the%20original%20video%20while%20maintaining%20state-of-the-art%20photorealism%20quality%20in%20our%20experiments.&entry.1838667208=http%3A//arxiv.org/abs/2511.14719v1&entry.124074799=Read"},
{"title": "Towards Understanding 3D Vision: the Role of Gaussian Curvature", "author": "Sherlon Almeida da Silva and Davi Geiger and Luiz Velho and Moacir Antonelli Ponti", "abstract": "Recent advances in computer vision have predominantly relied on data-driven approaches that leverage deep learning and large-scale datasets. Deep neural networks have achieved remarkable success in tasks such as stereo matching and monocular depth reconstruction. However, these methods lack explicit models of 3D geometry that can be directly analyzed, transferred across modalities, or systematically modified for controlled experimentation. We investigate the role of Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being an invariant quantity under change of observers or coordinate systems, we demonstrate using the Middlebury stereo dataset that it offers a sparse and compact description of 3D surfaces. Furthermore, we show a strong correlation between the performance rank of top state-of-the-art stereo and monocular methods and the low total absolute Gaussian curvature. We propose that this property can serve as a geometric prior to improve future 3D reconstruction algorithms.", "link": "http://arxiv.org/abs/2508.11825v2", "date": "2025-11-18", "relevancy": 2.9145, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5923}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5826}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Understanding%203D%20Vision%3A%20the%20Role%20of%20Gaussian%20Curvature&body=Title%3A%20Towards%20Understanding%203D%20Vision%3A%20the%20Role%20of%20Gaussian%20Curvature%0AAuthor%3A%20Sherlon%20Almeida%20da%20Silva%20and%20Davi%20Geiger%20and%20Luiz%20Velho%20and%20Moacir%20Antonelli%20Ponti%0AAbstract%3A%20Recent%20advances%20in%20computer%20vision%20have%20predominantly%20relied%20on%20data-driven%20approaches%20that%20leverage%20deep%20learning%20and%20large-scale%20datasets.%20Deep%20neural%20networks%20have%20achieved%20remarkable%20success%20in%20tasks%20such%20as%20stereo%20matching%20and%20monocular%20depth%20reconstruction.%20However%2C%20these%20methods%20lack%20explicit%20models%20of%203D%20geometry%20that%20can%20be%20directly%20analyzed%2C%20transferred%20across%20modalities%2C%20or%20systematically%20modified%20for%20controlled%20experimentation.%20We%20investigate%20the%20role%20of%20Gaussian%20curvature%20in%203D%20surface%20modeling.%20Besides%20Gaussian%20curvature%20being%20an%20invariant%20quantity%20under%20change%20of%20observers%20or%20coordinate%20systems%2C%20we%20demonstrate%20using%20the%20Middlebury%20stereo%20dataset%20that%20it%20offers%20a%20sparse%20and%20compact%20description%20of%203D%20surfaces.%20Furthermore%2C%20we%20show%20a%20strong%20correlation%20between%20the%20performance%20rank%20of%20top%20state-of-the-art%20stereo%20and%20monocular%20methods%20and%20the%20low%20total%20absolute%20Gaussian%20curvature.%20We%20propose%20that%20this%20property%20can%20serve%20as%20a%20geometric%20prior%20to%20improve%20future%203D%20reconstruction%20algorithms.%0ALink%3A%20http%3A//arxiv.org/abs/2508.11825v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Understanding%25203D%2520Vision%253A%2520the%2520Role%2520of%2520Gaussian%2520Curvature%26entry.906535625%3DSherlon%2520Almeida%2520da%2520Silva%2520and%2520Davi%2520Geiger%2520and%2520Luiz%2520Velho%2520and%2520Moacir%2520Antonelli%2520Ponti%26entry.1292438233%3DRecent%2520advances%2520in%2520computer%2520vision%2520have%2520predominantly%2520relied%2520on%2520data-driven%2520approaches%2520that%2520leverage%2520deep%2520learning%2520and%2520large-scale%2520datasets.%2520Deep%2520neural%2520networks%2520have%2520achieved%2520remarkable%2520success%2520in%2520tasks%2520such%2520as%2520stereo%2520matching%2520and%2520monocular%2520depth%2520reconstruction.%2520However%252C%2520these%2520methods%2520lack%2520explicit%2520models%2520of%25203D%2520geometry%2520that%2520can%2520be%2520directly%2520analyzed%252C%2520transferred%2520across%2520modalities%252C%2520or%2520systematically%2520modified%2520for%2520controlled%2520experimentation.%2520We%2520investigate%2520the%2520role%2520of%2520Gaussian%2520curvature%2520in%25203D%2520surface%2520modeling.%2520Besides%2520Gaussian%2520curvature%2520being%2520an%2520invariant%2520quantity%2520under%2520change%2520of%2520observers%2520or%2520coordinate%2520systems%252C%2520we%2520demonstrate%2520using%2520the%2520Middlebury%2520stereo%2520dataset%2520that%2520it%2520offers%2520a%2520sparse%2520and%2520compact%2520description%2520of%25203D%2520surfaces.%2520Furthermore%252C%2520we%2520show%2520a%2520strong%2520correlation%2520between%2520the%2520performance%2520rank%2520of%2520top%2520state-of-the-art%2520stereo%2520and%2520monocular%2520methods%2520and%2520the%2520low%2520total%2520absolute%2520Gaussian%2520curvature.%2520We%2520propose%2520that%2520this%2520property%2520can%2520serve%2520as%2520a%2520geometric%2520prior%2520to%2520improve%2520future%25203D%2520reconstruction%2520algorithms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11825v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Understanding%203D%20Vision%3A%20the%20Role%20of%20Gaussian%20Curvature&entry.906535625=Sherlon%20Almeida%20da%20Silva%20and%20Davi%20Geiger%20and%20Luiz%20Velho%20and%20Moacir%20Antonelli%20Ponti&entry.1292438233=Recent%20advances%20in%20computer%20vision%20have%20predominantly%20relied%20on%20data-driven%20approaches%20that%20leverage%20deep%20learning%20and%20large-scale%20datasets.%20Deep%20neural%20networks%20have%20achieved%20remarkable%20success%20in%20tasks%20such%20as%20stereo%20matching%20and%20monocular%20depth%20reconstruction.%20However%2C%20these%20methods%20lack%20explicit%20models%20of%203D%20geometry%20that%20can%20be%20directly%20analyzed%2C%20transferred%20across%20modalities%2C%20or%20systematically%20modified%20for%20controlled%20experimentation.%20We%20investigate%20the%20role%20of%20Gaussian%20curvature%20in%203D%20surface%20modeling.%20Besides%20Gaussian%20curvature%20being%20an%20invariant%20quantity%20under%20change%20of%20observers%20or%20coordinate%20systems%2C%20we%20demonstrate%20using%20the%20Middlebury%20stereo%20dataset%20that%20it%20offers%20a%20sparse%20and%20compact%20description%20of%203D%20surfaces.%20Furthermore%2C%20we%20show%20a%20strong%20correlation%20between%20the%20performance%20rank%20of%20top%20state-of-the-art%20stereo%20and%20monocular%20methods%20and%20the%20low%20total%20absolute%20Gaussian%20curvature.%20We%20propose%20that%20this%20property%20can%20serve%20as%20a%20geometric%20prior%20to%20improve%20future%203D%20reconstruction%20algorithms.&entry.1838667208=http%3A//arxiv.org/abs/2508.11825v2&entry.124074799=Read"},
{"title": "Language as an Anchor: Preserving Relative Visual Geometry for Domain Incremental Learning", "author": "Shuyi Geng and Tao Zhou and Yi Zhou", "abstract": "A key challenge in Domain Incremental Learning (DIL) is to continually learn under shifting distributions while preserving knowledge from previous domains. Existing methods face a fundamental dilemma. On one hand, projecting all domains into a single unified visual space leads to inter-domain interference and semantic distortion, as large shifts may vary with not only visual appearance but also underlying semantics. On the other hand, isolating domain-specific parameters causes knowledge fragmentation, creating \"knowledge islands\" that hamper knowledge reuse and exacerbate forgetting. To address this issue, we propose LAVA (Language-Anchored Visual Alignment), a novel DIL framework that replaces direct feature alignment with relative alignment driven by a text-based reference anchor. LAVA guides the visual representations of each incoming domain to preserve a consistent relative geometry, which is defined by mirroring the pairwise semantic similarities between the class names. This anchored geometric structure acts as a bridge across domains, enabling the retrieval of class-aware prior knowledge and facilitating robust feature aggregation. Extensive experiments on standard DIL benchmarks demonstrate that LAVA achieves significant performance improvements over state-of-the-arts. Code is available at https://github.com/ShuyiGeng/LAVA.", "link": "http://arxiv.org/abs/2511.14401v1", "date": "2025-11-18", "relevancy": 2.8755, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.595}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20as%20an%20Anchor%3A%20Preserving%20Relative%20Visual%20Geometry%20for%20Domain%20Incremental%20Learning&body=Title%3A%20Language%20as%20an%20Anchor%3A%20Preserving%20Relative%20Visual%20Geometry%20for%20Domain%20Incremental%20Learning%0AAuthor%3A%20Shuyi%20Geng%20and%20Tao%20Zhou%20and%20Yi%20Zhou%0AAbstract%3A%20A%20key%20challenge%20in%20Domain%20Incremental%20Learning%20%28DIL%29%20is%20to%20continually%20learn%20under%20shifting%20distributions%20while%20preserving%20knowledge%20from%20previous%20domains.%20Existing%20methods%20face%20a%20fundamental%20dilemma.%20On%20one%20hand%2C%20projecting%20all%20domains%20into%20a%20single%20unified%20visual%20space%20leads%20to%20inter-domain%20interference%20and%20semantic%20distortion%2C%20as%20large%20shifts%20may%20vary%20with%20not%20only%20visual%20appearance%20but%20also%20underlying%20semantics.%20On%20the%20other%20hand%2C%20isolating%20domain-specific%20parameters%20causes%20knowledge%20fragmentation%2C%20creating%20%22knowledge%20islands%22%20that%20hamper%20knowledge%20reuse%20and%20exacerbate%20forgetting.%20To%20address%20this%20issue%2C%20we%20propose%20LAVA%20%28Language-Anchored%20Visual%20Alignment%29%2C%20a%20novel%20DIL%20framework%20that%20replaces%20direct%20feature%20alignment%20with%20relative%20alignment%20driven%20by%20a%20text-based%20reference%20anchor.%20LAVA%20guides%20the%20visual%20representations%20of%20each%20incoming%20domain%20to%20preserve%20a%20consistent%20relative%20geometry%2C%20which%20is%20defined%20by%20mirroring%20the%20pairwise%20semantic%20similarities%20between%20the%20class%20names.%20This%20anchored%20geometric%20structure%20acts%20as%20a%20bridge%20across%20domains%2C%20enabling%20the%20retrieval%20of%20class-aware%20prior%20knowledge%20and%20facilitating%20robust%20feature%20aggregation.%20Extensive%20experiments%20on%20standard%20DIL%20benchmarks%20demonstrate%20that%20LAVA%20achieves%20significant%20performance%20improvements%20over%20state-of-the-arts.%20Code%20is%20available%20at%20https%3A//github.com/ShuyiGeng/LAVA.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520as%2520an%2520Anchor%253A%2520Preserving%2520Relative%2520Visual%2520Geometry%2520for%2520Domain%2520Incremental%2520Learning%26entry.906535625%3DShuyi%2520Geng%2520and%2520Tao%2520Zhou%2520and%2520Yi%2520Zhou%26entry.1292438233%3DA%2520key%2520challenge%2520in%2520Domain%2520Incremental%2520Learning%2520%2528DIL%2529%2520is%2520to%2520continually%2520learn%2520under%2520shifting%2520distributions%2520while%2520preserving%2520knowledge%2520from%2520previous%2520domains.%2520Existing%2520methods%2520face%2520a%2520fundamental%2520dilemma.%2520On%2520one%2520hand%252C%2520projecting%2520all%2520domains%2520into%2520a%2520single%2520unified%2520visual%2520space%2520leads%2520to%2520inter-domain%2520interference%2520and%2520semantic%2520distortion%252C%2520as%2520large%2520shifts%2520may%2520vary%2520with%2520not%2520only%2520visual%2520appearance%2520but%2520also%2520underlying%2520semantics.%2520On%2520the%2520other%2520hand%252C%2520isolating%2520domain-specific%2520parameters%2520causes%2520knowledge%2520fragmentation%252C%2520creating%2520%2522knowledge%2520islands%2522%2520that%2520hamper%2520knowledge%2520reuse%2520and%2520exacerbate%2520forgetting.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520LAVA%2520%2528Language-Anchored%2520Visual%2520Alignment%2529%252C%2520a%2520novel%2520DIL%2520framework%2520that%2520replaces%2520direct%2520feature%2520alignment%2520with%2520relative%2520alignment%2520driven%2520by%2520a%2520text-based%2520reference%2520anchor.%2520LAVA%2520guides%2520the%2520visual%2520representations%2520of%2520each%2520incoming%2520domain%2520to%2520preserve%2520a%2520consistent%2520relative%2520geometry%252C%2520which%2520is%2520defined%2520by%2520mirroring%2520the%2520pairwise%2520semantic%2520similarities%2520between%2520the%2520class%2520names.%2520This%2520anchored%2520geometric%2520structure%2520acts%2520as%2520a%2520bridge%2520across%2520domains%252C%2520enabling%2520the%2520retrieval%2520of%2520class-aware%2520prior%2520knowledge%2520and%2520facilitating%2520robust%2520feature%2520aggregation.%2520Extensive%2520experiments%2520on%2520standard%2520DIL%2520benchmarks%2520demonstrate%2520that%2520LAVA%2520achieves%2520significant%2520performance%2520improvements%2520over%2520state-of-the-arts.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/ShuyiGeng/LAVA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20as%20an%20Anchor%3A%20Preserving%20Relative%20Visual%20Geometry%20for%20Domain%20Incremental%20Learning&entry.906535625=Shuyi%20Geng%20and%20Tao%20Zhou%20and%20Yi%20Zhou&entry.1292438233=A%20key%20challenge%20in%20Domain%20Incremental%20Learning%20%28DIL%29%20is%20to%20continually%20learn%20under%20shifting%20distributions%20while%20preserving%20knowledge%20from%20previous%20domains.%20Existing%20methods%20face%20a%20fundamental%20dilemma.%20On%20one%20hand%2C%20projecting%20all%20domains%20into%20a%20single%20unified%20visual%20space%20leads%20to%20inter-domain%20interference%20and%20semantic%20distortion%2C%20as%20large%20shifts%20may%20vary%20with%20not%20only%20visual%20appearance%20but%20also%20underlying%20semantics.%20On%20the%20other%20hand%2C%20isolating%20domain-specific%20parameters%20causes%20knowledge%20fragmentation%2C%20creating%20%22knowledge%20islands%22%20that%20hamper%20knowledge%20reuse%20and%20exacerbate%20forgetting.%20To%20address%20this%20issue%2C%20we%20propose%20LAVA%20%28Language-Anchored%20Visual%20Alignment%29%2C%20a%20novel%20DIL%20framework%20that%20replaces%20direct%20feature%20alignment%20with%20relative%20alignment%20driven%20by%20a%20text-based%20reference%20anchor.%20LAVA%20guides%20the%20visual%20representations%20of%20each%20incoming%20domain%20to%20preserve%20a%20consistent%20relative%20geometry%2C%20which%20is%20defined%20by%20mirroring%20the%20pairwise%20semantic%20similarities%20between%20the%20class%20names.%20This%20anchored%20geometric%20structure%20acts%20as%20a%20bridge%20across%20domains%2C%20enabling%20the%20retrieval%20of%20class-aware%20prior%20knowledge%20and%20facilitating%20robust%20feature%20aggregation.%20Extensive%20experiments%20on%20standard%20DIL%20benchmarks%20demonstrate%20that%20LAVA%20achieves%20significant%20performance%20improvements%20over%20state-of-the-arts.%20Code%20is%20available%20at%20https%3A//github.com/ShuyiGeng/LAVA.&entry.1838667208=http%3A//arxiv.org/abs/2511.14401v1&entry.124074799=Read"},
{"title": "Towards Reliable Human Evaluations in Gesture Generation: Insights from a Community-Driven State-of-the-Art Benchmark", "author": "Rajmund Nagy and Hendric Voss and Thanh Hoang-Minh and Mihail Tsakov and Teodor Nikolov and Zeyi Zhang and Tenglong Ao and Sicheng Yang and Shaoli Huang and Yongkang Cheng and M. Hamza Mughal and Rishabh Dabral and Kiran Chhatre and Christian Theobalt and Libin Liu and Stefan Kopp and Rachel McDonnell and Michael Neff and Taras Kucherenko and Youngwoo Yoon and Gustav Eje Henter", "abstract": "We review human evaluation practices in automated, speech-driven 3D gesture generation and find a lack of standardisation and frequent use of flawed experimental setups. This leads to a situation where it is impossible to know how different methods compare, or what the state of the art is. In order to address common shortcomings of evaluation design, and to standardise future user studies in gesture-generation works, we introduce a detailed human evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using this protocol, we conduct large-scale crowdsourced evaluation to rank six recent gesture-generation models -- each trained by its original authors -- across two key evaluation dimensions: motion realism and speech-gesture alignment. Our results provide strong evidence that 1) newer models do not consistently outperform earlier approaches; 2) published claims of high motion realism or speech-gesture alignment may not hold up under rigorous evaluation; and 3) the field must adopt disentangled assessments of motion quality and multimodal alignment for accurate benchmarking in order to make progress. Finally, in order to drive standardisation and enable new evaluation research, we will release five hours of synthetic motion from the benchmarked models; over 750 rendered video stimuli from the user studies -- enabling new evaluations without model reimplementation required -- alongside our open-source rendering script, and the 16,000 pairwise human preference votes collected for our benchmark.", "link": "http://arxiv.org/abs/2511.01233v2", "date": "2025-11-18", "relevancy": 2.8347, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5793}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Reliable%20Human%20Evaluations%20in%20Gesture%20Generation%3A%20Insights%20from%20a%20Community-Driven%20State-of-the-Art%20Benchmark&body=Title%3A%20Towards%20Reliable%20Human%20Evaluations%20in%20Gesture%20Generation%3A%20Insights%20from%20a%20Community-Driven%20State-of-the-Art%20Benchmark%0AAuthor%3A%20Rajmund%20Nagy%20and%20Hendric%20Voss%20and%20Thanh%20Hoang-Minh%20and%20Mihail%20Tsakov%20and%20Teodor%20Nikolov%20and%20Zeyi%20Zhang%20and%20Tenglong%20Ao%20and%20Sicheng%20Yang%20and%20Shaoli%20Huang%20and%20Yongkang%20Cheng%20and%20M.%20Hamza%20Mughal%20and%20Rishabh%20Dabral%20and%20Kiran%20Chhatre%20and%20Christian%20Theobalt%20and%20Libin%20Liu%20and%20Stefan%20Kopp%20and%20Rachel%20McDonnell%20and%20Michael%20Neff%20and%20Taras%20Kucherenko%20and%20Youngwoo%20Yoon%20and%20Gustav%20Eje%20Henter%0AAbstract%3A%20We%20review%20human%20evaluation%20practices%20in%20automated%2C%20speech-driven%203D%20gesture%20generation%20and%20find%20a%20lack%20of%20standardisation%20and%20frequent%20use%20of%20flawed%20experimental%20setups.%20This%20leads%20to%20a%20situation%20where%20it%20is%20impossible%20to%20know%20how%20different%20methods%20compare%2C%20or%20what%20the%20state%20of%20the%20art%20is.%20In%20order%20to%20address%20common%20shortcomings%20of%20evaluation%20design%2C%20and%20to%20standardise%20future%20user%20studies%20in%20gesture-generation%20works%2C%20we%20introduce%20a%20detailed%20human%20evaluation%20protocol%20for%20the%20widely-used%20BEAT2%20motion-capture%20dataset.%20Using%20this%20protocol%2C%20we%20conduct%20large-scale%20crowdsourced%20evaluation%20to%20rank%20six%20recent%20gesture-generation%20models%20--%20each%20trained%20by%20its%20original%20authors%20--%20across%20two%20key%20evaluation%20dimensions%3A%20motion%20realism%20and%20speech-gesture%20alignment.%20Our%20results%20provide%20strong%20evidence%20that%201%29%20newer%20models%20do%20not%20consistently%20outperform%20earlier%20approaches%3B%202%29%20published%20claims%20of%20high%20motion%20realism%20or%20speech-gesture%20alignment%20may%20not%20hold%20up%20under%20rigorous%20evaluation%3B%20and%203%29%20the%20field%20must%20adopt%20disentangled%20assessments%20of%20motion%20quality%20and%20multimodal%20alignment%20for%20accurate%20benchmarking%20in%20order%20to%20make%20progress.%20Finally%2C%20in%20order%20to%20drive%20standardisation%20and%20enable%20new%20evaluation%20research%2C%20we%20will%20release%20five%20hours%20of%20synthetic%20motion%20from%20the%20benchmarked%20models%3B%20over%20750%20rendered%20video%20stimuli%20from%20the%20user%20studies%20--%20enabling%20new%20evaluations%20without%20model%20reimplementation%20required%20--%20alongside%20our%20open-source%20rendering%20script%2C%20and%20the%2016%2C000%20pairwise%20human%20preference%20votes%20collected%20for%20our%20benchmark.%0ALink%3A%20http%3A//arxiv.org/abs/2511.01233v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Reliable%2520Human%2520Evaluations%2520in%2520Gesture%2520Generation%253A%2520Insights%2520from%2520a%2520Community-Driven%2520State-of-the-Art%2520Benchmark%26entry.906535625%3DRajmund%2520Nagy%2520and%2520Hendric%2520Voss%2520and%2520Thanh%2520Hoang-Minh%2520and%2520Mihail%2520Tsakov%2520and%2520Teodor%2520Nikolov%2520and%2520Zeyi%2520Zhang%2520and%2520Tenglong%2520Ao%2520and%2520Sicheng%2520Yang%2520and%2520Shaoli%2520Huang%2520and%2520Yongkang%2520Cheng%2520and%2520M.%2520Hamza%2520Mughal%2520and%2520Rishabh%2520Dabral%2520and%2520Kiran%2520Chhatre%2520and%2520Christian%2520Theobalt%2520and%2520Libin%2520Liu%2520and%2520Stefan%2520Kopp%2520and%2520Rachel%2520McDonnell%2520and%2520Michael%2520Neff%2520and%2520Taras%2520Kucherenko%2520and%2520Youngwoo%2520Yoon%2520and%2520Gustav%2520Eje%2520Henter%26entry.1292438233%3DWe%2520review%2520human%2520evaluation%2520practices%2520in%2520automated%252C%2520speech-driven%25203D%2520gesture%2520generation%2520and%2520find%2520a%2520lack%2520of%2520standardisation%2520and%2520frequent%2520use%2520of%2520flawed%2520experimental%2520setups.%2520This%2520leads%2520to%2520a%2520situation%2520where%2520it%2520is%2520impossible%2520to%2520know%2520how%2520different%2520methods%2520compare%252C%2520or%2520what%2520the%2520state%2520of%2520the%2520art%2520is.%2520In%2520order%2520to%2520address%2520common%2520shortcomings%2520of%2520evaluation%2520design%252C%2520and%2520to%2520standardise%2520future%2520user%2520studies%2520in%2520gesture-generation%2520works%252C%2520we%2520introduce%2520a%2520detailed%2520human%2520evaluation%2520protocol%2520for%2520the%2520widely-used%2520BEAT2%2520motion-capture%2520dataset.%2520Using%2520this%2520protocol%252C%2520we%2520conduct%2520large-scale%2520crowdsourced%2520evaluation%2520to%2520rank%2520six%2520recent%2520gesture-generation%2520models%2520--%2520each%2520trained%2520by%2520its%2520original%2520authors%2520--%2520across%2520two%2520key%2520evaluation%2520dimensions%253A%2520motion%2520realism%2520and%2520speech-gesture%2520alignment.%2520Our%2520results%2520provide%2520strong%2520evidence%2520that%25201%2529%2520newer%2520models%2520do%2520not%2520consistently%2520outperform%2520earlier%2520approaches%253B%25202%2529%2520published%2520claims%2520of%2520high%2520motion%2520realism%2520or%2520speech-gesture%2520alignment%2520may%2520not%2520hold%2520up%2520under%2520rigorous%2520evaluation%253B%2520and%25203%2529%2520the%2520field%2520must%2520adopt%2520disentangled%2520assessments%2520of%2520motion%2520quality%2520and%2520multimodal%2520alignment%2520for%2520accurate%2520benchmarking%2520in%2520order%2520to%2520make%2520progress.%2520Finally%252C%2520in%2520order%2520to%2520drive%2520standardisation%2520and%2520enable%2520new%2520evaluation%2520research%252C%2520we%2520will%2520release%2520five%2520hours%2520of%2520synthetic%2520motion%2520from%2520the%2520benchmarked%2520models%253B%2520over%2520750%2520rendered%2520video%2520stimuli%2520from%2520the%2520user%2520studies%2520--%2520enabling%2520new%2520evaluations%2520without%2520model%2520reimplementation%2520required%2520--%2520alongside%2520our%2520open-source%2520rendering%2520script%252C%2520and%2520the%252016%252C000%2520pairwise%2520human%2520preference%2520votes%2520collected%2520for%2520our%2520benchmark.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.01233v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Reliable%20Human%20Evaluations%20in%20Gesture%20Generation%3A%20Insights%20from%20a%20Community-Driven%20State-of-the-Art%20Benchmark&entry.906535625=Rajmund%20Nagy%20and%20Hendric%20Voss%20and%20Thanh%20Hoang-Minh%20and%20Mihail%20Tsakov%20and%20Teodor%20Nikolov%20and%20Zeyi%20Zhang%20and%20Tenglong%20Ao%20and%20Sicheng%20Yang%20and%20Shaoli%20Huang%20and%20Yongkang%20Cheng%20and%20M.%20Hamza%20Mughal%20and%20Rishabh%20Dabral%20and%20Kiran%20Chhatre%20and%20Christian%20Theobalt%20and%20Libin%20Liu%20and%20Stefan%20Kopp%20and%20Rachel%20McDonnell%20and%20Michael%20Neff%20and%20Taras%20Kucherenko%20and%20Youngwoo%20Yoon%20and%20Gustav%20Eje%20Henter&entry.1292438233=We%20review%20human%20evaluation%20practices%20in%20automated%2C%20speech-driven%203D%20gesture%20generation%20and%20find%20a%20lack%20of%20standardisation%20and%20frequent%20use%20of%20flawed%20experimental%20setups.%20This%20leads%20to%20a%20situation%20where%20it%20is%20impossible%20to%20know%20how%20different%20methods%20compare%2C%20or%20what%20the%20state%20of%20the%20art%20is.%20In%20order%20to%20address%20common%20shortcomings%20of%20evaluation%20design%2C%20and%20to%20standardise%20future%20user%20studies%20in%20gesture-generation%20works%2C%20we%20introduce%20a%20detailed%20human%20evaluation%20protocol%20for%20the%20widely-used%20BEAT2%20motion-capture%20dataset.%20Using%20this%20protocol%2C%20we%20conduct%20large-scale%20crowdsourced%20evaluation%20to%20rank%20six%20recent%20gesture-generation%20models%20--%20each%20trained%20by%20its%20original%20authors%20--%20across%20two%20key%20evaluation%20dimensions%3A%20motion%20realism%20and%20speech-gesture%20alignment.%20Our%20results%20provide%20strong%20evidence%20that%201%29%20newer%20models%20do%20not%20consistently%20outperform%20earlier%20approaches%3B%202%29%20published%20claims%20of%20high%20motion%20realism%20or%20speech-gesture%20alignment%20may%20not%20hold%20up%20under%20rigorous%20evaluation%3B%20and%203%29%20the%20field%20must%20adopt%20disentangled%20assessments%20of%20motion%20quality%20and%20multimodal%20alignment%20for%20accurate%20benchmarking%20in%20order%20to%20make%20progress.%20Finally%2C%20in%20order%20to%20drive%20standardisation%20and%20enable%20new%20evaluation%20research%2C%20we%20will%20release%20five%20hours%20of%20synthetic%20motion%20from%20the%20benchmarked%20models%3B%20over%20750%20rendered%20video%20stimuli%20from%20the%20user%20studies%20--%20enabling%20new%20evaluations%20without%20model%20reimplementation%20required%20--%20alongside%20our%20open-source%20rendering%20script%2C%20and%20the%2016%2C000%20pairwise%20human%20preference%20votes%20collected%20for%20our%20benchmark.&entry.1838667208=http%3A//arxiv.org/abs/2511.01233v2&entry.124074799=Read"},
{"title": "Explaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions", "author": "Hubert Baniecki and Maximilian Muschalik and Fabian Fumagalli and Barbara Hammer and Eyke H\u00fcllermeier and Przemyslaw Biecek", "abstract": "Language-image pre-training (LIP) enables the development of vision-language models capable of zero-shot classification, localization, multimodal retrieval, and semantic understanding. Various explanation methods have been proposed to visualize the importance of input image-text pairs on the model's similarity outputs. However, popular saliency maps are limited by capturing only first-order attributions, overlooking the complex cross-modal interactions intrinsic to such encoders. We introduce faithful interaction explanations of LIP models (FIxLIP) as a unified approach to decomposing the similarity in vision-language encoders. FIxLIP is rooted in game theory, where we analyze how using the weighted Banzhaf interaction index offers greater flexibility and improves computational efficiency over the Shapley interaction quantification framework. From a practical perspective, we propose how to naturally extend explanation evaluation metrics, such as the pointing game and area between the insertion/deletion curves, to second-order interaction explanations. Experiments on the MS COCO and ImageNet-1k benchmarks validate that second-order methods, such as FIxLIP, outperform first-order attribution methods. Beyond delivering high-quality explanations, we demonstrate the utility of FIxLIP in comparing different models, e.g. CLIP vs. SigLIP-2.", "link": "http://arxiv.org/abs/2508.05430v2", "date": "2025-11-18", "relevancy": 2.8261, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5809}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5809}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20Similarity%20in%20Vision-Language%20Encoders%20with%20Weighted%20Banzhaf%20Interactions&body=Title%3A%20Explaining%20Similarity%20in%20Vision-Language%20Encoders%20with%20Weighted%20Banzhaf%20Interactions%0AAuthor%3A%20Hubert%20Baniecki%20and%20Maximilian%20Muschalik%20and%20Fabian%20Fumagalli%20and%20Barbara%20Hammer%20and%20Eyke%20H%C3%BCllermeier%20and%20Przemyslaw%20Biecek%0AAbstract%3A%20Language-image%20pre-training%20%28LIP%29%20enables%20the%20development%20of%20vision-language%20models%20capable%20of%20zero-shot%20classification%2C%20localization%2C%20multimodal%20retrieval%2C%20and%20semantic%20understanding.%20Various%20explanation%20methods%20have%20been%20proposed%20to%20visualize%20the%20importance%20of%20input%20image-text%20pairs%20on%20the%20model%27s%20similarity%20outputs.%20However%2C%20popular%20saliency%20maps%20are%20limited%20by%20capturing%20only%20first-order%20attributions%2C%20overlooking%20the%20complex%20cross-modal%20interactions%20intrinsic%20to%20such%20encoders.%20We%20introduce%20faithful%20interaction%20explanations%20of%20LIP%20models%20%28FIxLIP%29%20as%20a%20unified%20approach%20to%20decomposing%20the%20similarity%20in%20vision-language%20encoders.%20FIxLIP%20is%20rooted%20in%20game%20theory%2C%20where%20we%20analyze%20how%20using%20the%20weighted%20Banzhaf%20interaction%20index%20offers%20greater%20flexibility%20and%20improves%20computational%20efficiency%20over%20the%20Shapley%20interaction%20quantification%20framework.%20From%20a%20practical%20perspective%2C%20we%20propose%20how%20to%20naturally%20extend%20explanation%20evaluation%20metrics%2C%20such%20as%20the%20pointing%20game%20and%20area%20between%20the%20insertion/deletion%20curves%2C%20to%20second-order%20interaction%20explanations.%20Experiments%20on%20the%20MS%20COCO%20and%20ImageNet-1k%20benchmarks%20validate%20that%20second-order%20methods%2C%20such%20as%20FIxLIP%2C%20outperform%20first-order%20attribution%20methods.%20Beyond%20delivering%20high-quality%20explanations%2C%20we%20demonstrate%20the%20utility%20of%20FIxLIP%20in%20comparing%20different%20models%2C%20e.g.%20CLIP%20vs.%20SigLIP-2.%0ALink%3A%20http%3A//arxiv.org/abs/2508.05430v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520Similarity%2520in%2520Vision-Language%2520Encoders%2520with%2520Weighted%2520Banzhaf%2520Interactions%26entry.906535625%3DHubert%2520Baniecki%2520and%2520Maximilian%2520Muschalik%2520and%2520Fabian%2520Fumagalli%2520and%2520Barbara%2520Hammer%2520and%2520Eyke%2520H%25C3%25BCllermeier%2520and%2520Przemyslaw%2520Biecek%26entry.1292438233%3DLanguage-image%2520pre-training%2520%2528LIP%2529%2520enables%2520the%2520development%2520of%2520vision-language%2520models%2520capable%2520of%2520zero-shot%2520classification%252C%2520localization%252C%2520multimodal%2520retrieval%252C%2520and%2520semantic%2520understanding.%2520Various%2520explanation%2520methods%2520have%2520been%2520proposed%2520to%2520visualize%2520the%2520importance%2520of%2520input%2520image-text%2520pairs%2520on%2520the%2520model%2527s%2520similarity%2520outputs.%2520However%252C%2520popular%2520saliency%2520maps%2520are%2520limited%2520by%2520capturing%2520only%2520first-order%2520attributions%252C%2520overlooking%2520the%2520complex%2520cross-modal%2520interactions%2520intrinsic%2520to%2520such%2520encoders.%2520We%2520introduce%2520faithful%2520interaction%2520explanations%2520of%2520LIP%2520models%2520%2528FIxLIP%2529%2520as%2520a%2520unified%2520approach%2520to%2520decomposing%2520the%2520similarity%2520in%2520vision-language%2520encoders.%2520FIxLIP%2520is%2520rooted%2520in%2520game%2520theory%252C%2520where%2520we%2520analyze%2520how%2520using%2520the%2520weighted%2520Banzhaf%2520interaction%2520index%2520offers%2520greater%2520flexibility%2520and%2520improves%2520computational%2520efficiency%2520over%2520the%2520Shapley%2520interaction%2520quantification%2520framework.%2520From%2520a%2520practical%2520perspective%252C%2520we%2520propose%2520how%2520to%2520naturally%2520extend%2520explanation%2520evaluation%2520metrics%252C%2520such%2520as%2520the%2520pointing%2520game%2520and%2520area%2520between%2520the%2520insertion/deletion%2520curves%252C%2520to%2520second-order%2520interaction%2520explanations.%2520Experiments%2520on%2520the%2520MS%2520COCO%2520and%2520ImageNet-1k%2520benchmarks%2520validate%2520that%2520second-order%2520methods%252C%2520such%2520as%2520FIxLIP%252C%2520outperform%2520first-order%2520attribution%2520methods.%2520Beyond%2520delivering%2520high-quality%2520explanations%252C%2520we%2520demonstrate%2520the%2520utility%2520of%2520FIxLIP%2520in%2520comparing%2520different%2520models%252C%2520e.g.%2520CLIP%2520vs.%2520SigLIP-2.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05430v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20Similarity%20in%20Vision-Language%20Encoders%20with%20Weighted%20Banzhaf%20Interactions&entry.906535625=Hubert%20Baniecki%20and%20Maximilian%20Muschalik%20and%20Fabian%20Fumagalli%20and%20Barbara%20Hammer%20and%20Eyke%20H%C3%BCllermeier%20and%20Przemyslaw%20Biecek&entry.1292438233=Language-image%20pre-training%20%28LIP%29%20enables%20the%20development%20of%20vision-language%20models%20capable%20of%20zero-shot%20classification%2C%20localization%2C%20multimodal%20retrieval%2C%20and%20semantic%20understanding.%20Various%20explanation%20methods%20have%20been%20proposed%20to%20visualize%20the%20importance%20of%20input%20image-text%20pairs%20on%20the%20model%27s%20similarity%20outputs.%20However%2C%20popular%20saliency%20maps%20are%20limited%20by%20capturing%20only%20first-order%20attributions%2C%20overlooking%20the%20complex%20cross-modal%20interactions%20intrinsic%20to%20such%20encoders.%20We%20introduce%20faithful%20interaction%20explanations%20of%20LIP%20models%20%28FIxLIP%29%20as%20a%20unified%20approach%20to%20decomposing%20the%20similarity%20in%20vision-language%20encoders.%20FIxLIP%20is%20rooted%20in%20game%20theory%2C%20where%20we%20analyze%20how%20using%20the%20weighted%20Banzhaf%20interaction%20index%20offers%20greater%20flexibility%20and%20improves%20computational%20efficiency%20over%20the%20Shapley%20interaction%20quantification%20framework.%20From%20a%20practical%20perspective%2C%20we%20propose%20how%20to%20naturally%20extend%20explanation%20evaluation%20metrics%2C%20such%20as%20the%20pointing%20game%20and%20area%20between%20the%20insertion/deletion%20curves%2C%20to%20second-order%20interaction%20explanations.%20Experiments%20on%20the%20MS%20COCO%20and%20ImageNet-1k%20benchmarks%20validate%20that%20second-order%20methods%2C%20such%20as%20FIxLIP%2C%20outperform%20first-order%20attribution%20methods.%20Beyond%20delivering%20high-quality%20explanations%2C%20we%20demonstrate%20the%20utility%20of%20FIxLIP%20in%20comparing%20different%20models%2C%20e.g.%20CLIP%20vs.%20SigLIP-2.&entry.1838667208=http%3A//arxiv.org/abs/2508.05430v2&entry.124074799=Read"},
{"title": "Geometry Meets Light: Leveraging Geometric Priors for Universal Photometric Stereo under Limited Multi-Illumination Cues", "author": "King-Man Tam and Satoshi Ikehata and Yuta Asano and Zhaoyi An and Rei Kawakami", "abstract": "Universal Photometric Stereo is a promising approach for recovering surface normals without strict lighting assumptions. However, it struggles when multi-illumination cues are unreliable, such as under biased lighting or in shadows or self-occluded regions of complex in-the-wild scenes. We propose GeoUniPS, a universal photometric stereo network that integrates synthetic supervision with high-level geometric priors from large-scale 3D reconstruction models pretrained on massive in-the-wild data. Our key insight is that these 3D reconstruction models serve as visual-geometry foundation models, inherently encoding rich geometric knowledge of real scenes. To leverage this, we design a Light-Geometry Dual-Branch Encoder that extracts both multi-illumination cues and geometric priors from the frozen 3D reconstruction model. We also address the limitations of the conventional orthographic projection assumption by introducing the PS-Perp dataset with realistic perspective projection to enable learning of spatially varying view directions. Extensive experiments demonstrate that GeoUniPS delivers state-of-the-arts performance across multiple datasets, both quantitatively and qualitatively, especially in the complex in-the-wild scenes.", "link": "http://arxiv.org/abs/2511.13015v2", "date": "2025-11-18", "relevancy": 2.8131, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5651}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20Meets%20Light%3A%20Leveraging%20Geometric%20Priors%20for%20Universal%20Photometric%20Stereo%20under%20Limited%20Multi-Illumination%20Cues&body=Title%3A%20Geometry%20Meets%20Light%3A%20Leveraging%20Geometric%20Priors%20for%20Universal%20Photometric%20Stereo%20under%20Limited%20Multi-Illumination%20Cues%0AAuthor%3A%20King-Man%20Tam%20and%20Satoshi%20Ikehata%20and%20Yuta%20Asano%20and%20Zhaoyi%20An%20and%20Rei%20Kawakami%0AAbstract%3A%20Universal%20Photometric%20Stereo%20is%20a%20promising%20approach%20for%20recovering%20surface%20normals%20without%20strict%20lighting%20assumptions.%20However%2C%20it%20struggles%20when%20multi-illumination%20cues%20are%20unreliable%2C%20such%20as%20under%20biased%20lighting%20or%20in%20shadows%20or%20self-occluded%20regions%20of%20complex%20in-the-wild%20scenes.%20We%20propose%20GeoUniPS%2C%20a%20universal%20photometric%20stereo%20network%20that%20integrates%20synthetic%20supervision%20with%20high-level%20geometric%20priors%20from%20large-scale%203D%20reconstruction%20models%20pretrained%20on%20massive%20in-the-wild%20data.%20Our%20key%20insight%20is%20that%20these%203D%20reconstruction%20models%20serve%20as%20visual-geometry%20foundation%20models%2C%20inherently%20encoding%20rich%20geometric%20knowledge%20of%20real%20scenes.%20To%20leverage%20this%2C%20we%20design%20a%20Light-Geometry%20Dual-Branch%20Encoder%20that%20extracts%20both%20multi-illumination%20cues%20and%20geometric%20priors%20from%20the%20frozen%203D%20reconstruction%20model.%20We%20also%20address%20the%20limitations%20of%20the%20conventional%20orthographic%20projection%20assumption%20by%20introducing%20the%20PS-Perp%20dataset%20with%20realistic%20perspective%20projection%20to%20enable%20learning%20of%20spatially%20varying%20view%20directions.%20Extensive%20experiments%20demonstrate%20that%20GeoUniPS%20delivers%20state-of-the-arts%20performance%20across%20multiple%20datasets%2C%20both%20quantitatively%20and%20qualitatively%2C%20especially%20in%20the%20complex%20in-the-wild%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13015v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520Meets%2520Light%253A%2520Leveraging%2520Geometric%2520Priors%2520for%2520Universal%2520Photometric%2520Stereo%2520under%2520Limited%2520Multi-Illumination%2520Cues%26entry.906535625%3DKing-Man%2520Tam%2520and%2520Satoshi%2520Ikehata%2520and%2520Yuta%2520Asano%2520and%2520Zhaoyi%2520An%2520and%2520Rei%2520Kawakami%26entry.1292438233%3DUniversal%2520Photometric%2520Stereo%2520is%2520a%2520promising%2520approach%2520for%2520recovering%2520surface%2520normals%2520without%2520strict%2520lighting%2520assumptions.%2520However%252C%2520it%2520struggles%2520when%2520multi-illumination%2520cues%2520are%2520unreliable%252C%2520such%2520as%2520under%2520biased%2520lighting%2520or%2520in%2520shadows%2520or%2520self-occluded%2520regions%2520of%2520complex%2520in-the-wild%2520scenes.%2520We%2520propose%2520GeoUniPS%252C%2520a%2520universal%2520photometric%2520stereo%2520network%2520that%2520integrates%2520synthetic%2520supervision%2520with%2520high-level%2520geometric%2520priors%2520from%2520large-scale%25203D%2520reconstruction%2520models%2520pretrained%2520on%2520massive%2520in-the-wild%2520data.%2520Our%2520key%2520insight%2520is%2520that%2520these%25203D%2520reconstruction%2520models%2520serve%2520as%2520visual-geometry%2520foundation%2520models%252C%2520inherently%2520encoding%2520rich%2520geometric%2520knowledge%2520of%2520real%2520scenes.%2520To%2520leverage%2520this%252C%2520we%2520design%2520a%2520Light-Geometry%2520Dual-Branch%2520Encoder%2520that%2520extracts%2520both%2520multi-illumination%2520cues%2520and%2520geometric%2520priors%2520from%2520the%2520frozen%25203D%2520reconstruction%2520model.%2520We%2520also%2520address%2520the%2520limitations%2520of%2520the%2520conventional%2520orthographic%2520projection%2520assumption%2520by%2520introducing%2520the%2520PS-Perp%2520dataset%2520with%2520realistic%2520perspective%2520projection%2520to%2520enable%2520learning%2520of%2520spatially%2520varying%2520view%2520directions.%2520Extensive%2520experiments%2520demonstrate%2520that%2520GeoUniPS%2520delivers%2520state-of-the-arts%2520performance%2520across%2520multiple%2520datasets%252C%2520both%2520quantitatively%2520and%2520qualitatively%252C%2520especially%2520in%2520the%2520complex%2520in-the-wild%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13015v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20Meets%20Light%3A%20Leveraging%20Geometric%20Priors%20for%20Universal%20Photometric%20Stereo%20under%20Limited%20Multi-Illumination%20Cues&entry.906535625=King-Man%20Tam%20and%20Satoshi%20Ikehata%20and%20Yuta%20Asano%20and%20Zhaoyi%20An%20and%20Rei%20Kawakami&entry.1292438233=Universal%20Photometric%20Stereo%20is%20a%20promising%20approach%20for%20recovering%20surface%20normals%20without%20strict%20lighting%20assumptions.%20However%2C%20it%20struggles%20when%20multi-illumination%20cues%20are%20unreliable%2C%20such%20as%20under%20biased%20lighting%20or%20in%20shadows%20or%20self-occluded%20regions%20of%20complex%20in-the-wild%20scenes.%20We%20propose%20GeoUniPS%2C%20a%20universal%20photometric%20stereo%20network%20that%20integrates%20synthetic%20supervision%20with%20high-level%20geometric%20priors%20from%20large-scale%203D%20reconstruction%20models%20pretrained%20on%20massive%20in-the-wild%20data.%20Our%20key%20insight%20is%20that%20these%203D%20reconstruction%20models%20serve%20as%20visual-geometry%20foundation%20models%2C%20inherently%20encoding%20rich%20geometric%20knowledge%20of%20real%20scenes.%20To%20leverage%20this%2C%20we%20design%20a%20Light-Geometry%20Dual-Branch%20Encoder%20that%20extracts%20both%20multi-illumination%20cues%20and%20geometric%20priors%20from%20the%20frozen%203D%20reconstruction%20model.%20We%20also%20address%20the%20limitations%20of%20the%20conventional%20orthographic%20projection%20assumption%20by%20introducing%20the%20PS-Perp%20dataset%20with%20realistic%20perspective%20projection%20to%20enable%20learning%20of%20spatially%20varying%20view%20directions.%20Extensive%20experiments%20demonstrate%20that%20GeoUniPS%20delivers%20state-of-the-arts%20performance%20across%20multiple%20datasets%2C%20both%20quantitatively%20and%20qualitatively%2C%20especially%20in%20the%20complex%20in-the-wild%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2511.13015v2&entry.124074799=Read"},
{"title": "CARScenes: Semantic VLM Dataset for Safe Autonomous Driving", "author": "Yuankai He and Weisong Shi", "abstract": "CAR-Scenes is a frame-level dataset for autonomous driving that enables training and evaluation of vision-language models (VLMs) for interpretable, scene-level understanding. We annotate 5,192 images drawn from Argoverse 1, Cityscapes, KITTI, and nuScenes using a 28-key category/sub-category knowledge base covering environment, road geometry, background-vehicle behavior, ego-vehicle behavior, vulnerable road users, sensor states, and a discrete severity scale (1-10), totaling 350+ leaf attributes. Labels are produced by a GPT-4o-assisted vision-language pipeline with human-in-the-loop verification; we release the exact prompts, post-processing rules, and per-field baseline model performance. CAR-Scenes also provides attribute co-occurrence graphs and JSONL records that support semantic retrieval, dataset triage, and risk-aware scenario mining across sources. To calibrate task difficulty, we include reproducible, non-benchmark baselines, notably a LoRA-tuned Qwen2-VL-2B with deterministic decoding, evaluated via scalar accuracy, micro-averaged F1 for list attributes, and severity MAE/RMSE on a fixed validation split. We publicly release the annotation and analysis scripts, including graph construction and evaluation scripts, to enable explainable, data-centric workflows for future intelligent vehicles. Dataset: https://github.com/Croquembouche/CAR-Scenes", "link": "http://arxiv.org/abs/2511.10701v2", "date": "2025-11-18", "relevancy": 2.8023, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5688}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5688}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CARScenes%3A%20Semantic%20VLM%20Dataset%20for%20Safe%20Autonomous%20Driving&body=Title%3A%20CARScenes%3A%20Semantic%20VLM%20Dataset%20for%20Safe%20Autonomous%20Driving%0AAuthor%3A%20Yuankai%20He%20and%20Weisong%20Shi%0AAbstract%3A%20CAR-Scenes%20is%20a%20frame-level%20dataset%20for%20autonomous%20driving%20that%20enables%20training%20and%20evaluation%20of%20vision-language%20models%20%28VLMs%29%20for%20interpretable%2C%20scene-level%20understanding.%20We%20annotate%205%2C192%20images%20drawn%20from%20Argoverse%201%2C%20Cityscapes%2C%20KITTI%2C%20and%20nuScenes%20using%20a%2028-key%20category/sub-category%20knowledge%20base%20covering%20environment%2C%20road%20geometry%2C%20background-vehicle%20behavior%2C%20ego-vehicle%20behavior%2C%20vulnerable%20road%20users%2C%20sensor%20states%2C%20and%20a%20discrete%20severity%20scale%20%281-10%29%2C%20totaling%20350%2B%20leaf%20attributes.%20Labels%20are%20produced%20by%20a%20GPT-4o-assisted%20vision-language%20pipeline%20with%20human-in-the-loop%20verification%3B%20we%20release%20the%20exact%20prompts%2C%20post-processing%20rules%2C%20and%20per-field%20baseline%20model%20performance.%20CAR-Scenes%20also%20provides%20attribute%20co-occurrence%20graphs%20and%20JSONL%20records%20that%20support%20semantic%20retrieval%2C%20dataset%20triage%2C%20and%20risk-aware%20scenario%20mining%20across%20sources.%20To%20calibrate%20task%20difficulty%2C%20we%20include%20reproducible%2C%20non-benchmark%20baselines%2C%20notably%20a%20LoRA-tuned%20Qwen2-VL-2B%20with%20deterministic%20decoding%2C%20evaluated%20via%20scalar%20accuracy%2C%20micro-averaged%20F1%20for%20list%20attributes%2C%20and%20severity%20MAE/RMSE%20on%20a%20fixed%20validation%20split.%20We%20publicly%20release%20the%20annotation%20and%20analysis%20scripts%2C%20including%20graph%20construction%20and%20evaluation%20scripts%2C%20to%20enable%20explainable%2C%20data-centric%20workflows%20for%20future%20intelligent%20vehicles.%20Dataset%3A%20https%3A//github.com/Croquembouche/CAR-Scenes%0ALink%3A%20http%3A//arxiv.org/abs/2511.10701v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCARScenes%253A%2520Semantic%2520VLM%2520Dataset%2520for%2520Safe%2520Autonomous%2520Driving%26entry.906535625%3DYuankai%2520He%2520and%2520Weisong%2520Shi%26entry.1292438233%3DCAR-Scenes%2520is%2520a%2520frame-level%2520dataset%2520for%2520autonomous%2520driving%2520that%2520enables%2520training%2520and%2520evaluation%2520of%2520vision-language%2520models%2520%2528VLMs%2529%2520for%2520interpretable%252C%2520scene-level%2520understanding.%2520We%2520annotate%25205%252C192%2520images%2520drawn%2520from%2520Argoverse%25201%252C%2520Cityscapes%252C%2520KITTI%252C%2520and%2520nuScenes%2520using%2520a%252028-key%2520category/sub-category%2520knowledge%2520base%2520covering%2520environment%252C%2520road%2520geometry%252C%2520background-vehicle%2520behavior%252C%2520ego-vehicle%2520behavior%252C%2520vulnerable%2520road%2520users%252C%2520sensor%2520states%252C%2520and%2520a%2520discrete%2520severity%2520scale%2520%25281-10%2529%252C%2520totaling%2520350%252B%2520leaf%2520attributes.%2520Labels%2520are%2520produced%2520by%2520a%2520GPT-4o-assisted%2520vision-language%2520pipeline%2520with%2520human-in-the-loop%2520verification%253B%2520we%2520release%2520the%2520exact%2520prompts%252C%2520post-processing%2520rules%252C%2520and%2520per-field%2520baseline%2520model%2520performance.%2520CAR-Scenes%2520also%2520provides%2520attribute%2520co-occurrence%2520graphs%2520and%2520JSONL%2520records%2520that%2520support%2520semantic%2520retrieval%252C%2520dataset%2520triage%252C%2520and%2520risk-aware%2520scenario%2520mining%2520across%2520sources.%2520To%2520calibrate%2520task%2520difficulty%252C%2520we%2520include%2520reproducible%252C%2520non-benchmark%2520baselines%252C%2520notably%2520a%2520LoRA-tuned%2520Qwen2-VL-2B%2520with%2520deterministic%2520decoding%252C%2520evaluated%2520via%2520scalar%2520accuracy%252C%2520micro-averaged%2520F1%2520for%2520list%2520attributes%252C%2520and%2520severity%2520MAE/RMSE%2520on%2520a%2520fixed%2520validation%2520split.%2520We%2520publicly%2520release%2520the%2520annotation%2520and%2520analysis%2520scripts%252C%2520including%2520graph%2520construction%2520and%2520evaluation%2520scripts%252C%2520to%2520enable%2520explainable%252C%2520data-centric%2520workflows%2520for%2520future%2520intelligent%2520vehicles.%2520Dataset%253A%2520https%253A//github.com/Croquembouche/CAR-Scenes%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10701v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CARScenes%3A%20Semantic%20VLM%20Dataset%20for%20Safe%20Autonomous%20Driving&entry.906535625=Yuankai%20He%20and%20Weisong%20Shi&entry.1292438233=CAR-Scenes%20is%20a%20frame-level%20dataset%20for%20autonomous%20driving%20that%20enables%20training%20and%20evaluation%20of%20vision-language%20models%20%28VLMs%29%20for%20interpretable%2C%20scene-level%20understanding.%20We%20annotate%205%2C192%20images%20drawn%20from%20Argoverse%201%2C%20Cityscapes%2C%20KITTI%2C%20and%20nuScenes%20using%20a%2028-key%20category/sub-category%20knowledge%20base%20covering%20environment%2C%20road%20geometry%2C%20background-vehicle%20behavior%2C%20ego-vehicle%20behavior%2C%20vulnerable%20road%20users%2C%20sensor%20states%2C%20and%20a%20discrete%20severity%20scale%20%281-10%29%2C%20totaling%20350%2B%20leaf%20attributes.%20Labels%20are%20produced%20by%20a%20GPT-4o-assisted%20vision-language%20pipeline%20with%20human-in-the-loop%20verification%3B%20we%20release%20the%20exact%20prompts%2C%20post-processing%20rules%2C%20and%20per-field%20baseline%20model%20performance.%20CAR-Scenes%20also%20provides%20attribute%20co-occurrence%20graphs%20and%20JSONL%20records%20that%20support%20semantic%20retrieval%2C%20dataset%20triage%2C%20and%20risk-aware%20scenario%20mining%20across%20sources.%20To%20calibrate%20task%20difficulty%2C%20we%20include%20reproducible%2C%20non-benchmark%20baselines%2C%20notably%20a%20LoRA-tuned%20Qwen2-VL-2B%20with%20deterministic%20decoding%2C%20evaluated%20via%20scalar%20accuracy%2C%20micro-averaged%20F1%20for%20list%20attributes%2C%20and%20severity%20MAE/RMSE%20on%20a%20fixed%20validation%20split.%20We%20publicly%20release%20the%20annotation%20and%20analysis%20scripts%2C%20including%20graph%20construction%20and%20evaluation%20scripts%2C%20to%20enable%20explainable%2C%20data-centric%20workflows%20for%20future%20intelligent%20vehicles.%20Dataset%3A%20https%3A//github.com/Croquembouche/CAR-Scenes&entry.1838667208=http%3A//arxiv.org/abs/2511.10701v2&entry.124074799=Read"},
{"title": "ArchMap: Arch-Flattening and Knowledge-Guided Vision Language Model for Tooth Counting and Structured Dental Understanding", "author": "Bohan Zhang and Yiyi Miao and Taoyu Wu and Tong Chen and Ji Jiang and Zhuoxiao Li and Zhe Tang and Limin Yu and Jionglong Su", "abstract": "A structured understanding of intraoral 3D scans is essential for digital orthodontics. However, existing deep-learning approaches rely heavily on modality-specific training, large annotated datasets, and controlled scanning conditions, which limit generalization across devices and hinder deployment in real clinical workflows. Moreover, raw intraoral meshes exhibit substantial variation in arch pose, incomplete geometry caused by occlusion or tooth contact, and a lack of texture cues, making unified semantic interpretation highly challenging. To address these limitations, we propose ArchMap, a training-free and knowledge-guided framework for robust structured dental understanding. ArchMap first introduces a geometry-aware arch-flattening module that standardizes raw 3D meshes into spatially aligned, continuity-preserving multi-view projections. We then construct a Dental Knowledge Base (DKB) encoding hierarchical tooth ontology, dentition-stage policies, and clinical semantics to constrain the symbolic reasoning space. We validate ArchMap on 1060 pre-/post-orthodontic cases, demonstrating robust performance in tooth counting, anatomical partitioning, dentition-stage classification, and the identification of clinical conditions such as crowding, missing teeth, prosthetics, and caries. Compared with supervised pipelines and prompted VLM baselines, ArchMap achieves higher accuracy, reduced semantic drift, and superior stability under sparse or artifact-prone conditions. As a fully training-free system, ArchMap demonstrates that combining geometric normalization with ontology-guided multimodal reasoning offers a practical and scalable solution for the structured analysis of 3D intraoral scans in modern digital orthodontics.", "link": "http://arxiv.org/abs/2511.14336v1", "date": "2025-11-18", "relevancy": 2.7803, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArchMap%3A%20Arch-Flattening%20and%20Knowledge-Guided%20Vision%20Language%20Model%20for%20Tooth%20Counting%20and%20Structured%20Dental%20Understanding&body=Title%3A%20ArchMap%3A%20Arch-Flattening%20and%20Knowledge-Guided%20Vision%20Language%20Model%20for%20Tooth%20Counting%20and%20Structured%20Dental%20Understanding%0AAuthor%3A%20Bohan%20Zhang%20and%20Yiyi%20Miao%20and%20Taoyu%20Wu%20and%20Tong%20Chen%20and%20Ji%20Jiang%20and%20Zhuoxiao%20Li%20and%20Zhe%20Tang%20and%20Limin%20Yu%20and%20Jionglong%20Su%0AAbstract%3A%20A%20structured%20understanding%20of%20intraoral%203D%20scans%20is%20essential%20for%20digital%20orthodontics.%20However%2C%20existing%20deep-learning%20approaches%20rely%20heavily%20on%20modality-specific%20training%2C%20large%20annotated%20datasets%2C%20and%20controlled%20scanning%20conditions%2C%20which%20limit%20generalization%20across%20devices%20and%20hinder%20deployment%20in%20real%20clinical%20workflows.%20Moreover%2C%20raw%20intraoral%20meshes%20exhibit%20substantial%20variation%20in%20arch%20pose%2C%20incomplete%20geometry%20caused%20by%20occlusion%20or%20tooth%20contact%2C%20and%20a%20lack%20of%20texture%20cues%2C%20making%20unified%20semantic%20interpretation%20highly%20challenging.%20To%20address%20these%20limitations%2C%20we%20propose%20ArchMap%2C%20a%20training-free%20and%20knowledge-guided%20framework%20for%20robust%20structured%20dental%20understanding.%20ArchMap%20first%20introduces%20a%20geometry-aware%20arch-flattening%20module%20that%20standardizes%20raw%203D%20meshes%20into%20spatially%20aligned%2C%20continuity-preserving%20multi-view%20projections.%20We%20then%20construct%20a%20Dental%20Knowledge%20Base%20%28DKB%29%20encoding%20hierarchical%20tooth%20ontology%2C%20dentition-stage%20policies%2C%20and%20clinical%20semantics%20to%20constrain%20the%20symbolic%20reasoning%20space.%20We%20validate%20ArchMap%20on%201060%20pre-/post-orthodontic%20cases%2C%20demonstrating%20robust%20performance%20in%20tooth%20counting%2C%20anatomical%20partitioning%2C%20dentition-stage%20classification%2C%20and%20the%20identification%20of%20clinical%20conditions%20such%20as%20crowding%2C%20missing%20teeth%2C%20prosthetics%2C%20and%20caries.%20Compared%20with%20supervised%20pipelines%20and%20prompted%20VLM%20baselines%2C%20ArchMap%20achieves%20higher%20accuracy%2C%20reduced%20semantic%20drift%2C%20and%20superior%20stability%20under%20sparse%20or%20artifact-prone%20conditions.%20As%20a%20fully%20training-free%20system%2C%20ArchMap%20demonstrates%20that%20combining%20geometric%20normalization%20with%20ontology-guided%20multimodal%20reasoning%20offers%20a%20practical%20and%20scalable%20solution%20for%20the%20structured%20analysis%20of%203D%20intraoral%20scans%20in%20modern%20digital%20orthodontics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArchMap%253A%2520Arch-Flattening%2520and%2520Knowledge-Guided%2520Vision%2520Language%2520Model%2520for%2520Tooth%2520Counting%2520and%2520Structured%2520Dental%2520Understanding%26entry.906535625%3DBohan%2520Zhang%2520and%2520Yiyi%2520Miao%2520and%2520Taoyu%2520Wu%2520and%2520Tong%2520Chen%2520and%2520Ji%2520Jiang%2520and%2520Zhuoxiao%2520Li%2520and%2520Zhe%2520Tang%2520and%2520Limin%2520Yu%2520and%2520Jionglong%2520Su%26entry.1292438233%3DA%2520structured%2520understanding%2520of%2520intraoral%25203D%2520scans%2520is%2520essential%2520for%2520digital%2520orthodontics.%2520However%252C%2520existing%2520deep-learning%2520approaches%2520rely%2520heavily%2520on%2520modality-specific%2520training%252C%2520large%2520annotated%2520datasets%252C%2520and%2520controlled%2520scanning%2520conditions%252C%2520which%2520limit%2520generalization%2520across%2520devices%2520and%2520hinder%2520deployment%2520in%2520real%2520clinical%2520workflows.%2520Moreover%252C%2520raw%2520intraoral%2520meshes%2520exhibit%2520substantial%2520variation%2520in%2520arch%2520pose%252C%2520incomplete%2520geometry%2520caused%2520by%2520occlusion%2520or%2520tooth%2520contact%252C%2520and%2520a%2520lack%2520of%2520texture%2520cues%252C%2520making%2520unified%2520semantic%2520interpretation%2520highly%2520challenging.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520ArchMap%252C%2520a%2520training-free%2520and%2520knowledge-guided%2520framework%2520for%2520robust%2520structured%2520dental%2520understanding.%2520ArchMap%2520first%2520introduces%2520a%2520geometry-aware%2520arch-flattening%2520module%2520that%2520standardizes%2520raw%25203D%2520meshes%2520into%2520spatially%2520aligned%252C%2520continuity-preserving%2520multi-view%2520projections.%2520We%2520then%2520construct%2520a%2520Dental%2520Knowledge%2520Base%2520%2528DKB%2529%2520encoding%2520hierarchical%2520tooth%2520ontology%252C%2520dentition-stage%2520policies%252C%2520and%2520clinical%2520semantics%2520to%2520constrain%2520the%2520symbolic%2520reasoning%2520space.%2520We%2520validate%2520ArchMap%2520on%25201060%2520pre-/post-orthodontic%2520cases%252C%2520demonstrating%2520robust%2520performance%2520in%2520tooth%2520counting%252C%2520anatomical%2520partitioning%252C%2520dentition-stage%2520classification%252C%2520and%2520the%2520identification%2520of%2520clinical%2520conditions%2520such%2520as%2520crowding%252C%2520missing%2520teeth%252C%2520prosthetics%252C%2520and%2520caries.%2520Compared%2520with%2520supervised%2520pipelines%2520and%2520prompted%2520VLM%2520baselines%252C%2520ArchMap%2520achieves%2520higher%2520accuracy%252C%2520reduced%2520semantic%2520drift%252C%2520and%2520superior%2520stability%2520under%2520sparse%2520or%2520artifact-prone%2520conditions.%2520As%2520a%2520fully%2520training-free%2520system%252C%2520ArchMap%2520demonstrates%2520that%2520combining%2520geometric%2520normalization%2520with%2520ontology-guided%2520multimodal%2520reasoning%2520offers%2520a%2520practical%2520and%2520scalable%2520solution%2520for%2520the%2520structured%2520analysis%2520of%25203D%2520intraoral%2520scans%2520in%2520modern%2520digital%2520orthodontics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArchMap%3A%20Arch-Flattening%20and%20Knowledge-Guided%20Vision%20Language%20Model%20for%20Tooth%20Counting%20and%20Structured%20Dental%20Understanding&entry.906535625=Bohan%20Zhang%20and%20Yiyi%20Miao%20and%20Taoyu%20Wu%20and%20Tong%20Chen%20and%20Ji%20Jiang%20and%20Zhuoxiao%20Li%20and%20Zhe%20Tang%20and%20Limin%20Yu%20and%20Jionglong%20Su&entry.1292438233=A%20structured%20understanding%20of%20intraoral%203D%20scans%20is%20essential%20for%20digital%20orthodontics.%20However%2C%20existing%20deep-learning%20approaches%20rely%20heavily%20on%20modality-specific%20training%2C%20large%20annotated%20datasets%2C%20and%20controlled%20scanning%20conditions%2C%20which%20limit%20generalization%20across%20devices%20and%20hinder%20deployment%20in%20real%20clinical%20workflows.%20Moreover%2C%20raw%20intraoral%20meshes%20exhibit%20substantial%20variation%20in%20arch%20pose%2C%20incomplete%20geometry%20caused%20by%20occlusion%20or%20tooth%20contact%2C%20and%20a%20lack%20of%20texture%20cues%2C%20making%20unified%20semantic%20interpretation%20highly%20challenging.%20To%20address%20these%20limitations%2C%20we%20propose%20ArchMap%2C%20a%20training-free%20and%20knowledge-guided%20framework%20for%20robust%20structured%20dental%20understanding.%20ArchMap%20first%20introduces%20a%20geometry-aware%20arch-flattening%20module%20that%20standardizes%20raw%203D%20meshes%20into%20spatially%20aligned%2C%20continuity-preserving%20multi-view%20projections.%20We%20then%20construct%20a%20Dental%20Knowledge%20Base%20%28DKB%29%20encoding%20hierarchical%20tooth%20ontology%2C%20dentition-stage%20policies%2C%20and%20clinical%20semantics%20to%20constrain%20the%20symbolic%20reasoning%20space.%20We%20validate%20ArchMap%20on%201060%20pre-/post-orthodontic%20cases%2C%20demonstrating%20robust%20performance%20in%20tooth%20counting%2C%20anatomical%20partitioning%2C%20dentition-stage%20classification%2C%20and%20the%20identification%20of%20clinical%20conditions%20such%20as%20crowding%2C%20missing%20teeth%2C%20prosthetics%2C%20and%20caries.%20Compared%20with%20supervised%20pipelines%20and%20prompted%20VLM%20baselines%2C%20ArchMap%20achieves%20higher%20accuracy%2C%20reduced%20semantic%20drift%2C%20and%20superior%20stability%20under%20sparse%20or%20artifact-prone%20conditions.%20As%20a%20fully%20training-free%20system%2C%20ArchMap%20demonstrates%20that%20combining%20geometric%20normalization%20with%20ontology-guided%20multimodal%20reasoning%20offers%20a%20practical%20and%20scalable%20solution%20for%20the%20structured%20analysis%20of%203D%20intraoral%20scans%20in%20modern%20digital%20orthodontics.&entry.1838667208=http%3A//arxiv.org/abs/2511.14336v1&entry.124074799=Read"},
{"title": "Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning", "author": "Hongkuan Zhou and Lavdim Halilaj and Sebastian Monka and Stefan Schmid and Yuqicheng Zhu and Jingcheng Wu and Nadeem Nazer and Steffen Staab", "abstract": "Open-domain visual entity recognition aims to identify and link entities depicted in images to a vast and evolving set of real-world concepts, such as those found in Wikidata. Unlike conventional classification tasks with fixed label sets, it operates under open-set conditions, where most target entities are unseen during training and exhibit long-tail distributions. This makes the task inherently challenging due to limited supervision, high visual ambiguity, and the need for semantic disambiguation. We propose a Knowledge-guided Contrastive Learning (KnowCoL) framework that combines both images and text descriptions into a shared semantic space grounded by structured information from Wikidata. By abstracting visual and textual inputs to a conceptual level, the model leverages entity descriptions, type hierarchies, and relational context to support zero-shot entity recognition. We evaluate our approach on the OVEN benchmark, a large-scale open-domain visual recognition dataset with Wikidata IDs as the label space. Our experiments show that using visual, textual, and structured knowledge greatly improves accuracy, especially for rare and unseen entities. Our smallest model improves the accuracy on unseen entities by 10.5% compared to the state-of-the-art, despite being 35 times smaller.", "link": "http://arxiv.org/abs/2510.13675v2", "date": "2025-11-18", "relevancy": 2.7669, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5552}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20and%20Knowing%20in%20the%20Wild%3A%20Open-domain%20Visual%20Entity%20Recognition%20with%20Large-scale%20Knowledge%20Graphs%20via%20Contrastive%20Learning&body=Title%3A%20Seeing%20and%20Knowing%20in%20the%20Wild%3A%20Open-domain%20Visual%20Entity%20Recognition%20with%20Large-scale%20Knowledge%20Graphs%20via%20Contrastive%20Learning%0AAuthor%3A%20Hongkuan%20Zhou%20and%20Lavdim%20Halilaj%20and%20Sebastian%20Monka%20and%20Stefan%20Schmid%20and%20Yuqicheng%20Zhu%20and%20Jingcheng%20Wu%20and%20Nadeem%20Nazer%20and%20Steffen%20Staab%0AAbstract%3A%20Open-domain%20visual%20entity%20recognition%20aims%20to%20identify%20and%20link%20entities%20depicted%20in%20images%20to%20a%20vast%20and%20evolving%20set%20of%20real-world%20concepts%2C%20such%20as%20those%20found%20in%20Wikidata.%20Unlike%20conventional%20classification%20tasks%20with%20fixed%20label%20sets%2C%20it%20operates%20under%20open-set%20conditions%2C%20where%20most%20target%20entities%20are%20unseen%20during%20training%20and%20exhibit%20long-tail%20distributions.%20This%20makes%20the%20task%20inherently%20challenging%20due%20to%20limited%20supervision%2C%20high%20visual%20ambiguity%2C%20and%20the%20need%20for%20semantic%20disambiguation.%20We%20propose%20a%20Knowledge-guided%20Contrastive%20Learning%20%28KnowCoL%29%20framework%20that%20combines%20both%20images%20and%20text%20descriptions%20into%20a%20shared%20semantic%20space%20grounded%20by%20structured%20information%20from%20Wikidata.%20By%20abstracting%20visual%20and%20textual%20inputs%20to%20a%20conceptual%20level%2C%20the%20model%20leverages%20entity%20descriptions%2C%20type%20hierarchies%2C%20and%20relational%20context%20to%20support%20zero-shot%20entity%20recognition.%20We%20evaluate%20our%20approach%20on%20the%20OVEN%20benchmark%2C%20a%20large-scale%20open-domain%20visual%20recognition%20dataset%20with%20Wikidata%20IDs%20as%20the%20label%20space.%20Our%20experiments%20show%20that%20using%20visual%2C%20textual%2C%20and%20structured%20knowledge%20greatly%20improves%20accuracy%2C%20especially%20for%20rare%20and%20unseen%20entities.%20Our%20smallest%20model%20improves%20the%20accuracy%20on%20unseen%20entities%20by%2010.5%25%20compared%20to%20the%20state-of-the-art%2C%20despite%20being%2035%20times%20smaller.%0ALink%3A%20http%3A//arxiv.org/abs/2510.13675v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520and%2520Knowing%2520in%2520the%2520Wild%253A%2520Open-domain%2520Visual%2520Entity%2520Recognition%2520with%2520Large-scale%2520Knowledge%2520Graphs%2520via%2520Contrastive%2520Learning%26entry.906535625%3DHongkuan%2520Zhou%2520and%2520Lavdim%2520Halilaj%2520and%2520Sebastian%2520Monka%2520and%2520Stefan%2520Schmid%2520and%2520Yuqicheng%2520Zhu%2520and%2520Jingcheng%2520Wu%2520and%2520Nadeem%2520Nazer%2520and%2520Steffen%2520Staab%26entry.1292438233%3DOpen-domain%2520visual%2520entity%2520recognition%2520aims%2520to%2520identify%2520and%2520link%2520entities%2520depicted%2520in%2520images%2520to%2520a%2520vast%2520and%2520evolving%2520set%2520of%2520real-world%2520concepts%252C%2520such%2520as%2520those%2520found%2520in%2520Wikidata.%2520Unlike%2520conventional%2520classification%2520tasks%2520with%2520fixed%2520label%2520sets%252C%2520it%2520operates%2520under%2520open-set%2520conditions%252C%2520where%2520most%2520target%2520entities%2520are%2520unseen%2520during%2520training%2520and%2520exhibit%2520long-tail%2520distributions.%2520This%2520makes%2520the%2520task%2520inherently%2520challenging%2520due%2520to%2520limited%2520supervision%252C%2520high%2520visual%2520ambiguity%252C%2520and%2520the%2520need%2520for%2520semantic%2520disambiguation.%2520We%2520propose%2520a%2520Knowledge-guided%2520Contrastive%2520Learning%2520%2528KnowCoL%2529%2520framework%2520that%2520combines%2520both%2520images%2520and%2520text%2520descriptions%2520into%2520a%2520shared%2520semantic%2520space%2520grounded%2520by%2520structured%2520information%2520from%2520Wikidata.%2520By%2520abstracting%2520visual%2520and%2520textual%2520inputs%2520to%2520a%2520conceptual%2520level%252C%2520the%2520model%2520leverages%2520entity%2520descriptions%252C%2520type%2520hierarchies%252C%2520and%2520relational%2520context%2520to%2520support%2520zero-shot%2520entity%2520recognition.%2520We%2520evaluate%2520our%2520approach%2520on%2520the%2520OVEN%2520benchmark%252C%2520a%2520large-scale%2520open-domain%2520visual%2520recognition%2520dataset%2520with%2520Wikidata%2520IDs%2520as%2520the%2520label%2520space.%2520Our%2520experiments%2520show%2520that%2520using%2520visual%252C%2520textual%252C%2520and%2520structured%2520knowledge%2520greatly%2520improves%2520accuracy%252C%2520especially%2520for%2520rare%2520and%2520unseen%2520entities.%2520Our%2520smallest%2520model%2520improves%2520the%2520accuracy%2520on%2520unseen%2520entities%2520by%252010.5%2525%2520compared%2520to%2520the%2520state-of-the-art%252C%2520despite%2520being%252035%2520times%2520smaller.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13675v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20and%20Knowing%20in%20the%20Wild%3A%20Open-domain%20Visual%20Entity%20Recognition%20with%20Large-scale%20Knowledge%20Graphs%20via%20Contrastive%20Learning&entry.906535625=Hongkuan%20Zhou%20and%20Lavdim%20Halilaj%20and%20Sebastian%20Monka%20and%20Stefan%20Schmid%20and%20Yuqicheng%20Zhu%20and%20Jingcheng%20Wu%20and%20Nadeem%20Nazer%20and%20Steffen%20Staab&entry.1292438233=Open-domain%20visual%20entity%20recognition%20aims%20to%20identify%20and%20link%20entities%20depicted%20in%20images%20to%20a%20vast%20and%20evolving%20set%20of%20real-world%20concepts%2C%20such%20as%20those%20found%20in%20Wikidata.%20Unlike%20conventional%20classification%20tasks%20with%20fixed%20label%20sets%2C%20it%20operates%20under%20open-set%20conditions%2C%20where%20most%20target%20entities%20are%20unseen%20during%20training%20and%20exhibit%20long-tail%20distributions.%20This%20makes%20the%20task%20inherently%20challenging%20due%20to%20limited%20supervision%2C%20high%20visual%20ambiguity%2C%20and%20the%20need%20for%20semantic%20disambiguation.%20We%20propose%20a%20Knowledge-guided%20Contrastive%20Learning%20%28KnowCoL%29%20framework%20that%20combines%20both%20images%20and%20text%20descriptions%20into%20a%20shared%20semantic%20space%20grounded%20by%20structured%20information%20from%20Wikidata.%20By%20abstracting%20visual%20and%20textual%20inputs%20to%20a%20conceptual%20level%2C%20the%20model%20leverages%20entity%20descriptions%2C%20type%20hierarchies%2C%20and%20relational%20context%20to%20support%20zero-shot%20entity%20recognition.%20We%20evaluate%20our%20approach%20on%20the%20OVEN%20benchmark%2C%20a%20large-scale%20open-domain%20visual%20recognition%20dataset%20with%20Wikidata%20IDs%20as%20the%20label%20space.%20Our%20experiments%20show%20that%20using%20visual%2C%20textual%2C%20and%20structured%20knowledge%20greatly%20improves%20accuracy%2C%20especially%20for%20rare%20and%20unseen%20entities.%20Our%20smallest%20model%20improves%20the%20accuracy%20on%20unseen%20entities%20by%2010.5%25%20compared%20to%20the%20state-of-the-art%2C%20despite%20being%2035%20times%20smaller.&entry.1838667208=http%3A//arxiv.org/abs/2510.13675v2&entry.124074799=Read"},
{"title": "SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology", "author": "Marco Acerbis and Swarnadip Chatterjee and Christophe Avenel and Joakim Lindblad", "abstract": "Computational cytology faces two major challenges: i) instance-level labels are unreliable and prohibitively costly to obtain, ii) witness rates are extremely low. We propose SLAM-AGS, a Slide-Label-Aware Multitask pretraining framework that jointly optimizes (i) a weakly supervised similarity objective on slide-negative patches and (ii) a self-supervised contrastive objective on slide-positive patches, yielding stronger performance on downstream tasks. To stabilize learning, we apply Adaptive Gradient Surgery to tackle conflicting task gradients and prevent model collapse. We integrate the pretrained encoder into an attention-based Multiple Instance Learning aggregator for bag-level prediction and attention-guided retrieval of the most abnormal instances in a bag. On a publicly available bone-marrow cytology dataset, with simulated witness rates from 10% down to 0.5%, SLAM-AGS improves bag-level F1-Score and Top 400 positive cell retrieval over other pretraining methods, with the largest gains at low witness rates, showing that resolving gradient interference enables stable pretraining and better performance on downstream tasks. To facilitate reproducibility, we share our complete implementation and evaluation framework as open source: https://github.com/Ace95/SLAM-AGS.", "link": "http://arxiv.org/abs/2511.14639v1", "date": "2025-11-18", "relevancy": 2.7659, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5973}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5398}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLAM-AGS%3A%20Slide-Label%20Aware%20Multi-Task%20Pretraining%20Using%20Adaptive%20Gradient%20Surgery%20in%20Computational%20Cytology&body=Title%3A%20SLAM-AGS%3A%20Slide-Label%20Aware%20Multi-Task%20Pretraining%20Using%20Adaptive%20Gradient%20Surgery%20in%20Computational%20Cytology%0AAuthor%3A%20Marco%20Acerbis%20and%20Swarnadip%20Chatterjee%20and%20Christophe%20Avenel%20and%20Joakim%20Lindblad%0AAbstract%3A%20Computational%20cytology%20faces%20two%20major%20challenges%3A%20i%29%20instance-level%20labels%20are%20unreliable%20and%20prohibitively%20costly%20to%20obtain%2C%20ii%29%20witness%20rates%20are%20extremely%20low.%20We%20propose%20SLAM-AGS%2C%20a%20Slide-Label-Aware%20Multitask%20pretraining%20framework%20that%20jointly%20optimizes%20%28i%29%20a%20weakly%20supervised%20similarity%20objective%20on%20slide-negative%20patches%20and%20%28ii%29%20a%20self-supervised%20contrastive%20objective%20on%20slide-positive%20patches%2C%20yielding%20stronger%20performance%20on%20downstream%20tasks.%20To%20stabilize%20learning%2C%20we%20apply%20Adaptive%20Gradient%20Surgery%20to%20tackle%20conflicting%20task%20gradients%20and%20prevent%20model%20collapse.%20We%20integrate%20the%20pretrained%20encoder%20into%20an%20attention-based%20Multiple%20Instance%20Learning%20aggregator%20for%20bag-level%20prediction%20and%20attention-guided%20retrieval%20of%20the%20most%20abnormal%20instances%20in%20a%20bag.%20On%20a%20publicly%20available%20bone-marrow%20cytology%20dataset%2C%20with%20simulated%20witness%20rates%20from%2010%25%20down%20to%200.5%25%2C%20SLAM-AGS%20improves%20bag-level%20F1-Score%20and%20Top%20400%20positive%20cell%20retrieval%20over%20other%20pretraining%20methods%2C%20with%20the%20largest%20gains%20at%20low%20witness%20rates%2C%20showing%20that%20resolving%20gradient%20interference%20enables%20stable%20pretraining%20and%20better%20performance%20on%20downstream%20tasks.%20To%20facilitate%20reproducibility%2C%20we%20share%20our%20complete%20implementation%20and%20evaluation%20framework%20as%20open%20source%3A%20https%3A//github.com/Ace95/SLAM-AGS.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14639v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLAM-AGS%253A%2520Slide-Label%2520Aware%2520Multi-Task%2520Pretraining%2520Using%2520Adaptive%2520Gradient%2520Surgery%2520in%2520Computational%2520Cytology%26entry.906535625%3DMarco%2520Acerbis%2520and%2520Swarnadip%2520Chatterjee%2520and%2520Christophe%2520Avenel%2520and%2520Joakim%2520Lindblad%26entry.1292438233%3DComputational%2520cytology%2520faces%2520two%2520major%2520challenges%253A%2520i%2529%2520instance-level%2520labels%2520are%2520unreliable%2520and%2520prohibitively%2520costly%2520to%2520obtain%252C%2520ii%2529%2520witness%2520rates%2520are%2520extremely%2520low.%2520We%2520propose%2520SLAM-AGS%252C%2520a%2520Slide-Label-Aware%2520Multitask%2520pretraining%2520framework%2520that%2520jointly%2520optimizes%2520%2528i%2529%2520a%2520weakly%2520supervised%2520similarity%2520objective%2520on%2520slide-negative%2520patches%2520and%2520%2528ii%2529%2520a%2520self-supervised%2520contrastive%2520objective%2520on%2520slide-positive%2520patches%252C%2520yielding%2520stronger%2520performance%2520on%2520downstream%2520tasks.%2520To%2520stabilize%2520learning%252C%2520we%2520apply%2520Adaptive%2520Gradient%2520Surgery%2520to%2520tackle%2520conflicting%2520task%2520gradients%2520and%2520prevent%2520model%2520collapse.%2520We%2520integrate%2520the%2520pretrained%2520encoder%2520into%2520an%2520attention-based%2520Multiple%2520Instance%2520Learning%2520aggregator%2520for%2520bag-level%2520prediction%2520and%2520attention-guided%2520retrieval%2520of%2520the%2520most%2520abnormal%2520instances%2520in%2520a%2520bag.%2520On%2520a%2520publicly%2520available%2520bone-marrow%2520cytology%2520dataset%252C%2520with%2520simulated%2520witness%2520rates%2520from%252010%2525%2520down%2520to%25200.5%2525%252C%2520SLAM-AGS%2520improves%2520bag-level%2520F1-Score%2520and%2520Top%2520400%2520positive%2520cell%2520retrieval%2520over%2520other%2520pretraining%2520methods%252C%2520with%2520the%2520largest%2520gains%2520at%2520low%2520witness%2520rates%252C%2520showing%2520that%2520resolving%2520gradient%2520interference%2520enables%2520stable%2520pretraining%2520and%2520better%2520performance%2520on%2520downstream%2520tasks.%2520To%2520facilitate%2520reproducibility%252C%2520we%2520share%2520our%2520complete%2520implementation%2520and%2520evaluation%2520framework%2520as%2520open%2520source%253A%2520https%253A//github.com/Ace95/SLAM-AGS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14639v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLAM-AGS%3A%20Slide-Label%20Aware%20Multi-Task%20Pretraining%20Using%20Adaptive%20Gradient%20Surgery%20in%20Computational%20Cytology&entry.906535625=Marco%20Acerbis%20and%20Swarnadip%20Chatterjee%20and%20Christophe%20Avenel%20and%20Joakim%20Lindblad&entry.1292438233=Computational%20cytology%20faces%20two%20major%20challenges%3A%20i%29%20instance-level%20labels%20are%20unreliable%20and%20prohibitively%20costly%20to%20obtain%2C%20ii%29%20witness%20rates%20are%20extremely%20low.%20We%20propose%20SLAM-AGS%2C%20a%20Slide-Label-Aware%20Multitask%20pretraining%20framework%20that%20jointly%20optimizes%20%28i%29%20a%20weakly%20supervised%20similarity%20objective%20on%20slide-negative%20patches%20and%20%28ii%29%20a%20self-supervised%20contrastive%20objective%20on%20slide-positive%20patches%2C%20yielding%20stronger%20performance%20on%20downstream%20tasks.%20To%20stabilize%20learning%2C%20we%20apply%20Adaptive%20Gradient%20Surgery%20to%20tackle%20conflicting%20task%20gradients%20and%20prevent%20model%20collapse.%20We%20integrate%20the%20pretrained%20encoder%20into%20an%20attention-based%20Multiple%20Instance%20Learning%20aggregator%20for%20bag-level%20prediction%20and%20attention-guided%20retrieval%20of%20the%20most%20abnormal%20instances%20in%20a%20bag.%20On%20a%20publicly%20available%20bone-marrow%20cytology%20dataset%2C%20with%20simulated%20witness%20rates%20from%2010%25%20down%20to%200.5%25%2C%20SLAM-AGS%20improves%20bag-level%20F1-Score%20and%20Top%20400%20positive%20cell%20retrieval%20over%20other%20pretraining%20methods%2C%20with%20the%20largest%20gains%20at%20low%20witness%20rates%2C%20showing%20that%20resolving%20gradient%20interference%20enables%20stable%20pretraining%20and%20better%20performance%20on%20downstream%20tasks.%20To%20facilitate%20reproducibility%2C%20we%20share%20our%20complete%20implementation%20and%20evaluation%20framework%20as%20open%20source%3A%20https%3A//github.com/Ace95/SLAM-AGS.&entry.1838667208=http%3A//arxiv.org/abs/2511.14639v1&entry.124074799=Read"},
{"title": "GMAT: Grounded Multi-Agent Clinical Description Generation for Text Encoder in Vision-Language MIL for Whole Slide Image Classification", "author": "Ngoc Bui Lam Quang and Nam Le Nguyen Binh and Thanh-Huy Nguyen and Le Thien Phuc Nguyen and Quan Nguyen and Ulas Bagci", "abstract": "Multiple Instance Learning (MIL) is the leading approach for whole slide image (WSI) classification, enabling efficient analysis of gigapixel pathology slides. Recent work has introduced vision-language models (VLMs) into MIL pipelines to incorporate medical knowledge through text-based class descriptions rather than simple class names. However, when these methods rely on large language models (LLMs) to generate clinical descriptions or use fixed-length prompts to represent complex pathology concepts, the limited token capacity of VLMs often constrains the expressiveness and richness of the encoded class information. Additionally, descriptions generated solely by LLMs may lack domain grounding and fine-grained medical specificity, leading to suboptimal alignment with visual features. To address these challenges, we propose a vision-language MIL framework with two key contributions: (1) A grounded multi-agent description generation system that leverages curated pathology textbooks and agent specialization (e.g., morphology, spatial context) to produce accurate and diverse clinical descriptions; (2) A text encoding strategy using a list of descriptions rather than a single prompt, capturing fine-grained and complementary clinical signals for better alignment with visual features. Integrated into a VLM-MIL pipeline, our approach shows improved performance over single-prompt class baselines and achieves results comparable to state-of-the-art models, as demonstrated on renal and lung cancer datasets.", "link": "http://arxiv.org/abs/2508.01293v2", "date": "2025-11-18", "relevancy": 2.7648, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GMAT%3A%20Grounded%20Multi-Agent%20Clinical%20Description%20Generation%20for%20Text%20Encoder%20in%20Vision-Language%20MIL%20for%20Whole%20Slide%20Image%20Classification&body=Title%3A%20GMAT%3A%20Grounded%20Multi-Agent%20Clinical%20Description%20Generation%20for%20Text%20Encoder%20in%20Vision-Language%20MIL%20for%20Whole%20Slide%20Image%20Classification%0AAuthor%3A%20Ngoc%20Bui%20Lam%20Quang%20and%20Nam%20Le%20Nguyen%20Binh%20and%20Thanh-Huy%20Nguyen%20and%20Le%20Thien%20Phuc%20Nguyen%20and%20Quan%20Nguyen%20and%20Ulas%20Bagci%0AAbstract%3A%20Multiple%20Instance%20Learning%20%28MIL%29%20is%20the%20leading%20approach%20for%20whole%20slide%20image%20%28WSI%29%20classification%2C%20enabling%20efficient%20analysis%20of%20gigapixel%20pathology%20slides.%20Recent%20work%20has%20introduced%20vision-language%20models%20%28VLMs%29%20into%20MIL%20pipelines%20to%20incorporate%20medical%20knowledge%20through%20text-based%20class%20descriptions%20rather%20than%20simple%20class%20names.%20However%2C%20when%20these%20methods%20rely%20on%20large%20language%20models%20%28LLMs%29%20to%20generate%20clinical%20descriptions%20or%20use%20fixed-length%20prompts%20to%20represent%20complex%20pathology%20concepts%2C%20the%20limited%20token%20capacity%20of%20VLMs%20often%20constrains%20the%20expressiveness%20and%20richness%20of%20the%20encoded%20class%20information.%20Additionally%2C%20descriptions%20generated%20solely%20by%20LLMs%20may%20lack%20domain%20grounding%20and%20fine-grained%20medical%20specificity%2C%20leading%20to%20suboptimal%20alignment%20with%20visual%20features.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20vision-language%20MIL%20framework%20with%20two%20key%20contributions%3A%20%281%29%20A%20grounded%20multi-agent%20description%20generation%20system%20that%20leverages%20curated%20pathology%20textbooks%20and%20agent%20specialization%20%28e.g.%2C%20morphology%2C%20spatial%20context%29%20to%20produce%20accurate%20and%20diverse%20clinical%20descriptions%3B%20%282%29%20A%20text%20encoding%20strategy%20using%20a%20list%20of%20descriptions%20rather%20than%20a%20single%20prompt%2C%20capturing%20fine-grained%20and%20complementary%20clinical%20signals%20for%20better%20alignment%20with%20visual%20features.%20Integrated%20into%20a%20VLM-MIL%20pipeline%2C%20our%20approach%20shows%20improved%20performance%20over%20single-prompt%20class%20baselines%20and%20achieves%20results%20comparable%20to%20state-of-the-art%20models%2C%20as%20demonstrated%20on%20renal%20and%20lung%20cancer%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2508.01293v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGMAT%253A%2520Grounded%2520Multi-Agent%2520Clinical%2520Description%2520Generation%2520for%2520Text%2520Encoder%2520in%2520Vision-Language%2520MIL%2520for%2520Whole%2520Slide%2520Image%2520Classification%26entry.906535625%3DNgoc%2520Bui%2520Lam%2520Quang%2520and%2520Nam%2520Le%2520Nguyen%2520Binh%2520and%2520Thanh-Huy%2520Nguyen%2520and%2520Le%2520Thien%2520Phuc%2520Nguyen%2520and%2520Quan%2520Nguyen%2520and%2520Ulas%2520Bagci%26entry.1292438233%3DMultiple%2520Instance%2520Learning%2520%2528MIL%2529%2520is%2520the%2520leading%2520approach%2520for%2520whole%2520slide%2520image%2520%2528WSI%2529%2520classification%252C%2520enabling%2520efficient%2520analysis%2520of%2520gigapixel%2520pathology%2520slides.%2520Recent%2520work%2520has%2520introduced%2520vision-language%2520models%2520%2528VLMs%2529%2520into%2520MIL%2520pipelines%2520to%2520incorporate%2520medical%2520knowledge%2520through%2520text-based%2520class%2520descriptions%2520rather%2520than%2520simple%2520class%2520names.%2520However%252C%2520when%2520these%2520methods%2520rely%2520on%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520generate%2520clinical%2520descriptions%2520or%2520use%2520fixed-length%2520prompts%2520to%2520represent%2520complex%2520pathology%2520concepts%252C%2520the%2520limited%2520token%2520capacity%2520of%2520VLMs%2520often%2520constrains%2520the%2520expressiveness%2520and%2520richness%2520of%2520the%2520encoded%2520class%2520information.%2520Additionally%252C%2520descriptions%2520generated%2520solely%2520by%2520LLMs%2520may%2520lack%2520domain%2520grounding%2520and%2520fine-grained%2520medical%2520specificity%252C%2520leading%2520to%2520suboptimal%2520alignment%2520with%2520visual%2520features.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520vision-language%2520MIL%2520framework%2520with%2520two%2520key%2520contributions%253A%2520%25281%2529%2520A%2520grounded%2520multi-agent%2520description%2520generation%2520system%2520that%2520leverages%2520curated%2520pathology%2520textbooks%2520and%2520agent%2520specialization%2520%2528e.g.%252C%2520morphology%252C%2520spatial%2520context%2529%2520to%2520produce%2520accurate%2520and%2520diverse%2520clinical%2520descriptions%253B%2520%25282%2529%2520A%2520text%2520encoding%2520strategy%2520using%2520a%2520list%2520of%2520descriptions%2520rather%2520than%2520a%2520single%2520prompt%252C%2520capturing%2520fine-grained%2520and%2520complementary%2520clinical%2520signals%2520for%2520better%2520alignment%2520with%2520visual%2520features.%2520Integrated%2520into%2520a%2520VLM-MIL%2520pipeline%252C%2520our%2520approach%2520shows%2520improved%2520performance%2520over%2520single-prompt%2520class%2520baselines%2520and%2520achieves%2520results%2520comparable%2520to%2520state-of-the-art%2520models%252C%2520as%2520demonstrated%2520on%2520renal%2520and%2520lung%2520cancer%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01293v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GMAT%3A%20Grounded%20Multi-Agent%20Clinical%20Description%20Generation%20for%20Text%20Encoder%20in%20Vision-Language%20MIL%20for%20Whole%20Slide%20Image%20Classification&entry.906535625=Ngoc%20Bui%20Lam%20Quang%20and%20Nam%20Le%20Nguyen%20Binh%20and%20Thanh-Huy%20Nguyen%20and%20Le%20Thien%20Phuc%20Nguyen%20and%20Quan%20Nguyen%20and%20Ulas%20Bagci&entry.1292438233=Multiple%20Instance%20Learning%20%28MIL%29%20is%20the%20leading%20approach%20for%20whole%20slide%20image%20%28WSI%29%20classification%2C%20enabling%20efficient%20analysis%20of%20gigapixel%20pathology%20slides.%20Recent%20work%20has%20introduced%20vision-language%20models%20%28VLMs%29%20into%20MIL%20pipelines%20to%20incorporate%20medical%20knowledge%20through%20text-based%20class%20descriptions%20rather%20than%20simple%20class%20names.%20However%2C%20when%20these%20methods%20rely%20on%20large%20language%20models%20%28LLMs%29%20to%20generate%20clinical%20descriptions%20or%20use%20fixed-length%20prompts%20to%20represent%20complex%20pathology%20concepts%2C%20the%20limited%20token%20capacity%20of%20VLMs%20often%20constrains%20the%20expressiveness%20and%20richness%20of%20the%20encoded%20class%20information.%20Additionally%2C%20descriptions%20generated%20solely%20by%20LLMs%20may%20lack%20domain%20grounding%20and%20fine-grained%20medical%20specificity%2C%20leading%20to%20suboptimal%20alignment%20with%20visual%20features.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20vision-language%20MIL%20framework%20with%20two%20key%20contributions%3A%20%281%29%20A%20grounded%20multi-agent%20description%20generation%20system%20that%20leverages%20curated%20pathology%20textbooks%20and%20agent%20specialization%20%28e.g.%2C%20morphology%2C%20spatial%20context%29%20to%20produce%20accurate%20and%20diverse%20clinical%20descriptions%3B%20%282%29%20A%20text%20encoding%20strategy%20using%20a%20list%20of%20descriptions%20rather%20than%20a%20single%20prompt%2C%20capturing%20fine-grained%20and%20complementary%20clinical%20signals%20for%20better%20alignment%20with%20visual%20features.%20Integrated%20into%20a%20VLM-MIL%20pipeline%2C%20our%20approach%20shows%20improved%20performance%20over%20single-prompt%20class%20baselines%20and%20achieves%20results%20comparable%20to%20state-of-the-art%20models%2C%20as%20demonstrated%20on%20renal%20and%20lung%20cancer%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2508.01293v2&entry.124074799=Read"},
{"title": "MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising", "author": "Chenghan Fu and Daoze Zhang and Yukang Lin and Zhanheng Nie and Xiang Zhang and Jianyu Liu and Yueran Liu and Wanxian Guan and Pengjie Wang and Jian Xu and Bo Zheng", "abstract": "We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of \"Pretraining, Post-training, and Application\", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.", "link": "http://arxiv.org/abs/2511.11305v2", "date": "2025-11-18", "relevancy": 2.7592, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6013}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5311}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOON%20Embedding%3A%20Multimodal%20Representation%20Learning%20for%20E-commerce%20Search%20Advertising&body=Title%3A%20MOON%20Embedding%3A%20Multimodal%20Representation%20Learning%20for%20E-commerce%20Search%20Advertising%0AAuthor%3A%20Chenghan%20Fu%20and%20Daoze%20Zhang%20and%20Yukang%20Lin%20and%20Zhanheng%20Nie%20and%20Xiang%20Zhang%20and%20Jianyu%20Liu%20and%20Yueran%20Liu%20and%20Wanxian%20Guan%20and%20Pengjie%20Wang%20and%20Jian%20Xu%20and%20Bo%20Zheng%0AAbstract%3A%20We%20introduce%20MOON%2C%20our%20comprehensive%20set%20of%20sustainable%20iterative%20practices%20for%20multimodal%20representation%20learning%20for%20e-commerce%20applications.%20MOON%20has%20already%20been%20fully%20deployed%20across%20all%20stages%20of%20Taobao%20search%20advertising%20system%2C%20including%20retrieval%2C%20relevance%2C%20ranking%2C%20and%20so%20on.%20The%20performance%20gains%20are%20particularly%20significant%20on%20click-through%20rate%20%28CTR%29%20prediction%20task%2C%20which%20achieves%20an%20overall%20%2B20.00%25%20online%20CTR%20improvement.%20Over%20the%20past%20three%20years%2C%20this%20project%20has%20delivered%20the%20largest%20improvement%20on%20CTR%20prediction%20task%20and%20undergone%20five%20full-scale%20iterations.%20Throughout%20the%20exploration%20and%20iteration%20of%20our%20MOON%2C%20we%20have%20accumulated%20valuable%20insights%20and%20practical%20experience%20that%20we%20believe%20will%20benefit%20the%20research%20community.%20MOON%20contains%20a%20three-stage%20training%20paradigm%20of%20%22Pretraining%2C%20Post-training%2C%20and%20Application%22%2C%20allowing%20effective%20integration%20of%20multimodal%20representations%20with%20downstream%20tasks.%20Notably%2C%20to%20bridge%20the%20misalignment%20between%20the%20objectives%20of%20multimodal%20representation%20learning%20and%20downstream%20training%2C%20we%20define%20the%20exchange%20rate%20to%20quantify%20how%20effectively%20improvements%20in%20an%20intermediate%20metric%20can%20translate%20into%20downstream%20gains.%20Through%20this%20analysis%2C%20we%20identify%20the%20image-based%20search%20recall%20as%20a%20critical%20intermediate%20metric%20guiding%20the%20optimization%20of%20multimodal%20models.%20Over%20three%20years%20and%20five%20iterations%2C%20MOON%20has%20evolved%20along%20four%20critical%20dimensions%3A%20data%20processing%2C%20training%20strategy%2C%20model%20architecture%2C%20and%20downstream%20application.%20The%20lessons%20and%20insights%20gained%20through%20the%20iterative%20improvements%20will%20also%20be%20shared.%20As%20part%20of%20our%20exploration%20into%20scaling%20effects%20in%20the%20e-commerce%20field%2C%20we%20further%20conduct%20a%20systematic%20study%20of%20the%20scaling%20laws%20governing%20multimodal%20representation%20learning%2C%20examining%20multiple%20factors%20such%20as%20the%20number%20of%20training%20tokens%2C%20negative%20samples%2C%20and%20the%20length%20of%20user%20behavior%20sequences.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11305v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOON%2520Embedding%253A%2520Multimodal%2520Representation%2520Learning%2520for%2520E-commerce%2520Search%2520Advertising%26entry.906535625%3DChenghan%2520Fu%2520and%2520Daoze%2520Zhang%2520and%2520Yukang%2520Lin%2520and%2520Zhanheng%2520Nie%2520and%2520Xiang%2520Zhang%2520and%2520Jianyu%2520Liu%2520and%2520Yueran%2520Liu%2520and%2520Wanxian%2520Guan%2520and%2520Pengjie%2520Wang%2520and%2520Jian%2520Xu%2520and%2520Bo%2520Zheng%26entry.1292438233%3DWe%2520introduce%2520MOON%252C%2520our%2520comprehensive%2520set%2520of%2520sustainable%2520iterative%2520practices%2520for%2520multimodal%2520representation%2520learning%2520for%2520e-commerce%2520applications.%2520MOON%2520has%2520already%2520been%2520fully%2520deployed%2520across%2520all%2520stages%2520of%2520Taobao%2520search%2520advertising%2520system%252C%2520including%2520retrieval%252C%2520relevance%252C%2520ranking%252C%2520and%2520so%2520on.%2520The%2520performance%2520gains%2520are%2520particularly%2520significant%2520on%2520click-through%2520rate%2520%2528CTR%2529%2520prediction%2520task%252C%2520which%2520achieves%2520an%2520overall%2520%252B20.00%2525%2520online%2520CTR%2520improvement.%2520Over%2520the%2520past%2520three%2520years%252C%2520this%2520project%2520has%2520delivered%2520the%2520largest%2520improvement%2520on%2520CTR%2520prediction%2520task%2520and%2520undergone%2520five%2520full-scale%2520iterations.%2520Throughout%2520the%2520exploration%2520and%2520iteration%2520of%2520our%2520MOON%252C%2520we%2520have%2520accumulated%2520valuable%2520insights%2520and%2520practical%2520experience%2520that%2520we%2520believe%2520will%2520benefit%2520the%2520research%2520community.%2520MOON%2520contains%2520a%2520three-stage%2520training%2520paradigm%2520of%2520%2522Pretraining%252C%2520Post-training%252C%2520and%2520Application%2522%252C%2520allowing%2520effective%2520integration%2520of%2520multimodal%2520representations%2520with%2520downstream%2520tasks.%2520Notably%252C%2520to%2520bridge%2520the%2520misalignment%2520between%2520the%2520objectives%2520of%2520multimodal%2520representation%2520learning%2520and%2520downstream%2520training%252C%2520we%2520define%2520the%2520exchange%2520rate%2520to%2520quantify%2520how%2520effectively%2520improvements%2520in%2520an%2520intermediate%2520metric%2520can%2520translate%2520into%2520downstream%2520gains.%2520Through%2520this%2520analysis%252C%2520we%2520identify%2520the%2520image-based%2520search%2520recall%2520as%2520a%2520critical%2520intermediate%2520metric%2520guiding%2520the%2520optimization%2520of%2520multimodal%2520models.%2520Over%2520three%2520years%2520and%2520five%2520iterations%252C%2520MOON%2520has%2520evolved%2520along%2520four%2520critical%2520dimensions%253A%2520data%2520processing%252C%2520training%2520strategy%252C%2520model%2520architecture%252C%2520and%2520downstream%2520application.%2520The%2520lessons%2520and%2520insights%2520gained%2520through%2520the%2520iterative%2520improvements%2520will%2520also%2520be%2520shared.%2520As%2520part%2520of%2520our%2520exploration%2520into%2520scaling%2520effects%2520in%2520the%2520e-commerce%2520field%252C%2520we%2520further%2520conduct%2520a%2520systematic%2520study%2520of%2520the%2520scaling%2520laws%2520governing%2520multimodal%2520representation%2520learning%252C%2520examining%2520multiple%2520factors%2520such%2520as%2520the%2520number%2520of%2520training%2520tokens%252C%2520negative%2520samples%252C%2520and%2520the%2520length%2520of%2520user%2520behavior%2520sequences.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11305v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOON%20Embedding%3A%20Multimodal%20Representation%20Learning%20for%20E-commerce%20Search%20Advertising&entry.906535625=Chenghan%20Fu%20and%20Daoze%20Zhang%20and%20Yukang%20Lin%20and%20Zhanheng%20Nie%20and%20Xiang%20Zhang%20and%20Jianyu%20Liu%20and%20Yueran%20Liu%20and%20Wanxian%20Guan%20and%20Pengjie%20Wang%20and%20Jian%20Xu%20and%20Bo%20Zheng&entry.1292438233=We%20introduce%20MOON%2C%20our%20comprehensive%20set%20of%20sustainable%20iterative%20practices%20for%20multimodal%20representation%20learning%20for%20e-commerce%20applications.%20MOON%20has%20already%20been%20fully%20deployed%20across%20all%20stages%20of%20Taobao%20search%20advertising%20system%2C%20including%20retrieval%2C%20relevance%2C%20ranking%2C%20and%20so%20on.%20The%20performance%20gains%20are%20particularly%20significant%20on%20click-through%20rate%20%28CTR%29%20prediction%20task%2C%20which%20achieves%20an%20overall%20%2B20.00%25%20online%20CTR%20improvement.%20Over%20the%20past%20three%20years%2C%20this%20project%20has%20delivered%20the%20largest%20improvement%20on%20CTR%20prediction%20task%20and%20undergone%20five%20full-scale%20iterations.%20Throughout%20the%20exploration%20and%20iteration%20of%20our%20MOON%2C%20we%20have%20accumulated%20valuable%20insights%20and%20practical%20experience%20that%20we%20believe%20will%20benefit%20the%20research%20community.%20MOON%20contains%20a%20three-stage%20training%20paradigm%20of%20%22Pretraining%2C%20Post-training%2C%20and%20Application%22%2C%20allowing%20effective%20integration%20of%20multimodal%20representations%20with%20downstream%20tasks.%20Notably%2C%20to%20bridge%20the%20misalignment%20between%20the%20objectives%20of%20multimodal%20representation%20learning%20and%20downstream%20training%2C%20we%20define%20the%20exchange%20rate%20to%20quantify%20how%20effectively%20improvements%20in%20an%20intermediate%20metric%20can%20translate%20into%20downstream%20gains.%20Through%20this%20analysis%2C%20we%20identify%20the%20image-based%20search%20recall%20as%20a%20critical%20intermediate%20metric%20guiding%20the%20optimization%20of%20multimodal%20models.%20Over%20three%20years%20and%20five%20iterations%2C%20MOON%20has%20evolved%20along%20four%20critical%20dimensions%3A%20data%20processing%2C%20training%20strategy%2C%20model%20architecture%2C%20and%20downstream%20application.%20The%20lessons%20and%20insights%20gained%20through%20the%20iterative%20improvements%20will%20also%20be%20shared.%20As%20part%20of%20our%20exploration%20into%20scaling%20effects%20in%20the%20e-commerce%20field%2C%20we%20further%20conduct%20a%20systematic%20study%20of%20the%20scaling%20laws%20governing%20multimodal%20representation%20learning%2C%20examining%20multiple%20factors%20such%20as%20the%20number%20of%20training%20tokens%2C%20negative%20samples%2C%20and%20the%20length%20of%20user%20behavior%20sequences.&entry.1838667208=http%3A//arxiv.org/abs/2511.11305v2&entry.124074799=Read"},
{"title": "Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental Modelling", "author": "Julia Peters and Karin Mora and Miguel D. Mahecha and Chaonan Ji and David Montero and Clemens Mosig and Guido Kraemer", "abstract": "Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.", "link": "http://arxiv.org/abs/2511.11706v2", "date": "2025-11-18", "relevancy": 2.7551, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Aware%20Multimodal%20Representation%20Learning%20for%20Spatio-Temporally%20Explicit%20Environmental%20Modelling&body=Title%3A%20Context-Aware%20Multimodal%20Representation%20Learning%20for%20Spatio-Temporally%20Explicit%20Environmental%20Modelling%0AAuthor%3A%20Julia%20Peters%20and%20Karin%20Mora%20and%20Miguel%20D.%20Mahecha%20and%20Chaonan%20Ji%20and%20David%20Montero%20and%20Clemens%20Mosig%20and%20Guido%20Kraemer%0AAbstract%3A%20Earth%20observation%20%28EO%29%20foundation%20models%20have%20emerged%20as%20an%20effective%20approach%20to%20derive%20latent%20representations%20of%20the%20Earth%20system%20from%20various%20remote%20sensing%20sensors.%20These%20models%20produce%20embeddings%20that%20can%20be%20used%20as%20analysis-ready%20datasets%2C%20enabling%20the%20modelling%20of%20ecosystem%20dynamics%20without%20extensive%20sensor-specific%20preprocessing.%20However%2C%20existing%20models%20typically%20operate%20at%20fixed%20spatial%20or%20temporal%20scales%2C%20limiting%20their%20use%20for%20ecological%20analyses%20that%20require%20both%20fine%20spatial%20detail%20and%20high%20temporal%20fidelity.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20representation%20learning%20framework%20that%20integrates%20different%20EO%20modalities%20into%20a%20unified%20feature%20space%20at%20high%20spatio-temporal%20resolution.%20We%20introduce%20the%20framework%20using%20Sentinel-1%20and%20Sentinel-2%20data%20as%20representative%20modalities.%20Our%20approach%20produces%20a%20latent%20space%20at%20native%2010%20m%20resolution%20and%20the%20temporal%20frequency%20of%20cloud-free%20Sentinel-2%20acquisitions.%20Each%20sensor%20is%20first%20modeled%20independently%20to%20capture%20its%20sensor-specific%20characteristics.%20Their%20representations%20are%20then%20combined%20into%20a%20shared%20model.%20This%20two-stage%20design%20enables%20modality-specific%20optimisation%20and%20easy%20extension%20to%20new%20sensors%2C%20retaining%20pretrained%20encoders%20while%20retraining%20only%20fusion%20layers.%20This%20enables%20the%20model%20to%20capture%20complementary%20remote%20sensing%20data%20and%20to%20preserve%20coherence%20across%20space%20and%20time.%20Qualitative%20analyses%20reveal%20that%20the%20learned%20embeddings%20exhibit%20high%20spatial%20and%20semantic%20consistency%20across%20heterogeneous%20landscapes.%20Quantitative%20evaluation%20in%20modelling%20Gross%20Primary%20Production%20reveals%20that%20they%20encode%20ecologically%20meaningful%20patterns%20and%20retain%20sufficient%20temporal%20fidelity%20to%20support%20fine-scale%20analyses.%20Overall%2C%20the%20proposed%20framework%20provides%20a%20flexible%2C%20analysis-ready%20representation%20learning%20approach%20for%20environmental%20applications%20requiring%20diverse%20spatial%20and%20temporal%20resolutions.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11706v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Aware%2520Multimodal%2520Representation%2520Learning%2520for%2520Spatio-Temporally%2520Explicit%2520Environmental%2520Modelling%26entry.906535625%3DJulia%2520Peters%2520and%2520Karin%2520Mora%2520and%2520Miguel%2520D.%2520Mahecha%2520and%2520Chaonan%2520Ji%2520and%2520David%2520Montero%2520and%2520Clemens%2520Mosig%2520and%2520Guido%2520Kraemer%26entry.1292438233%3DEarth%2520observation%2520%2528EO%2529%2520foundation%2520models%2520have%2520emerged%2520as%2520an%2520effective%2520approach%2520to%2520derive%2520latent%2520representations%2520of%2520the%2520Earth%2520system%2520from%2520various%2520remote%2520sensing%2520sensors.%2520These%2520models%2520produce%2520embeddings%2520that%2520can%2520be%2520used%2520as%2520analysis-ready%2520datasets%252C%2520enabling%2520the%2520modelling%2520of%2520ecosystem%2520dynamics%2520without%2520extensive%2520sensor-specific%2520preprocessing.%2520However%252C%2520existing%2520models%2520typically%2520operate%2520at%2520fixed%2520spatial%2520or%2520temporal%2520scales%252C%2520limiting%2520their%2520use%2520for%2520ecological%2520analyses%2520that%2520require%2520both%2520fine%2520spatial%2520detail%2520and%2520high%2520temporal%2520fidelity.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520representation%2520learning%2520framework%2520that%2520integrates%2520different%2520EO%2520modalities%2520into%2520a%2520unified%2520feature%2520space%2520at%2520high%2520spatio-temporal%2520resolution.%2520We%2520introduce%2520the%2520framework%2520using%2520Sentinel-1%2520and%2520Sentinel-2%2520data%2520as%2520representative%2520modalities.%2520Our%2520approach%2520produces%2520a%2520latent%2520space%2520at%2520native%252010%2520m%2520resolution%2520and%2520the%2520temporal%2520frequency%2520of%2520cloud-free%2520Sentinel-2%2520acquisitions.%2520Each%2520sensor%2520is%2520first%2520modeled%2520independently%2520to%2520capture%2520its%2520sensor-specific%2520characteristics.%2520Their%2520representations%2520are%2520then%2520combined%2520into%2520a%2520shared%2520model.%2520This%2520two-stage%2520design%2520enables%2520modality-specific%2520optimisation%2520and%2520easy%2520extension%2520to%2520new%2520sensors%252C%2520retaining%2520pretrained%2520encoders%2520while%2520retraining%2520only%2520fusion%2520layers.%2520This%2520enables%2520the%2520model%2520to%2520capture%2520complementary%2520remote%2520sensing%2520data%2520and%2520to%2520preserve%2520coherence%2520across%2520space%2520and%2520time.%2520Qualitative%2520analyses%2520reveal%2520that%2520the%2520learned%2520embeddings%2520exhibit%2520high%2520spatial%2520and%2520semantic%2520consistency%2520across%2520heterogeneous%2520landscapes.%2520Quantitative%2520evaluation%2520in%2520modelling%2520Gross%2520Primary%2520Production%2520reveals%2520that%2520they%2520encode%2520ecologically%2520meaningful%2520patterns%2520and%2520retain%2520sufficient%2520temporal%2520fidelity%2520to%2520support%2520fine-scale%2520analyses.%2520Overall%252C%2520the%2520proposed%2520framework%2520provides%2520a%2520flexible%252C%2520analysis-ready%2520representation%2520learning%2520approach%2520for%2520environmental%2520applications%2520requiring%2520diverse%2520spatial%2520and%2520temporal%2520resolutions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11706v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Aware%20Multimodal%20Representation%20Learning%20for%20Spatio-Temporally%20Explicit%20Environmental%20Modelling&entry.906535625=Julia%20Peters%20and%20Karin%20Mora%20and%20Miguel%20D.%20Mahecha%20and%20Chaonan%20Ji%20and%20David%20Montero%20and%20Clemens%20Mosig%20and%20Guido%20Kraemer&entry.1292438233=Earth%20observation%20%28EO%29%20foundation%20models%20have%20emerged%20as%20an%20effective%20approach%20to%20derive%20latent%20representations%20of%20the%20Earth%20system%20from%20various%20remote%20sensing%20sensors.%20These%20models%20produce%20embeddings%20that%20can%20be%20used%20as%20analysis-ready%20datasets%2C%20enabling%20the%20modelling%20of%20ecosystem%20dynamics%20without%20extensive%20sensor-specific%20preprocessing.%20However%2C%20existing%20models%20typically%20operate%20at%20fixed%20spatial%20or%20temporal%20scales%2C%20limiting%20their%20use%20for%20ecological%20analyses%20that%20require%20both%20fine%20spatial%20detail%20and%20high%20temporal%20fidelity.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20representation%20learning%20framework%20that%20integrates%20different%20EO%20modalities%20into%20a%20unified%20feature%20space%20at%20high%20spatio-temporal%20resolution.%20We%20introduce%20the%20framework%20using%20Sentinel-1%20and%20Sentinel-2%20data%20as%20representative%20modalities.%20Our%20approach%20produces%20a%20latent%20space%20at%20native%2010%20m%20resolution%20and%20the%20temporal%20frequency%20of%20cloud-free%20Sentinel-2%20acquisitions.%20Each%20sensor%20is%20first%20modeled%20independently%20to%20capture%20its%20sensor-specific%20characteristics.%20Their%20representations%20are%20then%20combined%20into%20a%20shared%20model.%20This%20two-stage%20design%20enables%20modality-specific%20optimisation%20and%20easy%20extension%20to%20new%20sensors%2C%20retaining%20pretrained%20encoders%20while%20retraining%20only%20fusion%20layers.%20This%20enables%20the%20model%20to%20capture%20complementary%20remote%20sensing%20data%20and%20to%20preserve%20coherence%20across%20space%20and%20time.%20Qualitative%20analyses%20reveal%20that%20the%20learned%20embeddings%20exhibit%20high%20spatial%20and%20semantic%20consistency%20across%20heterogeneous%20landscapes.%20Quantitative%20evaluation%20in%20modelling%20Gross%20Primary%20Production%20reveals%20that%20they%20encode%20ecologically%20meaningful%20patterns%20and%20retain%20sufficient%20temporal%20fidelity%20to%20support%20fine-scale%20analyses.%20Overall%2C%20the%20proposed%20framework%20provides%20a%20flexible%2C%20analysis-ready%20representation%20learning%20approach%20for%20environmental%20applications%20requiring%20diverse%20spatial%20and%20temporal%20resolutions.&entry.1838667208=http%3A//arxiv.org/abs/2511.11706v2&entry.124074799=Read"},
{"title": "OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models", "author": "Keda Tao and Kele Shao and Bohan Yu and Weiqiang Wang and Jian liu and Huan Wang", "abstract": "Omnimodal large language models (OmniLLMs) have attracted increasing research attention of late towards unified audio-video understanding, wherein processing audio-video token sequences creates a significant computational bottleneck, however. Existing token compression methods have yet to accommodate this emerging need of jointly compressing multimodal tokens. To bridge this gap, we present OmniZip, a training-free, audio-guided audio-visual token-compression framework that optimizes multimodal token representation and accelerates inference. Specifically, OmniZip first identifies salient audio tokens, then computes an audio retention score for each time group to capture information density, thereby dynamically guiding video token pruning and preserving cues from audio anchors enhanced by cross-modal similarity. For each time window, OmniZip compresses the video tokens using an interleaved spatio-temporal scheme. Extensive empirical results demonstrate the merits of OmniZip - it achieves 3.42X inference speedup and 1.4X memory reduction over other top-performing counterparts, while maintaining performance with no training.", "link": "http://arxiv.org/abs/2511.14582v1", "date": "2025-11-18", "relevancy": 2.7532, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5512}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniZip%3A%20Audio-Guided%20Dynamic%20Token%20Compression%20for%20Fast%20Omnimodal%20Large%20Language%20Models&body=Title%3A%20OmniZip%3A%20Audio-Guided%20Dynamic%20Token%20Compression%20for%20Fast%20Omnimodal%20Large%20Language%20Models%0AAuthor%3A%20Keda%20Tao%20and%20Kele%20Shao%20and%20Bohan%20Yu%20and%20Weiqiang%20Wang%20and%20Jian%20liu%20and%20Huan%20Wang%0AAbstract%3A%20Omnimodal%20large%20language%20models%20%28OmniLLMs%29%20have%20attracted%20increasing%20research%20attention%20of%20late%20towards%20unified%20audio-video%20understanding%2C%20wherein%20processing%20audio-video%20token%20sequences%20creates%20a%20significant%20computational%20bottleneck%2C%20however.%20Existing%20token%20compression%20methods%20have%20yet%20to%20accommodate%20this%20emerging%20need%20of%20jointly%20compressing%20multimodal%20tokens.%20To%20bridge%20this%20gap%2C%20we%20present%20OmniZip%2C%20a%20training-free%2C%20audio-guided%20audio-visual%20token-compression%20framework%20that%20optimizes%20multimodal%20token%20representation%20and%20accelerates%20inference.%20Specifically%2C%20OmniZip%20first%20identifies%20salient%20audio%20tokens%2C%20then%20computes%20an%20audio%20retention%20score%20for%20each%20time%20group%20to%20capture%20information%20density%2C%20thereby%20dynamically%20guiding%20video%20token%20pruning%20and%20preserving%20cues%20from%20audio%20anchors%20enhanced%20by%20cross-modal%20similarity.%20For%20each%20time%20window%2C%20OmniZip%20compresses%20the%20video%20tokens%20using%20an%20interleaved%20spatio-temporal%20scheme.%20Extensive%20empirical%20results%20demonstrate%20the%20merits%20of%20OmniZip%20-%20it%20achieves%203.42X%20inference%20speedup%20and%201.4X%20memory%20reduction%20over%20other%20top-performing%20counterparts%2C%20while%20maintaining%20performance%20with%20no%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniZip%253A%2520Audio-Guided%2520Dynamic%2520Token%2520Compression%2520for%2520Fast%2520Omnimodal%2520Large%2520Language%2520Models%26entry.906535625%3DKeda%2520Tao%2520and%2520Kele%2520Shao%2520and%2520Bohan%2520Yu%2520and%2520Weiqiang%2520Wang%2520and%2520Jian%2520liu%2520and%2520Huan%2520Wang%26entry.1292438233%3DOmnimodal%2520large%2520language%2520models%2520%2528OmniLLMs%2529%2520have%2520attracted%2520increasing%2520research%2520attention%2520of%2520late%2520towards%2520unified%2520audio-video%2520understanding%252C%2520wherein%2520processing%2520audio-video%2520token%2520sequences%2520creates%2520a%2520significant%2520computational%2520bottleneck%252C%2520however.%2520Existing%2520token%2520compression%2520methods%2520have%2520yet%2520to%2520accommodate%2520this%2520emerging%2520need%2520of%2520jointly%2520compressing%2520multimodal%2520tokens.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520OmniZip%252C%2520a%2520training-free%252C%2520audio-guided%2520audio-visual%2520token-compression%2520framework%2520that%2520optimizes%2520multimodal%2520token%2520representation%2520and%2520accelerates%2520inference.%2520Specifically%252C%2520OmniZip%2520first%2520identifies%2520salient%2520audio%2520tokens%252C%2520then%2520computes%2520an%2520audio%2520retention%2520score%2520for%2520each%2520time%2520group%2520to%2520capture%2520information%2520density%252C%2520thereby%2520dynamically%2520guiding%2520video%2520token%2520pruning%2520and%2520preserving%2520cues%2520from%2520audio%2520anchors%2520enhanced%2520by%2520cross-modal%2520similarity.%2520For%2520each%2520time%2520window%252C%2520OmniZip%2520compresses%2520the%2520video%2520tokens%2520using%2520an%2520interleaved%2520spatio-temporal%2520scheme.%2520Extensive%2520empirical%2520results%2520demonstrate%2520the%2520merits%2520of%2520OmniZip%2520-%2520it%2520achieves%25203.42X%2520inference%2520speedup%2520and%25201.4X%2520memory%2520reduction%2520over%2520other%2520top-performing%2520counterparts%252C%2520while%2520maintaining%2520performance%2520with%2520no%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniZip%3A%20Audio-Guided%20Dynamic%20Token%20Compression%20for%20Fast%20Omnimodal%20Large%20Language%20Models&entry.906535625=Keda%20Tao%20and%20Kele%20Shao%20and%20Bohan%20Yu%20and%20Weiqiang%20Wang%20and%20Jian%20liu%20and%20Huan%20Wang&entry.1292438233=Omnimodal%20large%20language%20models%20%28OmniLLMs%29%20have%20attracted%20increasing%20research%20attention%20of%20late%20towards%20unified%20audio-video%20understanding%2C%20wherein%20processing%20audio-video%20token%20sequences%20creates%20a%20significant%20computational%20bottleneck%2C%20however.%20Existing%20token%20compression%20methods%20have%20yet%20to%20accommodate%20this%20emerging%20need%20of%20jointly%20compressing%20multimodal%20tokens.%20To%20bridge%20this%20gap%2C%20we%20present%20OmniZip%2C%20a%20training-free%2C%20audio-guided%20audio-visual%20token-compression%20framework%20that%20optimizes%20multimodal%20token%20representation%20and%20accelerates%20inference.%20Specifically%2C%20OmniZip%20first%20identifies%20salient%20audio%20tokens%2C%20then%20computes%20an%20audio%20retention%20score%20for%20each%20time%20group%20to%20capture%20information%20density%2C%20thereby%20dynamically%20guiding%20video%20token%20pruning%20and%20preserving%20cues%20from%20audio%20anchors%20enhanced%20by%20cross-modal%20similarity.%20For%20each%20time%20window%2C%20OmniZip%20compresses%20the%20video%20tokens%20using%20an%20interleaved%20spatio-temporal%20scheme.%20Extensive%20empirical%20results%20demonstrate%20the%20merits%20of%20OmniZip%20-%20it%20achieves%203.42X%20inference%20speedup%20and%201.4X%20memory%20reduction%20over%20other%20top-performing%20counterparts%2C%20while%20maintaining%20performance%20with%20no%20training.&entry.1838667208=http%3A//arxiv.org/abs/2511.14582v1&entry.124074799=Read"},
{"title": "PALM: A Dataset and Baseline for Learning Multi-subject Hand Prior", "author": "Zicong Fan and Edoardo Remelli and David Dimond and Fadime Sener and Liuhao Ge and Bugra Tekin and Cem Keskin and Shreyas Hampali", "abstract": "The ability to grasp objects, signal with gestures, and share emotion through touch all stem from the unique capabilities of human hands. Yet creating high-quality personalized hand avatars from images remains challenging due to complex geometry, appearance, and articulation, particularly under unconstrained lighting and limited views. Progress has also been limited by the lack of datasets that jointly provide accurate 3D geometry, high-resolution multiview imagery, and a diverse population of subjects. To address this, we present PALM, a large-scale dataset comprising 13k high-quality hand scans from 263 subjects and 90k multi-view images, capturing rich variation in skin tone, age, and geometry. To show its utility, we present a baseline PALM-Net, a multi-subject prior over hand geometry and material properties learned via physically based inverse rendering, enabling realistic, relightable single-image hand avatar personalization. PALM's scale and diversity make it a valuable real-world resource for hand modeling and related research.", "link": "http://arxiv.org/abs/2511.05403v2", "date": "2025-11-18", "relevancy": 2.7486, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5527}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5495}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PALM%3A%20A%20Dataset%20and%20Baseline%20for%20Learning%20Multi-subject%20Hand%20Prior&body=Title%3A%20PALM%3A%20A%20Dataset%20and%20Baseline%20for%20Learning%20Multi-subject%20Hand%20Prior%0AAuthor%3A%20Zicong%20Fan%20and%20Edoardo%20Remelli%20and%20David%20Dimond%20and%20Fadime%20Sener%20and%20Liuhao%20Ge%20and%20Bugra%20Tekin%20and%20Cem%20Keskin%20and%20Shreyas%20Hampali%0AAbstract%3A%20The%20ability%20to%20grasp%20objects%2C%20signal%20with%20gestures%2C%20and%20share%20emotion%20through%20touch%20all%20stem%20from%20the%20unique%20capabilities%20of%20human%20hands.%20Yet%20creating%20high-quality%20personalized%20hand%20avatars%20from%20images%20remains%20challenging%20due%20to%20complex%20geometry%2C%20appearance%2C%20and%20articulation%2C%20particularly%20under%20unconstrained%20lighting%20and%20limited%20views.%20Progress%20has%20also%20been%20limited%20by%20the%20lack%20of%20datasets%20that%20jointly%20provide%20accurate%203D%20geometry%2C%20high-resolution%20multiview%20imagery%2C%20and%20a%20diverse%20population%20of%20subjects.%20To%20address%20this%2C%20we%20present%20PALM%2C%20a%20large-scale%20dataset%20comprising%2013k%20high-quality%20hand%20scans%20from%20263%20subjects%20and%2090k%20multi-view%20images%2C%20capturing%20rich%20variation%20in%20skin%20tone%2C%20age%2C%20and%20geometry.%20To%20show%20its%20utility%2C%20we%20present%20a%20baseline%20PALM-Net%2C%20a%20multi-subject%20prior%20over%20hand%20geometry%20and%20material%20properties%20learned%20via%20physically%20based%20inverse%20rendering%2C%20enabling%20realistic%2C%20relightable%20single-image%20hand%20avatar%20personalization.%20PALM%27s%20scale%20and%20diversity%20make%20it%20a%20valuable%20real-world%20resource%20for%20hand%20modeling%20and%20related%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.05403v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPALM%253A%2520A%2520Dataset%2520and%2520Baseline%2520for%2520Learning%2520Multi-subject%2520Hand%2520Prior%26entry.906535625%3DZicong%2520Fan%2520and%2520Edoardo%2520Remelli%2520and%2520David%2520Dimond%2520and%2520Fadime%2520Sener%2520and%2520Liuhao%2520Ge%2520and%2520Bugra%2520Tekin%2520and%2520Cem%2520Keskin%2520and%2520Shreyas%2520Hampali%26entry.1292438233%3DThe%2520ability%2520to%2520grasp%2520objects%252C%2520signal%2520with%2520gestures%252C%2520and%2520share%2520emotion%2520through%2520touch%2520all%2520stem%2520from%2520the%2520unique%2520capabilities%2520of%2520human%2520hands.%2520Yet%2520creating%2520high-quality%2520personalized%2520hand%2520avatars%2520from%2520images%2520remains%2520challenging%2520due%2520to%2520complex%2520geometry%252C%2520appearance%252C%2520and%2520articulation%252C%2520particularly%2520under%2520unconstrained%2520lighting%2520and%2520limited%2520views.%2520Progress%2520has%2520also%2520been%2520limited%2520by%2520the%2520lack%2520of%2520datasets%2520that%2520jointly%2520provide%2520accurate%25203D%2520geometry%252C%2520high-resolution%2520multiview%2520imagery%252C%2520and%2520a%2520diverse%2520population%2520of%2520subjects.%2520To%2520address%2520this%252C%2520we%2520present%2520PALM%252C%2520a%2520large-scale%2520dataset%2520comprising%252013k%2520high-quality%2520hand%2520scans%2520from%2520263%2520subjects%2520and%252090k%2520multi-view%2520images%252C%2520capturing%2520rich%2520variation%2520in%2520skin%2520tone%252C%2520age%252C%2520and%2520geometry.%2520To%2520show%2520its%2520utility%252C%2520we%2520present%2520a%2520baseline%2520PALM-Net%252C%2520a%2520multi-subject%2520prior%2520over%2520hand%2520geometry%2520and%2520material%2520properties%2520learned%2520via%2520physically%2520based%2520inverse%2520rendering%252C%2520enabling%2520realistic%252C%2520relightable%2520single-image%2520hand%2520avatar%2520personalization.%2520PALM%2527s%2520scale%2520and%2520diversity%2520make%2520it%2520a%2520valuable%2520real-world%2520resource%2520for%2520hand%2520modeling%2520and%2520related%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05403v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PALM%3A%20A%20Dataset%20and%20Baseline%20for%20Learning%20Multi-subject%20Hand%20Prior&entry.906535625=Zicong%20Fan%20and%20Edoardo%20Remelli%20and%20David%20Dimond%20and%20Fadime%20Sener%20and%20Liuhao%20Ge%20and%20Bugra%20Tekin%20and%20Cem%20Keskin%20and%20Shreyas%20Hampali&entry.1292438233=The%20ability%20to%20grasp%20objects%2C%20signal%20with%20gestures%2C%20and%20share%20emotion%20through%20touch%20all%20stem%20from%20the%20unique%20capabilities%20of%20human%20hands.%20Yet%20creating%20high-quality%20personalized%20hand%20avatars%20from%20images%20remains%20challenging%20due%20to%20complex%20geometry%2C%20appearance%2C%20and%20articulation%2C%20particularly%20under%20unconstrained%20lighting%20and%20limited%20views.%20Progress%20has%20also%20been%20limited%20by%20the%20lack%20of%20datasets%20that%20jointly%20provide%20accurate%203D%20geometry%2C%20high-resolution%20multiview%20imagery%2C%20and%20a%20diverse%20population%20of%20subjects.%20To%20address%20this%2C%20we%20present%20PALM%2C%20a%20large-scale%20dataset%20comprising%2013k%20high-quality%20hand%20scans%20from%20263%20subjects%20and%2090k%20multi-view%20images%2C%20capturing%20rich%20variation%20in%20skin%20tone%2C%20age%2C%20and%20geometry.%20To%20show%20its%20utility%2C%20we%20present%20a%20baseline%20PALM-Net%2C%20a%20multi-subject%20prior%20over%20hand%20geometry%20and%20material%20properties%20learned%20via%20physically%20based%20inverse%20rendering%2C%20enabling%20realistic%2C%20relightable%20single-image%20hand%20avatar%20personalization.%20PALM%27s%20scale%20and%20diversity%20make%20it%20a%20valuable%20real-world%20resource%20for%20hand%20modeling%20and%20related%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.05403v2&entry.124074799=Read"},
{"title": "3D-Guided Scalable Flow Matching for Generating Volumetric Tissue Spatial Transcriptomics from Serial Histology", "author": "Mohammad Vali Sanian and Arshia Hemmat and Amirhossein Vahidi and Jonas Maaskola and Jimmy Tsz Hang Lee and Stanislaw Makarchuk and Yeliz Demirci and Nana-Jane Chipampe and Omer Bayraktar and Lassi Paavolainen and Mohammad Lotfollahi", "abstract": "A scalable and robust 3D tissue transcriptomics profile can enable a holistic understanding of tissue organization and provide deeper insights into human biology and disease. Most predictive algorithms that infer ST directly from histology treat each section independently and ignore 3D structure, while existing 3D-aware approaches are not generative and do not scale well. We present Holographic Tissue Expression Inpainting and Analysis (HoloTea), a 3D-aware flow-matching framework that imputes spot-level gene expression from H&E while explicitly using information from adjacent sections. Our key idea is to retrieve morphologically corresponding spots on neighboring slides in a shared feature space and fuse this cross section context into a lightweight ControlNet, allowing conditioning to follow anatomical continuity. To better capture the count nature of the data, we introduce a 3D-consistent prior for flow matching that combines a learned zero-inflated negative binomial (ZINB) prior with a spatial-empirical prior constructed from neighboring sections. A global attention block introduces 3D H&E scaling linearly with the number of spots in the slide, enabling training and inference on large 3D ST datasets. Across three spatial transcriptomics datasets spanning different tissue types and resolutions, HoloTea consistently improves 3D expression accuracy and generalization compared to 2D and 3D baselines. We envision HoloTea advancing the creation of accurate 3D virtual tissues, ultimately accelerating biomarker discovery and deepening our understanding of disease.", "link": "http://arxiv.org/abs/2511.14613v1", "date": "2025-11-18", "relevancy": 2.7319, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5549}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5467}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-Guided%20Scalable%20Flow%20Matching%20for%20Generating%20Volumetric%20Tissue%20Spatial%20Transcriptomics%20from%20Serial%20Histology&body=Title%3A%203D-Guided%20Scalable%20Flow%20Matching%20for%20Generating%20Volumetric%20Tissue%20Spatial%20Transcriptomics%20from%20Serial%20Histology%0AAuthor%3A%20Mohammad%20Vali%20Sanian%20and%20Arshia%20Hemmat%20and%20Amirhossein%20Vahidi%20and%20Jonas%20Maaskola%20and%20Jimmy%20Tsz%20Hang%20Lee%20and%20Stanislaw%20Makarchuk%20and%20Yeliz%20Demirci%20and%20Nana-Jane%20Chipampe%20and%20Omer%20Bayraktar%20and%20Lassi%20Paavolainen%20and%20Mohammad%20Lotfollahi%0AAbstract%3A%20A%20scalable%20and%20robust%203D%20tissue%20transcriptomics%20profile%20can%20enable%20a%20holistic%20understanding%20of%20tissue%20organization%20and%20provide%20deeper%20insights%20into%20human%20biology%20and%20disease.%20Most%20predictive%20algorithms%20that%20infer%20ST%20directly%20from%20histology%20treat%20each%20section%20independently%20and%20ignore%203D%20structure%2C%20while%20existing%203D-aware%20approaches%20are%20not%20generative%20and%20do%20not%20scale%20well.%20We%20present%20Holographic%20Tissue%20Expression%20Inpainting%20and%20Analysis%20%28HoloTea%29%2C%20a%203D-aware%20flow-matching%20framework%20that%20imputes%20spot-level%20gene%20expression%20from%20H%26E%20while%20explicitly%20using%20information%20from%20adjacent%20sections.%20Our%20key%20idea%20is%20to%20retrieve%20morphologically%20corresponding%20spots%20on%20neighboring%20slides%20in%20a%20shared%20feature%20space%20and%20fuse%20this%20cross%20section%20context%20into%20a%20lightweight%20ControlNet%2C%20allowing%20conditioning%20to%20follow%20anatomical%20continuity.%20To%20better%20capture%20the%20count%20nature%20of%20the%20data%2C%20we%20introduce%20a%203D-consistent%20prior%20for%20flow%20matching%20that%20combines%20a%20learned%20zero-inflated%20negative%20binomial%20%28ZINB%29%20prior%20with%20a%20spatial-empirical%20prior%20constructed%20from%20neighboring%20sections.%20A%20global%20attention%20block%20introduces%203D%20H%26E%20scaling%20linearly%20with%20the%20number%20of%20spots%20in%20the%20slide%2C%20enabling%20training%20and%20inference%20on%20large%203D%20ST%20datasets.%20Across%20three%20spatial%20transcriptomics%20datasets%20spanning%20different%20tissue%20types%20and%20resolutions%2C%20HoloTea%20consistently%20improves%203D%20expression%20accuracy%20and%20generalization%20compared%20to%202D%20and%203D%20baselines.%20We%20envision%20HoloTea%20advancing%20the%20creation%20of%20accurate%203D%20virtual%20tissues%2C%20ultimately%20accelerating%20biomarker%20discovery%20and%20deepening%20our%20understanding%20of%20disease.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-Guided%2520Scalable%2520Flow%2520Matching%2520for%2520Generating%2520Volumetric%2520Tissue%2520Spatial%2520Transcriptomics%2520from%2520Serial%2520Histology%26entry.906535625%3DMohammad%2520Vali%2520Sanian%2520and%2520Arshia%2520Hemmat%2520and%2520Amirhossein%2520Vahidi%2520and%2520Jonas%2520Maaskola%2520and%2520Jimmy%2520Tsz%2520Hang%2520Lee%2520and%2520Stanislaw%2520Makarchuk%2520and%2520Yeliz%2520Demirci%2520and%2520Nana-Jane%2520Chipampe%2520and%2520Omer%2520Bayraktar%2520and%2520Lassi%2520Paavolainen%2520and%2520Mohammad%2520Lotfollahi%26entry.1292438233%3DA%2520scalable%2520and%2520robust%25203D%2520tissue%2520transcriptomics%2520profile%2520can%2520enable%2520a%2520holistic%2520understanding%2520of%2520tissue%2520organization%2520and%2520provide%2520deeper%2520insights%2520into%2520human%2520biology%2520and%2520disease.%2520Most%2520predictive%2520algorithms%2520that%2520infer%2520ST%2520directly%2520from%2520histology%2520treat%2520each%2520section%2520independently%2520and%2520ignore%25203D%2520structure%252C%2520while%2520existing%25203D-aware%2520approaches%2520are%2520not%2520generative%2520and%2520do%2520not%2520scale%2520well.%2520We%2520present%2520Holographic%2520Tissue%2520Expression%2520Inpainting%2520and%2520Analysis%2520%2528HoloTea%2529%252C%2520a%25203D-aware%2520flow-matching%2520framework%2520that%2520imputes%2520spot-level%2520gene%2520expression%2520from%2520H%2526E%2520while%2520explicitly%2520using%2520information%2520from%2520adjacent%2520sections.%2520Our%2520key%2520idea%2520is%2520to%2520retrieve%2520morphologically%2520corresponding%2520spots%2520on%2520neighboring%2520slides%2520in%2520a%2520shared%2520feature%2520space%2520and%2520fuse%2520this%2520cross%2520section%2520context%2520into%2520a%2520lightweight%2520ControlNet%252C%2520allowing%2520conditioning%2520to%2520follow%2520anatomical%2520continuity.%2520To%2520better%2520capture%2520the%2520count%2520nature%2520of%2520the%2520data%252C%2520we%2520introduce%2520a%25203D-consistent%2520prior%2520for%2520flow%2520matching%2520that%2520combines%2520a%2520learned%2520zero-inflated%2520negative%2520binomial%2520%2528ZINB%2529%2520prior%2520with%2520a%2520spatial-empirical%2520prior%2520constructed%2520from%2520neighboring%2520sections.%2520A%2520global%2520attention%2520block%2520introduces%25203D%2520H%2526E%2520scaling%2520linearly%2520with%2520the%2520number%2520of%2520spots%2520in%2520the%2520slide%252C%2520enabling%2520training%2520and%2520inference%2520on%2520large%25203D%2520ST%2520datasets.%2520Across%2520three%2520spatial%2520transcriptomics%2520datasets%2520spanning%2520different%2520tissue%2520types%2520and%2520resolutions%252C%2520HoloTea%2520consistently%2520improves%25203D%2520expression%2520accuracy%2520and%2520generalization%2520compared%2520to%25202D%2520and%25203D%2520baselines.%2520We%2520envision%2520HoloTea%2520advancing%2520the%2520creation%2520of%2520accurate%25203D%2520virtual%2520tissues%252C%2520ultimately%2520accelerating%2520biomarker%2520discovery%2520and%2520deepening%2520our%2520understanding%2520of%2520disease.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-Guided%20Scalable%20Flow%20Matching%20for%20Generating%20Volumetric%20Tissue%20Spatial%20Transcriptomics%20from%20Serial%20Histology&entry.906535625=Mohammad%20Vali%20Sanian%20and%20Arshia%20Hemmat%20and%20Amirhossein%20Vahidi%20and%20Jonas%20Maaskola%20and%20Jimmy%20Tsz%20Hang%20Lee%20and%20Stanislaw%20Makarchuk%20and%20Yeliz%20Demirci%20and%20Nana-Jane%20Chipampe%20and%20Omer%20Bayraktar%20and%20Lassi%20Paavolainen%20and%20Mohammad%20Lotfollahi&entry.1292438233=A%20scalable%20and%20robust%203D%20tissue%20transcriptomics%20profile%20can%20enable%20a%20holistic%20understanding%20of%20tissue%20organization%20and%20provide%20deeper%20insights%20into%20human%20biology%20and%20disease.%20Most%20predictive%20algorithms%20that%20infer%20ST%20directly%20from%20histology%20treat%20each%20section%20independently%20and%20ignore%203D%20structure%2C%20while%20existing%203D-aware%20approaches%20are%20not%20generative%20and%20do%20not%20scale%20well.%20We%20present%20Holographic%20Tissue%20Expression%20Inpainting%20and%20Analysis%20%28HoloTea%29%2C%20a%203D-aware%20flow-matching%20framework%20that%20imputes%20spot-level%20gene%20expression%20from%20H%26E%20while%20explicitly%20using%20information%20from%20adjacent%20sections.%20Our%20key%20idea%20is%20to%20retrieve%20morphologically%20corresponding%20spots%20on%20neighboring%20slides%20in%20a%20shared%20feature%20space%20and%20fuse%20this%20cross%20section%20context%20into%20a%20lightweight%20ControlNet%2C%20allowing%20conditioning%20to%20follow%20anatomical%20continuity.%20To%20better%20capture%20the%20count%20nature%20of%20the%20data%2C%20we%20introduce%20a%203D-consistent%20prior%20for%20flow%20matching%20that%20combines%20a%20learned%20zero-inflated%20negative%20binomial%20%28ZINB%29%20prior%20with%20a%20spatial-empirical%20prior%20constructed%20from%20neighboring%20sections.%20A%20global%20attention%20block%20introduces%203D%20H%26E%20scaling%20linearly%20with%20the%20number%20of%20spots%20in%20the%20slide%2C%20enabling%20training%20and%20inference%20on%20large%203D%20ST%20datasets.%20Across%20three%20spatial%20transcriptomics%20datasets%20spanning%20different%20tissue%20types%20and%20resolutions%2C%20HoloTea%20consistently%20improves%203D%20expression%20accuracy%20and%20generalization%20compared%20to%202D%20and%203D%20baselines.%20We%20envision%20HoloTea%20advancing%20the%20creation%20of%20accurate%203D%20virtual%20tissues%2C%20ultimately%20accelerating%20biomarker%20discovery%20and%20deepening%20our%20understanding%20of%20disease.&entry.1838667208=http%3A//arxiv.org/abs/2511.14613v1&entry.124074799=Read"},
{"title": "SlotMatch: Distilling Object-Centric Representations for Unsupervised Video Segmentation", "author": "Diana-Nicoleta Grigore and Neelu Madan and Andreas Mogelmose and Thomas B. Moeslund and Radu Tudor Ionescu", "abstract": "Unsupervised video segmentation is a challenging computer vision task, especially due to the lack of supervisory signals coupled with the complexity of visual scenes. To overcome this challenge, state-of-the-art models based on slot attention often have to rely on large and computationally expensive neural architectures. To this end, we propose a simple knowledge distillation framework that effectively transfers object-centric representations to a lightweight student. The proposed framework, called SlotMatch, aligns corresponding teacher and student slots via the cosine similarity, requiring no additional distillation objectives or auxiliary supervision. The simplicity of SlotMatch is confirmed via theoretical and empirical evidence, both indicating that integrating additional losses is redundant. We conduct experiments on three datasets to compare the state-of-the-art teacher model, SlotContrast, with our distilled student. The results show that our student based on SlotMatch matches and even outperforms its teacher, while using 3.6x less parameters and running up to 2.7x faster. Moreover, our student surpasses all other state-of-the-art unsupervised video segmentation models.", "link": "http://arxiv.org/abs/2508.03411v3", "date": "2025-11-18", "relevancy": 2.7213, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.555}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5391}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlotMatch%3A%20Distilling%20Object-Centric%20Representations%20for%20Unsupervised%20Video%20Segmentation&body=Title%3A%20SlotMatch%3A%20Distilling%20Object-Centric%20Representations%20for%20Unsupervised%20Video%20Segmentation%0AAuthor%3A%20Diana-Nicoleta%20Grigore%20and%20Neelu%20Madan%20and%20Andreas%20Mogelmose%20and%20Thomas%20B.%20Moeslund%20and%20Radu%20Tudor%20Ionescu%0AAbstract%3A%20Unsupervised%20video%20segmentation%20is%20a%20challenging%20computer%20vision%20task%2C%20especially%20due%20to%20the%20lack%20of%20supervisory%20signals%20coupled%20with%20the%20complexity%20of%20visual%20scenes.%20To%20overcome%20this%20challenge%2C%20state-of-the-art%20models%20based%20on%20slot%20attention%20often%20have%20to%20rely%20on%20large%20and%20computationally%20expensive%20neural%20architectures.%20To%20this%20end%2C%20we%20propose%20a%20simple%20knowledge%20distillation%20framework%20that%20effectively%20transfers%20object-centric%20representations%20to%20a%20lightweight%20student.%20The%20proposed%20framework%2C%20called%20SlotMatch%2C%20aligns%20corresponding%20teacher%20and%20student%20slots%20via%20the%20cosine%20similarity%2C%20requiring%20no%20additional%20distillation%20objectives%20or%20auxiliary%20supervision.%20The%20simplicity%20of%20SlotMatch%20is%20confirmed%20via%20theoretical%20and%20empirical%20evidence%2C%20both%20indicating%20that%20integrating%20additional%20losses%20is%20redundant.%20We%20conduct%20experiments%20on%20three%20datasets%20to%20compare%20the%20state-of-the-art%20teacher%20model%2C%20SlotContrast%2C%20with%20our%20distilled%20student.%20The%20results%20show%20that%20our%20student%20based%20on%20SlotMatch%20matches%20and%20even%20outperforms%20its%20teacher%2C%20while%20using%203.6x%20less%20parameters%20and%20running%20up%20to%202.7x%20faster.%20Moreover%2C%20our%20student%20surpasses%20all%20other%20state-of-the-art%20unsupervised%20video%20segmentation%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2508.03411v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlotMatch%253A%2520Distilling%2520Object-Centric%2520Representations%2520for%2520Unsupervised%2520Video%2520Segmentation%26entry.906535625%3DDiana-Nicoleta%2520Grigore%2520and%2520Neelu%2520Madan%2520and%2520Andreas%2520Mogelmose%2520and%2520Thomas%2520B.%2520Moeslund%2520and%2520Radu%2520Tudor%2520Ionescu%26entry.1292438233%3DUnsupervised%2520video%2520segmentation%2520is%2520a%2520challenging%2520computer%2520vision%2520task%252C%2520especially%2520due%2520to%2520the%2520lack%2520of%2520supervisory%2520signals%2520coupled%2520with%2520the%2520complexity%2520of%2520visual%2520scenes.%2520To%2520overcome%2520this%2520challenge%252C%2520state-of-the-art%2520models%2520based%2520on%2520slot%2520attention%2520often%2520have%2520to%2520rely%2520on%2520large%2520and%2520computationally%2520expensive%2520neural%2520architectures.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520simple%2520knowledge%2520distillation%2520framework%2520that%2520effectively%2520transfers%2520object-centric%2520representations%2520to%2520a%2520lightweight%2520student.%2520The%2520proposed%2520framework%252C%2520called%2520SlotMatch%252C%2520aligns%2520corresponding%2520teacher%2520and%2520student%2520slots%2520via%2520the%2520cosine%2520similarity%252C%2520requiring%2520no%2520additional%2520distillation%2520objectives%2520or%2520auxiliary%2520supervision.%2520The%2520simplicity%2520of%2520SlotMatch%2520is%2520confirmed%2520via%2520theoretical%2520and%2520empirical%2520evidence%252C%2520both%2520indicating%2520that%2520integrating%2520additional%2520losses%2520is%2520redundant.%2520We%2520conduct%2520experiments%2520on%2520three%2520datasets%2520to%2520compare%2520the%2520state-of-the-art%2520teacher%2520model%252C%2520SlotContrast%252C%2520with%2520our%2520distilled%2520student.%2520The%2520results%2520show%2520that%2520our%2520student%2520based%2520on%2520SlotMatch%2520matches%2520and%2520even%2520outperforms%2520its%2520teacher%252C%2520while%2520using%25203.6x%2520less%2520parameters%2520and%2520running%2520up%2520to%25202.7x%2520faster.%2520Moreover%252C%2520our%2520student%2520surpasses%2520all%2520other%2520state-of-the-art%2520unsupervised%2520video%2520segmentation%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03411v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlotMatch%3A%20Distilling%20Object-Centric%20Representations%20for%20Unsupervised%20Video%20Segmentation&entry.906535625=Diana-Nicoleta%20Grigore%20and%20Neelu%20Madan%20and%20Andreas%20Mogelmose%20and%20Thomas%20B.%20Moeslund%20and%20Radu%20Tudor%20Ionescu&entry.1292438233=Unsupervised%20video%20segmentation%20is%20a%20challenging%20computer%20vision%20task%2C%20especially%20due%20to%20the%20lack%20of%20supervisory%20signals%20coupled%20with%20the%20complexity%20of%20visual%20scenes.%20To%20overcome%20this%20challenge%2C%20state-of-the-art%20models%20based%20on%20slot%20attention%20often%20have%20to%20rely%20on%20large%20and%20computationally%20expensive%20neural%20architectures.%20To%20this%20end%2C%20we%20propose%20a%20simple%20knowledge%20distillation%20framework%20that%20effectively%20transfers%20object-centric%20representations%20to%20a%20lightweight%20student.%20The%20proposed%20framework%2C%20called%20SlotMatch%2C%20aligns%20corresponding%20teacher%20and%20student%20slots%20via%20the%20cosine%20similarity%2C%20requiring%20no%20additional%20distillation%20objectives%20or%20auxiliary%20supervision.%20The%20simplicity%20of%20SlotMatch%20is%20confirmed%20via%20theoretical%20and%20empirical%20evidence%2C%20both%20indicating%20that%20integrating%20additional%20losses%20is%20redundant.%20We%20conduct%20experiments%20on%20three%20datasets%20to%20compare%20the%20state-of-the-art%20teacher%20model%2C%20SlotContrast%2C%20with%20our%20distilled%20student.%20The%20results%20show%20that%20our%20student%20based%20on%20SlotMatch%20matches%20and%20even%20outperforms%20its%20teacher%2C%20while%20using%203.6x%20less%20parameters%20and%20running%20up%20to%202.7x%20faster.%20Moreover%2C%20our%20student%20surpasses%20all%20other%20state-of-the-art%20unsupervised%20video%20segmentation%20models.&entry.1838667208=http%3A//arxiv.org/abs/2508.03411v3&entry.124074799=Read"},
{"title": "Cranio-ID: Graph-Based Craniofacial Identification via Automatic Landmark Annotation in 2D Multi-View X-rays", "author": "Ravi Shankar Prasad and Nandani Sharma and Dinesh Singh", "abstract": "In forensic craniofacial identification and in many biomedical applications, craniometric landmarks are important. Traditional methods for locating landmarks are time-consuming and require specialized knowledge and expertise. Current methods utilize superimposition and deep learning-based methods that employ automatic annotation of landmarks. However, these methods are not reliable due to insufficient large-scale validation studies. In this paper, we proposed a novel framework Cranio-ID: First, an automatic annotation of landmarks on 2D skulls (which are X-ray scans of faces) with their respective optical images using our trained YOLO-pose models. Second, cross-modal matching by formulating these landmarks into graph representations and then finding semantic correspondence between graphs of these two modalities using cross-attention and optimal transport framework. Our proposed framework is validated on the S2F and CUHK datasets (CUHK dataset resembles with S2F dataset). Extensive experiments have been conducted to evaluate the performance of our proposed framework, which demonstrates significant improvements in both reliability and accuracy, as well as its effectiveness in cross-domain skull-to-face and sketch-to-face matching in forensic science.", "link": "http://arxiv.org/abs/2511.14411v1", "date": "2025-11-18", "relevancy": 2.7178, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5888}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5318}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cranio-ID%3A%20Graph-Based%20Craniofacial%20Identification%20via%20Automatic%20Landmark%20Annotation%20in%202D%20Multi-View%20X-rays&body=Title%3A%20Cranio-ID%3A%20Graph-Based%20Craniofacial%20Identification%20via%20Automatic%20Landmark%20Annotation%20in%202D%20Multi-View%20X-rays%0AAuthor%3A%20Ravi%20Shankar%20Prasad%20and%20Nandani%20Sharma%20and%20Dinesh%20Singh%0AAbstract%3A%20In%20forensic%20craniofacial%20identification%20and%20in%20many%20biomedical%20applications%2C%20craniometric%20landmarks%20are%20important.%20Traditional%20methods%20for%20locating%20landmarks%20are%20time-consuming%20and%20require%20specialized%20knowledge%20and%20expertise.%20Current%20methods%20utilize%20superimposition%20and%20deep%20learning-based%20methods%20that%20employ%20automatic%20annotation%20of%20landmarks.%20However%2C%20these%20methods%20are%20not%20reliable%20due%20to%20insufficient%20large-scale%20validation%20studies.%20In%20this%20paper%2C%20we%20proposed%20a%20novel%20framework%20Cranio-ID%3A%20First%2C%20an%20automatic%20annotation%20of%20landmarks%20on%202D%20skulls%20%28which%20are%20X-ray%20scans%20of%20faces%29%20with%20their%20respective%20optical%20images%20using%20our%20trained%20YOLO-pose%20models.%20Second%2C%20cross-modal%20matching%20by%20formulating%20these%20landmarks%20into%20graph%20representations%20and%20then%20finding%20semantic%20correspondence%20between%20graphs%20of%20these%20two%20modalities%20using%20cross-attention%20and%20optimal%20transport%20framework.%20Our%20proposed%20framework%20is%20validated%20on%20the%20S2F%20and%20CUHK%20datasets%20%28CUHK%20dataset%20resembles%20with%20S2F%20dataset%29.%20Extensive%20experiments%20have%20been%20conducted%20to%20evaluate%20the%20performance%20of%20our%20proposed%20framework%2C%20which%20demonstrates%20significant%20improvements%20in%20both%20reliability%20and%20accuracy%2C%20as%20well%20as%20its%20effectiveness%20in%20cross-domain%20skull-to-face%20and%20sketch-to-face%20matching%20in%20forensic%20science.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCranio-ID%253A%2520Graph-Based%2520Craniofacial%2520Identification%2520via%2520Automatic%2520Landmark%2520Annotation%2520in%25202D%2520Multi-View%2520X-rays%26entry.906535625%3DRavi%2520Shankar%2520Prasad%2520and%2520Nandani%2520Sharma%2520and%2520Dinesh%2520Singh%26entry.1292438233%3DIn%2520forensic%2520craniofacial%2520identification%2520and%2520in%2520many%2520biomedical%2520applications%252C%2520craniometric%2520landmarks%2520are%2520important.%2520Traditional%2520methods%2520for%2520locating%2520landmarks%2520are%2520time-consuming%2520and%2520require%2520specialized%2520knowledge%2520and%2520expertise.%2520Current%2520methods%2520utilize%2520superimposition%2520and%2520deep%2520learning-based%2520methods%2520that%2520employ%2520automatic%2520annotation%2520of%2520landmarks.%2520However%252C%2520these%2520methods%2520are%2520not%2520reliable%2520due%2520to%2520insufficient%2520large-scale%2520validation%2520studies.%2520In%2520this%2520paper%252C%2520we%2520proposed%2520a%2520novel%2520framework%2520Cranio-ID%253A%2520First%252C%2520an%2520automatic%2520annotation%2520of%2520landmarks%2520on%25202D%2520skulls%2520%2528which%2520are%2520X-ray%2520scans%2520of%2520faces%2529%2520with%2520their%2520respective%2520optical%2520images%2520using%2520our%2520trained%2520YOLO-pose%2520models.%2520Second%252C%2520cross-modal%2520matching%2520by%2520formulating%2520these%2520landmarks%2520into%2520graph%2520representations%2520and%2520then%2520finding%2520semantic%2520correspondence%2520between%2520graphs%2520of%2520these%2520two%2520modalities%2520using%2520cross-attention%2520and%2520optimal%2520transport%2520framework.%2520Our%2520proposed%2520framework%2520is%2520validated%2520on%2520the%2520S2F%2520and%2520CUHK%2520datasets%2520%2528CUHK%2520dataset%2520resembles%2520with%2520S2F%2520dataset%2529.%2520Extensive%2520experiments%2520have%2520been%2520conducted%2520to%2520evaluate%2520the%2520performance%2520of%2520our%2520proposed%2520framework%252C%2520which%2520demonstrates%2520significant%2520improvements%2520in%2520both%2520reliability%2520and%2520accuracy%252C%2520as%2520well%2520as%2520its%2520effectiveness%2520in%2520cross-domain%2520skull-to-face%2520and%2520sketch-to-face%2520matching%2520in%2520forensic%2520science.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cranio-ID%3A%20Graph-Based%20Craniofacial%20Identification%20via%20Automatic%20Landmark%20Annotation%20in%202D%20Multi-View%20X-rays&entry.906535625=Ravi%20Shankar%20Prasad%20and%20Nandani%20Sharma%20and%20Dinesh%20Singh&entry.1292438233=In%20forensic%20craniofacial%20identification%20and%20in%20many%20biomedical%20applications%2C%20craniometric%20landmarks%20are%20important.%20Traditional%20methods%20for%20locating%20landmarks%20are%20time-consuming%20and%20require%20specialized%20knowledge%20and%20expertise.%20Current%20methods%20utilize%20superimposition%20and%20deep%20learning-based%20methods%20that%20employ%20automatic%20annotation%20of%20landmarks.%20However%2C%20these%20methods%20are%20not%20reliable%20due%20to%20insufficient%20large-scale%20validation%20studies.%20In%20this%20paper%2C%20we%20proposed%20a%20novel%20framework%20Cranio-ID%3A%20First%2C%20an%20automatic%20annotation%20of%20landmarks%20on%202D%20skulls%20%28which%20are%20X-ray%20scans%20of%20faces%29%20with%20their%20respective%20optical%20images%20using%20our%20trained%20YOLO-pose%20models.%20Second%2C%20cross-modal%20matching%20by%20formulating%20these%20landmarks%20into%20graph%20representations%20and%20then%20finding%20semantic%20correspondence%20between%20graphs%20of%20these%20two%20modalities%20using%20cross-attention%20and%20optimal%20transport%20framework.%20Our%20proposed%20framework%20is%20validated%20on%20the%20S2F%20and%20CUHK%20datasets%20%28CUHK%20dataset%20resembles%20with%20S2F%20dataset%29.%20Extensive%20experiments%20have%20been%20conducted%20to%20evaluate%20the%20performance%20of%20our%20proposed%20framework%2C%20which%20demonstrates%20significant%20improvements%20in%20both%20reliability%20and%20accuracy%2C%20as%20well%20as%20its%20effectiveness%20in%20cross-domain%20skull-to-face%20and%20sketch-to-face%20matching%20in%20forensic%20science.&entry.1838667208=http%3A//arxiv.org/abs/2511.14411v1&entry.124074799=Read"},
{"title": "Logos as a Well-Tempered Pre-train for Sign Language Recognition", "author": "Ilya Ovodov and Petr Surovtsev and Karina Kvanchiani and Alexander Kapitanov and Alexander Nagaev", "abstract": "This paper examines two aspects of the isolated sign language recognition (ISLR) task. First, although a certain number of datasets is available, the data for individual sign languages is limited. It poses the challenge of cross-language ISLR model training, including transfer learning. Second, similar signs can have different semantic meanings. It leads to ambiguity in dataset labeling and raises the question of the best policy for annotating such signs. To address these issues, this study presents Logos, a novel Russian Sign Language (RSL) dataset, the most extensive available ISLR dataset by the number of signers, one of the most extensive datasets in size and vocabulary, and the largest RSL dataset. It is shown that a model, pre-trained on the Logos dataset can be used as a universal encoder for other language SLR tasks, including few-shot learning. We explore cross-language transfer learning approaches and find that joint training using multiple classification heads benefits accuracy for the target low-resource datasets the most. The key feature of the Logos dataset is explicitly annotated visually similar sign groups. We show that explicitly labeling visually similar signs improves trained model quality as a visual encoder for downstream tasks. Based on the proposed contributions, we outperform current state-of-the-art results for the WLASL dataset and get competitive results for the AUTSL dataset, with a single stream model processing solely RGB video. The source code, dataset, and pre-trained models are publicly available.", "link": "http://arxiv.org/abs/2505.10481v2", "date": "2025-11-18", "relevancy": 2.7141, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logos%20as%20a%20Well-Tempered%20Pre-train%20for%20Sign%20Language%20Recognition&body=Title%3A%20Logos%20as%20a%20Well-Tempered%20Pre-train%20for%20Sign%20Language%20Recognition%0AAuthor%3A%20Ilya%20Ovodov%20and%20Petr%20Surovtsev%20and%20Karina%20Kvanchiani%20and%20Alexander%20Kapitanov%20and%20Alexander%20Nagaev%0AAbstract%3A%20This%20paper%20examines%20two%20aspects%20of%20the%20isolated%20sign%20language%20recognition%20%28ISLR%29%20task.%20First%2C%20although%20a%20certain%20number%20of%20datasets%20is%20available%2C%20the%20data%20for%20individual%20sign%20languages%20is%20limited.%20It%20poses%20the%20challenge%20of%20cross-language%20ISLR%20model%20training%2C%20including%20transfer%20learning.%20Second%2C%20similar%20signs%20can%20have%20different%20semantic%20meanings.%20It%20leads%20to%20ambiguity%20in%20dataset%20labeling%20and%20raises%20the%20question%20of%20the%20best%20policy%20for%20annotating%20such%20signs.%20To%20address%20these%20issues%2C%20this%20study%20presents%20Logos%2C%20a%20novel%20Russian%20Sign%20Language%20%28RSL%29%20dataset%2C%20the%20most%20extensive%20available%20ISLR%20dataset%20by%20the%20number%20of%20signers%2C%20one%20of%20the%20most%20extensive%20datasets%20in%20size%20and%20vocabulary%2C%20and%20the%20largest%20RSL%20dataset.%20It%20is%20shown%20that%20a%20model%2C%20pre-trained%20on%20the%20Logos%20dataset%20can%20be%20used%20as%20a%20universal%20encoder%20for%20other%20language%20SLR%20tasks%2C%20including%20few-shot%20learning.%20We%20explore%20cross-language%20transfer%20learning%20approaches%20and%20find%20that%20joint%20training%20using%20multiple%20classification%20heads%20benefits%20accuracy%20for%20the%20target%20low-resource%20datasets%20the%20most.%20The%20key%20feature%20of%20the%20Logos%20dataset%20is%20explicitly%20annotated%20visually%20similar%20sign%20groups.%20We%20show%20that%20explicitly%20labeling%20visually%20similar%20signs%20improves%20trained%20model%20quality%20as%20a%20visual%20encoder%20for%20downstream%20tasks.%20Based%20on%20the%20proposed%20contributions%2C%20we%20outperform%20current%20state-of-the-art%20results%20for%20the%20WLASL%20dataset%20and%20get%20competitive%20results%20for%20the%20AUTSL%20dataset%2C%20with%20a%20single%20stream%20model%20processing%20solely%20RGB%20video.%20The%20source%20code%2C%20dataset%2C%20and%20pre-trained%20models%20are%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2505.10481v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogos%2520as%2520a%2520Well-Tempered%2520Pre-train%2520for%2520Sign%2520Language%2520Recognition%26entry.906535625%3DIlya%2520Ovodov%2520and%2520Petr%2520Surovtsev%2520and%2520Karina%2520Kvanchiani%2520and%2520Alexander%2520Kapitanov%2520and%2520Alexander%2520Nagaev%26entry.1292438233%3DThis%2520paper%2520examines%2520two%2520aspects%2520of%2520the%2520isolated%2520sign%2520language%2520recognition%2520%2528ISLR%2529%2520task.%2520First%252C%2520although%2520a%2520certain%2520number%2520of%2520datasets%2520is%2520available%252C%2520the%2520data%2520for%2520individual%2520sign%2520languages%2520is%2520limited.%2520It%2520poses%2520the%2520challenge%2520of%2520cross-language%2520ISLR%2520model%2520training%252C%2520including%2520transfer%2520learning.%2520Second%252C%2520similar%2520signs%2520can%2520have%2520different%2520semantic%2520meanings.%2520It%2520leads%2520to%2520ambiguity%2520in%2520dataset%2520labeling%2520and%2520raises%2520the%2520question%2520of%2520the%2520best%2520policy%2520for%2520annotating%2520such%2520signs.%2520To%2520address%2520these%2520issues%252C%2520this%2520study%2520presents%2520Logos%252C%2520a%2520novel%2520Russian%2520Sign%2520Language%2520%2528RSL%2529%2520dataset%252C%2520the%2520most%2520extensive%2520available%2520ISLR%2520dataset%2520by%2520the%2520number%2520of%2520signers%252C%2520one%2520of%2520the%2520most%2520extensive%2520datasets%2520in%2520size%2520and%2520vocabulary%252C%2520and%2520the%2520largest%2520RSL%2520dataset.%2520It%2520is%2520shown%2520that%2520a%2520model%252C%2520pre-trained%2520on%2520the%2520Logos%2520dataset%2520can%2520be%2520used%2520as%2520a%2520universal%2520encoder%2520for%2520other%2520language%2520SLR%2520tasks%252C%2520including%2520few-shot%2520learning.%2520We%2520explore%2520cross-language%2520transfer%2520learning%2520approaches%2520and%2520find%2520that%2520joint%2520training%2520using%2520multiple%2520classification%2520heads%2520benefits%2520accuracy%2520for%2520the%2520target%2520low-resource%2520datasets%2520the%2520most.%2520The%2520key%2520feature%2520of%2520the%2520Logos%2520dataset%2520is%2520explicitly%2520annotated%2520visually%2520similar%2520sign%2520groups.%2520We%2520show%2520that%2520explicitly%2520labeling%2520visually%2520similar%2520signs%2520improves%2520trained%2520model%2520quality%2520as%2520a%2520visual%2520encoder%2520for%2520downstream%2520tasks.%2520Based%2520on%2520the%2520proposed%2520contributions%252C%2520we%2520outperform%2520current%2520state-of-the-art%2520results%2520for%2520the%2520WLASL%2520dataset%2520and%2520get%2520competitive%2520results%2520for%2520the%2520AUTSL%2520dataset%252C%2520with%2520a%2520single%2520stream%2520model%2520processing%2520solely%2520RGB%2520video.%2520The%2520source%2520code%252C%2520dataset%252C%2520and%2520pre-trained%2520models%2520are%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10481v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logos%20as%20a%20Well-Tempered%20Pre-train%20for%20Sign%20Language%20Recognition&entry.906535625=Ilya%20Ovodov%20and%20Petr%20Surovtsev%20and%20Karina%20Kvanchiani%20and%20Alexander%20Kapitanov%20and%20Alexander%20Nagaev&entry.1292438233=This%20paper%20examines%20two%20aspects%20of%20the%20isolated%20sign%20language%20recognition%20%28ISLR%29%20task.%20First%2C%20although%20a%20certain%20number%20of%20datasets%20is%20available%2C%20the%20data%20for%20individual%20sign%20languages%20is%20limited.%20It%20poses%20the%20challenge%20of%20cross-language%20ISLR%20model%20training%2C%20including%20transfer%20learning.%20Second%2C%20similar%20signs%20can%20have%20different%20semantic%20meanings.%20It%20leads%20to%20ambiguity%20in%20dataset%20labeling%20and%20raises%20the%20question%20of%20the%20best%20policy%20for%20annotating%20such%20signs.%20To%20address%20these%20issues%2C%20this%20study%20presents%20Logos%2C%20a%20novel%20Russian%20Sign%20Language%20%28RSL%29%20dataset%2C%20the%20most%20extensive%20available%20ISLR%20dataset%20by%20the%20number%20of%20signers%2C%20one%20of%20the%20most%20extensive%20datasets%20in%20size%20and%20vocabulary%2C%20and%20the%20largest%20RSL%20dataset.%20It%20is%20shown%20that%20a%20model%2C%20pre-trained%20on%20the%20Logos%20dataset%20can%20be%20used%20as%20a%20universal%20encoder%20for%20other%20language%20SLR%20tasks%2C%20including%20few-shot%20learning.%20We%20explore%20cross-language%20transfer%20learning%20approaches%20and%20find%20that%20joint%20training%20using%20multiple%20classification%20heads%20benefits%20accuracy%20for%20the%20target%20low-resource%20datasets%20the%20most.%20The%20key%20feature%20of%20the%20Logos%20dataset%20is%20explicitly%20annotated%20visually%20similar%20sign%20groups.%20We%20show%20that%20explicitly%20labeling%20visually%20similar%20signs%20improves%20trained%20model%20quality%20as%20a%20visual%20encoder%20for%20downstream%20tasks.%20Based%20on%20the%20proposed%20contributions%2C%20we%20outperform%20current%20state-of-the-art%20results%20for%20the%20WLASL%20dataset%20and%20get%20competitive%20results%20for%20the%20AUTSL%20dataset%2C%20with%20a%20single%20stream%20model%20processing%20solely%20RGB%20video.%20The%20source%20code%2C%20dataset%2C%20and%20pre-trained%20models%20are%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2505.10481v2&entry.124074799=Read"},
{"title": "Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference", "author": "Alexey Nekrasov and Ali Athar and Daan de Geus and Alexander Hermans and Bastian Leibe", "abstract": "Sa2VA is a recent model for language-guided dense grounding in images and video that achieves state-of-the-art results on multiple segmentation benchmarks and that has become widely popular. However, we found that Sa2VA does not perform according to its full potential for referring video object segmentation tasks. We identify inconsistencies between training and inference procedures as the key factor holding it back. To mitigate this issue, we propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and improves the results. In fact, Sa2VA-i sets a new state of the art for multiple video benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the original Sa2VA-26B model on the MeViS benchmark. We hope that this work will show the importance of seemingly trivial implementation details and that it will provide valuable insights for the referring video segmentation field. We provide the code and updated models at https://github.com/kumuji/sa2va-i", "link": "http://arxiv.org/abs/2509.19082v2", "date": "2025-11-18", "relevancy": 2.7071, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.546}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sa2VA-i%3A%20Improving%20Sa2VA%20Results%20with%20Consistent%20Training%20and%20Inference&body=Title%3A%20Sa2VA-i%3A%20Improving%20Sa2VA%20Results%20with%20Consistent%20Training%20and%20Inference%0AAuthor%3A%20Alexey%20Nekrasov%20and%20Ali%20Athar%20and%20Daan%20de%20Geus%20and%20Alexander%20Hermans%20and%20Bastian%20Leibe%0AAbstract%3A%20Sa2VA%20is%20a%20recent%20model%20for%20language-guided%20dense%20grounding%20in%20images%20and%20video%20that%20achieves%20state-of-the-art%20results%20on%20multiple%20segmentation%20benchmarks%20and%20that%20has%20become%20widely%20popular.%20However%2C%20we%20found%20that%20Sa2VA%20does%20not%20perform%20according%20to%20its%20full%20potential%20for%20referring%20video%20object%20segmentation%20tasks.%20We%20identify%20inconsistencies%20between%20training%20and%20inference%20procedures%20as%20the%20key%20factor%20holding%20it%20back.%20To%20mitigate%20this%20issue%2C%20we%20propose%20an%20improved%20version%20of%20Sa2VA%2C%20Sa2VA-i%2C%20that%20rectifies%20these%20issues%20and%20improves%20the%20results.%20In%20fact%2C%20Sa2VA-i%20sets%20a%20new%20state%20of%20the%20art%20for%20multiple%20video%20benchmarks%20and%20achieves%20improvements%20of%20up%20to%20%2B11.6%20J%26F%20on%20MeViS%2C%20%2B1.4%20on%20Ref-YT-VOS%2C%20%2B3.3%20on%20Ref-DAVIS%20and%20%2B4.1%20on%20ReVOS%20using%20the%20same%20Sa2VA%20checkpoints.%20With%20our%20fixes%2C%20the%20Sa2VA-i-1B%20model%20even%20performs%20on%20par%20with%20the%20original%20Sa2VA-26B%20model%20on%20the%20MeViS%20benchmark.%20We%20hope%20that%20this%20work%20will%20show%20the%20importance%20of%20seemingly%20trivial%20implementation%20details%20and%20that%20it%20will%20provide%20valuable%20insights%20for%20the%20referring%20video%20segmentation%20field.%20We%20provide%20the%20code%20and%20updated%20models%20at%20https%3A//github.com/kumuji/sa2va-i%0ALink%3A%20http%3A//arxiv.org/abs/2509.19082v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSa2VA-i%253A%2520Improving%2520Sa2VA%2520Results%2520with%2520Consistent%2520Training%2520and%2520Inference%26entry.906535625%3DAlexey%2520Nekrasov%2520and%2520Ali%2520Athar%2520and%2520Daan%2520de%2520Geus%2520and%2520Alexander%2520Hermans%2520and%2520Bastian%2520Leibe%26entry.1292438233%3DSa2VA%2520is%2520a%2520recent%2520model%2520for%2520language-guided%2520dense%2520grounding%2520in%2520images%2520and%2520video%2520that%2520achieves%2520state-of-the-art%2520results%2520on%2520multiple%2520segmentation%2520benchmarks%2520and%2520that%2520has%2520become%2520widely%2520popular.%2520However%252C%2520we%2520found%2520that%2520Sa2VA%2520does%2520not%2520perform%2520according%2520to%2520its%2520full%2520potential%2520for%2520referring%2520video%2520object%2520segmentation%2520tasks.%2520We%2520identify%2520inconsistencies%2520between%2520training%2520and%2520inference%2520procedures%2520as%2520the%2520key%2520factor%2520holding%2520it%2520back.%2520To%2520mitigate%2520this%2520issue%252C%2520we%2520propose%2520an%2520improved%2520version%2520of%2520Sa2VA%252C%2520Sa2VA-i%252C%2520that%2520rectifies%2520these%2520issues%2520and%2520improves%2520the%2520results.%2520In%2520fact%252C%2520Sa2VA-i%2520sets%2520a%2520new%2520state%2520of%2520the%2520art%2520for%2520multiple%2520video%2520benchmarks%2520and%2520achieves%2520improvements%2520of%2520up%2520to%2520%252B11.6%2520J%2526F%2520on%2520MeViS%252C%2520%252B1.4%2520on%2520Ref-YT-VOS%252C%2520%252B3.3%2520on%2520Ref-DAVIS%2520and%2520%252B4.1%2520on%2520ReVOS%2520using%2520the%2520same%2520Sa2VA%2520checkpoints.%2520With%2520our%2520fixes%252C%2520the%2520Sa2VA-i-1B%2520model%2520even%2520performs%2520on%2520par%2520with%2520the%2520original%2520Sa2VA-26B%2520model%2520on%2520the%2520MeViS%2520benchmark.%2520We%2520hope%2520that%2520this%2520work%2520will%2520show%2520the%2520importance%2520of%2520seemingly%2520trivial%2520implementation%2520details%2520and%2520that%2520it%2520will%2520provide%2520valuable%2520insights%2520for%2520the%2520referring%2520video%2520segmentation%2520field.%2520We%2520provide%2520the%2520code%2520and%2520updated%2520models%2520at%2520https%253A//github.com/kumuji/sa2va-i%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19082v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sa2VA-i%3A%20Improving%20Sa2VA%20Results%20with%20Consistent%20Training%20and%20Inference&entry.906535625=Alexey%20Nekrasov%20and%20Ali%20Athar%20and%20Daan%20de%20Geus%20and%20Alexander%20Hermans%20and%20Bastian%20Leibe&entry.1292438233=Sa2VA%20is%20a%20recent%20model%20for%20language-guided%20dense%20grounding%20in%20images%20and%20video%20that%20achieves%20state-of-the-art%20results%20on%20multiple%20segmentation%20benchmarks%20and%20that%20has%20become%20widely%20popular.%20However%2C%20we%20found%20that%20Sa2VA%20does%20not%20perform%20according%20to%20its%20full%20potential%20for%20referring%20video%20object%20segmentation%20tasks.%20We%20identify%20inconsistencies%20between%20training%20and%20inference%20procedures%20as%20the%20key%20factor%20holding%20it%20back.%20To%20mitigate%20this%20issue%2C%20we%20propose%20an%20improved%20version%20of%20Sa2VA%2C%20Sa2VA-i%2C%20that%20rectifies%20these%20issues%20and%20improves%20the%20results.%20In%20fact%2C%20Sa2VA-i%20sets%20a%20new%20state%20of%20the%20art%20for%20multiple%20video%20benchmarks%20and%20achieves%20improvements%20of%20up%20to%20%2B11.6%20J%26F%20on%20MeViS%2C%20%2B1.4%20on%20Ref-YT-VOS%2C%20%2B3.3%20on%20Ref-DAVIS%20and%20%2B4.1%20on%20ReVOS%20using%20the%20same%20Sa2VA%20checkpoints.%20With%20our%20fixes%2C%20the%20Sa2VA-i-1B%20model%20even%20performs%20on%20par%20with%20the%20original%20Sa2VA-26B%20model%20on%20the%20MeViS%20benchmark.%20We%20hope%20that%20this%20work%20will%20show%20the%20importance%20of%20seemingly%20trivial%20implementation%20details%20and%20that%20it%20will%20provide%20valuable%20insights%20for%20the%20referring%20video%20segmentation%20field.%20We%20provide%20the%20code%20and%20updated%20models%20at%20https%3A//github.com/kumuji/sa2va-i&entry.1838667208=http%3A//arxiv.org/abs/2509.19082v2&entry.124074799=Read"},
{"title": "RelTopo: Multi-Level Relational Modeling for Driving Scene Topology Reasoning", "author": "Yueru Luo and Changqing Zhou and Yiming Yang and Erlong Li and Chao Zheng and Shuqi Mei and Shuguang Cui and Zhen Li", "abstract": "Accurate road topology reasoning is critical for autonomous driving, as it requires both perceiving road elements and understanding how lanes connect to each other (L2L) and to traffic elements (L2T). Existing methods often focus on either perception or L2L reasoning, leaving L2T underexplored and fall short of jointly optimizing perception and reasoning. Moreover, although topology prediction inherently involves relations, relational modeling itself is seldom incorporated into feature extraction or supervision. As humans naturally leverage contextual relationships to recognize road element and infer their connectivity, we posit that relational modeling can likewise benefit both perception and reasoning, and that these two tasks should be mutually enhancing. To this end, we propose RelTopo, a multi-level relational modeling approach that systematically integrates relational cues across three levels: 1) perception-level: a relation-aware lane detector with geometry-biased self-attention and curve-guided cross-attention enriches lane representations; 2) reasoning-level: relation-enhanced topology heads, including a geometry-enhanced L2L head and a cross-view L2T head, enhance topology inference via relational cues; and 3) supervision-level: a contrastive InfoNCE strategy regularizes relational embeddings. This design enables perception and reasoning to be optimized jointly. Extensive experiments on OpenLane-V2 demonstrate that RelTopo significantly improves both detection and topology reasoning, with gains of +3.1 in DET$_l$, +5.3 in TOP$_{ll}$, +4.9 in TOP$_{lt}$, and +4.4 overall in OLS, setting a new state-of-the-art. Code will be released.", "link": "http://arxiv.org/abs/2506.13553v3", "date": "2025-11-18", "relevancy": 2.6845, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5411}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5411}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RelTopo%3A%20Multi-Level%20Relational%20Modeling%20for%20Driving%20Scene%20Topology%20Reasoning&body=Title%3A%20RelTopo%3A%20Multi-Level%20Relational%20Modeling%20for%20Driving%20Scene%20Topology%20Reasoning%0AAuthor%3A%20Yueru%20Luo%20and%20Changqing%20Zhou%20and%20Yiming%20Yang%20and%20Erlong%20Li%20and%20Chao%20Zheng%20and%20Shuqi%20Mei%20and%20Shuguang%20Cui%20and%20Zhen%20Li%0AAbstract%3A%20Accurate%20road%20topology%20reasoning%20is%20critical%20for%20autonomous%20driving%2C%20as%20it%20requires%20both%20perceiving%20road%20elements%20and%20understanding%20how%20lanes%20connect%20to%20each%20other%20%28L2L%29%20and%20to%20traffic%20elements%20%28L2T%29.%20Existing%20methods%20often%20focus%20on%20either%20perception%20or%20L2L%20reasoning%2C%20leaving%20L2T%20underexplored%20and%20fall%20short%20of%20jointly%20optimizing%20perception%20and%20reasoning.%20Moreover%2C%20although%20topology%20prediction%20inherently%20involves%20relations%2C%20relational%20modeling%20itself%20is%20seldom%20incorporated%20into%20feature%20extraction%20or%20supervision.%20As%20humans%20naturally%20leverage%20contextual%20relationships%20to%20recognize%20road%20element%20and%20infer%20their%20connectivity%2C%20we%20posit%20that%20relational%20modeling%20can%20likewise%20benefit%20both%20perception%20and%20reasoning%2C%20and%20that%20these%20two%20tasks%20should%20be%20mutually%20enhancing.%20To%20this%20end%2C%20we%20propose%20RelTopo%2C%20a%20multi-level%20relational%20modeling%20approach%20that%20systematically%20integrates%20relational%20cues%20across%20three%20levels%3A%201%29%20perception-level%3A%20a%20relation-aware%20lane%20detector%20with%20geometry-biased%20self-attention%20and%20curve-guided%20cross-attention%20enriches%20lane%20representations%3B%202%29%20reasoning-level%3A%20relation-enhanced%20topology%20heads%2C%20including%20a%20geometry-enhanced%20L2L%20head%20and%20a%20cross-view%20L2T%20head%2C%20enhance%20topology%20inference%20via%20relational%20cues%3B%20and%203%29%20supervision-level%3A%20a%20contrastive%20InfoNCE%20strategy%20regularizes%20relational%20embeddings.%20This%20design%20enables%20perception%20and%20reasoning%20to%20be%20optimized%20jointly.%20Extensive%20experiments%20on%20OpenLane-V2%20demonstrate%20that%20RelTopo%20significantly%20improves%20both%20detection%20and%20topology%20reasoning%2C%20with%20gains%20of%20%2B3.1%20in%20DET%24_l%24%2C%20%2B5.3%20in%20TOP%24_%7Bll%7D%24%2C%20%2B4.9%20in%20TOP%24_%7Blt%7D%24%2C%20and%20%2B4.4%20overall%20in%20OLS%2C%20setting%20a%20new%20state-of-the-art.%20Code%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2506.13553v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelTopo%253A%2520Multi-Level%2520Relational%2520Modeling%2520for%2520Driving%2520Scene%2520Topology%2520Reasoning%26entry.906535625%3DYueru%2520Luo%2520and%2520Changqing%2520Zhou%2520and%2520Yiming%2520Yang%2520and%2520Erlong%2520Li%2520and%2520Chao%2520Zheng%2520and%2520Shuqi%2520Mei%2520and%2520Shuguang%2520Cui%2520and%2520Zhen%2520Li%26entry.1292438233%3DAccurate%2520road%2520topology%2520reasoning%2520is%2520critical%2520for%2520autonomous%2520driving%252C%2520as%2520it%2520requires%2520both%2520perceiving%2520road%2520elements%2520and%2520understanding%2520how%2520lanes%2520connect%2520to%2520each%2520other%2520%2528L2L%2529%2520and%2520to%2520traffic%2520elements%2520%2528L2T%2529.%2520Existing%2520methods%2520often%2520focus%2520on%2520either%2520perception%2520or%2520L2L%2520reasoning%252C%2520leaving%2520L2T%2520underexplored%2520and%2520fall%2520short%2520of%2520jointly%2520optimizing%2520perception%2520and%2520reasoning.%2520Moreover%252C%2520although%2520topology%2520prediction%2520inherently%2520involves%2520relations%252C%2520relational%2520modeling%2520itself%2520is%2520seldom%2520incorporated%2520into%2520feature%2520extraction%2520or%2520supervision.%2520As%2520humans%2520naturally%2520leverage%2520contextual%2520relationships%2520to%2520recognize%2520road%2520element%2520and%2520infer%2520their%2520connectivity%252C%2520we%2520posit%2520that%2520relational%2520modeling%2520can%2520likewise%2520benefit%2520both%2520perception%2520and%2520reasoning%252C%2520and%2520that%2520these%2520two%2520tasks%2520should%2520be%2520mutually%2520enhancing.%2520To%2520this%2520end%252C%2520we%2520propose%2520RelTopo%252C%2520a%2520multi-level%2520relational%2520modeling%2520approach%2520that%2520systematically%2520integrates%2520relational%2520cues%2520across%2520three%2520levels%253A%25201%2529%2520perception-level%253A%2520a%2520relation-aware%2520lane%2520detector%2520with%2520geometry-biased%2520self-attention%2520and%2520curve-guided%2520cross-attention%2520enriches%2520lane%2520representations%253B%25202%2529%2520reasoning-level%253A%2520relation-enhanced%2520topology%2520heads%252C%2520including%2520a%2520geometry-enhanced%2520L2L%2520head%2520and%2520a%2520cross-view%2520L2T%2520head%252C%2520enhance%2520topology%2520inference%2520via%2520relational%2520cues%253B%2520and%25203%2529%2520supervision-level%253A%2520a%2520contrastive%2520InfoNCE%2520strategy%2520regularizes%2520relational%2520embeddings.%2520This%2520design%2520enables%2520perception%2520and%2520reasoning%2520to%2520be%2520optimized%2520jointly.%2520Extensive%2520experiments%2520on%2520OpenLane-V2%2520demonstrate%2520that%2520RelTopo%2520significantly%2520improves%2520both%2520detection%2520and%2520topology%2520reasoning%252C%2520with%2520gains%2520of%2520%252B3.1%2520in%2520DET%2524_l%2524%252C%2520%252B5.3%2520in%2520TOP%2524_%257Bll%257D%2524%252C%2520%252B4.9%2520in%2520TOP%2524_%257Blt%257D%2524%252C%2520and%2520%252B4.4%2520overall%2520in%2520OLS%252C%2520setting%2520a%2520new%2520state-of-the-art.%2520Code%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13553v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RelTopo%3A%20Multi-Level%20Relational%20Modeling%20for%20Driving%20Scene%20Topology%20Reasoning&entry.906535625=Yueru%20Luo%20and%20Changqing%20Zhou%20and%20Yiming%20Yang%20and%20Erlong%20Li%20and%20Chao%20Zheng%20and%20Shuqi%20Mei%20and%20Shuguang%20Cui%20and%20Zhen%20Li&entry.1292438233=Accurate%20road%20topology%20reasoning%20is%20critical%20for%20autonomous%20driving%2C%20as%20it%20requires%20both%20perceiving%20road%20elements%20and%20understanding%20how%20lanes%20connect%20to%20each%20other%20%28L2L%29%20and%20to%20traffic%20elements%20%28L2T%29.%20Existing%20methods%20often%20focus%20on%20either%20perception%20or%20L2L%20reasoning%2C%20leaving%20L2T%20underexplored%20and%20fall%20short%20of%20jointly%20optimizing%20perception%20and%20reasoning.%20Moreover%2C%20although%20topology%20prediction%20inherently%20involves%20relations%2C%20relational%20modeling%20itself%20is%20seldom%20incorporated%20into%20feature%20extraction%20or%20supervision.%20As%20humans%20naturally%20leverage%20contextual%20relationships%20to%20recognize%20road%20element%20and%20infer%20their%20connectivity%2C%20we%20posit%20that%20relational%20modeling%20can%20likewise%20benefit%20both%20perception%20and%20reasoning%2C%20and%20that%20these%20two%20tasks%20should%20be%20mutually%20enhancing.%20To%20this%20end%2C%20we%20propose%20RelTopo%2C%20a%20multi-level%20relational%20modeling%20approach%20that%20systematically%20integrates%20relational%20cues%20across%20three%20levels%3A%201%29%20perception-level%3A%20a%20relation-aware%20lane%20detector%20with%20geometry-biased%20self-attention%20and%20curve-guided%20cross-attention%20enriches%20lane%20representations%3B%202%29%20reasoning-level%3A%20relation-enhanced%20topology%20heads%2C%20including%20a%20geometry-enhanced%20L2L%20head%20and%20a%20cross-view%20L2T%20head%2C%20enhance%20topology%20inference%20via%20relational%20cues%3B%20and%203%29%20supervision-level%3A%20a%20contrastive%20InfoNCE%20strategy%20regularizes%20relational%20embeddings.%20This%20design%20enables%20perception%20and%20reasoning%20to%20be%20optimized%20jointly.%20Extensive%20experiments%20on%20OpenLane-V2%20demonstrate%20that%20RelTopo%20significantly%20improves%20both%20detection%20and%20topology%20reasoning%2C%20with%20gains%20of%20%2B3.1%20in%20DET%24_l%24%2C%20%2B5.3%20in%20TOP%24_%7Bll%7D%24%2C%20%2B4.9%20in%20TOP%24_%7Blt%7D%24%2C%20and%20%2B4.4%20overall%20in%20OLS%2C%20setting%20a%20new%20state-of-the-art.%20Code%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2506.13553v3&entry.124074799=Read"},
{"title": "Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN", "author": "Madhumati Pol and Anvay Anturkar and Anushka Khot and Ayush Andure and Aniruddha Ghosh and Anvit Magadum and Anvay Bahadur", "abstract": "This study investigates the performance of 3D Convolutional Neural Networks (3D CNNs) and Long Short-Term Memory (LSTM) networks for real-time American Sign Language (ASL) recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences, LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1,200 ASL signs across 50 classes, comparing their accuracy, computational efficiency, and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2% more processing time per frame compared to LSTMs, which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNNLSTM model shows decent performance, which suggests that context-dependent architecture selection is crucial for practical implementation.This project provides professional benchmarks for developing assistive technologies, highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments.", "link": "http://arxiv.org/abs/2510.13137v2", "date": "2025-11-18", "relevancy": 2.6686, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5371}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Sign%20Language%20to%20text%20Translation%20using%20Deep%20Learning%3A%20A%20Comparative%20study%20of%20LSTM%20and%203D%20CNN&body=Title%3A%20Real-Time%20Sign%20Language%20to%20text%20Translation%20using%20Deep%20Learning%3A%20A%20Comparative%20study%20of%20LSTM%20and%203D%20CNN%0AAuthor%3A%20Madhumati%20Pol%20and%20Anvay%20Anturkar%20and%20Anushka%20Khot%20and%20Ayush%20Andure%20and%20Aniruddha%20Ghosh%20and%20Anvit%20Magadum%20and%20Anvay%20Bahadur%0AAbstract%3A%20This%20study%20investigates%20the%20performance%20of%203D%20Convolutional%20Neural%20Networks%20%283D%20CNNs%29%20and%20Long%20Short-Term%20Memory%20%28LSTM%29%20networks%20for%20real-time%20American%20Sign%20Language%20%28ASL%29%20recognition.%20Though%203D%20CNNs%20are%20good%20at%20spatiotemporal%20feature%20extraction%20from%20video%20sequences%2C%20LSTMs%20are%20optimized%20for%20modeling%20temporal%20dependencies%20in%20sequential%20data.%20We%20evaluate%20both%20architectures%20on%20a%20dataset%20containing%201%2C200%20ASL%20signs%20across%2050%20classes%2C%20comparing%20their%20accuracy%2C%20computational%20efficiency%2C%20and%20latency%20under%20similar%20training%20conditions.%20Experimental%20results%20demonstrate%20that%203D%20CNNs%20achieve%2092.4%25%20recognition%20accuracy%20but%20require%203.2%25%20more%20processing%20time%20per%20frame%20compared%20to%20LSTMs%2C%20which%20maintain%2086.7%25%20accuracy%20with%20significantly%20lower%20resource%20consumption.%20The%20hybrid%203D%20CNNLSTM%20model%20shows%20decent%20performance%2C%20which%20suggests%20that%20context-dependent%20architecture%20selection%20is%20crucial%20for%20practical%20implementation.This%20project%20provides%20professional%20benchmarks%20for%20developing%20assistive%20technologies%2C%20highlighting%20trade-offs%20between%20recognition%20precision%20and%20real-time%20operational%20requirements%20in%20edge%20computing%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2510.13137v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Sign%2520Language%2520to%2520text%2520Translation%2520using%2520Deep%2520Learning%253A%2520A%2520Comparative%2520study%2520of%2520LSTM%2520and%25203D%2520CNN%26entry.906535625%3DMadhumati%2520Pol%2520and%2520Anvay%2520Anturkar%2520and%2520Anushka%2520Khot%2520and%2520Ayush%2520Andure%2520and%2520Aniruddha%2520Ghosh%2520and%2520Anvit%2520Magadum%2520and%2520Anvay%2520Bahadur%26entry.1292438233%3DThis%2520study%2520investigates%2520the%2520performance%2520of%25203D%2520Convolutional%2520Neural%2520Networks%2520%25283D%2520CNNs%2529%2520and%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520networks%2520for%2520real-time%2520American%2520Sign%2520Language%2520%2528ASL%2529%2520recognition.%2520Though%25203D%2520CNNs%2520are%2520good%2520at%2520spatiotemporal%2520feature%2520extraction%2520from%2520video%2520sequences%252C%2520LSTMs%2520are%2520optimized%2520for%2520modeling%2520temporal%2520dependencies%2520in%2520sequential%2520data.%2520We%2520evaluate%2520both%2520architectures%2520on%2520a%2520dataset%2520containing%25201%252C200%2520ASL%2520signs%2520across%252050%2520classes%252C%2520comparing%2520their%2520accuracy%252C%2520computational%2520efficiency%252C%2520and%2520latency%2520under%2520similar%2520training%2520conditions.%2520Experimental%2520results%2520demonstrate%2520that%25203D%2520CNNs%2520achieve%252092.4%2525%2520recognition%2520accuracy%2520but%2520require%25203.2%2525%2520more%2520processing%2520time%2520per%2520frame%2520compared%2520to%2520LSTMs%252C%2520which%2520maintain%252086.7%2525%2520accuracy%2520with%2520significantly%2520lower%2520resource%2520consumption.%2520The%2520hybrid%25203D%2520CNNLSTM%2520model%2520shows%2520decent%2520performance%252C%2520which%2520suggests%2520that%2520context-dependent%2520architecture%2520selection%2520is%2520crucial%2520for%2520practical%2520implementation.This%2520project%2520provides%2520professional%2520benchmarks%2520for%2520developing%2520assistive%2520technologies%252C%2520highlighting%2520trade-offs%2520between%2520recognition%2520precision%2520and%2520real-time%2520operational%2520requirements%2520in%2520edge%2520computing%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13137v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Sign%20Language%20to%20text%20Translation%20using%20Deep%20Learning%3A%20A%20Comparative%20study%20of%20LSTM%20and%203D%20CNN&entry.906535625=Madhumati%20Pol%20and%20Anvay%20Anturkar%20and%20Anushka%20Khot%20and%20Ayush%20Andure%20and%20Aniruddha%20Ghosh%20and%20Anvit%20Magadum%20and%20Anvay%20Bahadur&entry.1292438233=This%20study%20investigates%20the%20performance%20of%203D%20Convolutional%20Neural%20Networks%20%283D%20CNNs%29%20and%20Long%20Short-Term%20Memory%20%28LSTM%29%20networks%20for%20real-time%20American%20Sign%20Language%20%28ASL%29%20recognition.%20Though%203D%20CNNs%20are%20good%20at%20spatiotemporal%20feature%20extraction%20from%20video%20sequences%2C%20LSTMs%20are%20optimized%20for%20modeling%20temporal%20dependencies%20in%20sequential%20data.%20We%20evaluate%20both%20architectures%20on%20a%20dataset%20containing%201%2C200%20ASL%20signs%20across%2050%20classes%2C%20comparing%20their%20accuracy%2C%20computational%20efficiency%2C%20and%20latency%20under%20similar%20training%20conditions.%20Experimental%20results%20demonstrate%20that%203D%20CNNs%20achieve%2092.4%25%20recognition%20accuracy%20but%20require%203.2%25%20more%20processing%20time%20per%20frame%20compared%20to%20LSTMs%2C%20which%20maintain%2086.7%25%20accuracy%20with%20significantly%20lower%20resource%20consumption.%20The%20hybrid%203D%20CNNLSTM%20model%20shows%20decent%20performance%2C%20which%20suggests%20that%20context-dependent%20architecture%20selection%20is%20crucial%20for%20practical%20implementation.This%20project%20provides%20professional%20benchmarks%20for%20developing%20assistive%20technologies%2C%20highlighting%20trade-offs%20between%20recognition%20precision%20and%20real-time%20operational%20requirements%20in%20edge%20computing%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2510.13137v2&entry.124074799=Read"},
{"title": "Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers", "author": "Yutian Chen and Yuheng Qiu and Ruogu Li and Ali Agha and Shayegan Omidshafiei and Jay Patrikar and Sebastian Scherer", "abstract": "We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\\times$ and $7.2\\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction.", "link": "http://arxiv.org/abs/2511.14751v1", "date": "2025-11-18", "relevancy": 2.6612, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5368}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5331}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Co-Me%3A%20Confidence-Guided%20Token%20Merging%20for%20Visual%20Geometric%20Transformers&body=Title%3A%20Co-Me%3A%20Confidence-Guided%20Token%20Merging%20for%20Visual%20Geometric%20Transformers%0AAuthor%3A%20Yutian%20Chen%20and%20Yuheng%20Qiu%20and%20Ruogu%20Li%20and%20Ali%20Agha%20and%20Shayegan%20Omidshafiei%20and%20Jay%20Patrikar%20and%20Sebastian%20Scherer%0AAbstract%3A%20We%20propose%20Confidence-Guided%20Token%20Merging%20%28Co-Me%29%2C%20an%20acceleration%20mechanism%20for%20visual%20geometric%20transformers%20without%20retraining%20or%20finetuning%20the%20base%20model.%20Co-Me%20distilled%20a%20light-weight%20confidence%20predictor%20to%20rank%20tokens%20by%20uncertainty%20and%20selectively%20merge%20low-confidence%20ones%2C%20effectively%20reducing%20computation%20while%20maintaining%20spatial%20coverage.%20Compared%20to%20similarity-based%20merging%20or%20pruning%2C%20the%20confidence%20signal%20in%20Co-Me%20reliably%20indicates%20regions%20emphasized%20by%20the%20transformer%2C%20enabling%20substantial%20acceleration%20without%20degrading%20performance.%20Co-Me%20applies%20seamlessly%20to%20various%20multi-view%20and%20streaming%20visual%20geometric%20transformers%2C%20achieving%20speedups%20that%20scale%20with%20sequence%20length.%20When%20applied%20to%20VGGT%20and%20MapAnything%2C%20Co-Me%20achieves%20up%20to%20%2411.3%5Ctimes%24%20and%20%247.2%5Ctimes%24%20speedup%2C%20making%20visual%20geometric%20transformers%20practical%20for%20real-time%203D%20perception%20and%20reconstruction.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCo-Me%253A%2520Confidence-Guided%2520Token%2520Merging%2520for%2520Visual%2520Geometric%2520Transformers%26entry.906535625%3DYutian%2520Chen%2520and%2520Yuheng%2520Qiu%2520and%2520Ruogu%2520Li%2520and%2520Ali%2520Agha%2520and%2520Shayegan%2520Omidshafiei%2520and%2520Jay%2520Patrikar%2520and%2520Sebastian%2520Scherer%26entry.1292438233%3DWe%2520propose%2520Confidence-Guided%2520Token%2520Merging%2520%2528Co-Me%2529%252C%2520an%2520acceleration%2520mechanism%2520for%2520visual%2520geometric%2520transformers%2520without%2520retraining%2520or%2520finetuning%2520the%2520base%2520model.%2520Co-Me%2520distilled%2520a%2520light-weight%2520confidence%2520predictor%2520to%2520rank%2520tokens%2520by%2520uncertainty%2520and%2520selectively%2520merge%2520low-confidence%2520ones%252C%2520effectively%2520reducing%2520computation%2520while%2520maintaining%2520spatial%2520coverage.%2520Compared%2520to%2520similarity-based%2520merging%2520or%2520pruning%252C%2520the%2520confidence%2520signal%2520in%2520Co-Me%2520reliably%2520indicates%2520regions%2520emphasized%2520by%2520the%2520transformer%252C%2520enabling%2520substantial%2520acceleration%2520without%2520degrading%2520performance.%2520Co-Me%2520applies%2520seamlessly%2520to%2520various%2520multi-view%2520and%2520streaming%2520visual%2520geometric%2520transformers%252C%2520achieving%2520speedups%2520that%2520scale%2520with%2520sequence%2520length.%2520When%2520applied%2520to%2520VGGT%2520and%2520MapAnything%252C%2520Co-Me%2520achieves%2520up%2520to%2520%252411.3%255Ctimes%2524%2520and%2520%25247.2%255Ctimes%2524%2520speedup%252C%2520making%2520visual%2520geometric%2520transformers%2520practical%2520for%2520real-time%25203D%2520perception%2520and%2520reconstruction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-Me%3A%20Confidence-Guided%20Token%20Merging%20for%20Visual%20Geometric%20Transformers&entry.906535625=Yutian%20Chen%20and%20Yuheng%20Qiu%20and%20Ruogu%20Li%20and%20Ali%20Agha%20and%20Shayegan%20Omidshafiei%20and%20Jay%20Patrikar%20and%20Sebastian%20Scherer&entry.1292438233=We%20propose%20Confidence-Guided%20Token%20Merging%20%28Co-Me%29%2C%20an%20acceleration%20mechanism%20for%20visual%20geometric%20transformers%20without%20retraining%20or%20finetuning%20the%20base%20model.%20Co-Me%20distilled%20a%20light-weight%20confidence%20predictor%20to%20rank%20tokens%20by%20uncertainty%20and%20selectively%20merge%20low-confidence%20ones%2C%20effectively%20reducing%20computation%20while%20maintaining%20spatial%20coverage.%20Compared%20to%20similarity-based%20merging%20or%20pruning%2C%20the%20confidence%20signal%20in%20Co-Me%20reliably%20indicates%20regions%20emphasized%20by%20the%20transformer%2C%20enabling%20substantial%20acceleration%20without%20degrading%20performance.%20Co-Me%20applies%20seamlessly%20to%20various%20multi-view%20and%20streaming%20visual%20geometric%20transformers%2C%20achieving%20speedups%20that%20scale%20with%20sequence%20length.%20When%20applied%20to%20VGGT%20and%20MapAnything%2C%20Co-Me%20achieves%20up%20to%20%2411.3%5Ctimes%24%20and%20%247.2%5Ctimes%24%20speedup%2C%20making%20visual%20geometric%20transformers%20practical%20for%20real-time%203D%20perception%20and%20reconstruction.&entry.1838667208=http%3A//arxiv.org/abs/2511.14751v1&entry.124074799=Read"},
{"title": "Foundation Models in Medical Imaging: A Review and Outlook", "author": "Vivien van Veldhuizen and Vanessa Botha and Chunyao Lu and Melis Erdal Cesur and Kevin Groot Lipman and Edwin D. de Jong and Hugo Horlings and Cl\u00e1risa I. Sanchez and Cees G. M. Snoek and Lodewyk Wessels and Ritse Mann and Eric Marcus and Jonas Teuwen", "abstract": "Foundation models (FMs) are changing the way medical images are analyzed by learning from large collections of unlabeled data. Instead of relying on manually annotated examples, FMs are pre-trained to learn general-purpose visual features that can later be adapted to specific clinical tasks with little additional supervision. In this review, we examine how FMs are being developed and applied in pathology, radiology, and ophthalmology, drawing on evidence from over 150 studies. We explain the core components of FM pipelines, including model architectures, self-supervised learning methods, and strategies for downstream adaptation. We also review how FMs are being used in each imaging domain and compare design choices across applications. Finally, we discuss key challenges and open questions to guide future research.", "link": "http://arxiv.org/abs/2506.09095v4", "date": "2025-11-18", "relevancy": 2.6558, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Models%20in%20Medical%20Imaging%3A%20A%20Review%20and%20Outlook&body=Title%3A%20Foundation%20Models%20in%20Medical%20Imaging%3A%20A%20Review%20and%20Outlook%0AAuthor%3A%20Vivien%20van%20Veldhuizen%20and%20Vanessa%20Botha%20and%20Chunyao%20Lu%20and%20Melis%20Erdal%20Cesur%20and%20Kevin%20Groot%20Lipman%20and%20Edwin%20D.%20de%20Jong%20and%20Hugo%20Horlings%20and%20Cl%C3%A1risa%20I.%20Sanchez%20and%20Cees%20G.%20M.%20Snoek%20and%20Lodewyk%20Wessels%20and%20Ritse%20Mann%20and%20Eric%20Marcus%20and%20Jonas%20Teuwen%0AAbstract%3A%20Foundation%20models%20%28FMs%29%20are%20changing%20the%20way%20medical%20images%20are%20analyzed%20by%20learning%20from%20large%20collections%20of%20unlabeled%20data.%20Instead%20of%20relying%20on%20manually%20annotated%20examples%2C%20FMs%20are%20pre-trained%20to%20learn%20general-purpose%20visual%20features%20that%20can%20later%20be%20adapted%20to%20specific%20clinical%20tasks%20with%20little%20additional%20supervision.%20In%20this%20review%2C%20we%20examine%20how%20FMs%20are%20being%20developed%20and%20applied%20in%20pathology%2C%20radiology%2C%20and%20ophthalmology%2C%20drawing%20on%20evidence%20from%20over%20150%20studies.%20We%20explain%20the%20core%20components%20of%20FM%20pipelines%2C%20including%20model%20architectures%2C%20self-supervised%20learning%20methods%2C%20and%20strategies%20for%20downstream%20adaptation.%20We%20also%20review%20how%20FMs%20are%20being%20used%20in%20each%20imaging%20domain%20and%20compare%20design%20choices%20across%20applications.%20Finally%2C%20we%20discuss%20key%20challenges%20and%20open%20questions%20to%20guide%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2506.09095v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Models%2520in%2520Medical%2520Imaging%253A%2520A%2520Review%2520and%2520Outlook%26entry.906535625%3DVivien%2520van%2520Veldhuizen%2520and%2520Vanessa%2520Botha%2520and%2520Chunyao%2520Lu%2520and%2520Melis%2520Erdal%2520Cesur%2520and%2520Kevin%2520Groot%2520Lipman%2520and%2520Edwin%2520D.%2520de%2520Jong%2520and%2520Hugo%2520Horlings%2520and%2520Cl%25C3%25A1risa%2520I.%2520Sanchez%2520and%2520Cees%2520G.%2520M.%2520Snoek%2520and%2520Lodewyk%2520Wessels%2520and%2520Ritse%2520Mann%2520and%2520Eric%2520Marcus%2520and%2520Jonas%2520Teuwen%26entry.1292438233%3DFoundation%2520models%2520%2528FMs%2529%2520are%2520changing%2520the%2520way%2520medical%2520images%2520are%2520analyzed%2520by%2520learning%2520from%2520large%2520collections%2520of%2520unlabeled%2520data.%2520Instead%2520of%2520relying%2520on%2520manually%2520annotated%2520examples%252C%2520FMs%2520are%2520pre-trained%2520to%2520learn%2520general-purpose%2520visual%2520features%2520that%2520can%2520later%2520be%2520adapted%2520to%2520specific%2520clinical%2520tasks%2520with%2520little%2520additional%2520supervision.%2520In%2520this%2520review%252C%2520we%2520examine%2520how%2520FMs%2520are%2520being%2520developed%2520and%2520applied%2520in%2520pathology%252C%2520radiology%252C%2520and%2520ophthalmology%252C%2520drawing%2520on%2520evidence%2520from%2520over%2520150%2520studies.%2520We%2520explain%2520the%2520core%2520components%2520of%2520FM%2520pipelines%252C%2520including%2520model%2520architectures%252C%2520self-supervised%2520learning%2520methods%252C%2520and%2520strategies%2520for%2520downstream%2520adaptation.%2520We%2520also%2520review%2520how%2520FMs%2520are%2520being%2520used%2520in%2520each%2520imaging%2520domain%2520and%2520compare%2520design%2520choices%2520across%2520applications.%2520Finally%252C%2520we%2520discuss%2520key%2520challenges%2520and%2520open%2520questions%2520to%2520guide%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09095v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Models%20in%20Medical%20Imaging%3A%20A%20Review%20and%20Outlook&entry.906535625=Vivien%20van%20Veldhuizen%20and%20Vanessa%20Botha%20and%20Chunyao%20Lu%20and%20Melis%20Erdal%20Cesur%20and%20Kevin%20Groot%20Lipman%20and%20Edwin%20D.%20de%20Jong%20and%20Hugo%20Horlings%20and%20Cl%C3%A1risa%20I.%20Sanchez%20and%20Cees%20G.%20M.%20Snoek%20and%20Lodewyk%20Wessels%20and%20Ritse%20Mann%20and%20Eric%20Marcus%20and%20Jonas%20Teuwen&entry.1292438233=Foundation%20models%20%28FMs%29%20are%20changing%20the%20way%20medical%20images%20are%20analyzed%20by%20learning%20from%20large%20collections%20of%20unlabeled%20data.%20Instead%20of%20relying%20on%20manually%20annotated%20examples%2C%20FMs%20are%20pre-trained%20to%20learn%20general-purpose%20visual%20features%20that%20can%20later%20be%20adapted%20to%20specific%20clinical%20tasks%20with%20little%20additional%20supervision.%20In%20this%20review%2C%20we%20examine%20how%20FMs%20are%20being%20developed%20and%20applied%20in%20pathology%2C%20radiology%2C%20and%20ophthalmology%2C%20drawing%20on%20evidence%20from%20over%20150%20studies.%20We%20explain%20the%20core%20components%20of%20FM%20pipelines%2C%20including%20model%20architectures%2C%20self-supervised%20learning%20methods%2C%20and%20strategies%20for%20downstream%20adaptation.%20We%20also%20review%20how%20FMs%20are%20being%20used%20in%20each%20imaging%20domain%20and%20compare%20design%20choices%20across%20applications.%20Finally%2C%20we%20discuss%20key%20challenges%20and%20open%20questions%20to%20guide%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2506.09095v4&entry.124074799=Read"},
{"title": "ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries", "author": "Junfu Pu and Teng Wang and Yixiao Ge and Yuying Ge and Chen Li and Ying Shan", "abstract": "The proliferation of hour-long videos (e.g., lectures, podcasts, documentaries) has intensified demand for efficient content structuring. However, existing approaches are constrained by small-scale training with annotations that are typical short and coarse, restricting generalization to nuanced transitions in long videos. We introduce ARC-Chapter, the first large-scale video chaptering model trained on over million-level long video chapters, featuring bilingual, temporally grounded, and hierarchical chapter annotations. To achieve this goal, we curated a bilingual English-Chinese chapter dataset via a structured pipeline that unifies ASR transcripts, scene texts, visual captions into multi-level annotations, from short title to long summaries. We demonstrate clear performance improvements with data scaling, both in data volume and label intensity. Moreover, we design a new evaluation metric termed GRACE, which incorporates many-to-one segment overlaps and semantic similarity, better reflecting real-world chaptering flexibility. Extensive experiments demonstrate that ARC-Chapter establishes a new state-of-the-art by a significant margin, outperforming the previous best by 14.0% in F1 score and 11.3% in SODA score. Moreover, ARC-Chapter shows excellent transferability, improving the state-of-the-art on downstream tasks like dense video captioning on YouCook2.", "link": "http://arxiv.org/abs/2511.14349v1", "date": "2025-11-18", "relevancy": 2.609, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.531}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARC-Chapter%3A%20Structuring%20Hour-Long%20Videos%20into%20Navigable%20Chapters%20and%20Hierarchical%20Summaries&body=Title%3A%20ARC-Chapter%3A%20Structuring%20Hour-Long%20Videos%20into%20Navigable%20Chapters%20and%20Hierarchical%20Summaries%0AAuthor%3A%20Junfu%20Pu%20and%20Teng%20Wang%20and%20Yixiao%20Ge%20and%20Yuying%20Ge%20and%20Chen%20Li%20and%20Ying%20Shan%0AAbstract%3A%20The%20proliferation%20of%20hour-long%20videos%20%28e.g.%2C%20lectures%2C%20podcasts%2C%20documentaries%29%20has%20intensified%20demand%20for%20efficient%20content%20structuring.%20However%2C%20existing%20approaches%20are%20constrained%20by%20small-scale%20training%20with%20annotations%20that%20are%20typical%20short%20and%20coarse%2C%20restricting%20generalization%20to%20nuanced%20transitions%20in%20long%20videos.%20We%20introduce%20ARC-Chapter%2C%20the%20first%20large-scale%20video%20chaptering%20model%20trained%20on%20over%20million-level%20long%20video%20chapters%2C%20featuring%20bilingual%2C%20temporally%20grounded%2C%20and%20hierarchical%20chapter%20annotations.%20To%20achieve%20this%20goal%2C%20we%20curated%20a%20bilingual%20English-Chinese%20chapter%20dataset%20via%20a%20structured%20pipeline%20that%20unifies%20ASR%20transcripts%2C%20scene%20texts%2C%20visual%20captions%20into%20multi-level%20annotations%2C%20from%20short%20title%20to%20long%20summaries.%20We%20demonstrate%20clear%20performance%20improvements%20with%20data%20scaling%2C%20both%20in%20data%20volume%20and%20label%20intensity.%20Moreover%2C%20we%20design%20a%20new%20evaluation%20metric%20termed%20GRACE%2C%20which%20incorporates%20many-to-one%20segment%20overlaps%20and%20semantic%20similarity%2C%20better%20reflecting%20real-world%20chaptering%20flexibility.%20Extensive%20experiments%20demonstrate%20that%20ARC-Chapter%20establishes%20a%20new%20state-of-the-art%20by%20a%20significant%20margin%2C%20outperforming%20the%20previous%20best%20by%2014.0%25%20in%20F1%20score%20and%2011.3%25%20in%20SODA%20score.%20Moreover%2C%20ARC-Chapter%20shows%20excellent%20transferability%2C%20improving%20the%20state-of-the-art%20on%20downstream%20tasks%20like%20dense%20video%20captioning%20on%20YouCook2.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARC-Chapter%253A%2520Structuring%2520Hour-Long%2520Videos%2520into%2520Navigable%2520Chapters%2520and%2520Hierarchical%2520Summaries%26entry.906535625%3DJunfu%2520Pu%2520and%2520Teng%2520Wang%2520and%2520Yixiao%2520Ge%2520and%2520Yuying%2520Ge%2520and%2520Chen%2520Li%2520and%2520Ying%2520Shan%26entry.1292438233%3DThe%2520proliferation%2520of%2520hour-long%2520videos%2520%2528e.g.%252C%2520lectures%252C%2520podcasts%252C%2520documentaries%2529%2520has%2520intensified%2520demand%2520for%2520efficient%2520content%2520structuring.%2520However%252C%2520existing%2520approaches%2520are%2520constrained%2520by%2520small-scale%2520training%2520with%2520annotations%2520that%2520are%2520typical%2520short%2520and%2520coarse%252C%2520restricting%2520generalization%2520to%2520nuanced%2520transitions%2520in%2520long%2520videos.%2520We%2520introduce%2520ARC-Chapter%252C%2520the%2520first%2520large-scale%2520video%2520chaptering%2520model%2520trained%2520on%2520over%2520million-level%2520long%2520video%2520chapters%252C%2520featuring%2520bilingual%252C%2520temporally%2520grounded%252C%2520and%2520hierarchical%2520chapter%2520annotations.%2520To%2520achieve%2520this%2520goal%252C%2520we%2520curated%2520a%2520bilingual%2520English-Chinese%2520chapter%2520dataset%2520via%2520a%2520structured%2520pipeline%2520that%2520unifies%2520ASR%2520transcripts%252C%2520scene%2520texts%252C%2520visual%2520captions%2520into%2520multi-level%2520annotations%252C%2520from%2520short%2520title%2520to%2520long%2520summaries.%2520We%2520demonstrate%2520clear%2520performance%2520improvements%2520with%2520data%2520scaling%252C%2520both%2520in%2520data%2520volume%2520and%2520label%2520intensity.%2520Moreover%252C%2520we%2520design%2520a%2520new%2520evaluation%2520metric%2520termed%2520GRACE%252C%2520which%2520incorporates%2520many-to-one%2520segment%2520overlaps%2520and%2520semantic%2520similarity%252C%2520better%2520reflecting%2520real-world%2520chaptering%2520flexibility.%2520Extensive%2520experiments%2520demonstrate%2520that%2520ARC-Chapter%2520establishes%2520a%2520new%2520state-of-the-art%2520by%2520a%2520significant%2520margin%252C%2520outperforming%2520the%2520previous%2520best%2520by%252014.0%2525%2520in%2520F1%2520score%2520and%252011.3%2525%2520in%2520SODA%2520score.%2520Moreover%252C%2520ARC-Chapter%2520shows%2520excellent%2520transferability%252C%2520improving%2520the%2520state-of-the-art%2520on%2520downstream%2520tasks%2520like%2520dense%2520video%2520captioning%2520on%2520YouCook2.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARC-Chapter%3A%20Structuring%20Hour-Long%20Videos%20into%20Navigable%20Chapters%20and%20Hierarchical%20Summaries&entry.906535625=Junfu%20Pu%20and%20Teng%20Wang%20and%20Yixiao%20Ge%20and%20Yuying%20Ge%20and%20Chen%20Li%20and%20Ying%20Shan&entry.1292438233=The%20proliferation%20of%20hour-long%20videos%20%28e.g.%2C%20lectures%2C%20podcasts%2C%20documentaries%29%20has%20intensified%20demand%20for%20efficient%20content%20structuring.%20However%2C%20existing%20approaches%20are%20constrained%20by%20small-scale%20training%20with%20annotations%20that%20are%20typical%20short%20and%20coarse%2C%20restricting%20generalization%20to%20nuanced%20transitions%20in%20long%20videos.%20We%20introduce%20ARC-Chapter%2C%20the%20first%20large-scale%20video%20chaptering%20model%20trained%20on%20over%20million-level%20long%20video%20chapters%2C%20featuring%20bilingual%2C%20temporally%20grounded%2C%20and%20hierarchical%20chapter%20annotations.%20To%20achieve%20this%20goal%2C%20we%20curated%20a%20bilingual%20English-Chinese%20chapter%20dataset%20via%20a%20structured%20pipeline%20that%20unifies%20ASR%20transcripts%2C%20scene%20texts%2C%20visual%20captions%20into%20multi-level%20annotations%2C%20from%20short%20title%20to%20long%20summaries.%20We%20demonstrate%20clear%20performance%20improvements%20with%20data%20scaling%2C%20both%20in%20data%20volume%20and%20label%20intensity.%20Moreover%2C%20we%20design%20a%20new%20evaluation%20metric%20termed%20GRACE%2C%20which%20incorporates%20many-to-one%20segment%20overlaps%20and%20semantic%20similarity%2C%20better%20reflecting%20real-world%20chaptering%20flexibility.%20Extensive%20experiments%20demonstrate%20that%20ARC-Chapter%20establishes%20a%20new%20state-of-the-art%20by%20a%20significant%20margin%2C%20outperforming%20the%20previous%20best%20by%2014.0%25%20in%20F1%20score%20and%2011.3%25%20in%20SODA%20score.%20Moreover%2C%20ARC-Chapter%20shows%20excellent%20transferability%2C%20improving%20the%20state-of-the-art%20on%20downstream%20tasks%20like%20dense%20video%20captioning%20on%20YouCook2.&entry.1838667208=http%3A//arxiv.org/abs/2511.14349v1&entry.124074799=Read"},
{"title": "MRI Embeddings Complement Clinical Predictors for Cognitive Decline Modeling in Alzheimer's Disease Cohorts", "author": "Nathaniel Putera and Daniel Vilet Rodr\u00edguez and Noah Videcrantz and Julia Machnio and Mostafa Mehdipour Ghazi", "abstract": "Accurate modeling of cognitive decline in Alzheimer's disease is essential for early stratification and personalized management. While tabular predictors provide robust markers of global risk, their ability to capture subtle brain changes remains limited. In this study, we evaluate the predictive contributions of tabular and imaging-based representations, with a focus on transformer-derived Magnetic Resonance Imaging (MRI) embeddings. We introduce a trajectory-aware labeling strategy based on Dynamic Time Warping clustering to capture heterogeneous patterns of cognitive change, and train a 3D Vision Transformer (ViT) via unsupervised reconstruction on harmonized and augmented MRI data to obtain anatomy-preserving embeddings without progression labels. The pretrained encoder embeddings are subsequently assessed using both traditional machine learning classifiers and deep learning heads, and compared against tabular representations and convolutional network baselines. Results highlight complementary strengths across modalities. Clinical and volumetric features achieved the highest AUCs of around 0.70 for predicting mild and severe progression, underscoring their utility in capturing global decline trajectories. In contrast, MRI embeddings from the ViT model were most effective in distinguishing cognitively stable individuals with an AUC of 0.71. However, all approaches struggled in the heterogeneous moderate group. These findings indicate that clinical features excel in identifying high-risk extremes, whereas transformer-based MRI embeddings are more sensitive to subtle markers of stability, motivating multimodal fusion strategies for AD progression modeling.", "link": "http://arxiv.org/abs/2511.14601v1", "date": "2025-11-18", "relevancy": 2.5984, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5122}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MRI%20Embeddings%20Complement%20Clinical%20Predictors%20for%20Cognitive%20Decline%20Modeling%20in%20Alzheimer%27s%20Disease%20Cohorts&body=Title%3A%20MRI%20Embeddings%20Complement%20Clinical%20Predictors%20for%20Cognitive%20Decline%20Modeling%20in%20Alzheimer%27s%20Disease%20Cohorts%0AAuthor%3A%20Nathaniel%20Putera%20and%20Daniel%20Vilet%20Rodr%C3%ADguez%20and%20Noah%20Videcrantz%20and%20Julia%20Machnio%20and%20Mostafa%20Mehdipour%20Ghazi%0AAbstract%3A%20Accurate%20modeling%20of%20cognitive%20decline%20in%20Alzheimer%27s%20disease%20is%20essential%20for%20early%20stratification%20and%20personalized%20management.%20While%20tabular%20predictors%20provide%20robust%20markers%20of%20global%20risk%2C%20their%20ability%20to%20capture%20subtle%20brain%20changes%20remains%20limited.%20In%20this%20study%2C%20we%20evaluate%20the%20predictive%20contributions%20of%20tabular%20and%20imaging-based%20representations%2C%20with%20a%20focus%20on%20transformer-derived%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20embeddings.%20We%20introduce%20a%20trajectory-aware%20labeling%20strategy%20based%20on%20Dynamic%20Time%20Warping%20clustering%20to%20capture%20heterogeneous%20patterns%20of%20cognitive%20change%2C%20and%20train%20a%203D%20Vision%20Transformer%20%28ViT%29%20via%20unsupervised%20reconstruction%20on%20harmonized%20and%20augmented%20MRI%20data%20to%20obtain%20anatomy-preserving%20embeddings%20without%20progression%20labels.%20The%20pretrained%20encoder%20embeddings%20are%20subsequently%20assessed%20using%20both%20traditional%20machine%20learning%20classifiers%20and%20deep%20learning%20heads%2C%20and%20compared%20against%20tabular%20representations%20and%20convolutional%20network%20baselines.%20Results%20highlight%20complementary%20strengths%20across%20modalities.%20Clinical%20and%20volumetric%20features%20achieved%20the%20highest%20AUCs%20of%20around%200.70%20for%20predicting%20mild%20and%20severe%20progression%2C%20underscoring%20their%20utility%20in%20capturing%20global%20decline%20trajectories.%20In%20contrast%2C%20MRI%20embeddings%20from%20the%20ViT%20model%20were%20most%20effective%20in%20distinguishing%20cognitively%20stable%20individuals%20with%20an%20AUC%20of%200.71.%20However%2C%20all%20approaches%20struggled%20in%20the%20heterogeneous%20moderate%20group.%20These%20findings%20indicate%20that%20clinical%20features%20excel%20in%20identifying%20high-risk%20extremes%2C%20whereas%20transformer-based%20MRI%20embeddings%20are%20more%20sensitive%20to%20subtle%20markers%20of%20stability%2C%20motivating%20multimodal%20fusion%20strategies%20for%20AD%20progression%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMRI%2520Embeddings%2520Complement%2520Clinical%2520Predictors%2520for%2520Cognitive%2520Decline%2520Modeling%2520in%2520Alzheimer%2527s%2520Disease%2520Cohorts%26entry.906535625%3DNathaniel%2520Putera%2520and%2520Daniel%2520Vilet%2520Rodr%25C3%25ADguez%2520and%2520Noah%2520Videcrantz%2520and%2520Julia%2520Machnio%2520and%2520Mostafa%2520Mehdipour%2520Ghazi%26entry.1292438233%3DAccurate%2520modeling%2520of%2520cognitive%2520decline%2520in%2520Alzheimer%2527s%2520disease%2520is%2520essential%2520for%2520early%2520stratification%2520and%2520personalized%2520management.%2520While%2520tabular%2520predictors%2520provide%2520robust%2520markers%2520of%2520global%2520risk%252C%2520their%2520ability%2520to%2520capture%2520subtle%2520brain%2520changes%2520remains%2520limited.%2520In%2520this%2520study%252C%2520we%2520evaluate%2520the%2520predictive%2520contributions%2520of%2520tabular%2520and%2520imaging-based%2520representations%252C%2520with%2520a%2520focus%2520on%2520transformer-derived%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520embeddings.%2520We%2520introduce%2520a%2520trajectory-aware%2520labeling%2520strategy%2520based%2520on%2520Dynamic%2520Time%2520Warping%2520clustering%2520to%2520capture%2520heterogeneous%2520patterns%2520of%2520cognitive%2520change%252C%2520and%2520train%2520a%25203D%2520Vision%2520Transformer%2520%2528ViT%2529%2520via%2520unsupervised%2520reconstruction%2520on%2520harmonized%2520and%2520augmented%2520MRI%2520data%2520to%2520obtain%2520anatomy-preserving%2520embeddings%2520without%2520progression%2520labels.%2520The%2520pretrained%2520encoder%2520embeddings%2520are%2520subsequently%2520assessed%2520using%2520both%2520traditional%2520machine%2520learning%2520classifiers%2520and%2520deep%2520learning%2520heads%252C%2520and%2520compared%2520against%2520tabular%2520representations%2520and%2520convolutional%2520network%2520baselines.%2520Results%2520highlight%2520complementary%2520strengths%2520across%2520modalities.%2520Clinical%2520and%2520volumetric%2520features%2520achieved%2520the%2520highest%2520AUCs%2520of%2520around%25200.70%2520for%2520predicting%2520mild%2520and%2520severe%2520progression%252C%2520underscoring%2520their%2520utility%2520in%2520capturing%2520global%2520decline%2520trajectories.%2520In%2520contrast%252C%2520MRI%2520embeddings%2520from%2520the%2520ViT%2520model%2520were%2520most%2520effective%2520in%2520distinguishing%2520cognitively%2520stable%2520individuals%2520with%2520an%2520AUC%2520of%25200.71.%2520However%252C%2520all%2520approaches%2520struggled%2520in%2520the%2520heterogeneous%2520moderate%2520group.%2520These%2520findings%2520indicate%2520that%2520clinical%2520features%2520excel%2520in%2520identifying%2520high-risk%2520extremes%252C%2520whereas%2520transformer-based%2520MRI%2520embeddings%2520are%2520more%2520sensitive%2520to%2520subtle%2520markers%2520of%2520stability%252C%2520motivating%2520multimodal%2520fusion%2520strategies%2520for%2520AD%2520progression%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MRI%20Embeddings%20Complement%20Clinical%20Predictors%20for%20Cognitive%20Decline%20Modeling%20in%20Alzheimer%27s%20Disease%20Cohorts&entry.906535625=Nathaniel%20Putera%20and%20Daniel%20Vilet%20Rodr%C3%ADguez%20and%20Noah%20Videcrantz%20and%20Julia%20Machnio%20and%20Mostafa%20Mehdipour%20Ghazi&entry.1292438233=Accurate%20modeling%20of%20cognitive%20decline%20in%20Alzheimer%27s%20disease%20is%20essential%20for%20early%20stratification%20and%20personalized%20management.%20While%20tabular%20predictors%20provide%20robust%20markers%20of%20global%20risk%2C%20their%20ability%20to%20capture%20subtle%20brain%20changes%20remains%20limited.%20In%20this%20study%2C%20we%20evaluate%20the%20predictive%20contributions%20of%20tabular%20and%20imaging-based%20representations%2C%20with%20a%20focus%20on%20transformer-derived%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20embeddings.%20We%20introduce%20a%20trajectory-aware%20labeling%20strategy%20based%20on%20Dynamic%20Time%20Warping%20clustering%20to%20capture%20heterogeneous%20patterns%20of%20cognitive%20change%2C%20and%20train%20a%203D%20Vision%20Transformer%20%28ViT%29%20via%20unsupervised%20reconstruction%20on%20harmonized%20and%20augmented%20MRI%20data%20to%20obtain%20anatomy-preserving%20embeddings%20without%20progression%20labels.%20The%20pretrained%20encoder%20embeddings%20are%20subsequently%20assessed%20using%20both%20traditional%20machine%20learning%20classifiers%20and%20deep%20learning%20heads%2C%20and%20compared%20against%20tabular%20representations%20and%20convolutional%20network%20baselines.%20Results%20highlight%20complementary%20strengths%20across%20modalities.%20Clinical%20and%20volumetric%20features%20achieved%20the%20highest%20AUCs%20of%20around%200.70%20for%20predicting%20mild%20and%20severe%20progression%2C%20underscoring%20their%20utility%20in%20capturing%20global%20decline%20trajectories.%20In%20contrast%2C%20MRI%20embeddings%20from%20the%20ViT%20model%20were%20most%20effective%20in%20distinguishing%20cognitively%20stable%20individuals%20with%20an%20AUC%20of%200.71.%20However%2C%20all%20approaches%20struggled%20in%20the%20heterogeneous%20moderate%20group.%20These%20findings%20indicate%20that%20clinical%20features%20excel%20in%20identifying%20high-risk%20extremes%2C%20whereas%20transformer-based%20MRI%20embeddings%20are%20more%20sensitive%20to%20subtle%20markers%20of%20stability%2C%20motivating%20multimodal%20fusion%20strategies%20for%20AD%20progression%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2511.14601v1&entry.124074799=Read"},
{"title": "Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks", "author": "Xianhui Meng and Yuchen Zhang and Zhijian Huang and Zheng Lu and Ziling Ji and Yaoyao Yin and Hongyuan Zhang and Guangfeng Jiang and Yandan Lin and Long Chen and Hangjun Ye and Li Zhang and Jun Liu and Xiaoshuai Hao", "abstract": "Vision-Language Models (VLMs) show great promise for autonomous driving, but their suitability for safety-critical scenarios is largely unexplored, raising safety concerns. This issue arises from the lack of comprehensive benchmarks that assess both external environmental risks and in-cabin driving behavior safety simultaneously. To bridge this critical gap, we introduce DSBench, the first comprehensive Driving Safety Benchmark designed to assess a VLM's awareness of various safety risks in a unified manner. DSBench encompasses two major categories: external environmental risks and in-cabin driving behavior safety, divided into 10 key categories and a total of 28 sub-categories. This comprehensive evaluation covers a wide range of scenarios, ensuring a thorough assessment of VLMs' performance in safety-critical contexts. Extensive evaluations across various mainstream open-source and closed-source VLMs reveal significant performance degradation under complex safety-critical situations, highlighting urgent safety concerns. To address this, we constructed a large dataset of 98K instances focused on in-cabin and external safety scenarios, showing that fine-tuning on this dataset significantly enhances the safety performance of existing VLMs and paves the way for advancing autonomous driving technology. The benchmark toolkit, code, and model checkpoints will be publicly accessible.", "link": "http://arxiv.org/abs/2511.14592v1", "date": "2025-11-18", "relevancy": 2.5847, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Your%20VLM%20for%20Autonomous%20Driving%20Safety-Ready%3F%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20External%20and%20In-Cabin%20Risks&body=Title%3A%20Is%20Your%20VLM%20for%20Autonomous%20Driving%20Safety-Ready%3F%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20External%20and%20In-Cabin%20Risks%0AAuthor%3A%20Xianhui%20Meng%20and%20Yuchen%20Zhang%20and%20Zhijian%20Huang%20and%20Zheng%20Lu%20and%20Ziling%20Ji%20and%20Yaoyao%20Yin%20and%20Hongyuan%20Zhang%20and%20Guangfeng%20Jiang%20and%20Yandan%20Lin%20and%20Long%20Chen%20and%20Hangjun%20Ye%20and%20Li%20Zhang%20and%20Jun%20Liu%20and%20Xiaoshuai%20Hao%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20show%20great%20promise%20for%20autonomous%20driving%2C%20but%20their%20suitability%20for%20safety-critical%20scenarios%20is%20largely%20unexplored%2C%20raising%20safety%20concerns.%20This%20issue%20arises%20from%20the%20lack%20of%20comprehensive%20benchmarks%20that%20assess%20both%20external%20environmental%20risks%20and%20in-cabin%20driving%20behavior%20safety%20simultaneously.%20To%20bridge%20this%20critical%20gap%2C%20we%20introduce%20DSBench%2C%20the%20first%20comprehensive%20Driving%20Safety%20Benchmark%20designed%20to%20assess%20a%20VLM%27s%20awareness%20of%20various%20safety%20risks%20in%20a%20unified%20manner.%20DSBench%20encompasses%20two%20major%20categories%3A%20external%20environmental%20risks%20and%20in-cabin%20driving%20behavior%20safety%2C%20divided%20into%2010%20key%20categories%20and%20a%20total%20of%2028%20sub-categories.%20This%20comprehensive%20evaluation%20covers%20a%20wide%20range%20of%20scenarios%2C%20ensuring%20a%20thorough%20assessment%20of%20VLMs%27%20performance%20in%20safety-critical%20contexts.%20Extensive%20evaluations%20across%20various%20mainstream%20open-source%20and%20closed-source%20VLMs%20reveal%20significant%20performance%20degradation%20under%20complex%20safety-critical%20situations%2C%20highlighting%20urgent%20safety%20concerns.%20To%20address%20this%2C%20we%20constructed%20a%20large%20dataset%20of%2098K%20instances%20focused%20on%20in-cabin%20and%20external%20safety%20scenarios%2C%20showing%20that%20fine-tuning%20on%20this%20dataset%20significantly%20enhances%20the%20safety%20performance%20of%20existing%20VLMs%20and%20paves%20the%20way%20for%20advancing%20autonomous%20driving%20technology.%20The%20benchmark%20toolkit%2C%20code%2C%20and%20model%20checkpoints%20will%20be%20publicly%20accessible.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Your%2520VLM%2520for%2520Autonomous%2520Driving%2520Safety-Ready%253F%2520A%2520Comprehensive%2520Benchmark%2520for%2520Evaluating%2520External%2520and%2520In-Cabin%2520Risks%26entry.906535625%3DXianhui%2520Meng%2520and%2520Yuchen%2520Zhang%2520and%2520Zhijian%2520Huang%2520and%2520Zheng%2520Lu%2520and%2520Ziling%2520Ji%2520and%2520Yaoyao%2520Yin%2520and%2520Hongyuan%2520Zhang%2520and%2520Guangfeng%2520Jiang%2520and%2520Yandan%2520Lin%2520and%2520Long%2520Chen%2520and%2520Hangjun%2520Ye%2520and%2520Li%2520Zhang%2520and%2520Jun%2520Liu%2520and%2520Xiaoshuai%2520Hao%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520show%2520great%2520promise%2520for%2520autonomous%2520driving%252C%2520but%2520their%2520suitability%2520for%2520safety-critical%2520scenarios%2520is%2520largely%2520unexplored%252C%2520raising%2520safety%2520concerns.%2520This%2520issue%2520arises%2520from%2520the%2520lack%2520of%2520comprehensive%2520benchmarks%2520that%2520assess%2520both%2520external%2520environmental%2520risks%2520and%2520in-cabin%2520driving%2520behavior%2520safety%2520simultaneously.%2520To%2520bridge%2520this%2520critical%2520gap%252C%2520we%2520introduce%2520DSBench%252C%2520the%2520first%2520comprehensive%2520Driving%2520Safety%2520Benchmark%2520designed%2520to%2520assess%2520a%2520VLM%2527s%2520awareness%2520of%2520various%2520safety%2520risks%2520in%2520a%2520unified%2520manner.%2520DSBench%2520encompasses%2520two%2520major%2520categories%253A%2520external%2520environmental%2520risks%2520and%2520in-cabin%2520driving%2520behavior%2520safety%252C%2520divided%2520into%252010%2520key%2520categories%2520and%2520a%2520total%2520of%252028%2520sub-categories.%2520This%2520comprehensive%2520evaluation%2520covers%2520a%2520wide%2520range%2520of%2520scenarios%252C%2520ensuring%2520a%2520thorough%2520assessment%2520of%2520VLMs%2527%2520performance%2520in%2520safety-critical%2520contexts.%2520Extensive%2520evaluations%2520across%2520various%2520mainstream%2520open-source%2520and%2520closed-source%2520VLMs%2520reveal%2520significant%2520performance%2520degradation%2520under%2520complex%2520safety-critical%2520situations%252C%2520highlighting%2520urgent%2520safety%2520concerns.%2520To%2520address%2520this%252C%2520we%2520constructed%2520a%2520large%2520dataset%2520of%252098K%2520instances%2520focused%2520on%2520in-cabin%2520and%2520external%2520safety%2520scenarios%252C%2520showing%2520that%2520fine-tuning%2520on%2520this%2520dataset%2520significantly%2520enhances%2520the%2520safety%2520performance%2520of%2520existing%2520VLMs%2520and%2520paves%2520the%2520way%2520for%2520advancing%2520autonomous%2520driving%2520technology.%2520The%2520benchmark%2520toolkit%252C%2520code%252C%2520and%2520model%2520checkpoints%2520will%2520be%2520publicly%2520accessible.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Your%20VLM%20for%20Autonomous%20Driving%20Safety-Ready%3F%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20External%20and%20In-Cabin%20Risks&entry.906535625=Xianhui%20Meng%20and%20Yuchen%20Zhang%20and%20Zhijian%20Huang%20and%20Zheng%20Lu%20and%20Ziling%20Ji%20and%20Yaoyao%20Yin%20and%20Hongyuan%20Zhang%20and%20Guangfeng%20Jiang%20and%20Yandan%20Lin%20and%20Long%20Chen%20and%20Hangjun%20Ye%20and%20Li%20Zhang%20and%20Jun%20Liu%20and%20Xiaoshuai%20Hao&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20show%20great%20promise%20for%20autonomous%20driving%2C%20but%20their%20suitability%20for%20safety-critical%20scenarios%20is%20largely%20unexplored%2C%20raising%20safety%20concerns.%20This%20issue%20arises%20from%20the%20lack%20of%20comprehensive%20benchmarks%20that%20assess%20both%20external%20environmental%20risks%20and%20in-cabin%20driving%20behavior%20safety%20simultaneously.%20To%20bridge%20this%20critical%20gap%2C%20we%20introduce%20DSBench%2C%20the%20first%20comprehensive%20Driving%20Safety%20Benchmark%20designed%20to%20assess%20a%20VLM%27s%20awareness%20of%20various%20safety%20risks%20in%20a%20unified%20manner.%20DSBench%20encompasses%20two%20major%20categories%3A%20external%20environmental%20risks%20and%20in-cabin%20driving%20behavior%20safety%2C%20divided%20into%2010%20key%20categories%20and%20a%20total%20of%2028%20sub-categories.%20This%20comprehensive%20evaluation%20covers%20a%20wide%20range%20of%20scenarios%2C%20ensuring%20a%20thorough%20assessment%20of%20VLMs%27%20performance%20in%20safety-critical%20contexts.%20Extensive%20evaluations%20across%20various%20mainstream%20open-source%20and%20closed-source%20VLMs%20reveal%20significant%20performance%20degradation%20under%20complex%20safety-critical%20situations%2C%20highlighting%20urgent%20safety%20concerns.%20To%20address%20this%2C%20we%20constructed%20a%20large%20dataset%20of%2098K%20instances%20focused%20on%20in-cabin%20and%20external%20safety%20scenarios%2C%20showing%20that%20fine-tuning%20on%20this%20dataset%20significantly%20enhances%20the%20safety%20performance%20of%20existing%20VLMs%20and%20paves%20the%20way%20for%20advancing%20autonomous%20driving%20technology.%20The%20benchmark%20toolkit%2C%20code%2C%20and%20model%20checkpoints%20will%20be%20publicly%20accessible.&entry.1838667208=http%3A//arxiv.org/abs/2511.14592v1&entry.124074799=Read"},
{"title": "Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact", "author": "Satyanarayan Pati", "abstract": "Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the \"performance loss\" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective \"sweet spot,\" achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.", "link": "http://arxiv.org/abs/2511.13057v2", "date": "2025-11-18", "relevancy": 2.574, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dimension%20vs.%20Precision%3A%20A%20Comparative%20Analysis%20of%20Autoencoders%20and%20Quantization%20for%20Efficient%20Vector%20Retrieval%20on%20BEIR%20SciFact&body=Title%3A%20Dimension%20vs.%20Precision%3A%20A%20Comparative%20Analysis%20of%20Autoencoders%20and%20Quantization%20for%20Efficient%20Vector%20Retrieval%20on%20BEIR%20SciFact%0AAuthor%3A%20Satyanarayan%20Pati%0AAbstract%3A%20Dense%20retrieval%20models%20have%20become%20a%20standard%20for%20state-of-the-art%20information%20retrieval.%20However%2C%20their%20high-dimensional%2C%20high-precision%20%28float32%29%20vector%20embeddings%20create%20significant%20storage%20and%20memory%20challenges%20for%20real-world%20deployment.%20To%20address%20this%2C%20we%20conduct%20a%20rigorous%20empirical%20study%20on%20the%20BEIR%20SciFact%20benchmark%2C%20evaluating%20the%20trade-offs%20between%20two%20primary%20compression%20strategies%3A%20%281%29%20Dimensionality%20Reduction%20via%20deep%20Autoencoders%20%28AE%29%2C%20reducing%20original%20384-dim%20vectors%20to%20latent%20spaces%20from%20384%20down%20to%2012%2C%20and%20%282%29%20Precision%20Reduction%20via%20Quantization%20%28float16%2C%20int8%2C%20and%20binary%29.%20We%20systematically%20compare%20each%20method%20by%20measuring%20the%20%22performance%20loss%22%20%28or%20gain%29%20relative%20to%20a%20float32%20baseline%20across%20a%20full%20suite%20of%20retrieval%20metrics%20%28NDCG%2C%20MAP%2C%20MRR%2C%20Recall%2C%20Precision%29%20at%20various%20k%20cutoffs.%20Our%20results%20show%20that%20int8%20scalar%20quantization%20provides%20the%20most%20effective%20%22sweet%20spot%2C%22%20achieving%20a%204x%20compression%20with%20a%20negligible%20%5B~1-2%25%5D%20drop%20in%20nDCG%4010.%20In%20contrast%2C%20Autoencoders%20show%20a%20graceful%20degradation%20but%20suffer%20a%20more%20significant%20performance%20loss%20at%20equivalent%204x%20compression%20ratios%20%28AE-96%29.%20binary%20quantization%20was%20found%20to%20be%20unsuitable%20for%20this%20task%20due%20to%20catastrophic%20performance%20drops.%20This%20work%20provides%20a%20practical%20guide%20for%20deploying%20efficient%2C%20high-performance%20retrieval%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13057v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDimension%2520vs.%2520Precision%253A%2520A%2520Comparative%2520Analysis%2520of%2520Autoencoders%2520and%2520Quantization%2520for%2520Efficient%2520Vector%2520Retrieval%2520on%2520BEIR%2520SciFact%26entry.906535625%3DSatyanarayan%2520Pati%26entry.1292438233%3DDense%2520retrieval%2520models%2520have%2520become%2520a%2520standard%2520for%2520state-of-the-art%2520information%2520retrieval.%2520However%252C%2520their%2520high-dimensional%252C%2520high-precision%2520%2528float32%2529%2520vector%2520embeddings%2520create%2520significant%2520storage%2520and%2520memory%2520challenges%2520for%2520real-world%2520deployment.%2520To%2520address%2520this%252C%2520we%2520conduct%2520a%2520rigorous%2520empirical%2520study%2520on%2520the%2520BEIR%2520SciFact%2520benchmark%252C%2520evaluating%2520the%2520trade-offs%2520between%2520two%2520primary%2520compression%2520strategies%253A%2520%25281%2529%2520Dimensionality%2520Reduction%2520via%2520deep%2520Autoencoders%2520%2528AE%2529%252C%2520reducing%2520original%2520384-dim%2520vectors%2520to%2520latent%2520spaces%2520from%2520384%2520down%2520to%252012%252C%2520and%2520%25282%2529%2520Precision%2520Reduction%2520via%2520Quantization%2520%2528float16%252C%2520int8%252C%2520and%2520binary%2529.%2520We%2520systematically%2520compare%2520each%2520method%2520by%2520measuring%2520the%2520%2522performance%2520loss%2522%2520%2528or%2520gain%2529%2520relative%2520to%2520a%2520float32%2520baseline%2520across%2520a%2520full%2520suite%2520of%2520retrieval%2520metrics%2520%2528NDCG%252C%2520MAP%252C%2520MRR%252C%2520Recall%252C%2520Precision%2529%2520at%2520various%2520k%2520cutoffs.%2520Our%2520results%2520show%2520that%2520int8%2520scalar%2520quantization%2520provides%2520the%2520most%2520effective%2520%2522sweet%2520spot%252C%2522%2520achieving%2520a%25204x%2520compression%2520with%2520a%2520negligible%2520%255B~1-2%2525%255D%2520drop%2520in%2520nDCG%254010.%2520In%2520contrast%252C%2520Autoencoders%2520show%2520a%2520graceful%2520degradation%2520but%2520suffer%2520a%2520more%2520significant%2520performance%2520loss%2520at%2520equivalent%25204x%2520compression%2520ratios%2520%2528AE-96%2529.%2520binary%2520quantization%2520was%2520found%2520to%2520be%2520unsuitable%2520for%2520this%2520task%2520due%2520to%2520catastrophic%2520performance%2520drops.%2520This%2520work%2520provides%2520a%2520practical%2520guide%2520for%2520deploying%2520efficient%252C%2520high-performance%2520retrieval%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13057v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dimension%20vs.%20Precision%3A%20A%20Comparative%20Analysis%20of%20Autoencoders%20and%20Quantization%20for%20Efficient%20Vector%20Retrieval%20on%20BEIR%20SciFact&entry.906535625=Satyanarayan%20Pati&entry.1292438233=Dense%20retrieval%20models%20have%20become%20a%20standard%20for%20state-of-the-art%20information%20retrieval.%20However%2C%20their%20high-dimensional%2C%20high-precision%20%28float32%29%20vector%20embeddings%20create%20significant%20storage%20and%20memory%20challenges%20for%20real-world%20deployment.%20To%20address%20this%2C%20we%20conduct%20a%20rigorous%20empirical%20study%20on%20the%20BEIR%20SciFact%20benchmark%2C%20evaluating%20the%20trade-offs%20between%20two%20primary%20compression%20strategies%3A%20%281%29%20Dimensionality%20Reduction%20via%20deep%20Autoencoders%20%28AE%29%2C%20reducing%20original%20384-dim%20vectors%20to%20latent%20spaces%20from%20384%20down%20to%2012%2C%20and%20%282%29%20Precision%20Reduction%20via%20Quantization%20%28float16%2C%20int8%2C%20and%20binary%29.%20We%20systematically%20compare%20each%20method%20by%20measuring%20the%20%22performance%20loss%22%20%28or%20gain%29%20relative%20to%20a%20float32%20baseline%20across%20a%20full%20suite%20of%20retrieval%20metrics%20%28NDCG%2C%20MAP%2C%20MRR%2C%20Recall%2C%20Precision%29%20at%20various%20k%20cutoffs.%20Our%20results%20show%20that%20int8%20scalar%20quantization%20provides%20the%20most%20effective%20%22sweet%20spot%2C%22%20achieving%20a%204x%20compression%20with%20a%20negligible%20%5B~1-2%25%5D%20drop%20in%20nDCG%4010.%20In%20contrast%2C%20Autoencoders%20show%20a%20graceful%20degradation%20but%20suffer%20a%20more%20significant%20performance%20loss%20at%20equivalent%204x%20compression%20ratios%20%28AE-96%29.%20binary%20quantization%20was%20found%20to%20be%20unsuitable%20for%20this%20task%20due%20to%20catastrophic%20performance%20drops.%20This%20work%20provides%20a%20practical%20guide%20for%20deploying%20efficient%2C%20high-performance%20retrieval%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.13057v2&entry.124074799=Read"},
{"title": "Beyond Flatlands: Unlocking Spatial Intelligence by Decoupling 3D Reasoning from Numerical Regression", "author": "Zhongbin Guo and Jiahe Liu and Yushan Li and Wenyu Gao and Zhen Yang and Chenzhi Li and Xinyue Zhang and Ping Jian", "abstract": "Existing Vision Language Models (VLMs) architecturally rooted in \"flatland\" perception, fundamentally struggle to comprehend real-world 3D spatial intelligence. This failure stems from a dual-bottleneck: input-stage conflict between computationally exorbitant geometric-aware encoders and superficial 2D-only features, and output-stage misalignment where discrete tokenizers are structurally incapable of producing precise, continuous numerical values. To break this impasse, we introduce GEODE (Geometric-Output and Decoupled-Input Engine), a novel architecture that resolves this dual-bottleneck by decoupling 3D reasoning from numerical generation. GEODE augments main VLM with two specialized, plug-and-play modules: Decoupled Rationale Module (DRM) that acts as spatial co-processor, aligning explicit 3D data with 2D visual features via cross-attention and distilling spatial Chain-of-Thought (CoT) logic into injectable Rationale Tokens; and Direct Regression Head (DRH), an \"Embedding-as-Value\" paradigm which routes specialized control tokens to a lightweight MLP for precise, continuous regression of scalars and 3D bounding boxes. The synergy of these modules allows our 1.5B parameter model to function as a high-level semantic dispatcher, achieving state-of-the-art spatial reasoning performance that rivals 7B+ models.", "link": "http://arxiv.org/abs/2511.11239v2", "date": "2025-11-18", "relevancy": 2.5604, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6556}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Flatlands%3A%20Unlocking%20Spatial%20Intelligence%20by%20Decoupling%203D%20Reasoning%20from%20Numerical%20Regression&body=Title%3A%20Beyond%20Flatlands%3A%20Unlocking%20Spatial%20Intelligence%20by%20Decoupling%203D%20Reasoning%20from%20Numerical%20Regression%0AAuthor%3A%20Zhongbin%20Guo%20and%20Jiahe%20Liu%20and%20Yushan%20Li%20and%20Wenyu%20Gao%20and%20Zhen%20Yang%20and%20Chenzhi%20Li%20and%20Xinyue%20Zhang%20and%20Ping%20Jian%0AAbstract%3A%20Existing%20Vision%20Language%20Models%20%28VLMs%29%20architecturally%20rooted%20in%20%22flatland%22%20perception%2C%20fundamentally%20struggle%20to%20comprehend%20real-world%203D%20spatial%20intelligence.%20This%20failure%20stems%20from%20a%20dual-bottleneck%3A%20input-stage%20conflict%20between%20computationally%20exorbitant%20geometric-aware%20encoders%20and%20superficial%202D-only%20features%2C%20and%20output-stage%20misalignment%20where%20discrete%20tokenizers%20are%20structurally%20incapable%20of%20producing%20precise%2C%20continuous%20numerical%20values.%20To%20break%20this%20impasse%2C%20we%20introduce%20GEODE%20%28Geometric-Output%20and%20Decoupled-Input%20Engine%29%2C%20a%20novel%20architecture%20that%20resolves%20this%20dual-bottleneck%20by%20decoupling%203D%20reasoning%20from%20numerical%20generation.%20GEODE%20augments%20main%20VLM%20with%20two%20specialized%2C%20plug-and-play%20modules%3A%20Decoupled%20Rationale%20Module%20%28DRM%29%20that%20acts%20as%20spatial%20co-processor%2C%20aligning%20explicit%203D%20data%20with%202D%20visual%20features%20via%20cross-attention%20and%20distilling%20spatial%20Chain-of-Thought%20%28CoT%29%20logic%20into%20injectable%20Rationale%20Tokens%3B%20and%20Direct%20Regression%20Head%20%28DRH%29%2C%20an%20%22Embedding-as-Value%22%20paradigm%20which%20routes%20specialized%20control%20tokens%20to%20a%20lightweight%20MLP%20for%20precise%2C%20continuous%20regression%20of%20scalars%20and%203D%20bounding%20boxes.%20The%20synergy%20of%20these%20modules%20allows%20our%201.5B%20parameter%20model%20to%20function%20as%20a%20high-level%20semantic%20dispatcher%2C%20achieving%20state-of-the-art%20spatial%20reasoning%20performance%20that%20rivals%207B%2B%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11239v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Flatlands%253A%2520Unlocking%2520Spatial%2520Intelligence%2520by%2520Decoupling%25203D%2520Reasoning%2520from%2520Numerical%2520Regression%26entry.906535625%3DZhongbin%2520Guo%2520and%2520Jiahe%2520Liu%2520and%2520Yushan%2520Li%2520and%2520Wenyu%2520Gao%2520and%2520Zhen%2520Yang%2520and%2520Chenzhi%2520Li%2520and%2520Xinyue%2520Zhang%2520and%2520Ping%2520Jian%26entry.1292438233%3DExisting%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520architecturally%2520rooted%2520in%2520%2522flatland%2522%2520perception%252C%2520fundamentally%2520struggle%2520to%2520comprehend%2520real-world%25203D%2520spatial%2520intelligence.%2520This%2520failure%2520stems%2520from%2520a%2520dual-bottleneck%253A%2520input-stage%2520conflict%2520between%2520computationally%2520exorbitant%2520geometric-aware%2520encoders%2520and%2520superficial%25202D-only%2520features%252C%2520and%2520output-stage%2520misalignment%2520where%2520discrete%2520tokenizers%2520are%2520structurally%2520incapable%2520of%2520producing%2520precise%252C%2520continuous%2520numerical%2520values.%2520To%2520break%2520this%2520impasse%252C%2520we%2520introduce%2520GEODE%2520%2528Geometric-Output%2520and%2520Decoupled-Input%2520Engine%2529%252C%2520a%2520novel%2520architecture%2520that%2520resolves%2520this%2520dual-bottleneck%2520by%2520decoupling%25203D%2520reasoning%2520from%2520numerical%2520generation.%2520GEODE%2520augments%2520main%2520VLM%2520with%2520two%2520specialized%252C%2520plug-and-play%2520modules%253A%2520Decoupled%2520Rationale%2520Module%2520%2528DRM%2529%2520that%2520acts%2520as%2520spatial%2520co-processor%252C%2520aligning%2520explicit%25203D%2520data%2520with%25202D%2520visual%2520features%2520via%2520cross-attention%2520and%2520distilling%2520spatial%2520Chain-of-Thought%2520%2528CoT%2529%2520logic%2520into%2520injectable%2520Rationale%2520Tokens%253B%2520and%2520Direct%2520Regression%2520Head%2520%2528DRH%2529%252C%2520an%2520%2522Embedding-as-Value%2522%2520paradigm%2520which%2520routes%2520specialized%2520control%2520tokens%2520to%2520a%2520lightweight%2520MLP%2520for%2520precise%252C%2520continuous%2520regression%2520of%2520scalars%2520and%25203D%2520bounding%2520boxes.%2520The%2520synergy%2520of%2520these%2520modules%2520allows%2520our%25201.5B%2520parameter%2520model%2520to%2520function%2520as%2520a%2520high-level%2520semantic%2520dispatcher%252C%2520achieving%2520state-of-the-art%2520spatial%2520reasoning%2520performance%2520that%2520rivals%25207B%252B%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11239v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Flatlands%3A%20Unlocking%20Spatial%20Intelligence%20by%20Decoupling%203D%20Reasoning%20from%20Numerical%20Regression&entry.906535625=Zhongbin%20Guo%20and%20Jiahe%20Liu%20and%20Yushan%20Li%20and%20Wenyu%20Gao%20and%20Zhen%20Yang%20and%20Chenzhi%20Li%20and%20Xinyue%20Zhang%20and%20Ping%20Jian&entry.1292438233=Existing%20Vision%20Language%20Models%20%28VLMs%29%20architecturally%20rooted%20in%20%22flatland%22%20perception%2C%20fundamentally%20struggle%20to%20comprehend%20real-world%203D%20spatial%20intelligence.%20This%20failure%20stems%20from%20a%20dual-bottleneck%3A%20input-stage%20conflict%20between%20computationally%20exorbitant%20geometric-aware%20encoders%20and%20superficial%202D-only%20features%2C%20and%20output-stage%20misalignment%20where%20discrete%20tokenizers%20are%20structurally%20incapable%20of%20producing%20precise%2C%20continuous%20numerical%20values.%20To%20break%20this%20impasse%2C%20we%20introduce%20GEODE%20%28Geometric-Output%20and%20Decoupled-Input%20Engine%29%2C%20a%20novel%20architecture%20that%20resolves%20this%20dual-bottleneck%20by%20decoupling%203D%20reasoning%20from%20numerical%20generation.%20GEODE%20augments%20main%20VLM%20with%20two%20specialized%2C%20plug-and-play%20modules%3A%20Decoupled%20Rationale%20Module%20%28DRM%29%20that%20acts%20as%20spatial%20co-processor%2C%20aligning%20explicit%203D%20data%20with%202D%20visual%20features%20via%20cross-attention%20and%20distilling%20spatial%20Chain-of-Thought%20%28CoT%29%20logic%20into%20injectable%20Rationale%20Tokens%3B%20and%20Direct%20Regression%20Head%20%28DRH%29%2C%20an%20%22Embedding-as-Value%22%20paradigm%20which%20routes%20specialized%20control%20tokens%20to%20a%20lightweight%20MLP%20for%20precise%2C%20continuous%20regression%20of%20scalars%20and%203D%20bounding%20boxes.%20The%20synergy%20of%20these%20modules%20allows%20our%201.5B%20parameter%20model%20to%20function%20as%20a%20high-level%20semantic%20dispatcher%2C%20achieving%20state-of-the-art%20spatial%20reasoning%20performance%20that%20rivals%207B%2B%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.11239v2&entry.124074799=Read"},
{"title": "MAVias: Mitigate any Visual Bias", "author": "Ioannis Sarridis and Christos Koutlis and Symeon Papadopoulos and Christos Diou", "abstract": "Mitigating biases in computer vision models is an essential step towards the trustworthiness of artificial intelligence models. Existing bias mitigation methods focus on a small set of predefined biases, limiting their applicability in visual datasets where multiple, possibly unknown biases exist. To address this limitation, we introduce MAVias, an open-set bias mitigation approach leveraging foundation models to discover spurious associations between visual attributes and target classes. MAVias first captures a wide variety of visual features in natural language via a foundation image tagging model, and then leverages a large language model to select those visual features defining the target class, resulting in a set of language-coded potential visual biases. We then translate this set of potential biases into vision-language embeddings and introduce an in-processing bias mitigation approach to prevent the model from encoding information related to them. Our experiments on diverse datasets, including CelebA, Waterbirds, ImageNet, and UrbanCars, show that MAVias effectively detects and mitigates a wide range of biases in visual recognition tasks outperforming current state-of-the-art.", "link": "http://arxiv.org/abs/2412.06632v2", "date": "2025-11-18", "relevancy": 2.56, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAVias%3A%20Mitigate%20any%20Visual%20Bias&body=Title%3A%20MAVias%3A%20Mitigate%20any%20Visual%20Bias%0AAuthor%3A%20Ioannis%20Sarridis%20and%20Christos%20Koutlis%20and%20Symeon%20Papadopoulos%20and%20Christos%20Diou%0AAbstract%3A%20Mitigating%20biases%20in%20computer%20vision%20models%20is%20an%20essential%20step%20towards%20the%20trustworthiness%20of%20artificial%20intelligence%20models.%20Existing%20bias%20mitigation%20methods%20focus%20on%20a%20small%20set%20of%20predefined%20biases%2C%20limiting%20their%20applicability%20in%20visual%20datasets%20where%20multiple%2C%20possibly%20unknown%20biases%20exist.%20To%20address%20this%20limitation%2C%20we%20introduce%20MAVias%2C%20an%20open-set%20bias%20mitigation%20approach%20leveraging%20foundation%20models%20to%20discover%20spurious%20associations%20between%20visual%20attributes%20and%20target%20classes.%20MAVias%20first%20captures%20a%20wide%20variety%20of%20visual%20features%20in%20natural%20language%20via%20a%20foundation%20image%20tagging%20model%2C%20and%20then%20leverages%20a%20large%20language%20model%20to%20select%20those%20visual%20features%20defining%20the%20target%20class%2C%20resulting%20in%20a%20set%20of%20language-coded%20potential%20visual%20biases.%20We%20then%20translate%20this%20set%20of%20potential%20biases%20into%20vision-language%20embeddings%20and%20introduce%20an%20in-processing%20bias%20mitigation%20approach%20to%20prevent%20the%20model%20from%20encoding%20information%20related%20to%20them.%20Our%20experiments%20on%20diverse%20datasets%2C%20including%20CelebA%2C%20Waterbirds%2C%20ImageNet%2C%20and%20UrbanCars%2C%20show%20that%20MAVias%20effectively%20detects%20and%20mitigates%20a%20wide%20range%20of%20biases%20in%20visual%20recognition%20tasks%20outperforming%20current%20state-of-the-art.%0ALink%3A%20http%3A//arxiv.org/abs/2412.06632v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAVias%253A%2520Mitigate%2520any%2520Visual%2520Bias%26entry.906535625%3DIoannis%2520Sarridis%2520and%2520Christos%2520Koutlis%2520and%2520Symeon%2520Papadopoulos%2520and%2520Christos%2520Diou%26entry.1292438233%3DMitigating%2520biases%2520in%2520computer%2520vision%2520models%2520is%2520an%2520essential%2520step%2520towards%2520the%2520trustworthiness%2520of%2520artificial%2520intelligence%2520models.%2520Existing%2520bias%2520mitigation%2520methods%2520focus%2520on%2520a%2520small%2520set%2520of%2520predefined%2520biases%252C%2520limiting%2520their%2520applicability%2520in%2520visual%2520datasets%2520where%2520multiple%252C%2520possibly%2520unknown%2520biases%2520exist.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520MAVias%252C%2520an%2520open-set%2520bias%2520mitigation%2520approach%2520leveraging%2520foundation%2520models%2520to%2520discover%2520spurious%2520associations%2520between%2520visual%2520attributes%2520and%2520target%2520classes.%2520MAVias%2520first%2520captures%2520a%2520wide%2520variety%2520of%2520visual%2520features%2520in%2520natural%2520language%2520via%2520a%2520foundation%2520image%2520tagging%2520model%252C%2520and%2520then%2520leverages%2520a%2520large%2520language%2520model%2520to%2520select%2520those%2520visual%2520features%2520defining%2520the%2520target%2520class%252C%2520resulting%2520in%2520a%2520set%2520of%2520language-coded%2520potential%2520visual%2520biases.%2520We%2520then%2520translate%2520this%2520set%2520of%2520potential%2520biases%2520into%2520vision-language%2520embeddings%2520and%2520introduce%2520an%2520in-processing%2520bias%2520mitigation%2520approach%2520to%2520prevent%2520the%2520model%2520from%2520encoding%2520information%2520related%2520to%2520them.%2520Our%2520experiments%2520on%2520diverse%2520datasets%252C%2520including%2520CelebA%252C%2520Waterbirds%252C%2520ImageNet%252C%2520and%2520UrbanCars%252C%2520show%2520that%2520MAVias%2520effectively%2520detects%2520and%2520mitigates%2520a%2520wide%2520range%2520of%2520biases%2520in%2520visual%2520recognition%2520tasks%2520outperforming%2520current%2520state-of-the-art.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06632v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAVias%3A%20Mitigate%20any%20Visual%20Bias&entry.906535625=Ioannis%20Sarridis%20and%20Christos%20Koutlis%20and%20Symeon%20Papadopoulos%20and%20Christos%20Diou&entry.1292438233=Mitigating%20biases%20in%20computer%20vision%20models%20is%20an%20essential%20step%20towards%20the%20trustworthiness%20of%20artificial%20intelligence%20models.%20Existing%20bias%20mitigation%20methods%20focus%20on%20a%20small%20set%20of%20predefined%20biases%2C%20limiting%20their%20applicability%20in%20visual%20datasets%20where%20multiple%2C%20possibly%20unknown%20biases%20exist.%20To%20address%20this%20limitation%2C%20we%20introduce%20MAVias%2C%20an%20open-set%20bias%20mitigation%20approach%20leveraging%20foundation%20models%20to%20discover%20spurious%20associations%20between%20visual%20attributes%20and%20target%20classes.%20MAVias%20first%20captures%20a%20wide%20variety%20of%20visual%20features%20in%20natural%20language%20via%20a%20foundation%20image%20tagging%20model%2C%20and%20then%20leverages%20a%20large%20language%20model%20to%20select%20those%20visual%20features%20defining%20the%20target%20class%2C%20resulting%20in%20a%20set%20of%20language-coded%20potential%20visual%20biases.%20We%20then%20translate%20this%20set%20of%20potential%20biases%20into%20vision-language%20embeddings%20and%20introduce%20an%20in-processing%20bias%20mitigation%20approach%20to%20prevent%20the%20model%20from%20encoding%20information%20related%20to%20them.%20Our%20experiments%20on%20diverse%20datasets%2C%20including%20CelebA%2C%20Waterbirds%2C%20ImageNet%2C%20and%20UrbanCars%2C%20show%20that%20MAVias%20effectively%20detects%20and%20mitigates%20a%20wide%20range%20of%20biases%20in%20visual%20recognition%20tasks%20outperforming%20current%20state-of-the-art.&entry.1838667208=http%3A//arxiv.org/abs/2412.06632v2&entry.124074799=Read"},
{"title": "A Neural Field-Based Approach for View Computation & Data Exploration in 3D Urban Environments", "author": "Stefan Cobeli and Kazi Shahrukh Omar and Rodrigo Valen\u00e7a and Nivan Ferreira and Fabio Miranda", "abstract": "Despite the growing availability of 3D urban datasets, extracting insights remains challenging due to computational bottlenecks and the complexity of interacting with data. In fact, the intricate geometry of 3D urban environments results in high degrees of occlusion and requires extensive manual viewpoint adjustments that make large-scale exploration inefficient. To address this, we propose a view-based approach for 3D data exploration, where a vector field encodes views from the environment. To support this approach, we introduce a neural field-based method that constructs an efficient implicit representation of 3D environments. This representation enables both faster direct queries, which consist of the computation of view assessment indices, and inverse queries, which help avoid occlusion and facilitate the search for views that match desired data patterns. Our approach supports key urban analysis tasks such as visibility assessments, solar exposure evaluation, and assessing the visual impact of new developments. We validate our method through quantitative experiments, case studies informed by real-world urban challenges, and feedback from domain experts. Results show its effectiveness in finding desirable viewpoints, analyzing building facade visibility, and evaluating views from outdoor spaces. Code and data are publicly available at https://urbantk.org/neural-3d.", "link": "http://arxiv.org/abs/2511.14742v1", "date": "2025-11-18", "relevancy": 2.5263, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6393}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.63}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.63}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Neural%20Field-Based%20Approach%20for%20View%20Computation%20%26%20Data%20Exploration%20in%203D%20Urban%20Environments&body=Title%3A%20A%20Neural%20Field-Based%20Approach%20for%20View%20Computation%20%26%20Data%20Exploration%20in%203D%20Urban%20Environments%0AAuthor%3A%20Stefan%20Cobeli%20and%20Kazi%20Shahrukh%20Omar%20and%20Rodrigo%20Valen%C3%A7a%20and%20Nivan%20Ferreira%20and%20Fabio%20Miranda%0AAbstract%3A%20Despite%20the%20growing%20availability%20of%203D%20urban%20datasets%2C%20extracting%20insights%20remains%20challenging%20due%20to%20computational%20bottlenecks%20and%20the%20complexity%20of%20interacting%20with%20data.%20In%20fact%2C%20the%20intricate%20geometry%20of%203D%20urban%20environments%20results%20in%20high%20degrees%20of%20occlusion%20and%20requires%20extensive%20manual%20viewpoint%20adjustments%20that%20make%20large-scale%20exploration%20inefficient.%20To%20address%20this%2C%20we%20propose%20a%20view-based%20approach%20for%203D%20data%20exploration%2C%20where%20a%20vector%20field%20encodes%20views%20from%20the%20environment.%20To%20support%20this%20approach%2C%20we%20introduce%20a%20neural%20field-based%20method%20that%20constructs%20an%20efficient%20implicit%20representation%20of%203D%20environments.%20This%20representation%20enables%20both%20faster%20direct%20queries%2C%20which%20consist%20of%20the%20computation%20of%20view%20assessment%20indices%2C%20and%20inverse%20queries%2C%20which%20help%20avoid%20occlusion%20and%20facilitate%20the%20search%20for%20views%20that%20match%20desired%20data%20patterns.%20Our%20approach%20supports%20key%20urban%20analysis%20tasks%20such%20as%20visibility%20assessments%2C%20solar%20exposure%20evaluation%2C%20and%20assessing%20the%20visual%20impact%20of%20new%20developments.%20We%20validate%20our%20method%20through%20quantitative%20experiments%2C%20case%20studies%20informed%20by%20real-world%20urban%20challenges%2C%20and%20feedback%20from%20domain%20experts.%20Results%20show%20its%20effectiveness%20in%20finding%20desirable%20viewpoints%2C%20analyzing%20building%20facade%20visibility%2C%20and%20evaluating%20views%20from%20outdoor%20spaces.%20Code%20and%20data%20are%20publicly%20available%20at%20https%3A//urbantk.org/neural-3d.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Neural%2520Field-Based%2520Approach%2520for%2520View%2520Computation%2520%2526%2520Data%2520Exploration%2520in%25203D%2520Urban%2520Environments%26entry.906535625%3DStefan%2520Cobeli%2520and%2520Kazi%2520Shahrukh%2520Omar%2520and%2520Rodrigo%2520Valen%25C3%25A7a%2520and%2520Nivan%2520Ferreira%2520and%2520Fabio%2520Miranda%26entry.1292438233%3DDespite%2520the%2520growing%2520availability%2520of%25203D%2520urban%2520datasets%252C%2520extracting%2520insights%2520remains%2520challenging%2520due%2520to%2520computational%2520bottlenecks%2520and%2520the%2520complexity%2520of%2520interacting%2520with%2520data.%2520In%2520fact%252C%2520the%2520intricate%2520geometry%2520of%25203D%2520urban%2520environments%2520results%2520in%2520high%2520degrees%2520of%2520occlusion%2520and%2520requires%2520extensive%2520manual%2520viewpoint%2520adjustments%2520that%2520make%2520large-scale%2520exploration%2520inefficient.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520view-based%2520approach%2520for%25203D%2520data%2520exploration%252C%2520where%2520a%2520vector%2520field%2520encodes%2520views%2520from%2520the%2520environment.%2520To%2520support%2520this%2520approach%252C%2520we%2520introduce%2520a%2520neural%2520field-based%2520method%2520that%2520constructs%2520an%2520efficient%2520implicit%2520representation%2520of%25203D%2520environments.%2520This%2520representation%2520enables%2520both%2520faster%2520direct%2520queries%252C%2520which%2520consist%2520of%2520the%2520computation%2520of%2520view%2520assessment%2520indices%252C%2520and%2520inverse%2520queries%252C%2520which%2520help%2520avoid%2520occlusion%2520and%2520facilitate%2520the%2520search%2520for%2520views%2520that%2520match%2520desired%2520data%2520patterns.%2520Our%2520approach%2520supports%2520key%2520urban%2520analysis%2520tasks%2520such%2520as%2520visibility%2520assessments%252C%2520solar%2520exposure%2520evaluation%252C%2520and%2520assessing%2520the%2520visual%2520impact%2520of%2520new%2520developments.%2520We%2520validate%2520our%2520method%2520through%2520quantitative%2520experiments%252C%2520case%2520studies%2520informed%2520by%2520real-world%2520urban%2520challenges%252C%2520and%2520feedback%2520from%2520domain%2520experts.%2520Results%2520show%2520its%2520effectiveness%2520in%2520finding%2520desirable%2520viewpoints%252C%2520analyzing%2520building%2520facade%2520visibility%252C%2520and%2520evaluating%2520views%2520from%2520outdoor%2520spaces.%2520Code%2520and%2520data%2520are%2520publicly%2520available%2520at%2520https%253A//urbantk.org/neural-3d.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Neural%20Field-Based%20Approach%20for%20View%20Computation%20%26%20Data%20Exploration%20in%203D%20Urban%20Environments&entry.906535625=Stefan%20Cobeli%20and%20Kazi%20Shahrukh%20Omar%20and%20Rodrigo%20Valen%C3%A7a%20and%20Nivan%20Ferreira%20and%20Fabio%20Miranda&entry.1292438233=Despite%20the%20growing%20availability%20of%203D%20urban%20datasets%2C%20extracting%20insights%20remains%20challenging%20due%20to%20computational%20bottlenecks%20and%20the%20complexity%20of%20interacting%20with%20data.%20In%20fact%2C%20the%20intricate%20geometry%20of%203D%20urban%20environments%20results%20in%20high%20degrees%20of%20occlusion%20and%20requires%20extensive%20manual%20viewpoint%20adjustments%20that%20make%20large-scale%20exploration%20inefficient.%20To%20address%20this%2C%20we%20propose%20a%20view-based%20approach%20for%203D%20data%20exploration%2C%20where%20a%20vector%20field%20encodes%20views%20from%20the%20environment.%20To%20support%20this%20approach%2C%20we%20introduce%20a%20neural%20field-based%20method%20that%20constructs%20an%20efficient%20implicit%20representation%20of%203D%20environments.%20This%20representation%20enables%20both%20faster%20direct%20queries%2C%20which%20consist%20of%20the%20computation%20of%20view%20assessment%20indices%2C%20and%20inverse%20queries%2C%20which%20help%20avoid%20occlusion%20and%20facilitate%20the%20search%20for%20views%20that%20match%20desired%20data%20patterns.%20Our%20approach%20supports%20key%20urban%20analysis%20tasks%20such%20as%20visibility%20assessments%2C%20solar%20exposure%20evaluation%2C%20and%20assessing%20the%20visual%20impact%20of%20new%20developments.%20We%20validate%20our%20method%20through%20quantitative%20experiments%2C%20case%20studies%20informed%20by%20real-world%20urban%20challenges%2C%20and%20feedback%20from%20domain%20experts.%20Results%20show%20its%20effectiveness%20in%20finding%20desirable%20viewpoints%2C%20analyzing%20building%20facade%20visibility%2C%20and%20evaluating%20views%20from%20outdoor%20spaces.%20Code%20and%20data%20are%20publicly%20available%20at%20https%3A//urbantk.org/neural-3d.&entry.1838667208=http%3A//arxiv.org/abs/2511.14742v1&entry.124074799=Read"},
{"title": "Improving Greenland Bed Topography Mapping with Uncertainty-Aware Graph Learning on Sparse Radar Data", "author": "Bayu Adhi Tama and Homayra Alam and Mostafa Cham and Omar Faruque and Jianwu Wang and Vandana Janeja", "abstract": "Accurate maps of Greenland's subglacial bed are essential for sea-level projections, but radar observations are sparse and uneven. We introduce GraphTopoNet, a graph-learning framework that fuses heterogeneous supervision and explicitly models uncertainty via Monte Carlo dropout. Spatial graphs built from surface observables (elevation, velocity, mass balance) are augmented with gradient features and polynomial trends to capture both local variability and broad structure. To handle data gaps, we employ a hybrid loss that combines confidence-weighted radar supervision with dynamically balanced regularization. Applied to three Greenland subregions, GraphTopoNet outperforms interpolation, convolutional, and graph-based baselines, reducing error by up to 60 percent while preserving fine-scale glacial features. The resulting bed maps improve reliability for operational modeling, supporting agencies engaged in climate forecasting and policy. More broadly, GraphTopoNet shows how graph machine learning can convert sparse, uncertain geophysical observations into actionable knowledge at continental scale.", "link": "http://arxiv.org/abs/2509.08571v2", "date": "2025-11-18", "relevancy": 2.5258, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5502}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.49}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Greenland%20Bed%20Topography%20Mapping%20with%20Uncertainty-Aware%20Graph%20Learning%20on%20Sparse%20Radar%20Data&body=Title%3A%20Improving%20Greenland%20Bed%20Topography%20Mapping%20with%20Uncertainty-Aware%20Graph%20Learning%20on%20Sparse%20Radar%20Data%0AAuthor%3A%20Bayu%20Adhi%20Tama%20and%20Homayra%20Alam%20and%20Mostafa%20Cham%20and%20Omar%20Faruque%20and%20Jianwu%20Wang%20and%20Vandana%20Janeja%0AAbstract%3A%20Accurate%20maps%20of%20Greenland%27s%20subglacial%20bed%20are%20essential%20for%20sea-level%20projections%2C%20but%20radar%20observations%20are%20sparse%20and%20uneven.%20We%20introduce%20GraphTopoNet%2C%20a%20graph-learning%20framework%20that%20fuses%20heterogeneous%20supervision%20and%20explicitly%20models%20uncertainty%20via%20Monte%20Carlo%20dropout.%20Spatial%20graphs%20built%20from%20surface%20observables%20%28elevation%2C%20velocity%2C%20mass%20balance%29%20are%20augmented%20with%20gradient%20features%20and%20polynomial%20trends%20to%20capture%20both%20local%20variability%20and%20broad%20structure.%20To%20handle%20data%20gaps%2C%20we%20employ%20a%20hybrid%20loss%20that%20combines%20confidence-weighted%20radar%20supervision%20with%20dynamically%20balanced%20regularization.%20Applied%20to%20three%20Greenland%20subregions%2C%20GraphTopoNet%20outperforms%20interpolation%2C%20convolutional%2C%20and%20graph-based%20baselines%2C%20reducing%20error%20by%20up%20to%2060%20percent%20while%20preserving%20fine-scale%20glacial%20features.%20The%20resulting%20bed%20maps%20improve%20reliability%20for%20operational%20modeling%2C%20supporting%20agencies%20engaged%20in%20climate%20forecasting%20and%20policy.%20More%20broadly%2C%20GraphTopoNet%20shows%20how%20graph%20machine%20learning%20can%20convert%20sparse%2C%20uncertain%20geophysical%20observations%20into%20actionable%20knowledge%20at%20continental%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2509.08571v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Greenland%2520Bed%2520Topography%2520Mapping%2520with%2520Uncertainty-Aware%2520Graph%2520Learning%2520on%2520Sparse%2520Radar%2520Data%26entry.906535625%3DBayu%2520Adhi%2520Tama%2520and%2520Homayra%2520Alam%2520and%2520Mostafa%2520Cham%2520and%2520Omar%2520Faruque%2520and%2520Jianwu%2520Wang%2520and%2520Vandana%2520Janeja%26entry.1292438233%3DAccurate%2520maps%2520of%2520Greenland%2527s%2520subglacial%2520bed%2520are%2520essential%2520for%2520sea-level%2520projections%252C%2520but%2520radar%2520observations%2520are%2520sparse%2520and%2520uneven.%2520We%2520introduce%2520GraphTopoNet%252C%2520a%2520graph-learning%2520framework%2520that%2520fuses%2520heterogeneous%2520supervision%2520and%2520explicitly%2520models%2520uncertainty%2520via%2520Monte%2520Carlo%2520dropout.%2520Spatial%2520graphs%2520built%2520from%2520surface%2520observables%2520%2528elevation%252C%2520velocity%252C%2520mass%2520balance%2529%2520are%2520augmented%2520with%2520gradient%2520features%2520and%2520polynomial%2520trends%2520to%2520capture%2520both%2520local%2520variability%2520and%2520broad%2520structure.%2520To%2520handle%2520data%2520gaps%252C%2520we%2520employ%2520a%2520hybrid%2520loss%2520that%2520combines%2520confidence-weighted%2520radar%2520supervision%2520with%2520dynamically%2520balanced%2520regularization.%2520Applied%2520to%2520three%2520Greenland%2520subregions%252C%2520GraphTopoNet%2520outperforms%2520interpolation%252C%2520convolutional%252C%2520and%2520graph-based%2520baselines%252C%2520reducing%2520error%2520by%2520up%2520to%252060%2520percent%2520while%2520preserving%2520fine-scale%2520glacial%2520features.%2520The%2520resulting%2520bed%2520maps%2520improve%2520reliability%2520for%2520operational%2520modeling%252C%2520supporting%2520agencies%2520engaged%2520in%2520climate%2520forecasting%2520and%2520policy.%2520More%2520broadly%252C%2520GraphTopoNet%2520shows%2520how%2520graph%2520machine%2520learning%2520can%2520convert%2520sparse%252C%2520uncertain%2520geophysical%2520observations%2520into%2520actionable%2520knowledge%2520at%2520continental%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08571v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Greenland%20Bed%20Topography%20Mapping%20with%20Uncertainty-Aware%20Graph%20Learning%20on%20Sparse%20Radar%20Data&entry.906535625=Bayu%20Adhi%20Tama%20and%20Homayra%20Alam%20and%20Mostafa%20Cham%20and%20Omar%20Faruque%20and%20Jianwu%20Wang%20and%20Vandana%20Janeja&entry.1292438233=Accurate%20maps%20of%20Greenland%27s%20subglacial%20bed%20are%20essential%20for%20sea-level%20projections%2C%20but%20radar%20observations%20are%20sparse%20and%20uneven.%20We%20introduce%20GraphTopoNet%2C%20a%20graph-learning%20framework%20that%20fuses%20heterogeneous%20supervision%20and%20explicitly%20models%20uncertainty%20via%20Monte%20Carlo%20dropout.%20Spatial%20graphs%20built%20from%20surface%20observables%20%28elevation%2C%20velocity%2C%20mass%20balance%29%20are%20augmented%20with%20gradient%20features%20and%20polynomial%20trends%20to%20capture%20both%20local%20variability%20and%20broad%20structure.%20To%20handle%20data%20gaps%2C%20we%20employ%20a%20hybrid%20loss%20that%20combines%20confidence-weighted%20radar%20supervision%20with%20dynamically%20balanced%20regularization.%20Applied%20to%20three%20Greenland%20subregions%2C%20GraphTopoNet%20outperforms%20interpolation%2C%20convolutional%2C%20and%20graph-based%20baselines%2C%20reducing%20error%20by%20up%20to%2060%20percent%20while%20preserving%20fine-scale%20glacial%20features.%20The%20resulting%20bed%20maps%20improve%20reliability%20for%20operational%20modeling%2C%20supporting%20agencies%20engaged%20in%20climate%20forecasting%20and%20policy.%20More%20broadly%2C%20GraphTopoNet%20shows%20how%20graph%20machine%20learning%20can%20convert%20sparse%2C%20uncertain%20geophysical%20observations%20into%20actionable%20knowledge%20at%20continental%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2509.08571v2&entry.124074799=Read"},
{"title": "DecNefLab: A Modular and Interpretable Simulation Framework for Decoded Neurofeedback", "author": "Alexander Olza and Roberto Santana and David Soto", "abstract": "Decoded Neurofeedback (DecNef) is a flourishing non-invasive approach to brain modulation with wide-ranging applications in neuromedicine and cognitive neuroscience. However, progress in DecNef research remains constrained by subject-dependent learning variability, reliance on indirect measures to quantify progress, and the high cost and time demands of experimentation.\n  We present DecNefLab, a modular and interpretable simulation framework that formalizes DecNef as a machine learning problem. Beyond providing a virtual laboratory, DecNefLab enables researchers to model, analyze and understand neurofeedback dynamics. Using latent variable generative models as simulated participants, DecNefLab allows direct observation of internal cognitive states and systematic evaluation of how different protocol designs and subject characteristics influence learning.\n  We demonstrate how this approach can (i) reproduce empirical phenomena of DecNef learning, (ii) identify conditions under which DecNef feedback fails to induce learning, and (iii) guide the design of more robust and reliable DecNef protocols in silico before human implementation.\n  In summary, DecNefLab bridges computational modeling and cognitive neuroscience, offering a principled foundation for methodological innovation, robust protocol design, and ultimately, a deeper understanding of DecNef-based brain modulation.", "link": "http://arxiv.org/abs/2511.14555v1", "date": "2025-11-18", "relevancy": 2.518, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DecNefLab%3A%20A%20Modular%20and%20Interpretable%20Simulation%20Framework%20for%20Decoded%20Neurofeedback&body=Title%3A%20DecNefLab%3A%20A%20Modular%20and%20Interpretable%20Simulation%20Framework%20for%20Decoded%20Neurofeedback%0AAuthor%3A%20Alexander%20Olza%20and%20Roberto%20Santana%20and%20David%20Soto%0AAbstract%3A%20Decoded%20Neurofeedback%20%28DecNef%29%20is%20a%20flourishing%20non-invasive%20approach%20to%20brain%20modulation%20with%20wide-ranging%20applications%20in%20neuromedicine%20and%20cognitive%20neuroscience.%20However%2C%20progress%20in%20DecNef%20research%20remains%20constrained%20by%20subject-dependent%20learning%20variability%2C%20reliance%20on%20indirect%20measures%20to%20quantify%20progress%2C%20and%20the%20high%20cost%20and%20time%20demands%20of%20experimentation.%0A%20%20We%20present%20DecNefLab%2C%20a%20modular%20and%20interpretable%20simulation%20framework%20that%20formalizes%20DecNef%20as%20a%20machine%20learning%20problem.%20Beyond%20providing%20a%20virtual%20laboratory%2C%20DecNefLab%20enables%20researchers%20to%20model%2C%20analyze%20and%20understand%20neurofeedback%20dynamics.%20Using%20latent%20variable%20generative%20models%20as%20simulated%20participants%2C%20DecNefLab%20allows%20direct%20observation%20of%20internal%20cognitive%20states%20and%20systematic%20evaluation%20of%20how%20different%20protocol%20designs%20and%20subject%20characteristics%20influence%20learning.%0A%20%20We%20demonstrate%20how%20this%20approach%20can%20%28i%29%20reproduce%20empirical%20phenomena%20of%20DecNef%20learning%2C%20%28ii%29%20identify%20conditions%20under%20which%20DecNef%20feedback%20fails%20to%20induce%20learning%2C%20and%20%28iii%29%20guide%20the%20design%20of%20more%20robust%20and%20reliable%20DecNef%20protocols%20in%20silico%20before%20human%20implementation.%0A%20%20In%20summary%2C%20DecNefLab%20bridges%20computational%20modeling%20and%20cognitive%20neuroscience%2C%20offering%20a%20principled%20foundation%20for%20methodological%20innovation%2C%20robust%20protocol%20design%2C%20and%20ultimately%2C%20a%20deeper%20understanding%20of%20DecNef-based%20brain%20modulation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecNefLab%253A%2520A%2520Modular%2520and%2520Interpretable%2520Simulation%2520Framework%2520for%2520Decoded%2520Neurofeedback%26entry.906535625%3DAlexander%2520Olza%2520and%2520Roberto%2520Santana%2520and%2520David%2520Soto%26entry.1292438233%3DDecoded%2520Neurofeedback%2520%2528DecNef%2529%2520is%2520a%2520flourishing%2520non-invasive%2520approach%2520to%2520brain%2520modulation%2520with%2520wide-ranging%2520applications%2520in%2520neuromedicine%2520and%2520cognitive%2520neuroscience.%2520However%252C%2520progress%2520in%2520DecNef%2520research%2520remains%2520constrained%2520by%2520subject-dependent%2520learning%2520variability%252C%2520reliance%2520on%2520indirect%2520measures%2520to%2520quantify%2520progress%252C%2520and%2520the%2520high%2520cost%2520and%2520time%2520demands%2520of%2520experimentation.%250A%2520%2520We%2520present%2520DecNefLab%252C%2520a%2520modular%2520and%2520interpretable%2520simulation%2520framework%2520that%2520formalizes%2520DecNef%2520as%2520a%2520machine%2520learning%2520problem.%2520Beyond%2520providing%2520a%2520virtual%2520laboratory%252C%2520DecNefLab%2520enables%2520researchers%2520to%2520model%252C%2520analyze%2520and%2520understand%2520neurofeedback%2520dynamics.%2520Using%2520latent%2520variable%2520generative%2520models%2520as%2520simulated%2520participants%252C%2520DecNefLab%2520allows%2520direct%2520observation%2520of%2520internal%2520cognitive%2520states%2520and%2520systematic%2520evaluation%2520of%2520how%2520different%2520protocol%2520designs%2520and%2520subject%2520characteristics%2520influence%2520learning.%250A%2520%2520We%2520demonstrate%2520how%2520this%2520approach%2520can%2520%2528i%2529%2520reproduce%2520empirical%2520phenomena%2520of%2520DecNef%2520learning%252C%2520%2528ii%2529%2520identify%2520conditions%2520under%2520which%2520DecNef%2520feedback%2520fails%2520to%2520induce%2520learning%252C%2520and%2520%2528iii%2529%2520guide%2520the%2520design%2520of%2520more%2520robust%2520and%2520reliable%2520DecNef%2520protocols%2520in%2520silico%2520before%2520human%2520implementation.%250A%2520%2520In%2520summary%252C%2520DecNefLab%2520bridges%2520computational%2520modeling%2520and%2520cognitive%2520neuroscience%252C%2520offering%2520a%2520principled%2520foundation%2520for%2520methodological%2520innovation%252C%2520robust%2520protocol%2520design%252C%2520and%2520ultimately%252C%2520a%2520deeper%2520understanding%2520of%2520DecNef-based%2520brain%2520modulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DecNefLab%3A%20A%20Modular%20and%20Interpretable%20Simulation%20Framework%20for%20Decoded%20Neurofeedback&entry.906535625=Alexander%20Olza%20and%20Roberto%20Santana%20and%20David%20Soto&entry.1292438233=Decoded%20Neurofeedback%20%28DecNef%29%20is%20a%20flourishing%20non-invasive%20approach%20to%20brain%20modulation%20with%20wide-ranging%20applications%20in%20neuromedicine%20and%20cognitive%20neuroscience.%20However%2C%20progress%20in%20DecNef%20research%20remains%20constrained%20by%20subject-dependent%20learning%20variability%2C%20reliance%20on%20indirect%20measures%20to%20quantify%20progress%2C%20and%20the%20high%20cost%20and%20time%20demands%20of%20experimentation.%0A%20%20We%20present%20DecNefLab%2C%20a%20modular%20and%20interpretable%20simulation%20framework%20that%20formalizes%20DecNef%20as%20a%20machine%20learning%20problem.%20Beyond%20providing%20a%20virtual%20laboratory%2C%20DecNefLab%20enables%20researchers%20to%20model%2C%20analyze%20and%20understand%20neurofeedback%20dynamics.%20Using%20latent%20variable%20generative%20models%20as%20simulated%20participants%2C%20DecNefLab%20allows%20direct%20observation%20of%20internal%20cognitive%20states%20and%20systematic%20evaluation%20of%20how%20different%20protocol%20designs%20and%20subject%20characteristics%20influence%20learning.%0A%20%20We%20demonstrate%20how%20this%20approach%20can%20%28i%29%20reproduce%20empirical%20phenomena%20of%20DecNef%20learning%2C%20%28ii%29%20identify%20conditions%20under%20which%20DecNef%20feedback%20fails%20to%20induce%20learning%2C%20and%20%28iii%29%20guide%20the%20design%20of%20more%20robust%20and%20reliable%20DecNef%20protocols%20in%20silico%20before%20human%20implementation.%0A%20%20In%20summary%2C%20DecNefLab%20bridges%20computational%20modeling%20and%20cognitive%20neuroscience%2C%20offering%20a%20principled%20foundation%20for%20methodological%20innovation%2C%20robust%20protocol%20design%2C%20and%20ultimately%2C%20a%20deeper%20understanding%20of%20DecNef-based%20brain%20modulation.&entry.1838667208=http%3A//arxiv.org/abs/2511.14555v1&entry.124074799=Read"},
{"title": "MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization", "author": "Runhao Jiang and Chengzhi Jiang and Rui Yan and Huajin Tang", "abstract": "The surrogate gradient (SG) method has shown significant promise in enhancing the performance of deep spiking neural networks (SNNs), but it also introduces vulnerabilities to adversarial attacks. Although spike coding strategies and neural dynamics parameters have been extensively studied for their impact on robustness, the critical role of gradient magnitude, which reflects the model's sensitivity to input perturbations, remains underexplored. In SNNs, the gradient magnitude is primarily determined by the interaction between the membrane potential distribution (MPD) and the SG function. In this study, we investigate the relationship between the MPD and SG and their implications for improving the robustness of SNNs. Our theoretical analysis reveals that reducing the proportion of membrane potentials lying within the gradient-available range of the SG function effectively mitigates the sensitivity of SNNs to input perturbations. Building upon this insight, we propose a novel MPD-driven surrogate gradient regularization (MPD-SGR) method, which enhances robustness by explicitly regularizing the MPD based on its interaction with the SG function. Extensive experiments across multiple image classification benchmarks and diverse network architectures confirm that the MPD-SGR method significantly enhances the resilience of SNNs to adversarial perturbations and exhibits strong generalizability across diverse network configurations, SG functions, and spike encoding schemes.", "link": "http://arxiv.org/abs/2511.12199v2", "date": "2025-11-18", "relevancy": 2.5013, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5085}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4968}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MPD-SGR%3A%20Robust%20Spiking%20Neural%20Networks%20with%20Membrane%20Potential%20Distribution-Driven%20Surrogate%20Gradient%20Regularization&body=Title%3A%20MPD-SGR%3A%20Robust%20Spiking%20Neural%20Networks%20with%20Membrane%20Potential%20Distribution-Driven%20Surrogate%20Gradient%20Regularization%0AAuthor%3A%20Runhao%20Jiang%20and%20Chengzhi%20Jiang%20and%20Rui%20Yan%20and%20Huajin%20Tang%0AAbstract%3A%20The%20surrogate%20gradient%20%28SG%29%20method%20has%20shown%20significant%20promise%20in%20enhancing%20the%20performance%20of%20deep%20spiking%20neural%20networks%20%28SNNs%29%2C%20but%20it%20also%20introduces%20vulnerabilities%20to%20adversarial%20attacks.%20Although%20spike%20coding%20strategies%20and%20neural%20dynamics%20parameters%20have%20been%20extensively%20studied%20for%20their%20impact%20on%20robustness%2C%20the%20critical%20role%20of%20gradient%20magnitude%2C%20which%20reflects%20the%20model%27s%20sensitivity%20to%20input%20perturbations%2C%20remains%20underexplored.%20In%20SNNs%2C%20the%20gradient%20magnitude%20is%20primarily%20determined%20by%20the%20interaction%20between%20the%20membrane%20potential%20distribution%20%28MPD%29%20and%20the%20SG%20function.%20In%20this%20study%2C%20we%20investigate%20the%20relationship%20between%20the%20MPD%20and%20SG%20and%20their%20implications%20for%20improving%20the%20robustness%20of%20SNNs.%20Our%20theoretical%20analysis%20reveals%20that%20reducing%20the%20proportion%20of%20membrane%20potentials%20lying%20within%20the%20gradient-available%20range%20of%20the%20SG%20function%20effectively%20mitigates%20the%20sensitivity%20of%20SNNs%20to%20input%20perturbations.%20Building%20upon%20this%20insight%2C%20we%20propose%20a%20novel%20MPD-driven%20surrogate%20gradient%20regularization%20%28MPD-SGR%29%20method%2C%20which%20enhances%20robustness%20by%20explicitly%20regularizing%20the%20MPD%20based%20on%20its%20interaction%20with%20the%20SG%20function.%20Extensive%20experiments%20across%20multiple%20image%20classification%20benchmarks%20and%20diverse%20network%20architectures%20confirm%20that%20the%20MPD-SGR%20method%20significantly%20enhances%20the%20resilience%20of%20SNNs%20to%20adversarial%20perturbations%20and%20exhibits%20strong%20generalizability%20across%20diverse%20network%20configurations%2C%20SG%20functions%2C%20and%20spike%20encoding%20schemes.%0ALink%3A%20http%3A//arxiv.org/abs/2511.12199v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMPD-SGR%253A%2520Robust%2520Spiking%2520Neural%2520Networks%2520with%2520Membrane%2520Potential%2520Distribution-Driven%2520Surrogate%2520Gradient%2520Regularization%26entry.906535625%3DRunhao%2520Jiang%2520and%2520Chengzhi%2520Jiang%2520and%2520Rui%2520Yan%2520and%2520Huajin%2520Tang%26entry.1292438233%3DThe%2520surrogate%2520gradient%2520%2528SG%2529%2520method%2520has%2520shown%2520significant%2520promise%2520in%2520enhancing%2520the%2520performance%2520of%2520deep%2520spiking%2520neural%2520networks%2520%2528SNNs%2529%252C%2520but%2520it%2520also%2520introduces%2520vulnerabilities%2520to%2520adversarial%2520attacks.%2520Although%2520spike%2520coding%2520strategies%2520and%2520neural%2520dynamics%2520parameters%2520have%2520been%2520extensively%2520studied%2520for%2520their%2520impact%2520on%2520robustness%252C%2520the%2520critical%2520role%2520of%2520gradient%2520magnitude%252C%2520which%2520reflects%2520the%2520model%2527s%2520sensitivity%2520to%2520input%2520perturbations%252C%2520remains%2520underexplored.%2520In%2520SNNs%252C%2520the%2520gradient%2520magnitude%2520is%2520primarily%2520determined%2520by%2520the%2520interaction%2520between%2520the%2520membrane%2520potential%2520distribution%2520%2528MPD%2529%2520and%2520the%2520SG%2520function.%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%2520relationship%2520between%2520the%2520MPD%2520and%2520SG%2520and%2520their%2520implications%2520for%2520improving%2520the%2520robustness%2520of%2520SNNs.%2520Our%2520theoretical%2520analysis%2520reveals%2520that%2520reducing%2520the%2520proportion%2520of%2520membrane%2520potentials%2520lying%2520within%2520the%2520gradient-available%2520range%2520of%2520the%2520SG%2520function%2520effectively%2520mitigates%2520the%2520sensitivity%2520of%2520SNNs%2520to%2520input%2520perturbations.%2520Building%2520upon%2520this%2520insight%252C%2520we%2520propose%2520a%2520novel%2520MPD-driven%2520surrogate%2520gradient%2520regularization%2520%2528MPD-SGR%2529%2520method%252C%2520which%2520enhances%2520robustness%2520by%2520explicitly%2520regularizing%2520the%2520MPD%2520based%2520on%2520its%2520interaction%2520with%2520the%2520SG%2520function.%2520Extensive%2520experiments%2520across%2520multiple%2520image%2520classification%2520benchmarks%2520and%2520diverse%2520network%2520architectures%2520confirm%2520that%2520the%2520MPD-SGR%2520method%2520significantly%2520enhances%2520the%2520resilience%2520of%2520SNNs%2520to%2520adversarial%2520perturbations%2520and%2520exhibits%2520strong%2520generalizability%2520across%2520diverse%2520network%2520configurations%252C%2520SG%2520functions%252C%2520and%2520spike%2520encoding%2520schemes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.12199v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MPD-SGR%3A%20Robust%20Spiking%20Neural%20Networks%20with%20Membrane%20Potential%20Distribution-Driven%20Surrogate%20Gradient%20Regularization&entry.906535625=Runhao%20Jiang%20and%20Chengzhi%20Jiang%20and%20Rui%20Yan%20and%20Huajin%20Tang&entry.1292438233=The%20surrogate%20gradient%20%28SG%29%20method%20has%20shown%20significant%20promise%20in%20enhancing%20the%20performance%20of%20deep%20spiking%20neural%20networks%20%28SNNs%29%2C%20but%20it%20also%20introduces%20vulnerabilities%20to%20adversarial%20attacks.%20Although%20spike%20coding%20strategies%20and%20neural%20dynamics%20parameters%20have%20been%20extensively%20studied%20for%20their%20impact%20on%20robustness%2C%20the%20critical%20role%20of%20gradient%20magnitude%2C%20which%20reflects%20the%20model%27s%20sensitivity%20to%20input%20perturbations%2C%20remains%20underexplored.%20In%20SNNs%2C%20the%20gradient%20magnitude%20is%20primarily%20determined%20by%20the%20interaction%20between%20the%20membrane%20potential%20distribution%20%28MPD%29%20and%20the%20SG%20function.%20In%20this%20study%2C%20we%20investigate%20the%20relationship%20between%20the%20MPD%20and%20SG%20and%20their%20implications%20for%20improving%20the%20robustness%20of%20SNNs.%20Our%20theoretical%20analysis%20reveals%20that%20reducing%20the%20proportion%20of%20membrane%20potentials%20lying%20within%20the%20gradient-available%20range%20of%20the%20SG%20function%20effectively%20mitigates%20the%20sensitivity%20of%20SNNs%20to%20input%20perturbations.%20Building%20upon%20this%20insight%2C%20we%20propose%20a%20novel%20MPD-driven%20surrogate%20gradient%20regularization%20%28MPD-SGR%29%20method%2C%20which%20enhances%20robustness%20by%20explicitly%20regularizing%20the%20MPD%20based%20on%20its%20interaction%20with%20the%20SG%20function.%20Extensive%20experiments%20across%20multiple%20image%20classification%20benchmarks%20and%20diverse%20network%20architectures%20confirm%20that%20the%20MPD-SGR%20method%20significantly%20enhances%20the%20resilience%20of%20SNNs%20to%20adversarial%20perturbations%20and%20exhibits%20strong%20generalizability%20across%20diverse%20network%20configurations%2C%20SG%20functions%2C%20and%20spike%20encoding%20schemes.&entry.1838667208=http%3A//arxiv.org/abs/2511.12199v2&entry.124074799=Read"},
{"title": "Iterative Diffusion-Refined Neural Attenuation Fields for Multi-Source Stationary CT Reconstruction: NAF Meets Diffusion Model", "author": "Jiancheng Fang and Shaoyu Wang and Junlin Wang and Weiwen Wu and Yikun Zhang and Qiegen Liu", "abstract": "Multi-source stationary computed tomography (CT) has recently attracted attention for its ability to achieve rapid image reconstruction, making it suitable for time-sensitive clinical and industrial applications. However, practical systems are often constrained by ultra-sparse-view sampling, which significantly degrades reconstruction quality. Traditional methods struggle under ultra-sparse-view settings, where interpolation becomes inaccurate and the resulting reconstructions are unsatisfactory. To address this challenge, this study proposes Diffusion-Refined Neural Attenuation Fields (Diff-NAF), an iterative framework tailored for multi-source stationary CT under ultra-sparse-view conditions. Diff-NAF combines a Neural Attenuation Field representation with a dual-branch conditional diffusion model. The process begins by training an initial NAF using ultra-sparse-view projections. New projections are then generated through an Angle-Prior Guided Projection Synthesis strategy that exploits inter view priors, and are subsequently refined by a Diffusion-driven Reuse Projection Refinement Module. The refined projections are incorporated as pseudo-labels into the training set for the next iteration. Through iterative refinement, Diff-NAF progressively enhances projection completeness and reconstruction fidelity under ultra-sparse-view conditions, ultimately yielding high-quality CT reconstructions. Experimental results on multiple simulated 3D CT volumes and real projection data demonstrate that Diff-NAF achieves the best performance under ultra-sparse-view conditions.", "link": "http://arxiv.org/abs/2511.14310v1", "date": "2025-11-18", "relevancy": 2.5003, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.629}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.629}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Diffusion-Refined%20Neural%20Attenuation%20Fields%20for%20Multi-Source%20Stationary%20CT%20Reconstruction%3A%20NAF%20Meets%20Diffusion%20Model&body=Title%3A%20Iterative%20Diffusion-Refined%20Neural%20Attenuation%20Fields%20for%20Multi-Source%20Stationary%20CT%20Reconstruction%3A%20NAF%20Meets%20Diffusion%20Model%0AAuthor%3A%20Jiancheng%20Fang%20and%20Shaoyu%20Wang%20and%20Junlin%20Wang%20and%20Weiwen%20Wu%20and%20Yikun%20Zhang%20and%20Qiegen%20Liu%0AAbstract%3A%20Multi-source%20stationary%20computed%20tomography%20%28CT%29%20has%20recently%20attracted%20attention%20for%20its%20ability%20to%20achieve%20rapid%20image%20reconstruction%2C%20making%20it%20suitable%20for%20time-sensitive%20clinical%20and%20industrial%20applications.%20However%2C%20practical%20systems%20are%20often%20constrained%20by%20ultra-sparse-view%20sampling%2C%20which%20significantly%20degrades%20reconstruction%20quality.%20Traditional%20methods%20struggle%20under%20ultra-sparse-view%20settings%2C%20where%20interpolation%20becomes%20inaccurate%20and%20the%20resulting%20reconstructions%20are%20unsatisfactory.%20To%20address%20this%20challenge%2C%20this%20study%20proposes%20Diffusion-Refined%20Neural%20Attenuation%20Fields%20%28Diff-NAF%29%2C%20an%20iterative%20framework%20tailored%20for%20multi-source%20stationary%20CT%20under%20ultra-sparse-view%20conditions.%20Diff-NAF%20combines%20a%20Neural%20Attenuation%20Field%20representation%20with%20a%20dual-branch%20conditional%20diffusion%20model.%20The%20process%20begins%20by%20training%20an%20initial%20NAF%20using%20ultra-sparse-view%20projections.%20New%20projections%20are%20then%20generated%20through%20an%20Angle-Prior%20Guided%20Projection%20Synthesis%20strategy%20that%20exploits%20inter%20view%20priors%2C%20and%20are%20subsequently%20refined%20by%20a%20Diffusion-driven%20Reuse%20Projection%20Refinement%20Module.%20The%20refined%20projections%20are%20incorporated%20as%20pseudo-labels%20into%20the%20training%20set%20for%20the%20next%20iteration.%20Through%20iterative%20refinement%2C%20Diff-NAF%20progressively%20enhances%20projection%20completeness%20and%20reconstruction%20fidelity%20under%20ultra-sparse-view%20conditions%2C%20ultimately%20yielding%20high-quality%20CT%20reconstructions.%20Experimental%20results%20on%20multiple%20simulated%203D%20CT%20volumes%20and%20real%20projection%20data%20demonstrate%20that%20Diff-NAF%20achieves%20the%20best%20performance%20under%20ultra-sparse-view%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Diffusion-Refined%2520Neural%2520Attenuation%2520Fields%2520for%2520Multi-Source%2520Stationary%2520CT%2520Reconstruction%253A%2520NAF%2520Meets%2520Diffusion%2520Model%26entry.906535625%3DJiancheng%2520Fang%2520and%2520Shaoyu%2520Wang%2520and%2520Junlin%2520Wang%2520and%2520Weiwen%2520Wu%2520and%2520Yikun%2520Zhang%2520and%2520Qiegen%2520Liu%26entry.1292438233%3DMulti-source%2520stationary%2520computed%2520tomography%2520%2528CT%2529%2520has%2520recently%2520attracted%2520attention%2520for%2520its%2520ability%2520to%2520achieve%2520rapid%2520image%2520reconstruction%252C%2520making%2520it%2520suitable%2520for%2520time-sensitive%2520clinical%2520and%2520industrial%2520applications.%2520However%252C%2520practical%2520systems%2520are%2520often%2520constrained%2520by%2520ultra-sparse-view%2520sampling%252C%2520which%2520significantly%2520degrades%2520reconstruction%2520quality.%2520Traditional%2520methods%2520struggle%2520under%2520ultra-sparse-view%2520settings%252C%2520where%2520interpolation%2520becomes%2520inaccurate%2520and%2520the%2520resulting%2520reconstructions%2520are%2520unsatisfactory.%2520To%2520address%2520this%2520challenge%252C%2520this%2520study%2520proposes%2520Diffusion-Refined%2520Neural%2520Attenuation%2520Fields%2520%2528Diff-NAF%2529%252C%2520an%2520iterative%2520framework%2520tailored%2520for%2520multi-source%2520stationary%2520CT%2520under%2520ultra-sparse-view%2520conditions.%2520Diff-NAF%2520combines%2520a%2520Neural%2520Attenuation%2520Field%2520representation%2520with%2520a%2520dual-branch%2520conditional%2520diffusion%2520model.%2520The%2520process%2520begins%2520by%2520training%2520an%2520initial%2520NAF%2520using%2520ultra-sparse-view%2520projections.%2520New%2520projections%2520are%2520then%2520generated%2520through%2520an%2520Angle-Prior%2520Guided%2520Projection%2520Synthesis%2520strategy%2520that%2520exploits%2520inter%2520view%2520priors%252C%2520and%2520are%2520subsequently%2520refined%2520by%2520a%2520Diffusion-driven%2520Reuse%2520Projection%2520Refinement%2520Module.%2520The%2520refined%2520projections%2520are%2520incorporated%2520as%2520pseudo-labels%2520into%2520the%2520training%2520set%2520for%2520the%2520next%2520iteration.%2520Through%2520iterative%2520refinement%252C%2520Diff-NAF%2520progressively%2520enhances%2520projection%2520completeness%2520and%2520reconstruction%2520fidelity%2520under%2520ultra-sparse-view%2520conditions%252C%2520ultimately%2520yielding%2520high-quality%2520CT%2520reconstructions.%2520Experimental%2520results%2520on%2520multiple%2520simulated%25203D%2520CT%2520volumes%2520and%2520real%2520projection%2520data%2520demonstrate%2520that%2520Diff-NAF%2520achieves%2520the%2520best%2520performance%2520under%2520ultra-sparse-view%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Diffusion-Refined%20Neural%20Attenuation%20Fields%20for%20Multi-Source%20Stationary%20CT%20Reconstruction%3A%20NAF%20Meets%20Diffusion%20Model&entry.906535625=Jiancheng%20Fang%20and%20Shaoyu%20Wang%20and%20Junlin%20Wang%20and%20Weiwen%20Wu%20and%20Yikun%20Zhang%20and%20Qiegen%20Liu&entry.1292438233=Multi-source%20stationary%20computed%20tomography%20%28CT%29%20has%20recently%20attracted%20attention%20for%20its%20ability%20to%20achieve%20rapid%20image%20reconstruction%2C%20making%20it%20suitable%20for%20time-sensitive%20clinical%20and%20industrial%20applications.%20However%2C%20practical%20systems%20are%20often%20constrained%20by%20ultra-sparse-view%20sampling%2C%20which%20significantly%20degrades%20reconstruction%20quality.%20Traditional%20methods%20struggle%20under%20ultra-sparse-view%20settings%2C%20where%20interpolation%20becomes%20inaccurate%20and%20the%20resulting%20reconstructions%20are%20unsatisfactory.%20To%20address%20this%20challenge%2C%20this%20study%20proposes%20Diffusion-Refined%20Neural%20Attenuation%20Fields%20%28Diff-NAF%29%2C%20an%20iterative%20framework%20tailored%20for%20multi-source%20stationary%20CT%20under%20ultra-sparse-view%20conditions.%20Diff-NAF%20combines%20a%20Neural%20Attenuation%20Field%20representation%20with%20a%20dual-branch%20conditional%20diffusion%20model.%20The%20process%20begins%20by%20training%20an%20initial%20NAF%20using%20ultra-sparse-view%20projections.%20New%20projections%20are%20then%20generated%20through%20an%20Angle-Prior%20Guided%20Projection%20Synthesis%20strategy%20that%20exploits%20inter%20view%20priors%2C%20and%20are%20subsequently%20refined%20by%20a%20Diffusion-driven%20Reuse%20Projection%20Refinement%20Module.%20The%20refined%20projections%20are%20incorporated%20as%20pseudo-labels%20into%20the%20training%20set%20for%20the%20next%20iteration.%20Through%20iterative%20refinement%2C%20Diff-NAF%20progressively%20enhances%20projection%20completeness%20and%20reconstruction%20fidelity%20under%20ultra-sparse-view%20conditions%2C%20ultimately%20yielding%20high-quality%20CT%20reconstructions.%20Experimental%20results%20on%20multiple%20simulated%203D%20CT%20volumes%20and%20real%20projection%20data%20demonstrate%20that%20Diff-NAF%20achieves%20the%20best%20performance%20under%20ultra-sparse-view%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2511.14310v1&entry.124074799=Read"},
{"title": "FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation", "author": "Yunfeng Wu and Jiayi Song and Zhenxiong Tan and Zihao He and Songhua Liu", "abstract": "The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim", "link": "http://arxiv.org/abs/2511.14712v1", "date": "2025-11-18", "relevancy": 2.4911, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6362}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6173}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeSwim%3A%20Revisiting%20Sliding-Window%20Attention%20Mechanisms%20for%20Training-Free%20Ultra-High-Resolution%20Video%20Generation&body=Title%3A%20FreeSwim%3A%20Revisiting%20Sliding-Window%20Attention%20Mechanisms%20for%20Training-Free%20Ultra-High-Resolution%20Video%20Generation%0AAuthor%3A%20Yunfeng%20Wu%20and%20Jiayi%20Song%20and%20Zhenxiong%20Tan%20and%20Zihao%20He%20and%20Songhua%20Liu%0AAbstract%3A%20The%20quadratic%20time%20and%20memory%20complexity%20of%20the%20attention%20mechanism%20in%20modern%20Transformer%20based%20video%20generators%20makes%20end-to-end%20training%20for%20ultra%20high%20resolution%20videos%20prohibitively%20expensive.%20Motivated%20by%20this%20limitation%2C%20we%20introduce%20a%20training-free%20approach%20that%20leverages%20video%20Diffusion%20Transformers%20pretrained%20at%20their%20native%20scale%20to%20synthesize%20higher%20resolution%20videos%20without%20any%20additional%20training%20or%20adaptation.%20At%20the%20core%20of%20our%20method%20lies%20an%20inward%20sliding%20window%20attention%20mechanism%2C%20which%20originates%20from%20a%20key%20observation%3A%20maintaining%20each%20query%20token%27s%20training%20scale%20receptive%20field%20is%20crucial%20for%20preserving%20visual%20fidelity%20and%20detail.%20However%2C%20naive%20local%20window%20attention%2C%20unfortunately%2C%20often%20leads%20to%20repetitive%20content%20and%20exhibits%20a%20lack%20of%20global%20coherence%20in%20the%20generated%20results.%20To%20overcome%20this%20challenge%2C%20we%20devise%20a%20dual-path%20pipeline%20that%20backs%20up%20window%20attention%20with%20a%20novel%20cross-attention%20override%20strategy%2C%20enabling%20the%20semantic%20content%20produced%20by%20local%20attention%20to%20be%20guided%20by%20another%20branch%20with%20a%20full%20receptive%20field%20and%2C%20therefore%2C%20ensuring%20holistic%20consistency.%20Furthermore%2C%20to%20improve%20efficiency%2C%20we%20incorporate%20a%20cross-attention%20caching%20strategy%20for%20this%20branch%20to%20avoid%20the%20frequent%20computation%20of%20full%203D%20attention.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20delivers%20ultra-high-resolution%20videos%20with%20fine-grained%20visual%20details%20and%20high%20efficiency%20in%20a%20training-free%20paradigm.%20Meanwhile%2C%20it%20achieves%20superior%20performance%20on%20VBench%2C%20even%20compared%20to%20training-based%20alternatives%2C%20with%20competitive%20or%20improved%20efficiency.%20Codes%20are%20available%20at%3A%20https%3A//github.com/WillWu111/FreeSwim%0ALink%3A%20http%3A//arxiv.org/abs/2511.14712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeSwim%253A%2520Revisiting%2520Sliding-Window%2520Attention%2520Mechanisms%2520for%2520Training-Free%2520Ultra-High-Resolution%2520Video%2520Generation%26entry.906535625%3DYunfeng%2520Wu%2520and%2520Jiayi%2520Song%2520and%2520Zhenxiong%2520Tan%2520and%2520Zihao%2520He%2520and%2520Songhua%2520Liu%26entry.1292438233%3DThe%2520quadratic%2520time%2520and%2520memory%2520complexity%2520of%2520the%2520attention%2520mechanism%2520in%2520modern%2520Transformer%2520based%2520video%2520generators%2520makes%2520end-to-end%2520training%2520for%2520ultra%2520high%2520resolution%2520videos%2520prohibitively%2520expensive.%2520Motivated%2520by%2520this%2520limitation%252C%2520we%2520introduce%2520a%2520training-free%2520approach%2520that%2520leverages%2520video%2520Diffusion%2520Transformers%2520pretrained%2520at%2520their%2520native%2520scale%2520to%2520synthesize%2520higher%2520resolution%2520videos%2520without%2520any%2520additional%2520training%2520or%2520adaptation.%2520At%2520the%2520core%2520of%2520our%2520method%2520lies%2520an%2520inward%2520sliding%2520window%2520attention%2520mechanism%252C%2520which%2520originates%2520from%2520a%2520key%2520observation%253A%2520maintaining%2520each%2520query%2520token%2527s%2520training%2520scale%2520receptive%2520field%2520is%2520crucial%2520for%2520preserving%2520visual%2520fidelity%2520and%2520detail.%2520However%252C%2520naive%2520local%2520window%2520attention%252C%2520unfortunately%252C%2520often%2520leads%2520to%2520repetitive%2520content%2520and%2520exhibits%2520a%2520lack%2520of%2520global%2520coherence%2520in%2520the%2520generated%2520results.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520devise%2520a%2520dual-path%2520pipeline%2520that%2520backs%2520up%2520window%2520attention%2520with%2520a%2520novel%2520cross-attention%2520override%2520strategy%252C%2520enabling%2520the%2520semantic%2520content%2520produced%2520by%2520local%2520attention%2520to%2520be%2520guided%2520by%2520another%2520branch%2520with%2520a%2520full%2520receptive%2520field%2520and%252C%2520therefore%252C%2520ensuring%2520holistic%2520consistency.%2520Furthermore%252C%2520to%2520improve%2520efficiency%252C%2520we%2520incorporate%2520a%2520cross-attention%2520caching%2520strategy%2520for%2520this%2520branch%2520to%2520avoid%2520the%2520frequent%2520computation%2520of%2520full%25203D%2520attention.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520delivers%2520ultra-high-resolution%2520videos%2520with%2520fine-grained%2520visual%2520details%2520and%2520high%2520efficiency%2520in%2520a%2520training-free%2520paradigm.%2520Meanwhile%252C%2520it%2520achieves%2520superior%2520performance%2520on%2520VBench%252C%2520even%2520compared%2520to%2520training-based%2520alternatives%252C%2520with%2520competitive%2520or%2520improved%2520efficiency.%2520Codes%2520are%2520available%2520at%253A%2520https%253A//github.com/WillWu111/FreeSwim%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeSwim%3A%20Revisiting%20Sliding-Window%20Attention%20Mechanisms%20for%20Training-Free%20Ultra-High-Resolution%20Video%20Generation&entry.906535625=Yunfeng%20Wu%20and%20Jiayi%20Song%20and%20Zhenxiong%20Tan%20and%20Zihao%20He%20and%20Songhua%20Liu&entry.1292438233=The%20quadratic%20time%20and%20memory%20complexity%20of%20the%20attention%20mechanism%20in%20modern%20Transformer%20based%20video%20generators%20makes%20end-to-end%20training%20for%20ultra%20high%20resolution%20videos%20prohibitively%20expensive.%20Motivated%20by%20this%20limitation%2C%20we%20introduce%20a%20training-free%20approach%20that%20leverages%20video%20Diffusion%20Transformers%20pretrained%20at%20their%20native%20scale%20to%20synthesize%20higher%20resolution%20videos%20without%20any%20additional%20training%20or%20adaptation.%20At%20the%20core%20of%20our%20method%20lies%20an%20inward%20sliding%20window%20attention%20mechanism%2C%20which%20originates%20from%20a%20key%20observation%3A%20maintaining%20each%20query%20token%27s%20training%20scale%20receptive%20field%20is%20crucial%20for%20preserving%20visual%20fidelity%20and%20detail.%20However%2C%20naive%20local%20window%20attention%2C%20unfortunately%2C%20often%20leads%20to%20repetitive%20content%20and%20exhibits%20a%20lack%20of%20global%20coherence%20in%20the%20generated%20results.%20To%20overcome%20this%20challenge%2C%20we%20devise%20a%20dual-path%20pipeline%20that%20backs%20up%20window%20attention%20with%20a%20novel%20cross-attention%20override%20strategy%2C%20enabling%20the%20semantic%20content%20produced%20by%20local%20attention%20to%20be%20guided%20by%20another%20branch%20with%20a%20full%20receptive%20field%20and%2C%20therefore%2C%20ensuring%20holistic%20consistency.%20Furthermore%2C%20to%20improve%20efficiency%2C%20we%20incorporate%20a%20cross-attention%20caching%20strategy%20for%20this%20branch%20to%20avoid%20the%20frequent%20computation%20of%20full%203D%20attention.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20delivers%20ultra-high-resolution%20videos%20with%20fine-grained%20visual%20details%20and%20high%20efficiency%20in%20a%20training-free%20paradigm.%20Meanwhile%2C%20it%20achieves%20superior%20performance%20on%20VBench%2C%20even%20compared%20to%20training-based%20alternatives%2C%20with%20competitive%20or%20improved%20efficiency.%20Codes%20are%20available%20at%3A%20https%3A//github.com/WillWu111/FreeSwim&entry.1838667208=http%3A//arxiv.org/abs/2511.14712v1&entry.124074799=Read"},
{"title": "The Tokenization Bottleneck: How Vocabulary Extension Improves Chemistry Representation Learning in Pretrained Language Models", "author": "Prathamesh Kalamkar and Ned Letcher and Meissane Chami and Sahger Lad and Shayan Mohanty and Prasanna Pendse", "abstract": "The application of large language models (LLMs) to chemistry is frequently hampered by a \"tokenization bottleneck\", where tokenizers tuned on general-domain text tend to fragment chemical representations such as SMILES into semantically uninformative sub-tokens. This paper introduces a principled methodology to resolve this bottleneck by unifying the representation of natural language and molecular structures within a single model. Our approach involves targeted vocabulary extension-augmenting a pretrained LLM's vocabulary with chemically salient tokens, followed by continued pretraining on chemistry-domain text to integrate this new knowledge. We provide an empirical demonstration of the effectiveness of this strategy, showing that our methodology leads to superior performance on a range of downstream chemical tasks.", "link": "http://arxiv.org/abs/2511.14365v1", "date": "2025-11-18", "relevancy": 2.4826, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.503}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4992}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Tokenization%20Bottleneck%3A%20How%20Vocabulary%20Extension%20Improves%20Chemistry%20Representation%20Learning%20in%20Pretrained%20Language%20Models&body=Title%3A%20The%20Tokenization%20Bottleneck%3A%20How%20Vocabulary%20Extension%20Improves%20Chemistry%20Representation%20Learning%20in%20Pretrained%20Language%20Models%0AAuthor%3A%20Prathamesh%20Kalamkar%20and%20Ned%20Letcher%20and%20Meissane%20Chami%20and%20Sahger%20Lad%20and%20Shayan%20Mohanty%20and%20Prasanna%20Pendse%0AAbstract%3A%20The%20application%20of%20large%20language%20models%20%28LLMs%29%20to%20chemistry%20is%20frequently%20hampered%20by%20a%20%22tokenization%20bottleneck%22%2C%20where%20tokenizers%20tuned%20on%20general-domain%20text%20tend%20to%20fragment%20chemical%20representations%20such%20as%20SMILES%20into%20semantically%20uninformative%20sub-tokens.%20This%20paper%20introduces%20a%20principled%20methodology%20to%20resolve%20this%20bottleneck%20by%20unifying%20the%20representation%20of%20natural%20language%20and%20molecular%20structures%20within%20a%20single%20model.%20Our%20approach%20involves%20targeted%20vocabulary%20extension-augmenting%20a%20pretrained%20LLM%27s%20vocabulary%20with%20chemically%20salient%20tokens%2C%20followed%20by%20continued%20pretraining%20on%20chemistry-domain%20text%20to%20integrate%20this%20new%20knowledge.%20We%20provide%20an%20empirical%20demonstration%20of%20the%20effectiveness%20of%20this%20strategy%2C%20showing%20that%20our%20methodology%20leads%20to%20superior%20performance%20on%20a%20range%20of%20downstream%20chemical%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Tokenization%2520Bottleneck%253A%2520How%2520Vocabulary%2520Extension%2520Improves%2520Chemistry%2520Representation%2520Learning%2520in%2520Pretrained%2520Language%2520Models%26entry.906535625%3DPrathamesh%2520Kalamkar%2520and%2520Ned%2520Letcher%2520and%2520Meissane%2520Chami%2520and%2520Sahger%2520Lad%2520and%2520Shayan%2520Mohanty%2520and%2520Prasanna%2520Pendse%26entry.1292438233%3DThe%2520application%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520chemistry%2520is%2520frequently%2520hampered%2520by%2520a%2520%2522tokenization%2520bottleneck%2522%252C%2520where%2520tokenizers%2520tuned%2520on%2520general-domain%2520text%2520tend%2520to%2520fragment%2520chemical%2520representations%2520such%2520as%2520SMILES%2520into%2520semantically%2520uninformative%2520sub-tokens.%2520This%2520paper%2520introduces%2520a%2520principled%2520methodology%2520to%2520resolve%2520this%2520bottleneck%2520by%2520unifying%2520the%2520representation%2520of%2520natural%2520language%2520and%2520molecular%2520structures%2520within%2520a%2520single%2520model.%2520Our%2520approach%2520involves%2520targeted%2520vocabulary%2520extension-augmenting%2520a%2520pretrained%2520LLM%2527s%2520vocabulary%2520with%2520chemically%2520salient%2520tokens%252C%2520followed%2520by%2520continued%2520pretraining%2520on%2520chemistry-domain%2520text%2520to%2520integrate%2520this%2520new%2520knowledge.%2520We%2520provide%2520an%2520empirical%2520demonstration%2520of%2520the%2520effectiveness%2520of%2520this%2520strategy%252C%2520showing%2520that%2520our%2520methodology%2520leads%2520to%2520superior%2520performance%2520on%2520a%2520range%2520of%2520downstream%2520chemical%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Tokenization%20Bottleneck%3A%20How%20Vocabulary%20Extension%20Improves%20Chemistry%20Representation%20Learning%20in%20Pretrained%20Language%20Models&entry.906535625=Prathamesh%20Kalamkar%20and%20Ned%20Letcher%20and%20Meissane%20Chami%20and%20Sahger%20Lad%20and%20Shayan%20Mohanty%20and%20Prasanna%20Pendse&entry.1292438233=The%20application%20of%20large%20language%20models%20%28LLMs%29%20to%20chemistry%20is%20frequently%20hampered%20by%20a%20%22tokenization%20bottleneck%22%2C%20where%20tokenizers%20tuned%20on%20general-domain%20text%20tend%20to%20fragment%20chemical%20representations%20such%20as%20SMILES%20into%20semantically%20uninformative%20sub-tokens.%20This%20paper%20introduces%20a%20principled%20methodology%20to%20resolve%20this%20bottleneck%20by%20unifying%20the%20representation%20of%20natural%20language%20and%20molecular%20structures%20within%20a%20single%20model.%20Our%20approach%20involves%20targeted%20vocabulary%20extension-augmenting%20a%20pretrained%20LLM%27s%20vocabulary%20with%20chemically%20salient%20tokens%2C%20followed%20by%20continued%20pretraining%20on%20chemistry-domain%20text%20to%20integrate%20this%20new%20knowledge.%20We%20provide%20an%20empirical%20demonstration%20of%20the%20effectiveness%20of%20this%20strategy%2C%20showing%20that%20our%20methodology%20leads%20to%20superior%20performance%20on%20a%20range%20of%20downstream%20chemical%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2511.14365v1&entry.124074799=Read"},
{"title": "BEDLAM2.0: Synthetic Humans and Cameras in Motion", "author": "Joachim Tesch and Giorgio Becherini and Prerana Achar and Anastasios Yiannakidis and Muhammed Kocabas and Priyanka Patel and Michael J. Black", "abstract": "Inferring 3D human motion from video remains a challenging problem with many applications. While traditional methods estimate the human in image coordinates, many applications require human motion to be estimated in world coordinates. This is particularly challenging when there is both human and camera motion. Progress on this topic has been limited by the lack of rich video data with ground truth human and camera movement. We address this with BEDLAM2.0, a new dataset that goes beyond the popular BEDLAM dataset in important ways. In addition to introducing more diverse and realistic cameras and camera motions, BEDLAM2.0 increases diversity and realism of body shape, motions, clothing, hair, and 3D environments. Additionally, it adds shoes, which were missing in BEDLAM. BEDLAM has become a key resource for training 3D human pose and motion regressors today and we show that BEDLAM2.0 is significantly better, particularly for training methods that estimate humans in world coordinates. We compare state-of-the art methods trained on BEDLAM and BEDLAM2.0, and find that BEDLAM2.0 significantly improves accuracy over BEDLAM. For research purposes, we provide the rendered videos, ground truth body parameters, and camera motions. We also provide the 3D assets to which we have rights and links to those from third parties.", "link": "http://arxiv.org/abs/2511.14394v1", "date": "2025-11-18", "relevancy": 2.4734, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6554}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5938}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEDLAM2.0%3A%20Synthetic%20Humans%20and%20Cameras%20in%20Motion&body=Title%3A%20BEDLAM2.0%3A%20Synthetic%20Humans%20and%20Cameras%20in%20Motion%0AAuthor%3A%20Joachim%20Tesch%20and%20Giorgio%20Becherini%20and%20Prerana%20Achar%20and%20Anastasios%20Yiannakidis%20and%20Muhammed%20Kocabas%20and%20Priyanka%20Patel%20and%20Michael%20J.%20Black%0AAbstract%3A%20Inferring%203D%20human%20motion%20from%20video%20remains%20a%20challenging%20problem%20with%20many%20applications.%20While%20traditional%20methods%20estimate%20the%20human%20in%20image%20coordinates%2C%20many%20applications%20require%20human%20motion%20to%20be%20estimated%20in%20world%20coordinates.%20This%20is%20particularly%20challenging%20when%20there%20is%20both%20human%20and%20camera%20motion.%20Progress%20on%20this%20topic%20has%20been%20limited%20by%20the%20lack%20of%20rich%20video%20data%20with%20ground%20truth%20human%20and%20camera%20movement.%20We%20address%20this%20with%20BEDLAM2.0%2C%20a%20new%20dataset%20that%20goes%20beyond%20the%20popular%20BEDLAM%20dataset%20in%20important%20ways.%20In%20addition%20to%20introducing%20more%20diverse%20and%20realistic%20cameras%20and%20camera%20motions%2C%20BEDLAM2.0%20increases%20diversity%20and%20realism%20of%20body%20shape%2C%20motions%2C%20clothing%2C%20hair%2C%20and%203D%20environments.%20Additionally%2C%20it%20adds%20shoes%2C%20which%20were%20missing%20in%20BEDLAM.%20BEDLAM%20has%20become%20a%20key%20resource%20for%20training%203D%20human%20pose%20and%20motion%20regressors%20today%20and%20we%20show%20that%20BEDLAM2.0%20is%20significantly%20better%2C%20particularly%20for%20training%20methods%20that%20estimate%20humans%20in%20world%20coordinates.%20We%20compare%20state-of-the%20art%20methods%20trained%20on%20BEDLAM%20and%20BEDLAM2.0%2C%20and%20find%20that%20BEDLAM2.0%20significantly%20improves%20accuracy%20over%20BEDLAM.%20For%20research%20purposes%2C%20we%20provide%20the%20rendered%20videos%2C%20ground%20truth%20body%20parameters%2C%20and%20camera%20motions.%20We%20also%20provide%20the%203D%20assets%20to%20which%20we%20have%20rights%20and%20links%20to%20those%20from%20third%20parties.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEDLAM2.0%253A%2520Synthetic%2520Humans%2520and%2520Cameras%2520in%2520Motion%26entry.906535625%3DJoachim%2520Tesch%2520and%2520Giorgio%2520Becherini%2520and%2520Prerana%2520Achar%2520and%2520Anastasios%2520Yiannakidis%2520and%2520Muhammed%2520Kocabas%2520and%2520Priyanka%2520Patel%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3DInferring%25203D%2520human%2520motion%2520from%2520video%2520remains%2520a%2520challenging%2520problem%2520with%2520many%2520applications.%2520While%2520traditional%2520methods%2520estimate%2520the%2520human%2520in%2520image%2520coordinates%252C%2520many%2520applications%2520require%2520human%2520motion%2520to%2520be%2520estimated%2520in%2520world%2520coordinates.%2520This%2520is%2520particularly%2520challenging%2520when%2520there%2520is%2520both%2520human%2520and%2520camera%2520motion.%2520Progress%2520on%2520this%2520topic%2520has%2520been%2520limited%2520by%2520the%2520lack%2520of%2520rich%2520video%2520data%2520with%2520ground%2520truth%2520human%2520and%2520camera%2520movement.%2520We%2520address%2520this%2520with%2520BEDLAM2.0%252C%2520a%2520new%2520dataset%2520that%2520goes%2520beyond%2520the%2520popular%2520BEDLAM%2520dataset%2520in%2520important%2520ways.%2520In%2520addition%2520to%2520introducing%2520more%2520diverse%2520and%2520realistic%2520cameras%2520and%2520camera%2520motions%252C%2520BEDLAM2.0%2520increases%2520diversity%2520and%2520realism%2520of%2520body%2520shape%252C%2520motions%252C%2520clothing%252C%2520hair%252C%2520and%25203D%2520environments.%2520Additionally%252C%2520it%2520adds%2520shoes%252C%2520which%2520were%2520missing%2520in%2520BEDLAM.%2520BEDLAM%2520has%2520become%2520a%2520key%2520resource%2520for%2520training%25203D%2520human%2520pose%2520and%2520motion%2520regressors%2520today%2520and%2520we%2520show%2520that%2520BEDLAM2.0%2520is%2520significantly%2520better%252C%2520particularly%2520for%2520training%2520methods%2520that%2520estimate%2520humans%2520in%2520world%2520coordinates.%2520We%2520compare%2520state-of-the%2520art%2520methods%2520trained%2520on%2520BEDLAM%2520and%2520BEDLAM2.0%252C%2520and%2520find%2520that%2520BEDLAM2.0%2520significantly%2520improves%2520accuracy%2520over%2520BEDLAM.%2520For%2520research%2520purposes%252C%2520we%2520provide%2520the%2520rendered%2520videos%252C%2520ground%2520truth%2520body%2520parameters%252C%2520and%2520camera%2520motions.%2520We%2520also%2520provide%2520the%25203D%2520assets%2520to%2520which%2520we%2520have%2520rights%2520and%2520links%2520to%2520those%2520from%2520third%2520parties.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEDLAM2.0%3A%20Synthetic%20Humans%20and%20Cameras%20in%20Motion&entry.906535625=Joachim%20Tesch%20and%20Giorgio%20Becherini%20and%20Prerana%20Achar%20and%20Anastasios%20Yiannakidis%20and%20Muhammed%20Kocabas%20and%20Priyanka%20Patel%20and%20Michael%20J.%20Black&entry.1292438233=Inferring%203D%20human%20motion%20from%20video%20remains%20a%20challenging%20problem%20with%20many%20applications.%20While%20traditional%20methods%20estimate%20the%20human%20in%20image%20coordinates%2C%20many%20applications%20require%20human%20motion%20to%20be%20estimated%20in%20world%20coordinates.%20This%20is%20particularly%20challenging%20when%20there%20is%20both%20human%20and%20camera%20motion.%20Progress%20on%20this%20topic%20has%20been%20limited%20by%20the%20lack%20of%20rich%20video%20data%20with%20ground%20truth%20human%20and%20camera%20movement.%20We%20address%20this%20with%20BEDLAM2.0%2C%20a%20new%20dataset%20that%20goes%20beyond%20the%20popular%20BEDLAM%20dataset%20in%20important%20ways.%20In%20addition%20to%20introducing%20more%20diverse%20and%20realistic%20cameras%20and%20camera%20motions%2C%20BEDLAM2.0%20increases%20diversity%20and%20realism%20of%20body%20shape%2C%20motions%2C%20clothing%2C%20hair%2C%20and%203D%20environments.%20Additionally%2C%20it%20adds%20shoes%2C%20which%20were%20missing%20in%20BEDLAM.%20BEDLAM%20has%20become%20a%20key%20resource%20for%20training%203D%20human%20pose%20and%20motion%20regressors%20today%20and%20we%20show%20that%20BEDLAM2.0%20is%20significantly%20better%2C%20particularly%20for%20training%20methods%20that%20estimate%20humans%20in%20world%20coordinates.%20We%20compare%20state-of-the%20art%20methods%20trained%20on%20BEDLAM%20and%20BEDLAM2.0%2C%20and%20find%20that%20BEDLAM2.0%20significantly%20improves%20accuracy%20over%20BEDLAM.%20For%20research%20purposes%2C%20we%20provide%20the%20rendered%20videos%2C%20ground%20truth%20body%20parameters%2C%20and%20camera%20motions.%20We%20also%20provide%20the%203D%20assets%20to%20which%20we%20have%20rights%20and%20links%20to%20those%20from%20third%20parties.&entry.1838667208=http%3A//arxiv.org/abs/2511.14394v1&entry.124074799=Read"},
{"title": "Silhouette-to-Contour Registration: Aligning Intraoral Scan Models with Cephalometric Radiographs", "author": "Yiyi Miao and Taoyu Wu and Ji Jiang and Tong Chen and Zhe Tang and Zhengyong Jiang and Angelos Stefanidis and Limin Yu and Jionglong Su", "abstract": "Reliable 3D-2D alignment between intraoral scan (IOS) models and lateral cephalometric radiographs is critical for orthodontic diagnosis, yet conventional intensity-driven registration methods struggle under real clinical conditions, where cephalograms exhibit projective magnification, geometric distortion, low-contrast dental crowns, and acquisition-dependent variation. These factors hinder the stability of appearance-based similarity metrics and often lead to convergence failures or anatomically implausible alignments. To address these limitations, we propose DentalSCR, a pose-stable, contour-guided framework for accurate and interpretable silhouette-to-contour registration. Our method first constructs a U-Midline Dental Axis (UMDA) to establish a unified cross-arch anatomical coordinate system, thereby stabilizing initialization and standardizing projection geometry across cases. Using this reference frame, we generate radiograph-like projections via a surface-based DRR formulation with coronal-axis perspective and Gaussian splatting, which preserves clinical source-object-detector magnification and emphasizes external silhouettes. Registration is then formulated as a 2D similarity transform optimized with a symmetric bidirectional Chamfer distance under a hierarchical coarse-to-fine schedule, enabling both large capture range and subpixel-level contour agreement. We evaluate DentalSCR on 34 expert-annotated clinical cases. Experimental results demonstrate substantial reductions in landmark error-particularly at posterior teeth-tighter dispersion on the lower jaw, and low Chamfer and controlled Hausdorff distances at the curve level. These findings indicate that DentalSCR robustly handles real-world cephalograms and delivers high-fidelity, clinically inspectable 3D--2D alignment, outperforming conventional baselines.", "link": "http://arxiv.org/abs/2511.14343v1", "date": "2025-11-18", "relevancy": 2.4637, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5035}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4909}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Silhouette-to-Contour%20Registration%3A%20Aligning%20Intraoral%20Scan%20Models%20with%20Cephalometric%20Radiographs&body=Title%3A%20Silhouette-to-Contour%20Registration%3A%20Aligning%20Intraoral%20Scan%20Models%20with%20Cephalometric%20Radiographs%0AAuthor%3A%20Yiyi%20Miao%20and%20Taoyu%20Wu%20and%20Ji%20Jiang%20and%20Tong%20Chen%20and%20Zhe%20Tang%20and%20Zhengyong%20Jiang%20and%20Angelos%20Stefanidis%20and%20Limin%20Yu%20and%20Jionglong%20Su%0AAbstract%3A%20Reliable%203D-2D%20alignment%20between%20intraoral%20scan%20%28IOS%29%20models%20and%20lateral%20cephalometric%20radiographs%20is%20critical%20for%20orthodontic%20diagnosis%2C%20yet%20conventional%20intensity-driven%20registration%20methods%20struggle%20under%20real%20clinical%20conditions%2C%20where%20cephalograms%20exhibit%20projective%20magnification%2C%20geometric%20distortion%2C%20low-contrast%20dental%20crowns%2C%20and%20acquisition-dependent%20variation.%20These%20factors%20hinder%20the%20stability%20of%20appearance-based%20similarity%20metrics%20and%20often%20lead%20to%20convergence%20failures%20or%20anatomically%20implausible%20alignments.%20To%20address%20these%20limitations%2C%20we%20propose%20DentalSCR%2C%20a%20pose-stable%2C%20contour-guided%20framework%20for%20accurate%20and%20interpretable%20silhouette-to-contour%20registration.%20Our%20method%20first%20constructs%20a%20U-Midline%20Dental%20Axis%20%28UMDA%29%20to%20establish%20a%20unified%20cross-arch%20anatomical%20coordinate%20system%2C%20thereby%20stabilizing%20initialization%20and%20standardizing%20projection%20geometry%20across%20cases.%20Using%20this%20reference%20frame%2C%20we%20generate%20radiograph-like%20projections%20via%20a%20surface-based%20DRR%20formulation%20with%20coronal-axis%20perspective%20and%20Gaussian%20splatting%2C%20which%20preserves%20clinical%20source-object-detector%20magnification%20and%20emphasizes%20external%20silhouettes.%20Registration%20is%20then%20formulated%20as%20a%202D%20similarity%20transform%20optimized%20with%20a%20symmetric%20bidirectional%20Chamfer%20distance%20under%20a%20hierarchical%20coarse-to-fine%20schedule%2C%20enabling%20both%20large%20capture%20range%20and%20subpixel-level%20contour%20agreement.%20We%20evaluate%20DentalSCR%20on%2034%20expert-annotated%20clinical%20cases.%20Experimental%20results%20demonstrate%20substantial%20reductions%20in%20landmark%20error-particularly%20at%20posterior%20teeth-tighter%20dispersion%20on%20the%20lower%20jaw%2C%20and%20low%20Chamfer%20and%20controlled%20Hausdorff%20distances%20at%20the%20curve%20level.%20These%20findings%20indicate%20that%20DentalSCR%20robustly%20handles%20real-world%20cephalograms%20and%20delivers%20high-fidelity%2C%20clinically%20inspectable%203D--2D%20alignment%2C%20outperforming%20conventional%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSilhouette-to-Contour%2520Registration%253A%2520Aligning%2520Intraoral%2520Scan%2520Models%2520with%2520Cephalometric%2520Radiographs%26entry.906535625%3DYiyi%2520Miao%2520and%2520Taoyu%2520Wu%2520and%2520Ji%2520Jiang%2520and%2520Tong%2520Chen%2520and%2520Zhe%2520Tang%2520and%2520Zhengyong%2520Jiang%2520and%2520Angelos%2520Stefanidis%2520and%2520Limin%2520Yu%2520and%2520Jionglong%2520Su%26entry.1292438233%3DReliable%25203D-2D%2520alignment%2520between%2520intraoral%2520scan%2520%2528IOS%2529%2520models%2520and%2520lateral%2520cephalometric%2520radiographs%2520is%2520critical%2520for%2520orthodontic%2520diagnosis%252C%2520yet%2520conventional%2520intensity-driven%2520registration%2520methods%2520struggle%2520under%2520real%2520clinical%2520conditions%252C%2520where%2520cephalograms%2520exhibit%2520projective%2520magnification%252C%2520geometric%2520distortion%252C%2520low-contrast%2520dental%2520crowns%252C%2520and%2520acquisition-dependent%2520variation.%2520These%2520factors%2520hinder%2520the%2520stability%2520of%2520appearance-based%2520similarity%2520metrics%2520and%2520often%2520lead%2520to%2520convergence%2520failures%2520or%2520anatomically%2520implausible%2520alignments.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520DentalSCR%252C%2520a%2520pose-stable%252C%2520contour-guided%2520framework%2520for%2520accurate%2520and%2520interpretable%2520silhouette-to-contour%2520registration.%2520Our%2520method%2520first%2520constructs%2520a%2520U-Midline%2520Dental%2520Axis%2520%2528UMDA%2529%2520to%2520establish%2520a%2520unified%2520cross-arch%2520anatomical%2520coordinate%2520system%252C%2520thereby%2520stabilizing%2520initialization%2520and%2520standardizing%2520projection%2520geometry%2520across%2520cases.%2520Using%2520this%2520reference%2520frame%252C%2520we%2520generate%2520radiograph-like%2520projections%2520via%2520a%2520surface-based%2520DRR%2520formulation%2520with%2520coronal-axis%2520perspective%2520and%2520Gaussian%2520splatting%252C%2520which%2520preserves%2520clinical%2520source-object-detector%2520magnification%2520and%2520emphasizes%2520external%2520silhouettes.%2520Registration%2520is%2520then%2520formulated%2520as%2520a%25202D%2520similarity%2520transform%2520optimized%2520with%2520a%2520symmetric%2520bidirectional%2520Chamfer%2520distance%2520under%2520a%2520hierarchical%2520coarse-to-fine%2520schedule%252C%2520enabling%2520both%2520large%2520capture%2520range%2520and%2520subpixel-level%2520contour%2520agreement.%2520We%2520evaluate%2520DentalSCR%2520on%252034%2520expert-annotated%2520clinical%2520cases.%2520Experimental%2520results%2520demonstrate%2520substantial%2520reductions%2520in%2520landmark%2520error-particularly%2520at%2520posterior%2520teeth-tighter%2520dispersion%2520on%2520the%2520lower%2520jaw%252C%2520and%2520low%2520Chamfer%2520and%2520controlled%2520Hausdorff%2520distances%2520at%2520the%2520curve%2520level.%2520These%2520findings%2520indicate%2520that%2520DentalSCR%2520robustly%2520handles%2520real-world%2520cephalograms%2520and%2520delivers%2520high-fidelity%252C%2520clinically%2520inspectable%25203D--2D%2520alignment%252C%2520outperforming%2520conventional%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Silhouette-to-Contour%20Registration%3A%20Aligning%20Intraoral%20Scan%20Models%20with%20Cephalometric%20Radiographs&entry.906535625=Yiyi%20Miao%20and%20Taoyu%20Wu%20and%20Ji%20Jiang%20and%20Tong%20Chen%20and%20Zhe%20Tang%20and%20Zhengyong%20Jiang%20and%20Angelos%20Stefanidis%20and%20Limin%20Yu%20and%20Jionglong%20Su&entry.1292438233=Reliable%203D-2D%20alignment%20between%20intraoral%20scan%20%28IOS%29%20models%20and%20lateral%20cephalometric%20radiographs%20is%20critical%20for%20orthodontic%20diagnosis%2C%20yet%20conventional%20intensity-driven%20registration%20methods%20struggle%20under%20real%20clinical%20conditions%2C%20where%20cephalograms%20exhibit%20projective%20magnification%2C%20geometric%20distortion%2C%20low-contrast%20dental%20crowns%2C%20and%20acquisition-dependent%20variation.%20These%20factors%20hinder%20the%20stability%20of%20appearance-based%20similarity%20metrics%20and%20often%20lead%20to%20convergence%20failures%20or%20anatomically%20implausible%20alignments.%20To%20address%20these%20limitations%2C%20we%20propose%20DentalSCR%2C%20a%20pose-stable%2C%20contour-guided%20framework%20for%20accurate%20and%20interpretable%20silhouette-to-contour%20registration.%20Our%20method%20first%20constructs%20a%20U-Midline%20Dental%20Axis%20%28UMDA%29%20to%20establish%20a%20unified%20cross-arch%20anatomical%20coordinate%20system%2C%20thereby%20stabilizing%20initialization%20and%20standardizing%20projection%20geometry%20across%20cases.%20Using%20this%20reference%20frame%2C%20we%20generate%20radiograph-like%20projections%20via%20a%20surface-based%20DRR%20formulation%20with%20coronal-axis%20perspective%20and%20Gaussian%20splatting%2C%20which%20preserves%20clinical%20source-object-detector%20magnification%20and%20emphasizes%20external%20silhouettes.%20Registration%20is%20then%20formulated%20as%20a%202D%20similarity%20transform%20optimized%20with%20a%20symmetric%20bidirectional%20Chamfer%20distance%20under%20a%20hierarchical%20coarse-to-fine%20schedule%2C%20enabling%20both%20large%20capture%20range%20and%20subpixel-level%20contour%20agreement.%20We%20evaluate%20DentalSCR%20on%2034%20expert-annotated%20clinical%20cases.%20Experimental%20results%20demonstrate%20substantial%20reductions%20in%20landmark%20error-particularly%20at%20posterior%20teeth-tighter%20dispersion%20on%20the%20lower%20jaw%2C%20and%20low%20Chamfer%20and%20controlled%20Hausdorff%20distances%20at%20the%20curve%20level.%20These%20findings%20indicate%20that%20DentalSCR%20robustly%20handles%20real-world%20cephalograms%20and%20delivers%20high-fidelity%2C%20clinically%20inspectable%203D--2D%20alignment%2C%20outperforming%20conventional%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2511.14343v1&entry.124074799=Read"},
{"title": "Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL", "author": "Zijin Hong and Zheng Yuan and Qinggang Zhang and Hao Chen and Junnan Dong and Feiran Huang and Xiao Huang", "abstract": "Generating accurate SQL from users' natural language questions (text-to-SQL) remains a long-standing challenge due to the complexities involved in user question understanding, database schema comprehension, and SQL generation. Traditional text-to-SQL systems, which combine human engineering and deep neural networks, have made significant progress. Subsequently, pre-trained language models (PLMs) have been developed for text-to-SQL tasks, achieving promising results. However, as modern databases and user questions grow more complex, PLMs with a limited parameter size often produce incorrect SQL. This necessitates more sophisticated and tailored optimization methods, which restricts the application of PLM-based systems. Recently, large language models (LLMs) have shown significant capabilities in natural language understanding as model scale increases. Thus, integrating LLM-based solutions can bring unique opportunities, improvements, and solutions to text-to-SQL research. In this survey, we provide a comprehensive review of existing LLM-based text-to-SQL studies. Specifically, we offer a brief overview of the technical challenges and evolutionary process of text-to-SQL. Next, we introduce the datasets and metrics designed to evaluate text-to-SQL systems. Subsequently, we present a systematic analysis of recent advances in LLM-based text-to-SQL. Finally, we make a summarization and discuss the remaining challenges in this field and suggest expectations for future research directions. All the related resources of LLM-based, including research papers, benchmarks, and open-source projects, are collected for the community in our repository: https://github.com/DEEP-PolyU/Awesome-LLM-based-Text2SQL.", "link": "http://arxiv.org/abs/2406.08426v7", "date": "2025-11-18", "relevancy": 2.4486, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4992}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4992}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next-Generation%20Database%20Interfaces%3A%20A%20Survey%20of%20LLM-based%20Text-to-SQL&body=Title%3A%20Next-Generation%20Database%20Interfaces%3A%20A%20Survey%20of%20LLM-based%20Text-to-SQL%0AAuthor%3A%20Zijin%20Hong%20and%20Zheng%20Yuan%20and%20Qinggang%20Zhang%20and%20Hao%20Chen%20and%20Junnan%20Dong%20and%20Feiran%20Huang%20and%20Xiao%20Huang%0AAbstract%3A%20Generating%20accurate%20SQL%20from%20users%27%20natural%20language%20questions%20%28text-to-SQL%29%20remains%20a%20long-standing%20challenge%20due%20to%20the%20complexities%20involved%20in%20user%20question%20understanding%2C%20database%20schema%20comprehension%2C%20and%20SQL%20generation.%20Traditional%20text-to-SQL%20systems%2C%20which%20combine%20human%20engineering%20and%20deep%20neural%20networks%2C%20have%20made%20significant%20progress.%20Subsequently%2C%20pre-trained%20language%20models%20%28PLMs%29%20have%20been%20developed%20for%20text-to-SQL%20tasks%2C%20achieving%20promising%20results.%20However%2C%20as%20modern%20databases%20and%20user%20questions%20grow%20more%20complex%2C%20PLMs%20with%20a%20limited%20parameter%20size%20often%20produce%20incorrect%20SQL.%20This%20necessitates%20more%20sophisticated%20and%20tailored%20optimization%20methods%2C%20which%20restricts%20the%20application%20of%20PLM-based%20systems.%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20shown%20significant%20capabilities%20in%20natural%20language%20understanding%20as%20model%20scale%20increases.%20Thus%2C%20integrating%20LLM-based%20solutions%20can%20bring%20unique%20opportunities%2C%20improvements%2C%20and%20solutions%20to%20text-to-SQL%20research.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20review%20of%20existing%20LLM-based%20text-to-SQL%20studies.%20Specifically%2C%20we%20offer%20a%20brief%20overview%20of%20the%20technical%20challenges%20and%20evolutionary%20process%20of%20text-to-SQL.%20Next%2C%20we%20introduce%20the%20datasets%20and%20metrics%20designed%20to%20evaluate%20text-to-SQL%20systems.%20Subsequently%2C%20we%20present%20a%20systematic%20analysis%20of%20recent%20advances%20in%20LLM-based%20text-to-SQL.%20Finally%2C%20we%20make%20a%20summarization%20and%20discuss%20the%20remaining%20challenges%20in%20this%20field%20and%20suggest%20expectations%20for%20future%20research%20directions.%20All%20the%20related%20resources%20of%20LLM-based%2C%20including%20research%20papers%2C%20benchmarks%2C%20and%20open-source%20projects%2C%20are%20collected%20for%20the%20community%20in%20our%20repository%3A%20https%3A//github.com/DEEP-PolyU/Awesome-LLM-based-Text2SQL.%0ALink%3A%20http%3A//arxiv.org/abs/2406.08426v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext-Generation%2520Database%2520Interfaces%253A%2520A%2520Survey%2520of%2520LLM-based%2520Text-to-SQL%26entry.906535625%3DZijin%2520Hong%2520and%2520Zheng%2520Yuan%2520and%2520Qinggang%2520Zhang%2520and%2520Hao%2520Chen%2520and%2520Junnan%2520Dong%2520and%2520Feiran%2520Huang%2520and%2520Xiao%2520Huang%26entry.1292438233%3DGenerating%2520accurate%2520SQL%2520from%2520users%2527%2520natural%2520language%2520questions%2520%2528text-to-SQL%2529%2520remains%2520a%2520long-standing%2520challenge%2520due%2520to%2520the%2520complexities%2520involved%2520in%2520user%2520question%2520understanding%252C%2520database%2520schema%2520comprehension%252C%2520and%2520SQL%2520generation.%2520Traditional%2520text-to-SQL%2520systems%252C%2520which%2520combine%2520human%2520engineering%2520and%2520deep%2520neural%2520networks%252C%2520have%2520made%2520significant%2520progress.%2520Subsequently%252C%2520pre-trained%2520language%2520models%2520%2528PLMs%2529%2520have%2520been%2520developed%2520for%2520text-to-SQL%2520tasks%252C%2520achieving%2520promising%2520results.%2520However%252C%2520as%2520modern%2520databases%2520and%2520user%2520questions%2520grow%2520more%2520complex%252C%2520PLMs%2520with%2520a%2520limited%2520parameter%2520size%2520often%2520produce%2520incorrect%2520SQL.%2520This%2520necessitates%2520more%2520sophisticated%2520and%2520tailored%2520optimization%2520methods%252C%2520which%2520restricts%2520the%2520application%2520of%2520PLM-based%2520systems.%2520Recently%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520significant%2520capabilities%2520in%2520natural%2520language%2520understanding%2520as%2520model%2520scale%2520increases.%2520Thus%252C%2520integrating%2520LLM-based%2520solutions%2520can%2520bring%2520unique%2520opportunities%252C%2520improvements%252C%2520and%2520solutions%2520to%2520text-to-SQL%2520research.%2520In%2520this%2520survey%252C%2520we%2520provide%2520a%2520comprehensive%2520review%2520of%2520existing%2520LLM-based%2520text-to-SQL%2520studies.%2520Specifically%252C%2520we%2520offer%2520a%2520brief%2520overview%2520of%2520the%2520technical%2520challenges%2520and%2520evolutionary%2520process%2520of%2520text-to-SQL.%2520Next%252C%2520we%2520introduce%2520the%2520datasets%2520and%2520metrics%2520designed%2520to%2520evaluate%2520text-to-SQL%2520systems.%2520Subsequently%252C%2520we%2520present%2520a%2520systematic%2520analysis%2520of%2520recent%2520advances%2520in%2520LLM-based%2520text-to-SQL.%2520Finally%252C%2520we%2520make%2520a%2520summarization%2520and%2520discuss%2520the%2520remaining%2520challenges%2520in%2520this%2520field%2520and%2520suggest%2520expectations%2520for%2520future%2520research%2520directions.%2520All%2520the%2520related%2520resources%2520of%2520LLM-based%252C%2520including%2520research%2520papers%252C%2520benchmarks%252C%2520and%2520open-source%2520projects%252C%2520are%2520collected%2520for%2520the%2520community%2520in%2520our%2520repository%253A%2520https%253A//github.com/DEEP-PolyU/Awesome-LLM-based-Text2SQL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08426v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next-Generation%20Database%20Interfaces%3A%20A%20Survey%20of%20LLM-based%20Text-to-SQL&entry.906535625=Zijin%20Hong%20and%20Zheng%20Yuan%20and%20Qinggang%20Zhang%20and%20Hao%20Chen%20and%20Junnan%20Dong%20and%20Feiran%20Huang%20and%20Xiao%20Huang&entry.1292438233=Generating%20accurate%20SQL%20from%20users%27%20natural%20language%20questions%20%28text-to-SQL%29%20remains%20a%20long-standing%20challenge%20due%20to%20the%20complexities%20involved%20in%20user%20question%20understanding%2C%20database%20schema%20comprehension%2C%20and%20SQL%20generation.%20Traditional%20text-to-SQL%20systems%2C%20which%20combine%20human%20engineering%20and%20deep%20neural%20networks%2C%20have%20made%20significant%20progress.%20Subsequently%2C%20pre-trained%20language%20models%20%28PLMs%29%20have%20been%20developed%20for%20text-to-SQL%20tasks%2C%20achieving%20promising%20results.%20However%2C%20as%20modern%20databases%20and%20user%20questions%20grow%20more%20complex%2C%20PLMs%20with%20a%20limited%20parameter%20size%20often%20produce%20incorrect%20SQL.%20This%20necessitates%20more%20sophisticated%20and%20tailored%20optimization%20methods%2C%20which%20restricts%20the%20application%20of%20PLM-based%20systems.%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20shown%20significant%20capabilities%20in%20natural%20language%20understanding%20as%20model%20scale%20increases.%20Thus%2C%20integrating%20LLM-based%20solutions%20can%20bring%20unique%20opportunities%2C%20improvements%2C%20and%20solutions%20to%20text-to-SQL%20research.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20review%20of%20existing%20LLM-based%20text-to-SQL%20studies.%20Specifically%2C%20we%20offer%20a%20brief%20overview%20of%20the%20technical%20challenges%20and%20evolutionary%20process%20of%20text-to-SQL.%20Next%2C%20we%20introduce%20the%20datasets%20and%20metrics%20designed%20to%20evaluate%20text-to-SQL%20systems.%20Subsequently%2C%20we%20present%20a%20systematic%20analysis%20of%20recent%20advances%20in%20LLM-based%20text-to-SQL.%20Finally%2C%20we%20make%20a%20summarization%20and%20discuss%20the%20remaining%20challenges%20in%20this%20field%20and%20suggest%20expectations%20for%20future%20research%20directions.%20All%20the%20related%20resources%20of%20LLM-based%2C%20including%20research%20papers%2C%20benchmarks%2C%20and%20open-source%20projects%2C%20are%20collected%20for%20the%20community%20in%20our%20repository%3A%20https%3A//github.com/DEEP-PolyU/Awesome-LLM-based-Text2SQL.&entry.1838667208=http%3A//arxiv.org/abs/2406.08426v7&entry.124074799=Read"},
{"title": "Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images", "author": "Farheen Ramzan and Yusuf Kiberu and Nikesh Jathanna and Meryem Jabrane and Vicente Grau and Shahnaz Jamil-Copley and Richard H. Clayton and  Chen and  Chen", "abstract": "Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to \"see beyond the image\", setting a new direction for robust and physiologically grounded cardiac scar segmentation.", "link": "http://arxiv.org/abs/2511.14702v1", "date": "2025-11-18", "relevancy": 2.4452, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4939}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4908}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20Beyond%20the%20Image%3A%20ECG%20and%20Anatomical%20Knowledge-Guided%20Myocardial%20Scar%20Segmentation%20from%20Late%20Gadolinium-Enhanced%20Images&body=Title%3A%20Seeing%20Beyond%20the%20Image%3A%20ECG%20and%20Anatomical%20Knowledge-Guided%20Myocardial%20Scar%20Segmentation%20from%20Late%20Gadolinium-Enhanced%20Images%0AAuthor%3A%20Farheen%20Ramzan%20and%20Yusuf%20Kiberu%20and%20Nikesh%20Jathanna%20and%20Meryem%20Jabrane%20and%20Vicente%20Grau%20and%20Shahnaz%20Jamil-Copley%20and%20Richard%20H.%20Clayton%20and%20%20Chen%20and%20%20Chen%0AAbstract%3A%20Accurate%20segmentation%20of%20myocardial%20scar%20from%20late%20gadolinium%20enhanced%20%28LGE%29%20cardiac%20MRI%20is%20essential%20for%20evaluating%20tissue%20viability%2C%20yet%20remains%20challenging%20due%20to%20variable%20contrast%20and%20imaging%20artifacts.%20Electrocardiogram%20%28ECG%29%20signals%20provide%20complementary%20physiological%20information%2C%20as%20conduction%20abnormalities%20can%20help%20localize%20or%20suggest%20scarred%20myocardial%20regions.%20In%20this%20work%2C%20we%20propose%20a%20novel%20multimodal%20framework%20that%20integrates%20ECG-derived%20electrophysiological%20information%20with%20anatomical%20priors%20from%20the%20AHA-17%20atlas%20for%20physiologically%20consistent%20LGE-based%20scar%20segmentation.%20As%20ECGs%20and%20LGE-MRIs%20are%20not%20acquired%20simultaneously%2C%20we%20introduce%20a%20Temporal%20Aware%20Feature%20Fusion%20%28TAFF%29%20mechanism%20that%20dynamically%20weights%20and%20fuses%20features%20based%20on%20their%20acquisition%20time%20difference.%20Our%20method%20was%20evaluated%20on%20a%20clinical%20dataset%20and%20achieved%20substantial%20gains%20over%20the%20state-of-the-art%20image-only%20baseline%20%28nnU-Net%29%2C%20increasing%20the%20average%20Dice%20score%20for%20scars%20from%200.6149%20to%200.8463%20and%20achieving%20high%20performance%20in%20both%20precision%20%280.9115%29%20and%20sensitivity%20%280.9043%29.%20These%20results%20show%20that%20integrating%20physiological%20and%20anatomical%20knowledge%20allows%20the%20model%20to%20%22see%20beyond%20the%20image%22%2C%20setting%20a%20new%20direction%20for%20robust%20and%20physiologically%20grounded%20cardiac%20scar%20segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520Beyond%2520the%2520Image%253A%2520ECG%2520and%2520Anatomical%2520Knowledge-Guided%2520Myocardial%2520Scar%2520Segmentation%2520from%2520Late%2520Gadolinium-Enhanced%2520Images%26entry.906535625%3DFarheen%2520Ramzan%2520and%2520Yusuf%2520Kiberu%2520and%2520Nikesh%2520Jathanna%2520and%2520Meryem%2520Jabrane%2520and%2520Vicente%2520Grau%2520and%2520Shahnaz%2520Jamil-Copley%2520and%2520Richard%2520H.%2520Clayton%2520and%2520%2520Chen%2520and%2520%2520Chen%26entry.1292438233%3DAccurate%2520segmentation%2520of%2520myocardial%2520scar%2520from%2520late%2520gadolinium%2520enhanced%2520%2528LGE%2529%2520cardiac%2520MRI%2520is%2520essential%2520for%2520evaluating%2520tissue%2520viability%252C%2520yet%2520remains%2520challenging%2520due%2520to%2520variable%2520contrast%2520and%2520imaging%2520artifacts.%2520Electrocardiogram%2520%2528ECG%2529%2520signals%2520provide%2520complementary%2520physiological%2520information%252C%2520as%2520conduction%2520abnormalities%2520can%2520help%2520localize%2520or%2520suggest%2520scarred%2520myocardial%2520regions.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520multimodal%2520framework%2520that%2520integrates%2520ECG-derived%2520electrophysiological%2520information%2520with%2520anatomical%2520priors%2520from%2520the%2520AHA-17%2520atlas%2520for%2520physiologically%2520consistent%2520LGE-based%2520scar%2520segmentation.%2520As%2520ECGs%2520and%2520LGE-MRIs%2520are%2520not%2520acquired%2520simultaneously%252C%2520we%2520introduce%2520a%2520Temporal%2520Aware%2520Feature%2520Fusion%2520%2528TAFF%2529%2520mechanism%2520that%2520dynamically%2520weights%2520and%2520fuses%2520features%2520based%2520on%2520their%2520acquisition%2520time%2520difference.%2520Our%2520method%2520was%2520evaluated%2520on%2520a%2520clinical%2520dataset%2520and%2520achieved%2520substantial%2520gains%2520over%2520the%2520state-of-the-art%2520image-only%2520baseline%2520%2528nnU-Net%2529%252C%2520increasing%2520the%2520average%2520Dice%2520score%2520for%2520scars%2520from%25200.6149%2520to%25200.8463%2520and%2520achieving%2520high%2520performance%2520in%2520both%2520precision%2520%25280.9115%2529%2520and%2520sensitivity%2520%25280.9043%2529.%2520These%2520results%2520show%2520that%2520integrating%2520physiological%2520and%2520anatomical%2520knowledge%2520allows%2520the%2520model%2520to%2520%2522see%2520beyond%2520the%2520image%2522%252C%2520setting%2520a%2520new%2520direction%2520for%2520robust%2520and%2520physiologically%2520grounded%2520cardiac%2520scar%2520segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20Beyond%20the%20Image%3A%20ECG%20and%20Anatomical%20Knowledge-Guided%20Myocardial%20Scar%20Segmentation%20from%20Late%20Gadolinium-Enhanced%20Images&entry.906535625=Farheen%20Ramzan%20and%20Yusuf%20Kiberu%20and%20Nikesh%20Jathanna%20and%20Meryem%20Jabrane%20and%20Vicente%20Grau%20and%20Shahnaz%20Jamil-Copley%20and%20Richard%20H.%20Clayton%20and%20%20Chen%20and%20%20Chen&entry.1292438233=Accurate%20segmentation%20of%20myocardial%20scar%20from%20late%20gadolinium%20enhanced%20%28LGE%29%20cardiac%20MRI%20is%20essential%20for%20evaluating%20tissue%20viability%2C%20yet%20remains%20challenging%20due%20to%20variable%20contrast%20and%20imaging%20artifacts.%20Electrocardiogram%20%28ECG%29%20signals%20provide%20complementary%20physiological%20information%2C%20as%20conduction%20abnormalities%20can%20help%20localize%20or%20suggest%20scarred%20myocardial%20regions.%20In%20this%20work%2C%20we%20propose%20a%20novel%20multimodal%20framework%20that%20integrates%20ECG-derived%20electrophysiological%20information%20with%20anatomical%20priors%20from%20the%20AHA-17%20atlas%20for%20physiologically%20consistent%20LGE-based%20scar%20segmentation.%20As%20ECGs%20and%20LGE-MRIs%20are%20not%20acquired%20simultaneously%2C%20we%20introduce%20a%20Temporal%20Aware%20Feature%20Fusion%20%28TAFF%29%20mechanism%20that%20dynamically%20weights%20and%20fuses%20features%20based%20on%20their%20acquisition%20time%20difference.%20Our%20method%20was%20evaluated%20on%20a%20clinical%20dataset%20and%20achieved%20substantial%20gains%20over%20the%20state-of-the-art%20image-only%20baseline%20%28nnU-Net%29%2C%20increasing%20the%20average%20Dice%20score%20for%20scars%20from%200.6149%20to%200.8463%20and%20achieving%20high%20performance%20in%20both%20precision%20%280.9115%29%20and%20sensitivity%20%280.9043%29.%20These%20results%20show%20that%20integrating%20physiological%20and%20anatomical%20knowledge%20allows%20the%20model%20to%20%22see%20beyond%20the%20image%22%2C%20setting%20a%20new%20direction%20for%20robust%20and%20physiologically%20grounded%20cardiac%20scar%20segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2511.14702v1&entry.124074799=Read"},
{"title": "4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration", "author": "Jiahui Zhang and Yurui Chen and Yueming Xu and Ze Huang and Yanpeng Zhou and Yu-Jie Yuan and Xinyue Cai and Guowei Huang and Xingyue Quan and Hang Xu and Li Zhang", "abstract": "Leveraging diverse robotic data for pretraining remains a critical challenge. Existing methods typically model the dataset's action distribution using simple observations as inputs. However, these inputs are often incomplete, resulting in a dispersed conditional action distribution-an issue we refer to as coordinate system chaos and state chaos. This inconsistency significantly hampers pretraining efficiency. To address this, we propose 4D-VLA, a novel approach that effectively integrates 4D information into the input to mitigate these sources of chaos. Our model introduces depth and temporal information into visual features with sequential RGB-D inputs, aligning the coordinate systems of the robot and the scene. This alignment endows the model with strong spatiotemporal reasoning capabilities while minimizing training overhead. Additionally, we introduce memory bank sampling, a frame sampling strategy designed to extract informative frames from historical images, further improving effectiveness and efficiency. Experimental results demonstrate that our pretraining method and architectural components substantially enhance model performance. In both simulated and real-world experiments, our model achieves a significant increase in success rate over OpenVLA. To further assess spatial perception and generalization to novel views, we introduce MV-Bench, a multi-view simulation benchmark. Our model consistently outperforms existing methods, demonstrating stronger spatial understanding and adaptability.", "link": "http://arxiv.org/abs/2506.22242v2", "date": "2025-11-18", "relevancy": 2.4282, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6115}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6062}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204D-VLA%3A%20Spatiotemporal%20Vision-Language-Action%20Pretraining%20with%20Cross-Scene%20Calibration&body=Title%3A%204D-VLA%3A%20Spatiotemporal%20Vision-Language-Action%20Pretraining%20with%20Cross-Scene%20Calibration%0AAuthor%3A%20Jiahui%20Zhang%20and%20Yurui%20Chen%20and%20Yueming%20Xu%20and%20Ze%20Huang%20and%20Yanpeng%20Zhou%20and%20Yu-Jie%20Yuan%20and%20Xinyue%20Cai%20and%20Guowei%20Huang%20and%20Xingyue%20Quan%20and%20Hang%20Xu%20and%20Li%20Zhang%0AAbstract%3A%20Leveraging%20diverse%20robotic%20data%20for%20pretraining%20remains%20a%20critical%20challenge.%20Existing%20methods%20typically%20model%20the%20dataset%27s%20action%20distribution%20using%20simple%20observations%20as%20inputs.%20However%2C%20these%20inputs%20are%20often%20incomplete%2C%20resulting%20in%20a%20dispersed%20conditional%20action%20distribution-an%20issue%20we%20refer%20to%20as%20coordinate%20system%20chaos%20and%20state%20chaos.%20This%20inconsistency%20significantly%20hampers%20pretraining%20efficiency.%20To%20address%20this%2C%20we%20propose%204D-VLA%2C%20a%20novel%20approach%20that%20effectively%20integrates%204D%20information%20into%20the%20input%20to%20mitigate%20these%20sources%20of%20chaos.%20Our%20model%20introduces%20depth%20and%20temporal%20information%20into%20visual%20features%20with%20sequential%20RGB-D%20inputs%2C%20aligning%20the%20coordinate%20systems%20of%20the%20robot%20and%20the%20scene.%20This%20alignment%20endows%20the%20model%20with%20strong%20spatiotemporal%20reasoning%20capabilities%20while%20minimizing%20training%20overhead.%20Additionally%2C%20we%20introduce%20memory%20bank%20sampling%2C%20a%20frame%20sampling%20strategy%20designed%20to%20extract%20informative%20frames%20from%20historical%20images%2C%20further%20improving%20effectiveness%20and%20efficiency.%20Experimental%20results%20demonstrate%20that%20our%20pretraining%20method%20and%20architectural%20components%20substantially%20enhance%20model%20performance.%20In%20both%20simulated%20and%20real-world%20experiments%2C%20our%20model%20achieves%20a%20significant%20increase%20in%20success%20rate%20over%20OpenVLA.%20To%20further%20assess%20spatial%20perception%20and%20generalization%20to%20novel%20views%2C%20we%20introduce%20MV-Bench%2C%20a%20multi-view%20simulation%20benchmark.%20Our%20model%20consistently%20outperforms%20existing%20methods%2C%20demonstrating%20stronger%20spatial%20understanding%20and%20adaptability.%0ALink%3A%20http%3A//arxiv.org/abs/2506.22242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4D-VLA%253A%2520Spatiotemporal%2520Vision-Language-Action%2520Pretraining%2520with%2520Cross-Scene%2520Calibration%26entry.906535625%3DJiahui%2520Zhang%2520and%2520Yurui%2520Chen%2520and%2520Yueming%2520Xu%2520and%2520Ze%2520Huang%2520and%2520Yanpeng%2520Zhou%2520and%2520Yu-Jie%2520Yuan%2520and%2520Xinyue%2520Cai%2520and%2520Guowei%2520Huang%2520and%2520Xingyue%2520Quan%2520and%2520Hang%2520Xu%2520and%2520Li%2520Zhang%26entry.1292438233%3DLeveraging%2520diverse%2520robotic%2520data%2520for%2520pretraining%2520remains%2520a%2520critical%2520challenge.%2520Existing%2520methods%2520typically%2520model%2520the%2520dataset%2527s%2520action%2520distribution%2520using%2520simple%2520observations%2520as%2520inputs.%2520However%252C%2520these%2520inputs%2520are%2520often%2520incomplete%252C%2520resulting%2520in%2520a%2520dispersed%2520conditional%2520action%2520distribution-an%2520issue%2520we%2520refer%2520to%2520as%2520coordinate%2520system%2520chaos%2520and%2520state%2520chaos.%2520This%2520inconsistency%2520significantly%2520hampers%2520pretraining%2520efficiency.%2520To%2520address%2520this%252C%2520we%2520propose%25204D-VLA%252C%2520a%2520novel%2520approach%2520that%2520effectively%2520integrates%25204D%2520information%2520into%2520the%2520input%2520to%2520mitigate%2520these%2520sources%2520of%2520chaos.%2520Our%2520model%2520introduces%2520depth%2520and%2520temporal%2520information%2520into%2520visual%2520features%2520with%2520sequential%2520RGB-D%2520inputs%252C%2520aligning%2520the%2520coordinate%2520systems%2520of%2520the%2520robot%2520and%2520the%2520scene.%2520This%2520alignment%2520endows%2520the%2520model%2520with%2520strong%2520spatiotemporal%2520reasoning%2520capabilities%2520while%2520minimizing%2520training%2520overhead.%2520Additionally%252C%2520we%2520introduce%2520memory%2520bank%2520sampling%252C%2520a%2520frame%2520sampling%2520strategy%2520designed%2520to%2520extract%2520informative%2520frames%2520from%2520historical%2520images%252C%2520further%2520improving%2520effectiveness%2520and%2520efficiency.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520pretraining%2520method%2520and%2520architectural%2520components%2520substantially%2520enhance%2520model%2520performance.%2520In%2520both%2520simulated%2520and%2520real-world%2520experiments%252C%2520our%2520model%2520achieves%2520a%2520significant%2520increase%2520in%2520success%2520rate%2520over%2520OpenVLA.%2520To%2520further%2520assess%2520spatial%2520perception%2520and%2520generalization%2520to%2520novel%2520views%252C%2520we%2520introduce%2520MV-Bench%252C%2520a%2520multi-view%2520simulation%2520benchmark.%2520Our%2520model%2520consistently%2520outperforms%2520existing%2520methods%252C%2520demonstrating%2520stronger%2520spatial%2520understanding%2520and%2520adaptability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.22242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4D-VLA%3A%20Spatiotemporal%20Vision-Language-Action%20Pretraining%20with%20Cross-Scene%20Calibration&entry.906535625=Jiahui%20Zhang%20and%20Yurui%20Chen%20and%20Yueming%20Xu%20and%20Ze%20Huang%20and%20Yanpeng%20Zhou%20and%20Yu-Jie%20Yuan%20and%20Xinyue%20Cai%20and%20Guowei%20Huang%20and%20Xingyue%20Quan%20and%20Hang%20Xu%20and%20Li%20Zhang&entry.1292438233=Leveraging%20diverse%20robotic%20data%20for%20pretraining%20remains%20a%20critical%20challenge.%20Existing%20methods%20typically%20model%20the%20dataset%27s%20action%20distribution%20using%20simple%20observations%20as%20inputs.%20However%2C%20these%20inputs%20are%20often%20incomplete%2C%20resulting%20in%20a%20dispersed%20conditional%20action%20distribution-an%20issue%20we%20refer%20to%20as%20coordinate%20system%20chaos%20and%20state%20chaos.%20This%20inconsistency%20significantly%20hampers%20pretraining%20efficiency.%20To%20address%20this%2C%20we%20propose%204D-VLA%2C%20a%20novel%20approach%20that%20effectively%20integrates%204D%20information%20into%20the%20input%20to%20mitigate%20these%20sources%20of%20chaos.%20Our%20model%20introduces%20depth%20and%20temporal%20information%20into%20visual%20features%20with%20sequential%20RGB-D%20inputs%2C%20aligning%20the%20coordinate%20systems%20of%20the%20robot%20and%20the%20scene.%20This%20alignment%20endows%20the%20model%20with%20strong%20spatiotemporal%20reasoning%20capabilities%20while%20minimizing%20training%20overhead.%20Additionally%2C%20we%20introduce%20memory%20bank%20sampling%2C%20a%20frame%20sampling%20strategy%20designed%20to%20extract%20informative%20frames%20from%20historical%20images%2C%20further%20improving%20effectiveness%20and%20efficiency.%20Experimental%20results%20demonstrate%20that%20our%20pretraining%20method%20and%20architectural%20components%20substantially%20enhance%20model%20performance.%20In%20both%20simulated%20and%20real-world%20experiments%2C%20our%20model%20achieves%20a%20significant%20increase%20in%20success%20rate%20over%20OpenVLA.%20To%20further%20assess%20spatial%20perception%20and%20generalization%20to%20novel%20views%2C%20we%20introduce%20MV-Bench%2C%20a%20multi-view%20simulation%20benchmark.%20Our%20model%20consistently%20outperforms%20existing%20methods%2C%20demonstrating%20stronger%20spatial%20understanding%20and%20adaptability.&entry.1838667208=http%3A//arxiv.org/abs/2506.22242v2&entry.124074799=Read"},
{"title": "MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning", "author": "Yizhen Yin and Yuhua Qi and Dapeng Feng and Hongbo Chen and Hongjun Ma and Jin Wu and Yi Jiang", "abstract": "Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system's movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community. While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery. In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments. In pursuit of this objective, we put forward a novel structured map representation. By discretizing the spatial data and integrating the boundary points and the historical trajectory, the structured map succinctly and effectively encapsulates the visited regions, thereby serving as input for the deep reinforcement learning based decision module. Instead of sequentially predicting the next action step within the decision module, we have implemented an advanced global planner to optimize the exploration path by leveraging long-range target points. We conducted experiments in three simulation environments and deployed in a real unmanned ground vehicle (UGV), the results demonstrate that our approach significantly reduces both the duration and distance of exploration compared with state-of-the-art methods.", "link": "http://arxiv.org/abs/2511.14330v1", "date": "2025-11-18", "relevancy": 2.4205, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6163}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6014}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MA-SLAM%3A%20Active%20SLAM%20in%20Large-Scale%20Unknown%20Environment%20using%20Map%20Aware%20Deep%20Reinforcement%20Learning&body=Title%3A%20MA-SLAM%3A%20Active%20SLAM%20in%20Large-Scale%20Unknown%20Environment%20using%20Map%20Aware%20Deep%20Reinforcement%20Learning%0AAuthor%3A%20Yizhen%20Yin%20and%20Yuhua%20Qi%20and%20Dapeng%20Feng%20and%20Hongbo%20Chen%20and%20Hongjun%20Ma%20and%20Jin%20Wu%20and%20Yi%20Jiang%0AAbstract%3A%20Active%20Simultaneous%20Localization%20and%20Mapping%20%28Active%20SLAM%29%20involves%20the%20strategic%20planning%20and%20precise%20control%20of%20a%20robotic%20system%27s%20movement%20in%20order%20to%20construct%20a%20highly%20accurate%20and%20comprehensive%20representation%20of%20its%20surrounding%20environment%2C%20which%20has%20garnered%20significant%20attention%20within%20the%20research%20community.%20While%20the%20current%20methods%20demonstrate%20efficacy%20in%20small%20and%20controlled%20settings%2C%20they%20face%20challenges%20when%20applied%20to%20large-scale%20and%20diverse%20environments%2C%20marked%20by%20extended%20periods%20of%20exploration%20and%20suboptimal%20paths%20of%20discovery.%20In%20this%20paper%2C%20we%20propose%20MA-SLAM%2C%20a%20Map-Aware%20Active%20SLAM%20system%20based%20on%20Deep%20Reinforcement%20Learning%20%28DRL%29%2C%20designed%20to%20address%20the%20challenge%20of%20efficient%20exploration%20in%20large-scale%20environments.%20In%20pursuit%20of%20this%20objective%2C%20we%20put%20forward%20a%20novel%20structured%20map%20representation.%20By%20discretizing%20the%20spatial%20data%20and%20integrating%20the%20boundary%20points%20and%20the%20historical%20trajectory%2C%20the%20structured%20map%20succinctly%20and%20effectively%20encapsulates%20the%20visited%20regions%2C%20thereby%20serving%20as%20input%20for%20the%20deep%20reinforcement%20learning%20based%20decision%20module.%20Instead%20of%20sequentially%20predicting%20the%20next%20action%20step%20within%20the%20decision%20module%2C%20we%20have%20implemented%20an%20advanced%20global%20planner%20to%20optimize%20the%20exploration%20path%20by%20leveraging%20long-range%20target%20points.%20We%20conducted%20experiments%20in%20three%20simulation%20environments%20and%20deployed%20in%20a%20real%20unmanned%20ground%20vehicle%20%28UGV%29%2C%20the%20results%20demonstrate%20that%20our%20approach%20significantly%20reduces%20both%20the%20duration%20and%20distance%20of%20exploration%20compared%20with%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMA-SLAM%253A%2520Active%2520SLAM%2520in%2520Large-Scale%2520Unknown%2520Environment%2520using%2520Map%2520Aware%2520Deep%2520Reinforcement%2520Learning%26entry.906535625%3DYizhen%2520Yin%2520and%2520Yuhua%2520Qi%2520and%2520Dapeng%2520Feng%2520and%2520Hongbo%2520Chen%2520and%2520Hongjun%2520Ma%2520and%2520Jin%2520Wu%2520and%2520Yi%2520Jiang%26entry.1292438233%3DActive%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528Active%2520SLAM%2529%2520involves%2520the%2520strategic%2520planning%2520and%2520precise%2520control%2520of%2520a%2520robotic%2520system%2527s%2520movement%2520in%2520order%2520to%2520construct%2520a%2520highly%2520accurate%2520and%2520comprehensive%2520representation%2520of%2520its%2520surrounding%2520environment%252C%2520which%2520has%2520garnered%2520significant%2520attention%2520within%2520the%2520research%2520community.%2520While%2520the%2520current%2520methods%2520demonstrate%2520efficacy%2520in%2520small%2520and%2520controlled%2520settings%252C%2520they%2520face%2520challenges%2520when%2520applied%2520to%2520large-scale%2520and%2520diverse%2520environments%252C%2520marked%2520by%2520extended%2520periods%2520of%2520exploration%2520and%2520suboptimal%2520paths%2520of%2520discovery.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MA-SLAM%252C%2520a%2520Map-Aware%2520Active%2520SLAM%2520system%2520based%2520on%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%252C%2520designed%2520to%2520address%2520the%2520challenge%2520of%2520efficient%2520exploration%2520in%2520large-scale%2520environments.%2520In%2520pursuit%2520of%2520this%2520objective%252C%2520we%2520put%2520forward%2520a%2520novel%2520structured%2520map%2520representation.%2520By%2520discretizing%2520the%2520spatial%2520data%2520and%2520integrating%2520the%2520boundary%2520points%2520and%2520the%2520historical%2520trajectory%252C%2520the%2520structured%2520map%2520succinctly%2520and%2520effectively%2520encapsulates%2520the%2520visited%2520regions%252C%2520thereby%2520serving%2520as%2520input%2520for%2520the%2520deep%2520reinforcement%2520learning%2520based%2520decision%2520module.%2520Instead%2520of%2520sequentially%2520predicting%2520the%2520next%2520action%2520step%2520within%2520the%2520decision%2520module%252C%2520we%2520have%2520implemented%2520an%2520advanced%2520global%2520planner%2520to%2520optimize%2520the%2520exploration%2520path%2520by%2520leveraging%2520long-range%2520target%2520points.%2520We%2520conducted%2520experiments%2520in%2520three%2520simulation%2520environments%2520and%2520deployed%2520in%2520a%2520real%2520unmanned%2520ground%2520vehicle%2520%2528UGV%2529%252C%2520the%2520results%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520reduces%2520both%2520the%2520duration%2520and%2520distance%2520of%2520exploration%2520compared%2520with%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MA-SLAM%3A%20Active%20SLAM%20in%20Large-Scale%20Unknown%20Environment%20using%20Map%20Aware%20Deep%20Reinforcement%20Learning&entry.906535625=Yizhen%20Yin%20and%20Yuhua%20Qi%20and%20Dapeng%20Feng%20and%20Hongbo%20Chen%20and%20Hongjun%20Ma%20and%20Jin%20Wu%20and%20Yi%20Jiang&entry.1292438233=Active%20Simultaneous%20Localization%20and%20Mapping%20%28Active%20SLAM%29%20involves%20the%20strategic%20planning%20and%20precise%20control%20of%20a%20robotic%20system%27s%20movement%20in%20order%20to%20construct%20a%20highly%20accurate%20and%20comprehensive%20representation%20of%20its%20surrounding%20environment%2C%20which%20has%20garnered%20significant%20attention%20within%20the%20research%20community.%20While%20the%20current%20methods%20demonstrate%20efficacy%20in%20small%20and%20controlled%20settings%2C%20they%20face%20challenges%20when%20applied%20to%20large-scale%20and%20diverse%20environments%2C%20marked%20by%20extended%20periods%20of%20exploration%20and%20suboptimal%20paths%20of%20discovery.%20In%20this%20paper%2C%20we%20propose%20MA-SLAM%2C%20a%20Map-Aware%20Active%20SLAM%20system%20based%20on%20Deep%20Reinforcement%20Learning%20%28DRL%29%2C%20designed%20to%20address%20the%20challenge%20of%20efficient%20exploration%20in%20large-scale%20environments.%20In%20pursuit%20of%20this%20objective%2C%20we%20put%20forward%20a%20novel%20structured%20map%20representation.%20By%20discretizing%20the%20spatial%20data%20and%20integrating%20the%20boundary%20points%20and%20the%20historical%20trajectory%2C%20the%20structured%20map%20succinctly%20and%20effectively%20encapsulates%20the%20visited%20regions%2C%20thereby%20serving%20as%20input%20for%20the%20deep%20reinforcement%20learning%20based%20decision%20module.%20Instead%20of%20sequentially%20predicting%20the%20next%20action%20step%20within%20the%20decision%20module%2C%20we%20have%20implemented%20an%20advanced%20global%20planner%20to%20optimize%20the%20exploration%20path%20by%20leveraging%20long-range%20target%20points.%20We%20conducted%20experiments%20in%20three%20simulation%20environments%20and%20deployed%20in%20a%20real%20unmanned%20ground%20vehicle%20%28UGV%29%2C%20the%20results%20demonstrate%20that%20our%20approach%20significantly%20reduces%20both%20the%20duration%20and%20distance%20of%20exploration%20compared%20with%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.14330v1&entry.124074799=Read"},
{"title": "Gallant: Voxel Grid-based Humanoid Locomotion and Local-navigation across 3D Constrained Terrains", "author": "Qingwei Ben and Botian Xu and Kailin Li and Feiyu Jia and Wentao Zhang and Jingping Wang and Jingbo Wang and Dahua Lin and Jiangmiao Pang", "abstract": "Robust humanoid locomotion requires accurate and globally consistent perception of the surrounding 3D environment. However, existing perception modules, mainly based on depth images or elevation maps, offer only partial and locally flattened views of the environment, failing to capture the full 3D structure. This paper presents Gallant, a voxel-grid-based framework for humanoid locomotion and local navigation in 3D constrained terrains. It leverages voxelized LiDAR data as a lightweight and structured perceptual representation, and employs a z-grouped 2D CNN to map this representation to the control policy, enabling fully end-to-end optimization. A high-fidelity LiDAR simulation that dynamically generates realistic observations is developed to support scalable, LiDAR-based training and ensure sim-to-real consistency. Experimental results show that Gallant's broader perceptual coverage facilitates the use of a single policy that goes beyond the limitations of previous methods confined to ground-level obstacles, extending to lateral clutter, overhead constraints, multi-level structures, and narrow passages. Gallant also firstly achieves near 100% success rates in challenging scenarios such as stair climbing and stepping onto elevated platforms through improved end-to-end optimization.", "link": "http://arxiv.org/abs/2511.14625v1", "date": "2025-11-18", "relevancy": 2.4084, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6136}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5981}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gallant%3A%20Voxel%20Grid-based%20Humanoid%20Locomotion%20and%20Local-navigation%20across%203D%20Constrained%20Terrains&body=Title%3A%20Gallant%3A%20Voxel%20Grid-based%20Humanoid%20Locomotion%20and%20Local-navigation%20across%203D%20Constrained%20Terrains%0AAuthor%3A%20Qingwei%20Ben%20and%20Botian%20Xu%20and%20Kailin%20Li%20and%20Feiyu%20Jia%20and%20Wentao%20Zhang%20and%20Jingping%20Wang%20and%20Jingbo%20Wang%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang%0AAbstract%3A%20Robust%20humanoid%20locomotion%20requires%20accurate%20and%20globally%20consistent%20perception%20of%20the%20surrounding%203D%20environment.%20However%2C%20existing%20perception%20modules%2C%20mainly%20based%20on%20depth%20images%20or%20elevation%20maps%2C%20offer%20only%20partial%20and%20locally%20flattened%20views%20of%20the%20environment%2C%20failing%20to%20capture%20the%20full%203D%20structure.%20This%20paper%20presents%20Gallant%2C%20a%20voxel-grid-based%20framework%20for%20humanoid%20locomotion%20and%20local%20navigation%20in%203D%20constrained%20terrains.%20It%20leverages%20voxelized%20LiDAR%20data%20as%20a%20lightweight%20and%20structured%20perceptual%20representation%2C%20and%20employs%20a%20z-grouped%202D%20CNN%20to%20map%20this%20representation%20to%20the%20control%20policy%2C%20enabling%20fully%20end-to-end%20optimization.%20A%20high-fidelity%20LiDAR%20simulation%20that%20dynamically%20generates%20realistic%20observations%20is%20developed%20to%20support%20scalable%2C%20LiDAR-based%20training%20and%20ensure%20sim-to-real%20consistency.%20Experimental%20results%20show%20that%20Gallant%27s%20broader%20perceptual%20coverage%20facilitates%20the%20use%20of%20a%20single%20policy%20that%20goes%20beyond%20the%20limitations%20of%20previous%20methods%20confined%20to%20ground-level%20obstacles%2C%20extending%20to%20lateral%20clutter%2C%20overhead%20constraints%2C%20multi-level%20structures%2C%20and%20narrow%20passages.%20Gallant%20also%20firstly%20achieves%20near%20100%25%20success%20rates%20in%20challenging%20scenarios%20such%20as%20stair%20climbing%20and%20stepping%20onto%20elevated%20platforms%20through%20improved%20end-to-end%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGallant%253A%2520Voxel%2520Grid-based%2520Humanoid%2520Locomotion%2520and%2520Local-navigation%2520across%25203D%2520Constrained%2520Terrains%26entry.906535625%3DQingwei%2520Ben%2520and%2520Botian%2520Xu%2520and%2520Kailin%2520Li%2520and%2520Feiyu%2520Jia%2520and%2520Wentao%2520Zhang%2520and%2520Jingping%2520Wang%2520and%2520Jingbo%2520Wang%2520and%2520Dahua%2520Lin%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3DRobust%2520humanoid%2520locomotion%2520requires%2520accurate%2520and%2520globally%2520consistent%2520perception%2520of%2520the%2520surrounding%25203D%2520environment.%2520However%252C%2520existing%2520perception%2520modules%252C%2520mainly%2520based%2520on%2520depth%2520images%2520or%2520elevation%2520maps%252C%2520offer%2520only%2520partial%2520and%2520locally%2520flattened%2520views%2520of%2520the%2520environment%252C%2520failing%2520to%2520capture%2520the%2520full%25203D%2520structure.%2520This%2520paper%2520presents%2520Gallant%252C%2520a%2520voxel-grid-based%2520framework%2520for%2520humanoid%2520locomotion%2520and%2520local%2520navigation%2520in%25203D%2520constrained%2520terrains.%2520It%2520leverages%2520voxelized%2520LiDAR%2520data%2520as%2520a%2520lightweight%2520and%2520structured%2520perceptual%2520representation%252C%2520and%2520employs%2520a%2520z-grouped%25202D%2520CNN%2520to%2520map%2520this%2520representation%2520to%2520the%2520control%2520policy%252C%2520enabling%2520fully%2520end-to-end%2520optimization.%2520A%2520high-fidelity%2520LiDAR%2520simulation%2520that%2520dynamically%2520generates%2520realistic%2520observations%2520is%2520developed%2520to%2520support%2520scalable%252C%2520LiDAR-based%2520training%2520and%2520ensure%2520sim-to-real%2520consistency.%2520Experimental%2520results%2520show%2520that%2520Gallant%2527s%2520broader%2520perceptual%2520coverage%2520facilitates%2520the%2520use%2520of%2520a%2520single%2520policy%2520that%2520goes%2520beyond%2520the%2520limitations%2520of%2520previous%2520methods%2520confined%2520to%2520ground-level%2520obstacles%252C%2520extending%2520to%2520lateral%2520clutter%252C%2520overhead%2520constraints%252C%2520multi-level%2520structures%252C%2520and%2520narrow%2520passages.%2520Gallant%2520also%2520firstly%2520achieves%2520near%2520100%2525%2520success%2520rates%2520in%2520challenging%2520scenarios%2520such%2520as%2520stair%2520climbing%2520and%2520stepping%2520onto%2520elevated%2520platforms%2520through%2520improved%2520end-to-end%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gallant%3A%20Voxel%20Grid-based%20Humanoid%20Locomotion%20and%20Local-navigation%20across%203D%20Constrained%20Terrains&entry.906535625=Qingwei%20Ben%20and%20Botian%20Xu%20and%20Kailin%20Li%20and%20Feiyu%20Jia%20and%20Wentao%20Zhang%20and%20Jingping%20Wang%20and%20Jingbo%20Wang%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang&entry.1292438233=Robust%20humanoid%20locomotion%20requires%20accurate%20and%20globally%20consistent%20perception%20of%20the%20surrounding%203D%20environment.%20However%2C%20existing%20perception%20modules%2C%20mainly%20based%20on%20depth%20images%20or%20elevation%20maps%2C%20offer%20only%20partial%20and%20locally%20flattened%20views%20of%20the%20environment%2C%20failing%20to%20capture%20the%20full%203D%20structure.%20This%20paper%20presents%20Gallant%2C%20a%20voxel-grid-based%20framework%20for%20humanoid%20locomotion%20and%20local%20navigation%20in%203D%20constrained%20terrains.%20It%20leverages%20voxelized%20LiDAR%20data%20as%20a%20lightweight%20and%20structured%20perceptual%20representation%2C%20and%20employs%20a%20z-grouped%202D%20CNN%20to%20map%20this%20representation%20to%20the%20control%20policy%2C%20enabling%20fully%20end-to-end%20optimization.%20A%20high-fidelity%20LiDAR%20simulation%20that%20dynamically%20generates%20realistic%20observations%20is%20developed%20to%20support%20scalable%2C%20LiDAR-based%20training%20and%20ensure%20sim-to-real%20consistency.%20Experimental%20results%20show%20that%20Gallant%27s%20broader%20perceptual%20coverage%20facilitates%20the%20use%20of%20a%20single%20policy%20that%20goes%20beyond%20the%20limitations%20of%20previous%20methods%20confined%20to%20ground-level%20obstacles%2C%20extending%20to%20lateral%20clutter%2C%20overhead%20constraints%2C%20multi-level%20structures%2C%20and%20narrow%20passages.%20Gallant%20also%20firstly%20achieves%20near%20100%25%20success%20rates%20in%20challenging%20scenarios%20such%20as%20stair%20climbing%20and%20stepping%20onto%20elevated%20platforms%20through%20improved%20end-to-end%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2511.14625v1&entry.124074799=Read"},
{"title": "Mind the Gaps: Measuring Visual Artifacts in Dimensionality Reduction", "author": "Jaume Ros and Alessio Arleo and Fernando Paulovich", "abstract": "Dimensionality Reduction (DR) techniques are commonly used for the visual exploration and analysis of high-dimensional data due to their ability to project datasets of high-dimensional points onto the 2D plane. However, projecting datasets in lower dimensions often entails some distortion, which is not necessarily easy to recognize but can lead users to misleading conclusions. Several Projection Quality Metrics (PQMs) have been developed as tools to quantify the goodness-of-fit of a DR projection; however, they mostly focus on measuring how well the projection captures the global or local structure of the data, without taking into account the visual distortion of the resulting plots, thus often ignoring the presence of outliers or artifacts that can mislead a visual analysis of the projection. In this work, we introduce the Warping Index (WI), a new metric for measuring the quality of DR projections onto the 2D plane, based on the assumption that the correct preservation of empty regions between points is of crucial importance towards a faithful visual representation of the data.", "link": "http://arxiv.org/abs/2511.14544v1", "date": "2025-11-18", "relevancy": 2.4063, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4964}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4806}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20Gaps%3A%20Measuring%20Visual%20Artifacts%20in%20Dimensionality%20Reduction&body=Title%3A%20Mind%20the%20Gaps%3A%20Measuring%20Visual%20Artifacts%20in%20Dimensionality%20Reduction%0AAuthor%3A%20Jaume%20Ros%20and%20Alessio%20Arleo%20and%20Fernando%20Paulovich%0AAbstract%3A%20Dimensionality%20Reduction%20%28DR%29%20techniques%20are%20commonly%20used%20for%20the%20visual%20exploration%20and%20analysis%20of%20high-dimensional%20data%20due%20to%20their%20ability%20to%20project%20datasets%20of%20high-dimensional%20points%20onto%20the%202D%20plane.%20However%2C%20projecting%20datasets%20in%20lower%20dimensions%20often%20entails%20some%20distortion%2C%20which%20is%20not%20necessarily%20easy%20to%20recognize%20but%20can%20lead%20users%20to%20misleading%20conclusions.%20Several%20Projection%20Quality%20Metrics%20%28PQMs%29%20have%20been%20developed%20as%20tools%20to%20quantify%20the%20goodness-of-fit%20of%20a%20DR%20projection%3B%20however%2C%20they%20mostly%20focus%20on%20measuring%20how%20well%20the%20projection%20captures%20the%20global%20or%20local%20structure%20of%20the%20data%2C%20without%20taking%20into%20account%20the%20visual%20distortion%20of%20the%20resulting%20plots%2C%20thus%20often%20ignoring%20the%20presence%20of%20outliers%20or%20artifacts%20that%20can%20mislead%20a%20visual%20analysis%20of%20the%20projection.%20In%20this%20work%2C%20we%20introduce%20the%20Warping%20Index%20%28WI%29%2C%20a%20new%20metric%20for%20measuring%20the%20quality%20of%20DR%20projections%20onto%20the%202D%20plane%2C%20based%20on%20the%20assumption%20that%20the%20correct%20preservation%20of%20empty%20regions%20between%20points%20is%20of%20crucial%20importance%20towards%20a%20faithful%20visual%20representation%20of%20the%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520Gaps%253A%2520Measuring%2520Visual%2520Artifacts%2520in%2520Dimensionality%2520Reduction%26entry.906535625%3DJaume%2520Ros%2520and%2520Alessio%2520Arleo%2520and%2520Fernando%2520Paulovich%26entry.1292438233%3DDimensionality%2520Reduction%2520%2528DR%2529%2520techniques%2520are%2520commonly%2520used%2520for%2520the%2520visual%2520exploration%2520and%2520analysis%2520of%2520high-dimensional%2520data%2520due%2520to%2520their%2520ability%2520to%2520project%2520datasets%2520of%2520high-dimensional%2520points%2520onto%2520the%25202D%2520plane.%2520However%252C%2520projecting%2520datasets%2520in%2520lower%2520dimensions%2520often%2520entails%2520some%2520distortion%252C%2520which%2520is%2520not%2520necessarily%2520easy%2520to%2520recognize%2520but%2520can%2520lead%2520users%2520to%2520misleading%2520conclusions.%2520Several%2520Projection%2520Quality%2520Metrics%2520%2528PQMs%2529%2520have%2520been%2520developed%2520as%2520tools%2520to%2520quantify%2520the%2520goodness-of-fit%2520of%2520a%2520DR%2520projection%253B%2520however%252C%2520they%2520mostly%2520focus%2520on%2520measuring%2520how%2520well%2520the%2520projection%2520captures%2520the%2520global%2520or%2520local%2520structure%2520of%2520the%2520data%252C%2520without%2520taking%2520into%2520account%2520the%2520visual%2520distortion%2520of%2520the%2520resulting%2520plots%252C%2520thus%2520often%2520ignoring%2520the%2520presence%2520of%2520outliers%2520or%2520artifacts%2520that%2520can%2520mislead%2520a%2520visual%2520analysis%2520of%2520the%2520projection.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520Warping%2520Index%2520%2528WI%2529%252C%2520a%2520new%2520metric%2520for%2520measuring%2520the%2520quality%2520of%2520DR%2520projections%2520onto%2520the%25202D%2520plane%252C%2520based%2520on%2520the%2520assumption%2520that%2520the%2520correct%2520preservation%2520of%2520empty%2520regions%2520between%2520points%2520is%2520of%2520crucial%2520importance%2520towards%2520a%2520faithful%2520visual%2520representation%2520of%2520the%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20Gaps%3A%20Measuring%20Visual%20Artifacts%20in%20Dimensionality%20Reduction&entry.906535625=Jaume%20Ros%20and%20Alessio%20Arleo%20and%20Fernando%20Paulovich&entry.1292438233=Dimensionality%20Reduction%20%28DR%29%20techniques%20are%20commonly%20used%20for%20the%20visual%20exploration%20and%20analysis%20of%20high-dimensional%20data%20due%20to%20their%20ability%20to%20project%20datasets%20of%20high-dimensional%20points%20onto%20the%202D%20plane.%20However%2C%20projecting%20datasets%20in%20lower%20dimensions%20often%20entails%20some%20distortion%2C%20which%20is%20not%20necessarily%20easy%20to%20recognize%20but%20can%20lead%20users%20to%20misleading%20conclusions.%20Several%20Projection%20Quality%20Metrics%20%28PQMs%29%20have%20been%20developed%20as%20tools%20to%20quantify%20the%20goodness-of-fit%20of%20a%20DR%20projection%3B%20however%2C%20they%20mostly%20focus%20on%20measuring%20how%20well%20the%20projection%20captures%20the%20global%20or%20local%20structure%20of%20the%20data%2C%20without%20taking%20into%20account%20the%20visual%20distortion%20of%20the%20resulting%20plots%2C%20thus%20often%20ignoring%20the%20presence%20of%20outliers%20or%20artifacts%20that%20can%20mislead%20a%20visual%20analysis%20of%20the%20projection.%20In%20this%20work%2C%20we%20introduce%20the%20Warping%20Index%20%28WI%29%2C%20a%20new%20metric%20for%20measuring%20the%20quality%20of%20DR%20projections%20onto%20the%202D%20plane%2C%20based%20on%20the%20assumption%20that%20the%20correct%20preservation%20of%20empty%20regions%20between%20points%20is%20of%20crucial%20importance%20towards%20a%20faithful%20visual%20representation%20of%20the%20data.&entry.1838667208=http%3A//arxiv.org/abs/2511.14544v1&entry.124074799=Read"},
{"title": "ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models", "author": "Singon Kim and Gunho Jung and Seong-Whan Lee", "abstract": "Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However,retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language modelbased compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy-reducing documents, making it highly useful in real-world scenarios.", "link": "http://arxiv.org/abs/2504.12673v2", "date": "2025-11-18", "relevancy": 2.3738, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4778}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACoRN%3A%20Noise-Robust%20Abstractive%20Compression%20in%20Retrieval-Augmented%20Language%20Models&body=Title%3A%20ACoRN%3A%20Noise-Robust%20Abstractive%20Compression%20in%20Retrieval-Augmented%20Language%20Models%0AAuthor%3A%20Singon%20Kim%20and%20Gunho%20Jung%20and%20Seong-Whan%20Lee%0AAbstract%3A%20Abstractive%20compression%20utilizes%20smaller%20langauge%20models%20to%20condense%20query-relevant%20context%2C%20reducing%20computational%20costs%20in%20retrieval-augmented%20generation%20%28RAG%29.%20However%2Cretrieved%20documents%20often%20include%20information%20that%20is%20either%20irrelevant%20to%20answering%20the%20query%20or%20misleading%20due%20to%20factual%20incorrect%20content%2C%20despite%20having%20high%20relevance%20scores.%20This%20behavior%20indicates%20that%20abstractive%20compressors%20are%20more%20likely%20to%20omit%20important%20information%20essential%20for%20the%20correct%20answer%2C%20especially%20in%20long%20contexts%20where%20attention%20dispersion%20occurs.%20To%20address%20this%20issue%2C%20we%20categorize%20retrieved%20documents%20in%20a%20more%20fine-grained%20manner%20and%20propose%20Abstractive%20Compression%20Robust%20against%20Noise%20%28ACoRN%29%2C%20which%20introduces%20two%20novel%20training%20steps.%20First%2C%20we%20use%20offline%20data%20augmentation%20on%20the%20training%20dataset%20to%20enhance%20compressor%20robustness%20against%20two%20distinct%20types%20of%20retrieval%20noise.%20Second%2C%20since%20the%20language%20modelbased%20compressor%20cannot%20fully%20utilize%20information%20from%20multiple%20retrieved%20documents%20and%20exhibits%20positional%20bias%2C%20we%20perform%20finetuning%20to%20generate%20summaries%20centered%20around%20key%20information%20that%20directly%20supports%20the%20correct%20answer.%20Our%20experiments%20demonstrate%20that%20T5-large%2C%20trained%20with%20ACoRN%20as%20a%20compressor%2C%20improves%20EM%20and%20F1%20scores%20while%20preserving%20the%20answer%20string%2C%20which%20could%20serve%20as%20direct%20evidence.%20ACoRN%20excels%20on%20datasets%20with%20many%20accuracy-reducing%20documents%2C%20making%20it%20highly%20useful%20in%20real-world%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2504.12673v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACoRN%253A%2520Noise-Robust%2520Abstractive%2520Compression%2520in%2520Retrieval-Augmented%2520Language%2520Models%26entry.906535625%3DSingon%2520Kim%2520and%2520Gunho%2520Jung%2520and%2520Seong-Whan%2520Lee%26entry.1292438233%3DAbstractive%2520compression%2520utilizes%2520smaller%2520langauge%2520models%2520to%2520condense%2520query-relevant%2520context%252C%2520reducing%2520computational%2520costs%2520in%2520retrieval-augmented%2520generation%2520%2528RAG%2529.%2520However%252Cretrieved%2520documents%2520often%2520include%2520information%2520that%2520is%2520either%2520irrelevant%2520to%2520answering%2520the%2520query%2520or%2520misleading%2520due%2520to%2520factual%2520incorrect%2520content%252C%2520despite%2520having%2520high%2520relevance%2520scores.%2520This%2520behavior%2520indicates%2520that%2520abstractive%2520compressors%2520are%2520more%2520likely%2520to%2520omit%2520important%2520information%2520essential%2520for%2520the%2520correct%2520answer%252C%2520especially%2520in%2520long%2520contexts%2520where%2520attention%2520dispersion%2520occurs.%2520To%2520address%2520this%2520issue%252C%2520we%2520categorize%2520retrieved%2520documents%2520in%2520a%2520more%2520fine-grained%2520manner%2520and%2520propose%2520Abstractive%2520Compression%2520Robust%2520against%2520Noise%2520%2528ACoRN%2529%252C%2520which%2520introduces%2520two%2520novel%2520training%2520steps.%2520First%252C%2520we%2520use%2520offline%2520data%2520augmentation%2520on%2520the%2520training%2520dataset%2520to%2520enhance%2520compressor%2520robustness%2520against%2520two%2520distinct%2520types%2520of%2520retrieval%2520noise.%2520Second%252C%2520since%2520the%2520language%2520modelbased%2520compressor%2520cannot%2520fully%2520utilize%2520information%2520from%2520multiple%2520retrieved%2520documents%2520and%2520exhibits%2520positional%2520bias%252C%2520we%2520perform%2520finetuning%2520to%2520generate%2520summaries%2520centered%2520around%2520key%2520information%2520that%2520directly%2520supports%2520the%2520correct%2520answer.%2520Our%2520experiments%2520demonstrate%2520that%2520T5-large%252C%2520trained%2520with%2520ACoRN%2520as%2520a%2520compressor%252C%2520improves%2520EM%2520and%2520F1%2520scores%2520while%2520preserving%2520the%2520answer%2520string%252C%2520which%2520could%2520serve%2520as%2520direct%2520evidence.%2520ACoRN%2520excels%2520on%2520datasets%2520with%2520many%2520accuracy-reducing%2520documents%252C%2520making%2520it%2520highly%2520useful%2520in%2520real-world%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12673v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACoRN%3A%20Noise-Robust%20Abstractive%20Compression%20in%20Retrieval-Augmented%20Language%20Models&entry.906535625=Singon%20Kim%20and%20Gunho%20Jung%20and%20Seong-Whan%20Lee&entry.1292438233=Abstractive%20compression%20utilizes%20smaller%20langauge%20models%20to%20condense%20query-relevant%20context%2C%20reducing%20computational%20costs%20in%20retrieval-augmented%20generation%20%28RAG%29.%20However%2Cretrieved%20documents%20often%20include%20information%20that%20is%20either%20irrelevant%20to%20answering%20the%20query%20or%20misleading%20due%20to%20factual%20incorrect%20content%2C%20despite%20having%20high%20relevance%20scores.%20This%20behavior%20indicates%20that%20abstractive%20compressors%20are%20more%20likely%20to%20omit%20important%20information%20essential%20for%20the%20correct%20answer%2C%20especially%20in%20long%20contexts%20where%20attention%20dispersion%20occurs.%20To%20address%20this%20issue%2C%20we%20categorize%20retrieved%20documents%20in%20a%20more%20fine-grained%20manner%20and%20propose%20Abstractive%20Compression%20Robust%20against%20Noise%20%28ACoRN%29%2C%20which%20introduces%20two%20novel%20training%20steps.%20First%2C%20we%20use%20offline%20data%20augmentation%20on%20the%20training%20dataset%20to%20enhance%20compressor%20robustness%20against%20two%20distinct%20types%20of%20retrieval%20noise.%20Second%2C%20since%20the%20language%20modelbased%20compressor%20cannot%20fully%20utilize%20information%20from%20multiple%20retrieved%20documents%20and%20exhibits%20positional%20bias%2C%20we%20perform%20finetuning%20to%20generate%20summaries%20centered%20around%20key%20information%20that%20directly%20supports%20the%20correct%20answer.%20Our%20experiments%20demonstrate%20that%20T5-large%2C%20trained%20with%20ACoRN%20as%20a%20compressor%2C%20improves%20EM%20and%20F1%20scores%20while%20preserving%20the%20answer%20string%2C%20which%20could%20serve%20as%20direct%20evidence.%20ACoRN%20excels%20on%20datasets%20with%20many%20accuracy-reducing%20documents%2C%20making%20it%20highly%20useful%20in%20real-world%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2504.12673v2&entry.124074799=Read"},
{"title": "OG-VLA: Orthographic Image Generation for 3D-Aware Vision-Language Action Model", "author": "Ishika Singh and Ankit Goyal and Stan Birchfield and Dieter Fox and Animesh Garg and Valts Blukis", "abstract": "We introduce OG-VLA, a novel architecture and learning framework that combines the generalization strengths of Vision Language Action models (VLAs) with the robustness of 3D-aware policies. We address the challenge of mapping natural language instructions and one or more RGBD observations to quasi-static robot actions. 3D-aware robot policies achieve state-of-the-art performance on precise robot manipulation tasks, but struggle with generalization to unseen instructions, scenes, and objects. On the other hand, VLAs excel at generalizing across instructions and scenes, but can be sensitive to camera and robot pose variations. We leverage prior knowledge embedded in language and vision foundation models to improve generalization of 3D-aware keyframe policies. OG-VLA unprojects input observations from diverse views into a point cloud which is then rendered from canonical orthographic views, ensuring input view invariance and consistency between input and output spaces. These canonical views are processed with a vision backbone, a Large Language Model (LLM), and an image diffusion model to generate images that encode the next position and orientation of the end-effector on the input scene. Evaluations on the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization to unseen environments, with over 40% relative improvements while maintaining robust performance in seen settings. We also show real-world adaption in 3 to 5 demonstrations along with strong generalization. Videos and resources at https://og-vla.github.io/", "link": "http://arxiv.org/abs/2506.01196v2", "date": "2025-11-18", "relevancy": 2.3557, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5936}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5936}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OG-VLA%3A%20Orthographic%20Image%20Generation%20for%203D-Aware%20Vision-Language%20Action%20Model&body=Title%3A%20OG-VLA%3A%20Orthographic%20Image%20Generation%20for%203D-Aware%20Vision-Language%20Action%20Model%0AAuthor%3A%20Ishika%20Singh%20and%20Ankit%20Goyal%20and%20Stan%20Birchfield%20and%20Dieter%20Fox%20and%20Animesh%20Garg%20and%20Valts%20Blukis%0AAbstract%3A%20We%20introduce%20OG-VLA%2C%20a%20novel%20architecture%20and%20learning%20framework%20that%20combines%20the%20generalization%20strengths%20of%20Vision%20Language%20Action%20models%20%28VLAs%29%20with%20the%20robustness%20of%203D-aware%20policies.%20We%20address%20the%20challenge%20of%20mapping%20natural%20language%20instructions%20and%20one%20or%20more%20RGBD%20observations%20to%20quasi-static%20robot%20actions.%203D-aware%20robot%20policies%20achieve%20state-of-the-art%20performance%20on%20precise%20robot%20manipulation%20tasks%2C%20but%20struggle%20with%20generalization%20to%20unseen%20instructions%2C%20scenes%2C%20and%20objects.%20On%20the%20other%20hand%2C%20VLAs%20excel%20at%20generalizing%20across%20instructions%20and%20scenes%2C%20but%20can%20be%20sensitive%20to%20camera%20and%20robot%20pose%20variations.%20We%20leverage%20prior%20knowledge%20embedded%20in%20language%20and%20vision%20foundation%20models%20to%20improve%20generalization%20of%203D-aware%20keyframe%20policies.%20OG-VLA%20unprojects%20input%20observations%20from%20diverse%20views%20into%20a%20point%20cloud%20which%20is%20then%20rendered%20from%20canonical%20orthographic%20views%2C%20ensuring%20input%20view%20invariance%20and%20consistency%20between%20input%20and%20output%20spaces.%20These%20canonical%20views%20are%20processed%20with%20a%20vision%20backbone%2C%20a%20Large%20Language%20Model%20%28LLM%29%2C%20and%20an%20image%20diffusion%20model%20to%20generate%20images%20that%20encode%20the%20next%20position%20and%20orientation%20of%20the%20end-effector%20on%20the%20input%20scene.%20Evaluations%20on%20the%20Arnold%20and%20Colosseum%20benchmarks%20demonstrate%20state-of-the-art%20generalization%20to%20unseen%20environments%2C%20with%20over%2040%25%20relative%20improvements%20while%20maintaining%20robust%20performance%20in%20seen%20settings.%20We%20also%20show%20real-world%20adaption%20in%203%20to%205%20demonstrations%20along%20with%20strong%20generalization.%20Videos%20and%20resources%20at%20https%3A//og-vla.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2506.01196v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOG-VLA%253A%2520Orthographic%2520Image%2520Generation%2520for%25203D-Aware%2520Vision-Language%2520Action%2520Model%26entry.906535625%3DIshika%2520Singh%2520and%2520Ankit%2520Goyal%2520and%2520Stan%2520Birchfield%2520and%2520Dieter%2520Fox%2520and%2520Animesh%2520Garg%2520and%2520Valts%2520Blukis%26entry.1292438233%3DWe%2520introduce%2520OG-VLA%252C%2520a%2520novel%2520architecture%2520and%2520learning%2520framework%2520that%2520combines%2520the%2520generalization%2520strengths%2520of%2520Vision%2520Language%2520Action%2520models%2520%2528VLAs%2529%2520with%2520the%2520robustness%2520of%25203D-aware%2520policies.%2520We%2520address%2520the%2520challenge%2520of%2520mapping%2520natural%2520language%2520instructions%2520and%2520one%2520or%2520more%2520RGBD%2520observations%2520to%2520quasi-static%2520robot%2520actions.%25203D-aware%2520robot%2520policies%2520achieve%2520state-of-the-art%2520performance%2520on%2520precise%2520robot%2520manipulation%2520tasks%252C%2520but%2520struggle%2520with%2520generalization%2520to%2520unseen%2520instructions%252C%2520scenes%252C%2520and%2520objects.%2520On%2520the%2520other%2520hand%252C%2520VLAs%2520excel%2520at%2520generalizing%2520across%2520instructions%2520and%2520scenes%252C%2520but%2520can%2520be%2520sensitive%2520to%2520camera%2520and%2520robot%2520pose%2520variations.%2520We%2520leverage%2520prior%2520knowledge%2520embedded%2520in%2520language%2520and%2520vision%2520foundation%2520models%2520to%2520improve%2520generalization%2520of%25203D-aware%2520keyframe%2520policies.%2520OG-VLA%2520unprojects%2520input%2520observations%2520from%2520diverse%2520views%2520into%2520a%2520point%2520cloud%2520which%2520is%2520then%2520rendered%2520from%2520canonical%2520orthographic%2520views%252C%2520ensuring%2520input%2520view%2520invariance%2520and%2520consistency%2520between%2520input%2520and%2520output%2520spaces.%2520These%2520canonical%2520views%2520are%2520processed%2520with%2520a%2520vision%2520backbone%252C%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%252C%2520and%2520an%2520image%2520diffusion%2520model%2520to%2520generate%2520images%2520that%2520encode%2520the%2520next%2520position%2520and%2520orientation%2520of%2520the%2520end-effector%2520on%2520the%2520input%2520scene.%2520Evaluations%2520on%2520the%2520Arnold%2520and%2520Colosseum%2520benchmarks%2520demonstrate%2520state-of-the-art%2520generalization%2520to%2520unseen%2520environments%252C%2520with%2520over%252040%2525%2520relative%2520improvements%2520while%2520maintaining%2520robust%2520performance%2520in%2520seen%2520settings.%2520We%2520also%2520show%2520real-world%2520adaption%2520in%25203%2520to%25205%2520demonstrations%2520along%2520with%2520strong%2520generalization.%2520Videos%2520and%2520resources%2520at%2520https%253A//og-vla.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01196v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OG-VLA%3A%20Orthographic%20Image%20Generation%20for%203D-Aware%20Vision-Language%20Action%20Model&entry.906535625=Ishika%20Singh%20and%20Ankit%20Goyal%20and%20Stan%20Birchfield%20and%20Dieter%20Fox%20and%20Animesh%20Garg%20and%20Valts%20Blukis&entry.1292438233=We%20introduce%20OG-VLA%2C%20a%20novel%20architecture%20and%20learning%20framework%20that%20combines%20the%20generalization%20strengths%20of%20Vision%20Language%20Action%20models%20%28VLAs%29%20with%20the%20robustness%20of%203D-aware%20policies.%20We%20address%20the%20challenge%20of%20mapping%20natural%20language%20instructions%20and%20one%20or%20more%20RGBD%20observations%20to%20quasi-static%20robot%20actions.%203D-aware%20robot%20policies%20achieve%20state-of-the-art%20performance%20on%20precise%20robot%20manipulation%20tasks%2C%20but%20struggle%20with%20generalization%20to%20unseen%20instructions%2C%20scenes%2C%20and%20objects.%20On%20the%20other%20hand%2C%20VLAs%20excel%20at%20generalizing%20across%20instructions%20and%20scenes%2C%20but%20can%20be%20sensitive%20to%20camera%20and%20robot%20pose%20variations.%20We%20leverage%20prior%20knowledge%20embedded%20in%20language%20and%20vision%20foundation%20models%20to%20improve%20generalization%20of%203D-aware%20keyframe%20policies.%20OG-VLA%20unprojects%20input%20observations%20from%20diverse%20views%20into%20a%20point%20cloud%20which%20is%20then%20rendered%20from%20canonical%20orthographic%20views%2C%20ensuring%20input%20view%20invariance%20and%20consistency%20between%20input%20and%20output%20spaces.%20These%20canonical%20views%20are%20processed%20with%20a%20vision%20backbone%2C%20a%20Large%20Language%20Model%20%28LLM%29%2C%20and%20an%20image%20diffusion%20model%20to%20generate%20images%20that%20encode%20the%20next%20position%20and%20orientation%20of%20the%20end-effector%20on%20the%20input%20scene.%20Evaluations%20on%20the%20Arnold%20and%20Colosseum%20benchmarks%20demonstrate%20state-of-the-art%20generalization%20to%20unseen%20environments%2C%20with%20over%2040%25%20relative%20improvements%20while%20maintaining%20robust%20performance%20in%20seen%20settings.%20We%20also%20show%20real-world%20adaption%20in%203%20to%205%20demonstrations%20along%20with%20strong%20generalization.%20Videos%20and%20resources%20at%20https%3A//og-vla.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2506.01196v2&entry.124074799=Read"},
{"title": "Deep Learning-Based Regional White Matter Hyperintensity Mapping as a Robust Biomarker for Alzheimer's Disease", "author": "Julia Machnio and Mads Nielsen and Mostafa Mehdipour Ghazi", "abstract": "White matter hyperintensities (WMH) are key imaging markers in cognitive aging, Alzheimer's disease (AD), and related dementias. Although automated methods for WMH segmentation have advanced, most provide only global lesion load and overlook their spatial distribution across distinct white matter regions. We propose a deep learning framework for robust WMH segmentation and localization, evaluated across public datasets and an independent Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort. Our results show that the predicted lesion loads are in line with the reference WMH estimates, confirming the robustness to variations in lesion load, acquisition, and demographics. Beyond accurate segmentation, we quantify WMH load within anatomically defined regions and combine these measures with brain structure volumes to assess diagnostic value. Regional WMH volumes consistently outperform global lesion burden for disease classification, and integration with brain atrophy metrics further improves performance, reaching area under the curve (AUC) values up to 0.97. Several spatially distinct regions, particularly within anterior white matter tracts, are reproducibly associated with diagnostic status, indicating localized vulnerability in AD. These results highlight the added value of regional WMH quantification. Incorporating localized lesion metrics alongside atrophy markers may enhance early diagnosis and stratification in neurodegenerative disorders.", "link": "http://arxiv.org/abs/2511.14588v1", "date": "2025-11-18", "relevancy": 2.3515, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.491}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4774}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning-Based%20Regional%20White%20Matter%20Hyperintensity%20Mapping%20as%20a%20Robust%20Biomarker%20for%20Alzheimer%27s%20Disease&body=Title%3A%20Deep%20Learning-Based%20Regional%20White%20Matter%20Hyperintensity%20Mapping%20as%20a%20Robust%20Biomarker%20for%20Alzheimer%27s%20Disease%0AAuthor%3A%20Julia%20Machnio%20and%20Mads%20Nielsen%20and%20Mostafa%20Mehdipour%20Ghazi%0AAbstract%3A%20White%20matter%20hyperintensities%20%28WMH%29%20are%20key%20imaging%20markers%20in%20cognitive%20aging%2C%20Alzheimer%27s%20disease%20%28AD%29%2C%20and%20related%20dementias.%20Although%20automated%20methods%20for%20WMH%20segmentation%20have%20advanced%2C%20most%20provide%20only%20global%20lesion%20load%20and%20overlook%20their%20spatial%20distribution%20across%20distinct%20white%20matter%20regions.%20We%20propose%20a%20deep%20learning%20framework%20for%20robust%20WMH%20segmentation%20and%20localization%2C%20evaluated%20across%20public%20datasets%20and%20an%20independent%20Alzheimer%27s%20Disease%20Neuroimaging%20Initiative%20%28ADNI%29%20cohort.%20Our%20results%20show%20that%20the%20predicted%20lesion%20loads%20are%20in%20line%20with%20the%20reference%20WMH%20estimates%2C%20confirming%20the%20robustness%20to%20variations%20in%20lesion%20load%2C%20acquisition%2C%20and%20demographics.%20Beyond%20accurate%20segmentation%2C%20we%20quantify%20WMH%20load%20within%20anatomically%20defined%20regions%20and%20combine%20these%20measures%20with%20brain%20structure%20volumes%20to%20assess%20diagnostic%20value.%20Regional%20WMH%20volumes%20consistently%20outperform%20global%20lesion%20burden%20for%20disease%20classification%2C%20and%20integration%20with%20brain%20atrophy%20metrics%20further%20improves%20performance%2C%20reaching%20area%20under%20the%20curve%20%28AUC%29%20values%20up%20to%200.97.%20Several%20spatially%20distinct%20regions%2C%20particularly%20within%20anterior%20white%20matter%20tracts%2C%20are%20reproducibly%20associated%20with%20diagnostic%20status%2C%20indicating%20localized%20vulnerability%20in%20AD.%20These%20results%20highlight%20the%20added%20value%20of%20regional%20WMH%20quantification.%20Incorporating%20localized%20lesion%20metrics%20alongside%20atrophy%20markers%20may%20enhance%20early%20diagnosis%20and%20stratification%20in%20neurodegenerative%20disorders.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning-Based%2520Regional%2520White%2520Matter%2520Hyperintensity%2520Mapping%2520as%2520a%2520Robust%2520Biomarker%2520for%2520Alzheimer%2527s%2520Disease%26entry.906535625%3DJulia%2520Machnio%2520and%2520Mads%2520Nielsen%2520and%2520Mostafa%2520Mehdipour%2520Ghazi%26entry.1292438233%3DWhite%2520matter%2520hyperintensities%2520%2528WMH%2529%2520are%2520key%2520imaging%2520markers%2520in%2520cognitive%2520aging%252C%2520Alzheimer%2527s%2520disease%2520%2528AD%2529%252C%2520and%2520related%2520dementias.%2520Although%2520automated%2520methods%2520for%2520WMH%2520segmentation%2520have%2520advanced%252C%2520most%2520provide%2520only%2520global%2520lesion%2520load%2520and%2520overlook%2520their%2520spatial%2520distribution%2520across%2520distinct%2520white%2520matter%2520regions.%2520We%2520propose%2520a%2520deep%2520learning%2520framework%2520for%2520robust%2520WMH%2520segmentation%2520and%2520localization%252C%2520evaluated%2520across%2520public%2520datasets%2520and%2520an%2520independent%2520Alzheimer%2527s%2520Disease%2520Neuroimaging%2520Initiative%2520%2528ADNI%2529%2520cohort.%2520Our%2520results%2520show%2520that%2520the%2520predicted%2520lesion%2520loads%2520are%2520in%2520line%2520with%2520the%2520reference%2520WMH%2520estimates%252C%2520confirming%2520the%2520robustness%2520to%2520variations%2520in%2520lesion%2520load%252C%2520acquisition%252C%2520and%2520demographics.%2520Beyond%2520accurate%2520segmentation%252C%2520we%2520quantify%2520WMH%2520load%2520within%2520anatomically%2520defined%2520regions%2520and%2520combine%2520these%2520measures%2520with%2520brain%2520structure%2520volumes%2520to%2520assess%2520diagnostic%2520value.%2520Regional%2520WMH%2520volumes%2520consistently%2520outperform%2520global%2520lesion%2520burden%2520for%2520disease%2520classification%252C%2520and%2520integration%2520with%2520brain%2520atrophy%2520metrics%2520further%2520improves%2520performance%252C%2520reaching%2520area%2520under%2520the%2520curve%2520%2528AUC%2529%2520values%2520up%2520to%25200.97.%2520Several%2520spatially%2520distinct%2520regions%252C%2520particularly%2520within%2520anterior%2520white%2520matter%2520tracts%252C%2520are%2520reproducibly%2520associated%2520with%2520diagnostic%2520status%252C%2520indicating%2520localized%2520vulnerability%2520in%2520AD.%2520These%2520results%2520highlight%2520the%2520added%2520value%2520of%2520regional%2520WMH%2520quantification.%2520Incorporating%2520localized%2520lesion%2520metrics%2520alongside%2520atrophy%2520markers%2520may%2520enhance%2520early%2520diagnosis%2520and%2520stratification%2520in%2520neurodegenerative%2520disorders.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning-Based%20Regional%20White%20Matter%20Hyperintensity%20Mapping%20as%20a%20Robust%20Biomarker%20for%20Alzheimer%27s%20Disease&entry.906535625=Julia%20Machnio%20and%20Mads%20Nielsen%20and%20Mostafa%20Mehdipour%20Ghazi&entry.1292438233=White%20matter%20hyperintensities%20%28WMH%29%20are%20key%20imaging%20markers%20in%20cognitive%20aging%2C%20Alzheimer%27s%20disease%20%28AD%29%2C%20and%20related%20dementias.%20Although%20automated%20methods%20for%20WMH%20segmentation%20have%20advanced%2C%20most%20provide%20only%20global%20lesion%20load%20and%20overlook%20their%20spatial%20distribution%20across%20distinct%20white%20matter%20regions.%20We%20propose%20a%20deep%20learning%20framework%20for%20robust%20WMH%20segmentation%20and%20localization%2C%20evaluated%20across%20public%20datasets%20and%20an%20independent%20Alzheimer%27s%20Disease%20Neuroimaging%20Initiative%20%28ADNI%29%20cohort.%20Our%20results%20show%20that%20the%20predicted%20lesion%20loads%20are%20in%20line%20with%20the%20reference%20WMH%20estimates%2C%20confirming%20the%20robustness%20to%20variations%20in%20lesion%20load%2C%20acquisition%2C%20and%20demographics.%20Beyond%20accurate%20segmentation%2C%20we%20quantify%20WMH%20load%20within%20anatomically%20defined%20regions%20and%20combine%20these%20measures%20with%20brain%20structure%20volumes%20to%20assess%20diagnostic%20value.%20Regional%20WMH%20volumes%20consistently%20outperform%20global%20lesion%20burden%20for%20disease%20classification%2C%20and%20integration%20with%20brain%20atrophy%20metrics%20further%20improves%20performance%2C%20reaching%20area%20under%20the%20curve%20%28AUC%29%20values%20up%20to%200.97.%20Several%20spatially%20distinct%20regions%2C%20particularly%20within%20anterior%20white%20matter%20tracts%2C%20are%20reproducibly%20associated%20with%20diagnostic%20status%2C%20indicating%20localized%20vulnerability%20in%20AD.%20These%20results%20highlight%20the%20added%20value%20of%20regional%20WMH%20quantification.%20Incorporating%20localized%20lesion%20metrics%20alongside%20atrophy%20markers%20may%20enhance%20early%20diagnosis%20and%20stratification%20in%20neurodegenerative%20disorders.&entry.1838667208=http%3A//arxiv.org/abs/2511.14588v1&entry.124074799=Read"},
{"title": "Graph Neural Networks Based Analog Circuit Link Prediction", "author": "Guanyuan Pan and Tiansheng Zhou and Jianxiang Zhao and Zhi Li and Yugui Lin and Bingtao Ma and Yaqi Wang and Pietro Li\u00f2 and Shuai Wang", "abstract": "Circuit link prediction, which identifies missing component connections from incomplete netlists, is crucial in analog circuit design automation. However, existing methods face three main challenges: 1) Insufficient use of topological patterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to the complexity of annotations hinders model generalization; 3) Limited adaptability to various netlist formats restricts model flexibility. We propose Graph Neural Networks Based Analog Circuit Link Prediction (GNN-ACLP), a graph neural networks (GNNs) based method featuring three innovations to tackle these challenges. First, we introduce the SEAL (learning from Subgraphs, Embeddings, and Attributes for Link prediction) framework and achieve port-level accuracy in circuit link prediction. Second, we propose Netlist Babel Fish, a netlist format conversion tool that leverages retrieval-augmented generation (RAG) with a large language model (LLM) to enhance the compatibility of netlist formats. Finally, we build a comprehensive dataset, SpiceNetlist, comprising 775 annotated circuits of 7 different types across 10 component classes. Experiments demonstrate accuracy improvements of 16.08% on SpiceNetlist, 11.38% on Image2Net, and 16.01% on Masala-CHAI compared to the baseline in intra-dataset evaluation, while maintaining accuracy from 92.05% to 99.07% in cross-dataset evaluation, demonstrating robust feature transfer capabilities. However, its linear computational complexity makes processing large-scale netlists challenging and requires future addressing.", "link": "http://arxiv.org/abs/2504.10240v5", "date": "2025-11-18", "relevancy": 2.3361, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4915}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4852}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20Based%20Analog%20Circuit%20Link%20Prediction&body=Title%3A%20Graph%20Neural%20Networks%20Based%20Analog%20Circuit%20Link%20Prediction%0AAuthor%3A%20Guanyuan%20Pan%20and%20Tiansheng%20Zhou%20and%20Jianxiang%20Zhao%20and%20Zhi%20Li%20and%20Yugui%20Lin%20and%20Bingtao%20Ma%20and%20Yaqi%20Wang%20and%20Pietro%20Li%C3%B2%20and%20Shuai%20Wang%0AAbstract%3A%20Circuit%20link%20prediction%2C%20which%20identifies%20missing%20component%20connections%20from%20incomplete%20netlists%2C%20is%20crucial%20in%20analog%20circuit%20design%20automation.%20However%2C%20existing%20methods%20face%20three%20main%20challenges%3A%201%29%20Insufficient%20use%20of%20topological%20patterns%20in%20circuit%20graphs%20reduces%20prediction%20accuracy%3B%202%29%20Data%20scarcity%20due%20to%20the%20complexity%20of%20annotations%20hinders%20model%20generalization%3B%203%29%20Limited%20adaptability%20to%20various%20netlist%20formats%20restricts%20model%20flexibility.%20We%20propose%20Graph%20Neural%20Networks%20Based%20Analog%20Circuit%20Link%20Prediction%20%28GNN-ACLP%29%2C%20a%20graph%20neural%20networks%20%28GNNs%29%20based%20method%20featuring%20three%20innovations%20to%20tackle%20these%20challenges.%20First%2C%20we%20introduce%20the%20SEAL%20%28learning%20from%20Subgraphs%2C%20Embeddings%2C%20and%20Attributes%20for%20Link%20prediction%29%20framework%20and%20achieve%20port-level%20accuracy%20in%20circuit%20link%20prediction.%20Second%2C%20we%20propose%20Netlist%20Babel%20Fish%2C%20a%20netlist%20format%20conversion%20tool%20that%20leverages%20retrieval-augmented%20generation%20%28RAG%29%20with%20a%20large%20language%20model%20%28LLM%29%20to%20enhance%20the%20compatibility%20of%20netlist%20formats.%20Finally%2C%20we%20build%20a%20comprehensive%20dataset%2C%20SpiceNetlist%2C%20comprising%20775%20annotated%20circuits%20of%207%20different%20types%20across%2010%20component%20classes.%20Experiments%20demonstrate%20accuracy%20improvements%20of%2016.08%25%20on%20SpiceNetlist%2C%2011.38%25%20on%20Image2Net%2C%20and%2016.01%25%20on%20Masala-CHAI%20compared%20to%20the%20baseline%20in%20intra-dataset%20evaluation%2C%20while%20maintaining%20accuracy%20from%2092.05%25%20to%2099.07%25%20in%20cross-dataset%20evaluation%2C%20demonstrating%20robust%20feature%20transfer%20capabilities.%20However%2C%20its%20linear%20computational%20complexity%20makes%20processing%20large-scale%20netlists%20challenging%20and%20requires%20future%20addressing.%0ALink%3A%20http%3A//arxiv.org/abs/2504.10240v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520Based%2520Analog%2520Circuit%2520Link%2520Prediction%26entry.906535625%3DGuanyuan%2520Pan%2520and%2520Tiansheng%2520Zhou%2520and%2520Jianxiang%2520Zhao%2520and%2520Zhi%2520Li%2520and%2520Yugui%2520Lin%2520and%2520Bingtao%2520Ma%2520and%2520Yaqi%2520Wang%2520and%2520Pietro%2520Li%25C3%25B2%2520and%2520Shuai%2520Wang%26entry.1292438233%3DCircuit%2520link%2520prediction%252C%2520which%2520identifies%2520missing%2520component%2520connections%2520from%2520incomplete%2520netlists%252C%2520is%2520crucial%2520in%2520analog%2520circuit%2520design%2520automation.%2520However%252C%2520existing%2520methods%2520face%2520three%2520main%2520challenges%253A%25201%2529%2520Insufficient%2520use%2520of%2520topological%2520patterns%2520in%2520circuit%2520graphs%2520reduces%2520prediction%2520accuracy%253B%25202%2529%2520Data%2520scarcity%2520due%2520to%2520the%2520complexity%2520of%2520annotations%2520hinders%2520model%2520generalization%253B%25203%2529%2520Limited%2520adaptability%2520to%2520various%2520netlist%2520formats%2520restricts%2520model%2520flexibility.%2520We%2520propose%2520Graph%2520Neural%2520Networks%2520Based%2520Analog%2520Circuit%2520Link%2520Prediction%2520%2528GNN-ACLP%2529%252C%2520a%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520based%2520method%2520featuring%2520three%2520innovations%2520to%2520tackle%2520these%2520challenges.%2520First%252C%2520we%2520introduce%2520the%2520SEAL%2520%2528learning%2520from%2520Subgraphs%252C%2520Embeddings%252C%2520and%2520Attributes%2520for%2520Link%2520prediction%2529%2520framework%2520and%2520achieve%2520port-level%2520accuracy%2520in%2520circuit%2520link%2520prediction.%2520Second%252C%2520we%2520propose%2520Netlist%2520Babel%2520Fish%252C%2520a%2520netlist%2520format%2520conversion%2520tool%2520that%2520leverages%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520with%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520enhance%2520the%2520compatibility%2520of%2520netlist%2520formats.%2520Finally%252C%2520we%2520build%2520a%2520comprehensive%2520dataset%252C%2520SpiceNetlist%252C%2520comprising%2520775%2520annotated%2520circuits%2520of%25207%2520different%2520types%2520across%252010%2520component%2520classes.%2520Experiments%2520demonstrate%2520accuracy%2520improvements%2520of%252016.08%2525%2520on%2520SpiceNetlist%252C%252011.38%2525%2520on%2520Image2Net%252C%2520and%252016.01%2525%2520on%2520Masala-CHAI%2520compared%2520to%2520the%2520baseline%2520in%2520intra-dataset%2520evaluation%252C%2520while%2520maintaining%2520accuracy%2520from%252092.05%2525%2520to%252099.07%2525%2520in%2520cross-dataset%2520evaluation%252C%2520demonstrating%2520robust%2520feature%2520transfer%2520capabilities.%2520However%252C%2520its%2520linear%2520computational%2520complexity%2520makes%2520processing%2520large-scale%2520netlists%2520challenging%2520and%2520requires%2520future%2520addressing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10240v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20Based%20Analog%20Circuit%20Link%20Prediction&entry.906535625=Guanyuan%20Pan%20and%20Tiansheng%20Zhou%20and%20Jianxiang%20Zhao%20and%20Zhi%20Li%20and%20Yugui%20Lin%20and%20Bingtao%20Ma%20and%20Yaqi%20Wang%20and%20Pietro%20Li%C3%B2%20and%20Shuai%20Wang&entry.1292438233=Circuit%20link%20prediction%2C%20which%20identifies%20missing%20component%20connections%20from%20incomplete%20netlists%2C%20is%20crucial%20in%20analog%20circuit%20design%20automation.%20However%2C%20existing%20methods%20face%20three%20main%20challenges%3A%201%29%20Insufficient%20use%20of%20topological%20patterns%20in%20circuit%20graphs%20reduces%20prediction%20accuracy%3B%202%29%20Data%20scarcity%20due%20to%20the%20complexity%20of%20annotations%20hinders%20model%20generalization%3B%203%29%20Limited%20adaptability%20to%20various%20netlist%20formats%20restricts%20model%20flexibility.%20We%20propose%20Graph%20Neural%20Networks%20Based%20Analog%20Circuit%20Link%20Prediction%20%28GNN-ACLP%29%2C%20a%20graph%20neural%20networks%20%28GNNs%29%20based%20method%20featuring%20three%20innovations%20to%20tackle%20these%20challenges.%20First%2C%20we%20introduce%20the%20SEAL%20%28learning%20from%20Subgraphs%2C%20Embeddings%2C%20and%20Attributes%20for%20Link%20prediction%29%20framework%20and%20achieve%20port-level%20accuracy%20in%20circuit%20link%20prediction.%20Second%2C%20we%20propose%20Netlist%20Babel%20Fish%2C%20a%20netlist%20format%20conversion%20tool%20that%20leverages%20retrieval-augmented%20generation%20%28RAG%29%20with%20a%20large%20language%20model%20%28LLM%29%20to%20enhance%20the%20compatibility%20of%20netlist%20formats.%20Finally%2C%20we%20build%20a%20comprehensive%20dataset%2C%20SpiceNetlist%2C%20comprising%20775%20annotated%20circuits%20of%207%20different%20types%20across%2010%20component%20classes.%20Experiments%20demonstrate%20accuracy%20improvements%20of%2016.08%25%20on%20SpiceNetlist%2C%2011.38%25%20on%20Image2Net%2C%20and%2016.01%25%20on%20Masala-CHAI%20compared%20to%20the%20baseline%20in%20intra-dataset%20evaluation%2C%20while%20maintaining%20accuracy%20from%2092.05%25%20to%2099.07%25%20in%20cross-dataset%20evaluation%2C%20demonstrating%20robust%20feature%20transfer%20capabilities.%20However%2C%20its%20linear%20computational%20complexity%20makes%20processing%20large-scale%20netlists%20challenging%20and%20requires%20future%20addressing.&entry.1838667208=http%3A//arxiv.org/abs/2504.10240v5&entry.124074799=Read"},
{"title": "INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers", "author": "Hao Wei and Aleksandra Franz and Bjoern List and Nils Thuerey", "abstract": "When simulating partial differential equations, hybrid solvers combine coarse numerical solvers with learned correctors. They promise accelerated simulations while adhering to physical constraints. However, as shown in our theoretical framework, directly applying learned corrections to solver outputs leads to significant autoregressive errors, which originate from amplified perturbations that accumulate during long-term rollouts, especially in chaotic regimes. To overcome this, we propose the Indirect Neural Corrector ($\\mathrm{INC}$), which integrates learned corrections into the governing equations rather than applying direct state updates. Our key insight is that $\\mathrm{INC}$ reduces the error amplification on the order of $\u0394t^{-1} + L$, where $\u0394t$ is the timestep and $L$ the Lipschitz constant. At the same time, our framework poses no architectural requirements and integrates seamlessly with arbitrary neural networks and solvers. We test $\\mathrm{INC}$ in extensive benchmarks, covering numerous differentiable solvers, neural backbones, and test cases ranging from a 1D chaotic system to 3D turbulence. $\\mathrm{INC}$ improves the long-term trajectory performance ($R^2$) by up to 158.7%, stabilizes blowups under aggressive coarsening, and for complex 3D turbulence cases yields speed-ups of several orders of magnitude. $\\mathrm{INC}$ thus enables stable, efficient PDE emulation with formal error reduction, paving the way for faster scientific and engineering simulations with reliable physics guarantees. Our source code is available at https://github.com/tum-pbs/INC", "link": "http://arxiv.org/abs/2511.12764v2", "date": "2025-11-18", "relevancy": 2.3267, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4997}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4513}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20INC%3A%20An%20Indirect%20Neural%20Corrector%20for%20Auto-Regressive%20Hybrid%20PDE%20Solvers&body=Title%3A%20INC%3A%20An%20Indirect%20Neural%20Corrector%20for%20Auto-Regressive%20Hybrid%20PDE%20Solvers%0AAuthor%3A%20Hao%20Wei%20and%20Aleksandra%20Franz%20and%20Bjoern%20List%20and%20Nils%20Thuerey%0AAbstract%3A%20When%20simulating%20partial%20differential%20equations%2C%20hybrid%20solvers%20combine%20coarse%20numerical%20solvers%20with%20learned%20correctors.%20They%20promise%20accelerated%20simulations%20while%20adhering%20to%20physical%20constraints.%20However%2C%20as%20shown%20in%20our%20theoretical%20framework%2C%20directly%20applying%20learned%20corrections%20to%20solver%20outputs%20leads%20to%20significant%20autoregressive%20errors%2C%20which%20originate%20from%20amplified%20perturbations%20that%20accumulate%20during%20long-term%20rollouts%2C%20especially%20in%20chaotic%20regimes.%20To%20overcome%20this%2C%20we%20propose%20the%20Indirect%20Neural%20Corrector%20%28%24%5Cmathrm%7BINC%7D%24%29%2C%20which%20integrates%20learned%20corrections%20into%20the%20governing%20equations%20rather%20than%20applying%20direct%20state%20updates.%20Our%20key%20insight%20is%20that%20%24%5Cmathrm%7BINC%7D%24%20reduces%20the%20error%20amplification%20on%20the%20order%20of%20%24%CE%94t%5E%7B-1%7D%20%2B%20L%24%2C%20where%20%24%CE%94t%24%20is%20the%20timestep%20and%20%24L%24%20the%20Lipschitz%20constant.%20At%20the%20same%20time%2C%20our%20framework%20poses%20no%20architectural%20requirements%20and%20integrates%20seamlessly%20with%20arbitrary%20neural%20networks%20and%20solvers.%20We%20test%20%24%5Cmathrm%7BINC%7D%24%20in%20extensive%20benchmarks%2C%20covering%20numerous%20differentiable%20solvers%2C%20neural%20backbones%2C%20and%20test%20cases%20ranging%20from%20a%201D%20chaotic%20system%20to%203D%20turbulence.%20%24%5Cmathrm%7BINC%7D%24%20improves%20the%20long-term%20trajectory%20performance%20%28%24R%5E2%24%29%20by%20up%20to%20158.7%25%2C%20stabilizes%20blowups%20under%20aggressive%20coarsening%2C%20and%20for%20complex%203D%20turbulence%20cases%20yields%20speed-ups%20of%20several%20orders%20of%20magnitude.%20%24%5Cmathrm%7BINC%7D%24%20thus%20enables%20stable%2C%20efficient%20PDE%20emulation%20with%20formal%20error%20reduction%2C%20paving%20the%20way%20for%20faster%20scientific%20and%20engineering%20simulations%20with%20reliable%20physics%20guarantees.%20Our%20source%20code%20is%20available%20at%20https%3A//github.com/tum-pbs/INC%0ALink%3A%20http%3A//arxiv.org/abs/2511.12764v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DINC%253A%2520An%2520Indirect%2520Neural%2520Corrector%2520for%2520Auto-Regressive%2520Hybrid%2520PDE%2520Solvers%26entry.906535625%3DHao%2520Wei%2520and%2520Aleksandra%2520Franz%2520and%2520Bjoern%2520List%2520and%2520Nils%2520Thuerey%26entry.1292438233%3DWhen%2520simulating%2520partial%2520differential%2520equations%252C%2520hybrid%2520solvers%2520combine%2520coarse%2520numerical%2520solvers%2520with%2520learned%2520correctors.%2520They%2520promise%2520accelerated%2520simulations%2520while%2520adhering%2520to%2520physical%2520constraints.%2520However%252C%2520as%2520shown%2520in%2520our%2520theoretical%2520framework%252C%2520directly%2520applying%2520learned%2520corrections%2520to%2520solver%2520outputs%2520leads%2520to%2520significant%2520autoregressive%2520errors%252C%2520which%2520originate%2520from%2520amplified%2520perturbations%2520that%2520accumulate%2520during%2520long-term%2520rollouts%252C%2520especially%2520in%2520chaotic%2520regimes.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520the%2520Indirect%2520Neural%2520Corrector%2520%2528%2524%255Cmathrm%257BINC%257D%2524%2529%252C%2520which%2520integrates%2520learned%2520corrections%2520into%2520the%2520governing%2520equations%2520rather%2520than%2520applying%2520direct%2520state%2520updates.%2520Our%2520key%2520insight%2520is%2520that%2520%2524%255Cmathrm%257BINC%257D%2524%2520reduces%2520the%2520error%2520amplification%2520on%2520the%2520order%2520of%2520%2524%25CE%2594t%255E%257B-1%257D%2520%252B%2520L%2524%252C%2520where%2520%2524%25CE%2594t%2524%2520is%2520the%2520timestep%2520and%2520%2524L%2524%2520the%2520Lipschitz%2520constant.%2520At%2520the%2520same%2520time%252C%2520our%2520framework%2520poses%2520no%2520architectural%2520requirements%2520and%2520integrates%2520seamlessly%2520with%2520arbitrary%2520neural%2520networks%2520and%2520solvers.%2520We%2520test%2520%2524%255Cmathrm%257BINC%257D%2524%2520in%2520extensive%2520benchmarks%252C%2520covering%2520numerous%2520differentiable%2520solvers%252C%2520neural%2520backbones%252C%2520and%2520test%2520cases%2520ranging%2520from%2520a%25201D%2520chaotic%2520system%2520to%25203D%2520turbulence.%2520%2524%255Cmathrm%257BINC%257D%2524%2520improves%2520the%2520long-term%2520trajectory%2520performance%2520%2528%2524R%255E2%2524%2529%2520by%2520up%2520to%2520158.7%2525%252C%2520stabilizes%2520blowups%2520under%2520aggressive%2520coarsening%252C%2520and%2520for%2520complex%25203D%2520turbulence%2520cases%2520yields%2520speed-ups%2520of%2520several%2520orders%2520of%2520magnitude.%2520%2524%255Cmathrm%257BINC%257D%2524%2520thus%2520enables%2520stable%252C%2520efficient%2520PDE%2520emulation%2520with%2520formal%2520error%2520reduction%252C%2520paving%2520the%2520way%2520for%2520faster%2520scientific%2520and%2520engineering%2520simulations%2520with%2520reliable%2520physics%2520guarantees.%2520Our%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/tum-pbs/INC%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.12764v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=INC%3A%20An%20Indirect%20Neural%20Corrector%20for%20Auto-Regressive%20Hybrid%20PDE%20Solvers&entry.906535625=Hao%20Wei%20and%20Aleksandra%20Franz%20and%20Bjoern%20List%20and%20Nils%20Thuerey&entry.1292438233=When%20simulating%20partial%20differential%20equations%2C%20hybrid%20solvers%20combine%20coarse%20numerical%20solvers%20with%20learned%20correctors.%20They%20promise%20accelerated%20simulations%20while%20adhering%20to%20physical%20constraints.%20However%2C%20as%20shown%20in%20our%20theoretical%20framework%2C%20directly%20applying%20learned%20corrections%20to%20solver%20outputs%20leads%20to%20significant%20autoregressive%20errors%2C%20which%20originate%20from%20amplified%20perturbations%20that%20accumulate%20during%20long-term%20rollouts%2C%20especially%20in%20chaotic%20regimes.%20To%20overcome%20this%2C%20we%20propose%20the%20Indirect%20Neural%20Corrector%20%28%24%5Cmathrm%7BINC%7D%24%29%2C%20which%20integrates%20learned%20corrections%20into%20the%20governing%20equations%20rather%20than%20applying%20direct%20state%20updates.%20Our%20key%20insight%20is%20that%20%24%5Cmathrm%7BINC%7D%24%20reduces%20the%20error%20amplification%20on%20the%20order%20of%20%24%CE%94t%5E%7B-1%7D%20%2B%20L%24%2C%20where%20%24%CE%94t%24%20is%20the%20timestep%20and%20%24L%24%20the%20Lipschitz%20constant.%20At%20the%20same%20time%2C%20our%20framework%20poses%20no%20architectural%20requirements%20and%20integrates%20seamlessly%20with%20arbitrary%20neural%20networks%20and%20solvers.%20We%20test%20%24%5Cmathrm%7BINC%7D%24%20in%20extensive%20benchmarks%2C%20covering%20numerous%20differentiable%20solvers%2C%20neural%20backbones%2C%20and%20test%20cases%20ranging%20from%20a%201D%20chaotic%20system%20to%203D%20turbulence.%20%24%5Cmathrm%7BINC%7D%24%20improves%20the%20long-term%20trajectory%20performance%20%28%24R%5E2%24%29%20by%20up%20to%20158.7%25%2C%20stabilizes%20blowups%20under%20aggressive%20coarsening%2C%20and%20for%20complex%203D%20turbulence%20cases%20yields%20speed-ups%20of%20several%20orders%20of%20magnitude.%20%24%5Cmathrm%7BINC%7D%24%20thus%20enables%20stable%2C%20efficient%20PDE%20emulation%20with%20formal%20error%20reduction%2C%20paving%20the%20way%20for%20faster%20scientific%20and%20engineering%20simulations%20with%20reliable%20physics%20guarantees.%20Our%20source%20code%20is%20available%20at%20https%3A//github.com/tum-pbs/INC&entry.1838667208=http%3A//arxiv.org/abs/2511.12764v2&entry.124074799=Read"},
{"title": "UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning", "author": "Rui Tian and Mingfei Gao and Haiming Gang and Jiasen Lu and Zhe Gan and Yinfei Yang and Zuxuan Wu and Afshin Dehghan", "abstract": "We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.", "link": "http://arxiv.org/abs/2511.14760v1", "date": "2025-11-18", "relevancy": 2.325, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5935}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5909}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniGen-1.5%3A%20Enhancing%20Image%20Generation%20and%20Editing%20through%20Reward%20Unification%20in%20Reinforcement%20Learning&body=Title%3A%20UniGen-1.5%3A%20Enhancing%20Image%20Generation%20and%20Editing%20through%20Reward%20Unification%20in%20Reinforcement%20Learning%0AAuthor%3A%20Rui%20Tian%20and%20Mingfei%20Gao%20and%20Haiming%20Gang%20and%20Jiasen%20Lu%20and%20Zhe%20Gan%20and%20Yinfei%20Yang%20and%20Zuxuan%20Wu%20and%20Afshin%20Dehghan%0AAbstract%3A%20We%20present%20UniGen-1.5%2C%20a%20unified%20multimodal%20large%20language%20model%20%28MLLM%29%20for%20advanced%20image%20understanding%2C%20generation%20and%20editing.%20Building%20upon%20UniGen%2C%20we%20comprehensively%20enhance%20the%20model%20architecture%20and%20training%20pipeline%20to%20strengthen%20the%20image%20understanding%20and%20generation%20capabilities%20while%20unlocking%20strong%20image%20editing%20ability.%20Especially%2C%20we%20propose%20a%20unified%20Reinforcement%20Learning%20%28RL%29%20strategy%20that%20improves%20both%20image%20generation%20and%20image%20editing%20jointly%20via%20shared%20reward%20models.%20To%20further%20enhance%20image%20editing%20performance%2C%20we%20propose%20a%20light%20Edit%20Instruction%20Alignment%20stage%20that%20significantly%20improves%20the%20editing%20instruction%20comprehension%20that%20is%20essential%20for%20the%20success%20of%20the%20RL%20training.%20Experimental%20results%20show%20that%20UniGen-1.5%20demonstrates%20competitive%20understanding%20and%20generation%20performance.%20Specifically%2C%20UniGen-1.5%20achieves%200.89%20and%204.31%20overall%20scores%20on%20GenEval%20and%20ImgEdit%20that%20surpass%20the%20state-of-the-art%20models%20such%20as%20BAGEL%20and%20reaching%20performance%20comparable%20to%20proprietary%20models%20such%20as%20GPT-Image-1.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniGen-1.5%253A%2520Enhancing%2520Image%2520Generation%2520and%2520Editing%2520through%2520Reward%2520Unification%2520in%2520Reinforcement%2520Learning%26entry.906535625%3DRui%2520Tian%2520and%2520Mingfei%2520Gao%2520and%2520Haiming%2520Gang%2520and%2520Jiasen%2520Lu%2520and%2520Zhe%2520Gan%2520and%2520Yinfei%2520Yang%2520and%2520Zuxuan%2520Wu%2520and%2520Afshin%2520Dehghan%26entry.1292438233%3DWe%2520present%2520UniGen-1.5%252C%2520a%2520unified%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529%2520for%2520advanced%2520image%2520understanding%252C%2520generation%2520and%2520editing.%2520Building%2520upon%2520UniGen%252C%2520we%2520comprehensively%2520enhance%2520the%2520model%2520architecture%2520and%2520training%2520pipeline%2520to%2520strengthen%2520the%2520image%2520understanding%2520and%2520generation%2520capabilities%2520while%2520unlocking%2520strong%2520image%2520editing%2520ability.%2520Especially%252C%2520we%2520propose%2520a%2520unified%2520Reinforcement%2520Learning%2520%2528RL%2529%2520strategy%2520that%2520improves%2520both%2520image%2520generation%2520and%2520image%2520editing%2520jointly%2520via%2520shared%2520reward%2520models.%2520To%2520further%2520enhance%2520image%2520editing%2520performance%252C%2520we%2520propose%2520a%2520light%2520Edit%2520Instruction%2520Alignment%2520stage%2520that%2520significantly%2520improves%2520the%2520editing%2520instruction%2520comprehension%2520that%2520is%2520essential%2520for%2520the%2520success%2520of%2520the%2520RL%2520training.%2520Experimental%2520results%2520show%2520that%2520UniGen-1.5%2520demonstrates%2520competitive%2520understanding%2520and%2520generation%2520performance.%2520Specifically%252C%2520UniGen-1.5%2520achieves%25200.89%2520and%25204.31%2520overall%2520scores%2520on%2520GenEval%2520and%2520ImgEdit%2520that%2520surpass%2520the%2520state-of-the-art%2520models%2520such%2520as%2520BAGEL%2520and%2520reaching%2520performance%2520comparable%2520to%2520proprietary%2520models%2520such%2520as%2520GPT-Image-1.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniGen-1.5%3A%20Enhancing%20Image%20Generation%20and%20Editing%20through%20Reward%20Unification%20in%20Reinforcement%20Learning&entry.906535625=Rui%20Tian%20and%20Mingfei%20Gao%20and%20Haiming%20Gang%20and%20Jiasen%20Lu%20and%20Zhe%20Gan%20and%20Yinfei%20Yang%20and%20Zuxuan%20Wu%20and%20Afshin%20Dehghan&entry.1292438233=We%20present%20UniGen-1.5%2C%20a%20unified%20multimodal%20large%20language%20model%20%28MLLM%29%20for%20advanced%20image%20understanding%2C%20generation%20and%20editing.%20Building%20upon%20UniGen%2C%20we%20comprehensively%20enhance%20the%20model%20architecture%20and%20training%20pipeline%20to%20strengthen%20the%20image%20understanding%20and%20generation%20capabilities%20while%20unlocking%20strong%20image%20editing%20ability.%20Especially%2C%20we%20propose%20a%20unified%20Reinforcement%20Learning%20%28RL%29%20strategy%20that%20improves%20both%20image%20generation%20and%20image%20editing%20jointly%20via%20shared%20reward%20models.%20To%20further%20enhance%20image%20editing%20performance%2C%20we%20propose%20a%20light%20Edit%20Instruction%20Alignment%20stage%20that%20significantly%20improves%20the%20editing%20instruction%20comprehension%20that%20is%20essential%20for%20the%20success%20of%20the%20RL%20training.%20Experimental%20results%20show%20that%20UniGen-1.5%20demonstrates%20competitive%20understanding%20and%20generation%20performance.%20Specifically%2C%20UniGen-1.5%20achieves%200.89%20and%204.31%20overall%20scores%20on%20GenEval%20and%20ImgEdit%20that%20surpass%20the%20state-of-the-art%20models%20such%20as%20BAGEL%20and%20reaching%20performance%20comparable%20to%20proprietary%20models%20such%20as%20GPT-Image-1.&entry.1838667208=http%3A//arxiv.org/abs/2511.14760v1&entry.124074799=Read"},
{"title": "Scalable Feature Learning on Huge Knowledge Graphs for Downstream Machine Learning", "author": "F\u00e9lix Lefebvre and Ga\u00ebl Varoquaux", "abstract": "Many machine learning tasks can benefit from external knowledge. Large knowledge graphs store such knowledge, and embedding methods can be used to distill it into ready-to-use vector representations for downstream applications. For this purpose, current models have however two limitations: they are primarily optimized for link prediction, via local contrastive learning, and their application to the largest graphs requires significant engineering effort due to GPU memory limits. To address these, we introduce SEPAL: a Scalable Embedding Propagation ALgorithm for large knowledge graphs designed to produce high-quality embeddings for downstream tasks at scale. The key idea of SEPAL is to ensure global embedding consistency by optimizing embeddings only on a small core of entities, and then propagating them to the rest of the graph with message passing. We evaluate SEPAL on 7 large-scale knowledge graphs and 46 downstream machine learning tasks. Our results show that SEPAL significantly outperforms previous methods on downstream tasks. In addition, SEPAL scales up its base embedding model, enabling fitting huge knowledge graphs on commodity hardware.", "link": "http://arxiv.org/abs/2507.00965v2", "date": "2025-11-18", "relevancy": 2.3201, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4804}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Feature%20Learning%20on%20Huge%20Knowledge%20Graphs%20for%20Downstream%20Machine%20Learning&body=Title%3A%20Scalable%20Feature%20Learning%20on%20Huge%20Knowledge%20Graphs%20for%20Downstream%20Machine%20Learning%0AAuthor%3A%20F%C3%A9lix%20Lefebvre%20and%20Ga%C3%ABl%20Varoquaux%0AAbstract%3A%20Many%20machine%20learning%20tasks%20can%20benefit%20from%20external%20knowledge.%20Large%20knowledge%20graphs%20store%20such%20knowledge%2C%20and%20embedding%20methods%20can%20be%20used%20to%20distill%20it%20into%20ready-to-use%20vector%20representations%20for%20downstream%20applications.%20For%20this%20purpose%2C%20current%20models%20have%20however%20two%20limitations%3A%20they%20are%20primarily%20optimized%20for%20link%20prediction%2C%20via%20local%20contrastive%20learning%2C%20and%20their%20application%20to%20the%20largest%20graphs%20requires%20significant%20engineering%20effort%20due%20to%20GPU%20memory%20limits.%20To%20address%20these%2C%20we%20introduce%20SEPAL%3A%20a%20Scalable%20Embedding%20Propagation%20ALgorithm%20for%20large%20knowledge%20graphs%20designed%20to%20produce%20high-quality%20embeddings%20for%20downstream%20tasks%20at%20scale.%20The%20key%20idea%20of%20SEPAL%20is%20to%20ensure%20global%20embedding%20consistency%20by%20optimizing%20embeddings%20only%20on%20a%20small%20core%20of%20entities%2C%20and%20then%20propagating%20them%20to%20the%20rest%20of%20the%20graph%20with%20message%20passing.%20We%20evaluate%20SEPAL%20on%207%20large-scale%20knowledge%20graphs%20and%2046%20downstream%20machine%20learning%20tasks.%20Our%20results%20show%20that%20SEPAL%20significantly%20outperforms%20previous%20methods%20on%20downstream%20tasks.%20In%20addition%2C%20SEPAL%20scales%20up%20its%20base%20embedding%20model%2C%20enabling%20fitting%20huge%20knowledge%20graphs%20on%20commodity%20hardware.%0ALink%3A%20http%3A//arxiv.org/abs/2507.00965v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Feature%2520Learning%2520on%2520Huge%2520Knowledge%2520Graphs%2520for%2520Downstream%2520Machine%2520Learning%26entry.906535625%3DF%25C3%25A9lix%2520Lefebvre%2520and%2520Ga%25C3%25ABl%2520Varoquaux%26entry.1292438233%3DMany%2520machine%2520learning%2520tasks%2520can%2520benefit%2520from%2520external%2520knowledge.%2520Large%2520knowledge%2520graphs%2520store%2520such%2520knowledge%252C%2520and%2520embedding%2520methods%2520can%2520be%2520used%2520to%2520distill%2520it%2520into%2520ready-to-use%2520vector%2520representations%2520for%2520downstream%2520applications.%2520For%2520this%2520purpose%252C%2520current%2520models%2520have%2520however%2520two%2520limitations%253A%2520they%2520are%2520primarily%2520optimized%2520for%2520link%2520prediction%252C%2520via%2520local%2520contrastive%2520learning%252C%2520and%2520their%2520application%2520to%2520the%2520largest%2520graphs%2520requires%2520significant%2520engineering%2520effort%2520due%2520to%2520GPU%2520memory%2520limits.%2520To%2520address%2520these%252C%2520we%2520introduce%2520SEPAL%253A%2520a%2520Scalable%2520Embedding%2520Propagation%2520ALgorithm%2520for%2520large%2520knowledge%2520graphs%2520designed%2520to%2520produce%2520high-quality%2520embeddings%2520for%2520downstream%2520tasks%2520at%2520scale.%2520The%2520key%2520idea%2520of%2520SEPAL%2520is%2520to%2520ensure%2520global%2520embedding%2520consistency%2520by%2520optimizing%2520embeddings%2520only%2520on%2520a%2520small%2520core%2520of%2520entities%252C%2520and%2520then%2520propagating%2520them%2520to%2520the%2520rest%2520of%2520the%2520graph%2520with%2520message%2520passing.%2520We%2520evaluate%2520SEPAL%2520on%25207%2520large-scale%2520knowledge%2520graphs%2520and%252046%2520downstream%2520machine%2520learning%2520tasks.%2520Our%2520results%2520show%2520that%2520SEPAL%2520significantly%2520outperforms%2520previous%2520methods%2520on%2520downstream%2520tasks.%2520In%2520addition%252C%2520SEPAL%2520scales%2520up%2520its%2520base%2520embedding%2520model%252C%2520enabling%2520fitting%2520huge%2520knowledge%2520graphs%2520on%2520commodity%2520hardware.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00965v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Feature%20Learning%20on%20Huge%20Knowledge%20Graphs%20for%20Downstream%20Machine%20Learning&entry.906535625=F%C3%A9lix%20Lefebvre%20and%20Ga%C3%ABl%20Varoquaux&entry.1292438233=Many%20machine%20learning%20tasks%20can%20benefit%20from%20external%20knowledge.%20Large%20knowledge%20graphs%20store%20such%20knowledge%2C%20and%20embedding%20methods%20can%20be%20used%20to%20distill%20it%20into%20ready-to-use%20vector%20representations%20for%20downstream%20applications.%20For%20this%20purpose%2C%20current%20models%20have%20however%20two%20limitations%3A%20they%20are%20primarily%20optimized%20for%20link%20prediction%2C%20via%20local%20contrastive%20learning%2C%20and%20their%20application%20to%20the%20largest%20graphs%20requires%20significant%20engineering%20effort%20due%20to%20GPU%20memory%20limits.%20To%20address%20these%2C%20we%20introduce%20SEPAL%3A%20a%20Scalable%20Embedding%20Propagation%20ALgorithm%20for%20large%20knowledge%20graphs%20designed%20to%20produce%20high-quality%20embeddings%20for%20downstream%20tasks%20at%20scale.%20The%20key%20idea%20of%20SEPAL%20is%20to%20ensure%20global%20embedding%20consistency%20by%20optimizing%20embeddings%20only%20on%20a%20small%20core%20of%20entities%2C%20and%20then%20propagating%20them%20to%20the%20rest%20of%20the%20graph%20with%20message%20passing.%20We%20evaluate%20SEPAL%20on%207%20large-scale%20knowledge%20graphs%20and%2046%20downstream%20machine%20learning%20tasks.%20Our%20results%20show%20that%20SEPAL%20significantly%20outperforms%20previous%20methods%20on%20downstream%20tasks.%20In%20addition%2C%20SEPAL%20scales%20up%20its%20base%20embedding%20model%2C%20enabling%20fitting%20huge%20knowledge%20graphs%20on%20commodity%20hardware.&entry.1838667208=http%3A//arxiv.org/abs/2507.00965v2&entry.124074799=Read"},
{"title": "Systematic Evaluation of Time-Frequency Features for Binaural Sound Source Localization", "author": "Davoud Shariat Panah and Alessandro Ragano and Dan Barry and Jan Skoglund and Andrew Hines", "abstract": "This study presents a systematic evaluation of time-frequency feature design for binaural sound source localization (SSL), focusing on how feature selection influences model performance across diverse conditions. We investigate the performance of a convolutional neural network (CNN) model using various combinations of amplitude-based features (magnitude spectrogram, interaural level difference - ILD) and phase-based features (phase spectrogram, interaural phase difference - IPD). Evaluations on in-domain and out-of-domain data with mismatched head-related transfer functions (HRTFs) reveal that carefully chosen feature combinations often outperform increases in model complexity. While two-feature sets such as ILD + IPD are sufficient for in-domain SSL, generalization to diverse content requires richer inputs combining channel spectrograms with both ILD and IPD. Using the optimal feature sets, our low-complexity CNN model achieves competitive performance. Our findings underscore the importance of feature design in binaural SSL and provide practical guidance for both domain-specific and general-purpose localization.", "link": "http://arxiv.org/abs/2511.13487v2", "date": "2025-11-18", "relevancy": 2.3154, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4915}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Systematic%20Evaluation%20of%20Time-Frequency%20Features%20for%20Binaural%20Sound%20Source%20Localization&body=Title%3A%20Systematic%20Evaluation%20of%20Time-Frequency%20Features%20for%20Binaural%20Sound%20Source%20Localization%0AAuthor%3A%20Davoud%20Shariat%20Panah%20and%20Alessandro%20Ragano%20and%20Dan%20Barry%20and%20Jan%20Skoglund%20and%20Andrew%20Hines%0AAbstract%3A%20This%20study%20presents%20a%20systematic%20evaluation%20of%20time-frequency%20feature%20design%20for%20binaural%20sound%20source%20localization%20%28SSL%29%2C%20focusing%20on%20how%20feature%20selection%20influences%20model%20performance%20across%20diverse%20conditions.%20We%20investigate%20the%20performance%20of%20a%20convolutional%20neural%20network%20%28CNN%29%20model%20using%20various%20combinations%20of%20amplitude-based%20features%20%28magnitude%20spectrogram%2C%20interaural%20level%20difference%20-%20ILD%29%20and%20phase-based%20features%20%28phase%20spectrogram%2C%20interaural%20phase%20difference%20-%20IPD%29.%20Evaluations%20on%20in-domain%20and%20out-of-domain%20data%20with%20mismatched%20head-related%20transfer%20functions%20%28HRTFs%29%20reveal%20that%20carefully%20chosen%20feature%20combinations%20often%20outperform%20increases%20in%20model%20complexity.%20While%20two-feature%20sets%20such%20as%20ILD%20%2B%20IPD%20are%20sufficient%20for%20in-domain%20SSL%2C%20generalization%20to%20diverse%20content%20requires%20richer%20inputs%20combining%20channel%20spectrograms%20with%20both%20ILD%20and%20IPD.%20Using%20the%20optimal%20feature%20sets%2C%20our%20low-complexity%20CNN%20model%20achieves%20competitive%20performance.%20Our%20findings%20underscore%20the%20importance%20of%20feature%20design%20in%20binaural%20SSL%20and%20provide%20practical%20guidance%20for%20both%20domain-specific%20and%20general-purpose%20localization.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13487v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystematic%2520Evaluation%2520of%2520Time-Frequency%2520Features%2520for%2520Binaural%2520Sound%2520Source%2520Localization%26entry.906535625%3DDavoud%2520Shariat%2520Panah%2520and%2520Alessandro%2520Ragano%2520and%2520Dan%2520Barry%2520and%2520Jan%2520Skoglund%2520and%2520Andrew%2520Hines%26entry.1292438233%3DThis%2520study%2520presents%2520a%2520systematic%2520evaluation%2520of%2520time-frequency%2520feature%2520design%2520for%2520binaural%2520sound%2520source%2520localization%2520%2528SSL%2529%252C%2520focusing%2520on%2520how%2520feature%2520selection%2520influences%2520model%2520performance%2520across%2520diverse%2520conditions.%2520We%2520investigate%2520the%2520performance%2520of%2520a%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520model%2520using%2520various%2520combinations%2520of%2520amplitude-based%2520features%2520%2528magnitude%2520spectrogram%252C%2520interaural%2520level%2520difference%2520-%2520ILD%2529%2520and%2520phase-based%2520features%2520%2528phase%2520spectrogram%252C%2520interaural%2520phase%2520difference%2520-%2520IPD%2529.%2520Evaluations%2520on%2520in-domain%2520and%2520out-of-domain%2520data%2520with%2520mismatched%2520head-related%2520transfer%2520functions%2520%2528HRTFs%2529%2520reveal%2520that%2520carefully%2520chosen%2520feature%2520combinations%2520often%2520outperform%2520increases%2520in%2520model%2520complexity.%2520While%2520two-feature%2520sets%2520such%2520as%2520ILD%2520%252B%2520IPD%2520are%2520sufficient%2520for%2520in-domain%2520SSL%252C%2520generalization%2520to%2520diverse%2520content%2520requires%2520richer%2520inputs%2520combining%2520channel%2520spectrograms%2520with%2520both%2520ILD%2520and%2520IPD.%2520Using%2520the%2520optimal%2520feature%2520sets%252C%2520our%2520low-complexity%2520CNN%2520model%2520achieves%2520competitive%2520performance.%2520Our%2520findings%2520underscore%2520the%2520importance%2520of%2520feature%2520design%2520in%2520binaural%2520SSL%2520and%2520provide%2520practical%2520guidance%2520for%2520both%2520domain-specific%2520and%2520general-purpose%2520localization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13487v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Systematic%20Evaluation%20of%20Time-Frequency%20Features%20for%20Binaural%20Sound%20Source%20Localization&entry.906535625=Davoud%20Shariat%20Panah%20and%20Alessandro%20Ragano%20and%20Dan%20Barry%20and%20Jan%20Skoglund%20and%20Andrew%20Hines&entry.1292438233=This%20study%20presents%20a%20systematic%20evaluation%20of%20time-frequency%20feature%20design%20for%20binaural%20sound%20source%20localization%20%28SSL%29%2C%20focusing%20on%20how%20feature%20selection%20influences%20model%20performance%20across%20diverse%20conditions.%20We%20investigate%20the%20performance%20of%20a%20convolutional%20neural%20network%20%28CNN%29%20model%20using%20various%20combinations%20of%20amplitude-based%20features%20%28magnitude%20spectrogram%2C%20interaural%20level%20difference%20-%20ILD%29%20and%20phase-based%20features%20%28phase%20spectrogram%2C%20interaural%20phase%20difference%20-%20IPD%29.%20Evaluations%20on%20in-domain%20and%20out-of-domain%20data%20with%20mismatched%20head-related%20transfer%20functions%20%28HRTFs%29%20reveal%20that%20carefully%20chosen%20feature%20combinations%20often%20outperform%20increases%20in%20model%20complexity.%20While%20two-feature%20sets%20such%20as%20ILD%20%2B%20IPD%20are%20sufficient%20for%20in-domain%20SSL%2C%20generalization%20to%20diverse%20content%20requires%20richer%20inputs%20combining%20channel%20spectrograms%20with%20both%20ILD%20and%20IPD.%20Using%20the%20optimal%20feature%20sets%2C%20our%20low-complexity%20CNN%20model%20achieves%20competitive%20performance.%20Our%20findings%20underscore%20the%20importance%20of%20feature%20design%20in%20binaural%20SSL%20and%20provide%20practical%20guidance%20for%20both%20domain-specific%20and%20general-purpose%20localization.&entry.1838667208=http%3A//arxiv.org/abs/2511.13487v2&entry.124074799=Read"},
{"title": "Divide and Merge: Motion and Semantic Learning in End-to-End Autonomous Driving", "author": "Yinzhe Shen and Omer Sahin Tas and Kaiwen Wang and Royden Wagner and Christoph Stiller", "abstract": "Perceiving the environment and its changes over time corresponds to two fundamental yet heterogeneous types of information: semantics and motion. Previous end-to-end autonomous driving works represent both types of information in a single feature vector. However, including motion related tasks, such as prediction and planning, impairs detection and tracking performance, a phenomenon known as negative transfer in multi-task learning. To address this issue, we propose Neural-Bayes motion decoding, a novel parallel detection, tracking, and prediction method that separates semantic and motion learning. Specifically, we employ a set of learned motion queries that operate in parallel with detection and tracking queries, sharing a unified set of recursively updated reference points. Moreover, we employ interactive semantic decoding to enhance information exchange in semantic tasks, promoting positive transfer. Experiments on the nuScenes dataset with UniAD and SparseDrive confirm the effectiveness of our divide and merge approach, resulting in performance improvements across perception, prediction, and planning. Our code is available at https://github.com/shenyinzhe/DMAD.", "link": "http://arxiv.org/abs/2502.07631v3", "date": "2025-11-18", "relevancy": 2.3127, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6167}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5879}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Divide%20and%20Merge%3A%20Motion%20and%20Semantic%20Learning%20in%20End-to-End%20Autonomous%20Driving&body=Title%3A%20Divide%20and%20Merge%3A%20Motion%20and%20Semantic%20Learning%20in%20End-to-End%20Autonomous%20Driving%0AAuthor%3A%20Yinzhe%20Shen%20and%20Omer%20Sahin%20Tas%20and%20Kaiwen%20Wang%20and%20Royden%20Wagner%20and%20Christoph%20Stiller%0AAbstract%3A%20Perceiving%20the%20environment%20and%20its%20changes%20over%20time%20corresponds%20to%20two%20fundamental%20yet%20heterogeneous%20types%20of%20information%3A%20semantics%20and%20motion.%20Previous%20end-to-end%20autonomous%20driving%20works%20represent%20both%20types%20of%20information%20in%20a%20single%20feature%20vector.%20However%2C%20including%20motion%20related%20tasks%2C%20such%20as%20prediction%20and%20planning%2C%20impairs%20detection%20and%20tracking%20performance%2C%20a%20phenomenon%20known%20as%20negative%20transfer%20in%20multi-task%20learning.%20To%20address%20this%20issue%2C%20we%20propose%20Neural-Bayes%20motion%20decoding%2C%20a%20novel%20parallel%20detection%2C%20tracking%2C%20and%20prediction%20method%20that%20separates%20semantic%20and%20motion%20learning.%20Specifically%2C%20we%20employ%20a%20set%20of%20learned%20motion%20queries%20that%20operate%20in%20parallel%20with%20detection%20and%20tracking%20queries%2C%20sharing%20a%20unified%20set%20of%20recursively%20updated%20reference%20points.%20Moreover%2C%20we%20employ%20interactive%20semantic%20decoding%20to%20enhance%20information%20exchange%20in%20semantic%20tasks%2C%20promoting%20positive%20transfer.%20Experiments%20on%20the%20nuScenes%20dataset%20with%20UniAD%20and%20SparseDrive%20confirm%20the%20effectiveness%20of%20our%20divide%20and%20merge%20approach%2C%20resulting%20in%20performance%20improvements%20across%20perception%2C%20prediction%2C%20and%20planning.%20Our%20code%20is%20available%20at%20https%3A//github.com/shenyinzhe/DMAD.%0ALink%3A%20http%3A//arxiv.org/abs/2502.07631v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivide%2520and%2520Merge%253A%2520Motion%2520and%2520Semantic%2520Learning%2520in%2520End-to-End%2520Autonomous%2520Driving%26entry.906535625%3DYinzhe%2520Shen%2520and%2520Omer%2520Sahin%2520Tas%2520and%2520Kaiwen%2520Wang%2520and%2520Royden%2520Wagner%2520and%2520Christoph%2520Stiller%26entry.1292438233%3DPerceiving%2520the%2520environment%2520and%2520its%2520changes%2520over%2520time%2520corresponds%2520to%2520two%2520fundamental%2520yet%2520heterogeneous%2520types%2520of%2520information%253A%2520semantics%2520and%2520motion.%2520Previous%2520end-to-end%2520autonomous%2520driving%2520works%2520represent%2520both%2520types%2520of%2520information%2520in%2520a%2520single%2520feature%2520vector.%2520However%252C%2520including%2520motion%2520related%2520tasks%252C%2520such%2520as%2520prediction%2520and%2520planning%252C%2520impairs%2520detection%2520and%2520tracking%2520performance%252C%2520a%2520phenomenon%2520known%2520as%2520negative%2520transfer%2520in%2520multi-task%2520learning.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Neural-Bayes%2520motion%2520decoding%252C%2520a%2520novel%2520parallel%2520detection%252C%2520tracking%252C%2520and%2520prediction%2520method%2520that%2520separates%2520semantic%2520and%2520motion%2520learning.%2520Specifically%252C%2520we%2520employ%2520a%2520set%2520of%2520learned%2520motion%2520queries%2520that%2520operate%2520in%2520parallel%2520with%2520detection%2520and%2520tracking%2520queries%252C%2520sharing%2520a%2520unified%2520set%2520of%2520recursively%2520updated%2520reference%2520points.%2520Moreover%252C%2520we%2520employ%2520interactive%2520semantic%2520decoding%2520to%2520enhance%2520information%2520exchange%2520in%2520semantic%2520tasks%252C%2520promoting%2520positive%2520transfer.%2520Experiments%2520on%2520the%2520nuScenes%2520dataset%2520with%2520UniAD%2520and%2520SparseDrive%2520confirm%2520the%2520effectiveness%2520of%2520our%2520divide%2520and%2520merge%2520approach%252C%2520resulting%2520in%2520performance%2520improvements%2520across%2520perception%252C%2520prediction%252C%2520and%2520planning.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/shenyinzhe/DMAD.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07631v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Divide%20and%20Merge%3A%20Motion%20and%20Semantic%20Learning%20in%20End-to-End%20Autonomous%20Driving&entry.906535625=Yinzhe%20Shen%20and%20Omer%20Sahin%20Tas%20and%20Kaiwen%20Wang%20and%20Royden%20Wagner%20and%20Christoph%20Stiller&entry.1292438233=Perceiving%20the%20environment%20and%20its%20changes%20over%20time%20corresponds%20to%20two%20fundamental%20yet%20heterogeneous%20types%20of%20information%3A%20semantics%20and%20motion.%20Previous%20end-to-end%20autonomous%20driving%20works%20represent%20both%20types%20of%20information%20in%20a%20single%20feature%20vector.%20However%2C%20including%20motion%20related%20tasks%2C%20such%20as%20prediction%20and%20planning%2C%20impairs%20detection%20and%20tracking%20performance%2C%20a%20phenomenon%20known%20as%20negative%20transfer%20in%20multi-task%20learning.%20To%20address%20this%20issue%2C%20we%20propose%20Neural-Bayes%20motion%20decoding%2C%20a%20novel%20parallel%20detection%2C%20tracking%2C%20and%20prediction%20method%20that%20separates%20semantic%20and%20motion%20learning.%20Specifically%2C%20we%20employ%20a%20set%20of%20learned%20motion%20queries%20that%20operate%20in%20parallel%20with%20detection%20and%20tracking%20queries%2C%20sharing%20a%20unified%20set%20of%20recursively%20updated%20reference%20points.%20Moreover%2C%20we%20employ%20interactive%20semantic%20decoding%20to%20enhance%20information%20exchange%20in%20semantic%20tasks%2C%20promoting%20positive%20transfer.%20Experiments%20on%20the%20nuScenes%20dataset%20with%20UniAD%20and%20SparseDrive%20confirm%20the%20effectiveness%20of%20our%20divide%20and%20merge%20approach%2C%20resulting%20in%20performance%20improvements%20across%20perception%2C%20prediction%2C%20and%20planning.%20Our%20code%20is%20available%20at%20https%3A//github.com/shenyinzhe/DMAD.&entry.1838667208=http%3A//arxiv.org/abs/2502.07631v3&entry.124074799=Read"},
{"title": "RynnEC: Bringing MLLMs into Embodied World", "author": "Ronghao Dang and Yuqian Yuan and Yunxuan Mao and Kehan Li and Jiangpin Liu and Zhikai Wang and Xin Li and Fan Wang and Deli Zhao", "abstract": "We introduce RynnEC, a video multimodal large language model designed for embodied cognition. Built upon a general-purpose vision-language foundation model, RynnEC incorporates a region encoder and a mask decoder, enabling flexible region-level video interaction. Despite its compact architecture, RynnEC achieves state-of-the-art performance in object property understanding, object segmentation, and spatial reasoning. Conceptually, it offers a region-centric video paradigm for the brain of embodied agents, providing fine-grained perception of the physical world and enabling more precise interactions. To mitigate the scarcity of annotated 3D datasets, we propose an egocentric video based pipeline for generating embodied cognition data. Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for evaluating embodied cognitive capabilities. We anticipate that RynnEC will advance the development of general-purpose cognitive cores for embodied agents and facilitate generalization across diverse embodied tasks. The code, model checkpoints, and benchmark are available at: https://github.com/alibaba-damo-academy/RynnEC", "link": "http://arxiv.org/abs/2508.14160v2", "date": "2025-11-18", "relevancy": 2.3072, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5795}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5795}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RynnEC%3A%20Bringing%20MLLMs%20into%20Embodied%20World&body=Title%3A%20RynnEC%3A%20Bringing%20MLLMs%20into%20Embodied%20World%0AAuthor%3A%20Ronghao%20Dang%20and%20Yuqian%20Yuan%20and%20Yunxuan%20Mao%20and%20Kehan%20Li%20and%20Jiangpin%20Liu%20and%20Zhikai%20Wang%20and%20Xin%20Li%20and%20Fan%20Wang%20and%20Deli%20Zhao%0AAbstract%3A%20We%20introduce%20RynnEC%2C%20a%20video%20multimodal%20large%20language%20model%20designed%20for%20embodied%20cognition.%20Built%20upon%20a%20general-purpose%20vision-language%20foundation%20model%2C%20RynnEC%20incorporates%20a%20region%20encoder%20and%20a%20mask%20decoder%2C%20enabling%20flexible%20region-level%20video%20interaction.%20Despite%20its%20compact%20architecture%2C%20RynnEC%20achieves%20state-of-the-art%20performance%20in%20object%20property%20understanding%2C%20object%20segmentation%2C%20and%20spatial%20reasoning.%20Conceptually%2C%20it%20offers%20a%20region-centric%20video%20paradigm%20for%20the%20brain%20of%20embodied%20agents%2C%20providing%20fine-grained%20perception%20of%20the%20physical%20world%20and%20enabling%20more%20precise%20interactions.%20To%20mitigate%20the%20scarcity%20of%20annotated%203D%20datasets%2C%20we%20propose%20an%20egocentric%20video%20based%20pipeline%20for%20generating%20embodied%20cognition%20data.%20Furthermore%2C%20we%20introduce%20RynnEC-Bench%2C%20a%20region-centered%20benchmark%20for%20evaluating%20embodied%20cognitive%20capabilities.%20We%20anticipate%20that%20RynnEC%20will%20advance%20the%20development%20of%20general-purpose%20cognitive%20cores%20for%20embodied%20agents%20and%20facilitate%20generalization%20across%20diverse%20embodied%20tasks.%20The%20code%2C%20model%20checkpoints%2C%20and%20benchmark%20are%20available%20at%3A%20https%3A//github.com/alibaba-damo-academy/RynnEC%0ALink%3A%20http%3A//arxiv.org/abs/2508.14160v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRynnEC%253A%2520Bringing%2520MLLMs%2520into%2520Embodied%2520World%26entry.906535625%3DRonghao%2520Dang%2520and%2520Yuqian%2520Yuan%2520and%2520Yunxuan%2520Mao%2520and%2520Kehan%2520Li%2520and%2520Jiangpin%2520Liu%2520and%2520Zhikai%2520Wang%2520and%2520Xin%2520Li%2520and%2520Fan%2520Wang%2520and%2520Deli%2520Zhao%26entry.1292438233%3DWe%2520introduce%2520RynnEC%252C%2520a%2520video%2520multimodal%2520large%2520language%2520model%2520designed%2520for%2520embodied%2520cognition.%2520Built%2520upon%2520a%2520general-purpose%2520vision-language%2520foundation%2520model%252C%2520RynnEC%2520incorporates%2520a%2520region%2520encoder%2520and%2520a%2520mask%2520decoder%252C%2520enabling%2520flexible%2520region-level%2520video%2520interaction.%2520Despite%2520its%2520compact%2520architecture%252C%2520RynnEC%2520achieves%2520state-of-the-art%2520performance%2520in%2520object%2520property%2520understanding%252C%2520object%2520segmentation%252C%2520and%2520spatial%2520reasoning.%2520Conceptually%252C%2520it%2520offers%2520a%2520region-centric%2520video%2520paradigm%2520for%2520the%2520brain%2520of%2520embodied%2520agents%252C%2520providing%2520fine-grained%2520perception%2520of%2520the%2520physical%2520world%2520and%2520enabling%2520more%2520precise%2520interactions.%2520To%2520mitigate%2520the%2520scarcity%2520of%2520annotated%25203D%2520datasets%252C%2520we%2520propose%2520an%2520egocentric%2520video%2520based%2520pipeline%2520for%2520generating%2520embodied%2520cognition%2520data.%2520Furthermore%252C%2520we%2520introduce%2520RynnEC-Bench%252C%2520a%2520region-centered%2520benchmark%2520for%2520evaluating%2520embodied%2520cognitive%2520capabilities.%2520We%2520anticipate%2520that%2520RynnEC%2520will%2520advance%2520the%2520development%2520of%2520general-purpose%2520cognitive%2520cores%2520for%2520embodied%2520agents%2520and%2520facilitate%2520generalization%2520across%2520diverse%2520embodied%2520tasks.%2520The%2520code%252C%2520model%2520checkpoints%252C%2520and%2520benchmark%2520are%2520available%2520at%253A%2520https%253A//github.com/alibaba-damo-academy/RynnEC%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14160v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RynnEC%3A%20Bringing%20MLLMs%20into%20Embodied%20World&entry.906535625=Ronghao%20Dang%20and%20Yuqian%20Yuan%20and%20Yunxuan%20Mao%20and%20Kehan%20Li%20and%20Jiangpin%20Liu%20and%20Zhikai%20Wang%20and%20Xin%20Li%20and%20Fan%20Wang%20and%20Deli%20Zhao&entry.1292438233=We%20introduce%20RynnEC%2C%20a%20video%20multimodal%20large%20language%20model%20designed%20for%20embodied%20cognition.%20Built%20upon%20a%20general-purpose%20vision-language%20foundation%20model%2C%20RynnEC%20incorporates%20a%20region%20encoder%20and%20a%20mask%20decoder%2C%20enabling%20flexible%20region-level%20video%20interaction.%20Despite%20its%20compact%20architecture%2C%20RynnEC%20achieves%20state-of-the-art%20performance%20in%20object%20property%20understanding%2C%20object%20segmentation%2C%20and%20spatial%20reasoning.%20Conceptually%2C%20it%20offers%20a%20region-centric%20video%20paradigm%20for%20the%20brain%20of%20embodied%20agents%2C%20providing%20fine-grained%20perception%20of%20the%20physical%20world%20and%20enabling%20more%20precise%20interactions.%20To%20mitigate%20the%20scarcity%20of%20annotated%203D%20datasets%2C%20we%20propose%20an%20egocentric%20video%20based%20pipeline%20for%20generating%20embodied%20cognition%20data.%20Furthermore%2C%20we%20introduce%20RynnEC-Bench%2C%20a%20region-centered%20benchmark%20for%20evaluating%20embodied%20cognitive%20capabilities.%20We%20anticipate%20that%20RynnEC%20will%20advance%20the%20development%20of%20general-purpose%20cognitive%20cores%20for%20embodied%20agents%20and%20facilitate%20generalization%20across%20diverse%20embodied%20tasks.%20The%20code%2C%20model%20checkpoints%2C%20and%20benchmark%20are%20available%20at%3A%20https%3A//github.com/alibaba-damo-academy/RynnEC&entry.1838667208=http%3A//arxiv.org/abs/2508.14160v2&entry.124074799=Read"},
{"title": "MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding", "author": "Daoze Zhang and Zhanheng Nie and Jianyu Liu and Chenghan Fu and Wanxian Guan and Yuan Gao and Jun Song and Pengjie Wang and Jian Xu and Bo Zheng", "abstract": "With the rapid advancement of e-commerce, exploring general representations rather than task-specific ones has attracted increasing research attention. For product understanding, although existing discriminative dual-flow architectures drive progress in this field, they inherently struggle to model the many-to-one alignment between multiple images and texts of products. Therefore, we argue that generative Multimodal Large Language Models (MLLMs) hold significant potential for improving product representation learning. Nevertheless, achieving this goal still remains non-trivial due to several key challenges: the lack of multimodal and aspect-aware modeling modules in typical LLMs; the common presence of background noise in product images; and the absence of a standard benchmark for evaluation. To address these issues, we propose the first generative MLLM-based model named MOON for product representation learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for targeted modeling of multimodal and aspect-specific product content; (2) effectively detects core semantic regions in product images to mitigate the distraction and interference caused by background noise; and (3) introduces the specialized negative sampling strategy to increase the difficulty and diversity of negative samples. In addition, we release a large-scale multimodal benchmark MBE for various product understanding tasks. Experimentally, our model demonstrates competitive zero-shot performance on both our benchmark and the public dataset, showcasing strong generalization across various downstream tasks, including cross-modal retrieval, product classification, and attribute prediction. Furthermore, the case study and visualization illustrate the effectiveness of MOON for product understanding.", "link": "http://arxiv.org/abs/2508.11999v2", "date": "2025-11-18", "relevancy": 2.3062, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.588}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5781}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOON%3A%20Generative%20MLLM-based%20Multimodal%20Representation%20Learning%20for%20E-commerce%20Product%20Understanding&body=Title%3A%20MOON%3A%20Generative%20MLLM-based%20Multimodal%20Representation%20Learning%20for%20E-commerce%20Product%20Understanding%0AAuthor%3A%20Daoze%20Zhang%20and%20Zhanheng%20Nie%20and%20Jianyu%20Liu%20and%20Chenghan%20Fu%20and%20Wanxian%20Guan%20and%20Yuan%20Gao%20and%20Jun%20Song%20and%20Pengjie%20Wang%20and%20Jian%20Xu%20and%20Bo%20Zheng%0AAbstract%3A%20With%20the%20rapid%20advancement%20of%20e-commerce%2C%20exploring%20general%20representations%20rather%20than%20task-specific%20ones%20has%20attracted%20increasing%20research%20attention.%20For%20product%20understanding%2C%20although%20existing%20discriminative%20dual-flow%20architectures%20drive%20progress%20in%20this%20field%2C%20they%20inherently%20struggle%20to%20model%20the%20many-to-one%20alignment%20between%20multiple%20images%20and%20texts%20of%20products.%20Therefore%2C%20we%20argue%20that%20generative%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20hold%20significant%20potential%20for%20improving%20product%20representation%20learning.%20Nevertheless%2C%20achieving%20this%20goal%20still%20remains%20non-trivial%20due%20to%20several%20key%20challenges%3A%20the%20lack%20of%20multimodal%20and%20aspect-aware%20modeling%20modules%20in%20typical%20LLMs%3B%20the%20common%20presence%20of%20background%20noise%20in%20product%20images%3B%20and%20the%20absence%20of%20a%20standard%20benchmark%20for%20evaluation.%20To%20address%20these%20issues%2C%20we%20propose%20the%20first%20generative%20MLLM-based%20model%20named%20MOON%20for%20product%20representation%20learning.%20Our%20method%20%281%29%20employs%20a%20guided%20Mixture-of-Experts%20%28MoE%29%20module%20for%20targeted%20modeling%20of%20multimodal%20and%20aspect-specific%20product%20content%3B%20%282%29%20effectively%20detects%20core%20semantic%20regions%20in%20product%20images%20to%20mitigate%20the%20distraction%20and%20interference%20caused%20by%20background%20noise%3B%20and%20%283%29%20introduces%20the%20specialized%20negative%20sampling%20strategy%20to%20increase%20the%20difficulty%20and%20diversity%20of%20negative%20samples.%20In%20addition%2C%20we%20release%20a%20large-scale%20multimodal%20benchmark%20MBE%20for%20various%20product%20understanding%20tasks.%20Experimentally%2C%20our%20model%20demonstrates%20competitive%20zero-shot%20performance%20on%20both%20our%20benchmark%20and%20the%20public%20dataset%2C%20showcasing%20strong%20generalization%20across%20various%20downstream%20tasks%2C%20including%20cross-modal%20retrieval%2C%20product%20classification%2C%20and%20attribute%20prediction.%20Furthermore%2C%20the%20case%20study%20and%20visualization%20illustrate%20the%20effectiveness%20of%20MOON%20for%20product%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2508.11999v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOON%253A%2520Generative%2520MLLM-based%2520Multimodal%2520Representation%2520Learning%2520for%2520E-commerce%2520Product%2520Understanding%26entry.906535625%3DDaoze%2520Zhang%2520and%2520Zhanheng%2520Nie%2520and%2520Jianyu%2520Liu%2520and%2520Chenghan%2520Fu%2520and%2520Wanxian%2520Guan%2520and%2520Yuan%2520Gao%2520and%2520Jun%2520Song%2520and%2520Pengjie%2520Wang%2520and%2520Jian%2520Xu%2520and%2520Bo%2520Zheng%26entry.1292438233%3DWith%2520the%2520rapid%2520advancement%2520of%2520e-commerce%252C%2520exploring%2520general%2520representations%2520rather%2520than%2520task-specific%2520ones%2520has%2520attracted%2520increasing%2520research%2520attention.%2520For%2520product%2520understanding%252C%2520although%2520existing%2520discriminative%2520dual-flow%2520architectures%2520drive%2520progress%2520in%2520this%2520field%252C%2520they%2520inherently%2520struggle%2520to%2520model%2520the%2520many-to-one%2520alignment%2520between%2520multiple%2520images%2520and%2520texts%2520of%2520products.%2520Therefore%252C%2520we%2520argue%2520that%2520generative%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520hold%2520significant%2520potential%2520for%2520improving%2520product%2520representation%2520learning.%2520Nevertheless%252C%2520achieving%2520this%2520goal%2520still%2520remains%2520non-trivial%2520due%2520to%2520several%2520key%2520challenges%253A%2520the%2520lack%2520of%2520multimodal%2520and%2520aspect-aware%2520modeling%2520modules%2520in%2520typical%2520LLMs%253B%2520the%2520common%2520presence%2520of%2520background%2520noise%2520in%2520product%2520images%253B%2520and%2520the%2520absence%2520of%2520a%2520standard%2520benchmark%2520for%2520evaluation.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520the%2520first%2520generative%2520MLLM-based%2520model%2520named%2520MOON%2520for%2520product%2520representation%2520learning.%2520Our%2520method%2520%25281%2529%2520employs%2520a%2520guided%2520Mixture-of-Experts%2520%2528MoE%2529%2520module%2520for%2520targeted%2520modeling%2520of%2520multimodal%2520and%2520aspect-specific%2520product%2520content%253B%2520%25282%2529%2520effectively%2520detects%2520core%2520semantic%2520regions%2520in%2520product%2520images%2520to%2520mitigate%2520the%2520distraction%2520and%2520interference%2520caused%2520by%2520background%2520noise%253B%2520and%2520%25283%2529%2520introduces%2520the%2520specialized%2520negative%2520sampling%2520strategy%2520to%2520increase%2520the%2520difficulty%2520and%2520diversity%2520of%2520negative%2520samples.%2520In%2520addition%252C%2520we%2520release%2520a%2520large-scale%2520multimodal%2520benchmark%2520MBE%2520for%2520various%2520product%2520understanding%2520tasks.%2520Experimentally%252C%2520our%2520model%2520demonstrates%2520competitive%2520zero-shot%2520performance%2520on%2520both%2520our%2520benchmark%2520and%2520the%2520public%2520dataset%252C%2520showcasing%2520strong%2520generalization%2520across%2520various%2520downstream%2520tasks%252C%2520including%2520cross-modal%2520retrieval%252C%2520product%2520classification%252C%2520and%2520attribute%2520prediction.%2520Furthermore%252C%2520the%2520case%2520study%2520and%2520visualization%2520illustrate%2520the%2520effectiveness%2520of%2520MOON%2520for%2520product%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11999v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOON%3A%20Generative%20MLLM-based%20Multimodal%20Representation%20Learning%20for%20E-commerce%20Product%20Understanding&entry.906535625=Daoze%20Zhang%20and%20Zhanheng%20Nie%20and%20Jianyu%20Liu%20and%20Chenghan%20Fu%20and%20Wanxian%20Guan%20and%20Yuan%20Gao%20and%20Jun%20Song%20and%20Pengjie%20Wang%20and%20Jian%20Xu%20and%20Bo%20Zheng&entry.1292438233=With%20the%20rapid%20advancement%20of%20e-commerce%2C%20exploring%20general%20representations%20rather%20than%20task-specific%20ones%20has%20attracted%20increasing%20research%20attention.%20For%20product%20understanding%2C%20although%20existing%20discriminative%20dual-flow%20architectures%20drive%20progress%20in%20this%20field%2C%20they%20inherently%20struggle%20to%20model%20the%20many-to-one%20alignment%20between%20multiple%20images%20and%20texts%20of%20products.%20Therefore%2C%20we%20argue%20that%20generative%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20hold%20significant%20potential%20for%20improving%20product%20representation%20learning.%20Nevertheless%2C%20achieving%20this%20goal%20still%20remains%20non-trivial%20due%20to%20several%20key%20challenges%3A%20the%20lack%20of%20multimodal%20and%20aspect-aware%20modeling%20modules%20in%20typical%20LLMs%3B%20the%20common%20presence%20of%20background%20noise%20in%20product%20images%3B%20and%20the%20absence%20of%20a%20standard%20benchmark%20for%20evaluation.%20To%20address%20these%20issues%2C%20we%20propose%20the%20first%20generative%20MLLM-based%20model%20named%20MOON%20for%20product%20representation%20learning.%20Our%20method%20%281%29%20employs%20a%20guided%20Mixture-of-Experts%20%28MoE%29%20module%20for%20targeted%20modeling%20of%20multimodal%20and%20aspect-specific%20product%20content%3B%20%282%29%20effectively%20detects%20core%20semantic%20regions%20in%20product%20images%20to%20mitigate%20the%20distraction%20and%20interference%20caused%20by%20background%20noise%3B%20and%20%283%29%20introduces%20the%20specialized%20negative%20sampling%20strategy%20to%20increase%20the%20difficulty%20and%20diversity%20of%20negative%20samples.%20In%20addition%2C%20we%20release%20a%20large-scale%20multimodal%20benchmark%20MBE%20for%20various%20product%20understanding%20tasks.%20Experimentally%2C%20our%20model%20demonstrates%20competitive%20zero-shot%20performance%20on%20both%20our%20benchmark%20and%20the%20public%20dataset%2C%20showcasing%20strong%20generalization%20across%20various%20downstream%20tasks%2C%20including%20cross-modal%20retrieval%2C%20product%20classification%2C%20and%20attribute%20prediction.%20Furthermore%2C%20the%20case%20study%20and%20visualization%20illustrate%20the%20effectiveness%20of%20MOON%20for%20product%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2508.11999v2&entry.124074799=Read"},
{"title": "Rasterized Steered Mixture of Experts for Efficient 2D Image Regression", "author": "Yi-Hsin Li and M\u00e5rten Sj\u00f6str\u00f6m and Sebastian Knorr and Thomas Sikora", "abstract": "The Steered Mixture of Experts regression framework has demonstrated strong performance in image reconstruction, compression, denoising, and super-resolution. However, its high computational cost limits practical applications. This work introduces a rasterization-based optimization strategy that combines the efficiency of rasterized Gaussian kernel rendering with the edge-aware gating mechanism of the Steered Mixture of Experts. The proposed method is designed to accelerate two-dimensional image regression while maintaining the model's inherent sparsity and reconstruction quality. By replacing global iterative optimization with a rasterized formulation, the method achieves significantly faster parameter updates and more memory-efficient model representations. In addition, the proposed framework supports applications such as native super-resolution and image denoising, which are not directly achievable with standard rasterized Gaussian kernel approaches. The combination of fast rasterized optimization with the edge-aware structure of the Steered Mixture of Experts provides a new balance between computational efficiency and reconstruction fidelity for two-dimensional image processing tasks.", "link": "http://arxiv.org/abs/2510.05814v2", "date": "2025-11-18", "relevancy": 2.3008, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5842}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5788}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rasterized%20Steered%20Mixture%20of%20Experts%20for%20Efficient%202D%20Image%20Regression&body=Title%3A%20Rasterized%20Steered%20Mixture%20of%20Experts%20for%20Efficient%202D%20Image%20Regression%0AAuthor%3A%20Yi-Hsin%20Li%20and%20M%C3%A5rten%20Sj%C3%B6str%C3%B6m%20and%20Sebastian%20Knorr%20and%20Thomas%20Sikora%0AAbstract%3A%20The%20Steered%20Mixture%20of%20Experts%20regression%20framework%20has%20demonstrated%20strong%20performance%20in%20image%20reconstruction%2C%20compression%2C%20denoising%2C%20and%20super-resolution.%20However%2C%20its%20high%20computational%20cost%20limits%20practical%20applications.%20This%20work%20introduces%20a%20rasterization-based%20optimization%20strategy%20that%20combines%20the%20efficiency%20of%20rasterized%20Gaussian%20kernel%20rendering%20with%20the%20edge-aware%20gating%20mechanism%20of%20the%20Steered%20Mixture%20of%20Experts.%20The%20proposed%20method%20is%20designed%20to%20accelerate%20two-dimensional%20image%20regression%20while%20maintaining%20the%20model%27s%20inherent%20sparsity%20and%20reconstruction%20quality.%20By%20replacing%20global%20iterative%20optimization%20with%20a%20rasterized%20formulation%2C%20the%20method%20achieves%20significantly%20faster%20parameter%20updates%20and%20more%20memory-efficient%20model%20representations.%20In%20addition%2C%20the%20proposed%20framework%20supports%20applications%20such%20as%20native%20super-resolution%20and%20image%20denoising%2C%20which%20are%20not%20directly%20achievable%20with%20standard%20rasterized%20Gaussian%20kernel%20approaches.%20The%20combination%20of%20fast%20rasterized%20optimization%20with%20the%20edge-aware%20structure%20of%20the%20Steered%20Mixture%20of%20Experts%20provides%20a%20new%20balance%20between%20computational%20efficiency%20and%20reconstruction%20fidelity%20for%20two-dimensional%20image%20processing%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2510.05814v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRasterized%2520Steered%2520Mixture%2520of%2520Experts%2520for%2520Efficient%25202D%2520Image%2520Regression%26entry.906535625%3DYi-Hsin%2520Li%2520and%2520M%25C3%25A5rten%2520Sj%25C3%25B6str%25C3%25B6m%2520and%2520Sebastian%2520Knorr%2520and%2520Thomas%2520Sikora%26entry.1292438233%3DThe%2520Steered%2520Mixture%2520of%2520Experts%2520regression%2520framework%2520has%2520demonstrated%2520strong%2520performance%2520in%2520image%2520reconstruction%252C%2520compression%252C%2520denoising%252C%2520and%2520super-resolution.%2520However%252C%2520its%2520high%2520computational%2520cost%2520limits%2520practical%2520applications.%2520This%2520work%2520introduces%2520a%2520rasterization-based%2520optimization%2520strategy%2520that%2520combines%2520the%2520efficiency%2520of%2520rasterized%2520Gaussian%2520kernel%2520rendering%2520with%2520the%2520edge-aware%2520gating%2520mechanism%2520of%2520the%2520Steered%2520Mixture%2520of%2520Experts.%2520The%2520proposed%2520method%2520is%2520designed%2520to%2520accelerate%2520two-dimensional%2520image%2520regression%2520while%2520maintaining%2520the%2520model%2527s%2520inherent%2520sparsity%2520and%2520reconstruction%2520quality.%2520By%2520replacing%2520global%2520iterative%2520optimization%2520with%2520a%2520rasterized%2520formulation%252C%2520the%2520method%2520achieves%2520significantly%2520faster%2520parameter%2520updates%2520and%2520more%2520memory-efficient%2520model%2520representations.%2520In%2520addition%252C%2520the%2520proposed%2520framework%2520supports%2520applications%2520such%2520as%2520native%2520super-resolution%2520and%2520image%2520denoising%252C%2520which%2520are%2520not%2520directly%2520achievable%2520with%2520standard%2520rasterized%2520Gaussian%2520kernel%2520approaches.%2520The%2520combination%2520of%2520fast%2520rasterized%2520optimization%2520with%2520the%2520edge-aware%2520structure%2520of%2520the%2520Steered%2520Mixture%2520of%2520Experts%2520provides%2520a%2520new%2520balance%2520between%2520computational%2520efficiency%2520and%2520reconstruction%2520fidelity%2520for%2520two-dimensional%2520image%2520processing%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05814v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rasterized%20Steered%20Mixture%20of%20Experts%20for%20Efficient%202D%20Image%20Regression&entry.906535625=Yi-Hsin%20Li%20and%20M%C3%A5rten%20Sj%C3%B6str%C3%B6m%20and%20Sebastian%20Knorr%20and%20Thomas%20Sikora&entry.1292438233=The%20Steered%20Mixture%20of%20Experts%20regression%20framework%20has%20demonstrated%20strong%20performance%20in%20image%20reconstruction%2C%20compression%2C%20denoising%2C%20and%20super-resolution.%20However%2C%20its%20high%20computational%20cost%20limits%20practical%20applications.%20This%20work%20introduces%20a%20rasterization-based%20optimization%20strategy%20that%20combines%20the%20efficiency%20of%20rasterized%20Gaussian%20kernel%20rendering%20with%20the%20edge-aware%20gating%20mechanism%20of%20the%20Steered%20Mixture%20of%20Experts.%20The%20proposed%20method%20is%20designed%20to%20accelerate%20two-dimensional%20image%20regression%20while%20maintaining%20the%20model%27s%20inherent%20sparsity%20and%20reconstruction%20quality.%20By%20replacing%20global%20iterative%20optimization%20with%20a%20rasterized%20formulation%2C%20the%20method%20achieves%20significantly%20faster%20parameter%20updates%20and%20more%20memory-efficient%20model%20representations.%20In%20addition%2C%20the%20proposed%20framework%20supports%20applications%20such%20as%20native%20super-resolution%20and%20image%20denoising%2C%20which%20are%20not%20directly%20achievable%20with%20standard%20rasterized%20Gaussian%20kernel%20approaches.%20The%20combination%20of%20fast%20rasterized%20optimization%20with%20the%20edge-aware%20structure%20of%20the%20Steered%20Mixture%20of%20Experts%20provides%20a%20new%20balance%20between%20computational%20efficiency%20and%20reconstruction%20fidelity%20for%20two-dimensional%20image%20processing%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2510.05814v2&entry.124074799=Read"},
{"title": "Learning to See Through a Baby's Eyes: Early Visual Diets Enable Robust Visual Intelligence in Humans and Machines", "author": "Yusen Cai and Bhargava Satya Nunna and Qing Lin and Mengmi Zhang", "abstract": "Newborns perceive the world with low-acuity, color-degraded, and temporally continuous vision, which gradually sharpens as infants develop. To explore the ecological advantages of such staged \"visual diets\", we train self-supervised learning (SSL) models on object-centric videos under constraints that simulate infant vision: grayscale-to-color (C), blur-to-sharp (A), and preserved temporal continuity (T)-collectively termed CATDiet. For evaluation, we establish a comprehensive benchmark across ten datasets, covering clean and corrupted image recognition, texture-shape cue conflict tests, silhouette recognition, depth-order classification, and the visual cliff paradigm. All CATDiet variants demonstrate enhanced robustness in object recognition, despite being trained solely on object-centric videos. Remarkably, models also exhibit biologically aligned developmental patterns, including neural plasticity changes mirroring synaptic density in macaque V1 and behaviors resembling infants' visual cliff responses. Building on these insights, CombDiet initializes SSL with CATDiet before standard training while preserving temporal continuity. Trained on object-centric or head-mounted infant videos, CombDiet outperforms standard SSL on both in-domain and out-of-domain object recognition and depth perception. Together, these results suggest that the developmental progression of early infant visual experience offers a powerful reverse-engineering framework for understanding the emergence of robust visual intelligence in machines. All code, data, and models will be publicly released.", "link": "http://arxiv.org/abs/2511.14440v1", "date": "2025-11-18", "relevancy": 2.2817, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5718}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5718}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20See%20Through%20a%20Baby%27s%20Eyes%3A%20Early%20Visual%20Diets%20Enable%20Robust%20Visual%20Intelligence%20in%20Humans%20and%20Machines&body=Title%3A%20Learning%20to%20See%20Through%20a%20Baby%27s%20Eyes%3A%20Early%20Visual%20Diets%20Enable%20Robust%20Visual%20Intelligence%20in%20Humans%20and%20Machines%0AAuthor%3A%20Yusen%20Cai%20and%20Bhargava%20Satya%20Nunna%20and%20Qing%20Lin%20and%20Mengmi%20Zhang%0AAbstract%3A%20Newborns%20perceive%20the%20world%20with%20low-acuity%2C%20color-degraded%2C%20and%20temporally%20continuous%20vision%2C%20which%20gradually%20sharpens%20as%20infants%20develop.%20To%20explore%20the%20ecological%20advantages%20of%20such%20staged%20%22visual%20diets%22%2C%20we%20train%20self-supervised%20learning%20%28SSL%29%20models%20on%20object-centric%20videos%20under%20constraints%20that%20simulate%20infant%20vision%3A%20grayscale-to-color%20%28C%29%2C%20blur-to-sharp%20%28A%29%2C%20and%20preserved%20temporal%20continuity%20%28T%29-collectively%20termed%20CATDiet.%20For%20evaluation%2C%20we%20establish%20a%20comprehensive%20benchmark%20across%20ten%20datasets%2C%20covering%20clean%20and%20corrupted%20image%20recognition%2C%20texture-shape%20cue%20conflict%20tests%2C%20silhouette%20recognition%2C%20depth-order%20classification%2C%20and%20the%20visual%20cliff%20paradigm.%20All%20CATDiet%20variants%20demonstrate%20enhanced%20robustness%20in%20object%20recognition%2C%20despite%20being%20trained%20solely%20on%20object-centric%20videos.%20Remarkably%2C%20models%20also%20exhibit%20biologically%20aligned%20developmental%20patterns%2C%20including%20neural%20plasticity%20changes%20mirroring%20synaptic%20density%20in%20macaque%20V1%20and%20behaviors%20resembling%20infants%27%20visual%20cliff%20responses.%20Building%20on%20these%20insights%2C%20CombDiet%20initializes%20SSL%20with%20CATDiet%20before%20standard%20training%20while%20preserving%20temporal%20continuity.%20Trained%20on%20object-centric%20or%20head-mounted%20infant%20videos%2C%20CombDiet%20outperforms%20standard%20SSL%20on%20both%20in-domain%20and%20out-of-domain%20object%20recognition%20and%20depth%20perception.%20Together%2C%20these%20results%20suggest%20that%20the%20developmental%20progression%20of%20early%20infant%20visual%20experience%20offers%20a%20powerful%20reverse-engineering%20framework%20for%20understanding%20the%20emergence%20of%20robust%20visual%20intelligence%20in%20machines.%20All%20code%2C%20data%2C%20and%20models%20will%20be%20publicly%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520See%2520Through%2520a%2520Baby%2527s%2520Eyes%253A%2520Early%2520Visual%2520Diets%2520Enable%2520Robust%2520Visual%2520Intelligence%2520in%2520Humans%2520and%2520Machines%26entry.906535625%3DYusen%2520Cai%2520and%2520Bhargava%2520Satya%2520Nunna%2520and%2520Qing%2520Lin%2520and%2520Mengmi%2520Zhang%26entry.1292438233%3DNewborns%2520perceive%2520the%2520world%2520with%2520low-acuity%252C%2520color-degraded%252C%2520and%2520temporally%2520continuous%2520vision%252C%2520which%2520gradually%2520sharpens%2520as%2520infants%2520develop.%2520To%2520explore%2520the%2520ecological%2520advantages%2520of%2520such%2520staged%2520%2522visual%2520diets%2522%252C%2520we%2520train%2520self-supervised%2520learning%2520%2528SSL%2529%2520models%2520on%2520object-centric%2520videos%2520under%2520constraints%2520that%2520simulate%2520infant%2520vision%253A%2520grayscale-to-color%2520%2528C%2529%252C%2520blur-to-sharp%2520%2528A%2529%252C%2520and%2520preserved%2520temporal%2520continuity%2520%2528T%2529-collectively%2520termed%2520CATDiet.%2520For%2520evaluation%252C%2520we%2520establish%2520a%2520comprehensive%2520benchmark%2520across%2520ten%2520datasets%252C%2520covering%2520clean%2520and%2520corrupted%2520image%2520recognition%252C%2520texture-shape%2520cue%2520conflict%2520tests%252C%2520silhouette%2520recognition%252C%2520depth-order%2520classification%252C%2520and%2520the%2520visual%2520cliff%2520paradigm.%2520All%2520CATDiet%2520variants%2520demonstrate%2520enhanced%2520robustness%2520in%2520object%2520recognition%252C%2520despite%2520being%2520trained%2520solely%2520on%2520object-centric%2520videos.%2520Remarkably%252C%2520models%2520also%2520exhibit%2520biologically%2520aligned%2520developmental%2520patterns%252C%2520including%2520neural%2520plasticity%2520changes%2520mirroring%2520synaptic%2520density%2520in%2520macaque%2520V1%2520and%2520behaviors%2520resembling%2520infants%2527%2520visual%2520cliff%2520responses.%2520Building%2520on%2520these%2520insights%252C%2520CombDiet%2520initializes%2520SSL%2520with%2520CATDiet%2520before%2520standard%2520training%2520while%2520preserving%2520temporal%2520continuity.%2520Trained%2520on%2520object-centric%2520or%2520head-mounted%2520infant%2520videos%252C%2520CombDiet%2520outperforms%2520standard%2520SSL%2520on%2520both%2520in-domain%2520and%2520out-of-domain%2520object%2520recognition%2520and%2520depth%2520perception.%2520Together%252C%2520these%2520results%2520suggest%2520that%2520the%2520developmental%2520progression%2520of%2520early%2520infant%2520visual%2520experience%2520offers%2520a%2520powerful%2520reverse-engineering%2520framework%2520for%2520understanding%2520the%2520emergence%2520of%2520robust%2520visual%2520intelligence%2520in%2520machines.%2520All%2520code%252C%2520data%252C%2520and%2520models%2520will%2520be%2520publicly%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20See%20Through%20a%20Baby%27s%20Eyes%3A%20Early%20Visual%20Diets%20Enable%20Robust%20Visual%20Intelligence%20in%20Humans%20and%20Machines&entry.906535625=Yusen%20Cai%20and%20Bhargava%20Satya%20Nunna%20and%20Qing%20Lin%20and%20Mengmi%20Zhang&entry.1292438233=Newborns%20perceive%20the%20world%20with%20low-acuity%2C%20color-degraded%2C%20and%20temporally%20continuous%20vision%2C%20which%20gradually%20sharpens%20as%20infants%20develop.%20To%20explore%20the%20ecological%20advantages%20of%20such%20staged%20%22visual%20diets%22%2C%20we%20train%20self-supervised%20learning%20%28SSL%29%20models%20on%20object-centric%20videos%20under%20constraints%20that%20simulate%20infant%20vision%3A%20grayscale-to-color%20%28C%29%2C%20blur-to-sharp%20%28A%29%2C%20and%20preserved%20temporal%20continuity%20%28T%29-collectively%20termed%20CATDiet.%20For%20evaluation%2C%20we%20establish%20a%20comprehensive%20benchmark%20across%20ten%20datasets%2C%20covering%20clean%20and%20corrupted%20image%20recognition%2C%20texture-shape%20cue%20conflict%20tests%2C%20silhouette%20recognition%2C%20depth-order%20classification%2C%20and%20the%20visual%20cliff%20paradigm.%20All%20CATDiet%20variants%20demonstrate%20enhanced%20robustness%20in%20object%20recognition%2C%20despite%20being%20trained%20solely%20on%20object-centric%20videos.%20Remarkably%2C%20models%20also%20exhibit%20biologically%20aligned%20developmental%20patterns%2C%20including%20neural%20plasticity%20changes%20mirroring%20synaptic%20density%20in%20macaque%20V1%20and%20behaviors%20resembling%20infants%27%20visual%20cliff%20responses.%20Building%20on%20these%20insights%2C%20CombDiet%20initializes%20SSL%20with%20CATDiet%20before%20standard%20training%20while%20preserving%20temporal%20continuity.%20Trained%20on%20object-centric%20or%20head-mounted%20infant%20videos%2C%20CombDiet%20outperforms%20standard%20SSL%20on%20both%20in-domain%20and%20out-of-domain%20object%20recognition%20and%20depth%20perception.%20Together%2C%20these%20results%20suggest%20that%20the%20developmental%20progression%20of%20early%20infant%20visual%20experience%20offers%20a%20powerful%20reverse-engineering%20framework%20for%20understanding%20the%20emergence%20of%20robust%20visual%20intelligence%20in%20machines.%20All%20code%2C%20data%2C%20and%20models%20will%20be%20publicly%20released.&entry.1838667208=http%3A//arxiv.org/abs/2511.14440v1&entry.124074799=Read"},
{"title": "ARC Is a Vision Problem!", "author": "Keya Hu and Ali Cy and Linlu Qiu and Xiaoman Delores Ding and Runqian Wang and Yeyin Eva Zhu and Jacob Andreas and Kaiming He", "abstract": "The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a \"canvas\" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.", "link": "http://arxiv.org/abs/2511.14761v1", "date": "2025-11-18", "relevancy": 2.2798, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5801}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARC%20Is%20a%20Vision%20Problem%21&body=Title%3A%20ARC%20Is%20a%20Vision%20Problem%21%0AAuthor%3A%20Keya%20Hu%20and%20Ali%20Cy%20and%20Linlu%20Qiu%20and%20Xiaoman%20Delores%20Ding%20and%20Runqian%20Wang%20and%20Yeyin%20Eva%20Zhu%20and%20Jacob%20Andreas%20and%20Kaiming%20He%0AAbstract%3A%20The%20Abstraction%20and%20Reasoning%20Corpus%20%28ARC%29%20is%20designed%20to%20promote%20research%20on%20abstract%20reasoning%2C%20a%20fundamental%20aspect%20of%20human%20intelligence.%20Common%20approaches%20to%20ARC%20treat%20it%20as%20a%20language-oriented%20problem%2C%20addressed%20by%20large%20language%20models%20%28LLMs%29%20or%20recurrent%20reasoning%20models.%20However%2C%20although%20the%20puzzle-like%20tasks%20in%20ARC%20are%20inherently%20visual%2C%20existing%20research%20has%20rarely%20approached%20the%20problem%20from%20a%20vision-centric%20perspective.%20In%20this%20work%2C%20we%20formulate%20ARC%20within%20a%20vision%20paradigm%2C%20framing%20it%20as%20an%20image-to-image%20translation%20problem.%20To%20incorporate%20visual%20priors%2C%20we%20represent%20the%20inputs%20on%20a%20%22canvas%22%20that%20can%20be%20processed%20like%20natural%20images.%20It%20is%20then%20natural%20for%20us%20to%20apply%20standard%20vision%20architectures%2C%20such%20as%20a%20vanilla%20Vision%20Transformer%20%28ViT%29%2C%20to%20perform%20image-to-image%20mapping.%20Our%20model%20is%20trained%20from%20scratch%20solely%20on%20ARC%20data%20and%20generalizes%20to%20unseen%20tasks%20through%20test-time%20training.%20Our%20framework%2C%20termed%20Vision%20ARC%20%28VARC%29%2C%20achieves%2060.4%25%20accuracy%20on%20the%20ARC-1%20benchmark%2C%20substantially%20outperforming%20existing%20methods%20that%20are%20also%20trained%20from%20scratch.%20Our%20results%20are%20competitive%20with%20those%20of%20leading%20LLMs%20and%20close%20the%20gap%20to%20average%20human%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARC%2520Is%2520a%2520Vision%2520Problem%2521%26entry.906535625%3DKeya%2520Hu%2520and%2520Ali%2520Cy%2520and%2520Linlu%2520Qiu%2520and%2520Xiaoman%2520Delores%2520Ding%2520and%2520Runqian%2520Wang%2520and%2520Yeyin%2520Eva%2520Zhu%2520and%2520Jacob%2520Andreas%2520and%2520Kaiming%2520He%26entry.1292438233%3DThe%2520Abstraction%2520and%2520Reasoning%2520Corpus%2520%2528ARC%2529%2520is%2520designed%2520to%2520promote%2520research%2520on%2520abstract%2520reasoning%252C%2520a%2520fundamental%2520aspect%2520of%2520human%2520intelligence.%2520Common%2520approaches%2520to%2520ARC%2520treat%2520it%2520as%2520a%2520language-oriented%2520problem%252C%2520addressed%2520by%2520large%2520language%2520models%2520%2528LLMs%2529%2520or%2520recurrent%2520reasoning%2520models.%2520However%252C%2520although%2520the%2520puzzle-like%2520tasks%2520in%2520ARC%2520are%2520inherently%2520visual%252C%2520existing%2520research%2520has%2520rarely%2520approached%2520the%2520problem%2520from%2520a%2520vision-centric%2520perspective.%2520In%2520this%2520work%252C%2520we%2520formulate%2520ARC%2520within%2520a%2520vision%2520paradigm%252C%2520framing%2520it%2520as%2520an%2520image-to-image%2520translation%2520problem.%2520To%2520incorporate%2520visual%2520priors%252C%2520we%2520represent%2520the%2520inputs%2520on%2520a%2520%2522canvas%2522%2520that%2520can%2520be%2520processed%2520like%2520natural%2520images.%2520It%2520is%2520then%2520natural%2520for%2520us%2520to%2520apply%2520standard%2520vision%2520architectures%252C%2520such%2520as%2520a%2520vanilla%2520Vision%2520Transformer%2520%2528ViT%2529%252C%2520to%2520perform%2520image-to-image%2520mapping.%2520Our%2520model%2520is%2520trained%2520from%2520scratch%2520solely%2520on%2520ARC%2520data%2520and%2520generalizes%2520to%2520unseen%2520tasks%2520through%2520test-time%2520training.%2520Our%2520framework%252C%2520termed%2520Vision%2520ARC%2520%2528VARC%2529%252C%2520achieves%252060.4%2525%2520accuracy%2520on%2520the%2520ARC-1%2520benchmark%252C%2520substantially%2520outperforming%2520existing%2520methods%2520that%2520are%2520also%2520trained%2520from%2520scratch.%2520Our%2520results%2520are%2520competitive%2520with%2520those%2520of%2520leading%2520LLMs%2520and%2520close%2520the%2520gap%2520to%2520average%2520human%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARC%20Is%20a%20Vision%20Problem%21&entry.906535625=Keya%20Hu%20and%20Ali%20Cy%20and%20Linlu%20Qiu%20and%20Xiaoman%20Delores%20Ding%20and%20Runqian%20Wang%20and%20Yeyin%20Eva%20Zhu%20and%20Jacob%20Andreas%20and%20Kaiming%20He&entry.1292438233=The%20Abstraction%20and%20Reasoning%20Corpus%20%28ARC%29%20is%20designed%20to%20promote%20research%20on%20abstract%20reasoning%2C%20a%20fundamental%20aspect%20of%20human%20intelligence.%20Common%20approaches%20to%20ARC%20treat%20it%20as%20a%20language-oriented%20problem%2C%20addressed%20by%20large%20language%20models%20%28LLMs%29%20or%20recurrent%20reasoning%20models.%20However%2C%20although%20the%20puzzle-like%20tasks%20in%20ARC%20are%20inherently%20visual%2C%20existing%20research%20has%20rarely%20approached%20the%20problem%20from%20a%20vision-centric%20perspective.%20In%20this%20work%2C%20we%20formulate%20ARC%20within%20a%20vision%20paradigm%2C%20framing%20it%20as%20an%20image-to-image%20translation%20problem.%20To%20incorporate%20visual%20priors%2C%20we%20represent%20the%20inputs%20on%20a%20%22canvas%22%20that%20can%20be%20processed%20like%20natural%20images.%20It%20is%20then%20natural%20for%20us%20to%20apply%20standard%20vision%20architectures%2C%20such%20as%20a%20vanilla%20Vision%20Transformer%20%28ViT%29%2C%20to%20perform%20image-to-image%20mapping.%20Our%20model%20is%20trained%20from%20scratch%20solely%20on%20ARC%20data%20and%20generalizes%20to%20unseen%20tasks%20through%20test-time%20training.%20Our%20framework%2C%20termed%20Vision%20ARC%20%28VARC%29%2C%20achieves%2060.4%25%20accuracy%20on%20the%20ARC-1%20benchmark%2C%20substantially%20outperforming%20existing%20methods%20that%20are%20also%20trained%20from%20scratch.%20Our%20results%20are%20competitive%20with%20those%20of%20leading%20LLMs%20and%20close%20the%20gap%20to%20average%20human%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2511.14761v1&entry.124074799=Read"},
{"title": "Audio Question Answering with GRPO-Based Fine-Tuning and Calibrated Segment-Level Predictions", "author": "Marcel Gibier and Nolwenn Celton and Rapha\u00ebl Duroselle and Pierre Serrano and Olivier Boeffard and Jean-Fran\u00e7ois Bonastre", "abstract": "In this report, we describe our submission to Track 5 of the DCASE 2025 Challenge for the task of Audio Question Answering(AQA). Our system leverages the SSL backbone BEATs to extract frame-level audio features, which are then processed by a classification head to generate segment-level predictions of acoustic events, following the Audioset ontology. These segment-level predictions are subsequently calibrated before producing event-level predictions. Finally, these predictions are incorporated into a structured prompt, along with the question and candidate answers. This prompt is then fed to a fine-tuned version of Qwen2.5-7B-Instruct, trained using the GRPO algorithm with a simple reward function. Our method achieves an accuracy of 62.6 % on the development set, demonstrating the effectiveness of combining acoustic event reasoning with instruction-tuned large language models for AQA.", "link": "http://arxiv.org/abs/2511.14307v1", "date": "2025-11-18", "relevancy": 2.2741, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio%20Question%20Answering%20with%20GRPO-Based%20Fine-Tuning%20and%20Calibrated%20Segment-Level%20Predictions&body=Title%3A%20Audio%20Question%20Answering%20with%20GRPO-Based%20Fine-Tuning%20and%20Calibrated%20Segment-Level%20Predictions%0AAuthor%3A%20Marcel%20Gibier%20and%20Nolwenn%20Celton%20and%20Rapha%C3%ABl%20Duroselle%20and%20Pierre%20Serrano%20and%20Olivier%20Boeffard%20and%20Jean-Fran%C3%A7ois%20Bonastre%0AAbstract%3A%20In%20this%20report%2C%20we%20describe%20our%20submission%20to%20Track%205%20of%20the%20DCASE%202025%20Challenge%20for%20the%20task%20of%20Audio%20Question%20Answering%28AQA%29.%20Our%20system%20leverages%20the%20SSL%20backbone%20BEATs%20to%20extract%20frame-level%20audio%20features%2C%20which%20are%20then%20processed%20by%20a%20classification%20head%20to%20generate%20segment-level%20predictions%20of%20acoustic%20events%2C%20following%20the%20Audioset%20ontology.%20These%20segment-level%20predictions%20are%20subsequently%20calibrated%20before%20producing%20event-level%20predictions.%20Finally%2C%20these%20predictions%20are%20incorporated%20into%20a%20structured%20prompt%2C%20along%20with%20the%20question%20and%20candidate%20answers.%20This%20prompt%20is%20then%20fed%20to%20a%20fine-tuned%20version%20of%20Qwen2.5-7B-Instruct%2C%20trained%20using%20the%20GRPO%20algorithm%20with%20a%20simple%20reward%20function.%20Our%20method%20achieves%20an%20accuracy%20of%2062.6%20%25%20on%20the%20development%20set%2C%20demonstrating%20the%20effectiveness%20of%20combining%20acoustic%20event%20reasoning%20with%20instruction-tuned%20large%20language%20models%20for%20AQA.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio%2520Question%2520Answering%2520with%2520GRPO-Based%2520Fine-Tuning%2520and%2520Calibrated%2520Segment-Level%2520Predictions%26entry.906535625%3DMarcel%2520Gibier%2520and%2520Nolwenn%2520Celton%2520and%2520Rapha%25C3%25ABl%2520Duroselle%2520and%2520Pierre%2520Serrano%2520and%2520Olivier%2520Boeffard%2520and%2520Jean-Fran%25C3%25A7ois%2520Bonastre%26entry.1292438233%3DIn%2520this%2520report%252C%2520we%2520describe%2520our%2520submission%2520to%2520Track%25205%2520of%2520the%2520DCASE%25202025%2520Challenge%2520for%2520the%2520task%2520of%2520Audio%2520Question%2520Answering%2528AQA%2529.%2520Our%2520system%2520leverages%2520the%2520SSL%2520backbone%2520BEATs%2520to%2520extract%2520frame-level%2520audio%2520features%252C%2520which%2520are%2520then%2520processed%2520by%2520a%2520classification%2520head%2520to%2520generate%2520segment-level%2520predictions%2520of%2520acoustic%2520events%252C%2520following%2520the%2520Audioset%2520ontology.%2520These%2520segment-level%2520predictions%2520are%2520subsequently%2520calibrated%2520before%2520producing%2520event-level%2520predictions.%2520Finally%252C%2520these%2520predictions%2520are%2520incorporated%2520into%2520a%2520structured%2520prompt%252C%2520along%2520with%2520the%2520question%2520and%2520candidate%2520answers.%2520This%2520prompt%2520is%2520then%2520fed%2520to%2520a%2520fine-tuned%2520version%2520of%2520Qwen2.5-7B-Instruct%252C%2520trained%2520using%2520the%2520GRPO%2520algorithm%2520with%2520a%2520simple%2520reward%2520function.%2520Our%2520method%2520achieves%2520an%2520accuracy%2520of%252062.6%2520%2525%2520on%2520the%2520development%2520set%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520combining%2520acoustic%2520event%2520reasoning%2520with%2520instruction-tuned%2520large%2520language%2520models%2520for%2520AQA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio%20Question%20Answering%20with%20GRPO-Based%20Fine-Tuning%20and%20Calibrated%20Segment-Level%20Predictions&entry.906535625=Marcel%20Gibier%20and%20Nolwenn%20Celton%20and%20Rapha%C3%ABl%20Duroselle%20and%20Pierre%20Serrano%20and%20Olivier%20Boeffard%20and%20Jean-Fran%C3%A7ois%20Bonastre&entry.1292438233=In%20this%20report%2C%20we%20describe%20our%20submission%20to%20Track%205%20of%20the%20DCASE%202025%20Challenge%20for%20the%20task%20of%20Audio%20Question%20Answering%28AQA%29.%20Our%20system%20leverages%20the%20SSL%20backbone%20BEATs%20to%20extract%20frame-level%20audio%20features%2C%20which%20are%20then%20processed%20by%20a%20classification%20head%20to%20generate%20segment-level%20predictions%20of%20acoustic%20events%2C%20following%20the%20Audioset%20ontology.%20These%20segment-level%20predictions%20are%20subsequently%20calibrated%20before%20producing%20event-level%20predictions.%20Finally%2C%20these%20predictions%20are%20incorporated%20into%20a%20structured%20prompt%2C%20along%20with%20the%20question%20and%20candidate%20answers.%20This%20prompt%20is%20then%20fed%20to%20a%20fine-tuned%20version%20of%20Qwen2.5-7B-Instruct%2C%20trained%20using%20the%20GRPO%20algorithm%20with%20a%20simple%20reward%20function.%20Our%20method%20achieves%20an%20accuracy%20of%2062.6%20%25%20on%20the%20development%20set%2C%20demonstrating%20the%20effectiveness%20of%20combining%20acoustic%20event%20reasoning%20with%20instruction-tuned%20large%20language%20models%20for%20AQA.&entry.1838667208=http%3A//arxiv.org/abs/2511.14307v1&entry.124074799=Read"},
{"title": "SweeperBot: Making 3D Browsing Accessible through View Analysis and Visual Question Answering", "author": "Chen Chen and Cuong Nguyen and Alexa Siu and Dingzeyu Li and Nadir Weibel", "abstract": "Accessing 3D models remains challenging for Screen Reader (SR) users. While some existing 3D viewers allow creators to provide alternative text, they often lack sufficient detail about the 3D models. Grounded on a formative study, this paper introduces SweeperBot, a system that enables SR users to leverage visual question answering to explore and compare 3D models. SweeperBot answers SR users' visual questions by combining an optimal view selection technique with the strength of generative- and recognition-based foundation models. An expert review with 10 Blind and Low-Vision (BLV) users with SR experience demonstrated the feasibility of using SweeperBot to assist BLV users in exploring and comparing 3D models. The quality of the descriptions generated by SweeperBot was validated by a second survey study with 30 sighted participants.", "link": "http://arxiv.org/abs/2511.14567v1", "date": "2025-11-18", "relevancy": 2.2669, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SweeperBot%3A%20Making%203D%20Browsing%20Accessible%20through%20View%20Analysis%20and%20Visual%20Question%20Answering&body=Title%3A%20SweeperBot%3A%20Making%203D%20Browsing%20Accessible%20through%20View%20Analysis%20and%20Visual%20Question%20Answering%0AAuthor%3A%20Chen%20Chen%20and%20Cuong%20Nguyen%20and%20Alexa%20Siu%20and%20Dingzeyu%20Li%20and%20Nadir%20Weibel%0AAbstract%3A%20Accessing%203D%20models%20remains%20challenging%20for%20Screen%20Reader%20%28SR%29%20users.%20While%20some%20existing%203D%20viewers%20allow%20creators%20to%20provide%20alternative%20text%2C%20they%20often%20lack%20sufficient%20detail%20about%20the%203D%20models.%20Grounded%20on%20a%20formative%20study%2C%20this%20paper%20introduces%20SweeperBot%2C%20a%20system%20that%20enables%20SR%20users%20to%20leverage%20visual%20question%20answering%20to%20explore%20and%20compare%203D%20models.%20SweeperBot%20answers%20SR%20users%27%20visual%20questions%20by%20combining%20an%20optimal%20view%20selection%20technique%20with%20the%20strength%20of%20generative-%20and%20recognition-based%20foundation%20models.%20An%20expert%20review%20with%2010%20Blind%20and%20Low-Vision%20%28BLV%29%20users%20with%20SR%20experience%20demonstrated%20the%20feasibility%20of%20using%20SweeperBot%20to%20assist%20BLV%20users%20in%20exploring%20and%20comparing%203D%20models.%20The%20quality%20of%20the%20descriptions%20generated%20by%20SweeperBot%20was%20validated%20by%20a%20second%20survey%20study%20with%2030%20sighted%20participants.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSweeperBot%253A%2520Making%25203D%2520Browsing%2520Accessible%2520through%2520View%2520Analysis%2520and%2520Visual%2520Question%2520Answering%26entry.906535625%3DChen%2520Chen%2520and%2520Cuong%2520Nguyen%2520and%2520Alexa%2520Siu%2520and%2520Dingzeyu%2520Li%2520and%2520Nadir%2520Weibel%26entry.1292438233%3DAccessing%25203D%2520models%2520remains%2520challenging%2520for%2520Screen%2520Reader%2520%2528SR%2529%2520users.%2520While%2520some%2520existing%25203D%2520viewers%2520allow%2520creators%2520to%2520provide%2520alternative%2520text%252C%2520they%2520often%2520lack%2520sufficient%2520detail%2520about%2520the%25203D%2520models.%2520Grounded%2520on%2520a%2520formative%2520study%252C%2520this%2520paper%2520introduces%2520SweeperBot%252C%2520a%2520system%2520that%2520enables%2520SR%2520users%2520to%2520leverage%2520visual%2520question%2520answering%2520to%2520explore%2520and%2520compare%25203D%2520models.%2520SweeperBot%2520answers%2520SR%2520users%2527%2520visual%2520questions%2520by%2520combining%2520an%2520optimal%2520view%2520selection%2520technique%2520with%2520the%2520strength%2520of%2520generative-%2520and%2520recognition-based%2520foundation%2520models.%2520An%2520expert%2520review%2520with%252010%2520Blind%2520and%2520Low-Vision%2520%2528BLV%2529%2520users%2520with%2520SR%2520experience%2520demonstrated%2520the%2520feasibility%2520of%2520using%2520SweeperBot%2520to%2520assist%2520BLV%2520users%2520in%2520exploring%2520and%2520comparing%25203D%2520models.%2520The%2520quality%2520of%2520the%2520descriptions%2520generated%2520by%2520SweeperBot%2520was%2520validated%2520by%2520a%2520second%2520survey%2520study%2520with%252030%2520sighted%2520participants.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SweeperBot%3A%20Making%203D%20Browsing%20Accessible%20through%20View%20Analysis%20and%20Visual%20Question%20Answering&entry.906535625=Chen%20Chen%20and%20Cuong%20Nguyen%20and%20Alexa%20Siu%20and%20Dingzeyu%20Li%20and%20Nadir%20Weibel&entry.1292438233=Accessing%203D%20models%20remains%20challenging%20for%20Screen%20Reader%20%28SR%29%20users.%20While%20some%20existing%203D%20viewers%20allow%20creators%20to%20provide%20alternative%20text%2C%20they%20often%20lack%20sufficient%20detail%20about%20the%203D%20models.%20Grounded%20on%20a%20formative%20study%2C%20this%20paper%20introduces%20SweeperBot%2C%20a%20system%20that%20enables%20SR%20users%20to%20leverage%20visual%20question%20answering%20to%20explore%20and%20compare%203D%20models.%20SweeperBot%20answers%20SR%20users%27%20visual%20questions%20by%20combining%20an%20optimal%20view%20selection%20technique%20with%20the%20strength%20of%20generative-%20and%20recognition-based%20foundation%20models.%20An%20expert%20review%20with%2010%20Blind%20and%20Low-Vision%20%28BLV%29%20users%20with%20SR%20experience%20demonstrated%20the%20feasibility%20of%20using%20SweeperBot%20to%20assist%20BLV%20users%20in%20exploring%20and%20comparing%203D%20models.%20The%20quality%20of%20the%20descriptions%20generated%20by%20SweeperBot%20was%20validated%20by%20a%20second%20survey%20study%20with%2030%20sighted%20participants.&entry.1838667208=http%3A//arxiv.org/abs/2511.14567v1&entry.124074799=Read"},
{"title": "HyMAD: A Hybrid Multi-Activity Detection Approach for Border Surveillance and Monitoring", "author": "Sriram Srinivasan and Srinivasan Aruchamy and Siva Ram Krisha Vadali", "abstract": "Seismic sensing has emerged as a promising solution for border surveillance and monitoring; the seismic sensors that are often buried underground are small and cannot be noticed easily, making them difficult for intruders to detect, avoid, or vandalize. This significantly enhances their effectiveness compared to highly visible cameras or fences. However, accurately detecting and distinguishing between overlapping activities that are happening simultaneously, such as human intrusions, animal movements, and vehicle rumbling, remains a major challenge due to the complex and noisy nature of seismic signals. Correctly identifying simultaneous activities is critical because failing to separate them can lead to misclassification, missed detections, and an incomplete understanding of the situation, thereby reducing the reliability of surveillance systems. To tackle this problem, we propose HyMAD (Hybrid Multi-Activity Detection), a deep neural architecture based on spatio-temporal feature fusion. The framework integrates spectral features extracted with SincNet and temporal dependencies modeled by a recurrent neural network (RNN). In addition, HyMAD employs self-attention layers to strengthen intra-modal representations and a cross-modal fusion module to achieve robust multi-label classification of seismic events. e evaluate our approach on a dataset constructed from real-world field recordings collected in the context of border surveillance and monitoring, demonstrating its ability to generalize to complex, simultaneous activity scenarios involving humans, animals, and vehicles. Our method achieves competitive performance and offers a modular framework for extending seismic-based activity recognition in real-world security applications.", "link": "http://arxiv.org/abs/2511.14698v1", "date": "2025-11-18", "relevancy": 2.2636, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6076}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5647}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyMAD%3A%20A%20Hybrid%20Multi-Activity%20Detection%20Approach%20for%20Border%20Surveillance%20and%20Monitoring&body=Title%3A%20HyMAD%3A%20A%20Hybrid%20Multi-Activity%20Detection%20Approach%20for%20Border%20Surveillance%20and%20Monitoring%0AAuthor%3A%20Sriram%20Srinivasan%20and%20Srinivasan%20Aruchamy%20and%20Siva%20Ram%20Krisha%20Vadali%0AAbstract%3A%20Seismic%20sensing%20has%20emerged%20as%20a%20promising%20solution%20for%20border%20surveillance%20and%20monitoring%3B%20the%20seismic%20sensors%20that%20are%20often%20buried%20underground%20are%20small%20and%20cannot%20be%20noticed%20easily%2C%20making%20them%20difficult%20for%20intruders%20to%20detect%2C%20avoid%2C%20or%20vandalize.%20This%20significantly%20enhances%20their%20effectiveness%20compared%20to%20highly%20visible%20cameras%20or%20fences.%20However%2C%20accurately%20detecting%20and%20distinguishing%20between%20overlapping%20activities%20that%20are%20happening%20simultaneously%2C%20such%20as%20human%20intrusions%2C%20animal%20movements%2C%20and%20vehicle%20rumbling%2C%20remains%20a%20major%20challenge%20due%20to%20the%20complex%20and%20noisy%20nature%20of%20seismic%20signals.%20Correctly%20identifying%20simultaneous%20activities%20is%20critical%20because%20failing%20to%20separate%20them%20can%20lead%20to%20misclassification%2C%20missed%20detections%2C%20and%20an%20incomplete%20understanding%20of%20the%20situation%2C%20thereby%20reducing%20the%20reliability%20of%20surveillance%20systems.%20To%20tackle%20this%20problem%2C%20we%20propose%20HyMAD%20%28Hybrid%20Multi-Activity%20Detection%29%2C%20a%20deep%20neural%20architecture%20based%20on%20spatio-temporal%20feature%20fusion.%20The%20framework%20integrates%20spectral%20features%20extracted%20with%20SincNet%20and%20temporal%20dependencies%20modeled%20by%20a%20recurrent%20neural%20network%20%28RNN%29.%20In%20addition%2C%20HyMAD%20employs%20self-attention%20layers%20to%20strengthen%20intra-modal%20representations%20and%20a%20cross-modal%20fusion%20module%20to%20achieve%20robust%20multi-label%20classification%20of%20seismic%20events.%20e%20evaluate%20our%20approach%20on%20a%20dataset%20constructed%20from%20real-world%20field%20recordings%20collected%20in%20the%20context%20of%20border%20surveillance%20and%20monitoring%2C%20demonstrating%20its%20ability%20to%20generalize%20to%20complex%2C%20simultaneous%20activity%20scenarios%20involving%20humans%2C%20animals%2C%20and%20vehicles.%20Our%20method%20achieves%20competitive%20performance%20and%20offers%20a%20modular%20framework%20for%20extending%20seismic-based%20activity%20recognition%20in%20real-world%20security%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyMAD%253A%2520A%2520Hybrid%2520Multi-Activity%2520Detection%2520Approach%2520for%2520Border%2520Surveillance%2520and%2520Monitoring%26entry.906535625%3DSriram%2520Srinivasan%2520and%2520Srinivasan%2520Aruchamy%2520and%2520Siva%2520Ram%2520Krisha%2520Vadali%26entry.1292438233%3DSeismic%2520sensing%2520has%2520emerged%2520as%2520a%2520promising%2520solution%2520for%2520border%2520surveillance%2520and%2520monitoring%253B%2520the%2520seismic%2520sensors%2520that%2520are%2520often%2520buried%2520underground%2520are%2520small%2520and%2520cannot%2520be%2520noticed%2520easily%252C%2520making%2520them%2520difficult%2520for%2520intruders%2520to%2520detect%252C%2520avoid%252C%2520or%2520vandalize.%2520This%2520significantly%2520enhances%2520their%2520effectiveness%2520compared%2520to%2520highly%2520visible%2520cameras%2520or%2520fences.%2520However%252C%2520accurately%2520detecting%2520and%2520distinguishing%2520between%2520overlapping%2520activities%2520that%2520are%2520happening%2520simultaneously%252C%2520such%2520as%2520human%2520intrusions%252C%2520animal%2520movements%252C%2520and%2520vehicle%2520rumbling%252C%2520remains%2520a%2520major%2520challenge%2520due%2520to%2520the%2520complex%2520and%2520noisy%2520nature%2520of%2520seismic%2520signals.%2520Correctly%2520identifying%2520simultaneous%2520activities%2520is%2520critical%2520because%2520failing%2520to%2520separate%2520them%2520can%2520lead%2520to%2520misclassification%252C%2520missed%2520detections%252C%2520and%2520an%2520incomplete%2520understanding%2520of%2520the%2520situation%252C%2520thereby%2520reducing%2520the%2520reliability%2520of%2520surveillance%2520systems.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520HyMAD%2520%2528Hybrid%2520Multi-Activity%2520Detection%2529%252C%2520a%2520deep%2520neural%2520architecture%2520based%2520on%2520spatio-temporal%2520feature%2520fusion.%2520The%2520framework%2520integrates%2520spectral%2520features%2520extracted%2520with%2520SincNet%2520and%2520temporal%2520dependencies%2520modeled%2520by%2520a%2520recurrent%2520neural%2520network%2520%2528RNN%2529.%2520In%2520addition%252C%2520HyMAD%2520employs%2520self-attention%2520layers%2520to%2520strengthen%2520intra-modal%2520representations%2520and%2520a%2520cross-modal%2520fusion%2520module%2520to%2520achieve%2520robust%2520multi-label%2520classification%2520of%2520seismic%2520events.%2520e%2520evaluate%2520our%2520approach%2520on%2520a%2520dataset%2520constructed%2520from%2520real-world%2520field%2520recordings%2520collected%2520in%2520the%2520context%2520of%2520border%2520surveillance%2520and%2520monitoring%252C%2520demonstrating%2520its%2520ability%2520to%2520generalize%2520to%2520complex%252C%2520simultaneous%2520activity%2520scenarios%2520involving%2520humans%252C%2520animals%252C%2520and%2520vehicles.%2520Our%2520method%2520achieves%2520competitive%2520performance%2520and%2520offers%2520a%2520modular%2520framework%2520for%2520extending%2520seismic-based%2520activity%2520recognition%2520in%2520real-world%2520security%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyMAD%3A%20A%20Hybrid%20Multi-Activity%20Detection%20Approach%20for%20Border%20Surveillance%20and%20Monitoring&entry.906535625=Sriram%20Srinivasan%20and%20Srinivasan%20Aruchamy%20and%20Siva%20Ram%20Krisha%20Vadali&entry.1292438233=Seismic%20sensing%20has%20emerged%20as%20a%20promising%20solution%20for%20border%20surveillance%20and%20monitoring%3B%20the%20seismic%20sensors%20that%20are%20often%20buried%20underground%20are%20small%20and%20cannot%20be%20noticed%20easily%2C%20making%20them%20difficult%20for%20intruders%20to%20detect%2C%20avoid%2C%20or%20vandalize.%20This%20significantly%20enhances%20their%20effectiveness%20compared%20to%20highly%20visible%20cameras%20or%20fences.%20However%2C%20accurately%20detecting%20and%20distinguishing%20between%20overlapping%20activities%20that%20are%20happening%20simultaneously%2C%20such%20as%20human%20intrusions%2C%20animal%20movements%2C%20and%20vehicle%20rumbling%2C%20remains%20a%20major%20challenge%20due%20to%20the%20complex%20and%20noisy%20nature%20of%20seismic%20signals.%20Correctly%20identifying%20simultaneous%20activities%20is%20critical%20because%20failing%20to%20separate%20them%20can%20lead%20to%20misclassification%2C%20missed%20detections%2C%20and%20an%20incomplete%20understanding%20of%20the%20situation%2C%20thereby%20reducing%20the%20reliability%20of%20surveillance%20systems.%20To%20tackle%20this%20problem%2C%20we%20propose%20HyMAD%20%28Hybrid%20Multi-Activity%20Detection%29%2C%20a%20deep%20neural%20architecture%20based%20on%20spatio-temporal%20feature%20fusion.%20The%20framework%20integrates%20spectral%20features%20extracted%20with%20SincNet%20and%20temporal%20dependencies%20modeled%20by%20a%20recurrent%20neural%20network%20%28RNN%29.%20In%20addition%2C%20HyMAD%20employs%20self-attention%20layers%20to%20strengthen%20intra-modal%20representations%20and%20a%20cross-modal%20fusion%20module%20to%20achieve%20robust%20multi-label%20classification%20of%20seismic%20events.%20e%20evaluate%20our%20approach%20on%20a%20dataset%20constructed%20from%20real-world%20field%20recordings%20collected%20in%20the%20context%20of%20border%20surveillance%20and%20monitoring%2C%20demonstrating%20its%20ability%20to%20generalize%20to%20complex%2C%20simultaneous%20activity%20scenarios%20involving%20humans%2C%20animals%2C%20and%20vehicles.%20Our%20method%20achieves%20competitive%20performance%20and%20offers%20a%20modular%20framework%20for%20extending%20seismic-based%20activity%20recognition%20in%20real-world%20security%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2511.14698v1&entry.124074799=Read"},
{"title": "Beyond Means: A Dynamic Framework for Predicting Customer Satisfaction", "author": "Christof Naumzik and Abdurahman Maarouf and Stefan Feuerriegel and Markus Weinmann", "abstract": "Online ratings influence customer decision-making, yet standard aggregation methods, such as the sample mean, fail to adapt to quality changes over time and ignore review heterogeneity (e.g., review sentiment, a review's helpfulness). To address these challenges, we demonstrate the value of using the Gaussian process (GP) framework for rating aggregation. Specifically, we present a tailored GP model that captures the dynamics of ratings over time while additionally accounting for review heterogeneity. Based on 121,123 ratings from Yelp, we compare the predictive power of different rating aggregation methods in predicting future ratings, thereby finding that the GP model is considerably more accurate and reduces the mean absolute error by 10.2% compared to the sample mean. Our findings have important implications for marketing practitioners and customers. By moving beyond means, designers of online reputation systems can display more informative and adaptive aggregated rating scores that are accurate signals of expected customer satisfaction.", "link": "http://arxiv.org/abs/2511.14743v1", "date": "2025-11-18", "relevancy": 2.2541, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4861}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4333}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Means%3A%20A%20Dynamic%20Framework%20for%20Predicting%20Customer%20Satisfaction&body=Title%3A%20Beyond%20Means%3A%20A%20Dynamic%20Framework%20for%20Predicting%20Customer%20Satisfaction%0AAuthor%3A%20Christof%20Naumzik%20and%20Abdurahman%20Maarouf%20and%20Stefan%20Feuerriegel%20and%20Markus%20Weinmann%0AAbstract%3A%20Online%20ratings%20influence%20customer%20decision-making%2C%20yet%20standard%20aggregation%20methods%2C%20such%20as%20the%20sample%20mean%2C%20fail%20to%20adapt%20to%20quality%20changes%20over%20time%20and%20ignore%20review%20heterogeneity%20%28e.g.%2C%20review%20sentiment%2C%20a%20review%27s%20helpfulness%29.%20To%20address%20these%20challenges%2C%20we%20demonstrate%20the%20value%20of%20using%20the%20Gaussian%20process%20%28GP%29%20framework%20for%20rating%20aggregation.%20Specifically%2C%20we%20present%20a%20tailored%20GP%20model%20that%20captures%20the%20dynamics%20of%20ratings%20over%20time%20while%20additionally%20accounting%20for%20review%20heterogeneity.%20Based%20on%20121%2C123%20ratings%20from%20Yelp%2C%20we%20compare%20the%20predictive%20power%20of%20different%20rating%20aggregation%20methods%20in%20predicting%20future%20ratings%2C%20thereby%20finding%20that%20the%20GP%20model%20is%20considerably%20more%20accurate%20and%20reduces%20the%20mean%20absolute%20error%20by%2010.2%25%20compared%20to%20the%20sample%20mean.%20Our%20findings%20have%20important%20implications%20for%20marketing%20practitioners%20and%20customers.%20By%20moving%20beyond%20means%2C%20designers%20of%20online%20reputation%20systems%20can%20display%20more%20informative%20and%20adaptive%20aggregated%20rating%20scores%20that%20are%20accurate%20signals%20of%20expected%20customer%20satisfaction.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Means%253A%2520A%2520Dynamic%2520Framework%2520for%2520Predicting%2520Customer%2520Satisfaction%26entry.906535625%3DChristof%2520Naumzik%2520and%2520Abdurahman%2520Maarouf%2520and%2520Stefan%2520Feuerriegel%2520and%2520Markus%2520Weinmann%26entry.1292438233%3DOnline%2520ratings%2520influence%2520customer%2520decision-making%252C%2520yet%2520standard%2520aggregation%2520methods%252C%2520such%2520as%2520the%2520sample%2520mean%252C%2520fail%2520to%2520adapt%2520to%2520quality%2520changes%2520over%2520time%2520and%2520ignore%2520review%2520heterogeneity%2520%2528e.g.%252C%2520review%2520sentiment%252C%2520a%2520review%2527s%2520helpfulness%2529.%2520To%2520address%2520these%2520challenges%252C%2520we%2520demonstrate%2520the%2520value%2520of%2520using%2520the%2520Gaussian%2520process%2520%2528GP%2529%2520framework%2520for%2520rating%2520aggregation.%2520Specifically%252C%2520we%2520present%2520a%2520tailored%2520GP%2520model%2520that%2520captures%2520the%2520dynamics%2520of%2520ratings%2520over%2520time%2520while%2520additionally%2520accounting%2520for%2520review%2520heterogeneity.%2520Based%2520on%2520121%252C123%2520ratings%2520from%2520Yelp%252C%2520we%2520compare%2520the%2520predictive%2520power%2520of%2520different%2520rating%2520aggregation%2520methods%2520in%2520predicting%2520future%2520ratings%252C%2520thereby%2520finding%2520that%2520the%2520GP%2520model%2520is%2520considerably%2520more%2520accurate%2520and%2520reduces%2520the%2520mean%2520absolute%2520error%2520by%252010.2%2525%2520compared%2520to%2520the%2520sample%2520mean.%2520Our%2520findings%2520have%2520important%2520implications%2520for%2520marketing%2520practitioners%2520and%2520customers.%2520By%2520moving%2520beyond%2520means%252C%2520designers%2520of%2520online%2520reputation%2520systems%2520can%2520display%2520more%2520informative%2520and%2520adaptive%2520aggregated%2520rating%2520scores%2520that%2520are%2520accurate%2520signals%2520of%2520expected%2520customer%2520satisfaction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Means%3A%20A%20Dynamic%20Framework%20for%20Predicting%20Customer%20Satisfaction&entry.906535625=Christof%20Naumzik%20and%20Abdurahman%20Maarouf%20and%20Stefan%20Feuerriegel%20and%20Markus%20Weinmann&entry.1292438233=Online%20ratings%20influence%20customer%20decision-making%2C%20yet%20standard%20aggregation%20methods%2C%20such%20as%20the%20sample%20mean%2C%20fail%20to%20adapt%20to%20quality%20changes%20over%20time%20and%20ignore%20review%20heterogeneity%20%28e.g.%2C%20review%20sentiment%2C%20a%20review%27s%20helpfulness%29.%20To%20address%20these%20challenges%2C%20we%20demonstrate%20the%20value%20of%20using%20the%20Gaussian%20process%20%28GP%29%20framework%20for%20rating%20aggregation.%20Specifically%2C%20we%20present%20a%20tailored%20GP%20model%20that%20captures%20the%20dynamics%20of%20ratings%20over%20time%20while%20additionally%20accounting%20for%20review%20heterogeneity.%20Based%20on%20121%2C123%20ratings%20from%20Yelp%2C%20we%20compare%20the%20predictive%20power%20of%20different%20rating%20aggregation%20methods%20in%20predicting%20future%20ratings%2C%20thereby%20finding%20that%20the%20GP%20model%20is%20considerably%20more%20accurate%20and%20reduces%20the%20mean%20absolute%20error%20by%2010.2%25%20compared%20to%20the%20sample%20mean.%20Our%20findings%20have%20important%20implications%20for%20marketing%20practitioners%20and%20customers.%20By%20moving%20beyond%20means%2C%20designers%20of%20online%20reputation%20systems%20can%20display%20more%20informative%20and%20adaptive%20aggregated%20rating%20scores%20that%20are%20accurate%20signals%20of%20expected%20customer%20satisfaction.&entry.1838667208=http%3A//arxiv.org/abs/2511.14743v1&entry.124074799=Read"},
{"title": "Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning", "author": "Fabian Stricker and David Bermbach and Christian Zirpins", "abstract": "Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.\n  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.", "link": "http://arxiv.org/abs/2511.14456v1", "date": "2025-11-18", "relevancy": 2.2485, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.46}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4445}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20the%20Impact%20of%20Participant%20Failures%20in%20Cross-Silo%20Federated%20Learning&body=Title%3A%20Analyzing%20the%20Impact%20of%20Participant%20Failures%20in%20Cross-Silo%20Federated%20Learning%0AAuthor%3A%20Fabian%20Stricker%20and%20David%20Bermbach%20and%20Christian%20Zirpins%0AAbstract%3A%20Federated%20learning%20%28FL%29%20is%20a%20new%20paradigm%20for%20training%20machine%20learning%20%28ML%29%20models%20without%20sharing%20data.%20While%20applying%20FL%20in%20cross-silo%20scenarios%2C%20where%20organizations%20collaborate%2C%20it%20is%20necessary%20that%20the%20FL%20system%20is%20reliable%3B%20however%2C%20participants%20can%20fail%20due%20to%20various%20reasons%20%28e.g.%2C%20communication%20issues%20or%20misconfigurations%29.%20In%20order%20to%20provide%20a%20reliable%20system%2C%20it%20is%20necessary%20to%20analyze%20the%20impact%20of%20participant%20failures.%20While%20this%20problem%20received%20attention%20in%20cross-device%20FL%20where%20mobile%20devices%20with%20limited%20resources%20participate%2C%20there%20is%20comparatively%20little%20research%20in%20cross-silo%20FL.%0A%20%20Therefore%2C%20we%20conduct%20an%20extensive%20study%20for%20analyzing%20the%20impact%20of%20participant%20failures%20on%20the%20model%20quality%20in%20the%20context%20of%20inter-organizational%20cross-silo%20FL%20with%20few%20participants.%20In%20our%20study%2C%20we%20focus%20on%20analyzing%20generally%20influential%20factors%20such%20as%20the%20impact%20of%20the%20timing%20and%20the%20data%20as%20well%20as%20the%20impact%20on%20the%20evaluation%2C%20which%20is%20important%20for%20deciding%2C%20if%20the%20model%20should%20be%20deployed.%20We%20show%20that%20under%20high%20skews%20the%20evaluation%20is%20optimistic%20and%20hides%20the%20real%20impact.%20Furthermore%2C%20we%20demonstrate%20that%20the%20timing%20impacts%20the%20quality%20of%20the%20trained%20model.%20Our%20results%20offer%20insights%20for%20researchers%20and%20software%20architects%20aiming%20to%20build%20robust%20FL%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520the%2520Impact%2520of%2520Participant%2520Failures%2520in%2520Cross-Silo%2520Federated%2520Learning%26entry.906535625%3DFabian%2520Stricker%2520and%2520David%2520Bermbach%2520and%2520Christian%2520Zirpins%26entry.1292438233%3DFederated%2520learning%2520%2528FL%2529%2520is%2520a%2520new%2520paradigm%2520for%2520training%2520machine%2520learning%2520%2528ML%2529%2520models%2520without%2520sharing%2520data.%2520While%2520applying%2520FL%2520in%2520cross-silo%2520scenarios%252C%2520where%2520organizations%2520collaborate%252C%2520it%2520is%2520necessary%2520that%2520the%2520FL%2520system%2520is%2520reliable%253B%2520however%252C%2520participants%2520can%2520fail%2520due%2520to%2520various%2520reasons%2520%2528e.g.%252C%2520communication%2520issues%2520or%2520misconfigurations%2529.%2520In%2520order%2520to%2520provide%2520a%2520reliable%2520system%252C%2520it%2520is%2520necessary%2520to%2520analyze%2520the%2520impact%2520of%2520participant%2520failures.%2520While%2520this%2520problem%2520received%2520attention%2520in%2520cross-device%2520FL%2520where%2520mobile%2520devices%2520with%2520limited%2520resources%2520participate%252C%2520there%2520is%2520comparatively%2520little%2520research%2520in%2520cross-silo%2520FL.%250A%2520%2520Therefore%252C%2520we%2520conduct%2520an%2520extensive%2520study%2520for%2520analyzing%2520the%2520impact%2520of%2520participant%2520failures%2520on%2520the%2520model%2520quality%2520in%2520the%2520context%2520of%2520inter-organizational%2520cross-silo%2520FL%2520with%2520few%2520participants.%2520In%2520our%2520study%252C%2520we%2520focus%2520on%2520analyzing%2520generally%2520influential%2520factors%2520such%2520as%2520the%2520impact%2520of%2520the%2520timing%2520and%2520the%2520data%2520as%2520well%2520as%2520the%2520impact%2520on%2520the%2520evaluation%252C%2520which%2520is%2520important%2520for%2520deciding%252C%2520if%2520the%2520model%2520should%2520be%2520deployed.%2520We%2520show%2520that%2520under%2520high%2520skews%2520the%2520evaluation%2520is%2520optimistic%2520and%2520hides%2520the%2520real%2520impact.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520the%2520timing%2520impacts%2520the%2520quality%2520of%2520the%2520trained%2520model.%2520Our%2520results%2520offer%2520insights%2520for%2520researchers%2520and%2520software%2520architects%2520aiming%2520to%2520build%2520robust%2520FL%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20the%20Impact%20of%20Participant%20Failures%20in%20Cross-Silo%20Federated%20Learning&entry.906535625=Fabian%20Stricker%20and%20David%20Bermbach%20and%20Christian%20Zirpins&entry.1292438233=Federated%20learning%20%28FL%29%20is%20a%20new%20paradigm%20for%20training%20machine%20learning%20%28ML%29%20models%20without%20sharing%20data.%20While%20applying%20FL%20in%20cross-silo%20scenarios%2C%20where%20organizations%20collaborate%2C%20it%20is%20necessary%20that%20the%20FL%20system%20is%20reliable%3B%20however%2C%20participants%20can%20fail%20due%20to%20various%20reasons%20%28e.g.%2C%20communication%20issues%20or%20misconfigurations%29.%20In%20order%20to%20provide%20a%20reliable%20system%2C%20it%20is%20necessary%20to%20analyze%20the%20impact%20of%20participant%20failures.%20While%20this%20problem%20received%20attention%20in%20cross-device%20FL%20where%20mobile%20devices%20with%20limited%20resources%20participate%2C%20there%20is%20comparatively%20little%20research%20in%20cross-silo%20FL.%0A%20%20Therefore%2C%20we%20conduct%20an%20extensive%20study%20for%20analyzing%20the%20impact%20of%20participant%20failures%20on%20the%20model%20quality%20in%20the%20context%20of%20inter-organizational%20cross-silo%20FL%20with%20few%20participants.%20In%20our%20study%2C%20we%20focus%20on%20analyzing%20generally%20influential%20factors%20such%20as%20the%20impact%20of%20the%20timing%20and%20the%20data%20as%20well%20as%20the%20impact%20on%20the%20evaluation%2C%20which%20is%20important%20for%20deciding%2C%20if%20the%20model%20should%20be%20deployed.%20We%20show%20that%20under%20high%20skews%20the%20evaluation%20is%20optimistic%20and%20hides%20the%20real%20impact.%20Furthermore%2C%20we%20demonstrate%20that%20the%20timing%20impacts%20the%20quality%20of%20the%20trained%20model.%20Our%20results%20offer%20insights%20for%20researchers%20and%20software%20architects%20aiming%20to%20build%20robust%20FL%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.14456v1&entry.124074799=Read"},
{"title": "Learning Compact Latent Space for Representing Neural Signed Distance Functions with High-fidelity Geometry Details", "author": "Qiang Bai and Bojian Wu and Xi Yang and Zhizhong Han", "abstract": "Neural signed distance functions (SDFs) have been a vital representation to represent 3D shapes or scenes with neural networks. An SDF is an implicit function that can query signed distances at specific coordinates for recovering a 3D surface. Although implicit functions work well on a single shape or scene, they pose obstacles when analyzing multiple SDFs with high-fidelity geometry details, due to the limited information encoded in the latent space for SDFs and the loss of geometry details. To overcome these obstacles, we introduce a method to represent multiple SDFs in a common space, aiming to recover more high-fidelity geometry details with more compact latent representations. Our key idea is to take full advantage of the benefits of generalization-based and overfitting-based learning strategies, which manage to preserve high-fidelity geometry details with compact latent codes. Based on this framework, we also introduce a novel sampling strategy to sample training queries. The sampling can improve the training efficiency and eliminate artifacts caused by the influence of other SDFs. We report numerical and visual evaluations on widely used benchmarks to validate our designs and show advantages over the latest methods in terms of the representative ability and compactness.", "link": "http://arxiv.org/abs/2511.14539v1", "date": "2025-11-18", "relevancy": 2.2268, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5636}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5582}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Compact%20Latent%20Space%20for%20Representing%20Neural%20Signed%20Distance%20Functions%20with%20High-fidelity%20Geometry%20Details&body=Title%3A%20Learning%20Compact%20Latent%20Space%20for%20Representing%20Neural%20Signed%20Distance%20Functions%20with%20High-fidelity%20Geometry%20Details%0AAuthor%3A%20Qiang%20Bai%20and%20Bojian%20Wu%20and%20Xi%20Yang%20and%20Zhizhong%20Han%0AAbstract%3A%20Neural%20signed%20distance%20functions%20%28SDFs%29%20have%20been%20a%20vital%20representation%20to%20represent%203D%20shapes%20or%20scenes%20with%20neural%20networks.%20An%20SDF%20is%20an%20implicit%20function%20that%20can%20query%20signed%20distances%20at%20specific%20coordinates%20for%20recovering%20a%203D%20surface.%20Although%20implicit%20functions%20work%20well%20on%20a%20single%20shape%20or%20scene%2C%20they%20pose%20obstacles%20when%20analyzing%20multiple%20SDFs%20with%20high-fidelity%20geometry%20details%2C%20due%20to%20the%20limited%20information%20encoded%20in%20the%20latent%20space%20for%20SDFs%20and%20the%20loss%20of%20geometry%20details.%20To%20overcome%20these%20obstacles%2C%20we%20introduce%20a%20method%20to%20represent%20multiple%20SDFs%20in%20a%20common%20space%2C%20aiming%20to%20recover%20more%20high-fidelity%20geometry%20details%20with%20more%20compact%20latent%20representations.%20Our%20key%20idea%20is%20to%20take%20full%20advantage%20of%20the%20benefits%20of%20generalization-based%20and%20overfitting-based%20learning%20strategies%2C%20which%20manage%20to%20preserve%20high-fidelity%20geometry%20details%20with%20compact%20latent%20codes.%20Based%20on%20this%20framework%2C%20we%20also%20introduce%20a%20novel%20sampling%20strategy%20to%20sample%20training%20queries.%20The%20sampling%20can%20improve%20the%20training%20efficiency%20and%20eliminate%20artifacts%20caused%20by%20the%20influence%20of%20other%20SDFs.%20We%20report%20numerical%20and%20visual%20evaluations%20on%20widely%20used%20benchmarks%20to%20validate%20our%20designs%20and%20show%20advantages%20over%20the%20latest%20methods%20in%20terms%20of%20the%20representative%20ability%20and%20compactness.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Compact%2520Latent%2520Space%2520for%2520Representing%2520Neural%2520Signed%2520Distance%2520Functions%2520with%2520High-fidelity%2520Geometry%2520Details%26entry.906535625%3DQiang%2520Bai%2520and%2520Bojian%2520Wu%2520and%2520Xi%2520Yang%2520and%2520Zhizhong%2520Han%26entry.1292438233%3DNeural%2520signed%2520distance%2520functions%2520%2528SDFs%2529%2520have%2520been%2520a%2520vital%2520representation%2520to%2520represent%25203D%2520shapes%2520or%2520scenes%2520with%2520neural%2520networks.%2520An%2520SDF%2520is%2520an%2520implicit%2520function%2520that%2520can%2520query%2520signed%2520distances%2520at%2520specific%2520coordinates%2520for%2520recovering%2520a%25203D%2520surface.%2520Although%2520implicit%2520functions%2520work%2520well%2520on%2520a%2520single%2520shape%2520or%2520scene%252C%2520they%2520pose%2520obstacles%2520when%2520analyzing%2520multiple%2520SDFs%2520with%2520high-fidelity%2520geometry%2520details%252C%2520due%2520to%2520the%2520limited%2520information%2520encoded%2520in%2520the%2520latent%2520space%2520for%2520SDFs%2520and%2520the%2520loss%2520of%2520geometry%2520details.%2520To%2520overcome%2520these%2520obstacles%252C%2520we%2520introduce%2520a%2520method%2520to%2520represent%2520multiple%2520SDFs%2520in%2520a%2520common%2520space%252C%2520aiming%2520to%2520recover%2520more%2520high-fidelity%2520geometry%2520details%2520with%2520more%2520compact%2520latent%2520representations.%2520Our%2520key%2520idea%2520is%2520to%2520take%2520full%2520advantage%2520of%2520the%2520benefits%2520of%2520generalization-based%2520and%2520overfitting-based%2520learning%2520strategies%252C%2520which%2520manage%2520to%2520preserve%2520high-fidelity%2520geometry%2520details%2520with%2520compact%2520latent%2520codes.%2520Based%2520on%2520this%2520framework%252C%2520we%2520also%2520introduce%2520a%2520novel%2520sampling%2520strategy%2520to%2520sample%2520training%2520queries.%2520The%2520sampling%2520can%2520improve%2520the%2520training%2520efficiency%2520and%2520eliminate%2520artifacts%2520caused%2520by%2520the%2520influence%2520of%2520other%2520SDFs.%2520We%2520report%2520numerical%2520and%2520visual%2520evaluations%2520on%2520widely%2520used%2520benchmarks%2520to%2520validate%2520our%2520designs%2520and%2520show%2520advantages%2520over%2520the%2520latest%2520methods%2520in%2520terms%2520of%2520the%2520representative%2520ability%2520and%2520compactness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Compact%20Latent%20Space%20for%20Representing%20Neural%20Signed%20Distance%20Functions%20with%20High-fidelity%20Geometry%20Details&entry.906535625=Qiang%20Bai%20and%20Bojian%20Wu%20and%20Xi%20Yang%20and%20Zhizhong%20Han&entry.1292438233=Neural%20signed%20distance%20functions%20%28SDFs%29%20have%20been%20a%20vital%20representation%20to%20represent%203D%20shapes%20or%20scenes%20with%20neural%20networks.%20An%20SDF%20is%20an%20implicit%20function%20that%20can%20query%20signed%20distances%20at%20specific%20coordinates%20for%20recovering%20a%203D%20surface.%20Although%20implicit%20functions%20work%20well%20on%20a%20single%20shape%20or%20scene%2C%20they%20pose%20obstacles%20when%20analyzing%20multiple%20SDFs%20with%20high-fidelity%20geometry%20details%2C%20due%20to%20the%20limited%20information%20encoded%20in%20the%20latent%20space%20for%20SDFs%20and%20the%20loss%20of%20geometry%20details.%20To%20overcome%20these%20obstacles%2C%20we%20introduce%20a%20method%20to%20represent%20multiple%20SDFs%20in%20a%20common%20space%2C%20aiming%20to%20recover%20more%20high-fidelity%20geometry%20details%20with%20more%20compact%20latent%20representations.%20Our%20key%20idea%20is%20to%20take%20full%20advantage%20of%20the%20benefits%20of%20generalization-based%20and%20overfitting-based%20learning%20strategies%2C%20which%20manage%20to%20preserve%20high-fidelity%20geometry%20details%20with%20compact%20latent%20codes.%20Based%20on%20this%20framework%2C%20we%20also%20introduce%20a%20novel%20sampling%20strategy%20to%20sample%20training%20queries.%20The%20sampling%20can%20improve%20the%20training%20efficiency%20and%20eliminate%20artifacts%20caused%20by%20the%20influence%20of%20other%20SDFs.%20We%20report%20numerical%20and%20visual%20evaluations%20on%20widely%20used%20benchmarks%20to%20validate%20our%20designs%20and%20show%20advantages%20over%20the%20latest%20methods%20in%20terms%20of%20the%20representative%20ability%20and%20compactness.&entry.1838667208=http%3A//arxiv.org/abs/2511.14539v1&entry.124074799=Read"},
{"title": "Ground Truth Generation for Multilingual Historical NLP using LLMs", "author": "Clovis Gladstone and Zhao Fang and Spencer Dean Stewart", "abstract": "Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.", "link": "http://arxiv.org/abs/2511.14688v1", "date": "2025-11-18", "relevancy": 2.2261, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4705}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ground%20Truth%20Generation%20for%20Multilingual%20Historical%20NLP%20using%20LLMs&body=Title%3A%20Ground%20Truth%20Generation%20for%20Multilingual%20Historical%20NLP%20using%20LLMs%0AAuthor%3A%20Clovis%20Gladstone%20and%20Zhao%20Fang%20and%20Spencer%20Dean%20Stewart%0AAbstract%3A%20Historical%20and%20low-resource%20NLP%20remains%20challenging%20due%20to%20limited%20annotated%20data%20and%20domain%20mismatches%20with%20modern%2C%20web-sourced%20corpora.%20This%20paper%20outlines%20our%20work%20in%20using%20large%20language%20models%20%28LLMs%29%20to%20create%20ground-truth%20annotations%20for%20historical%20French%20%2816th-20th%20centuries%29%20and%20Chinese%20%281900-1950%29%20texts.%20By%20leveraging%20LLM-generated%20ground%20truth%20on%20a%20subset%20of%20our%20corpus%2C%20we%20were%20able%20to%20fine-tune%20spaCy%20to%20achieve%20significant%20gains%20on%20period-specific%20tests%20for%20part-of-speech%20%28POS%29%20annotations%2C%20lemmatization%2C%20and%20named%20entity%20recognition%20%28NER%29.%20Our%20results%20underscore%20the%20importance%20of%20domain-specific%20models%20and%20demonstrate%20that%20even%20relatively%20limited%20amounts%20of%20synthetic%20data%20can%20improve%20NLP%20tools%20for%20under-resourced%20corpora%20in%20computational%20humanities%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGround%2520Truth%2520Generation%2520for%2520Multilingual%2520Historical%2520NLP%2520using%2520LLMs%26entry.906535625%3DClovis%2520Gladstone%2520and%2520Zhao%2520Fang%2520and%2520Spencer%2520Dean%2520Stewart%26entry.1292438233%3DHistorical%2520and%2520low-resource%2520NLP%2520remains%2520challenging%2520due%2520to%2520limited%2520annotated%2520data%2520and%2520domain%2520mismatches%2520with%2520modern%252C%2520web-sourced%2520corpora.%2520This%2520paper%2520outlines%2520our%2520work%2520in%2520using%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520create%2520ground-truth%2520annotations%2520for%2520historical%2520French%2520%252816th-20th%2520centuries%2529%2520and%2520Chinese%2520%25281900-1950%2529%2520texts.%2520By%2520leveraging%2520LLM-generated%2520ground%2520truth%2520on%2520a%2520subset%2520of%2520our%2520corpus%252C%2520we%2520were%2520able%2520to%2520fine-tune%2520spaCy%2520to%2520achieve%2520significant%2520gains%2520on%2520period-specific%2520tests%2520for%2520part-of-speech%2520%2528POS%2529%2520annotations%252C%2520lemmatization%252C%2520and%2520named%2520entity%2520recognition%2520%2528NER%2529.%2520Our%2520results%2520underscore%2520the%2520importance%2520of%2520domain-specific%2520models%2520and%2520demonstrate%2520that%2520even%2520relatively%2520limited%2520amounts%2520of%2520synthetic%2520data%2520can%2520improve%2520NLP%2520tools%2520for%2520under-resourced%2520corpora%2520in%2520computational%2520humanities%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ground%20Truth%20Generation%20for%20Multilingual%20Historical%20NLP%20using%20LLMs&entry.906535625=Clovis%20Gladstone%20and%20Zhao%20Fang%20and%20Spencer%20Dean%20Stewart&entry.1292438233=Historical%20and%20low-resource%20NLP%20remains%20challenging%20due%20to%20limited%20annotated%20data%20and%20domain%20mismatches%20with%20modern%2C%20web-sourced%20corpora.%20This%20paper%20outlines%20our%20work%20in%20using%20large%20language%20models%20%28LLMs%29%20to%20create%20ground-truth%20annotations%20for%20historical%20French%20%2816th-20th%20centuries%29%20and%20Chinese%20%281900-1950%29%20texts.%20By%20leveraging%20LLM-generated%20ground%20truth%20on%20a%20subset%20of%20our%20corpus%2C%20we%20were%20able%20to%20fine-tune%20spaCy%20to%20achieve%20significant%20gains%20on%20period-specific%20tests%20for%20part-of-speech%20%28POS%29%20annotations%2C%20lemmatization%2C%20and%20named%20entity%20recognition%20%28NER%29.%20Our%20results%20underscore%20the%20importance%20of%20domain-specific%20models%20and%20demonstrate%20that%20even%20relatively%20limited%20amounts%20of%20synthetic%20data%20can%20improve%20NLP%20tools%20for%20under-resourced%20corpora%20in%20computational%20humanities%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.14688v1&entry.124074799=Read"},
{"title": "LED: Light Enhanced Depth Estimation at Night", "author": "Simon de Moreau and Yasser Almehio and Andrei Bursuc and Hafid El-Idrissi and Bogdan Stanciulescu and Fabien Moutarde", "abstract": "Nighttime camera-based depth estimation is a highly challenging task, especially for autonomous driving applications, where accurate depth perception is essential for ensuring safe navigation. Models trained on daytime data often fail in the absence of precise but costly LiDAR. Even vision foundation models trained on large amounts of data are unreliable in low-light conditions. In this work, we aim to improve the reliability of perception systems at night time. To this end, we introduce Light Enhanced Depth (LED), a novel, cost-effective approach that significantly improves depth estimation in low-light environments by harnessing a pattern projected by high definition headlights available in modern vehicles. LED leads to significant performance boosts across multiple depth-estimation architectures (encoder-decoder, Adabins, DepthFormer, Depth Anything V2) both on synthetic and real datasets. Furthermore, increased performances beyond illuminated areas reveal a holistic enhancement in scene understanding. Finally, we release the Nighttime Synthetic Drive Dataset, a synthetic and photo-realistic nighttime dataset, which comprises 49,990 comprehensively annotated images.", "link": "http://arxiv.org/abs/2409.08031v3", "date": "2025-11-18", "relevancy": 2.2242, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LED%3A%20Light%20Enhanced%20Depth%20Estimation%20at%20Night&body=Title%3A%20LED%3A%20Light%20Enhanced%20Depth%20Estimation%20at%20Night%0AAuthor%3A%20Simon%20de%20Moreau%20and%20Yasser%20Almehio%20and%20Andrei%20Bursuc%20and%20Hafid%20El-Idrissi%20and%20Bogdan%20Stanciulescu%20and%20Fabien%20Moutarde%0AAbstract%3A%20Nighttime%20camera-based%20depth%20estimation%20is%20a%20highly%20challenging%20task%2C%20especially%20for%20autonomous%20driving%20applications%2C%20where%20accurate%20depth%20perception%20is%20essential%20for%20ensuring%20safe%20navigation.%20Models%20trained%20on%20daytime%20data%20often%20fail%20in%20the%20absence%20of%20precise%20but%20costly%20LiDAR.%20Even%20vision%20foundation%20models%20trained%20on%20large%20amounts%20of%20data%20are%20unreliable%20in%20low-light%20conditions.%20In%20this%20work%2C%20we%20aim%20to%20improve%20the%20reliability%20of%20perception%20systems%20at%20night%20time.%20To%20this%20end%2C%20we%20introduce%20Light%20Enhanced%20Depth%20%28LED%29%2C%20a%20novel%2C%20cost-effective%20approach%20that%20significantly%20improves%20depth%20estimation%20in%20low-light%20environments%20by%20harnessing%20a%20pattern%20projected%20by%20high%20definition%20headlights%20available%20in%20modern%20vehicles.%20LED%20leads%20to%20significant%20performance%20boosts%20across%20multiple%20depth-estimation%20architectures%20%28encoder-decoder%2C%20Adabins%2C%20DepthFormer%2C%20Depth%20Anything%20V2%29%20both%20on%20synthetic%20and%20real%20datasets.%20Furthermore%2C%20increased%20performances%20beyond%20illuminated%20areas%20reveal%20a%20holistic%20enhancement%20in%20scene%20understanding.%20Finally%2C%20we%20release%20the%20Nighttime%20Synthetic%20Drive%20Dataset%2C%20a%20synthetic%20and%20photo-realistic%20nighttime%20dataset%2C%20which%20comprises%2049%2C990%20comprehensively%20annotated%20images.%0ALink%3A%20http%3A//arxiv.org/abs/2409.08031v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLED%253A%2520Light%2520Enhanced%2520Depth%2520Estimation%2520at%2520Night%26entry.906535625%3DSimon%2520de%2520Moreau%2520and%2520Yasser%2520Almehio%2520and%2520Andrei%2520Bursuc%2520and%2520Hafid%2520El-Idrissi%2520and%2520Bogdan%2520Stanciulescu%2520and%2520Fabien%2520Moutarde%26entry.1292438233%3DNighttime%2520camera-based%2520depth%2520estimation%2520is%2520a%2520highly%2520challenging%2520task%252C%2520especially%2520for%2520autonomous%2520driving%2520applications%252C%2520where%2520accurate%2520depth%2520perception%2520is%2520essential%2520for%2520ensuring%2520safe%2520navigation.%2520Models%2520trained%2520on%2520daytime%2520data%2520often%2520fail%2520in%2520the%2520absence%2520of%2520precise%2520but%2520costly%2520LiDAR.%2520Even%2520vision%2520foundation%2520models%2520trained%2520on%2520large%2520amounts%2520of%2520data%2520are%2520unreliable%2520in%2520low-light%2520conditions.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520improve%2520the%2520reliability%2520of%2520perception%2520systems%2520at%2520night%2520time.%2520To%2520this%2520end%252C%2520we%2520introduce%2520Light%2520Enhanced%2520Depth%2520%2528LED%2529%252C%2520a%2520novel%252C%2520cost-effective%2520approach%2520that%2520significantly%2520improves%2520depth%2520estimation%2520in%2520low-light%2520environments%2520by%2520harnessing%2520a%2520pattern%2520projected%2520by%2520high%2520definition%2520headlights%2520available%2520in%2520modern%2520vehicles.%2520LED%2520leads%2520to%2520significant%2520performance%2520boosts%2520across%2520multiple%2520depth-estimation%2520architectures%2520%2528encoder-decoder%252C%2520Adabins%252C%2520DepthFormer%252C%2520Depth%2520Anything%2520V2%2529%2520both%2520on%2520synthetic%2520and%2520real%2520datasets.%2520Furthermore%252C%2520increased%2520performances%2520beyond%2520illuminated%2520areas%2520reveal%2520a%2520holistic%2520enhancement%2520in%2520scene%2520understanding.%2520Finally%252C%2520we%2520release%2520the%2520Nighttime%2520Synthetic%2520Drive%2520Dataset%252C%2520a%2520synthetic%2520and%2520photo-realistic%2520nighttime%2520dataset%252C%2520which%2520comprises%252049%252C990%2520comprehensively%2520annotated%2520images.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08031v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LED%3A%20Light%20Enhanced%20Depth%20Estimation%20at%20Night&entry.906535625=Simon%20de%20Moreau%20and%20Yasser%20Almehio%20and%20Andrei%20Bursuc%20and%20Hafid%20El-Idrissi%20and%20Bogdan%20Stanciulescu%20and%20Fabien%20Moutarde&entry.1292438233=Nighttime%20camera-based%20depth%20estimation%20is%20a%20highly%20challenging%20task%2C%20especially%20for%20autonomous%20driving%20applications%2C%20where%20accurate%20depth%20perception%20is%20essential%20for%20ensuring%20safe%20navigation.%20Models%20trained%20on%20daytime%20data%20often%20fail%20in%20the%20absence%20of%20precise%20but%20costly%20LiDAR.%20Even%20vision%20foundation%20models%20trained%20on%20large%20amounts%20of%20data%20are%20unreliable%20in%20low-light%20conditions.%20In%20this%20work%2C%20we%20aim%20to%20improve%20the%20reliability%20of%20perception%20systems%20at%20night%20time.%20To%20this%20end%2C%20we%20introduce%20Light%20Enhanced%20Depth%20%28LED%29%2C%20a%20novel%2C%20cost-effective%20approach%20that%20significantly%20improves%20depth%20estimation%20in%20low-light%20environments%20by%20harnessing%20a%20pattern%20projected%20by%20high%20definition%20headlights%20available%20in%20modern%20vehicles.%20LED%20leads%20to%20significant%20performance%20boosts%20across%20multiple%20depth-estimation%20architectures%20%28encoder-decoder%2C%20Adabins%2C%20DepthFormer%2C%20Depth%20Anything%20V2%29%20both%20on%20synthetic%20and%20real%20datasets.%20Furthermore%2C%20increased%20performances%20beyond%20illuminated%20areas%20reveal%20a%20holistic%20enhancement%20in%20scene%20understanding.%20Finally%2C%20we%20release%20the%20Nighttime%20Synthetic%20Drive%20Dataset%2C%20a%20synthetic%20and%20photo-realistic%20nighttime%20dataset%2C%20which%20comprises%2049%2C990%20comprehensively%20annotated%20images.&entry.1838667208=http%3A//arxiv.org/abs/2409.08031v3&entry.124074799=Read"},
{"title": "Watch Out for the Lifespan: Evaluating Backdoor Attacks Against Federated Model Adaptation", "author": "Bastien Vuillod and Pierre-Alain Moellic and Jean-Max Dutertre", "abstract": "Large models adaptation through Federated Learning (FL) addresses a wide range of use cases and is enabled by Parameter-Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA). However, this distributed learning paradigm faces several security threats, particularly to its integrity, such as backdoor attacks that aim to inject malicious behavior during the local training steps of certain clients. We present the first analysis of the influence of LoRA on state-of-the-art backdoor attacks targeting model adaptation in FL. Specifically, we focus on backdoor lifespan, a critical characteristic in FL, that can vary depending on the attack scenario and the attacker's ability to effectively inject the backdoor. A key finding in our experiments is that for an optimally injected backdoor, the backdoor persistence after the attack is longer when the LoRA's rank is lower. Importantly, our work highlights evaluation issues of backdoor attacks against FL and contributes to the development of more robust and fair evaluations of backdoor attacks, enhancing the reliability of risk assessments for critical FL systems. Our code is publicly available.", "link": "http://arxiv.org/abs/2511.14406v1", "date": "2025-11-18", "relevancy": 2.224, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4561}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.442}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Watch%20Out%20for%20the%20Lifespan%3A%20Evaluating%20Backdoor%20Attacks%20Against%20Federated%20Model%20Adaptation&body=Title%3A%20Watch%20Out%20for%20the%20Lifespan%3A%20Evaluating%20Backdoor%20Attacks%20Against%20Federated%20Model%20Adaptation%0AAuthor%3A%20Bastien%20Vuillod%20and%20Pierre-Alain%20Moellic%20and%20Jean-Max%20Dutertre%0AAbstract%3A%20Large%20models%20adaptation%20through%20Federated%20Learning%20%28FL%29%20addresses%20a%20wide%20range%20of%20use%20cases%20and%20is%20enabled%20by%20Parameter-Efficient%20Fine-Tuning%20techniques%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29.%20However%2C%20this%20distributed%20learning%20paradigm%20faces%20several%20security%20threats%2C%20particularly%20to%20its%20integrity%2C%20such%20as%20backdoor%20attacks%20that%20aim%20to%20inject%20malicious%20behavior%20during%20the%20local%20training%20steps%20of%20certain%20clients.%20We%20present%20the%20first%20analysis%20of%20the%20influence%20of%20LoRA%20on%20state-of-the-art%20backdoor%20attacks%20targeting%20model%20adaptation%20in%20FL.%20Specifically%2C%20we%20focus%20on%20backdoor%20lifespan%2C%20a%20critical%20characteristic%20in%20FL%2C%20that%20can%20vary%20depending%20on%20the%20attack%20scenario%20and%20the%20attacker%27s%20ability%20to%20effectively%20inject%20the%20backdoor.%20A%20key%20finding%20in%20our%20experiments%20is%20that%20for%20an%20optimally%20injected%20backdoor%2C%20the%20backdoor%20persistence%20after%20the%20attack%20is%20longer%20when%20the%20LoRA%27s%20rank%20is%20lower.%20Importantly%2C%20our%20work%20highlights%20evaluation%20issues%20of%20backdoor%20attacks%20against%20FL%20and%20contributes%20to%20the%20development%20of%20more%20robust%20and%20fair%20evaluations%20of%20backdoor%20attacks%2C%20enhancing%20the%20reliability%20of%20risk%20assessments%20for%20critical%20FL%20systems.%20Our%20code%20is%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWatch%2520Out%2520for%2520the%2520Lifespan%253A%2520Evaluating%2520Backdoor%2520Attacks%2520Against%2520Federated%2520Model%2520Adaptation%26entry.906535625%3DBastien%2520Vuillod%2520and%2520Pierre-Alain%2520Moellic%2520and%2520Jean-Max%2520Dutertre%26entry.1292438233%3DLarge%2520models%2520adaptation%2520through%2520Federated%2520Learning%2520%2528FL%2529%2520addresses%2520a%2520wide%2520range%2520of%2520use%2520cases%2520and%2520is%2520enabled%2520by%2520Parameter-Efficient%2520Fine-Tuning%2520techniques%2520such%2520as%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529.%2520However%252C%2520this%2520distributed%2520learning%2520paradigm%2520faces%2520several%2520security%2520threats%252C%2520particularly%2520to%2520its%2520integrity%252C%2520such%2520as%2520backdoor%2520attacks%2520that%2520aim%2520to%2520inject%2520malicious%2520behavior%2520during%2520the%2520local%2520training%2520steps%2520of%2520certain%2520clients.%2520We%2520present%2520the%2520first%2520analysis%2520of%2520the%2520influence%2520of%2520LoRA%2520on%2520state-of-the-art%2520backdoor%2520attacks%2520targeting%2520model%2520adaptation%2520in%2520FL.%2520Specifically%252C%2520we%2520focus%2520on%2520backdoor%2520lifespan%252C%2520a%2520critical%2520characteristic%2520in%2520FL%252C%2520that%2520can%2520vary%2520depending%2520on%2520the%2520attack%2520scenario%2520and%2520the%2520attacker%2527s%2520ability%2520to%2520effectively%2520inject%2520the%2520backdoor.%2520A%2520key%2520finding%2520in%2520our%2520experiments%2520is%2520that%2520for%2520an%2520optimally%2520injected%2520backdoor%252C%2520the%2520backdoor%2520persistence%2520after%2520the%2520attack%2520is%2520longer%2520when%2520the%2520LoRA%2527s%2520rank%2520is%2520lower.%2520Importantly%252C%2520our%2520work%2520highlights%2520evaluation%2520issues%2520of%2520backdoor%2520attacks%2520against%2520FL%2520and%2520contributes%2520to%2520the%2520development%2520of%2520more%2520robust%2520and%2520fair%2520evaluations%2520of%2520backdoor%2520attacks%252C%2520enhancing%2520the%2520reliability%2520of%2520risk%2520assessments%2520for%2520critical%2520FL%2520systems.%2520Our%2520code%2520is%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Watch%20Out%20for%20the%20Lifespan%3A%20Evaluating%20Backdoor%20Attacks%20Against%20Federated%20Model%20Adaptation&entry.906535625=Bastien%20Vuillod%20and%20Pierre-Alain%20Moellic%20and%20Jean-Max%20Dutertre&entry.1292438233=Large%20models%20adaptation%20through%20Federated%20Learning%20%28FL%29%20addresses%20a%20wide%20range%20of%20use%20cases%20and%20is%20enabled%20by%20Parameter-Efficient%20Fine-Tuning%20techniques%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29.%20However%2C%20this%20distributed%20learning%20paradigm%20faces%20several%20security%20threats%2C%20particularly%20to%20its%20integrity%2C%20such%20as%20backdoor%20attacks%20that%20aim%20to%20inject%20malicious%20behavior%20during%20the%20local%20training%20steps%20of%20certain%20clients.%20We%20present%20the%20first%20analysis%20of%20the%20influence%20of%20LoRA%20on%20state-of-the-art%20backdoor%20attacks%20targeting%20model%20adaptation%20in%20FL.%20Specifically%2C%20we%20focus%20on%20backdoor%20lifespan%2C%20a%20critical%20characteristic%20in%20FL%2C%20that%20can%20vary%20depending%20on%20the%20attack%20scenario%20and%20the%20attacker%27s%20ability%20to%20effectively%20inject%20the%20backdoor.%20A%20key%20finding%20in%20our%20experiments%20is%20that%20for%20an%20optimally%20injected%20backdoor%2C%20the%20backdoor%20persistence%20after%20the%20attack%20is%20longer%20when%20the%20LoRA%27s%20rank%20is%20lower.%20Importantly%2C%20our%20work%20highlights%20evaluation%20issues%20of%20backdoor%20attacks%20against%20FL%20and%20contributes%20to%20the%20development%20of%20more%20robust%20and%20fair%20evaluations%20of%20backdoor%20attacks%2C%20enhancing%20the%20reliability%20of%20risk%20assessments%20for%20critical%20FL%20systems.%20Our%20code%20is%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2511.14406v1&entry.124074799=Read"},
{"title": "DeCo-VAE: Learning Compact Latents for Video Reconstruction via Decoupled Representation", "author": "Xiangchen Yin and Jiahui Yuan and Zhangchi Hu and Wenzhang Sun and Jie Chen and Xiaozhen Qiao and Hao Li and Xiaoyan Sun", "abstract": "Existing video Variational Autoencoders (VAEs) generally overlook the similarity between frame contents, leading to redundant latent modeling. In this paper, we propose decoupled VAE (DeCo-VAE) to achieve compact latent representation. Instead of encoding RGB pixels directly, we decompose video content into distinct components via explicit decoupling: keyframe, motion and residual, and learn dedicated latent representation for each. To avoid cross-component interference, we design dedicated encoders for each decoupled component and adopt a shared 3D decoder to maintain spatiotemporal consistency during reconstruction. We further utilize a decoupled adaptation strategy that freezes partial encoders while training the others sequentially, ensuring stable training and accurate learning of both static and dynamic features. Extensive quantitative and qualitative experiments demonstrate that DeCo-VAE achieves superior video reconstruction performance.", "link": "http://arxiv.org/abs/2511.14530v1", "date": "2025-11-18", "relevancy": 2.2143, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.56}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5531}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeCo-VAE%3A%20Learning%20Compact%20Latents%20for%20Video%20Reconstruction%20via%20Decoupled%20Representation&body=Title%3A%20DeCo-VAE%3A%20Learning%20Compact%20Latents%20for%20Video%20Reconstruction%20via%20Decoupled%20Representation%0AAuthor%3A%20Xiangchen%20Yin%20and%20Jiahui%20Yuan%20and%20Zhangchi%20Hu%20and%20Wenzhang%20Sun%20and%20Jie%20Chen%20and%20Xiaozhen%20Qiao%20and%20Hao%20Li%20and%20Xiaoyan%20Sun%0AAbstract%3A%20Existing%20video%20Variational%20Autoencoders%20%28VAEs%29%20generally%20overlook%20the%20similarity%20between%20frame%20contents%2C%20leading%20to%20redundant%20latent%20modeling.%20In%20this%20paper%2C%20we%20propose%20decoupled%20VAE%20%28DeCo-VAE%29%20to%20achieve%20compact%20latent%20representation.%20Instead%20of%20encoding%20RGB%20pixels%20directly%2C%20we%20decompose%20video%20content%20into%20distinct%20components%20via%20explicit%20decoupling%3A%20keyframe%2C%20motion%20and%20residual%2C%20and%20learn%20dedicated%20latent%20representation%20for%20each.%20To%20avoid%20cross-component%20interference%2C%20we%20design%20dedicated%20encoders%20for%20each%20decoupled%20component%20and%20adopt%20a%20shared%203D%20decoder%20to%20maintain%20spatiotemporal%20consistency%20during%20reconstruction.%20We%20further%20utilize%20a%20decoupled%20adaptation%20strategy%20that%20freezes%20partial%20encoders%20while%20training%20the%20others%20sequentially%2C%20ensuring%20stable%20training%20and%20accurate%20learning%20of%20both%20static%20and%20dynamic%20features.%20Extensive%20quantitative%20and%20qualitative%20experiments%20demonstrate%20that%20DeCo-VAE%20achieves%20superior%20video%20reconstruction%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeCo-VAE%253A%2520Learning%2520Compact%2520Latents%2520for%2520Video%2520Reconstruction%2520via%2520Decoupled%2520Representation%26entry.906535625%3DXiangchen%2520Yin%2520and%2520Jiahui%2520Yuan%2520and%2520Zhangchi%2520Hu%2520and%2520Wenzhang%2520Sun%2520and%2520Jie%2520Chen%2520and%2520Xiaozhen%2520Qiao%2520and%2520Hao%2520Li%2520and%2520Xiaoyan%2520Sun%26entry.1292438233%3DExisting%2520video%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520generally%2520overlook%2520the%2520similarity%2520between%2520frame%2520contents%252C%2520leading%2520to%2520redundant%2520latent%2520modeling.%2520In%2520this%2520paper%252C%2520we%2520propose%2520decoupled%2520VAE%2520%2528DeCo-VAE%2529%2520to%2520achieve%2520compact%2520latent%2520representation.%2520Instead%2520of%2520encoding%2520RGB%2520pixels%2520directly%252C%2520we%2520decompose%2520video%2520content%2520into%2520distinct%2520components%2520via%2520explicit%2520decoupling%253A%2520keyframe%252C%2520motion%2520and%2520residual%252C%2520and%2520learn%2520dedicated%2520latent%2520representation%2520for%2520each.%2520To%2520avoid%2520cross-component%2520interference%252C%2520we%2520design%2520dedicated%2520encoders%2520for%2520each%2520decoupled%2520component%2520and%2520adopt%2520a%2520shared%25203D%2520decoder%2520to%2520maintain%2520spatiotemporal%2520consistency%2520during%2520reconstruction.%2520We%2520further%2520utilize%2520a%2520decoupled%2520adaptation%2520strategy%2520that%2520freezes%2520partial%2520encoders%2520while%2520training%2520the%2520others%2520sequentially%252C%2520ensuring%2520stable%2520training%2520and%2520accurate%2520learning%2520of%2520both%2520static%2520and%2520dynamic%2520features.%2520Extensive%2520quantitative%2520and%2520qualitative%2520experiments%2520demonstrate%2520that%2520DeCo-VAE%2520achieves%2520superior%2520video%2520reconstruction%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeCo-VAE%3A%20Learning%20Compact%20Latents%20for%20Video%20Reconstruction%20via%20Decoupled%20Representation&entry.906535625=Xiangchen%20Yin%20and%20Jiahui%20Yuan%20and%20Zhangchi%20Hu%20and%20Wenzhang%20Sun%20and%20Jie%20Chen%20and%20Xiaozhen%20Qiao%20and%20Hao%20Li%20and%20Xiaoyan%20Sun&entry.1292438233=Existing%20video%20Variational%20Autoencoders%20%28VAEs%29%20generally%20overlook%20the%20similarity%20between%20frame%20contents%2C%20leading%20to%20redundant%20latent%20modeling.%20In%20this%20paper%2C%20we%20propose%20decoupled%20VAE%20%28DeCo-VAE%29%20to%20achieve%20compact%20latent%20representation.%20Instead%20of%20encoding%20RGB%20pixels%20directly%2C%20we%20decompose%20video%20content%20into%20distinct%20components%20via%20explicit%20decoupling%3A%20keyframe%2C%20motion%20and%20residual%2C%20and%20learn%20dedicated%20latent%20representation%20for%20each.%20To%20avoid%20cross-component%20interference%2C%20we%20design%20dedicated%20encoders%20for%20each%20decoupled%20component%20and%20adopt%20a%20shared%203D%20decoder%20to%20maintain%20spatiotemporal%20consistency%20during%20reconstruction.%20We%20further%20utilize%20a%20decoupled%20adaptation%20strategy%20that%20freezes%20partial%20encoders%20while%20training%20the%20others%20sequentially%2C%20ensuring%20stable%20training%20and%20accurate%20learning%20of%20both%20static%20and%20dynamic%20features.%20Extensive%20quantitative%20and%20qualitative%20experiments%20demonstrate%20that%20DeCo-VAE%20achieves%20superior%20video%20reconstruction%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2511.14530v1&entry.124074799=Read"},
{"title": "Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior", "author": "Dalia Ali and Dora Zhao and Allison Koenecke and Orestis Papakyriakopoulos", "abstract": "Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collected alignment data from US and German participants (N = 1,095, 27,375 ratings) who rated LLM responses across five dimensions: Toxicity, Emotional Awareness (EA), Sensitivity, Stereotypical Bias, and Helpfulness. We fine-tuned multiple Large Language Models and Large Reasoning Models using preferences from different social groups while varying rating scales, disagreement handling methods, and optimization techniques. The results revealed systematic demographic effects: male participants rated responses 18% less toxic than female participants; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants, respectively. Models fine-tuned on group-specific preferences exhibited distinct behaviors. Technical design choices showed strong effects: the preservation of rater disagreement achieved roughly 53% greater toxicity reduction than majority voting, and 5-point scales yielded about 22% more reduction than binary formats; and Direct Preference Optimization (DPO) consistently outperformed Group Relative Policy Optimization (GRPO) in multi-value optimization. These findings represent a preliminary step in answering a critical question: How should alignment balance expert-driven and user-driven signals to ensure both safety and fair representation?", "link": "http://arxiv.org/abs/2511.14476v1", "date": "2025-11-18", "relevancy": 2.214, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4464}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4464}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Operationalizing%20Pluralistic%20Values%20in%20Large%20Language%20Model%20Alignment%20Reveals%20Trade-offs%20in%20Safety%2C%20Inclusivity%2C%20and%20Model%20Behavior&body=Title%3A%20Operationalizing%20Pluralistic%20Values%20in%20Large%20Language%20Model%20Alignment%20Reveals%20Trade-offs%20in%20Safety%2C%20Inclusivity%2C%20and%20Model%20Behavior%0AAuthor%3A%20Dalia%20Ali%20and%20Dora%20Zhao%20and%20Allison%20Koenecke%20and%20Orestis%20Papakyriakopoulos%0AAbstract%3A%20Although%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20trained%20using%20human%20feedback%20for%20safety%20and%20alignment%20with%20human%20values%2C%20alignment%20decisions%20often%20overlook%20human%20social%20diversity.%20This%20study%20examines%20how%20incorporating%20pluralistic%20values%20affects%20LLM%20behavior%20by%20systematically%20evaluating%20demographic%20variation%20and%20design%20parameters%20in%20the%20alignment%20pipeline.%20We%20collected%20alignment%20data%20from%20US%20and%20German%20participants%20%28N%20%3D%201%2C095%2C%2027%2C375%20ratings%29%20who%20rated%20LLM%20responses%20across%20five%20dimensions%3A%20Toxicity%2C%20Emotional%20Awareness%20%28EA%29%2C%20Sensitivity%2C%20Stereotypical%20Bias%2C%20and%20Helpfulness.%20We%20fine-tuned%20multiple%20Large%20Language%20Models%20and%20Large%20Reasoning%20Models%20using%20preferences%20from%20different%20social%20groups%20while%20varying%20rating%20scales%2C%20disagreement%20handling%20methods%2C%20and%20optimization%20techniques.%20The%20results%20revealed%20systematic%20demographic%20effects%3A%20male%20participants%20rated%20responses%2018%25%20less%20toxic%20than%20female%20participants%3B%20conservative%20and%20Black%20participants%20rated%20responses%2027.9%25%20and%2044%25%20more%20emotionally%20aware%20than%20liberal%20and%20White%20participants%2C%20respectively.%20Models%20fine-tuned%20on%20group-specific%20preferences%20exhibited%20distinct%20behaviors.%20Technical%20design%20choices%20showed%20strong%20effects%3A%20the%20preservation%20of%20rater%20disagreement%20achieved%20roughly%2053%25%20greater%20toxicity%20reduction%20than%20majority%20voting%2C%20and%205-point%20scales%20yielded%20about%2022%25%20more%20reduction%20than%20binary%20formats%3B%20and%20Direct%20Preference%20Optimization%20%28DPO%29%20consistently%20outperformed%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20in%20multi-value%20optimization.%20These%20findings%20represent%20a%20preliminary%20step%20in%20answering%20a%20critical%20question%3A%20How%20should%20alignment%20balance%20expert-driven%20and%20user-driven%20signals%20to%20ensure%20both%20safety%20and%20fair%20representation%3F%0ALink%3A%20http%3A//arxiv.org/abs/2511.14476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOperationalizing%2520Pluralistic%2520Values%2520in%2520Large%2520Language%2520Model%2520Alignment%2520Reveals%2520Trade-offs%2520in%2520Safety%252C%2520Inclusivity%252C%2520and%2520Model%2520Behavior%26entry.906535625%3DDalia%2520Ali%2520and%2520Dora%2520Zhao%2520and%2520Allison%2520Koenecke%2520and%2520Orestis%2520Papakyriakopoulos%26entry.1292438233%3DAlthough%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520trained%2520using%2520human%2520feedback%2520for%2520safety%2520and%2520alignment%2520with%2520human%2520values%252C%2520alignment%2520decisions%2520often%2520overlook%2520human%2520social%2520diversity.%2520This%2520study%2520examines%2520how%2520incorporating%2520pluralistic%2520values%2520affects%2520LLM%2520behavior%2520by%2520systematically%2520evaluating%2520demographic%2520variation%2520and%2520design%2520parameters%2520in%2520the%2520alignment%2520pipeline.%2520We%2520collected%2520alignment%2520data%2520from%2520US%2520and%2520German%2520participants%2520%2528N%2520%253D%25201%252C095%252C%252027%252C375%2520ratings%2529%2520who%2520rated%2520LLM%2520responses%2520across%2520five%2520dimensions%253A%2520Toxicity%252C%2520Emotional%2520Awareness%2520%2528EA%2529%252C%2520Sensitivity%252C%2520Stereotypical%2520Bias%252C%2520and%2520Helpfulness.%2520We%2520fine-tuned%2520multiple%2520Large%2520Language%2520Models%2520and%2520Large%2520Reasoning%2520Models%2520using%2520preferences%2520from%2520different%2520social%2520groups%2520while%2520varying%2520rating%2520scales%252C%2520disagreement%2520handling%2520methods%252C%2520and%2520optimization%2520techniques.%2520The%2520results%2520revealed%2520systematic%2520demographic%2520effects%253A%2520male%2520participants%2520rated%2520responses%252018%2525%2520less%2520toxic%2520than%2520female%2520participants%253B%2520conservative%2520and%2520Black%2520participants%2520rated%2520responses%252027.9%2525%2520and%252044%2525%2520more%2520emotionally%2520aware%2520than%2520liberal%2520and%2520White%2520participants%252C%2520respectively.%2520Models%2520fine-tuned%2520on%2520group-specific%2520preferences%2520exhibited%2520distinct%2520behaviors.%2520Technical%2520design%2520choices%2520showed%2520strong%2520effects%253A%2520the%2520preservation%2520of%2520rater%2520disagreement%2520achieved%2520roughly%252053%2525%2520greater%2520toxicity%2520reduction%2520than%2520majority%2520voting%252C%2520and%25205-point%2520scales%2520yielded%2520about%252022%2525%2520more%2520reduction%2520than%2520binary%2520formats%253B%2520and%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520consistently%2520outperformed%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520in%2520multi-value%2520optimization.%2520These%2520findings%2520represent%2520a%2520preliminary%2520step%2520in%2520answering%2520a%2520critical%2520question%253A%2520How%2520should%2520alignment%2520balance%2520expert-driven%2520and%2520user-driven%2520signals%2520to%2520ensure%2520both%2520safety%2520and%2520fair%2520representation%253F%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Operationalizing%20Pluralistic%20Values%20in%20Large%20Language%20Model%20Alignment%20Reveals%20Trade-offs%20in%20Safety%2C%20Inclusivity%2C%20and%20Model%20Behavior&entry.906535625=Dalia%20Ali%20and%20Dora%20Zhao%20and%20Allison%20Koenecke%20and%20Orestis%20Papakyriakopoulos&entry.1292438233=Although%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20trained%20using%20human%20feedback%20for%20safety%20and%20alignment%20with%20human%20values%2C%20alignment%20decisions%20often%20overlook%20human%20social%20diversity.%20This%20study%20examines%20how%20incorporating%20pluralistic%20values%20affects%20LLM%20behavior%20by%20systematically%20evaluating%20demographic%20variation%20and%20design%20parameters%20in%20the%20alignment%20pipeline.%20We%20collected%20alignment%20data%20from%20US%20and%20German%20participants%20%28N%20%3D%201%2C095%2C%2027%2C375%20ratings%29%20who%20rated%20LLM%20responses%20across%20five%20dimensions%3A%20Toxicity%2C%20Emotional%20Awareness%20%28EA%29%2C%20Sensitivity%2C%20Stereotypical%20Bias%2C%20and%20Helpfulness.%20We%20fine-tuned%20multiple%20Large%20Language%20Models%20and%20Large%20Reasoning%20Models%20using%20preferences%20from%20different%20social%20groups%20while%20varying%20rating%20scales%2C%20disagreement%20handling%20methods%2C%20and%20optimization%20techniques.%20The%20results%20revealed%20systematic%20demographic%20effects%3A%20male%20participants%20rated%20responses%2018%25%20less%20toxic%20than%20female%20participants%3B%20conservative%20and%20Black%20participants%20rated%20responses%2027.9%25%20and%2044%25%20more%20emotionally%20aware%20than%20liberal%20and%20White%20participants%2C%20respectively.%20Models%20fine-tuned%20on%20group-specific%20preferences%20exhibited%20distinct%20behaviors.%20Technical%20design%20choices%20showed%20strong%20effects%3A%20the%20preservation%20of%20rater%20disagreement%20achieved%20roughly%2053%25%20greater%20toxicity%20reduction%20than%20majority%20voting%2C%20and%205-point%20scales%20yielded%20about%2022%25%20more%20reduction%20than%20binary%20formats%3B%20and%20Direct%20Preference%20Optimization%20%28DPO%29%20consistently%20outperformed%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20in%20multi-value%20optimization.%20These%20findings%20represent%20a%20preliminary%20step%20in%20answering%20a%20critical%20question%3A%20How%20should%20alignment%20balance%20expert-driven%20and%20user-driven%20signals%20to%20ensure%20both%20safety%20and%20fair%20representation%3F&entry.1838667208=http%3A//arxiv.org/abs/2511.14476v1&entry.124074799=Read"},
{"title": "Parameter Aware Mamba Model for Multi-task Dense Prediction", "author": "Xinzhuo Yu and Yunzhi Zhuge and Sitong Gong and Lu Zhang and Pingping Zhang and Huchuan Lu", "abstract": "Understanding the inter-relations and interactions between tasks is crucial for multi-task dense prediction. Existing methods predominantly utilize convolutional layers and attention mechanisms to explore task-level interactions. In this work, we introduce a novel decoder-based framework, Parameter Aware Mamba Model (PAMM), specifically designed for dense prediction in multi-task learning setting. Distinct from approaches that employ Transformers to model holistic task relationships, PAMM leverages the rich, scalable parameters of state space models to enhance task interconnectivity. It features dual state space parameter experts that integrate and set task-specific parameter priors, capturing the intrinsic properties of each task. This approach not only facilitates precise multi-task interactions but also allows for the global integration of task priors through the structured state space sequence model (S4). Furthermore, we employ the Multi-Directional Hilbert Scanning method to construct multi-angle feature sequences, thereby enhancing the sequence model's perceptual capabilities for 2D data. Extensive experiments on the NYUD-v2 and PASCAL-Context benchmarks demonstrate the effectiveness of our proposed method. Our code is available at https://github.com/CQC-gogopro/PAMM.", "link": "http://arxiv.org/abs/2511.14503v1", "date": "2025-11-18", "relevancy": 2.2072, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5826}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5796}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter%20Aware%20Mamba%20Model%20for%20Multi-task%20Dense%20Prediction&body=Title%3A%20Parameter%20Aware%20Mamba%20Model%20for%20Multi-task%20Dense%20Prediction%0AAuthor%3A%20Xinzhuo%20Yu%20and%20Yunzhi%20Zhuge%20and%20Sitong%20Gong%20and%20Lu%20Zhang%20and%20Pingping%20Zhang%20and%20Huchuan%20Lu%0AAbstract%3A%20Understanding%20the%20inter-relations%20and%20interactions%20between%20tasks%20is%20crucial%20for%20multi-task%20dense%20prediction.%20Existing%20methods%20predominantly%20utilize%20convolutional%20layers%20and%20attention%20mechanisms%20to%20explore%20task-level%20interactions.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20decoder-based%20framework%2C%20Parameter%20Aware%20Mamba%20Model%20%28PAMM%29%2C%20specifically%20designed%20for%20dense%20prediction%20in%20multi-task%20learning%20setting.%20Distinct%20from%20approaches%20that%20employ%20Transformers%20to%20model%20holistic%20task%20relationships%2C%20PAMM%20leverages%20the%20rich%2C%20scalable%20parameters%20of%20state%20space%20models%20to%20enhance%20task%20interconnectivity.%20It%20features%20dual%20state%20space%20parameter%20experts%20that%20integrate%20and%20set%20task-specific%20parameter%20priors%2C%20capturing%20the%20intrinsic%20properties%20of%20each%20task.%20This%20approach%20not%20only%20facilitates%20precise%20multi-task%20interactions%20but%20also%20allows%20for%20the%20global%20integration%20of%20task%20priors%20through%20the%20structured%20state%20space%20sequence%20model%20%28S4%29.%20Furthermore%2C%20we%20employ%20the%20Multi-Directional%20Hilbert%20Scanning%20method%20to%20construct%20multi-angle%20feature%20sequences%2C%20thereby%20enhancing%20the%20sequence%20model%27s%20perceptual%20capabilities%20for%202D%20data.%20Extensive%20experiments%20on%20the%20NYUD-v2%20and%20PASCAL-Context%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method.%20Our%20code%20is%20available%20at%20https%3A//github.com/CQC-gogopro/PAMM.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter%2520Aware%2520Mamba%2520Model%2520for%2520Multi-task%2520Dense%2520Prediction%26entry.906535625%3DXinzhuo%2520Yu%2520and%2520Yunzhi%2520Zhuge%2520and%2520Sitong%2520Gong%2520and%2520Lu%2520Zhang%2520and%2520Pingping%2520Zhang%2520and%2520Huchuan%2520Lu%26entry.1292438233%3DUnderstanding%2520the%2520inter-relations%2520and%2520interactions%2520between%2520tasks%2520is%2520crucial%2520for%2520multi-task%2520dense%2520prediction.%2520Existing%2520methods%2520predominantly%2520utilize%2520convolutional%2520layers%2520and%2520attention%2520mechanisms%2520to%2520explore%2520task-level%2520interactions.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520decoder-based%2520framework%252C%2520Parameter%2520Aware%2520Mamba%2520Model%2520%2528PAMM%2529%252C%2520specifically%2520designed%2520for%2520dense%2520prediction%2520in%2520multi-task%2520learning%2520setting.%2520Distinct%2520from%2520approaches%2520that%2520employ%2520Transformers%2520to%2520model%2520holistic%2520task%2520relationships%252C%2520PAMM%2520leverages%2520the%2520rich%252C%2520scalable%2520parameters%2520of%2520state%2520space%2520models%2520to%2520enhance%2520task%2520interconnectivity.%2520It%2520features%2520dual%2520state%2520space%2520parameter%2520experts%2520that%2520integrate%2520and%2520set%2520task-specific%2520parameter%2520priors%252C%2520capturing%2520the%2520intrinsic%2520properties%2520of%2520each%2520task.%2520This%2520approach%2520not%2520only%2520facilitates%2520precise%2520multi-task%2520interactions%2520but%2520also%2520allows%2520for%2520the%2520global%2520integration%2520of%2520task%2520priors%2520through%2520the%2520structured%2520state%2520space%2520sequence%2520model%2520%2528S4%2529.%2520Furthermore%252C%2520we%2520employ%2520the%2520Multi-Directional%2520Hilbert%2520Scanning%2520method%2520to%2520construct%2520multi-angle%2520feature%2520sequences%252C%2520thereby%2520enhancing%2520the%2520sequence%2520model%2527s%2520perceptual%2520capabilities%2520for%25202D%2520data.%2520Extensive%2520experiments%2520on%2520the%2520NYUD-v2%2520and%2520PASCAL-Context%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/CQC-gogopro/PAMM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter%20Aware%20Mamba%20Model%20for%20Multi-task%20Dense%20Prediction&entry.906535625=Xinzhuo%20Yu%20and%20Yunzhi%20Zhuge%20and%20Sitong%20Gong%20and%20Lu%20Zhang%20and%20Pingping%20Zhang%20and%20Huchuan%20Lu&entry.1292438233=Understanding%20the%20inter-relations%20and%20interactions%20between%20tasks%20is%20crucial%20for%20multi-task%20dense%20prediction.%20Existing%20methods%20predominantly%20utilize%20convolutional%20layers%20and%20attention%20mechanisms%20to%20explore%20task-level%20interactions.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20decoder-based%20framework%2C%20Parameter%20Aware%20Mamba%20Model%20%28PAMM%29%2C%20specifically%20designed%20for%20dense%20prediction%20in%20multi-task%20learning%20setting.%20Distinct%20from%20approaches%20that%20employ%20Transformers%20to%20model%20holistic%20task%20relationships%2C%20PAMM%20leverages%20the%20rich%2C%20scalable%20parameters%20of%20state%20space%20models%20to%20enhance%20task%20interconnectivity.%20It%20features%20dual%20state%20space%20parameter%20experts%20that%20integrate%20and%20set%20task-specific%20parameter%20priors%2C%20capturing%20the%20intrinsic%20properties%20of%20each%20task.%20This%20approach%20not%20only%20facilitates%20precise%20multi-task%20interactions%20but%20also%20allows%20for%20the%20global%20integration%20of%20task%20priors%20through%20the%20structured%20state%20space%20sequence%20model%20%28S4%29.%20Furthermore%2C%20we%20employ%20the%20Multi-Directional%20Hilbert%20Scanning%20method%20to%20construct%20multi-angle%20feature%20sequences%2C%20thereby%20enhancing%20the%20sequence%20model%27s%20perceptual%20capabilities%20for%202D%20data.%20Extensive%20experiments%20on%20the%20NYUD-v2%20and%20PASCAL-Context%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20method.%20Our%20code%20is%20available%20at%20https%3A//github.com/CQC-gogopro/PAMM.&entry.1838667208=http%3A//arxiv.org/abs/2511.14503v1&entry.124074799=Read"},
{"title": "CCSD: Cross-Modal Compositional Self-Distillation for Robust Brain Tumor Segmentation with Missing Modalities", "author": "Dongqing Xie and Yonghuang Wu and Zisheng Ai and Jun Min and Zhencun Jiang and Shaojin Geng and Lei Wang", "abstract": "The accurate segmentation of brain tumors from multi-modal MRI is critical for clinical diagnosis and treatment planning. While integrating complementary information from various MRI sequences is a common practice, the frequent absence of one or more modalities in real-world clinical settings poses a significant challenge, severely compromising the performance and generalizability of deep learning-based segmentation models. To address this challenge, we propose a novel Cross-Modal Compositional Self-Distillation (CCSD) framework that can flexibly handle arbitrary combinations of input modalities. CCSD adopts a shared-specific encoder-decoder architecture and incorporates two self-distillation strategies: (i) a hierarchical modality self-distillation mechanism that transfers knowledge across modality hierarchies to reduce semantic discrepancies, and (ii) a progressive modality combination distillation approach that enhances robustness to missing modalities by simulating gradual modality dropout during training. Extensive experiments on public brain tumor segmentation benchmarks demonstrate that CCSD achieves state-of-the-art performance across various missing-modality scenarios, with strong generalization and stability.", "link": "http://arxiv.org/abs/2511.14599v1", "date": "2025-11-18", "relevancy": 2.2046, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5837}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5303}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CCSD%3A%20Cross-Modal%20Compositional%20Self-Distillation%20for%20Robust%20Brain%20Tumor%20Segmentation%20with%20Missing%20Modalities&body=Title%3A%20CCSD%3A%20Cross-Modal%20Compositional%20Self-Distillation%20for%20Robust%20Brain%20Tumor%20Segmentation%20with%20Missing%20Modalities%0AAuthor%3A%20Dongqing%20Xie%20and%20Yonghuang%20Wu%20and%20Zisheng%20Ai%20and%20Jun%20Min%20and%20Zhencun%20Jiang%20and%20Shaojin%20Geng%20and%20Lei%20Wang%0AAbstract%3A%20The%20accurate%20segmentation%20of%20brain%20tumors%20from%20multi-modal%20MRI%20is%20critical%20for%20clinical%20diagnosis%20and%20treatment%20planning.%20While%20integrating%20complementary%20information%20from%20various%20MRI%20sequences%20is%20a%20common%20practice%2C%20the%20frequent%20absence%20of%20one%20or%20more%20modalities%20in%20real-world%20clinical%20settings%20poses%20a%20significant%20challenge%2C%20severely%20compromising%20the%20performance%20and%20generalizability%20of%20deep%20learning-based%20segmentation%20models.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20Cross-Modal%20Compositional%20Self-Distillation%20%28CCSD%29%20framework%20that%20can%20flexibly%20handle%20arbitrary%20combinations%20of%20input%20modalities.%20CCSD%20adopts%20a%20shared-specific%20encoder-decoder%20architecture%20and%20incorporates%20two%20self-distillation%20strategies%3A%20%28i%29%20a%20hierarchical%20modality%20self-distillation%20mechanism%20that%20transfers%20knowledge%20across%20modality%20hierarchies%20to%20reduce%20semantic%20discrepancies%2C%20and%20%28ii%29%20a%20progressive%20modality%20combination%20distillation%20approach%20that%20enhances%20robustness%20to%20missing%20modalities%20by%20simulating%20gradual%20modality%20dropout%20during%20training.%20Extensive%20experiments%20on%20public%20brain%20tumor%20segmentation%20benchmarks%20demonstrate%20that%20CCSD%20achieves%20state-of-the-art%20performance%20across%20various%20missing-modality%20scenarios%2C%20with%20strong%20generalization%20and%20stability.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCCSD%253A%2520Cross-Modal%2520Compositional%2520Self-Distillation%2520for%2520Robust%2520Brain%2520Tumor%2520Segmentation%2520with%2520Missing%2520Modalities%26entry.906535625%3DDongqing%2520Xie%2520and%2520Yonghuang%2520Wu%2520and%2520Zisheng%2520Ai%2520and%2520Jun%2520Min%2520and%2520Zhencun%2520Jiang%2520and%2520Shaojin%2520Geng%2520and%2520Lei%2520Wang%26entry.1292438233%3DThe%2520accurate%2520segmentation%2520of%2520brain%2520tumors%2520from%2520multi-modal%2520MRI%2520is%2520critical%2520for%2520clinical%2520diagnosis%2520and%2520treatment%2520planning.%2520While%2520integrating%2520complementary%2520information%2520from%2520various%2520MRI%2520sequences%2520is%2520a%2520common%2520practice%252C%2520the%2520frequent%2520absence%2520of%2520one%2520or%2520more%2520modalities%2520in%2520real-world%2520clinical%2520settings%2520poses%2520a%2520significant%2520challenge%252C%2520severely%2520compromising%2520the%2520performance%2520and%2520generalizability%2520of%2520deep%2520learning-based%2520segmentation%2520models.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520Cross-Modal%2520Compositional%2520Self-Distillation%2520%2528CCSD%2529%2520framework%2520that%2520can%2520flexibly%2520handle%2520arbitrary%2520combinations%2520of%2520input%2520modalities.%2520CCSD%2520adopts%2520a%2520shared-specific%2520encoder-decoder%2520architecture%2520and%2520incorporates%2520two%2520self-distillation%2520strategies%253A%2520%2528i%2529%2520a%2520hierarchical%2520modality%2520self-distillation%2520mechanism%2520that%2520transfers%2520knowledge%2520across%2520modality%2520hierarchies%2520to%2520reduce%2520semantic%2520discrepancies%252C%2520and%2520%2528ii%2529%2520a%2520progressive%2520modality%2520combination%2520distillation%2520approach%2520that%2520enhances%2520robustness%2520to%2520missing%2520modalities%2520by%2520simulating%2520gradual%2520modality%2520dropout%2520during%2520training.%2520Extensive%2520experiments%2520on%2520public%2520brain%2520tumor%2520segmentation%2520benchmarks%2520demonstrate%2520that%2520CCSD%2520achieves%2520state-of-the-art%2520performance%2520across%2520various%2520missing-modality%2520scenarios%252C%2520with%2520strong%2520generalization%2520and%2520stability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CCSD%3A%20Cross-Modal%20Compositional%20Self-Distillation%20for%20Robust%20Brain%20Tumor%20Segmentation%20with%20Missing%20Modalities&entry.906535625=Dongqing%20Xie%20and%20Yonghuang%20Wu%20and%20Zisheng%20Ai%20and%20Jun%20Min%20and%20Zhencun%20Jiang%20and%20Shaojin%20Geng%20and%20Lei%20Wang&entry.1292438233=The%20accurate%20segmentation%20of%20brain%20tumors%20from%20multi-modal%20MRI%20is%20critical%20for%20clinical%20diagnosis%20and%20treatment%20planning.%20While%20integrating%20complementary%20information%20from%20various%20MRI%20sequences%20is%20a%20common%20practice%2C%20the%20frequent%20absence%20of%20one%20or%20more%20modalities%20in%20real-world%20clinical%20settings%20poses%20a%20significant%20challenge%2C%20severely%20compromising%20the%20performance%20and%20generalizability%20of%20deep%20learning-based%20segmentation%20models.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20Cross-Modal%20Compositional%20Self-Distillation%20%28CCSD%29%20framework%20that%20can%20flexibly%20handle%20arbitrary%20combinations%20of%20input%20modalities.%20CCSD%20adopts%20a%20shared-specific%20encoder-decoder%20architecture%20and%20incorporates%20two%20self-distillation%20strategies%3A%20%28i%29%20a%20hierarchical%20modality%20self-distillation%20mechanism%20that%20transfers%20knowledge%20across%20modality%20hierarchies%20to%20reduce%20semantic%20discrepancies%2C%20and%20%28ii%29%20a%20progressive%20modality%20combination%20distillation%20approach%20that%20enhances%20robustness%20to%20missing%20modalities%20by%20simulating%20gradual%20modality%20dropout%20during%20training.%20Extensive%20experiments%20on%20public%20brain%20tumor%20segmentation%20benchmarks%20demonstrate%20that%20CCSD%20achieves%20state-of-the-art%20performance%20across%20various%20missing-modality%20scenarios%2C%20with%20strong%20generalization%20and%20stability.&entry.1838667208=http%3A//arxiv.org/abs/2511.14599v1&entry.124074799=Read"},
{"title": "Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis", "author": "Albert Lin and Alessandro Pinto and Somil Bansal", "abstract": "As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote.", "link": "http://arxiv.org/abs/2511.14755v1", "date": "2025-11-18", "relevancy": 2.1966, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5777}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5435}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Verification%20of%20Controllers%20under%20State%20Uncertainty%20via%20Hamilton-Jacobi%20Reachability%20Analysis&body=Title%3A%20Robust%20Verification%20of%20Controllers%20under%20State%20Uncertainty%20via%20Hamilton-Jacobi%20Reachability%20Analysis%0AAuthor%3A%20Albert%20Lin%20and%20Alessandro%20Pinto%20and%20Somil%20Bansal%0AAbstract%3A%20As%20perception-based%20controllers%20for%20autonomous%20systems%20become%20increasingly%20popular%20in%20the%20real%20world%2C%20it%20is%20important%20that%20we%20can%20formally%20verify%20their%20safety%20and%20performance%20despite%20perceptual%20uncertainty.%20Unfortunately%2C%20the%20verification%20of%20such%20systems%20remains%20challenging%2C%20largely%20due%20to%20the%20complexity%20of%20the%20controllers%2C%20which%20are%20often%20nonlinear%2C%20nonconvex%2C%20learning-based%2C%20and/or%20black-box.%20Prior%20works%20propose%20verification%20algorithms%20that%20are%20based%20on%20approximate%20reachability%20methods%2C%20but%20they%20often%20restrict%20the%20class%20of%20controllers%20and%20systems%20that%20can%20be%20handled%20or%20result%20in%20overly%20conservative%20analyses.%20Hamilton-Jacobi%20%28HJ%29%20reachability%20analysis%20is%20a%20popular%20formal%20verification%20tool%20for%20general%20nonlinear%20systems%20that%20can%20compute%20optimal%20reachable%20sets%20under%20worst-case%20system%20uncertainties%3B%20however%2C%20its%20application%20to%20perception-based%20systems%20is%20currently%20underexplored.%20In%20this%20work%2C%20we%20propose%20RoVer-CoRe%2C%20a%20framework%20for%20the%20Robust%20Verification%20of%20Controllers%20via%20HJ%20Reachability.%20To%20the%20best%20of%20our%20knowledge%2C%20RoVer-CoRe%20is%20the%20first%20HJ%20reachability-based%20framework%20for%20the%20verification%20of%20perception-based%20systems%20under%20perceptual%20uncertainty.%20Our%20key%20insight%20is%20to%20concatenate%20the%20system%20controller%2C%20observation%20function%2C%20and%20the%20state%20estimation%20modules%20to%20obtain%20an%20equivalent%20closed-loop%20system%20that%20is%20readily%20compatible%20with%20existing%20reachability%20frameworks.%20Within%20RoVer-CoRe%2C%20we%20propose%20novel%20methods%20for%20formal%20safety%20verification%20and%20robust%20controller%20design.%20We%20demonstrate%20the%20efficacy%20of%20the%20framework%20in%20case%20studies%20involving%20aircraft%20taxiing%20and%20NN-based%20rover%20navigation.%20Code%20is%20available%20at%20the%20link%20in%20the%20footnote.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Verification%2520of%2520Controllers%2520under%2520State%2520Uncertainty%2520via%2520Hamilton-Jacobi%2520Reachability%2520Analysis%26entry.906535625%3DAlbert%2520Lin%2520and%2520Alessandro%2520Pinto%2520and%2520Somil%2520Bansal%26entry.1292438233%3DAs%2520perception-based%2520controllers%2520for%2520autonomous%2520systems%2520become%2520increasingly%2520popular%2520in%2520the%2520real%2520world%252C%2520it%2520is%2520important%2520that%2520we%2520can%2520formally%2520verify%2520their%2520safety%2520and%2520performance%2520despite%2520perceptual%2520uncertainty.%2520Unfortunately%252C%2520the%2520verification%2520of%2520such%2520systems%2520remains%2520challenging%252C%2520largely%2520due%2520to%2520the%2520complexity%2520of%2520the%2520controllers%252C%2520which%2520are%2520often%2520nonlinear%252C%2520nonconvex%252C%2520learning-based%252C%2520and/or%2520black-box.%2520Prior%2520works%2520propose%2520verification%2520algorithms%2520that%2520are%2520based%2520on%2520approximate%2520reachability%2520methods%252C%2520but%2520they%2520often%2520restrict%2520the%2520class%2520of%2520controllers%2520and%2520systems%2520that%2520can%2520be%2520handled%2520or%2520result%2520in%2520overly%2520conservative%2520analyses.%2520Hamilton-Jacobi%2520%2528HJ%2529%2520reachability%2520analysis%2520is%2520a%2520popular%2520formal%2520verification%2520tool%2520for%2520general%2520nonlinear%2520systems%2520that%2520can%2520compute%2520optimal%2520reachable%2520sets%2520under%2520worst-case%2520system%2520uncertainties%253B%2520however%252C%2520its%2520application%2520to%2520perception-based%2520systems%2520is%2520currently%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520propose%2520RoVer-CoRe%252C%2520a%2520framework%2520for%2520the%2520Robust%2520Verification%2520of%2520Controllers%2520via%2520HJ%2520Reachability.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520RoVer-CoRe%2520is%2520the%2520first%2520HJ%2520reachability-based%2520framework%2520for%2520the%2520verification%2520of%2520perception-based%2520systems%2520under%2520perceptual%2520uncertainty.%2520Our%2520key%2520insight%2520is%2520to%2520concatenate%2520the%2520system%2520controller%252C%2520observation%2520function%252C%2520and%2520the%2520state%2520estimation%2520modules%2520to%2520obtain%2520an%2520equivalent%2520closed-loop%2520system%2520that%2520is%2520readily%2520compatible%2520with%2520existing%2520reachability%2520frameworks.%2520Within%2520RoVer-CoRe%252C%2520we%2520propose%2520novel%2520methods%2520for%2520formal%2520safety%2520verification%2520and%2520robust%2520controller%2520design.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520the%2520framework%2520in%2520case%2520studies%2520involving%2520aircraft%2520taxiing%2520and%2520NN-based%2520rover%2520navigation.%2520Code%2520is%2520available%2520at%2520the%2520link%2520in%2520the%2520footnote.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Verification%20of%20Controllers%20under%20State%20Uncertainty%20via%20Hamilton-Jacobi%20Reachability%20Analysis&entry.906535625=Albert%20Lin%20and%20Alessandro%20Pinto%20and%20Somil%20Bansal&entry.1292438233=As%20perception-based%20controllers%20for%20autonomous%20systems%20become%20increasingly%20popular%20in%20the%20real%20world%2C%20it%20is%20important%20that%20we%20can%20formally%20verify%20their%20safety%20and%20performance%20despite%20perceptual%20uncertainty.%20Unfortunately%2C%20the%20verification%20of%20such%20systems%20remains%20challenging%2C%20largely%20due%20to%20the%20complexity%20of%20the%20controllers%2C%20which%20are%20often%20nonlinear%2C%20nonconvex%2C%20learning-based%2C%20and/or%20black-box.%20Prior%20works%20propose%20verification%20algorithms%20that%20are%20based%20on%20approximate%20reachability%20methods%2C%20but%20they%20often%20restrict%20the%20class%20of%20controllers%20and%20systems%20that%20can%20be%20handled%20or%20result%20in%20overly%20conservative%20analyses.%20Hamilton-Jacobi%20%28HJ%29%20reachability%20analysis%20is%20a%20popular%20formal%20verification%20tool%20for%20general%20nonlinear%20systems%20that%20can%20compute%20optimal%20reachable%20sets%20under%20worst-case%20system%20uncertainties%3B%20however%2C%20its%20application%20to%20perception-based%20systems%20is%20currently%20underexplored.%20In%20this%20work%2C%20we%20propose%20RoVer-CoRe%2C%20a%20framework%20for%20the%20Robust%20Verification%20of%20Controllers%20via%20HJ%20Reachability.%20To%20the%20best%20of%20our%20knowledge%2C%20RoVer-CoRe%20is%20the%20first%20HJ%20reachability-based%20framework%20for%20the%20verification%20of%20perception-based%20systems%20under%20perceptual%20uncertainty.%20Our%20key%20insight%20is%20to%20concatenate%20the%20system%20controller%2C%20observation%20function%2C%20and%20the%20state%20estimation%20modules%20to%20obtain%20an%20equivalent%20closed-loop%20system%20that%20is%20readily%20compatible%20with%20existing%20reachability%20frameworks.%20Within%20RoVer-CoRe%2C%20we%20propose%20novel%20methods%20for%20formal%20safety%20verification%20and%20robust%20controller%20design.%20We%20demonstrate%20the%20efficacy%20of%20the%20framework%20in%20case%20studies%20involving%20aircraft%20taxiing%20and%20NN-based%20rover%20navigation.%20Code%20is%20available%20at%20the%20link%20in%20the%20footnote.&entry.1838667208=http%3A//arxiv.org/abs/2511.14755v1&entry.124074799=Read"},
{"title": "Deep Equilibrium models for Poisson Imaging Inverse problems via Mirror Descent", "author": "Christian Daniele and Silvia Villa and Samuel Vaiter and Luca Calatroni", "abstract": "Deep Equilibrium Models (DEQs) are implicit neural networks with fixed points, which have recently gained attention for learning image regularization functionals, particularly in settings involving Gaussian fidelities, where assumptions on the forward operator ensure contractiveness of standard (proximal) Gradient Descent operators. In this work, we extend the application of DEQs to Poisson inverse problems, where the data fidelity term is more appropriately modeled by the Kullback--Leibler divergence. To this end, we introduce a novel DEQ formulation based on Mirror Descent defined in terms of a tailored non-Euclidean geometry that naturally adapts with the structure of the data term. This enables the learning of neural regularizers within a principled training framework. We derive sufficient conditions and establish refined convergence results based on the Kurdyka--Lojasiewicz framework for subanalytic functions with non-closed domains to guarantee the convergence of the learned reconstruction scheme and propose computational strategies that enable both efficient training and parameter-free inference. Numerical experiments show that our method outperforms traditional model-based approaches and it is comparable to the performance of Bregman Plug-and-Play methods, while mitigating their typical drawbacks, such as time-consuming tuning of hyper-parameters. The code is publicly available at https://github.com/christiandaniele/DEQ-MD.", "link": "http://arxiv.org/abs/2507.11461v2", "date": "2025-11-18", "relevancy": 2.1956, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5802}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5276}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Equilibrium%20models%20for%20Poisson%20Imaging%20Inverse%20problems%20via%20Mirror%20Descent&body=Title%3A%20Deep%20Equilibrium%20models%20for%20Poisson%20Imaging%20Inverse%20problems%20via%20Mirror%20Descent%0AAuthor%3A%20Christian%20Daniele%20and%20Silvia%20Villa%20and%20Samuel%20Vaiter%20and%20Luca%20Calatroni%0AAbstract%3A%20Deep%20Equilibrium%20Models%20%28DEQs%29%20are%20implicit%20neural%20networks%20with%20fixed%20points%2C%20which%20have%20recently%20gained%20attention%20for%20learning%20image%20regularization%20functionals%2C%20particularly%20in%20settings%20involving%20Gaussian%20fidelities%2C%20where%20assumptions%20on%20the%20forward%20operator%20ensure%20contractiveness%20of%20standard%20%28proximal%29%20Gradient%20Descent%20operators.%20In%20this%20work%2C%20we%20extend%20the%20application%20of%20DEQs%20to%20Poisson%20inverse%20problems%2C%20where%20the%20data%20fidelity%20term%20is%20more%20appropriately%20modeled%20by%20the%20Kullback--Leibler%20divergence.%20To%20this%20end%2C%20we%20introduce%20a%20novel%20DEQ%20formulation%20based%20on%20Mirror%20Descent%20defined%20in%20terms%20of%20a%20tailored%20non-Euclidean%20geometry%20that%20naturally%20adapts%20with%20the%20structure%20of%20the%20data%20term.%20This%20enables%20the%20learning%20of%20neural%20regularizers%20within%20a%20principled%20training%20framework.%20We%20derive%20sufficient%20conditions%20and%20establish%20refined%20convergence%20results%20based%20on%20the%20Kurdyka--Lojasiewicz%20framework%20for%20subanalytic%20functions%20with%20non-closed%20domains%20to%20guarantee%20the%20convergence%20of%20the%20learned%20reconstruction%20scheme%20and%20propose%20computational%20strategies%20that%20enable%20both%20efficient%20training%20and%20parameter-free%20inference.%20Numerical%20experiments%20show%20that%20our%20method%20outperforms%20traditional%20model-based%20approaches%20and%20it%20is%20comparable%20to%20the%20performance%20of%20Bregman%20Plug-and-Play%20methods%2C%20while%20mitigating%20their%20typical%20drawbacks%2C%20such%20as%20time-consuming%20tuning%20of%20hyper-parameters.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/christiandaniele/DEQ-MD.%0ALink%3A%20http%3A//arxiv.org/abs/2507.11461v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Equilibrium%2520models%2520for%2520Poisson%2520Imaging%2520Inverse%2520problems%2520via%2520Mirror%2520Descent%26entry.906535625%3DChristian%2520Daniele%2520and%2520Silvia%2520Villa%2520and%2520Samuel%2520Vaiter%2520and%2520Luca%2520Calatroni%26entry.1292438233%3DDeep%2520Equilibrium%2520Models%2520%2528DEQs%2529%2520are%2520implicit%2520neural%2520networks%2520with%2520fixed%2520points%252C%2520which%2520have%2520recently%2520gained%2520attention%2520for%2520learning%2520image%2520regularization%2520functionals%252C%2520particularly%2520in%2520settings%2520involving%2520Gaussian%2520fidelities%252C%2520where%2520assumptions%2520on%2520the%2520forward%2520operator%2520ensure%2520contractiveness%2520of%2520standard%2520%2528proximal%2529%2520Gradient%2520Descent%2520operators.%2520In%2520this%2520work%252C%2520we%2520extend%2520the%2520application%2520of%2520DEQs%2520to%2520Poisson%2520inverse%2520problems%252C%2520where%2520the%2520data%2520fidelity%2520term%2520is%2520more%2520appropriately%2520modeled%2520by%2520the%2520Kullback--Leibler%2520divergence.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520novel%2520DEQ%2520formulation%2520based%2520on%2520Mirror%2520Descent%2520defined%2520in%2520terms%2520of%2520a%2520tailored%2520non-Euclidean%2520geometry%2520that%2520naturally%2520adapts%2520with%2520the%2520structure%2520of%2520the%2520data%2520term.%2520This%2520enables%2520the%2520learning%2520of%2520neural%2520regularizers%2520within%2520a%2520principled%2520training%2520framework.%2520We%2520derive%2520sufficient%2520conditions%2520and%2520establish%2520refined%2520convergence%2520results%2520based%2520on%2520the%2520Kurdyka--Lojasiewicz%2520framework%2520for%2520subanalytic%2520functions%2520with%2520non-closed%2520domains%2520to%2520guarantee%2520the%2520convergence%2520of%2520the%2520learned%2520reconstruction%2520scheme%2520and%2520propose%2520computational%2520strategies%2520that%2520enable%2520both%2520efficient%2520training%2520and%2520parameter-free%2520inference.%2520Numerical%2520experiments%2520show%2520that%2520our%2520method%2520outperforms%2520traditional%2520model-based%2520approaches%2520and%2520it%2520is%2520comparable%2520to%2520the%2520performance%2520of%2520Bregman%2520Plug-and-Play%2520methods%252C%2520while%2520mitigating%2520their%2520typical%2520drawbacks%252C%2520such%2520as%2520time-consuming%2520tuning%2520of%2520hyper-parameters.%2520The%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/christiandaniele/DEQ-MD.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11461v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Equilibrium%20models%20for%20Poisson%20Imaging%20Inverse%20problems%20via%20Mirror%20Descent&entry.906535625=Christian%20Daniele%20and%20Silvia%20Villa%20and%20Samuel%20Vaiter%20and%20Luca%20Calatroni&entry.1292438233=Deep%20Equilibrium%20Models%20%28DEQs%29%20are%20implicit%20neural%20networks%20with%20fixed%20points%2C%20which%20have%20recently%20gained%20attention%20for%20learning%20image%20regularization%20functionals%2C%20particularly%20in%20settings%20involving%20Gaussian%20fidelities%2C%20where%20assumptions%20on%20the%20forward%20operator%20ensure%20contractiveness%20of%20standard%20%28proximal%29%20Gradient%20Descent%20operators.%20In%20this%20work%2C%20we%20extend%20the%20application%20of%20DEQs%20to%20Poisson%20inverse%20problems%2C%20where%20the%20data%20fidelity%20term%20is%20more%20appropriately%20modeled%20by%20the%20Kullback--Leibler%20divergence.%20To%20this%20end%2C%20we%20introduce%20a%20novel%20DEQ%20formulation%20based%20on%20Mirror%20Descent%20defined%20in%20terms%20of%20a%20tailored%20non-Euclidean%20geometry%20that%20naturally%20adapts%20with%20the%20structure%20of%20the%20data%20term.%20This%20enables%20the%20learning%20of%20neural%20regularizers%20within%20a%20principled%20training%20framework.%20We%20derive%20sufficient%20conditions%20and%20establish%20refined%20convergence%20results%20based%20on%20the%20Kurdyka--Lojasiewicz%20framework%20for%20subanalytic%20functions%20with%20non-closed%20domains%20to%20guarantee%20the%20convergence%20of%20the%20learned%20reconstruction%20scheme%20and%20propose%20computational%20strategies%20that%20enable%20both%20efficient%20training%20and%20parameter-free%20inference.%20Numerical%20experiments%20show%20that%20our%20method%20outperforms%20traditional%20model-based%20approaches%20and%20it%20is%20comparable%20to%20the%20performance%20of%20Bregman%20Plug-and-Play%20methods%2C%20while%20mitigating%20their%20typical%20drawbacks%2C%20such%20as%20time-consuming%20tuning%20of%20hyper-parameters.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/christiandaniele/DEQ-MD.&entry.1838667208=http%3A//arxiv.org/abs/2507.11461v2&entry.124074799=Read"},
{"title": "Resilient by Design -- Active Inference for Distributed Continuum Intelligence", "author": "Praveen Kumar Donta and Alfreds Lapkovskis and Enzo Mingozzi and Schahram Dustdar", "abstract": "Failures are the norm in highly complex and heterogeneous devices spanning the distributed computing continuum (DCC), from resource-constrained IoT and edge nodes to high-performance computing systems. Ensuring reliability and global consistency across these layers remains a major challenge, especially for AI-driven workloads requiring real-time, adaptive coordination. This work-in-progress paper introduces a Probabilistic Active Inference Resilience Agent (PAIR-Agent) to achieve resilience in DCC systems. PAIR-Agent performs three core operations: (i) constructing a causal fault graph from device logs, (ii) identifying faults while managing certainties and uncertainties using Markov blankets and the free energy principle, and (iii) autonomously healing issues through active inference. Through continuous monitoring and adaptive reconfiguration, the agent maintains service continuity and stability under diverse failure conditions. Theoretical validations confirm the reliability and effectiveness of the proposed framework.", "link": "http://arxiv.org/abs/2511.07202v2", "date": "2025-11-18", "relevancy": 1.8795, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5077}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4714}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resilient%20by%20Design%20--%20Active%20Inference%20for%20Distributed%20Continuum%20Intelligence&body=Title%3A%20Resilient%20by%20Design%20--%20Active%20Inference%20for%20Distributed%20Continuum%20Intelligence%0AAuthor%3A%20Praveen%20Kumar%20Donta%20and%20Alfreds%20Lapkovskis%20and%20Enzo%20Mingozzi%20and%20Schahram%20Dustdar%0AAbstract%3A%20Failures%20are%20the%20norm%20in%20highly%20complex%20and%20heterogeneous%20devices%20spanning%20the%20distributed%20computing%20continuum%20%28DCC%29%2C%20from%20resource-constrained%20IoT%20and%20edge%20nodes%20to%20high-performance%20computing%20systems.%20Ensuring%20reliability%20and%20global%20consistency%20across%20these%20layers%20remains%20a%20major%20challenge%2C%20especially%20for%20AI-driven%20workloads%20requiring%20real-time%2C%20adaptive%20coordination.%20This%20work-in-progress%20paper%20introduces%20a%20Probabilistic%20Active%20Inference%20Resilience%20Agent%20%28PAIR-Agent%29%20to%20achieve%20resilience%20in%20DCC%20systems.%20PAIR-Agent%20performs%20three%20core%20operations%3A%20%28i%29%20constructing%20a%20causal%20fault%20graph%20from%20device%20logs%2C%20%28ii%29%20identifying%20faults%20while%20managing%20certainties%20and%20uncertainties%20using%20Markov%20blankets%20and%20the%20free%20energy%20principle%2C%20and%20%28iii%29%20autonomously%20healing%20issues%20through%20active%20inference.%20Through%20continuous%20monitoring%20and%20adaptive%20reconfiguration%2C%20the%20agent%20maintains%20service%20continuity%20and%20stability%20under%20diverse%20failure%20conditions.%20Theoretical%20validations%20confirm%20the%20reliability%20and%20effectiveness%20of%20the%20proposed%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2511.07202v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResilient%2520by%2520Design%2520--%2520Active%2520Inference%2520for%2520Distributed%2520Continuum%2520Intelligence%26entry.906535625%3DPraveen%2520Kumar%2520Donta%2520and%2520Alfreds%2520Lapkovskis%2520and%2520Enzo%2520Mingozzi%2520and%2520Schahram%2520Dustdar%26entry.1292438233%3DFailures%2520are%2520the%2520norm%2520in%2520highly%2520complex%2520and%2520heterogeneous%2520devices%2520spanning%2520the%2520distributed%2520computing%2520continuum%2520%2528DCC%2529%252C%2520from%2520resource-constrained%2520IoT%2520and%2520edge%2520nodes%2520to%2520high-performance%2520computing%2520systems.%2520Ensuring%2520reliability%2520and%2520global%2520consistency%2520across%2520these%2520layers%2520remains%2520a%2520major%2520challenge%252C%2520especially%2520for%2520AI-driven%2520workloads%2520requiring%2520real-time%252C%2520adaptive%2520coordination.%2520This%2520work-in-progress%2520paper%2520introduces%2520a%2520Probabilistic%2520Active%2520Inference%2520Resilience%2520Agent%2520%2528PAIR-Agent%2529%2520to%2520achieve%2520resilience%2520in%2520DCC%2520systems.%2520PAIR-Agent%2520performs%2520three%2520core%2520operations%253A%2520%2528i%2529%2520constructing%2520a%2520causal%2520fault%2520graph%2520from%2520device%2520logs%252C%2520%2528ii%2529%2520identifying%2520faults%2520while%2520managing%2520certainties%2520and%2520uncertainties%2520using%2520Markov%2520blankets%2520and%2520the%2520free%2520energy%2520principle%252C%2520and%2520%2528iii%2529%2520autonomously%2520healing%2520issues%2520through%2520active%2520inference.%2520Through%2520continuous%2520monitoring%2520and%2520adaptive%2520reconfiguration%252C%2520the%2520agent%2520maintains%2520service%2520continuity%2520and%2520stability%2520under%2520diverse%2520failure%2520conditions.%2520Theoretical%2520validations%2520confirm%2520the%2520reliability%2520and%2520effectiveness%2520of%2520the%2520proposed%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.07202v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resilient%20by%20Design%20--%20Active%20Inference%20for%20Distributed%20Continuum%20Intelligence&entry.906535625=Praveen%20Kumar%20Donta%20and%20Alfreds%20Lapkovskis%20and%20Enzo%20Mingozzi%20and%20Schahram%20Dustdar&entry.1292438233=Failures%20are%20the%20norm%20in%20highly%20complex%20and%20heterogeneous%20devices%20spanning%20the%20distributed%20computing%20continuum%20%28DCC%29%2C%20from%20resource-constrained%20IoT%20and%20edge%20nodes%20to%20high-performance%20computing%20systems.%20Ensuring%20reliability%20and%20global%20consistency%20across%20these%20layers%20remains%20a%20major%20challenge%2C%20especially%20for%20AI-driven%20workloads%20requiring%20real-time%2C%20adaptive%20coordination.%20This%20work-in-progress%20paper%20introduces%20a%20Probabilistic%20Active%20Inference%20Resilience%20Agent%20%28PAIR-Agent%29%20to%20achieve%20resilience%20in%20DCC%20systems.%20PAIR-Agent%20performs%20three%20core%20operations%3A%20%28i%29%20constructing%20a%20causal%20fault%20graph%20from%20device%20logs%2C%20%28ii%29%20identifying%20faults%20while%20managing%20certainties%20and%20uncertainties%20using%20Markov%20blankets%20and%20the%20free%20energy%20principle%2C%20and%20%28iii%29%20autonomously%20healing%20issues%20through%20active%20inference.%20Through%20continuous%20monitoring%20and%20adaptive%20reconfiguration%2C%20the%20agent%20maintains%20service%20continuity%20and%20stability%20under%20diverse%20failure%20conditions.%20Theoretical%20validations%20confirm%20the%20reliability%20and%20effectiveness%20of%20the%20proposed%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2511.07202v2&entry.124074799=Read"},
{"title": "Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks", "author": "Nicola Rares Franco and Lorenzo Tedesco", "abstract": "We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\\varphi=\\varphi(x,u)$ such that $\\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.", "link": "http://arxiv.org/abs/2511.14455v1", "date": "2025-11-18", "relevancy": 1.4615, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5023}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4694}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonparametric%20estimation%20of%20conditional%20probability%20distributions%20using%20a%20generative%20approach%20based%20on%20conditional%20push-forward%20neural%20networks&body=Title%3A%20Nonparametric%20estimation%20of%20conditional%20probability%20distributions%20using%20a%20generative%20approach%20based%20on%20conditional%20push-forward%20neural%20networks%0AAuthor%3A%20Nicola%20Rares%20Franco%20and%20Lorenzo%20Tedesco%0AAbstract%3A%20We%20introduce%20conditional%20push-forward%20neural%20networks%20%28CPFN%29%2C%20a%20generative%20framework%20for%20conditional%20distribution%20estimation.%20Instead%20of%20directly%20modeling%20the%20conditional%20density%20%24f_%7BY%7CX%7D%24%2C%20CPFN%20learns%20a%20stochastic%20map%20%24%5Cvarphi%3D%5Cvarphi%28x%2Cu%29%24%20such%20that%20%24%5Cvarphi%28x%2CU%29%24%20and%20%24Y%7CX%3Dx%24%20follow%20approximately%20the%20same%20law%2C%20with%20%24U%24%20a%20suitable%20random%20vector%20of%20pre-defined%20latent%20variables.%20This%20enables%20efficient%20conditional%20sampling%20and%20straightforward%20estimation%20of%20conditional%20statistics%20through%20Monte%20Carlo%20methods.%20The%20model%20is%20trained%20via%20an%20objective%20function%20derived%20from%20a%20Kullback-Leibler%20formulation%2C%20without%20requiring%20invertibility%20or%20adversarial%20training.%20We%20establish%20a%20near-asymptotic%20consistency%20result%20and%20demonstrate%20experimentally%20that%20CPFN%20can%20achieve%20performance%20competitive%20with%2C%20or%20even%20superior%20to%2C%20state-of-the-art%20methods%2C%20including%20kernel%20estimators%2C%20tree-based%20algorithms%2C%20and%20popular%20deep%20learning%20techniques%2C%20all%20while%20remaining%20lightweight%20and%20easy%20to%20train.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonparametric%2520estimation%2520of%2520conditional%2520probability%2520distributions%2520using%2520a%2520generative%2520approach%2520based%2520on%2520conditional%2520push-forward%2520neural%2520networks%26entry.906535625%3DNicola%2520Rares%2520Franco%2520and%2520Lorenzo%2520Tedesco%26entry.1292438233%3DWe%2520introduce%2520conditional%2520push-forward%2520neural%2520networks%2520%2528CPFN%2529%252C%2520a%2520generative%2520framework%2520for%2520conditional%2520distribution%2520estimation.%2520Instead%2520of%2520directly%2520modeling%2520the%2520conditional%2520density%2520%2524f_%257BY%257CX%257D%2524%252C%2520CPFN%2520learns%2520a%2520stochastic%2520map%2520%2524%255Cvarphi%253D%255Cvarphi%2528x%252Cu%2529%2524%2520such%2520that%2520%2524%255Cvarphi%2528x%252CU%2529%2524%2520and%2520%2524Y%257CX%253Dx%2524%2520follow%2520approximately%2520the%2520same%2520law%252C%2520with%2520%2524U%2524%2520a%2520suitable%2520random%2520vector%2520of%2520pre-defined%2520latent%2520variables.%2520This%2520enables%2520efficient%2520conditional%2520sampling%2520and%2520straightforward%2520estimation%2520of%2520conditional%2520statistics%2520through%2520Monte%2520Carlo%2520methods.%2520The%2520model%2520is%2520trained%2520via%2520an%2520objective%2520function%2520derived%2520from%2520a%2520Kullback-Leibler%2520formulation%252C%2520without%2520requiring%2520invertibility%2520or%2520adversarial%2520training.%2520We%2520establish%2520a%2520near-asymptotic%2520consistency%2520result%2520and%2520demonstrate%2520experimentally%2520that%2520CPFN%2520can%2520achieve%2520performance%2520competitive%2520with%252C%2520or%2520even%2520superior%2520to%252C%2520state-of-the-art%2520methods%252C%2520including%2520kernel%2520estimators%252C%2520tree-based%2520algorithms%252C%2520and%2520popular%2520deep%2520learning%2520techniques%252C%2520all%2520while%2520remaining%2520lightweight%2520and%2520easy%2520to%2520train.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonparametric%20estimation%20of%20conditional%20probability%20distributions%20using%20a%20generative%20approach%20based%20on%20conditional%20push-forward%20neural%20networks&entry.906535625=Nicola%20Rares%20Franco%20and%20Lorenzo%20Tedesco&entry.1292438233=We%20introduce%20conditional%20push-forward%20neural%20networks%20%28CPFN%29%2C%20a%20generative%20framework%20for%20conditional%20distribution%20estimation.%20Instead%20of%20directly%20modeling%20the%20conditional%20density%20%24f_%7BY%7CX%7D%24%2C%20CPFN%20learns%20a%20stochastic%20map%20%24%5Cvarphi%3D%5Cvarphi%28x%2Cu%29%24%20such%20that%20%24%5Cvarphi%28x%2CU%29%24%20and%20%24Y%7CX%3Dx%24%20follow%20approximately%20the%20same%20law%2C%20with%20%24U%24%20a%20suitable%20random%20vector%20of%20pre-defined%20latent%20variables.%20This%20enables%20efficient%20conditional%20sampling%20and%20straightforward%20estimation%20of%20conditional%20statistics%20through%20Monte%20Carlo%20methods.%20The%20model%20is%20trained%20via%20an%20objective%20function%20derived%20from%20a%20Kullback-Leibler%20formulation%2C%20without%20requiring%20invertibility%20or%20adversarial%20training.%20We%20establish%20a%20near-asymptotic%20consistency%20result%20and%20demonstrate%20experimentally%20that%20CPFN%20can%20achieve%20performance%20competitive%20with%2C%20or%20even%20superior%20to%2C%20state-of-the-art%20methods%2C%20including%20kernel%20estimators%2C%20tree-based%20algorithms%2C%20and%20popular%20deep%20learning%20techniques%2C%20all%20while%20remaining%20lightweight%20and%20easy%20to%20train.&entry.1838667208=http%3A//arxiv.org/abs/2511.14455v1&entry.124074799=Read"},
{"title": "AdamHD: Decoupled Huber Decay Regularization for Language Model Pre-Training", "author": "Fu-Ming Guo and Yingfang Fan", "abstract": "Adaptive optimizers with decoupled weight decay, such as AdamW, are the de facto standard for pre-training large transformer-based generative models. Yet the quadratic nature of the $\\ell_2$ penalty embedded in weight decay drives all parameters toward the origin at the same rate, making the update vulnerable to rare but extreme gradient directions and often over-penalizing well-conditioned coordinates. We propose AdamHuberDecay, a drop-in replacement for AdamW that substitutes the $\\ell_2$ penalty with a decoupled smooth Huber regularizer. The resulting update decays parameters quadratically while their magnitude remains below a threshold $\u03b4$, and linearly ($\\ell_1$-like) once they exceed $\u03b4$, yielding (i) bounded regularization gradients, (ii) invariance to per-coordinate second-moment rescaling, and (iii) stronger sparsity pressure on overgrown weights.\n  We derive the closed-form decoupled Huber decay step and show how to integrate it with any Adam-family optimizer at $O(1)$ extra cost. Extensive experiments on GPT-2 and GPT-3 pre-training demonstrate that AdamHuberDecay (a) converges 10-15% faster in wall-clock time, (b) reduces validation perplexity by up to 4 points, (c) delivers performance improvements of 2.5-4.7% across downstream tasks, and (d) yields visibly sparser weight histograms that translate into 20-30% memory savings after magnitude pruning, without tuning the decay coefficient beyond the default grid used for AdamW. Ablations confirm robustness to outlier gradients and large-batch regimes, together with theoretical analyses that bound the expected parameter norm under noisy updates. AdamHuberDecay therefore provides a simple, principled path toward more efficient and resilient training of next-generation foundational generative transformers.", "link": "http://arxiv.org/abs/2511.14721v1", "date": "2025-11-18", "relevancy": 2.0225, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5394}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5142}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdamHD%3A%20Decoupled%20Huber%20Decay%20Regularization%20for%20Language%20Model%20Pre-Training&body=Title%3A%20AdamHD%3A%20Decoupled%20Huber%20Decay%20Regularization%20for%20Language%20Model%20Pre-Training%0AAuthor%3A%20Fu-Ming%20Guo%20and%20Yingfang%20Fan%0AAbstract%3A%20Adaptive%20optimizers%20with%20decoupled%20weight%20decay%2C%20such%20as%20AdamW%2C%20are%20the%20de%20facto%20standard%20for%20pre-training%20large%20transformer-based%20generative%20models.%20Yet%20the%20quadratic%20nature%20of%20the%20%24%5Cell_2%24%20penalty%20embedded%20in%20weight%20decay%20drives%20all%20parameters%20toward%20the%20origin%20at%20the%20same%20rate%2C%20making%20the%20update%20vulnerable%20to%20rare%20but%20extreme%20gradient%20directions%20and%20often%20over-penalizing%20well-conditioned%20coordinates.%20We%20propose%20AdamHuberDecay%2C%20a%20drop-in%20replacement%20for%20AdamW%20that%20substitutes%20the%20%24%5Cell_2%24%20penalty%20with%20a%20decoupled%20smooth%20Huber%20regularizer.%20The%20resulting%20update%20decays%20parameters%20quadratically%20while%20their%20magnitude%20remains%20below%20a%20threshold%20%24%CE%B4%24%2C%20and%20linearly%20%28%24%5Cell_1%24-like%29%20once%20they%20exceed%20%24%CE%B4%24%2C%20yielding%20%28i%29%20bounded%20regularization%20gradients%2C%20%28ii%29%20invariance%20to%20per-coordinate%20second-moment%20rescaling%2C%20and%20%28iii%29%20stronger%20sparsity%20pressure%20on%20overgrown%20weights.%0A%20%20We%20derive%20the%20closed-form%20decoupled%20Huber%20decay%20step%20and%20show%20how%20to%20integrate%20it%20with%20any%20Adam-family%20optimizer%20at%20%24O%281%29%24%20extra%20cost.%20Extensive%20experiments%20on%20GPT-2%20and%20GPT-3%20pre-training%20demonstrate%20that%20AdamHuberDecay%20%28a%29%20converges%2010-15%25%20faster%20in%20wall-clock%20time%2C%20%28b%29%20reduces%20validation%20perplexity%20by%20up%20to%204%20points%2C%20%28c%29%20delivers%20performance%20improvements%20of%202.5-4.7%25%20across%20downstream%20tasks%2C%20and%20%28d%29%20yields%20visibly%20sparser%20weight%20histograms%20that%20translate%20into%2020-30%25%20memory%20savings%20after%20magnitude%20pruning%2C%20without%20tuning%20the%20decay%20coefficient%20beyond%20the%20default%20grid%20used%20for%20AdamW.%20Ablations%20confirm%20robustness%20to%20outlier%20gradients%20and%20large-batch%20regimes%2C%20together%20with%20theoretical%20analyses%20that%20bound%20the%20expected%20parameter%20norm%20under%20noisy%20updates.%20AdamHuberDecay%20therefore%20provides%20a%20simple%2C%20principled%20path%20toward%20more%20efficient%20and%20resilient%20training%20of%20next-generation%20foundational%20generative%20transformers.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdamHD%253A%2520Decoupled%2520Huber%2520Decay%2520Regularization%2520for%2520Language%2520Model%2520Pre-Training%26entry.906535625%3DFu-Ming%2520Guo%2520and%2520Yingfang%2520Fan%26entry.1292438233%3DAdaptive%2520optimizers%2520with%2520decoupled%2520weight%2520decay%252C%2520such%2520as%2520AdamW%252C%2520are%2520the%2520de%2520facto%2520standard%2520for%2520pre-training%2520large%2520transformer-based%2520generative%2520models.%2520Yet%2520the%2520quadratic%2520nature%2520of%2520the%2520%2524%255Cell_2%2524%2520penalty%2520embedded%2520in%2520weight%2520decay%2520drives%2520all%2520parameters%2520toward%2520the%2520origin%2520at%2520the%2520same%2520rate%252C%2520making%2520the%2520update%2520vulnerable%2520to%2520rare%2520but%2520extreme%2520gradient%2520directions%2520and%2520often%2520over-penalizing%2520well-conditioned%2520coordinates.%2520We%2520propose%2520AdamHuberDecay%252C%2520a%2520drop-in%2520replacement%2520for%2520AdamW%2520that%2520substitutes%2520the%2520%2524%255Cell_2%2524%2520penalty%2520with%2520a%2520decoupled%2520smooth%2520Huber%2520regularizer.%2520The%2520resulting%2520update%2520decays%2520parameters%2520quadratically%2520while%2520their%2520magnitude%2520remains%2520below%2520a%2520threshold%2520%2524%25CE%25B4%2524%252C%2520and%2520linearly%2520%2528%2524%255Cell_1%2524-like%2529%2520once%2520they%2520exceed%2520%2524%25CE%25B4%2524%252C%2520yielding%2520%2528i%2529%2520bounded%2520regularization%2520gradients%252C%2520%2528ii%2529%2520invariance%2520to%2520per-coordinate%2520second-moment%2520rescaling%252C%2520and%2520%2528iii%2529%2520stronger%2520sparsity%2520pressure%2520on%2520overgrown%2520weights.%250A%2520%2520We%2520derive%2520the%2520closed-form%2520decoupled%2520Huber%2520decay%2520step%2520and%2520show%2520how%2520to%2520integrate%2520it%2520with%2520any%2520Adam-family%2520optimizer%2520at%2520%2524O%25281%2529%2524%2520extra%2520cost.%2520Extensive%2520experiments%2520on%2520GPT-2%2520and%2520GPT-3%2520pre-training%2520demonstrate%2520that%2520AdamHuberDecay%2520%2528a%2529%2520converges%252010-15%2525%2520faster%2520in%2520wall-clock%2520time%252C%2520%2528b%2529%2520reduces%2520validation%2520perplexity%2520by%2520up%2520to%25204%2520points%252C%2520%2528c%2529%2520delivers%2520performance%2520improvements%2520of%25202.5-4.7%2525%2520across%2520downstream%2520tasks%252C%2520and%2520%2528d%2529%2520yields%2520visibly%2520sparser%2520weight%2520histograms%2520that%2520translate%2520into%252020-30%2525%2520memory%2520savings%2520after%2520magnitude%2520pruning%252C%2520without%2520tuning%2520the%2520decay%2520coefficient%2520beyond%2520the%2520default%2520grid%2520used%2520for%2520AdamW.%2520Ablations%2520confirm%2520robustness%2520to%2520outlier%2520gradients%2520and%2520large-batch%2520regimes%252C%2520together%2520with%2520theoretical%2520analyses%2520that%2520bound%2520the%2520expected%2520parameter%2520norm%2520under%2520noisy%2520updates.%2520AdamHuberDecay%2520therefore%2520provides%2520a%2520simple%252C%2520principled%2520path%2520toward%2520more%2520efficient%2520and%2520resilient%2520training%2520of%2520next-generation%2520foundational%2520generative%2520transformers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdamHD%3A%20Decoupled%20Huber%20Decay%20Regularization%20for%20Language%20Model%20Pre-Training&entry.906535625=Fu-Ming%20Guo%20and%20Yingfang%20Fan&entry.1292438233=Adaptive%20optimizers%20with%20decoupled%20weight%20decay%2C%20such%20as%20AdamW%2C%20are%20the%20de%20facto%20standard%20for%20pre-training%20large%20transformer-based%20generative%20models.%20Yet%20the%20quadratic%20nature%20of%20the%20%24%5Cell_2%24%20penalty%20embedded%20in%20weight%20decay%20drives%20all%20parameters%20toward%20the%20origin%20at%20the%20same%20rate%2C%20making%20the%20update%20vulnerable%20to%20rare%20but%20extreme%20gradient%20directions%20and%20often%20over-penalizing%20well-conditioned%20coordinates.%20We%20propose%20AdamHuberDecay%2C%20a%20drop-in%20replacement%20for%20AdamW%20that%20substitutes%20the%20%24%5Cell_2%24%20penalty%20with%20a%20decoupled%20smooth%20Huber%20regularizer.%20The%20resulting%20update%20decays%20parameters%20quadratically%20while%20their%20magnitude%20remains%20below%20a%20threshold%20%24%CE%B4%24%2C%20and%20linearly%20%28%24%5Cell_1%24-like%29%20once%20they%20exceed%20%24%CE%B4%24%2C%20yielding%20%28i%29%20bounded%20regularization%20gradients%2C%20%28ii%29%20invariance%20to%20per-coordinate%20second-moment%20rescaling%2C%20and%20%28iii%29%20stronger%20sparsity%20pressure%20on%20overgrown%20weights.%0A%20%20We%20derive%20the%20closed-form%20decoupled%20Huber%20decay%20step%20and%20show%20how%20to%20integrate%20it%20with%20any%20Adam-family%20optimizer%20at%20%24O%281%29%24%20extra%20cost.%20Extensive%20experiments%20on%20GPT-2%20and%20GPT-3%20pre-training%20demonstrate%20that%20AdamHuberDecay%20%28a%29%20converges%2010-15%25%20faster%20in%20wall-clock%20time%2C%20%28b%29%20reduces%20validation%20perplexity%20by%20up%20to%204%20points%2C%20%28c%29%20delivers%20performance%20improvements%20of%202.5-4.7%25%20across%20downstream%20tasks%2C%20and%20%28d%29%20yields%20visibly%20sparser%20weight%20histograms%20that%20translate%20into%2020-30%25%20memory%20savings%20after%20magnitude%20pruning%2C%20without%20tuning%20the%20decay%20coefficient%20beyond%20the%20default%20grid%20used%20for%20AdamW.%20Ablations%20confirm%20robustness%20to%20outlier%20gradients%20and%20large-batch%20regimes%2C%20together%20with%20theoretical%20analyses%20that%20bound%20the%20expected%20parameter%20norm%20under%20noisy%20updates.%20AdamHuberDecay%20therefore%20provides%20a%20simple%2C%20principled%20path%20toward%20more%20efficient%20and%20resilient%20training%20of%20next-generation%20foundational%20generative%20transformers.&entry.1838667208=http%3A//arxiv.org/abs/2511.14721v1&entry.124074799=Read"},
{"title": "LSP-YOLO: A Lightweight Single-Stage Network for Sitting Posture Recognition on Embedded Devices", "author": "Nanjun Li and Ziyue Hao and Quanqiang Wang and Xuanyin Wang", "abstract": "With the rise in sedentary behavior, health problems caused by poor sitting posture have drawn increasing attention. Most existing methods, whether using invasive sensors or computer vision, rely on two-stage pipelines, which result in high intrusiveness, intensive computation, and poor real-time performance on embedded edge devices. Inspired by YOLOv11-Pose, a lightweight single-stage network for sitting posture recognition on embedded edge devices termed LSP-YOLO was proposed. By integrating partial convolution(PConv) and Similarity-Aware Activation Module(SimAM), a lightweight module, Light-C3k2, was designed to reduce computational cost while maintaining feature extraction capability. In the recognition head, keypoints were directly mapped to posture classes through pointwise convolution, and intermediate supervision was employed to enable efficient fusion of pose estimation and classification. Furthermore, a dataset containing 5,000 images across six posture categories was constructed for model training and testing. The smallest trained model, LSP-YOLO-n, achieved 94.2% accuracy and 251 Fps on personal computer(PC) with a model size of only 1.9 MB. Meanwhile, real-time and high-accuracy inference under constrained computational resources was demonstrated on the SV830C + GC030A platform. The proposed approach is characterized by high efficiency, lightweight design and deployability, making it suitable for smart classrooms, rehabilitation, and human-computer interaction applications.", "link": "http://arxiv.org/abs/2511.14322v1", "date": "2025-11-18", "relevancy": 2.0286, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.52}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5024}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LSP-YOLO%3A%20A%20Lightweight%20Single-Stage%20Network%20for%20Sitting%20Posture%20Recognition%20on%20Embedded%20Devices&body=Title%3A%20LSP-YOLO%3A%20A%20Lightweight%20Single-Stage%20Network%20for%20Sitting%20Posture%20Recognition%20on%20Embedded%20Devices%0AAuthor%3A%20Nanjun%20Li%20and%20Ziyue%20Hao%20and%20Quanqiang%20Wang%20and%20Xuanyin%20Wang%0AAbstract%3A%20With%20the%20rise%20in%20sedentary%20behavior%2C%20health%20problems%20caused%20by%20poor%20sitting%20posture%20have%20drawn%20increasing%20attention.%20Most%20existing%20methods%2C%20whether%20using%20invasive%20sensors%20or%20computer%20vision%2C%20rely%20on%20two-stage%20pipelines%2C%20which%20result%20in%20high%20intrusiveness%2C%20intensive%20computation%2C%20and%20poor%20real-time%20performance%20on%20embedded%20edge%20devices.%20Inspired%20by%20YOLOv11-Pose%2C%20a%20lightweight%20single-stage%20network%20for%20sitting%20posture%20recognition%20on%20embedded%20edge%20devices%20termed%20LSP-YOLO%20was%20proposed.%20By%20integrating%20partial%20convolution%28PConv%29%20and%20Similarity-Aware%20Activation%20Module%28SimAM%29%2C%20a%20lightweight%20module%2C%20Light-C3k2%2C%20was%20designed%20to%20reduce%20computational%20cost%20while%20maintaining%20feature%20extraction%20capability.%20In%20the%20recognition%20head%2C%20keypoints%20were%20directly%20mapped%20to%20posture%20classes%20through%20pointwise%20convolution%2C%20and%20intermediate%20supervision%20was%20employed%20to%20enable%20efficient%20fusion%20of%20pose%20estimation%20and%20classification.%20Furthermore%2C%20a%20dataset%20containing%205%2C000%20images%20across%20six%20posture%20categories%20was%20constructed%20for%20model%20training%20and%20testing.%20The%20smallest%20trained%20model%2C%20LSP-YOLO-n%2C%20achieved%2094.2%25%20accuracy%20and%20251%20Fps%20on%20personal%20computer%28PC%29%20with%20a%20model%20size%20of%20only%201.9%20MB.%20Meanwhile%2C%20real-time%20and%20high-accuracy%20inference%20under%20constrained%20computational%20resources%20was%20demonstrated%20on%20the%20SV830C%20%2B%20GC030A%20platform.%20The%20proposed%20approach%20is%20characterized%20by%20high%20efficiency%2C%20lightweight%20design%20and%20deployability%2C%20making%20it%20suitable%20for%20smart%20classrooms%2C%20rehabilitation%2C%20and%20human-computer%20interaction%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLSP-YOLO%253A%2520A%2520Lightweight%2520Single-Stage%2520Network%2520for%2520Sitting%2520Posture%2520Recognition%2520on%2520Embedded%2520Devices%26entry.906535625%3DNanjun%2520Li%2520and%2520Ziyue%2520Hao%2520and%2520Quanqiang%2520Wang%2520and%2520Xuanyin%2520Wang%26entry.1292438233%3DWith%2520the%2520rise%2520in%2520sedentary%2520behavior%252C%2520health%2520problems%2520caused%2520by%2520poor%2520sitting%2520posture%2520have%2520drawn%2520increasing%2520attention.%2520Most%2520existing%2520methods%252C%2520whether%2520using%2520invasive%2520sensors%2520or%2520computer%2520vision%252C%2520rely%2520on%2520two-stage%2520pipelines%252C%2520which%2520result%2520in%2520high%2520intrusiveness%252C%2520intensive%2520computation%252C%2520and%2520poor%2520real-time%2520performance%2520on%2520embedded%2520edge%2520devices.%2520Inspired%2520by%2520YOLOv11-Pose%252C%2520a%2520lightweight%2520single-stage%2520network%2520for%2520sitting%2520posture%2520recognition%2520on%2520embedded%2520edge%2520devices%2520termed%2520LSP-YOLO%2520was%2520proposed.%2520By%2520integrating%2520partial%2520convolution%2528PConv%2529%2520and%2520Similarity-Aware%2520Activation%2520Module%2528SimAM%2529%252C%2520a%2520lightweight%2520module%252C%2520Light-C3k2%252C%2520was%2520designed%2520to%2520reduce%2520computational%2520cost%2520while%2520maintaining%2520feature%2520extraction%2520capability.%2520In%2520the%2520recognition%2520head%252C%2520keypoints%2520were%2520directly%2520mapped%2520to%2520posture%2520classes%2520through%2520pointwise%2520convolution%252C%2520and%2520intermediate%2520supervision%2520was%2520employed%2520to%2520enable%2520efficient%2520fusion%2520of%2520pose%2520estimation%2520and%2520classification.%2520Furthermore%252C%2520a%2520dataset%2520containing%25205%252C000%2520images%2520across%2520six%2520posture%2520categories%2520was%2520constructed%2520for%2520model%2520training%2520and%2520testing.%2520The%2520smallest%2520trained%2520model%252C%2520LSP-YOLO-n%252C%2520achieved%252094.2%2525%2520accuracy%2520and%2520251%2520Fps%2520on%2520personal%2520computer%2528PC%2529%2520with%2520a%2520model%2520size%2520of%2520only%25201.9%2520MB.%2520Meanwhile%252C%2520real-time%2520and%2520high-accuracy%2520inference%2520under%2520constrained%2520computational%2520resources%2520was%2520demonstrated%2520on%2520the%2520SV830C%2520%252B%2520GC030A%2520platform.%2520The%2520proposed%2520approach%2520is%2520characterized%2520by%2520high%2520efficiency%252C%2520lightweight%2520design%2520and%2520deployability%252C%2520making%2520it%2520suitable%2520for%2520smart%2520classrooms%252C%2520rehabilitation%252C%2520and%2520human-computer%2520interaction%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LSP-YOLO%3A%20A%20Lightweight%20Single-Stage%20Network%20for%20Sitting%20Posture%20Recognition%20on%20Embedded%20Devices&entry.906535625=Nanjun%20Li%20and%20Ziyue%20Hao%20and%20Quanqiang%20Wang%20and%20Xuanyin%20Wang&entry.1292438233=With%20the%20rise%20in%20sedentary%20behavior%2C%20health%20problems%20caused%20by%20poor%20sitting%20posture%20have%20drawn%20increasing%20attention.%20Most%20existing%20methods%2C%20whether%20using%20invasive%20sensors%20or%20computer%20vision%2C%20rely%20on%20two-stage%20pipelines%2C%20which%20result%20in%20high%20intrusiveness%2C%20intensive%20computation%2C%20and%20poor%20real-time%20performance%20on%20embedded%20edge%20devices.%20Inspired%20by%20YOLOv11-Pose%2C%20a%20lightweight%20single-stage%20network%20for%20sitting%20posture%20recognition%20on%20embedded%20edge%20devices%20termed%20LSP-YOLO%20was%20proposed.%20By%20integrating%20partial%20convolution%28PConv%29%20and%20Similarity-Aware%20Activation%20Module%28SimAM%29%2C%20a%20lightweight%20module%2C%20Light-C3k2%2C%20was%20designed%20to%20reduce%20computational%20cost%20while%20maintaining%20feature%20extraction%20capability.%20In%20the%20recognition%20head%2C%20keypoints%20were%20directly%20mapped%20to%20posture%20classes%20through%20pointwise%20convolution%2C%20and%20intermediate%20supervision%20was%20employed%20to%20enable%20efficient%20fusion%20of%20pose%20estimation%20and%20classification.%20Furthermore%2C%20a%20dataset%20containing%205%2C000%20images%20across%20six%20posture%20categories%20was%20constructed%20for%20model%20training%20and%20testing.%20The%20smallest%20trained%20model%2C%20LSP-YOLO-n%2C%20achieved%2094.2%25%20accuracy%20and%20251%20Fps%20on%20personal%20computer%28PC%29%20with%20a%20model%20size%20of%20only%201.9%20MB.%20Meanwhile%2C%20real-time%20and%20high-accuracy%20inference%20under%20constrained%20computational%20resources%20was%20demonstrated%20on%20the%20SV830C%20%2B%20GC030A%20platform.%20The%20proposed%20approach%20is%20characterized%20by%20high%20efficiency%2C%20lightweight%20design%20and%20deployability%2C%20making%20it%20suitable%20for%20smart%20classrooms%2C%20rehabilitation%2C%20and%20human-computer%20interaction%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2511.14322v1&entry.124074799=Read"},
{"title": "Failure to Mix: Large language models struggle to answer according to desired probability distributions", "author": "Ivy Yuqian Yang and David Yu Zhang", "abstract": "Scientific idea generation and selection requires exploration following a target probability distribution. In contrast, current AI benchmarks have objectively correct answers, and training large language models (LLMs) via reinforcement learning against these benchmarks discourages probabilistic exploration. Here, we conducted systematic experiments requesting LLMs to produce outputs following simple probabilistic distributions, and found that all modern LLMs tested grossly fail to follow the distributions. For example, requesting a binary output of \"1\" 49% of the time produces an answer of \"0\" nearly 100% of the time. This step function-like behavior of near-exclusively generating the output with marginally highest probability even overrules even strong in-built LLM biases.", "link": "http://arxiv.org/abs/2511.14630v1", "date": "2025-11-18", "relevancy": 1.8161, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4599}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4591}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Failure%20to%20Mix%3A%20Large%20language%20models%20struggle%20to%20answer%20according%20to%20desired%20probability%20distributions&body=Title%3A%20Failure%20to%20Mix%3A%20Large%20language%20models%20struggle%20to%20answer%20according%20to%20desired%20probability%20distributions%0AAuthor%3A%20Ivy%20Yuqian%20Yang%20and%20David%20Yu%20Zhang%0AAbstract%3A%20Scientific%20idea%20generation%20and%20selection%20requires%20exploration%20following%20a%20target%20probability%20distribution.%20In%20contrast%2C%20current%20AI%20benchmarks%20have%20objectively%20correct%20answers%2C%20and%20training%20large%20language%20models%20%28LLMs%29%20via%20reinforcement%20learning%20against%20these%20benchmarks%20discourages%20probabilistic%20exploration.%20Here%2C%20we%20conducted%20systematic%20experiments%20requesting%20LLMs%20to%20produce%20outputs%20following%20simple%20probabilistic%20distributions%2C%20and%20found%20that%20all%20modern%20LLMs%20tested%20grossly%20fail%20to%20follow%20the%20distributions.%20For%20example%2C%20requesting%20a%20binary%20output%20of%20%221%22%2049%25%20of%20the%20time%20produces%20an%20answer%20of%20%220%22%20nearly%20100%25%20of%20the%20time.%20This%20step%20function-like%20behavior%20of%20near-exclusively%20generating%20the%20output%20with%20marginally%20highest%20probability%20even%20overrules%20even%20strong%20in-built%20LLM%20biases.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFailure%2520to%2520Mix%253A%2520Large%2520language%2520models%2520struggle%2520to%2520answer%2520according%2520to%2520desired%2520probability%2520distributions%26entry.906535625%3DIvy%2520Yuqian%2520Yang%2520and%2520David%2520Yu%2520Zhang%26entry.1292438233%3DScientific%2520idea%2520generation%2520and%2520selection%2520requires%2520exploration%2520following%2520a%2520target%2520probability%2520distribution.%2520In%2520contrast%252C%2520current%2520AI%2520benchmarks%2520have%2520objectively%2520correct%2520answers%252C%2520and%2520training%2520large%2520language%2520models%2520%2528LLMs%2529%2520via%2520reinforcement%2520learning%2520against%2520these%2520benchmarks%2520discourages%2520probabilistic%2520exploration.%2520Here%252C%2520we%2520conducted%2520systematic%2520experiments%2520requesting%2520LLMs%2520to%2520produce%2520outputs%2520following%2520simple%2520probabilistic%2520distributions%252C%2520and%2520found%2520that%2520all%2520modern%2520LLMs%2520tested%2520grossly%2520fail%2520to%2520follow%2520the%2520distributions.%2520For%2520example%252C%2520requesting%2520a%2520binary%2520output%2520of%2520%25221%2522%252049%2525%2520of%2520the%2520time%2520produces%2520an%2520answer%2520of%2520%25220%2522%2520nearly%2520100%2525%2520of%2520the%2520time.%2520This%2520step%2520function-like%2520behavior%2520of%2520near-exclusively%2520generating%2520the%2520output%2520with%2520marginally%2520highest%2520probability%2520even%2520overrules%2520even%2520strong%2520in-built%2520LLM%2520biases.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Failure%20to%20Mix%3A%20Large%20language%20models%20struggle%20to%20answer%20according%20to%20desired%20probability%20distributions&entry.906535625=Ivy%20Yuqian%20Yang%20and%20David%20Yu%20Zhang&entry.1292438233=Scientific%20idea%20generation%20and%20selection%20requires%20exploration%20following%20a%20target%20probability%20distribution.%20In%20contrast%2C%20current%20AI%20benchmarks%20have%20objectively%20correct%20answers%2C%20and%20training%20large%20language%20models%20%28LLMs%29%20via%20reinforcement%20learning%20against%20these%20benchmarks%20discourages%20probabilistic%20exploration.%20Here%2C%20we%20conducted%20systematic%20experiments%20requesting%20LLMs%20to%20produce%20outputs%20following%20simple%20probabilistic%20distributions%2C%20and%20found%20that%20all%20modern%20LLMs%20tested%20grossly%20fail%20to%20follow%20the%20distributions.%20For%20example%2C%20requesting%20a%20binary%20output%20of%20%221%22%2049%25%20of%20the%20time%20produces%20an%20answer%20of%20%220%22%20nearly%20100%25%20of%20the%20time.%20This%20step%20function-like%20behavior%20of%20near-exclusively%20generating%20the%20output%20with%20marginally%20highest%20probability%20even%20overrules%20even%20strong%20in-built%20LLM%20biases.&entry.1838667208=http%3A//arxiv.org/abs/2511.14630v1&entry.124074799=Read"},
{"title": "Expert-Guided POMDP Learning for Data-Efficient Modeling in Healthcare", "author": "Marco Locatelli and Arjen Hommersom and Roberto Clemens Cerioli and Daniela Besozzi and Fabio Stella", "abstract": "Learning the parameters of Partially Observable Markov Decision Processes (POMDPs) from limited data is a significant challenge. We introduce the Fuzzy MAP EM algorithm, a novel approach that incorporates expert knowledge into the parameter estimation process by enriching the Expectation Maximization (EM) framework with fuzzy pseudo-counts derived from an expert-defined fuzzy model. This integration naturally reformulates the problem as a Maximum A Posteriori (MAP) estimation, effectively guiding learning in environments with limited data. In synthetic medical simulations, our method consistently outperforms the standard EM algorithm under both low-data and high-noise conditions. Furthermore, a case study on Myasthenia Gravis illustrates the ability of the Fuzzy MAP EM algorithm to recover a clinically coherent POMDP, demonstrating its potential as a practical tool for data-efficient modeling in healthcare.", "link": "http://arxiv.org/abs/2511.14619v1", "date": "2025-11-18", "relevancy": 1.4905, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5017}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4963}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expert-Guided%20POMDP%20Learning%20for%20Data-Efficient%20Modeling%20in%20Healthcare&body=Title%3A%20Expert-Guided%20POMDP%20Learning%20for%20Data-Efficient%20Modeling%20in%20Healthcare%0AAuthor%3A%20Marco%20Locatelli%20and%20Arjen%20Hommersom%20and%20Roberto%20Clemens%20Cerioli%20and%20Daniela%20Besozzi%20and%20Fabio%20Stella%0AAbstract%3A%20Learning%20the%20parameters%20of%20Partially%20Observable%20Markov%20Decision%20Processes%20%28POMDPs%29%20from%20limited%20data%20is%20a%20significant%20challenge.%20We%20introduce%20the%20Fuzzy%20MAP%20EM%20algorithm%2C%20a%20novel%20approach%20that%20incorporates%20expert%20knowledge%20into%20the%20parameter%20estimation%20process%20by%20enriching%20the%20Expectation%20Maximization%20%28EM%29%20framework%20with%20fuzzy%20pseudo-counts%20derived%20from%20an%20expert-defined%20fuzzy%20model.%20This%20integration%20naturally%20reformulates%20the%20problem%20as%20a%20Maximum%20A%20Posteriori%20%28MAP%29%20estimation%2C%20effectively%20guiding%20learning%20in%20environments%20with%20limited%20data.%20In%20synthetic%20medical%20simulations%2C%20our%20method%20consistently%20outperforms%20the%20standard%20EM%20algorithm%20under%20both%20low-data%20and%20high-noise%20conditions.%20Furthermore%2C%20a%20case%20study%20on%20Myasthenia%20Gravis%20illustrates%20the%20ability%20of%20the%20Fuzzy%20MAP%20EM%20algorithm%20to%20recover%20a%20clinically%20coherent%20POMDP%2C%20demonstrating%20its%20potential%20as%20a%20practical%20tool%20for%20data-efficient%20modeling%20in%20healthcare.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpert-Guided%2520POMDP%2520Learning%2520for%2520Data-Efficient%2520Modeling%2520in%2520Healthcare%26entry.906535625%3DMarco%2520Locatelli%2520and%2520Arjen%2520Hommersom%2520and%2520Roberto%2520Clemens%2520Cerioli%2520and%2520Daniela%2520Besozzi%2520and%2520Fabio%2520Stella%26entry.1292438233%3DLearning%2520the%2520parameters%2520of%2520Partially%2520Observable%2520Markov%2520Decision%2520Processes%2520%2528POMDPs%2529%2520from%2520limited%2520data%2520is%2520a%2520significant%2520challenge.%2520We%2520introduce%2520the%2520Fuzzy%2520MAP%2520EM%2520algorithm%252C%2520a%2520novel%2520approach%2520that%2520incorporates%2520expert%2520knowledge%2520into%2520the%2520parameter%2520estimation%2520process%2520by%2520enriching%2520the%2520Expectation%2520Maximization%2520%2528EM%2529%2520framework%2520with%2520fuzzy%2520pseudo-counts%2520derived%2520from%2520an%2520expert-defined%2520fuzzy%2520model.%2520This%2520integration%2520naturally%2520reformulates%2520the%2520problem%2520as%2520a%2520Maximum%2520A%2520Posteriori%2520%2528MAP%2529%2520estimation%252C%2520effectively%2520guiding%2520learning%2520in%2520environments%2520with%2520limited%2520data.%2520In%2520synthetic%2520medical%2520simulations%252C%2520our%2520method%2520consistently%2520outperforms%2520the%2520standard%2520EM%2520algorithm%2520under%2520both%2520low-data%2520and%2520high-noise%2520conditions.%2520Furthermore%252C%2520a%2520case%2520study%2520on%2520Myasthenia%2520Gravis%2520illustrates%2520the%2520ability%2520of%2520the%2520Fuzzy%2520MAP%2520EM%2520algorithm%2520to%2520recover%2520a%2520clinically%2520coherent%2520POMDP%252C%2520demonstrating%2520its%2520potential%2520as%2520a%2520practical%2520tool%2520for%2520data-efficient%2520modeling%2520in%2520healthcare.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expert-Guided%20POMDP%20Learning%20for%20Data-Efficient%20Modeling%20in%20Healthcare&entry.906535625=Marco%20Locatelli%20and%20Arjen%20Hommersom%20and%20Roberto%20Clemens%20Cerioli%20and%20Daniela%20Besozzi%20and%20Fabio%20Stella&entry.1292438233=Learning%20the%20parameters%20of%20Partially%20Observable%20Markov%20Decision%20Processes%20%28POMDPs%29%20from%20limited%20data%20is%20a%20significant%20challenge.%20We%20introduce%20the%20Fuzzy%20MAP%20EM%20algorithm%2C%20a%20novel%20approach%20that%20incorporates%20expert%20knowledge%20into%20the%20parameter%20estimation%20process%20by%20enriching%20the%20Expectation%20Maximization%20%28EM%29%20framework%20with%20fuzzy%20pseudo-counts%20derived%20from%20an%20expert-defined%20fuzzy%20model.%20This%20integration%20naturally%20reformulates%20the%20problem%20as%20a%20Maximum%20A%20Posteriori%20%28MAP%29%20estimation%2C%20effectively%20guiding%20learning%20in%20environments%20with%20limited%20data.%20In%20synthetic%20medical%20simulations%2C%20our%20method%20consistently%20outperforms%20the%20standard%20EM%20algorithm%20under%20both%20low-data%20and%20high-noise%20conditions.%20Furthermore%2C%20a%20case%20study%20on%20Myasthenia%20Gravis%20illustrates%20the%20ability%20of%20the%20Fuzzy%20MAP%20EM%20algorithm%20to%20recover%20a%20clinically%20coherent%20POMDP%2C%20demonstrating%20its%20potential%20as%20a%20practical%20tool%20for%20data-efficient%20modeling%20in%20healthcare.&entry.1838667208=http%3A//arxiv.org/abs/2511.14619v1&entry.124074799=Read"},
{"title": "Mutation Testing for Industrial Robotic Systems", "author": "Marcela Gon\u00e7alves dos Santos and Sylvain Hall\u00e9 and F\u00e1bio Petrillo", "abstract": "Industrial robotic systems (IRS) are increasingly deployed in diverse environments, where failures can result in severe accidents and costly downtime. Ensuring the reliability of the software controlling these systems is therefore critical. Mutation testing, a technique widely used in software engineering, evaluates the effectiveness of test suites by introducing small faults, or mutants, into the code. However, traditional mutation operators are poorly suited to robotic programs, which involve message-based commands and interactions with the physical world. This paper explores the adaptation of mutation testing to IRS by defining domain-specific mutation operators that capture the semantics of robot actions and sensor readings. We propose a methodology for generating meaningful mutants at the level of high-level read and write operations, including movement, gripper actions, and sensor noise injection. An empirical study on a pick-and-place scenario demonstrates that our approach produces more informative mutants and reduces the number of invalid or equivalent cases compared to conventional operators. Results highlight the potential of mutation testing to enhance test suite quality and contribute to safer, more reliable industrial robotic systems.", "link": "http://arxiv.org/abs/2511.14432v1", "date": "2025-11-18", "relevancy": 1.3501, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4632}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mutation%20Testing%20for%20Industrial%20Robotic%20Systems&body=Title%3A%20Mutation%20Testing%20for%20Industrial%20Robotic%20Systems%0AAuthor%3A%20Marcela%20Gon%C3%A7alves%20dos%20Santos%20and%20Sylvain%20Hall%C3%A9%20and%20F%C3%A1bio%20Petrillo%0AAbstract%3A%20Industrial%20robotic%20systems%20%28IRS%29%20are%20increasingly%20deployed%20in%20diverse%20environments%2C%20where%20failures%20can%20result%20in%20severe%20accidents%20and%20costly%20downtime.%20Ensuring%20the%20reliability%20of%20the%20software%20controlling%20these%20systems%20is%20therefore%20critical.%20Mutation%20testing%2C%20a%20technique%20widely%20used%20in%20software%20engineering%2C%20evaluates%20the%20effectiveness%20of%20test%20suites%20by%20introducing%20small%20faults%2C%20or%20mutants%2C%20into%20the%20code.%20However%2C%20traditional%20mutation%20operators%20are%20poorly%20suited%20to%20robotic%20programs%2C%20which%20involve%20message-based%20commands%20and%20interactions%20with%20the%20physical%20world.%20This%20paper%20explores%20the%20adaptation%20of%20mutation%20testing%20to%20IRS%20by%20defining%20domain-specific%20mutation%20operators%20that%20capture%20the%20semantics%20of%20robot%20actions%20and%20sensor%20readings.%20We%20propose%20a%20methodology%20for%20generating%20meaningful%20mutants%20at%20the%20level%20of%20high-level%20read%20and%20write%20operations%2C%20including%20movement%2C%20gripper%20actions%2C%20and%20sensor%20noise%20injection.%20An%20empirical%20study%20on%20a%20pick-and-place%20scenario%20demonstrates%20that%20our%20approach%20produces%20more%20informative%20mutants%20and%20reduces%20the%20number%20of%20invalid%20or%20equivalent%20cases%20compared%20to%20conventional%20operators.%20Results%20highlight%20the%20potential%20of%20mutation%20testing%20to%20enhance%20test%20suite%20quality%20and%20contribute%20to%20safer%2C%20more%20reliable%20industrial%20robotic%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMutation%2520Testing%2520for%2520Industrial%2520Robotic%2520Systems%26entry.906535625%3DMarcela%2520Gon%25C3%25A7alves%2520dos%2520Santos%2520and%2520Sylvain%2520Hall%25C3%25A9%2520and%2520F%25C3%25A1bio%2520Petrillo%26entry.1292438233%3DIndustrial%2520robotic%2520systems%2520%2528IRS%2529%2520are%2520increasingly%2520deployed%2520in%2520diverse%2520environments%252C%2520where%2520failures%2520can%2520result%2520in%2520severe%2520accidents%2520and%2520costly%2520downtime.%2520Ensuring%2520the%2520reliability%2520of%2520the%2520software%2520controlling%2520these%2520systems%2520is%2520therefore%2520critical.%2520Mutation%2520testing%252C%2520a%2520technique%2520widely%2520used%2520in%2520software%2520engineering%252C%2520evaluates%2520the%2520effectiveness%2520of%2520test%2520suites%2520by%2520introducing%2520small%2520faults%252C%2520or%2520mutants%252C%2520into%2520the%2520code.%2520However%252C%2520traditional%2520mutation%2520operators%2520are%2520poorly%2520suited%2520to%2520robotic%2520programs%252C%2520which%2520involve%2520message-based%2520commands%2520and%2520interactions%2520with%2520the%2520physical%2520world.%2520This%2520paper%2520explores%2520the%2520adaptation%2520of%2520mutation%2520testing%2520to%2520IRS%2520by%2520defining%2520domain-specific%2520mutation%2520operators%2520that%2520capture%2520the%2520semantics%2520of%2520robot%2520actions%2520and%2520sensor%2520readings.%2520We%2520propose%2520a%2520methodology%2520for%2520generating%2520meaningful%2520mutants%2520at%2520the%2520level%2520of%2520high-level%2520read%2520and%2520write%2520operations%252C%2520including%2520movement%252C%2520gripper%2520actions%252C%2520and%2520sensor%2520noise%2520injection.%2520An%2520empirical%2520study%2520on%2520a%2520pick-and-place%2520scenario%2520demonstrates%2520that%2520our%2520approach%2520produces%2520more%2520informative%2520mutants%2520and%2520reduces%2520the%2520number%2520of%2520invalid%2520or%2520equivalent%2520cases%2520compared%2520to%2520conventional%2520operators.%2520Results%2520highlight%2520the%2520potential%2520of%2520mutation%2520testing%2520to%2520enhance%2520test%2520suite%2520quality%2520and%2520contribute%2520to%2520safer%252C%2520more%2520reliable%2520industrial%2520robotic%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mutation%20Testing%20for%20Industrial%20Robotic%20Systems&entry.906535625=Marcela%20Gon%C3%A7alves%20dos%20Santos%20and%20Sylvain%20Hall%C3%A9%20and%20F%C3%A1bio%20Petrillo&entry.1292438233=Industrial%20robotic%20systems%20%28IRS%29%20are%20increasingly%20deployed%20in%20diverse%20environments%2C%20where%20failures%20can%20result%20in%20severe%20accidents%20and%20costly%20downtime.%20Ensuring%20the%20reliability%20of%20the%20software%20controlling%20these%20systems%20is%20therefore%20critical.%20Mutation%20testing%2C%20a%20technique%20widely%20used%20in%20software%20engineering%2C%20evaluates%20the%20effectiveness%20of%20test%20suites%20by%20introducing%20small%20faults%2C%20or%20mutants%2C%20into%20the%20code.%20However%2C%20traditional%20mutation%20operators%20are%20poorly%20suited%20to%20robotic%20programs%2C%20which%20involve%20message-based%20commands%20and%20interactions%20with%20the%20physical%20world.%20This%20paper%20explores%20the%20adaptation%20of%20mutation%20testing%20to%20IRS%20by%20defining%20domain-specific%20mutation%20operators%20that%20capture%20the%20semantics%20of%20robot%20actions%20and%20sensor%20readings.%20We%20propose%20a%20methodology%20for%20generating%20meaningful%20mutants%20at%20the%20level%20of%20high-level%20read%20and%20write%20operations%2C%20including%20movement%2C%20gripper%20actions%2C%20and%20sensor%20noise%20injection.%20An%20empirical%20study%20on%20a%20pick-and-place%20scenario%20demonstrates%20that%20our%20approach%20produces%20more%20informative%20mutants%20and%20reduces%20the%20number%20of%20invalid%20or%20equivalent%20cases%20compared%20to%20conventional%20operators.%20Results%20highlight%20the%20potential%20of%20mutation%20testing%20to%20enhance%20test%20suite%20quality%20and%20contribute%20to%20safer%2C%20more%20reliable%20industrial%20robotic%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.14432v1&entry.124074799=Read"},
{"title": "Generating Streamlining Constraints with Large Language Models", "author": "Florentina Voboril and Vaidyanathan Peruvemba Ramaswamy and Stefan Szeider", "abstract": "Streamlining constraints (or streamliners, for short) narrow the search space, enhancing the speed and feasibility of solving complex constraint satisfaction problems. Traditionally, streamliners were crafted manually or generated through systematically combined atomic constraints with high-effort offline testing. Our approach utilizes the creativity of Large Language Models (LLMs) to propose effective streamliners for problems specified in the MiniZinc constraint programming language and integrates feedback to the LLM with quick empirical tests for validation. Evaluated across seven diverse constraint satisfaction problems, our method achieves substantial runtime reductions. We compare the results to obfuscated and disguised variants of the problem to see whether the results depend on LLM memorization. We also analyze whether longer off-line runs improve the quality of streamliners and whether the LLM can propose good combinations of streamliners.", "link": "http://arxiv.org/abs/2408.10268v3", "date": "2025-11-18", "relevancy": 1.8459, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4634}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Streamlining%20Constraints%20with%20Large%20Language%20Models&body=Title%3A%20Generating%20Streamlining%20Constraints%20with%20Large%20Language%20Models%0AAuthor%3A%20Florentina%20Voboril%20and%20Vaidyanathan%20Peruvemba%20Ramaswamy%20and%20Stefan%20Szeider%0AAbstract%3A%20Streamlining%20constraints%20%28or%20streamliners%2C%20for%20short%29%20narrow%20the%20search%20space%2C%20enhancing%20the%20speed%20and%20feasibility%20of%20solving%20complex%20constraint%20satisfaction%20problems.%20Traditionally%2C%20streamliners%20were%20crafted%20manually%20or%20generated%20through%20systematically%20combined%20atomic%20constraints%20with%20high-effort%20offline%20testing.%20Our%20approach%20utilizes%20the%20creativity%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20propose%20effective%20streamliners%20for%20problems%20specified%20in%20the%20MiniZinc%20constraint%20programming%20language%20and%20integrates%20feedback%20to%20the%20LLM%20with%20quick%20empirical%20tests%20for%20validation.%20Evaluated%20across%20seven%20diverse%20constraint%20satisfaction%20problems%2C%20our%20method%20achieves%20substantial%20runtime%20reductions.%20We%20compare%20the%20results%20to%20obfuscated%20and%20disguised%20variants%20of%20the%20problem%20to%20see%20whether%20the%20results%20depend%20on%20LLM%20memorization.%20We%20also%20analyze%20whether%20longer%20off-line%20runs%20improve%20the%20quality%20of%20streamliners%20and%20whether%20the%20LLM%20can%20propose%20good%20combinations%20of%20streamliners.%0ALink%3A%20http%3A//arxiv.org/abs/2408.10268v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Streamlining%2520Constraints%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DFlorentina%2520Voboril%2520and%2520Vaidyanathan%2520Peruvemba%2520Ramaswamy%2520and%2520Stefan%2520Szeider%26entry.1292438233%3DStreamlining%2520constraints%2520%2528or%2520streamliners%252C%2520for%2520short%2529%2520narrow%2520the%2520search%2520space%252C%2520enhancing%2520the%2520speed%2520and%2520feasibility%2520of%2520solving%2520complex%2520constraint%2520satisfaction%2520problems.%2520Traditionally%252C%2520streamliners%2520were%2520crafted%2520manually%2520or%2520generated%2520through%2520systematically%2520combined%2520atomic%2520constraints%2520with%2520high-effort%2520offline%2520testing.%2520Our%2520approach%2520utilizes%2520the%2520creativity%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520propose%2520effective%2520streamliners%2520for%2520problems%2520specified%2520in%2520the%2520MiniZinc%2520constraint%2520programming%2520language%2520and%2520integrates%2520feedback%2520to%2520the%2520LLM%2520with%2520quick%2520empirical%2520tests%2520for%2520validation.%2520Evaluated%2520across%2520seven%2520diverse%2520constraint%2520satisfaction%2520problems%252C%2520our%2520method%2520achieves%2520substantial%2520runtime%2520reductions.%2520We%2520compare%2520the%2520results%2520to%2520obfuscated%2520and%2520disguised%2520variants%2520of%2520the%2520problem%2520to%2520see%2520whether%2520the%2520results%2520depend%2520on%2520LLM%2520memorization.%2520We%2520also%2520analyze%2520whether%2520longer%2520off-line%2520runs%2520improve%2520the%2520quality%2520of%2520streamliners%2520and%2520whether%2520the%2520LLM%2520can%2520propose%2520good%2520combinations%2520of%2520streamliners.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10268v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Streamlining%20Constraints%20with%20Large%20Language%20Models&entry.906535625=Florentina%20Voboril%20and%20Vaidyanathan%20Peruvemba%20Ramaswamy%20and%20Stefan%20Szeider&entry.1292438233=Streamlining%20constraints%20%28or%20streamliners%2C%20for%20short%29%20narrow%20the%20search%20space%2C%20enhancing%20the%20speed%20and%20feasibility%20of%20solving%20complex%20constraint%20satisfaction%20problems.%20Traditionally%2C%20streamliners%20were%20crafted%20manually%20or%20generated%20through%20systematically%20combined%20atomic%20constraints%20with%20high-effort%20offline%20testing.%20Our%20approach%20utilizes%20the%20creativity%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20propose%20effective%20streamliners%20for%20problems%20specified%20in%20the%20MiniZinc%20constraint%20programming%20language%20and%20integrates%20feedback%20to%20the%20LLM%20with%20quick%20empirical%20tests%20for%20validation.%20Evaluated%20across%20seven%20diverse%20constraint%20satisfaction%20problems%2C%20our%20method%20achieves%20substantial%20runtime%20reductions.%20We%20compare%20the%20results%20to%20obfuscated%20and%20disguised%20variants%20of%20the%20problem%20to%20see%20whether%20the%20results%20depend%20on%20LLM%20memorization.%20We%20also%20analyze%20whether%20longer%20off-line%20runs%20improve%20the%20quality%20of%20streamliners%20and%20whether%20the%20LLM%20can%20propose%20good%20combinations%20of%20streamliners.&entry.1838667208=http%3A//arxiv.org/abs/2408.10268v3&entry.124074799=Read"},
{"title": "SAM-Fed: SAM-Guided Federated Semi-Supervised Learning for Medical Image Segmentation", "author": "Sahar Nasirihaghighi and Negin Ghamsarian and Yiping Li and Marcel Breeuwer and Raphael Sznitman and Klaus Schoeffmann", "abstract": "Medical image segmentation is clinically important, yet data privacy and the cost of expert annotation limit the availability of labeled data. Federated semi-supervised learning (FSSL) offers a solution but faces two challenges: pseudo-label reliability depends on the strength of local models, and client devices often require compact or heterogeneous architectures due to limited computational resources. These constraints reduce the quality and stability of pseudo-labels, while large models, though more accurate, cannot be trained or used for routine inference on client devices. We propose SAM-Fed, a federated semi-supervised framework that leverages a high-capacity segmentation foundation model to guide lightweight clients during training. SAM-Fed combines dual knowledge distillation with an adaptive agreement mechanism to refine pixel-level supervision. Experiments on skin lesion and polyp segmentation across homogeneous and heterogeneous settings show that SAM-Fed consistently outperforms state-of-the-art FSSL methods.", "link": "http://arxiv.org/abs/2511.14302v1", "date": "2025-11-18", "relevancy": 1.9479, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5214}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4846}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM-Fed%3A%20SAM-Guided%20Federated%20Semi-Supervised%20Learning%20for%20Medical%20Image%20Segmentation&body=Title%3A%20SAM-Fed%3A%20SAM-Guided%20Federated%20Semi-Supervised%20Learning%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Sahar%20Nasirihaghighi%20and%20Negin%20Ghamsarian%20and%20Yiping%20Li%20and%20Marcel%20Breeuwer%20and%20Raphael%20Sznitman%20and%20Klaus%20Schoeffmann%0AAbstract%3A%20Medical%20image%20segmentation%20is%20clinically%20important%2C%20yet%20data%20privacy%20and%20the%20cost%20of%20expert%20annotation%20limit%20the%20availability%20of%20labeled%20data.%20Federated%20semi-supervised%20learning%20%28FSSL%29%20offers%20a%20solution%20but%20faces%20two%20challenges%3A%20pseudo-label%20reliability%20depends%20on%20the%20strength%20of%20local%20models%2C%20and%20client%20devices%20often%20require%20compact%20or%20heterogeneous%20architectures%20due%20to%20limited%20computational%20resources.%20These%20constraints%20reduce%20the%20quality%20and%20stability%20of%20pseudo-labels%2C%20while%20large%20models%2C%20though%20more%20accurate%2C%20cannot%20be%20trained%20or%20used%20for%20routine%20inference%20on%20client%20devices.%20We%20propose%20SAM-Fed%2C%20a%20federated%20semi-supervised%20framework%20that%20leverages%20a%20high-capacity%20segmentation%20foundation%20model%20to%20guide%20lightweight%20clients%20during%20training.%20SAM-Fed%20combines%20dual%20knowledge%20distillation%20with%20an%20adaptive%20agreement%20mechanism%20to%20refine%20pixel-level%20supervision.%20Experiments%20on%20skin%20lesion%20and%20polyp%20segmentation%20across%20homogeneous%20and%20heterogeneous%20settings%20show%20that%20SAM-Fed%20consistently%20outperforms%20state-of-the-art%20FSSL%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM-Fed%253A%2520SAM-Guided%2520Federated%2520Semi-Supervised%2520Learning%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DSahar%2520Nasirihaghighi%2520and%2520Negin%2520Ghamsarian%2520and%2520Yiping%2520Li%2520and%2520Marcel%2520Breeuwer%2520and%2520Raphael%2520Sznitman%2520and%2520Klaus%2520Schoeffmann%26entry.1292438233%3DMedical%2520image%2520segmentation%2520is%2520clinically%2520important%252C%2520yet%2520data%2520privacy%2520and%2520the%2520cost%2520of%2520expert%2520annotation%2520limit%2520the%2520availability%2520of%2520labeled%2520data.%2520Federated%2520semi-supervised%2520learning%2520%2528FSSL%2529%2520offers%2520a%2520solution%2520but%2520faces%2520two%2520challenges%253A%2520pseudo-label%2520reliability%2520depends%2520on%2520the%2520strength%2520of%2520local%2520models%252C%2520and%2520client%2520devices%2520often%2520require%2520compact%2520or%2520heterogeneous%2520architectures%2520due%2520to%2520limited%2520computational%2520resources.%2520These%2520constraints%2520reduce%2520the%2520quality%2520and%2520stability%2520of%2520pseudo-labels%252C%2520while%2520large%2520models%252C%2520though%2520more%2520accurate%252C%2520cannot%2520be%2520trained%2520or%2520used%2520for%2520routine%2520inference%2520on%2520client%2520devices.%2520We%2520propose%2520SAM-Fed%252C%2520a%2520federated%2520semi-supervised%2520framework%2520that%2520leverages%2520a%2520high-capacity%2520segmentation%2520foundation%2520model%2520to%2520guide%2520lightweight%2520clients%2520during%2520training.%2520SAM-Fed%2520combines%2520dual%2520knowledge%2520distillation%2520with%2520an%2520adaptive%2520agreement%2520mechanism%2520to%2520refine%2520pixel-level%2520supervision.%2520Experiments%2520on%2520skin%2520lesion%2520and%2520polyp%2520segmentation%2520across%2520homogeneous%2520and%2520heterogeneous%2520settings%2520show%2520that%2520SAM-Fed%2520consistently%2520outperforms%2520state-of-the-art%2520FSSL%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM-Fed%3A%20SAM-Guided%20Federated%20Semi-Supervised%20Learning%20for%20Medical%20Image%20Segmentation&entry.906535625=Sahar%20Nasirihaghighi%20and%20Negin%20Ghamsarian%20and%20Yiping%20Li%20and%20Marcel%20Breeuwer%20and%20Raphael%20Sznitman%20and%20Klaus%20Schoeffmann&entry.1292438233=Medical%20image%20segmentation%20is%20clinically%20important%2C%20yet%20data%20privacy%20and%20the%20cost%20of%20expert%20annotation%20limit%20the%20availability%20of%20labeled%20data.%20Federated%20semi-supervised%20learning%20%28FSSL%29%20offers%20a%20solution%20but%20faces%20two%20challenges%3A%20pseudo-label%20reliability%20depends%20on%20the%20strength%20of%20local%20models%2C%20and%20client%20devices%20often%20require%20compact%20or%20heterogeneous%20architectures%20due%20to%20limited%20computational%20resources.%20These%20constraints%20reduce%20the%20quality%20and%20stability%20of%20pseudo-labels%2C%20while%20large%20models%2C%20though%20more%20accurate%2C%20cannot%20be%20trained%20or%20used%20for%20routine%20inference%20on%20client%20devices.%20We%20propose%20SAM-Fed%2C%20a%20federated%20semi-supervised%20framework%20that%20leverages%20a%20high-capacity%20segmentation%20foundation%20model%20to%20guide%20lightweight%20clients%20during%20training.%20SAM-Fed%20combines%20dual%20knowledge%20distillation%20with%20an%20adaptive%20agreement%20mechanism%20to%20refine%20pixel-level%20supervision.%20Experiments%20on%20skin%20lesion%20and%20polyp%20segmentation%20across%20homogeneous%20and%20heterogeneous%20settings%20show%20that%20SAM-Fed%20consistently%20outperforms%20state-of-the-art%20FSSL%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.14302v1&entry.124074799=Read"},
{"title": "Regularized Schr\u00f6dinger Bridge: Alleviating Distortion and Exposure Bias in Solving Inverse Problems", "author": "Qing Yao and Lijian Gao and Qirong Mao and Dong Ming", "abstract": "Diffusion models serve as a powerful generative framework for solving inverse problems. However, they still face two key challenges: 1) the distortion-perception tradeoff, where improving perceptual quality often degrades reconstruction fidelity, and 2) the exposure bias problem, where the training-inference input mismatch leads to prediction error accumulation and reduced reconstruction quality. In this work, we propose the Regularized Schr\u00f6dinger Bridge (RSB), an adaptation of Schr\u00f6dinger Bridge tailored for inverse problems that addresses the above limitations. RSB employs a novel regularized training strategy that perturbs both the input states and targets, effectively mitigating exposure bias by exposing the model to simulated prediction errors and also alleviating distortion by well-designed interpolation via the posterior mean. Extensive experiments on two typical inverse problems for speech enhancement demonstrate that RSB outperforms state-of-the-art methods, significantly improving distortion metrics and effectively reducing exposure bias.", "link": "http://arxiv.org/abs/2511.11686v2", "date": "2025-11-18", "relevancy": 1.0756, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5744}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5215}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regularized%20Schr%C3%B6dinger%20Bridge%3A%20Alleviating%20Distortion%20and%20Exposure%20Bias%20in%20Solving%20Inverse%20Problems&body=Title%3A%20Regularized%20Schr%C3%B6dinger%20Bridge%3A%20Alleviating%20Distortion%20and%20Exposure%20Bias%20in%20Solving%20Inverse%20Problems%0AAuthor%3A%20Qing%20Yao%20and%20Lijian%20Gao%20and%20Qirong%20Mao%20and%20Dong%20Ming%0AAbstract%3A%20Diffusion%20models%20serve%20as%20a%20powerful%20generative%20framework%20for%20solving%20inverse%20problems.%20However%2C%20they%20still%20face%20two%20key%20challenges%3A%201%29%20the%20distortion-perception%20tradeoff%2C%20where%20improving%20perceptual%20quality%20often%20degrades%20reconstruction%20fidelity%2C%20and%202%29%20the%20exposure%20bias%20problem%2C%20where%20the%20training-inference%20input%20mismatch%20leads%20to%20prediction%20error%20accumulation%20and%20reduced%20reconstruction%20quality.%20In%20this%20work%2C%20we%20propose%20the%20Regularized%20Schr%C3%B6dinger%20Bridge%20%28RSB%29%2C%20an%20adaptation%20of%20Schr%C3%B6dinger%20Bridge%20tailored%20for%20inverse%20problems%20that%20addresses%20the%20above%20limitations.%20RSB%20employs%20a%20novel%20regularized%20training%20strategy%20that%20perturbs%20both%20the%20input%20states%20and%20targets%2C%20effectively%20mitigating%20exposure%20bias%20by%20exposing%20the%20model%20to%20simulated%20prediction%20errors%20and%20also%20alleviating%20distortion%20by%20well-designed%20interpolation%20via%20the%20posterior%20mean.%20Extensive%20experiments%20on%20two%20typical%20inverse%20problems%20for%20speech%20enhancement%20demonstrate%20that%20RSB%20outperforms%20state-of-the-art%20methods%2C%20significantly%20improving%20distortion%20metrics%20and%20effectively%20reducing%20exposure%20bias.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11686v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegularized%2520Schr%25C3%25B6dinger%2520Bridge%253A%2520Alleviating%2520Distortion%2520and%2520Exposure%2520Bias%2520in%2520Solving%2520Inverse%2520Problems%26entry.906535625%3DQing%2520Yao%2520and%2520Lijian%2520Gao%2520and%2520Qirong%2520Mao%2520and%2520Dong%2520Ming%26entry.1292438233%3DDiffusion%2520models%2520serve%2520as%2520a%2520powerful%2520generative%2520framework%2520for%2520solving%2520inverse%2520problems.%2520However%252C%2520they%2520still%2520face%2520two%2520key%2520challenges%253A%25201%2529%2520the%2520distortion-perception%2520tradeoff%252C%2520where%2520improving%2520perceptual%2520quality%2520often%2520degrades%2520reconstruction%2520fidelity%252C%2520and%25202%2529%2520the%2520exposure%2520bias%2520problem%252C%2520where%2520the%2520training-inference%2520input%2520mismatch%2520leads%2520to%2520prediction%2520error%2520accumulation%2520and%2520reduced%2520reconstruction%2520quality.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Regularized%2520Schr%25C3%25B6dinger%2520Bridge%2520%2528RSB%2529%252C%2520an%2520adaptation%2520of%2520Schr%25C3%25B6dinger%2520Bridge%2520tailored%2520for%2520inverse%2520problems%2520that%2520addresses%2520the%2520above%2520limitations.%2520RSB%2520employs%2520a%2520novel%2520regularized%2520training%2520strategy%2520that%2520perturbs%2520both%2520the%2520input%2520states%2520and%2520targets%252C%2520effectively%2520mitigating%2520exposure%2520bias%2520by%2520exposing%2520the%2520model%2520to%2520simulated%2520prediction%2520errors%2520and%2520also%2520alleviating%2520distortion%2520by%2520well-designed%2520interpolation%2520via%2520the%2520posterior%2520mean.%2520Extensive%2520experiments%2520on%2520two%2520typical%2520inverse%2520problems%2520for%2520speech%2520enhancement%2520demonstrate%2520that%2520RSB%2520outperforms%2520state-of-the-art%2520methods%252C%2520significantly%2520improving%2520distortion%2520metrics%2520and%2520effectively%2520reducing%2520exposure%2520bias.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11686v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularized%20Schr%C3%B6dinger%20Bridge%3A%20Alleviating%20Distortion%20and%20Exposure%20Bias%20in%20Solving%20Inverse%20Problems&entry.906535625=Qing%20Yao%20and%20Lijian%20Gao%20and%20Qirong%20Mao%20and%20Dong%20Ming&entry.1292438233=Diffusion%20models%20serve%20as%20a%20powerful%20generative%20framework%20for%20solving%20inverse%20problems.%20However%2C%20they%20still%20face%20two%20key%20challenges%3A%201%29%20the%20distortion-perception%20tradeoff%2C%20where%20improving%20perceptual%20quality%20often%20degrades%20reconstruction%20fidelity%2C%20and%202%29%20the%20exposure%20bias%20problem%2C%20where%20the%20training-inference%20input%20mismatch%20leads%20to%20prediction%20error%20accumulation%20and%20reduced%20reconstruction%20quality.%20In%20this%20work%2C%20we%20propose%20the%20Regularized%20Schr%C3%B6dinger%20Bridge%20%28RSB%29%2C%20an%20adaptation%20of%20Schr%C3%B6dinger%20Bridge%20tailored%20for%20inverse%20problems%20that%20addresses%20the%20above%20limitations.%20RSB%20employs%20a%20novel%20regularized%20training%20strategy%20that%20perturbs%20both%20the%20input%20states%20and%20targets%2C%20effectively%20mitigating%20exposure%20bias%20by%20exposing%20the%20model%20to%20simulated%20prediction%20errors%20and%20also%20alleviating%20distortion%20by%20well-designed%20interpolation%20via%20the%20posterior%20mean.%20Extensive%20experiments%20on%20two%20typical%20inverse%20problems%20for%20speech%20enhancement%20demonstrate%20that%20RSB%20outperforms%20state-of-the-art%20methods%2C%20significantly%20improving%20distortion%20metrics%20and%20effectively%20reducing%20exposure%20bias.&entry.1838667208=http%3A//arxiv.org/abs/2511.11686v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


