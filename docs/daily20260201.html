<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260129.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction", "author": "Changjian Jiang and Kerui Ren and Xudong Li and Kaiwen Song and Linning Xu and Tao Lu and Junting Dong and Yu Zhang and Bo Dai and Mulin Yu", "abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ .", "link": "http://arxiv.org/abs/2601.22046v1", "date": "2026-01-29", "relevancy": 3.352, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7013}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6958}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PLANING%3A%20A%20Loosely%20Coupled%20Triangle-Gaussian%20Framework%20for%20Streaming%203D%20Reconstruction&body=Title%3A%20PLANING%3A%20A%20Loosely%20Coupled%20Triangle-Gaussian%20Framework%20for%20Streaming%203D%20Reconstruction%0AAuthor%3A%20Changjian%20Jiang%20and%20Kerui%20Ren%20and%20Xudong%20Li%20and%20Kaiwen%20Song%20and%20Linning%20Xu%20and%20Tao%20Lu%20and%20Junting%20Dong%20and%20Yu%20Zhang%20and%20Bo%20Dai%20and%20Mulin%20Yu%0AAbstract%3A%20Streaming%20reconstruction%20from%20monocular%20image%20sequences%20remains%20challenging%2C%20as%20existing%20methods%20typically%20favor%20either%20high-quality%20rendering%20or%20accurate%20geometry%2C%20but%20rarely%20both.%20We%20present%20PLANING%2C%20an%20efficient%20on-the-fly%20reconstruction%20framework%20built%20on%20a%20hybrid%20representation%20that%20loosely%20couples%20explicit%20geometric%20primitives%20with%20neural%20Gaussians%2C%20enabling%20geometry%20and%20appearance%20to%20be%20modeled%20in%20a%20decoupled%20manner.%20This%20decoupling%20supports%20an%20online%20initialization%20and%20optimization%20strategy%20that%20separates%20geometry%20and%20appearance%20updates%2C%20yielding%20stable%20streaming%20reconstruction%20with%20substantially%20reduced%20structural%20redundancy.%20PLANING%20improves%20dense%20mesh%20Chamfer-L2%20by%2018.52%25%20over%20PGSR%2C%20surpasses%20ARTDECO%20by%201.31%20dB%20PSNR%2C%20and%20reconstructs%20ScanNetV2%20scenes%20in%20under%20100%20seconds%2C%20over%205x%20faster%20than%202D%20Gaussian%20Splatting%2C%20while%20matching%20the%20quality%20of%20offline%20per-scene%20optimization.%20Beyond%20reconstruction%20quality%2C%20the%20structural%20clarity%20and%20computational%20efficiency%20of%20%5Cmodelname~make%20it%20well%20suited%20for%20a%20broad%20range%20of%20downstream%20applications%2C%20such%20as%20enabling%20large-scale%20scene%20modeling%20and%20simulation-ready%20environments%20for%20embodied%20AI.%20Project%20page%3A%20https%3A//city-super.github.io/PLANING/%20.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPLANING%253A%2520A%2520Loosely%2520Coupled%2520Triangle-Gaussian%2520Framework%2520for%2520Streaming%25203D%2520Reconstruction%26entry.906535625%3DChangjian%2520Jiang%2520and%2520Kerui%2520Ren%2520and%2520Xudong%2520Li%2520and%2520Kaiwen%2520Song%2520and%2520Linning%2520Xu%2520and%2520Tao%2520Lu%2520and%2520Junting%2520Dong%2520and%2520Yu%2520Zhang%2520and%2520Bo%2520Dai%2520and%2520Mulin%2520Yu%26entry.1292438233%3DStreaming%2520reconstruction%2520from%2520monocular%2520image%2520sequences%2520remains%2520challenging%252C%2520as%2520existing%2520methods%2520typically%2520favor%2520either%2520high-quality%2520rendering%2520or%2520accurate%2520geometry%252C%2520but%2520rarely%2520both.%2520We%2520present%2520PLANING%252C%2520an%2520efficient%2520on-the-fly%2520reconstruction%2520framework%2520built%2520on%2520a%2520hybrid%2520representation%2520that%2520loosely%2520couples%2520explicit%2520geometric%2520primitives%2520with%2520neural%2520Gaussians%252C%2520enabling%2520geometry%2520and%2520appearance%2520to%2520be%2520modeled%2520in%2520a%2520decoupled%2520manner.%2520This%2520decoupling%2520supports%2520an%2520online%2520initialization%2520and%2520optimization%2520strategy%2520that%2520separates%2520geometry%2520and%2520appearance%2520updates%252C%2520yielding%2520stable%2520streaming%2520reconstruction%2520with%2520substantially%2520reduced%2520structural%2520redundancy.%2520PLANING%2520improves%2520dense%2520mesh%2520Chamfer-L2%2520by%252018.52%2525%2520over%2520PGSR%252C%2520surpasses%2520ARTDECO%2520by%25201.31%2520dB%2520PSNR%252C%2520and%2520reconstructs%2520ScanNetV2%2520scenes%2520in%2520under%2520100%2520seconds%252C%2520over%25205x%2520faster%2520than%25202D%2520Gaussian%2520Splatting%252C%2520while%2520matching%2520the%2520quality%2520of%2520offline%2520per-scene%2520optimization.%2520Beyond%2520reconstruction%2520quality%252C%2520the%2520structural%2520clarity%2520and%2520computational%2520efficiency%2520of%2520%255Cmodelname~make%2520it%2520well%2520suited%2520for%2520a%2520broad%2520range%2520of%2520downstream%2520applications%252C%2520such%2520as%2520enabling%2520large-scale%2520scene%2520modeling%2520and%2520simulation-ready%2520environments%2520for%2520embodied%2520AI.%2520Project%2520page%253A%2520https%253A//city-super.github.io/PLANING/%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLANING%3A%20A%20Loosely%20Coupled%20Triangle-Gaussian%20Framework%20for%20Streaming%203D%20Reconstruction&entry.906535625=Changjian%20Jiang%20and%20Kerui%20Ren%20and%20Xudong%20Li%20and%20Kaiwen%20Song%20and%20Linning%20Xu%20and%20Tao%20Lu%20and%20Junting%20Dong%20and%20Yu%20Zhang%20and%20Bo%20Dai%20and%20Mulin%20Yu&entry.1292438233=Streaming%20reconstruction%20from%20monocular%20image%20sequences%20remains%20challenging%2C%20as%20existing%20methods%20typically%20favor%20either%20high-quality%20rendering%20or%20accurate%20geometry%2C%20but%20rarely%20both.%20We%20present%20PLANING%2C%20an%20efficient%20on-the-fly%20reconstruction%20framework%20built%20on%20a%20hybrid%20representation%20that%20loosely%20couples%20explicit%20geometric%20primitives%20with%20neural%20Gaussians%2C%20enabling%20geometry%20and%20appearance%20to%20be%20modeled%20in%20a%20decoupled%20manner.%20This%20decoupling%20supports%20an%20online%20initialization%20and%20optimization%20strategy%20that%20separates%20geometry%20and%20appearance%20updates%2C%20yielding%20stable%20streaming%20reconstruction%20with%20substantially%20reduced%20structural%20redundancy.%20PLANING%20improves%20dense%20mesh%20Chamfer-L2%20by%2018.52%25%20over%20PGSR%2C%20surpasses%20ARTDECO%20by%201.31%20dB%20PSNR%2C%20and%20reconstructs%20ScanNetV2%20scenes%20in%20under%20100%20seconds%2C%20over%205x%20faster%20than%202D%20Gaussian%20Splatting%2C%20while%20matching%20the%20quality%20of%20offline%20per-scene%20optimization.%20Beyond%20reconstruction%20quality%2C%20the%20structural%20clarity%20and%20computational%20efficiency%20of%20%5Cmodelname~make%20it%20well%20suited%20for%20a%20broad%20range%20of%20downstream%20applications%2C%20such%20as%20enabling%20large-scale%20scene%20modeling%20and%20simulation-ready%20environments%20for%20embodied%20AI.%20Project%20page%3A%20https%3A//city-super.github.io/PLANING/%20.&entry.1838667208=http%3A//arxiv.org/abs/2601.22046v1&entry.124074799=Read"},
{"title": "Efficient4D: Fast Dynamic 3D Object Generation from a Single-view Video", "author": "Zijie Pan and Zeyu Yang and Xiatian Zhu and Li Zhang", "abstract": "Generating dynamic 3D object from a single-view video is challenging due to the lack of 4D labeled data. An intuitive approach is to extend previous image-to-3D pipelines by transferring off-the-shelf image generation models such as score distillation sampling.However, this approach would be slow and expensive to scale due to the need for back-propagating the information-limited supervision signals through a large pretrained model. To address this, we propose an efficient video-to-4D object generation framework called Efficient4D. It generates high-quality spacetime-consistent images under different camera views, and then uses them as labeled data to directly reconstruct the 4D content through a 4D Gaussian splatting model. Importantly, our method can achieve real-time rendering under continuous camera trajectories. To enable robust reconstruction under sparse views, we introduce inconsistency-aware confidence-weighted loss design, along with a lightly weighted score distillation loss. Extensive experiments on both synthetic and real videos show that Efficient4D offers a remarkable 10-fold increase in speed when compared to prior art alternatives while preserving the quality of novel view synthesis. For example, Efficient4D takes only 10 minutes to model a dynamic object, vs 120 minutes by the previous art model Consistent4D.", "link": "http://arxiv.org/abs/2401.08742v5", "date": "2026-01-29", "relevancy": 3.3141, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6632}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6632}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient4D%3A%20Fast%20Dynamic%203D%20Object%20Generation%20from%20a%20Single-view%20Video&body=Title%3A%20Efficient4D%3A%20Fast%20Dynamic%203D%20Object%20Generation%20from%20a%20Single-view%20Video%0AAuthor%3A%20Zijie%20Pan%20and%20Zeyu%20Yang%20and%20Xiatian%20Zhu%20and%20Li%20Zhang%0AAbstract%3A%20Generating%20dynamic%203D%20object%20from%20a%20single-view%20video%20is%20challenging%20due%20to%20the%20lack%20of%204D%20labeled%20data.%20An%20intuitive%20approach%20is%20to%20extend%20previous%20image-to-3D%20pipelines%20by%20transferring%20off-the-shelf%20image%20generation%20models%20such%20as%20score%20distillation%20sampling.However%2C%20this%20approach%20would%20be%20slow%20and%20expensive%20to%20scale%20due%20to%20the%20need%20for%20back-propagating%20the%20information-limited%20supervision%20signals%20through%20a%20large%20pretrained%20model.%20To%20address%20this%2C%20we%20propose%20an%20efficient%20video-to-4D%20object%20generation%20framework%20called%20Efficient4D.%20It%20generates%20high-quality%20spacetime-consistent%20images%20under%20different%20camera%20views%2C%20and%20then%20uses%20them%20as%20labeled%20data%20to%20directly%20reconstruct%20the%204D%20content%20through%20a%204D%20Gaussian%20splatting%20model.%20Importantly%2C%20our%20method%20can%20achieve%20real-time%20rendering%20under%20continuous%20camera%20trajectories.%20To%20enable%20robust%20reconstruction%20under%20sparse%20views%2C%20we%20introduce%20inconsistency-aware%20confidence-weighted%20loss%20design%2C%20along%20with%20a%20lightly%20weighted%20score%20distillation%20loss.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real%20videos%20show%20that%20Efficient4D%20offers%20a%20remarkable%2010-fold%20increase%20in%20speed%20when%20compared%20to%20prior%20art%20alternatives%20while%20preserving%20the%20quality%20of%20novel%20view%20synthesis.%20For%20example%2C%20Efficient4D%20takes%20only%2010%20minutes%20to%20model%20a%20dynamic%20object%2C%20vs%20120%20minutes%20by%20the%20previous%20art%20model%20Consistent4D.%0ALink%3A%20http%3A//arxiv.org/abs/2401.08742v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient4D%253A%2520Fast%2520Dynamic%25203D%2520Object%2520Generation%2520from%2520a%2520Single-view%2520Video%26entry.906535625%3DZijie%2520Pan%2520and%2520Zeyu%2520Yang%2520and%2520Xiatian%2520Zhu%2520and%2520Li%2520Zhang%26entry.1292438233%3DGenerating%2520dynamic%25203D%2520object%2520from%2520a%2520single-view%2520video%2520is%2520challenging%2520due%2520to%2520the%2520lack%2520of%25204D%2520labeled%2520data.%2520An%2520intuitive%2520approach%2520is%2520to%2520extend%2520previous%2520image-to-3D%2520pipelines%2520by%2520transferring%2520off-the-shelf%2520image%2520generation%2520models%2520such%2520as%2520score%2520distillation%2520sampling.However%252C%2520this%2520approach%2520would%2520be%2520slow%2520and%2520expensive%2520to%2520scale%2520due%2520to%2520the%2520need%2520for%2520back-propagating%2520the%2520information-limited%2520supervision%2520signals%2520through%2520a%2520large%2520pretrained%2520model.%2520To%2520address%2520this%252C%2520we%2520propose%2520an%2520efficient%2520video-to-4D%2520object%2520generation%2520framework%2520called%2520Efficient4D.%2520It%2520generates%2520high-quality%2520spacetime-consistent%2520images%2520under%2520different%2520camera%2520views%252C%2520and%2520then%2520uses%2520them%2520as%2520labeled%2520data%2520to%2520directly%2520reconstruct%2520the%25204D%2520content%2520through%2520a%25204D%2520Gaussian%2520splatting%2520model.%2520Importantly%252C%2520our%2520method%2520can%2520achieve%2520real-time%2520rendering%2520under%2520continuous%2520camera%2520trajectories.%2520To%2520enable%2520robust%2520reconstruction%2520under%2520sparse%2520views%252C%2520we%2520introduce%2520inconsistency-aware%2520confidence-weighted%2520loss%2520design%252C%2520along%2520with%2520a%2520lightly%2520weighted%2520score%2520distillation%2520loss.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real%2520videos%2520show%2520that%2520Efficient4D%2520offers%2520a%2520remarkable%252010-fold%2520increase%2520in%2520speed%2520when%2520compared%2520to%2520prior%2520art%2520alternatives%2520while%2520preserving%2520the%2520quality%2520of%2520novel%2520view%2520synthesis.%2520For%2520example%252C%2520Efficient4D%2520takes%2520only%252010%2520minutes%2520to%2520model%2520a%2520dynamic%2520object%252C%2520vs%2520120%2520minutes%2520by%2520the%2520previous%2520art%2520model%2520Consistent4D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.08742v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient4D%3A%20Fast%20Dynamic%203D%20Object%20Generation%20from%20a%20Single-view%20Video&entry.906535625=Zijie%20Pan%20and%20Zeyu%20Yang%20and%20Xiatian%20Zhu%20and%20Li%20Zhang&entry.1292438233=Generating%20dynamic%203D%20object%20from%20a%20single-view%20video%20is%20challenging%20due%20to%20the%20lack%20of%204D%20labeled%20data.%20An%20intuitive%20approach%20is%20to%20extend%20previous%20image-to-3D%20pipelines%20by%20transferring%20off-the-shelf%20image%20generation%20models%20such%20as%20score%20distillation%20sampling.However%2C%20this%20approach%20would%20be%20slow%20and%20expensive%20to%20scale%20due%20to%20the%20need%20for%20back-propagating%20the%20information-limited%20supervision%20signals%20through%20a%20large%20pretrained%20model.%20To%20address%20this%2C%20we%20propose%20an%20efficient%20video-to-4D%20object%20generation%20framework%20called%20Efficient4D.%20It%20generates%20high-quality%20spacetime-consistent%20images%20under%20different%20camera%20views%2C%20and%20then%20uses%20them%20as%20labeled%20data%20to%20directly%20reconstruct%20the%204D%20content%20through%20a%204D%20Gaussian%20splatting%20model.%20Importantly%2C%20our%20method%20can%20achieve%20real-time%20rendering%20under%20continuous%20camera%20trajectories.%20To%20enable%20robust%20reconstruction%20under%20sparse%20views%2C%20we%20introduce%20inconsistency-aware%20confidence-weighted%20loss%20design%2C%20along%20with%20a%20lightly%20weighted%20score%20distillation%20loss.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real%20videos%20show%20that%20Efficient4D%20offers%20a%20remarkable%2010-fold%20increase%20in%20speed%20when%20compared%20to%20prior%20art%20alternatives%20while%20preserving%20the%20quality%20of%20novel%20view%20synthesis.%20For%20example%2C%20Efficient4D%20takes%20only%2010%20minutes%20to%20model%20a%20dynamic%20object%2C%20vs%20120%20minutes%20by%20the%20previous%20art%20model%20Consistent4D.&entry.1838667208=http%3A//arxiv.org/abs/2401.08742v5&entry.124074799=Read"},
{"title": "DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning", "author": "Mingshuang Luo and Shuang Liang and Zhengkun Rong and Yuxuan Luo and Tianshu Hu and Ruibing Hou and Hong Chang and Yong Li and Yuan Zhang and Mingyuan Gao", "abstract": "Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a \"see-saw\", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: https://grisoon.github.io/DreamActor-M2/", "link": "http://arxiv.org/abs/2601.21716v1", "date": "2026-01-29", "relevancy": 3.1577, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6637}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6597}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamActor-M2%3A%20Universal%20Character%20Image%20Animation%20via%20Spatiotemporal%20In-Context%20Learning&body=Title%3A%20DreamActor-M2%3A%20Universal%20Character%20Image%20Animation%20via%20Spatiotemporal%20In-Context%20Learning%0AAuthor%3A%20Mingshuang%20Luo%20and%20Shuang%20Liang%20and%20Zhengkun%20Rong%20and%20Yuxuan%20Luo%20and%20Tianshu%20Hu%20and%20Ruibing%20Hou%20and%20Hong%20Chang%20and%20Yong%20Li%20and%20Yuan%20Zhang%20and%20Mingyuan%20Gao%0AAbstract%3A%20Character%20image%20animation%20aims%20to%20synthesize%20high-fidelity%20videos%20by%20transferring%20motion%20from%20a%20driving%20sequence%20to%20a%20static%20reference%20image.%20Despite%20recent%20advancements%2C%20existing%20methods%20suffer%20from%20two%20fundamental%20challenges%3A%20%281%29%20suboptimal%20motion%20injection%20strategies%20that%20lead%20to%20a%20trade-off%20between%20identity%20preservation%20and%20motion%20consistency%2C%20manifesting%20as%20a%20%22see-saw%22%2C%20and%20%282%29%20an%20over-reliance%20on%20explicit%20pose%20priors%20%28e.g.%2C%20skeletons%29%2C%20which%20inadequately%20capture%20intricate%20dynamics%20and%20hinder%20generalization%20to%20arbitrary%2C%20non-humanoid%20characters.%20To%20address%20these%20challenges%2C%20we%20present%20DreamActor-M2%2C%20a%20universal%20animation%20framework%20that%20reimagines%20motion%20conditioning%20as%20an%20in-context%20learning%20problem.%20Our%20approach%20follows%20a%20two-stage%20paradigm.%20First%2C%20we%20bridge%20the%20input%20modality%20gap%20by%20fusing%20reference%20appearance%20and%20motion%20cues%20into%20a%20unified%20latent%20space%2C%20enabling%20the%20model%20to%20jointly%20reason%20about%20spatial%20identity%20and%20temporal%20dynamics%20by%20leveraging%20the%20generative%20prior%20of%20foundational%20models.%20Second%2C%20we%20introduce%20a%20self-bootstrapped%20data%20synthesis%20pipeline%20that%20curates%20pseudo%20cross-identity%20training%20pairs%2C%20facilitating%20a%20seamless%20transition%20from%20pose-dependent%20control%20to%20direct%2C%20end-to-end%20RGB-driven%20animation.%20This%20strategy%20significantly%20enhances%20generalization%20across%20diverse%20characters%20and%20motion%20scenarios.%20To%20facilitate%20comprehensive%20evaluation%2C%20we%20further%20introduce%20AW%20Bench%2C%20a%20versatile%20benchmark%20encompassing%20a%20wide%20spectrum%20of%20characters%20types%20and%20motion%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20DreamActor-M2%20achieves%20state-of-the-art%20performance%2C%20delivering%20superior%20visual%20fidelity%20and%20robust%20cross-domain%20generalization.%20Project%20Page%3A%20https%3A//grisoon.github.io/DreamActor-M2/%0ALink%3A%20http%3A//arxiv.org/abs/2601.21716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamActor-M2%253A%2520Universal%2520Character%2520Image%2520Animation%2520via%2520Spatiotemporal%2520In-Context%2520Learning%26entry.906535625%3DMingshuang%2520Luo%2520and%2520Shuang%2520Liang%2520and%2520Zhengkun%2520Rong%2520and%2520Yuxuan%2520Luo%2520and%2520Tianshu%2520Hu%2520and%2520Ruibing%2520Hou%2520and%2520Hong%2520Chang%2520and%2520Yong%2520Li%2520and%2520Yuan%2520Zhang%2520and%2520Mingyuan%2520Gao%26entry.1292438233%3DCharacter%2520image%2520animation%2520aims%2520to%2520synthesize%2520high-fidelity%2520videos%2520by%2520transferring%2520motion%2520from%2520a%2520driving%2520sequence%2520to%2520a%2520static%2520reference%2520image.%2520Despite%2520recent%2520advancements%252C%2520existing%2520methods%2520suffer%2520from%2520two%2520fundamental%2520challenges%253A%2520%25281%2529%2520suboptimal%2520motion%2520injection%2520strategies%2520that%2520lead%2520to%2520a%2520trade-off%2520between%2520identity%2520preservation%2520and%2520motion%2520consistency%252C%2520manifesting%2520as%2520a%2520%2522see-saw%2522%252C%2520and%2520%25282%2529%2520an%2520over-reliance%2520on%2520explicit%2520pose%2520priors%2520%2528e.g.%252C%2520skeletons%2529%252C%2520which%2520inadequately%2520capture%2520intricate%2520dynamics%2520and%2520hinder%2520generalization%2520to%2520arbitrary%252C%2520non-humanoid%2520characters.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520DreamActor-M2%252C%2520a%2520universal%2520animation%2520framework%2520that%2520reimagines%2520motion%2520conditioning%2520as%2520an%2520in-context%2520learning%2520problem.%2520Our%2520approach%2520follows%2520a%2520two-stage%2520paradigm.%2520First%252C%2520we%2520bridge%2520the%2520input%2520modality%2520gap%2520by%2520fusing%2520reference%2520appearance%2520and%2520motion%2520cues%2520into%2520a%2520unified%2520latent%2520space%252C%2520enabling%2520the%2520model%2520to%2520jointly%2520reason%2520about%2520spatial%2520identity%2520and%2520temporal%2520dynamics%2520by%2520leveraging%2520the%2520generative%2520prior%2520of%2520foundational%2520models.%2520Second%252C%2520we%2520introduce%2520a%2520self-bootstrapped%2520data%2520synthesis%2520pipeline%2520that%2520curates%2520pseudo%2520cross-identity%2520training%2520pairs%252C%2520facilitating%2520a%2520seamless%2520transition%2520from%2520pose-dependent%2520control%2520to%2520direct%252C%2520end-to-end%2520RGB-driven%2520animation.%2520This%2520strategy%2520significantly%2520enhances%2520generalization%2520across%2520diverse%2520characters%2520and%2520motion%2520scenarios.%2520To%2520facilitate%2520comprehensive%2520evaluation%252C%2520we%2520further%2520introduce%2520AW%2520Bench%252C%2520a%2520versatile%2520benchmark%2520encompassing%2520a%2520wide%2520spectrum%2520of%2520characters%2520types%2520and%2520motion%2520scenarios.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DreamActor-M2%2520achieves%2520state-of-the-art%2520performance%252C%2520delivering%2520superior%2520visual%2520fidelity%2520and%2520robust%2520cross-domain%2520generalization.%2520Project%2520Page%253A%2520https%253A//grisoon.github.io/DreamActor-M2/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamActor-M2%3A%20Universal%20Character%20Image%20Animation%20via%20Spatiotemporal%20In-Context%20Learning&entry.906535625=Mingshuang%20Luo%20and%20Shuang%20Liang%20and%20Zhengkun%20Rong%20and%20Yuxuan%20Luo%20and%20Tianshu%20Hu%20and%20Ruibing%20Hou%20and%20Hong%20Chang%20and%20Yong%20Li%20and%20Yuan%20Zhang%20and%20Mingyuan%20Gao&entry.1292438233=Character%20image%20animation%20aims%20to%20synthesize%20high-fidelity%20videos%20by%20transferring%20motion%20from%20a%20driving%20sequence%20to%20a%20static%20reference%20image.%20Despite%20recent%20advancements%2C%20existing%20methods%20suffer%20from%20two%20fundamental%20challenges%3A%20%281%29%20suboptimal%20motion%20injection%20strategies%20that%20lead%20to%20a%20trade-off%20between%20identity%20preservation%20and%20motion%20consistency%2C%20manifesting%20as%20a%20%22see-saw%22%2C%20and%20%282%29%20an%20over-reliance%20on%20explicit%20pose%20priors%20%28e.g.%2C%20skeletons%29%2C%20which%20inadequately%20capture%20intricate%20dynamics%20and%20hinder%20generalization%20to%20arbitrary%2C%20non-humanoid%20characters.%20To%20address%20these%20challenges%2C%20we%20present%20DreamActor-M2%2C%20a%20universal%20animation%20framework%20that%20reimagines%20motion%20conditioning%20as%20an%20in-context%20learning%20problem.%20Our%20approach%20follows%20a%20two-stage%20paradigm.%20First%2C%20we%20bridge%20the%20input%20modality%20gap%20by%20fusing%20reference%20appearance%20and%20motion%20cues%20into%20a%20unified%20latent%20space%2C%20enabling%20the%20model%20to%20jointly%20reason%20about%20spatial%20identity%20and%20temporal%20dynamics%20by%20leveraging%20the%20generative%20prior%20of%20foundational%20models.%20Second%2C%20we%20introduce%20a%20self-bootstrapped%20data%20synthesis%20pipeline%20that%20curates%20pseudo%20cross-identity%20training%20pairs%2C%20facilitating%20a%20seamless%20transition%20from%20pose-dependent%20control%20to%20direct%2C%20end-to-end%20RGB-driven%20animation.%20This%20strategy%20significantly%20enhances%20generalization%20across%20diverse%20characters%20and%20motion%20scenarios.%20To%20facilitate%20comprehensive%20evaluation%2C%20we%20further%20introduce%20AW%20Bench%2C%20a%20versatile%20benchmark%20encompassing%20a%20wide%20spectrum%20of%20characters%20types%20and%20motion%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20DreamActor-M2%20achieves%20state-of-the-art%20performance%2C%20delivering%20superior%20visual%20fidelity%20and%20robust%20cross-domain%20generalization.%20Project%20Page%3A%20https%3A//grisoon.github.io/DreamActor-M2/&entry.1838667208=http%3A//arxiv.org/abs/2601.21716v1&entry.124074799=Read"},
{"title": "Synthetic-to-Real Domain Bridging for Single-View 3D Reconstruction of Ships for Maritime Monitoring", "author": "Borja Carrillo-Perez and Felix Sattler and Angel Bueno Rodriguez and Maurice Stephan and Sarah Barnes", "abstract": "Three-dimensional (3D) reconstruction of ships is an important part of maritime monitoring, allowing improved visualization, inspection, and decision-making in real-world monitoring environments. However, most state-ofthe-art 3D reconstruction methods require multi-view supervision, annotated 3D ground truth, or are computationally intensive, making them impractical for real-time maritime deployment. In this work, we present an efficient pipeline for single-view 3D reconstruction of real ships by training entirely on synthetic data and requiring only a single view at inference. Our approach uses the Splatter Image network, which represents objects as sparse sets of 3D Gaussians for rapid and accurate reconstruction from single images. The model is first fine-tuned on synthetic ShapeNet vessels and further refined with a diverse custom dataset of 3D ships, bridging the domain gap between synthetic and real-world imagery. We integrate a state-of-the-art segmentation module based on YOLOv8 and custom preprocessing to ensure compatibility with the reconstruction network. Postprocessing steps include real-world scaling, centering, and orientation alignment, followed by georeferenced placement on an interactive web map using AIS metadata and homography-based mapping. Quantitative evaluation on synthetic validation data demonstrates strong reconstruction fidelity, while qualitative results on real maritime images from the ShipSG dataset confirm the potential for transfer to operational maritime settings. The final system provides interactive 3D inspection of real ships without requiring real-world 3D annotations. This pipeline provides an efficient, scalable solution for maritime monitoring and highlights a path toward real-time 3D ship visualization in practical applications. Interactive demo: https://dlr-mi.github.io/ship3d-demo/.", "link": "http://arxiv.org/abs/2601.21786v1", "date": "2026-01-29", "relevancy": 3.0875, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.62}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6162}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic-to-Real%20Domain%20Bridging%20for%20Single-View%203D%20Reconstruction%20of%20Ships%20for%20Maritime%20Monitoring&body=Title%3A%20Synthetic-to-Real%20Domain%20Bridging%20for%20Single-View%203D%20Reconstruction%20of%20Ships%20for%20Maritime%20Monitoring%0AAuthor%3A%20Borja%20Carrillo-Perez%20and%20Felix%20Sattler%20and%20Angel%20Bueno%20Rodriguez%20and%20Maurice%20Stephan%20and%20Sarah%20Barnes%0AAbstract%3A%20Three-dimensional%20%283D%29%20reconstruction%20of%20ships%20is%20an%20important%20part%20of%20maritime%20monitoring%2C%20allowing%20improved%20visualization%2C%20inspection%2C%20and%20decision-making%20in%20real-world%20monitoring%20environments.%20However%2C%20most%20state-ofthe-art%203D%20reconstruction%20methods%20require%20multi-view%20supervision%2C%20annotated%203D%20ground%20truth%2C%20or%20are%20computationally%20intensive%2C%20making%20them%20impractical%20for%20real-time%20maritime%20deployment.%20In%20this%20work%2C%20we%20present%20an%20efficient%20pipeline%20for%20single-view%203D%20reconstruction%20of%20real%20ships%20by%20training%20entirely%20on%20synthetic%20data%20and%20requiring%20only%20a%20single%20view%20at%20inference.%20Our%20approach%20uses%20the%20Splatter%20Image%20network%2C%20which%20represents%20objects%20as%20sparse%20sets%20of%203D%20Gaussians%20for%20rapid%20and%20accurate%20reconstruction%20from%20single%20images.%20The%20model%20is%20first%20fine-tuned%20on%20synthetic%20ShapeNet%20vessels%20and%20further%20refined%20with%20a%20diverse%20custom%20dataset%20of%203D%20ships%2C%20bridging%20the%20domain%20gap%20between%20synthetic%20and%20real-world%20imagery.%20We%20integrate%20a%20state-of-the-art%20segmentation%20module%20based%20on%20YOLOv8%20and%20custom%20preprocessing%20to%20ensure%20compatibility%20with%20the%20reconstruction%20network.%20Postprocessing%20steps%20include%20real-world%20scaling%2C%20centering%2C%20and%20orientation%20alignment%2C%20followed%20by%20georeferenced%20placement%20on%20an%20interactive%20web%20map%20using%20AIS%20metadata%20and%20homography-based%20mapping.%20Quantitative%20evaluation%20on%20synthetic%20validation%20data%20demonstrates%20strong%20reconstruction%20fidelity%2C%20while%20qualitative%20results%20on%20real%20maritime%20images%20from%20the%20ShipSG%20dataset%20confirm%20the%20potential%20for%20transfer%20to%20operational%20maritime%20settings.%20The%20final%20system%20provides%20interactive%203D%20inspection%20of%20real%20ships%20without%20requiring%20real-world%203D%20annotations.%20This%20pipeline%20provides%20an%20efficient%2C%20scalable%20solution%20for%20maritime%20monitoring%20and%20highlights%20a%20path%20toward%20real-time%203D%20ship%20visualization%20in%20practical%20applications.%20Interactive%20demo%3A%20https%3A//dlr-mi.github.io/ship3d-demo/.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic-to-Real%2520Domain%2520Bridging%2520for%2520Single-View%25203D%2520Reconstruction%2520of%2520Ships%2520for%2520Maritime%2520Monitoring%26entry.906535625%3DBorja%2520Carrillo-Perez%2520and%2520Felix%2520Sattler%2520and%2520Angel%2520Bueno%2520Rodriguez%2520and%2520Maurice%2520Stephan%2520and%2520Sarah%2520Barnes%26entry.1292438233%3DThree-dimensional%2520%25283D%2529%2520reconstruction%2520of%2520ships%2520is%2520an%2520important%2520part%2520of%2520maritime%2520monitoring%252C%2520allowing%2520improved%2520visualization%252C%2520inspection%252C%2520and%2520decision-making%2520in%2520real-world%2520monitoring%2520environments.%2520However%252C%2520most%2520state-ofthe-art%25203D%2520reconstruction%2520methods%2520require%2520multi-view%2520supervision%252C%2520annotated%25203D%2520ground%2520truth%252C%2520or%2520are%2520computationally%2520intensive%252C%2520making%2520them%2520impractical%2520for%2520real-time%2520maritime%2520deployment.%2520In%2520this%2520work%252C%2520we%2520present%2520an%2520efficient%2520pipeline%2520for%2520single-view%25203D%2520reconstruction%2520of%2520real%2520ships%2520by%2520training%2520entirely%2520on%2520synthetic%2520data%2520and%2520requiring%2520only%2520a%2520single%2520view%2520at%2520inference.%2520Our%2520approach%2520uses%2520the%2520Splatter%2520Image%2520network%252C%2520which%2520represents%2520objects%2520as%2520sparse%2520sets%2520of%25203D%2520Gaussians%2520for%2520rapid%2520and%2520accurate%2520reconstruction%2520from%2520single%2520images.%2520The%2520model%2520is%2520first%2520fine-tuned%2520on%2520synthetic%2520ShapeNet%2520vessels%2520and%2520further%2520refined%2520with%2520a%2520diverse%2520custom%2520dataset%2520of%25203D%2520ships%252C%2520bridging%2520the%2520domain%2520gap%2520between%2520synthetic%2520and%2520real-world%2520imagery.%2520We%2520integrate%2520a%2520state-of-the-art%2520segmentation%2520module%2520based%2520on%2520YOLOv8%2520and%2520custom%2520preprocessing%2520to%2520ensure%2520compatibility%2520with%2520the%2520reconstruction%2520network.%2520Postprocessing%2520steps%2520include%2520real-world%2520scaling%252C%2520centering%252C%2520and%2520orientation%2520alignment%252C%2520followed%2520by%2520georeferenced%2520placement%2520on%2520an%2520interactive%2520web%2520map%2520using%2520AIS%2520metadata%2520and%2520homography-based%2520mapping.%2520Quantitative%2520evaluation%2520on%2520synthetic%2520validation%2520data%2520demonstrates%2520strong%2520reconstruction%2520fidelity%252C%2520while%2520qualitative%2520results%2520on%2520real%2520maritime%2520images%2520from%2520the%2520ShipSG%2520dataset%2520confirm%2520the%2520potential%2520for%2520transfer%2520to%2520operational%2520maritime%2520settings.%2520The%2520final%2520system%2520provides%2520interactive%25203D%2520inspection%2520of%2520real%2520ships%2520without%2520requiring%2520real-world%25203D%2520annotations.%2520This%2520pipeline%2520provides%2520an%2520efficient%252C%2520scalable%2520solution%2520for%2520maritime%2520monitoring%2520and%2520highlights%2520a%2520path%2520toward%2520real-time%25203D%2520ship%2520visualization%2520in%2520practical%2520applications.%2520Interactive%2520demo%253A%2520https%253A//dlr-mi.github.io/ship3d-demo/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic-to-Real%20Domain%20Bridging%20for%20Single-View%203D%20Reconstruction%20of%20Ships%20for%20Maritime%20Monitoring&entry.906535625=Borja%20Carrillo-Perez%20and%20Felix%20Sattler%20and%20Angel%20Bueno%20Rodriguez%20and%20Maurice%20Stephan%20and%20Sarah%20Barnes&entry.1292438233=Three-dimensional%20%283D%29%20reconstruction%20of%20ships%20is%20an%20important%20part%20of%20maritime%20monitoring%2C%20allowing%20improved%20visualization%2C%20inspection%2C%20and%20decision-making%20in%20real-world%20monitoring%20environments.%20However%2C%20most%20state-ofthe-art%203D%20reconstruction%20methods%20require%20multi-view%20supervision%2C%20annotated%203D%20ground%20truth%2C%20or%20are%20computationally%20intensive%2C%20making%20them%20impractical%20for%20real-time%20maritime%20deployment.%20In%20this%20work%2C%20we%20present%20an%20efficient%20pipeline%20for%20single-view%203D%20reconstruction%20of%20real%20ships%20by%20training%20entirely%20on%20synthetic%20data%20and%20requiring%20only%20a%20single%20view%20at%20inference.%20Our%20approach%20uses%20the%20Splatter%20Image%20network%2C%20which%20represents%20objects%20as%20sparse%20sets%20of%203D%20Gaussians%20for%20rapid%20and%20accurate%20reconstruction%20from%20single%20images.%20The%20model%20is%20first%20fine-tuned%20on%20synthetic%20ShapeNet%20vessels%20and%20further%20refined%20with%20a%20diverse%20custom%20dataset%20of%203D%20ships%2C%20bridging%20the%20domain%20gap%20between%20synthetic%20and%20real-world%20imagery.%20We%20integrate%20a%20state-of-the-art%20segmentation%20module%20based%20on%20YOLOv8%20and%20custom%20preprocessing%20to%20ensure%20compatibility%20with%20the%20reconstruction%20network.%20Postprocessing%20steps%20include%20real-world%20scaling%2C%20centering%2C%20and%20orientation%20alignment%2C%20followed%20by%20georeferenced%20placement%20on%20an%20interactive%20web%20map%20using%20AIS%20metadata%20and%20homography-based%20mapping.%20Quantitative%20evaluation%20on%20synthetic%20validation%20data%20demonstrates%20strong%20reconstruction%20fidelity%2C%20while%20qualitative%20results%20on%20real%20maritime%20images%20from%20the%20ShipSG%20dataset%20confirm%20the%20potential%20for%20transfer%20to%20operational%20maritime%20settings.%20The%20final%20system%20provides%20interactive%203D%20inspection%20of%20real%20ships%20without%20requiring%20real-world%203D%20annotations.%20This%20pipeline%20provides%20an%20efficient%2C%20scalable%20solution%20for%20maritime%20monitoring%20and%20highlights%20a%20path%20toward%20real-time%203D%20ship%20visualization%20in%20practical%20applications.%20Interactive%20demo%3A%20https%3A//dlr-mi.github.io/ship3d-demo/.&entry.1838667208=http%3A//arxiv.org/abs/2601.21786v1&entry.124074799=Read"},
{"title": "Hybrid Foveated Path Tracing with Peripheral Gaussians for Immersive Anatomy", "author": "Constantin Kleinbeck and Luisa Theelke and Hannah Schieber and Ulrich Eck and R\u00fcdiger von Eisenhart-Rothe and Daniel Roth", "abstract": "Volumetric medical imaging offers great potential for understanding complex pathologies. Yet, traditional 2D slices provide little support for interpreting spatial relationships, forcing users to mentally reconstruct anatomy into three dimensions. Direct volumetric path tracing and VR rendering can improve perception but are computationally expensive, while precomputed representations, like Gaussian Splatting, require planning ahead. Both approaches limit interactive use.\n  We propose a hybrid rendering approach for high-quality, interactive, and immersive anatomical visualization. Our method combines streamed foveated path tracing with a lightweight Gaussian Splatting approximation of the periphery. The peripheral model generation is optimized with volume data and continuously refined using foveal renderings, enabling interactive updates. Depth-guided reprojection further improves robustness to latency and allows users to balance fidelity with refresh rate.\n  We compare our method against direct path tracing and Gaussian Splatting. Our results highlight how their combination can preserve strengths in visual quality while re-generating the peripheral model in under a second, eliminating extensive preprocessing and approximations. This opens new options for interactive medical visualization.", "link": "http://arxiv.org/abs/2601.22026v1", "date": "2026-01-29", "relevancy": 3.0151, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.606}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6043}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Foveated%20Path%20Tracing%20with%20Peripheral%20Gaussians%20for%20Immersive%20Anatomy&body=Title%3A%20Hybrid%20Foveated%20Path%20Tracing%20with%20Peripheral%20Gaussians%20for%20Immersive%20Anatomy%0AAuthor%3A%20Constantin%20Kleinbeck%20and%20Luisa%20Theelke%20and%20Hannah%20Schieber%20and%20Ulrich%20Eck%20and%20R%C3%BCdiger%20von%20Eisenhart-Rothe%20and%20Daniel%20Roth%0AAbstract%3A%20Volumetric%20medical%20imaging%20offers%20great%20potential%20for%20understanding%20complex%20pathologies.%20Yet%2C%20traditional%202D%20slices%20provide%20little%20support%20for%20interpreting%20spatial%20relationships%2C%20forcing%20users%20to%20mentally%20reconstruct%20anatomy%20into%20three%20dimensions.%20Direct%20volumetric%20path%20tracing%20and%20VR%20rendering%20can%20improve%20perception%20but%20are%20computationally%20expensive%2C%20while%20precomputed%20representations%2C%20like%20Gaussian%20Splatting%2C%20require%20planning%20ahead.%20Both%20approaches%20limit%20interactive%20use.%0A%20%20We%20propose%20a%20hybrid%20rendering%20approach%20for%20high-quality%2C%20interactive%2C%20and%20immersive%20anatomical%20visualization.%20Our%20method%20combines%20streamed%20foveated%20path%20tracing%20with%20a%20lightweight%20Gaussian%20Splatting%20approximation%20of%20the%20periphery.%20The%20peripheral%20model%20generation%20is%20optimized%20with%20volume%20data%20and%20continuously%20refined%20using%20foveal%20renderings%2C%20enabling%20interactive%20updates.%20Depth-guided%20reprojection%20further%20improves%20robustness%20to%20latency%20and%20allows%20users%20to%20balance%20fidelity%20with%20refresh%20rate.%0A%20%20We%20compare%20our%20method%20against%20direct%20path%20tracing%20and%20Gaussian%20Splatting.%20Our%20results%20highlight%20how%20their%20combination%20can%20preserve%20strengths%20in%20visual%20quality%20while%20re-generating%20the%20peripheral%20model%20in%20under%20a%20second%2C%20eliminating%20extensive%20preprocessing%20and%20approximations.%20This%20opens%20new%20options%20for%20interactive%20medical%20visualization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Foveated%2520Path%2520Tracing%2520with%2520Peripheral%2520Gaussians%2520for%2520Immersive%2520Anatomy%26entry.906535625%3DConstantin%2520Kleinbeck%2520and%2520Luisa%2520Theelke%2520and%2520Hannah%2520Schieber%2520and%2520Ulrich%2520Eck%2520and%2520R%25C3%25BCdiger%2520von%2520Eisenhart-Rothe%2520and%2520Daniel%2520Roth%26entry.1292438233%3DVolumetric%2520medical%2520imaging%2520offers%2520great%2520potential%2520for%2520understanding%2520complex%2520pathologies.%2520Yet%252C%2520traditional%25202D%2520slices%2520provide%2520little%2520support%2520for%2520interpreting%2520spatial%2520relationships%252C%2520forcing%2520users%2520to%2520mentally%2520reconstruct%2520anatomy%2520into%2520three%2520dimensions.%2520Direct%2520volumetric%2520path%2520tracing%2520and%2520VR%2520rendering%2520can%2520improve%2520perception%2520but%2520are%2520computationally%2520expensive%252C%2520while%2520precomputed%2520representations%252C%2520like%2520Gaussian%2520Splatting%252C%2520require%2520planning%2520ahead.%2520Both%2520approaches%2520limit%2520interactive%2520use.%250A%2520%2520We%2520propose%2520a%2520hybrid%2520rendering%2520approach%2520for%2520high-quality%252C%2520interactive%252C%2520and%2520immersive%2520anatomical%2520visualization.%2520Our%2520method%2520combines%2520streamed%2520foveated%2520path%2520tracing%2520with%2520a%2520lightweight%2520Gaussian%2520Splatting%2520approximation%2520of%2520the%2520periphery.%2520The%2520peripheral%2520model%2520generation%2520is%2520optimized%2520with%2520volume%2520data%2520and%2520continuously%2520refined%2520using%2520foveal%2520renderings%252C%2520enabling%2520interactive%2520updates.%2520Depth-guided%2520reprojection%2520further%2520improves%2520robustness%2520to%2520latency%2520and%2520allows%2520users%2520to%2520balance%2520fidelity%2520with%2520refresh%2520rate.%250A%2520%2520We%2520compare%2520our%2520method%2520against%2520direct%2520path%2520tracing%2520and%2520Gaussian%2520Splatting.%2520Our%2520results%2520highlight%2520how%2520their%2520combination%2520can%2520preserve%2520strengths%2520in%2520visual%2520quality%2520while%2520re-generating%2520the%2520peripheral%2520model%2520in%2520under%2520a%2520second%252C%2520eliminating%2520extensive%2520preprocessing%2520and%2520approximations.%2520This%2520opens%2520new%2520options%2520for%2520interactive%2520medical%2520visualization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Foveated%20Path%20Tracing%20with%20Peripheral%20Gaussians%20for%20Immersive%20Anatomy&entry.906535625=Constantin%20Kleinbeck%20and%20Luisa%20Theelke%20and%20Hannah%20Schieber%20and%20Ulrich%20Eck%20and%20R%C3%BCdiger%20von%20Eisenhart-Rothe%20and%20Daniel%20Roth&entry.1292438233=Volumetric%20medical%20imaging%20offers%20great%20potential%20for%20understanding%20complex%20pathologies.%20Yet%2C%20traditional%202D%20slices%20provide%20little%20support%20for%20interpreting%20spatial%20relationships%2C%20forcing%20users%20to%20mentally%20reconstruct%20anatomy%20into%20three%20dimensions.%20Direct%20volumetric%20path%20tracing%20and%20VR%20rendering%20can%20improve%20perception%20but%20are%20computationally%20expensive%2C%20while%20precomputed%20representations%2C%20like%20Gaussian%20Splatting%2C%20require%20planning%20ahead.%20Both%20approaches%20limit%20interactive%20use.%0A%20%20We%20propose%20a%20hybrid%20rendering%20approach%20for%20high-quality%2C%20interactive%2C%20and%20immersive%20anatomical%20visualization.%20Our%20method%20combines%20streamed%20foveated%20path%20tracing%20with%20a%20lightweight%20Gaussian%20Splatting%20approximation%20of%20the%20periphery.%20The%20peripheral%20model%20generation%20is%20optimized%20with%20volume%20data%20and%20continuously%20refined%20using%20foveal%20renderings%2C%20enabling%20interactive%20updates.%20Depth-guided%20reprojection%20further%20improves%20robustness%20to%20latency%20and%20allows%20users%20to%20balance%20fidelity%20with%20refresh%20rate.%0A%20%20We%20compare%20our%20method%20against%20direct%20path%20tracing%20and%20Gaussian%20Splatting.%20Our%20results%20highlight%20how%20their%20combination%20can%20preserve%20strengths%20in%20visual%20quality%20while%20re-generating%20the%20peripheral%20model%20in%20under%20a%20second%2C%20eliminating%20extensive%20preprocessing%20and%20approximations.%20This%20opens%20new%20options%20for%20interactive%20medical%20visualization.&entry.1838667208=http%3A//arxiv.org/abs/2601.22026v1&entry.124074799=Read"},
{"title": "Edge Collaborative Gaussian Splatting with Integrated Rendering and Communication", "author": "Yujie Wan and Chenxuan Liu and Shuai Wang and Tong Zhang and James Jianqiao Yu and Kejiang Ye and Dusit Niyato and Chengzhong Xu", "abstract": "Gaussian splatting (GS) struggles with degraded rendering quality on low-cost devices. To address this issue, we present edge collaborative GS (ECO-GS), where each user can switch between a local small GS model to guarantee timeliness and a remote large GS model to guarantee fidelity. However, deciding how to engage the large GS model is nontrivial, due to the interdependency between rendering requirements and resource conditions. To this end, we propose integrated rendering and communication (IRAC), which jointly optimizes collaboration status (i.e., deciding whether to engage large GS) and edge power allocation (i.e., enabling remote rendering) under communication constraints across different users by minimizing a newly-derived GS switching function. Despite the nonconvexity of the problem, we propose an efficient penalty majorization minimization (PMM) algorithm to obtain the critical point solution. Furthermore, we develop an imitation learning optimization (ILO) algorithm, which reduces the computational time by over 100x compared to PMM. Experiments demonstrate the superiority of PMM and the real-time execution capability of ILO.", "link": "http://arxiv.org/abs/2510.22718v2", "date": "2026-01-29", "relevancy": 3.0055, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6307}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5875}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge%20Collaborative%20Gaussian%20Splatting%20with%20Integrated%20Rendering%20and%20Communication&body=Title%3A%20Edge%20Collaborative%20Gaussian%20Splatting%20with%20Integrated%20Rendering%20and%20Communication%0AAuthor%3A%20Yujie%20Wan%20and%20Chenxuan%20Liu%20and%20Shuai%20Wang%20and%20Tong%20Zhang%20and%20James%20Jianqiao%20Yu%20and%20Kejiang%20Ye%20and%20Dusit%20Niyato%20and%20Chengzhong%20Xu%0AAbstract%3A%20Gaussian%20splatting%20%28GS%29%20struggles%20with%20degraded%20rendering%20quality%20on%20low-cost%20devices.%20To%20address%20this%20issue%2C%20we%20present%20edge%20collaborative%20GS%20%28ECO-GS%29%2C%20where%20each%20user%20can%20switch%20between%20a%20local%20small%20GS%20model%20to%20guarantee%20timeliness%20and%20a%20remote%20large%20GS%20model%20to%20guarantee%20fidelity.%20However%2C%20deciding%20how%20to%20engage%20the%20large%20GS%20model%20is%20nontrivial%2C%20due%20to%20the%20interdependency%20between%20rendering%20requirements%20and%20resource%20conditions.%20To%20this%20end%2C%20we%20propose%20integrated%20rendering%20and%20communication%20%28IRAC%29%2C%20which%20jointly%20optimizes%20collaboration%20status%20%28i.e.%2C%20deciding%20whether%20to%20engage%20large%20GS%29%20and%20edge%20power%20allocation%20%28i.e.%2C%20enabling%20remote%20rendering%29%20under%20communication%20constraints%20across%20different%20users%20by%20minimizing%20a%20newly-derived%20GS%20switching%20function.%20Despite%20the%20nonconvexity%20of%20the%20problem%2C%20we%20propose%20an%20efficient%20penalty%20majorization%20minimization%20%28PMM%29%20algorithm%20to%20obtain%20the%20critical%20point%20solution.%20Furthermore%2C%20we%20develop%20an%20imitation%20learning%20optimization%20%28ILO%29%20algorithm%2C%20which%20reduces%20the%20computational%20time%20by%20over%20100x%20compared%20to%20PMM.%20Experiments%20demonstrate%20the%20superiority%20of%20PMM%20and%20the%20real-time%20execution%20capability%20of%20ILO.%0ALink%3A%20http%3A//arxiv.org/abs/2510.22718v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge%2520Collaborative%2520Gaussian%2520Splatting%2520with%2520Integrated%2520Rendering%2520and%2520Communication%26entry.906535625%3DYujie%2520Wan%2520and%2520Chenxuan%2520Liu%2520and%2520Shuai%2520Wang%2520and%2520Tong%2520Zhang%2520and%2520James%2520Jianqiao%2520Yu%2520and%2520Kejiang%2520Ye%2520and%2520Dusit%2520Niyato%2520and%2520Chengzhong%2520Xu%26entry.1292438233%3DGaussian%2520splatting%2520%2528GS%2529%2520struggles%2520with%2520degraded%2520rendering%2520quality%2520on%2520low-cost%2520devices.%2520To%2520address%2520this%2520issue%252C%2520we%2520present%2520edge%2520collaborative%2520GS%2520%2528ECO-GS%2529%252C%2520where%2520each%2520user%2520can%2520switch%2520between%2520a%2520local%2520small%2520GS%2520model%2520to%2520guarantee%2520timeliness%2520and%2520a%2520remote%2520large%2520GS%2520model%2520to%2520guarantee%2520fidelity.%2520However%252C%2520deciding%2520how%2520to%2520engage%2520the%2520large%2520GS%2520model%2520is%2520nontrivial%252C%2520due%2520to%2520the%2520interdependency%2520between%2520rendering%2520requirements%2520and%2520resource%2520conditions.%2520To%2520this%2520end%252C%2520we%2520propose%2520integrated%2520rendering%2520and%2520communication%2520%2528IRAC%2529%252C%2520which%2520jointly%2520optimizes%2520collaboration%2520status%2520%2528i.e.%252C%2520deciding%2520whether%2520to%2520engage%2520large%2520GS%2529%2520and%2520edge%2520power%2520allocation%2520%2528i.e.%252C%2520enabling%2520remote%2520rendering%2529%2520under%2520communication%2520constraints%2520across%2520different%2520users%2520by%2520minimizing%2520a%2520newly-derived%2520GS%2520switching%2520function.%2520Despite%2520the%2520nonconvexity%2520of%2520the%2520problem%252C%2520we%2520propose%2520an%2520efficient%2520penalty%2520majorization%2520minimization%2520%2528PMM%2529%2520algorithm%2520to%2520obtain%2520the%2520critical%2520point%2520solution.%2520Furthermore%252C%2520we%2520develop%2520an%2520imitation%2520learning%2520optimization%2520%2528ILO%2529%2520algorithm%252C%2520which%2520reduces%2520the%2520computational%2520time%2520by%2520over%2520100x%2520compared%2520to%2520PMM.%2520Experiments%2520demonstrate%2520the%2520superiority%2520of%2520PMM%2520and%2520the%2520real-time%2520execution%2520capability%2520of%2520ILO.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.22718v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge%20Collaborative%20Gaussian%20Splatting%20with%20Integrated%20Rendering%20and%20Communication&entry.906535625=Yujie%20Wan%20and%20Chenxuan%20Liu%20and%20Shuai%20Wang%20and%20Tong%20Zhang%20and%20James%20Jianqiao%20Yu%20and%20Kejiang%20Ye%20and%20Dusit%20Niyato%20and%20Chengzhong%20Xu&entry.1292438233=Gaussian%20splatting%20%28GS%29%20struggles%20with%20degraded%20rendering%20quality%20on%20low-cost%20devices.%20To%20address%20this%20issue%2C%20we%20present%20edge%20collaborative%20GS%20%28ECO-GS%29%2C%20where%20each%20user%20can%20switch%20between%20a%20local%20small%20GS%20model%20to%20guarantee%20timeliness%20and%20a%20remote%20large%20GS%20model%20to%20guarantee%20fidelity.%20However%2C%20deciding%20how%20to%20engage%20the%20large%20GS%20model%20is%20nontrivial%2C%20due%20to%20the%20interdependency%20between%20rendering%20requirements%20and%20resource%20conditions.%20To%20this%20end%2C%20we%20propose%20integrated%20rendering%20and%20communication%20%28IRAC%29%2C%20which%20jointly%20optimizes%20collaboration%20status%20%28i.e.%2C%20deciding%20whether%20to%20engage%20large%20GS%29%20and%20edge%20power%20allocation%20%28i.e.%2C%20enabling%20remote%20rendering%29%20under%20communication%20constraints%20across%20different%20users%20by%20minimizing%20a%20newly-derived%20GS%20switching%20function.%20Despite%20the%20nonconvexity%20of%20the%20problem%2C%20we%20propose%20an%20efficient%20penalty%20majorization%20minimization%20%28PMM%29%20algorithm%20to%20obtain%20the%20critical%20point%20solution.%20Furthermore%2C%20we%20develop%20an%20imitation%20learning%20optimization%20%28ILO%29%20algorithm%2C%20which%20reduces%20the%20computational%20time%20by%20over%20100x%20compared%20to%20PMM.%20Experiments%20demonstrate%20the%20superiority%20of%20PMM%20and%20the%20real-time%20execution%20capability%20of%20ILO.&entry.1838667208=http%3A//arxiv.org/abs/2510.22718v2&entry.124074799=Read"},
{"title": "SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds", "author": "Alexander Dow and Manduhu Manduhu and Matheus Santos and Ben Bartlett and Gerard Dooly and James Riordan", "abstract": "This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.", "link": "http://arxiv.org/abs/2512.08557v3", "date": "2026-01-29", "relevancy": 2.9777, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6275}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5795}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSCATeR%3A%20Sparse%20Scatter-Based%20Convolution%20Algorithm%20with%20Temporal%20Data%20Recycling%20for%20Real-Time%203D%20Object%20Detection%20in%20LiDAR%20Point%20Clouds&body=Title%3A%20SSCATeR%3A%20Sparse%20Scatter-Based%20Convolution%20Algorithm%20with%20Temporal%20Data%20Recycling%20for%20Real-Time%203D%20Object%20Detection%20in%20LiDAR%20Point%20Clouds%0AAuthor%3A%20Alexander%20Dow%20and%20Manduhu%20Manduhu%20and%20Matheus%20Santos%20and%20Ben%20Bartlett%20and%20Gerard%20Dooly%20and%20James%20Riordan%0AAbstract%3A%20This%20work%20leverages%20the%20continuous%20sweeping%20motion%20of%20LiDAR%20scanning%20to%20concentrate%20object%20detection%20efforts%20on%20specific%20regions%20that%20receive%20a%20change%20in%20point%20data%20from%20one%20frame%20to%20another.%20We%20achieve%20this%20by%20using%20a%20sliding%20time%20window%20with%20short%20strides%20and%20consider%20the%20temporal%20dimension%20by%20storing%20convolution%20results%20between%20passes.%20This%20allows%20us%20to%20ignore%20unchanged%20regions%2C%20significantly%20reducing%20the%20number%20of%20convolution%20operations%20per%20forward%20pass%20without%20sacrificing%20accuracy.%20This%20data%20reuse%20scheme%20introduces%20extreme%20sparsity%20to%20detection%20data.%20To%20exploit%20this%20sparsity%2C%20we%20extend%20our%20previous%20work%20on%20scatter-based%20convolutions%20to%20allow%20for%20data%20reuse%2C%20and%20as%20such%20propose%20Sparse%20Scatter-Based%20Convolution%20Algorithm%20with%20Temporal%20Data%20Recycling%20%28SSCATeR%29.%20This%20operation%20treats%20incoming%20LiDAR%20data%20as%20a%20continuous%20stream%20and%20acts%20only%20on%20the%20changing%20parts%20of%20the%20point%20cloud.%20By%20doing%20so%2C%20we%20achieve%20the%20same%20results%20with%20as%20much%20as%20a%206.61-fold%20reduction%20in%20processing%20time.%20Our%20test%20results%20show%20that%20the%20feature%20maps%20output%20by%20our%20method%20are%20identical%20to%20those%20produced%20by%20traditional%20sparse%20convolution%20techniques%2C%20whilst%20greatly%20increasing%20the%20computational%20efficiency%20of%20the%20network.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08557v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSCATeR%253A%2520Sparse%2520Scatter-Based%2520Convolution%2520Algorithm%2520with%2520Temporal%2520Data%2520Recycling%2520for%2520Real-Time%25203D%2520Object%2520Detection%2520in%2520LiDAR%2520Point%2520Clouds%26entry.906535625%3DAlexander%2520Dow%2520and%2520Manduhu%2520Manduhu%2520and%2520Matheus%2520Santos%2520and%2520Ben%2520Bartlett%2520and%2520Gerard%2520Dooly%2520and%2520James%2520Riordan%26entry.1292438233%3DThis%2520work%2520leverages%2520the%2520continuous%2520sweeping%2520motion%2520of%2520LiDAR%2520scanning%2520to%2520concentrate%2520object%2520detection%2520efforts%2520on%2520specific%2520regions%2520that%2520receive%2520a%2520change%2520in%2520point%2520data%2520from%2520one%2520frame%2520to%2520another.%2520We%2520achieve%2520this%2520by%2520using%2520a%2520sliding%2520time%2520window%2520with%2520short%2520strides%2520and%2520consider%2520the%2520temporal%2520dimension%2520by%2520storing%2520convolution%2520results%2520between%2520passes.%2520This%2520allows%2520us%2520to%2520ignore%2520unchanged%2520regions%252C%2520significantly%2520reducing%2520the%2520number%2520of%2520convolution%2520operations%2520per%2520forward%2520pass%2520without%2520sacrificing%2520accuracy.%2520This%2520data%2520reuse%2520scheme%2520introduces%2520extreme%2520sparsity%2520to%2520detection%2520data.%2520To%2520exploit%2520this%2520sparsity%252C%2520we%2520extend%2520our%2520previous%2520work%2520on%2520scatter-based%2520convolutions%2520to%2520allow%2520for%2520data%2520reuse%252C%2520and%2520as%2520such%2520propose%2520Sparse%2520Scatter-Based%2520Convolution%2520Algorithm%2520with%2520Temporal%2520Data%2520Recycling%2520%2528SSCATeR%2529.%2520This%2520operation%2520treats%2520incoming%2520LiDAR%2520data%2520as%2520a%2520continuous%2520stream%2520and%2520acts%2520only%2520on%2520the%2520changing%2520parts%2520of%2520the%2520point%2520cloud.%2520By%2520doing%2520so%252C%2520we%2520achieve%2520the%2520same%2520results%2520with%2520as%2520much%2520as%2520a%25206.61-fold%2520reduction%2520in%2520processing%2520time.%2520Our%2520test%2520results%2520show%2520that%2520the%2520feature%2520maps%2520output%2520by%2520our%2520method%2520are%2520identical%2520to%2520those%2520produced%2520by%2520traditional%2520sparse%2520convolution%2520techniques%252C%2520whilst%2520greatly%2520increasing%2520the%2520computational%2520efficiency%2520of%2520the%2520network.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08557v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSCATeR%3A%20Sparse%20Scatter-Based%20Convolution%20Algorithm%20with%20Temporal%20Data%20Recycling%20for%20Real-Time%203D%20Object%20Detection%20in%20LiDAR%20Point%20Clouds&entry.906535625=Alexander%20Dow%20and%20Manduhu%20Manduhu%20and%20Matheus%20Santos%20and%20Ben%20Bartlett%20and%20Gerard%20Dooly%20and%20James%20Riordan&entry.1292438233=This%20work%20leverages%20the%20continuous%20sweeping%20motion%20of%20LiDAR%20scanning%20to%20concentrate%20object%20detection%20efforts%20on%20specific%20regions%20that%20receive%20a%20change%20in%20point%20data%20from%20one%20frame%20to%20another.%20We%20achieve%20this%20by%20using%20a%20sliding%20time%20window%20with%20short%20strides%20and%20consider%20the%20temporal%20dimension%20by%20storing%20convolution%20results%20between%20passes.%20This%20allows%20us%20to%20ignore%20unchanged%20regions%2C%20significantly%20reducing%20the%20number%20of%20convolution%20operations%20per%20forward%20pass%20without%20sacrificing%20accuracy.%20This%20data%20reuse%20scheme%20introduces%20extreme%20sparsity%20to%20detection%20data.%20To%20exploit%20this%20sparsity%2C%20we%20extend%20our%20previous%20work%20on%20scatter-based%20convolutions%20to%20allow%20for%20data%20reuse%2C%20and%20as%20such%20propose%20Sparse%20Scatter-Based%20Convolution%20Algorithm%20with%20Temporal%20Data%20Recycling%20%28SSCATeR%29.%20This%20operation%20treats%20incoming%20LiDAR%20data%20as%20a%20continuous%20stream%20and%20acts%20only%20on%20the%20changing%20parts%20of%20the%20point%20cloud.%20By%20doing%20so%2C%20we%20achieve%20the%20same%20results%20with%20as%20much%20as%20a%206.61-fold%20reduction%20in%20processing%20time.%20Our%20test%20results%20show%20that%20the%20feature%20maps%20output%20by%20our%20method%20are%20identical%20to%20those%20produced%20by%20traditional%20sparse%20convolution%20techniques%2C%20whilst%20greatly%20increasing%20the%20computational%20efficiency%20of%20the%20network.&entry.1838667208=http%3A//arxiv.org/abs/2512.08557v3&entry.124074799=Read"},
{"title": "Vidmento: Creating Video Stories Through Context-Aware Expansion With Generative Video", "author": "Catherine Yeh and Anh Truong and Mira Dontcheva and Bryan Wang", "abstract": "Video storytelling is often constrained by available material, limiting creative expression and leaving undesired narrative gaps. Generative video offers a new way to address these limitations by augmenting captured media with tailored visuals. To explore this potential, we interviewed eight video creators to identify opportunities and challenges in integrating generative video into their workflows. Building on these insights and established filmmaking principles, we developed Vidmento, a tool for authoring hybrid video stories that combine captured and generated media through context-aware expansion. Vidmento surfaces opportunities for story development, generates clips that blend stylistically and narratively with surrounding media, and provides controls for refinement. In a study with 12 creators, Vidmento supported narrative development and exploration by systematically expanding initial materials with generative media, enabling expressive video storytelling aligned with creative intent. We highlight how creators bridge story gaps with generative content and where they find this blending capability most valuable.", "link": "http://arxiv.org/abs/2601.22013v1", "date": "2026-01-29", "relevancy": 2.9761, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6107}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6079}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vidmento%3A%20Creating%20Video%20Stories%20Through%20Context-Aware%20Expansion%20With%20Generative%20Video&body=Title%3A%20Vidmento%3A%20Creating%20Video%20Stories%20Through%20Context-Aware%20Expansion%20With%20Generative%20Video%0AAuthor%3A%20Catherine%20Yeh%20and%20Anh%20Truong%20and%20Mira%20Dontcheva%20and%20Bryan%20Wang%0AAbstract%3A%20Video%20storytelling%20is%20often%20constrained%20by%20available%20material%2C%20limiting%20creative%20expression%20and%20leaving%20undesired%20narrative%20gaps.%20Generative%20video%20offers%20a%20new%20way%20to%20address%20these%20limitations%20by%20augmenting%20captured%20media%20with%20tailored%20visuals.%20To%20explore%20this%20potential%2C%20we%20interviewed%20eight%20video%20creators%20to%20identify%20opportunities%20and%20challenges%20in%20integrating%20generative%20video%20into%20their%20workflows.%20Building%20on%20these%20insights%20and%20established%20filmmaking%20principles%2C%20we%20developed%20Vidmento%2C%20a%20tool%20for%20authoring%20hybrid%20video%20stories%20that%20combine%20captured%20and%20generated%20media%20through%20context-aware%20expansion.%20Vidmento%20surfaces%20opportunities%20for%20story%20development%2C%20generates%20clips%20that%20blend%20stylistically%20and%20narratively%20with%20surrounding%20media%2C%20and%20provides%20controls%20for%20refinement.%20In%20a%20study%20with%2012%20creators%2C%20Vidmento%20supported%20narrative%20development%20and%20exploration%20by%20systematically%20expanding%20initial%20materials%20with%20generative%20media%2C%20enabling%20expressive%20video%20storytelling%20aligned%20with%20creative%20intent.%20We%20highlight%20how%20creators%20bridge%20story%20gaps%20with%20generative%20content%20and%20where%20they%20find%20this%20blending%20capability%20most%20valuable.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidmento%253A%2520Creating%2520Video%2520Stories%2520Through%2520Context-Aware%2520Expansion%2520With%2520Generative%2520Video%26entry.906535625%3DCatherine%2520Yeh%2520and%2520Anh%2520Truong%2520and%2520Mira%2520Dontcheva%2520and%2520Bryan%2520Wang%26entry.1292438233%3DVideo%2520storytelling%2520is%2520often%2520constrained%2520by%2520available%2520material%252C%2520limiting%2520creative%2520expression%2520and%2520leaving%2520undesired%2520narrative%2520gaps.%2520Generative%2520video%2520offers%2520a%2520new%2520way%2520to%2520address%2520these%2520limitations%2520by%2520augmenting%2520captured%2520media%2520with%2520tailored%2520visuals.%2520To%2520explore%2520this%2520potential%252C%2520we%2520interviewed%2520eight%2520video%2520creators%2520to%2520identify%2520opportunities%2520and%2520challenges%2520in%2520integrating%2520generative%2520video%2520into%2520their%2520workflows.%2520Building%2520on%2520these%2520insights%2520and%2520established%2520filmmaking%2520principles%252C%2520we%2520developed%2520Vidmento%252C%2520a%2520tool%2520for%2520authoring%2520hybrid%2520video%2520stories%2520that%2520combine%2520captured%2520and%2520generated%2520media%2520through%2520context-aware%2520expansion.%2520Vidmento%2520surfaces%2520opportunities%2520for%2520story%2520development%252C%2520generates%2520clips%2520that%2520blend%2520stylistically%2520and%2520narratively%2520with%2520surrounding%2520media%252C%2520and%2520provides%2520controls%2520for%2520refinement.%2520In%2520a%2520study%2520with%252012%2520creators%252C%2520Vidmento%2520supported%2520narrative%2520development%2520and%2520exploration%2520by%2520systematically%2520expanding%2520initial%2520materials%2520with%2520generative%2520media%252C%2520enabling%2520expressive%2520video%2520storytelling%2520aligned%2520with%2520creative%2520intent.%2520We%2520highlight%2520how%2520creators%2520bridge%2520story%2520gaps%2520with%2520generative%2520content%2520and%2520where%2520they%2520find%2520this%2520blending%2520capability%2520most%2520valuable.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vidmento%3A%20Creating%20Video%20Stories%20Through%20Context-Aware%20Expansion%20With%20Generative%20Video&entry.906535625=Catherine%20Yeh%20and%20Anh%20Truong%20and%20Mira%20Dontcheva%20and%20Bryan%20Wang&entry.1292438233=Video%20storytelling%20is%20often%20constrained%20by%20available%20material%2C%20limiting%20creative%20expression%20and%20leaving%20undesired%20narrative%20gaps.%20Generative%20video%20offers%20a%20new%20way%20to%20address%20these%20limitations%20by%20augmenting%20captured%20media%20with%20tailored%20visuals.%20To%20explore%20this%20potential%2C%20we%20interviewed%20eight%20video%20creators%20to%20identify%20opportunities%20and%20challenges%20in%20integrating%20generative%20video%20into%20their%20workflows.%20Building%20on%20these%20insights%20and%20established%20filmmaking%20principles%2C%20we%20developed%20Vidmento%2C%20a%20tool%20for%20authoring%20hybrid%20video%20stories%20that%20combine%20captured%20and%20generated%20media%20through%20context-aware%20expansion.%20Vidmento%20surfaces%20opportunities%20for%20story%20development%2C%20generates%20clips%20that%20blend%20stylistically%20and%20narratively%20with%20surrounding%20media%2C%20and%20provides%20controls%20for%20refinement.%20In%20a%20study%20with%2012%20creators%2C%20Vidmento%20supported%20narrative%20development%20and%20exploration%20by%20systematically%20expanding%20initial%20materials%20with%20generative%20media%2C%20enabling%20expressive%20video%20storytelling%20aligned%20with%20creative%20intent.%20We%20highlight%20how%20creators%20bridge%20story%20gaps%20with%20generative%20content%20and%20where%20they%20find%20this%20blending%20capability%20most%20valuable.&entry.1838667208=http%3A//arxiv.org/abs/2601.22013v1&entry.124074799=Read"},
{"title": "CG-MLLM: Captioning and Generating 3D content via Multi-modal Large Language Models", "author": "Junming Huang and Weiwei Xu", "abstract": "Large Language Models(LLMs) have revolutionized text generation and multimodal perception, but their capabilities in 3D content generation remain underexplored. Existing methods compromise by producing either low-resolution meshes or coarse structural proxies, failing to capture fine-grained geometry natively. In this paper, we propose CG-MLLM, a novel Multi-modal Large Language Model (MLLM) capable of 3D captioning and high-resolution 3D generation in a single framework. Leveraging the Mixture-of-Transformer architecture, CG-MLLM decouples disparate modeling needs, where the Token-level Autoregressive (TokenAR) Transformer handles token-level content, and the Block-level Autoregressive (BlockAR) Transformer handles block-level content. By integrating a pre-trained vision-language backbone with a specialized 3D VAE latent space, CG-MLLM facilitates long-context interactions between standard tokens and spatial blocks within a single integrated architecture. Experimental results show that CG-MLLM significantly outperforms existing MLLMs in generating high-fidelity 3D objects, effectively bringing high-resolution 3D content creation into the mainstream LLM paradigm.", "link": "http://arxiv.org/abs/2601.21798v1", "date": "2026-01-29", "relevancy": 2.9201, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6161}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CG-MLLM%3A%20Captioning%20and%20Generating%203D%20content%20via%20Multi-modal%20Large%20Language%20Models&body=Title%3A%20CG-MLLM%3A%20Captioning%20and%20Generating%203D%20content%20via%20Multi-modal%20Large%20Language%20Models%0AAuthor%3A%20Junming%20Huang%20and%20Weiwei%20Xu%0AAbstract%3A%20Large%20Language%20Models%28LLMs%29%20have%20revolutionized%20text%20generation%20and%20multimodal%20perception%2C%20but%20their%20capabilities%20in%203D%20content%20generation%20remain%20underexplored.%20Existing%20methods%20compromise%20by%20producing%20either%20low-resolution%20meshes%20or%20coarse%20structural%20proxies%2C%20failing%20to%20capture%20fine-grained%20geometry%20natively.%20In%20this%20paper%2C%20we%20propose%20CG-MLLM%2C%20a%20novel%20Multi-modal%20Large%20Language%20Model%20%28MLLM%29%20capable%20of%203D%20captioning%20and%20high-resolution%203D%20generation%20in%20a%20single%20framework.%20Leveraging%20the%20Mixture-of-Transformer%20architecture%2C%20CG-MLLM%20decouples%20disparate%20modeling%20needs%2C%20where%20the%20Token-level%20Autoregressive%20%28TokenAR%29%20Transformer%20handles%20token-level%20content%2C%20and%20the%20Block-level%20Autoregressive%20%28BlockAR%29%20Transformer%20handles%20block-level%20content.%20By%20integrating%20a%20pre-trained%20vision-language%20backbone%20with%20a%20specialized%203D%20VAE%20latent%20space%2C%20CG-MLLM%20facilitates%20long-context%20interactions%20between%20standard%20tokens%20and%20spatial%20blocks%20within%20a%20single%20integrated%20architecture.%20Experimental%20results%20show%20that%20CG-MLLM%20significantly%20outperforms%20existing%20MLLMs%20in%20generating%20high-fidelity%203D%20objects%2C%20effectively%20bringing%20high-resolution%203D%20content%20creation%20into%20the%20mainstream%20LLM%20paradigm.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCG-MLLM%253A%2520Captioning%2520and%2520Generating%25203D%2520content%2520via%2520Multi-modal%2520Large%2520Language%2520Models%26entry.906535625%3DJunming%2520Huang%2520and%2520Weiwei%2520Xu%26entry.1292438233%3DLarge%2520Language%2520Models%2528LLMs%2529%2520have%2520revolutionized%2520text%2520generation%2520and%2520multimodal%2520perception%252C%2520but%2520their%2520capabilities%2520in%25203D%2520content%2520generation%2520remain%2520underexplored.%2520Existing%2520methods%2520compromise%2520by%2520producing%2520either%2520low-resolution%2520meshes%2520or%2520coarse%2520structural%2520proxies%252C%2520failing%2520to%2520capture%2520fine-grained%2520geometry%2520natively.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CG-MLLM%252C%2520a%2520novel%2520Multi-modal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520capable%2520of%25203D%2520captioning%2520and%2520high-resolution%25203D%2520generation%2520in%2520a%2520single%2520framework.%2520Leveraging%2520the%2520Mixture-of-Transformer%2520architecture%252C%2520CG-MLLM%2520decouples%2520disparate%2520modeling%2520needs%252C%2520where%2520the%2520Token-level%2520Autoregressive%2520%2528TokenAR%2529%2520Transformer%2520handles%2520token-level%2520content%252C%2520and%2520the%2520Block-level%2520Autoregressive%2520%2528BlockAR%2529%2520Transformer%2520handles%2520block-level%2520content.%2520By%2520integrating%2520a%2520pre-trained%2520vision-language%2520backbone%2520with%2520a%2520specialized%25203D%2520VAE%2520latent%2520space%252C%2520CG-MLLM%2520facilitates%2520long-context%2520interactions%2520between%2520standard%2520tokens%2520and%2520spatial%2520blocks%2520within%2520a%2520single%2520integrated%2520architecture.%2520Experimental%2520results%2520show%2520that%2520CG-MLLM%2520significantly%2520outperforms%2520existing%2520MLLMs%2520in%2520generating%2520high-fidelity%25203D%2520objects%252C%2520effectively%2520bringing%2520high-resolution%25203D%2520content%2520creation%2520into%2520the%2520mainstream%2520LLM%2520paradigm.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CG-MLLM%3A%20Captioning%20and%20Generating%203D%20content%20via%20Multi-modal%20Large%20Language%20Models&entry.906535625=Junming%20Huang%20and%20Weiwei%20Xu&entry.1292438233=Large%20Language%20Models%28LLMs%29%20have%20revolutionized%20text%20generation%20and%20multimodal%20perception%2C%20but%20their%20capabilities%20in%203D%20content%20generation%20remain%20underexplored.%20Existing%20methods%20compromise%20by%20producing%20either%20low-resolution%20meshes%20or%20coarse%20structural%20proxies%2C%20failing%20to%20capture%20fine-grained%20geometry%20natively.%20In%20this%20paper%2C%20we%20propose%20CG-MLLM%2C%20a%20novel%20Multi-modal%20Large%20Language%20Model%20%28MLLM%29%20capable%20of%203D%20captioning%20and%20high-resolution%203D%20generation%20in%20a%20single%20framework.%20Leveraging%20the%20Mixture-of-Transformer%20architecture%2C%20CG-MLLM%20decouples%20disparate%20modeling%20needs%2C%20where%20the%20Token-level%20Autoregressive%20%28TokenAR%29%20Transformer%20handles%20token-level%20content%2C%20and%20the%20Block-level%20Autoregressive%20%28BlockAR%29%20Transformer%20handles%20block-level%20content.%20By%20integrating%20a%20pre-trained%20vision-language%20backbone%20with%20a%20specialized%203D%20VAE%20latent%20space%2C%20CG-MLLM%20facilitates%20long-context%20interactions%20between%20standard%20tokens%20and%20spatial%20blocks%20within%20a%20single%20integrated%20architecture.%20Experimental%20results%20show%20that%20CG-MLLM%20significantly%20outperforms%20existing%20MLLMs%20in%20generating%20high-fidelity%203D%20objects%2C%20effectively%20bringing%20high-resolution%203D%20content%20creation%20into%20the%20mainstream%20LLM%20paradigm.&entry.1838667208=http%3A//arxiv.org/abs/2601.21798v1&entry.124074799=Read"},
{"title": "Beyond Global Alignment: Fine-Grained Motion-Language Retrieval via Pyramidal Shapley-Taylor Learning", "author": "Hanmo Chen and Guangtao Lyu and Chenghao Xu and Jiexi Yan and Xu Yang and Cheng Deng", "abstract": "As a foundational task in human-centric cross-modal intelligence, motion-language retrieval aims to bridge the semantic gap between natural language and human motion, enabling intuitive motion analysis, yet existing approaches predominantly focus on aligning entire motion sequences with global textual representations. This global-centric paradigm overlooks fine-grained interactions between local motion segments and individual body joints and text tokens, inevitably leading to suboptimal retrieval performance. To address this limitation, we draw inspiration from the pyramidal process of human motion perception (from joint dynamics to segment coherence, and finally to holistic comprehension) and propose a novel Pyramidal Shapley-Taylor (PST) learning framework for fine-grained motion-language retrieval. Specifically, the framework decomposes human motion into temporal segments and spatial body joints, and learns cross-modal correspondences through progressive joint-wise and segment-wise alignment in a pyramidal fashion, effectively capturing both local semantic details and hierarchical structural relationships. Extensive experiments on multiple public benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, achieving precise alignment between motion segments and body joints and their corresponding text tokens. The code of this work will be released upon acceptance.", "link": "http://arxiv.org/abs/2601.21904v1", "date": "2026-01-29", "relevancy": 2.9123, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5853}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5817}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Global%20Alignment%3A%20Fine-Grained%20Motion-Language%20Retrieval%20via%20Pyramidal%20Shapley-Taylor%20Learning&body=Title%3A%20Beyond%20Global%20Alignment%3A%20Fine-Grained%20Motion-Language%20Retrieval%20via%20Pyramidal%20Shapley-Taylor%20Learning%0AAuthor%3A%20Hanmo%20Chen%20and%20Guangtao%20Lyu%20and%20Chenghao%20Xu%20and%20Jiexi%20Yan%20and%20Xu%20Yang%20and%20Cheng%20Deng%0AAbstract%3A%20As%20a%20foundational%20task%20in%20human-centric%20cross-modal%20intelligence%2C%20motion-language%20retrieval%20aims%20to%20bridge%20the%20semantic%20gap%20between%20natural%20language%20and%20human%20motion%2C%20enabling%20intuitive%20motion%20analysis%2C%20yet%20existing%20approaches%20predominantly%20focus%20on%20aligning%20entire%20motion%20sequences%20with%20global%20textual%20representations.%20This%20global-centric%20paradigm%20overlooks%20fine-grained%20interactions%20between%20local%20motion%20segments%20and%20individual%20body%20joints%20and%20text%20tokens%2C%20inevitably%20leading%20to%20suboptimal%20retrieval%20performance.%20To%20address%20this%20limitation%2C%20we%20draw%20inspiration%20from%20the%20pyramidal%20process%20of%20human%20motion%20perception%20%28from%20joint%20dynamics%20to%20segment%20coherence%2C%20and%20finally%20to%20holistic%20comprehension%29%20and%20propose%20a%20novel%20Pyramidal%20Shapley-Taylor%20%28PST%29%20learning%20framework%20for%20fine-grained%20motion-language%20retrieval.%20Specifically%2C%20the%20framework%20decomposes%20human%20motion%20into%20temporal%20segments%20and%20spatial%20body%20joints%2C%20and%20learns%20cross-modal%20correspondences%20through%20progressive%20joint-wise%20and%20segment-wise%20alignment%20in%20a%20pyramidal%20fashion%2C%20effectively%20capturing%20both%20local%20semantic%20details%20and%20hierarchical%20structural%20relationships.%20Extensive%20experiments%20on%20multiple%20public%20benchmark%20datasets%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20state-of-the-art%20methods%2C%20achieving%20precise%20alignment%20between%20motion%20segments%20and%20body%20joints%20and%20their%20corresponding%20text%20tokens.%20The%20code%20of%20this%20work%20will%20be%20released%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Global%2520Alignment%253A%2520Fine-Grained%2520Motion-Language%2520Retrieval%2520via%2520Pyramidal%2520Shapley-Taylor%2520Learning%26entry.906535625%3DHanmo%2520Chen%2520and%2520Guangtao%2520Lyu%2520and%2520Chenghao%2520Xu%2520and%2520Jiexi%2520Yan%2520and%2520Xu%2520Yang%2520and%2520Cheng%2520Deng%26entry.1292438233%3DAs%2520a%2520foundational%2520task%2520in%2520human-centric%2520cross-modal%2520intelligence%252C%2520motion-language%2520retrieval%2520aims%2520to%2520bridge%2520the%2520semantic%2520gap%2520between%2520natural%2520language%2520and%2520human%2520motion%252C%2520enabling%2520intuitive%2520motion%2520analysis%252C%2520yet%2520existing%2520approaches%2520predominantly%2520focus%2520on%2520aligning%2520entire%2520motion%2520sequences%2520with%2520global%2520textual%2520representations.%2520This%2520global-centric%2520paradigm%2520overlooks%2520fine-grained%2520interactions%2520between%2520local%2520motion%2520segments%2520and%2520individual%2520body%2520joints%2520and%2520text%2520tokens%252C%2520inevitably%2520leading%2520to%2520suboptimal%2520retrieval%2520performance.%2520To%2520address%2520this%2520limitation%252C%2520we%2520draw%2520inspiration%2520from%2520the%2520pyramidal%2520process%2520of%2520human%2520motion%2520perception%2520%2528from%2520joint%2520dynamics%2520to%2520segment%2520coherence%252C%2520and%2520finally%2520to%2520holistic%2520comprehension%2529%2520and%2520propose%2520a%2520novel%2520Pyramidal%2520Shapley-Taylor%2520%2528PST%2529%2520learning%2520framework%2520for%2520fine-grained%2520motion-language%2520retrieval.%2520Specifically%252C%2520the%2520framework%2520decomposes%2520human%2520motion%2520into%2520temporal%2520segments%2520and%2520spatial%2520body%2520joints%252C%2520and%2520learns%2520cross-modal%2520correspondences%2520through%2520progressive%2520joint-wise%2520and%2520segment-wise%2520alignment%2520in%2520a%2520pyramidal%2520fashion%252C%2520effectively%2520capturing%2520both%2520local%2520semantic%2520details%2520and%2520hierarchical%2520structural%2520relationships.%2520Extensive%2520experiments%2520on%2520multiple%2520public%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520state-of-the-art%2520methods%252C%2520achieving%2520precise%2520alignment%2520between%2520motion%2520segments%2520and%2520body%2520joints%2520and%2520their%2520corresponding%2520text%2520tokens.%2520The%2520code%2520of%2520this%2520work%2520will%2520be%2520released%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Global%20Alignment%3A%20Fine-Grained%20Motion-Language%20Retrieval%20via%20Pyramidal%20Shapley-Taylor%20Learning&entry.906535625=Hanmo%20Chen%20and%20Guangtao%20Lyu%20and%20Chenghao%20Xu%20and%20Jiexi%20Yan%20and%20Xu%20Yang%20and%20Cheng%20Deng&entry.1292438233=As%20a%20foundational%20task%20in%20human-centric%20cross-modal%20intelligence%2C%20motion-language%20retrieval%20aims%20to%20bridge%20the%20semantic%20gap%20between%20natural%20language%20and%20human%20motion%2C%20enabling%20intuitive%20motion%20analysis%2C%20yet%20existing%20approaches%20predominantly%20focus%20on%20aligning%20entire%20motion%20sequences%20with%20global%20textual%20representations.%20This%20global-centric%20paradigm%20overlooks%20fine-grained%20interactions%20between%20local%20motion%20segments%20and%20individual%20body%20joints%20and%20text%20tokens%2C%20inevitably%20leading%20to%20suboptimal%20retrieval%20performance.%20To%20address%20this%20limitation%2C%20we%20draw%20inspiration%20from%20the%20pyramidal%20process%20of%20human%20motion%20perception%20%28from%20joint%20dynamics%20to%20segment%20coherence%2C%20and%20finally%20to%20holistic%20comprehension%29%20and%20propose%20a%20novel%20Pyramidal%20Shapley-Taylor%20%28PST%29%20learning%20framework%20for%20fine-grained%20motion-language%20retrieval.%20Specifically%2C%20the%20framework%20decomposes%20human%20motion%20into%20temporal%20segments%20and%20spatial%20body%20joints%2C%20and%20learns%20cross-modal%20correspondences%20through%20progressive%20joint-wise%20and%20segment-wise%20alignment%20in%20a%20pyramidal%20fashion%2C%20effectively%20capturing%20both%20local%20semantic%20details%20and%20hierarchical%20structural%20relationships.%20Extensive%20experiments%20on%20multiple%20public%20benchmark%20datasets%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20state-of-the-art%20methods%2C%20achieving%20precise%20alignment%20between%20motion%20segments%20and%20body%20joints%20and%20their%20corresponding%20text%20tokens.%20The%20code%20of%20this%20work%20will%20be%20released%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2601.21904v1&entry.124074799=Read"},
{"title": "Deep Models, Shallow Alignment: Uncovering the Granularity Mismatch in Neural Decoding", "author": "Yang Du and Siyuan Dai and Yonghao Song and Paul M. Thompson and Haoteng Tang and Liang Zhan", "abstract": "Neural visual decoding is a central problem in brain computer interface research, aiming to reconstruct human visual perception and to elucidate the structure of neural representations. However, existing approaches overlook a fundamental granularity mismatch between human and machine vision, where deep vision models emphasize semantic invariance by suppressing local texture information, whereas neural signals preserve an intricate mixture of low-level visual attributes and high-level semantic content. To address this mismatch, we propose Shallow Alignment, a novel contrastive learning strategy that aligns neural signals with intermediate representations of visual encoders rather than their final outputs, thereby striking a better balance between low-level texture details and high-level semantic features. Extensive experiments across multiple benchmarks demonstrate that Shallow Alignment significantly outperforms standard final-layer alignment, with performance gains ranging from 22% to 58% across diverse vision backbones. Notably, our approach effectively unlocks the scaling law in neural visual decoding, enabling decoding performance to scale predictably with the capacity of pre-trained vision backbones. We further conduct systematic empirical analyses to shed light on the mechanisms underlying the observed performance gains.", "link": "http://arxiv.org/abs/2601.21948v1", "date": "2026-01-29", "relevancy": 2.8883, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5975}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5975}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Models%2C%20Shallow%20Alignment%3A%20Uncovering%20the%20Granularity%20Mismatch%20in%20Neural%20Decoding&body=Title%3A%20Deep%20Models%2C%20Shallow%20Alignment%3A%20Uncovering%20the%20Granularity%20Mismatch%20in%20Neural%20Decoding%0AAuthor%3A%20Yang%20Du%20and%20Siyuan%20Dai%20and%20Yonghao%20Song%20and%20Paul%20M.%20Thompson%20and%20Haoteng%20Tang%20and%20Liang%20Zhan%0AAbstract%3A%20Neural%20visual%20decoding%20is%20a%20central%20problem%20in%20brain%20computer%20interface%20research%2C%20aiming%20to%20reconstruct%20human%20visual%20perception%20and%20to%20elucidate%20the%20structure%20of%20neural%20representations.%20However%2C%20existing%20approaches%20overlook%20a%20fundamental%20granularity%20mismatch%20between%20human%20and%20machine%20vision%2C%20where%20deep%20vision%20models%20emphasize%20semantic%20invariance%20by%20suppressing%20local%20texture%20information%2C%20whereas%20neural%20signals%20preserve%20an%20intricate%20mixture%20of%20low-level%20visual%20attributes%20and%20high-level%20semantic%20content.%20To%20address%20this%20mismatch%2C%20we%20propose%20Shallow%20Alignment%2C%20a%20novel%20contrastive%20learning%20strategy%20that%20aligns%20neural%20signals%20with%20intermediate%20representations%20of%20visual%20encoders%20rather%20than%20their%20final%20outputs%2C%20thereby%20striking%20a%20better%20balance%20between%20low-level%20texture%20details%20and%20high-level%20semantic%20features.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20Shallow%20Alignment%20significantly%20outperforms%20standard%20final-layer%20alignment%2C%20with%20performance%20gains%20ranging%20from%2022%25%20to%2058%25%20across%20diverse%20vision%20backbones.%20Notably%2C%20our%20approach%20effectively%20unlocks%20the%20scaling%20law%20in%20neural%20visual%20decoding%2C%20enabling%20decoding%20performance%20to%20scale%20predictably%20with%20the%20capacity%20of%20pre-trained%20vision%20backbones.%20We%20further%20conduct%20systematic%20empirical%20analyses%20to%20shed%20light%20on%20the%20mechanisms%20underlying%20the%20observed%20performance%20gains.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Models%252C%2520Shallow%2520Alignment%253A%2520Uncovering%2520the%2520Granularity%2520Mismatch%2520in%2520Neural%2520Decoding%26entry.906535625%3DYang%2520Du%2520and%2520Siyuan%2520Dai%2520and%2520Yonghao%2520Song%2520and%2520Paul%2520M.%2520Thompson%2520and%2520Haoteng%2520Tang%2520and%2520Liang%2520Zhan%26entry.1292438233%3DNeural%2520visual%2520decoding%2520is%2520a%2520central%2520problem%2520in%2520brain%2520computer%2520interface%2520research%252C%2520aiming%2520to%2520reconstruct%2520human%2520visual%2520perception%2520and%2520to%2520elucidate%2520the%2520structure%2520of%2520neural%2520representations.%2520However%252C%2520existing%2520approaches%2520overlook%2520a%2520fundamental%2520granularity%2520mismatch%2520between%2520human%2520and%2520machine%2520vision%252C%2520where%2520deep%2520vision%2520models%2520emphasize%2520semantic%2520invariance%2520by%2520suppressing%2520local%2520texture%2520information%252C%2520whereas%2520neural%2520signals%2520preserve%2520an%2520intricate%2520mixture%2520of%2520low-level%2520visual%2520attributes%2520and%2520high-level%2520semantic%2520content.%2520To%2520address%2520this%2520mismatch%252C%2520we%2520propose%2520Shallow%2520Alignment%252C%2520a%2520novel%2520contrastive%2520learning%2520strategy%2520that%2520aligns%2520neural%2520signals%2520with%2520intermediate%2520representations%2520of%2520visual%2520encoders%2520rather%2520than%2520their%2520final%2520outputs%252C%2520thereby%2520striking%2520a%2520better%2520balance%2520between%2520low-level%2520texture%2520details%2520and%2520high-level%2520semantic%2520features.%2520Extensive%2520experiments%2520across%2520multiple%2520benchmarks%2520demonstrate%2520that%2520Shallow%2520Alignment%2520significantly%2520outperforms%2520standard%2520final-layer%2520alignment%252C%2520with%2520performance%2520gains%2520ranging%2520from%252022%2525%2520to%252058%2525%2520across%2520diverse%2520vision%2520backbones.%2520Notably%252C%2520our%2520approach%2520effectively%2520unlocks%2520the%2520scaling%2520law%2520in%2520neural%2520visual%2520decoding%252C%2520enabling%2520decoding%2520performance%2520to%2520scale%2520predictably%2520with%2520the%2520capacity%2520of%2520pre-trained%2520vision%2520backbones.%2520We%2520further%2520conduct%2520systematic%2520empirical%2520analyses%2520to%2520shed%2520light%2520on%2520the%2520mechanisms%2520underlying%2520the%2520observed%2520performance%2520gains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Models%2C%20Shallow%20Alignment%3A%20Uncovering%20the%20Granularity%20Mismatch%20in%20Neural%20Decoding&entry.906535625=Yang%20Du%20and%20Siyuan%20Dai%20and%20Yonghao%20Song%20and%20Paul%20M.%20Thompson%20and%20Haoteng%20Tang%20and%20Liang%20Zhan&entry.1292438233=Neural%20visual%20decoding%20is%20a%20central%20problem%20in%20brain%20computer%20interface%20research%2C%20aiming%20to%20reconstruct%20human%20visual%20perception%20and%20to%20elucidate%20the%20structure%20of%20neural%20representations.%20However%2C%20existing%20approaches%20overlook%20a%20fundamental%20granularity%20mismatch%20between%20human%20and%20machine%20vision%2C%20where%20deep%20vision%20models%20emphasize%20semantic%20invariance%20by%20suppressing%20local%20texture%20information%2C%20whereas%20neural%20signals%20preserve%20an%20intricate%20mixture%20of%20low-level%20visual%20attributes%20and%20high-level%20semantic%20content.%20To%20address%20this%20mismatch%2C%20we%20propose%20Shallow%20Alignment%2C%20a%20novel%20contrastive%20learning%20strategy%20that%20aligns%20neural%20signals%20with%20intermediate%20representations%20of%20visual%20encoders%20rather%20than%20their%20final%20outputs%2C%20thereby%20striking%20a%20better%20balance%20between%20low-level%20texture%20details%20and%20high-level%20semantic%20features.%20Extensive%20experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20Shallow%20Alignment%20significantly%20outperforms%20standard%20final-layer%20alignment%2C%20with%20performance%20gains%20ranging%20from%2022%25%20to%2058%25%20across%20diverse%20vision%20backbones.%20Notably%2C%20our%20approach%20effectively%20unlocks%20the%20scaling%20law%20in%20neural%20visual%20decoding%2C%20enabling%20decoding%20performance%20to%20scale%20predictably%20with%20the%20capacity%20of%20pre-trained%20vision%20backbones.%20We%20further%20conduct%20systematic%20empirical%20analyses%20to%20shed%20light%20on%20the%20mechanisms%20underlying%20the%20observed%20performance%20gains.&entry.1838667208=http%3A//arxiv.org/abs/2601.21948v1&entry.124074799=Read"},
{"title": "Virtual Reflections on a Dynamic 2D Eye Model Improve Spatial Reference Identification", "author": "Matti Kr\u00fcger and Yutaka Oshima and Yu Fang", "abstract": "The visible orientation of human eyes creates some transparency about people's spatial attention and other mental states. This leads to a dual role of the eyes as a means of sensing and communication. Accordingly, artificial eye models are being explored as communication media in human-machine interaction scenarios. One challenge in the use of eye models for communication consists of resolving spatial reference ambiguities, especially for screen-based models. To address this challenge, we introduce an approach that incorporates reflection-like features that are contingent on the movements of artificial eyes. We conducted a user study with 30 participants in which participants had to use spatial references provided by dynamic eye models to advance in a fast-paced group interaction task. Compared to a non-reflective eye model and a pure reflection mode, the superimposition of screen-based eyes with gaze-contingent virtual reflections resulted in a higher identification accuracy and user experience, suggesting a synergistic benefit.", "link": "http://arxiv.org/abs/2412.07344v4", "date": "2026-01-29", "relevancy": 2.8847, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6197}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Virtual%20Reflections%20on%20a%20Dynamic%202D%20Eye%20Model%20Improve%20Spatial%20Reference%20Identification&body=Title%3A%20Virtual%20Reflections%20on%20a%20Dynamic%202D%20Eye%20Model%20Improve%20Spatial%20Reference%20Identification%0AAuthor%3A%20Matti%20Kr%C3%BCger%20and%20Yutaka%20Oshima%20and%20Yu%20Fang%0AAbstract%3A%20The%20visible%20orientation%20of%20human%20eyes%20creates%20some%20transparency%20about%20people%27s%20spatial%20attention%20and%20other%20mental%20states.%20This%20leads%20to%20a%20dual%20role%20of%20the%20eyes%20as%20a%20means%20of%20sensing%20and%20communication.%20Accordingly%2C%20artificial%20eye%20models%20are%20being%20explored%20as%20communication%20media%20in%20human-machine%20interaction%20scenarios.%20One%20challenge%20in%20the%20use%20of%20eye%20models%20for%20communication%20consists%20of%20resolving%20spatial%20reference%20ambiguities%2C%20especially%20for%20screen-based%20models.%20To%20address%20this%20challenge%2C%20we%20introduce%20an%20approach%20that%20incorporates%20reflection-like%20features%20that%20are%20contingent%20on%20the%20movements%20of%20artificial%20eyes.%20We%20conducted%20a%20user%20study%20with%2030%20participants%20in%20which%20participants%20had%20to%20use%20spatial%20references%20provided%20by%20dynamic%20eye%20models%20to%20advance%20in%20a%20fast-paced%20group%20interaction%20task.%20Compared%20to%20a%20non-reflective%20eye%20model%20and%20a%20pure%20reflection%20mode%2C%20the%20superimposition%20of%20screen-based%20eyes%20with%20gaze-contingent%20virtual%20reflections%20resulted%20in%20a%20higher%20identification%20accuracy%20and%20user%20experience%2C%20suggesting%20a%20synergistic%20benefit.%0ALink%3A%20http%3A//arxiv.org/abs/2412.07344v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVirtual%2520Reflections%2520on%2520a%2520Dynamic%25202D%2520Eye%2520Model%2520Improve%2520Spatial%2520Reference%2520Identification%26entry.906535625%3DMatti%2520Kr%25C3%25BCger%2520and%2520Yutaka%2520Oshima%2520and%2520Yu%2520Fang%26entry.1292438233%3DThe%2520visible%2520orientation%2520of%2520human%2520eyes%2520creates%2520some%2520transparency%2520about%2520people%2527s%2520spatial%2520attention%2520and%2520other%2520mental%2520states.%2520This%2520leads%2520to%2520a%2520dual%2520role%2520of%2520the%2520eyes%2520as%2520a%2520means%2520of%2520sensing%2520and%2520communication.%2520Accordingly%252C%2520artificial%2520eye%2520models%2520are%2520being%2520explored%2520as%2520communication%2520media%2520in%2520human-machine%2520interaction%2520scenarios.%2520One%2520challenge%2520in%2520the%2520use%2520of%2520eye%2520models%2520for%2520communication%2520consists%2520of%2520resolving%2520spatial%2520reference%2520ambiguities%252C%2520especially%2520for%2520screen-based%2520models.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520an%2520approach%2520that%2520incorporates%2520reflection-like%2520features%2520that%2520are%2520contingent%2520on%2520the%2520movements%2520of%2520artificial%2520eyes.%2520We%2520conducted%2520a%2520user%2520study%2520with%252030%2520participants%2520in%2520which%2520participants%2520had%2520to%2520use%2520spatial%2520references%2520provided%2520by%2520dynamic%2520eye%2520models%2520to%2520advance%2520in%2520a%2520fast-paced%2520group%2520interaction%2520task.%2520Compared%2520to%2520a%2520non-reflective%2520eye%2520model%2520and%2520a%2520pure%2520reflection%2520mode%252C%2520the%2520superimposition%2520of%2520screen-based%2520eyes%2520with%2520gaze-contingent%2520virtual%2520reflections%2520resulted%2520in%2520a%2520higher%2520identification%2520accuracy%2520and%2520user%2520experience%252C%2520suggesting%2520a%2520synergistic%2520benefit.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07344v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Virtual%20Reflections%20on%20a%20Dynamic%202D%20Eye%20Model%20Improve%20Spatial%20Reference%20Identification&entry.906535625=Matti%20Kr%C3%BCger%20and%20Yutaka%20Oshima%20and%20Yu%20Fang&entry.1292438233=The%20visible%20orientation%20of%20human%20eyes%20creates%20some%20transparency%20about%20people%27s%20spatial%20attention%20and%20other%20mental%20states.%20This%20leads%20to%20a%20dual%20role%20of%20the%20eyes%20as%20a%20means%20of%20sensing%20and%20communication.%20Accordingly%2C%20artificial%20eye%20models%20are%20being%20explored%20as%20communication%20media%20in%20human-machine%20interaction%20scenarios.%20One%20challenge%20in%20the%20use%20of%20eye%20models%20for%20communication%20consists%20of%20resolving%20spatial%20reference%20ambiguities%2C%20especially%20for%20screen-based%20models.%20To%20address%20this%20challenge%2C%20we%20introduce%20an%20approach%20that%20incorporates%20reflection-like%20features%20that%20are%20contingent%20on%20the%20movements%20of%20artificial%20eyes.%20We%20conducted%20a%20user%20study%20with%2030%20participants%20in%20which%20participants%20had%20to%20use%20spatial%20references%20provided%20by%20dynamic%20eye%20models%20to%20advance%20in%20a%20fast-paced%20group%20interaction%20task.%20Compared%20to%20a%20non-reflective%20eye%20model%20and%20a%20pure%20reflection%20mode%2C%20the%20superimposition%20of%20screen-based%20eyes%20with%20gaze-contingent%20virtual%20reflections%20resulted%20in%20a%20higher%20identification%20accuracy%20and%20user%20experience%2C%20suggesting%20a%20synergistic%20benefit.&entry.1838667208=http%3A//arxiv.org/abs/2412.07344v4&entry.124074799=Read"},
{"title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources", "author": "Baorui Ma and Jiahui Yang and Donglin Di and Xuancheng Zhang and Jianxun Cui and Hao Li and Yan Xie and Wei Chen", "abstract": "Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.", "link": "http://arxiv.org/abs/2601.22054v1", "date": "2026-01-29", "relevancy": 2.8705, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5778}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5723}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetricAnything%3A%20Scaling%20Metric%20Depth%20Pretraining%20with%20Noisy%20Heterogeneous%20Sources&body=Title%3A%20MetricAnything%3A%20Scaling%20Metric%20Depth%20Pretraining%20with%20Noisy%20Heterogeneous%20Sources%0AAuthor%3A%20Baorui%20Ma%20and%20Jiahui%20Yang%20and%20Donglin%20Di%20and%20Xuancheng%20Zhang%20and%20Jianxun%20Cui%20and%20Hao%20Li%20and%20Yan%20Xie%20and%20Wei%20Chen%0AAbstract%3A%20Scaling%20has%20powered%20recent%20advances%20in%20vision%20foundation%20models%2C%20yet%20extending%20this%20paradigm%20to%20metric%20depth%20estimation%20remains%20challenging%20due%20to%20heterogeneous%20sensor%20noise%2C%20camera-dependent%20biases%2C%20and%20metric%20ambiguity%20in%20noisy%20cross-source%203D%20data.%20We%20introduce%20Metric%20Anything%2C%20a%20simple%20and%20scalable%20pretraining%20framework%20that%20learns%20metric%20depth%20from%20noisy%2C%20diverse%203D%20sources%20without%20manually%20engineered%20prompts%2C%20camera-specific%20modeling%2C%20or%20task-specific%20architectures.%20Central%20to%20our%20approach%20is%20the%20Sparse%20Metric%20Prompt%2C%20created%20by%20randomly%20masking%20depth%20maps%2C%20which%20serves%20as%20a%20universal%20interface%20that%20decouples%20spatial%20reasoning%20from%20sensor%20and%20camera%20biases.%20Using%20about%2020M%20image-depth%20pairs%20spanning%20reconstructed%2C%20captured%2C%20and%20rendered%203D%20data%20across%2010000%20camera%20models%2C%20we%20demonstrate-for%20the%20first%20time-a%20clear%20scaling%20trend%20in%20the%20metric%20depth%20track.%20The%20pretrained%20model%20excels%20at%20prompt-driven%20tasks%20such%20as%20depth%20completion%2C%20super-resolution%20and%20Radar-camera%20fusion%2C%20while%20its%20distilled%20prompt-free%20student%20achieves%20state-of-the-art%20results%20on%20monocular%20depth%20estimation%2C%20camera%20intrinsics%20recovery%2C%20single/multi-view%20metric%203D%20reconstruction%2C%20and%20VLA%20planning.%20We%20also%20show%20that%20using%20pretrained%20ViT%20of%20Metric%20Anything%20as%20a%20visual%20encoder%20significantly%20boosts%20Multimodal%20Large%20Language%20Model%20capabilities%20in%20spatial%20intelligence.%20These%20results%20show%20that%20metric%20depth%20estimation%20can%20benefit%20from%20the%20same%20scaling%20laws%20that%20drive%20modern%20foundation%20models%2C%20establishing%20a%20new%20path%20toward%20scalable%20and%20efficient%20real-world%20metric%20perception.%20We%20open-source%20MetricAnything%20at%20http%3A//metric-anything.github.io/metric-anything-io/%20to%20support%20community%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetricAnything%253A%2520Scaling%2520Metric%2520Depth%2520Pretraining%2520with%2520Noisy%2520Heterogeneous%2520Sources%26entry.906535625%3DBaorui%2520Ma%2520and%2520Jiahui%2520Yang%2520and%2520Donglin%2520Di%2520and%2520Xuancheng%2520Zhang%2520and%2520Jianxun%2520Cui%2520and%2520Hao%2520Li%2520and%2520Yan%2520Xie%2520and%2520Wei%2520Chen%26entry.1292438233%3DScaling%2520has%2520powered%2520recent%2520advances%2520in%2520vision%2520foundation%2520models%252C%2520yet%2520extending%2520this%2520paradigm%2520to%2520metric%2520depth%2520estimation%2520remains%2520challenging%2520due%2520to%2520heterogeneous%2520sensor%2520noise%252C%2520camera-dependent%2520biases%252C%2520and%2520metric%2520ambiguity%2520in%2520noisy%2520cross-source%25203D%2520data.%2520We%2520introduce%2520Metric%2520Anything%252C%2520a%2520simple%2520and%2520scalable%2520pretraining%2520framework%2520that%2520learns%2520metric%2520depth%2520from%2520noisy%252C%2520diverse%25203D%2520sources%2520without%2520manually%2520engineered%2520prompts%252C%2520camera-specific%2520modeling%252C%2520or%2520task-specific%2520architectures.%2520Central%2520to%2520our%2520approach%2520is%2520the%2520Sparse%2520Metric%2520Prompt%252C%2520created%2520by%2520randomly%2520masking%2520depth%2520maps%252C%2520which%2520serves%2520as%2520a%2520universal%2520interface%2520that%2520decouples%2520spatial%2520reasoning%2520from%2520sensor%2520and%2520camera%2520biases.%2520Using%2520about%252020M%2520image-depth%2520pairs%2520spanning%2520reconstructed%252C%2520captured%252C%2520and%2520rendered%25203D%2520data%2520across%252010000%2520camera%2520models%252C%2520we%2520demonstrate-for%2520the%2520first%2520time-a%2520clear%2520scaling%2520trend%2520in%2520the%2520metric%2520depth%2520track.%2520The%2520pretrained%2520model%2520excels%2520at%2520prompt-driven%2520tasks%2520such%2520as%2520depth%2520completion%252C%2520super-resolution%2520and%2520Radar-camera%2520fusion%252C%2520while%2520its%2520distilled%2520prompt-free%2520student%2520achieves%2520state-of-the-art%2520results%2520on%2520monocular%2520depth%2520estimation%252C%2520camera%2520intrinsics%2520recovery%252C%2520single/multi-view%2520metric%25203D%2520reconstruction%252C%2520and%2520VLA%2520planning.%2520We%2520also%2520show%2520that%2520using%2520pretrained%2520ViT%2520of%2520Metric%2520Anything%2520as%2520a%2520visual%2520encoder%2520significantly%2520boosts%2520Multimodal%2520Large%2520Language%2520Model%2520capabilities%2520in%2520spatial%2520intelligence.%2520These%2520results%2520show%2520that%2520metric%2520depth%2520estimation%2520can%2520benefit%2520from%2520the%2520same%2520scaling%2520laws%2520that%2520drive%2520modern%2520foundation%2520models%252C%2520establishing%2520a%2520new%2520path%2520toward%2520scalable%2520and%2520efficient%2520real-world%2520metric%2520perception.%2520We%2520open-source%2520MetricAnything%2520at%2520http%253A//metric-anything.github.io/metric-anything-io/%2520to%2520support%2520community%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetricAnything%3A%20Scaling%20Metric%20Depth%20Pretraining%20with%20Noisy%20Heterogeneous%20Sources&entry.906535625=Baorui%20Ma%20and%20Jiahui%20Yang%20and%20Donglin%20Di%20and%20Xuancheng%20Zhang%20and%20Jianxun%20Cui%20and%20Hao%20Li%20and%20Yan%20Xie%20and%20Wei%20Chen&entry.1292438233=Scaling%20has%20powered%20recent%20advances%20in%20vision%20foundation%20models%2C%20yet%20extending%20this%20paradigm%20to%20metric%20depth%20estimation%20remains%20challenging%20due%20to%20heterogeneous%20sensor%20noise%2C%20camera-dependent%20biases%2C%20and%20metric%20ambiguity%20in%20noisy%20cross-source%203D%20data.%20We%20introduce%20Metric%20Anything%2C%20a%20simple%20and%20scalable%20pretraining%20framework%20that%20learns%20metric%20depth%20from%20noisy%2C%20diverse%203D%20sources%20without%20manually%20engineered%20prompts%2C%20camera-specific%20modeling%2C%20or%20task-specific%20architectures.%20Central%20to%20our%20approach%20is%20the%20Sparse%20Metric%20Prompt%2C%20created%20by%20randomly%20masking%20depth%20maps%2C%20which%20serves%20as%20a%20universal%20interface%20that%20decouples%20spatial%20reasoning%20from%20sensor%20and%20camera%20biases.%20Using%20about%2020M%20image-depth%20pairs%20spanning%20reconstructed%2C%20captured%2C%20and%20rendered%203D%20data%20across%2010000%20camera%20models%2C%20we%20demonstrate-for%20the%20first%20time-a%20clear%20scaling%20trend%20in%20the%20metric%20depth%20track.%20The%20pretrained%20model%20excels%20at%20prompt-driven%20tasks%20such%20as%20depth%20completion%2C%20super-resolution%20and%20Radar-camera%20fusion%2C%20while%20its%20distilled%20prompt-free%20student%20achieves%20state-of-the-art%20results%20on%20monocular%20depth%20estimation%2C%20camera%20intrinsics%20recovery%2C%20single/multi-view%20metric%203D%20reconstruction%2C%20and%20VLA%20planning.%20We%20also%20show%20that%20using%20pretrained%20ViT%20of%20Metric%20Anything%20as%20a%20visual%20encoder%20significantly%20boosts%20Multimodal%20Large%20Language%20Model%20capabilities%20in%20spatial%20intelligence.%20These%20results%20show%20that%20metric%20depth%20estimation%20can%20benefit%20from%20the%20same%20scaling%20laws%20that%20drive%20modern%20foundation%20models%2C%20establishing%20a%20new%20path%20toward%20scalable%20and%20efficient%20real-world%20metric%20perception.%20We%20open-source%20MetricAnything%20at%20http%3A//metric-anything.github.io/metric-anything-io/%20to%20support%20community%20research.&entry.1838667208=http%3A//arxiv.org/abs/2601.22054v1&entry.124074799=Read"},
{"title": "Multimodal Visual Surrogate Compression for Alzheimer's Disease Classification", "author": "Dexuan Ding and Ciyuan Peng and Endrowednes Kuantama and Jingcai Guo and Jia Wu and Jian Yang and Amin Beheshti and Ming-Hsuan Yang and Yuankai Qi", "abstract": "High-dimensional structural MRI (sMRI) images are widely used for Alzheimer's Disease (AD) diagnosis. Most existing methods for sMRI representation learning rely on 3D architectures (e.g., 3D CNNs), slice-wise feature extraction with late aggregation, or apply training-free feature extractions using 2D foundation models (e.g., DINO). However, these three paradigms suffer from high computational cost, loss of cross-slice relations, and limited ability to extract discriminative features, respectively. To address these challenges, we propose Multimodal Visual Surrogate Compression (MVSC). It learns to compress and adapt large 3D sMRI volumes into compact 2D features, termed as visual surrogates, which are better aligned with frozen 2D foundation models to extract powerful representations for final AD classification. MVSC has two key components: a Volume Context Encoder that captures global cross-slice context under textual guidance, and an Adaptive Slice Fusion module that aggregates slice-level information in a text-enhanced, patch-wise manner. Extensive experiments on three large-scale Alzheimer's disease benchmarks demonstrate our MVSC performs favourably on both binary and multi-class classification tasks compared against state-of-the-art methods.", "link": "http://arxiv.org/abs/2601.21673v1", "date": "2026-01-29", "relevancy": 2.8654, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6067}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Visual%20Surrogate%20Compression%20for%20Alzheimer%27s%20Disease%20Classification&body=Title%3A%20Multimodal%20Visual%20Surrogate%20Compression%20for%20Alzheimer%27s%20Disease%20Classification%0AAuthor%3A%20Dexuan%20Ding%20and%20Ciyuan%20Peng%20and%20Endrowednes%20Kuantama%20and%20Jingcai%20Guo%20and%20Jia%20Wu%20and%20Jian%20Yang%20and%20Amin%20Beheshti%20and%20Ming-Hsuan%20Yang%20and%20Yuankai%20Qi%0AAbstract%3A%20High-dimensional%20structural%20MRI%20%28sMRI%29%20images%20are%20widely%20used%20for%20Alzheimer%27s%20Disease%20%28AD%29%20diagnosis.%20Most%20existing%20methods%20for%20sMRI%20representation%20learning%20rely%20on%203D%20architectures%20%28e.g.%2C%203D%20CNNs%29%2C%20slice-wise%20feature%20extraction%20with%20late%20aggregation%2C%20or%20apply%20training-free%20feature%20extractions%20using%202D%20foundation%20models%20%28e.g.%2C%20DINO%29.%20However%2C%20these%20three%20paradigms%20suffer%20from%20high%20computational%20cost%2C%20loss%20of%20cross-slice%20relations%2C%20and%20limited%20ability%20to%20extract%20discriminative%20features%2C%20respectively.%20To%20address%20these%20challenges%2C%20we%20propose%20Multimodal%20Visual%20Surrogate%20Compression%20%28MVSC%29.%20It%20learns%20to%20compress%20and%20adapt%20large%203D%20sMRI%20volumes%20into%20compact%202D%20features%2C%20termed%20as%20visual%20surrogates%2C%20which%20are%20better%20aligned%20with%20frozen%202D%20foundation%20models%20to%20extract%20powerful%20representations%20for%20final%20AD%20classification.%20MVSC%20has%20two%20key%20components%3A%20a%20Volume%20Context%20Encoder%20that%20captures%20global%20cross-slice%20context%20under%20textual%20guidance%2C%20and%20an%20Adaptive%20Slice%20Fusion%20module%20that%20aggregates%20slice-level%20information%20in%20a%20text-enhanced%2C%20patch-wise%20manner.%20Extensive%20experiments%20on%20three%20large-scale%20Alzheimer%27s%20disease%20benchmarks%20demonstrate%20our%20MVSC%20performs%20favourably%20on%20both%20binary%20and%20multi-class%20classification%20tasks%20compared%20against%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Visual%2520Surrogate%2520Compression%2520for%2520Alzheimer%2527s%2520Disease%2520Classification%26entry.906535625%3DDexuan%2520Ding%2520and%2520Ciyuan%2520Peng%2520and%2520Endrowednes%2520Kuantama%2520and%2520Jingcai%2520Guo%2520and%2520Jia%2520Wu%2520and%2520Jian%2520Yang%2520and%2520Amin%2520Beheshti%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Yuankai%2520Qi%26entry.1292438233%3DHigh-dimensional%2520structural%2520MRI%2520%2528sMRI%2529%2520images%2520are%2520widely%2520used%2520for%2520Alzheimer%2527s%2520Disease%2520%2528AD%2529%2520diagnosis.%2520Most%2520existing%2520methods%2520for%2520sMRI%2520representation%2520learning%2520rely%2520on%25203D%2520architectures%2520%2528e.g.%252C%25203D%2520CNNs%2529%252C%2520slice-wise%2520feature%2520extraction%2520with%2520late%2520aggregation%252C%2520or%2520apply%2520training-free%2520feature%2520extractions%2520using%25202D%2520foundation%2520models%2520%2528e.g.%252C%2520DINO%2529.%2520However%252C%2520these%2520three%2520paradigms%2520suffer%2520from%2520high%2520computational%2520cost%252C%2520loss%2520of%2520cross-slice%2520relations%252C%2520and%2520limited%2520ability%2520to%2520extract%2520discriminative%2520features%252C%2520respectively.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Multimodal%2520Visual%2520Surrogate%2520Compression%2520%2528MVSC%2529.%2520It%2520learns%2520to%2520compress%2520and%2520adapt%2520large%25203D%2520sMRI%2520volumes%2520into%2520compact%25202D%2520features%252C%2520termed%2520as%2520visual%2520surrogates%252C%2520which%2520are%2520better%2520aligned%2520with%2520frozen%25202D%2520foundation%2520models%2520to%2520extract%2520powerful%2520representations%2520for%2520final%2520AD%2520classification.%2520MVSC%2520has%2520two%2520key%2520components%253A%2520a%2520Volume%2520Context%2520Encoder%2520that%2520captures%2520global%2520cross-slice%2520context%2520under%2520textual%2520guidance%252C%2520and%2520an%2520Adaptive%2520Slice%2520Fusion%2520module%2520that%2520aggregates%2520slice-level%2520information%2520in%2520a%2520text-enhanced%252C%2520patch-wise%2520manner.%2520Extensive%2520experiments%2520on%2520three%2520large-scale%2520Alzheimer%2527s%2520disease%2520benchmarks%2520demonstrate%2520our%2520MVSC%2520performs%2520favourably%2520on%2520both%2520binary%2520and%2520multi-class%2520classification%2520tasks%2520compared%2520against%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Visual%20Surrogate%20Compression%20for%20Alzheimer%27s%20Disease%20Classification&entry.906535625=Dexuan%20Ding%20and%20Ciyuan%20Peng%20and%20Endrowednes%20Kuantama%20and%20Jingcai%20Guo%20and%20Jia%20Wu%20and%20Jian%20Yang%20and%20Amin%20Beheshti%20and%20Ming-Hsuan%20Yang%20and%20Yuankai%20Qi&entry.1292438233=High-dimensional%20structural%20MRI%20%28sMRI%29%20images%20are%20widely%20used%20for%20Alzheimer%27s%20Disease%20%28AD%29%20diagnosis.%20Most%20existing%20methods%20for%20sMRI%20representation%20learning%20rely%20on%203D%20architectures%20%28e.g.%2C%203D%20CNNs%29%2C%20slice-wise%20feature%20extraction%20with%20late%20aggregation%2C%20or%20apply%20training-free%20feature%20extractions%20using%202D%20foundation%20models%20%28e.g.%2C%20DINO%29.%20However%2C%20these%20three%20paradigms%20suffer%20from%20high%20computational%20cost%2C%20loss%20of%20cross-slice%20relations%2C%20and%20limited%20ability%20to%20extract%20discriminative%20features%2C%20respectively.%20To%20address%20these%20challenges%2C%20we%20propose%20Multimodal%20Visual%20Surrogate%20Compression%20%28MVSC%29.%20It%20learns%20to%20compress%20and%20adapt%20large%203D%20sMRI%20volumes%20into%20compact%202D%20features%2C%20termed%20as%20visual%20surrogates%2C%20which%20are%20better%20aligned%20with%20frozen%202D%20foundation%20models%20to%20extract%20powerful%20representations%20for%20final%20AD%20classification.%20MVSC%20has%20two%20key%20components%3A%20a%20Volume%20Context%20Encoder%20that%20captures%20global%20cross-slice%20context%20under%20textual%20guidance%2C%20and%20an%20Adaptive%20Slice%20Fusion%20module%20that%20aggregates%20slice-level%20information%20in%20a%20text-enhanced%2C%20patch-wise%20manner.%20Extensive%20experiments%20on%20three%20large-scale%20Alzheimer%27s%20disease%20benchmarks%20demonstrate%20our%20MVSC%20performs%20favourably%20on%20both%20binary%20and%20multi-class%20classification%20tasks%20compared%20against%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.21673v1&entry.124074799=Read"},
{"title": "DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning", "author": "Tianrun Xu and Haoda Jing and Ye Li and Yuquan Wei and Jun Feng and Guanyu Chen and Haichuan Gao and Tianren Zhang and Feng Chen", "abstract": "Recent advances in multimodal language models (MLLMs) have achieved remarkable progress in vision-language reasoning, especially with the emergence of \"thinking with images,\" which integrates explicit visual steps into the reasoning process. While this paradigm strengthens image-based reasoning, a significant challenge remains: models may arrive at correct answers by relying on irrelevant or spurious regions, driven by prior knowledge or dataset biases. Even when the answer is correct, flawed reasoning indicates that the model has not truly understood the image, highlighting the critical importance of reasoning fidelity in multimodal tasks. To address this issue, we propose DeFacto, a counterfactual reasoning framework that jointly enforces accurate answering and faithful reasoning. A key component of our approach is the design of three complementary training paradigms: (i) positive, (ii) counterfactual, and (iii) random-masking. To enable these paradigms, we develop a pipeline that automatically localizes question-relevant evidence and constructs positive, counterfactual, and random variants, resulting in a dataset of about 100k images. Building on this framework, we train multimodal language models with GRPO-based reinforcement learning, where we design three complementary rewards to guide the model toward accurate answering and evidence-grounded reasoning. Experiments on diverse benchmarks demonstrate that DeFacto substantially improves both answer accuracy and reasoning faithfulness, establishing a stronger foundation for interpretable multimodal reasoning. The code is available on GitHub and the dataset is released on HuggingFace.", "link": "http://arxiv.org/abs/2509.20912v2", "date": "2026-01-29", "relevancy": 2.8634, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5626}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeFacto%3A%20Counterfactual%20Thinking%20with%20Images%20for%20Enforcing%20Evidence-Grounded%20and%20Faithful%20Reasoning&body=Title%3A%20DeFacto%3A%20Counterfactual%20Thinking%20with%20Images%20for%20Enforcing%20Evidence-Grounded%20and%20Faithful%20Reasoning%0AAuthor%3A%20Tianrun%20Xu%20and%20Haoda%20Jing%20and%20Ye%20Li%20and%20Yuquan%20Wei%20and%20Jun%20Feng%20and%20Guanyu%20Chen%20and%20Haichuan%20Gao%20and%20Tianren%20Zhang%20and%20Feng%20Chen%0AAbstract%3A%20Recent%20advances%20in%20multimodal%20language%20models%20%28MLLMs%29%20have%20achieved%20remarkable%20progress%20in%20vision-language%20reasoning%2C%20especially%20with%20the%20emergence%20of%20%22thinking%20with%20images%2C%22%20which%20integrates%20explicit%20visual%20steps%20into%20the%20reasoning%20process.%20While%20this%20paradigm%20strengthens%20image-based%20reasoning%2C%20a%20significant%20challenge%20remains%3A%20models%20may%20arrive%20at%20correct%20answers%20by%20relying%20on%20irrelevant%20or%20spurious%20regions%2C%20driven%20by%20prior%20knowledge%20or%20dataset%20biases.%20Even%20when%20the%20answer%20is%20correct%2C%20flawed%20reasoning%20indicates%20that%20the%20model%20has%20not%20truly%20understood%20the%20image%2C%20highlighting%20the%20critical%20importance%20of%20reasoning%20fidelity%20in%20multimodal%20tasks.%20To%20address%20this%20issue%2C%20we%20propose%20DeFacto%2C%20a%20counterfactual%20reasoning%20framework%20that%20jointly%20enforces%20accurate%20answering%20and%20faithful%20reasoning.%20A%20key%20component%20of%20our%20approach%20is%20the%20design%20of%20three%20complementary%20training%20paradigms%3A%20%28i%29%20positive%2C%20%28ii%29%20counterfactual%2C%20and%20%28iii%29%20random-masking.%20To%20enable%20these%20paradigms%2C%20we%20develop%20a%20pipeline%20that%20automatically%20localizes%20question-relevant%20evidence%20and%20constructs%20positive%2C%20counterfactual%2C%20and%20random%20variants%2C%20resulting%20in%20a%20dataset%20of%20about%20100k%20images.%20Building%20on%20this%20framework%2C%20we%20train%20multimodal%20language%20models%20with%20GRPO-based%20reinforcement%20learning%2C%20where%20we%20design%20three%20complementary%20rewards%20to%20guide%20the%20model%20toward%20accurate%20answering%20and%20evidence-grounded%20reasoning.%20Experiments%20on%20diverse%20benchmarks%20demonstrate%20that%20DeFacto%20substantially%20improves%20both%20answer%20accuracy%20and%20reasoning%20faithfulness%2C%20establishing%20a%20stronger%20foundation%20for%20interpretable%20multimodal%20reasoning.%20The%20code%20is%20available%20on%20GitHub%20and%20the%20dataset%20is%20released%20on%20HuggingFace.%0ALink%3A%20http%3A//arxiv.org/abs/2509.20912v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeFacto%253A%2520Counterfactual%2520Thinking%2520with%2520Images%2520for%2520Enforcing%2520Evidence-Grounded%2520and%2520Faithful%2520Reasoning%26entry.906535625%3DTianrun%2520Xu%2520and%2520Haoda%2520Jing%2520and%2520Ye%2520Li%2520and%2520Yuquan%2520Wei%2520and%2520Jun%2520Feng%2520and%2520Guanyu%2520Chen%2520and%2520Haichuan%2520Gao%2520and%2520Tianren%2520Zhang%2520and%2520Feng%2520Chen%26entry.1292438233%3DRecent%2520advances%2520in%2520multimodal%2520language%2520models%2520%2528MLLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520in%2520vision-language%2520reasoning%252C%2520especially%2520with%2520the%2520emergence%2520of%2520%2522thinking%2520with%2520images%252C%2522%2520which%2520integrates%2520explicit%2520visual%2520steps%2520into%2520the%2520reasoning%2520process.%2520While%2520this%2520paradigm%2520strengthens%2520image-based%2520reasoning%252C%2520a%2520significant%2520challenge%2520remains%253A%2520models%2520may%2520arrive%2520at%2520correct%2520answers%2520by%2520relying%2520on%2520irrelevant%2520or%2520spurious%2520regions%252C%2520driven%2520by%2520prior%2520knowledge%2520or%2520dataset%2520biases.%2520Even%2520when%2520the%2520answer%2520is%2520correct%252C%2520flawed%2520reasoning%2520indicates%2520that%2520the%2520model%2520has%2520not%2520truly%2520understood%2520the%2520image%252C%2520highlighting%2520the%2520critical%2520importance%2520of%2520reasoning%2520fidelity%2520in%2520multimodal%2520tasks.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520DeFacto%252C%2520a%2520counterfactual%2520reasoning%2520framework%2520that%2520jointly%2520enforces%2520accurate%2520answering%2520and%2520faithful%2520reasoning.%2520A%2520key%2520component%2520of%2520our%2520approach%2520is%2520the%2520design%2520of%2520three%2520complementary%2520training%2520paradigms%253A%2520%2528i%2529%2520positive%252C%2520%2528ii%2529%2520counterfactual%252C%2520and%2520%2528iii%2529%2520random-masking.%2520To%2520enable%2520these%2520paradigms%252C%2520we%2520develop%2520a%2520pipeline%2520that%2520automatically%2520localizes%2520question-relevant%2520evidence%2520and%2520constructs%2520positive%252C%2520counterfactual%252C%2520and%2520random%2520variants%252C%2520resulting%2520in%2520a%2520dataset%2520of%2520about%2520100k%2520images.%2520Building%2520on%2520this%2520framework%252C%2520we%2520train%2520multimodal%2520language%2520models%2520with%2520GRPO-based%2520reinforcement%2520learning%252C%2520where%2520we%2520design%2520three%2520complementary%2520rewards%2520to%2520guide%2520the%2520model%2520toward%2520accurate%2520answering%2520and%2520evidence-grounded%2520reasoning.%2520Experiments%2520on%2520diverse%2520benchmarks%2520demonstrate%2520that%2520DeFacto%2520substantially%2520improves%2520both%2520answer%2520accuracy%2520and%2520reasoning%2520faithfulness%252C%2520establishing%2520a%2520stronger%2520foundation%2520for%2520interpretable%2520multimodal%2520reasoning.%2520The%2520code%2520is%2520available%2520on%2520GitHub%2520and%2520the%2520dataset%2520is%2520released%2520on%2520HuggingFace.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20912v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeFacto%3A%20Counterfactual%20Thinking%20with%20Images%20for%20Enforcing%20Evidence-Grounded%20and%20Faithful%20Reasoning&entry.906535625=Tianrun%20Xu%20and%20Haoda%20Jing%20and%20Ye%20Li%20and%20Yuquan%20Wei%20and%20Jun%20Feng%20and%20Guanyu%20Chen%20and%20Haichuan%20Gao%20and%20Tianren%20Zhang%20and%20Feng%20Chen&entry.1292438233=Recent%20advances%20in%20multimodal%20language%20models%20%28MLLMs%29%20have%20achieved%20remarkable%20progress%20in%20vision-language%20reasoning%2C%20especially%20with%20the%20emergence%20of%20%22thinking%20with%20images%2C%22%20which%20integrates%20explicit%20visual%20steps%20into%20the%20reasoning%20process.%20While%20this%20paradigm%20strengthens%20image-based%20reasoning%2C%20a%20significant%20challenge%20remains%3A%20models%20may%20arrive%20at%20correct%20answers%20by%20relying%20on%20irrelevant%20or%20spurious%20regions%2C%20driven%20by%20prior%20knowledge%20or%20dataset%20biases.%20Even%20when%20the%20answer%20is%20correct%2C%20flawed%20reasoning%20indicates%20that%20the%20model%20has%20not%20truly%20understood%20the%20image%2C%20highlighting%20the%20critical%20importance%20of%20reasoning%20fidelity%20in%20multimodal%20tasks.%20To%20address%20this%20issue%2C%20we%20propose%20DeFacto%2C%20a%20counterfactual%20reasoning%20framework%20that%20jointly%20enforces%20accurate%20answering%20and%20faithful%20reasoning.%20A%20key%20component%20of%20our%20approach%20is%20the%20design%20of%20three%20complementary%20training%20paradigms%3A%20%28i%29%20positive%2C%20%28ii%29%20counterfactual%2C%20and%20%28iii%29%20random-masking.%20To%20enable%20these%20paradigms%2C%20we%20develop%20a%20pipeline%20that%20automatically%20localizes%20question-relevant%20evidence%20and%20constructs%20positive%2C%20counterfactual%2C%20and%20random%20variants%2C%20resulting%20in%20a%20dataset%20of%20about%20100k%20images.%20Building%20on%20this%20framework%2C%20we%20train%20multimodal%20language%20models%20with%20GRPO-based%20reinforcement%20learning%2C%20where%20we%20design%20three%20complementary%20rewards%20to%20guide%20the%20model%20toward%20accurate%20answering%20and%20evidence-grounded%20reasoning.%20Experiments%20on%20diverse%20benchmarks%20demonstrate%20that%20DeFacto%20substantially%20improves%20both%20answer%20accuracy%20and%20reasoning%20faithfulness%2C%20establishing%20a%20stronger%20foundation%20for%20interpretable%20multimodal%20reasoning.%20The%20code%20is%20available%20on%20GitHub%20and%20the%20dataset%20is%20released%20on%20HuggingFace.&entry.1838667208=http%3A//arxiv.org/abs/2509.20912v2&entry.124074799=Read"},
{"title": "MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods", "author": "Honglin Lin and Zheng Liu and Yun Zhu and Chonghan Qin and Juekai Lin and Xiaoran Shang and Conghui He and Wentao Zhang and Lijun Wu", "abstract": "Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a \"less is more\" phenomenon via our difficulty-aware filtering strategy: a subset of just 7\\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.", "link": "http://arxiv.org/abs/2601.21821v1", "date": "2026-01-29", "relevancy": 2.8015, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5758}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5758}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMFineReason%3A%20Closing%20the%20Multimodal%20Reasoning%20Gap%20via%20Open%20Data-Centric%20Methods&body=Title%3A%20MMFineReason%3A%20Closing%20the%20Multimodal%20Reasoning%20Gap%20via%20Open%20Data-Centric%20Methods%0AAuthor%3A%20Honglin%20Lin%20and%20Zheng%20Liu%20and%20Yun%20Zhu%20and%20Chonghan%20Qin%20and%20Juekai%20Lin%20and%20Xiaoran%20Shang%20and%20Conghui%20He%20and%20Wentao%20Zhang%20and%20Lijun%20Wu%0AAbstract%3A%20Recent%20advances%20in%20Vision%20Language%20Models%20%28VLMs%29%20have%20driven%20significant%20progress%20in%20visual%20reasoning.%20However%2C%20open-source%20VLMs%20still%20lag%20behind%20proprietary%20systems%2C%20largely%20due%20to%20the%20lack%20of%20high-quality%20reasoning%20data.%20Existing%20datasets%20offer%20limited%20coverage%20of%20challenging%20domains%20such%20as%20STEM%20diagrams%20and%20visual%20puzzles%2C%20and%20lack%20consistent%2C%20long-form%20Chain-of-Thought%20%28CoT%29%20annotations%20essential%20for%20eliciting%20strong%20reasoning%20capabilities.%20To%20bridge%20this%20gap%2C%20we%20introduce%20MMFineReason%2C%20a%20large-scale%20multimodal%20reasoning%20dataset%20comprising%201.8M%20samples%20and%205.1B%20solution%20tokens%2C%20featuring%20high-quality%20reasoning%20annotations%20distilled%20from%20Qwen3-VL-235B-A22B-Thinking.%20The%20dataset%20is%20established%20via%20a%20systematic%20three-stage%20pipeline%3A%20%281%29%20large-scale%20data%20collection%20and%20standardization%2C%20%282%29%20CoT%20rationale%20generation%2C%20and%20%283%29%20comprehensive%20selection%20based%20on%20reasoning%20quality%20and%20difficulty%20awareness.%20The%20resulting%20dataset%20spans%20STEM%20problems%2C%20visual%20puzzles%2C%20games%2C%20and%20complex%20diagrams%2C%20with%20each%20sample%20annotated%20with%20visually%20grounded%20reasoning%20traces.%20We%20fine-tune%20Qwen3-VL-Instruct%20on%20MMFineReason%20to%20develop%20MMFineReason-2B/4B/8B%20versions.%20Our%20models%20establish%20new%20state-of-the-art%20results%20for%20their%20size%20class.%20Notably%2C%20MMFineReason-4B%20succesfully%20surpasses%20Qwen3-VL-8B-Thinking%2C%20and%20MMFineReason-8B%20even%20outperforms%20Qwen3-VL-30B-A3B-Thinking%20while%20approaching%20Qwen3-VL-32B-Thinking%2C%20demonstrating%20remarkable%20parameter%20efficiency.%20Crucially%2C%20we%20uncover%20a%20%22less%20is%20more%22%20phenomenon%20via%20our%20difficulty-aware%20filtering%20strategy%3A%20a%20subset%20of%20just%207%5C%25%20%28123K%20samples%29%20achieves%20performance%20comparable%20to%20the%20full%20dataset.%20Notably%2C%20we%20reveal%20a%20synergistic%20effect%20where%20reasoning-oriented%20data%20composition%20simultaneously%20boosts%20general%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMFineReason%253A%2520Closing%2520the%2520Multimodal%2520Reasoning%2520Gap%2520via%2520Open%2520Data-Centric%2520Methods%26entry.906535625%3DHonglin%2520Lin%2520and%2520Zheng%2520Liu%2520and%2520Yun%2520Zhu%2520and%2520Chonghan%2520Qin%2520and%2520Juekai%2520Lin%2520and%2520Xiaoran%2520Shang%2520and%2520Conghui%2520He%2520and%2520Wentao%2520Zhang%2520and%2520Lijun%2520Wu%26entry.1292438233%3DRecent%2520advances%2520in%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520driven%2520significant%2520progress%2520in%2520visual%2520reasoning.%2520However%252C%2520open-source%2520VLMs%2520still%2520lag%2520behind%2520proprietary%2520systems%252C%2520largely%2520due%2520to%2520the%2520lack%2520of%2520high-quality%2520reasoning%2520data.%2520Existing%2520datasets%2520offer%2520limited%2520coverage%2520of%2520challenging%2520domains%2520such%2520as%2520STEM%2520diagrams%2520and%2520visual%2520puzzles%252C%2520and%2520lack%2520consistent%252C%2520long-form%2520Chain-of-Thought%2520%2528CoT%2529%2520annotations%2520essential%2520for%2520eliciting%2520strong%2520reasoning%2520capabilities.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520MMFineReason%252C%2520a%2520large-scale%2520multimodal%2520reasoning%2520dataset%2520comprising%25201.8M%2520samples%2520and%25205.1B%2520solution%2520tokens%252C%2520featuring%2520high-quality%2520reasoning%2520annotations%2520distilled%2520from%2520Qwen3-VL-235B-A22B-Thinking.%2520The%2520dataset%2520is%2520established%2520via%2520a%2520systematic%2520three-stage%2520pipeline%253A%2520%25281%2529%2520large-scale%2520data%2520collection%2520and%2520standardization%252C%2520%25282%2529%2520CoT%2520rationale%2520generation%252C%2520and%2520%25283%2529%2520comprehensive%2520selection%2520based%2520on%2520reasoning%2520quality%2520and%2520difficulty%2520awareness.%2520The%2520resulting%2520dataset%2520spans%2520STEM%2520problems%252C%2520visual%2520puzzles%252C%2520games%252C%2520and%2520complex%2520diagrams%252C%2520with%2520each%2520sample%2520annotated%2520with%2520visually%2520grounded%2520reasoning%2520traces.%2520We%2520fine-tune%2520Qwen3-VL-Instruct%2520on%2520MMFineReason%2520to%2520develop%2520MMFineReason-2B/4B/8B%2520versions.%2520Our%2520models%2520establish%2520new%2520state-of-the-art%2520results%2520for%2520their%2520size%2520class.%2520Notably%252C%2520MMFineReason-4B%2520succesfully%2520surpasses%2520Qwen3-VL-8B-Thinking%252C%2520and%2520MMFineReason-8B%2520even%2520outperforms%2520Qwen3-VL-30B-A3B-Thinking%2520while%2520approaching%2520Qwen3-VL-32B-Thinking%252C%2520demonstrating%2520remarkable%2520parameter%2520efficiency.%2520Crucially%252C%2520we%2520uncover%2520a%2520%2522less%2520is%2520more%2522%2520phenomenon%2520via%2520our%2520difficulty-aware%2520filtering%2520strategy%253A%2520a%2520subset%2520of%2520just%25207%255C%2525%2520%2528123K%2520samples%2529%2520achieves%2520performance%2520comparable%2520to%2520the%2520full%2520dataset.%2520Notably%252C%2520we%2520reveal%2520a%2520synergistic%2520effect%2520where%2520reasoning-oriented%2520data%2520composition%2520simultaneously%2520boosts%2520general%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMFineReason%3A%20Closing%20the%20Multimodal%20Reasoning%20Gap%20via%20Open%20Data-Centric%20Methods&entry.906535625=Honglin%20Lin%20and%20Zheng%20Liu%20and%20Yun%20Zhu%20and%20Chonghan%20Qin%20and%20Juekai%20Lin%20and%20Xiaoran%20Shang%20and%20Conghui%20He%20and%20Wentao%20Zhang%20and%20Lijun%20Wu&entry.1292438233=Recent%20advances%20in%20Vision%20Language%20Models%20%28VLMs%29%20have%20driven%20significant%20progress%20in%20visual%20reasoning.%20However%2C%20open-source%20VLMs%20still%20lag%20behind%20proprietary%20systems%2C%20largely%20due%20to%20the%20lack%20of%20high-quality%20reasoning%20data.%20Existing%20datasets%20offer%20limited%20coverage%20of%20challenging%20domains%20such%20as%20STEM%20diagrams%20and%20visual%20puzzles%2C%20and%20lack%20consistent%2C%20long-form%20Chain-of-Thought%20%28CoT%29%20annotations%20essential%20for%20eliciting%20strong%20reasoning%20capabilities.%20To%20bridge%20this%20gap%2C%20we%20introduce%20MMFineReason%2C%20a%20large-scale%20multimodal%20reasoning%20dataset%20comprising%201.8M%20samples%20and%205.1B%20solution%20tokens%2C%20featuring%20high-quality%20reasoning%20annotations%20distilled%20from%20Qwen3-VL-235B-A22B-Thinking.%20The%20dataset%20is%20established%20via%20a%20systematic%20three-stage%20pipeline%3A%20%281%29%20large-scale%20data%20collection%20and%20standardization%2C%20%282%29%20CoT%20rationale%20generation%2C%20and%20%283%29%20comprehensive%20selection%20based%20on%20reasoning%20quality%20and%20difficulty%20awareness.%20The%20resulting%20dataset%20spans%20STEM%20problems%2C%20visual%20puzzles%2C%20games%2C%20and%20complex%20diagrams%2C%20with%20each%20sample%20annotated%20with%20visually%20grounded%20reasoning%20traces.%20We%20fine-tune%20Qwen3-VL-Instruct%20on%20MMFineReason%20to%20develop%20MMFineReason-2B/4B/8B%20versions.%20Our%20models%20establish%20new%20state-of-the-art%20results%20for%20their%20size%20class.%20Notably%2C%20MMFineReason-4B%20succesfully%20surpasses%20Qwen3-VL-8B-Thinking%2C%20and%20MMFineReason-8B%20even%20outperforms%20Qwen3-VL-30B-A3B-Thinking%20while%20approaching%20Qwen3-VL-32B-Thinking%2C%20demonstrating%20remarkable%20parameter%20efficiency.%20Crucially%2C%20we%20uncover%20a%20%22less%20is%20more%22%20phenomenon%20via%20our%20difficulty-aware%20filtering%20strategy%3A%20a%20subset%20of%20just%207%5C%25%20%28123K%20samples%29%20achieves%20performance%20comparable%20to%20the%20full%20dataset.%20Notably%2C%20we%20reveal%20a%20synergistic%20effect%20where%20reasoning-oriented%20data%20composition%20simultaneously%20boosts%20general%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2601.21821v1&entry.124074799=Read"},
{"title": "Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions", "author": "Xiaoxiao Sun and Mingyang Li and Kun yuan and Min Woo Sun and Mark Endo and Shengguang Wu and Changlin Li and Yuhui Zhang and Zeyu Wang and Serena Yeung-Levy", "abstract": "Large Vision-Language Models (VLMs) often answer classic visual illusions \"correctly\" on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.", "link": "http://arxiv.org/abs/2601.22150v1", "date": "2026-01-29", "relevancy": 2.799, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5673}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5673}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20VLMs%20Perceive%20or%20Recall%3F%20Probing%20Visual%20Perception%20vs.%20Memory%20with%20Classic%20Visual%20Illusions&body=Title%3A%20Do%20VLMs%20Perceive%20or%20Recall%3F%20Probing%20Visual%20Perception%20vs.%20Memory%20with%20Classic%20Visual%20Illusions%0AAuthor%3A%20Xiaoxiao%20Sun%20and%20Mingyang%20Li%20and%20Kun%20yuan%20and%20Min%20Woo%20Sun%20and%20Mark%20Endo%20and%20Shengguang%20Wu%20and%20Changlin%20Li%20and%20Yuhui%20Zhang%20and%20Zeyu%20Wang%20and%20Serena%20Yeung-Levy%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28VLMs%29%20often%20answer%20classic%20visual%20illusions%20%22correctly%22%20on%20original%20images%2C%20yet%20persist%20with%20the%20same%20responses%20when%20illusion%20factors%20are%20inverted%2C%20even%20though%20the%20visual%20change%20is%20obvious%20to%20humans.%20This%20raises%20a%20fundamental%20question%3A%20do%20VLMs%20perceive%20visual%20changes%20or%20merely%20recall%20memorized%20patterns%3F%20While%20several%20studies%20have%20noted%20this%20phenomenon%2C%20the%20underlying%20causes%20remain%20unclear.%20To%20move%20from%20observations%20to%20systematic%20understanding%2C%20this%20paper%20introduces%20VI-Probe%2C%20a%20controllable%20visual-illusion%20framework%20with%20graded%20perturbations%20and%20matched%20visual%20controls%20%28without%20illusion%20inducer%29%20that%20disentangles%20visually%20grounded%20perception%20from%20language-driven%20recall.%20Unlike%20prior%20work%20that%20focuses%20on%20averaged%20accuracy%2C%20we%20measure%20stability%20and%20sensitivity%20using%20Polarity-Flip%20Consistency%2C%20Template%20Fixation%20Index%2C%20and%20an%20illusion%20multiplier%20normalized%20against%20matched%20controls.%20Experiments%20across%20different%20families%20reveal%20that%20response%20persistence%20arises%20from%20heterogeneous%20causes%20rather%20than%20a%20single%20mechanism.%20For%20instance%2C%20GPT-5%20exhibits%20memory%20override%2C%20Claude-Opus-4.1%20shows%20perception-memory%20competition%2C%20while%20Qwen%20variants%20suggest%20visual-processing%20limits.%20Our%20findings%20challenge%20single-cause%20views%20and%20motivate%20probing-based%20evaluation%20that%20measures%20both%20knowledge%20and%20sensitivity%20to%20controlled%20visual%20change.%20Data%20and%20code%20are%20available%20at%20https%3A//sites.google.com/view/vi-probe/.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520VLMs%2520Perceive%2520or%2520Recall%253F%2520Probing%2520Visual%2520Perception%2520vs.%2520Memory%2520with%2520Classic%2520Visual%2520Illusions%26entry.906535625%3DXiaoxiao%2520Sun%2520and%2520Mingyang%2520Li%2520and%2520Kun%2520yuan%2520and%2520Min%2520Woo%2520Sun%2520and%2520Mark%2520Endo%2520and%2520Shengguang%2520Wu%2520and%2520Changlin%2520Li%2520and%2520Yuhui%2520Zhang%2520and%2520Zeyu%2520Wang%2520and%2520Serena%2520Yeung-Levy%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520often%2520answer%2520classic%2520visual%2520illusions%2520%2522correctly%2522%2520on%2520original%2520images%252C%2520yet%2520persist%2520with%2520the%2520same%2520responses%2520when%2520illusion%2520factors%2520are%2520inverted%252C%2520even%2520though%2520the%2520visual%2520change%2520is%2520obvious%2520to%2520humans.%2520This%2520raises%2520a%2520fundamental%2520question%253A%2520do%2520VLMs%2520perceive%2520visual%2520changes%2520or%2520merely%2520recall%2520memorized%2520patterns%253F%2520While%2520several%2520studies%2520have%2520noted%2520this%2520phenomenon%252C%2520the%2520underlying%2520causes%2520remain%2520unclear.%2520To%2520move%2520from%2520observations%2520to%2520systematic%2520understanding%252C%2520this%2520paper%2520introduces%2520VI-Probe%252C%2520a%2520controllable%2520visual-illusion%2520framework%2520with%2520graded%2520perturbations%2520and%2520matched%2520visual%2520controls%2520%2528without%2520illusion%2520inducer%2529%2520that%2520disentangles%2520visually%2520grounded%2520perception%2520from%2520language-driven%2520recall.%2520Unlike%2520prior%2520work%2520that%2520focuses%2520on%2520averaged%2520accuracy%252C%2520we%2520measure%2520stability%2520and%2520sensitivity%2520using%2520Polarity-Flip%2520Consistency%252C%2520Template%2520Fixation%2520Index%252C%2520and%2520an%2520illusion%2520multiplier%2520normalized%2520against%2520matched%2520controls.%2520Experiments%2520across%2520different%2520families%2520reveal%2520that%2520response%2520persistence%2520arises%2520from%2520heterogeneous%2520causes%2520rather%2520than%2520a%2520single%2520mechanism.%2520For%2520instance%252C%2520GPT-5%2520exhibits%2520memory%2520override%252C%2520Claude-Opus-4.1%2520shows%2520perception-memory%2520competition%252C%2520while%2520Qwen%2520variants%2520suggest%2520visual-processing%2520limits.%2520Our%2520findings%2520challenge%2520single-cause%2520views%2520and%2520motivate%2520probing-based%2520evaluation%2520that%2520measures%2520both%2520knowledge%2520and%2520sensitivity%2520to%2520controlled%2520visual%2520change.%2520Data%2520and%2520code%2520are%2520available%2520at%2520https%253A//sites.google.com/view/vi-probe/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20VLMs%20Perceive%20or%20Recall%3F%20Probing%20Visual%20Perception%20vs.%20Memory%20with%20Classic%20Visual%20Illusions&entry.906535625=Xiaoxiao%20Sun%20and%20Mingyang%20Li%20and%20Kun%20yuan%20and%20Min%20Woo%20Sun%20and%20Mark%20Endo%20and%20Shengguang%20Wu%20and%20Changlin%20Li%20and%20Yuhui%20Zhang%20and%20Zeyu%20Wang%20and%20Serena%20Yeung-Levy&entry.1292438233=Large%20Vision-Language%20Models%20%28VLMs%29%20often%20answer%20classic%20visual%20illusions%20%22correctly%22%20on%20original%20images%2C%20yet%20persist%20with%20the%20same%20responses%20when%20illusion%20factors%20are%20inverted%2C%20even%20though%20the%20visual%20change%20is%20obvious%20to%20humans.%20This%20raises%20a%20fundamental%20question%3A%20do%20VLMs%20perceive%20visual%20changes%20or%20merely%20recall%20memorized%20patterns%3F%20While%20several%20studies%20have%20noted%20this%20phenomenon%2C%20the%20underlying%20causes%20remain%20unclear.%20To%20move%20from%20observations%20to%20systematic%20understanding%2C%20this%20paper%20introduces%20VI-Probe%2C%20a%20controllable%20visual-illusion%20framework%20with%20graded%20perturbations%20and%20matched%20visual%20controls%20%28without%20illusion%20inducer%29%20that%20disentangles%20visually%20grounded%20perception%20from%20language-driven%20recall.%20Unlike%20prior%20work%20that%20focuses%20on%20averaged%20accuracy%2C%20we%20measure%20stability%20and%20sensitivity%20using%20Polarity-Flip%20Consistency%2C%20Template%20Fixation%20Index%2C%20and%20an%20illusion%20multiplier%20normalized%20against%20matched%20controls.%20Experiments%20across%20different%20families%20reveal%20that%20response%20persistence%20arises%20from%20heterogeneous%20causes%20rather%20than%20a%20single%20mechanism.%20For%20instance%2C%20GPT-5%20exhibits%20memory%20override%2C%20Claude-Opus-4.1%20shows%20perception-memory%20competition%2C%20while%20Qwen%20variants%20suggest%20visual-processing%20limits.%20Our%20findings%20challenge%20single-cause%20views%20and%20motivate%20probing-based%20evaluation%20that%20measures%20both%20knowledge%20and%20sensitivity%20to%20controlled%20visual%20change.%20Data%20and%20code%20are%20available%20at%20https%3A//sites.google.com/view/vi-probe/.&entry.1838667208=http%3A//arxiv.org/abs/2601.22150v1&entry.124074799=Read"},
{"title": "Geometry-Aware Deep Congruence Networks for Manifold Learning in Cross-Subject Motor Imagery", "author": "Sanjeev Manivannan and Chandrashekar Lakshminarayan", "abstract": "Cross-subject motor-imagery decoding remains a major challenge in EEG-based brain-computer interfaces. To mitigate strong inter-subject variability, recent work has emphasized manifold-based approaches operating on covariance representations. Yet dispersion scaling and orientation alignment remain largely unaddressed in existing methods. In this paper, we address both issues through congruence transforms and introduce three complementary geometry-aware models: (i) Discriminative Congruence Transform (DCT), (ii) Deep Linear DCT (DLDCT), and (iii) Deep DCT-UNet (DDCT-UNet). These models are evaluated both as pre-alignment modules for downstream classifiers and as end-to-end discriminative systems trained via cross-entropy backpropagation with a custom logistic-regression head. Across challenging motor-imagery benchmarks, the proposed framework improves transductive cross-subject accuracy by 2-3%, demonstrating the value of geometry-aware congruence learning.", "link": "http://arxiv.org/abs/2511.18940v2", "date": "2026-01-29", "relevancy": 2.797, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6014}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5386}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Aware%20Deep%20Congruence%20Networks%20for%20Manifold%20Learning%20in%20Cross-Subject%20Motor%20Imagery&body=Title%3A%20Geometry-Aware%20Deep%20Congruence%20Networks%20for%20Manifold%20Learning%20in%20Cross-Subject%20Motor%20Imagery%0AAuthor%3A%20Sanjeev%20Manivannan%20and%20Chandrashekar%20Lakshminarayan%0AAbstract%3A%20Cross-subject%20motor-imagery%20decoding%20remains%20a%20major%20challenge%20in%20EEG-based%20brain-computer%20interfaces.%20To%20mitigate%20strong%20inter-subject%20variability%2C%20recent%20work%20has%20emphasized%20manifold-based%20approaches%20operating%20on%20covariance%20representations.%20Yet%20dispersion%20scaling%20and%20orientation%20alignment%20remain%20largely%20unaddressed%20in%20existing%20methods.%20In%20this%20paper%2C%20we%20address%20both%20issues%20through%20congruence%20transforms%20and%20introduce%20three%20complementary%20geometry-aware%20models%3A%20%28i%29%20Discriminative%20Congruence%20Transform%20%28DCT%29%2C%20%28ii%29%20Deep%20Linear%20DCT%20%28DLDCT%29%2C%20and%20%28iii%29%20Deep%20DCT-UNet%20%28DDCT-UNet%29.%20These%20models%20are%20evaluated%20both%20as%20pre-alignment%20modules%20for%20downstream%20classifiers%20and%20as%20end-to-end%20discriminative%20systems%20trained%20via%20cross-entropy%20backpropagation%20with%20a%20custom%20logistic-regression%20head.%20Across%20challenging%20motor-imagery%20benchmarks%2C%20the%20proposed%20framework%20improves%20transductive%20cross-subject%20accuracy%20by%202-3%25%2C%20demonstrating%20the%20value%20of%20geometry-aware%20congruence%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18940v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Aware%2520Deep%2520Congruence%2520Networks%2520for%2520Manifold%2520Learning%2520in%2520Cross-Subject%2520Motor%2520Imagery%26entry.906535625%3DSanjeev%2520Manivannan%2520and%2520Chandrashekar%2520Lakshminarayan%26entry.1292438233%3DCross-subject%2520motor-imagery%2520decoding%2520remains%2520a%2520major%2520challenge%2520in%2520EEG-based%2520brain-computer%2520interfaces.%2520To%2520mitigate%2520strong%2520inter-subject%2520variability%252C%2520recent%2520work%2520has%2520emphasized%2520manifold-based%2520approaches%2520operating%2520on%2520covariance%2520representations.%2520Yet%2520dispersion%2520scaling%2520and%2520orientation%2520alignment%2520remain%2520largely%2520unaddressed%2520in%2520existing%2520methods.%2520In%2520this%2520paper%252C%2520we%2520address%2520both%2520issues%2520through%2520congruence%2520transforms%2520and%2520introduce%2520three%2520complementary%2520geometry-aware%2520models%253A%2520%2528i%2529%2520Discriminative%2520Congruence%2520Transform%2520%2528DCT%2529%252C%2520%2528ii%2529%2520Deep%2520Linear%2520DCT%2520%2528DLDCT%2529%252C%2520and%2520%2528iii%2529%2520Deep%2520DCT-UNet%2520%2528DDCT-UNet%2529.%2520These%2520models%2520are%2520evaluated%2520both%2520as%2520pre-alignment%2520modules%2520for%2520downstream%2520classifiers%2520and%2520as%2520end-to-end%2520discriminative%2520systems%2520trained%2520via%2520cross-entropy%2520backpropagation%2520with%2520a%2520custom%2520logistic-regression%2520head.%2520Across%2520challenging%2520motor-imagery%2520benchmarks%252C%2520the%2520proposed%2520framework%2520improves%2520transductive%2520cross-subject%2520accuracy%2520by%25202-3%2525%252C%2520demonstrating%2520the%2520value%2520of%2520geometry-aware%2520congruence%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18940v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Aware%20Deep%20Congruence%20Networks%20for%20Manifold%20Learning%20in%20Cross-Subject%20Motor%20Imagery&entry.906535625=Sanjeev%20Manivannan%20and%20Chandrashekar%20Lakshminarayan&entry.1292438233=Cross-subject%20motor-imagery%20decoding%20remains%20a%20major%20challenge%20in%20EEG-based%20brain-computer%20interfaces.%20To%20mitigate%20strong%20inter-subject%20variability%2C%20recent%20work%20has%20emphasized%20manifold-based%20approaches%20operating%20on%20covariance%20representations.%20Yet%20dispersion%20scaling%20and%20orientation%20alignment%20remain%20largely%20unaddressed%20in%20existing%20methods.%20In%20this%20paper%2C%20we%20address%20both%20issues%20through%20congruence%20transforms%20and%20introduce%20three%20complementary%20geometry-aware%20models%3A%20%28i%29%20Discriminative%20Congruence%20Transform%20%28DCT%29%2C%20%28ii%29%20Deep%20Linear%20DCT%20%28DLDCT%29%2C%20and%20%28iii%29%20Deep%20DCT-UNet%20%28DDCT-UNet%29.%20These%20models%20are%20evaluated%20both%20as%20pre-alignment%20modules%20for%20downstream%20classifiers%20and%20as%20end-to-end%20discriminative%20systems%20trained%20via%20cross-entropy%20backpropagation%20with%20a%20custom%20logistic-regression%20head.%20Across%20challenging%20motor-imagery%20benchmarks%2C%20the%20proposed%20framework%20improves%20transductive%20cross-subject%20accuracy%20by%202-3%25%2C%20demonstrating%20the%20value%20of%20geometry-aware%20congruence%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2511.18940v2&entry.124074799=Read"},
{"title": "Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion", "author": "Da Li and Chen Yao and Tong Mao and Jiacheng Bao and Houjun Sun", "abstract": "Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.", "link": "http://arxiv.org/abs/2601.22045v1", "date": "2026-01-29", "relevancy": 2.7874, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6059}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5442}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Urban%20Neural%20Surface%20Reconstruction%20from%20Constrained%20Sparse%20Aerial%20Imagery%20with%203D%20SAR%20Fusion&body=Title%3A%20Urban%20Neural%20Surface%20Reconstruction%20from%20Constrained%20Sparse%20Aerial%20Imagery%20with%203D%20SAR%20Fusion%0AAuthor%3A%20Da%20Li%20and%20Chen%20Yao%20and%20Tong%20Mao%20and%20Jiacheng%20Bao%20and%20Houjun%20Sun%0AAbstract%3A%20Neural%20surface%20reconstruction%20%28NSR%29%20has%20recently%20shown%20strong%20potential%20for%20urban%203D%20reconstruction%20from%20multi-view%20aerial%20imagery.%20However%2C%20existing%20NSR%20methods%20often%20suffer%20from%20geometric%20ambiguity%20and%20instability%2C%20particularly%20under%20sparse-view%20conditions.%20This%20issue%20is%20critical%20in%20large-scale%20urban%20remote%20sensing%2C%20where%20aerial%20image%20acquisition%20is%20limited%20by%20flight%20paths%2C%20terrain%2C%20and%20cost.%20To%20address%20this%20challenge%2C%20we%20present%20the%20first%20urban%20NSR%20framework%20that%20fuses%203D%20synthetic%20aperture%20radar%20%28SAR%29%20point%20clouds%20with%20aerial%20imagery%20for%20high-fidelity%20reconstruction%20under%20constrained%2C%20sparse-view%20settings.%203D%20SAR%20can%20efficiently%20capture%20large-scale%20geometry%20even%20from%20a%20single%20side-looking%20flight%20path%2C%20providing%20robust%20priors%20that%20complement%20photometric%20cues%20from%20images.%20Our%20framework%20integrates%20radar-derived%20spatial%20constraints%20into%20an%20SDF-based%20NSR%20backbone%2C%20guiding%20structure-aware%20ray%20selection%20and%20adaptive%20sampling%20for%20stable%20and%20efficient%20optimization.%20We%20also%20construct%20the%20first%20benchmark%20dataset%20with%20co-registered%203D%20SAR%20point%20clouds%20and%20aerial%20imagery%2C%20facilitating%20systematic%20evaluation%20of%20cross-modal%203D%20reconstruction.%20Extensive%20experiments%20show%20that%20incorporating%203D%20SAR%20markedly%20enhances%20reconstruction%20accuracy%2C%20completeness%2C%20and%20robustness%20compared%20with%20single-modality%20baselines%20under%20highly%20sparse%20and%20oblique-view%20conditions%2C%20highlighting%20a%20viable%20route%20toward%20scalable%20high-fidelity%20urban%20reconstruction%20with%20advanced%20airborne%20and%20spaceborne%20optical-SAR%20sensing.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrban%2520Neural%2520Surface%2520Reconstruction%2520from%2520Constrained%2520Sparse%2520Aerial%2520Imagery%2520with%25203D%2520SAR%2520Fusion%26entry.906535625%3DDa%2520Li%2520and%2520Chen%2520Yao%2520and%2520Tong%2520Mao%2520and%2520Jiacheng%2520Bao%2520and%2520Houjun%2520Sun%26entry.1292438233%3DNeural%2520surface%2520reconstruction%2520%2528NSR%2529%2520has%2520recently%2520shown%2520strong%2520potential%2520for%2520urban%25203D%2520reconstruction%2520from%2520multi-view%2520aerial%2520imagery.%2520However%252C%2520existing%2520NSR%2520methods%2520often%2520suffer%2520from%2520geometric%2520ambiguity%2520and%2520instability%252C%2520particularly%2520under%2520sparse-view%2520conditions.%2520This%2520issue%2520is%2520critical%2520in%2520large-scale%2520urban%2520remote%2520sensing%252C%2520where%2520aerial%2520image%2520acquisition%2520is%2520limited%2520by%2520flight%2520paths%252C%2520terrain%252C%2520and%2520cost.%2520To%2520address%2520this%2520challenge%252C%2520we%2520present%2520the%2520first%2520urban%2520NSR%2520framework%2520that%2520fuses%25203D%2520synthetic%2520aperture%2520radar%2520%2528SAR%2529%2520point%2520clouds%2520with%2520aerial%2520imagery%2520for%2520high-fidelity%2520reconstruction%2520under%2520constrained%252C%2520sparse-view%2520settings.%25203D%2520SAR%2520can%2520efficiently%2520capture%2520large-scale%2520geometry%2520even%2520from%2520a%2520single%2520side-looking%2520flight%2520path%252C%2520providing%2520robust%2520priors%2520that%2520complement%2520photometric%2520cues%2520from%2520images.%2520Our%2520framework%2520integrates%2520radar-derived%2520spatial%2520constraints%2520into%2520an%2520SDF-based%2520NSR%2520backbone%252C%2520guiding%2520structure-aware%2520ray%2520selection%2520and%2520adaptive%2520sampling%2520for%2520stable%2520and%2520efficient%2520optimization.%2520We%2520also%2520construct%2520the%2520first%2520benchmark%2520dataset%2520with%2520co-registered%25203D%2520SAR%2520point%2520clouds%2520and%2520aerial%2520imagery%252C%2520facilitating%2520systematic%2520evaluation%2520of%2520cross-modal%25203D%2520reconstruction.%2520Extensive%2520experiments%2520show%2520that%2520incorporating%25203D%2520SAR%2520markedly%2520enhances%2520reconstruction%2520accuracy%252C%2520completeness%252C%2520and%2520robustness%2520compared%2520with%2520single-modality%2520baselines%2520under%2520highly%2520sparse%2520and%2520oblique-view%2520conditions%252C%2520highlighting%2520a%2520viable%2520route%2520toward%2520scalable%2520high-fidelity%2520urban%2520reconstruction%2520with%2520advanced%2520airborne%2520and%2520spaceborne%2520optical-SAR%2520sensing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Urban%20Neural%20Surface%20Reconstruction%20from%20Constrained%20Sparse%20Aerial%20Imagery%20with%203D%20SAR%20Fusion&entry.906535625=Da%20Li%20and%20Chen%20Yao%20and%20Tong%20Mao%20and%20Jiacheng%20Bao%20and%20Houjun%20Sun&entry.1292438233=Neural%20surface%20reconstruction%20%28NSR%29%20has%20recently%20shown%20strong%20potential%20for%20urban%203D%20reconstruction%20from%20multi-view%20aerial%20imagery.%20However%2C%20existing%20NSR%20methods%20often%20suffer%20from%20geometric%20ambiguity%20and%20instability%2C%20particularly%20under%20sparse-view%20conditions.%20This%20issue%20is%20critical%20in%20large-scale%20urban%20remote%20sensing%2C%20where%20aerial%20image%20acquisition%20is%20limited%20by%20flight%20paths%2C%20terrain%2C%20and%20cost.%20To%20address%20this%20challenge%2C%20we%20present%20the%20first%20urban%20NSR%20framework%20that%20fuses%203D%20synthetic%20aperture%20radar%20%28SAR%29%20point%20clouds%20with%20aerial%20imagery%20for%20high-fidelity%20reconstruction%20under%20constrained%2C%20sparse-view%20settings.%203D%20SAR%20can%20efficiently%20capture%20large-scale%20geometry%20even%20from%20a%20single%20side-looking%20flight%20path%2C%20providing%20robust%20priors%20that%20complement%20photometric%20cues%20from%20images.%20Our%20framework%20integrates%20radar-derived%20spatial%20constraints%20into%20an%20SDF-based%20NSR%20backbone%2C%20guiding%20structure-aware%20ray%20selection%20and%20adaptive%20sampling%20for%20stable%20and%20efficient%20optimization.%20We%20also%20construct%20the%20first%20benchmark%20dataset%20with%20co-registered%203D%20SAR%20point%20clouds%20and%20aerial%20imagery%2C%20facilitating%20systematic%20evaluation%20of%20cross-modal%203D%20reconstruction.%20Extensive%20experiments%20show%20that%20incorporating%203D%20SAR%20markedly%20enhances%20reconstruction%20accuracy%2C%20completeness%2C%20and%20robustness%20compared%20with%20single-modality%20baselines%20under%20highly%20sparse%20and%20oblique-view%20conditions%2C%20highlighting%20a%20viable%20route%20toward%20scalable%20high-fidelity%20urban%20reconstruction%20with%20advanced%20airborne%20and%20spaceborne%20optical-SAR%20sensing.&entry.1838667208=http%3A//arxiv.org/abs/2601.22045v1&entry.124074799=Read"},
{"title": "Enhancing Semantic Segmentation with Continual Self-Supervised Pre-training", "author": "Brown Ebouky and Ajad Chhatkuli and Cristiano Malossi and Christoph Studer and Roy Assaf and Andrea Bartezzaghi", "abstract": "Self-supervised learning (SSL) has emerged as a central paradigm for training foundation models by leveraging large-scale unlabeled datasets, often producing representations with strong generalization capabilities. These models are typically pre-trained on general-purpose datasets such as ImageNet and subsequently adapted to various downstream tasks through finetuning. While prior work has investigated parameter-efficient adaptation methods like adapters, LoRA, and prompt tuning, primarily targeting downstream finetuning, extending the SSL pre-training itself in a continual manner to new domains under limited data remains largely underexplored, especially for downstream dense prediction tasks like semantic segmentation. In this work, we address the challenge of adapting vision foundation models to low-data target domains through continual self-supervised pre-training, specifically targeting downstream semantic segmentation. We propose GLARE (Global Local and Regional Enforcement), a novel continual self-supervised pre-training task designed to enhance downstream semantic segmentation performance. GLARE introduces patch-level augmentations to encourage local consistency and incorporates a regional consistency constraint that leverages spatial semantics in the data. For efficient continual pre-training, we initialize Vision Transformers (ViTs) with weights from existing SSL models and update only lightweight adapter modules specifically UniAdapter - while keeping the rest of the backbone frozen. Experiments across multiple semantic segmentation benchmarks on different domains demonstrate that GLARE consistently improves downstream performance with minimal computational and parameter overhead.", "link": "http://arxiv.org/abs/2509.17816v2", "date": "2026-01-29", "relevancy": 2.742, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5743}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5374}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Semantic%20Segmentation%20with%20Continual%20Self-Supervised%20Pre-training&body=Title%3A%20Enhancing%20Semantic%20Segmentation%20with%20Continual%20Self-Supervised%20Pre-training%0AAuthor%3A%20Brown%20Ebouky%20and%20Ajad%20Chhatkuli%20and%20Cristiano%20Malossi%20and%20Christoph%20Studer%20and%20Roy%20Assaf%20and%20Andrea%20Bartezzaghi%0AAbstract%3A%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20central%20paradigm%20for%20training%20foundation%20models%20by%20leveraging%20large-scale%20unlabeled%20datasets%2C%20often%20producing%20representations%20with%20strong%20generalization%20capabilities.%20These%20models%20are%20typically%20pre-trained%20on%20general-purpose%20datasets%20such%20as%20ImageNet%20and%20subsequently%20adapted%20to%20various%20downstream%20tasks%20through%20finetuning.%20While%20prior%20work%20has%20investigated%20parameter-efficient%20adaptation%20methods%20like%20adapters%2C%20LoRA%2C%20and%20prompt%20tuning%2C%20primarily%20targeting%20downstream%20finetuning%2C%20extending%20the%20SSL%20pre-training%20itself%20in%20a%20continual%20manner%20to%20new%20domains%20under%20limited%20data%20remains%20largely%20underexplored%2C%20especially%20for%20downstream%20dense%20prediction%20tasks%20like%20semantic%20segmentation.%20In%20this%20work%2C%20we%20address%20the%20challenge%20of%20adapting%20vision%20foundation%20models%20to%20low-data%20target%20domains%20through%20continual%20self-supervised%20pre-training%2C%20specifically%20targeting%20downstream%20semantic%20segmentation.%20We%20propose%20GLARE%20%28Global%20Local%20and%20Regional%20Enforcement%29%2C%20a%20novel%20continual%20self-supervised%20pre-training%20task%20designed%20to%20enhance%20downstream%20semantic%20segmentation%20performance.%20GLARE%20introduces%20patch-level%20augmentations%20to%20encourage%20local%20consistency%20and%20incorporates%20a%20regional%20consistency%20constraint%20that%20leverages%20spatial%20semantics%20in%20the%20data.%20For%20efficient%20continual%20pre-training%2C%20we%20initialize%20Vision%20Transformers%20%28ViTs%29%20with%20weights%20from%20existing%20SSL%20models%20and%20update%20only%20lightweight%20adapter%20modules%20specifically%20UniAdapter%20-%20while%20keeping%20the%20rest%20of%20the%20backbone%20frozen.%20Experiments%20across%20multiple%20semantic%20segmentation%20benchmarks%20on%20different%20domains%20demonstrate%20that%20GLARE%20consistently%20improves%20downstream%20performance%20with%20minimal%20computational%20and%20parameter%20overhead.%0ALink%3A%20http%3A//arxiv.org/abs/2509.17816v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Semantic%2520Segmentation%2520with%2520Continual%2520Self-Supervised%2520Pre-training%26entry.906535625%3DBrown%2520Ebouky%2520and%2520Ajad%2520Chhatkuli%2520and%2520Cristiano%2520Malossi%2520and%2520Christoph%2520Studer%2520and%2520Roy%2520Assaf%2520and%2520Andrea%2520Bartezzaghi%26entry.1292438233%3DSelf-supervised%2520learning%2520%2528SSL%2529%2520has%2520emerged%2520as%2520a%2520central%2520paradigm%2520for%2520training%2520foundation%2520models%2520by%2520leveraging%2520large-scale%2520unlabeled%2520datasets%252C%2520often%2520producing%2520representations%2520with%2520strong%2520generalization%2520capabilities.%2520These%2520models%2520are%2520typically%2520pre-trained%2520on%2520general-purpose%2520datasets%2520such%2520as%2520ImageNet%2520and%2520subsequently%2520adapted%2520to%2520various%2520downstream%2520tasks%2520through%2520finetuning.%2520While%2520prior%2520work%2520has%2520investigated%2520parameter-efficient%2520adaptation%2520methods%2520like%2520adapters%252C%2520LoRA%252C%2520and%2520prompt%2520tuning%252C%2520primarily%2520targeting%2520downstream%2520finetuning%252C%2520extending%2520the%2520SSL%2520pre-training%2520itself%2520in%2520a%2520continual%2520manner%2520to%2520new%2520domains%2520under%2520limited%2520data%2520remains%2520largely%2520underexplored%252C%2520especially%2520for%2520downstream%2520dense%2520prediction%2520tasks%2520like%2520semantic%2520segmentation.%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520challenge%2520of%2520adapting%2520vision%2520foundation%2520models%2520to%2520low-data%2520target%2520domains%2520through%2520continual%2520self-supervised%2520pre-training%252C%2520specifically%2520targeting%2520downstream%2520semantic%2520segmentation.%2520We%2520propose%2520GLARE%2520%2528Global%2520Local%2520and%2520Regional%2520Enforcement%2529%252C%2520a%2520novel%2520continual%2520self-supervised%2520pre-training%2520task%2520designed%2520to%2520enhance%2520downstream%2520semantic%2520segmentation%2520performance.%2520GLARE%2520introduces%2520patch-level%2520augmentations%2520to%2520encourage%2520local%2520consistency%2520and%2520incorporates%2520a%2520regional%2520consistency%2520constraint%2520that%2520leverages%2520spatial%2520semantics%2520in%2520the%2520data.%2520For%2520efficient%2520continual%2520pre-training%252C%2520we%2520initialize%2520Vision%2520Transformers%2520%2528ViTs%2529%2520with%2520weights%2520from%2520existing%2520SSL%2520models%2520and%2520update%2520only%2520lightweight%2520adapter%2520modules%2520specifically%2520UniAdapter%2520-%2520while%2520keeping%2520the%2520rest%2520of%2520the%2520backbone%2520frozen.%2520Experiments%2520across%2520multiple%2520semantic%2520segmentation%2520benchmarks%2520on%2520different%2520domains%2520demonstrate%2520that%2520GLARE%2520consistently%2520improves%2520downstream%2520performance%2520with%2520minimal%2520computational%2520and%2520parameter%2520overhead.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17816v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Semantic%20Segmentation%20with%20Continual%20Self-Supervised%20Pre-training&entry.906535625=Brown%20Ebouky%20and%20Ajad%20Chhatkuli%20and%20Cristiano%20Malossi%20and%20Christoph%20Studer%20and%20Roy%20Assaf%20and%20Andrea%20Bartezzaghi&entry.1292438233=Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20central%20paradigm%20for%20training%20foundation%20models%20by%20leveraging%20large-scale%20unlabeled%20datasets%2C%20often%20producing%20representations%20with%20strong%20generalization%20capabilities.%20These%20models%20are%20typically%20pre-trained%20on%20general-purpose%20datasets%20such%20as%20ImageNet%20and%20subsequently%20adapted%20to%20various%20downstream%20tasks%20through%20finetuning.%20While%20prior%20work%20has%20investigated%20parameter-efficient%20adaptation%20methods%20like%20adapters%2C%20LoRA%2C%20and%20prompt%20tuning%2C%20primarily%20targeting%20downstream%20finetuning%2C%20extending%20the%20SSL%20pre-training%20itself%20in%20a%20continual%20manner%20to%20new%20domains%20under%20limited%20data%20remains%20largely%20underexplored%2C%20especially%20for%20downstream%20dense%20prediction%20tasks%20like%20semantic%20segmentation.%20In%20this%20work%2C%20we%20address%20the%20challenge%20of%20adapting%20vision%20foundation%20models%20to%20low-data%20target%20domains%20through%20continual%20self-supervised%20pre-training%2C%20specifically%20targeting%20downstream%20semantic%20segmentation.%20We%20propose%20GLARE%20%28Global%20Local%20and%20Regional%20Enforcement%29%2C%20a%20novel%20continual%20self-supervised%20pre-training%20task%20designed%20to%20enhance%20downstream%20semantic%20segmentation%20performance.%20GLARE%20introduces%20patch-level%20augmentations%20to%20encourage%20local%20consistency%20and%20incorporates%20a%20regional%20consistency%20constraint%20that%20leverages%20spatial%20semantics%20in%20the%20data.%20For%20efficient%20continual%20pre-training%2C%20we%20initialize%20Vision%20Transformers%20%28ViTs%29%20with%20weights%20from%20existing%20SSL%20models%20and%20update%20only%20lightweight%20adapter%20modules%20specifically%20UniAdapter%20-%20while%20keeping%20the%20rest%20of%20the%20backbone%20frozen.%20Experiments%20across%20multiple%20semantic%20segmentation%20benchmarks%20on%20different%20domains%20demonstrate%20that%20GLARE%20consistently%20improves%20downstream%20performance%20with%20minimal%20computational%20and%20parameter%20overhead.&entry.1838667208=http%3A//arxiv.org/abs/2509.17816v2&entry.124074799=Read"},
{"title": "BLO-Inst: Bi-Level Optimization Based Alignment of YOLO and SAM for Robust Instance Segmentation", "author": "Li Zhang and Pengtao Xie", "abstract": "The Segment Anything Model has revolutionized image segmentation with its zero-shot capabilities, yet its reliance on manual prompts hinders fully automated deployment. While integrating object detectors as prompt generators offers a pathway to automation, existing pipelines suffer from two fundamental limitations: objective mismatch, where detectors optimized for geometric localization do not correspond to the optimal prompting context required by SAM, and alignment overfitting in standard joint training, where the detector simply memorizes specific prompt adjustments for training samples rather than learning a generalizable policy. To bridge this gap, we introduce BLO-Inst, a unified framework that aligns detection and segmentation objectives by bi-level optimization. We formulate the alignment as a nested optimization problem over disjoint data splits. In the lower level, the SAM is fine-tuned to maximize segmentation fidelity given the current detection proposals on a subset ($D_1$). In the upper level, the detector is updated to generate bounding boxes that explicitly minimize the validation loss of the fine-tuned SAM on a separate subset ($D_2$). This effectively transforms the detector into a segmentation-aware prompt generator, optimizing the bounding boxes not just for localization accuracy, but for downstream mask quality. Extensive experiments demonstrate that BLO-Inst achieves superior performance, outperforming standard baselines on tasks in general and biomedical domains.", "link": "http://arxiv.org/abs/2601.22061v1", "date": "2026-01-29", "relevancy": 2.7193, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5506}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5427}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLO-Inst%3A%20Bi-Level%20Optimization%20Based%20Alignment%20of%20YOLO%20and%20SAM%20for%20Robust%20Instance%20Segmentation&body=Title%3A%20BLO-Inst%3A%20Bi-Level%20Optimization%20Based%20Alignment%20of%20YOLO%20and%20SAM%20for%20Robust%20Instance%20Segmentation%0AAuthor%3A%20Li%20Zhang%20and%20Pengtao%20Xie%0AAbstract%3A%20The%20Segment%20Anything%20Model%20has%20revolutionized%20image%20segmentation%20with%20its%20zero-shot%20capabilities%2C%20yet%20its%20reliance%20on%20manual%20prompts%20hinders%20fully%20automated%20deployment.%20While%20integrating%20object%20detectors%20as%20prompt%20generators%20offers%20a%20pathway%20to%20automation%2C%20existing%20pipelines%20suffer%20from%20two%20fundamental%20limitations%3A%20objective%20mismatch%2C%20where%20detectors%20optimized%20for%20geometric%20localization%20do%20not%20correspond%20to%20the%20optimal%20prompting%20context%20required%20by%20SAM%2C%20and%20alignment%20overfitting%20in%20standard%20joint%20training%2C%20where%20the%20detector%20simply%20memorizes%20specific%20prompt%20adjustments%20for%20training%20samples%20rather%20than%20learning%20a%20generalizable%20policy.%20To%20bridge%20this%20gap%2C%20we%20introduce%20BLO-Inst%2C%20a%20unified%20framework%20that%20aligns%20detection%20and%20segmentation%20objectives%20by%20bi-level%20optimization.%20We%20formulate%20the%20alignment%20as%20a%20nested%20optimization%20problem%20over%20disjoint%20data%20splits.%20In%20the%20lower%20level%2C%20the%20SAM%20is%20fine-tuned%20to%20maximize%20segmentation%20fidelity%20given%20the%20current%20detection%20proposals%20on%20a%20subset%20%28%24D_1%24%29.%20In%20the%20upper%20level%2C%20the%20detector%20is%20updated%20to%20generate%20bounding%20boxes%20that%20explicitly%20minimize%20the%20validation%20loss%20of%20the%20fine-tuned%20SAM%20on%20a%20separate%20subset%20%28%24D_2%24%29.%20This%20effectively%20transforms%20the%20detector%20into%20a%20segmentation-aware%20prompt%20generator%2C%20optimizing%20the%20bounding%20boxes%20not%20just%20for%20localization%20accuracy%2C%20but%20for%20downstream%20mask%20quality.%20Extensive%20experiments%20demonstrate%20that%20BLO-Inst%20achieves%20superior%20performance%2C%20outperforming%20standard%20baselines%20on%20tasks%20in%20general%20and%20biomedical%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLO-Inst%253A%2520Bi-Level%2520Optimization%2520Based%2520Alignment%2520of%2520YOLO%2520and%2520SAM%2520for%2520Robust%2520Instance%2520Segmentation%26entry.906535625%3DLi%2520Zhang%2520and%2520Pengtao%2520Xie%26entry.1292438233%3DThe%2520Segment%2520Anything%2520Model%2520has%2520revolutionized%2520image%2520segmentation%2520with%2520its%2520zero-shot%2520capabilities%252C%2520yet%2520its%2520reliance%2520on%2520manual%2520prompts%2520hinders%2520fully%2520automated%2520deployment.%2520While%2520integrating%2520object%2520detectors%2520as%2520prompt%2520generators%2520offers%2520a%2520pathway%2520to%2520automation%252C%2520existing%2520pipelines%2520suffer%2520from%2520two%2520fundamental%2520limitations%253A%2520objective%2520mismatch%252C%2520where%2520detectors%2520optimized%2520for%2520geometric%2520localization%2520do%2520not%2520correspond%2520to%2520the%2520optimal%2520prompting%2520context%2520required%2520by%2520SAM%252C%2520and%2520alignment%2520overfitting%2520in%2520standard%2520joint%2520training%252C%2520where%2520the%2520detector%2520simply%2520memorizes%2520specific%2520prompt%2520adjustments%2520for%2520training%2520samples%2520rather%2520than%2520learning%2520a%2520generalizable%2520policy.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520BLO-Inst%252C%2520a%2520unified%2520framework%2520that%2520aligns%2520detection%2520and%2520segmentation%2520objectives%2520by%2520bi-level%2520optimization.%2520We%2520formulate%2520the%2520alignment%2520as%2520a%2520nested%2520optimization%2520problem%2520over%2520disjoint%2520data%2520splits.%2520In%2520the%2520lower%2520level%252C%2520the%2520SAM%2520is%2520fine-tuned%2520to%2520maximize%2520segmentation%2520fidelity%2520given%2520the%2520current%2520detection%2520proposals%2520on%2520a%2520subset%2520%2528%2524D_1%2524%2529.%2520In%2520the%2520upper%2520level%252C%2520the%2520detector%2520is%2520updated%2520to%2520generate%2520bounding%2520boxes%2520that%2520explicitly%2520minimize%2520the%2520validation%2520loss%2520of%2520the%2520fine-tuned%2520SAM%2520on%2520a%2520separate%2520subset%2520%2528%2524D_2%2524%2529.%2520This%2520effectively%2520transforms%2520the%2520detector%2520into%2520a%2520segmentation-aware%2520prompt%2520generator%252C%2520optimizing%2520the%2520bounding%2520boxes%2520not%2520just%2520for%2520localization%2520accuracy%252C%2520but%2520for%2520downstream%2520mask%2520quality.%2520Extensive%2520experiments%2520demonstrate%2520that%2520BLO-Inst%2520achieves%2520superior%2520performance%252C%2520outperforming%2520standard%2520baselines%2520on%2520tasks%2520in%2520general%2520and%2520biomedical%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLO-Inst%3A%20Bi-Level%20Optimization%20Based%20Alignment%20of%20YOLO%20and%20SAM%20for%20Robust%20Instance%20Segmentation&entry.906535625=Li%20Zhang%20and%20Pengtao%20Xie&entry.1292438233=The%20Segment%20Anything%20Model%20has%20revolutionized%20image%20segmentation%20with%20its%20zero-shot%20capabilities%2C%20yet%20its%20reliance%20on%20manual%20prompts%20hinders%20fully%20automated%20deployment.%20While%20integrating%20object%20detectors%20as%20prompt%20generators%20offers%20a%20pathway%20to%20automation%2C%20existing%20pipelines%20suffer%20from%20two%20fundamental%20limitations%3A%20objective%20mismatch%2C%20where%20detectors%20optimized%20for%20geometric%20localization%20do%20not%20correspond%20to%20the%20optimal%20prompting%20context%20required%20by%20SAM%2C%20and%20alignment%20overfitting%20in%20standard%20joint%20training%2C%20where%20the%20detector%20simply%20memorizes%20specific%20prompt%20adjustments%20for%20training%20samples%20rather%20than%20learning%20a%20generalizable%20policy.%20To%20bridge%20this%20gap%2C%20we%20introduce%20BLO-Inst%2C%20a%20unified%20framework%20that%20aligns%20detection%20and%20segmentation%20objectives%20by%20bi-level%20optimization.%20We%20formulate%20the%20alignment%20as%20a%20nested%20optimization%20problem%20over%20disjoint%20data%20splits.%20In%20the%20lower%20level%2C%20the%20SAM%20is%20fine-tuned%20to%20maximize%20segmentation%20fidelity%20given%20the%20current%20detection%20proposals%20on%20a%20subset%20%28%24D_1%24%29.%20In%20the%20upper%20level%2C%20the%20detector%20is%20updated%20to%20generate%20bounding%20boxes%20that%20explicitly%20minimize%20the%20validation%20loss%20of%20the%20fine-tuned%20SAM%20on%20a%20separate%20subset%20%28%24D_2%24%29.%20This%20effectively%20transforms%20the%20detector%20into%20a%20segmentation-aware%20prompt%20generator%2C%20optimizing%20the%20bounding%20boxes%20not%20just%20for%20localization%20accuracy%2C%20but%20for%20downstream%20mask%20quality.%20Extensive%20experiments%20demonstrate%20that%20BLO-Inst%20achieves%20superior%20performance%2C%20outperforming%20standard%20baselines%20on%20tasks%20in%20general%20and%20biomedical%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2601.22061v1&entry.124074799=Read"},
{"title": "MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning", "author": "Vera Pavlova and Mohammed Makhlouf", "abstract": "We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain Contrastive learning), a multi-stage framework for domain adaptation of text embedding models that incorporates joint domain-specific masked supervision. Our approach addresses the challenges of adapting large-scale general-domain text embedding models to specialized domains. By jointly optimizing masked language modeling (MLM) and contrastive objectives within a unified training pipeline, our method enables effective learning of domain-relevant representations while preserving the robust semantic discrimination properties of the original model. We empirically validate our approach on both high-resource and low-resource domains, achieving improvements up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong general-domain baselines. Comprehensive ablation studies further demonstrate the effectiveness of each component, highlighting the importance of balanced joint supervision and staged adaptation.", "link": "http://arxiv.org/abs/2510.16797v2", "date": "2026-01-29", "relevancy": 2.7092, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5806}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5289}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOSAIC%3A%20Masked%20Objective%20with%20Selective%20Adaptation%20for%20In-domain%20Contrastive%20Learning&body=Title%3A%20MOSAIC%3A%20Masked%20Objective%20with%20Selective%20Adaptation%20for%20In-domain%20Contrastive%20Learning%0AAuthor%3A%20Vera%20Pavlova%20and%20Mohammed%20Makhlouf%0AAbstract%3A%20We%20introduce%20MOSAIC%20%28Masked%20Objective%20with%20Selective%20Adaptation%20for%20In-domain%20Contrastive%20learning%29%2C%20a%20multi-stage%20framework%20for%20domain%20adaptation%20of%20text%20embedding%20models%20that%20incorporates%20joint%20domain-specific%20masked%20supervision.%20Our%20approach%20addresses%20the%20challenges%20of%20adapting%20large-scale%20general-domain%20text%20embedding%20models%20to%20specialized%20domains.%20By%20jointly%20optimizing%20masked%20language%20modeling%20%28MLM%29%20and%20contrastive%20objectives%20within%20a%20unified%20training%20pipeline%2C%20our%20method%20enables%20effective%20learning%20of%20domain-relevant%20representations%20while%20preserving%20the%20robust%20semantic%20discrimination%20properties%20of%20the%20original%20model.%20We%20empirically%20validate%20our%20approach%20on%20both%20high-resource%20and%20low-resource%20domains%2C%20achieving%20improvements%20up%20to%2013.4%25%20in%20NDCG%4010%20%28Normalized%20Discounted%20Cumulative%20Gain%29%20over%20strong%20general-domain%20baselines.%20Comprehensive%20ablation%20studies%20further%20demonstrate%20the%20effectiveness%20of%20each%20component%2C%20highlighting%20the%20importance%20of%20balanced%20joint%20supervision%20and%20staged%20adaptation.%0ALink%3A%20http%3A//arxiv.org/abs/2510.16797v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOSAIC%253A%2520Masked%2520Objective%2520with%2520Selective%2520Adaptation%2520for%2520In-domain%2520Contrastive%2520Learning%26entry.906535625%3DVera%2520Pavlova%2520and%2520Mohammed%2520Makhlouf%26entry.1292438233%3DWe%2520introduce%2520MOSAIC%2520%2528Masked%2520Objective%2520with%2520Selective%2520Adaptation%2520for%2520In-domain%2520Contrastive%2520learning%2529%252C%2520a%2520multi-stage%2520framework%2520for%2520domain%2520adaptation%2520of%2520text%2520embedding%2520models%2520that%2520incorporates%2520joint%2520domain-specific%2520masked%2520supervision.%2520Our%2520approach%2520addresses%2520the%2520challenges%2520of%2520adapting%2520large-scale%2520general-domain%2520text%2520embedding%2520models%2520to%2520specialized%2520domains.%2520By%2520jointly%2520optimizing%2520masked%2520language%2520modeling%2520%2528MLM%2529%2520and%2520contrastive%2520objectives%2520within%2520a%2520unified%2520training%2520pipeline%252C%2520our%2520method%2520enables%2520effective%2520learning%2520of%2520domain-relevant%2520representations%2520while%2520preserving%2520the%2520robust%2520semantic%2520discrimination%2520properties%2520of%2520the%2520original%2520model.%2520We%2520empirically%2520validate%2520our%2520approach%2520on%2520both%2520high-resource%2520and%2520low-resource%2520domains%252C%2520achieving%2520improvements%2520up%2520to%252013.4%2525%2520in%2520NDCG%254010%2520%2528Normalized%2520Discounted%2520Cumulative%2520Gain%2529%2520over%2520strong%2520general-domain%2520baselines.%2520Comprehensive%2520ablation%2520studies%2520further%2520demonstrate%2520the%2520effectiveness%2520of%2520each%2520component%252C%2520highlighting%2520the%2520importance%2520of%2520balanced%2520joint%2520supervision%2520and%2520staged%2520adaptation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16797v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOSAIC%3A%20Masked%20Objective%20with%20Selective%20Adaptation%20for%20In-domain%20Contrastive%20Learning&entry.906535625=Vera%20Pavlova%20and%20Mohammed%20Makhlouf&entry.1292438233=We%20introduce%20MOSAIC%20%28Masked%20Objective%20with%20Selective%20Adaptation%20for%20In-domain%20Contrastive%20learning%29%2C%20a%20multi-stage%20framework%20for%20domain%20adaptation%20of%20text%20embedding%20models%20that%20incorporates%20joint%20domain-specific%20masked%20supervision.%20Our%20approach%20addresses%20the%20challenges%20of%20adapting%20large-scale%20general-domain%20text%20embedding%20models%20to%20specialized%20domains.%20By%20jointly%20optimizing%20masked%20language%20modeling%20%28MLM%29%20and%20contrastive%20objectives%20within%20a%20unified%20training%20pipeline%2C%20our%20method%20enables%20effective%20learning%20of%20domain-relevant%20representations%20while%20preserving%20the%20robust%20semantic%20discrimination%20properties%20of%20the%20original%20model.%20We%20empirically%20validate%20our%20approach%20on%20both%20high-resource%20and%20low-resource%20domains%2C%20achieving%20improvements%20up%20to%2013.4%25%20in%20NDCG%4010%20%28Normalized%20Discounted%20Cumulative%20Gain%29%20over%20strong%20general-domain%20baselines.%20Comprehensive%20ablation%20studies%20further%20demonstrate%20the%20effectiveness%20of%20each%20component%2C%20highlighting%20the%20importance%20of%20balanced%20joint%20supervision%20and%20staged%20adaptation.&entry.1838667208=http%3A//arxiv.org/abs/2510.16797v2&entry.124074799=Read"},
{"title": "Procedural Pretraining: Warming Up Language Models with Abstract Data", "author": "Liangze Jiang and Zachary Shinnick and Anton van den Hengel and Hemanth Saratchandran and Damien Teney", "abstract": "Pretraining directly on web-scale corpora is the de facto paradigm for building language models. We study an alternative setting where the model is initially exposed to abstract structured data, as a means to ease the subsequent acquisition of rich semantic knowledge, much like humans learn simple logic and mathematics before higher reasoning. We specifically focus on procedural data, generated by formal languages and other simple algorithms, as such abstract data.\n  We first diagnose the algorithmic skills that different forms of procedural data can improve, often significantly. For example, on context recall (Needle-in-a-haystack), the accuracy jumps from 10 to 98% when pretraining on Dyck sequences (balanced brackets). Second, we study how these gains are reflected in pretraining larger models (up to 1.3B). We find that front-loading as little as 0.1% procedural data significantly outperforms standard pretraining on natural language, code, and informal mathematics (C4, CodeParrot, and DeepMind-Math datasets). Notably, this procedural pretraining enables the models to reach the same loss value with only 55, 67, 86% of the original data. Third, we explore the mechanisms behind and find that procedural pretraining instils non-trivial structure in both attention and MLP layers. The former is particularly important for structured domains (e.g. code), and the latter for language. Finally, we lay a path for combining multiple forms of procedural data. Our results show that procedural pretraining is a simple, lightweight means to improving performance and accelerating language model pretraining, ultimately suggesting the promise of disentangling knowledge acquisition from reasoning in LLMs.", "link": "http://arxiv.org/abs/2601.21725v1", "date": "2026-01-29", "relevancy": 2.7037, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5597}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Procedural%20Pretraining%3A%20Warming%20Up%20Language%20Models%20with%20Abstract%20Data&body=Title%3A%20Procedural%20Pretraining%3A%20Warming%20Up%20Language%20Models%20with%20Abstract%20Data%0AAuthor%3A%20Liangze%20Jiang%20and%20Zachary%20Shinnick%20and%20Anton%20van%20den%20Hengel%20and%20Hemanth%20Saratchandran%20and%20Damien%20Teney%0AAbstract%3A%20Pretraining%20directly%20on%20web-scale%20corpora%20is%20the%20de%20facto%20paradigm%20for%20building%20language%20models.%20We%20study%20an%20alternative%20setting%20where%20the%20model%20is%20initially%20exposed%20to%20abstract%20structured%20data%2C%20as%20a%20means%20to%20ease%20the%20subsequent%20acquisition%20of%20rich%20semantic%20knowledge%2C%20much%20like%20humans%20learn%20simple%20logic%20and%20mathematics%20before%20higher%20reasoning.%20We%20specifically%20focus%20on%20procedural%20data%2C%20generated%20by%20formal%20languages%20and%20other%20simple%20algorithms%2C%20as%20such%20abstract%20data.%0A%20%20We%20first%20diagnose%20the%20algorithmic%20skills%20that%20different%20forms%20of%20procedural%20data%20can%20improve%2C%20often%20significantly.%20For%20example%2C%20on%20context%20recall%20%28Needle-in-a-haystack%29%2C%20the%20accuracy%20jumps%20from%2010%20to%2098%25%20when%20pretraining%20on%20Dyck%20sequences%20%28balanced%20brackets%29.%20Second%2C%20we%20study%20how%20these%20gains%20are%20reflected%20in%20pretraining%20larger%20models%20%28up%20to%201.3B%29.%20We%20find%20that%20front-loading%20as%20little%20as%200.1%25%20procedural%20data%20significantly%20outperforms%20standard%20pretraining%20on%20natural%20language%2C%20code%2C%20and%20informal%20mathematics%20%28C4%2C%20CodeParrot%2C%20and%20DeepMind-Math%20datasets%29.%20Notably%2C%20this%20procedural%20pretraining%20enables%20the%20models%20to%20reach%20the%20same%20loss%20value%20with%20only%2055%2C%2067%2C%2086%25%20of%20the%20original%20data.%20Third%2C%20we%20explore%20the%20mechanisms%20behind%20and%20find%20that%20procedural%20pretraining%20instils%20non-trivial%20structure%20in%20both%20attention%20and%20MLP%20layers.%20The%20former%20is%20particularly%20important%20for%20structured%20domains%20%28e.g.%20code%29%2C%20and%20the%20latter%20for%20language.%20Finally%2C%20we%20lay%20a%20path%20for%20combining%20multiple%20forms%20of%20procedural%20data.%20Our%20results%20show%20that%20procedural%20pretraining%20is%20a%20simple%2C%20lightweight%20means%20to%20improving%20performance%20and%20accelerating%20language%20model%20pretraining%2C%20ultimately%20suggesting%20the%20promise%20of%20disentangling%20knowledge%20acquisition%20from%20reasoning%20in%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProcedural%2520Pretraining%253A%2520Warming%2520Up%2520Language%2520Models%2520with%2520Abstract%2520Data%26entry.906535625%3DLiangze%2520Jiang%2520and%2520Zachary%2520Shinnick%2520and%2520Anton%2520van%2520den%2520Hengel%2520and%2520Hemanth%2520Saratchandran%2520and%2520Damien%2520Teney%26entry.1292438233%3DPretraining%2520directly%2520on%2520web-scale%2520corpora%2520is%2520the%2520de%2520facto%2520paradigm%2520for%2520building%2520language%2520models.%2520We%2520study%2520an%2520alternative%2520setting%2520where%2520the%2520model%2520is%2520initially%2520exposed%2520to%2520abstract%2520structured%2520data%252C%2520as%2520a%2520means%2520to%2520ease%2520the%2520subsequent%2520acquisition%2520of%2520rich%2520semantic%2520knowledge%252C%2520much%2520like%2520humans%2520learn%2520simple%2520logic%2520and%2520mathematics%2520before%2520higher%2520reasoning.%2520We%2520specifically%2520focus%2520on%2520procedural%2520data%252C%2520generated%2520by%2520formal%2520languages%2520and%2520other%2520simple%2520algorithms%252C%2520as%2520such%2520abstract%2520data.%250A%2520%2520We%2520first%2520diagnose%2520the%2520algorithmic%2520skills%2520that%2520different%2520forms%2520of%2520procedural%2520data%2520can%2520improve%252C%2520often%2520significantly.%2520For%2520example%252C%2520on%2520context%2520recall%2520%2528Needle-in-a-haystack%2529%252C%2520the%2520accuracy%2520jumps%2520from%252010%2520to%252098%2525%2520when%2520pretraining%2520on%2520Dyck%2520sequences%2520%2528balanced%2520brackets%2529.%2520Second%252C%2520we%2520study%2520how%2520these%2520gains%2520are%2520reflected%2520in%2520pretraining%2520larger%2520models%2520%2528up%2520to%25201.3B%2529.%2520We%2520find%2520that%2520front-loading%2520as%2520little%2520as%25200.1%2525%2520procedural%2520data%2520significantly%2520outperforms%2520standard%2520pretraining%2520on%2520natural%2520language%252C%2520code%252C%2520and%2520informal%2520mathematics%2520%2528C4%252C%2520CodeParrot%252C%2520and%2520DeepMind-Math%2520datasets%2529.%2520Notably%252C%2520this%2520procedural%2520pretraining%2520enables%2520the%2520models%2520to%2520reach%2520the%2520same%2520loss%2520value%2520with%2520only%252055%252C%252067%252C%252086%2525%2520of%2520the%2520original%2520data.%2520Third%252C%2520we%2520explore%2520the%2520mechanisms%2520behind%2520and%2520find%2520that%2520procedural%2520pretraining%2520instils%2520non-trivial%2520structure%2520in%2520both%2520attention%2520and%2520MLP%2520layers.%2520The%2520former%2520is%2520particularly%2520important%2520for%2520structured%2520domains%2520%2528e.g.%2520code%2529%252C%2520and%2520the%2520latter%2520for%2520language.%2520Finally%252C%2520we%2520lay%2520a%2520path%2520for%2520combining%2520multiple%2520forms%2520of%2520procedural%2520data.%2520Our%2520results%2520show%2520that%2520procedural%2520pretraining%2520is%2520a%2520simple%252C%2520lightweight%2520means%2520to%2520improving%2520performance%2520and%2520accelerating%2520language%2520model%2520pretraining%252C%2520ultimately%2520suggesting%2520the%2520promise%2520of%2520disentangling%2520knowledge%2520acquisition%2520from%2520reasoning%2520in%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Procedural%20Pretraining%3A%20Warming%20Up%20Language%20Models%20with%20Abstract%20Data&entry.906535625=Liangze%20Jiang%20and%20Zachary%20Shinnick%20and%20Anton%20van%20den%20Hengel%20and%20Hemanth%20Saratchandran%20and%20Damien%20Teney&entry.1292438233=Pretraining%20directly%20on%20web-scale%20corpora%20is%20the%20de%20facto%20paradigm%20for%20building%20language%20models.%20We%20study%20an%20alternative%20setting%20where%20the%20model%20is%20initially%20exposed%20to%20abstract%20structured%20data%2C%20as%20a%20means%20to%20ease%20the%20subsequent%20acquisition%20of%20rich%20semantic%20knowledge%2C%20much%20like%20humans%20learn%20simple%20logic%20and%20mathematics%20before%20higher%20reasoning.%20We%20specifically%20focus%20on%20procedural%20data%2C%20generated%20by%20formal%20languages%20and%20other%20simple%20algorithms%2C%20as%20such%20abstract%20data.%0A%20%20We%20first%20diagnose%20the%20algorithmic%20skills%20that%20different%20forms%20of%20procedural%20data%20can%20improve%2C%20often%20significantly.%20For%20example%2C%20on%20context%20recall%20%28Needle-in-a-haystack%29%2C%20the%20accuracy%20jumps%20from%2010%20to%2098%25%20when%20pretraining%20on%20Dyck%20sequences%20%28balanced%20brackets%29.%20Second%2C%20we%20study%20how%20these%20gains%20are%20reflected%20in%20pretraining%20larger%20models%20%28up%20to%201.3B%29.%20We%20find%20that%20front-loading%20as%20little%20as%200.1%25%20procedural%20data%20significantly%20outperforms%20standard%20pretraining%20on%20natural%20language%2C%20code%2C%20and%20informal%20mathematics%20%28C4%2C%20CodeParrot%2C%20and%20DeepMind-Math%20datasets%29.%20Notably%2C%20this%20procedural%20pretraining%20enables%20the%20models%20to%20reach%20the%20same%20loss%20value%20with%20only%2055%2C%2067%2C%2086%25%20of%20the%20original%20data.%20Third%2C%20we%20explore%20the%20mechanisms%20behind%20and%20find%20that%20procedural%20pretraining%20instils%20non-trivial%20structure%20in%20both%20attention%20and%20MLP%20layers.%20The%20former%20is%20particularly%20important%20for%20structured%20domains%20%28e.g.%20code%29%2C%20and%20the%20latter%20for%20language.%20Finally%2C%20we%20lay%20a%20path%20for%20combining%20multiple%20forms%20of%20procedural%20data.%20Our%20results%20show%20that%20procedural%20pretraining%20is%20a%20simple%2C%20lightweight%20means%20to%20improving%20performance%20and%20accelerating%20language%20model%20pretraining%2C%20ultimately%20suggesting%20the%20promise%20of%20disentangling%20knowledge%20acquisition%20from%20reasoning%20in%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2601.21725v1&entry.124074799=Read"},
{"title": "Geodesic Calculus on Implicitly Defined Latent Manifolds", "author": "Florine Hartwig and Josua Sassen and Juliane Braunsmann and Martin Rumpf and Benedikt Wirth", "abstract": "Latent manifolds of autoencoders provide low-dimensional representations of data, which can be studied from a geometric perspective. We propose to describe these latent manifolds as implicit submanifolds of some ambient latent space. Based on this, we develop tools for a discrete Riemannian calculus approximating classical geometric operators. These tools are robust against inaccuracies of the implicit representation often occurring in practical examples. To obtain a suitable implicit representation, we propose to learn an approximate projection onto the latent manifold by minimizing a denoising objective. This approach is independent of the underlying autoencoder and supports the use of different Riemannian geometries on the latent manifolds. The framework in particular enables the computation of geodesic paths connecting given end points and shooting geodesics via the Riemannian exponential maps on latent manifolds. We evaluate our approach on various autoencoders trained on synthetic and real data.", "link": "http://arxiv.org/abs/2510.09468v2", "date": "2026-01-29", "relevancy": 2.6819, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.61}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5065}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geodesic%20Calculus%20on%20Implicitly%20Defined%20Latent%20Manifolds&body=Title%3A%20Geodesic%20Calculus%20on%20Implicitly%20Defined%20Latent%20Manifolds%0AAuthor%3A%20Florine%20Hartwig%20and%20Josua%20Sassen%20and%20Juliane%20Braunsmann%20and%20Martin%20Rumpf%20and%20Benedikt%20Wirth%0AAbstract%3A%20Latent%20manifolds%20of%20autoencoders%20provide%20low-dimensional%20representations%20of%20data%2C%20which%20can%20be%20studied%20from%20a%20geometric%20perspective.%20We%20propose%20to%20describe%20these%20latent%20manifolds%20as%20implicit%20submanifolds%20of%20some%20ambient%20latent%20space.%20Based%20on%20this%2C%20we%20develop%20tools%20for%20a%20discrete%20Riemannian%20calculus%20approximating%20classical%20geometric%20operators.%20These%20tools%20are%20robust%20against%20inaccuracies%20of%20the%20implicit%20representation%20often%20occurring%20in%20practical%20examples.%20To%20obtain%20a%20suitable%20implicit%20representation%2C%20we%20propose%20to%20learn%20an%20approximate%20projection%20onto%20the%20latent%20manifold%20by%20minimizing%20a%20denoising%20objective.%20This%20approach%20is%20independent%20of%20the%20underlying%20autoencoder%20and%20supports%20the%20use%20of%20different%20Riemannian%20geometries%20on%20the%20latent%20manifolds.%20The%20framework%20in%20particular%20enables%20the%20computation%20of%20geodesic%20paths%20connecting%20given%20end%20points%20and%20shooting%20geodesics%20via%20the%20Riemannian%20exponential%20maps%20on%20latent%20manifolds.%20We%20evaluate%20our%20approach%20on%20various%20autoencoders%20trained%20on%20synthetic%20and%20real%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2510.09468v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeodesic%2520Calculus%2520on%2520Implicitly%2520Defined%2520Latent%2520Manifolds%26entry.906535625%3DFlorine%2520Hartwig%2520and%2520Josua%2520Sassen%2520and%2520Juliane%2520Braunsmann%2520and%2520Martin%2520Rumpf%2520and%2520Benedikt%2520Wirth%26entry.1292438233%3DLatent%2520manifolds%2520of%2520autoencoders%2520provide%2520low-dimensional%2520representations%2520of%2520data%252C%2520which%2520can%2520be%2520studied%2520from%2520a%2520geometric%2520perspective.%2520We%2520propose%2520to%2520describe%2520these%2520latent%2520manifolds%2520as%2520implicit%2520submanifolds%2520of%2520some%2520ambient%2520latent%2520space.%2520Based%2520on%2520this%252C%2520we%2520develop%2520tools%2520for%2520a%2520discrete%2520Riemannian%2520calculus%2520approximating%2520classical%2520geometric%2520operators.%2520These%2520tools%2520are%2520robust%2520against%2520inaccuracies%2520of%2520the%2520implicit%2520representation%2520often%2520occurring%2520in%2520practical%2520examples.%2520To%2520obtain%2520a%2520suitable%2520implicit%2520representation%252C%2520we%2520propose%2520to%2520learn%2520an%2520approximate%2520projection%2520onto%2520the%2520latent%2520manifold%2520by%2520minimizing%2520a%2520denoising%2520objective.%2520This%2520approach%2520is%2520independent%2520of%2520the%2520underlying%2520autoencoder%2520and%2520supports%2520the%2520use%2520of%2520different%2520Riemannian%2520geometries%2520on%2520the%2520latent%2520manifolds.%2520The%2520framework%2520in%2520particular%2520enables%2520the%2520computation%2520of%2520geodesic%2520paths%2520connecting%2520given%2520end%2520points%2520and%2520shooting%2520geodesics%2520via%2520the%2520Riemannian%2520exponential%2520maps%2520on%2520latent%2520manifolds.%2520We%2520evaluate%2520our%2520approach%2520on%2520various%2520autoencoders%2520trained%2520on%2520synthetic%2520and%2520real%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09468v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geodesic%20Calculus%20on%20Implicitly%20Defined%20Latent%20Manifolds&entry.906535625=Florine%20Hartwig%20and%20Josua%20Sassen%20and%20Juliane%20Braunsmann%20and%20Martin%20Rumpf%20and%20Benedikt%20Wirth&entry.1292438233=Latent%20manifolds%20of%20autoencoders%20provide%20low-dimensional%20representations%20of%20data%2C%20which%20can%20be%20studied%20from%20a%20geometric%20perspective.%20We%20propose%20to%20describe%20these%20latent%20manifolds%20as%20implicit%20submanifolds%20of%20some%20ambient%20latent%20space.%20Based%20on%20this%2C%20we%20develop%20tools%20for%20a%20discrete%20Riemannian%20calculus%20approximating%20classical%20geometric%20operators.%20These%20tools%20are%20robust%20against%20inaccuracies%20of%20the%20implicit%20representation%20often%20occurring%20in%20practical%20examples.%20To%20obtain%20a%20suitable%20implicit%20representation%2C%20we%20propose%20to%20learn%20an%20approximate%20projection%20onto%20the%20latent%20manifold%20by%20minimizing%20a%20denoising%20objective.%20This%20approach%20is%20independent%20of%20the%20underlying%20autoencoder%20and%20supports%20the%20use%20of%20different%20Riemannian%20geometries%20on%20the%20latent%20manifolds.%20The%20framework%20in%20particular%20enables%20the%20computation%20of%20geodesic%20paths%20connecting%20given%20end%20points%20and%20shooting%20geodesics%20via%20the%20Riemannian%20exponential%20maps%20on%20latent%20manifolds.%20We%20evaluate%20our%20approach%20on%20various%20autoencoders%20trained%20on%20synthetic%20and%20real%20data.&entry.1838667208=http%3A//arxiv.org/abs/2510.09468v2&entry.124074799=Read"},
{"title": "Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models", "author": "Konstantinos P. Panousis and Diego Marcos", "abstract": "The widespread adoption of Vision-Language Models (VLMs) across fields has amplified concerns about model interpretability. Distressingly, these models are often treated as black-boxes, with limited or non-existent investigation of their decision making process. Despite numerous post- and ante-hoc interepretability methods, systematic and objective evaluation of the learned representations remains limited, particularly for sparsity-aware methods that are increasingly considered to \"induce interpretability\". In this work, we focus on Concept Bottleneck Models and investigate how different modeling decisions affect the emerging representations. We introduce the notion of clarity, a measure, capturing the interplay between the downstream performance and the sparsity and precision of the concept representation, while proposing an interpretability assessment framework using datasets with ground truth concept annotations. We consider both VLM- and attribute predictor-based CBMs, and three different sparsity-inducing strategies: per example $\\ell_1, \\ell_0$ and Bernoulli-based formulations. Our experiments reveal a critical trade-off between flexibility and interpretability, under which a given method can exhibit markedly different behaviors even at comparable performance levels. The code will be made publicly available upon publication.", "link": "http://arxiv.org/abs/2601.21944v1", "date": "2026-01-29", "relevancy": 2.6697, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clarity%3A%20The%20Flexibility-Interpretability%20Trade-Off%20in%20Sparsity-aware%20Concept%20Bottleneck%20Models&body=Title%3A%20Clarity%3A%20The%20Flexibility-Interpretability%20Trade-Off%20in%20Sparsity-aware%20Concept%20Bottleneck%20Models%0AAuthor%3A%20Konstantinos%20P.%20Panousis%20and%20Diego%20Marcos%0AAbstract%3A%20The%20widespread%20adoption%20of%20Vision-Language%20Models%20%28VLMs%29%20across%20fields%20has%20amplified%20concerns%20about%20model%20interpretability.%20Distressingly%2C%20these%20models%20are%20often%20treated%20as%20black-boxes%2C%20with%20limited%20or%20non-existent%20investigation%20of%20their%20decision%20making%20process.%20Despite%20numerous%20post-%20and%20ante-hoc%20interepretability%20methods%2C%20systematic%20and%20objective%20evaluation%20of%20the%20learned%20representations%20remains%20limited%2C%20particularly%20for%20sparsity-aware%20methods%20that%20are%20increasingly%20considered%20to%20%22induce%20interpretability%22.%20In%20this%20work%2C%20we%20focus%20on%20Concept%20Bottleneck%20Models%20and%20investigate%20how%20different%20modeling%20decisions%20affect%20the%20emerging%20representations.%20We%20introduce%20the%20notion%20of%20clarity%2C%20a%20measure%2C%20capturing%20the%20interplay%20between%20the%20downstream%20performance%20and%20the%20sparsity%20and%20precision%20of%20the%20concept%20representation%2C%20while%20proposing%20an%20interpretability%20assessment%20framework%20using%20datasets%20with%20ground%20truth%20concept%20annotations.%20We%20consider%20both%20VLM-%20and%20attribute%20predictor-based%20CBMs%2C%20and%20three%20different%20sparsity-inducing%20strategies%3A%20per%20example%20%24%5Cell_1%2C%20%5Cell_0%24%20and%20Bernoulli-based%20formulations.%20Our%20experiments%20reveal%20a%20critical%20trade-off%20between%20flexibility%20and%20interpretability%2C%20under%20which%20a%20given%20method%20can%20exhibit%20markedly%20different%20behaviors%20even%20at%20comparable%20performance%20levels.%20The%20code%20will%20be%20made%20publicly%20available%20upon%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21944v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClarity%253A%2520The%2520Flexibility-Interpretability%2520Trade-Off%2520in%2520Sparsity-aware%2520Concept%2520Bottleneck%2520Models%26entry.906535625%3DKonstantinos%2520P.%2520Panousis%2520and%2520Diego%2520Marcos%26entry.1292438233%3DThe%2520widespread%2520adoption%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520across%2520fields%2520has%2520amplified%2520concerns%2520about%2520model%2520interpretability.%2520Distressingly%252C%2520these%2520models%2520are%2520often%2520treated%2520as%2520black-boxes%252C%2520with%2520limited%2520or%2520non-existent%2520investigation%2520of%2520their%2520decision%2520making%2520process.%2520Despite%2520numerous%2520post-%2520and%2520ante-hoc%2520interepretability%2520methods%252C%2520systematic%2520and%2520objective%2520evaluation%2520of%2520the%2520learned%2520representations%2520remains%2520limited%252C%2520particularly%2520for%2520sparsity-aware%2520methods%2520that%2520are%2520increasingly%2520considered%2520to%2520%2522induce%2520interpretability%2522.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520Concept%2520Bottleneck%2520Models%2520and%2520investigate%2520how%2520different%2520modeling%2520decisions%2520affect%2520the%2520emerging%2520representations.%2520We%2520introduce%2520the%2520notion%2520of%2520clarity%252C%2520a%2520measure%252C%2520capturing%2520the%2520interplay%2520between%2520the%2520downstream%2520performance%2520and%2520the%2520sparsity%2520and%2520precision%2520of%2520the%2520concept%2520representation%252C%2520while%2520proposing%2520an%2520interpretability%2520assessment%2520framework%2520using%2520datasets%2520with%2520ground%2520truth%2520concept%2520annotations.%2520We%2520consider%2520both%2520VLM-%2520and%2520attribute%2520predictor-based%2520CBMs%252C%2520and%2520three%2520different%2520sparsity-inducing%2520strategies%253A%2520per%2520example%2520%2524%255Cell_1%252C%2520%255Cell_0%2524%2520and%2520Bernoulli-based%2520formulations.%2520Our%2520experiments%2520reveal%2520a%2520critical%2520trade-off%2520between%2520flexibility%2520and%2520interpretability%252C%2520under%2520which%2520a%2520given%2520method%2520can%2520exhibit%2520markedly%2520different%2520behaviors%2520even%2520at%2520comparable%2520performance%2520levels.%2520The%2520code%2520will%2520be%2520made%2520publicly%2520available%2520upon%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21944v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clarity%3A%20The%20Flexibility-Interpretability%20Trade-Off%20in%20Sparsity-aware%20Concept%20Bottleneck%20Models&entry.906535625=Konstantinos%20P.%20Panousis%20and%20Diego%20Marcos&entry.1292438233=The%20widespread%20adoption%20of%20Vision-Language%20Models%20%28VLMs%29%20across%20fields%20has%20amplified%20concerns%20about%20model%20interpretability.%20Distressingly%2C%20these%20models%20are%20often%20treated%20as%20black-boxes%2C%20with%20limited%20or%20non-existent%20investigation%20of%20their%20decision%20making%20process.%20Despite%20numerous%20post-%20and%20ante-hoc%20interepretability%20methods%2C%20systematic%20and%20objective%20evaluation%20of%20the%20learned%20representations%20remains%20limited%2C%20particularly%20for%20sparsity-aware%20methods%20that%20are%20increasingly%20considered%20to%20%22induce%20interpretability%22.%20In%20this%20work%2C%20we%20focus%20on%20Concept%20Bottleneck%20Models%20and%20investigate%20how%20different%20modeling%20decisions%20affect%20the%20emerging%20representations.%20We%20introduce%20the%20notion%20of%20clarity%2C%20a%20measure%2C%20capturing%20the%20interplay%20between%20the%20downstream%20performance%20and%20the%20sparsity%20and%20precision%20of%20the%20concept%20representation%2C%20while%20proposing%20an%20interpretability%20assessment%20framework%20using%20datasets%20with%20ground%20truth%20concept%20annotations.%20We%20consider%20both%20VLM-%20and%20attribute%20predictor-based%20CBMs%2C%20and%20three%20different%20sparsity-inducing%20strategies%3A%20per%20example%20%24%5Cell_1%2C%20%5Cell_0%24%20and%20Bernoulli-based%20formulations.%20Our%20experiments%20reveal%20a%20critical%20trade-off%20between%20flexibility%20and%20interpretability%2C%20under%20which%20a%20given%20method%20can%20exhibit%20markedly%20different%20behaviors%20even%20at%20comparable%20performance%20levels.%20The%20code%20will%20be%20made%20publicly%20available%20upon%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2601.21944v1&entry.124074799=Read"},
{"title": "SAL: Selective Adaptive Learning for Backpropagation-Free Training with Sparsification", "author": "Fanping Liu and Hua Yang and Jiasi Zou", "abstract": "Standard deep learning relies on Backpropagation (BP), which is constrained by biologically implausible weight symmetry and suffers from significant gradient interference within dense representations. To mitigate these bottlenecks, we propose Selective Adaptive Learning (SAL), a training method that combines selective parameter activation with adaptive area partitioning. Specifically, SAL decomposes the parameter space into mutually exclusive, sample-dependent regions. This decoupling mitigates gradient interference across divergent semantic patterns and addresses explicit weight symmetry requirements through our refined feedback alignment. Empirically, SAL demonstrates competitive convergence rates, leading to improved classification performance across 10 standard benchmarks. Additionally, SAL achieves numerical consistency and competitive accuracy even in deep regimes (up to 128 layers) and large-scale models (up to 1B parameters). Our approach is loosely inspired by biological learning mechanisms, offering a plausible alternative that contributes to the study of scalable neural network training.", "link": "http://arxiv.org/abs/2601.21561v1", "date": "2026-01-29", "relevancy": 2.6437, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5573}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5164}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAL%3A%20Selective%20Adaptive%20Learning%20for%20Backpropagation-Free%20Training%20with%20Sparsification&body=Title%3A%20SAL%3A%20Selective%20Adaptive%20Learning%20for%20Backpropagation-Free%20Training%20with%20Sparsification%0AAuthor%3A%20Fanping%20Liu%20and%20Hua%20Yang%20and%20Jiasi%20Zou%0AAbstract%3A%20Standard%20deep%20learning%20relies%20on%20Backpropagation%20%28BP%29%2C%20which%20is%20constrained%20by%20biologically%20implausible%20weight%20symmetry%20and%20suffers%20from%20significant%20gradient%20interference%20within%20dense%20representations.%20To%20mitigate%20these%20bottlenecks%2C%20we%20propose%20Selective%20Adaptive%20Learning%20%28SAL%29%2C%20a%20training%20method%20that%20combines%20selective%20parameter%20activation%20with%20adaptive%20area%20partitioning.%20Specifically%2C%20SAL%20decomposes%20the%20parameter%20space%20into%20mutually%20exclusive%2C%20sample-dependent%20regions.%20This%20decoupling%20mitigates%20gradient%20interference%20across%20divergent%20semantic%20patterns%20and%20addresses%20explicit%20weight%20symmetry%20requirements%20through%20our%20refined%20feedback%20alignment.%20Empirically%2C%20SAL%20demonstrates%20competitive%20convergence%20rates%2C%20leading%20to%20improved%20classification%20performance%20across%2010%20standard%20benchmarks.%20Additionally%2C%20SAL%20achieves%20numerical%20consistency%20and%20competitive%20accuracy%20even%20in%20deep%20regimes%20%28up%20to%20128%20layers%29%20and%20large-scale%20models%20%28up%20to%201B%20parameters%29.%20Our%20approach%20is%20loosely%20inspired%20by%20biological%20learning%20mechanisms%2C%20offering%20a%20plausible%20alternative%20that%20contributes%20to%20the%20study%20of%20scalable%20neural%20network%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAL%253A%2520Selective%2520Adaptive%2520Learning%2520for%2520Backpropagation-Free%2520Training%2520with%2520Sparsification%26entry.906535625%3DFanping%2520Liu%2520and%2520Hua%2520Yang%2520and%2520Jiasi%2520Zou%26entry.1292438233%3DStandard%2520deep%2520learning%2520relies%2520on%2520Backpropagation%2520%2528BP%2529%252C%2520which%2520is%2520constrained%2520by%2520biologically%2520implausible%2520weight%2520symmetry%2520and%2520suffers%2520from%2520significant%2520gradient%2520interference%2520within%2520dense%2520representations.%2520To%2520mitigate%2520these%2520bottlenecks%252C%2520we%2520propose%2520Selective%2520Adaptive%2520Learning%2520%2528SAL%2529%252C%2520a%2520training%2520method%2520that%2520combines%2520selective%2520parameter%2520activation%2520with%2520adaptive%2520area%2520partitioning.%2520Specifically%252C%2520SAL%2520decomposes%2520the%2520parameter%2520space%2520into%2520mutually%2520exclusive%252C%2520sample-dependent%2520regions.%2520This%2520decoupling%2520mitigates%2520gradient%2520interference%2520across%2520divergent%2520semantic%2520patterns%2520and%2520addresses%2520explicit%2520weight%2520symmetry%2520requirements%2520through%2520our%2520refined%2520feedback%2520alignment.%2520Empirically%252C%2520SAL%2520demonstrates%2520competitive%2520convergence%2520rates%252C%2520leading%2520to%2520improved%2520classification%2520performance%2520across%252010%2520standard%2520benchmarks.%2520Additionally%252C%2520SAL%2520achieves%2520numerical%2520consistency%2520and%2520competitive%2520accuracy%2520even%2520in%2520deep%2520regimes%2520%2528up%2520to%2520128%2520layers%2529%2520and%2520large-scale%2520models%2520%2528up%2520to%25201B%2520parameters%2529.%2520Our%2520approach%2520is%2520loosely%2520inspired%2520by%2520biological%2520learning%2520mechanisms%252C%2520offering%2520a%2520plausible%2520alternative%2520that%2520contributes%2520to%2520the%2520study%2520of%2520scalable%2520neural%2520network%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAL%3A%20Selective%20Adaptive%20Learning%20for%20Backpropagation-Free%20Training%20with%20Sparsification&entry.906535625=Fanping%20Liu%20and%20Hua%20Yang%20and%20Jiasi%20Zou&entry.1292438233=Standard%20deep%20learning%20relies%20on%20Backpropagation%20%28BP%29%2C%20which%20is%20constrained%20by%20biologically%20implausible%20weight%20symmetry%20and%20suffers%20from%20significant%20gradient%20interference%20within%20dense%20representations.%20To%20mitigate%20these%20bottlenecks%2C%20we%20propose%20Selective%20Adaptive%20Learning%20%28SAL%29%2C%20a%20training%20method%20that%20combines%20selective%20parameter%20activation%20with%20adaptive%20area%20partitioning.%20Specifically%2C%20SAL%20decomposes%20the%20parameter%20space%20into%20mutually%20exclusive%2C%20sample-dependent%20regions.%20This%20decoupling%20mitigates%20gradient%20interference%20across%20divergent%20semantic%20patterns%20and%20addresses%20explicit%20weight%20symmetry%20requirements%20through%20our%20refined%20feedback%20alignment.%20Empirically%2C%20SAL%20demonstrates%20competitive%20convergence%20rates%2C%20leading%20to%20improved%20classification%20performance%20across%2010%20standard%20benchmarks.%20Additionally%2C%20SAL%20achieves%20numerical%20consistency%20and%20competitive%20accuracy%20even%20in%20deep%20regimes%20%28up%20to%20128%20layers%29%20and%20large-scale%20models%20%28up%20to%201B%20parameters%29.%20Our%20approach%20is%20loosely%20inspired%20by%20biological%20learning%20mechanisms%2C%20offering%20a%20plausible%20alternative%20that%20contributes%20to%20the%20study%20of%20scalable%20neural%20network%20training.&entry.1838667208=http%3A//arxiv.org/abs/2601.21561v1&entry.124074799=Read"},
{"title": "TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning", "author": "Mingzu Liu and Hao Fang and Runmin Cong", "abstract": "Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.", "link": "http://arxiv.org/abs/2601.21692v1", "date": "2026-01-29", "relevancy": 2.6398, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.532}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5262}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TCAP%3A%20Tri-Component%20Attention%20Profiling%20for%20Unsupervised%20Backdoor%20Detection%20in%20MLLM%20Fine-Tuning&body=Title%3A%20TCAP%3A%20Tri-Component%20Attention%20Profiling%20for%20Unsupervised%20Backdoor%20Detection%20in%20MLLM%20Fine-Tuning%0AAuthor%3A%20Mingzu%20Liu%20and%20Hao%20Fang%20and%20Runmin%20Cong%0AAbstract%3A%20Fine-Tuning-as-a-Service%20%28FTaaS%29%20facilitates%20the%20customization%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20but%20introduces%20critical%20backdoor%20risks%20via%20poisoned%20data.%20Existing%20defenses%20either%20rely%20on%20supervised%20signals%20or%20fail%20to%20generalize%20across%20diverse%20trigger%20types%20and%20modalities.%20In%20this%20work%2C%20we%20uncover%20a%20universal%20backdoor%20fingerprint-attention%20allocation%20divergence-where%20poisoned%20samples%20disrupt%20the%20balanced%20attention%20distribution%20across%20three%20functional%20components%3A%20system%20instructions%2C%20vision%20inputs%2C%20and%20user%20textual%20queries%2C%20regardless%20of%20trigger%20morphology.%20Motivated%20by%20this%20insight%2C%20we%20propose%20Tri-Component%20Attention%20Profiling%20%28TCAP%29%2C%20an%20unsupervised%20defense%20framework%20to%20filter%20backdoor%20samples.%20TCAP%20decomposes%20cross-modal%20attention%20maps%20into%20the%20three%20components%2C%20identifies%20trigger-responsive%20attention%20heads%20via%20Gaussian%20Mixture%20Model%20%28GMM%29%20statistical%20profiling%2C%20and%20isolates%20poisoned%20samples%20through%20EM-based%20vote%20aggregation.%20Extensive%20experiments%20across%20diverse%20MLLM%20architectures%20and%20attack%20methods%20demonstrate%20that%20TCAP%20achieves%20consistently%20strong%20performance%2C%20establishing%20it%20as%20a%20robust%20and%20practical%20backdoor%20defense%20in%20MLLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTCAP%253A%2520Tri-Component%2520Attention%2520Profiling%2520for%2520Unsupervised%2520Backdoor%2520Detection%2520in%2520MLLM%2520Fine-Tuning%26entry.906535625%3DMingzu%2520Liu%2520and%2520Hao%2520Fang%2520and%2520Runmin%2520Cong%26entry.1292438233%3DFine-Tuning-as-a-Service%2520%2528FTaaS%2529%2520facilitates%2520the%2520customization%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520but%2520introduces%2520critical%2520backdoor%2520risks%2520via%2520poisoned%2520data.%2520Existing%2520defenses%2520either%2520rely%2520on%2520supervised%2520signals%2520or%2520fail%2520to%2520generalize%2520across%2520diverse%2520trigger%2520types%2520and%2520modalities.%2520In%2520this%2520work%252C%2520we%2520uncover%2520a%2520universal%2520backdoor%2520fingerprint-attention%2520allocation%2520divergence-where%2520poisoned%2520samples%2520disrupt%2520the%2520balanced%2520attention%2520distribution%2520across%2520three%2520functional%2520components%253A%2520system%2520instructions%252C%2520vision%2520inputs%252C%2520and%2520user%2520textual%2520queries%252C%2520regardless%2520of%2520trigger%2520morphology.%2520Motivated%2520by%2520this%2520insight%252C%2520we%2520propose%2520Tri-Component%2520Attention%2520Profiling%2520%2528TCAP%2529%252C%2520an%2520unsupervised%2520defense%2520framework%2520to%2520filter%2520backdoor%2520samples.%2520TCAP%2520decomposes%2520cross-modal%2520attention%2520maps%2520into%2520the%2520three%2520components%252C%2520identifies%2520trigger-responsive%2520attention%2520heads%2520via%2520Gaussian%2520Mixture%2520Model%2520%2528GMM%2529%2520statistical%2520profiling%252C%2520and%2520isolates%2520poisoned%2520samples%2520through%2520EM-based%2520vote%2520aggregation.%2520Extensive%2520experiments%2520across%2520diverse%2520MLLM%2520architectures%2520and%2520attack%2520methods%2520demonstrate%2520that%2520TCAP%2520achieves%2520consistently%2520strong%2520performance%252C%2520establishing%2520it%2520as%2520a%2520robust%2520and%2520practical%2520backdoor%2520defense%2520in%2520MLLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TCAP%3A%20Tri-Component%20Attention%20Profiling%20for%20Unsupervised%20Backdoor%20Detection%20in%20MLLM%20Fine-Tuning&entry.906535625=Mingzu%20Liu%20and%20Hao%20Fang%20and%20Runmin%20Cong&entry.1292438233=Fine-Tuning-as-a-Service%20%28FTaaS%29%20facilitates%20the%20customization%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20but%20introduces%20critical%20backdoor%20risks%20via%20poisoned%20data.%20Existing%20defenses%20either%20rely%20on%20supervised%20signals%20or%20fail%20to%20generalize%20across%20diverse%20trigger%20types%20and%20modalities.%20In%20this%20work%2C%20we%20uncover%20a%20universal%20backdoor%20fingerprint-attention%20allocation%20divergence-where%20poisoned%20samples%20disrupt%20the%20balanced%20attention%20distribution%20across%20three%20functional%20components%3A%20system%20instructions%2C%20vision%20inputs%2C%20and%20user%20textual%20queries%2C%20regardless%20of%20trigger%20morphology.%20Motivated%20by%20this%20insight%2C%20we%20propose%20Tri-Component%20Attention%20Profiling%20%28TCAP%29%2C%20an%20unsupervised%20defense%20framework%20to%20filter%20backdoor%20samples.%20TCAP%20decomposes%20cross-modal%20attention%20maps%20into%20the%20three%20components%2C%20identifies%20trigger-responsive%20attention%20heads%20via%20Gaussian%20Mixture%20Model%20%28GMM%29%20statistical%20profiling%2C%20and%20isolates%20poisoned%20samples%20through%20EM-based%20vote%20aggregation.%20Extensive%20experiments%20across%20diverse%20MLLM%20architectures%20and%20attack%20methods%20demonstrate%20that%20TCAP%20achieves%20consistently%20strong%20performance%2C%20establishing%20it%20as%20a%20robust%20and%20practical%20backdoor%20defense%20in%20MLLMs.&entry.1838667208=http%3A//arxiv.org/abs/2601.21692v1&entry.124074799=Read"},
{"title": "When Gradient Optimization Is Not Enough: $\\dagger$ Dispersive and Anchoring Geometric Regularizer for Multimodal Learning", "author": "Zixuan Xia and Hao Wang and Pengcheng Weng and Yanyu Qian and Yangxin Xu and William Dan and Fei Wang", "abstract": "Multimodal learning aims to integrate complementary information from heterogeneous modalities, yet strong optimization alone does not guaranty well-structured representations. Even under carefully balanced training schemes, multimodal models often exhibit geometric pathologies, including intra-modal representation collapse and sample-level cross-modal inconsistency, which degrade both unimodal robustness and multimodal fusion.\n  We identify representation geometry as a missing control axis in multimodal learning and propose \\regName, a lightweight geometry-aware regularization framework. \\regName enforces two complementary constraints on intermediate embeddings: an intra-modal dispersive regularization that promotes representation diversity, and an inter-modal anchoring regularization that bounds sample-level cross-modal drift without rigid alignment. The proposed regularizer is plug-and-play, requires no architectural modifications, and is compatible with various training paradigms.\n  Extensive experiments across multiple multimodal benchmarks demonstrate consistent improvements in both multimodal and unimodal performance, showing that explicitly regulating representation geometry effectively mitigates modality trade-offs.", "link": "http://arxiv.org/abs/2601.21670v1", "date": "2026-01-29", "relevancy": 2.6136, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5375}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5192}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Gradient%20Optimization%20Is%20Not%20Enough%3A%20%24%5Cdagger%24%20Dispersive%20and%20Anchoring%20Geometric%20Regularizer%20for%20Multimodal%20Learning&body=Title%3A%20When%20Gradient%20Optimization%20Is%20Not%20Enough%3A%20%24%5Cdagger%24%20Dispersive%20and%20Anchoring%20Geometric%20Regularizer%20for%20Multimodal%20Learning%0AAuthor%3A%20Zixuan%20Xia%20and%20Hao%20Wang%20and%20Pengcheng%20Weng%20and%20Yanyu%20Qian%20and%20Yangxin%20Xu%20and%20William%20Dan%20and%20Fei%20Wang%0AAbstract%3A%20Multimodal%20learning%20aims%20to%20integrate%20complementary%20information%20from%20heterogeneous%20modalities%2C%20yet%20strong%20optimization%20alone%20does%20not%20guaranty%20well-structured%20representations.%20Even%20under%20carefully%20balanced%20training%20schemes%2C%20multimodal%20models%20often%20exhibit%20geometric%20pathologies%2C%20including%20intra-modal%20representation%20collapse%20and%20sample-level%20cross-modal%20inconsistency%2C%20which%20degrade%20both%20unimodal%20robustness%20and%20multimodal%20fusion.%0A%20%20We%20identify%20representation%20geometry%20as%20a%20missing%20control%20axis%20in%20multimodal%20learning%20and%20propose%20%5CregName%2C%20a%20lightweight%20geometry-aware%20regularization%20framework.%20%5CregName%20enforces%20two%20complementary%20constraints%20on%20intermediate%20embeddings%3A%20an%20intra-modal%20dispersive%20regularization%20that%20promotes%20representation%20diversity%2C%20and%20an%20inter-modal%20anchoring%20regularization%20that%20bounds%20sample-level%20cross-modal%20drift%20without%20rigid%20alignment.%20The%20proposed%20regularizer%20is%20plug-and-play%2C%20requires%20no%20architectural%20modifications%2C%20and%20is%20compatible%20with%20various%20training%20paradigms.%0A%20%20Extensive%20experiments%20across%20multiple%20multimodal%20benchmarks%20demonstrate%20consistent%20improvements%20in%20both%20multimodal%20and%20unimodal%20performance%2C%20showing%20that%20explicitly%20regulating%20representation%20geometry%20effectively%20mitigates%20modality%20trade-offs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Gradient%2520Optimization%2520Is%2520Not%2520Enough%253A%2520%2524%255Cdagger%2524%2520Dispersive%2520and%2520Anchoring%2520Geometric%2520Regularizer%2520for%2520Multimodal%2520Learning%26entry.906535625%3DZixuan%2520Xia%2520and%2520Hao%2520Wang%2520and%2520Pengcheng%2520Weng%2520and%2520Yanyu%2520Qian%2520and%2520Yangxin%2520Xu%2520and%2520William%2520Dan%2520and%2520Fei%2520Wang%26entry.1292438233%3DMultimodal%2520learning%2520aims%2520to%2520integrate%2520complementary%2520information%2520from%2520heterogeneous%2520modalities%252C%2520yet%2520strong%2520optimization%2520alone%2520does%2520not%2520guaranty%2520well-structured%2520representations.%2520Even%2520under%2520carefully%2520balanced%2520training%2520schemes%252C%2520multimodal%2520models%2520often%2520exhibit%2520geometric%2520pathologies%252C%2520including%2520intra-modal%2520representation%2520collapse%2520and%2520sample-level%2520cross-modal%2520inconsistency%252C%2520which%2520degrade%2520both%2520unimodal%2520robustness%2520and%2520multimodal%2520fusion.%250A%2520%2520We%2520identify%2520representation%2520geometry%2520as%2520a%2520missing%2520control%2520axis%2520in%2520multimodal%2520learning%2520and%2520propose%2520%255CregName%252C%2520a%2520lightweight%2520geometry-aware%2520regularization%2520framework.%2520%255CregName%2520enforces%2520two%2520complementary%2520constraints%2520on%2520intermediate%2520embeddings%253A%2520an%2520intra-modal%2520dispersive%2520regularization%2520that%2520promotes%2520representation%2520diversity%252C%2520and%2520an%2520inter-modal%2520anchoring%2520regularization%2520that%2520bounds%2520sample-level%2520cross-modal%2520drift%2520without%2520rigid%2520alignment.%2520The%2520proposed%2520regularizer%2520is%2520plug-and-play%252C%2520requires%2520no%2520architectural%2520modifications%252C%2520and%2520is%2520compatible%2520with%2520various%2520training%2520paradigms.%250A%2520%2520Extensive%2520experiments%2520across%2520multiple%2520multimodal%2520benchmarks%2520demonstrate%2520consistent%2520improvements%2520in%2520both%2520multimodal%2520and%2520unimodal%2520performance%252C%2520showing%2520that%2520explicitly%2520regulating%2520representation%2520geometry%2520effectively%2520mitigates%2520modality%2520trade-offs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Gradient%20Optimization%20Is%20Not%20Enough%3A%20%24%5Cdagger%24%20Dispersive%20and%20Anchoring%20Geometric%20Regularizer%20for%20Multimodal%20Learning&entry.906535625=Zixuan%20Xia%20and%20Hao%20Wang%20and%20Pengcheng%20Weng%20and%20Yanyu%20Qian%20and%20Yangxin%20Xu%20and%20William%20Dan%20and%20Fei%20Wang&entry.1292438233=Multimodal%20learning%20aims%20to%20integrate%20complementary%20information%20from%20heterogeneous%20modalities%2C%20yet%20strong%20optimization%20alone%20does%20not%20guaranty%20well-structured%20representations.%20Even%20under%20carefully%20balanced%20training%20schemes%2C%20multimodal%20models%20often%20exhibit%20geometric%20pathologies%2C%20including%20intra-modal%20representation%20collapse%20and%20sample-level%20cross-modal%20inconsistency%2C%20which%20degrade%20both%20unimodal%20robustness%20and%20multimodal%20fusion.%0A%20%20We%20identify%20representation%20geometry%20as%20a%20missing%20control%20axis%20in%20multimodal%20learning%20and%20propose%20%5CregName%2C%20a%20lightweight%20geometry-aware%20regularization%20framework.%20%5CregName%20enforces%20two%20complementary%20constraints%20on%20intermediate%20embeddings%3A%20an%20intra-modal%20dispersive%20regularization%20that%20promotes%20representation%20diversity%2C%20and%20an%20inter-modal%20anchoring%20regularization%20that%20bounds%20sample-level%20cross-modal%20drift%20without%20rigid%20alignment.%20The%20proposed%20regularizer%20is%20plug-and-play%2C%20requires%20no%20architectural%20modifications%2C%20and%20is%20compatible%20with%20various%20training%20paradigms.%0A%20%20Extensive%20experiments%20across%20multiple%20multimodal%20benchmarks%20demonstrate%20consistent%20improvements%20in%20both%20multimodal%20and%20unimodal%20performance%2C%20showing%20that%20explicitly%20regulating%20representation%20geometry%20effectively%20mitigates%20modality%20trade-offs.&entry.1838667208=http%3A//arxiv.org/abs/2601.21670v1&entry.124074799=Read"},
{"title": "RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation", "author": "Hanzhuo Huang and Qingyang Bao and Zekai Gu and Zhongshuo Du and Cheng Lin and Yuan Liu and Sibei Yang", "abstract": "In this paper, we propose a 3D asset-referenced diffusion model for image generation, exploring how to integrate 3D assets into image diffusion models. Existing reference-based image generation methods leverage large-scale pretrained diffusion models and demonstrate strong capability in generating diverse images conditioned on a single reference image. However, these methods are limited to single-image references and cannot leverage 3D assets, constraining their practical versatility. To address this gap, we present a cross-domain diffusion model with dual-branch perception that leverages multi-view RGB images and point maps of 3D assets to jointly model their colors and canonical-space coordinates, achieving precise consistency between generated images and the 3D references. Our spatially aligned dual-branch generation architecture and domain-decoupled generation mechanism ensure the simultaneous generation of two spatially aligned but content-disentangled outputs, RGB images and point maps, linking 2D image attributes with 3D asset attributes. Experiments show that our approach effectively uses 3D assets as references to produce images consistent with the given assets, opening new possibilities for combining diffusion models with 3D content creation.", "link": "http://arxiv.org/abs/2601.22094v1", "date": "2026-01-29", "relevancy": 2.6128, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6592}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6592}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RefAny3D%3A%203D%20Asset-Referenced%20Diffusion%20Models%20for%20Image%20Generation&body=Title%3A%20RefAny3D%3A%203D%20Asset-Referenced%20Diffusion%20Models%20for%20Image%20Generation%0AAuthor%3A%20Hanzhuo%20Huang%20and%20Qingyang%20Bao%20and%20Zekai%20Gu%20and%20Zhongshuo%20Du%20and%20Cheng%20Lin%20and%20Yuan%20Liu%20and%20Sibei%20Yang%0AAbstract%3A%20In%20this%20paper%2C%20we%20propose%20a%203D%20asset-referenced%20diffusion%20model%20for%20image%20generation%2C%20exploring%20how%20to%20integrate%203D%20assets%20into%20image%20diffusion%20models.%20Existing%20reference-based%20image%20generation%20methods%20leverage%20large-scale%20pretrained%20diffusion%20models%20and%20demonstrate%20strong%20capability%20in%20generating%20diverse%20images%20conditioned%20on%20a%20single%20reference%20image.%20However%2C%20these%20methods%20are%20limited%20to%20single-image%20references%20and%20cannot%20leverage%203D%20assets%2C%20constraining%20their%20practical%20versatility.%20To%20address%20this%20gap%2C%20we%20present%20a%20cross-domain%20diffusion%20model%20with%20dual-branch%20perception%20that%20leverages%20multi-view%20RGB%20images%20and%20point%20maps%20of%203D%20assets%20to%20jointly%20model%20their%20colors%20and%20canonical-space%20coordinates%2C%20achieving%20precise%20consistency%20between%20generated%20images%20and%20the%203D%20references.%20Our%20spatially%20aligned%20dual-branch%20generation%20architecture%20and%20domain-decoupled%20generation%20mechanism%20ensure%20the%20simultaneous%20generation%20of%20two%20spatially%20aligned%20but%20content-disentangled%20outputs%2C%20RGB%20images%20and%20point%20maps%2C%20linking%202D%20image%20attributes%20with%203D%20asset%20attributes.%20Experiments%20show%20that%20our%20approach%20effectively%20uses%203D%20assets%20as%20references%20to%20produce%20images%20consistent%20with%20the%20given%20assets%2C%20opening%20new%20possibilities%20for%20combining%20diffusion%20models%20with%203D%20content%20creation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefAny3D%253A%25203D%2520Asset-Referenced%2520Diffusion%2520Models%2520for%2520Image%2520Generation%26entry.906535625%3DHanzhuo%2520Huang%2520and%2520Qingyang%2520Bao%2520and%2520Zekai%2520Gu%2520and%2520Zhongshuo%2520Du%2520and%2520Cheng%2520Lin%2520and%2520Yuan%2520Liu%2520and%2520Sibei%2520Yang%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520propose%2520a%25203D%2520asset-referenced%2520diffusion%2520model%2520for%2520image%2520generation%252C%2520exploring%2520how%2520to%2520integrate%25203D%2520assets%2520into%2520image%2520diffusion%2520models.%2520Existing%2520reference-based%2520image%2520generation%2520methods%2520leverage%2520large-scale%2520pretrained%2520diffusion%2520models%2520and%2520demonstrate%2520strong%2520capability%2520in%2520generating%2520diverse%2520images%2520conditioned%2520on%2520a%2520single%2520reference%2520image.%2520However%252C%2520these%2520methods%2520are%2520limited%2520to%2520single-image%2520references%2520and%2520cannot%2520leverage%25203D%2520assets%252C%2520constraining%2520their%2520practical%2520versatility.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520a%2520cross-domain%2520diffusion%2520model%2520with%2520dual-branch%2520perception%2520that%2520leverages%2520multi-view%2520RGB%2520images%2520and%2520point%2520maps%2520of%25203D%2520assets%2520to%2520jointly%2520model%2520their%2520colors%2520and%2520canonical-space%2520coordinates%252C%2520achieving%2520precise%2520consistency%2520between%2520generated%2520images%2520and%2520the%25203D%2520references.%2520Our%2520spatially%2520aligned%2520dual-branch%2520generation%2520architecture%2520and%2520domain-decoupled%2520generation%2520mechanism%2520ensure%2520the%2520simultaneous%2520generation%2520of%2520two%2520spatially%2520aligned%2520but%2520content-disentangled%2520outputs%252C%2520RGB%2520images%2520and%2520point%2520maps%252C%2520linking%25202D%2520image%2520attributes%2520with%25203D%2520asset%2520attributes.%2520Experiments%2520show%2520that%2520our%2520approach%2520effectively%2520uses%25203D%2520assets%2520as%2520references%2520to%2520produce%2520images%2520consistent%2520with%2520the%2520given%2520assets%252C%2520opening%2520new%2520possibilities%2520for%2520combining%2520diffusion%2520models%2520with%25203D%2520content%2520creation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RefAny3D%3A%203D%20Asset-Referenced%20Diffusion%20Models%20for%20Image%20Generation&entry.906535625=Hanzhuo%20Huang%20and%20Qingyang%20Bao%20and%20Zekai%20Gu%20and%20Zhongshuo%20Du%20and%20Cheng%20Lin%20and%20Yuan%20Liu%20and%20Sibei%20Yang&entry.1292438233=In%20this%20paper%2C%20we%20propose%20a%203D%20asset-referenced%20diffusion%20model%20for%20image%20generation%2C%20exploring%20how%20to%20integrate%203D%20assets%20into%20image%20diffusion%20models.%20Existing%20reference-based%20image%20generation%20methods%20leverage%20large-scale%20pretrained%20diffusion%20models%20and%20demonstrate%20strong%20capability%20in%20generating%20diverse%20images%20conditioned%20on%20a%20single%20reference%20image.%20However%2C%20these%20methods%20are%20limited%20to%20single-image%20references%20and%20cannot%20leverage%203D%20assets%2C%20constraining%20their%20practical%20versatility.%20To%20address%20this%20gap%2C%20we%20present%20a%20cross-domain%20diffusion%20model%20with%20dual-branch%20perception%20that%20leverages%20multi-view%20RGB%20images%20and%20point%20maps%20of%203D%20assets%20to%20jointly%20model%20their%20colors%20and%20canonical-space%20coordinates%2C%20achieving%20precise%20consistency%20between%20generated%20images%20and%20the%203D%20references.%20Our%20spatially%20aligned%20dual-branch%20generation%20architecture%20and%20domain-decoupled%20generation%20mechanism%20ensure%20the%20simultaneous%20generation%20of%20two%20spatially%20aligned%20but%20content-disentangled%20outputs%2C%20RGB%20images%20and%20point%20maps%2C%20linking%202D%20image%20attributes%20with%203D%20asset%20attributes.%20Experiments%20show%20that%20our%20approach%20effectively%20uses%203D%20assets%20as%20references%20to%20produce%20images%20consistent%20with%20the%20given%20assets%2C%20opening%20new%20possibilities%20for%20combining%20diffusion%20models%20with%203D%20content%20creation.&entry.1838667208=http%3A//arxiv.org/abs/2601.22094v1&entry.124074799=Read"},
{"title": "RNAGenScape: Property-Guided, Optimized Generation of mRNA Sequences with Manifold Langevin Dynamics", "author": "Danqi Liao and Chen Liu and Xingzhi Sun and Di\u00e9 Tang and Haochen Wang and Scott Youlten and Srikar Krishna Gopinath and Haejeong Lee and Ethan C. Strayer and Antonio J. Giraldez and Smita Krishnaswamy", "abstract": "Generating property-optimized mRNA sequences is central to applications such as vaccine design and protein replacement therapy, but remains challenging due to limited data, complex sequence-function relationships, and the narrow space of biologically viable sequences. Generative methods that drift away from the data manifold can yield sequences that fail to fold, translate poorly, or are otherwise nonfunctional. We present RNAGenScape, a property-guided manifold Langevin dynamics framework for mRNA sequence generation that operates directly on a learned manifold of real data. By performing iterative local optimization constrained to this manifold, RNAGenScape preserves biological viability, accesses reliable guidance, and avoids excursions into nonfunctional regions of the ambient sequence space. The framework integrates three components: (1) an autoencoder jointly trained with a property predictor to learn a property-organized latent manifold, (2) a denoising autoencoder that projects updates back onto the manifold, and (3) a property-guided Langevin dynamics procedure that performs optimization along the manifold. Across three real-world mRNA datasets spanning two orders of magnitude in size, RNAGenScape increases median property gain by up to 148% and success rate by up to 30% while ensuring biological viability of generated sequences, and achieves competitive inference efficiency relative to existing generative approaches.", "link": "http://arxiv.org/abs/2510.24736v2", "date": "2026-01-29", "relevancy": 2.5943, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5355}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5199}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RNAGenScape%3A%20Property-Guided%2C%20Optimized%20Generation%20of%20mRNA%20Sequences%20with%20Manifold%20Langevin%20Dynamics&body=Title%3A%20RNAGenScape%3A%20Property-Guided%2C%20Optimized%20Generation%20of%20mRNA%20Sequences%20with%20Manifold%20Langevin%20Dynamics%0AAuthor%3A%20Danqi%20Liao%20and%20Chen%20Liu%20and%20Xingzhi%20Sun%20and%20Di%C3%A9%20Tang%20and%20Haochen%20Wang%20and%20Scott%20Youlten%20and%20Srikar%20Krishna%20Gopinath%20and%20Haejeong%20Lee%20and%20Ethan%20C.%20Strayer%20and%20Antonio%20J.%20Giraldez%20and%20Smita%20Krishnaswamy%0AAbstract%3A%20Generating%20property-optimized%20mRNA%20sequences%20is%20central%20to%20applications%20such%20as%20vaccine%20design%20and%20protein%20replacement%20therapy%2C%20but%20remains%20challenging%20due%20to%20limited%20data%2C%20complex%20sequence-function%20relationships%2C%20and%20the%20narrow%20space%20of%20biologically%20viable%20sequences.%20Generative%20methods%20that%20drift%20away%20from%20the%20data%20manifold%20can%20yield%20sequences%20that%20fail%20to%20fold%2C%20translate%20poorly%2C%20or%20are%20otherwise%20nonfunctional.%20We%20present%20RNAGenScape%2C%20a%20property-guided%20manifold%20Langevin%20dynamics%20framework%20for%20mRNA%20sequence%20generation%20that%20operates%20directly%20on%20a%20learned%20manifold%20of%20real%20data.%20By%20performing%20iterative%20local%20optimization%20constrained%20to%20this%20manifold%2C%20RNAGenScape%20preserves%20biological%20viability%2C%20accesses%20reliable%20guidance%2C%20and%20avoids%20excursions%20into%20nonfunctional%20regions%20of%20the%20ambient%20sequence%20space.%20The%20framework%20integrates%20three%20components%3A%20%281%29%20an%20autoencoder%20jointly%20trained%20with%20a%20property%20predictor%20to%20learn%20a%20property-organized%20latent%20manifold%2C%20%282%29%20a%20denoising%20autoencoder%20that%20projects%20updates%20back%20onto%20the%20manifold%2C%20and%20%283%29%20a%20property-guided%20Langevin%20dynamics%20procedure%20that%20performs%20optimization%20along%20the%20manifold.%20Across%20three%20real-world%20mRNA%20datasets%20spanning%20two%20orders%20of%20magnitude%20in%20size%2C%20RNAGenScape%20increases%20median%20property%20gain%20by%20up%20to%20148%25%20and%20success%20rate%20by%20up%20to%2030%25%20while%20ensuring%20biological%20viability%20of%20generated%20sequences%2C%20and%20achieves%20competitive%20inference%20efficiency%20relative%20to%20existing%20generative%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2510.24736v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRNAGenScape%253A%2520Property-Guided%252C%2520Optimized%2520Generation%2520of%2520mRNA%2520Sequences%2520with%2520Manifold%2520Langevin%2520Dynamics%26entry.906535625%3DDanqi%2520Liao%2520and%2520Chen%2520Liu%2520and%2520Xingzhi%2520Sun%2520and%2520Di%25C3%25A9%2520Tang%2520and%2520Haochen%2520Wang%2520and%2520Scott%2520Youlten%2520and%2520Srikar%2520Krishna%2520Gopinath%2520and%2520Haejeong%2520Lee%2520and%2520Ethan%2520C.%2520Strayer%2520and%2520Antonio%2520J.%2520Giraldez%2520and%2520Smita%2520Krishnaswamy%26entry.1292438233%3DGenerating%2520property-optimized%2520mRNA%2520sequences%2520is%2520central%2520to%2520applications%2520such%2520as%2520vaccine%2520design%2520and%2520protein%2520replacement%2520therapy%252C%2520but%2520remains%2520challenging%2520due%2520to%2520limited%2520data%252C%2520complex%2520sequence-function%2520relationships%252C%2520and%2520the%2520narrow%2520space%2520of%2520biologically%2520viable%2520sequences.%2520Generative%2520methods%2520that%2520drift%2520away%2520from%2520the%2520data%2520manifold%2520can%2520yield%2520sequences%2520that%2520fail%2520to%2520fold%252C%2520translate%2520poorly%252C%2520or%2520are%2520otherwise%2520nonfunctional.%2520We%2520present%2520RNAGenScape%252C%2520a%2520property-guided%2520manifold%2520Langevin%2520dynamics%2520framework%2520for%2520mRNA%2520sequence%2520generation%2520that%2520operates%2520directly%2520on%2520a%2520learned%2520manifold%2520of%2520real%2520data.%2520By%2520performing%2520iterative%2520local%2520optimization%2520constrained%2520to%2520this%2520manifold%252C%2520RNAGenScape%2520preserves%2520biological%2520viability%252C%2520accesses%2520reliable%2520guidance%252C%2520and%2520avoids%2520excursions%2520into%2520nonfunctional%2520regions%2520of%2520the%2520ambient%2520sequence%2520space.%2520The%2520framework%2520integrates%2520three%2520components%253A%2520%25281%2529%2520an%2520autoencoder%2520jointly%2520trained%2520with%2520a%2520property%2520predictor%2520to%2520learn%2520a%2520property-organized%2520latent%2520manifold%252C%2520%25282%2529%2520a%2520denoising%2520autoencoder%2520that%2520projects%2520updates%2520back%2520onto%2520the%2520manifold%252C%2520and%2520%25283%2529%2520a%2520property-guided%2520Langevin%2520dynamics%2520procedure%2520that%2520performs%2520optimization%2520along%2520the%2520manifold.%2520Across%2520three%2520real-world%2520mRNA%2520datasets%2520spanning%2520two%2520orders%2520of%2520magnitude%2520in%2520size%252C%2520RNAGenScape%2520increases%2520median%2520property%2520gain%2520by%2520up%2520to%2520148%2525%2520and%2520success%2520rate%2520by%2520up%2520to%252030%2525%2520while%2520ensuring%2520biological%2520viability%2520of%2520generated%2520sequences%252C%2520and%2520achieves%2520competitive%2520inference%2520efficiency%2520relative%2520to%2520existing%2520generative%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.24736v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RNAGenScape%3A%20Property-Guided%2C%20Optimized%20Generation%20of%20mRNA%20Sequences%20with%20Manifold%20Langevin%20Dynamics&entry.906535625=Danqi%20Liao%20and%20Chen%20Liu%20and%20Xingzhi%20Sun%20and%20Di%C3%A9%20Tang%20and%20Haochen%20Wang%20and%20Scott%20Youlten%20and%20Srikar%20Krishna%20Gopinath%20and%20Haejeong%20Lee%20and%20Ethan%20C.%20Strayer%20and%20Antonio%20J.%20Giraldez%20and%20Smita%20Krishnaswamy&entry.1292438233=Generating%20property-optimized%20mRNA%20sequences%20is%20central%20to%20applications%20such%20as%20vaccine%20design%20and%20protein%20replacement%20therapy%2C%20but%20remains%20challenging%20due%20to%20limited%20data%2C%20complex%20sequence-function%20relationships%2C%20and%20the%20narrow%20space%20of%20biologically%20viable%20sequences.%20Generative%20methods%20that%20drift%20away%20from%20the%20data%20manifold%20can%20yield%20sequences%20that%20fail%20to%20fold%2C%20translate%20poorly%2C%20or%20are%20otherwise%20nonfunctional.%20We%20present%20RNAGenScape%2C%20a%20property-guided%20manifold%20Langevin%20dynamics%20framework%20for%20mRNA%20sequence%20generation%20that%20operates%20directly%20on%20a%20learned%20manifold%20of%20real%20data.%20By%20performing%20iterative%20local%20optimization%20constrained%20to%20this%20manifold%2C%20RNAGenScape%20preserves%20biological%20viability%2C%20accesses%20reliable%20guidance%2C%20and%20avoids%20excursions%20into%20nonfunctional%20regions%20of%20the%20ambient%20sequence%20space.%20The%20framework%20integrates%20three%20components%3A%20%281%29%20an%20autoencoder%20jointly%20trained%20with%20a%20property%20predictor%20to%20learn%20a%20property-organized%20latent%20manifold%2C%20%282%29%20a%20denoising%20autoencoder%20that%20projects%20updates%20back%20onto%20the%20manifold%2C%20and%20%283%29%20a%20property-guided%20Langevin%20dynamics%20procedure%20that%20performs%20optimization%20along%20the%20manifold.%20Across%20three%20real-world%20mRNA%20datasets%20spanning%20two%20orders%20of%20magnitude%20in%20size%2C%20RNAGenScape%20increases%20median%20property%20gain%20by%20up%20to%20148%25%20and%20success%20rate%20by%20up%20to%2030%25%20while%20ensuring%20biological%20viability%20of%20generated%20sequences%2C%20and%20achieves%20competitive%20inference%20efficiency%20relative%20to%20existing%20generative%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2510.24736v2&entry.124074799=Read"},
{"title": "EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers", "author": "John Flynn and Wolfgang Paier and Dimitar Dinev and Sam Nhut Nguyen and Hayk Poghosyan and Manuel Toribio and Sandipan Banerjee and Guy Gafni", "abstract": "Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.", "link": "http://arxiv.org/abs/2601.22127v1", "date": "2026-01-29", "relevancy": 2.5833, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6852}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.675}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EditYourself%3A%20Audio-Driven%20Generation%20and%20Manipulation%20of%20Talking%20Head%20Videos%20with%20Diffusion%20Transformers&body=Title%3A%20EditYourself%3A%20Audio-Driven%20Generation%20and%20Manipulation%20of%20Talking%20Head%20Videos%20with%20Diffusion%20Transformers%0AAuthor%3A%20John%20Flynn%20and%20Wolfgang%20Paier%20and%20Dimitar%20Dinev%20and%20Sam%20Nhut%20Nguyen%20and%20Hayk%20Poghosyan%20and%20Manuel%20Toribio%20and%20Sandipan%20Banerjee%20and%20Guy%20Gafni%0AAbstract%3A%20Current%20generative%20video%20models%20excel%20at%20producing%20novel%20content%20from%20text%20and%20image%20prompts%2C%20but%20leave%20a%20critical%20gap%20in%20editing%20existing%20pre-recorded%20videos%2C%20where%20minor%20alterations%20to%20the%20spoken%20script%20require%20preserving%20motion%2C%20temporal%20coherence%2C%20speaker%20identity%2C%20and%20accurate%20lip%20synchronization.%20We%20introduce%20EditYourself%2C%20a%20DiT-based%20framework%20for%20audio-driven%20video-to-video%20%28V2V%29%20editing%20that%20enables%20transcript-based%20modification%20of%20talking%20head%20videos%2C%20including%20the%20seamless%20addition%2C%20removal%2C%20and%20retiming%20of%20visually%20spoken%20content.%20Building%20on%20a%20general-purpose%20video%20diffusion%20model%2C%20EditYourself%20augments%20its%20V2V%20capabilities%20with%20audio%20conditioning%20and%20region-aware%2C%20edit-focused%20training%20extensions.%20This%20enables%20precise%20lip%20synchronization%20and%20temporally%20coherent%20restructuring%20of%20existing%20performances%20via%20spatiotemporal%20inpainting%2C%20including%20the%20synthesis%20of%20realistic%20human%20motion%20in%20newly%20added%20segments%2C%20while%20maintaining%20visual%20fidelity%20and%20identity%20consistency%20over%20long%20durations.%20This%20work%20represents%20a%20foundational%20step%20toward%20generative%20video%20models%20as%20practical%20tools%20for%20professional%20video%20post-production.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22127v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEditYourself%253A%2520Audio-Driven%2520Generation%2520and%2520Manipulation%2520of%2520Talking%2520Head%2520Videos%2520with%2520Diffusion%2520Transformers%26entry.906535625%3DJohn%2520Flynn%2520and%2520Wolfgang%2520Paier%2520and%2520Dimitar%2520Dinev%2520and%2520Sam%2520Nhut%2520Nguyen%2520and%2520Hayk%2520Poghosyan%2520and%2520Manuel%2520Toribio%2520and%2520Sandipan%2520Banerjee%2520and%2520Guy%2520Gafni%26entry.1292438233%3DCurrent%2520generative%2520video%2520models%2520excel%2520at%2520producing%2520novel%2520content%2520from%2520text%2520and%2520image%2520prompts%252C%2520but%2520leave%2520a%2520critical%2520gap%2520in%2520editing%2520existing%2520pre-recorded%2520videos%252C%2520where%2520minor%2520alterations%2520to%2520the%2520spoken%2520script%2520require%2520preserving%2520motion%252C%2520temporal%2520coherence%252C%2520speaker%2520identity%252C%2520and%2520accurate%2520lip%2520synchronization.%2520We%2520introduce%2520EditYourself%252C%2520a%2520DiT-based%2520framework%2520for%2520audio-driven%2520video-to-video%2520%2528V2V%2529%2520editing%2520that%2520enables%2520transcript-based%2520modification%2520of%2520talking%2520head%2520videos%252C%2520including%2520the%2520seamless%2520addition%252C%2520removal%252C%2520and%2520retiming%2520of%2520visually%2520spoken%2520content.%2520Building%2520on%2520a%2520general-purpose%2520video%2520diffusion%2520model%252C%2520EditYourself%2520augments%2520its%2520V2V%2520capabilities%2520with%2520audio%2520conditioning%2520and%2520region-aware%252C%2520edit-focused%2520training%2520extensions.%2520This%2520enables%2520precise%2520lip%2520synchronization%2520and%2520temporally%2520coherent%2520restructuring%2520of%2520existing%2520performances%2520via%2520spatiotemporal%2520inpainting%252C%2520including%2520the%2520synthesis%2520of%2520realistic%2520human%2520motion%2520in%2520newly%2520added%2520segments%252C%2520while%2520maintaining%2520visual%2520fidelity%2520and%2520identity%2520consistency%2520over%2520long%2520durations.%2520This%2520work%2520represents%2520a%2520foundational%2520step%2520toward%2520generative%2520video%2520models%2520as%2520practical%2520tools%2520for%2520professional%2520video%2520post-production.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22127v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EditYourself%3A%20Audio-Driven%20Generation%20and%20Manipulation%20of%20Talking%20Head%20Videos%20with%20Diffusion%20Transformers&entry.906535625=John%20Flynn%20and%20Wolfgang%20Paier%20and%20Dimitar%20Dinev%20and%20Sam%20Nhut%20Nguyen%20and%20Hayk%20Poghosyan%20and%20Manuel%20Toribio%20and%20Sandipan%20Banerjee%20and%20Guy%20Gafni&entry.1292438233=Current%20generative%20video%20models%20excel%20at%20producing%20novel%20content%20from%20text%20and%20image%20prompts%2C%20but%20leave%20a%20critical%20gap%20in%20editing%20existing%20pre-recorded%20videos%2C%20where%20minor%20alterations%20to%20the%20spoken%20script%20require%20preserving%20motion%2C%20temporal%20coherence%2C%20speaker%20identity%2C%20and%20accurate%20lip%20synchronization.%20We%20introduce%20EditYourself%2C%20a%20DiT-based%20framework%20for%20audio-driven%20video-to-video%20%28V2V%29%20editing%20that%20enables%20transcript-based%20modification%20of%20talking%20head%20videos%2C%20including%20the%20seamless%20addition%2C%20removal%2C%20and%20retiming%20of%20visually%20spoken%20content.%20Building%20on%20a%20general-purpose%20video%20diffusion%20model%2C%20EditYourself%20augments%20its%20V2V%20capabilities%20with%20audio%20conditioning%20and%20region-aware%2C%20edit-focused%20training%20extensions.%20This%20enables%20precise%20lip%20synchronization%20and%20temporally%20coherent%20restructuring%20of%20existing%20performances%20via%20spatiotemporal%20inpainting%2C%20including%20the%20synthesis%20of%20realistic%20human%20motion%20in%20newly%20added%20segments%2C%20while%20maintaining%20visual%20fidelity%20and%20identity%20consistency%20over%20long%20durations.%20This%20work%20represents%20a%20foundational%20step%20toward%20generative%20video%20models%20as%20practical%20tools%20for%20professional%20video%20post-production.&entry.1838667208=http%3A//arxiv.org/abs/2601.22127v1&entry.124074799=Read"},
{"title": "On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression", "author": "Xinwei Zhang and Hangcheng Liu and Li Bai and Hao Wang and Qingqing Ye and Tianwei Zhang and Haibo Hu", "abstract": "Visual token compression is widely used to accelerate large vision-language models (LVLMs) by pruning or merging visual tokens, yet its adversarial robustness remains unexplored. We show that existing encoder-based attacks can substantially overestimate the robustness of compressed LVLMs, due to an optimization-inference mismatch: perturbations are optimized on the full-token representation, while inference is performed through a token-compression bottleneck. To address this gap, we propose the Compression-AliGnEd attack (CAGE), which aligns perturbation optimization with compression inference without assuming access to the deployed compression mechanism or its token budget. CAGE combines (i) expected feature disruption, which concentrates distortion on tokens likely to survive across plausible budgets, and (ii) rank distortion alignment, which actively aligns token distortions with rank scores to promote the retention of highly distorted evidence. Across diverse representative plug-and-play compression mechanisms and datasets, our results show that CAGE consistently achieves lower robust accuracy than the baseline. This work highlights that robustness assessments ignoring compression can be overly optimistic, calling for compression-aware security evaluation and defenses for efficient LVLMs.", "link": "http://arxiv.org/abs/2601.21531v1", "date": "2026-01-29", "relevancy": 2.5806, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Adversarial%20Robustness%20of%20Large%20Vision-Language%20Models%20under%20Visual%20Token%20Compression&body=Title%3A%20On%20the%20Adversarial%20Robustness%20of%20Large%20Vision-Language%20Models%20under%20Visual%20Token%20Compression%0AAuthor%3A%20Xinwei%20Zhang%20and%20Hangcheng%20Liu%20and%20Li%20Bai%20and%20Hao%20Wang%20and%20Qingqing%20Ye%20and%20Tianwei%20Zhang%20and%20Haibo%20Hu%0AAbstract%3A%20Visual%20token%20compression%20is%20widely%20used%20to%20accelerate%20large%20vision-language%20models%20%28LVLMs%29%20by%20pruning%20or%20merging%20visual%20tokens%2C%20yet%20its%20adversarial%20robustness%20remains%20unexplored.%20We%20show%20that%20existing%20encoder-based%20attacks%20can%20substantially%20overestimate%20the%20robustness%20of%20compressed%20LVLMs%2C%20due%20to%20an%20optimization-inference%20mismatch%3A%20perturbations%20are%20optimized%20on%20the%20full-token%20representation%2C%20while%20inference%20is%20performed%20through%20a%20token-compression%20bottleneck.%20To%20address%20this%20gap%2C%20we%20propose%20the%20Compression-AliGnEd%20attack%20%28CAGE%29%2C%20which%20aligns%20perturbation%20optimization%20with%20compression%20inference%20without%20assuming%20access%20to%20the%20deployed%20compression%20mechanism%20or%20its%20token%20budget.%20CAGE%20combines%20%28i%29%20expected%20feature%20disruption%2C%20which%20concentrates%20distortion%20on%20tokens%20likely%20to%20survive%20across%20plausible%20budgets%2C%20and%20%28ii%29%20rank%20distortion%20alignment%2C%20which%20actively%20aligns%20token%20distortions%20with%20rank%20scores%20to%20promote%20the%20retention%20of%20highly%20distorted%20evidence.%20Across%20diverse%20representative%20plug-and-play%20compression%20mechanisms%20and%20datasets%2C%20our%20results%20show%20that%20CAGE%20consistently%20achieves%20lower%20robust%20accuracy%20than%20the%20baseline.%20This%20work%20highlights%20that%20robustness%20assessments%20ignoring%20compression%20can%20be%20overly%20optimistic%2C%20calling%20for%20compression-aware%20security%20evaluation%20and%20defenses%20for%20efficient%20LVLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Adversarial%2520Robustness%2520of%2520Large%2520Vision-Language%2520Models%2520under%2520Visual%2520Token%2520Compression%26entry.906535625%3DXinwei%2520Zhang%2520and%2520Hangcheng%2520Liu%2520and%2520Li%2520Bai%2520and%2520Hao%2520Wang%2520and%2520Qingqing%2520Ye%2520and%2520Tianwei%2520Zhang%2520and%2520Haibo%2520Hu%26entry.1292438233%3DVisual%2520token%2520compression%2520is%2520widely%2520used%2520to%2520accelerate%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520by%2520pruning%2520or%2520merging%2520visual%2520tokens%252C%2520yet%2520its%2520adversarial%2520robustness%2520remains%2520unexplored.%2520We%2520show%2520that%2520existing%2520encoder-based%2520attacks%2520can%2520substantially%2520overestimate%2520the%2520robustness%2520of%2520compressed%2520LVLMs%252C%2520due%2520to%2520an%2520optimization-inference%2520mismatch%253A%2520perturbations%2520are%2520optimized%2520on%2520the%2520full-token%2520representation%252C%2520while%2520inference%2520is%2520performed%2520through%2520a%2520token-compression%2520bottleneck.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520the%2520Compression-AliGnEd%2520attack%2520%2528CAGE%2529%252C%2520which%2520aligns%2520perturbation%2520optimization%2520with%2520compression%2520inference%2520without%2520assuming%2520access%2520to%2520the%2520deployed%2520compression%2520mechanism%2520or%2520its%2520token%2520budget.%2520CAGE%2520combines%2520%2528i%2529%2520expected%2520feature%2520disruption%252C%2520which%2520concentrates%2520distortion%2520on%2520tokens%2520likely%2520to%2520survive%2520across%2520plausible%2520budgets%252C%2520and%2520%2528ii%2529%2520rank%2520distortion%2520alignment%252C%2520which%2520actively%2520aligns%2520token%2520distortions%2520with%2520rank%2520scores%2520to%2520promote%2520the%2520retention%2520of%2520highly%2520distorted%2520evidence.%2520Across%2520diverse%2520representative%2520plug-and-play%2520compression%2520mechanisms%2520and%2520datasets%252C%2520our%2520results%2520show%2520that%2520CAGE%2520consistently%2520achieves%2520lower%2520robust%2520accuracy%2520than%2520the%2520baseline.%2520This%2520work%2520highlights%2520that%2520robustness%2520assessments%2520ignoring%2520compression%2520can%2520be%2520overly%2520optimistic%252C%2520calling%2520for%2520compression-aware%2520security%2520evaluation%2520and%2520defenses%2520for%2520efficient%2520LVLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Adversarial%20Robustness%20of%20Large%20Vision-Language%20Models%20under%20Visual%20Token%20Compression&entry.906535625=Xinwei%20Zhang%20and%20Hangcheng%20Liu%20and%20Li%20Bai%20and%20Hao%20Wang%20and%20Qingqing%20Ye%20and%20Tianwei%20Zhang%20and%20Haibo%20Hu&entry.1292438233=Visual%20token%20compression%20is%20widely%20used%20to%20accelerate%20large%20vision-language%20models%20%28LVLMs%29%20by%20pruning%20or%20merging%20visual%20tokens%2C%20yet%20its%20adversarial%20robustness%20remains%20unexplored.%20We%20show%20that%20existing%20encoder-based%20attacks%20can%20substantially%20overestimate%20the%20robustness%20of%20compressed%20LVLMs%2C%20due%20to%20an%20optimization-inference%20mismatch%3A%20perturbations%20are%20optimized%20on%20the%20full-token%20representation%2C%20while%20inference%20is%20performed%20through%20a%20token-compression%20bottleneck.%20To%20address%20this%20gap%2C%20we%20propose%20the%20Compression-AliGnEd%20attack%20%28CAGE%29%2C%20which%20aligns%20perturbation%20optimization%20with%20compression%20inference%20without%20assuming%20access%20to%20the%20deployed%20compression%20mechanism%20or%20its%20token%20budget.%20CAGE%20combines%20%28i%29%20expected%20feature%20disruption%2C%20which%20concentrates%20distortion%20on%20tokens%20likely%20to%20survive%20across%20plausible%20budgets%2C%20and%20%28ii%29%20rank%20distortion%20alignment%2C%20which%20actively%20aligns%20token%20distortions%20with%20rank%20scores%20to%20promote%20the%20retention%20of%20highly%20distorted%20evidence.%20Across%20diverse%20representative%20plug-and-play%20compression%20mechanisms%20and%20datasets%2C%20our%20results%20show%20that%20CAGE%20consistently%20achieves%20lower%20robust%20accuracy%20than%20the%20baseline.%20This%20work%20highlights%20that%20robustness%20assessments%20ignoring%20compression%20can%20be%20overly%20optimistic%2C%20calling%20for%20compression-aware%20security%20evaluation%20and%20defenses%20for%20efficient%20LVLMs.&entry.1838667208=http%3A//arxiv.org/abs/2601.21531v1&entry.124074799=Read"},
{"title": "LEMUR: Learned Multi-Vector Retrieval", "author": "Elias J\u00e4\u00e4saari and Ville Hyv\u00f6nen and Teemu Roos", "abstract": "Multi-vector representations generated by late interaction models, such as ColBERT, enable superior retrieval quality compared to single-vector representations in information retrieval applications. In multi-vector retrieval systems, both queries and documents are encoded using one embedding for each token, and similarity between queries and documents is measured by the MaxSim similarity measure. However, the improved recall of multi-vector retrieval comes at the expense of significantly increased latency. This necessitates designing efficient approximate nearest neighbor search (ANNS) algorithms for multi-vector search. In this work, we introduce LEMUR, a simple-yet-efficient framework for multi-vector similarity search. LEMUR consists of two consecutive problem reductions: We first formulate multi-vector similarity search as a supervised learning problem that can be solved using a one-hidden-layer neural network. Second, we reduce inference under this model to single-vector similarity search in its latent space, which enables the use of existing single-vector ANNS methods for speeding up retrieval. In addition to performance evaluation on ColBERTv2 embeddings, we evaluate LEMUR on embeddings generated by modern multi-vector text models and multi-vector visual document retrieval models. LEMUR is an order of magnitude faster than earlier multi-vector similarity search methods.", "link": "http://arxiv.org/abs/2601.21853v1", "date": "2026-01-29", "relevancy": 2.5662, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5194}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5152}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEMUR%3A%20Learned%20Multi-Vector%20Retrieval&body=Title%3A%20LEMUR%3A%20Learned%20Multi-Vector%20Retrieval%0AAuthor%3A%20Elias%20J%C3%A4%C3%A4saari%20and%20Ville%20Hyv%C3%B6nen%20and%20Teemu%20Roos%0AAbstract%3A%20Multi-vector%20representations%20generated%20by%20late%20interaction%20models%2C%20such%20as%20ColBERT%2C%20enable%20superior%20retrieval%20quality%20compared%20to%20single-vector%20representations%20in%20information%20retrieval%20applications.%20In%20multi-vector%20retrieval%20systems%2C%20both%20queries%20and%20documents%20are%20encoded%20using%20one%20embedding%20for%20each%20token%2C%20and%20similarity%20between%20queries%20and%20documents%20is%20measured%20by%20the%20MaxSim%20similarity%20measure.%20However%2C%20the%20improved%20recall%20of%20multi-vector%20retrieval%20comes%20at%20the%20expense%20of%20significantly%20increased%20latency.%20This%20necessitates%20designing%20efficient%20approximate%20nearest%20neighbor%20search%20%28ANNS%29%20algorithms%20for%20multi-vector%20search.%20In%20this%20work%2C%20we%20introduce%20LEMUR%2C%20a%20simple-yet-efficient%20framework%20for%20multi-vector%20similarity%20search.%20LEMUR%20consists%20of%20two%20consecutive%20problem%20reductions%3A%20We%20first%20formulate%20multi-vector%20similarity%20search%20as%20a%20supervised%20learning%20problem%20that%20can%20be%20solved%20using%20a%20one-hidden-layer%20neural%20network.%20Second%2C%20we%20reduce%20inference%20under%20this%20model%20to%20single-vector%20similarity%20search%20in%20its%20latent%20space%2C%20which%20enables%20the%20use%20of%20existing%20single-vector%20ANNS%20methods%20for%20speeding%20up%20retrieval.%20In%20addition%20to%20performance%20evaluation%20on%20ColBERTv2%20embeddings%2C%20we%20evaluate%20LEMUR%20on%20embeddings%20generated%20by%20modern%20multi-vector%20text%20models%20and%20multi-vector%20visual%20document%20retrieval%20models.%20LEMUR%20is%20an%20order%20of%20magnitude%20faster%20than%20earlier%20multi-vector%20similarity%20search%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEMUR%253A%2520Learned%2520Multi-Vector%2520Retrieval%26entry.906535625%3DElias%2520J%25C3%25A4%25C3%25A4saari%2520and%2520Ville%2520Hyv%25C3%25B6nen%2520and%2520Teemu%2520Roos%26entry.1292438233%3DMulti-vector%2520representations%2520generated%2520by%2520late%2520interaction%2520models%252C%2520such%2520as%2520ColBERT%252C%2520enable%2520superior%2520retrieval%2520quality%2520compared%2520to%2520single-vector%2520representations%2520in%2520information%2520retrieval%2520applications.%2520In%2520multi-vector%2520retrieval%2520systems%252C%2520both%2520queries%2520and%2520documents%2520are%2520encoded%2520using%2520one%2520embedding%2520for%2520each%2520token%252C%2520and%2520similarity%2520between%2520queries%2520and%2520documents%2520is%2520measured%2520by%2520the%2520MaxSim%2520similarity%2520measure.%2520However%252C%2520the%2520improved%2520recall%2520of%2520multi-vector%2520retrieval%2520comes%2520at%2520the%2520expense%2520of%2520significantly%2520increased%2520latency.%2520This%2520necessitates%2520designing%2520efficient%2520approximate%2520nearest%2520neighbor%2520search%2520%2528ANNS%2529%2520algorithms%2520for%2520multi-vector%2520search.%2520In%2520this%2520work%252C%2520we%2520introduce%2520LEMUR%252C%2520a%2520simple-yet-efficient%2520framework%2520for%2520multi-vector%2520similarity%2520search.%2520LEMUR%2520consists%2520of%2520two%2520consecutive%2520problem%2520reductions%253A%2520We%2520first%2520formulate%2520multi-vector%2520similarity%2520search%2520as%2520a%2520supervised%2520learning%2520problem%2520that%2520can%2520be%2520solved%2520using%2520a%2520one-hidden-layer%2520neural%2520network.%2520Second%252C%2520we%2520reduce%2520inference%2520under%2520this%2520model%2520to%2520single-vector%2520similarity%2520search%2520in%2520its%2520latent%2520space%252C%2520which%2520enables%2520the%2520use%2520of%2520existing%2520single-vector%2520ANNS%2520methods%2520for%2520speeding%2520up%2520retrieval.%2520In%2520addition%2520to%2520performance%2520evaluation%2520on%2520ColBERTv2%2520embeddings%252C%2520we%2520evaluate%2520LEMUR%2520on%2520embeddings%2520generated%2520by%2520modern%2520multi-vector%2520text%2520models%2520and%2520multi-vector%2520visual%2520document%2520retrieval%2520models.%2520LEMUR%2520is%2520an%2520order%2520of%2520magnitude%2520faster%2520than%2520earlier%2520multi-vector%2520similarity%2520search%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEMUR%3A%20Learned%20Multi-Vector%20Retrieval&entry.906535625=Elias%20J%C3%A4%C3%A4saari%20and%20Ville%20Hyv%C3%B6nen%20and%20Teemu%20Roos&entry.1292438233=Multi-vector%20representations%20generated%20by%20late%20interaction%20models%2C%20such%20as%20ColBERT%2C%20enable%20superior%20retrieval%20quality%20compared%20to%20single-vector%20representations%20in%20information%20retrieval%20applications.%20In%20multi-vector%20retrieval%20systems%2C%20both%20queries%20and%20documents%20are%20encoded%20using%20one%20embedding%20for%20each%20token%2C%20and%20similarity%20between%20queries%20and%20documents%20is%20measured%20by%20the%20MaxSim%20similarity%20measure.%20However%2C%20the%20improved%20recall%20of%20multi-vector%20retrieval%20comes%20at%20the%20expense%20of%20significantly%20increased%20latency.%20This%20necessitates%20designing%20efficient%20approximate%20nearest%20neighbor%20search%20%28ANNS%29%20algorithms%20for%20multi-vector%20search.%20In%20this%20work%2C%20we%20introduce%20LEMUR%2C%20a%20simple-yet-efficient%20framework%20for%20multi-vector%20similarity%20search.%20LEMUR%20consists%20of%20two%20consecutive%20problem%20reductions%3A%20We%20first%20formulate%20multi-vector%20similarity%20search%20as%20a%20supervised%20learning%20problem%20that%20can%20be%20solved%20using%20a%20one-hidden-layer%20neural%20network.%20Second%2C%20we%20reduce%20inference%20under%20this%20model%20to%20single-vector%20similarity%20search%20in%20its%20latent%20space%2C%20which%20enables%20the%20use%20of%20existing%20single-vector%20ANNS%20methods%20for%20speeding%20up%20retrieval.%20In%20addition%20to%20performance%20evaluation%20on%20ColBERTv2%20embeddings%2C%20we%20evaluate%20LEMUR%20on%20embeddings%20generated%20by%20modern%20multi-vector%20text%20models%20and%20multi-vector%20visual%20document%20retrieval%20models.%20LEMUR%20is%20an%20order%20of%20magnitude%20faster%20than%20earlier%20multi-vector%20similarity%20search%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.21853v1&entry.124074799=Read"},
{"title": "Discovering Multi-Scale Semantic Structure in Text Corpora Using Density-Based Trees and LLM Embeddings", "author": "Thomas Haschka and Joseph Bakarji", "abstract": "Recent advances in large language models enable documents to be represented as dense semantic embeddings, supporting similarity-based operations over large text collections. However, many web-scale systems still rely on flat clustering or predefined taxonomies, limiting insight into hierarchical topic relationships. In this paper we operationalize hierarchical density modeling on large language model embeddings in a way not previously explored. Instead of enforcing a fixed taxonomy or single clustering resolution, the method progressively relaxes local density constraints, revealing how compact semantic groups merge into broader thematic regions. The resulting tree encodes multi-scale semantic organization directly from data, making structural relationships between topics explicit. We evaluate the hierarchies on standard text benchmarks, showing that semantic alignment peaks at intermediate density levels and that abrupt transitions correspond to meaningful changes in semantic resolution. Beyond benchmarks, the approach is applied to large institutional and scientific corpora, exposing dominant fields, cross-disciplinary proximities, and emerging thematic clusters. By framing hierarchical structure as an emergent property of density in embedding spaces, this method provides an interpretable, multi-scale representation of semantic structure suitable for large, evolving text collections.", "link": "http://arxiv.org/abs/2512.23471v2", "date": "2026-01-29", "relevancy": 2.5625, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5128}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20Multi-Scale%20Semantic%20Structure%20in%20Text%20Corpora%20Using%20Density-Based%20Trees%20and%20LLM%20Embeddings&body=Title%3A%20Discovering%20Multi-Scale%20Semantic%20Structure%20in%20Text%20Corpora%20Using%20Density-Based%20Trees%20and%20LLM%20Embeddings%0AAuthor%3A%20Thomas%20Haschka%20and%20Joseph%20Bakarji%0AAbstract%3A%20Recent%20advances%20in%20large%20language%20models%20enable%20documents%20to%20be%20represented%20as%20dense%20semantic%20embeddings%2C%20supporting%20similarity-based%20operations%20over%20large%20text%20collections.%20However%2C%20many%20web-scale%20systems%20still%20rely%20on%20flat%20clustering%20or%20predefined%20taxonomies%2C%20limiting%20insight%20into%20hierarchical%20topic%20relationships.%20In%20this%20paper%20we%20operationalize%20hierarchical%20density%20modeling%20on%20large%20language%20model%20embeddings%20in%20a%20way%20not%20previously%20explored.%20Instead%20of%20enforcing%20a%20fixed%20taxonomy%20or%20single%20clustering%20resolution%2C%20the%20method%20progressively%20relaxes%20local%20density%20constraints%2C%20revealing%20how%20compact%20semantic%20groups%20merge%20into%20broader%20thematic%20regions.%20The%20resulting%20tree%20encodes%20multi-scale%20semantic%20organization%20directly%20from%20data%2C%20making%20structural%20relationships%20between%20topics%20explicit.%20We%20evaluate%20the%20hierarchies%20on%20standard%20text%20benchmarks%2C%20showing%20that%20semantic%20alignment%20peaks%20at%20intermediate%20density%20levels%20and%20that%20abrupt%20transitions%20correspond%20to%20meaningful%20changes%20in%20semantic%20resolution.%20Beyond%20benchmarks%2C%20the%20approach%20is%20applied%20to%20large%20institutional%20and%20scientific%20corpora%2C%20exposing%20dominant%20fields%2C%20cross-disciplinary%20proximities%2C%20and%20emerging%20thematic%20clusters.%20By%20framing%20hierarchical%20structure%20as%20an%20emergent%20property%20of%20density%20in%20embedding%20spaces%2C%20this%20method%20provides%20an%20interpretable%2C%20multi-scale%20representation%20of%20semantic%20structure%20suitable%20for%20large%2C%20evolving%20text%20collections.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23471v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520Multi-Scale%2520Semantic%2520Structure%2520in%2520Text%2520Corpora%2520Using%2520Density-Based%2520Trees%2520and%2520LLM%2520Embeddings%26entry.906535625%3DThomas%2520Haschka%2520and%2520Joseph%2520Bakarji%26entry.1292438233%3DRecent%2520advances%2520in%2520large%2520language%2520models%2520enable%2520documents%2520to%2520be%2520represented%2520as%2520dense%2520semantic%2520embeddings%252C%2520supporting%2520similarity-based%2520operations%2520over%2520large%2520text%2520collections.%2520However%252C%2520many%2520web-scale%2520systems%2520still%2520rely%2520on%2520flat%2520clustering%2520or%2520predefined%2520taxonomies%252C%2520limiting%2520insight%2520into%2520hierarchical%2520topic%2520relationships.%2520In%2520this%2520paper%2520we%2520operationalize%2520hierarchical%2520density%2520modeling%2520on%2520large%2520language%2520model%2520embeddings%2520in%2520a%2520way%2520not%2520previously%2520explored.%2520Instead%2520of%2520enforcing%2520a%2520fixed%2520taxonomy%2520or%2520single%2520clustering%2520resolution%252C%2520the%2520method%2520progressively%2520relaxes%2520local%2520density%2520constraints%252C%2520revealing%2520how%2520compact%2520semantic%2520groups%2520merge%2520into%2520broader%2520thematic%2520regions.%2520The%2520resulting%2520tree%2520encodes%2520multi-scale%2520semantic%2520organization%2520directly%2520from%2520data%252C%2520making%2520structural%2520relationships%2520between%2520topics%2520explicit.%2520We%2520evaluate%2520the%2520hierarchies%2520on%2520standard%2520text%2520benchmarks%252C%2520showing%2520that%2520semantic%2520alignment%2520peaks%2520at%2520intermediate%2520density%2520levels%2520and%2520that%2520abrupt%2520transitions%2520correspond%2520to%2520meaningful%2520changes%2520in%2520semantic%2520resolution.%2520Beyond%2520benchmarks%252C%2520the%2520approach%2520is%2520applied%2520to%2520large%2520institutional%2520and%2520scientific%2520corpora%252C%2520exposing%2520dominant%2520fields%252C%2520cross-disciplinary%2520proximities%252C%2520and%2520emerging%2520thematic%2520clusters.%2520By%2520framing%2520hierarchical%2520structure%2520as%2520an%2520emergent%2520property%2520of%2520density%2520in%2520embedding%2520spaces%252C%2520this%2520method%2520provides%2520an%2520interpretable%252C%2520multi-scale%2520representation%2520of%2520semantic%2520structure%2520suitable%2520for%2520large%252C%2520evolving%2520text%2520collections.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23471v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20Multi-Scale%20Semantic%20Structure%20in%20Text%20Corpora%20Using%20Density-Based%20Trees%20and%20LLM%20Embeddings&entry.906535625=Thomas%20Haschka%20and%20Joseph%20Bakarji&entry.1292438233=Recent%20advances%20in%20large%20language%20models%20enable%20documents%20to%20be%20represented%20as%20dense%20semantic%20embeddings%2C%20supporting%20similarity-based%20operations%20over%20large%20text%20collections.%20However%2C%20many%20web-scale%20systems%20still%20rely%20on%20flat%20clustering%20or%20predefined%20taxonomies%2C%20limiting%20insight%20into%20hierarchical%20topic%20relationships.%20In%20this%20paper%20we%20operationalize%20hierarchical%20density%20modeling%20on%20large%20language%20model%20embeddings%20in%20a%20way%20not%20previously%20explored.%20Instead%20of%20enforcing%20a%20fixed%20taxonomy%20or%20single%20clustering%20resolution%2C%20the%20method%20progressively%20relaxes%20local%20density%20constraints%2C%20revealing%20how%20compact%20semantic%20groups%20merge%20into%20broader%20thematic%20regions.%20The%20resulting%20tree%20encodes%20multi-scale%20semantic%20organization%20directly%20from%20data%2C%20making%20structural%20relationships%20between%20topics%20explicit.%20We%20evaluate%20the%20hierarchies%20on%20standard%20text%20benchmarks%2C%20showing%20that%20semantic%20alignment%20peaks%20at%20intermediate%20density%20levels%20and%20that%20abrupt%20transitions%20correspond%20to%20meaningful%20changes%20in%20semantic%20resolution.%20Beyond%20benchmarks%2C%20the%20approach%20is%20applied%20to%20large%20institutional%20and%20scientific%20corpora%2C%20exposing%20dominant%20fields%2C%20cross-disciplinary%20proximities%2C%20and%20emerging%20thematic%20clusters.%20By%20framing%20hierarchical%20structure%20as%20an%20emergent%20property%20of%20density%20in%20embedding%20spaces%2C%20this%20method%20provides%20an%20interpretable%2C%20multi-scale%20representation%20of%20semantic%20structure%20suitable%20for%20large%2C%20evolving%20text%20collections.&entry.1838667208=http%3A//arxiv.org/abs/2512.23471v2&entry.124074799=Read"},
{"title": "Representation Unlearning: Forgetting through Information Compression", "author": "Antonio Almud\u00e9var and Alfonso Ortega", "abstract": "Machine unlearning seeks to remove the influence of specific training data from a model, a need driven by privacy regulations and robustness concerns. Existing approaches typically modify model parameters, but such updates can be unstable, computationally costly, and limited by local approximations. We introduce Representation Unlearning, a framework that performs unlearning directly in the model's representation space. Instead of modifying model parameters, we learn a transformation over representations that imposes an information bottleneck: maximizing mutual information with retained data while suppressing information about data to be forgotten. We derive variational surrogates that make this objective tractable and show how they can be instantiated in two practical regimes: when both retain and forget data are available, and in a zero-shot setting where only forget data can be accessed. Experiments across several benchmarks demonstrate that Representation Unlearning achieves more reliable forgetting, better utility retention, and greater computational efficiency than parameter-centric baselines.", "link": "http://arxiv.org/abs/2601.21564v1", "date": "2026-01-29", "relevancy": 2.5619, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5462}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5092}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation%20Unlearning%3A%20Forgetting%20through%20Information%20Compression&body=Title%3A%20Representation%20Unlearning%3A%20Forgetting%20through%20Information%20Compression%0AAuthor%3A%20Antonio%20Almud%C3%A9var%20and%20Alfonso%20Ortega%0AAbstract%3A%20Machine%20unlearning%20seeks%20to%20remove%20the%20influence%20of%20specific%20training%20data%20from%20a%20model%2C%20a%20need%20driven%20by%20privacy%20regulations%20and%20robustness%20concerns.%20Existing%20approaches%20typically%20modify%20model%20parameters%2C%20but%20such%20updates%20can%20be%20unstable%2C%20computationally%20costly%2C%20and%20limited%20by%20local%20approximations.%20We%20introduce%20Representation%20Unlearning%2C%20a%20framework%20that%20performs%20unlearning%20directly%20in%20the%20model%27s%20representation%20space.%20Instead%20of%20modifying%20model%20parameters%2C%20we%20learn%20a%20transformation%20over%20representations%20that%20imposes%20an%20information%20bottleneck%3A%20maximizing%20mutual%20information%20with%20retained%20data%20while%20suppressing%20information%20about%20data%20to%20be%20forgotten.%20We%20derive%20variational%20surrogates%20that%20make%20this%20objective%20tractable%20and%20show%20how%20they%20can%20be%20instantiated%20in%20two%20practical%20regimes%3A%20when%20both%20retain%20and%20forget%20data%20are%20available%2C%20and%20in%20a%20zero-shot%20setting%20where%20only%20forget%20data%20can%20be%20accessed.%20Experiments%20across%20several%20benchmarks%20demonstrate%20that%20Representation%20Unlearning%20achieves%20more%20reliable%20forgetting%2C%20better%20utility%20retention%2C%20and%20greater%20computational%20efficiency%20than%20parameter-centric%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation%2520Unlearning%253A%2520Forgetting%2520through%2520Information%2520Compression%26entry.906535625%3DAntonio%2520Almud%25C3%25A9var%2520and%2520Alfonso%2520Ortega%26entry.1292438233%3DMachine%2520unlearning%2520seeks%2520to%2520remove%2520the%2520influence%2520of%2520specific%2520training%2520data%2520from%2520a%2520model%252C%2520a%2520need%2520driven%2520by%2520privacy%2520regulations%2520and%2520robustness%2520concerns.%2520Existing%2520approaches%2520typically%2520modify%2520model%2520parameters%252C%2520but%2520such%2520updates%2520can%2520be%2520unstable%252C%2520computationally%2520costly%252C%2520and%2520limited%2520by%2520local%2520approximations.%2520We%2520introduce%2520Representation%2520Unlearning%252C%2520a%2520framework%2520that%2520performs%2520unlearning%2520directly%2520in%2520the%2520model%2527s%2520representation%2520space.%2520Instead%2520of%2520modifying%2520model%2520parameters%252C%2520we%2520learn%2520a%2520transformation%2520over%2520representations%2520that%2520imposes%2520an%2520information%2520bottleneck%253A%2520maximizing%2520mutual%2520information%2520with%2520retained%2520data%2520while%2520suppressing%2520information%2520about%2520data%2520to%2520be%2520forgotten.%2520We%2520derive%2520variational%2520surrogates%2520that%2520make%2520this%2520objective%2520tractable%2520and%2520show%2520how%2520they%2520can%2520be%2520instantiated%2520in%2520two%2520practical%2520regimes%253A%2520when%2520both%2520retain%2520and%2520forget%2520data%2520are%2520available%252C%2520and%2520in%2520a%2520zero-shot%2520setting%2520where%2520only%2520forget%2520data%2520can%2520be%2520accessed.%2520Experiments%2520across%2520several%2520benchmarks%2520demonstrate%2520that%2520Representation%2520Unlearning%2520achieves%2520more%2520reliable%2520forgetting%252C%2520better%2520utility%2520retention%252C%2520and%2520greater%2520computational%2520efficiency%2520than%2520parameter-centric%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation%20Unlearning%3A%20Forgetting%20through%20Information%20Compression&entry.906535625=Antonio%20Almud%C3%A9var%20and%20Alfonso%20Ortega&entry.1292438233=Machine%20unlearning%20seeks%20to%20remove%20the%20influence%20of%20specific%20training%20data%20from%20a%20model%2C%20a%20need%20driven%20by%20privacy%20regulations%20and%20robustness%20concerns.%20Existing%20approaches%20typically%20modify%20model%20parameters%2C%20but%20such%20updates%20can%20be%20unstable%2C%20computationally%20costly%2C%20and%20limited%20by%20local%20approximations.%20We%20introduce%20Representation%20Unlearning%2C%20a%20framework%20that%20performs%20unlearning%20directly%20in%20the%20model%27s%20representation%20space.%20Instead%20of%20modifying%20model%20parameters%2C%20we%20learn%20a%20transformation%20over%20representations%20that%20imposes%20an%20information%20bottleneck%3A%20maximizing%20mutual%20information%20with%20retained%20data%20while%20suppressing%20information%20about%20data%20to%20be%20forgotten.%20We%20derive%20variational%20surrogates%20that%20make%20this%20objective%20tractable%20and%20show%20how%20they%20can%20be%20instantiated%20in%20two%20practical%20regimes%3A%20when%20both%20retain%20and%20forget%20data%20are%20available%2C%20and%20in%20a%20zero-shot%20setting%20where%20only%20forget%20data%20can%20be%20accessed.%20Experiments%20across%20several%20benchmarks%20demonstrate%20that%20Representation%20Unlearning%20achieves%20more%20reliable%20forgetting%2C%20better%20utility%20retention%2C%20and%20greater%20computational%20efficiency%20than%20parameter-centric%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.21564v1&entry.124074799=Read"},
{"title": "Can Local Learning Match Self-Supervised Backpropagation?", "author": "Wu S. Zihan and Ariane Delrocq and Wulfram Gerstner and Guillaume Bellec", "abstract": "While end-to-end self-supervised learning with backpropagation (global BP-SSL) has become central for training modern AI systems, theories of local self-supervised learning (local-SSL) have struggled to build functional representations in deep neural networks. To establish a link between global and local rules, we first develop a theory for deep linear networks: we identify conditions for local-SSL algorithms (like Forward-forward or CLAPP) to implement exactly the same weight update as a global BP-SSL. Starting from the theoretical insights, we then develop novel variants of local-SSL algorithms to approximate global BP-SSL in deep non-linear convolutional neural networks. Variants that improve the similarity between gradient updates of local-SSL with those of global BP-SSL also show better performance on image datasets (CIFAR-10, STL-10, and Tiny ImageNet). The best local-SSL rule with the CLAPP loss function matches the performance of a comparable global BP-SSL with InfoNCE or CPC-like loss functions, and improves upon state-of-the-art for local SSL on these benchmarks.", "link": "http://arxiv.org/abs/2601.21683v1", "date": "2026-01-29", "relevancy": 2.5513, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.537}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5146}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Local%20Learning%20Match%20Self-Supervised%20Backpropagation%3F&body=Title%3A%20Can%20Local%20Learning%20Match%20Self-Supervised%20Backpropagation%3F%0AAuthor%3A%20Wu%20S.%20Zihan%20and%20Ariane%20Delrocq%20and%20Wulfram%20Gerstner%20and%20Guillaume%20Bellec%0AAbstract%3A%20While%20end-to-end%20self-supervised%20learning%20with%20backpropagation%20%28global%20BP-SSL%29%20has%20become%20central%20for%20training%20modern%20AI%20systems%2C%20theories%20of%20local%20self-supervised%20learning%20%28local-SSL%29%20have%20struggled%20to%20build%20functional%20representations%20in%20deep%20neural%20networks.%20To%20establish%20a%20link%20between%20global%20and%20local%20rules%2C%20we%20first%20develop%20a%20theory%20for%20deep%20linear%20networks%3A%20we%20identify%20conditions%20for%20local-SSL%20algorithms%20%28like%20Forward-forward%20or%20CLAPP%29%20to%20implement%20exactly%20the%20same%20weight%20update%20as%20a%20global%20BP-SSL.%20Starting%20from%20the%20theoretical%20insights%2C%20we%20then%20develop%20novel%20variants%20of%20local-SSL%20algorithms%20to%20approximate%20global%20BP-SSL%20in%20deep%20non-linear%20convolutional%20neural%20networks.%20Variants%20that%20improve%20the%20similarity%20between%20gradient%20updates%20of%20local-SSL%20with%20those%20of%20global%20BP-SSL%20also%20show%20better%20performance%20on%20image%20datasets%20%28CIFAR-10%2C%20STL-10%2C%20and%20Tiny%20ImageNet%29.%20The%20best%20local-SSL%20rule%20with%20the%20CLAPP%20loss%20function%20matches%20the%20performance%20of%20a%20comparable%20global%20BP-SSL%20with%20InfoNCE%20or%20CPC-like%20loss%20functions%2C%20and%20improves%20upon%20state-of-the-art%20for%20local%20SSL%20on%20these%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Local%2520Learning%2520Match%2520Self-Supervised%2520Backpropagation%253F%26entry.906535625%3DWu%2520S.%2520Zihan%2520and%2520Ariane%2520Delrocq%2520and%2520Wulfram%2520Gerstner%2520and%2520Guillaume%2520Bellec%26entry.1292438233%3DWhile%2520end-to-end%2520self-supervised%2520learning%2520with%2520backpropagation%2520%2528global%2520BP-SSL%2529%2520has%2520become%2520central%2520for%2520training%2520modern%2520AI%2520systems%252C%2520theories%2520of%2520local%2520self-supervised%2520learning%2520%2528local-SSL%2529%2520have%2520struggled%2520to%2520build%2520functional%2520representations%2520in%2520deep%2520neural%2520networks.%2520To%2520establish%2520a%2520link%2520between%2520global%2520and%2520local%2520rules%252C%2520we%2520first%2520develop%2520a%2520theory%2520for%2520deep%2520linear%2520networks%253A%2520we%2520identify%2520conditions%2520for%2520local-SSL%2520algorithms%2520%2528like%2520Forward-forward%2520or%2520CLAPP%2529%2520to%2520implement%2520exactly%2520the%2520same%2520weight%2520update%2520as%2520a%2520global%2520BP-SSL.%2520Starting%2520from%2520the%2520theoretical%2520insights%252C%2520we%2520then%2520develop%2520novel%2520variants%2520of%2520local-SSL%2520algorithms%2520to%2520approximate%2520global%2520BP-SSL%2520in%2520deep%2520non-linear%2520convolutional%2520neural%2520networks.%2520Variants%2520that%2520improve%2520the%2520similarity%2520between%2520gradient%2520updates%2520of%2520local-SSL%2520with%2520those%2520of%2520global%2520BP-SSL%2520also%2520show%2520better%2520performance%2520on%2520image%2520datasets%2520%2528CIFAR-10%252C%2520STL-10%252C%2520and%2520Tiny%2520ImageNet%2529.%2520The%2520best%2520local-SSL%2520rule%2520with%2520the%2520CLAPP%2520loss%2520function%2520matches%2520the%2520performance%2520of%2520a%2520comparable%2520global%2520BP-SSL%2520with%2520InfoNCE%2520or%2520CPC-like%2520loss%2520functions%252C%2520and%2520improves%2520upon%2520state-of-the-art%2520for%2520local%2520SSL%2520on%2520these%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Local%20Learning%20Match%20Self-Supervised%20Backpropagation%3F&entry.906535625=Wu%20S.%20Zihan%20and%20Ariane%20Delrocq%20and%20Wulfram%20Gerstner%20and%20Guillaume%20Bellec&entry.1292438233=While%20end-to-end%20self-supervised%20learning%20with%20backpropagation%20%28global%20BP-SSL%29%20has%20become%20central%20for%20training%20modern%20AI%20systems%2C%20theories%20of%20local%20self-supervised%20learning%20%28local-SSL%29%20have%20struggled%20to%20build%20functional%20representations%20in%20deep%20neural%20networks.%20To%20establish%20a%20link%20between%20global%20and%20local%20rules%2C%20we%20first%20develop%20a%20theory%20for%20deep%20linear%20networks%3A%20we%20identify%20conditions%20for%20local-SSL%20algorithms%20%28like%20Forward-forward%20or%20CLAPP%29%20to%20implement%20exactly%20the%20same%20weight%20update%20as%20a%20global%20BP-SSL.%20Starting%20from%20the%20theoretical%20insights%2C%20we%20then%20develop%20novel%20variants%20of%20local-SSL%20algorithms%20to%20approximate%20global%20BP-SSL%20in%20deep%20non-linear%20convolutional%20neural%20networks.%20Variants%20that%20improve%20the%20similarity%20between%20gradient%20updates%20of%20local-SSL%20with%20those%20of%20global%20BP-SSL%20also%20show%20better%20performance%20on%20image%20datasets%20%28CIFAR-10%2C%20STL-10%2C%20and%20Tiny%20ImageNet%29.%20The%20best%20local-SSL%20rule%20with%20the%20CLAPP%20loss%20function%20matches%20the%20performance%20of%20a%20comparable%20global%20BP-SSL%20with%20InfoNCE%20or%20CPC-like%20loss%20functions%2C%20and%20improves%20upon%20state-of-the-art%20for%20local%20SSL%20on%20these%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2601.21683v1&entry.124074799=Read"},
{"title": "Understanding Post-Training Structural Changes in Large Language Models", "author": "Xinyu He and Xianghui Cao", "abstract": "Post-training fundamentally alters the behavior of large language models (LLMs), yet its impact on the internal parameter space remains poorly understood. In this work, we conduct a systematic singular value decomposition (SVD) analysis of principal linear layers in pretrained LLMs, focusing on two widely adopted post-training methods: instruction tuning and long-chain-of-thought (Long-CoT) distillation. Our analysis reveals two unexpected and robust structural changes: (1) a near-uniform geometric scaling of singular values across layers; and (2) highly consistent orthogonal transformations are applied to the left and right singular vectors of each matrix. Based on these findings, We propose a simple yet effective framework to describe the coordinated dynamics of parameters in LLMs, which elucidates why post-training inherently relies on the foundational capabilities developed during pre-training. Further experiments demonstrate that singular value scaling underpins the temperature-controlled regulatory mechanisms of post-training, while the coordinated rotation of singular vectors encodes the essential semantic alignment. These results challenge the prevailing view of the parameter space in large models as a black box, uncovering the first clear regularities in how parameters evolve during training, and providing a new perspective for deeper investigation into model parameter changes.", "link": "http://arxiv.org/abs/2509.17866v4", "date": "2026-01-29", "relevancy": 2.5453, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5145}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5145}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Post-Training%20Structural%20Changes%20in%20Large%20Language%20Models&body=Title%3A%20Understanding%20Post-Training%20Structural%20Changes%20in%20Large%20Language%20Models%0AAuthor%3A%20Xinyu%20He%20and%20Xianghui%20Cao%0AAbstract%3A%20Post-training%20fundamentally%20alters%20the%20behavior%20of%20large%20language%20models%20%28LLMs%29%2C%20yet%20its%20impact%20on%20the%20internal%20parameter%20space%20remains%20poorly%20understood.%20In%20this%20work%2C%20we%20conduct%20a%20systematic%20singular%20value%20decomposition%20%28SVD%29%20analysis%20of%20principal%20linear%20layers%20in%20pretrained%20LLMs%2C%20focusing%20on%20two%20widely%20adopted%20post-training%20methods%3A%20instruction%20tuning%20and%20long-chain-of-thought%20%28Long-CoT%29%20distillation.%20Our%20analysis%20reveals%20two%20unexpected%20and%20robust%20structural%20changes%3A%20%281%29%20a%20near-uniform%20geometric%20scaling%20of%20singular%20values%20across%20layers%3B%20and%20%282%29%20highly%20consistent%20orthogonal%20transformations%20are%20applied%20to%20the%20left%20and%20right%20singular%20vectors%20of%20each%20matrix.%20Based%20on%20these%20findings%2C%20We%20propose%20a%20simple%20yet%20effective%20framework%20to%20describe%20the%20coordinated%20dynamics%20of%20parameters%20in%20LLMs%2C%20which%20elucidates%20why%20post-training%20inherently%20relies%20on%20the%20foundational%20capabilities%20developed%20during%20pre-training.%20Further%20experiments%20demonstrate%20that%20singular%20value%20scaling%20underpins%20the%20temperature-controlled%20regulatory%20mechanisms%20of%20post-training%2C%20while%20the%20coordinated%20rotation%20of%20singular%20vectors%20encodes%20the%20essential%20semantic%20alignment.%20These%20results%20challenge%20the%20prevailing%20view%20of%20the%20parameter%20space%20in%20large%20models%20as%20a%20black%20box%2C%20uncovering%20the%20first%20clear%20regularities%20in%20how%20parameters%20evolve%20during%20training%2C%20and%20providing%20a%20new%20perspective%20for%20deeper%20investigation%20into%20model%20parameter%20changes.%0ALink%3A%20http%3A//arxiv.org/abs/2509.17866v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Post-Training%2520Structural%2520Changes%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DXinyu%2520He%2520and%2520Xianghui%2520Cao%26entry.1292438233%3DPost-training%2520fundamentally%2520alters%2520the%2520behavior%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520yet%2520its%2520impact%2520on%2520the%2520internal%2520parameter%2520space%2520remains%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520conduct%2520a%2520systematic%2520singular%2520value%2520decomposition%2520%2528SVD%2529%2520analysis%2520of%2520principal%2520linear%2520layers%2520in%2520pretrained%2520LLMs%252C%2520focusing%2520on%2520two%2520widely%2520adopted%2520post-training%2520methods%253A%2520instruction%2520tuning%2520and%2520long-chain-of-thought%2520%2528Long-CoT%2529%2520distillation.%2520Our%2520analysis%2520reveals%2520two%2520unexpected%2520and%2520robust%2520structural%2520changes%253A%2520%25281%2529%2520a%2520near-uniform%2520geometric%2520scaling%2520of%2520singular%2520values%2520across%2520layers%253B%2520and%2520%25282%2529%2520highly%2520consistent%2520orthogonal%2520transformations%2520are%2520applied%2520to%2520the%2520left%2520and%2520right%2520singular%2520vectors%2520of%2520each%2520matrix.%2520Based%2520on%2520these%2520findings%252C%2520We%2520propose%2520a%2520simple%2520yet%2520effective%2520framework%2520to%2520describe%2520the%2520coordinated%2520dynamics%2520of%2520parameters%2520in%2520LLMs%252C%2520which%2520elucidates%2520why%2520post-training%2520inherently%2520relies%2520on%2520the%2520foundational%2520capabilities%2520developed%2520during%2520pre-training.%2520Further%2520experiments%2520demonstrate%2520that%2520singular%2520value%2520scaling%2520underpins%2520the%2520temperature-controlled%2520regulatory%2520mechanisms%2520of%2520post-training%252C%2520while%2520the%2520coordinated%2520rotation%2520of%2520singular%2520vectors%2520encodes%2520the%2520essential%2520semantic%2520alignment.%2520These%2520results%2520challenge%2520the%2520prevailing%2520view%2520of%2520the%2520parameter%2520space%2520in%2520large%2520models%2520as%2520a%2520black%2520box%252C%2520uncovering%2520the%2520first%2520clear%2520regularities%2520in%2520how%2520parameters%2520evolve%2520during%2520training%252C%2520and%2520providing%2520a%2520new%2520perspective%2520for%2520deeper%2520investigation%2520into%2520model%2520parameter%2520changes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17866v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Post-Training%20Structural%20Changes%20in%20Large%20Language%20Models&entry.906535625=Xinyu%20He%20and%20Xianghui%20Cao&entry.1292438233=Post-training%20fundamentally%20alters%20the%20behavior%20of%20large%20language%20models%20%28LLMs%29%2C%20yet%20its%20impact%20on%20the%20internal%20parameter%20space%20remains%20poorly%20understood.%20In%20this%20work%2C%20we%20conduct%20a%20systematic%20singular%20value%20decomposition%20%28SVD%29%20analysis%20of%20principal%20linear%20layers%20in%20pretrained%20LLMs%2C%20focusing%20on%20two%20widely%20adopted%20post-training%20methods%3A%20instruction%20tuning%20and%20long-chain-of-thought%20%28Long-CoT%29%20distillation.%20Our%20analysis%20reveals%20two%20unexpected%20and%20robust%20structural%20changes%3A%20%281%29%20a%20near-uniform%20geometric%20scaling%20of%20singular%20values%20across%20layers%3B%20and%20%282%29%20highly%20consistent%20orthogonal%20transformations%20are%20applied%20to%20the%20left%20and%20right%20singular%20vectors%20of%20each%20matrix.%20Based%20on%20these%20findings%2C%20We%20propose%20a%20simple%20yet%20effective%20framework%20to%20describe%20the%20coordinated%20dynamics%20of%20parameters%20in%20LLMs%2C%20which%20elucidates%20why%20post-training%20inherently%20relies%20on%20the%20foundational%20capabilities%20developed%20during%20pre-training.%20Further%20experiments%20demonstrate%20that%20singular%20value%20scaling%20underpins%20the%20temperature-controlled%20regulatory%20mechanisms%20of%20post-training%2C%20while%20the%20coordinated%20rotation%20of%20singular%20vectors%20encodes%20the%20essential%20semantic%20alignment.%20These%20results%20challenge%20the%20prevailing%20view%20of%20the%20parameter%20space%20in%20large%20models%20as%20a%20black%20box%2C%20uncovering%20the%20first%20clear%20regularities%20in%20how%20parameters%20evolve%20during%20training%2C%20and%20providing%20a%20new%20perspective%20for%20deeper%20investigation%20into%20model%20parameter%20changes.&entry.1838667208=http%3A//arxiv.org/abs/2509.17866v4&entry.124074799=Read"},
{"title": "Negatives-Dominant Contrastive Learning for Generalization in Imbalanced Domains", "author": "Meng Cao and Jiexi Liu and Songcan Chen", "abstract": "Imbalanced Domain Generalization (IDG) focuses on mitigating both domain and label shifts, both of which fundamentally shape the model's decision boundaries, particularly under heterogeneous long-tailed distributions across domains. Despite its practical significance, it remains underexplored, primarily due to the technical complexity of handling their entanglement and the paucity of theoretical foundations. In this paper, we begin by theoretically establishing the generalization bound for IDG, highlighting the role of posterior discrepancy and decision margin. This bound motivates us to focus on directly steering decision boundaries, marking a clear departure from existing methods. Subsequently, we technically propose a novel Negative-Dominant Contrastive Learning (NDCL) for IDG to enhance discriminability while enforce posterior consistency across domains. Specifically, inter-class decision-boundary separation is enhanced by placing greater emphasis on negatives as the primary signal in our contrastive learning, naturally amplifying gradient signals for minority classes to avoid the decision boundary being biased toward majority classes. Meanwhile, intra-class compactness is encouraged through a re-weighted cross-entropy strategy, and posterior consistency across domains is enforced through a prediction-central alignment strategy. Finally, rigorous yet challenging experiments on benchmarks validate the effectiveness of our NDCL. The code is available at https://github.com/Alrash/NDCL.", "link": "http://arxiv.org/abs/2601.21999v1", "date": "2026-01-29", "relevancy": 2.544, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5346}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4966}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Negatives-Dominant%20Contrastive%20Learning%20for%20Generalization%20in%20Imbalanced%20Domains&body=Title%3A%20Negatives-Dominant%20Contrastive%20Learning%20for%20Generalization%20in%20Imbalanced%20Domains%0AAuthor%3A%20Meng%20Cao%20and%20Jiexi%20Liu%20and%20Songcan%20Chen%0AAbstract%3A%20Imbalanced%20Domain%20Generalization%20%28IDG%29%20focuses%20on%20mitigating%20both%20domain%20and%20label%20shifts%2C%20both%20of%20which%20fundamentally%20shape%20the%20model%27s%20decision%20boundaries%2C%20particularly%20under%20heterogeneous%20long-tailed%20distributions%20across%20domains.%20Despite%20its%20practical%20significance%2C%20it%20remains%20underexplored%2C%20primarily%20due%20to%20the%20technical%20complexity%20of%20handling%20their%20entanglement%20and%20the%20paucity%20of%20theoretical%20foundations.%20In%20this%20paper%2C%20we%20begin%20by%20theoretically%20establishing%20the%20generalization%20bound%20for%20IDG%2C%20highlighting%20the%20role%20of%20posterior%20discrepancy%20and%20decision%20margin.%20This%20bound%20motivates%20us%20to%20focus%20on%20directly%20steering%20decision%20boundaries%2C%20marking%20a%20clear%20departure%20from%20existing%20methods.%20Subsequently%2C%20we%20technically%20propose%20a%20novel%20Negative-Dominant%20Contrastive%20Learning%20%28NDCL%29%20for%20IDG%20to%20enhance%20discriminability%20while%20enforce%20posterior%20consistency%20across%20domains.%20Specifically%2C%20inter-class%20decision-boundary%20separation%20is%20enhanced%20by%20placing%20greater%20emphasis%20on%20negatives%20as%20the%20primary%20signal%20in%20our%20contrastive%20learning%2C%20naturally%20amplifying%20gradient%20signals%20for%20minority%20classes%20to%20avoid%20the%20decision%20boundary%20being%20biased%20toward%20majority%20classes.%20Meanwhile%2C%20intra-class%20compactness%20is%20encouraged%20through%20a%20re-weighted%20cross-entropy%20strategy%2C%20and%20posterior%20consistency%20across%20domains%20is%20enforced%20through%20a%20prediction-central%20alignment%20strategy.%20Finally%2C%20rigorous%20yet%20challenging%20experiments%20on%20benchmarks%20validate%20the%20effectiveness%20of%20our%20NDCL.%20The%20code%20is%20available%20at%20https%3A//github.com/Alrash/NDCL.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNegatives-Dominant%2520Contrastive%2520Learning%2520for%2520Generalization%2520in%2520Imbalanced%2520Domains%26entry.906535625%3DMeng%2520Cao%2520and%2520Jiexi%2520Liu%2520and%2520Songcan%2520Chen%26entry.1292438233%3DImbalanced%2520Domain%2520Generalization%2520%2528IDG%2529%2520focuses%2520on%2520mitigating%2520both%2520domain%2520and%2520label%2520shifts%252C%2520both%2520of%2520which%2520fundamentally%2520shape%2520the%2520model%2527s%2520decision%2520boundaries%252C%2520particularly%2520under%2520heterogeneous%2520long-tailed%2520distributions%2520across%2520domains.%2520Despite%2520its%2520practical%2520significance%252C%2520it%2520remains%2520underexplored%252C%2520primarily%2520due%2520to%2520the%2520technical%2520complexity%2520of%2520handling%2520their%2520entanglement%2520and%2520the%2520paucity%2520of%2520theoretical%2520foundations.%2520In%2520this%2520paper%252C%2520we%2520begin%2520by%2520theoretically%2520establishing%2520the%2520generalization%2520bound%2520for%2520IDG%252C%2520highlighting%2520the%2520role%2520of%2520posterior%2520discrepancy%2520and%2520decision%2520margin.%2520This%2520bound%2520motivates%2520us%2520to%2520focus%2520on%2520directly%2520steering%2520decision%2520boundaries%252C%2520marking%2520a%2520clear%2520departure%2520from%2520existing%2520methods.%2520Subsequently%252C%2520we%2520technically%2520propose%2520a%2520novel%2520Negative-Dominant%2520Contrastive%2520Learning%2520%2528NDCL%2529%2520for%2520IDG%2520to%2520enhance%2520discriminability%2520while%2520enforce%2520posterior%2520consistency%2520across%2520domains.%2520Specifically%252C%2520inter-class%2520decision-boundary%2520separation%2520is%2520enhanced%2520by%2520placing%2520greater%2520emphasis%2520on%2520negatives%2520as%2520the%2520primary%2520signal%2520in%2520our%2520contrastive%2520learning%252C%2520naturally%2520amplifying%2520gradient%2520signals%2520for%2520minority%2520classes%2520to%2520avoid%2520the%2520decision%2520boundary%2520being%2520biased%2520toward%2520majority%2520classes.%2520Meanwhile%252C%2520intra-class%2520compactness%2520is%2520encouraged%2520through%2520a%2520re-weighted%2520cross-entropy%2520strategy%252C%2520and%2520posterior%2520consistency%2520across%2520domains%2520is%2520enforced%2520through%2520a%2520prediction-central%2520alignment%2520strategy.%2520Finally%252C%2520rigorous%2520yet%2520challenging%2520experiments%2520on%2520benchmarks%2520validate%2520the%2520effectiveness%2520of%2520our%2520NDCL.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/Alrash/NDCL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Negatives-Dominant%20Contrastive%20Learning%20for%20Generalization%20in%20Imbalanced%20Domains&entry.906535625=Meng%20Cao%20and%20Jiexi%20Liu%20and%20Songcan%20Chen&entry.1292438233=Imbalanced%20Domain%20Generalization%20%28IDG%29%20focuses%20on%20mitigating%20both%20domain%20and%20label%20shifts%2C%20both%20of%20which%20fundamentally%20shape%20the%20model%27s%20decision%20boundaries%2C%20particularly%20under%20heterogeneous%20long-tailed%20distributions%20across%20domains.%20Despite%20its%20practical%20significance%2C%20it%20remains%20underexplored%2C%20primarily%20due%20to%20the%20technical%20complexity%20of%20handling%20their%20entanglement%20and%20the%20paucity%20of%20theoretical%20foundations.%20In%20this%20paper%2C%20we%20begin%20by%20theoretically%20establishing%20the%20generalization%20bound%20for%20IDG%2C%20highlighting%20the%20role%20of%20posterior%20discrepancy%20and%20decision%20margin.%20This%20bound%20motivates%20us%20to%20focus%20on%20directly%20steering%20decision%20boundaries%2C%20marking%20a%20clear%20departure%20from%20existing%20methods.%20Subsequently%2C%20we%20technically%20propose%20a%20novel%20Negative-Dominant%20Contrastive%20Learning%20%28NDCL%29%20for%20IDG%20to%20enhance%20discriminability%20while%20enforce%20posterior%20consistency%20across%20domains.%20Specifically%2C%20inter-class%20decision-boundary%20separation%20is%20enhanced%20by%20placing%20greater%20emphasis%20on%20negatives%20as%20the%20primary%20signal%20in%20our%20contrastive%20learning%2C%20naturally%20amplifying%20gradient%20signals%20for%20minority%20classes%20to%20avoid%20the%20decision%20boundary%20being%20biased%20toward%20majority%20classes.%20Meanwhile%2C%20intra-class%20compactness%20is%20encouraged%20through%20a%20re-weighted%20cross-entropy%20strategy%2C%20and%20posterior%20consistency%20across%20domains%20is%20enforced%20through%20a%20prediction-central%20alignment%20strategy.%20Finally%2C%20rigorous%20yet%20challenging%20experiments%20on%20benchmarks%20validate%20the%20effectiveness%20of%20our%20NDCL.%20The%20code%20is%20available%20at%20https%3A//github.com/Alrash/NDCL.&entry.1838667208=http%3A//arxiv.org/abs/2601.21999v1&entry.124074799=Read"},
{"title": "A Trainable Optimizer", "author": "Ruiqi Wang and Diego Klabjan", "abstract": "The concept of learning to optimize involves utilizing a trainable optimization strategy rather than relying on manually defined full gradient estimations such as ADAM. We present a framework that jointly trains the full gradient estimator and the trainable weights of the model. Specifically, we prove that pseudo-linear TO (Trainable Optimizer), a linear approximation of the full gradient, matches SGD's convergence rate while effectively reducing variance. Pseudo-linear TO incurs negligible computational overhead, requiring only minimal additional tensor multiplications. To further improve computational efficiency, we introduce two simplified variants of Pseudo-linear TO. Experiments demonstrate that TO methods converge faster than benchmark algorithms (e.g., ADAM) in both strongly convex and non-convex settings, and fine tuning of an LLM.", "link": "http://arxiv.org/abs/2508.01764v2", "date": "2026-01-29", "relevancy": 2.5372, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5348}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4957}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Trainable%20Optimizer&body=Title%3A%20A%20Trainable%20Optimizer%0AAuthor%3A%20Ruiqi%20Wang%20and%20Diego%20Klabjan%0AAbstract%3A%20The%20concept%20of%20learning%20to%20optimize%20involves%20utilizing%20a%20trainable%20optimization%20strategy%20rather%20than%20relying%20on%20manually%20defined%20full%20gradient%20estimations%20such%20as%20ADAM.%20We%20present%20a%20framework%20that%20jointly%20trains%20the%20full%20gradient%20estimator%20and%20the%20trainable%20weights%20of%20the%20model.%20Specifically%2C%20we%20prove%20that%20pseudo-linear%20TO%20%28Trainable%20Optimizer%29%2C%20a%20linear%20approximation%20of%20the%20full%20gradient%2C%20matches%20SGD%27s%20convergence%20rate%20while%20effectively%20reducing%20variance.%20Pseudo-linear%20TO%20incurs%20negligible%20computational%20overhead%2C%20requiring%20only%20minimal%20additional%20tensor%20multiplications.%20To%20further%20improve%20computational%20efficiency%2C%20we%20introduce%20two%20simplified%20variants%20of%20Pseudo-linear%20TO.%20Experiments%20demonstrate%20that%20TO%20methods%20converge%20faster%20than%20benchmark%20algorithms%20%28e.g.%2C%20ADAM%29%20in%20both%20strongly%20convex%20and%20non-convex%20settings%2C%20and%20fine%20tuning%20of%20an%20LLM.%0ALink%3A%20http%3A//arxiv.org/abs/2508.01764v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Trainable%2520Optimizer%26entry.906535625%3DRuiqi%2520Wang%2520and%2520Diego%2520Klabjan%26entry.1292438233%3DThe%2520concept%2520of%2520learning%2520to%2520optimize%2520involves%2520utilizing%2520a%2520trainable%2520optimization%2520strategy%2520rather%2520than%2520relying%2520on%2520manually%2520defined%2520full%2520gradient%2520estimations%2520such%2520as%2520ADAM.%2520We%2520present%2520a%2520framework%2520that%2520jointly%2520trains%2520the%2520full%2520gradient%2520estimator%2520and%2520the%2520trainable%2520weights%2520of%2520the%2520model.%2520Specifically%252C%2520we%2520prove%2520that%2520pseudo-linear%2520TO%2520%2528Trainable%2520Optimizer%2529%252C%2520a%2520linear%2520approximation%2520of%2520the%2520full%2520gradient%252C%2520matches%2520SGD%2527s%2520convergence%2520rate%2520while%2520effectively%2520reducing%2520variance.%2520Pseudo-linear%2520TO%2520incurs%2520negligible%2520computational%2520overhead%252C%2520requiring%2520only%2520minimal%2520additional%2520tensor%2520multiplications.%2520To%2520further%2520improve%2520computational%2520efficiency%252C%2520we%2520introduce%2520two%2520simplified%2520variants%2520of%2520Pseudo-linear%2520TO.%2520Experiments%2520demonstrate%2520that%2520TO%2520methods%2520converge%2520faster%2520than%2520benchmark%2520algorithms%2520%2528e.g.%252C%2520ADAM%2529%2520in%2520both%2520strongly%2520convex%2520and%2520non-convex%2520settings%252C%2520and%2520fine%2520tuning%2520of%2520an%2520LLM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01764v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Trainable%20Optimizer&entry.906535625=Ruiqi%20Wang%20and%20Diego%20Klabjan&entry.1292438233=The%20concept%20of%20learning%20to%20optimize%20involves%20utilizing%20a%20trainable%20optimization%20strategy%20rather%20than%20relying%20on%20manually%20defined%20full%20gradient%20estimations%20such%20as%20ADAM.%20We%20present%20a%20framework%20that%20jointly%20trains%20the%20full%20gradient%20estimator%20and%20the%20trainable%20weights%20of%20the%20model.%20Specifically%2C%20we%20prove%20that%20pseudo-linear%20TO%20%28Trainable%20Optimizer%29%2C%20a%20linear%20approximation%20of%20the%20full%20gradient%2C%20matches%20SGD%27s%20convergence%20rate%20while%20effectively%20reducing%20variance.%20Pseudo-linear%20TO%20incurs%20negligible%20computational%20overhead%2C%20requiring%20only%20minimal%20additional%20tensor%20multiplications.%20To%20further%20improve%20computational%20efficiency%2C%20we%20introduce%20two%20simplified%20variants%20of%20Pseudo-linear%20TO.%20Experiments%20demonstrate%20that%20TO%20methods%20converge%20faster%20than%20benchmark%20algorithms%20%28e.g.%2C%20ADAM%29%20in%20both%20strongly%20convex%20and%20non-convex%20settings%2C%20and%20fine%20tuning%20of%20an%20LLM.&entry.1838667208=http%3A//arxiv.org/abs/2508.01764v2&entry.124074799=Read"},
{"title": "Value-Based Pre-Training with Downstream Feedback", "author": "Shuqi Ke and Giulia Fanti", "abstract": "Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.", "link": "http://arxiv.org/abs/2601.22108v1", "date": "2026-01-29", "relevancy": 2.537, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5288}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5023}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Value-Based%20Pre-Training%20with%20Downstream%20Feedback&body=Title%3A%20Value-Based%20Pre-Training%20with%20Downstream%20Feedback%0AAuthor%3A%20Shuqi%20Ke%20and%20Giulia%20Fanti%0AAbstract%3A%20Can%20a%20small%20amount%20of%20verified%20goal%20information%20steer%20the%20expensive%20self-supervised%20pretraining%20of%20foundation%20models%3F%20Standard%20pretraining%20optimizes%20a%20fixed%20proxy%20objective%20%28e.g.%2C%20next-token%20prediction%29%2C%20which%20can%20misallocate%20compute%20away%20from%20downstream%20capabilities%20of%20interest.%20We%20introduce%20V-Pretraining%3A%20a%20value-based%2C%20modality-agnostic%20method%20for%20controlled%20continued%20pretraining%20in%20which%20a%20lightweight%20task%20designer%20reshapes%20the%20pretraining%20task%20to%20maximize%20the%20value%20of%20each%20gradient%20step.%20For%20example%2C%20consider%20self-supervised%20learning%20%28SSL%29%20with%20sample%20augmentation.%20The%20V-Pretraining%20task%20designer%20selects%20pretraining%20tasks%20%28e.g.%2C%20augmentations%29%20for%20which%20the%20pretraining%20loss%20gradient%20is%20aligned%20with%20a%20gradient%20computed%20over%20a%20downstream%20task%20%28e.g.%2C%20image%20segmentation%29.%20This%20helps%20steer%20pretraining%20towards%20relevant%20downstream%20capabilities.%20Notably%2C%20the%20pretrained%20model%20is%20never%20updated%20on%20downstream%20task%20labels%3B%20they%20are%20used%20only%20to%20shape%20the%20pretraining%20task.%20Under%20matched%20learner%20update%20budgets%2C%20V-Pretraining%20of%200.5B--7B%20language%20models%20improves%20reasoning%20%28GSM8K%20test%20Pass%401%29%20by%20up%20to%2018%25%20relative%20over%20standard%20next-token%20prediction%20using%20only%2012%25%20of%20GSM8K%20training%20examples%20as%20feedback.%20In%20vision%20SSL%2C%20we%20improve%20the%20state-of-the-art%20results%20on%20ADE20K%20by%20up%20to%201.07%20mIoU%20and%20reduce%20NYUv2%20RMSE%20while%20improving%20ImageNet%20linear%20accuracy%2C%20and%20we%20provide%20pilot%20evidence%20of%20improved%20token%20efficiency%20in%20continued%20pretraining.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22108v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DValue-Based%2520Pre-Training%2520with%2520Downstream%2520Feedback%26entry.906535625%3DShuqi%2520Ke%2520and%2520Giulia%2520Fanti%26entry.1292438233%3DCan%2520a%2520small%2520amount%2520of%2520verified%2520goal%2520information%2520steer%2520the%2520expensive%2520self-supervised%2520pretraining%2520of%2520foundation%2520models%253F%2520Standard%2520pretraining%2520optimizes%2520a%2520fixed%2520proxy%2520objective%2520%2528e.g.%252C%2520next-token%2520prediction%2529%252C%2520which%2520can%2520misallocate%2520compute%2520away%2520from%2520downstream%2520capabilities%2520of%2520interest.%2520We%2520introduce%2520V-Pretraining%253A%2520a%2520value-based%252C%2520modality-agnostic%2520method%2520for%2520controlled%2520continued%2520pretraining%2520in%2520which%2520a%2520lightweight%2520task%2520designer%2520reshapes%2520the%2520pretraining%2520task%2520to%2520maximize%2520the%2520value%2520of%2520each%2520gradient%2520step.%2520For%2520example%252C%2520consider%2520self-supervised%2520learning%2520%2528SSL%2529%2520with%2520sample%2520augmentation.%2520The%2520V-Pretraining%2520task%2520designer%2520selects%2520pretraining%2520tasks%2520%2528e.g.%252C%2520augmentations%2529%2520for%2520which%2520the%2520pretraining%2520loss%2520gradient%2520is%2520aligned%2520with%2520a%2520gradient%2520computed%2520over%2520a%2520downstream%2520task%2520%2528e.g.%252C%2520image%2520segmentation%2529.%2520This%2520helps%2520steer%2520pretraining%2520towards%2520relevant%2520downstream%2520capabilities.%2520Notably%252C%2520the%2520pretrained%2520model%2520is%2520never%2520updated%2520on%2520downstream%2520task%2520labels%253B%2520they%2520are%2520used%2520only%2520to%2520shape%2520the%2520pretraining%2520task.%2520Under%2520matched%2520learner%2520update%2520budgets%252C%2520V-Pretraining%2520of%25200.5B--7B%2520language%2520models%2520improves%2520reasoning%2520%2528GSM8K%2520test%2520Pass%25401%2529%2520by%2520up%2520to%252018%2525%2520relative%2520over%2520standard%2520next-token%2520prediction%2520using%2520only%252012%2525%2520of%2520GSM8K%2520training%2520examples%2520as%2520feedback.%2520In%2520vision%2520SSL%252C%2520we%2520improve%2520the%2520state-of-the-art%2520results%2520on%2520ADE20K%2520by%2520up%2520to%25201.07%2520mIoU%2520and%2520reduce%2520NYUv2%2520RMSE%2520while%2520improving%2520ImageNet%2520linear%2520accuracy%252C%2520and%2520we%2520provide%2520pilot%2520evidence%2520of%2520improved%2520token%2520efficiency%2520in%2520continued%2520pretraining.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22108v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Value-Based%20Pre-Training%20with%20Downstream%20Feedback&entry.906535625=Shuqi%20Ke%20and%20Giulia%20Fanti&entry.1292438233=Can%20a%20small%20amount%20of%20verified%20goal%20information%20steer%20the%20expensive%20self-supervised%20pretraining%20of%20foundation%20models%3F%20Standard%20pretraining%20optimizes%20a%20fixed%20proxy%20objective%20%28e.g.%2C%20next-token%20prediction%29%2C%20which%20can%20misallocate%20compute%20away%20from%20downstream%20capabilities%20of%20interest.%20We%20introduce%20V-Pretraining%3A%20a%20value-based%2C%20modality-agnostic%20method%20for%20controlled%20continued%20pretraining%20in%20which%20a%20lightweight%20task%20designer%20reshapes%20the%20pretraining%20task%20to%20maximize%20the%20value%20of%20each%20gradient%20step.%20For%20example%2C%20consider%20self-supervised%20learning%20%28SSL%29%20with%20sample%20augmentation.%20The%20V-Pretraining%20task%20designer%20selects%20pretraining%20tasks%20%28e.g.%2C%20augmentations%29%20for%20which%20the%20pretraining%20loss%20gradient%20is%20aligned%20with%20a%20gradient%20computed%20over%20a%20downstream%20task%20%28e.g.%2C%20image%20segmentation%29.%20This%20helps%20steer%20pretraining%20towards%20relevant%20downstream%20capabilities.%20Notably%2C%20the%20pretrained%20model%20is%20never%20updated%20on%20downstream%20task%20labels%3B%20they%20are%20used%20only%20to%20shape%20the%20pretraining%20task.%20Under%20matched%20learner%20update%20budgets%2C%20V-Pretraining%20of%200.5B--7B%20language%20models%20improves%20reasoning%20%28GSM8K%20test%20Pass%401%29%20by%20up%20to%2018%25%20relative%20over%20standard%20next-token%20prediction%20using%20only%2012%25%20of%20GSM8K%20training%20examples%20as%20feedback.%20In%20vision%20SSL%2C%20we%20improve%20the%20state-of-the-art%20results%20on%20ADE20K%20by%20up%20to%201.07%20mIoU%20and%20reduce%20NYUv2%20RMSE%20while%20improving%20ImageNet%20linear%20accuracy%2C%20and%20we%20provide%20pilot%20evidence%20of%20improved%20token%20efficiency%20in%20continued%20pretraining.&entry.1838667208=http%3A//arxiv.org/abs/2601.22108v1&entry.124074799=Read"},
{"title": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data", "author": "Grzegorz Stefanski and Alberto Presta and Michal Byra", "abstract": "In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.", "link": "http://arxiv.org/abs/2601.22141v1", "date": "2026-01-29", "relevancy": 2.5329, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5103}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5056}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Routing%20the%20Lottery%3A%20Adaptive%20Subnetworks%20for%20Heterogeneous%20Data&body=Title%3A%20Routing%20the%20Lottery%3A%20Adaptive%20Subnetworks%20for%20Heterogeneous%20Data%0AAuthor%3A%20Grzegorz%20Stefanski%20and%20Alberto%20Presta%20and%20Michal%20Byra%0AAbstract%3A%20In%20pruning%2C%20the%20Lottery%20Ticket%20Hypothesis%20posits%20that%20large%20networks%20contain%20sparse%20subnetworks%2C%20or%20winning%20tickets%2C%20that%20can%20be%20trained%20in%20isolation%20to%20match%20the%20performance%20of%20their%20dense%20counterparts.%20However%2C%20most%20existing%20approaches%20assume%20a%20single%20universal%20winning%20ticket%20shared%20across%20all%20inputs%2C%20ignoring%20the%20inherent%20heterogeneity%20of%20real-world%20data.%20In%20this%20work%2C%20we%20propose%20Routing%20the%20Lottery%20%28RTL%29%2C%20an%20adaptive%20pruning%20framework%20that%20discovers%20multiple%20specialized%20subnetworks%2C%20called%20adaptive%20tickets%2C%20each%20tailored%20to%20a%20class%2C%20semantic%20cluster%2C%20or%20environmental%20condition.%20Across%20diverse%20datasets%20and%20tasks%2C%20RTL%20consistently%20outperforms%20single-%20and%20multi-model%20baselines%20in%20balanced%20accuracy%20and%20recall%2C%20while%20using%20up%20to%2010%20times%20fewer%20parameters%20than%20independent%20models%20and%20exhibiting%20semantically%20aligned.%20Furthermore%2C%20we%20identify%20subnetwork%20collapse%2C%20a%20performance%20drop%20under%20aggressive%20pruning%2C%20and%20introduce%20a%20subnetwork%20similarity%20score%20that%20enables%20label-free%20diagnosis%20of%20oversparsification.%20Overall%2C%20our%20results%20recast%20pruning%20as%20a%20mechanism%20for%20aligning%20model%20structure%20with%20data%20heterogeneity%2C%20paving%20the%20way%20toward%20more%20modular%20and%20context-aware%20deep%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22141v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRouting%2520the%2520Lottery%253A%2520Adaptive%2520Subnetworks%2520for%2520Heterogeneous%2520Data%26entry.906535625%3DGrzegorz%2520Stefanski%2520and%2520Alberto%2520Presta%2520and%2520Michal%2520Byra%26entry.1292438233%3DIn%2520pruning%252C%2520the%2520Lottery%2520Ticket%2520Hypothesis%2520posits%2520that%2520large%2520networks%2520contain%2520sparse%2520subnetworks%252C%2520or%2520winning%2520tickets%252C%2520that%2520can%2520be%2520trained%2520in%2520isolation%2520to%2520match%2520the%2520performance%2520of%2520their%2520dense%2520counterparts.%2520However%252C%2520most%2520existing%2520approaches%2520assume%2520a%2520single%2520universal%2520winning%2520ticket%2520shared%2520across%2520all%2520inputs%252C%2520ignoring%2520the%2520inherent%2520heterogeneity%2520of%2520real-world%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%2520Routing%2520the%2520Lottery%2520%2528RTL%2529%252C%2520an%2520adaptive%2520pruning%2520framework%2520that%2520discovers%2520multiple%2520specialized%2520subnetworks%252C%2520called%2520adaptive%2520tickets%252C%2520each%2520tailored%2520to%2520a%2520class%252C%2520semantic%2520cluster%252C%2520or%2520environmental%2520condition.%2520Across%2520diverse%2520datasets%2520and%2520tasks%252C%2520RTL%2520consistently%2520outperforms%2520single-%2520and%2520multi-model%2520baselines%2520in%2520balanced%2520accuracy%2520and%2520recall%252C%2520while%2520using%2520up%2520to%252010%2520times%2520fewer%2520parameters%2520than%2520independent%2520models%2520and%2520exhibiting%2520semantically%2520aligned.%2520Furthermore%252C%2520we%2520identify%2520subnetwork%2520collapse%252C%2520a%2520performance%2520drop%2520under%2520aggressive%2520pruning%252C%2520and%2520introduce%2520a%2520subnetwork%2520similarity%2520score%2520that%2520enables%2520label-free%2520diagnosis%2520of%2520oversparsification.%2520Overall%252C%2520our%2520results%2520recast%2520pruning%2520as%2520a%2520mechanism%2520for%2520aligning%2520model%2520structure%2520with%2520data%2520heterogeneity%252C%2520paving%2520the%2520way%2520toward%2520more%2520modular%2520and%2520context-aware%2520deep%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22141v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Routing%20the%20Lottery%3A%20Adaptive%20Subnetworks%20for%20Heterogeneous%20Data&entry.906535625=Grzegorz%20Stefanski%20and%20Alberto%20Presta%20and%20Michal%20Byra&entry.1292438233=In%20pruning%2C%20the%20Lottery%20Ticket%20Hypothesis%20posits%20that%20large%20networks%20contain%20sparse%20subnetworks%2C%20or%20winning%20tickets%2C%20that%20can%20be%20trained%20in%20isolation%20to%20match%20the%20performance%20of%20their%20dense%20counterparts.%20However%2C%20most%20existing%20approaches%20assume%20a%20single%20universal%20winning%20ticket%20shared%20across%20all%20inputs%2C%20ignoring%20the%20inherent%20heterogeneity%20of%20real-world%20data.%20In%20this%20work%2C%20we%20propose%20Routing%20the%20Lottery%20%28RTL%29%2C%20an%20adaptive%20pruning%20framework%20that%20discovers%20multiple%20specialized%20subnetworks%2C%20called%20adaptive%20tickets%2C%20each%20tailored%20to%20a%20class%2C%20semantic%20cluster%2C%20or%20environmental%20condition.%20Across%20diverse%20datasets%20and%20tasks%2C%20RTL%20consistently%20outperforms%20single-%20and%20multi-model%20baselines%20in%20balanced%20accuracy%20and%20recall%2C%20while%20using%20up%20to%2010%20times%20fewer%20parameters%20than%20independent%20models%20and%20exhibiting%20semantically%20aligned.%20Furthermore%2C%20we%20identify%20subnetwork%20collapse%2C%20a%20performance%20drop%20under%20aggressive%20pruning%2C%20and%20introduce%20a%20subnetwork%20similarity%20score%20that%20enables%20label-free%20diagnosis%20of%20oversparsification.%20Overall%2C%20our%20results%20recast%20pruning%20as%20a%20mechanism%20for%20aligning%20model%20structure%20with%20data%20heterogeneity%2C%20paving%20the%20way%20toward%20more%20modular%20and%20context-aware%20deep%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2601.22141v1&entry.124074799=Read"},
{"title": "ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models", "author": "Bowen Fang and Wen Ye and Yunyue Su and Jinghao Zhang and Qiang Liu and Yesheng Liu and Xin Sun and Shu Wu and Jiabing Yang and Baole Wei and Liang Wang", "abstract": "Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.", "link": "http://arxiv.org/abs/2601.21947v1", "date": "2026-01-29", "relevancy": 2.5276, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5001}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToolWeaver%3A%20Weaving%20Collaborative%20Semantics%20for%20Scalable%20Tool%20Use%20in%20Large%20Language%20Models&body=Title%3A%20ToolWeaver%3A%20Weaving%20Collaborative%20Semantics%20for%20Scalable%20Tool%20Use%20in%20Large%20Language%20Models%0AAuthor%3A%20Bowen%20Fang%20and%20Wen%20Ye%20and%20Yunyue%20Su%20and%20Jinghao%20Zhang%20and%20Qiang%20Liu%20and%20Yesheng%20Liu%20and%20Xin%20Sun%20and%20Shu%20Wu%20and%20Jiabing%20Yang%20and%20Baole%20Wei%20and%20Liang%20Wang%0AAbstract%3A%20Prevalent%20retrieval-based%20tool-use%20pipelines%20struggle%20with%20a%20dual%20semantic%20challenge%3A%20their%20retrievers%20often%20employ%20encoders%20that%20fail%20to%20capture%20complex%20semantics%2C%20while%20the%20Large%20Language%20Model%20%28LLM%29%20itself%20lacks%20intrinsic%20tool%20knowledge%20from%20its%20natural%20language%20pretraining.%20Generative%20methods%20offer%20a%20powerful%20alternative%20by%20unifying%20selection%20and%20execution%2C%20tasking%20the%20LLM%20to%20directly%20learn%20and%20generate%20tool%20identifiers.%20However%2C%20the%20common%20practice%20of%20mapping%20each%20tool%20to%20a%20unique%20new%20token%20introduces%20substantial%20limitations%3A%20it%20creates%20a%20scalability%20and%20generalization%20crisis%2C%20as%20the%20vocabulary%20size%20explodes%20and%20each%20tool%20is%20assigned%20a%20semantically%20isolated%20token.%20This%20approach%20also%20creates%20a%20semantic%20bottleneck%20that%20hinders%20the%20learning%20of%20collaborative%20tool%20relationships%2C%20as%20the%20model%20must%20infer%20them%20from%20sparse%20co-occurrences%20of%20monolithic%20tool%20IDs%20within%20a%20vast%20library.%20To%20address%20these%20limitations%2C%20we%20propose%20ToolWeaver%2C%20a%20novel%20generative%20tool%20learning%20framework%20that%20encodes%20tools%20into%20hierarchical%20sequences.%20This%20approach%20makes%20vocabulary%20expansion%20logarithmic%20to%20the%20number%20of%20tools.%20Crucially%2C%20it%20enables%20the%20model%20to%20learn%20collaborative%20patterns%20from%20the%20dense%20co-occurrence%20of%20shared%20codes%2C%20rather%20than%20the%20sparse%20co-occurrence%20of%20monolithic%20tool%20IDs.%20We%20generate%20these%20structured%20codes%20through%20a%20novel%20tokenization%20process%20designed%20to%20weave%20together%20a%20tool%27s%20intrinsic%20semantics%20with%20its%20extrinsic%20co-usage%20patterns.%20These%20structured%20codes%20are%20then%20integrated%20into%20the%20LLM%20through%20a%20generative%20alignment%20stage%2C%20where%20the%20model%20is%20fine-tuned%20to%20produce%20the%20hierarchical%20code%20sequences.%20Evaluation%20results%20with%20nearly%2047%2C000%20tools%20show%20that%20ToolWeaver%20significantly%20outperforms%20state-of-the-art%20methods%2C%20establishing%20a%20more%20scalable%2C%20generalizable%2C%20and%20semantically-aware%20foundation%20for%20advanced%20tool-augmented%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToolWeaver%253A%2520Weaving%2520Collaborative%2520Semantics%2520for%2520Scalable%2520Tool%2520Use%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DBowen%2520Fang%2520and%2520Wen%2520Ye%2520and%2520Yunyue%2520Su%2520and%2520Jinghao%2520Zhang%2520and%2520Qiang%2520Liu%2520and%2520Yesheng%2520Liu%2520and%2520Xin%2520Sun%2520and%2520Shu%2520Wu%2520and%2520Jiabing%2520Yang%2520and%2520Baole%2520Wei%2520and%2520Liang%2520Wang%26entry.1292438233%3DPrevalent%2520retrieval-based%2520tool-use%2520pipelines%2520struggle%2520with%2520a%2520dual%2520semantic%2520challenge%253A%2520their%2520retrievers%2520often%2520employ%2520encoders%2520that%2520fail%2520to%2520capture%2520complex%2520semantics%252C%2520while%2520the%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520itself%2520lacks%2520intrinsic%2520tool%2520knowledge%2520from%2520its%2520natural%2520language%2520pretraining.%2520Generative%2520methods%2520offer%2520a%2520powerful%2520alternative%2520by%2520unifying%2520selection%2520and%2520execution%252C%2520tasking%2520the%2520LLM%2520to%2520directly%2520learn%2520and%2520generate%2520tool%2520identifiers.%2520However%252C%2520the%2520common%2520practice%2520of%2520mapping%2520each%2520tool%2520to%2520a%2520unique%2520new%2520token%2520introduces%2520substantial%2520limitations%253A%2520it%2520creates%2520a%2520scalability%2520and%2520generalization%2520crisis%252C%2520as%2520the%2520vocabulary%2520size%2520explodes%2520and%2520each%2520tool%2520is%2520assigned%2520a%2520semantically%2520isolated%2520token.%2520This%2520approach%2520also%2520creates%2520a%2520semantic%2520bottleneck%2520that%2520hinders%2520the%2520learning%2520of%2520collaborative%2520tool%2520relationships%252C%2520as%2520the%2520model%2520must%2520infer%2520them%2520from%2520sparse%2520co-occurrences%2520of%2520monolithic%2520tool%2520IDs%2520within%2520a%2520vast%2520library.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520ToolWeaver%252C%2520a%2520novel%2520generative%2520tool%2520learning%2520framework%2520that%2520encodes%2520tools%2520into%2520hierarchical%2520sequences.%2520This%2520approach%2520makes%2520vocabulary%2520expansion%2520logarithmic%2520to%2520the%2520number%2520of%2520tools.%2520Crucially%252C%2520it%2520enables%2520the%2520model%2520to%2520learn%2520collaborative%2520patterns%2520from%2520the%2520dense%2520co-occurrence%2520of%2520shared%2520codes%252C%2520rather%2520than%2520the%2520sparse%2520co-occurrence%2520of%2520monolithic%2520tool%2520IDs.%2520We%2520generate%2520these%2520structured%2520codes%2520through%2520a%2520novel%2520tokenization%2520process%2520designed%2520to%2520weave%2520together%2520a%2520tool%2527s%2520intrinsic%2520semantics%2520with%2520its%2520extrinsic%2520co-usage%2520patterns.%2520These%2520structured%2520codes%2520are%2520then%2520integrated%2520into%2520the%2520LLM%2520through%2520a%2520generative%2520alignment%2520stage%252C%2520where%2520the%2520model%2520is%2520fine-tuned%2520to%2520produce%2520the%2520hierarchical%2520code%2520sequences.%2520Evaluation%2520results%2520with%2520nearly%252047%252C000%2520tools%2520show%2520that%2520ToolWeaver%2520significantly%2520outperforms%2520state-of-the-art%2520methods%252C%2520establishing%2520a%2520more%2520scalable%252C%2520generalizable%252C%2520and%2520semantically-aware%2520foundation%2520for%2520advanced%2520tool-augmented%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToolWeaver%3A%20Weaving%20Collaborative%20Semantics%20for%20Scalable%20Tool%20Use%20in%20Large%20Language%20Models&entry.906535625=Bowen%20Fang%20and%20Wen%20Ye%20and%20Yunyue%20Su%20and%20Jinghao%20Zhang%20and%20Qiang%20Liu%20and%20Yesheng%20Liu%20and%20Xin%20Sun%20and%20Shu%20Wu%20and%20Jiabing%20Yang%20and%20Baole%20Wei%20and%20Liang%20Wang&entry.1292438233=Prevalent%20retrieval-based%20tool-use%20pipelines%20struggle%20with%20a%20dual%20semantic%20challenge%3A%20their%20retrievers%20often%20employ%20encoders%20that%20fail%20to%20capture%20complex%20semantics%2C%20while%20the%20Large%20Language%20Model%20%28LLM%29%20itself%20lacks%20intrinsic%20tool%20knowledge%20from%20its%20natural%20language%20pretraining.%20Generative%20methods%20offer%20a%20powerful%20alternative%20by%20unifying%20selection%20and%20execution%2C%20tasking%20the%20LLM%20to%20directly%20learn%20and%20generate%20tool%20identifiers.%20However%2C%20the%20common%20practice%20of%20mapping%20each%20tool%20to%20a%20unique%20new%20token%20introduces%20substantial%20limitations%3A%20it%20creates%20a%20scalability%20and%20generalization%20crisis%2C%20as%20the%20vocabulary%20size%20explodes%20and%20each%20tool%20is%20assigned%20a%20semantically%20isolated%20token.%20This%20approach%20also%20creates%20a%20semantic%20bottleneck%20that%20hinders%20the%20learning%20of%20collaborative%20tool%20relationships%2C%20as%20the%20model%20must%20infer%20them%20from%20sparse%20co-occurrences%20of%20monolithic%20tool%20IDs%20within%20a%20vast%20library.%20To%20address%20these%20limitations%2C%20we%20propose%20ToolWeaver%2C%20a%20novel%20generative%20tool%20learning%20framework%20that%20encodes%20tools%20into%20hierarchical%20sequences.%20This%20approach%20makes%20vocabulary%20expansion%20logarithmic%20to%20the%20number%20of%20tools.%20Crucially%2C%20it%20enables%20the%20model%20to%20learn%20collaborative%20patterns%20from%20the%20dense%20co-occurrence%20of%20shared%20codes%2C%20rather%20than%20the%20sparse%20co-occurrence%20of%20monolithic%20tool%20IDs.%20We%20generate%20these%20structured%20codes%20through%20a%20novel%20tokenization%20process%20designed%20to%20weave%20together%20a%20tool%27s%20intrinsic%20semantics%20with%20its%20extrinsic%20co-usage%20patterns.%20These%20structured%20codes%20are%20then%20integrated%20into%20the%20LLM%20through%20a%20generative%20alignment%20stage%2C%20where%20the%20model%20is%20fine-tuned%20to%20produce%20the%20hierarchical%20code%20sequences.%20Evaluation%20results%20with%20nearly%2047%2C000%20tools%20show%20that%20ToolWeaver%20significantly%20outperforms%20state-of-the-art%20methods%2C%20establishing%20a%20more%20scalable%2C%20generalizable%2C%20and%20semantically-aware%20foundation%20for%20advanced%20tool-augmented%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2601.21947v1&entry.124074799=Read"},
{"title": "Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves", "author": "Jonas Knupp and Jan Hendrik Metzen and Jeremias Bohn and Georg Groh and Kristian Kersting", "abstract": "Depth-recurrence facilitates latent reasoning by sharing parameters across depths. However, prior work lacks combined FLOP-, parameter-, and memory-matched baselines, underutilizes depth-recurrence due to partially fixed layer stacks, and ignores the bottleneck of constant hidden-sizes that restricts many-step latent reasoning. To address this, we introduce a modular framework of depth-recurrent attention mixtures (Dreamer), combining sequence attention, depth attention, and sparse expert attention. It alleviates the hidden-size bottleneck through attention along depth, decouples scaling dimensions, and allows depth-recurrent models to scale efficiently and effectively. Across language reasoning benchmarks, our models require 2 to 8x fewer training tokens for the same accuracy as FLOP-, parameter-, and memory-matched SOTA, and outperform ca. 2x larger SOTA models with the same training tokens. We further present insights into knowledge usage across depths, e.g., showing 2 to 11x larger expert selection diversity than SOTA MoEs.", "link": "http://arxiv.org/abs/2601.21582v1", "date": "2026-01-29", "relevancy": 2.5267, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5405}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4897}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth-Recurrent%20Attention%20Mixtures%3A%20Giving%20Latent%20Reasoning%20the%20Attention%20it%20Deserves&body=Title%3A%20Depth-Recurrent%20Attention%20Mixtures%3A%20Giving%20Latent%20Reasoning%20the%20Attention%20it%20Deserves%0AAuthor%3A%20Jonas%20Knupp%20and%20Jan%20Hendrik%20Metzen%20and%20Jeremias%20Bohn%20and%20Georg%20Groh%20and%20Kristian%20Kersting%0AAbstract%3A%20Depth-recurrence%20facilitates%20latent%20reasoning%20by%20sharing%20parameters%20across%20depths.%20However%2C%20prior%20work%20lacks%20combined%20FLOP-%2C%20parameter-%2C%20and%20memory-matched%20baselines%2C%20underutilizes%20depth-recurrence%20due%20to%20partially%20fixed%20layer%20stacks%2C%20and%20ignores%20the%20bottleneck%20of%20constant%20hidden-sizes%20that%20restricts%20many-step%20latent%20reasoning.%20To%20address%20this%2C%20we%20introduce%20a%20modular%20framework%20of%20depth-recurrent%20attention%20mixtures%20%28Dreamer%29%2C%20combining%20sequence%20attention%2C%20depth%20attention%2C%20and%20sparse%20expert%20attention.%20It%20alleviates%20the%20hidden-size%20bottleneck%20through%20attention%20along%20depth%2C%20decouples%20scaling%20dimensions%2C%20and%20allows%20depth-recurrent%20models%20to%20scale%20efficiently%20and%20effectively.%20Across%20language%20reasoning%20benchmarks%2C%20our%20models%20require%202%20to%208x%20fewer%20training%20tokens%20for%20the%20same%20accuracy%20as%20FLOP-%2C%20parameter-%2C%20and%20memory-matched%20SOTA%2C%20and%20outperform%20ca.%202x%20larger%20SOTA%20models%20with%20the%20same%20training%20tokens.%20We%20further%20present%20insights%20into%20knowledge%20usage%20across%20depths%2C%20e.g.%2C%20showing%202%20to%2011x%20larger%20expert%20selection%20diversity%20than%20SOTA%20MoEs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth-Recurrent%2520Attention%2520Mixtures%253A%2520Giving%2520Latent%2520Reasoning%2520the%2520Attention%2520it%2520Deserves%26entry.906535625%3DJonas%2520Knupp%2520and%2520Jan%2520Hendrik%2520Metzen%2520and%2520Jeremias%2520Bohn%2520and%2520Georg%2520Groh%2520and%2520Kristian%2520Kersting%26entry.1292438233%3DDepth-recurrence%2520facilitates%2520latent%2520reasoning%2520by%2520sharing%2520parameters%2520across%2520depths.%2520However%252C%2520prior%2520work%2520lacks%2520combined%2520FLOP-%252C%2520parameter-%252C%2520and%2520memory-matched%2520baselines%252C%2520underutilizes%2520depth-recurrence%2520due%2520to%2520partially%2520fixed%2520layer%2520stacks%252C%2520and%2520ignores%2520the%2520bottleneck%2520of%2520constant%2520hidden-sizes%2520that%2520restricts%2520many-step%2520latent%2520reasoning.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520modular%2520framework%2520of%2520depth-recurrent%2520attention%2520mixtures%2520%2528Dreamer%2529%252C%2520combining%2520sequence%2520attention%252C%2520depth%2520attention%252C%2520and%2520sparse%2520expert%2520attention.%2520It%2520alleviates%2520the%2520hidden-size%2520bottleneck%2520through%2520attention%2520along%2520depth%252C%2520decouples%2520scaling%2520dimensions%252C%2520and%2520allows%2520depth-recurrent%2520models%2520to%2520scale%2520efficiently%2520and%2520effectively.%2520Across%2520language%2520reasoning%2520benchmarks%252C%2520our%2520models%2520require%25202%2520to%25208x%2520fewer%2520training%2520tokens%2520for%2520the%2520same%2520accuracy%2520as%2520FLOP-%252C%2520parameter-%252C%2520and%2520memory-matched%2520SOTA%252C%2520and%2520outperform%2520ca.%25202x%2520larger%2520SOTA%2520models%2520with%2520the%2520same%2520training%2520tokens.%2520We%2520further%2520present%2520insights%2520into%2520knowledge%2520usage%2520across%2520depths%252C%2520e.g.%252C%2520showing%25202%2520to%252011x%2520larger%2520expert%2520selection%2520diversity%2520than%2520SOTA%2520MoEs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-Recurrent%20Attention%20Mixtures%3A%20Giving%20Latent%20Reasoning%20the%20Attention%20it%20Deserves&entry.906535625=Jonas%20Knupp%20and%20Jan%20Hendrik%20Metzen%20and%20Jeremias%20Bohn%20and%20Georg%20Groh%20and%20Kristian%20Kersting&entry.1292438233=Depth-recurrence%20facilitates%20latent%20reasoning%20by%20sharing%20parameters%20across%20depths.%20However%2C%20prior%20work%20lacks%20combined%20FLOP-%2C%20parameter-%2C%20and%20memory-matched%20baselines%2C%20underutilizes%20depth-recurrence%20due%20to%20partially%20fixed%20layer%20stacks%2C%20and%20ignores%20the%20bottleneck%20of%20constant%20hidden-sizes%20that%20restricts%20many-step%20latent%20reasoning.%20To%20address%20this%2C%20we%20introduce%20a%20modular%20framework%20of%20depth-recurrent%20attention%20mixtures%20%28Dreamer%29%2C%20combining%20sequence%20attention%2C%20depth%20attention%2C%20and%20sparse%20expert%20attention.%20It%20alleviates%20the%20hidden-size%20bottleneck%20through%20attention%20along%20depth%2C%20decouples%20scaling%20dimensions%2C%20and%20allows%20depth-recurrent%20models%20to%20scale%20efficiently%20and%20effectively.%20Across%20language%20reasoning%20benchmarks%2C%20our%20models%20require%202%20to%208x%20fewer%20training%20tokens%20for%20the%20same%20accuracy%20as%20FLOP-%2C%20parameter-%2C%20and%20memory-matched%20SOTA%2C%20and%20outperform%20ca.%202x%20larger%20SOTA%20models%20with%20the%20same%20training%20tokens.%20We%20further%20present%20insights%20into%20knowledge%20usage%20across%20depths%2C%20e.g.%2C%20showing%202%20to%2011x%20larger%20expert%20selection%20diversity%20than%20SOTA%20MoEs.&entry.1838667208=http%3A//arxiv.org/abs/2601.21582v1&entry.124074799=Read"},
{"title": "CORE: Collaborative Reasoning via Cross Teaching", "author": "Kshitij Mishra and Mirat Aubakirov and Martin Takac and Nils Lukas and Salem Lahlou", "abstract": "Large language models exhibit complementary reasoning errors: on the same instance, one model may succeed with a particular decomposition while another fails. We propose Collaborative Reasoning (CORE), a training-time collaboration framework that converts peer success into a learning signal via a cross-teaching protocol. Each problem is solved in two stages: a cold round of independent sampling, followed by a contexted rescue round in which models that failed receive hint extracted from a successful peer. CORE optimizes a combined reward that balances (i) correctness, (ii) a lightweight DPP-inspired diversity term to reduce error overlap, and (iii) an explicit rescue bonus for successful recovery. We evaluate CORE across four standard reasoning datasets GSM8K, MATH, AIME, and GPQA. With only 1,000 training examples, a pair of small open source models (3B+4B) reaches Pass@2 of 99.54% on GSM8K and 92.08% on MATH, compared to 82.50% and 74.82% for single-model training. On harder datasets, the 3B+4B pair reaches Pass@2 of 77.34% on GPQA (trained on 348 examples) and 79.65% on AIME (trained on 792 examples), using a training-time budget of at most 1536 context tokens and 3072 generated tokens. Overall, these results show that training-time collaboration can reliably convert model complementarity into large gains without scaling model size.", "link": "http://arxiv.org/abs/2601.21600v1", "date": "2026-01-29", "relevancy": 2.5216, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5052}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CORE%3A%20Collaborative%20Reasoning%20via%20Cross%20Teaching&body=Title%3A%20CORE%3A%20Collaborative%20Reasoning%20via%20Cross%20Teaching%0AAuthor%3A%20Kshitij%20Mishra%20and%20Mirat%20Aubakirov%20and%20Martin%20Takac%20and%20Nils%20Lukas%20and%20Salem%20Lahlou%0AAbstract%3A%20Large%20language%20models%20exhibit%20complementary%20reasoning%20errors%3A%20on%20the%20same%20instance%2C%20one%20model%20may%20succeed%20with%20a%20particular%20decomposition%20while%20another%20fails.%20We%20propose%20Collaborative%20Reasoning%20%28CORE%29%2C%20a%20training-time%20collaboration%20framework%20that%20converts%20peer%20success%20into%20a%20learning%20signal%20via%20a%20cross-teaching%20protocol.%20Each%20problem%20is%20solved%20in%20two%20stages%3A%20a%20cold%20round%20of%20independent%20sampling%2C%20followed%20by%20a%20contexted%20rescue%20round%20in%20which%20models%20that%20failed%20receive%20hint%20extracted%20from%20a%20successful%20peer.%20CORE%20optimizes%20a%20combined%20reward%20that%20balances%20%28i%29%20correctness%2C%20%28ii%29%20a%20lightweight%20DPP-inspired%20diversity%20term%20to%20reduce%20error%20overlap%2C%20and%20%28iii%29%20an%20explicit%20rescue%20bonus%20for%20successful%20recovery.%20We%20evaluate%20CORE%20across%20four%20standard%20reasoning%20datasets%20GSM8K%2C%20MATH%2C%20AIME%2C%20and%20GPQA.%20With%20only%201%2C000%20training%20examples%2C%20a%20pair%20of%20small%20open%20source%20models%20%283B%2B4B%29%20reaches%20Pass%402%20of%2099.54%25%20on%20GSM8K%20and%2092.08%25%20on%20MATH%2C%20compared%20to%2082.50%25%20and%2074.82%25%20for%20single-model%20training.%20On%20harder%20datasets%2C%20the%203B%2B4B%20pair%20reaches%20Pass%402%20of%2077.34%25%20on%20GPQA%20%28trained%20on%20348%20examples%29%20and%2079.65%25%20on%20AIME%20%28trained%20on%20792%20examples%29%2C%20using%20a%20training-time%20budget%20of%20at%20most%201536%20context%20tokens%20and%203072%20generated%20tokens.%20Overall%2C%20these%20results%20show%20that%20training-time%20collaboration%20can%20reliably%20convert%20model%20complementarity%20into%20large%20gains%20without%20scaling%20model%20size.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCORE%253A%2520Collaborative%2520Reasoning%2520via%2520Cross%2520Teaching%26entry.906535625%3DKshitij%2520Mishra%2520and%2520Mirat%2520Aubakirov%2520and%2520Martin%2520Takac%2520and%2520Nils%2520Lukas%2520and%2520Salem%2520Lahlou%26entry.1292438233%3DLarge%2520language%2520models%2520exhibit%2520complementary%2520reasoning%2520errors%253A%2520on%2520the%2520same%2520instance%252C%2520one%2520model%2520may%2520succeed%2520with%2520a%2520particular%2520decomposition%2520while%2520another%2520fails.%2520We%2520propose%2520Collaborative%2520Reasoning%2520%2528CORE%2529%252C%2520a%2520training-time%2520collaboration%2520framework%2520that%2520converts%2520peer%2520success%2520into%2520a%2520learning%2520signal%2520via%2520a%2520cross-teaching%2520protocol.%2520Each%2520problem%2520is%2520solved%2520in%2520two%2520stages%253A%2520a%2520cold%2520round%2520of%2520independent%2520sampling%252C%2520followed%2520by%2520a%2520contexted%2520rescue%2520round%2520in%2520which%2520models%2520that%2520failed%2520receive%2520hint%2520extracted%2520from%2520a%2520successful%2520peer.%2520CORE%2520optimizes%2520a%2520combined%2520reward%2520that%2520balances%2520%2528i%2529%2520correctness%252C%2520%2528ii%2529%2520a%2520lightweight%2520DPP-inspired%2520diversity%2520term%2520to%2520reduce%2520error%2520overlap%252C%2520and%2520%2528iii%2529%2520an%2520explicit%2520rescue%2520bonus%2520for%2520successful%2520recovery.%2520We%2520evaluate%2520CORE%2520across%2520four%2520standard%2520reasoning%2520datasets%2520GSM8K%252C%2520MATH%252C%2520AIME%252C%2520and%2520GPQA.%2520With%2520only%25201%252C000%2520training%2520examples%252C%2520a%2520pair%2520of%2520small%2520open%2520source%2520models%2520%25283B%252B4B%2529%2520reaches%2520Pass%25402%2520of%252099.54%2525%2520on%2520GSM8K%2520and%252092.08%2525%2520on%2520MATH%252C%2520compared%2520to%252082.50%2525%2520and%252074.82%2525%2520for%2520single-model%2520training.%2520On%2520harder%2520datasets%252C%2520the%25203B%252B4B%2520pair%2520reaches%2520Pass%25402%2520of%252077.34%2525%2520on%2520GPQA%2520%2528trained%2520on%2520348%2520examples%2529%2520and%252079.65%2525%2520on%2520AIME%2520%2528trained%2520on%2520792%2520examples%2529%252C%2520using%2520a%2520training-time%2520budget%2520of%2520at%2520most%25201536%2520context%2520tokens%2520and%25203072%2520generated%2520tokens.%2520Overall%252C%2520these%2520results%2520show%2520that%2520training-time%2520collaboration%2520can%2520reliably%2520convert%2520model%2520complementarity%2520into%2520large%2520gains%2520without%2520scaling%2520model%2520size.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CORE%3A%20Collaborative%20Reasoning%20via%20Cross%20Teaching&entry.906535625=Kshitij%20Mishra%20and%20Mirat%20Aubakirov%20and%20Martin%20Takac%20and%20Nils%20Lukas%20and%20Salem%20Lahlou&entry.1292438233=Large%20language%20models%20exhibit%20complementary%20reasoning%20errors%3A%20on%20the%20same%20instance%2C%20one%20model%20may%20succeed%20with%20a%20particular%20decomposition%20while%20another%20fails.%20We%20propose%20Collaborative%20Reasoning%20%28CORE%29%2C%20a%20training-time%20collaboration%20framework%20that%20converts%20peer%20success%20into%20a%20learning%20signal%20via%20a%20cross-teaching%20protocol.%20Each%20problem%20is%20solved%20in%20two%20stages%3A%20a%20cold%20round%20of%20independent%20sampling%2C%20followed%20by%20a%20contexted%20rescue%20round%20in%20which%20models%20that%20failed%20receive%20hint%20extracted%20from%20a%20successful%20peer.%20CORE%20optimizes%20a%20combined%20reward%20that%20balances%20%28i%29%20correctness%2C%20%28ii%29%20a%20lightweight%20DPP-inspired%20diversity%20term%20to%20reduce%20error%20overlap%2C%20and%20%28iii%29%20an%20explicit%20rescue%20bonus%20for%20successful%20recovery.%20We%20evaluate%20CORE%20across%20four%20standard%20reasoning%20datasets%20GSM8K%2C%20MATH%2C%20AIME%2C%20and%20GPQA.%20With%20only%201%2C000%20training%20examples%2C%20a%20pair%20of%20small%20open%20source%20models%20%283B%2B4B%29%20reaches%20Pass%402%20of%2099.54%25%20on%20GSM8K%20and%2092.08%25%20on%20MATH%2C%20compared%20to%2082.50%25%20and%2074.82%25%20for%20single-model%20training.%20On%20harder%20datasets%2C%20the%203B%2B4B%20pair%20reaches%20Pass%402%20of%2077.34%25%20on%20GPQA%20%28trained%20on%20348%20examples%29%20and%2079.65%25%20on%20AIME%20%28trained%20on%20792%20examples%29%2C%20using%20a%20training-time%20budget%20of%20at%20most%201536%20context%20tokens%20and%203072%20generated%20tokens.%20Overall%2C%20these%20results%20show%20that%20training-time%20collaboration%20can%20reliably%20convert%20model%20complementarity%20into%20large%20gains%20without%20scaling%20model%20size.&entry.1838667208=http%3A//arxiv.org/abs/2601.21600v1&entry.124074799=Read"},
{"title": "Signal-Adaptive Trust Regions for Gradient-Free Optimization of Recurrent Spiking Neural Networks", "author": "Jinhao Li and Yuhao Sun and Zhiyuan Ma and Hao He and Xinche Zhang and Xing Chen and Jin Li and Sen Song", "abstract": "Recurrent spiking neural networks (RSNNs) are a promising substrate for energy-efficient control policies, but training them for high-dimensional, long-horizon reinforcement learning remains challenging. Population-based, gradient-free optimization circumvents backpropagation through non-differentiable spike dynamics by estimating gradients. However, with finite populations, high variance of these estimates can induce harmful and overly aggressive update steps. Inspired by trust-region methods in reinforcement learning that constrain policy updates in distribution space, we propose \\textbf{Signal-Adaptive Trust Regions (SATR)}, a distributional update rule that constrains relative change by bounding KL divergence normalized by an estimated signal energy. SATR automatically expands the trust region under strong signals and contracts it when updates are noise-dominated. We instantiate SATR for Bernoulli connectivity distributions, which have shown strong empirical performance for RSNN optimization. Across a suite of high-dimensional continuous-control benchmarks, SATR improves stability under limited populations and reaches competitive returns against strong baselines including PPO-LSTM. In addition, to make SATR practical at scale, we introduce a bitset implementation for binary spiking and binary weights, substantially reducing wall-clock training time and enabling fast RSNN policy search.", "link": "http://arxiv.org/abs/2601.21572v1", "date": "2026-01-29", "relevancy": 2.5107, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5113}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5024}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Signal-Adaptive%20Trust%20Regions%20for%20Gradient-Free%20Optimization%20of%20Recurrent%20Spiking%20Neural%20Networks&body=Title%3A%20Signal-Adaptive%20Trust%20Regions%20for%20Gradient-Free%20Optimization%20of%20Recurrent%20Spiking%20Neural%20Networks%0AAuthor%3A%20Jinhao%20Li%20and%20Yuhao%20Sun%20and%20Zhiyuan%20Ma%20and%20Hao%20He%20and%20Xinche%20Zhang%20and%20Xing%20Chen%20and%20Jin%20Li%20and%20Sen%20Song%0AAbstract%3A%20Recurrent%20spiking%20neural%20networks%20%28RSNNs%29%20are%20a%20promising%20substrate%20for%20energy-efficient%20control%20policies%2C%20but%20training%20them%20for%20high-dimensional%2C%20long-horizon%20reinforcement%20learning%20remains%20challenging.%20Population-based%2C%20gradient-free%20optimization%20circumvents%20backpropagation%20through%20non-differentiable%20spike%20dynamics%20by%20estimating%20gradients.%20However%2C%20with%20finite%20populations%2C%20high%20variance%20of%20these%20estimates%20can%20induce%20harmful%20and%20overly%20aggressive%20update%20steps.%20Inspired%20by%20trust-region%20methods%20in%20reinforcement%20learning%20that%20constrain%20policy%20updates%20in%20distribution%20space%2C%20we%20propose%20%5Ctextbf%7BSignal-Adaptive%20Trust%20Regions%20%28SATR%29%7D%2C%20a%20distributional%20update%20rule%20that%20constrains%20relative%20change%20by%20bounding%20KL%20divergence%20normalized%20by%20an%20estimated%20signal%20energy.%20SATR%20automatically%20expands%20the%20trust%20region%20under%20strong%20signals%20and%20contracts%20it%20when%20updates%20are%20noise-dominated.%20We%20instantiate%20SATR%20for%20Bernoulli%20connectivity%20distributions%2C%20which%20have%20shown%20strong%20empirical%20performance%20for%20RSNN%20optimization.%20Across%20a%20suite%20of%20high-dimensional%20continuous-control%20benchmarks%2C%20SATR%20improves%20stability%20under%20limited%20populations%20and%20reaches%20competitive%20returns%20against%20strong%20baselines%20including%20PPO-LSTM.%20In%20addition%2C%20to%20make%20SATR%20practical%20at%20scale%2C%20we%20introduce%20a%20bitset%20implementation%20for%20binary%20spiking%20and%20binary%20weights%2C%20substantially%20reducing%20wall-clock%20training%20time%20and%20enabling%20fast%20RSNN%20policy%20search.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21572v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignal-Adaptive%2520Trust%2520Regions%2520for%2520Gradient-Free%2520Optimization%2520of%2520Recurrent%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DJinhao%2520Li%2520and%2520Yuhao%2520Sun%2520and%2520Zhiyuan%2520Ma%2520and%2520Hao%2520He%2520and%2520Xinche%2520Zhang%2520and%2520Xing%2520Chen%2520and%2520Jin%2520Li%2520and%2520Sen%2520Song%26entry.1292438233%3DRecurrent%2520spiking%2520neural%2520networks%2520%2528RSNNs%2529%2520are%2520a%2520promising%2520substrate%2520for%2520energy-efficient%2520control%2520policies%252C%2520but%2520training%2520them%2520for%2520high-dimensional%252C%2520long-horizon%2520reinforcement%2520learning%2520remains%2520challenging.%2520Population-based%252C%2520gradient-free%2520optimization%2520circumvents%2520backpropagation%2520through%2520non-differentiable%2520spike%2520dynamics%2520by%2520estimating%2520gradients.%2520However%252C%2520with%2520finite%2520populations%252C%2520high%2520variance%2520of%2520these%2520estimates%2520can%2520induce%2520harmful%2520and%2520overly%2520aggressive%2520update%2520steps.%2520Inspired%2520by%2520trust-region%2520methods%2520in%2520reinforcement%2520learning%2520that%2520constrain%2520policy%2520updates%2520in%2520distribution%2520space%252C%2520we%2520propose%2520%255Ctextbf%257BSignal-Adaptive%2520Trust%2520Regions%2520%2528SATR%2529%257D%252C%2520a%2520distributional%2520update%2520rule%2520that%2520constrains%2520relative%2520change%2520by%2520bounding%2520KL%2520divergence%2520normalized%2520by%2520an%2520estimated%2520signal%2520energy.%2520SATR%2520automatically%2520expands%2520the%2520trust%2520region%2520under%2520strong%2520signals%2520and%2520contracts%2520it%2520when%2520updates%2520are%2520noise-dominated.%2520We%2520instantiate%2520SATR%2520for%2520Bernoulli%2520connectivity%2520distributions%252C%2520which%2520have%2520shown%2520strong%2520empirical%2520performance%2520for%2520RSNN%2520optimization.%2520Across%2520a%2520suite%2520of%2520high-dimensional%2520continuous-control%2520benchmarks%252C%2520SATR%2520improves%2520stability%2520under%2520limited%2520populations%2520and%2520reaches%2520competitive%2520returns%2520against%2520strong%2520baselines%2520including%2520PPO-LSTM.%2520In%2520addition%252C%2520to%2520make%2520SATR%2520practical%2520at%2520scale%252C%2520we%2520introduce%2520a%2520bitset%2520implementation%2520for%2520binary%2520spiking%2520and%2520binary%2520weights%252C%2520substantially%2520reducing%2520wall-clock%2520training%2520time%2520and%2520enabling%2520fast%2520RSNN%2520policy%2520search.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21572v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Signal-Adaptive%20Trust%20Regions%20for%20Gradient-Free%20Optimization%20of%20Recurrent%20Spiking%20Neural%20Networks&entry.906535625=Jinhao%20Li%20and%20Yuhao%20Sun%20and%20Zhiyuan%20Ma%20and%20Hao%20He%20and%20Xinche%20Zhang%20and%20Xing%20Chen%20and%20Jin%20Li%20and%20Sen%20Song&entry.1292438233=Recurrent%20spiking%20neural%20networks%20%28RSNNs%29%20are%20a%20promising%20substrate%20for%20energy-efficient%20control%20policies%2C%20but%20training%20them%20for%20high-dimensional%2C%20long-horizon%20reinforcement%20learning%20remains%20challenging.%20Population-based%2C%20gradient-free%20optimization%20circumvents%20backpropagation%20through%20non-differentiable%20spike%20dynamics%20by%20estimating%20gradients.%20However%2C%20with%20finite%20populations%2C%20high%20variance%20of%20these%20estimates%20can%20induce%20harmful%20and%20overly%20aggressive%20update%20steps.%20Inspired%20by%20trust-region%20methods%20in%20reinforcement%20learning%20that%20constrain%20policy%20updates%20in%20distribution%20space%2C%20we%20propose%20%5Ctextbf%7BSignal-Adaptive%20Trust%20Regions%20%28SATR%29%7D%2C%20a%20distributional%20update%20rule%20that%20constrains%20relative%20change%20by%20bounding%20KL%20divergence%20normalized%20by%20an%20estimated%20signal%20energy.%20SATR%20automatically%20expands%20the%20trust%20region%20under%20strong%20signals%20and%20contracts%20it%20when%20updates%20are%20noise-dominated.%20We%20instantiate%20SATR%20for%20Bernoulli%20connectivity%20distributions%2C%20which%20have%20shown%20strong%20empirical%20performance%20for%20RSNN%20optimization.%20Across%20a%20suite%20of%20high-dimensional%20continuous-control%20benchmarks%2C%20SATR%20improves%20stability%20under%20limited%20populations%20and%20reaches%20competitive%20returns%20against%20strong%20baselines%20including%20PPO-LSTM.%20In%20addition%2C%20to%20make%20SATR%20practical%20at%20scale%2C%20we%20introduce%20a%20bitset%20implementation%20for%20binary%20spiking%20and%20binary%20weights%2C%20substantially%20reducing%20wall-clock%20training%20time%20and%20enabling%20fast%20RSNN%20policy%20search.&entry.1838667208=http%3A//arxiv.org/abs/2601.21572v1&entry.124074799=Read"},
{"title": "Pushing the Limits of Distillation-Based Class-Incremental Learning via Lightweight Plugins", "author": "Zhiming Xu and Baile Xu and Jian Zhao and Furao Shen and Suorong Yang", "abstract": "Existing replay and distillation-based class-incremental learning (CIL) methods are effective at retaining past knowledge but are still constrained by the stability-plasticity dilemma. Since their resulting models are learned over a sequence of incremental tasks, they encode rich representations and can be regarded as pre-trained bases. Building on this view, we propose a plug-in extension paradigm termed Deployment of LoRA Components (DLC) to enhance them. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model's deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable content in software, DLC serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4\\% of the parameters of a standard ResNet-18, our DLC model achieves a significant 8\\% improvement in accuracy, demonstrating exceptional efficiency. Under a fixed memory budget, methods equipped with DLC surpass state-of-the-art expansion-based methods.", "link": "http://arxiv.org/abs/2512.03537v2", "date": "2026-01-29", "relevancy": 2.5094, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5102}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5071}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pushing%20the%20Limits%20of%20Distillation-Based%20Class-Incremental%20Learning%20via%20Lightweight%20Plugins&body=Title%3A%20Pushing%20the%20Limits%20of%20Distillation-Based%20Class-Incremental%20Learning%20via%20Lightweight%20Plugins%0AAuthor%3A%20Zhiming%20Xu%20and%20Baile%20Xu%20and%20Jian%20Zhao%20and%20Furao%20Shen%20and%20Suorong%20Yang%0AAbstract%3A%20Existing%20replay%20and%20distillation-based%20class-incremental%20learning%20%28CIL%29%20methods%20are%20effective%20at%20retaining%20past%20knowledge%20but%20are%20still%20constrained%20by%20the%20stability-plasticity%20dilemma.%20Since%20their%20resulting%20models%20are%20learned%20over%20a%20sequence%20of%20incremental%20tasks%2C%20they%20encode%20rich%20representations%20and%20can%20be%20regarded%20as%20pre-trained%20bases.%20Building%20on%20this%20view%2C%20we%20propose%20a%20plug-in%20extension%20paradigm%20termed%20Deployment%20of%20LoRA%20Components%20%28DLC%29%20to%20enhance%20them.%20For%20each%20task%2C%20we%20use%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20inject%20task-specific%20residuals%20into%20the%20base%20model%27s%20deep%20layers.%20During%20inference%2C%20representations%20with%20task-specific%20residuals%20are%20aggregated%20to%20produce%20classification%20predictions.%20To%20mitigate%20interference%20from%20non-target%20LoRA%20plugins%2C%20we%20introduce%20a%20lightweight%20weighting%20unit.%20This%20unit%20learns%20to%20assign%20importance%20scores%20to%20different%20LoRA-tuned%20representations.%20Like%20downloadable%20content%20in%20software%2C%20DLC%20serves%20as%20a%20plug-and-play%20enhancement%20that%20efficiently%20extends%20the%20base%20methods.%20Remarkably%2C%20on%20the%20large-scale%20ImageNet-100%2C%20with%20merely%204%5C%25%20of%20the%20parameters%20of%20a%20standard%20ResNet-18%2C%20our%20DLC%20model%20achieves%20a%20significant%208%5C%25%20improvement%20in%20accuracy%2C%20demonstrating%20exceptional%20efficiency.%20Under%20a%20fixed%20memory%20budget%2C%20methods%20equipped%20with%20DLC%20surpass%20state-of-the-art%20expansion-based%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03537v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPushing%2520the%2520Limits%2520of%2520Distillation-Based%2520Class-Incremental%2520Learning%2520via%2520Lightweight%2520Plugins%26entry.906535625%3DZhiming%2520Xu%2520and%2520Baile%2520Xu%2520and%2520Jian%2520Zhao%2520and%2520Furao%2520Shen%2520and%2520Suorong%2520Yang%26entry.1292438233%3DExisting%2520replay%2520and%2520distillation-based%2520class-incremental%2520learning%2520%2528CIL%2529%2520methods%2520are%2520effective%2520at%2520retaining%2520past%2520knowledge%2520but%2520are%2520still%2520constrained%2520by%2520the%2520stability-plasticity%2520dilemma.%2520Since%2520their%2520resulting%2520models%2520are%2520learned%2520over%2520a%2520sequence%2520of%2520incremental%2520tasks%252C%2520they%2520encode%2520rich%2520representations%2520and%2520can%2520be%2520regarded%2520as%2520pre-trained%2520bases.%2520Building%2520on%2520this%2520view%252C%2520we%2520propose%2520a%2520plug-in%2520extension%2520paradigm%2520termed%2520Deployment%2520of%2520LoRA%2520Components%2520%2528DLC%2529%2520to%2520enhance%2520them.%2520For%2520each%2520task%252C%2520we%2520use%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520to%2520inject%2520task-specific%2520residuals%2520into%2520the%2520base%2520model%2527s%2520deep%2520layers.%2520During%2520inference%252C%2520representations%2520with%2520task-specific%2520residuals%2520are%2520aggregated%2520to%2520produce%2520classification%2520predictions.%2520To%2520mitigate%2520interference%2520from%2520non-target%2520LoRA%2520plugins%252C%2520we%2520introduce%2520a%2520lightweight%2520weighting%2520unit.%2520This%2520unit%2520learns%2520to%2520assign%2520importance%2520scores%2520to%2520different%2520LoRA-tuned%2520representations.%2520Like%2520downloadable%2520content%2520in%2520software%252C%2520DLC%2520serves%2520as%2520a%2520plug-and-play%2520enhancement%2520that%2520efficiently%2520extends%2520the%2520base%2520methods.%2520Remarkably%252C%2520on%2520the%2520large-scale%2520ImageNet-100%252C%2520with%2520merely%25204%255C%2525%2520of%2520the%2520parameters%2520of%2520a%2520standard%2520ResNet-18%252C%2520our%2520DLC%2520model%2520achieves%2520a%2520significant%25208%255C%2525%2520improvement%2520in%2520accuracy%252C%2520demonstrating%2520exceptional%2520efficiency.%2520Under%2520a%2520fixed%2520memory%2520budget%252C%2520methods%2520equipped%2520with%2520DLC%2520surpass%2520state-of-the-art%2520expansion-based%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03537v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pushing%20the%20Limits%20of%20Distillation-Based%20Class-Incremental%20Learning%20via%20Lightweight%20Plugins&entry.906535625=Zhiming%20Xu%20and%20Baile%20Xu%20and%20Jian%20Zhao%20and%20Furao%20Shen%20and%20Suorong%20Yang&entry.1292438233=Existing%20replay%20and%20distillation-based%20class-incremental%20learning%20%28CIL%29%20methods%20are%20effective%20at%20retaining%20past%20knowledge%20but%20are%20still%20constrained%20by%20the%20stability-plasticity%20dilemma.%20Since%20their%20resulting%20models%20are%20learned%20over%20a%20sequence%20of%20incremental%20tasks%2C%20they%20encode%20rich%20representations%20and%20can%20be%20regarded%20as%20pre-trained%20bases.%20Building%20on%20this%20view%2C%20we%20propose%20a%20plug-in%20extension%20paradigm%20termed%20Deployment%20of%20LoRA%20Components%20%28DLC%29%20to%20enhance%20them.%20For%20each%20task%2C%20we%20use%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20inject%20task-specific%20residuals%20into%20the%20base%20model%27s%20deep%20layers.%20During%20inference%2C%20representations%20with%20task-specific%20residuals%20are%20aggregated%20to%20produce%20classification%20predictions.%20To%20mitigate%20interference%20from%20non-target%20LoRA%20plugins%2C%20we%20introduce%20a%20lightweight%20weighting%20unit.%20This%20unit%20learns%20to%20assign%20importance%20scores%20to%20different%20LoRA-tuned%20representations.%20Like%20downloadable%20content%20in%20software%2C%20DLC%20serves%20as%20a%20plug-and-play%20enhancement%20that%20efficiently%20extends%20the%20base%20methods.%20Remarkably%2C%20on%20the%20large-scale%20ImageNet-100%2C%20with%20merely%204%5C%25%20of%20the%20parameters%20of%20a%20standard%20ResNet-18%2C%20our%20DLC%20model%20achieves%20a%20significant%208%5C%25%20improvement%20in%20accuracy%2C%20demonstrating%20exceptional%20efficiency.%20Under%20a%20fixed%20memory%20budget%2C%20methods%20equipped%20with%20DLC%20surpass%20state-of-the-art%20expansion-based%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.03537v2&entry.124074799=Read"},
{"title": "Mean-Field Control on Sparse Graphs: From Local Limits to GNNs via Neighborhood Distributions", "author": "Tobias Schmidt and Kai Cui", "abstract": "Mean-field control (MFC) offers a scalable solution to the curse of dimensionality in multi-agent systems but traditionally hinges on the restrictive assumption of exchangeability via dense, all-to-all interactions. In this work, we bridge the gap to real-world network structures by proposing a rigorous framework for MFC on large sparse graphs. We redefine the system state as a probability measure over decorated rooted neighborhoods, effectively capturing local heterogeneity. Our central contribution is a theoretical foundation for scalable reinforcement learning in this setting. We prove horizon-dependent locality: for finite-horizon problems, an agent's optimal policy at time t depends strictly on its (T-t)-hop neighborhood. This result renders the infinite-dimensional control problem tractable and underpins a novel Dynamic Programming Principle (DPP) on the lifted space of neighborhood distributions. Furthermore, we formally and experimentally justify the use of Graph Neural Networks (GNNs) for actor-critic algorithms in this context. Our framework naturally recovers classical MFC as a degenerate case while enabling efficient, theoretically grounded control on complex sparse topologies.", "link": "http://arxiv.org/abs/2601.21477v1", "date": "2026-01-29", "relevancy": 2.5077, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5078}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4993}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mean-Field%20Control%20on%20Sparse%20Graphs%3A%20From%20Local%20Limits%20to%20GNNs%20via%20Neighborhood%20Distributions&body=Title%3A%20Mean-Field%20Control%20on%20Sparse%20Graphs%3A%20From%20Local%20Limits%20to%20GNNs%20via%20Neighborhood%20Distributions%0AAuthor%3A%20Tobias%20Schmidt%20and%20Kai%20Cui%0AAbstract%3A%20Mean-field%20control%20%28MFC%29%20offers%20a%20scalable%20solution%20to%20the%20curse%20of%20dimensionality%20in%20multi-agent%20systems%20but%20traditionally%20hinges%20on%20the%20restrictive%20assumption%20of%20exchangeability%20via%20dense%2C%20all-to-all%20interactions.%20In%20this%20work%2C%20we%20bridge%20the%20gap%20to%20real-world%20network%20structures%20by%20proposing%20a%20rigorous%20framework%20for%20MFC%20on%20large%20sparse%20graphs.%20We%20redefine%20the%20system%20state%20as%20a%20probability%20measure%20over%20decorated%20rooted%20neighborhoods%2C%20effectively%20capturing%20local%20heterogeneity.%20Our%20central%20contribution%20is%20a%20theoretical%20foundation%20for%20scalable%20reinforcement%20learning%20in%20this%20setting.%20We%20prove%20horizon-dependent%20locality%3A%20for%20finite-horizon%20problems%2C%20an%20agent%27s%20optimal%20policy%20at%20time%20t%20depends%20strictly%20on%20its%20%28T-t%29-hop%20neighborhood.%20This%20result%20renders%20the%20infinite-dimensional%20control%20problem%20tractable%20and%20underpins%20a%20novel%20Dynamic%20Programming%20Principle%20%28DPP%29%20on%20the%20lifted%20space%20of%20neighborhood%20distributions.%20Furthermore%2C%20we%20formally%20and%20experimentally%20justify%20the%20use%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20for%20actor-critic%20algorithms%20in%20this%20context.%20Our%20framework%20naturally%20recovers%20classical%20MFC%20as%20a%20degenerate%20case%20while%20enabling%20efficient%2C%20theoretically%20grounded%20control%20on%20complex%20sparse%20topologies.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMean-Field%2520Control%2520on%2520Sparse%2520Graphs%253A%2520From%2520Local%2520Limits%2520to%2520GNNs%2520via%2520Neighborhood%2520Distributions%26entry.906535625%3DTobias%2520Schmidt%2520and%2520Kai%2520Cui%26entry.1292438233%3DMean-field%2520control%2520%2528MFC%2529%2520offers%2520a%2520scalable%2520solution%2520to%2520the%2520curse%2520of%2520dimensionality%2520in%2520multi-agent%2520systems%2520but%2520traditionally%2520hinges%2520on%2520the%2520restrictive%2520assumption%2520of%2520exchangeability%2520via%2520dense%252C%2520all-to-all%2520interactions.%2520In%2520this%2520work%252C%2520we%2520bridge%2520the%2520gap%2520to%2520real-world%2520network%2520structures%2520by%2520proposing%2520a%2520rigorous%2520framework%2520for%2520MFC%2520on%2520large%2520sparse%2520graphs.%2520We%2520redefine%2520the%2520system%2520state%2520as%2520a%2520probability%2520measure%2520over%2520decorated%2520rooted%2520neighborhoods%252C%2520effectively%2520capturing%2520local%2520heterogeneity.%2520Our%2520central%2520contribution%2520is%2520a%2520theoretical%2520foundation%2520for%2520scalable%2520reinforcement%2520learning%2520in%2520this%2520setting.%2520We%2520prove%2520horizon-dependent%2520locality%253A%2520for%2520finite-horizon%2520problems%252C%2520an%2520agent%2527s%2520optimal%2520policy%2520at%2520time%2520t%2520depends%2520strictly%2520on%2520its%2520%2528T-t%2529-hop%2520neighborhood.%2520This%2520result%2520renders%2520the%2520infinite-dimensional%2520control%2520problem%2520tractable%2520and%2520underpins%2520a%2520novel%2520Dynamic%2520Programming%2520Principle%2520%2528DPP%2529%2520on%2520the%2520lifted%2520space%2520of%2520neighborhood%2520distributions.%2520Furthermore%252C%2520we%2520formally%2520and%2520experimentally%2520justify%2520the%2520use%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520for%2520actor-critic%2520algorithms%2520in%2520this%2520context.%2520Our%2520framework%2520naturally%2520recovers%2520classical%2520MFC%2520as%2520a%2520degenerate%2520case%2520while%2520enabling%2520efficient%252C%2520theoretically%2520grounded%2520control%2520on%2520complex%2520sparse%2520topologies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mean-Field%20Control%20on%20Sparse%20Graphs%3A%20From%20Local%20Limits%20to%20GNNs%20via%20Neighborhood%20Distributions&entry.906535625=Tobias%20Schmidt%20and%20Kai%20Cui&entry.1292438233=Mean-field%20control%20%28MFC%29%20offers%20a%20scalable%20solution%20to%20the%20curse%20of%20dimensionality%20in%20multi-agent%20systems%20but%20traditionally%20hinges%20on%20the%20restrictive%20assumption%20of%20exchangeability%20via%20dense%2C%20all-to-all%20interactions.%20In%20this%20work%2C%20we%20bridge%20the%20gap%20to%20real-world%20network%20structures%20by%20proposing%20a%20rigorous%20framework%20for%20MFC%20on%20large%20sparse%20graphs.%20We%20redefine%20the%20system%20state%20as%20a%20probability%20measure%20over%20decorated%20rooted%20neighborhoods%2C%20effectively%20capturing%20local%20heterogeneity.%20Our%20central%20contribution%20is%20a%20theoretical%20foundation%20for%20scalable%20reinforcement%20learning%20in%20this%20setting.%20We%20prove%20horizon-dependent%20locality%3A%20for%20finite-horizon%20problems%2C%20an%20agent%27s%20optimal%20policy%20at%20time%20t%20depends%20strictly%20on%20its%20%28T-t%29-hop%20neighborhood.%20This%20result%20renders%20the%20infinite-dimensional%20control%20problem%20tractable%20and%20underpins%20a%20novel%20Dynamic%20Programming%20Principle%20%28DPP%29%20on%20the%20lifted%20space%20of%20neighborhood%20distributions.%20Furthermore%2C%20we%20formally%20and%20experimentally%20justify%20the%20use%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20for%20actor-critic%20algorithms%20in%20this%20context.%20Our%20framework%20naturally%20recovers%20classical%20MFC%20as%20a%20degenerate%20case%20while%20enabling%20efficient%2C%20theoretically%20grounded%20control%20on%20complex%20sparse%20topologies.&entry.1838667208=http%3A//arxiv.org/abs/2601.21477v1&entry.124074799=Read"},
{"title": "XFACTORS: Disentangled Information Bottleneck via Contrastive Supervision", "author": "Alexandre Myara and Nicolas Bourriez and Thomas Boyer and Thomas Lemercier and Ihab Bendidi and Auguste Genovesio", "abstract": "Disentangled representation learning aims to map independent factors of variation to independent representation components. On one hand, purely unsupervised approaches have proven successful on fully disentangled synthetic data, but fail to recover semantic factors from real data without strong inductive biases. On the other hand, supervised approaches are unstable and hard to scale to large attribute sets because they rely on adversarial objectives or auxiliary classifiers.\n  We introduce \\textsc{XFactors}, a weakly-supervised VAE framework that disentangles and provides explicit control over a chosen set of factors. Building on the Disentangled Information Bottleneck perspective, we decompose the representation into a residual subspace $\\mathcal{S}$ and factor-specific subspaces $\\mathcal{T}_1,\\ldots,\\mathcal{T}_K$ and a residual subspace $\\mathcal{S}$. Each target factor is encoded in its assigned $\\mathcal{T}_i$ through contrastive supervision: an InfoNCE loss pulls together latents sharing the same factor value and pushes apart mismatched pairs. In parallel, KL regularization imposes a Gaussian structure on both $\\mathcal{S}$ and the aggregated factor subspaces, organizing the geometry without additional supervision for non-targeted factors and avoiding adversarial training and classifiers.\n  Across multiple datasets, with constant hyperparameters, \\textsc{XFactors} achieves state-of-the-art disentanglement scores and yields consistent qualitative factor alignment in the corresponding subspaces, enabling controlled factor swapping via latent replacement. We further demonstrate that our method scales correctly with increasing latent capacity and evaluate it on the real-world dataset CelebA. Our code is available at \\href{https://github.com/ICML26-anon/XFactors}{github.com/ICML26-anon/XFactors}.", "link": "http://arxiv.org/abs/2601.21688v1", "date": "2026-01-29", "relevancy": 2.4916, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5012}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5012}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XFACTORS%3A%20Disentangled%20Information%20Bottleneck%20via%20Contrastive%20Supervision&body=Title%3A%20XFACTORS%3A%20Disentangled%20Information%20Bottleneck%20via%20Contrastive%20Supervision%0AAuthor%3A%20Alexandre%20Myara%20and%20Nicolas%20Bourriez%20and%20Thomas%20Boyer%20and%20Thomas%20Lemercier%20and%20Ihab%20Bendidi%20and%20Auguste%20Genovesio%0AAbstract%3A%20Disentangled%20representation%20learning%20aims%20to%20map%20independent%20factors%20of%20variation%20to%20independent%20representation%20components.%20On%20one%20hand%2C%20purely%20unsupervised%20approaches%20have%20proven%20successful%20on%20fully%20disentangled%20synthetic%20data%2C%20but%20fail%20to%20recover%20semantic%20factors%20from%20real%20data%20without%20strong%20inductive%20biases.%20On%20the%20other%20hand%2C%20supervised%20approaches%20are%20unstable%20and%20hard%20to%20scale%20to%20large%20attribute%20sets%20because%20they%20rely%20on%20adversarial%20objectives%20or%20auxiliary%20classifiers.%0A%20%20We%20introduce%20%5Ctextsc%7BXFactors%7D%2C%20a%20weakly-supervised%20VAE%20framework%20that%20disentangles%20and%20provides%20explicit%20control%20over%20a%20chosen%20set%20of%20factors.%20Building%20on%20the%20Disentangled%20Information%20Bottleneck%20perspective%2C%20we%20decompose%20the%20representation%20into%20a%20residual%20subspace%20%24%5Cmathcal%7BS%7D%24%20and%20factor-specific%20subspaces%20%24%5Cmathcal%7BT%7D_1%2C%5Cldots%2C%5Cmathcal%7BT%7D_K%24%20and%20a%20residual%20subspace%20%24%5Cmathcal%7BS%7D%24.%20Each%20target%20factor%20is%20encoded%20in%20its%20assigned%20%24%5Cmathcal%7BT%7D_i%24%20through%20contrastive%20supervision%3A%20an%20InfoNCE%20loss%20pulls%20together%20latents%20sharing%20the%20same%20factor%20value%20and%20pushes%20apart%20mismatched%20pairs.%20In%20parallel%2C%20KL%20regularization%20imposes%20a%20Gaussian%20structure%20on%20both%20%24%5Cmathcal%7BS%7D%24%20and%20the%20aggregated%20factor%20subspaces%2C%20organizing%20the%20geometry%20without%20additional%20supervision%20for%20non-targeted%20factors%20and%20avoiding%20adversarial%20training%20and%20classifiers.%0A%20%20Across%20multiple%20datasets%2C%20with%20constant%20hyperparameters%2C%20%5Ctextsc%7BXFactors%7D%20achieves%20state-of-the-art%20disentanglement%20scores%20and%20yields%20consistent%20qualitative%20factor%20alignment%20in%20the%20corresponding%20subspaces%2C%20enabling%20controlled%20factor%20swapping%20via%20latent%20replacement.%20We%20further%20demonstrate%20that%20our%20method%20scales%20correctly%20with%20increasing%20latent%20capacity%20and%20evaluate%20it%20on%20the%20real-world%20dataset%20CelebA.%20Our%20code%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/ICML26-anon/XFactors%7D%7Bgithub.com/ICML26-anon/XFactors%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXFACTORS%253A%2520Disentangled%2520Information%2520Bottleneck%2520via%2520Contrastive%2520Supervision%26entry.906535625%3DAlexandre%2520Myara%2520and%2520Nicolas%2520Bourriez%2520and%2520Thomas%2520Boyer%2520and%2520Thomas%2520Lemercier%2520and%2520Ihab%2520Bendidi%2520and%2520Auguste%2520Genovesio%26entry.1292438233%3DDisentangled%2520representation%2520learning%2520aims%2520to%2520map%2520independent%2520factors%2520of%2520variation%2520to%2520independent%2520representation%2520components.%2520On%2520one%2520hand%252C%2520purely%2520unsupervised%2520approaches%2520have%2520proven%2520successful%2520on%2520fully%2520disentangled%2520synthetic%2520data%252C%2520but%2520fail%2520to%2520recover%2520semantic%2520factors%2520from%2520real%2520data%2520without%2520strong%2520inductive%2520biases.%2520On%2520the%2520other%2520hand%252C%2520supervised%2520approaches%2520are%2520unstable%2520and%2520hard%2520to%2520scale%2520to%2520large%2520attribute%2520sets%2520because%2520they%2520rely%2520on%2520adversarial%2520objectives%2520or%2520auxiliary%2520classifiers.%250A%2520%2520We%2520introduce%2520%255Ctextsc%257BXFactors%257D%252C%2520a%2520weakly-supervised%2520VAE%2520framework%2520that%2520disentangles%2520and%2520provides%2520explicit%2520control%2520over%2520a%2520chosen%2520set%2520of%2520factors.%2520Building%2520on%2520the%2520Disentangled%2520Information%2520Bottleneck%2520perspective%252C%2520we%2520decompose%2520the%2520representation%2520into%2520a%2520residual%2520subspace%2520%2524%255Cmathcal%257BS%257D%2524%2520and%2520factor-specific%2520subspaces%2520%2524%255Cmathcal%257BT%257D_1%252C%255Cldots%252C%255Cmathcal%257BT%257D_K%2524%2520and%2520a%2520residual%2520subspace%2520%2524%255Cmathcal%257BS%257D%2524.%2520Each%2520target%2520factor%2520is%2520encoded%2520in%2520its%2520assigned%2520%2524%255Cmathcal%257BT%257D_i%2524%2520through%2520contrastive%2520supervision%253A%2520an%2520InfoNCE%2520loss%2520pulls%2520together%2520latents%2520sharing%2520the%2520same%2520factor%2520value%2520and%2520pushes%2520apart%2520mismatched%2520pairs.%2520In%2520parallel%252C%2520KL%2520regularization%2520imposes%2520a%2520Gaussian%2520structure%2520on%2520both%2520%2524%255Cmathcal%257BS%257D%2524%2520and%2520the%2520aggregated%2520factor%2520subspaces%252C%2520organizing%2520the%2520geometry%2520without%2520additional%2520supervision%2520for%2520non-targeted%2520factors%2520and%2520avoiding%2520adversarial%2520training%2520and%2520classifiers.%250A%2520%2520Across%2520multiple%2520datasets%252C%2520with%2520constant%2520hyperparameters%252C%2520%255Ctextsc%257BXFactors%257D%2520achieves%2520state-of-the-art%2520disentanglement%2520scores%2520and%2520yields%2520consistent%2520qualitative%2520factor%2520alignment%2520in%2520the%2520corresponding%2520subspaces%252C%2520enabling%2520controlled%2520factor%2520swapping%2520via%2520latent%2520replacement.%2520We%2520further%2520demonstrate%2520that%2520our%2520method%2520scales%2520correctly%2520with%2520increasing%2520latent%2520capacity%2520and%2520evaluate%2520it%2520on%2520the%2520real-world%2520dataset%2520CelebA.%2520Our%2520code%2520is%2520available%2520at%2520%255Chref%257Bhttps%253A//github.com/ICML26-anon/XFactors%257D%257Bgithub.com/ICML26-anon/XFactors%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XFACTORS%3A%20Disentangled%20Information%20Bottleneck%20via%20Contrastive%20Supervision&entry.906535625=Alexandre%20Myara%20and%20Nicolas%20Bourriez%20and%20Thomas%20Boyer%20and%20Thomas%20Lemercier%20and%20Ihab%20Bendidi%20and%20Auguste%20Genovesio&entry.1292438233=Disentangled%20representation%20learning%20aims%20to%20map%20independent%20factors%20of%20variation%20to%20independent%20representation%20components.%20On%20one%20hand%2C%20purely%20unsupervised%20approaches%20have%20proven%20successful%20on%20fully%20disentangled%20synthetic%20data%2C%20but%20fail%20to%20recover%20semantic%20factors%20from%20real%20data%20without%20strong%20inductive%20biases.%20On%20the%20other%20hand%2C%20supervised%20approaches%20are%20unstable%20and%20hard%20to%20scale%20to%20large%20attribute%20sets%20because%20they%20rely%20on%20adversarial%20objectives%20or%20auxiliary%20classifiers.%0A%20%20We%20introduce%20%5Ctextsc%7BXFactors%7D%2C%20a%20weakly-supervised%20VAE%20framework%20that%20disentangles%20and%20provides%20explicit%20control%20over%20a%20chosen%20set%20of%20factors.%20Building%20on%20the%20Disentangled%20Information%20Bottleneck%20perspective%2C%20we%20decompose%20the%20representation%20into%20a%20residual%20subspace%20%24%5Cmathcal%7BS%7D%24%20and%20factor-specific%20subspaces%20%24%5Cmathcal%7BT%7D_1%2C%5Cldots%2C%5Cmathcal%7BT%7D_K%24%20and%20a%20residual%20subspace%20%24%5Cmathcal%7BS%7D%24.%20Each%20target%20factor%20is%20encoded%20in%20its%20assigned%20%24%5Cmathcal%7BT%7D_i%24%20through%20contrastive%20supervision%3A%20an%20InfoNCE%20loss%20pulls%20together%20latents%20sharing%20the%20same%20factor%20value%20and%20pushes%20apart%20mismatched%20pairs.%20In%20parallel%2C%20KL%20regularization%20imposes%20a%20Gaussian%20structure%20on%20both%20%24%5Cmathcal%7BS%7D%24%20and%20the%20aggregated%20factor%20subspaces%2C%20organizing%20the%20geometry%20without%20additional%20supervision%20for%20non-targeted%20factors%20and%20avoiding%20adversarial%20training%20and%20classifiers.%0A%20%20Across%20multiple%20datasets%2C%20with%20constant%20hyperparameters%2C%20%5Ctextsc%7BXFactors%7D%20achieves%20state-of-the-art%20disentanglement%20scores%20and%20yields%20consistent%20qualitative%20factor%20alignment%20in%20the%20corresponding%20subspaces%2C%20enabling%20controlled%20factor%20swapping%20via%20latent%20replacement.%20We%20further%20demonstrate%20that%20our%20method%20scales%20correctly%20with%20increasing%20latent%20capacity%20and%20evaluate%20it%20on%20the%20real-world%20dataset%20CelebA.%20Our%20code%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/ICML26-anon/XFactors%7D%7Bgithub.com/ICML26-anon/XFactors%7D.&entry.1838667208=http%3A//arxiv.org/abs/2601.21688v1&entry.124074799=Read"},
{"title": "From Global to Granular: Revealing IQA Model Performance via Correlation Surface", "author": "Baoliang Chen and Danni Huang and Hanwei Zhu and Lingyu Zhu and Wei Zhou and Shiqi Wang and Yuming Fang and Weisi Lin", "abstract": "Evaluation of Image Quality Assessment (IQA) models has long been dominated by global correlation metrics, such as Pearson Linear Correlation Coefficient (PLCC) and Spearman Rank-Order Correlation Coefficient (SRCC). While widely adopted, these metrics reduce performance to a single scalar, failing to capture how ranking consistency varies across the local quality spectrum. For example, two IQA models may achieve identical SRCC values, yet one ranks high-quality images (related to high Mean Opinion Score, MOS) more reliably, while the other better discriminates image pairs with small quality/MOS differences (related to $|\u0394$MOS$|$). Such complementary behaviors are invisible under global metrics. Moreover, SRCC and PLCC are sensitive to test-sample quality distributions, yielding unstable comparisons across test sets. To address these limitations, we propose \\textbf{Granularity-Modulated Correlation (GMC)}, which provides a structured, fine-grained analysis of IQA performance. GMC includes: (1) a \\textbf{Granularity Modulator} that applies Gaussian-weighted correlations conditioned on absolute MOS values and pairwise MOS differences ($|\u0394$MOS$|$) to examine local performance variations, and (2) a \\textbf{Distribution Regulator} that regularizes correlations to mitigate biases from non-uniform quality distributions. The resulting \\textbf{correlation surface} maps correlation values as a joint function of MOS and $|\u0394$MOS$|$, providing a 3D representation of IQA performance. Experiments on standard benchmarks show that GMC reveals performance characteristics invisible to scalar metrics, offering a more informative and reliable paradigm for analyzing, comparing, and deploying IQA models. Codes are available at https://github.com/Dniaaa/GMC.", "link": "http://arxiv.org/abs/2601.21738v1", "date": "2026-01-29", "relevancy": 2.4783, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5036}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4986}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Global%20to%20Granular%3A%20Revealing%20IQA%20Model%20Performance%20via%20Correlation%20Surface&body=Title%3A%20From%20Global%20to%20Granular%3A%20Revealing%20IQA%20Model%20Performance%20via%20Correlation%20Surface%0AAuthor%3A%20Baoliang%20Chen%20and%20Danni%20Huang%20and%20Hanwei%20Zhu%20and%20Lingyu%20Zhu%20and%20Wei%20Zhou%20and%20Shiqi%20Wang%20and%20Yuming%20Fang%20and%20Weisi%20Lin%0AAbstract%3A%20Evaluation%20of%20Image%20Quality%20Assessment%20%28IQA%29%20models%20has%20long%20been%20dominated%20by%20global%20correlation%20metrics%2C%20such%20as%20Pearson%20Linear%20Correlation%20Coefficient%20%28PLCC%29%20and%20Spearman%20Rank-Order%20Correlation%20Coefficient%20%28SRCC%29.%20While%20widely%20adopted%2C%20these%20metrics%20reduce%20performance%20to%20a%20single%20scalar%2C%20failing%20to%20capture%20how%20ranking%20consistency%20varies%20across%20the%20local%20quality%20spectrum.%20For%20example%2C%20two%20IQA%20models%20may%20achieve%20identical%20SRCC%20values%2C%20yet%20one%20ranks%20high-quality%20images%20%28related%20to%20high%20Mean%20Opinion%20Score%2C%20MOS%29%20more%20reliably%2C%20while%20the%20other%20better%20discriminates%20image%20pairs%20with%20small%20quality/MOS%20differences%20%28related%20to%20%24%7C%CE%94%24MOS%24%7C%24%29.%20Such%20complementary%20behaviors%20are%20invisible%20under%20global%20metrics.%20Moreover%2C%20SRCC%20and%20PLCC%20are%20sensitive%20to%20test-sample%20quality%20distributions%2C%20yielding%20unstable%20comparisons%20across%20test%20sets.%20To%20address%20these%20limitations%2C%20we%20propose%20%5Ctextbf%7BGranularity-Modulated%20Correlation%20%28GMC%29%7D%2C%20which%20provides%20a%20structured%2C%20fine-grained%20analysis%20of%20IQA%20performance.%20GMC%20includes%3A%20%281%29%20a%20%5Ctextbf%7BGranularity%20Modulator%7D%20that%20applies%20Gaussian-weighted%20correlations%20conditioned%20on%20absolute%20MOS%20values%20and%20pairwise%20MOS%20differences%20%28%24%7C%CE%94%24MOS%24%7C%24%29%20to%20examine%20local%20performance%20variations%2C%20and%20%282%29%20a%20%5Ctextbf%7BDistribution%20Regulator%7D%20that%20regularizes%20correlations%20to%20mitigate%20biases%20from%20non-uniform%20quality%20distributions.%20The%20resulting%20%5Ctextbf%7Bcorrelation%20surface%7D%20maps%20correlation%20values%20as%20a%20joint%20function%20of%20MOS%20and%20%24%7C%CE%94%24MOS%24%7C%24%2C%20providing%20a%203D%20representation%20of%20IQA%20performance.%20Experiments%20on%20standard%20benchmarks%20show%20that%20GMC%20reveals%20performance%20characteristics%20invisible%20to%20scalar%20metrics%2C%20offering%20a%20more%20informative%20and%20reliable%20paradigm%20for%20analyzing%2C%20comparing%2C%20and%20deploying%20IQA%20models.%20Codes%20are%20available%20at%20https%3A//github.com/Dniaaa/GMC.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Global%2520to%2520Granular%253A%2520Revealing%2520IQA%2520Model%2520Performance%2520via%2520Correlation%2520Surface%26entry.906535625%3DBaoliang%2520Chen%2520and%2520Danni%2520Huang%2520and%2520Hanwei%2520Zhu%2520and%2520Lingyu%2520Zhu%2520and%2520Wei%2520Zhou%2520and%2520Shiqi%2520Wang%2520and%2520Yuming%2520Fang%2520and%2520Weisi%2520Lin%26entry.1292438233%3DEvaluation%2520of%2520Image%2520Quality%2520Assessment%2520%2528IQA%2529%2520models%2520has%2520long%2520been%2520dominated%2520by%2520global%2520correlation%2520metrics%252C%2520such%2520as%2520Pearson%2520Linear%2520Correlation%2520Coefficient%2520%2528PLCC%2529%2520and%2520Spearman%2520Rank-Order%2520Correlation%2520Coefficient%2520%2528SRCC%2529.%2520While%2520widely%2520adopted%252C%2520these%2520metrics%2520reduce%2520performance%2520to%2520a%2520single%2520scalar%252C%2520failing%2520to%2520capture%2520how%2520ranking%2520consistency%2520varies%2520across%2520the%2520local%2520quality%2520spectrum.%2520For%2520example%252C%2520two%2520IQA%2520models%2520may%2520achieve%2520identical%2520SRCC%2520values%252C%2520yet%2520one%2520ranks%2520high-quality%2520images%2520%2528related%2520to%2520high%2520Mean%2520Opinion%2520Score%252C%2520MOS%2529%2520more%2520reliably%252C%2520while%2520the%2520other%2520better%2520discriminates%2520image%2520pairs%2520with%2520small%2520quality/MOS%2520differences%2520%2528related%2520to%2520%2524%257C%25CE%2594%2524MOS%2524%257C%2524%2529.%2520Such%2520complementary%2520behaviors%2520are%2520invisible%2520under%2520global%2520metrics.%2520Moreover%252C%2520SRCC%2520and%2520PLCC%2520are%2520sensitive%2520to%2520test-sample%2520quality%2520distributions%252C%2520yielding%2520unstable%2520comparisons%2520across%2520test%2520sets.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520%255Ctextbf%257BGranularity-Modulated%2520Correlation%2520%2528GMC%2529%257D%252C%2520which%2520provides%2520a%2520structured%252C%2520fine-grained%2520analysis%2520of%2520IQA%2520performance.%2520GMC%2520includes%253A%2520%25281%2529%2520a%2520%255Ctextbf%257BGranularity%2520Modulator%257D%2520that%2520applies%2520Gaussian-weighted%2520correlations%2520conditioned%2520on%2520absolute%2520MOS%2520values%2520and%2520pairwise%2520MOS%2520differences%2520%2528%2524%257C%25CE%2594%2524MOS%2524%257C%2524%2529%2520to%2520examine%2520local%2520performance%2520variations%252C%2520and%2520%25282%2529%2520a%2520%255Ctextbf%257BDistribution%2520Regulator%257D%2520that%2520regularizes%2520correlations%2520to%2520mitigate%2520biases%2520from%2520non-uniform%2520quality%2520distributions.%2520The%2520resulting%2520%255Ctextbf%257Bcorrelation%2520surface%257D%2520maps%2520correlation%2520values%2520as%2520a%2520joint%2520function%2520of%2520MOS%2520and%2520%2524%257C%25CE%2594%2524MOS%2524%257C%2524%252C%2520providing%2520a%25203D%2520representation%2520of%2520IQA%2520performance.%2520Experiments%2520on%2520standard%2520benchmarks%2520show%2520that%2520GMC%2520reveals%2520performance%2520characteristics%2520invisible%2520to%2520scalar%2520metrics%252C%2520offering%2520a%2520more%2520informative%2520and%2520reliable%2520paradigm%2520for%2520analyzing%252C%2520comparing%252C%2520and%2520deploying%2520IQA%2520models.%2520Codes%2520are%2520available%2520at%2520https%253A//github.com/Dniaaa/GMC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Global%20to%20Granular%3A%20Revealing%20IQA%20Model%20Performance%20via%20Correlation%20Surface&entry.906535625=Baoliang%20Chen%20and%20Danni%20Huang%20and%20Hanwei%20Zhu%20and%20Lingyu%20Zhu%20and%20Wei%20Zhou%20and%20Shiqi%20Wang%20and%20Yuming%20Fang%20and%20Weisi%20Lin&entry.1292438233=Evaluation%20of%20Image%20Quality%20Assessment%20%28IQA%29%20models%20has%20long%20been%20dominated%20by%20global%20correlation%20metrics%2C%20such%20as%20Pearson%20Linear%20Correlation%20Coefficient%20%28PLCC%29%20and%20Spearman%20Rank-Order%20Correlation%20Coefficient%20%28SRCC%29.%20While%20widely%20adopted%2C%20these%20metrics%20reduce%20performance%20to%20a%20single%20scalar%2C%20failing%20to%20capture%20how%20ranking%20consistency%20varies%20across%20the%20local%20quality%20spectrum.%20For%20example%2C%20two%20IQA%20models%20may%20achieve%20identical%20SRCC%20values%2C%20yet%20one%20ranks%20high-quality%20images%20%28related%20to%20high%20Mean%20Opinion%20Score%2C%20MOS%29%20more%20reliably%2C%20while%20the%20other%20better%20discriminates%20image%20pairs%20with%20small%20quality/MOS%20differences%20%28related%20to%20%24%7C%CE%94%24MOS%24%7C%24%29.%20Such%20complementary%20behaviors%20are%20invisible%20under%20global%20metrics.%20Moreover%2C%20SRCC%20and%20PLCC%20are%20sensitive%20to%20test-sample%20quality%20distributions%2C%20yielding%20unstable%20comparisons%20across%20test%20sets.%20To%20address%20these%20limitations%2C%20we%20propose%20%5Ctextbf%7BGranularity-Modulated%20Correlation%20%28GMC%29%7D%2C%20which%20provides%20a%20structured%2C%20fine-grained%20analysis%20of%20IQA%20performance.%20GMC%20includes%3A%20%281%29%20a%20%5Ctextbf%7BGranularity%20Modulator%7D%20that%20applies%20Gaussian-weighted%20correlations%20conditioned%20on%20absolute%20MOS%20values%20and%20pairwise%20MOS%20differences%20%28%24%7C%CE%94%24MOS%24%7C%24%29%20to%20examine%20local%20performance%20variations%2C%20and%20%282%29%20a%20%5Ctextbf%7BDistribution%20Regulator%7D%20that%20regularizes%20correlations%20to%20mitigate%20biases%20from%20non-uniform%20quality%20distributions.%20The%20resulting%20%5Ctextbf%7Bcorrelation%20surface%7D%20maps%20correlation%20values%20as%20a%20joint%20function%20of%20MOS%20and%20%24%7C%CE%94%24MOS%24%7C%24%2C%20providing%20a%203D%20representation%20of%20IQA%20performance.%20Experiments%20on%20standard%20benchmarks%20show%20that%20GMC%20reveals%20performance%20characteristics%20invisible%20to%20scalar%20metrics%2C%20offering%20a%20more%20informative%20and%20reliable%20paradigm%20for%20analyzing%2C%20comparing%2C%20and%20deploying%20IQA%20models.%20Codes%20are%20available%20at%20https%3A//github.com/Dniaaa/GMC.&entry.1838667208=http%3A//arxiv.org/abs/2601.21738v1&entry.124074799=Read"},
{"title": "Discovering Hidden Gems in Model Repositories", "author": "Jonathan Kahana and Eliahu Horwitz and Yedid Hoshen", "abstract": "Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of \"hidden gems\", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.", "link": "http://arxiv.org/abs/2601.22157v1", "date": "2026-01-29", "relevancy": 2.4633, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.494}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.494}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20Hidden%20Gems%20in%20Model%20Repositories&body=Title%3A%20Discovering%20Hidden%20Gems%20in%20Model%20Repositories%0AAuthor%3A%20Jonathan%20Kahana%20and%20Eliahu%20Horwitz%20and%20Yedid%20Hoshen%0AAbstract%3A%20Public%20repositories%20host%20millions%20of%20fine-tuned%20models%2C%20yet%20community%20usage%20remains%20disproportionately%20concentrated%20on%20a%20small%20number%20of%20foundation%20checkpoints.%20We%20investigate%20whether%20this%20concentration%20reflects%20efficient%20market%20selection%20or%20if%20superior%20models%20are%20systematically%20overlooked.%20Through%20an%20extensive%20evaluation%20of%20over%202%2C000%20models%2C%20we%20show%20the%20prevalence%20of%20%22hidden%20gems%22%2C%20unpopular%20fine-tunes%20that%20significantly%20outperform%20their%20popular%20counterparts.%20Notably%2C%20within%20the%20Llama-3.1-8B%20family%2C%20we%20find%20rarely%20downloaded%20checkpoints%20that%20improve%20math%20performance%20from%2083.2%25%20to%2096.0%25%20without%20increasing%20inference%20costs.%20However%2C%20discovering%20these%20models%20through%20exhaustive%20evaluation%20of%20every%20uploaded%20model%20is%20computationally%20infeasible.%20We%20therefore%20formulate%20model%20discovery%20as%20a%20Multi-Armed%20Bandit%20problem%20and%20accelerate%20the%20Sequential%20Halving%20search%20algorithm%20by%20using%20shared%20query%20sets%20and%20aggressive%20elimination%20schedules.%20Our%20method%20retrieves%20top%20models%20with%20as%20few%20as%2050%20queries%20per%20candidate%2C%20accelerating%20discovery%20by%20over%2050x.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520Hidden%2520Gems%2520in%2520Model%2520Repositories%26entry.906535625%3DJonathan%2520Kahana%2520and%2520Eliahu%2520Horwitz%2520and%2520Yedid%2520Hoshen%26entry.1292438233%3DPublic%2520repositories%2520host%2520millions%2520of%2520fine-tuned%2520models%252C%2520yet%2520community%2520usage%2520remains%2520disproportionately%2520concentrated%2520on%2520a%2520small%2520number%2520of%2520foundation%2520checkpoints.%2520We%2520investigate%2520whether%2520this%2520concentration%2520reflects%2520efficient%2520market%2520selection%2520or%2520if%2520superior%2520models%2520are%2520systematically%2520overlooked.%2520Through%2520an%2520extensive%2520evaluation%2520of%2520over%25202%252C000%2520models%252C%2520we%2520show%2520the%2520prevalence%2520of%2520%2522hidden%2520gems%2522%252C%2520unpopular%2520fine-tunes%2520that%2520significantly%2520outperform%2520their%2520popular%2520counterparts.%2520Notably%252C%2520within%2520the%2520Llama-3.1-8B%2520family%252C%2520we%2520find%2520rarely%2520downloaded%2520checkpoints%2520that%2520improve%2520math%2520performance%2520from%252083.2%2525%2520to%252096.0%2525%2520without%2520increasing%2520inference%2520costs.%2520However%252C%2520discovering%2520these%2520models%2520through%2520exhaustive%2520evaluation%2520of%2520every%2520uploaded%2520model%2520is%2520computationally%2520infeasible.%2520We%2520therefore%2520formulate%2520model%2520discovery%2520as%2520a%2520Multi-Armed%2520Bandit%2520problem%2520and%2520accelerate%2520the%2520Sequential%2520Halving%2520search%2520algorithm%2520by%2520using%2520shared%2520query%2520sets%2520and%2520aggressive%2520elimination%2520schedules.%2520Our%2520method%2520retrieves%2520top%2520models%2520with%2520as%2520few%2520as%252050%2520queries%2520per%2520candidate%252C%2520accelerating%2520discovery%2520by%2520over%252050x.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20Hidden%20Gems%20in%20Model%20Repositories&entry.906535625=Jonathan%20Kahana%20and%20Eliahu%20Horwitz%20and%20Yedid%20Hoshen&entry.1292438233=Public%20repositories%20host%20millions%20of%20fine-tuned%20models%2C%20yet%20community%20usage%20remains%20disproportionately%20concentrated%20on%20a%20small%20number%20of%20foundation%20checkpoints.%20We%20investigate%20whether%20this%20concentration%20reflects%20efficient%20market%20selection%20or%20if%20superior%20models%20are%20systematically%20overlooked.%20Through%20an%20extensive%20evaluation%20of%20over%202%2C000%20models%2C%20we%20show%20the%20prevalence%20of%20%22hidden%20gems%22%2C%20unpopular%20fine-tunes%20that%20significantly%20outperform%20their%20popular%20counterparts.%20Notably%2C%20within%20the%20Llama-3.1-8B%20family%2C%20we%20find%20rarely%20downloaded%20checkpoints%20that%20improve%20math%20performance%20from%2083.2%25%20to%2096.0%25%20without%20increasing%20inference%20costs.%20However%2C%20discovering%20these%20models%20through%20exhaustive%20evaluation%20of%20every%20uploaded%20model%20is%20computationally%20infeasible.%20We%20therefore%20formulate%20model%20discovery%20as%20a%20Multi-Armed%20Bandit%20problem%20and%20accelerate%20the%20Sequential%20Halving%20search%20algorithm%20by%20using%20shared%20query%20sets%20and%20aggressive%20elimination%20schedules.%20Our%20method%20retrieves%20top%20models%20with%20as%20few%20as%2050%20queries%20per%20candidate%2C%20accelerating%20discovery%20by%20over%2050x.&entry.1838667208=http%3A//arxiv.org/abs/2601.22157v1&entry.124074799=Read"},
{"title": "The Path of Least Resistance: Guiding LLM Reasining Trajectories with Prefix Consensus", "author": "Ishan Jindal and Sai Prashanth Akuthota and Jayant Taneja and Sachin Dev Sharma", "abstract": "Large language models achieve strong reasoning performance, but inference strategies such as Self-Consistency (SC) are computationally expensive, as they fully expand all reasoning traces. We introduce PoLR (Path of Least Resistance), the first inference-time method to leverage prefix consistency for compute-efficient reasoning. PoLR clusters short prefixes of reasoning traces, identifies the dominant cluster, and expands all paths in that cluster, preserving the accuracy benefits of SC while substantially reducing token usage and latency. Our theoretical analysis, framed via mutual information and entropy, explains why early reasoning steps encode strong signals predictive of final correctness. Empirically, PoLR consistently matches or exceeds SC across GSM8K, MATH500, AIME24/25, and GPQA-DIAMOND, reducing token usage by up to 60% and wall-clock latency by up to 50%. Moreover, PoLR is fully complementary to adaptive inference methods (e.g., Adaptive Consistency, Early-Stopping SC) and can serve as a drop-in pre-filter, making SC substantially more efficient and scalable without requiring model fine-tuning.", "link": "http://arxiv.org/abs/2601.21494v1", "date": "2026-01-29", "relevancy": 2.4614, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5011}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5011}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Path%20of%20Least%20Resistance%3A%20Guiding%20LLM%20Reasining%20Trajectories%20with%20Prefix%20Consensus&body=Title%3A%20The%20Path%20of%20Least%20Resistance%3A%20Guiding%20LLM%20Reasining%20Trajectories%20with%20Prefix%20Consensus%0AAuthor%3A%20Ishan%20Jindal%20and%20Sai%20Prashanth%20Akuthota%20and%20Jayant%20Taneja%20and%20Sachin%20Dev%20Sharma%0AAbstract%3A%20Large%20language%20models%20achieve%20strong%20reasoning%20performance%2C%20but%20inference%20strategies%20such%20as%20Self-Consistency%20%28SC%29%20are%20computationally%20expensive%2C%20as%20they%20fully%20expand%20all%20reasoning%20traces.%20We%20introduce%20PoLR%20%28Path%20of%20Least%20Resistance%29%2C%20the%20first%20inference-time%20method%20to%20leverage%20prefix%20consistency%20for%20compute-efficient%20reasoning.%20PoLR%20clusters%20short%20prefixes%20of%20reasoning%20traces%2C%20identifies%20the%20dominant%20cluster%2C%20and%20expands%20all%20paths%20in%20that%20cluster%2C%20preserving%20the%20accuracy%20benefits%20of%20SC%20while%20substantially%20reducing%20token%20usage%20and%20latency.%20Our%20theoretical%20analysis%2C%20framed%20via%20mutual%20information%20and%20entropy%2C%20explains%20why%20early%20reasoning%20steps%20encode%20strong%20signals%20predictive%20of%20final%20correctness.%20Empirically%2C%20PoLR%20consistently%20matches%20or%20exceeds%20SC%20across%20GSM8K%2C%20MATH500%2C%20AIME24/25%2C%20and%20GPQA-DIAMOND%2C%20reducing%20token%20usage%20by%20up%20to%2060%25%20and%20wall-clock%20latency%20by%20up%20to%2050%25.%20Moreover%2C%20PoLR%20is%20fully%20complementary%20to%20adaptive%20inference%20methods%20%28e.g.%2C%20Adaptive%20Consistency%2C%20Early-Stopping%20SC%29%20and%20can%20serve%20as%20a%20drop-in%20pre-filter%2C%20making%20SC%20substantially%20more%20efficient%20and%20scalable%20without%20requiring%20model%20fine-tuning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Path%2520of%2520Least%2520Resistance%253A%2520Guiding%2520LLM%2520Reasining%2520Trajectories%2520with%2520Prefix%2520Consensus%26entry.906535625%3DIshan%2520Jindal%2520and%2520Sai%2520Prashanth%2520Akuthota%2520and%2520Jayant%2520Taneja%2520and%2520Sachin%2520Dev%2520Sharma%26entry.1292438233%3DLarge%2520language%2520models%2520achieve%2520strong%2520reasoning%2520performance%252C%2520but%2520inference%2520strategies%2520such%2520as%2520Self-Consistency%2520%2528SC%2529%2520are%2520computationally%2520expensive%252C%2520as%2520they%2520fully%2520expand%2520all%2520reasoning%2520traces.%2520We%2520introduce%2520PoLR%2520%2528Path%2520of%2520Least%2520Resistance%2529%252C%2520the%2520first%2520inference-time%2520method%2520to%2520leverage%2520prefix%2520consistency%2520for%2520compute-efficient%2520reasoning.%2520PoLR%2520clusters%2520short%2520prefixes%2520of%2520reasoning%2520traces%252C%2520identifies%2520the%2520dominant%2520cluster%252C%2520and%2520expands%2520all%2520paths%2520in%2520that%2520cluster%252C%2520preserving%2520the%2520accuracy%2520benefits%2520of%2520SC%2520while%2520substantially%2520reducing%2520token%2520usage%2520and%2520latency.%2520Our%2520theoretical%2520analysis%252C%2520framed%2520via%2520mutual%2520information%2520and%2520entropy%252C%2520explains%2520why%2520early%2520reasoning%2520steps%2520encode%2520strong%2520signals%2520predictive%2520of%2520final%2520correctness.%2520Empirically%252C%2520PoLR%2520consistently%2520matches%2520or%2520exceeds%2520SC%2520across%2520GSM8K%252C%2520MATH500%252C%2520AIME24/25%252C%2520and%2520GPQA-DIAMOND%252C%2520reducing%2520token%2520usage%2520by%2520up%2520to%252060%2525%2520and%2520wall-clock%2520latency%2520by%2520up%2520to%252050%2525.%2520Moreover%252C%2520PoLR%2520is%2520fully%2520complementary%2520to%2520adaptive%2520inference%2520methods%2520%2528e.g.%252C%2520Adaptive%2520Consistency%252C%2520Early-Stopping%2520SC%2529%2520and%2520can%2520serve%2520as%2520a%2520drop-in%2520pre-filter%252C%2520making%2520SC%2520substantially%2520more%2520efficient%2520and%2520scalable%2520without%2520requiring%2520model%2520fine-tuning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Path%20of%20Least%20Resistance%3A%20Guiding%20LLM%20Reasining%20Trajectories%20with%20Prefix%20Consensus&entry.906535625=Ishan%20Jindal%20and%20Sai%20Prashanth%20Akuthota%20and%20Jayant%20Taneja%20and%20Sachin%20Dev%20Sharma&entry.1292438233=Large%20language%20models%20achieve%20strong%20reasoning%20performance%2C%20but%20inference%20strategies%20such%20as%20Self-Consistency%20%28SC%29%20are%20computationally%20expensive%2C%20as%20they%20fully%20expand%20all%20reasoning%20traces.%20We%20introduce%20PoLR%20%28Path%20of%20Least%20Resistance%29%2C%20the%20first%20inference-time%20method%20to%20leverage%20prefix%20consistency%20for%20compute-efficient%20reasoning.%20PoLR%20clusters%20short%20prefixes%20of%20reasoning%20traces%2C%20identifies%20the%20dominant%20cluster%2C%20and%20expands%20all%20paths%20in%20that%20cluster%2C%20preserving%20the%20accuracy%20benefits%20of%20SC%20while%20substantially%20reducing%20token%20usage%20and%20latency.%20Our%20theoretical%20analysis%2C%20framed%20via%20mutual%20information%20and%20entropy%2C%20explains%20why%20early%20reasoning%20steps%20encode%20strong%20signals%20predictive%20of%20final%20correctness.%20Empirically%2C%20PoLR%20consistently%20matches%20or%20exceeds%20SC%20across%20GSM8K%2C%20MATH500%2C%20AIME24/25%2C%20and%20GPQA-DIAMOND%2C%20reducing%20token%20usage%20by%20up%20to%2060%25%20and%20wall-clock%20latency%20by%20up%20to%2050%25.%20Moreover%2C%20PoLR%20is%20fully%20complementary%20to%20adaptive%20inference%20methods%20%28e.g.%2C%20Adaptive%20Consistency%2C%20Early-Stopping%20SC%29%20and%20can%20serve%20as%20a%20drop-in%20pre-filter%2C%20making%20SC%20substantially%20more%20efficient%20and%20scalable%20without%20requiring%20model%20fine-tuning.&entry.1838667208=http%3A//arxiv.org/abs/2601.21494v1&entry.124074799=Read"},
{"title": "PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing", "author": "Cheng Cui and Ting Sun and Suyin Liang and Tingquan Gao and Zelun Zhang and Jiaxuan Liu and Xueqing Wang and Changda Zhou and Hongen Liu and Manhui Lin and Yue Zhang and Yubo Zhang and Yi Liu and Dianhai Yu and Yanjun Ma", "abstract": "We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-art (SOTA) accuracy of 94.5% on OmniDocBench v1.5. To rigorously evaluate robustness against real-world physical distortions, including scanning, skew, warping, screen-photography, and illumination, we propose the Real5-OmniDocBench benchmark. Experimental results demonstrate that this enhanced model attains SOTA performance on the newly curated benchmark. Furthermore, we extend the model's capabilities by incorporating seal recognition and text spotting tasks, while remaining a 0.9B ultra-compact VLM with high efficiency. Code: https://github.com/PaddlePaddle/PaddleOCR", "link": "http://arxiv.org/abs/2601.21957v1", "date": "2026-01-29", "relevancy": 2.4586, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PaddleOCR-VL-1.5%3A%20Towards%20a%20Multi-Task%200.9B%20VLM%20for%20Robust%20In-the-Wild%20Document%20Parsing&body=Title%3A%20PaddleOCR-VL-1.5%3A%20Towards%20a%20Multi-Task%200.9B%20VLM%20for%20Robust%20In-the-Wild%20Document%20Parsing%0AAuthor%3A%20Cheng%20Cui%20and%20Ting%20Sun%20and%20Suyin%20Liang%20and%20Tingquan%20Gao%20and%20Zelun%20Zhang%20and%20Jiaxuan%20Liu%20and%20Xueqing%20Wang%20and%20Changda%20Zhou%20and%20Hongen%20Liu%20and%20Manhui%20Lin%20and%20Yue%20Zhang%20and%20Yubo%20Zhang%20and%20Yi%20Liu%20and%20Dianhai%20Yu%20and%20Yanjun%20Ma%0AAbstract%3A%20We%20introduce%20PaddleOCR-VL-1.5%2C%20an%20upgraded%20model%20achieving%20a%20new%20state-of-the-art%20%28SOTA%29%20accuracy%20of%2094.5%25%20on%20OmniDocBench%20v1.5.%20To%20rigorously%20evaluate%20robustness%20against%20real-world%20physical%20distortions%2C%20including%20scanning%2C%20skew%2C%20warping%2C%20screen-photography%2C%20and%20illumination%2C%20we%20propose%20the%20Real5-OmniDocBench%20benchmark.%20Experimental%20results%20demonstrate%20that%20this%20enhanced%20model%20attains%20SOTA%20performance%20on%20the%20newly%20curated%20benchmark.%20Furthermore%2C%20we%20extend%20the%20model%27s%20capabilities%20by%20incorporating%20seal%20recognition%20and%20text%20spotting%20tasks%2C%20while%20remaining%20a%200.9B%20ultra-compact%20VLM%20with%20high%20efficiency.%20Code%3A%20https%3A//github.com/PaddlePaddle/PaddleOCR%0ALink%3A%20http%3A//arxiv.org/abs/2601.21957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaddleOCR-VL-1.5%253A%2520Towards%2520a%2520Multi-Task%25200.9B%2520VLM%2520for%2520Robust%2520In-the-Wild%2520Document%2520Parsing%26entry.906535625%3DCheng%2520Cui%2520and%2520Ting%2520Sun%2520and%2520Suyin%2520Liang%2520and%2520Tingquan%2520Gao%2520and%2520Zelun%2520Zhang%2520and%2520Jiaxuan%2520Liu%2520and%2520Xueqing%2520Wang%2520and%2520Changda%2520Zhou%2520and%2520Hongen%2520Liu%2520and%2520Manhui%2520Lin%2520and%2520Yue%2520Zhang%2520and%2520Yubo%2520Zhang%2520and%2520Yi%2520Liu%2520and%2520Dianhai%2520Yu%2520and%2520Yanjun%2520Ma%26entry.1292438233%3DWe%2520introduce%2520PaddleOCR-VL-1.5%252C%2520an%2520upgraded%2520model%2520achieving%2520a%2520new%2520state-of-the-art%2520%2528SOTA%2529%2520accuracy%2520of%252094.5%2525%2520on%2520OmniDocBench%2520v1.5.%2520To%2520rigorously%2520evaluate%2520robustness%2520against%2520real-world%2520physical%2520distortions%252C%2520including%2520scanning%252C%2520skew%252C%2520warping%252C%2520screen-photography%252C%2520and%2520illumination%252C%2520we%2520propose%2520the%2520Real5-OmniDocBench%2520benchmark.%2520Experimental%2520results%2520demonstrate%2520that%2520this%2520enhanced%2520model%2520attains%2520SOTA%2520performance%2520on%2520the%2520newly%2520curated%2520benchmark.%2520Furthermore%252C%2520we%2520extend%2520the%2520model%2527s%2520capabilities%2520by%2520incorporating%2520seal%2520recognition%2520and%2520text%2520spotting%2520tasks%252C%2520while%2520remaining%2520a%25200.9B%2520ultra-compact%2520VLM%2520with%2520high%2520efficiency.%2520Code%253A%2520https%253A//github.com/PaddlePaddle/PaddleOCR%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PaddleOCR-VL-1.5%3A%20Towards%20a%20Multi-Task%200.9B%20VLM%20for%20Robust%20In-the-Wild%20Document%20Parsing&entry.906535625=Cheng%20Cui%20and%20Ting%20Sun%20and%20Suyin%20Liang%20and%20Tingquan%20Gao%20and%20Zelun%20Zhang%20and%20Jiaxuan%20Liu%20and%20Xueqing%20Wang%20and%20Changda%20Zhou%20and%20Hongen%20Liu%20and%20Manhui%20Lin%20and%20Yue%20Zhang%20and%20Yubo%20Zhang%20and%20Yi%20Liu%20and%20Dianhai%20Yu%20and%20Yanjun%20Ma&entry.1292438233=We%20introduce%20PaddleOCR-VL-1.5%2C%20an%20upgraded%20model%20achieving%20a%20new%20state-of-the-art%20%28SOTA%29%20accuracy%20of%2094.5%25%20on%20OmniDocBench%20v1.5.%20To%20rigorously%20evaluate%20robustness%20against%20real-world%20physical%20distortions%2C%20including%20scanning%2C%20skew%2C%20warping%2C%20screen-photography%2C%20and%20illumination%2C%20we%20propose%20the%20Real5-OmniDocBench%20benchmark.%20Experimental%20results%20demonstrate%20that%20this%20enhanced%20model%20attains%20SOTA%20performance%20on%20the%20newly%20curated%20benchmark.%20Furthermore%2C%20we%20extend%20the%20model%27s%20capabilities%20by%20incorporating%20seal%20recognition%20and%20text%20spotting%20tasks%2C%20while%20remaining%20a%200.9B%20ultra-compact%20VLM%20with%20high%20efficiency.%20Code%3A%20https%3A//github.com/PaddlePaddle/PaddleOCR&entry.1838667208=http%3A//arxiv.org/abs/2601.21957v1&entry.124074799=Read"},
{"title": "Low-Rank Plus Sparse Matrix Transfer Learning under Growing Representations and Ambient Dimensions", "author": "Jinhang Chai and Xuyuan Liu and Elynn Chen and Yujun Yan", "abstract": "Learning systems often expand their ambient features or latent representations over time, embedding earlier representations into larger spaces with limited new latent structure. We study transfer learning for structured matrix estimation under simultaneous growth of the ambient dimension and the intrinsic representation, where a well-estimated source task is embedded as a subspace of a higher-dimensional target task.\n  We propose a general transfer framework in which the target parameter decomposes into an embedded source component, low-dimensional low-rank innovations, and sparse edits, and develop an anchored alternating projection estimator that preserves transferred subspaces while estimating only low-dimensional innovations and sparse modifications. We establish deterministic error bounds that separate target noise, representation growth, and source estimation error, yielding strictly improved rates when rank and sparsity increments are small.\n  We demonstrate the generality of the framework by applying it to two canonical problems. For Markov transition matrix estimation from a single trajectory, we derive end-to-end theoretical guarantees under dependent noise. For structured covariance estimation under enlarged dimensions, we provide complementary theoretical analysis in the appendix and empirically validate consistent transfer gains.", "link": "http://arxiv.org/abs/2601.21873v1", "date": "2026-01-29", "relevancy": 2.4575, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.503}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.487}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Rank%20Plus%20Sparse%20Matrix%20Transfer%20Learning%20under%20Growing%20Representations%20and%20Ambient%20Dimensions&body=Title%3A%20Low-Rank%20Plus%20Sparse%20Matrix%20Transfer%20Learning%20under%20Growing%20Representations%20and%20Ambient%20Dimensions%0AAuthor%3A%20Jinhang%20Chai%20and%20Xuyuan%20Liu%20and%20Elynn%20Chen%20and%20Yujun%20Yan%0AAbstract%3A%20Learning%20systems%20often%20expand%20their%20ambient%20features%20or%20latent%20representations%20over%20time%2C%20embedding%20earlier%20representations%20into%20larger%20spaces%20with%20limited%20new%20latent%20structure.%20We%20study%20transfer%20learning%20for%20structured%20matrix%20estimation%20under%20simultaneous%20growth%20of%20the%20ambient%20dimension%20and%20the%20intrinsic%20representation%2C%20where%20a%20well-estimated%20source%20task%20is%20embedded%20as%20a%20subspace%20of%20a%20higher-dimensional%20target%20task.%0A%20%20We%20propose%20a%20general%20transfer%20framework%20in%20which%20the%20target%20parameter%20decomposes%20into%20an%20embedded%20source%20component%2C%20low-dimensional%20low-rank%20innovations%2C%20and%20sparse%20edits%2C%20and%20develop%20an%20anchored%20alternating%20projection%20estimator%20that%20preserves%20transferred%20subspaces%20while%20estimating%20only%20low-dimensional%20innovations%20and%20sparse%20modifications.%20We%20establish%20deterministic%20error%20bounds%20that%20separate%20target%20noise%2C%20representation%20growth%2C%20and%20source%20estimation%20error%2C%20yielding%20strictly%20improved%20rates%20when%20rank%20and%20sparsity%20increments%20are%20small.%0A%20%20We%20demonstrate%20the%20generality%20of%20the%20framework%20by%20applying%20it%20to%20two%20canonical%20problems.%20For%20Markov%20transition%20matrix%20estimation%20from%20a%20single%20trajectory%2C%20we%20derive%20end-to-end%20theoretical%20guarantees%20under%20dependent%20noise.%20For%20structured%20covariance%20estimation%20under%20enlarged%20dimensions%2C%20we%20provide%20complementary%20theoretical%20analysis%20in%20the%20appendix%20and%20empirically%20validate%20consistent%20transfer%20gains.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21873v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Rank%2520Plus%2520Sparse%2520Matrix%2520Transfer%2520Learning%2520under%2520Growing%2520Representations%2520and%2520Ambient%2520Dimensions%26entry.906535625%3DJinhang%2520Chai%2520and%2520Xuyuan%2520Liu%2520and%2520Elynn%2520Chen%2520and%2520Yujun%2520Yan%26entry.1292438233%3DLearning%2520systems%2520often%2520expand%2520their%2520ambient%2520features%2520or%2520latent%2520representations%2520over%2520time%252C%2520embedding%2520earlier%2520representations%2520into%2520larger%2520spaces%2520with%2520limited%2520new%2520latent%2520structure.%2520We%2520study%2520transfer%2520learning%2520for%2520structured%2520matrix%2520estimation%2520under%2520simultaneous%2520growth%2520of%2520the%2520ambient%2520dimension%2520and%2520the%2520intrinsic%2520representation%252C%2520where%2520a%2520well-estimated%2520source%2520task%2520is%2520embedded%2520as%2520a%2520subspace%2520of%2520a%2520higher-dimensional%2520target%2520task.%250A%2520%2520We%2520propose%2520a%2520general%2520transfer%2520framework%2520in%2520which%2520the%2520target%2520parameter%2520decomposes%2520into%2520an%2520embedded%2520source%2520component%252C%2520low-dimensional%2520low-rank%2520innovations%252C%2520and%2520sparse%2520edits%252C%2520and%2520develop%2520an%2520anchored%2520alternating%2520projection%2520estimator%2520that%2520preserves%2520transferred%2520subspaces%2520while%2520estimating%2520only%2520low-dimensional%2520innovations%2520and%2520sparse%2520modifications.%2520We%2520establish%2520deterministic%2520error%2520bounds%2520that%2520separate%2520target%2520noise%252C%2520representation%2520growth%252C%2520and%2520source%2520estimation%2520error%252C%2520yielding%2520strictly%2520improved%2520rates%2520when%2520rank%2520and%2520sparsity%2520increments%2520are%2520small.%250A%2520%2520We%2520demonstrate%2520the%2520generality%2520of%2520the%2520framework%2520by%2520applying%2520it%2520to%2520two%2520canonical%2520problems.%2520For%2520Markov%2520transition%2520matrix%2520estimation%2520from%2520a%2520single%2520trajectory%252C%2520we%2520derive%2520end-to-end%2520theoretical%2520guarantees%2520under%2520dependent%2520noise.%2520For%2520structured%2520covariance%2520estimation%2520under%2520enlarged%2520dimensions%252C%2520we%2520provide%2520complementary%2520theoretical%2520analysis%2520in%2520the%2520appendix%2520and%2520empirically%2520validate%2520consistent%2520transfer%2520gains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21873v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Rank%20Plus%20Sparse%20Matrix%20Transfer%20Learning%20under%20Growing%20Representations%20and%20Ambient%20Dimensions&entry.906535625=Jinhang%20Chai%20and%20Xuyuan%20Liu%20and%20Elynn%20Chen%20and%20Yujun%20Yan&entry.1292438233=Learning%20systems%20often%20expand%20their%20ambient%20features%20or%20latent%20representations%20over%20time%2C%20embedding%20earlier%20representations%20into%20larger%20spaces%20with%20limited%20new%20latent%20structure.%20We%20study%20transfer%20learning%20for%20structured%20matrix%20estimation%20under%20simultaneous%20growth%20of%20the%20ambient%20dimension%20and%20the%20intrinsic%20representation%2C%20where%20a%20well-estimated%20source%20task%20is%20embedded%20as%20a%20subspace%20of%20a%20higher-dimensional%20target%20task.%0A%20%20We%20propose%20a%20general%20transfer%20framework%20in%20which%20the%20target%20parameter%20decomposes%20into%20an%20embedded%20source%20component%2C%20low-dimensional%20low-rank%20innovations%2C%20and%20sparse%20edits%2C%20and%20develop%20an%20anchored%20alternating%20projection%20estimator%20that%20preserves%20transferred%20subspaces%20while%20estimating%20only%20low-dimensional%20innovations%20and%20sparse%20modifications.%20We%20establish%20deterministic%20error%20bounds%20that%20separate%20target%20noise%2C%20representation%20growth%2C%20and%20source%20estimation%20error%2C%20yielding%20strictly%20improved%20rates%20when%20rank%20and%20sparsity%20increments%20are%20small.%0A%20%20We%20demonstrate%20the%20generality%20of%20the%20framework%20by%20applying%20it%20to%20two%20canonical%20problems.%20For%20Markov%20transition%20matrix%20estimation%20from%20a%20single%20trajectory%2C%20we%20derive%20end-to-end%20theoretical%20guarantees%20under%20dependent%20noise.%20For%20structured%20covariance%20estimation%20under%20enlarged%20dimensions%2C%20we%20provide%20complementary%20theoretical%20analysis%20in%20the%20appendix%20and%20empirically%20validate%20consistent%20transfer%20gains.&entry.1838667208=http%3A//arxiv.org/abs/2601.21873v1&entry.124074799=Read"},
{"title": "NetMamba+: A Framework of Pre-trained Models for Efficient and Accurate Network Traffic Classification", "author": "Tongze Wang and Xiaohui Xie and Wenduo Wang and Chuyi Wang and Jinzhou Liu and Boyan Huang and Yannan Hu and Youjian Zhao and Yong Cui", "abstract": "With the rapid growth of encrypted network traffic, effective traffic classification has become essential for network security and quality of service management. Current machine learning and deep learning approaches for traffic classification face three critical challenges: computational inefficiency of Transformer architectures, inadequate traffic representations with loss of crucial byte-level features while retaining detrimental biases, and poor handling of long-tail distributions in real-world data. We propose NetMamba+, a framework that addresses these challenges through three key innovations: (1) an efficient architecture considering Mamba and Flash Attention mechanisms, (2) a multimodal traffic representation scheme that preserves essential traffic information while eliminating biases, and (3) a label distribution-aware fine-tuning strategy. Evaluation experiments on massive datasets encompassing four main classification tasks showcase NetMamba+'s superior classification performance compared to state-of-the-art baselines, with improvements of up to 6.44\\% in F1 score. Moreover, NetMamba+ demonstrates excellent efficiency, achieving 1.7x higher inference throughput than the best baseline while maintaining comparably low memory usage. Furthermore, NetMamba+ exhibits superior few-shot learning abilities, achieving better classification performance with fewer labeled data. Additionally, we implement an online traffic classification system that demonstrates robust real-world performance with a throughput of 261.87 Mb/s. As the first framework to adapt Mamba architecture for network traffic classification, NetMamba+ opens new possibilities for efficient and accurate traffic analysis in complex network environments.", "link": "http://arxiv.org/abs/2601.21792v1", "date": "2026-01-29", "relevancy": 2.4564, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5057}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4939}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NetMamba%2B%3A%20A%20Framework%20of%20Pre-trained%20Models%20for%20Efficient%20and%20Accurate%20Network%20Traffic%20Classification&body=Title%3A%20NetMamba%2B%3A%20A%20Framework%20of%20Pre-trained%20Models%20for%20Efficient%20and%20Accurate%20Network%20Traffic%20Classification%0AAuthor%3A%20Tongze%20Wang%20and%20Xiaohui%20Xie%20and%20Wenduo%20Wang%20and%20Chuyi%20Wang%20and%20Jinzhou%20Liu%20and%20Boyan%20Huang%20and%20Yannan%20Hu%20and%20Youjian%20Zhao%20and%20Yong%20Cui%0AAbstract%3A%20With%20the%20rapid%20growth%20of%20encrypted%20network%20traffic%2C%20effective%20traffic%20classification%20has%20become%20essential%20for%20network%20security%20and%20quality%20of%20service%20management.%20Current%20machine%20learning%20and%20deep%20learning%20approaches%20for%20traffic%20classification%20face%20three%20critical%20challenges%3A%20computational%20inefficiency%20of%20Transformer%20architectures%2C%20inadequate%20traffic%20representations%20with%20loss%20of%20crucial%20byte-level%20features%20while%20retaining%20detrimental%20biases%2C%20and%20poor%20handling%20of%20long-tail%20distributions%20in%20real-world%20data.%20We%20propose%20NetMamba%2B%2C%20a%20framework%20that%20addresses%20these%20challenges%20through%20three%20key%20innovations%3A%20%281%29%20an%20efficient%20architecture%20considering%20Mamba%20and%20Flash%20Attention%20mechanisms%2C%20%282%29%20a%20multimodal%20traffic%20representation%20scheme%20that%20preserves%20essential%20traffic%20information%20while%20eliminating%20biases%2C%20and%20%283%29%20a%20label%20distribution-aware%20fine-tuning%20strategy.%20Evaluation%20experiments%20on%20massive%20datasets%20encompassing%20four%20main%20classification%20tasks%20showcase%20NetMamba%2B%27s%20superior%20classification%20performance%20compared%20to%20state-of-the-art%20baselines%2C%20with%20improvements%20of%20up%20to%206.44%5C%25%20in%20F1%20score.%20Moreover%2C%20NetMamba%2B%20demonstrates%20excellent%20efficiency%2C%20achieving%201.7x%20higher%20inference%20throughput%20than%20the%20best%20baseline%20while%20maintaining%20comparably%20low%20memory%20usage.%20Furthermore%2C%20NetMamba%2B%20exhibits%20superior%20few-shot%20learning%20abilities%2C%20achieving%20better%20classification%20performance%20with%20fewer%20labeled%20data.%20Additionally%2C%20we%20implement%20an%20online%20traffic%20classification%20system%20that%20demonstrates%20robust%20real-world%20performance%20with%20a%20throughput%20of%20261.87%20Mb/s.%20As%20the%20first%20framework%20to%20adapt%20Mamba%20architecture%20for%20network%20traffic%20classification%2C%20NetMamba%2B%20opens%20new%20possibilities%20for%20efficient%20and%20accurate%20traffic%20analysis%20in%20complex%20network%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNetMamba%252B%253A%2520A%2520Framework%2520of%2520Pre-trained%2520Models%2520for%2520Efficient%2520and%2520Accurate%2520Network%2520Traffic%2520Classification%26entry.906535625%3DTongze%2520Wang%2520and%2520Xiaohui%2520Xie%2520and%2520Wenduo%2520Wang%2520and%2520Chuyi%2520Wang%2520and%2520Jinzhou%2520Liu%2520and%2520Boyan%2520Huang%2520and%2520Yannan%2520Hu%2520and%2520Youjian%2520Zhao%2520and%2520Yong%2520Cui%26entry.1292438233%3DWith%2520the%2520rapid%2520growth%2520of%2520encrypted%2520network%2520traffic%252C%2520effective%2520traffic%2520classification%2520has%2520become%2520essential%2520for%2520network%2520security%2520and%2520quality%2520of%2520service%2520management.%2520Current%2520machine%2520learning%2520and%2520deep%2520learning%2520approaches%2520for%2520traffic%2520classification%2520face%2520three%2520critical%2520challenges%253A%2520computational%2520inefficiency%2520of%2520Transformer%2520architectures%252C%2520inadequate%2520traffic%2520representations%2520with%2520loss%2520of%2520crucial%2520byte-level%2520features%2520while%2520retaining%2520detrimental%2520biases%252C%2520and%2520poor%2520handling%2520of%2520long-tail%2520distributions%2520in%2520real-world%2520data.%2520We%2520propose%2520NetMamba%252B%252C%2520a%2520framework%2520that%2520addresses%2520these%2520challenges%2520through%2520three%2520key%2520innovations%253A%2520%25281%2529%2520an%2520efficient%2520architecture%2520considering%2520Mamba%2520and%2520Flash%2520Attention%2520mechanisms%252C%2520%25282%2529%2520a%2520multimodal%2520traffic%2520representation%2520scheme%2520that%2520preserves%2520essential%2520traffic%2520information%2520while%2520eliminating%2520biases%252C%2520and%2520%25283%2529%2520a%2520label%2520distribution-aware%2520fine-tuning%2520strategy.%2520Evaluation%2520experiments%2520on%2520massive%2520datasets%2520encompassing%2520four%2520main%2520classification%2520tasks%2520showcase%2520NetMamba%252B%2527s%2520superior%2520classification%2520performance%2520compared%2520to%2520state-of-the-art%2520baselines%252C%2520with%2520improvements%2520of%2520up%2520to%25206.44%255C%2525%2520in%2520F1%2520score.%2520Moreover%252C%2520NetMamba%252B%2520demonstrates%2520excellent%2520efficiency%252C%2520achieving%25201.7x%2520higher%2520inference%2520throughput%2520than%2520the%2520best%2520baseline%2520while%2520maintaining%2520comparably%2520low%2520memory%2520usage.%2520Furthermore%252C%2520NetMamba%252B%2520exhibits%2520superior%2520few-shot%2520learning%2520abilities%252C%2520achieving%2520better%2520classification%2520performance%2520with%2520fewer%2520labeled%2520data.%2520Additionally%252C%2520we%2520implement%2520an%2520online%2520traffic%2520classification%2520system%2520that%2520demonstrates%2520robust%2520real-world%2520performance%2520with%2520a%2520throughput%2520of%2520261.87%2520Mb/s.%2520As%2520the%2520first%2520framework%2520to%2520adapt%2520Mamba%2520architecture%2520for%2520network%2520traffic%2520classification%252C%2520NetMamba%252B%2520opens%2520new%2520possibilities%2520for%2520efficient%2520and%2520accurate%2520traffic%2520analysis%2520in%2520complex%2520network%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NetMamba%2B%3A%20A%20Framework%20of%20Pre-trained%20Models%20for%20Efficient%20and%20Accurate%20Network%20Traffic%20Classification&entry.906535625=Tongze%20Wang%20and%20Xiaohui%20Xie%20and%20Wenduo%20Wang%20and%20Chuyi%20Wang%20and%20Jinzhou%20Liu%20and%20Boyan%20Huang%20and%20Yannan%20Hu%20and%20Youjian%20Zhao%20and%20Yong%20Cui&entry.1292438233=With%20the%20rapid%20growth%20of%20encrypted%20network%20traffic%2C%20effective%20traffic%20classification%20has%20become%20essential%20for%20network%20security%20and%20quality%20of%20service%20management.%20Current%20machine%20learning%20and%20deep%20learning%20approaches%20for%20traffic%20classification%20face%20three%20critical%20challenges%3A%20computational%20inefficiency%20of%20Transformer%20architectures%2C%20inadequate%20traffic%20representations%20with%20loss%20of%20crucial%20byte-level%20features%20while%20retaining%20detrimental%20biases%2C%20and%20poor%20handling%20of%20long-tail%20distributions%20in%20real-world%20data.%20We%20propose%20NetMamba%2B%2C%20a%20framework%20that%20addresses%20these%20challenges%20through%20three%20key%20innovations%3A%20%281%29%20an%20efficient%20architecture%20considering%20Mamba%20and%20Flash%20Attention%20mechanisms%2C%20%282%29%20a%20multimodal%20traffic%20representation%20scheme%20that%20preserves%20essential%20traffic%20information%20while%20eliminating%20biases%2C%20and%20%283%29%20a%20label%20distribution-aware%20fine-tuning%20strategy.%20Evaluation%20experiments%20on%20massive%20datasets%20encompassing%20four%20main%20classification%20tasks%20showcase%20NetMamba%2B%27s%20superior%20classification%20performance%20compared%20to%20state-of-the-art%20baselines%2C%20with%20improvements%20of%20up%20to%206.44%5C%25%20in%20F1%20score.%20Moreover%2C%20NetMamba%2B%20demonstrates%20excellent%20efficiency%2C%20achieving%201.7x%20higher%20inference%20throughput%20than%20the%20best%20baseline%20while%20maintaining%20comparably%20low%20memory%20usage.%20Furthermore%2C%20NetMamba%2B%20exhibits%20superior%20few-shot%20learning%20abilities%2C%20achieving%20better%20classification%20performance%20with%20fewer%20labeled%20data.%20Additionally%2C%20we%20implement%20an%20online%20traffic%20classification%20system%20that%20demonstrates%20robust%20real-world%20performance%20with%20a%20throughput%20of%20261.87%20Mb/s.%20As%20the%20first%20framework%20to%20adapt%20Mamba%20architecture%20for%20network%20traffic%20classification%2C%20NetMamba%2B%20opens%20new%20possibilities%20for%20efficient%20and%20accurate%20traffic%20analysis%20in%20complex%20network%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2601.21792v1&entry.124074799=Read"},
{"title": "GR3EN: Generative Relighting for 3D Environments", "author": "Xiaoyan Xing and Philipp Henzler and Junhwa Hur and Runze Li and Jonathan T. Barron and Pratul P. Srinivasan and Dor Verbin", "abstract": "We present a method for relighting 3D reconstructions of large room-scale environments. Existing solutions for 3D scene relighting often require solving under-determined or ill-conditioned inverse rendering problems, and are as such unable to produce high-quality results on complex real-world scenes. Though recent progress in using generative image and video diffusion models for relighting has been promising, these techniques are either limited to 2D image and video relighting or 3D relighting of individual objects. Our approach enables controllable 3D relighting of room-scale scenes by distilling the outputs of a video-to-video relighting diffusion model into a 3D reconstruction. This side-steps the need to solve a difficult inverse rendering problem, and results in a flexible system that can relight 3D reconstructions of complex real-world scenes. We validate our approach on both synthetic and real-world datasets to show that it can faithfully render novel views of scenes under new lighting conditions.", "link": "http://arxiv.org/abs/2601.16272v2", "date": "2026-01-29", "relevancy": 2.4474, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6215}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6104}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GR3EN%3A%20Generative%20Relighting%20for%203D%20Environments&body=Title%3A%20GR3EN%3A%20Generative%20Relighting%20for%203D%20Environments%0AAuthor%3A%20Xiaoyan%20Xing%20and%20Philipp%20Henzler%20and%20Junhwa%20Hur%20and%20Runze%20Li%20and%20Jonathan%20T.%20Barron%20and%20Pratul%20P.%20Srinivasan%20and%20Dor%20Verbin%0AAbstract%3A%20We%20present%20a%20method%20for%20relighting%203D%20reconstructions%20of%20large%20room-scale%20environments.%20Existing%20solutions%20for%203D%20scene%20relighting%20often%20require%20solving%20under-determined%20or%20ill-conditioned%20inverse%20rendering%20problems%2C%20and%20are%20as%20such%20unable%20to%20produce%20high-quality%20results%20on%20complex%20real-world%20scenes.%20Though%20recent%20progress%20in%20using%20generative%20image%20and%20video%20diffusion%20models%20for%20relighting%20has%20been%20promising%2C%20these%20techniques%20are%20either%20limited%20to%202D%20image%20and%20video%20relighting%20or%203D%20relighting%20of%20individual%20objects.%20Our%20approach%20enables%20controllable%203D%20relighting%20of%20room-scale%20scenes%20by%20distilling%20the%20outputs%20of%20a%20video-to-video%20relighting%20diffusion%20model%20into%20a%203D%20reconstruction.%20This%20side-steps%20the%20need%20to%20solve%20a%20difficult%20inverse%20rendering%20problem%2C%20and%20results%20in%20a%20flexible%20system%20that%20can%20relight%203D%20reconstructions%20of%20complex%20real-world%20scenes.%20We%20validate%20our%20approach%20on%20both%20synthetic%20and%20real-world%20datasets%20to%20show%20that%20it%20can%20faithfully%20render%20novel%20views%20of%20scenes%20under%20new%20lighting%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16272v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGR3EN%253A%2520Generative%2520Relighting%2520for%25203D%2520Environments%26entry.906535625%3DXiaoyan%2520Xing%2520and%2520Philipp%2520Henzler%2520and%2520Junhwa%2520Hur%2520and%2520Runze%2520Li%2520and%2520Jonathan%2520T.%2520Barron%2520and%2520Pratul%2520P.%2520Srinivasan%2520and%2520Dor%2520Verbin%26entry.1292438233%3DWe%2520present%2520a%2520method%2520for%2520relighting%25203D%2520reconstructions%2520of%2520large%2520room-scale%2520environments.%2520Existing%2520solutions%2520for%25203D%2520scene%2520relighting%2520often%2520require%2520solving%2520under-determined%2520or%2520ill-conditioned%2520inverse%2520rendering%2520problems%252C%2520and%2520are%2520as%2520such%2520unable%2520to%2520produce%2520high-quality%2520results%2520on%2520complex%2520real-world%2520scenes.%2520Though%2520recent%2520progress%2520in%2520using%2520generative%2520image%2520and%2520video%2520diffusion%2520models%2520for%2520relighting%2520has%2520been%2520promising%252C%2520these%2520techniques%2520are%2520either%2520limited%2520to%25202D%2520image%2520and%2520video%2520relighting%2520or%25203D%2520relighting%2520of%2520individual%2520objects.%2520Our%2520approach%2520enables%2520controllable%25203D%2520relighting%2520of%2520room-scale%2520scenes%2520by%2520distilling%2520the%2520outputs%2520of%2520a%2520video-to-video%2520relighting%2520diffusion%2520model%2520into%2520a%25203D%2520reconstruction.%2520This%2520side-steps%2520the%2520need%2520to%2520solve%2520a%2520difficult%2520inverse%2520rendering%2520problem%252C%2520and%2520results%2520in%2520a%2520flexible%2520system%2520that%2520can%2520relight%25203D%2520reconstructions%2520of%2520complex%2520real-world%2520scenes.%2520We%2520validate%2520our%2520approach%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520to%2520show%2520that%2520it%2520can%2520faithfully%2520render%2520novel%2520views%2520of%2520scenes%2520under%2520new%2520lighting%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16272v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GR3EN%3A%20Generative%20Relighting%20for%203D%20Environments&entry.906535625=Xiaoyan%20Xing%20and%20Philipp%20Henzler%20and%20Junhwa%20Hur%20and%20Runze%20Li%20and%20Jonathan%20T.%20Barron%20and%20Pratul%20P.%20Srinivasan%20and%20Dor%20Verbin&entry.1292438233=We%20present%20a%20method%20for%20relighting%203D%20reconstructions%20of%20large%20room-scale%20environments.%20Existing%20solutions%20for%203D%20scene%20relighting%20often%20require%20solving%20under-determined%20or%20ill-conditioned%20inverse%20rendering%20problems%2C%20and%20are%20as%20such%20unable%20to%20produce%20high-quality%20results%20on%20complex%20real-world%20scenes.%20Though%20recent%20progress%20in%20using%20generative%20image%20and%20video%20diffusion%20models%20for%20relighting%20has%20been%20promising%2C%20these%20techniques%20are%20either%20limited%20to%202D%20image%20and%20video%20relighting%20or%203D%20relighting%20of%20individual%20objects.%20Our%20approach%20enables%20controllable%203D%20relighting%20of%20room-scale%20scenes%20by%20distilling%20the%20outputs%20of%20a%20video-to-video%20relighting%20diffusion%20model%20into%20a%203D%20reconstruction.%20This%20side-steps%20the%20need%20to%20solve%20a%20difficult%20inverse%20rendering%20problem%2C%20and%20results%20in%20a%20flexible%20system%20that%20can%20relight%203D%20reconstructions%20of%20complex%20real-world%20scenes.%20We%20validate%20our%20approach%20on%20both%20synthetic%20and%20real-world%20datasets%20to%20show%20that%20it%20can%20faithfully%20render%20novel%20views%20of%20scenes%20under%20new%20lighting%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2601.16272v2&entry.124074799=Read"},
{"title": "From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning", "author": "Haoran Tang and Rajiv Khanna", "abstract": "Most LLM unlearning methods aim to approximate retrain-from-scratch behaviors with minimal distribution shift, often via alignment-style objectives defined in the prediction space. While effective at reducing forgotten content generation, such approaches may act as suppression: forgotten concepts can persist in representations and remain entangled with retained knowledge. We introduce CLReg, a contrastive representation regularizer that identifies forget features while pushing them away from retain features, explicitly reducing forget-retain interference with minimal shifts on retain features. We provide first theoretical insights that relate representation shaping to entanglement reduction. Across unlearning benchmarks and LLMs of different sizes, CLReg decreases forget-retain representation entanglement that facilitates mainstream unlearning methods without positing extra privacy risks, inspiring future work that reshapes the representation space to remove forget concepts.", "link": "http://arxiv.org/abs/2601.22028v1", "date": "2026-01-29", "relevancy": 2.4374, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5016}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4911}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Logits%20to%20Latents%3A%20Contrastive%20Representation%20Shaping%20for%20LLM%20Unlearning&body=Title%3A%20From%20Logits%20to%20Latents%3A%20Contrastive%20Representation%20Shaping%20for%20LLM%20Unlearning%0AAuthor%3A%20Haoran%20Tang%20and%20Rajiv%20Khanna%0AAbstract%3A%20Most%20LLM%20unlearning%20methods%20aim%20to%20approximate%20retrain-from-scratch%20behaviors%20with%20minimal%20distribution%20shift%2C%20often%20via%20alignment-style%20objectives%20defined%20in%20the%20prediction%20space.%20While%20effective%20at%20reducing%20forgotten%20content%20generation%2C%20such%20approaches%20may%20act%20as%20suppression%3A%20forgotten%20concepts%20can%20persist%20in%20representations%20and%20remain%20entangled%20with%20retained%20knowledge.%20We%20introduce%20CLReg%2C%20a%20contrastive%20representation%20regularizer%20that%20identifies%20forget%20features%20while%20pushing%20them%20away%20from%20retain%20features%2C%20explicitly%20reducing%20forget-retain%20interference%20with%20minimal%20shifts%20on%20retain%20features.%20We%20provide%20first%20theoretical%20insights%20that%20relate%20representation%20shaping%20to%20entanglement%20reduction.%20Across%20unlearning%20benchmarks%20and%20LLMs%20of%20different%20sizes%2C%20CLReg%20decreases%20forget-retain%20representation%20entanglement%20that%20facilitates%20mainstream%20unlearning%20methods%20without%20positing%20extra%20privacy%20risks%2C%20inspiring%20future%20work%20that%20reshapes%20the%20representation%20space%20to%20remove%20forget%20concepts.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Logits%2520to%2520Latents%253A%2520Contrastive%2520Representation%2520Shaping%2520for%2520LLM%2520Unlearning%26entry.906535625%3DHaoran%2520Tang%2520and%2520Rajiv%2520Khanna%26entry.1292438233%3DMost%2520LLM%2520unlearning%2520methods%2520aim%2520to%2520approximate%2520retrain-from-scratch%2520behaviors%2520with%2520minimal%2520distribution%2520shift%252C%2520often%2520via%2520alignment-style%2520objectives%2520defined%2520in%2520the%2520prediction%2520space.%2520While%2520effective%2520at%2520reducing%2520forgotten%2520content%2520generation%252C%2520such%2520approaches%2520may%2520act%2520as%2520suppression%253A%2520forgotten%2520concepts%2520can%2520persist%2520in%2520representations%2520and%2520remain%2520entangled%2520with%2520retained%2520knowledge.%2520We%2520introduce%2520CLReg%252C%2520a%2520contrastive%2520representation%2520regularizer%2520that%2520identifies%2520forget%2520features%2520while%2520pushing%2520them%2520away%2520from%2520retain%2520features%252C%2520explicitly%2520reducing%2520forget-retain%2520interference%2520with%2520minimal%2520shifts%2520on%2520retain%2520features.%2520We%2520provide%2520first%2520theoretical%2520insights%2520that%2520relate%2520representation%2520shaping%2520to%2520entanglement%2520reduction.%2520Across%2520unlearning%2520benchmarks%2520and%2520LLMs%2520of%2520different%2520sizes%252C%2520CLReg%2520decreases%2520forget-retain%2520representation%2520entanglement%2520that%2520facilitates%2520mainstream%2520unlearning%2520methods%2520without%2520positing%2520extra%2520privacy%2520risks%252C%2520inspiring%2520future%2520work%2520that%2520reshapes%2520the%2520representation%2520space%2520to%2520remove%2520forget%2520concepts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Logits%20to%20Latents%3A%20Contrastive%20Representation%20Shaping%20for%20LLM%20Unlearning&entry.906535625=Haoran%20Tang%20and%20Rajiv%20Khanna&entry.1292438233=Most%20LLM%20unlearning%20methods%20aim%20to%20approximate%20retrain-from-scratch%20behaviors%20with%20minimal%20distribution%20shift%2C%20often%20via%20alignment-style%20objectives%20defined%20in%20the%20prediction%20space.%20While%20effective%20at%20reducing%20forgotten%20content%20generation%2C%20such%20approaches%20may%20act%20as%20suppression%3A%20forgotten%20concepts%20can%20persist%20in%20representations%20and%20remain%20entangled%20with%20retained%20knowledge.%20We%20introduce%20CLReg%2C%20a%20contrastive%20representation%20regularizer%20that%20identifies%20forget%20features%20while%20pushing%20them%20away%20from%20retain%20features%2C%20explicitly%20reducing%20forget-retain%20interference%20with%20minimal%20shifts%20on%20retain%20features.%20We%20provide%20first%20theoretical%20insights%20that%20relate%20representation%20shaping%20to%20entanglement%20reduction.%20Across%20unlearning%20benchmarks%20and%20LLMs%20of%20different%20sizes%2C%20CLReg%20decreases%20forget-retain%20representation%20entanglement%20that%20facilitates%20mainstream%20unlearning%20methods%20without%20positing%20extra%20privacy%20risks%2C%20inspiring%20future%20work%20that%20reshapes%20the%20representation%20space%20to%20remove%20forget%20concepts.&entry.1838667208=http%3A//arxiv.org/abs/2601.22028v1&entry.124074799=Read"},
{"title": "Vecchia-Inducing-Points Full-Scale Approximations for Gaussian Processes", "author": "Tim Gyger and Reinhard Furrer and Fabio Sigrist", "abstract": "Gaussian processes are flexible, probabilistic, non-parametric models widely used in machine learning and statistics. However, their scalability to large data sets is limited by computational constraints. To overcome these challenges, we propose Vecchia-inducing-points full-scale (VIF) approximations combining the strengths of global inducing points and local Vecchia approximations. Vecchia approximations excel in settings with low-dimensional inputs and moderately smooth covariance functions, while inducing point methods are better suited to high-dimensional inputs and smoother covariance functions. Our VIF approach bridges these two regimes by using an efficient correlation-based neighbor-finding strategy for the Vecchia approximation of the residual process, implemented via a modified cover tree algorithm. We further extend our framework to non-Gaussian likelihoods by introducing iterative methods that substantially reduce computational costs for training and prediction by several orders of magnitudes compared to Cholesky-based computations when using a Laplace approximation. In particular, we propose and compare novel preconditioners and provide theoretical convergence results. Extensive numerical experiments on simulated and real-world data sets show that VIF approximations are both computationally efficient as well as more accurate and numerically stable than state-of-the-art alternatives. All methods are implemented in the open source C++ library GPBoost with high-level Python and R interfaces.", "link": "http://arxiv.org/abs/2507.05064v2", "date": "2026-01-29", "relevancy": 2.4345, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5098}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.477}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vecchia-Inducing-Points%20Full-Scale%20Approximations%20for%20Gaussian%20Processes&body=Title%3A%20Vecchia-Inducing-Points%20Full-Scale%20Approximations%20for%20Gaussian%20Processes%0AAuthor%3A%20Tim%20Gyger%20and%20Reinhard%20Furrer%20and%20Fabio%20Sigrist%0AAbstract%3A%20Gaussian%20processes%20are%20flexible%2C%20probabilistic%2C%20non-parametric%20models%20widely%20used%20in%20machine%20learning%20and%20statistics.%20However%2C%20their%20scalability%20to%20large%20data%20sets%20is%20limited%20by%20computational%20constraints.%20To%20overcome%20these%20challenges%2C%20we%20propose%20Vecchia-inducing-points%20full-scale%20%28VIF%29%20approximations%20combining%20the%20strengths%20of%20global%20inducing%20points%20and%20local%20Vecchia%20approximations.%20Vecchia%20approximations%20excel%20in%20settings%20with%20low-dimensional%20inputs%20and%20moderately%20smooth%20covariance%20functions%2C%20while%20inducing%20point%20methods%20are%20better%20suited%20to%20high-dimensional%20inputs%20and%20smoother%20covariance%20functions.%20Our%20VIF%20approach%20bridges%20these%20two%20regimes%20by%20using%20an%20efficient%20correlation-based%20neighbor-finding%20strategy%20for%20the%20Vecchia%20approximation%20of%20the%20residual%20process%2C%20implemented%20via%20a%20modified%20cover%20tree%20algorithm.%20We%20further%20extend%20our%20framework%20to%20non-Gaussian%20likelihoods%20by%20introducing%20iterative%20methods%20that%20substantially%20reduce%20computational%20costs%20for%20training%20and%20prediction%20by%20several%20orders%20of%20magnitudes%20compared%20to%20Cholesky-based%20computations%20when%20using%20a%20Laplace%20approximation.%20In%20particular%2C%20we%20propose%20and%20compare%20novel%20preconditioners%20and%20provide%20theoretical%20convergence%20results.%20Extensive%20numerical%20experiments%20on%20simulated%20and%20real-world%20data%20sets%20show%20that%20VIF%20approximations%20are%20both%20computationally%20efficient%20as%20well%20as%20more%20accurate%20and%20numerically%20stable%20than%20state-of-the-art%20alternatives.%20All%20methods%20are%20implemented%20in%20the%20open%20source%20C%2B%2B%20library%20GPBoost%20with%20high-level%20Python%20and%20R%20interfaces.%0ALink%3A%20http%3A//arxiv.org/abs/2507.05064v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVecchia-Inducing-Points%2520Full-Scale%2520Approximations%2520for%2520Gaussian%2520Processes%26entry.906535625%3DTim%2520Gyger%2520and%2520Reinhard%2520Furrer%2520and%2520Fabio%2520Sigrist%26entry.1292438233%3DGaussian%2520processes%2520are%2520flexible%252C%2520probabilistic%252C%2520non-parametric%2520models%2520widely%2520used%2520in%2520machine%2520learning%2520and%2520statistics.%2520However%252C%2520their%2520scalability%2520to%2520large%2520data%2520sets%2520is%2520limited%2520by%2520computational%2520constraints.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520Vecchia-inducing-points%2520full-scale%2520%2528VIF%2529%2520approximations%2520combining%2520the%2520strengths%2520of%2520global%2520inducing%2520points%2520and%2520local%2520Vecchia%2520approximations.%2520Vecchia%2520approximations%2520excel%2520in%2520settings%2520with%2520low-dimensional%2520inputs%2520and%2520moderately%2520smooth%2520covariance%2520functions%252C%2520while%2520inducing%2520point%2520methods%2520are%2520better%2520suited%2520to%2520high-dimensional%2520inputs%2520and%2520smoother%2520covariance%2520functions.%2520Our%2520VIF%2520approach%2520bridges%2520these%2520two%2520regimes%2520by%2520using%2520an%2520efficient%2520correlation-based%2520neighbor-finding%2520strategy%2520for%2520the%2520Vecchia%2520approximation%2520of%2520the%2520residual%2520process%252C%2520implemented%2520via%2520a%2520modified%2520cover%2520tree%2520algorithm.%2520We%2520further%2520extend%2520our%2520framework%2520to%2520non-Gaussian%2520likelihoods%2520by%2520introducing%2520iterative%2520methods%2520that%2520substantially%2520reduce%2520computational%2520costs%2520for%2520training%2520and%2520prediction%2520by%2520several%2520orders%2520of%2520magnitudes%2520compared%2520to%2520Cholesky-based%2520computations%2520when%2520using%2520a%2520Laplace%2520approximation.%2520In%2520particular%252C%2520we%2520propose%2520and%2520compare%2520novel%2520preconditioners%2520and%2520provide%2520theoretical%2520convergence%2520results.%2520Extensive%2520numerical%2520experiments%2520on%2520simulated%2520and%2520real-world%2520data%2520sets%2520show%2520that%2520VIF%2520approximations%2520are%2520both%2520computationally%2520efficient%2520as%2520well%2520as%2520more%2520accurate%2520and%2520numerically%2520stable%2520than%2520state-of-the-art%2520alternatives.%2520All%2520methods%2520are%2520implemented%2520in%2520the%2520open%2520source%2520C%252B%252B%2520library%2520GPBoost%2520with%2520high-level%2520Python%2520and%2520R%2520interfaces.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05064v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vecchia-Inducing-Points%20Full-Scale%20Approximations%20for%20Gaussian%20Processes&entry.906535625=Tim%20Gyger%20and%20Reinhard%20Furrer%20and%20Fabio%20Sigrist&entry.1292438233=Gaussian%20processes%20are%20flexible%2C%20probabilistic%2C%20non-parametric%20models%20widely%20used%20in%20machine%20learning%20and%20statistics.%20However%2C%20their%20scalability%20to%20large%20data%20sets%20is%20limited%20by%20computational%20constraints.%20To%20overcome%20these%20challenges%2C%20we%20propose%20Vecchia-inducing-points%20full-scale%20%28VIF%29%20approximations%20combining%20the%20strengths%20of%20global%20inducing%20points%20and%20local%20Vecchia%20approximations.%20Vecchia%20approximations%20excel%20in%20settings%20with%20low-dimensional%20inputs%20and%20moderately%20smooth%20covariance%20functions%2C%20while%20inducing%20point%20methods%20are%20better%20suited%20to%20high-dimensional%20inputs%20and%20smoother%20covariance%20functions.%20Our%20VIF%20approach%20bridges%20these%20two%20regimes%20by%20using%20an%20efficient%20correlation-based%20neighbor-finding%20strategy%20for%20the%20Vecchia%20approximation%20of%20the%20residual%20process%2C%20implemented%20via%20a%20modified%20cover%20tree%20algorithm.%20We%20further%20extend%20our%20framework%20to%20non-Gaussian%20likelihoods%20by%20introducing%20iterative%20methods%20that%20substantially%20reduce%20computational%20costs%20for%20training%20and%20prediction%20by%20several%20orders%20of%20magnitudes%20compared%20to%20Cholesky-based%20computations%20when%20using%20a%20Laplace%20approximation.%20In%20particular%2C%20we%20propose%20and%20compare%20novel%20preconditioners%20and%20provide%20theoretical%20convergence%20results.%20Extensive%20numerical%20experiments%20on%20simulated%20and%20real-world%20data%20sets%20show%20that%20VIF%20approximations%20are%20both%20computationally%20efficient%20as%20well%20as%20more%20accurate%20and%20numerically%20stable%20than%20state-of-the-art%20alternatives.%20All%20methods%20are%20implemented%20in%20the%20open%20source%20C%2B%2B%20library%20GPBoost%20with%20high-level%20Python%20and%20R%20interfaces.&entry.1838667208=http%3A//arxiv.org/abs/2507.05064v2&entry.124074799=Read"},
{"title": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models", "author": "Yufeng Zhong and Lei Chen and Xuanle Zhao and Wenkang Han and Liming Zheng and Jing Huang and Deyang Jiang and Yilin Cao and Lin Ma and Zhixiong Zeng", "abstract": "The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (\\textbf{Text-centric OCR}), neglecting the identification of visual elements from visually information-dense image sources (\\textbf{Vision-centric OCR}), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose \\textbf{OCRVerse}, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.", "link": "http://arxiv.org/abs/2601.21639v1", "date": "2026-01-29", "relevancy": 2.4267, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6165}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6165}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OCRVerse%3A%20Towards%20Holistic%20OCR%20in%20End-to-End%20Vision-Language%20Models&body=Title%3A%20OCRVerse%3A%20Towards%20Holistic%20OCR%20in%20End-to-End%20Vision-Language%20Models%0AAuthor%3A%20Yufeng%20Zhong%20and%20Lei%20Chen%20and%20Xuanle%20Zhao%20and%20Wenkang%20Han%20and%20Liming%20Zheng%20and%20Jing%20Huang%20and%20Deyang%20Jiang%20and%20Yilin%20Cao%20and%20Lin%20Ma%20and%20Zhixiong%20Zeng%0AAbstract%3A%20The%20development%20of%20large%20vision%20language%20models%20drives%20the%20demand%20for%20managing%2C%20and%20applying%20massive%20amounts%20of%20multimodal%20data%2C%20making%20OCR%20technology%2C%20which%20extracts%20information%20from%20visual%20images%2C%20increasingly%20popular.%20However%2C%20existing%20OCR%20methods%20primarily%20focus%20on%20recognizing%20text%20elements%20from%20images%20or%20scanned%20documents%20%28%5Ctextbf%7BText-centric%20OCR%7D%29%2C%20neglecting%20the%20identification%20of%20visual%20elements%20from%20visually%20information-dense%20image%20sources%20%28%5Ctextbf%7BVision-centric%20OCR%7D%29%2C%20such%20as%20charts%2C%20web%20pages%20and%20science%20plots.%20In%20reality%2C%20these%20visually%20information-dense%20images%20are%20widespread%20on%20the%20internet%20and%20have%20significant%20real-world%20application%20value%2C%20such%20as%20data%20visualization%20and%20web%20page%20analysis.%20In%20this%20technical%20report%2C%20we%20propose%20%5Ctextbf%7BOCRVerse%7D%2C%20the%20first%20holistic%20OCR%20method%20in%20end-to-end%20manner%20that%20enables%20unified%20text-centric%20OCR%20and%20vision-centric%20OCR.%20To%20this%20end%2C%20we%20constructe%20comprehensive%20data%20engineering%20to%20cover%20a%20wide%20range%20of%20text-centric%20documents%2C%20such%20as%20newspapers%2C%20magazines%20and%20books%2C%20as%20well%20as%20vision-centric%20rendered%20composites%2C%20including%20charts%2C%20web%20pages%20and%20scientific%20plots.%20Moreover%2C%20we%20propose%20a%20two-stage%20SFT-RL%20multi-domain%20training%20method%20for%20OCRVerse.%20SFT%20directly%20mixes%20cross-domain%20data%20to%20train%20and%20establish%20initial%20domain%20knowledge%2C%20while%20RL%20focuses%20on%20designing%20personalized%20reward%20strategies%20for%20the%20characteristics%20of%20each%20domain.%20Specifically%2C%20since%20different%20domains%20require%20various%20output%20formats%20and%20expected%20outputs%2C%20we%20provide%20sufficient%20flexibility%20in%20the%20RL%20stage%20to%20customize%20flexible%20reward%20signals%20for%20each%20domain%2C%20thereby%20improving%20cross-domain%20fusion%20and%20avoiding%20data%20conflicts.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20OCRVerse%2C%20achieving%20competitive%20results%20across%20text-centric%20and%20vision-centric%20data%20types%2C%20even%20comparable%20to%20large-scale%20open-source%20and%20closed-source%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21639v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOCRVerse%253A%2520Towards%2520Holistic%2520OCR%2520in%2520End-to-End%2520Vision-Language%2520Models%26entry.906535625%3DYufeng%2520Zhong%2520and%2520Lei%2520Chen%2520and%2520Xuanle%2520Zhao%2520and%2520Wenkang%2520Han%2520and%2520Liming%2520Zheng%2520and%2520Jing%2520Huang%2520and%2520Deyang%2520Jiang%2520and%2520Yilin%2520Cao%2520and%2520Lin%2520Ma%2520and%2520Zhixiong%2520Zeng%26entry.1292438233%3DThe%2520development%2520of%2520large%2520vision%2520language%2520models%2520drives%2520the%2520demand%2520for%2520managing%252C%2520and%2520applying%2520massive%2520amounts%2520of%2520multimodal%2520data%252C%2520making%2520OCR%2520technology%252C%2520which%2520extracts%2520information%2520from%2520visual%2520images%252C%2520increasingly%2520popular.%2520However%252C%2520existing%2520OCR%2520methods%2520primarily%2520focus%2520on%2520recognizing%2520text%2520elements%2520from%2520images%2520or%2520scanned%2520documents%2520%2528%255Ctextbf%257BText-centric%2520OCR%257D%2529%252C%2520neglecting%2520the%2520identification%2520of%2520visual%2520elements%2520from%2520visually%2520information-dense%2520image%2520sources%2520%2528%255Ctextbf%257BVision-centric%2520OCR%257D%2529%252C%2520such%2520as%2520charts%252C%2520web%2520pages%2520and%2520science%2520plots.%2520In%2520reality%252C%2520these%2520visually%2520information-dense%2520images%2520are%2520widespread%2520on%2520the%2520internet%2520and%2520have%2520significant%2520real-world%2520application%2520value%252C%2520such%2520as%2520data%2520visualization%2520and%2520web%2520page%2520analysis.%2520In%2520this%2520technical%2520report%252C%2520we%2520propose%2520%255Ctextbf%257BOCRVerse%257D%252C%2520the%2520first%2520holistic%2520OCR%2520method%2520in%2520end-to-end%2520manner%2520that%2520enables%2520unified%2520text-centric%2520OCR%2520and%2520vision-centric%2520OCR.%2520To%2520this%2520end%252C%2520we%2520constructe%2520comprehensive%2520data%2520engineering%2520to%2520cover%2520a%2520wide%2520range%2520of%2520text-centric%2520documents%252C%2520such%2520as%2520newspapers%252C%2520magazines%2520and%2520books%252C%2520as%2520well%2520as%2520vision-centric%2520rendered%2520composites%252C%2520including%2520charts%252C%2520web%2520pages%2520and%2520scientific%2520plots.%2520Moreover%252C%2520we%2520propose%2520a%2520two-stage%2520SFT-RL%2520multi-domain%2520training%2520method%2520for%2520OCRVerse.%2520SFT%2520directly%2520mixes%2520cross-domain%2520data%2520to%2520train%2520and%2520establish%2520initial%2520domain%2520knowledge%252C%2520while%2520RL%2520focuses%2520on%2520designing%2520personalized%2520reward%2520strategies%2520for%2520the%2520characteristics%2520of%2520each%2520domain.%2520Specifically%252C%2520since%2520different%2520domains%2520require%2520various%2520output%2520formats%2520and%2520expected%2520outputs%252C%2520we%2520provide%2520sufficient%2520flexibility%2520in%2520the%2520RL%2520stage%2520to%2520customize%2520flexible%2520reward%2520signals%2520for%2520each%2520domain%252C%2520thereby%2520improving%2520cross-domain%2520fusion%2520and%2520avoiding%2520data%2520conflicts.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520OCRVerse%252C%2520achieving%2520competitive%2520results%2520across%2520text-centric%2520and%2520vision-centric%2520data%2520types%252C%2520even%2520comparable%2520to%2520large-scale%2520open-source%2520and%2520closed-source%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21639v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OCRVerse%3A%20Towards%20Holistic%20OCR%20in%20End-to-End%20Vision-Language%20Models&entry.906535625=Yufeng%20Zhong%20and%20Lei%20Chen%20and%20Xuanle%20Zhao%20and%20Wenkang%20Han%20and%20Liming%20Zheng%20and%20Jing%20Huang%20and%20Deyang%20Jiang%20and%20Yilin%20Cao%20and%20Lin%20Ma%20and%20Zhixiong%20Zeng&entry.1292438233=The%20development%20of%20large%20vision%20language%20models%20drives%20the%20demand%20for%20managing%2C%20and%20applying%20massive%20amounts%20of%20multimodal%20data%2C%20making%20OCR%20technology%2C%20which%20extracts%20information%20from%20visual%20images%2C%20increasingly%20popular.%20However%2C%20existing%20OCR%20methods%20primarily%20focus%20on%20recognizing%20text%20elements%20from%20images%20or%20scanned%20documents%20%28%5Ctextbf%7BText-centric%20OCR%7D%29%2C%20neglecting%20the%20identification%20of%20visual%20elements%20from%20visually%20information-dense%20image%20sources%20%28%5Ctextbf%7BVision-centric%20OCR%7D%29%2C%20such%20as%20charts%2C%20web%20pages%20and%20science%20plots.%20In%20reality%2C%20these%20visually%20information-dense%20images%20are%20widespread%20on%20the%20internet%20and%20have%20significant%20real-world%20application%20value%2C%20such%20as%20data%20visualization%20and%20web%20page%20analysis.%20In%20this%20technical%20report%2C%20we%20propose%20%5Ctextbf%7BOCRVerse%7D%2C%20the%20first%20holistic%20OCR%20method%20in%20end-to-end%20manner%20that%20enables%20unified%20text-centric%20OCR%20and%20vision-centric%20OCR.%20To%20this%20end%2C%20we%20constructe%20comprehensive%20data%20engineering%20to%20cover%20a%20wide%20range%20of%20text-centric%20documents%2C%20such%20as%20newspapers%2C%20magazines%20and%20books%2C%20as%20well%20as%20vision-centric%20rendered%20composites%2C%20including%20charts%2C%20web%20pages%20and%20scientific%20plots.%20Moreover%2C%20we%20propose%20a%20two-stage%20SFT-RL%20multi-domain%20training%20method%20for%20OCRVerse.%20SFT%20directly%20mixes%20cross-domain%20data%20to%20train%20and%20establish%20initial%20domain%20knowledge%2C%20while%20RL%20focuses%20on%20designing%20personalized%20reward%20strategies%20for%20the%20characteristics%20of%20each%20domain.%20Specifically%2C%20since%20different%20domains%20require%20various%20output%20formats%20and%20expected%20outputs%2C%20we%20provide%20sufficient%20flexibility%20in%20the%20RL%20stage%20to%20customize%20flexible%20reward%20signals%20for%20each%20domain%2C%20thereby%20improving%20cross-domain%20fusion%20and%20avoiding%20data%20conflicts.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20OCRVerse%2C%20achieving%20competitive%20results%20across%20text-centric%20and%20vision-centric%20data%20types%2C%20even%20comparable%20to%20large-scale%20open-source%20and%20closed-source%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.21639v1&entry.124074799=Read"},
{"title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models", "author": "Wenxuan Huang and Yu Zeng and Qiuchen Wang and Zhen Fang and Shaosheng Cao and Zheng Chu and Qingyu Yin and Shuang Chen and Zhenfei Yin and Lin Chen and Zehui Chen and Yao Hu and Philip Torr and Feng Zhao and Wanli Ouyang", "abstract": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", "link": "http://arxiv.org/abs/2601.22060v1", "date": "2026-01-29", "relevancy": 2.4229, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6121}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-DeepResearch%3A%20Incentivizing%20DeepResearch%20Capability%20in%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Vision-DeepResearch%3A%20Incentivizing%20DeepResearch%20Capability%20in%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Wenxuan%20Huang%20and%20Yu%20Zeng%20and%20Qiuchen%20Wang%20and%20Zhen%20Fang%20and%20Shaosheng%20Cao%20and%20Zheng%20Chu%20and%20Qingyu%20Yin%20and%20Shuang%20Chen%20and%20Zhenfei%20Yin%20and%20Lin%20Chen%20and%20Zehui%20Chen%20and%20Yao%20Hu%20and%20Philip%20Torr%20and%20Feng%20Zhao%20and%20Wanli%20Ouyang%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20achieved%20remarkable%20success%20across%20a%20broad%20range%20of%20vision%20tasks.%20However%2C%20constrained%20by%20the%20capacity%20of%20their%20internal%20world%20knowledge%2C%20prior%20work%20has%20proposed%20augmenting%20MLLMs%20by%20%60%60reasoning-then-tool-call%27%27%20for%20visual%20and%20textual%20search%20engines%20to%20obtain%20substantial%20gains%20on%20tasks%20requiring%20extensive%20factual%20information.%20However%2C%20these%20approaches%20typically%20define%20multimodal%20search%20in%20a%20naive%20setting%2C%20assuming%20that%20a%20single%20full-level%20or%20entity-level%20image%20query%20and%20few%20text%20query%20suffices%20to%20retrieve%20the%20key%20evidence%20needed%20to%20answer%20the%20question%2C%20which%20is%20unrealistic%20in%20real-world%20scenarios%20with%20substantial%20visual%20noise.%20Moreover%2C%20they%20are%20often%20limited%20in%20the%20reasoning%20depth%20and%20search%20breadth%2C%20making%20it%20difficult%20to%20solve%20complex%20questions%20that%20require%20aggregating%20evidence%20from%20diverse%20visual%20and%20textual%20sources.%20Building%20on%20this%2C%20we%20propose%20Vision-DeepResearch%2C%20which%20proposes%20one%20new%20multimodal%20deep-research%20paradigm%2C%20i.e.%2C%20performs%20multi-turn%2C%20multi-entity%20and%20multi-scale%20visual%20and%20textual%20search%20to%20robustly%20hit%20real-world%20search%20engines%20under%20heavy%20noise.%20Our%20Vision-DeepResearch%20supports%20dozens%20of%20reasoning%20steps%20and%20hundreds%20of%20engine%20interactions%2C%20while%20internalizing%20deep-research%20capabilities%20into%20the%20MLLM%20via%20cold-start%20supervision%20and%20RL%20training%2C%20resulting%20in%20a%20strong%20end-to-end%20multimodal%20deep-research%20MLLM.%20It%20substantially%20outperforming%20existing%20multimodal%20deep-research%20MLLMs%2C%20and%20workflows%20built%20on%20strong%20closed-source%20foundation%20model%20such%20as%20GPT-5%2C%20Gemini-2.5-pro%20and%20Claude-4-Sonnet.%20The%20code%20will%20be%20released%20in%20https%3A//github.com/Osilly/Vision-DeepResearch.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-DeepResearch%253A%2520Incentivizing%2520DeepResearch%2520Capability%2520in%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DWenxuan%2520Huang%2520and%2520Yu%2520Zeng%2520and%2520Qiuchen%2520Wang%2520and%2520Zhen%2520Fang%2520and%2520Shaosheng%2520Cao%2520and%2520Zheng%2520Chu%2520and%2520Qingyu%2520Yin%2520and%2520Shuang%2520Chen%2520and%2520Zhenfei%2520Yin%2520and%2520Lin%2520Chen%2520and%2520Zehui%2520Chen%2520and%2520Yao%2520Hu%2520and%2520Philip%2520Torr%2520and%2520Feng%2520Zhao%2520and%2520Wanli%2520Ouyang%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520across%2520a%2520broad%2520range%2520of%2520vision%2520tasks.%2520However%252C%2520constrained%2520by%2520the%2520capacity%2520of%2520their%2520internal%2520world%2520knowledge%252C%2520prior%2520work%2520has%2520proposed%2520augmenting%2520MLLMs%2520by%2520%2560%2560reasoning-then-tool-call%2527%2527%2520for%2520visual%2520and%2520textual%2520search%2520engines%2520to%2520obtain%2520substantial%2520gains%2520on%2520tasks%2520requiring%2520extensive%2520factual%2520information.%2520However%252C%2520these%2520approaches%2520typically%2520define%2520multimodal%2520search%2520in%2520a%2520naive%2520setting%252C%2520assuming%2520that%2520a%2520single%2520full-level%2520or%2520entity-level%2520image%2520query%2520and%2520few%2520text%2520query%2520suffices%2520to%2520retrieve%2520the%2520key%2520evidence%2520needed%2520to%2520answer%2520the%2520question%252C%2520which%2520is%2520unrealistic%2520in%2520real-world%2520scenarios%2520with%2520substantial%2520visual%2520noise.%2520Moreover%252C%2520they%2520are%2520often%2520limited%2520in%2520the%2520reasoning%2520depth%2520and%2520search%2520breadth%252C%2520making%2520it%2520difficult%2520to%2520solve%2520complex%2520questions%2520that%2520require%2520aggregating%2520evidence%2520from%2520diverse%2520visual%2520and%2520textual%2520sources.%2520Building%2520on%2520this%252C%2520we%2520propose%2520Vision-DeepResearch%252C%2520which%2520proposes%2520one%2520new%2520multimodal%2520deep-research%2520paradigm%252C%2520i.e.%252C%2520performs%2520multi-turn%252C%2520multi-entity%2520and%2520multi-scale%2520visual%2520and%2520textual%2520search%2520to%2520robustly%2520hit%2520real-world%2520search%2520engines%2520under%2520heavy%2520noise.%2520Our%2520Vision-DeepResearch%2520supports%2520dozens%2520of%2520reasoning%2520steps%2520and%2520hundreds%2520of%2520engine%2520interactions%252C%2520while%2520internalizing%2520deep-research%2520capabilities%2520into%2520the%2520MLLM%2520via%2520cold-start%2520supervision%2520and%2520RL%2520training%252C%2520resulting%2520in%2520a%2520strong%2520end-to-end%2520multimodal%2520deep-research%2520MLLM.%2520It%2520substantially%2520outperforming%2520existing%2520multimodal%2520deep-research%2520MLLMs%252C%2520and%2520workflows%2520built%2520on%2520strong%2520closed-source%2520foundation%2520model%2520such%2520as%2520GPT-5%252C%2520Gemini-2.5-pro%2520and%2520Claude-4-Sonnet.%2520The%2520code%2520will%2520be%2520released%2520in%2520https%253A//github.com/Osilly/Vision-DeepResearch.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-DeepResearch%3A%20Incentivizing%20DeepResearch%20Capability%20in%20Multimodal%20Large%20Language%20Models&entry.906535625=Wenxuan%20Huang%20and%20Yu%20Zeng%20and%20Qiuchen%20Wang%20and%20Zhen%20Fang%20and%20Shaosheng%20Cao%20and%20Zheng%20Chu%20and%20Qingyu%20Yin%20and%20Shuang%20Chen%20and%20Zhenfei%20Yin%20and%20Lin%20Chen%20and%20Zehui%20Chen%20and%20Yao%20Hu%20and%20Philip%20Torr%20and%20Feng%20Zhao%20and%20Wanli%20Ouyang&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20achieved%20remarkable%20success%20across%20a%20broad%20range%20of%20vision%20tasks.%20However%2C%20constrained%20by%20the%20capacity%20of%20their%20internal%20world%20knowledge%2C%20prior%20work%20has%20proposed%20augmenting%20MLLMs%20by%20%60%60reasoning-then-tool-call%27%27%20for%20visual%20and%20textual%20search%20engines%20to%20obtain%20substantial%20gains%20on%20tasks%20requiring%20extensive%20factual%20information.%20However%2C%20these%20approaches%20typically%20define%20multimodal%20search%20in%20a%20naive%20setting%2C%20assuming%20that%20a%20single%20full-level%20or%20entity-level%20image%20query%20and%20few%20text%20query%20suffices%20to%20retrieve%20the%20key%20evidence%20needed%20to%20answer%20the%20question%2C%20which%20is%20unrealistic%20in%20real-world%20scenarios%20with%20substantial%20visual%20noise.%20Moreover%2C%20they%20are%20often%20limited%20in%20the%20reasoning%20depth%20and%20search%20breadth%2C%20making%20it%20difficult%20to%20solve%20complex%20questions%20that%20require%20aggregating%20evidence%20from%20diverse%20visual%20and%20textual%20sources.%20Building%20on%20this%2C%20we%20propose%20Vision-DeepResearch%2C%20which%20proposes%20one%20new%20multimodal%20deep-research%20paradigm%2C%20i.e.%2C%20performs%20multi-turn%2C%20multi-entity%20and%20multi-scale%20visual%20and%20textual%20search%20to%20robustly%20hit%20real-world%20search%20engines%20under%20heavy%20noise.%20Our%20Vision-DeepResearch%20supports%20dozens%20of%20reasoning%20steps%20and%20hundreds%20of%20engine%20interactions%2C%20while%20internalizing%20deep-research%20capabilities%20into%20the%20MLLM%20via%20cold-start%20supervision%20and%20RL%20training%2C%20resulting%20in%20a%20strong%20end-to-end%20multimodal%20deep-research%20MLLM.%20It%20substantially%20outperforming%20existing%20multimodal%20deep-research%20MLLMs%2C%20and%20workflows%20built%20on%20strong%20closed-source%20foundation%20model%20such%20as%20GPT-5%2C%20Gemini-2.5-pro%20and%20Claude-4-Sonnet.%20The%20code%20will%20be%20released%20in%20https%3A//github.com/Osilly/Vision-DeepResearch.&entry.1838667208=http%3A//arxiv.org/abs/2601.22060v1&entry.124074799=Read"},
{"title": "Physics-Aware Heterogeneous GNN Architecture for Real-Time BESS Optimization in Unbalanced Distribution Systems", "author": "Aoxiang Ma and Salah Ghamizi and Jun Cao and Pedro Rodriguez", "abstract": "Battery energy storage systems (BESS) have become increasingly vital in three-phase unbalanced distribution grids for maintaining voltage stability and enabling optimal dispatch. However, existing deep learning approaches often lack explicit three-phase representation, making it difficult to accurately model phase-specific dynamics and enforce operational constraints--leading to infeasible dispatch solutions. This paper demonstrates that by embedding detailed three-phase grid information--including phase voltages, unbalanced loads, and BESS states--into heterogeneous graph nodes, diverse GNN architectures (GCN, GAT, GraphSAGE, GPS) can jointly predict network state variables with high accuracy. Moreover, a physics-informed loss function incorporates critical battery constraints--SoC and C-rate limits--via soft penalties during training. Experimental validation on the CIGRE 18-bus distribution system shows that this embedding-loss approach achieves low prediction errors, with bus voltage MSEs of 6.92e-07 (GCN), 1.21e-06 (GAT), 3.29e-05 (GPS), and 9.04e-07 (SAGE). Importantly, the physics-informed method ensures nearly zero SoC and C-rate constraint violations, confirming its effectiveness for reliable, constraint-compliant dispatch.", "link": "http://arxiv.org/abs/2512.09780v2", "date": "2026-01-29", "relevancy": 2.4204, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5437}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.458}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Aware%20Heterogeneous%20GNN%20Architecture%20for%20Real-Time%20BESS%20Optimization%20in%20Unbalanced%20Distribution%20Systems&body=Title%3A%20Physics-Aware%20Heterogeneous%20GNN%20Architecture%20for%20Real-Time%20BESS%20Optimization%20in%20Unbalanced%20Distribution%20Systems%0AAuthor%3A%20Aoxiang%20Ma%20and%20Salah%20Ghamizi%20and%20Jun%20Cao%20and%20Pedro%20Rodriguez%0AAbstract%3A%20Battery%20energy%20storage%20systems%20%28BESS%29%20have%20become%20increasingly%20vital%20in%20three-phase%20unbalanced%20distribution%20grids%20for%20maintaining%20voltage%20stability%20and%20enabling%20optimal%20dispatch.%20However%2C%20existing%20deep%20learning%20approaches%20often%20lack%20explicit%20three-phase%20representation%2C%20making%20it%20difficult%20to%20accurately%20model%20phase-specific%20dynamics%20and%20enforce%20operational%20constraints--leading%20to%20infeasible%20dispatch%20solutions.%20This%20paper%20demonstrates%20that%20by%20embedding%20detailed%20three-phase%20grid%20information--including%20phase%20voltages%2C%20unbalanced%20loads%2C%20and%20BESS%20states--into%20heterogeneous%20graph%20nodes%2C%20diverse%20GNN%20architectures%20%28GCN%2C%20GAT%2C%20GraphSAGE%2C%20GPS%29%20can%20jointly%20predict%20network%20state%20variables%20with%20high%20accuracy.%20Moreover%2C%20a%20physics-informed%20loss%20function%20incorporates%20critical%20battery%20constraints--SoC%20and%20C-rate%20limits--via%20soft%20penalties%20during%20training.%20Experimental%20validation%20on%20the%20CIGRE%2018-bus%20distribution%20system%20shows%20that%20this%20embedding-loss%20approach%20achieves%20low%20prediction%20errors%2C%20with%20bus%20voltage%20MSEs%20of%206.92e-07%20%28GCN%29%2C%201.21e-06%20%28GAT%29%2C%203.29e-05%20%28GPS%29%2C%20and%209.04e-07%20%28SAGE%29.%20Importantly%2C%20the%20physics-informed%20method%20ensures%20nearly%20zero%20SoC%20and%20C-rate%20constraint%20violations%2C%20confirming%20its%20effectiveness%20for%20reliable%2C%20constraint-compliant%20dispatch.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09780v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Aware%2520Heterogeneous%2520GNN%2520Architecture%2520for%2520Real-Time%2520BESS%2520Optimization%2520in%2520Unbalanced%2520Distribution%2520Systems%26entry.906535625%3DAoxiang%2520Ma%2520and%2520Salah%2520Ghamizi%2520and%2520Jun%2520Cao%2520and%2520Pedro%2520Rodriguez%26entry.1292438233%3DBattery%2520energy%2520storage%2520systems%2520%2528BESS%2529%2520have%2520become%2520increasingly%2520vital%2520in%2520three-phase%2520unbalanced%2520distribution%2520grids%2520for%2520maintaining%2520voltage%2520stability%2520and%2520enabling%2520optimal%2520dispatch.%2520However%252C%2520existing%2520deep%2520learning%2520approaches%2520often%2520lack%2520explicit%2520three-phase%2520representation%252C%2520making%2520it%2520difficult%2520to%2520accurately%2520model%2520phase-specific%2520dynamics%2520and%2520enforce%2520operational%2520constraints--leading%2520to%2520infeasible%2520dispatch%2520solutions.%2520This%2520paper%2520demonstrates%2520that%2520by%2520embedding%2520detailed%2520three-phase%2520grid%2520information--including%2520phase%2520voltages%252C%2520unbalanced%2520loads%252C%2520and%2520BESS%2520states--into%2520heterogeneous%2520graph%2520nodes%252C%2520diverse%2520GNN%2520architectures%2520%2528GCN%252C%2520GAT%252C%2520GraphSAGE%252C%2520GPS%2529%2520can%2520jointly%2520predict%2520network%2520state%2520variables%2520with%2520high%2520accuracy.%2520Moreover%252C%2520a%2520physics-informed%2520loss%2520function%2520incorporates%2520critical%2520battery%2520constraints--SoC%2520and%2520C-rate%2520limits--via%2520soft%2520penalties%2520during%2520training.%2520Experimental%2520validation%2520on%2520the%2520CIGRE%252018-bus%2520distribution%2520system%2520shows%2520that%2520this%2520embedding-loss%2520approach%2520achieves%2520low%2520prediction%2520errors%252C%2520with%2520bus%2520voltage%2520MSEs%2520of%25206.92e-07%2520%2528GCN%2529%252C%25201.21e-06%2520%2528GAT%2529%252C%25203.29e-05%2520%2528GPS%2529%252C%2520and%25209.04e-07%2520%2528SAGE%2529.%2520Importantly%252C%2520the%2520physics-informed%2520method%2520ensures%2520nearly%2520zero%2520SoC%2520and%2520C-rate%2520constraint%2520violations%252C%2520confirming%2520its%2520effectiveness%2520for%2520reliable%252C%2520constraint-compliant%2520dispatch.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09780v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Aware%20Heterogeneous%20GNN%20Architecture%20for%20Real-Time%20BESS%20Optimization%20in%20Unbalanced%20Distribution%20Systems&entry.906535625=Aoxiang%20Ma%20and%20Salah%20Ghamizi%20and%20Jun%20Cao%20and%20Pedro%20Rodriguez&entry.1292438233=Battery%20energy%20storage%20systems%20%28BESS%29%20have%20become%20increasingly%20vital%20in%20three-phase%20unbalanced%20distribution%20grids%20for%20maintaining%20voltage%20stability%20and%20enabling%20optimal%20dispatch.%20However%2C%20existing%20deep%20learning%20approaches%20often%20lack%20explicit%20three-phase%20representation%2C%20making%20it%20difficult%20to%20accurately%20model%20phase-specific%20dynamics%20and%20enforce%20operational%20constraints--leading%20to%20infeasible%20dispatch%20solutions.%20This%20paper%20demonstrates%20that%20by%20embedding%20detailed%20three-phase%20grid%20information--including%20phase%20voltages%2C%20unbalanced%20loads%2C%20and%20BESS%20states--into%20heterogeneous%20graph%20nodes%2C%20diverse%20GNN%20architectures%20%28GCN%2C%20GAT%2C%20GraphSAGE%2C%20GPS%29%20can%20jointly%20predict%20network%20state%20variables%20with%20high%20accuracy.%20Moreover%2C%20a%20physics-informed%20loss%20function%20incorporates%20critical%20battery%20constraints--SoC%20and%20C-rate%20limits--via%20soft%20penalties%20during%20training.%20Experimental%20validation%20on%20the%20CIGRE%2018-bus%20distribution%20system%20shows%20that%20this%20embedding-loss%20approach%20achieves%20low%20prediction%20errors%2C%20with%20bus%20voltage%20MSEs%20of%206.92e-07%20%28GCN%29%2C%201.21e-06%20%28GAT%29%2C%203.29e-05%20%28GPS%29%2C%20and%209.04e-07%20%28SAGE%29.%20Importantly%2C%20the%20physics-informed%20method%20ensures%20nearly%20zero%20SoC%20and%20C-rate%20constraint%20violations%2C%20confirming%20its%20effectiveness%20for%20reliable%2C%20constraint-compliant%20dispatch.&entry.1838667208=http%3A//arxiv.org/abs/2512.09780v2&entry.124074799=Read"},
{"title": "Training Memory in Deep Neural Networks: Mechanisms, Evidence, and Measurement Gaps", "author": "Vasileios Sevetlidis and George Pavlidis", "abstract": "Modern deep-learning training is not memoryless. Updates depend on optimizer moments and averaging, data-order policies (random reshuffling vs with-replacement, staged augmentations and replay), the nonconvex path, and auxiliary state (teacher EMA/SWA, contrastive queues, BatchNorm statistics). This survey organizes mechanisms by source, lifetime, and visibility. It introduces seed-paired, function-space causal estimands; portable perturbation primitives (carry/reset of momentum/Adam/EMA/BN, order-window swaps, queue/teacher tweaks); and a reporting checklist with audit artifacts (order hashes, buffer/BN checksums, RNG contracts). The conclusion is a protocol for portable, causal, uncertainty-aware measurement that attributes how much training history matters across models, data, and regimes.", "link": "http://arxiv.org/abs/2601.21624v1", "date": "2026-01-29", "relevancy": 2.4195, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4932}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4914}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Memory%20in%20Deep%20Neural%20Networks%3A%20Mechanisms%2C%20Evidence%2C%20and%20Measurement%20Gaps&body=Title%3A%20Training%20Memory%20in%20Deep%20Neural%20Networks%3A%20Mechanisms%2C%20Evidence%2C%20and%20Measurement%20Gaps%0AAuthor%3A%20Vasileios%20Sevetlidis%20and%20George%20Pavlidis%0AAbstract%3A%20Modern%20deep-learning%20training%20is%20not%20memoryless.%20Updates%20depend%20on%20optimizer%20moments%20and%20averaging%2C%20data-order%20policies%20%28random%20reshuffling%20vs%20with-replacement%2C%20staged%20augmentations%20and%20replay%29%2C%20the%20nonconvex%20path%2C%20and%20auxiliary%20state%20%28teacher%20EMA/SWA%2C%20contrastive%20queues%2C%20BatchNorm%20statistics%29.%20This%20survey%20organizes%20mechanisms%20by%20source%2C%20lifetime%2C%20and%20visibility.%20It%20introduces%20seed-paired%2C%20function-space%20causal%20estimands%3B%20portable%20perturbation%20primitives%20%28carry/reset%20of%20momentum/Adam/EMA/BN%2C%20order-window%20swaps%2C%20queue/teacher%20tweaks%29%3B%20and%20a%20reporting%20checklist%20with%20audit%20artifacts%20%28order%20hashes%2C%20buffer/BN%20checksums%2C%20RNG%20contracts%29.%20The%20conclusion%20is%20a%20protocol%20for%20portable%2C%20causal%2C%20uncertainty-aware%20measurement%20that%20attributes%20how%20much%20training%20history%20matters%20across%20models%2C%20data%2C%20and%20regimes.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Memory%2520in%2520Deep%2520Neural%2520Networks%253A%2520Mechanisms%252C%2520Evidence%252C%2520and%2520Measurement%2520Gaps%26entry.906535625%3DVasileios%2520Sevetlidis%2520and%2520George%2520Pavlidis%26entry.1292438233%3DModern%2520deep-learning%2520training%2520is%2520not%2520memoryless.%2520Updates%2520depend%2520on%2520optimizer%2520moments%2520and%2520averaging%252C%2520data-order%2520policies%2520%2528random%2520reshuffling%2520vs%2520with-replacement%252C%2520staged%2520augmentations%2520and%2520replay%2529%252C%2520the%2520nonconvex%2520path%252C%2520and%2520auxiliary%2520state%2520%2528teacher%2520EMA/SWA%252C%2520contrastive%2520queues%252C%2520BatchNorm%2520statistics%2529.%2520This%2520survey%2520organizes%2520mechanisms%2520by%2520source%252C%2520lifetime%252C%2520and%2520visibility.%2520It%2520introduces%2520seed-paired%252C%2520function-space%2520causal%2520estimands%253B%2520portable%2520perturbation%2520primitives%2520%2528carry/reset%2520of%2520momentum/Adam/EMA/BN%252C%2520order-window%2520swaps%252C%2520queue/teacher%2520tweaks%2529%253B%2520and%2520a%2520reporting%2520checklist%2520with%2520audit%2520artifacts%2520%2528order%2520hashes%252C%2520buffer/BN%2520checksums%252C%2520RNG%2520contracts%2529.%2520The%2520conclusion%2520is%2520a%2520protocol%2520for%2520portable%252C%2520causal%252C%2520uncertainty-aware%2520measurement%2520that%2520attributes%2520how%2520much%2520training%2520history%2520matters%2520across%2520models%252C%2520data%252C%2520and%2520regimes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Memory%20in%20Deep%20Neural%20Networks%3A%20Mechanisms%2C%20Evidence%2C%20and%20Measurement%20Gaps&entry.906535625=Vasileios%20Sevetlidis%20and%20George%20Pavlidis&entry.1292438233=Modern%20deep-learning%20training%20is%20not%20memoryless.%20Updates%20depend%20on%20optimizer%20moments%20and%20averaging%2C%20data-order%20policies%20%28random%20reshuffling%20vs%20with-replacement%2C%20staged%20augmentations%20and%20replay%29%2C%20the%20nonconvex%20path%2C%20and%20auxiliary%20state%20%28teacher%20EMA/SWA%2C%20contrastive%20queues%2C%20BatchNorm%20statistics%29.%20This%20survey%20organizes%20mechanisms%20by%20source%2C%20lifetime%2C%20and%20visibility.%20It%20introduces%20seed-paired%2C%20function-space%20causal%20estimands%3B%20portable%20perturbation%20primitives%20%28carry/reset%20of%20momentum/Adam/EMA/BN%2C%20order-window%20swaps%2C%20queue/teacher%20tweaks%29%3B%20and%20a%20reporting%20checklist%20with%20audit%20artifacts%20%28order%20hashes%2C%20buffer/BN%20checksums%2C%20RNG%20contracts%29.%20The%20conclusion%20is%20a%20protocol%20for%20portable%2C%20causal%2C%20uncertainty-aware%20measurement%20that%20attributes%20how%20much%20training%20history%20matters%20across%20models%2C%20data%2C%20and%20regimes.&entry.1838667208=http%3A//arxiv.org/abs/2601.21624v1&entry.124074799=Read"},
{"title": "iPEAR: Iterative Pyramid Estimation with Attention and Residuals for Deformable Medical Image Registration", "author": "Heming Wu and Di Wang and Tai Ma and Peng Zhao and Yubin Xiao and Zhongke Wu and Xing-Ce Wang and Xuan Wu and You Zhou", "abstract": "Existing pyramid registration networks may accumulate anatomical misalignments and lack an effective mechanism to dynamically determine the number of optimization iterations under varying deformation requirements across images, leading to degraded performance. To solve these limitations, we propose iPEAR. Specifically, iPEAR adopts our proposed Fused Attention-Residual Module (FARM) for decoding, which comprises an attention pathway and a residual pathway to alleviate the accumulation of anatomical misalignment. We further propose a dual-stage Threshold-Controlled Iterative (TCI) strategy that adaptively determines the number of optimization iterations for varying images by evaluating registration stability and convergence. Extensive experiments on three public brain MRI datasets and one public abdomen CT dataset show that iPEAR outperforms state-of-the-art (SOTA) registration networks in terms of accuracy, while achieving on-par inference speed and model parameter size. Generalization and ablation studies further validate the effectiveness of the proposed FARM and TCI.", "link": "http://arxiv.org/abs/2510.07666v2", "date": "2026-01-29", "relevancy": 2.4155, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4875}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4824}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iPEAR%3A%20Iterative%20Pyramid%20Estimation%20with%20Attention%20and%20Residuals%20for%20Deformable%20Medical%20Image%20Registration&body=Title%3A%20iPEAR%3A%20Iterative%20Pyramid%20Estimation%20with%20Attention%20and%20Residuals%20for%20Deformable%20Medical%20Image%20Registration%0AAuthor%3A%20Heming%20Wu%20and%20Di%20Wang%20and%20Tai%20Ma%20and%20Peng%20Zhao%20and%20Yubin%20Xiao%20and%20Zhongke%20Wu%20and%20Xing-Ce%20Wang%20and%20Xuan%20Wu%20and%20You%20Zhou%0AAbstract%3A%20Existing%20pyramid%20registration%20networks%20may%20accumulate%20anatomical%20misalignments%20and%20lack%20an%20effective%20mechanism%20to%20dynamically%20determine%20the%20number%20of%20optimization%20iterations%20under%20varying%20deformation%20requirements%20across%20images%2C%20leading%20to%20degraded%20performance.%20To%20solve%20these%20limitations%2C%20we%20propose%20iPEAR.%20Specifically%2C%20iPEAR%20adopts%20our%20proposed%20Fused%20Attention-Residual%20Module%20%28FARM%29%20for%20decoding%2C%20which%20comprises%20an%20attention%20pathway%20and%20a%20residual%20pathway%20to%20alleviate%20the%20accumulation%20of%20anatomical%20misalignment.%20We%20further%20propose%20a%20dual-stage%20Threshold-Controlled%20Iterative%20%28TCI%29%20strategy%20that%20adaptively%20determines%20the%20number%20of%20optimization%20iterations%20for%20varying%20images%20by%20evaluating%20registration%20stability%20and%20convergence.%20Extensive%20experiments%20on%20three%20public%20brain%20MRI%20datasets%20and%20one%20public%20abdomen%20CT%20dataset%20show%20that%20iPEAR%20outperforms%20state-of-the-art%20%28SOTA%29%20registration%20networks%20in%20terms%20of%20accuracy%2C%20while%20achieving%20on-par%20inference%20speed%20and%20model%20parameter%20size.%20Generalization%20and%20ablation%20studies%20further%20validate%20the%20effectiveness%20of%20the%20proposed%20FARM%20and%20TCI.%0ALink%3A%20http%3A//arxiv.org/abs/2510.07666v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiPEAR%253A%2520Iterative%2520Pyramid%2520Estimation%2520with%2520Attention%2520and%2520Residuals%2520for%2520Deformable%2520Medical%2520Image%2520Registration%26entry.906535625%3DHeming%2520Wu%2520and%2520Di%2520Wang%2520and%2520Tai%2520Ma%2520and%2520Peng%2520Zhao%2520and%2520Yubin%2520Xiao%2520and%2520Zhongke%2520Wu%2520and%2520Xing-Ce%2520Wang%2520and%2520Xuan%2520Wu%2520and%2520You%2520Zhou%26entry.1292438233%3DExisting%2520pyramid%2520registration%2520networks%2520may%2520accumulate%2520anatomical%2520misalignments%2520and%2520lack%2520an%2520effective%2520mechanism%2520to%2520dynamically%2520determine%2520the%2520number%2520of%2520optimization%2520iterations%2520under%2520varying%2520deformation%2520requirements%2520across%2520images%252C%2520leading%2520to%2520degraded%2520performance.%2520To%2520solve%2520these%2520limitations%252C%2520we%2520propose%2520iPEAR.%2520Specifically%252C%2520iPEAR%2520adopts%2520our%2520proposed%2520Fused%2520Attention-Residual%2520Module%2520%2528FARM%2529%2520for%2520decoding%252C%2520which%2520comprises%2520an%2520attention%2520pathway%2520and%2520a%2520residual%2520pathway%2520to%2520alleviate%2520the%2520accumulation%2520of%2520anatomical%2520misalignment.%2520We%2520further%2520propose%2520a%2520dual-stage%2520Threshold-Controlled%2520Iterative%2520%2528TCI%2529%2520strategy%2520that%2520adaptively%2520determines%2520the%2520number%2520of%2520optimization%2520iterations%2520for%2520varying%2520images%2520by%2520evaluating%2520registration%2520stability%2520and%2520convergence.%2520Extensive%2520experiments%2520on%2520three%2520public%2520brain%2520MRI%2520datasets%2520and%2520one%2520public%2520abdomen%2520CT%2520dataset%2520show%2520that%2520iPEAR%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%2520registration%2520networks%2520in%2520terms%2520of%2520accuracy%252C%2520while%2520achieving%2520on-par%2520inference%2520speed%2520and%2520model%2520parameter%2520size.%2520Generalization%2520and%2520ablation%2520studies%2520further%2520validate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520FARM%2520and%2520TCI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07666v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iPEAR%3A%20Iterative%20Pyramid%20Estimation%20with%20Attention%20and%20Residuals%20for%20Deformable%20Medical%20Image%20Registration&entry.906535625=Heming%20Wu%20and%20Di%20Wang%20and%20Tai%20Ma%20and%20Peng%20Zhao%20and%20Yubin%20Xiao%20and%20Zhongke%20Wu%20and%20Xing-Ce%20Wang%20and%20Xuan%20Wu%20and%20You%20Zhou&entry.1292438233=Existing%20pyramid%20registration%20networks%20may%20accumulate%20anatomical%20misalignments%20and%20lack%20an%20effective%20mechanism%20to%20dynamically%20determine%20the%20number%20of%20optimization%20iterations%20under%20varying%20deformation%20requirements%20across%20images%2C%20leading%20to%20degraded%20performance.%20To%20solve%20these%20limitations%2C%20we%20propose%20iPEAR.%20Specifically%2C%20iPEAR%20adopts%20our%20proposed%20Fused%20Attention-Residual%20Module%20%28FARM%29%20for%20decoding%2C%20which%20comprises%20an%20attention%20pathway%20and%20a%20residual%20pathway%20to%20alleviate%20the%20accumulation%20of%20anatomical%20misalignment.%20We%20further%20propose%20a%20dual-stage%20Threshold-Controlled%20Iterative%20%28TCI%29%20strategy%20that%20adaptively%20determines%20the%20number%20of%20optimization%20iterations%20for%20varying%20images%20by%20evaluating%20registration%20stability%20and%20convergence.%20Extensive%20experiments%20on%20three%20public%20brain%20MRI%20datasets%20and%20one%20public%20abdomen%20CT%20dataset%20show%20that%20iPEAR%20outperforms%20state-of-the-art%20%28SOTA%29%20registration%20networks%20in%20terms%20of%20accuracy%2C%20while%20achieving%20on-par%20inference%20speed%20and%20model%20parameter%20size.%20Generalization%20and%20ablation%20studies%20further%20validate%20the%20effectiveness%20of%20the%20proposed%20FARM%20and%20TCI.&entry.1838667208=http%3A//arxiv.org/abs/2510.07666v2&entry.124074799=Read"},
{"title": "TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning", "author": "Huiyuan Lai and Malvina Nissim", "abstract": "Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with redundant intermediate steps. To improve learning and reasoning efficiency, while preserving or even enhancing performance, we propose TACLer, a model-tailored curriculum reinforcement learning framework that gradually increases the complexity of the data based on the model's proficiency in multi-stage RL training. TACLer features two core components: (i) tailored curriculum learning that determines what knowledge the model lacks and needs to learn in progressive stages; (ii) a hybrid Thinking/NoThinking reasoning paradigm that balances accuracy and efficiency by enabling or disabling the Thinking mode. Our experiments show that TACLer yields a twofold advantage in learning and reasoning: (i) it reduces computational cost, cutting training compute by over 50% compared to long thinking models and reducing inference token usage by over 42% relative to the base model; and (ii) it improves accuracy by over 9% on the base model, consistently outperforming state-of-the-art Nothinking and Thinking baselines across four math datasets with complex problems.", "link": "http://arxiv.org/abs/2601.21711v1", "date": "2026-01-29", "relevancy": 2.4056, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.514}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TACLer%3A%20Tailored%20Curriculum%20Reinforcement%20Learning%20for%20Efficient%20Reasoning&body=Title%3A%20TACLer%3A%20Tailored%20Curriculum%20Reinforcement%20Learning%20for%20Efficient%20Reasoning%0AAuthor%3A%20Huiyuan%20Lai%20and%20Malvina%20Nissim%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20performance%20on%20complex%20reasoning%20tasks%2C%20especially%20when%20equipped%20with%20long%20chain-of-thought%20%28CoT%29%20reasoning.%20However%2C%20eliciting%20long%20CoT%20typically%20requires%20large-scale%20reinforcement%20learning%20%28RL%29%20training%2C%20while%20often%20leading%20to%20overthinking%20with%20redundant%20intermediate%20steps.%20To%20improve%20learning%20and%20reasoning%20efficiency%2C%20while%20preserving%20or%20even%20enhancing%20performance%2C%20we%20propose%20TACLer%2C%20a%20model-tailored%20curriculum%20reinforcement%20learning%20framework%20that%20gradually%20increases%20the%20complexity%20of%20the%20data%20based%20on%20the%20model%27s%20proficiency%20in%20multi-stage%20RL%20training.%20TACLer%20features%20two%20core%20components%3A%20%28i%29%20tailored%20curriculum%20learning%20that%20determines%20what%20knowledge%20the%20model%20lacks%20and%20needs%20to%20learn%20in%20progressive%20stages%3B%20%28ii%29%20a%20hybrid%20Thinking/NoThinking%20reasoning%20paradigm%20that%20balances%20accuracy%20and%20efficiency%20by%20enabling%20or%20disabling%20the%20Thinking%20mode.%20Our%20experiments%20show%20that%20TACLer%20yields%20a%20twofold%20advantage%20in%20learning%20and%20reasoning%3A%20%28i%29%20it%20reduces%20computational%20cost%2C%20cutting%20training%20compute%20by%20over%2050%25%20compared%20to%20long%20thinking%20models%20and%20reducing%20inference%20token%20usage%20by%20over%2042%25%20relative%20to%20the%20base%20model%3B%20and%20%28ii%29%20it%20improves%20accuracy%20by%20over%209%25%20on%20the%20base%20model%2C%20consistently%20outperforming%20state-of-the-art%20Nothinking%20and%20Thinking%20baselines%20across%20four%20math%20datasets%20with%20complex%20problems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTACLer%253A%2520Tailored%2520Curriculum%2520Reinforcement%2520Learning%2520for%2520Efficient%2520Reasoning%26entry.906535625%3DHuiyuan%2520Lai%2520and%2520Malvina%2520Nissim%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520performance%2520on%2520complex%2520reasoning%2520tasks%252C%2520especially%2520when%2520equipped%2520with%2520long%2520chain-of-thought%2520%2528CoT%2529%2520reasoning.%2520However%252C%2520eliciting%2520long%2520CoT%2520typically%2520requires%2520large-scale%2520reinforcement%2520learning%2520%2528RL%2529%2520training%252C%2520while%2520often%2520leading%2520to%2520overthinking%2520with%2520redundant%2520intermediate%2520steps.%2520To%2520improve%2520learning%2520and%2520reasoning%2520efficiency%252C%2520while%2520preserving%2520or%2520even%2520enhancing%2520performance%252C%2520we%2520propose%2520TACLer%252C%2520a%2520model-tailored%2520curriculum%2520reinforcement%2520learning%2520framework%2520that%2520gradually%2520increases%2520the%2520complexity%2520of%2520the%2520data%2520based%2520on%2520the%2520model%2527s%2520proficiency%2520in%2520multi-stage%2520RL%2520training.%2520TACLer%2520features%2520two%2520core%2520components%253A%2520%2528i%2529%2520tailored%2520curriculum%2520learning%2520that%2520determines%2520what%2520knowledge%2520the%2520model%2520lacks%2520and%2520needs%2520to%2520learn%2520in%2520progressive%2520stages%253B%2520%2528ii%2529%2520a%2520hybrid%2520Thinking/NoThinking%2520reasoning%2520paradigm%2520that%2520balances%2520accuracy%2520and%2520efficiency%2520by%2520enabling%2520or%2520disabling%2520the%2520Thinking%2520mode.%2520Our%2520experiments%2520show%2520that%2520TACLer%2520yields%2520a%2520twofold%2520advantage%2520in%2520learning%2520and%2520reasoning%253A%2520%2528i%2529%2520it%2520reduces%2520computational%2520cost%252C%2520cutting%2520training%2520compute%2520by%2520over%252050%2525%2520compared%2520to%2520long%2520thinking%2520models%2520and%2520reducing%2520inference%2520token%2520usage%2520by%2520over%252042%2525%2520relative%2520to%2520the%2520base%2520model%253B%2520and%2520%2528ii%2529%2520it%2520improves%2520accuracy%2520by%2520over%25209%2525%2520on%2520the%2520base%2520model%252C%2520consistently%2520outperforming%2520state-of-the-art%2520Nothinking%2520and%2520Thinking%2520baselines%2520across%2520four%2520math%2520datasets%2520with%2520complex%2520problems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TACLer%3A%20Tailored%20Curriculum%20Reinforcement%20Learning%20for%20Efficient%20Reasoning&entry.906535625=Huiyuan%20Lai%20and%20Malvina%20Nissim&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20performance%20on%20complex%20reasoning%20tasks%2C%20especially%20when%20equipped%20with%20long%20chain-of-thought%20%28CoT%29%20reasoning.%20However%2C%20eliciting%20long%20CoT%20typically%20requires%20large-scale%20reinforcement%20learning%20%28RL%29%20training%2C%20while%20often%20leading%20to%20overthinking%20with%20redundant%20intermediate%20steps.%20To%20improve%20learning%20and%20reasoning%20efficiency%2C%20while%20preserving%20or%20even%20enhancing%20performance%2C%20we%20propose%20TACLer%2C%20a%20model-tailored%20curriculum%20reinforcement%20learning%20framework%20that%20gradually%20increases%20the%20complexity%20of%20the%20data%20based%20on%20the%20model%27s%20proficiency%20in%20multi-stage%20RL%20training.%20TACLer%20features%20two%20core%20components%3A%20%28i%29%20tailored%20curriculum%20learning%20that%20determines%20what%20knowledge%20the%20model%20lacks%20and%20needs%20to%20learn%20in%20progressive%20stages%3B%20%28ii%29%20a%20hybrid%20Thinking/NoThinking%20reasoning%20paradigm%20that%20balances%20accuracy%20and%20efficiency%20by%20enabling%20or%20disabling%20the%20Thinking%20mode.%20Our%20experiments%20show%20that%20TACLer%20yields%20a%20twofold%20advantage%20in%20learning%20and%20reasoning%3A%20%28i%29%20it%20reduces%20computational%20cost%2C%20cutting%20training%20compute%20by%20over%2050%25%20compared%20to%20long%20thinking%20models%20and%20reducing%20inference%20token%20usage%20by%20over%2042%25%20relative%20to%20the%20base%20model%3B%20and%20%28ii%29%20it%20improves%20accuracy%20by%20over%209%25%20on%20the%20base%20model%2C%20consistently%20outperforming%20state-of-the-art%20Nothinking%20and%20Thinking%20baselines%20across%20four%20math%20datasets%20with%20complex%20problems.&entry.1838667208=http%3A//arxiv.org/abs/2601.21711v1&entry.124074799=Read"},
{"title": "GenOM: Ontology Matching with Description Generation and Large Language Model", "author": "Yiping Song and Jiaoyan Chen and Renate A. Schmidt", "abstract": "Ontology matching (OM) plays an essential role in enabling semantic interoperability and integration across heterogeneous knowledge sources, particularly in the biomedical domain which contains numerous complex concepts related to diseases and pharmaceuticals. This paper introduces GenOM, a large language model (LLM)-based ontology alignment framework, which enriches the semantic representations of ontology concepts via generating textual definitions, retrieves alignment candidates with an embedding model, and incorporates exact matching-based tools to improve precision. Extensive experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often achieve competitive performance, surpassing many baselines including traditional OM systems and recent LLM-based methods. Further ablation studies confirm the effectiveness of semantic enrichment and few-shot prompting, highlighting the framework's robustness and adaptability.", "link": "http://arxiv.org/abs/2508.10703v2", "date": "2026-01-29", "relevancy": 2.4011, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4798}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenOM%3A%20Ontology%20Matching%20with%20Description%20Generation%20and%20Large%20Language%20Model&body=Title%3A%20GenOM%3A%20Ontology%20Matching%20with%20Description%20Generation%20and%20Large%20Language%20Model%0AAuthor%3A%20Yiping%20Song%20and%20Jiaoyan%20Chen%20and%20Renate%20A.%20Schmidt%0AAbstract%3A%20Ontology%20matching%20%28OM%29%20plays%20an%20essential%20role%20in%20enabling%20semantic%20interoperability%20and%20integration%20across%20heterogeneous%20knowledge%20sources%2C%20particularly%20in%20the%20biomedical%20domain%20which%20contains%20numerous%20complex%20concepts%20related%20to%20diseases%20and%20pharmaceuticals.%20This%20paper%20introduces%20GenOM%2C%20a%20large%20language%20model%20%28LLM%29-based%20ontology%20alignment%20framework%2C%20which%20enriches%20the%20semantic%20representations%20of%20ontology%20concepts%20via%20generating%20textual%20definitions%2C%20retrieves%20alignment%20candidates%20with%20an%20embedding%20model%2C%20and%20incorporates%20exact%20matching-based%20tools%20to%20improve%20precision.%20Extensive%20experiments%20conducted%20on%20the%20OAEI%20Bio-ML%20track%20demonstrate%20that%20GenOM%20can%20often%20achieve%20competitive%20performance%2C%20surpassing%20many%20baselines%20including%20traditional%20OM%20systems%20and%20recent%20LLM-based%20methods.%20Further%20ablation%20studies%20confirm%20the%20effectiveness%20of%20semantic%20enrichment%20and%20few-shot%20prompting%2C%20highlighting%20the%20framework%27s%20robustness%20and%20adaptability.%0ALink%3A%20http%3A//arxiv.org/abs/2508.10703v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenOM%253A%2520Ontology%2520Matching%2520with%2520Description%2520Generation%2520and%2520Large%2520Language%2520Model%26entry.906535625%3DYiping%2520Song%2520and%2520Jiaoyan%2520Chen%2520and%2520Renate%2520A.%2520Schmidt%26entry.1292438233%3DOntology%2520matching%2520%2528OM%2529%2520plays%2520an%2520essential%2520role%2520in%2520enabling%2520semantic%2520interoperability%2520and%2520integration%2520across%2520heterogeneous%2520knowledge%2520sources%252C%2520particularly%2520in%2520the%2520biomedical%2520domain%2520which%2520contains%2520numerous%2520complex%2520concepts%2520related%2520to%2520diseases%2520and%2520pharmaceuticals.%2520This%2520paper%2520introduces%2520GenOM%252C%2520a%2520large%2520language%2520model%2520%2528LLM%2529-based%2520ontology%2520alignment%2520framework%252C%2520which%2520enriches%2520the%2520semantic%2520representations%2520of%2520ontology%2520concepts%2520via%2520generating%2520textual%2520definitions%252C%2520retrieves%2520alignment%2520candidates%2520with%2520an%2520embedding%2520model%252C%2520and%2520incorporates%2520exact%2520matching-based%2520tools%2520to%2520improve%2520precision.%2520Extensive%2520experiments%2520conducted%2520on%2520the%2520OAEI%2520Bio-ML%2520track%2520demonstrate%2520that%2520GenOM%2520can%2520often%2520achieve%2520competitive%2520performance%252C%2520surpassing%2520many%2520baselines%2520including%2520traditional%2520OM%2520systems%2520and%2520recent%2520LLM-based%2520methods.%2520Further%2520ablation%2520studies%2520confirm%2520the%2520effectiveness%2520of%2520semantic%2520enrichment%2520and%2520few-shot%2520prompting%252C%2520highlighting%2520the%2520framework%2527s%2520robustness%2520and%2520adaptability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10703v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenOM%3A%20Ontology%20Matching%20with%20Description%20Generation%20and%20Large%20Language%20Model&entry.906535625=Yiping%20Song%20and%20Jiaoyan%20Chen%20and%20Renate%20A.%20Schmidt&entry.1292438233=Ontology%20matching%20%28OM%29%20plays%20an%20essential%20role%20in%20enabling%20semantic%20interoperability%20and%20integration%20across%20heterogeneous%20knowledge%20sources%2C%20particularly%20in%20the%20biomedical%20domain%20which%20contains%20numerous%20complex%20concepts%20related%20to%20diseases%20and%20pharmaceuticals.%20This%20paper%20introduces%20GenOM%2C%20a%20large%20language%20model%20%28LLM%29-based%20ontology%20alignment%20framework%2C%20which%20enriches%20the%20semantic%20representations%20of%20ontology%20concepts%20via%20generating%20textual%20definitions%2C%20retrieves%20alignment%20candidates%20with%20an%20embedding%20model%2C%20and%20incorporates%20exact%20matching-based%20tools%20to%20improve%20precision.%20Extensive%20experiments%20conducted%20on%20the%20OAEI%20Bio-ML%20track%20demonstrate%20that%20GenOM%20can%20often%20achieve%20competitive%20performance%2C%20surpassing%20many%20baselines%20including%20traditional%20OM%20systems%20and%20recent%20LLM-based%20methods.%20Further%20ablation%20studies%20confirm%20the%20effectiveness%20of%20semantic%20enrichment%20and%20few-shot%20prompting%2C%20highlighting%20the%20framework%27s%20robustness%20and%20adaptability.&entry.1838667208=http%3A//arxiv.org/abs/2508.10703v2&entry.124074799=Read"},
{"title": "Gauge-invariant representation holonomy", "author": "Vasileios Sevetlidis and George Pavlidis", "abstract": "Deep networks learn internal representations whose geometry--how features bend, rotate, and evolve--affects both generalization and robustness. Existing similarity measures such as CKA or SVCCA capture pointwise overlap between activation sets, but miss how representations change along input paths. Two models may appear nearly identical under these metrics yet respond very differently to perturbations or adversarial stress. We introduce representation holonomy, a gauge-invariant statistic that measures this path dependence. Conceptually, holonomy quantifies the \"twist\" accumulated when features are parallel-transported around a small loop in input space: flat representations yield zero holonomy, while nonzero values reveal hidden curvature. Our estimator fixes gauge through global whitening, aligns neighborhoods using shared subspaces and rotation-only Procrustes, and embeds the result back to the full feature space. We prove invariance to orthogonal (and affine, post-whitening) transformations, establish a linear null for affine layers, and show that holonomy vanishes at small radii. Empirically, holonomy increases with loop radius, separates models that appear similar under CKA, and correlates with adversarial and corruption robustness. It also tracks training dynamics as features form and stabilize. Together, these results position representation holonomy as a practical and scalable diagnostic for probing the geometric structure of learned representations beyond pointwise similarity.", "link": "http://arxiv.org/abs/2601.21653v1", "date": "2026-01-29", "relevancy": 2.4008, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4869}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4776}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gauge-invariant%20representation%20holonomy&body=Title%3A%20Gauge-invariant%20representation%20holonomy%0AAuthor%3A%20Vasileios%20Sevetlidis%20and%20George%20Pavlidis%0AAbstract%3A%20Deep%20networks%20learn%20internal%20representations%20whose%20geometry--how%20features%20bend%2C%20rotate%2C%20and%20evolve--affects%20both%20generalization%20and%20robustness.%20Existing%20similarity%20measures%20such%20as%20CKA%20or%20SVCCA%20capture%20pointwise%20overlap%20between%20activation%20sets%2C%20but%20miss%20how%20representations%20change%20along%20input%20paths.%20Two%20models%20may%20appear%20nearly%20identical%20under%20these%20metrics%20yet%20respond%20very%20differently%20to%20perturbations%20or%20adversarial%20stress.%20We%20introduce%20representation%20holonomy%2C%20a%20gauge-invariant%20statistic%20that%20measures%20this%20path%20dependence.%20Conceptually%2C%20holonomy%20quantifies%20the%20%22twist%22%20accumulated%20when%20features%20are%20parallel-transported%20around%20a%20small%20loop%20in%20input%20space%3A%20flat%20representations%20yield%20zero%20holonomy%2C%20while%20nonzero%20values%20reveal%20hidden%20curvature.%20Our%20estimator%20fixes%20gauge%20through%20global%20whitening%2C%20aligns%20neighborhoods%20using%20shared%20subspaces%20and%20rotation-only%20Procrustes%2C%20and%20embeds%20the%20result%20back%20to%20the%20full%20feature%20space.%20We%20prove%20invariance%20to%20orthogonal%20%28and%20affine%2C%20post-whitening%29%20transformations%2C%20establish%20a%20linear%20null%20for%20affine%20layers%2C%20and%20show%20that%20holonomy%20vanishes%20at%20small%20radii.%20Empirically%2C%20holonomy%20increases%20with%20loop%20radius%2C%20separates%20models%20that%20appear%20similar%20under%20CKA%2C%20and%20correlates%20with%20adversarial%20and%20corruption%20robustness.%20It%20also%20tracks%20training%20dynamics%20as%20features%20form%20and%20stabilize.%20Together%2C%20these%20results%20position%20representation%20holonomy%20as%20a%20practical%20and%20scalable%20diagnostic%20for%20probing%20the%20geometric%20structure%20of%20learned%20representations%20beyond%20pointwise%20similarity.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGauge-invariant%2520representation%2520holonomy%26entry.906535625%3DVasileios%2520Sevetlidis%2520and%2520George%2520Pavlidis%26entry.1292438233%3DDeep%2520networks%2520learn%2520internal%2520representations%2520whose%2520geometry--how%2520features%2520bend%252C%2520rotate%252C%2520and%2520evolve--affects%2520both%2520generalization%2520and%2520robustness.%2520Existing%2520similarity%2520measures%2520such%2520as%2520CKA%2520or%2520SVCCA%2520capture%2520pointwise%2520overlap%2520between%2520activation%2520sets%252C%2520but%2520miss%2520how%2520representations%2520change%2520along%2520input%2520paths.%2520Two%2520models%2520may%2520appear%2520nearly%2520identical%2520under%2520these%2520metrics%2520yet%2520respond%2520very%2520differently%2520to%2520perturbations%2520or%2520adversarial%2520stress.%2520We%2520introduce%2520representation%2520holonomy%252C%2520a%2520gauge-invariant%2520statistic%2520that%2520measures%2520this%2520path%2520dependence.%2520Conceptually%252C%2520holonomy%2520quantifies%2520the%2520%2522twist%2522%2520accumulated%2520when%2520features%2520are%2520parallel-transported%2520around%2520a%2520small%2520loop%2520in%2520input%2520space%253A%2520flat%2520representations%2520yield%2520zero%2520holonomy%252C%2520while%2520nonzero%2520values%2520reveal%2520hidden%2520curvature.%2520Our%2520estimator%2520fixes%2520gauge%2520through%2520global%2520whitening%252C%2520aligns%2520neighborhoods%2520using%2520shared%2520subspaces%2520and%2520rotation-only%2520Procrustes%252C%2520and%2520embeds%2520the%2520result%2520back%2520to%2520the%2520full%2520feature%2520space.%2520We%2520prove%2520invariance%2520to%2520orthogonal%2520%2528and%2520affine%252C%2520post-whitening%2529%2520transformations%252C%2520establish%2520a%2520linear%2520null%2520for%2520affine%2520layers%252C%2520and%2520show%2520that%2520holonomy%2520vanishes%2520at%2520small%2520radii.%2520Empirically%252C%2520holonomy%2520increases%2520with%2520loop%2520radius%252C%2520separates%2520models%2520that%2520appear%2520similar%2520under%2520CKA%252C%2520and%2520correlates%2520with%2520adversarial%2520and%2520corruption%2520robustness.%2520It%2520also%2520tracks%2520training%2520dynamics%2520as%2520features%2520form%2520and%2520stabilize.%2520Together%252C%2520these%2520results%2520position%2520representation%2520holonomy%2520as%2520a%2520practical%2520and%2520scalable%2520diagnostic%2520for%2520probing%2520the%2520geometric%2520structure%2520of%2520learned%2520representations%2520beyond%2520pointwise%2520similarity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gauge-invariant%20representation%20holonomy&entry.906535625=Vasileios%20Sevetlidis%20and%20George%20Pavlidis&entry.1292438233=Deep%20networks%20learn%20internal%20representations%20whose%20geometry--how%20features%20bend%2C%20rotate%2C%20and%20evolve--affects%20both%20generalization%20and%20robustness.%20Existing%20similarity%20measures%20such%20as%20CKA%20or%20SVCCA%20capture%20pointwise%20overlap%20between%20activation%20sets%2C%20but%20miss%20how%20representations%20change%20along%20input%20paths.%20Two%20models%20may%20appear%20nearly%20identical%20under%20these%20metrics%20yet%20respond%20very%20differently%20to%20perturbations%20or%20adversarial%20stress.%20We%20introduce%20representation%20holonomy%2C%20a%20gauge-invariant%20statistic%20that%20measures%20this%20path%20dependence.%20Conceptually%2C%20holonomy%20quantifies%20the%20%22twist%22%20accumulated%20when%20features%20are%20parallel-transported%20around%20a%20small%20loop%20in%20input%20space%3A%20flat%20representations%20yield%20zero%20holonomy%2C%20while%20nonzero%20values%20reveal%20hidden%20curvature.%20Our%20estimator%20fixes%20gauge%20through%20global%20whitening%2C%20aligns%20neighborhoods%20using%20shared%20subspaces%20and%20rotation-only%20Procrustes%2C%20and%20embeds%20the%20result%20back%20to%20the%20full%20feature%20space.%20We%20prove%20invariance%20to%20orthogonal%20%28and%20affine%2C%20post-whitening%29%20transformations%2C%20establish%20a%20linear%20null%20for%20affine%20layers%2C%20and%20show%20that%20holonomy%20vanishes%20at%20small%20radii.%20Empirically%2C%20holonomy%20increases%20with%20loop%20radius%2C%20separates%20models%20that%20appear%20similar%20under%20CKA%2C%20and%20correlates%20with%20adversarial%20and%20corruption%20robustness.%20It%20also%20tracks%20training%20dynamics%20as%20features%20form%20and%20stabilize.%20Together%2C%20these%20results%20position%20representation%20holonomy%20as%20a%20practical%20and%20scalable%20diagnostic%20for%20probing%20the%20geometric%20structure%20of%20learned%20representations%20beyond%20pointwise%20similarity.&entry.1838667208=http%3A//arxiv.org/abs/2601.21653v1&entry.124074799=Read"},
{"title": "Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics", "author": "Mohamed Elgaar and Hadi Amiri", "abstract": "Curriculum learning changes the order of pre-training data, but it remains unclear whether it changes the learning trajectory or mainly reorders exposure over a fixed trajectory. We train Pythia models (14M-410M parameters) for 300B tokens under three linguistically motivated curricula-Age-of-Acquisition, word frequency, and Verb Variation (VV)-and compare each against Random ordering; at 1B parameters we compare Random and VV. Across orderings, training follows a shared sequence of latent phases, while curricula mainly change within-phase data exposure. In smaller models (up to 160M parameters), Random ordering exhibits higher gradient noise and stronger late-training output-head spectral saturation, alongside lower final accuracy; curricula reduce both effects at matched compute. At larger scales, saturation differences are smaller and curriculum gains shrink. We formalize the link between difficulty pacing and optimization stability in an idealized analysis based on gradient-variance control, and our results point to a practical takeaway: curricula help by stabilizing within-phase optimization rather than by creating new phases.", "link": "http://arxiv.org/abs/2601.21698v1", "date": "2026-01-29", "relevancy": 2.3812, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4766}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4761}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curriculum%20Learning%20for%20LLM%20Pretraining%3A%20An%20Analysis%20of%20Learning%20Dynamics&body=Title%3A%20Curriculum%20Learning%20for%20LLM%20Pretraining%3A%20An%20Analysis%20of%20Learning%20Dynamics%0AAuthor%3A%20Mohamed%20Elgaar%20and%20Hadi%20Amiri%0AAbstract%3A%20Curriculum%20learning%20changes%20the%20order%20of%20pre-training%20data%2C%20but%20it%20remains%20unclear%20whether%20it%20changes%20the%20learning%20trajectory%20or%20mainly%20reorders%20exposure%20over%20a%20fixed%20trajectory.%20We%20train%20Pythia%20models%20%2814M-410M%20parameters%29%20for%20300B%20tokens%20under%20three%20linguistically%20motivated%20curricula-Age-of-Acquisition%2C%20word%20frequency%2C%20and%20Verb%20Variation%20%28VV%29-and%20compare%20each%20against%20Random%20ordering%3B%20at%201B%20parameters%20we%20compare%20Random%20and%20VV.%20Across%20orderings%2C%20training%20follows%20a%20shared%20sequence%20of%20latent%20phases%2C%20while%20curricula%20mainly%20change%20within-phase%20data%20exposure.%20In%20smaller%20models%20%28up%20to%20160M%20parameters%29%2C%20Random%20ordering%20exhibits%20higher%20gradient%20noise%20and%20stronger%20late-training%20output-head%20spectral%20saturation%2C%20alongside%20lower%20final%20accuracy%3B%20curricula%20reduce%20both%20effects%20at%20matched%20compute.%20At%20larger%20scales%2C%20saturation%20differences%20are%20smaller%20and%20curriculum%20gains%20shrink.%20We%20formalize%20the%20link%20between%20difficulty%20pacing%20and%20optimization%20stability%20in%20an%20idealized%20analysis%20based%20on%20gradient-variance%20control%2C%20and%20our%20results%20point%20to%20a%20practical%20takeaway%3A%20curricula%20help%20by%20stabilizing%20within-phase%20optimization%20rather%20than%20by%20creating%20new%20phases.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurriculum%2520Learning%2520for%2520LLM%2520Pretraining%253A%2520An%2520Analysis%2520of%2520Learning%2520Dynamics%26entry.906535625%3DMohamed%2520Elgaar%2520and%2520Hadi%2520Amiri%26entry.1292438233%3DCurriculum%2520learning%2520changes%2520the%2520order%2520of%2520pre-training%2520data%252C%2520but%2520it%2520remains%2520unclear%2520whether%2520it%2520changes%2520the%2520learning%2520trajectory%2520or%2520mainly%2520reorders%2520exposure%2520over%2520a%2520fixed%2520trajectory.%2520We%2520train%2520Pythia%2520models%2520%252814M-410M%2520parameters%2529%2520for%2520300B%2520tokens%2520under%2520three%2520linguistically%2520motivated%2520curricula-Age-of-Acquisition%252C%2520word%2520frequency%252C%2520and%2520Verb%2520Variation%2520%2528VV%2529-and%2520compare%2520each%2520against%2520Random%2520ordering%253B%2520at%25201B%2520parameters%2520we%2520compare%2520Random%2520and%2520VV.%2520Across%2520orderings%252C%2520training%2520follows%2520a%2520shared%2520sequence%2520of%2520latent%2520phases%252C%2520while%2520curricula%2520mainly%2520change%2520within-phase%2520data%2520exposure.%2520In%2520smaller%2520models%2520%2528up%2520to%2520160M%2520parameters%2529%252C%2520Random%2520ordering%2520exhibits%2520higher%2520gradient%2520noise%2520and%2520stronger%2520late-training%2520output-head%2520spectral%2520saturation%252C%2520alongside%2520lower%2520final%2520accuracy%253B%2520curricula%2520reduce%2520both%2520effects%2520at%2520matched%2520compute.%2520At%2520larger%2520scales%252C%2520saturation%2520differences%2520are%2520smaller%2520and%2520curriculum%2520gains%2520shrink.%2520We%2520formalize%2520the%2520link%2520between%2520difficulty%2520pacing%2520and%2520optimization%2520stability%2520in%2520an%2520idealized%2520analysis%2520based%2520on%2520gradient-variance%2520control%252C%2520and%2520our%2520results%2520point%2520to%2520a%2520practical%2520takeaway%253A%2520curricula%2520help%2520by%2520stabilizing%2520within-phase%2520optimization%2520rather%2520than%2520by%2520creating%2520new%2520phases.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curriculum%20Learning%20for%20LLM%20Pretraining%3A%20An%20Analysis%20of%20Learning%20Dynamics&entry.906535625=Mohamed%20Elgaar%20and%20Hadi%20Amiri&entry.1292438233=Curriculum%20learning%20changes%20the%20order%20of%20pre-training%20data%2C%20but%20it%20remains%20unclear%20whether%20it%20changes%20the%20learning%20trajectory%20or%20mainly%20reorders%20exposure%20over%20a%20fixed%20trajectory.%20We%20train%20Pythia%20models%20%2814M-410M%20parameters%29%20for%20300B%20tokens%20under%20three%20linguistically%20motivated%20curricula-Age-of-Acquisition%2C%20word%20frequency%2C%20and%20Verb%20Variation%20%28VV%29-and%20compare%20each%20against%20Random%20ordering%3B%20at%201B%20parameters%20we%20compare%20Random%20and%20VV.%20Across%20orderings%2C%20training%20follows%20a%20shared%20sequence%20of%20latent%20phases%2C%20while%20curricula%20mainly%20change%20within-phase%20data%20exposure.%20In%20smaller%20models%20%28up%20to%20160M%20parameters%29%2C%20Random%20ordering%20exhibits%20higher%20gradient%20noise%20and%20stronger%20late-training%20output-head%20spectral%20saturation%2C%20alongside%20lower%20final%20accuracy%3B%20curricula%20reduce%20both%20effects%20at%20matched%20compute.%20At%20larger%20scales%2C%20saturation%20differences%20are%20smaller%20and%20curriculum%20gains%20shrink.%20We%20formalize%20the%20link%20between%20difficulty%20pacing%20and%20optimization%20stability%20in%20an%20idealized%20analysis%20based%20on%20gradient-variance%20control%2C%20and%20our%20results%20point%20to%20a%20practical%20takeaway%3A%20curricula%20help%20by%20stabilizing%20within-phase%20optimization%20rather%20than%20by%20creating%20new%20phases.&entry.1838667208=http%3A//arxiv.org/abs/2601.21698v1&entry.124074799=Read"},
{"title": "SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing", "author": "Thanh-Nhan Vo and Trong-Thuan Nguyen and Tam V. Nguyen and Minh-Triet Tran", "abstract": "Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.", "link": "http://arxiv.org/abs/2601.21498v1", "date": "2026-01-29", "relevancy": 2.3798, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6233}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5985}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.58}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimGraph%3A%20A%20Unified%20Framework%20for%20Scene%20Graph-Based%20Image%20Generation%20and%20Editing&body=Title%3A%20SimGraph%3A%20A%20Unified%20Framework%20for%20Scene%20Graph-Based%20Image%20Generation%20and%20Editing%0AAuthor%3A%20Thanh-Nhan%20Vo%20and%20Trong-Thuan%20Nguyen%20and%20Tam%20V.%20Nguyen%20and%20Minh-Triet%20Tran%0AAbstract%3A%20Recent%20advancements%20in%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20have%20significantly%20enhanced%20the%20capabilities%20of%20both%20image%20generation%20and%20editing.%20However%2C%20current%20approaches%20often%20treat%20these%20tasks%20separately%2C%20leading%20to%20inefficiencies%20and%20challenges%20in%20maintaining%20spatial%20consistency%20and%20semantic%20coherence%20between%20generated%20content%20and%20edits.%20Moreover%2C%20a%20major%20obstacle%20is%20the%20lack%20of%20structured%20control%20over%20object%20relationships%20and%20spatial%20arrangements.%20Scene%20graph-based%20methods%2C%20which%20represent%20objects%20and%20their%20interrelationships%20in%20a%20structured%20format%2C%20offer%20a%20solution%20by%20providing%20greater%20control%20over%20composition%20and%20interactions%20in%20both%20image%20generation%20and%20editing.%20To%20address%20this%2C%20we%20introduce%20SimGraph%2C%20a%20unified%20framework%20that%20integrates%20scene%20graph-based%20image%20generation%20and%20editing%2C%20enabling%20precise%20control%20over%20object%20interactions%2C%20layouts%2C%20and%20spatial%20coherence.%20In%20particular%2C%20our%20framework%20integrates%20token-based%20generation%20and%20diffusion-based%20editing%20within%20a%20single%20scene%20graph-driven%20model%2C%20ensuring%20high-quality%20and%20consistent%20results.%20Through%20extensive%20experiments%2C%20we%20empirically%20demonstrate%20that%20our%20approach%20outperforms%20existing%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimGraph%253A%2520A%2520Unified%2520Framework%2520for%2520Scene%2520Graph-Based%2520Image%2520Generation%2520and%2520Editing%26entry.906535625%3DThanh-Nhan%2520Vo%2520and%2520Trong-Thuan%2520Nguyen%2520and%2520Tam%2520V.%2520Nguyen%2520and%2520Minh-Triet%2520Tran%26entry.1292438233%3DRecent%2520advancements%2520in%2520Generative%2520Artificial%2520Intelligence%2520%2528GenAI%2529%2520have%2520significantly%2520enhanced%2520the%2520capabilities%2520of%2520both%2520image%2520generation%2520and%2520editing.%2520However%252C%2520current%2520approaches%2520often%2520treat%2520these%2520tasks%2520separately%252C%2520leading%2520to%2520inefficiencies%2520and%2520challenges%2520in%2520maintaining%2520spatial%2520consistency%2520and%2520semantic%2520coherence%2520between%2520generated%2520content%2520and%2520edits.%2520Moreover%252C%2520a%2520major%2520obstacle%2520is%2520the%2520lack%2520of%2520structured%2520control%2520over%2520object%2520relationships%2520and%2520spatial%2520arrangements.%2520Scene%2520graph-based%2520methods%252C%2520which%2520represent%2520objects%2520and%2520their%2520interrelationships%2520in%2520a%2520structured%2520format%252C%2520offer%2520a%2520solution%2520by%2520providing%2520greater%2520control%2520over%2520composition%2520and%2520interactions%2520in%2520both%2520image%2520generation%2520and%2520editing.%2520To%2520address%2520this%252C%2520we%2520introduce%2520SimGraph%252C%2520a%2520unified%2520framework%2520that%2520integrates%2520scene%2520graph-based%2520image%2520generation%2520and%2520editing%252C%2520enabling%2520precise%2520control%2520over%2520object%2520interactions%252C%2520layouts%252C%2520and%2520spatial%2520coherence.%2520In%2520particular%252C%2520our%2520framework%2520integrates%2520token-based%2520generation%2520and%2520diffusion-based%2520editing%2520within%2520a%2520single%2520scene%2520graph-driven%2520model%252C%2520ensuring%2520high-quality%2520and%2520consistent%2520results.%2520Through%2520extensive%2520experiments%252C%2520we%2520empirically%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520existing%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimGraph%3A%20A%20Unified%20Framework%20for%20Scene%20Graph-Based%20Image%20Generation%20and%20Editing&entry.906535625=Thanh-Nhan%20Vo%20and%20Trong-Thuan%20Nguyen%20and%20Tam%20V.%20Nguyen%20and%20Minh-Triet%20Tran&entry.1292438233=Recent%20advancements%20in%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20have%20significantly%20enhanced%20the%20capabilities%20of%20both%20image%20generation%20and%20editing.%20However%2C%20current%20approaches%20often%20treat%20these%20tasks%20separately%2C%20leading%20to%20inefficiencies%20and%20challenges%20in%20maintaining%20spatial%20consistency%20and%20semantic%20coherence%20between%20generated%20content%20and%20edits.%20Moreover%2C%20a%20major%20obstacle%20is%20the%20lack%20of%20structured%20control%20over%20object%20relationships%20and%20spatial%20arrangements.%20Scene%20graph-based%20methods%2C%20which%20represent%20objects%20and%20their%20interrelationships%20in%20a%20structured%20format%2C%20offer%20a%20solution%20by%20providing%20greater%20control%20over%20composition%20and%20interactions%20in%20both%20image%20generation%20and%20editing.%20To%20address%20this%2C%20we%20introduce%20SimGraph%2C%20a%20unified%20framework%20that%20integrates%20scene%20graph-based%20image%20generation%20and%20editing%2C%20enabling%20precise%20control%20over%20object%20interactions%2C%20layouts%2C%20and%20spatial%20coherence.%20In%20particular%2C%20our%20framework%20integrates%20token-based%20generation%20and%20diffusion-based%20editing%20within%20a%20single%20scene%20graph-driven%20model%2C%20ensuring%20high-quality%20and%20consistent%20results.%20Through%20extensive%20experiments%2C%20we%20empirically%20demonstrate%20that%20our%20approach%20outperforms%20existing%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.21498v1&entry.124074799=Read"},
{"title": "Two Heads are Better than One: Distilling Large Language Model Features Into Small Models with Feature Decomposition and Mixture", "author": "Tianhao Fu and Xinxin Xu and Weichen Xu and Jue Chen and Ruilong Ren and Bowen Deng and Xinyu Zhao and Jian Cao and Xixin Cao", "abstract": "Market making (MM) through Reinforcement Learning (RL) has attracted significant attention in financial trading. With the development of Large Language Models (LLMs), more and more attempts are being made to apply LLMs to financial areas. A simple, direct application of LLM as an agent shows significant performance. Such methods are hindered by their slow inference speed, while most of the current research has not studied LLM distillation for this specific task. To address this, we first propose the normalized fluorescent probe to study the mechanism of the LLM's feature. Based on the observation found by our investigation, we propose Cooperative Market Making (CMM), a novel framework that decouples LLM features across three orthogonal dimensions: layer, task, and data. Various student models collaboratively learn simple LLM features along with different dimensions, with each model responsible for a distinct feature to achieve knowledge distillation. Furthermore, CMM introduces an H\u00e1jek-MoE to integrate the output of the student models by investigating the contribution of different models in a kernel function-generated common feature space. Extensive experimental results on four real-world market datasets demonstrate the superiority of CMM over the current distillation method and RL-based market-making strategies.", "link": "http://arxiv.org/abs/2511.07110v3", "date": "2026-01-29", "relevancy": 2.3794, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4693}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two%20Heads%20are%20Better%20than%20One%3A%20Distilling%20Large%20Language%20Model%20Features%20Into%20Small%20Models%20with%20Feature%20Decomposition%20and%20Mixture&body=Title%3A%20Two%20Heads%20are%20Better%20than%20One%3A%20Distilling%20Large%20Language%20Model%20Features%20Into%20Small%20Models%20with%20Feature%20Decomposition%20and%20Mixture%0AAuthor%3A%20Tianhao%20Fu%20and%20Xinxin%20Xu%20and%20Weichen%20Xu%20and%20Jue%20Chen%20and%20Ruilong%20Ren%20and%20Bowen%20Deng%20and%20Xinyu%20Zhao%20and%20Jian%20Cao%20and%20Xixin%20Cao%0AAbstract%3A%20Market%20making%20%28MM%29%20through%20Reinforcement%20Learning%20%28RL%29%20has%20attracted%20significant%20attention%20in%20financial%20trading.%20With%20the%20development%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20more%20and%20more%20attempts%20are%20being%20made%20to%20apply%20LLMs%20to%20financial%20areas.%20A%20simple%2C%20direct%20application%20of%20LLM%20as%20an%20agent%20shows%20significant%20performance.%20Such%20methods%20are%20hindered%20by%20their%20slow%20inference%20speed%2C%20while%20most%20of%20the%20current%20research%20has%20not%20studied%20LLM%20distillation%20for%20this%20specific%20task.%20To%20address%20this%2C%20we%20first%20propose%20the%20normalized%20fluorescent%20probe%20to%20study%20the%20mechanism%20of%20the%20LLM%27s%20feature.%20Based%20on%20the%20observation%20found%20by%20our%20investigation%2C%20we%20propose%20Cooperative%20Market%20Making%20%28CMM%29%2C%20a%20novel%20framework%20that%20decouples%20LLM%20features%20across%20three%20orthogonal%20dimensions%3A%20layer%2C%20task%2C%20and%20data.%20Various%20student%20models%20collaboratively%20learn%20simple%20LLM%20features%20along%20with%20different%20dimensions%2C%20with%20each%20model%20responsible%20for%20a%20distinct%20feature%20to%20achieve%20knowledge%20distillation.%20Furthermore%2C%20CMM%20introduces%20an%20H%C3%A1jek-MoE%20to%20integrate%20the%20output%20of%20the%20student%20models%20by%20investigating%20the%20contribution%20of%20different%20models%20in%20a%20kernel%20function-generated%20common%20feature%20space.%20Extensive%20experimental%20results%20on%20four%20real-world%20market%20datasets%20demonstrate%20the%20superiority%20of%20CMM%20over%20the%20current%20distillation%20method%20and%20RL-based%20market-making%20strategies.%0ALink%3A%20http%3A//arxiv.org/abs/2511.07110v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo%2520Heads%2520are%2520Better%2520than%2520One%253A%2520Distilling%2520Large%2520Language%2520Model%2520Features%2520Into%2520Small%2520Models%2520with%2520Feature%2520Decomposition%2520and%2520Mixture%26entry.906535625%3DTianhao%2520Fu%2520and%2520Xinxin%2520Xu%2520and%2520Weichen%2520Xu%2520and%2520Jue%2520Chen%2520and%2520Ruilong%2520Ren%2520and%2520Bowen%2520Deng%2520and%2520Xinyu%2520Zhao%2520and%2520Jian%2520Cao%2520and%2520Xixin%2520Cao%26entry.1292438233%3DMarket%2520making%2520%2528MM%2529%2520through%2520Reinforcement%2520Learning%2520%2528RL%2529%2520has%2520attracted%2520significant%2520attention%2520in%2520financial%2520trading.%2520With%2520the%2520development%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520more%2520and%2520more%2520attempts%2520are%2520being%2520made%2520to%2520apply%2520LLMs%2520to%2520financial%2520areas.%2520A%2520simple%252C%2520direct%2520application%2520of%2520LLM%2520as%2520an%2520agent%2520shows%2520significant%2520performance.%2520Such%2520methods%2520are%2520hindered%2520by%2520their%2520slow%2520inference%2520speed%252C%2520while%2520most%2520of%2520the%2520current%2520research%2520has%2520not%2520studied%2520LLM%2520distillation%2520for%2520this%2520specific%2520task.%2520To%2520address%2520this%252C%2520we%2520first%2520propose%2520the%2520normalized%2520fluorescent%2520probe%2520to%2520study%2520the%2520mechanism%2520of%2520the%2520LLM%2527s%2520feature.%2520Based%2520on%2520the%2520observation%2520found%2520by%2520our%2520investigation%252C%2520we%2520propose%2520Cooperative%2520Market%2520Making%2520%2528CMM%2529%252C%2520a%2520novel%2520framework%2520that%2520decouples%2520LLM%2520features%2520across%2520three%2520orthogonal%2520dimensions%253A%2520layer%252C%2520task%252C%2520and%2520data.%2520Various%2520student%2520models%2520collaboratively%2520learn%2520simple%2520LLM%2520features%2520along%2520with%2520different%2520dimensions%252C%2520with%2520each%2520model%2520responsible%2520for%2520a%2520distinct%2520feature%2520to%2520achieve%2520knowledge%2520distillation.%2520Furthermore%252C%2520CMM%2520introduces%2520an%2520H%25C3%25A1jek-MoE%2520to%2520integrate%2520the%2520output%2520of%2520the%2520student%2520models%2520by%2520investigating%2520the%2520contribution%2520of%2520different%2520models%2520in%2520a%2520kernel%2520function-generated%2520common%2520feature%2520space.%2520Extensive%2520experimental%2520results%2520on%2520four%2520real-world%2520market%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520CMM%2520over%2520the%2520current%2520distillation%2520method%2520and%2520RL-based%2520market-making%2520strategies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.07110v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Heads%20are%20Better%20than%20One%3A%20Distilling%20Large%20Language%20Model%20Features%20Into%20Small%20Models%20with%20Feature%20Decomposition%20and%20Mixture&entry.906535625=Tianhao%20Fu%20and%20Xinxin%20Xu%20and%20Weichen%20Xu%20and%20Jue%20Chen%20and%20Ruilong%20Ren%20and%20Bowen%20Deng%20and%20Xinyu%20Zhao%20and%20Jian%20Cao%20and%20Xixin%20Cao&entry.1292438233=Market%20making%20%28MM%29%20through%20Reinforcement%20Learning%20%28RL%29%20has%20attracted%20significant%20attention%20in%20financial%20trading.%20With%20the%20development%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20more%20and%20more%20attempts%20are%20being%20made%20to%20apply%20LLMs%20to%20financial%20areas.%20A%20simple%2C%20direct%20application%20of%20LLM%20as%20an%20agent%20shows%20significant%20performance.%20Such%20methods%20are%20hindered%20by%20their%20slow%20inference%20speed%2C%20while%20most%20of%20the%20current%20research%20has%20not%20studied%20LLM%20distillation%20for%20this%20specific%20task.%20To%20address%20this%2C%20we%20first%20propose%20the%20normalized%20fluorescent%20probe%20to%20study%20the%20mechanism%20of%20the%20LLM%27s%20feature.%20Based%20on%20the%20observation%20found%20by%20our%20investigation%2C%20we%20propose%20Cooperative%20Market%20Making%20%28CMM%29%2C%20a%20novel%20framework%20that%20decouples%20LLM%20features%20across%20three%20orthogonal%20dimensions%3A%20layer%2C%20task%2C%20and%20data.%20Various%20student%20models%20collaboratively%20learn%20simple%20LLM%20features%20along%20with%20different%20dimensions%2C%20with%20each%20model%20responsible%20for%20a%20distinct%20feature%20to%20achieve%20knowledge%20distillation.%20Furthermore%2C%20CMM%20introduces%20an%20H%C3%A1jek-MoE%20to%20integrate%20the%20output%20of%20the%20student%20models%20by%20investigating%20the%20contribution%20of%20different%20models%20in%20a%20kernel%20function-generated%20common%20feature%20space.%20Extensive%20experimental%20results%20on%20four%20real-world%20market%20datasets%20demonstrate%20the%20superiority%20of%20CMM%20over%20the%20current%20distillation%20method%20and%20RL-based%20market-making%20strategies.&entry.1838667208=http%3A//arxiv.org/abs/2511.07110v3&entry.124074799=Read"},
{"title": "Learning Transient Convective Heat Transfer with Geometry Aware World Models", "author": "Onur T. Doganay and Alexander Klawonn and Martin Eigel and Hanno Gottschalk", "abstract": "Partial differential equation (PDE) simulations are fundamental to engineering and physics but are often computationally prohibitive for real-time applications. While generative AI offers a promising avenue for surrogate modeling, standard video generation architectures lack the specific control and data compatibility required for physical simulations. This paper introduces a geometry aware world model architecture, derived from a video generation architecture (LongVideoGAN), designed to learn transient physics. We introduce two key architecture elements: (1) a twofold conditioning mechanism incorporating global physical parameters and local geometric masks, and (2) an architectural adaptation to support arbitrary channel dimensions, moving beyond standard RGB constraints. We evaluate this approach on a 2D transient computational fluid dynamics (CFD) problem involving convective heat transfer from buoyancy-driven flow coupled to a heat flow in a solid structure. We demonstrate that the conditioned model successfully reproduces complex temporal dynamics and spatial correlations of the training data. Furthermore, we assess the model's generalization capabilities on unseen geometric configurations, highlighting both its potential for controlled simulation synthesis and current limitations in spatial precision for out-of-distribution samples.", "link": "http://arxiv.org/abs/2601.22086v1", "date": "2026-01-29", "relevancy": 2.3787, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6123}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5939}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Transient%20Convective%20Heat%20Transfer%20with%20Geometry%20Aware%20World%20Models&body=Title%3A%20Learning%20Transient%20Convective%20Heat%20Transfer%20with%20Geometry%20Aware%20World%20Models%0AAuthor%3A%20Onur%20T.%20Doganay%20and%20Alexander%20Klawonn%20and%20Martin%20Eigel%20and%20Hanno%20Gottschalk%0AAbstract%3A%20Partial%20differential%20equation%20%28PDE%29%20simulations%20are%20fundamental%20to%20engineering%20and%20physics%20but%20are%20often%20computationally%20prohibitive%20for%20real-time%20applications.%20While%20generative%20AI%20offers%20a%20promising%20avenue%20for%20surrogate%20modeling%2C%20standard%20video%20generation%20architectures%20lack%20the%20specific%20control%20and%20data%20compatibility%20required%20for%20physical%20simulations.%20This%20paper%20introduces%20a%20geometry%20aware%20world%20model%20architecture%2C%20derived%20from%20a%20video%20generation%20architecture%20%28LongVideoGAN%29%2C%20designed%20to%20learn%20transient%20physics.%20We%20introduce%20two%20key%20architecture%20elements%3A%20%281%29%20a%20twofold%20conditioning%20mechanism%20incorporating%20global%20physical%20parameters%20and%20local%20geometric%20masks%2C%20and%20%282%29%20an%20architectural%20adaptation%20to%20support%20arbitrary%20channel%20dimensions%2C%20moving%20beyond%20standard%20RGB%20constraints.%20We%20evaluate%20this%20approach%20on%20a%202D%20transient%20computational%20fluid%20dynamics%20%28CFD%29%20problem%20involving%20convective%20heat%20transfer%20from%20buoyancy-driven%20flow%20coupled%20to%20a%20heat%20flow%20in%20a%20solid%20structure.%20We%20demonstrate%20that%20the%20conditioned%20model%20successfully%20reproduces%20complex%20temporal%20dynamics%20and%20spatial%20correlations%20of%20the%20training%20data.%20Furthermore%2C%20we%20assess%20the%20model%27s%20generalization%20capabilities%20on%20unseen%20geometric%20configurations%2C%20highlighting%20both%20its%20potential%20for%20controlled%20simulation%20synthesis%20and%20current%20limitations%20in%20spatial%20precision%20for%20out-of-distribution%20samples.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Transient%2520Convective%2520Heat%2520Transfer%2520with%2520Geometry%2520Aware%2520World%2520Models%26entry.906535625%3DOnur%2520T.%2520Doganay%2520and%2520Alexander%2520Klawonn%2520and%2520Martin%2520Eigel%2520and%2520Hanno%2520Gottschalk%26entry.1292438233%3DPartial%2520differential%2520equation%2520%2528PDE%2529%2520simulations%2520are%2520fundamental%2520to%2520engineering%2520and%2520physics%2520but%2520are%2520often%2520computationally%2520prohibitive%2520for%2520real-time%2520applications.%2520While%2520generative%2520AI%2520offers%2520a%2520promising%2520avenue%2520for%2520surrogate%2520modeling%252C%2520standard%2520video%2520generation%2520architectures%2520lack%2520the%2520specific%2520control%2520and%2520data%2520compatibility%2520required%2520for%2520physical%2520simulations.%2520This%2520paper%2520introduces%2520a%2520geometry%2520aware%2520world%2520model%2520architecture%252C%2520derived%2520from%2520a%2520video%2520generation%2520architecture%2520%2528LongVideoGAN%2529%252C%2520designed%2520to%2520learn%2520transient%2520physics.%2520We%2520introduce%2520two%2520key%2520architecture%2520elements%253A%2520%25281%2529%2520a%2520twofold%2520conditioning%2520mechanism%2520incorporating%2520global%2520physical%2520parameters%2520and%2520local%2520geometric%2520masks%252C%2520and%2520%25282%2529%2520an%2520architectural%2520adaptation%2520to%2520support%2520arbitrary%2520channel%2520dimensions%252C%2520moving%2520beyond%2520standard%2520RGB%2520constraints.%2520We%2520evaluate%2520this%2520approach%2520on%2520a%25202D%2520transient%2520computational%2520fluid%2520dynamics%2520%2528CFD%2529%2520problem%2520involving%2520convective%2520heat%2520transfer%2520from%2520buoyancy-driven%2520flow%2520coupled%2520to%2520a%2520heat%2520flow%2520in%2520a%2520solid%2520structure.%2520We%2520demonstrate%2520that%2520the%2520conditioned%2520model%2520successfully%2520reproduces%2520complex%2520temporal%2520dynamics%2520and%2520spatial%2520correlations%2520of%2520the%2520training%2520data.%2520Furthermore%252C%2520we%2520assess%2520the%2520model%2527s%2520generalization%2520capabilities%2520on%2520unseen%2520geometric%2520configurations%252C%2520highlighting%2520both%2520its%2520potential%2520for%2520controlled%2520simulation%2520synthesis%2520and%2520current%2520limitations%2520in%2520spatial%2520precision%2520for%2520out-of-distribution%2520samples.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Transient%20Convective%20Heat%20Transfer%20with%20Geometry%20Aware%20World%20Models&entry.906535625=Onur%20T.%20Doganay%20and%20Alexander%20Klawonn%20and%20Martin%20Eigel%20and%20Hanno%20Gottschalk&entry.1292438233=Partial%20differential%20equation%20%28PDE%29%20simulations%20are%20fundamental%20to%20engineering%20and%20physics%20but%20are%20often%20computationally%20prohibitive%20for%20real-time%20applications.%20While%20generative%20AI%20offers%20a%20promising%20avenue%20for%20surrogate%20modeling%2C%20standard%20video%20generation%20architectures%20lack%20the%20specific%20control%20and%20data%20compatibility%20required%20for%20physical%20simulations.%20This%20paper%20introduces%20a%20geometry%20aware%20world%20model%20architecture%2C%20derived%20from%20a%20video%20generation%20architecture%20%28LongVideoGAN%29%2C%20designed%20to%20learn%20transient%20physics.%20We%20introduce%20two%20key%20architecture%20elements%3A%20%281%29%20a%20twofold%20conditioning%20mechanism%20incorporating%20global%20physical%20parameters%20and%20local%20geometric%20masks%2C%20and%20%282%29%20an%20architectural%20adaptation%20to%20support%20arbitrary%20channel%20dimensions%2C%20moving%20beyond%20standard%20RGB%20constraints.%20We%20evaluate%20this%20approach%20on%20a%202D%20transient%20computational%20fluid%20dynamics%20%28CFD%29%20problem%20involving%20convective%20heat%20transfer%20from%20buoyancy-driven%20flow%20coupled%20to%20a%20heat%20flow%20in%20a%20solid%20structure.%20We%20demonstrate%20that%20the%20conditioned%20model%20successfully%20reproduces%20complex%20temporal%20dynamics%20and%20spatial%20correlations%20of%20the%20training%20data.%20Furthermore%2C%20we%20assess%20the%20model%27s%20generalization%20capabilities%20on%20unseen%20geometric%20configurations%2C%20highlighting%20both%20its%20potential%20for%20controlled%20simulation%20synthesis%20and%20current%20limitations%20in%20spatial%20precision%20for%20out-of-distribution%20samples.&entry.1838667208=http%3A//arxiv.org/abs/2601.22086v1&entry.124074799=Read"},
{"title": "Scalable Linearized Laplace Approximation via Surrogate Neural Kernel", "author": "Luis A. Ortega and Sim\u00f3n Rodr\u00edguez-Santana and Daniel Hern\u00e1ndez-Lobato", "abstract": "We introduce a scalable method to approximate the kernel of the Linearized Laplace Approximation (LLA). For this, we use a surrogate deep neural network (DNN) that learns a compact feature representation whose inner product replicates the Neural Tangent Kernel (NTK). This avoids the need to compute large Jacobians. Training relies solely on efficient Jacobian-vector products, allowing to compute predictive uncertainty on large-scale pre-trained DNNs. Experimental results show similar or improved uncertainty estimation and calibration compared to existing LLA approximations. Notwithstanding, biasing the learned kernel significantly enhances out-of-distribution detection. This remarks the benefits of the proposed method for finding better kernels than the NTK in the context of LLA to compute prediction uncertainty given a pre-trained DNN.", "link": "http://arxiv.org/abs/2601.21835v1", "date": "2026-01-29", "relevancy": 2.3775, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5018}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4654}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Linearized%20Laplace%20Approximation%20via%20Surrogate%20Neural%20Kernel&body=Title%3A%20Scalable%20Linearized%20Laplace%20Approximation%20via%20Surrogate%20Neural%20Kernel%0AAuthor%3A%20Luis%20A.%20Ortega%20and%20Sim%C3%B3n%20Rodr%C3%ADguez-Santana%20and%20Daniel%20Hern%C3%A1ndez-Lobato%0AAbstract%3A%20We%20introduce%20a%20scalable%20method%20to%20approximate%20the%20kernel%20of%20the%20Linearized%20Laplace%20Approximation%20%28LLA%29.%20For%20this%2C%20we%20use%20a%20surrogate%20deep%20neural%20network%20%28DNN%29%20that%20learns%20a%20compact%20feature%20representation%20whose%20inner%20product%20replicates%20the%20Neural%20Tangent%20Kernel%20%28NTK%29.%20This%20avoids%20the%20need%20to%20compute%20large%20Jacobians.%20Training%20relies%20solely%20on%20efficient%20Jacobian-vector%20products%2C%20allowing%20to%20compute%20predictive%20uncertainty%20on%20large-scale%20pre-trained%20DNNs.%20Experimental%20results%20show%20similar%20or%20improved%20uncertainty%20estimation%20and%20calibration%20compared%20to%20existing%20LLA%20approximations.%20Notwithstanding%2C%20biasing%20the%20learned%20kernel%20significantly%20enhances%20out-of-distribution%20detection.%20This%20remarks%20the%20benefits%20of%20the%20proposed%20method%20for%20finding%20better%20kernels%20than%20the%20NTK%20in%20the%20context%20of%20LLA%20to%20compute%20prediction%20uncertainty%20given%20a%20pre-trained%20DNN.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Linearized%2520Laplace%2520Approximation%2520via%2520Surrogate%2520Neural%2520Kernel%26entry.906535625%3DLuis%2520A.%2520Ortega%2520and%2520Sim%25C3%25B3n%2520Rodr%25C3%25ADguez-Santana%2520and%2520Daniel%2520Hern%25C3%25A1ndez-Lobato%26entry.1292438233%3DWe%2520introduce%2520a%2520scalable%2520method%2520to%2520approximate%2520the%2520kernel%2520of%2520the%2520Linearized%2520Laplace%2520Approximation%2520%2528LLA%2529.%2520For%2520this%252C%2520we%2520use%2520a%2520surrogate%2520deep%2520neural%2520network%2520%2528DNN%2529%2520that%2520learns%2520a%2520compact%2520feature%2520representation%2520whose%2520inner%2520product%2520replicates%2520the%2520Neural%2520Tangent%2520Kernel%2520%2528NTK%2529.%2520This%2520avoids%2520the%2520need%2520to%2520compute%2520large%2520Jacobians.%2520Training%2520relies%2520solely%2520on%2520efficient%2520Jacobian-vector%2520products%252C%2520allowing%2520to%2520compute%2520predictive%2520uncertainty%2520on%2520large-scale%2520pre-trained%2520DNNs.%2520Experimental%2520results%2520show%2520similar%2520or%2520improved%2520uncertainty%2520estimation%2520and%2520calibration%2520compared%2520to%2520existing%2520LLA%2520approximations.%2520Notwithstanding%252C%2520biasing%2520the%2520learned%2520kernel%2520significantly%2520enhances%2520out-of-distribution%2520detection.%2520This%2520remarks%2520the%2520benefits%2520of%2520the%2520proposed%2520method%2520for%2520finding%2520better%2520kernels%2520than%2520the%2520NTK%2520in%2520the%2520context%2520of%2520LLA%2520to%2520compute%2520prediction%2520uncertainty%2520given%2520a%2520pre-trained%2520DNN.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Linearized%20Laplace%20Approximation%20via%20Surrogate%20Neural%20Kernel&entry.906535625=Luis%20A.%20Ortega%20and%20Sim%C3%B3n%20Rodr%C3%ADguez-Santana%20and%20Daniel%20Hern%C3%A1ndez-Lobato&entry.1292438233=We%20introduce%20a%20scalable%20method%20to%20approximate%20the%20kernel%20of%20the%20Linearized%20Laplace%20Approximation%20%28LLA%29.%20For%20this%2C%20we%20use%20a%20surrogate%20deep%20neural%20network%20%28DNN%29%20that%20learns%20a%20compact%20feature%20representation%20whose%20inner%20product%20replicates%20the%20Neural%20Tangent%20Kernel%20%28NTK%29.%20This%20avoids%20the%20need%20to%20compute%20large%20Jacobians.%20Training%20relies%20solely%20on%20efficient%20Jacobian-vector%20products%2C%20allowing%20to%20compute%20predictive%20uncertainty%20on%20large-scale%20pre-trained%20DNNs.%20Experimental%20results%20show%20similar%20or%20improved%20uncertainty%20estimation%20and%20calibration%20compared%20to%20existing%20LLA%20approximations.%20Notwithstanding%2C%20biasing%20the%20learned%20kernel%20significantly%20enhances%20out-of-distribution%20detection.%20This%20remarks%20the%20benefits%20of%20the%20proposed%20method%20for%20finding%20better%20kernels%20than%20the%20NTK%20in%20the%20context%20of%20LLA%20to%20compute%20prediction%20uncertainty%20given%20a%20pre-trained%20DNN.&entry.1838667208=http%3A//arxiv.org/abs/2601.21835v1&entry.124074799=Read"},
{"title": "EROAM: Event-based Camera Rotational Odometry and Mapping in Real-time", "author": "Wanli Xing and Shijie Lin and Linhan Yang and Zeqing Zhang and Yanjun Du and Maolin Lei and Yipeng Pan and Chen Wang and Jia Pan", "abstract": "This paper presents EROAM, a novel event-based rotational odometry and mapping system that achieves real-time, accurate camera rotation estimation. Unlike existing approaches that rely on event generation models or contrast maximization, EROAM employs a spherical event representation by projecting events onto a unit sphere and introduces Event Spherical Iterative Closest Point (ES-ICP), a novel geometric optimization framework designed specifically for event camera data. The spherical representation simplifies rotational motion formulation while operating in a continuous spherical domain, enabling enhanced spatial resolution. Our system features an efficient map management approach using incremental k-d tree structures and intelligent regional density control, ensuring optimal computational performance during long-term operation. Combined with parallel point-to-line optimization, EROAM achieves efficient computation without compromising accuracy. Extensive experiments on both synthetic and real-world datasets show that EROAM significantly outperforms state-of-the-art methods in terms of accuracy, robustness, and computational efficiency. Our method maintains consistent performance under challenging conditions, including high angular velocities and extended sequences, where other methods often fail or show significant drift. Additionally, EROAM produces high-quality panoramic reconstructions with preserved fine structural details.", "link": "http://arxiv.org/abs/2411.11004v2", "date": "2026-01-29", "relevancy": 2.3773, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6098}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6011}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EROAM%3A%20Event-based%20Camera%20Rotational%20Odometry%20and%20Mapping%20in%20Real-time&body=Title%3A%20EROAM%3A%20Event-based%20Camera%20Rotational%20Odometry%20and%20Mapping%20in%20Real-time%0AAuthor%3A%20Wanli%20Xing%20and%20Shijie%20Lin%20and%20Linhan%20Yang%20and%20Zeqing%20Zhang%20and%20Yanjun%20Du%20and%20Maolin%20Lei%20and%20Yipeng%20Pan%20and%20Chen%20Wang%20and%20Jia%20Pan%0AAbstract%3A%20This%20paper%20presents%20EROAM%2C%20a%20novel%20event-based%20rotational%20odometry%20and%20mapping%20system%20that%20achieves%20real-time%2C%20accurate%20camera%20rotation%20estimation.%20Unlike%20existing%20approaches%20that%20rely%20on%20event%20generation%20models%20or%20contrast%20maximization%2C%20EROAM%20employs%20a%20spherical%20event%20representation%20by%20projecting%20events%20onto%20a%20unit%20sphere%20and%20introduces%20Event%20Spherical%20Iterative%20Closest%20Point%20%28ES-ICP%29%2C%20a%20novel%20geometric%20optimization%20framework%20designed%20specifically%20for%20event%20camera%20data.%20The%20spherical%20representation%20simplifies%20rotational%20motion%20formulation%20while%20operating%20in%20a%20continuous%20spherical%20domain%2C%20enabling%20enhanced%20spatial%20resolution.%20Our%20system%20features%20an%20efficient%20map%20management%20approach%20using%20incremental%20k-d%20tree%20structures%20and%20intelligent%20regional%20density%20control%2C%20ensuring%20optimal%20computational%20performance%20during%20long-term%20operation.%20Combined%20with%20parallel%20point-to-line%20optimization%2C%20EROAM%20achieves%20efficient%20computation%20without%20compromising%20accuracy.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20show%20that%20EROAM%20significantly%20outperforms%20state-of-the-art%20methods%20in%20terms%20of%20accuracy%2C%20robustness%2C%20and%20computational%20efficiency.%20Our%20method%20maintains%20consistent%20performance%20under%20challenging%20conditions%2C%20including%20high%20angular%20velocities%20and%20extended%20sequences%2C%20where%20other%20methods%20often%20fail%20or%20show%20significant%20drift.%20Additionally%2C%20EROAM%20produces%20high-quality%20panoramic%20reconstructions%20with%20preserved%20fine%20structural%20details.%0ALink%3A%20http%3A//arxiv.org/abs/2411.11004v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEROAM%253A%2520Event-based%2520Camera%2520Rotational%2520Odometry%2520and%2520Mapping%2520in%2520Real-time%26entry.906535625%3DWanli%2520Xing%2520and%2520Shijie%2520Lin%2520and%2520Linhan%2520Yang%2520and%2520Zeqing%2520Zhang%2520and%2520Yanjun%2520Du%2520and%2520Maolin%2520Lei%2520and%2520Yipeng%2520Pan%2520and%2520Chen%2520Wang%2520and%2520Jia%2520Pan%26entry.1292438233%3DThis%2520paper%2520presents%2520EROAM%252C%2520a%2520novel%2520event-based%2520rotational%2520odometry%2520and%2520mapping%2520system%2520that%2520achieves%2520real-time%252C%2520accurate%2520camera%2520rotation%2520estimation.%2520Unlike%2520existing%2520approaches%2520that%2520rely%2520on%2520event%2520generation%2520models%2520or%2520contrast%2520maximization%252C%2520EROAM%2520employs%2520a%2520spherical%2520event%2520representation%2520by%2520projecting%2520events%2520onto%2520a%2520unit%2520sphere%2520and%2520introduces%2520Event%2520Spherical%2520Iterative%2520Closest%2520Point%2520%2528ES-ICP%2529%252C%2520a%2520novel%2520geometric%2520optimization%2520framework%2520designed%2520specifically%2520for%2520event%2520camera%2520data.%2520The%2520spherical%2520representation%2520simplifies%2520rotational%2520motion%2520formulation%2520while%2520operating%2520in%2520a%2520continuous%2520spherical%2520domain%252C%2520enabling%2520enhanced%2520spatial%2520resolution.%2520Our%2520system%2520features%2520an%2520efficient%2520map%2520management%2520approach%2520using%2520incremental%2520k-d%2520tree%2520structures%2520and%2520intelligent%2520regional%2520density%2520control%252C%2520ensuring%2520optimal%2520computational%2520performance%2520during%2520long-term%2520operation.%2520Combined%2520with%2520parallel%2520point-to-line%2520optimization%252C%2520EROAM%2520achieves%2520efficient%2520computation%2520without%2520compromising%2520accuracy.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520show%2520that%2520EROAM%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520accuracy%252C%2520robustness%252C%2520and%2520computational%2520efficiency.%2520Our%2520method%2520maintains%2520consistent%2520performance%2520under%2520challenging%2520conditions%252C%2520including%2520high%2520angular%2520velocities%2520and%2520extended%2520sequences%252C%2520where%2520other%2520methods%2520often%2520fail%2520or%2520show%2520significant%2520drift.%2520Additionally%252C%2520EROAM%2520produces%2520high-quality%2520panoramic%2520reconstructions%2520with%2520preserved%2520fine%2520structural%2520details.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11004v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EROAM%3A%20Event-based%20Camera%20Rotational%20Odometry%20and%20Mapping%20in%20Real-time&entry.906535625=Wanli%20Xing%20and%20Shijie%20Lin%20and%20Linhan%20Yang%20and%20Zeqing%20Zhang%20and%20Yanjun%20Du%20and%20Maolin%20Lei%20and%20Yipeng%20Pan%20and%20Chen%20Wang%20and%20Jia%20Pan&entry.1292438233=This%20paper%20presents%20EROAM%2C%20a%20novel%20event-based%20rotational%20odometry%20and%20mapping%20system%20that%20achieves%20real-time%2C%20accurate%20camera%20rotation%20estimation.%20Unlike%20existing%20approaches%20that%20rely%20on%20event%20generation%20models%20or%20contrast%20maximization%2C%20EROAM%20employs%20a%20spherical%20event%20representation%20by%20projecting%20events%20onto%20a%20unit%20sphere%20and%20introduces%20Event%20Spherical%20Iterative%20Closest%20Point%20%28ES-ICP%29%2C%20a%20novel%20geometric%20optimization%20framework%20designed%20specifically%20for%20event%20camera%20data.%20The%20spherical%20representation%20simplifies%20rotational%20motion%20formulation%20while%20operating%20in%20a%20continuous%20spherical%20domain%2C%20enabling%20enhanced%20spatial%20resolution.%20Our%20system%20features%20an%20efficient%20map%20management%20approach%20using%20incremental%20k-d%20tree%20structures%20and%20intelligent%20regional%20density%20control%2C%20ensuring%20optimal%20computational%20performance%20during%20long-term%20operation.%20Combined%20with%20parallel%20point-to-line%20optimization%2C%20EROAM%20achieves%20efficient%20computation%20without%20compromising%20accuracy.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20show%20that%20EROAM%20significantly%20outperforms%20state-of-the-art%20methods%20in%20terms%20of%20accuracy%2C%20robustness%2C%20and%20computational%20efficiency.%20Our%20method%20maintains%20consistent%20performance%20under%20challenging%20conditions%2C%20including%20high%20angular%20velocities%20and%20extended%20sequences%2C%20where%20other%20methods%20often%20fail%20or%20show%20significant%20drift.%20Additionally%2C%20EROAM%20produces%20high-quality%20panoramic%20reconstructions%20with%20preserved%20fine%20structural%20details.&entry.1838667208=http%3A//arxiv.org/abs/2411.11004v2&entry.124074799=Read"},
{"title": "JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory Generation", "author": "Guillem Capellera and Luis Ferraz and Antonio Rubio and Alexandre Alahi and Antonio Agudo", "abstract": "Generative models often treat continuous data and discrete events as separate processes, creating a gap in modeling complex systems where they interact synchronously. To bridge this gap, we introduce JointDiff, a novel diffusion framework designed to unify these two processes by simultaneously generating continuous spatio-temporal data and synchronous discrete events. We demonstrate its efficacy in the sports domain by simultaneously modeling multi-agent trajectories and key possession events. This joint modeling is validated with non-controllable generation and two novel controllable generation scenarios: weak-possessor-guidance, which offers flexible semantic control over game dynamics through a simple list of intended ball possessors, and text-guidance, which enables fine-grained, language-driven generation. To enable the conditioning with these guidance signals, we introduce CrossGuid, an effective conditioning operation for multi-agent domains. We also share a new unified sports benchmark enhanced with textual descriptions for soccer and football datasets. JointDiff achieves state-of-the-art performance, demonstrating that joint modeling is crucial for building realistic and controllable generative models for interactive systems. https://guillem-cf.github.io/JointDiff/", "link": "http://arxiv.org/abs/2509.22522v3", "date": "2026-01-29", "relevancy": 2.343, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5996}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5776}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JointDiff%3A%20Bridging%20Continuous%20and%20Discrete%20in%20Multi-Agent%20Trajectory%20Generation&body=Title%3A%20JointDiff%3A%20Bridging%20Continuous%20and%20Discrete%20in%20Multi-Agent%20Trajectory%20Generation%0AAuthor%3A%20Guillem%20Capellera%20and%20Luis%20Ferraz%20and%20Antonio%20Rubio%20and%20Alexandre%20Alahi%20and%20Antonio%20Agudo%0AAbstract%3A%20Generative%20models%20often%20treat%20continuous%20data%20and%20discrete%20events%20as%20separate%20processes%2C%20creating%20a%20gap%20in%20modeling%20complex%20systems%20where%20they%20interact%20synchronously.%20To%20bridge%20this%20gap%2C%20we%20introduce%20JointDiff%2C%20a%20novel%20diffusion%20framework%20designed%20to%20unify%20these%20two%20processes%20by%20simultaneously%20generating%20continuous%20spatio-temporal%20data%20and%20synchronous%20discrete%20events.%20We%20demonstrate%20its%20efficacy%20in%20the%20sports%20domain%20by%20simultaneously%20modeling%20multi-agent%20trajectories%20and%20key%20possession%20events.%20This%20joint%20modeling%20is%20validated%20with%20non-controllable%20generation%20and%20two%20novel%20controllable%20generation%20scenarios%3A%20weak-possessor-guidance%2C%20which%20offers%20flexible%20semantic%20control%20over%20game%20dynamics%20through%20a%20simple%20list%20of%20intended%20ball%20possessors%2C%20and%20text-guidance%2C%20which%20enables%20fine-grained%2C%20language-driven%20generation.%20To%20enable%20the%20conditioning%20with%20these%20guidance%20signals%2C%20we%20introduce%20CrossGuid%2C%20an%20effective%20conditioning%20operation%20for%20multi-agent%20domains.%20We%20also%20share%20a%20new%20unified%20sports%20benchmark%20enhanced%20with%20textual%20descriptions%20for%20soccer%20and%20football%20datasets.%20JointDiff%20achieves%20state-of-the-art%20performance%2C%20demonstrating%20that%20joint%20modeling%20is%20crucial%20for%20building%20realistic%20and%20controllable%20generative%20models%20for%20interactive%20systems.%20https%3A//guillem-cf.github.io/JointDiff/%0ALink%3A%20http%3A//arxiv.org/abs/2509.22522v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJointDiff%253A%2520Bridging%2520Continuous%2520and%2520Discrete%2520in%2520Multi-Agent%2520Trajectory%2520Generation%26entry.906535625%3DGuillem%2520Capellera%2520and%2520Luis%2520Ferraz%2520and%2520Antonio%2520Rubio%2520and%2520Alexandre%2520Alahi%2520and%2520Antonio%2520Agudo%26entry.1292438233%3DGenerative%2520models%2520often%2520treat%2520continuous%2520data%2520and%2520discrete%2520events%2520as%2520separate%2520processes%252C%2520creating%2520a%2520gap%2520in%2520modeling%2520complex%2520systems%2520where%2520they%2520interact%2520synchronously.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520JointDiff%252C%2520a%2520novel%2520diffusion%2520framework%2520designed%2520to%2520unify%2520these%2520two%2520processes%2520by%2520simultaneously%2520generating%2520continuous%2520spatio-temporal%2520data%2520and%2520synchronous%2520discrete%2520events.%2520We%2520demonstrate%2520its%2520efficacy%2520in%2520the%2520sports%2520domain%2520by%2520simultaneously%2520modeling%2520multi-agent%2520trajectories%2520and%2520key%2520possession%2520events.%2520This%2520joint%2520modeling%2520is%2520validated%2520with%2520non-controllable%2520generation%2520and%2520two%2520novel%2520controllable%2520generation%2520scenarios%253A%2520weak-possessor-guidance%252C%2520which%2520offers%2520flexible%2520semantic%2520control%2520over%2520game%2520dynamics%2520through%2520a%2520simple%2520list%2520of%2520intended%2520ball%2520possessors%252C%2520and%2520text-guidance%252C%2520which%2520enables%2520fine-grained%252C%2520language-driven%2520generation.%2520To%2520enable%2520the%2520conditioning%2520with%2520these%2520guidance%2520signals%252C%2520we%2520introduce%2520CrossGuid%252C%2520an%2520effective%2520conditioning%2520operation%2520for%2520multi-agent%2520domains.%2520We%2520also%2520share%2520a%2520new%2520unified%2520sports%2520benchmark%2520enhanced%2520with%2520textual%2520descriptions%2520for%2520soccer%2520and%2520football%2520datasets.%2520JointDiff%2520achieves%2520state-of-the-art%2520performance%252C%2520demonstrating%2520that%2520joint%2520modeling%2520is%2520crucial%2520for%2520building%2520realistic%2520and%2520controllable%2520generative%2520models%2520for%2520interactive%2520systems.%2520https%253A//guillem-cf.github.io/JointDiff/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22522v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JointDiff%3A%20Bridging%20Continuous%20and%20Discrete%20in%20Multi-Agent%20Trajectory%20Generation&entry.906535625=Guillem%20Capellera%20and%20Luis%20Ferraz%20and%20Antonio%20Rubio%20and%20Alexandre%20Alahi%20and%20Antonio%20Agudo&entry.1292438233=Generative%20models%20often%20treat%20continuous%20data%20and%20discrete%20events%20as%20separate%20processes%2C%20creating%20a%20gap%20in%20modeling%20complex%20systems%20where%20they%20interact%20synchronously.%20To%20bridge%20this%20gap%2C%20we%20introduce%20JointDiff%2C%20a%20novel%20diffusion%20framework%20designed%20to%20unify%20these%20two%20processes%20by%20simultaneously%20generating%20continuous%20spatio-temporal%20data%20and%20synchronous%20discrete%20events.%20We%20demonstrate%20its%20efficacy%20in%20the%20sports%20domain%20by%20simultaneously%20modeling%20multi-agent%20trajectories%20and%20key%20possession%20events.%20This%20joint%20modeling%20is%20validated%20with%20non-controllable%20generation%20and%20two%20novel%20controllable%20generation%20scenarios%3A%20weak-possessor-guidance%2C%20which%20offers%20flexible%20semantic%20control%20over%20game%20dynamics%20through%20a%20simple%20list%20of%20intended%20ball%20possessors%2C%20and%20text-guidance%2C%20which%20enables%20fine-grained%2C%20language-driven%20generation.%20To%20enable%20the%20conditioning%20with%20these%20guidance%20signals%2C%20we%20introduce%20CrossGuid%2C%20an%20effective%20conditioning%20operation%20for%20multi-agent%20domains.%20We%20also%20share%20a%20new%20unified%20sports%20benchmark%20enhanced%20with%20textual%20descriptions%20for%20soccer%20and%20football%20datasets.%20JointDiff%20achieves%20state-of-the-art%20performance%2C%20demonstrating%20that%20joint%20modeling%20is%20crucial%20for%20building%20realistic%20and%20controllable%20generative%20models%20for%20interactive%20systems.%20https%3A//guillem-cf.github.io/JointDiff/&entry.1838667208=http%3A//arxiv.org/abs/2509.22522v3&entry.124074799=Read"},
{"title": "Shaping capabilities with token-level data filtering", "author": "Neil Rathi and Alec Radford", "abstract": "Current approaches to reducing undesired capabilities in language models are largely post hoc, and can thus be easily bypassed by adversaries. A natural alternative is to shape capabilities during pretraining itself. On the proxy task of removing medical capabilities, we show that the simple intervention of filtering pretraining data is highly effective, robust, and inexpensive at scale. Inspired by work on data attribution, we show that filtering tokens is more effective than filtering documents, achieving the same hit to undesired capabilities at a lower cost to benign ones. Training models spanning two orders of magnitude, we then demonstrate that filtering gets more effective with scale: for our largest models, token filtering leads to a 7000x compute slowdown on the forget domain. We also show that models trained with token filtering can still be aligned on the forget domain. Along the way, we introduce a methodology for labeling tokens with sparse autoencoders and distilling cheap, high-quality classifiers. We also demonstrate that filtering can be robust to noisy labels with sufficient pretraining compute.", "link": "http://arxiv.org/abs/2601.21571v1", "date": "2026-01-29", "relevancy": 2.3335, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5038}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4535}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shaping%20capabilities%20with%20token-level%20data%20filtering&body=Title%3A%20Shaping%20capabilities%20with%20token-level%20data%20filtering%0AAuthor%3A%20Neil%20Rathi%20and%20Alec%20Radford%0AAbstract%3A%20Current%20approaches%20to%20reducing%20undesired%20capabilities%20in%20language%20models%20are%20largely%20post%20hoc%2C%20and%20can%20thus%20be%20easily%20bypassed%20by%20adversaries.%20A%20natural%20alternative%20is%20to%20shape%20capabilities%20during%20pretraining%20itself.%20On%20the%20proxy%20task%20of%20removing%20medical%20capabilities%2C%20we%20show%20that%20the%20simple%20intervention%20of%20filtering%20pretraining%20data%20is%20highly%20effective%2C%20robust%2C%20and%20inexpensive%20at%20scale.%20Inspired%20by%20work%20on%20data%20attribution%2C%20we%20show%20that%20filtering%20tokens%20is%20more%20effective%20than%20filtering%20documents%2C%20achieving%20the%20same%20hit%20to%20undesired%20capabilities%20at%20a%20lower%20cost%20to%20benign%20ones.%20Training%20models%20spanning%20two%20orders%20of%20magnitude%2C%20we%20then%20demonstrate%20that%20filtering%20gets%20more%20effective%20with%20scale%3A%20for%20our%20largest%20models%2C%20token%20filtering%20leads%20to%20a%207000x%20compute%20slowdown%20on%20the%20forget%20domain.%20We%20also%20show%20that%20models%20trained%20with%20token%20filtering%20can%20still%20be%20aligned%20on%20the%20forget%20domain.%20Along%20the%20way%2C%20we%20introduce%20a%20methodology%20for%20labeling%20tokens%20with%20sparse%20autoencoders%20and%20distilling%20cheap%2C%20high-quality%20classifiers.%20We%20also%20demonstrate%20that%20filtering%20can%20be%20robust%20to%20noisy%20labels%20with%20sufficient%20pretraining%20compute.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShaping%2520capabilities%2520with%2520token-level%2520data%2520filtering%26entry.906535625%3DNeil%2520Rathi%2520and%2520Alec%2520Radford%26entry.1292438233%3DCurrent%2520approaches%2520to%2520reducing%2520undesired%2520capabilities%2520in%2520language%2520models%2520are%2520largely%2520post%2520hoc%252C%2520and%2520can%2520thus%2520be%2520easily%2520bypassed%2520by%2520adversaries.%2520A%2520natural%2520alternative%2520is%2520to%2520shape%2520capabilities%2520during%2520pretraining%2520itself.%2520On%2520the%2520proxy%2520task%2520of%2520removing%2520medical%2520capabilities%252C%2520we%2520show%2520that%2520the%2520simple%2520intervention%2520of%2520filtering%2520pretraining%2520data%2520is%2520highly%2520effective%252C%2520robust%252C%2520and%2520inexpensive%2520at%2520scale.%2520Inspired%2520by%2520work%2520on%2520data%2520attribution%252C%2520we%2520show%2520that%2520filtering%2520tokens%2520is%2520more%2520effective%2520than%2520filtering%2520documents%252C%2520achieving%2520the%2520same%2520hit%2520to%2520undesired%2520capabilities%2520at%2520a%2520lower%2520cost%2520to%2520benign%2520ones.%2520Training%2520models%2520spanning%2520two%2520orders%2520of%2520magnitude%252C%2520we%2520then%2520demonstrate%2520that%2520filtering%2520gets%2520more%2520effective%2520with%2520scale%253A%2520for%2520our%2520largest%2520models%252C%2520token%2520filtering%2520leads%2520to%2520a%25207000x%2520compute%2520slowdown%2520on%2520the%2520forget%2520domain.%2520We%2520also%2520show%2520that%2520models%2520trained%2520with%2520token%2520filtering%2520can%2520still%2520be%2520aligned%2520on%2520the%2520forget%2520domain.%2520Along%2520the%2520way%252C%2520we%2520introduce%2520a%2520methodology%2520for%2520labeling%2520tokens%2520with%2520sparse%2520autoencoders%2520and%2520distilling%2520cheap%252C%2520high-quality%2520classifiers.%2520We%2520also%2520demonstrate%2520that%2520filtering%2520can%2520be%2520robust%2520to%2520noisy%2520labels%2520with%2520sufficient%2520pretraining%2520compute.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shaping%20capabilities%20with%20token-level%20data%20filtering&entry.906535625=Neil%20Rathi%20and%20Alec%20Radford&entry.1292438233=Current%20approaches%20to%20reducing%20undesired%20capabilities%20in%20language%20models%20are%20largely%20post%20hoc%2C%20and%20can%20thus%20be%20easily%20bypassed%20by%20adversaries.%20A%20natural%20alternative%20is%20to%20shape%20capabilities%20during%20pretraining%20itself.%20On%20the%20proxy%20task%20of%20removing%20medical%20capabilities%2C%20we%20show%20that%20the%20simple%20intervention%20of%20filtering%20pretraining%20data%20is%20highly%20effective%2C%20robust%2C%20and%20inexpensive%20at%20scale.%20Inspired%20by%20work%20on%20data%20attribution%2C%20we%20show%20that%20filtering%20tokens%20is%20more%20effective%20than%20filtering%20documents%2C%20achieving%20the%20same%20hit%20to%20undesired%20capabilities%20at%20a%20lower%20cost%20to%20benign%20ones.%20Training%20models%20spanning%20two%20orders%20of%20magnitude%2C%20we%20then%20demonstrate%20that%20filtering%20gets%20more%20effective%20with%20scale%3A%20for%20our%20largest%20models%2C%20token%20filtering%20leads%20to%20a%207000x%20compute%20slowdown%20on%20the%20forget%20domain.%20We%20also%20show%20that%20models%20trained%20with%20token%20filtering%20can%20still%20be%20aligned%20on%20the%20forget%20domain.%20Along%20the%20way%2C%20we%20introduce%20a%20methodology%20for%20labeling%20tokens%20with%20sparse%20autoencoders%20and%20distilling%20cheap%2C%20high-quality%20classifiers.%20We%20also%20demonstrate%20that%20filtering%20can%20be%20robust%20to%20noisy%20labels%20with%20sufficient%20pretraining%20compute.&entry.1838667208=http%3A//arxiv.org/abs/2601.21571v1&entry.124074799=Read"},
{"title": "Managing Solution Stability in Decision-Focused Learning with Cost Regularization", "author": "Victor Spitzer and Francois Sanson", "abstract": "Decision-focused learning integrates predictive modeling and combinatorial optimization by training models to directly improve decision quality rather than prediction accuracy alone. Differentiating through combinatorial optimization problems represents a central challenge, and recent approaches tackle this difficulty by introducing perturbation-based approximations. In this work, we focus on estimating the objective function coefficients of a combinatorial optimization problem. Our study demonstrates that fluctuations in perturbation intensity occurring during the learning phase can lead to ineffective training, by establishing a theoretical link to the notion of solution stability in combinatorial optimization. We propose addressing this issue by introducing a regularization of the estimated cost vectors which improves the robustness and reliability of the learning process, as demonstrated by extensive numerical experiments.", "link": "http://arxiv.org/abs/2601.21883v1", "date": "2026-01-29", "relevancy": 2.3233, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.473}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4696}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Managing%20Solution%20Stability%20in%20Decision-Focused%20Learning%20with%20Cost%20Regularization&body=Title%3A%20Managing%20Solution%20Stability%20in%20Decision-Focused%20Learning%20with%20Cost%20Regularization%0AAuthor%3A%20Victor%20Spitzer%20and%20Francois%20Sanson%0AAbstract%3A%20Decision-focused%20learning%20integrates%20predictive%20modeling%20and%20combinatorial%20optimization%20by%20training%20models%20to%20directly%20improve%20decision%20quality%20rather%20than%20prediction%20accuracy%20alone.%20Differentiating%20through%20combinatorial%20optimization%20problems%20represents%20a%20central%20challenge%2C%20and%20recent%20approaches%20tackle%20this%20difficulty%20by%20introducing%20perturbation-based%20approximations.%20In%20this%20work%2C%20we%20focus%20on%20estimating%20the%20objective%20function%20coefficients%20of%20a%20combinatorial%20optimization%20problem.%20Our%20study%20demonstrates%20that%20fluctuations%20in%20perturbation%20intensity%20occurring%20during%20the%20learning%20phase%20can%20lead%20to%20ineffective%20training%2C%20by%20establishing%20a%20theoretical%20link%20to%20the%20notion%20of%20solution%20stability%20in%20combinatorial%20optimization.%20We%20propose%20addressing%20this%20issue%20by%20introducing%20a%20regularization%20of%20the%20estimated%20cost%20vectors%20which%20improves%20the%20robustness%20and%20reliability%20of%20the%20learning%20process%2C%20as%20demonstrated%20by%20extensive%20numerical%20experiments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DManaging%2520Solution%2520Stability%2520in%2520Decision-Focused%2520Learning%2520with%2520Cost%2520Regularization%26entry.906535625%3DVictor%2520Spitzer%2520and%2520Francois%2520Sanson%26entry.1292438233%3DDecision-focused%2520learning%2520integrates%2520predictive%2520modeling%2520and%2520combinatorial%2520optimization%2520by%2520training%2520models%2520to%2520directly%2520improve%2520decision%2520quality%2520rather%2520than%2520prediction%2520accuracy%2520alone.%2520Differentiating%2520through%2520combinatorial%2520optimization%2520problems%2520represents%2520a%2520central%2520challenge%252C%2520and%2520recent%2520approaches%2520tackle%2520this%2520difficulty%2520by%2520introducing%2520perturbation-based%2520approximations.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520estimating%2520the%2520objective%2520function%2520coefficients%2520of%2520a%2520combinatorial%2520optimization%2520problem.%2520Our%2520study%2520demonstrates%2520that%2520fluctuations%2520in%2520perturbation%2520intensity%2520occurring%2520during%2520the%2520learning%2520phase%2520can%2520lead%2520to%2520ineffective%2520training%252C%2520by%2520establishing%2520a%2520theoretical%2520link%2520to%2520the%2520notion%2520of%2520solution%2520stability%2520in%2520combinatorial%2520optimization.%2520We%2520propose%2520addressing%2520this%2520issue%2520by%2520introducing%2520a%2520regularization%2520of%2520the%2520estimated%2520cost%2520vectors%2520which%2520improves%2520the%2520robustness%2520and%2520reliability%2520of%2520the%2520learning%2520process%252C%2520as%2520demonstrated%2520by%2520extensive%2520numerical%2520experiments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Managing%20Solution%20Stability%20in%20Decision-Focused%20Learning%20with%20Cost%20Regularization&entry.906535625=Victor%20Spitzer%20and%20Francois%20Sanson&entry.1292438233=Decision-focused%20learning%20integrates%20predictive%20modeling%20and%20combinatorial%20optimization%20by%20training%20models%20to%20directly%20improve%20decision%20quality%20rather%20than%20prediction%20accuracy%20alone.%20Differentiating%20through%20combinatorial%20optimization%20problems%20represents%20a%20central%20challenge%2C%20and%20recent%20approaches%20tackle%20this%20difficulty%20by%20introducing%20perturbation-based%20approximations.%20In%20this%20work%2C%20we%20focus%20on%20estimating%20the%20objective%20function%20coefficients%20of%20a%20combinatorial%20optimization%20problem.%20Our%20study%20demonstrates%20that%20fluctuations%20in%20perturbation%20intensity%20occurring%20during%20the%20learning%20phase%20can%20lead%20to%20ineffective%20training%2C%20by%20establishing%20a%20theoretical%20link%20to%20the%20notion%20of%20solution%20stability%20in%20combinatorial%20optimization.%20We%20propose%20addressing%20this%20issue%20by%20introducing%20a%20regularization%20of%20the%20estimated%20cost%20vectors%20which%20improves%20the%20robustness%20and%20reliability%20of%20the%20learning%20process%2C%20as%20demonstrated%20by%20extensive%20numerical%20experiments.&entry.1838667208=http%3A//arxiv.org/abs/2601.21883v1&entry.124074799=Read"},
{"title": "A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances", "author": "Brian B. Moser and Arundhati S. Shanbhag and Stanislav Frolov and Federico Raue and Joachim Folz and Andreas Dengel", "abstract": "Coreset selection targets the challenge of finding a small, representative subset of a large dataset that preserves essential patterns for effective machine learning. Although several surveys have examined data reduction strategies before, most focus narrowly on either classical geometry-based methods or active learning techniques. In contrast, this survey presents a more comprehensive view by unifying three major lines of coreset research, namely, training-free, training-oriented, and label-free approaches, into a single taxonomy. We present subfields often overlooked by existing work, including submodular formulations, bilevel optimization, and recent progress in pseudo-labeling for unlabeled datasets. Additionally, we examine how pruning strategies influence generalization and neural scaling laws, offering new insights that are absent from prior reviews. Finally, we compare these methods under varying computational, robustness, and performance demands and highlight open challenges, such as robustness, outlier filtering, and adapting coreset selection to foundation models, for future research.", "link": "http://arxiv.org/abs/2505.17799v2", "date": "2026-01-29", "relevancy": 2.3228, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Coreset%20Selection%20of%20Coreset%20Selection%20Literature%3A%20Introduction%20and%20Recent%20Advances&body=Title%3A%20A%20Coreset%20Selection%20of%20Coreset%20Selection%20Literature%3A%20Introduction%20and%20Recent%20Advances%0AAuthor%3A%20Brian%20B.%20Moser%20and%20Arundhati%20S.%20Shanbhag%20and%20Stanislav%20Frolov%20and%20Federico%20Raue%20and%20Joachim%20Folz%20and%20Andreas%20Dengel%0AAbstract%3A%20Coreset%20selection%20targets%20the%20challenge%20of%20finding%20a%20small%2C%20representative%20subset%20of%20a%20large%20dataset%20that%20preserves%20essential%20patterns%20for%20effective%20machine%20learning.%20Although%20several%20surveys%20have%20examined%20data%20reduction%20strategies%20before%2C%20most%20focus%20narrowly%20on%20either%20classical%20geometry-based%20methods%20or%20active%20learning%20techniques.%20In%20contrast%2C%20this%20survey%20presents%20a%20more%20comprehensive%20view%20by%20unifying%20three%20major%20lines%20of%20coreset%20research%2C%20namely%2C%20training-free%2C%20training-oriented%2C%20and%20label-free%20approaches%2C%20into%20a%20single%20taxonomy.%20We%20present%20subfields%20often%20overlooked%20by%20existing%20work%2C%20including%20submodular%20formulations%2C%20bilevel%20optimization%2C%20and%20recent%20progress%20in%20pseudo-labeling%20for%20unlabeled%20datasets.%20Additionally%2C%20we%20examine%20how%20pruning%20strategies%20influence%20generalization%20and%20neural%20scaling%20laws%2C%20offering%20new%20insights%20that%20are%20absent%20from%20prior%20reviews.%20Finally%2C%20we%20compare%20these%20methods%20under%20varying%20computational%2C%20robustness%2C%20and%20performance%20demands%20and%20highlight%20open%20challenges%2C%20such%20as%20robustness%2C%20outlier%20filtering%2C%20and%20adapting%20coreset%20selection%20to%20foundation%20models%2C%20for%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2505.17799v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Coreset%2520Selection%2520of%2520Coreset%2520Selection%2520Literature%253A%2520Introduction%2520and%2520Recent%2520Advances%26entry.906535625%3DBrian%2520B.%2520Moser%2520and%2520Arundhati%2520S.%2520Shanbhag%2520and%2520Stanislav%2520Frolov%2520and%2520Federico%2520Raue%2520and%2520Joachim%2520Folz%2520and%2520Andreas%2520Dengel%26entry.1292438233%3DCoreset%2520selection%2520targets%2520the%2520challenge%2520of%2520finding%2520a%2520small%252C%2520representative%2520subset%2520of%2520a%2520large%2520dataset%2520that%2520preserves%2520essential%2520patterns%2520for%2520effective%2520machine%2520learning.%2520Although%2520several%2520surveys%2520have%2520examined%2520data%2520reduction%2520strategies%2520before%252C%2520most%2520focus%2520narrowly%2520on%2520either%2520classical%2520geometry-based%2520methods%2520or%2520active%2520learning%2520techniques.%2520In%2520contrast%252C%2520this%2520survey%2520presents%2520a%2520more%2520comprehensive%2520view%2520by%2520unifying%2520three%2520major%2520lines%2520of%2520coreset%2520research%252C%2520namely%252C%2520training-free%252C%2520training-oriented%252C%2520and%2520label-free%2520approaches%252C%2520into%2520a%2520single%2520taxonomy.%2520We%2520present%2520subfields%2520often%2520overlooked%2520by%2520existing%2520work%252C%2520including%2520submodular%2520formulations%252C%2520bilevel%2520optimization%252C%2520and%2520recent%2520progress%2520in%2520pseudo-labeling%2520for%2520unlabeled%2520datasets.%2520Additionally%252C%2520we%2520examine%2520how%2520pruning%2520strategies%2520influence%2520generalization%2520and%2520neural%2520scaling%2520laws%252C%2520offering%2520new%2520insights%2520that%2520are%2520absent%2520from%2520prior%2520reviews.%2520Finally%252C%2520we%2520compare%2520these%2520methods%2520under%2520varying%2520computational%252C%2520robustness%252C%2520and%2520performance%2520demands%2520and%2520highlight%2520open%2520challenges%252C%2520such%2520as%2520robustness%252C%2520outlier%2520filtering%252C%2520and%2520adapting%2520coreset%2520selection%2520to%2520foundation%2520models%252C%2520for%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17799v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Coreset%20Selection%20of%20Coreset%20Selection%20Literature%3A%20Introduction%20and%20Recent%20Advances&entry.906535625=Brian%20B.%20Moser%20and%20Arundhati%20S.%20Shanbhag%20and%20Stanislav%20Frolov%20and%20Federico%20Raue%20and%20Joachim%20Folz%20and%20Andreas%20Dengel&entry.1292438233=Coreset%20selection%20targets%20the%20challenge%20of%20finding%20a%20small%2C%20representative%20subset%20of%20a%20large%20dataset%20that%20preserves%20essential%20patterns%20for%20effective%20machine%20learning.%20Although%20several%20surveys%20have%20examined%20data%20reduction%20strategies%20before%2C%20most%20focus%20narrowly%20on%20either%20classical%20geometry-based%20methods%20or%20active%20learning%20techniques.%20In%20contrast%2C%20this%20survey%20presents%20a%20more%20comprehensive%20view%20by%20unifying%20three%20major%20lines%20of%20coreset%20research%2C%20namely%2C%20training-free%2C%20training-oriented%2C%20and%20label-free%20approaches%2C%20into%20a%20single%20taxonomy.%20We%20present%20subfields%20often%20overlooked%20by%20existing%20work%2C%20including%20submodular%20formulations%2C%20bilevel%20optimization%2C%20and%20recent%20progress%20in%20pseudo-labeling%20for%20unlabeled%20datasets.%20Additionally%2C%20we%20examine%20how%20pruning%20strategies%20influence%20generalization%20and%20neural%20scaling%20laws%2C%20offering%20new%20insights%20that%20are%20absent%20from%20prior%20reviews.%20Finally%2C%20we%20compare%20these%20methods%20under%20varying%20computational%2C%20robustness%2C%20and%20performance%20demands%20and%20highlight%20open%20challenges%2C%20such%20as%20robustness%2C%20outlier%20filtering%2C%20and%20adapting%20coreset%20selection%20to%20foundation%20models%2C%20for%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2505.17799v2&entry.124074799=Read"},
{"title": "EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition", "author": "Maryam Mirzaei and Farzaneh Shayegh and Hamed Narimani", "abstract": "Accurate recognition of human emotional states is critical for effective human-machine interaction. Electroencephalography (EEG) offers a reliable source for emotion recognition due to its high temporal resolution and its direct reflection of neural activity. Nevertheless, variations across recording sessions present a major challenge for model generalization. To address this issue, we propose EGDA, a framework that reduces cross-session discrepancies by jointly aligning the global (marginal) and class-specific (conditional) distributions, while preserving the intrinsic structure of EEG data through graph regularization. Experimental results on the SEED-IV dataset demonstrate that EGDA achieves robust cross-session performance, obtaining accuracies of 81.22%, 80.15%, and 83.27% across three transfer tasks, and surpassing several baseline methods. Furthermore, the analysis highlights the Gamma frequency band as the most discriminative and identifies the central-parietal and prefrontal brain regions as critical for reliable emotion recognition.", "link": "http://arxiv.org/abs/2512.23526v2", "date": "2026-01-29", "relevancy": 2.3142, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4675}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4652}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG-based%20Graph-guided%20Domain%20Adaptation%20for%20Robust%20Cross-Session%20Emotion%20Recognition&body=Title%3A%20EEG-based%20Graph-guided%20Domain%20Adaptation%20for%20Robust%20Cross-Session%20Emotion%20Recognition%0AAuthor%3A%20Maryam%20Mirzaei%20and%20Farzaneh%20Shayegh%20and%20Hamed%20Narimani%0AAbstract%3A%20Accurate%20recognition%20of%20human%20emotional%20states%20is%20critical%20for%20effective%20human-machine%20interaction.%20Electroencephalography%20%28EEG%29%20offers%20a%20reliable%20source%20for%20emotion%20recognition%20due%20to%20its%20high%20temporal%20resolution%20and%20its%20direct%20reflection%20of%20neural%20activity.%20Nevertheless%2C%20variations%20across%20recording%20sessions%20present%20a%20major%20challenge%20for%20model%20generalization.%20To%20address%20this%20issue%2C%20we%20propose%20EGDA%2C%20a%20framework%20that%20reduces%20cross-session%20discrepancies%20by%20jointly%20aligning%20the%20global%20%28marginal%29%20and%20class-specific%20%28conditional%29%20distributions%2C%20while%20preserving%20the%20intrinsic%20structure%20of%20EEG%20data%20through%20graph%20regularization.%20Experimental%20results%20on%20the%20SEED-IV%20dataset%20demonstrate%20that%20EGDA%20achieves%20robust%20cross-session%20performance%2C%20obtaining%20accuracies%20of%2081.22%25%2C%2080.15%25%2C%20and%2083.27%25%20across%20three%20transfer%20tasks%2C%20and%20surpassing%20several%20baseline%20methods.%20Furthermore%2C%20the%20analysis%20highlights%20the%20Gamma%20frequency%20band%20as%20the%20most%20discriminative%20and%20identifies%20the%20central-parietal%20and%20prefrontal%20brain%20regions%20as%20critical%20for%20reliable%20emotion%20recognition.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23526v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG-based%2520Graph-guided%2520Domain%2520Adaptation%2520for%2520Robust%2520Cross-Session%2520Emotion%2520Recognition%26entry.906535625%3DMaryam%2520Mirzaei%2520and%2520Farzaneh%2520Shayegh%2520and%2520Hamed%2520Narimani%26entry.1292438233%3DAccurate%2520recognition%2520of%2520human%2520emotional%2520states%2520is%2520critical%2520for%2520effective%2520human-machine%2520interaction.%2520Electroencephalography%2520%2528EEG%2529%2520offers%2520a%2520reliable%2520source%2520for%2520emotion%2520recognition%2520due%2520to%2520its%2520high%2520temporal%2520resolution%2520and%2520its%2520direct%2520reflection%2520of%2520neural%2520activity.%2520Nevertheless%252C%2520variations%2520across%2520recording%2520sessions%2520present%2520a%2520major%2520challenge%2520for%2520model%2520generalization.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520EGDA%252C%2520a%2520framework%2520that%2520reduces%2520cross-session%2520discrepancies%2520by%2520jointly%2520aligning%2520the%2520global%2520%2528marginal%2529%2520and%2520class-specific%2520%2528conditional%2529%2520distributions%252C%2520while%2520preserving%2520the%2520intrinsic%2520structure%2520of%2520EEG%2520data%2520through%2520graph%2520regularization.%2520Experimental%2520results%2520on%2520the%2520SEED-IV%2520dataset%2520demonstrate%2520that%2520EGDA%2520achieves%2520robust%2520cross-session%2520performance%252C%2520obtaining%2520accuracies%2520of%252081.22%2525%252C%252080.15%2525%252C%2520and%252083.27%2525%2520across%2520three%2520transfer%2520tasks%252C%2520and%2520surpassing%2520several%2520baseline%2520methods.%2520Furthermore%252C%2520the%2520analysis%2520highlights%2520the%2520Gamma%2520frequency%2520band%2520as%2520the%2520most%2520discriminative%2520and%2520identifies%2520the%2520central-parietal%2520and%2520prefrontal%2520brain%2520regions%2520as%2520critical%2520for%2520reliable%2520emotion%2520recognition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23526v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG-based%20Graph-guided%20Domain%20Adaptation%20for%20Robust%20Cross-Session%20Emotion%20Recognition&entry.906535625=Maryam%20Mirzaei%20and%20Farzaneh%20Shayegh%20and%20Hamed%20Narimani&entry.1292438233=Accurate%20recognition%20of%20human%20emotional%20states%20is%20critical%20for%20effective%20human-machine%20interaction.%20Electroencephalography%20%28EEG%29%20offers%20a%20reliable%20source%20for%20emotion%20recognition%20due%20to%20its%20high%20temporal%20resolution%20and%20its%20direct%20reflection%20of%20neural%20activity.%20Nevertheless%2C%20variations%20across%20recording%20sessions%20present%20a%20major%20challenge%20for%20model%20generalization.%20To%20address%20this%20issue%2C%20we%20propose%20EGDA%2C%20a%20framework%20that%20reduces%20cross-session%20discrepancies%20by%20jointly%20aligning%20the%20global%20%28marginal%29%20and%20class-specific%20%28conditional%29%20distributions%2C%20while%20preserving%20the%20intrinsic%20structure%20of%20EEG%20data%20through%20graph%20regularization.%20Experimental%20results%20on%20the%20SEED-IV%20dataset%20demonstrate%20that%20EGDA%20achieves%20robust%20cross-session%20performance%2C%20obtaining%20accuracies%20of%2081.22%25%2C%2080.15%25%2C%20and%2083.27%25%20across%20three%20transfer%20tasks%2C%20and%20surpassing%20several%20baseline%20methods.%20Furthermore%2C%20the%20analysis%20highlights%20the%20Gamma%20frequency%20band%20as%20the%20most%20discriminative%20and%20identifies%20the%20central-parietal%20and%20prefrontal%20brain%20regions%20as%20critical%20for%20reliable%20emotion%20recognition.&entry.1838667208=http%3A//arxiv.org/abs/2512.23526v2&entry.124074799=Read"},
{"title": "Effective LoRA Adapter Routing using Task Representations", "author": "Akash Dhasade and Anne-Marie Kermarrec and Igor Pavlovic and Diana Petrescu and Rafael Pires and Mathis Randl and Martijn de Vos", "abstract": "Low-rank adaptation (LoRA) enables parameter efficient specialization of large language models (LLMs) through modular adapters, resulting in rapidly growing public adapter pools spanning diverse tasks. Effectively using these adapters requires routing: selecting and composing the appropriate adapters for a query. We introduce LORAUTER, a novel routing framework that selects and composes LoRA adapters using task representations rather than adapter characteristics. Unlike existing approaches that map queries directly to adapters, LORAUTER routes queries via task embeddings derived from small validation sets and does not require adapter training data. By operating at the task level, LORAUTER achieves efficient routing that scales with the number of tasks rather than the number of adapters. Experiments across multiple tasks show that LORAUTER consistently outperforms baseline routing approaches, matching Oracle performance (101.2%) when task-aligned adapters exist and achieving state-of-the-art results on unseen tasks (+5.2 points). We further demonstrate the robustness of LORAUTER to very large, noisy adapter pools by scaling it to over 1500 adapters.", "link": "http://arxiv.org/abs/2601.21795v1", "date": "2026-01-29", "relevancy": 2.3023, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.498}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.446}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effective%20LoRA%20Adapter%20Routing%20using%20Task%20Representations&body=Title%3A%20Effective%20LoRA%20Adapter%20Routing%20using%20Task%20Representations%0AAuthor%3A%20Akash%20Dhasade%20and%20Anne-Marie%20Kermarrec%20and%20Igor%20Pavlovic%20and%20Diana%20Petrescu%20and%20Rafael%20Pires%20and%20Mathis%20Randl%20and%20Martijn%20de%20Vos%0AAbstract%3A%20Low-rank%20adaptation%20%28LoRA%29%20enables%20parameter%20efficient%20specialization%20of%20large%20language%20models%20%28LLMs%29%20through%20modular%20adapters%2C%20resulting%20in%20rapidly%20growing%20public%20adapter%20pools%20spanning%20diverse%20tasks.%20Effectively%20using%20these%20adapters%20requires%20routing%3A%20selecting%20and%20composing%20the%20appropriate%20adapters%20for%20a%20query.%20We%20introduce%20LORAUTER%2C%20a%20novel%20routing%20framework%20that%20selects%20and%20composes%20LoRA%20adapters%20using%20task%20representations%20rather%20than%20adapter%20characteristics.%20Unlike%20existing%20approaches%20that%20map%20queries%20directly%20to%20adapters%2C%20LORAUTER%20routes%20queries%20via%20task%20embeddings%20derived%20from%20small%20validation%20sets%20and%20does%20not%20require%20adapter%20training%20data.%20By%20operating%20at%20the%20task%20level%2C%20LORAUTER%20achieves%20efficient%20routing%20that%20scales%20with%20the%20number%20of%20tasks%20rather%20than%20the%20number%20of%20adapters.%20Experiments%20across%20multiple%20tasks%20show%20that%20LORAUTER%20consistently%20outperforms%20baseline%20routing%20approaches%2C%20matching%20Oracle%20performance%20%28101.2%25%29%20when%20task-aligned%20adapters%20exist%20and%20achieving%20state-of-the-art%20results%20on%20unseen%20tasks%20%28%2B5.2%20points%29.%20We%20further%20demonstrate%20the%20robustness%20of%20LORAUTER%20to%20very%20large%2C%20noisy%20adapter%20pools%20by%20scaling%20it%20to%20over%201500%20adapters.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffective%2520LoRA%2520Adapter%2520Routing%2520using%2520Task%2520Representations%26entry.906535625%3DAkash%2520Dhasade%2520and%2520Anne-Marie%2520Kermarrec%2520and%2520Igor%2520Pavlovic%2520and%2520Diana%2520Petrescu%2520and%2520Rafael%2520Pires%2520and%2520Mathis%2520Randl%2520and%2520Martijn%2520de%2520Vos%26entry.1292438233%3DLow-rank%2520adaptation%2520%2528LoRA%2529%2520enables%2520parameter%2520efficient%2520specialization%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520through%2520modular%2520adapters%252C%2520resulting%2520in%2520rapidly%2520growing%2520public%2520adapter%2520pools%2520spanning%2520diverse%2520tasks.%2520Effectively%2520using%2520these%2520adapters%2520requires%2520routing%253A%2520selecting%2520and%2520composing%2520the%2520appropriate%2520adapters%2520for%2520a%2520query.%2520We%2520introduce%2520LORAUTER%252C%2520a%2520novel%2520routing%2520framework%2520that%2520selects%2520and%2520composes%2520LoRA%2520adapters%2520using%2520task%2520representations%2520rather%2520than%2520adapter%2520characteristics.%2520Unlike%2520existing%2520approaches%2520that%2520map%2520queries%2520directly%2520to%2520adapters%252C%2520LORAUTER%2520routes%2520queries%2520via%2520task%2520embeddings%2520derived%2520from%2520small%2520validation%2520sets%2520and%2520does%2520not%2520require%2520adapter%2520training%2520data.%2520By%2520operating%2520at%2520the%2520task%2520level%252C%2520LORAUTER%2520achieves%2520efficient%2520routing%2520that%2520scales%2520with%2520the%2520number%2520of%2520tasks%2520rather%2520than%2520the%2520number%2520of%2520adapters.%2520Experiments%2520across%2520multiple%2520tasks%2520show%2520that%2520LORAUTER%2520consistently%2520outperforms%2520baseline%2520routing%2520approaches%252C%2520matching%2520Oracle%2520performance%2520%2528101.2%2525%2529%2520when%2520task-aligned%2520adapters%2520exist%2520and%2520achieving%2520state-of-the-art%2520results%2520on%2520unseen%2520tasks%2520%2528%252B5.2%2520points%2529.%2520We%2520further%2520demonstrate%2520the%2520robustness%2520of%2520LORAUTER%2520to%2520very%2520large%252C%2520noisy%2520adapter%2520pools%2520by%2520scaling%2520it%2520to%2520over%25201500%2520adapters.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effective%20LoRA%20Adapter%20Routing%20using%20Task%20Representations&entry.906535625=Akash%20Dhasade%20and%20Anne-Marie%20Kermarrec%20and%20Igor%20Pavlovic%20and%20Diana%20Petrescu%20and%20Rafael%20Pires%20and%20Mathis%20Randl%20and%20Martijn%20de%20Vos&entry.1292438233=Low-rank%20adaptation%20%28LoRA%29%20enables%20parameter%20efficient%20specialization%20of%20large%20language%20models%20%28LLMs%29%20through%20modular%20adapters%2C%20resulting%20in%20rapidly%20growing%20public%20adapter%20pools%20spanning%20diverse%20tasks.%20Effectively%20using%20these%20adapters%20requires%20routing%3A%20selecting%20and%20composing%20the%20appropriate%20adapters%20for%20a%20query.%20We%20introduce%20LORAUTER%2C%20a%20novel%20routing%20framework%20that%20selects%20and%20composes%20LoRA%20adapters%20using%20task%20representations%20rather%20than%20adapter%20characteristics.%20Unlike%20existing%20approaches%20that%20map%20queries%20directly%20to%20adapters%2C%20LORAUTER%20routes%20queries%20via%20task%20embeddings%20derived%20from%20small%20validation%20sets%20and%20does%20not%20require%20adapter%20training%20data.%20By%20operating%20at%20the%20task%20level%2C%20LORAUTER%20achieves%20efficient%20routing%20that%20scales%20with%20the%20number%20of%20tasks%20rather%20than%20the%20number%20of%20adapters.%20Experiments%20across%20multiple%20tasks%20show%20that%20LORAUTER%20consistently%20outperforms%20baseline%20routing%20approaches%2C%20matching%20Oracle%20performance%20%28101.2%25%29%20when%20task-aligned%20adapters%20exist%20and%20achieving%20state-of-the-art%20results%20on%20unseen%20tasks%20%28%2B5.2%20points%29.%20We%20further%20demonstrate%20the%20robustness%20of%20LORAUTER%20to%20very%20large%2C%20noisy%20adapter%20pools%20by%20scaling%20it%20to%20over%201500%20adapters.&entry.1838667208=http%3A//arxiv.org/abs/2601.21795v1&entry.124074799=Read"},
{"title": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion", "author": "Anthony Chen and Naomi Ken Korem and Tavi Halperin and Matan Ben Yosef and Urska Jelercic and Ofir Bibi and Or Patashnik and Daniel Cohen-Or", "abstract": "Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.", "link": "http://arxiv.org/abs/2601.22143v1", "date": "2026-01-29", "relevancy": 2.2988, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6079}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5765}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JUST-DUB-IT%3A%20Video%20Dubbing%20via%20Joint%20Audio-Visual%20Diffusion&body=Title%3A%20JUST-DUB-IT%3A%20Video%20Dubbing%20via%20Joint%20Audio-Visual%20Diffusion%0AAuthor%3A%20Anthony%20Chen%20and%20Naomi%20Ken%20Korem%20and%20Tavi%20Halperin%20and%20Matan%20Ben%20Yosef%20and%20Urska%20Jelercic%20and%20Ofir%20Bibi%20and%20Or%20Patashnik%20and%20Daniel%20Cohen-Or%0AAbstract%3A%20Audio-Visual%20Foundation%20Models%2C%20which%20are%20pretrained%20to%20jointly%20generate%20sound%20and%20visual%20content%2C%20have%20recently%20shown%20an%20unprecedented%20ability%20to%20model%20multi-modal%20generation%20and%20editing%2C%20opening%20new%20opportunities%20for%20downstream%20tasks.%20Among%20these%20tasks%2C%20video%20dubbing%20could%20greatly%20benefit%20from%20such%20priors%2C%20yet%20most%20existing%20solutions%20still%20rely%20on%20complex%2C%20task-specific%20pipelines%20that%20struggle%20in%20real-world%20settings.%20In%20this%20work%2C%20we%20introduce%20a%20single-model%20approach%20that%20adapts%20a%20foundational%20audio-video%20diffusion%20model%20for%20video-to-video%20dubbing%20via%20a%20lightweight%20LoRA.%20The%20LoRA%20enables%20the%20model%20to%20condition%20on%20an%20input%20audio-video%20while%20jointly%20generating%20translated%20audio%20and%20synchronized%20facial%20motion.%20To%20train%20this%20LoRA%2C%20we%20leverage%20the%20generative%20model%20itself%20to%20synthesize%20paired%20multilingual%20videos%20of%20the%20same%20speaker.%20Specifically%2C%20we%20generate%20multilingual%20videos%20with%20language%20switches%20within%20a%20single%20clip%2C%20and%20then%20inpaint%20the%20face%20and%20audio%20in%20each%20half%20to%20match%20the%20language%20of%20the%20other%20half.%20By%20leveraging%20the%20rich%20generative%20prior%20of%20the%20audio-visual%20model%2C%20our%20approach%20preserves%20speaker%20identity%20and%20lip%20synchronization%20while%20remaining%20robust%20to%20complex%20motion%20and%20real-world%20dynamics.%20We%20demonstrate%20that%20our%20approach%20produces%20high-quality%20dubbed%20videos%20with%20improved%20visual%20fidelity%2C%20lip%20synchronization%2C%20and%20robustness%20compared%20to%20existing%20dubbing%20pipelines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJUST-DUB-IT%253A%2520Video%2520Dubbing%2520via%2520Joint%2520Audio-Visual%2520Diffusion%26entry.906535625%3DAnthony%2520Chen%2520and%2520Naomi%2520Ken%2520Korem%2520and%2520Tavi%2520Halperin%2520and%2520Matan%2520Ben%2520Yosef%2520and%2520Urska%2520Jelercic%2520and%2520Ofir%2520Bibi%2520and%2520Or%2520Patashnik%2520and%2520Daniel%2520Cohen-Or%26entry.1292438233%3DAudio-Visual%2520Foundation%2520Models%252C%2520which%2520are%2520pretrained%2520to%2520jointly%2520generate%2520sound%2520and%2520visual%2520content%252C%2520have%2520recently%2520shown%2520an%2520unprecedented%2520ability%2520to%2520model%2520multi-modal%2520generation%2520and%2520editing%252C%2520opening%2520new%2520opportunities%2520for%2520downstream%2520tasks.%2520Among%2520these%2520tasks%252C%2520video%2520dubbing%2520could%2520greatly%2520benefit%2520from%2520such%2520priors%252C%2520yet%2520most%2520existing%2520solutions%2520still%2520rely%2520on%2520complex%252C%2520task-specific%2520pipelines%2520that%2520struggle%2520in%2520real-world%2520settings.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520single-model%2520approach%2520that%2520adapts%2520a%2520foundational%2520audio-video%2520diffusion%2520model%2520for%2520video-to-video%2520dubbing%2520via%2520a%2520lightweight%2520LoRA.%2520The%2520LoRA%2520enables%2520the%2520model%2520to%2520condition%2520on%2520an%2520input%2520audio-video%2520while%2520jointly%2520generating%2520translated%2520audio%2520and%2520synchronized%2520facial%2520motion.%2520To%2520train%2520this%2520LoRA%252C%2520we%2520leverage%2520the%2520generative%2520model%2520itself%2520to%2520synthesize%2520paired%2520multilingual%2520videos%2520of%2520the%2520same%2520speaker.%2520Specifically%252C%2520we%2520generate%2520multilingual%2520videos%2520with%2520language%2520switches%2520within%2520a%2520single%2520clip%252C%2520and%2520then%2520inpaint%2520the%2520face%2520and%2520audio%2520in%2520each%2520half%2520to%2520match%2520the%2520language%2520of%2520the%2520other%2520half.%2520By%2520leveraging%2520the%2520rich%2520generative%2520prior%2520of%2520the%2520audio-visual%2520model%252C%2520our%2520approach%2520preserves%2520speaker%2520identity%2520and%2520lip%2520synchronization%2520while%2520remaining%2520robust%2520to%2520complex%2520motion%2520and%2520real-world%2520dynamics.%2520We%2520demonstrate%2520that%2520our%2520approach%2520produces%2520high-quality%2520dubbed%2520videos%2520with%2520improved%2520visual%2520fidelity%252C%2520lip%2520synchronization%252C%2520and%2520robustness%2520compared%2520to%2520existing%2520dubbing%2520pipelines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JUST-DUB-IT%3A%20Video%20Dubbing%20via%20Joint%20Audio-Visual%20Diffusion&entry.906535625=Anthony%20Chen%20and%20Naomi%20Ken%20Korem%20and%20Tavi%20Halperin%20and%20Matan%20Ben%20Yosef%20and%20Urska%20Jelercic%20and%20Ofir%20Bibi%20and%20Or%20Patashnik%20and%20Daniel%20Cohen-Or&entry.1292438233=Audio-Visual%20Foundation%20Models%2C%20which%20are%20pretrained%20to%20jointly%20generate%20sound%20and%20visual%20content%2C%20have%20recently%20shown%20an%20unprecedented%20ability%20to%20model%20multi-modal%20generation%20and%20editing%2C%20opening%20new%20opportunities%20for%20downstream%20tasks.%20Among%20these%20tasks%2C%20video%20dubbing%20could%20greatly%20benefit%20from%20such%20priors%2C%20yet%20most%20existing%20solutions%20still%20rely%20on%20complex%2C%20task-specific%20pipelines%20that%20struggle%20in%20real-world%20settings.%20In%20this%20work%2C%20we%20introduce%20a%20single-model%20approach%20that%20adapts%20a%20foundational%20audio-video%20diffusion%20model%20for%20video-to-video%20dubbing%20via%20a%20lightweight%20LoRA.%20The%20LoRA%20enables%20the%20model%20to%20condition%20on%20an%20input%20audio-video%20while%20jointly%20generating%20translated%20audio%20and%20synchronized%20facial%20motion.%20To%20train%20this%20LoRA%2C%20we%20leverage%20the%20generative%20model%20itself%20to%20synthesize%20paired%20multilingual%20videos%20of%20the%20same%20speaker.%20Specifically%2C%20we%20generate%20multilingual%20videos%20with%20language%20switches%20within%20a%20single%20clip%2C%20and%20then%20inpaint%20the%20face%20and%20audio%20in%20each%20half%20to%20match%20the%20language%20of%20the%20other%20half.%20By%20leveraging%20the%20rich%20generative%20prior%20of%20the%20audio-visual%20model%2C%20our%20approach%20preserves%20speaker%20identity%20and%20lip%20synchronization%20while%20remaining%20robust%20to%20complex%20motion%20and%20real-world%20dynamics.%20We%20demonstrate%20that%20our%20approach%20produces%20high-quality%20dubbed%20videos%20with%20improved%20visual%20fidelity%2C%20lip%20synchronization%2C%20and%20robustness%20compared%20to%20existing%20dubbing%20pipelines.&entry.1838667208=http%3A//arxiv.org/abs/2601.22143v1&entry.124074799=Read"},
{"title": "IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation", "author": "Joonhee Lee and Hyunseung Shin and Jeonggil Ko", "abstract": "Indoor mobile robot navigation requires fast responsiveness and robust semantic understanding, yet existing methods struggle to provide both. Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning. Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues. Vision-Language Models (VLMs) provide richer contextual inference but suffer from high computational latency, making them unsuitable for real-time operation on embedded platforms. In this work, we present IROS, a real-time navigation framework that combines VLM-level contextual reasoning with the efficiency of lightweight perceptual modules on low-cost, on-device hardware. Inspired by Dual Process Theory, IROS separates fast reflexive decisions (System One) from slow deliberative reasoning (System Two), invoking the VLM only when necessary. Furthermore, by augmenting compact VLMs with spatial and textual cues, IROS delivers robust, human-like navigation with minimal latency. Across five real-world buildings, IROS improves decision accuracy and reduces latency by 66% compared to continuous VLM-based navigation.", "link": "http://arxiv.org/abs/2601.21506v1", "date": "2026-01-29", "relevancy": 2.293, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5907}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5838}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IROS%3A%20A%20Dual-Process%20Architecture%20for%20Real-Time%20VLM-Based%20Indoor%20Navigation&body=Title%3A%20IROS%3A%20A%20Dual-Process%20Architecture%20for%20Real-Time%20VLM-Based%20Indoor%20Navigation%0AAuthor%3A%20Joonhee%20Lee%20and%20Hyunseung%20Shin%20and%20Jeonggil%20Ko%0AAbstract%3A%20Indoor%20mobile%20robot%20navigation%20requires%20fast%20responsiveness%20and%20robust%20semantic%20understanding%2C%20yet%20existing%20methods%20struggle%20to%20provide%20both.%20Classical%20geometric%20approaches%20such%20as%20SLAM%20offer%20reliable%20localization%20but%20depend%20on%20detailed%20maps%20and%20cannot%20interpret%20human-targeted%20cues%20%28e.g.%2C%20signs%2C%20room%20numbers%29%20essential%20for%20indoor%20reasoning.%20Vision-Language-Action%20%28VLA%29%20models%20introduce%20semantic%20grounding%20but%20remain%20strictly%20reactive%2C%20basing%20decisions%20only%20on%20visible%20frames%20and%20failing%20to%20anticipate%20unseen%20intersections%20or%20reason%20about%20distant%20textual%20cues.%20Vision-Language%20Models%20%28VLMs%29%20provide%20richer%20contextual%20inference%20but%20suffer%20from%20high%20computational%20latency%2C%20making%20them%20unsuitable%20for%20real-time%20operation%20on%20embedded%20platforms.%20In%20this%20work%2C%20we%20present%20IROS%2C%20a%20real-time%20navigation%20framework%20that%20combines%20VLM-level%20contextual%20reasoning%20with%20the%20efficiency%20of%20lightweight%20perceptual%20modules%20on%20low-cost%2C%20on-device%20hardware.%20Inspired%20by%20Dual%20Process%20Theory%2C%20IROS%20separates%20fast%20reflexive%20decisions%20%28System%20One%29%20from%20slow%20deliberative%20reasoning%20%28System%20Two%29%2C%20invoking%20the%20VLM%20only%20when%20necessary.%20Furthermore%2C%20by%20augmenting%20compact%20VLMs%20with%20spatial%20and%20textual%20cues%2C%20IROS%20delivers%20robust%2C%20human-like%20navigation%20with%20minimal%20latency.%20Across%20five%20real-world%20buildings%2C%20IROS%20improves%20decision%20accuracy%20and%20reduces%20latency%20by%2066%25%20compared%20to%20continuous%20VLM-based%20navigation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIROS%253A%2520A%2520Dual-Process%2520Architecture%2520for%2520Real-Time%2520VLM-Based%2520Indoor%2520Navigation%26entry.906535625%3DJoonhee%2520Lee%2520and%2520Hyunseung%2520Shin%2520and%2520Jeonggil%2520Ko%26entry.1292438233%3DIndoor%2520mobile%2520robot%2520navigation%2520requires%2520fast%2520responsiveness%2520and%2520robust%2520semantic%2520understanding%252C%2520yet%2520existing%2520methods%2520struggle%2520to%2520provide%2520both.%2520Classical%2520geometric%2520approaches%2520such%2520as%2520SLAM%2520offer%2520reliable%2520localization%2520but%2520depend%2520on%2520detailed%2520maps%2520and%2520cannot%2520interpret%2520human-targeted%2520cues%2520%2528e.g.%252C%2520signs%252C%2520room%2520numbers%2529%2520essential%2520for%2520indoor%2520reasoning.%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520introduce%2520semantic%2520grounding%2520but%2520remain%2520strictly%2520reactive%252C%2520basing%2520decisions%2520only%2520on%2520visible%2520frames%2520and%2520failing%2520to%2520anticipate%2520unseen%2520intersections%2520or%2520reason%2520about%2520distant%2520textual%2520cues.%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520provide%2520richer%2520contextual%2520inference%2520but%2520suffer%2520from%2520high%2520computational%2520latency%252C%2520making%2520them%2520unsuitable%2520for%2520real-time%2520operation%2520on%2520embedded%2520platforms.%2520In%2520this%2520work%252C%2520we%2520present%2520IROS%252C%2520a%2520real-time%2520navigation%2520framework%2520that%2520combines%2520VLM-level%2520contextual%2520reasoning%2520with%2520the%2520efficiency%2520of%2520lightweight%2520perceptual%2520modules%2520on%2520low-cost%252C%2520on-device%2520hardware.%2520Inspired%2520by%2520Dual%2520Process%2520Theory%252C%2520IROS%2520separates%2520fast%2520reflexive%2520decisions%2520%2528System%2520One%2529%2520from%2520slow%2520deliberative%2520reasoning%2520%2528System%2520Two%2529%252C%2520invoking%2520the%2520VLM%2520only%2520when%2520necessary.%2520Furthermore%252C%2520by%2520augmenting%2520compact%2520VLMs%2520with%2520spatial%2520and%2520textual%2520cues%252C%2520IROS%2520delivers%2520robust%252C%2520human-like%2520navigation%2520with%2520minimal%2520latency.%2520Across%2520five%2520real-world%2520buildings%252C%2520IROS%2520improves%2520decision%2520accuracy%2520and%2520reduces%2520latency%2520by%252066%2525%2520compared%2520to%2520continuous%2520VLM-based%2520navigation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IROS%3A%20A%20Dual-Process%20Architecture%20for%20Real-Time%20VLM-Based%20Indoor%20Navigation&entry.906535625=Joonhee%20Lee%20and%20Hyunseung%20Shin%20and%20Jeonggil%20Ko&entry.1292438233=Indoor%20mobile%20robot%20navigation%20requires%20fast%20responsiveness%20and%20robust%20semantic%20understanding%2C%20yet%20existing%20methods%20struggle%20to%20provide%20both.%20Classical%20geometric%20approaches%20such%20as%20SLAM%20offer%20reliable%20localization%20but%20depend%20on%20detailed%20maps%20and%20cannot%20interpret%20human-targeted%20cues%20%28e.g.%2C%20signs%2C%20room%20numbers%29%20essential%20for%20indoor%20reasoning.%20Vision-Language-Action%20%28VLA%29%20models%20introduce%20semantic%20grounding%20but%20remain%20strictly%20reactive%2C%20basing%20decisions%20only%20on%20visible%20frames%20and%20failing%20to%20anticipate%20unseen%20intersections%20or%20reason%20about%20distant%20textual%20cues.%20Vision-Language%20Models%20%28VLMs%29%20provide%20richer%20contextual%20inference%20but%20suffer%20from%20high%20computational%20latency%2C%20making%20them%20unsuitable%20for%20real-time%20operation%20on%20embedded%20platforms.%20In%20this%20work%2C%20we%20present%20IROS%2C%20a%20real-time%20navigation%20framework%20that%20combines%20VLM-level%20contextual%20reasoning%20with%20the%20efficiency%20of%20lightweight%20perceptual%20modules%20on%20low-cost%2C%20on-device%20hardware.%20Inspired%20by%20Dual%20Process%20Theory%2C%20IROS%20separates%20fast%20reflexive%20decisions%20%28System%20One%29%20from%20slow%20deliberative%20reasoning%20%28System%20Two%29%2C%20invoking%20the%20VLM%20only%20when%20necessary.%20Furthermore%2C%20by%20augmenting%20compact%20VLMs%20with%20spatial%20and%20textual%20cues%2C%20IROS%20delivers%20robust%2C%20human-like%20navigation%20with%20minimal%20latency.%20Across%20five%20real-world%20buildings%2C%20IROS%20improves%20decision%20accuracy%20and%20reduces%20latency%20by%2066%25%20compared%20to%20continuous%20VLM-based%20navigation.&entry.1838667208=http%3A//arxiv.org/abs/2601.21506v1&entry.124074799=Read"},
{"title": "Epistemic Uncertainty Quantification for Pre-trained VLMs via Riemannian Flow Matching", "author": "Li Ju and Mayank Nautiyal and Andreas Hellander and Ekta Vats and Prashant Singh", "abstract": "Vision-Language Models (VLMs) are typically deterministic in nature and lack intrinsic mechanisms to quantify epistemic uncertainty, which reflects the model's lack of knowledge or ignorance of its own representations. We theoretically motivate negative log-density of an embedding as a proxy for the epistemic uncertainty, where low-density regions signify model ignorance. The proposed method REPVLM computes the probability density on the hyperspherical manifold of the VLM embeddings using Riemannian Flow Matching. We empirically demonstrate that REPVLM achieves near-perfect correlation between uncertainty and prediction error, significantly outperforming existing baselines. Beyond classification, we also demonstrate that the model also provides a scalable metric for out-of-distribution detection and automated data curation.", "link": "http://arxiv.org/abs/2601.21662v1", "date": "2026-01-29", "relevancy": 2.2912, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5848}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.565}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Epistemic%20Uncertainty%20Quantification%20for%20Pre-trained%20VLMs%20via%20Riemannian%20Flow%20Matching&body=Title%3A%20Epistemic%20Uncertainty%20Quantification%20for%20Pre-trained%20VLMs%20via%20Riemannian%20Flow%20Matching%0AAuthor%3A%20Li%20Ju%20and%20Mayank%20Nautiyal%20and%20Andreas%20Hellander%20and%20Ekta%20Vats%20and%20Prashant%20Singh%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20are%20typically%20deterministic%20in%20nature%20and%20lack%20intrinsic%20mechanisms%20to%20quantify%20epistemic%20uncertainty%2C%20which%20reflects%20the%20model%27s%20lack%20of%20knowledge%20or%20ignorance%20of%20its%20own%20representations.%20We%20theoretically%20motivate%20negative%20log-density%20of%20an%20embedding%20as%20a%20proxy%20for%20the%20epistemic%20uncertainty%2C%20where%20low-density%20regions%20signify%20model%20ignorance.%20The%20proposed%20method%20REPVLM%20computes%20the%20probability%20density%20on%20the%20hyperspherical%20manifold%20of%20the%20VLM%20embeddings%20using%20Riemannian%20Flow%20Matching.%20We%20empirically%20demonstrate%20that%20REPVLM%20achieves%20near-perfect%20correlation%20between%20uncertainty%20and%20prediction%20error%2C%20significantly%20outperforming%20existing%20baselines.%20Beyond%20classification%2C%20we%20also%20demonstrate%20that%20the%20model%20also%20provides%20a%20scalable%20metric%20for%20out-of-distribution%20detection%20and%20automated%20data%20curation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEpistemic%2520Uncertainty%2520Quantification%2520for%2520Pre-trained%2520VLMs%2520via%2520Riemannian%2520Flow%2520Matching%26entry.906535625%3DLi%2520Ju%2520and%2520Mayank%2520Nautiyal%2520and%2520Andreas%2520Hellander%2520and%2520Ekta%2520Vats%2520and%2520Prashant%2520Singh%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520are%2520typically%2520deterministic%2520in%2520nature%2520and%2520lack%2520intrinsic%2520mechanisms%2520to%2520quantify%2520epistemic%2520uncertainty%252C%2520which%2520reflects%2520the%2520model%2527s%2520lack%2520of%2520knowledge%2520or%2520ignorance%2520of%2520its%2520own%2520representations.%2520We%2520theoretically%2520motivate%2520negative%2520log-density%2520of%2520an%2520embedding%2520as%2520a%2520proxy%2520for%2520the%2520epistemic%2520uncertainty%252C%2520where%2520low-density%2520regions%2520signify%2520model%2520ignorance.%2520The%2520proposed%2520method%2520REPVLM%2520computes%2520the%2520probability%2520density%2520on%2520the%2520hyperspherical%2520manifold%2520of%2520the%2520VLM%2520embeddings%2520using%2520Riemannian%2520Flow%2520Matching.%2520We%2520empirically%2520demonstrate%2520that%2520REPVLM%2520achieves%2520near-perfect%2520correlation%2520between%2520uncertainty%2520and%2520prediction%2520error%252C%2520significantly%2520outperforming%2520existing%2520baselines.%2520Beyond%2520classification%252C%2520we%2520also%2520demonstrate%2520that%2520the%2520model%2520also%2520provides%2520a%2520scalable%2520metric%2520for%2520out-of-distribution%2520detection%2520and%2520automated%2520data%2520curation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Epistemic%20Uncertainty%20Quantification%20for%20Pre-trained%20VLMs%20via%20Riemannian%20Flow%20Matching&entry.906535625=Li%20Ju%20and%20Mayank%20Nautiyal%20and%20Andreas%20Hellander%20and%20Ekta%20Vats%20and%20Prashant%20Singh&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20are%20typically%20deterministic%20in%20nature%20and%20lack%20intrinsic%20mechanisms%20to%20quantify%20epistemic%20uncertainty%2C%20which%20reflects%20the%20model%27s%20lack%20of%20knowledge%20or%20ignorance%20of%20its%20own%20representations.%20We%20theoretically%20motivate%20negative%20log-density%20of%20an%20embedding%20as%20a%20proxy%20for%20the%20epistemic%20uncertainty%2C%20where%20low-density%20regions%20signify%20model%20ignorance.%20The%20proposed%20method%20REPVLM%20computes%20the%20probability%20density%20on%20the%20hyperspherical%20manifold%20of%20the%20VLM%20embeddings%20using%20Riemannian%20Flow%20Matching.%20We%20empirically%20demonstrate%20that%20REPVLM%20achieves%20near-perfect%20correlation%20between%20uncertainty%20and%20prediction%20error%2C%20significantly%20outperforming%20existing%20baselines.%20Beyond%20classification%2C%20we%20also%20demonstrate%20that%20the%20model%20also%20provides%20a%20scalable%20metric%20for%20out-of-distribution%20detection%20and%20automated%20data%20curation.&entry.1838667208=http%3A//arxiv.org/abs/2601.21662v1&entry.124074799=Read"},
{"title": "Cascaded Transfer: Learning Many Tasks under Budget Constraints", "author": "Eloi Campagne and Yvenn Amara-Ouali and Yannig Goude and Mathilde Mougeot and Argyris Kalogeratos", "abstract": "Many-Task Learning refers to the setting where a large number of related tasks need to be learned, the exact relationships between tasks are not known. We introduce the Cascaded Transfer Learning, a novel many-task transfer learning paradigm where information (e.g. model parameters) cascades hierarchically through tasks that are learned by individual models of the same class, while respecting given budget constraints. The cascade is organized as a rooted tree that specifies the order in which tasks are learned and refined. We design a cascaded transfer mechanism deployed over a minimum spanning tree structure that connects the tasks according to a suitable distance measure, and allocates the available training budget along its branches. Experiments on synthetic and real many-task settings show that the resulting method enables more accurate and cost effective adaptation across large task collections compared to alternative approaches.", "link": "http://arxiv.org/abs/2601.21513v1", "date": "2026-01-29", "relevancy": 2.2862, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4733}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4518}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cascaded%20Transfer%3A%20Learning%20Many%20Tasks%20under%20Budget%20Constraints&body=Title%3A%20Cascaded%20Transfer%3A%20Learning%20Many%20Tasks%20under%20Budget%20Constraints%0AAuthor%3A%20Eloi%20Campagne%20and%20Yvenn%20Amara-Ouali%20and%20Yannig%20Goude%20and%20Mathilde%20Mougeot%20and%20Argyris%20Kalogeratos%0AAbstract%3A%20Many-Task%20Learning%20refers%20to%20the%20setting%20where%20a%20large%20number%20of%20related%20tasks%20need%20to%20be%20learned%2C%20the%20exact%20relationships%20between%20tasks%20are%20not%20known.%20We%20introduce%20the%20Cascaded%20Transfer%20Learning%2C%20a%20novel%20many-task%20transfer%20learning%20paradigm%20where%20information%20%28e.g.%20model%20parameters%29%20cascades%20hierarchically%20through%20tasks%20that%20are%20learned%20by%20individual%20models%20of%20the%20same%20class%2C%20while%20respecting%20given%20budget%20constraints.%20The%20cascade%20is%20organized%20as%20a%20rooted%20tree%20that%20specifies%20the%20order%20in%20which%20tasks%20are%20learned%20and%20refined.%20We%20design%20a%20cascaded%20transfer%20mechanism%20deployed%20over%20a%20minimum%20spanning%20tree%20structure%20that%20connects%20the%20tasks%20according%20to%20a%20suitable%20distance%20measure%2C%20and%20allocates%20the%20available%20training%20budget%20along%20its%20branches.%20Experiments%20on%20synthetic%20and%20real%20many-task%20settings%20show%20that%20the%20resulting%20method%20enables%20more%20accurate%20and%20cost%20effective%20adaptation%20across%20large%20task%20collections%20compared%20to%20alternative%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCascaded%2520Transfer%253A%2520Learning%2520Many%2520Tasks%2520under%2520Budget%2520Constraints%26entry.906535625%3DEloi%2520Campagne%2520and%2520Yvenn%2520Amara-Ouali%2520and%2520Yannig%2520Goude%2520and%2520Mathilde%2520Mougeot%2520and%2520Argyris%2520Kalogeratos%26entry.1292438233%3DMany-Task%2520Learning%2520refers%2520to%2520the%2520setting%2520where%2520a%2520large%2520number%2520of%2520related%2520tasks%2520need%2520to%2520be%2520learned%252C%2520the%2520exact%2520relationships%2520between%2520tasks%2520are%2520not%2520known.%2520We%2520introduce%2520the%2520Cascaded%2520Transfer%2520Learning%252C%2520a%2520novel%2520many-task%2520transfer%2520learning%2520paradigm%2520where%2520information%2520%2528e.g.%2520model%2520parameters%2529%2520cascades%2520hierarchically%2520through%2520tasks%2520that%2520are%2520learned%2520by%2520individual%2520models%2520of%2520the%2520same%2520class%252C%2520while%2520respecting%2520given%2520budget%2520constraints.%2520The%2520cascade%2520is%2520organized%2520as%2520a%2520rooted%2520tree%2520that%2520specifies%2520the%2520order%2520in%2520which%2520tasks%2520are%2520learned%2520and%2520refined.%2520We%2520design%2520a%2520cascaded%2520transfer%2520mechanism%2520deployed%2520over%2520a%2520minimum%2520spanning%2520tree%2520structure%2520that%2520connects%2520the%2520tasks%2520according%2520to%2520a%2520suitable%2520distance%2520measure%252C%2520and%2520allocates%2520the%2520available%2520training%2520budget%2520along%2520its%2520branches.%2520Experiments%2520on%2520synthetic%2520and%2520real%2520many-task%2520settings%2520show%2520that%2520the%2520resulting%2520method%2520enables%2520more%2520accurate%2520and%2520cost%2520effective%2520adaptation%2520across%2520large%2520task%2520collections%2520compared%2520to%2520alternative%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cascaded%20Transfer%3A%20Learning%20Many%20Tasks%20under%20Budget%20Constraints&entry.906535625=Eloi%20Campagne%20and%20Yvenn%20Amara-Ouali%20and%20Yannig%20Goude%20and%20Mathilde%20Mougeot%20and%20Argyris%20Kalogeratos&entry.1292438233=Many-Task%20Learning%20refers%20to%20the%20setting%20where%20a%20large%20number%20of%20related%20tasks%20need%20to%20be%20learned%2C%20the%20exact%20relationships%20between%20tasks%20are%20not%20known.%20We%20introduce%20the%20Cascaded%20Transfer%20Learning%2C%20a%20novel%20many-task%20transfer%20learning%20paradigm%20where%20information%20%28e.g.%20model%20parameters%29%20cascades%20hierarchically%20through%20tasks%20that%20are%20learned%20by%20individual%20models%20of%20the%20same%20class%2C%20while%20respecting%20given%20budget%20constraints.%20The%20cascade%20is%20organized%20as%20a%20rooted%20tree%20that%20specifies%20the%20order%20in%20which%20tasks%20are%20learned%20and%20refined.%20We%20design%20a%20cascaded%20transfer%20mechanism%20deployed%20over%20a%20minimum%20spanning%20tree%20structure%20that%20connects%20the%20tasks%20according%20to%20a%20suitable%20distance%20measure%2C%20and%20allocates%20the%20available%20training%20budget%20along%20its%20branches.%20Experiments%20on%20synthetic%20and%20real%20many-task%20settings%20show%20that%20the%20resulting%20method%20enables%20more%20accurate%20and%20cost%20effective%20adaptation%20across%20large%20task%20collections%20compared%20to%20alternative%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2601.21513v1&entry.124074799=Read"},
{"title": "VideoAesBench: Benchmarking the Video Aesthetics Perception Capabilities of Large Multimodal Models", "author": "Yunhao Li and Sijing Wu and Zhilin Gao and Zicheng Zhang and Qi Jia and Huiyu Duan and Xiongkuo Min and Guangtao Zhai", "abstract": "Large multimodal models (LMMs) have demonstrated outstanding capabilities in various visual perception tasks, which has in turn made the evaluation of LMMs significant. However, the capability of video aesthetic quality assessment, which is a fundamental ability for human, remains underexplored for LMMs. To address this, we introduce VideoAesBench, a comprehensive benchmark for evaluating LMMs' understanding of video aesthetic quality. VideoAesBench has several significant characteristics: (1) Diverse content including 1,804 videos from multiple video sources including user-generated (UGC), AI-generated (AIGC), compressed, robotic-generated (RGC), and game videos. (2) Multiple question formats containing traditional single-choice questions, multi-choice questions, True or False questions, and a novel open-ended questions for video aesthetics description. (3) Holistic video aesthetics dimensions including visual form related questions from 5 aspects, visual style related questions from 4 aspects, and visual affectiveness questions from 3 aspects. Based on VideoAesBench, we benchmark 23 open-source and commercial large multimodal models. Our findings show that current LMMs only contain basic video aesthetics perception ability, their performance remains incomplete and imprecise. We hope our VideoAesBench can be served as a strong testbed and offer insights for explainable video aesthetics assessment.", "link": "http://arxiv.org/abs/2601.21915v1", "date": "2026-01-29", "relevancy": 2.2851, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5748}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5748}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoAesBench%3A%20Benchmarking%20the%20Video%20Aesthetics%20Perception%20Capabilities%20of%20Large%20Multimodal%20Models&body=Title%3A%20VideoAesBench%3A%20Benchmarking%20the%20Video%20Aesthetics%20Perception%20Capabilities%20of%20Large%20Multimodal%20Models%0AAuthor%3A%20Yunhao%20Li%20and%20Sijing%20Wu%20and%20Zhilin%20Gao%20and%20Zicheng%20Zhang%20and%20Qi%20Jia%20and%20Huiyu%20Duan%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%0AAbstract%3A%20Large%20multimodal%20models%20%28LMMs%29%20have%20demonstrated%20outstanding%20capabilities%20in%20various%20visual%20perception%20tasks%2C%20which%20has%20in%20turn%20made%20the%20evaluation%20of%20LMMs%20significant.%20However%2C%20the%20capability%20of%20video%20aesthetic%20quality%20assessment%2C%20which%20is%20a%20fundamental%20ability%20for%20human%2C%20remains%20underexplored%20for%20LMMs.%20To%20address%20this%2C%20we%20introduce%20VideoAesBench%2C%20a%20comprehensive%20benchmark%20for%20evaluating%20LMMs%27%20understanding%20of%20video%20aesthetic%20quality.%20VideoAesBench%20has%20several%20significant%20characteristics%3A%20%281%29%20Diverse%20content%20including%201%2C804%20videos%20from%20multiple%20video%20sources%20including%20user-generated%20%28UGC%29%2C%20AI-generated%20%28AIGC%29%2C%20compressed%2C%20robotic-generated%20%28RGC%29%2C%20and%20game%20videos.%20%282%29%20Multiple%20question%20formats%20containing%20traditional%20single-choice%20questions%2C%20multi-choice%20questions%2C%20True%20or%20False%20questions%2C%20and%20a%20novel%20open-ended%20questions%20for%20video%20aesthetics%20description.%20%283%29%20Holistic%20video%20aesthetics%20dimensions%20including%20visual%20form%20related%20questions%20from%205%20aspects%2C%20visual%20style%20related%20questions%20from%204%20aspects%2C%20and%20visual%20affectiveness%20questions%20from%203%20aspects.%20Based%20on%20VideoAesBench%2C%20we%20benchmark%2023%20open-source%20and%20commercial%20large%20multimodal%20models.%20Our%20findings%20show%20that%20current%20LMMs%20only%20contain%20basic%20video%20aesthetics%20perception%20ability%2C%20their%20performance%20remains%20incomplete%20and%20imprecise.%20We%20hope%20our%20VideoAesBench%20can%20be%20served%20as%20a%20strong%20testbed%20and%20offer%20insights%20for%20explainable%20video%20aesthetics%20assessment.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoAesBench%253A%2520Benchmarking%2520the%2520Video%2520Aesthetics%2520Perception%2520Capabilities%2520of%2520Large%2520Multimodal%2520Models%26entry.906535625%3DYunhao%2520Li%2520and%2520Sijing%2520Wu%2520and%2520Zhilin%2520Gao%2520and%2520Zicheng%2520Zhang%2520and%2520Qi%2520Jia%2520and%2520Huiyu%2520Duan%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3DLarge%2520multimodal%2520models%2520%2528LMMs%2529%2520have%2520demonstrated%2520outstanding%2520capabilities%2520in%2520various%2520visual%2520perception%2520tasks%252C%2520which%2520has%2520in%2520turn%2520made%2520the%2520evaluation%2520of%2520LMMs%2520significant.%2520However%252C%2520the%2520capability%2520of%2520video%2520aesthetic%2520quality%2520assessment%252C%2520which%2520is%2520a%2520fundamental%2520ability%2520for%2520human%252C%2520remains%2520underexplored%2520for%2520LMMs.%2520To%2520address%2520this%252C%2520we%2520introduce%2520VideoAesBench%252C%2520a%2520comprehensive%2520benchmark%2520for%2520evaluating%2520LMMs%2527%2520understanding%2520of%2520video%2520aesthetic%2520quality.%2520VideoAesBench%2520has%2520several%2520significant%2520characteristics%253A%2520%25281%2529%2520Diverse%2520content%2520including%25201%252C804%2520videos%2520from%2520multiple%2520video%2520sources%2520including%2520user-generated%2520%2528UGC%2529%252C%2520AI-generated%2520%2528AIGC%2529%252C%2520compressed%252C%2520robotic-generated%2520%2528RGC%2529%252C%2520and%2520game%2520videos.%2520%25282%2529%2520Multiple%2520question%2520formats%2520containing%2520traditional%2520single-choice%2520questions%252C%2520multi-choice%2520questions%252C%2520True%2520or%2520False%2520questions%252C%2520and%2520a%2520novel%2520open-ended%2520questions%2520for%2520video%2520aesthetics%2520description.%2520%25283%2529%2520Holistic%2520video%2520aesthetics%2520dimensions%2520including%2520visual%2520form%2520related%2520questions%2520from%25205%2520aspects%252C%2520visual%2520style%2520related%2520questions%2520from%25204%2520aspects%252C%2520and%2520visual%2520affectiveness%2520questions%2520from%25203%2520aspects.%2520Based%2520on%2520VideoAesBench%252C%2520we%2520benchmark%252023%2520open-source%2520and%2520commercial%2520large%2520multimodal%2520models.%2520Our%2520findings%2520show%2520that%2520current%2520LMMs%2520only%2520contain%2520basic%2520video%2520aesthetics%2520perception%2520ability%252C%2520their%2520performance%2520remains%2520incomplete%2520and%2520imprecise.%2520We%2520hope%2520our%2520VideoAesBench%2520can%2520be%2520served%2520as%2520a%2520strong%2520testbed%2520and%2520offer%2520insights%2520for%2520explainable%2520video%2520aesthetics%2520assessment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoAesBench%3A%20Benchmarking%20the%20Video%20Aesthetics%20Perception%20Capabilities%20of%20Large%20Multimodal%20Models&entry.906535625=Yunhao%20Li%20and%20Sijing%20Wu%20and%20Zhilin%20Gao%20and%20Zicheng%20Zhang%20and%20Qi%20Jia%20and%20Huiyu%20Duan%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai&entry.1292438233=Large%20multimodal%20models%20%28LMMs%29%20have%20demonstrated%20outstanding%20capabilities%20in%20various%20visual%20perception%20tasks%2C%20which%20has%20in%20turn%20made%20the%20evaluation%20of%20LMMs%20significant.%20However%2C%20the%20capability%20of%20video%20aesthetic%20quality%20assessment%2C%20which%20is%20a%20fundamental%20ability%20for%20human%2C%20remains%20underexplored%20for%20LMMs.%20To%20address%20this%2C%20we%20introduce%20VideoAesBench%2C%20a%20comprehensive%20benchmark%20for%20evaluating%20LMMs%27%20understanding%20of%20video%20aesthetic%20quality.%20VideoAesBench%20has%20several%20significant%20characteristics%3A%20%281%29%20Diverse%20content%20including%201%2C804%20videos%20from%20multiple%20video%20sources%20including%20user-generated%20%28UGC%29%2C%20AI-generated%20%28AIGC%29%2C%20compressed%2C%20robotic-generated%20%28RGC%29%2C%20and%20game%20videos.%20%282%29%20Multiple%20question%20formats%20containing%20traditional%20single-choice%20questions%2C%20multi-choice%20questions%2C%20True%20or%20False%20questions%2C%20and%20a%20novel%20open-ended%20questions%20for%20video%20aesthetics%20description.%20%283%29%20Holistic%20video%20aesthetics%20dimensions%20including%20visual%20form%20related%20questions%20from%205%20aspects%2C%20visual%20style%20related%20questions%20from%204%20aspects%2C%20and%20visual%20affectiveness%20questions%20from%203%20aspects.%20Based%20on%20VideoAesBench%2C%20we%20benchmark%2023%20open-source%20and%20commercial%20large%20multimodal%20models.%20Our%20findings%20show%20that%20current%20LMMs%20only%20contain%20basic%20video%20aesthetics%20perception%20ability%2C%20their%20performance%20remains%20incomplete%20and%20imprecise.%20We%20hope%20our%20VideoAesBench%20can%20be%20served%20as%20a%20strong%20testbed%20and%20offer%20insights%20for%20explainable%20video%20aesthetics%20assessment.&entry.1838667208=http%3A//arxiv.org/abs/2601.21915v1&entry.124074799=Read"},
{"title": "A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion", "author": "Willams de Lima Costa and Thifany Ketuli Silva de Souza and Jonas Ferreira Silva and Carlos Gabriel Bezerra Pereira and Bruno Reis Vila Nova and Leonardo Silvino Brito and Rafael Raider Leoni and Juliano Silva Filho and Valter Ferreira and Sibele Miguel Soares Neto and Samantha Uehara and Daniel Giacometti Amaral and Jo\u00e3o Marcelo Teixeira and Veronica Teichrieb and Cristiano Coelho de Ara\u00fajo", "abstract": "Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.", "link": "http://arxiv.org/abs/2601.20847v2", "date": "2026-01-29", "relevancy": 2.2729, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6044}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5761}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20New%20Dataset%20and%20Framework%20for%20Robust%20Road%20Surface%20Classification%20via%20Camera-IMU%20Fusion&body=Title%3A%20A%20New%20Dataset%20and%20Framework%20for%20Robust%20Road%20Surface%20Classification%20via%20Camera-IMU%20Fusion%0AAuthor%3A%20Willams%20de%20Lima%20Costa%20and%20Thifany%20Ketuli%20Silva%20de%20Souza%20and%20Jonas%20Ferreira%20Silva%20and%20Carlos%20Gabriel%20Bezerra%20Pereira%20and%20Bruno%20Reis%20Vila%20Nova%20and%20Leonardo%20Silvino%20Brito%20and%20Rafael%20Raider%20Leoni%20and%20Juliano%20Silva%20Filho%20and%20Valter%20Ferreira%20and%20Sibele%20Miguel%20Soares%20Neto%20and%20Samantha%20Uehara%20and%20Daniel%20Giacometti%20Amaral%20and%20Jo%C3%A3o%20Marcelo%20Teixeira%20and%20Veronica%20Teichrieb%20and%20Cristiano%20Coelho%20de%20Ara%C3%BAjo%0AAbstract%3A%20Road%20surface%20classification%20%28RSC%29%20is%20a%20key%20enabler%20for%20environment-aware%20predictive%20maintenance%20systems.%20However%2C%20existing%20RSC%20techniques%20often%20fail%20to%20generalize%20beyond%20narrow%20operational%20conditions%20due%20to%20limited%20sensing%20modalities%20and%20datasets%20that%20lack%20environmental%20diversity.%20This%20work%20addresses%20these%20limitations%20by%20introducing%20a%20multimodal%20framework%20that%20fuses%20images%20and%20inertial%20measurements%20using%20a%20lightweight%20bidirectional%20cross-attention%20module%20followed%20by%20an%20adaptive%20gating%20layer%20that%20adjusts%20modality%20contributions%20under%20domain%20shifts.%20Given%20the%20limitations%20of%20current%20benchmarks%2C%20especially%20regarding%20lack%20of%20variability%2C%20we%20introduce%20ROAD%2C%20a%20new%20dataset%20composed%20of%20three%20complementary%20subsets%3A%20%28i%29%20real-world%20multimodal%20recordings%20with%20RGB-IMU%20streams%20synchronized%20using%20a%20gold-standard%20industry%20datalogger%2C%20captured%20across%20diverse%20lighting%2C%20weather%2C%20and%20surface%20conditions%3B%20%28ii%29%20a%20large%20vision-only%20subset%20designed%20to%20assess%20robustness%20under%20adverse%20illumination%20and%20heterogeneous%20capture%20setups%3B%20and%20%28iii%29%20a%20synthetic%20subset%20generated%20to%20study%20out-of-distribution%20generalization%20in%20scenarios%20difficult%20to%20obtain%20in%20practice.%20Experiments%20show%20that%20our%20method%20achieves%20a%20%2B1.4%20pp%20improvement%20over%20the%20previous%20state-of-the-art%20on%20the%20PVS%20benchmark%20and%20an%20%2B11.6%20pp%20improvement%20on%20our%20multimodal%20ROAD%20subset%2C%20with%20consistently%20higher%20F1-scores%20on%20minority%20classes.%20The%20framework%20also%20demonstrates%20stable%20performance%20across%20challenging%20visual%20conditions%2C%20including%20nighttime%2C%20heavy%20rain%2C%20and%20mixed-surface%20transitions.%20These%20findings%20indicate%20that%20combining%20affordable%20camera%20and%20IMU%20sensors%20with%20multimodal%20attention%20mechanisms%20provides%20a%20scalable%2C%20robust%20foundation%20for%20road%20surface%20understanding%2C%20particularly%20relevant%20for%20regions%20where%20environmental%20variability%20and%20cost%20constraints%20limit%20the%20adoption%20of%20high-end%20sensing%20suites.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20847v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520New%2520Dataset%2520and%2520Framework%2520for%2520Robust%2520Road%2520Surface%2520Classification%2520via%2520Camera-IMU%2520Fusion%26entry.906535625%3DWillams%2520de%2520Lima%2520Costa%2520and%2520Thifany%2520Ketuli%2520Silva%2520de%2520Souza%2520and%2520Jonas%2520Ferreira%2520Silva%2520and%2520Carlos%2520Gabriel%2520Bezerra%2520Pereira%2520and%2520Bruno%2520Reis%2520Vila%2520Nova%2520and%2520Leonardo%2520Silvino%2520Brito%2520and%2520Rafael%2520Raider%2520Leoni%2520and%2520Juliano%2520Silva%2520Filho%2520and%2520Valter%2520Ferreira%2520and%2520Sibele%2520Miguel%2520Soares%2520Neto%2520and%2520Samantha%2520Uehara%2520and%2520Daniel%2520Giacometti%2520Amaral%2520and%2520Jo%25C3%25A3o%2520Marcelo%2520Teixeira%2520and%2520Veronica%2520Teichrieb%2520and%2520Cristiano%2520Coelho%2520de%2520Ara%25C3%25BAjo%26entry.1292438233%3DRoad%2520surface%2520classification%2520%2528RSC%2529%2520is%2520a%2520key%2520enabler%2520for%2520environment-aware%2520predictive%2520maintenance%2520systems.%2520However%252C%2520existing%2520RSC%2520techniques%2520often%2520fail%2520to%2520generalize%2520beyond%2520narrow%2520operational%2520conditions%2520due%2520to%2520limited%2520sensing%2520modalities%2520and%2520datasets%2520that%2520lack%2520environmental%2520diversity.%2520This%2520work%2520addresses%2520these%2520limitations%2520by%2520introducing%2520a%2520multimodal%2520framework%2520that%2520fuses%2520images%2520and%2520inertial%2520measurements%2520using%2520a%2520lightweight%2520bidirectional%2520cross-attention%2520module%2520followed%2520by%2520an%2520adaptive%2520gating%2520layer%2520that%2520adjusts%2520modality%2520contributions%2520under%2520domain%2520shifts.%2520Given%2520the%2520limitations%2520of%2520current%2520benchmarks%252C%2520especially%2520regarding%2520lack%2520of%2520variability%252C%2520we%2520introduce%2520ROAD%252C%2520a%2520new%2520dataset%2520composed%2520of%2520three%2520complementary%2520subsets%253A%2520%2528i%2529%2520real-world%2520multimodal%2520recordings%2520with%2520RGB-IMU%2520streams%2520synchronized%2520using%2520a%2520gold-standard%2520industry%2520datalogger%252C%2520captured%2520across%2520diverse%2520lighting%252C%2520weather%252C%2520and%2520surface%2520conditions%253B%2520%2528ii%2529%2520a%2520large%2520vision-only%2520subset%2520designed%2520to%2520assess%2520robustness%2520under%2520adverse%2520illumination%2520and%2520heterogeneous%2520capture%2520setups%253B%2520and%2520%2528iii%2529%2520a%2520synthetic%2520subset%2520generated%2520to%2520study%2520out-of-distribution%2520generalization%2520in%2520scenarios%2520difficult%2520to%2520obtain%2520in%2520practice.%2520Experiments%2520show%2520that%2520our%2520method%2520achieves%2520a%2520%252B1.4%2520pp%2520improvement%2520over%2520the%2520previous%2520state-of-the-art%2520on%2520the%2520PVS%2520benchmark%2520and%2520an%2520%252B11.6%2520pp%2520improvement%2520on%2520our%2520multimodal%2520ROAD%2520subset%252C%2520with%2520consistently%2520higher%2520F1-scores%2520on%2520minority%2520classes.%2520The%2520framework%2520also%2520demonstrates%2520stable%2520performance%2520across%2520challenging%2520visual%2520conditions%252C%2520including%2520nighttime%252C%2520heavy%2520rain%252C%2520and%2520mixed-surface%2520transitions.%2520These%2520findings%2520indicate%2520that%2520combining%2520affordable%2520camera%2520and%2520IMU%2520sensors%2520with%2520multimodal%2520attention%2520mechanisms%2520provides%2520a%2520scalable%252C%2520robust%2520foundation%2520for%2520road%2520surface%2520understanding%252C%2520particularly%2520relevant%2520for%2520regions%2520where%2520environmental%2520variability%2520and%2520cost%2520constraints%2520limit%2520the%2520adoption%2520of%2520high-end%2520sensing%2520suites.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20847v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Dataset%20and%20Framework%20for%20Robust%20Road%20Surface%20Classification%20via%20Camera-IMU%20Fusion&entry.906535625=Willams%20de%20Lima%20Costa%20and%20Thifany%20Ketuli%20Silva%20de%20Souza%20and%20Jonas%20Ferreira%20Silva%20and%20Carlos%20Gabriel%20Bezerra%20Pereira%20and%20Bruno%20Reis%20Vila%20Nova%20and%20Leonardo%20Silvino%20Brito%20and%20Rafael%20Raider%20Leoni%20and%20Juliano%20Silva%20Filho%20and%20Valter%20Ferreira%20and%20Sibele%20Miguel%20Soares%20Neto%20and%20Samantha%20Uehara%20and%20Daniel%20Giacometti%20Amaral%20and%20Jo%C3%A3o%20Marcelo%20Teixeira%20and%20Veronica%20Teichrieb%20and%20Cristiano%20Coelho%20de%20Ara%C3%BAjo&entry.1292438233=Road%20surface%20classification%20%28RSC%29%20is%20a%20key%20enabler%20for%20environment-aware%20predictive%20maintenance%20systems.%20However%2C%20existing%20RSC%20techniques%20often%20fail%20to%20generalize%20beyond%20narrow%20operational%20conditions%20due%20to%20limited%20sensing%20modalities%20and%20datasets%20that%20lack%20environmental%20diversity.%20This%20work%20addresses%20these%20limitations%20by%20introducing%20a%20multimodal%20framework%20that%20fuses%20images%20and%20inertial%20measurements%20using%20a%20lightweight%20bidirectional%20cross-attention%20module%20followed%20by%20an%20adaptive%20gating%20layer%20that%20adjusts%20modality%20contributions%20under%20domain%20shifts.%20Given%20the%20limitations%20of%20current%20benchmarks%2C%20especially%20regarding%20lack%20of%20variability%2C%20we%20introduce%20ROAD%2C%20a%20new%20dataset%20composed%20of%20three%20complementary%20subsets%3A%20%28i%29%20real-world%20multimodal%20recordings%20with%20RGB-IMU%20streams%20synchronized%20using%20a%20gold-standard%20industry%20datalogger%2C%20captured%20across%20diverse%20lighting%2C%20weather%2C%20and%20surface%20conditions%3B%20%28ii%29%20a%20large%20vision-only%20subset%20designed%20to%20assess%20robustness%20under%20adverse%20illumination%20and%20heterogeneous%20capture%20setups%3B%20and%20%28iii%29%20a%20synthetic%20subset%20generated%20to%20study%20out-of-distribution%20generalization%20in%20scenarios%20difficult%20to%20obtain%20in%20practice.%20Experiments%20show%20that%20our%20method%20achieves%20a%20%2B1.4%20pp%20improvement%20over%20the%20previous%20state-of-the-art%20on%20the%20PVS%20benchmark%20and%20an%20%2B11.6%20pp%20improvement%20on%20our%20multimodal%20ROAD%20subset%2C%20with%20consistently%20higher%20F1-scores%20on%20minority%20classes.%20The%20framework%20also%20demonstrates%20stable%20performance%20across%20challenging%20visual%20conditions%2C%20including%20nighttime%2C%20heavy%20rain%2C%20and%20mixed-surface%20transitions.%20These%20findings%20indicate%20that%20combining%20affordable%20camera%20and%20IMU%20sensors%20with%20multimodal%20attention%20mechanisms%20provides%20a%20scalable%2C%20robust%20foundation%20for%20road%20surface%20understanding%2C%20particularly%20relevant%20for%20regions%20where%20environmental%20variability%20and%20cost%20constraints%20limit%20the%20adoption%20of%20high-end%20sensing%20suites.&entry.1838667208=http%3A//arxiv.org/abs/2601.20847v2&entry.124074799=Read"},
{"title": "Residual Reservoir Memory Networks", "author": "Matteo Pinna and Andrea Ceni and Claudio Gallicchio", "abstract": "We introduce a novel class of untrained Recurrent Neural Networks (RNNs) within the Reservoir Computing (RC) paradigm, called Residual Reservoir Memory Networks (ResRMNs). ResRMN combines a linear memory reservoir with a non-linear reservoir, where the latter is based on residual orthogonal connections along the temporal dimension for enhanced long-term propagation of the input. The resulting reservoir state dynamics are studied through the lens of linear stability analysis, and we investigate diverse configurations for the temporal residual connections. The proposed approach is empirically assessed on time-series and pixel-level 1-D classification tasks. Our experimental results highlight the advantages of the proposed approach over other conventional RC models.", "link": "http://arxiv.org/abs/2508.09925v2", "date": "2026-01-29", "relevancy": 2.2701, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4792}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4461}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20Reservoir%20Memory%20Networks&body=Title%3A%20Residual%20Reservoir%20Memory%20Networks%0AAuthor%3A%20Matteo%20Pinna%20and%20Andrea%20Ceni%20and%20Claudio%20Gallicchio%0AAbstract%3A%20We%20introduce%20a%20novel%20class%20of%20untrained%20Recurrent%20Neural%20Networks%20%28RNNs%29%20within%20the%20Reservoir%20Computing%20%28RC%29%20paradigm%2C%20called%20Residual%20Reservoir%20Memory%20Networks%20%28ResRMNs%29.%20ResRMN%20combines%20a%20linear%20memory%20reservoir%20with%20a%20non-linear%20reservoir%2C%20where%20the%20latter%20is%20based%20on%20residual%20orthogonal%20connections%20along%20the%20temporal%20dimension%20for%20enhanced%20long-term%20propagation%20of%20the%20input.%20The%20resulting%20reservoir%20state%20dynamics%20are%20studied%20through%20the%20lens%20of%20linear%20stability%20analysis%2C%20and%20we%20investigate%20diverse%20configurations%20for%20the%20temporal%20residual%20connections.%20The%20proposed%20approach%20is%20empirically%20assessed%20on%20time-series%20and%20pixel-level%201-D%20classification%20tasks.%20Our%20experimental%20results%20highlight%20the%20advantages%20of%20the%20proposed%20approach%20over%20other%20conventional%20RC%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2508.09925v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520Reservoir%2520Memory%2520Networks%26entry.906535625%3DMatteo%2520Pinna%2520and%2520Andrea%2520Ceni%2520and%2520Claudio%2520Gallicchio%26entry.1292438233%3DWe%2520introduce%2520a%2520novel%2520class%2520of%2520untrained%2520Recurrent%2520Neural%2520Networks%2520%2528RNNs%2529%2520within%2520the%2520Reservoir%2520Computing%2520%2528RC%2529%2520paradigm%252C%2520called%2520Residual%2520Reservoir%2520Memory%2520Networks%2520%2528ResRMNs%2529.%2520ResRMN%2520combines%2520a%2520linear%2520memory%2520reservoir%2520with%2520a%2520non-linear%2520reservoir%252C%2520where%2520the%2520latter%2520is%2520based%2520on%2520residual%2520orthogonal%2520connections%2520along%2520the%2520temporal%2520dimension%2520for%2520enhanced%2520long-term%2520propagation%2520of%2520the%2520input.%2520The%2520resulting%2520reservoir%2520state%2520dynamics%2520are%2520studied%2520through%2520the%2520lens%2520of%2520linear%2520stability%2520analysis%252C%2520and%2520we%2520investigate%2520diverse%2520configurations%2520for%2520the%2520temporal%2520residual%2520connections.%2520The%2520proposed%2520approach%2520is%2520empirically%2520assessed%2520on%2520time-series%2520and%2520pixel-level%25201-D%2520classification%2520tasks.%2520Our%2520experimental%2520results%2520highlight%2520the%2520advantages%2520of%2520the%2520proposed%2520approach%2520over%2520other%2520conventional%2520RC%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09925v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Reservoir%20Memory%20Networks&entry.906535625=Matteo%20Pinna%20and%20Andrea%20Ceni%20and%20Claudio%20Gallicchio&entry.1292438233=We%20introduce%20a%20novel%20class%20of%20untrained%20Recurrent%20Neural%20Networks%20%28RNNs%29%20within%20the%20Reservoir%20Computing%20%28RC%29%20paradigm%2C%20called%20Residual%20Reservoir%20Memory%20Networks%20%28ResRMNs%29.%20ResRMN%20combines%20a%20linear%20memory%20reservoir%20with%20a%20non-linear%20reservoir%2C%20where%20the%20latter%20is%20based%20on%20residual%20orthogonal%20connections%20along%20the%20temporal%20dimension%20for%20enhanced%20long-term%20propagation%20of%20the%20input.%20The%20resulting%20reservoir%20state%20dynamics%20are%20studied%20through%20the%20lens%20of%20linear%20stability%20analysis%2C%20and%20we%20investigate%20diverse%20configurations%20for%20the%20temporal%20residual%20connections.%20The%20proposed%20approach%20is%20empirically%20assessed%20on%20time-series%20and%20pixel-level%201-D%20classification%20tasks.%20Our%20experimental%20results%20highlight%20the%20advantages%20of%20the%20proposed%20approach%20over%20other%20conventional%20RC%20models.&entry.1838667208=http%3A//arxiv.org/abs/2508.09925v2&entry.124074799=Read"},
{"title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation", "author": "Haozhe Xie and Beichen Wen and Jiarui Zheng and Zhaoxi Chen and Fangzhou Hong and Haiwen Diao and Ziwei Liu", "abstract": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.", "link": "http://arxiv.org/abs/2601.22153v1", "date": "2026-01-29", "relevancy": 2.2652, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5788}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynamicVLA%3A%20A%20Vision-Language-Action%20Model%20for%20Dynamic%20Object%20Manipulation&body=Title%3A%20DynamicVLA%3A%20A%20Vision-Language-Action%20Model%20for%20Dynamic%20Object%20Manipulation%0AAuthor%3A%20Haozhe%20Xie%20and%20Beichen%20Wen%20and%20Jiarui%20Zheng%20and%20Zhaoxi%20Chen%20and%20Fangzhou%20Hong%20and%20Haiwen%20Diao%20and%20Ziwei%20Liu%0AAbstract%3A%20Manipulating%20dynamic%20objects%20remains%20an%20open%20challenge%20for%20Vision-Language-Action%20%28VLA%29%20models%2C%20which%2C%20despite%20strong%20generalization%20in%20static%20manipulation%2C%20struggle%20in%20dynamic%20scenarios%20requiring%20rapid%20perception%2C%20temporal%20anticipation%2C%20and%20continuous%20control.%20We%20present%20DynamicVLA%2C%20a%20framework%20for%20dynamic%20object%20manipulation%20that%20integrates%20temporal%20reasoning%20and%20closed-loop%20adaptation%20through%20three%20key%20designs%3A%201%29%20a%20compact%200.4B%20VLA%20using%20a%20convolutional%20vision%20encoder%20for%20spatially%20efficient%2C%20structurally%20faithful%20encoding%2C%20enabling%20fast%20multimodal%20inference%3B%202%29%20Continuous%20Inference%2C%20enabling%20overlapping%20reasoning%20and%20execution%20for%20lower%20latency%20and%20timely%20adaptation%20to%20object%20motion%3B%20and%203%29%20Latent-aware%20Action%20Streaming%2C%20which%20bridges%20the%20perception-execution%20gap%20by%20enforcing%20temporally%20aligned%20action%20execution.%20To%20fill%20the%20missing%20foundation%20of%20dynamic%20manipulation%20data%2C%20we%20introduce%20the%20Dynamic%20Object%20Manipulation%20%28DOM%29%20benchmark%2C%20built%20from%20scratch%20with%20an%20auto%20data%20collection%20pipeline%20that%20efficiently%20gathers%20200K%20synthetic%20episodes%20across%202.8K%20scenes%20and%20206%20objects%2C%20and%20enables%20fast%20collection%20of%202K%20real-world%20episodes%20without%20teleoperation.%20Extensive%20evaluations%20demonstrate%20remarkable%20improvements%20in%20response%20speed%2C%20perception%2C%20and%20generalization%2C%20positioning%20DynamicVLA%20as%20a%20unified%20framework%20for%20general%20dynamic%20object%20manipulation%20across%20embodiments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamicVLA%253A%2520A%2520Vision-Language-Action%2520Model%2520for%2520Dynamic%2520Object%2520Manipulation%26entry.906535625%3DHaozhe%2520Xie%2520and%2520Beichen%2520Wen%2520and%2520Jiarui%2520Zheng%2520and%2520Zhaoxi%2520Chen%2520and%2520Fangzhou%2520Hong%2520and%2520Haiwen%2520Diao%2520and%2520Ziwei%2520Liu%26entry.1292438233%3DManipulating%2520dynamic%2520objects%2520remains%2520an%2520open%2520challenge%2520for%2520Vision-Language-Action%2520%2528VLA%2529%2520models%252C%2520which%252C%2520despite%2520strong%2520generalization%2520in%2520static%2520manipulation%252C%2520struggle%2520in%2520dynamic%2520scenarios%2520requiring%2520rapid%2520perception%252C%2520temporal%2520anticipation%252C%2520and%2520continuous%2520control.%2520We%2520present%2520DynamicVLA%252C%2520a%2520framework%2520for%2520dynamic%2520object%2520manipulation%2520that%2520integrates%2520temporal%2520reasoning%2520and%2520closed-loop%2520adaptation%2520through%2520three%2520key%2520designs%253A%25201%2529%2520a%2520compact%25200.4B%2520VLA%2520using%2520a%2520convolutional%2520vision%2520encoder%2520for%2520spatially%2520efficient%252C%2520structurally%2520faithful%2520encoding%252C%2520enabling%2520fast%2520multimodal%2520inference%253B%25202%2529%2520Continuous%2520Inference%252C%2520enabling%2520overlapping%2520reasoning%2520and%2520execution%2520for%2520lower%2520latency%2520and%2520timely%2520adaptation%2520to%2520object%2520motion%253B%2520and%25203%2529%2520Latent-aware%2520Action%2520Streaming%252C%2520which%2520bridges%2520the%2520perception-execution%2520gap%2520by%2520enforcing%2520temporally%2520aligned%2520action%2520execution.%2520To%2520fill%2520the%2520missing%2520foundation%2520of%2520dynamic%2520manipulation%2520data%252C%2520we%2520introduce%2520the%2520Dynamic%2520Object%2520Manipulation%2520%2528DOM%2529%2520benchmark%252C%2520built%2520from%2520scratch%2520with%2520an%2520auto%2520data%2520collection%2520pipeline%2520that%2520efficiently%2520gathers%2520200K%2520synthetic%2520episodes%2520across%25202.8K%2520scenes%2520and%2520206%2520objects%252C%2520and%2520enables%2520fast%2520collection%2520of%25202K%2520real-world%2520episodes%2520without%2520teleoperation.%2520Extensive%2520evaluations%2520demonstrate%2520remarkable%2520improvements%2520in%2520response%2520speed%252C%2520perception%252C%2520and%2520generalization%252C%2520positioning%2520DynamicVLA%2520as%2520a%2520unified%2520framework%2520for%2520general%2520dynamic%2520object%2520manipulation%2520across%2520embodiments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynamicVLA%3A%20A%20Vision-Language-Action%20Model%20for%20Dynamic%20Object%20Manipulation&entry.906535625=Haozhe%20Xie%20and%20Beichen%20Wen%20and%20Jiarui%20Zheng%20and%20Zhaoxi%20Chen%20and%20Fangzhou%20Hong%20and%20Haiwen%20Diao%20and%20Ziwei%20Liu&entry.1292438233=Manipulating%20dynamic%20objects%20remains%20an%20open%20challenge%20for%20Vision-Language-Action%20%28VLA%29%20models%2C%20which%2C%20despite%20strong%20generalization%20in%20static%20manipulation%2C%20struggle%20in%20dynamic%20scenarios%20requiring%20rapid%20perception%2C%20temporal%20anticipation%2C%20and%20continuous%20control.%20We%20present%20DynamicVLA%2C%20a%20framework%20for%20dynamic%20object%20manipulation%20that%20integrates%20temporal%20reasoning%20and%20closed-loop%20adaptation%20through%20three%20key%20designs%3A%201%29%20a%20compact%200.4B%20VLA%20using%20a%20convolutional%20vision%20encoder%20for%20spatially%20efficient%2C%20structurally%20faithful%20encoding%2C%20enabling%20fast%20multimodal%20inference%3B%202%29%20Continuous%20Inference%2C%20enabling%20overlapping%20reasoning%20and%20execution%20for%20lower%20latency%20and%20timely%20adaptation%20to%20object%20motion%3B%20and%203%29%20Latent-aware%20Action%20Streaming%2C%20which%20bridges%20the%20perception-execution%20gap%20by%20enforcing%20temporally%20aligned%20action%20execution.%20To%20fill%20the%20missing%20foundation%20of%20dynamic%20manipulation%20data%2C%20we%20introduce%20the%20Dynamic%20Object%20Manipulation%20%28DOM%29%20benchmark%2C%20built%20from%20scratch%20with%20an%20auto%20data%20collection%20pipeline%20that%20efficiently%20gathers%20200K%20synthetic%20episodes%20across%202.8K%20scenes%20and%20206%20objects%2C%20and%20enables%20fast%20collection%20of%202K%20real-world%20episodes%20without%20teleoperation.%20Extensive%20evaluations%20demonstrate%20remarkable%20improvements%20in%20response%20speed%2C%20perception%2C%20and%20generalization%2C%20positioning%20DynamicVLA%20as%20a%20unified%20framework%20for%20general%20dynamic%20object%20manipulation%20across%20embodiments.&entry.1838667208=http%3A//arxiv.org/abs/2601.22153v1&entry.124074799=Read"},
{"title": "SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding", "author": "Ahmed Y. Radwan and Christos Emmanouilidis and Hina Tabassum and Deval Pandya and Shaina Raza", "abstract": "Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard", "link": "http://arxiv.org/abs/2601.21666v1", "date": "2026-01-29", "relevancy": 2.2588, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5777}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5777}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SONIC-O1%3A%20A%20Real-World%20Benchmark%20for%20Evaluating%20Multimodal%20Large%20Language%20Models%20on%20Audio-Video%20Understanding&body=Title%3A%20SONIC-O1%3A%20A%20Real-World%20Benchmark%20for%20Evaluating%20Multimodal%20Large%20Language%20Models%20on%20Audio-Video%20Understanding%0AAuthor%3A%20Ahmed%20Y.%20Radwan%20and%20Christos%20Emmanouilidis%20and%20Hina%20Tabassum%20and%20Deval%20Pandya%20and%20Shaina%20Raza%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20a%20major%20focus%20of%20recent%20AI%20research.%20However%2C%20most%20prior%20work%20focuses%20on%20static%20image%20understanding%2C%20while%20their%20ability%20to%20process%20sequential%20audio-video%20data%20remains%20underexplored.%20This%20gap%20highlights%20the%20need%20for%20a%20high-quality%20benchmark%20to%20systematically%20evaluate%20MLLM%20performance%20in%20a%20real-world%20setting.%20We%20introduce%20SONIC-O1%2C%20a%20comprehensive%2C%20fully%20human-verified%20benchmark%20spanning%2013%20real-world%20conversational%20domains%20with%204%2C958%20annotations%20and%20demographic%20metadata.%20SONIC-O1%20evaluates%20MLLMs%20on%20key%20tasks%2C%20including%20open-ended%20summarization%2C%20multiple-choice%20question%20%28MCQ%29%20answering%2C%20and%20temporal%20localization%20with%20supporting%20rationales%20%28reasoning%29.%20Experiments%20on%20closed-%20and%20open-source%20models%20reveal%20limitations.%20While%20the%20performance%20gap%20in%20MCQ%20accuracy%20between%20two%20model%20families%20is%20relatively%20small%2C%20we%20observe%20a%20substantial%2022.6%25%20performance%20difference%20in%20temporal%20localization%20between%20the%20best%20performing%20closed-source%20and%20open-source%20models.%20Performance%20further%20degrades%20across%20demographic%20groups%2C%20indicating%20persistent%20disparities%20in%20model%20behavior.%20Overall%2C%20SONIC-O1%20provides%20an%20open%20evaluation%20suite%20for%20temporally%20grounded%20and%20socially%20robust%20multimodal%20understanding.%20We%20release%20SONIC-O1%20for%20reproducibility%20and%20research%3A%20Project%20page%3A%20https%3A//vectorinstitute.github.io/sonic-o1/%20Dataset%3A%20https%3A//huggingface.co/datasets/vector-institute/sonic-o1%20Github%3A%20https%3A//github.com/vectorinstitute/sonic-o1%20Leaderboard%3A%20https%3A//huggingface.co/spaces/vector-institute/sonic-o1-leaderboard%0ALink%3A%20http%3A//arxiv.org/abs/2601.21666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSONIC-O1%253A%2520A%2520Real-World%2520Benchmark%2520for%2520Evaluating%2520Multimodal%2520Large%2520Language%2520Models%2520on%2520Audio-Video%2520Understanding%26entry.906535625%3DAhmed%2520Y.%2520Radwan%2520and%2520Christos%2520Emmanouilidis%2520and%2520Hina%2520Tabassum%2520and%2520Deval%2520Pandya%2520and%2520Shaina%2520Raza%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520are%2520a%2520major%2520focus%2520of%2520recent%2520AI%2520research.%2520However%252C%2520most%2520prior%2520work%2520focuses%2520on%2520static%2520image%2520understanding%252C%2520while%2520their%2520ability%2520to%2520process%2520sequential%2520audio-video%2520data%2520remains%2520underexplored.%2520This%2520gap%2520highlights%2520the%2520need%2520for%2520a%2520high-quality%2520benchmark%2520to%2520systematically%2520evaluate%2520MLLM%2520performance%2520in%2520a%2520real-world%2520setting.%2520We%2520introduce%2520SONIC-O1%252C%2520a%2520comprehensive%252C%2520fully%2520human-verified%2520benchmark%2520spanning%252013%2520real-world%2520conversational%2520domains%2520with%25204%252C958%2520annotations%2520and%2520demographic%2520metadata.%2520SONIC-O1%2520evaluates%2520MLLMs%2520on%2520key%2520tasks%252C%2520including%2520open-ended%2520summarization%252C%2520multiple-choice%2520question%2520%2528MCQ%2529%2520answering%252C%2520and%2520temporal%2520localization%2520with%2520supporting%2520rationales%2520%2528reasoning%2529.%2520Experiments%2520on%2520closed-%2520and%2520open-source%2520models%2520reveal%2520limitations.%2520While%2520the%2520performance%2520gap%2520in%2520MCQ%2520accuracy%2520between%2520two%2520model%2520families%2520is%2520relatively%2520small%252C%2520we%2520observe%2520a%2520substantial%252022.6%2525%2520performance%2520difference%2520in%2520temporal%2520localization%2520between%2520the%2520best%2520performing%2520closed-source%2520and%2520open-source%2520models.%2520Performance%2520further%2520degrades%2520across%2520demographic%2520groups%252C%2520indicating%2520persistent%2520disparities%2520in%2520model%2520behavior.%2520Overall%252C%2520SONIC-O1%2520provides%2520an%2520open%2520evaluation%2520suite%2520for%2520temporally%2520grounded%2520and%2520socially%2520robust%2520multimodal%2520understanding.%2520We%2520release%2520SONIC-O1%2520for%2520reproducibility%2520and%2520research%253A%2520Project%2520page%253A%2520https%253A//vectorinstitute.github.io/sonic-o1/%2520Dataset%253A%2520https%253A//huggingface.co/datasets/vector-institute/sonic-o1%2520Github%253A%2520https%253A//github.com/vectorinstitute/sonic-o1%2520Leaderboard%253A%2520https%253A//huggingface.co/spaces/vector-institute/sonic-o1-leaderboard%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SONIC-O1%3A%20A%20Real-World%20Benchmark%20for%20Evaluating%20Multimodal%20Large%20Language%20Models%20on%20Audio-Video%20Understanding&entry.906535625=Ahmed%20Y.%20Radwan%20and%20Christos%20Emmanouilidis%20and%20Hina%20Tabassum%20and%20Deval%20Pandya%20and%20Shaina%20Raza&entry.1292438233=Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20a%20major%20focus%20of%20recent%20AI%20research.%20However%2C%20most%20prior%20work%20focuses%20on%20static%20image%20understanding%2C%20while%20their%20ability%20to%20process%20sequential%20audio-video%20data%20remains%20underexplored.%20This%20gap%20highlights%20the%20need%20for%20a%20high-quality%20benchmark%20to%20systematically%20evaluate%20MLLM%20performance%20in%20a%20real-world%20setting.%20We%20introduce%20SONIC-O1%2C%20a%20comprehensive%2C%20fully%20human-verified%20benchmark%20spanning%2013%20real-world%20conversational%20domains%20with%204%2C958%20annotations%20and%20demographic%20metadata.%20SONIC-O1%20evaluates%20MLLMs%20on%20key%20tasks%2C%20including%20open-ended%20summarization%2C%20multiple-choice%20question%20%28MCQ%29%20answering%2C%20and%20temporal%20localization%20with%20supporting%20rationales%20%28reasoning%29.%20Experiments%20on%20closed-%20and%20open-source%20models%20reveal%20limitations.%20While%20the%20performance%20gap%20in%20MCQ%20accuracy%20between%20two%20model%20families%20is%20relatively%20small%2C%20we%20observe%20a%20substantial%2022.6%25%20performance%20difference%20in%20temporal%20localization%20between%20the%20best%20performing%20closed-source%20and%20open-source%20models.%20Performance%20further%20degrades%20across%20demographic%20groups%2C%20indicating%20persistent%20disparities%20in%20model%20behavior.%20Overall%2C%20SONIC-O1%20provides%20an%20open%20evaluation%20suite%20for%20temporally%20grounded%20and%20socially%20robust%20multimodal%20understanding.%20We%20release%20SONIC-O1%20for%20reproducibility%20and%20research%3A%20Project%20page%3A%20https%3A//vectorinstitute.github.io/sonic-o1/%20Dataset%3A%20https%3A//huggingface.co/datasets/vector-institute/sonic-o1%20Github%3A%20https%3A//github.com/vectorinstitute/sonic-o1%20Leaderboard%3A%20https%3A//huggingface.co/spaces/vector-institute/sonic-o1-leaderboard&entry.1838667208=http%3A//arxiv.org/abs/2601.21666v1&entry.124074799=Read"},
{"title": "KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices", "author": "Wuyang Zhou and Yuxuan Gu and Giorgos Iacovides and Danilo Mandic", "abstract": "The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive $\\mathcal{O}(n^3C)$ parameter complexity with $n$ as the width of the residual stream and $C$ as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, $\\mathcal{O} \\left( nC \\cdot n! \\right)$. To address both challenges, we propose \\textbf{KromHC}, which uses the \\underline{Kro}necker products of smaller doubly stochastic matrices to parametrize the residual matrix in \\underline{mHC}. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to $\\mathcal{O}(n^2C)$. Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at \\texttt{https://github.com/wz1119/KromHC}.", "link": "http://arxiv.org/abs/2601.21579v1", "date": "2026-01-29", "relevancy": 2.2549, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4622}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4483}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KromHC%3A%20Manifold-Constrained%20Hyper-Connections%20with%20Kronecker-Product%20Residual%20Matrices&body=Title%3A%20KromHC%3A%20Manifold-Constrained%20Hyper-Connections%20with%20Kronecker-Product%20Residual%20Matrices%0AAuthor%3A%20Wuyang%20Zhou%20and%20Yuxuan%20Gu%20and%20Giorgos%20Iacovides%20and%20Danilo%20Mandic%0AAbstract%3A%20The%20success%20of%20Hyper-Connections%20%28HC%29%20in%20neural%20networks%20%28NN%29%20has%20also%20highlighted%20issues%20related%20to%20its%20training%20instability%20and%20restricted%20scalability.%20The%20Manifold-Constrained%20Hyper-Connections%20%28mHC%29%20mitigate%20these%20challenges%20by%20projecting%20the%20residual%20connection%20space%20onto%20a%20Birkhoff%20polytope%2C%20however%2C%20it%20faces%20two%20issues%3A%201%29%20its%20iterative%20Sinkhorn-Knopp%20%28SK%29%20algorithm%20does%20not%20always%20yield%20exact%20doubly%20stochastic%20residual%20matrices%3B%202%29%20mHC%20incurs%20a%20prohibitive%20%24%5Cmathcal%7BO%7D%28n%5E3C%29%24%20parameter%20complexity%20with%20%24n%24%20as%20the%20width%20of%20the%20residual%20stream%20and%20%24C%24%20as%20the%20feature%20dimension.%20The%20recently%20proposed%20mHC-lite%20reparametrizes%20the%20residual%20matrix%20via%20the%20Birkhoff-von-Neumann%20theorem%20to%20guarantee%20double%20stochasticity%2C%20but%20also%20faces%20a%20factorial%20explosion%20in%20its%20parameter%20complexity%2C%20%24%5Cmathcal%7BO%7D%20%5Cleft%28%20nC%20%5Ccdot%20n%21%20%5Cright%29%24.%20To%20address%20both%20challenges%2C%20we%20propose%20%5Ctextbf%7BKromHC%7D%2C%20which%20uses%20the%20%5Cunderline%7BKro%7Dnecker%20products%20of%20smaller%20doubly%20stochastic%20matrices%20to%20parametrize%20the%20residual%20matrix%20in%20%5Cunderline%7BmHC%7D.%20By%20enforcing%20manifold%20constraints%20across%20the%20factor%20residual%20matrices%20along%20each%20mode%20of%20the%20tensorized%20residual%20stream%2C%20KromHC%20guarantees%20exact%20double%20stochasticity%20of%20the%20residual%20matrices%20while%20reducing%20parameter%20complexity%20to%20%24%5Cmathcal%7BO%7D%28n%5E2C%29%24.%20Comprehensive%20experiments%20demonstrate%20that%20KromHC%20matches%20or%20even%20outperforms%20state-of-the-art%20%28SOTA%29%20mHC%20variants%2C%20while%20requiring%20significantly%20fewer%20trainable%20parameters.%20The%20code%20is%20available%20at%20%5Ctexttt%7Bhttps%3A//github.com/wz1119/KromHC%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKromHC%253A%2520Manifold-Constrained%2520Hyper-Connections%2520with%2520Kronecker-Product%2520Residual%2520Matrices%26entry.906535625%3DWuyang%2520Zhou%2520and%2520Yuxuan%2520Gu%2520and%2520Giorgos%2520Iacovides%2520and%2520Danilo%2520Mandic%26entry.1292438233%3DThe%2520success%2520of%2520Hyper-Connections%2520%2528HC%2529%2520in%2520neural%2520networks%2520%2528NN%2529%2520has%2520also%2520highlighted%2520issues%2520related%2520to%2520its%2520training%2520instability%2520and%2520restricted%2520scalability.%2520The%2520Manifold-Constrained%2520Hyper-Connections%2520%2528mHC%2529%2520mitigate%2520these%2520challenges%2520by%2520projecting%2520the%2520residual%2520connection%2520space%2520onto%2520a%2520Birkhoff%2520polytope%252C%2520however%252C%2520it%2520faces%2520two%2520issues%253A%25201%2529%2520its%2520iterative%2520Sinkhorn-Knopp%2520%2528SK%2529%2520algorithm%2520does%2520not%2520always%2520yield%2520exact%2520doubly%2520stochastic%2520residual%2520matrices%253B%25202%2529%2520mHC%2520incurs%2520a%2520prohibitive%2520%2524%255Cmathcal%257BO%257D%2528n%255E3C%2529%2524%2520parameter%2520complexity%2520with%2520%2524n%2524%2520as%2520the%2520width%2520of%2520the%2520residual%2520stream%2520and%2520%2524C%2524%2520as%2520the%2520feature%2520dimension.%2520The%2520recently%2520proposed%2520mHC-lite%2520reparametrizes%2520the%2520residual%2520matrix%2520via%2520the%2520Birkhoff-von-Neumann%2520theorem%2520to%2520guarantee%2520double%2520stochasticity%252C%2520but%2520also%2520faces%2520a%2520factorial%2520explosion%2520in%2520its%2520parameter%2520complexity%252C%2520%2524%255Cmathcal%257BO%257D%2520%255Cleft%2528%2520nC%2520%255Ccdot%2520n%2521%2520%255Cright%2529%2524.%2520To%2520address%2520both%2520challenges%252C%2520we%2520propose%2520%255Ctextbf%257BKromHC%257D%252C%2520which%2520uses%2520the%2520%255Cunderline%257BKro%257Dnecker%2520products%2520of%2520smaller%2520doubly%2520stochastic%2520matrices%2520to%2520parametrize%2520the%2520residual%2520matrix%2520in%2520%255Cunderline%257BmHC%257D.%2520By%2520enforcing%2520manifold%2520constraints%2520across%2520the%2520factor%2520residual%2520matrices%2520along%2520each%2520mode%2520of%2520the%2520tensorized%2520residual%2520stream%252C%2520KromHC%2520guarantees%2520exact%2520double%2520stochasticity%2520of%2520the%2520residual%2520matrices%2520while%2520reducing%2520parameter%2520complexity%2520to%2520%2524%255Cmathcal%257BO%257D%2528n%255E2C%2529%2524.%2520Comprehensive%2520experiments%2520demonstrate%2520that%2520KromHC%2520matches%2520or%2520even%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%2520mHC%2520variants%252C%2520while%2520requiring%2520significantly%2520fewer%2520trainable%2520parameters.%2520The%2520code%2520is%2520available%2520at%2520%255Ctexttt%257Bhttps%253A//github.com/wz1119/KromHC%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KromHC%3A%20Manifold-Constrained%20Hyper-Connections%20with%20Kronecker-Product%20Residual%20Matrices&entry.906535625=Wuyang%20Zhou%20and%20Yuxuan%20Gu%20and%20Giorgos%20Iacovides%20and%20Danilo%20Mandic&entry.1292438233=The%20success%20of%20Hyper-Connections%20%28HC%29%20in%20neural%20networks%20%28NN%29%20has%20also%20highlighted%20issues%20related%20to%20its%20training%20instability%20and%20restricted%20scalability.%20The%20Manifold-Constrained%20Hyper-Connections%20%28mHC%29%20mitigate%20these%20challenges%20by%20projecting%20the%20residual%20connection%20space%20onto%20a%20Birkhoff%20polytope%2C%20however%2C%20it%20faces%20two%20issues%3A%201%29%20its%20iterative%20Sinkhorn-Knopp%20%28SK%29%20algorithm%20does%20not%20always%20yield%20exact%20doubly%20stochastic%20residual%20matrices%3B%202%29%20mHC%20incurs%20a%20prohibitive%20%24%5Cmathcal%7BO%7D%28n%5E3C%29%24%20parameter%20complexity%20with%20%24n%24%20as%20the%20width%20of%20the%20residual%20stream%20and%20%24C%24%20as%20the%20feature%20dimension.%20The%20recently%20proposed%20mHC-lite%20reparametrizes%20the%20residual%20matrix%20via%20the%20Birkhoff-von-Neumann%20theorem%20to%20guarantee%20double%20stochasticity%2C%20but%20also%20faces%20a%20factorial%20explosion%20in%20its%20parameter%20complexity%2C%20%24%5Cmathcal%7BO%7D%20%5Cleft%28%20nC%20%5Ccdot%20n%21%20%5Cright%29%24.%20To%20address%20both%20challenges%2C%20we%20propose%20%5Ctextbf%7BKromHC%7D%2C%20which%20uses%20the%20%5Cunderline%7BKro%7Dnecker%20products%20of%20smaller%20doubly%20stochastic%20matrices%20to%20parametrize%20the%20residual%20matrix%20in%20%5Cunderline%7BmHC%7D.%20By%20enforcing%20manifold%20constraints%20across%20the%20factor%20residual%20matrices%20along%20each%20mode%20of%20the%20tensorized%20residual%20stream%2C%20KromHC%20guarantees%20exact%20double%20stochasticity%20of%20the%20residual%20matrices%20while%20reducing%20parameter%20complexity%20to%20%24%5Cmathcal%7BO%7D%28n%5E2C%29%24.%20Comprehensive%20experiments%20demonstrate%20that%20KromHC%20matches%20or%20even%20outperforms%20state-of-the-art%20%28SOTA%29%20mHC%20variants%2C%20while%20requiring%20significantly%20fewer%20trainable%20parameters.%20The%20code%20is%20available%20at%20%5Ctexttt%7Bhttps%3A//github.com/wz1119/KromHC%7D.&entry.1838667208=http%3A//arxiv.org/abs/2601.21579v1&entry.124074799=Read"},
{"title": "Revisiting Reweighted Risk for Calibration: AURC, Focal, and Inverse Focal Loss", "author": "Han Zhou and Sebastian G. Gruber and Teodora Popordanoska and Matthew B. Blaschko", "abstract": "Several variants of reweighted risk functionals, such as focal loss, inverse focal loss, and the Area Under the Risk Coverage Curve (AURC), have been proposed for improving model calibration; yet their theoretical connections to calibration errors remain under-explored. In this paper, we revisit a broad class of weighted risk functions and find a principled connection between calibration error and selective classification. We show that minimizing calibration error is closely linked to the selective classification paradigm and demonstrate that optimizing selective risk in low confidence regions naturally improves calibration. Our proposed loss shares a similar reweighting strategy with dual focal loss but offers greater flexibility through the choice of confidence score functions (CSFs). Furthermore, our approach utilizes a bin-based cumulative distribution function (CDF) approximation, enabling efficient gradient-based optimization with O(nM) complexity for n samples and M bins. Empirical evaluations demonstrate that our method achieves competitive calibration performance across a range of datasets and model architectures.", "link": "http://arxiv.org/abs/2505.23463v5", "date": "2026-01-29", "relevancy": 2.2533, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4694}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.447}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Reweighted%20Risk%20for%20Calibration%3A%20AURC%2C%20Focal%2C%20and%20Inverse%20Focal%20Loss&body=Title%3A%20Revisiting%20Reweighted%20Risk%20for%20Calibration%3A%20AURC%2C%20Focal%2C%20and%20Inverse%20Focal%20Loss%0AAuthor%3A%20Han%20Zhou%20and%20Sebastian%20G.%20Gruber%20and%20Teodora%20Popordanoska%20and%20Matthew%20B.%20Blaschko%0AAbstract%3A%20Several%20variants%20of%20reweighted%20risk%20functionals%2C%20such%20as%20focal%20loss%2C%20inverse%20focal%20loss%2C%20and%20the%20Area%20Under%20the%20Risk%20Coverage%20Curve%20%28AURC%29%2C%20have%20been%20proposed%20for%20improving%20model%20calibration%3B%20yet%20their%20theoretical%20connections%20to%20calibration%20errors%20remain%20under-explored.%20In%20this%20paper%2C%20we%20revisit%20a%20broad%20class%20of%20weighted%20risk%20functions%20and%20find%20a%20principled%20connection%20between%20calibration%20error%20and%20selective%20classification.%20We%20show%20that%20minimizing%20calibration%20error%20is%20closely%20linked%20to%20the%20selective%20classification%20paradigm%20and%20demonstrate%20that%20optimizing%20selective%20risk%20in%20low%20confidence%20regions%20naturally%20improves%20calibration.%20Our%20proposed%20loss%20shares%20a%20similar%20reweighting%20strategy%20with%20dual%20focal%20loss%20but%20offers%20greater%20flexibility%20through%20the%20choice%20of%20confidence%20score%20functions%20%28CSFs%29.%20Furthermore%2C%20our%20approach%20utilizes%20a%20bin-based%20cumulative%20distribution%20function%20%28CDF%29%20approximation%2C%20enabling%20efficient%20gradient-based%20optimization%20with%20O%28nM%29%20complexity%20for%20n%20samples%20and%20M%20bins.%20Empirical%20evaluations%20demonstrate%20that%20our%20method%20achieves%20competitive%20calibration%20performance%20across%20a%20range%20of%20datasets%20and%20model%20architectures.%0ALink%3A%20http%3A//arxiv.org/abs/2505.23463v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Reweighted%2520Risk%2520for%2520Calibration%253A%2520AURC%252C%2520Focal%252C%2520and%2520Inverse%2520Focal%2520Loss%26entry.906535625%3DHan%2520Zhou%2520and%2520Sebastian%2520G.%2520Gruber%2520and%2520Teodora%2520Popordanoska%2520and%2520Matthew%2520B.%2520Blaschko%26entry.1292438233%3DSeveral%2520variants%2520of%2520reweighted%2520risk%2520functionals%252C%2520such%2520as%2520focal%2520loss%252C%2520inverse%2520focal%2520loss%252C%2520and%2520the%2520Area%2520Under%2520the%2520Risk%2520Coverage%2520Curve%2520%2528AURC%2529%252C%2520have%2520been%2520proposed%2520for%2520improving%2520model%2520calibration%253B%2520yet%2520their%2520theoretical%2520connections%2520to%2520calibration%2520errors%2520remain%2520under-explored.%2520In%2520this%2520paper%252C%2520we%2520revisit%2520a%2520broad%2520class%2520of%2520weighted%2520risk%2520functions%2520and%2520find%2520a%2520principled%2520connection%2520between%2520calibration%2520error%2520and%2520selective%2520classification.%2520We%2520show%2520that%2520minimizing%2520calibration%2520error%2520is%2520closely%2520linked%2520to%2520the%2520selective%2520classification%2520paradigm%2520and%2520demonstrate%2520that%2520optimizing%2520selective%2520risk%2520in%2520low%2520confidence%2520regions%2520naturally%2520improves%2520calibration.%2520Our%2520proposed%2520loss%2520shares%2520a%2520similar%2520reweighting%2520strategy%2520with%2520dual%2520focal%2520loss%2520but%2520offers%2520greater%2520flexibility%2520through%2520the%2520choice%2520of%2520confidence%2520score%2520functions%2520%2528CSFs%2529.%2520Furthermore%252C%2520our%2520approach%2520utilizes%2520a%2520bin-based%2520cumulative%2520distribution%2520function%2520%2528CDF%2529%2520approximation%252C%2520enabling%2520efficient%2520gradient-based%2520optimization%2520with%2520O%2528nM%2529%2520complexity%2520for%2520n%2520samples%2520and%2520M%2520bins.%2520Empirical%2520evaluations%2520demonstrate%2520that%2520our%2520method%2520achieves%2520competitive%2520calibration%2520performance%2520across%2520a%2520range%2520of%2520datasets%2520and%2520model%2520architectures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23463v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Reweighted%20Risk%20for%20Calibration%3A%20AURC%2C%20Focal%2C%20and%20Inverse%20Focal%20Loss&entry.906535625=Han%20Zhou%20and%20Sebastian%20G.%20Gruber%20and%20Teodora%20Popordanoska%20and%20Matthew%20B.%20Blaschko&entry.1292438233=Several%20variants%20of%20reweighted%20risk%20functionals%2C%20such%20as%20focal%20loss%2C%20inverse%20focal%20loss%2C%20and%20the%20Area%20Under%20the%20Risk%20Coverage%20Curve%20%28AURC%29%2C%20have%20been%20proposed%20for%20improving%20model%20calibration%3B%20yet%20their%20theoretical%20connections%20to%20calibration%20errors%20remain%20under-explored.%20In%20this%20paper%2C%20we%20revisit%20a%20broad%20class%20of%20weighted%20risk%20functions%20and%20find%20a%20principled%20connection%20between%20calibration%20error%20and%20selective%20classification.%20We%20show%20that%20minimizing%20calibration%20error%20is%20closely%20linked%20to%20the%20selective%20classification%20paradigm%20and%20demonstrate%20that%20optimizing%20selective%20risk%20in%20low%20confidence%20regions%20naturally%20improves%20calibration.%20Our%20proposed%20loss%20shares%20a%20similar%20reweighting%20strategy%20with%20dual%20focal%20loss%20but%20offers%20greater%20flexibility%20through%20the%20choice%20of%20confidence%20score%20functions%20%28CSFs%29.%20Furthermore%2C%20our%20approach%20utilizes%20a%20bin-based%20cumulative%20distribution%20function%20%28CDF%29%20approximation%2C%20enabling%20efficient%20gradient-based%20optimization%20with%20O%28nM%29%20complexity%20for%20n%20samples%20and%20M%20bins.%20Empirical%20evaluations%20demonstrate%20that%20our%20method%20achieves%20competitive%20calibration%20performance%20across%20a%20range%20of%20datasets%20and%20model%20architectures.&entry.1838667208=http%3A//arxiv.org/abs/2505.23463v5&entry.124074799=Read"},
{"title": "Heterogeneity-Aware Knowledge Sharing for Graph Federated Learning", "author": "Wentao Yu and Sheng Wan and Shuo Chen and Bo Han and Chen Gong", "abstract": "Graph Federated Learning (GFL) enables distributed graph representation learning while protecting the privacy of graph data. However, GFL suffers from heterogeneity arising from diverse node features and structural topologies across multiple clients. To address both types of heterogeneity, we propose a novel graph Federated learning method via Semantic and Structural Alignment (FedSSA), which shares the knowledge of both node features and structural topologies. For node feature heterogeneity, we propose a novel variational model to infer class-wise node distributions, so that we can cluster clients based on inferred distributions and construct cluster-level representative distributions. We then minimize the divergence between local and cluster-level distributions to facilitate semantic knowledge sharing. For structural heterogeneity, we employ spectral Graph Neural Networks (GNNs) and propose a spectral energy measure to characterize structural information, so that we can cluster clients based on spectral energy and build cluster-level spectral GNNs. We then align the spectral characteristics of local spectral GNNs with those of cluster-level spectral GNNs to enable structural knowledge sharing. Experiments on six homophilic and five heterophilic graph datasets under both non-overlapping and overlapping partitioning settings demonstrate that FedSSA consistently outperforms eleven state-of-the-art methods.", "link": "http://arxiv.org/abs/2601.21589v1", "date": "2026-01-29", "relevancy": 2.2523, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4559}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4494}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneity-Aware%20Knowledge%20Sharing%20for%20Graph%20Federated%20Learning&body=Title%3A%20Heterogeneity-Aware%20Knowledge%20Sharing%20for%20Graph%20Federated%20Learning%0AAuthor%3A%20Wentao%20Yu%20and%20Sheng%20Wan%20and%20Shuo%20Chen%20and%20Bo%20Han%20and%20Chen%20Gong%0AAbstract%3A%20Graph%20Federated%20Learning%20%28GFL%29%20enables%20distributed%20graph%20representation%20learning%20while%20protecting%20the%20privacy%20of%20graph%20data.%20However%2C%20GFL%20suffers%20from%20heterogeneity%20arising%20from%20diverse%20node%20features%20and%20structural%20topologies%20across%20multiple%20clients.%20To%20address%20both%20types%20of%20heterogeneity%2C%20we%20propose%20a%20novel%20graph%20Federated%20learning%20method%20via%20Semantic%20and%20Structural%20Alignment%20%28FedSSA%29%2C%20which%20shares%20the%20knowledge%20of%20both%20node%20features%20and%20structural%20topologies.%20For%20node%20feature%20heterogeneity%2C%20we%20propose%20a%20novel%20variational%20model%20to%20infer%20class-wise%20node%20distributions%2C%20so%20that%20we%20can%20cluster%20clients%20based%20on%20inferred%20distributions%20and%20construct%20cluster-level%20representative%20distributions.%20We%20then%20minimize%20the%20divergence%20between%20local%20and%20cluster-level%20distributions%20to%20facilitate%20semantic%20knowledge%20sharing.%20For%20structural%20heterogeneity%2C%20we%20employ%20spectral%20Graph%20Neural%20Networks%20%28GNNs%29%20and%20propose%20a%20spectral%20energy%20measure%20to%20characterize%20structural%20information%2C%20so%20that%20we%20can%20cluster%20clients%20based%20on%20spectral%20energy%20and%20build%20cluster-level%20spectral%20GNNs.%20We%20then%20align%20the%20spectral%20characteristics%20of%20local%20spectral%20GNNs%20with%20those%20of%20cluster-level%20spectral%20GNNs%20to%20enable%20structural%20knowledge%20sharing.%20Experiments%20on%20six%20homophilic%20and%20five%20heterophilic%20graph%20datasets%20under%20both%20non-overlapping%20and%20overlapping%20partitioning%20settings%20demonstrate%20that%20FedSSA%20consistently%20outperforms%20eleven%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneity-Aware%2520Knowledge%2520Sharing%2520for%2520Graph%2520Federated%2520Learning%26entry.906535625%3DWentao%2520Yu%2520and%2520Sheng%2520Wan%2520and%2520Shuo%2520Chen%2520and%2520Bo%2520Han%2520and%2520Chen%2520Gong%26entry.1292438233%3DGraph%2520Federated%2520Learning%2520%2528GFL%2529%2520enables%2520distributed%2520graph%2520representation%2520learning%2520while%2520protecting%2520the%2520privacy%2520of%2520graph%2520data.%2520However%252C%2520GFL%2520suffers%2520from%2520heterogeneity%2520arising%2520from%2520diverse%2520node%2520features%2520and%2520structural%2520topologies%2520across%2520multiple%2520clients.%2520To%2520address%2520both%2520types%2520of%2520heterogeneity%252C%2520we%2520propose%2520a%2520novel%2520graph%2520Federated%2520learning%2520method%2520via%2520Semantic%2520and%2520Structural%2520Alignment%2520%2528FedSSA%2529%252C%2520which%2520shares%2520the%2520knowledge%2520of%2520both%2520node%2520features%2520and%2520structural%2520topologies.%2520For%2520node%2520feature%2520heterogeneity%252C%2520we%2520propose%2520a%2520novel%2520variational%2520model%2520to%2520infer%2520class-wise%2520node%2520distributions%252C%2520so%2520that%2520we%2520can%2520cluster%2520clients%2520based%2520on%2520inferred%2520distributions%2520and%2520construct%2520cluster-level%2520representative%2520distributions.%2520We%2520then%2520minimize%2520the%2520divergence%2520between%2520local%2520and%2520cluster-level%2520distributions%2520to%2520facilitate%2520semantic%2520knowledge%2520sharing.%2520For%2520structural%2520heterogeneity%252C%2520we%2520employ%2520spectral%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520and%2520propose%2520a%2520spectral%2520energy%2520measure%2520to%2520characterize%2520structural%2520information%252C%2520so%2520that%2520we%2520can%2520cluster%2520clients%2520based%2520on%2520spectral%2520energy%2520and%2520build%2520cluster-level%2520spectral%2520GNNs.%2520We%2520then%2520align%2520the%2520spectral%2520characteristics%2520of%2520local%2520spectral%2520GNNs%2520with%2520those%2520of%2520cluster-level%2520spectral%2520GNNs%2520to%2520enable%2520structural%2520knowledge%2520sharing.%2520Experiments%2520on%2520six%2520homophilic%2520and%2520five%2520heterophilic%2520graph%2520datasets%2520under%2520both%2520non-overlapping%2520and%2520overlapping%2520partitioning%2520settings%2520demonstrate%2520that%2520FedSSA%2520consistently%2520outperforms%2520eleven%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneity-Aware%20Knowledge%20Sharing%20for%20Graph%20Federated%20Learning&entry.906535625=Wentao%20Yu%20and%20Sheng%20Wan%20and%20Shuo%20Chen%20and%20Bo%20Han%20and%20Chen%20Gong&entry.1292438233=Graph%20Federated%20Learning%20%28GFL%29%20enables%20distributed%20graph%20representation%20learning%20while%20protecting%20the%20privacy%20of%20graph%20data.%20However%2C%20GFL%20suffers%20from%20heterogeneity%20arising%20from%20diverse%20node%20features%20and%20structural%20topologies%20across%20multiple%20clients.%20To%20address%20both%20types%20of%20heterogeneity%2C%20we%20propose%20a%20novel%20graph%20Federated%20learning%20method%20via%20Semantic%20and%20Structural%20Alignment%20%28FedSSA%29%2C%20which%20shares%20the%20knowledge%20of%20both%20node%20features%20and%20structural%20topologies.%20For%20node%20feature%20heterogeneity%2C%20we%20propose%20a%20novel%20variational%20model%20to%20infer%20class-wise%20node%20distributions%2C%20so%20that%20we%20can%20cluster%20clients%20based%20on%20inferred%20distributions%20and%20construct%20cluster-level%20representative%20distributions.%20We%20then%20minimize%20the%20divergence%20between%20local%20and%20cluster-level%20distributions%20to%20facilitate%20semantic%20knowledge%20sharing.%20For%20structural%20heterogeneity%2C%20we%20employ%20spectral%20Graph%20Neural%20Networks%20%28GNNs%29%20and%20propose%20a%20spectral%20energy%20measure%20to%20characterize%20structural%20information%2C%20so%20that%20we%20can%20cluster%20clients%20based%20on%20spectral%20energy%20and%20build%20cluster-level%20spectral%20GNNs.%20We%20then%20align%20the%20spectral%20characteristics%20of%20local%20spectral%20GNNs%20with%20those%20of%20cluster-level%20spectral%20GNNs%20to%20enable%20structural%20knowledge%20sharing.%20Experiments%20on%20six%20homophilic%20and%20five%20heterophilic%20graph%20datasets%20under%20both%20non-overlapping%20and%20overlapping%20partitioning%20settings%20demonstrate%20that%20FedSSA%20consistently%20outperforms%20eleven%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.21589v1&entry.124074799=Read"},
{"title": "PI-Light: Physics-Inspired Diffusion for Full-Image Relighting", "author": "Zhexin Liang and Zhaoxi Chen and Yongwei Chen and Tianyi Wei and Tengfei Wang and Xingang Pan", "abstract": "Full-image relighting remains a challenging problem due to the difficulty of collecting large-scale structured paired data, the difficulty of maintaining physical plausibility, and the limited generalizability imposed by data-driven priors. Existing attempts to bridge the synthetic-to-real gap for full-scene relighting remain suboptimal. To tackle these challenges, we introduce Physics-Inspired diffusion for full-image reLight ($\u03c0$-Light, or PI-Light), a two-stage framework that leverages physics-inspired diffusion models. Our design incorporates (i) batch-aware attention, which improves the consistency of intrinsic predictions across a collection of images, (ii) a physics-guided neural rendering module that enforces physically plausible light transport, (iii) physics-inspired losses that regularize training dynamics toward a physically meaningful landscape, thereby enhancing generalizability to real-world image editing, and (iv) a carefully curated dataset of diverse objects and scenes captured under controlled lighting conditions. Together, these components enable efficient finetuning of pretrained diffusion models while also providing a solid benchmark for downstream evaluation. Experiments demonstrate that $\u03c0$-Light synthesizes specular highlights and diffuse reflections across a wide variety of materials, achieving superior generalization to real-world scenes compared with prior approaches.", "link": "http://arxiv.org/abs/2601.22135v1", "date": "2026-01-29", "relevancy": 2.2488, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5704}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5656}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PI-Light%3A%20Physics-Inspired%20Diffusion%20for%20Full-Image%20Relighting&body=Title%3A%20PI-Light%3A%20Physics-Inspired%20Diffusion%20for%20Full-Image%20Relighting%0AAuthor%3A%20Zhexin%20Liang%20and%20Zhaoxi%20Chen%20and%20Yongwei%20Chen%20and%20Tianyi%20Wei%20and%20Tengfei%20Wang%20and%20Xingang%20Pan%0AAbstract%3A%20Full-image%20relighting%20remains%20a%20challenging%20problem%20due%20to%20the%20difficulty%20of%20collecting%20large-scale%20structured%20paired%20data%2C%20the%20difficulty%20of%20maintaining%20physical%20plausibility%2C%20and%20the%20limited%20generalizability%20imposed%20by%20data-driven%20priors.%20Existing%20attempts%20to%20bridge%20the%20synthetic-to-real%20gap%20for%20full-scene%20relighting%20remain%20suboptimal.%20To%20tackle%20these%20challenges%2C%20we%20introduce%20Physics-Inspired%20diffusion%20for%20full-image%20reLight%20%28%24%CF%80%24-Light%2C%20or%20PI-Light%29%2C%20a%20two-stage%20framework%20that%20leverages%20physics-inspired%20diffusion%20models.%20Our%20design%20incorporates%20%28i%29%20batch-aware%20attention%2C%20which%20improves%20the%20consistency%20of%20intrinsic%20predictions%20across%20a%20collection%20of%20images%2C%20%28ii%29%20a%20physics-guided%20neural%20rendering%20module%20that%20enforces%20physically%20plausible%20light%20transport%2C%20%28iii%29%20physics-inspired%20losses%20that%20regularize%20training%20dynamics%20toward%20a%20physically%20meaningful%20landscape%2C%20thereby%20enhancing%20generalizability%20to%20real-world%20image%20editing%2C%20and%20%28iv%29%20a%20carefully%20curated%20dataset%20of%20diverse%20objects%20and%20scenes%20captured%20under%20controlled%20lighting%20conditions.%20Together%2C%20these%20components%20enable%20efficient%20finetuning%20of%20pretrained%20diffusion%20models%20while%20also%20providing%20a%20solid%20benchmark%20for%20downstream%20evaluation.%20Experiments%20demonstrate%20that%20%24%CF%80%24-Light%20synthesizes%20specular%20highlights%20and%20diffuse%20reflections%20across%20a%20wide%20variety%20of%20materials%2C%20achieving%20superior%20generalization%20to%20real-world%20scenes%20compared%20with%20prior%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPI-Light%253A%2520Physics-Inspired%2520Diffusion%2520for%2520Full-Image%2520Relighting%26entry.906535625%3DZhexin%2520Liang%2520and%2520Zhaoxi%2520Chen%2520and%2520Yongwei%2520Chen%2520and%2520Tianyi%2520Wei%2520and%2520Tengfei%2520Wang%2520and%2520Xingang%2520Pan%26entry.1292438233%3DFull-image%2520relighting%2520remains%2520a%2520challenging%2520problem%2520due%2520to%2520the%2520difficulty%2520of%2520collecting%2520large-scale%2520structured%2520paired%2520data%252C%2520the%2520difficulty%2520of%2520maintaining%2520physical%2520plausibility%252C%2520and%2520the%2520limited%2520generalizability%2520imposed%2520by%2520data-driven%2520priors.%2520Existing%2520attempts%2520to%2520bridge%2520the%2520synthetic-to-real%2520gap%2520for%2520full-scene%2520relighting%2520remain%2520suboptimal.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520introduce%2520Physics-Inspired%2520diffusion%2520for%2520full-image%2520reLight%2520%2528%2524%25CF%2580%2524-Light%252C%2520or%2520PI-Light%2529%252C%2520a%2520two-stage%2520framework%2520that%2520leverages%2520physics-inspired%2520diffusion%2520models.%2520Our%2520design%2520incorporates%2520%2528i%2529%2520batch-aware%2520attention%252C%2520which%2520improves%2520the%2520consistency%2520of%2520intrinsic%2520predictions%2520across%2520a%2520collection%2520of%2520images%252C%2520%2528ii%2529%2520a%2520physics-guided%2520neural%2520rendering%2520module%2520that%2520enforces%2520physically%2520plausible%2520light%2520transport%252C%2520%2528iii%2529%2520physics-inspired%2520losses%2520that%2520regularize%2520training%2520dynamics%2520toward%2520a%2520physically%2520meaningful%2520landscape%252C%2520thereby%2520enhancing%2520generalizability%2520to%2520real-world%2520image%2520editing%252C%2520and%2520%2528iv%2529%2520a%2520carefully%2520curated%2520dataset%2520of%2520diverse%2520objects%2520and%2520scenes%2520captured%2520under%2520controlled%2520lighting%2520conditions.%2520Together%252C%2520these%2520components%2520enable%2520efficient%2520finetuning%2520of%2520pretrained%2520diffusion%2520models%2520while%2520also%2520providing%2520a%2520solid%2520benchmark%2520for%2520downstream%2520evaluation.%2520Experiments%2520demonstrate%2520that%2520%2524%25CF%2580%2524-Light%2520synthesizes%2520specular%2520highlights%2520and%2520diffuse%2520reflections%2520across%2520a%2520wide%2520variety%2520of%2520materials%252C%2520achieving%2520superior%2520generalization%2520to%2520real-world%2520scenes%2520compared%2520with%2520prior%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PI-Light%3A%20Physics-Inspired%20Diffusion%20for%20Full-Image%20Relighting&entry.906535625=Zhexin%20Liang%20and%20Zhaoxi%20Chen%20and%20Yongwei%20Chen%20and%20Tianyi%20Wei%20and%20Tengfei%20Wang%20and%20Xingang%20Pan&entry.1292438233=Full-image%20relighting%20remains%20a%20challenging%20problem%20due%20to%20the%20difficulty%20of%20collecting%20large-scale%20structured%20paired%20data%2C%20the%20difficulty%20of%20maintaining%20physical%20plausibility%2C%20and%20the%20limited%20generalizability%20imposed%20by%20data-driven%20priors.%20Existing%20attempts%20to%20bridge%20the%20synthetic-to-real%20gap%20for%20full-scene%20relighting%20remain%20suboptimal.%20To%20tackle%20these%20challenges%2C%20we%20introduce%20Physics-Inspired%20diffusion%20for%20full-image%20reLight%20%28%24%CF%80%24-Light%2C%20or%20PI-Light%29%2C%20a%20two-stage%20framework%20that%20leverages%20physics-inspired%20diffusion%20models.%20Our%20design%20incorporates%20%28i%29%20batch-aware%20attention%2C%20which%20improves%20the%20consistency%20of%20intrinsic%20predictions%20across%20a%20collection%20of%20images%2C%20%28ii%29%20a%20physics-guided%20neural%20rendering%20module%20that%20enforces%20physically%20plausible%20light%20transport%2C%20%28iii%29%20physics-inspired%20losses%20that%20regularize%20training%20dynamics%20toward%20a%20physically%20meaningful%20landscape%2C%20thereby%20enhancing%20generalizability%20to%20real-world%20image%20editing%2C%20and%20%28iv%29%20a%20carefully%20curated%20dataset%20of%20diverse%20objects%20and%20scenes%20captured%20under%20controlled%20lighting%20conditions.%20Together%2C%20these%20components%20enable%20efficient%20finetuning%20of%20pretrained%20diffusion%20models%20while%20also%20providing%20a%20solid%20benchmark%20for%20downstream%20evaluation.%20Experiments%20demonstrate%20that%20%24%CF%80%24-Light%20synthesizes%20specular%20highlights%20and%20diffuse%20reflections%20across%20a%20wide%20variety%20of%20materials%2C%20achieving%20superior%20generalization%20to%20real-world%20scenes%20compared%20with%20prior%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2601.22135v1&entry.124074799=Read"},
{"title": "UEval: A Benchmark for Unified Multimodal Generation", "author": "Bo Li and Yida Yin and Wenhao Chai and Xingyu Fu and Zhuang Liu", "abstract": "We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.", "link": "http://arxiv.org/abs/2601.22155v1", "date": "2026-01-29", "relevancy": 2.1796, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5525}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UEval%3A%20A%20Benchmark%20for%20Unified%20Multimodal%20Generation&body=Title%3A%20UEval%3A%20A%20Benchmark%20for%20Unified%20Multimodal%20Generation%0AAuthor%3A%20Bo%20Li%20and%20Yida%20Yin%20and%20Wenhao%20Chai%20and%20Xingyu%20Fu%20and%20Zhuang%20Liu%0AAbstract%3A%20We%20introduce%20UEval%2C%20a%20benchmark%20to%20evaluate%20unified%20models%2C%20i.e.%2C%20models%20capable%20of%20generating%20both%20images%20and%20text.%20UEval%20comprises%201%2C000%20expert-curated%20questions%20that%20require%20both%20images%20and%20text%20in%20the%20model%20output%2C%20sourced%20from%208%20real-world%20tasks.%20Our%20curated%20questions%20cover%20a%20wide%20range%20of%20reasoning%20types%2C%20from%20step-by-step%20guides%20to%20textbook%20explanations.%20Evaluating%20open-ended%20multimodal%20generation%20is%20non-trivial%2C%20as%20simple%20LLM-as-a-judge%20methods%20can%20miss%20the%20subtleties.%20Different%20from%20previous%20works%20that%20rely%20on%20multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%20rate%20image%20quality%20or%20text%20accuracy%2C%20we%20design%20a%20rubric-based%20scoring%20system%20in%20UEval.%20For%20each%20question%2C%20reference%20images%20and%20text%20answers%20are%20provided%20to%20a%20MLLM%20to%20generate%20an%20initial%20rubric%2C%20consisting%20of%20multiple%20evaluation%20criteria%2C%20and%20human%20experts%20then%20refine%20and%20validate%20these%20rubrics.%20In%20total%2C%20UEval%20contains%2010%2C417%20validated%20rubric%20criteria%2C%20enabling%20scalable%20and%20fine-grained%20automatic%20scoring.%20UEval%20is%20challenging%20for%20current%20unified%20models%3A%20GPT-5-Thinking%20scores%20only%2066.4%20out%20of%20100%2C%20while%20the%20best%20open-source%20model%20reaches%20merely%2049.1.%20We%20observe%20that%20reasoning%20models%20often%20outperform%20non-reasoning%20ones%2C%20and%20transferring%20reasoning%20traces%20from%20a%20reasoning%20model%20to%20a%20non-reasoning%20model%20significantly%20narrows%20the%20gap.%20This%20suggests%20that%20reasoning%20may%20be%20important%20for%20tasks%20requiring%20complex%20multimodal%20understanding%20and%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUEval%253A%2520A%2520Benchmark%2520for%2520Unified%2520Multimodal%2520Generation%26entry.906535625%3DBo%2520Li%2520and%2520Yida%2520Yin%2520and%2520Wenhao%2520Chai%2520and%2520Xingyu%2520Fu%2520and%2520Zhuang%2520Liu%26entry.1292438233%3DWe%2520introduce%2520UEval%252C%2520a%2520benchmark%2520to%2520evaluate%2520unified%2520models%252C%2520i.e.%252C%2520models%2520capable%2520of%2520generating%2520both%2520images%2520and%2520text.%2520UEval%2520comprises%25201%252C000%2520expert-curated%2520questions%2520that%2520require%2520both%2520images%2520and%2520text%2520in%2520the%2520model%2520output%252C%2520sourced%2520from%25208%2520real-world%2520tasks.%2520Our%2520curated%2520questions%2520cover%2520a%2520wide%2520range%2520of%2520reasoning%2520types%252C%2520from%2520step-by-step%2520guides%2520to%2520textbook%2520explanations.%2520Evaluating%2520open-ended%2520multimodal%2520generation%2520is%2520non-trivial%252C%2520as%2520simple%2520LLM-as-a-judge%2520methods%2520can%2520miss%2520the%2520subtleties.%2520Different%2520from%2520previous%2520works%2520that%2520rely%2520on%2520multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520to%2520rate%2520image%2520quality%2520or%2520text%2520accuracy%252C%2520we%2520design%2520a%2520rubric-based%2520scoring%2520system%2520in%2520UEval.%2520For%2520each%2520question%252C%2520reference%2520images%2520and%2520text%2520answers%2520are%2520provided%2520to%2520a%2520MLLM%2520to%2520generate%2520an%2520initial%2520rubric%252C%2520consisting%2520of%2520multiple%2520evaluation%2520criteria%252C%2520and%2520human%2520experts%2520then%2520refine%2520and%2520validate%2520these%2520rubrics.%2520In%2520total%252C%2520UEval%2520contains%252010%252C417%2520validated%2520rubric%2520criteria%252C%2520enabling%2520scalable%2520and%2520fine-grained%2520automatic%2520scoring.%2520UEval%2520is%2520challenging%2520for%2520current%2520unified%2520models%253A%2520GPT-5-Thinking%2520scores%2520only%252066.4%2520out%2520of%2520100%252C%2520while%2520the%2520best%2520open-source%2520model%2520reaches%2520merely%252049.1.%2520We%2520observe%2520that%2520reasoning%2520models%2520often%2520outperform%2520non-reasoning%2520ones%252C%2520and%2520transferring%2520reasoning%2520traces%2520from%2520a%2520reasoning%2520model%2520to%2520a%2520non-reasoning%2520model%2520significantly%2520narrows%2520the%2520gap.%2520This%2520suggests%2520that%2520reasoning%2520may%2520be%2520important%2520for%2520tasks%2520requiring%2520complex%2520multimodal%2520understanding%2520and%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UEval%3A%20A%20Benchmark%20for%20Unified%20Multimodal%20Generation&entry.906535625=Bo%20Li%20and%20Yida%20Yin%20and%20Wenhao%20Chai%20and%20Xingyu%20Fu%20and%20Zhuang%20Liu&entry.1292438233=We%20introduce%20UEval%2C%20a%20benchmark%20to%20evaluate%20unified%20models%2C%20i.e.%2C%20models%20capable%20of%20generating%20both%20images%20and%20text.%20UEval%20comprises%201%2C000%20expert-curated%20questions%20that%20require%20both%20images%20and%20text%20in%20the%20model%20output%2C%20sourced%20from%208%20real-world%20tasks.%20Our%20curated%20questions%20cover%20a%20wide%20range%20of%20reasoning%20types%2C%20from%20step-by-step%20guides%20to%20textbook%20explanations.%20Evaluating%20open-ended%20multimodal%20generation%20is%20non-trivial%2C%20as%20simple%20LLM-as-a-judge%20methods%20can%20miss%20the%20subtleties.%20Different%20from%20previous%20works%20that%20rely%20on%20multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%20rate%20image%20quality%20or%20text%20accuracy%2C%20we%20design%20a%20rubric-based%20scoring%20system%20in%20UEval.%20For%20each%20question%2C%20reference%20images%20and%20text%20answers%20are%20provided%20to%20a%20MLLM%20to%20generate%20an%20initial%20rubric%2C%20consisting%20of%20multiple%20evaluation%20criteria%2C%20and%20human%20experts%20then%20refine%20and%20validate%20these%20rubrics.%20In%20total%2C%20UEval%20contains%2010%2C417%20validated%20rubric%20criteria%2C%20enabling%20scalable%20and%20fine-grained%20automatic%20scoring.%20UEval%20is%20challenging%20for%20current%20unified%20models%3A%20GPT-5-Thinking%20scores%20only%2066.4%20out%20of%20100%2C%20while%20the%20best%20open-source%20model%20reaches%20merely%2049.1.%20We%20observe%20that%20reasoning%20models%20often%20outperform%20non-reasoning%20ones%2C%20and%20transferring%20reasoning%20traces%20from%20a%20reasoning%20model%20to%20a%20non-reasoning%20model%20significantly%20narrows%20the%20gap.%20This%20suggests%20that%20reasoning%20may%20be%20important%20for%20tasks%20requiring%20complex%20multimodal%20understanding%20and%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2601.22155v1&entry.124074799=Read"},
{"title": "GeoNorm: Unify Pre-Norm and Post-Norm with Geodesic Optimization", "author": "Chuanyang Zheng and Jiankai Sun and Yihang Gao and Chi Wang and Yuehao Wang and Jing Xiong and Liliang Ren and Bo Peng and Qingmei Wang and Xiaoran Shang and Mac Schwager and Anderson Schneider and Yuriy Nevmyvaka and Xiaodong Liu", "abstract": "The placement of normalization layers, specifically Pre-Norm and Post-Norm, remains an open question in Transformer architecture design. In this work, we rethink these approaches through the lens of manifold optimization, interpreting the outputs of the Feed-Forward Network (FFN) and attention layers as update directions in optimization. Building on this perspective, we introduce GeoNorm, a novel method that replaces standard normalization with geodesic updates on the manifold. Furthermore, analogous to learning rate schedules, we propose a layer-wise update decay for the FFN and attention components. Comprehensive experiments demonstrate that GeoNorm consistently outperforms existing normalization methods in Transformer models. Crucially, GeoNorm can be seamlessly integrated into standard Transformer architectures, achieving performance improvements with negligible additional computational cost.", "link": "http://arxiv.org/abs/2601.22095v1", "date": "2026-01-29", "relevancy": 1.8681, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5254}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4597}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoNorm%3A%20Unify%20Pre-Norm%20and%20Post-Norm%20with%20Geodesic%20Optimization&body=Title%3A%20GeoNorm%3A%20Unify%20Pre-Norm%20and%20Post-Norm%20with%20Geodesic%20Optimization%0AAuthor%3A%20Chuanyang%20Zheng%20and%20Jiankai%20Sun%20and%20Yihang%20Gao%20and%20Chi%20Wang%20and%20Yuehao%20Wang%20and%20Jing%20Xiong%20and%20Liliang%20Ren%20and%20Bo%20Peng%20and%20Qingmei%20Wang%20and%20Xiaoran%20Shang%20and%20Mac%20Schwager%20and%20Anderson%20Schneider%20and%20Yuriy%20Nevmyvaka%20and%20Xiaodong%20Liu%0AAbstract%3A%20The%20placement%20of%20normalization%20layers%2C%20specifically%20Pre-Norm%20and%20Post-Norm%2C%20remains%20an%20open%20question%20in%20Transformer%20architecture%20design.%20In%20this%20work%2C%20we%20rethink%20these%20approaches%20through%20the%20lens%20of%20manifold%20optimization%2C%20interpreting%20the%20outputs%20of%20the%20Feed-Forward%20Network%20%28FFN%29%20and%20attention%20layers%20as%20update%20directions%20in%20optimization.%20Building%20on%20this%20perspective%2C%20we%20introduce%20GeoNorm%2C%20a%20novel%20method%20that%20replaces%20standard%20normalization%20with%20geodesic%20updates%20on%20the%20manifold.%20Furthermore%2C%20analogous%20to%20learning%20rate%20schedules%2C%20we%20propose%20a%20layer-wise%20update%20decay%20for%20the%20FFN%20and%20attention%20components.%20Comprehensive%20experiments%20demonstrate%20that%20GeoNorm%20consistently%20outperforms%20existing%20normalization%20methods%20in%20Transformer%20models.%20Crucially%2C%20GeoNorm%20can%20be%20seamlessly%20integrated%20into%20standard%20Transformer%20architectures%2C%20achieving%20performance%20improvements%20with%20negligible%20additional%20computational%20cost.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoNorm%253A%2520Unify%2520Pre-Norm%2520and%2520Post-Norm%2520with%2520Geodesic%2520Optimization%26entry.906535625%3DChuanyang%2520Zheng%2520and%2520Jiankai%2520Sun%2520and%2520Yihang%2520Gao%2520and%2520Chi%2520Wang%2520and%2520Yuehao%2520Wang%2520and%2520Jing%2520Xiong%2520and%2520Liliang%2520Ren%2520and%2520Bo%2520Peng%2520and%2520Qingmei%2520Wang%2520and%2520Xiaoran%2520Shang%2520and%2520Mac%2520Schwager%2520and%2520Anderson%2520Schneider%2520and%2520Yuriy%2520Nevmyvaka%2520and%2520Xiaodong%2520Liu%26entry.1292438233%3DThe%2520placement%2520of%2520normalization%2520layers%252C%2520specifically%2520Pre-Norm%2520and%2520Post-Norm%252C%2520remains%2520an%2520open%2520question%2520in%2520Transformer%2520architecture%2520design.%2520In%2520this%2520work%252C%2520we%2520rethink%2520these%2520approaches%2520through%2520the%2520lens%2520of%2520manifold%2520optimization%252C%2520interpreting%2520the%2520outputs%2520of%2520the%2520Feed-Forward%2520Network%2520%2528FFN%2529%2520and%2520attention%2520layers%2520as%2520update%2520directions%2520in%2520optimization.%2520Building%2520on%2520this%2520perspective%252C%2520we%2520introduce%2520GeoNorm%252C%2520a%2520novel%2520method%2520that%2520replaces%2520standard%2520normalization%2520with%2520geodesic%2520updates%2520on%2520the%2520manifold.%2520Furthermore%252C%2520analogous%2520to%2520learning%2520rate%2520schedules%252C%2520we%2520propose%2520a%2520layer-wise%2520update%2520decay%2520for%2520the%2520FFN%2520and%2520attention%2520components.%2520Comprehensive%2520experiments%2520demonstrate%2520that%2520GeoNorm%2520consistently%2520outperforms%2520existing%2520normalization%2520methods%2520in%2520Transformer%2520models.%2520Crucially%252C%2520GeoNorm%2520can%2520be%2520seamlessly%2520integrated%2520into%2520standard%2520Transformer%2520architectures%252C%2520achieving%2520performance%2520improvements%2520with%2520negligible%2520additional%2520computational%2520cost.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoNorm%3A%20Unify%20Pre-Norm%20and%20Post-Norm%20with%20Geodesic%20Optimization&entry.906535625=Chuanyang%20Zheng%20and%20Jiankai%20Sun%20and%20Yihang%20Gao%20and%20Chi%20Wang%20and%20Yuehao%20Wang%20and%20Jing%20Xiong%20and%20Liliang%20Ren%20and%20Bo%20Peng%20and%20Qingmei%20Wang%20and%20Xiaoran%20Shang%20and%20Mac%20Schwager%20and%20Anderson%20Schneider%20and%20Yuriy%20Nevmyvaka%20and%20Xiaodong%20Liu&entry.1292438233=The%20placement%20of%20normalization%20layers%2C%20specifically%20Pre-Norm%20and%20Post-Norm%2C%20remains%20an%20open%20question%20in%20Transformer%20architecture%20design.%20In%20this%20work%2C%20we%20rethink%20these%20approaches%20through%20the%20lens%20of%20manifold%20optimization%2C%20interpreting%20the%20outputs%20of%20the%20Feed-Forward%20Network%20%28FFN%29%20and%20attention%20layers%20as%20update%20directions%20in%20optimization.%20Building%20on%20this%20perspective%2C%20we%20introduce%20GeoNorm%2C%20a%20novel%20method%20that%20replaces%20standard%20normalization%20with%20geodesic%20updates%20on%20the%20manifold.%20Furthermore%2C%20analogous%20to%20learning%20rate%20schedules%2C%20we%20propose%20a%20layer-wise%20update%20decay%20for%20the%20FFN%20and%20attention%20components.%20Comprehensive%20experiments%20demonstrate%20that%20GeoNorm%20consistently%20outperforms%20existing%20normalization%20methods%20in%20Transformer%20models.%20Crucially%2C%20GeoNorm%20can%20be%20seamlessly%20integrated%20into%20standard%20Transformer%20architectures%2C%20achieving%20performance%20improvements%20with%20negligible%20additional%20computational%20cost.&entry.1838667208=http%3A//arxiv.org/abs/2601.22095v1&entry.124074799=Read"},
{"title": "Language-based Trial and Error Falls Behind in the Era of Experience", "author": "Haoyu Wang and Guozheng Ma and Shugang Cui and Yilun Kong and Haotian Luo and Li Shen and Mengya Gao and Yichao Wu and Xiaogang Wang and Dacheng Tao", "abstract": "While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight \"scouts\" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.", "link": "http://arxiv.org/abs/2601.21754v1", "date": "2026-01-29", "relevancy": 2.1978, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5487}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-based%20Trial%20and%20Error%20Falls%20Behind%20in%20the%20Era%20of%20Experience&body=Title%3A%20Language-based%20Trial%20and%20Error%20Falls%20Behind%20in%20the%20Era%20of%20Experience%0AAuthor%3A%20Haoyu%20Wang%20and%20Guozheng%20Ma%20and%20Shugang%20Cui%20and%20Yilun%20Kong%20and%20Haotian%20Luo%20and%20Li%20Shen%20and%20Mengya%20Gao%20and%20Yichao%20Wu%20and%20Xiaogang%20Wang%20and%20Dacheng%20Tao%0AAbstract%3A%20While%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20language-based%20agentic%20tasks%2C%20their%20applicability%20to%20unseen%2C%20nonlinguistic%20environments%20%28e.g.%2C%20symbolic%20or%20spatial%20tasks%29%20remains%20limited.%20Previous%20work%20attributes%20this%20performance%20gap%20to%20the%20mismatch%20between%20the%20pretraining%20distribution%20and%20the%20testing%20distribution.%20In%20this%20work%2C%20we%20demonstrate%20the%20primary%20bottleneck%20is%20the%20prohibitive%20cost%20of%20exploration%3A%20mastering%20these%20tasks%20requires%20extensive%20trial-and-error%2C%20which%20is%20computationally%20unsustainable%20for%20parameter-heavy%20LLMs%20operating%20in%20a%20high%20dimensional%20semantic%20space.%20To%20address%20this%2C%20we%20propose%20SCOUT%20%28Sub-Scale%20Collaboration%20On%20Unseen%20Tasks%29%2C%20a%20novel%20framework%20that%20decouples%20exploration%20from%20exploitation.%20We%20employ%20lightweight%20%22scouts%22%20%28e.g.%2C%20small%20MLPs%29%20to%20probe%20environmental%20dynamics%20at%20a%20speed%20and%20scale%20far%20exceeding%20LLMs.%20The%20collected%20trajectories%20are%20utilized%20to%20bootstrap%20the%20LLM%20via%20Supervised%20Fine-Tuning%20%28SFT%29%2C%20followed%20by%20multi-turn%20Reinforcement%20Learning%20%28RL%29%20to%20activate%20its%20latent%20world%20knowledge.%20Empirically%2C%20SCOUT%20enables%20a%20Qwen2.5-3B-Instruct%20model%20to%20achieve%20an%20average%20score%20of%200.86%2C%20significantly%20outperforming%20proprietary%20models%2C%20including%20Gemini-2.5-Pro%20%280.60%29%2C%20while%20saving%20about%2060%25%20GPU%20hours%20consumption.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-based%2520Trial%2520and%2520Error%2520Falls%2520Behind%2520in%2520the%2520Era%2520of%2520Experience%26entry.906535625%3DHaoyu%2520Wang%2520and%2520Guozheng%2520Ma%2520and%2520Shugang%2520Cui%2520and%2520Yilun%2520Kong%2520and%2520Haotian%2520Luo%2520and%2520Li%2520Shen%2520and%2520Mengya%2520Gao%2520and%2520Yichao%2520Wu%2520and%2520Xiaogang%2520Wang%2520and%2520Dacheng%2520Tao%26entry.1292438233%3DWhile%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520in%2520language-based%2520agentic%2520tasks%252C%2520their%2520applicability%2520to%2520unseen%252C%2520nonlinguistic%2520environments%2520%2528e.g.%252C%2520symbolic%2520or%2520spatial%2520tasks%2529%2520remains%2520limited.%2520Previous%2520work%2520attributes%2520this%2520performance%2520gap%2520to%2520the%2520mismatch%2520between%2520the%2520pretraining%2520distribution%2520and%2520the%2520testing%2520distribution.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520the%2520primary%2520bottleneck%2520is%2520the%2520prohibitive%2520cost%2520of%2520exploration%253A%2520mastering%2520these%2520tasks%2520requires%2520extensive%2520trial-and-error%252C%2520which%2520is%2520computationally%2520unsustainable%2520for%2520parameter-heavy%2520LLMs%2520operating%2520in%2520a%2520high%2520dimensional%2520semantic%2520space.%2520To%2520address%2520this%252C%2520we%2520propose%2520SCOUT%2520%2528Sub-Scale%2520Collaboration%2520On%2520Unseen%2520Tasks%2529%252C%2520a%2520novel%2520framework%2520that%2520decouples%2520exploration%2520from%2520exploitation.%2520We%2520employ%2520lightweight%2520%2522scouts%2522%2520%2528e.g.%252C%2520small%2520MLPs%2529%2520to%2520probe%2520environmental%2520dynamics%2520at%2520a%2520speed%2520and%2520scale%2520far%2520exceeding%2520LLMs.%2520The%2520collected%2520trajectories%2520are%2520utilized%2520to%2520bootstrap%2520the%2520LLM%2520via%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%252C%2520followed%2520by%2520multi-turn%2520Reinforcement%2520Learning%2520%2528RL%2529%2520to%2520activate%2520its%2520latent%2520world%2520knowledge.%2520Empirically%252C%2520SCOUT%2520enables%2520a%2520Qwen2.5-3B-Instruct%2520model%2520to%2520achieve%2520an%2520average%2520score%2520of%25200.86%252C%2520significantly%2520outperforming%2520proprietary%2520models%252C%2520including%2520Gemini-2.5-Pro%2520%25280.60%2529%252C%2520while%2520saving%2520about%252060%2525%2520GPU%2520hours%2520consumption.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-based%20Trial%20and%20Error%20Falls%20Behind%20in%20the%20Era%20of%20Experience&entry.906535625=Haoyu%20Wang%20and%20Guozheng%20Ma%20and%20Shugang%20Cui%20and%20Yilun%20Kong%20and%20Haotian%20Luo%20and%20Li%20Shen%20and%20Mengya%20Gao%20and%20Yichao%20Wu%20and%20Xiaogang%20Wang%20and%20Dacheng%20Tao&entry.1292438233=While%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20language-based%20agentic%20tasks%2C%20their%20applicability%20to%20unseen%2C%20nonlinguistic%20environments%20%28e.g.%2C%20symbolic%20or%20spatial%20tasks%29%20remains%20limited.%20Previous%20work%20attributes%20this%20performance%20gap%20to%20the%20mismatch%20between%20the%20pretraining%20distribution%20and%20the%20testing%20distribution.%20In%20this%20work%2C%20we%20demonstrate%20the%20primary%20bottleneck%20is%20the%20prohibitive%20cost%20of%20exploration%3A%20mastering%20these%20tasks%20requires%20extensive%20trial-and-error%2C%20which%20is%20computationally%20unsustainable%20for%20parameter-heavy%20LLMs%20operating%20in%20a%20high%20dimensional%20semantic%20space.%20To%20address%20this%2C%20we%20propose%20SCOUT%20%28Sub-Scale%20Collaboration%20On%20Unseen%20Tasks%29%2C%20a%20novel%20framework%20that%20decouples%20exploration%20from%20exploitation.%20We%20employ%20lightweight%20%22scouts%22%20%28e.g.%2C%20small%20MLPs%29%20to%20probe%20environmental%20dynamics%20at%20a%20speed%20and%20scale%20far%20exceeding%20LLMs.%20The%20collected%20trajectories%20are%20utilized%20to%20bootstrap%20the%20LLM%20via%20Supervised%20Fine-Tuning%20%28SFT%29%2C%20followed%20by%20multi-turn%20Reinforcement%20Learning%20%28RL%29%20to%20activate%20its%20latent%20world%20knowledge.%20Empirically%2C%20SCOUT%20enables%20a%20Qwen2.5-3B-Instruct%20model%20to%20achieve%20an%20average%20score%20of%200.86%2C%20significantly%20outperforming%20proprietary%20models%2C%20including%20Gemini-2.5-Pro%20%280.60%29%2C%20while%20saving%20about%2060%25%20GPU%20hours%20consumption.&entry.1838667208=http%3A//arxiv.org/abs/2601.21754v1&entry.124074799=Read"},
{"title": "Explicit Credit Assignment through Local Rewards and Dependence Graphs in Multi-Agent Reinforcement Learning", "author": "Bang Giang Le and Viet Cuong Ta", "abstract": "To promote cooperation in Multi-Agent Reinforcement Learning, the reward signals of all agents can be aggregated together, forming global rewards that are commonly known as the fully cooperative setting. However, global rewards are usually noisy because they contain the contributions of all agents, which have to be resolved in the credit assignment process. On the other hand, using local reward benefits from faster learning due to the separation of agents' contributions, but can be suboptimal as agents myopically optimize their own reward while disregarding the global optimality. In this work, we propose a method that combines the merits of both approaches. By using a graph of interaction between agents, our method discerns the individual agent contribution in a more fine-grained manner than a global reward, while alleviating the cooperation problem with agents' local reward. We also introduce a practical approach for approximating such a graph. Our experiments demonstrate the flexibility of the approach, enabling improvements over the traditional local and global reward settings.", "link": "http://arxiv.org/abs/2601.21523v1", "date": "2026-01-29", "relevancy": 1.9137, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4946}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4677}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicit%20Credit%20Assignment%20through%20Local%20Rewards%20and%20Dependence%20Graphs%20in%20Multi-Agent%20Reinforcement%20Learning&body=Title%3A%20Explicit%20Credit%20Assignment%20through%20Local%20Rewards%20and%20Dependence%20Graphs%20in%20Multi-Agent%20Reinforcement%20Learning%0AAuthor%3A%20Bang%20Giang%20Le%20and%20Viet%20Cuong%20Ta%0AAbstract%3A%20To%20promote%20cooperation%20in%20Multi-Agent%20Reinforcement%20Learning%2C%20the%20reward%20signals%20of%20all%20agents%20can%20be%20aggregated%20together%2C%20forming%20global%20rewards%20that%20are%20commonly%20known%20as%20the%20fully%20cooperative%20setting.%20However%2C%20global%20rewards%20are%20usually%20noisy%20because%20they%20contain%20the%20contributions%20of%20all%20agents%2C%20which%20have%20to%20be%20resolved%20in%20the%20credit%20assignment%20process.%20On%20the%20other%20hand%2C%20using%20local%20reward%20benefits%20from%20faster%20learning%20due%20to%20the%20separation%20of%20agents%27%20contributions%2C%20but%20can%20be%20suboptimal%20as%20agents%20myopically%20optimize%20their%20own%20reward%20while%20disregarding%20the%20global%20optimality.%20In%20this%20work%2C%20we%20propose%20a%20method%20that%20combines%20the%20merits%20of%20both%20approaches.%20By%20using%20a%20graph%20of%20interaction%20between%20agents%2C%20our%20method%20discerns%20the%20individual%20agent%20contribution%20in%20a%20more%20fine-grained%20manner%20than%20a%20global%20reward%2C%20while%20alleviating%20the%20cooperation%20problem%20with%20agents%27%20local%20reward.%20We%20also%20introduce%20a%20practical%20approach%20for%20approximating%20such%20a%20graph.%20Our%20experiments%20demonstrate%20the%20flexibility%20of%20the%20approach%2C%20enabling%20improvements%20over%20the%20traditional%20local%20and%20global%20reward%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicit%2520Credit%2520Assignment%2520through%2520Local%2520Rewards%2520and%2520Dependence%2520Graphs%2520in%2520Multi-Agent%2520Reinforcement%2520Learning%26entry.906535625%3DBang%2520Giang%2520Le%2520and%2520Viet%2520Cuong%2520Ta%26entry.1292438233%3DTo%2520promote%2520cooperation%2520in%2520Multi-Agent%2520Reinforcement%2520Learning%252C%2520the%2520reward%2520signals%2520of%2520all%2520agents%2520can%2520be%2520aggregated%2520together%252C%2520forming%2520global%2520rewards%2520that%2520are%2520commonly%2520known%2520as%2520the%2520fully%2520cooperative%2520setting.%2520However%252C%2520global%2520rewards%2520are%2520usually%2520noisy%2520because%2520they%2520contain%2520the%2520contributions%2520of%2520all%2520agents%252C%2520which%2520have%2520to%2520be%2520resolved%2520in%2520the%2520credit%2520assignment%2520process.%2520On%2520the%2520other%2520hand%252C%2520using%2520local%2520reward%2520benefits%2520from%2520faster%2520learning%2520due%2520to%2520the%2520separation%2520of%2520agents%2527%2520contributions%252C%2520but%2520can%2520be%2520suboptimal%2520as%2520agents%2520myopically%2520optimize%2520their%2520own%2520reward%2520while%2520disregarding%2520the%2520global%2520optimality.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520method%2520that%2520combines%2520the%2520merits%2520of%2520both%2520approaches.%2520By%2520using%2520a%2520graph%2520of%2520interaction%2520between%2520agents%252C%2520our%2520method%2520discerns%2520the%2520individual%2520agent%2520contribution%2520in%2520a%2520more%2520fine-grained%2520manner%2520than%2520a%2520global%2520reward%252C%2520while%2520alleviating%2520the%2520cooperation%2520problem%2520with%2520agents%2527%2520local%2520reward.%2520We%2520also%2520introduce%2520a%2520practical%2520approach%2520for%2520approximating%2520such%2520a%2520graph.%2520Our%2520experiments%2520demonstrate%2520the%2520flexibility%2520of%2520the%2520approach%252C%2520enabling%2520improvements%2520over%2520the%2520traditional%2520local%2520and%2520global%2520reward%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicit%20Credit%20Assignment%20through%20Local%20Rewards%20and%20Dependence%20Graphs%20in%20Multi-Agent%20Reinforcement%20Learning&entry.906535625=Bang%20Giang%20Le%20and%20Viet%20Cuong%20Ta&entry.1292438233=To%20promote%20cooperation%20in%20Multi-Agent%20Reinforcement%20Learning%2C%20the%20reward%20signals%20of%20all%20agents%20can%20be%20aggregated%20together%2C%20forming%20global%20rewards%20that%20are%20commonly%20known%20as%20the%20fully%20cooperative%20setting.%20However%2C%20global%20rewards%20are%20usually%20noisy%20because%20they%20contain%20the%20contributions%20of%20all%20agents%2C%20which%20have%20to%20be%20resolved%20in%20the%20credit%20assignment%20process.%20On%20the%20other%20hand%2C%20using%20local%20reward%20benefits%20from%20faster%20learning%20due%20to%20the%20separation%20of%20agents%27%20contributions%2C%20but%20can%20be%20suboptimal%20as%20agents%20myopically%20optimize%20their%20own%20reward%20while%20disregarding%20the%20global%20optimality.%20In%20this%20work%2C%20we%20propose%20a%20method%20that%20combines%20the%20merits%20of%20both%20approaches.%20By%20using%20a%20graph%20of%20interaction%20between%20agents%2C%20our%20method%20discerns%20the%20individual%20agent%20contribution%20in%20a%20more%20fine-grained%20manner%20than%20a%20global%20reward%2C%20while%20alleviating%20the%20cooperation%20problem%20with%20agents%27%20local%20reward.%20We%20also%20introduce%20a%20practical%20approach%20for%20approximating%20such%20a%20graph.%20Our%20experiments%20demonstrate%20the%20flexibility%20of%20the%20approach%2C%20enabling%20improvements%20over%20the%20traditional%20local%20and%20global%20reward%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2601.21523v1&entry.124074799=Read"},
{"title": "The Illusion of Certainty: Uncertainty Quantification for LLMs Fails under Ambiguity", "author": "Tim Tomov and Dominik Fuchsgruber and Tom Wollschl\u00e4ger and Stephan G\u00fcnnemann", "abstract": "Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is critical for trustworthy deployment. While real-world language is inherently ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically benchmarked against tasks with no ambiguity. In this work, we demonstrate that while current uncertainty estimators perform well under the restrictive assumption of no ambiguity, they degrade to close-to-random performance on ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first ambiguous question-answering (QA) datasets equipped with ground-truth answer distributions estimated from factual co-occurrence. We find this performance deterioration to be consistent across different estimation paradigms: using the predictive distribution itself, internal representations throughout the model, and an ensemble of models. We show that this phenomenon can be theoretically explained, revealing that predictive-distribution and ensemble-based estimators are fundamentally limited under ambiguity. Overall, our study reveals a key shortcoming of current UQ methods for LLMs and motivates a rethinking of current modeling paradigms.", "link": "http://arxiv.org/abs/2511.04418v2", "date": "2026-01-29", "relevancy": 2.1987, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6166}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5501}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Illusion%20of%20Certainty%3A%20Uncertainty%20Quantification%20for%20LLMs%20Fails%20under%20Ambiguity&body=Title%3A%20The%20Illusion%20of%20Certainty%3A%20Uncertainty%20Quantification%20for%20LLMs%20Fails%20under%20Ambiguity%0AAuthor%3A%20Tim%20Tomov%20and%20Dominik%20Fuchsgruber%20and%20Tom%20Wollschl%C3%A4ger%20and%20Stephan%20G%C3%BCnnemann%0AAbstract%3A%20Accurate%20uncertainty%20quantification%20%28UQ%29%20in%20Large%20Language%20Models%20%28LLMs%29%20is%20critical%20for%20trustworthy%20deployment.%20While%20real-world%20language%20is%20inherently%20ambiguous%2C%20reflecting%20aleatoric%20uncertainty%2C%20existing%20UQ%20methods%20are%20typically%20benchmarked%20against%20tasks%20with%20no%20ambiguity.%20In%20this%20work%2C%20we%20demonstrate%20that%20while%20current%20uncertainty%20estimators%20perform%20well%20under%20the%20restrictive%20assumption%20of%20no%20ambiguity%2C%20they%20degrade%20to%20close-to-random%20performance%20on%20ambiguous%20data.%20To%20this%20end%2C%20we%20introduce%20MAQA%2A%20and%20AmbigQA%2A%2C%20the%20first%20ambiguous%20question-answering%20%28QA%29%20datasets%20equipped%20with%20ground-truth%20answer%20distributions%20estimated%20from%20factual%20co-occurrence.%20We%20find%20this%20performance%20deterioration%20to%20be%20consistent%20across%20different%20estimation%20paradigms%3A%20using%20the%20predictive%20distribution%20itself%2C%20internal%20representations%20throughout%20the%20model%2C%20and%20an%20ensemble%20of%20models.%20We%20show%20that%20this%20phenomenon%20can%20be%20theoretically%20explained%2C%20revealing%20that%20predictive-distribution%20and%20ensemble-based%20estimators%20are%20fundamentally%20limited%20under%20ambiguity.%20Overall%2C%20our%20study%20reveals%20a%20key%20shortcoming%20of%20current%20UQ%20methods%20for%20LLMs%20and%20motivates%20a%20rethinking%20of%20current%20modeling%20paradigms.%0ALink%3A%20http%3A//arxiv.org/abs/2511.04418v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Illusion%2520of%2520Certainty%253A%2520Uncertainty%2520Quantification%2520for%2520LLMs%2520Fails%2520under%2520Ambiguity%26entry.906535625%3DTim%2520Tomov%2520and%2520Dominik%2520Fuchsgruber%2520and%2520Tom%2520Wollschl%25C3%25A4ger%2520and%2520Stephan%2520G%25C3%25BCnnemann%26entry.1292438233%3DAccurate%2520uncertainty%2520quantification%2520%2528UQ%2529%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520critical%2520for%2520trustworthy%2520deployment.%2520While%2520real-world%2520language%2520is%2520inherently%2520ambiguous%252C%2520reflecting%2520aleatoric%2520uncertainty%252C%2520existing%2520UQ%2520methods%2520are%2520typically%2520benchmarked%2520against%2520tasks%2520with%2520no%2520ambiguity.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520while%2520current%2520uncertainty%2520estimators%2520perform%2520well%2520under%2520the%2520restrictive%2520assumption%2520of%2520no%2520ambiguity%252C%2520they%2520degrade%2520to%2520close-to-random%2520performance%2520on%2520ambiguous%2520data.%2520To%2520this%2520end%252C%2520we%2520introduce%2520MAQA%252A%2520and%2520AmbigQA%252A%252C%2520the%2520first%2520ambiguous%2520question-answering%2520%2528QA%2529%2520datasets%2520equipped%2520with%2520ground-truth%2520answer%2520distributions%2520estimated%2520from%2520factual%2520co-occurrence.%2520We%2520find%2520this%2520performance%2520deterioration%2520to%2520be%2520consistent%2520across%2520different%2520estimation%2520paradigms%253A%2520using%2520the%2520predictive%2520distribution%2520itself%252C%2520internal%2520representations%2520throughout%2520the%2520model%252C%2520and%2520an%2520ensemble%2520of%2520models.%2520We%2520show%2520that%2520this%2520phenomenon%2520can%2520be%2520theoretically%2520explained%252C%2520revealing%2520that%2520predictive-distribution%2520and%2520ensemble-based%2520estimators%2520are%2520fundamentally%2520limited%2520under%2520ambiguity.%2520Overall%252C%2520our%2520study%2520reveals%2520a%2520key%2520shortcoming%2520of%2520current%2520UQ%2520methods%2520for%2520LLMs%2520and%2520motivates%2520a%2520rethinking%2520of%2520current%2520modeling%2520paradigms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.04418v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Illusion%20of%20Certainty%3A%20Uncertainty%20Quantification%20for%20LLMs%20Fails%20under%20Ambiguity&entry.906535625=Tim%20Tomov%20and%20Dominik%20Fuchsgruber%20and%20Tom%20Wollschl%C3%A4ger%20and%20Stephan%20G%C3%BCnnemann&entry.1292438233=Accurate%20uncertainty%20quantification%20%28UQ%29%20in%20Large%20Language%20Models%20%28LLMs%29%20is%20critical%20for%20trustworthy%20deployment.%20While%20real-world%20language%20is%20inherently%20ambiguous%2C%20reflecting%20aleatoric%20uncertainty%2C%20existing%20UQ%20methods%20are%20typically%20benchmarked%20against%20tasks%20with%20no%20ambiguity.%20In%20this%20work%2C%20we%20demonstrate%20that%20while%20current%20uncertainty%20estimators%20perform%20well%20under%20the%20restrictive%20assumption%20of%20no%20ambiguity%2C%20they%20degrade%20to%20close-to-random%20performance%20on%20ambiguous%20data.%20To%20this%20end%2C%20we%20introduce%20MAQA%2A%20and%20AmbigQA%2A%2C%20the%20first%20ambiguous%20question-answering%20%28QA%29%20datasets%20equipped%20with%20ground-truth%20answer%20distributions%20estimated%20from%20factual%20co-occurrence.%20We%20find%20this%20performance%20deterioration%20to%20be%20consistent%20across%20different%20estimation%20paradigms%3A%20using%20the%20predictive%20distribution%20itself%2C%20internal%20representations%20throughout%20the%20model%2C%20and%20an%20ensemble%20of%20models.%20We%20show%20that%20this%20phenomenon%20can%20be%20theoretically%20explained%2C%20revealing%20that%20predictive-distribution%20and%20ensemble-based%20estimators%20are%20fundamentally%20limited%20under%20ambiguity.%20Overall%2C%20our%20study%20reveals%20a%20key%20shortcoming%20of%20current%20UQ%20methods%20for%20LLMs%20and%20motivates%20a%20rethinking%20of%20current%20modeling%20paradigms.&entry.1838667208=http%3A//arxiv.org/abs/2511.04418v2&entry.124074799=Read"},
{"title": "CoFrGeNet: Continued Fraction Architectures for Language Generation", "author": "Amit Dhurandhar and Vijil Chenthamarakshan and Dennis Wei and Tejaswini Pedapati and Karthikeyan Natesan Ramamurthy and Rahul Nair", "abstract": "Transformers are arguably the preferred architecture for language generation. In this paper, inspired by continued fractions, we introduce a new function class for generative modeling. The architecture family implementing this function class is named CoFrGeNets - Continued Fraction Generative Networks. We design novel architectural components based on this function class that can replace Multi-head Attention and Feed-Forward Networks in Transformer blocks while requiring much fewer parameters. We derive custom gradient formulations to optimize the proposed components more accurately and efficiently than using standard PyTorch-based gradients. Our components are a plug-in replacement requiring little change in training or inference procedures that have already been put in place for Transformer-based models thus making our approach easy to incorporate in large industrial workflows. We experiment on two very different transformer architectures GPT2-xl (1.5B) and Llama3 (3.2B), where the former we pre-train on OpenWebText and GneissWeb, while the latter we pre-train on the docling data mix which consists of nine different datasets. Results show that the performance on downstream classification, Q\\& A, reasoning and text understanding tasks of our models is competitive and sometimes even superior to the original models with $\\frac{2}{3}$ to $\\frac{1}{2}$ the parameters and shorter pre-training time. We believe that future implementations customized to hardware will further bring out the true potential of our architectures.", "link": "http://arxiv.org/abs/2601.21766v1", "date": "2026-01-29", "relevancy": 2.1672, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6199}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5453}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoFrGeNet%3A%20Continued%20Fraction%20Architectures%20for%20Language%20Generation&body=Title%3A%20CoFrGeNet%3A%20Continued%20Fraction%20Architectures%20for%20Language%20Generation%0AAuthor%3A%20Amit%20Dhurandhar%20and%20Vijil%20Chenthamarakshan%20and%20Dennis%20Wei%20and%20Tejaswini%20Pedapati%20and%20Karthikeyan%20Natesan%20Ramamurthy%20and%20Rahul%20Nair%0AAbstract%3A%20Transformers%20are%20arguably%20the%20preferred%20architecture%20for%20language%20generation.%20In%20this%20paper%2C%20inspired%20by%20continued%20fractions%2C%20we%20introduce%20a%20new%20function%20class%20for%20generative%20modeling.%20The%20architecture%20family%20implementing%20this%20function%20class%20is%20named%20CoFrGeNets%20-%20Continued%20Fraction%20Generative%20Networks.%20We%20design%20novel%20architectural%20components%20based%20on%20this%20function%20class%20that%20can%20replace%20Multi-head%20Attention%20and%20Feed-Forward%20Networks%20in%20Transformer%20blocks%20while%20requiring%20much%20fewer%20parameters.%20We%20derive%20custom%20gradient%20formulations%20to%20optimize%20the%20proposed%20components%20more%20accurately%20and%20efficiently%20than%20using%20standard%20PyTorch-based%20gradients.%20Our%20components%20are%20a%20plug-in%20replacement%20requiring%20little%20change%20in%20training%20or%20inference%20procedures%20that%20have%20already%20been%20put%20in%20place%20for%20Transformer-based%20models%20thus%20making%20our%20approach%20easy%20to%20incorporate%20in%20large%20industrial%20workflows.%20We%20experiment%20on%20two%20very%20different%20transformer%20architectures%20GPT2-xl%20%281.5B%29%20and%20Llama3%20%283.2B%29%2C%20where%20the%20former%20we%20pre-train%20on%20OpenWebText%20and%20GneissWeb%2C%20while%20the%20latter%20we%20pre-train%20on%20the%20docling%20data%20mix%20which%20consists%20of%20nine%20different%20datasets.%20Results%20show%20that%20the%20performance%20on%20downstream%20classification%2C%20Q%5C%26%20A%2C%20reasoning%20and%20text%20understanding%20tasks%20of%20our%20models%20is%20competitive%20and%20sometimes%20even%20superior%20to%20the%20original%20models%20with%20%24%5Cfrac%7B2%7D%7B3%7D%24%20to%20%24%5Cfrac%7B1%7D%7B2%7D%24%20the%20parameters%20and%20shorter%20pre-training%20time.%20We%20believe%20that%20future%20implementations%20customized%20to%20hardware%20will%20further%20bring%20out%20the%20true%20potential%20of%20our%20architectures.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoFrGeNet%253A%2520Continued%2520Fraction%2520Architectures%2520for%2520Language%2520Generation%26entry.906535625%3DAmit%2520Dhurandhar%2520and%2520Vijil%2520Chenthamarakshan%2520and%2520Dennis%2520Wei%2520and%2520Tejaswini%2520Pedapati%2520and%2520Karthikeyan%2520Natesan%2520Ramamurthy%2520and%2520Rahul%2520Nair%26entry.1292438233%3DTransformers%2520are%2520arguably%2520the%2520preferred%2520architecture%2520for%2520language%2520generation.%2520In%2520this%2520paper%252C%2520inspired%2520by%2520continued%2520fractions%252C%2520we%2520introduce%2520a%2520new%2520function%2520class%2520for%2520generative%2520modeling.%2520The%2520architecture%2520family%2520implementing%2520this%2520function%2520class%2520is%2520named%2520CoFrGeNets%2520-%2520Continued%2520Fraction%2520Generative%2520Networks.%2520We%2520design%2520novel%2520architectural%2520components%2520based%2520on%2520this%2520function%2520class%2520that%2520can%2520replace%2520Multi-head%2520Attention%2520and%2520Feed-Forward%2520Networks%2520in%2520Transformer%2520blocks%2520while%2520requiring%2520much%2520fewer%2520parameters.%2520We%2520derive%2520custom%2520gradient%2520formulations%2520to%2520optimize%2520the%2520proposed%2520components%2520more%2520accurately%2520and%2520efficiently%2520than%2520using%2520standard%2520PyTorch-based%2520gradients.%2520Our%2520components%2520are%2520a%2520plug-in%2520replacement%2520requiring%2520little%2520change%2520in%2520training%2520or%2520inference%2520procedures%2520that%2520have%2520already%2520been%2520put%2520in%2520place%2520for%2520Transformer-based%2520models%2520thus%2520making%2520our%2520approach%2520easy%2520to%2520incorporate%2520in%2520large%2520industrial%2520workflows.%2520We%2520experiment%2520on%2520two%2520very%2520different%2520transformer%2520architectures%2520GPT2-xl%2520%25281.5B%2529%2520and%2520Llama3%2520%25283.2B%2529%252C%2520where%2520the%2520former%2520we%2520pre-train%2520on%2520OpenWebText%2520and%2520GneissWeb%252C%2520while%2520the%2520latter%2520we%2520pre-train%2520on%2520the%2520docling%2520data%2520mix%2520which%2520consists%2520of%2520nine%2520different%2520datasets.%2520Results%2520show%2520that%2520the%2520performance%2520on%2520downstream%2520classification%252C%2520Q%255C%2526%2520A%252C%2520reasoning%2520and%2520text%2520understanding%2520tasks%2520of%2520our%2520models%2520is%2520competitive%2520and%2520sometimes%2520even%2520superior%2520to%2520the%2520original%2520models%2520with%2520%2524%255Cfrac%257B2%257D%257B3%257D%2524%2520to%2520%2524%255Cfrac%257B1%257D%257B2%257D%2524%2520the%2520parameters%2520and%2520shorter%2520pre-training%2520time.%2520We%2520believe%2520that%2520future%2520implementations%2520customized%2520to%2520hardware%2520will%2520further%2520bring%2520out%2520the%2520true%2520potential%2520of%2520our%2520architectures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoFrGeNet%3A%20Continued%20Fraction%20Architectures%20for%20Language%20Generation&entry.906535625=Amit%20Dhurandhar%20and%20Vijil%20Chenthamarakshan%20and%20Dennis%20Wei%20and%20Tejaswini%20Pedapati%20and%20Karthikeyan%20Natesan%20Ramamurthy%20and%20Rahul%20Nair&entry.1292438233=Transformers%20are%20arguably%20the%20preferred%20architecture%20for%20language%20generation.%20In%20this%20paper%2C%20inspired%20by%20continued%20fractions%2C%20we%20introduce%20a%20new%20function%20class%20for%20generative%20modeling.%20The%20architecture%20family%20implementing%20this%20function%20class%20is%20named%20CoFrGeNets%20-%20Continued%20Fraction%20Generative%20Networks.%20We%20design%20novel%20architectural%20components%20based%20on%20this%20function%20class%20that%20can%20replace%20Multi-head%20Attention%20and%20Feed-Forward%20Networks%20in%20Transformer%20blocks%20while%20requiring%20much%20fewer%20parameters.%20We%20derive%20custom%20gradient%20formulations%20to%20optimize%20the%20proposed%20components%20more%20accurately%20and%20efficiently%20than%20using%20standard%20PyTorch-based%20gradients.%20Our%20components%20are%20a%20plug-in%20replacement%20requiring%20little%20change%20in%20training%20or%20inference%20procedures%20that%20have%20already%20been%20put%20in%20place%20for%20Transformer-based%20models%20thus%20making%20our%20approach%20easy%20to%20incorporate%20in%20large%20industrial%20workflows.%20We%20experiment%20on%20two%20very%20different%20transformer%20architectures%20GPT2-xl%20%281.5B%29%20and%20Llama3%20%283.2B%29%2C%20where%20the%20former%20we%20pre-train%20on%20OpenWebText%20and%20GneissWeb%2C%20while%20the%20latter%20we%20pre-train%20on%20the%20docling%20data%20mix%20which%20consists%20of%20nine%20different%20datasets.%20Results%20show%20that%20the%20performance%20on%20downstream%20classification%2C%20Q%5C%26%20A%2C%20reasoning%20and%20text%20understanding%20tasks%20of%20our%20models%20is%20competitive%20and%20sometimes%20even%20superior%20to%20the%20original%20models%20with%20%24%5Cfrac%7B2%7D%7B3%7D%24%20to%20%24%5Cfrac%7B1%7D%7B2%7D%24%20the%20parameters%20and%20shorter%20pre-training%20time.%20We%20believe%20that%20future%20implementations%20customized%20to%20hardware%20will%20further%20bring%20out%20the%20true%20potential%20of%20our%20architectures.&entry.1838667208=http%3A//arxiv.org/abs/2601.21766v1&entry.124074799=Read"},
{"title": "PROMA: Projected Microbatch Accumulation for Reference-Free Proximal Policy Updates", "author": "Nilin Abrahamsen", "abstract": "This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy method that modifies gradient accumulation across microbatches rather than relying on likelihood ratios relative to a reference policy. During accumulation, PROMA projects the partially accumulated gradient to be orthogonal to the sequence-wise gradients of the current microbatch. This projection is applied layer-wise during the backward pass, enabling efficient implementation. A within-microbatch variant (Intra-PROMA) acts independently across microbatches. Empirically, PROMA achieves proximal updates without entropy collapse while providing tighter local KL control than GRPO.", "link": "http://arxiv.org/abs/2601.10498v3", "date": "2026-01-29", "relevancy": 1.7143, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4516}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4286}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PROMA%3A%20Projected%20Microbatch%20Accumulation%20for%20Reference-Free%20Proximal%20Policy%20Updates&body=Title%3A%20PROMA%3A%20Projected%20Microbatch%20Accumulation%20for%20Reference-Free%20Proximal%20Policy%20Updates%0AAuthor%3A%20Nilin%20Abrahamsen%0AAbstract%3A%20This%20note%20introduces%20Projected%20Microbatch%20Accumulation%20%28PROMA%29%2C%20a%20proximal%20policy%20method%20that%20modifies%20gradient%20accumulation%20across%20microbatches%20rather%20than%20relying%20on%20likelihood%20ratios%20relative%20to%20a%20reference%20policy.%20During%20accumulation%2C%20PROMA%20projects%20the%20partially%20accumulated%20gradient%20to%20be%20orthogonal%20to%20the%20sequence-wise%20gradients%20of%20the%20current%20microbatch.%20This%20projection%20is%20applied%20layer-wise%20during%20the%20backward%20pass%2C%20enabling%20efficient%20implementation.%20A%20within-microbatch%20variant%20%28Intra-PROMA%29%20acts%20independently%20across%20microbatches.%20Empirically%2C%20PROMA%20achieves%20proximal%20updates%20without%20entropy%20collapse%20while%20providing%20tighter%20local%20KL%20control%20than%20GRPO.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10498v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPROMA%253A%2520Projected%2520Microbatch%2520Accumulation%2520for%2520Reference-Free%2520Proximal%2520Policy%2520Updates%26entry.906535625%3DNilin%2520Abrahamsen%26entry.1292438233%3DThis%2520note%2520introduces%2520Projected%2520Microbatch%2520Accumulation%2520%2528PROMA%2529%252C%2520a%2520proximal%2520policy%2520method%2520that%2520modifies%2520gradient%2520accumulation%2520across%2520microbatches%2520rather%2520than%2520relying%2520on%2520likelihood%2520ratios%2520relative%2520to%2520a%2520reference%2520policy.%2520During%2520accumulation%252C%2520PROMA%2520projects%2520the%2520partially%2520accumulated%2520gradient%2520to%2520be%2520orthogonal%2520to%2520the%2520sequence-wise%2520gradients%2520of%2520the%2520current%2520microbatch.%2520This%2520projection%2520is%2520applied%2520layer-wise%2520during%2520the%2520backward%2520pass%252C%2520enabling%2520efficient%2520implementation.%2520A%2520within-microbatch%2520variant%2520%2528Intra-PROMA%2529%2520acts%2520independently%2520across%2520microbatches.%2520Empirically%252C%2520PROMA%2520achieves%2520proximal%2520updates%2520without%2520entropy%2520collapse%2520while%2520providing%2520tighter%2520local%2520KL%2520control%2520than%2520GRPO.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10498v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PROMA%3A%20Projected%20Microbatch%20Accumulation%20for%20Reference-Free%20Proximal%20Policy%20Updates&entry.906535625=Nilin%20Abrahamsen&entry.1292438233=This%20note%20introduces%20Projected%20Microbatch%20Accumulation%20%28PROMA%29%2C%20a%20proximal%20policy%20method%20that%20modifies%20gradient%20accumulation%20across%20microbatches%20rather%20than%20relying%20on%20likelihood%20ratios%20relative%20to%20a%20reference%20policy.%20During%20accumulation%2C%20PROMA%20projects%20the%20partially%20accumulated%20gradient%20to%20be%20orthogonal%20to%20the%20sequence-wise%20gradients%20of%20the%20current%20microbatch.%20This%20projection%20is%20applied%20layer-wise%20during%20the%20backward%20pass%2C%20enabling%20efficient%20implementation.%20A%20within-microbatch%20variant%20%28Intra-PROMA%29%20acts%20independently%20across%20microbatches.%20Empirically%2C%20PROMA%20achieves%20proximal%20updates%20without%20entropy%20collapse%20while%20providing%20tighter%20local%20KL%20control%20than%20GRPO.&entry.1838667208=http%3A//arxiv.org/abs/2601.10498v3&entry.124074799=Read"},
{"title": "Age Matters: Analyzing Age-Related Discussions in App Reviews", "author": "Shashiwadana Nirmania and Garima Sharma and Hourieh Khalajzadeh and Mojtaba Shahin", "abstract": "In recent years, mobile applications have become indispensable tools for managing various aspects of life. From enhancing productivity to providing personalized entertainment, mobile apps have revolutionized people's daily routines. Despite this rapid growth and popularity, gaps remain in how these apps address the needs of users from different age groups. Users of varying ages face distinct challenges when interacting with mobile apps, from younger users dealing with inappropriate content to older users having difficulty with usability due to age-related vision and cognition impairments. Although there have been initiatives to create age-inclusive apps, a limited understanding of user perspectives on age-related issues may hinder developers from recognizing specific challenges and implementing effective solutions. In this study, we explore age discussions in app reviews to gain insights into how mobile apps should cater to users across different age groups.We manually curated a dataset of 4,163 app reviews from the Google Play Store and identified 1,429 age-related reviews and 2,734 non-age-related reviews. We employed eight machine learning, deep learning, and large language models to automatically detect age discussions, with RoBERTa performing the best, achieving a precision of 92.46%. Additionally, a qualitative analysis of the 1,429 age-related reviews uncovers six dominant themes reflecting user concerns.", "link": "http://arxiv.org/abs/2601.21605v1", "date": "2026-01-29", "relevancy": 1.4914, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3817}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.3804}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Age%20Matters%3A%20Analyzing%20Age-Related%20Discussions%20in%20App%20Reviews&body=Title%3A%20Age%20Matters%3A%20Analyzing%20Age-Related%20Discussions%20in%20App%20Reviews%0AAuthor%3A%20Shashiwadana%20Nirmania%20and%20Garima%20Sharma%20and%20Hourieh%20Khalajzadeh%20and%20Mojtaba%20Shahin%0AAbstract%3A%20In%20recent%20years%2C%20mobile%20applications%20have%20become%20indispensable%20tools%20for%20managing%20various%20aspects%20of%20life.%20From%20enhancing%20productivity%20to%20providing%20personalized%20entertainment%2C%20mobile%20apps%20have%20revolutionized%20people%27s%20daily%20routines.%20Despite%20this%20rapid%20growth%20and%20popularity%2C%20gaps%20remain%20in%20how%20these%20apps%20address%20the%20needs%20of%20users%20from%20different%20age%20groups.%20Users%20of%20varying%20ages%20face%20distinct%20challenges%20when%20interacting%20with%20mobile%20apps%2C%20from%20younger%20users%20dealing%20with%20inappropriate%20content%20to%20older%20users%20having%20difficulty%20with%20usability%20due%20to%20age-related%20vision%20and%20cognition%20impairments.%20Although%20there%20have%20been%20initiatives%20to%20create%20age-inclusive%20apps%2C%20a%20limited%20understanding%20of%20user%20perspectives%20on%20age-related%20issues%20may%20hinder%20developers%20from%20recognizing%20specific%20challenges%20and%20implementing%20effective%20solutions.%20In%20this%20study%2C%20we%20explore%20age%20discussions%20in%20app%20reviews%20to%20gain%20insights%20into%20how%20mobile%20apps%20should%20cater%20to%20users%20across%20different%20age%20groups.We%20manually%20curated%20a%20dataset%20of%204%2C163%20app%20reviews%20from%20the%20Google%20Play%20Store%20and%20identified%201%2C429%20age-related%20reviews%20and%202%2C734%20non-age-related%20reviews.%20We%20employed%20eight%20machine%20learning%2C%20deep%20learning%2C%20and%20large%20language%20models%20to%20automatically%20detect%20age%20discussions%2C%20with%20RoBERTa%20performing%20the%20best%2C%20achieving%20a%20precision%20of%2092.46%25.%20Additionally%2C%20a%20qualitative%20analysis%20of%20the%201%2C429%20age-related%20reviews%20uncovers%20six%20dominant%20themes%20reflecting%20user%20concerns.%0ALink%3A%20http%3A//arxiv.org/abs/2601.21605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAge%2520Matters%253A%2520Analyzing%2520Age-Related%2520Discussions%2520in%2520App%2520Reviews%26entry.906535625%3DShashiwadana%2520Nirmania%2520and%2520Garima%2520Sharma%2520and%2520Hourieh%2520Khalajzadeh%2520and%2520Mojtaba%2520Shahin%26entry.1292438233%3DIn%2520recent%2520years%252C%2520mobile%2520applications%2520have%2520become%2520indispensable%2520tools%2520for%2520managing%2520various%2520aspects%2520of%2520life.%2520From%2520enhancing%2520productivity%2520to%2520providing%2520personalized%2520entertainment%252C%2520mobile%2520apps%2520have%2520revolutionized%2520people%2527s%2520daily%2520routines.%2520Despite%2520this%2520rapid%2520growth%2520and%2520popularity%252C%2520gaps%2520remain%2520in%2520how%2520these%2520apps%2520address%2520the%2520needs%2520of%2520users%2520from%2520different%2520age%2520groups.%2520Users%2520of%2520varying%2520ages%2520face%2520distinct%2520challenges%2520when%2520interacting%2520with%2520mobile%2520apps%252C%2520from%2520younger%2520users%2520dealing%2520with%2520inappropriate%2520content%2520to%2520older%2520users%2520having%2520difficulty%2520with%2520usability%2520due%2520to%2520age-related%2520vision%2520and%2520cognition%2520impairments.%2520Although%2520there%2520have%2520been%2520initiatives%2520to%2520create%2520age-inclusive%2520apps%252C%2520a%2520limited%2520understanding%2520of%2520user%2520perspectives%2520on%2520age-related%2520issues%2520may%2520hinder%2520developers%2520from%2520recognizing%2520specific%2520challenges%2520and%2520implementing%2520effective%2520solutions.%2520In%2520this%2520study%252C%2520we%2520explore%2520age%2520discussions%2520in%2520app%2520reviews%2520to%2520gain%2520insights%2520into%2520how%2520mobile%2520apps%2520should%2520cater%2520to%2520users%2520across%2520different%2520age%2520groups.We%2520manually%2520curated%2520a%2520dataset%2520of%25204%252C163%2520app%2520reviews%2520from%2520the%2520Google%2520Play%2520Store%2520and%2520identified%25201%252C429%2520age-related%2520reviews%2520and%25202%252C734%2520non-age-related%2520reviews.%2520We%2520employed%2520eight%2520machine%2520learning%252C%2520deep%2520learning%252C%2520and%2520large%2520language%2520models%2520to%2520automatically%2520detect%2520age%2520discussions%252C%2520with%2520RoBERTa%2520performing%2520the%2520best%252C%2520achieving%2520a%2520precision%2520of%252092.46%2525.%2520Additionally%252C%2520a%2520qualitative%2520analysis%2520of%2520the%25201%252C429%2520age-related%2520reviews%2520uncovers%2520six%2520dominant%2520themes%2520reflecting%2520user%2520concerns.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.21605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Age%20Matters%3A%20Analyzing%20Age-Related%20Discussions%20in%20App%20Reviews&entry.906535625=Shashiwadana%20Nirmania%20and%20Garima%20Sharma%20and%20Hourieh%20Khalajzadeh%20and%20Mojtaba%20Shahin&entry.1292438233=In%20recent%20years%2C%20mobile%20applications%20have%20become%20indispensable%20tools%20for%20managing%20various%20aspects%20of%20life.%20From%20enhancing%20productivity%20to%20providing%20personalized%20entertainment%2C%20mobile%20apps%20have%20revolutionized%20people%27s%20daily%20routines.%20Despite%20this%20rapid%20growth%20and%20popularity%2C%20gaps%20remain%20in%20how%20these%20apps%20address%20the%20needs%20of%20users%20from%20different%20age%20groups.%20Users%20of%20varying%20ages%20face%20distinct%20challenges%20when%20interacting%20with%20mobile%20apps%2C%20from%20younger%20users%20dealing%20with%20inappropriate%20content%20to%20older%20users%20having%20difficulty%20with%20usability%20due%20to%20age-related%20vision%20and%20cognition%20impairments.%20Although%20there%20have%20been%20initiatives%20to%20create%20age-inclusive%20apps%2C%20a%20limited%20understanding%20of%20user%20perspectives%20on%20age-related%20issues%20may%20hinder%20developers%20from%20recognizing%20specific%20challenges%20and%20implementing%20effective%20solutions.%20In%20this%20study%2C%20we%20explore%20age%20discussions%20in%20app%20reviews%20to%20gain%20insights%20into%20how%20mobile%20apps%20should%20cater%20to%20users%20across%20different%20age%20groups.We%20manually%20curated%20a%20dataset%20of%204%2C163%20app%20reviews%20from%20the%20Google%20Play%20Store%20and%20identified%201%2C429%20age-related%20reviews%20and%202%2C734%20non-age-related%20reviews.%20We%20employed%20eight%20machine%20learning%2C%20deep%20learning%2C%20and%20large%20language%20models%20to%20automatically%20detect%20age%20discussions%2C%20with%20RoBERTa%20performing%20the%20best%2C%20achieving%20a%20precision%20of%2092.46%25.%20Additionally%2C%20a%20qualitative%20analysis%20of%20the%201%2C429%20age-related%20reviews%20uncovers%20six%20dominant%20themes%20reflecting%20user%20concerns.&entry.1838667208=http%3A//arxiv.org/abs/2601.21605v1&entry.124074799=Read"},
{"title": "Thinking Out of Order: When Output Order Stops Reflecting Reasoning Order in Diffusion Language Models", "author": "Longxuan Yu and Yu Fu and Shaorong Zhang and Hui Liu and Mukund Varma T and Greg Ver Steeg and Yue Dong", "abstract": "Autoregressive (AR) language models enforce a fixed left-to-right generation order, creating a fundamental limitation when the required output structure conflicts with natural reasoning (e.g., producing answers before explanations due to presentation or schema constraints). In such cases, AR models must commit to answers before generating intermediate reasoning, and this rigid constraint forces premature commitment. Masked diffusion language models (MDLMs), which iteratively refine all tokens in parallel, offer a way to decouple computation order from output structure. We validate this capability on GSM8K, Math500, and ReasonOrderQA, a benchmark we introduce with controlled difficulty and order-level evaluation. When prompts request answers before reasoning, AR models exhibit large accuracy gaps compared to standard chain-of-thought ordering (up to 67% relative drop), while MDLMs remain stable ($\\leq$14% relative drop), a property we term \"order robustness\". Using ReasonOrderQA, we present evidence that MDLMs achieve order robustness by stabilizing simpler tokens (e.g., reasoning steps) earlier in the diffusion process than complex ones (e.g., final answers), enabling reasoning tokens to stabilize before answer commitment. Finally, we identify failure conditions where this advantage weakens, outlining the limits required for order robustness.", "link": "http://arxiv.org/abs/2601.22035v1", "date": "2026-01-29", "relevancy": 0.9961, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5019}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4995}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20Out%20of%20Order%3A%20When%20Output%20Order%20Stops%20Reflecting%20Reasoning%20Order%20in%20Diffusion%20Language%20Models&body=Title%3A%20Thinking%20Out%20of%20Order%3A%20When%20Output%20Order%20Stops%20Reflecting%20Reasoning%20Order%20in%20Diffusion%20Language%20Models%0AAuthor%3A%20Longxuan%20Yu%20and%20Yu%20Fu%20and%20Shaorong%20Zhang%20and%20Hui%20Liu%20and%20Mukund%20Varma%20T%20and%20Greg%20Ver%20Steeg%20and%20Yue%20Dong%0AAbstract%3A%20Autoregressive%20%28AR%29%20language%20models%20enforce%20a%20fixed%20left-to-right%20generation%20order%2C%20creating%20a%20fundamental%20limitation%20when%20the%20required%20output%20structure%20conflicts%20with%20natural%20reasoning%20%28e.g.%2C%20producing%20answers%20before%20explanations%20due%20to%20presentation%20or%20schema%20constraints%29.%20In%20such%20cases%2C%20AR%20models%20must%20commit%20to%20answers%20before%20generating%20intermediate%20reasoning%2C%20and%20this%20rigid%20constraint%20forces%20premature%20commitment.%20Masked%20diffusion%20language%20models%20%28MDLMs%29%2C%20which%20iteratively%20refine%20all%20tokens%20in%20parallel%2C%20offer%20a%20way%20to%20decouple%20computation%20order%20from%20output%20structure.%20We%20validate%20this%20capability%20on%20GSM8K%2C%20Math500%2C%20and%20ReasonOrderQA%2C%20a%20benchmark%20we%20introduce%20with%20controlled%20difficulty%20and%20order-level%20evaluation.%20When%20prompts%20request%20answers%20before%20reasoning%2C%20AR%20models%20exhibit%20large%20accuracy%20gaps%20compared%20to%20standard%20chain-of-thought%20ordering%20%28up%20to%2067%25%20relative%20drop%29%2C%20while%20MDLMs%20remain%20stable%20%28%24%5Cleq%2414%25%20relative%20drop%29%2C%20a%20property%20we%20term%20%22order%20robustness%22.%20Using%20ReasonOrderQA%2C%20we%20present%20evidence%20that%20MDLMs%20achieve%20order%20robustness%20by%20stabilizing%20simpler%20tokens%20%28e.g.%2C%20reasoning%20steps%29%20earlier%20in%20the%20diffusion%20process%20than%20complex%20ones%20%28e.g.%2C%20final%20answers%29%2C%20enabling%20reasoning%20tokens%20to%20stabilize%20before%20answer%20commitment.%20Finally%2C%20we%20identify%20failure%20conditions%20where%20this%20advantage%20weakens%2C%20outlining%20the%20limits%20required%20for%20order%20robustness.%0ALink%3A%20http%3A//arxiv.org/abs/2601.22035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520Out%2520of%2520Order%253A%2520When%2520Output%2520Order%2520Stops%2520Reflecting%2520Reasoning%2520Order%2520in%2520Diffusion%2520Language%2520Models%26entry.906535625%3DLongxuan%2520Yu%2520and%2520Yu%2520Fu%2520and%2520Shaorong%2520Zhang%2520and%2520Hui%2520Liu%2520and%2520Mukund%2520Varma%2520T%2520and%2520Greg%2520Ver%2520Steeg%2520and%2520Yue%2520Dong%26entry.1292438233%3DAutoregressive%2520%2528AR%2529%2520language%2520models%2520enforce%2520a%2520fixed%2520left-to-right%2520generation%2520order%252C%2520creating%2520a%2520fundamental%2520limitation%2520when%2520the%2520required%2520output%2520structure%2520conflicts%2520with%2520natural%2520reasoning%2520%2528e.g.%252C%2520producing%2520answers%2520before%2520explanations%2520due%2520to%2520presentation%2520or%2520schema%2520constraints%2529.%2520In%2520such%2520cases%252C%2520AR%2520models%2520must%2520commit%2520to%2520answers%2520before%2520generating%2520intermediate%2520reasoning%252C%2520and%2520this%2520rigid%2520constraint%2520forces%2520premature%2520commitment.%2520Masked%2520diffusion%2520language%2520models%2520%2528MDLMs%2529%252C%2520which%2520iteratively%2520refine%2520all%2520tokens%2520in%2520parallel%252C%2520offer%2520a%2520way%2520to%2520decouple%2520computation%2520order%2520from%2520output%2520structure.%2520We%2520validate%2520this%2520capability%2520on%2520GSM8K%252C%2520Math500%252C%2520and%2520ReasonOrderQA%252C%2520a%2520benchmark%2520we%2520introduce%2520with%2520controlled%2520difficulty%2520and%2520order-level%2520evaluation.%2520When%2520prompts%2520request%2520answers%2520before%2520reasoning%252C%2520AR%2520models%2520exhibit%2520large%2520accuracy%2520gaps%2520compared%2520to%2520standard%2520chain-of-thought%2520ordering%2520%2528up%2520to%252067%2525%2520relative%2520drop%2529%252C%2520while%2520MDLMs%2520remain%2520stable%2520%2528%2524%255Cleq%252414%2525%2520relative%2520drop%2529%252C%2520a%2520property%2520we%2520term%2520%2522order%2520robustness%2522.%2520Using%2520ReasonOrderQA%252C%2520we%2520present%2520evidence%2520that%2520MDLMs%2520achieve%2520order%2520robustness%2520by%2520stabilizing%2520simpler%2520tokens%2520%2528e.g.%252C%2520reasoning%2520steps%2529%2520earlier%2520in%2520the%2520diffusion%2520process%2520than%2520complex%2520ones%2520%2528e.g.%252C%2520final%2520answers%2529%252C%2520enabling%2520reasoning%2520tokens%2520to%2520stabilize%2520before%2520answer%2520commitment.%2520Finally%252C%2520we%2520identify%2520failure%2520conditions%2520where%2520this%2520advantage%2520weakens%252C%2520outlining%2520the%2520limits%2520required%2520for%2520order%2520robustness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.22035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20Out%20of%20Order%3A%20When%20Output%20Order%20Stops%20Reflecting%20Reasoning%20Order%20in%20Diffusion%20Language%20Models&entry.906535625=Longxuan%20Yu%20and%20Yu%20Fu%20and%20Shaorong%20Zhang%20and%20Hui%20Liu%20and%20Mukund%20Varma%20T%20and%20Greg%20Ver%20Steeg%20and%20Yue%20Dong&entry.1292438233=Autoregressive%20%28AR%29%20language%20models%20enforce%20a%20fixed%20left-to-right%20generation%20order%2C%20creating%20a%20fundamental%20limitation%20when%20the%20required%20output%20structure%20conflicts%20with%20natural%20reasoning%20%28e.g.%2C%20producing%20answers%20before%20explanations%20due%20to%20presentation%20or%20schema%20constraints%29.%20In%20such%20cases%2C%20AR%20models%20must%20commit%20to%20answers%20before%20generating%20intermediate%20reasoning%2C%20and%20this%20rigid%20constraint%20forces%20premature%20commitment.%20Masked%20diffusion%20language%20models%20%28MDLMs%29%2C%20which%20iteratively%20refine%20all%20tokens%20in%20parallel%2C%20offer%20a%20way%20to%20decouple%20computation%20order%20from%20output%20structure.%20We%20validate%20this%20capability%20on%20GSM8K%2C%20Math500%2C%20and%20ReasonOrderQA%2C%20a%20benchmark%20we%20introduce%20with%20controlled%20difficulty%20and%20order-level%20evaluation.%20When%20prompts%20request%20answers%20before%20reasoning%2C%20AR%20models%20exhibit%20large%20accuracy%20gaps%20compared%20to%20standard%20chain-of-thought%20ordering%20%28up%20to%2067%25%20relative%20drop%29%2C%20while%20MDLMs%20remain%20stable%20%28%24%5Cleq%2414%25%20relative%20drop%29%2C%20a%20property%20we%20term%20%22order%20robustness%22.%20Using%20ReasonOrderQA%2C%20we%20present%20evidence%20that%20MDLMs%20achieve%20order%20robustness%20by%20stabilizing%20simpler%20tokens%20%28e.g.%2C%20reasoning%20steps%29%20earlier%20in%20the%20diffusion%20process%20than%20complex%20ones%20%28e.g.%2C%20final%20answers%29%2C%20enabling%20reasoning%20tokens%20to%20stabilize%20before%20answer%20commitment.%20Finally%2C%20we%20identify%20failure%20conditions%20where%20this%20advantage%20weakens%2C%20outlining%20the%20limits%20required%20for%20order%20robustness.&entry.1838667208=http%3A//arxiv.org/abs/2601.22035v1&entry.124074799=Read"},
{"title": "SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights", "author": "Lorenz K. M\u00fcller and Philippe Bich and Jiawei Zhuang and Ahmet \u00c7elik and Luca Benfenati and Lukas Cavigelli", "abstract": "Post-training quantization has emerged as the most widely used strategy for deploying large language models at low precision. Still, current methods show perplexity degradation at bit-widths less than or equal to 4, partly because representing outliers causes precision issues in parameters that share the same scales as these outliers. This problem is especially pronounced for calibration-free, uniform quantization methods. We introduce SINQ to augment existing post-training quantizers with an additional second-axis scale factor and a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize per-row and per-column variances. We show that this approximates activation-aware quantization by recovering column scales from the weight matrix structure that are predictive of the typical activation magnitudes the matrix received during training. Our method has no interactions between layers and can be trivially applied to new architectures to quantize any linear layer. We evaluate our method on the Qwen3 model family, among others. SINQ reduces the perplexity gap on WikiText2 and C4 by over 50% against uncalibrated uniform quantization baselines, incurs zero to negligible compute overhead, and can be further enhanced by combining it with calibration and non-uniform quantization levels. Code is available at https://github.com/huawei-csl/SINQ.", "link": "http://arxiv.org/abs/2509.22944v4", "date": "2026-01-29", "relevancy": 1.8921, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5088}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.467}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SINQ%3A%20Sinkhorn-Normalized%20Quantization%20for%20Calibration-Free%20Low-Precision%20LLM%20Weights&body=Title%3A%20SINQ%3A%20Sinkhorn-Normalized%20Quantization%20for%20Calibration-Free%20Low-Precision%20LLM%20Weights%0AAuthor%3A%20Lorenz%20K.%20M%C3%BCller%20and%20Philippe%20Bich%20and%20Jiawei%20Zhuang%20and%20Ahmet%20%C3%87elik%20and%20Luca%20Benfenati%20and%20Lukas%20Cavigelli%0AAbstract%3A%20Post-training%20quantization%20has%20emerged%20as%20the%20most%20widely%20used%20strategy%20for%20deploying%20large%20language%20models%20at%20low%20precision.%20Still%2C%20current%20methods%20show%20perplexity%20degradation%20at%20bit-widths%20less%20than%20or%20equal%20to%204%2C%20partly%20because%20representing%20outliers%20causes%20precision%20issues%20in%20parameters%20that%20share%20the%20same%20scales%20as%20these%20outliers.%20This%20problem%20is%20especially%20pronounced%20for%20calibration-free%2C%20uniform%20quantization%20methods.%20We%20introduce%20SINQ%20to%20augment%20existing%20post-training%20quantizers%20with%20an%20additional%20second-axis%20scale%20factor%20and%20a%20fast%20Sinkhorn-Knopp-style%20algorithm%20that%20finds%20scales%20to%20normalize%20per-row%20and%20per-column%20variances.%20We%20show%20that%20this%20approximates%20activation-aware%20quantization%20by%20recovering%20column%20scales%20from%20the%20weight%20matrix%20structure%20that%20are%20predictive%20of%20the%20typical%20activation%20magnitudes%20the%20matrix%20received%20during%20training.%20Our%20method%20has%20no%20interactions%20between%20layers%20and%20can%20be%20trivially%20applied%20to%20new%20architectures%20to%20quantize%20any%20linear%20layer.%20We%20evaluate%20our%20method%20on%20the%20Qwen3%20model%20family%2C%20among%20others.%20SINQ%20reduces%20the%20perplexity%20gap%20on%20WikiText2%20and%20C4%20by%20over%2050%25%20against%20uncalibrated%20uniform%20quantization%20baselines%2C%20incurs%20zero%20to%20negligible%20compute%20overhead%2C%20and%20can%20be%20further%20enhanced%20by%20combining%20it%20with%20calibration%20and%20non-uniform%20quantization%20levels.%20Code%20is%20available%20at%20https%3A//github.com/huawei-csl/SINQ.%0ALink%3A%20http%3A//arxiv.org/abs/2509.22944v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSINQ%253A%2520Sinkhorn-Normalized%2520Quantization%2520for%2520Calibration-Free%2520Low-Precision%2520LLM%2520Weights%26entry.906535625%3DLorenz%2520K.%2520M%25C3%25BCller%2520and%2520Philippe%2520Bich%2520and%2520Jiawei%2520Zhuang%2520and%2520Ahmet%2520%25C3%2587elik%2520and%2520Luca%2520Benfenati%2520and%2520Lukas%2520Cavigelli%26entry.1292438233%3DPost-training%2520quantization%2520has%2520emerged%2520as%2520the%2520most%2520widely%2520used%2520strategy%2520for%2520deploying%2520large%2520language%2520models%2520at%2520low%2520precision.%2520Still%252C%2520current%2520methods%2520show%2520perplexity%2520degradation%2520at%2520bit-widths%2520less%2520than%2520or%2520equal%2520to%25204%252C%2520partly%2520because%2520representing%2520outliers%2520causes%2520precision%2520issues%2520in%2520parameters%2520that%2520share%2520the%2520same%2520scales%2520as%2520these%2520outliers.%2520This%2520problem%2520is%2520especially%2520pronounced%2520for%2520calibration-free%252C%2520uniform%2520quantization%2520methods.%2520We%2520introduce%2520SINQ%2520to%2520augment%2520existing%2520post-training%2520quantizers%2520with%2520an%2520additional%2520second-axis%2520scale%2520factor%2520and%2520a%2520fast%2520Sinkhorn-Knopp-style%2520algorithm%2520that%2520finds%2520scales%2520to%2520normalize%2520per-row%2520and%2520per-column%2520variances.%2520We%2520show%2520that%2520this%2520approximates%2520activation-aware%2520quantization%2520by%2520recovering%2520column%2520scales%2520from%2520the%2520weight%2520matrix%2520structure%2520that%2520are%2520predictive%2520of%2520the%2520typical%2520activation%2520magnitudes%2520the%2520matrix%2520received%2520during%2520training.%2520Our%2520method%2520has%2520no%2520interactions%2520between%2520layers%2520and%2520can%2520be%2520trivially%2520applied%2520to%2520new%2520architectures%2520to%2520quantize%2520any%2520linear%2520layer.%2520We%2520evaluate%2520our%2520method%2520on%2520the%2520Qwen3%2520model%2520family%252C%2520among%2520others.%2520SINQ%2520reduces%2520the%2520perplexity%2520gap%2520on%2520WikiText2%2520and%2520C4%2520by%2520over%252050%2525%2520against%2520uncalibrated%2520uniform%2520quantization%2520baselines%252C%2520incurs%2520zero%2520to%2520negligible%2520compute%2520overhead%252C%2520and%2520can%2520be%2520further%2520enhanced%2520by%2520combining%2520it%2520with%2520calibration%2520and%2520non-uniform%2520quantization%2520levels.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/huawei-csl/SINQ.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22944v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SINQ%3A%20Sinkhorn-Normalized%20Quantization%20for%20Calibration-Free%20Low-Precision%20LLM%20Weights&entry.906535625=Lorenz%20K.%20M%C3%BCller%20and%20Philippe%20Bich%20and%20Jiawei%20Zhuang%20and%20Ahmet%20%C3%87elik%20and%20Luca%20Benfenati%20and%20Lukas%20Cavigelli&entry.1292438233=Post-training%20quantization%20has%20emerged%20as%20the%20most%20widely%20used%20strategy%20for%20deploying%20large%20language%20models%20at%20low%20precision.%20Still%2C%20current%20methods%20show%20perplexity%20degradation%20at%20bit-widths%20less%20than%20or%20equal%20to%204%2C%20partly%20because%20representing%20outliers%20causes%20precision%20issues%20in%20parameters%20that%20share%20the%20same%20scales%20as%20these%20outliers.%20This%20problem%20is%20especially%20pronounced%20for%20calibration-free%2C%20uniform%20quantization%20methods.%20We%20introduce%20SINQ%20to%20augment%20existing%20post-training%20quantizers%20with%20an%20additional%20second-axis%20scale%20factor%20and%20a%20fast%20Sinkhorn-Knopp-style%20algorithm%20that%20finds%20scales%20to%20normalize%20per-row%20and%20per-column%20variances.%20We%20show%20that%20this%20approximates%20activation-aware%20quantization%20by%20recovering%20column%20scales%20from%20the%20weight%20matrix%20structure%20that%20are%20predictive%20of%20the%20typical%20activation%20magnitudes%20the%20matrix%20received%20during%20training.%20Our%20method%20has%20no%20interactions%20between%20layers%20and%20can%20be%20trivially%20applied%20to%20new%20architectures%20to%20quantize%20any%20linear%20layer.%20We%20evaluate%20our%20method%20on%20the%20Qwen3%20model%20family%2C%20among%20others.%20SINQ%20reduces%20the%20perplexity%20gap%20on%20WikiText2%20and%20C4%20by%20over%2050%25%20against%20uncalibrated%20uniform%20quantization%20baselines%2C%20incurs%20zero%20to%20negligible%20compute%20overhead%2C%20and%20can%20be%20further%20enhanced%20by%20combining%20it%20with%20calibration%20and%20non-uniform%20quantization%20levels.%20Code%20is%20available%20at%20https%3A//github.com/huawei-csl/SINQ.&entry.1838667208=http%3A//arxiv.org/abs/2509.22944v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


