<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250812.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Fancy123: One Image to High-Quality 3D Mesh Generation via Plug-and-Play\n  Deformation", "author": "Qiao Yu and Xianzhi Li and Yuan Tang and Xu Han and Long Hu and Yixue Hao and Min Chen", "abstract": "  Generating 3D meshes from a single image is an important but ill-posed task.\nExisting methods mainly adopt 2D multiview diffusion models to generate\nintermediate multiview images, and use the Large Reconstruction Model (LRM) to\ncreate the final meshes. However, the multiview images exhibit local\ninconsistencies, and the meshes often lack fidelity to the input image or look\nblurry. We propose Fancy123, featuring two enhancement modules and an\nunprojection operation to address the above three issues, respectively. The\nappearance enhancement module deforms the 2D multiview images to realign\nmisaligned pixels for better multiview consistency. The fidelity enhancement\nmodule deforms the 3D mesh to match the input image. The unprojection of the\ninput image and deformed multiview images onto LRM's generated mesh ensures\nhigh clarity, discarding LRM's predicted blurry-looking mesh colors. Extensive\nqualitative and quantitative experiments verify Fancy123's SoTA performance\nwith significant improvement. Also, the two enhancement modules are\nplug-and-play and work at inference time, allowing seamless integration into\nvarious existing single-image-to-3D methods. Code at:\nhttps://github.com/YuQiao0303/Fancy123\n", "link": "http://arxiv.org/abs/2411.16185v2", "date": "2025-08-12", "relevancy": 3.3765, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6931}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6931}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fancy123%3A%20One%20Image%20to%20High-Quality%203D%20Mesh%20Generation%20via%20Plug-and-Play%0A%20%20Deformation&body=Title%3A%20Fancy123%3A%20One%20Image%20to%20High-Quality%203D%20Mesh%20Generation%20via%20Plug-and-Play%0A%20%20Deformation%0AAuthor%3A%20Qiao%20Yu%20and%20Xianzhi%20Li%20and%20Yuan%20Tang%20and%20Xu%20Han%20and%20Long%20Hu%20and%20Yixue%20Hao%20and%20Min%20Chen%0AAbstract%3A%20%20%20Generating%203D%20meshes%20from%20a%20single%20image%20is%20an%20important%20but%20ill-posed%20task.%0AExisting%20methods%20mainly%20adopt%202D%20multiview%20diffusion%20models%20to%20generate%0Aintermediate%20multiview%20images%2C%20and%20use%20the%20Large%20Reconstruction%20Model%20%28LRM%29%20to%0Acreate%20the%20final%20meshes.%20However%2C%20the%20multiview%20images%20exhibit%20local%0Ainconsistencies%2C%20and%20the%20meshes%20often%20lack%20fidelity%20to%20the%20input%20image%20or%20look%0Ablurry.%20We%20propose%20Fancy123%2C%20featuring%20two%20enhancement%20modules%20and%20an%0Aunprojection%20operation%20to%20address%20the%20above%20three%20issues%2C%20respectively.%20The%0Aappearance%20enhancement%20module%20deforms%20the%202D%20multiview%20images%20to%20realign%0Amisaligned%20pixels%20for%20better%20multiview%20consistency.%20The%20fidelity%20enhancement%0Amodule%20deforms%20the%203D%20mesh%20to%20match%20the%20input%20image.%20The%20unprojection%20of%20the%0Ainput%20image%20and%20deformed%20multiview%20images%20onto%20LRM%27s%20generated%20mesh%20ensures%0Ahigh%20clarity%2C%20discarding%20LRM%27s%20predicted%20blurry-looking%20mesh%20colors.%20Extensive%0Aqualitative%20and%20quantitative%20experiments%20verify%20Fancy123%27s%20SoTA%20performance%0Awith%20significant%20improvement.%20Also%2C%20the%20two%20enhancement%20modules%20are%0Aplug-and-play%20and%20work%20at%20inference%20time%2C%20allowing%20seamless%20integration%20into%0Avarious%20existing%20single-image-to-3D%20methods.%20Code%20at%3A%0Ahttps%3A//github.com/YuQiao0303/Fancy123%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16185v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFancy123%253A%2520One%2520Image%2520to%2520High-Quality%25203D%2520Mesh%2520Generation%2520via%2520Plug-and-Play%250A%2520%2520Deformation%26entry.906535625%3DQiao%2520Yu%2520and%2520Xianzhi%2520Li%2520and%2520Yuan%2520Tang%2520and%2520Xu%2520Han%2520and%2520Long%2520Hu%2520and%2520Yixue%2520Hao%2520and%2520Min%2520Chen%26entry.1292438233%3D%2520%2520Generating%25203D%2520meshes%2520from%2520a%2520single%2520image%2520is%2520an%2520important%2520but%2520ill-posed%2520task.%250AExisting%2520methods%2520mainly%2520adopt%25202D%2520multiview%2520diffusion%2520models%2520to%2520generate%250Aintermediate%2520multiview%2520images%252C%2520and%2520use%2520the%2520Large%2520Reconstruction%2520Model%2520%2528LRM%2529%2520to%250Acreate%2520the%2520final%2520meshes.%2520However%252C%2520the%2520multiview%2520images%2520exhibit%2520local%250Ainconsistencies%252C%2520and%2520the%2520meshes%2520often%2520lack%2520fidelity%2520to%2520the%2520input%2520image%2520or%2520look%250Ablurry.%2520We%2520propose%2520Fancy123%252C%2520featuring%2520two%2520enhancement%2520modules%2520and%2520an%250Aunprojection%2520operation%2520to%2520address%2520the%2520above%2520three%2520issues%252C%2520respectively.%2520The%250Aappearance%2520enhancement%2520module%2520deforms%2520the%25202D%2520multiview%2520images%2520to%2520realign%250Amisaligned%2520pixels%2520for%2520better%2520multiview%2520consistency.%2520The%2520fidelity%2520enhancement%250Amodule%2520deforms%2520the%25203D%2520mesh%2520to%2520match%2520the%2520input%2520image.%2520The%2520unprojection%2520of%2520the%250Ainput%2520image%2520and%2520deformed%2520multiview%2520images%2520onto%2520LRM%2527s%2520generated%2520mesh%2520ensures%250Ahigh%2520clarity%252C%2520discarding%2520LRM%2527s%2520predicted%2520blurry-looking%2520mesh%2520colors.%2520Extensive%250Aqualitative%2520and%2520quantitative%2520experiments%2520verify%2520Fancy123%2527s%2520SoTA%2520performance%250Awith%2520significant%2520improvement.%2520Also%252C%2520the%2520two%2520enhancement%2520modules%2520are%250Aplug-and-play%2520and%2520work%2520at%2520inference%2520time%252C%2520allowing%2520seamless%2520integration%2520into%250Avarious%2520existing%2520single-image-to-3D%2520methods.%2520Code%2520at%253A%250Ahttps%253A//github.com/YuQiao0303/Fancy123%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16185v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fancy123%3A%20One%20Image%20to%20High-Quality%203D%20Mesh%20Generation%20via%20Plug-and-Play%0A%20%20Deformation&entry.906535625=Qiao%20Yu%20and%20Xianzhi%20Li%20and%20Yuan%20Tang%20and%20Xu%20Han%20and%20Long%20Hu%20and%20Yixue%20Hao%20and%20Min%20Chen&entry.1292438233=%20%20Generating%203D%20meshes%20from%20a%20single%20image%20is%20an%20important%20but%20ill-posed%20task.%0AExisting%20methods%20mainly%20adopt%202D%20multiview%20diffusion%20models%20to%20generate%0Aintermediate%20multiview%20images%2C%20and%20use%20the%20Large%20Reconstruction%20Model%20%28LRM%29%20to%0Acreate%20the%20final%20meshes.%20However%2C%20the%20multiview%20images%20exhibit%20local%0Ainconsistencies%2C%20and%20the%20meshes%20often%20lack%20fidelity%20to%20the%20input%20image%20or%20look%0Ablurry.%20We%20propose%20Fancy123%2C%20featuring%20two%20enhancement%20modules%20and%20an%0Aunprojection%20operation%20to%20address%20the%20above%20three%20issues%2C%20respectively.%20The%0Aappearance%20enhancement%20module%20deforms%20the%202D%20multiview%20images%20to%20realign%0Amisaligned%20pixels%20for%20better%20multiview%20consistency.%20The%20fidelity%20enhancement%0Amodule%20deforms%20the%203D%20mesh%20to%20match%20the%20input%20image.%20The%20unprojection%20of%20the%0Ainput%20image%20and%20deformed%20multiview%20images%20onto%20LRM%27s%20generated%20mesh%20ensures%0Ahigh%20clarity%2C%20discarding%20LRM%27s%20predicted%20blurry-looking%20mesh%20colors.%20Extensive%0Aqualitative%20and%20quantitative%20experiments%20verify%20Fancy123%27s%20SoTA%20performance%0Awith%20significant%20improvement.%20Also%2C%20the%20two%20enhancement%20modules%20are%0Aplug-and-play%20and%20work%20at%20inference%20time%2C%20allowing%20seamless%20integration%20into%0Avarious%20existing%20single-image-to-3D%20methods.%20Code%20at%3A%0Ahttps%3A//github.com/YuQiao0303/Fancy123%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16185v2&entry.124074799=Read"},
{"title": "VLM-3D:End-to-End Vision-Language Models for Open-World 3D Perception", "author": "Fuhao Chang and Shuxin Li and Yabei Li and Lei He", "abstract": "  Open-set perception in complex traffic environments poses a critical\nchallenge for autonomous driving systems, particularly in identifying\npreviously unseen object categories, which is vital for ensuring safety. Visual\nLanguage Models (VLMs), with their rich world knowledge and strong semantic\nreasoning capabilities, offer new possibilities for addressing this task.\nHowever, existing approaches typically leverage VLMs to extract visual features\nand couple them with traditional object detectors, resulting in multi-stage\nerror propagation that hinders perception accuracy. To overcome this\nlimitation, we propose VLM-3D, the first end-to-end framework that enables VLMs\nto perform 3D geometric perception in autonomous driving scenarios. VLM-3D\nincorporates Low-Rank Adaptation (LoRA) to efficiently adapt VLMs to driving\ntasks with minimal computational overhead, and introduces a joint\nsemantic-geometric loss design: token-level semantic loss is applied during\nearly training to ensure stable convergence, while 3D IoU loss is introduced in\nlater stages to refine the accuracy of 3D bounding box predictions. Evaluations\non the nuScenes dataset demonstrate that the proposed joint semantic-geometric\nloss in VLM-3D leads to a 12.8% improvement in perception accuracy, fully\nvalidating the effectiveness and advancement of our method.\n", "link": "http://arxiv.org/abs/2508.09061v1", "date": "2025-08-12", "relevancy": 3.3086, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6864}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLM-3D%3AEnd-to-End%20Vision-Language%20Models%20for%20Open-World%203D%20Perception&body=Title%3A%20VLM-3D%3AEnd-to-End%20Vision-Language%20Models%20for%20Open-World%203D%20Perception%0AAuthor%3A%20Fuhao%20Chang%20and%20Shuxin%20Li%20and%20Yabei%20Li%20and%20Lei%20He%0AAbstract%3A%20%20%20Open-set%20perception%20in%20complex%20traffic%20environments%20poses%20a%20critical%0Achallenge%20for%20autonomous%20driving%20systems%2C%20particularly%20in%20identifying%0Apreviously%20unseen%20object%20categories%2C%20which%20is%20vital%20for%20ensuring%20safety.%20Visual%0ALanguage%20Models%20%28VLMs%29%2C%20with%20their%20rich%20world%20knowledge%20and%20strong%20semantic%0Areasoning%20capabilities%2C%20offer%20new%20possibilities%20for%20addressing%20this%20task.%0AHowever%2C%20existing%20approaches%20typically%20leverage%20VLMs%20to%20extract%20visual%20features%0Aand%20couple%20them%20with%20traditional%20object%20detectors%2C%20resulting%20in%20multi-stage%0Aerror%20propagation%20that%20hinders%20perception%20accuracy.%20To%20overcome%20this%0Alimitation%2C%20we%20propose%20VLM-3D%2C%20the%20first%20end-to-end%20framework%20that%20enables%20VLMs%0Ato%20perform%203D%20geometric%20perception%20in%20autonomous%20driving%20scenarios.%20VLM-3D%0Aincorporates%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20efficiently%20adapt%20VLMs%20to%20driving%0Atasks%20with%20minimal%20computational%20overhead%2C%20and%20introduces%20a%20joint%0Asemantic-geometric%20loss%20design%3A%20token-level%20semantic%20loss%20is%20applied%20during%0Aearly%20training%20to%20ensure%20stable%20convergence%2C%20while%203D%20IoU%20loss%20is%20introduced%20in%0Alater%20stages%20to%20refine%20the%20accuracy%20of%203D%20bounding%20box%20predictions.%20Evaluations%0Aon%20the%20nuScenes%20dataset%20demonstrate%20that%20the%20proposed%20joint%20semantic-geometric%0Aloss%20in%20VLM-3D%20leads%20to%20a%2012.8%25%20improvement%20in%20perception%20accuracy%2C%20fully%0Avalidating%20the%20effectiveness%20and%20advancement%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLM-3D%253AEnd-to-End%2520Vision-Language%2520Models%2520for%2520Open-World%25203D%2520Perception%26entry.906535625%3DFuhao%2520Chang%2520and%2520Shuxin%2520Li%2520and%2520Yabei%2520Li%2520and%2520Lei%2520He%26entry.1292438233%3D%2520%2520Open-set%2520perception%2520in%2520complex%2520traffic%2520environments%2520poses%2520a%2520critical%250Achallenge%2520for%2520autonomous%2520driving%2520systems%252C%2520particularly%2520in%2520identifying%250Apreviously%2520unseen%2520object%2520categories%252C%2520which%2520is%2520vital%2520for%2520ensuring%2520safety.%2520Visual%250ALanguage%2520Models%2520%2528VLMs%2529%252C%2520with%2520their%2520rich%2520world%2520knowledge%2520and%2520strong%2520semantic%250Areasoning%2520capabilities%252C%2520offer%2520new%2520possibilities%2520for%2520addressing%2520this%2520task.%250AHowever%252C%2520existing%2520approaches%2520typically%2520leverage%2520VLMs%2520to%2520extract%2520visual%2520features%250Aand%2520couple%2520them%2520with%2520traditional%2520object%2520detectors%252C%2520resulting%2520in%2520multi-stage%250Aerror%2520propagation%2520that%2520hinders%2520perception%2520accuracy.%2520To%2520overcome%2520this%250Alimitation%252C%2520we%2520propose%2520VLM-3D%252C%2520the%2520first%2520end-to-end%2520framework%2520that%2520enables%2520VLMs%250Ato%2520perform%25203D%2520geometric%2520perception%2520in%2520autonomous%2520driving%2520scenarios.%2520VLM-3D%250Aincorporates%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520to%2520efficiently%2520adapt%2520VLMs%2520to%2520driving%250Atasks%2520with%2520minimal%2520computational%2520overhead%252C%2520and%2520introduces%2520a%2520joint%250Asemantic-geometric%2520loss%2520design%253A%2520token-level%2520semantic%2520loss%2520is%2520applied%2520during%250Aearly%2520training%2520to%2520ensure%2520stable%2520convergence%252C%2520while%25203D%2520IoU%2520loss%2520is%2520introduced%2520in%250Alater%2520stages%2520to%2520refine%2520the%2520accuracy%2520of%25203D%2520bounding%2520box%2520predictions.%2520Evaluations%250Aon%2520the%2520nuScenes%2520dataset%2520demonstrate%2520that%2520the%2520proposed%2520joint%2520semantic-geometric%250Aloss%2520in%2520VLM-3D%2520leads%2520to%2520a%252012.8%2525%2520improvement%2520in%2520perception%2520accuracy%252C%2520fully%250Avalidating%2520the%2520effectiveness%2520and%2520advancement%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLM-3D%3AEnd-to-End%20Vision-Language%20Models%20for%20Open-World%203D%20Perception&entry.906535625=Fuhao%20Chang%20and%20Shuxin%20Li%20and%20Yabei%20Li%20and%20Lei%20He&entry.1292438233=%20%20Open-set%20perception%20in%20complex%20traffic%20environments%20poses%20a%20critical%0Achallenge%20for%20autonomous%20driving%20systems%2C%20particularly%20in%20identifying%0Apreviously%20unseen%20object%20categories%2C%20which%20is%20vital%20for%20ensuring%20safety.%20Visual%0ALanguage%20Models%20%28VLMs%29%2C%20with%20their%20rich%20world%20knowledge%20and%20strong%20semantic%0Areasoning%20capabilities%2C%20offer%20new%20possibilities%20for%20addressing%20this%20task.%0AHowever%2C%20existing%20approaches%20typically%20leverage%20VLMs%20to%20extract%20visual%20features%0Aand%20couple%20them%20with%20traditional%20object%20detectors%2C%20resulting%20in%20multi-stage%0Aerror%20propagation%20that%20hinders%20perception%20accuracy.%20To%20overcome%20this%0Alimitation%2C%20we%20propose%20VLM-3D%2C%20the%20first%20end-to-end%20framework%20that%20enables%20VLMs%0Ato%20perform%203D%20geometric%20perception%20in%20autonomous%20driving%20scenarios.%20VLM-3D%0Aincorporates%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20efficiently%20adapt%20VLMs%20to%20driving%0Atasks%20with%20minimal%20computational%20overhead%2C%20and%20introduces%20a%20joint%0Asemantic-geometric%20loss%20design%3A%20token-level%20semantic%20loss%20is%20applied%20during%0Aearly%20training%20to%20ensure%20stable%20convergence%2C%20while%203D%20IoU%20loss%20is%20introduced%20in%0Alater%20stages%20to%20refine%20the%20accuracy%20of%203D%20bounding%20box%20predictions.%20Evaluations%0Aon%20the%20nuScenes%20dataset%20demonstrate%20that%20the%20proposed%20joint%20semantic-geometric%0Aloss%20in%20VLM-3D%20leads%20to%20a%2012.8%25%20improvement%20in%20perception%20accuracy%2C%20fully%0Avalidating%20the%20effectiveness%20and%20advancement%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09061v1&entry.124074799=Read"},
{"title": "3D Human Mesh Estimation from Single View RGBD", "author": "Ozhan Suat and Bedirhan Uguz and Batuhan Karagoz and Muhammed Can Keles and Emre Akbas", "abstract": "  Despite significant progress in 3D human mesh estimation from RGB images;\nRGBD cameras, offering additional depth data, remain underutilized. In this\npaper, we present a method for accurate 3D human mesh estimation from a single\nRGBD view, leveraging the affordability and widespread adoption of RGBD cameras\nfor real-world applications. A fully supervised approach for this problem,\nrequires a dataset with RGBD image and 3D mesh label pairs. However, collecting\nsuch a dataset is costly and challenging, hence, existing datasets are small,\nand limited in pose and shape diversity. To overcome this data scarcity, we\nleverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D\nmeshes from the body models found in MoCap datasets, and create partial,\nsingle-view versions of them by projection to a virtual camera. This simulates\nthe depth data provided by an RGBD camera from a single viewpoint. Then, we\ntrain a masked autoencoder to complete the partial, single-view mesh. During\ninference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',\nmatches the depth values coming from the sensor to vertices of a template human\nmesh, which creates a partial, single-view mesh. We effectively recover parts\nof the 3D human body mesh model that are not visible, resulting in a full body\nmesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL\nand CAPE datasets, respectively; outperforming existing methods that use\nfull-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE\ndataset, outperforming a recently published RGB based method by 18.4 mm,\nhighlighting the usefulness of depth data. Code will be released.\n", "link": "http://arxiv.org/abs/2508.08178v2", "date": "2025-08-12", "relevancy": 3.1873, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6663}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6396}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Human%20Mesh%20Estimation%20from%20Single%20View%20RGBD&body=Title%3A%203D%20Human%20Mesh%20Estimation%20from%20Single%20View%20RGBD%0AAuthor%3A%20Ozhan%20Suat%20and%20Bedirhan%20Uguz%20and%20Batuhan%20Karagoz%20and%20Muhammed%20Can%20Keles%20and%20Emre%20Akbas%0AAbstract%3A%20%20%20Despite%20significant%20progress%20in%203D%20human%20mesh%20estimation%20from%20RGB%20images%3B%0ARGBD%20cameras%2C%20offering%20additional%20depth%20data%2C%20remain%20underutilized.%20In%20this%0Apaper%2C%20we%20present%20a%20method%20for%20accurate%203D%20human%20mesh%20estimation%20from%20a%20single%0ARGBD%20view%2C%20leveraging%20the%20affordability%20and%20widespread%20adoption%20of%20RGBD%20cameras%0Afor%20real-world%20applications.%20A%20fully%20supervised%20approach%20for%20this%20problem%2C%0Arequires%20a%20dataset%20with%20RGBD%20image%20and%203D%20mesh%20label%20pairs.%20However%2C%20collecting%0Asuch%20a%20dataset%20is%20costly%20and%20challenging%2C%20hence%2C%20existing%20datasets%20are%20small%2C%0Aand%20limited%20in%20pose%20and%20shape%20diversity.%20To%20overcome%20this%20data%20scarcity%2C%20we%0Aleverage%20existing%20Motion%20Capture%20%28MoCap%29%20datasets.%20We%20first%20obtain%20complete%203D%0Ameshes%20from%20the%20body%20models%20found%20in%20MoCap%20datasets%2C%20and%20create%20partial%2C%0Asingle-view%20versions%20of%20them%20by%20projection%20to%20a%20virtual%20camera.%20This%20simulates%0Athe%20depth%20data%20provided%20by%20an%20RGBD%20camera%20from%20a%20single%20viewpoint.%20Then%2C%20we%0Atrain%20a%20masked%20autoencoder%20to%20complete%20the%20partial%2C%20single-view%20mesh.%20During%0Ainference%2C%20our%20method%2C%20which%20we%20name%20as%20M%24%5E3%24%20for%20%60%60Masked%20Mesh%20Modeling%27%27%2C%0Amatches%20the%20depth%20values%20coming%20from%20the%20sensor%20to%20vertices%20of%20a%20template%20human%0Amesh%2C%20which%20creates%20a%20partial%2C%20single-view%20mesh.%20We%20effectively%20recover%20parts%0Aof%20the%203D%20human%20body%20mesh%20model%20that%20are%20not%20visible%2C%20resulting%20in%20a%20full%20body%0Amesh.%20M%24%5E3%24%20achieves%2016.8%20mm%20and%2022.0%20mm%20per-vertex-error%20%28PVE%29%20on%20the%20SURREAL%0Aand%20CAPE%20datasets%2C%20respectively%3B%20outperforming%20existing%20methods%20that%20use%0Afull-body%20point%20clouds%20as%20input.%20We%20obtain%20a%20competitive%2070.9%20PVE%20on%20the%20BEHAVE%0Adataset%2C%20outperforming%20a%20recently%20published%20RGB%20based%20method%20by%2018.4%20mm%2C%0Ahighlighting%20the%20usefulness%20of%20depth%20data.%20Code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08178v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Human%2520Mesh%2520Estimation%2520from%2520Single%2520View%2520RGBD%26entry.906535625%3DOzhan%2520Suat%2520and%2520Bedirhan%2520Uguz%2520and%2520Batuhan%2520Karagoz%2520and%2520Muhammed%2520Can%2520Keles%2520and%2520Emre%2520Akbas%26entry.1292438233%3D%2520%2520Despite%2520significant%2520progress%2520in%25203D%2520human%2520mesh%2520estimation%2520from%2520RGB%2520images%253B%250ARGBD%2520cameras%252C%2520offering%2520additional%2520depth%2520data%252C%2520remain%2520underutilized.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520method%2520for%2520accurate%25203D%2520human%2520mesh%2520estimation%2520from%2520a%2520single%250ARGBD%2520view%252C%2520leveraging%2520the%2520affordability%2520and%2520widespread%2520adoption%2520of%2520RGBD%2520cameras%250Afor%2520real-world%2520applications.%2520A%2520fully%2520supervised%2520approach%2520for%2520this%2520problem%252C%250Arequires%2520a%2520dataset%2520with%2520RGBD%2520image%2520and%25203D%2520mesh%2520label%2520pairs.%2520However%252C%2520collecting%250Asuch%2520a%2520dataset%2520is%2520costly%2520and%2520challenging%252C%2520hence%252C%2520existing%2520datasets%2520are%2520small%252C%250Aand%2520limited%2520in%2520pose%2520and%2520shape%2520diversity.%2520To%2520overcome%2520this%2520data%2520scarcity%252C%2520we%250Aleverage%2520existing%2520Motion%2520Capture%2520%2528MoCap%2529%2520datasets.%2520We%2520first%2520obtain%2520complete%25203D%250Ameshes%2520from%2520the%2520body%2520models%2520found%2520in%2520MoCap%2520datasets%252C%2520and%2520create%2520partial%252C%250Asingle-view%2520versions%2520of%2520them%2520by%2520projection%2520to%2520a%2520virtual%2520camera.%2520This%2520simulates%250Athe%2520depth%2520data%2520provided%2520by%2520an%2520RGBD%2520camera%2520from%2520a%2520single%2520viewpoint.%2520Then%252C%2520we%250Atrain%2520a%2520masked%2520autoencoder%2520to%2520complete%2520the%2520partial%252C%2520single-view%2520mesh.%2520During%250Ainference%252C%2520our%2520method%252C%2520which%2520we%2520name%2520as%2520M%2524%255E3%2524%2520for%2520%2560%2560Masked%2520Mesh%2520Modeling%2527%2527%252C%250Amatches%2520the%2520depth%2520values%2520coming%2520from%2520the%2520sensor%2520to%2520vertices%2520of%2520a%2520template%2520human%250Amesh%252C%2520which%2520creates%2520a%2520partial%252C%2520single-view%2520mesh.%2520We%2520effectively%2520recover%2520parts%250Aof%2520the%25203D%2520human%2520body%2520mesh%2520model%2520that%2520are%2520not%2520visible%252C%2520resulting%2520in%2520a%2520full%2520body%250Amesh.%2520M%2524%255E3%2524%2520achieves%252016.8%2520mm%2520and%252022.0%2520mm%2520per-vertex-error%2520%2528PVE%2529%2520on%2520the%2520SURREAL%250Aand%2520CAPE%2520datasets%252C%2520respectively%253B%2520outperforming%2520existing%2520methods%2520that%2520use%250Afull-body%2520point%2520clouds%2520as%2520input.%2520We%2520obtain%2520a%2520competitive%252070.9%2520PVE%2520on%2520the%2520BEHAVE%250Adataset%252C%2520outperforming%2520a%2520recently%2520published%2520RGB%2520based%2520method%2520by%252018.4%2520mm%252C%250Ahighlighting%2520the%2520usefulness%2520of%2520depth%2520data.%2520Code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08178v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Human%20Mesh%20Estimation%20from%20Single%20View%20RGBD&entry.906535625=Ozhan%20Suat%20and%20Bedirhan%20Uguz%20and%20Batuhan%20Karagoz%20and%20Muhammed%20Can%20Keles%20and%20Emre%20Akbas&entry.1292438233=%20%20Despite%20significant%20progress%20in%203D%20human%20mesh%20estimation%20from%20RGB%20images%3B%0ARGBD%20cameras%2C%20offering%20additional%20depth%20data%2C%20remain%20underutilized.%20In%20this%0Apaper%2C%20we%20present%20a%20method%20for%20accurate%203D%20human%20mesh%20estimation%20from%20a%20single%0ARGBD%20view%2C%20leveraging%20the%20affordability%20and%20widespread%20adoption%20of%20RGBD%20cameras%0Afor%20real-world%20applications.%20A%20fully%20supervised%20approach%20for%20this%20problem%2C%0Arequires%20a%20dataset%20with%20RGBD%20image%20and%203D%20mesh%20label%20pairs.%20However%2C%20collecting%0Asuch%20a%20dataset%20is%20costly%20and%20challenging%2C%20hence%2C%20existing%20datasets%20are%20small%2C%0Aand%20limited%20in%20pose%20and%20shape%20diversity.%20To%20overcome%20this%20data%20scarcity%2C%20we%0Aleverage%20existing%20Motion%20Capture%20%28MoCap%29%20datasets.%20We%20first%20obtain%20complete%203D%0Ameshes%20from%20the%20body%20models%20found%20in%20MoCap%20datasets%2C%20and%20create%20partial%2C%0Asingle-view%20versions%20of%20them%20by%20projection%20to%20a%20virtual%20camera.%20This%20simulates%0Athe%20depth%20data%20provided%20by%20an%20RGBD%20camera%20from%20a%20single%20viewpoint.%20Then%2C%20we%0Atrain%20a%20masked%20autoencoder%20to%20complete%20the%20partial%2C%20single-view%20mesh.%20During%0Ainference%2C%20our%20method%2C%20which%20we%20name%20as%20M%24%5E3%24%20for%20%60%60Masked%20Mesh%20Modeling%27%27%2C%0Amatches%20the%20depth%20values%20coming%20from%20the%20sensor%20to%20vertices%20of%20a%20template%20human%0Amesh%2C%20which%20creates%20a%20partial%2C%20single-view%20mesh.%20We%20effectively%20recover%20parts%0Aof%20the%203D%20human%20body%20mesh%20model%20that%20are%20not%20visible%2C%20resulting%20in%20a%20full%20body%0Amesh.%20M%24%5E3%24%20achieves%2016.8%20mm%20and%2022.0%20mm%20per-vertex-error%20%28PVE%29%20on%20the%20SURREAL%0Aand%20CAPE%20datasets%2C%20respectively%3B%20outperforming%20existing%20methods%20that%20use%0Afull-body%20point%20clouds%20as%20input.%20We%20obtain%20a%20competitive%2070.9%20PVE%20on%20the%20BEHAVE%0Adataset%2C%20outperforming%20a%20recently%20published%20RGB%20based%20method%20by%2018.4%20mm%2C%0Ahighlighting%20the%20usefulness%20of%20depth%20data.%20Code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08178v2&entry.124074799=Read"},
{"title": "Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding", "author": "Maxim A. Patratskiy and Alexey K. Kovalev and Aleksandr I. Panov", "abstract": "  Vision-Language-Action models have demonstrated remarkable capabilities in\npredicting agent movements within virtual environments and real-world scenarios\nbased on visual observations and textual instructions. Although recent research\nhas focused on enhancing spatial and temporal understanding independently, this\npaper presents a novel approach that integrates both aspects through visual\nprompting. We introduce a method that projects visual traces of key points from\nobservations onto depth maps, enabling models to capture both spatial and\ntemporal information simultaneously. The experiments in SimplerEnv show that\nthe mean number of tasks successfully solved increased for 4% compared to\nSpatialVLA and 19% compared to TraceVLA. Furthermore, we show that this\nenhancement can be achieved with minimal training data, making it particularly\nvaluable for real-world applications where data collection is challenging. The\nproject page is available at https://ampiromax.github.io/ST-VLA.\n", "link": "http://arxiv.org/abs/2508.09032v1", "date": "2025-08-12", "relevancy": 2.9186, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5874}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5874}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20Traces%3A%20Enhancing%20VLA%20Models%20with%20Spatial-Temporal%20Understanding&body=Title%3A%20Spatial%20Traces%3A%20Enhancing%20VLA%20Models%20with%20Spatial-Temporal%20Understanding%0AAuthor%3A%20Maxim%20A.%20Patratskiy%20and%20Alexey%20K.%20Kovalev%20and%20Aleksandr%20I.%20Panov%0AAbstract%3A%20%20%20Vision-Language-Action%20models%20have%20demonstrated%20remarkable%20capabilities%20in%0Apredicting%20agent%20movements%20within%20virtual%20environments%20and%20real-world%20scenarios%0Abased%20on%20visual%20observations%20and%20textual%20instructions.%20Although%20recent%20research%0Ahas%20focused%20on%20enhancing%20spatial%20and%20temporal%20understanding%20independently%2C%20this%0Apaper%20presents%20a%20novel%20approach%20that%20integrates%20both%20aspects%20through%20visual%0Aprompting.%20We%20introduce%20a%20method%20that%20projects%20visual%20traces%20of%20key%20points%20from%0Aobservations%20onto%20depth%20maps%2C%20enabling%20models%20to%20capture%20both%20spatial%20and%0Atemporal%20information%20simultaneously.%20The%20experiments%20in%20SimplerEnv%20show%20that%0Athe%20mean%20number%20of%20tasks%20successfully%20solved%20increased%20for%204%25%20compared%20to%0ASpatialVLA%20and%2019%25%20compared%20to%20TraceVLA.%20Furthermore%2C%20we%20show%20that%20this%0Aenhancement%20can%20be%20achieved%20with%20minimal%20training%20data%2C%20making%20it%20particularly%0Avaluable%20for%20real-world%20applications%20where%20data%20collection%20is%20challenging.%20The%0Aproject%20page%20is%20available%20at%20https%3A//ampiromax.github.io/ST-VLA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520Traces%253A%2520Enhancing%2520VLA%2520Models%2520with%2520Spatial-Temporal%2520Understanding%26entry.906535625%3DMaxim%2520A.%2520Patratskiy%2520and%2520Alexey%2520K.%2520Kovalev%2520and%2520Aleksandr%2520I.%2520Panov%26entry.1292438233%3D%2520%2520Vision-Language-Action%2520models%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%250Apredicting%2520agent%2520movements%2520within%2520virtual%2520environments%2520and%2520real-world%2520scenarios%250Abased%2520on%2520visual%2520observations%2520and%2520textual%2520instructions.%2520Although%2520recent%2520research%250Ahas%2520focused%2520on%2520enhancing%2520spatial%2520and%2520temporal%2520understanding%2520independently%252C%2520this%250Apaper%2520presents%2520a%2520novel%2520approach%2520that%2520integrates%2520both%2520aspects%2520through%2520visual%250Aprompting.%2520We%2520introduce%2520a%2520method%2520that%2520projects%2520visual%2520traces%2520of%2520key%2520points%2520from%250Aobservations%2520onto%2520depth%2520maps%252C%2520enabling%2520models%2520to%2520capture%2520both%2520spatial%2520and%250Atemporal%2520information%2520simultaneously.%2520The%2520experiments%2520in%2520SimplerEnv%2520show%2520that%250Athe%2520mean%2520number%2520of%2520tasks%2520successfully%2520solved%2520increased%2520for%25204%2525%2520compared%2520to%250ASpatialVLA%2520and%252019%2525%2520compared%2520to%2520TraceVLA.%2520Furthermore%252C%2520we%2520show%2520that%2520this%250Aenhancement%2520can%2520be%2520achieved%2520with%2520minimal%2520training%2520data%252C%2520making%2520it%2520particularly%250Avaluable%2520for%2520real-world%2520applications%2520where%2520data%2520collection%2520is%2520challenging.%2520The%250Aproject%2520page%2520is%2520available%2520at%2520https%253A//ampiromax.github.io/ST-VLA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Traces%3A%20Enhancing%20VLA%20Models%20with%20Spatial-Temporal%20Understanding&entry.906535625=Maxim%20A.%20Patratskiy%20and%20Alexey%20K.%20Kovalev%20and%20Aleksandr%20I.%20Panov&entry.1292438233=%20%20Vision-Language-Action%20models%20have%20demonstrated%20remarkable%20capabilities%20in%0Apredicting%20agent%20movements%20within%20virtual%20environments%20and%20real-world%20scenarios%0Abased%20on%20visual%20observations%20and%20textual%20instructions.%20Although%20recent%20research%0Ahas%20focused%20on%20enhancing%20spatial%20and%20temporal%20understanding%20independently%2C%20this%0Apaper%20presents%20a%20novel%20approach%20that%20integrates%20both%20aspects%20through%20visual%0Aprompting.%20We%20introduce%20a%20method%20that%20projects%20visual%20traces%20of%20key%20points%20from%0Aobservations%20onto%20depth%20maps%2C%20enabling%20models%20to%20capture%20both%20spatial%20and%0Atemporal%20information%20simultaneously.%20The%20experiments%20in%20SimplerEnv%20show%20that%0Athe%20mean%20number%20of%20tasks%20successfully%20solved%20increased%20for%204%25%20compared%20to%0ASpatialVLA%20and%2019%25%20compared%20to%20TraceVLA.%20Furthermore%2C%20we%20show%20that%20this%0Aenhancement%20can%20be%20achieved%20with%20minimal%20training%20data%2C%20making%20it%20particularly%0Avaluable%20for%20real-world%20applications%20where%20data%20collection%20is%20challenging.%20The%0Aproject%20page%20is%20available%20at%20https%3A//ampiromax.github.io/ST-VLA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09032v1&entry.124074799=Read"},
{"title": "HumanOLAT: A Large-Scale Dataset for Full-Body Human Relighting and\n  Novel-View Synthesis", "author": "Timo Teufel and Pulkit Gera and Xilong Zhou and Umar Iqbal and Pramod Rao and Jan Kautz and Vladislav Golyanik and Christian Theobalt", "abstract": "  Simultaneous relighting and novel-view rendering of digital human\nrepresentations is an important yet challenging task with numerous\napplications. Progress in this area has been significantly limited due to the\nlack of publicly available, high-quality datasets, especially for full-body\nhuman captures. To address this critical gap, we introduce the HumanOLAT\ndataset, the first publicly accessible large-scale dataset of multi-view\nOne-Light-at-a-Time (OLAT) captures of full-body humans. The dataset includes\nHDR RGB frames under various illuminations, such as white light, environment\nmaps, color gradients and fine-grained OLAT illuminations. Our evaluations of\nstate-of-the-art relighting and novel-view synthesis methods underscore both\nthe dataset's value and the significant challenges still present in modeling\ncomplex human-centric appearance and lighting interactions. We believe\nHumanOLAT will significantly facilitate future research, enabling rigorous\nbenchmarking and advancements in both general and human-specific relighting and\nrendering techniques.\n", "link": "http://arxiv.org/abs/2508.09137v1", "date": "2025-08-12", "relevancy": 2.8767, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5941}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5666}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanOLAT%3A%20A%20Large-Scale%20Dataset%20for%20Full-Body%20Human%20Relighting%20and%0A%20%20Novel-View%20Synthesis&body=Title%3A%20HumanOLAT%3A%20A%20Large-Scale%20Dataset%20for%20Full-Body%20Human%20Relighting%20and%0A%20%20Novel-View%20Synthesis%0AAuthor%3A%20Timo%20Teufel%20and%20Pulkit%20Gera%20and%20Xilong%20Zhou%20and%20Umar%20Iqbal%20and%20Pramod%20Rao%20and%20Jan%20Kautz%20and%20Vladislav%20Golyanik%20and%20Christian%20Theobalt%0AAbstract%3A%20%20%20Simultaneous%20relighting%20and%20novel-view%20rendering%20of%20digital%20human%0Arepresentations%20is%20an%20important%20yet%20challenging%20task%20with%20numerous%0Aapplications.%20Progress%20in%20this%20area%20has%20been%20significantly%20limited%20due%20to%20the%0Alack%20of%20publicly%20available%2C%20high-quality%20datasets%2C%20especially%20for%20full-body%0Ahuman%20captures.%20To%20address%20this%20critical%20gap%2C%20we%20introduce%20the%20HumanOLAT%0Adataset%2C%20the%20first%20publicly%20accessible%20large-scale%20dataset%20of%20multi-view%0AOne-Light-at-a-Time%20%28OLAT%29%20captures%20of%20full-body%20humans.%20The%20dataset%20includes%0AHDR%20RGB%20frames%20under%20various%20illuminations%2C%20such%20as%20white%20light%2C%20environment%0Amaps%2C%20color%20gradients%20and%20fine-grained%20OLAT%20illuminations.%20Our%20evaluations%20of%0Astate-of-the-art%20relighting%20and%20novel-view%20synthesis%20methods%20underscore%20both%0Athe%20dataset%27s%20value%20and%20the%20significant%20challenges%20still%20present%20in%20modeling%0Acomplex%20human-centric%20appearance%20and%20lighting%20interactions.%20We%20believe%0AHumanOLAT%20will%20significantly%20facilitate%20future%20research%2C%20enabling%20rigorous%0Abenchmarking%20and%20advancements%20in%20both%20general%20and%20human-specific%20relighting%20and%0Arendering%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanOLAT%253A%2520A%2520Large-Scale%2520Dataset%2520for%2520Full-Body%2520Human%2520Relighting%2520and%250A%2520%2520Novel-View%2520Synthesis%26entry.906535625%3DTimo%2520Teufel%2520and%2520Pulkit%2520Gera%2520and%2520Xilong%2520Zhou%2520and%2520Umar%2520Iqbal%2520and%2520Pramod%2520Rao%2520and%2520Jan%2520Kautz%2520and%2520Vladislav%2520Golyanik%2520and%2520Christian%2520Theobalt%26entry.1292438233%3D%2520%2520Simultaneous%2520relighting%2520and%2520novel-view%2520rendering%2520of%2520digital%2520human%250Arepresentations%2520is%2520an%2520important%2520yet%2520challenging%2520task%2520with%2520numerous%250Aapplications.%2520Progress%2520in%2520this%2520area%2520has%2520been%2520significantly%2520limited%2520due%2520to%2520the%250Alack%2520of%2520publicly%2520available%252C%2520high-quality%2520datasets%252C%2520especially%2520for%2520full-body%250Ahuman%2520captures.%2520To%2520address%2520this%2520critical%2520gap%252C%2520we%2520introduce%2520the%2520HumanOLAT%250Adataset%252C%2520the%2520first%2520publicly%2520accessible%2520large-scale%2520dataset%2520of%2520multi-view%250AOne-Light-at-a-Time%2520%2528OLAT%2529%2520captures%2520of%2520full-body%2520humans.%2520The%2520dataset%2520includes%250AHDR%2520RGB%2520frames%2520under%2520various%2520illuminations%252C%2520such%2520as%2520white%2520light%252C%2520environment%250Amaps%252C%2520color%2520gradients%2520and%2520fine-grained%2520OLAT%2520illuminations.%2520Our%2520evaluations%2520of%250Astate-of-the-art%2520relighting%2520and%2520novel-view%2520synthesis%2520methods%2520underscore%2520both%250Athe%2520dataset%2527s%2520value%2520and%2520the%2520significant%2520challenges%2520still%2520present%2520in%2520modeling%250Acomplex%2520human-centric%2520appearance%2520and%2520lighting%2520interactions.%2520We%2520believe%250AHumanOLAT%2520will%2520significantly%2520facilitate%2520future%2520research%252C%2520enabling%2520rigorous%250Abenchmarking%2520and%2520advancements%2520in%2520both%2520general%2520and%2520human-specific%2520relighting%2520and%250Arendering%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanOLAT%3A%20A%20Large-Scale%20Dataset%20for%20Full-Body%20Human%20Relighting%20and%0A%20%20Novel-View%20Synthesis&entry.906535625=Timo%20Teufel%20and%20Pulkit%20Gera%20and%20Xilong%20Zhou%20and%20Umar%20Iqbal%20and%20Pramod%20Rao%20and%20Jan%20Kautz%20and%20Vladislav%20Golyanik%20and%20Christian%20Theobalt&entry.1292438233=%20%20Simultaneous%20relighting%20and%20novel-view%20rendering%20of%20digital%20human%0Arepresentations%20is%20an%20important%20yet%20challenging%20task%20with%20numerous%0Aapplications.%20Progress%20in%20this%20area%20has%20been%20significantly%20limited%20due%20to%20the%0Alack%20of%20publicly%20available%2C%20high-quality%20datasets%2C%20especially%20for%20full-body%0Ahuman%20captures.%20To%20address%20this%20critical%20gap%2C%20we%20introduce%20the%20HumanOLAT%0Adataset%2C%20the%20first%20publicly%20accessible%20large-scale%20dataset%20of%20multi-view%0AOne-Light-at-a-Time%20%28OLAT%29%20captures%20of%20full-body%20humans.%20The%20dataset%20includes%0AHDR%20RGB%20frames%20under%20various%20illuminations%2C%20such%20as%20white%20light%2C%20environment%0Amaps%2C%20color%20gradients%20and%20fine-grained%20OLAT%20illuminations.%20Our%20evaluations%20of%0Astate-of-the-art%20relighting%20and%20novel-view%20synthesis%20methods%20underscore%20both%0Athe%20dataset%27s%20value%20and%20the%20significant%20challenges%20still%20present%20in%20modeling%0Acomplex%20human-centric%20appearance%20and%20lighting%20interactions.%20We%20believe%0AHumanOLAT%20will%20significantly%20facilitate%20future%20research%2C%20enabling%20rigorous%0Abenchmarking%20and%20advancements%20in%20both%20general%20and%20human-specific%20relighting%20and%0Arendering%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09137v1&entry.124074799=Read"},
{"title": "OpenCUA: Open Foundations for Computer-Use Agents", "author": "Xinyuan Wang and Bowen Wang and Dunjie Lu and Junlin Yang and Tianbao Xie and Junli Wang and Jiaqi Deng and Xiaole Guo and Yiheng Xu and Chen Henry Wu and Zhennan Shen and Zhuokai Li and Ryan Li and Xiaochuan Li and Junda Chen and Boyuan Zheng and Peihang Li and Fangyu Lei and Ruisheng Cao and Yeqiao Fu and Dongchan Shin and Martin Shin and Jiarui Hu and Yuyan Wang and Jixuan Chen and Yuxiao Ye and Danyang Zhang and Dikang Du and Hao Hu and Huarong Chen and Zaida Zhou and Yipu Wang and Heng Wang and Diyi Yang and Victor Zhong and Flood Sung and Y. Charles and Zhilin Yang and Tao Yu", "abstract": "  Vision-language models have demonstrated impressive capabilities as\ncomputer-use agents (CUAs) capable of automating diverse computer tasks. As\ntheir commercial potential grows, critical details of the most capable CUA\nsystems remain closed. As these agents will increasingly mediate digital\ninteractions and execute consequential decisions on our behalf, the research\ncommunity needs access to open CUA frameworks to study their capabilities,\nlimitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive\nopen-source framework for scaling CUA data and foundation models. Our framework\nconsists of: (1) an annotation infrastructure that seamlessly captures human\ncomputer-use demonstrations; (2) AgentNet, the first large-scale computer-use\ntask dataset spanning 3 operating systems and 200+ applications and websites;\n(3) a scalable pipeline that transforms demonstrations into state-action pairs\nwith reflective long Chain-of-Thought reasoning that sustain robust performance\ngains as data scales. Our end-to-end agent models demonstrate strong\nperformance across CUA benchmarks. In particular, OpenCUA-32B achieves an\naverage success rate of 34.8% on OSWorld-Verified, establishing a new\nstate-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA\n(GPT-4o). Further analysis confirms that our approach generalizes well across\ndomains and benefits significantly from increased test-time computation. We\nrelease our annotation tool, datasets, code, and models to build open\nfoundations for further CUA research.\n", "link": "http://arxiv.org/abs/2508.09123v1", "date": "2025-08-12", "relevancy": 2.6525, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5399}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenCUA%3A%20Open%20Foundations%20for%20Computer-Use%20Agents&body=Title%3A%20OpenCUA%3A%20Open%20Foundations%20for%20Computer-Use%20Agents%0AAuthor%3A%20Xinyuan%20Wang%20and%20Bowen%20Wang%20and%20Dunjie%20Lu%20and%20Junlin%20Yang%20and%20Tianbao%20Xie%20and%20Junli%20Wang%20and%20Jiaqi%20Deng%20and%20Xiaole%20Guo%20and%20Yiheng%20Xu%20and%20Chen%20Henry%20Wu%20and%20Zhennan%20Shen%20and%20Zhuokai%20Li%20and%20Ryan%20Li%20and%20Xiaochuan%20Li%20and%20Junda%20Chen%20and%20Boyuan%20Zheng%20and%20Peihang%20Li%20and%20Fangyu%20Lei%20and%20Ruisheng%20Cao%20and%20Yeqiao%20Fu%20and%20Dongchan%20Shin%20and%20Martin%20Shin%20and%20Jiarui%20Hu%20and%20Yuyan%20Wang%20and%20Jixuan%20Chen%20and%20Yuxiao%20Ye%20and%20Danyang%20Zhang%20and%20Dikang%20Du%20and%20Hao%20Hu%20and%20Huarong%20Chen%20and%20Zaida%20Zhou%20and%20Yipu%20Wang%20and%20Heng%20Wang%20and%20Diyi%20Yang%20and%20Victor%20Zhong%20and%20Flood%20Sung%20and%20Y.%20Charles%20and%20Zhilin%20Yang%20and%20Tao%20Yu%0AAbstract%3A%20%20%20Vision-language%20models%20have%20demonstrated%20impressive%20capabilities%20as%0Acomputer-use%20agents%20%28CUAs%29%20capable%20of%20automating%20diverse%20computer%20tasks.%20As%0Atheir%20commercial%20potential%20grows%2C%20critical%20details%20of%20the%20most%20capable%20CUA%0Asystems%20remain%20closed.%20As%20these%20agents%20will%20increasingly%20mediate%20digital%0Ainteractions%20and%20execute%20consequential%20decisions%20on%20our%20behalf%2C%20the%20research%0Acommunity%20needs%20access%20to%20open%20CUA%20frameworks%20to%20study%20their%20capabilities%2C%0Alimitations%2C%20and%20risks.%20To%20bridge%20this%20gap%2C%20we%20propose%20OpenCUA%2C%20a%20comprehensive%0Aopen-source%20framework%20for%20scaling%20CUA%20data%20and%20foundation%20models.%20Our%20framework%0Aconsists%20of%3A%20%281%29%20an%20annotation%20infrastructure%20that%20seamlessly%20captures%20human%0Acomputer-use%20demonstrations%3B%20%282%29%20AgentNet%2C%20the%20first%20large-scale%20computer-use%0Atask%20dataset%20spanning%203%20operating%20systems%20and%20200%2B%20applications%20and%20websites%3B%0A%283%29%20a%20scalable%20pipeline%20that%20transforms%20demonstrations%20into%20state-action%20pairs%0Awith%20reflective%20long%20Chain-of-Thought%20reasoning%20that%20sustain%20robust%20performance%0Agains%20as%20data%20scales.%20Our%20end-to-end%20agent%20models%20demonstrate%20strong%0Aperformance%20across%20CUA%20benchmarks.%20In%20particular%2C%20OpenCUA-32B%20achieves%20an%0Aaverage%20success%20rate%20of%2034.8%25%20on%20OSWorld-Verified%2C%20establishing%20a%20new%0Astate-of-the-art%20%28SOTA%29%20among%20open-source%20models%20and%20surpassing%20OpenAI%20CUA%0A%28GPT-4o%29.%20Further%20analysis%20confirms%20that%20our%20approach%20generalizes%20well%20across%0Adomains%20and%20benefits%20significantly%20from%20increased%20test-time%20computation.%20We%0Arelease%20our%20annotation%20tool%2C%20datasets%2C%20code%2C%20and%20models%20to%20build%20open%0Afoundations%20for%20further%20CUA%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenCUA%253A%2520Open%2520Foundations%2520for%2520Computer-Use%2520Agents%26entry.906535625%3DXinyuan%2520Wang%2520and%2520Bowen%2520Wang%2520and%2520Dunjie%2520Lu%2520and%2520Junlin%2520Yang%2520and%2520Tianbao%2520Xie%2520and%2520Junli%2520Wang%2520and%2520Jiaqi%2520Deng%2520and%2520Xiaole%2520Guo%2520and%2520Yiheng%2520Xu%2520and%2520Chen%2520Henry%2520Wu%2520and%2520Zhennan%2520Shen%2520and%2520Zhuokai%2520Li%2520and%2520Ryan%2520Li%2520and%2520Xiaochuan%2520Li%2520and%2520Junda%2520Chen%2520and%2520Boyuan%2520Zheng%2520and%2520Peihang%2520Li%2520and%2520Fangyu%2520Lei%2520and%2520Ruisheng%2520Cao%2520and%2520Yeqiao%2520Fu%2520and%2520Dongchan%2520Shin%2520and%2520Martin%2520Shin%2520and%2520Jiarui%2520Hu%2520and%2520Yuyan%2520Wang%2520and%2520Jixuan%2520Chen%2520and%2520Yuxiao%2520Ye%2520and%2520Danyang%2520Zhang%2520and%2520Dikang%2520Du%2520and%2520Hao%2520Hu%2520and%2520Huarong%2520Chen%2520and%2520Zaida%2520Zhou%2520and%2520Yipu%2520Wang%2520and%2520Heng%2520Wang%2520and%2520Diyi%2520Yang%2520and%2520Victor%2520Zhong%2520and%2520Flood%2520Sung%2520and%2520Y.%2520Charles%2520and%2520Zhilin%2520Yang%2520and%2520Tao%2520Yu%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520have%2520demonstrated%2520impressive%2520capabilities%2520as%250Acomputer-use%2520agents%2520%2528CUAs%2529%2520capable%2520of%2520automating%2520diverse%2520computer%2520tasks.%2520As%250Atheir%2520commercial%2520potential%2520grows%252C%2520critical%2520details%2520of%2520the%2520most%2520capable%2520CUA%250Asystems%2520remain%2520closed.%2520As%2520these%2520agents%2520will%2520increasingly%2520mediate%2520digital%250Ainteractions%2520and%2520execute%2520consequential%2520decisions%2520on%2520our%2520behalf%252C%2520the%2520research%250Acommunity%2520needs%2520access%2520to%2520open%2520CUA%2520frameworks%2520to%2520study%2520their%2520capabilities%252C%250Alimitations%252C%2520and%2520risks.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520OpenCUA%252C%2520a%2520comprehensive%250Aopen-source%2520framework%2520for%2520scaling%2520CUA%2520data%2520and%2520foundation%2520models.%2520Our%2520framework%250Aconsists%2520of%253A%2520%25281%2529%2520an%2520annotation%2520infrastructure%2520that%2520seamlessly%2520captures%2520human%250Acomputer-use%2520demonstrations%253B%2520%25282%2529%2520AgentNet%252C%2520the%2520first%2520large-scale%2520computer-use%250Atask%2520dataset%2520spanning%25203%2520operating%2520systems%2520and%2520200%252B%2520applications%2520and%2520websites%253B%250A%25283%2529%2520a%2520scalable%2520pipeline%2520that%2520transforms%2520demonstrations%2520into%2520state-action%2520pairs%250Awith%2520reflective%2520long%2520Chain-of-Thought%2520reasoning%2520that%2520sustain%2520robust%2520performance%250Agains%2520as%2520data%2520scales.%2520Our%2520end-to-end%2520agent%2520models%2520demonstrate%2520strong%250Aperformance%2520across%2520CUA%2520benchmarks.%2520In%2520particular%252C%2520OpenCUA-32B%2520achieves%2520an%250Aaverage%2520success%2520rate%2520of%252034.8%2525%2520on%2520OSWorld-Verified%252C%2520establishing%2520a%2520new%250Astate-of-the-art%2520%2528SOTA%2529%2520among%2520open-source%2520models%2520and%2520surpassing%2520OpenAI%2520CUA%250A%2528GPT-4o%2529.%2520Further%2520analysis%2520confirms%2520that%2520our%2520approach%2520generalizes%2520well%2520across%250Adomains%2520and%2520benefits%2520significantly%2520from%2520increased%2520test-time%2520computation.%2520We%250Arelease%2520our%2520annotation%2520tool%252C%2520datasets%252C%2520code%252C%2520and%2520models%2520to%2520build%2520open%250Afoundations%2520for%2520further%2520CUA%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenCUA%3A%20Open%20Foundations%20for%20Computer-Use%20Agents&entry.906535625=Xinyuan%20Wang%20and%20Bowen%20Wang%20and%20Dunjie%20Lu%20and%20Junlin%20Yang%20and%20Tianbao%20Xie%20and%20Junli%20Wang%20and%20Jiaqi%20Deng%20and%20Xiaole%20Guo%20and%20Yiheng%20Xu%20and%20Chen%20Henry%20Wu%20and%20Zhennan%20Shen%20and%20Zhuokai%20Li%20and%20Ryan%20Li%20and%20Xiaochuan%20Li%20and%20Junda%20Chen%20and%20Boyuan%20Zheng%20and%20Peihang%20Li%20and%20Fangyu%20Lei%20and%20Ruisheng%20Cao%20and%20Yeqiao%20Fu%20and%20Dongchan%20Shin%20and%20Martin%20Shin%20and%20Jiarui%20Hu%20and%20Yuyan%20Wang%20and%20Jixuan%20Chen%20and%20Yuxiao%20Ye%20and%20Danyang%20Zhang%20and%20Dikang%20Du%20and%20Hao%20Hu%20and%20Huarong%20Chen%20and%20Zaida%20Zhou%20and%20Yipu%20Wang%20and%20Heng%20Wang%20and%20Diyi%20Yang%20and%20Victor%20Zhong%20and%20Flood%20Sung%20and%20Y.%20Charles%20and%20Zhilin%20Yang%20and%20Tao%20Yu&entry.1292438233=%20%20Vision-language%20models%20have%20demonstrated%20impressive%20capabilities%20as%0Acomputer-use%20agents%20%28CUAs%29%20capable%20of%20automating%20diverse%20computer%20tasks.%20As%0Atheir%20commercial%20potential%20grows%2C%20critical%20details%20of%20the%20most%20capable%20CUA%0Asystems%20remain%20closed.%20As%20these%20agents%20will%20increasingly%20mediate%20digital%0Ainteractions%20and%20execute%20consequential%20decisions%20on%20our%20behalf%2C%20the%20research%0Acommunity%20needs%20access%20to%20open%20CUA%20frameworks%20to%20study%20their%20capabilities%2C%0Alimitations%2C%20and%20risks.%20To%20bridge%20this%20gap%2C%20we%20propose%20OpenCUA%2C%20a%20comprehensive%0Aopen-source%20framework%20for%20scaling%20CUA%20data%20and%20foundation%20models.%20Our%20framework%0Aconsists%20of%3A%20%281%29%20an%20annotation%20infrastructure%20that%20seamlessly%20captures%20human%0Acomputer-use%20demonstrations%3B%20%282%29%20AgentNet%2C%20the%20first%20large-scale%20computer-use%0Atask%20dataset%20spanning%203%20operating%20systems%20and%20200%2B%20applications%20and%20websites%3B%0A%283%29%20a%20scalable%20pipeline%20that%20transforms%20demonstrations%20into%20state-action%20pairs%0Awith%20reflective%20long%20Chain-of-Thought%20reasoning%20that%20sustain%20robust%20performance%0Agains%20as%20data%20scales.%20Our%20end-to-end%20agent%20models%20demonstrate%20strong%0Aperformance%20across%20CUA%20benchmarks.%20In%20particular%2C%20OpenCUA-32B%20achieves%20an%0Aaverage%20success%20rate%20of%2034.8%25%20on%20OSWorld-Verified%2C%20establishing%20a%20new%0Astate-of-the-art%20%28SOTA%29%20among%20open-source%20models%20and%20surpassing%20OpenAI%20CUA%0A%28GPT-4o%29.%20Further%20analysis%20confirms%20that%20our%20approach%20generalizes%20well%20across%0Adomains%20and%20benefits%20significantly%20from%20increased%20test-time%20computation.%20We%0Arelease%20our%20annotation%20tool%2C%20datasets%2C%20code%2C%20and%20models%20to%20build%20open%0Afoundations%20for%20further%20CUA%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09123v1&entry.124074799=Read"},
{"title": "Half-Physics: Enabling Kinematic 3D Human Model with Physical\n  Interactions", "author": "Li Siyao and Yao Feng and Omid Taheri and Chen Change Loy and Michael J. Black", "abstract": "  While current general-purpose 3D human models (e.g., SMPL-X) efficiently\nrepresent accurate human shape and pose, they lacks the ability to physically\ninteract with the environment due to the kinematic nature. As a result,\nkinematic-based interaction models often suffer from issues such as\ninterpenetration and unrealistic object dynamics. To address this limitation,\nwe introduce a novel approach that embeds SMPL-X into a tangible entity capable\nof dynamic physical interactions with its surroundings. Specifically, we\npropose a \"half-physics\" mechanism that transforms 3D kinematic motion into a\nphysics simulation. Our approach maintains kinematic control over inherent\nSMPL-X poses while ensuring physically plausible interactions with scenes and\nobjects, effectively eliminating penetration and unrealistic object dynamics.\nUnlike reinforcement learning-based methods, which demand extensive and complex\ntraining, our half-physics method is learning-free and generalizes to any body\nshape and motion; meanwhile, it operates in real time. Moreover, it preserves\nthe fidelity of the original kinematic motion while seamlessly integrating\nphysical interactions\n", "link": "http://arxiv.org/abs/2507.23778v2", "date": "2025-08-12", "relevancy": 2.5851, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6807}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6327}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Half-Physics%3A%20Enabling%20Kinematic%203D%20Human%20Model%20with%20Physical%0A%20%20Interactions&body=Title%3A%20Half-Physics%3A%20Enabling%20Kinematic%203D%20Human%20Model%20with%20Physical%0A%20%20Interactions%0AAuthor%3A%20Li%20Siyao%20and%20Yao%20Feng%20and%20Omid%20Taheri%20and%20Chen%20Change%20Loy%20and%20Michael%20J.%20Black%0AAbstract%3A%20%20%20While%20current%20general-purpose%203D%20human%20models%20%28e.g.%2C%20SMPL-X%29%20efficiently%0Arepresent%20accurate%20human%20shape%20and%20pose%2C%20they%20lacks%20the%20ability%20to%20physically%0Ainteract%20with%20the%20environment%20due%20to%20the%20kinematic%20nature.%20As%20a%20result%2C%0Akinematic-based%20interaction%20models%20often%20suffer%20from%20issues%20such%20as%0Ainterpenetration%20and%20unrealistic%20object%20dynamics.%20To%20address%20this%20limitation%2C%0Awe%20introduce%20a%20novel%20approach%20that%20embeds%20SMPL-X%20into%20a%20tangible%20entity%20capable%0Aof%20dynamic%20physical%20interactions%20with%20its%20surroundings.%20Specifically%2C%20we%0Apropose%20a%20%22half-physics%22%20mechanism%20that%20transforms%203D%20kinematic%20motion%20into%20a%0Aphysics%20simulation.%20Our%20approach%20maintains%20kinematic%20control%20over%20inherent%0ASMPL-X%20poses%20while%20ensuring%20physically%20plausible%20interactions%20with%20scenes%20and%0Aobjects%2C%20effectively%20eliminating%20penetration%20and%20unrealistic%20object%20dynamics.%0AUnlike%20reinforcement%20learning-based%20methods%2C%20which%20demand%20extensive%20and%20complex%0Atraining%2C%20our%20half-physics%20method%20is%20learning-free%20and%20generalizes%20to%20any%20body%0Ashape%20and%20motion%3B%20meanwhile%2C%20it%20operates%20in%20real%20time.%20Moreover%2C%20it%20preserves%0Athe%20fidelity%20of%20the%20original%20kinematic%20motion%20while%20seamlessly%20integrating%0Aphysical%20interactions%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23778v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHalf-Physics%253A%2520Enabling%2520Kinematic%25203D%2520Human%2520Model%2520with%2520Physical%250A%2520%2520Interactions%26entry.906535625%3DLi%2520Siyao%2520and%2520Yao%2520Feng%2520and%2520Omid%2520Taheri%2520and%2520Chen%2520Change%2520Loy%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3D%2520%2520While%2520current%2520general-purpose%25203D%2520human%2520models%2520%2528e.g.%252C%2520SMPL-X%2529%2520efficiently%250Arepresent%2520accurate%2520human%2520shape%2520and%2520pose%252C%2520they%2520lacks%2520the%2520ability%2520to%2520physically%250Ainteract%2520with%2520the%2520environment%2520due%2520to%2520the%2520kinematic%2520nature.%2520As%2520a%2520result%252C%250Akinematic-based%2520interaction%2520models%2520often%2520suffer%2520from%2520issues%2520such%2520as%250Ainterpenetration%2520and%2520unrealistic%2520object%2520dynamics.%2520To%2520address%2520this%2520limitation%252C%250Awe%2520introduce%2520a%2520novel%2520approach%2520that%2520embeds%2520SMPL-X%2520into%2520a%2520tangible%2520entity%2520capable%250Aof%2520dynamic%2520physical%2520interactions%2520with%2520its%2520surroundings.%2520Specifically%252C%2520we%250Apropose%2520a%2520%2522half-physics%2522%2520mechanism%2520that%2520transforms%25203D%2520kinematic%2520motion%2520into%2520a%250Aphysics%2520simulation.%2520Our%2520approach%2520maintains%2520kinematic%2520control%2520over%2520inherent%250ASMPL-X%2520poses%2520while%2520ensuring%2520physically%2520plausible%2520interactions%2520with%2520scenes%2520and%250Aobjects%252C%2520effectively%2520eliminating%2520penetration%2520and%2520unrealistic%2520object%2520dynamics.%250AUnlike%2520reinforcement%2520learning-based%2520methods%252C%2520which%2520demand%2520extensive%2520and%2520complex%250Atraining%252C%2520our%2520half-physics%2520method%2520is%2520learning-free%2520and%2520generalizes%2520to%2520any%2520body%250Ashape%2520and%2520motion%253B%2520meanwhile%252C%2520it%2520operates%2520in%2520real%2520time.%2520Moreover%252C%2520it%2520preserves%250Athe%2520fidelity%2520of%2520the%2520original%2520kinematic%2520motion%2520while%2520seamlessly%2520integrating%250Aphysical%2520interactions%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23778v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Half-Physics%3A%20Enabling%20Kinematic%203D%20Human%20Model%20with%20Physical%0A%20%20Interactions&entry.906535625=Li%20Siyao%20and%20Yao%20Feng%20and%20Omid%20Taheri%20and%20Chen%20Change%20Loy%20and%20Michael%20J.%20Black&entry.1292438233=%20%20While%20current%20general-purpose%203D%20human%20models%20%28e.g.%2C%20SMPL-X%29%20efficiently%0Arepresent%20accurate%20human%20shape%20and%20pose%2C%20they%20lacks%20the%20ability%20to%20physically%0Ainteract%20with%20the%20environment%20due%20to%20the%20kinematic%20nature.%20As%20a%20result%2C%0Akinematic-based%20interaction%20models%20often%20suffer%20from%20issues%20such%20as%0Ainterpenetration%20and%20unrealistic%20object%20dynamics.%20To%20address%20this%20limitation%2C%0Awe%20introduce%20a%20novel%20approach%20that%20embeds%20SMPL-X%20into%20a%20tangible%20entity%20capable%0Aof%20dynamic%20physical%20interactions%20with%20its%20surroundings.%20Specifically%2C%20we%0Apropose%20a%20%22half-physics%22%20mechanism%20that%20transforms%203D%20kinematic%20motion%20into%20a%0Aphysics%20simulation.%20Our%20approach%20maintains%20kinematic%20control%20over%20inherent%0ASMPL-X%20poses%20while%20ensuring%20physically%20plausible%20interactions%20with%20scenes%20and%0Aobjects%2C%20effectively%20eliminating%20penetration%20and%20unrealistic%20object%20dynamics.%0AUnlike%20reinforcement%20learning-based%20methods%2C%20which%20demand%20extensive%20and%20complex%0Atraining%2C%20our%20half-physics%20method%20is%20learning-free%20and%20generalizes%20to%20any%20body%0Ashape%20and%20motion%3B%20meanwhile%2C%20it%20operates%20in%20real%20time.%20Moreover%2C%20it%20preserves%0Athe%20fidelity%20of%20the%20original%20kinematic%20motion%20while%20seamlessly%20integrating%0Aphysical%20interactions%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23778v2&entry.124074799=Read"},
{"title": "Retrospective Sparse Attention for Efficient Long-Context Generation", "author": "Seonghwan Choi and Beomseok Kang and Dongwon Jo and Jae-Joon Kim", "abstract": "  Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%.\n", "link": "http://arxiv.org/abs/2508.09001v1", "date": "2025-08-12", "relevancy": 2.5815, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5722}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4883}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrospective%20Sparse%20Attention%20for%20Efficient%20Long-Context%20Generation&body=Title%3A%20Retrospective%20Sparse%20Attention%20for%20Efficient%20Long-Context%20Generation%0AAuthor%3A%20Seonghwan%20Choi%20and%20Beomseok%20Kang%20and%20Dongwon%20Jo%20and%20Jae-Joon%20Kim%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20long-context%20tasks%0Asuch%20as%20reasoning%2C%20code%20generation%2C%20and%20multi-turn%20dialogue.%20However%2C%20inference%0Aover%20extended%20contexts%20is%20bottlenecked%20by%20the%20Key-Value%20%28KV%29%20cache%2C%20whose%0Amemory%20footprint%20grows%20linearly%20with%20sequence%20length%20and%20dominates%20latency%20at%0Aeach%20decoding%20step.%20While%20recent%20KV%20cache%20compression%20methods%20identify%20and%20load%0Aimportant%20tokens%2C%20they%20focus%20predominantly%20on%20input%20contexts%20and%20fail%20to%0Aaddress%20the%20cumulative%20attention%20errors%20that%20arise%20during%20long%20decoding.%20In%0Athis%20paper%2C%20we%20introduce%20RetroAttention%2C%20a%20novel%20KV%20cache%20update%20technique%20that%0Aretrospectively%20revises%20past%20attention%20outputs%20using%20newly%20arrived%20KV%20entries%0Afrom%20subsequent%20decoding%20steps.%20By%20maintaining%20a%20lightweight%20output%20cache%2C%0ARetroAttention%20enables%20past%20queries%20to%20efficiently%20access%20more%20relevant%0Acontext%2C%20while%20incurring%20minimal%20latency%20overhead.%20This%20breaks%20the%0Afixed-attention-output%20paradigm%20and%20allows%20continual%20correction%20of%20prior%0Aapproximations.%20Extensive%20experiments%20on%20long-generation%20benchmarks%20show%20that%0ARetroAttention%20consistently%20outperforms%20state-of-the-art%20%28SOTA%29%20KV%20compression%0Amethods%2C%20increasing%20effective%20KV%20exposure%20by%20up%20to%201.6%24%5Ctimes%24%20and%20accuracy%20by%0Aup%20to%2021.9%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrospective%2520Sparse%2520Attention%2520for%2520Efficient%2520Long-Context%2520Generation%26entry.906535625%3DSeonghwan%2520Choi%2520and%2520Beomseok%2520Kang%2520and%2520Dongwon%2520Jo%2520and%2520Jae-Joon%2520Kim%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520long-context%2520tasks%250Asuch%2520as%2520reasoning%252C%2520code%2520generation%252C%2520and%2520multi-turn%2520dialogue.%2520However%252C%2520inference%250Aover%2520extended%2520contexts%2520is%2520bottlenecked%2520by%2520the%2520Key-Value%2520%2528KV%2529%2520cache%252C%2520whose%250Amemory%2520footprint%2520grows%2520linearly%2520with%2520sequence%2520length%2520and%2520dominates%2520latency%2520at%250Aeach%2520decoding%2520step.%2520While%2520recent%2520KV%2520cache%2520compression%2520methods%2520identify%2520and%2520load%250Aimportant%2520tokens%252C%2520they%2520focus%2520predominantly%2520on%2520input%2520contexts%2520and%2520fail%2520to%250Aaddress%2520the%2520cumulative%2520attention%2520errors%2520that%2520arise%2520during%2520long%2520decoding.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520RetroAttention%252C%2520a%2520novel%2520KV%2520cache%2520update%2520technique%2520that%250Aretrospectively%2520revises%2520past%2520attention%2520outputs%2520using%2520newly%2520arrived%2520KV%2520entries%250Afrom%2520subsequent%2520decoding%2520steps.%2520By%2520maintaining%2520a%2520lightweight%2520output%2520cache%252C%250ARetroAttention%2520enables%2520past%2520queries%2520to%2520efficiently%2520access%2520more%2520relevant%250Acontext%252C%2520while%2520incurring%2520minimal%2520latency%2520overhead.%2520This%2520breaks%2520the%250Afixed-attention-output%2520paradigm%2520and%2520allows%2520continual%2520correction%2520of%2520prior%250Aapproximations.%2520Extensive%2520experiments%2520on%2520long-generation%2520benchmarks%2520show%2520that%250ARetroAttention%2520consistently%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%2520KV%2520compression%250Amethods%252C%2520increasing%2520effective%2520KV%2520exposure%2520by%2520up%2520to%25201.6%2524%255Ctimes%2524%2520and%2520accuracy%2520by%250Aup%2520to%252021.9%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrospective%20Sparse%20Attention%20for%20Efficient%20Long-Context%20Generation&entry.906535625=Seonghwan%20Choi%20and%20Beomseok%20Kang%20and%20Dongwon%20Jo%20and%20Jae-Joon%20Kim&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20long-context%20tasks%0Asuch%20as%20reasoning%2C%20code%20generation%2C%20and%20multi-turn%20dialogue.%20However%2C%20inference%0Aover%20extended%20contexts%20is%20bottlenecked%20by%20the%20Key-Value%20%28KV%29%20cache%2C%20whose%0Amemory%20footprint%20grows%20linearly%20with%20sequence%20length%20and%20dominates%20latency%20at%0Aeach%20decoding%20step.%20While%20recent%20KV%20cache%20compression%20methods%20identify%20and%20load%0Aimportant%20tokens%2C%20they%20focus%20predominantly%20on%20input%20contexts%20and%20fail%20to%0Aaddress%20the%20cumulative%20attention%20errors%20that%20arise%20during%20long%20decoding.%20In%0Athis%20paper%2C%20we%20introduce%20RetroAttention%2C%20a%20novel%20KV%20cache%20update%20technique%20that%0Aretrospectively%20revises%20past%20attention%20outputs%20using%20newly%20arrived%20KV%20entries%0Afrom%20subsequent%20decoding%20steps.%20By%20maintaining%20a%20lightweight%20output%20cache%2C%0ARetroAttention%20enables%20past%20queries%20to%20efficiently%20access%20more%20relevant%0Acontext%2C%20while%20incurring%20minimal%20latency%20overhead.%20This%20breaks%20the%0Afixed-attention-output%20paradigm%20and%20allows%20continual%20correction%20of%20prior%0Aapproximations.%20Extensive%20experiments%20on%20long-generation%20benchmarks%20show%20that%0ARetroAttention%20consistently%20outperforms%20state-of-the-art%20%28SOTA%29%20KV%20compression%0Amethods%2C%20increasing%20effective%20KV%20exposure%20by%20up%20to%201.6%24%5Ctimes%24%20and%20accuracy%20by%0Aup%20to%2021.9%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09001v1&entry.124074799=Read"},
{"title": "A Survey on Training-free Alignment of Large Language Models", "author": "Birong Pan and Yongqi Li and Weiyu Zhang and Wenpeng Lu and Mayi Xu and Shen Zhou and Yuanyuan Zhu and Ming Zhong and Tieyun Qian", "abstract": "  The alignment of large language models (LLMs) aims to ensure their outputs\nadhere to human values, ethical standards, and legal norms. Traditional\nalignment methods often rely on resource-intensive fine-tuning (FT), which may\nsuffer from knowledge degradation and face challenges in scenarios where the\nmodel accessibility or computational resources are constrained. In contrast,\ntraining-free (TF) alignment techniques--leveraging in-context learning,\ndecoding-time adjustments, and post-generation corrections--offer a promising\nalternative by enabling alignment without heavily retraining LLMs, making them\nadaptable to both open-source and closed-source environments. This paper\npresents the first systematic review of TF alignment methods, categorizing them\nby stages of pre-decoding, in-decoding, and post-decoding. For each stage, we\nprovide a detailed examination from the viewpoint of LLMs and multimodal LLMs\n(MLLMs), highlighting their mechanisms and limitations. Furthermore, we\nidentify key challenges and future directions, paving the way for more\ninclusive and effective TF alignment techniques. By synthesizing and organizing\nthe rapidly growing body of research, this survey offers a guidance for\npractitioners and advances the development of safer and more reliable LLMs.\n", "link": "http://arxiv.org/abs/2508.09016v1", "date": "2025-08-12", "relevancy": 2.5665, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5208}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Training-free%20Alignment%20of%20Large%20Language%20Models&body=Title%3A%20A%20Survey%20on%20Training-free%20Alignment%20of%20Large%20Language%20Models%0AAuthor%3A%20Birong%20Pan%20and%20Yongqi%20Li%20and%20Weiyu%20Zhang%20and%20Wenpeng%20Lu%20and%20Mayi%20Xu%20and%20Shen%20Zhou%20and%20Yuanyuan%20Zhu%20and%20Ming%20Zhong%20and%20Tieyun%20Qian%0AAbstract%3A%20%20%20The%20alignment%20of%20large%20language%20models%20%28LLMs%29%20aims%20to%20ensure%20their%20outputs%0Aadhere%20to%20human%20values%2C%20ethical%20standards%2C%20and%20legal%20norms.%20Traditional%0Aalignment%20methods%20often%20rely%20on%20resource-intensive%20fine-tuning%20%28FT%29%2C%20which%20may%0Asuffer%20from%20knowledge%20degradation%20and%20face%20challenges%20in%20scenarios%20where%20the%0Amodel%20accessibility%20or%20computational%20resources%20are%20constrained.%20In%20contrast%2C%0Atraining-free%20%28TF%29%20alignment%20techniques--leveraging%20in-context%20learning%2C%0Adecoding-time%20adjustments%2C%20and%20post-generation%20corrections--offer%20a%20promising%0Aalternative%20by%20enabling%20alignment%20without%20heavily%20retraining%20LLMs%2C%20making%20them%0Aadaptable%20to%20both%20open-source%20and%20closed-source%20environments.%20This%20paper%0Apresents%20the%20first%20systematic%20review%20of%20TF%20alignment%20methods%2C%20categorizing%20them%0Aby%20stages%20of%20pre-decoding%2C%20in-decoding%2C%20and%20post-decoding.%20For%20each%20stage%2C%20we%0Aprovide%20a%20detailed%20examination%20from%20the%20viewpoint%20of%20LLMs%20and%20multimodal%20LLMs%0A%28MLLMs%29%2C%20highlighting%20their%20mechanisms%20and%20limitations.%20Furthermore%2C%20we%0Aidentify%20key%20challenges%20and%20future%20directions%2C%20paving%20the%20way%20for%20more%0Ainclusive%20and%20effective%20TF%20alignment%20techniques.%20By%20synthesizing%20and%20organizing%0Athe%20rapidly%20growing%20body%20of%20research%2C%20this%20survey%20offers%20a%20guidance%20for%0Apractitioners%20and%20advances%20the%20development%20of%20safer%20and%20more%20reliable%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Training-free%2520Alignment%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DBirong%2520Pan%2520and%2520Yongqi%2520Li%2520and%2520Weiyu%2520Zhang%2520and%2520Wenpeng%2520Lu%2520and%2520Mayi%2520Xu%2520and%2520Shen%2520Zhou%2520and%2520Yuanyuan%2520Zhu%2520and%2520Ming%2520Zhong%2520and%2520Tieyun%2520Qian%26entry.1292438233%3D%2520%2520The%2520alignment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520aims%2520to%2520ensure%2520their%2520outputs%250Aadhere%2520to%2520human%2520values%252C%2520ethical%2520standards%252C%2520and%2520legal%2520norms.%2520Traditional%250Aalignment%2520methods%2520often%2520rely%2520on%2520resource-intensive%2520fine-tuning%2520%2528FT%2529%252C%2520which%2520may%250Asuffer%2520from%2520knowledge%2520degradation%2520and%2520face%2520challenges%2520in%2520scenarios%2520where%2520the%250Amodel%2520accessibility%2520or%2520computational%2520resources%2520are%2520constrained.%2520In%2520contrast%252C%250Atraining-free%2520%2528TF%2529%2520alignment%2520techniques--leveraging%2520in-context%2520learning%252C%250Adecoding-time%2520adjustments%252C%2520and%2520post-generation%2520corrections--offer%2520a%2520promising%250Aalternative%2520by%2520enabling%2520alignment%2520without%2520heavily%2520retraining%2520LLMs%252C%2520making%2520them%250Aadaptable%2520to%2520both%2520open-source%2520and%2520closed-source%2520environments.%2520This%2520paper%250Apresents%2520the%2520first%2520systematic%2520review%2520of%2520TF%2520alignment%2520methods%252C%2520categorizing%2520them%250Aby%2520stages%2520of%2520pre-decoding%252C%2520in-decoding%252C%2520and%2520post-decoding.%2520For%2520each%2520stage%252C%2520we%250Aprovide%2520a%2520detailed%2520examination%2520from%2520the%2520viewpoint%2520of%2520LLMs%2520and%2520multimodal%2520LLMs%250A%2528MLLMs%2529%252C%2520highlighting%2520their%2520mechanisms%2520and%2520limitations.%2520Furthermore%252C%2520we%250Aidentify%2520key%2520challenges%2520and%2520future%2520directions%252C%2520paving%2520the%2520way%2520for%2520more%250Ainclusive%2520and%2520effective%2520TF%2520alignment%2520techniques.%2520By%2520synthesizing%2520and%2520organizing%250Athe%2520rapidly%2520growing%2520body%2520of%2520research%252C%2520this%2520survey%2520offers%2520a%2520guidance%2520for%250Apractitioners%2520and%2520advances%2520the%2520development%2520of%2520safer%2520and%2520more%2520reliable%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Training-free%20Alignment%20of%20Large%20Language%20Models&entry.906535625=Birong%20Pan%20and%20Yongqi%20Li%20and%20Weiyu%20Zhang%20and%20Wenpeng%20Lu%20and%20Mayi%20Xu%20and%20Shen%20Zhou%20and%20Yuanyuan%20Zhu%20and%20Ming%20Zhong%20and%20Tieyun%20Qian&entry.1292438233=%20%20The%20alignment%20of%20large%20language%20models%20%28LLMs%29%20aims%20to%20ensure%20their%20outputs%0Aadhere%20to%20human%20values%2C%20ethical%20standards%2C%20and%20legal%20norms.%20Traditional%0Aalignment%20methods%20often%20rely%20on%20resource-intensive%20fine-tuning%20%28FT%29%2C%20which%20may%0Asuffer%20from%20knowledge%20degradation%20and%20face%20challenges%20in%20scenarios%20where%20the%0Amodel%20accessibility%20or%20computational%20resources%20are%20constrained.%20In%20contrast%2C%0Atraining-free%20%28TF%29%20alignment%20techniques--leveraging%20in-context%20learning%2C%0Adecoding-time%20adjustments%2C%20and%20post-generation%20corrections--offer%20a%20promising%0Aalternative%20by%20enabling%20alignment%20without%20heavily%20retraining%20LLMs%2C%20making%20them%0Aadaptable%20to%20both%20open-source%20and%20closed-source%20environments.%20This%20paper%0Apresents%20the%20first%20systematic%20review%20of%20TF%20alignment%20methods%2C%20categorizing%20them%0Aby%20stages%20of%20pre-decoding%2C%20in-decoding%2C%20and%20post-decoding.%20For%20each%20stage%2C%20we%0Aprovide%20a%20detailed%20examination%20from%20the%20viewpoint%20of%20LLMs%20and%20multimodal%20LLMs%0A%28MLLMs%29%2C%20highlighting%20their%20mechanisms%20and%20limitations.%20Furthermore%2C%20we%0Aidentify%20key%20challenges%20and%20future%20directions%2C%20paving%20the%20way%20for%20more%0Ainclusive%20and%20effective%20TF%20alignment%20techniques.%20By%20synthesizing%20and%20organizing%0Athe%20rapidly%20growing%20body%20of%20research%2C%20this%20survey%20offers%20a%20guidance%20for%0Apractitioners%20and%20advances%20the%20development%20of%20safer%20and%20more%20reliable%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09016v1&entry.124074799=Read"},
{"title": "Understanding Dynamic Scenes in Ego Centric 4D Point Clouds", "author": "Junsheng Huang and Shengyu Hao and Bocheng Hu and Gaoang Wang", "abstract": "  Understanding dynamic 4D scenes from an egocentric perspective-modeling\nchanges in 3D spatial structure over time-is crucial for human-machine\ninteraction, autonomous navigation, and embodied intelligence. While existing\negocentric datasets contain dynamic scenes, they lack unified 4D annotations\nand task-driven evaluation protocols for fine-grained spatio-temporal\nreasoning, especially on motion of objects and human, together with their\ninteractions. To address this gap, we introduce EgoDynamic4D, a novel QA\nbenchmark on highly dynamic scenes, comprising RGB-D video, camera poses,\nglobally unique instance masks, and 4D bounding boxes. We construct 927K QA\npairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,\nstep-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks covering\nagent motion, human-object interaction, trajectory prediction, relation\nunderstanding, and temporal-causal reasoning, with fine-grained,\nmultidimensional metrics. To tackle these tasks, we propose an end-to-end\nspatio-temporal reasoning framework that unifies dynamic and static scene\ninformation, using instance-aware feature encoding, time and camera encoding,\nand spatially adaptive down-sampling to compress large 4D scenes into token\nsequences manageable by LLMs. Experiments on EgoDynamic4D show that our method\nconsistently outperforms baselines, validating the effectiveness of multimodal\ntemporal modeling for egocentric dynamic scene understanding.\n", "link": "http://arxiv.org/abs/2508.07251v2", "date": "2025-08-12", "relevancy": 2.5472, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.641}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.641}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Dynamic%20Scenes%20in%20Ego%20Centric%204D%20Point%20Clouds&body=Title%3A%20Understanding%20Dynamic%20Scenes%20in%20Ego%20Centric%204D%20Point%20Clouds%0AAuthor%3A%20Junsheng%20Huang%20and%20Shengyu%20Hao%20and%20Bocheng%20Hu%20and%20Gaoang%20Wang%0AAbstract%3A%20%20%20Understanding%20dynamic%204D%20scenes%20from%20an%20egocentric%20perspective-modeling%0Achanges%20in%203D%20spatial%20structure%20over%20time-is%20crucial%20for%20human-machine%0Ainteraction%2C%20autonomous%20navigation%2C%20and%20embodied%20intelligence.%20While%20existing%0Aegocentric%20datasets%20contain%20dynamic%20scenes%2C%20they%20lack%20unified%204D%20annotations%0Aand%20task-driven%20evaluation%20protocols%20for%20fine-grained%20spatio-temporal%0Areasoning%2C%20especially%20on%20motion%20of%20objects%20and%20human%2C%20together%20with%20their%0Ainteractions.%20To%20address%20this%20gap%2C%20we%20introduce%20EgoDynamic4D%2C%20a%20novel%20QA%0Abenchmark%20on%20highly%20dynamic%20scenes%2C%20comprising%20RGB-D%20video%2C%20camera%20poses%2C%0Aglobally%20unique%20instance%20masks%2C%20and%204D%20bounding%20boxes.%20We%20construct%20927K%20QA%0Apairs%20accompanied%20by%20explicit%20Chain-of-Thought%20%28CoT%29%2C%20enabling%20verifiable%2C%0Astep-by-step%20spatio-temporal%20reasoning.%20We%20design%2012%20dynamic%20QA%20tasks%20covering%0Aagent%20motion%2C%20human-object%20interaction%2C%20trajectory%20prediction%2C%20relation%0Aunderstanding%2C%20and%20temporal-causal%20reasoning%2C%20with%20fine-grained%2C%0Amultidimensional%20metrics.%20To%20tackle%20these%20tasks%2C%20we%20propose%20an%20end-to-end%0Aspatio-temporal%20reasoning%20framework%20that%20unifies%20dynamic%20and%20static%20scene%0Ainformation%2C%20using%20instance-aware%20feature%20encoding%2C%20time%20and%20camera%20encoding%2C%0Aand%20spatially%20adaptive%20down-sampling%20to%20compress%20large%204D%20scenes%20into%20token%0Asequences%20manageable%20by%20LLMs.%20Experiments%20on%20EgoDynamic4D%20show%20that%20our%20method%0Aconsistently%20outperforms%20baselines%2C%20validating%20the%20effectiveness%20of%20multimodal%0Atemporal%20modeling%20for%20egocentric%20dynamic%20scene%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07251v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Dynamic%2520Scenes%2520in%2520Ego%2520Centric%25204D%2520Point%2520Clouds%26entry.906535625%3DJunsheng%2520Huang%2520and%2520Shengyu%2520Hao%2520and%2520Bocheng%2520Hu%2520and%2520Gaoang%2520Wang%26entry.1292438233%3D%2520%2520Understanding%2520dynamic%25204D%2520scenes%2520from%2520an%2520egocentric%2520perspective-modeling%250Achanges%2520in%25203D%2520spatial%2520structure%2520over%2520time-is%2520crucial%2520for%2520human-machine%250Ainteraction%252C%2520autonomous%2520navigation%252C%2520and%2520embodied%2520intelligence.%2520While%2520existing%250Aegocentric%2520datasets%2520contain%2520dynamic%2520scenes%252C%2520they%2520lack%2520unified%25204D%2520annotations%250Aand%2520task-driven%2520evaluation%2520protocols%2520for%2520fine-grained%2520spatio-temporal%250Areasoning%252C%2520especially%2520on%2520motion%2520of%2520objects%2520and%2520human%252C%2520together%2520with%2520their%250Ainteractions.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520EgoDynamic4D%252C%2520a%2520novel%2520QA%250Abenchmark%2520on%2520highly%2520dynamic%2520scenes%252C%2520comprising%2520RGB-D%2520video%252C%2520camera%2520poses%252C%250Aglobally%2520unique%2520instance%2520masks%252C%2520and%25204D%2520bounding%2520boxes.%2520We%2520construct%2520927K%2520QA%250Apairs%2520accompanied%2520by%2520explicit%2520Chain-of-Thought%2520%2528CoT%2529%252C%2520enabling%2520verifiable%252C%250Astep-by-step%2520spatio-temporal%2520reasoning.%2520We%2520design%252012%2520dynamic%2520QA%2520tasks%2520covering%250Aagent%2520motion%252C%2520human-object%2520interaction%252C%2520trajectory%2520prediction%252C%2520relation%250Aunderstanding%252C%2520and%2520temporal-causal%2520reasoning%252C%2520with%2520fine-grained%252C%250Amultidimensional%2520metrics.%2520To%2520tackle%2520these%2520tasks%252C%2520we%2520propose%2520an%2520end-to-end%250Aspatio-temporal%2520reasoning%2520framework%2520that%2520unifies%2520dynamic%2520and%2520static%2520scene%250Ainformation%252C%2520using%2520instance-aware%2520feature%2520encoding%252C%2520time%2520and%2520camera%2520encoding%252C%250Aand%2520spatially%2520adaptive%2520down-sampling%2520to%2520compress%2520large%25204D%2520scenes%2520into%2520token%250Asequences%2520manageable%2520by%2520LLMs.%2520Experiments%2520on%2520EgoDynamic4D%2520show%2520that%2520our%2520method%250Aconsistently%2520outperforms%2520baselines%252C%2520validating%2520the%2520effectiveness%2520of%2520multimodal%250Atemporal%2520modeling%2520for%2520egocentric%2520dynamic%2520scene%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07251v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Dynamic%20Scenes%20in%20Ego%20Centric%204D%20Point%20Clouds&entry.906535625=Junsheng%20Huang%20and%20Shengyu%20Hao%20and%20Bocheng%20Hu%20and%20Gaoang%20Wang&entry.1292438233=%20%20Understanding%20dynamic%204D%20scenes%20from%20an%20egocentric%20perspective-modeling%0Achanges%20in%203D%20spatial%20structure%20over%20time-is%20crucial%20for%20human-machine%0Ainteraction%2C%20autonomous%20navigation%2C%20and%20embodied%20intelligence.%20While%20existing%0Aegocentric%20datasets%20contain%20dynamic%20scenes%2C%20they%20lack%20unified%204D%20annotations%0Aand%20task-driven%20evaluation%20protocols%20for%20fine-grained%20spatio-temporal%0Areasoning%2C%20especially%20on%20motion%20of%20objects%20and%20human%2C%20together%20with%20their%0Ainteractions.%20To%20address%20this%20gap%2C%20we%20introduce%20EgoDynamic4D%2C%20a%20novel%20QA%0Abenchmark%20on%20highly%20dynamic%20scenes%2C%20comprising%20RGB-D%20video%2C%20camera%20poses%2C%0Aglobally%20unique%20instance%20masks%2C%20and%204D%20bounding%20boxes.%20We%20construct%20927K%20QA%0Apairs%20accompanied%20by%20explicit%20Chain-of-Thought%20%28CoT%29%2C%20enabling%20verifiable%2C%0Astep-by-step%20spatio-temporal%20reasoning.%20We%20design%2012%20dynamic%20QA%20tasks%20covering%0Aagent%20motion%2C%20human-object%20interaction%2C%20trajectory%20prediction%2C%20relation%0Aunderstanding%2C%20and%20temporal-causal%20reasoning%2C%20with%20fine-grained%2C%0Amultidimensional%20metrics.%20To%20tackle%20these%20tasks%2C%20we%20propose%20an%20end-to-end%0Aspatio-temporal%20reasoning%20framework%20that%20unifies%20dynamic%20and%20static%20scene%0Ainformation%2C%20using%20instance-aware%20feature%20encoding%2C%20time%20and%20camera%20encoding%2C%0Aand%20spatially%20adaptive%20down-sampling%20to%20compress%20large%204D%20scenes%20into%20token%0Asequences%20manageable%20by%20LLMs.%20Experiments%20on%20EgoDynamic4D%20show%20that%20our%20method%0Aconsistently%20outperforms%20baselines%2C%20validating%20the%20effectiveness%20of%20multimodal%0Atemporal%20modeling%20for%20egocentric%20dynamic%20scene%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07251v2&entry.124074799=Read"},
{"title": "ViStoryBench: Comprehensive Benchmark Suite for Story Visualization", "author": "Cailin Zhuang and Ailin Huang and Wei Cheng and Jingwei Wu and Yaoqi Hu and Jiaqi Liao and Hongyuan Wang and Xinyao Liao and Weiwei Cai and Hengyuan Xu and Xuanyang Zhang and Xianfang Zeng and Zhewei Huang and Gang Yu and Chi Zhang", "abstract": "  Story visualization aims to generate coherent image sequences that faithfully\ndepict a narrative and align with character references. Despite progress in\ngenerative models, existing benchmarks are narrow in scope, often limited to\nshort prompts, no character reference, or single-image cases, and fall short of\nreal-world storytelling complexity. This hinders a nuanced understanding of\nmodel capabilities and limitations. We present ViStoryBench, a comprehensive\nbenchmark designed to evaluate story visualization models across diverse\nnarrative structures, visual styles, and character settings. The benchmark\nfeatures richly annotated multi-shot scripts derived from curated stories\nspanning literature, film, and folklore. Large language models assist in story\nsummarization and script generation, with all outputs verified by humans to\nensure coherence and fidelity. Character references are carefully curated to\nmaintain intra-story consistency across varying artistic styles. To enable\nthorough evaluation, ViStoryBench introduces a set of automated metrics that\nassess character consistency, style similarity, prompt adherence, aesthetic\nquality, and generation artifacts such as copy-paste behavior. These metrics\nare validated through human studies, and used to benchmark a broad range of\nopen-source and commercial models. ViStoryBench offers a high-fidelity,\nmulti-dimensional evaluation suite that facilitates systematic analysis and\nfosters future progress in visual storytelling.\n", "link": "http://arxiv.org/abs/2505.24862v3", "date": "2025-08-12", "relevancy": 2.5234, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5352}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5352}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViStoryBench%3A%20Comprehensive%20Benchmark%20Suite%20for%20Story%20Visualization&body=Title%3A%20ViStoryBench%3A%20Comprehensive%20Benchmark%20Suite%20for%20Story%20Visualization%0AAuthor%3A%20Cailin%20Zhuang%20and%20Ailin%20Huang%20and%20Wei%20Cheng%20and%20Jingwei%20Wu%20and%20Yaoqi%20Hu%20and%20Jiaqi%20Liao%20and%20Hongyuan%20Wang%20and%20Xinyao%20Liao%20and%20Weiwei%20Cai%20and%20Hengyuan%20Xu%20and%20Xuanyang%20Zhang%20and%20Xianfang%20Zeng%20and%20Zhewei%20Huang%20and%20Gang%20Yu%20and%20Chi%20Zhang%0AAbstract%3A%20%20%20Story%20visualization%20aims%20to%20generate%20coherent%20image%20sequences%20that%20faithfully%0Adepict%20a%20narrative%20and%20align%20with%20character%20references.%20Despite%20progress%20in%0Agenerative%20models%2C%20existing%20benchmarks%20are%20narrow%20in%20scope%2C%20often%20limited%20to%0Ashort%20prompts%2C%20no%20character%20reference%2C%20or%20single-image%20cases%2C%20and%20fall%20short%20of%0Areal-world%20storytelling%20complexity.%20This%20hinders%20a%20nuanced%20understanding%20of%0Amodel%20capabilities%20and%20limitations.%20We%20present%20ViStoryBench%2C%20a%20comprehensive%0Abenchmark%20designed%20to%20evaluate%20story%20visualization%20models%20across%20diverse%0Anarrative%20structures%2C%20visual%20styles%2C%20and%20character%20settings.%20The%20benchmark%0Afeatures%20richly%20annotated%20multi-shot%20scripts%20derived%20from%20curated%20stories%0Aspanning%20literature%2C%20film%2C%20and%20folklore.%20Large%20language%20models%20assist%20in%20story%0Asummarization%20and%20script%20generation%2C%20with%20all%20outputs%20verified%20by%20humans%20to%0Aensure%20coherence%20and%20fidelity.%20Character%20references%20are%20carefully%20curated%20to%0Amaintain%20intra-story%20consistency%20across%20varying%20artistic%20styles.%20To%20enable%0Athorough%20evaluation%2C%20ViStoryBench%20introduces%20a%20set%20of%20automated%20metrics%20that%0Aassess%20character%20consistency%2C%20style%20similarity%2C%20prompt%20adherence%2C%20aesthetic%0Aquality%2C%20and%20generation%20artifacts%20such%20as%20copy-paste%20behavior.%20These%20metrics%0Aare%20validated%20through%20human%20studies%2C%20and%20used%20to%20benchmark%20a%20broad%20range%20of%0Aopen-source%20and%20commercial%20models.%20ViStoryBench%20offers%20a%20high-fidelity%2C%0Amulti-dimensional%20evaluation%20suite%20that%20facilitates%20systematic%20analysis%20and%0Afosters%20future%20progress%20in%20visual%20storytelling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24862v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViStoryBench%253A%2520Comprehensive%2520Benchmark%2520Suite%2520for%2520Story%2520Visualization%26entry.906535625%3DCailin%2520Zhuang%2520and%2520Ailin%2520Huang%2520and%2520Wei%2520Cheng%2520and%2520Jingwei%2520Wu%2520and%2520Yaoqi%2520Hu%2520and%2520Jiaqi%2520Liao%2520and%2520Hongyuan%2520Wang%2520and%2520Xinyao%2520Liao%2520and%2520Weiwei%2520Cai%2520and%2520Hengyuan%2520Xu%2520and%2520Xuanyang%2520Zhang%2520and%2520Xianfang%2520Zeng%2520and%2520Zhewei%2520Huang%2520and%2520Gang%2520Yu%2520and%2520Chi%2520Zhang%26entry.1292438233%3D%2520%2520Story%2520visualization%2520aims%2520to%2520generate%2520coherent%2520image%2520sequences%2520that%2520faithfully%250Adepict%2520a%2520narrative%2520and%2520align%2520with%2520character%2520references.%2520Despite%2520progress%2520in%250Agenerative%2520models%252C%2520existing%2520benchmarks%2520are%2520narrow%2520in%2520scope%252C%2520often%2520limited%2520to%250Ashort%2520prompts%252C%2520no%2520character%2520reference%252C%2520or%2520single-image%2520cases%252C%2520and%2520fall%2520short%2520of%250Areal-world%2520storytelling%2520complexity.%2520This%2520hinders%2520a%2520nuanced%2520understanding%2520of%250Amodel%2520capabilities%2520and%2520limitations.%2520We%2520present%2520ViStoryBench%252C%2520a%2520comprehensive%250Abenchmark%2520designed%2520to%2520evaluate%2520story%2520visualization%2520models%2520across%2520diverse%250Anarrative%2520structures%252C%2520visual%2520styles%252C%2520and%2520character%2520settings.%2520The%2520benchmark%250Afeatures%2520richly%2520annotated%2520multi-shot%2520scripts%2520derived%2520from%2520curated%2520stories%250Aspanning%2520literature%252C%2520film%252C%2520and%2520folklore.%2520Large%2520language%2520models%2520assist%2520in%2520story%250Asummarization%2520and%2520script%2520generation%252C%2520with%2520all%2520outputs%2520verified%2520by%2520humans%2520to%250Aensure%2520coherence%2520and%2520fidelity.%2520Character%2520references%2520are%2520carefully%2520curated%2520to%250Amaintain%2520intra-story%2520consistency%2520across%2520varying%2520artistic%2520styles.%2520To%2520enable%250Athorough%2520evaluation%252C%2520ViStoryBench%2520introduces%2520a%2520set%2520of%2520automated%2520metrics%2520that%250Aassess%2520character%2520consistency%252C%2520style%2520similarity%252C%2520prompt%2520adherence%252C%2520aesthetic%250Aquality%252C%2520and%2520generation%2520artifacts%2520such%2520as%2520copy-paste%2520behavior.%2520These%2520metrics%250Aare%2520validated%2520through%2520human%2520studies%252C%2520and%2520used%2520to%2520benchmark%2520a%2520broad%2520range%2520of%250Aopen-source%2520and%2520commercial%2520models.%2520ViStoryBench%2520offers%2520a%2520high-fidelity%252C%250Amulti-dimensional%2520evaluation%2520suite%2520that%2520facilitates%2520systematic%2520analysis%2520and%250Afosters%2520future%2520progress%2520in%2520visual%2520storytelling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24862v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViStoryBench%3A%20Comprehensive%20Benchmark%20Suite%20for%20Story%20Visualization&entry.906535625=Cailin%20Zhuang%20and%20Ailin%20Huang%20and%20Wei%20Cheng%20and%20Jingwei%20Wu%20and%20Yaoqi%20Hu%20and%20Jiaqi%20Liao%20and%20Hongyuan%20Wang%20and%20Xinyao%20Liao%20and%20Weiwei%20Cai%20and%20Hengyuan%20Xu%20and%20Xuanyang%20Zhang%20and%20Xianfang%20Zeng%20and%20Zhewei%20Huang%20and%20Gang%20Yu%20and%20Chi%20Zhang&entry.1292438233=%20%20Story%20visualization%20aims%20to%20generate%20coherent%20image%20sequences%20that%20faithfully%0Adepict%20a%20narrative%20and%20align%20with%20character%20references.%20Despite%20progress%20in%0Agenerative%20models%2C%20existing%20benchmarks%20are%20narrow%20in%20scope%2C%20often%20limited%20to%0Ashort%20prompts%2C%20no%20character%20reference%2C%20or%20single-image%20cases%2C%20and%20fall%20short%20of%0Areal-world%20storytelling%20complexity.%20This%20hinders%20a%20nuanced%20understanding%20of%0Amodel%20capabilities%20and%20limitations.%20We%20present%20ViStoryBench%2C%20a%20comprehensive%0Abenchmark%20designed%20to%20evaluate%20story%20visualization%20models%20across%20diverse%0Anarrative%20structures%2C%20visual%20styles%2C%20and%20character%20settings.%20The%20benchmark%0Afeatures%20richly%20annotated%20multi-shot%20scripts%20derived%20from%20curated%20stories%0Aspanning%20literature%2C%20film%2C%20and%20folklore.%20Large%20language%20models%20assist%20in%20story%0Asummarization%20and%20script%20generation%2C%20with%20all%20outputs%20verified%20by%20humans%20to%0Aensure%20coherence%20and%20fidelity.%20Character%20references%20are%20carefully%20curated%20to%0Amaintain%20intra-story%20consistency%20across%20varying%20artistic%20styles.%20To%20enable%0Athorough%20evaluation%2C%20ViStoryBench%20introduces%20a%20set%20of%20automated%20metrics%20that%0Aassess%20character%20consistency%2C%20style%20similarity%2C%20prompt%20adherence%2C%20aesthetic%0Aquality%2C%20and%20generation%20artifacts%20such%20as%20copy-paste%20behavior.%20These%20metrics%0Aare%20validated%20through%20human%20studies%2C%20and%20used%20to%20benchmark%20a%20broad%20range%20of%0Aopen-source%20and%20commercial%20models.%20ViStoryBench%20offers%20a%20high-fidelity%2C%0Amulti-dimensional%20evaluation%20suite%20that%20facilitates%20systematic%20analysis%20and%0Afosters%20future%20progress%20in%20visual%20storytelling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24862v3&entry.124074799=Read"},
{"title": "OE3DIS: Open-Ended 3D Point Cloud Instance Segmentation", "author": "Phuc D. A. Nguyen and Minh Luu and Anh Tran and Cuong Pham and Khoi Nguyen", "abstract": "  Open-Vocab 3D Instance Segmentation methods (OV-3DIS) have recently\ndemonstrated their ability to generalize to unseen objects. However, these\nmethods still depend on predefined class names during testing, restricting the\nautonomy of agents. To mitigate this constraint, we propose a novel problem\ntermed Open-Ended 3D Instance Segmentation (OE-3DIS), which eliminates the\nnecessity for predefined class names during testing. Moreover, we contribute a\ncomprehensive set of strong baselines, derived from OV-3DIS approaches and\nleveraging 2D Multimodal Large Language Models. To assess the performance of\nour OE-3DIS system, we introduce a novel Open-Ended score, evaluating both the\nsemantic and geometric quality of predicted masks and their associated class\nnames, alongside the standard AP score. Our approach demonstrates significant\nperformance improvements over the baselines on the ScanNet200 and ScanNet++\ndatasets. Remarkably, our method surpasses the performance of Open3DIS, the\ncurrent state-of-the-art method in OV-3DIS, even in the absence of ground-truth\nobject class names.\n", "link": "http://arxiv.org/abs/2408.11747v2", "date": "2025-08-12", "relevancy": 2.4979, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6303}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6303}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OE3DIS%3A%20Open-Ended%203D%20Point%20Cloud%20Instance%20Segmentation&body=Title%3A%20OE3DIS%3A%20Open-Ended%203D%20Point%20Cloud%20Instance%20Segmentation%0AAuthor%3A%20Phuc%20D.%20A.%20Nguyen%20and%20Minh%20Luu%20and%20Anh%20Tran%20and%20Cuong%20Pham%20and%20Khoi%20Nguyen%0AAbstract%3A%20%20%20Open-Vocab%203D%20Instance%20Segmentation%20methods%20%28OV-3DIS%29%20have%20recently%0Ademonstrated%20their%20ability%20to%20generalize%20to%20unseen%20objects.%20However%2C%20these%0Amethods%20still%20depend%20on%20predefined%20class%20names%20during%20testing%2C%20restricting%20the%0Aautonomy%20of%20agents.%20To%20mitigate%20this%20constraint%2C%20we%20propose%20a%20novel%20problem%0Atermed%20Open-Ended%203D%20Instance%20Segmentation%20%28OE-3DIS%29%2C%20which%20eliminates%20the%0Anecessity%20for%20predefined%20class%20names%20during%20testing.%20Moreover%2C%20we%20contribute%20a%0Acomprehensive%20set%20of%20strong%20baselines%2C%20derived%20from%20OV-3DIS%20approaches%20and%0Aleveraging%202D%20Multimodal%20Large%20Language%20Models.%20To%20assess%20the%20performance%20of%0Aour%20OE-3DIS%20system%2C%20we%20introduce%20a%20novel%20Open-Ended%20score%2C%20evaluating%20both%20the%0Asemantic%20and%20geometric%20quality%20of%20predicted%20masks%20and%20their%20associated%20class%0Anames%2C%20alongside%20the%20standard%20AP%20score.%20Our%20approach%20demonstrates%20significant%0Aperformance%20improvements%20over%20the%20baselines%20on%20the%20ScanNet200%20and%20ScanNet%2B%2B%0Adatasets.%20Remarkably%2C%20our%20method%20surpasses%20the%20performance%20of%20Open3DIS%2C%20the%0Acurrent%20state-of-the-art%20method%20in%20OV-3DIS%2C%20even%20in%20the%20absence%20of%20ground-truth%0Aobject%20class%20names.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11747v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOE3DIS%253A%2520Open-Ended%25203D%2520Point%2520Cloud%2520Instance%2520Segmentation%26entry.906535625%3DPhuc%2520D.%2520A.%2520Nguyen%2520and%2520Minh%2520Luu%2520and%2520Anh%2520Tran%2520and%2520Cuong%2520Pham%2520and%2520Khoi%2520Nguyen%26entry.1292438233%3D%2520%2520Open-Vocab%25203D%2520Instance%2520Segmentation%2520methods%2520%2528OV-3DIS%2529%2520have%2520recently%250Ademonstrated%2520their%2520ability%2520to%2520generalize%2520to%2520unseen%2520objects.%2520However%252C%2520these%250Amethods%2520still%2520depend%2520on%2520predefined%2520class%2520names%2520during%2520testing%252C%2520restricting%2520the%250Aautonomy%2520of%2520agents.%2520To%2520mitigate%2520this%2520constraint%252C%2520we%2520propose%2520a%2520novel%2520problem%250Atermed%2520Open-Ended%25203D%2520Instance%2520Segmentation%2520%2528OE-3DIS%2529%252C%2520which%2520eliminates%2520the%250Anecessity%2520for%2520predefined%2520class%2520names%2520during%2520testing.%2520Moreover%252C%2520we%2520contribute%2520a%250Acomprehensive%2520set%2520of%2520strong%2520baselines%252C%2520derived%2520from%2520OV-3DIS%2520approaches%2520and%250Aleveraging%25202D%2520Multimodal%2520Large%2520Language%2520Models.%2520To%2520assess%2520the%2520performance%2520of%250Aour%2520OE-3DIS%2520system%252C%2520we%2520introduce%2520a%2520novel%2520Open-Ended%2520score%252C%2520evaluating%2520both%2520the%250Asemantic%2520and%2520geometric%2520quality%2520of%2520predicted%2520masks%2520and%2520their%2520associated%2520class%250Anames%252C%2520alongside%2520the%2520standard%2520AP%2520score.%2520Our%2520approach%2520demonstrates%2520significant%250Aperformance%2520improvements%2520over%2520the%2520baselines%2520on%2520the%2520ScanNet200%2520and%2520ScanNet%252B%252B%250Adatasets.%2520Remarkably%252C%2520our%2520method%2520surpasses%2520the%2520performance%2520of%2520Open3DIS%252C%2520the%250Acurrent%2520state-of-the-art%2520method%2520in%2520OV-3DIS%252C%2520even%2520in%2520the%2520absence%2520of%2520ground-truth%250Aobject%2520class%2520names.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11747v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OE3DIS%3A%20Open-Ended%203D%20Point%20Cloud%20Instance%20Segmentation&entry.906535625=Phuc%20D.%20A.%20Nguyen%20and%20Minh%20Luu%20and%20Anh%20Tran%20and%20Cuong%20Pham%20and%20Khoi%20Nguyen&entry.1292438233=%20%20Open-Vocab%203D%20Instance%20Segmentation%20methods%20%28OV-3DIS%29%20have%20recently%0Ademonstrated%20their%20ability%20to%20generalize%20to%20unseen%20objects.%20However%2C%20these%0Amethods%20still%20depend%20on%20predefined%20class%20names%20during%20testing%2C%20restricting%20the%0Aautonomy%20of%20agents.%20To%20mitigate%20this%20constraint%2C%20we%20propose%20a%20novel%20problem%0Atermed%20Open-Ended%203D%20Instance%20Segmentation%20%28OE-3DIS%29%2C%20which%20eliminates%20the%0Anecessity%20for%20predefined%20class%20names%20during%20testing.%20Moreover%2C%20we%20contribute%20a%0Acomprehensive%20set%20of%20strong%20baselines%2C%20derived%20from%20OV-3DIS%20approaches%20and%0Aleveraging%202D%20Multimodal%20Large%20Language%20Models.%20To%20assess%20the%20performance%20of%0Aour%20OE-3DIS%20system%2C%20we%20introduce%20a%20novel%20Open-Ended%20score%2C%20evaluating%20both%20the%0Asemantic%20and%20geometric%20quality%20of%20predicted%20masks%20and%20their%20associated%20class%0Anames%2C%20alongside%20the%20standard%20AP%20score.%20Our%20approach%20demonstrates%20significant%0Aperformance%20improvements%20over%20the%20baselines%20on%20the%20ScanNet200%20and%20ScanNet%2B%2B%0Adatasets.%20Remarkably%2C%20our%20method%20surpasses%20the%20performance%20of%20Open3DIS%2C%20the%0Acurrent%20state-of-the-art%20method%20in%20OV-3DIS%2C%20even%20in%20the%20absence%20of%20ground-truth%0Aobject%20class%20names.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11747v2&entry.124074799=Read"},
{"title": "LM-MCVT: A Lightweight Multi-modal Multi-view Convolutional-Vision\n  Transformer Approach for 3D Object Recognition", "author": "Songsong Xiong and Hamidreza Kasaei", "abstract": "  In human-centered environments such as restaurants, homes, and warehouses,\nrobots often face challenges in accurately recognizing 3D objects. These\nchallenges stem from the complexity and variability of these environments,\nincluding diverse object shapes. In this paper, we propose a novel Lightweight\nMulti-modal Multi-view Convolutional-Vision Transformer network (LM-MCVT) to\nenhance 3D object recognition in robotic applications. Our approach leverages\nthe Globally Entropy-based Embeddings Fusion (GEEF) method to integrate\nmulti-views efficiently. The LM-MCVT architecture incorporates pre- and\nmid-level convolutional encoders and local and global transformers to enhance\nfeature extraction and recognition accuracy. We evaluate our method on the\nsynthetic ModelNet40 dataset and achieve a recognition accuracy of 95.6% using\na four-view setup, surpassing existing state-of-the-art methods. To further\nvalidate its effectiveness, we conduct 5-fold cross-validation on the\nreal-world OmniObject3D dataset using the same configuration. Results\nconsistently show superior performance, demonstrating the method's robustness\nin 3D object recognition across synthetic and real-world 3D data.\n", "link": "http://arxiv.org/abs/2504.19256v2", "date": "2025-08-12", "relevancy": 2.453, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6325}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6184}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LM-MCVT%3A%20A%20Lightweight%20Multi-modal%20Multi-view%20Convolutional-Vision%0A%20%20Transformer%20Approach%20for%203D%20Object%20Recognition&body=Title%3A%20LM-MCVT%3A%20A%20Lightweight%20Multi-modal%20Multi-view%20Convolutional-Vision%0A%20%20Transformer%20Approach%20for%203D%20Object%20Recognition%0AAuthor%3A%20Songsong%20Xiong%20and%20Hamidreza%20Kasaei%0AAbstract%3A%20%20%20In%20human-centered%20environments%20such%20as%20restaurants%2C%20homes%2C%20and%20warehouses%2C%0Arobots%20often%20face%20challenges%20in%20accurately%20recognizing%203D%20objects.%20These%0Achallenges%20stem%20from%20the%20complexity%20and%20variability%20of%20these%20environments%2C%0Aincluding%20diverse%20object%20shapes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Lightweight%0AMulti-modal%20Multi-view%20Convolutional-Vision%20Transformer%20network%20%28LM-MCVT%29%20to%0Aenhance%203D%20object%20recognition%20in%20robotic%20applications.%20Our%20approach%20leverages%0Athe%20Globally%20Entropy-based%20Embeddings%20Fusion%20%28GEEF%29%20method%20to%20integrate%0Amulti-views%20efficiently.%20The%20LM-MCVT%20architecture%20incorporates%20pre-%20and%0Amid-level%20convolutional%20encoders%20and%20local%20and%20global%20transformers%20to%20enhance%0Afeature%20extraction%20and%20recognition%20accuracy.%20We%20evaluate%20our%20method%20on%20the%0Asynthetic%20ModelNet40%20dataset%20and%20achieve%20a%20recognition%20accuracy%20of%2095.6%25%20using%0Aa%20four-view%20setup%2C%20surpassing%20existing%20state-of-the-art%20methods.%20To%20further%0Avalidate%20its%20effectiveness%2C%20we%20conduct%205-fold%20cross-validation%20on%20the%0Areal-world%20OmniObject3D%20dataset%20using%20the%20same%20configuration.%20Results%0Aconsistently%20show%20superior%20performance%2C%20demonstrating%20the%20method%27s%20robustness%0Ain%203D%20object%20recognition%20across%20synthetic%20and%20real-world%203D%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19256v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLM-MCVT%253A%2520A%2520Lightweight%2520Multi-modal%2520Multi-view%2520Convolutional-Vision%250A%2520%2520Transformer%2520Approach%2520for%25203D%2520Object%2520Recognition%26entry.906535625%3DSongsong%2520Xiong%2520and%2520Hamidreza%2520Kasaei%26entry.1292438233%3D%2520%2520In%2520human-centered%2520environments%2520such%2520as%2520restaurants%252C%2520homes%252C%2520and%2520warehouses%252C%250Arobots%2520often%2520face%2520challenges%2520in%2520accurately%2520recognizing%25203D%2520objects.%2520These%250Achallenges%2520stem%2520from%2520the%2520complexity%2520and%2520variability%2520of%2520these%2520environments%252C%250Aincluding%2520diverse%2520object%2520shapes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Lightweight%250AMulti-modal%2520Multi-view%2520Convolutional-Vision%2520Transformer%2520network%2520%2528LM-MCVT%2529%2520to%250Aenhance%25203D%2520object%2520recognition%2520in%2520robotic%2520applications.%2520Our%2520approach%2520leverages%250Athe%2520Globally%2520Entropy-based%2520Embeddings%2520Fusion%2520%2528GEEF%2529%2520method%2520to%2520integrate%250Amulti-views%2520efficiently.%2520The%2520LM-MCVT%2520architecture%2520incorporates%2520pre-%2520and%250Amid-level%2520convolutional%2520encoders%2520and%2520local%2520and%2520global%2520transformers%2520to%2520enhance%250Afeature%2520extraction%2520and%2520recognition%2520accuracy.%2520We%2520evaluate%2520our%2520method%2520on%2520the%250Asynthetic%2520ModelNet40%2520dataset%2520and%2520achieve%2520a%2520recognition%2520accuracy%2520of%252095.6%2525%2520using%250Aa%2520four-view%2520setup%252C%2520surpassing%2520existing%2520state-of-the-art%2520methods.%2520To%2520further%250Avalidate%2520its%2520effectiveness%252C%2520we%2520conduct%25205-fold%2520cross-validation%2520on%2520the%250Areal-world%2520OmniObject3D%2520dataset%2520using%2520the%2520same%2520configuration.%2520Results%250Aconsistently%2520show%2520superior%2520performance%252C%2520demonstrating%2520the%2520method%2527s%2520robustness%250Ain%25203D%2520object%2520recognition%2520across%2520synthetic%2520and%2520real-world%25203D%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19256v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LM-MCVT%3A%20A%20Lightweight%20Multi-modal%20Multi-view%20Convolutional-Vision%0A%20%20Transformer%20Approach%20for%203D%20Object%20Recognition&entry.906535625=Songsong%20Xiong%20and%20Hamidreza%20Kasaei&entry.1292438233=%20%20In%20human-centered%20environments%20such%20as%20restaurants%2C%20homes%2C%20and%20warehouses%2C%0Arobots%20often%20face%20challenges%20in%20accurately%20recognizing%203D%20objects.%20These%0Achallenges%20stem%20from%20the%20complexity%20and%20variability%20of%20these%20environments%2C%0Aincluding%20diverse%20object%20shapes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Lightweight%0AMulti-modal%20Multi-view%20Convolutional-Vision%20Transformer%20network%20%28LM-MCVT%29%20to%0Aenhance%203D%20object%20recognition%20in%20robotic%20applications.%20Our%20approach%20leverages%0Athe%20Globally%20Entropy-based%20Embeddings%20Fusion%20%28GEEF%29%20method%20to%20integrate%0Amulti-views%20efficiently.%20The%20LM-MCVT%20architecture%20incorporates%20pre-%20and%0Amid-level%20convolutional%20encoders%20and%20local%20and%20global%20transformers%20to%20enhance%0Afeature%20extraction%20and%20recognition%20accuracy.%20We%20evaluate%20our%20method%20on%20the%0Asynthetic%20ModelNet40%20dataset%20and%20achieve%20a%20recognition%20accuracy%20of%2095.6%25%20using%0Aa%20four-view%20setup%2C%20surpassing%20existing%20state-of-the-art%20methods.%20To%20further%0Avalidate%20its%20effectiveness%2C%20we%20conduct%205-fold%20cross-validation%20on%20the%0Areal-world%20OmniObject3D%20dataset%20using%20the%20same%20configuration.%20Results%0Aconsistently%20show%20superior%20performance%2C%20demonstrating%20the%20method%27s%20robustness%0Ain%203D%20object%20recognition%20across%20synthetic%20and%20real-world%203D%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19256v2&entry.124074799=Read"},
{"title": "Saturation Self-Organizing Map", "author": "Igor Urbanik and Pawe\u0142 Gajewski", "abstract": "  Continual learning poses a fundamental challenge for neural systems, which\noften suffer from catastrophic forgetting when exposed to sequential tasks.\nSelf-Organizing Maps (SOMs), despite their interpretability and efficiency, are\nnot immune to this issue. In this paper, we introduce Saturation\nSelf-Organizing Maps (SatSOM)-an extension of SOMs designed to improve\nknowledge retention in continual learning scenarios. SatSOM incorporates a\nnovel saturation mechanism that gradually reduces the learning rate and\nneighborhood radius of neurons as they accumulate information. This effectively\nfreezes well-trained neurons and redirects learning to underutilized areas of\nthe map.\n", "link": "http://arxiv.org/abs/2506.10680v3", "date": "2025-08-12", "relevancy": 2.4416, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5012}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4906}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Saturation%20Self-Organizing%20Map&body=Title%3A%20Saturation%20Self-Organizing%20Map%0AAuthor%3A%20Igor%20Urbanik%20and%20Pawe%C5%82%20Gajewski%0AAbstract%3A%20%20%20Continual%20learning%20poses%20a%20fundamental%20challenge%20for%20neural%20systems%2C%20which%0Aoften%20suffer%20from%20catastrophic%20forgetting%20when%20exposed%20to%20sequential%20tasks.%0ASelf-Organizing%20Maps%20%28SOMs%29%2C%20despite%20their%20interpretability%20and%20efficiency%2C%20are%0Anot%20immune%20to%20this%20issue.%20In%20this%20paper%2C%20we%20introduce%20Saturation%0ASelf-Organizing%20Maps%20%28SatSOM%29-an%20extension%20of%20SOMs%20designed%20to%20improve%0Aknowledge%20retention%20in%20continual%20learning%20scenarios.%20SatSOM%20incorporates%20a%0Anovel%20saturation%20mechanism%20that%20gradually%20reduces%20the%20learning%20rate%20and%0Aneighborhood%20radius%20of%20neurons%20as%20they%20accumulate%20information.%20This%20effectively%0Afreezes%20well-trained%20neurons%20and%20redirects%20learning%20to%20underutilized%20areas%20of%0Athe%20map.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10680v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSaturation%2520Self-Organizing%2520Map%26entry.906535625%3DIgor%2520Urbanik%2520and%2520Pawe%25C5%2582%2520Gajewski%26entry.1292438233%3D%2520%2520Continual%2520learning%2520poses%2520a%2520fundamental%2520challenge%2520for%2520neural%2520systems%252C%2520which%250Aoften%2520suffer%2520from%2520catastrophic%2520forgetting%2520when%2520exposed%2520to%2520sequential%2520tasks.%250ASelf-Organizing%2520Maps%2520%2528SOMs%2529%252C%2520despite%2520their%2520interpretability%2520and%2520efficiency%252C%2520are%250Anot%2520immune%2520to%2520this%2520issue.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Saturation%250ASelf-Organizing%2520Maps%2520%2528SatSOM%2529-an%2520extension%2520of%2520SOMs%2520designed%2520to%2520improve%250Aknowledge%2520retention%2520in%2520continual%2520learning%2520scenarios.%2520SatSOM%2520incorporates%2520a%250Anovel%2520saturation%2520mechanism%2520that%2520gradually%2520reduces%2520the%2520learning%2520rate%2520and%250Aneighborhood%2520radius%2520of%2520neurons%2520as%2520they%2520accumulate%2520information.%2520This%2520effectively%250Afreezes%2520well-trained%2520neurons%2520and%2520redirects%2520learning%2520to%2520underutilized%2520areas%2520of%250Athe%2520map.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10680v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Saturation%20Self-Organizing%20Map&entry.906535625=Igor%20Urbanik%20and%20Pawe%C5%82%20Gajewski&entry.1292438233=%20%20Continual%20learning%20poses%20a%20fundamental%20challenge%20for%20neural%20systems%2C%20which%0Aoften%20suffer%20from%20catastrophic%20forgetting%20when%20exposed%20to%20sequential%20tasks.%0ASelf-Organizing%20Maps%20%28SOMs%29%2C%20despite%20their%20interpretability%20and%20efficiency%2C%20are%0Anot%20immune%20to%20this%20issue.%20In%20this%20paper%2C%20we%20introduce%20Saturation%0ASelf-Organizing%20Maps%20%28SatSOM%29-an%20extension%20of%20SOMs%20designed%20to%20improve%0Aknowledge%20retention%20in%20continual%20learning%20scenarios.%20SatSOM%20incorporates%20a%0Anovel%20saturation%20mechanism%20that%20gradually%20reduces%20the%20learning%20rate%20and%0Aneighborhood%20radius%20of%20neurons%20as%20they%20accumulate%20information.%20This%20effectively%0Afreezes%20well-trained%20neurons%20and%20redirects%20learning%20to%20underutilized%20areas%20of%0Athe%20map.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10680v3&entry.124074799=Read"},
{"title": "OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting\n  Conditions", "author": "Maxim Popov and Regina Kurkova and Mikhail Iumanov and Jaafar Mahmoud and Sergey Kolyubin", "abstract": "  Open Semantic Mapping (OSM) is a key technology in robotic perception,\ncombining semantic segmentation and SLAM techniques. This paper introduces a\ndynamically configurable and highly automated LLM/LVLM-powered pipeline for\nevaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark).\nThe study focuses on evaluating state-of-the-art semantic mapping algorithms\nunder varying indoor lighting conditions, a critical challenge in indoor\nenvironments. We introduce a novel dataset with simulated RGB-D sequences and\nground truth 3D reconstructions, facilitating the rigorous analysis of mapping\nperformance across different lighting conditions. Through experiments on\nleading models such as ConceptGraphs, BBQ and OpenScene, we evaluate the\nsemantic fidelity of object recognition and segmentation. Additionally, we\nintroduce a Scene Graph evaluation method to analyze the ability of models to\ninterpret semantic structure. The results provide insights into the robustness\nof these models, forming future research directions for developing resilient\nand adaptable robotic systems. Project page is available at\nhttps://be2rlab.github.io/OSMa-Bench/.\n", "link": "http://arxiv.org/abs/2503.10331v2", "date": "2025-08-12", "relevancy": 2.4404, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6172}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6087}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OSMa-Bench%3A%20Evaluating%20Open%20Semantic%20Mapping%20Under%20Varying%20Lighting%0A%20%20Conditions&body=Title%3A%20OSMa-Bench%3A%20Evaluating%20Open%20Semantic%20Mapping%20Under%20Varying%20Lighting%0A%20%20Conditions%0AAuthor%3A%20Maxim%20Popov%20and%20Regina%20Kurkova%20and%20Mikhail%20Iumanov%20and%20Jaafar%20Mahmoud%20and%20Sergey%20Kolyubin%0AAbstract%3A%20%20%20Open%20Semantic%20Mapping%20%28OSM%29%20is%20a%20key%20technology%20in%20robotic%20perception%2C%0Acombining%20semantic%20segmentation%20and%20SLAM%20techniques.%20This%20paper%20introduces%20a%0Adynamically%20configurable%20and%20highly%20automated%20LLM/LVLM-powered%20pipeline%20for%0Aevaluating%20OSM%20solutions%20called%20OSMa-Bench%20%28Open%20Semantic%20Mapping%20Benchmark%29.%0AThe%20study%20focuses%20on%20evaluating%20state-of-the-art%20semantic%20mapping%20algorithms%0Aunder%20varying%20indoor%20lighting%20conditions%2C%20a%20critical%20challenge%20in%20indoor%0Aenvironments.%20We%20introduce%20a%20novel%20dataset%20with%20simulated%20RGB-D%20sequences%20and%0Aground%20truth%203D%20reconstructions%2C%20facilitating%20the%20rigorous%20analysis%20of%20mapping%0Aperformance%20across%20different%20lighting%20conditions.%20Through%20experiments%20on%0Aleading%20models%20such%20as%20ConceptGraphs%2C%20BBQ%20and%20OpenScene%2C%20we%20evaluate%20the%0Asemantic%20fidelity%20of%20object%20recognition%20and%20segmentation.%20Additionally%2C%20we%0Aintroduce%20a%20Scene%20Graph%20evaluation%20method%20to%20analyze%20the%20ability%20of%20models%20to%0Ainterpret%20semantic%20structure.%20The%20results%20provide%20insights%20into%20the%20robustness%0Aof%20these%20models%2C%20forming%20future%20research%20directions%20for%20developing%20resilient%0Aand%20adaptable%20robotic%20systems.%20Project%20page%20is%20available%20at%0Ahttps%3A//be2rlab.github.io/OSMa-Bench/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10331v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOSMa-Bench%253A%2520Evaluating%2520Open%2520Semantic%2520Mapping%2520Under%2520Varying%2520Lighting%250A%2520%2520Conditions%26entry.906535625%3DMaxim%2520Popov%2520and%2520Regina%2520Kurkova%2520and%2520Mikhail%2520Iumanov%2520and%2520Jaafar%2520Mahmoud%2520and%2520Sergey%2520Kolyubin%26entry.1292438233%3D%2520%2520Open%2520Semantic%2520Mapping%2520%2528OSM%2529%2520is%2520a%2520key%2520technology%2520in%2520robotic%2520perception%252C%250Acombining%2520semantic%2520segmentation%2520and%2520SLAM%2520techniques.%2520This%2520paper%2520introduces%2520a%250Adynamically%2520configurable%2520and%2520highly%2520automated%2520LLM/LVLM-powered%2520pipeline%2520for%250Aevaluating%2520OSM%2520solutions%2520called%2520OSMa-Bench%2520%2528Open%2520Semantic%2520Mapping%2520Benchmark%2529.%250AThe%2520study%2520focuses%2520on%2520evaluating%2520state-of-the-art%2520semantic%2520mapping%2520algorithms%250Aunder%2520varying%2520indoor%2520lighting%2520conditions%252C%2520a%2520critical%2520challenge%2520in%2520indoor%250Aenvironments.%2520We%2520introduce%2520a%2520novel%2520dataset%2520with%2520simulated%2520RGB-D%2520sequences%2520and%250Aground%2520truth%25203D%2520reconstructions%252C%2520facilitating%2520the%2520rigorous%2520analysis%2520of%2520mapping%250Aperformance%2520across%2520different%2520lighting%2520conditions.%2520Through%2520experiments%2520on%250Aleading%2520models%2520such%2520as%2520ConceptGraphs%252C%2520BBQ%2520and%2520OpenScene%252C%2520we%2520evaluate%2520the%250Asemantic%2520fidelity%2520of%2520object%2520recognition%2520and%2520segmentation.%2520Additionally%252C%2520we%250Aintroduce%2520a%2520Scene%2520Graph%2520evaluation%2520method%2520to%2520analyze%2520the%2520ability%2520of%2520models%2520to%250Ainterpret%2520semantic%2520structure.%2520The%2520results%2520provide%2520insights%2520into%2520the%2520robustness%250Aof%2520these%2520models%252C%2520forming%2520future%2520research%2520directions%2520for%2520developing%2520resilient%250Aand%2520adaptable%2520robotic%2520systems.%2520Project%2520page%2520is%2520available%2520at%250Ahttps%253A//be2rlab.github.io/OSMa-Bench/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10331v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OSMa-Bench%3A%20Evaluating%20Open%20Semantic%20Mapping%20Under%20Varying%20Lighting%0A%20%20Conditions&entry.906535625=Maxim%20Popov%20and%20Regina%20Kurkova%20and%20Mikhail%20Iumanov%20and%20Jaafar%20Mahmoud%20and%20Sergey%20Kolyubin&entry.1292438233=%20%20Open%20Semantic%20Mapping%20%28OSM%29%20is%20a%20key%20technology%20in%20robotic%20perception%2C%0Acombining%20semantic%20segmentation%20and%20SLAM%20techniques.%20This%20paper%20introduces%20a%0Adynamically%20configurable%20and%20highly%20automated%20LLM/LVLM-powered%20pipeline%20for%0Aevaluating%20OSM%20solutions%20called%20OSMa-Bench%20%28Open%20Semantic%20Mapping%20Benchmark%29.%0AThe%20study%20focuses%20on%20evaluating%20state-of-the-art%20semantic%20mapping%20algorithms%0Aunder%20varying%20indoor%20lighting%20conditions%2C%20a%20critical%20challenge%20in%20indoor%0Aenvironments.%20We%20introduce%20a%20novel%20dataset%20with%20simulated%20RGB-D%20sequences%20and%0Aground%20truth%203D%20reconstructions%2C%20facilitating%20the%20rigorous%20analysis%20of%20mapping%0Aperformance%20across%20different%20lighting%20conditions.%20Through%20experiments%20on%0Aleading%20models%20such%20as%20ConceptGraphs%2C%20BBQ%20and%20OpenScene%2C%20we%20evaluate%20the%0Asemantic%20fidelity%20of%20object%20recognition%20and%20segmentation.%20Additionally%2C%20we%0Aintroduce%20a%20Scene%20Graph%20evaluation%20method%20to%20analyze%20the%20ability%20of%20models%20to%0Ainterpret%20semantic%20structure.%20The%20results%20provide%20insights%20into%20the%20robustness%0Aof%20these%20models%2C%20forming%20future%20research%20directions%20for%20developing%20resilient%0Aand%20adaptable%20robotic%20systems.%20Project%20page%20is%20available%20at%0Ahttps%3A//be2rlab.github.io/OSMa-Bench/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10331v2&entry.124074799=Read"},
{"title": "GeoVLA: Empowering 3D Representations in Vision-Language-Action Models", "author": "Lin Sun and Bin Xie and Yingfei Liu and Hao Shi and Tiancai Wang and Jiale Cao", "abstract": "  Vision-Language-Action (VLA) models have emerged as a promising approach for\nenabling robots to follow language instructions and predict corresponding\nactions.However, current VLA models mainly rely on 2D visual inputs, neglecting\nthe rich geometric information in the 3D physical world, which limits their\nspatial awareness and adaptability. In this paper, we present GeoVLA, a novel\nVLA framework that effectively integrates 3D information to advance robotic\nmanipulation. It uses a vision-language model (VLM) to process images and\nlanguage instructions,extracting fused vision-language embeddings. In parallel,\nit converts depth maps into point clouds and employs a customized point\nencoder, called Point Embedding Network, to generate 3D geometric embeddings\nindependently. These produced embeddings are then concatenated and processed by\nour proposed spatial-aware action expert, called 3D-enhanced Action Expert,\nwhich combines information from different sensor modalities to produce precise\naction sequences. Through extensive experiments in both simulation and\nreal-world environments, GeoVLA demonstrates superior performance and\nrobustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2\nsimulation benchmarks and shows remarkable robustness in real-world tasks\nrequiring height adaptability, scale awareness and viewpoint invariance.\n", "link": "http://arxiv.org/abs/2508.09071v1", "date": "2025-08-12", "relevancy": 2.3904, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6066}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6066}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoVLA%3A%20Empowering%203D%20Representations%20in%20Vision-Language-Action%20Models&body=Title%3A%20GeoVLA%3A%20Empowering%203D%20Representations%20in%20Vision-Language-Action%20Models%0AAuthor%3A%20Lin%20Sun%20and%20Bin%20Xie%20and%20Yingfei%20Liu%20and%20Hao%20Shi%20and%20Tiancai%20Wang%20and%20Jiale%20Cao%0AAbstract%3A%20%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20a%20promising%20approach%20for%0Aenabling%20robots%20to%20follow%20language%20instructions%20and%20predict%20corresponding%0Aactions.However%2C%20current%20VLA%20models%20mainly%20rely%20on%202D%20visual%20inputs%2C%20neglecting%0Athe%20rich%20geometric%20information%20in%20the%203D%20physical%20world%2C%20which%20limits%20their%0Aspatial%20awareness%20and%20adaptability.%20In%20this%20paper%2C%20we%20present%20GeoVLA%2C%20a%20novel%0AVLA%20framework%20that%20effectively%20integrates%203D%20information%20to%20advance%20robotic%0Amanipulation.%20It%20uses%20a%20vision-language%20model%20%28VLM%29%20to%20process%20images%20and%0Alanguage%20instructions%2Cextracting%20fused%20vision-language%20embeddings.%20In%20parallel%2C%0Ait%20converts%20depth%20maps%20into%20point%20clouds%20and%20employs%20a%20customized%20point%0Aencoder%2C%20called%20Point%20Embedding%20Network%2C%20to%20generate%203D%20geometric%20embeddings%0Aindependently.%20These%20produced%20embeddings%20are%20then%20concatenated%20and%20processed%20by%0Aour%20proposed%20spatial-aware%20action%20expert%2C%20called%203D-enhanced%20Action%20Expert%2C%0Awhich%20combines%20information%20from%20different%20sensor%20modalities%20to%20produce%20precise%0Aaction%20sequences.%20Through%20extensive%20experiments%20in%20both%20simulation%20and%0Areal-world%20environments%2C%20GeoVLA%20demonstrates%20superior%20performance%20and%0Arobustness.%20It%20achieves%20state-of-the-art%20results%20in%20the%20LIBERO%20and%20ManiSkill2%0Asimulation%20benchmarks%20and%20shows%20remarkable%20robustness%20in%20real-world%20tasks%0Arequiring%20height%20adaptability%2C%20scale%20awareness%20and%20viewpoint%20invariance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoVLA%253A%2520Empowering%25203D%2520Representations%2520in%2520Vision-Language-Action%2520Models%26entry.906535625%3DLin%2520Sun%2520and%2520Bin%2520Xie%2520and%2520Yingfei%2520Liu%2520and%2520Hao%2520Shi%2520and%2520Tiancai%2520Wang%2520and%2520Jiale%2520Cao%26entry.1292438233%3D%2520%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520for%250Aenabling%2520robots%2520to%2520follow%2520language%2520instructions%2520and%2520predict%2520corresponding%250Aactions.However%252C%2520current%2520VLA%2520models%2520mainly%2520rely%2520on%25202D%2520visual%2520inputs%252C%2520neglecting%250Athe%2520rich%2520geometric%2520information%2520in%2520the%25203D%2520physical%2520world%252C%2520which%2520limits%2520their%250Aspatial%2520awareness%2520and%2520adaptability.%2520In%2520this%2520paper%252C%2520we%2520present%2520GeoVLA%252C%2520a%2520novel%250AVLA%2520framework%2520that%2520effectively%2520integrates%25203D%2520information%2520to%2520advance%2520robotic%250Amanipulation.%2520It%2520uses%2520a%2520vision-language%2520model%2520%2528VLM%2529%2520to%2520process%2520images%2520and%250Alanguage%2520instructions%252Cextracting%2520fused%2520vision-language%2520embeddings.%2520In%2520parallel%252C%250Ait%2520converts%2520depth%2520maps%2520into%2520point%2520clouds%2520and%2520employs%2520a%2520customized%2520point%250Aencoder%252C%2520called%2520Point%2520Embedding%2520Network%252C%2520to%2520generate%25203D%2520geometric%2520embeddings%250Aindependently.%2520These%2520produced%2520embeddings%2520are%2520then%2520concatenated%2520and%2520processed%2520by%250Aour%2520proposed%2520spatial-aware%2520action%2520expert%252C%2520called%25203D-enhanced%2520Action%2520Expert%252C%250Awhich%2520combines%2520information%2520from%2520different%2520sensor%2520modalities%2520to%2520produce%2520precise%250Aaction%2520sequences.%2520Through%2520extensive%2520experiments%2520in%2520both%2520simulation%2520and%250Areal-world%2520environments%252C%2520GeoVLA%2520demonstrates%2520superior%2520performance%2520and%250Arobustness.%2520It%2520achieves%2520state-of-the-art%2520results%2520in%2520the%2520LIBERO%2520and%2520ManiSkill2%250Asimulation%2520benchmarks%2520and%2520shows%2520remarkable%2520robustness%2520in%2520real-world%2520tasks%250Arequiring%2520height%2520adaptability%252C%2520scale%2520awareness%2520and%2520viewpoint%2520invariance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoVLA%3A%20Empowering%203D%20Representations%20in%20Vision-Language-Action%20Models&entry.906535625=Lin%20Sun%20and%20Bin%20Xie%20and%20Yingfei%20Liu%20and%20Hao%20Shi%20and%20Tiancai%20Wang%20and%20Jiale%20Cao&entry.1292438233=%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20a%20promising%20approach%20for%0Aenabling%20robots%20to%20follow%20language%20instructions%20and%20predict%20corresponding%0Aactions.However%2C%20current%20VLA%20models%20mainly%20rely%20on%202D%20visual%20inputs%2C%20neglecting%0Athe%20rich%20geometric%20information%20in%20the%203D%20physical%20world%2C%20which%20limits%20their%0Aspatial%20awareness%20and%20adaptability.%20In%20this%20paper%2C%20we%20present%20GeoVLA%2C%20a%20novel%0AVLA%20framework%20that%20effectively%20integrates%203D%20information%20to%20advance%20robotic%0Amanipulation.%20It%20uses%20a%20vision-language%20model%20%28VLM%29%20to%20process%20images%20and%0Alanguage%20instructions%2Cextracting%20fused%20vision-language%20embeddings.%20In%20parallel%2C%0Ait%20converts%20depth%20maps%20into%20point%20clouds%20and%20employs%20a%20customized%20point%0Aencoder%2C%20called%20Point%20Embedding%20Network%2C%20to%20generate%203D%20geometric%20embeddings%0Aindependently.%20These%20produced%20embeddings%20are%20then%20concatenated%20and%20processed%20by%0Aour%20proposed%20spatial-aware%20action%20expert%2C%20called%203D-enhanced%20Action%20Expert%2C%0Awhich%20combines%20information%20from%20different%20sensor%20modalities%20to%20produce%20precise%0Aaction%20sequences.%20Through%20extensive%20experiments%20in%20both%20simulation%20and%0Areal-world%20environments%2C%20GeoVLA%20demonstrates%20superior%20performance%20and%0Arobustness.%20It%20achieves%20state-of-the-art%20results%20in%20the%20LIBERO%20and%20ManiSkill2%0Asimulation%20benchmarks%20and%20shows%20remarkable%20robustness%20in%20real-world%20tasks%0Arequiring%20height%20adaptability%2C%20scale%20awareness%20and%20viewpoint%20invariance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09071v1&entry.124074799=Read"},
{"title": "Chi-Geometry: A Library for Benchmarking Chirality Prediction of GNNs", "author": "Rylie Weaver and Massamiliano Lupo Pasini", "abstract": "  We introduce Chi-Geometry - a library that generates graph data for testing\nand benchmarking GNNs' ability to predict chirality. Chi-Geometry generates\nsynthetic graph samples with (i) user-specified geometric and topological\ntraits to isolate certain types of samples and (ii) randomized node positions\nand species to minimize extraneous correlations. Each generated graph contains\nexactly one chiral center labeled either R or S, while all other nodes are\nlabeled N/A (non-chiral). The generated samples are then combined into a\ncohesive dataset that can be used to assess a GNN's ability to predict\nchirality as a node classification task. Chi-Geometry allows more interpretable\nand less confounding benchmarking of GNNs for prediction of chirality in the\ngraph samples which can guide the design of new GNN architectures with improved\npredictive performance. We illustrate Chi-Geometry's efficacy by using it to\ngenerate synthetic datasets for benchmarking various state-of-the-art (SOTA)\nGNN architectures. The conclusions of these benchmarking results guided our\ndesign of two new GNN architectures. The first GNN architecture established\nall-to-all connections in the graph to accurately predict chirality across all\nchallenging configurations where previously tested SOTA models failed, but at a\ncomputational cost (both for training and inference) that grows quadratically\nwith the number of graph nodes. The second GNN architecture avoids all-to-all\nconnections by introducing a virtual node in the original graph structure of\nthe data, which restores the linear scaling of training and inference\ncomputational cost with respect to the number of nodes in the graph, while\nstill ensuring competitive accuracy in detecting chirality with respect to SOTA\nGNN architectures.\n", "link": "http://arxiv.org/abs/2508.09097v1", "date": "2025-08-12", "relevancy": 2.3869, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5247}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4552}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chi-Geometry%3A%20A%20Library%20for%20Benchmarking%20Chirality%20Prediction%20of%20GNNs&body=Title%3A%20Chi-Geometry%3A%20A%20Library%20for%20Benchmarking%20Chirality%20Prediction%20of%20GNNs%0AAuthor%3A%20Rylie%20Weaver%20and%20Massamiliano%20Lupo%20Pasini%0AAbstract%3A%20%20%20We%20introduce%20Chi-Geometry%20-%20a%20library%20that%20generates%20graph%20data%20for%20testing%0Aand%20benchmarking%20GNNs%27%20ability%20to%20predict%20chirality.%20Chi-Geometry%20generates%0Asynthetic%20graph%20samples%20with%20%28i%29%20user-specified%20geometric%20and%20topological%0Atraits%20to%20isolate%20certain%20types%20of%20samples%20and%20%28ii%29%20randomized%20node%20positions%0Aand%20species%20to%20minimize%20extraneous%20correlations.%20Each%20generated%20graph%20contains%0Aexactly%20one%20chiral%20center%20labeled%20either%20R%20or%20S%2C%20while%20all%20other%20nodes%20are%0Alabeled%20N/A%20%28non-chiral%29.%20The%20generated%20samples%20are%20then%20combined%20into%20a%0Acohesive%20dataset%20that%20can%20be%20used%20to%20assess%20a%20GNN%27s%20ability%20to%20predict%0Achirality%20as%20a%20node%20classification%20task.%20Chi-Geometry%20allows%20more%20interpretable%0Aand%20less%20confounding%20benchmarking%20of%20GNNs%20for%20prediction%20of%20chirality%20in%20the%0Agraph%20samples%20which%20can%20guide%20the%20design%20of%20new%20GNN%20architectures%20with%20improved%0Apredictive%20performance.%20We%20illustrate%20Chi-Geometry%27s%20efficacy%20by%20using%20it%20to%0Agenerate%20synthetic%20datasets%20for%20benchmarking%20various%20state-of-the-art%20%28SOTA%29%0AGNN%20architectures.%20The%20conclusions%20of%20these%20benchmarking%20results%20guided%20our%0Adesign%20of%20two%20new%20GNN%20architectures.%20The%20first%20GNN%20architecture%20established%0Aall-to-all%20connections%20in%20the%20graph%20to%20accurately%20predict%20chirality%20across%20all%0Achallenging%20configurations%20where%20previously%20tested%20SOTA%20models%20failed%2C%20but%20at%20a%0Acomputational%20cost%20%28both%20for%20training%20and%20inference%29%20that%20grows%20quadratically%0Awith%20the%20number%20of%20graph%20nodes.%20The%20second%20GNN%20architecture%20avoids%20all-to-all%0Aconnections%20by%20introducing%20a%20virtual%20node%20in%20the%20original%20graph%20structure%20of%0Athe%20data%2C%20which%20restores%20the%20linear%20scaling%20of%20training%20and%20inference%0Acomputational%20cost%20with%20respect%20to%20the%20number%20of%20nodes%20in%20the%20graph%2C%20while%0Astill%20ensuring%20competitive%20accuracy%20in%20detecting%20chirality%20with%20respect%20to%20SOTA%0AGNN%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChi-Geometry%253A%2520A%2520Library%2520for%2520Benchmarking%2520Chirality%2520Prediction%2520of%2520GNNs%26entry.906535625%3DRylie%2520Weaver%2520and%2520Massamiliano%2520Lupo%2520Pasini%26entry.1292438233%3D%2520%2520We%2520introduce%2520Chi-Geometry%2520-%2520a%2520library%2520that%2520generates%2520graph%2520data%2520for%2520testing%250Aand%2520benchmarking%2520GNNs%2527%2520ability%2520to%2520predict%2520chirality.%2520Chi-Geometry%2520generates%250Asynthetic%2520graph%2520samples%2520with%2520%2528i%2529%2520user-specified%2520geometric%2520and%2520topological%250Atraits%2520to%2520isolate%2520certain%2520types%2520of%2520samples%2520and%2520%2528ii%2529%2520randomized%2520node%2520positions%250Aand%2520species%2520to%2520minimize%2520extraneous%2520correlations.%2520Each%2520generated%2520graph%2520contains%250Aexactly%2520one%2520chiral%2520center%2520labeled%2520either%2520R%2520or%2520S%252C%2520while%2520all%2520other%2520nodes%2520are%250Alabeled%2520N/A%2520%2528non-chiral%2529.%2520The%2520generated%2520samples%2520are%2520then%2520combined%2520into%2520a%250Acohesive%2520dataset%2520that%2520can%2520be%2520used%2520to%2520assess%2520a%2520GNN%2527s%2520ability%2520to%2520predict%250Achirality%2520as%2520a%2520node%2520classification%2520task.%2520Chi-Geometry%2520allows%2520more%2520interpretable%250Aand%2520less%2520confounding%2520benchmarking%2520of%2520GNNs%2520for%2520prediction%2520of%2520chirality%2520in%2520the%250Agraph%2520samples%2520which%2520can%2520guide%2520the%2520design%2520of%2520new%2520GNN%2520architectures%2520with%2520improved%250Apredictive%2520performance.%2520We%2520illustrate%2520Chi-Geometry%2527s%2520efficacy%2520by%2520using%2520it%2520to%250Agenerate%2520synthetic%2520datasets%2520for%2520benchmarking%2520various%2520state-of-the-art%2520%2528SOTA%2529%250AGNN%2520architectures.%2520The%2520conclusions%2520of%2520these%2520benchmarking%2520results%2520guided%2520our%250Adesign%2520of%2520two%2520new%2520GNN%2520architectures.%2520The%2520first%2520GNN%2520architecture%2520established%250Aall-to-all%2520connections%2520in%2520the%2520graph%2520to%2520accurately%2520predict%2520chirality%2520across%2520all%250Achallenging%2520configurations%2520where%2520previously%2520tested%2520SOTA%2520models%2520failed%252C%2520but%2520at%2520a%250Acomputational%2520cost%2520%2528both%2520for%2520training%2520and%2520inference%2529%2520that%2520grows%2520quadratically%250Awith%2520the%2520number%2520of%2520graph%2520nodes.%2520The%2520second%2520GNN%2520architecture%2520avoids%2520all-to-all%250Aconnections%2520by%2520introducing%2520a%2520virtual%2520node%2520in%2520the%2520original%2520graph%2520structure%2520of%250Athe%2520data%252C%2520which%2520restores%2520the%2520linear%2520scaling%2520of%2520training%2520and%2520inference%250Acomputational%2520cost%2520with%2520respect%2520to%2520the%2520number%2520of%2520nodes%2520in%2520the%2520graph%252C%2520while%250Astill%2520ensuring%2520competitive%2520accuracy%2520in%2520detecting%2520chirality%2520with%2520respect%2520to%2520SOTA%250AGNN%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chi-Geometry%3A%20A%20Library%20for%20Benchmarking%20Chirality%20Prediction%20of%20GNNs&entry.906535625=Rylie%20Weaver%20and%20Massamiliano%20Lupo%20Pasini&entry.1292438233=%20%20We%20introduce%20Chi-Geometry%20-%20a%20library%20that%20generates%20graph%20data%20for%20testing%0Aand%20benchmarking%20GNNs%27%20ability%20to%20predict%20chirality.%20Chi-Geometry%20generates%0Asynthetic%20graph%20samples%20with%20%28i%29%20user-specified%20geometric%20and%20topological%0Atraits%20to%20isolate%20certain%20types%20of%20samples%20and%20%28ii%29%20randomized%20node%20positions%0Aand%20species%20to%20minimize%20extraneous%20correlations.%20Each%20generated%20graph%20contains%0Aexactly%20one%20chiral%20center%20labeled%20either%20R%20or%20S%2C%20while%20all%20other%20nodes%20are%0Alabeled%20N/A%20%28non-chiral%29.%20The%20generated%20samples%20are%20then%20combined%20into%20a%0Acohesive%20dataset%20that%20can%20be%20used%20to%20assess%20a%20GNN%27s%20ability%20to%20predict%0Achirality%20as%20a%20node%20classification%20task.%20Chi-Geometry%20allows%20more%20interpretable%0Aand%20less%20confounding%20benchmarking%20of%20GNNs%20for%20prediction%20of%20chirality%20in%20the%0Agraph%20samples%20which%20can%20guide%20the%20design%20of%20new%20GNN%20architectures%20with%20improved%0Apredictive%20performance.%20We%20illustrate%20Chi-Geometry%27s%20efficacy%20by%20using%20it%20to%0Agenerate%20synthetic%20datasets%20for%20benchmarking%20various%20state-of-the-art%20%28SOTA%29%0AGNN%20architectures.%20The%20conclusions%20of%20these%20benchmarking%20results%20guided%20our%0Adesign%20of%20two%20new%20GNN%20architectures.%20The%20first%20GNN%20architecture%20established%0Aall-to-all%20connections%20in%20the%20graph%20to%20accurately%20predict%20chirality%20across%20all%0Achallenging%20configurations%20where%20previously%20tested%20SOTA%20models%20failed%2C%20but%20at%20a%0Acomputational%20cost%20%28both%20for%20training%20and%20inference%29%20that%20grows%20quadratically%0Awith%20the%20number%20of%20graph%20nodes.%20The%20second%20GNN%20architecture%20avoids%20all-to-all%0Aconnections%20by%20introducing%20a%20virtual%20node%20in%20the%20original%20graph%20structure%20of%0Athe%20data%2C%20which%20restores%20the%20linear%20scaling%20of%20training%20and%20inference%0Acomputational%20cost%20with%20respect%20to%20the%20number%20of%20nodes%20in%20the%20graph%2C%20while%0Astill%20ensuring%20competitive%20accuracy%20in%20detecting%20chirality%20with%20respect%20to%20SOTA%0AGNN%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09097v1&entry.124074799=Read"},
{"title": "Edge-Cloud Collaborative Computing on Distributed Intelligence and Model\n  Optimization: A Survey", "author": "Jing Liu and Yao Du and Kun Yang and Jiaqi Wu and Yan Wang and Xiping Hu and Zehua Wang and Yang Liu and Peng Sun and Azzedine Boukerche and Victor C. M. Leung", "abstract": "  Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm\nfor addressing the computational demands of modern intelligent applications,\nintegrating cloud resources with edge devices to enable efficient, low-latency\nprocessing. Recent advancements in AI, particularly deep learning and large\nlanguage models (LLMs), have dramatically enhanced the capabilities of these\ndistributed systems, yet introduce significant challenges in model deployment\nand resource management. In this survey, we comprehensive examine the\nintersection of distributed intelligence and model optimization within\nedge-cloud environments, providing a structured tutorial on fundamental\narchitectures, enabling technologies, and emerging applications. Additionally,\nwe systematically analyze model optimization approaches, including compression,\nadaptation, and neural architecture search, alongside AI-driven resource\nmanagement strategies that balance performance, energy efficiency, and latency\nrequirements. We further explore critical aspects of privacy protection and\nsecurity enhancement within ECCC systems and examines practical deployments\nthrough diverse applications, spanning autonomous driving, healthcare, and\nindustrial automation. Performance analysis and benchmarking techniques are\nalso thoroughly explored to establish evaluation standards for these complex\nsystems. Furthermore, the review identifies critical research directions\nincluding LLMs deployment, 6G integration, neuromorphic computing, and quantum\ncomputing, offering a roadmap for addressing persistent challenges in\nheterogeneity management, real-time processing, and scalability. By bridging\ntheoretical advancements and practical deployments, this survey offers\nresearchers and practitioners a holistic perspective on leveraging AI to\noptimize distributed computing environments, fostering innovation in\nnext-generation intelligent systems.\n", "link": "http://arxiv.org/abs/2505.01821v3", "date": "2025-08-12", "relevancy": 2.3817, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4765}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4763}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge-Cloud%20Collaborative%20Computing%20on%20Distributed%20Intelligence%20and%20Model%0A%20%20Optimization%3A%20A%20Survey&body=Title%3A%20Edge-Cloud%20Collaborative%20Computing%20on%20Distributed%20Intelligence%20and%20Model%0A%20%20Optimization%3A%20A%20Survey%0AAuthor%3A%20Jing%20Liu%20and%20Yao%20Du%20and%20Kun%20Yang%20and%20Jiaqi%20Wu%20and%20Yan%20Wang%20and%20Xiping%20Hu%20and%20Zehua%20Wang%20and%20Yang%20Liu%20and%20Peng%20Sun%20and%20Azzedine%20Boukerche%20and%20Victor%20C.%20M.%20Leung%0AAbstract%3A%20%20%20Edge-cloud%20collaborative%20computing%20%28ECCC%29%20has%20emerged%20as%20a%20pivotal%20paradigm%0Afor%20addressing%20the%20computational%20demands%20of%20modern%20intelligent%20applications%2C%0Aintegrating%20cloud%20resources%20with%20edge%20devices%20to%20enable%20efficient%2C%20low-latency%0Aprocessing.%20Recent%20advancements%20in%20AI%2C%20particularly%20deep%20learning%20and%20large%0Alanguage%20models%20%28LLMs%29%2C%20have%20dramatically%20enhanced%20the%20capabilities%20of%20these%0Adistributed%20systems%2C%20yet%20introduce%20significant%20challenges%20in%20model%20deployment%0Aand%20resource%20management.%20In%20this%20survey%2C%20we%20comprehensive%20examine%20the%0Aintersection%20of%20distributed%20intelligence%20and%20model%20optimization%20within%0Aedge-cloud%20environments%2C%20providing%20a%20structured%20tutorial%20on%20fundamental%0Aarchitectures%2C%20enabling%20technologies%2C%20and%20emerging%20applications.%20Additionally%2C%0Awe%20systematically%20analyze%20model%20optimization%20approaches%2C%20including%20compression%2C%0Aadaptation%2C%20and%20neural%20architecture%20search%2C%20alongside%20AI-driven%20resource%0Amanagement%20strategies%20that%20balance%20performance%2C%20energy%20efficiency%2C%20and%20latency%0Arequirements.%20We%20further%20explore%20critical%20aspects%20of%20privacy%20protection%20and%0Asecurity%20enhancement%20within%20ECCC%20systems%20and%20examines%20practical%20deployments%0Athrough%20diverse%20applications%2C%20spanning%20autonomous%20driving%2C%20healthcare%2C%20and%0Aindustrial%20automation.%20Performance%20analysis%20and%20benchmarking%20techniques%20are%0Aalso%20thoroughly%20explored%20to%20establish%20evaluation%20standards%20for%20these%20complex%0Asystems.%20Furthermore%2C%20the%20review%20identifies%20critical%20research%20directions%0Aincluding%20LLMs%20deployment%2C%206G%20integration%2C%20neuromorphic%20computing%2C%20and%20quantum%0Acomputing%2C%20offering%20a%20roadmap%20for%20addressing%20persistent%20challenges%20in%0Aheterogeneity%20management%2C%20real-time%20processing%2C%20and%20scalability.%20By%20bridging%0Atheoretical%20advancements%20and%20practical%20deployments%2C%20this%20survey%20offers%0Aresearchers%20and%20practitioners%20a%20holistic%20perspective%20on%20leveraging%20AI%20to%0Aoptimize%20distributed%20computing%20environments%2C%20fostering%20innovation%20in%0Anext-generation%20intelligent%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01821v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge-Cloud%2520Collaborative%2520Computing%2520on%2520Distributed%2520Intelligence%2520and%2520Model%250A%2520%2520Optimization%253A%2520A%2520Survey%26entry.906535625%3DJing%2520Liu%2520and%2520Yao%2520Du%2520and%2520Kun%2520Yang%2520and%2520Jiaqi%2520Wu%2520and%2520Yan%2520Wang%2520and%2520Xiping%2520Hu%2520and%2520Zehua%2520Wang%2520and%2520Yang%2520Liu%2520and%2520Peng%2520Sun%2520and%2520Azzedine%2520Boukerche%2520and%2520Victor%2520C.%2520M.%2520Leung%26entry.1292438233%3D%2520%2520Edge-cloud%2520collaborative%2520computing%2520%2528ECCC%2529%2520has%2520emerged%2520as%2520a%2520pivotal%2520paradigm%250Afor%2520addressing%2520the%2520computational%2520demands%2520of%2520modern%2520intelligent%2520applications%252C%250Aintegrating%2520cloud%2520resources%2520with%2520edge%2520devices%2520to%2520enable%2520efficient%252C%2520low-latency%250Aprocessing.%2520Recent%2520advancements%2520in%2520AI%252C%2520particularly%2520deep%2520learning%2520and%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520have%2520dramatically%2520enhanced%2520the%2520capabilities%2520of%2520these%250Adistributed%2520systems%252C%2520yet%2520introduce%2520significant%2520challenges%2520in%2520model%2520deployment%250Aand%2520resource%2520management.%2520In%2520this%2520survey%252C%2520we%2520comprehensive%2520examine%2520the%250Aintersection%2520of%2520distributed%2520intelligence%2520and%2520model%2520optimization%2520within%250Aedge-cloud%2520environments%252C%2520providing%2520a%2520structured%2520tutorial%2520on%2520fundamental%250Aarchitectures%252C%2520enabling%2520technologies%252C%2520and%2520emerging%2520applications.%2520Additionally%252C%250Awe%2520systematically%2520analyze%2520model%2520optimization%2520approaches%252C%2520including%2520compression%252C%250Aadaptation%252C%2520and%2520neural%2520architecture%2520search%252C%2520alongside%2520AI-driven%2520resource%250Amanagement%2520strategies%2520that%2520balance%2520performance%252C%2520energy%2520efficiency%252C%2520and%2520latency%250Arequirements.%2520We%2520further%2520explore%2520critical%2520aspects%2520of%2520privacy%2520protection%2520and%250Asecurity%2520enhancement%2520within%2520ECCC%2520systems%2520and%2520examines%2520practical%2520deployments%250Athrough%2520diverse%2520applications%252C%2520spanning%2520autonomous%2520driving%252C%2520healthcare%252C%2520and%250Aindustrial%2520automation.%2520Performance%2520analysis%2520and%2520benchmarking%2520techniques%2520are%250Aalso%2520thoroughly%2520explored%2520to%2520establish%2520evaluation%2520standards%2520for%2520these%2520complex%250Asystems.%2520Furthermore%252C%2520the%2520review%2520identifies%2520critical%2520research%2520directions%250Aincluding%2520LLMs%2520deployment%252C%25206G%2520integration%252C%2520neuromorphic%2520computing%252C%2520and%2520quantum%250Acomputing%252C%2520offering%2520a%2520roadmap%2520for%2520addressing%2520persistent%2520challenges%2520in%250Aheterogeneity%2520management%252C%2520real-time%2520processing%252C%2520and%2520scalability.%2520By%2520bridging%250Atheoretical%2520advancements%2520and%2520practical%2520deployments%252C%2520this%2520survey%2520offers%250Aresearchers%2520and%2520practitioners%2520a%2520holistic%2520perspective%2520on%2520leveraging%2520AI%2520to%250Aoptimize%2520distributed%2520computing%2520environments%252C%2520fostering%2520innovation%2520in%250Anext-generation%2520intelligent%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01821v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge-Cloud%20Collaborative%20Computing%20on%20Distributed%20Intelligence%20and%20Model%0A%20%20Optimization%3A%20A%20Survey&entry.906535625=Jing%20Liu%20and%20Yao%20Du%20and%20Kun%20Yang%20and%20Jiaqi%20Wu%20and%20Yan%20Wang%20and%20Xiping%20Hu%20and%20Zehua%20Wang%20and%20Yang%20Liu%20and%20Peng%20Sun%20and%20Azzedine%20Boukerche%20and%20Victor%20C.%20M.%20Leung&entry.1292438233=%20%20Edge-cloud%20collaborative%20computing%20%28ECCC%29%20has%20emerged%20as%20a%20pivotal%20paradigm%0Afor%20addressing%20the%20computational%20demands%20of%20modern%20intelligent%20applications%2C%0Aintegrating%20cloud%20resources%20with%20edge%20devices%20to%20enable%20efficient%2C%20low-latency%0Aprocessing.%20Recent%20advancements%20in%20AI%2C%20particularly%20deep%20learning%20and%20large%0Alanguage%20models%20%28LLMs%29%2C%20have%20dramatically%20enhanced%20the%20capabilities%20of%20these%0Adistributed%20systems%2C%20yet%20introduce%20significant%20challenges%20in%20model%20deployment%0Aand%20resource%20management.%20In%20this%20survey%2C%20we%20comprehensive%20examine%20the%0Aintersection%20of%20distributed%20intelligence%20and%20model%20optimization%20within%0Aedge-cloud%20environments%2C%20providing%20a%20structured%20tutorial%20on%20fundamental%0Aarchitectures%2C%20enabling%20technologies%2C%20and%20emerging%20applications.%20Additionally%2C%0Awe%20systematically%20analyze%20model%20optimization%20approaches%2C%20including%20compression%2C%0Aadaptation%2C%20and%20neural%20architecture%20search%2C%20alongside%20AI-driven%20resource%0Amanagement%20strategies%20that%20balance%20performance%2C%20energy%20efficiency%2C%20and%20latency%0Arequirements.%20We%20further%20explore%20critical%20aspects%20of%20privacy%20protection%20and%0Asecurity%20enhancement%20within%20ECCC%20systems%20and%20examines%20practical%20deployments%0Athrough%20diverse%20applications%2C%20spanning%20autonomous%20driving%2C%20healthcare%2C%20and%0Aindustrial%20automation.%20Performance%20analysis%20and%20benchmarking%20techniques%20are%0Aalso%20thoroughly%20explored%20to%20establish%20evaluation%20standards%20for%20these%20complex%0Asystems.%20Furthermore%2C%20the%20review%20identifies%20critical%20research%20directions%0Aincluding%20LLMs%20deployment%2C%206G%20integration%2C%20neuromorphic%20computing%2C%20and%20quantum%0Acomputing%2C%20offering%20a%20roadmap%20for%20addressing%20persistent%20challenges%20in%0Aheterogeneity%20management%2C%20real-time%20processing%2C%20and%20scalability.%20By%20bridging%0Atheoretical%20advancements%20and%20practical%20deployments%2C%20this%20survey%20offers%0Aresearchers%20and%20practitioners%20a%20holistic%20perspective%20on%20leveraging%20AI%20to%0Aoptimize%20distributed%20computing%20environments%2C%20fostering%20innovation%20in%0Anext-generation%20intelligent%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01821v3&entry.124074799=Read"},
{"title": "KFFocus: Highlighting Keyframes for Enhanced Video Understanding", "author": "Ming Nie and Chunwei Wang and Hang Xu and Li Zhang", "abstract": "  Recently, with the emergence of large language models, multimodal LLMs have\ndemonstrated exceptional capabilities in image and video modalities. Despite\nadvancements in video comprehension, the substantial computational demands of\nlong video sequences lead current video LLMs (Vid-LLMs) to employ compression\nstrategies at both the inter-frame level (e.g., uniform sampling of video\nframes) and intra-frame level (e.g., condensing all visual tokens of each frame\ninto a limited number). However, this approach often neglects the uneven\ntemporal distribution of critical information across frames, risking the\nomission of keyframes that contain essential temporal and semantic details. To\ntackle these challenges, we propose KFFocus, a method designed to efficiently\ncompress video tokens and emphasize the informative context present within\nvideo frames. We substitute uniform sampling with a refined approach inspired\nby classic video compression principles to identify and capture keyframes based\non their temporal redundancy. By assigning varying condensation ratios to\nframes based on their contextual relevance, KFFocus efficiently reduces token\nredundancy while preserving informative content details. Additionally, we\nintroduce a spatiotemporal modeling module that encodes both the temporal\nrelationships between video frames and the spatial structure within each frame,\nthus providing Vid-LLMs with a nuanced understanding of spatial-temporal\ndynamics. Extensive experiments on widely recognized video understanding\nbenchmarks, especially long video scenarios, demonstrate that KFFocus\nsignificantly outperforms existing methods, achieving substantial computational\nefficiency and enhanced accuracy.\n", "link": "http://arxiv.org/abs/2508.08989v1", "date": "2025-08-12", "relevancy": 2.3803, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5984}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5984}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KFFocus%3A%20Highlighting%20Keyframes%20for%20Enhanced%20Video%20Understanding&body=Title%3A%20KFFocus%3A%20Highlighting%20Keyframes%20for%20Enhanced%20Video%20Understanding%0AAuthor%3A%20Ming%20Nie%20and%20Chunwei%20Wang%20and%20Hang%20Xu%20and%20Li%20Zhang%0AAbstract%3A%20%20%20Recently%2C%20with%20the%20emergence%20of%20large%20language%20models%2C%20multimodal%20LLMs%20have%0Ademonstrated%20exceptional%20capabilities%20in%20image%20and%20video%20modalities.%20Despite%0Aadvancements%20in%20video%20comprehension%2C%20the%20substantial%20computational%20demands%20of%0Along%20video%20sequences%20lead%20current%20video%20LLMs%20%28Vid-LLMs%29%20to%20employ%20compression%0Astrategies%20at%20both%20the%20inter-frame%20level%20%28e.g.%2C%20uniform%20sampling%20of%20video%0Aframes%29%20and%20intra-frame%20level%20%28e.g.%2C%20condensing%20all%20visual%20tokens%20of%20each%20frame%0Ainto%20a%20limited%20number%29.%20However%2C%20this%20approach%20often%20neglects%20the%20uneven%0Atemporal%20distribution%20of%20critical%20information%20across%20frames%2C%20risking%20the%0Aomission%20of%20keyframes%20that%20contain%20essential%20temporal%20and%20semantic%20details.%20To%0Atackle%20these%20challenges%2C%20we%20propose%20KFFocus%2C%20a%20method%20designed%20to%20efficiently%0Acompress%20video%20tokens%20and%20emphasize%20the%20informative%20context%20present%20within%0Avideo%20frames.%20We%20substitute%20uniform%20sampling%20with%20a%20refined%20approach%20inspired%0Aby%20classic%20video%20compression%20principles%20to%20identify%20and%20capture%20keyframes%20based%0Aon%20their%20temporal%20redundancy.%20By%20assigning%20varying%20condensation%20ratios%20to%0Aframes%20based%20on%20their%20contextual%20relevance%2C%20KFFocus%20efficiently%20reduces%20token%0Aredundancy%20while%20preserving%20informative%20content%20details.%20Additionally%2C%20we%0Aintroduce%20a%20spatiotemporal%20modeling%20module%20that%20encodes%20both%20the%20temporal%0Arelationships%20between%20video%20frames%20and%20the%20spatial%20structure%20within%20each%20frame%2C%0Athus%20providing%20Vid-LLMs%20with%20a%20nuanced%20understanding%20of%20spatial-temporal%0Adynamics.%20Extensive%20experiments%20on%20widely%20recognized%20video%20understanding%0Abenchmarks%2C%20especially%20long%20video%20scenarios%2C%20demonstrate%20that%20KFFocus%0Asignificantly%20outperforms%20existing%20methods%2C%20achieving%20substantial%20computational%0Aefficiency%20and%20enhanced%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKFFocus%253A%2520Highlighting%2520Keyframes%2520for%2520Enhanced%2520Video%2520Understanding%26entry.906535625%3DMing%2520Nie%2520and%2520Chunwei%2520Wang%2520and%2520Hang%2520Xu%2520and%2520Li%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%2520with%2520the%2520emergence%2520of%2520large%2520language%2520models%252C%2520multimodal%2520LLMs%2520have%250Ademonstrated%2520exceptional%2520capabilities%2520in%2520image%2520and%2520video%2520modalities.%2520Despite%250Aadvancements%2520in%2520video%2520comprehension%252C%2520the%2520substantial%2520computational%2520demands%2520of%250Along%2520video%2520sequences%2520lead%2520current%2520video%2520LLMs%2520%2528Vid-LLMs%2529%2520to%2520employ%2520compression%250Astrategies%2520at%2520both%2520the%2520inter-frame%2520level%2520%2528e.g.%252C%2520uniform%2520sampling%2520of%2520video%250Aframes%2529%2520and%2520intra-frame%2520level%2520%2528e.g.%252C%2520condensing%2520all%2520visual%2520tokens%2520of%2520each%2520frame%250Ainto%2520a%2520limited%2520number%2529.%2520However%252C%2520this%2520approach%2520often%2520neglects%2520the%2520uneven%250Atemporal%2520distribution%2520of%2520critical%2520information%2520across%2520frames%252C%2520risking%2520the%250Aomission%2520of%2520keyframes%2520that%2520contain%2520essential%2520temporal%2520and%2520semantic%2520details.%2520To%250Atackle%2520these%2520challenges%252C%2520we%2520propose%2520KFFocus%252C%2520a%2520method%2520designed%2520to%2520efficiently%250Acompress%2520video%2520tokens%2520and%2520emphasize%2520the%2520informative%2520context%2520present%2520within%250Avideo%2520frames.%2520We%2520substitute%2520uniform%2520sampling%2520with%2520a%2520refined%2520approach%2520inspired%250Aby%2520classic%2520video%2520compression%2520principles%2520to%2520identify%2520and%2520capture%2520keyframes%2520based%250Aon%2520their%2520temporal%2520redundancy.%2520By%2520assigning%2520varying%2520condensation%2520ratios%2520to%250Aframes%2520based%2520on%2520their%2520contextual%2520relevance%252C%2520KFFocus%2520efficiently%2520reduces%2520token%250Aredundancy%2520while%2520preserving%2520informative%2520content%2520details.%2520Additionally%252C%2520we%250Aintroduce%2520a%2520spatiotemporal%2520modeling%2520module%2520that%2520encodes%2520both%2520the%2520temporal%250Arelationships%2520between%2520video%2520frames%2520and%2520the%2520spatial%2520structure%2520within%2520each%2520frame%252C%250Athus%2520providing%2520Vid-LLMs%2520with%2520a%2520nuanced%2520understanding%2520of%2520spatial-temporal%250Adynamics.%2520Extensive%2520experiments%2520on%2520widely%2520recognized%2520video%2520understanding%250Abenchmarks%252C%2520especially%2520long%2520video%2520scenarios%252C%2520demonstrate%2520that%2520KFFocus%250Asignificantly%2520outperforms%2520existing%2520methods%252C%2520achieving%2520substantial%2520computational%250Aefficiency%2520and%2520enhanced%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KFFocus%3A%20Highlighting%20Keyframes%20for%20Enhanced%20Video%20Understanding&entry.906535625=Ming%20Nie%20and%20Chunwei%20Wang%20and%20Hang%20Xu%20and%20Li%20Zhang&entry.1292438233=%20%20Recently%2C%20with%20the%20emergence%20of%20large%20language%20models%2C%20multimodal%20LLMs%20have%0Ademonstrated%20exceptional%20capabilities%20in%20image%20and%20video%20modalities.%20Despite%0Aadvancements%20in%20video%20comprehension%2C%20the%20substantial%20computational%20demands%20of%0Along%20video%20sequences%20lead%20current%20video%20LLMs%20%28Vid-LLMs%29%20to%20employ%20compression%0Astrategies%20at%20both%20the%20inter-frame%20level%20%28e.g.%2C%20uniform%20sampling%20of%20video%0Aframes%29%20and%20intra-frame%20level%20%28e.g.%2C%20condensing%20all%20visual%20tokens%20of%20each%20frame%0Ainto%20a%20limited%20number%29.%20However%2C%20this%20approach%20often%20neglects%20the%20uneven%0Atemporal%20distribution%20of%20critical%20information%20across%20frames%2C%20risking%20the%0Aomission%20of%20keyframes%20that%20contain%20essential%20temporal%20and%20semantic%20details.%20To%0Atackle%20these%20challenges%2C%20we%20propose%20KFFocus%2C%20a%20method%20designed%20to%20efficiently%0Acompress%20video%20tokens%20and%20emphasize%20the%20informative%20context%20present%20within%0Avideo%20frames.%20We%20substitute%20uniform%20sampling%20with%20a%20refined%20approach%20inspired%0Aby%20classic%20video%20compression%20principles%20to%20identify%20and%20capture%20keyframes%20based%0Aon%20their%20temporal%20redundancy.%20By%20assigning%20varying%20condensation%20ratios%20to%0Aframes%20based%20on%20their%20contextual%20relevance%2C%20KFFocus%20efficiently%20reduces%20token%0Aredundancy%20while%20preserving%20informative%20content%20details.%20Additionally%2C%20we%0Aintroduce%20a%20spatiotemporal%20modeling%20module%20that%20encodes%20both%20the%20temporal%0Arelationships%20between%20video%20frames%20and%20the%20spatial%20structure%20within%20each%20frame%2C%0Athus%20providing%20Vid-LLMs%20with%20a%20nuanced%20understanding%20of%20spatial-temporal%0Adynamics.%20Extensive%20experiments%20on%20widely%20recognized%20video%20understanding%0Abenchmarks%2C%20especially%20long%20video%20scenarios%2C%20demonstrate%20that%20KFFocus%0Asignificantly%20outperforms%20existing%20methods%2C%20achieving%20substantial%20computational%0Aefficiency%20and%20enhanced%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08989v1&entry.124074799=Read"},
{"title": "Achieving More with Less: Additive Prompt Tuning for Rehearsal-Free\n  Class-Incremental Learning", "author": "Haoran Chen and Ping Wang and Zihan Zhou and Xu Zhang and Zuxuan Wu and Yu-Gang Jiang", "abstract": "  Class-incremental learning (CIL) enables models to learn new classes\nprogressively while preserving knowledge of previously learned ones. Recent\nadvances in this field have shifted towards parameter-efficient fine-tuning\ntechniques, with many approaches building upon the framework that maintains a\npool of learnable prompts. Although effective, these methods introduce\nsubstantial computational overhead, primarily due to prompt pool querying and\nincreased input sequence lengths from prompt concatenation. In this work, we\npresent a novel prompt-based approach that addresses this limitation. Our\nmethod trains a single set of shared prompts across all tasks and, rather than\nconcatenating prompts to the input, directly modifies the CLS token's attention\ncomputation by adding the prompts to it. This simple and lightweight design not\nonly significantly reduces computational complexity-both in terms of inference\ncosts and the number of trainable parameters-but also eliminates the need to\noptimize prompt lengths for different downstream tasks, offering a more\nefficient yet powerful solution for rehearsal-free class-incremental learning.\nExtensive experiments across a diverse range of CIL benchmarks demonstrate the\neffectiveness of our approach, highlighting its potential to establish a new\nprompt-based CIL paradigm. Furthermore, experiments on general recognition\nbenchmarks beyond the CIL setting also show strong performance, positioning our\nmethod as a promising candidate for a general parameter-efficient fine-tuning\napproach.\n", "link": "http://arxiv.org/abs/2503.07979v2", "date": "2025-08-12", "relevancy": 2.3712, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4818}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4728}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Achieving%20More%20with%20Less%3A%20Additive%20Prompt%20Tuning%20for%20Rehearsal-Free%0A%20%20Class-Incremental%20Learning&body=Title%3A%20Achieving%20More%20with%20Less%3A%20Additive%20Prompt%20Tuning%20for%20Rehearsal-Free%0A%20%20Class-Incremental%20Learning%0AAuthor%3A%20Haoran%20Chen%20and%20Ping%20Wang%20and%20Zihan%20Zhou%20and%20Xu%20Zhang%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Class-incremental%20learning%20%28CIL%29%20enables%20models%20to%20learn%20new%20classes%0Aprogressively%20while%20preserving%20knowledge%20of%20previously%20learned%20ones.%20Recent%0Aadvances%20in%20this%20field%20have%20shifted%20towards%20parameter-efficient%20fine-tuning%0Atechniques%2C%20with%20many%20approaches%20building%20upon%20the%20framework%20that%20maintains%20a%0Apool%20of%20learnable%20prompts.%20Although%20effective%2C%20these%20methods%20introduce%0Asubstantial%20computational%20overhead%2C%20primarily%20due%20to%20prompt%20pool%20querying%20and%0Aincreased%20input%20sequence%20lengths%20from%20prompt%20concatenation.%20In%20this%20work%2C%20we%0Apresent%20a%20novel%20prompt-based%20approach%20that%20addresses%20this%20limitation.%20Our%0Amethod%20trains%20a%20single%20set%20of%20shared%20prompts%20across%20all%20tasks%20and%2C%20rather%20than%0Aconcatenating%20prompts%20to%20the%20input%2C%20directly%20modifies%20the%20CLS%20token%27s%20attention%0Acomputation%20by%20adding%20the%20prompts%20to%20it.%20This%20simple%20and%20lightweight%20design%20not%0Aonly%20significantly%20reduces%20computational%20complexity-both%20in%20terms%20of%20inference%0Acosts%20and%20the%20number%20of%20trainable%20parameters-but%20also%20eliminates%20the%20need%20to%0Aoptimize%20prompt%20lengths%20for%20different%20downstream%20tasks%2C%20offering%20a%20more%0Aefficient%20yet%20powerful%20solution%20for%20rehearsal-free%20class-incremental%20learning.%0AExtensive%20experiments%20across%20a%20diverse%20range%20of%20CIL%20benchmarks%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%2C%20highlighting%20its%20potential%20to%20establish%20a%20new%0Aprompt-based%20CIL%20paradigm.%20Furthermore%2C%20experiments%20on%20general%20recognition%0Abenchmarks%20beyond%20the%20CIL%20setting%20also%20show%20strong%20performance%2C%20positioning%20our%0Amethod%20as%20a%20promising%20candidate%20for%20a%20general%20parameter-efficient%20fine-tuning%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07979v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAchieving%2520More%2520with%2520Less%253A%2520Additive%2520Prompt%2520Tuning%2520for%2520Rehearsal-Free%250A%2520%2520Class-Incremental%2520Learning%26entry.906535625%3DHaoran%2520Chen%2520and%2520Ping%2520Wang%2520and%2520Zihan%2520Zhou%2520and%2520Xu%2520Zhang%2520and%2520Zuxuan%2520Wu%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Class-incremental%2520learning%2520%2528CIL%2529%2520enables%2520models%2520to%2520learn%2520new%2520classes%250Aprogressively%2520while%2520preserving%2520knowledge%2520of%2520previously%2520learned%2520ones.%2520Recent%250Aadvances%2520in%2520this%2520field%2520have%2520shifted%2520towards%2520parameter-efficient%2520fine-tuning%250Atechniques%252C%2520with%2520many%2520approaches%2520building%2520upon%2520the%2520framework%2520that%2520maintains%2520a%250Apool%2520of%2520learnable%2520prompts.%2520Although%2520effective%252C%2520these%2520methods%2520introduce%250Asubstantial%2520computational%2520overhead%252C%2520primarily%2520due%2520to%2520prompt%2520pool%2520querying%2520and%250Aincreased%2520input%2520sequence%2520lengths%2520from%2520prompt%2520concatenation.%2520In%2520this%2520work%252C%2520we%250Apresent%2520a%2520novel%2520prompt-based%2520approach%2520that%2520addresses%2520this%2520limitation.%2520Our%250Amethod%2520trains%2520a%2520single%2520set%2520of%2520shared%2520prompts%2520across%2520all%2520tasks%2520and%252C%2520rather%2520than%250Aconcatenating%2520prompts%2520to%2520the%2520input%252C%2520directly%2520modifies%2520the%2520CLS%2520token%2527s%2520attention%250Acomputation%2520by%2520adding%2520the%2520prompts%2520to%2520it.%2520This%2520simple%2520and%2520lightweight%2520design%2520not%250Aonly%2520significantly%2520reduces%2520computational%2520complexity-both%2520in%2520terms%2520of%2520inference%250Acosts%2520and%2520the%2520number%2520of%2520trainable%2520parameters-but%2520also%2520eliminates%2520the%2520need%2520to%250Aoptimize%2520prompt%2520lengths%2520for%2520different%2520downstream%2520tasks%252C%2520offering%2520a%2520more%250Aefficient%2520yet%2520powerful%2520solution%2520for%2520rehearsal-free%2520class-incremental%2520learning.%250AExtensive%2520experiments%2520across%2520a%2520diverse%2520range%2520of%2520CIL%2520benchmarks%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach%252C%2520highlighting%2520its%2520potential%2520to%2520establish%2520a%2520new%250Aprompt-based%2520CIL%2520paradigm.%2520Furthermore%252C%2520experiments%2520on%2520general%2520recognition%250Abenchmarks%2520beyond%2520the%2520CIL%2520setting%2520also%2520show%2520strong%2520performance%252C%2520positioning%2520our%250Amethod%2520as%2520a%2520promising%2520candidate%2520for%2520a%2520general%2520parameter-efficient%2520fine-tuning%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07979v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Achieving%20More%20with%20Less%3A%20Additive%20Prompt%20Tuning%20for%20Rehearsal-Free%0A%20%20Class-Incremental%20Learning&entry.906535625=Haoran%20Chen%20and%20Ping%20Wang%20and%20Zihan%20Zhou%20and%20Xu%20Zhang%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Class-incremental%20learning%20%28CIL%29%20enables%20models%20to%20learn%20new%20classes%0Aprogressively%20while%20preserving%20knowledge%20of%20previously%20learned%20ones.%20Recent%0Aadvances%20in%20this%20field%20have%20shifted%20towards%20parameter-efficient%20fine-tuning%0Atechniques%2C%20with%20many%20approaches%20building%20upon%20the%20framework%20that%20maintains%20a%0Apool%20of%20learnable%20prompts.%20Although%20effective%2C%20these%20methods%20introduce%0Asubstantial%20computational%20overhead%2C%20primarily%20due%20to%20prompt%20pool%20querying%20and%0Aincreased%20input%20sequence%20lengths%20from%20prompt%20concatenation.%20In%20this%20work%2C%20we%0Apresent%20a%20novel%20prompt-based%20approach%20that%20addresses%20this%20limitation.%20Our%0Amethod%20trains%20a%20single%20set%20of%20shared%20prompts%20across%20all%20tasks%20and%2C%20rather%20than%0Aconcatenating%20prompts%20to%20the%20input%2C%20directly%20modifies%20the%20CLS%20token%27s%20attention%0Acomputation%20by%20adding%20the%20prompts%20to%20it.%20This%20simple%20and%20lightweight%20design%20not%0Aonly%20significantly%20reduces%20computational%20complexity-both%20in%20terms%20of%20inference%0Acosts%20and%20the%20number%20of%20trainable%20parameters-but%20also%20eliminates%20the%20need%20to%0Aoptimize%20prompt%20lengths%20for%20different%20downstream%20tasks%2C%20offering%20a%20more%0Aefficient%20yet%20powerful%20solution%20for%20rehearsal-free%20class-incremental%20learning.%0AExtensive%20experiments%20across%20a%20diverse%20range%20of%20CIL%20benchmarks%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%2C%20highlighting%20its%20potential%20to%20establish%20a%20new%0Aprompt-based%20CIL%20paradigm.%20Furthermore%2C%20experiments%20on%20general%20recognition%0Abenchmarks%20beyond%20the%20CIL%20setting%20also%20show%20strong%20performance%2C%20positioning%20our%0Amethod%20as%20a%20promising%20candidate%20for%20a%20general%20parameter-efficient%20fine-tuning%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07979v2&entry.124074799=Read"},
{"title": "HiMat: DiT-based Ultra-High Resolution SVBRDF Generation", "author": "Zixiong Wang and Jian Yang and Yiwei Hu and Milos Hasan and Beibei Wang", "abstract": "  Creating highly detailed SVBRDFs is essential for 3D content creation. The\nrise of high-resolution text-to-image generative models, based on diffusion\ntransformers (DiT), suggests an opportunity to finetune them for this task.\nHowever, retargeting the models to produce multiple aligned SVBRDF maps instead\nof just RGB images, while achieving high efficiency and ensuring consistency\nacross different maps, remains a challenge. In this paper, we introduce HiMat:\na memory- and computation-efficient diffusion-based framework capable of\ngenerating native 4K-resolution SVBRDFs. A key challenge we address is\nmaintaining consistency across different maps in a lightweight manner, without\nrelying on training new VAEs or significantly altering the DiT backbone (which\nwould damage its prior capabilities). To tackle this, we introduce the\nCrossStitch module, a lightweight convolutional module that captures inter-map\ndependencies through localized operations. Its weights are initialized such\nthat the DiT backbone operation is unchanged before finetuning starts. HiMat\nenables generation with strong structural coherence and high-frequency details.\nResults with a large set of text prompts demonstrate the effectiveness of our\napproach for 4K SVBRDF generation. Further experiments suggest generalization\nto tasks such as intrinsic decomposition.\n", "link": "http://arxiv.org/abs/2508.07011v2", "date": "2025-08-12", "relevancy": 2.3524, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5912}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5875}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiMat%3A%20DiT-based%20Ultra-High%20Resolution%20SVBRDF%20Generation&body=Title%3A%20HiMat%3A%20DiT-based%20Ultra-High%20Resolution%20SVBRDF%20Generation%0AAuthor%3A%20Zixiong%20Wang%20and%20Jian%20Yang%20and%20Yiwei%20Hu%20and%20Milos%20Hasan%20and%20Beibei%20Wang%0AAbstract%3A%20%20%20Creating%20highly%20detailed%20SVBRDFs%20is%20essential%20for%203D%20content%20creation.%20The%0Arise%20of%20high-resolution%20text-to-image%20generative%20models%2C%20based%20on%20diffusion%0Atransformers%20%28DiT%29%2C%20suggests%20an%20opportunity%20to%20finetune%20them%20for%20this%20task.%0AHowever%2C%20retargeting%20the%20models%20to%20produce%20multiple%20aligned%20SVBRDF%20maps%20instead%0Aof%20just%20RGB%20images%2C%20while%20achieving%20high%20efficiency%20and%20ensuring%20consistency%0Aacross%20different%20maps%2C%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20introduce%20HiMat%3A%0Aa%20memory-%20and%20computation-efficient%20diffusion-based%20framework%20capable%20of%0Agenerating%20native%204K-resolution%20SVBRDFs.%20A%20key%20challenge%20we%20address%20is%0Amaintaining%20consistency%20across%20different%20maps%20in%20a%20lightweight%20manner%2C%20without%0Arelying%20on%20training%20new%20VAEs%20or%20significantly%20altering%20the%20DiT%20backbone%20%28which%0Awould%20damage%20its%20prior%20capabilities%29.%20To%20tackle%20this%2C%20we%20introduce%20the%0ACrossStitch%20module%2C%20a%20lightweight%20convolutional%20module%20that%20captures%20inter-map%0Adependencies%20through%20localized%20operations.%20Its%20weights%20are%20initialized%20such%0Athat%20the%20DiT%20backbone%20operation%20is%20unchanged%20before%20finetuning%20starts.%20HiMat%0Aenables%20generation%20with%20strong%20structural%20coherence%20and%20high-frequency%20details.%0AResults%20with%20a%20large%20set%20of%20text%20prompts%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%20for%204K%20SVBRDF%20generation.%20Further%20experiments%20suggest%20generalization%0Ato%20tasks%20such%20as%20intrinsic%20decomposition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07011v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiMat%253A%2520DiT-based%2520Ultra-High%2520Resolution%2520SVBRDF%2520Generation%26entry.906535625%3DZixiong%2520Wang%2520and%2520Jian%2520Yang%2520and%2520Yiwei%2520Hu%2520and%2520Milos%2520Hasan%2520and%2520Beibei%2520Wang%26entry.1292438233%3D%2520%2520Creating%2520highly%2520detailed%2520SVBRDFs%2520is%2520essential%2520for%25203D%2520content%2520creation.%2520The%250Arise%2520of%2520high-resolution%2520text-to-image%2520generative%2520models%252C%2520based%2520on%2520diffusion%250Atransformers%2520%2528DiT%2529%252C%2520suggests%2520an%2520opportunity%2520to%2520finetune%2520them%2520for%2520this%2520task.%250AHowever%252C%2520retargeting%2520the%2520models%2520to%2520produce%2520multiple%2520aligned%2520SVBRDF%2520maps%2520instead%250Aof%2520just%2520RGB%2520images%252C%2520while%2520achieving%2520high%2520efficiency%2520and%2520ensuring%2520consistency%250Aacross%2520different%2520maps%252C%2520remains%2520a%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520HiMat%253A%250Aa%2520memory-%2520and%2520computation-efficient%2520diffusion-based%2520framework%2520capable%2520of%250Agenerating%2520native%25204K-resolution%2520SVBRDFs.%2520A%2520key%2520challenge%2520we%2520address%2520is%250Amaintaining%2520consistency%2520across%2520different%2520maps%2520in%2520a%2520lightweight%2520manner%252C%2520without%250Arelying%2520on%2520training%2520new%2520VAEs%2520or%2520significantly%2520altering%2520the%2520DiT%2520backbone%2520%2528which%250Awould%2520damage%2520its%2520prior%2520capabilities%2529.%2520To%2520tackle%2520this%252C%2520we%2520introduce%2520the%250ACrossStitch%2520module%252C%2520a%2520lightweight%2520convolutional%2520module%2520that%2520captures%2520inter-map%250Adependencies%2520through%2520localized%2520operations.%2520Its%2520weights%2520are%2520initialized%2520such%250Athat%2520the%2520DiT%2520backbone%2520operation%2520is%2520unchanged%2520before%2520finetuning%2520starts.%2520HiMat%250Aenables%2520generation%2520with%2520strong%2520structural%2520coherence%2520and%2520high-frequency%2520details.%250AResults%2520with%2520a%2520large%2520set%2520of%2520text%2520prompts%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aapproach%2520for%25204K%2520SVBRDF%2520generation.%2520Further%2520experiments%2520suggest%2520generalization%250Ato%2520tasks%2520such%2520as%2520intrinsic%2520decomposition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07011v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiMat%3A%20DiT-based%20Ultra-High%20Resolution%20SVBRDF%20Generation&entry.906535625=Zixiong%20Wang%20and%20Jian%20Yang%20and%20Yiwei%20Hu%20and%20Milos%20Hasan%20and%20Beibei%20Wang&entry.1292438233=%20%20Creating%20highly%20detailed%20SVBRDFs%20is%20essential%20for%203D%20content%20creation.%20The%0Arise%20of%20high-resolution%20text-to-image%20generative%20models%2C%20based%20on%20diffusion%0Atransformers%20%28DiT%29%2C%20suggests%20an%20opportunity%20to%20finetune%20them%20for%20this%20task.%0AHowever%2C%20retargeting%20the%20models%20to%20produce%20multiple%20aligned%20SVBRDF%20maps%20instead%0Aof%20just%20RGB%20images%2C%20while%20achieving%20high%20efficiency%20and%20ensuring%20consistency%0Aacross%20different%20maps%2C%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20introduce%20HiMat%3A%0Aa%20memory-%20and%20computation-efficient%20diffusion-based%20framework%20capable%20of%0Agenerating%20native%204K-resolution%20SVBRDFs.%20A%20key%20challenge%20we%20address%20is%0Amaintaining%20consistency%20across%20different%20maps%20in%20a%20lightweight%20manner%2C%20without%0Arelying%20on%20training%20new%20VAEs%20or%20significantly%20altering%20the%20DiT%20backbone%20%28which%0Awould%20damage%20its%20prior%20capabilities%29.%20To%20tackle%20this%2C%20we%20introduce%20the%0ACrossStitch%20module%2C%20a%20lightweight%20convolutional%20module%20that%20captures%20inter-map%0Adependencies%20through%20localized%20operations.%20Its%20weights%20are%20initialized%20such%0Athat%20the%20DiT%20backbone%20operation%20is%20unchanged%20before%20finetuning%20starts.%20HiMat%0Aenables%20generation%20with%20strong%20structural%20coherence%20and%20high-frequency%20details.%0AResults%20with%20a%20large%20set%20of%20text%20prompts%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%20for%204K%20SVBRDF%20generation.%20Further%20experiments%20suggest%20generalization%0Ato%20tasks%20such%20as%20intrinsic%20decomposition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07011v2&entry.124074799=Read"},
{"title": "Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation", "author": "Zan Wang and Jingze Zhang and Yixin Chen and Baoxiong Jia and Wei Liang and Siyuan Huang", "abstract": "  Despite significant advancements in human motion generation, current motion\nrepresentations, typically formulated as discrete frame sequences, still face\ntwo critical limitations: (i) they fail to capture motion from a multi-scale\nperspective, limiting the capability in complex patterns modeling; (ii) they\nlack compositional flexibility, which is crucial for model's generalization in\ndiverse generation tasks. To address these challenges, we introduce MSQ, a\nnovel quantization method that compresses the motion sequence into multi-scale\ndiscrete tokens across spatial and temporal dimensions. MSQ employs distinct\nencoders to capture body parts at varying spatial granularities and temporally\ninterpolates the encoded features into multiple scales before quantizing them\ninto discrete tokens. Building on this representation, we establish a\ngenerative mask modeling model to effectively support motion editing, motion\ncontrol, and conditional motion generation. Through quantitative and\nqualitative analysis, we show that our quantization method enables the seamless\ncomposition of motion tokens without requiring specialized design or\nre-training. Furthermore, extensive evaluations demonstrate that our approach\noutperforms existing baseline methods on various benchmarks.\n", "link": "http://arxiv.org/abs/2508.08991v1", "date": "2025-08-12", "relevancy": 2.3408, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.613}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5799}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-Temporal%20Multi-Scale%20Quantization%20for%20Flexible%20Motion%20Generation&body=Title%3A%20Spatial-Temporal%20Multi-Scale%20Quantization%20for%20Flexible%20Motion%20Generation%0AAuthor%3A%20Zan%20Wang%20and%20Jingze%20Zhang%20and%20Yixin%20Chen%20and%20Baoxiong%20Jia%20and%20Wei%20Liang%20and%20Siyuan%20Huang%0AAbstract%3A%20%20%20Despite%20significant%20advancements%20in%20human%20motion%20generation%2C%20current%20motion%0Arepresentations%2C%20typically%20formulated%20as%20discrete%20frame%20sequences%2C%20still%20face%0Atwo%20critical%20limitations%3A%20%28i%29%20they%20fail%20to%20capture%20motion%20from%20a%20multi-scale%0Aperspective%2C%20limiting%20the%20capability%20in%20complex%20patterns%20modeling%3B%20%28ii%29%20they%0Alack%20compositional%20flexibility%2C%20which%20is%20crucial%20for%20model%27s%20generalization%20in%0Adiverse%20generation%20tasks.%20To%20address%20these%20challenges%2C%20we%20introduce%20MSQ%2C%20a%0Anovel%20quantization%20method%20that%20compresses%20the%20motion%20sequence%20into%20multi-scale%0Adiscrete%20tokens%20across%20spatial%20and%20temporal%20dimensions.%20MSQ%20employs%20distinct%0Aencoders%20to%20capture%20body%20parts%20at%20varying%20spatial%20granularities%20and%20temporally%0Ainterpolates%20the%20encoded%20features%20into%20multiple%20scales%20before%20quantizing%20them%0Ainto%20discrete%20tokens.%20Building%20on%20this%20representation%2C%20we%20establish%20a%0Agenerative%20mask%20modeling%20model%20to%20effectively%20support%20motion%20editing%2C%20motion%0Acontrol%2C%20and%20conditional%20motion%20generation.%20Through%20quantitative%20and%0Aqualitative%20analysis%2C%20we%20show%20that%20our%20quantization%20method%20enables%20the%20seamless%0Acomposition%20of%20motion%20tokens%20without%20requiring%20specialized%20design%20or%0Are-training.%20Furthermore%2C%20extensive%20evaluations%20demonstrate%20that%20our%20approach%0Aoutperforms%20existing%20baseline%20methods%20on%20various%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-Temporal%2520Multi-Scale%2520Quantization%2520for%2520Flexible%2520Motion%2520Generation%26entry.906535625%3DZan%2520Wang%2520and%2520Jingze%2520Zhang%2520and%2520Yixin%2520Chen%2520and%2520Baoxiong%2520Jia%2520and%2520Wei%2520Liang%2520and%2520Siyuan%2520Huang%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advancements%2520in%2520human%2520motion%2520generation%252C%2520current%2520motion%250Arepresentations%252C%2520typically%2520formulated%2520as%2520discrete%2520frame%2520sequences%252C%2520still%2520face%250Atwo%2520critical%2520limitations%253A%2520%2528i%2529%2520they%2520fail%2520to%2520capture%2520motion%2520from%2520a%2520multi-scale%250Aperspective%252C%2520limiting%2520the%2520capability%2520in%2520complex%2520patterns%2520modeling%253B%2520%2528ii%2529%2520they%250Alack%2520compositional%2520flexibility%252C%2520which%2520is%2520crucial%2520for%2520model%2527s%2520generalization%2520in%250Adiverse%2520generation%2520tasks.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520MSQ%252C%2520a%250Anovel%2520quantization%2520method%2520that%2520compresses%2520the%2520motion%2520sequence%2520into%2520multi-scale%250Adiscrete%2520tokens%2520across%2520spatial%2520and%2520temporal%2520dimensions.%2520MSQ%2520employs%2520distinct%250Aencoders%2520to%2520capture%2520body%2520parts%2520at%2520varying%2520spatial%2520granularities%2520and%2520temporally%250Ainterpolates%2520the%2520encoded%2520features%2520into%2520multiple%2520scales%2520before%2520quantizing%2520them%250Ainto%2520discrete%2520tokens.%2520Building%2520on%2520this%2520representation%252C%2520we%2520establish%2520a%250Agenerative%2520mask%2520modeling%2520model%2520to%2520effectively%2520support%2520motion%2520editing%252C%2520motion%250Acontrol%252C%2520and%2520conditional%2520motion%2520generation.%2520Through%2520quantitative%2520and%250Aqualitative%2520analysis%252C%2520we%2520show%2520that%2520our%2520quantization%2520method%2520enables%2520the%2520seamless%250Acomposition%2520of%2520motion%2520tokens%2520without%2520requiring%2520specialized%2520design%2520or%250Are-training.%2520Furthermore%252C%2520extensive%2520evaluations%2520demonstrate%2520that%2520our%2520approach%250Aoutperforms%2520existing%2520baseline%2520methods%2520on%2520various%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-Temporal%20Multi-Scale%20Quantization%20for%20Flexible%20Motion%20Generation&entry.906535625=Zan%20Wang%20and%20Jingze%20Zhang%20and%20Yixin%20Chen%20and%20Baoxiong%20Jia%20and%20Wei%20Liang%20and%20Siyuan%20Huang&entry.1292438233=%20%20Despite%20significant%20advancements%20in%20human%20motion%20generation%2C%20current%20motion%0Arepresentations%2C%20typically%20formulated%20as%20discrete%20frame%20sequences%2C%20still%20face%0Atwo%20critical%20limitations%3A%20%28i%29%20they%20fail%20to%20capture%20motion%20from%20a%20multi-scale%0Aperspective%2C%20limiting%20the%20capability%20in%20complex%20patterns%20modeling%3B%20%28ii%29%20they%0Alack%20compositional%20flexibility%2C%20which%20is%20crucial%20for%20model%27s%20generalization%20in%0Adiverse%20generation%20tasks.%20To%20address%20these%20challenges%2C%20we%20introduce%20MSQ%2C%20a%0Anovel%20quantization%20method%20that%20compresses%20the%20motion%20sequence%20into%20multi-scale%0Adiscrete%20tokens%20across%20spatial%20and%20temporal%20dimensions.%20MSQ%20employs%20distinct%0Aencoders%20to%20capture%20body%20parts%20at%20varying%20spatial%20granularities%20and%20temporally%0Ainterpolates%20the%20encoded%20features%20into%20multiple%20scales%20before%20quantizing%20them%0Ainto%20discrete%20tokens.%20Building%20on%20this%20representation%2C%20we%20establish%20a%0Agenerative%20mask%20modeling%20model%20to%20effectively%20support%20motion%20editing%2C%20motion%0Acontrol%2C%20and%20conditional%20motion%20generation.%20Through%20quantitative%20and%0Aqualitative%20analysis%2C%20we%20show%20that%20our%20quantization%20method%20enables%20the%20seamless%0Acomposition%20of%20motion%20tokens%20without%20requiring%20specialized%20design%20or%0Are-training.%20Furthermore%2C%20extensive%20evaluations%20demonstrate%20that%20our%20approach%0Aoutperforms%20existing%20baseline%20methods%20on%20various%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08991v1&entry.124074799=Read"},
{"title": "Complex Logical Instruction Generation", "author": "Mian Zhang and Shujian Liu and Sixun Dong and Ming Yin and Yebowen Hu and Xun Wang and Steven Ma and Song Wang and Sathish Reddy Indurthi and Haoyun Deng and Zhiyu Zoey Chen and Kaiqiang Song", "abstract": "  Instruction following has catalyzed the recent era of Large Language Models\n(LLMs) and is the foundational skill underpinning more advanced capabilities\nsuch as reasoning and agentic behaviors. As tasks grow more challenging, the\nlogic structures embedded in natural language instructions becomes increasingly\nintricate. However, how well LLMs perform on such logic-rich instructions\nremains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a\nscalable, automated framework for generating verifiable instructions from code\nfunctions, which can naturally express rich logic such as conditionals,\nnesting, recursion, and function calls. We further curate a collection of\ncomplex code functions and use LogicIFGen to construct LogicIFEval, a benchmark\ncomprising 426 verifiable logic-rich instructions. Our experiments demonstrate\nthat current state-of-the-art LLMs still struggle to correctly follow the\ninstructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the\ninstructions, revealing significant deficiencies in the instruction-following\nability. Code and Benchmark: https://github.com/mianzhang/LogicIF\n", "link": "http://arxiv.org/abs/2508.09125v1", "date": "2025-08-12", "relevancy": 2.3314, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.47}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Complex%20Logical%20Instruction%20Generation&body=Title%3A%20Complex%20Logical%20Instruction%20Generation%0AAuthor%3A%20Mian%20Zhang%20and%20Shujian%20Liu%20and%20Sixun%20Dong%20and%20Ming%20Yin%20and%20Yebowen%20Hu%20and%20Xun%20Wang%20and%20Steven%20Ma%20and%20Song%20Wang%20and%20Sathish%20Reddy%20Indurthi%20and%20Haoyun%20Deng%20and%20Zhiyu%20Zoey%20Chen%20and%20Kaiqiang%20Song%0AAbstract%3A%20%20%20Instruction%20following%20has%20catalyzed%20the%20recent%20era%20of%20Large%20Language%20Models%0A%28LLMs%29%20and%20is%20the%20foundational%20skill%20underpinning%20more%20advanced%20capabilities%0Asuch%20as%20reasoning%20and%20agentic%20behaviors.%20As%20tasks%20grow%20more%20challenging%2C%20the%0Alogic%20structures%20embedded%20in%20natural%20language%20instructions%20becomes%20increasingly%0Aintricate.%20However%2C%20how%20well%20LLMs%20perform%20on%20such%20logic-rich%20instructions%0Aremains%20under-explored.%20We%20propose%20LogicIFGen%20and%20LogicIFEval.%20LogicIFGen%20is%20a%0Ascalable%2C%20automated%20framework%20for%20generating%20verifiable%20instructions%20from%20code%0Afunctions%2C%20which%20can%20naturally%20express%20rich%20logic%20such%20as%20conditionals%2C%0Anesting%2C%20recursion%2C%20and%20function%20calls.%20We%20further%20curate%20a%20collection%20of%0Acomplex%20code%20functions%20and%20use%20LogicIFGen%20to%20construct%20LogicIFEval%2C%20a%20benchmark%0Acomprising%20426%20verifiable%20logic-rich%20instructions.%20Our%20experiments%20demonstrate%0Athat%20current%20state-of-the-art%20LLMs%20still%20struggle%20to%20correctly%20follow%20the%0Ainstructions%20in%20LogicIFEval.%20Most%20LLMs%20can%20only%20follow%20fewer%20than%2060%25%20of%20the%0Ainstructions%2C%20revealing%20significant%20deficiencies%20in%20the%20instruction-following%0Aability.%20Code%20and%20Benchmark%3A%20https%3A//github.com/mianzhang/LogicIF%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComplex%2520Logical%2520Instruction%2520Generation%26entry.906535625%3DMian%2520Zhang%2520and%2520Shujian%2520Liu%2520and%2520Sixun%2520Dong%2520and%2520Ming%2520Yin%2520and%2520Yebowen%2520Hu%2520and%2520Xun%2520Wang%2520and%2520Steven%2520Ma%2520and%2520Song%2520Wang%2520and%2520Sathish%2520Reddy%2520Indurthi%2520and%2520Haoyun%2520Deng%2520and%2520Zhiyu%2520Zoey%2520Chen%2520and%2520Kaiqiang%2520Song%26entry.1292438233%3D%2520%2520Instruction%2520following%2520has%2520catalyzed%2520the%2520recent%2520era%2520of%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520and%2520is%2520the%2520foundational%2520skill%2520underpinning%2520more%2520advanced%2520capabilities%250Asuch%2520as%2520reasoning%2520and%2520agentic%2520behaviors.%2520As%2520tasks%2520grow%2520more%2520challenging%252C%2520the%250Alogic%2520structures%2520embedded%2520in%2520natural%2520language%2520instructions%2520becomes%2520increasingly%250Aintricate.%2520However%252C%2520how%2520well%2520LLMs%2520perform%2520on%2520such%2520logic-rich%2520instructions%250Aremains%2520under-explored.%2520We%2520propose%2520LogicIFGen%2520and%2520LogicIFEval.%2520LogicIFGen%2520is%2520a%250Ascalable%252C%2520automated%2520framework%2520for%2520generating%2520verifiable%2520instructions%2520from%2520code%250Afunctions%252C%2520which%2520can%2520naturally%2520express%2520rich%2520logic%2520such%2520as%2520conditionals%252C%250Anesting%252C%2520recursion%252C%2520and%2520function%2520calls.%2520We%2520further%2520curate%2520a%2520collection%2520of%250Acomplex%2520code%2520functions%2520and%2520use%2520LogicIFGen%2520to%2520construct%2520LogicIFEval%252C%2520a%2520benchmark%250Acomprising%2520426%2520verifiable%2520logic-rich%2520instructions.%2520Our%2520experiments%2520demonstrate%250Athat%2520current%2520state-of-the-art%2520LLMs%2520still%2520struggle%2520to%2520correctly%2520follow%2520the%250Ainstructions%2520in%2520LogicIFEval.%2520Most%2520LLMs%2520can%2520only%2520follow%2520fewer%2520than%252060%2525%2520of%2520the%250Ainstructions%252C%2520revealing%2520significant%2520deficiencies%2520in%2520the%2520instruction-following%250Aability.%2520Code%2520and%2520Benchmark%253A%2520https%253A//github.com/mianzhang/LogicIF%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Complex%20Logical%20Instruction%20Generation&entry.906535625=Mian%20Zhang%20and%20Shujian%20Liu%20and%20Sixun%20Dong%20and%20Ming%20Yin%20and%20Yebowen%20Hu%20and%20Xun%20Wang%20and%20Steven%20Ma%20and%20Song%20Wang%20and%20Sathish%20Reddy%20Indurthi%20and%20Haoyun%20Deng%20and%20Zhiyu%20Zoey%20Chen%20and%20Kaiqiang%20Song&entry.1292438233=%20%20Instruction%20following%20has%20catalyzed%20the%20recent%20era%20of%20Large%20Language%20Models%0A%28LLMs%29%20and%20is%20the%20foundational%20skill%20underpinning%20more%20advanced%20capabilities%0Asuch%20as%20reasoning%20and%20agentic%20behaviors.%20As%20tasks%20grow%20more%20challenging%2C%20the%0Alogic%20structures%20embedded%20in%20natural%20language%20instructions%20becomes%20increasingly%0Aintricate.%20However%2C%20how%20well%20LLMs%20perform%20on%20such%20logic-rich%20instructions%0Aremains%20under-explored.%20We%20propose%20LogicIFGen%20and%20LogicIFEval.%20LogicIFGen%20is%20a%0Ascalable%2C%20automated%20framework%20for%20generating%20verifiable%20instructions%20from%20code%0Afunctions%2C%20which%20can%20naturally%20express%20rich%20logic%20such%20as%20conditionals%2C%0Anesting%2C%20recursion%2C%20and%20function%20calls.%20We%20further%20curate%20a%20collection%20of%0Acomplex%20code%20functions%20and%20use%20LogicIFGen%20to%20construct%20LogicIFEval%2C%20a%20benchmark%0Acomprising%20426%20verifiable%20logic-rich%20instructions.%20Our%20experiments%20demonstrate%0Athat%20current%20state-of-the-art%20LLMs%20still%20struggle%20to%20correctly%20follow%20the%0Ainstructions%20in%20LogicIFEval.%20Most%20LLMs%20can%20only%20follow%20fewer%20than%2060%25%20of%20the%0Ainstructions%2C%20revealing%20significant%20deficiencies%20in%20the%20instruction-following%0Aability.%20Code%20and%20Benchmark%3A%20https%3A//github.com/mianzhang/LogicIF%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09125v1&entry.124074799=Read"},
{"title": "TaoCache: Structure-Maintained Video Generation Acceleration", "author": "Zhentao Fan and Zongzuo Wang and Weiwei Zhang", "abstract": "  Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups.\n", "link": "http://arxiv.org/abs/2508.08978v1", "date": "2025-08-12", "relevancy": 2.3216, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6004}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.584}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TaoCache%3A%20Structure-Maintained%20Video%20Generation%20Acceleration&body=Title%3A%20TaoCache%3A%20Structure-Maintained%20Video%20Generation%20Acceleration%0AAuthor%3A%20Zhentao%20Fan%20and%20Zongzuo%20Wang%20and%20Weiwei%20Zhang%0AAbstract%3A%20%20%20Existing%20cache-based%20acceleration%20methods%20for%20video%20diffusion%20models%0Aprimarily%20skip%20early%20or%20mid%20denoising%20steps%2C%20which%20often%20leads%20to%20structural%0Adiscrepancies%20relative%20to%20full-timestep%20generation%20and%20can%20hinder%20instruction%0Afollowing%20and%20character%20consistency.%20We%20present%20TaoCache%2C%20a%20training-free%2C%0Aplug-and-play%20caching%20strategy%20that%2C%20instead%20of%20residual-based%20caching%2C%20adopts%0Aa%20fixed-point%20perspective%20to%20predict%20the%20model%27s%20noise%20output%20and%20is%0Aspecifically%20effective%20in%20late%20denoising%20stages.%20By%20calibrating%20cosine%0Asimilarities%20and%20norm%20ratios%20of%20consecutive%20noise%20deltas%2C%20TaoCache%20preserves%0Ahigh-resolution%20structure%20while%20enabling%20aggressive%20skipping.%20The%20approach%20is%0Aorthogonal%20to%20complementary%20accelerations%20such%20as%20Pyramid%20Attention%20Broadcast%0A%28PAB%29%20and%20TeaCache%2C%20and%20it%20integrates%20seamlessly%20into%20DiT-based%20frameworks.%0AAcross%20Latte-1%2C%20OpenSora-Plan%20v110%2C%20and%20Wan2.1%2C%20TaoCache%20attains%20substantially%0Ahigher%20visual%20quality%20%28LPIPS%2C%20SSIM%2C%20PSNR%29%20than%20prior%20caching%20methods%20under%20the%0Asame%20speedups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaoCache%253A%2520Structure-Maintained%2520Video%2520Generation%2520Acceleration%26entry.906535625%3DZhentao%2520Fan%2520and%2520Zongzuo%2520Wang%2520and%2520Weiwei%2520Zhang%26entry.1292438233%3D%2520%2520Existing%2520cache-based%2520acceleration%2520methods%2520for%2520video%2520diffusion%2520models%250Aprimarily%2520skip%2520early%2520or%2520mid%2520denoising%2520steps%252C%2520which%2520often%2520leads%2520to%2520structural%250Adiscrepancies%2520relative%2520to%2520full-timestep%2520generation%2520and%2520can%2520hinder%2520instruction%250Afollowing%2520and%2520character%2520consistency.%2520We%2520present%2520TaoCache%252C%2520a%2520training-free%252C%250Aplug-and-play%2520caching%2520strategy%2520that%252C%2520instead%2520of%2520residual-based%2520caching%252C%2520adopts%250Aa%2520fixed-point%2520perspective%2520to%2520predict%2520the%2520model%2527s%2520noise%2520output%2520and%2520is%250Aspecifically%2520effective%2520in%2520late%2520denoising%2520stages.%2520By%2520calibrating%2520cosine%250Asimilarities%2520and%2520norm%2520ratios%2520of%2520consecutive%2520noise%2520deltas%252C%2520TaoCache%2520preserves%250Ahigh-resolution%2520structure%2520while%2520enabling%2520aggressive%2520skipping.%2520The%2520approach%2520is%250Aorthogonal%2520to%2520complementary%2520accelerations%2520such%2520as%2520Pyramid%2520Attention%2520Broadcast%250A%2528PAB%2529%2520and%2520TeaCache%252C%2520and%2520it%2520integrates%2520seamlessly%2520into%2520DiT-based%2520frameworks.%250AAcross%2520Latte-1%252C%2520OpenSora-Plan%2520v110%252C%2520and%2520Wan2.1%252C%2520TaoCache%2520attains%2520substantially%250Ahigher%2520visual%2520quality%2520%2528LPIPS%252C%2520SSIM%252C%2520PSNR%2529%2520than%2520prior%2520caching%2520methods%2520under%2520the%250Asame%2520speedups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TaoCache%3A%20Structure-Maintained%20Video%20Generation%20Acceleration&entry.906535625=Zhentao%20Fan%20and%20Zongzuo%20Wang%20and%20Weiwei%20Zhang&entry.1292438233=%20%20Existing%20cache-based%20acceleration%20methods%20for%20video%20diffusion%20models%0Aprimarily%20skip%20early%20or%20mid%20denoising%20steps%2C%20which%20often%20leads%20to%20structural%0Adiscrepancies%20relative%20to%20full-timestep%20generation%20and%20can%20hinder%20instruction%0Afollowing%20and%20character%20consistency.%20We%20present%20TaoCache%2C%20a%20training-free%2C%0Aplug-and-play%20caching%20strategy%20that%2C%20instead%20of%20residual-based%20caching%2C%20adopts%0Aa%20fixed-point%20perspective%20to%20predict%20the%20model%27s%20noise%20output%20and%20is%0Aspecifically%20effective%20in%20late%20denoising%20stages.%20By%20calibrating%20cosine%0Asimilarities%20and%20norm%20ratios%20of%20consecutive%20noise%20deltas%2C%20TaoCache%20preserves%0Ahigh-resolution%20structure%20while%20enabling%20aggressive%20skipping.%20The%20approach%20is%0Aorthogonal%20to%20complementary%20accelerations%20such%20as%20Pyramid%20Attention%20Broadcast%0A%28PAB%29%20and%20TeaCache%2C%20and%20it%20integrates%20seamlessly%20into%20DiT-based%20frameworks.%0AAcross%20Latte-1%2C%20OpenSora-Plan%20v110%2C%20and%20Wan2.1%2C%20TaoCache%20attains%20substantially%0Ahigher%20visual%20quality%20%28LPIPS%2C%20SSIM%2C%20PSNR%29%20than%20prior%20caching%20methods%20under%20the%0Asame%20speedups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08978v1&entry.124074799=Read"},
{"title": "Argus Inspection: Do Multimodal Large Language Models Possess the Eye of\n  Panoptes?", "author": "Yang Yao and Lingyu Li and Jiaxin Song and Chiyu Chen and Zhenqi He and Yixu Wang and Xin Wang and Tianle Gu and Jie Li and Yan Teng and Yingchun Wang", "abstract": "  As Multimodal Large Language Models (MLLMs) continue to evolve, their\ncognitive and reasoning capabilities have seen remarkable progress. However,\nchallenges in visual fine-grained perception and commonsense causal inference\npersist. This paper introduces Argus Inspection, a multimodal benchmark with\ntwo levels of difficulty, emphasizing detailed visual recognition while\nincorporating real-world commonsense understanding to evaluate causal reasoning\nabilities. Expanding on it, we present the Eye of Panoptes framework, which\nintegrates a binary parametric Sigmoid metric with an indicator function,\nenabling a more holistic evaluation of MLLMs' responses in opinion-based\nreasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the\nhighest performance in visual fine-grained reasoning reaches only 0.46,\nhighlighting considerable potential for enhancement. Our research offers\nvaluable perspectives for the continued refinement of MLLMs.\n", "link": "http://arxiv.org/abs/2506.14805v2", "date": "2025-08-12", "relevancy": 2.3145, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5832}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5832}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Argus%20Inspection%3A%20Do%20Multimodal%20Large%20Language%20Models%20Possess%20the%20Eye%20of%0A%20%20Panoptes%3F&body=Title%3A%20Argus%20Inspection%3A%20Do%20Multimodal%20Large%20Language%20Models%20Possess%20the%20Eye%20of%0A%20%20Panoptes%3F%0AAuthor%3A%20Yang%20Yao%20and%20Lingyu%20Li%20and%20Jiaxin%20Song%20and%20Chiyu%20Chen%20and%20Zhenqi%20He%20and%20Yixu%20Wang%20and%20Xin%20Wang%20and%20Tianle%20Gu%20and%20Jie%20Li%20and%20Yan%20Teng%20and%20Yingchun%20Wang%0AAbstract%3A%20%20%20As%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20continue%20to%20evolve%2C%20their%0Acognitive%20and%20reasoning%20capabilities%20have%20seen%20remarkable%20progress.%20However%2C%0Achallenges%20in%20visual%20fine-grained%20perception%20and%20commonsense%20causal%20inference%0Apersist.%20This%20paper%20introduces%20Argus%20Inspection%2C%20a%20multimodal%20benchmark%20with%0Atwo%20levels%20of%20difficulty%2C%20emphasizing%20detailed%20visual%20recognition%20while%0Aincorporating%20real-world%20commonsense%20understanding%20to%20evaluate%20causal%20reasoning%0Aabilities.%20Expanding%20on%20it%2C%20we%20present%20the%20Eye%20of%20Panoptes%20framework%2C%20which%0Aintegrates%20a%20binary%20parametric%20Sigmoid%20metric%20with%20an%20indicator%20function%2C%0Aenabling%20a%20more%20holistic%20evaluation%20of%20MLLMs%27%20responses%20in%20opinion-based%0Areasoning%20tasks.%20Experiments%20conducted%20on%2026%20mainstream%20MLLMs%20reveal%20that%20the%0Ahighest%20performance%20in%20visual%20fine-grained%20reasoning%20reaches%20only%200.46%2C%0Ahighlighting%20considerable%20potential%20for%20enhancement.%20Our%20research%20offers%0Avaluable%20perspectives%20for%20the%20continued%20refinement%20of%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.14805v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArgus%2520Inspection%253A%2520Do%2520Multimodal%2520Large%2520Language%2520Models%2520Possess%2520the%2520Eye%2520of%250A%2520%2520Panoptes%253F%26entry.906535625%3DYang%2520Yao%2520and%2520Lingyu%2520Li%2520and%2520Jiaxin%2520Song%2520and%2520Chiyu%2520Chen%2520and%2520Zhenqi%2520He%2520and%2520Yixu%2520Wang%2520and%2520Xin%2520Wang%2520and%2520Tianle%2520Gu%2520and%2520Jie%2520Li%2520and%2520Yan%2520Teng%2520and%2520Yingchun%2520Wang%26entry.1292438233%3D%2520%2520As%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520continue%2520to%2520evolve%252C%2520their%250Acognitive%2520and%2520reasoning%2520capabilities%2520have%2520seen%2520remarkable%2520progress.%2520However%252C%250Achallenges%2520in%2520visual%2520fine-grained%2520perception%2520and%2520commonsense%2520causal%2520inference%250Apersist.%2520This%2520paper%2520introduces%2520Argus%2520Inspection%252C%2520a%2520multimodal%2520benchmark%2520with%250Atwo%2520levels%2520of%2520difficulty%252C%2520emphasizing%2520detailed%2520visual%2520recognition%2520while%250Aincorporating%2520real-world%2520commonsense%2520understanding%2520to%2520evaluate%2520causal%2520reasoning%250Aabilities.%2520Expanding%2520on%2520it%252C%2520we%2520present%2520the%2520Eye%2520of%2520Panoptes%2520framework%252C%2520which%250Aintegrates%2520a%2520binary%2520parametric%2520Sigmoid%2520metric%2520with%2520an%2520indicator%2520function%252C%250Aenabling%2520a%2520more%2520holistic%2520evaluation%2520of%2520MLLMs%2527%2520responses%2520in%2520opinion-based%250Areasoning%2520tasks.%2520Experiments%2520conducted%2520on%252026%2520mainstream%2520MLLMs%2520reveal%2520that%2520the%250Ahighest%2520performance%2520in%2520visual%2520fine-grained%2520reasoning%2520reaches%2520only%25200.46%252C%250Ahighlighting%2520considerable%2520potential%2520for%2520enhancement.%2520Our%2520research%2520offers%250Avaluable%2520perspectives%2520for%2520the%2520continued%2520refinement%2520of%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14805v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Argus%20Inspection%3A%20Do%20Multimodal%20Large%20Language%20Models%20Possess%20the%20Eye%20of%0A%20%20Panoptes%3F&entry.906535625=Yang%20Yao%20and%20Lingyu%20Li%20and%20Jiaxin%20Song%20and%20Chiyu%20Chen%20and%20Zhenqi%20He%20and%20Yixu%20Wang%20and%20Xin%20Wang%20and%20Tianle%20Gu%20and%20Jie%20Li%20and%20Yan%20Teng%20and%20Yingchun%20Wang&entry.1292438233=%20%20As%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20continue%20to%20evolve%2C%20their%0Acognitive%20and%20reasoning%20capabilities%20have%20seen%20remarkable%20progress.%20However%2C%0Achallenges%20in%20visual%20fine-grained%20perception%20and%20commonsense%20causal%20inference%0Apersist.%20This%20paper%20introduces%20Argus%20Inspection%2C%20a%20multimodal%20benchmark%20with%0Atwo%20levels%20of%20difficulty%2C%20emphasizing%20detailed%20visual%20recognition%20while%0Aincorporating%20real-world%20commonsense%20understanding%20to%20evaluate%20causal%20reasoning%0Aabilities.%20Expanding%20on%20it%2C%20we%20present%20the%20Eye%20of%20Panoptes%20framework%2C%20which%0Aintegrates%20a%20binary%20parametric%20Sigmoid%20metric%20with%20an%20indicator%20function%2C%0Aenabling%20a%20more%20holistic%20evaluation%20of%20MLLMs%27%20responses%20in%20opinion-based%0Areasoning%20tasks.%20Experiments%20conducted%20on%2026%20mainstream%20MLLMs%20reveal%20that%20the%0Ahighest%20performance%20in%20visual%20fine-grained%20reasoning%20reaches%20only%200.46%2C%0Ahighlighting%20considerable%20potential%20for%20enhancement.%20Our%20research%20offers%0Avaluable%20perspectives%20for%20the%20continued%20refinement%20of%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.14805v2&entry.124074799=Read"},
{"title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from\n  Experience", "author": "Zeyi Sun and Ziyu Liu and Yuhang Zang and Yuhang Cao and Xiaoyi Dong and Tong Wu and Dahua Lin and Jiaqi Wang", "abstract": "  Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS.\n", "link": "http://arxiv.org/abs/2508.04700v2", "date": "2025-08-12", "relevancy": 2.291, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6042}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5688}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEAgent%3A%20Self-Evolving%20Computer%20Use%20Agent%20with%20Autonomous%20Learning%20from%0A%20%20Experience&body=Title%3A%20SEAgent%3A%20Self-Evolving%20Computer%20Use%20Agent%20with%20Autonomous%20Learning%20from%0A%20%20Experience%0AAuthor%3A%20Zeyi%20Sun%20and%20Ziyu%20Liu%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Xiaoyi%20Dong%20and%20Tong%20Wu%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Repurposing%20large%20vision-language%20models%20%28LVLMs%29%20as%20computer%20use%20agents%0A%28CUAs%29%20has%20led%20to%20substantial%20breakthroughs%2C%20primarily%20driven%20by%20human-labeled%0Adata.%20However%2C%20these%20models%20often%20struggle%20with%20novel%20and%20specialized%20software%2C%0Aparticularly%20in%20scenarios%20lacking%20human%20annotations.%20To%20address%20this%20challenge%2C%0Awe%20propose%20SEAgent%2C%20an%20agentic%20self-evolving%20framework%20enabling%20CUAs%20to%0Aautonomously%20evolve%20through%20interactions%20with%20unfamiliar%20software.%0ASpecifically%2C%20SEAgent%20empowers%20computer-use%20agents%20to%20autonomously%20master%20novel%0Asoftware%20environments%20via%20experiential%20learning%2C%20where%20agents%20explore%20new%0Asoftware%2C%20learn%20through%20iterative%20trial-and-error%2C%20and%20progressively%20tackle%0Aauto-generated%20tasks%20organized%20from%20simple%20to%20complex.%20To%20achieve%20this%20goal%2C%20we%0Adesign%20a%20World%20State%20Model%20for%20step-wise%20trajectory%20assessment%2C%20along%20with%20a%0ACurriculum%20Generator%20that%20generates%20increasingly%20diverse%20and%20challenging%20tasks.%0AThe%20agent%27s%20policy%20is%20updated%20through%20experiential%20learning%2C%20comprised%20of%0Aadversarial%20imitation%20of%20failure%20actions%20and%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%20on%20successful%20ones.%20Furthermore%2C%20we%20introduce%20a%20specialist-to-generalist%0Atraining%20strategy%20that%20integrates%20individual%20experiential%20insights%20from%0Aspecialist%20agents%2C%20facilitating%20the%20development%20of%20a%20stronger%20generalist%20CUA%0Acapable%20of%20continuous%20autonomous%20evolution.%20This%20unified%20agent%20ultimately%0Aachieves%20performance%20surpassing%20ensembles%20of%20individual%20specialist%20agents%20on%0Atheir%20specialized%20software.%20We%20validate%20the%20effectiveness%20of%20SEAgent%20across%0Afive%20novel%20software%20environments%20within%20OS-World.%20Our%20approach%20achieves%20a%0Asignificant%20improvement%20of%2023.2%25%20in%20success%20rate%2C%20from%2011.3%25%20to%2034.5%25%2C%20over%20a%0Acompetitive%20open-source%20CUA%2C%20i.e.%2C%20UI-TARS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04700v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEAgent%253A%2520Self-Evolving%2520Computer%2520Use%2520Agent%2520with%2520Autonomous%2520Learning%2520from%250A%2520%2520Experience%26entry.906535625%3DZeyi%2520Sun%2520and%2520Ziyu%2520Liu%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Xiaoyi%2520Dong%2520and%2520Tong%2520Wu%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Repurposing%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520as%2520computer%2520use%2520agents%250A%2528CUAs%2529%2520has%2520led%2520to%2520substantial%2520breakthroughs%252C%2520primarily%2520driven%2520by%2520human-labeled%250Adata.%2520However%252C%2520these%2520models%2520often%2520struggle%2520with%2520novel%2520and%2520specialized%2520software%252C%250Aparticularly%2520in%2520scenarios%2520lacking%2520human%2520annotations.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520propose%2520SEAgent%252C%2520an%2520agentic%2520self-evolving%2520framework%2520enabling%2520CUAs%2520to%250Aautonomously%2520evolve%2520through%2520interactions%2520with%2520unfamiliar%2520software.%250ASpecifically%252C%2520SEAgent%2520empowers%2520computer-use%2520agents%2520to%2520autonomously%2520master%2520novel%250Asoftware%2520environments%2520via%2520experiential%2520learning%252C%2520where%2520agents%2520explore%2520new%250Asoftware%252C%2520learn%2520through%2520iterative%2520trial-and-error%252C%2520and%2520progressively%2520tackle%250Aauto-generated%2520tasks%2520organized%2520from%2520simple%2520to%2520complex.%2520To%2520achieve%2520this%2520goal%252C%2520we%250Adesign%2520a%2520World%2520State%2520Model%2520for%2520step-wise%2520trajectory%2520assessment%252C%2520along%2520with%2520a%250ACurriculum%2520Generator%2520that%2520generates%2520increasingly%2520diverse%2520and%2520challenging%2520tasks.%250AThe%2520agent%2527s%2520policy%2520is%2520updated%2520through%2520experiential%2520learning%252C%2520comprised%2520of%250Aadversarial%2520imitation%2520of%2520failure%2520actions%2520and%2520Group%2520Relative%2520Policy%2520Optimization%250A%2528GRPO%2529%2520on%2520successful%2520ones.%2520Furthermore%252C%2520we%2520introduce%2520a%2520specialist-to-generalist%250Atraining%2520strategy%2520that%2520integrates%2520individual%2520experiential%2520insights%2520from%250Aspecialist%2520agents%252C%2520facilitating%2520the%2520development%2520of%2520a%2520stronger%2520generalist%2520CUA%250Acapable%2520of%2520continuous%2520autonomous%2520evolution.%2520This%2520unified%2520agent%2520ultimately%250Aachieves%2520performance%2520surpassing%2520ensembles%2520of%2520individual%2520specialist%2520agents%2520on%250Atheir%2520specialized%2520software.%2520We%2520validate%2520the%2520effectiveness%2520of%2520SEAgent%2520across%250Afive%2520novel%2520software%2520environments%2520within%2520OS-World.%2520Our%2520approach%2520achieves%2520a%250Asignificant%2520improvement%2520of%252023.2%2525%2520in%2520success%2520rate%252C%2520from%252011.3%2525%2520to%252034.5%2525%252C%2520over%2520a%250Acompetitive%2520open-source%2520CUA%252C%2520i.e.%252C%2520UI-TARS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04700v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEAgent%3A%20Self-Evolving%20Computer%20Use%20Agent%20with%20Autonomous%20Learning%20from%0A%20%20Experience&entry.906535625=Zeyi%20Sun%20and%20Ziyu%20Liu%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Xiaoyi%20Dong%20and%20Tong%20Wu%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Repurposing%20large%20vision-language%20models%20%28LVLMs%29%20as%20computer%20use%20agents%0A%28CUAs%29%20has%20led%20to%20substantial%20breakthroughs%2C%20primarily%20driven%20by%20human-labeled%0Adata.%20However%2C%20these%20models%20often%20struggle%20with%20novel%20and%20specialized%20software%2C%0Aparticularly%20in%20scenarios%20lacking%20human%20annotations.%20To%20address%20this%20challenge%2C%0Awe%20propose%20SEAgent%2C%20an%20agentic%20self-evolving%20framework%20enabling%20CUAs%20to%0Aautonomously%20evolve%20through%20interactions%20with%20unfamiliar%20software.%0ASpecifically%2C%20SEAgent%20empowers%20computer-use%20agents%20to%20autonomously%20master%20novel%0Asoftware%20environments%20via%20experiential%20learning%2C%20where%20agents%20explore%20new%0Asoftware%2C%20learn%20through%20iterative%20trial-and-error%2C%20and%20progressively%20tackle%0Aauto-generated%20tasks%20organized%20from%20simple%20to%20complex.%20To%20achieve%20this%20goal%2C%20we%0Adesign%20a%20World%20State%20Model%20for%20step-wise%20trajectory%20assessment%2C%20along%20with%20a%0ACurriculum%20Generator%20that%20generates%20increasingly%20diverse%20and%20challenging%20tasks.%0AThe%20agent%27s%20policy%20is%20updated%20through%20experiential%20learning%2C%20comprised%20of%0Aadversarial%20imitation%20of%20failure%20actions%20and%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%20on%20successful%20ones.%20Furthermore%2C%20we%20introduce%20a%20specialist-to-generalist%0Atraining%20strategy%20that%20integrates%20individual%20experiential%20insights%20from%0Aspecialist%20agents%2C%20facilitating%20the%20development%20of%20a%20stronger%20generalist%20CUA%0Acapable%20of%20continuous%20autonomous%20evolution.%20This%20unified%20agent%20ultimately%0Aachieves%20performance%20surpassing%20ensembles%20of%20individual%20specialist%20agents%20on%0Atheir%20specialized%20software.%20We%20validate%20the%20effectiveness%20of%20SEAgent%20across%0Afive%20novel%20software%20environments%20within%20OS-World.%20Our%20approach%20achieves%20a%0Asignificant%20improvement%20of%2023.2%25%20in%20success%20rate%2C%20from%2011.3%25%20to%2034.5%25%2C%20over%20a%0Acompetitive%20open-source%20CUA%2C%20i.e.%2C%20UI-TARS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04700v2&entry.124074799=Read"},
{"title": "A new dataset and comparison for multi-camera frame synthesis", "author": "Conall Daly and Anil Kokaram", "abstract": "  Many methods exist for frame synthesis in image sequences but can be broadly\ncategorised into frame interpolation and view synthesis techniques.\nFundamentally, both frame interpolation and view synthesis tackle the same\ntask, interpolating a frame given surrounding frames in time or space. However,\nmost frame interpolation datasets focus on temporal aspects with single cameras\nmoving through time and space, while view synthesis datasets are typically\nbiased toward stereoscopic depth estimation use cases. This makes direct\ncomparison between view synthesis and frame interpolation methods challenging.\nIn this paper, we develop a novel multi-camera dataset using a custom-built\ndense linear camera array to enable fair comparison between these approaches.\nWe evaluate classical and deep learning frame interpolators against a view\nsynthesis method (3D Gaussian Splatting) for the task of view in-betweening.\nOur results reveal that deep learning methods do not significantly outperform\nclassical methods on real image data, with 3D Gaussian Splatting actually\nunderperforming frame interpolators by as much as 3.5 dB PSNR. However, in\nsynthetic scenes, the situation reverses -- 3D Gaussian Splatting outperforms\nframe interpolation algorithms by almost 5 dB PSNR at a 95% confidence level.\n", "link": "http://arxiv.org/abs/2508.09068v1", "date": "2025-08-12", "relevancy": 2.2809, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5884}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5725}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20new%20dataset%20and%20comparison%20for%20multi-camera%20frame%20synthesis&body=Title%3A%20A%20new%20dataset%20and%20comparison%20for%20multi-camera%20frame%20synthesis%0AAuthor%3A%20Conall%20Daly%20and%20Anil%20Kokaram%0AAbstract%3A%20%20%20Many%20methods%20exist%20for%20frame%20synthesis%20in%20image%20sequences%20but%20can%20be%20broadly%0Acategorised%20into%20frame%20interpolation%20and%20view%20synthesis%20techniques.%0AFundamentally%2C%20both%20frame%20interpolation%20and%20view%20synthesis%20tackle%20the%20same%0Atask%2C%20interpolating%20a%20frame%20given%20surrounding%20frames%20in%20time%20or%20space.%20However%2C%0Amost%20frame%20interpolation%20datasets%20focus%20on%20temporal%20aspects%20with%20single%20cameras%0Amoving%20through%20time%20and%20space%2C%20while%20view%20synthesis%20datasets%20are%20typically%0Abiased%20toward%20stereoscopic%20depth%20estimation%20use%20cases.%20This%20makes%20direct%0Acomparison%20between%20view%20synthesis%20and%20frame%20interpolation%20methods%20challenging.%0AIn%20this%20paper%2C%20we%20develop%20a%20novel%20multi-camera%20dataset%20using%20a%20custom-built%0Adense%20linear%20camera%20array%20to%20enable%20fair%20comparison%20between%20these%20approaches.%0AWe%20evaluate%20classical%20and%20deep%20learning%20frame%20interpolators%20against%20a%20view%0Asynthesis%20method%20%283D%20Gaussian%20Splatting%29%20for%20the%20task%20of%20view%20in-betweening.%0AOur%20results%20reveal%20that%20deep%20learning%20methods%20do%20not%20significantly%20outperform%0Aclassical%20methods%20on%20real%20image%20data%2C%20with%203D%20Gaussian%20Splatting%20actually%0Aunderperforming%20frame%20interpolators%20by%20as%20much%20as%203.5%20dB%20PSNR.%20However%2C%20in%0Asynthetic%20scenes%2C%20the%20situation%20reverses%20--%203D%20Gaussian%20Splatting%20outperforms%0Aframe%20interpolation%20algorithms%20by%20almost%205%20dB%20PSNR%20at%20a%2095%25%20confidence%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520new%2520dataset%2520and%2520comparison%2520for%2520multi-camera%2520frame%2520synthesis%26entry.906535625%3DConall%2520Daly%2520and%2520Anil%2520Kokaram%26entry.1292438233%3D%2520%2520Many%2520methods%2520exist%2520for%2520frame%2520synthesis%2520in%2520image%2520sequences%2520but%2520can%2520be%2520broadly%250Acategorised%2520into%2520frame%2520interpolation%2520and%2520view%2520synthesis%2520techniques.%250AFundamentally%252C%2520both%2520frame%2520interpolation%2520and%2520view%2520synthesis%2520tackle%2520the%2520same%250Atask%252C%2520interpolating%2520a%2520frame%2520given%2520surrounding%2520frames%2520in%2520time%2520or%2520space.%2520However%252C%250Amost%2520frame%2520interpolation%2520datasets%2520focus%2520on%2520temporal%2520aspects%2520with%2520single%2520cameras%250Amoving%2520through%2520time%2520and%2520space%252C%2520while%2520view%2520synthesis%2520datasets%2520are%2520typically%250Abiased%2520toward%2520stereoscopic%2520depth%2520estimation%2520use%2520cases.%2520This%2520makes%2520direct%250Acomparison%2520between%2520view%2520synthesis%2520and%2520frame%2520interpolation%2520methods%2520challenging.%250AIn%2520this%2520paper%252C%2520we%2520develop%2520a%2520novel%2520multi-camera%2520dataset%2520using%2520a%2520custom-built%250Adense%2520linear%2520camera%2520array%2520to%2520enable%2520fair%2520comparison%2520between%2520these%2520approaches.%250AWe%2520evaluate%2520classical%2520and%2520deep%2520learning%2520frame%2520interpolators%2520against%2520a%2520view%250Asynthesis%2520method%2520%25283D%2520Gaussian%2520Splatting%2529%2520for%2520the%2520task%2520of%2520view%2520in-betweening.%250AOur%2520results%2520reveal%2520that%2520deep%2520learning%2520methods%2520do%2520not%2520significantly%2520outperform%250Aclassical%2520methods%2520on%2520real%2520image%2520data%252C%2520with%25203D%2520Gaussian%2520Splatting%2520actually%250Aunderperforming%2520frame%2520interpolators%2520by%2520as%2520much%2520as%25203.5%2520dB%2520PSNR.%2520However%252C%2520in%250Asynthetic%2520scenes%252C%2520the%2520situation%2520reverses%2520--%25203D%2520Gaussian%2520Splatting%2520outperforms%250Aframe%2520interpolation%2520algorithms%2520by%2520almost%25205%2520dB%2520PSNR%2520at%2520a%252095%2525%2520confidence%2520level.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20new%20dataset%20and%20comparison%20for%20multi-camera%20frame%20synthesis&entry.906535625=Conall%20Daly%20and%20Anil%20Kokaram&entry.1292438233=%20%20Many%20methods%20exist%20for%20frame%20synthesis%20in%20image%20sequences%20but%20can%20be%20broadly%0Acategorised%20into%20frame%20interpolation%20and%20view%20synthesis%20techniques.%0AFundamentally%2C%20both%20frame%20interpolation%20and%20view%20synthesis%20tackle%20the%20same%0Atask%2C%20interpolating%20a%20frame%20given%20surrounding%20frames%20in%20time%20or%20space.%20However%2C%0Amost%20frame%20interpolation%20datasets%20focus%20on%20temporal%20aspects%20with%20single%20cameras%0Amoving%20through%20time%20and%20space%2C%20while%20view%20synthesis%20datasets%20are%20typically%0Abiased%20toward%20stereoscopic%20depth%20estimation%20use%20cases.%20This%20makes%20direct%0Acomparison%20between%20view%20synthesis%20and%20frame%20interpolation%20methods%20challenging.%0AIn%20this%20paper%2C%20we%20develop%20a%20novel%20multi-camera%20dataset%20using%20a%20custom-built%0Adense%20linear%20camera%20array%20to%20enable%20fair%20comparison%20between%20these%20approaches.%0AWe%20evaluate%20classical%20and%20deep%20learning%20frame%20interpolators%20against%20a%20view%0Asynthesis%20method%20%283D%20Gaussian%20Splatting%29%20for%20the%20task%20of%20view%20in-betweening.%0AOur%20results%20reveal%20that%20deep%20learning%20methods%20do%20not%20significantly%20outperform%0Aclassical%20methods%20on%20real%20image%20data%2C%20with%203D%20Gaussian%20Splatting%20actually%0Aunderperforming%20frame%20interpolators%20by%20as%20much%20as%203.5%20dB%20PSNR.%20However%2C%20in%0Asynthetic%20scenes%2C%20the%20situation%20reverses%20--%203D%20Gaussian%20Splatting%20outperforms%0Aframe%20interpolation%20algorithms%20by%20almost%205%20dB%20PSNR%20at%20a%2095%25%20confidence%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09068v1&entry.124074799=Read"},
{"title": "ALFred: An Active Learning Framework for Real-world Semi-supervised\n  Anomaly Detection with Adaptive Thresholds", "author": "Shanle Yao and Ghazal Alinezhad Noghre and Armin Danesh Pazho and Hamed Tabkhi", "abstract": "  Video Anomaly Detection (VAD) can play a key role in spotting unusual\nactivities in video footage. VAD is difficult to use in real-world settings due\nto the dynamic nature of human actions, environmental variations, and domain\nshifts. Traditional evaluation metrics often prove inadequate for such\nscenarios, as they rely on static assumptions and fall short of identifying a\nthreshold that distinguishes normal from anomalous behavior in dynamic\nsettings. To address this, we introduce an active learning framework tailored\nfor VAD, designed for adapting to the ever-changing real-world conditions. Our\napproach leverages active learning to continuously select the most informative\ndata points for labeling, thereby enhancing model adaptability. A critical\ninnovation is the incorporation of a human-in-the-loop mechanism, which enables\nthe identification of actual normal and anomalous instances from\npseudo-labeling results generated by AI. This collected data allows the\nframework to define an adaptive threshold tailored to different environments,\nensuring that the system remains effective as the definition of 'normal' shifts\nacross various settings. Implemented within a lab-based framework that\nsimulates real-world conditions, our approach allows rigorous testing and\nrefinement of VAD algorithms with a new metric. Experimental results show that\nour method achieves an EBI (Error Balance Index) of 68.91 for Q3 in real-world\nsimulated scenarios, demonstrating its practical effectiveness and\nsignificantly enhancing the applicability of VAD in dynamic environments.\n", "link": "http://arxiv.org/abs/2508.09058v1", "date": "2025-08-12", "relevancy": 2.2436, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5759}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.57}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALFred%3A%20An%20Active%20Learning%20Framework%20for%20Real-world%20Semi-supervised%0A%20%20Anomaly%20Detection%20with%20Adaptive%20Thresholds&body=Title%3A%20ALFred%3A%20An%20Active%20Learning%20Framework%20for%20Real-world%20Semi-supervised%0A%20%20Anomaly%20Detection%20with%20Adaptive%20Thresholds%0AAuthor%3A%20Shanle%20Yao%20and%20Ghazal%20Alinezhad%20Noghre%20and%20Armin%20Danesh%20Pazho%20and%20Hamed%20Tabkhi%0AAbstract%3A%20%20%20Video%20Anomaly%20Detection%20%28VAD%29%20can%20play%20a%20key%20role%20in%20spotting%20unusual%0Aactivities%20in%20video%20footage.%20VAD%20is%20difficult%20to%20use%20in%20real-world%20settings%20due%0Ato%20the%20dynamic%20nature%20of%20human%20actions%2C%20environmental%20variations%2C%20and%20domain%0Ashifts.%20Traditional%20evaluation%20metrics%20often%20prove%20inadequate%20for%20such%0Ascenarios%2C%20as%20they%20rely%20on%20static%20assumptions%20and%20fall%20short%20of%20identifying%20a%0Athreshold%20that%20distinguishes%20normal%20from%20anomalous%20behavior%20in%20dynamic%0Asettings.%20To%20address%20this%2C%20we%20introduce%20an%20active%20learning%20framework%20tailored%0Afor%20VAD%2C%20designed%20for%20adapting%20to%20the%20ever-changing%20real-world%20conditions.%20Our%0Aapproach%20leverages%20active%20learning%20to%20continuously%20select%20the%20most%20informative%0Adata%20points%20for%20labeling%2C%20thereby%20enhancing%20model%20adaptability.%20A%20critical%0Ainnovation%20is%20the%20incorporation%20of%20a%20human-in-the-loop%20mechanism%2C%20which%20enables%0Athe%20identification%20of%20actual%20normal%20and%20anomalous%20instances%20from%0Apseudo-labeling%20results%20generated%20by%20AI.%20This%20collected%20data%20allows%20the%0Aframework%20to%20define%20an%20adaptive%20threshold%20tailored%20to%20different%20environments%2C%0Aensuring%20that%20the%20system%20remains%20effective%20as%20the%20definition%20of%20%27normal%27%20shifts%0Aacross%20various%20settings.%20Implemented%20within%20a%20lab-based%20framework%20that%0Asimulates%20real-world%20conditions%2C%20our%20approach%20allows%20rigorous%20testing%20and%0Arefinement%20of%20VAD%20algorithms%20with%20a%20new%20metric.%20Experimental%20results%20show%20that%0Aour%20method%20achieves%20an%20EBI%20%28Error%20Balance%20Index%29%20of%2068.91%20for%20Q3%20in%20real-world%0Asimulated%20scenarios%2C%20demonstrating%20its%20practical%20effectiveness%20and%0Asignificantly%20enhancing%20the%20applicability%20of%20VAD%20in%20dynamic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALFred%253A%2520An%2520Active%2520Learning%2520Framework%2520for%2520Real-world%2520Semi-supervised%250A%2520%2520Anomaly%2520Detection%2520with%2520Adaptive%2520Thresholds%26entry.906535625%3DShanle%2520Yao%2520and%2520Ghazal%2520Alinezhad%2520Noghre%2520and%2520Armin%2520Danesh%2520Pazho%2520and%2520Hamed%2520Tabkhi%26entry.1292438233%3D%2520%2520Video%2520Anomaly%2520Detection%2520%2528VAD%2529%2520can%2520play%2520a%2520key%2520role%2520in%2520spotting%2520unusual%250Aactivities%2520in%2520video%2520footage.%2520VAD%2520is%2520difficult%2520to%2520use%2520in%2520real-world%2520settings%2520due%250Ato%2520the%2520dynamic%2520nature%2520of%2520human%2520actions%252C%2520environmental%2520variations%252C%2520and%2520domain%250Ashifts.%2520Traditional%2520evaluation%2520metrics%2520often%2520prove%2520inadequate%2520for%2520such%250Ascenarios%252C%2520as%2520they%2520rely%2520on%2520static%2520assumptions%2520and%2520fall%2520short%2520of%2520identifying%2520a%250Athreshold%2520that%2520distinguishes%2520normal%2520from%2520anomalous%2520behavior%2520in%2520dynamic%250Asettings.%2520To%2520address%2520this%252C%2520we%2520introduce%2520an%2520active%2520learning%2520framework%2520tailored%250Afor%2520VAD%252C%2520designed%2520for%2520adapting%2520to%2520the%2520ever-changing%2520real-world%2520conditions.%2520Our%250Aapproach%2520leverages%2520active%2520learning%2520to%2520continuously%2520select%2520the%2520most%2520informative%250Adata%2520points%2520for%2520labeling%252C%2520thereby%2520enhancing%2520model%2520adaptability.%2520A%2520critical%250Ainnovation%2520is%2520the%2520incorporation%2520of%2520a%2520human-in-the-loop%2520mechanism%252C%2520which%2520enables%250Athe%2520identification%2520of%2520actual%2520normal%2520and%2520anomalous%2520instances%2520from%250Apseudo-labeling%2520results%2520generated%2520by%2520AI.%2520This%2520collected%2520data%2520allows%2520the%250Aframework%2520to%2520define%2520an%2520adaptive%2520threshold%2520tailored%2520to%2520different%2520environments%252C%250Aensuring%2520that%2520the%2520system%2520remains%2520effective%2520as%2520the%2520definition%2520of%2520%2527normal%2527%2520shifts%250Aacross%2520various%2520settings.%2520Implemented%2520within%2520a%2520lab-based%2520framework%2520that%250Asimulates%2520real-world%2520conditions%252C%2520our%2520approach%2520allows%2520rigorous%2520testing%2520and%250Arefinement%2520of%2520VAD%2520algorithms%2520with%2520a%2520new%2520metric.%2520Experimental%2520results%2520show%2520that%250Aour%2520method%2520achieves%2520an%2520EBI%2520%2528Error%2520Balance%2520Index%2529%2520of%252068.91%2520for%2520Q3%2520in%2520real-world%250Asimulated%2520scenarios%252C%2520demonstrating%2520its%2520practical%2520effectiveness%2520and%250Asignificantly%2520enhancing%2520the%2520applicability%2520of%2520VAD%2520in%2520dynamic%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALFred%3A%20An%20Active%20Learning%20Framework%20for%20Real-world%20Semi-supervised%0A%20%20Anomaly%20Detection%20with%20Adaptive%20Thresholds&entry.906535625=Shanle%20Yao%20and%20Ghazal%20Alinezhad%20Noghre%20and%20Armin%20Danesh%20Pazho%20and%20Hamed%20Tabkhi&entry.1292438233=%20%20Video%20Anomaly%20Detection%20%28VAD%29%20can%20play%20a%20key%20role%20in%20spotting%20unusual%0Aactivities%20in%20video%20footage.%20VAD%20is%20difficult%20to%20use%20in%20real-world%20settings%20due%0Ato%20the%20dynamic%20nature%20of%20human%20actions%2C%20environmental%20variations%2C%20and%20domain%0Ashifts.%20Traditional%20evaluation%20metrics%20often%20prove%20inadequate%20for%20such%0Ascenarios%2C%20as%20they%20rely%20on%20static%20assumptions%20and%20fall%20short%20of%20identifying%20a%0Athreshold%20that%20distinguishes%20normal%20from%20anomalous%20behavior%20in%20dynamic%0Asettings.%20To%20address%20this%2C%20we%20introduce%20an%20active%20learning%20framework%20tailored%0Afor%20VAD%2C%20designed%20for%20adapting%20to%20the%20ever-changing%20real-world%20conditions.%20Our%0Aapproach%20leverages%20active%20learning%20to%20continuously%20select%20the%20most%20informative%0Adata%20points%20for%20labeling%2C%20thereby%20enhancing%20model%20adaptability.%20A%20critical%0Ainnovation%20is%20the%20incorporation%20of%20a%20human-in-the-loop%20mechanism%2C%20which%20enables%0Athe%20identification%20of%20actual%20normal%20and%20anomalous%20instances%20from%0Apseudo-labeling%20results%20generated%20by%20AI.%20This%20collected%20data%20allows%20the%0Aframework%20to%20define%20an%20adaptive%20threshold%20tailored%20to%20different%20environments%2C%0Aensuring%20that%20the%20system%20remains%20effective%20as%20the%20definition%20of%20%27normal%27%20shifts%0Aacross%20various%20settings.%20Implemented%20within%20a%20lab-based%20framework%20that%0Asimulates%20real-world%20conditions%2C%20our%20approach%20allows%20rigorous%20testing%20and%0Arefinement%20of%20VAD%20algorithms%20with%20a%20new%20metric.%20Experimental%20results%20show%20that%0Aour%20method%20achieves%20an%20EBI%20%28Error%20Balance%20Index%29%20of%2068.91%20for%20Q3%20in%20real-world%0Asimulated%20scenarios%2C%20demonstrating%20its%20practical%20effectiveness%20and%0Asignificantly%20enhancing%20the%20applicability%20of%20VAD%20in%20dynamic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09058v1&entry.124074799=Read"},
{"title": "Uncertainty-aware Cross-training for Semi-supervised Medical Image\n  Segmentation", "author": "Kaiwen Huang and Tao Zhou and Huazhu Fu and Yizhe Zhang and Yi Zhou and Xiao-Jun Wu", "abstract": "  Semi-supervised learning has gained considerable popularity in medical image\nsegmentation tasks due to its capability to reduce reliance on expert-examined\nannotations. Several mean-teacher (MT) based semi-supervised methods utilize\nconsistency regularization to effectively leverage valuable information from\nunlabeled data. However, these methods often heavily rely on the student model\nand overlook the potential impact of cognitive biases within the model.\nFurthermore, some methods employ co-training using pseudo-labels derived from\ndifferent inputs, yet generating high-confidence pseudo-labels from perturbed\ninputs during training remains a significant challenge. In this paper, we\npropose an Uncertainty-aware Cross-training framework for semi-supervised\nmedical image Segmentation (UC-Seg). Our UC-Seg framework incorporates two\ndistinct subnets to effectively explore and leverage the correlation between\nthem, thereby mitigating cognitive biases within the model. Specifically, we\npresent a Cross-subnet Consistency Preservation (CCP) strategy to enhance\nfeature representation capability and ensure feature consistency across the two\nsubnets. This strategy enables each subnet to correct its own biases and learn\nshared semantics from both labeled and unlabeled data. Additionally, we propose\nan Uncertainty-aware Pseudo-label Generation (UPG) component that leverages\nsegmentation results and corresponding uncertainty maps from both subnets to\ngenerate high-confidence pseudo-labels. We extensively evaluate the proposed\nUC-Seg on various medical image segmentation tasks involving different modality\nimages, such as MRI, CT, ultrasound, colonoscopy, and so on. The results\ndemonstrate that our method achieves superior segmentation accuracy and\ngeneralization performance compared to other state-of-the-art semi-supervised\nmethods. Our code will be released at https://github.com/taozh2017/UCSeg.\n", "link": "http://arxiv.org/abs/2508.09014v1", "date": "2025-08-12", "relevancy": 2.2207, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6166}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5673}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-aware%20Cross-training%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20Uncertainty-aware%20Cross-training%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Kaiwen%20Huang%20and%20Tao%20Zhou%20and%20Huazhu%20Fu%20and%20Yizhe%20Zhang%20and%20Yi%20Zhou%20and%20Xiao-Jun%20Wu%0AAbstract%3A%20%20%20Semi-supervised%20learning%20has%20gained%20considerable%20popularity%20in%20medical%20image%0Asegmentation%20tasks%20due%20to%20its%20capability%20to%20reduce%20reliance%20on%20expert-examined%0Aannotations.%20Several%20mean-teacher%20%28MT%29%20based%20semi-supervised%20methods%20utilize%0Aconsistency%20regularization%20to%20effectively%20leverage%20valuable%20information%20from%0Aunlabeled%20data.%20However%2C%20these%20methods%20often%20heavily%20rely%20on%20the%20student%20model%0Aand%20overlook%20the%20potential%20impact%20of%20cognitive%20biases%20within%20the%20model.%0AFurthermore%2C%20some%20methods%20employ%20co-training%20using%20pseudo-labels%20derived%20from%0Adifferent%20inputs%2C%20yet%20generating%20high-confidence%20pseudo-labels%20from%20perturbed%0Ainputs%20during%20training%20remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%0Apropose%20an%20Uncertainty-aware%20Cross-training%20framework%20for%20semi-supervised%0Amedical%20image%20Segmentation%20%28UC-Seg%29.%20Our%20UC-Seg%20framework%20incorporates%20two%0Adistinct%20subnets%20to%20effectively%20explore%20and%20leverage%20the%20correlation%20between%0Athem%2C%20thereby%20mitigating%20cognitive%20biases%20within%20the%20model.%20Specifically%2C%20we%0Apresent%20a%20Cross-subnet%20Consistency%20Preservation%20%28CCP%29%20strategy%20to%20enhance%0Afeature%20representation%20capability%20and%20ensure%20feature%20consistency%20across%20the%20two%0Asubnets.%20This%20strategy%20enables%20each%20subnet%20to%20correct%20its%20own%20biases%20and%20learn%0Ashared%20semantics%20from%20both%20labeled%20and%20unlabeled%20data.%20Additionally%2C%20we%20propose%0Aan%20Uncertainty-aware%20Pseudo-label%20Generation%20%28UPG%29%20component%20that%20leverages%0Asegmentation%20results%20and%20corresponding%20uncertainty%20maps%20from%20both%20subnets%20to%0Agenerate%20high-confidence%20pseudo-labels.%20We%20extensively%20evaluate%20the%20proposed%0AUC-Seg%20on%20various%20medical%20image%20segmentation%20tasks%20involving%20different%20modality%0Aimages%2C%20such%20as%20MRI%2C%20CT%2C%20ultrasound%2C%20colonoscopy%2C%20and%20so%20on.%20The%20results%0Ademonstrate%20that%20our%20method%20achieves%20superior%20segmentation%20accuracy%20and%0Ageneralization%20performance%20compared%20to%20other%20state-of-the-art%20semi-supervised%0Amethods.%20Our%20code%20will%20be%20released%20at%20https%3A//github.com/taozh2017/UCSeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09014v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-aware%2520Cross-training%2520for%2520Semi-supervised%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DKaiwen%2520Huang%2520and%2520Tao%2520Zhou%2520and%2520Huazhu%2520Fu%2520and%2520Yizhe%2520Zhang%2520and%2520Yi%2520Zhou%2520and%2520Xiao-Jun%2520Wu%26entry.1292438233%3D%2520%2520Semi-supervised%2520learning%2520has%2520gained%2520considerable%2520popularity%2520in%2520medical%2520image%250Asegmentation%2520tasks%2520due%2520to%2520its%2520capability%2520to%2520reduce%2520reliance%2520on%2520expert-examined%250Aannotations.%2520Several%2520mean-teacher%2520%2528MT%2529%2520based%2520semi-supervised%2520methods%2520utilize%250Aconsistency%2520regularization%2520to%2520effectively%2520leverage%2520valuable%2520information%2520from%250Aunlabeled%2520data.%2520However%252C%2520these%2520methods%2520often%2520heavily%2520rely%2520on%2520the%2520student%2520model%250Aand%2520overlook%2520the%2520potential%2520impact%2520of%2520cognitive%2520biases%2520within%2520the%2520model.%250AFurthermore%252C%2520some%2520methods%2520employ%2520co-training%2520using%2520pseudo-labels%2520derived%2520from%250Adifferent%2520inputs%252C%2520yet%2520generating%2520high-confidence%2520pseudo-labels%2520from%2520perturbed%250Ainputs%2520during%2520training%2520remains%2520a%2520significant%2520challenge.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520an%2520Uncertainty-aware%2520Cross-training%2520framework%2520for%2520semi-supervised%250Amedical%2520image%2520Segmentation%2520%2528UC-Seg%2529.%2520Our%2520UC-Seg%2520framework%2520incorporates%2520two%250Adistinct%2520subnets%2520to%2520effectively%2520explore%2520and%2520leverage%2520the%2520correlation%2520between%250Athem%252C%2520thereby%2520mitigating%2520cognitive%2520biases%2520within%2520the%2520model.%2520Specifically%252C%2520we%250Apresent%2520a%2520Cross-subnet%2520Consistency%2520Preservation%2520%2528CCP%2529%2520strategy%2520to%2520enhance%250Afeature%2520representation%2520capability%2520and%2520ensure%2520feature%2520consistency%2520across%2520the%2520two%250Asubnets.%2520This%2520strategy%2520enables%2520each%2520subnet%2520to%2520correct%2520its%2520own%2520biases%2520and%2520learn%250Ashared%2520semantics%2520from%2520both%2520labeled%2520and%2520unlabeled%2520data.%2520Additionally%252C%2520we%2520propose%250Aan%2520Uncertainty-aware%2520Pseudo-label%2520Generation%2520%2528UPG%2529%2520component%2520that%2520leverages%250Asegmentation%2520results%2520and%2520corresponding%2520uncertainty%2520maps%2520from%2520both%2520subnets%2520to%250Agenerate%2520high-confidence%2520pseudo-labels.%2520We%2520extensively%2520evaluate%2520the%2520proposed%250AUC-Seg%2520on%2520various%2520medical%2520image%2520segmentation%2520tasks%2520involving%2520different%2520modality%250Aimages%252C%2520such%2520as%2520MRI%252C%2520CT%252C%2520ultrasound%252C%2520colonoscopy%252C%2520and%2520so%2520on.%2520The%2520results%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520superior%2520segmentation%2520accuracy%2520and%250Ageneralization%2520performance%2520compared%2520to%2520other%2520state-of-the-art%2520semi-supervised%250Amethods.%2520Our%2520code%2520will%2520be%2520released%2520at%2520https%253A//github.com/taozh2017/UCSeg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09014v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-aware%20Cross-training%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Kaiwen%20Huang%20and%20Tao%20Zhou%20and%20Huazhu%20Fu%20and%20Yizhe%20Zhang%20and%20Yi%20Zhou%20and%20Xiao-Jun%20Wu&entry.1292438233=%20%20Semi-supervised%20learning%20has%20gained%20considerable%20popularity%20in%20medical%20image%0Asegmentation%20tasks%20due%20to%20its%20capability%20to%20reduce%20reliance%20on%20expert-examined%0Aannotations.%20Several%20mean-teacher%20%28MT%29%20based%20semi-supervised%20methods%20utilize%0Aconsistency%20regularization%20to%20effectively%20leverage%20valuable%20information%20from%0Aunlabeled%20data.%20However%2C%20these%20methods%20often%20heavily%20rely%20on%20the%20student%20model%0Aand%20overlook%20the%20potential%20impact%20of%20cognitive%20biases%20within%20the%20model.%0AFurthermore%2C%20some%20methods%20employ%20co-training%20using%20pseudo-labels%20derived%20from%0Adifferent%20inputs%2C%20yet%20generating%20high-confidence%20pseudo-labels%20from%20perturbed%0Ainputs%20during%20training%20remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%0Apropose%20an%20Uncertainty-aware%20Cross-training%20framework%20for%20semi-supervised%0Amedical%20image%20Segmentation%20%28UC-Seg%29.%20Our%20UC-Seg%20framework%20incorporates%20two%0Adistinct%20subnets%20to%20effectively%20explore%20and%20leverage%20the%20correlation%20between%0Athem%2C%20thereby%20mitigating%20cognitive%20biases%20within%20the%20model.%20Specifically%2C%20we%0Apresent%20a%20Cross-subnet%20Consistency%20Preservation%20%28CCP%29%20strategy%20to%20enhance%0Afeature%20representation%20capability%20and%20ensure%20feature%20consistency%20across%20the%20two%0Asubnets.%20This%20strategy%20enables%20each%20subnet%20to%20correct%20its%20own%20biases%20and%20learn%0Ashared%20semantics%20from%20both%20labeled%20and%20unlabeled%20data.%20Additionally%2C%20we%20propose%0Aan%20Uncertainty-aware%20Pseudo-label%20Generation%20%28UPG%29%20component%20that%20leverages%0Asegmentation%20results%20and%20corresponding%20uncertainty%20maps%20from%20both%20subnets%20to%0Agenerate%20high-confidence%20pseudo-labels.%20We%20extensively%20evaluate%20the%20proposed%0AUC-Seg%20on%20various%20medical%20image%20segmentation%20tasks%20involving%20different%20modality%0Aimages%2C%20such%20as%20MRI%2C%20CT%2C%20ultrasound%2C%20colonoscopy%2C%20and%20so%20on.%20The%20results%0Ademonstrate%20that%20our%20method%20achieves%20superior%20segmentation%20accuracy%20and%0Ageneralization%20performance%20compared%20to%20other%20state-of-the-art%20semi-supervised%0Amethods.%20Our%20code%20will%20be%20released%20at%20https%3A//github.com/taozh2017/UCSeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09014v1&entry.124074799=Read"},
{"title": "Turbo-VAED: Fast and Stable Transfer of Video-VAEs to Mobile Devices", "author": "Ya Zou and Jingfeng Yao and Siyuan Yu and Shuai Zhang and Wenyu Liu and Xinggang Wang", "abstract": "  There is a growing demand for deploying large generative AI models on mobile\ndevices. For recent popular video generative models, however, the Variational\nAutoEncoder (VAE) represents one of the major computational bottlenecks. Both\nlarge parameter sizes and mismatched kernels cause out-of-memory errors or\nextremely slow inference on mobile devices. To address this, we propose a\nlow-cost solution that efficiently transfers widely used video VAEs to mobile\ndevices. (1) We analyze redundancy in existing VAE architectures and get\nempirical design insights. By integrating 3D depthwise separable convolutions\ninto our model, we significantly reduce the number of parameters. (2) We\nobserve that the upsampling techniques in mainstream video VAEs are poorly\nsuited to mobile hardware and form the main bottleneck. In response, we propose\na decoupled 3D pixel shuffle scheme that slashes end-to-end delay. Building\nupon these, we develop a universal mobile-oriented VAE decoder, Turbo-VAED. (3)\nWe propose an efficient VAE decoder training method. Since only the decoder is\nused during deployment, we distill it to Turbo-VAED instead of retraining the\nfull VAE, enabling fast mobile adaptation with minimal performance loss. To our\nknowledge, our method enables real-time 720p video VAE decoding on mobile\ndevices for the first time. This approach is widely applicable to most video\nVAEs. When integrated into four representative models, with training cost as\nlow as $95, it accelerates original VAEs by up to 84.5x at 720p resolution on\nGPUs, uses as low as 17.5% of original parameter count, and retains 96.9% of\nthe original reconstruction quality. Compared to mobile-optimized VAEs,\nTurbo-VAED achieves a 2.9x speedup in FPS and better reconstruction quality on\nthe iPhone 16 Pro. The code and models will soon be available at\nhttps://github.com/hustvl/Turbo-VAED.\n", "link": "http://arxiv.org/abs/2508.09136v1", "date": "2025-08-12", "relevancy": 2.2195, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5651}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5506}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Turbo-VAED%3A%20Fast%20and%20Stable%20Transfer%20of%20Video-VAEs%20to%20Mobile%20Devices&body=Title%3A%20Turbo-VAED%3A%20Fast%20and%20Stable%20Transfer%20of%20Video-VAEs%20to%20Mobile%20Devices%0AAuthor%3A%20Ya%20Zou%20and%20Jingfeng%20Yao%20and%20Siyuan%20Yu%20and%20Shuai%20Zhang%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20There%20is%20a%20growing%20demand%20for%20deploying%20large%20generative%20AI%20models%20on%20mobile%0Adevices.%20For%20recent%20popular%20video%20generative%20models%2C%20however%2C%20the%20Variational%0AAutoEncoder%20%28VAE%29%20represents%20one%20of%20the%20major%20computational%20bottlenecks.%20Both%0Alarge%20parameter%20sizes%20and%20mismatched%20kernels%20cause%20out-of-memory%20errors%20or%0Aextremely%20slow%20inference%20on%20mobile%20devices.%20To%20address%20this%2C%20we%20propose%20a%0Alow-cost%20solution%20that%20efficiently%20transfers%20widely%20used%20video%20VAEs%20to%20mobile%0Adevices.%20%281%29%20We%20analyze%20redundancy%20in%20existing%20VAE%20architectures%20and%20get%0Aempirical%20design%20insights.%20By%20integrating%203D%20depthwise%20separable%20convolutions%0Ainto%20our%20model%2C%20we%20significantly%20reduce%20the%20number%20of%20parameters.%20%282%29%20We%0Aobserve%20that%20the%20upsampling%20techniques%20in%20mainstream%20video%20VAEs%20are%20poorly%0Asuited%20to%20mobile%20hardware%20and%20form%20the%20main%20bottleneck.%20In%20response%2C%20we%20propose%0Aa%20decoupled%203D%20pixel%20shuffle%20scheme%20that%20slashes%20end-to-end%20delay.%20Building%0Aupon%20these%2C%20we%20develop%20a%20universal%20mobile-oriented%20VAE%20decoder%2C%20Turbo-VAED.%20%283%29%0AWe%20propose%20an%20efficient%20VAE%20decoder%20training%20method.%20Since%20only%20the%20decoder%20is%0Aused%20during%20deployment%2C%20we%20distill%20it%20to%20Turbo-VAED%20instead%20of%20retraining%20the%0Afull%20VAE%2C%20enabling%20fast%20mobile%20adaptation%20with%20minimal%20performance%20loss.%20To%20our%0Aknowledge%2C%20our%20method%20enables%20real-time%20720p%20video%20VAE%20decoding%20on%20mobile%0Adevices%20for%20the%20first%20time.%20This%20approach%20is%20widely%20applicable%20to%20most%20video%0AVAEs.%20When%20integrated%20into%20four%20representative%20models%2C%20with%20training%20cost%20as%0Alow%20as%20%2495%2C%20it%20accelerates%20original%20VAEs%20by%20up%20to%2084.5x%20at%20720p%20resolution%20on%0AGPUs%2C%20uses%20as%20low%20as%2017.5%25%20of%20original%20parameter%20count%2C%20and%20retains%2096.9%25%20of%0Athe%20original%20reconstruction%20quality.%20Compared%20to%20mobile-optimized%20VAEs%2C%0ATurbo-VAED%20achieves%20a%202.9x%20speedup%20in%20FPS%20and%20better%20reconstruction%20quality%20on%0Athe%20iPhone%2016%20Pro.%20The%20code%20and%20models%20will%20soon%20be%20available%20at%0Ahttps%3A//github.com/hustvl/Turbo-VAED.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTurbo-VAED%253A%2520Fast%2520and%2520Stable%2520Transfer%2520of%2520Video-VAEs%2520to%2520Mobile%2520Devices%26entry.906535625%3DYa%2520Zou%2520and%2520Jingfeng%2520Yao%2520and%2520Siyuan%2520Yu%2520and%2520Shuai%2520Zhang%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520There%2520is%2520a%2520growing%2520demand%2520for%2520deploying%2520large%2520generative%2520AI%2520models%2520on%2520mobile%250Adevices.%2520For%2520recent%2520popular%2520video%2520generative%2520models%252C%2520however%252C%2520the%2520Variational%250AAutoEncoder%2520%2528VAE%2529%2520represents%2520one%2520of%2520the%2520major%2520computational%2520bottlenecks.%2520Both%250Alarge%2520parameter%2520sizes%2520and%2520mismatched%2520kernels%2520cause%2520out-of-memory%2520errors%2520or%250Aextremely%2520slow%2520inference%2520on%2520mobile%2520devices.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Alow-cost%2520solution%2520that%2520efficiently%2520transfers%2520widely%2520used%2520video%2520VAEs%2520to%2520mobile%250Adevices.%2520%25281%2529%2520We%2520analyze%2520redundancy%2520in%2520existing%2520VAE%2520architectures%2520and%2520get%250Aempirical%2520design%2520insights.%2520By%2520integrating%25203D%2520depthwise%2520separable%2520convolutions%250Ainto%2520our%2520model%252C%2520we%2520significantly%2520reduce%2520the%2520number%2520of%2520parameters.%2520%25282%2529%2520We%250Aobserve%2520that%2520the%2520upsampling%2520techniques%2520in%2520mainstream%2520video%2520VAEs%2520are%2520poorly%250Asuited%2520to%2520mobile%2520hardware%2520and%2520form%2520the%2520main%2520bottleneck.%2520In%2520response%252C%2520we%2520propose%250Aa%2520decoupled%25203D%2520pixel%2520shuffle%2520scheme%2520that%2520slashes%2520end-to-end%2520delay.%2520Building%250Aupon%2520these%252C%2520we%2520develop%2520a%2520universal%2520mobile-oriented%2520VAE%2520decoder%252C%2520Turbo-VAED.%2520%25283%2529%250AWe%2520propose%2520an%2520efficient%2520VAE%2520decoder%2520training%2520method.%2520Since%2520only%2520the%2520decoder%2520is%250Aused%2520during%2520deployment%252C%2520we%2520distill%2520it%2520to%2520Turbo-VAED%2520instead%2520of%2520retraining%2520the%250Afull%2520VAE%252C%2520enabling%2520fast%2520mobile%2520adaptation%2520with%2520minimal%2520performance%2520loss.%2520To%2520our%250Aknowledge%252C%2520our%2520method%2520enables%2520real-time%2520720p%2520video%2520VAE%2520decoding%2520on%2520mobile%250Adevices%2520for%2520the%2520first%2520time.%2520This%2520approach%2520is%2520widely%2520applicable%2520to%2520most%2520video%250AVAEs.%2520When%2520integrated%2520into%2520four%2520representative%2520models%252C%2520with%2520training%2520cost%2520as%250Alow%2520as%2520%252495%252C%2520it%2520accelerates%2520original%2520VAEs%2520by%2520up%2520to%252084.5x%2520at%2520720p%2520resolution%2520on%250AGPUs%252C%2520uses%2520as%2520low%2520as%252017.5%2525%2520of%2520original%2520parameter%2520count%252C%2520and%2520retains%252096.9%2525%2520of%250Athe%2520original%2520reconstruction%2520quality.%2520Compared%2520to%2520mobile-optimized%2520VAEs%252C%250ATurbo-VAED%2520achieves%2520a%25202.9x%2520speedup%2520in%2520FPS%2520and%2520better%2520reconstruction%2520quality%2520on%250Athe%2520iPhone%252016%2520Pro.%2520The%2520code%2520and%2520models%2520will%2520soon%2520be%2520available%2520at%250Ahttps%253A//github.com/hustvl/Turbo-VAED.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Turbo-VAED%3A%20Fast%20and%20Stable%20Transfer%20of%20Video-VAEs%20to%20Mobile%20Devices&entry.906535625=Ya%20Zou%20and%20Jingfeng%20Yao%20and%20Siyuan%20Yu%20and%20Shuai%20Zhang%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=%20%20There%20is%20a%20growing%20demand%20for%20deploying%20large%20generative%20AI%20models%20on%20mobile%0Adevices.%20For%20recent%20popular%20video%20generative%20models%2C%20however%2C%20the%20Variational%0AAutoEncoder%20%28VAE%29%20represents%20one%20of%20the%20major%20computational%20bottlenecks.%20Both%0Alarge%20parameter%20sizes%20and%20mismatched%20kernels%20cause%20out-of-memory%20errors%20or%0Aextremely%20slow%20inference%20on%20mobile%20devices.%20To%20address%20this%2C%20we%20propose%20a%0Alow-cost%20solution%20that%20efficiently%20transfers%20widely%20used%20video%20VAEs%20to%20mobile%0Adevices.%20%281%29%20We%20analyze%20redundancy%20in%20existing%20VAE%20architectures%20and%20get%0Aempirical%20design%20insights.%20By%20integrating%203D%20depthwise%20separable%20convolutions%0Ainto%20our%20model%2C%20we%20significantly%20reduce%20the%20number%20of%20parameters.%20%282%29%20We%0Aobserve%20that%20the%20upsampling%20techniques%20in%20mainstream%20video%20VAEs%20are%20poorly%0Asuited%20to%20mobile%20hardware%20and%20form%20the%20main%20bottleneck.%20In%20response%2C%20we%20propose%0Aa%20decoupled%203D%20pixel%20shuffle%20scheme%20that%20slashes%20end-to-end%20delay.%20Building%0Aupon%20these%2C%20we%20develop%20a%20universal%20mobile-oriented%20VAE%20decoder%2C%20Turbo-VAED.%20%283%29%0AWe%20propose%20an%20efficient%20VAE%20decoder%20training%20method.%20Since%20only%20the%20decoder%20is%0Aused%20during%20deployment%2C%20we%20distill%20it%20to%20Turbo-VAED%20instead%20of%20retraining%20the%0Afull%20VAE%2C%20enabling%20fast%20mobile%20adaptation%20with%20minimal%20performance%20loss.%20To%20our%0Aknowledge%2C%20our%20method%20enables%20real-time%20720p%20video%20VAE%20decoding%20on%20mobile%0Adevices%20for%20the%20first%20time.%20This%20approach%20is%20widely%20applicable%20to%20most%20video%0AVAEs.%20When%20integrated%20into%20four%20representative%20models%2C%20with%20training%20cost%20as%0Alow%20as%20%2495%2C%20it%20accelerates%20original%20VAEs%20by%20up%20to%2084.5x%20at%20720p%20resolution%20on%0AGPUs%2C%20uses%20as%20low%20as%2017.5%25%20of%20original%20parameter%20count%2C%20and%20retains%2096.9%25%20of%0Athe%20original%20reconstruction%20quality.%20Compared%20to%20mobile-optimized%20VAEs%2C%0ATurbo-VAED%20achieves%20a%202.9x%20speedup%20in%20FPS%20and%20better%20reconstruction%20quality%20on%0Athe%20iPhone%2016%20Pro.%20The%20code%20and%20models%20will%20soon%20be%20available%20at%0Ahttps%3A//github.com/hustvl/Turbo-VAED.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09136v1&entry.124074799=Read"},
{"title": "Per-Query Visual Concept Learning", "author": "Ori Malca and Dvir Samuel and Gal Chechik", "abstract": "  Visual concept learning, also known as Text-to-image personalization, is the\nprocess of teaching new concepts to a pretrained model. This has numerous\napplications from product placement to entertainment and personalized design.\nHere we show that many existing methods can be substantially augmented by\nadding a personalization step that is (1) specific to the prompt and noise\nseed, and (2) using two loss terms based on the self- and cross- attention,\ncapturing the identity of the personalized concept. Specifically, we leverage\nPDM features -- previously designed to capture identity -- and show how they\ncan be used to improve personalized semantic similarity. We evaluate the\nbenefit that our method gains on top of six different personalization methods,\nand several base text-to-image models (both UNet- and DiT-based). We find\nsignificant improvements even over previous per-query personalization methods.\n", "link": "http://arxiv.org/abs/2508.09045v1", "date": "2025-08-12", "relevancy": 2.2018, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5636}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5496}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Per-Query%20Visual%20Concept%20Learning&body=Title%3A%20Per-Query%20Visual%20Concept%20Learning%0AAuthor%3A%20Ori%20Malca%20and%20Dvir%20Samuel%20and%20Gal%20Chechik%0AAbstract%3A%20%20%20Visual%20concept%20learning%2C%20also%20known%20as%20Text-to-image%20personalization%2C%20is%20the%0Aprocess%20of%20teaching%20new%20concepts%20to%20a%20pretrained%20model.%20This%20has%20numerous%0Aapplications%20from%20product%20placement%20to%20entertainment%20and%20personalized%20design.%0AHere%20we%20show%20that%20many%20existing%20methods%20can%20be%20substantially%20augmented%20by%0Aadding%20a%20personalization%20step%20that%20is%20%281%29%20specific%20to%20the%20prompt%20and%20noise%0Aseed%2C%20and%20%282%29%20using%20two%20loss%20terms%20based%20on%20the%20self-%20and%20cross-%20attention%2C%0Acapturing%20the%20identity%20of%20the%20personalized%20concept.%20Specifically%2C%20we%20leverage%0APDM%20features%20--%20previously%20designed%20to%20capture%20identity%20--%20and%20show%20how%20they%0Acan%20be%20used%20to%20improve%20personalized%20semantic%20similarity.%20We%20evaluate%20the%0Abenefit%20that%20our%20method%20gains%20on%20top%20of%20six%20different%20personalization%20methods%2C%0Aand%20several%20base%20text-to-image%20models%20%28both%20UNet-%20and%20DiT-based%29.%20We%20find%0Asignificant%20improvements%20even%20over%20previous%20per-query%20personalization%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPer-Query%2520Visual%2520Concept%2520Learning%26entry.906535625%3DOri%2520Malca%2520and%2520Dvir%2520Samuel%2520and%2520Gal%2520Chechik%26entry.1292438233%3D%2520%2520Visual%2520concept%2520learning%252C%2520also%2520known%2520as%2520Text-to-image%2520personalization%252C%2520is%2520the%250Aprocess%2520of%2520teaching%2520new%2520concepts%2520to%2520a%2520pretrained%2520model.%2520This%2520has%2520numerous%250Aapplications%2520from%2520product%2520placement%2520to%2520entertainment%2520and%2520personalized%2520design.%250AHere%2520we%2520show%2520that%2520many%2520existing%2520methods%2520can%2520be%2520substantially%2520augmented%2520by%250Aadding%2520a%2520personalization%2520step%2520that%2520is%2520%25281%2529%2520specific%2520to%2520the%2520prompt%2520and%2520noise%250Aseed%252C%2520and%2520%25282%2529%2520using%2520two%2520loss%2520terms%2520based%2520on%2520the%2520self-%2520and%2520cross-%2520attention%252C%250Acapturing%2520the%2520identity%2520of%2520the%2520personalized%2520concept.%2520Specifically%252C%2520we%2520leverage%250APDM%2520features%2520--%2520previously%2520designed%2520to%2520capture%2520identity%2520--%2520and%2520show%2520how%2520they%250Acan%2520be%2520used%2520to%2520improve%2520personalized%2520semantic%2520similarity.%2520We%2520evaluate%2520the%250Abenefit%2520that%2520our%2520method%2520gains%2520on%2520top%2520of%2520six%2520different%2520personalization%2520methods%252C%250Aand%2520several%2520base%2520text-to-image%2520models%2520%2528both%2520UNet-%2520and%2520DiT-based%2529.%2520We%2520find%250Asignificant%2520improvements%2520even%2520over%2520previous%2520per-query%2520personalization%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Per-Query%20Visual%20Concept%20Learning&entry.906535625=Ori%20Malca%20and%20Dvir%20Samuel%20and%20Gal%20Chechik&entry.1292438233=%20%20Visual%20concept%20learning%2C%20also%20known%20as%20Text-to-image%20personalization%2C%20is%20the%0Aprocess%20of%20teaching%20new%20concepts%20to%20a%20pretrained%20model.%20This%20has%20numerous%0Aapplications%20from%20product%20placement%20to%20entertainment%20and%20personalized%20design.%0AHere%20we%20show%20that%20many%20existing%20methods%20can%20be%20substantially%20augmented%20by%0Aadding%20a%20personalization%20step%20that%20is%20%281%29%20specific%20to%20the%20prompt%20and%20noise%0Aseed%2C%20and%20%282%29%20using%20two%20loss%20terms%20based%20on%20the%20self-%20and%20cross-%20attention%2C%0Acapturing%20the%20identity%20of%20the%20personalized%20concept.%20Specifically%2C%20we%20leverage%0APDM%20features%20--%20previously%20designed%20to%20capture%20identity%20--%20and%20show%20how%20they%0Acan%20be%20used%20to%20improve%20personalized%20semantic%20similarity.%20We%20evaluate%20the%0Abenefit%20that%20our%20method%20gains%20on%20top%20of%20six%20different%20personalization%20methods%2C%0Aand%20several%20base%20text-to-image%20models%20%28both%20UNet-%20and%20DiT-based%29.%20We%20find%0Asignificant%20improvements%20even%20over%20previous%20per-query%20personalization%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09045v1&entry.124074799=Read"},
{"title": "Euclid Quick Data Release (Q1). Active galactic nuclei identification\n  using diffusion-based inpainting of Euclid VIS images", "author": " Euclid Collaboration and G. Stevens and S. Fotopoulou and M. N. Bremer and T. Matamoro Zatarain and K. Jahnke and B. Margalef-Bentabol and M. Huertas-Company and M. J. Smith and M. Walmsley and M. Salvato and M. Mezcua and A. Paulino-Afonso and M. Siudek and M. Talia and F. Ricci and W. Roster and N. Aghanim and B. Altieri and S. Andreon and H. Aussel and C. Baccigalupi and M. Baldi and S. Bardelli and P. Battaglia and A. Biviano and A. Bonchi and E. Branchini and M. Brescia and J. Brinchmann and S. Camera and G. Ca\u00f1as-Herrera and V. Capobianco and C. Carbone and J. Carretero and M. Castellano and G. Castignani and S. Cavuoti and K. C. Chambers and A. Cimatti and C. Colodro-Conde and G. Congedo and C. J. Conselice and L. Conversi and Y. Copin and A. Costille and F. Courbin and H. M. Courtois and M. Cropper and A. Da Silva and H. Degaudenzi and G. De Lucia and C. Dolding and H. Dole and M. Douspis and F. Dubath and X. Dupac and S. Dusini and S. Escoffier and M. Farina and S. Ferriol and K. George and C. Giocoli and B. R. Granett and A. Grazian and F. Grupp and S. V. H. Haugan and I. M. Hook and F. Hormuth and A. Hornstrup and P. Hudelot and M. Jhabvala and E. Keih\u00e4nen and S. Kermiche and A. Kiessling and M. Kilbinger and B. Kubik and M. K\u00fcmmel and H. Kurki-Suonio and Q. Le Boulc'h and A. M. C. Le Brun and D. Le Mignant and P. B. Lilje and V. Lindholm and I. Lloro and G. Mainetti and D. Maino and E. Maiorano and O. Marggraf and M. Martinelli and N. Martinet and F. Marulli and R. Massey and S. Maurogordato and H. J. McCracken and E. Medinaceli and S. Mei and M. Melchior and M. Meneghetti and E. Merlin and G. Meylan and A. Mora and M. Moresco and L. Moscardini and R. Nakajima and C. Neissner and S. -M. Niemi and C. Padilla and S. Paltani and F. Pasian and K. Pedersen and W. J. Percival and V. Pettorino and G. Polenta and M. Poncet and L. A. Popa and L. Pozzetti and F. Raison and R. Rebolo and A. Renzi and J. Rhodes and G. Riccio and E. Romelli and M. Roncarelli and R. Saglia and A. G. S\u00e1nchez and D. Sapone and J. A. Schewtschenko and M. Schirmer and P. Schneider and T. Schrabback and A. Secroun and S. Serrano and P. Simon and C. Sirignano and G. Sirri and J. Skottfelt and L. Stanco and J. Steinwagner and P. Tallada-Cresp\u00ed and A. N. Taylor and I. Tereno and S. Toft and R. Toledo-Moreo and F. Torradeflot and I. Tutusaus and L. Valenziano and J. Valiviita and T. Vassallo and G. Verdoes Kleijn and A. Veropalumbo and Y. Wang and J. Weller and A. Zacchei and G. Zamorani and F. M. Zerbi and I. A. Zinchenko and E. Zucca and V. Allevato and M. Ballardini and M. Bolzonella and E. Bozzo and C. Burigana and R. Cabanac and A. Cappi and J. A. Escartin Vigo and L. Gabarra and W. G. Hartley and J. Mart\u00edn-Fleitas and S. Matthew and R. B. Metcalf and A. Pezzotta and M. P\u00f6ntinen and I. Risso and V. Scottez and M. Sereno and M. Tenti and M. Wiesmann and Y. Akrami and S. Alvi and I. T. Andika and S. Anselmi and M. Archidiacono and F. Atrio-Barandela and D. Bertacca and M. Bethermin and L. Bisigello and A. Blanchard and L. Blot and S. Borgani and M. L. Brown and S. Bruton and A. Calabro and F. Caro and T. Castro and F. Cogato and S. Davini and G. Desprez and A. D\u00edaz-S\u00e1nchez and J. J. Diaz and S. Di Domizio and J. M. Diego and P. -A. Duc and A. Enia and Y. Fang and A. G. Ferrari and A. Finoguenov and A. Fontana and A. Franco and J. Garc\u00eda-Bellido and T. Gasparetto and V. Gautard and E. Gaztanaga and F. Giacomini and F. Gianotti and M. Guidi and C. M. Gutierrez and A. Hall and S. Hemmati and H. Hildebrandt and J. Hjorth and J. J. E. Kajava and Y. Kang and V. Kansal and D. Karagiannis and C. C. Kirkpatrick and S. Kruk and L. Legrand and M. Lembo and F. Lepori and G. Leroy and J. Lesgourgues and L. Leuzzi and T. I. Liaudat and J. Macias-Perez and M. Magliocchetti and F. Mannucci and R. Maoli and C. J. A. P. Martins and L. Maurin and M. Miluzio and P. Monaco and G. Morgante and K. Naidoo and A. Navarro-Alsina and F. Passalacqua and K. Paterson and L. Patrizii and A. Pisani and D. Potter and S. Quai and M. Radovich and P. -F. Rocci and G. Rodighiero and S. Sacquegna and M. Sahl\u00e9n and D. B. Sanders and E. Sarpa and A. Schneider and M. Schultheis and D. Sciotti and E. Sellentin and F. Shankar and L. C. Smith and K. Tanidis and G. Testera and R. Teyssier and S. Tosi and A. Troja and M. Tucci and C. Valieri and D. Vergani and G. Verza and N. A. Walton", "abstract": "  Light emission from galaxies exhibit diverse brightness profiles, influenced\nby factors such as galaxy type, structural features and interactions with other\ngalaxies. Elliptical galaxies feature more uniform light distributions, while\nspiral and irregular galaxies have complex, varied light profiles due to their\nstructural heterogeneity and star-forming activity. In addition, galaxies with\nan active galactic nucleus (AGN) feature intense, concentrated emission from\ngas accretion around supermassive black holes, superimposed on regular galactic\nlight, while quasi-stellar objects (QSO) are the extreme case of the AGN\nemission dominating the galaxy. The challenge of identifying AGN and QSO has\nbeen discussed many times in the literature, often requiring multi-wavelength\nobservations. This paper introduces a novel approach to identify AGN and QSO\nfrom a single image. Diffusion models have been recently developed in the\nmachine-learning literature to generate realistic-looking images of everyday\nobjects. Utilising the spatial resolving power of the Euclid VIS images, we\ncreated a diffusion model trained on one million sources, without using any\nsource pre-selection or labels. The model learns to reconstruct light\ndistributions of normal galaxies, since the population is dominated by them. We\ncondition the prediction of the central light distribution by masking the\ncentral few pixels of each source and reconstruct the light according to the\ndiffusion model. We further use this prediction to identify sources that\ndeviate from this profile by examining the reconstruction error of the few\ncentral pixels regenerated in each source's core. Our approach, solely using\nVIS imaging, features high completeness compared to traditional methods of AGN\nand QSO selection, including optical, near-infrared, mid-infrared, and X-rays.\n", "link": "http://arxiv.org/abs/2503.15321v2", "date": "2025-08-12", "relevancy": 2.1591, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.569}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5339}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Euclid%20Quick%20Data%20Release%20%28Q1%29.%20Active%20galactic%20nuclei%20identification%0A%20%20using%20diffusion-based%20inpainting%20of%20Euclid%20VIS%20images&body=Title%3A%20Euclid%20Quick%20Data%20Release%20%28Q1%29.%20Active%20galactic%20nuclei%20identification%0A%20%20using%20diffusion-based%20inpainting%20of%20Euclid%20VIS%20images%0AAuthor%3A%20%20Euclid%20Collaboration%20and%20G.%20Stevens%20and%20S.%20Fotopoulou%20and%20M.%20N.%20Bremer%20and%20T.%20Matamoro%20Zatarain%20and%20K.%20Jahnke%20and%20B.%20Margalef-Bentabol%20and%20M.%20Huertas-Company%20and%20M.%20J.%20Smith%20and%20M.%20Walmsley%20and%20M.%20Salvato%20and%20M.%20Mezcua%20and%20A.%20Paulino-Afonso%20and%20M.%20Siudek%20and%20M.%20Talia%20and%20F.%20Ricci%20and%20W.%20Roster%20and%20N.%20Aghanim%20and%20B.%20Altieri%20and%20S.%20Andreon%20and%20H.%20Aussel%20and%20C.%20Baccigalupi%20and%20M.%20Baldi%20and%20S.%20Bardelli%20and%20P.%20Battaglia%20and%20A.%20Biviano%20and%20A.%20Bonchi%20and%20E.%20Branchini%20and%20M.%20Brescia%20and%20J.%20Brinchmann%20and%20S.%20Camera%20and%20G.%20Ca%C3%B1as-Herrera%20and%20V.%20Capobianco%20and%20C.%20Carbone%20and%20J.%20Carretero%20and%20M.%20Castellano%20and%20G.%20Castignani%20and%20S.%20Cavuoti%20and%20K.%20C.%20Chambers%20and%20A.%20Cimatti%20and%20C.%20Colodro-Conde%20and%20G.%20Congedo%20and%20C.%20J.%20Conselice%20and%20L.%20Conversi%20and%20Y.%20Copin%20and%20A.%20Costille%20and%20F.%20Courbin%20and%20H.%20M.%20Courtois%20and%20M.%20Cropper%20and%20A.%20Da%20Silva%20and%20H.%20Degaudenzi%20and%20G.%20De%20Lucia%20and%20C.%20Dolding%20and%20H.%20Dole%20and%20M.%20Douspis%20and%20F.%20Dubath%20and%20X.%20Dupac%20and%20S.%20Dusini%20and%20S.%20Escoffier%20and%20M.%20Farina%20and%20S.%20Ferriol%20and%20K.%20George%20and%20C.%20Giocoli%20and%20B.%20R.%20Granett%20and%20A.%20Grazian%20and%20F.%20Grupp%20and%20S.%20V.%20H.%20Haugan%20and%20I.%20M.%20Hook%20and%20F.%20Hormuth%20and%20A.%20Hornstrup%20and%20P.%20Hudelot%20and%20M.%20Jhabvala%20and%20E.%20Keih%C3%A4nen%20and%20S.%20Kermiche%20and%20A.%20Kiessling%20and%20M.%20Kilbinger%20and%20B.%20Kubik%20and%20M.%20K%C3%BCmmel%20and%20H.%20Kurki-Suonio%20and%20Q.%20Le%20Boulc%27h%20and%20A.%20M.%20C.%20Le%20Brun%20and%20D.%20Le%20Mignant%20and%20P.%20B.%20Lilje%20and%20V.%20Lindholm%20and%20I.%20Lloro%20and%20G.%20Mainetti%20and%20D.%20Maino%20and%20E.%20Maiorano%20and%20O.%20Marggraf%20and%20M.%20Martinelli%20and%20N.%20Martinet%20and%20F.%20Marulli%20and%20R.%20Massey%20and%20S.%20Maurogordato%20and%20H.%20J.%20McCracken%20and%20E.%20Medinaceli%20and%20S.%20Mei%20and%20M.%20Melchior%20and%20M.%20Meneghetti%20and%20E.%20Merlin%20and%20G.%20Meylan%20and%20A.%20Mora%20and%20M.%20Moresco%20and%20L.%20Moscardini%20and%20R.%20Nakajima%20and%20C.%20Neissner%20and%20S.%20-M.%20Niemi%20and%20C.%20Padilla%20and%20S.%20Paltani%20and%20F.%20Pasian%20and%20K.%20Pedersen%20and%20W.%20J.%20Percival%20and%20V.%20Pettorino%20and%20G.%20Polenta%20and%20M.%20Poncet%20and%20L.%20A.%20Popa%20and%20L.%20Pozzetti%20and%20F.%20Raison%20and%20R.%20Rebolo%20and%20A.%20Renzi%20and%20J.%20Rhodes%20and%20G.%20Riccio%20and%20E.%20Romelli%20and%20M.%20Roncarelli%20and%20R.%20Saglia%20and%20A.%20G.%20S%C3%A1nchez%20and%20D.%20Sapone%20and%20J.%20A.%20Schewtschenko%20and%20M.%20Schirmer%20and%20P.%20Schneider%20and%20T.%20Schrabback%20and%20A.%20Secroun%20and%20S.%20Serrano%20and%20P.%20Simon%20and%20C.%20Sirignano%20and%20G.%20Sirri%20and%20J.%20Skottfelt%20and%20L.%20Stanco%20and%20J.%20Steinwagner%20and%20P.%20Tallada-Cresp%C3%AD%20and%20A.%20N.%20Taylor%20and%20I.%20Tereno%20and%20S.%20Toft%20and%20R.%20Toledo-Moreo%20and%20F.%20Torradeflot%20and%20I.%20Tutusaus%20and%20L.%20Valenziano%20and%20J.%20Valiviita%20and%20T.%20Vassallo%20and%20G.%20Verdoes%20Kleijn%20and%20A.%20Veropalumbo%20and%20Y.%20Wang%20and%20J.%20Weller%20and%20A.%20Zacchei%20and%20G.%20Zamorani%20and%20F.%20M.%20Zerbi%20and%20I.%20A.%20Zinchenko%20and%20E.%20Zucca%20and%20V.%20Allevato%20and%20M.%20Ballardini%20and%20M.%20Bolzonella%20and%20E.%20Bozzo%20and%20C.%20Burigana%20and%20R.%20Cabanac%20and%20A.%20Cappi%20and%20J.%20A.%20Escartin%20Vigo%20and%20L.%20Gabarra%20and%20W.%20G.%20Hartley%20and%20J.%20Mart%C3%ADn-Fleitas%20and%20S.%20Matthew%20and%20R.%20B.%20Metcalf%20and%20A.%20Pezzotta%20and%20M.%20P%C3%B6ntinen%20and%20I.%20Risso%20and%20V.%20Scottez%20and%20M.%20Sereno%20and%20M.%20Tenti%20and%20M.%20Wiesmann%20and%20Y.%20Akrami%20and%20S.%20Alvi%20and%20I.%20T.%20Andika%20and%20S.%20Anselmi%20and%20M.%20Archidiacono%20and%20F.%20Atrio-Barandela%20and%20D.%20Bertacca%20and%20M.%20Bethermin%20and%20L.%20Bisigello%20and%20A.%20Blanchard%20and%20L.%20Blot%20and%20S.%20Borgani%20and%20M.%20L.%20Brown%20and%20S.%20Bruton%20and%20A.%20Calabro%20and%20F.%20Caro%20and%20T.%20Castro%20and%20F.%20Cogato%20and%20S.%20Davini%20and%20G.%20Desprez%20and%20A.%20D%C3%ADaz-S%C3%A1nchez%20and%20J.%20J.%20Diaz%20and%20S.%20Di%20Domizio%20and%20J.%20M.%20Diego%20and%20P.%20-A.%20Duc%20and%20A.%20Enia%20and%20Y.%20Fang%20and%20A.%20G.%20Ferrari%20and%20A.%20Finoguenov%20and%20A.%20Fontana%20and%20A.%20Franco%20and%20J.%20Garc%C3%ADa-Bellido%20and%20T.%20Gasparetto%20and%20V.%20Gautard%20and%20E.%20Gaztanaga%20and%20F.%20Giacomini%20and%20F.%20Gianotti%20and%20M.%20Guidi%20and%20C.%20M.%20Gutierrez%20and%20A.%20Hall%20and%20S.%20Hemmati%20and%20H.%20Hildebrandt%20and%20J.%20Hjorth%20and%20J.%20J.%20E.%20Kajava%20and%20Y.%20Kang%20and%20V.%20Kansal%20and%20D.%20Karagiannis%20and%20C.%20C.%20Kirkpatrick%20and%20S.%20Kruk%20and%20L.%20Legrand%20and%20M.%20Lembo%20and%20F.%20Lepori%20and%20G.%20Leroy%20and%20J.%20Lesgourgues%20and%20L.%20Leuzzi%20and%20T.%20I.%20Liaudat%20and%20J.%20Macias-Perez%20and%20M.%20Magliocchetti%20and%20F.%20Mannucci%20and%20R.%20Maoli%20and%20C.%20J.%20A.%20P.%20Martins%20and%20L.%20Maurin%20and%20M.%20Miluzio%20and%20P.%20Monaco%20and%20G.%20Morgante%20and%20K.%20Naidoo%20and%20A.%20Navarro-Alsina%20and%20F.%20Passalacqua%20and%20K.%20Paterson%20and%20L.%20Patrizii%20and%20A.%20Pisani%20and%20D.%20Potter%20and%20S.%20Quai%20and%20M.%20Radovich%20and%20P.%20-F.%20Rocci%20and%20G.%20Rodighiero%20and%20S.%20Sacquegna%20and%20M.%20Sahl%C3%A9n%20and%20D.%20B.%20Sanders%20and%20E.%20Sarpa%20and%20A.%20Schneider%20and%20M.%20Schultheis%20and%20D.%20Sciotti%20and%20E.%20Sellentin%20and%20F.%20Shankar%20and%20L.%20C.%20Smith%20and%20K.%20Tanidis%20and%20G.%20Testera%20and%20R.%20Teyssier%20and%20S.%20Tosi%20and%20A.%20Troja%20and%20M.%20Tucci%20and%20C.%20Valieri%20and%20D.%20Vergani%20and%20G.%20Verza%20and%20N.%20A.%20Walton%0AAbstract%3A%20%20%20Light%20emission%20from%20galaxies%20exhibit%20diverse%20brightness%20profiles%2C%20influenced%0Aby%20factors%20such%20as%20galaxy%20type%2C%20structural%20features%20and%20interactions%20with%20other%0Agalaxies.%20Elliptical%20galaxies%20feature%20more%20uniform%20light%20distributions%2C%20while%0Aspiral%20and%20irregular%20galaxies%20have%20complex%2C%20varied%20light%20profiles%20due%20to%20their%0Astructural%20heterogeneity%20and%20star-forming%20activity.%20In%20addition%2C%20galaxies%20with%0Aan%20active%20galactic%20nucleus%20%28AGN%29%20feature%20intense%2C%20concentrated%20emission%20from%0Agas%20accretion%20around%20supermassive%20black%20holes%2C%20superimposed%20on%20regular%20galactic%0Alight%2C%20while%20quasi-stellar%20objects%20%28QSO%29%20are%20the%20extreme%20case%20of%20the%20AGN%0Aemission%20dominating%20the%20galaxy.%20The%20challenge%20of%20identifying%20AGN%20and%20QSO%20has%0Abeen%20discussed%20many%20times%20in%20the%20literature%2C%20often%20requiring%20multi-wavelength%0Aobservations.%20This%20paper%20introduces%20a%20novel%20approach%20to%20identify%20AGN%20and%20QSO%0Afrom%20a%20single%20image.%20Diffusion%20models%20have%20been%20recently%20developed%20in%20the%0Amachine-learning%20literature%20to%20generate%20realistic-looking%20images%20of%20everyday%0Aobjects.%20Utilising%20the%20spatial%20resolving%20power%20of%20the%20Euclid%20VIS%20images%2C%20we%0Acreated%20a%20diffusion%20model%20trained%20on%20one%20million%20sources%2C%20without%20using%20any%0Asource%20pre-selection%20or%20labels.%20The%20model%20learns%20to%20reconstruct%20light%0Adistributions%20of%20normal%20galaxies%2C%20since%20the%20population%20is%20dominated%20by%20them.%20We%0Acondition%20the%20prediction%20of%20the%20central%20light%20distribution%20by%20masking%20the%0Acentral%20few%20pixels%20of%20each%20source%20and%20reconstruct%20the%20light%20according%20to%20the%0Adiffusion%20model.%20We%20further%20use%20this%20prediction%20to%20identify%20sources%20that%0Adeviate%20from%20this%20profile%20by%20examining%20the%20reconstruction%20error%20of%20the%20few%0Acentral%20pixels%20regenerated%20in%20each%20source%27s%20core.%20Our%20approach%2C%20solely%20using%0AVIS%20imaging%2C%20features%20high%20completeness%20compared%20to%20traditional%20methods%20of%20AGN%0Aand%20QSO%20selection%2C%20including%20optical%2C%20near-infrared%2C%20mid-infrared%2C%20and%20X-rays.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.15321v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEuclid%2520Quick%2520Data%2520Release%2520%2528Q1%2529.%2520Active%2520galactic%2520nuclei%2520identification%250A%2520%2520using%2520diffusion-based%2520inpainting%2520of%2520Euclid%2520VIS%2520images%26entry.906535625%3D%2520Euclid%2520Collaboration%2520and%2520G.%2520Stevens%2520and%2520S.%2520Fotopoulou%2520and%2520M.%2520N.%2520Bremer%2520and%2520T.%2520Matamoro%2520Zatarain%2520and%2520K.%2520Jahnke%2520and%2520B.%2520Margalef-Bentabol%2520and%2520M.%2520Huertas-Company%2520and%2520M.%2520J.%2520Smith%2520and%2520M.%2520Walmsley%2520and%2520M.%2520Salvato%2520and%2520M.%2520Mezcua%2520and%2520A.%2520Paulino-Afonso%2520and%2520M.%2520Siudek%2520and%2520M.%2520Talia%2520and%2520F.%2520Ricci%2520and%2520W.%2520Roster%2520and%2520N.%2520Aghanim%2520and%2520B.%2520Altieri%2520and%2520S.%2520Andreon%2520and%2520H.%2520Aussel%2520and%2520C.%2520Baccigalupi%2520and%2520M.%2520Baldi%2520and%2520S.%2520Bardelli%2520and%2520P.%2520Battaglia%2520and%2520A.%2520Biviano%2520and%2520A.%2520Bonchi%2520and%2520E.%2520Branchini%2520and%2520M.%2520Brescia%2520and%2520J.%2520Brinchmann%2520and%2520S.%2520Camera%2520and%2520G.%2520Ca%25C3%25B1as-Herrera%2520and%2520V.%2520Capobianco%2520and%2520C.%2520Carbone%2520and%2520J.%2520Carretero%2520and%2520M.%2520Castellano%2520and%2520G.%2520Castignani%2520and%2520S.%2520Cavuoti%2520and%2520K.%2520C.%2520Chambers%2520and%2520A.%2520Cimatti%2520and%2520C.%2520Colodro-Conde%2520and%2520G.%2520Congedo%2520and%2520C.%2520J.%2520Conselice%2520and%2520L.%2520Conversi%2520and%2520Y.%2520Copin%2520and%2520A.%2520Costille%2520and%2520F.%2520Courbin%2520and%2520H.%2520M.%2520Courtois%2520and%2520M.%2520Cropper%2520and%2520A.%2520Da%2520Silva%2520and%2520H.%2520Degaudenzi%2520and%2520G.%2520De%2520Lucia%2520and%2520C.%2520Dolding%2520and%2520H.%2520Dole%2520and%2520M.%2520Douspis%2520and%2520F.%2520Dubath%2520and%2520X.%2520Dupac%2520and%2520S.%2520Dusini%2520and%2520S.%2520Escoffier%2520and%2520M.%2520Farina%2520and%2520S.%2520Ferriol%2520and%2520K.%2520George%2520and%2520C.%2520Giocoli%2520and%2520B.%2520R.%2520Granett%2520and%2520A.%2520Grazian%2520and%2520F.%2520Grupp%2520and%2520S.%2520V.%2520H.%2520Haugan%2520and%2520I.%2520M.%2520Hook%2520and%2520F.%2520Hormuth%2520and%2520A.%2520Hornstrup%2520and%2520P.%2520Hudelot%2520and%2520M.%2520Jhabvala%2520and%2520E.%2520Keih%25C3%25A4nen%2520and%2520S.%2520Kermiche%2520and%2520A.%2520Kiessling%2520and%2520M.%2520Kilbinger%2520and%2520B.%2520Kubik%2520and%2520M.%2520K%25C3%25BCmmel%2520and%2520H.%2520Kurki-Suonio%2520and%2520Q.%2520Le%2520Boulc%2527h%2520and%2520A.%2520M.%2520C.%2520Le%2520Brun%2520and%2520D.%2520Le%2520Mignant%2520and%2520P.%2520B.%2520Lilje%2520and%2520V.%2520Lindholm%2520and%2520I.%2520Lloro%2520and%2520G.%2520Mainetti%2520and%2520D.%2520Maino%2520and%2520E.%2520Maiorano%2520and%2520O.%2520Marggraf%2520and%2520M.%2520Martinelli%2520and%2520N.%2520Martinet%2520and%2520F.%2520Marulli%2520and%2520R.%2520Massey%2520and%2520S.%2520Maurogordato%2520and%2520H.%2520J.%2520McCracken%2520and%2520E.%2520Medinaceli%2520and%2520S.%2520Mei%2520and%2520M.%2520Melchior%2520and%2520M.%2520Meneghetti%2520and%2520E.%2520Merlin%2520and%2520G.%2520Meylan%2520and%2520A.%2520Mora%2520and%2520M.%2520Moresco%2520and%2520L.%2520Moscardini%2520and%2520R.%2520Nakajima%2520and%2520C.%2520Neissner%2520and%2520S.%2520-M.%2520Niemi%2520and%2520C.%2520Padilla%2520and%2520S.%2520Paltani%2520and%2520F.%2520Pasian%2520and%2520K.%2520Pedersen%2520and%2520W.%2520J.%2520Percival%2520and%2520V.%2520Pettorino%2520and%2520G.%2520Polenta%2520and%2520M.%2520Poncet%2520and%2520L.%2520A.%2520Popa%2520and%2520L.%2520Pozzetti%2520and%2520F.%2520Raison%2520and%2520R.%2520Rebolo%2520and%2520A.%2520Renzi%2520and%2520J.%2520Rhodes%2520and%2520G.%2520Riccio%2520and%2520E.%2520Romelli%2520and%2520M.%2520Roncarelli%2520and%2520R.%2520Saglia%2520and%2520A.%2520G.%2520S%25C3%25A1nchez%2520and%2520D.%2520Sapone%2520and%2520J.%2520A.%2520Schewtschenko%2520and%2520M.%2520Schirmer%2520and%2520P.%2520Schneider%2520and%2520T.%2520Schrabback%2520and%2520A.%2520Secroun%2520and%2520S.%2520Serrano%2520and%2520P.%2520Simon%2520and%2520C.%2520Sirignano%2520and%2520G.%2520Sirri%2520and%2520J.%2520Skottfelt%2520and%2520L.%2520Stanco%2520and%2520J.%2520Steinwagner%2520and%2520P.%2520Tallada-Cresp%25C3%25AD%2520and%2520A.%2520N.%2520Taylor%2520and%2520I.%2520Tereno%2520and%2520S.%2520Toft%2520and%2520R.%2520Toledo-Moreo%2520and%2520F.%2520Torradeflot%2520and%2520I.%2520Tutusaus%2520and%2520L.%2520Valenziano%2520and%2520J.%2520Valiviita%2520and%2520T.%2520Vassallo%2520and%2520G.%2520Verdoes%2520Kleijn%2520and%2520A.%2520Veropalumbo%2520and%2520Y.%2520Wang%2520and%2520J.%2520Weller%2520and%2520A.%2520Zacchei%2520and%2520G.%2520Zamorani%2520and%2520F.%2520M.%2520Zerbi%2520and%2520I.%2520A.%2520Zinchenko%2520and%2520E.%2520Zucca%2520and%2520V.%2520Allevato%2520and%2520M.%2520Ballardini%2520and%2520M.%2520Bolzonella%2520and%2520E.%2520Bozzo%2520and%2520C.%2520Burigana%2520and%2520R.%2520Cabanac%2520and%2520A.%2520Cappi%2520and%2520J.%2520A.%2520Escartin%2520Vigo%2520and%2520L.%2520Gabarra%2520and%2520W.%2520G.%2520Hartley%2520and%2520J.%2520Mart%25C3%25ADn-Fleitas%2520and%2520S.%2520Matthew%2520and%2520R.%2520B.%2520Metcalf%2520and%2520A.%2520Pezzotta%2520and%2520M.%2520P%25C3%25B6ntinen%2520and%2520I.%2520Risso%2520and%2520V.%2520Scottez%2520and%2520M.%2520Sereno%2520and%2520M.%2520Tenti%2520and%2520M.%2520Wiesmann%2520and%2520Y.%2520Akrami%2520and%2520S.%2520Alvi%2520and%2520I.%2520T.%2520Andika%2520and%2520S.%2520Anselmi%2520and%2520M.%2520Archidiacono%2520and%2520F.%2520Atrio-Barandela%2520and%2520D.%2520Bertacca%2520and%2520M.%2520Bethermin%2520and%2520L.%2520Bisigello%2520and%2520A.%2520Blanchard%2520and%2520L.%2520Blot%2520and%2520S.%2520Borgani%2520and%2520M.%2520L.%2520Brown%2520and%2520S.%2520Bruton%2520and%2520A.%2520Calabro%2520and%2520F.%2520Caro%2520and%2520T.%2520Castro%2520and%2520F.%2520Cogato%2520and%2520S.%2520Davini%2520and%2520G.%2520Desprez%2520and%2520A.%2520D%25C3%25ADaz-S%25C3%25A1nchez%2520and%2520J.%2520J.%2520Diaz%2520and%2520S.%2520Di%2520Domizio%2520and%2520J.%2520M.%2520Diego%2520and%2520P.%2520-A.%2520Duc%2520and%2520A.%2520Enia%2520and%2520Y.%2520Fang%2520and%2520A.%2520G.%2520Ferrari%2520and%2520A.%2520Finoguenov%2520and%2520A.%2520Fontana%2520and%2520A.%2520Franco%2520and%2520J.%2520Garc%25C3%25ADa-Bellido%2520and%2520T.%2520Gasparetto%2520and%2520V.%2520Gautard%2520and%2520E.%2520Gaztanaga%2520and%2520F.%2520Giacomini%2520and%2520F.%2520Gianotti%2520and%2520M.%2520Guidi%2520and%2520C.%2520M.%2520Gutierrez%2520and%2520A.%2520Hall%2520and%2520S.%2520Hemmati%2520and%2520H.%2520Hildebrandt%2520and%2520J.%2520Hjorth%2520and%2520J.%2520J.%2520E.%2520Kajava%2520and%2520Y.%2520Kang%2520and%2520V.%2520Kansal%2520and%2520D.%2520Karagiannis%2520and%2520C.%2520C.%2520Kirkpatrick%2520and%2520S.%2520Kruk%2520and%2520L.%2520Legrand%2520and%2520M.%2520Lembo%2520and%2520F.%2520Lepori%2520and%2520G.%2520Leroy%2520and%2520J.%2520Lesgourgues%2520and%2520L.%2520Leuzzi%2520and%2520T.%2520I.%2520Liaudat%2520and%2520J.%2520Macias-Perez%2520and%2520M.%2520Magliocchetti%2520and%2520F.%2520Mannucci%2520and%2520R.%2520Maoli%2520and%2520C.%2520J.%2520A.%2520P.%2520Martins%2520and%2520L.%2520Maurin%2520and%2520M.%2520Miluzio%2520and%2520P.%2520Monaco%2520and%2520G.%2520Morgante%2520and%2520K.%2520Naidoo%2520and%2520A.%2520Navarro-Alsina%2520and%2520F.%2520Passalacqua%2520and%2520K.%2520Paterson%2520and%2520L.%2520Patrizii%2520and%2520A.%2520Pisani%2520and%2520D.%2520Potter%2520and%2520S.%2520Quai%2520and%2520M.%2520Radovich%2520and%2520P.%2520-F.%2520Rocci%2520and%2520G.%2520Rodighiero%2520and%2520S.%2520Sacquegna%2520and%2520M.%2520Sahl%25C3%25A9n%2520and%2520D.%2520B.%2520Sanders%2520and%2520E.%2520Sarpa%2520and%2520A.%2520Schneider%2520and%2520M.%2520Schultheis%2520and%2520D.%2520Sciotti%2520and%2520E.%2520Sellentin%2520and%2520F.%2520Shankar%2520and%2520L.%2520C.%2520Smith%2520and%2520K.%2520Tanidis%2520and%2520G.%2520Testera%2520and%2520R.%2520Teyssier%2520and%2520S.%2520Tosi%2520and%2520A.%2520Troja%2520and%2520M.%2520Tucci%2520and%2520C.%2520Valieri%2520and%2520D.%2520Vergani%2520and%2520G.%2520Verza%2520and%2520N.%2520A.%2520Walton%26entry.1292438233%3D%2520%2520Light%2520emission%2520from%2520galaxies%2520exhibit%2520diverse%2520brightness%2520profiles%252C%2520influenced%250Aby%2520factors%2520such%2520as%2520galaxy%2520type%252C%2520structural%2520features%2520and%2520interactions%2520with%2520other%250Agalaxies.%2520Elliptical%2520galaxies%2520feature%2520more%2520uniform%2520light%2520distributions%252C%2520while%250Aspiral%2520and%2520irregular%2520galaxies%2520have%2520complex%252C%2520varied%2520light%2520profiles%2520due%2520to%2520their%250Astructural%2520heterogeneity%2520and%2520star-forming%2520activity.%2520In%2520addition%252C%2520galaxies%2520with%250Aan%2520active%2520galactic%2520nucleus%2520%2528AGN%2529%2520feature%2520intense%252C%2520concentrated%2520emission%2520from%250Agas%2520accretion%2520around%2520supermassive%2520black%2520holes%252C%2520superimposed%2520on%2520regular%2520galactic%250Alight%252C%2520while%2520quasi-stellar%2520objects%2520%2528QSO%2529%2520are%2520the%2520extreme%2520case%2520of%2520the%2520AGN%250Aemission%2520dominating%2520the%2520galaxy.%2520The%2520challenge%2520of%2520identifying%2520AGN%2520and%2520QSO%2520has%250Abeen%2520discussed%2520many%2520times%2520in%2520the%2520literature%252C%2520often%2520requiring%2520multi-wavelength%250Aobservations.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%2520identify%2520AGN%2520and%2520QSO%250Afrom%2520a%2520single%2520image.%2520Diffusion%2520models%2520have%2520been%2520recently%2520developed%2520in%2520the%250Amachine-learning%2520literature%2520to%2520generate%2520realistic-looking%2520images%2520of%2520everyday%250Aobjects.%2520Utilising%2520the%2520spatial%2520resolving%2520power%2520of%2520the%2520Euclid%2520VIS%2520images%252C%2520we%250Acreated%2520a%2520diffusion%2520model%2520trained%2520on%2520one%2520million%2520sources%252C%2520without%2520using%2520any%250Asource%2520pre-selection%2520or%2520labels.%2520The%2520model%2520learns%2520to%2520reconstruct%2520light%250Adistributions%2520of%2520normal%2520galaxies%252C%2520since%2520the%2520population%2520is%2520dominated%2520by%2520them.%2520We%250Acondition%2520the%2520prediction%2520of%2520the%2520central%2520light%2520distribution%2520by%2520masking%2520the%250Acentral%2520few%2520pixels%2520of%2520each%2520source%2520and%2520reconstruct%2520the%2520light%2520according%2520to%2520the%250Adiffusion%2520model.%2520We%2520further%2520use%2520this%2520prediction%2520to%2520identify%2520sources%2520that%250Adeviate%2520from%2520this%2520profile%2520by%2520examining%2520the%2520reconstruction%2520error%2520of%2520the%2520few%250Acentral%2520pixels%2520regenerated%2520in%2520each%2520source%2527s%2520core.%2520Our%2520approach%252C%2520solely%2520using%250AVIS%2520imaging%252C%2520features%2520high%2520completeness%2520compared%2520to%2520traditional%2520methods%2520of%2520AGN%250Aand%2520QSO%2520selection%252C%2520including%2520optical%252C%2520near-infrared%252C%2520mid-infrared%252C%2520and%2520X-rays.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.15321v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Euclid%20Quick%20Data%20Release%20%28Q1%29.%20Active%20galactic%20nuclei%20identification%0A%20%20using%20diffusion-based%20inpainting%20of%20Euclid%20VIS%20images&entry.906535625=%20Euclid%20Collaboration%20and%20G.%20Stevens%20and%20S.%20Fotopoulou%20and%20M.%20N.%20Bremer%20and%20T.%20Matamoro%20Zatarain%20and%20K.%20Jahnke%20and%20B.%20Margalef-Bentabol%20and%20M.%20Huertas-Company%20and%20M.%20J.%20Smith%20and%20M.%20Walmsley%20and%20M.%20Salvato%20and%20M.%20Mezcua%20and%20A.%20Paulino-Afonso%20and%20M.%20Siudek%20and%20M.%20Talia%20and%20F.%20Ricci%20and%20W.%20Roster%20and%20N.%20Aghanim%20and%20B.%20Altieri%20and%20S.%20Andreon%20and%20H.%20Aussel%20and%20C.%20Baccigalupi%20and%20M.%20Baldi%20and%20S.%20Bardelli%20and%20P.%20Battaglia%20and%20A.%20Biviano%20and%20A.%20Bonchi%20and%20E.%20Branchini%20and%20M.%20Brescia%20and%20J.%20Brinchmann%20and%20S.%20Camera%20and%20G.%20Ca%C3%B1as-Herrera%20and%20V.%20Capobianco%20and%20C.%20Carbone%20and%20J.%20Carretero%20and%20M.%20Castellano%20and%20G.%20Castignani%20and%20S.%20Cavuoti%20and%20K.%20C.%20Chambers%20and%20A.%20Cimatti%20and%20C.%20Colodro-Conde%20and%20G.%20Congedo%20and%20C.%20J.%20Conselice%20and%20L.%20Conversi%20and%20Y.%20Copin%20and%20A.%20Costille%20and%20F.%20Courbin%20and%20H.%20M.%20Courtois%20and%20M.%20Cropper%20and%20A.%20Da%20Silva%20and%20H.%20Degaudenzi%20and%20G.%20De%20Lucia%20and%20C.%20Dolding%20and%20H.%20Dole%20and%20M.%20Douspis%20and%20F.%20Dubath%20and%20X.%20Dupac%20and%20S.%20Dusini%20and%20S.%20Escoffier%20and%20M.%20Farina%20and%20S.%20Ferriol%20and%20K.%20George%20and%20C.%20Giocoli%20and%20B.%20R.%20Granett%20and%20A.%20Grazian%20and%20F.%20Grupp%20and%20S.%20V.%20H.%20Haugan%20and%20I.%20M.%20Hook%20and%20F.%20Hormuth%20and%20A.%20Hornstrup%20and%20P.%20Hudelot%20and%20M.%20Jhabvala%20and%20E.%20Keih%C3%A4nen%20and%20S.%20Kermiche%20and%20A.%20Kiessling%20and%20M.%20Kilbinger%20and%20B.%20Kubik%20and%20M.%20K%C3%BCmmel%20and%20H.%20Kurki-Suonio%20and%20Q.%20Le%20Boulc%27h%20and%20A.%20M.%20C.%20Le%20Brun%20and%20D.%20Le%20Mignant%20and%20P.%20B.%20Lilje%20and%20V.%20Lindholm%20and%20I.%20Lloro%20and%20G.%20Mainetti%20and%20D.%20Maino%20and%20E.%20Maiorano%20and%20O.%20Marggraf%20and%20M.%20Martinelli%20and%20N.%20Martinet%20and%20F.%20Marulli%20and%20R.%20Massey%20and%20S.%20Maurogordato%20and%20H.%20J.%20McCracken%20and%20E.%20Medinaceli%20and%20S.%20Mei%20and%20M.%20Melchior%20and%20M.%20Meneghetti%20and%20E.%20Merlin%20and%20G.%20Meylan%20and%20A.%20Mora%20and%20M.%20Moresco%20and%20L.%20Moscardini%20and%20R.%20Nakajima%20and%20C.%20Neissner%20and%20S.%20-M.%20Niemi%20and%20C.%20Padilla%20and%20S.%20Paltani%20and%20F.%20Pasian%20and%20K.%20Pedersen%20and%20W.%20J.%20Percival%20and%20V.%20Pettorino%20and%20G.%20Polenta%20and%20M.%20Poncet%20and%20L.%20A.%20Popa%20and%20L.%20Pozzetti%20and%20F.%20Raison%20and%20R.%20Rebolo%20and%20A.%20Renzi%20and%20J.%20Rhodes%20and%20G.%20Riccio%20and%20E.%20Romelli%20and%20M.%20Roncarelli%20and%20R.%20Saglia%20and%20A.%20G.%20S%C3%A1nchez%20and%20D.%20Sapone%20and%20J.%20A.%20Schewtschenko%20and%20M.%20Schirmer%20and%20P.%20Schneider%20and%20T.%20Schrabback%20and%20A.%20Secroun%20and%20S.%20Serrano%20and%20P.%20Simon%20and%20C.%20Sirignano%20and%20G.%20Sirri%20and%20J.%20Skottfelt%20and%20L.%20Stanco%20and%20J.%20Steinwagner%20and%20P.%20Tallada-Cresp%C3%AD%20and%20A.%20N.%20Taylor%20and%20I.%20Tereno%20and%20S.%20Toft%20and%20R.%20Toledo-Moreo%20and%20F.%20Torradeflot%20and%20I.%20Tutusaus%20and%20L.%20Valenziano%20and%20J.%20Valiviita%20and%20T.%20Vassallo%20and%20G.%20Verdoes%20Kleijn%20and%20A.%20Veropalumbo%20and%20Y.%20Wang%20and%20J.%20Weller%20and%20A.%20Zacchei%20and%20G.%20Zamorani%20and%20F.%20M.%20Zerbi%20and%20I.%20A.%20Zinchenko%20and%20E.%20Zucca%20and%20V.%20Allevato%20and%20M.%20Ballardini%20and%20M.%20Bolzonella%20and%20E.%20Bozzo%20and%20C.%20Burigana%20and%20R.%20Cabanac%20and%20A.%20Cappi%20and%20J.%20A.%20Escartin%20Vigo%20and%20L.%20Gabarra%20and%20W.%20G.%20Hartley%20and%20J.%20Mart%C3%ADn-Fleitas%20and%20S.%20Matthew%20and%20R.%20B.%20Metcalf%20and%20A.%20Pezzotta%20and%20M.%20P%C3%B6ntinen%20and%20I.%20Risso%20and%20V.%20Scottez%20and%20M.%20Sereno%20and%20M.%20Tenti%20and%20M.%20Wiesmann%20and%20Y.%20Akrami%20and%20S.%20Alvi%20and%20I.%20T.%20Andika%20and%20S.%20Anselmi%20and%20M.%20Archidiacono%20and%20F.%20Atrio-Barandela%20and%20D.%20Bertacca%20and%20M.%20Bethermin%20and%20L.%20Bisigello%20and%20A.%20Blanchard%20and%20L.%20Blot%20and%20S.%20Borgani%20and%20M.%20L.%20Brown%20and%20S.%20Bruton%20and%20A.%20Calabro%20and%20F.%20Caro%20and%20T.%20Castro%20and%20F.%20Cogato%20and%20S.%20Davini%20and%20G.%20Desprez%20and%20A.%20D%C3%ADaz-S%C3%A1nchez%20and%20J.%20J.%20Diaz%20and%20S.%20Di%20Domizio%20and%20J.%20M.%20Diego%20and%20P.%20-A.%20Duc%20and%20A.%20Enia%20and%20Y.%20Fang%20and%20A.%20G.%20Ferrari%20and%20A.%20Finoguenov%20and%20A.%20Fontana%20and%20A.%20Franco%20and%20J.%20Garc%C3%ADa-Bellido%20and%20T.%20Gasparetto%20and%20V.%20Gautard%20and%20E.%20Gaztanaga%20and%20F.%20Giacomini%20and%20F.%20Gianotti%20and%20M.%20Guidi%20and%20C.%20M.%20Gutierrez%20and%20A.%20Hall%20and%20S.%20Hemmati%20and%20H.%20Hildebrandt%20and%20J.%20Hjorth%20and%20J.%20J.%20E.%20Kajava%20and%20Y.%20Kang%20and%20V.%20Kansal%20and%20D.%20Karagiannis%20and%20C.%20C.%20Kirkpatrick%20and%20S.%20Kruk%20and%20L.%20Legrand%20and%20M.%20Lembo%20and%20F.%20Lepori%20and%20G.%20Leroy%20and%20J.%20Lesgourgues%20and%20L.%20Leuzzi%20and%20T.%20I.%20Liaudat%20and%20J.%20Macias-Perez%20and%20M.%20Magliocchetti%20and%20F.%20Mannucci%20and%20R.%20Maoli%20and%20C.%20J.%20A.%20P.%20Martins%20and%20L.%20Maurin%20and%20M.%20Miluzio%20and%20P.%20Monaco%20and%20G.%20Morgante%20and%20K.%20Naidoo%20and%20A.%20Navarro-Alsina%20and%20F.%20Passalacqua%20and%20K.%20Paterson%20and%20L.%20Patrizii%20and%20A.%20Pisani%20and%20D.%20Potter%20and%20S.%20Quai%20and%20M.%20Radovich%20and%20P.%20-F.%20Rocci%20and%20G.%20Rodighiero%20and%20S.%20Sacquegna%20and%20M.%20Sahl%C3%A9n%20and%20D.%20B.%20Sanders%20and%20E.%20Sarpa%20and%20A.%20Schneider%20and%20M.%20Schultheis%20and%20D.%20Sciotti%20and%20E.%20Sellentin%20and%20F.%20Shankar%20and%20L.%20C.%20Smith%20and%20K.%20Tanidis%20and%20G.%20Testera%20and%20R.%20Teyssier%20and%20S.%20Tosi%20and%20A.%20Troja%20and%20M.%20Tucci%20and%20C.%20Valieri%20and%20D.%20Vergani%20and%20G.%20Verza%20and%20N.%20A.%20Walton&entry.1292438233=%20%20Light%20emission%20from%20galaxies%20exhibit%20diverse%20brightness%20profiles%2C%20influenced%0Aby%20factors%20such%20as%20galaxy%20type%2C%20structural%20features%20and%20interactions%20with%20other%0Agalaxies.%20Elliptical%20galaxies%20feature%20more%20uniform%20light%20distributions%2C%20while%0Aspiral%20and%20irregular%20galaxies%20have%20complex%2C%20varied%20light%20profiles%20due%20to%20their%0Astructural%20heterogeneity%20and%20star-forming%20activity.%20In%20addition%2C%20galaxies%20with%0Aan%20active%20galactic%20nucleus%20%28AGN%29%20feature%20intense%2C%20concentrated%20emission%20from%0Agas%20accretion%20around%20supermassive%20black%20holes%2C%20superimposed%20on%20regular%20galactic%0Alight%2C%20while%20quasi-stellar%20objects%20%28QSO%29%20are%20the%20extreme%20case%20of%20the%20AGN%0Aemission%20dominating%20the%20galaxy.%20The%20challenge%20of%20identifying%20AGN%20and%20QSO%20has%0Abeen%20discussed%20many%20times%20in%20the%20literature%2C%20often%20requiring%20multi-wavelength%0Aobservations.%20This%20paper%20introduces%20a%20novel%20approach%20to%20identify%20AGN%20and%20QSO%0Afrom%20a%20single%20image.%20Diffusion%20models%20have%20been%20recently%20developed%20in%20the%0Amachine-learning%20literature%20to%20generate%20realistic-looking%20images%20of%20everyday%0Aobjects.%20Utilising%20the%20spatial%20resolving%20power%20of%20the%20Euclid%20VIS%20images%2C%20we%0Acreated%20a%20diffusion%20model%20trained%20on%20one%20million%20sources%2C%20without%20using%20any%0Asource%20pre-selection%20or%20labels.%20The%20model%20learns%20to%20reconstruct%20light%0Adistributions%20of%20normal%20galaxies%2C%20since%20the%20population%20is%20dominated%20by%20them.%20We%0Acondition%20the%20prediction%20of%20the%20central%20light%20distribution%20by%20masking%20the%0Acentral%20few%20pixels%20of%20each%20source%20and%20reconstruct%20the%20light%20according%20to%20the%0Adiffusion%20model.%20We%20further%20use%20this%20prediction%20to%20identify%20sources%20that%0Adeviate%20from%20this%20profile%20by%20examining%20the%20reconstruction%20error%20of%20the%20few%0Acentral%20pixels%20regenerated%20in%20each%20source%27s%20core.%20Our%20approach%2C%20solely%20using%0AVIS%20imaging%2C%20features%20high%20completeness%20compared%20to%20traditional%20methods%20of%20AGN%0Aand%20QSO%20selection%2C%20including%20optical%2C%20near-infrared%2C%20mid-infrared%2C%20and%20X-rays.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.15321v2&entry.124074799=Read"},
{"title": "Bridging Formal Language with Chain-of-Thought Reasoning to Geometry\n  Problem Solving", "author": "Tianyun Yang and Yunwen Li and Ziniu Li and Zhihang Lin and Ruoyu Sun and Tian Ding", "abstract": "  Large vision language models exhibit notable limitations on Geometry Problem\nSolving (GPS) because of their unreliable diagram interpretation and pure\nnatural-language reasoning. A recent line of work mitigates this by using\nsymbolic solvers: the model directly generates a formal program that a geometry\nsolver can execute. However, this direct program generation lacks intermediate\nreasoning, making the decision process opaque and prone to errors. In this\nwork, we explore a new approach that integrates Chain-of-Thought (CoT) with\nformal language. The model interleaves natural language reasoning with\nincremental emission of solver-executable code, producing a hybrid reasoning\ntrace in which critical derivations are expressed in formal language. To teach\nthis behavior at scale, we combine (1) supervised fine-tuning on an 11K newly\ndeveloped synthetic dataset with interleaved natural language reasoning and\nautomatic formalization, and (2) solver-in-the-loop reinforcement learning that\njointly optimizes both the CoT narrative and the resulting program through\noutcome-based rewards. Built on Qwen2.5-VL-7B, our new model, named\nGF-Reasoner, achieves up to 15% accuracy improvements on standard GPS\nbenchmarks, surpassing both 7B-scale peers and the much larger model\nQwen2.5-VL-72B. By exploiting high-order geometric knowledge and offloading\nsymbolic computation to the solver, the generated reasoning traces are\nnoticeably shorter and cleaner. Furthermore, we present a comprehensive\nanalysis of method design choices (e.g., reasoning paradigms, data synthesis,\ntraining epochs, etc.), providing actionable insights for future research.\n", "link": "http://arxiv.org/abs/2508.09099v1", "date": "2025-08-12", "relevancy": 2.14, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.541}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Formal%20Language%20with%20Chain-of-Thought%20Reasoning%20to%20Geometry%0A%20%20Problem%20Solving&body=Title%3A%20Bridging%20Formal%20Language%20with%20Chain-of-Thought%20Reasoning%20to%20Geometry%0A%20%20Problem%20Solving%0AAuthor%3A%20Tianyun%20Yang%20and%20Yunwen%20Li%20and%20Ziniu%20Li%20and%20Zhihang%20Lin%20and%20Ruoyu%20Sun%20and%20Tian%20Ding%0AAbstract%3A%20%20%20Large%20vision%20language%20models%20exhibit%20notable%20limitations%20on%20Geometry%20Problem%0ASolving%20%28GPS%29%20because%20of%20their%20unreliable%20diagram%20interpretation%20and%20pure%0Anatural-language%20reasoning.%20A%20recent%20line%20of%20work%20mitigates%20this%20by%20using%0Asymbolic%20solvers%3A%20the%20model%20directly%20generates%20a%20formal%20program%20that%20a%20geometry%0Asolver%20can%20execute.%20However%2C%20this%20direct%20program%20generation%20lacks%20intermediate%0Areasoning%2C%20making%20the%20decision%20process%20opaque%20and%20prone%20to%20errors.%20In%20this%0Awork%2C%20we%20explore%20a%20new%20approach%20that%20integrates%20Chain-of-Thought%20%28CoT%29%20with%0Aformal%20language.%20The%20model%20interleaves%20natural%20language%20reasoning%20with%0Aincremental%20emission%20of%20solver-executable%20code%2C%20producing%20a%20hybrid%20reasoning%0Atrace%20in%20which%20critical%20derivations%20are%20expressed%20in%20formal%20language.%20To%20teach%0Athis%20behavior%20at%20scale%2C%20we%20combine%20%281%29%20supervised%20fine-tuning%20on%20an%2011K%20newly%0Adeveloped%20synthetic%20dataset%20with%20interleaved%20natural%20language%20reasoning%20and%0Aautomatic%20formalization%2C%20and%20%282%29%20solver-in-the-loop%20reinforcement%20learning%20that%0Ajointly%20optimizes%20both%20the%20CoT%20narrative%20and%20the%20resulting%20program%20through%0Aoutcome-based%20rewards.%20Built%20on%20Qwen2.5-VL-7B%2C%20our%20new%20model%2C%20named%0AGF-Reasoner%2C%20achieves%20up%20to%2015%25%20accuracy%20improvements%20on%20standard%20GPS%0Abenchmarks%2C%20surpassing%20both%207B-scale%20peers%20and%20the%20much%20larger%20model%0AQwen2.5-VL-72B.%20By%20exploiting%20high-order%20geometric%20knowledge%20and%20offloading%0Asymbolic%20computation%20to%20the%20solver%2C%20the%20generated%20reasoning%20traces%20are%0Anoticeably%20shorter%20and%20cleaner.%20Furthermore%2C%20we%20present%20a%20comprehensive%0Aanalysis%20of%20method%20design%20choices%20%28e.g.%2C%20reasoning%20paradigms%2C%20data%20synthesis%2C%0Atraining%20epochs%2C%20etc.%29%2C%20providing%20actionable%20insights%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Formal%2520Language%2520with%2520Chain-of-Thought%2520Reasoning%2520to%2520Geometry%250A%2520%2520Problem%2520Solving%26entry.906535625%3DTianyun%2520Yang%2520and%2520Yunwen%2520Li%2520and%2520Ziniu%2520Li%2520and%2520Zhihang%2520Lin%2520and%2520Ruoyu%2520Sun%2520and%2520Tian%2520Ding%26entry.1292438233%3D%2520%2520Large%2520vision%2520language%2520models%2520exhibit%2520notable%2520limitations%2520on%2520Geometry%2520Problem%250ASolving%2520%2528GPS%2529%2520because%2520of%2520their%2520unreliable%2520diagram%2520interpretation%2520and%2520pure%250Anatural-language%2520reasoning.%2520A%2520recent%2520line%2520of%2520work%2520mitigates%2520this%2520by%2520using%250Asymbolic%2520solvers%253A%2520the%2520model%2520directly%2520generates%2520a%2520formal%2520program%2520that%2520a%2520geometry%250Asolver%2520can%2520execute.%2520However%252C%2520this%2520direct%2520program%2520generation%2520lacks%2520intermediate%250Areasoning%252C%2520making%2520the%2520decision%2520process%2520opaque%2520and%2520prone%2520to%2520errors.%2520In%2520this%250Awork%252C%2520we%2520explore%2520a%2520new%2520approach%2520that%2520integrates%2520Chain-of-Thought%2520%2528CoT%2529%2520with%250Aformal%2520language.%2520The%2520model%2520interleaves%2520natural%2520language%2520reasoning%2520with%250Aincremental%2520emission%2520of%2520solver-executable%2520code%252C%2520producing%2520a%2520hybrid%2520reasoning%250Atrace%2520in%2520which%2520critical%2520derivations%2520are%2520expressed%2520in%2520formal%2520language.%2520To%2520teach%250Athis%2520behavior%2520at%2520scale%252C%2520we%2520combine%2520%25281%2529%2520supervised%2520fine-tuning%2520on%2520an%252011K%2520newly%250Adeveloped%2520synthetic%2520dataset%2520with%2520interleaved%2520natural%2520language%2520reasoning%2520and%250Aautomatic%2520formalization%252C%2520and%2520%25282%2529%2520solver-in-the-loop%2520reinforcement%2520learning%2520that%250Ajointly%2520optimizes%2520both%2520the%2520CoT%2520narrative%2520and%2520the%2520resulting%2520program%2520through%250Aoutcome-based%2520rewards.%2520Built%2520on%2520Qwen2.5-VL-7B%252C%2520our%2520new%2520model%252C%2520named%250AGF-Reasoner%252C%2520achieves%2520up%2520to%252015%2525%2520accuracy%2520improvements%2520on%2520standard%2520GPS%250Abenchmarks%252C%2520surpassing%2520both%25207B-scale%2520peers%2520and%2520the%2520much%2520larger%2520model%250AQwen2.5-VL-72B.%2520By%2520exploiting%2520high-order%2520geometric%2520knowledge%2520and%2520offloading%250Asymbolic%2520computation%2520to%2520the%2520solver%252C%2520the%2520generated%2520reasoning%2520traces%2520are%250Anoticeably%2520shorter%2520and%2520cleaner.%2520Furthermore%252C%2520we%2520present%2520a%2520comprehensive%250Aanalysis%2520of%2520method%2520design%2520choices%2520%2528e.g.%252C%2520reasoning%2520paradigms%252C%2520data%2520synthesis%252C%250Atraining%2520epochs%252C%2520etc.%2529%252C%2520providing%2520actionable%2520insights%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Formal%20Language%20with%20Chain-of-Thought%20Reasoning%20to%20Geometry%0A%20%20Problem%20Solving&entry.906535625=Tianyun%20Yang%20and%20Yunwen%20Li%20and%20Ziniu%20Li%20and%20Zhihang%20Lin%20and%20Ruoyu%20Sun%20and%20Tian%20Ding&entry.1292438233=%20%20Large%20vision%20language%20models%20exhibit%20notable%20limitations%20on%20Geometry%20Problem%0ASolving%20%28GPS%29%20because%20of%20their%20unreliable%20diagram%20interpretation%20and%20pure%0Anatural-language%20reasoning.%20A%20recent%20line%20of%20work%20mitigates%20this%20by%20using%0Asymbolic%20solvers%3A%20the%20model%20directly%20generates%20a%20formal%20program%20that%20a%20geometry%0Asolver%20can%20execute.%20However%2C%20this%20direct%20program%20generation%20lacks%20intermediate%0Areasoning%2C%20making%20the%20decision%20process%20opaque%20and%20prone%20to%20errors.%20In%20this%0Awork%2C%20we%20explore%20a%20new%20approach%20that%20integrates%20Chain-of-Thought%20%28CoT%29%20with%0Aformal%20language.%20The%20model%20interleaves%20natural%20language%20reasoning%20with%0Aincremental%20emission%20of%20solver-executable%20code%2C%20producing%20a%20hybrid%20reasoning%0Atrace%20in%20which%20critical%20derivations%20are%20expressed%20in%20formal%20language.%20To%20teach%0Athis%20behavior%20at%20scale%2C%20we%20combine%20%281%29%20supervised%20fine-tuning%20on%20an%2011K%20newly%0Adeveloped%20synthetic%20dataset%20with%20interleaved%20natural%20language%20reasoning%20and%0Aautomatic%20formalization%2C%20and%20%282%29%20solver-in-the-loop%20reinforcement%20learning%20that%0Ajointly%20optimizes%20both%20the%20CoT%20narrative%20and%20the%20resulting%20program%20through%0Aoutcome-based%20rewards.%20Built%20on%20Qwen2.5-VL-7B%2C%20our%20new%20model%2C%20named%0AGF-Reasoner%2C%20achieves%20up%20to%2015%25%20accuracy%20improvements%20on%20standard%20GPS%0Abenchmarks%2C%20surpassing%20both%207B-scale%20peers%20and%20the%20much%20larger%20model%0AQwen2.5-VL-72B.%20By%20exploiting%20high-order%20geometric%20knowledge%20and%20offloading%0Asymbolic%20computation%20to%20the%20solver%2C%20the%20generated%20reasoning%20traces%20are%0Anoticeably%20shorter%20and%20cleaner.%20Furthermore%2C%20we%20present%20a%20comprehensive%0Aanalysis%20of%20method%20design%20choices%20%28e.g.%2C%20reasoning%20paradigms%2C%20data%20synthesis%2C%0Atraining%20epochs%2C%20etc.%29%2C%20providing%20actionable%20insights%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09099v1&entry.124074799=Read"},
{"title": "LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA", "author": "Adri\u00e1n Gude and Roi Santos-R\u00edos and Francisco Prado-Vali\u00f1o and Ana Ezquerro and Jes\u00fas Vilares", "abstract": "  This paper describes our participation in SemEval 2025 Task 8, focused on\nTabular Question Answering. We developed a zero-shot pipeline that leverages an\nLarge Language Model to generate functional code capable of extracting the\nrelevant information from tabular data based on an input question. Our approach\nconsists of a modular pipeline where the main code generator module is\nsupported by additional components that identify the most relevant columns and\nanalyze their data types to improve extraction accuracy. In the event that the\ngenerated code fails, an iterative refinement process is triggered,\nincorporating the error feedback into a new generation prompt to enhance\nrobustness. Our results show that zero-shot code generation is a valid approach\nfor Tabular QA, achieving rank 33 of 53 in the test phase despite the lack of\ntask-specific fine-tuning.\n", "link": "http://arxiv.org/abs/2508.09012v1", "date": "2025-08-12", "relevancy": 2.1153, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4336}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4234}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LyS%20at%20SemEval%202025%20Task%208%3A%20Zero-Shot%20Code%20Generation%20for%20Tabular%20QA&body=Title%3A%20LyS%20at%20SemEval%202025%20Task%208%3A%20Zero-Shot%20Code%20Generation%20for%20Tabular%20QA%0AAuthor%3A%20Adri%C3%A1n%20Gude%20and%20Roi%20Santos-R%C3%ADos%20and%20Francisco%20Prado-Vali%C3%B1o%20and%20Ana%20Ezquerro%20and%20Jes%C3%BAs%20Vilares%0AAbstract%3A%20%20%20This%20paper%20describes%20our%20participation%20in%20SemEval%202025%20Task%208%2C%20focused%20on%0ATabular%20Question%20Answering.%20We%20developed%20a%20zero-shot%20pipeline%20that%20leverages%20an%0ALarge%20Language%20Model%20to%20generate%20functional%20code%20capable%20of%20extracting%20the%0Arelevant%20information%20from%20tabular%20data%20based%20on%20an%20input%20question.%20Our%20approach%0Aconsists%20of%20a%20modular%20pipeline%20where%20the%20main%20code%20generator%20module%20is%0Asupported%20by%20additional%20components%20that%20identify%20the%20most%20relevant%20columns%20and%0Aanalyze%20their%20data%20types%20to%20improve%20extraction%20accuracy.%20In%20the%20event%20that%20the%0Agenerated%20code%20fails%2C%20an%20iterative%20refinement%20process%20is%20triggered%2C%0Aincorporating%20the%20error%20feedback%20into%20a%20new%20generation%20prompt%20to%20enhance%0Arobustness.%20Our%20results%20show%20that%20zero-shot%20code%20generation%20is%20a%20valid%20approach%0Afor%20Tabular%20QA%2C%20achieving%20rank%2033%20of%2053%20in%20the%20test%20phase%20despite%20the%20lack%20of%0Atask-specific%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLyS%2520at%2520SemEval%25202025%2520Task%25208%253A%2520Zero-Shot%2520Code%2520Generation%2520for%2520Tabular%2520QA%26entry.906535625%3DAdri%25C3%25A1n%2520Gude%2520and%2520Roi%2520Santos-R%25C3%25ADos%2520and%2520Francisco%2520Prado-Vali%25C3%25B1o%2520and%2520Ana%2520Ezquerro%2520and%2520Jes%25C3%25BAs%2520Vilares%26entry.1292438233%3D%2520%2520This%2520paper%2520describes%2520our%2520participation%2520in%2520SemEval%25202025%2520Task%25208%252C%2520focused%2520on%250ATabular%2520Question%2520Answering.%2520We%2520developed%2520a%2520zero-shot%2520pipeline%2520that%2520leverages%2520an%250ALarge%2520Language%2520Model%2520to%2520generate%2520functional%2520code%2520capable%2520of%2520extracting%2520the%250Arelevant%2520information%2520from%2520tabular%2520data%2520based%2520on%2520an%2520input%2520question.%2520Our%2520approach%250Aconsists%2520of%2520a%2520modular%2520pipeline%2520where%2520the%2520main%2520code%2520generator%2520module%2520is%250Asupported%2520by%2520additional%2520components%2520that%2520identify%2520the%2520most%2520relevant%2520columns%2520and%250Aanalyze%2520their%2520data%2520types%2520to%2520improve%2520extraction%2520accuracy.%2520In%2520the%2520event%2520that%2520the%250Agenerated%2520code%2520fails%252C%2520an%2520iterative%2520refinement%2520process%2520is%2520triggered%252C%250Aincorporating%2520the%2520error%2520feedback%2520into%2520a%2520new%2520generation%2520prompt%2520to%2520enhance%250Arobustness.%2520Our%2520results%2520show%2520that%2520zero-shot%2520code%2520generation%2520is%2520a%2520valid%2520approach%250Afor%2520Tabular%2520QA%252C%2520achieving%2520rank%252033%2520of%252053%2520in%2520the%2520test%2520phase%2520despite%2520the%2520lack%2520of%250Atask-specific%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LyS%20at%20SemEval%202025%20Task%208%3A%20Zero-Shot%20Code%20Generation%20for%20Tabular%20QA&entry.906535625=Adri%C3%A1n%20Gude%20and%20Roi%20Santos-R%C3%ADos%20and%20Francisco%20Prado-Vali%C3%B1o%20and%20Ana%20Ezquerro%20and%20Jes%C3%BAs%20Vilares&entry.1292438233=%20%20This%20paper%20describes%20our%20participation%20in%20SemEval%202025%20Task%208%2C%20focused%20on%0ATabular%20Question%20Answering.%20We%20developed%20a%20zero-shot%20pipeline%20that%20leverages%20an%0ALarge%20Language%20Model%20to%20generate%20functional%20code%20capable%20of%20extracting%20the%0Arelevant%20information%20from%20tabular%20data%20based%20on%20an%20input%20question.%20Our%20approach%0Aconsists%20of%20a%20modular%20pipeline%20where%20the%20main%20code%20generator%20module%20is%0Asupported%20by%20additional%20components%20that%20identify%20the%20most%20relevant%20columns%20and%0Aanalyze%20their%20data%20types%20to%20improve%20extraction%20accuracy.%20In%20the%20event%20that%20the%0Agenerated%20code%20fails%2C%20an%20iterative%20refinement%20process%20is%20triggered%2C%0Aincorporating%20the%20error%20feedback%20into%20a%20new%20generation%20prompt%20to%20enhance%0Arobustness.%20Our%20results%20show%20that%20zero-shot%20code%20generation%20is%20a%20valid%20approach%0Afor%20Tabular%20QA%2C%20achieving%20rank%2033%20of%2053%20in%20the%20test%20phase%20despite%20the%20lack%20of%0Atask-specific%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09012v1&entry.124074799=Read"},
{"title": "Deep Neural Network Calibration by Reducing Classifier Shift with\n  Stochastic Masking", "author": "Jiani Ni and He Zhao and Yibo Yang and Dandan Guo", "abstract": "  In recent years, deep neural networks (DNNs) have shown competitive results\nin many fields. Despite this success, they often suffer from poor calibration,\nespecially in safety-critical scenarios such as autonomous driving and\nhealthcare, where unreliable confidence estimates can lead to serious\nconsequences. Recent studies have focused on improving calibration by modifying\nthe classifier, yet such efforts remain limited. Moreover, most existing\napproaches overlook calibration errors caused by underconfidence, which can be\nequally detrimental. To address these challenges, we propose MaC-Cal, a novel\nmask-based classifier calibration method that leverages stochastic sparsity to\nenhance the alignment between confidence and accuracy. MaC-Cal adopts a\ntwo-stage training scheme with adaptive sparsity, dynamically adjusting mask\nretention rates based on the deviation between confidence and accuracy.\nExtensive experiments show that MaC-Cal achieves superior calibration\nperformance and robustness under data corruption, offering a practical and\neffective solution for reliable confidence estimation in DNNs.\n", "link": "http://arxiv.org/abs/2508.09116v1", "date": "2025-08-12", "relevancy": 2.099, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.54}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5151}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Neural%20Network%20Calibration%20by%20Reducing%20Classifier%20Shift%20with%0A%20%20Stochastic%20Masking&body=Title%3A%20Deep%20Neural%20Network%20Calibration%20by%20Reducing%20Classifier%20Shift%20with%0A%20%20Stochastic%20Masking%0AAuthor%3A%20Jiani%20Ni%20and%20He%20Zhao%20and%20Yibo%20Yang%20and%20Dandan%20Guo%0AAbstract%3A%20%20%20In%20recent%20years%2C%20deep%20neural%20networks%20%28DNNs%29%20have%20shown%20competitive%20results%0Ain%20many%20fields.%20Despite%20this%20success%2C%20they%20often%20suffer%20from%20poor%20calibration%2C%0Aespecially%20in%20safety-critical%20scenarios%20such%20as%20autonomous%20driving%20and%0Ahealthcare%2C%20where%20unreliable%20confidence%20estimates%20can%20lead%20to%20serious%0Aconsequences.%20Recent%20studies%20have%20focused%20on%20improving%20calibration%20by%20modifying%0Athe%20classifier%2C%20yet%20such%20efforts%20remain%20limited.%20Moreover%2C%20most%20existing%0Aapproaches%20overlook%20calibration%20errors%20caused%20by%20underconfidence%2C%20which%20can%20be%0Aequally%20detrimental.%20To%20address%20these%20challenges%2C%20we%20propose%20MaC-Cal%2C%20a%20novel%0Amask-based%20classifier%20calibration%20method%20that%20leverages%20stochastic%20sparsity%20to%0Aenhance%20the%20alignment%20between%20confidence%20and%20accuracy.%20MaC-Cal%20adopts%20a%0Atwo-stage%20training%20scheme%20with%20adaptive%20sparsity%2C%20dynamically%20adjusting%20mask%0Aretention%20rates%20based%20on%20the%20deviation%20between%20confidence%20and%20accuracy.%0AExtensive%20experiments%20show%20that%20MaC-Cal%20achieves%20superior%20calibration%0Aperformance%20and%20robustness%20under%20data%20corruption%2C%20offering%20a%20practical%20and%0Aeffective%20solution%20for%20reliable%20confidence%20estimation%20in%20DNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Neural%2520Network%2520Calibration%2520by%2520Reducing%2520Classifier%2520Shift%2520with%250A%2520%2520Stochastic%2520Masking%26entry.906535625%3DJiani%2520Ni%2520and%2520He%2520Zhao%2520and%2520Yibo%2520Yang%2520and%2520Dandan%2520Guo%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520have%2520shown%2520competitive%2520results%250Ain%2520many%2520fields.%2520Despite%2520this%2520success%252C%2520they%2520often%2520suffer%2520from%2520poor%2520calibration%252C%250Aespecially%2520in%2520safety-critical%2520scenarios%2520such%2520as%2520autonomous%2520driving%2520and%250Ahealthcare%252C%2520where%2520unreliable%2520confidence%2520estimates%2520can%2520lead%2520to%2520serious%250Aconsequences.%2520Recent%2520studies%2520have%2520focused%2520on%2520improving%2520calibration%2520by%2520modifying%250Athe%2520classifier%252C%2520yet%2520such%2520efforts%2520remain%2520limited.%2520Moreover%252C%2520most%2520existing%250Aapproaches%2520overlook%2520calibration%2520errors%2520caused%2520by%2520underconfidence%252C%2520which%2520can%2520be%250Aequally%2520detrimental.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520MaC-Cal%252C%2520a%2520novel%250Amask-based%2520classifier%2520calibration%2520method%2520that%2520leverages%2520stochastic%2520sparsity%2520to%250Aenhance%2520the%2520alignment%2520between%2520confidence%2520and%2520accuracy.%2520MaC-Cal%2520adopts%2520a%250Atwo-stage%2520training%2520scheme%2520with%2520adaptive%2520sparsity%252C%2520dynamically%2520adjusting%2520mask%250Aretention%2520rates%2520based%2520on%2520the%2520deviation%2520between%2520confidence%2520and%2520accuracy.%250AExtensive%2520experiments%2520show%2520that%2520MaC-Cal%2520achieves%2520superior%2520calibration%250Aperformance%2520and%2520robustness%2520under%2520data%2520corruption%252C%2520offering%2520a%2520practical%2520and%250Aeffective%2520solution%2520for%2520reliable%2520confidence%2520estimation%2520in%2520DNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Neural%20Network%20Calibration%20by%20Reducing%20Classifier%20Shift%20with%0A%20%20Stochastic%20Masking&entry.906535625=Jiani%20Ni%20and%20He%20Zhao%20and%20Yibo%20Yang%20and%20Dandan%20Guo&entry.1292438233=%20%20In%20recent%20years%2C%20deep%20neural%20networks%20%28DNNs%29%20have%20shown%20competitive%20results%0Ain%20many%20fields.%20Despite%20this%20success%2C%20they%20often%20suffer%20from%20poor%20calibration%2C%0Aespecially%20in%20safety-critical%20scenarios%20such%20as%20autonomous%20driving%20and%0Ahealthcare%2C%20where%20unreliable%20confidence%20estimates%20can%20lead%20to%20serious%0Aconsequences.%20Recent%20studies%20have%20focused%20on%20improving%20calibration%20by%20modifying%0Athe%20classifier%2C%20yet%20such%20efforts%20remain%20limited.%20Moreover%2C%20most%20existing%0Aapproaches%20overlook%20calibration%20errors%20caused%20by%20underconfidence%2C%20which%20can%20be%0Aequally%20detrimental.%20To%20address%20these%20challenges%2C%20we%20propose%20MaC-Cal%2C%20a%20novel%0Amask-based%20classifier%20calibration%20method%20that%20leverages%20stochastic%20sparsity%20to%0Aenhance%20the%20alignment%20between%20confidence%20and%20accuracy.%20MaC-Cal%20adopts%20a%0Atwo-stage%20training%20scheme%20with%20adaptive%20sparsity%2C%20dynamically%20adjusting%20mask%0Aretention%20rates%20based%20on%20the%20deviation%20between%20confidence%20and%20accuracy.%0AExtensive%20experiments%20show%20that%20MaC-Cal%20achieves%20superior%20calibration%0Aperformance%20and%20robustness%20under%20data%20corruption%2C%20offering%20a%20practical%20and%0Aeffective%20solution%20for%20reliable%20confidence%20estimation%20in%20DNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09116v1&entry.124074799=Read"},
{"title": "CulturalFrames: Assessing Cultural Expectation Alignment in\n  Text-to-Image Models and Evaluation Metrics", "author": "Shravan Nayak and Mehar Bhatia and Xiaofeng Zhang and Verena Rieser and Lisa Anne Hendricks and Sjoerd van Steenkiste and Yash Goyal and Karolina Sta\u0144czak and Aishwarya Agrawal", "abstract": "  The increasing ubiquity of text-to-image (T2I) models as tools for visual\ncontent generation raises concerns about their ability to accurately represent\ndiverse cultural contexts -- where missed cues can stereotype communities and\nundermine usability. In this work, we present the first study to systematically\nquantify the alignment of T2I models and evaluation metrics with respect to\nboth explicit (stated) as well as implicit (unstated, implied by the prompt's\ncultural context) cultural expectations. To this end, we introduce\nCulturalFrames, a novel benchmark designed for rigorous human evaluation of\ncultural representation in visual generations. Spanning 10 countries and 5\nsocio-cultural domains, CulturalFrames comprises 983 prompts, 3637\ncorresponding images generated by 4 state-of-the-art T2I models, and over 10k\ndetailed human annotations. We find that across models and countries, cultural\nexpectations are missed an average of 44% of the time. Among these failures,\nexplicit expectations are missed at a surprisingly high average rate of 68%,\nwhile implicit expectation failures are also significant, averaging 49%.\nFurthermore, we show that existing T2I evaluation metrics correlate poorly with\nhuman judgments of cultural alignment, irrespective of their internal\nreasoning. Collectively, our findings expose critical gaps, provide a concrete\ntestbed, and outline actionable directions for developing culturally informed\nT2I models and metrics that improve global usability.\n", "link": "http://arxiv.org/abs/2506.08835v2", "date": "2025-08-12", "relevancy": 2.0818, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5581}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5142}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CulturalFrames%3A%20Assessing%20Cultural%20Expectation%20Alignment%20in%0A%20%20Text-to-Image%20Models%20and%20Evaluation%20Metrics&body=Title%3A%20CulturalFrames%3A%20Assessing%20Cultural%20Expectation%20Alignment%20in%0A%20%20Text-to-Image%20Models%20and%20Evaluation%20Metrics%0AAuthor%3A%20Shravan%20Nayak%20and%20Mehar%20Bhatia%20and%20Xiaofeng%20Zhang%20and%20Verena%20Rieser%20and%20Lisa%20Anne%20Hendricks%20and%20Sjoerd%20van%20Steenkiste%20and%20Yash%20Goyal%20and%20Karolina%20Sta%C5%84czak%20and%20Aishwarya%20Agrawal%0AAbstract%3A%20%20%20The%20increasing%20ubiquity%20of%20text-to-image%20%28T2I%29%20models%20as%20tools%20for%20visual%0Acontent%20generation%20raises%20concerns%20about%20their%20ability%20to%20accurately%20represent%0Adiverse%20cultural%20contexts%20--%20where%20missed%20cues%20can%20stereotype%20communities%20and%0Aundermine%20usability.%20In%20this%20work%2C%20we%20present%20the%20first%20study%20to%20systematically%0Aquantify%20the%20alignment%20of%20T2I%20models%20and%20evaluation%20metrics%20with%20respect%20to%0Aboth%20explicit%20%28stated%29%20as%20well%20as%20implicit%20%28unstated%2C%20implied%20by%20the%20prompt%27s%0Acultural%20context%29%20cultural%20expectations.%20To%20this%20end%2C%20we%20introduce%0ACulturalFrames%2C%20a%20novel%20benchmark%20designed%20for%20rigorous%20human%20evaluation%20of%0Acultural%20representation%20in%20visual%20generations.%20Spanning%2010%20countries%20and%205%0Asocio-cultural%20domains%2C%20CulturalFrames%20comprises%20983%20prompts%2C%203637%0Acorresponding%20images%20generated%20by%204%20state-of-the-art%20T2I%20models%2C%20and%20over%2010k%0Adetailed%20human%20annotations.%20We%20find%20that%20across%20models%20and%20countries%2C%20cultural%0Aexpectations%20are%20missed%20an%20average%20of%2044%25%20of%20the%20time.%20Among%20these%20failures%2C%0Aexplicit%20expectations%20are%20missed%20at%20a%20surprisingly%20high%20average%20rate%20of%2068%25%2C%0Awhile%20implicit%20expectation%20failures%20are%20also%20significant%2C%20averaging%2049%25.%0AFurthermore%2C%20we%20show%20that%20existing%20T2I%20evaluation%20metrics%20correlate%20poorly%20with%0Ahuman%20judgments%20of%20cultural%20alignment%2C%20irrespective%20of%20their%20internal%0Areasoning.%20Collectively%2C%20our%20findings%20expose%20critical%20gaps%2C%20provide%20a%20concrete%0Atestbed%2C%20and%20outline%20actionable%20directions%20for%20developing%20culturally%20informed%0AT2I%20models%20and%20metrics%20that%20improve%20global%20usability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08835v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCulturalFrames%253A%2520Assessing%2520Cultural%2520Expectation%2520Alignment%2520in%250A%2520%2520Text-to-Image%2520Models%2520and%2520Evaluation%2520Metrics%26entry.906535625%3DShravan%2520Nayak%2520and%2520Mehar%2520Bhatia%2520and%2520Xiaofeng%2520Zhang%2520and%2520Verena%2520Rieser%2520and%2520Lisa%2520Anne%2520Hendricks%2520and%2520Sjoerd%2520van%2520Steenkiste%2520and%2520Yash%2520Goyal%2520and%2520Karolina%2520Sta%25C5%2584czak%2520and%2520Aishwarya%2520Agrawal%26entry.1292438233%3D%2520%2520The%2520increasing%2520ubiquity%2520of%2520text-to-image%2520%2528T2I%2529%2520models%2520as%2520tools%2520for%2520visual%250Acontent%2520generation%2520raises%2520concerns%2520about%2520their%2520ability%2520to%2520accurately%2520represent%250Adiverse%2520cultural%2520contexts%2520--%2520where%2520missed%2520cues%2520can%2520stereotype%2520communities%2520and%250Aundermine%2520usability.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%2520study%2520to%2520systematically%250Aquantify%2520the%2520alignment%2520of%2520T2I%2520models%2520and%2520evaluation%2520metrics%2520with%2520respect%2520to%250Aboth%2520explicit%2520%2528stated%2529%2520as%2520well%2520as%2520implicit%2520%2528unstated%252C%2520implied%2520by%2520the%2520prompt%2527s%250Acultural%2520context%2529%2520cultural%2520expectations.%2520To%2520this%2520end%252C%2520we%2520introduce%250ACulturalFrames%252C%2520a%2520novel%2520benchmark%2520designed%2520for%2520rigorous%2520human%2520evaluation%2520of%250Acultural%2520representation%2520in%2520visual%2520generations.%2520Spanning%252010%2520countries%2520and%25205%250Asocio-cultural%2520domains%252C%2520CulturalFrames%2520comprises%2520983%2520prompts%252C%25203637%250Acorresponding%2520images%2520generated%2520by%25204%2520state-of-the-art%2520T2I%2520models%252C%2520and%2520over%252010k%250Adetailed%2520human%2520annotations.%2520We%2520find%2520that%2520across%2520models%2520and%2520countries%252C%2520cultural%250Aexpectations%2520are%2520missed%2520an%2520average%2520of%252044%2525%2520of%2520the%2520time.%2520Among%2520these%2520failures%252C%250Aexplicit%2520expectations%2520are%2520missed%2520at%2520a%2520surprisingly%2520high%2520average%2520rate%2520of%252068%2525%252C%250Awhile%2520implicit%2520expectation%2520failures%2520are%2520also%2520significant%252C%2520averaging%252049%2525.%250AFurthermore%252C%2520we%2520show%2520that%2520existing%2520T2I%2520evaluation%2520metrics%2520correlate%2520poorly%2520with%250Ahuman%2520judgments%2520of%2520cultural%2520alignment%252C%2520irrespective%2520of%2520their%2520internal%250Areasoning.%2520Collectively%252C%2520our%2520findings%2520expose%2520critical%2520gaps%252C%2520provide%2520a%2520concrete%250Atestbed%252C%2520and%2520outline%2520actionable%2520directions%2520for%2520developing%2520culturally%2520informed%250AT2I%2520models%2520and%2520metrics%2520that%2520improve%2520global%2520usability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08835v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CulturalFrames%3A%20Assessing%20Cultural%20Expectation%20Alignment%20in%0A%20%20Text-to-Image%20Models%20and%20Evaluation%20Metrics&entry.906535625=Shravan%20Nayak%20and%20Mehar%20Bhatia%20and%20Xiaofeng%20Zhang%20and%20Verena%20Rieser%20and%20Lisa%20Anne%20Hendricks%20and%20Sjoerd%20van%20Steenkiste%20and%20Yash%20Goyal%20and%20Karolina%20Sta%C5%84czak%20and%20Aishwarya%20Agrawal&entry.1292438233=%20%20The%20increasing%20ubiquity%20of%20text-to-image%20%28T2I%29%20models%20as%20tools%20for%20visual%0Acontent%20generation%20raises%20concerns%20about%20their%20ability%20to%20accurately%20represent%0Adiverse%20cultural%20contexts%20--%20where%20missed%20cues%20can%20stereotype%20communities%20and%0Aundermine%20usability.%20In%20this%20work%2C%20we%20present%20the%20first%20study%20to%20systematically%0Aquantify%20the%20alignment%20of%20T2I%20models%20and%20evaluation%20metrics%20with%20respect%20to%0Aboth%20explicit%20%28stated%29%20as%20well%20as%20implicit%20%28unstated%2C%20implied%20by%20the%20prompt%27s%0Acultural%20context%29%20cultural%20expectations.%20To%20this%20end%2C%20we%20introduce%0ACulturalFrames%2C%20a%20novel%20benchmark%20designed%20for%20rigorous%20human%20evaluation%20of%0Acultural%20representation%20in%20visual%20generations.%20Spanning%2010%20countries%20and%205%0Asocio-cultural%20domains%2C%20CulturalFrames%20comprises%20983%20prompts%2C%203637%0Acorresponding%20images%20generated%20by%204%20state-of-the-art%20T2I%20models%2C%20and%20over%2010k%0Adetailed%20human%20annotations.%20We%20find%20that%20across%20models%20and%20countries%2C%20cultural%0Aexpectations%20are%20missed%20an%20average%20of%2044%25%20of%20the%20time.%20Among%20these%20failures%2C%0Aexplicit%20expectations%20are%20missed%20at%20a%20surprisingly%20high%20average%20rate%20of%2068%25%2C%0Awhile%20implicit%20expectation%20failures%20are%20also%20significant%2C%20averaging%2049%25.%0AFurthermore%2C%20we%20show%20that%20existing%20T2I%20evaluation%20metrics%20correlate%20poorly%20with%0Ahuman%20judgments%20of%20cultural%20alignment%2C%20irrespective%20of%20their%20internal%0Areasoning.%20Collectively%2C%20our%20findings%20expose%20critical%20gaps%2C%20provide%20a%20concrete%0Atestbed%2C%20and%20outline%20actionable%20directions%20for%20developing%20culturally%20informed%0AT2I%20models%20and%20metrics%20that%20improve%20global%20usability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08835v2&entry.124074799=Read"},
{"title": "VertexRegen: Mesh Generation with Continuous Level of Detail", "author": "Xiang Zhang and Yawar Siddiqui and Armen Avetisyan and Chris Xie and Jakob Engel and Henry Howard-Jenkins", "abstract": "  We introduce VertexRegen, a novel mesh generation framework that enables\ngeneration at a continuous level of detail. Existing autoregressive methods\ngenerate meshes in a partial-to-complete manner and thus intermediate steps of\ngeneration represent incomplete structures. VertexRegen takes inspiration from\nprogressive meshes and reformulates the process as the reversal of edge\ncollapse, i.e. vertex split, learned through a generative model. Experimental\nresults demonstrate that VertexRegen produces meshes of comparable quality to\nstate-of-the-art methods while uniquely offering anytime generation with the\nflexibility to halt at any step to yield valid meshes with varying levels of\ndetail.\n", "link": "http://arxiv.org/abs/2508.09062v1", "date": "2025-08-12", "relevancy": 2.0802, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5647}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4992}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VertexRegen%3A%20Mesh%20Generation%20with%20Continuous%20Level%20of%20Detail&body=Title%3A%20VertexRegen%3A%20Mesh%20Generation%20with%20Continuous%20Level%20of%20Detail%0AAuthor%3A%20Xiang%20Zhang%20and%20Yawar%20Siddiqui%20and%20Armen%20Avetisyan%20and%20Chris%20Xie%20and%20Jakob%20Engel%20and%20Henry%20Howard-Jenkins%0AAbstract%3A%20%20%20We%20introduce%20VertexRegen%2C%20a%20novel%20mesh%20generation%20framework%20that%20enables%0Ageneration%20at%20a%20continuous%20level%20of%20detail.%20Existing%20autoregressive%20methods%0Agenerate%20meshes%20in%20a%20partial-to-complete%20manner%20and%20thus%20intermediate%20steps%20of%0Ageneration%20represent%20incomplete%20structures.%20VertexRegen%20takes%20inspiration%20from%0Aprogressive%20meshes%20and%20reformulates%20the%20process%20as%20the%20reversal%20of%20edge%0Acollapse%2C%20i.e.%20vertex%20split%2C%20learned%20through%20a%20generative%20model.%20Experimental%0Aresults%20demonstrate%20that%20VertexRegen%20produces%20meshes%20of%20comparable%20quality%20to%0Astate-of-the-art%20methods%20while%20uniquely%20offering%20anytime%20generation%20with%20the%0Aflexibility%20to%20halt%20at%20any%20step%20to%20yield%20valid%20meshes%20with%20varying%20levels%20of%0Adetail.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09062v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVertexRegen%253A%2520Mesh%2520Generation%2520with%2520Continuous%2520Level%2520of%2520Detail%26entry.906535625%3DXiang%2520Zhang%2520and%2520Yawar%2520Siddiqui%2520and%2520Armen%2520Avetisyan%2520and%2520Chris%2520Xie%2520and%2520Jakob%2520Engel%2520and%2520Henry%2520Howard-Jenkins%26entry.1292438233%3D%2520%2520We%2520introduce%2520VertexRegen%252C%2520a%2520novel%2520mesh%2520generation%2520framework%2520that%2520enables%250Ageneration%2520at%2520a%2520continuous%2520level%2520of%2520detail.%2520Existing%2520autoregressive%2520methods%250Agenerate%2520meshes%2520in%2520a%2520partial-to-complete%2520manner%2520and%2520thus%2520intermediate%2520steps%2520of%250Ageneration%2520represent%2520incomplete%2520structures.%2520VertexRegen%2520takes%2520inspiration%2520from%250Aprogressive%2520meshes%2520and%2520reformulates%2520the%2520process%2520as%2520the%2520reversal%2520of%2520edge%250Acollapse%252C%2520i.e.%2520vertex%2520split%252C%2520learned%2520through%2520a%2520generative%2520model.%2520Experimental%250Aresults%2520demonstrate%2520that%2520VertexRegen%2520produces%2520meshes%2520of%2520comparable%2520quality%2520to%250Astate-of-the-art%2520methods%2520while%2520uniquely%2520offering%2520anytime%2520generation%2520with%2520the%250Aflexibility%2520to%2520halt%2520at%2520any%2520step%2520to%2520yield%2520valid%2520meshes%2520with%2520varying%2520levels%2520of%250Adetail.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09062v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VertexRegen%3A%20Mesh%20Generation%20with%20Continuous%20Level%20of%20Detail&entry.906535625=Xiang%20Zhang%20and%20Yawar%20Siddiqui%20and%20Armen%20Avetisyan%20and%20Chris%20Xie%20and%20Jakob%20Engel%20and%20Henry%20Howard-Jenkins&entry.1292438233=%20%20We%20introduce%20VertexRegen%2C%20a%20novel%20mesh%20generation%20framework%20that%20enables%0Ageneration%20at%20a%20continuous%20level%20of%20detail.%20Existing%20autoregressive%20methods%0Agenerate%20meshes%20in%20a%20partial-to-complete%20manner%20and%20thus%20intermediate%20steps%20of%0Ageneration%20represent%20incomplete%20structures.%20VertexRegen%20takes%20inspiration%20from%0Aprogressive%20meshes%20and%20reformulates%20the%20process%20as%20the%20reversal%20of%20edge%0Acollapse%2C%20i.e.%20vertex%20split%2C%20learned%20through%20a%20generative%20model.%20Experimental%0Aresults%20demonstrate%20that%20VertexRegen%20produces%20meshes%20of%20comparable%20quality%20to%0Astate-of-the-art%20methods%20while%20uniquely%20offering%20anytime%20generation%20with%20the%0Aflexibility%20to%20halt%20at%20any%20step%20to%20yield%20valid%20meshes%20with%20varying%20levels%20of%0Adetail.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09062v1&entry.124074799=Read"},
{"title": "Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through\n  Structured Contextual Memory", "author": "Sizhe Yuen and Francisco Gomez Medina and Ting Su and Yali Du and Adam J. Sobey", "abstract": "  Multi-agent systems built on Large Language Models (LLMs) show exceptional\npromise for complex collaborative problem-solving, yet they face fundamental\nchallenges stemming from context window limitations that impair memory\nconsistency, role adherence, and procedural integrity. This paper introduces\nIntrinsic Memory Agents, a novel framework that addresses these limitations\nthrough structured agent-specific memories that evolve intrinsically with agent\noutputs. Specifically, our method maintains role-aligned memory templates that\npreserve specialized perspectives while focusing on task-relevant information.\nWe benchmark our approach on the PDDL dataset, comparing its performance to\nexisting state-of-the-art multi-agentic memory approaches and showing an\nimprovement of 38.6\\% with the highest token efficiency. An additional\nevaluation is performed on a complex data pipeline design task, we demonstrate\nthat our approach produces higher quality designs when comparing 5 metrics:\nscalability, reliability, usability, cost-effectiveness and documentation with\nadditional qualitative evidence of the improvements. Our findings suggest that\naddressing memory limitations through structured, intrinsic approaches can\nimprove the capabilities of multi-agent LLM systems on structured planning\ntasks.\n", "link": "http://arxiv.org/abs/2508.08997v1", "date": "2025-08-12", "relevancy": 2.0725, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5371}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5192}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intrinsic%20Memory%20Agents%3A%20Heterogeneous%20Multi-Agent%20LLM%20Systems%20through%0A%20%20Structured%20Contextual%20Memory&body=Title%3A%20Intrinsic%20Memory%20Agents%3A%20Heterogeneous%20Multi-Agent%20LLM%20Systems%20through%0A%20%20Structured%20Contextual%20Memory%0AAuthor%3A%20Sizhe%20Yuen%20and%20Francisco%20Gomez%20Medina%20and%20Ting%20Su%20and%20Yali%20Du%20and%20Adam%20J.%20Sobey%0AAbstract%3A%20%20%20Multi-agent%20systems%20built%20on%20Large%20Language%20Models%20%28LLMs%29%20show%20exceptional%0Apromise%20for%20complex%20collaborative%20problem-solving%2C%20yet%20they%20face%20fundamental%0Achallenges%20stemming%20from%20context%20window%20limitations%20that%20impair%20memory%0Aconsistency%2C%20role%20adherence%2C%20and%20procedural%20integrity.%20This%20paper%20introduces%0AIntrinsic%20Memory%20Agents%2C%20a%20novel%20framework%20that%20addresses%20these%20limitations%0Athrough%20structured%20agent-specific%20memories%20that%20evolve%20intrinsically%20with%20agent%0Aoutputs.%20Specifically%2C%20our%20method%20maintains%20role-aligned%20memory%20templates%20that%0Apreserve%20specialized%20perspectives%20while%20focusing%20on%20task-relevant%20information.%0AWe%20benchmark%20our%20approach%20on%20the%20PDDL%20dataset%2C%20comparing%20its%20performance%20to%0Aexisting%20state-of-the-art%20multi-agentic%20memory%20approaches%20and%20showing%20an%0Aimprovement%20of%2038.6%5C%25%20with%20the%20highest%20token%20efficiency.%20An%20additional%0Aevaluation%20is%20performed%20on%20a%20complex%20data%20pipeline%20design%20task%2C%20we%20demonstrate%0Athat%20our%20approach%20produces%20higher%20quality%20designs%20when%20comparing%205%20metrics%3A%0Ascalability%2C%20reliability%2C%20usability%2C%20cost-effectiveness%20and%20documentation%20with%0Aadditional%20qualitative%20evidence%20of%20the%20improvements.%20Our%20findings%20suggest%20that%0Aaddressing%20memory%20limitations%20through%20structured%2C%20intrinsic%20approaches%20can%0Aimprove%20the%20capabilities%20of%20multi-agent%20LLM%20systems%20on%20structured%20planning%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntrinsic%2520Memory%2520Agents%253A%2520Heterogeneous%2520Multi-Agent%2520LLM%2520Systems%2520through%250A%2520%2520Structured%2520Contextual%2520Memory%26entry.906535625%3DSizhe%2520Yuen%2520and%2520Francisco%2520Gomez%2520Medina%2520and%2520Ting%2520Su%2520and%2520Yali%2520Du%2520and%2520Adam%2520J.%2520Sobey%26entry.1292438233%3D%2520%2520Multi-agent%2520systems%2520built%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520show%2520exceptional%250Apromise%2520for%2520complex%2520collaborative%2520problem-solving%252C%2520yet%2520they%2520face%2520fundamental%250Achallenges%2520stemming%2520from%2520context%2520window%2520limitations%2520that%2520impair%2520memory%250Aconsistency%252C%2520role%2520adherence%252C%2520and%2520procedural%2520integrity.%2520This%2520paper%2520introduces%250AIntrinsic%2520Memory%2520Agents%252C%2520a%2520novel%2520framework%2520that%2520addresses%2520these%2520limitations%250Athrough%2520structured%2520agent-specific%2520memories%2520that%2520evolve%2520intrinsically%2520with%2520agent%250Aoutputs.%2520Specifically%252C%2520our%2520method%2520maintains%2520role-aligned%2520memory%2520templates%2520that%250Apreserve%2520specialized%2520perspectives%2520while%2520focusing%2520on%2520task-relevant%2520information.%250AWe%2520benchmark%2520our%2520approach%2520on%2520the%2520PDDL%2520dataset%252C%2520comparing%2520its%2520performance%2520to%250Aexisting%2520state-of-the-art%2520multi-agentic%2520memory%2520approaches%2520and%2520showing%2520an%250Aimprovement%2520of%252038.6%255C%2525%2520with%2520the%2520highest%2520token%2520efficiency.%2520An%2520additional%250Aevaluation%2520is%2520performed%2520on%2520a%2520complex%2520data%2520pipeline%2520design%2520task%252C%2520we%2520demonstrate%250Athat%2520our%2520approach%2520produces%2520higher%2520quality%2520designs%2520when%2520comparing%25205%2520metrics%253A%250Ascalability%252C%2520reliability%252C%2520usability%252C%2520cost-effectiveness%2520and%2520documentation%2520with%250Aadditional%2520qualitative%2520evidence%2520of%2520the%2520improvements.%2520Our%2520findings%2520suggest%2520that%250Aaddressing%2520memory%2520limitations%2520through%2520structured%252C%2520intrinsic%2520approaches%2520can%250Aimprove%2520the%2520capabilities%2520of%2520multi-agent%2520LLM%2520systems%2520on%2520structured%2520planning%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intrinsic%20Memory%20Agents%3A%20Heterogeneous%20Multi-Agent%20LLM%20Systems%20through%0A%20%20Structured%20Contextual%20Memory&entry.906535625=Sizhe%20Yuen%20and%20Francisco%20Gomez%20Medina%20and%20Ting%20Su%20and%20Yali%20Du%20and%20Adam%20J.%20Sobey&entry.1292438233=%20%20Multi-agent%20systems%20built%20on%20Large%20Language%20Models%20%28LLMs%29%20show%20exceptional%0Apromise%20for%20complex%20collaborative%20problem-solving%2C%20yet%20they%20face%20fundamental%0Achallenges%20stemming%20from%20context%20window%20limitations%20that%20impair%20memory%0Aconsistency%2C%20role%20adherence%2C%20and%20procedural%20integrity.%20This%20paper%20introduces%0AIntrinsic%20Memory%20Agents%2C%20a%20novel%20framework%20that%20addresses%20these%20limitations%0Athrough%20structured%20agent-specific%20memories%20that%20evolve%20intrinsically%20with%20agent%0Aoutputs.%20Specifically%2C%20our%20method%20maintains%20role-aligned%20memory%20templates%20that%0Apreserve%20specialized%20perspectives%20while%20focusing%20on%20task-relevant%20information.%0AWe%20benchmark%20our%20approach%20on%20the%20PDDL%20dataset%2C%20comparing%20its%20performance%20to%0Aexisting%20state-of-the-art%20multi-agentic%20memory%20approaches%20and%20showing%20an%0Aimprovement%20of%2038.6%5C%25%20with%20the%20highest%20token%20efficiency.%20An%20additional%0Aevaluation%20is%20performed%20on%20a%20complex%20data%20pipeline%20design%20task%2C%20we%20demonstrate%0Athat%20our%20approach%20produces%20higher%20quality%20designs%20when%20comparing%205%20metrics%3A%0Ascalability%2C%20reliability%2C%20usability%2C%20cost-effectiveness%20and%20documentation%20with%0Aadditional%20qualitative%20evidence%20of%20the%20improvements.%20Our%20findings%20suggest%20that%0Aaddressing%20memory%20limitations%20through%20structured%2C%20intrinsic%20approaches%20can%0Aimprove%20the%20capabilities%20of%20multi-agent%20LLM%20systems%20on%20structured%20planning%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08997v1&entry.124074799=Read"},
{"title": "Joint State and Noise Covariance Estimation", "author": "Kasra Khosoussi and Iman Shames", "abstract": "  This paper tackles the problem of jointly estimating the noise covariance\nmatrix alongside states (parameters such as poses and points) from measurements\ncorrupted by Gaussian noise and, if available, prior information. In such\nsettings, the noise covariance matrix determines the weights assigned to\nindividual measurements in the least squares problem. We show that the joint\nproblem exhibits a convex structure and provide a full characterization of the\noptimal noise covariance estimate (with analytical solutions) within joint\nmaximum a posteriori and likelihood frameworks and several variants. Leveraging\nthis theoretical result, we propose two novel algorithms that jointly estimate\nthe primary parameters and the noise covariance matrix. Our BCD algorithm can\nbe easily integrated into existing nonlinear least squares solvers, with\nnegligible per-iteration computational overhead. To validate our approach, we\nconduct extensive experiments across diverse scenarios and offer practical\ninsights into their application in robotics and computer vision estimation\nproblems with a particular focus on SLAM.\n", "link": "http://arxiv.org/abs/2502.04584v3", "date": "2025-08-12", "relevancy": 2.0706, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5852}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5212}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20State%20and%20Noise%20Covariance%20Estimation&body=Title%3A%20Joint%20State%20and%20Noise%20Covariance%20Estimation%0AAuthor%3A%20Kasra%20Khosoussi%20and%20Iman%20Shames%0AAbstract%3A%20%20%20This%20paper%20tackles%20the%20problem%20of%20jointly%20estimating%20the%20noise%20covariance%0Amatrix%20alongside%20states%20%28parameters%20such%20as%20poses%20and%20points%29%20from%20measurements%0Acorrupted%20by%20Gaussian%20noise%20and%2C%20if%20available%2C%20prior%20information.%20In%20such%0Asettings%2C%20the%20noise%20covariance%20matrix%20determines%20the%20weights%20assigned%20to%0Aindividual%20measurements%20in%20the%20least%20squares%20problem.%20We%20show%20that%20the%20joint%0Aproblem%20exhibits%20a%20convex%20structure%20and%20provide%20a%20full%20characterization%20of%20the%0Aoptimal%20noise%20covariance%20estimate%20%28with%20analytical%20solutions%29%20within%20joint%0Amaximum%20a%20posteriori%20and%20likelihood%20frameworks%20and%20several%20variants.%20Leveraging%0Athis%20theoretical%20result%2C%20we%20propose%20two%20novel%20algorithms%20that%20jointly%20estimate%0Athe%20primary%20parameters%20and%20the%20noise%20covariance%20matrix.%20Our%20BCD%20algorithm%20can%0Abe%20easily%20integrated%20into%20existing%20nonlinear%20least%20squares%20solvers%2C%20with%0Anegligible%20per-iteration%20computational%20overhead.%20To%20validate%20our%20approach%2C%20we%0Aconduct%20extensive%20experiments%20across%20diverse%20scenarios%20and%20offer%20practical%0Ainsights%20into%20their%20application%20in%20robotics%20and%20computer%20vision%20estimation%0Aproblems%20with%20a%20particular%20focus%20on%20SLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04584v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520State%2520and%2520Noise%2520Covariance%2520Estimation%26entry.906535625%3DKasra%2520Khosoussi%2520and%2520Iman%2520Shames%26entry.1292438233%3D%2520%2520This%2520paper%2520tackles%2520the%2520problem%2520of%2520jointly%2520estimating%2520the%2520noise%2520covariance%250Amatrix%2520alongside%2520states%2520%2528parameters%2520such%2520as%2520poses%2520and%2520points%2529%2520from%2520measurements%250Acorrupted%2520by%2520Gaussian%2520noise%2520and%252C%2520if%2520available%252C%2520prior%2520information.%2520In%2520such%250Asettings%252C%2520the%2520noise%2520covariance%2520matrix%2520determines%2520the%2520weights%2520assigned%2520to%250Aindividual%2520measurements%2520in%2520the%2520least%2520squares%2520problem.%2520We%2520show%2520that%2520the%2520joint%250Aproblem%2520exhibits%2520a%2520convex%2520structure%2520and%2520provide%2520a%2520full%2520characterization%2520of%2520the%250Aoptimal%2520noise%2520covariance%2520estimate%2520%2528with%2520analytical%2520solutions%2529%2520within%2520joint%250Amaximum%2520a%2520posteriori%2520and%2520likelihood%2520frameworks%2520and%2520several%2520variants.%2520Leveraging%250Athis%2520theoretical%2520result%252C%2520we%2520propose%2520two%2520novel%2520algorithms%2520that%2520jointly%2520estimate%250Athe%2520primary%2520parameters%2520and%2520the%2520noise%2520covariance%2520matrix.%2520Our%2520BCD%2520algorithm%2520can%250Abe%2520easily%2520integrated%2520into%2520existing%2520nonlinear%2520least%2520squares%2520solvers%252C%2520with%250Anegligible%2520per-iteration%2520computational%2520overhead.%2520To%2520validate%2520our%2520approach%252C%2520we%250Aconduct%2520extensive%2520experiments%2520across%2520diverse%2520scenarios%2520and%2520offer%2520practical%250Ainsights%2520into%2520their%2520application%2520in%2520robotics%2520and%2520computer%2520vision%2520estimation%250Aproblems%2520with%2520a%2520particular%2520focus%2520on%2520SLAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04584v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20State%20and%20Noise%20Covariance%20Estimation&entry.906535625=Kasra%20Khosoussi%20and%20Iman%20Shames&entry.1292438233=%20%20This%20paper%20tackles%20the%20problem%20of%20jointly%20estimating%20the%20noise%20covariance%0Amatrix%20alongside%20states%20%28parameters%20such%20as%20poses%20and%20points%29%20from%20measurements%0Acorrupted%20by%20Gaussian%20noise%20and%2C%20if%20available%2C%20prior%20information.%20In%20such%0Asettings%2C%20the%20noise%20covariance%20matrix%20determines%20the%20weights%20assigned%20to%0Aindividual%20measurements%20in%20the%20least%20squares%20problem.%20We%20show%20that%20the%20joint%0Aproblem%20exhibits%20a%20convex%20structure%20and%20provide%20a%20full%20characterization%20of%20the%0Aoptimal%20noise%20covariance%20estimate%20%28with%20analytical%20solutions%29%20within%20joint%0Amaximum%20a%20posteriori%20and%20likelihood%20frameworks%20and%20several%20variants.%20Leveraging%0Athis%20theoretical%20result%2C%20we%20propose%20two%20novel%20algorithms%20that%20jointly%20estimate%0Athe%20primary%20parameters%20and%20the%20noise%20covariance%20matrix.%20Our%20BCD%20algorithm%20can%0Abe%20easily%20integrated%20into%20existing%20nonlinear%20least%20squares%20solvers%2C%20with%0Anegligible%20per-iteration%20computational%20overhead.%20To%20validate%20our%20approach%2C%20we%0Aconduct%20extensive%20experiments%20across%20diverse%20scenarios%20and%20offer%20practical%0Ainsights%20into%20their%20application%20in%20robotics%20and%20computer%20vision%20estimation%0Aproblems%20with%20a%20particular%20focus%20on%20SLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04584v3&entry.124074799=Read"},
{"title": "ColorGPT: Leveraging Large Language Models for Multimodal Color\n  Recommendation", "author": "Ding Xia and Naoto Inoue and Qianru Qiu and Kotaro Kikuchi", "abstract": "  Colors play a crucial role in the design of vector graphic documents by\nenhancing visual appeal, facilitating communication, improving usability, and\nensuring accessibility. In this context, color recommendation involves\nsuggesting appropriate colors to complete or refine a design when one or more\ncolors are missing or require alteration. Traditional methods often struggled\nwith these challenges due to the complex nature of color design and the limited\ndata availability. In this study, we explored the use of pretrained Large\nLanguage Models (LLMs) and their commonsense reasoning capabilities for color\nrecommendation, raising the question: Can pretrained LLMs serve as superior\ndesigners for color recommendation tasks? To investigate this, we developed a\nrobust, rigorously validated pipeline, ColorGPT, that was built by\nsystematically testing multiple color representations and applying effective\nprompt engineering techniques. Our approach primarily targeted color palette\ncompletion by recommending colors based on a set of given colors and\naccompanying context. Moreover, our method can be extended to full palette\ngeneration, producing an entire color palette corresponding to a provided\ntextual description. Experimental results demonstrated that our LLM-based\npipeline outperformed existing methods in terms of color suggestion accuracy\nand the distribution of colors in the color palette completion task. For the\nfull palette generation task, our approach also yielded improvements in color\ndiversity and similarity compared to current techniques.\n", "link": "http://arxiv.org/abs/2508.08987v1", "date": "2025-08-12", "relevancy": 2.0687, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5145}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ColorGPT%3A%20Leveraging%20Large%20Language%20Models%20for%20Multimodal%20Color%0A%20%20Recommendation&body=Title%3A%20ColorGPT%3A%20Leveraging%20Large%20Language%20Models%20for%20Multimodal%20Color%0A%20%20Recommendation%0AAuthor%3A%20Ding%20Xia%20and%20Naoto%20Inoue%20and%20Qianru%20Qiu%20and%20Kotaro%20Kikuchi%0AAbstract%3A%20%20%20Colors%20play%20a%20crucial%20role%20in%20the%20design%20of%20vector%20graphic%20documents%20by%0Aenhancing%20visual%20appeal%2C%20facilitating%20communication%2C%20improving%20usability%2C%20and%0Aensuring%20accessibility.%20In%20this%20context%2C%20color%20recommendation%20involves%0Asuggesting%20appropriate%20colors%20to%20complete%20or%20refine%20a%20design%20when%20one%20or%20more%0Acolors%20are%20missing%20or%20require%20alteration.%20Traditional%20methods%20often%20struggled%0Awith%20these%20challenges%20due%20to%20the%20complex%20nature%20of%20color%20design%20and%20the%20limited%0Adata%20availability.%20In%20this%20study%2C%20we%20explored%20the%20use%20of%20pretrained%20Large%0ALanguage%20Models%20%28LLMs%29%20and%20their%20commonsense%20reasoning%20capabilities%20for%20color%0Arecommendation%2C%20raising%20the%20question%3A%20Can%20pretrained%20LLMs%20serve%20as%20superior%0Adesigners%20for%20color%20recommendation%20tasks%3F%20To%20investigate%20this%2C%20we%20developed%20a%0Arobust%2C%20rigorously%20validated%20pipeline%2C%20ColorGPT%2C%20that%20was%20built%20by%0Asystematically%20testing%20multiple%20color%20representations%20and%20applying%20effective%0Aprompt%20engineering%20techniques.%20Our%20approach%20primarily%20targeted%20color%20palette%0Acompletion%20by%20recommending%20colors%20based%20on%20a%20set%20of%20given%20colors%20and%0Aaccompanying%20context.%20Moreover%2C%20our%20method%20can%20be%20extended%20to%20full%20palette%0Ageneration%2C%20producing%20an%20entire%20color%20palette%20corresponding%20to%20a%20provided%0Atextual%20description.%20Experimental%20results%20demonstrated%20that%20our%20LLM-based%0Apipeline%20outperformed%20existing%20methods%20in%20terms%20of%20color%20suggestion%20accuracy%0Aand%20the%20distribution%20of%20colors%20in%20the%20color%20palette%20completion%20task.%20For%20the%0Afull%20palette%20generation%20task%2C%20our%20approach%20also%20yielded%20improvements%20in%20color%0Adiversity%20and%20similarity%20compared%20to%20current%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColorGPT%253A%2520Leveraging%2520Large%2520Language%2520Models%2520for%2520Multimodal%2520Color%250A%2520%2520Recommendation%26entry.906535625%3DDing%2520Xia%2520and%2520Naoto%2520Inoue%2520and%2520Qianru%2520Qiu%2520and%2520Kotaro%2520Kikuchi%26entry.1292438233%3D%2520%2520Colors%2520play%2520a%2520crucial%2520role%2520in%2520the%2520design%2520of%2520vector%2520graphic%2520documents%2520by%250Aenhancing%2520visual%2520appeal%252C%2520facilitating%2520communication%252C%2520improving%2520usability%252C%2520and%250Aensuring%2520accessibility.%2520In%2520this%2520context%252C%2520color%2520recommendation%2520involves%250Asuggesting%2520appropriate%2520colors%2520to%2520complete%2520or%2520refine%2520a%2520design%2520when%2520one%2520or%2520more%250Acolors%2520are%2520missing%2520or%2520require%2520alteration.%2520Traditional%2520methods%2520often%2520struggled%250Awith%2520these%2520challenges%2520due%2520to%2520the%2520complex%2520nature%2520of%2520color%2520design%2520and%2520the%2520limited%250Adata%2520availability.%2520In%2520this%2520study%252C%2520we%2520explored%2520the%2520use%2520of%2520pretrained%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520and%2520their%2520commonsense%2520reasoning%2520capabilities%2520for%2520color%250Arecommendation%252C%2520raising%2520the%2520question%253A%2520Can%2520pretrained%2520LLMs%2520serve%2520as%2520superior%250Adesigners%2520for%2520color%2520recommendation%2520tasks%253F%2520To%2520investigate%2520this%252C%2520we%2520developed%2520a%250Arobust%252C%2520rigorously%2520validated%2520pipeline%252C%2520ColorGPT%252C%2520that%2520was%2520built%2520by%250Asystematically%2520testing%2520multiple%2520color%2520representations%2520and%2520applying%2520effective%250Aprompt%2520engineering%2520techniques.%2520Our%2520approach%2520primarily%2520targeted%2520color%2520palette%250Acompletion%2520by%2520recommending%2520colors%2520based%2520on%2520a%2520set%2520of%2520given%2520colors%2520and%250Aaccompanying%2520context.%2520Moreover%252C%2520our%2520method%2520can%2520be%2520extended%2520to%2520full%2520palette%250Ageneration%252C%2520producing%2520an%2520entire%2520color%2520palette%2520corresponding%2520to%2520a%2520provided%250Atextual%2520description.%2520Experimental%2520results%2520demonstrated%2520that%2520our%2520LLM-based%250Apipeline%2520outperformed%2520existing%2520methods%2520in%2520terms%2520of%2520color%2520suggestion%2520accuracy%250Aand%2520the%2520distribution%2520of%2520colors%2520in%2520the%2520color%2520palette%2520completion%2520task.%2520For%2520the%250Afull%2520palette%2520generation%2520task%252C%2520our%2520approach%2520also%2520yielded%2520improvements%2520in%2520color%250Adiversity%2520and%2520similarity%2520compared%2520to%2520current%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ColorGPT%3A%20Leveraging%20Large%20Language%20Models%20for%20Multimodal%20Color%0A%20%20Recommendation&entry.906535625=Ding%20Xia%20and%20Naoto%20Inoue%20and%20Qianru%20Qiu%20and%20Kotaro%20Kikuchi&entry.1292438233=%20%20Colors%20play%20a%20crucial%20role%20in%20the%20design%20of%20vector%20graphic%20documents%20by%0Aenhancing%20visual%20appeal%2C%20facilitating%20communication%2C%20improving%20usability%2C%20and%0Aensuring%20accessibility.%20In%20this%20context%2C%20color%20recommendation%20involves%0Asuggesting%20appropriate%20colors%20to%20complete%20or%20refine%20a%20design%20when%20one%20or%20more%0Acolors%20are%20missing%20or%20require%20alteration.%20Traditional%20methods%20often%20struggled%0Awith%20these%20challenges%20due%20to%20the%20complex%20nature%20of%20color%20design%20and%20the%20limited%0Adata%20availability.%20In%20this%20study%2C%20we%20explored%20the%20use%20of%20pretrained%20Large%0ALanguage%20Models%20%28LLMs%29%20and%20their%20commonsense%20reasoning%20capabilities%20for%20color%0Arecommendation%2C%20raising%20the%20question%3A%20Can%20pretrained%20LLMs%20serve%20as%20superior%0Adesigners%20for%20color%20recommendation%20tasks%3F%20To%20investigate%20this%2C%20we%20developed%20a%0Arobust%2C%20rigorously%20validated%20pipeline%2C%20ColorGPT%2C%20that%20was%20built%20by%0Asystematically%20testing%20multiple%20color%20representations%20and%20applying%20effective%0Aprompt%20engineering%20techniques.%20Our%20approach%20primarily%20targeted%20color%20palette%0Acompletion%20by%20recommending%20colors%20based%20on%20a%20set%20of%20given%20colors%20and%0Aaccompanying%20context.%20Moreover%2C%20our%20method%20can%20be%20extended%20to%20full%20palette%0Ageneration%2C%20producing%20an%20entire%20color%20palette%20corresponding%20to%20a%20provided%0Atextual%20description.%20Experimental%20results%20demonstrated%20that%20our%20LLM-based%0Apipeline%20outperformed%20existing%20methods%20in%20terms%20of%20color%20suggestion%20accuracy%0Aand%20the%20distribution%20of%20colors%20in%20the%20color%20palette%20completion%20task.%20For%20the%0Afull%20palette%20generation%20task%2C%20our%20approach%20also%20yielded%20improvements%20in%20color%0Adiversity%20and%20similarity%20compared%20to%20current%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08987v1&entry.124074799=Read"},
{"title": "Efficient motion-based metrics for video frame interpolation", "author": "Conall Daly and Darren Ramsook and Anil Kokaram", "abstract": "  Video frame interpolation (VFI) offers a way to generate intermediate frames\nbetween consecutive frames of a video sequence. Although the development of\nadvanced frame interpolation algorithms has received increased attention in\nrecent years, assessing the perceptual quality of interpolated content remains\nan ongoing area of research. In this paper, we investigate simple ways to\nprocess motion fields, with the purposes of using them as video quality metric\nfor evaluating frame interpolation algorithms. We evaluate these quality\nmetrics using the BVI-VFI dataset which contains perceptual scores measured for\ninterpolated sequences. From our investigation we propose a motion metric based\non measuring the divergence of motion fields. This metric correlates reasonably\nwith these perceptual scores (PLCC=0.51) and is more computationally efficient\n(x2.7 speedup) compared to FloLPIPS (a well known motion-based metric). We then\nuse our new proposed metrics to evaluate a range of state of the art frame\ninterpolation metrics and find our metrics tend to favour more perceptual\npleasing interpolated frames that may not score highly in terms of PSNR or\nSSIM.\n", "link": "http://arxiv.org/abs/2508.09078v1", "date": "2025-08-12", "relevancy": 2.0659, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5641}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5037}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20motion-based%20metrics%20for%20video%20frame%20interpolation&body=Title%3A%20Efficient%20motion-based%20metrics%20for%20video%20frame%20interpolation%0AAuthor%3A%20Conall%20Daly%20and%20Darren%20Ramsook%20and%20Anil%20Kokaram%0AAbstract%3A%20%20%20Video%20frame%20interpolation%20%28VFI%29%20offers%20a%20way%20to%20generate%20intermediate%20frames%0Abetween%20consecutive%20frames%20of%20a%20video%20sequence.%20Although%20the%20development%20of%0Aadvanced%20frame%20interpolation%20algorithms%20has%20received%20increased%20attention%20in%0Arecent%20years%2C%20assessing%20the%20perceptual%20quality%20of%20interpolated%20content%20remains%0Aan%20ongoing%20area%20of%20research.%20In%20this%20paper%2C%20we%20investigate%20simple%20ways%20to%0Aprocess%20motion%20fields%2C%20with%20the%20purposes%20of%20using%20them%20as%20video%20quality%20metric%0Afor%20evaluating%20frame%20interpolation%20algorithms.%20We%20evaluate%20these%20quality%0Ametrics%20using%20the%20BVI-VFI%20dataset%20which%20contains%20perceptual%20scores%20measured%20for%0Ainterpolated%20sequences.%20From%20our%20investigation%20we%20propose%20a%20motion%20metric%20based%0Aon%20measuring%20the%20divergence%20of%20motion%20fields.%20This%20metric%20correlates%20reasonably%0Awith%20these%20perceptual%20scores%20%28PLCC%3D0.51%29%20and%20is%20more%20computationally%20efficient%0A%28x2.7%20speedup%29%20compared%20to%20FloLPIPS%20%28a%20well%20known%20motion-based%20metric%29.%20We%20then%0Ause%20our%20new%20proposed%20metrics%20to%20evaluate%20a%20range%20of%20state%20of%20the%20art%20frame%0Ainterpolation%20metrics%20and%20find%20our%20metrics%20tend%20to%20favour%20more%20perceptual%0Apleasing%20interpolated%20frames%20that%20may%20not%20score%20highly%20in%20terms%20of%20PSNR%20or%0ASSIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520motion-based%2520metrics%2520for%2520video%2520frame%2520interpolation%26entry.906535625%3DConall%2520Daly%2520and%2520Darren%2520Ramsook%2520and%2520Anil%2520Kokaram%26entry.1292438233%3D%2520%2520Video%2520frame%2520interpolation%2520%2528VFI%2529%2520offers%2520a%2520way%2520to%2520generate%2520intermediate%2520frames%250Abetween%2520consecutive%2520frames%2520of%2520a%2520video%2520sequence.%2520Although%2520the%2520development%2520of%250Aadvanced%2520frame%2520interpolation%2520algorithms%2520has%2520received%2520increased%2520attention%2520in%250Arecent%2520years%252C%2520assessing%2520the%2520perceptual%2520quality%2520of%2520interpolated%2520content%2520remains%250Aan%2520ongoing%2520area%2520of%2520research.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520simple%2520ways%2520to%250Aprocess%2520motion%2520fields%252C%2520with%2520the%2520purposes%2520of%2520using%2520them%2520as%2520video%2520quality%2520metric%250Afor%2520evaluating%2520frame%2520interpolation%2520algorithms.%2520We%2520evaluate%2520these%2520quality%250Ametrics%2520using%2520the%2520BVI-VFI%2520dataset%2520which%2520contains%2520perceptual%2520scores%2520measured%2520for%250Ainterpolated%2520sequences.%2520From%2520our%2520investigation%2520we%2520propose%2520a%2520motion%2520metric%2520based%250Aon%2520measuring%2520the%2520divergence%2520of%2520motion%2520fields.%2520This%2520metric%2520correlates%2520reasonably%250Awith%2520these%2520perceptual%2520scores%2520%2528PLCC%253D0.51%2529%2520and%2520is%2520more%2520computationally%2520efficient%250A%2528x2.7%2520speedup%2529%2520compared%2520to%2520FloLPIPS%2520%2528a%2520well%2520known%2520motion-based%2520metric%2529.%2520We%2520then%250Ause%2520our%2520new%2520proposed%2520metrics%2520to%2520evaluate%2520a%2520range%2520of%2520state%2520of%2520the%2520art%2520frame%250Ainterpolation%2520metrics%2520and%2520find%2520our%2520metrics%2520tend%2520to%2520favour%2520more%2520perceptual%250Apleasing%2520interpolated%2520frames%2520that%2520may%2520not%2520score%2520highly%2520in%2520terms%2520of%2520PSNR%2520or%250ASSIM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20motion-based%20metrics%20for%20video%20frame%20interpolation&entry.906535625=Conall%20Daly%20and%20Darren%20Ramsook%20and%20Anil%20Kokaram&entry.1292438233=%20%20Video%20frame%20interpolation%20%28VFI%29%20offers%20a%20way%20to%20generate%20intermediate%20frames%0Abetween%20consecutive%20frames%20of%20a%20video%20sequence.%20Although%20the%20development%20of%0Aadvanced%20frame%20interpolation%20algorithms%20has%20received%20increased%20attention%20in%0Arecent%20years%2C%20assessing%20the%20perceptual%20quality%20of%20interpolated%20content%20remains%0Aan%20ongoing%20area%20of%20research.%20In%20this%20paper%2C%20we%20investigate%20simple%20ways%20to%0Aprocess%20motion%20fields%2C%20with%20the%20purposes%20of%20using%20them%20as%20video%20quality%20metric%0Afor%20evaluating%20frame%20interpolation%20algorithms.%20We%20evaluate%20these%20quality%0Ametrics%20using%20the%20BVI-VFI%20dataset%20which%20contains%20perceptual%20scores%20measured%20for%0Ainterpolated%20sequences.%20From%20our%20investigation%20we%20propose%20a%20motion%20metric%20based%0Aon%20measuring%20the%20divergence%20of%20motion%20fields.%20This%20metric%20correlates%20reasonably%0Awith%20these%20perceptual%20scores%20%28PLCC%3D0.51%29%20and%20is%20more%20computationally%20efficient%0A%28x2.7%20speedup%29%20compared%20to%20FloLPIPS%20%28a%20well%20known%20motion-based%20metric%29.%20We%20then%0Ause%20our%20new%20proposed%20metrics%20to%20evaluate%20a%20range%20of%20state%20of%20the%20art%20frame%0Ainterpolation%20metrics%20and%20find%20our%20metrics%20tend%20to%20favour%20more%20perceptual%0Apleasing%20interpolated%20frames%20that%20may%20not%20score%20highly%20in%20terms%20of%20PSNR%20or%0ASSIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09078v1&entry.124074799=Read"},
{"title": "BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented\n  Programmatic Agent Pair", "author": "Xianghe Pang and Shuo Tang and Rui Ye and Yuwen Du and Yaxin Du and Siheng Chen", "abstract": "  Effective information seeking in the vast and ever-growing digital landscape\nrequires balancing expansive search with strategic reasoning. Current large\nlanguage model (LLM)-based agents struggle to achieve this balance due to\nlimitations in search breadth and reasoning depth, where slow, serial querying\nrestricts coverage of relevant sources and noisy raw inputs disrupt the\ncontinuity of multi-step reasoning. To address these challenges, we propose\nBrowseMaster, a scalable framework built around a programmatically augmented\nplanner-executor agent pair. The planner formulates and adapts search\nstrategies based on task constraints, while the executor conducts efficient,\ntargeted retrieval to supply the planner with concise, relevant evidence. This\ndivision of labor preserves coherent, long-horizon reasoning while sustaining\nbroad and systematic exploration, overcoming the trade-off that limits existing\nagents. Extensive experiments on challenging English and Chinese benchmarks\nshow that BrowseMaster consistently outperforms open-source and proprietary\nbaselines, achieving scores of 30.0 on BrowseComp-en and 46.5 on BrowseComp-zh,\nwhich demonstrates its strong capability in complex, reasoning-heavy\ninformation-seeking tasks at scale.\n", "link": "http://arxiv.org/abs/2508.09129v1", "date": "2025-08-12", "relevancy": 2.0657, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5265}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5144}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BrowseMaster%3A%20Towards%20Scalable%20Web%20Browsing%20via%20Tool-Augmented%0A%20%20Programmatic%20Agent%20Pair&body=Title%3A%20BrowseMaster%3A%20Towards%20Scalable%20Web%20Browsing%20via%20Tool-Augmented%0A%20%20Programmatic%20Agent%20Pair%0AAuthor%3A%20Xianghe%20Pang%20and%20Shuo%20Tang%20and%20Rui%20Ye%20and%20Yuwen%20Du%20and%20Yaxin%20Du%20and%20Siheng%20Chen%0AAbstract%3A%20%20%20Effective%20information%20seeking%20in%20the%20vast%20and%20ever-growing%20digital%20landscape%0Arequires%20balancing%20expansive%20search%20with%20strategic%20reasoning.%20Current%20large%0Alanguage%20model%20%28LLM%29-based%20agents%20struggle%20to%20achieve%20this%20balance%20due%20to%0Alimitations%20in%20search%20breadth%20and%20reasoning%20depth%2C%20where%20slow%2C%20serial%20querying%0Arestricts%20coverage%20of%20relevant%20sources%20and%20noisy%20raw%20inputs%20disrupt%20the%0Acontinuity%20of%20multi-step%20reasoning.%20To%20address%20these%20challenges%2C%20we%20propose%0ABrowseMaster%2C%20a%20scalable%20framework%20built%20around%20a%20programmatically%20augmented%0Aplanner-executor%20agent%20pair.%20The%20planner%20formulates%20and%20adapts%20search%0Astrategies%20based%20on%20task%20constraints%2C%20while%20the%20executor%20conducts%20efficient%2C%0Atargeted%20retrieval%20to%20supply%20the%20planner%20with%20concise%2C%20relevant%20evidence.%20This%0Adivision%20of%20labor%20preserves%20coherent%2C%20long-horizon%20reasoning%20while%20sustaining%0Abroad%20and%20systematic%20exploration%2C%20overcoming%20the%20trade-off%20that%20limits%20existing%0Aagents.%20Extensive%20experiments%20on%20challenging%20English%20and%20Chinese%20benchmarks%0Ashow%20that%20BrowseMaster%20consistently%20outperforms%20open-source%20and%20proprietary%0Abaselines%2C%20achieving%20scores%20of%2030.0%20on%20BrowseComp-en%20and%2046.5%20on%20BrowseComp-zh%2C%0Awhich%20demonstrates%20its%20strong%20capability%20in%20complex%2C%20reasoning-heavy%0Ainformation-seeking%20tasks%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrowseMaster%253A%2520Towards%2520Scalable%2520Web%2520Browsing%2520via%2520Tool-Augmented%250A%2520%2520Programmatic%2520Agent%2520Pair%26entry.906535625%3DXianghe%2520Pang%2520and%2520Shuo%2520Tang%2520and%2520Rui%2520Ye%2520and%2520Yuwen%2520Du%2520and%2520Yaxin%2520Du%2520and%2520Siheng%2520Chen%26entry.1292438233%3D%2520%2520Effective%2520information%2520seeking%2520in%2520the%2520vast%2520and%2520ever-growing%2520digital%2520landscape%250Arequires%2520balancing%2520expansive%2520search%2520with%2520strategic%2520reasoning.%2520Current%2520large%250Alanguage%2520model%2520%2528LLM%2529-based%2520agents%2520struggle%2520to%2520achieve%2520this%2520balance%2520due%2520to%250Alimitations%2520in%2520search%2520breadth%2520and%2520reasoning%2520depth%252C%2520where%2520slow%252C%2520serial%2520querying%250Arestricts%2520coverage%2520of%2520relevant%2520sources%2520and%2520noisy%2520raw%2520inputs%2520disrupt%2520the%250Acontinuity%2520of%2520multi-step%2520reasoning.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250ABrowseMaster%252C%2520a%2520scalable%2520framework%2520built%2520around%2520a%2520programmatically%2520augmented%250Aplanner-executor%2520agent%2520pair.%2520The%2520planner%2520formulates%2520and%2520adapts%2520search%250Astrategies%2520based%2520on%2520task%2520constraints%252C%2520while%2520the%2520executor%2520conducts%2520efficient%252C%250Atargeted%2520retrieval%2520to%2520supply%2520the%2520planner%2520with%2520concise%252C%2520relevant%2520evidence.%2520This%250Adivision%2520of%2520labor%2520preserves%2520coherent%252C%2520long-horizon%2520reasoning%2520while%2520sustaining%250Abroad%2520and%2520systematic%2520exploration%252C%2520overcoming%2520the%2520trade-off%2520that%2520limits%2520existing%250Aagents.%2520Extensive%2520experiments%2520on%2520challenging%2520English%2520and%2520Chinese%2520benchmarks%250Ashow%2520that%2520BrowseMaster%2520consistently%2520outperforms%2520open-source%2520and%2520proprietary%250Abaselines%252C%2520achieving%2520scores%2520of%252030.0%2520on%2520BrowseComp-en%2520and%252046.5%2520on%2520BrowseComp-zh%252C%250Awhich%2520demonstrates%2520its%2520strong%2520capability%2520in%2520complex%252C%2520reasoning-heavy%250Ainformation-seeking%2520tasks%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BrowseMaster%3A%20Towards%20Scalable%20Web%20Browsing%20via%20Tool-Augmented%0A%20%20Programmatic%20Agent%20Pair&entry.906535625=Xianghe%20Pang%20and%20Shuo%20Tang%20and%20Rui%20Ye%20and%20Yuwen%20Du%20and%20Yaxin%20Du%20and%20Siheng%20Chen&entry.1292438233=%20%20Effective%20information%20seeking%20in%20the%20vast%20and%20ever-growing%20digital%20landscape%0Arequires%20balancing%20expansive%20search%20with%20strategic%20reasoning.%20Current%20large%0Alanguage%20model%20%28LLM%29-based%20agents%20struggle%20to%20achieve%20this%20balance%20due%20to%0Alimitations%20in%20search%20breadth%20and%20reasoning%20depth%2C%20where%20slow%2C%20serial%20querying%0Arestricts%20coverage%20of%20relevant%20sources%20and%20noisy%20raw%20inputs%20disrupt%20the%0Acontinuity%20of%20multi-step%20reasoning.%20To%20address%20these%20challenges%2C%20we%20propose%0ABrowseMaster%2C%20a%20scalable%20framework%20built%20around%20a%20programmatically%20augmented%0Aplanner-executor%20agent%20pair.%20The%20planner%20formulates%20and%20adapts%20search%0Astrategies%20based%20on%20task%20constraints%2C%20while%20the%20executor%20conducts%20efficient%2C%0Atargeted%20retrieval%20to%20supply%20the%20planner%20with%20concise%2C%20relevant%20evidence.%20This%0Adivision%20of%20labor%20preserves%20coherent%2C%20long-horizon%20reasoning%20while%20sustaining%0Abroad%20and%20systematic%20exploration%2C%20overcoming%20the%20trade-off%20that%20limits%20existing%0Aagents.%20Extensive%20experiments%20on%20challenging%20English%20and%20Chinese%20benchmarks%0Ashow%20that%20BrowseMaster%20consistently%20outperforms%20open-source%20and%20proprietary%0Abaselines%2C%20achieving%20scores%20of%2030.0%20on%20BrowseComp-en%20and%2046.5%20on%20BrowseComp-zh%2C%0Awhich%20demonstrates%20its%20strong%20capability%20in%20complex%2C%20reasoning-heavy%0Ainformation-seeking%20tasks%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09129v1&entry.124074799=Read"},
{"title": "Sleepless Nights, Sugary Days: Creating Synthetic Users with Health\n  Conditions for Realistic Coaching Agent Interactions", "author": "Taedong Yun and Eric Yang and Mustafa Safdari and Jong Ha Lee and Vaishnavi Vinod Kumar and S. Sara Mahdavi and Jonathan Amar and Derek Peyton and Reut Aharony and Andreas Michaelides and Logan Schneider and Isaac Galatzer-Levy and Yugang Jia and John Canny and Arthur Gretton and Maja Matari\u0107", "abstract": "  We present an end-to-end framework for generating synthetic users for\nevaluating interactive agents designed to encourage positive behavior changes,\nsuch as in health and lifestyle coaching. The synthetic users are grounded in\nhealth and lifestyle conditions, specifically sleep and diabetes management in\nthis study, to ensure realistic interactions with the health coaching agent.\nSynthetic users are created in two stages: first, structured data are generated\ngrounded in real-world health and lifestyle factors in addition to basic\ndemographics and behavioral attributes; second, full profiles of the synthetic\nusers are developed conditioned on the structured data. Interactions between\nsynthetic users and the coaching agent are simulated using generative\nagent-based models such as Concordia, or directly by prompting a language\nmodel. Using two independently-developed agents for sleep and diabetes coaching\nas case studies, the validity of this framework is demonstrated by analyzing\nthe coaching agent's understanding of the synthetic users' needs and\nchallenges. Finally, through multiple blinded evaluations of user-coach\ninteractions by human experts, we demonstrate that our synthetic users with\nhealth and behavioral attributes more accurately portray real human users with\nthe same attributes, compared to generic synthetic users not grounded in such\nattributes. The proposed framework lays the foundation for efficient\ndevelopment of conversational agents through extensive, realistic, and grounded\nsimulated interactions.\n", "link": "http://arxiv.org/abs/2502.13135v3", "date": "2025-08-12", "relevancy": 2.0547, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.521}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5109}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sleepless%20Nights%2C%20Sugary%20Days%3A%20Creating%20Synthetic%20Users%20with%20Health%0A%20%20Conditions%20for%20Realistic%20Coaching%20Agent%20Interactions&body=Title%3A%20Sleepless%20Nights%2C%20Sugary%20Days%3A%20Creating%20Synthetic%20Users%20with%20Health%0A%20%20Conditions%20for%20Realistic%20Coaching%20Agent%20Interactions%0AAuthor%3A%20Taedong%20Yun%20and%20Eric%20Yang%20and%20Mustafa%20Safdari%20and%20Jong%20Ha%20Lee%20and%20Vaishnavi%20Vinod%20Kumar%20and%20S.%20Sara%20Mahdavi%20and%20Jonathan%20Amar%20and%20Derek%20Peyton%20and%20Reut%20Aharony%20and%20Andreas%20Michaelides%20and%20Logan%20Schneider%20and%20Isaac%20Galatzer-Levy%20and%20Yugang%20Jia%20and%20John%20Canny%20and%20Arthur%20Gretton%20and%20Maja%20Matari%C4%87%0AAbstract%3A%20%20%20We%20present%20an%20end-to-end%20framework%20for%20generating%20synthetic%20users%20for%0Aevaluating%20interactive%20agents%20designed%20to%20encourage%20positive%20behavior%20changes%2C%0Asuch%20as%20in%20health%20and%20lifestyle%20coaching.%20The%20synthetic%20users%20are%20grounded%20in%0Ahealth%20and%20lifestyle%20conditions%2C%20specifically%20sleep%20and%20diabetes%20management%20in%0Athis%20study%2C%20to%20ensure%20realistic%20interactions%20with%20the%20health%20coaching%20agent.%0ASynthetic%20users%20are%20created%20in%20two%20stages%3A%20first%2C%20structured%20data%20are%20generated%0Agrounded%20in%20real-world%20health%20and%20lifestyle%20factors%20in%20addition%20to%20basic%0Ademographics%20and%20behavioral%20attributes%3B%20second%2C%20full%20profiles%20of%20the%20synthetic%0Ausers%20are%20developed%20conditioned%20on%20the%20structured%20data.%20Interactions%20between%0Asynthetic%20users%20and%20the%20coaching%20agent%20are%20simulated%20using%20generative%0Aagent-based%20models%20such%20as%20Concordia%2C%20or%20directly%20by%20prompting%20a%20language%0Amodel.%20Using%20two%20independently-developed%20agents%20for%20sleep%20and%20diabetes%20coaching%0Aas%20case%20studies%2C%20the%20validity%20of%20this%20framework%20is%20demonstrated%20by%20analyzing%0Athe%20coaching%20agent%27s%20understanding%20of%20the%20synthetic%20users%27%20needs%20and%0Achallenges.%20Finally%2C%20through%20multiple%20blinded%20evaluations%20of%20user-coach%0Ainteractions%20by%20human%20experts%2C%20we%20demonstrate%20that%20our%20synthetic%20users%20with%0Ahealth%20and%20behavioral%20attributes%20more%20accurately%20portray%20real%20human%20users%20with%0Athe%20same%20attributes%2C%20compared%20to%20generic%20synthetic%20users%20not%20grounded%20in%20such%0Aattributes.%20The%20proposed%20framework%20lays%20the%20foundation%20for%20efficient%0Adevelopment%20of%20conversational%20agents%20through%20extensive%2C%20realistic%2C%20and%20grounded%0Asimulated%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13135v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSleepless%2520Nights%252C%2520Sugary%2520Days%253A%2520Creating%2520Synthetic%2520Users%2520with%2520Health%250A%2520%2520Conditions%2520for%2520Realistic%2520Coaching%2520Agent%2520Interactions%26entry.906535625%3DTaedong%2520Yun%2520and%2520Eric%2520Yang%2520and%2520Mustafa%2520Safdari%2520and%2520Jong%2520Ha%2520Lee%2520and%2520Vaishnavi%2520Vinod%2520Kumar%2520and%2520S.%2520Sara%2520Mahdavi%2520and%2520Jonathan%2520Amar%2520and%2520Derek%2520Peyton%2520and%2520Reut%2520Aharony%2520and%2520Andreas%2520Michaelides%2520and%2520Logan%2520Schneider%2520and%2520Isaac%2520Galatzer-Levy%2520and%2520Yugang%2520Jia%2520and%2520John%2520Canny%2520and%2520Arthur%2520Gretton%2520and%2520Maja%2520Matari%25C4%2587%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520end-to-end%2520framework%2520for%2520generating%2520synthetic%2520users%2520for%250Aevaluating%2520interactive%2520agents%2520designed%2520to%2520encourage%2520positive%2520behavior%2520changes%252C%250Asuch%2520as%2520in%2520health%2520and%2520lifestyle%2520coaching.%2520The%2520synthetic%2520users%2520are%2520grounded%2520in%250Ahealth%2520and%2520lifestyle%2520conditions%252C%2520specifically%2520sleep%2520and%2520diabetes%2520management%2520in%250Athis%2520study%252C%2520to%2520ensure%2520realistic%2520interactions%2520with%2520the%2520health%2520coaching%2520agent.%250ASynthetic%2520users%2520are%2520created%2520in%2520two%2520stages%253A%2520first%252C%2520structured%2520data%2520are%2520generated%250Agrounded%2520in%2520real-world%2520health%2520and%2520lifestyle%2520factors%2520in%2520addition%2520to%2520basic%250Ademographics%2520and%2520behavioral%2520attributes%253B%2520second%252C%2520full%2520profiles%2520of%2520the%2520synthetic%250Ausers%2520are%2520developed%2520conditioned%2520on%2520the%2520structured%2520data.%2520Interactions%2520between%250Asynthetic%2520users%2520and%2520the%2520coaching%2520agent%2520are%2520simulated%2520using%2520generative%250Aagent-based%2520models%2520such%2520as%2520Concordia%252C%2520or%2520directly%2520by%2520prompting%2520a%2520language%250Amodel.%2520Using%2520two%2520independently-developed%2520agents%2520for%2520sleep%2520and%2520diabetes%2520coaching%250Aas%2520case%2520studies%252C%2520the%2520validity%2520of%2520this%2520framework%2520is%2520demonstrated%2520by%2520analyzing%250Athe%2520coaching%2520agent%2527s%2520understanding%2520of%2520the%2520synthetic%2520users%2527%2520needs%2520and%250Achallenges.%2520Finally%252C%2520through%2520multiple%2520blinded%2520evaluations%2520of%2520user-coach%250Ainteractions%2520by%2520human%2520experts%252C%2520we%2520demonstrate%2520that%2520our%2520synthetic%2520users%2520with%250Ahealth%2520and%2520behavioral%2520attributes%2520more%2520accurately%2520portray%2520real%2520human%2520users%2520with%250Athe%2520same%2520attributes%252C%2520compared%2520to%2520generic%2520synthetic%2520users%2520not%2520grounded%2520in%2520such%250Aattributes.%2520The%2520proposed%2520framework%2520lays%2520the%2520foundation%2520for%2520efficient%250Adevelopment%2520of%2520conversational%2520agents%2520through%2520extensive%252C%2520realistic%252C%2520and%2520grounded%250Asimulated%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13135v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sleepless%20Nights%2C%20Sugary%20Days%3A%20Creating%20Synthetic%20Users%20with%20Health%0A%20%20Conditions%20for%20Realistic%20Coaching%20Agent%20Interactions&entry.906535625=Taedong%20Yun%20and%20Eric%20Yang%20and%20Mustafa%20Safdari%20and%20Jong%20Ha%20Lee%20and%20Vaishnavi%20Vinod%20Kumar%20and%20S.%20Sara%20Mahdavi%20and%20Jonathan%20Amar%20and%20Derek%20Peyton%20and%20Reut%20Aharony%20and%20Andreas%20Michaelides%20and%20Logan%20Schneider%20and%20Isaac%20Galatzer-Levy%20and%20Yugang%20Jia%20and%20John%20Canny%20and%20Arthur%20Gretton%20and%20Maja%20Matari%C4%87&entry.1292438233=%20%20We%20present%20an%20end-to-end%20framework%20for%20generating%20synthetic%20users%20for%0Aevaluating%20interactive%20agents%20designed%20to%20encourage%20positive%20behavior%20changes%2C%0Asuch%20as%20in%20health%20and%20lifestyle%20coaching.%20The%20synthetic%20users%20are%20grounded%20in%0Ahealth%20and%20lifestyle%20conditions%2C%20specifically%20sleep%20and%20diabetes%20management%20in%0Athis%20study%2C%20to%20ensure%20realistic%20interactions%20with%20the%20health%20coaching%20agent.%0ASynthetic%20users%20are%20created%20in%20two%20stages%3A%20first%2C%20structured%20data%20are%20generated%0Agrounded%20in%20real-world%20health%20and%20lifestyle%20factors%20in%20addition%20to%20basic%0Ademographics%20and%20behavioral%20attributes%3B%20second%2C%20full%20profiles%20of%20the%20synthetic%0Ausers%20are%20developed%20conditioned%20on%20the%20structured%20data.%20Interactions%20between%0Asynthetic%20users%20and%20the%20coaching%20agent%20are%20simulated%20using%20generative%0Aagent-based%20models%20such%20as%20Concordia%2C%20or%20directly%20by%20prompting%20a%20language%0Amodel.%20Using%20two%20independently-developed%20agents%20for%20sleep%20and%20diabetes%20coaching%0Aas%20case%20studies%2C%20the%20validity%20of%20this%20framework%20is%20demonstrated%20by%20analyzing%0Athe%20coaching%20agent%27s%20understanding%20of%20the%20synthetic%20users%27%20needs%20and%0Achallenges.%20Finally%2C%20through%20multiple%20blinded%20evaluations%20of%20user-coach%0Ainteractions%20by%20human%20experts%2C%20we%20demonstrate%20that%20our%20synthetic%20users%20with%0Ahealth%20and%20behavioral%20attributes%20more%20accurately%20portray%20real%20human%20users%20with%0Athe%20same%20attributes%2C%20compared%20to%20generic%20synthetic%20users%20not%20grounded%20in%20such%0Aattributes.%20The%20proposed%20framework%20lays%20the%20foundation%20for%20efficient%0Adevelopment%20of%20conversational%20agents%20through%20extensive%2C%20realistic%2C%20and%20grounded%0Asimulated%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13135v3&entry.124074799=Read"},
{"title": "Towards Universal Neural Inference", "author": "Shreyas Bhat Brahmavar and Yang Li and Junier Oliva", "abstract": "  Real-world data often appears in diverse, disjoint forms -- with varying\nschemas, inconsistent semantics, and no fixed feature ordering -- making it\nchallenging to build general-purpose models that can leverage information\nacross datasets. We introduce ASPIRE, Arbitrary Set-based Permutation-Invariant\nReasoning Engine, a Universal Neural Inference model for semantic reasoning and\nprediction over heterogeneous structured data. ASPIRE combines a\npermutation-invariant, set-based Transformer with a semantic grounding module\nthat incorporates natural language descriptions, dataset metadata, and\nin-context examples to learn cross-dataset feature dependencies. This\narchitecture allows ASPIRE to ingest arbitrary sets of feature--value pairs and\nsupport examples, align semantics across disjoint tables, and make predictions\nfor any specified target. Once trained, ASPIRE generalizes to new inference\ntasks without additional tuning. In addition to delivering strong results\nacross diverse benchmarks, ASPIRE naturally supports cost-aware active feature\nacquisition in an open-world setting, selecting informative features under\ntest-time budget constraints for an arbitrary unseen dataset. These\ncapabilities position ASPIRE as a step toward truly universal, semantics-aware\ninference over structured data.\n", "link": "http://arxiv.org/abs/2508.09100v1", "date": "2025-08-12", "relevancy": 2.0475, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5312}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Universal%20Neural%20Inference&body=Title%3A%20Towards%20Universal%20Neural%20Inference%0AAuthor%3A%20Shreyas%20Bhat%20Brahmavar%20and%20Yang%20Li%20and%20Junier%20Oliva%0AAbstract%3A%20%20%20Real-world%20data%20often%20appears%20in%20diverse%2C%20disjoint%20forms%20--%20with%20varying%0Aschemas%2C%20inconsistent%20semantics%2C%20and%20no%20fixed%20feature%20ordering%20--%20making%20it%0Achallenging%20to%20build%20general-purpose%20models%20that%20can%20leverage%20information%0Aacross%20datasets.%20We%20introduce%20ASPIRE%2C%20Arbitrary%20Set-based%20Permutation-Invariant%0AReasoning%20Engine%2C%20a%20Universal%20Neural%20Inference%20model%20for%20semantic%20reasoning%20and%0Aprediction%20over%20heterogeneous%20structured%20data.%20ASPIRE%20combines%20a%0Apermutation-invariant%2C%20set-based%20Transformer%20with%20a%20semantic%20grounding%20module%0Athat%20incorporates%20natural%20language%20descriptions%2C%20dataset%20metadata%2C%20and%0Ain-context%20examples%20to%20learn%20cross-dataset%20feature%20dependencies.%20This%0Aarchitecture%20allows%20ASPIRE%20to%20ingest%20arbitrary%20sets%20of%20feature--value%20pairs%20and%0Asupport%20examples%2C%20align%20semantics%20across%20disjoint%20tables%2C%20and%20make%20predictions%0Afor%20any%20specified%20target.%20Once%20trained%2C%20ASPIRE%20generalizes%20to%20new%20inference%0Atasks%20without%20additional%20tuning.%20In%20addition%20to%20delivering%20strong%20results%0Aacross%20diverse%20benchmarks%2C%20ASPIRE%20naturally%20supports%20cost-aware%20active%20feature%0Aacquisition%20in%20an%20open-world%20setting%2C%20selecting%20informative%20features%20under%0Atest-time%20budget%20constraints%20for%20an%20arbitrary%20unseen%20dataset.%20These%0Acapabilities%20position%20ASPIRE%20as%20a%20step%20toward%20truly%20universal%2C%20semantics-aware%0Ainference%20over%20structured%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Universal%2520Neural%2520Inference%26entry.906535625%3DShreyas%2520Bhat%2520Brahmavar%2520and%2520Yang%2520Li%2520and%2520Junier%2520Oliva%26entry.1292438233%3D%2520%2520Real-world%2520data%2520often%2520appears%2520in%2520diverse%252C%2520disjoint%2520forms%2520--%2520with%2520varying%250Aschemas%252C%2520inconsistent%2520semantics%252C%2520and%2520no%2520fixed%2520feature%2520ordering%2520--%2520making%2520it%250Achallenging%2520to%2520build%2520general-purpose%2520models%2520that%2520can%2520leverage%2520information%250Aacross%2520datasets.%2520We%2520introduce%2520ASPIRE%252C%2520Arbitrary%2520Set-based%2520Permutation-Invariant%250AReasoning%2520Engine%252C%2520a%2520Universal%2520Neural%2520Inference%2520model%2520for%2520semantic%2520reasoning%2520and%250Aprediction%2520over%2520heterogeneous%2520structured%2520data.%2520ASPIRE%2520combines%2520a%250Apermutation-invariant%252C%2520set-based%2520Transformer%2520with%2520a%2520semantic%2520grounding%2520module%250Athat%2520incorporates%2520natural%2520language%2520descriptions%252C%2520dataset%2520metadata%252C%2520and%250Ain-context%2520examples%2520to%2520learn%2520cross-dataset%2520feature%2520dependencies.%2520This%250Aarchitecture%2520allows%2520ASPIRE%2520to%2520ingest%2520arbitrary%2520sets%2520of%2520feature--value%2520pairs%2520and%250Asupport%2520examples%252C%2520align%2520semantics%2520across%2520disjoint%2520tables%252C%2520and%2520make%2520predictions%250Afor%2520any%2520specified%2520target.%2520Once%2520trained%252C%2520ASPIRE%2520generalizes%2520to%2520new%2520inference%250Atasks%2520without%2520additional%2520tuning.%2520In%2520addition%2520to%2520delivering%2520strong%2520results%250Aacross%2520diverse%2520benchmarks%252C%2520ASPIRE%2520naturally%2520supports%2520cost-aware%2520active%2520feature%250Aacquisition%2520in%2520an%2520open-world%2520setting%252C%2520selecting%2520informative%2520features%2520under%250Atest-time%2520budget%2520constraints%2520for%2520an%2520arbitrary%2520unseen%2520dataset.%2520These%250Acapabilities%2520position%2520ASPIRE%2520as%2520a%2520step%2520toward%2520truly%2520universal%252C%2520semantics-aware%250Ainference%2520over%2520structured%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Universal%20Neural%20Inference&entry.906535625=Shreyas%20Bhat%20Brahmavar%20and%20Yang%20Li%20and%20Junier%20Oliva&entry.1292438233=%20%20Real-world%20data%20often%20appears%20in%20diverse%2C%20disjoint%20forms%20--%20with%20varying%0Aschemas%2C%20inconsistent%20semantics%2C%20and%20no%20fixed%20feature%20ordering%20--%20making%20it%0Achallenging%20to%20build%20general-purpose%20models%20that%20can%20leverage%20information%0Aacross%20datasets.%20We%20introduce%20ASPIRE%2C%20Arbitrary%20Set-based%20Permutation-Invariant%0AReasoning%20Engine%2C%20a%20Universal%20Neural%20Inference%20model%20for%20semantic%20reasoning%20and%0Aprediction%20over%20heterogeneous%20structured%20data.%20ASPIRE%20combines%20a%0Apermutation-invariant%2C%20set-based%20Transformer%20with%20a%20semantic%20grounding%20module%0Athat%20incorporates%20natural%20language%20descriptions%2C%20dataset%20metadata%2C%20and%0Ain-context%20examples%20to%20learn%20cross-dataset%20feature%20dependencies.%20This%0Aarchitecture%20allows%20ASPIRE%20to%20ingest%20arbitrary%20sets%20of%20feature--value%20pairs%20and%0Asupport%20examples%2C%20align%20semantics%20across%20disjoint%20tables%2C%20and%20make%20predictions%0Afor%20any%20specified%20target.%20Once%20trained%2C%20ASPIRE%20generalizes%20to%20new%20inference%0Atasks%20without%20additional%20tuning.%20In%20addition%20to%20delivering%20strong%20results%0Aacross%20diverse%20benchmarks%2C%20ASPIRE%20naturally%20supports%20cost-aware%20active%20feature%0Aacquisition%20in%20an%20open-world%20setting%2C%20selecting%20informative%20features%20under%0Atest-time%20budget%20constraints%20for%20an%20arbitrary%20unseen%20dataset.%20These%0Acapabilities%20position%20ASPIRE%20as%20a%20step%20toward%20truly%20universal%2C%20semantics-aware%0Ainference%20over%20structured%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09100v1&entry.124074799=Read"},
{"title": "Technical Report: Full-Stack Fine-Tuning for the Q Programming Language", "author": "Brendan R. Hogan and Will Brown and Adel Boyarsky and Anderson Schneider and Yuriy Nevmyvaka", "abstract": "  Even though large language models are becoming increasingly capable, it is\nstill unreasonable to expect them to excel at tasks that are under-represented\non the Internet. Leveraging LLMs for specialized applications, particularly in\nniche programming languages and private domains, remains challenging and\nlargely unsolved. In this work, we address this gap by presenting a\ncomprehensive, open-source approach for adapting LLMs to the Q programming\nlanguage, a popular tool in quantitative finance that is much less present on\nthe Internet compared to Python, C, Java, and other ``mainstream\" languages and\nis therefore not a strong suit of general-purpose AI models. We introduce a new\nLeetcode style evaluation dataset for Q, benchmark major frontier models on the\ndataset, then do pretraining, supervised fine tuning, and reinforcement\nlearning to train a suite of reasoning and non-reasoning models based on the\nQwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our\nbest model achieves a pass@1 accuracy of 59 percent on our Q benchmark,\nsurpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.\nAdditionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.\nIn addition to releasing models, code, and data, we provide a detailed\nblueprint for dataset construction, model pretraining, supervised fine-tuning,\nand reinforcement learning. Our methodology is broadly applicable, and we\ndiscuss how these techniques can be extended to other tasks, including those\nwhere evaluation may rely on soft or subjective signals.\n", "link": "http://arxiv.org/abs/2508.06813v2", "date": "2025-08-12", "relevancy": 2.0306, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Technical%20Report%3A%20Full-Stack%20Fine-Tuning%20for%20the%20Q%20Programming%20Language&body=Title%3A%20Technical%20Report%3A%20Full-Stack%20Fine-Tuning%20for%20the%20Q%20Programming%20Language%0AAuthor%3A%20Brendan%20R.%20Hogan%20and%20Will%20Brown%20and%20Adel%20Boyarsky%20and%20Anderson%20Schneider%20and%20Yuriy%20Nevmyvaka%0AAbstract%3A%20%20%20Even%20though%20large%20language%20models%20are%20becoming%20increasingly%20capable%2C%20it%20is%0Astill%20unreasonable%20to%20expect%20them%20to%20excel%20at%20tasks%20that%20are%20under-represented%0Aon%20the%20Internet.%20Leveraging%20LLMs%20for%20specialized%20applications%2C%20particularly%20in%0Aniche%20programming%20languages%20and%20private%20domains%2C%20remains%20challenging%20and%0Alargely%20unsolved.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20presenting%20a%0Acomprehensive%2C%20open-source%20approach%20for%20adapting%20LLMs%20to%20the%20Q%20programming%0Alanguage%2C%20a%20popular%20tool%20in%20quantitative%20finance%20that%20is%20much%20less%20present%20on%0Athe%20Internet%20compared%20to%20Python%2C%20C%2C%20Java%2C%20and%20other%20%60%60mainstream%22%20languages%20and%0Ais%20therefore%20not%20a%20strong%20suit%20of%20general-purpose%20AI%20models.%20We%20introduce%20a%20new%0ALeetcode%20style%20evaluation%20dataset%20for%20Q%2C%20benchmark%20major%20frontier%20models%20on%20the%0Adataset%2C%20then%20do%20pretraining%2C%20supervised%20fine%20tuning%2C%20and%20reinforcement%0Alearning%20to%20train%20a%20suite%20of%20reasoning%20and%20non-reasoning%20models%20based%20on%20the%0AQwen-2.5%20series%2C%20spanning%20five%20parameter%20sizes%20%281.5B%2C%203B%2C%207B%2C%2014B%2C%2032B%29.%20Our%0Abest%20model%20achieves%20a%20pass%401%20accuracy%20of%2059%20percent%20on%20our%20Q%20benchmark%2C%0Asurpassing%20the%20best-performing%20frontier%20model%2C%20Claude%20Opus-4%20by%2029.5%20percent.%0AAdditionally%2C%20all%20models%2C%20even%20our%201.5B%20model%2C%20outperform%20GPT-4.1%20on%20this%20task.%0AIn%20addition%20to%20releasing%20models%2C%20code%2C%20and%20data%2C%20we%20provide%20a%20detailed%0Ablueprint%20for%20dataset%20construction%2C%20model%20pretraining%2C%20supervised%20fine-tuning%2C%0Aand%20reinforcement%20learning.%20Our%20methodology%20is%20broadly%20applicable%2C%20and%20we%0Adiscuss%20how%20these%20techniques%20can%20be%20extended%20to%20other%20tasks%2C%20including%20those%0Awhere%20evaluation%20may%20rely%20on%20soft%20or%20subjective%20signals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06813v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTechnical%2520Report%253A%2520Full-Stack%2520Fine-Tuning%2520for%2520the%2520Q%2520Programming%2520Language%26entry.906535625%3DBrendan%2520R.%2520Hogan%2520and%2520Will%2520Brown%2520and%2520Adel%2520Boyarsky%2520and%2520Anderson%2520Schneider%2520and%2520Yuriy%2520Nevmyvaka%26entry.1292438233%3D%2520%2520Even%2520though%2520large%2520language%2520models%2520are%2520becoming%2520increasingly%2520capable%252C%2520it%2520is%250Astill%2520unreasonable%2520to%2520expect%2520them%2520to%2520excel%2520at%2520tasks%2520that%2520are%2520under-represented%250Aon%2520the%2520Internet.%2520Leveraging%2520LLMs%2520for%2520specialized%2520applications%252C%2520particularly%2520in%250Aniche%2520programming%2520languages%2520and%2520private%2520domains%252C%2520remains%2520challenging%2520and%250Alargely%2520unsolved.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520gap%2520by%2520presenting%2520a%250Acomprehensive%252C%2520open-source%2520approach%2520for%2520adapting%2520LLMs%2520to%2520the%2520Q%2520programming%250Alanguage%252C%2520a%2520popular%2520tool%2520in%2520quantitative%2520finance%2520that%2520is%2520much%2520less%2520present%2520on%250Athe%2520Internet%2520compared%2520to%2520Python%252C%2520C%252C%2520Java%252C%2520and%2520other%2520%2560%2560mainstream%2522%2520languages%2520and%250Ais%2520therefore%2520not%2520a%2520strong%2520suit%2520of%2520general-purpose%2520AI%2520models.%2520We%2520introduce%2520a%2520new%250ALeetcode%2520style%2520evaluation%2520dataset%2520for%2520Q%252C%2520benchmark%2520major%2520frontier%2520models%2520on%2520the%250Adataset%252C%2520then%2520do%2520pretraining%252C%2520supervised%2520fine%2520tuning%252C%2520and%2520reinforcement%250Alearning%2520to%2520train%2520a%2520suite%2520of%2520reasoning%2520and%2520non-reasoning%2520models%2520based%2520on%2520the%250AQwen-2.5%2520series%252C%2520spanning%2520five%2520parameter%2520sizes%2520%25281.5B%252C%25203B%252C%25207B%252C%252014B%252C%252032B%2529.%2520Our%250Abest%2520model%2520achieves%2520a%2520pass%25401%2520accuracy%2520of%252059%2520percent%2520on%2520our%2520Q%2520benchmark%252C%250Asurpassing%2520the%2520best-performing%2520frontier%2520model%252C%2520Claude%2520Opus-4%2520by%252029.5%2520percent.%250AAdditionally%252C%2520all%2520models%252C%2520even%2520our%25201.5B%2520model%252C%2520outperform%2520GPT-4.1%2520on%2520this%2520task.%250AIn%2520addition%2520to%2520releasing%2520models%252C%2520code%252C%2520and%2520data%252C%2520we%2520provide%2520a%2520detailed%250Ablueprint%2520for%2520dataset%2520construction%252C%2520model%2520pretraining%252C%2520supervised%2520fine-tuning%252C%250Aand%2520reinforcement%2520learning.%2520Our%2520methodology%2520is%2520broadly%2520applicable%252C%2520and%2520we%250Adiscuss%2520how%2520these%2520techniques%2520can%2520be%2520extended%2520to%2520other%2520tasks%252C%2520including%2520those%250Awhere%2520evaluation%2520may%2520rely%2520on%2520soft%2520or%2520subjective%2520signals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06813v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Technical%20Report%3A%20Full-Stack%20Fine-Tuning%20for%20the%20Q%20Programming%20Language&entry.906535625=Brendan%20R.%20Hogan%20and%20Will%20Brown%20and%20Adel%20Boyarsky%20and%20Anderson%20Schneider%20and%20Yuriy%20Nevmyvaka&entry.1292438233=%20%20Even%20though%20large%20language%20models%20are%20becoming%20increasingly%20capable%2C%20it%20is%0Astill%20unreasonable%20to%20expect%20them%20to%20excel%20at%20tasks%20that%20are%20under-represented%0Aon%20the%20Internet.%20Leveraging%20LLMs%20for%20specialized%20applications%2C%20particularly%20in%0Aniche%20programming%20languages%20and%20private%20domains%2C%20remains%20challenging%20and%0Alargely%20unsolved.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20presenting%20a%0Acomprehensive%2C%20open-source%20approach%20for%20adapting%20LLMs%20to%20the%20Q%20programming%0Alanguage%2C%20a%20popular%20tool%20in%20quantitative%20finance%20that%20is%20much%20less%20present%20on%0Athe%20Internet%20compared%20to%20Python%2C%20C%2C%20Java%2C%20and%20other%20%60%60mainstream%22%20languages%20and%0Ais%20therefore%20not%20a%20strong%20suit%20of%20general-purpose%20AI%20models.%20We%20introduce%20a%20new%0ALeetcode%20style%20evaluation%20dataset%20for%20Q%2C%20benchmark%20major%20frontier%20models%20on%20the%0Adataset%2C%20then%20do%20pretraining%2C%20supervised%20fine%20tuning%2C%20and%20reinforcement%0Alearning%20to%20train%20a%20suite%20of%20reasoning%20and%20non-reasoning%20models%20based%20on%20the%0AQwen-2.5%20series%2C%20spanning%20five%20parameter%20sizes%20%281.5B%2C%203B%2C%207B%2C%2014B%2C%2032B%29.%20Our%0Abest%20model%20achieves%20a%20pass%401%20accuracy%20of%2059%20percent%20on%20our%20Q%20benchmark%2C%0Asurpassing%20the%20best-performing%20frontier%20model%2C%20Claude%20Opus-4%20by%2029.5%20percent.%0AAdditionally%2C%20all%20models%2C%20even%20our%201.5B%20model%2C%20outperform%20GPT-4.1%20on%20this%20task.%0AIn%20addition%20to%20releasing%20models%2C%20code%2C%20and%20data%2C%20we%20provide%20a%20detailed%0Ablueprint%20for%20dataset%20construction%2C%20model%20pretraining%2C%20supervised%20fine-tuning%2C%0Aand%20reinforcement%20learning.%20Our%20methodology%20is%20broadly%20applicable%2C%20and%20we%0Adiscuss%20how%20these%20techniques%20can%20be%20extended%20to%20other%20tasks%2C%20including%20those%0Awhere%20evaluation%20may%20rely%20on%20soft%20or%20subjective%20signals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06813v2&entry.124074799=Read"},
{"title": "Deep Learning Models for Robust Facial Liveness Detection", "author": "Oleksandr Kuznetsov and Emanuele Frontoni and Luca Romeo and Riccardo Rosati and Andrea Maranesi and Alessandro Muscatello", "abstract": "  In the rapidly evolving landscape of digital security, biometric\nauthentication systems, particularly facial recognition, have emerged as\nintegral components of various security protocols. However, the reliability of\nthese systems is compromised by sophisticated spoofing attacks, where imposters\ngain unauthorized access by falsifying biometric traits. Current literature\nreveals a concerning gap: existing liveness detection methodologies - designed\nto counteract these breaches - fall short against advanced spoofing tactics\nemploying deepfakes and other artificial intelligence-driven manipulations.\nThis study introduces a robust solution through novel deep learning models\naddressing the deficiencies in contemporary anti-spoofing techniques. By\ninnovatively integrating texture analysis and reflective properties associated\nwith genuine human traits, our models distinguish authentic presence from\nreplicas with remarkable precision. Extensive evaluations were conducted across\nfive diverse datasets, encompassing a wide range of attack vectors and\nenvironmental conditions. Results demonstrate substantial advancement over\nexisting systems, with our best model (AttackNet V2.2) achieving 99.9% average\naccuracy when trained on combined data. Moreover, our research unveils critical\ninsights into the behavioral patterns of impostor attacks, contributing to a\nmore nuanced understanding of their evolving nature. The implications are\nprofound: our models do not merely fortify the authentication processes but\nalso instill confidence in biometric systems across various sectors reliant on\nsecure access.\n", "link": "http://arxiv.org/abs/2508.09094v1", "date": "2025-08-12", "relevancy": 2.0237, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5102}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.504}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Models%20for%20Robust%20Facial%20Liveness%20Detection&body=Title%3A%20Deep%20Learning%20Models%20for%20Robust%20Facial%20Liveness%20Detection%0AAuthor%3A%20Oleksandr%20Kuznetsov%20and%20Emanuele%20Frontoni%20and%20Luca%20Romeo%20and%20Riccardo%20Rosati%20and%20Andrea%20Maranesi%20and%20Alessandro%20Muscatello%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20landscape%20of%20digital%20security%2C%20biometric%0Aauthentication%20systems%2C%20particularly%20facial%20recognition%2C%20have%20emerged%20as%0Aintegral%20components%20of%20various%20security%20protocols.%20However%2C%20the%20reliability%20of%0Athese%20systems%20is%20compromised%20by%20sophisticated%20spoofing%20attacks%2C%20where%20imposters%0Again%20unauthorized%20access%20by%20falsifying%20biometric%20traits.%20Current%20literature%0Areveals%20a%20concerning%20gap%3A%20existing%20liveness%20detection%20methodologies%20-%20designed%0Ato%20counteract%20these%20breaches%20-%20fall%20short%20against%20advanced%20spoofing%20tactics%0Aemploying%20deepfakes%20and%20other%20artificial%20intelligence-driven%20manipulations.%0AThis%20study%20introduces%20a%20robust%20solution%20through%20novel%20deep%20learning%20models%0Aaddressing%20the%20deficiencies%20in%20contemporary%20anti-spoofing%20techniques.%20By%0Ainnovatively%20integrating%20texture%20analysis%20and%20reflective%20properties%20associated%0Awith%20genuine%20human%20traits%2C%20our%20models%20distinguish%20authentic%20presence%20from%0Areplicas%20with%20remarkable%20precision.%20Extensive%20evaluations%20were%20conducted%20across%0Afive%20diverse%20datasets%2C%20encompassing%20a%20wide%20range%20of%20attack%20vectors%20and%0Aenvironmental%20conditions.%20Results%20demonstrate%20substantial%20advancement%20over%0Aexisting%20systems%2C%20with%20our%20best%20model%20%28AttackNet%20V2.2%29%20achieving%2099.9%25%20average%0Aaccuracy%20when%20trained%20on%20combined%20data.%20Moreover%2C%20our%20research%20unveils%20critical%0Ainsights%20into%20the%20behavioral%20patterns%20of%20impostor%20attacks%2C%20contributing%20to%20a%0Amore%20nuanced%20understanding%20of%20their%20evolving%20nature.%20The%20implications%20are%0Aprofound%3A%20our%20models%20do%20not%20merely%20fortify%20the%20authentication%20processes%20but%0Aalso%20instill%20confidence%20in%20biometric%20systems%20across%20various%20sectors%20reliant%20on%0Asecure%20access.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Models%2520for%2520Robust%2520Facial%2520Liveness%2520Detection%26entry.906535625%3DOleksandr%2520Kuznetsov%2520and%2520Emanuele%2520Frontoni%2520and%2520Luca%2520Romeo%2520and%2520Riccardo%2520Rosati%2520and%2520Andrea%2520Maranesi%2520and%2520Alessandro%2520Muscatello%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520landscape%2520of%2520digital%2520security%252C%2520biometric%250Aauthentication%2520systems%252C%2520particularly%2520facial%2520recognition%252C%2520have%2520emerged%2520as%250Aintegral%2520components%2520of%2520various%2520security%2520protocols.%2520However%252C%2520the%2520reliability%2520of%250Athese%2520systems%2520is%2520compromised%2520by%2520sophisticated%2520spoofing%2520attacks%252C%2520where%2520imposters%250Again%2520unauthorized%2520access%2520by%2520falsifying%2520biometric%2520traits.%2520Current%2520literature%250Areveals%2520a%2520concerning%2520gap%253A%2520existing%2520liveness%2520detection%2520methodologies%2520-%2520designed%250Ato%2520counteract%2520these%2520breaches%2520-%2520fall%2520short%2520against%2520advanced%2520spoofing%2520tactics%250Aemploying%2520deepfakes%2520and%2520other%2520artificial%2520intelligence-driven%2520manipulations.%250AThis%2520study%2520introduces%2520a%2520robust%2520solution%2520through%2520novel%2520deep%2520learning%2520models%250Aaddressing%2520the%2520deficiencies%2520in%2520contemporary%2520anti-spoofing%2520techniques.%2520By%250Ainnovatively%2520integrating%2520texture%2520analysis%2520and%2520reflective%2520properties%2520associated%250Awith%2520genuine%2520human%2520traits%252C%2520our%2520models%2520distinguish%2520authentic%2520presence%2520from%250Areplicas%2520with%2520remarkable%2520precision.%2520Extensive%2520evaluations%2520were%2520conducted%2520across%250Afive%2520diverse%2520datasets%252C%2520encompassing%2520a%2520wide%2520range%2520of%2520attack%2520vectors%2520and%250Aenvironmental%2520conditions.%2520Results%2520demonstrate%2520substantial%2520advancement%2520over%250Aexisting%2520systems%252C%2520with%2520our%2520best%2520model%2520%2528AttackNet%2520V2.2%2529%2520achieving%252099.9%2525%2520average%250Aaccuracy%2520when%2520trained%2520on%2520combined%2520data.%2520Moreover%252C%2520our%2520research%2520unveils%2520critical%250Ainsights%2520into%2520the%2520behavioral%2520patterns%2520of%2520impostor%2520attacks%252C%2520contributing%2520to%2520a%250Amore%2520nuanced%2520understanding%2520of%2520their%2520evolving%2520nature.%2520The%2520implications%2520are%250Aprofound%253A%2520our%2520models%2520do%2520not%2520merely%2520fortify%2520the%2520authentication%2520processes%2520but%250Aalso%2520instill%2520confidence%2520in%2520biometric%2520systems%2520across%2520various%2520sectors%2520reliant%2520on%250Asecure%2520access.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Models%20for%20Robust%20Facial%20Liveness%20Detection&entry.906535625=Oleksandr%20Kuznetsov%20and%20Emanuele%20Frontoni%20and%20Luca%20Romeo%20and%20Riccardo%20Rosati%20and%20Andrea%20Maranesi%20and%20Alessandro%20Muscatello&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20landscape%20of%20digital%20security%2C%20biometric%0Aauthentication%20systems%2C%20particularly%20facial%20recognition%2C%20have%20emerged%20as%0Aintegral%20components%20of%20various%20security%20protocols.%20However%2C%20the%20reliability%20of%0Athese%20systems%20is%20compromised%20by%20sophisticated%20spoofing%20attacks%2C%20where%20imposters%0Again%20unauthorized%20access%20by%20falsifying%20biometric%20traits.%20Current%20literature%0Areveals%20a%20concerning%20gap%3A%20existing%20liveness%20detection%20methodologies%20-%20designed%0Ato%20counteract%20these%20breaches%20-%20fall%20short%20against%20advanced%20spoofing%20tactics%0Aemploying%20deepfakes%20and%20other%20artificial%20intelligence-driven%20manipulations.%0AThis%20study%20introduces%20a%20robust%20solution%20through%20novel%20deep%20learning%20models%0Aaddressing%20the%20deficiencies%20in%20contemporary%20anti-spoofing%20techniques.%20By%0Ainnovatively%20integrating%20texture%20analysis%20and%20reflective%20properties%20associated%0Awith%20genuine%20human%20traits%2C%20our%20models%20distinguish%20authentic%20presence%20from%0Areplicas%20with%20remarkable%20precision.%20Extensive%20evaluations%20were%20conducted%20across%0Afive%20diverse%20datasets%2C%20encompassing%20a%20wide%20range%20of%20attack%20vectors%20and%0Aenvironmental%20conditions.%20Results%20demonstrate%20substantial%20advancement%20over%0Aexisting%20systems%2C%20with%20our%20best%20model%20%28AttackNet%20V2.2%29%20achieving%2099.9%25%20average%0Aaccuracy%20when%20trained%20on%20combined%20data.%20Moreover%2C%20our%20research%20unveils%20critical%0Ainsights%20into%20the%20behavioral%20patterns%20of%20impostor%20attacks%2C%20contributing%20to%20a%0Amore%20nuanced%20understanding%20of%20their%20evolving%20nature.%20The%20implications%20are%0Aprofound%3A%20our%20models%20do%20not%20merely%20fortify%20the%20authentication%20processes%20but%0Aalso%20instill%20confidence%20in%20biometric%20systems%20across%20various%20sectors%20reliant%20on%0Asecure%20access.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09094v1&entry.124074799=Read"},
{"title": "BeyondMimic: From Motion Tracking to Versatile Humanoid Control via\n  Guided Diffusion", "author": "Takara E. Truong and Qiayuan Liao and Xiaoyu Huang and Guy Tevet and C. Karen Liu and Koushil Sreenath", "abstract": "  Learning skills from human motions offers a promising path toward\ngeneralizable policies for whole-body humanoid control, yet two key\ncornerstones are missing: (1) a high-quality motion tracking framework that\nfaithfully transforms large-scale kinematic references into robust and\nextremely dynamic motions on real hardware, and (2) a distillation approach\nthat can effectively learn these motion primitives and compose them to solve\ndownstream tasks. We address these gaps with BeyondMimic, a real-world\nframework to learn from human motions for versatile and naturalistic humanoid\ncontrol via guided diffusion. Our framework provides a motion tracking pipeline\ncapable of challenging skills such as jumping spins, sprinting, and cartwheels\nwith state-of-the-art motion quality. Moving beyond mimicking existing motions\nand synthesize novel ones, we further introduce a unified diffusion policy that\nenables zero-shot task-specific control at test time using simple cost\nfunctions. Deployed on hardware, BeyondMimic performs diverse tasks at test\ntime, including waypoint navigation, joystick teleoperation, and obstacle\navoidance, bridging sim-to-real motion tracking and flexible synthesis of human\nmotion primitives for whole-body control. https://beyondmimic.github.io/.\n", "link": "http://arxiv.org/abs/2508.08241v2", "date": "2025-08-12", "relevancy": 2.0127, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7328}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6114}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BeyondMimic%3A%20From%20Motion%20Tracking%20to%20Versatile%20Humanoid%20Control%20via%0A%20%20Guided%20Diffusion&body=Title%3A%20BeyondMimic%3A%20From%20Motion%20Tracking%20to%20Versatile%20Humanoid%20Control%20via%0A%20%20Guided%20Diffusion%0AAuthor%3A%20Takara%20E.%20Truong%20and%20Qiayuan%20Liao%20and%20Xiaoyu%20Huang%20and%20Guy%20Tevet%20and%20C.%20Karen%20Liu%20and%20Koushil%20Sreenath%0AAbstract%3A%20%20%20Learning%20skills%20from%20human%20motions%20offers%20a%20promising%20path%20toward%0Ageneralizable%20policies%20for%20whole-body%20humanoid%20control%2C%20yet%20two%20key%0Acornerstones%20are%20missing%3A%20%281%29%20a%20high-quality%20motion%20tracking%20framework%20that%0Afaithfully%20transforms%20large-scale%20kinematic%20references%20into%20robust%20and%0Aextremely%20dynamic%20motions%20on%20real%20hardware%2C%20and%20%282%29%20a%20distillation%20approach%0Athat%20can%20effectively%20learn%20these%20motion%20primitives%20and%20compose%20them%20to%20solve%0Adownstream%20tasks.%20We%20address%20these%20gaps%20with%20BeyondMimic%2C%20a%20real-world%0Aframework%20to%20learn%20from%20human%20motions%20for%20versatile%20and%20naturalistic%20humanoid%0Acontrol%20via%20guided%20diffusion.%20Our%20framework%20provides%20a%20motion%20tracking%20pipeline%0Acapable%20of%20challenging%20skills%20such%20as%20jumping%20spins%2C%20sprinting%2C%20and%20cartwheels%0Awith%20state-of-the-art%20motion%20quality.%20Moving%20beyond%20mimicking%20existing%20motions%0Aand%20synthesize%20novel%20ones%2C%20we%20further%20introduce%20a%20unified%20diffusion%20policy%20that%0Aenables%20zero-shot%20task-specific%20control%20at%20test%20time%20using%20simple%20cost%0Afunctions.%20Deployed%20on%20hardware%2C%20BeyondMimic%20performs%20diverse%20tasks%20at%20test%0Atime%2C%20including%20waypoint%20navigation%2C%20joystick%20teleoperation%2C%20and%20obstacle%0Aavoidance%2C%20bridging%20sim-to-real%20motion%20tracking%20and%20flexible%20synthesis%20of%20human%0Amotion%20primitives%20for%20whole-body%20control.%20https%3A//beyondmimic.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08241v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyondMimic%253A%2520From%2520Motion%2520Tracking%2520to%2520Versatile%2520Humanoid%2520Control%2520via%250A%2520%2520Guided%2520Diffusion%26entry.906535625%3DTakara%2520E.%2520Truong%2520and%2520Qiayuan%2520Liao%2520and%2520Xiaoyu%2520Huang%2520and%2520Guy%2520Tevet%2520and%2520C.%2520Karen%2520Liu%2520and%2520Koushil%2520Sreenath%26entry.1292438233%3D%2520%2520Learning%2520skills%2520from%2520human%2520motions%2520offers%2520a%2520promising%2520path%2520toward%250Ageneralizable%2520policies%2520for%2520whole-body%2520humanoid%2520control%252C%2520yet%2520two%2520key%250Acornerstones%2520are%2520missing%253A%2520%25281%2529%2520a%2520high-quality%2520motion%2520tracking%2520framework%2520that%250Afaithfully%2520transforms%2520large-scale%2520kinematic%2520references%2520into%2520robust%2520and%250Aextremely%2520dynamic%2520motions%2520on%2520real%2520hardware%252C%2520and%2520%25282%2529%2520a%2520distillation%2520approach%250Athat%2520can%2520effectively%2520learn%2520these%2520motion%2520primitives%2520and%2520compose%2520them%2520to%2520solve%250Adownstream%2520tasks.%2520We%2520address%2520these%2520gaps%2520with%2520BeyondMimic%252C%2520a%2520real-world%250Aframework%2520to%2520learn%2520from%2520human%2520motions%2520for%2520versatile%2520and%2520naturalistic%2520humanoid%250Acontrol%2520via%2520guided%2520diffusion.%2520Our%2520framework%2520provides%2520a%2520motion%2520tracking%2520pipeline%250Acapable%2520of%2520challenging%2520skills%2520such%2520as%2520jumping%2520spins%252C%2520sprinting%252C%2520and%2520cartwheels%250Awith%2520state-of-the-art%2520motion%2520quality.%2520Moving%2520beyond%2520mimicking%2520existing%2520motions%250Aand%2520synthesize%2520novel%2520ones%252C%2520we%2520further%2520introduce%2520a%2520unified%2520diffusion%2520policy%2520that%250Aenables%2520zero-shot%2520task-specific%2520control%2520at%2520test%2520time%2520using%2520simple%2520cost%250Afunctions.%2520Deployed%2520on%2520hardware%252C%2520BeyondMimic%2520performs%2520diverse%2520tasks%2520at%2520test%250Atime%252C%2520including%2520waypoint%2520navigation%252C%2520joystick%2520teleoperation%252C%2520and%2520obstacle%250Aavoidance%252C%2520bridging%2520sim-to-real%2520motion%2520tracking%2520and%2520flexible%2520synthesis%2520of%2520human%250Amotion%2520primitives%2520for%2520whole-body%2520control.%2520https%253A//beyondmimic.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08241v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BeyondMimic%3A%20From%20Motion%20Tracking%20to%20Versatile%20Humanoid%20Control%20via%0A%20%20Guided%20Diffusion&entry.906535625=Takara%20E.%20Truong%20and%20Qiayuan%20Liao%20and%20Xiaoyu%20Huang%20and%20Guy%20Tevet%20and%20C.%20Karen%20Liu%20and%20Koushil%20Sreenath&entry.1292438233=%20%20Learning%20skills%20from%20human%20motions%20offers%20a%20promising%20path%20toward%0Ageneralizable%20policies%20for%20whole-body%20humanoid%20control%2C%20yet%20two%20key%0Acornerstones%20are%20missing%3A%20%281%29%20a%20high-quality%20motion%20tracking%20framework%20that%0Afaithfully%20transforms%20large-scale%20kinematic%20references%20into%20robust%20and%0Aextremely%20dynamic%20motions%20on%20real%20hardware%2C%20and%20%282%29%20a%20distillation%20approach%0Athat%20can%20effectively%20learn%20these%20motion%20primitives%20and%20compose%20them%20to%20solve%0Adownstream%20tasks.%20We%20address%20these%20gaps%20with%20BeyondMimic%2C%20a%20real-world%0Aframework%20to%20learn%20from%20human%20motions%20for%20versatile%20and%20naturalistic%20humanoid%0Acontrol%20via%20guided%20diffusion.%20Our%20framework%20provides%20a%20motion%20tracking%20pipeline%0Acapable%20of%20challenging%20skills%20such%20as%20jumping%20spins%2C%20sprinting%2C%20and%20cartwheels%0Awith%20state-of-the-art%20motion%20quality.%20Moving%20beyond%20mimicking%20existing%20motions%0Aand%20synthesize%20novel%20ones%2C%20we%20further%20introduce%20a%20unified%20diffusion%20policy%20that%0Aenables%20zero-shot%20task-specific%20control%20at%20test%20time%20using%20simple%20cost%0Afunctions.%20Deployed%20on%20hardware%2C%20BeyondMimic%20performs%20diverse%20tasks%20at%20test%0Atime%2C%20including%20waypoint%20navigation%2C%20joystick%20teleoperation%2C%20and%20obstacle%0Aavoidance%2C%20bridging%20sim-to-real%20motion%20tracking%20and%20flexible%20synthesis%20of%20human%0Amotion%20primitives%20for%20whole-body%20control.%20https%3A//beyondmimic.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08241v2&entry.124074799=Read"},
{"title": "FBFL: A Field-Based Coordination Approach for Data Heterogeneity in\n  Federated Learning", "author": "Davide Domini and Gianluca Aguzzi and Lukas Esterle and Mirko Viroli", "abstract": "  In the last years, Federated learning (FL) has become a popular solution to\ntrain machine learning models in domains with high privacy concerns. However,\nFL scalability and performance face significant challenges in real-world\ndeployments where data across devices are non-independently and identically\ndistributed (non-IID). The heterogeneity in data distribution frequently arises\nfrom spatial distribution of devices, leading to degraded model performance in\nthe absence of proper handling. Additionally, FL typical reliance on\ncentralized architectures introduces bottlenecks and single-point-of-failure\nrisks, particularly problematic at scale or in dynamic environments. To close\nthis gap, we propose Field-Based Federated Learning (FBFL), a novel approach\nleveraging macroprogramming and field coordination to address these limitations\nthrough: (i) distributed spatial-based leader election for personalization to\nmitigate non-IID data challenges; and (ii) construction of a self-organizing,\nhierarchical architecture using advanced macroprogramming patterns. Moreover,\nFBFL not only overcomes the aforementioned limitations, but also enables the\ndevelopment of more specialized models tailored to the specific data\ndistribution in each subregion. This paper formalizes FBFL and evaluates it\nextensively using MNIST, FashionMNIST, and Extended MNIST datasets. We\ndemonstrate that, when operating under IID data conditions, FBFL performs\ncomparably to the widely-used FedAvg algorithm. Furthermore, in challenging\nnon-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other\nstate-of-the-art methods, namely FedProx and Scaffold, which have been\nspecifically designed to address non-IID data distributions. Additionally, we\nshowcase the resilience of FBFL's self-organizing hierarchical architecture\nagainst server failures.\n", "link": "http://arxiv.org/abs/2502.08577v2", "date": "2025-08-12", "relevancy": 1.9917, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5112}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4887}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FBFL%3A%20A%20Field-Based%20Coordination%20Approach%20for%20Data%20Heterogeneity%20in%0A%20%20Federated%20Learning&body=Title%3A%20FBFL%3A%20A%20Field-Based%20Coordination%20Approach%20for%20Data%20Heterogeneity%20in%0A%20%20Federated%20Learning%0AAuthor%3A%20Davide%20Domini%20and%20Gianluca%20Aguzzi%20and%20Lukas%20Esterle%20and%20Mirko%20Viroli%0AAbstract%3A%20%20%20In%20the%20last%20years%2C%20Federated%20learning%20%28FL%29%20has%20become%20a%20popular%20solution%20to%0Atrain%20machine%20learning%20models%20in%20domains%20with%20high%20privacy%20concerns.%20However%2C%0AFL%20scalability%20and%20performance%20face%20significant%20challenges%20in%20real-world%0Adeployments%20where%20data%20across%20devices%20are%20non-independently%20and%20identically%0Adistributed%20%28non-IID%29.%20The%20heterogeneity%20in%20data%20distribution%20frequently%20arises%0Afrom%20spatial%20distribution%20of%20devices%2C%20leading%20to%20degraded%20model%20performance%20in%0Athe%20absence%20of%20proper%20handling.%20Additionally%2C%20FL%20typical%20reliance%20on%0Acentralized%20architectures%20introduces%20bottlenecks%20and%20single-point-of-failure%0Arisks%2C%20particularly%20problematic%20at%20scale%20or%20in%20dynamic%20environments.%20To%20close%0Athis%20gap%2C%20we%20propose%20Field-Based%20Federated%20Learning%20%28FBFL%29%2C%20a%20novel%20approach%0Aleveraging%20macroprogramming%20and%20field%20coordination%20to%20address%20these%20limitations%0Athrough%3A%20%28i%29%20distributed%20spatial-based%20leader%20election%20for%20personalization%20to%0Amitigate%20non-IID%20data%20challenges%3B%20and%20%28ii%29%20construction%20of%20a%20self-organizing%2C%0Ahierarchical%20architecture%20using%20advanced%20macroprogramming%20patterns.%20Moreover%2C%0AFBFL%20not%20only%20overcomes%20the%20aforementioned%20limitations%2C%20but%20also%20enables%20the%0Adevelopment%20of%20more%20specialized%20models%20tailored%20to%20the%20specific%20data%0Adistribution%20in%20each%20subregion.%20This%20paper%20formalizes%20FBFL%20and%20evaluates%20it%0Aextensively%20using%20MNIST%2C%20FashionMNIST%2C%20and%20Extended%20MNIST%20datasets.%20We%0Ademonstrate%20that%2C%20when%20operating%20under%20IID%20data%20conditions%2C%20FBFL%20performs%0Acomparably%20to%20the%20widely-used%20FedAvg%20algorithm.%20Furthermore%2C%20in%20challenging%0Anon-IID%20scenarios%2C%20FBFL%20not%20only%20outperforms%20FedAvg%20but%20also%20surpasses%20other%0Astate-of-the-art%20methods%2C%20namely%20FedProx%20and%20Scaffold%2C%20which%20have%20been%0Aspecifically%20designed%20to%20address%20non-IID%20data%20distributions.%20Additionally%2C%20we%0Ashowcase%20the%20resilience%20of%20FBFL%27s%20self-organizing%20hierarchical%20architecture%0Aagainst%20server%20failures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08577v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFBFL%253A%2520A%2520Field-Based%2520Coordination%2520Approach%2520for%2520Data%2520Heterogeneity%2520in%250A%2520%2520Federated%2520Learning%26entry.906535625%3DDavide%2520Domini%2520and%2520Gianluca%2520Aguzzi%2520and%2520Lukas%2520Esterle%2520and%2520Mirko%2520Viroli%26entry.1292438233%3D%2520%2520In%2520the%2520last%2520years%252C%2520Federated%2520learning%2520%2528FL%2529%2520has%2520become%2520a%2520popular%2520solution%2520to%250Atrain%2520machine%2520learning%2520models%2520in%2520domains%2520with%2520high%2520privacy%2520concerns.%2520However%252C%250AFL%2520scalability%2520and%2520performance%2520face%2520significant%2520challenges%2520in%2520real-world%250Adeployments%2520where%2520data%2520across%2520devices%2520are%2520non-independently%2520and%2520identically%250Adistributed%2520%2528non-IID%2529.%2520The%2520heterogeneity%2520in%2520data%2520distribution%2520frequently%2520arises%250Afrom%2520spatial%2520distribution%2520of%2520devices%252C%2520leading%2520to%2520degraded%2520model%2520performance%2520in%250Athe%2520absence%2520of%2520proper%2520handling.%2520Additionally%252C%2520FL%2520typical%2520reliance%2520on%250Acentralized%2520architectures%2520introduces%2520bottlenecks%2520and%2520single-point-of-failure%250Arisks%252C%2520particularly%2520problematic%2520at%2520scale%2520or%2520in%2520dynamic%2520environments.%2520To%2520close%250Athis%2520gap%252C%2520we%2520propose%2520Field-Based%2520Federated%2520Learning%2520%2528FBFL%2529%252C%2520a%2520novel%2520approach%250Aleveraging%2520macroprogramming%2520and%2520field%2520coordination%2520to%2520address%2520these%2520limitations%250Athrough%253A%2520%2528i%2529%2520distributed%2520spatial-based%2520leader%2520election%2520for%2520personalization%2520to%250Amitigate%2520non-IID%2520data%2520challenges%253B%2520and%2520%2528ii%2529%2520construction%2520of%2520a%2520self-organizing%252C%250Ahierarchical%2520architecture%2520using%2520advanced%2520macroprogramming%2520patterns.%2520Moreover%252C%250AFBFL%2520not%2520only%2520overcomes%2520the%2520aforementioned%2520limitations%252C%2520but%2520also%2520enables%2520the%250Adevelopment%2520of%2520more%2520specialized%2520models%2520tailored%2520to%2520the%2520specific%2520data%250Adistribution%2520in%2520each%2520subregion.%2520This%2520paper%2520formalizes%2520FBFL%2520and%2520evaluates%2520it%250Aextensively%2520using%2520MNIST%252C%2520FashionMNIST%252C%2520and%2520Extended%2520MNIST%2520datasets.%2520We%250Ademonstrate%2520that%252C%2520when%2520operating%2520under%2520IID%2520data%2520conditions%252C%2520FBFL%2520performs%250Acomparably%2520to%2520the%2520widely-used%2520FedAvg%2520algorithm.%2520Furthermore%252C%2520in%2520challenging%250Anon-IID%2520scenarios%252C%2520FBFL%2520not%2520only%2520outperforms%2520FedAvg%2520but%2520also%2520surpasses%2520other%250Astate-of-the-art%2520methods%252C%2520namely%2520FedProx%2520and%2520Scaffold%252C%2520which%2520have%2520been%250Aspecifically%2520designed%2520to%2520address%2520non-IID%2520data%2520distributions.%2520Additionally%252C%2520we%250Ashowcase%2520the%2520resilience%2520of%2520FBFL%2527s%2520self-organizing%2520hierarchical%2520architecture%250Aagainst%2520server%2520failures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08577v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FBFL%3A%20A%20Field-Based%20Coordination%20Approach%20for%20Data%20Heterogeneity%20in%0A%20%20Federated%20Learning&entry.906535625=Davide%20Domini%20and%20Gianluca%20Aguzzi%20and%20Lukas%20Esterle%20and%20Mirko%20Viroli&entry.1292438233=%20%20In%20the%20last%20years%2C%20Federated%20learning%20%28FL%29%20has%20become%20a%20popular%20solution%20to%0Atrain%20machine%20learning%20models%20in%20domains%20with%20high%20privacy%20concerns.%20However%2C%0AFL%20scalability%20and%20performance%20face%20significant%20challenges%20in%20real-world%0Adeployments%20where%20data%20across%20devices%20are%20non-independently%20and%20identically%0Adistributed%20%28non-IID%29.%20The%20heterogeneity%20in%20data%20distribution%20frequently%20arises%0Afrom%20spatial%20distribution%20of%20devices%2C%20leading%20to%20degraded%20model%20performance%20in%0Athe%20absence%20of%20proper%20handling.%20Additionally%2C%20FL%20typical%20reliance%20on%0Acentralized%20architectures%20introduces%20bottlenecks%20and%20single-point-of-failure%0Arisks%2C%20particularly%20problematic%20at%20scale%20or%20in%20dynamic%20environments.%20To%20close%0Athis%20gap%2C%20we%20propose%20Field-Based%20Federated%20Learning%20%28FBFL%29%2C%20a%20novel%20approach%0Aleveraging%20macroprogramming%20and%20field%20coordination%20to%20address%20these%20limitations%0Athrough%3A%20%28i%29%20distributed%20spatial-based%20leader%20election%20for%20personalization%20to%0Amitigate%20non-IID%20data%20challenges%3B%20and%20%28ii%29%20construction%20of%20a%20self-organizing%2C%0Ahierarchical%20architecture%20using%20advanced%20macroprogramming%20patterns.%20Moreover%2C%0AFBFL%20not%20only%20overcomes%20the%20aforementioned%20limitations%2C%20but%20also%20enables%20the%0Adevelopment%20of%20more%20specialized%20models%20tailored%20to%20the%20specific%20data%0Adistribution%20in%20each%20subregion.%20This%20paper%20formalizes%20FBFL%20and%20evaluates%20it%0Aextensively%20using%20MNIST%2C%20FashionMNIST%2C%20and%20Extended%20MNIST%20datasets.%20We%0Ademonstrate%20that%2C%20when%20operating%20under%20IID%20data%20conditions%2C%20FBFL%20performs%0Acomparably%20to%20the%20widely-used%20FedAvg%20algorithm.%20Furthermore%2C%20in%20challenging%0Anon-IID%20scenarios%2C%20FBFL%20not%20only%20outperforms%20FedAvg%20but%20also%20surpasses%20other%0Astate-of-the-art%20methods%2C%20namely%20FedProx%20and%20Scaffold%2C%20which%20have%20been%0Aspecifically%20designed%20to%20address%20non-IID%20data%20distributions.%20Additionally%2C%20we%0Ashowcase%20the%20resilience%20of%20FBFL%27s%20self-organizing%20hierarchical%20architecture%0Aagainst%20server%20failures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08577v2&entry.124074799=Read"},
{"title": "Chartwin: a Case Study on Channel Charting-aided Localization in Dynamic\n  Digital Network Twins", "author": "Lorenzo Cazzella and Francesco Linsalata and Mahdi Maleki and Damiano Badini and Matteo Matteucci and Umberto Spagnolini", "abstract": "  Wireless communication systems can significantly benefit from the\navailability of spatially consistent representations of the wireless channel to\nefficiently perform a wide range of communication tasks. Towards this purpose,\nchannel charting has been introduced as an effective unsupervised learning\ntechnique to achieve both locally and globally consistent radio maps. In this\nletter, we propose Chartwin, a case study on the integration of\nlocalization-oriented channel charting with dynamic Digital Network Twins\n(DNTs). Numerical results showcase the significant performance of\nsemi-supervised channel charting in constructing a spatially consistent chart\nof the considered extended urban environment. The considered method results in\n$\\approx$ 4.5 m localization error for the static DNT and $\\approx$ 6 m in the\ndynamic DNT, fostering DNT-aided channel charting and localization.\n", "link": "http://arxiv.org/abs/2508.09055v1", "date": "2025-08-12", "relevancy": 1.9394, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5226}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4679}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chartwin%3A%20a%20Case%20Study%20on%20Channel%20Charting-aided%20Localization%20in%20Dynamic%0A%20%20Digital%20Network%20Twins&body=Title%3A%20Chartwin%3A%20a%20Case%20Study%20on%20Channel%20Charting-aided%20Localization%20in%20Dynamic%0A%20%20Digital%20Network%20Twins%0AAuthor%3A%20Lorenzo%20Cazzella%20and%20Francesco%20Linsalata%20and%20Mahdi%20Maleki%20and%20Damiano%20Badini%20and%20Matteo%20Matteucci%20and%20Umberto%20Spagnolini%0AAbstract%3A%20%20%20Wireless%20communication%20systems%20can%20significantly%20benefit%20from%20the%0Aavailability%20of%20spatially%20consistent%20representations%20of%20the%20wireless%20channel%20to%0Aefficiently%20perform%20a%20wide%20range%20of%20communication%20tasks.%20Towards%20this%20purpose%2C%0Achannel%20charting%20has%20been%20introduced%20as%20an%20effective%20unsupervised%20learning%0Atechnique%20to%20achieve%20both%20locally%20and%20globally%20consistent%20radio%20maps.%20In%20this%0Aletter%2C%20we%20propose%20Chartwin%2C%20a%20case%20study%20on%20the%20integration%20of%0Alocalization-oriented%20channel%20charting%20with%20dynamic%20Digital%20Network%20Twins%0A%28DNTs%29.%20Numerical%20results%20showcase%20the%20significant%20performance%20of%0Asemi-supervised%20channel%20charting%20in%20constructing%20a%20spatially%20consistent%20chart%0Aof%20the%20considered%20extended%20urban%20environment.%20The%20considered%20method%20results%20in%0A%24%5Capprox%24%204.5%20m%20localization%20error%20for%20the%20static%20DNT%20and%20%24%5Capprox%24%206%20m%20in%20the%0Adynamic%20DNT%2C%20fostering%20DNT-aided%20channel%20charting%20and%20localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChartwin%253A%2520a%2520Case%2520Study%2520on%2520Channel%2520Charting-aided%2520Localization%2520in%2520Dynamic%250A%2520%2520Digital%2520Network%2520Twins%26entry.906535625%3DLorenzo%2520Cazzella%2520and%2520Francesco%2520Linsalata%2520and%2520Mahdi%2520Maleki%2520and%2520Damiano%2520Badini%2520and%2520Matteo%2520Matteucci%2520and%2520Umberto%2520Spagnolini%26entry.1292438233%3D%2520%2520Wireless%2520communication%2520systems%2520can%2520significantly%2520benefit%2520from%2520the%250Aavailability%2520of%2520spatially%2520consistent%2520representations%2520of%2520the%2520wireless%2520channel%2520to%250Aefficiently%2520perform%2520a%2520wide%2520range%2520of%2520communication%2520tasks.%2520Towards%2520this%2520purpose%252C%250Achannel%2520charting%2520has%2520been%2520introduced%2520as%2520an%2520effective%2520unsupervised%2520learning%250Atechnique%2520to%2520achieve%2520both%2520locally%2520and%2520globally%2520consistent%2520radio%2520maps.%2520In%2520this%250Aletter%252C%2520we%2520propose%2520Chartwin%252C%2520a%2520case%2520study%2520on%2520the%2520integration%2520of%250Alocalization-oriented%2520channel%2520charting%2520with%2520dynamic%2520Digital%2520Network%2520Twins%250A%2528DNTs%2529.%2520Numerical%2520results%2520showcase%2520the%2520significant%2520performance%2520of%250Asemi-supervised%2520channel%2520charting%2520in%2520constructing%2520a%2520spatially%2520consistent%2520chart%250Aof%2520the%2520considered%2520extended%2520urban%2520environment.%2520The%2520considered%2520method%2520results%2520in%250A%2524%255Capprox%2524%25204.5%2520m%2520localization%2520error%2520for%2520the%2520static%2520DNT%2520and%2520%2524%255Capprox%2524%25206%2520m%2520in%2520the%250Adynamic%2520DNT%252C%2520fostering%2520DNT-aided%2520channel%2520charting%2520and%2520localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chartwin%3A%20a%20Case%20Study%20on%20Channel%20Charting-aided%20Localization%20in%20Dynamic%0A%20%20Digital%20Network%20Twins&entry.906535625=Lorenzo%20Cazzella%20and%20Francesco%20Linsalata%20and%20Mahdi%20Maleki%20and%20Damiano%20Badini%20and%20Matteo%20Matteucci%20and%20Umberto%20Spagnolini&entry.1292438233=%20%20Wireless%20communication%20systems%20can%20significantly%20benefit%20from%20the%0Aavailability%20of%20spatially%20consistent%20representations%20of%20the%20wireless%20channel%20to%0Aefficiently%20perform%20a%20wide%20range%20of%20communication%20tasks.%20Towards%20this%20purpose%2C%0Achannel%20charting%20has%20been%20introduced%20as%20an%20effective%20unsupervised%20learning%0Atechnique%20to%20achieve%20both%20locally%20and%20globally%20consistent%20radio%20maps.%20In%20this%0Aletter%2C%20we%20propose%20Chartwin%2C%20a%20case%20study%20on%20the%20integration%20of%0Alocalization-oriented%20channel%20charting%20with%20dynamic%20Digital%20Network%20Twins%0A%28DNTs%29.%20Numerical%20results%20showcase%20the%20significant%20performance%20of%0Asemi-supervised%20channel%20charting%20in%20constructing%20a%20spatially%20consistent%20chart%0Aof%20the%20considered%20extended%20urban%20environment.%20The%20considered%20method%20results%20in%0A%24%5Capprox%24%204.5%20m%20localization%20error%20for%20the%20static%20DNT%20and%20%24%5Capprox%24%206%20m%20in%20the%0Adynamic%20DNT%2C%20fostering%20DNT-aided%20channel%20charting%20and%20localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09055v1&entry.124074799=Read"},
{"title": "Echo: Decoupling Inference and Training for Large-Scale RL Alignment on\n  Heterogeneous Swarms", "author": "Jie Xiao and Changyuan Fan and Qingnan Ren and Alfred Long and Yuchen Zhang and Rymon Yu and Eric Yang and Lynn Ai and Shaoduo Gan", "abstract": "  Modern RL-based post-training for large language models (LLMs) co-locate\ntrajectory sampling and policy optimisation on the same GPU cluster, forcing\nthe system to switch between inference and training workloads. This serial\ncontext switching violates the single-program-multiple-data (SPMD) assumption\nunderlying today's distributed training systems. We present Echo, the RL system\nthat cleanly decouples these two phases across heterogeneous \"inference\" and\n\"training\" swarms while preserving statistical efficiency. Echo introduces two\nlightweight synchronization protocols: a sequential pull mode that refreshes\npolicy weights according to API call for minimal bias, and an asynchronous\npush-pull mode that streams version-tagged rollouts through a replay buffer to\nmaximise hardware utilisation. Training four representative RL workloads with\nQwen3-4B, Qwen2.5-7B, Qwen3-30B-A3B-Thinking-2507 and Qwen3-32B on a\ngeographically distributed cluster, Echo matches a fully co-located Verl\nbaseline in convergence speed and final reward while off-loading trajectory\ngeneration to commodity edge hardware. These promising results demonstrate that\nlarge-scale RL for LLMs could achieve datacentre-grade performance using\ndecentralised, heterogeneous resources.\n", "link": "http://arxiv.org/abs/2508.05387v3", "date": "2025-08-12", "relevancy": 1.8855, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4858}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4614}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Echo%3A%20Decoupling%20Inference%20and%20Training%20for%20Large-Scale%20RL%20Alignment%20on%0A%20%20Heterogeneous%20Swarms&body=Title%3A%20Echo%3A%20Decoupling%20Inference%20and%20Training%20for%20Large-Scale%20RL%20Alignment%20on%0A%20%20Heterogeneous%20Swarms%0AAuthor%3A%20Jie%20Xiao%20and%20Changyuan%20Fan%20and%20Qingnan%20Ren%20and%20Alfred%20Long%20and%20Yuchen%20Zhang%20and%20Rymon%20Yu%20and%20Eric%20Yang%20and%20Lynn%20Ai%20and%20Shaoduo%20Gan%0AAbstract%3A%20%20%20Modern%20RL-based%20post-training%20for%20large%20language%20models%20%28LLMs%29%20co-locate%0Atrajectory%20sampling%20and%20policy%20optimisation%20on%20the%20same%20GPU%20cluster%2C%20forcing%0Athe%20system%20to%20switch%20between%20inference%20and%20training%20workloads.%20This%20serial%0Acontext%20switching%20violates%20the%20single-program-multiple-data%20%28SPMD%29%20assumption%0Aunderlying%20today%27s%20distributed%20training%20systems.%20We%20present%20Echo%2C%20the%20RL%20system%0Athat%20cleanly%20decouples%20these%20two%20phases%20across%20heterogeneous%20%22inference%22%20and%0A%22training%22%20swarms%20while%20preserving%20statistical%20efficiency.%20Echo%20introduces%20two%0Alightweight%20synchronization%20protocols%3A%20a%20sequential%20pull%20mode%20that%20refreshes%0Apolicy%20weights%20according%20to%20API%20call%20for%20minimal%20bias%2C%20and%20an%20asynchronous%0Apush-pull%20mode%20that%20streams%20version-tagged%20rollouts%20through%20a%20replay%20buffer%20to%0Amaximise%20hardware%20utilisation.%20Training%20four%20representative%20RL%20workloads%20with%0AQwen3-4B%2C%20Qwen2.5-7B%2C%20Qwen3-30B-A3B-Thinking-2507%20and%20Qwen3-32B%20on%20a%0Ageographically%20distributed%20cluster%2C%20Echo%20matches%20a%20fully%20co-located%20Verl%0Abaseline%20in%20convergence%20speed%20and%20final%20reward%20while%20off-loading%20trajectory%0Ageneration%20to%20commodity%20edge%20hardware.%20These%20promising%20results%20demonstrate%20that%0Alarge-scale%20RL%20for%20LLMs%20could%20achieve%20datacentre-grade%20performance%20using%0Adecentralised%2C%20heterogeneous%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05387v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEcho%253A%2520Decoupling%2520Inference%2520and%2520Training%2520for%2520Large-Scale%2520RL%2520Alignment%2520on%250A%2520%2520Heterogeneous%2520Swarms%26entry.906535625%3DJie%2520Xiao%2520and%2520Changyuan%2520Fan%2520and%2520Qingnan%2520Ren%2520and%2520Alfred%2520Long%2520and%2520Yuchen%2520Zhang%2520and%2520Rymon%2520Yu%2520and%2520Eric%2520Yang%2520and%2520Lynn%2520Ai%2520and%2520Shaoduo%2520Gan%26entry.1292438233%3D%2520%2520Modern%2520RL-based%2520post-training%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520co-locate%250Atrajectory%2520sampling%2520and%2520policy%2520optimisation%2520on%2520the%2520same%2520GPU%2520cluster%252C%2520forcing%250Athe%2520system%2520to%2520switch%2520between%2520inference%2520and%2520training%2520workloads.%2520This%2520serial%250Acontext%2520switching%2520violates%2520the%2520single-program-multiple-data%2520%2528SPMD%2529%2520assumption%250Aunderlying%2520today%2527s%2520distributed%2520training%2520systems.%2520We%2520present%2520Echo%252C%2520the%2520RL%2520system%250Athat%2520cleanly%2520decouples%2520these%2520two%2520phases%2520across%2520heterogeneous%2520%2522inference%2522%2520and%250A%2522training%2522%2520swarms%2520while%2520preserving%2520statistical%2520efficiency.%2520Echo%2520introduces%2520two%250Alightweight%2520synchronization%2520protocols%253A%2520a%2520sequential%2520pull%2520mode%2520that%2520refreshes%250Apolicy%2520weights%2520according%2520to%2520API%2520call%2520for%2520minimal%2520bias%252C%2520and%2520an%2520asynchronous%250Apush-pull%2520mode%2520that%2520streams%2520version-tagged%2520rollouts%2520through%2520a%2520replay%2520buffer%2520to%250Amaximise%2520hardware%2520utilisation.%2520Training%2520four%2520representative%2520RL%2520workloads%2520with%250AQwen3-4B%252C%2520Qwen2.5-7B%252C%2520Qwen3-30B-A3B-Thinking-2507%2520and%2520Qwen3-32B%2520on%2520a%250Ageographically%2520distributed%2520cluster%252C%2520Echo%2520matches%2520a%2520fully%2520co-located%2520Verl%250Abaseline%2520in%2520convergence%2520speed%2520and%2520final%2520reward%2520while%2520off-loading%2520trajectory%250Ageneration%2520to%2520commodity%2520edge%2520hardware.%2520These%2520promising%2520results%2520demonstrate%2520that%250Alarge-scale%2520RL%2520for%2520LLMs%2520could%2520achieve%2520datacentre-grade%2520performance%2520using%250Adecentralised%252C%2520heterogeneous%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05387v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Echo%3A%20Decoupling%20Inference%20and%20Training%20for%20Large-Scale%20RL%20Alignment%20on%0A%20%20Heterogeneous%20Swarms&entry.906535625=Jie%20Xiao%20and%20Changyuan%20Fan%20and%20Qingnan%20Ren%20and%20Alfred%20Long%20and%20Yuchen%20Zhang%20and%20Rymon%20Yu%20and%20Eric%20Yang%20and%20Lynn%20Ai%20and%20Shaoduo%20Gan&entry.1292438233=%20%20Modern%20RL-based%20post-training%20for%20large%20language%20models%20%28LLMs%29%20co-locate%0Atrajectory%20sampling%20and%20policy%20optimisation%20on%20the%20same%20GPU%20cluster%2C%20forcing%0Athe%20system%20to%20switch%20between%20inference%20and%20training%20workloads.%20This%20serial%0Acontext%20switching%20violates%20the%20single-program-multiple-data%20%28SPMD%29%20assumption%0Aunderlying%20today%27s%20distributed%20training%20systems.%20We%20present%20Echo%2C%20the%20RL%20system%0Athat%20cleanly%20decouples%20these%20two%20phases%20across%20heterogeneous%20%22inference%22%20and%0A%22training%22%20swarms%20while%20preserving%20statistical%20efficiency.%20Echo%20introduces%20two%0Alightweight%20synchronization%20protocols%3A%20a%20sequential%20pull%20mode%20that%20refreshes%0Apolicy%20weights%20according%20to%20API%20call%20for%20minimal%20bias%2C%20and%20an%20asynchronous%0Apush-pull%20mode%20that%20streams%20version-tagged%20rollouts%20through%20a%20replay%20buffer%20to%0Amaximise%20hardware%20utilisation.%20Training%20four%20representative%20RL%20workloads%20with%0AQwen3-4B%2C%20Qwen2.5-7B%2C%20Qwen3-30B-A3B-Thinking-2507%20and%20Qwen3-32B%20on%20a%0Ageographically%20distributed%20cluster%2C%20Echo%20matches%20a%20fully%20co-located%20Verl%0Abaseline%20in%20convergence%20speed%20and%20final%20reward%20while%20off-loading%20trajectory%0Ageneration%20to%20commodity%20edge%20hardware.%20These%20promising%20results%20demonstrate%20that%0Alarge-scale%20RL%20for%20LLMs%20could%20achieve%20datacentre-grade%20performance%20using%0Adecentralised%2C%20heterogeneous%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05387v3&entry.124074799=Read"},
{"title": "FetFIDS: A Feature Embedding Attention based Federated Network Intrusion\n  Detection Algorithm", "author": "Shreya Ghosh and Abu Shafin Mohammad Mahdee Jameel and Aly El Gamal", "abstract": "  Intrusion Detection Systems (IDS) have an increasingly important role in\npreventing exploitation of network vulnerabilities by malicious actors. Recent\ndeep learning based developments have resulted in significant improvements in\nthe performance of IDS systems. In this paper, we present FetFIDS, where we\nexplore the employment of feature embedding instead of positional embedding to\nimprove intrusion detection performance of a transformer based deep learning\nsystem. Our model is developed with the aim of deployments in edge learning\nscenarios, where federated learning over multiple communication rounds can\nensure both privacy and localized performance improvements. FetFIDS outperforms\nmultiple state-of-the-art intrusion detection systems in a federated\nenvironment and demonstrates a high degree of suitability to federated\nlearning. The code for this work can be found at\nhttps://github.com/ghosh64/fetfids.\n", "link": "http://arxiv.org/abs/2508.09056v1", "date": "2025-08-12", "relevancy": 1.8766, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4943}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4521}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FetFIDS%3A%20A%20Feature%20Embedding%20Attention%20based%20Federated%20Network%20Intrusion%0A%20%20Detection%20Algorithm&body=Title%3A%20FetFIDS%3A%20A%20Feature%20Embedding%20Attention%20based%20Federated%20Network%20Intrusion%0A%20%20Detection%20Algorithm%0AAuthor%3A%20Shreya%20Ghosh%20and%20Abu%20Shafin%20Mohammad%20Mahdee%20Jameel%20and%20Aly%20El%20Gamal%0AAbstract%3A%20%20%20Intrusion%20Detection%20Systems%20%28IDS%29%20have%20an%20increasingly%20important%20role%20in%0Apreventing%20exploitation%20of%20network%20vulnerabilities%20by%20malicious%20actors.%20Recent%0Adeep%20learning%20based%20developments%20have%20resulted%20in%20significant%20improvements%20in%0Athe%20performance%20of%20IDS%20systems.%20In%20this%20paper%2C%20we%20present%20FetFIDS%2C%20where%20we%0Aexplore%20the%20employment%20of%20feature%20embedding%20instead%20of%20positional%20embedding%20to%0Aimprove%20intrusion%20detection%20performance%20of%20a%20transformer%20based%20deep%20learning%0Asystem.%20Our%20model%20is%20developed%20with%20the%20aim%20of%20deployments%20in%20edge%20learning%0Ascenarios%2C%20where%20federated%20learning%20over%20multiple%20communication%20rounds%20can%0Aensure%20both%20privacy%20and%20localized%20performance%20improvements.%20FetFIDS%20outperforms%0Amultiple%20state-of-the-art%20intrusion%20detection%20systems%20in%20a%20federated%0Aenvironment%20and%20demonstrates%20a%20high%20degree%20of%20suitability%20to%20federated%0Alearning.%20The%20code%20for%20this%20work%20can%20be%20found%20at%0Ahttps%3A//github.com/ghosh64/fetfids.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFetFIDS%253A%2520A%2520Feature%2520Embedding%2520Attention%2520based%2520Federated%2520Network%2520Intrusion%250A%2520%2520Detection%2520Algorithm%26entry.906535625%3DShreya%2520Ghosh%2520and%2520Abu%2520Shafin%2520Mohammad%2520Mahdee%2520Jameel%2520and%2520Aly%2520El%2520Gamal%26entry.1292438233%3D%2520%2520Intrusion%2520Detection%2520Systems%2520%2528IDS%2529%2520have%2520an%2520increasingly%2520important%2520role%2520in%250Apreventing%2520exploitation%2520of%2520network%2520vulnerabilities%2520by%2520malicious%2520actors.%2520Recent%250Adeep%2520learning%2520based%2520developments%2520have%2520resulted%2520in%2520significant%2520improvements%2520in%250Athe%2520performance%2520of%2520IDS%2520systems.%2520In%2520this%2520paper%252C%2520we%2520present%2520FetFIDS%252C%2520where%2520we%250Aexplore%2520the%2520employment%2520of%2520feature%2520embedding%2520instead%2520of%2520positional%2520embedding%2520to%250Aimprove%2520intrusion%2520detection%2520performance%2520of%2520a%2520transformer%2520based%2520deep%2520learning%250Asystem.%2520Our%2520model%2520is%2520developed%2520with%2520the%2520aim%2520of%2520deployments%2520in%2520edge%2520learning%250Ascenarios%252C%2520where%2520federated%2520learning%2520over%2520multiple%2520communication%2520rounds%2520can%250Aensure%2520both%2520privacy%2520and%2520localized%2520performance%2520improvements.%2520FetFIDS%2520outperforms%250Amultiple%2520state-of-the-art%2520intrusion%2520detection%2520systems%2520in%2520a%2520federated%250Aenvironment%2520and%2520demonstrates%2520a%2520high%2520degree%2520of%2520suitability%2520to%2520federated%250Alearning.%2520The%2520code%2520for%2520this%2520work%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/ghosh64/fetfids.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FetFIDS%3A%20A%20Feature%20Embedding%20Attention%20based%20Federated%20Network%20Intrusion%0A%20%20Detection%20Algorithm&entry.906535625=Shreya%20Ghosh%20and%20Abu%20Shafin%20Mohammad%20Mahdee%20Jameel%20and%20Aly%20El%20Gamal&entry.1292438233=%20%20Intrusion%20Detection%20Systems%20%28IDS%29%20have%20an%20increasingly%20important%20role%20in%0Apreventing%20exploitation%20of%20network%20vulnerabilities%20by%20malicious%20actors.%20Recent%0Adeep%20learning%20based%20developments%20have%20resulted%20in%20significant%20improvements%20in%0Athe%20performance%20of%20IDS%20systems.%20In%20this%20paper%2C%20we%20present%20FetFIDS%2C%20where%20we%0Aexplore%20the%20employment%20of%20feature%20embedding%20instead%20of%20positional%20embedding%20to%0Aimprove%20intrusion%20detection%20performance%20of%20a%20transformer%20based%20deep%20learning%0Asystem.%20Our%20model%20is%20developed%20with%20the%20aim%20of%20deployments%20in%20edge%20learning%0Ascenarios%2C%20where%20federated%20learning%20over%20multiple%20communication%20rounds%20can%0Aensure%20both%20privacy%20and%20localized%20performance%20improvements.%20FetFIDS%20outperforms%0Amultiple%20state-of-the-art%20intrusion%20detection%20systems%20in%20a%20federated%0Aenvironment%20and%20demonstrates%20a%20high%20degree%20of%20suitability%20to%20federated%0Alearning.%20The%20code%20for%20this%20work%20can%20be%20found%20at%0Ahttps%3A//github.com/ghosh64/fetfids.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09056v1&entry.124074799=Read"},
{"title": "SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG\n  Controlling", "author": "Shixuan Sun and Siyuan Liang and Ruoyu Chen and Jianjie Huang and Jingzhi Li and Xiaochun Cao", "abstract": "  Retrieval-Augmented Generation (RAG) and its Multimodal Retrieval-Augmented\nGeneration (MRAG) significantly improve the knowledge coverage and contextual\nunderstanding of Large Language Models (LLMs) by introducing external knowledge\nsources. However, retrieval and multimodal fusion obscure content provenance,\nrendering existing membership inference methods unable to reliably attribute\ngenerated outputs to pre-training, external retrieval, or user input, thus\nundermining privacy leakage accountability\n  To address these challenges, we propose the first Source-aware Membership\nAudit (SMA) that enables fine-grained source attribution of generated content\nin a semi-black-box setting with retrieval control capabilities.To address the\nenvironmental constraints of semi-black-box auditing, we further design an\nattribution estimation mechanism based on zero-order optimization, which\nrobustly approximates the true influence of input tokens on the output through\nlarge-scale perturbation sampling and ridge regression modeling. In addition,\nSMA introduces a cross-modal attribution technique that projects image inputs\ninto textual descriptions via MLLMs, enabling token-level attribution in the\ntext modality, which for the first time facilitates membership inference on\nimage retrieval traces in MRAG systems. This work shifts the focus of\nmembership inference from 'whether the data has been memorized' to 'where the\ncontent is sourced from', offering a novel perspective for auditing data\nprovenance in complex generative systems.\n", "link": "http://arxiv.org/abs/2508.09105v1", "date": "2025-08-12", "relevancy": 1.8758, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4742}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4733}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMA%3A%20Who%20Said%20That%3F%20Auditing%20Membership%20Leakage%20in%20Semi-Black-box%20RAG%0A%20%20Controlling&body=Title%3A%20SMA%3A%20Who%20Said%20That%3F%20Auditing%20Membership%20Leakage%20in%20Semi-Black-box%20RAG%0A%20%20Controlling%0AAuthor%3A%20Shixuan%20Sun%20and%20Siyuan%20Liang%20and%20Ruoyu%20Chen%20and%20Jianjie%20Huang%20and%20Jingzhi%20Li%20and%20Xiaochun%20Cao%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20and%20its%20Multimodal%20Retrieval-Augmented%0AGeneration%20%28MRAG%29%20significantly%20improve%20the%20knowledge%20coverage%20and%20contextual%0Aunderstanding%20of%20Large%20Language%20Models%20%28LLMs%29%20by%20introducing%20external%20knowledge%0Asources.%20However%2C%20retrieval%20and%20multimodal%20fusion%20obscure%20content%20provenance%2C%0Arendering%20existing%20membership%20inference%20methods%20unable%20to%20reliably%20attribute%0Agenerated%20outputs%20to%20pre-training%2C%20external%20retrieval%2C%20or%20user%20input%2C%20thus%0Aundermining%20privacy%20leakage%20accountability%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20the%20first%20Source-aware%20Membership%0AAudit%20%28SMA%29%20that%20enables%20fine-grained%20source%20attribution%20of%20generated%20content%0Ain%20a%20semi-black-box%20setting%20with%20retrieval%20control%20capabilities.To%20address%20the%0Aenvironmental%20constraints%20of%20semi-black-box%20auditing%2C%20we%20further%20design%20an%0Aattribution%20estimation%20mechanism%20based%20on%20zero-order%20optimization%2C%20which%0Arobustly%20approximates%20the%20true%20influence%20of%20input%20tokens%20on%20the%20output%20through%0Alarge-scale%20perturbation%20sampling%20and%20ridge%20regression%20modeling.%20In%20addition%2C%0ASMA%20introduces%20a%20cross-modal%20attribution%20technique%20that%20projects%20image%20inputs%0Ainto%20textual%20descriptions%20via%20MLLMs%2C%20enabling%20token-level%20attribution%20in%20the%0Atext%20modality%2C%20which%20for%20the%20first%20time%20facilitates%20membership%20inference%20on%0Aimage%20retrieval%20traces%20in%20MRAG%20systems.%20This%20work%20shifts%20the%20focus%20of%0Amembership%20inference%20from%20%27whether%20the%20data%20has%20been%20memorized%27%20to%20%27where%20the%0Acontent%20is%20sourced%20from%27%2C%20offering%20a%20novel%20perspective%20for%20auditing%20data%0Aprovenance%20in%20complex%20generative%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMA%253A%2520Who%2520Said%2520That%253F%2520Auditing%2520Membership%2520Leakage%2520in%2520Semi-Black-box%2520RAG%250A%2520%2520Controlling%26entry.906535625%3DShixuan%2520Sun%2520and%2520Siyuan%2520Liang%2520and%2520Ruoyu%2520Chen%2520and%2520Jianjie%2520Huang%2520and%2520Jingzhi%2520Li%2520and%2520Xiaochun%2520Cao%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520and%2520its%2520Multimodal%2520Retrieval-Augmented%250AGeneration%2520%2528MRAG%2529%2520significantly%2520improve%2520the%2520knowledge%2520coverage%2520and%2520contextual%250Aunderstanding%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520by%2520introducing%2520external%2520knowledge%250Asources.%2520However%252C%2520retrieval%2520and%2520multimodal%2520fusion%2520obscure%2520content%2520provenance%252C%250Arendering%2520existing%2520membership%2520inference%2520methods%2520unable%2520to%2520reliably%2520attribute%250Agenerated%2520outputs%2520to%2520pre-training%252C%2520external%2520retrieval%252C%2520or%2520user%2520input%252C%2520thus%250Aundermining%2520privacy%2520leakage%2520accountability%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520the%2520first%2520Source-aware%2520Membership%250AAudit%2520%2528SMA%2529%2520that%2520enables%2520fine-grained%2520source%2520attribution%2520of%2520generated%2520content%250Ain%2520a%2520semi-black-box%2520setting%2520with%2520retrieval%2520control%2520capabilities.To%2520address%2520the%250Aenvironmental%2520constraints%2520of%2520semi-black-box%2520auditing%252C%2520we%2520further%2520design%2520an%250Aattribution%2520estimation%2520mechanism%2520based%2520on%2520zero-order%2520optimization%252C%2520which%250Arobustly%2520approximates%2520the%2520true%2520influence%2520of%2520input%2520tokens%2520on%2520the%2520output%2520through%250Alarge-scale%2520perturbation%2520sampling%2520and%2520ridge%2520regression%2520modeling.%2520In%2520addition%252C%250ASMA%2520introduces%2520a%2520cross-modal%2520attribution%2520technique%2520that%2520projects%2520image%2520inputs%250Ainto%2520textual%2520descriptions%2520via%2520MLLMs%252C%2520enabling%2520token-level%2520attribution%2520in%2520the%250Atext%2520modality%252C%2520which%2520for%2520the%2520first%2520time%2520facilitates%2520membership%2520inference%2520on%250Aimage%2520retrieval%2520traces%2520in%2520MRAG%2520systems.%2520This%2520work%2520shifts%2520the%2520focus%2520of%250Amembership%2520inference%2520from%2520%2527whether%2520the%2520data%2520has%2520been%2520memorized%2527%2520to%2520%2527where%2520the%250Acontent%2520is%2520sourced%2520from%2527%252C%2520offering%2520a%2520novel%2520perspective%2520for%2520auditing%2520data%250Aprovenance%2520in%2520complex%2520generative%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMA%3A%20Who%20Said%20That%3F%20Auditing%20Membership%20Leakage%20in%20Semi-Black-box%20RAG%0A%20%20Controlling&entry.906535625=Shixuan%20Sun%20and%20Siyuan%20Liang%20and%20Ruoyu%20Chen%20and%20Jianjie%20Huang%20and%20Jingzhi%20Li%20and%20Xiaochun%20Cao&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20and%20its%20Multimodal%20Retrieval-Augmented%0AGeneration%20%28MRAG%29%20significantly%20improve%20the%20knowledge%20coverage%20and%20contextual%0Aunderstanding%20of%20Large%20Language%20Models%20%28LLMs%29%20by%20introducing%20external%20knowledge%0Asources.%20However%2C%20retrieval%20and%20multimodal%20fusion%20obscure%20content%20provenance%2C%0Arendering%20existing%20membership%20inference%20methods%20unable%20to%20reliably%20attribute%0Agenerated%20outputs%20to%20pre-training%2C%20external%20retrieval%2C%20or%20user%20input%2C%20thus%0Aundermining%20privacy%20leakage%20accountability%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20the%20first%20Source-aware%20Membership%0AAudit%20%28SMA%29%20that%20enables%20fine-grained%20source%20attribution%20of%20generated%20content%0Ain%20a%20semi-black-box%20setting%20with%20retrieval%20control%20capabilities.To%20address%20the%0Aenvironmental%20constraints%20of%20semi-black-box%20auditing%2C%20we%20further%20design%20an%0Aattribution%20estimation%20mechanism%20based%20on%20zero-order%20optimization%2C%20which%0Arobustly%20approximates%20the%20true%20influence%20of%20input%20tokens%20on%20the%20output%20through%0Alarge-scale%20perturbation%20sampling%20and%20ridge%20regression%20modeling.%20In%20addition%2C%0ASMA%20introduces%20a%20cross-modal%20attribution%20technique%20that%20projects%20image%20inputs%0Ainto%20textual%20descriptions%20via%20MLLMs%2C%20enabling%20token-level%20attribution%20in%20the%0Atext%20modality%2C%20which%20for%20the%20first%20time%20facilitates%20membership%20inference%20on%0Aimage%20retrieval%20traces%20in%20MRAG%20systems.%20This%20work%20shifts%20the%20focus%20of%0Amembership%20inference%20from%20%27whether%20the%20data%20has%20been%20memorized%27%20to%20%27where%20the%0Acontent%20is%20sourced%20from%27%2C%20offering%20a%20novel%20perspective%20for%20auditing%20data%0Aprovenance%20in%20complex%20generative%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09105v1&entry.124074799=Read"},
{"title": "Understanding Aggregations of Proper Learners in Multiclass\n  Classification", "author": "Julian Asilis and Mikael M\u00f8ller H\u00f8gsgaard and Grigoris Velegkas", "abstract": "  Multiclass learnability is known to exhibit a properness barrier: there are\nlearnable classes which cannot be learned by any proper learner. Binary\nclassification faces no such barrier for learnability, but a similar one for\noptimal learning, which can in general only be achieved by improper learners.\nFortunately, recent advances in binary classification have demonstrated that\nthis requirement can be satisfied using aggregations of proper learners, some\nof which are strikingly simple. This raises a natural question: to what extent\ncan simple aggregations of proper learners overcome the properness barrier in\nmulticlass classification?\n  We give a positive answer to this question for classes which have finite\nGraph dimension, $d_G$. Namely, we demonstrate that the optimal binary learners\nof Hanneke, Larsen, and Aden-Ali et al. (appropriately generalized to the\nmulticlass setting) achieve sample complexity $O\\left(\\frac{d_G + \\ln(1 /\n\\delta)}{\\epsilon}\\right)$. This forms a strict improvement upon the sample\ncomplexity of ERM. We complement this with a lower bound demonstrating that for\ncertain classes of Graph dimension $d_G$, majorities of ERM learners require\n$\\Omega \\left( \\frac{d_G + \\ln(1 / \\delta)}{\\epsilon}\\right)$ samples.\nFurthermore, we show that a single ERM requires $\\Omega \\left(\\frac{d_G \\ln(1 /\n\\epsilon) + \\ln(1 / \\delta)}{\\epsilon}\\right)$ samples on such classes,\nexceeding the lower bound of Daniely et al. (2015) by a factor of $\\ln(1 /\n\\epsilon)$. For multiclass learning in full generality -- i.e., for classes of\nfinite DS dimension but possibly infinite Graph dimension -- we give a strong\nrefutation to these learning strategies, by exhibiting a learnable class which\ncannot be learned to constant error by any aggregation of a finite number of\nproper learners.\n", "link": "http://arxiv.org/abs/2410.22749v2", "date": "2025-08-12", "relevancy": 1.875, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4777}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4726}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Aggregations%20of%20Proper%20Learners%20in%20Multiclass%0A%20%20Classification&body=Title%3A%20Understanding%20Aggregations%20of%20Proper%20Learners%20in%20Multiclass%0A%20%20Classification%0AAuthor%3A%20Julian%20Asilis%20and%20Mikael%20M%C3%B8ller%20H%C3%B8gsgaard%20and%20Grigoris%20Velegkas%0AAbstract%3A%20%20%20Multiclass%20learnability%20is%20known%20to%20exhibit%20a%20properness%20barrier%3A%20there%20are%0Alearnable%20classes%20which%20cannot%20be%20learned%20by%20any%20proper%20learner.%20Binary%0Aclassification%20faces%20no%20such%20barrier%20for%20learnability%2C%20but%20a%20similar%20one%20for%0Aoptimal%20learning%2C%20which%20can%20in%20general%20only%20be%20achieved%20by%20improper%20learners.%0AFortunately%2C%20recent%20advances%20in%20binary%20classification%20have%20demonstrated%20that%0Athis%20requirement%20can%20be%20satisfied%20using%20aggregations%20of%20proper%20learners%2C%20some%0Aof%20which%20are%20strikingly%20simple.%20This%20raises%20a%20natural%20question%3A%20to%20what%20extent%0Acan%20simple%20aggregations%20of%20proper%20learners%20overcome%20the%20properness%20barrier%20in%0Amulticlass%20classification%3F%0A%20%20We%20give%20a%20positive%20answer%20to%20this%20question%20for%20classes%20which%20have%20finite%0AGraph%20dimension%2C%20%24d_G%24.%20Namely%2C%20we%20demonstrate%20that%20the%20optimal%20binary%20learners%0Aof%20Hanneke%2C%20Larsen%2C%20and%20Aden-Ali%20et%20al.%20%28appropriately%20generalized%20to%20the%0Amulticlass%20setting%29%20achieve%20sample%20complexity%20%24O%5Cleft%28%5Cfrac%7Bd_G%20%2B%20%5Cln%281%20/%0A%5Cdelta%29%7D%7B%5Cepsilon%7D%5Cright%29%24.%20This%20forms%20a%20strict%20improvement%20upon%20the%20sample%0Acomplexity%20of%20ERM.%20We%20complement%20this%20with%20a%20lower%20bound%20demonstrating%20that%20for%0Acertain%20classes%20of%20Graph%20dimension%20%24d_G%24%2C%20majorities%20of%20ERM%20learners%20require%0A%24%5COmega%20%5Cleft%28%20%5Cfrac%7Bd_G%20%2B%20%5Cln%281%20/%20%5Cdelta%29%7D%7B%5Cepsilon%7D%5Cright%29%24%20samples.%0AFurthermore%2C%20we%20show%20that%20a%20single%20ERM%20requires%20%24%5COmega%20%5Cleft%28%5Cfrac%7Bd_G%20%5Cln%281%20/%0A%5Cepsilon%29%20%2B%20%5Cln%281%20/%20%5Cdelta%29%7D%7B%5Cepsilon%7D%5Cright%29%24%20samples%20on%20such%20classes%2C%0Aexceeding%20the%20lower%20bound%20of%20Daniely%20et%20al.%20%282015%29%20by%20a%20factor%20of%20%24%5Cln%281%20/%0A%5Cepsilon%29%24.%20For%20multiclass%20learning%20in%20full%20generality%20--%20i.e.%2C%20for%20classes%20of%0Afinite%20DS%20dimension%20but%20possibly%20infinite%20Graph%20dimension%20--%20we%20give%20a%20strong%0Arefutation%20to%20these%20learning%20strategies%2C%20by%20exhibiting%20a%20learnable%20class%20which%0Acannot%20be%20learned%20to%20constant%20error%20by%20any%20aggregation%20of%20a%20finite%20number%20of%0Aproper%20learners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22749v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Aggregations%2520of%2520Proper%2520Learners%2520in%2520Multiclass%250A%2520%2520Classification%26entry.906535625%3DJulian%2520Asilis%2520and%2520Mikael%2520M%25C3%25B8ller%2520H%25C3%25B8gsgaard%2520and%2520Grigoris%2520Velegkas%26entry.1292438233%3D%2520%2520Multiclass%2520learnability%2520is%2520known%2520to%2520exhibit%2520a%2520properness%2520barrier%253A%2520there%2520are%250Alearnable%2520classes%2520which%2520cannot%2520be%2520learned%2520by%2520any%2520proper%2520learner.%2520Binary%250Aclassification%2520faces%2520no%2520such%2520barrier%2520for%2520learnability%252C%2520but%2520a%2520similar%2520one%2520for%250Aoptimal%2520learning%252C%2520which%2520can%2520in%2520general%2520only%2520be%2520achieved%2520by%2520improper%2520learners.%250AFortunately%252C%2520recent%2520advances%2520in%2520binary%2520classification%2520have%2520demonstrated%2520that%250Athis%2520requirement%2520can%2520be%2520satisfied%2520using%2520aggregations%2520of%2520proper%2520learners%252C%2520some%250Aof%2520which%2520are%2520strikingly%2520simple.%2520This%2520raises%2520a%2520natural%2520question%253A%2520to%2520what%2520extent%250Acan%2520simple%2520aggregations%2520of%2520proper%2520learners%2520overcome%2520the%2520properness%2520barrier%2520in%250Amulticlass%2520classification%253F%250A%2520%2520We%2520give%2520a%2520positive%2520answer%2520to%2520this%2520question%2520for%2520classes%2520which%2520have%2520finite%250AGraph%2520dimension%252C%2520%2524d_G%2524.%2520Namely%252C%2520we%2520demonstrate%2520that%2520the%2520optimal%2520binary%2520learners%250Aof%2520Hanneke%252C%2520Larsen%252C%2520and%2520Aden-Ali%2520et%2520al.%2520%2528appropriately%2520generalized%2520to%2520the%250Amulticlass%2520setting%2529%2520achieve%2520sample%2520complexity%2520%2524O%255Cleft%2528%255Cfrac%257Bd_G%2520%252B%2520%255Cln%25281%2520/%250A%255Cdelta%2529%257D%257B%255Cepsilon%257D%255Cright%2529%2524.%2520This%2520forms%2520a%2520strict%2520improvement%2520upon%2520the%2520sample%250Acomplexity%2520of%2520ERM.%2520We%2520complement%2520this%2520with%2520a%2520lower%2520bound%2520demonstrating%2520that%2520for%250Acertain%2520classes%2520of%2520Graph%2520dimension%2520%2524d_G%2524%252C%2520majorities%2520of%2520ERM%2520learners%2520require%250A%2524%255COmega%2520%255Cleft%2528%2520%255Cfrac%257Bd_G%2520%252B%2520%255Cln%25281%2520/%2520%255Cdelta%2529%257D%257B%255Cepsilon%257D%255Cright%2529%2524%2520samples.%250AFurthermore%252C%2520we%2520show%2520that%2520a%2520single%2520ERM%2520requires%2520%2524%255COmega%2520%255Cleft%2528%255Cfrac%257Bd_G%2520%255Cln%25281%2520/%250A%255Cepsilon%2529%2520%252B%2520%255Cln%25281%2520/%2520%255Cdelta%2529%257D%257B%255Cepsilon%257D%255Cright%2529%2524%2520samples%2520on%2520such%2520classes%252C%250Aexceeding%2520the%2520lower%2520bound%2520of%2520Daniely%2520et%2520al.%2520%25282015%2529%2520by%2520a%2520factor%2520of%2520%2524%255Cln%25281%2520/%250A%255Cepsilon%2529%2524.%2520For%2520multiclass%2520learning%2520in%2520full%2520generality%2520--%2520i.e.%252C%2520for%2520classes%2520of%250Afinite%2520DS%2520dimension%2520but%2520possibly%2520infinite%2520Graph%2520dimension%2520--%2520we%2520give%2520a%2520strong%250Arefutation%2520to%2520these%2520learning%2520strategies%252C%2520by%2520exhibiting%2520a%2520learnable%2520class%2520which%250Acannot%2520be%2520learned%2520to%2520constant%2520error%2520by%2520any%2520aggregation%2520of%2520a%2520finite%2520number%2520of%250Aproper%2520learners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22749v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Aggregations%20of%20Proper%20Learners%20in%20Multiclass%0A%20%20Classification&entry.906535625=Julian%20Asilis%20and%20Mikael%20M%C3%B8ller%20H%C3%B8gsgaard%20and%20Grigoris%20Velegkas&entry.1292438233=%20%20Multiclass%20learnability%20is%20known%20to%20exhibit%20a%20properness%20barrier%3A%20there%20are%0Alearnable%20classes%20which%20cannot%20be%20learned%20by%20any%20proper%20learner.%20Binary%0Aclassification%20faces%20no%20such%20barrier%20for%20learnability%2C%20but%20a%20similar%20one%20for%0Aoptimal%20learning%2C%20which%20can%20in%20general%20only%20be%20achieved%20by%20improper%20learners.%0AFortunately%2C%20recent%20advances%20in%20binary%20classification%20have%20demonstrated%20that%0Athis%20requirement%20can%20be%20satisfied%20using%20aggregations%20of%20proper%20learners%2C%20some%0Aof%20which%20are%20strikingly%20simple.%20This%20raises%20a%20natural%20question%3A%20to%20what%20extent%0Acan%20simple%20aggregations%20of%20proper%20learners%20overcome%20the%20properness%20barrier%20in%0Amulticlass%20classification%3F%0A%20%20We%20give%20a%20positive%20answer%20to%20this%20question%20for%20classes%20which%20have%20finite%0AGraph%20dimension%2C%20%24d_G%24.%20Namely%2C%20we%20demonstrate%20that%20the%20optimal%20binary%20learners%0Aof%20Hanneke%2C%20Larsen%2C%20and%20Aden-Ali%20et%20al.%20%28appropriately%20generalized%20to%20the%0Amulticlass%20setting%29%20achieve%20sample%20complexity%20%24O%5Cleft%28%5Cfrac%7Bd_G%20%2B%20%5Cln%281%20/%0A%5Cdelta%29%7D%7B%5Cepsilon%7D%5Cright%29%24.%20This%20forms%20a%20strict%20improvement%20upon%20the%20sample%0Acomplexity%20of%20ERM.%20We%20complement%20this%20with%20a%20lower%20bound%20demonstrating%20that%20for%0Acertain%20classes%20of%20Graph%20dimension%20%24d_G%24%2C%20majorities%20of%20ERM%20learners%20require%0A%24%5COmega%20%5Cleft%28%20%5Cfrac%7Bd_G%20%2B%20%5Cln%281%20/%20%5Cdelta%29%7D%7B%5Cepsilon%7D%5Cright%29%24%20samples.%0AFurthermore%2C%20we%20show%20that%20a%20single%20ERM%20requires%20%24%5COmega%20%5Cleft%28%5Cfrac%7Bd_G%20%5Cln%281%20/%0A%5Cepsilon%29%20%2B%20%5Cln%281%20/%20%5Cdelta%29%7D%7B%5Cepsilon%7D%5Cright%29%24%20samples%20on%20such%20classes%2C%0Aexceeding%20the%20lower%20bound%20of%20Daniely%20et%20al.%20%282015%29%20by%20a%20factor%20of%20%24%5Cln%281%20/%0A%5Cepsilon%29%24.%20For%20multiclass%20learning%20in%20full%20generality%20--%20i.e.%2C%20for%20classes%20of%0Afinite%20DS%20dimension%20but%20possibly%20infinite%20Graph%20dimension%20--%20we%20give%20a%20strong%0Arefutation%20to%20these%20learning%20strategies%2C%20by%20exhibiting%20a%20learnable%20class%20which%0Acannot%20be%20learned%20to%20constant%20error%20by%20any%20aggregation%20of%20a%20finite%20number%20of%0Aproper%20learners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22749v2&entry.124074799=Read"},
{"title": "Developing a Transferable Federated Network Intrusion Detection System", "author": "Abu Shafin Mohammad Mahdee Jameel and Shreya Ghosh and Aly El Gamal", "abstract": "  Intrusion Detection Systems (IDS) are a vital part of a network-connected\ndevice. In this paper, we develop a deep learning based intrusion detection\nsystem that is deployed in a distributed setup across devices connected to a\nnetwork. Our aim is to better equip deep learning models against unknown\nattacks using knowledge from known attacks. To this end, we develop algorithms\nto maximize the number of transferability relationships. We propose a\nConvolutional Neural Network (CNN) model, along with two algorithms that\nmaximize the number of relationships observed. One is a two step data\npre-processing stage, and the other is a Block-Based Smart Aggregation (BBSA)\nalgorithm. The proposed system succeeds in achieving superior transferability\nperformance while maintaining impressive local detection rates. We also show\nthat our method is generalizable, exhibiting transferability potential across\ndatasets and even with different backbones. The code for this work can be found\nat https://github.com/ghosh64/tabfidsv2.\n", "link": "http://arxiv.org/abs/2508.09060v1", "date": "2025-08-12", "relevancy": 1.8662, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4934}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4664}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Developing%20a%20Transferable%20Federated%20Network%20Intrusion%20Detection%20System&body=Title%3A%20Developing%20a%20Transferable%20Federated%20Network%20Intrusion%20Detection%20System%0AAuthor%3A%20Abu%20Shafin%20Mohammad%20Mahdee%20Jameel%20and%20Shreya%20Ghosh%20and%20Aly%20El%20Gamal%0AAbstract%3A%20%20%20Intrusion%20Detection%20Systems%20%28IDS%29%20are%20a%20vital%20part%20of%20a%20network-connected%0Adevice.%20In%20this%20paper%2C%20we%20develop%20a%20deep%20learning%20based%20intrusion%20detection%0Asystem%20that%20is%20deployed%20in%20a%20distributed%20setup%20across%20devices%20connected%20to%20a%0Anetwork.%20Our%20aim%20is%20to%20better%20equip%20deep%20learning%20models%20against%20unknown%0Aattacks%20using%20knowledge%20from%20known%20attacks.%20To%20this%20end%2C%20we%20develop%20algorithms%0Ato%20maximize%20the%20number%20of%20transferability%20relationships.%20We%20propose%20a%0AConvolutional%20Neural%20Network%20%28CNN%29%20model%2C%20along%20with%20two%20algorithms%20that%0Amaximize%20the%20number%20of%20relationships%20observed.%20One%20is%20a%20two%20step%20data%0Apre-processing%20stage%2C%20and%20the%20other%20is%20a%20Block-Based%20Smart%20Aggregation%20%28BBSA%29%0Aalgorithm.%20The%20proposed%20system%20succeeds%20in%20achieving%20superior%20transferability%0Aperformance%20while%20maintaining%20impressive%20local%20detection%20rates.%20We%20also%20show%0Athat%20our%20method%20is%20generalizable%2C%20exhibiting%20transferability%20potential%20across%0Adatasets%20and%20even%20with%20different%20backbones.%20The%20code%20for%20this%20work%20can%20be%20found%0Aat%20https%3A//github.com/ghosh64/tabfidsv2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeveloping%2520a%2520Transferable%2520Federated%2520Network%2520Intrusion%2520Detection%2520System%26entry.906535625%3DAbu%2520Shafin%2520Mohammad%2520Mahdee%2520Jameel%2520and%2520Shreya%2520Ghosh%2520and%2520Aly%2520El%2520Gamal%26entry.1292438233%3D%2520%2520Intrusion%2520Detection%2520Systems%2520%2528IDS%2529%2520are%2520a%2520vital%2520part%2520of%2520a%2520network-connected%250Adevice.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520deep%2520learning%2520based%2520intrusion%2520detection%250Asystem%2520that%2520is%2520deployed%2520in%2520a%2520distributed%2520setup%2520across%2520devices%2520connected%2520to%2520a%250Anetwork.%2520Our%2520aim%2520is%2520to%2520better%2520equip%2520deep%2520learning%2520models%2520against%2520unknown%250Aattacks%2520using%2520knowledge%2520from%2520known%2520attacks.%2520To%2520this%2520end%252C%2520we%2520develop%2520algorithms%250Ato%2520maximize%2520the%2520number%2520of%2520transferability%2520relationships.%2520We%2520propose%2520a%250AConvolutional%2520Neural%2520Network%2520%2528CNN%2529%2520model%252C%2520along%2520with%2520two%2520algorithms%2520that%250Amaximize%2520the%2520number%2520of%2520relationships%2520observed.%2520One%2520is%2520a%2520two%2520step%2520data%250Apre-processing%2520stage%252C%2520and%2520the%2520other%2520is%2520a%2520Block-Based%2520Smart%2520Aggregation%2520%2528BBSA%2529%250Aalgorithm.%2520The%2520proposed%2520system%2520succeeds%2520in%2520achieving%2520superior%2520transferability%250Aperformance%2520while%2520maintaining%2520impressive%2520local%2520detection%2520rates.%2520We%2520also%2520show%250Athat%2520our%2520method%2520is%2520generalizable%252C%2520exhibiting%2520transferability%2520potential%2520across%250Adatasets%2520and%2520even%2520with%2520different%2520backbones.%2520The%2520code%2520for%2520this%2520work%2520can%2520be%2520found%250Aat%2520https%253A//github.com/ghosh64/tabfidsv2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Developing%20a%20Transferable%20Federated%20Network%20Intrusion%20Detection%20System&entry.906535625=Abu%20Shafin%20Mohammad%20Mahdee%20Jameel%20and%20Shreya%20Ghosh%20and%20Aly%20El%20Gamal&entry.1292438233=%20%20Intrusion%20Detection%20Systems%20%28IDS%29%20are%20a%20vital%20part%20of%20a%20network-connected%0Adevice.%20In%20this%20paper%2C%20we%20develop%20a%20deep%20learning%20based%20intrusion%20detection%0Asystem%20that%20is%20deployed%20in%20a%20distributed%20setup%20across%20devices%20connected%20to%20a%0Anetwork.%20Our%20aim%20is%20to%20better%20equip%20deep%20learning%20models%20against%20unknown%0Aattacks%20using%20knowledge%20from%20known%20attacks.%20To%20this%20end%2C%20we%20develop%20algorithms%0Ato%20maximize%20the%20number%20of%20transferability%20relationships.%20We%20propose%20a%0AConvolutional%20Neural%20Network%20%28CNN%29%20model%2C%20along%20with%20two%20algorithms%20that%0Amaximize%20the%20number%20of%20relationships%20observed.%20One%20is%20a%20two%20step%20data%0Apre-processing%20stage%2C%20and%20the%20other%20is%20a%20Block-Based%20Smart%20Aggregation%20%28BBSA%29%0Aalgorithm.%20The%20proposed%20system%20succeeds%20in%20achieving%20superior%20transferability%0Aperformance%20while%20maintaining%20impressive%20local%20detection%20rates.%20We%20also%20show%0Athat%20our%20method%20is%20generalizable%2C%20exhibiting%20transferability%20potential%20across%0Adatasets%20and%20even%20with%20different%20backbones.%20The%20code%20for%20this%20work%20can%20be%20found%0Aat%20https%3A//github.com/ghosh64/tabfidsv2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09060v1&entry.124074799=Read"},
{"title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy\n  Entropy", "author": "Hongze Tan and Jianfei Pan", "abstract": "  Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models.\n", "link": "http://arxiv.org/abs/2508.04349v2", "date": "2025-08-12", "relevancy": 1.8636, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4884}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.451}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GTPO%20and%20GRPO-S%3A%20Token%20and%20Sequence-Level%20Reward%20Shaping%20with%20Policy%0A%20%20Entropy&body=Title%3A%20GTPO%20and%20GRPO-S%3A%20Token%20and%20Sequence-Level%20Reward%20Shaping%20with%20Policy%0A%20%20Entropy%0AAuthor%3A%20Hongze%20Tan%20and%20Jianfei%20Pan%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20with%20algorithms%20like%20Group%20Relative%20Policy%0AOptimization%20%28GRPO%29%20improves%20Large%20Language%20Model%20%28LLM%29%20reasoning%2C%20but%20is%0Alimited%20by%20a%20coarse-grained%20credit%20assignment%20that%20applies%20a%20uniform%20reward%20to%0Aall%20tokens%20in%20a%20sequence.%20This%20is%20a%20major%20flaw%20in%20long-chain%20reasoning%20tasks.%0AThis%20paper%20solves%20this%20with%20%5Ctextbf%7BDynamic%20Entropy%20Weighting%7D.%20Our%20core%20idea%0Ais%20that%20high-entropy%20tokens%20in%20correct%20responses%20can%20guide%20the%20policy%20toward%20a%0Ahigher%20performance%20ceiling.%20This%20allows%20us%20to%20create%20more%20fine-grained%20reward%0Asignals%20for%20precise%20policy%20updates%20via%20two%20ways%3A%201%29%20%5Ctextbf%7BGroup%20Token%20Policy%0AOptimization%7D%20%28%5Ctextbf%7BGTPO%7D%29%2C%20we%20assigns%20a%20entropy-weighted%20reward%20to%20each%0Atoken%20for%20fine-grained%20credit%20assignment.%202%29%20%5Ctextbf%7BSequence-Level%20Group%0ARelative%20Policy%20Optimization%7D%20%28%5Ctextbf%7BGRPO-S%7D%29%2C%20we%20assigns%20a%20entropy-weighted%0Areward%20to%20each%20sequence%20based%20on%20its%20average%20token%20entropy.%20Experiments%20show%0Aour%20methods%20significantly%20outperform%20the%20strong%20DAPO%20baseline.%20The%20results%0Aconfirm%20that%20our%20entropy-weighting%20mechanism%20is%20the%20key%20driver%20of%20this%0Aperformance%20boost%2C%20offering%20a%20better%20path%20to%20enhance%20deep%20reasoning%20in%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGTPO%2520and%2520GRPO-S%253A%2520Token%2520and%2520Sequence-Level%2520Reward%2520Shaping%2520with%2520Policy%250A%2520%2520Entropy%26entry.906535625%3DHongze%2520Tan%2520and%2520Jianfei%2520Pan%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520with%2520algorithms%2520like%2520Group%2520Relative%2520Policy%250AOptimization%2520%2528GRPO%2529%2520improves%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520reasoning%252C%2520but%2520is%250Alimited%2520by%2520a%2520coarse-grained%2520credit%2520assignment%2520that%2520applies%2520a%2520uniform%2520reward%2520to%250Aall%2520tokens%2520in%2520a%2520sequence.%2520This%2520is%2520a%2520major%2520flaw%2520in%2520long-chain%2520reasoning%2520tasks.%250AThis%2520paper%2520solves%2520this%2520with%2520%255Ctextbf%257BDynamic%2520Entropy%2520Weighting%257D.%2520Our%2520core%2520idea%250Ais%2520that%2520high-entropy%2520tokens%2520in%2520correct%2520responses%2520can%2520guide%2520the%2520policy%2520toward%2520a%250Ahigher%2520performance%2520ceiling.%2520This%2520allows%2520us%2520to%2520create%2520more%2520fine-grained%2520reward%250Asignals%2520for%2520precise%2520policy%2520updates%2520via%2520two%2520ways%253A%25201%2529%2520%255Ctextbf%257BGroup%2520Token%2520Policy%250AOptimization%257D%2520%2528%255Ctextbf%257BGTPO%257D%2529%252C%2520we%2520assigns%2520a%2520entropy-weighted%2520reward%2520to%2520each%250Atoken%2520for%2520fine-grained%2520credit%2520assignment.%25202%2529%2520%255Ctextbf%257BSequence-Level%2520Group%250ARelative%2520Policy%2520Optimization%257D%2520%2528%255Ctextbf%257BGRPO-S%257D%2529%252C%2520we%2520assigns%2520a%2520entropy-weighted%250Areward%2520to%2520each%2520sequence%2520based%2520on%2520its%2520average%2520token%2520entropy.%2520Experiments%2520show%250Aour%2520methods%2520significantly%2520outperform%2520the%2520strong%2520DAPO%2520baseline.%2520The%2520results%250Aconfirm%2520that%2520our%2520entropy-weighting%2520mechanism%2520is%2520the%2520key%2520driver%2520of%2520this%250Aperformance%2520boost%252C%2520offering%2520a%2520better%2520path%2520to%2520enhance%2520deep%2520reasoning%2520in%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GTPO%20and%20GRPO-S%3A%20Token%20and%20Sequence-Level%20Reward%20Shaping%20with%20Policy%0A%20%20Entropy&entry.906535625=Hongze%20Tan%20and%20Jianfei%20Pan&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20with%20algorithms%20like%20Group%20Relative%20Policy%0AOptimization%20%28GRPO%29%20improves%20Large%20Language%20Model%20%28LLM%29%20reasoning%2C%20but%20is%0Alimited%20by%20a%20coarse-grained%20credit%20assignment%20that%20applies%20a%20uniform%20reward%20to%0Aall%20tokens%20in%20a%20sequence.%20This%20is%20a%20major%20flaw%20in%20long-chain%20reasoning%20tasks.%0AThis%20paper%20solves%20this%20with%20%5Ctextbf%7BDynamic%20Entropy%20Weighting%7D.%20Our%20core%20idea%0Ais%20that%20high-entropy%20tokens%20in%20correct%20responses%20can%20guide%20the%20policy%20toward%20a%0Ahigher%20performance%20ceiling.%20This%20allows%20us%20to%20create%20more%20fine-grained%20reward%0Asignals%20for%20precise%20policy%20updates%20via%20two%20ways%3A%201%29%20%5Ctextbf%7BGroup%20Token%20Policy%0AOptimization%7D%20%28%5Ctextbf%7BGTPO%7D%29%2C%20we%20assigns%20a%20entropy-weighted%20reward%20to%20each%0Atoken%20for%20fine-grained%20credit%20assignment.%202%29%20%5Ctextbf%7BSequence-Level%20Group%0ARelative%20Policy%20Optimization%7D%20%28%5Ctextbf%7BGRPO-S%7D%29%2C%20we%20assigns%20a%20entropy-weighted%0Areward%20to%20each%20sequence%20based%20on%20its%20average%20token%20entropy.%20Experiments%20show%0Aour%20methods%20significantly%20outperform%20the%20strong%20DAPO%20baseline.%20The%20results%0Aconfirm%20that%20our%20entropy-weighting%20mechanism%20is%20the%20key%20driver%20of%20this%0Aperformance%20boost%2C%20offering%20a%20better%20path%20to%20enhance%20deep%20reasoning%20in%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04349v2&entry.124074799=Read"},
{"title": "TIDE : Temporal-Aware Sparse Autoencoders for Interpretable Diffusion\n  Transformers in Image Generation", "author": "Victor Shea-Jay Huang and Le Zhuo and Yi Xin and Zhaokai Wang and Fu-Yun Wang and Yuchi Wang and Renrui Zhang and Peng Gao and Hongsheng Li", "abstract": "  Diffusion Transformers (DiTs) are a powerful yet underexplored class of\ngenerative models compared to U-Net-based diffusion architectures. We propose\nTIDE-Temporal-aware sparse autoencoders for Interpretable Diffusion\ntransformErs-a framework designed to extract sparse, interpretable activation\nfeatures across timesteps in DiTs. TIDE effectively captures temporally-varying\nrepresentations and reveals that DiTs naturally learn hierarchical semantics\n(e.g., 3D structure, object class, and fine-grained concepts) during\nlarge-scale pretraining. Experiments show that TIDE enhances interpretability\nand controllability while maintaining reasonable generation quality, enabling\napplications such as safe image editing and style transfer.\n", "link": "http://arxiv.org/abs/2503.07050v2", "date": "2025-08-12", "relevancy": 1.86, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6573}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6314}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TIDE%20%3A%20Temporal-Aware%20Sparse%20Autoencoders%20for%20Interpretable%20Diffusion%0A%20%20Transformers%20in%20Image%20Generation&body=Title%3A%20TIDE%20%3A%20Temporal-Aware%20Sparse%20Autoencoders%20for%20Interpretable%20Diffusion%0A%20%20Transformers%20in%20Image%20Generation%0AAuthor%3A%20Victor%20Shea-Jay%20Huang%20and%20Le%20Zhuo%20and%20Yi%20Xin%20and%20Zhaokai%20Wang%20and%20Fu-Yun%20Wang%20and%20Yuchi%20Wang%20and%20Renrui%20Zhang%20and%20Peng%20Gao%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20Diffusion%20Transformers%20%28DiTs%29%20are%20a%20powerful%20yet%20underexplored%20class%20of%0Agenerative%20models%20compared%20to%20U-Net-based%20diffusion%20architectures.%20We%20propose%0ATIDE-Temporal-aware%20sparse%20autoencoders%20for%20Interpretable%20Diffusion%0AtransformErs-a%20framework%20designed%20to%20extract%20sparse%2C%20interpretable%20activation%0Afeatures%20across%20timesteps%20in%20DiTs.%20TIDE%20effectively%20captures%20temporally-varying%0Arepresentations%20and%20reveals%20that%20DiTs%20naturally%20learn%20hierarchical%20semantics%0A%28e.g.%2C%203D%20structure%2C%20object%20class%2C%20and%20fine-grained%20concepts%29%20during%0Alarge-scale%20pretraining.%20Experiments%20show%20that%20TIDE%20enhances%20interpretability%0Aand%20controllability%20while%20maintaining%20reasonable%20generation%20quality%2C%20enabling%0Aapplications%20such%20as%20safe%20image%20editing%20and%20style%20transfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07050v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTIDE%2520%253A%2520Temporal-Aware%2520Sparse%2520Autoencoders%2520for%2520Interpretable%2520Diffusion%250A%2520%2520Transformers%2520in%2520Image%2520Generation%26entry.906535625%3DVictor%2520Shea-Jay%2520Huang%2520and%2520Le%2520Zhuo%2520and%2520Yi%2520Xin%2520and%2520Zhaokai%2520Wang%2520and%2520Fu-Yun%2520Wang%2520and%2520Yuchi%2520Wang%2520and%2520Renrui%2520Zhang%2520and%2520Peng%2520Gao%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520Diffusion%2520Transformers%2520%2528DiTs%2529%2520are%2520a%2520powerful%2520yet%2520underexplored%2520class%2520of%250Agenerative%2520models%2520compared%2520to%2520U-Net-based%2520diffusion%2520architectures.%2520We%2520propose%250ATIDE-Temporal-aware%2520sparse%2520autoencoders%2520for%2520Interpretable%2520Diffusion%250AtransformErs-a%2520framework%2520designed%2520to%2520extract%2520sparse%252C%2520interpretable%2520activation%250Afeatures%2520across%2520timesteps%2520in%2520DiTs.%2520TIDE%2520effectively%2520captures%2520temporally-varying%250Arepresentations%2520and%2520reveals%2520that%2520DiTs%2520naturally%2520learn%2520hierarchical%2520semantics%250A%2528e.g.%252C%25203D%2520structure%252C%2520object%2520class%252C%2520and%2520fine-grained%2520concepts%2529%2520during%250Alarge-scale%2520pretraining.%2520Experiments%2520show%2520that%2520TIDE%2520enhances%2520interpretability%250Aand%2520controllability%2520while%2520maintaining%2520reasonable%2520generation%2520quality%252C%2520enabling%250Aapplications%2520such%2520as%2520safe%2520image%2520editing%2520and%2520style%2520transfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07050v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIDE%20%3A%20Temporal-Aware%20Sparse%20Autoencoders%20for%20Interpretable%20Diffusion%0A%20%20Transformers%20in%20Image%20Generation&entry.906535625=Victor%20Shea-Jay%20Huang%20and%20Le%20Zhuo%20and%20Yi%20Xin%20and%20Zhaokai%20Wang%20and%20Fu-Yun%20Wang%20and%20Yuchi%20Wang%20and%20Renrui%20Zhang%20and%20Peng%20Gao%20and%20Hongsheng%20Li&entry.1292438233=%20%20Diffusion%20Transformers%20%28DiTs%29%20are%20a%20powerful%20yet%20underexplored%20class%20of%0Agenerative%20models%20compared%20to%20U-Net-based%20diffusion%20architectures.%20We%20propose%0ATIDE-Temporal-aware%20sparse%20autoencoders%20for%20Interpretable%20Diffusion%0AtransformErs-a%20framework%20designed%20to%20extract%20sparse%2C%20interpretable%20activation%0Afeatures%20across%20timesteps%20in%20DiTs.%20TIDE%20effectively%20captures%20temporally-varying%0Arepresentations%20and%20reveals%20that%20DiTs%20naturally%20learn%20hierarchical%20semantics%0A%28e.g.%2C%203D%20structure%2C%20object%20class%2C%20and%20fine-grained%20concepts%29%20during%0Alarge-scale%20pretraining.%20Experiments%20show%20that%20TIDE%20enhances%20interpretability%0Aand%20controllability%20while%20maintaining%20reasonable%20generation%20quality%2C%20enabling%0Aapplications%20such%20as%20safe%20image%20editing%20and%20style%20transfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07050v2&entry.124074799=Read"},
{"title": "Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy\n  and AI Governance Exams", "author": "Zane Witherspoon and Thet Mon Aye and YingYing Hao", "abstract": "  The rapid emergence of large language models (LLMs) has raised urgent\nquestions across the modern workforce about this new technology's strengths,\nweaknesses, and capabilities. For privacy professionals, the question is\nwhether these AI systems can provide reliable support on regulatory compliance,\nprivacy program management, and AI governance. In this study, we evaluate ten\nleading open and closed LLMs, including models from OpenAI, Anthropic, Google\nDeepMind, Meta, and DeepSeek, by benchmarking their performance on\nindustry-standard certification exams: CIPP/US, CIPM, CIPT, and AIGP from the\nInternational Association of Privacy Professionals (IAPP). Each model was\ntested using official sample exams in a closed-book setting and compared to\nIAPP's passing thresholds. Our findings show that several frontier models such\nas Gemini 2.5 Pro and OpenAI's GPT-5 consistently achieve scores exceeding the\nstandards for professional human certification - demonstrating substantial\nexpertise in privacy law, technical controls, and AI governance. The results\nhighlight both the strengths and domain-specific gaps of current LLMs and offer\npractical insights for privacy officers, compliance leads, and technologists\nassessing the readiness of AI tools for high-stakes data governance roles. This\npaper provides an overview for professionals navigating the intersection of AI\nadvancement and regulatory risk and establishes a machine benchmark based on\nhuman-centric evaluations.\n", "link": "http://arxiv.org/abs/2508.09036v1", "date": "2025-08-12", "relevancy": 1.8277, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4857}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20We%20Trust%20AI%20to%20Govern%20AI%3F%20Benchmarking%20LLM%20Performance%20on%20Privacy%0A%20%20and%20AI%20Governance%20Exams&body=Title%3A%20Can%20We%20Trust%20AI%20to%20Govern%20AI%3F%20Benchmarking%20LLM%20Performance%20on%20Privacy%0A%20%20and%20AI%20Governance%20Exams%0AAuthor%3A%20Zane%20Witherspoon%20and%20Thet%20Mon%20Aye%20and%20YingYing%20Hao%0AAbstract%3A%20%20%20The%20rapid%20emergence%20of%20large%20language%20models%20%28LLMs%29%20has%20raised%20urgent%0Aquestions%20across%20the%20modern%20workforce%20about%20this%20new%20technology%27s%20strengths%2C%0Aweaknesses%2C%20and%20capabilities.%20For%20privacy%20professionals%2C%20the%20question%20is%0Awhether%20these%20AI%20systems%20can%20provide%20reliable%20support%20on%20regulatory%20compliance%2C%0Aprivacy%20program%20management%2C%20and%20AI%20governance.%20In%20this%20study%2C%20we%20evaluate%20ten%0Aleading%20open%20and%20closed%20LLMs%2C%20including%20models%20from%20OpenAI%2C%20Anthropic%2C%20Google%0ADeepMind%2C%20Meta%2C%20and%20DeepSeek%2C%20by%20benchmarking%20their%20performance%20on%0Aindustry-standard%20certification%20exams%3A%20CIPP/US%2C%20CIPM%2C%20CIPT%2C%20and%20AIGP%20from%20the%0AInternational%20Association%20of%20Privacy%20Professionals%20%28IAPP%29.%20Each%20model%20was%0Atested%20using%20official%20sample%20exams%20in%20a%20closed-book%20setting%20and%20compared%20to%0AIAPP%27s%20passing%20thresholds.%20Our%20findings%20show%20that%20several%20frontier%20models%20such%0Aas%20Gemini%202.5%20Pro%20and%20OpenAI%27s%20GPT-5%20consistently%20achieve%20scores%20exceeding%20the%0Astandards%20for%20professional%20human%20certification%20-%20demonstrating%20substantial%0Aexpertise%20in%20privacy%20law%2C%20technical%20controls%2C%20and%20AI%20governance.%20The%20results%0Ahighlight%20both%20the%20strengths%20and%20domain-specific%20gaps%20of%20current%20LLMs%20and%20offer%0Apractical%20insights%20for%20privacy%20officers%2C%20compliance%20leads%2C%20and%20technologists%0Aassessing%20the%20readiness%20of%20AI%20tools%20for%20high-stakes%20data%20governance%20roles.%20This%0Apaper%20provides%20an%20overview%20for%20professionals%20navigating%20the%20intersection%20of%20AI%0Aadvancement%20and%20regulatory%20risk%20and%20establishes%20a%20machine%20benchmark%20based%20on%0Ahuman-centric%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09036v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520We%2520Trust%2520AI%2520to%2520Govern%2520AI%253F%2520Benchmarking%2520LLM%2520Performance%2520on%2520Privacy%250A%2520%2520and%2520AI%2520Governance%2520Exams%26entry.906535625%3DZane%2520Witherspoon%2520and%2520Thet%2520Mon%2520Aye%2520and%2520YingYing%2520Hao%26entry.1292438233%3D%2520%2520The%2520rapid%2520emergence%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520raised%2520urgent%250Aquestions%2520across%2520the%2520modern%2520workforce%2520about%2520this%2520new%2520technology%2527s%2520strengths%252C%250Aweaknesses%252C%2520and%2520capabilities.%2520For%2520privacy%2520professionals%252C%2520the%2520question%2520is%250Awhether%2520these%2520AI%2520systems%2520can%2520provide%2520reliable%2520support%2520on%2520regulatory%2520compliance%252C%250Aprivacy%2520program%2520management%252C%2520and%2520AI%2520governance.%2520In%2520this%2520study%252C%2520we%2520evaluate%2520ten%250Aleading%2520open%2520and%2520closed%2520LLMs%252C%2520including%2520models%2520from%2520OpenAI%252C%2520Anthropic%252C%2520Google%250ADeepMind%252C%2520Meta%252C%2520and%2520DeepSeek%252C%2520by%2520benchmarking%2520their%2520performance%2520on%250Aindustry-standard%2520certification%2520exams%253A%2520CIPP/US%252C%2520CIPM%252C%2520CIPT%252C%2520and%2520AIGP%2520from%2520the%250AInternational%2520Association%2520of%2520Privacy%2520Professionals%2520%2528IAPP%2529.%2520Each%2520model%2520was%250Atested%2520using%2520official%2520sample%2520exams%2520in%2520a%2520closed-book%2520setting%2520and%2520compared%2520to%250AIAPP%2527s%2520passing%2520thresholds.%2520Our%2520findings%2520show%2520that%2520several%2520frontier%2520models%2520such%250Aas%2520Gemini%25202.5%2520Pro%2520and%2520OpenAI%2527s%2520GPT-5%2520consistently%2520achieve%2520scores%2520exceeding%2520the%250Astandards%2520for%2520professional%2520human%2520certification%2520-%2520demonstrating%2520substantial%250Aexpertise%2520in%2520privacy%2520law%252C%2520technical%2520controls%252C%2520and%2520AI%2520governance.%2520The%2520results%250Ahighlight%2520both%2520the%2520strengths%2520and%2520domain-specific%2520gaps%2520of%2520current%2520LLMs%2520and%2520offer%250Apractical%2520insights%2520for%2520privacy%2520officers%252C%2520compliance%2520leads%252C%2520and%2520technologists%250Aassessing%2520the%2520readiness%2520of%2520AI%2520tools%2520for%2520high-stakes%2520data%2520governance%2520roles.%2520This%250Apaper%2520provides%2520an%2520overview%2520for%2520professionals%2520navigating%2520the%2520intersection%2520of%2520AI%250Aadvancement%2520and%2520regulatory%2520risk%2520and%2520establishes%2520a%2520machine%2520benchmark%2520based%2520on%250Ahuman-centric%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09036v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20We%20Trust%20AI%20to%20Govern%20AI%3F%20Benchmarking%20LLM%20Performance%20on%20Privacy%0A%20%20and%20AI%20Governance%20Exams&entry.906535625=Zane%20Witherspoon%20and%20Thet%20Mon%20Aye%20and%20YingYing%20Hao&entry.1292438233=%20%20The%20rapid%20emergence%20of%20large%20language%20models%20%28LLMs%29%20has%20raised%20urgent%0Aquestions%20across%20the%20modern%20workforce%20about%20this%20new%20technology%27s%20strengths%2C%0Aweaknesses%2C%20and%20capabilities.%20For%20privacy%20professionals%2C%20the%20question%20is%0Awhether%20these%20AI%20systems%20can%20provide%20reliable%20support%20on%20regulatory%20compliance%2C%0Aprivacy%20program%20management%2C%20and%20AI%20governance.%20In%20this%20study%2C%20we%20evaluate%20ten%0Aleading%20open%20and%20closed%20LLMs%2C%20including%20models%20from%20OpenAI%2C%20Anthropic%2C%20Google%0ADeepMind%2C%20Meta%2C%20and%20DeepSeek%2C%20by%20benchmarking%20their%20performance%20on%0Aindustry-standard%20certification%20exams%3A%20CIPP/US%2C%20CIPM%2C%20CIPT%2C%20and%20AIGP%20from%20the%0AInternational%20Association%20of%20Privacy%20Professionals%20%28IAPP%29.%20Each%20model%20was%0Atested%20using%20official%20sample%20exams%20in%20a%20closed-book%20setting%20and%20compared%20to%0AIAPP%27s%20passing%20thresholds.%20Our%20findings%20show%20that%20several%20frontier%20models%20such%0Aas%20Gemini%202.5%20Pro%20and%20OpenAI%27s%20GPT-5%20consistently%20achieve%20scores%20exceeding%20the%0Astandards%20for%20professional%20human%20certification%20-%20demonstrating%20substantial%0Aexpertise%20in%20privacy%20law%2C%20technical%20controls%2C%20and%20AI%20governance.%20The%20results%0Ahighlight%20both%20the%20strengths%20and%20domain-specific%20gaps%20of%20current%20LLMs%20and%20offer%0Apractical%20insights%20for%20privacy%20officers%2C%20compliance%20leads%2C%20and%20technologists%0Aassessing%20the%20readiness%20of%20AI%20tools%20for%20high-stakes%20data%20governance%20roles.%20This%0Apaper%20provides%20an%20overview%20for%20professionals%20navigating%20the%20intersection%20of%20AI%0Aadvancement%20and%20regulatory%20risk%20and%20establishes%20a%20machine%20benchmark%20based%20on%0Ahuman-centric%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09036v1&entry.124074799=Read"},
{"title": "Generation of Real-time Robotic Emotional Expressions Learning from\n  Human Demonstration in Mixed Reality", "author": "Chao Wang and Michael Gienger and Fan Zhang", "abstract": "  Expressive behaviors in robots are critical for effectively conveying their\nemotional states during interactions with humans. In this work, we present a\nframework that autonomously generates realistic and diverse robotic emotional\nexpressions based on expert human demonstrations captured in Mixed Reality\n(MR). Our system enables experts to teleoperate a virtual robot from a\nfirst-person perspective, capturing their facial expressions, head movements,\nand upper-body gestures, and mapping these behaviors onto corresponding robotic\ncomponents including eyes, ears, neck, and arms. Leveraging a\nflow-matching-based generative process, our model learns to produce coherent\nand varied behaviors in real-time in response to moving objects, conditioned\nexplicitly on given emotional states. A preliminary test validated the\neffectiveness of our approach for generating autonomous expressions.\n", "link": "http://arxiv.org/abs/2508.08999v1", "date": "2025-08-12", "relevancy": 1.7467, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6056}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5763}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generation%20of%20Real-time%20Robotic%20Emotional%20Expressions%20Learning%20from%0A%20%20Human%20Demonstration%20in%20Mixed%20Reality&body=Title%3A%20Generation%20of%20Real-time%20Robotic%20Emotional%20Expressions%20Learning%20from%0A%20%20Human%20Demonstration%20in%20Mixed%20Reality%0AAuthor%3A%20Chao%20Wang%20and%20Michael%20Gienger%20and%20Fan%20Zhang%0AAbstract%3A%20%20%20Expressive%20behaviors%20in%20robots%20are%20critical%20for%20effectively%20conveying%20their%0Aemotional%20states%20during%20interactions%20with%20humans.%20In%20this%20work%2C%20we%20present%20a%0Aframework%20that%20autonomously%20generates%20realistic%20and%20diverse%20robotic%20emotional%0Aexpressions%20based%20on%20expert%20human%20demonstrations%20captured%20in%20Mixed%20Reality%0A%28MR%29.%20Our%20system%20enables%20experts%20to%20teleoperate%20a%20virtual%20robot%20from%20a%0Afirst-person%20perspective%2C%20capturing%20their%20facial%20expressions%2C%20head%20movements%2C%0Aand%20upper-body%20gestures%2C%20and%20mapping%20these%20behaviors%20onto%20corresponding%20robotic%0Acomponents%20including%20eyes%2C%20ears%2C%20neck%2C%20and%20arms.%20Leveraging%20a%0Aflow-matching-based%20generative%20process%2C%20our%20model%20learns%20to%20produce%20coherent%0Aand%20varied%20behaviors%20in%20real-time%20in%20response%20to%20moving%20objects%2C%20conditioned%0Aexplicitly%20on%20given%20emotional%20states.%20A%20preliminary%20test%20validated%20the%0Aeffectiveness%20of%20our%20approach%20for%20generating%20autonomous%20expressions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneration%2520of%2520Real-time%2520Robotic%2520Emotional%2520Expressions%2520Learning%2520from%250A%2520%2520Human%2520Demonstration%2520in%2520Mixed%2520Reality%26entry.906535625%3DChao%2520Wang%2520and%2520Michael%2520Gienger%2520and%2520Fan%2520Zhang%26entry.1292438233%3D%2520%2520Expressive%2520behaviors%2520in%2520robots%2520are%2520critical%2520for%2520effectively%2520conveying%2520their%250Aemotional%2520states%2520during%2520interactions%2520with%2520humans.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Aframework%2520that%2520autonomously%2520generates%2520realistic%2520and%2520diverse%2520robotic%2520emotional%250Aexpressions%2520based%2520on%2520expert%2520human%2520demonstrations%2520captured%2520in%2520Mixed%2520Reality%250A%2528MR%2529.%2520Our%2520system%2520enables%2520experts%2520to%2520teleoperate%2520a%2520virtual%2520robot%2520from%2520a%250Afirst-person%2520perspective%252C%2520capturing%2520their%2520facial%2520expressions%252C%2520head%2520movements%252C%250Aand%2520upper-body%2520gestures%252C%2520and%2520mapping%2520these%2520behaviors%2520onto%2520corresponding%2520robotic%250Acomponents%2520including%2520eyes%252C%2520ears%252C%2520neck%252C%2520and%2520arms.%2520Leveraging%2520a%250Aflow-matching-based%2520generative%2520process%252C%2520our%2520model%2520learns%2520to%2520produce%2520coherent%250Aand%2520varied%2520behaviors%2520in%2520real-time%2520in%2520response%2520to%2520moving%2520objects%252C%2520conditioned%250Aexplicitly%2520on%2520given%2520emotional%2520states.%2520A%2520preliminary%2520test%2520validated%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520for%2520generating%2520autonomous%2520expressions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generation%20of%20Real-time%20Robotic%20Emotional%20Expressions%20Learning%20from%0A%20%20Human%20Demonstration%20in%20Mixed%20Reality&entry.906535625=Chao%20Wang%20and%20Michael%20Gienger%20and%20Fan%20Zhang&entry.1292438233=%20%20Expressive%20behaviors%20in%20robots%20are%20critical%20for%20effectively%20conveying%20their%0Aemotional%20states%20during%20interactions%20with%20humans.%20In%20this%20work%2C%20we%20present%20a%0Aframework%20that%20autonomously%20generates%20realistic%20and%20diverse%20robotic%20emotional%0Aexpressions%20based%20on%20expert%20human%20demonstrations%20captured%20in%20Mixed%20Reality%0A%28MR%29.%20Our%20system%20enables%20experts%20to%20teleoperate%20a%20virtual%20robot%20from%20a%0Afirst-person%20perspective%2C%20capturing%20their%20facial%20expressions%2C%20head%20movements%2C%0Aand%20upper-body%20gestures%2C%20and%20mapping%20these%20behaviors%20onto%20corresponding%20robotic%0Acomponents%20including%20eyes%2C%20ears%2C%20neck%2C%20and%20arms.%20Leveraging%20a%0Aflow-matching-based%20generative%20process%2C%20our%20model%20learns%20to%20produce%20coherent%0Aand%20varied%20behaviors%20in%20real-time%20in%20response%20to%20moving%20objects%2C%20conditioned%0Aexplicitly%20on%20given%20emotional%20states.%20A%20preliminary%20test%20validated%20the%0Aeffectiveness%20of%20our%20approach%20for%20generating%20autonomous%20expressions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08999v1&entry.124074799=Read"},
{"title": "BELLA: Black box model Explanations by Local Linear Approximations", "author": "Nedeljko Radulovic and Albert Bifet and Fabian Suchanek", "abstract": "  Understanding the decision-making process of black-box models has become not\njust a legal requirement, but also an additional way to assess their\nperformance. However, the state of the art post-hoc explanation approaches for\nregression models rely on synthetic data generation, which introduces\nuncertainty and can hurt the reliability of the explanations. Furthermore, they\ntend to produce explanations that apply to only very few data points. In this\npaper, we present BELLA, a deterministic model-agnostic post-hoc approach for\nexplaining the individual predictions of regression black-box models. BELLA\nprovides explanations in the form of a linear model trained in the feature\nspace. BELLA maximizes the size of the neighborhood to which the linear model\napplies so that the explanations are accurate, simple, general, and robust.\n", "link": "http://arxiv.org/abs/2305.11311v3", "date": "2025-08-12", "relevancy": 1.7335, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4398}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4321}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BELLA%3A%20Black%20box%20model%20Explanations%20by%20Local%20Linear%20Approximations&body=Title%3A%20BELLA%3A%20Black%20box%20model%20Explanations%20by%20Local%20Linear%20Approximations%0AAuthor%3A%20Nedeljko%20Radulovic%20and%20Albert%20Bifet%20and%20Fabian%20Suchanek%0AAbstract%3A%20%20%20Understanding%20the%20decision-making%20process%20of%20black-box%20models%20has%20become%20not%0Ajust%20a%20legal%20requirement%2C%20but%20also%20an%20additional%20way%20to%20assess%20their%0Aperformance.%20However%2C%20the%20state%20of%20the%20art%20post-hoc%20explanation%20approaches%20for%0Aregression%20models%20rely%20on%20synthetic%20data%20generation%2C%20which%20introduces%0Auncertainty%20and%20can%20hurt%20the%20reliability%20of%20the%20explanations.%20Furthermore%2C%20they%0Atend%20to%20produce%20explanations%20that%20apply%20to%20only%20very%20few%20data%20points.%20In%20this%0Apaper%2C%20we%20present%20BELLA%2C%20a%20deterministic%20model-agnostic%20post-hoc%20approach%20for%0Aexplaining%20the%20individual%20predictions%20of%20regression%20black-box%20models.%20BELLA%0Aprovides%20explanations%20in%20the%20form%20of%20a%20linear%20model%20trained%20in%20the%20feature%0Aspace.%20BELLA%20maximizes%20the%20size%20of%20the%20neighborhood%20to%20which%20the%20linear%20model%0Aapplies%20so%20that%20the%20explanations%20are%20accurate%2C%20simple%2C%20general%2C%20and%20robust.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.11311v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBELLA%253A%2520Black%2520box%2520model%2520Explanations%2520by%2520Local%2520Linear%2520Approximations%26entry.906535625%3DNedeljko%2520Radulovic%2520and%2520Albert%2520Bifet%2520and%2520Fabian%2520Suchanek%26entry.1292438233%3D%2520%2520Understanding%2520the%2520decision-making%2520process%2520of%2520black-box%2520models%2520has%2520become%2520not%250Ajust%2520a%2520legal%2520requirement%252C%2520but%2520also%2520an%2520additional%2520way%2520to%2520assess%2520their%250Aperformance.%2520However%252C%2520the%2520state%2520of%2520the%2520art%2520post-hoc%2520explanation%2520approaches%2520for%250Aregression%2520models%2520rely%2520on%2520synthetic%2520data%2520generation%252C%2520which%2520introduces%250Auncertainty%2520and%2520can%2520hurt%2520the%2520reliability%2520of%2520the%2520explanations.%2520Furthermore%252C%2520they%250Atend%2520to%2520produce%2520explanations%2520that%2520apply%2520to%2520only%2520very%2520few%2520data%2520points.%2520In%2520this%250Apaper%252C%2520we%2520present%2520BELLA%252C%2520a%2520deterministic%2520model-agnostic%2520post-hoc%2520approach%2520for%250Aexplaining%2520the%2520individual%2520predictions%2520of%2520regression%2520black-box%2520models.%2520BELLA%250Aprovides%2520explanations%2520in%2520the%2520form%2520of%2520a%2520linear%2520model%2520trained%2520in%2520the%2520feature%250Aspace.%2520BELLA%2520maximizes%2520the%2520size%2520of%2520the%2520neighborhood%2520to%2520which%2520the%2520linear%2520model%250Aapplies%2520so%2520that%2520the%2520explanations%2520are%2520accurate%252C%2520simple%252C%2520general%252C%2520and%2520robust.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.11311v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BELLA%3A%20Black%20box%20model%20Explanations%20by%20Local%20Linear%20Approximations&entry.906535625=Nedeljko%20Radulovic%20and%20Albert%20Bifet%20and%20Fabian%20Suchanek&entry.1292438233=%20%20Understanding%20the%20decision-making%20process%20of%20black-box%20models%20has%20become%20not%0Ajust%20a%20legal%20requirement%2C%20but%20also%20an%20additional%20way%20to%20assess%20their%0Aperformance.%20However%2C%20the%20state%20of%20the%20art%20post-hoc%20explanation%20approaches%20for%0Aregression%20models%20rely%20on%20synthetic%20data%20generation%2C%20which%20introduces%0Auncertainty%20and%20can%20hurt%20the%20reliability%20of%20the%20explanations.%20Furthermore%2C%20they%0Atend%20to%20produce%20explanations%20that%20apply%20to%20only%20very%20few%20data%20points.%20In%20this%0Apaper%2C%20we%20present%20BELLA%2C%20a%20deterministic%20model-agnostic%20post-hoc%20approach%20for%0Aexplaining%20the%20individual%20predictions%20of%20regression%20black-box%20models.%20BELLA%0Aprovides%20explanations%20in%20the%20form%20of%20a%20linear%20model%20trained%20in%20the%20feature%0Aspace.%20BELLA%20maximizes%20the%20size%20of%20the%20neighborhood%20to%20which%20the%20linear%20model%0Aapplies%20so%20that%20the%20explanations%20are%20accurate%2C%20simple%2C%20general%2C%20and%20robust.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.11311v3&entry.124074799=Read"},
{"title": "Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health\n  Monitoring", "author": "Zihan Fang and Zheng Lin and Senkang Hu and Yihang Tao and Yiqin Deng and Xianhao Chen and Yuguang Fang", "abstract": "  Outdoor health monitoring is essential to detect early abnormal health status\nfor safeguarding human health and safety. Conventional outdoor monitoring\nrelies on static multimodal deep learning frameworks, which requires extensive\ndata training from scratch and fails to capture subtle health status changes.\nMultimodal large language models (MLLMs) emerge as a promising alternative,\nutilizing only small datasets to fine-tune pre-trained information-rich models\nfor enabling powerful health status monitoring. Unfortunately, MLLM-based\noutdoor health monitoring also faces significant challenges: I) sensor data\ncontains input noise stemming from sensor data acquisition and fluctuation\nnoise caused by sudden changes in physiological signals due to dynamic outdoor\nenvironments, thus degrading the training performance; ii) current transformer\nbased MLLMs struggle to achieve robust multimodal fusion, as they lack a design\nfor fusing the noisy modality; iii) modalities with varying noise levels hinder\naccurate recovery of missing data from fluctuating distributions. To combat\nthese challenges, we propose an uncertainty-aware multimodal fusion framework,\nnamed DUAL-Health, for outdoor health monitoring in dynamic and noisy\nenvironments. First, to assess the impact of noise, we accurately quantify\nmodality uncertainty caused by input and fluctuation noise with current and\ntemporal features. Second, to empower efficient muitimodal fusion with\nlow-quality modalities,we customize the fusion weight for each modality based\non quantified and calibrated uncertainty. Third, to enhance data recovery from\nfluctuating noisy modalities, we align modality distributions within a common\nsemantic space. Extensive experiments demonstrate that our DUAL-Health\noutperforms state-of-the-art baselines in detection accuracy and robustness.\n", "link": "http://arxiv.org/abs/2508.09085v1", "date": "2025-08-12", "relevancy": 1.7308, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6281}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5624}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Uncertainty-aware%20Multimodal%20Fusion%20for%20Outdoor%20Health%0A%20%20Monitoring&body=Title%3A%20Dynamic%20Uncertainty-aware%20Multimodal%20Fusion%20for%20Outdoor%20Health%0A%20%20Monitoring%0AAuthor%3A%20Zihan%20Fang%20and%20Zheng%20Lin%20and%20Senkang%20Hu%20and%20Yihang%20Tao%20and%20Yiqin%20Deng%20and%20Xianhao%20Chen%20and%20Yuguang%20Fang%0AAbstract%3A%20%20%20Outdoor%20health%20monitoring%20is%20essential%20to%20detect%20early%20abnormal%20health%20status%0Afor%20safeguarding%20human%20health%20and%20safety.%20Conventional%20outdoor%20monitoring%0Arelies%20on%20static%20multimodal%20deep%20learning%20frameworks%2C%20which%20requires%20extensive%0Adata%20training%20from%20scratch%20and%20fails%20to%20capture%20subtle%20health%20status%20changes.%0AMultimodal%20large%20language%20models%20%28MLLMs%29%20emerge%20as%20a%20promising%20alternative%2C%0Autilizing%20only%20small%20datasets%20to%20fine-tune%20pre-trained%20information-rich%20models%0Afor%20enabling%20powerful%20health%20status%20monitoring.%20Unfortunately%2C%20MLLM-based%0Aoutdoor%20health%20monitoring%20also%20faces%20significant%20challenges%3A%20I%29%20sensor%20data%0Acontains%20input%20noise%20stemming%20from%20sensor%20data%20acquisition%20and%20fluctuation%0Anoise%20caused%20by%20sudden%20changes%20in%20physiological%20signals%20due%20to%20dynamic%20outdoor%0Aenvironments%2C%20thus%20degrading%20the%20training%20performance%3B%20ii%29%20current%20transformer%0Abased%20MLLMs%20struggle%20to%20achieve%20robust%20multimodal%20fusion%2C%20as%20they%20lack%20a%20design%0Afor%20fusing%20the%20noisy%20modality%3B%20iii%29%20modalities%20with%20varying%20noise%20levels%20hinder%0Aaccurate%20recovery%20of%20missing%20data%20from%20fluctuating%20distributions.%20To%20combat%0Athese%20challenges%2C%20we%20propose%20an%20uncertainty-aware%20multimodal%20fusion%20framework%2C%0Anamed%20DUAL-Health%2C%20for%20outdoor%20health%20monitoring%20in%20dynamic%20and%20noisy%0Aenvironments.%20First%2C%20to%20assess%20the%20impact%20of%20noise%2C%20we%20accurately%20quantify%0Amodality%20uncertainty%20caused%20by%20input%20and%20fluctuation%20noise%20with%20current%20and%0Atemporal%20features.%20Second%2C%20to%20empower%20efficient%20muitimodal%20fusion%20with%0Alow-quality%20modalities%2Cwe%20customize%20the%20fusion%20weight%20for%20each%20modality%20based%0Aon%20quantified%20and%20calibrated%20uncertainty.%20Third%2C%20to%20enhance%20data%20recovery%20from%0Afluctuating%20noisy%20modalities%2C%20we%20align%20modality%20distributions%20within%20a%20common%0Asemantic%20space.%20Extensive%20experiments%20demonstrate%20that%20our%20DUAL-Health%0Aoutperforms%20state-of-the-art%20baselines%20in%20detection%20accuracy%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Uncertainty-aware%2520Multimodal%2520Fusion%2520for%2520Outdoor%2520Health%250A%2520%2520Monitoring%26entry.906535625%3DZihan%2520Fang%2520and%2520Zheng%2520Lin%2520and%2520Senkang%2520Hu%2520and%2520Yihang%2520Tao%2520and%2520Yiqin%2520Deng%2520and%2520Xianhao%2520Chen%2520and%2520Yuguang%2520Fang%26entry.1292438233%3D%2520%2520Outdoor%2520health%2520monitoring%2520is%2520essential%2520to%2520detect%2520early%2520abnormal%2520health%2520status%250Afor%2520safeguarding%2520human%2520health%2520and%2520safety.%2520Conventional%2520outdoor%2520monitoring%250Arelies%2520on%2520static%2520multimodal%2520deep%2520learning%2520frameworks%252C%2520which%2520requires%2520extensive%250Adata%2520training%2520from%2520scratch%2520and%2520fails%2520to%2520capture%2520subtle%2520health%2520status%2520changes.%250AMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520emerge%2520as%2520a%2520promising%2520alternative%252C%250Autilizing%2520only%2520small%2520datasets%2520to%2520fine-tune%2520pre-trained%2520information-rich%2520models%250Afor%2520enabling%2520powerful%2520health%2520status%2520monitoring.%2520Unfortunately%252C%2520MLLM-based%250Aoutdoor%2520health%2520monitoring%2520also%2520faces%2520significant%2520challenges%253A%2520I%2529%2520sensor%2520data%250Acontains%2520input%2520noise%2520stemming%2520from%2520sensor%2520data%2520acquisition%2520and%2520fluctuation%250Anoise%2520caused%2520by%2520sudden%2520changes%2520in%2520physiological%2520signals%2520due%2520to%2520dynamic%2520outdoor%250Aenvironments%252C%2520thus%2520degrading%2520the%2520training%2520performance%253B%2520ii%2529%2520current%2520transformer%250Abased%2520MLLMs%2520struggle%2520to%2520achieve%2520robust%2520multimodal%2520fusion%252C%2520as%2520they%2520lack%2520a%2520design%250Afor%2520fusing%2520the%2520noisy%2520modality%253B%2520iii%2529%2520modalities%2520with%2520varying%2520noise%2520levels%2520hinder%250Aaccurate%2520recovery%2520of%2520missing%2520data%2520from%2520fluctuating%2520distributions.%2520To%2520combat%250Athese%2520challenges%252C%2520we%2520propose%2520an%2520uncertainty-aware%2520multimodal%2520fusion%2520framework%252C%250Anamed%2520DUAL-Health%252C%2520for%2520outdoor%2520health%2520monitoring%2520in%2520dynamic%2520and%2520noisy%250Aenvironments.%2520First%252C%2520to%2520assess%2520the%2520impact%2520of%2520noise%252C%2520we%2520accurately%2520quantify%250Amodality%2520uncertainty%2520caused%2520by%2520input%2520and%2520fluctuation%2520noise%2520with%2520current%2520and%250Atemporal%2520features.%2520Second%252C%2520to%2520empower%2520efficient%2520muitimodal%2520fusion%2520with%250Alow-quality%2520modalities%252Cwe%2520customize%2520the%2520fusion%2520weight%2520for%2520each%2520modality%2520based%250Aon%2520quantified%2520and%2520calibrated%2520uncertainty.%2520Third%252C%2520to%2520enhance%2520data%2520recovery%2520from%250Afluctuating%2520noisy%2520modalities%252C%2520we%2520align%2520modality%2520distributions%2520within%2520a%2520common%250Asemantic%2520space.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520DUAL-Health%250Aoutperforms%2520state-of-the-art%2520baselines%2520in%2520detection%2520accuracy%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Uncertainty-aware%20Multimodal%20Fusion%20for%20Outdoor%20Health%0A%20%20Monitoring&entry.906535625=Zihan%20Fang%20and%20Zheng%20Lin%20and%20Senkang%20Hu%20and%20Yihang%20Tao%20and%20Yiqin%20Deng%20and%20Xianhao%20Chen%20and%20Yuguang%20Fang&entry.1292438233=%20%20Outdoor%20health%20monitoring%20is%20essential%20to%20detect%20early%20abnormal%20health%20status%0Afor%20safeguarding%20human%20health%20and%20safety.%20Conventional%20outdoor%20monitoring%0Arelies%20on%20static%20multimodal%20deep%20learning%20frameworks%2C%20which%20requires%20extensive%0Adata%20training%20from%20scratch%20and%20fails%20to%20capture%20subtle%20health%20status%20changes.%0AMultimodal%20large%20language%20models%20%28MLLMs%29%20emerge%20as%20a%20promising%20alternative%2C%0Autilizing%20only%20small%20datasets%20to%20fine-tune%20pre-trained%20information-rich%20models%0Afor%20enabling%20powerful%20health%20status%20monitoring.%20Unfortunately%2C%20MLLM-based%0Aoutdoor%20health%20monitoring%20also%20faces%20significant%20challenges%3A%20I%29%20sensor%20data%0Acontains%20input%20noise%20stemming%20from%20sensor%20data%20acquisition%20and%20fluctuation%0Anoise%20caused%20by%20sudden%20changes%20in%20physiological%20signals%20due%20to%20dynamic%20outdoor%0Aenvironments%2C%20thus%20degrading%20the%20training%20performance%3B%20ii%29%20current%20transformer%0Abased%20MLLMs%20struggle%20to%20achieve%20robust%20multimodal%20fusion%2C%20as%20they%20lack%20a%20design%0Afor%20fusing%20the%20noisy%20modality%3B%20iii%29%20modalities%20with%20varying%20noise%20levels%20hinder%0Aaccurate%20recovery%20of%20missing%20data%20from%20fluctuating%20distributions.%20To%20combat%0Athese%20challenges%2C%20we%20propose%20an%20uncertainty-aware%20multimodal%20fusion%20framework%2C%0Anamed%20DUAL-Health%2C%20for%20outdoor%20health%20monitoring%20in%20dynamic%20and%20noisy%0Aenvironments.%20First%2C%20to%20assess%20the%20impact%20of%20noise%2C%20we%20accurately%20quantify%0Amodality%20uncertainty%20caused%20by%20input%20and%20fluctuation%20noise%20with%20current%20and%0Atemporal%20features.%20Second%2C%20to%20empower%20efficient%20muitimodal%20fusion%20with%0Alow-quality%20modalities%2Cwe%20customize%20the%20fusion%20weight%20for%20each%20modality%20based%0Aon%20quantified%20and%20calibrated%20uncertainty.%20Third%2C%20to%20enhance%20data%20recovery%20from%0Afluctuating%20noisy%20modalities%2C%20we%20align%20modality%20distributions%20within%20a%20common%0Asemantic%20space.%20Extensive%20experiments%20demonstrate%20that%20our%20DUAL-Health%0Aoutperforms%20state-of-the-art%20baselines%20in%20detection%20accuracy%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09085v1&entry.124074799=Read"},
{"title": "Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language\n  Models", "author": "Wen Wang and Bozhen Fang and Chenchen Jing and Yongliang Shen and Yangyi Shen and Qiuyu Wang and Hao Ouyang and Hao Chen and Chunhua Shen", "abstract": "  Diffusion large language models (dLLMs) generate text through iterative\ndenoising, yet current decoding strategies discard rich intermediate\npredictions in favor of the final output. Our work here reveals a critical\nphenomenon, temporal oscillation, where correct answers often emerge in the\nmiddle process, but are overwritten in later denoising steps. To address this\nissue, we introduce two complementary methods that exploit temporal\nconsistency: 1) Temporal Self-Consistency Voting, a training-free, test-time\ndecoding strategy that aggregates predictions across denoising steps to select\nthe most consistent output; and 2) a post-training method termed Temporal\nConsistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a\nmeasure of semantic stability across intermediate predictions, as a reward\nsignal to encourage stable generations. Empirical results across multiple\nbenchmarks demonstrate the effectiveness of our approach. Using the negative\nTSE reward alone, we observe a remarkable average improvement of 24.7% on the\nCountdown dataset over an existing dLLM. Combined with the accuracy reward, we\nachieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and\n25.3% on Countdown, respectively. Our findings underscore the untapped\npotential of temporal dynamics in dLLMs and offer two simple yet effective\ntools to harness them.\n", "link": "http://arxiv.org/abs/2508.09138v1", "date": "2025-08-12", "relevancy": 1.7275, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6362}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5628}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time%20Is%20a%20Feature%3A%20Exploiting%20Temporal%20Dynamics%20in%20Diffusion%20Language%0A%20%20Models&body=Title%3A%20Time%20Is%20a%20Feature%3A%20Exploiting%20Temporal%20Dynamics%20in%20Diffusion%20Language%0A%20%20Models%0AAuthor%3A%20Wen%20Wang%20and%20Bozhen%20Fang%20and%20Chenchen%20Jing%20and%20Yongliang%20Shen%20and%20Yangyi%20Shen%20and%20Qiuyu%20Wang%20and%20Hao%20Ouyang%20and%20Hao%20Chen%20and%20Chunhua%20Shen%0AAbstract%3A%20%20%20Diffusion%20large%20language%20models%20%28dLLMs%29%20generate%20text%20through%20iterative%0Adenoising%2C%20yet%20current%20decoding%20strategies%20discard%20rich%20intermediate%0Apredictions%20in%20favor%20of%20the%20final%20output.%20Our%20work%20here%20reveals%20a%20critical%0Aphenomenon%2C%20temporal%20oscillation%2C%20where%20correct%20answers%20often%20emerge%20in%20the%0Amiddle%20process%2C%20but%20are%20overwritten%20in%20later%20denoising%20steps.%20To%20address%20this%0Aissue%2C%20we%20introduce%20two%20complementary%20methods%20that%20exploit%20temporal%0Aconsistency%3A%201%29%20Temporal%20Self-Consistency%20Voting%2C%20a%20training-free%2C%20test-time%0Adecoding%20strategy%20that%20aggregates%20predictions%20across%20denoising%20steps%20to%20select%0Athe%20most%20consistent%20output%3B%20and%202%29%20a%20post-training%20method%20termed%20Temporal%0AConsistency%20Reinforcement%2C%20which%20uses%20Temporal%20Semantic%20Entropy%20%28TSE%29%2C%20a%0Ameasure%20of%20semantic%20stability%20across%20intermediate%20predictions%2C%20as%20a%20reward%0Asignal%20to%20encourage%20stable%20generations.%20Empirical%20results%20across%20multiple%0Abenchmarks%20demonstrate%20the%20effectiveness%20of%20our%20approach.%20Using%20the%20negative%0ATSE%20reward%20alone%2C%20we%20observe%20a%20remarkable%20average%20improvement%20of%2024.7%25%20on%20the%0ACountdown%20dataset%20over%20an%20existing%20dLLM.%20Combined%20with%20the%20accuracy%20reward%2C%20we%0Aachieve%20absolute%20gains%20of%202.0%25%20on%20GSM8K%2C%204.3%25%20on%20MATH500%2C%206.6%25%20on%20SVAMP%2C%20and%0A25.3%25%20on%20Countdown%2C%20respectively.%20Our%20findings%20underscore%20the%20untapped%0Apotential%20of%20temporal%20dynamics%20in%20dLLMs%20and%20offer%20two%20simple%20yet%20effective%0Atools%20to%20harness%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09138v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime%2520Is%2520a%2520Feature%253A%2520Exploiting%2520Temporal%2520Dynamics%2520in%2520Diffusion%2520Language%250A%2520%2520Models%26entry.906535625%3DWen%2520Wang%2520and%2520Bozhen%2520Fang%2520and%2520Chenchen%2520Jing%2520and%2520Yongliang%2520Shen%2520and%2520Yangyi%2520Shen%2520and%2520Qiuyu%2520Wang%2520and%2520Hao%2520Ouyang%2520and%2520Hao%2520Chen%2520and%2520Chunhua%2520Shen%26entry.1292438233%3D%2520%2520Diffusion%2520large%2520language%2520models%2520%2528dLLMs%2529%2520generate%2520text%2520through%2520iterative%250Adenoising%252C%2520yet%2520current%2520decoding%2520strategies%2520discard%2520rich%2520intermediate%250Apredictions%2520in%2520favor%2520of%2520the%2520final%2520output.%2520Our%2520work%2520here%2520reveals%2520a%2520critical%250Aphenomenon%252C%2520temporal%2520oscillation%252C%2520where%2520correct%2520answers%2520often%2520emerge%2520in%2520the%250Amiddle%2520process%252C%2520but%2520are%2520overwritten%2520in%2520later%2520denoising%2520steps.%2520To%2520address%2520this%250Aissue%252C%2520we%2520introduce%2520two%2520complementary%2520methods%2520that%2520exploit%2520temporal%250Aconsistency%253A%25201%2529%2520Temporal%2520Self-Consistency%2520Voting%252C%2520a%2520training-free%252C%2520test-time%250Adecoding%2520strategy%2520that%2520aggregates%2520predictions%2520across%2520denoising%2520steps%2520to%2520select%250Athe%2520most%2520consistent%2520output%253B%2520and%25202%2529%2520a%2520post-training%2520method%2520termed%2520Temporal%250AConsistency%2520Reinforcement%252C%2520which%2520uses%2520Temporal%2520Semantic%2520Entropy%2520%2528TSE%2529%252C%2520a%250Ameasure%2520of%2520semantic%2520stability%2520across%2520intermediate%2520predictions%252C%2520as%2520a%2520reward%250Asignal%2520to%2520encourage%2520stable%2520generations.%2520Empirical%2520results%2520across%2520multiple%250Abenchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%2520Using%2520the%2520negative%250ATSE%2520reward%2520alone%252C%2520we%2520observe%2520a%2520remarkable%2520average%2520improvement%2520of%252024.7%2525%2520on%2520the%250ACountdown%2520dataset%2520over%2520an%2520existing%2520dLLM.%2520Combined%2520with%2520the%2520accuracy%2520reward%252C%2520we%250Aachieve%2520absolute%2520gains%2520of%25202.0%2525%2520on%2520GSM8K%252C%25204.3%2525%2520on%2520MATH500%252C%25206.6%2525%2520on%2520SVAMP%252C%2520and%250A25.3%2525%2520on%2520Countdown%252C%2520respectively.%2520Our%2520findings%2520underscore%2520the%2520untapped%250Apotential%2520of%2520temporal%2520dynamics%2520in%2520dLLMs%2520and%2520offer%2520two%2520simple%2520yet%2520effective%250Atools%2520to%2520harness%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09138v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time%20Is%20a%20Feature%3A%20Exploiting%20Temporal%20Dynamics%20in%20Diffusion%20Language%0A%20%20Models&entry.906535625=Wen%20Wang%20and%20Bozhen%20Fang%20and%20Chenchen%20Jing%20and%20Yongliang%20Shen%20and%20Yangyi%20Shen%20and%20Qiuyu%20Wang%20and%20Hao%20Ouyang%20and%20Hao%20Chen%20and%20Chunhua%20Shen&entry.1292438233=%20%20Diffusion%20large%20language%20models%20%28dLLMs%29%20generate%20text%20through%20iterative%0Adenoising%2C%20yet%20current%20decoding%20strategies%20discard%20rich%20intermediate%0Apredictions%20in%20favor%20of%20the%20final%20output.%20Our%20work%20here%20reveals%20a%20critical%0Aphenomenon%2C%20temporal%20oscillation%2C%20where%20correct%20answers%20often%20emerge%20in%20the%0Amiddle%20process%2C%20but%20are%20overwritten%20in%20later%20denoising%20steps.%20To%20address%20this%0Aissue%2C%20we%20introduce%20two%20complementary%20methods%20that%20exploit%20temporal%0Aconsistency%3A%201%29%20Temporal%20Self-Consistency%20Voting%2C%20a%20training-free%2C%20test-time%0Adecoding%20strategy%20that%20aggregates%20predictions%20across%20denoising%20steps%20to%20select%0Athe%20most%20consistent%20output%3B%20and%202%29%20a%20post-training%20method%20termed%20Temporal%0AConsistency%20Reinforcement%2C%20which%20uses%20Temporal%20Semantic%20Entropy%20%28TSE%29%2C%20a%0Ameasure%20of%20semantic%20stability%20across%20intermediate%20predictions%2C%20as%20a%20reward%0Asignal%20to%20encourage%20stable%20generations.%20Empirical%20results%20across%20multiple%0Abenchmarks%20demonstrate%20the%20effectiveness%20of%20our%20approach.%20Using%20the%20negative%0ATSE%20reward%20alone%2C%20we%20observe%20a%20remarkable%20average%20improvement%20of%2024.7%25%20on%20the%0ACountdown%20dataset%20over%20an%20existing%20dLLM.%20Combined%20with%20the%20accuracy%20reward%2C%20we%0Aachieve%20absolute%20gains%20of%202.0%25%20on%20GSM8K%2C%204.3%25%20on%20MATH500%2C%206.6%25%20on%20SVAMP%2C%20and%0A25.3%25%20on%20Countdown%2C%20respectively.%20Our%20findings%20underscore%20the%20untapped%0Apotential%20of%20temporal%20dynamics%20in%20dLLMs%20and%20offer%20two%20simple%20yet%20effective%0Atools%20to%20harness%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09138v1&entry.124074799=Read"},
{"title": "Un-EVIMO: Unsupervised Event-Based Independent Motion Segmentation", "author": "Ziyun Wang and Jinyuan Guo and Kostas Daniilidis", "abstract": "  Event cameras are a novel type of biologically inspired vision sensor known\nfor their high temporal resolution, high dynamic range, and low power\nconsumption. Because of these properties, they are well-suited for processing\nfast motions that require rapid reactions. Although event cameras have recently\nshown competitive performance in unsupervised optical flow estimation,\nperformance in detecting independently moving objects (IMOs) is lacking behind,\nalthough event-based methods would be suited for this task based on their low\nlatency and HDR properties. Previous approaches to event-based IMO segmentation\nhave been heavily dependent on labeled data. However, biological vision systems\nhave developed the ability to avoid moving objects through daily tasks without\nbeing given explicit labels. In this work, we propose the first event framework\nthat generates IMO pseudo-labels using geometric constraints. Due to its\nunsupervised nature, our method can handle an arbitrary number of not\npredetermined objects and is easily scalable to datasets where expensive IMO\nlabels are not readily available. We evaluate our approach on the EVIMO dataset\nand show that it performs competitively with supervised methods, both\nquantitatively and qualitatively.\n", "link": "http://arxiv.org/abs/2312.00114v2", "date": "2025-08-12", "relevancy": 1.7031, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5789}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5719}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Un-EVIMO%3A%20Unsupervised%20Event-Based%20Independent%20Motion%20Segmentation&body=Title%3A%20Un-EVIMO%3A%20Unsupervised%20Event-Based%20Independent%20Motion%20Segmentation%0AAuthor%3A%20Ziyun%20Wang%20and%20Jinyuan%20Guo%20and%20Kostas%20Daniilidis%0AAbstract%3A%20%20%20Event%20cameras%20are%20a%20novel%20type%20of%20biologically%20inspired%20vision%20sensor%20known%0Afor%20their%20high%20temporal%20resolution%2C%20high%20dynamic%20range%2C%20and%20low%20power%0Aconsumption.%20Because%20of%20these%20properties%2C%20they%20are%20well-suited%20for%20processing%0Afast%20motions%20that%20require%20rapid%20reactions.%20Although%20event%20cameras%20have%20recently%0Ashown%20competitive%20performance%20in%20unsupervised%20optical%20flow%20estimation%2C%0Aperformance%20in%20detecting%20independently%20moving%20objects%20%28IMOs%29%20is%20lacking%20behind%2C%0Aalthough%20event-based%20methods%20would%20be%20suited%20for%20this%20task%20based%20on%20their%20low%0Alatency%20and%20HDR%20properties.%20Previous%20approaches%20to%20event-based%20IMO%20segmentation%0Ahave%20been%20heavily%20dependent%20on%20labeled%20data.%20However%2C%20biological%20vision%20systems%0Ahave%20developed%20the%20ability%20to%20avoid%20moving%20objects%20through%20daily%20tasks%20without%0Abeing%20given%20explicit%20labels.%20In%20this%20work%2C%20we%20propose%20the%20first%20event%20framework%0Athat%20generates%20IMO%20pseudo-labels%20using%20geometric%20constraints.%20Due%20to%20its%0Aunsupervised%20nature%2C%20our%20method%20can%20handle%20an%20arbitrary%20number%20of%20not%0Apredetermined%20objects%20and%20is%20easily%20scalable%20to%20datasets%20where%20expensive%20IMO%0Alabels%20are%20not%20readily%20available.%20We%20evaluate%20our%20approach%20on%20the%20EVIMO%20dataset%0Aand%20show%20that%20it%20performs%20competitively%20with%20supervised%20methods%2C%20both%0Aquantitatively%20and%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00114v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUn-EVIMO%253A%2520Unsupervised%2520Event-Based%2520Independent%2520Motion%2520Segmentation%26entry.906535625%3DZiyun%2520Wang%2520and%2520Jinyuan%2520Guo%2520and%2520Kostas%2520Daniilidis%26entry.1292438233%3D%2520%2520Event%2520cameras%2520are%2520a%2520novel%2520type%2520of%2520biologically%2520inspired%2520vision%2520sensor%2520known%250Afor%2520their%2520high%2520temporal%2520resolution%252C%2520high%2520dynamic%2520range%252C%2520and%2520low%2520power%250Aconsumption.%2520Because%2520of%2520these%2520properties%252C%2520they%2520are%2520well-suited%2520for%2520processing%250Afast%2520motions%2520that%2520require%2520rapid%2520reactions.%2520Although%2520event%2520cameras%2520have%2520recently%250Ashown%2520competitive%2520performance%2520in%2520unsupervised%2520optical%2520flow%2520estimation%252C%250Aperformance%2520in%2520detecting%2520independently%2520moving%2520objects%2520%2528IMOs%2529%2520is%2520lacking%2520behind%252C%250Aalthough%2520event-based%2520methods%2520would%2520be%2520suited%2520for%2520this%2520task%2520based%2520on%2520their%2520low%250Alatency%2520and%2520HDR%2520properties.%2520Previous%2520approaches%2520to%2520event-based%2520IMO%2520segmentation%250Ahave%2520been%2520heavily%2520dependent%2520on%2520labeled%2520data.%2520However%252C%2520biological%2520vision%2520systems%250Ahave%2520developed%2520the%2520ability%2520to%2520avoid%2520moving%2520objects%2520through%2520daily%2520tasks%2520without%250Abeing%2520given%2520explicit%2520labels.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520first%2520event%2520framework%250Athat%2520generates%2520IMO%2520pseudo-labels%2520using%2520geometric%2520constraints.%2520Due%2520to%2520its%250Aunsupervised%2520nature%252C%2520our%2520method%2520can%2520handle%2520an%2520arbitrary%2520number%2520of%2520not%250Apredetermined%2520objects%2520and%2520is%2520easily%2520scalable%2520to%2520datasets%2520where%2520expensive%2520IMO%250Alabels%2520are%2520not%2520readily%2520available.%2520We%2520evaluate%2520our%2520approach%2520on%2520the%2520EVIMO%2520dataset%250Aand%2520show%2520that%2520it%2520performs%2520competitively%2520with%2520supervised%2520methods%252C%2520both%250Aquantitatively%2520and%2520qualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00114v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Un-EVIMO%3A%20Unsupervised%20Event-Based%20Independent%20Motion%20Segmentation&entry.906535625=Ziyun%20Wang%20and%20Jinyuan%20Guo%20and%20Kostas%20Daniilidis&entry.1292438233=%20%20Event%20cameras%20are%20a%20novel%20type%20of%20biologically%20inspired%20vision%20sensor%20known%0Afor%20their%20high%20temporal%20resolution%2C%20high%20dynamic%20range%2C%20and%20low%20power%0Aconsumption.%20Because%20of%20these%20properties%2C%20they%20are%20well-suited%20for%20processing%0Afast%20motions%20that%20require%20rapid%20reactions.%20Although%20event%20cameras%20have%20recently%0Ashown%20competitive%20performance%20in%20unsupervised%20optical%20flow%20estimation%2C%0Aperformance%20in%20detecting%20independently%20moving%20objects%20%28IMOs%29%20is%20lacking%20behind%2C%0Aalthough%20event-based%20methods%20would%20be%20suited%20for%20this%20task%20based%20on%20their%20low%0Alatency%20and%20HDR%20properties.%20Previous%20approaches%20to%20event-based%20IMO%20segmentation%0Ahave%20been%20heavily%20dependent%20on%20labeled%20data.%20However%2C%20biological%20vision%20systems%0Ahave%20developed%20the%20ability%20to%20avoid%20moving%20objects%20through%20daily%20tasks%20without%0Abeing%20given%20explicit%20labels.%20In%20this%20work%2C%20we%20propose%20the%20first%20event%20framework%0Athat%20generates%20IMO%20pseudo-labels%20using%20geometric%20constraints.%20Due%20to%20its%0Aunsupervised%20nature%2C%20our%20method%20can%20handle%20an%20arbitrary%20number%20of%20not%0Apredetermined%20objects%20and%20is%20easily%20scalable%20to%20datasets%20where%20expensive%20IMO%0Alabels%20are%20not%20readily%20available.%20We%20evaluate%20our%20approach%20on%20the%20EVIMO%20dataset%0Aand%20show%20that%20it%20performs%20competitively%20with%20supervised%20methods%2C%20both%0Aquantitatively%20and%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00114v2&entry.124074799=Read"},
{"title": "Rational Inverse Reasoning", "author": "Ben Zandonati and Tom\u00e1s Lozano-P\u00e9rez and Leslie Pack Kaelbling", "abstract": "  Humans can observe a single, imperfect demonstration and immediately\ngeneralize to very different problem settings. Robots, in contrast, often\nrequire hundreds of examples and still struggle to generalize beyond the\ntraining conditions. We argue that this limitation arises from the inability to\nrecover the latent explanations that underpin intelligent behavior, and that\nthese explanations can take the form of structured programs consisting of\nhigh-level goals, sub-task decomposition, and execution constraints. In this\nwork, we introduce Rational Inverse Reasoning (RIR), a framework for inferring\nthese latent programs through a hierarchical generative model of behavior. RIR\nframes few-shot imitation as Bayesian program induction: a vision-language\nmodel iteratively proposes structured symbolic task hypotheses, while a\nplanner-in-the-loop inference scheme scores each by the likelihood of the\nobserved demonstration under that hypothesis. This loop yields a posterior over\nconcise, executable programs. We evaluate RIR on a suite of continuous\nmanipulation tasks designed to test one-shot and few-shot generalization across\nvariations in object pose, count, geometry, and layout. With as little as one\ndemonstration, RIR infers the intended task structure and generalizes to novel\nsettings, outperforming state-of-the-art vision-language model baselines.\n", "link": "http://arxiv.org/abs/2508.08983v1", "date": "2025-08-12", "relevancy": 1.6954, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5847}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5735}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rational%20Inverse%20Reasoning&body=Title%3A%20Rational%20Inverse%20Reasoning%0AAuthor%3A%20Ben%20Zandonati%20and%20Tom%C3%A1s%20Lozano-P%C3%A9rez%20and%20Leslie%20Pack%20Kaelbling%0AAbstract%3A%20%20%20Humans%20can%20observe%20a%20single%2C%20imperfect%20demonstration%20and%20immediately%0Ageneralize%20to%20very%20different%20problem%20settings.%20Robots%2C%20in%20contrast%2C%20often%0Arequire%20hundreds%20of%20examples%20and%20still%20struggle%20to%20generalize%20beyond%20the%0Atraining%20conditions.%20We%20argue%20that%20this%20limitation%20arises%20from%20the%20inability%20to%0Arecover%20the%20latent%20explanations%20that%20underpin%20intelligent%20behavior%2C%20and%20that%0Athese%20explanations%20can%20take%20the%20form%20of%20structured%20programs%20consisting%20of%0Ahigh-level%20goals%2C%20sub-task%20decomposition%2C%20and%20execution%20constraints.%20In%20this%0Awork%2C%20we%20introduce%20Rational%20Inverse%20Reasoning%20%28RIR%29%2C%20a%20framework%20for%20inferring%0Athese%20latent%20programs%20through%20a%20hierarchical%20generative%20model%20of%20behavior.%20RIR%0Aframes%20few-shot%20imitation%20as%20Bayesian%20program%20induction%3A%20a%20vision-language%0Amodel%20iteratively%20proposes%20structured%20symbolic%20task%20hypotheses%2C%20while%20a%0Aplanner-in-the-loop%20inference%20scheme%20scores%20each%20by%20the%20likelihood%20of%20the%0Aobserved%20demonstration%20under%20that%20hypothesis.%20This%20loop%20yields%20a%20posterior%20over%0Aconcise%2C%20executable%20programs.%20We%20evaluate%20RIR%20on%20a%20suite%20of%20continuous%0Amanipulation%20tasks%20designed%20to%20test%20one-shot%20and%20few-shot%20generalization%20across%0Avariations%20in%20object%20pose%2C%20count%2C%20geometry%2C%20and%20layout.%20With%20as%20little%20as%20one%0Ademonstration%2C%20RIR%20infers%20the%20intended%20task%20structure%20and%20generalizes%20to%20novel%0Asettings%2C%20outperforming%20state-of-the-art%20vision-language%20model%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRational%2520Inverse%2520Reasoning%26entry.906535625%3DBen%2520Zandonati%2520and%2520Tom%25C3%25A1s%2520Lozano-P%25C3%25A9rez%2520and%2520Leslie%2520Pack%2520Kaelbling%26entry.1292438233%3D%2520%2520Humans%2520can%2520observe%2520a%2520single%252C%2520imperfect%2520demonstration%2520and%2520immediately%250Ageneralize%2520to%2520very%2520different%2520problem%2520settings.%2520Robots%252C%2520in%2520contrast%252C%2520often%250Arequire%2520hundreds%2520of%2520examples%2520and%2520still%2520struggle%2520to%2520generalize%2520beyond%2520the%250Atraining%2520conditions.%2520We%2520argue%2520that%2520this%2520limitation%2520arises%2520from%2520the%2520inability%2520to%250Arecover%2520the%2520latent%2520explanations%2520that%2520underpin%2520intelligent%2520behavior%252C%2520and%2520that%250Athese%2520explanations%2520can%2520take%2520the%2520form%2520of%2520structured%2520programs%2520consisting%2520of%250Ahigh-level%2520goals%252C%2520sub-task%2520decomposition%252C%2520and%2520execution%2520constraints.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520Rational%2520Inverse%2520Reasoning%2520%2528RIR%2529%252C%2520a%2520framework%2520for%2520inferring%250Athese%2520latent%2520programs%2520through%2520a%2520hierarchical%2520generative%2520model%2520of%2520behavior.%2520RIR%250Aframes%2520few-shot%2520imitation%2520as%2520Bayesian%2520program%2520induction%253A%2520a%2520vision-language%250Amodel%2520iteratively%2520proposes%2520structured%2520symbolic%2520task%2520hypotheses%252C%2520while%2520a%250Aplanner-in-the-loop%2520inference%2520scheme%2520scores%2520each%2520by%2520the%2520likelihood%2520of%2520the%250Aobserved%2520demonstration%2520under%2520that%2520hypothesis.%2520This%2520loop%2520yields%2520a%2520posterior%2520over%250Aconcise%252C%2520executable%2520programs.%2520We%2520evaluate%2520RIR%2520on%2520a%2520suite%2520of%2520continuous%250Amanipulation%2520tasks%2520designed%2520to%2520test%2520one-shot%2520and%2520few-shot%2520generalization%2520across%250Avariations%2520in%2520object%2520pose%252C%2520count%252C%2520geometry%252C%2520and%2520layout.%2520With%2520as%2520little%2520as%2520one%250Ademonstration%252C%2520RIR%2520infers%2520the%2520intended%2520task%2520structure%2520and%2520generalizes%2520to%2520novel%250Asettings%252C%2520outperforming%2520state-of-the-art%2520vision-language%2520model%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rational%20Inverse%20Reasoning&entry.906535625=Ben%20Zandonati%20and%20Tom%C3%A1s%20Lozano-P%C3%A9rez%20and%20Leslie%20Pack%20Kaelbling&entry.1292438233=%20%20Humans%20can%20observe%20a%20single%2C%20imperfect%20demonstration%20and%20immediately%0Ageneralize%20to%20very%20different%20problem%20settings.%20Robots%2C%20in%20contrast%2C%20often%0Arequire%20hundreds%20of%20examples%20and%20still%20struggle%20to%20generalize%20beyond%20the%0Atraining%20conditions.%20We%20argue%20that%20this%20limitation%20arises%20from%20the%20inability%20to%0Arecover%20the%20latent%20explanations%20that%20underpin%20intelligent%20behavior%2C%20and%20that%0Athese%20explanations%20can%20take%20the%20form%20of%20structured%20programs%20consisting%20of%0Ahigh-level%20goals%2C%20sub-task%20decomposition%2C%20and%20execution%20constraints.%20In%20this%0Awork%2C%20we%20introduce%20Rational%20Inverse%20Reasoning%20%28RIR%29%2C%20a%20framework%20for%20inferring%0Athese%20latent%20programs%20through%20a%20hierarchical%20generative%20model%20of%20behavior.%20RIR%0Aframes%20few-shot%20imitation%20as%20Bayesian%20program%20induction%3A%20a%20vision-language%0Amodel%20iteratively%20proposes%20structured%20symbolic%20task%20hypotheses%2C%20while%20a%0Aplanner-in-the-loop%20inference%20scheme%20scores%20each%20by%20the%20likelihood%20of%20the%0Aobserved%20demonstration%20under%20that%20hypothesis.%20This%20loop%20yields%20a%20posterior%20over%0Aconcise%2C%20executable%20programs.%20We%20evaluate%20RIR%20on%20a%20suite%20of%20continuous%0Amanipulation%20tasks%20designed%20to%20test%20one-shot%20and%20few-shot%20generalization%20across%0Avariations%20in%20object%20pose%2C%20count%2C%20geometry%2C%20and%20layout.%20With%20as%20little%20as%20one%0Ademonstration%2C%20RIR%20infers%20the%20intended%20task%20structure%20and%20generalizes%20to%20novel%0Asettings%2C%20outperforming%20state-of-the-art%20vision-language%20model%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08983v1&entry.124074799=Read"},
{"title": "Unsupervised Skill Discovery as Exploration for Learning Agile\n  Locomotion", "author": "Seungeun Rho and Kartik Garg and Morgan Byrd and Sehoon Ha", "abstract": "  Exploration is crucial for enabling legged robots to learn agile locomotion\nbehaviors that can overcome diverse obstacles. However, such exploration is\ninherently challenging, and we often rely on extensive reward engineering,\nexpert demonstrations, or curriculum learning - all of which limit\ngeneralizability. In this work, we propose Skill Discovery as Exploration\n(SDAX), a novel learning framework that significantly reduces human engineering\neffort. SDAX leverages unsupervised skill discovery to autonomously acquire a\ndiverse repertoire of skills for overcoming obstacles. To dynamically regulate\nthe level of exploration during training, SDAX employs a bi-level optimization\nprocess that autonomously adjusts the degree of exploration. We demonstrate\nthat SDAX enables quadrupedal robots to acquire highly agile behaviors\nincluding crawling, climbing, leaping, and executing complex maneuvers such as\njumping off vertical walls. Finally, we deploy the learned policy on real\nhardware, validating its successful transfer to the real world.\n", "link": "http://arxiv.org/abs/2508.08982v1", "date": "2025-08-12", "relevancy": 1.6914, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6592}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5391}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Skill%20Discovery%20as%20Exploration%20for%20Learning%20Agile%0A%20%20Locomotion&body=Title%3A%20Unsupervised%20Skill%20Discovery%20as%20Exploration%20for%20Learning%20Agile%0A%20%20Locomotion%0AAuthor%3A%20Seungeun%20Rho%20and%20Kartik%20Garg%20and%20Morgan%20Byrd%20and%20Sehoon%20Ha%0AAbstract%3A%20%20%20Exploration%20is%20crucial%20for%20enabling%20legged%20robots%20to%20learn%20agile%20locomotion%0Abehaviors%20that%20can%20overcome%20diverse%20obstacles.%20However%2C%20such%20exploration%20is%0Ainherently%20challenging%2C%20and%20we%20often%20rely%20on%20extensive%20reward%20engineering%2C%0Aexpert%20demonstrations%2C%20or%20curriculum%20learning%20-%20all%20of%20which%20limit%0Ageneralizability.%20In%20this%20work%2C%20we%20propose%20Skill%20Discovery%20as%20Exploration%0A%28SDAX%29%2C%20a%20novel%20learning%20framework%20that%20significantly%20reduces%20human%20engineering%0Aeffort.%20SDAX%20leverages%20unsupervised%20skill%20discovery%20to%20autonomously%20acquire%20a%0Adiverse%20repertoire%20of%20skills%20for%20overcoming%20obstacles.%20To%20dynamically%20regulate%0Athe%20level%20of%20exploration%20during%20training%2C%20SDAX%20employs%20a%20bi-level%20optimization%0Aprocess%20that%20autonomously%20adjusts%20the%20degree%20of%20exploration.%20We%20demonstrate%0Athat%20SDAX%20enables%20quadrupedal%20robots%20to%20acquire%20highly%20agile%20behaviors%0Aincluding%20crawling%2C%20climbing%2C%20leaping%2C%20and%20executing%20complex%20maneuvers%20such%20as%0Ajumping%20off%20vertical%20walls.%20Finally%2C%20we%20deploy%20the%20learned%20policy%20on%20real%0Ahardware%2C%20validating%20its%20successful%20transfer%20to%20the%20real%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Skill%2520Discovery%2520as%2520Exploration%2520for%2520Learning%2520Agile%250A%2520%2520Locomotion%26entry.906535625%3DSeungeun%2520Rho%2520and%2520Kartik%2520Garg%2520and%2520Morgan%2520Byrd%2520and%2520Sehoon%2520Ha%26entry.1292438233%3D%2520%2520Exploration%2520is%2520crucial%2520for%2520enabling%2520legged%2520robots%2520to%2520learn%2520agile%2520locomotion%250Abehaviors%2520that%2520can%2520overcome%2520diverse%2520obstacles.%2520However%252C%2520such%2520exploration%2520is%250Ainherently%2520challenging%252C%2520and%2520we%2520often%2520rely%2520on%2520extensive%2520reward%2520engineering%252C%250Aexpert%2520demonstrations%252C%2520or%2520curriculum%2520learning%2520-%2520all%2520of%2520which%2520limit%250Ageneralizability.%2520In%2520this%2520work%252C%2520we%2520propose%2520Skill%2520Discovery%2520as%2520Exploration%250A%2528SDAX%2529%252C%2520a%2520novel%2520learning%2520framework%2520that%2520significantly%2520reduces%2520human%2520engineering%250Aeffort.%2520SDAX%2520leverages%2520unsupervised%2520skill%2520discovery%2520to%2520autonomously%2520acquire%2520a%250Adiverse%2520repertoire%2520of%2520skills%2520for%2520overcoming%2520obstacles.%2520To%2520dynamically%2520regulate%250Athe%2520level%2520of%2520exploration%2520during%2520training%252C%2520SDAX%2520employs%2520a%2520bi-level%2520optimization%250Aprocess%2520that%2520autonomously%2520adjusts%2520the%2520degree%2520of%2520exploration.%2520We%2520demonstrate%250Athat%2520SDAX%2520enables%2520quadrupedal%2520robots%2520to%2520acquire%2520highly%2520agile%2520behaviors%250Aincluding%2520crawling%252C%2520climbing%252C%2520leaping%252C%2520and%2520executing%2520complex%2520maneuvers%2520such%2520as%250Ajumping%2520off%2520vertical%2520walls.%2520Finally%252C%2520we%2520deploy%2520the%2520learned%2520policy%2520on%2520real%250Ahardware%252C%2520validating%2520its%2520successful%2520transfer%2520to%2520the%2520real%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Skill%20Discovery%20as%20Exploration%20for%20Learning%20Agile%0A%20%20Locomotion&entry.906535625=Seungeun%20Rho%20and%20Kartik%20Garg%20and%20Morgan%20Byrd%20and%20Sehoon%20Ha&entry.1292438233=%20%20Exploration%20is%20crucial%20for%20enabling%20legged%20robots%20to%20learn%20agile%20locomotion%0Abehaviors%20that%20can%20overcome%20diverse%20obstacles.%20However%2C%20such%20exploration%20is%0Ainherently%20challenging%2C%20and%20we%20often%20rely%20on%20extensive%20reward%20engineering%2C%0Aexpert%20demonstrations%2C%20or%20curriculum%20learning%20-%20all%20of%20which%20limit%0Ageneralizability.%20In%20this%20work%2C%20we%20propose%20Skill%20Discovery%20as%20Exploration%0A%28SDAX%29%2C%20a%20novel%20learning%20framework%20that%20significantly%20reduces%20human%20engineering%0Aeffort.%20SDAX%20leverages%20unsupervised%20skill%20discovery%20to%20autonomously%20acquire%20a%0Adiverse%20repertoire%20of%20skills%20for%20overcoming%20obstacles.%20To%20dynamically%20regulate%0Athe%20level%20of%20exploration%20during%20training%2C%20SDAX%20employs%20a%20bi-level%20optimization%0Aprocess%20that%20autonomously%20adjusts%20the%20degree%20of%20exploration.%20We%20demonstrate%0Athat%20SDAX%20enables%20quadrupedal%20robots%20to%20acquire%20highly%20agile%20behaviors%0Aincluding%20crawling%2C%20climbing%2C%20leaping%2C%20and%20executing%20complex%20maneuvers%20such%20as%0Ajumping%20off%20vertical%20walls.%20Finally%2C%20we%20deploy%20the%20learned%20policy%20on%20real%0Ahardware%2C%20validating%20its%20successful%20transfer%20to%20the%20real%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08982v1&entry.124074799=Read"},
{"title": "fastkqr: A Fast Algorithm for Kernel Quantile Regression", "author": "Qian Tang and Yuwen Gu and Boxiang Wang", "abstract": "  Quantile regression is a powerful tool for robust and heterogeneous learning\nthat has seen applications in a diverse range of applied areas. However, its\nbroader application is often hindered by the substantial computational demands\narising from the non-smooth quantile loss function. In this paper, we introduce\na novel algorithm named fastkqr, which significantly advances the computation\nof quantile regression in reproducing kernel Hilbert spaces. The core of\nfastkqr is a finite smoothing algorithm that magically produces exact\nregression quantiles, rather than approximations. To further accelerate the\nalgorithm, we equip fastkqr with a novel spectral technique that carefully\nreutilizes matrix computations. In addition, we extend fastkqr to accommodate a\nflexible kernel quantile regression with a data-driven crossing penalty,\naddressing the interpretability challenges of crossing quantile curves at\nmultiple levels. We have implemented fastkqr in a publicly available R package.\nExtensive simulations and real applications show that fastkqr matches the\naccuracy of state-of-the-art algorithms but can operate up to an order of\nmagnitude faster.\n", "link": "http://arxiv.org/abs/2408.05393v2", "date": "2025-08-12", "relevancy": 1.6865, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4282}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4188}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20fastkqr%3A%20A%20Fast%20Algorithm%20for%20Kernel%20Quantile%20Regression&body=Title%3A%20fastkqr%3A%20A%20Fast%20Algorithm%20for%20Kernel%20Quantile%20Regression%0AAuthor%3A%20Qian%20Tang%20and%20Yuwen%20Gu%20and%20Boxiang%20Wang%0AAbstract%3A%20%20%20Quantile%20regression%20is%20a%20powerful%20tool%20for%20robust%20and%20heterogeneous%20learning%0Athat%20has%20seen%20applications%20in%20a%20diverse%20range%20of%20applied%20areas.%20However%2C%20its%0Abroader%20application%20is%20often%20hindered%20by%20the%20substantial%20computational%20demands%0Aarising%20from%20the%20non-smooth%20quantile%20loss%20function.%20In%20this%20paper%2C%20we%20introduce%0Aa%20novel%20algorithm%20named%20fastkqr%2C%20which%20significantly%20advances%20the%20computation%0Aof%20quantile%20regression%20in%20reproducing%20kernel%20Hilbert%20spaces.%20The%20core%20of%0Afastkqr%20is%20a%20finite%20smoothing%20algorithm%20that%20magically%20produces%20exact%0Aregression%20quantiles%2C%20rather%20than%20approximations.%20To%20further%20accelerate%20the%0Aalgorithm%2C%20we%20equip%20fastkqr%20with%20a%20novel%20spectral%20technique%20that%20carefully%0Areutilizes%20matrix%20computations.%20In%20addition%2C%20we%20extend%20fastkqr%20to%20accommodate%20a%0Aflexible%20kernel%20quantile%20regression%20with%20a%20data-driven%20crossing%20penalty%2C%0Aaddressing%20the%20interpretability%20challenges%20of%20crossing%20quantile%20curves%20at%0Amultiple%20levels.%20We%20have%20implemented%20fastkqr%20in%20a%20publicly%20available%20R%20package.%0AExtensive%20simulations%20and%20real%20applications%20show%20that%20fastkqr%20matches%20the%0Aaccuracy%20of%20state-of-the-art%20algorithms%20but%20can%20operate%20up%20to%20an%20order%20of%0Amagnitude%20faster.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05393v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dfastkqr%253A%2520A%2520Fast%2520Algorithm%2520for%2520Kernel%2520Quantile%2520Regression%26entry.906535625%3DQian%2520Tang%2520and%2520Yuwen%2520Gu%2520and%2520Boxiang%2520Wang%26entry.1292438233%3D%2520%2520Quantile%2520regression%2520is%2520a%2520powerful%2520tool%2520for%2520robust%2520and%2520heterogeneous%2520learning%250Athat%2520has%2520seen%2520applications%2520in%2520a%2520diverse%2520range%2520of%2520applied%2520areas.%2520However%252C%2520its%250Abroader%2520application%2520is%2520often%2520hindered%2520by%2520the%2520substantial%2520computational%2520demands%250Aarising%2520from%2520the%2520non-smooth%2520quantile%2520loss%2520function.%2520In%2520this%2520paper%252C%2520we%2520introduce%250Aa%2520novel%2520algorithm%2520named%2520fastkqr%252C%2520which%2520significantly%2520advances%2520the%2520computation%250Aof%2520quantile%2520regression%2520in%2520reproducing%2520kernel%2520Hilbert%2520spaces.%2520The%2520core%2520of%250Afastkqr%2520is%2520a%2520finite%2520smoothing%2520algorithm%2520that%2520magically%2520produces%2520exact%250Aregression%2520quantiles%252C%2520rather%2520than%2520approximations.%2520To%2520further%2520accelerate%2520the%250Aalgorithm%252C%2520we%2520equip%2520fastkqr%2520with%2520a%2520novel%2520spectral%2520technique%2520that%2520carefully%250Areutilizes%2520matrix%2520computations.%2520In%2520addition%252C%2520we%2520extend%2520fastkqr%2520to%2520accommodate%2520a%250Aflexible%2520kernel%2520quantile%2520regression%2520with%2520a%2520data-driven%2520crossing%2520penalty%252C%250Aaddressing%2520the%2520interpretability%2520challenges%2520of%2520crossing%2520quantile%2520curves%2520at%250Amultiple%2520levels.%2520We%2520have%2520implemented%2520fastkqr%2520in%2520a%2520publicly%2520available%2520R%2520package.%250AExtensive%2520simulations%2520and%2520real%2520applications%2520show%2520that%2520fastkqr%2520matches%2520the%250Aaccuracy%2520of%2520state-of-the-art%2520algorithms%2520but%2520can%2520operate%2520up%2520to%2520an%2520order%2520of%250Amagnitude%2520faster.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05393v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=fastkqr%3A%20A%20Fast%20Algorithm%20for%20Kernel%20Quantile%20Regression&entry.906535625=Qian%20Tang%20and%20Yuwen%20Gu%20and%20Boxiang%20Wang&entry.1292438233=%20%20Quantile%20regression%20is%20a%20powerful%20tool%20for%20robust%20and%20heterogeneous%20learning%0Athat%20has%20seen%20applications%20in%20a%20diverse%20range%20of%20applied%20areas.%20However%2C%20its%0Abroader%20application%20is%20often%20hindered%20by%20the%20substantial%20computational%20demands%0Aarising%20from%20the%20non-smooth%20quantile%20loss%20function.%20In%20this%20paper%2C%20we%20introduce%0Aa%20novel%20algorithm%20named%20fastkqr%2C%20which%20significantly%20advances%20the%20computation%0Aof%20quantile%20regression%20in%20reproducing%20kernel%20Hilbert%20spaces.%20The%20core%20of%0Afastkqr%20is%20a%20finite%20smoothing%20algorithm%20that%20magically%20produces%20exact%0Aregression%20quantiles%2C%20rather%20than%20approximations.%20To%20further%20accelerate%20the%0Aalgorithm%2C%20we%20equip%20fastkqr%20with%20a%20novel%20spectral%20technique%20that%20carefully%0Areutilizes%20matrix%20computations.%20In%20addition%2C%20we%20extend%20fastkqr%20to%20accommodate%20a%0Aflexible%20kernel%20quantile%20regression%20with%20a%20data-driven%20crossing%20penalty%2C%0Aaddressing%20the%20interpretability%20challenges%20of%20crossing%20quantile%20curves%20at%0Amultiple%20levels.%20We%20have%20implemented%20fastkqr%20in%20a%20publicly%20available%20R%20package.%0AExtensive%20simulations%20and%20real%20applications%20show%20that%20fastkqr%20matches%20the%0Aaccuracy%20of%20state-of-the-art%20algorithms%20but%20can%20operate%20up%20to%20an%20order%20of%0Amagnitude%20faster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05393v2&entry.124074799=Read"},
{"title": "Text-conditioned State Space Model For Domain-generalized Change\n  Detection Visual Question Answering", "author": "Elman Ghazaei and Erchan Aptoula", "abstract": "  The Earth's surface is constantly changing, and detecting these changes\nprovides valuable insights that benefit various aspects of human society. While\ntraditional change detection methods have been employed to detect changes from\nbi-temporal images, these approaches typically require expert knowledge for\naccurate interpretation. To enable broader and more flexible access to change\ninformation by non-expert users, the task of Change Detection Visual Question\nAnswering (CDVQA) has been introduced. However, existing CDVQA methods have\nbeen developed under the assumption that training and testing datasets share\nsimilar distributions. This assumption does not hold in real-world\napplications, where domain shifts often occur. In this paper, the CDVQA task is\nrevisited with a focus on addressing domain shift. To this end, a new\nmulti-modal and multi-domain dataset, BrightVQA, is introduced to facilitate\ndomain generalization research in CDVQA. Furthermore, a novel state space\nmodel, termed Text-Conditioned State Space Model (TCSSM), is proposed. The\nTCSSM framework is designed to leverage both bi-temporal imagery and\ngeo-disaster-related textual information in an unified manner to extract\ndomain-invariant features across domains. Input-dependent parameters existing\nin TCSSM are dynamically predicted by using both bi-temporal images and\ngeo-disaster-related description, thereby facilitating the alignment between\nbi-temporal visual data and the associated textual descriptions. Extensive\nexperiments are conducted to evaluate the proposed method against\nstate-of-the-art models, and superior performance is consistently demonstrated.\nThe code and dataset will be made publicly available upon acceptance at\nhttps://github.com/Elman295/TCSSM.\n", "link": "http://arxiv.org/abs/2508.08974v1", "date": "2025-08-12", "relevancy": 1.675, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5687}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5554}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-conditioned%20State%20Space%20Model%20For%20Domain-generalized%20Change%0A%20%20Detection%20Visual%20Question%20Answering&body=Title%3A%20Text-conditioned%20State%20Space%20Model%20For%20Domain-generalized%20Change%0A%20%20Detection%20Visual%20Question%20Answering%0AAuthor%3A%20Elman%20Ghazaei%20and%20Erchan%20Aptoula%0AAbstract%3A%20%20%20The%20Earth%27s%20surface%20is%20constantly%20changing%2C%20and%20detecting%20these%20changes%0Aprovides%20valuable%20insights%20that%20benefit%20various%20aspects%20of%20human%20society.%20While%0Atraditional%20change%20detection%20methods%20have%20been%20employed%20to%20detect%20changes%20from%0Abi-temporal%20images%2C%20these%20approaches%20typically%20require%20expert%20knowledge%20for%0Aaccurate%20interpretation.%20To%20enable%20broader%20and%20more%20flexible%20access%20to%20change%0Ainformation%20by%20non-expert%20users%2C%20the%20task%20of%20Change%20Detection%20Visual%20Question%0AAnswering%20%28CDVQA%29%20has%20been%20introduced.%20However%2C%20existing%20CDVQA%20methods%20have%0Abeen%20developed%20under%20the%20assumption%20that%20training%20and%20testing%20datasets%20share%0Asimilar%20distributions.%20This%20assumption%20does%20not%20hold%20in%20real-world%0Aapplications%2C%20where%20domain%20shifts%20often%20occur.%20In%20this%20paper%2C%20the%20CDVQA%20task%20is%0Arevisited%20with%20a%20focus%20on%20addressing%20domain%20shift.%20To%20this%20end%2C%20a%20new%0Amulti-modal%20and%20multi-domain%20dataset%2C%20BrightVQA%2C%20is%20introduced%20to%20facilitate%0Adomain%20generalization%20research%20in%20CDVQA.%20Furthermore%2C%20a%20novel%20state%20space%0Amodel%2C%20termed%20Text-Conditioned%20State%20Space%20Model%20%28TCSSM%29%2C%20is%20proposed.%20The%0ATCSSM%20framework%20is%20designed%20to%20leverage%20both%20bi-temporal%20imagery%20and%0Ageo-disaster-related%20textual%20information%20in%20an%20unified%20manner%20to%20extract%0Adomain-invariant%20features%20across%20domains.%20Input-dependent%20parameters%20existing%0Ain%20TCSSM%20are%20dynamically%20predicted%20by%20using%20both%20bi-temporal%20images%20and%0Ageo-disaster-related%20description%2C%20thereby%20facilitating%20the%20alignment%20between%0Abi-temporal%20visual%20data%20and%20the%20associated%20textual%20descriptions.%20Extensive%0Aexperiments%20are%20conducted%20to%20evaluate%20the%20proposed%20method%20against%0Astate-of-the-art%20models%2C%20and%20superior%20performance%20is%20consistently%20demonstrated.%0AThe%20code%20and%20dataset%20will%20be%20made%20publicly%20available%20upon%20acceptance%20at%0Ahttps%3A//github.com/Elman295/TCSSM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-conditioned%2520State%2520Space%2520Model%2520For%2520Domain-generalized%2520Change%250A%2520%2520Detection%2520Visual%2520Question%2520Answering%26entry.906535625%3DElman%2520Ghazaei%2520and%2520Erchan%2520Aptoula%26entry.1292438233%3D%2520%2520The%2520Earth%2527s%2520surface%2520is%2520constantly%2520changing%252C%2520and%2520detecting%2520these%2520changes%250Aprovides%2520valuable%2520insights%2520that%2520benefit%2520various%2520aspects%2520of%2520human%2520society.%2520While%250Atraditional%2520change%2520detection%2520methods%2520have%2520been%2520employed%2520to%2520detect%2520changes%2520from%250Abi-temporal%2520images%252C%2520these%2520approaches%2520typically%2520require%2520expert%2520knowledge%2520for%250Aaccurate%2520interpretation.%2520To%2520enable%2520broader%2520and%2520more%2520flexible%2520access%2520to%2520change%250Ainformation%2520by%2520non-expert%2520users%252C%2520the%2520task%2520of%2520Change%2520Detection%2520Visual%2520Question%250AAnswering%2520%2528CDVQA%2529%2520has%2520been%2520introduced.%2520However%252C%2520existing%2520CDVQA%2520methods%2520have%250Abeen%2520developed%2520under%2520the%2520assumption%2520that%2520training%2520and%2520testing%2520datasets%2520share%250Asimilar%2520distributions.%2520This%2520assumption%2520does%2520not%2520hold%2520in%2520real-world%250Aapplications%252C%2520where%2520domain%2520shifts%2520often%2520occur.%2520In%2520this%2520paper%252C%2520the%2520CDVQA%2520task%2520is%250Arevisited%2520with%2520a%2520focus%2520on%2520addressing%2520domain%2520shift.%2520To%2520this%2520end%252C%2520a%2520new%250Amulti-modal%2520and%2520multi-domain%2520dataset%252C%2520BrightVQA%252C%2520is%2520introduced%2520to%2520facilitate%250Adomain%2520generalization%2520research%2520in%2520CDVQA.%2520Furthermore%252C%2520a%2520novel%2520state%2520space%250Amodel%252C%2520termed%2520Text-Conditioned%2520State%2520Space%2520Model%2520%2528TCSSM%2529%252C%2520is%2520proposed.%2520The%250ATCSSM%2520framework%2520is%2520designed%2520to%2520leverage%2520both%2520bi-temporal%2520imagery%2520and%250Ageo-disaster-related%2520textual%2520information%2520in%2520an%2520unified%2520manner%2520to%2520extract%250Adomain-invariant%2520features%2520across%2520domains.%2520Input-dependent%2520parameters%2520existing%250Ain%2520TCSSM%2520are%2520dynamically%2520predicted%2520by%2520using%2520both%2520bi-temporal%2520images%2520and%250Ageo-disaster-related%2520description%252C%2520thereby%2520facilitating%2520the%2520alignment%2520between%250Abi-temporal%2520visual%2520data%2520and%2520the%2520associated%2520textual%2520descriptions.%2520Extensive%250Aexperiments%2520are%2520conducted%2520to%2520evaluate%2520the%2520proposed%2520method%2520against%250Astate-of-the-art%2520models%252C%2520and%2520superior%2520performance%2520is%2520consistently%2520demonstrated.%250AThe%2520code%2520and%2520dataset%2520will%2520be%2520made%2520publicly%2520available%2520upon%2520acceptance%2520at%250Ahttps%253A//github.com/Elman295/TCSSM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-conditioned%20State%20Space%20Model%20For%20Domain-generalized%20Change%0A%20%20Detection%20Visual%20Question%20Answering&entry.906535625=Elman%20Ghazaei%20and%20Erchan%20Aptoula&entry.1292438233=%20%20The%20Earth%27s%20surface%20is%20constantly%20changing%2C%20and%20detecting%20these%20changes%0Aprovides%20valuable%20insights%20that%20benefit%20various%20aspects%20of%20human%20society.%20While%0Atraditional%20change%20detection%20methods%20have%20been%20employed%20to%20detect%20changes%20from%0Abi-temporal%20images%2C%20these%20approaches%20typically%20require%20expert%20knowledge%20for%0Aaccurate%20interpretation.%20To%20enable%20broader%20and%20more%20flexible%20access%20to%20change%0Ainformation%20by%20non-expert%20users%2C%20the%20task%20of%20Change%20Detection%20Visual%20Question%0AAnswering%20%28CDVQA%29%20has%20been%20introduced.%20However%2C%20existing%20CDVQA%20methods%20have%0Abeen%20developed%20under%20the%20assumption%20that%20training%20and%20testing%20datasets%20share%0Asimilar%20distributions.%20This%20assumption%20does%20not%20hold%20in%20real-world%0Aapplications%2C%20where%20domain%20shifts%20often%20occur.%20In%20this%20paper%2C%20the%20CDVQA%20task%20is%0Arevisited%20with%20a%20focus%20on%20addressing%20domain%20shift.%20To%20this%20end%2C%20a%20new%0Amulti-modal%20and%20multi-domain%20dataset%2C%20BrightVQA%2C%20is%20introduced%20to%20facilitate%0Adomain%20generalization%20research%20in%20CDVQA.%20Furthermore%2C%20a%20novel%20state%20space%0Amodel%2C%20termed%20Text-Conditioned%20State%20Space%20Model%20%28TCSSM%29%2C%20is%20proposed.%20The%0ATCSSM%20framework%20is%20designed%20to%20leverage%20both%20bi-temporal%20imagery%20and%0Ageo-disaster-related%20textual%20information%20in%20an%20unified%20manner%20to%20extract%0Adomain-invariant%20features%20across%20domains.%20Input-dependent%20parameters%20existing%0Ain%20TCSSM%20are%20dynamically%20predicted%20by%20using%20both%20bi-temporal%20images%20and%0Ageo-disaster-related%20description%2C%20thereby%20facilitating%20the%20alignment%20between%0Abi-temporal%20visual%20data%20and%20the%20associated%20textual%20descriptions.%20Extensive%0Aexperiments%20are%20conducted%20to%20evaluate%20the%20proposed%20method%20against%0Astate-of-the-art%20models%2C%20and%20superior%20performance%20is%20consistently%20demonstrated.%0AThe%20code%20and%20dataset%20will%20be%20made%20publicly%20available%20upon%20acceptance%20at%0Ahttps%3A//github.com/Elman295/TCSSM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08974v1&entry.124074799=Read"},
{"title": "Towards Perfection: Building Inter-component Mutual Correction for\n  Retinex-based Low-light Image Enhancement", "author": "Luyang Cao and Han Xu and Jian Zhang and Lei Qi and Jiayi Ma and Yinghuan Shi and Yang Gao", "abstract": "  In low-light image enhancement, Retinex-based deep learning methods have\ngarnered significant attention due to their exceptional interpretability. These\nmethods decompose images into mutually independent illumination and reflectance\ncomponents, allows each component to be enhanced separately. In fact, achieving\nperfect decomposition of illumination and reflectance components proves to be\nquite challenging, with some residuals still existing after decomposition. In\nthis paper, we formally name these residuals as inter-component residuals\n(ICR), which has been largely underestimated by previous methods. In our\ninvestigation, ICR not only affects the accuracy of the decomposition but also\ncauses enhanced components to deviate from the ideal outcome, ultimately\nreducing the final synthesized image quality. To address this issue, we propose\na novel Inter-correction Retinex model (IRetinex) to alleviate ICR during the\ndecomposition and enhancement stage. In the decomposition stage, we leverage\ninter-component residual reduction module to reduce the feature similarity\nbetween illumination and reflectance components. In the enhancement stage, we\nutilize the feature similarity between the two components to detect and\nmitigate the impact of ICR within each enhancement unit. Extensive experiments\non three low-light benchmark datasets demonstrated that by reducing ICR, our\nmethod outperforms state-of-the-art approaches both qualitatively and\nquantitatively.\n", "link": "http://arxiv.org/abs/2508.09009v1", "date": "2025-08-12", "relevancy": 1.6705, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5837}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5236}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Perfection%3A%20Building%20Inter-component%20Mutual%20Correction%20for%0A%20%20Retinex-based%20Low-light%20Image%20Enhancement&body=Title%3A%20Towards%20Perfection%3A%20Building%20Inter-component%20Mutual%20Correction%20for%0A%20%20Retinex-based%20Low-light%20Image%20Enhancement%0AAuthor%3A%20Luyang%20Cao%20and%20Han%20Xu%20and%20Jian%20Zhang%20and%20Lei%20Qi%20and%20Jiayi%20Ma%20and%20Yinghuan%20Shi%20and%20Yang%20Gao%0AAbstract%3A%20%20%20In%20low-light%20image%20enhancement%2C%20Retinex-based%20deep%20learning%20methods%20have%0Agarnered%20significant%20attention%20due%20to%20their%20exceptional%20interpretability.%20These%0Amethods%20decompose%20images%20into%20mutually%20independent%20illumination%20and%20reflectance%0Acomponents%2C%20allows%20each%20component%20to%20be%20enhanced%20separately.%20In%20fact%2C%20achieving%0Aperfect%20decomposition%20of%20illumination%20and%20reflectance%20components%20proves%20to%20be%0Aquite%20challenging%2C%20with%20some%20residuals%20still%20existing%20after%20decomposition.%20In%0Athis%20paper%2C%20we%20formally%20name%20these%20residuals%20as%20inter-component%20residuals%0A%28ICR%29%2C%20which%20has%20been%20largely%20underestimated%20by%20previous%20methods.%20In%20our%0Ainvestigation%2C%20ICR%20not%20only%20affects%20the%20accuracy%20of%20the%20decomposition%20but%20also%0Acauses%20enhanced%20components%20to%20deviate%20from%20the%20ideal%20outcome%2C%20ultimately%0Areducing%20the%20final%20synthesized%20image%20quality.%20To%20address%20this%20issue%2C%20we%20propose%0Aa%20novel%20Inter-correction%20Retinex%20model%20%28IRetinex%29%20to%20alleviate%20ICR%20during%20the%0Adecomposition%20and%20enhancement%20stage.%20In%20the%20decomposition%20stage%2C%20we%20leverage%0Ainter-component%20residual%20reduction%20module%20to%20reduce%20the%20feature%20similarity%0Abetween%20illumination%20and%20reflectance%20components.%20In%20the%20enhancement%20stage%2C%20we%0Autilize%20the%20feature%20similarity%20between%20the%20two%20components%20to%20detect%20and%0Amitigate%20the%20impact%20of%20ICR%20within%20each%20enhancement%20unit.%20Extensive%20experiments%0Aon%20three%20low-light%20benchmark%20datasets%20demonstrated%20that%20by%20reducing%20ICR%2C%20our%0Amethod%20outperforms%20state-of-the-art%20approaches%20both%20qualitatively%20and%0Aquantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Perfection%253A%2520Building%2520Inter-component%2520Mutual%2520Correction%2520for%250A%2520%2520Retinex-based%2520Low-light%2520Image%2520Enhancement%26entry.906535625%3DLuyang%2520Cao%2520and%2520Han%2520Xu%2520and%2520Jian%2520Zhang%2520and%2520Lei%2520Qi%2520and%2520Jiayi%2520Ma%2520and%2520Yinghuan%2520Shi%2520and%2520Yang%2520Gao%26entry.1292438233%3D%2520%2520In%2520low-light%2520image%2520enhancement%252C%2520Retinex-based%2520deep%2520learning%2520methods%2520have%250Agarnered%2520significant%2520attention%2520due%2520to%2520their%2520exceptional%2520interpretability.%2520These%250Amethods%2520decompose%2520images%2520into%2520mutually%2520independent%2520illumination%2520and%2520reflectance%250Acomponents%252C%2520allows%2520each%2520component%2520to%2520be%2520enhanced%2520separately.%2520In%2520fact%252C%2520achieving%250Aperfect%2520decomposition%2520of%2520illumination%2520and%2520reflectance%2520components%2520proves%2520to%2520be%250Aquite%2520challenging%252C%2520with%2520some%2520residuals%2520still%2520existing%2520after%2520decomposition.%2520In%250Athis%2520paper%252C%2520we%2520formally%2520name%2520these%2520residuals%2520as%2520inter-component%2520residuals%250A%2528ICR%2529%252C%2520which%2520has%2520been%2520largely%2520underestimated%2520by%2520previous%2520methods.%2520In%2520our%250Ainvestigation%252C%2520ICR%2520not%2520only%2520affects%2520the%2520accuracy%2520of%2520the%2520decomposition%2520but%2520also%250Acauses%2520enhanced%2520components%2520to%2520deviate%2520from%2520the%2520ideal%2520outcome%252C%2520ultimately%250Areducing%2520the%2520final%2520synthesized%2520image%2520quality.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%250Aa%2520novel%2520Inter-correction%2520Retinex%2520model%2520%2528IRetinex%2529%2520to%2520alleviate%2520ICR%2520during%2520the%250Adecomposition%2520and%2520enhancement%2520stage.%2520In%2520the%2520decomposition%2520stage%252C%2520we%2520leverage%250Ainter-component%2520residual%2520reduction%2520module%2520to%2520reduce%2520the%2520feature%2520similarity%250Abetween%2520illumination%2520and%2520reflectance%2520components.%2520In%2520the%2520enhancement%2520stage%252C%2520we%250Autilize%2520the%2520feature%2520similarity%2520between%2520the%2520two%2520components%2520to%2520detect%2520and%250Amitigate%2520the%2520impact%2520of%2520ICR%2520within%2520each%2520enhancement%2520unit.%2520Extensive%2520experiments%250Aon%2520three%2520low-light%2520benchmark%2520datasets%2520demonstrated%2520that%2520by%2520reducing%2520ICR%252C%2520our%250Amethod%2520outperforms%2520state-of-the-art%2520approaches%2520both%2520qualitatively%2520and%250Aquantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Perfection%3A%20Building%20Inter-component%20Mutual%20Correction%20for%0A%20%20Retinex-based%20Low-light%20Image%20Enhancement&entry.906535625=Luyang%20Cao%20and%20Han%20Xu%20and%20Jian%20Zhang%20and%20Lei%20Qi%20and%20Jiayi%20Ma%20and%20Yinghuan%20Shi%20and%20Yang%20Gao&entry.1292438233=%20%20In%20low-light%20image%20enhancement%2C%20Retinex-based%20deep%20learning%20methods%20have%0Agarnered%20significant%20attention%20due%20to%20their%20exceptional%20interpretability.%20These%0Amethods%20decompose%20images%20into%20mutually%20independent%20illumination%20and%20reflectance%0Acomponents%2C%20allows%20each%20component%20to%20be%20enhanced%20separately.%20In%20fact%2C%20achieving%0Aperfect%20decomposition%20of%20illumination%20and%20reflectance%20components%20proves%20to%20be%0Aquite%20challenging%2C%20with%20some%20residuals%20still%20existing%20after%20decomposition.%20In%0Athis%20paper%2C%20we%20formally%20name%20these%20residuals%20as%20inter-component%20residuals%0A%28ICR%29%2C%20which%20has%20been%20largely%20underestimated%20by%20previous%20methods.%20In%20our%0Ainvestigation%2C%20ICR%20not%20only%20affects%20the%20accuracy%20of%20the%20decomposition%20but%20also%0Acauses%20enhanced%20components%20to%20deviate%20from%20the%20ideal%20outcome%2C%20ultimately%0Areducing%20the%20final%20synthesized%20image%20quality.%20To%20address%20this%20issue%2C%20we%20propose%0Aa%20novel%20Inter-correction%20Retinex%20model%20%28IRetinex%29%20to%20alleviate%20ICR%20during%20the%0Adecomposition%20and%20enhancement%20stage.%20In%20the%20decomposition%20stage%2C%20we%20leverage%0Ainter-component%20residual%20reduction%20module%20to%20reduce%20the%20feature%20similarity%0Abetween%20illumination%20and%20reflectance%20components.%20In%20the%20enhancement%20stage%2C%20we%0Autilize%20the%20feature%20similarity%20between%20the%20two%20components%20to%20detect%20and%0Amitigate%20the%20impact%20of%20ICR%20within%20each%20enhancement%20unit.%20Extensive%20experiments%0Aon%20three%20low-light%20benchmark%20datasets%20demonstrated%20that%20by%20reducing%20ICR%2C%20our%0Amethod%20outperforms%20state-of-the-art%20approaches%20both%20qualitatively%20and%0Aquantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09009v1&entry.124074799=Read"},
{"title": "GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for\n  End-to-End Autonomous Driving", "author": "Jian Wang and Chaokang Jiang and Haitao Xu", "abstract": "  Diffusion-based models are redefining the state-of-the-art in end-to-end\nautonomous driving, yet their performance is increasingly hampered by a\nreliance on transformer-based fusion. These architectures face fundamental\nlimitations: quadratic computational complexity restricts the use of\nhigh-resolution features, and a lack of spatial priors prevents them from\neffectively modeling the inherent structure of Bird's Eye View (BEV)\nrepresentations. This paper introduces GMF-Drive (Gated Mamba Fusion for\nDriving), an end-to-end framework that overcomes these challenges through two\nprincipled innovations. First, we supersede the information-limited\nhistogram-based LiDAR representation with a geometrically-augmented pillar\nformat encoding shape descriptors and statistical features, preserving critical\n3D geometric details. Second, we propose a novel hierarchical gated mamba\nfusion (GM-Fusion) architecture that substitutes an expensive transformer with\na highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM\nleverages directional sequencing and adaptive fusion mechanisms to capture\nlong-range dependencies with linear complexity, while explicitly respecting the\nunique spatial properties of the driving scene. Extensive experiments on the\nchallenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new\nstate-of-the-art performance, significantly outperforming DiffusionDrive.\nComprehensive ablation studies validate the efficacy of each component,\ndemonstrating that task-specific SSMs can surpass a general-purpose transformer\nin both performance and efficiency for autonomous driving.\n", "link": "http://arxiv.org/abs/2508.06113v2", "date": "2025-08-12", "relevancy": 1.6696, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.561}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5553}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GMF-Drive%3A%20Gated%20Mamba%20Fusion%20with%20Spatial-Aware%20BEV%20Representation%20for%0A%20%20End-to-End%20Autonomous%20Driving&body=Title%3A%20GMF-Drive%3A%20Gated%20Mamba%20Fusion%20with%20Spatial-Aware%20BEV%20Representation%20for%0A%20%20End-to-End%20Autonomous%20Driving%0AAuthor%3A%20Jian%20Wang%20and%20Chaokang%20Jiang%20and%20Haitao%20Xu%0AAbstract%3A%20%20%20Diffusion-based%20models%20are%20redefining%20the%20state-of-the-art%20in%20end-to-end%0Aautonomous%20driving%2C%20yet%20their%20performance%20is%20increasingly%20hampered%20by%20a%0Areliance%20on%20transformer-based%20fusion.%20These%20architectures%20face%20fundamental%0Alimitations%3A%20quadratic%20computational%20complexity%20restricts%20the%20use%20of%0Ahigh-resolution%20features%2C%20and%20a%20lack%20of%20spatial%20priors%20prevents%20them%20from%0Aeffectively%20modeling%20the%20inherent%20structure%20of%20Bird%27s%20Eye%20View%20%28BEV%29%0Arepresentations.%20This%20paper%20introduces%20GMF-Drive%20%28Gated%20Mamba%20Fusion%20for%0ADriving%29%2C%20an%20end-to-end%20framework%20that%20overcomes%20these%20challenges%20through%20two%0Aprincipled%20innovations.%20First%2C%20we%20supersede%20the%20information-limited%0Ahistogram-based%20LiDAR%20representation%20with%20a%20geometrically-augmented%20pillar%0Aformat%20encoding%20shape%20descriptors%20and%20statistical%20features%2C%20preserving%20critical%0A3D%20geometric%20details.%20Second%2C%20we%20propose%20a%20novel%20hierarchical%20gated%20mamba%0Afusion%20%28GM-Fusion%29%20architecture%20that%20substitutes%20an%20expensive%20transformer%20with%0Aa%20highly%20efficient%2C%20spatially-aware%20state-space%20model%20%28SSM%29.%20Our%20core%20BEV-SSM%0Aleverages%20directional%20sequencing%20and%20adaptive%20fusion%20mechanisms%20to%20capture%0Along-range%20dependencies%20with%20linear%20complexity%2C%20while%20explicitly%20respecting%20the%0Aunique%20spatial%20properties%20of%20the%20driving%20scene.%20Extensive%20experiments%20on%20the%0Achallenging%20NAVSIM%20benchmark%20demonstrate%20that%20GMF-Drive%20achieves%20a%20new%0Astate-of-the-art%20performance%2C%20significantly%20outperforming%20DiffusionDrive.%0AComprehensive%20ablation%20studies%20validate%20the%20efficacy%20of%20each%20component%2C%0Ademonstrating%20that%20task-specific%20SSMs%20can%20surpass%20a%20general-purpose%20transformer%0Ain%20both%20performance%20and%20efficiency%20for%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06113v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGMF-Drive%253A%2520Gated%2520Mamba%2520Fusion%2520with%2520Spatial-Aware%2520BEV%2520Representation%2520for%250A%2520%2520End-to-End%2520Autonomous%2520Driving%26entry.906535625%3DJian%2520Wang%2520and%2520Chaokang%2520Jiang%2520and%2520Haitao%2520Xu%26entry.1292438233%3D%2520%2520Diffusion-based%2520models%2520are%2520redefining%2520the%2520state-of-the-art%2520in%2520end-to-end%250Aautonomous%2520driving%252C%2520yet%2520their%2520performance%2520is%2520increasingly%2520hampered%2520by%2520a%250Areliance%2520on%2520transformer-based%2520fusion.%2520These%2520architectures%2520face%2520fundamental%250Alimitations%253A%2520quadratic%2520computational%2520complexity%2520restricts%2520the%2520use%2520of%250Ahigh-resolution%2520features%252C%2520and%2520a%2520lack%2520of%2520spatial%2520priors%2520prevents%2520them%2520from%250Aeffectively%2520modeling%2520the%2520inherent%2520structure%2520of%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%250Arepresentations.%2520This%2520paper%2520introduces%2520GMF-Drive%2520%2528Gated%2520Mamba%2520Fusion%2520for%250ADriving%2529%252C%2520an%2520end-to-end%2520framework%2520that%2520overcomes%2520these%2520challenges%2520through%2520two%250Aprincipled%2520innovations.%2520First%252C%2520we%2520supersede%2520the%2520information-limited%250Ahistogram-based%2520LiDAR%2520representation%2520with%2520a%2520geometrically-augmented%2520pillar%250Aformat%2520encoding%2520shape%2520descriptors%2520and%2520statistical%2520features%252C%2520preserving%2520critical%250A3D%2520geometric%2520details.%2520Second%252C%2520we%2520propose%2520a%2520novel%2520hierarchical%2520gated%2520mamba%250Afusion%2520%2528GM-Fusion%2529%2520architecture%2520that%2520substitutes%2520an%2520expensive%2520transformer%2520with%250Aa%2520highly%2520efficient%252C%2520spatially-aware%2520state-space%2520model%2520%2528SSM%2529.%2520Our%2520core%2520BEV-SSM%250Aleverages%2520directional%2520sequencing%2520and%2520adaptive%2520fusion%2520mechanisms%2520to%2520capture%250Along-range%2520dependencies%2520with%2520linear%2520complexity%252C%2520while%2520explicitly%2520respecting%2520the%250Aunique%2520spatial%2520properties%2520of%2520the%2520driving%2520scene.%2520Extensive%2520experiments%2520on%2520the%250Achallenging%2520NAVSIM%2520benchmark%2520demonstrate%2520that%2520GMF-Drive%2520achieves%2520a%2520new%250Astate-of-the-art%2520performance%252C%2520significantly%2520outperforming%2520DiffusionDrive.%250AComprehensive%2520ablation%2520studies%2520validate%2520the%2520efficacy%2520of%2520each%2520component%252C%250Ademonstrating%2520that%2520task-specific%2520SSMs%2520can%2520surpass%2520a%2520general-purpose%2520transformer%250Ain%2520both%2520performance%2520and%2520efficiency%2520for%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06113v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GMF-Drive%3A%20Gated%20Mamba%20Fusion%20with%20Spatial-Aware%20BEV%20Representation%20for%0A%20%20End-to-End%20Autonomous%20Driving&entry.906535625=Jian%20Wang%20and%20Chaokang%20Jiang%20and%20Haitao%20Xu&entry.1292438233=%20%20Diffusion-based%20models%20are%20redefining%20the%20state-of-the-art%20in%20end-to-end%0Aautonomous%20driving%2C%20yet%20their%20performance%20is%20increasingly%20hampered%20by%20a%0Areliance%20on%20transformer-based%20fusion.%20These%20architectures%20face%20fundamental%0Alimitations%3A%20quadratic%20computational%20complexity%20restricts%20the%20use%20of%0Ahigh-resolution%20features%2C%20and%20a%20lack%20of%20spatial%20priors%20prevents%20them%20from%0Aeffectively%20modeling%20the%20inherent%20structure%20of%20Bird%27s%20Eye%20View%20%28BEV%29%0Arepresentations.%20This%20paper%20introduces%20GMF-Drive%20%28Gated%20Mamba%20Fusion%20for%0ADriving%29%2C%20an%20end-to-end%20framework%20that%20overcomes%20these%20challenges%20through%20two%0Aprincipled%20innovations.%20First%2C%20we%20supersede%20the%20information-limited%0Ahistogram-based%20LiDAR%20representation%20with%20a%20geometrically-augmented%20pillar%0Aformat%20encoding%20shape%20descriptors%20and%20statistical%20features%2C%20preserving%20critical%0A3D%20geometric%20details.%20Second%2C%20we%20propose%20a%20novel%20hierarchical%20gated%20mamba%0Afusion%20%28GM-Fusion%29%20architecture%20that%20substitutes%20an%20expensive%20transformer%20with%0Aa%20highly%20efficient%2C%20spatially-aware%20state-space%20model%20%28SSM%29.%20Our%20core%20BEV-SSM%0Aleverages%20directional%20sequencing%20and%20adaptive%20fusion%20mechanisms%20to%20capture%0Along-range%20dependencies%20with%20linear%20complexity%2C%20while%20explicitly%20respecting%20the%0Aunique%20spatial%20properties%20of%20the%20driving%20scene.%20Extensive%20experiments%20on%20the%0Achallenging%20NAVSIM%20benchmark%20demonstrate%20that%20GMF-Drive%20achieves%20a%20new%0Astate-of-the-art%20performance%2C%20significantly%20outperforming%20DiffusionDrive.%0AComprehensive%20ablation%20studies%20validate%20the%20efficacy%20of%20each%20component%2C%0Ademonstrating%20that%20task-specific%20SSMs%20can%20surpass%20a%20general-purpose%20transformer%0Ain%20both%20performance%20and%20efficiency%20for%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06113v2&entry.124074799=Read"},
{"title": "E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and\n  Efficiency", "author": "Dongjie Xu and Yue Cui and Weijie Shi and Qingzhi Ma and Hanghui Guo and Jiaming Li and Yao Zhao and Ruiyuan Zhang and Shimin Di and Jia Zhu and Kai Zheng and Jiajie Xu", "abstract": "  SQL query rewriting aims to reformulate a query into a more efficient form\nwhile preserving equivalence. Most existing methods rely on predefined rewrite\nrules. However, such rule-based approaches face fundamental limitations: (1)\nfixed rule sets generalize poorly to novel query patterns and struggle with\ncomplex queries; (2) a wide range of effective rewriting strategies cannot be\nfully captured by declarative rules. To overcome these issues, we propose using\nlarge language models (LLMs) to generate rewrites. LLMs can capture complex\nstrategies, such as evaluation reordering and CTE rewriting. Despite this\npotential, directly applying LLMs often results in suboptimal or non-equivalent\nrewrites due to a lack of execution awareness and semantic grounding. To\naddress these challenges, We present E3-Rewrite, an LLM-based SQL rewriting\nframework that produces executable, equivalent, and efficient queries. It\nintegrates two core components: a context construction module and a\nreinforcement learning framework. First, the context module leverages execution\nplans and retrieved demonstrations to build bottleneck-aware prompts that guide\ninference-time rewriting. Second, we design a reward function targeting\nexecutability, equivalence, and efficiency, evaluated via syntax checks,\nequivalence verification, and cost estimation. Third, to ensure stable\nmulti-objective learning, we adopt a staged curriculum that first emphasizes\nexecutability and equivalence, then gradually incorporates efficiency.\nExtensive experiments show that E3-Rewrite achieves up to a 25.6\\% reduction in\nquery execution time compared to state-of-the-art methods across multiple SQL\nbenchmarks. Moreover, it delivers up to 24.4\\% more successful rewrites,\nexpanding coverage to complex queries that previous systems failed to handle.\n", "link": "http://arxiv.org/abs/2508.09023v1", "date": "2025-08-12", "relevancy": 1.6597, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4187}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4142}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E3-Rewrite%3A%20Learning%20to%20Rewrite%20SQL%20for%20Executability%2C%20Equivalence%2Cand%0A%20%20Efficiency&body=Title%3A%20E3-Rewrite%3A%20Learning%20to%20Rewrite%20SQL%20for%20Executability%2C%20Equivalence%2Cand%0A%20%20Efficiency%0AAuthor%3A%20Dongjie%20Xu%20and%20Yue%20Cui%20and%20Weijie%20Shi%20and%20Qingzhi%20Ma%20and%20Hanghui%20Guo%20and%20Jiaming%20Li%20and%20Yao%20Zhao%20and%20Ruiyuan%20Zhang%20and%20Shimin%20Di%20and%20Jia%20Zhu%20and%20Kai%20Zheng%20and%20Jiajie%20Xu%0AAbstract%3A%20%20%20SQL%20query%20rewriting%20aims%20to%20reformulate%20a%20query%20into%20a%20more%20efficient%20form%0Awhile%20preserving%20equivalence.%20Most%20existing%20methods%20rely%20on%20predefined%20rewrite%0Arules.%20However%2C%20such%20rule-based%20approaches%20face%20fundamental%20limitations%3A%20%281%29%0Afixed%20rule%20sets%20generalize%20poorly%20to%20novel%20query%20patterns%20and%20struggle%20with%0Acomplex%20queries%3B%20%282%29%20a%20wide%20range%20of%20effective%20rewriting%20strategies%20cannot%20be%0Afully%20captured%20by%20declarative%20rules.%20To%20overcome%20these%20issues%2C%20we%20propose%20using%0Alarge%20language%20models%20%28LLMs%29%20to%20generate%20rewrites.%20LLMs%20can%20capture%20complex%0Astrategies%2C%20such%20as%20evaluation%20reordering%20and%20CTE%20rewriting.%20Despite%20this%0Apotential%2C%20directly%20applying%20LLMs%20often%20results%20in%20suboptimal%20or%20non-equivalent%0Arewrites%20due%20to%20a%20lack%20of%20execution%20awareness%20and%20semantic%20grounding.%20To%0Aaddress%20these%20challenges%2C%20We%20present%20E3-Rewrite%2C%20an%20LLM-based%20SQL%20rewriting%0Aframework%20that%20produces%20executable%2C%20equivalent%2C%20and%20efficient%20queries.%20It%0Aintegrates%20two%20core%20components%3A%20a%20context%20construction%20module%20and%20a%0Areinforcement%20learning%20framework.%20First%2C%20the%20context%20module%20leverages%20execution%0Aplans%20and%20retrieved%20demonstrations%20to%20build%20bottleneck-aware%20prompts%20that%20guide%0Ainference-time%20rewriting.%20Second%2C%20we%20design%20a%20reward%20function%20targeting%0Aexecutability%2C%20equivalence%2C%20and%20efficiency%2C%20evaluated%20via%20syntax%20checks%2C%0Aequivalence%20verification%2C%20and%20cost%20estimation.%20Third%2C%20to%20ensure%20stable%0Amulti-objective%20learning%2C%20we%20adopt%20a%20staged%20curriculum%20that%20first%20emphasizes%0Aexecutability%20and%20equivalence%2C%20then%20gradually%20incorporates%20efficiency.%0AExtensive%20experiments%20show%20that%20E3-Rewrite%20achieves%20up%20to%20a%2025.6%5C%25%20reduction%20in%0Aquery%20execution%20time%20compared%20to%20state-of-the-art%20methods%20across%20multiple%20SQL%0Abenchmarks.%20Moreover%2C%20it%20delivers%20up%20to%2024.4%5C%25%20more%20successful%20rewrites%2C%0Aexpanding%20coverage%20to%20complex%20queries%20that%20previous%20systems%20failed%20to%20handle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE3-Rewrite%253A%2520Learning%2520to%2520Rewrite%2520SQL%2520for%2520Executability%252C%2520Equivalence%252Cand%250A%2520%2520Efficiency%26entry.906535625%3DDongjie%2520Xu%2520and%2520Yue%2520Cui%2520and%2520Weijie%2520Shi%2520and%2520Qingzhi%2520Ma%2520and%2520Hanghui%2520Guo%2520and%2520Jiaming%2520Li%2520and%2520Yao%2520Zhao%2520and%2520Ruiyuan%2520Zhang%2520and%2520Shimin%2520Di%2520and%2520Jia%2520Zhu%2520and%2520Kai%2520Zheng%2520and%2520Jiajie%2520Xu%26entry.1292438233%3D%2520%2520SQL%2520query%2520rewriting%2520aims%2520to%2520reformulate%2520a%2520query%2520into%2520a%2520more%2520efficient%2520form%250Awhile%2520preserving%2520equivalence.%2520Most%2520existing%2520methods%2520rely%2520on%2520predefined%2520rewrite%250Arules.%2520However%252C%2520such%2520rule-based%2520approaches%2520face%2520fundamental%2520limitations%253A%2520%25281%2529%250Afixed%2520rule%2520sets%2520generalize%2520poorly%2520to%2520novel%2520query%2520patterns%2520and%2520struggle%2520with%250Acomplex%2520queries%253B%2520%25282%2529%2520a%2520wide%2520range%2520of%2520effective%2520rewriting%2520strategies%2520cannot%2520be%250Afully%2520captured%2520by%2520declarative%2520rules.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520propose%2520using%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520to%2520generate%2520rewrites.%2520LLMs%2520can%2520capture%2520complex%250Astrategies%252C%2520such%2520as%2520evaluation%2520reordering%2520and%2520CTE%2520rewriting.%2520Despite%2520this%250Apotential%252C%2520directly%2520applying%2520LLMs%2520often%2520results%2520in%2520suboptimal%2520or%2520non-equivalent%250Arewrites%2520due%2520to%2520a%2520lack%2520of%2520execution%2520awareness%2520and%2520semantic%2520grounding.%2520To%250Aaddress%2520these%2520challenges%252C%2520We%2520present%2520E3-Rewrite%252C%2520an%2520LLM-based%2520SQL%2520rewriting%250Aframework%2520that%2520produces%2520executable%252C%2520equivalent%252C%2520and%2520efficient%2520queries.%2520It%250Aintegrates%2520two%2520core%2520components%253A%2520a%2520context%2520construction%2520module%2520and%2520a%250Areinforcement%2520learning%2520framework.%2520First%252C%2520the%2520context%2520module%2520leverages%2520execution%250Aplans%2520and%2520retrieved%2520demonstrations%2520to%2520build%2520bottleneck-aware%2520prompts%2520that%2520guide%250Ainference-time%2520rewriting.%2520Second%252C%2520we%2520design%2520a%2520reward%2520function%2520targeting%250Aexecutability%252C%2520equivalence%252C%2520and%2520efficiency%252C%2520evaluated%2520via%2520syntax%2520checks%252C%250Aequivalence%2520verification%252C%2520and%2520cost%2520estimation.%2520Third%252C%2520to%2520ensure%2520stable%250Amulti-objective%2520learning%252C%2520we%2520adopt%2520a%2520staged%2520curriculum%2520that%2520first%2520emphasizes%250Aexecutability%2520and%2520equivalence%252C%2520then%2520gradually%2520incorporates%2520efficiency.%250AExtensive%2520experiments%2520show%2520that%2520E3-Rewrite%2520achieves%2520up%2520to%2520a%252025.6%255C%2525%2520reduction%2520in%250Aquery%2520execution%2520time%2520compared%2520to%2520state-of-the-art%2520methods%2520across%2520multiple%2520SQL%250Abenchmarks.%2520Moreover%252C%2520it%2520delivers%2520up%2520to%252024.4%255C%2525%2520more%2520successful%2520rewrites%252C%250Aexpanding%2520coverage%2520to%2520complex%2520queries%2520that%2520previous%2520systems%2520failed%2520to%2520handle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E3-Rewrite%3A%20Learning%20to%20Rewrite%20SQL%20for%20Executability%2C%20Equivalence%2Cand%0A%20%20Efficiency&entry.906535625=Dongjie%20Xu%20and%20Yue%20Cui%20and%20Weijie%20Shi%20and%20Qingzhi%20Ma%20and%20Hanghui%20Guo%20and%20Jiaming%20Li%20and%20Yao%20Zhao%20and%20Ruiyuan%20Zhang%20and%20Shimin%20Di%20and%20Jia%20Zhu%20and%20Kai%20Zheng%20and%20Jiajie%20Xu&entry.1292438233=%20%20SQL%20query%20rewriting%20aims%20to%20reformulate%20a%20query%20into%20a%20more%20efficient%20form%0Awhile%20preserving%20equivalence.%20Most%20existing%20methods%20rely%20on%20predefined%20rewrite%0Arules.%20However%2C%20such%20rule-based%20approaches%20face%20fundamental%20limitations%3A%20%281%29%0Afixed%20rule%20sets%20generalize%20poorly%20to%20novel%20query%20patterns%20and%20struggle%20with%0Acomplex%20queries%3B%20%282%29%20a%20wide%20range%20of%20effective%20rewriting%20strategies%20cannot%20be%0Afully%20captured%20by%20declarative%20rules.%20To%20overcome%20these%20issues%2C%20we%20propose%20using%0Alarge%20language%20models%20%28LLMs%29%20to%20generate%20rewrites.%20LLMs%20can%20capture%20complex%0Astrategies%2C%20such%20as%20evaluation%20reordering%20and%20CTE%20rewriting.%20Despite%20this%0Apotential%2C%20directly%20applying%20LLMs%20often%20results%20in%20suboptimal%20or%20non-equivalent%0Arewrites%20due%20to%20a%20lack%20of%20execution%20awareness%20and%20semantic%20grounding.%20To%0Aaddress%20these%20challenges%2C%20We%20present%20E3-Rewrite%2C%20an%20LLM-based%20SQL%20rewriting%0Aframework%20that%20produces%20executable%2C%20equivalent%2C%20and%20efficient%20queries.%20It%0Aintegrates%20two%20core%20components%3A%20a%20context%20construction%20module%20and%20a%0Areinforcement%20learning%20framework.%20First%2C%20the%20context%20module%20leverages%20execution%0Aplans%20and%20retrieved%20demonstrations%20to%20build%20bottleneck-aware%20prompts%20that%20guide%0Ainference-time%20rewriting.%20Second%2C%20we%20design%20a%20reward%20function%20targeting%0Aexecutability%2C%20equivalence%2C%20and%20efficiency%2C%20evaluated%20via%20syntax%20checks%2C%0Aequivalence%20verification%2C%20and%20cost%20estimation.%20Third%2C%20to%20ensure%20stable%0Amulti-objective%20learning%2C%20we%20adopt%20a%20staged%20curriculum%20that%20first%20emphasizes%0Aexecutability%20and%20equivalence%2C%20then%20gradually%20incorporates%20efficiency.%0AExtensive%20experiments%20show%20that%20E3-Rewrite%20achieves%20up%20to%20a%2025.6%5C%25%20reduction%20in%0Aquery%20execution%20time%20compared%20to%20state-of-the-art%20methods%20across%20multiple%20SQL%0Abenchmarks.%20Moreover%2C%20it%20delivers%20up%20to%2024.4%5C%25%20more%20successful%20rewrites%2C%0Aexpanding%20coverage%20to%20complex%20queries%20that%20previous%20systems%20failed%20to%20handle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09023v1&entry.124074799=Read"},
{"title": "MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for\n  Audio-Visual Video Parsing", "author": "Langyu Wang and Bingke Zhu and Yingying Chen and Yiyuan Zhang and Ming Tang and Jinqiao Wang", "abstract": "  The weakly-supervised audio-visual video parsing (AVVP) aims to predict all\nmodality-specific events and locate their temporal boundaries. Despite\nsignificant progress, due to the limitations of the weakly-supervised and the\ndeficiencies of the model architecture, existing methods are lacking in\nsimultaneously improving both the segment-level prediction and the event-level\nprediction. In this work, we propose a audio-visual Mamba network with pseudo\nlabeling aUGmentation (MUG) for emphasising the uniqueness of each segment and\nexcluding the noise interference from the alternate modalities. Specifically,\nwe annotate some of the pseudo-labels based on previous work. Using unimodal\npseudo-labels, we perform cross-modal random combinations to generate new data,\nwhich can enhance the model's ability to parse various segment-level event\ncombinations. For feature processing and interaction, we employ a audio-visual\nmamba network. The AV-Mamba enhances the ability to perceive different segments\nand excludes additional modal noise while sharing similar modal information.\nOur extensive experiments demonstrate that MUG improves state-of-the-art\nresults on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of\nvisual Segment-level and audio Segment-level metrics). Our code is available at\nhttps://github.com/WangLY136/MUG.\n", "link": "http://arxiv.org/abs/2507.01384v2", "date": "2025-08-12", "relevancy": 1.6519, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5603}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5535}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MUG%3A%20Pseudo%20Labeling%20Augmented%20Audio-Visual%20Mamba%20Network%20for%0A%20%20Audio-Visual%20Video%20Parsing&body=Title%3A%20MUG%3A%20Pseudo%20Labeling%20Augmented%20Audio-Visual%20Mamba%20Network%20for%0A%20%20Audio-Visual%20Video%20Parsing%0AAuthor%3A%20Langyu%20Wang%20and%20Bingke%20Zhu%20and%20Yingying%20Chen%20and%20Yiyuan%20Zhang%20and%20Ming%20Tang%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20The%20weakly-supervised%20audio-visual%20video%20parsing%20%28AVVP%29%20aims%20to%20predict%20all%0Amodality-specific%20events%20and%20locate%20their%20temporal%20boundaries.%20Despite%0Asignificant%20progress%2C%20due%20to%20the%20limitations%20of%20the%20weakly-supervised%20and%20the%0Adeficiencies%20of%20the%20model%20architecture%2C%20existing%20methods%20are%20lacking%20in%0Asimultaneously%20improving%20both%20the%20segment-level%20prediction%20and%20the%20event-level%0Aprediction.%20In%20this%20work%2C%20we%20propose%20a%20audio-visual%20Mamba%20network%20with%20pseudo%0Alabeling%20aUGmentation%20%28MUG%29%20for%20emphasising%20the%20uniqueness%20of%20each%20segment%20and%0Aexcluding%20the%20noise%20interference%20from%20the%20alternate%20modalities.%20Specifically%2C%0Awe%20annotate%20some%20of%20the%20pseudo-labels%20based%20on%20previous%20work.%20Using%20unimodal%0Apseudo-labels%2C%20we%20perform%20cross-modal%20random%20combinations%20to%20generate%20new%20data%2C%0Awhich%20can%20enhance%20the%20model%27s%20ability%20to%20parse%20various%20segment-level%20event%0Acombinations.%20For%20feature%20processing%20and%20interaction%2C%20we%20employ%20a%20audio-visual%0Amamba%20network.%20The%20AV-Mamba%20enhances%20the%20ability%20to%20perceive%20different%20segments%0Aand%20excludes%20additional%20modal%20noise%20while%20sharing%20similar%20modal%20information.%0AOur%20extensive%20experiments%20demonstrate%20that%20MUG%20improves%20state-of-the-art%0Aresults%20on%20LLP%20dataset%20in%20all%20metrics%20%28e.g%2C%2C%20gains%20of%202.1%25%20and%201.2%25%20in%20terms%20of%0Avisual%20Segment-level%20and%20audio%20Segment-level%20metrics%29.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/WangLY136/MUG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01384v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMUG%253A%2520Pseudo%2520Labeling%2520Augmented%2520Audio-Visual%2520Mamba%2520Network%2520for%250A%2520%2520Audio-Visual%2520Video%2520Parsing%26entry.906535625%3DLangyu%2520Wang%2520and%2520Bingke%2520Zhu%2520and%2520Yingying%2520Chen%2520and%2520Yiyuan%2520Zhang%2520and%2520Ming%2520Tang%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520The%2520weakly-supervised%2520audio-visual%2520video%2520parsing%2520%2528AVVP%2529%2520aims%2520to%2520predict%2520all%250Amodality-specific%2520events%2520and%2520locate%2520their%2520temporal%2520boundaries.%2520Despite%250Asignificant%2520progress%252C%2520due%2520to%2520the%2520limitations%2520of%2520the%2520weakly-supervised%2520and%2520the%250Adeficiencies%2520of%2520the%2520model%2520architecture%252C%2520existing%2520methods%2520are%2520lacking%2520in%250Asimultaneously%2520improving%2520both%2520the%2520segment-level%2520prediction%2520and%2520the%2520event-level%250Aprediction.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520audio-visual%2520Mamba%2520network%2520with%2520pseudo%250Alabeling%2520aUGmentation%2520%2528MUG%2529%2520for%2520emphasising%2520the%2520uniqueness%2520of%2520each%2520segment%2520and%250Aexcluding%2520the%2520noise%2520interference%2520from%2520the%2520alternate%2520modalities.%2520Specifically%252C%250Awe%2520annotate%2520some%2520of%2520the%2520pseudo-labels%2520based%2520on%2520previous%2520work.%2520Using%2520unimodal%250Apseudo-labels%252C%2520we%2520perform%2520cross-modal%2520random%2520combinations%2520to%2520generate%2520new%2520data%252C%250Awhich%2520can%2520enhance%2520the%2520model%2527s%2520ability%2520to%2520parse%2520various%2520segment-level%2520event%250Acombinations.%2520For%2520feature%2520processing%2520and%2520interaction%252C%2520we%2520employ%2520a%2520audio-visual%250Amamba%2520network.%2520The%2520AV-Mamba%2520enhances%2520the%2520ability%2520to%2520perceive%2520different%2520segments%250Aand%2520excludes%2520additional%2520modal%2520noise%2520while%2520sharing%2520similar%2520modal%2520information.%250AOur%2520extensive%2520experiments%2520demonstrate%2520that%2520MUG%2520improves%2520state-of-the-art%250Aresults%2520on%2520LLP%2520dataset%2520in%2520all%2520metrics%2520%2528e.g%252C%252C%2520gains%2520of%25202.1%2525%2520and%25201.2%2525%2520in%2520terms%2520of%250Avisual%2520Segment-level%2520and%2520audio%2520Segment-level%2520metrics%2529.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/WangLY136/MUG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01384v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MUG%3A%20Pseudo%20Labeling%20Augmented%20Audio-Visual%20Mamba%20Network%20for%0A%20%20Audio-Visual%20Video%20Parsing&entry.906535625=Langyu%20Wang%20and%20Bingke%20Zhu%20and%20Yingying%20Chen%20and%20Yiyuan%20Zhang%20and%20Ming%20Tang%20and%20Jinqiao%20Wang&entry.1292438233=%20%20The%20weakly-supervised%20audio-visual%20video%20parsing%20%28AVVP%29%20aims%20to%20predict%20all%0Amodality-specific%20events%20and%20locate%20their%20temporal%20boundaries.%20Despite%0Asignificant%20progress%2C%20due%20to%20the%20limitations%20of%20the%20weakly-supervised%20and%20the%0Adeficiencies%20of%20the%20model%20architecture%2C%20existing%20methods%20are%20lacking%20in%0Asimultaneously%20improving%20both%20the%20segment-level%20prediction%20and%20the%20event-level%0Aprediction.%20In%20this%20work%2C%20we%20propose%20a%20audio-visual%20Mamba%20network%20with%20pseudo%0Alabeling%20aUGmentation%20%28MUG%29%20for%20emphasising%20the%20uniqueness%20of%20each%20segment%20and%0Aexcluding%20the%20noise%20interference%20from%20the%20alternate%20modalities.%20Specifically%2C%0Awe%20annotate%20some%20of%20the%20pseudo-labels%20based%20on%20previous%20work.%20Using%20unimodal%0Apseudo-labels%2C%20we%20perform%20cross-modal%20random%20combinations%20to%20generate%20new%20data%2C%0Awhich%20can%20enhance%20the%20model%27s%20ability%20to%20parse%20various%20segment-level%20event%0Acombinations.%20For%20feature%20processing%20and%20interaction%2C%20we%20employ%20a%20audio-visual%0Amamba%20network.%20The%20AV-Mamba%20enhances%20the%20ability%20to%20perceive%20different%20segments%0Aand%20excludes%20additional%20modal%20noise%20while%20sharing%20similar%20modal%20information.%0AOur%20extensive%20experiments%20demonstrate%20that%20MUG%20improves%20state-of-the-art%0Aresults%20on%20LLP%20dataset%20in%20all%20metrics%20%28e.g%2C%2C%20gains%20of%202.1%25%20and%201.2%25%20in%20terms%20of%0Avisual%20Segment-level%20and%20audio%20Segment-level%20metrics%29.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/WangLY136/MUG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01384v2&entry.124074799=Read"},
{"title": "Large Scale Robotic Material Handling: Learning, Planning, and Control", "author": "Filippo A. Spinelli and Yifan Zhai and Fang Nan and Pascal Egli and Julian Nubert and Thilo Bleumer and Lukas Miller and Ferdinand Hofmann and Marco Hutter", "abstract": "  Bulk material handling involves the efficient and precise moving of large\nquantities of materials, a core operation in many industries, including cargo\nship unloading, waste sorting, construction, and demolition. These repetitive,\nlabor-intensive, and safety-critical operations are typically performed using\nlarge hydraulic material handlers equipped with underactuated grippers. In this\nwork, we present a comprehensive framework for the autonomous execution of\nlarge-scale material handling tasks. The system integrates specialized modules\nfor environment perception, pile attack point selection, path planning, and\nmotion control. The main contributions of this work are two reinforcement\nlearning-based modules: an attack point planner that selects optimal grasping\nlocations on the material pile to maximize removal efficiency and minimize the\nnumber of scoops, and a robust trajectory following controller that addresses\nthe precision and safety challenges associated with underactuated grippers in\nmovement, while utilizing their free-swinging nature to release material\nthrough dynamic throwing. We validate our framework through real-world\nexperiments on a 40 t material handler in a representative worksite, focusing\non two key tasks: high-throughput bulk pile management and high-precision truck\nloading. Comparative evaluations against human operators demonstrate the\nsystem's effectiveness in terms of precision, repeatability, and operational\nsafety. To the best of our knowledge, this is the first complete automation of\nmaterial handling tasks on a full scale.\n", "link": "http://arxiv.org/abs/2508.09003v1", "date": "2025-08-12", "relevancy": 1.6493, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5901}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5487}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Scale%20Robotic%20Material%20Handling%3A%20Learning%2C%20Planning%2C%20and%20Control&body=Title%3A%20Large%20Scale%20Robotic%20Material%20Handling%3A%20Learning%2C%20Planning%2C%20and%20Control%0AAuthor%3A%20Filippo%20A.%20Spinelli%20and%20Yifan%20Zhai%20and%20Fang%20Nan%20and%20Pascal%20Egli%20and%20Julian%20Nubert%20and%20Thilo%20Bleumer%20and%20Lukas%20Miller%20and%20Ferdinand%20Hofmann%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Bulk%20material%20handling%20involves%20the%20efficient%20and%20precise%20moving%20of%20large%0Aquantities%20of%20materials%2C%20a%20core%20operation%20in%20many%20industries%2C%20including%20cargo%0Aship%20unloading%2C%20waste%20sorting%2C%20construction%2C%20and%20demolition.%20These%20repetitive%2C%0Alabor-intensive%2C%20and%20safety-critical%20operations%20are%20typically%20performed%20using%0Alarge%20hydraulic%20material%20handlers%20equipped%20with%20underactuated%20grippers.%20In%20this%0Awork%2C%20we%20present%20a%20comprehensive%20framework%20for%20the%20autonomous%20execution%20of%0Alarge-scale%20material%20handling%20tasks.%20The%20system%20integrates%20specialized%20modules%0Afor%20environment%20perception%2C%20pile%20attack%20point%20selection%2C%20path%20planning%2C%20and%0Amotion%20control.%20The%20main%20contributions%20of%20this%20work%20are%20two%20reinforcement%0Alearning-based%20modules%3A%20an%20attack%20point%20planner%20that%20selects%20optimal%20grasping%0Alocations%20on%20the%20material%20pile%20to%20maximize%20removal%20efficiency%20and%20minimize%20the%0Anumber%20of%20scoops%2C%20and%20a%20robust%20trajectory%20following%20controller%20that%20addresses%0Athe%20precision%20and%20safety%20challenges%20associated%20with%20underactuated%20grippers%20in%0Amovement%2C%20while%20utilizing%20their%20free-swinging%20nature%20to%20release%20material%0Athrough%20dynamic%20throwing.%20We%20validate%20our%20framework%20through%20real-world%0Aexperiments%20on%20a%2040%20t%20material%20handler%20in%20a%20representative%20worksite%2C%20focusing%0Aon%20two%20key%20tasks%3A%20high-throughput%20bulk%20pile%20management%20and%20high-precision%20truck%0Aloading.%20Comparative%20evaluations%20against%20human%20operators%20demonstrate%20the%0Asystem%27s%20effectiveness%20in%20terms%20of%20precision%2C%20repeatability%2C%20and%20operational%0Asafety.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20complete%20automation%20of%0Amaterial%20handling%20tasks%20on%20a%20full%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Scale%2520Robotic%2520Material%2520Handling%253A%2520Learning%252C%2520Planning%252C%2520and%2520Control%26entry.906535625%3DFilippo%2520A.%2520Spinelli%2520and%2520Yifan%2520Zhai%2520and%2520Fang%2520Nan%2520and%2520Pascal%2520Egli%2520and%2520Julian%2520Nubert%2520and%2520Thilo%2520Bleumer%2520and%2520Lukas%2520Miller%2520and%2520Ferdinand%2520Hofmann%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Bulk%2520material%2520handling%2520involves%2520the%2520efficient%2520and%2520precise%2520moving%2520of%2520large%250Aquantities%2520of%2520materials%252C%2520a%2520core%2520operation%2520in%2520many%2520industries%252C%2520including%2520cargo%250Aship%2520unloading%252C%2520waste%2520sorting%252C%2520construction%252C%2520and%2520demolition.%2520These%2520repetitive%252C%250Alabor-intensive%252C%2520and%2520safety-critical%2520operations%2520are%2520typically%2520performed%2520using%250Alarge%2520hydraulic%2520material%2520handlers%2520equipped%2520with%2520underactuated%2520grippers.%2520In%2520this%250Awork%252C%2520we%2520present%2520a%2520comprehensive%2520framework%2520for%2520the%2520autonomous%2520execution%2520of%250Alarge-scale%2520material%2520handling%2520tasks.%2520The%2520system%2520integrates%2520specialized%2520modules%250Afor%2520environment%2520perception%252C%2520pile%2520attack%2520point%2520selection%252C%2520path%2520planning%252C%2520and%250Amotion%2520control.%2520The%2520main%2520contributions%2520of%2520this%2520work%2520are%2520two%2520reinforcement%250Alearning-based%2520modules%253A%2520an%2520attack%2520point%2520planner%2520that%2520selects%2520optimal%2520grasping%250Alocations%2520on%2520the%2520material%2520pile%2520to%2520maximize%2520removal%2520efficiency%2520and%2520minimize%2520the%250Anumber%2520of%2520scoops%252C%2520and%2520a%2520robust%2520trajectory%2520following%2520controller%2520that%2520addresses%250Athe%2520precision%2520and%2520safety%2520challenges%2520associated%2520with%2520underactuated%2520grippers%2520in%250Amovement%252C%2520while%2520utilizing%2520their%2520free-swinging%2520nature%2520to%2520release%2520material%250Athrough%2520dynamic%2520throwing.%2520We%2520validate%2520our%2520framework%2520through%2520real-world%250Aexperiments%2520on%2520a%252040%2520t%2520material%2520handler%2520in%2520a%2520representative%2520worksite%252C%2520focusing%250Aon%2520two%2520key%2520tasks%253A%2520high-throughput%2520bulk%2520pile%2520management%2520and%2520high-precision%2520truck%250Aloading.%2520Comparative%2520evaluations%2520against%2520human%2520operators%2520demonstrate%2520the%250Asystem%2527s%2520effectiveness%2520in%2520terms%2520of%2520precision%252C%2520repeatability%252C%2520and%2520operational%250Asafety.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520complete%2520automation%2520of%250Amaterial%2520handling%2520tasks%2520on%2520a%2520full%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Scale%20Robotic%20Material%20Handling%3A%20Learning%2C%20Planning%2C%20and%20Control&entry.906535625=Filippo%20A.%20Spinelli%20and%20Yifan%20Zhai%20and%20Fang%20Nan%20and%20Pascal%20Egli%20and%20Julian%20Nubert%20and%20Thilo%20Bleumer%20and%20Lukas%20Miller%20and%20Ferdinand%20Hofmann%20and%20Marco%20Hutter&entry.1292438233=%20%20Bulk%20material%20handling%20involves%20the%20efficient%20and%20precise%20moving%20of%20large%0Aquantities%20of%20materials%2C%20a%20core%20operation%20in%20many%20industries%2C%20including%20cargo%0Aship%20unloading%2C%20waste%20sorting%2C%20construction%2C%20and%20demolition.%20These%20repetitive%2C%0Alabor-intensive%2C%20and%20safety-critical%20operations%20are%20typically%20performed%20using%0Alarge%20hydraulic%20material%20handlers%20equipped%20with%20underactuated%20grippers.%20In%20this%0Awork%2C%20we%20present%20a%20comprehensive%20framework%20for%20the%20autonomous%20execution%20of%0Alarge-scale%20material%20handling%20tasks.%20The%20system%20integrates%20specialized%20modules%0Afor%20environment%20perception%2C%20pile%20attack%20point%20selection%2C%20path%20planning%2C%20and%0Amotion%20control.%20The%20main%20contributions%20of%20this%20work%20are%20two%20reinforcement%0Alearning-based%20modules%3A%20an%20attack%20point%20planner%20that%20selects%20optimal%20grasping%0Alocations%20on%20the%20material%20pile%20to%20maximize%20removal%20efficiency%20and%20minimize%20the%0Anumber%20of%20scoops%2C%20and%20a%20robust%20trajectory%20following%20controller%20that%20addresses%0Athe%20precision%20and%20safety%20challenges%20associated%20with%20underactuated%20grippers%20in%0Amovement%2C%20while%20utilizing%20their%20free-swinging%20nature%20to%20release%20material%0Athrough%20dynamic%20throwing.%20We%20validate%20our%20framework%20through%20real-world%0Aexperiments%20on%20a%2040%20t%20material%20handler%20in%20a%20representative%20worksite%2C%20focusing%0Aon%20two%20key%20tasks%3A%20high-throughput%20bulk%20pile%20management%20and%20high-precision%20truck%0Aloading.%20Comparative%20evaluations%20against%20human%20operators%20demonstrate%20the%0Asystem%27s%20effectiveness%20in%20terms%20of%20precision%2C%20repeatability%2C%20and%20operational%0Asafety.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20complete%20automation%20of%0Amaterial%20handling%20tasks%20on%20a%20full%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09003v1&entry.124074799=Read"},
{"title": "When Deepfakes Look Real: Detecting AI-Generated Faces with Unlabeled\n  Data due to Annotation Challenges", "author": "Zhiqiang Yang and Renshuai Tao and Xiaolong Zheng and Guodong Yang and Chunjie Zhang", "abstract": "  Existing deepfake detection methods heavily depend on labeled training data.\nHowever, as AI-generated content becomes increasingly realistic, even\n\\textbf{human annotators struggle to distinguish} between deepfakes and\nauthentic images. This makes the labeling process both time-consuming and less\nreliable. Specifically, there is a growing demand for approaches that can\neffectively utilize large-scale unlabeled data from online social networks.\nUnlike typical unsupervised learning tasks, where categories are distinct,\nAI-generated faces closely mimic real image distributions and share strong\nsimilarities, causing performance drop in conventional strategies. In this\npaper, we introduce the Dual-Path Guidance Network (DPGNet), to tackle two key\nchallenges: (1) bridging the domain gap between faces from different generation\nmodels, and (2) utilizing unlabeled image samples. The method features two core\nmodules: text-guided cross-domain alignment, which uses learnable prompts to\nunify visual and textual embeddings into a domain-invariant feature space, and\ncurriculum-driven pseudo label generation, which dynamically exploit more\ninformative unlabeled samples. To prevent catastrophic forgetting, we also\nfacilitate bridging between domains via cross-domain knowledge distillation.\nExtensive experiments on \\textbf{11 popular datasets}, show that DPGNet\noutperforms SoTA approaches by \\textbf{6.3\\%}, highlighting its effectiveness\nin leveraging unlabeled data to address the annotation challenges posed by the\nincreasing realism of deepfakes.\n", "link": "http://arxiv.org/abs/2508.09022v1", "date": "2025-08-12", "relevancy": 1.6304, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5842}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5533}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Deepfakes%20Look%20Real%3A%20Detecting%20AI-Generated%20Faces%20with%20Unlabeled%0A%20%20Data%20due%20to%20Annotation%20Challenges&body=Title%3A%20When%20Deepfakes%20Look%20Real%3A%20Detecting%20AI-Generated%20Faces%20with%20Unlabeled%0A%20%20Data%20due%20to%20Annotation%20Challenges%0AAuthor%3A%20Zhiqiang%20Yang%20and%20Renshuai%20Tao%20and%20Xiaolong%20Zheng%20and%20Guodong%20Yang%20and%20Chunjie%20Zhang%0AAbstract%3A%20%20%20Existing%20deepfake%20detection%20methods%20heavily%20depend%20on%20labeled%20training%20data.%0AHowever%2C%20as%20AI-generated%20content%20becomes%20increasingly%20realistic%2C%20even%0A%5Ctextbf%7Bhuman%20annotators%20struggle%20to%20distinguish%7D%20between%20deepfakes%20and%0Aauthentic%20images.%20This%20makes%20the%20labeling%20process%20both%20time-consuming%20and%20less%0Areliable.%20Specifically%2C%20there%20is%20a%20growing%20demand%20for%20approaches%20that%20can%0Aeffectively%20utilize%20large-scale%20unlabeled%20data%20from%20online%20social%20networks.%0AUnlike%20typical%20unsupervised%20learning%20tasks%2C%20where%20categories%20are%20distinct%2C%0AAI-generated%20faces%20closely%20mimic%20real%20image%20distributions%20and%20share%20strong%0Asimilarities%2C%20causing%20performance%20drop%20in%20conventional%20strategies.%20In%20this%0Apaper%2C%20we%20introduce%20the%20Dual-Path%20Guidance%20Network%20%28DPGNet%29%2C%20to%20tackle%20two%20key%0Achallenges%3A%20%281%29%20bridging%20the%20domain%20gap%20between%20faces%20from%20different%20generation%0Amodels%2C%20and%20%282%29%20utilizing%20unlabeled%20image%20samples.%20The%20method%20features%20two%20core%0Amodules%3A%20text-guided%20cross-domain%20alignment%2C%20which%20uses%20learnable%20prompts%20to%0Aunify%20visual%20and%20textual%20embeddings%20into%20a%20domain-invariant%20feature%20space%2C%20and%0Acurriculum-driven%20pseudo%20label%20generation%2C%20which%20dynamically%20exploit%20more%0Ainformative%20unlabeled%20samples.%20To%20prevent%20catastrophic%20forgetting%2C%20we%20also%0Afacilitate%20bridging%20between%20domains%20via%20cross-domain%20knowledge%20distillation.%0AExtensive%20experiments%20on%20%5Ctextbf%7B11%20popular%20datasets%7D%2C%20show%20that%20DPGNet%0Aoutperforms%20SoTA%20approaches%20by%20%5Ctextbf%7B6.3%5C%25%7D%2C%20highlighting%20its%20effectiveness%0Ain%20leveraging%20unlabeled%20data%20to%20address%20the%20annotation%20challenges%20posed%20by%20the%0Aincreasing%20realism%20of%20deepfakes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Deepfakes%2520Look%2520Real%253A%2520Detecting%2520AI-Generated%2520Faces%2520with%2520Unlabeled%250A%2520%2520Data%2520due%2520to%2520Annotation%2520Challenges%26entry.906535625%3DZhiqiang%2520Yang%2520and%2520Renshuai%2520Tao%2520and%2520Xiaolong%2520Zheng%2520and%2520Guodong%2520Yang%2520and%2520Chunjie%2520Zhang%26entry.1292438233%3D%2520%2520Existing%2520deepfake%2520detection%2520methods%2520heavily%2520depend%2520on%2520labeled%2520training%2520data.%250AHowever%252C%2520as%2520AI-generated%2520content%2520becomes%2520increasingly%2520realistic%252C%2520even%250A%255Ctextbf%257Bhuman%2520annotators%2520struggle%2520to%2520distinguish%257D%2520between%2520deepfakes%2520and%250Aauthentic%2520images.%2520This%2520makes%2520the%2520labeling%2520process%2520both%2520time-consuming%2520and%2520less%250Areliable.%2520Specifically%252C%2520there%2520is%2520a%2520growing%2520demand%2520for%2520approaches%2520that%2520can%250Aeffectively%2520utilize%2520large-scale%2520unlabeled%2520data%2520from%2520online%2520social%2520networks.%250AUnlike%2520typical%2520unsupervised%2520learning%2520tasks%252C%2520where%2520categories%2520are%2520distinct%252C%250AAI-generated%2520faces%2520closely%2520mimic%2520real%2520image%2520distributions%2520and%2520share%2520strong%250Asimilarities%252C%2520causing%2520performance%2520drop%2520in%2520conventional%2520strategies.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520the%2520Dual-Path%2520Guidance%2520Network%2520%2528DPGNet%2529%252C%2520to%2520tackle%2520two%2520key%250Achallenges%253A%2520%25281%2529%2520bridging%2520the%2520domain%2520gap%2520between%2520faces%2520from%2520different%2520generation%250Amodels%252C%2520and%2520%25282%2529%2520utilizing%2520unlabeled%2520image%2520samples.%2520The%2520method%2520features%2520two%2520core%250Amodules%253A%2520text-guided%2520cross-domain%2520alignment%252C%2520which%2520uses%2520learnable%2520prompts%2520to%250Aunify%2520visual%2520and%2520textual%2520embeddings%2520into%2520a%2520domain-invariant%2520feature%2520space%252C%2520and%250Acurriculum-driven%2520pseudo%2520label%2520generation%252C%2520which%2520dynamically%2520exploit%2520more%250Ainformative%2520unlabeled%2520samples.%2520To%2520prevent%2520catastrophic%2520forgetting%252C%2520we%2520also%250Afacilitate%2520bridging%2520between%2520domains%2520via%2520cross-domain%2520knowledge%2520distillation.%250AExtensive%2520experiments%2520on%2520%255Ctextbf%257B11%2520popular%2520datasets%257D%252C%2520show%2520that%2520DPGNet%250Aoutperforms%2520SoTA%2520approaches%2520by%2520%255Ctextbf%257B6.3%255C%2525%257D%252C%2520highlighting%2520its%2520effectiveness%250Ain%2520leveraging%2520unlabeled%2520data%2520to%2520address%2520the%2520annotation%2520challenges%2520posed%2520by%2520the%250Aincreasing%2520realism%2520of%2520deepfakes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Deepfakes%20Look%20Real%3A%20Detecting%20AI-Generated%20Faces%20with%20Unlabeled%0A%20%20Data%20due%20to%20Annotation%20Challenges&entry.906535625=Zhiqiang%20Yang%20and%20Renshuai%20Tao%20and%20Xiaolong%20Zheng%20and%20Guodong%20Yang%20and%20Chunjie%20Zhang&entry.1292438233=%20%20Existing%20deepfake%20detection%20methods%20heavily%20depend%20on%20labeled%20training%20data.%0AHowever%2C%20as%20AI-generated%20content%20becomes%20increasingly%20realistic%2C%20even%0A%5Ctextbf%7Bhuman%20annotators%20struggle%20to%20distinguish%7D%20between%20deepfakes%20and%0Aauthentic%20images.%20This%20makes%20the%20labeling%20process%20both%20time-consuming%20and%20less%0Areliable.%20Specifically%2C%20there%20is%20a%20growing%20demand%20for%20approaches%20that%20can%0Aeffectively%20utilize%20large-scale%20unlabeled%20data%20from%20online%20social%20networks.%0AUnlike%20typical%20unsupervised%20learning%20tasks%2C%20where%20categories%20are%20distinct%2C%0AAI-generated%20faces%20closely%20mimic%20real%20image%20distributions%20and%20share%20strong%0Asimilarities%2C%20causing%20performance%20drop%20in%20conventional%20strategies.%20In%20this%0Apaper%2C%20we%20introduce%20the%20Dual-Path%20Guidance%20Network%20%28DPGNet%29%2C%20to%20tackle%20two%20key%0Achallenges%3A%20%281%29%20bridging%20the%20domain%20gap%20between%20faces%20from%20different%20generation%0Amodels%2C%20and%20%282%29%20utilizing%20unlabeled%20image%20samples.%20The%20method%20features%20two%20core%0Amodules%3A%20text-guided%20cross-domain%20alignment%2C%20which%20uses%20learnable%20prompts%20to%0Aunify%20visual%20and%20textual%20embeddings%20into%20a%20domain-invariant%20feature%20space%2C%20and%0Acurriculum-driven%20pseudo%20label%20generation%2C%20which%20dynamically%20exploit%20more%0Ainformative%20unlabeled%20samples.%20To%20prevent%20catastrophic%20forgetting%2C%20we%20also%0Afacilitate%20bridging%20between%20domains%20via%20cross-domain%20knowledge%20distillation.%0AExtensive%20experiments%20on%20%5Ctextbf%7B11%20popular%20datasets%7D%2C%20show%20that%20DPGNet%0Aoutperforms%20SoTA%20approaches%20by%20%5Ctextbf%7B6.3%5C%25%7D%2C%20highlighting%20its%20effectiveness%0Ain%20leveraging%20unlabeled%20data%20to%20address%20the%20annotation%20challenges%20posed%20by%20the%0Aincreasing%20realism%20of%20deepfakes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09022v1&entry.124074799=Read"},
{"title": "Addressing Bias in VLMs for Glaucoma Detection Without Protected\n  Attribute Supervision", "author": "Ahsan Habib Akash and Greg Murray and Annahita Amireskandari and Joel Palko and Carol Laxson and Binod Bhattarai and Prashnna Gyawali", "abstract": "  Vision-Language Models (VLMs) have achieved remarkable success on multimodal\ntasks such as image-text retrieval and zero-shot classification, yet they can\nexhibit demographic biases even when explicit protected attributes are absent\nduring training. In this work, we focus on automated glaucoma screening from\nretinal fundus images, a critical application given that glaucoma is a leading\ncause of irreversible blindness and disproportionately affects underserved\npopulations. Building on a reweighting-based contrastive learning framework, we\nintroduce an attribute-agnostic debiasing method that (i) infers proxy\nsubgroups via unsupervised clustering of image-image embeddings, (ii) computes\ngradient-similarity weights between the CLIP-style multimodal loss and a\nSimCLR-style image-pair contrastive loss, and (iii) applies these weights in a\njoint, top-$k$ weighted objective to upweight underperforming clusters. This\nlabel-free approach adaptively targets the hardest examples, thereby reducing\nsubgroup disparities. We evaluate our method on the Harvard FairVLMed glaucoma\nsubset, reporting Equalized Odds Distance (EOD), Equalized Subgroup AUC (ES\nAUC), and Groupwise AUC to demonstrate equitable performance across inferred\ndemographic subgroups.\n", "link": "http://arxiv.org/abs/2508.09087v1", "date": "2025-08-12", "relevancy": 1.6154, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5502}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5318}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20Bias%20in%20VLMs%20for%20Glaucoma%20Detection%20Without%20Protected%0A%20%20Attribute%20Supervision&body=Title%3A%20Addressing%20Bias%20in%20VLMs%20for%20Glaucoma%20Detection%20Without%20Protected%0A%20%20Attribute%20Supervision%0AAuthor%3A%20Ahsan%20Habib%20Akash%20and%20Greg%20Murray%20and%20Annahita%20Amireskandari%20and%20Joel%20Palko%20and%20Carol%20Laxson%20and%20Binod%20Bhattarai%20and%20Prashnna%20Gyawali%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20remarkable%20success%20on%20multimodal%0Atasks%20such%20as%20image-text%20retrieval%20and%20zero-shot%20classification%2C%20yet%20they%20can%0Aexhibit%20demographic%20biases%20even%20when%20explicit%20protected%20attributes%20are%20absent%0Aduring%20training.%20In%20this%20work%2C%20we%20focus%20on%20automated%20glaucoma%20screening%20from%0Aretinal%20fundus%20images%2C%20a%20critical%20application%20given%20that%20glaucoma%20is%20a%20leading%0Acause%20of%20irreversible%20blindness%20and%20disproportionately%20affects%20underserved%0Apopulations.%20Building%20on%20a%20reweighting-based%20contrastive%20learning%20framework%2C%20we%0Aintroduce%20an%20attribute-agnostic%20debiasing%20method%20that%20%28i%29%20infers%20proxy%0Asubgroups%20via%20unsupervised%20clustering%20of%20image-image%20embeddings%2C%20%28ii%29%20computes%0Agradient-similarity%20weights%20between%20the%20CLIP-style%20multimodal%20loss%20and%20a%0ASimCLR-style%20image-pair%20contrastive%20loss%2C%20and%20%28iii%29%20applies%20these%20weights%20in%20a%0Ajoint%2C%20top-%24k%24%20weighted%20objective%20to%20upweight%20underperforming%20clusters.%20This%0Alabel-free%20approach%20adaptively%20targets%20the%20hardest%20examples%2C%20thereby%20reducing%0Asubgroup%20disparities.%20We%20evaluate%20our%20method%20on%20the%20Harvard%20FairVLMed%20glaucoma%0Asubset%2C%20reporting%20Equalized%20Odds%20Distance%20%28EOD%29%2C%20Equalized%20Subgroup%20AUC%20%28ES%0AAUC%29%2C%20and%20Groupwise%20AUC%20to%20demonstrate%20equitable%20performance%20across%20inferred%0Ademographic%20subgroups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09087v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520Bias%2520in%2520VLMs%2520for%2520Glaucoma%2520Detection%2520Without%2520Protected%250A%2520%2520Attribute%2520Supervision%26entry.906535625%3DAhsan%2520Habib%2520Akash%2520and%2520Greg%2520Murray%2520and%2520Annahita%2520Amireskandari%2520and%2520Joel%2520Palko%2520and%2520Carol%2520Laxson%2520and%2520Binod%2520Bhattarai%2520and%2520Prashnna%2520Gyawali%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520on%2520multimodal%250Atasks%2520such%2520as%2520image-text%2520retrieval%2520and%2520zero-shot%2520classification%252C%2520yet%2520they%2520can%250Aexhibit%2520demographic%2520biases%2520even%2520when%2520explicit%2520protected%2520attributes%2520are%2520absent%250Aduring%2520training.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520automated%2520glaucoma%2520screening%2520from%250Aretinal%2520fundus%2520images%252C%2520a%2520critical%2520application%2520given%2520that%2520glaucoma%2520is%2520a%2520leading%250Acause%2520of%2520irreversible%2520blindness%2520and%2520disproportionately%2520affects%2520underserved%250Apopulations.%2520Building%2520on%2520a%2520reweighting-based%2520contrastive%2520learning%2520framework%252C%2520we%250Aintroduce%2520an%2520attribute-agnostic%2520debiasing%2520method%2520that%2520%2528i%2529%2520infers%2520proxy%250Asubgroups%2520via%2520unsupervised%2520clustering%2520of%2520image-image%2520embeddings%252C%2520%2528ii%2529%2520computes%250Agradient-similarity%2520weights%2520between%2520the%2520CLIP-style%2520multimodal%2520loss%2520and%2520a%250ASimCLR-style%2520image-pair%2520contrastive%2520loss%252C%2520and%2520%2528iii%2529%2520applies%2520these%2520weights%2520in%2520a%250Ajoint%252C%2520top-%2524k%2524%2520weighted%2520objective%2520to%2520upweight%2520underperforming%2520clusters.%2520This%250Alabel-free%2520approach%2520adaptively%2520targets%2520the%2520hardest%2520examples%252C%2520thereby%2520reducing%250Asubgroup%2520disparities.%2520We%2520evaluate%2520our%2520method%2520on%2520the%2520Harvard%2520FairVLMed%2520glaucoma%250Asubset%252C%2520reporting%2520Equalized%2520Odds%2520Distance%2520%2528EOD%2529%252C%2520Equalized%2520Subgroup%2520AUC%2520%2528ES%250AAUC%2529%252C%2520and%2520Groupwise%2520AUC%2520to%2520demonstrate%2520equitable%2520performance%2520across%2520inferred%250Ademographic%2520subgroups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09087v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Bias%20in%20VLMs%20for%20Glaucoma%20Detection%20Without%20Protected%0A%20%20Attribute%20Supervision&entry.906535625=Ahsan%20Habib%20Akash%20and%20Greg%20Murray%20and%20Annahita%20Amireskandari%20and%20Joel%20Palko%20and%20Carol%20Laxson%20and%20Binod%20Bhattarai%20and%20Prashnna%20Gyawali&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20remarkable%20success%20on%20multimodal%0Atasks%20such%20as%20image-text%20retrieval%20and%20zero-shot%20classification%2C%20yet%20they%20can%0Aexhibit%20demographic%20biases%20even%20when%20explicit%20protected%20attributes%20are%20absent%0Aduring%20training.%20In%20this%20work%2C%20we%20focus%20on%20automated%20glaucoma%20screening%20from%0Aretinal%20fundus%20images%2C%20a%20critical%20application%20given%20that%20glaucoma%20is%20a%20leading%0Acause%20of%20irreversible%20blindness%20and%20disproportionately%20affects%20underserved%0Apopulations.%20Building%20on%20a%20reweighting-based%20contrastive%20learning%20framework%2C%20we%0Aintroduce%20an%20attribute-agnostic%20debiasing%20method%20that%20%28i%29%20infers%20proxy%0Asubgroups%20via%20unsupervised%20clustering%20of%20image-image%20embeddings%2C%20%28ii%29%20computes%0Agradient-similarity%20weights%20between%20the%20CLIP-style%20multimodal%20loss%20and%20a%0ASimCLR-style%20image-pair%20contrastive%20loss%2C%20and%20%28iii%29%20applies%20these%20weights%20in%20a%0Ajoint%2C%20top-%24k%24%20weighted%20objective%20to%20upweight%20underperforming%20clusters.%20This%0Alabel-free%20approach%20adaptively%20targets%20the%20hardest%20examples%2C%20thereby%20reducing%0Asubgroup%20disparities.%20We%20evaluate%20our%20method%20on%20the%20Harvard%20FairVLMed%20glaucoma%0Asubset%2C%20reporting%20Equalized%20Odds%20Distance%20%28EOD%29%2C%20Equalized%20Subgroup%20AUC%20%28ES%0AAUC%29%2C%20and%20Groupwise%20AUC%20to%20demonstrate%20equitable%20performance%20across%20inferred%0Ademographic%20subgroups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09087v1&entry.124074799=Read"},
{"title": "Scaling Learned Image Compression Models up to 1 Billion", "author": "Yuqi Li and Haotian Zhang and Li Li and Dong Liu and Feng Wu", "abstract": "  Recent advances in large language models (LLMs) highlight a strong connection\nbetween intelligence and compression. Learned image compression, a fundamental\ntask in modern data compression, has made significant progress in recent years.\nHowever, current models remain limited in scale, restricting their\nrepresentation capacity, and how scaling model size influences compression\nperformance remains unexplored. In this work, we present a pioneering study on\nscaling up learned image compression models and revealing the performance\ntrends through scaling laws. Using the recent state-of-the-art HPCM model as\nbaseline, we scale model parameters from 68.5 millions to 1 billion and fit\npower-law relations between test loss and key scaling variables, including\nmodel size and optimal training compute. The results reveal a scaling trend,\nenabling extrapolation to larger scale models. Experimental results demonstrate\nthat the scaled-up HPCM-1B model achieves state-of-the-art rate-distortion\nperformance. We hope this work inspires future exploration of large-scale\ncompression models and deeper investigations into the connection between\ncompression and intelligence.\n", "link": "http://arxiv.org/abs/2508.09075v1", "date": "2025-08-12", "relevancy": 1.5942, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5866}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Learned%20Image%20Compression%20Models%20up%20to%201%20Billion&body=Title%3A%20Scaling%20Learned%20Image%20Compression%20Models%20up%20to%201%20Billion%0AAuthor%3A%20Yuqi%20Li%20and%20Haotian%20Zhang%20and%20Li%20Li%20and%20Dong%20Liu%20and%20Feng%20Wu%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20highlight%20a%20strong%20connection%0Abetween%20intelligence%20and%20compression.%20Learned%20image%20compression%2C%20a%20fundamental%0Atask%20in%20modern%20data%20compression%2C%20has%20made%20significant%20progress%20in%20recent%20years.%0AHowever%2C%20current%20models%20remain%20limited%20in%20scale%2C%20restricting%20their%0Arepresentation%20capacity%2C%20and%20how%20scaling%20model%20size%20influences%20compression%0Aperformance%20remains%20unexplored.%20In%20this%20work%2C%20we%20present%20a%20pioneering%20study%20on%0Ascaling%20up%20learned%20image%20compression%20models%20and%20revealing%20the%20performance%0Atrends%20through%20scaling%20laws.%20Using%20the%20recent%20state-of-the-art%20HPCM%20model%20as%0Abaseline%2C%20we%20scale%20model%20parameters%20from%2068.5%20millions%20to%201%20billion%20and%20fit%0Apower-law%20relations%20between%20test%20loss%20and%20key%20scaling%20variables%2C%20including%0Amodel%20size%20and%20optimal%20training%20compute.%20The%20results%20reveal%20a%20scaling%20trend%2C%0Aenabling%20extrapolation%20to%20larger%20scale%20models.%20Experimental%20results%20demonstrate%0Athat%20the%20scaled-up%20HPCM-1B%20model%20achieves%20state-of-the-art%20rate-distortion%0Aperformance.%20We%20hope%20this%20work%20inspires%20future%20exploration%20of%20large-scale%0Acompression%20models%20and%20deeper%20investigations%20into%20the%20connection%20between%0Acompression%20and%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Learned%2520Image%2520Compression%2520Models%2520up%2520to%25201%2520Billion%26entry.906535625%3DYuqi%2520Li%2520and%2520Haotian%2520Zhang%2520and%2520Li%2520Li%2520and%2520Dong%2520Liu%2520and%2520Feng%2520Wu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520highlight%2520a%2520strong%2520connection%250Abetween%2520intelligence%2520and%2520compression.%2520Learned%2520image%2520compression%252C%2520a%2520fundamental%250Atask%2520in%2520modern%2520data%2520compression%252C%2520has%2520made%2520significant%2520progress%2520in%2520recent%2520years.%250AHowever%252C%2520current%2520models%2520remain%2520limited%2520in%2520scale%252C%2520restricting%2520their%250Arepresentation%2520capacity%252C%2520and%2520how%2520scaling%2520model%2520size%2520influences%2520compression%250Aperformance%2520remains%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520pioneering%2520study%2520on%250Ascaling%2520up%2520learned%2520image%2520compression%2520models%2520and%2520revealing%2520the%2520performance%250Atrends%2520through%2520scaling%2520laws.%2520Using%2520the%2520recent%2520state-of-the-art%2520HPCM%2520model%2520as%250Abaseline%252C%2520we%2520scale%2520model%2520parameters%2520from%252068.5%2520millions%2520to%25201%2520billion%2520and%2520fit%250Apower-law%2520relations%2520between%2520test%2520loss%2520and%2520key%2520scaling%2520variables%252C%2520including%250Amodel%2520size%2520and%2520optimal%2520training%2520compute.%2520The%2520results%2520reveal%2520a%2520scaling%2520trend%252C%250Aenabling%2520extrapolation%2520to%2520larger%2520scale%2520models.%2520Experimental%2520results%2520demonstrate%250Athat%2520the%2520scaled-up%2520HPCM-1B%2520model%2520achieves%2520state-of-the-art%2520rate-distortion%250Aperformance.%2520We%2520hope%2520this%2520work%2520inspires%2520future%2520exploration%2520of%2520large-scale%250Acompression%2520models%2520and%2520deeper%2520investigations%2520into%2520the%2520connection%2520between%250Acompression%2520and%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Learned%20Image%20Compression%20Models%20up%20to%201%20Billion&entry.906535625=Yuqi%20Li%20and%20Haotian%20Zhang%20and%20Li%20Li%20and%20Dong%20Liu%20and%20Feng%20Wu&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20highlight%20a%20strong%20connection%0Abetween%20intelligence%20and%20compression.%20Learned%20image%20compression%2C%20a%20fundamental%0Atask%20in%20modern%20data%20compression%2C%20has%20made%20significant%20progress%20in%20recent%20years.%0AHowever%2C%20current%20models%20remain%20limited%20in%20scale%2C%20restricting%20their%0Arepresentation%20capacity%2C%20and%20how%20scaling%20model%20size%20influences%20compression%0Aperformance%20remains%20unexplored.%20In%20this%20work%2C%20we%20present%20a%20pioneering%20study%20on%0Ascaling%20up%20learned%20image%20compression%20models%20and%20revealing%20the%20performance%0Atrends%20through%20scaling%20laws.%20Using%20the%20recent%20state-of-the-art%20HPCM%20model%20as%0Abaseline%2C%20we%20scale%20model%20parameters%20from%2068.5%20millions%20to%201%20billion%20and%20fit%0Apower-law%20relations%20between%20test%20loss%20and%20key%20scaling%20variables%2C%20including%0Amodel%20size%20and%20optimal%20training%20compute.%20The%20results%20reveal%20a%20scaling%20trend%2C%0Aenabling%20extrapolation%20to%20larger%20scale%20models.%20Experimental%20results%20demonstrate%0Athat%20the%20scaled-up%20HPCM-1B%20model%20achieves%20state-of-the-art%20rate-distortion%0Aperformance.%20We%20hope%20this%20work%20inspires%20future%20exploration%20of%20large-scale%0Acompression%20models%20and%20deeper%20investigations%20into%20the%20connection%20between%0Acompression%20and%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09075v1&entry.124074799=Read"},
{"title": "P/D-Device: Disaggregated Large Language Model between Cloud and Devices", "author": "Yibo Jin and Yixu Xu and Yue Chen and Chengbin Wang and Tao Wang and Jiaqi Huang and Rongfei Zhang and Yiming Dong and Yuting Yan and Ke Cheng and Yingjie Zhu and Shulan Wang and Qianqian Tang and Shuaishuai Meng and Guanxin Cheng and Ze Wang and Shuyan Miao and Ketao Wang and Wen Liu and Yifan Yang and Tong Zhang and Anran Wang and Chengzhou Lu and Tiantian Dong and Yongsheng Zhang and Zhe Wang and Hefei Guo and Hongjie Liu and Wei Lu and Zhengyong Zhang", "abstract": "  Serving disaggregated large language models has been widely adopted in\nindustrial practice for enhanced performance. However, too many tokens\ngenerated in decoding phase, i.e., occupying the resources for a long time,\nessentially hamper the cloud from achieving a higher throughput. Meanwhile, due\nto limited on-device resources, the time to first token (TTFT), i.e., the\nlatency of prefill phase, increases dramatically with the growth on prompt\nlength. In order to concur with such a bottleneck on resources, i.e., long\noccupation in cloud and limited on-device computing capacity, we propose to\nseparate large language model between cloud and devices. That is, the cloud\nhelps a portion of the content for each device, only in its prefill phase.\nSpecifically, after receiving the first token from the cloud, decoupling with\nits own prefill, the device responds to the user immediately for a lower TTFT.\nThen, the following tokens from cloud are presented via a speed controller for\nsmoothed TPOT (the time per output token), until the device catches up with the\nprogress. On-device prefill is then amortized using received tokens while the\nresource usage in cloud is controlled. Moreover, during cloud prefill, the\nprompt can be refined, using those intermediate data already generated, to\nfurther speed up on-device inference. We implement such a scheme P/D-Device,\nand confirm its superiority over other alternatives. We further propose an\nalgorithm to decide the best settings. Real-trace experiments show that TTFT\ndecreases at least 60%, maximum TPOT is about tens of milliseconds, and cloud\nthroughput increases by up to 15x.\n", "link": "http://arxiv.org/abs/2508.09035v1", "date": "2025-08-12", "relevancy": 1.5898, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5679}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5282}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20P/D-Device%3A%20Disaggregated%20Large%20Language%20Model%20between%20Cloud%20and%20Devices&body=Title%3A%20P/D-Device%3A%20Disaggregated%20Large%20Language%20Model%20between%20Cloud%20and%20Devices%0AAuthor%3A%20Yibo%20Jin%20and%20Yixu%20Xu%20and%20Yue%20Chen%20and%20Chengbin%20Wang%20and%20Tao%20Wang%20and%20Jiaqi%20Huang%20and%20Rongfei%20Zhang%20and%20Yiming%20Dong%20and%20Yuting%20Yan%20and%20Ke%20Cheng%20and%20Yingjie%20Zhu%20and%20Shulan%20Wang%20and%20Qianqian%20Tang%20and%20Shuaishuai%20Meng%20and%20Guanxin%20Cheng%20and%20Ze%20Wang%20and%20Shuyan%20Miao%20and%20Ketao%20Wang%20and%20Wen%20Liu%20and%20Yifan%20Yang%20and%20Tong%20Zhang%20and%20Anran%20Wang%20and%20Chengzhou%20Lu%20and%20Tiantian%20Dong%20and%20Yongsheng%20Zhang%20and%20Zhe%20Wang%20and%20Hefei%20Guo%20and%20Hongjie%20Liu%20and%20Wei%20Lu%20and%20Zhengyong%20Zhang%0AAbstract%3A%20%20%20Serving%20disaggregated%20large%20language%20models%20has%20been%20widely%20adopted%20in%0Aindustrial%20practice%20for%20enhanced%20performance.%20However%2C%20too%20many%20tokens%0Agenerated%20in%20decoding%20phase%2C%20i.e.%2C%20occupying%20the%20resources%20for%20a%20long%20time%2C%0Aessentially%20hamper%20the%20cloud%20from%20achieving%20a%20higher%20throughput.%20Meanwhile%2C%20due%0Ato%20limited%20on-device%20resources%2C%20the%20time%20to%20first%20token%20%28TTFT%29%2C%20i.e.%2C%20the%0Alatency%20of%20prefill%20phase%2C%20increases%20dramatically%20with%20the%20growth%20on%20prompt%0Alength.%20In%20order%20to%20concur%20with%20such%20a%20bottleneck%20on%20resources%2C%20i.e.%2C%20long%0Aoccupation%20in%20cloud%20and%20limited%20on-device%20computing%20capacity%2C%20we%20propose%20to%0Aseparate%20large%20language%20model%20between%20cloud%20and%20devices.%20That%20is%2C%20the%20cloud%0Ahelps%20a%20portion%20of%20the%20content%20for%20each%20device%2C%20only%20in%20its%20prefill%20phase.%0ASpecifically%2C%20after%20receiving%20the%20first%20token%20from%20the%20cloud%2C%20decoupling%20with%0Aits%20own%20prefill%2C%20the%20device%20responds%20to%20the%20user%20immediately%20for%20a%20lower%20TTFT.%0AThen%2C%20the%20following%20tokens%20from%20cloud%20are%20presented%20via%20a%20speed%20controller%20for%0Asmoothed%20TPOT%20%28the%20time%20per%20output%20token%29%2C%20until%20the%20device%20catches%20up%20with%20the%0Aprogress.%20On-device%20prefill%20is%20then%20amortized%20using%20received%20tokens%20while%20the%0Aresource%20usage%20in%20cloud%20is%20controlled.%20Moreover%2C%20during%20cloud%20prefill%2C%20the%0Aprompt%20can%20be%20refined%2C%20using%20those%20intermediate%20data%20already%20generated%2C%20to%0Afurther%20speed%20up%20on-device%20inference.%20We%20implement%20such%20a%20scheme%20P/D-Device%2C%0Aand%20confirm%20its%20superiority%20over%20other%20alternatives.%20We%20further%20propose%20an%0Aalgorithm%20to%20decide%20the%20best%20settings.%20Real-trace%20experiments%20show%20that%20TTFT%0Adecreases%20at%20least%2060%25%2C%20maximum%20TPOT%20is%20about%20tens%20of%20milliseconds%2C%20and%20cloud%0Athroughput%20increases%20by%20up%20to%2015x.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DP/D-Device%253A%2520Disaggregated%2520Large%2520Language%2520Model%2520between%2520Cloud%2520and%2520Devices%26entry.906535625%3DYibo%2520Jin%2520and%2520Yixu%2520Xu%2520and%2520Yue%2520Chen%2520and%2520Chengbin%2520Wang%2520and%2520Tao%2520Wang%2520and%2520Jiaqi%2520Huang%2520and%2520Rongfei%2520Zhang%2520and%2520Yiming%2520Dong%2520and%2520Yuting%2520Yan%2520and%2520Ke%2520Cheng%2520and%2520Yingjie%2520Zhu%2520and%2520Shulan%2520Wang%2520and%2520Qianqian%2520Tang%2520and%2520Shuaishuai%2520Meng%2520and%2520Guanxin%2520Cheng%2520and%2520Ze%2520Wang%2520and%2520Shuyan%2520Miao%2520and%2520Ketao%2520Wang%2520and%2520Wen%2520Liu%2520and%2520Yifan%2520Yang%2520and%2520Tong%2520Zhang%2520and%2520Anran%2520Wang%2520and%2520Chengzhou%2520Lu%2520and%2520Tiantian%2520Dong%2520and%2520Yongsheng%2520Zhang%2520and%2520Zhe%2520Wang%2520and%2520Hefei%2520Guo%2520and%2520Hongjie%2520Liu%2520and%2520Wei%2520Lu%2520and%2520Zhengyong%2520Zhang%26entry.1292438233%3D%2520%2520Serving%2520disaggregated%2520large%2520language%2520models%2520has%2520been%2520widely%2520adopted%2520in%250Aindustrial%2520practice%2520for%2520enhanced%2520performance.%2520However%252C%2520too%2520many%2520tokens%250Agenerated%2520in%2520decoding%2520phase%252C%2520i.e.%252C%2520occupying%2520the%2520resources%2520for%2520a%2520long%2520time%252C%250Aessentially%2520hamper%2520the%2520cloud%2520from%2520achieving%2520a%2520higher%2520throughput.%2520Meanwhile%252C%2520due%250Ato%2520limited%2520on-device%2520resources%252C%2520the%2520time%2520to%2520first%2520token%2520%2528TTFT%2529%252C%2520i.e.%252C%2520the%250Alatency%2520of%2520prefill%2520phase%252C%2520increases%2520dramatically%2520with%2520the%2520growth%2520on%2520prompt%250Alength.%2520In%2520order%2520to%2520concur%2520with%2520such%2520a%2520bottleneck%2520on%2520resources%252C%2520i.e.%252C%2520long%250Aoccupation%2520in%2520cloud%2520and%2520limited%2520on-device%2520computing%2520capacity%252C%2520we%2520propose%2520to%250Aseparate%2520large%2520language%2520model%2520between%2520cloud%2520and%2520devices.%2520That%2520is%252C%2520the%2520cloud%250Ahelps%2520a%2520portion%2520of%2520the%2520content%2520for%2520each%2520device%252C%2520only%2520in%2520its%2520prefill%2520phase.%250ASpecifically%252C%2520after%2520receiving%2520the%2520first%2520token%2520from%2520the%2520cloud%252C%2520decoupling%2520with%250Aits%2520own%2520prefill%252C%2520the%2520device%2520responds%2520to%2520the%2520user%2520immediately%2520for%2520a%2520lower%2520TTFT.%250AThen%252C%2520the%2520following%2520tokens%2520from%2520cloud%2520are%2520presented%2520via%2520a%2520speed%2520controller%2520for%250Asmoothed%2520TPOT%2520%2528the%2520time%2520per%2520output%2520token%2529%252C%2520until%2520the%2520device%2520catches%2520up%2520with%2520the%250Aprogress.%2520On-device%2520prefill%2520is%2520then%2520amortized%2520using%2520received%2520tokens%2520while%2520the%250Aresource%2520usage%2520in%2520cloud%2520is%2520controlled.%2520Moreover%252C%2520during%2520cloud%2520prefill%252C%2520the%250Aprompt%2520can%2520be%2520refined%252C%2520using%2520those%2520intermediate%2520data%2520already%2520generated%252C%2520to%250Afurther%2520speed%2520up%2520on-device%2520inference.%2520We%2520implement%2520such%2520a%2520scheme%2520P/D-Device%252C%250Aand%2520confirm%2520its%2520superiority%2520over%2520other%2520alternatives.%2520We%2520further%2520propose%2520an%250Aalgorithm%2520to%2520decide%2520the%2520best%2520settings.%2520Real-trace%2520experiments%2520show%2520that%2520TTFT%250Adecreases%2520at%2520least%252060%2525%252C%2520maximum%2520TPOT%2520is%2520about%2520tens%2520of%2520milliseconds%252C%2520and%2520cloud%250Athroughput%2520increases%2520by%2520up%2520to%252015x.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P/D-Device%3A%20Disaggregated%20Large%20Language%20Model%20between%20Cloud%20and%20Devices&entry.906535625=Yibo%20Jin%20and%20Yixu%20Xu%20and%20Yue%20Chen%20and%20Chengbin%20Wang%20and%20Tao%20Wang%20and%20Jiaqi%20Huang%20and%20Rongfei%20Zhang%20and%20Yiming%20Dong%20and%20Yuting%20Yan%20and%20Ke%20Cheng%20and%20Yingjie%20Zhu%20and%20Shulan%20Wang%20and%20Qianqian%20Tang%20and%20Shuaishuai%20Meng%20and%20Guanxin%20Cheng%20and%20Ze%20Wang%20and%20Shuyan%20Miao%20and%20Ketao%20Wang%20and%20Wen%20Liu%20and%20Yifan%20Yang%20and%20Tong%20Zhang%20and%20Anran%20Wang%20and%20Chengzhou%20Lu%20and%20Tiantian%20Dong%20and%20Yongsheng%20Zhang%20and%20Zhe%20Wang%20and%20Hefei%20Guo%20and%20Hongjie%20Liu%20and%20Wei%20Lu%20and%20Zhengyong%20Zhang&entry.1292438233=%20%20Serving%20disaggregated%20large%20language%20models%20has%20been%20widely%20adopted%20in%0Aindustrial%20practice%20for%20enhanced%20performance.%20However%2C%20too%20many%20tokens%0Agenerated%20in%20decoding%20phase%2C%20i.e.%2C%20occupying%20the%20resources%20for%20a%20long%20time%2C%0Aessentially%20hamper%20the%20cloud%20from%20achieving%20a%20higher%20throughput.%20Meanwhile%2C%20due%0Ato%20limited%20on-device%20resources%2C%20the%20time%20to%20first%20token%20%28TTFT%29%2C%20i.e.%2C%20the%0Alatency%20of%20prefill%20phase%2C%20increases%20dramatically%20with%20the%20growth%20on%20prompt%0Alength.%20In%20order%20to%20concur%20with%20such%20a%20bottleneck%20on%20resources%2C%20i.e.%2C%20long%0Aoccupation%20in%20cloud%20and%20limited%20on-device%20computing%20capacity%2C%20we%20propose%20to%0Aseparate%20large%20language%20model%20between%20cloud%20and%20devices.%20That%20is%2C%20the%20cloud%0Ahelps%20a%20portion%20of%20the%20content%20for%20each%20device%2C%20only%20in%20its%20prefill%20phase.%0ASpecifically%2C%20after%20receiving%20the%20first%20token%20from%20the%20cloud%2C%20decoupling%20with%0Aits%20own%20prefill%2C%20the%20device%20responds%20to%20the%20user%20immediately%20for%20a%20lower%20TTFT.%0AThen%2C%20the%20following%20tokens%20from%20cloud%20are%20presented%20via%20a%20speed%20controller%20for%0Asmoothed%20TPOT%20%28the%20time%20per%20output%20token%29%2C%20until%20the%20device%20catches%20up%20with%20the%0Aprogress.%20On-device%20prefill%20is%20then%20amortized%20using%20received%20tokens%20while%20the%0Aresource%20usage%20in%20cloud%20is%20controlled.%20Moreover%2C%20during%20cloud%20prefill%2C%20the%0Aprompt%20can%20be%20refined%2C%20using%20those%20intermediate%20data%20already%20generated%2C%20to%0Afurther%20speed%20up%20on-device%20inference.%20We%20implement%20such%20a%20scheme%20P/D-Device%2C%0Aand%20confirm%20its%20superiority%20over%20other%20alternatives.%20We%20further%20propose%20an%0Aalgorithm%20to%20decide%20the%20best%20settings.%20Real-trace%20experiments%20show%20that%20TTFT%0Adecreases%20at%20least%2060%25%2C%20maximum%20TPOT%20is%20about%20tens%20of%20milliseconds%2C%20and%20cloud%0Athroughput%20increases%20by%20up%20to%2015x.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09035v1&entry.124074799=Read"},
{"title": "UniConvNet: Expanding Effective Receptive Field while Maintaining\n  Asymptotically Gaussian Distribution for ConvNets of Any Scale", "author": "Yuhao Wang and Wei Xi", "abstract": "  Convolutional neural networks (ConvNets) with large effective receptive field\n(ERF), still in their early stages, have demonstrated promising effectiveness\nwhile constrained by high parameters and FLOPs costs and disrupted\nasymptotically Gaussian distribution (AGD) of ERF. This paper proposes an\nalternative paradigm: rather than merely employing extremely large ERF, it is\nmore effective and efficient to expand the ERF while maintaining AGD of ERF by\nproper combination of smaller kernels, such as $7\\times{7}$, $9\\times{9}$,\n$11\\times{11}$. This paper introduces a Three-layer Receptive Field Aggregator\nand designs a Layer Operator as the fundamental operator from the perspective\nof receptive field. The ERF can be expanded to the level of existing\nlarge-kernel ConvNets through the stack of proposed modules while maintaining\nAGD of ERF. Using these designs, we propose a universal model for ConvNet of\nany scale, termed UniConvNet. Extensive experiments on ImageNet-1K, COCO2017,\nand ADE20K demonstrate that UniConvNet outperforms state-of-the-art CNNs and\nViTs across various vision recognition tasks for both lightweight and\nlarge-scale models with comparable throughput. Surprisingly, UniConvNet-T\nachieves $84.2\\%$ ImageNet top-1 accuracy with $30M$ parameters and $5.1G$\nFLOPs. UniConvNet-XL also shows competitive scalability to big data and large\nmodels, acquiring $88.4\\%$ top-1 accuracy on ImageNet. Code and models are\npublicly available at https://github.com/ai-paperwithcode/UniConvNet.\n", "link": "http://arxiv.org/abs/2508.09000v1", "date": "2025-08-12", "relevancy": 1.5884, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5507}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5241}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniConvNet%3A%20Expanding%20Effective%20Receptive%20Field%20while%20Maintaining%0A%20%20Asymptotically%20Gaussian%20Distribution%20for%20ConvNets%20of%20Any%20Scale&body=Title%3A%20UniConvNet%3A%20Expanding%20Effective%20Receptive%20Field%20while%20Maintaining%0A%20%20Asymptotically%20Gaussian%20Distribution%20for%20ConvNets%20of%20Any%20Scale%0AAuthor%3A%20Yuhao%20Wang%20and%20Wei%20Xi%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28ConvNets%29%20with%20large%20effective%20receptive%20field%0A%28ERF%29%2C%20still%20in%20their%20early%20stages%2C%20have%20demonstrated%20promising%20effectiveness%0Awhile%20constrained%20by%20high%20parameters%20and%20FLOPs%20costs%20and%20disrupted%0Aasymptotically%20Gaussian%20distribution%20%28AGD%29%20of%20ERF.%20This%20paper%20proposes%20an%0Aalternative%20paradigm%3A%20rather%20than%20merely%20employing%20extremely%20large%20ERF%2C%20it%20is%0Amore%20effective%20and%20efficient%20to%20expand%20the%20ERF%20while%20maintaining%20AGD%20of%20ERF%20by%0Aproper%20combination%20of%20smaller%20kernels%2C%20such%20as%20%247%5Ctimes%7B7%7D%24%2C%20%249%5Ctimes%7B9%7D%24%2C%0A%2411%5Ctimes%7B11%7D%24.%20This%20paper%20introduces%20a%20Three-layer%20Receptive%20Field%20Aggregator%0Aand%20designs%20a%20Layer%20Operator%20as%20the%20fundamental%20operator%20from%20the%20perspective%0Aof%20receptive%20field.%20The%20ERF%20can%20be%20expanded%20to%20the%20level%20of%20existing%0Alarge-kernel%20ConvNets%20through%20the%20stack%20of%20proposed%20modules%20while%20maintaining%0AAGD%20of%20ERF.%20Using%20these%20designs%2C%20we%20propose%20a%20universal%20model%20for%20ConvNet%20of%0Aany%20scale%2C%20termed%20UniConvNet.%20Extensive%20experiments%20on%20ImageNet-1K%2C%20COCO2017%2C%0Aand%20ADE20K%20demonstrate%20that%20UniConvNet%20outperforms%20state-of-the-art%20CNNs%20and%0AViTs%20across%20various%20vision%20recognition%20tasks%20for%20both%20lightweight%20and%0Alarge-scale%20models%20with%20comparable%20throughput.%20Surprisingly%2C%20UniConvNet-T%0Aachieves%20%2484.2%5C%25%24%20ImageNet%20top-1%20accuracy%20with%20%2430M%24%20parameters%20and%20%245.1G%24%0AFLOPs.%20UniConvNet-XL%20also%20shows%20competitive%20scalability%20to%20big%20data%20and%20large%0Amodels%2C%20acquiring%20%2488.4%5C%25%24%20top-1%20accuracy%20on%20ImageNet.%20Code%20and%20models%20are%0Apublicly%20available%20at%20https%3A//github.com/ai-paperwithcode/UniConvNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniConvNet%253A%2520Expanding%2520Effective%2520Receptive%2520Field%2520while%2520Maintaining%250A%2520%2520Asymptotically%2520Gaussian%2520Distribution%2520for%2520ConvNets%2520of%2520Any%2520Scale%26entry.906535625%3DYuhao%2520Wang%2520and%2520Wei%2520Xi%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528ConvNets%2529%2520with%2520large%2520effective%2520receptive%2520field%250A%2528ERF%2529%252C%2520still%2520in%2520their%2520early%2520stages%252C%2520have%2520demonstrated%2520promising%2520effectiveness%250Awhile%2520constrained%2520by%2520high%2520parameters%2520and%2520FLOPs%2520costs%2520and%2520disrupted%250Aasymptotically%2520Gaussian%2520distribution%2520%2528AGD%2529%2520of%2520ERF.%2520This%2520paper%2520proposes%2520an%250Aalternative%2520paradigm%253A%2520rather%2520than%2520merely%2520employing%2520extremely%2520large%2520ERF%252C%2520it%2520is%250Amore%2520effective%2520and%2520efficient%2520to%2520expand%2520the%2520ERF%2520while%2520maintaining%2520AGD%2520of%2520ERF%2520by%250Aproper%2520combination%2520of%2520smaller%2520kernels%252C%2520such%2520as%2520%25247%255Ctimes%257B7%257D%2524%252C%2520%25249%255Ctimes%257B9%257D%2524%252C%250A%252411%255Ctimes%257B11%257D%2524.%2520This%2520paper%2520introduces%2520a%2520Three-layer%2520Receptive%2520Field%2520Aggregator%250Aand%2520designs%2520a%2520Layer%2520Operator%2520as%2520the%2520fundamental%2520operator%2520from%2520the%2520perspective%250Aof%2520receptive%2520field.%2520The%2520ERF%2520can%2520be%2520expanded%2520to%2520the%2520level%2520of%2520existing%250Alarge-kernel%2520ConvNets%2520through%2520the%2520stack%2520of%2520proposed%2520modules%2520while%2520maintaining%250AAGD%2520of%2520ERF.%2520Using%2520these%2520designs%252C%2520we%2520propose%2520a%2520universal%2520model%2520for%2520ConvNet%2520of%250Aany%2520scale%252C%2520termed%2520UniConvNet.%2520Extensive%2520experiments%2520on%2520ImageNet-1K%252C%2520COCO2017%252C%250Aand%2520ADE20K%2520demonstrate%2520that%2520UniConvNet%2520outperforms%2520state-of-the-art%2520CNNs%2520and%250AViTs%2520across%2520various%2520vision%2520recognition%2520tasks%2520for%2520both%2520lightweight%2520and%250Alarge-scale%2520models%2520with%2520comparable%2520throughput.%2520Surprisingly%252C%2520UniConvNet-T%250Aachieves%2520%252484.2%255C%2525%2524%2520ImageNet%2520top-1%2520accuracy%2520with%2520%252430M%2524%2520parameters%2520and%2520%25245.1G%2524%250AFLOPs.%2520UniConvNet-XL%2520also%2520shows%2520competitive%2520scalability%2520to%2520big%2520data%2520and%2520large%250Amodels%252C%2520acquiring%2520%252488.4%255C%2525%2524%2520top-1%2520accuracy%2520on%2520ImageNet.%2520Code%2520and%2520models%2520are%250Apublicly%2520available%2520at%2520https%253A//github.com/ai-paperwithcode/UniConvNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniConvNet%3A%20Expanding%20Effective%20Receptive%20Field%20while%20Maintaining%0A%20%20Asymptotically%20Gaussian%20Distribution%20for%20ConvNets%20of%20Any%20Scale&entry.906535625=Yuhao%20Wang%20and%20Wei%20Xi&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28ConvNets%29%20with%20large%20effective%20receptive%20field%0A%28ERF%29%2C%20still%20in%20their%20early%20stages%2C%20have%20demonstrated%20promising%20effectiveness%0Awhile%20constrained%20by%20high%20parameters%20and%20FLOPs%20costs%20and%20disrupted%0Aasymptotically%20Gaussian%20distribution%20%28AGD%29%20of%20ERF.%20This%20paper%20proposes%20an%0Aalternative%20paradigm%3A%20rather%20than%20merely%20employing%20extremely%20large%20ERF%2C%20it%20is%0Amore%20effective%20and%20efficient%20to%20expand%20the%20ERF%20while%20maintaining%20AGD%20of%20ERF%20by%0Aproper%20combination%20of%20smaller%20kernels%2C%20such%20as%20%247%5Ctimes%7B7%7D%24%2C%20%249%5Ctimes%7B9%7D%24%2C%0A%2411%5Ctimes%7B11%7D%24.%20This%20paper%20introduces%20a%20Three-layer%20Receptive%20Field%20Aggregator%0Aand%20designs%20a%20Layer%20Operator%20as%20the%20fundamental%20operator%20from%20the%20perspective%0Aof%20receptive%20field.%20The%20ERF%20can%20be%20expanded%20to%20the%20level%20of%20existing%0Alarge-kernel%20ConvNets%20through%20the%20stack%20of%20proposed%20modules%20while%20maintaining%0AAGD%20of%20ERF.%20Using%20these%20designs%2C%20we%20propose%20a%20universal%20model%20for%20ConvNet%20of%0Aany%20scale%2C%20termed%20UniConvNet.%20Extensive%20experiments%20on%20ImageNet-1K%2C%20COCO2017%2C%0Aand%20ADE20K%20demonstrate%20that%20UniConvNet%20outperforms%20state-of-the-art%20CNNs%20and%0AViTs%20across%20various%20vision%20recognition%20tasks%20for%20both%20lightweight%20and%0Alarge-scale%20models%20with%20comparable%20throughput.%20Surprisingly%2C%20UniConvNet-T%0Aachieves%20%2484.2%5C%25%24%20ImageNet%20top-1%20accuracy%20with%20%2430M%24%20parameters%20and%20%245.1G%24%0AFLOPs.%20UniConvNet-XL%20also%20shows%20competitive%20scalability%20to%20big%20data%20and%20large%0Amodels%2C%20acquiring%20%2488.4%5C%25%24%20top-1%20accuracy%20on%20ImageNet.%20Code%20and%20models%20are%0Apublicly%20available%20at%20https%3A//github.com/ai-paperwithcode/UniConvNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09000v1&entry.124074799=Read"},
{"title": "MechaFormer: Sequence Learning for Kinematic Mechanism Design Automation", "author": "Diana Bolanos and Mohammadmehdi Ataei and Pradeep Kumar Jayaraman", "abstract": "  Designing mechanical mechanisms to trace specific paths is a classic yet\nnotoriously difficult engineering problem, characterized by a vast and complex\nsearch space of discrete topologies and continuous parameters. We introduce\nMechaFormer, a Transformer-based model that tackles this challenge by treating\nmechanism design as a conditional sequence generation task. Our model learns to\ntranslate a target curve into a domain-specific language (DSL) string,\nsimultaneously determining the mechanism's topology and geometric parameters in\na single, unified process. MechaFormer significantly outperforms existing\nbaselines, achieving state-of-the-art path-matching accuracy and generating a\nwide diversity of novel and valid designs. We demonstrate a suite of sampling\nstrategies that can dramatically improve solution quality and offer designers\nvaluable flexibility. Furthermore, we show that the high-quality outputs from\nMechaFormer serve as excellent starting points for traditional optimizers,\ncreating a hybrid approach that finds superior solutions with remarkable\nefficiency.\n", "link": "http://arxiv.org/abs/2508.09005v1", "date": "2025-08-12", "relevancy": 1.5722, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5452}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.523}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MechaFormer%3A%20Sequence%20Learning%20for%20Kinematic%20Mechanism%20Design%20Automation&body=Title%3A%20MechaFormer%3A%20Sequence%20Learning%20for%20Kinematic%20Mechanism%20Design%20Automation%0AAuthor%3A%20Diana%20Bolanos%20and%20Mohammadmehdi%20Ataei%20and%20Pradeep%20Kumar%20Jayaraman%0AAbstract%3A%20%20%20Designing%20mechanical%20mechanisms%20to%20trace%20specific%20paths%20is%20a%20classic%20yet%0Anotoriously%20difficult%20engineering%20problem%2C%20characterized%20by%20a%20vast%20and%20complex%0Asearch%20space%20of%20discrete%20topologies%20and%20continuous%20parameters.%20We%20introduce%0AMechaFormer%2C%20a%20Transformer-based%20model%20that%20tackles%20this%20challenge%20by%20treating%0Amechanism%20design%20as%20a%20conditional%20sequence%20generation%20task.%20Our%20model%20learns%20to%0Atranslate%20a%20target%20curve%20into%20a%20domain-specific%20language%20%28DSL%29%20string%2C%0Asimultaneously%20determining%20the%20mechanism%27s%20topology%20and%20geometric%20parameters%20in%0Aa%20single%2C%20unified%20process.%20MechaFormer%20significantly%20outperforms%20existing%0Abaselines%2C%20achieving%20state-of-the-art%20path-matching%20accuracy%20and%20generating%20a%0Awide%20diversity%20of%20novel%20and%20valid%20designs.%20We%20demonstrate%20a%20suite%20of%20sampling%0Astrategies%20that%20can%20dramatically%20improve%20solution%20quality%20and%20offer%20designers%0Avaluable%20flexibility.%20Furthermore%2C%20we%20show%20that%20the%20high-quality%20outputs%20from%0AMechaFormer%20serve%20as%20excellent%20starting%20points%20for%20traditional%20optimizers%2C%0Acreating%20a%20hybrid%20approach%20that%20finds%20superior%20solutions%20with%20remarkable%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMechaFormer%253A%2520Sequence%2520Learning%2520for%2520Kinematic%2520Mechanism%2520Design%2520Automation%26entry.906535625%3DDiana%2520Bolanos%2520and%2520Mohammadmehdi%2520Ataei%2520and%2520Pradeep%2520Kumar%2520Jayaraman%26entry.1292438233%3D%2520%2520Designing%2520mechanical%2520mechanisms%2520to%2520trace%2520specific%2520paths%2520is%2520a%2520classic%2520yet%250Anotoriously%2520difficult%2520engineering%2520problem%252C%2520characterized%2520by%2520a%2520vast%2520and%2520complex%250Asearch%2520space%2520of%2520discrete%2520topologies%2520and%2520continuous%2520parameters.%2520We%2520introduce%250AMechaFormer%252C%2520a%2520Transformer-based%2520model%2520that%2520tackles%2520this%2520challenge%2520by%2520treating%250Amechanism%2520design%2520as%2520a%2520conditional%2520sequence%2520generation%2520task.%2520Our%2520model%2520learns%2520to%250Atranslate%2520a%2520target%2520curve%2520into%2520a%2520domain-specific%2520language%2520%2528DSL%2529%2520string%252C%250Asimultaneously%2520determining%2520the%2520mechanism%2527s%2520topology%2520and%2520geometric%2520parameters%2520in%250Aa%2520single%252C%2520unified%2520process.%2520MechaFormer%2520significantly%2520outperforms%2520existing%250Abaselines%252C%2520achieving%2520state-of-the-art%2520path-matching%2520accuracy%2520and%2520generating%2520a%250Awide%2520diversity%2520of%2520novel%2520and%2520valid%2520designs.%2520We%2520demonstrate%2520a%2520suite%2520of%2520sampling%250Astrategies%2520that%2520can%2520dramatically%2520improve%2520solution%2520quality%2520and%2520offer%2520designers%250Avaluable%2520flexibility.%2520Furthermore%252C%2520we%2520show%2520that%2520the%2520high-quality%2520outputs%2520from%250AMechaFormer%2520serve%2520as%2520excellent%2520starting%2520points%2520for%2520traditional%2520optimizers%252C%250Acreating%2520a%2520hybrid%2520approach%2520that%2520finds%2520superior%2520solutions%2520with%2520remarkable%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MechaFormer%3A%20Sequence%20Learning%20for%20Kinematic%20Mechanism%20Design%20Automation&entry.906535625=Diana%20Bolanos%20and%20Mohammadmehdi%20Ataei%20and%20Pradeep%20Kumar%20Jayaraman&entry.1292438233=%20%20Designing%20mechanical%20mechanisms%20to%20trace%20specific%20paths%20is%20a%20classic%20yet%0Anotoriously%20difficult%20engineering%20problem%2C%20characterized%20by%20a%20vast%20and%20complex%0Asearch%20space%20of%20discrete%20topologies%20and%20continuous%20parameters.%20We%20introduce%0AMechaFormer%2C%20a%20Transformer-based%20model%20that%20tackles%20this%20challenge%20by%20treating%0Amechanism%20design%20as%20a%20conditional%20sequence%20generation%20task.%20Our%20model%20learns%20to%0Atranslate%20a%20target%20curve%20into%20a%20domain-specific%20language%20%28DSL%29%20string%2C%0Asimultaneously%20determining%20the%20mechanism%27s%20topology%20and%20geometric%20parameters%20in%0Aa%20single%2C%20unified%20process.%20MechaFormer%20significantly%20outperforms%20existing%0Abaselines%2C%20achieving%20state-of-the-art%20path-matching%20accuracy%20and%20generating%20a%0Awide%20diversity%20of%20novel%20and%20valid%20designs.%20We%20demonstrate%20a%20suite%20of%20sampling%0Astrategies%20that%20can%20dramatically%20improve%20solution%20quality%20and%20offer%20designers%0Avaluable%20flexibility.%20Furthermore%2C%20we%20show%20that%20the%20high-quality%20outputs%20from%0AMechaFormer%20serve%20as%20excellent%20starting%20points%20for%20traditional%20optimizers%2C%0Acreating%20a%20hybrid%20approach%20that%20finds%20superior%20solutions%20with%20remarkable%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09005v1&entry.124074799=Read"},
{"title": "From Lab to Field: Real-World Evaluation of an AI-Driven Smart Video\n  Solution to Enhance Community Safety", "author": "Shanle Yao and Babak Rahimi Ardabili and Armin Danesh Pazho and Ghazal Alinezhad Noghre and Christopher Neff and Lauren Bourque and Hamed Tabkhi", "abstract": "  This article adopts and evaluates an AI-enabled Smart Video Solution (SVS)\ndesigned to enhance safety in the real world. The system integrates with\nexisting infrastructure camera networks, leveraging recent advancements in AI\nfor easy adoption. Prioritizing privacy and ethical standards, pose based data\nis used for downstream AI tasks such as anomaly detection. Cloud-based\ninfrastructure and mobile app are deployed, enabling real-time alerts within\ncommunities. The SVS employs innovative data representation and visualization\ntechniques, such as the Occupancy Indicator, Statistical Anomaly Detection,\nBird's Eye View, and Heatmaps, to understand pedestrian behaviors and enhance\npublic safety. Evaluation of the SVS demonstrates its capacity to convert\ncomplex computer vision outputs into actionable insights for stakeholders,\ncommunity partners, law enforcement, urban planners, and social scientists.\nThis article presents a comprehensive real-world deployment and evaluation of\nthe SVS, implemented in a community college environment across 16 cameras. The\nsystem integrates AI-driven visual processing, supported by statistical\nanalysis, database management, cloud communication, and user notifications.\nAdditionally, the article evaluates the end-to-end latency from the moment an\nAI algorithm detects anomalous behavior in real-time at the camera level to the\ntime stakeholders receive a notification. The results demonstrate the system's\nrobustness, effectively managing 16 CCTV cameras with a consistent throughput\nof 16.5 frames per second (FPS) over a 21-hour period and an average end-to-end\nlatency of 26.76 seconds between anomaly detection and alert issuance.\n", "link": "http://arxiv.org/abs/2312.02078v3", "date": "2025-08-12", "relevancy": 1.5717, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.528}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5254}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Lab%20to%20Field%3A%20Real-World%20Evaluation%20of%20an%20AI-Driven%20Smart%20Video%0A%20%20Solution%20to%20Enhance%20Community%20Safety&body=Title%3A%20From%20Lab%20to%20Field%3A%20Real-World%20Evaluation%20of%20an%20AI-Driven%20Smart%20Video%0A%20%20Solution%20to%20Enhance%20Community%20Safety%0AAuthor%3A%20Shanle%20Yao%20and%20Babak%20Rahimi%20Ardabili%20and%20Armin%20Danesh%20Pazho%20and%20Ghazal%20Alinezhad%20Noghre%20and%20Christopher%20Neff%20and%20Lauren%20Bourque%20and%20Hamed%20Tabkhi%0AAbstract%3A%20%20%20This%20article%20adopts%20and%20evaluates%20an%20AI-enabled%20Smart%20Video%20Solution%20%28SVS%29%0Adesigned%20to%20enhance%20safety%20in%20the%20real%20world.%20The%20system%20integrates%20with%0Aexisting%20infrastructure%20camera%20networks%2C%20leveraging%20recent%20advancements%20in%20AI%0Afor%20easy%20adoption.%20Prioritizing%20privacy%20and%20ethical%20standards%2C%20pose%20based%20data%0Ais%20used%20for%20downstream%20AI%20tasks%20such%20as%20anomaly%20detection.%20Cloud-based%0Ainfrastructure%20and%20mobile%20app%20are%20deployed%2C%20enabling%20real-time%20alerts%20within%0Acommunities.%20The%20SVS%20employs%20innovative%20data%20representation%20and%20visualization%0Atechniques%2C%20such%20as%20the%20Occupancy%20Indicator%2C%20Statistical%20Anomaly%20Detection%2C%0ABird%27s%20Eye%20View%2C%20and%20Heatmaps%2C%20to%20understand%20pedestrian%20behaviors%20and%20enhance%0Apublic%20safety.%20Evaluation%20of%20the%20SVS%20demonstrates%20its%20capacity%20to%20convert%0Acomplex%20computer%20vision%20outputs%20into%20actionable%20insights%20for%20stakeholders%2C%0Acommunity%20partners%2C%20law%20enforcement%2C%20urban%20planners%2C%20and%20social%20scientists.%0AThis%20article%20presents%20a%20comprehensive%20real-world%20deployment%20and%20evaluation%20of%0Athe%20SVS%2C%20implemented%20in%20a%20community%20college%20environment%20across%2016%20cameras.%20The%0Asystem%20integrates%20AI-driven%20visual%20processing%2C%20supported%20by%20statistical%0Aanalysis%2C%20database%20management%2C%20cloud%20communication%2C%20and%20user%20notifications.%0AAdditionally%2C%20the%20article%20evaluates%20the%20end-to-end%20latency%20from%20the%20moment%20an%0AAI%20algorithm%20detects%20anomalous%20behavior%20in%20real-time%20at%20the%20camera%20level%20to%20the%0Atime%20stakeholders%20receive%20a%20notification.%20The%20results%20demonstrate%20the%20system%27s%0Arobustness%2C%20effectively%20managing%2016%20CCTV%20cameras%20with%20a%20consistent%20throughput%0Aof%2016.5%20frames%20per%20second%20%28FPS%29%20over%20a%2021-hour%20period%20and%20an%20average%20end-to-end%0Alatency%20of%2026.76%20seconds%20between%20anomaly%20detection%20and%20alert%20issuance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02078v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Lab%2520to%2520Field%253A%2520Real-World%2520Evaluation%2520of%2520an%2520AI-Driven%2520Smart%2520Video%250A%2520%2520Solution%2520to%2520Enhance%2520Community%2520Safety%26entry.906535625%3DShanle%2520Yao%2520and%2520Babak%2520Rahimi%2520Ardabili%2520and%2520Armin%2520Danesh%2520Pazho%2520and%2520Ghazal%2520Alinezhad%2520Noghre%2520and%2520Christopher%2520Neff%2520and%2520Lauren%2520Bourque%2520and%2520Hamed%2520Tabkhi%26entry.1292438233%3D%2520%2520This%2520article%2520adopts%2520and%2520evaluates%2520an%2520AI-enabled%2520Smart%2520Video%2520Solution%2520%2528SVS%2529%250Adesigned%2520to%2520enhance%2520safety%2520in%2520the%2520real%2520world.%2520The%2520system%2520integrates%2520with%250Aexisting%2520infrastructure%2520camera%2520networks%252C%2520leveraging%2520recent%2520advancements%2520in%2520AI%250Afor%2520easy%2520adoption.%2520Prioritizing%2520privacy%2520and%2520ethical%2520standards%252C%2520pose%2520based%2520data%250Ais%2520used%2520for%2520downstream%2520AI%2520tasks%2520such%2520as%2520anomaly%2520detection.%2520Cloud-based%250Ainfrastructure%2520and%2520mobile%2520app%2520are%2520deployed%252C%2520enabling%2520real-time%2520alerts%2520within%250Acommunities.%2520The%2520SVS%2520employs%2520innovative%2520data%2520representation%2520and%2520visualization%250Atechniques%252C%2520such%2520as%2520the%2520Occupancy%2520Indicator%252C%2520Statistical%2520Anomaly%2520Detection%252C%250ABird%2527s%2520Eye%2520View%252C%2520and%2520Heatmaps%252C%2520to%2520understand%2520pedestrian%2520behaviors%2520and%2520enhance%250Apublic%2520safety.%2520Evaluation%2520of%2520the%2520SVS%2520demonstrates%2520its%2520capacity%2520to%2520convert%250Acomplex%2520computer%2520vision%2520outputs%2520into%2520actionable%2520insights%2520for%2520stakeholders%252C%250Acommunity%2520partners%252C%2520law%2520enforcement%252C%2520urban%2520planners%252C%2520and%2520social%2520scientists.%250AThis%2520article%2520presents%2520a%2520comprehensive%2520real-world%2520deployment%2520and%2520evaluation%2520of%250Athe%2520SVS%252C%2520implemented%2520in%2520a%2520community%2520college%2520environment%2520across%252016%2520cameras.%2520The%250Asystem%2520integrates%2520AI-driven%2520visual%2520processing%252C%2520supported%2520by%2520statistical%250Aanalysis%252C%2520database%2520management%252C%2520cloud%2520communication%252C%2520and%2520user%2520notifications.%250AAdditionally%252C%2520the%2520article%2520evaluates%2520the%2520end-to-end%2520latency%2520from%2520the%2520moment%2520an%250AAI%2520algorithm%2520detects%2520anomalous%2520behavior%2520in%2520real-time%2520at%2520the%2520camera%2520level%2520to%2520the%250Atime%2520stakeholders%2520receive%2520a%2520notification.%2520The%2520results%2520demonstrate%2520the%2520system%2527s%250Arobustness%252C%2520effectively%2520managing%252016%2520CCTV%2520cameras%2520with%2520a%2520consistent%2520throughput%250Aof%252016.5%2520frames%2520per%2520second%2520%2528FPS%2529%2520over%2520a%252021-hour%2520period%2520and%2520an%2520average%2520end-to-end%250Alatency%2520of%252026.76%2520seconds%2520between%2520anomaly%2520detection%2520and%2520alert%2520issuance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02078v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Lab%20to%20Field%3A%20Real-World%20Evaluation%20of%20an%20AI-Driven%20Smart%20Video%0A%20%20Solution%20to%20Enhance%20Community%20Safety&entry.906535625=Shanle%20Yao%20and%20Babak%20Rahimi%20Ardabili%20and%20Armin%20Danesh%20Pazho%20and%20Ghazal%20Alinezhad%20Noghre%20and%20Christopher%20Neff%20and%20Lauren%20Bourque%20and%20Hamed%20Tabkhi&entry.1292438233=%20%20This%20article%20adopts%20and%20evaluates%20an%20AI-enabled%20Smart%20Video%20Solution%20%28SVS%29%0Adesigned%20to%20enhance%20safety%20in%20the%20real%20world.%20The%20system%20integrates%20with%0Aexisting%20infrastructure%20camera%20networks%2C%20leveraging%20recent%20advancements%20in%20AI%0Afor%20easy%20adoption.%20Prioritizing%20privacy%20and%20ethical%20standards%2C%20pose%20based%20data%0Ais%20used%20for%20downstream%20AI%20tasks%20such%20as%20anomaly%20detection.%20Cloud-based%0Ainfrastructure%20and%20mobile%20app%20are%20deployed%2C%20enabling%20real-time%20alerts%20within%0Acommunities.%20The%20SVS%20employs%20innovative%20data%20representation%20and%20visualization%0Atechniques%2C%20such%20as%20the%20Occupancy%20Indicator%2C%20Statistical%20Anomaly%20Detection%2C%0ABird%27s%20Eye%20View%2C%20and%20Heatmaps%2C%20to%20understand%20pedestrian%20behaviors%20and%20enhance%0Apublic%20safety.%20Evaluation%20of%20the%20SVS%20demonstrates%20its%20capacity%20to%20convert%0Acomplex%20computer%20vision%20outputs%20into%20actionable%20insights%20for%20stakeholders%2C%0Acommunity%20partners%2C%20law%20enforcement%2C%20urban%20planners%2C%20and%20social%20scientists.%0AThis%20article%20presents%20a%20comprehensive%20real-world%20deployment%20and%20evaluation%20of%0Athe%20SVS%2C%20implemented%20in%20a%20community%20college%20environment%20across%2016%20cameras.%20The%0Asystem%20integrates%20AI-driven%20visual%20processing%2C%20supported%20by%20statistical%0Aanalysis%2C%20database%20management%2C%20cloud%20communication%2C%20and%20user%20notifications.%0AAdditionally%2C%20the%20article%20evaluates%20the%20end-to-end%20latency%20from%20the%20moment%20an%0AAI%20algorithm%20detects%20anomalous%20behavior%20in%20real-time%20at%20the%20camera%20level%20to%20the%0Atime%20stakeholders%20receive%20a%20notification.%20The%20results%20demonstrate%20the%20system%27s%0Arobustness%2C%20effectively%20managing%2016%20CCTV%20cameras%20with%20a%20consistent%20throughput%0Aof%2016.5%20frames%20per%20second%20%28FPS%29%20over%20a%2021-hour%20period%20and%20an%20average%20end-to-end%0Alatency%20of%2026.76%20seconds%20between%20anomaly%20detection%20and%20alert%20issuance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02078v3&entry.124074799=Read"},
{"title": "Retrieval-Augmented Generation with Conflicting Evidence", "author": "Han Wang and Archiki Prasad and Elias Stengel-Eskin and Mohit Bansal", "abstract": "  Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.\n", "link": "http://arxiv.org/abs/2504.13079v2", "date": "2025-08-12", "relevancy": 1.565, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5588}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5208}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval-Augmented%20Generation%20with%20Conflicting%20Evidence&body=Title%3A%20Retrieval-Augmented%20Generation%20with%20Conflicting%20Evidence%0AAuthor%3A%20Han%20Wang%20and%20Archiki%20Prasad%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Large%20language%20model%20%28LLM%29%20agents%20are%20increasingly%20employing%0Aretrieval-augmented%20generation%20%28RAG%29%20to%20improve%20the%20factuality%20of%20their%0Aresponses.%20However%2C%20in%20practice%2C%20these%20systems%20often%20need%20to%20handle%20ambiguous%0Auser%20queries%20and%20potentially%20conflicting%20information%20from%20multiple%20sources%0Awhile%20also%20suppressing%20inaccurate%20information%20from%20noisy%20or%20irrelevant%0Adocuments.%20Prior%20work%20has%20generally%20studied%20and%20addressed%20these%20challenges%20in%0Aisolation%2C%20considering%20only%20one%20aspect%20at%20a%20time%2C%20such%20as%20handling%20ambiguity%20or%0Arobustness%20to%20noise%20and%20misinformation.%20We%20instead%20consider%20multiple%20factors%0Asimultaneously%2C%20proposing%20%28i%29%20RAMDocs%20%28Retrieval%20with%20Ambiguity%20and%0AMisinformation%20in%20Documents%29%2C%20a%20new%20dataset%20that%20simulates%20complex%20and%0Arealistic%20scenarios%20for%20conflicting%20evidence%20for%20a%20user%20query%2C%20including%0Aambiguity%2C%20misinformation%2C%20and%20noise%3B%20and%20%28ii%29%20MADAM-RAG%2C%20a%20multi-agent%0Aapproach%20in%20which%20LLM%20agents%20debate%20over%20the%20merits%20of%20an%20answer%20over%20multiple%0Arounds%2C%20allowing%20an%20aggregator%20to%20collate%20responses%20corresponding%20to%0Adisambiguated%20entities%20while%20discarding%20misinformation%20and%20noise%2C%20thereby%0Ahandling%20diverse%20sources%20of%20conflict%20jointly.%20We%20demonstrate%20the%20effectiveness%0Aof%20MADAM-RAG%20using%20both%20closed%20and%20open-source%20models%20on%20AmbigDocs%20--%20which%0Arequires%20presenting%20all%20valid%20answers%20for%20ambiguous%20queries%20--%20improving%20over%0Astrong%20RAG%20baselines%20by%20up%20to%2011.40%25%20and%20on%20FaithEval%20--%20which%20requires%0Asuppressing%20misinformation%20--%20where%20we%20improve%20by%20up%20to%2015.80%25%20%28absolute%29%20with%0ALlama3.3-70B-Instruct.%20Furthermore%2C%20we%20find%20that%20RAMDocs%20poses%20a%20challenge%20for%0Aexisting%20RAG%20baselines%20%28Llama3.3-70B-Instruct%20only%20obtains%2032.60%20exact%20match%0Ascore%29.%20While%20MADAM-RAG%20begins%20to%20address%20these%20conflicting%20factors%2C%20our%0Aanalysis%20indicates%20that%20a%20substantial%20gap%20remains%20especially%20when%20increasing%0Athe%20level%20of%20imbalance%20in%20supporting%20evidence%20and%20misinformation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13079v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval-Augmented%2520Generation%2520with%2520Conflicting%2520Evidence%26entry.906535625%3DHan%2520Wang%2520and%2520Archiki%2520Prasad%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Large%2520language%2520model%2520%2528LLM%2529%2520agents%2520are%2520increasingly%2520employing%250Aretrieval-augmented%2520generation%2520%2528RAG%2529%2520to%2520improve%2520the%2520factuality%2520of%2520their%250Aresponses.%2520However%252C%2520in%2520practice%252C%2520these%2520systems%2520often%2520need%2520to%2520handle%2520ambiguous%250Auser%2520queries%2520and%2520potentially%2520conflicting%2520information%2520from%2520multiple%2520sources%250Awhile%2520also%2520suppressing%2520inaccurate%2520information%2520from%2520noisy%2520or%2520irrelevant%250Adocuments.%2520Prior%2520work%2520has%2520generally%2520studied%2520and%2520addressed%2520these%2520challenges%2520in%250Aisolation%252C%2520considering%2520only%2520one%2520aspect%2520at%2520a%2520time%252C%2520such%2520as%2520handling%2520ambiguity%2520or%250Arobustness%2520to%2520noise%2520and%2520misinformation.%2520We%2520instead%2520consider%2520multiple%2520factors%250Asimultaneously%252C%2520proposing%2520%2528i%2529%2520RAMDocs%2520%2528Retrieval%2520with%2520Ambiguity%2520and%250AMisinformation%2520in%2520Documents%2529%252C%2520a%2520new%2520dataset%2520that%2520simulates%2520complex%2520and%250Arealistic%2520scenarios%2520for%2520conflicting%2520evidence%2520for%2520a%2520user%2520query%252C%2520including%250Aambiguity%252C%2520misinformation%252C%2520and%2520noise%253B%2520and%2520%2528ii%2529%2520MADAM-RAG%252C%2520a%2520multi-agent%250Aapproach%2520in%2520which%2520LLM%2520agents%2520debate%2520over%2520the%2520merits%2520of%2520an%2520answer%2520over%2520multiple%250Arounds%252C%2520allowing%2520an%2520aggregator%2520to%2520collate%2520responses%2520corresponding%2520to%250Adisambiguated%2520entities%2520while%2520discarding%2520misinformation%2520and%2520noise%252C%2520thereby%250Ahandling%2520diverse%2520sources%2520of%2520conflict%2520jointly.%2520We%2520demonstrate%2520the%2520effectiveness%250Aof%2520MADAM-RAG%2520using%2520both%2520closed%2520and%2520open-source%2520models%2520on%2520AmbigDocs%2520--%2520which%250Arequires%2520presenting%2520all%2520valid%2520answers%2520for%2520ambiguous%2520queries%2520--%2520improving%2520over%250Astrong%2520RAG%2520baselines%2520by%2520up%2520to%252011.40%2525%2520and%2520on%2520FaithEval%2520--%2520which%2520requires%250Asuppressing%2520misinformation%2520--%2520where%2520we%2520improve%2520by%2520up%2520to%252015.80%2525%2520%2528absolute%2529%2520with%250ALlama3.3-70B-Instruct.%2520Furthermore%252C%2520we%2520find%2520that%2520RAMDocs%2520poses%2520a%2520challenge%2520for%250Aexisting%2520RAG%2520baselines%2520%2528Llama3.3-70B-Instruct%2520only%2520obtains%252032.60%2520exact%2520match%250Ascore%2529.%2520While%2520MADAM-RAG%2520begins%2520to%2520address%2520these%2520conflicting%2520factors%252C%2520our%250Aanalysis%2520indicates%2520that%2520a%2520substantial%2520gap%2520remains%2520especially%2520when%2520increasing%250Athe%2520level%2520of%2520imbalance%2520in%2520supporting%2520evidence%2520and%2520misinformation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13079v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval-Augmented%20Generation%20with%20Conflicting%20Evidence&entry.906535625=Han%20Wang%20and%20Archiki%20Prasad%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal&entry.1292438233=%20%20Large%20language%20model%20%28LLM%29%20agents%20are%20increasingly%20employing%0Aretrieval-augmented%20generation%20%28RAG%29%20to%20improve%20the%20factuality%20of%20their%0Aresponses.%20However%2C%20in%20practice%2C%20these%20systems%20often%20need%20to%20handle%20ambiguous%0Auser%20queries%20and%20potentially%20conflicting%20information%20from%20multiple%20sources%0Awhile%20also%20suppressing%20inaccurate%20information%20from%20noisy%20or%20irrelevant%0Adocuments.%20Prior%20work%20has%20generally%20studied%20and%20addressed%20these%20challenges%20in%0Aisolation%2C%20considering%20only%20one%20aspect%20at%20a%20time%2C%20such%20as%20handling%20ambiguity%20or%0Arobustness%20to%20noise%20and%20misinformation.%20We%20instead%20consider%20multiple%20factors%0Asimultaneously%2C%20proposing%20%28i%29%20RAMDocs%20%28Retrieval%20with%20Ambiguity%20and%0AMisinformation%20in%20Documents%29%2C%20a%20new%20dataset%20that%20simulates%20complex%20and%0Arealistic%20scenarios%20for%20conflicting%20evidence%20for%20a%20user%20query%2C%20including%0Aambiguity%2C%20misinformation%2C%20and%20noise%3B%20and%20%28ii%29%20MADAM-RAG%2C%20a%20multi-agent%0Aapproach%20in%20which%20LLM%20agents%20debate%20over%20the%20merits%20of%20an%20answer%20over%20multiple%0Arounds%2C%20allowing%20an%20aggregator%20to%20collate%20responses%20corresponding%20to%0Adisambiguated%20entities%20while%20discarding%20misinformation%20and%20noise%2C%20thereby%0Ahandling%20diverse%20sources%20of%20conflict%20jointly.%20We%20demonstrate%20the%20effectiveness%0Aof%20MADAM-RAG%20using%20both%20closed%20and%20open-source%20models%20on%20AmbigDocs%20--%20which%0Arequires%20presenting%20all%20valid%20answers%20for%20ambiguous%20queries%20--%20improving%20over%0Astrong%20RAG%20baselines%20by%20up%20to%2011.40%25%20and%20on%20FaithEval%20--%20which%20requires%0Asuppressing%20misinformation%20--%20where%20we%20improve%20by%20up%20to%2015.80%25%20%28absolute%29%20with%0ALlama3.3-70B-Instruct.%20Furthermore%2C%20we%20find%20that%20RAMDocs%20poses%20a%20challenge%20for%0Aexisting%20RAG%20baselines%20%28Llama3.3-70B-Instruct%20only%20obtains%2032.60%20exact%20match%0Ascore%29.%20While%20MADAM-RAG%20begins%20to%20address%20these%20conflicting%20factors%2C%20our%0Aanalysis%20indicates%20that%20a%20substantial%20gap%20remains%20especially%20when%20increasing%0Athe%20level%20of%20imbalance%20in%20supporting%20evidence%20and%20misinformation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13079v2&entry.124074799=Read"},
{"title": "Opioid Named Entity Recognition (ONER-2025) from Reddit", "author": "Muhammad Ahmad and Rita Orji and Fida Ullah and Ildar Batyrshin and Grigori Sidorov", "abstract": "  The opioid overdose epidemic remains a critical public health crisis,\nparticularly in the United States, leading to significant mortality and\nsocietal costs. Social media platforms like Reddit provide vast amounts of\nunstructured data that offer insights into public perceptions, discussions, and\nexperiences related to opioid use. This study leverages Natural Language\nProcessing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to\nextract actionable information from these platforms. Our research makes four\nkey contributions. First, we created a unique, manually annotated dataset\nsourced from Reddit, where users share self-reported experiences of opioid use\nvia different administration routes. This dataset contains 331,285 tokens and\nincludes eight major opioid entity categories. Second, we detail our annotation\nprocess and guidelines while discussing the challenges of labeling the\nONER-2025 dataset. Third, we analyze key linguistic challenges, including\nslang, ambiguity, fragmented sentences, and emotionally charged language, in\nopioid discussions. Fourth, we propose a real-time monitoring system to process\nstreaming data from social media, healthcare records, and emergency services to\nidentify overdose events. Using 5-fold cross-validation in 11 experiments, our\nsystem integrates machine learning, deep learning, and transformer-based\nlanguage models with advanced contextual embeddings to enhance understanding.\nOur transformer-based models (bert-base-NER and roberta-base) achieved 97%\naccuracy and F1-score, outperforming baselines by 10.23% (RF=0.88).\n", "link": "http://arxiv.org/abs/2504.00027v4", "date": "2025-08-12", "relevancy": 1.5216, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.433}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3699}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Opioid%20Named%20Entity%20Recognition%20%28ONER-2025%29%20from%20Reddit&body=Title%3A%20Opioid%20Named%20Entity%20Recognition%20%28ONER-2025%29%20from%20Reddit%0AAuthor%3A%20Muhammad%20Ahmad%20and%20Rita%20Orji%20and%20Fida%20Ullah%20and%20Ildar%20Batyrshin%20and%20Grigori%20Sidorov%0AAbstract%3A%20%20%20The%20opioid%20overdose%20epidemic%20remains%20a%20critical%20public%20health%20crisis%2C%0Aparticularly%20in%20the%20United%20States%2C%20leading%20to%20significant%20mortality%20and%0Asocietal%20costs.%20Social%20media%20platforms%20like%20Reddit%20provide%20vast%20amounts%20of%0Aunstructured%20data%20that%20offer%20insights%20into%20public%20perceptions%2C%20discussions%2C%20and%0Aexperiences%20related%20to%20opioid%20use.%20This%20study%20leverages%20Natural%20Language%0AProcessing%20%28NLP%29%2C%20specifically%20Opioid%20Named%20Entity%20Recognition%20%28ONER-2025%29%2C%20to%0Aextract%20actionable%20information%20from%20these%20platforms.%20Our%20research%20makes%20four%0Akey%20contributions.%20First%2C%20we%20created%20a%20unique%2C%20manually%20annotated%20dataset%0Asourced%20from%20Reddit%2C%20where%20users%20share%20self-reported%20experiences%20of%20opioid%20use%0Avia%20different%20administration%20routes.%20This%20dataset%20contains%20331%2C285%20tokens%20and%0Aincludes%20eight%20major%20opioid%20entity%20categories.%20Second%2C%20we%20detail%20our%20annotation%0Aprocess%20and%20guidelines%20while%20discussing%20the%20challenges%20of%20labeling%20the%0AONER-2025%20dataset.%20Third%2C%20we%20analyze%20key%20linguistic%20challenges%2C%20including%0Aslang%2C%20ambiguity%2C%20fragmented%20sentences%2C%20and%20emotionally%20charged%20language%2C%20in%0Aopioid%20discussions.%20Fourth%2C%20we%20propose%20a%20real-time%20monitoring%20system%20to%20process%0Astreaming%20data%20from%20social%20media%2C%20healthcare%20records%2C%20and%20emergency%20services%20to%0Aidentify%20overdose%20events.%20Using%205-fold%20cross-validation%20in%2011%20experiments%2C%20our%0Asystem%20integrates%20machine%20learning%2C%20deep%20learning%2C%20and%20transformer-based%0Alanguage%20models%20with%20advanced%20contextual%20embeddings%20to%20enhance%20understanding.%0AOur%20transformer-based%20models%20%28bert-base-NER%20and%20roberta-base%29%20achieved%2097%25%0Aaccuracy%20and%20F1-score%2C%20outperforming%20baselines%20by%2010.23%25%20%28RF%3D0.88%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.00027v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpioid%2520Named%2520Entity%2520Recognition%2520%2528ONER-2025%2529%2520from%2520Reddit%26entry.906535625%3DMuhammad%2520Ahmad%2520and%2520Rita%2520Orji%2520and%2520Fida%2520Ullah%2520and%2520Ildar%2520Batyrshin%2520and%2520Grigori%2520Sidorov%26entry.1292438233%3D%2520%2520The%2520opioid%2520overdose%2520epidemic%2520remains%2520a%2520critical%2520public%2520health%2520crisis%252C%250Aparticularly%2520in%2520the%2520United%2520States%252C%2520leading%2520to%2520significant%2520mortality%2520and%250Asocietal%2520costs.%2520Social%2520media%2520platforms%2520like%2520Reddit%2520provide%2520vast%2520amounts%2520of%250Aunstructured%2520data%2520that%2520offer%2520insights%2520into%2520public%2520perceptions%252C%2520discussions%252C%2520and%250Aexperiences%2520related%2520to%2520opioid%2520use.%2520This%2520study%2520leverages%2520Natural%2520Language%250AProcessing%2520%2528NLP%2529%252C%2520specifically%2520Opioid%2520Named%2520Entity%2520Recognition%2520%2528ONER-2025%2529%252C%2520to%250Aextract%2520actionable%2520information%2520from%2520these%2520platforms.%2520Our%2520research%2520makes%2520four%250Akey%2520contributions.%2520First%252C%2520we%2520created%2520a%2520unique%252C%2520manually%2520annotated%2520dataset%250Asourced%2520from%2520Reddit%252C%2520where%2520users%2520share%2520self-reported%2520experiences%2520of%2520opioid%2520use%250Avia%2520different%2520administration%2520routes.%2520This%2520dataset%2520contains%2520331%252C285%2520tokens%2520and%250Aincludes%2520eight%2520major%2520opioid%2520entity%2520categories.%2520Second%252C%2520we%2520detail%2520our%2520annotation%250Aprocess%2520and%2520guidelines%2520while%2520discussing%2520the%2520challenges%2520of%2520labeling%2520the%250AONER-2025%2520dataset.%2520Third%252C%2520we%2520analyze%2520key%2520linguistic%2520challenges%252C%2520including%250Aslang%252C%2520ambiguity%252C%2520fragmented%2520sentences%252C%2520and%2520emotionally%2520charged%2520language%252C%2520in%250Aopioid%2520discussions.%2520Fourth%252C%2520we%2520propose%2520a%2520real-time%2520monitoring%2520system%2520to%2520process%250Astreaming%2520data%2520from%2520social%2520media%252C%2520healthcare%2520records%252C%2520and%2520emergency%2520services%2520to%250Aidentify%2520overdose%2520events.%2520Using%25205-fold%2520cross-validation%2520in%252011%2520experiments%252C%2520our%250Asystem%2520integrates%2520machine%2520learning%252C%2520deep%2520learning%252C%2520and%2520transformer-based%250Alanguage%2520models%2520with%2520advanced%2520contextual%2520embeddings%2520to%2520enhance%2520understanding.%250AOur%2520transformer-based%2520models%2520%2528bert-base-NER%2520and%2520roberta-base%2529%2520achieved%252097%2525%250Aaccuracy%2520and%2520F1-score%252C%2520outperforming%2520baselines%2520by%252010.23%2525%2520%2528RF%253D0.88%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.00027v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Opioid%20Named%20Entity%20Recognition%20%28ONER-2025%29%20from%20Reddit&entry.906535625=Muhammad%20Ahmad%20and%20Rita%20Orji%20and%20Fida%20Ullah%20and%20Ildar%20Batyrshin%20and%20Grigori%20Sidorov&entry.1292438233=%20%20The%20opioid%20overdose%20epidemic%20remains%20a%20critical%20public%20health%20crisis%2C%0Aparticularly%20in%20the%20United%20States%2C%20leading%20to%20significant%20mortality%20and%0Asocietal%20costs.%20Social%20media%20platforms%20like%20Reddit%20provide%20vast%20amounts%20of%0Aunstructured%20data%20that%20offer%20insights%20into%20public%20perceptions%2C%20discussions%2C%20and%0Aexperiences%20related%20to%20opioid%20use.%20This%20study%20leverages%20Natural%20Language%0AProcessing%20%28NLP%29%2C%20specifically%20Opioid%20Named%20Entity%20Recognition%20%28ONER-2025%29%2C%20to%0Aextract%20actionable%20information%20from%20these%20platforms.%20Our%20research%20makes%20four%0Akey%20contributions.%20First%2C%20we%20created%20a%20unique%2C%20manually%20annotated%20dataset%0Asourced%20from%20Reddit%2C%20where%20users%20share%20self-reported%20experiences%20of%20opioid%20use%0Avia%20different%20administration%20routes.%20This%20dataset%20contains%20331%2C285%20tokens%20and%0Aincludes%20eight%20major%20opioid%20entity%20categories.%20Second%2C%20we%20detail%20our%20annotation%0Aprocess%20and%20guidelines%20while%20discussing%20the%20challenges%20of%20labeling%20the%0AONER-2025%20dataset.%20Third%2C%20we%20analyze%20key%20linguistic%20challenges%2C%20including%0Aslang%2C%20ambiguity%2C%20fragmented%20sentences%2C%20and%20emotionally%20charged%20language%2C%20in%0Aopioid%20discussions.%20Fourth%2C%20we%20propose%20a%20real-time%20monitoring%20system%20to%20process%0Astreaming%20data%20from%20social%20media%2C%20healthcare%20records%2C%20and%20emergency%20services%20to%0Aidentify%20overdose%20events.%20Using%205-fold%20cross-validation%20in%2011%20experiments%2C%20our%0Asystem%20integrates%20machine%20learning%2C%20deep%20learning%2C%20and%20transformer-based%0Alanguage%20models%20with%20advanced%20contextual%20embeddings%20to%20enhance%20understanding.%0AOur%20transformer-based%20models%20%28bert-base-NER%20and%20roberta-base%29%20achieved%2097%25%0Aaccuracy%20and%20F1-score%2C%20outperforming%20baselines%20by%2010.23%25%20%28RF%3D0.88%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.00027v4&entry.124074799=Read"},
{"title": "Touch and Tell: Multimodal Decoding of Human Emotions and Social\n  Gestures for Robots", "author": "Qiaoqiao Ren and Remko Proesmans and Yuanbo Hou and Francis wyffels and Tony Belpaeme", "abstract": "  Human emotions are complex and can be conveyed through nuanced touch\ngestures. Previous research has primarily focused on how humans recognize\nemotions through touch or on identifying key features of emotional expression\nfor robots. However, there is a gap in understanding how reliably these\nemotions and gestures can be communicated to robots via touch and interpreted\nusing data driven methods. This study investigates the consistency and\ndistinguishability of emotional and gestural expressions through touch and\nsound. To this end, we integrated a custom piezoresistive pressure sensor as\nwell as a microphone on a social robot. Twenty-eight participants first\nconveyed ten different emotions to the robot using spontaneous touch gestures,\nthen they performed six predefined social touch gestures. Our findings reveal\nstatistically significant consistency in both emotion and gesture expression\namong participants. However, some emotions exhibited low intraclass correlation\nvalues, and certain emotions with similar levels of arousal or valence did not\nshow significant differences in their conveyance. To investigate emotion and\nsocial gesture decoding within affective human-robot tactile interaction, we\ndeveloped single-modality models and multimodal models integrating tactile and\nauditory features. A support vector machine (SVM) model trained on multimodal\nfeatures achieved the highest accuracy for classifying ten emotions, reaching\n40 %.For gesture classification, a Convolutional Neural Network- Long\nShort-Term Memory Network (CNN-LSTM) achieved 90.74 % accuracy. Our results\ndemonstrate that even though the unimodal models have the potential to decode\nemotions and touch gestures, the multimodal integration of touch and sound\nsignificantly outperforms unimodal approaches, enhancing the decoding of both\nemotions and gestures.\n", "link": "http://arxiv.org/abs/2412.03300v2", "date": "2025-08-12", "relevancy": 1.5151, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5845}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.497}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Touch%20and%20Tell%3A%20Multimodal%20Decoding%20of%20Human%20Emotions%20and%20Social%0A%20%20Gestures%20for%20Robots&body=Title%3A%20Touch%20and%20Tell%3A%20Multimodal%20Decoding%20of%20Human%20Emotions%20and%20Social%0A%20%20Gestures%20for%20Robots%0AAuthor%3A%20Qiaoqiao%20Ren%20and%20Remko%20Proesmans%20and%20Yuanbo%20Hou%20and%20Francis%20wyffels%20and%20Tony%20Belpaeme%0AAbstract%3A%20%20%20Human%20emotions%20are%20complex%20and%20can%20be%20conveyed%20through%20nuanced%20touch%0Agestures.%20Previous%20research%20has%20primarily%20focused%20on%20how%20humans%20recognize%0Aemotions%20through%20touch%20or%20on%20identifying%20key%20features%20of%20emotional%20expression%0Afor%20robots.%20However%2C%20there%20is%20a%20gap%20in%20understanding%20how%20reliably%20these%0Aemotions%20and%20gestures%20can%20be%20communicated%20to%20robots%20via%20touch%20and%20interpreted%0Ausing%20data%20driven%20methods.%20This%20study%20investigates%20the%20consistency%20and%0Adistinguishability%20of%20emotional%20and%20gestural%20expressions%20through%20touch%20and%0Asound.%20To%20this%20end%2C%20we%20integrated%20a%20custom%20piezoresistive%20pressure%20sensor%20as%0Awell%20as%20a%20microphone%20on%20a%20social%20robot.%20Twenty-eight%20participants%20first%0Aconveyed%20ten%20different%20emotions%20to%20the%20robot%20using%20spontaneous%20touch%20gestures%2C%0Athen%20they%20performed%20six%20predefined%20social%20touch%20gestures.%20Our%20findings%20reveal%0Astatistically%20significant%20consistency%20in%20both%20emotion%20and%20gesture%20expression%0Aamong%20participants.%20However%2C%20some%20emotions%20exhibited%20low%20intraclass%20correlation%0Avalues%2C%20and%20certain%20emotions%20with%20similar%20levels%20of%20arousal%20or%20valence%20did%20not%0Ashow%20significant%20differences%20in%20their%20conveyance.%20To%20investigate%20emotion%20and%0Asocial%20gesture%20decoding%20within%20affective%20human-robot%20tactile%20interaction%2C%20we%0Adeveloped%20single-modality%20models%20and%20multimodal%20models%20integrating%20tactile%20and%0Aauditory%20features.%20A%20support%20vector%20machine%20%28SVM%29%20model%20trained%20on%20multimodal%0Afeatures%20achieved%20the%20highest%20accuracy%20for%20classifying%20ten%20emotions%2C%20reaching%0A40%20%25.For%20gesture%20classification%2C%20a%20Convolutional%20Neural%20Network-%20Long%0AShort-Term%20Memory%20Network%20%28CNN-LSTM%29%20achieved%2090.74%20%25%20accuracy.%20Our%20results%0Ademonstrate%20that%20even%20though%20the%20unimodal%20models%20have%20the%20potential%20to%20decode%0Aemotions%20and%20touch%20gestures%2C%20the%20multimodal%20integration%20of%20touch%20and%20sound%0Asignificantly%20outperforms%20unimodal%20approaches%2C%20enhancing%20the%20decoding%20of%20both%0Aemotions%20and%20gestures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03300v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTouch%2520and%2520Tell%253A%2520Multimodal%2520Decoding%2520of%2520Human%2520Emotions%2520and%2520Social%250A%2520%2520Gestures%2520for%2520Robots%26entry.906535625%3DQiaoqiao%2520Ren%2520and%2520Remko%2520Proesmans%2520and%2520Yuanbo%2520Hou%2520and%2520Francis%2520wyffels%2520and%2520Tony%2520Belpaeme%26entry.1292438233%3D%2520%2520Human%2520emotions%2520are%2520complex%2520and%2520can%2520be%2520conveyed%2520through%2520nuanced%2520touch%250Agestures.%2520Previous%2520research%2520has%2520primarily%2520focused%2520on%2520how%2520humans%2520recognize%250Aemotions%2520through%2520touch%2520or%2520on%2520identifying%2520key%2520features%2520of%2520emotional%2520expression%250Afor%2520robots.%2520However%252C%2520there%2520is%2520a%2520gap%2520in%2520understanding%2520how%2520reliably%2520these%250Aemotions%2520and%2520gestures%2520can%2520be%2520communicated%2520to%2520robots%2520via%2520touch%2520and%2520interpreted%250Ausing%2520data%2520driven%2520methods.%2520This%2520study%2520investigates%2520the%2520consistency%2520and%250Adistinguishability%2520of%2520emotional%2520and%2520gestural%2520expressions%2520through%2520touch%2520and%250Asound.%2520To%2520this%2520end%252C%2520we%2520integrated%2520a%2520custom%2520piezoresistive%2520pressure%2520sensor%2520as%250Awell%2520as%2520a%2520microphone%2520on%2520a%2520social%2520robot.%2520Twenty-eight%2520participants%2520first%250Aconveyed%2520ten%2520different%2520emotions%2520to%2520the%2520robot%2520using%2520spontaneous%2520touch%2520gestures%252C%250Athen%2520they%2520performed%2520six%2520predefined%2520social%2520touch%2520gestures.%2520Our%2520findings%2520reveal%250Astatistically%2520significant%2520consistency%2520in%2520both%2520emotion%2520and%2520gesture%2520expression%250Aamong%2520participants.%2520However%252C%2520some%2520emotions%2520exhibited%2520low%2520intraclass%2520correlation%250Avalues%252C%2520and%2520certain%2520emotions%2520with%2520similar%2520levels%2520of%2520arousal%2520or%2520valence%2520did%2520not%250Ashow%2520significant%2520differences%2520in%2520their%2520conveyance.%2520To%2520investigate%2520emotion%2520and%250Asocial%2520gesture%2520decoding%2520within%2520affective%2520human-robot%2520tactile%2520interaction%252C%2520we%250Adeveloped%2520single-modality%2520models%2520and%2520multimodal%2520models%2520integrating%2520tactile%2520and%250Aauditory%2520features.%2520A%2520support%2520vector%2520machine%2520%2528SVM%2529%2520model%2520trained%2520on%2520multimodal%250Afeatures%2520achieved%2520the%2520highest%2520accuracy%2520for%2520classifying%2520ten%2520emotions%252C%2520reaching%250A40%2520%2525.For%2520gesture%2520classification%252C%2520a%2520Convolutional%2520Neural%2520Network-%2520Long%250AShort-Term%2520Memory%2520Network%2520%2528CNN-LSTM%2529%2520achieved%252090.74%2520%2525%2520accuracy.%2520Our%2520results%250Ademonstrate%2520that%2520even%2520though%2520the%2520unimodal%2520models%2520have%2520the%2520potential%2520to%2520decode%250Aemotions%2520and%2520touch%2520gestures%252C%2520the%2520multimodal%2520integration%2520of%2520touch%2520and%2520sound%250Asignificantly%2520outperforms%2520unimodal%2520approaches%252C%2520enhancing%2520the%2520decoding%2520of%2520both%250Aemotions%2520and%2520gestures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03300v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Touch%20and%20Tell%3A%20Multimodal%20Decoding%20of%20Human%20Emotions%20and%20Social%0A%20%20Gestures%20for%20Robots&entry.906535625=Qiaoqiao%20Ren%20and%20Remko%20Proesmans%20and%20Yuanbo%20Hou%20and%20Francis%20wyffels%20and%20Tony%20Belpaeme&entry.1292438233=%20%20Human%20emotions%20are%20complex%20and%20can%20be%20conveyed%20through%20nuanced%20touch%0Agestures.%20Previous%20research%20has%20primarily%20focused%20on%20how%20humans%20recognize%0Aemotions%20through%20touch%20or%20on%20identifying%20key%20features%20of%20emotional%20expression%0Afor%20robots.%20However%2C%20there%20is%20a%20gap%20in%20understanding%20how%20reliably%20these%0Aemotions%20and%20gestures%20can%20be%20communicated%20to%20robots%20via%20touch%20and%20interpreted%0Ausing%20data%20driven%20methods.%20This%20study%20investigates%20the%20consistency%20and%0Adistinguishability%20of%20emotional%20and%20gestural%20expressions%20through%20touch%20and%0Asound.%20To%20this%20end%2C%20we%20integrated%20a%20custom%20piezoresistive%20pressure%20sensor%20as%0Awell%20as%20a%20microphone%20on%20a%20social%20robot.%20Twenty-eight%20participants%20first%0Aconveyed%20ten%20different%20emotions%20to%20the%20robot%20using%20spontaneous%20touch%20gestures%2C%0Athen%20they%20performed%20six%20predefined%20social%20touch%20gestures.%20Our%20findings%20reveal%0Astatistically%20significant%20consistency%20in%20both%20emotion%20and%20gesture%20expression%0Aamong%20participants.%20However%2C%20some%20emotions%20exhibited%20low%20intraclass%20correlation%0Avalues%2C%20and%20certain%20emotions%20with%20similar%20levels%20of%20arousal%20or%20valence%20did%20not%0Ashow%20significant%20differences%20in%20their%20conveyance.%20To%20investigate%20emotion%20and%0Asocial%20gesture%20decoding%20within%20affective%20human-robot%20tactile%20interaction%2C%20we%0Adeveloped%20single-modality%20models%20and%20multimodal%20models%20integrating%20tactile%20and%0Aauditory%20features.%20A%20support%20vector%20machine%20%28SVM%29%20model%20trained%20on%20multimodal%0Afeatures%20achieved%20the%20highest%20accuracy%20for%20classifying%20ten%20emotions%2C%20reaching%0A40%20%25.For%20gesture%20classification%2C%20a%20Convolutional%20Neural%20Network-%20Long%0AShort-Term%20Memory%20Network%20%28CNN-LSTM%29%20achieved%2090.74%20%25%20accuracy.%20Our%20results%0Ademonstrate%20that%20even%20though%20the%20unimodal%20models%20have%20the%20potential%20to%20decode%0Aemotions%20and%20touch%20gestures%2C%20the%20multimodal%20integration%20of%20touch%20and%20sound%0Asignificantly%20outperforms%20unimodal%20approaches%2C%20enhancing%20the%20decoding%20of%20both%0Aemotions%20and%20gestures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03300v2&entry.124074799=Read"},
{"title": "Low-Regret and Low-Complexity Learning for Hierarchical Inference", "author": "Sameep Chattopadhyay and Vinay Sutar and Jaya Prakash Champati and Sharayu Moharir", "abstract": "  This work focuses on Hierarchical Inference (HI) in edge intelligence\nsystems, where a compact Local-ML model on an end-device works in conjunction\nwith a high-accuracy Remote-ML model on an edge-server. HI aims to reduce\nlatency, improve accuracy, and lower bandwidth usage by first using the\nLocal-ML model for inference and offloading to the Remote-ML only when the\nlocal inference is likely incorrect. A critical challenge in HI is estimating\nthe likelihood of the local inference being incorrect, especially when data\ndistributions and offloading costs change over time -- a problem we term\nHierarchical Inference Learning (HIL). We introduce a novel approach to HIL by\nmodeling the probability of correct inference by the Local-ML as an increasing\nfunction of the model's confidence measure, a structure motivated by empirical\nobservations but previously unexploited. We propose two policies, HI-LCB and\nHI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We\ndemonstrate that both policies achieve order-optimal regret of $O(\\log T)$, a\nsignificant improvement over existing HIL policies with $O(T^{2/3})$ regret\nguarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational\ncomplexity, making it well-suited for deployment on devices with severe\nresource limitations. Simulations using real-world datasets confirm that our\npolicies outperform existing state-of-the-art HIL methods.\n", "link": "http://arxiv.org/abs/2508.08985v1", "date": "2025-08-12", "relevancy": 1.5093, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5135}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5017}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Regret%20and%20Low-Complexity%20Learning%20for%20Hierarchical%20Inference&body=Title%3A%20Low-Regret%20and%20Low-Complexity%20Learning%20for%20Hierarchical%20Inference%0AAuthor%3A%20Sameep%20Chattopadhyay%20and%20Vinay%20Sutar%20and%20Jaya%20Prakash%20Champati%20and%20Sharayu%20Moharir%0AAbstract%3A%20%20%20This%20work%20focuses%20on%20Hierarchical%20Inference%20%28HI%29%20in%20edge%20intelligence%0Asystems%2C%20where%20a%20compact%20Local-ML%20model%20on%20an%20end-device%20works%20in%20conjunction%0Awith%20a%20high-accuracy%20Remote-ML%20model%20on%20an%20edge-server.%20HI%20aims%20to%20reduce%0Alatency%2C%20improve%20accuracy%2C%20and%20lower%20bandwidth%20usage%20by%20first%20using%20the%0ALocal-ML%20model%20for%20inference%20and%20offloading%20to%20the%20Remote-ML%20only%20when%20the%0Alocal%20inference%20is%20likely%20incorrect.%20A%20critical%20challenge%20in%20HI%20is%20estimating%0Athe%20likelihood%20of%20the%20local%20inference%20being%20incorrect%2C%20especially%20when%20data%0Adistributions%20and%20offloading%20costs%20change%20over%20time%20--%20a%20problem%20we%20term%0AHierarchical%20Inference%20Learning%20%28HIL%29.%20We%20introduce%20a%20novel%20approach%20to%20HIL%20by%0Amodeling%20the%20probability%20of%20correct%20inference%20by%20the%20Local-ML%20as%20an%20increasing%0Afunction%20of%20the%20model%27s%20confidence%20measure%2C%20a%20structure%20motivated%20by%20empirical%0Aobservations%20but%20previously%20unexploited.%20We%20propose%20two%20policies%2C%20HI-LCB%20and%0AHI-LCB-lite%2C%20based%20on%20the%20Upper%20Confidence%20Bound%20%28UCB%29%20framework.%20We%0Ademonstrate%20that%20both%20policies%20achieve%20order-optimal%20regret%20of%20%24O%28%5Clog%20T%29%24%2C%20a%0Asignificant%20improvement%20over%20existing%20HIL%20policies%20with%20%24O%28T%5E%7B2/3%7D%29%24%20regret%0Aguarantees.%20Notably%2C%20HI-LCB-lite%20has%20an%20%24O%281%29%24%20per-sample%20computational%0Acomplexity%2C%20making%20it%20well-suited%20for%20deployment%20on%20devices%20with%20severe%0Aresource%20limitations.%20Simulations%20using%20real-world%20datasets%20confirm%20that%20our%0Apolicies%20outperform%20existing%20state-of-the-art%20HIL%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Regret%2520and%2520Low-Complexity%2520Learning%2520for%2520Hierarchical%2520Inference%26entry.906535625%3DSameep%2520Chattopadhyay%2520and%2520Vinay%2520Sutar%2520and%2520Jaya%2520Prakash%2520Champati%2520and%2520Sharayu%2520Moharir%26entry.1292438233%3D%2520%2520This%2520work%2520focuses%2520on%2520Hierarchical%2520Inference%2520%2528HI%2529%2520in%2520edge%2520intelligence%250Asystems%252C%2520where%2520a%2520compact%2520Local-ML%2520model%2520on%2520an%2520end-device%2520works%2520in%2520conjunction%250Awith%2520a%2520high-accuracy%2520Remote-ML%2520model%2520on%2520an%2520edge-server.%2520HI%2520aims%2520to%2520reduce%250Alatency%252C%2520improve%2520accuracy%252C%2520and%2520lower%2520bandwidth%2520usage%2520by%2520first%2520using%2520the%250ALocal-ML%2520model%2520for%2520inference%2520and%2520offloading%2520to%2520the%2520Remote-ML%2520only%2520when%2520the%250Alocal%2520inference%2520is%2520likely%2520incorrect.%2520A%2520critical%2520challenge%2520in%2520HI%2520is%2520estimating%250Athe%2520likelihood%2520of%2520the%2520local%2520inference%2520being%2520incorrect%252C%2520especially%2520when%2520data%250Adistributions%2520and%2520offloading%2520costs%2520change%2520over%2520time%2520--%2520a%2520problem%2520we%2520term%250AHierarchical%2520Inference%2520Learning%2520%2528HIL%2529.%2520We%2520introduce%2520a%2520novel%2520approach%2520to%2520HIL%2520by%250Amodeling%2520the%2520probability%2520of%2520correct%2520inference%2520by%2520the%2520Local-ML%2520as%2520an%2520increasing%250Afunction%2520of%2520the%2520model%2527s%2520confidence%2520measure%252C%2520a%2520structure%2520motivated%2520by%2520empirical%250Aobservations%2520but%2520previously%2520unexploited.%2520We%2520propose%2520two%2520policies%252C%2520HI-LCB%2520and%250AHI-LCB-lite%252C%2520based%2520on%2520the%2520Upper%2520Confidence%2520Bound%2520%2528UCB%2529%2520framework.%2520We%250Ademonstrate%2520that%2520both%2520policies%2520achieve%2520order-optimal%2520regret%2520of%2520%2524O%2528%255Clog%2520T%2529%2524%252C%2520a%250Asignificant%2520improvement%2520over%2520existing%2520HIL%2520policies%2520with%2520%2524O%2528T%255E%257B2/3%257D%2529%2524%2520regret%250Aguarantees.%2520Notably%252C%2520HI-LCB-lite%2520has%2520an%2520%2524O%25281%2529%2524%2520per-sample%2520computational%250Acomplexity%252C%2520making%2520it%2520well-suited%2520for%2520deployment%2520on%2520devices%2520with%2520severe%250Aresource%2520limitations.%2520Simulations%2520using%2520real-world%2520datasets%2520confirm%2520that%2520our%250Apolicies%2520outperform%2520existing%2520state-of-the-art%2520HIL%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Regret%20and%20Low-Complexity%20Learning%20for%20Hierarchical%20Inference&entry.906535625=Sameep%20Chattopadhyay%20and%20Vinay%20Sutar%20and%20Jaya%20Prakash%20Champati%20and%20Sharayu%20Moharir&entry.1292438233=%20%20This%20work%20focuses%20on%20Hierarchical%20Inference%20%28HI%29%20in%20edge%20intelligence%0Asystems%2C%20where%20a%20compact%20Local-ML%20model%20on%20an%20end-device%20works%20in%20conjunction%0Awith%20a%20high-accuracy%20Remote-ML%20model%20on%20an%20edge-server.%20HI%20aims%20to%20reduce%0Alatency%2C%20improve%20accuracy%2C%20and%20lower%20bandwidth%20usage%20by%20first%20using%20the%0ALocal-ML%20model%20for%20inference%20and%20offloading%20to%20the%20Remote-ML%20only%20when%20the%0Alocal%20inference%20is%20likely%20incorrect.%20A%20critical%20challenge%20in%20HI%20is%20estimating%0Athe%20likelihood%20of%20the%20local%20inference%20being%20incorrect%2C%20especially%20when%20data%0Adistributions%20and%20offloading%20costs%20change%20over%20time%20--%20a%20problem%20we%20term%0AHierarchical%20Inference%20Learning%20%28HIL%29.%20We%20introduce%20a%20novel%20approach%20to%20HIL%20by%0Amodeling%20the%20probability%20of%20correct%20inference%20by%20the%20Local-ML%20as%20an%20increasing%0Afunction%20of%20the%20model%27s%20confidence%20measure%2C%20a%20structure%20motivated%20by%20empirical%0Aobservations%20but%20previously%20unexploited.%20We%20propose%20two%20policies%2C%20HI-LCB%20and%0AHI-LCB-lite%2C%20based%20on%20the%20Upper%20Confidence%20Bound%20%28UCB%29%20framework.%20We%0Ademonstrate%20that%20both%20policies%20achieve%20order-optimal%20regret%20of%20%24O%28%5Clog%20T%29%24%2C%20a%0Asignificant%20improvement%20over%20existing%20HIL%20policies%20with%20%24O%28T%5E%7B2/3%7D%29%24%20regret%0Aguarantees.%20Notably%2C%20HI-LCB-lite%20has%20an%20%24O%281%29%24%20per-sample%20computational%0Acomplexity%2C%20making%20it%20well-suited%20for%20deployment%20on%20devices%20with%20severe%0Aresource%20limitations.%20Simulations%20using%20real-world%20datasets%20confirm%20that%20our%0Apolicies%20outperform%20existing%20state-of-the-art%20HIL%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08985v1&entry.124074799=Read"},
{"title": "Meta-learning optimizes predictions of missing links in real-world\n  networks", "author": "Bisman Singh and Lucy Van Kleunen and Aaron Clauset", "abstract": "  Relational data are ubiquitous in real-world data applications, e.g., in\nsocial network analysis or biological modeling, but networks are nearly always\nincompletely observed. The state-of-the-art for predicting missing links in the\nhard case of a network without node attributes uses model stacking or neural\nnetwork techniques. It remains unknown which approach is best, and whether or\nhow the best choice of algorithm depends on the input network's\ncharacteristics. We answer these questions systematically using a large,\nstructurally diverse benchmark of 550 real-world networks under two standard\naccuracy measures (AUC and Top-k), comparing four stacking algorithms with 42\ntopological link predictors, two of which we introduce here, and two graph\nneural network algorithms. We show that no algorithm is best across all input\nnetworks, all algorithms perform well on most social networks, and few perform\nwell on economic and biological networks. Overall, model stacking with a random\nforest is both highly scalable and surpasses on AUC or is competitive with\ngraph neural networks on Top-k accuracy. But, algorithm performance depends\nstrongly on network characteristics like the degree distribution, triangle\ndensity, and degree assortativity. We introduce a meta-learning algorithm that\nexploits this variability to optimize link predictions for individual networks\nby selecting the best algorithm to apply, which we show outperforms all\nstate-of-the-art algorithms and scales to large networks.\n", "link": "http://arxiv.org/abs/2508.09069v1", "date": "2025-08-12", "relevancy": 1.5087, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5175}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4859}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-learning%20optimizes%20predictions%20of%20missing%20links%20in%20real-world%0A%20%20networks&body=Title%3A%20Meta-learning%20optimizes%20predictions%20of%20missing%20links%20in%20real-world%0A%20%20networks%0AAuthor%3A%20Bisman%20Singh%20and%20Lucy%20Van%20Kleunen%20and%20Aaron%20Clauset%0AAbstract%3A%20%20%20Relational%20data%20are%20ubiquitous%20in%20real-world%20data%20applications%2C%20e.g.%2C%20in%0Asocial%20network%20analysis%20or%20biological%20modeling%2C%20but%20networks%20are%20nearly%20always%0Aincompletely%20observed.%20The%20state-of-the-art%20for%20predicting%20missing%20links%20in%20the%0Ahard%20case%20of%20a%20network%20without%20node%20attributes%20uses%20model%20stacking%20or%20neural%0Anetwork%20techniques.%20It%20remains%20unknown%20which%20approach%20is%20best%2C%20and%20whether%20or%0Ahow%20the%20best%20choice%20of%20algorithm%20depends%20on%20the%20input%20network%27s%0Acharacteristics.%20We%20answer%20these%20questions%20systematically%20using%20a%20large%2C%0Astructurally%20diverse%20benchmark%20of%20550%20real-world%20networks%20under%20two%20standard%0Aaccuracy%20measures%20%28AUC%20and%20Top-k%29%2C%20comparing%20four%20stacking%20algorithms%20with%2042%0Atopological%20link%20predictors%2C%20two%20of%20which%20we%20introduce%20here%2C%20and%20two%20graph%0Aneural%20network%20algorithms.%20We%20show%20that%20no%20algorithm%20is%20best%20across%20all%20input%0Anetworks%2C%20all%20algorithms%20perform%20well%20on%20most%20social%20networks%2C%20and%20few%20perform%0Awell%20on%20economic%20and%20biological%20networks.%20Overall%2C%20model%20stacking%20with%20a%20random%0Aforest%20is%20both%20highly%20scalable%20and%20surpasses%20on%20AUC%20or%20is%20competitive%20with%0Agraph%20neural%20networks%20on%20Top-k%20accuracy.%20But%2C%20algorithm%20performance%20depends%0Astrongly%20on%20network%20characteristics%20like%20the%20degree%20distribution%2C%20triangle%0Adensity%2C%20and%20degree%20assortativity.%20We%20introduce%20a%20meta-learning%20algorithm%20that%0Aexploits%20this%20variability%20to%20optimize%20link%20predictions%20for%20individual%20networks%0Aby%20selecting%20the%20best%20algorithm%20to%20apply%2C%20which%20we%20show%20outperforms%20all%0Astate-of-the-art%20algorithms%20and%20scales%20to%20large%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09069v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-learning%2520optimizes%2520predictions%2520of%2520missing%2520links%2520in%2520real-world%250A%2520%2520networks%26entry.906535625%3DBisman%2520Singh%2520and%2520Lucy%2520Van%2520Kleunen%2520and%2520Aaron%2520Clauset%26entry.1292438233%3D%2520%2520Relational%2520data%2520are%2520ubiquitous%2520in%2520real-world%2520data%2520applications%252C%2520e.g.%252C%2520in%250Asocial%2520network%2520analysis%2520or%2520biological%2520modeling%252C%2520but%2520networks%2520are%2520nearly%2520always%250Aincompletely%2520observed.%2520The%2520state-of-the-art%2520for%2520predicting%2520missing%2520links%2520in%2520the%250Ahard%2520case%2520of%2520a%2520network%2520without%2520node%2520attributes%2520uses%2520model%2520stacking%2520or%2520neural%250Anetwork%2520techniques.%2520It%2520remains%2520unknown%2520which%2520approach%2520is%2520best%252C%2520and%2520whether%2520or%250Ahow%2520the%2520best%2520choice%2520of%2520algorithm%2520depends%2520on%2520the%2520input%2520network%2527s%250Acharacteristics.%2520We%2520answer%2520these%2520questions%2520systematically%2520using%2520a%2520large%252C%250Astructurally%2520diverse%2520benchmark%2520of%2520550%2520real-world%2520networks%2520under%2520two%2520standard%250Aaccuracy%2520measures%2520%2528AUC%2520and%2520Top-k%2529%252C%2520comparing%2520four%2520stacking%2520algorithms%2520with%252042%250Atopological%2520link%2520predictors%252C%2520two%2520of%2520which%2520we%2520introduce%2520here%252C%2520and%2520two%2520graph%250Aneural%2520network%2520algorithms.%2520We%2520show%2520that%2520no%2520algorithm%2520is%2520best%2520across%2520all%2520input%250Anetworks%252C%2520all%2520algorithms%2520perform%2520well%2520on%2520most%2520social%2520networks%252C%2520and%2520few%2520perform%250Awell%2520on%2520economic%2520and%2520biological%2520networks.%2520Overall%252C%2520model%2520stacking%2520with%2520a%2520random%250Aforest%2520is%2520both%2520highly%2520scalable%2520and%2520surpasses%2520on%2520AUC%2520or%2520is%2520competitive%2520with%250Agraph%2520neural%2520networks%2520on%2520Top-k%2520accuracy.%2520But%252C%2520algorithm%2520performance%2520depends%250Astrongly%2520on%2520network%2520characteristics%2520like%2520the%2520degree%2520distribution%252C%2520triangle%250Adensity%252C%2520and%2520degree%2520assortativity.%2520We%2520introduce%2520a%2520meta-learning%2520algorithm%2520that%250Aexploits%2520this%2520variability%2520to%2520optimize%2520link%2520predictions%2520for%2520individual%2520networks%250Aby%2520selecting%2520the%2520best%2520algorithm%2520to%2520apply%252C%2520which%2520we%2520show%2520outperforms%2520all%250Astate-of-the-art%2520algorithms%2520and%2520scales%2520to%2520large%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09069v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-learning%20optimizes%20predictions%20of%20missing%20links%20in%20real-world%0A%20%20networks&entry.906535625=Bisman%20Singh%20and%20Lucy%20Van%20Kleunen%20and%20Aaron%20Clauset&entry.1292438233=%20%20Relational%20data%20are%20ubiquitous%20in%20real-world%20data%20applications%2C%20e.g.%2C%20in%0Asocial%20network%20analysis%20or%20biological%20modeling%2C%20but%20networks%20are%20nearly%20always%0Aincompletely%20observed.%20The%20state-of-the-art%20for%20predicting%20missing%20links%20in%20the%0Ahard%20case%20of%20a%20network%20without%20node%20attributes%20uses%20model%20stacking%20or%20neural%0Anetwork%20techniques.%20It%20remains%20unknown%20which%20approach%20is%20best%2C%20and%20whether%20or%0Ahow%20the%20best%20choice%20of%20algorithm%20depends%20on%20the%20input%20network%27s%0Acharacteristics.%20We%20answer%20these%20questions%20systematically%20using%20a%20large%2C%0Astructurally%20diverse%20benchmark%20of%20550%20real-world%20networks%20under%20two%20standard%0Aaccuracy%20measures%20%28AUC%20and%20Top-k%29%2C%20comparing%20four%20stacking%20algorithms%20with%2042%0Atopological%20link%20predictors%2C%20two%20of%20which%20we%20introduce%20here%2C%20and%20two%20graph%0Aneural%20network%20algorithms.%20We%20show%20that%20no%20algorithm%20is%20best%20across%20all%20input%0Anetworks%2C%20all%20algorithms%20perform%20well%20on%20most%20social%20networks%2C%20and%20few%20perform%0Awell%20on%20economic%20and%20biological%20networks.%20Overall%2C%20model%20stacking%20with%20a%20random%0Aforest%20is%20both%20highly%20scalable%20and%20surpasses%20on%20AUC%20or%20is%20competitive%20with%0Agraph%20neural%20networks%20on%20Top-k%20accuracy.%20But%2C%20algorithm%20performance%20depends%0Astrongly%20on%20network%20characteristics%20like%20the%20degree%20distribution%2C%20triangle%0Adensity%2C%20and%20degree%20assortativity.%20We%20introduce%20a%20meta-learning%20algorithm%20that%0Aexploits%20this%20variability%20to%20optimize%20link%20predictions%20for%20individual%20networks%0Aby%20selecting%20the%20best%20algorithm%20to%20apply%2C%20which%20we%20show%20outperforms%20all%0Astate-of-the-art%20algorithms%20and%20scales%20to%20large%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09069v1&entry.124074799=Read"},
{"title": "Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making\n  under Epistemic Uncertainty", "author": "Rui Wang and Qihan Lin and Jiayu Liu and Qing Zong and Tianshi Zheng and Weiqi Wang and Yangqiu Song", "abstract": "  Prospect Theory (PT) models human decision-making under uncertainty, while\nepistemic markers (e.g., maybe) serve to express uncertainty in language.\nHowever, it remains largely unexplored whether Prospect Theory applies to\ncontemporary Large Language Models and whether epistemic markers, which express\nhuman uncertainty, affect their decision-making behaviour. To address these\nresearch gaps, we design a three-stage experiment based on economic\nquestionnaires. We propose a more general and precise evaluation framework to\nmodel LLMs' decision-making behaviour under PT, introducing uncertainty through\nthe empirical probability values associated with commonly used epistemic\nmarkers in comparable contexts. We then incorporate epistemic markers into the\nevaluation framework based on their corresponding probability values to examine\ntheir influence on LLM decision-making behaviours. Our findings suggest that\nmodelling LLMs' decision-making with PT is not consistently reliable,\nparticularly when uncertainty is expressed in diverse linguistic forms. Our\ncode is released in https://github.com/HKUST-KnowComp/MarPT.\n", "link": "http://arxiv.org/abs/2508.08992v1", "date": "2025-08-12", "relevancy": 1.5058, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5334}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4943}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prospect%20Theory%20Fails%20for%20LLMs%3A%20Revealing%20Instability%20of%20Decision-Making%0A%20%20under%20Epistemic%20Uncertainty&body=Title%3A%20Prospect%20Theory%20Fails%20for%20LLMs%3A%20Revealing%20Instability%20of%20Decision-Making%0A%20%20under%20Epistemic%20Uncertainty%0AAuthor%3A%20Rui%20Wang%20and%20Qihan%20Lin%20and%20Jiayu%20Liu%20and%20Qing%20Zong%20and%20Tianshi%20Zheng%20and%20Weiqi%20Wang%20and%20Yangqiu%20Song%0AAbstract%3A%20%20%20Prospect%20Theory%20%28PT%29%20models%20human%20decision-making%20under%20uncertainty%2C%20while%0Aepistemic%20markers%20%28e.g.%2C%20maybe%29%20serve%20to%20express%20uncertainty%20in%20language.%0AHowever%2C%20it%20remains%20largely%20unexplored%20whether%20Prospect%20Theory%20applies%20to%0Acontemporary%20Large%20Language%20Models%20and%20whether%20epistemic%20markers%2C%20which%20express%0Ahuman%20uncertainty%2C%20affect%20their%20decision-making%20behaviour.%20To%20address%20these%0Aresearch%20gaps%2C%20we%20design%20a%20three-stage%20experiment%20based%20on%20economic%0Aquestionnaires.%20We%20propose%20a%20more%20general%20and%20precise%20evaluation%20framework%20to%0Amodel%20LLMs%27%20decision-making%20behaviour%20under%20PT%2C%20introducing%20uncertainty%20through%0Athe%20empirical%20probability%20values%20associated%20with%20commonly%20used%20epistemic%0Amarkers%20in%20comparable%20contexts.%20We%20then%20incorporate%20epistemic%20markers%20into%20the%0Aevaluation%20framework%20based%20on%20their%20corresponding%20probability%20values%20to%20examine%0Atheir%20influence%20on%20LLM%20decision-making%20behaviours.%20Our%20findings%20suggest%20that%0Amodelling%20LLMs%27%20decision-making%20with%20PT%20is%20not%20consistently%20reliable%2C%0Aparticularly%20when%20uncertainty%20is%20expressed%20in%20diverse%20linguistic%20forms.%20Our%0Acode%20is%20released%20in%20https%3A//github.com/HKUST-KnowComp/MarPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProspect%2520Theory%2520Fails%2520for%2520LLMs%253A%2520Revealing%2520Instability%2520of%2520Decision-Making%250A%2520%2520under%2520Epistemic%2520Uncertainty%26entry.906535625%3DRui%2520Wang%2520and%2520Qihan%2520Lin%2520and%2520Jiayu%2520Liu%2520and%2520Qing%2520Zong%2520and%2520Tianshi%2520Zheng%2520and%2520Weiqi%2520Wang%2520and%2520Yangqiu%2520Song%26entry.1292438233%3D%2520%2520Prospect%2520Theory%2520%2528PT%2529%2520models%2520human%2520decision-making%2520under%2520uncertainty%252C%2520while%250Aepistemic%2520markers%2520%2528e.g.%252C%2520maybe%2529%2520serve%2520to%2520express%2520uncertainty%2520in%2520language.%250AHowever%252C%2520it%2520remains%2520largely%2520unexplored%2520whether%2520Prospect%2520Theory%2520applies%2520to%250Acontemporary%2520Large%2520Language%2520Models%2520and%2520whether%2520epistemic%2520markers%252C%2520which%2520express%250Ahuman%2520uncertainty%252C%2520affect%2520their%2520decision-making%2520behaviour.%2520To%2520address%2520these%250Aresearch%2520gaps%252C%2520we%2520design%2520a%2520three-stage%2520experiment%2520based%2520on%2520economic%250Aquestionnaires.%2520We%2520propose%2520a%2520more%2520general%2520and%2520precise%2520evaluation%2520framework%2520to%250Amodel%2520LLMs%2527%2520decision-making%2520behaviour%2520under%2520PT%252C%2520introducing%2520uncertainty%2520through%250Athe%2520empirical%2520probability%2520values%2520associated%2520with%2520commonly%2520used%2520epistemic%250Amarkers%2520in%2520comparable%2520contexts.%2520We%2520then%2520incorporate%2520epistemic%2520markers%2520into%2520the%250Aevaluation%2520framework%2520based%2520on%2520their%2520corresponding%2520probability%2520values%2520to%2520examine%250Atheir%2520influence%2520on%2520LLM%2520decision-making%2520behaviours.%2520Our%2520findings%2520suggest%2520that%250Amodelling%2520LLMs%2527%2520decision-making%2520with%2520PT%2520is%2520not%2520consistently%2520reliable%252C%250Aparticularly%2520when%2520uncertainty%2520is%2520expressed%2520in%2520diverse%2520linguistic%2520forms.%2520Our%250Acode%2520is%2520released%2520in%2520https%253A//github.com/HKUST-KnowComp/MarPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prospect%20Theory%20Fails%20for%20LLMs%3A%20Revealing%20Instability%20of%20Decision-Making%0A%20%20under%20Epistemic%20Uncertainty&entry.906535625=Rui%20Wang%20and%20Qihan%20Lin%20and%20Jiayu%20Liu%20and%20Qing%20Zong%20and%20Tianshi%20Zheng%20and%20Weiqi%20Wang%20and%20Yangqiu%20Song&entry.1292438233=%20%20Prospect%20Theory%20%28PT%29%20models%20human%20decision-making%20under%20uncertainty%2C%20while%0Aepistemic%20markers%20%28e.g.%2C%20maybe%29%20serve%20to%20express%20uncertainty%20in%20language.%0AHowever%2C%20it%20remains%20largely%20unexplored%20whether%20Prospect%20Theory%20applies%20to%0Acontemporary%20Large%20Language%20Models%20and%20whether%20epistemic%20markers%2C%20which%20express%0Ahuman%20uncertainty%2C%20affect%20their%20decision-making%20behaviour.%20To%20address%20these%0Aresearch%20gaps%2C%20we%20design%20a%20three-stage%20experiment%20based%20on%20economic%0Aquestionnaires.%20We%20propose%20a%20more%20general%20and%20precise%20evaluation%20framework%20to%0Amodel%20LLMs%27%20decision-making%20behaviour%20under%20PT%2C%20introducing%20uncertainty%20through%0Athe%20empirical%20probability%20values%20associated%20with%20commonly%20used%20epistemic%0Amarkers%20in%20comparable%20contexts.%20We%20then%20incorporate%20epistemic%20markers%20into%20the%0Aevaluation%20framework%20based%20on%20their%20corresponding%20probability%20values%20to%20examine%0Atheir%20influence%20on%20LLM%20decision-making%20behaviours.%20Our%20findings%20suggest%20that%0Amodelling%20LLMs%27%20decision-making%20with%20PT%20is%20not%20consistently%20reliable%2C%0Aparticularly%20when%20uncertainty%20is%20expressed%20in%20diverse%20linguistic%20forms.%20Our%0Acode%20is%20released%20in%20https%3A//github.com/HKUST-KnowComp/MarPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08992v1&entry.124074799=Read"},
{"title": "Cross-Modal Temporal Fusion for Financial Market Forecasting", "author": "Yunhua Pei and John Cartlidge and Anandadeep Mandal and Daniel Gold and Enrique Marcilio and Riccardo Mazzon", "abstract": "  Accurate forecasting in financial markets requires integrating diverse data\nsources, from historical prices to macroeconomic indicators and financial news.\nHowever, existing models often fail to align these modalities effectively,\nlimiting their practical use. In this paper, we introduce a transformer-based\ndeep learning framework, Cross-Modal Temporal Fusion (CMTF), that fuses\nstructured and unstructured financial data for improved market prediction. The\nmodel incorporates a tensor interpretation module for feature selection and an\nauto-training pipeline for efficient hyperparameter tuning. Experimental\nresults using FTSE 100 stock data demonstrate that CMTF achieves superior\nperformance in price direction classification compared to classical and deep\nlearning baselines. These findings suggest that our framework is an effective\nand scalable solution for real-world cross-modal financial forecasting tasks.\n", "link": "http://arxiv.org/abs/2504.13522v2", "date": "2025-08-12", "relevancy": 1.4924, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5035}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4989}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Modal%20Temporal%20Fusion%20for%20Financial%20Market%20Forecasting&body=Title%3A%20Cross-Modal%20Temporal%20Fusion%20for%20Financial%20Market%20Forecasting%0AAuthor%3A%20Yunhua%20Pei%20and%20John%20Cartlidge%20and%20Anandadeep%20Mandal%20and%20Daniel%20Gold%20and%20Enrique%20Marcilio%20and%20Riccardo%20Mazzon%0AAbstract%3A%20%20%20Accurate%20forecasting%20in%20financial%20markets%20requires%20integrating%20diverse%20data%0Asources%2C%20from%20historical%20prices%20to%20macroeconomic%20indicators%20and%20financial%20news.%0AHowever%2C%20existing%20models%20often%20fail%20to%20align%20these%20modalities%20effectively%2C%0Alimiting%20their%20practical%20use.%20In%20this%20paper%2C%20we%20introduce%20a%20transformer-based%0Adeep%20learning%20framework%2C%20Cross-Modal%20Temporal%20Fusion%20%28CMTF%29%2C%20that%20fuses%0Astructured%20and%20unstructured%20financial%20data%20for%20improved%20market%20prediction.%20The%0Amodel%20incorporates%20a%20tensor%20interpretation%20module%20for%20feature%20selection%20and%20an%0Aauto-training%20pipeline%20for%20efficient%20hyperparameter%20tuning.%20Experimental%0Aresults%20using%20FTSE%20100%20stock%20data%20demonstrate%20that%20CMTF%20achieves%20superior%0Aperformance%20in%20price%20direction%20classification%20compared%20to%20classical%20and%20deep%0Alearning%20baselines.%20These%20findings%20suggest%20that%20our%20framework%20is%20an%20effective%0Aand%20scalable%20solution%20for%20real-world%20cross-modal%20financial%20forecasting%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13522v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Modal%2520Temporal%2520Fusion%2520for%2520Financial%2520Market%2520Forecasting%26entry.906535625%3DYunhua%2520Pei%2520and%2520John%2520Cartlidge%2520and%2520Anandadeep%2520Mandal%2520and%2520Daniel%2520Gold%2520and%2520Enrique%2520Marcilio%2520and%2520Riccardo%2520Mazzon%26entry.1292438233%3D%2520%2520Accurate%2520forecasting%2520in%2520financial%2520markets%2520requires%2520integrating%2520diverse%2520data%250Asources%252C%2520from%2520historical%2520prices%2520to%2520macroeconomic%2520indicators%2520and%2520financial%2520news.%250AHowever%252C%2520existing%2520models%2520often%2520fail%2520to%2520align%2520these%2520modalities%2520effectively%252C%250Alimiting%2520their%2520practical%2520use.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520transformer-based%250Adeep%2520learning%2520framework%252C%2520Cross-Modal%2520Temporal%2520Fusion%2520%2528CMTF%2529%252C%2520that%2520fuses%250Astructured%2520and%2520unstructured%2520financial%2520data%2520for%2520improved%2520market%2520prediction.%2520The%250Amodel%2520incorporates%2520a%2520tensor%2520interpretation%2520module%2520for%2520feature%2520selection%2520and%2520an%250Aauto-training%2520pipeline%2520for%2520efficient%2520hyperparameter%2520tuning.%2520Experimental%250Aresults%2520using%2520FTSE%2520100%2520stock%2520data%2520demonstrate%2520that%2520CMTF%2520achieves%2520superior%250Aperformance%2520in%2520price%2520direction%2520classification%2520compared%2520to%2520classical%2520and%2520deep%250Alearning%2520baselines.%2520These%2520findings%2520suggest%2520that%2520our%2520framework%2520is%2520an%2520effective%250Aand%2520scalable%2520solution%2520for%2520real-world%2520cross-modal%2520financial%2520forecasting%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13522v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Modal%20Temporal%20Fusion%20for%20Financial%20Market%20Forecasting&entry.906535625=Yunhua%20Pei%20and%20John%20Cartlidge%20and%20Anandadeep%20Mandal%20and%20Daniel%20Gold%20and%20Enrique%20Marcilio%20and%20Riccardo%20Mazzon&entry.1292438233=%20%20Accurate%20forecasting%20in%20financial%20markets%20requires%20integrating%20diverse%20data%0Asources%2C%20from%20historical%20prices%20to%20macroeconomic%20indicators%20and%20financial%20news.%0AHowever%2C%20existing%20models%20often%20fail%20to%20align%20these%20modalities%20effectively%2C%0Alimiting%20their%20practical%20use.%20In%20this%20paper%2C%20we%20introduce%20a%20transformer-based%0Adeep%20learning%20framework%2C%20Cross-Modal%20Temporal%20Fusion%20%28CMTF%29%2C%20that%20fuses%0Astructured%20and%20unstructured%20financial%20data%20for%20improved%20market%20prediction.%20The%0Amodel%20incorporates%20a%20tensor%20interpretation%20module%20for%20feature%20selection%20and%20an%0Aauto-training%20pipeline%20for%20efficient%20hyperparameter%20tuning.%20Experimental%0Aresults%20using%20FTSE%20100%20stock%20data%20demonstrate%20that%20CMTF%20achieves%20superior%0Aperformance%20in%20price%20direction%20classification%20compared%20to%20classical%20and%20deep%0Alearning%20baselines.%20These%20findings%20suggest%20that%20our%20framework%20is%20an%20effective%0Aand%20scalable%20solution%20for%20real-world%20cross-modal%20financial%20forecasting%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13522v2&entry.124074799=Read"},
{"title": "SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via\n  Codebooks for recommender system", "author": "Jialiang Shi and Yaguang Dou and Tian Qi", "abstract": "  Modeling multi-interests has arisen as a core problem in real-world RS.\nCurrent multi-interest retrieval methods pose three major challenges: 1)\nInterests, typically extracted from predefined external knowledge, are\ninvariant. Failed to dynamically evolve with users' real-time consumption\npreferences. 2) Online inference typically employs an over-exploited strategy,\nmainly matching users' existing interests, lacking proactive exploration and\ndiscovery of novel and long-tail interests. To address these challenges, we\npropose a novel retrieval framework named SPARC(Soft Probabilistic Adaptive\nRetrieval Model via Codebooks). Our contribution is two folds. First, the\nframework utilizes Residual Quantized Variational Autoencoder (RQ-VAE) to\nconstruct a discretized interest space. It achieves joint training of the\nRQ-VAE with the industrial large scale recommendation model, mining\nbehavior-aware interests that can perceive user feedback and evolve\ndynamically. Secondly, a probabilistic interest module that predicts the\nprobability distribution over the entire dynamic and discrete interest space.\nThis facilitates an efficient \"soft-search\" strategy during online inference,\nrevolutionizing the retrieval paradigm from \"passive matching\" to \"proactive\nexploration\" and thereby effectively promoting interest discovery. Online A/B\ntests on an industrial platform with tens of millions daily active users, have\nachieved substantial gains in business metrics: +0.9% increase in user view\nduration, +0.4% increase in user page views (PV), and a +22.7% improvement in\nPV500(new content reaching 500 PVs in 24 hours). Offline evaluations are\nconducted on open-source Amazon Product datasets. Metrics, such as Recall@K and\nNormalized Discounted Cumulative Gain@K(NDCG@K), also showed consistent\nimprovement. Both online and offline experiments validate the efficacy and\npractical value of the proposed method.\n", "link": "http://arxiv.org/abs/2508.09090v1", "date": "2025-08-12", "relevancy": 1.477, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5457}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.481}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPARC%3A%20Soft%20Probabilistic%20Adaptive%20multi-interest%20Retrieval%20Model%20via%0A%20%20Codebooks%20for%20recommender%20system&body=Title%3A%20SPARC%3A%20Soft%20Probabilistic%20Adaptive%20multi-interest%20Retrieval%20Model%20via%0A%20%20Codebooks%20for%20recommender%20system%0AAuthor%3A%20Jialiang%20Shi%20and%20Yaguang%20Dou%20and%20Tian%20Qi%0AAbstract%3A%20%20%20Modeling%20multi-interests%20has%20arisen%20as%20a%20core%20problem%20in%20real-world%20RS.%0ACurrent%20multi-interest%20retrieval%20methods%20pose%20three%20major%20challenges%3A%201%29%0AInterests%2C%20typically%20extracted%20from%20predefined%20external%20knowledge%2C%20are%0Ainvariant.%20Failed%20to%20dynamically%20evolve%20with%20users%27%20real-time%20consumption%0Apreferences.%202%29%20Online%20inference%20typically%20employs%20an%20over-exploited%20strategy%2C%0Amainly%20matching%20users%27%20existing%20interests%2C%20lacking%20proactive%20exploration%20and%0Adiscovery%20of%20novel%20and%20long-tail%20interests.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20novel%20retrieval%20framework%20named%20SPARC%28Soft%20Probabilistic%20Adaptive%0ARetrieval%20Model%20via%20Codebooks%29.%20Our%20contribution%20is%20two%20folds.%20First%2C%20the%0Aframework%20utilizes%20Residual%20Quantized%20Variational%20Autoencoder%20%28RQ-VAE%29%20to%0Aconstruct%20a%20discretized%20interest%20space.%20It%20achieves%20joint%20training%20of%20the%0ARQ-VAE%20with%20the%20industrial%20large%20scale%20recommendation%20model%2C%20mining%0Abehavior-aware%20interests%20that%20can%20perceive%20user%20feedback%20and%20evolve%0Adynamically.%20Secondly%2C%20a%20probabilistic%20interest%20module%20that%20predicts%20the%0Aprobability%20distribution%20over%20the%20entire%20dynamic%20and%20discrete%20interest%20space.%0AThis%20facilitates%20an%20efficient%20%22soft-search%22%20strategy%20during%20online%20inference%2C%0Arevolutionizing%20the%20retrieval%20paradigm%20from%20%22passive%20matching%22%20to%20%22proactive%0Aexploration%22%20and%20thereby%20effectively%20promoting%20interest%20discovery.%20Online%20A/B%0Atests%20on%20an%20industrial%20platform%20with%20tens%20of%20millions%20daily%20active%20users%2C%20have%0Aachieved%20substantial%20gains%20in%20business%20metrics%3A%20%2B0.9%25%20increase%20in%20user%20view%0Aduration%2C%20%2B0.4%25%20increase%20in%20user%20page%20views%20%28PV%29%2C%20and%20a%20%2B22.7%25%20improvement%20in%0APV500%28new%20content%20reaching%20500%20PVs%20in%2024%20hours%29.%20Offline%20evaluations%20are%0Aconducted%20on%20open-source%20Amazon%20Product%20datasets.%20Metrics%2C%20such%20as%20Recall%40K%20and%0ANormalized%20Discounted%20Cumulative%20Gain%40K%28NDCG%40K%29%2C%20also%20showed%20consistent%0Aimprovement.%20Both%20online%20and%20offline%20experiments%20validate%20the%20efficacy%20and%0Apractical%20value%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPARC%253A%2520Soft%2520Probabilistic%2520Adaptive%2520multi-interest%2520Retrieval%2520Model%2520via%250A%2520%2520Codebooks%2520for%2520recommender%2520system%26entry.906535625%3DJialiang%2520Shi%2520and%2520Yaguang%2520Dou%2520and%2520Tian%2520Qi%26entry.1292438233%3D%2520%2520Modeling%2520multi-interests%2520has%2520arisen%2520as%2520a%2520core%2520problem%2520in%2520real-world%2520RS.%250ACurrent%2520multi-interest%2520retrieval%2520methods%2520pose%2520three%2520major%2520challenges%253A%25201%2529%250AInterests%252C%2520typically%2520extracted%2520from%2520predefined%2520external%2520knowledge%252C%2520are%250Ainvariant.%2520Failed%2520to%2520dynamically%2520evolve%2520with%2520users%2527%2520real-time%2520consumption%250Apreferences.%25202%2529%2520Online%2520inference%2520typically%2520employs%2520an%2520over-exploited%2520strategy%252C%250Amainly%2520matching%2520users%2527%2520existing%2520interests%252C%2520lacking%2520proactive%2520exploration%2520and%250Adiscovery%2520of%2520novel%2520and%2520long-tail%2520interests.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520novel%2520retrieval%2520framework%2520named%2520SPARC%2528Soft%2520Probabilistic%2520Adaptive%250ARetrieval%2520Model%2520via%2520Codebooks%2529.%2520Our%2520contribution%2520is%2520two%2520folds.%2520First%252C%2520the%250Aframework%2520utilizes%2520Residual%2520Quantized%2520Variational%2520Autoencoder%2520%2528RQ-VAE%2529%2520to%250Aconstruct%2520a%2520discretized%2520interest%2520space.%2520It%2520achieves%2520joint%2520training%2520of%2520the%250ARQ-VAE%2520with%2520the%2520industrial%2520large%2520scale%2520recommendation%2520model%252C%2520mining%250Abehavior-aware%2520interests%2520that%2520can%2520perceive%2520user%2520feedback%2520and%2520evolve%250Adynamically.%2520Secondly%252C%2520a%2520probabilistic%2520interest%2520module%2520that%2520predicts%2520the%250Aprobability%2520distribution%2520over%2520the%2520entire%2520dynamic%2520and%2520discrete%2520interest%2520space.%250AThis%2520facilitates%2520an%2520efficient%2520%2522soft-search%2522%2520strategy%2520during%2520online%2520inference%252C%250Arevolutionizing%2520the%2520retrieval%2520paradigm%2520from%2520%2522passive%2520matching%2522%2520to%2520%2522proactive%250Aexploration%2522%2520and%2520thereby%2520effectively%2520promoting%2520interest%2520discovery.%2520Online%2520A/B%250Atests%2520on%2520an%2520industrial%2520platform%2520with%2520tens%2520of%2520millions%2520daily%2520active%2520users%252C%2520have%250Aachieved%2520substantial%2520gains%2520in%2520business%2520metrics%253A%2520%252B0.9%2525%2520increase%2520in%2520user%2520view%250Aduration%252C%2520%252B0.4%2525%2520increase%2520in%2520user%2520page%2520views%2520%2528PV%2529%252C%2520and%2520a%2520%252B22.7%2525%2520improvement%2520in%250APV500%2528new%2520content%2520reaching%2520500%2520PVs%2520in%252024%2520hours%2529.%2520Offline%2520evaluations%2520are%250Aconducted%2520on%2520open-source%2520Amazon%2520Product%2520datasets.%2520Metrics%252C%2520such%2520as%2520Recall%2540K%2520and%250ANormalized%2520Discounted%2520Cumulative%2520Gain%2540K%2528NDCG%2540K%2529%252C%2520also%2520showed%2520consistent%250Aimprovement.%2520Both%2520online%2520and%2520offline%2520experiments%2520validate%2520the%2520efficacy%2520and%250Apractical%2520value%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPARC%3A%20Soft%20Probabilistic%20Adaptive%20multi-interest%20Retrieval%20Model%20via%0A%20%20Codebooks%20for%20recommender%20system&entry.906535625=Jialiang%20Shi%20and%20Yaguang%20Dou%20and%20Tian%20Qi&entry.1292438233=%20%20Modeling%20multi-interests%20has%20arisen%20as%20a%20core%20problem%20in%20real-world%20RS.%0ACurrent%20multi-interest%20retrieval%20methods%20pose%20three%20major%20challenges%3A%201%29%0AInterests%2C%20typically%20extracted%20from%20predefined%20external%20knowledge%2C%20are%0Ainvariant.%20Failed%20to%20dynamically%20evolve%20with%20users%27%20real-time%20consumption%0Apreferences.%202%29%20Online%20inference%20typically%20employs%20an%20over-exploited%20strategy%2C%0Amainly%20matching%20users%27%20existing%20interests%2C%20lacking%20proactive%20exploration%20and%0Adiscovery%20of%20novel%20and%20long-tail%20interests.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20novel%20retrieval%20framework%20named%20SPARC%28Soft%20Probabilistic%20Adaptive%0ARetrieval%20Model%20via%20Codebooks%29.%20Our%20contribution%20is%20two%20folds.%20First%2C%20the%0Aframework%20utilizes%20Residual%20Quantized%20Variational%20Autoencoder%20%28RQ-VAE%29%20to%0Aconstruct%20a%20discretized%20interest%20space.%20It%20achieves%20joint%20training%20of%20the%0ARQ-VAE%20with%20the%20industrial%20large%20scale%20recommendation%20model%2C%20mining%0Abehavior-aware%20interests%20that%20can%20perceive%20user%20feedback%20and%20evolve%0Adynamically.%20Secondly%2C%20a%20probabilistic%20interest%20module%20that%20predicts%20the%0Aprobability%20distribution%20over%20the%20entire%20dynamic%20and%20discrete%20interest%20space.%0AThis%20facilitates%20an%20efficient%20%22soft-search%22%20strategy%20during%20online%20inference%2C%0Arevolutionizing%20the%20retrieval%20paradigm%20from%20%22passive%20matching%22%20to%20%22proactive%0Aexploration%22%20and%20thereby%20effectively%20promoting%20interest%20discovery.%20Online%20A/B%0Atests%20on%20an%20industrial%20platform%20with%20tens%20of%20millions%20daily%20active%20users%2C%20have%0Aachieved%20substantial%20gains%20in%20business%20metrics%3A%20%2B0.9%25%20increase%20in%20user%20view%0Aduration%2C%20%2B0.4%25%20increase%20in%20user%20page%20views%20%28PV%29%2C%20and%20a%20%2B22.7%25%20improvement%20in%0APV500%28new%20content%20reaching%20500%20PVs%20in%2024%20hours%29.%20Offline%20evaluations%20are%0Aconducted%20on%20open-source%20Amazon%20Product%20datasets.%20Metrics%2C%20such%20as%20Recall%40K%20and%0ANormalized%20Discounted%20Cumulative%20Gain%40K%28NDCG%40K%29%2C%20also%20showed%20consistent%0Aimprovement.%20Both%20online%20and%20offline%20experiments%20validate%20the%20efficacy%20and%0Apractical%20value%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09090v1&entry.124074799=Read"},
{"title": "Activation Steering for Bias Mitigation: An Interpretable Approach to\n  Safer LLMs", "author": "Shivam Dubey", "abstract": "  As large language models (LLMs) become more integrated into societal systems,\nthe risk of them perpetuating and amplifying harmful biases becomes a critical\nsafety concern. Traditional methods for mitigating bias often rely on data\nfiltering or post-hoc output moderation, which treat the model as an opaque\nblack box. In this work, we introduce a complete, end-to-end system that uses\ntechniques from mechanistic interpretability to both identify and actively\nmitigate bias directly within a model's internal workings. Our method involves\ntwo primary stages. First, we train linear \"probes\" on the internal activations\nof a model to detect the latent representations of various biases (e.g.,\ngender, race, age). Our experiments on \\texttt{gpt2-large} demonstrate that\nthese probes can identify biased content with near-perfect accuracy, revealing\nthat bias representations become most salient in the model's later layers.\nSecond, we leverage these findings to compute \"steering vectors\" by contrasting\nthe model's activation patterns for biased and neutral statements. By adding\nthese vectors during inference, we can actively steer the model's generative\nprocess away from producing harmful, stereotypical, or biased content in\nreal-time. We demonstrate the efficacy of this activation steering technique,\nshowing that it successfully alters biased completions toward more neutral\nalternatives. We present our work as a robust and reproducible system that\noffers a more direct and interpretable approach to building safer and more\naccountable LLMs.\n", "link": "http://arxiv.org/abs/2508.09019v1", "date": "2025-08-12", "relevancy": 1.4567, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5114}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Activation%20Steering%20for%20Bias%20Mitigation%3A%20An%20Interpretable%20Approach%20to%0A%20%20Safer%20LLMs&body=Title%3A%20Activation%20Steering%20for%20Bias%20Mitigation%3A%20An%20Interpretable%20Approach%20to%0A%20%20Safer%20LLMs%0AAuthor%3A%20Shivam%20Dubey%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20become%20more%20integrated%20into%20societal%20systems%2C%0Athe%20risk%20of%20them%20perpetuating%20and%20amplifying%20harmful%20biases%20becomes%20a%20critical%0Asafety%20concern.%20Traditional%20methods%20for%20mitigating%20bias%20often%20rely%20on%20data%0Afiltering%20or%20post-hoc%20output%20moderation%2C%20which%20treat%20the%20model%20as%20an%20opaque%0Ablack%20box.%20In%20this%20work%2C%20we%20introduce%20a%20complete%2C%20end-to-end%20system%20that%20uses%0Atechniques%20from%20mechanistic%20interpretability%20to%20both%20identify%20and%20actively%0Amitigate%20bias%20directly%20within%20a%20model%27s%20internal%20workings.%20Our%20method%20involves%0Atwo%20primary%20stages.%20First%2C%20we%20train%20linear%20%22probes%22%20on%20the%20internal%20activations%0Aof%20a%20model%20to%20detect%20the%20latent%20representations%20of%20various%20biases%20%28e.g.%2C%0Agender%2C%20race%2C%20age%29.%20Our%20experiments%20on%20%5Ctexttt%7Bgpt2-large%7D%20demonstrate%20that%0Athese%20probes%20can%20identify%20biased%20content%20with%20near-perfect%20accuracy%2C%20revealing%0Athat%20bias%20representations%20become%20most%20salient%20in%20the%20model%27s%20later%20layers.%0ASecond%2C%20we%20leverage%20these%20findings%20to%20compute%20%22steering%20vectors%22%20by%20contrasting%0Athe%20model%27s%20activation%20patterns%20for%20biased%20and%20neutral%20statements.%20By%20adding%0Athese%20vectors%20during%20inference%2C%20we%20can%20actively%20steer%20the%20model%27s%20generative%0Aprocess%20away%20from%20producing%20harmful%2C%20stereotypical%2C%20or%20biased%20content%20in%0Areal-time.%20We%20demonstrate%20the%20efficacy%20of%20this%20activation%20steering%20technique%2C%0Ashowing%20that%20it%20successfully%20alters%20biased%20completions%20toward%20more%20neutral%0Aalternatives.%20We%20present%20our%20work%20as%20a%20robust%20and%20reproducible%20system%20that%0Aoffers%20a%20more%20direct%20and%20interpretable%20approach%20to%20building%20safer%20and%20more%0Aaccountable%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActivation%2520Steering%2520for%2520Bias%2520Mitigation%253A%2520An%2520Interpretable%2520Approach%2520to%250A%2520%2520Safer%2520LLMs%26entry.906535625%3DShivam%2520Dubey%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520become%2520more%2520integrated%2520into%2520societal%2520systems%252C%250Athe%2520risk%2520of%2520them%2520perpetuating%2520and%2520amplifying%2520harmful%2520biases%2520becomes%2520a%2520critical%250Asafety%2520concern.%2520Traditional%2520methods%2520for%2520mitigating%2520bias%2520often%2520rely%2520on%2520data%250Afiltering%2520or%2520post-hoc%2520output%2520moderation%252C%2520which%2520treat%2520the%2520model%2520as%2520an%2520opaque%250Ablack%2520box.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520complete%252C%2520end-to-end%2520system%2520that%2520uses%250Atechniques%2520from%2520mechanistic%2520interpretability%2520to%2520both%2520identify%2520and%2520actively%250Amitigate%2520bias%2520directly%2520within%2520a%2520model%2527s%2520internal%2520workings.%2520Our%2520method%2520involves%250Atwo%2520primary%2520stages.%2520First%252C%2520we%2520train%2520linear%2520%2522probes%2522%2520on%2520the%2520internal%2520activations%250Aof%2520a%2520model%2520to%2520detect%2520the%2520latent%2520representations%2520of%2520various%2520biases%2520%2528e.g.%252C%250Agender%252C%2520race%252C%2520age%2529.%2520Our%2520experiments%2520on%2520%255Ctexttt%257Bgpt2-large%257D%2520demonstrate%2520that%250Athese%2520probes%2520can%2520identify%2520biased%2520content%2520with%2520near-perfect%2520accuracy%252C%2520revealing%250Athat%2520bias%2520representations%2520become%2520most%2520salient%2520in%2520the%2520model%2527s%2520later%2520layers.%250ASecond%252C%2520we%2520leverage%2520these%2520findings%2520to%2520compute%2520%2522steering%2520vectors%2522%2520by%2520contrasting%250Athe%2520model%2527s%2520activation%2520patterns%2520for%2520biased%2520and%2520neutral%2520statements.%2520By%2520adding%250Athese%2520vectors%2520during%2520inference%252C%2520we%2520can%2520actively%2520steer%2520the%2520model%2527s%2520generative%250Aprocess%2520away%2520from%2520producing%2520harmful%252C%2520stereotypical%252C%2520or%2520biased%2520content%2520in%250Areal-time.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520this%2520activation%2520steering%2520technique%252C%250Ashowing%2520that%2520it%2520successfully%2520alters%2520biased%2520completions%2520toward%2520more%2520neutral%250Aalternatives.%2520We%2520present%2520our%2520work%2520as%2520a%2520robust%2520and%2520reproducible%2520system%2520that%250Aoffers%2520a%2520more%2520direct%2520and%2520interpretable%2520approach%2520to%2520building%2520safer%2520and%2520more%250Aaccountable%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Activation%20Steering%20for%20Bias%20Mitigation%3A%20An%20Interpretable%20Approach%20to%0A%20%20Safer%20LLMs&entry.906535625=Shivam%20Dubey&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20become%20more%20integrated%20into%20societal%20systems%2C%0Athe%20risk%20of%20them%20perpetuating%20and%20amplifying%20harmful%20biases%20becomes%20a%20critical%0Asafety%20concern.%20Traditional%20methods%20for%20mitigating%20bias%20often%20rely%20on%20data%0Afiltering%20or%20post-hoc%20output%20moderation%2C%20which%20treat%20the%20model%20as%20an%20opaque%0Ablack%20box.%20In%20this%20work%2C%20we%20introduce%20a%20complete%2C%20end-to-end%20system%20that%20uses%0Atechniques%20from%20mechanistic%20interpretability%20to%20both%20identify%20and%20actively%0Amitigate%20bias%20directly%20within%20a%20model%27s%20internal%20workings.%20Our%20method%20involves%0Atwo%20primary%20stages.%20First%2C%20we%20train%20linear%20%22probes%22%20on%20the%20internal%20activations%0Aof%20a%20model%20to%20detect%20the%20latent%20representations%20of%20various%20biases%20%28e.g.%2C%0Agender%2C%20race%2C%20age%29.%20Our%20experiments%20on%20%5Ctexttt%7Bgpt2-large%7D%20demonstrate%20that%0Athese%20probes%20can%20identify%20biased%20content%20with%20near-perfect%20accuracy%2C%20revealing%0Athat%20bias%20representations%20become%20most%20salient%20in%20the%20model%27s%20later%20layers.%0ASecond%2C%20we%20leverage%20these%20findings%20to%20compute%20%22steering%20vectors%22%20by%20contrasting%0Athe%20model%27s%20activation%20patterns%20for%20biased%20and%20neutral%20statements.%20By%20adding%0Athese%20vectors%20during%20inference%2C%20we%20can%20actively%20steer%20the%20model%27s%20generative%0Aprocess%20away%20from%20producing%20harmful%2C%20stereotypical%2C%20or%20biased%20content%20in%0Areal-time.%20We%20demonstrate%20the%20efficacy%20of%20this%20activation%20steering%20technique%2C%0Ashowing%20that%20it%20successfully%20alters%20biased%20completions%20toward%20more%20neutral%0Aalternatives.%20We%20present%20our%20work%20as%20a%20robust%20and%20reproducible%20system%20that%0Aoffers%20a%20more%20direct%20and%20interpretable%20approach%20to%20building%20safer%20and%20more%0Aaccountable%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09019v1&entry.124074799=Read"},
{"title": "Urban-STA4CLC: Urban Theory-Informed Spatio-Temporal Attention Model for\n  Predicting Post-Disaster Commercial Land Use Change", "author": "Ziyi Guo and Yan Wang", "abstract": "  Natural disasters such as hurricanes and wildfires increasingly introduce\nunusual disturbance on economic activities, which are especially likely to\nreshape commercial land use pattern given their sensitive to customer\nvisitation. However, current modeling approaches are limited in capturing such\ncomplex interplay between human activities and commercial land use change under\nand following disturbances. Such interactions have been more effectively\ncaptured in current resilient urban planning theories. This study designs and\ncalibrates a Urban Theory-Informed Spatio-Temporal Attention Model for\nPredicting Post-Disaster Commercial Land Use Change (Urban-STA4CLC) to predict\nboth the yearly decline and expansion of commercial land use at census block\nlevel under cumulative impact of disasters on human activities over two years.\nGuided by urban theories, Urban-STA4CLC integrates both spatial and temporal\nattention mechanisms with three theory-informed modules. Resilience theory\nguides a disaster-aware temporal attention module that captures visitation\ndynamics. Spatial economic theory informs a multi-relational spatial attention\nmodule for inter-block representation. Diffusion theory contributes a\nregularization term that constrains land use transitions. The model performs\nsignificantly better than non-theoretical baselines in predicting commercial\nland use change under the scenario of recurrent hurricanes, with around 19%\nimprovement in F1 score (0.8763). The effectiveness of the theory-guided\nmodules was further validated through ablation studies. The research\ndemonstrates that embedding urban theory into commercial land use modeling\nmodels may substantially enhance the capacity to capture its gains and losses.\nThese advances in commercial land use modeling contribute to land use research\nthat accounts for cumulative impacts of recurrent disasters and shifts in\neconomic activity patterns.\n", "link": "http://arxiv.org/abs/2508.08976v1", "date": "2025-08-12", "relevancy": 1.4456, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4913}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4763}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Urban-STA4CLC%3A%20Urban%20Theory-Informed%20Spatio-Temporal%20Attention%20Model%20for%0A%20%20Predicting%20Post-Disaster%20Commercial%20Land%20Use%20Change&body=Title%3A%20Urban-STA4CLC%3A%20Urban%20Theory-Informed%20Spatio-Temporal%20Attention%20Model%20for%0A%20%20Predicting%20Post-Disaster%20Commercial%20Land%20Use%20Change%0AAuthor%3A%20Ziyi%20Guo%20and%20Yan%20Wang%0AAbstract%3A%20%20%20Natural%20disasters%20such%20as%20hurricanes%20and%20wildfires%20increasingly%20introduce%0Aunusual%20disturbance%20on%20economic%20activities%2C%20which%20are%20especially%20likely%20to%0Areshape%20commercial%20land%20use%20pattern%20given%20their%20sensitive%20to%20customer%0Avisitation.%20However%2C%20current%20modeling%20approaches%20are%20limited%20in%20capturing%20such%0Acomplex%20interplay%20between%20human%20activities%20and%20commercial%20land%20use%20change%20under%0Aand%20following%20disturbances.%20Such%20interactions%20have%20been%20more%20effectively%0Acaptured%20in%20current%20resilient%20urban%20planning%20theories.%20This%20study%20designs%20and%0Acalibrates%20a%20Urban%20Theory-Informed%20Spatio-Temporal%20Attention%20Model%20for%0APredicting%20Post-Disaster%20Commercial%20Land%20Use%20Change%20%28Urban-STA4CLC%29%20to%20predict%0Aboth%20the%20yearly%20decline%20and%20expansion%20of%20commercial%20land%20use%20at%20census%20block%0Alevel%20under%20cumulative%20impact%20of%20disasters%20on%20human%20activities%20over%20two%20years.%0AGuided%20by%20urban%20theories%2C%20Urban-STA4CLC%20integrates%20both%20spatial%20and%20temporal%0Aattention%20mechanisms%20with%20three%20theory-informed%20modules.%20Resilience%20theory%0Aguides%20a%20disaster-aware%20temporal%20attention%20module%20that%20captures%20visitation%0Adynamics.%20Spatial%20economic%20theory%20informs%20a%20multi-relational%20spatial%20attention%0Amodule%20for%20inter-block%20representation.%20Diffusion%20theory%20contributes%20a%0Aregularization%20term%20that%20constrains%20land%20use%20transitions.%20The%20model%20performs%0Asignificantly%20better%20than%20non-theoretical%20baselines%20in%20predicting%20commercial%0Aland%20use%20change%20under%20the%20scenario%20of%20recurrent%20hurricanes%2C%20with%20around%2019%25%0Aimprovement%20in%20F1%20score%20%280.8763%29.%20The%20effectiveness%20of%20the%20theory-guided%0Amodules%20was%20further%20validated%20through%20ablation%20studies.%20The%20research%0Ademonstrates%20that%20embedding%20urban%20theory%20into%20commercial%20land%20use%20modeling%0Amodels%20may%20substantially%20enhance%20the%20capacity%20to%20capture%20its%20gains%20and%20losses.%0AThese%20advances%20in%20commercial%20land%20use%20modeling%20contribute%20to%20land%20use%20research%0Athat%20accounts%20for%20cumulative%20impacts%20of%20recurrent%20disasters%20and%20shifts%20in%0Aeconomic%20activity%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrban-STA4CLC%253A%2520Urban%2520Theory-Informed%2520Spatio-Temporal%2520Attention%2520Model%2520for%250A%2520%2520Predicting%2520Post-Disaster%2520Commercial%2520Land%2520Use%2520Change%26entry.906535625%3DZiyi%2520Guo%2520and%2520Yan%2520Wang%26entry.1292438233%3D%2520%2520Natural%2520disasters%2520such%2520as%2520hurricanes%2520and%2520wildfires%2520increasingly%2520introduce%250Aunusual%2520disturbance%2520on%2520economic%2520activities%252C%2520which%2520are%2520especially%2520likely%2520to%250Areshape%2520commercial%2520land%2520use%2520pattern%2520given%2520their%2520sensitive%2520to%2520customer%250Avisitation.%2520However%252C%2520current%2520modeling%2520approaches%2520are%2520limited%2520in%2520capturing%2520such%250Acomplex%2520interplay%2520between%2520human%2520activities%2520and%2520commercial%2520land%2520use%2520change%2520under%250Aand%2520following%2520disturbances.%2520Such%2520interactions%2520have%2520been%2520more%2520effectively%250Acaptured%2520in%2520current%2520resilient%2520urban%2520planning%2520theories.%2520This%2520study%2520designs%2520and%250Acalibrates%2520a%2520Urban%2520Theory-Informed%2520Spatio-Temporal%2520Attention%2520Model%2520for%250APredicting%2520Post-Disaster%2520Commercial%2520Land%2520Use%2520Change%2520%2528Urban-STA4CLC%2529%2520to%2520predict%250Aboth%2520the%2520yearly%2520decline%2520and%2520expansion%2520of%2520commercial%2520land%2520use%2520at%2520census%2520block%250Alevel%2520under%2520cumulative%2520impact%2520of%2520disasters%2520on%2520human%2520activities%2520over%2520two%2520years.%250AGuided%2520by%2520urban%2520theories%252C%2520Urban-STA4CLC%2520integrates%2520both%2520spatial%2520and%2520temporal%250Aattention%2520mechanisms%2520with%2520three%2520theory-informed%2520modules.%2520Resilience%2520theory%250Aguides%2520a%2520disaster-aware%2520temporal%2520attention%2520module%2520that%2520captures%2520visitation%250Adynamics.%2520Spatial%2520economic%2520theory%2520informs%2520a%2520multi-relational%2520spatial%2520attention%250Amodule%2520for%2520inter-block%2520representation.%2520Diffusion%2520theory%2520contributes%2520a%250Aregularization%2520term%2520that%2520constrains%2520land%2520use%2520transitions.%2520The%2520model%2520performs%250Asignificantly%2520better%2520than%2520non-theoretical%2520baselines%2520in%2520predicting%2520commercial%250Aland%2520use%2520change%2520under%2520the%2520scenario%2520of%2520recurrent%2520hurricanes%252C%2520with%2520around%252019%2525%250Aimprovement%2520in%2520F1%2520score%2520%25280.8763%2529.%2520The%2520effectiveness%2520of%2520the%2520theory-guided%250Amodules%2520was%2520further%2520validated%2520through%2520ablation%2520studies.%2520The%2520research%250Ademonstrates%2520that%2520embedding%2520urban%2520theory%2520into%2520commercial%2520land%2520use%2520modeling%250Amodels%2520may%2520substantially%2520enhance%2520the%2520capacity%2520to%2520capture%2520its%2520gains%2520and%2520losses.%250AThese%2520advances%2520in%2520commercial%2520land%2520use%2520modeling%2520contribute%2520to%2520land%2520use%2520research%250Athat%2520accounts%2520for%2520cumulative%2520impacts%2520of%2520recurrent%2520disasters%2520and%2520shifts%2520in%250Aeconomic%2520activity%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Urban-STA4CLC%3A%20Urban%20Theory-Informed%20Spatio-Temporal%20Attention%20Model%20for%0A%20%20Predicting%20Post-Disaster%20Commercial%20Land%20Use%20Change&entry.906535625=Ziyi%20Guo%20and%20Yan%20Wang&entry.1292438233=%20%20Natural%20disasters%20such%20as%20hurricanes%20and%20wildfires%20increasingly%20introduce%0Aunusual%20disturbance%20on%20economic%20activities%2C%20which%20are%20especially%20likely%20to%0Areshape%20commercial%20land%20use%20pattern%20given%20their%20sensitive%20to%20customer%0Avisitation.%20However%2C%20current%20modeling%20approaches%20are%20limited%20in%20capturing%20such%0Acomplex%20interplay%20between%20human%20activities%20and%20commercial%20land%20use%20change%20under%0Aand%20following%20disturbances.%20Such%20interactions%20have%20been%20more%20effectively%0Acaptured%20in%20current%20resilient%20urban%20planning%20theories.%20This%20study%20designs%20and%0Acalibrates%20a%20Urban%20Theory-Informed%20Spatio-Temporal%20Attention%20Model%20for%0APredicting%20Post-Disaster%20Commercial%20Land%20Use%20Change%20%28Urban-STA4CLC%29%20to%20predict%0Aboth%20the%20yearly%20decline%20and%20expansion%20of%20commercial%20land%20use%20at%20census%20block%0Alevel%20under%20cumulative%20impact%20of%20disasters%20on%20human%20activities%20over%20two%20years.%0AGuided%20by%20urban%20theories%2C%20Urban-STA4CLC%20integrates%20both%20spatial%20and%20temporal%0Aattention%20mechanisms%20with%20three%20theory-informed%20modules.%20Resilience%20theory%0Aguides%20a%20disaster-aware%20temporal%20attention%20module%20that%20captures%20visitation%0Adynamics.%20Spatial%20economic%20theory%20informs%20a%20multi-relational%20spatial%20attention%0Amodule%20for%20inter-block%20representation.%20Diffusion%20theory%20contributes%20a%0Aregularization%20term%20that%20constrains%20land%20use%20transitions.%20The%20model%20performs%0Asignificantly%20better%20than%20non-theoretical%20baselines%20in%20predicting%20commercial%0Aland%20use%20change%20under%20the%20scenario%20of%20recurrent%20hurricanes%2C%20with%20around%2019%25%0Aimprovement%20in%20F1%20score%20%280.8763%29.%20The%20effectiveness%20of%20the%20theory-guided%0Amodules%20was%20further%20validated%20through%20ablation%20studies.%20The%20research%0Ademonstrates%20that%20embedding%20urban%20theory%20into%20commercial%20land%20use%20modeling%0Amodels%20may%20substantially%20enhance%20the%20capacity%20to%20capture%20its%20gains%20and%20losses.%0AThese%20advances%20in%20commercial%20land%20use%20modeling%20contribute%20to%20land%20use%20research%0Athat%20accounts%20for%20cumulative%20impacts%20of%20recurrent%20disasters%20and%20shifts%20in%0Aeconomic%20activity%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08976v1&entry.124074799=Read"},
{"title": "RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM\n  Systems with Structured Memory", "author": "Jun Liu and Zhenglun Kong and Changdi Yang and Fan Yang and Tianqi Li and Peiyan Dong and Joannah Nanjekye and Hao Tang and Geng Yuan and Wei Niu and Wenbin Zhang and Pu Zhao and Xue Lin and Dong Huang and Yanzhi Wang", "abstract": "  Multi-agent large language model (LLM) systems have shown strong potential in\ncomplex reasoning and collaborative decision-making tasks. However, most\nexisting coordination schemes rely on static or full-context routing\nstrategies, which lead to excessive token consumption, redundant memory\nexposure, and limited adaptability across interaction rounds. We introduce\nRCR-Router, a modular and role-aware context routing framework designed to\nenable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,\nthis is the first routing approach that dynamically selects semantically\nrelevant memory subsets for each agent based on its role and task stage, while\nadhering to a strict token budget. A lightweight scoring policy guides memory\nselection, and agent outputs are iteratively integrated into a shared memory\nstore to facilitate progressive context refinement. To better evaluate model\nbehavior, we further propose an Answer Quality Score metric that captures\nLLM-generated explanations beyond standard QA accuracy. Experiments on three\nmulti-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate\nthat RCR-Router reduces token usage (up to 30%) while improving or maintaining\nanswer quality. These results highlight the importance of structured memory\nrouting and output-aware evaluation in advancing scalable multi-agent LLM\nsystems.\n", "link": "http://arxiv.org/abs/2508.04903v3", "date": "2025-08-12", "relevancy": 1.4304, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5461}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4807}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RCR-Router%3A%20Efficient%20Role-Aware%20Context%20Routing%20for%20Multi-Agent%20LLM%0A%20%20Systems%20with%20Structured%20Memory&body=Title%3A%20RCR-Router%3A%20Efficient%20Role-Aware%20Context%20Routing%20for%20Multi-Agent%20LLM%0A%20%20Systems%20with%20Structured%20Memory%0AAuthor%3A%20Jun%20Liu%20and%20Zhenglun%20Kong%20and%20Changdi%20Yang%20and%20Fan%20Yang%20and%20Tianqi%20Li%20and%20Peiyan%20Dong%20and%20Joannah%20Nanjekye%20and%20Hao%20Tang%20and%20Geng%20Yuan%20and%20Wei%20Niu%20and%20Wenbin%20Zhang%20and%20Pu%20Zhao%20and%20Xue%20Lin%20and%20Dong%20Huang%20and%20Yanzhi%20Wang%0AAbstract%3A%20%20%20Multi-agent%20large%20language%20model%20%28LLM%29%20systems%20have%20shown%20strong%20potential%20in%0Acomplex%20reasoning%20and%20collaborative%20decision-making%20tasks.%20However%2C%20most%0Aexisting%20coordination%20schemes%20rely%20on%20static%20or%20full-context%20routing%0Astrategies%2C%20which%20lead%20to%20excessive%20token%20consumption%2C%20redundant%20memory%0Aexposure%2C%20and%20limited%20adaptability%20across%20interaction%20rounds.%20We%20introduce%0ARCR-Router%2C%20a%20modular%20and%20role-aware%20context%20routing%20framework%20designed%20to%0Aenable%20efficient%2C%20adaptive%20collaboration%20in%20multi-agent%20LLMs.%20To%20our%20knowledge%2C%0Athis%20is%20the%20first%20routing%20approach%20that%20dynamically%20selects%20semantically%0Arelevant%20memory%20subsets%20for%20each%20agent%20based%20on%20its%20role%20and%20task%20stage%2C%20while%0Aadhering%20to%20a%20strict%20token%20budget.%20A%20lightweight%20scoring%20policy%20guides%20memory%0Aselection%2C%20and%20agent%20outputs%20are%20iteratively%20integrated%20into%20a%20shared%20memory%0Astore%20to%20facilitate%20progressive%20context%20refinement.%20To%20better%20evaluate%20model%0Abehavior%2C%20we%20further%20propose%20an%20Answer%20Quality%20Score%20metric%20that%20captures%0ALLM-generated%20explanations%20beyond%20standard%20QA%20accuracy.%20Experiments%20on%20three%0Amulti-hop%20QA%20benchmarks%20--%20HotPotQA%2C%20MuSiQue%2C%20and%202WikiMultihop%20--%20demonstrate%0Athat%20RCR-Router%20reduces%20token%20usage%20%28up%20to%2030%25%29%20while%20improving%20or%20maintaining%0Aanswer%20quality.%20These%20results%20highlight%20the%20importance%20of%20structured%20memory%0Arouting%20and%20output-aware%20evaluation%20in%20advancing%20scalable%20multi-agent%20LLM%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04903v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRCR-Router%253A%2520Efficient%2520Role-Aware%2520Context%2520Routing%2520for%2520Multi-Agent%2520LLM%250A%2520%2520Systems%2520with%2520Structured%2520Memory%26entry.906535625%3DJun%2520Liu%2520and%2520Zhenglun%2520Kong%2520and%2520Changdi%2520Yang%2520and%2520Fan%2520Yang%2520and%2520Tianqi%2520Li%2520and%2520Peiyan%2520Dong%2520and%2520Joannah%2520Nanjekye%2520and%2520Hao%2520Tang%2520and%2520Geng%2520Yuan%2520and%2520Wei%2520Niu%2520and%2520Wenbin%2520Zhang%2520and%2520Pu%2520Zhao%2520and%2520Xue%2520Lin%2520and%2520Dong%2520Huang%2520and%2520Yanzhi%2520Wang%26entry.1292438233%3D%2520%2520Multi-agent%2520large%2520language%2520model%2520%2528LLM%2529%2520systems%2520have%2520shown%2520strong%2520potential%2520in%250Acomplex%2520reasoning%2520and%2520collaborative%2520decision-making%2520tasks.%2520However%252C%2520most%250Aexisting%2520coordination%2520schemes%2520rely%2520on%2520static%2520or%2520full-context%2520routing%250Astrategies%252C%2520which%2520lead%2520to%2520excessive%2520token%2520consumption%252C%2520redundant%2520memory%250Aexposure%252C%2520and%2520limited%2520adaptability%2520across%2520interaction%2520rounds.%2520We%2520introduce%250ARCR-Router%252C%2520a%2520modular%2520and%2520role-aware%2520context%2520routing%2520framework%2520designed%2520to%250Aenable%2520efficient%252C%2520adaptive%2520collaboration%2520in%2520multi-agent%2520LLMs.%2520To%2520our%2520knowledge%252C%250Athis%2520is%2520the%2520first%2520routing%2520approach%2520that%2520dynamically%2520selects%2520semantically%250Arelevant%2520memory%2520subsets%2520for%2520each%2520agent%2520based%2520on%2520its%2520role%2520and%2520task%2520stage%252C%2520while%250Aadhering%2520to%2520a%2520strict%2520token%2520budget.%2520A%2520lightweight%2520scoring%2520policy%2520guides%2520memory%250Aselection%252C%2520and%2520agent%2520outputs%2520are%2520iteratively%2520integrated%2520into%2520a%2520shared%2520memory%250Astore%2520to%2520facilitate%2520progressive%2520context%2520refinement.%2520To%2520better%2520evaluate%2520model%250Abehavior%252C%2520we%2520further%2520propose%2520an%2520Answer%2520Quality%2520Score%2520metric%2520that%2520captures%250ALLM-generated%2520explanations%2520beyond%2520standard%2520QA%2520accuracy.%2520Experiments%2520on%2520three%250Amulti-hop%2520QA%2520benchmarks%2520--%2520HotPotQA%252C%2520MuSiQue%252C%2520and%25202WikiMultihop%2520--%2520demonstrate%250Athat%2520RCR-Router%2520reduces%2520token%2520usage%2520%2528up%2520to%252030%2525%2529%2520while%2520improving%2520or%2520maintaining%250Aanswer%2520quality.%2520These%2520results%2520highlight%2520the%2520importance%2520of%2520structured%2520memory%250Arouting%2520and%2520output-aware%2520evaluation%2520in%2520advancing%2520scalable%2520multi-agent%2520LLM%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04903v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RCR-Router%3A%20Efficient%20Role-Aware%20Context%20Routing%20for%20Multi-Agent%20LLM%0A%20%20Systems%20with%20Structured%20Memory&entry.906535625=Jun%20Liu%20and%20Zhenglun%20Kong%20and%20Changdi%20Yang%20and%20Fan%20Yang%20and%20Tianqi%20Li%20and%20Peiyan%20Dong%20and%20Joannah%20Nanjekye%20and%20Hao%20Tang%20and%20Geng%20Yuan%20and%20Wei%20Niu%20and%20Wenbin%20Zhang%20and%20Pu%20Zhao%20and%20Xue%20Lin%20and%20Dong%20Huang%20and%20Yanzhi%20Wang&entry.1292438233=%20%20Multi-agent%20large%20language%20model%20%28LLM%29%20systems%20have%20shown%20strong%20potential%20in%0Acomplex%20reasoning%20and%20collaborative%20decision-making%20tasks.%20However%2C%20most%0Aexisting%20coordination%20schemes%20rely%20on%20static%20or%20full-context%20routing%0Astrategies%2C%20which%20lead%20to%20excessive%20token%20consumption%2C%20redundant%20memory%0Aexposure%2C%20and%20limited%20adaptability%20across%20interaction%20rounds.%20We%20introduce%0ARCR-Router%2C%20a%20modular%20and%20role-aware%20context%20routing%20framework%20designed%20to%0Aenable%20efficient%2C%20adaptive%20collaboration%20in%20multi-agent%20LLMs.%20To%20our%20knowledge%2C%0Athis%20is%20the%20first%20routing%20approach%20that%20dynamically%20selects%20semantically%0Arelevant%20memory%20subsets%20for%20each%20agent%20based%20on%20its%20role%20and%20task%20stage%2C%20while%0Aadhering%20to%20a%20strict%20token%20budget.%20A%20lightweight%20scoring%20policy%20guides%20memory%0Aselection%2C%20and%20agent%20outputs%20are%20iteratively%20integrated%20into%20a%20shared%20memory%0Astore%20to%20facilitate%20progressive%20context%20refinement.%20To%20better%20evaluate%20model%0Abehavior%2C%20we%20further%20propose%20an%20Answer%20Quality%20Score%20metric%20that%20captures%0ALLM-generated%20explanations%20beyond%20standard%20QA%20accuracy.%20Experiments%20on%20three%0Amulti-hop%20QA%20benchmarks%20--%20HotPotQA%2C%20MuSiQue%2C%20and%202WikiMultihop%20--%20demonstrate%0Athat%20RCR-Router%20reduces%20token%20usage%20%28up%20to%2030%25%29%20while%20improving%20or%20maintaining%0Aanswer%20quality.%20These%20results%20highlight%20the%20importance%20of%20structured%20memory%0Arouting%20and%20output-aware%20evaluation%20in%20advancing%20scalable%20multi-agent%20LLM%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04903v3&entry.124074799=Read"},
{"title": "Attacks and Defenses Against LLM Fingerprinting", "author": "Kevin Kurian and Ethan Holland and Sean Oesch", "abstract": "  As large language models are increasingly deployed in sensitive environments,\nfingerprinting attacks pose significant privacy and security risks. We present\na study of LLM fingerprinting from both offensive and defensive perspectives.\nOur attack methodology uses reinforcement learning to automatically optimize\nquery selection, achieving better fingerprinting accuracy with only 3 queries\ncompared to randomly selecting 3 queries from the same pool. Our defensive\napproach employs semantic-preserving output filtering through a secondary LLM\nto obfuscate model identity while maintaining semantic integrity. The defensive\nmethod reduces fingerprinting accuracy across tested models while preserving\noutput quality. These contributions show the potential to improve\nfingerprinting tools capabilities while providing practical mitigation\nstrategies against fingerprinting attacks.\n", "link": "http://arxiv.org/abs/2508.09021v1", "date": "2025-08-12", "relevancy": 1.4133, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4873}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4648}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attacks%20and%20Defenses%20Against%20LLM%20Fingerprinting&body=Title%3A%20Attacks%20and%20Defenses%20Against%20LLM%20Fingerprinting%0AAuthor%3A%20Kevin%20Kurian%20and%20Ethan%20Holland%20and%20Sean%20Oesch%0AAbstract%3A%20%20%20As%20large%20language%20models%20are%20increasingly%20deployed%20in%20sensitive%20environments%2C%0Afingerprinting%20attacks%20pose%20significant%20privacy%20and%20security%20risks.%20We%20present%0Aa%20study%20of%20LLM%20fingerprinting%20from%20both%20offensive%20and%20defensive%20perspectives.%0AOur%20attack%20methodology%20uses%20reinforcement%20learning%20to%20automatically%20optimize%0Aquery%20selection%2C%20achieving%20better%20fingerprinting%20accuracy%20with%20only%203%20queries%0Acompared%20to%20randomly%20selecting%203%20queries%20from%20the%20same%20pool.%20Our%20defensive%0Aapproach%20employs%20semantic-preserving%20output%20filtering%20through%20a%20secondary%20LLM%0Ato%20obfuscate%20model%20identity%20while%20maintaining%20semantic%20integrity.%20The%20defensive%0Amethod%20reduces%20fingerprinting%20accuracy%20across%20tested%20models%20while%20preserving%0Aoutput%20quality.%20These%20contributions%20show%20the%20potential%20to%20improve%0Afingerprinting%20tools%20capabilities%20while%20providing%20practical%20mitigation%0Astrategies%20against%20fingerprinting%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttacks%2520and%2520Defenses%2520Against%2520LLM%2520Fingerprinting%26entry.906535625%3DKevin%2520Kurian%2520and%2520Ethan%2520Holland%2520and%2520Sean%2520Oesch%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520are%2520increasingly%2520deployed%2520in%2520sensitive%2520environments%252C%250Afingerprinting%2520attacks%2520pose%2520significant%2520privacy%2520and%2520security%2520risks.%2520We%2520present%250Aa%2520study%2520of%2520LLM%2520fingerprinting%2520from%2520both%2520offensive%2520and%2520defensive%2520perspectives.%250AOur%2520attack%2520methodology%2520uses%2520reinforcement%2520learning%2520to%2520automatically%2520optimize%250Aquery%2520selection%252C%2520achieving%2520better%2520fingerprinting%2520accuracy%2520with%2520only%25203%2520queries%250Acompared%2520to%2520randomly%2520selecting%25203%2520queries%2520from%2520the%2520same%2520pool.%2520Our%2520defensive%250Aapproach%2520employs%2520semantic-preserving%2520output%2520filtering%2520through%2520a%2520secondary%2520LLM%250Ato%2520obfuscate%2520model%2520identity%2520while%2520maintaining%2520semantic%2520integrity.%2520The%2520defensive%250Amethod%2520reduces%2520fingerprinting%2520accuracy%2520across%2520tested%2520models%2520while%2520preserving%250Aoutput%2520quality.%2520These%2520contributions%2520show%2520the%2520potential%2520to%2520improve%250Afingerprinting%2520tools%2520capabilities%2520while%2520providing%2520practical%2520mitigation%250Astrategies%2520against%2520fingerprinting%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attacks%20and%20Defenses%20Against%20LLM%20Fingerprinting&entry.906535625=Kevin%20Kurian%20and%20Ethan%20Holland%20and%20Sean%20Oesch&entry.1292438233=%20%20As%20large%20language%20models%20are%20increasingly%20deployed%20in%20sensitive%20environments%2C%0Afingerprinting%20attacks%20pose%20significant%20privacy%20and%20security%20risks.%20We%20present%0Aa%20study%20of%20LLM%20fingerprinting%20from%20both%20offensive%20and%20defensive%20perspectives.%0AOur%20attack%20methodology%20uses%20reinforcement%20learning%20to%20automatically%20optimize%0Aquery%20selection%2C%20achieving%20better%20fingerprinting%20accuracy%20with%20only%203%20queries%0Acompared%20to%20randomly%20selecting%203%20queries%20from%20the%20same%20pool.%20Our%20defensive%0Aapproach%20employs%20semantic-preserving%20output%20filtering%20through%20a%20secondary%20LLM%0Ato%20obfuscate%20model%20identity%20while%20maintaining%20semantic%20integrity.%20The%20defensive%0Amethod%20reduces%20fingerprinting%20accuracy%20across%20tested%20models%20while%20preserving%0Aoutput%20quality.%20These%20contributions%20show%20the%20potential%20to%20improve%0Afingerprinting%20tools%20capabilities%20while%20providing%20practical%20mitigation%0Astrategies%20against%20fingerprinting%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09021v1&entry.124074799=Read"},
{"title": "A First Look at Predictability and Explainability of Pre-request\n  Passenger Waiting Time in Ridesharing Systems", "author": "Jie Wang and Guang Wang", "abstract": "  Passenger waiting time prediction plays a critical role in enhancing both\nridesharing user experience and platform efficiency. While most existing\nresearch focuses on post-request waiting time prediction with knowing the\nmatched driver information, pre-request waiting time prediction (i.e., before\nsubmitting a ride request and without matching a driver) is also important, as\nit enables passengers to plan their trips more effectively and enhance the\nexperience of both passengers and drivers. However, it has not been fully\nstudied by existing works. In this paper, we take the first step toward\nunderstanding the predictability and explainability of pre-request passenger\nwaiting time in ridesharing systems. Particularly, we conduct an in-depth\ndata-driven study to investigate the impact of demand&supply dynamics on\npassenger waiting time. Based on this analysis and feature engineering, we\npropose FiXGBoost, a novel feature interaction-based XGBoost model designed to\npredict waiting time without knowing the assigned driver information. We\nfurther perform an importance analysis to quantify the contribution of each\nfactor. Experiments on a large-scale real-world ridesharing dataset including\nover 30 million trip records show that our FiXGBoost can achieve a good\nperformance for pre-request passenger waiting time prediction with high\nexplainability.\n", "link": "http://arxiv.org/abs/2508.09027v1", "date": "2025-08-12", "relevancy": 0.7876, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4133}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.3855}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20First%20Look%20at%20Predictability%20and%20Explainability%20of%20Pre-request%0A%20%20Passenger%20Waiting%20Time%20in%20Ridesharing%20Systems&body=Title%3A%20A%20First%20Look%20at%20Predictability%20and%20Explainability%20of%20Pre-request%0A%20%20Passenger%20Waiting%20Time%20in%20Ridesharing%20Systems%0AAuthor%3A%20Jie%20Wang%20and%20Guang%20Wang%0AAbstract%3A%20%20%20Passenger%20waiting%20time%20prediction%20plays%20a%20critical%20role%20in%20enhancing%20both%0Aridesharing%20user%20experience%20and%20platform%20efficiency.%20While%20most%20existing%0Aresearch%20focuses%20on%20post-request%20waiting%20time%20prediction%20with%20knowing%20the%0Amatched%20driver%20information%2C%20pre-request%20waiting%20time%20prediction%20%28i.e.%2C%20before%0Asubmitting%20a%20ride%20request%20and%20without%20matching%20a%20driver%29%20is%20also%20important%2C%20as%0Ait%20enables%20passengers%20to%20plan%20their%20trips%20more%20effectively%20and%20enhance%20the%0Aexperience%20of%20both%20passengers%20and%20drivers.%20However%2C%20it%20has%20not%20been%20fully%0Astudied%20by%20existing%20works.%20In%20this%20paper%2C%20we%20take%20the%20first%20step%20toward%0Aunderstanding%20the%20predictability%20and%20explainability%20of%20pre-request%20passenger%0Awaiting%20time%20in%20ridesharing%20systems.%20Particularly%2C%20we%20conduct%20an%20in-depth%0Adata-driven%20study%20to%20investigate%20the%20impact%20of%20demand%26supply%20dynamics%20on%0Apassenger%20waiting%20time.%20Based%20on%20this%20analysis%20and%20feature%20engineering%2C%20we%0Apropose%20FiXGBoost%2C%20a%20novel%20feature%20interaction-based%20XGBoost%20model%20designed%20to%0Apredict%20waiting%20time%20without%20knowing%20the%20assigned%20driver%20information.%20We%0Afurther%20perform%20an%20importance%20analysis%20to%20quantify%20the%20contribution%20of%20each%0Afactor.%20Experiments%20on%20a%20large-scale%20real-world%20ridesharing%20dataset%20including%0Aover%2030%20million%20trip%20records%20show%20that%20our%20FiXGBoost%20can%20achieve%20a%20good%0Aperformance%20for%20pre-request%20passenger%20waiting%20time%20prediction%20with%20high%0Aexplainability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09027v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520First%2520Look%2520at%2520Predictability%2520and%2520Explainability%2520of%2520Pre-request%250A%2520%2520Passenger%2520Waiting%2520Time%2520in%2520Ridesharing%2520Systems%26entry.906535625%3DJie%2520Wang%2520and%2520Guang%2520Wang%26entry.1292438233%3D%2520%2520Passenger%2520waiting%2520time%2520prediction%2520plays%2520a%2520critical%2520role%2520in%2520enhancing%2520both%250Aridesharing%2520user%2520experience%2520and%2520platform%2520efficiency.%2520While%2520most%2520existing%250Aresearch%2520focuses%2520on%2520post-request%2520waiting%2520time%2520prediction%2520with%2520knowing%2520the%250Amatched%2520driver%2520information%252C%2520pre-request%2520waiting%2520time%2520prediction%2520%2528i.e.%252C%2520before%250Asubmitting%2520a%2520ride%2520request%2520and%2520without%2520matching%2520a%2520driver%2529%2520is%2520also%2520important%252C%2520as%250Ait%2520enables%2520passengers%2520to%2520plan%2520their%2520trips%2520more%2520effectively%2520and%2520enhance%2520the%250Aexperience%2520of%2520both%2520passengers%2520and%2520drivers.%2520However%252C%2520it%2520has%2520not%2520been%2520fully%250Astudied%2520by%2520existing%2520works.%2520In%2520this%2520paper%252C%2520we%2520take%2520the%2520first%2520step%2520toward%250Aunderstanding%2520the%2520predictability%2520and%2520explainability%2520of%2520pre-request%2520passenger%250Awaiting%2520time%2520in%2520ridesharing%2520systems.%2520Particularly%252C%2520we%2520conduct%2520an%2520in-depth%250Adata-driven%2520study%2520to%2520investigate%2520the%2520impact%2520of%2520demand%2526supply%2520dynamics%2520on%250Apassenger%2520waiting%2520time.%2520Based%2520on%2520this%2520analysis%2520and%2520feature%2520engineering%252C%2520we%250Apropose%2520FiXGBoost%252C%2520a%2520novel%2520feature%2520interaction-based%2520XGBoost%2520model%2520designed%2520to%250Apredict%2520waiting%2520time%2520without%2520knowing%2520the%2520assigned%2520driver%2520information.%2520We%250Afurther%2520perform%2520an%2520importance%2520analysis%2520to%2520quantify%2520the%2520contribution%2520of%2520each%250Afactor.%2520Experiments%2520on%2520a%2520large-scale%2520real-world%2520ridesharing%2520dataset%2520including%250Aover%252030%2520million%2520trip%2520records%2520show%2520that%2520our%2520FiXGBoost%2520can%2520achieve%2520a%2520good%250Aperformance%2520for%2520pre-request%2520passenger%2520waiting%2520time%2520prediction%2520with%2520high%250Aexplainability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09027v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20First%20Look%20at%20Predictability%20and%20Explainability%20of%20Pre-request%0A%20%20Passenger%20Waiting%20Time%20in%20Ridesharing%20Systems&entry.906535625=Jie%20Wang%20and%20Guang%20Wang&entry.1292438233=%20%20Passenger%20waiting%20time%20prediction%20plays%20a%20critical%20role%20in%20enhancing%20both%0Aridesharing%20user%20experience%20and%20platform%20efficiency.%20While%20most%20existing%0Aresearch%20focuses%20on%20post-request%20waiting%20time%20prediction%20with%20knowing%20the%0Amatched%20driver%20information%2C%20pre-request%20waiting%20time%20prediction%20%28i.e.%2C%20before%0Asubmitting%20a%20ride%20request%20and%20without%20matching%20a%20driver%29%20is%20also%20important%2C%20as%0Ait%20enables%20passengers%20to%20plan%20their%20trips%20more%20effectively%20and%20enhance%20the%0Aexperience%20of%20both%20passengers%20and%20drivers.%20However%2C%20it%20has%20not%20been%20fully%0Astudied%20by%20existing%20works.%20In%20this%20paper%2C%20we%20take%20the%20first%20step%20toward%0Aunderstanding%20the%20predictability%20and%20explainability%20of%20pre-request%20passenger%0Awaiting%20time%20in%20ridesharing%20systems.%20Particularly%2C%20we%20conduct%20an%20in-depth%0Adata-driven%20study%20to%20investigate%20the%20impact%20of%20demand%26supply%20dynamics%20on%0Apassenger%20waiting%20time.%20Based%20on%20this%20analysis%20and%20feature%20engineering%2C%20we%0Apropose%20FiXGBoost%2C%20a%20novel%20feature%20interaction-based%20XGBoost%20model%20designed%20to%0Apredict%20waiting%20time%20without%20knowing%20the%20assigned%20driver%20information.%20We%0Afurther%20perform%20an%20importance%20analysis%20to%20quantify%20the%20contribution%20of%20each%0Afactor.%20Experiments%20on%20a%20large-scale%20real-world%20ridesharing%20dataset%20including%0Aover%2030%20million%20trip%20records%20show%20that%20our%20FiXGBoost%20can%20achieve%20a%20good%0Aperformance%20for%20pre-request%20passenger%20waiting%20time%20prediction%20with%20high%0Aexplainability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09027v1&entry.124074799=Read"},
{"title": "Scaling Up Active Testing to Large Language Models", "author": "Gabrielle Berrada and Jannik Kossen and Muhammed Razzak and Freddie Bickford Smith and Yarin Gal and Tom Rainforth", "abstract": "  Active testing enables label-efficient evaluation of models through careful\ndata acquisition. However, its significant computational costs have previously\nundermined its use for large models. We show how it can be successfully scaled\nup to the evaluation of large language models (LLMs). In particular we show\nthat the surrogate model used to guide data acquisition can be constructed\ncheaply using in-context learning, does not require updating within an\nactive-testing loop, and can be smaller than the target model. We even find we\ncan make good data-acquisition decisions without computing predictions with the\ntarget model and further introduce a single-run error estimator to asses how\nwell active testing is working on the fly. We find that our approach is able to\nmore effectively evaluate LLM performance with less data than current standard\npractices.\n", "link": "http://arxiv.org/abs/2508.09093v1", "date": "2025-08-12", "relevancy": 1.372, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4897}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4496}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Up%20Active%20Testing%20to%20Large%20Language%20Models&body=Title%3A%20Scaling%20Up%20Active%20Testing%20to%20Large%20Language%20Models%0AAuthor%3A%20Gabrielle%20Berrada%20and%20Jannik%20Kossen%20and%20Muhammed%20Razzak%20and%20Freddie%20Bickford%20Smith%20and%20Yarin%20Gal%20and%20Tom%20Rainforth%0AAbstract%3A%20%20%20Active%20testing%20enables%20label-efficient%20evaluation%20of%20models%20through%20careful%0Adata%20acquisition.%20However%2C%20its%20significant%20computational%20costs%20have%20previously%0Aundermined%20its%20use%20for%20large%20models.%20We%20show%20how%20it%20can%20be%20successfully%20scaled%0Aup%20to%20the%20evaluation%20of%20large%20language%20models%20%28LLMs%29.%20In%20particular%20we%20show%0Athat%20the%20surrogate%20model%20used%20to%20guide%20data%20acquisition%20can%20be%20constructed%0Acheaply%20using%20in-context%20learning%2C%20does%20not%20require%20updating%20within%20an%0Aactive-testing%20loop%2C%20and%20can%20be%20smaller%20than%20the%20target%20model.%20We%20even%20find%20we%0Acan%20make%20good%20data-acquisition%20decisions%20without%20computing%20predictions%20with%20the%0Atarget%20model%20and%20further%20introduce%20a%20single-run%20error%20estimator%20to%20asses%20how%0Awell%20active%20testing%20is%20working%20on%20the%20fly.%20We%20find%20that%20our%20approach%20is%20able%20to%0Amore%20effectively%20evaluate%20LLM%20performance%20with%20less%20data%20than%20current%20standard%0Apractices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Up%2520Active%2520Testing%2520to%2520Large%2520Language%2520Models%26entry.906535625%3DGabrielle%2520Berrada%2520and%2520Jannik%2520Kossen%2520and%2520Muhammed%2520Razzak%2520and%2520Freddie%2520Bickford%2520Smith%2520and%2520Yarin%2520Gal%2520and%2520Tom%2520Rainforth%26entry.1292438233%3D%2520%2520Active%2520testing%2520enables%2520label-efficient%2520evaluation%2520of%2520models%2520through%2520careful%250Adata%2520acquisition.%2520However%252C%2520its%2520significant%2520computational%2520costs%2520have%2520previously%250Aundermined%2520its%2520use%2520for%2520large%2520models.%2520We%2520show%2520how%2520it%2520can%2520be%2520successfully%2520scaled%250Aup%2520to%2520the%2520evaluation%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520In%2520particular%2520we%2520show%250Athat%2520the%2520surrogate%2520model%2520used%2520to%2520guide%2520data%2520acquisition%2520can%2520be%2520constructed%250Acheaply%2520using%2520in-context%2520learning%252C%2520does%2520not%2520require%2520updating%2520within%2520an%250Aactive-testing%2520loop%252C%2520and%2520can%2520be%2520smaller%2520than%2520the%2520target%2520model.%2520We%2520even%2520find%2520we%250Acan%2520make%2520good%2520data-acquisition%2520decisions%2520without%2520computing%2520predictions%2520with%2520the%250Atarget%2520model%2520and%2520further%2520introduce%2520a%2520single-run%2520error%2520estimator%2520to%2520asses%2520how%250Awell%2520active%2520testing%2520is%2520working%2520on%2520the%2520fly.%2520We%2520find%2520that%2520our%2520approach%2520is%2520able%2520to%250Amore%2520effectively%2520evaluate%2520LLM%2520performance%2520with%2520less%2520data%2520than%2520current%2520standard%250Apractices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Up%20Active%20Testing%20to%20Large%20Language%20Models&entry.906535625=Gabrielle%20Berrada%20and%20Jannik%20Kossen%20and%20Muhammed%20Razzak%20and%20Freddie%20Bickford%20Smith%20and%20Yarin%20Gal%20and%20Tom%20Rainforth&entry.1292438233=%20%20Active%20testing%20enables%20label-efficient%20evaluation%20of%20models%20through%20careful%0Adata%20acquisition.%20However%2C%20its%20significant%20computational%20costs%20have%20previously%0Aundermined%20its%20use%20for%20large%20models.%20We%20show%20how%20it%20can%20be%20successfully%20scaled%0Aup%20to%20the%20evaluation%20of%20large%20language%20models%20%28LLMs%29.%20In%20particular%20we%20show%0Athat%20the%20surrogate%20model%20used%20to%20guide%20data%20acquisition%20can%20be%20constructed%0Acheaply%20using%20in-context%20learning%2C%20does%20not%20require%20updating%20within%20an%0Aactive-testing%20loop%2C%20and%20can%20be%20smaller%20than%20the%20target%20model.%20We%20even%20find%20we%0Acan%20make%20good%20data-acquisition%20decisions%20without%20computing%20predictions%20with%20the%0Atarget%20model%20and%20further%20introduce%20a%20single-run%20error%20estimator%20to%20asses%20how%0Awell%20active%20testing%20is%20working%20on%20the%20fly.%20We%20find%20that%20our%20approach%20is%20able%20to%0Amore%20effectively%20evaluate%20LLM%20performance%20with%20less%20data%20than%20current%20standard%0Apractices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09093v1&entry.124074799=Read"},
{"title": "Efficient and Effective Query Context-Aware Learning-to-Rank Model for\n  Sequential Recommendation", "author": "Andrii Dzhoha and Alisa Mironenko and Evgeny Labzin and Vladimir Vlasov and Maarten Versteegh and Marjan Celikik", "abstract": "  Modern sequential recommender systems commonly use transformer-based models\nfor next-item prediction. While these models demonstrate a strong balance\nbetween efficiency and quality, integrating interleaving features - such as the\nquery context (e.g., browse category) under which next-item interactions occur\n- poses challenges. Effectively capturing query context is crucial for refining\nranking relevance and enhancing user engagement, as it provides valuable\nsignals about user intent within a session. Unlike item features, historical\nquery context is typically not aligned with item sequences and may be\nunavailable at inference due to privacy constraints or feature store\nlimitations - making its integration into transformers both challenging and\nerror-prone. This paper analyzes different strategies for incorporating query\ncontext into transformers trained with a causal language modeling procedure as\na case study. We propose a new method that effectively fuses the item sequence\nwith query context within the attention mechanism. Through extensive offline\nand online experiments on a large-scale online platform and open datasets, we\npresent evidence that our proposed method is an effective approach for\nintegrating query context to improve model ranking quality in terms of\nrelevance and diversity.\n", "link": "http://arxiv.org/abs/2507.03789v2", "date": "2025-08-12", "relevancy": 1.386, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4787}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Effective%20Query%20Context-Aware%20Learning-to-Rank%20Model%20for%0A%20%20Sequential%20Recommendation&body=Title%3A%20Efficient%20and%20Effective%20Query%20Context-Aware%20Learning-to-Rank%20Model%20for%0A%20%20Sequential%20Recommendation%0AAuthor%3A%20Andrii%20Dzhoha%20and%20Alisa%20Mironenko%20and%20Evgeny%20Labzin%20and%20Vladimir%20Vlasov%20and%20Maarten%20Versteegh%20and%20Marjan%20Celikik%0AAbstract%3A%20%20%20Modern%20sequential%20recommender%20systems%20commonly%20use%20transformer-based%20models%0Afor%20next-item%20prediction.%20While%20these%20models%20demonstrate%20a%20strong%20balance%0Abetween%20efficiency%20and%20quality%2C%20integrating%20interleaving%20features%20-%20such%20as%20the%0Aquery%20context%20%28e.g.%2C%20browse%20category%29%20under%20which%20next-item%20interactions%20occur%0A-%20poses%20challenges.%20Effectively%20capturing%20query%20context%20is%20crucial%20for%20refining%0Aranking%20relevance%20and%20enhancing%20user%20engagement%2C%20as%20it%20provides%20valuable%0Asignals%20about%20user%20intent%20within%20a%20session.%20Unlike%20item%20features%2C%20historical%0Aquery%20context%20is%20typically%20not%20aligned%20with%20item%20sequences%20and%20may%20be%0Aunavailable%20at%20inference%20due%20to%20privacy%20constraints%20or%20feature%20store%0Alimitations%20-%20making%20its%20integration%20into%20transformers%20both%20challenging%20and%0Aerror-prone.%20This%20paper%20analyzes%20different%20strategies%20for%20incorporating%20query%0Acontext%20into%20transformers%20trained%20with%20a%20causal%20language%20modeling%20procedure%20as%0Aa%20case%20study.%20We%20propose%20a%20new%20method%20that%20effectively%20fuses%20the%20item%20sequence%0Awith%20query%20context%20within%20the%20attention%20mechanism.%20Through%20extensive%20offline%0Aand%20online%20experiments%20on%20a%20large-scale%20online%20platform%20and%20open%20datasets%2C%20we%0Apresent%20evidence%20that%20our%20proposed%20method%20is%20an%20effective%20approach%20for%0Aintegrating%20query%20context%20to%20improve%20model%20ranking%20quality%20in%20terms%20of%0Arelevance%20and%20diversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03789v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520and%2520Effective%2520Query%2520Context-Aware%2520Learning-to-Rank%2520Model%2520for%250A%2520%2520Sequential%2520Recommendation%26entry.906535625%3DAndrii%2520Dzhoha%2520and%2520Alisa%2520Mironenko%2520and%2520Evgeny%2520Labzin%2520and%2520Vladimir%2520Vlasov%2520and%2520Maarten%2520Versteegh%2520and%2520Marjan%2520Celikik%26entry.1292438233%3D%2520%2520Modern%2520sequential%2520recommender%2520systems%2520commonly%2520use%2520transformer-based%2520models%250Afor%2520next-item%2520prediction.%2520While%2520these%2520models%2520demonstrate%2520a%2520strong%2520balance%250Abetween%2520efficiency%2520and%2520quality%252C%2520integrating%2520interleaving%2520features%2520-%2520such%2520as%2520the%250Aquery%2520context%2520%2528e.g.%252C%2520browse%2520category%2529%2520under%2520which%2520next-item%2520interactions%2520occur%250A-%2520poses%2520challenges.%2520Effectively%2520capturing%2520query%2520context%2520is%2520crucial%2520for%2520refining%250Aranking%2520relevance%2520and%2520enhancing%2520user%2520engagement%252C%2520as%2520it%2520provides%2520valuable%250Asignals%2520about%2520user%2520intent%2520within%2520a%2520session.%2520Unlike%2520item%2520features%252C%2520historical%250Aquery%2520context%2520is%2520typically%2520not%2520aligned%2520with%2520item%2520sequences%2520and%2520may%2520be%250Aunavailable%2520at%2520inference%2520due%2520to%2520privacy%2520constraints%2520or%2520feature%2520store%250Alimitations%2520-%2520making%2520its%2520integration%2520into%2520transformers%2520both%2520challenging%2520and%250Aerror-prone.%2520This%2520paper%2520analyzes%2520different%2520strategies%2520for%2520incorporating%2520query%250Acontext%2520into%2520transformers%2520trained%2520with%2520a%2520causal%2520language%2520modeling%2520procedure%2520as%250Aa%2520case%2520study.%2520We%2520propose%2520a%2520new%2520method%2520that%2520effectively%2520fuses%2520the%2520item%2520sequence%250Awith%2520query%2520context%2520within%2520the%2520attention%2520mechanism.%2520Through%2520extensive%2520offline%250Aand%2520online%2520experiments%2520on%2520a%2520large-scale%2520online%2520platform%2520and%2520open%2520datasets%252C%2520we%250Apresent%2520evidence%2520that%2520our%2520proposed%2520method%2520is%2520an%2520effective%2520approach%2520for%250Aintegrating%2520query%2520context%2520to%2520improve%2520model%2520ranking%2520quality%2520in%2520terms%2520of%250Arelevance%2520and%2520diversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03789v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Effective%20Query%20Context-Aware%20Learning-to-Rank%20Model%20for%0A%20%20Sequential%20Recommendation&entry.906535625=Andrii%20Dzhoha%20and%20Alisa%20Mironenko%20and%20Evgeny%20Labzin%20and%20Vladimir%20Vlasov%20and%20Maarten%20Versteegh%20and%20Marjan%20Celikik&entry.1292438233=%20%20Modern%20sequential%20recommender%20systems%20commonly%20use%20transformer-based%20models%0Afor%20next-item%20prediction.%20While%20these%20models%20demonstrate%20a%20strong%20balance%0Abetween%20efficiency%20and%20quality%2C%20integrating%20interleaving%20features%20-%20such%20as%20the%0Aquery%20context%20%28e.g.%2C%20browse%20category%29%20under%20which%20next-item%20interactions%20occur%0A-%20poses%20challenges.%20Effectively%20capturing%20query%20context%20is%20crucial%20for%20refining%0Aranking%20relevance%20and%20enhancing%20user%20engagement%2C%20as%20it%20provides%20valuable%0Asignals%20about%20user%20intent%20within%20a%20session.%20Unlike%20item%20features%2C%20historical%0Aquery%20context%20is%20typically%20not%20aligned%20with%20item%20sequences%20and%20may%20be%0Aunavailable%20at%20inference%20due%20to%20privacy%20constraints%20or%20feature%20store%0Alimitations%20-%20making%20its%20integration%20into%20transformers%20both%20challenging%20and%0Aerror-prone.%20This%20paper%20analyzes%20different%20strategies%20for%20incorporating%20query%0Acontext%20into%20transformers%20trained%20with%20a%20causal%20language%20modeling%20procedure%20as%0Aa%20case%20study.%20We%20propose%20a%20new%20method%20that%20effectively%20fuses%20the%20item%20sequence%0Awith%20query%20context%20within%20the%20attention%20mechanism.%20Through%20extensive%20offline%0Aand%20online%20experiments%20on%20a%20large-scale%20online%20platform%20and%20open%20datasets%2C%20we%0Apresent%20evidence%20that%20our%20proposed%20method%20is%20an%20effective%20approach%20for%0Aintegrating%20query%20context%20to%20improve%20model%20ranking%20quality%20in%20terms%20of%0Arelevance%20and%20diversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03789v2&entry.124074799=Read"},
{"title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion\n  Transformer", "author": "Zixin Yin and Xili Dai and Ling-Hao Chen and Deyu Zhou and Jianan Wang and Duomin Wang and Gang Yu and Lionel M. Ni and Heung-Yeung Shum", "abstract": "  Text-guided color editing in images and videos is a fundamental yet unsolved\nproblem, requiring fine-grained manipulation of color attributes, including\nalbedo, light source color, and ambient lighting, while preserving physical\nconsistency in geometry, material properties, and light-matter interactions.\nExisting training-free methods offer broad applicability across editing tasks\nbut struggle with precise color control and often introduce visual\ninconsistency in both edited and non-edited regions. In this work, we present\nColorCtrl, a training-free color editing method that leverages the attention\nmechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By\ndisentangling structure and color through targeted manipulation of attention\nmaps and value tokens, our method enables accurate and consistent color\nediting, along with word-level control of attribute intensity. Our method\nmodifies only the intended regions specified by the prompt, leaving unrelated\nareas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate\nthat ColorCtrl outperforms existing training-free approaches and achieves\nstate-of-the-art performances in both edit quality and consistency.\nFurthermore, our method surpasses strong commercial models such as FLUX.1\nKontext Max and GPT-4o Image Generation in terms of consistency. When extended\nto video models like CogVideoX, our approach exhibits greater advantages,\nparticularly in maintaining temporal coherence and editing stability. Finally,\nour method also generalizes to instruction-based editing diffusion models such\nas Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.\n", "link": "http://arxiv.org/abs/2508.09131v1", "date": "2025-08-12", "relevancy": 1.2502, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6432}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6202}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Text-Guided%20Color%20Editing%20with%20Multi-Modal%20Diffusion%0A%20%20Transformer&body=Title%3A%20Training-Free%20Text-Guided%20Color%20Editing%20with%20Multi-Modal%20Diffusion%0A%20%20Transformer%0AAuthor%3A%20Zixin%20Yin%20and%20Xili%20Dai%20and%20Ling-Hao%20Chen%20and%20Deyu%20Zhou%20and%20Jianan%20Wang%20and%20Duomin%20Wang%20and%20Gang%20Yu%20and%20Lionel%20M.%20Ni%20and%20Heung-Yeung%20Shum%0AAbstract%3A%20%20%20Text-guided%20color%20editing%20in%20images%20and%20videos%20is%20a%20fundamental%20yet%20unsolved%0Aproblem%2C%20requiring%20fine-grained%20manipulation%20of%20color%20attributes%2C%20including%0Aalbedo%2C%20light%20source%20color%2C%20and%20ambient%20lighting%2C%20while%20preserving%20physical%0Aconsistency%20in%20geometry%2C%20material%20properties%2C%20and%20light-matter%20interactions.%0AExisting%20training-free%20methods%20offer%20broad%20applicability%20across%20editing%20tasks%0Abut%20struggle%20with%20precise%20color%20control%20and%20often%20introduce%20visual%0Ainconsistency%20in%20both%20edited%20and%20non-edited%20regions.%20In%20this%20work%2C%20we%20present%0AColorCtrl%2C%20a%20training-free%20color%20editing%20method%20that%20leverages%20the%20attention%0Amechanisms%20of%20modern%20Multi-Modal%20Diffusion%20Transformers%20%28MM-DiT%29.%20By%0Adisentangling%20structure%20and%20color%20through%20targeted%20manipulation%20of%20attention%0Amaps%20and%20value%20tokens%2C%20our%20method%20enables%20accurate%20and%20consistent%20color%0Aediting%2C%20along%20with%20word-level%20control%20of%20attribute%20intensity.%20Our%20method%0Amodifies%20only%20the%20intended%20regions%20specified%20by%20the%20prompt%2C%20leaving%20unrelated%0Aareas%20untouched.%20Extensive%20experiments%20on%20both%20SD3%20and%20FLUX.1-dev%20demonstrate%0Athat%20ColorCtrl%20outperforms%20existing%20training-free%20approaches%20and%20achieves%0Astate-of-the-art%20performances%20in%20both%20edit%20quality%20and%20consistency.%0AFurthermore%2C%20our%20method%20surpasses%20strong%20commercial%20models%20such%20as%20FLUX.1%0AKontext%20Max%20and%20GPT-4o%20Image%20Generation%20in%20terms%20of%20consistency.%20When%20extended%0Ato%20video%20models%20like%20CogVideoX%2C%20our%20approach%20exhibits%20greater%20advantages%2C%0Aparticularly%20in%20maintaining%20temporal%20coherence%20and%20editing%20stability.%20Finally%2C%0Aour%20method%20also%20generalizes%20to%20instruction-based%20editing%20diffusion%20models%20such%0Aas%20Step1X-Edit%20and%20FLUX.1%20Kontext%20dev%2C%20further%20demonstrating%20its%20versatility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Text-Guided%2520Color%2520Editing%2520with%2520Multi-Modal%2520Diffusion%250A%2520%2520Transformer%26entry.906535625%3DZixin%2520Yin%2520and%2520Xili%2520Dai%2520and%2520Ling-Hao%2520Chen%2520and%2520Deyu%2520Zhou%2520and%2520Jianan%2520Wang%2520and%2520Duomin%2520Wang%2520and%2520Gang%2520Yu%2520and%2520Lionel%2520M.%2520Ni%2520and%2520Heung-Yeung%2520Shum%26entry.1292438233%3D%2520%2520Text-guided%2520color%2520editing%2520in%2520images%2520and%2520videos%2520is%2520a%2520fundamental%2520yet%2520unsolved%250Aproblem%252C%2520requiring%2520fine-grained%2520manipulation%2520of%2520color%2520attributes%252C%2520including%250Aalbedo%252C%2520light%2520source%2520color%252C%2520and%2520ambient%2520lighting%252C%2520while%2520preserving%2520physical%250Aconsistency%2520in%2520geometry%252C%2520material%2520properties%252C%2520and%2520light-matter%2520interactions.%250AExisting%2520training-free%2520methods%2520offer%2520broad%2520applicability%2520across%2520editing%2520tasks%250Abut%2520struggle%2520with%2520precise%2520color%2520control%2520and%2520often%2520introduce%2520visual%250Ainconsistency%2520in%2520both%2520edited%2520and%2520non-edited%2520regions.%2520In%2520this%2520work%252C%2520we%2520present%250AColorCtrl%252C%2520a%2520training-free%2520color%2520editing%2520method%2520that%2520leverages%2520the%2520attention%250Amechanisms%2520of%2520modern%2520Multi-Modal%2520Diffusion%2520Transformers%2520%2528MM-DiT%2529.%2520By%250Adisentangling%2520structure%2520and%2520color%2520through%2520targeted%2520manipulation%2520of%2520attention%250Amaps%2520and%2520value%2520tokens%252C%2520our%2520method%2520enables%2520accurate%2520and%2520consistent%2520color%250Aediting%252C%2520along%2520with%2520word-level%2520control%2520of%2520attribute%2520intensity.%2520Our%2520method%250Amodifies%2520only%2520the%2520intended%2520regions%2520specified%2520by%2520the%2520prompt%252C%2520leaving%2520unrelated%250Aareas%2520untouched.%2520Extensive%2520experiments%2520on%2520both%2520SD3%2520and%2520FLUX.1-dev%2520demonstrate%250Athat%2520ColorCtrl%2520outperforms%2520existing%2520training-free%2520approaches%2520and%2520achieves%250Astate-of-the-art%2520performances%2520in%2520both%2520edit%2520quality%2520and%2520consistency.%250AFurthermore%252C%2520our%2520method%2520surpasses%2520strong%2520commercial%2520models%2520such%2520as%2520FLUX.1%250AKontext%2520Max%2520and%2520GPT-4o%2520Image%2520Generation%2520in%2520terms%2520of%2520consistency.%2520When%2520extended%250Ato%2520video%2520models%2520like%2520CogVideoX%252C%2520our%2520approach%2520exhibits%2520greater%2520advantages%252C%250Aparticularly%2520in%2520maintaining%2520temporal%2520coherence%2520and%2520editing%2520stability.%2520Finally%252C%250Aour%2520method%2520also%2520generalizes%2520to%2520instruction-based%2520editing%2520diffusion%2520models%2520such%250Aas%2520Step1X-Edit%2520and%2520FLUX.1%2520Kontext%2520dev%252C%2520further%2520demonstrating%2520its%2520versatility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Text-Guided%20Color%20Editing%20with%20Multi-Modal%20Diffusion%0A%20%20Transformer&entry.906535625=Zixin%20Yin%20and%20Xili%20Dai%20and%20Ling-Hao%20Chen%20and%20Deyu%20Zhou%20and%20Jianan%20Wang%20and%20Duomin%20Wang%20and%20Gang%20Yu%20and%20Lionel%20M.%20Ni%20and%20Heung-Yeung%20Shum&entry.1292438233=%20%20Text-guided%20color%20editing%20in%20images%20and%20videos%20is%20a%20fundamental%20yet%20unsolved%0Aproblem%2C%20requiring%20fine-grained%20manipulation%20of%20color%20attributes%2C%20including%0Aalbedo%2C%20light%20source%20color%2C%20and%20ambient%20lighting%2C%20while%20preserving%20physical%0Aconsistency%20in%20geometry%2C%20material%20properties%2C%20and%20light-matter%20interactions.%0AExisting%20training-free%20methods%20offer%20broad%20applicability%20across%20editing%20tasks%0Abut%20struggle%20with%20precise%20color%20control%20and%20often%20introduce%20visual%0Ainconsistency%20in%20both%20edited%20and%20non-edited%20regions.%20In%20this%20work%2C%20we%20present%0AColorCtrl%2C%20a%20training-free%20color%20editing%20method%20that%20leverages%20the%20attention%0Amechanisms%20of%20modern%20Multi-Modal%20Diffusion%20Transformers%20%28MM-DiT%29.%20By%0Adisentangling%20structure%20and%20color%20through%20targeted%20manipulation%20of%20attention%0Amaps%20and%20value%20tokens%2C%20our%20method%20enables%20accurate%20and%20consistent%20color%0Aediting%2C%20along%20with%20word-level%20control%20of%20attribute%20intensity.%20Our%20method%0Amodifies%20only%20the%20intended%20regions%20specified%20by%20the%20prompt%2C%20leaving%20unrelated%0Aareas%20untouched.%20Extensive%20experiments%20on%20both%20SD3%20and%20FLUX.1-dev%20demonstrate%0Athat%20ColorCtrl%20outperforms%20existing%20training-free%20approaches%20and%20achieves%0Astate-of-the-art%20performances%20in%20both%20edit%20quality%20and%20consistency.%0AFurthermore%2C%20our%20method%20surpasses%20strong%20commercial%20models%20such%20as%20FLUX.1%0AKontext%20Max%20and%20GPT-4o%20Image%20Generation%20in%20terms%20of%20consistency.%20When%20extended%0Ato%20video%20models%20like%20CogVideoX%2C%20our%20approach%20exhibits%20greater%20advantages%2C%0Aparticularly%20in%20maintaining%20temporal%20coherence%20and%20editing%20stability.%20Finally%2C%0Aour%20method%20also%20generalizes%20to%20instruction-based%20editing%20diffusion%20models%20such%0Aas%20Step1X-Edit%20and%20FLUX.1%20Kontext%20dev%2C%20further%20demonstrating%20its%20versatility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09131v1&entry.124074799=Read"},
{"title": "Dynamic Spectrum Access for Ambient Backscatter Communication-assisted\n  D2D Systems with Quantum Reinforcement Learning", "author": "Nguyen Van Huynh and Bolun Zhang and Dinh-Hieu Tran and Dinh Thai Hoang and Diep N. Nguyen and Gan Zheng and Dusit Niyato and Quoc-Viet Pham", "abstract": "  Spectrum access is an essential problem in device-to-device (D2D)\ncommunications. However, with the recent growth in the number of mobile\ndevices, the wireless spectrum is becoming scarce, resulting in low spectral\nefficiency for D2D communications. To address this problem, this paper aims to\nintegrate the ambient backscatter communication technology into D2D devices to\nallow them to backscatter ambient RF signals to transmit their data when the\nshared spectrum is occupied by mobile users. To obtain the optimal spectrum\naccess policy, i.e., stay idle or access the shared spectrum and perform active\ntransmissions or backscattering ambient RF signals for transmissions, to\nmaximize the average throughput for D2D users, deep reinforcement learning\n(DRL) can be adopted. However, DRL-based solutions may require long training\ntime due to the curse of dimensionality issue as well as complex deep neural\nnetwork architectures. For that, we develop a novel quantum reinforcement\nlearning (RL) algorithm that can achieve a faster convergence rate with fewer\ntraining parameters compared to DRL thanks to the quantum superposition and\nquantum entanglement principles. Specifically, instead of using conventional\ndeep neural networks, the proposed quantum RL algorithm uses a parametrized\nquantum circuit to approximate an optimal policy. Extensive simulations then\ndemonstrate that the proposed solution not only can significantly improve the\naverage throughput of D2D devices when the shared spectrum is busy but also can\nachieve much better performance in terms of convergence rate and learning\ncomplexity compared to existing DRL-based methods.\n", "link": "http://arxiv.org/abs/2410.17971v2", "date": "2025-08-12", "relevancy": 1.3912, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.48}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4619}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Spectrum%20Access%20for%20Ambient%20Backscatter%20Communication-assisted%0A%20%20D2D%20Systems%20with%20Quantum%20Reinforcement%20Learning&body=Title%3A%20Dynamic%20Spectrum%20Access%20for%20Ambient%20Backscatter%20Communication-assisted%0A%20%20D2D%20Systems%20with%20Quantum%20Reinforcement%20Learning%0AAuthor%3A%20Nguyen%20Van%20Huynh%20and%20Bolun%20Zhang%20and%20Dinh-Hieu%20Tran%20and%20Dinh%20Thai%20Hoang%20and%20Diep%20N.%20Nguyen%20and%20Gan%20Zheng%20and%20Dusit%20Niyato%20and%20Quoc-Viet%20Pham%0AAbstract%3A%20%20%20Spectrum%20access%20is%20an%20essential%20problem%20in%20device-to-device%20%28D2D%29%0Acommunications.%20However%2C%20with%20the%20recent%20growth%20in%20the%20number%20of%20mobile%0Adevices%2C%20the%20wireless%20spectrum%20is%20becoming%20scarce%2C%20resulting%20in%20low%20spectral%0Aefficiency%20for%20D2D%20communications.%20To%20address%20this%20problem%2C%20this%20paper%20aims%20to%0Aintegrate%20the%20ambient%20backscatter%20communication%20technology%20into%20D2D%20devices%20to%0Aallow%20them%20to%20backscatter%20ambient%20RF%20signals%20to%20transmit%20their%20data%20when%20the%0Ashared%20spectrum%20is%20occupied%20by%20mobile%20users.%20To%20obtain%20the%20optimal%20spectrum%0Aaccess%20policy%2C%20i.e.%2C%20stay%20idle%20or%20access%20the%20shared%20spectrum%20and%20perform%20active%0Atransmissions%20or%20backscattering%20ambient%20RF%20signals%20for%20transmissions%2C%20to%0Amaximize%20the%20average%20throughput%20for%20D2D%20users%2C%20deep%20reinforcement%20learning%0A%28DRL%29%20can%20be%20adopted.%20However%2C%20DRL-based%20solutions%20may%20require%20long%20training%0Atime%20due%20to%20the%20curse%20of%20dimensionality%20issue%20as%20well%20as%20complex%20deep%20neural%0Anetwork%20architectures.%20For%20that%2C%20we%20develop%20a%20novel%20quantum%20reinforcement%0Alearning%20%28RL%29%20algorithm%20that%20can%20achieve%20a%20faster%20convergence%20rate%20with%20fewer%0Atraining%20parameters%20compared%20to%20DRL%20thanks%20to%20the%20quantum%20superposition%20and%0Aquantum%20entanglement%20principles.%20Specifically%2C%20instead%20of%20using%20conventional%0Adeep%20neural%20networks%2C%20the%20proposed%20quantum%20RL%20algorithm%20uses%20a%20parametrized%0Aquantum%20circuit%20to%20approximate%20an%20optimal%20policy.%20Extensive%20simulations%20then%0Ademonstrate%20that%20the%20proposed%20solution%20not%20only%20can%20significantly%20improve%20the%0Aaverage%20throughput%20of%20D2D%20devices%20when%20the%20shared%20spectrum%20is%20busy%20but%20also%20can%0Aachieve%20much%20better%20performance%20in%20terms%20of%20convergence%20rate%20and%20learning%0Acomplexity%20compared%20to%20existing%20DRL-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17971v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Spectrum%2520Access%2520for%2520Ambient%2520Backscatter%2520Communication-assisted%250A%2520%2520D2D%2520Systems%2520with%2520Quantum%2520Reinforcement%2520Learning%26entry.906535625%3DNguyen%2520Van%2520Huynh%2520and%2520Bolun%2520Zhang%2520and%2520Dinh-Hieu%2520Tran%2520and%2520Dinh%2520Thai%2520Hoang%2520and%2520Diep%2520N.%2520Nguyen%2520and%2520Gan%2520Zheng%2520and%2520Dusit%2520Niyato%2520and%2520Quoc-Viet%2520Pham%26entry.1292438233%3D%2520%2520Spectrum%2520access%2520is%2520an%2520essential%2520problem%2520in%2520device-to-device%2520%2528D2D%2529%250Acommunications.%2520However%252C%2520with%2520the%2520recent%2520growth%2520in%2520the%2520number%2520of%2520mobile%250Adevices%252C%2520the%2520wireless%2520spectrum%2520is%2520becoming%2520scarce%252C%2520resulting%2520in%2520low%2520spectral%250Aefficiency%2520for%2520D2D%2520communications.%2520To%2520address%2520this%2520problem%252C%2520this%2520paper%2520aims%2520to%250Aintegrate%2520the%2520ambient%2520backscatter%2520communication%2520technology%2520into%2520D2D%2520devices%2520to%250Aallow%2520them%2520to%2520backscatter%2520ambient%2520RF%2520signals%2520to%2520transmit%2520their%2520data%2520when%2520the%250Ashared%2520spectrum%2520is%2520occupied%2520by%2520mobile%2520users.%2520To%2520obtain%2520the%2520optimal%2520spectrum%250Aaccess%2520policy%252C%2520i.e.%252C%2520stay%2520idle%2520or%2520access%2520the%2520shared%2520spectrum%2520and%2520perform%2520active%250Atransmissions%2520or%2520backscattering%2520ambient%2520RF%2520signals%2520for%2520transmissions%252C%2520to%250Amaximize%2520the%2520average%2520throughput%2520for%2520D2D%2520users%252C%2520deep%2520reinforcement%2520learning%250A%2528DRL%2529%2520can%2520be%2520adopted.%2520However%252C%2520DRL-based%2520solutions%2520may%2520require%2520long%2520training%250Atime%2520due%2520to%2520the%2520curse%2520of%2520dimensionality%2520issue%2520as%2520well%2520as%2520complex%2520deep%2520neural%250Anetwork%2520architectures.%2520For%2520that%252C%2520we%2520develop%2520a%2520novel%2520quantum%2520reinforcement%250Alearning%2520%2528RL%2529%2520algorithm%2520that%2520can%2520achieve%2520a%2520faster%2520convergence%2520rate%2520with%2520fewer%250Atraining%2520parameters%2520compared%2520to%2520DRL%2520thanks%2520to%2520the%2520quantum%2520superposition%2520and%250Aquantum%2520entanglement%2520principles.%2520Specifically%252C%2520instead%2520of%2520using%2520conventional%250Adeep%2520neural%2520networks%252C%2520the%2520proposed%2520quantum%2520RL%2520algorithm%2520uses%2520a%2520parametrized%250Aquantum%2520circuit%2520to%2520approximate%2520an%2520optimal%2520policy.%2520Extensive%2520simulations%2520then%250Ademonstrate%2520that%2520the%2520proposed%2520solution%2520not%2520only%2520can%2520significantly%2520improve%2520the%250Aaverage%2520throughput%2520of%2520D2D%2520devices%2520when%2520the%2520shared%2520spectrum%2520is%2520busy%2520but%2520also%2520can%250Aachieve%2520much%2520better%2520performance%2520in%2520terms%2520of%2520convergence%2520rate%2520and%2520learning%250Acomplexity%2520compared%2520to%2520existing%2520DRL-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17971v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Spectrum%20Access%20for%20Ambient%20Backscatter%20Communication-assisted%0A%20%20D2D%20Systems%20with%20Quantum%20Reinforcement%20Learning&entry.906535625=Nguyen%20Van%20Huynh%20and%20Bolun%20Zhang%20and%20Dinh-Hieu%20Tran%20and%20Dinh%20Thai%20Hoang%20and%20Diep%20N.%20Nguyen%20and%20Gan%20Zheng%20and%20Dusit%20Niyato%20and%20Quoc-Viet%20Pham&entry.1292438233=%20%20Spectrum%20access%20is%20an%20essential%20problem%20in%20device-to-device%20%28D2D%29%0Acommunications.%20However%2C%20with%20the%20recent%20growth%20in%20the%20number%20of%20mobile%0Adevices%2C%20the%20wireless%20spectrum%20is%20becoming%20scarce%2C%20resulting%20in%20low%20spectral%0Aefficiency%20for%20D2D%20communications.%20To%20address%20this%20problem%2C%20this%20paper%20aims%20to%0Aintegrate%20the%20ambient%20backscatter%20communication%20technology%20into%20D2D%20devices%20to%0Aallow%20them%20to%20backscatter%20ambient%20RF%20signals%20to%20transmit%20their%20data%20when%20the%0Ashared%20spectrum%20is%20occupied%20by%20mobile%20users.%20To%20obtain%20the%20optimal%20spectrum%0Aaccess%20policy%2C%20i.e.%2C%20stay%20idle%20or%20access%20the%20shared%20spectrum%20and%20perform%20active%0Atransmissions%20or%20backscattering%20ambient%20RF%20signals%20for%20transmissions%2C%20to%0Amaximize%20the%20average%20throughput%20for%20D2D%20users%2C%20deep%20reinforcement%20learning%0A%28DRL%29%20can%20be%20adopted.%20However%2C%20DRL-based%20solutions%20may%20require%20long%20training%0Atime%20due%20to%20the%20curse%20of%20dimensionality%20issue%20as%20well%20as%20complex%20deep%20neural%0Anetwork%20architectures.%20For%20that%2C%20we%20develop%20a%20novel%20quantum%20reinforcement%0Alearning%20%28RL%29%20algorithm%20that%20can%20achieve%20a%20faster%20convergence%20rate%20with%20fewer%0Atraining%20parameters%20compared%20to%20DRL%20thanks%20to%20the%20quantum%20superposition%20and%0Aquantum%20entanglement%20principles.%20Specifically%2C%20instead%20of%20using%20conventional%0Adeep%20neural%20networks%2C%20the%20proposed%20quantum%20RL%20algorithm%20uses%20a%20parametrized%0Aquantum%20circuit%20to%20approximate%20an%20optimal%20policy.%20Extensive%20simulations%20then%0Ademonstrate%20that%20the%20proposed%20solution%20not%20only%20can%20significantly%20improve%20the%0Aaverage%20throughput%20of%20D2D%20devices%20when%20the%20shared%20spectrum%20is%20busy%20but%20also%20can%0Aachieve%20much%20better%20performance%20in%20terms%20of%20convergence%20rate%20and%20learning%0Acomplexity%20compared%20to%20existing%20DRL-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17971v2&entry.124074799=Read"},
{"title": "Chemist-aligned retrosynthesis by ensembling diverse inductive bias\n  models", "author": "Krzysztof Maziarz and Guoqing Liu and Hubert Misztela and Austin Tripp and Junren Li and Aleksei Kornev and Piotr Gai\u0144ski and Holger Hoefling and Mike Fortunato and Rishi Gupta and Marwin Segler", "abstract": "  Chemical synthesis remains a critical bottleneck in the discovery and\nmanufacture of functional small molecules. AI-based synthesis planning models\ncould be a potential remedy to find effective syntheses, and have made progress\nin recent years. However, they still struggle with less frequent, yet critical\nreactions for synthetic strategy, as well as hallucinated, incorrect\npredictions. This hampers multi-step search algorithms that rely on models, and\nleads to misalignment with chemists' expectations. Here we propose\nRetroChimera: a frontier retrosynthesis model, built upon two newly developed\ncomponents with complementary inductive biases, which we fuse together using a\nnew framework for integrating predictions from multiple sources via a\nlearning-based ensembling strategy. Through experiments across several orders\nof magnitude in data scale and splitting strategy, we show RetroChimera\noutperforms all major models by a large margin, demonstrating robustness\noutside the training data, as well as for the first time the ability to learn\nfrom even a very small number of examples per reaction class. Moreover,\nindustrial organic chemists prefer predictions from RetroChimera over the\nreactions it was trained on in terms of quality, revealing high levels of\nalignment. Finally, we demonstrate zero-shot transfer to an internal dataset\nfrom a major pharmaceutical company, showing robust generalization under\ndistribution shift. With the new dimension that our ensembling framework\nunlocks, we anticipate further acceleration in the development of even more\naccurate models.\n", "link": "http://arxiv.org/abs/2412.05269v2", "date": "2025-08-12", "relevancy": 1.3374, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4795}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4433}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chemist-aligned%20retrosynthesis%20by%20ensembling%20diverse%20inductive%20bias%0A%20%20models&body=Title%3A%20Chemist-aligned%20retrosynthesis%20by%20ensembling%20diverse%20inductive%20bias%0A%20%20models%0AAuthor%3A%20Krzysztof%20Maziarz%20and%20Guoqing%20Liu%20and%20Hubert%20Misztela%20and%20Austin%20Tripp%20and%20Junren%20Li%20and%20Aleksei%20Kornev%20and%20Piotr%20Gai%C5%84ski%20and%20Holger%20Hoefling%20and%20Mike%20Fortunato%20and%20Rishi%20Gupta%20and%20Marwin%20Segler%0AAbstract%3A%20%20%20Chemical%20synthesis%20remains%20a%20critical%20bottleneck%20in%20the%20discovery%20and%0Amanufacture%20of%20functional%20small%20molecules.%20AI-based%20synthesis%20planning%20models%0Acould%20be%20a%20potential%20remedy%20to%20find%20effective%20syntheses%2C%20and%20have%20made%20progress%0Ain%20recent%20years.%20However%2C%20they%20still%20struggle%20with%20less%20frequent%2C%20yet%20critical%0Areactions%20for%20synthetic%20strategy%2C%20as%20well%20as%20hallucinated%2C%20incorrect%0Apredictions.%20This%20hampers%20multi-step%20search%20algorithms%20that%20rely%20on%20models%2C%20and%0Aleads%20to%20misalignment%20with%20chemists%27%20expectations.%20Here%20we%20propose%0ARetroChimera%3A%20a%20frontier%20retrosynthesis%20model%2C%20built%20upon%20two%20newly%20developed%0Acomponents%20with%20complementary%20inductive%20biases%2C%20which%20we%20fuse%20together%20using%20a%0Anew%20framework%20for%20integrating%20predictions%20from%20multiple%20sources%20via%20a%0Alearning-based%20ensembling%20strategy.%20Through%20experiments%20across%20several%20orders%0Aof%20magnitude%20in%20data%20scale%20and%20splitting%20strategy%2C%20we%20show%20RetroChimera%0Aoutperforms%20all%20major%20models%20by%20a%20large%20margin%2C%20demonstrating%20robustness%0Aoutside%20the%20training%20data%2C%20as%20well%20as%20for%20the%20first%20time%20the%20ability%20to%20learn%0Afrom%20even%20a%20very%20small%20number%20of%20examples%20per%20reaction%20class.%20Moreover%2C%0Aindustrial%20organic%20chemists%20prefer%20predictions%20from%20RetroChimera%20over%20the%0Areactions%20it%20was%20trained%20on%20in%20terms%20of%20quality%2C%20revealing%20high%20levels%20of%0Aalignment.%20Finally%2C%20we%20demonstrate%20zero-shot%20transfer%20to%20an%20internal%20dataset%0Afrom%20a%20major%20pharmaceutical%20company%2C%20showing%20robust%20generalization%20under%0Adistribution%20shift.%20With%20the%20new%20dimension%20that%20our%20ensembling%20framework%0Aunlocks%2C%20we%20anticipate%20further%20acceleration%20in%20the%20development%20of%20even%20more%0Aaccurate%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05269v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChemist-aligned%2520retrosynthesis%2520by%2520ensembling%2520diverse%2520inductive%2520bias%250A%2520%2520models%26entry.906535625%3DKrzysztof%2520Maziarz%2520and%2520Guoqing%2520Liu%2520and%2520Hubert%2520Misztela%2520and%2520Austin%2520Tripp%2520and%2520Junren%2520Li%2520and%2520Aleksei%2520Kornev%2520and%2520Piotr%2520Gai%25C5%2584ski%2520and%2520Holger%2520Hoefling%2520and%2520Mike%2520Fortunato%2520and%2520Rishi%2520Gupta%2520and%2520Marwin%2520Segler%26entry.1292438233%3D%2520%2520Chemical%2520synthesis%2520remains%2520a%2520critical%2520bottleneck%2520in%2520the%2520discovery%2520and%250Amanufacture%2520of%2520functional%2520small%2520molecules.%2520AI-based%2520synthesis%2520planning%2520models%250Acould%2520be%2520a%2520potential%2520remedy%2520to%2520find%2520effective%2520syntheses%252C%2520and%2520have%2520made%2520progress%250Ain%2520recent%2520years.%2520However%252C%2520they%2520still%2520struggle%2520with%2520less%2520frequent%252C%2520yet%2520critical%250Areactions%2520for%2520synthetic%2520strategy%252C%2520as%2520well%2520as%2520hallucinated%252C%2520incorrect%250Apredictions.%2520This%2520hampers%2520multi-step%2520search%2520algorithms%2520that%2520rely%2520on%2520models%252C%2520and%250Aleads%2520to%2520misalignment%2520with%2520chemists%2527%2520expectations.%2520Here%2520we%2520propose%250ARetroChimera%253A%2520a%2520frontier%2520retrosynthesis%2520model%252C%2520built%2520upon%2520two%2520newly%2520developed%250Acomponents%2520with%2520complementary%2520inductive%2520biases%252C%2520which%2520we%2520fuse%2520together%2520using%2520a%250Anew%2520framework%2520for%2520integrating%2520predictions%2520from%2520multiple%2520sources%2520via%2520a%250Alearning-based%2520ensembling%2520strategy.%2520Through%2520experiments%2520across%2520several%2520orders%250Aof%2520magnitude%2520in%2520data%2520scale%2520and%2520splitting%2520strategy%252C%2520we%2520show%2520RetroChimera%250Aoutperforms%2520all%2520major%2520models%2520by%2520a%2520large%2520margin%252C%2520demonstrating%2520robustness%250Aoutside%2520the%2520training%2520data%252C%2520as%2520well%2520as%2520for%2520the%2520first%2520time%2520the%2520ability%2520to%2520learn%250Afrom%2520even%2520a%2520very%2520small%2520number%2520of%2520examples%2520per%2520reaction%2520class.%2520Moreover%252C%250Aindustrial%2520organic%2520chemists%2520prefer%2520predictions%2520from%2520RetroChimera%2520over%2520the%250Areactions%2520it%2520was%2520trained%2520on%2520in%2520terms%2520of%2520quality%252C%2520revealing%2520high%2520levels%2520of%250Aalignment.%2520Finally%252C%2520we%2520demonstrate%2520zero-shot%2520transfer%2520to%2520an%2520internal%2520dataset%250Afrom%2520a%2520major%2520pharmaceutical%2520company%252C%2520showing%2520robust%2520generalization%2520under%250Adistribution%2520shift.%2520With%2520the%2520new%2520dimension%2520that%2520our%2520ensembling%2520framework%250Aunlocks%252C%2520we%2520anticipate%2520further%2520acceleration%2520in%2520the%2520development%2520of%2520even%2520more%250Aaccurate%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05269v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chemist-aligned%20retrosynthesis%20by%20ensembling%20diverse%20inductive%20bias%0A%20%20models&entry.906535625=Krzysztof%20Maziarz%20and%20Guoqing%20Liu%20and%20Hubert%20Misztela%20and%20Austin%20Tripp%20and%20Junren%20Li%20and%20Aleksei%20Kornev%20and%20Piotr%20Gai%C5%84ski%20and%20Holger%20Hoefling%20and%20Mike%20Fortunato%20and%20Rishi%20Gupta%20and%20Marwin%20Segler&entry.1292438233=%20%20Chemical%20synthesis%20remains%20a%20critical%20bottleneck%20in%20the%20discovery%20and%0Amanufacture%20of%20functional%20small%20molecules.%20AI-based%20synthesis%20planning%20models%0Acould%20be%20a%20potential%20remedy%20to%20find%20effective%20syntheses%2C%20and%20have%20made%20progress%0Ain%20recent%20years.%20However%2C%20they%20still%20struggle%20with%20less%20frequent%2C%20yet%20critical%0Areactions%20for%20synthetic%20strategy%2C%20as%20well%20as%20hallucinated%2C%20incorrect%0Apredictions.%20This%20hampers%20multi-step%20search%20algorithms%20that%20rely%20on%20models%2C%20and%0Aleads%20to%20misalignment%20with%20chemists%27%20expectations.%20Here%20we%20propose%0ARetroChimera%3A%20a%20frontier%20retrosynthesis%20model%2C%20built%20upon%20two%20newly%20developed%0Acomponents%20with%20complementary%20inductive%20biases%2C%20which%20we%20fuse%20together%20using%20a%0Anew%20framework%20for%20integrating%20predictions%20from%20multiple%20sources%20via%20a%0Alearning-based%20ensembling%20strategy.%20Through%20experiments%20across%20several%20orders%0Aof%20magnitude%20in%20data%20scale%20and%20splitting%20strategy%2C%20we%20show%20RetroChimera%0Aoutperforms%20all%20major%20models%20by%20a%20large%20margin%2C%20demonstrating%20robustness%0Aoutside%20the%20training%20data%2C%20as%20well%20as%20for%20the%20first%20time%20the%20ability%20to%20learn%0Afrom%20even%20a%20very%20small%20number%20of%20examples%20per%20reaction%20class.%20Moreover%2C%0Aindustrial%20organic%20chemists%20prefer%20predictions%20from%20RetroChimera%20over%20the%0Areactions%20it%20was%20trained%20on%20in%20terms%20of%20quality%2C%20revealing%20high%20levels%20of%0Aalignment.%20Finally%2C%20we%20demonstrate%20zero-shot%20transfer%20to%20an%20internal%20dataset%0Afrom%20a%20major%20pharmaceutical%20company%2C%20showing%20robust%20generalization%20under%0Adistribution%20shift.%20With%20the%20new%20dimension%20that%20our%20ensembling%20framework%0Aunlocks%2C%20we%20anticipate%20further%20acceleration%20in%20the%20development%20of%20even%20more%0Aaccurate%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05269v2&entry.124074799=Read"},
{"title": "Discrete and Continuous Difference of Submodular Minimization", "author": "George Orfanides and Tim Hoheisel and Marwa El Halabi", "abstract": "  Submodular functions, defined on continuous or discrete domains, arise in\nnumerous applications. We study the minimization of the difference of two\nsubmodular (DS) functions, over both domains, extending prior work restricted\nto set functions. We show that all functions on discrete domains and all smooth\nfunctions on continuous domains are DS. For discrete domains, we observe that\nDS minimization is equivalent to minimizing the difference of two convex (DC)\nfunctions, as in the set function case. We propose a novel variant of the DC\nAlgorithm (DCA) and apply it to the resulting DC Program, obtaining comparable\ntheoretical guarantees as in the set function case. The algorithm can be\napplied to continuous domains via discretization. Experiments demonstrate that\nour method outperforms baselines in integer compressive sensing and integer\nleast squares.\n", "link": "http://arxiv.org/abs/2506.07952v2", "date": "2025-08-12", "relevancy": 1.2418, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4603}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4033}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discrete%20and%20Continuous%20Difference%20of%20Submodular%20Minimization&body=Title%3A%20Discrete%20and%20Continuous%20Difference%20of%20Submodular%20Minimization%0AAuthor%3A%20George%20Orfanides%20and%20Tim%20Hoheisel%20and%20Marwa%20El%20Halabi%0AAbstract%3A%20%20%20Submodular%20functions%2C%20defined%20on%20continuous%20or%20discrete%20domains%2C%20arise%20in%0Anumerous%20applications.%20We%20study%20the%20minimization%20of%20the%20difference%20of%20two%0Asubmodular%20%28DS%29%20functions%2C%20over%20both%20domains%2C%20extending%20prior%20work%20restricted%0Ato%20set%20functions.%20We%20show%20that%20all%20functions%20on%20discrete%20domains%20and%20all%20smooth%0Afunctions%20on%20continuous%20domains%20are%20DS.%20For%20discrete%20domains%2C%20we%20observe%20that%0ADS%20minimization%20is%20equivalent%20to%20minimizing%20the%20difference%20of%20two%20convex%20%28DC%29%0Afunctions%2C%20as%20in%20the%20set%20function%20case.%20We%20propose%20a%20novel%20variant%20of%20the%20DC%0AAlgorithm%20%28DCA%29%20and%20apply%20it%20to%20the%20resulting%20DC%20Program%2C%20obtaining%20comparable%0Atheoretical%20guarantees%20as%20in%20the%20set%20function%20case.%20The%20algorithm%20can%20be%0Aapplied%20to%20continuous%20domains%20via%20discretization.%20Experiments%20demonstrate%20that%0Aour%20method%20outperforms%20baselines%20in%20integer%20compressive%20sensing%20and%20integer%0Aleast%20squares.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07952v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscrete%2520and%2520Continuous%2520Difference%2520of%2520Submodular%2520Minimization%26entry.906535625%3DGeorge%2520Orfanides%2520and%2520Tim%2520Hoheisel%2520and%2520Marwa%2520El%2520Halabi%26entry.1292438233%3D%2520%2520Submodular%2520functions%252C%2520defined%2520on%2520continuous%2520or%2520discrete%2520domains%252C%2520arise%2520in%250Anumerous%2520applications.%2520We%2520study%2520the%2520minimization%2520of%2520the%2520difference%2520of%2520two%250Asubmodular%2520%2528DS%2529%2520functions%252C%2520over%2520both%2520domains%252C%2520extending%2520prior%2520work%2520restricted%250Ato%2520set%2520functions.%2520We%2520show%2520that%2520all%2520functions%2520on%2520discrete%2520domains%2520and%2520all%2520smooth%250Afunctions%2520on%2520continuous%2520domains%2520are%2520DS.%2520For%2520discrete%2520domains%252C%2520we%2520observe%2520that%250ADS%2520minimization%2520is%2520equivalent%2520to%2520minimizing%2520the%2520difference%2520of%2520two%2520convex%2520%2528DC%2529%250Afunctions%252C%2520as%2520in%2520the%2520set%2520function%2520case.%2520We%2520propose%2520a%2520novel%2520variant%2520of%2520the%2520DC%250AAlgorithm%2520%2528DCA%2529%2520and%2520apply%2520it%2520to%2520the%2520resulting%2520DC%2520Program%252C%2520obtaining%2520comparable%250Atheoretical%2520guarantees%2520as%2520in%2520the%2520set%2520function%2520case.%2520The%2520algorithm%2520can%2520be%250Aapplied%2520to%2520continuous%2520domains%2520via%2520discretization.%2520Experiments%2520demonstrate%2520that%250Aour%2520method%2520outperforms%2520baselines%2520in%2520integer%2520compressive%2520sensing%2520and%2520integer%250Aleast%2520squares.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07952v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discrete%20and%20Continuous%20Difference%20of%20Submodular%20Minimization&entry.906535625=George%20Orfanides%20and%20Tim%20Hoheisel%20and%20Marwa%20El%20Halabi&entry.1292438233=%20%20Submodular%20functions%2C%20defined%20on%20continuous%20or%20discrete%20domains%2C%20arise%20in%0Anumerous%20applications.%20We%20study%20the%20minimization%20of%20the%20difference%20of%20two%0Asubmodular%20%28DS%29%20functions%2C%20over%20both%20domains%2C%20extending%20prior%20work%20restricted%0Ato%20set%20functions.%20We%20show%20that%20all%20functions%20on%20discrete%20domains%20and%20all%20smooth%0Afunctions%20on%20continuous%20domains%20are%20DS.%20For%20discrete%20domains%2C%20we%20observe%20that%0ADS%20minimization%20is%20equivalent%20to%20minimizing%20the%20difference%20of%20two%20convex%20%28DC%29%0Afunctions%2C%20as%20in%20the%20set%20function%20case.%20We%20propose%20a%20novel%20variant%20of%20the%20DC%0AAlgorithm%20%28DCA%29%20and%20apply%20it%20to%20the%20resulting%20DC%20Program%2C%20obtaining%20comparable%0Atheoretical%20guarantees%20as%20in%20the%20set%20function%20case.%20The%20algorithm%20can%20be%0Aapplied%20to%20continuous%20domains%20via%20discretization.%20Experiments%20demonstrate%20that%0Aour%20method%20outperforms%20baselines%20in%20integer%20compressive%20sensing%20and%20integer%0Aleast%20squares.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07952v2&entry.124074799=Read"},
{"title": "Causal Machine Learning for Patient-Level Intraoperative Opioid Dose\n  Prediction from Electronic Health Records", "author": "Jonas Valbj\u00f8rn Andersena and Anders Peder H\u00f8jer Karlsen and Markus Harboe Olsen and Nikolaj Krebs Pedersen", "abstract": "  This paper introduces the OPIAID algorithm, a novel approach for predicting\nand recommending personalized opioid dosages for individual patients. The\nalgorithm optimizes pain management while minimizing opioid related adverse\nevents (ORADE) by employing machine learning models trained on observational\nelectronic health records (EHR) data. It leverages a causal machine learning\napproach to understand the relationship between opioid dose, case specific\npatient and intraoperative characteristics, and pain versus ORADE outcomes. The\nOPIAID algorithm considers patient-specific characteristics and the influence\nof different opiates, enabling personalized dose recommendations. This paper\noutlines the algorithm's methodology and architecture, and discusses key\nassumptions, and approaches to evaluating its performance.\n", "link": "http://arxiv.org/abs/2508.09059v1", "date": "2025-08-12", "relevancy": 1.2691, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4549}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4424}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Machine%20Learning%20for%20Patient-Level%20Intraoperative%20Opioid%20Dose%0A%20%20Prediction%20from%20Electronic%20Health%20Records&body=Title%3A%20Causal%20Machine%20Learning%20for%20Patient-Level%20Intraoperative%20Opioid%20Dose%0A%20%20Prediction%20from%20Electronic%20Health%20Records%0AAuthor%3A%20Jonas%20Valbj%C3%B8rn%20Andersena%20and%20Anders%20Peder%20H%C3%B8jer%20Karlsen%20and%20Markus%20Harboe%20Olsen%20and%20Nikolaj%20Krebs%20Pedersen%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20OPIAID%20algorithm%2C%20a%20novel%20approach%20for%20predicting%0Aand%20recommending%20personalized%20opioid%20dosages%20for%20individual%20patients.%20The%0Aalgorithm%20optimizes%20pain%20management%20while%20minimizing%20opioid%20related%20adverse%0Aevents%20%28ORADE%29%20by%20employing%20machine%20learning%20models%20trained%20on%20observational%0Aelectronic%20health%20records%20%28EHR%29%20data.%20It%20leverages%20a%20causal%20machine%20learning%0Aapproach%20to%20understand%20the%20relationship%20between%20opioid%20dose%2C%20case%20specific%0Apatient%20and%20intraoperative%20characteristics%2C%20and%20pain%20versus%20ORADE%20outcomes.%20The%0AOPIAID%20algorithm%20considers%20patient-specific%20characteristics%20and%20the%20influence%0Aof%20different%20opiates%2C%20enabling%20personalized%20dose%20recommendations.%20This%20paper%0Aoutlines%20the%20algorithm%27s%20methodology%20and%20architecture%2C%20and%20discusses%20key%0Aassumptions%2C%20and%20approaches%20to%20evaluating%20its%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Machine%2520Learning%2520for%2520Patient-Level%2520Intraoperative%2520Opioid%2520Dose%250A%2520%2520Prediction%2520from%2520Electronic%2520Health%2520Records%26entry.906535625%3DJonas%2520Valbj%25C3%25B8rn%2520Andersena%2520and%2520Anders%2520Peder%2520H%25C3%25B8jer%2520Karlsen%2520and%2520Markus%2520Harboe%2520Olsen%2520and%2520Nikolaj%2520Krebs%2520Pedersen%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520OPIAID%2520algorithm%252C%2520a%2520novel%2520approach%2520for%2520predicting%250Aand%2520recommending%2520personalized%2520opioid%2520dosages%2520for%2520individual%2520patients.%2520The%250Aalgorithm%2520optimizes%2520pain%2520management%2520while%2520minimizing%2520opioid%2520related%2520adverse%250Aevents%2520%2528ORADE%2529%2520by%2520employing%2520machine%2520learning%2520models%2520trained%2520on%2520observational%250Aelectronic%2520health%2520records%2520%2528EHR%2529%2520data.%2520It%2520leverages%2520a%2520causal%2520machine%2520learning%250Aapproach%2520to%2520understand%2520the%2520relationship%2520between%2520opioid%2520dose%252C%2520case%2520specific%250Apatient%2520and%2520intraoperative%2520characteristics%252C%2520and%2520pain%2520versus%2520ORADE%2520outcomes.%2520The%250AOPIAID%2520algorithm%2520considers%2520patient-specific%2520characteristics%2520and%2520the%2520influence%250Aof%2520different%2520opiates%252C%2520enabling%2520personalized%2520dose%2520recommendations.%2520This%2520paper%250Aoutlines%2520the%2520algorithm%2527s%2520methodology%2520and%2520architecture%252C%2520and%2520discusses%2520key%250Aassumptions%252C%2520and%2520approaches%2520to%2520evaluating%2520its%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Machine%20Learning%20for%20Patient-Level%20Intraoperative%20Opioid%20Dose%0A%20%20Prediction%20from%20Electronic%20Health%20Records&entry.906535625=Jonas%20Valbj%C3%B8rn%20Andersena%20and%20Anders%20Peder%20H%C3%B8jer%20Karlsen%20and%20Markus%20Harboe%20Olsen%20and%20Nikolaj%20Krebs%20Pedersen&entry.1292438233=%20%20This%20paper%20introduces%20the%20OPIAID%20algorithm%2C%20a%20novel%20approach%20for%20predicting%0Aand%20recommending%20personalized%20opioid%20dosages%20for%20individual%20patients.%20The%0Aalgorithm%20optimizes%20pain%20management%20while%20minimizing%20opioid%20related%20adverse%0Aevents%20%28ORADE%29%20by%20employing%20machine%20learning%20models%20trained%20on%20observational%0Aelectronic%20health%20records%20%28EHR%29%20data.%20It%20leverages%20a%20causal%20machine%20learning%0Aapproach%20to%20understand%20the%20relationship%20between%20opioid%20dose%2C%20case%20specific%0Apatient%20and%20intraoperative%20characteristics%2C%20and%20pain%20versus%20ORADE%20outcomes.%20The%0AOPIAID%20algorithm%20considers%20patient-specific%20characteristics%20and%20the%20influence%0Aof%20different%20opiates%2C%20enabling%20personalized%20dose%20recommendations.%20This%20paper%0Aoutlines%20the%20algorithm%27s%20methodology%20and%20architecture%2C%20and%20discusses%20key%0Aassumptions%2C%20and%20approaches%20to%20evaluating%20its%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09059v1&entry.124074799=Read"},
{"title": "CVCM Track Circuits Pre-emptive Failure Diagnostics for Predictive\n  Maintenance Using Deep Neural Networks", "author": "Debdeep Mukherjee and Eduardo Di Santi and Cl\u00e9ment Lefebvre and Nenad Mijatovic and Victor Martin and Thierry Josse and Jonathan Brown and Kenza Saiah", "abstract": "  Track circuits are critical for railway operations, acting as the main\nsignalling sub-system to locate trains. Continuous Variable Current Modulation\n(CVCM) is one such technology. Like any field-deployed, safety-critical asset,\nit can fail, triggering cascading disruptions. Many failures originate as\nsubtle anomalies that evolve over time, often not visually apparent in\nmonitored signals. Conventional approaches, which rely on clear signal changes,\nstruggle to detect them early. Early identification of failure types is\nessential to improve maintenance planning, minimising downtime and revenue\nloss. Leveraging deep neural networks, we propose a predictive maintenance\nframework that classifies anomalies well before they escalate into failures.\nValidated on 10 CVCM failure cases across different installations, the method\nis ISO-17359 compliant and outperforms conventional techniques, achieving\n99.31% overall accuracy with detection within 1% of anomaly onset. Through\nconformal prediction, we provide uncertainty estimates, reaching 99% confidence\nwith consistent coverage across classes. Given CVCMs global deployment, the\napproach is scalable and adaptable to other track circuits and railway systems,\nenhancing operational reliability.\n", "link": "http://arxiv.org/abs/2508.09054v1", "date": "2025-08-12", "relevancy": 1.3978, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4734}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4682}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CVCM%20Track%20Circuits%20Pre-emptive%20Failure%20Diagnostics%20for%20Predictive%0A%20%20Maintenance%20Using%20Deep%20Neural%20Networks&body=Title%3A%20CVCM%20Track%20Circuits%20Pre-emptive%20Failure%20Diagnostics%20for%20Predictive%0A%20%20Maintenance%20Using%20Deep%20Neural%20Networks%0AAuthor%3A%20Debdeep%20Mukherjee%20and%20Eduardo%20Di%20Santi%20and%20Cl%C3%A9ment%20Lefebvre%20and%20Nenad%20Mijatovic%20and%20Victor%20Martin%20and%20Thierry%20Josse%20and%20Jonathan%20Brown%20and%20Kenza%20Saiah%0AAbstract%3A%20%20%20Track%20circuits%20are%20critical%20for%20railway%20operations%2C%20acting%20as%20the%20main%0Asignalling%20sub-system%20to%20locate%20trains.%20Continuous%20Variable%20Current%20Modulation%0A%28CVCM%29%20is%20one%20such%20technology.%20Like%20any%20field-deployed%2C%20safety-critical%20asset%2C%0Ait%20can%20fail%2C%20triggering%20cascading%20disruptions.%20Many%20failures%20originate%20as%0Asubtle%20anomalies%20that%20evolve%20over%20time%2C%20often%20not%20visually%20apparent%20in%0Amonitored%20signals.%20Conventional%20approaches%2C%20which%20rely%20on%20clear%20signal%20changes%2C%0Astruggle%20to%20detect%20them%20early.%20Early%20identification%20of%20failure%20types%20is%0Aessential%20to%20improve%20maintenance%20planning%2C%20minimising%20downtime%20and%20revenue%0Aloss.%20Leveraging%20deep%20neural%20networks%2C%20we%20propose%20a%20predictive%20maintenance%0Aframework%20that%20classifies%20anomalies%20well%20before%20they%20escalate%20into%20failures.%0AValidated%20on%2010%20CVCM%20failure%20cases%20across%20different%20installations%2C%20the%20method%0Ais%20ISO-17359%20compliant%20and%20outperforms%20conventional%20techniques%2C%20achieving%0A99.31%25%20overall%20accuracy%20with%20detection%20within%201%25%20of%20anomaly%20onset.%20Through%0Aconformal%20prediction%2C%20we%20provide%20uncertainty%20estimates%2C%20reaching%2099%25%20confidence%0Awith%20consistent%20coverage%20across%20classes.%20Given%20CVCMs%20global%20deployment%2C%20the%0Aapproach%20is%20scalable%20and%20adaptable%20to%20other%20track%20circuits%20and%20railway%20systems%2C%0Aenhancing%20operational%20reliability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCVCM%2520Track%2520Circuits%2520Pre-emptive%2520Failure%2520Diagnostics%2520for%2520Predictive%250A%2520%2520Maintenance%2520Using%2520Deep%2520Neural%2520Networks%26entry.906535625%3DDebdeep%2520Mukherjee%2520and%2520Eduardo%2520Di%2520Santi%2520and%2520Cl%25C3%25A9ment%2520Lefebvre%2520and%2520Nenad%2520Mijatovic%2520and%2520Victor%2520Martin%2520and%2520Thierry%2520Josse%2520and%2520Jonathan%2520Brown%2520and%2520Kenza%2520Saiah%26entry.1292438233%3D%2520%2520Track%2520circuits%2520are%2520critical%2520for%2520railway%2520operations%252C%2520acting%2520as%2520the%2520main%250Asignalling%2520sub-system%2520to%2520locate%2520trains.%2520Continuous%2520Variable%2520Current%2520Modulation%250A%2528CVCM%2529%2520is%2520one%2520such%2520technology.%2520Like%2520any%2520field-deployed%252C%2520safety-critical%2520asset%252C%250Ait%2520can%2520fail%252C%2520triggering%2520cascading%2520disruptions.%2520Many%2520failures%2520originate%2520as%250Asubtle%2520anomalies%2520that%2520evolve%2520over%2520time%252C%2520often%2520not%2520visually%2520apparent%2520in%250Amonitored%2520signals.%2520Conventional%2520approaches%252C%2520which%2520rely%2520on%2520clear%2520signal%2520changes%252C%250Astruggle%2520to%2520detect%2520them%2520early.%2520Early%2520identification%2520of%2520failure%2520types%2520is%250Aessential%2520to%2520improve%2520maintenance%2520planning%252C%2520minimising%2520downtime%2520and%2520revenue%250Aloss.%2520Leveraging%2520deep%2520neural%2520networks%252C%2520we%2520propose%2520a%2520predictive%2520maintenance%250Aframework%2520that%2520classifies%2520anomalies%2520well%2520before%2520they%2520escalate%2520into%2520failures.%250AValidated%2520on%252010%2520CVCM%2520failure%2520cases%2520across%2520different%2520installations%252C%2520the%2520method%250Ais%2520ISO-17359%2520compliant%2520and%2520outperforms%2520conventional%2520techniques%252C%2520achieving%250A99.31%2525%2520overall%2520accuracy%2520with%2520detection%2520within%25201%2525%2520of%2520anomaly%2520onset.%2520Through%250Aconformal%2520prediction%252C%2520we%2520provide%2520uncertainty%2520estimates%252C%2520reaching%252099%2525%2520confidence%250Awith%2520consistent%2520coverage%2520across%2520classes.%2520Given%2520CVCMs%2520global%2520deployment%252C%2520the%250Aapproach%2520is%2520scalable%2520and%2520adaptable%2520to%2520other%2520track%2520circuits%2520and%2520railway%2520systems%252C%250Aenhancing%2520operational%2520reliability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CVCM%20Track%20Circuits%20Pre-emptive%20Failure%20Diagnostics%20for%20Predictive%0A%20%20Maintenance%20Using%20Deep%20Neural%20Networks&entry.906535625=Debdeep%20Mukherjee%20and%20Eduardo%20Di%20Santi%20and%20Cl%C3%A9ment%20Lefebvre%20and%20Nenad%20Mijatovic%20and%20Victor%20Martin%20and%20Thierry%20Josse%20and%20Jonathan%20Brown%20and%20Kenza%20Saiah&entry.1292438233=%20%20Track%20circuits%20are%20critical%20for%20railway%20operations%2C%20acting%20as%20the%20main%0Asignalling%20sub-system%20to%20locate%20trains.%20Continuous%20Variable%20Current%20Modulation%0A%28CVCM%29%20is%20one%20such%20technology.%20Like%20any%20field-deployed%2C%20safety-critical%20asset%2C%0Ait%20can%20fail%2C%20triggering%20cascading%20disruptions.%20Many%20failures%20originate%20as%0Asubtle%20anomalies%20that%20evolve%20over%20time%2C%20often%20not%20visually%20apparent%20in%0Amonitored%20signals.%20Conventional%20approaches%2C%20which%20rely%20on%20clear%20signal%20changes%2C%0Astruggle%20to%20detect%20them%20early.%20Early%20identification%20of%20failure%20types%20is%0Aessential%20to%20improve%20maintenance%20planning%2C%20minimising%20downtime%20and%20revenue%0Aloss.%20Leveraging%20deep%20neural%20networks%2C%20we%20propose%20a%20predictive%20maintenance%0Aframework%20that%20classifies%20anomalies%20well%20before%20they%20escalate%20into%20failures.%0AValidated%20on%2010%20CVCM%20failure%20cases%20across%20different%20installations%2C%20the%20method%0Ais%20ISO-17359%20compliant%20and%20outperforms%20conventional%20techniques%2C%20achieving%0A99.31%25%20overall%20accuracy%20with%20detection%20within%201%25%20of%20anomaly%20onset.%20Through%0Aconformal%20prediction%2C%20we%20provide%20uncertainty%20estimates%2C%20reaching%2099%25%20confidence%0Awith%20consistent%20coverage%20across%20classes.%20Given%20CVCMs%20global%20deployment%2C%20the%0Aapproach%20is%20scalable%20and%20adaptable%20to%20other%20track%20circuits%20and%20railway%20systems%2C%0Aenhancing%20operational%20reliability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09054v1&entry.124074799=Read"},
{"title": "Constrained free energy minimization for the design of thermal states\n  and stabilizer thermodynamic systems", "author": "Michele Minervini and Madison Chin and Jacob Kupperman and Nana Liu and Ivy Luo and Meghan Ly and Soorya Rethinasamy and Kathie Wang and Mark M. Wilde", "abstract": "  A quantum thermodynamic system is described by a Hamiltonian and a list of\nconserved, non-commuting charges, and a fundamental goal is to determine the\nminimum energy of the system subject to constraints on the charges. Recently,\n[Liu et al., arXiv:2505.04514] proposed first- and second-order classical and\nhybrid quantum-classical algorithms for solving a dual chemical potential\nmaximization problem, and they proved that these algorithms converge to global\noptima by means of gradient-ascent approaches. In this paper, we benchmark\nthese algorithms on several problems of interest in thermodynamics, including\none- and two-dimensional quantum Heisenberg models with nearest and\nnext-to-nearest neighbor interactions and with the charges set to the total\n$x$, $y$, and $z$ magnetizations. We also offer an alternative compelling\ninterpretation of these algorithms as methods for designing ground and thermal\nstates of controllable Hamiltonians, with potential applications in molecular\nand material design. Furthermore, we introduce stabilizer thermodynamic systems\nas thermodynamic systems based on stabilizer codes, with the Hamiltonian\nconstructed from a given code's stabilizer operators and the charges\nconstructed from the code's logical operators. We benchmark the aforementioned\nalgorithms on several examples of stabilizer thermodynamic systems, including\nthose constructed from the one-to-three-qubit repetition code, the perfect\none-to-five-qubit code, and the two-to-four-qubit error-detecting code.\nFinally, we observe that the aforementioned hybrid quantum-classical\nalgorithms, when applied to stabilizer thermodynamic systems, can serve as\nalternative methods for encoding qubits into stabilizer codes at a fixed\ntemperature, and we provide an effective method for warm-starting these\nencoding algorithms whenever a single qubit is encoded into multiple physical\nqubits.\n", "link": "http://arxiv.org/abs/2508.09103v1", "date": "2025-08-12", "relevancy": 0.7876, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3967}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3928}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constrained%20free%20energy%20minimization%20for%20the%20design%20of%20thermal%20states%0A%20%20and%20stabilizer%20thermodynamic%20systems&body=Title%3A%20Constrained%20free%20energy%20minimization%20for%20the%20design%20of%20thermal%20states%0A%20%20and%20stabilizer%20thermodynamic%20systems%0AAuthor%3A%20Michele%20Minervini%20and%20Madison%20Chin%20and%20Jacob%20Kupperman%20and%20Nana%20Liu%20and%20Ivy%20Luo%20and%20Meghan%20Ly%20and%20Soorya%20Rethinasamy%20and%20Kathie%20Wang%20and%20Mark%20M.%20Wilde%0AAbstract%3A%20%20%20A%20quantum%20thermodynamic%20system%20is%20described%20by%20a%20Hamiltonian%20and%20a%20list%20of%0Aconserved%2C%20non-commuting%20charges%2C%20and%20a%20fundamental%20goal%20is%20to%20determine%20the%0Aminimum%20energy%20of%20the%20system%20subject%20to%20constraints%20on%20the%20charges.%20Recently%2C%0A%5BLiu%20et%20al.%2C%20arXiv%3A2505.04514%5D%20proposed%20first-%20and%20second-order%20classical%20and%0Ahybrid%20quantum-classical%20algorithms%20for%20solving%20a%20dual%20chemical%20potential%0Amaximization%20problem%2C%20and%20they%20proved%20that%20these%20algorithms%20converge%20to%20global%0Aoptima%20by%20means%20of%20gradient-ascent%20approaches.%20In%20this%20paper%2C%20we%20benchmark%0Athese%20algorithms%20on%20several%20problems%20of%20interest%20in%20thermodynamics%2C%20including%0Aone-%20and%20two-dimensional%20quantum%20Heisenberg%20models%20with%20nearest%20and%0Anext-to-nearest%20neighbor%20interactions%20and%20with%20the%20charges%20set%20to%20the%20total%0A%24x%24%2C%20%24y%24%2C%20and%20%24z%24%20magnetizations.%20We%20also%20offer%20an%20alternative%20compelling%0Ainterpretation%20of%20these%20algorithms%20as%20methods%20for%20designing%20ground%20and%20thermal%0Astates%20of%20controllable%20Hamiltonians%2C%20with%20potential%20applications%20in%20molecular%0Aand%20material%20design.%20Furthermore%2C%20we%20introduce%20stabilizer%20thermodynamic%20systems%0Aas%20thermodynamic%20systems%20based%20on%20stabilizer%20codes%2C%20with%20the%20Hamiltonian%0Aconstructed%20from%20a%20given%20code%27s%20stabilizer%20operators%20and%20the%20charges%0Aconstructed%20from%20the%20code%27s%20logical%20operators.%20We%20benchmark%20the%20aforementioned%0Aalgorithms%20on%20several%20examples%20of%20stabilizer%20thermodynamic%20systems%2C%20including%0Athose%20constructed%20from%20the%20one-to-three-qubit%20repetition%20code%2C%20the%20perfect%0Aone-to-five-qubit%20code%2C%20and%20the%20two-to-four-qubit%20error-detecting%20code.%0AFinally%2C%20we%20observe%20that%20the%20aforementioned%20hybrid%20quantum-classical%0Aalgorithms%2C%20when%20applied%20to%20stabilizer%20thermodynamic%20systems%2C%20can%20serve%20as%0Aalternative%20methods%20for%20encoding%20qubits%20into%20stabilizer%20codes%20at%20a%20fixed%0Atemperature%2C%20and%20we%20provide%20an%20effective%20method%20for%20warm-starting%20these%0Aencoding%20algorithms%20whenever%20a%20single%20qubit%20is%20encoded%20into%20multiple%20physical%0Aqubits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstrained%2520free%2520energy%2520minimization%2520for%2520the%2520design%2520of%2520thermal%2520states%250A%2520%2520and%2520stabilizer%2520thermodynamic%2520systems%26entry.906535625%3DMichele%2520Minervini%2520and%2520Madison%2520Chin%2520and%2520Jacob%2520Kupperman%2520and%2520Nana%2520Liu%2520and%2520Ivy%2520Luo%2520and%2520Meghan%2520Ly%2520and%2520Soorya%2520Rethinasamy%2520and%2520Kathie%2520Wang%2520and%2520Mark%2520M.%2520Wilde%26entry.1292438233%3D%2520%2520A%2520quantum%2520thermodynamic%2520system%2520is%2520described%2520by%2520a%2520Hamiltonian%2520and%2520a%2520list%2520of%250Aconserved%252C%2520non-commuting%2520charges%252C%2520and%2520a%2520fundamental%2520goal%2520is%2520to%2520determine%2520the%250Aminimum%2520energy%2520of%2520the%2520system%2520subject%2520to%2520constraints%2520on%2520the%2520charges.%2520Recently%252C%250A%255BLiu%2520et%2520al.%252C%2520arXiv%253A2505.04514%255D%2520proposed%2520first-%2520and%2520second-order%2520classical%2520and%250Ahybrid%2520quantum-classical%2520algorithms%2520for%2520solving%2520a%2520dual%2520chemical%2520potential%250Amaximization%2520problem%252C%2520and%2520they%2520proved%2520that%2520these%2520algorithms%2520converge%2520to%2520global%250Aoptima%2520by%2520means%2520of%2520gradient-ascent%2520approaches.%2520In%2520this%2520paper%252C%2520we%2520benchmark%250Athese%2520algorithms%2520on%2520several%2520problems%2520of%2520interest%2520in%2520thermodynamics%252C%2520including%250Aone-%2520and%2520two-dimensional%2520quantum%2520Heisenberg%2520models%2520with%2520nearest%2520and%250Anext-to-nearest%2520neighbor%2520interactions%2520and%2520with%2520the%2520charges%2520set%2520to%2520the%2520total%250A%2524x%2524%252C%2520%2524y%2524%252C%2520and%2520%2524z%2524%2520magnetizations.%2520We%2520also%2520offer%2520an%2520alternative%2520compelling%250Ainterpretation%2520of%2520these%2520algorithms%2520as%2520methods%2520for%2520designing%2520ground%2520and%2520thermal%250Astates%2520of%2520controllable%2520Hamiltonians%252C%2520with%2520potential%2520applications%2520in%2520molecular%250Aand%2520material%2520design.%2520Furthermore%252C%2520we%2520introduce%2520stabilizer%2520thermodynamic%2520systems%250Aas%2520thermodynamic%2520systems%2520based%2520on%2520stabilizer%2520codes%252C%2520with%2520the%2520Hamiltonian%250Aconstructed%2520from%2520a%2520given%2520code%2527s%2520stabilizer%2520operators%2520and%2520the%2520charges%250Aconstructed%2520from%2520the%2520code%2527s%2520logical%2520operators.%2520We%2520benchmark%2520the%2520aforementioned%250Aalgorithms%2520on%2520several%2520examples%2520of%2520stabilizer%2520thermodynamic%2520systems%252C%2520including%250Athose%2520constructed%2520from%2520the%2520one-to-three-qubit%2520repetition%2520code%252C%2520the%2520perfect%250Aone-to-five-qubit%2520code%252C%2520and%2520the%2520two-to-four-qubit%2520error-detecting%2520code.%250AFinally%252C%2520we%2520observe%2520that%2520the%2520aforementioned%2520hybrid%2520quantum-classical%250Aalgorithms%252C%2520when%2520applied%2520to%2520stabilizer%2520thermodynamic%2520systems%252C%2520can%2520serve%2520as%250Aalternative%2520methods%2520for%2520encoding%2520qubits%2520into%2520stabilizer%2520codes%2520at%2520a%2520fixed%250Atemperature%252C%2520and%2520we%2520provide%2520an%2520effective%2520method%2520for%2520warm-starting%2520these%250Aencoding%2520algorithms%2520whenever%2520a%2520single%2520qubit%2520is%2520encoded%2520into%2520multiple%2520physical%250Aqubits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constrained%20free%20energy%20minimization%20for%20the%20design%20of%20thermal%20states%0A%20%20and%20stabilizer%20thermodynamic%20systems&entry.906535625=Michele%20Minervini%20and%20Madison%20Chin%20and%20Jacob%20Kupperman%20and%20Nana%20Liu%20and%20Ivy%20Luo%20and%20Meghan%20Ly%20and%20Soorya%20Rethinasamy%20and%20Kathie%20Wang%20and%20Mark%20M.%20Wilde&entry.1292438233=%20%20A%20quantum%20thermodynamic%20system%20is%20described%20by%20a%20Hamiltonian%20and%20a%20list%20of%0Aconserved%2C%20non-commuting%20charges%2C%20and%20a%20fundamental%20goal%20is%20to%20determine%20the%0Aminimum%20energy%20of%20the%20system%20subject%20to%20constraints%20on%20the%20charges.%20Recently%2C%0A%5BLiu%20et%20al.%2C%20arXiv%3A2505.04514%5D%20proposed%20first-%20and%20second-order%20classical%20and%0Ahybrid%20quantum-classical%20algorithms%20for%20solving%20a%20dual%20chemical%20potential%0Amaximization%20problem%2C%20and%20they%20proved%20that%20these%20algorithms%20converge%20to%20global%0Aoptima%20by%20means%20of%20gradient-ascent%20approaches.%20In%20this%20paper%2C%20we%20benchmark%0Athese%20algorithms%20on%20several%20problems%20of%20interest%20in%20thermodynamics%2C%20including%0Aone-%20and%20two-dimensional%20quantum%20Heisenberg%20models%20with%20nearest%20and%0Anext-to-nearest%20neighbor%20interactions%20and%20with%20the%20charges%20set%20to%20the%20total%0A%24x%24%2C%20%24y%24%2C%20and%20%24z%24%20magnetizations.%20We%20also%20offer%20an%20alternative%20compelling%0Ainterpretation%20of%20these%20algorithms%20as%20methods%20for%20designing%20ground%20and%20thermal%0Astates%20of%20controllable%20Hamiltonians%2C%20with%20potential%20applications%20in%20molecular%0Aand%20material%20design.%20Furthermore%2C%20we%20introduce%20stabilizer%20thermodynamic%20systems%0Aas%20thermodynamic%20systems%20based%20on%20stabilizer%20codes%2C%20with%20the%20Hamiltonian%0Aconstructed%20from%20a%20given%20code%27s%20stabilizer%20operators%20and%20the%20charges%0Aconstructed%20from%20the%20code%27s%20logical%20operators.%20We%20benchmark%20the%20aforementioned%0Aalgorithms%20on%20several%20examples%20of%20stabilizer%20thermodynamic%20systems%2C%20including%0Athose%20constructed%20from%20the%20one-to-three-qubit%20repetition%20code%2C%20the%20perfect%0Aone-to-five-qubit%20code%2C%20and%20the%20two-to-four-qubit%20error-detecting%20code.%0AFinally%2C%20we%20observe%20that%20the%20aforementioned%20hybrid%20quantum-classical%0Aalgorithms%2C%20when%20applied%20to%20stabilizer%20thermodynamic%20systems%2C%20can%20serve%20as%0Aalternative%20methods%20for%20encoding%20qubits%20into%20stabilizer%20codes%20at%20a%20fixed%0Atemperature%2C%20and%20we%20provide%20an%20effective%20method%20for%20warm-starting%20these%0Aencoding%20algorithms%20whenever%20a%20single%20qubit%20is%20encoded%20into%20multiple%20physical%0Aqubits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09103v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


