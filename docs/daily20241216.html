<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241215.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view\n  Diffusion", "author": "Jiapeng Tang and Davide Davoli and Tobias Kirschstein and Liam Schoneveld and Matthias Niessner", "abstract": "  We propose a novel approach for reconstructing animatable 3D Gaussian avatars\nfrom monocular videos captured by commodity devices like smartphones.\nPhotorealistic 3D head avatar reconstruction from such recordings is\nchallenging due to limited observations, which leaves unobserved regions\nunder-constrained and can lead to artifacts in novel views. To address this\nproblem, we introduce a multi-view head diffusion model, leveraging its priors\nto fill in missing regions and ensure view consistency in Gaussian splatting\nrenderings. To enable precise viewpoint control, we use normal maps rendered\nfrom FLAME-based head reconstruction, which provides pixel-aligned inductive\nbiases. We also condition the diffusion model on VAE features extracted from\nthe input image to preserve details of facial identity and appearance. For\nGaussian avatar reconstruction, we distill multi-view diffusion priors by using\niteratively denoised images as pseudo-ground truths, effectively mitigating\nover-saturation issues. To further improve photorealism, we apply latent\nupsampling to refine the denoised latent before decoding it into an image. We\nevaluate our method on the NeRSemble dataset, showing that GAF outperforms the\nprevious state-of-the-art methods in novel view synthesis by a 5.34\\% higher\nSSIM score. Furthermore, we demonstrate higher-fidelity avatar reconstructions\nfrom monocular videos captured on commodity devices.\n", "link": "http://arxiv.org/abs/2412.10209v1", "date": "2024-12-13", "relevancy": 3.6987, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7625}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7625}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAF%3A%20Gaussian%20Avatar%20Reconstruction%20from%20Monocular%20Videos%20via%20Multi-view%0A%20%20Diffusion&body=Title%3A%20GAF%3A%20Gaussian%20Avatar%20Reconstruction%20from%20Monocular%20Videos%20via%20Multi-view%0A%20%20Diffusion%0AAuthor%3A%20Jiapeng%20Tang%20and%20Davide%20Davoli%20and%20Tobias%20Kirschstein%20and%20Liam%20Schoneveld%20and%20Matthias%20Niessner%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20approach%20for%20reconstructing%20animatable%203D%20Gaussian%20avatars%0Afrom%20monocular%20videos%20captured%20by%20commodity%20devices%20like%20smartphones.%0APhotorealistic%203D%20head%20avatar%20reconstruction%20from%20such%20recordings%20is%0Achallenging%20due%20to%20limited%20observations%2C%20which%20leaves%20unobserved%20regions%0Aunder-constrained%20and%20can%20lead%20to%20artifacts%20in%20novel%20views.%20To%20address%20this%0Aproblem%2C%20we%20introduce%20a%20multi-view%20head%20diffusion%20model%2C%20leveraging%20its%20priors%0Ato%20fill%20in%20missing%20regions%20and%20ensure%20view%20consistency%20in%20Gaussian%20splatting%0Arenderings.%20To%20enable%20precise%20viewpoint%20control%2C%20we%20use%20normal%20maps%20rendered%0Afrom%20FLAME-based%20head%20reconstruction%2C%20which%20provides%20pixel-aligned%20inductive%0Abiases.%20We%20also%20condition%20the%20diffusion%20model%20on%20VAE%20features%20extracted%20from%0Athe%20input%20image%20to%20preserve%20details%20of%20facial%20identity%20and%20appearance.%20For%0AGaussian%20avatar%20reconstruction%2C%20we%20distill%20multi-view%20diffusion%20priors%20by%20using%0Aiteratively%20denoised%20images%20as%20pseudo-ground%20truths%2C%20effectively%20mitigating%0Aover-saturation%20issues.%20To%20further%20improve%20photorealism%2C%20we%20apply%20latent%0Aupsampling%20to%20refine%20the%20denoised%20latent%20before%20decoding%20it%20into%20an%20image.%20We%0Aevaluate%20our%20method%20on%20the%20NeRSemble%20dataset%2C%20showing%20that%20GAF%20outperforms%20the%0Aprevious%20state-of-the-art%20methods%20in%20novel%20view%20synthesis%20by%20a%205.34%5C%25%20higher%0ASSIM%20score.%20Furthermore%2C%20we%20demonstrate%20higher-fidelity%20avatar%20reconstructions%0Afrom%20monocular%20videos%20captured%20on%20commodity%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAF%253A%2520Gaussian%2520Avatar%2520Reconstruction%2520from%2520Monocular%2520Videos%2520via%2520Multi-view%250A%2520%2520Diffusion%26entry.906535625%3DJiapeng%2520Tang%2520and%2520Davide%2520Davoli%2520and%2520Tobias%2520Kirschstein%2520and%2520Liam%2520Schoneveld%2520and%2520Matthias%2520Niessner%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520approach%2520for%2520reconstructing%2520animatable%25203D%2520Gaussian%2520avatars%250Afrom%2520monocular%2520videos%2520captured%2520by%2520commodity%2520devices%2520like%2520smartphones.%250APhotorealistic%25203D%2520head%2520avatar%2520reconstruction%2520from%2520such%2520recordings%2520is%250Achallenging%2520due%2520to%2520limited%2520observations%252C%2520which%2520leaves%2520unobserved%2520regions%250Aunder-constrained%2520and%2520can%2520lead%2520to%2520artifacts%2520in%2520novel%2520views.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520introduce%2520a%2520multi-view%2520head%2520diffusion%2520model%252C%2520leveraging%2520its%2520priors%250Ato%2520fill%2520in%2520missing%2520regions%2520and%2520ensure%2520view%2520consistency%2520in%2520Gaussian%2520splatting%250Arenderings.%2520To%2520enable%2520precise%2520viewpoint%2520control%252C%2520we%2520use%2520normal%2520maps%2520rendered%250Afrom%2520FLAME-based%2520head%2520reconstruction%252C%2520which%2520provides%2520pixel-aligned%2520inductive%250Abiases.%2520We%2520also%2520condition%2520the%2520diffusion%2520model%2520on%2520VAE%2520features%2520extracted%2520from%250Athe%2520input%2520image%2520to%2520preserve%2520details%2520of%2520facial%2520identity%2520and%2520appearance.%2520For%250AGaussian%2520avatar%2520reconstruction%252C%2520we%2520distill%2520multi-view%2520diffusion%2520priors%2520by%2520using%250Aiteratively%2520denoised%2520images%2520as%2520pseudo-ground%2520truths%252C%2520effectively%2520mitigating%250Aover-saturation%2520issues.%2520To%2520further%2520improve%2520photorealism%252C%2520we%2520apply%2520latent%250Aupsampling%2520to%2520refine%2520the%2520denoised%2520latent%2520before%2520decoding%2520it%2520into%2520an%2520image.%2520We%250Aevaluate%2520our%2520method%2520on%2520the%2520NeRSemble%2520dataset%252C%2520showing%2520that%2520GAF%2520outperforms%2520the%250Aprevious%2520state-of-the-art%2520methods%2520in%2520novel%2520view%2520synthesis%2520by%2520a%25205.34%255C%2525%2520higher%250ASSIM%2520score.%2520Furthermore%252C%2520we%2520demonstrate%2520higher-fidelity%2520avatar%2520reconstructions%250Afrom%2520monocular%2520videos%2520captured%2520on%2520commodity%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAF%3A%20Gaussian%20Avatar%20Reconstruction%20from%20Monocular%20Videos%20via%20Multi-view%0A%20%20Diffusion&entry.906535625=Jiapeng%20Tang%20and%20Davide%20Davoli%20and%20Tobias%20Kirschstein%20and%20Liam%20Schoneveld%20and%20Matthias%20Niessner&entry.1292438233=%20%20We%20propose%20a%20novel%20approach%20for%20reconstructing%20animatable%203D%20Gaussian%20avatars%0Afrom%20monocular%20videos%20captured%20by%20commodity%20devices%20like%20smartphones.%0APhotorealistic%203D%20head%20avatar%20reconstruction%20from%20such%20recordings%20is%0Achallenging%20due%20to%20limited%20observations%2C%20which%20leaves%20unobserved%20regions%0Aunder-constrained%20and%20can%20lead%20to%20artifacts%20in%20novel%20views.%20To%20address%20this%0Aproblem%2C%20we%20introduce%20a%20multi-view%20head%20diffusion%20model%2C%20leveraging%20its%20priors%0Ato%20fill%20in%20missing%20regions%20and%20ensure%20view%20consistency%20in%20Gaussian%20splatting%0Arenderings.%20To%20enable%20precise%20viewpoint%20control%2C%20we%20use%20normal%20maps%20rendered%0Afrom%20FLAME-based%20head%20reconstruction%2C%20which%20provides%20pixel-aligned%20inductive%0Abiases.%20We%20also%20condition%20the%20diffusion%20model%20on%20VAE%20features%20extracted%20from%0Athe%20input%20image%20to%20preserve%20details%20of%20facial%20identity%20and%20appearance.%20For%0AGaussian%20avatar%20reconstruction%2C%20we%20distill%20multi-view%20diffusion%20priors%20by%20using%0Aiteratively%20denoised%20images%20as%20pseudo-ground%20truths%2C%20effectively%20mitigating%0Aover-saturation%20issues.%20To%20further%20improve%20photorealism%2C%20we%20apply%20latent%0Aupsampling%20to%20refine%20the%20denoised%20latent%20before%20decoding%20it%20into%20an%20image.%20We%0Aevaluate%20our%20method%20on%20the%20NeRSemble%20dataset%2C%20showing%20that%20GAF%20outperforms%20the%0Aprevious%20state-of-the-art%20methods%20in%20novel%20view%20synthesis%20by%20a%205.34%5C%25%20higher%0ASSIM%20score.%20Furthermore%2C%20we%20demonstrate%20higher-fidelity%20avatar%20reconstructions%0Afrom%20monocular%20videos%20captured%20on%20commodity%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10209v1&entry.124074799=Read"},
{"title": "TSGaussian: Semantic and Depth-Guided Target-Specific Gaussian Splatting\n  from Sparse Views", "author": "Liang Zhao and Zehan Bao and Yi Xie and Hong Chen and Yaohui Chen and Weifu Li", "abstract": "  Recent advances in Gaussian Splatting have significantly advanced the field,\nachieving both panoptic and interactive segmentation of 3D scenes. However,\nexisting methodologies often overlook the critical need for reconstructing\nspecified targets with complex structures from sparse views. To address this\nissue, we introduce TSGaussian, a novel framework that combines semantic\nconstraints with depth priors to avoid geometry degradation in challenging\nnovel view synthesis tasks. Our approach prioritizes computational resources on\ndesignated targets while minimizing background allocation. Bounding boxes from\nYOLOv9 serve as prompts for Segment Anything Model to generate 2D mask\npredictions, ensuring semantic accuracy and cost efficiency. TSGaussian\neffectively clusters 3D gaussians by introducing a compact identity encoding\nfor each Gaussian ellipsoid and incorporating 3D spatial consistency\nregularization. Leveraging these modules, we propose a pruning strategy to\neffectively reduce redundancy in 3D gaussians. Extensive experiments\ndemonstrate that TSGaussian outperforms state-of-the-art methods on three\nstandard datasets and a new challenging dataset we collected, achieving\nsuperior results in novel view synthesis of specific objects. Code is available\nat: https://github.com/leon2000-ai/TSGaussian.\n", "link": "http://arxiv.org/abs/2412.10051v1", "date": "2024-12-13", "relevancy": 3.5433, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7409}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.737}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TSGaussian%3A%20Semantic%20and%20Depth-Guided%20Target-Specific%20Gaussian%20Splatting%0A%20%20from%20Sparse%20Views&body=Title%3A%20TSGaussian%3A%20Semantic%20and%20Depth-Guided%20Target-Specific%20Gaussian%20Splatting%0A%20%20from%20Sparse%20Views%0AAuthor%3A%20Liang%20Zhao%20and%20Zehan%20Bao%20and%20Yi%20Xie%20and%20Hong%20Chen%20and%20Yaohui%20Chen%20and%20Weifu%20Li%0AAbstract%3A%20%20%20Recent%20advances%20in%20Gaussian%20Splatting%20have%20significantly%20advanced%20the%20field%2C%0Aachieving%20both%20panoptic%20and%20interactive%20segmentation%20of%203D%20scenes.%20However%2C%0Aexisting%20methodologies%20often%20overlook%20the%20critical%20need%20for%20reconstructing%0Aspecified%20targets%20with%20complex%20structures%20from%20sparse%20views.%20To%20address%20this%0Aissue%2C%20we%20introduce%20TSGaussian%2C%20a%20novel%20framework%20that%20combines%20semantic%0Aconstraints%20with%20depth%20priors%20to%20avoid%20geometry%20degradation%20in%20challenging%0Anovel%20view%20synthesis%20tasks.%20Our%20approach%20prioritizes%20computational%20resources%20on%0Adesignated%20targets%20while%20minimizing%20background%20allocation.%20Bounding%20boxes%20from%0AYOLOv9%20serve%20as%20prompts%20for%20Segment%20Anything%20Model%20to%20generate%202D%20mask%0Apredictions%2C%20ensuring%20semantic%20accuracy%20and%20cost%20efficiency.%20TSGaussian%0Aeffectively%20clusters%203D%20gaussians%20by%20introducing%20a%20compact%20identity%20encoding%0Afor%20each%20Gaussian%20ellipsoid%20and%20incorporating%203D%20spatial%20consistency%0Aregularization.%20Leveraging%20these%20modules%2C%20we%20propose%20a%20pruning%20strategy%20to%0Aeffectively%20reduce%20redundancy%20in%203D%20gaussians.%20Extensive%20experiments%0Ademonstrate%20that%20TSGaussian%20outperforms%20state-of-the-art%20methods%20on%20three%0Astandard%20datasets%20and%20a%20new%20challenging%20dataset%20we%20collected%2C%20achieving%0Asuperior%20results%20in%20novel%20view%20synthesis%20of%20specific%20objects.%20Code%20is%20available%0Aat%3A%20https%3A//github.com/leon2000-ai/TSGaussian.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTSGaussian%253A%2520Semantic%2520and%2520Depth-Guided%2520Target-Specific%2520Gaussian%2520Splatting%250A%2520%2520from%2520Sparse%2520Views%26entry.906535625%3DLiang%2520Zhao%2520and%2520Zehan%2520Bao%2520and%2520Yi%2520Xie%2520and%2520Hong%2520Chen%2520and%2520Yaohui%2520Chen%2520and%2520Weifu%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Gaussian%2520Splatting%2520have%2520significantly%2520advanced%2520the%2520field%252C%250Aachieving%2520both%2520panoptic%2520and%2520interactive%2520segmentation%2520of%25203D%2520scenes.%2520However%252C%250Aexisting%2520methodologies%2520often%2520overlook%2520the%2520critical%2520need%2520for%2520reconstructing%250Aspecified%2520targets%2520with%2520complex%2520structures%2520from%2520sparse%2520views.%2520To%2520address%2520this%250Aissue%252C%2520we%2520introduce%2520TSGaussian%252C%2520a%2520novel%2520framework%2520that%2520combines%2520semantic%250Aconstraints%2520with%2520depth%2520priors%2520to%2520avoid%2520geometry%2520degradation%2520in%2520challenging%250Anovel%2520view%2520synthesis%2520tasks.%2520Our%2520approach%2520prioritizes%2520computational%2520resources%2520on%250Adesignated%2520targets%2520while%2520minimizing%2520background%2520allocation.%2520Bounding%2520boxes%2520from%250AYOLOv9%2520serve%2520as%2520prompts%2520for%2520Segment%2520Anything%2520Model%2520to%2520generate%25202D%2520mask%250Apredictions%252C%2520ensuring%2520semantic%2520accuracy%2520and%2520cost%2520efficiency.%2520TSGaussian%250Aeffectively%2520clusters%25203D%2520gaussians%2520by%2520introducing%2520a%2520compact%2520identity%2520encoding%250Afor%2520each%2520Gaussian%2520ellipsoid%2520and%2520incorporating%25203D%2520spatial%2520consistency%250Aregularization.%2520Leveraging%2520these%2520modules%252C%2520we%2520propose%2520a%2520pruning%2520strategy%2520to%250Aeffectively%2520reduce%2520redundancy%2520in%25203D%2520gaussians.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520TSGaussian%2520outperforms%2520state-of-the-art%2520methods%2520on%2520three%250Astandard%2520datasets%2520and%2520a%2520new%2520challenging%2520dataset%2520we%2520collected%252C%2520achieving%250Asuperior%2520results%2520in%2520novel%2520view%2520synthesis%2520of%2520specific%2520objects.%2520Code%2520is%2520available%250Aat%253A%2520https%253A//github.com/leon2000-ai/TSGaussian.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TSGaussian%3A%20Semantic%20and%20Depth-Guided%20Target-Specific%20Gaussian%20Splatting%0A%20%20from%20Sparse%20Views&entry.906535625=Liang%20Zhao%20and%20Zehan%20Bao%20and%20Yi%20Xie%20and%20Hong%20Chen%20and%20Yaohui%20Chen%20and%20Weifu%20Li&entry.1292438233=%20%20Recent%20advances%20in%20Gaussian%20Splatting%20have%20significantly%20advanced%20the%20field%2C%0Aachieving%20both%20panoptic%20and%20interactive%20segmentation%20of%203D%20scenes.%20However%2C%0Aexisting%20methodologies%20often%20overlook%20the%20critical%20need%20for%20reconstructing%0Aspecified%20targets%20with%20complex%20structures%20from%20sparse%20views.%20To%20address%20this%0Aissue%2C%20we%20introduce%20TSGaussian%2C%20a%20novel%20framework%20that%20combines%20semantic%0Aconstraints%20with%20depth%20priors%20to%20avoid%20geometry%20degradation%20in%20challenging%0Anovel%20view%20synthesis%20tasks.%20Our%20approach%20prioritizes%20computational%20resources%20on%0Adesignated%20targets%20while%20minimizing%20background%20allocation.%20Bounding%20boxes%20from%0AYOLOv9%20serve%20as%20prompts%20for%20Segment%20Anything%20Model%20to%20generate%202D%20mask%0Apredictions%2C%20ensuring%20semantic%20accuracy%20and%20cost%20efficiency.%20TSGaussian%0Aeffectively%20clusters%203D%20gaussians%20by%20introducing%20a%20compact%20identity%20encoding%0Afor%20each%20Gaussian%20ellipsoid%20and%20incorporating%203D%20spatial%20consistency%0Aregularization.%20Leveraging%20these%20modules%2C%20we%20propose%20a%20pruning%20strategy%20to%0Aeffectively%20reduce%20redundancy%20in%203D%20gaussians.%20Extensive%20experiments%0Ademonstrate%20that%20TSGaussian%20outperforms%20state-of-the-art%20methods%20on%20three%0Astandard%20datasets%20and%20a%20new%20challenging%20dataset%20we%20collected%2C%20achieving%0Asuperior%20results%20in%20novel%20view%20synthesis%20of%20specific%20objects.%20Code%20is%20available%0Aat%3A%20https%3A//github.com/leon2000-ai/TSGaussian.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10051v1&entry.124074799=Read"},
{"title": "Toy-GS: Assembling Local Gaussians for Precisely Rendering Large-Scale\n  Free Camera Trajectories", "author": "Xiaohan Zhang and Zhenyu Sun and Yukui Qiu and Junyan Su and Qi Liu", "abstract": "  Currently, 3D rendering for large-scale free camera trajectories, namely,\narbitrary input camera trajectories, poses significant challenges: 1) The\ndistribution and observation angles of the cameras are irregular, and various\ntypes of scenes are included in the free trajectories; 2) Processing the entire\npoint cloud and all images at once for large-scale scenes requires a\nsubstantial amount of GPU memory. This paper presents a Toy-GS method for\naccurately rendering large-scale free camera trajectories. Specifically, we\npropose an adaptive spatial division approach for free trajectories to divide\ncameras and the sparse point cloud of the entire scene into various regions\naccording to camera poses. Training each local Gaussian in parallel for each\narea enables us to concentrate on texture details and minimize GPU memory\nusage. Next, we use the multi-view constraint and position-aware point adaptive\ncontrol (PPAC) to improve the rendering quality of texture details. In\naddition, our regional fusion approach combines local and global Gaussians to\nenhance rendering quality with an increasing number of divided areas. Extensive\nexperiments have been carried out to confirm the effectiveness and efficiency\nof Toy-GS, leading to state-of-the-art results on two public large-scale\ndatasets as well as our SCUTic dataset. Our proposal demonstrates an\nenhancement of 1.19 dB in PSNR and conserves 7 G of GPU memory when compared to\nvarious benchmarks.\n", "link": "http://arxiv.org/abs/2412.10078v1", "date": "2024-12-13", "relevancy": 3.3957, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7103}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6736}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toy-GS%3A%20Assembling%20Local%20Gaussians%20for%20Precisely%20Rendering%20Large-Scale%0A%20%20Free%20Camera%20Trajectories&body=Title%3A%20Toy-GS%3A%20Assembling%20Local%20Gaussians%20for%20Precisely%20Rendering%20Large-Scale%0A%20%20Free%20Camera%20Trajectories%0AAuthor%3A%20Xiaohan%20Zhang%20and%20Zhenyu%20Sun%20and%20Yukui%20Qiu%20and%20Junyan%20Su%20and%20Qi%20Liu%0AAbstract%3A%20%20%20Currently%2C%203D%20rendering%20for%20large-scale%20free%20camera%20trajectories%2C%20namely%2C%0Aarbitrary%20input%20camera%20trajectories%2C%20poses%20significant%20challenges%3A%201%29%20The%0Adistribution%20and%20observation%20angles%20of%20the%20cameras%20are%20irregular%2C%20and%20various%0Atypes%20of%20scenes%20are%20included%20in%20the%20free%20trajectories%3B%202%29%20Processing%20the%20entire%0Apoint%20cloud%20and%20all%20images%20at%20once%20for%20large-scale%20scenes%20requires%20a%0Asubstantial%20amount%20of%20GPU%20memory.%20This%20paper%20presents%20a%20Toy-GS%20method%20for%0Aaccurately%20rendering%20large-scale%20free%20camera%20trajectories.%20Specifically%2C%20we%0Apropose%20an%20adaptive%20spatial%20division%20approach%20for%20free%20trajectories%20to%20divide%0Acameras%20and%20the%20sparse%20point%20cloud%20of%20the%20entire%20scene%20into%20various%20regions%0Aaccording%20to%20camera%20poses.%20Training%20each%20local%20Gaussian%20in%20parallel%20for%20each%0Aarea%20enables%20us%20to%20concentrate%20on%20texture%20details%20and%20minimize%20GPU%20memory%0Ausage.%20Next%2C%20we%20use%20the%20multi-view%20constraint%20and%20position-aware%20point%20adaptive%0Acontrol%20%28PPAC%29%20to%20improve%20the%20rendering%20quality%20of%20texture%20details.%20In%0Aaddition%2C%20our%20regional%20fusion%20approach%20combines%20local%20and%20global%20Gaussians%20to%0Aenhance%20rendering%20quality%20with%20an%20increasing%20number%20of%20divided%20areas.%20Extensive%0Aexperiments%20have%20been%20carried%20out%20to%20confirm%20the%20effectiveness%20and%20efficiency%0Aof%20Toy-GS%2C%20leading%20to%20state-of-the-art%20results%20on%20two%20public%20large-scale%0Adatasets%20as%20well%20as%20our%20SCUTic%20dataset.%20Our%20proposal%20demonstrates%20an%0Aenhancement%20of%201.19%20dB%20in%20PSNR%20and%20conserves%207%20G%20of%20GPU%20memory%20when%20compared%20to%0Avarious%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToy-GS%253A%2520Assembling%2520Local%2520Gaussians%2520for%2520Precisely%2520Rendering%2520Large-Scale%250A%2520%2520Free%2520Camera%2520Trajectories%26entry.906535625%3DXiaohan%2520Zhang%2520and%2520Zhenyu%2520Sun%2520and%2520Yukui%2520Qiu%2520and%2520Junyan%2520Su%2520and%2520Qi%2520Liu%26entry.1292438233%3D%2520%2520Currently%252C%25203D%2520rendering%2520for%2520large-scale%2520free%2520camera%2520trajectories%252C%2520namely%252C%250Aarbitrary%2520input%2520camera%2520trajectories%252C%2520poses%2520significant%2520challenges%253A%25201%2529%2520The%250Adistribution%2520and%2520observation%2520angles%2520of%2520the%2520cameras%2520are%2520irregular%252C%2520and%2520various%250Atypes%2520of%2520scenes%2520are%2520included%2520in%2520the%2520free%2520trajectories%253B%25202%2529%2520Processing%2520the%2520entire%250Apoint%2520cloud%2520and%2520all%2520images%2520at%2520once%2520for%2520large-scale%2520scenes%2520requires%2520a%250Asubstantial%2520amount%2520of%2520GPU%2520memory.%2520This%2520paper%2520presents%2520a%2520Toy-GS%2520method%2520for%250Aaccurately%2520rendering%2520large-scale%2520free%2520camera%2520trajectories.%2520Specifically%252C%2520we%250Apropose%2520an%2520adaptive%2520spatial%2520division%2520approach%2520for%2520free%2520trajectories%2520to%2520divide%250Acameras%2520and%2520the%2520sparse%2520point%2520cloud%2520of%2520the%2520entire%2520scene%2520into%2520various%2520regions%250Aaccording%2520to%2520camera%2520poses.%2520Training%2520each%2520local%2520Gaussian%2520in%2520parallel%2520for%2520each%250Aarea%2520enables%2520us%2520to%2520concentrate%2520on%2520texture%2520details%2520and%2520minimize%2520GPU%2520memory%250Ausage.%2520Next%252C%2520we%2520use%2520the%2520multi-view%2520constraint%2520and%2520position-aware%2520point%2520adaptive%250Acontrol%2520%2528PPAC%2529%2520to%2520improve%2520the%2520rendering%2520quality%2520of%2520texture%2520details.%2520In%250Aaddition%252C%2520our%2520regional%2520fusion%2520approach%2520combines%2520local%2520and%2520global%2520Gaussians%2520to%250Aenhance%2520rendering%2520quality%2520with%2520an%2520increasing%2520number%2520of%2520divided%2520areas.%2520Extensive%250Aexperiments%2520have%2520been%2520carried%2520out%2520to%2520confirm%2520the%2520effectiveness%2520and%2520efficiency%250Aof%2520Toy-GS%252C%2520leading%2520to%2520state-of-the-art%2520results%2520on%2520two%2520public%2520large-scale%250Adatasets%2520as%2520well%2520as%2520our%2520SCUTic%2520dataset.%2520Our%2520proposal%2520demonstrates%2520an%250Aenhancement%2520of%25201.19%2520dB%2520in%2520PSNR%2520and%2520conserves%25207%2520G%2520of%2520GPU%2520memory%2520when%2520compared%2520to%250Avarious%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toy-GS%3A%20Assembling%20Local%20Gaussians%20for%20Precisely%20Rendering%20Large-Scale%0A%20%20Free%20Camera%20Trajectories&entry.906535625=Xiaohan%20Zhang%20and%20Zhenyu%20Sun%20and%20Yukui%20Qiu%20and%20Junyan%20Su%20and%20Qi%20Liu&entry.1292438233=%20%20Currently%2C%203D%20rendering%20for%20large-scale%20free%20camera%20trajectories%2C%20namely%2C%0Aarbitrary%20input%20camera%20trajectories%2C%20poses%20significant%20challenges%3A%201%29%20The%0Adistribution%20and%20observation%20angles%20of%20the%20cameras%20are%20irregular%2C%20and%20various%0Atypes%20of%20scenes%20are%20included%20in%20the%20free%20trajectories%3B%202%29%20Processing%20the%20entire%0Apoint%20cloud%20and%20all%20images%20at%20once%20for%20large-scale%20scenes%20requires%20a%0Asubstantial%20amount%20of%20GPU%20memory.%20This%20paper%20presents%20a%20Toy-GS%20method%20for%0Aaccurately%20rendering%20large-scale%20free%20camera%20trajectories.%20Specifically%2C%20we%0Apropose%20an%20adaptive%20spatial%20division%20approach%20for%20free%20trajectories%20to%20divide%0Acameras%20and%20the%20sparse%20point%20cloud%20of%20the%20entire%20scene%20into%20various%20regions%0Aaccording%20to%20camera%20poses.%20Training%20each%20local%20Gaussian%20in%20parallel%20for%20each%0Aarea%20enables%20us%20to%20concentrate%20on%20texture%20details%20and%20minimize%20GPU%20memory%0Ausage.%20Next%2C%20we%20use%20the%20multi-view%20constraint%20and%20position-aware%20point%20adaptive%0Acontrol%20%28PPAC%29%20to%20improve%20the%20rendering%20quality%20of%20texture%20details.%20In%0Aaddition%2C%20our%20regional%20fusion%20approach%20combines%20local%20and%20global%20Gaussians%20to%0Aenhance%20rendering%20quality%20with%20an%20increasing%20number%20of%20divided%20areas.%20Extensive%0Aexperiments%20have%20been%20carried%20out%20to%20confirm%20the%20effectiveness%20and%20efficiency%0Aof%20Toy-GS%2C%20leading%20to%20state-of-the-art%20results%20on%20two%20public%20large-scale%0Adatasets%20as%20well%20as%20our%20SCUTic%20dataset.%20Our%20proposal%20demonstrates%20an%0Aenhancement%20of%201.19%20dB%20in%20PSNR%20and%20conserves%207%20G%20of%20GPU%20memory%20when%20compared%20to%0Avarious%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10078v1&entry.124074799=Read"},
{"title": "SuperGSeg: Open-Vocabulary 3D Segmentation with Structured\n  Super-Gaussians", "author": "Siyun Liang and Sen Wang and Kunyi Li and Michael Niemeyer and Stefano Gasperini and Nassir Navab and Federico Tombari", "abstract": "  3D Gaussian Splatting has recently gained traction for its efficient training\nand real-time rendering. While the vanilla Gaussian Splatting representation is\nmainly designed for view synthesis, more recent works investigated how to\nextend it with scene understanding and language features. However, existing\nmethods lack a detailed comprehension of scenes, limiting their ability to\nsegment and interpret complex structures. To this end, We introduce SuperGSeg,\na novel approach that fosters cohesive, context-aware scene representation by\ndisentangling segmentation and language field distillation. SuperGSeg first\nemploys neural Gaussians to learn instance and hierarchical segmentation\nfeatures from multi-view images with the aid of off-the-shelf 2D masks. These\nfeatures are then leveraged to create a sparse set of what we call\nSuper-Gaussians. Super-Gaussians facilitate the distillation of 2D language\nfeatures into 3D space. Through Super-Gaussians, our method enables\nhigh-dimensional language feature rendering without extreme increases in GPU\nmemory. Extensive experiments demonstrate that SuperGSeg outperforms prior\nworks on both open-vocabulary object localization and semantic segmentation\ntasks.\n", "link": "http://arxiv.org/abs/2412.10231v1", "date": "2024-12-13", "relevancy": 3.272, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6685}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6661}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperGSeg%3A%20Open-Vocabulary%203D%20Segmentation%20with%20Structured%0A%20%20Super-Gaussians&body=Title%3A%20SuperGSeg%3A%20Open-Vocabulary%203D%20Segmentation%20with%20Structured%0A%20%20Super-Gaussians%0AAuthor%3A%20Siyun%20Liang%20and%20Sen%20Wang%20and%20Kunyi%20Li%20and%20Michael%20Niemeyer%20and%20Stefano%20Gasperini%20and%20Nassir%20Navab%20and%20Federico%20Tombari%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20has%20recently%20gained%20traction%20for%20its%20efficient%20training%0Aand%20real-time%20rendering.%20While%20the%20vanilla%20Gaussian%20Splatting%20representation%20is%0Amainly%20designed%20for%20view%20synthesis%2C%20more%20recent%20works%20investigated%20how%20to%0Aextend%20it%20with%20scene%20understanding%20and%20language%20features.%20However%2C%20existing%0Amethods%20lack%20a%20detailed%20comprehension%20of%20scenes%2C%20limiting%20their%20ability%20to%0Asegment%20and%20interpret%20complex%20structures.%20To%20this%20end%2C%20We%20introduce%20SuperGSeg%2C%0Aa%20novel%20approach%20that%20fosters%20cohesive%2C%20context-aware%20scene%20representation%20by%0Adisentangling%20segmentation%20and%20language%20field%20distillation.%20SuperGSeg%20first%0Aemploys%20neural%20Gaussians%20to%20learn%20instance%20and%20hierarchical%20segmentation%0Afeatures%20from%20multi-view%20images%20with%20the%20aid%20of%20off-the-shelf%202D%20masks.%20These%0Afeatures%20are%20then%20leveraged%20to%20create%20a%20sparse%20set%20of%20what%20we%20call%0ASuper-Gaussians.%20Super-Gaussians%20facilitate%20the%20distillation%20of%202D%20language%0Afeatures%20into%203D%20space.%20Through%20Super-Gaussians%2C%20our%20method%20enables%0Ahigh-dimensional%20language%20feature%20rendering%20without%20extreme%20increases%20in%20GPU%0Amemory.%20Extensive%20experiments%20demonstrate%20that%20SuperGSeg%20outperforms%20prior%0Aworks%20on%20both%20open-vocabulary%20object%20localization%20and%20semantic%20segmentation%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperGSeg%253A%2520Open-Vocabulary%25203D%2520Segmentation%2520with%2520Structured%250A%2520%2520Super-Gaussians%26entry.906535625%3DSiyun%2520Liang%2520and%2520Sen%2520Wang%2520and%2520Kunyi%2520Li%2520and%2520Michael%2520Niemeyer%2520and%2520Stefano%2520Gasperini%2520and%2520Nassir%2520Navab%2520and%2520Federico%2520Tombari%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520has%2520recently%2520gained%2520traction%2520for%2520its%2520efficient%2520training%250Aand%2520real-time%2520rendering.%2520While%2520the%2520vanilla%2520Gaussian%2520Splatting%2520representation%2520is%250Amainly%2520designed%2520for%2520view%2520synthesis%252C%2520more%2520recent%2520works%2520investigated%2520how%2520to%250Aextend%2520it%2520with%2520scene%2520understanding%2520and%2520language%2520features.%2520However%252C%2520existing%250Amethods%2520lack%2520a%2520detailed%2520comprehension%2520of%2520scenes%252C%2520limiting%2520their%2520ability%2520to%250Asegment%2520and%2520interpret%2520complex%2520structures.%2520To%2520this%2520end%252C%2520We%2520introduce%2520SuperGSeg%252C%250Aa%2520novel%2520approach%2520that%2520fosters%2520cohesive%252C%2520context-aware%2520scene%2520representation%2520by%250Adisentangling%2520segmentation%2520and%2520language%2520field%2520distillation.%2520SuperGSeg%2520first%250Aemploys%2520neural%2520Gaussians%2520to%2520learn%2520instance%2520and%2520hierarchical%2520segmentation%250Afeatures%2520from%2520multi-view%2520images%2520with%2520the%2520aid%2520of%2520off-the-shelf%25202D%2520masks.%2520These%250Afeatures%2520are%2520then%2520leveraged%2520to%2520create%2520a%2520sparse%2520set%2520of%2520what%2520we%2520call%250ASuper-Gaussians.%2520Super-Gaussians%2520facilitate%2520the%2520distillation%2520of%25202D%2520language%250Afeatures%2520into%25203D%2520space.%2520Through%2520Super-Gaussians%252C%2520our%2520method%2520enables%250Ahigh-dimensional%2520language%2520feature%2520rendering%2520without%2520extreme%2520increases%2520in%2520GPU%250Amemory.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SuperGSeg%2520outperforms%2520prior%250Aworks%2520on%2520both%2520open-vocabulary%2520object%2520localization%2520and%2520semantic%2520segmentation%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperGSeg%3A%20Open-Vocabulary%203D%20Segmentation%20with%20Structured%0A%20%20Super-Gaussians&entry.906535625=Siyun%20Liang%20and%20Sen%20Wang%20and%20Kunyi%20Li%20and%20Michael%20Niemeyer%20and%20Stefano%20Gasperini%20and%20Nassir%20Navab%20and%20Federico%20Tombari&entry.1292438233=%20%203D%20Gaussian%20Splatting%20has%20recently%20gained%20traction%20for%20its%20efficient%20training%0Aand%20real-time%20rendering.%20While%20the%20vanilla%20Gaussian%20Splatting%20representation%20is%0Amainly%20designed%20for%20view%20synthesis%2C%20more%20recent%20works%20investigated%20how%20to%0Aextend%20it%20with%20scene%20understanding%20and%20language%20features.%20However%2C%20existing%0Amethods%20lack%20a%20detailed%20comprehension%20of%20scenes%2C%20limiting%20their%20ability%20to%0Asegment%20and%20interpret%20complex%20structures.%20To%20this%20end%2C%20We%20introduce%20SuperGSeg%2C%0Aa%20novel%20approach%20that%20fosters%20cohesive%2C%20context-aware%20scene%20representation%20by%0Adisentangling%20segmentation%20and%20language%20field%20distillation.%20SuperGSeg%20first%0Aemploys%20neural%20Gaussians%20to%20learn%20instance%20and%20hierarchical%20segmentation%0Afeatures%20from%20multi-view%20images%20with%20the%20aid%20of%20off-the-shelf%202D%20masks.%20These%0Afeatures%20are%20then%20leveraged%20to%20create%20a%20sparse%20set%20of%20what%20we%20call%0ASuper-Gaussians.%20Super-Gaussians%20facilitate%20the%20distillation%20of%202D%20language%0Afeatures%20into%203D%20space.%20Through%20Super-Gaussians%2C%20our%20method%20enables%0Ahigh-dimensional%20language%20feature%20rendering%20without%20extreme%20increases%20in%20GPU%0Amemory.%20Extensive%20experiments%20demonstrate%20that%20SuperGSeg%20outperforms%20prior%0Aworks%20on%20both%20open-vocabulary%20object%20localization%20and%20semantic%20segmentation%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10231v1&entry.124074799=Read"},
{"title": "TrafficLoc: Localizing Traffic Surveillance Cameras in 3D Scenes", "author": "Yan Xia and Yunxiang Lu and Rui Song and Oussema Dhaouadi and Jo\u00e3o F. Henriques and Daniel Cremers", "abstract": "  We tackle the problem of localizing the traffic surveillance cameras in\ncooperative perception. To overcome the lack of large-scale real-world\nintersection datasets, we introduce Carla Intersection, a new simulated dataset\nwith 75 urban and rural intersections in Carla. Moreover, we introduce a novel\nneural network, TrafficLoc, localizing traffic cameras within a 3D reference\nmap. TrafficLoc employs a coarse-to-fine matching pipeline. For image-point\ncloud feature fusion, we propose a novel Geometry-guided Attention Loss to\naddress cross-modal viewpoint inconsistencies. During coarse matching, we\npropose an Inter-Intra Contrastive Learning to achieve precise alignment while\npreserving distinctiveness among local intra-features within image patch-point\ngroup pairs. Besides, we introduce Dense Training Alignment with a soft-argmax\noperator to consider additional features when regressing the final position.\nExtensive experiments show that our TrafficLoc improves the localization\naccuracy over the state-of-the-art Image-to-point cloud registration methods by\na large margin (up to 86%) on Carla Intersection and generalizes well to\nreal-world data. TrafficLoc also achieves new SOTA performance on KITTI and\nNuScenes datasets, demonstrating strong localization ability across both\nin-vehicle and traffic cameras. Our project page is publicly available at\nhttps://tum-luk.github.io/projects/trafficloc/.\n", "link": "http://arxiv.org/abs/2412.10308v1", "date": "2024-12-13", "relevancy": 3.1475, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6553}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6499}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TrafficLoc%3A%20Localizing%20Traffic%20Surveillance%20Cameras%20in%203D%20Scenes&body=Title%3A%20TrafficLoc%3A%20Localizing%20Traffic%20Surveillance%20Cameras%20in%203D%20Scenes%0AAuthor%3A%20Yan%20Xia%20and%20Yunxiang%20Lu%20and%20Rui%20Song%20and%20Oussema%20Dhaouadi%20and%20Jo%C3%A3o%20F.%20Henriques%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20We%20tackle%20the%20problem%20of%20localizing%20the%20traffic%20surveillance%20cameras%20in%0Acooperative%20perception.%20To%20overcome%20the%20lack%20of%20large-scale%20real-world%0Aintersection%20datasets%2C%20we%20introduce%20Carla%20Intersection%2C%20a%20new%20simulated%20dataset%0Awith%2075%20urban%20and%20rural%20intersections%20in%20Carla.%20Moreover%2C%20we%20introduce%20a%20novel%0Aneural%20network%2C%20TrafficLoc%2C%20localizing%20traffic%20cameras%20within%20a%203D%20reference%0Amap.%20TrafficLoc%20employs%20a%20coarse-to-fine%20matching%20pipeline.%20For%20image-point%0Acloud%20feature%20fusion%2C%20we%20propose%20a%20novel%20Geometry-guided%20Attention%20Loss%20to%0Aaddress%20cross-modal%20viewpoint%20inconsistencies.%20During%20coarse%20matching%2C%20we%0Apropose%20an%20Inter-Intra%20Contrastive%20Learning%20to%20achieve%20precise%20alignment%20while%0Apreserving%20distinctiveness%20among%20local%20intra-features%20within%20image%20patch-point%0Agroup%20pairs.%20Besides%2C%20we%20introduce%20Dense%20Training%20Alignment%20with%20a%20soft-argmax%0Aoperator%20to%20consider%20additional%20features%20when%20regressing%20the%20final%20position.%0AExtensive%20experiments%20show%20that%20our%20TrafficLoc%20improves%20the%20localization%0Aaccuracy%20over%20the%20state-of-the-art%20Image-to-point%20cloud%20registration%20methods%20by%0Aa%20large%20margin%20%28up%20to%2086%25%29%20on%20Carla%20Intersection%20and%20generalizes%20well%20to%0Areal-world%20data.%20TrafficLoc%20also%20achieves%20new%20SOTA%20performance%20on%20KITTI%20and%0ANuScenes%20datasets%2C%20demonstrating%20strong%20localization%20ability%20across%20both%0Ain-vehicle%20and%20traffic%20cameras.%20Our%20project%20page%20is%20publicly%20available%20at%0Ahttps%3A//tum-luk.github.io/projects/trafficloc/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10308v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrafficLoc%253A%2520Localizing%2520Traffic%2520Surveillance%2520Cameras%2520in%25203D%2520Scenes%26entry.906535625%3DYan%2520Xia%2520and%2520Yunxiang%2520Lu%2520and%2520Rui%2520Song%2520and%2520Oussema%2520Dhaouadi%2520and%2520Jo%25C3%25A3o%2520F.%2520Henriques%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520problem%2520of%2520localizing%2520the%2520traffic%2520surveillance%2520cameras%2520in%250Acooperative%2520perception.%2520To%2520overcome%2520the%2520lack%2520of%2520large-scale%2520real-world%250Aintersection%2520datasets%252C%2520we%2520introduce%2520Carla%2520Intersection%252C%2520a%2520new%2520simulated%2520dataset%250Awith%252075%2520urban%2520and%2520rural%2520intersections%2520in%2520Carla.%2520Moreover%252C%2520we%2520introduce%2520a%2520novel%250Aneural%2520network%252C%2520TrafficLoc%252C%2520localizing%2520traffic%2520cameras%2520within%2520a%25203D%2520reference%250Amap.%2520TrafficLoc%2520employs%2520a%2520coarse-to-fine%2520matching%2520pipeline.%2520For%2520image-point%250Acloud%2520feature%2520fusion%252C%2520we%2520propose%2520a%2520novel%2520Geometry-guided%2520Attention%2520Loss%2520to%250Aaddress%2520cross-modal%2520viewpoint%2520inconsistencies.%2520During%2520coarse%2520matching%252C%2520we%250Apropose%2520an%2520Inter-Intra%2520Contrastive%2520Learning%2520to%2520achieve%2520precise%2520alignment%2520while%250Apreserving%2520distinctiveness%2520among%2520local%2520intra-features%2520within%2520image%2520patch-point%250Agroup%2520pairs.%2520Besides%252C%2520we%2520introduce%2520Dense%2520Training%2520Alignment%2520with%2520a%2520soft-argmax%250Aoperator%2520to%2520consider%2520additional%2520features%2520when%2520regressing%2520the%2520final%2520position.%250AExtensive%2520experiments%2520show%2520that%2520our%2520TrafficLoc%2520improves%2520the%2520localization%250Aaccuracy%2520over%2520the%2520state-of-the-art%2520Image-to-point%2520cloud%2520registration%2520methods%2520by%250Aa%2520large%2520margin%2520%2528up%2520to%252086%2525%2529%2520on%2520Carla%2520Intersection%2520and%2520generalizes%2520well%2520to%250Areal-world%2520data.%2520TrafficLoc%2520also%2520achieves%2520new%2520SOTA%2520performance%2520on%2520KITTI%2520and%250ANuScenes%2520datasets%252C%2520demonstrating%2520strong%2520localization%2520ability%2520across%2520both%250Ain-vehicle%2520and%2520traffic%2520cameras.%2520Our%2520project%2520page%2520is%2520publicly%2520available%2520at%250Ahttps%253A//tum-luk.github.io/projects/trafficloc/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10308v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrafficLoc%3A%20Localizing%20Traffic%20Surveillance%20Cameras%20in%203D%20Scenes&entry.906535625=Yan%20Xia%20and%20Yunxiang%20Lu%20and%20Rui%20Song%20and%20Oussema%20Dhaouadi%20and%20Jo%C3%A3o%20F.%20Henriques%20and%20Daniel%20Cremers&entry.1292438233=%20%20We%20tackle%20the%20problem%20of%20localizing%20the%20traffic%20surveillance%20cameras%20in%0Acooperative%20perception.%20To%20overcome%20the%20lack%20of%20large-scale%20real-world%0Aintersection%20datasets%2C%20we%20introduce%20Carla%20Intersection%2C%20a%20new%20simulated%20dataset%0Awith%2075%20urban%20and%20rural%20intersections%20in%20Carla.%20Moreover%2C%20we%20introduce%20a%20novel%0Aneural%20network%2C%20TrafficLoc%2C%20localizing%20traffic%20cameras%20within%20a%203D%20reference%0Amap.%20TrafficLoc%20employs%20a%20coarse-to-fine%20matching%20pipeline.%20For%20image-point%0Acloud%20feature%20fusion%2C%20we%20propose%20a%20novel%20Geometry-guided%20Attention%20Loss%20to%0Aaddress%20cross-modal%20viewpoint%20inconsistencies.%20During%20coarse%20matching%2C%20we%0Apropose%20an%20Inter-Intra%20Contrastive%20Learning%20to%20achieve%20precise%20alignment%20while%0Apreserving%20distinctiveness%20among%20local%20intra-features%20within%20image%20patch-point%0Agroup%20pairs.%20Besides%2C%20we%20introduce%20Dense%20Training%20Alignment%20with%20a%20soft-argmax%0Aoperator%20to%20consider%20additional%20features%20when%20regressing%20the%20final%20position.%0AExtensive%20experiments%20show%20that%20our%20TrafficLoc%20improves%20the%20localization%0Aaccuracy%20over%20the%20state-of-the-art%20Image-to-point%20cloud%20registration%20methods%20by%0Aa%20large%20margin%20%28up%20to%2086%25%29%20on%20Carla%20Intersection%20and%20generalizes%20well%20to%0Areal-world%20data.%20TrafficLoc%20also%20achieves%20new%20SOTA%20performance%20on%20KITTI%20and%0ANuScenes%20datasets%2C%20demonstrating%20strong%20localization%20ability%20across%20both%0Ain-vehicle%20and%20traffic%20cameras.%20Our%20project%20page%20is%20publicly%20available%20at%0Ahttps%3A//tum-luk.github.io/projects/trafficloc/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10308v1&entry.124074799=Read"},
{"title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding", "author": "Zhiyu Wu and Xiaokang Chen and Zizheng Pan and Xingchao Liu and Wen Liu and Damai Dai and Huazuo Gao and Yiyang Ma and Chengyue Wu and Bingxuan Wang and Zhenda Xie and Yu Wu and Kai Hu and Jiawei Wang and Yaofeng Sun and Yukun Li and Yishi Piao and Kang Guan and Aixin Liu and Xin Xie and Yuxiang You and Kai Dong and Xingkai Yu and Haowei Zhang and Liang Zhao and Yisong Wang and Chong Ruan", "abstract": "  We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.\n", "link": "http://arxiv.org/abs/2412.10302v1", "date": "2024-12-13", "relevancy": 3.1468, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6644}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6644}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepSeek-VL2%3A%20Mixture-of-Experts%20Vision-Language%20Models%20for%20Advanced%0A%20%20Multimodal%20Understanding&body=Title%3A%20DeepSeek-VL2%3A%20Mixture-of-Experts%20Vision-Language%20Models%20for%20Advanced%0A%20%20Multimodal%20Understanding%0AAuthor%3A%20Zhiyu%20Wu%20and%20Xiaokang%20Chen%20and%20Zizheng%20Pan%20and%20Xingchao%20Liu%20and%20Wen%20Liu%20and%20Damai%20Dai%20and%20Huazuo%20Gao%20and%20Yiyang%20Ma%20and%20Chengyue%20Wu%20and%20Bingxuan%20Wang%20and%20Zhenda%20Xie%20and%20Yu%20Wu%20and%20Kai%20Hu%20and%20Jiawei%20Wang%20and%20Yaofeng%20Sun%20and%20Yukun%20Li%20and%20Yishi%20Piao%20and%20Kang%20Guan%20and%20Aixin%20Liu%20and%20Xin%20Xie%20and%20Yuxiang%20You%20and%20Kai%20Dong%20and%20Xingkai%20Yu%20and%20Haowei%20Zhang%20and%20Liang%20Zhao%20and%20Yisong%20Wang%20and%20Chong%20Ruan%0AAbstract%3A%20%20%20We%20present%20DeepSeek-VL2%2C%20an%20advanced%20series%20of%20large%20Mixture-of-Experts%20%28MoE%29%0AVision-Language%20Models%20that%20significantly%20improves%20upon%20its%20predecessor%2C%0ADeepSeek-VL%2C%20through%20two%20key%20major%20upgrades.%20For%20the%20vision%20component%2C%20we%0Aincorporate%20a%20dynamic%20tiling%20vision%20encoding%20strategy%20designed%20for%20processing%0Ahigh-resolution%20images%20with%20different%20aspect%20ratios.%20For%20the%20language%0Acomponent%2C%20we%20leverage%20DeepSeekMoE%20models%20with%20the%20Multi-head%20Latent%20Attention%0Amechanism%2C%20which%20compresses%20Key-Value%20cache%20into%20latent%20vectors%2C%20to%20enable%0Aefficient%20inference%20and%20high%20throughput.%20Trained%20on%20an%20improved%20vision-language%0Adataset%2C%20DeepSeek-VL2%20demonstrates%20superior%20capabilities%20across%20various%20tasks%2C%0Aincluding%20but%20not%20limited%20to%20visual%20question%20answering%2C%20optical%20character%0Arecognition%2C%20document/table/chart%20understanding%2C%20and%20visual%20grounding.%20Our%0Amodel%20series%20is%20composed%20of%20three%20variants%3A%20DeepSeek-VL2-Tiny%2C%0ADeepSeek-VL2-Small%20and%20DeepSeek-VL2%2C%20with%201.0B%2C%202.8B%20and%204.5B%20activated%0Aparameters%20respectively.%20DeepSeek-VL2%20achieves%20competitive%20or%20state-of-the-art%0Aperformance%20with%20similar%20or%20fewer%20activated%20parameters%20compared%20to%20existing%0Aopen-source%20dense%20and%20MoE-based%20models.%20Codes%20and%20pre-trained%20models%20are%0Apublicly%20accessible%20at%20https%3A//github.com/deepseek-ai/DeepSeek-VL2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepSeek-VL2%253A%2520Mixture-of-Experts%2520Vision-Language%2520Models%2520for%2520Advanced%250A%2520%2520Multimodal%2520Understanding%26entry.906535625%3DZhiyu%2520Wu%2520and%2520Xiaokang%2520Chen%2520and%2520Zizheng%2520Pan%2520and%2520Xingchao%2520Liu%2520and%2520Wen%2520Liu%2520and%2520Damai%2520Dai%2520and%2520Huazuo%2520Gao%2520and%2520Yiyang%2520Ma%2520and%2520Chengyue%2520Wu%2520and%2520Bingxuan%2520Wang%2520and%2520Zhenda%2520Xie%2520and%2520Yu%2520Wu%2520and%2520Kai%2520Hu%2520and%2520Jiawei%2520Wang%2520and%2520Yaofeng%2520Sun%2520and%2520Yukun%2520Li%2520and%2520Yishi%2520Piao%2520and%2520Kang%2520Guan%2520and%2520Aixin%2520Liu%2520and%2520Xin%2520Xie%2520and%2520Yuxiang%2520You%2520and%2520Kai%2520Dong%2520and%2520Xingkai%2520Yu%2520and%2520Haowei%2520Zhang%2520and%2520Liang%2520Zhao%2520and%2520Yisong%2520Wang%2520and%2520Chong%2520Ruan%26entry.1292438233%3D%2520%2520We%2520present%2520DeepSeek-VL2%252C%2520an%2520advanced%2520series%2520of%2520large%2520Mixture-of-Experts%2520%2528MoE%2529%250AVision-Language%2520Models%2520that%2520significantly%2520improves%2520upon%2520its%2520predecessor%252C%250ADeepSeek-VL%252C%2520through%2520two%2520key%2520major%2520upgrades.%2520For%2520the%2520vision%2520component%252C%2520we%250Aincorporate%2520a%2520dynamic%2520tiling%2520vision%2520encoding%2520strategy%2520designed%2520for%2520processing%250Ahigh-resolution%2520images%2520with%2520different%2520aspect%2520ratios.%2520For%2520the%2520language%250Acomponent%252C%2520we%2520leverage%2520DeepSeekMoE%2520models%2520with%2520the%2520Multi-head%2520Latent%2520Attention%250Amechanism%252C%2520which%2520compresses%2520Key-Value%2520cache%2520into%2520latent%2520vectors%252C%2520to%2520enable%250Aefficient%2520inference%2520and%2520high%2520throughput.%2520Trained%2520on%2520an%2520improved%2520vision-language%250Adataset%252C%2520DeepSeek-VL2%2520demonstrates%2520superior%2520capabilities%2520across%2520various%2520tasks%252C%250Aincluding%2520but%2520not%2520limited%2520to%2520visual%2520question%2520answering%252C%2520optical%2520character%250Arecognition%252C%2520document/table/chart%2520understanding%252C%2520and%2520visual%2520grounding.%2520Our%250Amodel%2520series%2520is%2520composed%2520of%2520three%2520variants%253A%2520DeepSeek-VL2-Tiny%252C%250ADeepSeek-VL2-Small%2520and%2520DeepSeek-VL2%252C%2520with%25201.0B%252C%25202.8B%2520and%25204.5B%2520activated%250Aparameters%2520respectively.%2520DeepSeek-VL2%2520achieves%2520competitive%2520or%2520state-of-the-art%250Aperformance%2520with%2520similar%2520or%2520fewer%2520activated%2520parameters%2520compared%2520to%2520existing%250Aopen-source%2520dense%2520and%2520MoE-based%2520models.%2520Codes%2520and%2520pre-trained%2520models%2520are%250Apublicly%2520accessible%2520at%2520https%253A//github.com/deepseek-ai/DeepSeek-VL2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepSeek-VL2%3A%20Mixture-of-Experts%20Vision-Language%20Models%20for%20Advanced%0A%20%20Multimodal%20Understanding&entry.906535625=Zhiyu%20Wu%20and%20Xiaokang%20Chen%20and%20Zizheng%20Pan%20and%20Xingchao%20Liu%20and%20Wen%20Liu%20and%20Damai%20Dai%20and%20Huazuo%20Gao%20and%20Yiyang%20Ma%20and%20Chengyue%20Wu%20and%20Bingxuan%20Wang%20and%20Zhenda%20Xie%20and%20Yu%20Wu%20and%20Kai%20Hu%20and%20Jiawei%20Wang%20and%20Yaofeng%20Sun%20and%20Yukun%20Li%20and%20Yishi%20Piao%20and%20Kang%20Guan%20and%20Aixin%20Liu%20and%20Xin%20Xie%20and%20Yuxiang%20You%20and%20Kai%20Dong%20and%20Xingkai%20Yu%20and%20Haowei%20Zhang%20and%20Liang%20Zhao%20and%20Yisong%20Wang%20and%20Chong%20Ruan&entry.1292438233=%20%20We%20present%20DeepSeek-VL2%2C%20an%20advanced%20series%20of%20large%20Mixture-of-Experts%20%28MoE%29%0AVision-Language%20Models%20that%20significantly%20improves%20upon%20its%20predecessor%2C%0ADeepSeek-VL%2C%20through%20two%20key%20major%20upgrades.%20For%20the%20vision%20component%2C%20we%0Aincorporate%20a%20dynamic%20tiling%20vision%20encoding%20strategy%20designed%20for%20processing%0Ahigh-resolution%20images%20with%20different%20aspect%20ratios.%20For%20the%20language%0Acomponent%2C%20we%20leverage%20DeepSeekMoE%20models%20with%20the%20Multi-head%20Latent%20Attention%0Amechanism%2C%20which%20compresses%20Key-Value%20cache%20into%20latent%20vectors%2C%20to%20enable%0Aefficient%20inference%20and%20high%20throughput.%20Trained%20on%20an%20improved%20vision-language%0Adataset%2C%20DeepSeek-VL2%20demonstrates%20superior%20capabilities%20across%20various%20tasks%2C%0Aincluding%20but%20not%20limited%20to%20visual%20question%20answering%2C%20optical%20character%0Arecognition%2C%20document/table/chart%20understanding%2C%20and%20visual%20grounding.%20Our%0Amodel%20series%20is%20composed%20of%20three%20variants%3A%20DeepSeek-VL2-Tiny%2C%0ADeepSeek-VL2-Small%20and%20DeepSeek-VL2%2C%20with%201.0B%2C%202.8B%20and%204.5B%20activated%0Aparameters%20respectively.%20DeepSeek-VL2%20achieves%20competitive%20or%20state-of-the-art%0Aperformance%20with%20similar%20or%20fewer%20activated%20parameters%20compared%20to%20existing%0Aopen-source%20dense%20and%20MoE-based%20models.%20Codes%20and%20pre-trained%20models%20are%0Apublicly%20accessible%20at%20https%3A//github.com/deepseek-ai/DeepSeek-VL2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10302v1&entry.124074799=Read"},
{"title": "A dual contrastive framework", "author": "Yuan Sun and Zhao Zhang and Jorge Ortiz", "abstract": "  In current multimodal tasks, models typically freeze the encoder and decoder\nwhile adapting intermediate layers to task-specific goals, such as region\ncaptioning. Region-level visual understanding presents significant challenges\nfor large-scale vision-language models. While limited spatial awareness is a\nknown issue, coarse-grained pretraining, in particular, exacerbates the\ndifficulty of optimizing latent representations for effective encoder-decoder\nalignment. We propose AlignCap, a framework designed to enhance region-level\nunderstanding through fine-grained alignment of latent spaces. Our approach\nintroduces a novel latent feature refinement module that enhances conditioned\nlatent space representations to improve region-level captioning performance. We\nalso propose an innovative alignment strategy, the semantic space alignment\nmodule, which boosts the quality of multimodal representations. Additionally,\nwe incorporate contrastive learning in a novel manner within both modules to\nfurther enhance region-level captioning performance. To address spatial\nlimitations, we employ a General Object Detection (GOD) method as a data\npreprocessing pipeline that enhances spatial reasoning at the regional level.\nExtensive experiments demonstrate that our approach significantly improves\nregion-level captioning performance across various tasks\n", "link": "http://arxiv.org/abs/2412.10348v1", "date": "2024-12-13", "relevancy": 3.1075, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6242}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20dual%20contrastive%20framework&body=Title%3A%20A%20dual%20contrastive%20framework%0AAuthor%3A%20Yuan%20Sun%20and%20Zhao%20Zhang%20and%20Jorge%20Ortiz%0AAbstract%3A%20%20%20In%20current%20multimodal%20tasks%2C%20models%20typically%20freeze%20the%20encoder%20and%20decoder%0Awhile%20adapting%20intermediate%20layers%20to%20task-specific%20goals%2C%20such%20as%20region%0Acaptioning.%20Region-level%20visual%20understanding%20presents%20significant%20challenges%0Afor%20large-scale%20vision-language%20models.%20While%20limited%20spatial%20awareness%20is%20a%0Aknown%20issue%2C%20coarse-grained%20pretraining%2C%20in%20particular%2C%20exacerbates%20the%0Adifficulty%20of%20optimizing%20latent%20representations%20for%20effective%20encoder-decoder%0Aalignment.%20We%20propose%20AlignCap%2C%20a%20framework%20designed%20to%20enhance%20region-level%0Aunderstanding%20through%20fine-grained%20alignment%20of%20latent%20spaces.%20Our%20approach%0Aintroduces%20a%20novel%20latent%20feature%20refinement%20module%20that%20enhances%20conditioned%0Alatent%20space%20representations%20to%20improve%20region-level%20captioning%20performance.%20We%0Aalso%20propose%20an%20innovative%20alignment%20strategy%2C%20the%20semantic%20space%20alignment%0Amodule%2C%20which%20boosts%20the%20quality%20of%20multimodal%20representations.%20Additionally%2C%0Awe%20incorporate%20contrastive%20learning%20in%20a%20novel%20manner%20within%20both%20modules%20to%0Afurther%20enhance%20region-level%20captioning%20performance.%20To%20address%20spatial%0Alimitations%2C%20we%20employ%20a%20General%20Object%20Detection%20%28GOD%29%20method%20as%20a%20data%0Apreprocessing%20pipeline%20that%20enhances%20spatial%20reasoning%20at%20the%20regional%20level.%0AExtensive%20experiments%20demonstrate%20that%20our%20approach%20significantly%20improves%0Aregion-level%20captioning%20performance%20across%20various%20tasks%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520dual%2520contrastive%2520framework%26entry.906535625%3DYuan%2520Sun%2520and%2520Zhao%2520Zhang%2520and%2520Jorge%2520Ortiz%26entry.1292438233%3D%2520%2520In%2520current%2520multimodal%2520tasks%252C%2520models%2520typically%2520freeze%2520the%2520encoder%2520and%2520decoder%250Awhile%2520adapting%2520intermediate%2520layers%2520to%2520task-specific%2520goals%252C%2520such%2520as%2520region%250Acaptioning.%2520Region-level%2520visual%2520understanding%2520presents%2520significant%2520challenges%250Afor%2520large-scale%2520vision-language%2520models.%2520While%2520limited%2520spatial%2520awareness%2520is%2520a%250Aknown%2520issue%252C%2520coarse-grained%2520pretraining%252C%2520in%2520particular%252C%2520exacerbates%2520the%250Adifficulty%2520of%2520optimizing%2520latent%2520representations%2520for%2520effective%2520encoder-decoder%250Aalignment.%2520We%2520propose%2520AlignCap%252C%2520a%2520framework%2520designed%2520to%2520enhance%2520region-level%250Aunderstanding%2520through%2520fine-grained%2520alignment%2520of%2520latent%2520spaces.%2520Our%2520approach%250Aintroduces%2520a%2520novel%2520latent%2520feature%2520refinement%2520module%2520that%2520enhances%2520conditioned%250Alatent%2520space%2520representations%2520to%2520improve%2520region-level%2520captioning%2520performance.%2520We%250Aalso%2520propose%2520an%2520innovative%2520alignment%2520strategy%252C%2520the%2520semantic%2520space%2520alignment%250Amodule%252C%2520which%2520boosts%2520the%2520quality%2520of%2520multimodal%2520representations.%2520Additionally%252C%250Awe%2520incorporate%2520contrastive%2520learning%2520in%2520a%2520novel%2520manner%2520within%2520both%2520modules%2520to%250Afurther%2520enhance%2520region-level%2520captioning%2520performance.%2520To%2520address%2520spatial%250Alimitations%252C%2520we%2520employ%2520a%2520General%2520Object%2520Detection%2520%2528GOD%2529%2520method%2520as%2520a%2520data%250Apreprocessing%2520pipeline%2520that%2520enhances%2520spatial%2520reasoning%2520at%2520the%2520regional%2520level.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520improves%250Aregion-level%2520captioning%2520performance%2520across%2520various%2520tasks%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20dual%20contrastive%20framework&entry.906535625=Yuan%20Sun%20and%20Zhao%20Zhang%20and%20Jorge%20Ortiz&entry.1292438233=%20%20In%20current%20multimodal%20tasks%2C%20models%20typically%20freeze%20the%20encoder%20and%20decoder%0Awhile%20adapting%20intermediate%20layers%20to%20task-specific%20goals%2C%20such%20as%20region%0Acaptioning.%20Region-level%20visual%20understanding%20presents%20significant%20challenges%0Afor%20large-scale%20vision-language%20models.%20While%20limited%20spatial%20awareness%20is%20a%0Aknown%20issue%2C%20coarse-grained%20pretraining%2C%20in%20particular%2C%20exacerbates%20the%0Adifficulty%20of%20optimizing%20latent%20representations%20for%20effective%20encoder-decoder%0Aalignment.%20We%20propose%20AlignCap%2C%20a%20framework%20designed%20to%20enhance%20region-level%0Aunderstanding%20through%20fine-grained%20alignment%20of%20latent%20spaces.%20Our%20approach%0Aintroduces%20a%20novel%20latent%20feature%20refinement%20module%20that%20enhances%20conditioned%0Alatent%20space%20representations%20to%20improve%20region-level%20captioning%20performance.%20We%0Aalso%20propose%20an%20innovative%20alignment%20strategy%2C%20the%20semantic%20space%20alignment%0Amodule%2C%20which%20boosts%20the%20quality%20of%20multimodal%20representations.%20Additionally%2C%0Awe%20incorporate%20contrastive%20learning%20in%20a%20novel%20manner%20within%20both%20modules%20to%0Afurther%20enhance%20region-level%20captioning%20performance.%20To%20address%20spatial%0Alimitations%2C%20we%20employ%20a%20General%20Object%20Detection%20%28GOD%29%20method%20as%20a%20data%0Apreprocessing%20pipeline%20that%20enhances%20spatial%20reasoning%20at%20the%20regional%20level.%0AExtensive%20experiments%20demonstrate%20that%20our%20approach%20significantly%20improves%0Aregion-level%20captioning%20performance%20across%20various%20tasks%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10348v1&entry.124074799=Read"},
{"title": "GaussianWorld: Gaussian World Model for Streaming 3D Occupancy\n  Prediction", "author": "Sicheng Zuo and Wenzhao Zheng and Yuanhui Huang and Jie Zhou and Jiwen Lu", "abstract": "  3D occupancy prediction is important for autonomous driving due to its\ncomprehensive perception of the surroundings. To incorporate sequential inputs,\nmost existing methods fuse representations from previous frames to infer the\ncurrent 3D occupancy. However, they fail to consider the continuity of driving\nscenarios and ignore the strong prior provided by the evolution of 3D scenes\n(e.g., only dynamic objects move). In this paper, we propose a\nworld-model-based framework to exploit the scene evolution for perception. We\nreformulate 3D occupancy prediction as a 4D occupancy forecasting problem\nconditioned on the current sensor input. We decompose the scene evolution into\nthree factors: 1) ego motion alignment of static scenes; 2) local movements of\ndynamic objects; and 3) completion of newly-observed scenes. We then employ a\nGaussian world model (GaussianWorld) to explicitly exploit these priors and\ninfer the scene evolution in the 3D Gaussian space considering the current RGB\nobservation. We evaluate the effectiveness of our framework on the widely used\nnuScenes dataset. Our GaussianWorld improves the performance of the\nsingle-frame counterpart by over 2% in mIoU without introducing additional\ncomputations. Code: https://github.com/zuosc19/GaussianWorld.\n", "link": "http://arxiv.org/abs/2412.10373v1", "date": "2024-12-13", "relevancy": 3.1022, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.638}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6275}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianWorld%3A%20Gaussian%20World%20Model%20for%20Streaming%203D%20Occupancy%0A%20%20Prediction&body=Title%3A%20GaussianWorld%3A%20Gaussian%20World%20Model%20for%20Streaming%203D%20Occupancy%0A%20%20Prediction%0AAuthor%3A%20Sicheng%20Zuo%20and%20Wenzhao%20Zheng%20and%20Yuanhui%20Huang%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%203D%20occupancy%20prediction%20is%20important%20for%20autonomous%20driving%20due%20to%20its%0Acomprehensive%20perception%20of%20the%20surroundings.%20To%20incorporate%20sequential%20inputs%2C%0Amost%20existing%20methods%20fuse%20representations%20from%20previous%20frames%20to%20infer%20the%0Acurrent%203D%20occupancy.%20However%2C%20they%20fail%20to%20consider%20the%20continuity%20of%20driving%0Ascenarios%20and%20ignore%20the%20strong%20prior%20provided%20by%20the%20evolution%20of%203D%20scenes%0A%28e.g.%2C%20only%20dynamic%20objects%20move%29.%20In%20this%20paper%2C%20we%20propose%20a%0Aworld-model-based%20framework%20to%20exploit%20the%20scene%20evolution%20for%20perception.%20We%0Areformulate%203D%20occupancy%20prediction%20as%20a%204D%20occupancy%20forecasting%20problem%0Aconditioned%20on%20the%20current%20sensor%20input.%20We%20decompose%20the%20scene%20evolution%20into%0Athree%20factors%3A%201%29%20ego%20motion%20alignment%20of%20static%20scenes%3B%202%29%20local%20movements%20of%0Adynamic%20objects%3B%20and%203%29%20completion%20of%20newly-observed%20scenes.%20We%20then%20employ%20a%0AGaussian%20world%20model%20%28GaussianWorld%29%20to%20explicitly%20exploit%20these%20priors%20and%0Ainfer%20the%20scene%20evolution%20in%20the%203D%20Gaussian%20space%20considering%20the%20current%20RGB%0Aobservation.%20We%20evaluate%20the%20effectiveness%20of%20our%20framework%20on%20the%20widely%20used%0AnuScenes%20dataset.%20Our%20GaussianWorld%20improves%20the%20performance%20of%20the%0Asingle-frame%20counterpart%20by%20over%202%25%20in%20mIoU%20without%20introducing%20additional%0Acomputations.%20Code%3A%20https%3A//github.com/zuosc19/GaussianWorld.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianWorld%253A%2520Gaussian%2520World%2520Model%2520for%2520Streaming%25203D%2520Occupancy%250A%2520%2520Prediction%26entry.906535625%3DSicheng%2520Zuo%2520and%2520Wenzhao%2520Zheng%2520and%2520Yuanhui%2520Huang%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%25203D%2520occupancy%2520prediction%2520is%2520important%2520for%2520autonomous%2520driving%2520due%2520to%2520its%250Acomprehensive%2520perception%2520of%2520the%2520surroundings.%2520To%2520incorporate%2520sequential%2520inputs%252C%250Amost%2520existing%2520methods%2520fuse%2520representations%2520from%2520previous%2520frames%2520to%2520infer%2520the%250Acurrent%25203D%2520occupancy.%2520However%252C%2520they%2520fail%2520to%2520consider%2520the%2520continuity%2520of%2520driving%250Ascenarios%2520and%2520ignore%2520the%2520strong%2520prior%2520provided%2520by%2520the%2520evolution%2520of%25203D%2520scenes%250A%2528e.g.%252C%2520only%2520dynamic%2520objects%2520move%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Aworld-model-based%2520framework%2520to%2520exploit%2520the%2520scene%2520evolution%2520for%2520perception.%2520We%250Areformulate%25203D%2520occupancy%2520prediction%2520as%2520a%25204D%2520occupancy%2520forecasting%2520problem%250Aconditioned%2520on%2520the%2520current%2520sensor%2520input.%2520We%2520decompose%2520the%2520scene%2520evolution%2520into%250Athree%2520factors%253A%25201%2529%2520ego%2520motion%2520alignment%2520of%2520static%2520scenes%253B%25202%2529%2520local%2520movements%2520of%250Adynamic%2520objects%253B%2520and%25203%2529%2520completion%2520of%2520newly-observed%2520scenes.%2520We%2520then%2520employ%2520a%250AGaussian%2520world%2520model%2520%2528GaussianWorld%2529%2520to%2520explicitly%2520exploit%2520these%2520priors%2520and%250Ainfer%2520the%2520scene%2520evolution%2520in%2520the%25203D%2520Gaussian%2520space%2520considering%2520the%2520current%2520RGB%250Aobservation.%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520our%2520framework%2520on%2520the%2520widely%2520used%250AnuScenes%2520dataset.%2520Our%2520GaussianWorld%2520improves%2520the%2520performance%2520of%2520the%250Asingle-frame%2520counterpart%2520by%2520over%25202%2525%2520in%2520mIoU%2520without%2520introducing%2520additional%250Acomputations.%2520Code%253A%2520https%253A//github.com/zuosc19/GaussianWorld.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianWorld%3A%20Gaussian%20World%20Model%20for%20Streaming%203D%20Occupancy%0A%20%20Prediction&entry.906535625=Sicheng%20Zuo%20and%20Wenzhao%20Zheng%20and%20Yuanhui%20Huang%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%203D%20occupancy%20prediction%20is%20important%20for%20autonomous%20driving%20due%20to%20its%0Acomprehensive%20perception%20of%20the%20surroundings.%20To%20incorporate%20sequential%20inputs%2C%0Amost%20existing%20methods%20fuse%20representations%20from%20previous%20frames%20to%20infer%20the%0Acurrent%203D%20occupancy.%20However%2C%20they%20fail%20to%20consider%20the%20continuity%20of%20driving%0Ascenarios%20and%20ignore%20the%20strong%20prior%20provided%20by%20the%20evolution%20of%203D%20scenes%0A%28e.g.%2C%20only%20dynamic%20objects%20move%29.%20In%20this%20paper%2C%20we%20propose%20a%0Aworld-model-based%20framework%20to%20exploit%20the%20scene%20evolution%20for%20perception.%20We%0Areformulate%203D%20occupancy%20prediction%20as%20a%204D%20occupancy%20forecasting%20problem%0Aconditioned%20on%20the%20current%20sensor%20input.%20We%20decompose%20the%20scene%20evolution%20into%0Athree%20factors%3A%201%29%20ego%20motion%20alignment%20of%20static%20scenes%3B%202%29%20local%20movements%20of%0Adynamic%20objects%3B%20and%203%29%20completion%20of%20newly-observed%20scenes.%20We%20then%20employ%20a%0AGaussian%20world%20model%20%28GaussianWorld%29%20to%20explicitly%20exploit%20these%20priors%20and%0Ainfer%20the%20scene%20evolution%20in%20the%203D%20Gaussian%20space%20considering%20the%20current%20RGB%0Aobservation.%20We%20evaluate%20the%20effectiveness%20of%20our%20framework%20on%20the%20widely%20used%0AnuScenes%20dataset.%20Our%20GaussianWorld%20improves%20the%20performance%20of%20the%0Asingle-frame%20counterpart%20by%20over%202%25%20in%20mIoU%20without%20introducing%20additional%0Acomputations.%20Code%3A%20https%3A//github.com/zuosc19/GaussianWorld.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10373v1&entry.124074799=Read"},
{"title": "Apollo: An Exploration of Video Understanding in Large Multimodal Models", "author": "Orr Zohar and Xiaohan Wang and Yann Dubois and Nikhil Mehta and Tong Xiao and Philippe Hansen-Estruch and Licheng Yu and Xiaofang Wang and Felix Juefei-Xu and Ning Zhang and Serena Yeung-Levy and Xide Xia", "abstract": "  Despite the rapid integration of video perception capabilities into Large\nMultimodal Models (LMMs), the underlying mechanisms driving their video\nunderstanding remain poorly understood. Consequently, many design decisions in\nthis domain are made without proper justification or analysis. The high\ncomputational cost of training and evaluating such models, coupled with limited\nopen research, hinders the development of video-LMMs. To address this, we\npresent a comprehensive study that helps uncover what effectively drives video\nunderstanding in LMMs.\n  We begin by critically examining the primary contributors to the high\ncomputational requirements associated with video-LMM research and discover\nScaling Consistency, wherein design and training decisions made on smaller\nmodels and datasets (up to a critical size) effectively transfer to larger\nmodels. Leveraging these insights, we explored many video-specific aspects of\nvideo-LMMs, including video sampling, architectures, data composition, training\nschedules, and more. For example, we demonstrated that fps sampling during\ntraining is vastly preferable to uniform frame sampling and which vision\nencoders are the best for video representation.\n  Guided by these findings, we introduce Apollo, a state-of-the-art family of\nLMMs that achieve superior performance across different model sizes. Our models\ncan perceive hour-long videos efficiently, with Apollo-3B outperforming most\nexisting $7$B models with an impressive 55.1 on LongVideoBench. Apollo-7B is\nstate-of-the-art compared to 7B LMMs with a 70.9 on MLVU, and 63.3 on\nVideo-MME.\n", "link": "http://arxiv.org/abs/2412.10360v1", "date": "2024-12-13", "relevancy": 2.9967, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6086}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6086}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Apollo%3A%20An%20Exploration%20of%20Video%20Understanding%20in%20Large%20Multimodal%20Models&body=Title%3A%20Apollo%3A%20An%20Exploration%20of%20Video%20Understanding%20in%20Large%20Multimodal%20Models%0AAuthor%3A%20Orr%20Zohar%20and%20Xiaohan%20Wang%20and%20Yann%20Dubois%20and%20Nikhil%20Mehta%20and%20Tong%20Xiao%20and%20Philippe%20Hansen-Estruch%20and%20Licheng%20Yu%20and%20Xiaofang%20Wang%20and%20Felix%20Juefei-Xu%20and%20Ning%20Zhang%20and%20Serena%20Yeung-Levy%20and%20Xide%20Xia%0AAbstract%3A%20%20%20Despite%20the%20rapid%20integration%20of%20video%20perception%20capabilities%20into%20Large%0AMultimodal%20Models%20%28LMMs%29%2C%20the%20underlying%20mechanisms%20driving%20their%20video%0Aunderstanding%20remain%20poorly%20understood.%20Consequently%2C%20many%20design%20decisions%20in%0Athis%20domain%20are%20made%20without%20proper%20justification%20or%20analysis.%20The%20high%0Acomputational%20cost%20of%20training%20and%20evaluating%20such%20models%2C%20coupled%20with%20limited%0Aopen%20research%2C%20hinders%20the%20development%20of%20video-LMMs.%20To%20address%20this%2C%20we%0Apresent%20a%20comprehensive%20study%20that%20helps%20uncover%20what%20effectively%20drives%20video%0Aunderstanding%20in%20LMMs.%0A%20%20We%20begin%20by%20critically%20examining%20the%20primary%20contributors%20to%20the%20high%0Acomputational%20requirements%20associated%20with%20video-LMM%20research%20and%20discover%0AScaling%20Consistency%2C%20wherein%20design%20and%20training%20decisions%20made%20on%20smaller%0Amodels%20and%20datasets%20%28up%20to%20a%20critical%20size%29%20effectively%20transfer%20to%20larger%0Amodels.%20Leveraging%20these%20insights%2C%20we%20explored%20many%20video-specific%20aspects%20of%0Avideo-LMMs%2C%20including%20video%20sampling%2C%20architectures%2C%20data%20composition%2C%20training%0Aschedules%2C%20and%20more.%20For%20example%2C%20we%20demonstrated%20that%20fps%20sampling%20during%0Atraining%20is%20vastly%20preferable%20to%20uniform%20frame%20sampling%20and%20which%20vision%0Aencoders%20are%20the%20best%20for%20video%20representation.%0A%20%20Guided%20by%20these%20findings%2C%20we%20introduce%20Apollo%2C%20a%20state-of-the-art%20family%20of%0ALMMs%20that%20achieve%20superior%20performance%20across%20different%20model%20sizes.%20Our%20models%0Acan%20perceive%20hour-long%20videos%20efficiently%2C%20with%20Apollo-3B%20outperforming%20most%0Aexisting%20%247%24B%20models%20with%20an%20impressive%2055.1%20on%20LongVideoBench.%20Apollo-7B%20is%0Astate-of-the-art%20compared%20to%207B%20LMMs%20with%20a%2070.9%20on%20MLVU%2C%20and%2063.3%20on%0AVideo-MME.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApollo%253A%2520An%2520Exploration%2520of%2520Video%2520Understanding%2520in%2520Large%2520Multimodal%2520Models%26entry.906535625%3DOrr%2520Zohar%2520and%2520Xiaohan%2520Wang%2520and%2520Yann%2520Dubois%2520and%2520Nikhil%2520Mehta%2520and%2520Tong%2520Xiao%2520and%2520Philippe%2520Hansen-Estruch%2520and%2520Licheng%2520Yu%2520and%2520Xiaofang%2520Wang%2520and%2520Felix%2520Juefei-Xu%2520and%2520Ning%2520Zhang%2520and%2520Serena%2520Yeung-Levy%2520and%2520Xide%2520Xia%26entry.1292438233%3D%2520%2520Despite%2520the%2520rapid%2520integration%2520of%2520video%2520perception%2520capabilities%2520into%2520Large%250AMultimodal%2520Models%2520%2528LMMs%2529%252C%2520the%2520underlying%2520mechanisms%2520driving%2520their%2520video%250Aunderstanding%2520remain%2520poorly%2520understood.%2520Consequently%252C%2520many%2520design%2520decisions%2520in%250Athis%2520domain%2520are%2520made%2520without%2520proper%2520justification%2520or%2520analysis.%2520The%2520high%250Acomputational%2520cost%2520of%2520training%2520and%2520evaluating%2520such%2520models%252C%2520coupled%2520with%2520limited%250Aopen%2520research%252C%2520hinders%2520the%2520development%2520of%2520video-LMMs.%2520To%2520address%2520this%252C%2520we%250Apresent%2520a%2520comprehensive%2520study%2520that%2520helps%2520uncover%2520what%2520effectively%2520drives%2520video%250Aunderstanding%2520in%2520LMMs.%250A%2520%2520We%2520begin%2520by%2520critically%2520examining%2520the%2520primary%2520contributors%2520to%2520the%2520high%250Acomputational%2520requirements%2520associated%2520with%2520video-LMM%2520research%2520and%2520discover%250AScaling%2520Consistency%252C%2520wherein%2520design%2520and%2520training%2520decisions%2520made%2520on%2520smaller%250Amodels%2520and%2520datasets%2520%2528up%2520to%2520a%2520critical%2520size%2529%2520effectively%2520transfer%2520to%2520larger%250Amodels.%2520Leveraging%2520these%2520insights%252C%2520we%2520explored%2520many%2520video-specific%2520aspects%2520of%250Avideo-LMMs%252C%2520including%2520video%2520sampling%252C%2520architectures%252C%2520data%2520composition%252C%2520training%250Aschedules%252C%2520and%2520more.%2520For%2520example%252C%2520we%2520demonstrated%2520that%2520fps%2520sampling%2520during%250Atraining%2520is%2520vastly%2520preferable%2520to%2520uniform%2520frame%2520sampling%2520and%2520which%2520vision%250Aencoders%2520are%2520the%2520best%2520for%2520video%2520representation.%250A%2520%2520Guided%2520by%2520these%2520findings%252C%2520we%2520introduce%2520Apollo%252C%2520a%2520state-of-the-art%2520family%2520of%250ALMMs%2520that%2520achieve%2520superior%2520performance%2520across%2520different%2520model%2520sizes.%2520Our%2520models%250Acan%2520perceive%2520hour-long%2520videos%2520efficiently%252C%2520with%2520Apollo-3B%2520outperforming%2520most%250Aexisting%2520%25247%2524B%2520models%2520with%2520an%2520impressive%252055.1%2520on%2520LongVideoBench.%2520Apollo-7B%2520is%250Astate-of-the-art%2520compared%2520to%25207B%2520LMMs%2520with%2520a%252070.9%2520on%2520MLVU%252C%2520and%252063.3%2520on%250AVideo-MME.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Apollo%3A%20An%20Exploration%20of%20Video%20Understanding%20in%20Large%20Multimodal%20Models&entry.906535625=Orr%20Zohar%20and%20Xiaohan%20Wang%20and%20Yann%20Dubois%20and%20Nikhil%20Mehta%20and%20Tong%20Xiao%20and%20Philippe%20Hansen-Estruch%20and%20Licheng%20Yu%20and%20Xiaofang%20Wang%20and%20Felix%20Juefei-Xu%20and%20Ning%20Zhang%20and%20Serena%20Yeung-Levy%20and%20Xide%20Xia&entry.1292438233=%20%20Despite%20the%20rapid%20integration%20of%20video%20perception%20capabilities%20into%20Large%0AMultimodal%20Models%20%28LMMs%29%2C%20the%20underlying%20mechanisms%20driving%20their%20video%0Aunderstanding%20remain%20poorly%20understood.%20Consequently%2C%20many%20design%20decisions%20in%0Athis%20domain%20are%20made%20without%20proper%20justification%20or%20analysis.%20The%20high%0Acomputational%20cost%20of%20training%20and%20evaluating%20such%20models%2C%20coupled%20with%20limited%0Aopen%20research%2C%20hinders%20the%20development%20of%20video-LMMs.%20To%20address%20this%2C%20we%0Apresent%20a%20comprehensive%20study%20that%20helps%20uncover%20what%20effectively%20drives%20video%0Aunderstanding%20in%20LMMs.%0A%20%20We%20begin%20by%20critically%20examining%20the%20primary%20contributors%20to%20the%20high%0Acomputational%20requirements%20associated%20with%20video-LMM%20research%20and%20discover%0AScaling%20Consistency%2C%20wherein%20design%20and%20training%20decisions%20made%20on%20smaller%0Amodels%20and%20datasets%20%28up%20to%20a%20critical%20size%29%20effectively%20transfer%20to%20larger%0Amodels.%20Leveraging%20these%20insights%2C%20we%20explored%20many%20video-specific%20aspects%20of%0Avideo-LMMs%2C%20including%20video%20sampling%2C%20architectures%2C%20data%20composition%2C%20training%0Aschedules%2C%20and%20more.%20For%20example%2C%20we%20demonstrated%20that%20fps%20sampling%20during%0Atraining%20is%20vastly%20preferable%20to%20uniform%20frame%20sampling%20and%20which%20vision%0Aencoders%20are%20the%20best%20for%20video%20representation.%0A%20%20Guided%20by%20these%20findings%2C%20we%20introduce%20Apollo%2C%20a%20state-of-the-art%20family%20of%0ALMMs%20that%20achieve%20superior%20performance%20across%20different%20model%20sizes.%20Our%20models%0Acan%20perceive%20hour-long%20videos%20efficiently%2C%20with%20Apollo-3B%20outperforming%20most%0Aexisting%20%247%24B%20models%20with%20an%20impressive%2055.1%20on%20LongVideoBench.%20Apollo-7B%20is%0Astate-of-the-art%20compared%20to%207B%20LMMs%20with%20a%2070.9%20on%20MLVU%2C%20and%2063.3%20on%0AVideo-MME.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10360v1&entry.124074799=Read"},
{"title": "GaussianAD: Gaussian-Centric End-to-End Autonomous Driving", "author": "Wenzhao Zheng and Junjie Wu and Yao Zheng and Sicheng Zuo and Zixun Xie and Longchao Yang and Yong Pan and Zhihui Hao and Peng Jia and Xianpeng Lang and Shanghang Zhang", "abstract": "  Vision-based autonomous driving shows great potential due to its satisfactory\nperformance and low costs. Most existing methods adopt dense representations\n(e.g., bird's eye view) or sparse representations (e.g., instance boxes) for\ndecision-making, which suffer from the trade-off between comprehensiveness and\nefficiency. This paper explores a Gaussian-centric end-to-end autonomous\ndriving (GaussianAD) framework and exploits 3D semantic Gaussians to\nextensively yet sparsely describe the scene. We initialize the scene with\nuniform 3D Gaussians and use surrounding-view images to progressively refine\nthem to obtain the 3D Gaussian scene representation. We then use sparse\nconvolutions to efficiently perform 3D perception (e.g., 3D detection, semantic\nmap construction). We predict 3D flows for the Gaussians with dynamic semantics\nand plan the ego trajectory accordingly with an objective of future scene\nforecasting. Our GaussianAD can be trained in an end-to-end manner with\noptional perception labels when available. Extensive experiments on the widely\nused nuScenes dataset verify the effectiveness of our end-to-end GaussianAD on\nvarious tasks including motion planning, 3D occupancy prediction, and 4D\noccupancy forecasting. Code: https://github.com/wzzheng/GaussianAD.\n", "link": "http://arxiv.org/abs/2412.10371v1", "date": "2024-12-13", "relevancy": 2.9881, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6114}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6094}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianAD%3A%20Gaussian-Centric%20End-to-End%20Autonomous%20Driving&body=Title%3A%20GaussianAD%3A%20Gaussian-Centric%20End-to-End%20Autonomous%20Driving%0AAuthor%3A%20Wenzhao%20Zheng%20and%20Junjie%20Wu%20and%20Yao%20Zheng%20and%20Sicheng%20Zuo%20and%20Zixun%20Xie%20and%20Longchao%20Yang%20and%20Yong%20Pan%20and%20Zhihui%20Hao%20and%20Peng%20Jia%20and%20Xianpeng%20Lang%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Vision-based%20autonomous%20driving%20shows%20great%20potential%20due%20to%20its%20satisfactory%0Aperformance%20and%20low%20costs.%20Most%20existing%20methods%20adopt%20dense%20representations%0A%28e.g.%2C%20bird%27s%20eye%20view%29%20or%20sparse%20representations%20%28e.g.%2C%20instance%20boxes%29%20for%0Adecision-making%2C%20which%20suffer%20from%20the%20trade-off%20between%20comprehensiveness%20and%0Aefficiency.%20This%20paper%20explores%20a%20Gaussian-centric%20end-to-end%20autonomous%0Adriving%20%28GaussianAD%29%20framework%20and%20exploits%203D%20semantic%20Gaussians%20to%0Aextensively%20yet%20sparsely%20describe%20the%20scene.%20We%20initialize%20the%20scene%20with%0Auniform%203D%20Gaussians%20and%20use%20surrounding-view%20images%20to%20progressively%20refine%0Athem%20to%20obtain%20the%203D%20Gaussian%20scene%20representation.%20We%20then%20use%20sparse%0Aconvolutions%20to%20efficiently%20perform%203D%20perception%20%28e.g.%2C%203D%20detection%2C%20semantic%0Amap%20construction%29.%20We%20predict%203D%20flows%20for%20the%20Gaussians%20with%20dynamic%20semantics%0Aand%20plan%20the%20ego%20trajectory%20accordingly%20with%20an%20objective%20of%20future%20scene%0Aforecasting.%20Our%20GaussianAD%20can%20be%20trained%20in%20an%20end-to-end%20manner%20with%0Aoptional%20perception%20labels%20when%20available.%20Extensive%20experiments%20on%20the%20widely%0Aused%20nuScenes%20dataset%20verify%20the%20effectiveness%20of%20our%20end-to-end%20GaussianAD%20on%0Avarious%20tasks%20including%20motion%20planning%2C%203D%20occupancy%20prediction%2C%20and%204D%0Aoccupancy%20forecasting.%20Code%3A%20https%3A//github.com/wzzheng/GaussianAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10371v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianAD%253A%2520Gaussian-Centric%2520End-to-End%2520Autonomous%2520Driving%26entry.906535625%3DWenzhao%2520Zheng%2520and%2520Junjie%2520Wu%2520and%2520Yao%2520Zheng%2520and%2520Sicheng%2520Zuo%2520and%2520Zixun%2520Xie%2520and%2520Longchao%2520Yang%2520and%2520Yong%2520Pan%2520and%2520Zhihui%2520Hao%2520and%2520Peng%2520Jia%2520and%2520Xianpeng%2520Lang%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Vision-based%2520autonomous%2520driving%2520shows%2520great%2520potential%2520due%2520to%2520its%2520satisfactory%250Aperformance%2520and%2520low%2520costs.%2520Most%2520existing%2520methods%2520adopt%2520dense%2520representations%250A%2528e.g.%252C%2520bird%2527s%2520eye%2520view%2529%2520or%2520sparse%2520representations%2520%2528e.g.%252C%2520instance%2520boxes%2529%2520for%250Adecision-making%252C%2520which%2520suffer%2520from%2520the%2520trade-off%2520between%2520comprehensiveness%2520and%250Aefficiency.%2520This%2520paper%2520explores%2520a%2520Gaussian-centric%2520end-to-end%2520autonomous%250Adriving%2520%2528GaussianAD%2529%2520framework%2520and%2520exploits%25203D%2520semantic%2520Gaussians%2520to%250Aextensively%2520yet%2520sparsely%2520describe%2520the%2520scene.%2520We%2520initialize%2520the%2520scene%2520with%250Auniform%25203D%2520Gaussians%2520and%2520use%2520surrounding-view%2520images%2520to%2520progressively%2520refine%250Athem%2520to%2520obtain%2520the%25203D%2520Gaussian%2520scene%2520representation.%2520We%2520then%2520use%2520sparse%250Aconvolutions%2520to%2520efficiently%2520perform%25203D%2520perception%2520%2528e.g.%252C%25203D%2520detection%252C%2520semantic%250Amap%2520construction%2529.%2520We%2520predict%25203D%2520flows%2520for%2520the%2520Gaussians%2520with%2520dynamic%2520semantics%250Aand%2520plan%2520the%2520ego%2520trajectory%2520accordingly%2520with%2520an%2520objective%2520of%2520future%2520scene%250Aforecasting.%2520Our%2520GaussianAD%2520can%2520be%2520trained%2520in%2520an%2520end-to-end%2520manner%2520with%250Aoptional%2520perception%2520labels%2520when%2520available.%2520Extensive%2520experiments%2520on%2520the%2520widely%250Aused%2520nuScenes%2520dataset%2520verify%2520the%2520effectiveness%2520of%2520our%2520end-to-end%2520GaussianAD%2520on%250Avarious%2520tasks%2520including%2520motion%2520planning%252C%25203D%2520occupancy%2520prediction%252C%2520and%25204D%250Aoccupancy%2520forecasting.%2520Code%253A%2520https%253A//github.com/wzzheng/GaussianAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10371v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianAD%3A%20Gaussian-Centric%20End-to-End%20Autonomous%20Driving&entry.906535625=Wenzhao%20Zheng%20and%20Junjie%20Wu%20and%20Yao%20Zheng%20and%20Sicheng%20Zuo%20and%20Zixun%20Xie%20and%20Longchao%20Yang%20and%20Yong%20Pan%20and%20Zhihui%20Hao%20and%20Peng%20Jia%20and%20Xianpeng%20Lang%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Vision-based%20autonomous%20driving%20shows%20great%20potential%20due%20to%20its%20satisfactory%0Aperformance%20and%20low%20costs.%20Most%20existing%20methods%20adopt%20dense%20representations%0A%28e.g.%2C%20bird%27s%20eye%20view%29%20or%20sparse%20representations%20%28e.g.%2C%20instance%20boxes%29%20for%0Adecision-making%2C%20which%20suffer%20from%20the%20trade-off%20between%20comprehensiveness%20and%0Aefficiency.%20This%20paper%20explores%20a%20Gaussian-centric%20end-to-end%20autonomous%0Adriving%20%28GaussianAD%29%20framework%20and%20exploits%203D%20semantic%20Gaussians%20to%0Aextensively%20yet%20sparsely%20describe%20the%20scene.%20We%20initialize%20the%20scene%20with%0Auniform%203D%20Gaussians%20and%20use%20surrounding-view%20images%20to%20progressively%20refine%0Athem%20to%20obtain%20the%203D%20Gaussian%20scene%20representation.%20We%20then%20use%20sparse%0Aconvolutions%20to%20efficiently%20perform%203D%20perception%20%28e.g.%2C%203D%20detection%2C%20semantic%0Amap%20construction%29.%20We%20predict%203D%20flows%20for%20the%20Gaussians%20with%20dynamic%20semantics%0Aand%20plan%20the%20ego%20trajectory%20accordingly%20with%20an%20objective%20of%20future%20scene%0Aforecasting.%20Our%20GaussianAD%20can%20be%20trained%20in%20an%20end-to-end%20manner%20with%0Aoptional%20perception%20labels%20when%20available.%20Extensive%20experiments%20on%20the%20widely%0Aused%20nuScenes%20dataset%20verify%20the%20effectiveness%20of%20our%20end-to-end%20GaussianAD%20on%0Avarious%20tasks%20including%20motion%20planning%2C%203D%20occupancy%20prediction%2C%20and%204D%0Aoccupancy%20forecasting.%20Code%3A%20https%3A//github.com/wzzheng/GaussianAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10371v1&entry.124074799=Read"},
{"title": "Quaffure: Real-Time Quasi-Static Neural Hair Simulation", "author": "Tuur Stuyck and Gene Wei-Chin Lin and Egor Larionov and Hsiao-yu Chen and Aljaz Bozic and Nikolaos Sarafianos and Doug Roble", "abstract": "  Realistic hair motion is crucial for high-quality avatars, but it is often\nlimited by the computational resources available for real-time applications. To\naddress this challenge, we propose a novel neural approach to predict\nphysically plausible hair deformations that generalizes to various body poses,\nshapes, and hairstyles. Our model is trained using a self-supervised loss,\neliminating the need for expensive data generation and storage. We demonstrate\nour method's effectiveness through numerous results across a wide range of pose\nand shape variations, showcasing its robust generalization capabilities and\ntemporally smooth results. Our approach is highly suitable for real-time\napplications with an inference time of only a few milliseconds on consumer\nhardware and its ability to scale to predicting the drape of 1000 grooms in 0.3\nseconds.\n", "link": "http://arxiv.org/abs/2412.10061v1", "date": "2024-12-13", "relevancy": 2.9652, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.614}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.614}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quaffure%3A%20Real-Time%20Quasi-Static%20Neural%20Hair%20Simulation&body=Title%3A%20Quaffure%3A%20Real-Time%20Quasi-Static%20Neural%20Hair%20Simulation%0AAuthor%3A%20Tuur%20Stuyck%20and%20Gene%20Wei-Chin%20Lin%20and%20Egor%20Larionov%20and%20Hsiao-yu%20Chen%20and%20Aljaz%20Bozic%20and%20Nikolaos%20Sarafianos%20and%20Doug%20Roble%0AAbstract%3A%20%20%20Realistic%20hair%20motion%20is%20crucial%20for%20high-quality%20avatars%2C%20but%20it%20is%20often%0Alimited%20by%20the%20computational%20resources%20available%20for%20real-time%20applications.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20a%20novel%20neural%20approach%20to%20predict%0Aphysically%20plausible%20hair%20deformations%20that%20generalizes%20to%20various%20body%20poses%2C%0Ashapes%2C%20and%20hairstyles.%20Our%20model%20is%20trained%20using%20a%20self-supervised%20loss%2C%0Aeliminating%20the%20need%20for%20expensive%20data%20generation%20and%20storage.%20We%20demonstrate%0Aour%20method%27s%20effectiveness%20through%20numerous%20results%20across%20a%20wide%20range%20of%20pose%0Aand%20shape%20variations%2C%20showcasing%20its%20robust%20generalization%20capabilities%20and%0Atemporally%20smooth%20results.%20Our%20approach%20is%20highly%20suitable%20for%20real-time%0Aapplications%20with%20an%20inference%20time%20of%20only%20a%20few%20milliseconds%20on%20consumer%0Ahardware%20and%20its%20ability%20to%20scale%20to%20predicting%20the%20drape%20of%201000%20grooms%20in%200.3%0Aseconds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuaffure%253A%2520Real-Time%2520Quasi-Static%2520Neural%2520Hair%2520Simulation%26entry.906535625%3DTuur%2520Stuyck%2520and%2520Gene%2520Wei-Chin%2520Lin%2520and%2520Egor%2520Larionov%2520and%2520Hsiao-yu%2520Chen%2520and%2520Aljaz%2520Bozic%2520and%2520Nikolaos%2520Sarafianos%2520and%2520Doug%2520Roble%26entry.1292438233%3D%2520%2520Realistic%2520hair%2520motion%2520is%2520crucial%2520for%2520high-quality%2520avatars%252C%2520but%2520it%2520is%2520often%250Alimited%2520by%2520the%2520computational%2520resources%2520available%2520for%2520real-time%2520applications.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520neural%2520approach%2520to%2520predict%250Aphysically%2520plausible%2520hair%2520deformations%2520that%2520generalizes%2520to%2520various%2520body%2520poses%252C%250Ashapes%252C%2520and%2520hairstyles.%2520Our%2520model%2520is%2520trained%2520using%2520a%2520self-supervised%2520loss%252C%250Aeliminating%2520the%2520need%2520for%2520expensive%2520data%2520generation%2520and%2520storage.%2520We%2520demonstrate%250Aour%2520method%2527s%2520effectiveness%2520through%2520numerous%2520results%2520across%2520a%2520wide%2520range%2520of%2520pose%250Aand%2520shape%2520variations%252C%2520showcasing%2520its%2520robust%2520generalization%2520capabilities%2520and%250Atemporally%2520smooth%2520results.%2520Our%2520approach%2520is%2520highly%2520suitable%2520for%2520real-time%250Aapplications%2520with%2520an%2520inference%2520time%2520of%2520only%2520a%2520few%2520milliseconds%2520on%2520consumer%250Ahardware%2520and%2520its%2520ability%2520to%2520scale%2520to%2520predicting%2520the%2520drape%2520of%25201000%2520grooms%2520in%25200.3%250Aseconds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quaffure%3A%20Real-Time%20Quasi-Static%20Neural%20Hair%20Simulation&entry.906535625=Tuur%20Stuyck%20and%20Gene%20Wei-Chin%20Lin%20and%20Egor%20Larionov%20and%20Hsiao-yu%20Chen%20and%20Aljaz%20Bozic%20and%20Nikolaos%20Sarafianos%20and%20Doug%20Roble&entry.1292438233=%20%20Realistic%20hair%20motion%20is%20crucial%20for%20high-quality%20avatars%2C%20but%20it%20is%20often%0Alimited%20by%20the%20computational%20resources%20available%20for%20real-time%20applications.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20a%20novel%20neural%20approach%20to%20predict%0Aphysically%20plausible%20hair%20deformations%20that%20generalizes%20to%20various%20body%20poses%2C%0Ashapes%2C%20and%20hairstyles.%20Our%20model%20is%20trained%20using%20a%20self-supervised%20loss%2C%0Aeliminating%20the%20need%20for%20expensive%20data%20generation%20and%20storage.%20We%20demonstrate%0Aour%20method%27s%20effectiveness%20through%20numerous%20results%20across%20a%20wide%20range%20of%20pose%0Aand%20shape%20variations%2C%20showcasing%20its%20robust%20generalization%20capabilities%20and%0Atemporally%20smooth%20results.%20Our%20approach%20is%20highly%20suitable%20for%20real-time%0Aapplications%20with%20an%20inference%20time%20of%20only%20a%20few%20milliseconds%20on%20consumer%0Ahardware%20and%20its%20ability%20to%20scale%20to%20predicting%20the%20drape%20of%201000%20grooms%20in%200.3%0Aseconds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10061v1&entry.124074799=Read"},
{"title": "Enhanced Low-Dose CT Image Reconstruction by Domain and Task Shifting\n  Gaussian Denoisers", "author": "Tim Selig and Thomas M\u00e4rz and Martin Storath and Andreas Weinmann", "abstract": "  Computed tomography from a low radiation dose (LDCT) is challenging due to\nhigh noise in the projection data. Popular approaches for LDCT image\nreconstruction are two-stage methods, typically consisting of the filtered\nbackprojection (FBP) algorithm followed by a neural network for LDCT image\nenhancement. Two-stage methods are attractive for their simplicity and\npotential for computational efficiency, typically requiring only a single FBP\nand a neural network forward pass for inference. However, the best\nreconstruction quality is currently achieved by unrolled iterative methods\n(Learned Primal-Dual and ItNet), which are more complex and thus have a higher\ncomputational cost for training and inference. We propose a method combining\nthe simplicity and efficiency of two-stage methods with state-of-the-art\nreconstruction quality. Our strategy utilizes a neural network pretrained for\nGaussian noise removal from natural grayscale images, fine-tuned for LDCT image\nenhancement. We call this method FBP-DTSGD (Domain and Task Shifted Gaussian\nDenoisers) as the fine-tuning is a task shift from Gaussian denoising to\nenhancing LDCT images and a domain shift from natural grayscale to LDCT images.\nAn ablation study with three different pretrained Gaussian denoisers indicates\nthat the performance of FBP-DTSGD does not depend on a specific denoising\narchitecture, suggesting future advancements in Gaussian denoising could\nbenefit the method. The study also shows that pretraining on natural images\nenhances LDCT reconstruction quality, especially with limited training data.\nNotably, pretraining involves no additional cost, as existing pretrained models\nare used. The proposed method currently holds the top mean position in the\nLoDoPaB-CT challenge.\n", "link": "http://arxiv.org/abs/2403.03551v3", "date": "2024-12-13", "relevancy": 2.8846, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5982}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5668}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Low-Dose%20CT%20Image%20Reconstruction%20by%20Domain%20and%20Task%20Shifting%0A%20%20Gaussian%20Denoisers&body=Title%3A%20Enhanced%20Low-Dose%20CT%20Image%20Reconstruction%20by%20Domain%20and%20Task%20Shifting%0A%20%20Gaussian%20Denoisers%0AAuthor%3A%20Tim%20Selig%20and%20Thomas%20M%C3%A4rz%20and%20Martin%20Storath%20and%20Andreas%20Weinmann%0AAbstract%3A%20%20%20Computed%20tomography%20from%20a%20low%20radiation%20dose%20%28LDCT%29%20is%20challenging%20due%20to%0Ahigh%20noise%20in%20the%20projection%20data.%20Popular%20approaches%20for%20LDCT%20image%0Areconstruction%20are%20two-stage%20methods%2C%20typically%20consisting%20of%20the%20filtered%0Abackprojection%20%28FBP%29%20algorithm%20followed%20by%20a%20neural%20network%20for%20LDCT%20image%0Aenhancement.%20Two-stage%20methods%20are%20attractive%20for%20their%20simplicity%20and%0Apotential%20for%20computational%20efficiency%2C%20typically%20requiring%20only%20a%20single%20FBP%0Aand%20a%20neural%20network%20forward%20pass%20for%20inference.%20However%2C%20the%20best%0Areconstruction%20quality%20is%20currently%20achieved%20by%20unrolled%20iterative%20methods%0A%28Learned%20Primal-Dual%20and%20ItNet%29%2C%20which%20are%20more%20complex%20and%20thus%20have%20a%20higher%0Acomputational%20cost%20for%20training%20and%20inference.%20We%20propose%20a%20method%20combining%0Athe%20simplicity%20and%20efficiency%20of%20two-stage%20methods%20with%20state-of-the-art%0Areconstruction%20quality.%20Our%20strategy%20utilizes%20a%20neural%20network%20pretrained%20for%0AGaussian%20noise%20removal%20from%20natural%20grayscale%20images%2C%20fine-tuned%20for%20LDCT%20image%0Aenhancement.%20We%20call%20this%20method%20FBP-DTSGD%20%28Domain%20and%20Task%20Shifted%20Gaussian%0ADenoisers%29%20as%20the%20fine-tuning%20is%20a%20task%20shift%20from%20Gaussian%20denoising%20to%0Aenhancing%20LDCT%20images%20and%20a%20domain%20shift%20from%20natural%20grayscale%20to%20LDCT%20images.%0AAn%20ablation%20study%20with%20three%20different%20pretrained%20Gaussian%20denoisers%20indicates%0Athat%20the%20performance%20of%20FBP-DTSGD%20does%20not%20depend%20on%20a%20specific%20denoising%0Aarchitecture%2C%20suggesting%20future%20advancements%20in%20Gaussian%20denoising%20could%0Abenefit%20the%20method.%20The%20study%20also%20shows%20that%20pretraining%20on%20natural%20images%0Aenhances%20LDCT%20reconstruction%20quality%2C%20especially%20with%20limited%20training%20data.%0ANotably%2C%20pretraining%20involves%20no%20additional%20cost%2C%20as%20existing%20pretrained%20models%0Aare%20used.%20The%20proposed%20method%20currently%20holds%20the%20top%20mean%20position%20in%20the%0ALoDoPaB-CT%20challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03551v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Low-Dose%2520CT%2520Image%2520Reconstruction%2520by%2520Domain%2520and%2520Task%2520Shifting%250A%2520%2520Gaussian%2520Denoisers%26entry.906535625%3DTim%2520Selig%2520and%2520Thomas%2520M%25C3%25A4rz%2520and%2520Martin%2520Storath%2520and%2520Andreas%2520Weinmann%26entry.1292438233%3D%2520%2520Computed%2520tomography%2520from%2520a%2520low%2520radiation%2520dose%2520%2528LDCT%2529%2520is%2520challenging%2520due%2520to%250Ahigh%2520noise%2520in%2520the%2520projection%2520data.%2520Popular%2520approaches%2520for%2520LDCT%2520image%250Areconstruction%2520are%2520two-stage%2520methods%252C%2520typically%2520consisting%2520of%2520the%2520filtered%250Abackprojection%2520%2528FBP%2529%2520algorithm%2520followed%2520by%2520a%2520neural%2520network%2520for%2520LDCT%2520image%250Aenhancement.%2520Two-stage%2520methods%2520are%2520attractive%2520for%2520their%2520simplicity%2520and%250Apotential%2520for%2520computational%2520efficiency%252C%2520typically%2520requiring%2520only%2520a%2520single%2520FBP%250Aand%2520a%2520neural%2520network%2520forward%2520pass%2520for%2520inference.%2520However%252C%2520the%2520best%250Areconstruction%2520quality%2520is%2520currently%2520achieved%2520by%2520unrolled%2520iterative%2520methods%250A%2528Learned%2520Primal-Dual%2520and%2520ItNet%2529%252C%2520which%2520are%2520more%2520complex%2520and%2520thus%2520have%2520a%2520higher%250Acomputational%2520cost%2520for%2520training%2520and%2520inference.%2520We%2520propose%2520a%2520method%2520combining%250Athe%2520simplicity%2520and%2520efficiency%2520of%2520two-stage%2520methods%2520with%2520state-of-the-art%250Areconstruction%2520quality.%2520Our%2520strategy%2520utilizes%2520a%2520neural%2520network%2520pretrained%2520for%250AGaussian%2520noise%2520removal%2520from%2520natural%2520grayscale%2520images%252C%2520fine-tuned%2520for%2520LDCT%2520image%250Aenhancement.%2520We%2520call%2520this%2520method%2520FBP-DTSGD%2520%2528Domain%2520and%2520Task%2520Shifted%2520Gaussian%250ADenoisers%2529%2520as%2520the%2520fine-tuning%2520is%2520a%2520task%2520shift%2520from%2520Gaussian%2520denoising%2520to%250Aenhancing%2520LDCT%2520images%2520and%2520a%2520domain%2520shift%2520from%2520natural%2520grayscale%2520to%2520LDCT%2520images.%250AAn%2520ablation%2520study%2520with%2520three%2520different%2520pretrained%2520Gaussian%2520denoisers%2520indicates%250Athat%2520the%2520performance%2520of%2520FBP-DTSGD%2520does%2520not%2520depend%2520on%2520a%2520specific%2520denoising%250Aarchitecture%252C%2520suggesting%2520future%2520advancements%2520in%2520Gaussian%2520denoising%2520could%250Abenefit%2520the%2520method.%2520The%2520study%2520also%2520shows%2520that%2520pretraining%2520on%2520natural%2520images%250Aenhances%2520LDCT%2520reconstruction%2520quality%252C%2520especially%2520with%2520limited%2520training%2520data.%250ANotably%252C%2520pretraining%2520involves%2520no%2520additional%2520cost%252C%2520as%2520existing%2520pretrained%2520models%250Aare%2520used.%2520The%2520proposed%2520method%2520currently%2520holds%2520the%2520top%2520mean%2520position%2520in%2520the%250ALoDoPaB-CT%2520challenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03551v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Low-Dose%20CT%20Image%20Reconstruction%20by%20Domain%20and%20Task%20Shifting%0A%20%20Gaussian%20Denoisers&entry.906535625=Tim%20Selig%20and%20Thomas%20M%C3%A4rz%20and%20Martin%20Storath%20and%20Andreas%20Weinmann&entry.1292438233=%20%20Computed%20tomography%20from%20a%20low%20radiation%20dose%20%28LDCT%29%20is%20challenging%20due%20to%0Ahigh%20noise%20in%20the%20projection%20data.%20Popular%20approaches%20for%20LDCT%20image%0Areconstruction%20are%20two-stage%20methods%2C%20typically%20consisting%20of%20the%20filtered%0Abackprojection%20%28FBP%29%20algorithm%20followed%20by%20a%20neural%20network%20for%20LDCT%20image%0Aenhancement.%20Two-stage%20methods%20are%20attractive%20for%20their%20simplicity%20and%0Apotential%20for%20computational%20efficiency%2C%20typically%20requiring%20only%20a%20single%20FBP%0Aand%20a%20neural%20network%20forward%20pass%20for%20inference.%20However%2C%20the%20best%0Areconstruction%20quality%20is%20currently%20achieved%20by%20unrolled%20iterative%20methods%0A%28Learned%20Primal-Dual%20and%20ItNet%29%2C%20which%20are%20more%20complex%20and%20thus%20have%20a%20higher%0Acomputational%20cost%20for%20training%20and%20inference.%20We%20propose%20a%20method%20combining%0Athe%20simplicity%20and%20efficiency%20of%20two-stage%20methods%20with%20state-of-the-art%0Areconstruction%20quality.%20Our%20strategy%20utilizes%20a%20neural%20network%20pretrained%20for%0AGaussian%20noise%20removal%20from%20natural%20grayscale%20images%2C%20fine-tuned%20for%20LDCT%20image%0Aenhancement.%20We%20call%20this%20method%20FBP-DTSGD%20%28Domain%20and%20Task%20Shifted%20Gaussian%0ADenoisers%29%20as%20the%20fine-tuning%20is%20a%20task%20shift%20from%20Gaussian%20denoising%20to%0Aenhancing%20LDCT%20images%20and%20a%20domain%20shift%20from%20natural%20grayscale%20to%20LDCT%20images.%0AAn%20ablation%20study%20with%20three%20different%20pretrained%20Gaussian%20denoisers%20indicates%0Athat%20the%20performance%20of%20FBP-DTSGD%20does%20not%20depend%20on%20a%20specific%20denoising%0Aarchitecture%2C%20suggesting%20future%20advancements%20in%20Gaussian%20denoising%20could%0Abenefit%20the%20method.%20The%20study%20also%20shows%20that%20pretraining%20on%20natural%20images%0Aenhances%20LDCT%20reconstruction%20quality%2C%20especially%20with%20limited%20training%20data.%0ANotably%2C%20pretraining%20involves%20no%20additional%20cost%2C%20as%20existing%20pretrained%20models%0Aare%20used.%20The%20proposed%20method%20currently%20holds%20the%20top%20mean%20position%20in%20the%0ALoDoPaB-CT%20challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03551v3&entry.124074799=Read"},
{"title": "Probabilistic Inverse Cameras: Image to 3D via Multiview Geometry", "author": "Rishabh Kabra and Drew A. Hudson and Sjoerd van Steenkiste and Joao Carreira and Niloy J. Mitra", "abstract": "  We introduce a hierarchical probabilistic approach to go from a 2D image to\nmultiview 3D: a diffusion \"prior\" models the unseen 3D geometry, which then\nconditions a diffusion \"decoder\" to generate novel views of the subject. We use\na pointmap-based geometric representation in a multiview image format to\ncoordinate the generation of multiple target views simultaneously. We\nfacilitate correspondence between views by assuming fixed target camera poses\nrelative to the source camera, and constructing a predictable distribution of\ngeometric features per target. Our modular, geometry-driven approach to\nnovel-view synthesis (called \"unPIC\") beats SoTA baselines such as CAT3D and\nOne-2-3-45 on held-out objects from ObjaverseXL, as well as real-world objects\nranging from Google Scanned Objects, Amazon Berkeley Objects, to the Digital\nTwin Catalog.\n", "link": "http://arxiv.org/abs/2412.10273v1", "date": "2024-12-13", "relevancy": 2.8025, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.725}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.725}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Inverse%20Cameras%3A%20Image%20to%203D%20via%20Multiview%20Geometry&body=Title%3A%20Probabilistic%20Inverse%20Cameras%3A%20Image%20to%203D%20via%20Multiview%20Geometry%0AAuthor%3A%20Rishabh%20Kabra%20and%20Drew%20A.%20Hudson%20and%20Sjoerd%20van%20Steenkiste%20and%20Joao%20Carreira%20and%20Niloy%20J.%20Mitra%0AAbstract%3A%20%20%20We%20introduce%20a%20hierarchical%20probabilistic%20approach%20to%20go%20from%20a%202D%20image%20to%0Amultiview%203D%3A%20a%20diffusion%20%22prior%22%20models%20the%20unseen%203D%20geometry%2C%20which%20then%0Aconditions%20a%20diffusion%20%22decoder%22%20to%20generate%20novel%20views%20of%20the%20subject.%20We%20use%0Aa%20pointmap-based%20geometric%20representation%20in%20a%20multiview%20image%20format%20to%0Acoordinate%20the%20generation%20of%20multiple%20target%20views%20simultaneously.%20We%0Afacilitate%20correspondence%20between%20views%20by%20assuming%20fixed%20target%20camera%20poses%0Arelative%20to%20the%20source%20camera%2C%20and%20constructing%20a%20predictable%20distribution%20of%0Ageometric%20features%20per%20target.%20Our%20modular%2C%20geometry-driven%20approach%20to%0Anovel-view%20synthesis%20%28called%20%22unPIC%22%29%20beats%20SoTA%20baselines%20such%20as%20CAT3D%20and%0AOne-2-3-45%20on%20held-out%20objects%20from%20ObjaverseXL%2C%20as%20well%20as%20real-world%20objects%0Aranging%20from%20Google%20Scanned%20Objects%2C%20Amazon%20Berkeley%20Objects%2C%20to%20the%20Digital%0ATwin%20Catalog.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Inverse%2520Cameras%253A%2520Image%2520to%25203D%2520via%2520Multiview%2520Geometry%26entry.906535625%3DRishabh%2520Kabra%2520and%2520Drew%2520A.%2520Hudson%2520and%2520Sjoerd%2520van%2520Steenkiste%2520and%2520Joao%2520Carreira%2520and%2520Niloy%2520J.%2520Mitra%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520hierarchical%2520probabilistic%2520approach%2520to%2520go%2520from%2520a%25202D%2520image%2520to%250Amultiview%25203D%253A%2520a%2520diffusion%2520%2522prior%2522%2520models%2520the%2520unseen%25203D%2520geometry%252C%2520which%2520then%250Aconditions%2520a%2520diffusion%2520%2522decoder%2522%2520to%2520generate%2520novel%2520views%2520of%2520the%2520subject.%2520We%2520use%250Aa%2520pointmap-based%2520geometric%2520representation%2520in%2520a%2520multiview%2520image%2520format%2520to%250Acoordinate%2520the%2520generation%2520of%2520multiple%2520target%2520views%2520simultaneously.%2520We%250Afacilitate%2520correspondence%2520between%2520views%2520by%2520assuming%2520fixed%2520target%2520camera%2520poses%250Arelative%2520to%2520the%2520source%2520camera%252C%2520and%2520constructing%2520a%2520predictable%2520distribution%2520of%250Ageometric%2520features%2520per%2520target.%2520Our%2520modular%252C%2520geometry-driven%2520approach%2520to%250Anovel-view%2520synthesis%2520%2528called%2520%2522unPIC%2522%2529%2520beats%2520SoTA%2520baselines%2520such%2520as%2520CAT3D%2520and%250AOne-2-3-45%2520on%2520held-out%2520objects%2520from%2520ObjaverseXL%252C%2520as%2520well%2520as%2520real-world%2520objects%250Aranging%2520from%2520Google%2520Scanned%2520Objects%252C%2520Amazon%2520Berkeley%2520Objects%252C%2520to%2520the%2520Digital%250ATwin%2520Catalog.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Inverse%20Cameras%3A%20Image%20to%203D%20via%20Multiview%20Geometry&entry.906535625=Rishabh%20Kabra%20and%20Drew%20A.%20Hudson%20and%20Sjoerd%20van%20Steenkiste%20and%20Joao%20Carreira%20and%20Niloy%20J.%20Mitra&entry.1292438233=%20%20We%20introduce%20a%20hierarchical%20probabilistic%20approach%20to%20go%20from%20a%202D%20image%20to%0Amultiview%203D%3A%20a%20diffusion%20%22prior%22%20models%20the%20unseen%203D%20geometry%2C%20which%20then%0Aconditions%20a%20diffusion%20%22decoder%22%20to%20generate%20novel%20views%20of%20the%20subject.%20We%20use%0Aa%20pointmap-based%20geometric%20representation%20in%20a%20multiview%20image%20format%20to%0Acoordinate%20the%20generation%20of%20multiple%20target%20views%20simultaneously.%20We%0Afacilitate%20correspondence%20between%20views%20by%20assuming%20fixed%20target%20camera%20poses%0Arelative%20to%20the%20source%20camera%2C%20and%20constructing%20a%20predictable%20distribution%20of%0Ageometric%20features%20per%20target.%20Our%20modular%2C%20geometry-driven%20approach%20to%0Anovel-view%20synthesis%20%28called%20%22unPIC%22%29%20beats%20SoTA%20baselines%20such%20as%20CAT3D%20and%0AOne-2-3-45%20on%20held-out%20objects%20from%20ObjaverseXL%2C%20as%20well%20as%20real-world%20objects%0Aranging%20from%20Google%20Scanned%20Objects%2C%20Amazon%20Berkeley%20Objects%2C%20to%20the%20Digital%0ATwin%20Catalog.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10273v1&entry.124074799=Read"},
{"title": "Synthetic to Authentic: Transferring Realism to 3D Face Renderings for\n  Boosting Face Recognition", "author": "Parsa Rahimi and Behrooz Razeghi and Sebastien Marcel", "abstract": "  In this paper, we investigate the potential of image-to-image translation\n(I2I) techniques for transferring realism to 3D-rendered facial images in the\ncontext of Face Recognition (FR) systems. The primary motivation for using\n3D-rendered facial images lies in their ability to circumvent the challenges\nassociated with collecting large real face datasets for training FR systems.\nThese images are generated entirely by 3D rendering engines, facilitating the\ngeneration of synthetic identities. However, it has been observed that FR\nsystems trained on such synthetic datasets underperform when compared to those\ntrained on real datasets, on various FR benchmarks. In this work, we\ndemonstrate that by transferring the realism to 3D-rendered images (i.e.,\nmaking the 3D-rendered images look more real), we can boost the performance of\nFR systems trained on these more photorealistic images. This improvement is\nevident when these systems are evaluated against FR benchmarks utilizing\nreal-world data, thereby paving new pathways for employing synthetic data in\nreal-world applications.\n", "link": "http://arxiv.org/abs/2407.07627v2", "date": "2024-12-13", "relevancy": 2.7802, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5612}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5586}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20to%20Authentic%3A%20Transferring%20Realism%20to%203D%20Face%20Renderings%20for%0A%20%20Boosting%20Face%20Recognition&body=Title%3A%20Synthetic%20to%20Authentic%3A%20Transferring%20Realism%20to%203D%20Face%20Renderings%20for%0A%20%20Boosting%20Face%20Recognition%0AAuthor%3A%20Parsa%20Rahimi%20and%20Behrooz%20Razeghi%20and%20Sebastien%20Marcel%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20potential%20of%20image-to-image%20translation%0A%28I2I%29%20techniques%20for%20transferring%20realism%20to%203D-rendered%20facial%20images%20in%20the%0Acontext%20of%20Face%20Recognition%20%28FR%29%20systems.%20The%20primary%20motivation%20for%20using%0A3D-rendered%20facial%20images%20lies%20in%20their%20ability%20to%20circumvent%20the%20challenges%0Aassociated%20with%20collecting%20large%20real%20face%20datasets%20for%20training%20FR%20systems.%0AThese%20images%20are%20generated%20entirely%20by%203D%20rendering%20engines%2C%20facilitating%20the%0Ageneration%20of%20synthetic%20identities.%20However%2C%20it%20has%20been%20observed%20that%20FR%0Asystems%20trained%20on%20such%20synthetic%20datasets%20underperform%20when%20compared%20to%20those%0Atrained%20on%20real%20datasets%2C%20on%20various%20FR%20benchmarks.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20by%20transferring%20the%20realism%20to%203D-rendered%20images%20%28i.e.%2C%0Amaking%20the%203D-rendered%20images%20look%20more%20real%29%2C%20we%20can%20boost%20the%20performance%20of%0AFR%20systems%20trained%20on%20these%20more%20photorealistic%20images.%20This%20improvement%20is%0Aevident%20when%20these%20systems%20are%20evaluated%20against%20FR%20benchmarks%20utilizing%0Areal-world%20data%2C%20thereby%20paving%20new%20pathways%20for%20employing%20synthetic%20data%20in%0Areal-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07627v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520to%2520Authentic%253A%2520Transferring%2520Realism%2520to%25203D%2520Face%2520Renderings%2520for%250A%2520%2520Boosting%2520Face%2520Recognition%26entry.906535625%3DParsa%2520Rahimi%2520and%2520Behrooz%2520Razeghi%2520and%2520Sebastien%2520Marcel%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520potential%2520of%2520image-to-image%2520translation%250A%2528I2I%2529%2520techniques%2520for%2520transferring%2520realism%2520to%25203D-rendered%2520facial%2520images%2520in%2520the%250Acontext%2520of%2520Face%2520Recognition%2520%2528FR%2529%2520systems.%2520The%2520primary%2520motivation%2520for%2520using%250A3D-rendered%2520facial%2520images%2520lies%2520in%2520their%2520ability%2520to%2520circumvent%2520the%2520challenges%250Aassociated%2520with%2520collecting%2520large%2520real%2520face%2520datasets%2520for%2520training%2520FR%2520systems.%250AThese%2520images%2520are%2520generated%2520entirely%2520by%25203D%2520rendering%2520engines%252C%2520facilitating%2520the%250Ageneration%2520of%2520synthetic%2520identities.%2520However%252C%2520it%2520has%2520been%2520observed%2520that%2520FR%250Asystems%2520trained%2520on%2520such%2520synthetic%2520datasets%2520underperform%2520when%2520compared%2520to%2520those%250Atrained%2520on%2520real%2520datasets%252C%2520on%2520various%2520FR%2520benchmarks.%2520In%2520this%2520work%252C%2520we%250Ademonstrate%2520that%2520by%2520transferring%2520the%2520realism%2520to%25203D-rendered%2520images%2520%2528i.e.%252C%250Amaking%2520the%25203D-rendered%2520images%2520look%2520more%2520real%2529%252C%2520we%2520can%2520boost%2520the%2520performance%2520of%250AFR%2520systems%2520trained%2520on%2520these%2520more%2520photorealistic%2520images.%2520This%2520improvement%2520is%250Aevident%2520when%2520these%2520systems%2520are%2520evaluated%2520against%2520FR%2520benchmarks%2520utilizing%250Areal-world%2520data%252C%2520thereby%2520paving%2520new%2520pathways%2520for%2520employing%2520synthetic%2520data%2520in%250Areal-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07627v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20to%20Authentic%3A%20Transferring%20Realism%20to%203D%20Face%20Renderings%20for%0A%20%20Boosting%20Face%20Recognition&entry.906535625=Parsa%20Rahimi%20and%20Behrooz%20Razeghi%20and%20Sebastien%20Marcel&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20potential%20of%20image-to-image%20translation%0A%28I2I%29%20techniques%20for%20transferring%20realism%20to%203D-rendered%20facial%20images%20in%20the%0Acontext%20of%20Face%20Recognition%20%28FR%29%20systems.%20The%20primary%20motivation%20for%20using%0A3D-rendered%20facial%20images%20lies%20in%20their%20ability%20to%20circumvent%20the%20challenges%0Aassociated%20with%20collecting%20large%20real%20face%20datasets%20for%20training%20FR%20systems.%0AThese%20images%20are%20generated%20entirely%20by%203D%20rendering%20engines%2C%20facilitating%20the%0Ageneration%20of%20synthetic%20identities.%20However%2C%20it%20has%20been%20observed%20that%20FR%0Asystems%20trained%20on%20such%20synthetic%20datasets%20underperform%20when%20compared%20to%20those%0Atrained%20on%20real%20datasets%2C%20on%20various%20FR%20benchmarks.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20by%20transferring%20the%20realism%20to%203D-rendered%20images%20%28i.e.%2C%0Amaking%20the%203D-rendered%20images%20look%20more%20real%29%2C%20we%20can%20boost%20the%20performance%20of%0AFR%20systems%20trained%20on%20these%20more%20photorealistic%20images.%20This%20improvement%20is%0Aevident%20when%20these%20systems%20are%20evaluated%20against%20FR%20benchmarks%20utilizing%0Areal-world%20data%2C%20thereby%20paving%20new%20pathways%20for%20employing%20synthetic%20data%20in%0Areal-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07627v2&entry.124074799=Read"},
{"title": "Enhancing Fine-Grained Vision-Language Pretraining with Negative\n  Augmented Samples", "author": "Yeyuan Wang and Dehong Gao and Lei Yi and Linbo Jin and Jinxia Zhang and Libin Yang and Xiaoyan Cai", "abstract": "  Existing Vision-Language Pretraining (VLP) methods have achieved remarkable\nimprovements across a variety of vision-language tasks, confirming their\neffectiveness in capturing coarse-grained semantic correlations. However, their\ncapability for fine-grained understanding, which is critical for many nuanced\nvision-language applications, remains limited. Prevailing VLP models often\noverlook the intricate distinctions in expressing different modal features and\ntypically depend on the similarity of holistic features for cross-modal\ninteractions. Moreover, these models directly align and integrate features from\ndifferent modalities, focusing more on coarse-grained general representations,\nthus failing to capture the nuanced differences necessary for tasks demanding a\nmore detailed perception. In response to these limitations, we introduce\nNegative Augmented Samples(NAS), a refined vision-language pretraining model\nthat innovatively incorporates NAS to specifically address the challenge of\nfine-grained understanding. NAS utilizes a Visual Dictionary(VD) as a semantic\nbridge between visual and linguistic domains. Additionally, it employs a\nNegative Visual Augmentation(NVA) method based on the VD to generate\nchallenging negative image samples. These samples deviate from positive samples\nexclusively at the token level, thereby necessitating that the model discerns\nthe subtle disparities between positive and negative samples with greater\nprecision. Comprehensive experiments validate the efficacy of NAS components\nand underscore its potential to enhance fine-grained vision-language\ncomprehension.\n", "link": "http://arxiv.org/abs/2412.10029v1", "date": "2024-12-13", "relevancy": 2.78, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5645}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5645}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Fine-Grained%20Vision-Language%20Pretraining%20with%20Negative%0A%20%20Augmented%20Samples&body=Title%3A%20Enhancing%20Fine-Grained%20Vision-Language%20Pretraining%20with%20Negative%0A%20%20Augmented%20Samples%0AAuthor%3A%20Yeyuan%20Wang%20and%20Dehong%20Gao%20and%20Lei%20Yi%20and%20Linbo%20Jin%20and%20Jinxia%20Zhang%20and%20Libin%20Yang%20and%20Xiaoyan%20Cai%0AAbstract%3A%20%20%20Existing%20Vision-Language%20Pretraining%20%28VLP%29%20methods%20have%20achieved%20remarkable%0Aimprovements%20across%20a%20variety%20of%20vision-language%20tasks%2C%20confirming%20their%0Aeffectiveness%20in%20capturing%20coarse-grained%20semantic%20correlations.%20However%2C%20their%0Acapability%20for%20fine-grained%20understanding%2C%20which%20is%20critical%20for%20many%20nuanced%0Avision-language%20applications%2C%20remains%20limited.%20Prevailing%20VLP%20models%20often%0Aoverlook%20the%20intricate%20distinctions%20in%20expressing%20different%20modal%20features%20and%0Atypically%20depend%20on%20the%20similarity%20of%20holistic%20features%20for%20cross-modal%0Ainteractions.%20Moreover%2C%20these%20models%20directly%20align%20and%20integrate%20features%20from%0Adifferent%20modalities%2C%20focusing%20more%20on%20coarse-grained%20general%20representations%2C%0Athus%20failing%20to%20capture%20the%20nuanced%20differences%20necessary%20for%20tasks%20demanding%20a%0Amore%20detailed%20perception.%20In%20response%20to%20these%20limitations%2C%20we%20introduce%0ANegative%20Augmented%20Samples%28NAS%29%2C%20a%20refined%20vision-language%20pretraining%20model%0Athat%20innovatively%20incorporates%20NAS%20to%20specifically%20address%20the%20challenge%20of%0Afine-grained%20understanding.%20NAS%20utilizes%20a%20Visual%20Dictionary%28VD%29%20as%20a%20semantic%0Abridge%20between%20visual%20and%20linguistic%20domains.%20Additionally%2C%20it%20employs%20a%0ANegative%20Visual%20Augmentation%28NVA%29%20method%20based%20on%20the%20VD%20to%20generate%0Achallenging%20negative%20image%20samples.%20These%20samples%20deviate%20from%20positive%20samples%0Aexclusively%20at%20the%20token%20level%2C%20thereby%20necessitating%20that%20the%20model%20discerns%0Athe%20subtle%20disparities%20between%20positive%20and%20negative%20samples%20with%20greater%0Aprecision.%20Comprehensive%20experiments%20validate%20the%20efficacy%20of%20NAS%20components%0Aand%20underscore%20its%20potential%20to%20enhance%20fine-grained%20vision-language%0Acomprehension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Fine-Grained%2520Vision-Language%2520Pretraining%2520with%2520Negative%250A%2520%2520Augmented%2520Samples%26entry.906535625%3DYeyuan%2520Wang%2520and%2520Dehong%2520Gao%2520and%2520Lei%2520Yi%2520and%2520Linbo%2520Jin%2520and%2520Jinxia%2520Zhang%2520and%2520Libin%2520Yang%2520and%2520Xiaoyan%2520Cai%26entry.1292438233%3D%2520%2520Existing%2520Vision-Language%2520Pretraining%2520%2528VLP%2529%2520methods%2520have%2520achieved%2520remarkable%250Aimprovements%2520across%2520a%2520variety%2520of%2520vision-language%2520tasks%252C%2520confirming%2520their%250Aeffectiveness%2520in%2520capturing%2520coarse-grained%2520semantic%2520correlations.%2520However%252C%2520their%250Acapability%2520for%2520fine-grained%2520understanding%252C%2520which%2520is%2520critical%2520for%2520many%2520nuanced%250Avision-language%2520applications%252C%2520remains%2520limited.%2520Prevailing%2520VLP%2520models%2520often%250Aoverlook%2520the%2520intricate%2520distinctions%2520in%2520expressing%2520different%2520modal%2520features%2520and%250Atypically%2520depend%2520on%2520the%2520similarity%2520of%2520holistic%2520features%2520for%2520cross-modal%250Ainteractions.%2520Moreover%252C%2520these%2520models%2520directly%2520align%2520and%2520integrate%2520features%2520from%250Adifferent%2520modalities%252C%2520focusing%2520more%2520on%2520coarse-grained%2520general%2520representations%252C%250Athus%2520failing%2520to%2520capture%2520the%2520nuanced%2520differences%2520necessary%2520for%2520tasks%2520demanding%2520a%250Amore%2520detailed%2520perception.%2520In%2520response%2520to%2520these%2520limitations%252C%2520we%2520introduce%250ANegative%2520Augmented%2520Samples%2528NAS%2529%252C%2520a%2520refined%2520vision-language%2520pretraining%2520model%250Athat%2520innovatively%2520incorporates%2520NAS%2520to%2520specifically%2520address%2520the%2520challenge%2520of%250Afine-grained%2520understanding.%2520NAS%2520utilizes%2520a%2520Visual%2520Dictionary%2528VD%2529%2520as%2520a%2520semantic%250Abridge%2520between%2520visual%2520and%2520linguistic%2520domains.%2520Additionally%252C%2520it%2520employs%2520a%250ANegative%2520Visual%2520Augmentation%2528NVA%2529%2520method%2520based%2520on%2520the%2520VD%2520to%2520generate%250Achallenging%2520negative%2520image%2520samples.%2520These%2520samples%2520deviate%2520from%2520positive%2520samples%250Aexclusively%2520at%2520the%2520token%2520level%252C%2520thereby%2520necessitating%2520that%2520the%2520model%2520discerns%250Athe%2520subtle%2520disparities%2520between%2520positive%2520and%2520negative%2520samples%2520with%2520greater%250Aprecision.%2520Comprehensive%2520experiments%2520validate%2520the%2520efficacy%2520of%2520NAS%2520components%250Aand%2520underscore%2520its%2520potential%2520to%2520enhance%2520fine-grained%2520vision-language%250Acomprehension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Fine-Grained%20Vision-Language%20Pretraining%20with%20Negative%0A%20%20Augmented%20Samples&entry.906535625=Yeyuan%20Wang%20and%20Dehong%20Gao%20and%20Lei%20Yi%20and%20Linbo%20Jin%20and%20Jinxia%20Zhang%20and%20Libin%20Yang%20and%20Xiaoyan%20Cai&entry.1292438233=%20%20Existing%20Vision-Language%20Pretraining%20%28VLP%29%20methods%20have%20achieved%20remarkable%0Aimprovements%20across%20a%20variety%20of%20vision-language%20tasks%2C%20confirming%20their%0Aeffectiveness%20in%20capturing%20coarse-grained%20semantic%20correlations.%20However%2C%20their%0Acapability%20for%20fine-grained%20understanding%2C%20which%20is%20critical%20for%20many%20nuanced%0Avision-language%20applications%2C%20remains%20limited.%20Prevailing%20VLP%20models%20often%0Aoverlook%20the%20intricate%20distinctions%20in%20expressing%20different%20modal%20features%20and%0Atypically%20depend%20on%20the%20similarity%20of%20holistic%20features%20for%20cross-modal%0Ainteractions.%20Moreover%2C%20these%20models%20directly%20align%20and%20integrate%20features%20from%0Adifferent%20modalities%2C%20focusing%20more%20on%20coarse-grained%20general%20representations%2C%0Athus%20failing%20to%20capture%20the%20nuanced%20differences%20necessary%20for%20tasks%20demanding%20a%0Amore%20detailed%20perception.%20In%20response%20to%20these%20limitations%2C%20we%20introduce%0ANegative%20Augmented%20Samples%28NAS%29%2C%20a%20refined%20vision-language%20pretraining%20model%0Athat%20innovatively%20incorporates%20NAS%20to%20specifically%20address%20the%20challenge%20of%0Afine-grained%20understanding.%20NAS%20utilizes%20a%20Visual%20Dictionary%28VD%29%20as%20a%20semantic%0Abridge%20between%20visual%20and%20linguistic%20domains.%20Additionally%2C%20it%20employs%20a%0ANegative%20Visual%20Augmentation%28NVA%29%20method%20based%20on%20the%20VD%20to%20generate%0Achallenging%20negative%20image%20samples.%20These%20samples%20deviate%20from%20positive%20samples%0Aexclusively%20at%20the%20token%20level%2C%20thereby%20necessitating%20that%20the%20model%20discerns%0Athe%20subtle%20disparities%20between%20positive%20and%20negative%20samples%20with%20greater%0Aprecision.%20Comprehensive%20experiments%20validate%20the%20efficacy%20of%20NAS%20components%0Aand%20underscore%20its%20potential%20to%20enhance%20fine-grained%20vision-language%0Acomprehension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10029v1&entry.124074799=Read"},
{"title": "MVC-VPR: Mutual Learning of Viewpoint Classification and Visual Place\n  Recognition", "author": "Qiwen Gu and Xufei Wang and Fenglin Zhang and Junqiao Zhao and Siyue Tao and Chen Ye and Tiantian Feng and Changjun Jiang", "abstract": "  Visual Place Recognition (VPR) aims to robustly identify locations by\nleveraging image retrieval based on descriptors encoded from environmental\nimages. However, drastic appearance changes of images captured from different\nviewpoints at the same location pose incoherent supervision signals for\ndescriptor learning, which severely hinder the performance of VPR. Previous\nwork proposes classifying images based on manually defined rules or ground\ntruth labels for viewpoints, followed by descriptor training based on the\nclassification results. However, not all datasets have ground truth labels of\nviewpoints and manually defined rules may be suboptimal, leading to degraded\ndescriptor performance.To address these challenges, we introduce the mutual\nlearning of viewpoint self-classification and VPR. Starting from coarse\nclassification based on geographical coordinates, we progress to finer\nclassification of viewpoints using simple clustering techniques. The dataset is\npartitioned in an unsupervised manner while simultaneously training a\ndescriptor extractor for place recognition. Experimental results show that this\napproach almost perfectly partitions the dataset based on viewpoints, thus\nachieving mutually reinforcing effects. Our method even excels state-of-the-art\n(SOTA) methods that partition datasets using ground truth labels.\n", "link": "http://arxiv.org/abs/2412.09199v2", "date": "2024-12-13", "relevancy": 2.763, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5574}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5566}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVC-VPR%3A%20Mutual%20Learning%20of%20Viewpoint%20Classification%20and%20Visual%20Place%0A%20%20Recognition&body=Title%3A%20MVC-VPR%3A%20Mutual%20Learning%20of%20Viewpoint%20Classification%20and%20Visual%20Place%0A%20%20Recognition%0AAuthor%3A%20Qiwen%20Gu%20and%20Xufei%20Wang%20and%20Fenglin%20Zhang%20and%20Junqiao%20Zhao%20and%20Siyue%20Tao%20and%20Chen%20Ye%20and%20Tiantian%20Feng%20and%20Changjun%20Jiang%0AAbstract%3A%20%20%20Visual%20Place%20Recognition%20%28VPR%29%20aims%20to%20robustly%20identify%20locations%20by%0Aleveraging%20image%20retrieval%20based%20on%20descriptors%20encoded%20from%20environmental%0Aimages.%20However%2C%20drastic%20appearance%20changes%20of%20images%20captured%20from%20different%0Aviewpoints%20at%20the%20same%20location%20pose%20incoherent%20supervision%20signals%20for%0Adescriptor%20learning%2C%20which%20severely%20hinder%20the%20performance%20of%20VPR.%20Previous%0Awork%20proposes%20classifying%20images%20based%20on%20manually%20defined%20rules%20or%20ground%0Atruth%20labels%20for%20viewpoints%2C%20followed%20by%20descriptor%20training%20based%20on%20the%0Aclassification%20results.%20However%2C%20not%20all%20datasets%20have%20ground%20truth%20labels%20of%0Aviewpoints%20and%20manually%20defined%20rules%20may%20be%20suboptimal%2C%20leading%20to%20degraded%0Adescriptor%20performance.To%20address%20these%20challenges%2C%20we%20introduce%20the%20mutual%0Alearning%20of%20viewpoint%20self-classification%20and%20VPR.%20Starting%20from%20coarse%0Aclassification%20based%20on%20geographical%20coordinates%2C%20we%20progress%20to%20finer%0Aclassification%20of%20viewpoints%20using%20simple%20clustering%20techniques.%20The%20dataset%20is%0Apartitioned%20in%20an%20unsupervised%20manner%20while%20simultaneously%20training%20a%0Adescriptor%20extractor%20for%20place%20recognition.%20Experimental%20results%20show%20that%20this%0Aapproach%20almost%20perfectly%20partitions%20the%20dataset%20based%20on%20viewpoints%2C%20thus%0Aachieving%20mutually%20reinforcing%20effects.%20Our%20method%20even%20excels%20state-of-the-art%0A%28SOTA%29%20methods%20that%20partition%20datasets%20using%20ground%20truth%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09199v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVC-VPR%253A%2520Mutual%2520Learning%2520of%2520Viewpoint%2520Classification%2520and%2520Visual%2520Place%250A%2520%2520Recognition%26entry.906535625%3DQiwen%2520Gu%2520and%2520Xufei%2520Wang%2520and%2520Fenglin%2520Zhang%2520and%2520Junqiao%2520Zhao%2520and%2520Siyue%2520Tao%2520and%2520Chen%2520Ye%2520and%2520Tiantian%2520Feng%2520and%2520Changjun%2520Jiang%26entry.1292438233%3D%2520%2520Visual%2520Place%2520Recognition%2520%2528VPR%2529%2520aims%2520to%2520robustly%2520identify%2520locations%2520by%250Aleveraging%2520image%2520retrieval%2520based%2520on%2520descriptors%2520encoded%2520from%2520environmental%250Aimages.%2520However%252C%2520drastic%2520appearance%2520changes%2520of%2520images%2520captured%2520from%2520different%250Aviewpoints%2520at%2520the%2520same%2520location%2520pose%2520incoherent%2520supervision%2520signals%2520for%250Adescriptor%2520learning%252C%2520which%2520severely%2520hinder%2520the%2520performance%2520of%2520VPR.%2520Previous%250Awork%2520proposes%2520classifying%2520images%2520based%2520on%2520manually%2520defined%2520rules%2520or%2520ground%250Atruth%2520labels%2520for%2520viewpoints%252C%2520followed%2520by%2520descriptor%2520training%2520based%2520on%2520the%250Aclassification%2520results.%2520However%252C%2520not%2520all%2520datasets%2520have%2520ground%2520truth%2520labels%2520of%250Aviewpoints%2520and%2520manually%2520defined%2520rules%2520may%2520be%2520suboptimal%252C%2520leading%2520to%2520degraded%250Adescriptor%2520performance.To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520the%2520mutual%250Alearning%2520of%2520viewpoint%2520self-classification%2520and%2520VPR.%2520Starting%2520from%2520coarse%250Aclassification%2520based%2520on%2520geographical%2520coordinates%252C%2520we%2520progress%2520to%2520finer%250Aclassification%2520of%2520viewpoints%2520using%2520simple%2520clustering%2520techniques.%2520The%2520dataset%2520is%250Apartitioned%2520in%2520an%2520unsupervised%2520manner%2520while%2520simultaneously%2520training%2520a%250Adescriptor%2520extractor%2520for%2520place%2520recognition.%2520Experimental%2520results%2520show%2520that%2520this%250Aapproach%2520almost%2520perfectly%2520partitions%2520the%2520dataset%2520based%2520on%2520viewpoints%252C%2520thus%250Aachieving%2520mutually%2520reinforcing%2520effects.%2520Our%2520method%2520even%2520excels%2520state-of-the-art%250A%2528SOTA%2529%2520methods%2520that%2520partition%2520datasets%2520using%2520ground%2520truth%2520labels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09199v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVC-VPR%3A%20Mutual%20Learning%20of%20Viewpoint%20Classification%20and%20Visual%20Place%0A%20%20Recognition&entry.906535625=Qiwen%20Gu%20and%20Xufei%20Wang%20and%20Fenglin%20Zhang%20and%20Junqiao%20Zhao%20and%20Siyue%20Tao%20and%20Chen%20Ye%20and%20Tiantian%20Feng%20and%20Changjun%20Jiang&entry.1292438233=%20%20Visual%20Place%20Recognition%20%28VPR%29%20aims%20to%20robustly%20identify%20locations%20by%0Aleveraging%20image%20retrieval%20based%20on%20descriptors%20encoded%20from%20environmental%0Aimages.%20However%2C%20drastic%20appearance%20changes%20of%20images%20captured%20from%20different%0Aviewpoints%20at%20the%20same%20location%20pose%20incoherent%20supervision%20signals%20for%0Adescriptor%20learning%2C%20which%20severely%20hinder%20the%20performance%20of%20VPR.%20Previous%0Awork%20proposes%20classifying%20images%20based%20on%20manually%20defined%20rules%20or%20ground%0Atruth%20labels%20for%20viewpoints%2C%20followed%20by%20descriptor%20training%20based%20on%20the%0Aclassification%20results.%20However%2C%20not%20all%20datasets%20have%20ground%20truth%20labels%20of%0Aviewpoints%20and%20manually%20defined%20rules%20may%20be%20suboptimal%2C%20leading%20to%20degraded%0Adescriptor%20performance.To%20address%20these%20challenges%2C%20we%20introduce%20the%20mutual%0Alearning%20of%20viewpoint%20self-classification%20and%20VPR.%20Starting%20from%20coarse%0Aclassification%20based%20on%20geographical%20coordinates%2C%20we%20progress%20to%20finer%0Aclassification%20of%20viewpoints%20using%20simple%20clustering%20techniques.%20The%20dataset%20is%0Apartitioned%20in%20an%20unsupervised%20manner%20while%20simultaneously%20training%20a%0Adescriptor%20extractor%20for%20place%20recognition.%20Experimental%20results%20show%20that%20this%0Aapproach%20almost%20perfectly%20partitions%20the%20dataset%20based%20on%20viewpoints%2C%20thus%0Aachieving%20mutually%20reinforcing%20effects.%20Our%20method%20even%20excels%20state-of-the-art%0A%28SOTA%29%20methods%20that%20partition%20datasets%20using%20ground%20truth%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09199v2&entry.124074799=Read"},
{"title": "TIV-Diffusion: Towards Object-Centric Movement for Text-driven Image to\n  Video Generation", "author": "Xingrui Wang and Xin Li and Yaosi Hu and Hanxin Zhu and Chen Hou and Cuiling Lan and Zhibo Chen", "abstract": "  Text-driven Image to Video Generation (TI2V) aims to generate controllable\nvideo given the first frame and corresponding textual description. The primary\nchallenges of this task lie in two parts: (i) how to identify the target\nobjects and ensure the consistency between the movement trajectory and the\ntextual description. (ii) how to improve the subjective quality of generated\nvideos. To tackle the above challenges, we propose a new diffusion-based TI2V\nframework, termed TIV-Diffusion, via object-centric textual-visual alignment,\nintending to achieve precise control and high-quality video generation based on\ntextual-described motion for different objects. Concretely, we enable our\nTIV-Diffuion model to perceive the textual-described objects and their motion\ntrajectory by incorporating the fused textual and visual knowledge through\nscale-offset modulation. Moreover, to mitigate the problems of object\ndisappearance and misaligned objects and motion, we introduce an object-centric\ntextual-visual alignment module, which reduces the risk of misaligned\nobjects/motion by decoupling the objects in the reference image and aligning\ntextual features with each object individually. Based on the above innovations,\nour TIV-Diffusion achieves state-of-the-art high-quality video generation\ncompared with existing TI2V methods.\n", "link": "http://arxiv.org/abs/2412.10275v1", "date": "2024-12-13", "relevancy": 2.7459, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7252}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7016}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TIV-Diffusion%3A%20Towards%20Object-Centric%20Movement%20for%20Text-driven%20Image%20to%0A%20%20Video%20Generation&body=Title%3A%20TIV-Diffusion%3A%20Towards%20Object-Centric%20Movement%20for%20Text-driven%20Image%20to%0A%20%20Video%20Generation%0AAuthor%3A%20Xingrui%20Wang%20and%20Xin%20Li%20and%20Yaosi%20Hu%20and%20Hanxin%20Zhu%20and%20Chen%20Hou%20and%20Cuiling%20Lan%20and%20Zhibo%20Chen%0AAbstract%3A%20%20%20Text-driven%20Image%20to%20Video%20Generation%20%28TI2V%29%20aims%20to%20generate%20controllable%0Avideo%20given%20the%20first%20frame%20and%20corresponding%20textual%20description.%20The%20primary%0Achallenges%20of%20this%20task%20lie%20in%20two%20parts%3A%20%28i%29%20how%20to%20identify%20the%20target%0Aobjects%20and%20ensure%20the%20consistency%20between%20the%20movement%20trajectory%20and%20the%0Atextual%20description.%20%28ii%29%20how%20to%20improve%20the%20subjective%20quality%20of%20generated%0Avideos.%20To%20tackle%20the%20above%20challenges%2C%20we%20propose%20a%20new%20diffusion-based%20TI2V%0Aframework%2C%20termed%20TIV-Diffusion%2C%20via%20object-centric%20textual-visual%20alignment%2C%0Aintending%20to%20achieve%20precise%20control%20and%20high-quality%20video%20generation%20based%20on%0Atextual-described%20motion%20for%20different%20objects.%20Concretely%2C%20we%20enable%20our%0ATIV-Diffuion%20model%20to%20perceive%20the%20textual-described%20objects%20and%20their%20motion%0Atrajectory%20by%20incorporating%20the%20fused%20textual%20and%20visual%20knowledge%20through%0Ascale-offset%20modulation.%20Moreover%2C%20to%20mitigate%20the%20problems%20of%20object%0Adisappearance%20and%20misaligned%20objects%20and%20motion%2C%20we%20introduce%20an%20object-centric%0Atextual-visual%20alignment%20module%2C%20which%20reduces%20the%20risk%20of%20misaligned%0Aobjects/motion%20by%20decoupling%20the%20objects%20in%20the%20reference%20image%20and%20aligning%0Atextual%20features%20with%20each%20object%20individually.%20Based%20on%20the%20above%20innovations%2C%0Aour%20TIV-Diffusion%20achieves%20state-of-the-art%20high-quality%20video%20generation%0Acompared%20with%20existing%20TI2V%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTIV-Diffusion%253A%2520Towards%2520Object-Centric%2520Movement%2520for%2520Text-driven%2520Image%2520to%250A%2520%2520Video%2520Generation%26entry.906535625%3DXingrui%2520Wang%2520and%2520Xin%2520Li%2520and%2520Yaosi%2520Hu%2520and%2520Hanxin%2520Zhu%2520and%2520Chen%2520Hou%2520and%2520Cuiling%2520Lan%2520and%2520Zhibo%2520Chen%26entry.1292438233%3D%2520%2520Text-driven%2520Image%2520to%2520Video%2520Generation%2520%2528TI2V%2529%2520aims%2520to%2520generate%2520controllable%250Avideo%2520given%2520the%2520first%2520frame%2520and%2520corresponding%2520textual%2520description.%2520The%2520primary%250Achallenges%2520of%2520this%2520task%2520lie%2520in%2520two%2520parts%253A%2520%2528i%2529%2520how%2520to%2520identify%2520the%2520target%250Aobjects%2520and%2520ensure%2520the%2520consistency%2520between%2520the%2520movement%2520trajectory%2520and%2520the%250Atextual%2520description.%2520%2528ii%2529%2520how%2520to%2520improve%2520the%2520subjective%2520quality%2520of%2520generated%250Avideos.%2520To%2520tackle%2520the%2520above%2520challenges%252C%2520we%2520propose%2520a%2520new%2520diffusion-based%2520TI2V%250Aframework%252C%2520termed%2520TIV-Diffusion%252C%2520via%2520object-centric%2520textual-visual%2520alignment%252C%250Aintending%2520to%2520achieve%2520precise%2520control%2520and%2520high-quality%2520video%2520generation%2520based%2520on%250Atextual-described%2520motion%2520for%2520different%2520objects.%2520Concretely%252C%2520we%2520enable%2520our%250ATIV-Diffuion%2520model%2520to%2520perceive%2520the%2520textual-described%2520objects%2520and%2520their%2520motion%250Atrajectory%2520by%2520incorporating%2520the%2520fused%2520textual%2520and%2520visual%2520knowledge%2520through%250Ascale-offset%2520modulation.%2520Moreover%252C%2520to%2520mitigate%2520the%2520problems%2520of%2520object%250Adisappearance%2520and%2520misaligned%2520objects%2520and%2520motion%252C%2520we%2520introduce%2520an%2520object-centric%250Atextual-visual%2520alignment%2520module%252C%2520which%2520reduces%2520the%2520risk%2520of%2520misaligned%250Aobjects/motion%2520by%2520decoupling%2520the%2520objects%2520in%2520the%2520reference%2520image%2520and%2520aligning%250Atextual%2520features%2520with%2520each%2520object%2520individually.%2520Based%2520on%2520the%2520above%2520innovations%252C%250Aour%2520TIV-Diffusion%2520achieves%2520state-of-the-art%2520high-quality%2520video%2520generation%250Acompared%2520with%2520existing%2520TI2V%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIV-Diffusion%3A%20Towards%20Object-Centric%20Movement%20for%20Text-driven%20Image%20to%0A%20%20Video%20Generation&entry.906535625=Xingrui%20Wang%20and%20Xin%20Li%20and%20Yaosi%20Hu%20and%20Hanxin%20Zhu%20and%20Chen%20Hou%20and%20Cuiling%20Lan%20and%20Zhibo%20Chen&entry.1292438233=%20%20Text-driven%20Image%20to%20Video%20Generation%20%28TI2V%29%20aims%20to%20generate%20controllable%0Avideo%20given%20the%20first%20frame%20and%20corresponding%20textual%20description.%20The%20primary%0Achallenges%20of%20this%20task%20lie%20in%20two%20parts%3A%20%28i%29%20how%20to%20identify%20the%20target%0Aobjects%20and%20ensure%20the%20consistency%20between%20the%20movement%20trajectory%20and%20the%0Atextual%20description.%20%28ii%29%20how%20to%20improve%20the%20subjective%20quality%20of%20generated%0Avideos.%20To%20tackle%20the%20above%20challenges%2C%20we%20propose%20a%20new%20diffusion-based%20TI2V%0Aframework%2C%20termed%20TIV-Diffusion%2C%20via%20object-centric%20textual-visual%20alignment%2C%0Aintending%20to%20achieve%20precise%20control%20and%20high-quality%20video%20generation%20based%20on%0Atextual-described%20motion%20for%20different%20objects.%20Concretely%2C%20we%20enable%20our%0ATIV-Diffuion%20model%20to%20perceive%20the%20textual-described%20objects%20and%20their%20motion%0Atrajectory%20by%20incorporating%20the%20fused%20textual%20and%20visual%20knowledge%20through%0Ascale-offset%20modulation.%20Moreover%2C%20to%20mitigate%20the%20problems%20of%20object%0Adisappearance%20and%20misaligned%20objects%20and%20motion%2C%20we%20introduce%20an%20object-centric%0Atextual-visual%20alignment%20module%2C%20which%20reduces%20the%20risk%20of%20misaligned%0Aobjects/motion%20by%20decoupling%20the%20objects%20in%20the%20reference%20image%20and%20aligning%0Atextual%20features%20with%20each%20object%20individually.%20Based%20on%20the%20above%20innovations%2C%0Aour%20TIV-Diffusion%20achieves%20state-of-the-art%20high-quality%20video%20generation%0Acompared%20with%20existing%20TI2V%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10275v1&entry.124074799=Read"},
{"title": "Arbitrary Reading Order Scene Text Spotter with Local Semantics Guidance", "author": "Jiahao Lyu and Wei Wang and Dongbao Yang and Jinwen Zhong and Yu Zhou", "abstract": "  Scene text spotting has attracted the enthusiasm of relative researchers in\nrecent years. Most existing scene text spotters follow the\ndetection-then-recognition paradigm, where the vanilla detection module hardly\ndetermines the reading order and leads to failure recognition. After rethinking\nthe auto-regressive scene text recognition method, we find that a well-trained\nrecognizer can implicitly perceive the local semantics of all characters in a\ncomplete word or a sentence without a character-level detection module. Local\nsemantic knowledge not only includes text content but also spatial information\nin the right reading order. Motivated by the above analysis, we propose the\nLocal Semantics Guided scene text Spotter (LSGSpotter), which auto-regressively\ndecodes the position and content of characters guided by the local semantics.\nSpecifically, two effective modules are proposed in LSGSpotter. On the one\nhand, we design a Start Point Localization Module (SPLM) for locating text\nstart points to determine the right reading order. On the other hand, a\nMulti-scale Adaptive Attention Module (MAAM) is proposed to adaptively\naggregate text features in a local area. In conclusion, LSGSpotter achieves the\narbitrary reading order spotting task without the limitation of sophisticated\ndetection, while alleviating the cost of computational resources with the grid\nsampling strategy. Extensive experiment results show LSGSpotter achieves\nstate-of-the-art performance on the InverseText benchmark. Moreover, our\nspotter demonstrates superior performance on English benchmarks for\narbitrary-shaped text, achieving improvements of 0.7\\% and 2.5\\% on Total-Text\nand SCUT-CTW1500, respectively. These results validate our text spotter is\neffective for scene texts in arbitrary reading order and shape.\n", "link": "http://arxiv.org/abs/2412.10159v1", "date": "2024-12-13", "relevancy": 2.6735, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.553}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Arbitrary%20Reading%20Order%20Scene%20Text%20Spotter%20with%20Local%20Semantics%20Guidance&body=Title%3A%20Arbitrary%20Reading%20Order%20Scene%20Text%20Spotter%20with%20Local%20Semantics%20Guidance%0AAuthor%3A%20Jiahao%20Lyu%20and%20Wei%20Wang%20and%20Dongbao%20Yang%20and%20Jinwen%20Zhong%20and%20Yu%20Zhou%0AAbstract%3A%20%20%20Scene%20text%20spotting%20has%20attracted%20the%20enthusiasm%20of%20relative%20researchers%20in%0Arecent%20years.%20Most%20existing%20scene%20text%20spotters%20follow%20the%0Adetection-then-recognition%20paradigm%2C%20where%20the%20vanilla%20detection%20module%20hardly%0Adetermines%20the%20reading%20order%20and%20leads%20to%20failure%20recognition.%20After%20rethinking%0Athe%20auto-regressive%20scene%20text%20recognition%20method%2C%20we%20find%20that%20a%20well-trained%0Arecognizer%20can%20implicitly%20perceive%20the%20local%20semantics%20of%20all%20characters%20in%20a%0Acomplete%20word%20or%20a%20sentence%20without%20a%20character-level%20detection%20module.%20Local%0Asemantic%20knowledge%20not%20only%20includes%20text%20content%20but%20also%20spatial%20information%0Ain%20the%20right%20reading%20order.%20Motivated%20by%20the%20above%20analysis%2C%20we%20propose%20the%0ALocal%20Semantics%20Guided%20scene%20text%20Spotter%20%28LSGSpotter%29%2C%20which%20auto-regressively%0Adecodes%20the%20position%20and%20content%20of%20characters%20guided%20by%20the%20local%20semantics.%0ASpecifically%2C%20two%20effective%20modules%20are%20proposed%20in%20LSGSpotter.%20On%20the%20one%0Ahand%2C%20we%20design%20a%20Start%20Point%20Localization%20Module%20%28SPLM%29%20for%20locating%20text%0Astart%20points%20to%20determine%20the%20right%20reading%20order.%20On%20the%20other%20hand%2C%20a%0AMulti-scale%20Adaptive%20Attention%20Module%20%28MAAM%29%20is%20proposed%20to%20adaptively%0Aaggregate%20text%20features%20in%20a%20local%20area.%20In%20conclusion%2C%20LSGSpotter%20achieves%20the%0Aarbitrary%20reading%20order%20spotting%20task%20without%20the%20limitation%20of%20sophisticated%0Adetection%2C%20while%20alleviating%20the%20cost%20of%20computational%20resources%20with%20the%20grid%0Asampling%20strategy.%20Extensive%20experiment%20results%20show%20LSGSpotter%20achieves%0Astate-of-the-art%20performance%20on%20the%20InverseText%20benchmark.%20Moreover%2C%20our%0Aspotter%20demonstrates%20superior%20performance%20on%20English%20benchmarks%20for%0Aarbitrary-shaped%20text%2C%20achieving%20improvements%20of%200.7%5C%25%20and%202.5%5C%25%20on%20Total-Text%0Aand%20SCUT-CTW1500%2C%20respectively.%20These%20results%20validate%20our%20text%20spotter%20is%0Aeffective%20for%20scene%20texts%20in%20arbitrary%20reading%20order%20and%20shape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArbitrary%2520Reading%2520Order%2520Scene%2520Text%2520Spotter%2520with%2520Local%2520Semantics%2520Guidance%26entry.906535625%3DJiahao%2520Lyu%2520and%2520Wei%2520Wang%2520and%2520Dongbao%2520Yang%2520and%2520Jinwen%2520Zhong%2520and%2520Yu%2520Zhou%26entry.1292438233%3D%2520%2520Scene%2520text%2520spotting%2520has%2520attracted%2520the%2520enthusiasm%2520of%2520relative%2520researchers%2520in%250Arecent%2520years.%2520Most%2520existing%2520scene%2520text%2520spotters%2520follow%2520the%250Adetection-then-recognition%2520paradigm%252C%2520where%2520the%2520vanilla%2520detection%2520module%2520hardly%250Adetermines%2520the%2520reading%2520order%2520and%2520leads%2520to%2520failure%2520recognition.%2520After%2520rethinking%250Athe%2520auto-regressive%2520scene%2520text%2520recognition%2520method%252C%2520we%2520find%2520that%2520a%2520well-trained%250Arecognizer%2520can%2520implicitly%2520perceive%2520the%2520local%2520semantics%2520of%2520all%2520characters%2520in%2520a%250Acomplete%2520word%2520or%2520a%2520sentence%2520without%2520a%2520character-level%2520detection%2520module.%2520Local%250Asemantic%2520knowledge%2520not%2520only%2520includes%2520text%2520content%2520but%2520also%2520spatial%2520information%250Ain%2520the%2520right%2520reading%2520order.%2520Motivated%2520by%2520the%2520above%2520analysis%252C%2520we%2520propose%2520the%250ALocal%2520Semantics%2520Guided%2520scene%2520text%2520Spotter%2520%2528LSGSpotter%2529%252C%2520which%2520auto-regressively%250Adecodes%2520the%2520position%2520and%2520content%2520of%2520characters%2520guided%2520by%2520the%2520local%2520semantics.%250ASpecifically%252C%2520two%2520effective%2520modules%2520are%2520proposed%2520in%2520LSGSpotter.%2520On%2520the%2520one%250Ahand%252C%2520we%2520design%2520a%2520Start%2520Point%2520Localization%2520Module%2520%2528SPLM%2529%2520for%2520locating%2520text%250Astart%2520points%2520to%2520determine%2520the%2520right%2520reading%2520order.%2520On%2520the%2520other%2520hand%252C%2520a%250AMulti-scale%2520Adaptive%2520Attention%2520Module%2520%2528MAAM%2529%2520is%2520proposed%2520to%2520adaptively%250Aaggregate%2520text%2520features%2520in%2520a%2520local%2520area.%2520In%2520conclusion%252C%2520LSGSpotter%2520achieves%2520the%250Aarbitrary%2520reading%2520order%2520spotting%2520task%2520without%2520the%2520limitation%2520of%2520sophisticated%250Adetection%252C%2520while%2520alleviating%2520the%2520cost%2520of%2520computational%2520resources%2520with%2520the%2520grid%250Asampling%2520strategy.%2520Extensive%2520experiment%2520results%2520show%2520LSGSpotter%2520achieves%250Astate-of-the-art%2520performance%2520on%2520the%2520InverseText%2520benchmark.%2520Moreover%252C%2520our%250Aspotter%2520demonstrates%2520superior%2520performance%2520on%2520English%2520benchmarks%2520for%250Aarbitrary-shaped%2520text%252C%2520achieving%2520improvements%2520of%25200.7%255C%2525%2520and%25202.5%255C%2525%2520on%2520Total-Text%250Aand%2520SCUT-CTW1500%252C%2520respectively.%2520These%2520results%2520validate%2520our%2520text%2520spotter%2520is%250Aeffective%2520for%2520scene%2520texts%2520in%2520arbitrary%2520reading%2520order%2520and%2520shape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arbitrary%20Reading%20Order%20Scene%20Text%20Spotter%20with%20Local%20Semantics%20Guidance&entry.906535625=Jiahao%20Lyu%20and%20Wei%20Wang%20and%20Dongbao%20Yang%20and%20Jinwen%20Zhong%20and%20Yu%20Zhou&entry.1292438233=%20%20Scene%20text%20spotting%20has%20attracted%20the%20enthusiasm%20of%20relative%20researchers%20in%0Arecent%20years.%20Most%20existing%20scene%20text%20spotters%20follow%20the%0Adetection-then-recognition%20paradigm%2C%20where%20the%20vanilla%20detection%20module%20hardly%0Adetermines%20the%20reading%20order%20and%20leads%20to%20failure%20recognition.%20After%20rethinking%0Athe%20auto-regressive%20scene%20text%20recognition%20method%2C%20we%20find%20that%20a%20well-trained%0Arecognizer%20can%20implicitly%20perceive%20the%20local%20semantics%20of%20all%20characters%20in%20a%0Acomplete%20word%20or%20a%20sentence%20without%20a%20character-level%20detection%20module.%20Local%0Asemantic%20knowledge%20not%20only%20includes%20text%20content%20but%20also%20spatial%20information%0Ain%20the%20right%20reading%20order.%20Motivated%20by%20the%20above%20analysis%2C%20we%20propose%20the%0ALocal%20Semantics%20Guided%20scene%20text%20Spotter%20%28LSGSpotter%29%2C%20which%20auto-regressively%0Adecodes%20the%20position%20and%20content%20of%20characters%20guided%20by%20the%20local%20semantics.%0ASpecifically%2C%20two%20effective%20modules%20are%20proposed%20in%20LSGSpotter.%20On%20the%20one%0Ahand%2C%20we%20design%20a%20Start%20Point%20Localization%20Module%20%28SPLM%29%20for%20locating%20text%0Astart%20points%20to%20determine%20the%20right%20reading%20order.%20On%20the%20other%20hand%2C%20a%0AMulti-scale%20Adaptive%20Attention%20Module%20%28MAAM%29%20is%20proposed%20to%20adaptively%0Aaggregate%20text%20features%20in%20a%20local%20area.%20In%20conclusion%2C%20LSGSpotter%20achieves%20the%0Aarbitrary%20reading%20order%20spotting%20task%20without%20the%20limitation%20of%20sophisticated%0Adetection%2C%20while%20alleviating%20the%20cost%20of%20computational%20resources%20with%20the%20grid%0Asampling%20strategy.%20Extensive%20experiment%20results%20show%20LSGSpotter%20achieves%0Astate-of-the-art%20performance%20on%20the%20InverseText%20benchmark.%20Moreover%2C%20our%0Aspotter%20demonstrates%20superior%20performance%20on%20English%20benchmarks%20for%0Aarbitrary-shaped%20text%2C%20achieving%20improvements%20of%200.7%5C%25%20and%202.5%5C%25%20on%20Total-Text%0Aand%20SCUT-CTW1500%2C%20respectively.%20These%20results%20validate%20our%20text%20spotter%20is%0Aeffective%20for%20scene%20texts%20in%20arbitrary%20reading%20order%20and%20shape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10159v1&entry.124074799=Read"},
{"title": "Olympus: A Universal Task Router for Computer Vision Tasks", "author": "Yuanze Lin and Yunsheng Li and Dongdong Chen and Weijian Xu and Ronald Clark and Philip H. S. Torr", "abstract": "  We introduce Olympus, a new approach that transforms Multimodal Large\nLanguage Models (MLLMs) into a unified framework capable of handling a wide\narray of computer vision tasks. Utilizing a controller MLLM, Olympus delegates\nover 20 specialized tasks across images, videos, and 3D objects to dedicated\nmodules. This instruction-based routing enables complex workflows through\nchained actions without the need for training heavy generative models. Olympus\neasily integrates with existing MLLMs, expanding their capabilities with\ncomparable performance. Experimental results demonstrate that Olympus achieves\nan average routing accuracy of 94.75% across 20 tasks and precision of 91.82%\nin chained action scenarios, showcasing its effectiveness as a universal task\nrouter that can solve a diverse range of computer vision tasks. Project page:\nhttp://yuanze-lin.me/Olympus_page/\n", "link": "http://arxiv.org/abs/2412.09612v2", "date": "2024-12-13", "relevancy": 2.653, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5351}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5283}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Olympus%3A%20A%20Universal%20Task%20Router%20for%20Computer%20Vision%20Tasks&body=Title%3A%20Olympus%3A%20A%20Universal%20Task%20Router%20for%20Computer%20Vision%20Tasks%0AAuthor%3A%20Yuanze%20Lin%20and%20Yunsheng%20Li%20and%20Dongdong%20Chen%20and%20Weijian%20Xu%20and%20Ronald%20Clark%20and%20Philip%20H.%20S.%20Torr%0AAbstract%3A%20%20%20We%20introduce%20Olympus%2C%20a%20new%20approach%20that%20transforms%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29%20into%20a%20unified%20framework%20capable%20of%20handling%20a%20wide%0Aarray%20of%20computer%20vision%20tasks.%20Utilizing%20a%20controller%20MLLM%2C%20Olympus%20delegates%0Aover%2020%20specialized%20tasks%20across%20images%2C%20videos%2C%20and%203D%20objects%20to%20dedicated%0Amodules.%20This%20instruction-based%20routing%20enables%20complex%20workflows%20through%0Achained%20actions%20without%20the%20need%20for%20training%20heavy%20generative%20models.%20Olympus%0Aeasily%20integrates%20with%20existing%20MLLMs%2C%20expanding%20their%20capabilities%20with%0Acomparable%20performance.%20Experimental%20results%20demonstrate%20that%20Olympus%20achieves%0Aan%20average%20routing%20accuracy%20of%2094.75%25%20across%2020%20tasks%20and%20precision%20of%2091.82%25%0Ain%20chained%20action%20scenarios%2C%20showcasing%20its%20effectiveness%20as%20a%20universal%20task%0Arouter%20that%20can%20solve%20a%20diverse%20range%20of%20computer%20vision%20tasks.%20Project%20page%3A%0Ahttp%3A//yuanze-lin.me/Olympus_page/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09612v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOlympus%253A%2520A%2520Universal%2520Task%2520Router%2520for%2520Computer%2520Vision%2520Tasks%26entry.906535625%3DYuanze%2520Lin%2520and%2520Yunsheng%2520Li%2520and%2520Dongdong%2520Chen%2520and%2520Weijian%2520Xu%2520and%2520Ronald%2520Clark%2520and%2520Philip%2520H.%2520S.%2520Torr%26entry.1292438233%3D%2520%2520We%2520introduce%2520Olympus%252C%2520a%2520new%2520approach%2520that%2520transforms%2520Multimodal%2520Large%250ALanguage%2520Models%2520%2528MLLMs%2529%2520into%2520a%2520unified%2520framework%2520capable%2520of%2520handling%2520a%2520wide%250Aarray%2520of%2520computer%2520vision%2520tasks.%2520Utilizing%2520a%2520controller%2520MLLM%252C%2520Olympus%2520delegates%250Aover%252020%2520specialized%2520tasks%2520across%2520images%252C%2520videos%252C%2520and%25203D%2520objects%2520to%2520dedicated%250Amodules.%2520This%2520instruction-based%2520routing%2520enables%2520complex%2520workflows%2520through%250Achained%2520actions%2520without%2520the%2520need%2520for%2520training%2520heavy%2520generative%2520models.%2520Olympus%250Aeasily%2520integrates%2520with%2520existing%2520MLLMs%252C%2520expanding%2520their%2520capabilities%2520with%250Acomparable%2520performance.%2520Experimental%2520results%2520demonstrate%2520that%2520Olympus%2520achieves%250Aan%2520average%2520routing%2520accuracy%2520of%252094.75%2525%2520across%252020%2520tasks%2520and%2520precision%2520of%252091.82%2525%250Ain%2520chained%2520action%2520scenarios%252C%2520showcasing%2520its%2520effectiveness%2520as%2520a%2520universal%2520task%250Arouter%2520that%2520can%2520solve%2520a%2520diverse%2520range%2520of%2520computer%2520vision%2520tasks.%2520Project%2520page%253A%250Ahttp%253A//yuanze-lin.me/Olympus_page/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09612v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Olympus%3A%20A%20Universal%20Task%20Router%20for%20Computer%20Vision%20Tasks&entry.906535625=Yuanze%20Lin%20and%20Yunsheng%20Li%20and%20Dongdong%20Chen%20and%20Weijian%20Xu%20and%20Ronald%20Clark%20and%20Philip%20H.%20S.%20Torr&entry.1292438233=%20%20We%20introduce%20Olympus%2C%20a%20new%20approach%20that%20transforms%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29%20into%20a%20unified%20framework%20capable%20of%20handling%20a%20wide%0Aarray%20of%20computer%20vision%20tasks.%20Utilizing%20a%20controller%20MLLM%2C%20Olympus%20delegates%0Aover%2020%20specialized%20tasks%20across%20images%2C%20videos%2C%20and%203D%20objects%20to%20dedicated%0Amodules.%20This%20instruction-based%20routing%20enables%20complex%20workflows%20through%0Achained%20actions%20without%20the%20need%20for%20training%20heavy%20generative%20models.%20Olympus%0Aeasily%20integrates%20with%20existing%20MLLMs%2C%20expanding%20their%20capabilities%20with%0Acomparable%20performance.%20Experimental%20results%20demonstrate%20that%20Olympus%20achieves%0Aan%20average%20routing%20accuracy%20of%2094.75%25%20across%2020%20tasks%20and%20precision%20of%2091.82%25%0Ain%20chained%20action%20scenarios%2C%20showcasing%20its%20effectiveness%20as%20a%20universal%20task%0Arouter%20that%20can%20solve%20a%20diverse%20range%20of%20computer%20vision%20tasks.%20Project%20page%3A%0Ahttp%3A//yuanze-lin.me/Olympus_page/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09612v2&entry.124074799=Read"},
{"title": "FIRE-3DV: Framework-Independent Rendering Engine for 3D Graphics using\n  Vulkan", "author": "Christopher John Allison and Haoying Zhou and Adnan Munawar and Peter Kazanzides and Juan Antonio Barragan", "abstract": "  Interactive dynamic simulators are an accelerator for developing novel\nrobotic control algorithms and complex systems involving humans and robots. In\nuser training and synthetic data generation applications, high-fidelity\nvisualizations from the simulation are essential. Yet, robotic simulators often\nlimit their rendering algorithms to preserve real-time interaction with the\nsimulation. Advancements in Graphics Processing Units (GPU) enable improved\nvisualization without compromising performance. However, these advancements\ncannot be fully leveraged in simulation frameworks that use legacy graphics\napplication programming interfaces (API) to interface with the GPU. This paper\npresents a performance-focused and lightweight rendering engine supporting the\nmodern Vulkan graphics API that can be easily integrated with other simulation\nframeworks to enhance visualizations. To illustrate the proposed method, our\nengine is used to modernize the legacy rendering pipeline of the Asynchronous\nMulti-Body Framework (AMBF), a dynamic simulation framework used extensively\nfor interactive robotics simulation development. This new rendering engine\nimplements graphical features such as physically based rendering (PBR),\nanti-aliasing, and ray-traced shadows, significantly improving the image\nfidelity of AMBF. Computational experiments show that the engine can render a\nsimulated scene with over seven million triangles while maintaining GPU\ncomputation times within two milliseconds.\n", "link": "http://arxiv.org/abs/2410.05095v2", "date": "2024-12-13", "relevancy": 2.6486, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5348}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5348}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FIRE-3DV%3A%20Framework-Independent%20Rendering%20Engine%20for%203D%20Graphics%20using%0A%20%20Vulkan&body=Title%3A%20FIRE-3DV%3A%20Framework-Independent%20Rendering%20Engine%20for%203D%20Graphics%20using%0A%20%20Vulkan%0AAuthor%3A%20Christopher%20John%20Allison%20and%20Haoying%20Zhou%20and%20Adnan%20Munawar%20and%20Peter%20Kazanzides%20and%20Juan%20Antonio%20Barragan%0AAbstract%3A%20%20%20Interactive%20dynamic%20simulators%20are%20an%20accelerator%20for%20developing%20novel%0Arobotic%20control%20algorithms%20and%20complex%20systems%20involving%20humans%20and%20robots.%20In%0Auser%20training%20and%20synthetic%20data%20generation%20applications%2C%20high-fidelity%0Avisualizations%20from%20the%20simulation%20are%20essential.%20Yet%2C%20robotic%20simulators%20often%0Alimit%20their%20rendering%20algorithms%20to%20preserve%20real-time%20interaction%20with%20the%0Asimulation.%20Advancements%20in%20Graphics%20Processing%20Units%20%28GPU%29%20enable%20improved%0Avisualization%20without%20compromising%20performance.%20However%2C%20these%20advancements%0Acannot%20be%20fully%20leveraged%20in%20simulation%20frameworks%20that%20use%20legacy%20graphics%0Aapplication%20programming%20interfaces%20%28API%29%20to%20interface%20with%20the%20GPU.%20This%20paper%0Apresents%20a%20performance-focused%20and%20lightweight%20rendering%20engine%20supporting%20the%0Amodern%20Vulkan%20graphics%20API%20that%20can%20be%20easily%20integrated%20with%20other%20simulation%0Aframeworks%20to%20enhance%20visualizations.%20To%20illustrate%20the%20proposed%20method%2C%20our%0Aengine%20is%20used%20to%20modernize%20the%20legacy%20rendering%20pipeline%20of%20the%20Asynchronous%0AMulti-Body%20Framework%20%28AMBF%29%2C%20a%20dynamic%20simulation%20framework%20used%20extensively%0Afor%20interactive%20robotics%20simulation%20development.%20This%20new%20rendering%20engine%0Aimplements%20graphical%20features%20such%20as%20physically%20based%20rendering%20%28PBR%29%2C%0Aanti-aliasing%2C%20and%20ray-traced%20shadows%2C%20significantly%20improving%20the%20image%0Afidelity%20of%20AMBF.%20Computational%20experiments%20show%20that%20the%20engine%20can%20render%20a%0Asimulated%20scene%20with%20over%20seven%20million%20triangles%20while%20maintaining%20GPU%0Acomputation%20times%20within%20two%20milliseconds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05095v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFIRE-3DV%253A%2520Framework-Independent%2520Rendering%2520Engine%2520for%25203D%2520Graphics%2520using%250A%2520%2520Vulkan%26entry.906535625%3DChristopher%2520John%2520Allison%2520and%2520Haoying%2520Zhou%2520and%2520Adnan%2520Munawar%2520and%2520Peter%2520Kazanzides%2520and%2520Juan%2520Antonio%2520Barragan%26entry.1292438233%3D%2520%2520Interactive%2520dynamic%2520simulators%2520are%2520an%2520accelerator%2520for%2520developing%2520novel%250Arobotic%2520control%2520algorithms%2520and%2520complex%2520systems%2520involving%2520humans%2520and%2520robots.%2520In%250Auser%2520training%2520and%2520synthetic%2520data%2520generation%2520applications%252C%2520high-fidelity%250Avisualizations%2520from%2520the%2520simulation%2520are%2520essential.%2520Yet%252C%2520robotic%2520simulators%2520often%250Alimit%2520their%2520rendering%2520algorithms%2520to%2520preserve%2520real-time%2520interaction%2520with%2520the%250Asimulation.%2520Advancements%2520in%2520Graphics%2520Processing%2520Units%2520%2528GPU%2529%2520enable%2520improved%250Avisualization%2520without%2520compromising%2520performance.%2520However%252C%2520these%2520advancements%250Acannot%2520be%2520fully%2520leveraged%2520in%2520simulation%2520frameworks%2520that%2520use%2520legacy%2520graphics%250Aapplication%2520programming%2520interfaces%2520%2528API%2529%2520to%2520interface%2520with%2520the%2520GPU.%2520This%2520paper%250Apresents%2520a%2520performance-focused%2520and%2520lightweight%2520rendering%2520engine%2520supporting%2520the%250Amodern%2520Vulkan%2520graphics%2520API%2520that%2520can%2520be%2520easily%2520integrated%2520with%2520other%2520simulation%250Aframeworks%2520to%2520enhance%2520visualizations.%2520To%2520illustrate%2520the%2520proposed%2520method%252C%2520our%250Aengine%2520is%2520used%2520to%2520modernize%2520the%2520legacy%2520rendering%2520pipeline%2520of%2520the%2520Asynchronous%250AMulti-Body%2520Framework%2520%2528AMBF%2529%252C%2520a%2520dynamic%2520simulation%2520framework%2520used%2520extensively%250Afor%2520interactive%2520robotics%2520simulation%2520development.%2520This%2520new%2520rendering%2520engine%250Aimplements%2520graphical%2520features%2520such%2520as%2520physically%2520based%2520rendering%2520%2528PBR%2529%252C%250Aanti-aliasing%252C%2520and%2520ray-traced%2520shadows%252C%2520significantly%2520improving%2520the%2520image%250Afidelity%2520of%2520AMBF.%2520Computational%2520experiments%2520show%2520that%2520the%2520engine%2520can%2520render%2520a%250Asimulated%2520scene%2520with%2520over%2520seven%2520million%2520triangles%2520while%2520maintaining%2520GPU%250Acomputation%2520times%2520within%2520two%2520milliseconds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05095v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FIRE-3DV%3A%20Framework-Independent%20Rendering%20Engine%20for%203D%20Graphics%20using%0A%20%20Vulkan&entry.906535625=Christopher%20John%20Allison%20and%20Haoying%20Zhou%20and%20Adnan%20Munawar%20and%20Peter%20Kazanzides%20and%20Juan%20Antonio%20Barragan&entry.1292438233=%20%20Interactive%20dynamic%20simulators%20are%20an%20accelerator%20for%20developing%20novel%0Arobotic%20control%20algorithms%20and%20complex%20systems%20involving%20humans%20and%20robots.%20In%0Auser%20training%20and%20synthetic%20data%20generation%20applications%2C%20high-fidelity%0Avisualizations%20from%20the%20simulation%20are%20essential.%20Yet%2C%20robotic%20simulators%20often%0Alimit%20their%20rendering%20algorithms%20to%20preserve%20real-time%20interaction%20with%20the%0Asimulation.%20Advancements%20in%20Graphics%20Processing%20Units%20%28GPU%29%20enable%20improved%0Avisualization%20without%20compromising%20performance.%20However%2C%20these%20advancements%0Acannot%20be%20fully%20leveraged%20in%20simulation%20frameworks%20that%20use%20legacy%20graphics%0Aapplication%20programming%20interfaces%20%28API%29%20to%20interface%20with%20the%20GPU.%20This%20paper%0Apresents%20a%20performance-focused%20and%20lightweight%20rendering%20engine%20supporting%20the%0Amodern%20Vulkan%20graphics%20API%20that%20can%20be%20easily%20integrated%20with%20other%20simulation%0Aframeworks%20to%20enhance%20visualizations.%20To%20illustrate%20the%20proposed%20method%2C%20our%0Aengine%20is%20used%20to%20modernize%20the%20legacy%20rendering%20pipeline%20of%20the%20Asynchronous%0AMulti-Body%20Framework%20%28AMBF%29%2C%20a%20dynamic%20simulation%20framework%20used%20extensively%0Afor%20interactive%20robotics%20simulation%20development.%20This%20new%20rendering%20engine%0Aimplements%20graphical%20features%20such%20as%20physically%20based%20rendering%20%28PBR%29%2C%0Aanti-aliasing%2C%20and%20ray-traced%20shadows%2C%20significantly%20improving%20the%20image%0Afidelity%20of%20AMBF.%20Computational%20experiments%20show%20that%20the%20engine%20can%20render%20a%0Asimulated%20scene%20with%20over%20seven%20million%20triangles%20while%20maintaining%20GPU%0Acomputation%20times%20within%20two%20milliseconds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05095v2&entry.124074799=Read"},
{"title": "Coherent 3D Scene Diffusion From a Single RGB Image", "author": "Manuel Dahnert and Angela Dai and Norman M\u00fcller and Matthias Nie\u00dfner", "abstract": "  We present a novel diffusion-based approach for coherent 3D scene\nreconstruction from a single RGB image. Our method utilizes an\nimage-conditioned 3D scene diffusion model to simultaneously denoise the 3D\nposes and geometries of all objects within the scene. Motivated by the\nill-posed nature of the task and to obtain consistent scene reconstruction\nresults, we learn a generative scene prior by conditioning on all scene objects\nsimultaneously to capture the scene context and by allowing the model to learn\ninter-object relationships throughout the diffusion process. We further propose\nan efficient surface alignment loss to facilitate training even in the absence\nof full ground-truth annotation, which is common in publicly available\ndatasets. This loss leverages an expressive shape representation, which enables\ndirect point sampling from intermediate shape predictions. By framing the task\nof single RGB image 3D scene reconstruction as a conditional diffusion process,\nour approach surpasses current state-of-the-art methods, achieving a 12.04%\nimprovement in AP3D on SUN RGB-D and a 13.43% increase in F-Score on Pix3D.\n", "link": "http://arxiv.org/abs/2412.10294v1", "date": "2024-12-13", "relevancy": 2.6419, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6643}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6643}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coherent%203D%20Scene%20Diffusion%20From%20a%20Single%20RGB%20Image&body=Title%3A%20Coherent%203D%20Scene%20Diffusion%20From%20a%20Single%20RGB%20Image%0AAuthor%3A%20Manuel%20Dahnert%20and%20Angela%20Dai%20and%20Norman%20M%C3%BCller%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20%20%20We%20present%20a%20novel%20diffusion-based%20approach%20for%20coherent%203D%20scene%0Areconstruction%20from%20a%20single%20RGB%20image.%20Our%20method%20utilizes%20an%0Aimage-conditioned%203D%20scene%20diffusion%20model%20to%20simultaneously%20denoise%20the%203D%0Aposes%20and%20geometries%20of%20all%20objects%20within%20the%20scene.%20Motivated%20by%20the%0Aill-posed%20nature%20of%20the%20task%20and%20to%20obtain%20consistent%20scene%20reconstruction%0Aresults%2C%20we%20learn%20a%20generative%20scene%20prior%20by%20conditioning%20on%20all%20scene%20objects%0Asimultaneously%20to%20capture%20the%20scene%20context%20and%20by%20allowing%20the%20model%20to%20learn%0Ainter-object%20relationships%20throughout%20the%20diffusion%20process.%20We%20further%20propose%0Aan%20efficient%20surface%20alignment%20loss%20to%20facilitate%20training%20even%20in%20the%20absence%0Aof%20full%20ground-truth%20annotation%2C%20which%20is%20common%20in%20publicly%20available%0Adatasets.%20This%20loss%20leverages%20an%20expressive%20shape%20representation%2C%20which%20enables%0Adirect%20point%20sampling%20from%20intermediate%20shape%20predictions.%20By%20framing%20the%20task%0Aof%20single%20RGB%20image%203D%20scene%20reconstruction%20as%20a%20conditional%20diffusion%20process%2C%0Aour%20approach%20surpasses%20current%20state-of-the-art%20methods%2C%20achieving%20a%2012.04%25%0Aimprovement%20in%20AP3D%20on%20SUN%20RGB-D%20and%20a%2013.43%25%20increase%20in%20F-Score%20on%20Pix3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10294v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoherent%25203D%2520Scene%2520Diffusion%2520From%2520a%2520Single%2520RGB%2520Image%26entry.906535625%3DManuel%2520Dahnert%2520and%2520Angela%2520Dai%2520and%2520Norman%2520M%25C3%25BCller%2520and%2520Matthias%2520Nie%25C3%259Fner%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520diffusion-based%2520approach%2520for%2520coherent%25203D%2520scene%250Areconstruction%2520from%2520a%2520single%2520RGB%2520image.%2520Our%2520method%2520utilizes%2520an%250Aimage-conditioned%25203D%2520scene%2520diffusion%2520model%2520to%2520simultaneously%2520denoise%2520the%25203D%250Aposes%2520and%2520geometries%2520of%2520all%2520objects%2520within%2520the%2520scene.%2520Motivated%2520by%2520the%250Aill-posed%2520nature%2520of%2520the%2520task%2520and%2520to%2520obtain%2520consistent%2520scene%2520reconstruction%250Aresults%252C%2520we%2520learn%2520a%2520generative%2520scene%2520prior%2520by%2520conditioning%2520on%2520all%2520scene%2520objects%250Asimultaneously%2520to%2520capture%2520the%2520scene%2520context%2520and%2520by%2520allowing%2520the%2520model%2520to%2520learn%250Ainter-object%2520relationships%2520throughout%2520the%2520diffusion%2520process.%2520We%2520further%2520propose%250Aan%2520efficient%2520surface%2520alignment%2520loss%2520to%2520facilitate%2520training%2520even%2520in%2520the%2520absence%250Aof%2520full%2520ground-truth%2520annotation%252C%2520which%2520is%2520common%2520in%2520publicly%2520available%250Adatasets.%2520This%2520loss%2520leverages%2520an%2520expressive%2520shape%2520representation%252C%2520which%2520enables%250Adirect%2520point%2520sampling%2520from%2520intermediate%2520shape%2520predictions.%2520By%2520framing%2520the%2520task%250Aof%2520single%2520RGB%2520image%25203D%2520scene%2520reconstruction%2520as%2520a%2520conditional%2520diffusion%2520process%252C%250Aour%2520approach%2520surpasses%2520current%2520state-of-the-art%2520methods%252C%2520achieving%2520a%252012.04%2525%250Aimprovement%2520in%2520AP3D%2520on%2520SUN%2520RGB-D%2520and%2520a%252013.43%2525%2520increase%2520in%2520F-Score%2520on%2520Pix3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10294v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coherent%203D%20Scene%20Diffusion%20From%20a%20Single%20RGB%20Image&entry.906535625=Manuel%20Dahnert%20and%20Angela%20Dai%20and%20Norman%20M%C3%BCller%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=%20%20We%20present%20a%20novel%20diffusion-based%20approach%20for%20coherent%203D%20scene%0Areconstruction%20from%20a%20single%20RGB%20image.%20Our%20method%20utilizes%20an%0Aimage-conditioned%203D%20scene%20diffusion%20model%20to%20simultaneously%20denoise%20the%203D%0Aposes%20and%20geometries%20of%20all%20objects%20within%20the%20scene.%20Motivated%20by%20the%0Aill-posed%20nature%20of%20the%20task%20and%20to%20obtain%20consistent%20scene%20reconstruction%0Aresults%2C%20we%20learn%20a%20generative%20scene%20prior%20by%20conditioning%20on%20all%20scene%20objects%0Asimultaneously%20to%20capture%20the%20scene%20context%20and%20by%20allowing%20the%20model%20to%20learn%0Ainter-object%20relationships%20throughout%20the%20diffusion%20process.%20We%20further%20propose%0Aan%20efficient%20surface%20alignment%20loss%20to%20facilitate%20training%20even%20in%20the%20absence%0Aof%20full%20ground-truth%20annotation%2C%20which%20is%20common%20in%20publicly%20available%0Adatasets.%20This%20loss%20leverages%20an%20expressive%20shape%20representation%2C%20which%20enables%0Adirect%20point%20sampling%20from%20intermediate%20shape%20predictions.%20By%20framing%20the%20task%0Aof%20single%20RGB%20image%203D%20scene%20reconstruction%20as%20a%20conditional%20diffusion%20process%2C%0Aour%20approach%20surpasses%20current%20state-of-the-art%20methods%2C%20achieving%20a%2012.04%25%0Aimprovement%20in%20AP3D%20on%20SUN%20RGB-D%20and%20a%2013.43%25%20increase%20in%20F-Score%20on%20Pix3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10294v1&entry.124074799=Read"},
{"title": "OP-LoRA: The Blessing of Dimensionality", "author": "Piotr Teterwak and Kate Saenko and Bryan A. Plummer and Ser-Nam Lim", "abstract": "  Low-rank adapters enable fine-tuning of large models with only a small number\nof parameters, thus reducing storage costs and minimizing the risk of\ncatastrophic forgetting. However, they often pose optimization challenges, with\npoor convergence. To overcome these challenges, we introduce an\nover-parameterized approach that accelerates training without increasing\ninference costs. This method reparameterizes low-rank adaptation by employing a\nseparate MLP and learned embedding for each layer. The learned embedding is\ninput to the MLP, which generates the adapter parameters. Such\noverparamaterization has been shown to implicitly function as an adaptive\nlearning rate and momentum, accelerating optimization. At inference time, the\nMLP can be discarded, leaving behind a standard low-rank adapter. To study the\neffect of MLP overparameterization on a small yet difficult proxy task, we\nimplement it for matrix factorization, and find it achieves faster convergence\nand lower final loss. Extending this approach to larger-scale tasks, we observe\nconsistent performance gains across domains. We achieve improvements in\nvision-language tasks and especially notable increases in image generation,\nwith CMMD scores improving by up to 15 points.\n", "link": "http://arxiv.org/abs/2412.10362v1", "date": "2024-12-13", "relevancy": 2.639, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.539}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OP-LoRA%3A%20The%20Blessing%20of%20Dimensionality&body=Title%3A%20OP-LoRA%3A%20The%20Blessing%20of%20Dimensionality%0AAuthor%3A%20Piotr%20Teterwak%20and%20Kate%20Saenko%20and%20Bryan%20A.%20Plummer%20and%20Ser-Nam%20Lim%0AAbstract%3A%20%20%20Low-rank%20adapters%20enable%20fine-tuning%20of%20large%20models%20with%20only%20a%20small%20number%0Aof%20parameters%2C%20thus%20reducing%20storage%20costs%20and%20minimizing%20the%20risk%20of%0Acatastrophic%20forgetting.%20However%2C%20they%20often%20pose%20optimization%20challenges%2C%20with%0Apoor%20convergence.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20an%0Aover-parameterized%20approach%20that%20accelerates%20training%20without%20increasing%0Ainference%20costs.%20This%20method%20reparameterizes%20low-rank%20adaptation%20by%20employing%20a%0Aseparate%20MLP%20and%20learned%20embedding%20for%20each%20layer.%20The%20learned%20embedding%20is%0Ainput%20to%20the%20MLP%2C%20which%20generates%20the%20adapter%20parameters.%20Such%0Aoverparamaterization%20has%20been%20shown%20to%20implicitly%20function%20as%20an%20adaptive%0Alearning%20rate%20and%20momentum%2C%20accelerating%20optimization.%20At%20inference%20time%2C%20the%0AMLP%20can%20be%20discarded%2C%20leaving%20behind%20a%20standard%20low-rank%20adapter.%20To%20study%20the%0Aeffect%20of%20MLP%20overparameterization%20on%20a%20small%20yet%20difficult%20proxy%20task%2C%20we%0Aimplement%20it%20for%20matrix%20factorization%2C%20and%20find%20it%20achieves%20faster%20convergence%0Aand%20lower%20final%20loss.%20Extending%20this%20approach%20to%20larger-scale%20tasks%2C%20we%20observe%0Aconsistent%20performance%20gains%20across%20domains.%20We%20achieve%20improvements%20in%0Avision-language%20tasks%20and%20especially%20notable%20increases%20in%20image%20generation%2C%0Awith%20CMMD%20scores%20improving%20by%20up%20to%2015%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOP-LoRA%253A%2520The%2520Blessing%2520of%2520Dimensionality%26entry.906535625%3DPiotr%2520Teterwak%2520and%2520Kate%2520Saenko%2520and%2520Bryan%2520A.%2520Plummer%2520and%2520Ser-Nam%2520Lim%26entry.1292438233%3D%2520%2520Low-rank%2520adapters%2520enable%2520fine-tuning%2520of%2520large%2520models%2520with%2520only%2520a%2520small%2520number%250Aof%2520parameters%252C%2520thus%2520reducing%2520storage%2520costs%2520and%2520minimizing%2520the%2520risk%2520of%250Acatastrophic%2520forgetting.%2520However%252C%2520they%2520often%2520pose%2520optimization%2520challenges%252C%2520with%250Apoor%2520convergence.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%2520an%250Aover-parameterized%2520approach%2520that%2520accelerates%2520training%2520without%2520increasing%250Ainference%2520costs.%2520This%2520method%2520reparameterizes%2520low-rank%2520adaptation%2520by%2520employing%2520a%250Aseparate%2520MLP%2520and%2520learned%2520embedding%2520for%2520each%2520layer.%2520The%2520learned%2520embedding%2520is%250Ainput%2520to%2520the%2520MLP%252C%2520which%2520generates%2520the%2520adapter%2520parameters.%2520Such%250Aoverparamaterization%2520has%2520been%2520shown%2520to%2520implicitly%2520function%2520as%2520an%2520adaptive%250Alearning%2520rate%2520and%2520momentum%252C%2520accelerating%2520optimization.%2520At%2520inference%2520time%252C%2520the%250AMLP%2520can%2520be%2520discarded%252C%2520leaving%2520behind%2520a%2520standard%2520low-rank%2520adapter.%2520To%2520study%2520the%250Aeffect%2520of%2520MLP%2520overparameterization%2520on%2520a%2520small%2520yet%2520difficult%2520proxy%2520task%252C%2520we%250Aimplement%2520it%2520for%2520matrix%2520factorization%252C%2520and%2520find%2520it%2520achieves%2520faster%2520convergence%250Aand%2520lower%2520final%2520loss.%2520Extending%2520this%2520approach%2520to%2520larger-scale%2520tasks%252C%2520we%2520observe%250Aconsistent%2520performance%2520gains%2520across%2520domains.%2520We%2520achieve%2520improvements%2520in%250Avision-language%2520tasks%2520and%2520especially%2520notable%2520increases%2520in%2520image%2520generation%252C%250Awith%2520CMMD%2520scores%2520improving%2520by%2520up%2520to%252015%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OP-LoRA%3A%20The%20Blessing%20of%20Dimensionality&entry.906535625=Piotr%20Teterwak%20and%20Kate%20Saenko%20and%20Bryan%20A.%20Plummer%20and%20Ser-Nam%20Lim&entry.1292438233=%20%20Low-rank%20adapters%20enable%20fine-tuning%20of%20large%20models%20with%20only%20a%20small%20number%0Aof%20parameters%2C%20thus%20reducing%20storage%20costs%20and%20minimizing%20the%20risk%20of%0Acatastrophic%20forgetting.%20However%2C%20they%20often%20pose%20optimization%20challenges%2C%20with%0Apoor%20convergence.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20an%0Aover-parameterized%20approach%20that%20accelerates%20training%20without%20increasing%0Ainference%20costs.%20This%20method%20reparameterizes%20low-rank%20adaptation%20by%20employing%20a%0Aseparate%20MLP%20and%20learned%20embedding%20for%20each%20layer.%20The%20learned%20embedding%20is%0Ainput%20to%20the%20MLP%2C%20which%20generates%20the%20adapter%20parameters.%20Such%0Aoverparamaterization%20has%20been%20shown%20to%20implicitly%20function%20as%20an%20adaptive%0Alearning%20rate%20and%20momentum%2C%20accelerating%20optimization.%20At%20inference%20time%2C%20the%0AMLP%20can%20be%20discarded%2C%20leaving%20behind%20a%20standard%20low-rank%20adapter.%20To%20study%20the%0Aeffect%20of%20MLP%20overparameterization%20on%20a%20small%20yet%20difficult%20proxy%20task%2C%20we%0Aimplement%20it%20for%20matrix%20factorization%2C%20and%20find%20it%20achieves%20faster%20convergence%0Aand%20lower%20final%20loss.%20Extending%20this%20approach%20to%20larger-scale%20tasks%2C%20we%20observe%0Aconsistent%20performance%20gains%20across%20domains.%20We%20achieve%20improvements%20in%0Avision-language%20tasks%20and%20especially%20notable%20increases%20in%20image%20generation%2C%0Awith%20CMMD%20scores%20improving%20by%20up%20to%2015%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10362v1&entry.124074799=Read"},
{"title": "Label-template based Few-Shot Text Classification with Contrastive\n  Learning", "author": "Guanghua Hou and Shuhui Cao and Deqiang Ouyang and Ning Wang", "abstract": "  As an algorithmic framework for learning to learn, meta-learning provides a\npromising solution for few-shot text classification. However, most existing\nresearch fail to give enough attention to class labels. Traditional basic\nframework building meta-learner based on prototype networks heavily relies on\ninter-class variance, and it is easily influenced by noise. To address these\nlimitations, we proposes a simple and effective few-shot text classification\nframework. In particular, the corresponding label templates are embed into\ninput sentences to fully utilize the potential value of class labels, guiding\nthe pre-trained model to generate more discriminative text representations\nthrough the semantic information conveyed by labels. With the continuous\ninfluence of label semantics, supervised contrastive learning is utilized to\nmodel the interaction information between support samples and query samples.\nFurthermore, the averaging mechanism is replaced with an attention mechanism to\nhighlight vital semantic information. To verify the proposed scheme, four\ntypical datasets are employed to assess the performance of different methods.\nExperimental results demonstrate that our method achieves substantial\nperformance enhancements and outperforms existing state-of-the-art models on\nfew-shot text classification tasks.\n", "link": "http://arxiv.org/abs/2412.10110v1", "date": "2024-12-13", "relevancy": 2.637, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5472}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5353}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label-template%20based%20Few-Shot%20Text%20Classification%20with%20Contrastive%0A%20%20Learning&body=Title%3A%20Label-template%20based%20Few-Shot%20Text%20Classification%20with%20Contrastive%0A%20%20Learning%0AAuthor%3A%20Guanghua%20Hou%20and%20Shuhui%20Cao%20and%20Deqiang%20Ouyang%20and%20Ning%20Wang%0AAbstract%3A%20%20%20As%20an%20algorithmic%20framework%20for%20learning%20to%20learn%2C%20meta-learning%20provides%20a%0Apromising%20solution%20for%20few-shot%20text%20classification.%20However%2C%20most%20existing%0Aresearch%20fail%20to%20give%20enough%20attention%20to%20class%20labels.%20Traditional%20basic%0Aframework%20building%20meta-learner%20based%20on%20prototype%20networks%20heavily%20relies%20on%0Ainter-class%20variance%2C%20and%20it%20is%20easily%20influenced%20by%20noise.%20To%20address%20these%0Alimitations%2C%20we%20proposes%20a%20simple%20and%20effective%20few-shot%20text%20classification%0Aframework.%20In%20particular%2C%20the%20corresponding%20label%20templates%20are%20embed%20into%0Ainput%20sentences%20to%20fully%20utilize%20the%20potential%20value%20of%20class%20labels%2C%20guiding%0Athe%20pre-trained%20model%20to%20generate%20more%20discriminative%20text%20representations%0Athrough%20the%20semantic%20information%20conveyed%20by%20labels.%20With%20the%20continuous%0Ainfluence%20of%20label%20semantics%2C%20supervised%20contrastive%20learning%20is%20utilized%20to%0Amodel%20the%20interaction%20information%20between%20support%20samples%20and%20query%20samples.%0AFurthermore%2C%20the%20averaging%20mechanism%20is%20replaced%20with%20an%20attention%20mechanism%20to%0Ahighlight%20vital%20semantic%20information.%20To%20verify%20the%20proposed%20scheme%2C%20four%0Atypical%20datasets%20are%20employed%20to%20assess%20the%20performance%20of%20different%20methods.%0AExperimental%20results%20demonstrate%20that%20our%20method%20achieves%20substantial%0Aperformance%20enhancements%20and%20outperforms%20existing%20state-of-the-art%20models%20on%0Afew-shot%20text%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel-template%2520based%2520Few-Shot%2520Text%2520Classification%2520with%2520Contrastive%250A%2520%2520Learning%26entry.906535625%3DGuanghua%2520Hou%2520and%2520Shuhui%2520Cao%2520and%2520Deqiang%2520Ouyang%2520and%2520Ning%2520Wang%26entry.1292438233%3D%2520%2520As%2520an%2520algorithmic%2520framework%2520for%2520learning%2520to%2520learn%252C%2520meta-learning%2520provides%2520a%250Apromising%2520solution%2520for%2520few-shot%2520text%2520classification.%2520However%252C%2520most%2520existing%250Aresearch%2520fail%2520to%2520give%2520enough%2520attention%2520to%2520class%2520labels.%2520Traditional%2520basic%250Aframework%2520building%2520meta-learner%2520based%2520on%2520prototype%2520networks%2520heavily%2520relies%2520on%250Ainter-class%2520variance%252C%2520and%2520it%2520is%2520easily%2520influenced%2520by%2520noise.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520proposes%2520a%2520simple%2520and%2520effective%2520few-shot%2520text%2520classification%250Aframework.%2520In%2520particular%252C%2520the%2520corresponding%2520label%2520templates%2520are%2520embed%2520into%250Ainput%2520sentences%2520to%2520fully%2520utilize%2520the%2520potential%2520value%2520of%2520class%2520labels%252C%2520guiding%250Athe%2520pre-trained%2520model%2520to%2520generate%2520more%2520discriminative%2520text%2520representations%250Athrough%2520the%2520semantic%2520information%2520conveyed%2520by%2520labels.%2520With%2520the%2520continuous%250Ainfluence%2520of%2520label%2520semantics%252C%2520supervised%2520contrastive%2520learning%2520is%2520utilized%2520to%250Amodel%2520the%2520interaction%2520information%2520between%2520support%2520samples%2520and%2520query%2520samples.%250AFurthermore%252C%2520the%2520averaging%2520mechanism%2520is%2520replaced%2520with%2520an%2520attention%2520mechanism%2520to%250Ahighlight%2520vital%2520semantic%2520information.%2520To%2520verify%2520the%2520proposed%2520scheme%252C%2520four%250Atypical%2520datasets%2520are%2520employed%2520to%2520assess%2520the%2520performance%2520of%2520different%2520methods.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520achieves%2520substantial%250Aperformance%2520enhancements%2520and%2520outperforms%2520existing%2520state-of-the-art%2520models%2520on%250Afew-shot%2520text%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label-template%20based%20Few-Shot%20Text%20Classification%20with%20Contrastive%0A%20%20Learning&entry.906535625=Guanghua%20Hou%20and%20Shuhui%20Cao%20and%20Deqiang%20Ouyang%20and%20Ning%20Wang&entry.1292438233=%20%20As%20an%20algorithmic%20framework%20for%20learning%20to%20learn%2C%20meta-learning%20provides%20a%0Apromising%20solution%20for%20few-shot%20text%20classification.%20However%2C%20most%20existing%0Aresearch%20fail%20to%20give%20enough%20attention%20to%20class%20labels.%20Traditional%20basic%0Aframework%20building%20meta-learner%20based%20on%20prototype%20networks%20heavily%20relies%20on%0Ainter-class%20variance%2C%20and%20it%20is%20easily%20influenced%20by%20noise.%20To%20address%20these%0Alimitations%2C%20we%20proposes%20a%20simple%20and%20effective%20few-shot%20text%20classification%0Aframework.%20In%20particular%2C%20the%20corresponding%20label%20templates%20are%20embed%20into%0Ainput%20sentences%20to%20fully%20utilize%20the%20potential%20value%20of%20class%20labels%2C%20guiding%0Athe%20pre-trained%20model%20to%20generate%20more%20discriminative%20text%20representations%0Athrough%20the%20semantic%20information%20conveyed%20by%20labels.%20With%20the%20continuous%0Ainfluence%20of%20label%20semantics%2C%20supervised%20contrastive%20learning%20is%20utilized%20to%0Amodel%20the%20interaction%20information%20between%20support%20samples%20and%20query%20samples.%0AFurthermore%2C%20the%20averaging%20mechanism%20is%20replaced%20with%20an%20attention%20mechanism%20to%0Ahighlight%20vital%20semantic%20information.%20To%20verify%20the%20proposed%20scheme%2C%20four%0Atypical%20datasets%20are%20employed%20to%20assess%20the%20performance%20of%20different%20methods.%0AExperimental%20results%20demonstrate%20that%20our%20method%20achieves%20substantial%0Aperformance%20enhancements%20and%20outperforms%20existing%20state-of-the-art%20models%20on%0Afew-shot%20text%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10110v1&entry.124074799=Read"},
{"title": "HiTZ at VarDial 2025 NorSID: Overcoming Data Scarcity with Language\n  Transfer and Automatic Data Annotation", "author": "Jaione Bengoetxea and Mikel Zubillaga and Ekhi Azurmendi and Maite Heredia and Julen Etxaniz and Markel Ferro and Jeremy Barnes", "abstract": "  In this paper we present our submission for the NorSID Shared Task as part of\nthe 2025 VarDial Workshop (Scherrer et al., 2025), consisting of three tasks:\nIntent Detection, Slot Filling and Dialect Identification, evaluated using data\nin different dialects of the Norwegian language. For Intent Detection and Slot\nFilling, we have fine-tuned a multitask model in a cross-lingual setting, to\nleverage the xSID dataset available in 17 languages. In the case of Dialect\nIdentification, our final submission consists of a model fine-tuned on the\nprovided development set, which has obtained the highest scores within our\nexperiments. Our final results on the test set show that our models do not drop\nin performance compared to the development set, likely due to the\ndomain-specificity of the dataset and the similar distribution of both subsets.\nFinally, we also report an in-depth analysis of the provided datasets and their\nartifacts, as well as other sets of experiments that have been carried out but\ndid not yield the best results. Additionally, we present an analysis on the\nreasons why some methods have been more successful than others; mainly the\nimpact of the combination of languages and domain-specificity of the training\ndata on the results.\n", "link": "http://arxiv.org/abs/2412.10095v1", "date": "2024-12-13", "relevancy": 2.5811, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5222}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5222}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiTZ%20at%20VarDial%202025%20NorSID%3A%20Overcoming%20Data%20Scarcity%20with%20Language%0A%20%20Transfer%20and%20Automatic%20Data%20Annotation&body=Title%3A%20HiTZ%20at%20VarDial%202025%20NorSID%3A%20Overcoming%20Data%20Scarcity%20with%20Language%0A%20%20Transfer%20and%20Automatic%20Data%20Annotation%0AAuthor%3A%20Jaione%20Bengoetxea%20and%20Mikel%20Zubillaga%20and%20Ekhi%20Azurmendi%20and%20Maite%20Heredia%20and%20Julen%20Etxaniz%20and%20Markel%20Ferro%20and%20Jeremy%20Barnes%0AAbstract%3A%20%20%20In%20this%20paper%20we%20present%20our%20submission%20for%20the%20NorSID%20Shared%20Task%20as%20part%20of%0Athe%202025%20VarDial%20Workshop%20%28Scherrer%20et%20al.%2C%202025%29%2C%20consisting%20of%20three%20tasks%3A%0AIntent%20Detection%2C%20Slot%20Filling%20and%20Dialect%20Identification%2C%20evaluated%20using%20data%0Ain%20different%20dialects%20of%20the%20Norwegian%20language.%20For%20Intent%20Detection%20and%20Slot%0AFilling%2C%20we%20have%20fine-tuned%20a%20multitask%20model%20in%20a%20cross-lingual%20setting%2C%20to%0Aleverage%20the%20xSID%20dataset%20available%20in%2017%20languages.%20In%20the%20case%20of%20Dialect%0AIdentification%2C%20our%20final%20submission%20consists%20of%20a%20model%20fine-tuned%20on%20the%0Aprovided%20development%20set%2C%20which%20has%20obtained%20the%20highest%20scores%20within%20our%0Aexperiments.%20Our%20final%20results%20on%20the%20test%20set%20show%20that%20our%20models%20do%20not%20drop%0Ain%20performance%20compared%20to%20the%20development%20set%2C%20likely%20due%20to%20the%0Adomain-specificity%20of%20the%20dataset%20and%20the%20similar%20distribution%20of%20both%20subsets.%0AFinally%2C%20we%20also%20report%20an%20in-depth%20analysis%20of%20the%20provided%20datasets%20and%20their%0Aartifacts%2C%20as%20well%20as%20other%20sets%20of%20experiments%20that%20have%20been%20carried%20out%20but%0Adid%20not%20yield%20the%20best%20results.%20Additionally%2C%20we%20present%20an%20analysis%20on%20the%0Areasons%20why%20some%20methods%20have%20been%20more%20successful%20than%20others%3B%20mainly%20the%0Aimpact%20of%20the%20combination%20of%20languages%20and%20domain-specificity%20of%20the%20training%0Adata%20on%20the%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiTZ%2520at%2520VarDial%25202025%2520NorSID%253A%2520Overcoming%2520Data%2520Scarcity%2520with%2520Language%250A%2520%2520Transfer%2520and%2520Automatic%2520Data%2520Annotation%26entry.906535625%3DJaione%2520Bengoetxea%2520and%2520Mikel%2520Zubillaga%2520and%2520Ekhi%2520Azurmendi%2520and%2520Maite%2520Heredia%2520and%2520Julen%2520Etxaniz%2520and%2520Markel%2520Ferro%2520and%2520Jeremy%2520Barnes%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520present%2520our%2520submission%2520for%2520the%2520NorSID%2520Shared%2520Task%2520as%2520part%2520of%250Athe%25202025%2520VarDial%2520Workshop%2520%2528Scherrer%2520et%2520al.%252C%25202025%2529%252C%2520consisting%2520of%2520three%2520tasks%253A%250AIntent%2520Detection%252C%2520Slot%2520Filling%2520and%2520Dialect%2520Identification%252C%2520evaluated%2520using%2520data%250Ain%2520different%2520dialects%2520of%2520the%2520Norwegian%2520language.%2520For%2520Intent%2520Detection%2520and%2520Slot%250AFilling%252C%2520we%2520have%2520fine-tuned%2520a%2520multitask%2520model%2520in%2520a%2520cross-lingual%2520setting%252C%2520to%250Aleverage%2520the%2520xSID%2520dataset%2520available%2520in%252017%2520languages.%2520In%2520the%2520case%2520of%2520Dialect%250AIdentification%252C%2520our%2520final%2520submission%2520consists%2520of%2520a%2520model%2520fine-tuned%2520on%2520the%250Aprovided%2520development%2520set%252C%2520which%2520has%2520obtained%2520the%2520highest%2520scores%2520within%2520our%250Aexperiments.%2520Our%2520final%2520results%2520on%2520the%2520test%2520set%2520show%2520that%2520our%2520models%2520do%2520not%2520drop%250Ain%2520performance%2520compared%2520to%2520the%2520development%2520set%252C%2520likely%2520due%2520to%2520the%250Adomain-specificity%2520of%2520the%2520dataset%2520and%2520the%2520similar%2520distribution%2520of%2520both%2520subsets.%250AFinally%252C%2520we%2520also%2520report%2520an%2520in-depth%2520analysis%2520of%2520the%2520provided%2520datasets%2520and%2520their%250Aartifacts%252C%2520as%2520well%2520as%2520other%2520sets%2520of%2520experiments%2520that%2520have%2520been%2520carried%2520out%2520but%250Adid%2520not%2520yield%2520the%2520best%2520results.%2520Additionally%252C%2520we%2520present%2520an%2520analysis%2520on%2520the%250Areasons%2520why%2520some%2520methods%2520have%2520been%2520more%2520successful%2520than%2520others%253B%2520mainly%2520the%250Aimpact%2520of%2520the%2520combination%2520of%2520languages%2520and%2520domain-specificity%2520of%2520the%2520training%250Adata%2520on%2520the%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiTZ%20at%20VarDial%202025%20NorSID%3A%20Overcoming%20Data%20Scarcity%20with%20Language%0A%20%20Transfer%20and%20Automatic%20Data%20Annotation&entry.906535625=Jaione%20Bengoetxea%20and%20Mikel%20Zubillaga%20and%20Ekhi%20Azurmendi%20and%20Maite%20Heredia%20and%20Julen%20Etxaniz%20and%20Markel%20Ferro%20and%20Jeremy%20Barnes&entry.1292438233=%20%20In%20this%20paper%20we%20present%20our%20submission%20for%20the%20NorSID%20Shared%20Task%20as%20part%20of%0Athe%202025%20VarDial%20Workshop%20%28Scherrer%20et%20al.%2C%202025%29%2C%20consisting%20of%20three%20tasks%3A%0AIntent%20Detection%2C%20Slot%20Filling%20and%20Dialect%20Identification%2C%20evaluated%20using%20data%0Ain%20different%20dialects%20of%20the%20Norwegian%20language.%20For%20Intent%20Detection%20and%20Slot%0AFilling%2C%20we%20have%20fine-tuned%20a%20multitask%20model%20in%20a%20cross-lingual%20setting%2C%20to%0Aleverage%20the%20xSID%20dataset%20available%20in%2017%20languages.%20In%20the%20case%20of%20Dialect%0AIdentification%2C%20our%20final%20submission%20consists%20of%20a%20model%20fine-tuned%20on%20the%0Aprovided%20development%20set%2C%20which%20has%20obtained%20the%20highest%20scores%20within%20our%0Aexperiments.%20Our%20final%20results%20on%20the%20test%20set%20show%20that%20our%20models%20do%20not%20drop%0Ain%20performance%20compared%20to%20the%20development%20set%2C%20likely%20due%20to%20the%0Adomain-specificity%20of%20the%20dataset%20and%20the%20similar%20distribution%20of%20both%20subsets.%0AFinally%2C%20we%20also%20report%20an%20in-depth%20analysis%20of%20the%20provided%20datasets%20and%20their%0Aartifacts%2C%20as%20well%20as%20other%20sets%20of%20experiments%20that%20have%20been%20carried%20out%20but%0Adid%20not%20yield%20the%20best%20results.%20Additionally%2C%20we%20present%20an%20analysis%20on%20the%0Areasons%20why%20some%20methods%20have%20been%20more%20successful%20than%20others%3B%20mainly%20the%0Aimpact%20of%20the%20combination%20of%20languages%20and%20domain-specificity%20of%20the%20training%0Adata%20on%20the%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10095v1&entry.124074799=Read"},
{"title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector", "author": "Weixiang Zhang and Shuzhao Xie and Chengwei Ren and Siyi Xie and Chen Tang and Shijia Ge and Mingzi Wang and Zhi Wang", "abstract": "  We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.\n", "link": "http://arxiv.org/abs/2412.10153v1", "date": "2024-12-13", "relevancy": 2.5805, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5512}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5165}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVOS%3A%20Efficient%20Implicit%20Neural%20Training%20via%20EVOlutionary%20Selector&body=Title%3A%20EVOS%3A%20Efficient%20Implicit%20Neural%20Training%20via%20EVOlutionary%20Selector%0AAuthor%3A%20Weixiang%20Zhang%20and%20Shuzhao%20Xie%20and%20Chengwei%20Ren%20and%20Siyi%20Xie%20and%20Chen%20Tang%20and%20Shijia%20Ge%20and%20Mingzi%20Wang%20and%20Zhi%20Wang%0AAbstract%3A%20%20%20We%20propose%20EVOlutionary%20Selector%20%28EVOS%29%2C%20an%20efficient%20training%20paradigm%20for%0Aaccelerating%20Implicit%20Neural%20Representation%20%28INR%29.%20Unlike%20conventional%20INR%0Atraining%20that%20feeds%20all%20samples%20through%20the%20neural%20network%20in%20each%20iteration%2C%0Aour%20approach%20restricts%20training%20to%20strategically%20selected%20points%2C%20reducing%0Acomputational%20overhead%20by%20eliminating%20redundant%20forward%20passes.%20Specifically%2C%0Awe%20treat%20each%20sample%20as%20an%20individual%20in%20an%20evolutionary%20process%2C%20where%20only%0Athose%20fittest%20ones%20survive%20and%20merit%20inclusion%20in%20training%2C%20adaptively%20evolving%0Awith%20the%20neural%20network%20dynamics.%20While%20this%20is%20conceptually%20similar%20to%0AEvolutionary%20Algorithms%2C%20their%20distinct%20objectives%20%28selection%20for%20acceleration%0Avs.%20iterative%20solution%20optimization%29%20require%20a%20fundamental%20redefinition%20of%0Aevolutionary%20mechanisms%20for%20our%20context.%20In%20response%2C%20we%20design%20sparse%20fitness%0Aevaluation%2C%20frequency-guided%20crossover%2C%20and%20augmented%20unbiased%20mutation%20to%0Acomprise%20EVOS.%20These%20components%20respectively%20guide%20sample%20selection%20with%0Areduced%20computational%20cost%2C%20enhance%20performance%20through%20frequency-domain%0Abalance%2C%20and%20mitigate%20selection%20bias%20from%20cached%20evaluation.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20approximately%2048%25-66%25%0Areduction%20in%20training%20time%20while%20ensuring%20superior%20convergence%20without%0Aadditional%20cost%2C%20establishing%20state-of-the-art%20acceleration%20among%20recent%0Asampling-based%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVOS%253A%2520Efficient%2520Implicit%2520Neural%2520Training%2520via%2520EVOlutionary%2520Selector%26entry.906535625%3DWeixiang%2520Zhang%2520and%2520Shuzhao%2520Xie%2520and%2520Chengwei%2520Ren%2520and%2520Siyi%2520Xie%2520and%2520Chen%2520Tang%2520and%2520Shijia%2520Ge%2520and%2520Mingzi%2520Wang%2520and%2520Zhi%2520Wang%26entry.1292438233%3D%2520%2520We%2520propose%2520EVOlutionary%2520Selector%2520%2528EVOS%2529%252C%2520an%2520efficient%2520training%2520paradigm%2520for%250Aaccelerating%2520Implicit%2520Neural%2520Representation%2520%2528INR%2529.%2520Unlike%2520conventional%2520INR%250Atraining%2520that%2520feeds%2520all%2520samples%2520through%2520the%2520neural%2520network%2520in%2520each%2520iteration%252C%250Aour%2520approach%2520restricts%2520training%2520to%2520strategically%2520selected%2520points%252C%2520reducing%250Acomputational%2520overhead%2520by%2520eliminating%2520redundant%2520forward%2520passes.%2520Specifically%252C%250Awe%2520treat%2520each%2520sample%2520as%2520an%2520individual%2520in%2520an%2520evolutionary%2520process%252C%2520where%2520only%250Athose%2520fittest%2520ones%2520survive%2520and%2520merit%2520inclusion%2520in%2520training%252C%2520adaptively%2520evolving%250Awith%2520the%2520neural%2520network%2520dynamics.%2520While%2520this%2520is%2520conceptually%2520similar%2520to%250AEvolutionary%2520Algorithms%252C%2520their%2520distinct%2520objectives%2520%2528selection%2520for%2520acceleration%250Avs.%2520iterative%2520solution%2520optimization%2529%2520require%2520a%2520fundamental%2520redefinition%2520of%250Aevolutionary%2520mechanisms%2520for%2520our%2520context.%2520In%2520response%252C%2520we%2520design%2520sparse%2520fitness%250Aevaluation%252C%2520frequency-guided%2520crossover%252C%2520and%2520augmented%2520unbiased%2520mutation%2520to%250Acomprise%2520EVOS.%2520These%2520components%2520respectively%2520guide%2520sample%2520selection%2520with%250Areduced%2520computational%2520cost%252C%2520enhance%2520performance%2520through%2520frequency-domain%250Abalance%252C%2520and%2520mitigate%2520selection%2520bias%2520from%2520cached%2520evaluation.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520approximately%252048%2525-66%2525%250Areduction%2520in%2520training%2520time%2520while%2520ensuring%2520superior%2520convergence%2520without%250Aadditional%2520cost%252C%2520establishing%2520state-of-the-art%2520acceleration%2520among%2520recent%250Asampling-based%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVOS%3A%20Efficient%20Implicit%20Neural%20Training%20via%20EVOlutionary%20Selector&entry.906535625=Weixiang%20Zhang%20and%20Shuzhao%20Xie%20and%20Chengwei%20Ren%20and%20Siyi%20Xie%20and%20Chen%20Tang%20and%20Shijia%20Ge%20and%20Mingzi%20Wang%20and%20Zhi%20Wang&entry.1292438233=%20%20We%20propose%20EVOlutionary%20Selector%20%28EVOS%29%2C%20an%20efficient%20training%20paradigm%20for%0Aaccelerating%20Implicit%20Neural%20Representation%20%28INR%29.%20Unlike%20conventional%20INR%0Atraining%20that%20feeds%20all%20samples%20through%20the%20neural%20network%20in%20each%20iteration%2C%0Aour%20approach%20restricts%20training%20to%20strategically%20selected%20points%2C%20reducing%0Acomputational%20overhead%20by%20eliminating%20redundant%20forward%20passes.%20Specifically%2C%0Awe%20treat%20each%20sample%20as%20an%20individual%20in%20an%20evolutionary%20process%2C%20where%20only%0Athose%20fittest%20ones%20survive%20and%20merit%20inclusion%20in%20training%2C%20adaptively%20evolving%0Awith%20the%20neural%20network%20dynamics.%20While%20this%20is%20conceptually%20similar%20to%0AEvolutionary%20Algorithms%2C%20their%20distinct%20objectives%20%28selection%20for%20acceleration%0Avs.%20iterative%20solution%20optimization%29%20require%20a%20fundamental%20redefinition%20of%0Aevolutionary%20mechanisms%20for%20our%20context.%20In%20response%2C%20we%20design%20sparse%20fitness%0Aevaluation%2C%20frequency-guided%20crossover%2C%20and%20augmented%20unbiased%20mutation%20to%0Acomprise%20EVOS.%20These%20components%20respectively%20guide%20sample%20selection%20with%0Areduced%20computational%20cost%2C%20enhance%20performance%20through%20frequency-domain%0Abalance%2C%20and%20mitigate%20selection%20bias%20from%20cached%20evaluation.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20approximately%2048%25-66%25%0Areduction%20in%20training%20time%20while%20ensuring%20superior%20convergence%20without%0Aadditional%20cost%2C%20establishing%20state-of-the-art%20acceleration%20among%20recent%0Asampling-based%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10153v1&entry.124074799=Read"},
{"title": "Clustering Algorithms and RAG Enhancing Semi-Supervised Text\n  Classification with Large LLMs", "author": "Shan Zhong and Jiahao Zeng and Yongxin Yu and Bohong Lin", "abstract": "  This paper introduces a novel semi-supervised learning framework specifically\ndesigned for text classification tasks, effectively addressing the challenge of\nvast datasets with limited labeled examples. By integrating multi-level\nsimilarity based data augmentation techniques from Retrieval-Augmented\nGeneration (RAG) to Large Language Model (LLM) rewriting and traditional word\nsubstitution-we constructed an intelligent augmentation pipeline. This\nframework innovatively employs the selection of representative landmarks\nthrough clustering, which serve as intermediaries in the retrieval and\nrewriting processes, ensuring that the augmented data maintains a distribution\nsimilar to the original dataset. Empirical results show that even in complex\ntext document classification scenarios with over 100 categories, our method\nachieves state-of-the-art accuracies of 95.41% and 82.43% on the Reuters and\nWeb of Science datasets, respectively. These findings highlight the\neffectiveness and broad applicability of our semi-supervised learning approach\nfor text classification tasks.\n", "link": "http://arxiv.org/abs/2411.06175v2", "date": "2024-12-13", "relevancy": 2.5383, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5234}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.52}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clustering%20Algorithms%20and%20RAG%20Enhancing%20Semi-Supervised%20Text%0A%20%20Classification%20with%20Large%20LLMs&body=Title%3A%20Clustering%20Algorithms%20and%20RAG%20Enhancing%20Semi-Supervised%20Text%0A%20%20Classification%20with%20Large%20LLMs%0AAuthor%3A%20Shan%20Zhong%20and%20Jiahao%20Zeng%20and%20Yongxin%20Yu%20and%20Bohong%20Lin%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20semi-supervised%20learning%20framework%20specifically%0Adesigned%20for%20text%20classification%20tasks%2C%20effectively%20addressing%20the%20challenge%20of%0Avast%20datasets%20with%20limited%20labeled%20examples.%20By%20integrating%20multi-level%0Asimilarity%20based%20data%20augmentation%20techniques%20from%20Retrieval-Augmented%0AGeneration%20%28RAG%29%20to%20Large%20Language%20Model%20%28LLM%29%20rewriting%20and%20traditional%20word%0Asubstitution-we%20constructed%20an%20intelligent%20augmentation%20pipeline.%20This%0Aframework%20innovatively%20employs%20the%20selection%20of%20representative%20landmarks%0Athrough%20clustering%2C%20which%20serve%20as%20intermediaries%20in%20the%20retrieval%20and%0Arewriting%20processes%2C%20ensuring%20that%20the%20augmented%20data%20maintains%20a%20distribution%0Asimilar%20to%20the%20original%20dataset.%20Empirical%20results%20show%20that%20even%20in%20complex%0Atext%20document%20classification%20scenarios%20with%20over%20100%20categories%2C%20our%20method%0Aachieves%20state-of-the-art%20accuracies%20of%2095.41%25%20and%2082.43%25%20on%20the%20Reuters%20and%0AWeb%20of%20Science%20datasets%2C%20respectively.%20These%20findings%20highlight%20the%0Aeffectiveness%20and%20broad%20applicability%20of%20our%20semi-supervised%20learning%20approach%0Afor%20text%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06175v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClustering%2520Algorithms%2520and%2520RAG%2520Enhancing%2520Semi-Supervised%2520Text%250A%2520%2520Classification%2520with%2520Large%2520LLMs%26entry.906535625%3DShan%2520Zhong%2520and%2520Jiahao%2520Zeng%2520and%2520Yongxin%2520Yu%2520and%2520Bohong%2520Lin%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520semi-supervised%2520learning%2520framework%2520specifically%250Adesigned%2520for%2520text%2520classification%2520tasks%252C%2520effectively%2520addressing%2520the%2520challenge%2520of%250Avast%2520datasets%2520with%2520limited%2520labeled%2520examples.%2520By%2520integrating%2520multi-level%250Asimilarity%2520based%2520data%2520augmentation%2520techniques%2520from%2520Retrieval-Augmented%250AGeneration%2520%2528RAG%2529%2520to%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520rewriting%2520and%2520traditional%2520word%250Asubstitution-we%2520constructed%2520an%2520intelligent%2520augmentation%2520pipeline.%2520This%250Aframework%2520innovatively%2520employs%2520the%2520selection%2520of%2520representative%2520landmarks%250Athrough%2520clustering%252C%2520which%2520serve%2520as%2520intermediaries%2520in%2520the%2520retrieval%2520and%250Arewriting%2520processes%252C%2520ensuring%2520that%2520the%2520augmented%2520data%2520maintains%2520a%2520distribution%250Asimilar%2520to%2520the%2520original%2520dataset.%2520Empirical%2520results%2520show%2520that%2520even%2520in%2520complex%250Atext%2520document%2520classification%2520scenarios%2520with%2520over%2520100%2520categories%252C%2520our%2520method%250Aachieves%2520state-of-the-art%2520accuracies%2520of%252095.41%2525%2520and%252082.43%2525%2520on%2520the%2520Reuters%2520and%250AWeb%2520of%2520Science%2520datasets%252C%2520respectively.%2520These%2520findings%2520highlight%2520the%250Aeffectiveness%2520and%2520broad%2520applicability%2520of%2520our%2520semi-supervised%2520learning%2520approach%250Afor%2520text%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06175v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clustering%20Algorithms%20and%20RAG%20Enhancing%20Semi-Supervised%20Text%0A%20%20Classification%20with%20Large%20LLMs&entry.906535625=Shan%20Zhong%20and%20Jiahao%20Zeng%20and%20Yongxin%20Yu%20and%20Bohong%20Lin&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20semi-supervised%20learning%20framework%20specifically%0Adesigned%20for%20text%20classification%20tasks%2C%20effectively%20addressing%20the%20challenge%20of%0Avast%20datasets%20with%20limited%20labeled%20examples.%20By%20integrating%20multi-level%0Asimilarity%20based%20data%20augmentation%20techniques%20from%20Retrieval-Augmented%0AGeneration%20%28RAG%29%20to%20Large%20Language%20Model%20%28LLM%29%20rewriting%20and%20traditional%20word%0Asubstitution-we%20constructed%20an%20intelligent%20augmentation%20pipeline.%20This%0Aframework%20innovatively%20employs%20the%20selection%20of%20representative%20landmarks%0Athrough%20clustering%2C%20which%20serve%20as%20intermediaries%20in%20the%20retrieval%20and%0Arewriting%20processes%2C%20ensuring%20that%20the%20augmented%20data%20maintains%20a%20distribution%0Asimilar%20to%20the%20original%20dataset.%20Empirical%20results%20show%20that%20even%20in%20complex%0Atext%20document%20classification%20scenarios%20with%20over%20100%20categories%2C%20our%20method%0Aachieves%20state-of-the-art%20accuracies%20of%2095.41%25%20and%2082.43%25%20on%20the%20Reuters%20and%0AWeb%20of%20Science%20datasets%2C%20respectively.%20These%20findings%20highlight%20the%0Aeffectiveness%20and%20broad%20applicability%20of%20our%20semi-supervised%20learning%20approach%0Afor%20text%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06175v2&entry.124074799=Read"},
{"title": "Efficient Generative Modeling with Residual Vector Quantization-Based\n  Tokens", "author": "Jaehyeon Kim and Taehong Moon and Keon Lee and Jaewoong Cho", "abstract": "  We explore the use of Residual Vector Quantization (RVQ) for high-fidelity\ngeneration in vector-quantized generative models. This quantization technique\nmaintains higher data fidelity by employing more in-depth tokens. However,\nincreasing the token number in generative models leads to slower inference\nspeeds. To this end, we introduce ResGen, an efficient RVQ-based discrete\ndiffusion model that generates high-fidelity samples without compromising\nsampling speed. Our key idea is a direct prediction of vector embedding of\ncollective tokens rather than individual ones. Moreover, we demonstrate that\nour proposed token masking and multi-token prediction method can be formulated\nwithin a principled probabilistic framework using a discrete diffusion process\nand variational inference. We validate the efficacy and generalizability of the\nproposed method on two challenging tasks across different modalities:\nconditional image generation} on ImageNet 256x256 and zero-shot text-to-speech\nsynthesis. Experimental results demonstrate that ResGen outperforms\nautoregressive counterparts in both tasks, delivering superior performance\nwithout compromising sampling speed. Furthermore, as we scale the depth of RVQ,\nour generative models exhibit enhanced generation fidelity or faster sampling\nspeeds compared to similarly sized baseline models. The project page can be\nfound at https://resgen-genai.github.io\n", "link": "http://arxiv.org/abs/2412.10208v1", "date": "2024-12-13", "relevancy": 2.5275, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6491}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6332}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Generative%20Modeling%20with%20Residual%20Vector%20Quantization-Based%0A%20%20Tokens&body=Title%3A%20Efficient%20Generative%20Modeling%20with%20Residual%20Vector%20Quantization-Based%0A%20%20Tokens%0AAuthor%3A%20Jaehyeon%20Kim%20and%20Taehong%20Moon%20and%20Keon%20Lee%20and%20Jaewoong%20Cho%0AAbstract%3A%20%20%20We%20explore%20the%20use%20of%20Residual%20Vector%20Quantization%20%28RVQ%29%20for%20high-fidelity%0Ageneration%20in%20vector-quantized%20generative%20models.%20This%20quantization%20technique%0Amaintains%20higher%20data%20fidelity%20by%20employing%20more%20in-depth%20tokens.%20However%2C%0Aincreasing%20the%20token%20number%20in%20generative%20models%20leads%20to%20slower%20inference%0Aspeeds.%20To%20this%20end%2C%20we%20introduce%20ResGen%2C%20an%20efficient%20RVQ-based%20discrete%0Adiffusion%20model%20that%20generates%20high-fidelity%20samples%20without%20compromising%0Asampling%20speed.%20Our%20key%20idea%20is%20a%20direct%20prediction%20of%20vector%20embedding%20of%0Acollective%20tokens%20rather%20than%20individual%20ones.%20Moreover%2C%20we%20demonstrate%20that%0Aour%20proposed%20token%20masking%20and%20multi-token%20prediction%20method%20can%20be%20formulated%0Awithin%20a%20principled%20probabilistic%20framework%20using%20a%20discrete%20diffusion%20process%0Aand%20variational%20inference.%20We%20validate%20the%20efficacy%20and%20generalizability%20of%20the%0Aproposed%20method%20on%20two%20challenging%20tasks%20across%20different%20modalities%3A%0Aconditional%20image%20generation%7D%20on%20ImageNet%20256x256%20and%20zero-shot%20text-to-speech%0Asynthesis.%20Experimental%20results%20demonstrate%20that%20ResGen%20outperforms%0Aautoregressive%20counterparts%20in%20both%20tasks%2C%20delivering%20superior%20performance%0Awithout%20compromising%20sampling%20speed.%20Furthermore%2C%20as%20we%20scale%20the%20depth%20of%20RVQ%2C%0Aour%20generative%20models%20exhibit%20enhanced%20generation%20fidelity%20or%20faster%20sampling%0Aspeeds%20compared%20to%20similarly%20sized%20baseline%20models.%20The%20project%20page%20can%20be%0Afound%20at%20https%3A//resgen-genai.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Generative%2520Modeling%2520with%2520Residual%2520Vector%2520Quantization-Based%250A%2520%2520Tokens%26entry.906535625%3DJaehyeon%2520Kim%2520and%2520Taehong%2520Moon%2520and%2520Keon%2520Lee%2520and%2520Jaewoong%2520Cho%26entry.1292438233%3D%2520%2520We%2520explore%2520the%2520use%2520of%2520Residual%2520Vector%2520Quantization%2520%2528RVQ%2529%2520for%2520high-fidelity%250Ageneration%2520in%2520vector-quantized%2520generative%2520models.%2520This%2520quantization%2520technique%250Amaintains%2520higher%2520data%2520fidelity%2520by%2520employing%2520more%2520in-depth%2520tokens.%2520However%252C%250Aincreasing%2520the%2520token%2520number%2520in%2520generative%2520models%2520leads%2520to%2520slower%2520inference%250Aspeeds.%2520To%2520this%2520end%252C%2520we%2520introduce%2520ResGen%252C%2520an%2520efficient%2520RVQ-based%2520discrete%250Adiffusion%2520model%2520that%2520generates%2520high-fidelity%2520samples%2520without%2520compromising%250Asampling%2520speed.%2520Our%2520key%2520idea%2520is%2520a%2520direct%2520prediction%2520of%2520vector%2520embedding%2520of%250Acollective%2520tokens%2520rather%2520than%2520individual%2520ones.%2520Moreover%252C%2520we%2520demonstrate%2520that%250Aour%2520proposed%2520token%2520masking%2520and%2520multi-token%2520prediction%2520method%2520can%2520be%2520formulated%250Awithin%2520a%2520principled%2520probabilistic%2520framework%2520using%2520a%2520discrete%2520diffusion%2520process%250Aand%2520variational%2520inference.%2520We%2520validate%2520the%2520efficacy%2520and%2520generalizability%2520of%2520the%250Aproposed%2520method%2520on%2520two%2520challenging%2520tasks%2520across%2520different%2520modalities%253A%250Aconditional%2520image%2520generation%257D%2520on%2520ImageNet%2520256x256%2520and%2520zero-shot%2520text-to-speech%250Asynthesis.%2520Experimental%2520results%2520demonstrate%2520that%2520ResGen%2520outperforms%250Aautoregressive%2520counterparts%2520in%2520both%2520tasks%252C%2520delivering%2520superior%2520performance%250Awithout%2520compromising%2520sampling%2520speed.%2520Furthermore%252C%2520as%2520we%2520scale%2520the%2520depth%2520of%2520RVQ%252C%250Aour%2520generative%2520models%2520exhibit%2520enhanced%2520generation%2520fidelity%2520or%2520faster%2520sampling%250Aspeeds%2520compared%2520to%2520similarly%2520sized%2520baseline%2520models.%2520The%2520project%2520page%2520can%2520be%250Afound%2520at%2520https%253A//resgen-genai.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Generative%20Modeling%20with%20Residual%20Vector%20Quantization-Based%0A%20%20Tokens&entry.906535625=Jaehyeon%20Kim%20and%20Taehong%20Moon%20and%20Keon%20Lee%20and%20Jaewoong%20Cho&entry.1292438233=%20%20We%20explore%20the%20use%20of%20Residual%20Vector%20Quantization%20%28RVQ%29%20for%20high-fidelity%0Ageneration%20in%20vector-quantized%20generative%20models.%20This%20quantization%20technique%0Amaintains%20higher%20data%20fidelity%20by%20employing%20more%20in-depth%20tokens.%20However%2C%0Aincreasing%20the%20token%20number%20in%20generative%20models%20leads%20to%20slower%20inference%0Aspeeds.%20To%20this%20end%2C%20we%20introduce%20ResGen%2C%20an%20efficient%20RVQ-based%20discrete%0Adiffusion%20model%20that%20generates%20high-fidelity%20samples%20without%20compromising%0Asampling%20speed.%20Our%20key%20idea%20is%20a%20direct%20prediction%20of%20vector%20embedding%20of%0Acollective%20tokens%20rather%20than%20individual%20ones.%20Moreover%2C%20we%20demonstrate%20that%0Aour%20proposed%20token%20masking%20and%20multi-token%20prediction%20method%20can%20be%20formulated%0Awithin%20a%20principled%20probabilistic%20framework%20using%20a%20discrete%20diffusion%20process%0Aand%20variational%20inference.%20We%20validate%20the%20efficacy%20and%20generalizability%20of%20the%0Aproposed%20method%20on%20two%20challenging%20tasks%20across%20different%20modalities%3A%0Aconditional%20image%20generation%7D%20on%20ImageNet%20256x256%20and%20zero-shot%20text-to-speech%0Asynthesis.%20Experimental%20results%20demonstrate%20that%20ResGen%20outperforms%0Aautoregressive%20counterparts%20in%20both%20tasks%2C%20delivering%20superior%20performance%0Awithout%20compromising%20sampling%20speed.%20Furthermore%2C%20as%20we%20scale%20the%20depth%20of%20RVQ%2C%0Aour%20generative%20models%20exhibit%20enhanced%20generation%20fidelity%20or%20faster%20sampling%0Aspeeds%20compared%20to%20similarly%20sized%20baseline%20models.%20The%20project%20page%20can%20be%0Afound%20at%20https%3A//resgen-genai.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10208v1&entry.124074799=Read"},
{"title": "WordVIS: A Color Worth A Thousand Words", "author": "Umar Khan and  Saifullah and Stefan Agne and Andreas Dengel and Sheraz Ahmed", "abstract": "  Document classification is considered a critical element in automated\ndocument processing systems. In recent years multi-modal approaches have become\nincreasingly popular for document classification. Despite their improvements,\nthese approaches are underutilized in the industry due to their requirement for\na tremendous volume of training data and extensive computational power. In this\npaper, we attempt to address these issues by embedding textual features\ndirectly into the visual space, allowing lightweight image-based classifiers to\nachieve state-of-the-art results using small-scale datasets in document\nclassification. To evaluate the efficacy of the visual features generated from\nour approach on limited data, we tested on the standard dataset Tobacco-3482.\nOur experiments show a tremendous improvement in image-based classifiers,\nachieving an improvement of 4.64% using ResNet50 with no document pre-training.\nIt also sets a new record for the best accuracy of the Tobacco-3482 dataset\nwith a score of 91.14% using the image-based DocXClassifier with no document\npre-training. The simplicity of the approach, its resource requirements, and\nsubsequent results provide a good prospect for its use in industrial use cases.\n", "link": "http://arxiv.org/abs/2412.10155v1", "date": "2024-12-13", "relevancy": 2.5178, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5001}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WordVIS%3A%20A%20Color%20Worth%20A%20Thousand%20Words&body=Title%3A%20WordVIS%3A%20A%20Color%20Worth%20A%20Thousand%20Words%0AAuthor%3A%20Umar%20Khan%20and%20%20Saifullah%20and%20Stefan%20Agne%20and%20Andreas%20Dengel%20and%20Sheraz%20Ahmed%0AAbstract%3A%20%20%20Document%20classification%20is%20considered%20a%20critical%20element%20in%20automated%0Adocument%20processing%20systems.%20In%20recent%20years%20multi-modal%20approaches%20have%20become%0Aincreasingly%20popular%20for%20document%20classification.%20Despite%20their%20improvements%2C%0Athese%20approaches%20are%20underutilized%20in%20the%20industry%20due%20to%20their%20requirement%20for%0Aa%20tremendous%20volume%20of%20training%20data%20and%20extensive%20computational%20power.%20In%20this%0Apaper%2C%20we%20attempt%20to%20address%20these%20issues%20by%20embedding%20textual%20features%0Adirectly%20into%20the%20visual%20space%2C%20allowing%20lightweight%20image-based%20classifiers%20to%0Aachieve%20state-of-the-art%20results%20using%20small-scale%20datasets%20in%20document%0Aclassification.%20To%20evaluate%20the%20efficacy%20of%20the%20visual%20features%20generated%20from%0Aour%20approach%20on%20limited%20data%2C%20we%20tested%20on%20the%20standard%20dataset%20Tobacco-3482.%0AOur%20experiments%20show%20a%20tremendous%20improvement%20in%20image-based%20classifiers%2C%0Aachieving%20an%20improvement%20of%204.64%25%20using%20ResNet50%20with%20no%20document%20pre-training.%0AIt%20also%20sets%20a%20new%20record%20for%20the%20best%20accuracy%20of%20the%20Tobacco-3482%20dataset%0Awith%20a%20score%20of%2091.14%25%20using%20the%20image-based%20DocXClassifier%20with%20no%20document%0Apre-training.%20The%20simplicity%20of%20the%20approach%2C%20its%20resource%20requirements%2C%20and%0Asubsequent%20results%20provide%20a%20good%20prospect%20for%20its%20use%20in%20industrial%20use%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWordVIS%253A%2520A%2520Color%2520Worth%2520A%2520Thousand%2520Words%26entry.906535625%3DUmar%2520Khan%2520and%2520%2520Saifullah%2520and%2520Stefan%2520Agne%2520and%2520Andreas%2520Dengel%2520and%2520Sheraz%2520Ahmed%26entry.1292438233%3D%2520%2520Document%2520classification%2520is%2520considered%2520a%2520critical%2520element%2520in%2520automated%250Adocument%2520processing%2520systems.%2520In%2520recent%2520years%2520multi-modal%2520approaches%2520have%2520become%250Aincreasingly%2520popular%2520for%2520document%2520classification.%2520Despite%2520their%2520improvements%252C%250Athese%2520approaches%2520are%2520underutilized%2520in%2520the%2520industry%2520due%2520to%2520their%2520requirement%2520for%250Aa%2520tremendous%2520volume%2520of%2520training%2520data%2520and%2520extensive%2520computational%2520power.%2520In%2520this%250Apaper%252C%2520we%2520attempt%2520to%2520address%2520these%2520issues%2520by%2520embedding%2520textual%2520features%250Adirectly%2520into%2520the%2520visual%2520space%252C%2520allowing%2520lightweight%2520image-based%2520classifiers%2520to%250Aachieve%2520state-of-the-art%2520results%2520using%2520small-scale%2520datasets%2520in%2520document%250Aclassification.%2520To%2520evaluate%2520the%2520efficacy%2520of%2520the%2520visual%2520features%2520generated%2520from%250Aour%2520approach%2520on%2520limited%2520data%252C%2520we%2520tested%2520on%2520the%2520standard%2520dataset%2520Tobacco-3482.%250AOur%2520experiments%2520show%2520a%2520tremendous%2520improvement%2520in%2520image-based%2520classifiers%252C%250Aachieving%2520an%2520improvement%2520of%25204.64%2525%2520using%2520ResNet50%2520with%2520no%2520document%2520pre-training.%250AIt%2520also%2520sets%2520a%2520new%2520record%2520for%2520the%2520best%2520accuracy%2520of%2520the%2520Tobacco-3482%2520dataset%250Awith%2520a%2520score%2520of%252091.14%2525%2520using%2520the%2520image-based%2520DocXClassifier%2520with%2520no%2520document%250Apre-training.%2520The%2520simplicity%2520of%2520the%2520approach%252C%2520its%2520resource%2520requirements%252C%2520and%250Asubsequent%2520results%2520provide%2520a%2520good%2520prospect%2520for%2520its%2520use%2520in%2520industrial%2520use%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WordVIS%3A%20A%20Color%20Worth%20A%20Thousand%20Words&entry.906535625=Umar%20Khan%20and%20%20Saifullah%20and%20Stefan%20Agne%20and%20Andreas%20Dengel%20and%20Sheraz%20Ahmed&entry.1292438233=%20%20Document%20classification%20is%20considered%20a%20critical%20element%20in%20automated%0Adocument%20processing%20systems.%20In%20recent%20years%20multi-modal%20approaches%20have%20become%0Aincreasingly%20popular%20for%20document%20classification.%20Despite%20their%20improvements%2C%0Athese%20approaches%20are%20underutilized%20in%20the%20industry%20due%20to%20their%20requirement%20for%0Aa%20tremendous%20volume%20of%20training%20data%20and%20extensive%20computational%20power.%20In%20this%0Apaper%2C%20we%20attempt%20to%20address%20these%20issues%20by%20embedding%20textual%20features%0Adirectly%20into%20the%20visual%20space%2C%20allowing%20lightweight%20image-based%20classifiers%20to%0Aachieve%20state-of-the-art%20results%20using%20small-scale%20datasets%20in%20document%0Aclassification.%20To%20evaluate%20the%20efficacy%20of%20the%20visual%20features%20generated%20from%0Aour%20approach%20on%20limited%20data%2C%20we%20tested%20on%20the%20standard%20dataset%20Tobacco-3482.%0AOur%20experiments%20show%20a%20tremendous%20improvement%20in%20image-based%20classifiers%2C%0Aachieving%20an%20improvement%20of%204.64%25%20using%20ResNet50%20with%20no%20document%20pre-training.%0AIt%20also%20sets%20a%20new%20record%20for%20the%20best%20accuracy%20of%20the%20Tobacco-3482%20dataset%0Awith%20a%20score%20of%2091.14%25%20using%20the%20image-based%20DocXClassifier%20with%20no%20document%0Apre-training.%20The%20simplicity%20of%20the%20approach%2C%20its%20resource%20requirements%2C%20and%0Asubsequent%20results%20provide%20a%20good%20prospect%20for%20its%20use%20in%20industrial%20use%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10155v1&entry.124074799=Read"},
{"title": "Towards the Characterization of Representations Learned via\n  Capsule-based Network Architectures", "author": "Saja Tawalbeh and Jos\u00e9 Oramas", "abstract": "  Capsule Networks (CapsNets) have been re-introduced as a more compact and\ninterpretable alternative to standard deep neural networks. While recent\nefforts have proved their compression capabilities, to date, their\ninterpretability properties have not been fully assessed. Here, we conduct a\nsystematic and principled study towards assessing the interpretability of these\ntypes of networks. Moreover, we pay special attention towards analyzing the\nlevel to which part-whole relationships are indeed encoded within the learned\nrepresentation. Our analysis in the MNIST, SVHN, PASCAL-part and CelebA\ndatasets suggest that the representations encoded in CapsNets might not be as\ndisentangled nor strictly related to parts-whole relationships as is commonly\nstated in the literature.\n", "link": "http://arxiv.org/abs/2305.05349v2", "date": "2024-12-13", "relevancy": 2.5172, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20the%20Characterization%20of%20Representations%20Learned%20via%0A%20%20Capsule-based%20Network%20Architectures&body=Title%3A%20Towards%20the%20Characterization%20of%20Representations%20Learned%20via%0A%20%20Capsule-based%20Network%20Architectures%0AAuthor%3A%20Saja%20Tawalbeh%20and%20Jos%C3%A9%20Oramas%0AAbstract%3A%20%20%20Capsule%20Networks%20%28CapsNets%29%20have%20been%20re-introduced%20as%20a%20more%20compact%20and%0Ainterpretable%20alternative%20to%20standard%20deep%20neural%20networks.%20While%20recent%0Aefforts%20have%20proved%20their%20compression%20capabilities%2C%20to%20date%2C%20their%0Ainterpretability%20properties%20have%20not%20been%20fully%20assessed.%20Here%2C%20we%20conduct%20a%0Asystematic%20and%20principled%20study%20towards%20assessing%20the%20interpretability%20of%20these%0Atypes%20of%20networks.%20Moreover%2C%20we%20pay%20special%20attention%20towards%20analyzing%20the%0Alevel%20to%20which%20part-whole%20relationships%20are%20indeed%20encoded%20within%20the%20learned%0Arepresentation.%20Our%20analysis%20in%20the%20MNIST%2C%20SVHN%2C%20PASCAL-part%20and%20CelebA%0Adatasets%20suggest%20that%20the%20representations%20encoded%20in%20CapsNets%20might%20not%20be%20as%0Adisentangled%20nor%20strictly%20related%20to%20parts-whole%20relationships%20as%20is%20commonly%0Astated%20in%20the%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.05349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520the%2520Characterization%2520of%2520Representations%2520Learned%2520via%250A%2520%2520Capsule-based%2520Network%2520Architectures%26entry.906535625%3DSaja%2520Tawalbeh%2520and%2520Jos%25C3%25A9%2520Oramas%26entry.1292438233%3D%2520%2520Capsule%2520Networks%2520%2528CapsNets%2529%2520have%2520been%2520re-introduced%2520as%2520a%2520more%2520compact%2520and%250Ainterpretable%2520alternative%2520to%2520standard%2520deep%2520neural%2520networks.%2520While%2520recent%250Aefforts%2520have%2520proved%2520their%2520compression%2520capabilities%252C%2520to%2520date%252C%2520their%250Ainterpretability%2520properties%2520have%2520not%2520been%2520fully%2520assessed.%2520Here%252C%2520we%2520conduct%2520a%250Asystematic%2520and%2520principled%2520study%2520towards%2520assessing%2520the%2520interpretability%2520of%2520these%250Atypes%2520of%2520networks.%2520Moreover%252C%2520we%2520pay%2520special%2520attention%2520towards%2520analyzing%2520the%250Alevel%2520to%2520which%2520part-whole%2520relationships%2520are%2520indeed%2520encoded%2520within%2520the%2520learned%250Arepresentation.%2520Our%2520analysis%2520in%2520the%2520MNIST%252C%2520SVHN%252C%2520PASCAL-part%2520and%2520CelebA%250Adatasets%2520suggest%2520that%2520the%2520representations%2520encoded%2520in%2520CapsNets%2520might%2520not%2520be%2520as%250Adisentangled%2520nor%2520strictly%2520related%2520to%2520parts-whole%2520relationships%2520as%2520is%2520commonly%250Astated%2520in%2520the%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.05349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20the%20Characterization%20of%20Representations%20Learned%20via%0A%20%20Capsule-based%20Network%20Architectures&entry.906535625=Saja%20Tawalbeh%20and%20Jos%C3%A9%20Oramas&entry.1292438233=%20%20Capsule%20Networks%20%28CapsNets%29%20have%20been%20re-introduced%20as%20a%20more%20compact%20and%0Ainterpretable%20alternative%20to%20standard%20deep%20neural%20networks.%20While%20recent%0Aefforts%20have%20proved%20their%20compression%20capabilities%2C%20to%20date%2C%20their%0Ainterpretability%20properties%20have%20not%20been%20fully%20assessed.%20Here%2C%20we%20conduct%20a%0Asystematic%20and%20principled%20study%20towards%20assessing%20the%20interpretability%20of%20these%0Atypes%20of%20networks.%20Moreover%2C%20we%20pay%20special%20attention%20towards%20analyzing%20the%0Alevel%20to%20which%20part-whole%20relationships%20are%20indeed%20encoded%20within%20the%20learned%0Arepresentation.%20Our%20analysis%20in%20the%20MNIST%2C%20SVHN%2C%20PASCAL-part%20and%20CelebA%0Adatasets%20suggest%20that%20the%20representations%20encoded%20in%20CapsNets%20might%20not%20be%20as%0Adisentangled%20nor%20strictly%20related%20to%20parts-whole%20relationships%20as%20is%20commonly%0Astated%20in%20the%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.05349v2&entry.124074799=Read"},
{"title": "VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval\n  Augmented Generation", "author": "Hyeonseok Lim and Dongjae Shin and Seohyun Song and Inho Won and Minjun Kim and Junghun Yuk and Haneol Jang and KyungTae Lim", "abstract": "  We propose the VLR-Bench, a visual question answering (VQA) benchmark for\nevaluating vision language models (VLMs) based on retrieval augmented\ngeneration (RAG). Unlike existing evaluation datasets for external\nknowledge-based VQA, the proposed VLR-Bench includes five input passages. This\nallows testing of the ability to determine which passage is useful for\nanswering a given query, a capability lacking in previous research. In this\ncontext, we constructed a dataset of 32,000 automatically generated\ninstruction-following examples, which we denote as VLR-IF. This dataset is\nspecifically designed to enhance the RAG capabilities of VLMs by enabling them\nto learn how to generate appropriate answers based on input passages. We\nevaluated the validity of the proposed benchmark and training data and verified\nits performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3\nmodel. The proposed VLR-Bench and VLR-IF datasets are publicly available\nonline.\n", "link": "http://arxiv.org/abs/2412.10151v1", "date": "2024-12-13", "relevancy": 2.5151, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLR-Bench%3A%20Multilingual%20Benchmark%20Dataset%20for%20Vision-Language%20Retrieval%0A%20%20Augmented%20Generation&body=Title%3A%20VLR-Bench%3A%20Multilingual%20Benchmark%20Dataset%20for%20Vision-Language%20Retrieval%0A%20%20Augmented%20Generation%0AAuthor%3A%20Hyeonseok%20Lim%20and%20Dongjae%20Shin%20and%20Seohyun%20Song%20and%20Inho%20Won%20and%20Minjun%20Kim%20and%20Junghun%20Yuk%20and%20Haneol%20Jang%20and%20KyungTae%20Lim%0AAbstract%3A%20%20%20We%20propose%20the%20VLR-Bench%2C%20a%20visual%20question%20answering%20%28VQA%29%20benchmark%20for%0Aevaluating%20vision%20language%20models%20%28VLMs%29%20based%20on%20retrieval%20augmented%0Ageneration%20%28RAG%29.%20Unlike%20existing%20evaluation%20datasets%20for%20external%0Aknowledge-based%20VQA%2C%20the%20proposed%20VLR-Bench%20includes%20five%20input%20passages.%20This%0Aallows%20testing%20of%20the%20ability%20to%20determine%20which%20passage%20is%20useful%20for%0Aanswering%20a%20given%20query%2C%20a%20capability%20lacking%20in%20previous%20research.%20In%20this%0Acontext%2C%20we%20constructed%20a%20dataset%20of%2032%2C000%20automatically%20generated%0Ainstruction-following%20examples%2C%20which%20we%20denote%20as%20VLR-IF.%20This%20dataset%20is%0Aspecifically%20designed%20to%20enhance%20the%20RAG%20capabilities%20of%20VLMs%20by%20enabling%20them%0Ato%20learn%20how%20to%20generate%20appropriate%20answers%20based%20on%20input%20passages.%20We%0Aevaluated%20the%20validity%20of%20the%20proposed%20benchmark%20and%20training%20data%20and%20verified%0Aits%20performance%20using%20the%20state-of-the-art%20Llama3-based%20VLM%2C%20the%20Llava-Llama-3%0Amodel.%20The%20proposed%20VLR-Bench%20and%20VLR-IF%20datasets%20are%20publicly%20available%0Aonline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLR-Bench%253A%2520Multilingual%2520Benchmark%2520Dataset%2520for%2520Vision-Language%2520Retrieval%250A%2520%2520Augmented%2520Generation%26entry.906535625%3DHyeonseok%2520Lim%2520and%2520Dongjae%2520Shin%2520and%2520Seohyun%2520Song%2520and%2520Inho%2520Won%2520and%2520Minjun%2520Kim%2520and%2520Junghun%2520Yuk%2520and%2520Haneol%2520Jang%2520and%2520KyungTae%2520Lim%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520VLR-Bench%252C%2520a%2520visual%2520question%2520answering%2520%2528VQA%2529%2520benchmark%2520for%250Aevaluating%2520vision%2520language%2520models%2520%2528VLMs%2529%2520based%2520on%2520retrieval%2520augmented%250Ageneration%2520%2528RAG%2529.%2520Unlike%2520existing%2520evaluation%2520datasets%2520for%2520external%250Aknowledge-based%2520VQA%252C%2520the%2520proposed%2520VLR-Bench%2520includes%2520five%2520input%2520passages.%2520This%250Aallows%2520testing%2520of%2520the%2520ability%2520to%2520determine%2520which%2520passage%2520is%2520useful%2520for%250Aanswering%2520a%2520given%2520query%252C%2520a%2520capability%2520lacking%2520in%2520previous%2520research.%2520In%2520this%250Acontext%252C%2520we%2520constructed%2520a%2520dataset%2520of%252032%252C000%2520automatically%2520generated%250Ainstruction-following%2520examples%252C%2520which%2520we%2520denote%2520as%2520VLR-IF.%2520This%2520dataset%2520is%250Aspecifically%2520designed%2520to%2520enhance%2520the%2520RAG%2520capabilities%2520of%2520VLMs%2520by%2520enabling%2520them%250Ato%2520learn%2520how%2520to%2520generate%2520appropriate%2520answers%2520based%2520on%2520input%2520passages.%2520We%250Aevaluated%2520the%2520validity%2520of%2520the%2520proposed%2520benchmark%2520and%2520training%2520data%2520and%2520verified%250Aits%2520performance%2520using%2520the%2520state-of-the-art%2520Llama3-based%2520VLM%252C%2520the%2520Llava-Llama-3%250Amodel.%2520The%2520proposed%2520VLR-Bench%2520and%2520VLR-IF%2520datasets%2520are%2520publicly%2520available%250Aonline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLR-Bench%3A%20Multilingual%20Benchmark%20Dataset%20for%20Vision-Language%20Retrieval%0A%20%20Augmented%20Generation&entry.906535625=Hyeonseok%20Lim%20and%20Dongjae%20Shin%20and%20Seohyun%20Song%20and%20Inho%20Won%20and%20Minjun%20Kim%20and%20Junghun%20Yuk%20and%20Haneol%20Jang%20and%20KyungTae%20Lim&entry.1292438233=%20%20We%20propose%20the%20VLR-Bench%2C%20a%20visual%20question%20answering%20%28VQA%29%20benchmark%20for%0Aevaluating%20vision%20language%20models%20%28VLMs%29%20based%20on%20retrieval%20augmented%0Ageneration%20%28RAG%29.%20Unlike%20existing%20evaluation%20datasets%20for%20external%0Aknowledge-based%20VQA%2C%20the%20proposed%20VLR-Bench%20includes%20five%20input%20passages.%20This%0Aallows%20testing%20of%20the%20ability%20to%20determine%20which%20passage%20is%20useful%20for%0Aanswering%20a%20given%20query%2C%20a%20capability%20lacking%20in%20previous%20research.%20In%20this%0Acontext%2C%20we%20constructed%20a%20dataset%20of%2032%2C000%20automatically%20generated%0Ainstruction-following%20examples%2C%20which%20we%20denote%20as%20VLR-IF.%20This%20dataset%20is%0Aspecifically%20designed%20to%20enhance%20the%20RAG%20capabilities%20of%20VLMs%20by%20enabling%20them%0Ato%20learn%20how%20to%20generate%20appropriate%20answers%20based%20on%20input%20passages.%20We%0Aevaluated%20the%20validity%20of%20the%20proposed%20benchmark%20and%20training%20data%20and%20verified%0Aits%20performance%20using%20the%20state-of-the-art%20Llama3-based%20VLM%2C%20the%20Llava-Llama-3%0Amodel.%20The%20proposed%20VLR-Bench%20and%20VLR-IF%20datasets%20are%20publicly%20available%0Aonline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10151v1&entry.124074799=Read"},
{"title": "Fair Decentralized Learning", "author": "Sayan Biswas and Anne-Marie Kermarrec and Rishi Sharma and Thibaud Trinca and Martijn de Vos", "abstract": "  Decentralized learning (DL) is an emerging approach that enables nodes to\ncollaboratively train a machine learning model without sharing raw data. In\nmany application domains, such as healthcare, this approach faces challenges\ndue to the high level of heterogeneity in the training data's feature space.\nSuch feature heterogeneity lowers model utility and negatively impacts\nfairness, particularly for nodes with under-represented training data. In this\npaper, we introduce \\textsc{Facade}, a clustering-based DL algorithm\nspecifically designed for fair model training when the training data exhibits\nseveral distinct features. The challenge of \\textsc{Facade} is to assign nodes\nto clusters, one for each feature, based on the similarity in the features of\ntheir local data, without requiring individual nodes to know apriori which\ncluster they belong to. \\textsc{Facade} (1) dynamically assigns nodes to their\nappropriate clusters over time, and (2) enables nodes to collaboratively train\na specialized model for each cluster in a fully decentralized manner. We\ntheoretically prove the convergence of \\textsc{Facade}, implement our\nalgorithm, and compare it against three state-of-the-art baselines. Our\nexperimental results on three datasets demonstrate the superiority of our\napproach in terms of model accuracy and fairness compared to all three\ncompetitors. Compared to the best-performing baseline, \\textsc{Facade} on the\nCIFAR-10 dataset also reduces communication costs by 32.3\\% to reach a target\naccuracy when cluster sizes are imbalanced.\n", "link": "http://arxiv.org/abs/2410.02541v2", "date": "2024-12-13", "relevancy": 2.4478, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4944}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4904}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20Decentralized%20Learning&body=Title%3A%20Fair%20Decentralized%20Learning%0AAuthor%3A%20Sayan%20Biswas%20and%20Anne-Marie%20Kermarrec%20and%20Rishi%20Sharma%20and%20Thibaud%20Trinca%20and%20Martijn%20de%20Vos%0AAbstract%3A%20%20%20Decentralized%20learning%20%28DL%29%20is%20an%20emerging%20approach%20that%20enables%20nodes%20to%0Acollaboratively%20train%20a%20machine%20learning%20model%20without%20sharing%20raw%20data.%20In%0Amany%20application%20domains%2C%20such%20as%20healthcare%2C%20this%20approach%20faces%20challenges%0Adue%20to%20the%20high%20level%20of%20heterogeneity%20in%20the%20training%20data%27s%20feature%20space.%0ASuch%20feature%20heterogeneity%20lowers%20model%20utility%20and%20negatively%20impacts%0Afairness%2C%20particularly%20for%20nodes%20with%20under-represented%20training%20data.%20In%20this%0Apaper%2C%20we%20introduce%20%5Ctextsc%7BFacade%7D%2C%20a%20clustering-based%20DL%20algorithm%0Aspecifically%20designed%20for%20fair%20model%20training%20when%20the%20training%20data%20exhibits%0Aseveral%20distinct%20features.%20The%20challenge%20of%20%5Ctextsc%7BFacade%7D%20is%20to%20assign%20nodes%0Ato%20clusters%2C%20one%20for%20each%20feature%2C%20based%20on%20the%20similarity%20in%20the%20features%20of%0Atheir%20local%20data%2C%20without%20requiring%20individual%20nodes%20to%20know%20apriori%20which%0Acluster%20they%20belong%20to.%20%5Ctextsc%7BFacade%7D%20%281%29%20dynamically%20assigns%20nodes%20to%20their%0Aappropriate%20clusters%20over%20time%2C%20and%20%282%29%20enables%20nodes%20to%20collaboratively%20train%0Aa%20specialized%20model%20for%20each%20cluster%20in%20a%20fully%20decentralized%20manner.%20We%0Atheoretically%20prove%20the%20convergence%20of%20%5Ctextsc%7BFacade%7D%2C%20implement%20our%0Aalgorithm%2C%20and%20compare%20it%20against%20three%20state-of-the-art%20baselines.%20Our%0Aexperimental%20results%20on%20three%20datasets%20demonstrate%20the%20superiority%20of%20our%0Aapproach%20in%20terms%20of%20model%20accuracy%20and%20fairness%20compared%20to%20all%20three%0Acompetitors.%20Compared%20to%20the%20best-performing%20baseline%2C%20%5Ctextsc%7BFacade%7D%20on%20the%0ACIFAR-10%20dataset%20also%20reduces%20communication%20costs%20by%2032.3%5C%25%20to%20reach%20a%20target%0Aaccuracy%20when%20cluster%20sizes%20are%20imbalanced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02541v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520Decentralized%2520Learning%26entry.906535625%3DSayan%2520Biswas%2520and%2520Anne-Marie%2520Kermarrec%2520and%2520Rishi%2520Sharma%2520and%2520Thibaud%2520Trinca%2520and%2520Martijn%2520de%2520Vos%26entry.1292438233%3D%2520%2520Decentralized%2520learning%2520%2528DL%2529%2520is%2520an%2520emerging%2520approach%2520that%2520enables%2520nodes%2520to%250Acollaboratively%2520train%2520a%2520machine%2520learning%2520model%2520without%2520sharing%2520raw%2520data.%2520In%250Amany%2520application%2520domains%252C%2520such%2520as%2520healthcare%252C%2520this%2520approach%2520faces%2520challenges%250Adue%2520to%2520the%2520high%2520level%2520of%2520heterogeneity%2520in%2520the%2520training%2520data%2527s%2520feature%2520space.%250ASuch%2520feature%2520heterogeneity%2520lowers%2520model%2520utility%2520and%2520negatively%2520impacts%250Afairness%252C%2520particularly%2520for%2520nodes%2520with%2520under-represented%2520training%2520data.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520%255Ctextsc%257BFacade%257D%252C%2520a%2520clustering-based%2520DL%2520algorithm%250Aspecifically%2520designed%2520for%2520fair%2520model%2520training%2520when%2520the%2520training%2520data%2520exhibits%250Aseveral%2520distinct%2520features.%2520The%2520challenge%2520of%2520%255Ctextsc%257BFacade%257D%2520is%2520to%2520assign%2520nodes%250Ato%2520clusters%252C%2520one%2520for%2520each%2520feature%252C%2520based%2520on%2520the%2520similarity%2520in%2520the%2520features%2520of%250Atheir%2520local%2520data%252C%2520without%2520requiring%2520individual%2520nodes%2520to%2520know%2520apriori%2520which%250Acluster%2520they%2520belong%2520to.%2520%255Ctextsc%257BFacade%257D%2520%25281%2529%2520dynamically%2520assigns%2520nodes%2520to%2520their%250Aappropriate%2520clusters%2520over%2520time%252C%2520and%2520%25282%2529%2520enables%2520nodes%2520to%2520collaboratively%2520train%250Aa%2520specialized%2520model%2520for%2520each%2520cluster%2520in%2520a%2520fully%2520decentralized%2520manner.%2520We%250Atheoretically%2520prove%2520the%2520convergence%2520of%2520%255Ctextsc%257BFacade%257D%252C%2520implement%2520our%250Aalgorithm%252C%2520and%2520compare%2520it%2520against%2520three%2520state-of-the-art%2520baselines.%2520Our%250Aexperimental%2520results%2520on%2520three%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520our%250Aapproach%2520in%2520terms%2520of%2520model%2520accuracy%2520and%2520fairness%2520compared%2520to%2520all%2520three%250Acompetitors.%2520Compared%2520to%2520the%2520best-performing%2520baseline%252C%2520%255Ctextsc%257BFacade%257D%2520on%2520the%250ACIFAR-10%2520dataset%2520also%2520reduces%2520communication%2520costs%2520by%252032.3%255C%2525%2520to%2520reach%2520a%2520target%250Aaccuracy%2520when%2520cluster%2520sizes%2520are%2520imbalanced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02541v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Decentralized%20Learning&entry.906535625=Sayan%20Biswas%20and%20Anne-Marie%20Kermarrec%20and%20Rishi%20Sharma%20and%20Thibaud%20Trinca%20and%20Martijn%20de%20Vos&entry.1292438233=%20%20Decentralized%20learning%20%28DL%29%20is%20an%20emerging%20approach%20that%20enables%20nodes%20to%0Acollaboratively%20train%20a%20machine%20learning%20model%20without%20sharing%20raw%20data.%20In%0Amany%20application%20domains%2C%20such%20as%20healthcare%2C%20this%20approach%20faces%20challenges%0Adue%20to%20the%20high%20level%20of%20heterogeneity%20in%20the%20training%20data%27s%20feature%20space.%0ASuch%20feature%20heterogeneity%20lowers%20model%20utility%20and%20negatively%20impacts%0Afairness%2C%20particularly%20for%20nodes%20with%20under-represented%20training%20data.%20In%20this%0Apaper%2C%20we%20introduce%20%5Ctextsc%7BFacade%7D%2C%20a%20clustering-based%20DL%20algorithm%0Aspecifically%20designed%20for%20fair%20model%20training%20when%20the%20training%20data%20exhibits%0Aseveral%20distinct%20features.%20The%20challenge%20of%20%5Ctextsc%7BFacade%7D%20is%20to%20assign%20nodes%0Ato%20clusters%2C%20one%20for%20each%20feature%2C%20based%20on%20the%20similarity%20in%20the%20features%20of%0Atheir%20local%20data%2C%20without%20requiring%20individual%20nodes%20to%20know%20apriori%20which%0Acluster%20they%20belong%20to.%20%5Ctextsc%7BFacade%7D%20%281%29%20dynamically%20assigns%20nodes%20to%20their%0Aappropriate%20clusters%20over%20time%2C%20and%20%282%29%20enables%20nodes%20to%20collaboratively%20train%0Aa%20specialized%20model%20for%20each%20cluster%20in%20a%20fully%20decentralized%20manner.%20We%0Atheoretically%20prove%20the%20convergence%20of%20%5Ctextsc%7BFacade%7D%2C%20implement%20our%0Aalgorithm%2C%20and%20compare%20it%20against%20three%20state-of-the-art%20baselines.%20Our%0Aexperimental%20results%20on%20three%20datasets%20demonstrate%20the%20superiority%20of%20our%0Aapproach%20in%20terms%20of%20model%20accuracy%20and%20fairness%20compared%20to%20all%20three%0Acompetitors.%20Compared%20to%20the%20best-performing%20baseline%2C%20%5Ctextsc%7BFacade%7D%20on%20the%0ACIFAR-10%20dataset%20also%20reduces%20communication%20costs%20by%2032.3%5C%25%20to%20reach%20a%20target%0Aaccuracy%20when%20cluster%20sizes%20are%20imbalanced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02541v2&entry.124074799=Read"},
{"title": "Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings\n  with Few-Shot Learning", "author": "Aditya Narayan Sankaran and Reza Farahbakhsh and Noel Crespi", "abstract": "  Online abusive content detection, particularly in low-resource settings and\nwithin the audio modality, remains underexplored. We investigate the potential\nof pre-trained audio representations for detecting abusive language in\nlow-resource languages, in this case, in Indian languages using Few Shot\nLearning (FSL). Leveraging powerful representations from models such as Wav2Vec\nand Whisper, we explore cross-lingual abuse detection using the ADIMA dataset\nwith FSL. Our approach integrates these representations within the\nModel-Agnostic Meta-Learning (MAML) framework to classify abusive language in\n10 languages. We experiment with various shot sizes (50-200) evaluating the\nimpact of limited data on performance. Additionally, a feature visualization\nstudy was conducted to better understand model behaviour. This study highlights\nthe generalization ability of pre-trained models in low-resource scenarios and\noffers valuable insights into detecting abusive language in multilingual\ncontexts.\n", "link": "http://arxiv.org/abs/2412.01408v3", "date": "2024-12-13", "relevancy": 2.4415, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4989}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.498}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Cross-Lingual%20Audio%20Abuse%20Detection%20in%20Low-Resource%20Settings%0A%20%20with%20Few-Shot%20Learning&body=Title%3A%20Towards%20Cross-Lingual%20Audio%20Abuse%20Detection%20in%20Low-Resource%20Settings%0A%20%20with%20Few-Shot%20Learning%0AAuthor%3A%20Aditya%20Narayan%20Sankaran%20and%20Reza%20Farahbakhsh%20and%20Noel%20Crespi%0AAbstract%3A%20%20%20Online%20abusive%20content%20detection%2C%20particularly%20in%20low-resource%20settings%20and%0Awithin%20the%20audio%20modality%2C%20remains%20underexplored.%20We%20investigate%20the%20potential%0Aof%20pre-trained%20audio%20representations%20for%20detecting%20abusive%20language%20in%0Alow-resource%20languages%2C%20in%20this%20case%2C%20in%20Indian%20languages%20using%20Few%20Shot%0ALearning%20%28FSL%29.%20Leveraging%20powerful%20representations%20from%20models%20such%20as%20Wav2Vec%0Aand%20Whisper%2C%20we%20explore%20cross-lingual%20abuse%20detection%20using%20the%20ADIMA%20dataset%0Awith%20FSL.%20Our%20approach%20integrates%20these%20representations%20within%20the%0AModel-Agnostic%20Meta-Learning%20%28MAML%29%20framework%20to%20classify%20abusive%20language%20in%0A10%20languages.%20We%20experiment%20with%20various%20shot%20sizes%20%2850-200%29%20evaluating%20the%0Aimpact%20of%20limited%20data%20on%20performance.%20Additionally%2C%20a%20feature%20visualization%0Astudy%20was%20conducted%20to%20better%20understand%20model%20behaviour.%20This%20study%20highlights%0Athe%20generalization%20ability%20of%20pre-trained%20models%20in%20low-resource%20scenarios%20and%0Aoffers%20valuable%20insights%20into%20detecting%20abusive%20language%20in%20multilingual%0Acontexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01408v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Cross-Lingual%2520Audio%2520Abuse%2520Detection%2520in%2520Low-Resource%2520Settings%250A%2520%2520with%2520Few-Shot%2520Learning%26entry.906535625%3DAditya%2520Narayan%2520Sankaran%2520and%2520Reza%2520Farahbakhsh%2520and%2520Noel%2520Crespi%26entry.1292438233%3D%2520%2520Online%2520abusive%2520content%2520detection%252C%2520particularly%2520in%2520low-resource%2520settings%2520and%250Awithin%2520the%2520audio%2520modality%252C%2520remains%2520underexplored.%2520We%2520investigate%2520the%2520potential%250Aof%2520pre-trained%2520audio%2520representations%2520for%2520detecting%2520abusive%2520language%2520in%250Alow-resource%2520languages%252C%2520in%2520this%2520case%252C%2520in%2520Indian%2520languages%2520using%2520Few%2520Shot%250ALearning%2520%2528FSL%2529.%2520Leveraging%2520powerful%2520representations%2520from%2520models%2520such%2520as%2520Wav2Vec%250Aand%2520Whisper%252C%2520we%2520explore%2520cross-lingual%2520abuse%2520detection%2520using%2520the%2520ADIMA%2520dataset%250Awith%2520FSL.%2520Our%2520approach%2520integrates%2520these%2520representations%2520within%2520the%250AModel-Agnostic%2520Meta-Learning%2520%2528MAML%2529%2520framework%2520to%2520classify%2520abusive%2520language%2520in%250A10%2520languages.%2520We%2520experiment%2520with%2520various%2520shot%2520sizes%2520%252850-200%2529%2520evaluating%2520the%250Aimpact%2520of%2520limited%2520data%2520on%2520performance.%2520Additionally%252C%2520a%2520feature%2520visualization%250Astudy%2520was%2520conducted%2520to%2520better%2520understand%2520model%2520behaviour.%2520This%2520study%2520highlights%250Athe%2520generalization%2520ability%2520of%2520pre-trained%2520models%2520in%2520low-resource%2520scenarios%2520and%250Aoffers%2520valuable%2520insights%2520into%2520detecting%2520abusive%2520language%2520in%2520multilingual%250Acontexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01408v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Cross-Lingual%20Audio%20Abuse%20Detection%20in%20Low-Resource%20Settings%0A%20%20with%20Few-Shot%20Learning&entry.906535625=Aditya%20Narayan%20Sankaran%20and%20Reza%20Farahbakhsh%20and%20Noel%20Crespi&entry.1292438233=%20%20Online%20abusive%20content%20detection%2C%20particularly%20in%20low-resource%20settings%20and%0Awithin%20the%20audio%20modality%2C%20remains%20underexplored.%20We%20investigate%20the%20potential%0Aof%20pre-trained%20audio%20representations%20for%20detecting%20abusive%20language%20in%0Alow-resource%20languages%2C%20in%20this%20case%2C%20in%20Indian%20languages%20using%20Few%20Shot%0ALearning%20%28FSL%29.%20Leveraging%20powerful%20representations%20from%20models%20such%20as%20Wav2Vec%0Aand%20Whisper%2C%20we%20explore%20cross-lingual%20abuse%20detection%20using%20the%20ADIMA%20dataset%0Awith%20FSL.%20Our%20approach%20integrates%20these%20representations%20within%20the%0AModel-Agnostic%20Meta-Learning%20%28MAML%29%20framework%20to%20classify%20abusive%20language%20in%0A10%20languages.%20We%20experiment%20with%20various%20shot%20sizes%20%2850-200%29%20evaluating%20the%0Aimpact%20of%20limited%20data%20on%20performance.%20Additionally%2C%20a%20feature%20visualization%0Astudy%20was%20conducted%20to%20better%20understand%20model%20behaviour.%20This%20study%20highlights%0Athe%20generalization%20ability%20of%20pre-trained%20models%20in%20low-resource%20scenarios%20and%0Aoffers%20valuable%20insights%20into%20detecting%20abusive%20language%20in%20multilingual%0Acontexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01408v3&entry.124074799=Read"},
{"title": "ADA-Track++: End-to-End Multi-Camera 3D Multi-Object Tracking with\n  Alternating Detection and Association", "author": "Shuxiao Ding and Lukas Schneider and Marius Cordts and Juergen Gall", "abstract": "  Many query-based approaches for 3D Multi-Object Tracking (MOT) adopt the\ntracking-by-attention paradigm, utilizing track queries for identity-consistent\ndetection and object queries for identity-agnostic track spawning.\nTracking-by-attention, however, entangles detection and tracking queries in one\nembedding for both the detection and tracking task, which is sub-optimal. Other\napproaches resemble the tracking-by-detection paradigm and detect objects using\ndecoupled track and detection queries followed by a subsequent association.\nThese methods, however, do not leverage synergies between the detection and\nassociation task. Combining the strengths of both paradigms, we introduce\nADA-Track++, a novel end-to-end framework for 3D MOT from multi-view cameras.\nWe introduce a learnable data association module based on edge-augmented\ncross-attention, leveraging appearance and geometric features. We also propose\nan auxiliary token in this attention-based association module, which helps\nmitigate disproportionately high attention to incorrect association targets\ncaused by attention normalization. Furthermore, we integrate this association\nmodule into the decoder layer of a DETR-based 3D detector, enabling\nsimultaneous DETR-like query-to-image cross-attention for detection and\nquery-to-query cross-attention for data association. By stacking these decoder\nlayers, queries are refined for the detection and association task alternately,\neffectively harnessing the task dependencies. We evaluate our method on the\nnuScenes dataset and demonstrate the advantage of our approach compared to the\ntwo previous paradigms.\n", "link": "http://arxiv.org/abs/2405.08909v2", "date": "2024-12-13", "relevancy": 2.4334, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6249}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6105}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ADA-Track%2B%2B%3A%20End-to-End%20Multi-Camera%203D%20Multi-Object%20Tracking%20with%0A%20%20Alternating%20Detection%20and%20Association&body=Title%3A%20ADA-Track%2B%2B%3A%20End-to-End%20Multi-Camera%203D%20Multi-Object%20Tracking%20with%0A%20%20Alternating%20Detection%20and%20Association%0AAuthor%3A%20Shuxiao%20Ding%20and%20Lukas%20Schneider%20and%20Marius%20Cordts%20and%20Juergen%20Gall%0AAbstract%3A%20%20%20Many%20query-based%20approaches%20for%203D%20Multi-Object%20Tracking%20%28MOT%29%20adopt%20the%0Atracking-by-attention%20paradigm%2C%20utilizing%20track%20queries%20for%20identity-consistent%0Adetection%20and%20object%20queries%20for%20identity-agnostic%20track%20spawning.%0ATracking-by-attention%2C%20however%2C%20entangles%20detection%20and%20tracking%20queries%20in%20one%0Aembedding%20for%20both%20the%20detection%20and%20tracking%20task%2C%20which%20is%20sub-optimal.%20Other%0Aapproaches%20resemble%20the%20tracking-by-detection%20paradigm%20and%20detect%20objects%20using%0Adecoupled%20track%20and%20detection%20queries%20followed%20by%20a%20subsequent%20association.%0AThese%20methods%2C%20however%2C%20do%20not%20leverage%20synergies%20between%20the%20detection%20and%0Aassociation%20task.%20Combining%20the%20strengths%20of%20both%20paradigms%2C%20we%20introduce%0AADA-Track%2B%2B%2C%20a%20novel%20end-to-end%20framework%20for%203D%20MOT%20from%20multi-view%20cameras.%0AWe%20introduce%20a%20learnable%20data%20association%20module%20based%20on%20edge-augmented%0Across-attention%2C%20leveraging%20appearance%20and%20geometric%20features.%20We%20also%20propose%0Aan%20auxiliary%20token%20in%20this%20attention-based%20association%20module%2C%20which%20helps%0Amitigate%20disproportionately%20high%20attention%20to%20incorrect%20association%20targets%0Acaused%20by%20attention%20normalization.%20Furthermore%2C%20we%20integrate%20this%20association%0Amodule%20into%20the%20decoder%20layer%20of%20a%20DETR-based%203D%20detector%2C%20enabling%0Asimultaneous%20DETR-like%20query-to-image%20cross-attention%20for%20detection%20and%0Aquery-to-query%20cross-attention%20for%20data%20association.%20By%20stacking%20these%20decoder%0Alayers%2C%20queries%20are%20refined%20for%20the%20detection%20and%20association%20task%20alternately%2C%0Aeffectively%20harnessing%20the%20task%20dependencies.%20We%20evaluate%20our%20method%20on%20the%0AnuScenes%20dataset%20and%20demonstrate%20the%20advantage%20of%20our%20approach%20compared%20to%20the%0Atwo%20previous%20paradigms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08909v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DADA-Track%252B%252B%253A%2520End-to-End%2520Multi-Camera%25203D%2520Multi-Object%2520Tracking%2520with%250A%2520%2520Alternating%2520Detection%2520and%2520Association%26entry.906535625%3DShuxiao%2520Ding%2520and%2520Lukas%2520Schneider%2520and%2520Marius%2520Cordts%2520and%2520Juergen%2520Gall%26entry.1292438233%3D%2520%2520Many%2520query-based%2520approaches%2520for%25203D%2520Multi-Object%2520Tracking%2520%2528MOT%2529%2520adopt%2520the%250Atracking-by-attention%2520paradigm%252C%2520utilizing%2520track%2520queries%2520for%2520identity-consistent%250Adetection%2520and%2520object%2520queries%2520for%2520identity-agnostic%2520track%2520spawning.%250ATracking-by-attention%252C%2520however%252C%2520entangles%2520detection%2520and%2520tracking%2520queries%2520in%2520one%250Aembedding%2520for%2520both%2520the%2520detection%2520and%2520tracking%2520task%252C%2520which%2520is%2520sub-optimal.%2520Other%250Aapproaches%2520resemble%2520the%2520tracking-by-detection%2520paradigm%2520and%2520detect%2520objects%2520using%250Adecoupled%2520track%2520and%2520detection%2520queries%2520followed%2520by%2520a%2520subsequent%2520association.%250AThese%2520methods%252C%2520however%252C%2520do%2520not%2520leverage%2520synergies%2520between%2520the%2520detection%2520and%250Aassociation%2520task.%2520Combining%2520the%2520strengths%2520of%2520both%2520paradigms%252C%2520we%2520introduce%250AADA-Track%252B%252B%252C%2520a%2520novel%2520end-to-end%2520framework%2520for%25203D%2520MOT%2520from%2520multi-view%2520cameras.%250AWe%2520introduce%2520a%2520learnable%2520data%2520association%2520module%2520based%2520on%2520edge-augmented%250Across-attention%252C%2520leveraging%2520appearance%2520and%2520geometric%2520features.%2520We%2520also%2520propose%250Aan%2520auxiliary%2520token%2520in%2520this%2520attention-based%2520association%2520module%252C%2520which%2520helps%250Amitigate%2520disproportionately%2520high%2520attention%2520to%2520incorrect%2520association%2520targets%250Acaused%2520by%2520attention%2520normalization.%2520Furthermore%252C%2520we%2520integrate%2520this%2520association%250Amodule%2520into%2520the%2520decoder%2520layer%2520of%2520a%2520DETR-based%25203D%2520detector%252C%2520enabling%250Asimultaneous%2520DETR-like%2520query-to-image%2520cross-attention%2520for%2520detection%2520and%250Aquery-to-query%2520cross-attention%2520for%2520data%2520association.%2520By%2520stacking%2520these%2520decoder%250Alayers%252C%2520queries%2520are%2520refined%2520for%2520the%2520detection%2520and%2520association%2520task%2520alternately%252C%250Aeffectively%2520harnessing%2520the%2520task%2520dependencies.%2520We%2520evaluate%2520our%2520method%2520on%2520the%250AnuScenes%2520dataset%2520and%2520demonstrate%2520the%2520advantage%2520of%2520our%2520approach%2520compared%2520to%2520the%250Atwo%2520previous%2520paradigms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08909v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ADA-Track%2B%2B%3A%20End-to-End%20Multi-Camera%203D%20Multi-Object%20Tracking%20with%0A%20%20Alternating%20Detection%20and%20Association&entry.906535625=Shuxiao%20Ding%20and%20Lukas%20Schneider%20and%20Marius%20Cordts%20and%20Juergen%20Gall&entry.1292438233=%20%20Many%20query-based%20approaches%20for%203D%20Multi-Object%20Tracking%20%28MOT%29%20adopt%20the%0Atracking-by-attention%20paradigm%2C%20utilizing%20track%20queries%20for%20identity-consistent%0Adetection%20and%20object%20queries%20for%20identity-agnostic%20track%20spawning.%0ATracking-by-attention%2C%20however%2C%20entangles%20detection%20and%20tracking%20queries%20in%20one%0Aembedding%20for%20both%20the%20detection%20and%20tracking%20task%2C%20which%20is%20sub-optimal.%20Other%0Aapproaches%20resemble%20the%20tracking-by-detection%20paradigm%20and%20detect%20objects%20using%0Adecoupled%20track%20and%20detection%20queries%20followed%20by%20a%20subsequent%20association.%0AThese%20methods%2C%20however%2C%20do%20not%20leverage%20synergies%20between%20the%20detection%20and%0Aassociation%20task.%20Combining%20the%20strengths%20of%20both%20paradigms%2C%20we%20introduce%0AADA-Track%2B%2B%2C%20a%20novel%20end-to-end%20framework%20for%203D%20MOT%20from%20multi-view%20cameras.%0AWe%20introduce%20a%20learnable%20data%20association%20module%20based%20on%20edge-augmented%0Across-attention%2C%20leveraging%20appearance%20and%20geometric%20features.%20We%20also%20propose%0Aan%20auxiliary%20token%20in%20this%20attention-based%20association%20module%2C%20which%20helps%0Amitigate%20disproportionately%20high%20attention%20to%20incorrect%20association%20targets%0Acaused%20by%20attention%20normalization.%20Furthermore%2C%20we%20integrate%20this%20association%0Amodule%20into%20the%20decoder%20layer%20of%20a%20DETR-based%203D%20detector%2C%20enabling%0Asimultaneous%20DETR-like%20query-to-image%20cross-attention%20for%20detection%20and%0Aquery-to-query%20cross-attention%20for%20data%20association.%20By%20stacking%20these%20decoder%0Alayers%2C%20queries%20are%20refined%20for%20the%20detection%20and%20association%20task%20alternately%2C%0Aeffectively%20harnessing%20the%20task%20dependencies.%20We%20evaluate%20our%20method%20on%20the%0AnuScenes%20dataset%20and%20demonstrate%20the%20advantage%20of%20our%20approach%20compared%20to%20the%0Atwo%20previous%20paradigms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08909v2&entry.124074799=Read"},
{"title": "Unveiling the optimization process of Physics Informed Neural Networks:\n  How accurate and competitive can PINNs be?", "author": "Jorge F. Urb\u00e1n and Petros Stefanou and Jos\u00e9 A. Pons", "abstract": "  This study investigates the potential accuracy boundaries of physics-informed\nneural networks, contrasting their approach with previous similar works and\ntraditional numerical methods. We find that selecting improved optimization\nalgorithms significantly enhances the accuracy of the results. Simple\nmodifications to the loss function may also improve precision, offering an\nadditional avenue for enhancement. Despite optimization algorithms having a\ngreater impact on convergence than adjustments to the loss function, practical\nconsiderations often favor tweaking the latter due to ease of implementation.\nOn a global scale, the integration of an enhanced optimizer and a marginally\nadjusted loss function enables a reduction in the loss function by several\norders of magnitude across diverse physical problems. Consequently, our results\nobtained using compact networks (typically comprising 2 or 3 layers of 20-30\nneurons) achieve accuracies comparable to finite difference schemes employing\nthousands of grid points. This study encourages the continued advancement of\nPINNs and associated optimization techniques for broader applications across\nvarious fields.\n", "link": "http://arxiv.org/abs/2405.04230v3", "date": "2024-12-13", "relevancy": 2.4185, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5038}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4744}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20optimization%20process%20of%20Physics%20Informed%20Neural%20Networks%3A%0A%20%20How%20accurate%20and%20competitive%20can%20PINNs%20be%3F&body=Title%3A%20Unveiling%20the%20optimization%20process%20of%20Physics%20Informed%20Neural%20Networks%3A%0A%20%20How%20accurate%20and%20competitive%20can%20PINNs%20be%3F%0AAuthor%3A%20Jorge%20F.%20Urb%C3%A1n%20and%20Petros%20Stefanou%20and%20Jos%C3%A9%20A.%20Pons%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20potential%20accuracy%20boundaries%20of%20physics-informed%0Aneural%20networks%2C%20contrasting%20their%20approach%20with%20previous%20similar%20works%20and%0Atraditional%20numerical%20methods.%20We%20find%20that%20selecting%20improved%20optimization%0Aalgorithms%20significantly%20enhances%20the%20accuracy%20of%20the%20results.%20Simple%0Amodifications%20to%20the%20loss%20function%20may%20also%20improve%20precision%2C%20offering%20an%0Aadditional%20avenue%20for%20enhancement.%20Despite%20optimization%20algorithms%20having%20a%0Agreater%20impact%20on%20convergence%20than%20adjustments%20to%20the%20loss%20function%2C%20practical%0Aconsiderations%20often%20favor%20tweaking%20the%20latter%20due%20to%20ease%20of%20implementation.%0AOn%20a%20global%20scale%2C%20the%20integration%20of%20an%20enhanced%20optimizer%20and%20a%20marginally%0Aadjusted%20loss%20function%20enables%20a%20reduction%20in%20the%20loss%20function%20by%20several%0Aorders%20of%20magnitude%20across%20diverse%20physical%20problems.%20Consequently%2C%20our%20results%0Aobtained%20using%20compact%20networks%20%28typically%20comprising%202%20or%203%20layers%20of%2020-30%0Aneurons%29%20achieve%20accuracies%20comparable%20to%20finite%20difference%20schemes%20employing%0Athousands%20of%20grid%20points.%20This%20study%20encourages%20the%20continued%20advancement%20of%0APINNs%20and%20associated%20optimization%20techniques%20for%20broader%20applications%20across%0Avarious%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04230v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520optimization%2520process%2520of%2520Physics%2520Informed%2520Neural%2520Networks%253A%250A%2520%2520How%2520accurate%2520and%2520competitive%2520can%2520PINNs%2520be%253F%26entry.906535625%3DJorge%2520F.%2520Urb%25C3%25A1n%2520and%2520Petros%2520Stefanou%2520and%2520Jos%25C3%25A9%2520A.%2520Pons%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520potential%2520accuracy%2520boundaries%2520of%2520physics-informed%250Aneural%2520networks%252C%2520contrasting%2520their%2520approach%2520with%2520previous%2520similar%2520works%2520and%250Atraditional%2520numerical%2520methods.%2520We%2520find%2520that%2520selecting%2520improved%2520optimization%250Aalgorithms%2520significantly%2520enhances%2520the%2520accuracy%2520of%2520the%2520results.%2520Simple%250Amodifications%2520to%2520the%2520loss%2520function%2520may%2520also%2520improve%2520precision%252C%2520offering%2520an%250Aadditional%2520avenue%2520for%2520enhancement.%2520Despite%2520optimization%2520algorithms%2520having%2520a%250Agreater%2520impact%2520on%2520convergence%2520than%2520adjustments%2520to%2520the%2520loss%2520function%252C%2520practical%250Aconsiderations%2520often%2520favor%2520tweaking%2520the%2520latter%2520due%2520to%2520ease%2520of%2520implementation.%250AOn%2520a%2520global%2520scale%252C%2520the%2520integration%2520of%2520an%2520enhanced%2520optimizer%2520and%2520a%2520marginally%250Aadjusted%2520loss%2520function%2520enables%2520a%2520reduction%2520in%2520the%2520loss%2520function%2520by%2520several%250Aorders%2520of%2520magnitude%2520across%2520diverse%2520physical%2520problems.%2520Consequently%252C%2520our%2520results%250Aobtained%2520using%2520compact%2520networks%2520%2528typically%2520comprising%25202%2520or%25203%2520layers%2520of%252020-30%250Aneurons%2529%2520achieve%2520accuracies%2520comparable%2520to%2520finite%2520difference%2520schemes%2520employing%250Athousands%2520of%2520grid%2520points.%2520This%2520study%2520encourages%2520the%2520continued%2520advancement%2520of%250APINNs%2520and%2520associated%2520optimization%2520techniques%2520for%2520broader%2520applications%2520across%250Avarious%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04230v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20optimization%20process%20of%20Physics%20Informed%20Neural%20Networks%3A%0A%20%20How%20accurate%20and%20competitive%20can%20PINNs%20be%3F&entry.906535625=Jorge%20F.%20Urb%C3%A1n%20and%20Petros%20Stefanou%20and%20Jos%C3%A9%20A.%20Pons&entry.1292438233=%20%20This%20study%20investigates%20the%20potential%20accuracy%20boundaries%20of%20physics-informed%0Aneural%20networks%2C%20contrasting%20their%20approach%20with%20previous%20similar%20works%20and%0Atraditional%20numerical%20methods.%20We%20find%20that%20selecting%20improved%20optimization%0Aalgorithms%20significantly%20enhances%20the%20accuracy%20of%20the%20results.%20Simple%0Amodifications%20to%20the%20loss%20function%20may%20also%20improve%20precision%2C%20offering%20an%0Aadditional%20avenue%20for%20enhancement.%20Despite%20optimization%20algorithms%20having%20a%0Agreater%20impact%20on%20convergence%20than%20adjustments%20to%20the%20loss%20function%2C%20practical%0Aconsiderations%20often%20favor%20tweaking%20the%20latter%20due%20to%20ease%20of%20implementation.%0AOn%20a%20global%20scale%2C%20the%20integration%20of%20an%20enhanced%20optimizer%20and%20a%20marginally%0Aadjusted%20loss%20function%20enables%20a%20reduction%20in%20the%20loss%20function%20by%20several%0Aorders%20of%20magnitude%20across%20diverse%20physical%20problems.%20Consequently%2C%20our%20results%0Aobtained%20using%20compact%20networks%20%28typically%20comprising%202%20or%203%20layers%20of%2020-30%0Aneurons%29%20achieve%20accuracies%20comparable%20to%20finite%20difference%20schemes%20employing%0Athousands%20of%20grid%20points.%20This%20study%20encourages%20the%20continued%20advancement%20of%0APINNs%20and%20associated%20optimization%20techniques%20for%20broader%20applications%20across%0Avarious%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04230v3&entry.124074799=Read"},
{"title": "Efficient Continual Pre-training of LLMs for Low-resource Languages", "author": "Arijit Nag and Soumen Chakrabarti and Animesh Mukherjee and Niloy Ganguly", "abstract": "  Open-source Large Language models (OsLLMs) propel the democratization of\nnatural language research by giving the flexibility to augment or update model\nparameters for performance improvement. Nevertheless, like proprietary LLMs,\nOs-LLMs offer poorer performance on low-resource languages (LRLs) than\nhigh-resource languages (HRLs), owing to smaller amounts of training data and\nunderrepresented vocabulary. On the other hand, continual pre-training (CPT)\nwith large amounts of language-specific data is a costly proposition in terms\nof data acquisition and computational resources. Our goal is to drastically\nreduce CPT cost. To that end, we first develop a new algorithm to select a\nsubset of texts from a larger corpus. We show the effectiveness of our\ntechnique using very little CPT data. In search of further improvement, we\ndesign a new algorithm to select tokens to include in the LLM vocabulary. We\nexperiment with the recent Llama-3 model and nine Indian languages with diverse\nscripts and extent of resource availability. For evaluation, we use\nIndicGenBench, a generation task benchmark dataset for Indic languages. We\nexperiment with various CPT corpora and augmented vocabulary size and offer\ninsights across language families.\n", "link": "http://arxiv.org/abs/2412.10244v1", "date": "2024-12-13", "relevancy": 2.4048, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4824}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4824}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Continual%20Pre-training%20of%20LLMs%20for%20Low-resource%20Languages&body=Title%3A%20Efficient%20Continual%20Pre-training%20of%20LLMs%20for%20Low-resource%20Languages%0AAuthor%3A%20Arijit%20Nag%20and%20Soumen%20Chakrabarti%20and%20Animesh%20Mukherjee%20and%20Niloy%20Ganguly%0AAbstract%3A%20%20%20Open-source%20Large%20Language%20models%20%28OsLLMs%29%20propel%20the%20democratization%20of%0Anatural%20language%20research%20by%20giving%20the%20flexibility%20to%20augment%20or%20update%20model%0Aparameters%20for%20performance%20improvement.%20Nevertheless%2C%20like%20proprietary%20LLMs%2C%0AOs-LLMs%20offer%20poorer%20performance%20on%20low-resource%20languages%20%28LRLs%29%20than%0Ahigh-resource%20languages%20%28HRLs%29%2C%20owing%20to%20smaller%20amounts%20of%20training%20data%20and%0Aunderrepresented%20vocabulary.%20On%20the%20other%20hand%2C%20continual%20pre-training%20%28CPT%29%0Awith%20large%20amounts%20of%20language-specific%20data%20is%20a%20costly%20proposition%20in%20terms%0Aof%20data%20acquisition%20and%20computational%20resources.%20Our%20goal%20is%20to%20drastically%0Areduce%20CPT%20cost.%20To%20that%20end%2C%20we%20first%20develop%20a%20new%20algorithm%20to%20select%20a%0Asubset%20of%20texts%20from%20a%20larger%20corpus.%20We%20show%20the%20effectiveness%20of%20our%0Atechnique%20using%20very%20little%20CPT%20data.%20In%20search%20of%20further%20improvement%2C%20we%0Adesign%20a%20new%20algorithm%20to%20select%20tokens%20to%20include%20in%20the%20LLM%20vocabulary.%20We%0Aexperiment%20with%20the%20recent%20Llama-3%20model%20and%20nine%20Indian%20languages%20with%20diverse%0Ascripts%20and%20extent%20of%20resource%20availability.%20For%20evaluation%2C%20we%20use%0AIndicGenBench%2C%20a%20generation%20task%20benchmark%20dataset%20for%20Indic%20languages.%20We%0Aexperiment%20with%20various%20CPT%20corpora%20and%20augmented%20vocabulary%20size%20and%20offer%0Ainsights%20across%20language%20families.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Continual%2520Pre-training%2520of%2520LLMs%2520for%2520Low-resource%2520Languages%26entry.906535625%3DArijit%2520Nag%2520and%2520Soumen%2520Chakrabarti%2520and%2520Animesh%2520Mukherjee%2520and%2520Niloy%2520Ganguly%26entry.1292438233%3D%2520%2520Open-source%2520Large%2520Language%2520models%2520%2528OsLLMs%2529%2520propel%2520the%2520democratization%2520of%250Anatural%2520language%2520research%2520by%2520giving%2520the%2520flexibility%2520to%2520augment%2520or%2520update%2520model%250Aparameters%2520for%2520performance%2520improvement.%2520Nevertheless%252C%2520like%2520proprietary%2520LLMs%252C%250AOs-LLMs%2520offer%2520poorer%2520performance%2520on%2520low-resource%2520languages%2520%2528LRLs%2529%2520than%250Ahigh-resource%2520languages%2520%2528HRLs%2529%252C%2520owing%2520to%2520smaller%2520amounts%2520of%2520training%2520data%2520and%250Aunderrepresented%2520vocabulary.%2520On%2520the%2520other%2520hand%252C%2520continual%2520pre-training%2520%2528CPT%2529%250Awith%2520large%2520amounts%2520of%2520language-specific%2520data%2520is%2520a%2520costly%2520proposition%2520in%2520terms%250Aof%2520data%2520acquisition%2520and%2520computational%2520resources.%2520Our%2520goal%2520is%2520to%2520drastically%250Areduce%2520CPT%2520cost.%2520To%2520that%2520end%252C%2520we%2520first%2520develop%2520a%2520new%2520algorithm%2520to%2520select%2520a%250Asubset%2520of%2520texts%2520from%2520a%2520larger%2520corpus.%2520We%2520show%2520the%2520effectiveness%2520of%2520our%250Atechnique%2520using%2520very%2520little%2520CPT%2520data.%2520In%2520search%2520of%2520further%2520improvement%252C%2520we%250Adesign%2520a%2520new%2520algorithm%2520to%2520select%2520tokens%2520to%2520include%2520in%2520the%2520LLM%2520vocabulary.%2520We%250Aexperiment%2520with%2520the%2520recent%2520Llama-3%2520model%2520and%2520nine%2520Indian%2520languages%2520with%2520diverse%250Ascripts%2520and%2520extent%2520of%2520resource%2520availability.%2520For%2520evaluation%252C%2520we%2520use%250AIndicGenBench%252C%2520a%2520generation%2520task%2520benchmark%2520dataset%2520for%2520Indic%2520languages.%2520We%250Aexperiment%2520with%2520various%2520CPT%2520corpora%2520and%2520augmented%2520vocabulary%2520size%2520and%2520offer%250Ainsights%2520across%2520language%2520families.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Continual%20Pre-training%20of%20LLMs%20for%20Low-resource%20Languages&entry.906535625=Arijit%20Nag%20and%20Soumen%20Chakrabarti%20and%20Animesh%20Mukherjee%20and%20Niloy%20Ganguly&entry.1292438233=%20%20Open-source%20Large%20Language%20models%20%28OsLLMs%29%20propel%20the%20democratization%20of%0Anatural%20language%20research%20by%20giving%20the%20flexibility%20to%20augment%20or%20update%20model%0Aparameters%20for%20performance%20improvement.%20Nevertheless%2C%20like%20proprietary%20LLMs%2C%0AOs-LLMs%20offer%20poorer%20performance%20on%20low-resource%20languages%20%28LRLs%29%20than%0Ahigh-resource%20languages%20%28HRLs%29%2C%20owing%20to%20smaller%20amounts%20of%20training%20data%20and%0Aunderrepresented%20vocabulary.%20On%20the%20other%20hand%2C%20continual%20pre-training%20%28CPT%29%0Awith%20large%20amounts%20of%20language-specific%20data%20is%20a%20costly%20proposition%20in%20terms%0Aof%20data%20acquisition%20and%20computational%20resources.%20Our%20goal%20is%20to%20drastically%0Areduce%20CPT%20cost.%20To%20that%20end%2C%20we%20first%20develop%20a%20new%20algorithm%20to%20select%20a%0Asubset%20of%20texts%20from%20a%20larger%20corpus.%20We%20show%20the%20effectiveness%20of%20our%0Atechnique%20using%20very%20little%20CPT%20data.%20In%20search%20of%20further%20improvement%2C%20we%0Adesign%20a%20new%20algorithm%20to%20select%20tokens%20to%20include%20in%20the%20LLM%20vocabulary.%20We%0Aexperiment%20with%20the%20recent%20Llama-3%20model%20and%20nine%20Indian%20languages%20with%20diverse%0Ascripts%20and%20extent%20of%20resource%20availability.%20For%20evaluation%2C%20we%20use%0AIndicGenBench%2C%20a%20generation%20task%20benchmark%20dataset%20for%20Indic%20languages.%20We%0Aexperiment%20with%20various%20CPT%20corpora%20and%20augmented%20vocabulary%20size%20and%20offer%0Ainsights%20across%20language%20families.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10244v1&entry.124074799=Read"},
{"title": "Multi-Feature Fusion and Compressed Bi-LSTM for Memory-Efficient\n  Heartbeat Classification on Wearable Devices", "author": "Reza Nikandish and Jiayu He and Benyamin Haghi", "abstract": "  In this article, we present a resource-efficient approach for\nelectrocardiogram (ECG) based heartbeat classification using multi-feature\nfusion and bidirectional long short-term memory (Bi-LSTM). The dataset\ncomprises five original classes from the MIT-BIH Arrhythmia Database: Normal\n(N), Left Bundle Branch Block (LBBB), Right Bundle Branch Block (RBBB),\nPremature Ventricular Contraction (PVC), and Paced Beat (PB). Preprocessing\nmethods including the discrete wavelet transform and dual moving average\nwindows are used to reduce noise and artifacts in the raw ECG signal, and\nextract the main points (PQRST) of the ECG waveform. Multi-feature fusion is\nachieved by utilizing time intervals and the proposed under-the-curve areas,\nwhich are inherently robust against noise, as input features. Simulations\ndemonstrated that incorporating under-the-curve area features improved the\nclassification accuracy for the challenging RBBB and LBBB classes from 31.4\\%\nto 84.3\\% for RBBB, and from 69.6\\% to 87.0\\% for LBBB. Using a Bi-LSTM\nnetwork, rather than a conventional LSTM network, resulted in higher accuracy\n(33.8\\% vs 21.8\\%) with a 28\\% reduction in required network parameters for the\nRBBB class. Multiple neural network models with varying parameter sizes,\nincluding tiny (84k), small (150k), medium (478k), and large (1.25M) models,\nare developed to achieve high accuracy \\textit{across all classes}, a more\ncrucial and challenging goal than overall classification accuracy.\n", "link": "http://arxiv.org/abs/2405.15312v2", "date": "2024-12-13", "relevancy": 2.397, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4877}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4806}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Feature%20Fusion%20and%20Compressed%20Bi-LSTM%20for%20Memory-Efficient%0A%20%20Heartbeat%20Classification%20on%20Wearable%20Devices&body=Title%3A%20Multi-Feature%20Fusion%20and%20Compressed%20Bi-LSTM%20for%20Memory-Efficient%0A%20%20Heartbeat%20Classification%20on%20Wearable%20Devices%0AAuthor%3A%20Reza%20Nikandish%20and%20Jiayu%20He%20and%20Benyamin%20Haghi%0AAbstract%3A%20%20%20In%20this%20article%2C%20we%20present%20a%20resource-efficient%20approach%20for%0Aelectrocardiogram%20%28ECG%29%20based%20heartbeat%20classification%20using%20multi-feature%0Afusion%20and%20bidirectional%20long%20short-term%20memory%20%28Bi-LSTM%29.%20The%20dataset%0Acomprises%20five%20original%20classes%20from%20the%20MIT-BIH%20Arrhythmia%20Database%3A%20Normal%0A%28N%29%2C%20Left%20Bundle%20Branch%20Block%20%28LBBB%29%2C%20Right%20Bundle%20Branch%20Block%20%28RBBB%29%2C%0APremature%20Ventricular%20Contraction%20%28PVC%29%2C%20and%20Paced%20Beat%20%28PB%29.%20Preprocessing%0Amethods%20including%20the%20discrete%20wavelet%20transform%20and%20dual%20moving%20average%0Awindows%20are%20used%20to%20reduce%20noise%20and%20artifacts%20in%20the%20raw%20ECG%20signal%2C%20and%0Aextract%20the%20main%20points%20%28PQRST%29%20of%20the%20ECG%20waveform.%20Multi-feature%20fusion%20is%0Aachieved%20by%20utilizing%20time%20intervals%20and%20the%20proposed%20under-the-curve%20areas%2C%0Awhich%20are%20inherently%20robust%20against%20noise%2C%20as%20input%20features.%20Simulations%0Ademonstrated%20that%20incorporating%20under-the-curve%20area%20features%20improved%20the%0Aclassification%20accuracy%20for%20the%20challenging%20RBBB%20and%20LBBB%20classes%20from%2031.4%5C%25%0Ato%2084.3%5C%25%20for%20RBBB%2C%20and%20from%2069.6%5C%25%20to%2087.0%5C%25%20for%20LBBB.%20Using%20a%20Bi-LSTM%0Anetwork%2C%20rather%20than%20a%20conventional%20LSTM%20network%2C%20resulted%20in%20higher%20accuracy%0A%2833.8%5C%25%20vs%2021.8%5C%25%29%20with%20a%2028%5C%25%20reduction%20in%20required%20network%20parameters%20for%20the%0ARBBB%20class.%20Multiple%20neural%20network%20models%20with%20varying%20parameter%20sizes%2C%0Aincluding%20tiny%20%2884k%29%2C%20small%20%28150k%29%2C%20medium%20%28478k%29%2C%20and%20large%20%281.25M%29%20models%2C%0Aare%20developed%20to%20achieve%20high%20accuracy%20%5Ctextit%7Bacross%20all%20classes%7D%2C%20a%20more%0Acrucial%20and%20challenging%20goal%20than%20overall%20classification%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15312v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Feature%2520Fusion%2520and%2520Compressed%2520Bi-LSTM%2520for%2520Memory-Efficient%250A%2520%2520Heartbeat%2520Classification%2520on%2520Wearable%2520Devices%26entry.906535625%3DReza%2520Nikandish%2520and%2520Jiayu%2520He%2520and%2520Benyamin%2520Haghi%26entry.1292438233%3D%2520%2520In%2520this%2520article%252C%2520we%2520present%2520a%2520resource-efficient%2520approach%2520for%250Aelectrocardiogram%2520%2528ECG%2529%2520based%2520heartbeat%2520classification%2520using%2520multi-feature%250Afusion%2520and%2520bidirectional%2520long%2520short-term%2520memory%2520%2528Bi-LSTM%2529.%2520The%2520dataset%250Acomprises%2520five%2520original%2520classes%2520from%2520the%2520MIT-BIH%2520Arrhythmia%2520Database%253A%2520Normal%250A%2528N%2529%252C%2520Left%2520Bundle%2520Branch%2520Block%2520%2528LBBB%2529%252C%2520Right%2520Bundle%2520Branch%2520Block%2520%2528RBBB%2529%252C%250APremature%2520Ventricular%2520Contraction%2520%2528PVC%2529%252C%2520and%2520Paced%2520Beat%2520%2528PB%2529.%2520Preprocessing%250Amethods%2520including%2520the%2520discrete%2520wavelet%2520transform%2520and%2520dual%2520moving%2520average%250Awindows%2520are%2520used%2520to%2520reduce%2520noise%2520and%2520artifacts%2520in%2520the%2520raw%2520ECG%2520signal%252C%2520and%250Aextract%2520the%2520main%2520points%2520%2528PQRST%2529%2520of%2520the%2520ECG%2520waveform.%2520Multi-feature%2520fusion%2520is%250Aachieved%2520by%2520utilizing%2520time%2520intervals%2520and%2520the%2520proposed%2520under-the-curve%2520areas%252C%250Awhich%2520are%2520inherently%2520robust%2520against%2520noise%252C%2520as%2520input%2520features.%2520Simulations%250Ademonstrated%2520that%2520incorporating%2520under-the-curve%2520area%2520features%2520improved%2520the%250Aclassification%2520accuracy%2520for%2520the%2520challenging%2520RBBB%2520and%2520LBBB%2520classes%2520from%252031.4%255C%2525%250Ato%252084.3%255C%2525%2520for%2520RBBB%252C%2520and%2520from%252069.6%255C%2525%2520to%252087.0%255C%2525%2520for%2520LBBB.%2520Using%2520a%2520Bi-LSTM%250Anetwork%252C%2520rather%2520than%2520a%2520conventional%2520LSTM%2520network%252C%2520resulted%2520in%2520higher%2520accuracy%250A%252833.8%255C%2525%2520vs%252021.8%255C%2525%2529%2520with%2520a%252028%255C%2525%2520reduction%2520in%2520required%2520network%2520parameters%2520for%2520the%250ARBBB%2520class.%2520Multiple%2520neural%2520network%2520models%2520with%2520varying%2520parameter%2520sizes%252C%250Aincluding%2520tiny%2520%252884k%2529%252C%2520small%2520%2528150k%2529%252C%2520medium%2520%2528478k%2529%252C%2520and%2520large%2520%25281.25M%2529%2520models%252C%250Aare%2520developed%2520to%2520achieve%2520high%2520accuracy%2520%255Ctextit%257Bacross%2520all%2520classes%257D%252C%2520a%2520more%250Acrucial%2520and%2520challenging%2520goal%2520than%2520overall%2520classification%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15312v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Feature%20Fusion%20and%20Compressed%20Bi-LSTM%20for%20Memory-Efficient%0A%20%20Heartbeat%20Classification%20on%20Wearable%20Devices&entry.906535625=Reza%20Nikandish%20and%20Jiayu%20He%20and%20Benyamin%20Haghi&entry.1292438233=%20%20In%20this%20article%2C%20we%20present%20a%20resource-efficient%20approach%20for%0Aelectrocardiogram%20%28ECG%29%20based%20heartbeat%20classification%20using%20multi-feature%0Afusion%20and%20bidirectional%20long%20short-term%20memory%20%28Bi-LSTM%29.%20The%20dataset%0Acomprises%20five%20original%20classes%20from%20the%20MIT-BIH%20Arrhythmia%20Database%3A%20Normal%0A%28N%29%2C%20Left%20Bundle%20Branch%20Block%20%28LBBB%29%2C%20Right%20Bundle%20Branch%20Block%20%28RBBB%29%2C%0APremature%20Ventricular%20Contraction%20%28PVC%29%2C%20and%20Paced%20Beat%20%28PB%29.%20Preprocessing%0Amethods%20including%20the%20discrete%20wavelet%20transform%20and%20dual%20moving%20average%0Awindows%20are%20used%20to%20reduce%20noise%20and%20artifacts%20in%20the%20raw%20ECG%20signal%2C%20and%0Aextract%20the%20main%20points%20%28PQRST%29%20of%20the%20ECG%20waveform.%20Multi-feature%20fusion%20is%0Aachieved%20by%20utilizing%20time%20intervals%20and%20the%20proposed%20under-the-curve%20areas%2C%0Awhich%20are%20inherently%20robust%20against%20noise%2C%20as%20input%20features.%20Simulations%0Ademonstrated%20that%20incorporating%20under-the-curve%20area%20features%20improved%20the%0Aclassification%20accuracy%20for%20the%20challenging%20RBBB%20and%20LBBB%20classes%20from%2031.4%5C%25%0Ato%2084.3%5C%25%20for%20RBBB%2C%20and%20from%2069.6%5C%25%20to%2087.0%5C%25%20for%20LBBB.%20Using%20a%20Bi-LSTM%0Anetwork%2C%20rather%20than%20a%20conventional%20LSTM%20network%2C%20resulted%20in%20higher%20accuracy%0A%2833.8%5C%25%20vs%2021.8%5C%25%29%20with%20a%2028%5C%25%20reduction%20in%20required%20network%20parameters%20for%20the%0ARBBB%20class.%20Multiple%20neural%20network%20models%20with%20varying%20parameter%20sizes%2C%0Aincluding%20tiny%20%2884k%29%2C%20small%20%28150k%29%2C%20medium%20%28478k%29%2C%20and%20large%20%281.25M%29%20models%2C%0Aare%20developed%20to%20achieve%20high%20accuracy%20%5Ctextit%7Bacross%20all%20classes%7D%2C%20a%20more%0Acrucial%20and%20challenging%20goal%20than%20overall%20classification%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15312v2&entry.124074799=Read"},
{"title": "Enhanced Speech Emotion Recognition with Efficient Channel Attention\n  Guided Deep CNN-BiLSTM Framework", "author": "Niloy Kumar Kundu and Sarah Kobir and Md. Rayhan Ahmed and Tahmina Aktar and Niloya Roy", "abstract": "  Speech emotion recognition (SER) is crucial for enhancing affective computing\nand enriching the domain of human-computer interaction. However, the main\nchallenge in SER lies in selecting relevant feature representations from speech\nsignals with lower computational costs. In this paper, we propose a lightweight\nSER architecture that integrates attention-based local feature blocks (ALFBs)\nto capture high-level relevant feature vectors from speech signals. We also\nincorporate a global feature block (GFB) technique to capture sequential,\nglobal information and long-term dependencies in speech signals. By aggregating\nattention-based local and global contextual feature vectors, our model\neffectively captures the internal correlation between salient features that\nreflect complex human emotional cues. To evaluate our approach, we extracted\nfour types of spectral features from speech audio samples: mel-frequency\ncepstral coefficients, mel-spectrogram, root mean square value, and\nzero-crossing rate. Through a 5-fold cross-validation strategy, we tested the\nproposed method on five multi-lingual standard benchmark datasets: TESS,\nRAVDESS, BanglaSER, SUBESCO, and Emo-DB, and obtained a mean accuracy of\n99.65%, 94.88%, 98.12%, 97.94%, and 97.19% respectively. The results indicate\nthat our model achieves state-of-the-art (SOTA) performance compared to most\nexisting methods.\n", "link": "http://arxiv.org/abs/2412.10011v1", "date": "2024-12-13", "relevancy": 2.3768, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Speech%20Emotion%20Recognition%20with%20Efficient%20Channel%20Attention%0A%20%20Guided%20Deep%20CNN-BiLSTM%20Framework&body=Title%3A%20Enhanced%20Speech%20Emotion%20Recognition%20with%20Efficient%20Channel%20Attention%0A%20%20Guided%20Deep%20CNN-BiLSTM%20Framework%0AAuthor%3A%20Niloy%20Kumar%20Kundu%20and%20Sarah%20Kobir%20and%20Md.%20Rayhan%20Ahmed%20and%20Tahmina%20Aktar%20and%20Niloya%20Roy%0AAbstract%3A%20%20%20Speech%20emotion%20recognition%20%28SER%29%20is%20crucial%20for%20enhancing%20affective%20computing%0Aand%20enriching%20the%20domain%20of%20human-computer%20interaction.%20However%2C%20the%20main%0Achallenge%20in%20SER%20lies%20in%20selecting%20relevant%20feature%20representations%20from%20speech%0Asignals%20with%20lower%20computational%20costs.%20In%20this%20paper%2C%20we%20propose%20a%20lightweight%0ASER%20architecture%20that%20integrates%20attention-based%20local%20feature%20blocks%20%28ALFBs%29%0Ato%20capture%20high-level%20relevant%20feature%20vectors%20from%20speech%20signals.%20We%20also%0Aincorporate%20a%20global%20feature%20block%20%28GFB%29%20technique%20to%20capture%20sequential%2C%0Aglobal%20information%20and%20long-term%20dependencies%20in%20speech%20signals.%20By%20aggregating%0Aattention-based%20local%20and%20global%20contextual%20feature%20vectors%2C%20our%20model%0Aeffectively%20captures%20the%20internal%20correlation%20between%20salient%20features%20that%0Areflect%20complex%20human%20emotional%20cues.%20To%20evaluate%20our%20approach%2C%20we%20extracted%0Afour%20types%20of%20spectral%20features%20from%20speech%20audio%20samples%3A%20mel-frequency%0Acepstral%20coefficients%2C%20mel-spectrogram%2C%20root%20mean%20square%20value%2C%20and%0Azero-crossing%20rate.%20Through%20a%205-fold%20cross-validation%20strategy%2C%20we%20tested%20the%0Aproposed%20method%20on%20five%20multi-lingual%20standard%20benchmark%20datasets%3A%20TESS%2C%0ARAVDESS%2C%20BanglaSER%2C%20SUBESCO%2C%20and%20Emo-DB%2C%20and%20obtained%20a%20mean%20accuracy%20of%0A99.65%25%2C%2094.88%25%2C%2098.12%25%2C%2097.94%25%2C%20and%2097.19%25%20respectively.%20The%20results%20indicate%0Athat%20our%20model%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20compared%20to%20most%0Aexisting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Speech%2520Emotion%2520Recognition%2520with%2520Efficient%2520Channel%2520Attention%250A%2520%2520Guided%2520Deep%2520CNN-BiLSTM%2520Framework%26entry.906535625%3DNiloy%2520Kumar%2520Kundu%2520and%2520Sarah%2520Kobir%2520and%2520Md.%2520Rayhan%2520Ahmed%2520and%2520Tahmina%2520Aktar%2520and%2520Niloya%2520Roy%26entry.1292438233%3D%2520%2520Speech%2520emotion%2520recognition%2520%2528SER%2529%2520is%2520crucial%2520for%2520enhancing%2520affective%2520computing%250Aand%2520enriching%2520the%2520domain%2520of%2520human-computer%2520interaction.%2520However%252C%2520the%2520main%250Achallenge%2520in%2520SER%2520lies%2520in%2520selecting%2520relevant%2520feature%2520representations%2520from%2520speech%250Asignals%2520with%2520lower%2520computational%2520costs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520lightweight%250ASER%2520architecture%2520that%2520integrates%2520attention-based%2520local%2520feature%2520blocks%2520%2528ALFBs%2529%250Ato%2520capture%2520high-level%2520relevant%2520feature%2520vectors%2520from%2520speech%2520signals.%2520We%2520also%250Aincorporate%2520a%2520global%2520feature%2520block%2520%2528GFB%2529%2520technique%2520to%2520capture%2520sequential%252C%250Aglobal%2520information%2520and%2520long-term%2520dependencies%2520in%2520speech%2520signals.%2520By%2520aggregating%250Aattention-based%2520local%2520and%2520global%2520contextual%2520feature%2520vectors%252C%2520our%2520model%250Aeffectively%2520captures%2520the%2520internal%2520correlation%2520between%2520salient%2520features%2520that%250Areflect%2520complex%2520human%2520emotional%2520cues.%2520To%2520evaluate%2520our%2520approach%252C%2520we%2520extracted%250Afour%2520types%2520of%2520spectral%2520features%2520from%2520speech%2520audio%2520samples%253A%2520mel-frequency%250Acepstral%2520coefficients%252C%2520mel-spectrogram%252C%2520root%2520mean%2520square%2520value%252C%2520and%250Azero-crossing%2520rate.%2520Through%2520a%25205-fold%2520cross-validation%2520strategy%252C%2520we%2520tested%2520the%250Aproposed%2520method%2520on%2520five%2520multi-lingual%2520standard%2520benchmark%2520datasets%253A%2520TESS%252C%250ARAVDESS%252C%2520BanglaSER%252C%2520SUBESCO%252C%2520and%2520Emo-DB%252C%2520and%2520obtained%2520a%2520mean%2520accuracy%2520of%250A99.65%2525%252C%252094.88%2525%252C%252098.12%2525%252C%252097.94%2525%252C%2520and%252097.19%2525%2520respectively.%2520The%2520results%2520indicate%250Athat%2520our%2520model%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520compared%2520to%2520most%250Aexisting%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Speech%20Emotion%20Recognition%20with%20Efficient%20Channel%20Attention%0A%20%20Guided%20Deep%20CNN-BiLSTM%20Framework&entry.906535625=Niloy%20Kumar%20Kundu%20and%20Sarah%20Kobir%20and%20Md.%20Rayhan%20Ahmed%20and%20Tahmina%20Aktar%20and%20Niloya%20Roy&entry.1292438233=%20%20Speech%20emotion%20recognition%20%28SER%29%20is%20crucial%20for%20enhancing%20affective%20computing%0Aand%20enriching%20the%20domain%20of%20human-computer%20interaction.%20However%2C%20the%20main%0Achallenge%20in%20SER%20lies%20in%20selecting%20relevant%20feature%20representations%20from%20speech%0Asignals%20with%20lower%20computational%20costs.%20In%20this%20paper%2C%20we%20propose%20a%20lightweight%0ASER%20architecture%20that%20integrates%20attention-based%20local%20feature%20blocks%20%28ALFBs%29%0Ato%20capture%20high-level%20relevant%20feature%20vectors%20from%20speech%20signals.%20We%20also%0Aincorporate%20a%20global%20feature%20block%20%28GFB%29%20technique%20to%20capture%20sequential%2C%0Aglobal%20information%20and%20long-term%20dependencies%20in%20speech%20signals.%20By%20aggregating%0Aattention-based%20local%20and%20global%20contextual%20feature%20vectors%2C%20our%20model%0Aeffectively%20captures%20the%20internal%20correlation%20between%20salient%20features%20that%0Areflect%20complex%20human%20emotional%20cues.%20To%20evaluate%20our%20approach%2C%20we%20extracted%0Afour%20types%20of%20spectral%20features%20from%20speech%20audio%20samples%3A%20mel-frequency%0Acepstral%20coefficients%2C%20mel-spectrogram%2C%20root%20mean%20square%20value%2C%20and%0Azero-crossing%20rate.%20Through%20a%205-fold%20cross-validation%20strategy%2C%20we%20tested%20the%0Aproposed%20method%20on%20five%20multi-lingual%20standard%20benchmark%20datasets%3A%20TESS%2C%0ARAVDESS%2C%20BanglaSER%2C%20SUBESCO%2C%20and%20Emo-DB%2C%20and%20obtained%20a%20mean%20accuracy%20of%0A99.65%25%2C%2094.88%25%2C%2098.12%25%2C%2097.94%25%2C%20and%2097.19%25%20respectively.%20The%20results%20indicate%0Athat%20our%20model%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20compared%20to%20most%0Aexisting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10011v1&entry.124074799=Read"},
{"title": "HS-FPN: High Frequency and Spatial Perception FPN for Tiny Object\n  Detection", "author": "Zican Shi and Jing Hu and Jie Ren and Hengkang Ye and Xuyang Yuan and Yan Ouyang and Jia He and Bo Ji and Junyu Guo", "abstract": "  The introduction of Feature Pyramid Network (FPN) has significantly improved\nobject detection performance. However, substantial challenges remain in\ndetecting tiny objects, as their features occupy only a very small proportion\nof the feature maps. Although FPN integrates multi-scale features, it does not\ndirectly enhance or enrich the features of tiny objects. Furthermore, FPN lacks\nspatial perception ability. To address these issues, we propose a novel High\nFrequency and Spatial Perception Feature Pyramid Network (HS-FPN) with two\ninnovative modules. First, we designed a high frequency perception module (HFP)\nthat generates high frequency responses through high pass filters. These high\nfrequency responses are used as mask weights from both spatial and channel\nperspectives to enrich and highlight the features of tiny objects in the\noriginal feature maps. Second, we developed a spatial dependency perception\nmodule (SDP) to capture the spatial dependencies that FPN lacks. Our\nexperiments demonstrate that detectors based on HS-FPN exhibit competitive\nadvantages over state-of-the-art models on the AI-TOD dataset for tiny object\ndetection.\n", "link": "http://arxiv.org/abs/2412.10116v1", "date": "2024-12-13", "relevancy": 2.3718, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4841}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4755}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HS-FPN%3A%20High%20Frequency%20and%20Spatial%20Perception%20FPN%20for%20Tiny%20Object%0A%20%20Detection&body=Title%3A%20HS-FPN%3A%20High%20Frequency%20and%20Spatial%20Perception%20FPN%20for%20Tiny%20Object%0A%20%20Detection%0AAuthor%3A%20Zican%20Shi%20and%20Jing%20Hu%20and%20Jie%20Ren%20and%20Hengkang%20Ye%20and%20Xuyang%20Yuan%20and%20Yan%20Ouyang%20and%20Jia%20He%20and%20Bo%20Ji%20and%20Junyu%20Guo%0AAbstract%3A%20%20%20The%20introduction%20of%20Feature%20Pyramid%20Network%20%28FPN%29%20has%20significantly%20improved%0Aobject%20detection%20performance.%20However%2C%20substantial%20challenges%20remain%20in%0Adetecting%20tiny%20objects%2C%20as%20their%20features%20occupy%20only%20a%20very%20small%20proportion%0Aof%20the%20feature%20maps.%20Although%20FPN%20integrates%20multi-scale%20features%2C%20it%20does%20not%0Adirectly%20enhance%20or%20enrich%20the%20features%20of%20tiny%20objects.%20Furthermore%2C%20FPN%20lacks%0Aspatial%20perception%20ability.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20High%0AFrequency%20and%20Spatial%20Perception%20Feature%20Pyramid%20Network%20%28HS-FPN%29%20with%20two%0Ainnovative%20modules.%20First%2C%20we%20designed%20a%20high%20frequency%20perception%20module%20%28HFP%29%0Athat%20generates%20high%20frequency%20responses%20through%20high%20pass%20filters.%20These%20high%0Afrequency%20responses%20are%20used%20as%20mask%20weights%20from%20both%20spatial%20and%20channel%0Aperspectives%20to%20enrich%20and%20highlight%20the%20features%20of%20tiny%20objects%20in%20the%0Aoriginal%20feature%20maps.%20Second%2C%20we%20developed%20a%20spatial%20dependency%20perception%0Amodule%20%28SDP%29%20to%20capture%20the%20spatial%20dependencies%20that%20FPN%20lacks.%20Our%0Aexperiments%20demonstrate%20that%20detectors%20based%20on%20HS-FPN%20exhibit%20competitive%0Aadvantages%20over%20state-of-the-art%20models%20on%20the%20AI-TOD%20dataset%20for%20tiny%20object%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHS-FPN%253A%2520High%2520Frequency%2520and%2520Spatial%2520Perception%2520FPN%2520for%2520Tiny%2520Object%250A%2520%2520Detection%26entry.906535625%3DZican%2520Shi%2520and%2520Jing%2520Hu%2520and%2520Jie%2520Ren%2520and%2520Hengkang%2520Ye%2520and%2520Xuyang%2520Yuan%2520and%2520Yan%2520Ouyang%2520and%2520Jia%2520He%2520and%2520Bo%2520Ji%2520and%2520Junyu%2520Guo%26entry.1292438233%3D%2520%2520The%2520introduction%2520of%2520Feature%2520Pyramid%2520Network%2520%2528FPN%2529%2520has%2520significantly%2520improved%250Aobject%2520detection%2520performance.%2520However%252C%2520substantial%2520challenges%2520remain%2520in%250Adetecting%2520tiny%2520objects%252C%2520as%2520their%2520features%2520occupy%2520only%2520a%2520very%2520small%2520proportion%250Aof%2520the%2520feature%2520maps.%2520Although%2520FPN%2520integrates%2520multi-scale%2520features%252C%2520it%2520does%2520not%250Adirectly%2520enhance%2520or%2520enrich%2520the%2520features%2520of%2520tiny%2520objects.%2520Furthermore%252C%2520FPN%2520lacks%250Aspatial%2520perception%2520ability.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520High%250AFrequency%2520and%2520Spatial%2520Perception%2520Feature%2520Pyramid%2520Network%2520%2528HS-FPN%2529%2520with%2520two%250Ainnovative%2520modules.%2520First%252C%2520we%2520designed%2520a%2520high%2520frequency%2520perception%2520module%2520%2528HFP%2529%250Athat%2520generates%2520high%2520frequency%2520responses%2520through%2520high%2520pass%2520filters.%2520These%2520high%250Afrequency%2520responses%2520are%2520used%2520as%2520mask%2520weights%2520from%2520both%2520spatial%2520and%2520channel%250Aperspectives%2520to%2520enrich%2520and%2520highlight%2520the%2520features%2520of%2520tiny%2520objects%2520in%2520the%250Aoriginal%2520feature%2520maps.%2520Second%252C%2520we%2520developed%2520a%2520spatial%2520dependency%2520perception%250Amodule%2520%2528SDP%2529%2520to%2520capture%2520the%2520spatial%2520dependencies%2520that%2520FPN%2520lacks.%2520Our%250Aexperiments%2520demonstrate%2520that%2520detectors%2520based%2520on%2520HS-FPN%2520exhibit%2520competitive%250Aadvantages%2520over%2520state-of-the-art%2520models%2520on%2520the%2520AI-TOD%2520dataset%2520for%2520tiny%2520object%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HS-FPN%3A%20High%20Frequency%20and%20Spatial%20Perception%20FPN%20for%20Tiny%20Object%0A%20%20Detection&entry.906535625=Zican%20Shi%20and%20Jing%20Hu%20and%20Jie%20Ren%20and%20Hengkang%20Ye%20and%20Xuyang%20Yuan%20and%20Yan%20Ouyang%20and%20Jia%20He%20and%20Bo%20Ji%20and%20Junyu%20Guo&entry.1292438233=%20%20The%20introduction%20of%20Feature%20Pyramid%20Network%20%28FPN%29%20has%20significantly%20improved%0Aobject%20detection%20performance.%20However%2C%20substantial%20challenges%20remain%20in%0Adetecting%20tiny%20objects%2C%20as%20their%20features%20occupy%20only%20a%20very%20small%20proportion%0Aof%20the%20feature%20maps.%20Although%20FPN%20integrates%20multi-scale%20features%2C%20it%20does%20not%0Adirectly%20enhance%20or%20enrich%20the%20features%20of%20tiny%20objects.%20Furthermore%2C%20FPN%20lacks%0Aspatial%20perception%20ability.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20High%0AFrequency%20and%20Spatial%20Perception%20Feature%20Pyramid%20Network%20%28HS-FPN%29%20with%20two%0Ainnovative%20modules.%20First%2C%20we%20designed%20a%20high%20frequency%20perception%20module%20%28HFP%29%0Athat%20generates%20high%20frequency%20responses%20through%20high%20pass%20filters.%20These%20high%0Afrequency%20responses%20are%20used%20as%20mask%20weights%20from%20both%20spatial%20and%20channel%0Aperspectives%20to%20enrich%20and%20highlight%20the%20features%20of%20tiny%20objects%20in%20the%0Aoriginal%20feature%20maps.%20Second%2C%20we%20developed%20a%20spatial%20dependency%20perception%0Amodule%20%28SDP%29%20to%20capture%20the%20spatial%20dependencies%20that%20FPN%20lacks.%20Our%0Aexperiments%20demonstrate%20that%20detectors%20based%20on%20HS-FPN%20exhibit%20competitive%0Aadvantages%20over%20state-of-the-art%20models%20on%20the%20AI-TOD%20dataset%20for%20tiny%20object%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10116v1&entry.124074799=Read"},
{"title": "GATEAU: Selecting Influential Sample for Long Context Alignment", "author": "Shuzheng Si and Haozhe Zhao and Gang Chen and Yunshui Li and Kangyang Luo and Chuancheng Lv and Kaikai An and Fanchao Qi and Baobao Chang and Maosong Sun", "abstract": "  Aligning large language models to handle instructions with extremely long\ncontexts has yet to be fully investigated. Previous studies attempt to scale up\nthe available data volume by synthesizing long instruction-following samples,\nas constructing such a dataset tends to be challenging for annotators. However,\na lack of a well-defined strategy for ensuring data quality may introduce\nlow-quality samples and restrict the model performance. Thus, we propose\nGATEAU, a novel framework to address the unique challenge of long context\nalignment by identifying the influential samples enriched with long-range\ndependency relations. Specifically, GATEAU measures the long-range dependencies\nfrom two essential aspects: the difficulty of generating target responses due\nto the long-range dependencies, and the difficulty of understanding long inputs\ndue to such dependencies. Comprehensive experiments indicate that GATEAU\neffectively identifies influential samples and the model trained on these\nselected samples exhibits better instruction-following and long-context\nunderstanding capabilities.\n", "link": "http://arxiv.org/abs/2410.15633v2", "date": "2024-12-13", "relevancy": 2.3713, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4624}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GATEAU%3A%20Selecting%20Influential%20Sample%20for%20Long%20Context%20Alignment&body=Title%3A%20GATEAU%3A%20Selecting%20Influential%20Sample%20for%20Long%20Context%20Alignment%0AAuthor%3A%20Shuzheng%20Si%20and%20Haozhe%20Zhao%20and%20Gang%20Chen%20and%20Yunshui%20Li%20and%20Kangyang%20Luo%20and%20Chuancheng%20Lv%20and%20Kaikai%20An%20and%20Fanchao%20Qi%20and%20Baobao%20Chang%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Aligning%20large%20language%20models%20to%20handle%20instructions%20with%20extremely%20long%0Acontexts%20has%20yet%20to%20be%20fully%20investigated.%20Previous%20studies%20attempt%20to%20scale%20up%0Athe%20available%20data%20volume%20by%20synthesizing%20long%20instruction-following%20samples%2C%0Aas%20constructing%20such%20a%20dataset%20tends%20to%20be%20challenging%20for%20annotators.%20However%2C%0Aa%20lack%20of%20a%20well-defined%20strategy%20for%20ensuring%20data%20quality%20may%20introduce%0Alow-quality%20samples%20and%20restrict%20the%20model%20performance.%20Thus%2C%20we%20propose%0AGATEAU%2C%20a%20novel%20framework%20to%20address%20the%20unique%20challenge%20of%20long%20context%0Aalignment%20by%20identifying%20the%20influential%20samples%20enriched%20with%20long-range%0Adependency%20relations.%20Specifically%2C%20GATEAU%20measures%20the%20long-range%20dependencies%0Afrom%20two%20essential%20aspects%3A%20the%20difficulty%20of%20generating%20target%20responses%20due%0Ato%20the%20long-range%20dependencies%2C%20and%20the%20difficulty%20of%20understanding%20long%20inputs%0Adue%20to%20such%20dependencies.%20Comprehensive%20experiments%20indicate%20that%20GATEAU%0Aeffectively%20identifies%20influential%20samples%20and%20the%20model%20trained%20on%20these%0Aselected%20samples%20exhibits%20better%20instruction-following%20and%20long-context%0Aunderstanding%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15633v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGATEAU%253A%2520Selecting%2520Influential%2520Sample%2520for%2520Long%2520Context%2520Alignment%26entry.906535625%3DShuzheng%2520Si%2520and%2520Haozhe%2520Zhao%2520and%2520Gang%2520Chen%2520and%2520Yunshui%2520Li%2520and%2520Kangyang%2520Luo%2520and%2520Chuancheng%2520Lv%2520and%2520Kaikai%2520An%2520and%2520Fanchao%2520Qi%2520and%2520Baobao%2520Chang%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Aligning%2520large%2520language%2520models%2520to%2520handle%2520instructions%2520with%2520extremely%2520long%250Acontexts%2520has%2520yet%2520to%2520be%2520fully%2520investigated.%2520Previous%2520studies%2520attempt%2520to%2520scale%2520up%250Athe%2520available%2520data%2520volume%2520by%2520synthesizing%2520long%2520instruction-following%2520samples%252C%250Aas%2520constructing%2520such%2520a%2520dataset%2520tends%2520to%2520be%2520challenging%2520for%2520annotators.%2520However%252C%250Aa%2520lack%2520of%2520a%2520well-defined%2520strategy%2520for%2520ensuring%2520data%2520quality%2520may%2520introduce%250Alow-quality%2520samples%2520and%2520restrict%2520the%2520model%2520performance.%2520Thus%252C%2520we%2520propose%250AGATEAU%252C%2520a%2520novel%2520framework%2520to%2520address%2520the%2520unique%2520challenge%2520of%2520long%2520context%250Aalignment%2520by%2520identifying%2520the%2520influential%2520samples%2520enriched%2520with%2520long-range%250Adependency%2520relations.%2520Specifically%252C%2520GATEAU%2520measures%2520the%2520long-range%2520dependencies%250Afrom%2520two%2520essential%2520aspects%253A%2520the%2520difficulty%2520of%2520generating%2520target%2520responses%2520due%250Ato%2520the%2520long-range%2520dependencies%252C%2520and%2520the%2520difficulty%2520of%2520understanding%2520long%2520inputs%250Adue%2520to%2520such%2520dependencies.%2520Comprehensive%2520experiments%2520indicate%2520that%2520GATEAU%250Aeffectively%2520identifies%2520influential%2520samples%2520and%2520the%2520model%2520trained%2520on%2520these%250Aselected%2520samples%2520exhibits%2520better%2520instruction-following%2520and%2520long-context%250Aunderstanding%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15633v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GATEAU%3A%20Selecting%20Influential%20Sample%20for%20Long%20Context%20Alignment&entry.906535625=Shuzheng%20Si%20and%20Haozhe%20Zhao%20and%20Gang%20Chen%20and%20Yunshui%20Li%20and%20Kangyang%20Luo%20and%20Chuancheng%20Lv%20and%20Kaikai%20An%20and%20Fanchao%20Qi%20and%20Baobao%20Chang%20and%20Maosong%20Sun&entry.1292438233=%20%20Aligning%20large%20language%20models%20to%20handle%20instructions%20with%20extremely%20long%0Acontexts%20has%20yet%20to%20be%20fully%20investigated.%20Previous%20studies%20attempt%20to%20scale%20up%0Athe%20available%20data%20volume%20by%20synthesizing%20long%20instruction-following%20samples%2C%0Aas%20constructing%20such%20a%20dataset%20tends%20to%20be%20challenging%20for%20annotators.%20However%2C%0Aa%20lack%20of%20a%20well-defined%20strategy%20for%20ensuring%20data%20quality%20may%20introduce%0Alow-quality%20samples%20and%20restrict%20the%20model%20performance.%20Thus%2C%20we%20propose%0AGATEAU%2C%20a%20novel%20framework%20to%20address%20the%20unique%20challenge%20of%20long%20context%0Aalignment%20by%20identifying%20the%20influential%20samples%20enriched%20with%20long-range%0Adependency%20relations.%20Specifically%2C%20GATEAU%20measures%20the%20long-range%20dependencies%0Afrom%20two%20essential%20aspects%3A%20the%20difficulty%20of%20generating%20target%20responses%20due%0Ato%20the%20long-range%20dependencies%2C%20and%20the%20difficulty%20of%20understanding%20long%20inputs%0Adue%20to%20such%20dependencies.%20Comprehensive%20experiments%20indicate%20that%20GATEAU%0Aeffectively%20identifies%20influential%20samples%20and%20the%20model%20trained%20on%20these%0Aselected%20samples%20exhibits%20better%20instruction-following%20and%20long-context%0Aunderstanding%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15633v2&entry.124074799=Read"},
{"title": "A Survey on Knowledge Graph Structure and Knowledge Graph Embeddings", "author": "Jeffrey Sardina and John D. Kelleher and Declan O'Sullivan", "abstract": "  Knowledge Graphs (KGs) and their machine learning counterpart, Knowledge\nGraph Embedding Models (KGEMs), have seen ever-increasing use in a wide variety\nof academic and applied settings. In particular, KGEMs are typically applied to\nKGs to solve the link prediction task; i.e. to predict new facts in the domain\nof a KG based on existing, observed facts. While this approach has been shown\nsubstantial power in many end-use cases, it remains incompletely characterised\nin terms of how KGEMs react differently to KG structure. This is of particular\nconcern in light of recent studies showing that KG structure can be a\nsignificant source of bias as well as partially determinant of overall KGEM\nperformance. This paper seeks to address this gap in the state-of-the-art. This\npaper provides, to the authors' knowledge, the first comprehensive survey\nexploring established relationships of Knowledge Graph Embedding Models and\nGraph structure in the literature. It is the hope of the authors that this work\nwill inspire further studies in this area, and contribute to a more holistic\nunderstanding of KGs, KGEMs, and the link prediction task.\n", "link": "http://arxiv.org/abs/2412.10092v1", "date": "2024-12-13", "relevancy": 2.37, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Knowledge%20Graph%20Structure%20and%20Knowledge%20Graph%20Embeddings&body=Title%3A%20A%20Survey%20on%20Knowledge%20Graph%20Structure%20and%20Knowledge%20Graph%20Embeddings%0AAuthor%3A%20Jeffrey%20Sardina%20and%20John%20D.%20Kelleher%20and%20Declan%20O%27Sullivan%0AAbstract%3A%20%20%20Knowledge%20Graphs%20%28KGs%29%20and%20their%20machine%20learning%20counterpart%2C%20Knowledge%0AGraph%20Embedding%20Models%20%28KGEMs%29%2C%20have%20seen%20ever-increasing%20use%20in%20a%20wide%20variety%0Aof%20academic%20and%20applied%20settings.%20In%20particular%2C%20KGEMs%20are%20typically%20applied%20to%0AKGs%20to%20solve%20the%20link%20prediction%20task%3B%20i.e.%20to%20predict%20new%20facts%20in%20the%20domain%0Aof%20a%20KG%20based%20on%20existing%2C%20observed%20facts.%20While%20this%20approach%20has%20been%20shown%0Asubstantial%20power%20in%20many%20end-use%20cases%2C%20it%20remains%20incompletely%20characterised%0Ain%20terms%20of%20how%20KGEMs%20react%20differently%20to%20KG%20structure.%20This%20is%20of%20particular%0Aconcern%20in%20light%20of%20recent%20studies%20showing%20that%20KG%20structure%20can%20be%20a%0Asignificant%20source%20of%20bias%20as%20well%20as%20partially%20determinant%20of%20overall%20KGEM%0Aperformance.%20This%20paper%20seeks%20to%20address%20this%20gap%20in%20the%20state-of-the-art.%20This%0Apaper%20provides%2C%20to%20the%20authors%27%20knowledge%2C%20the%20first%20comprehensive%20survey%0Aexploring%20established%20relationships%20of%20Knowledge%20Graph%20Embedding%20Models%20and%0AGraph%20structure%20in%20the%20literature.%20It%20is%20the%20hope%20of%20the%20authors%20that%20this%20work%0Awill%20inspire%20further%20studies%20in%20this%20area%2C%20and%20contribute%20to%20a%20more%20holistic%0Aunderstanding%20of%20KGs%2C%20KGEMs%2C%20and%20the%20link%20prediction%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Knowledge%2520Graph%2520Structure%2520and%2520Knowledge%2520Graph%2520Embeddings%26entry.906535625%3DJeffrey%2520Sardina%2520and%2520John%2520D.%2520Kelleher%2520and%2520Declan%2520O%2527Sullivan%26entry.1292438233%3D%2520%2520Knowledge%2520Graphs%2520%2528KGs%2529%2520and%2520their%2520machine%2520learning%2520counterpart%252C%2520Knowledge%250AGraph%2520Embedding%2520Models%2520%2528KGEMs%2529%252C%2520have%2520seen%2520ever-increasing%2520use%2520in%2520a%2520wide%2520variety%250Aof%2520academic%2520and%2520applied%2520settings.%2520In%2520particular%252C%2520KGEMs%2520are%2520typically%2520applied%2520to%250AKGs%2520to%2520solve%2520the%2520link%2520prediction%2520task%253B%2520i.e.%2520to%2520predict%2520new%2520facts%2520in%2520the%2520domain%250Aof%2520a%2520KG%2520based%2520on%2520existing%252C%2520observed%2520facts.%2520While%2520this%2520approach%2520has%2520been%2520shown%250Asubstantial%2520power%2520in%2520many%2520end-use%2520cases%252C%2520it%2520remains%2520incompletely%2520characterised%250Ain%2520terms%2520of%2520how%2520KGEMs%2520react%2520differently%2520to%2520KG%2520structure.%2520This%2520is%2520of%2520particular%250Aconcern%2520in%2520light%2520of%2520recent%2520studies%2520showing%2520that%2520KG%2520structure%2520can%2520be%2520a%250Asignificant%2520source%2520of%2520bias%2520as%2520well%2520as%2520partially%2520determinant%2520of%2520overall%2520KGEM%250Aperformance.%2520This%2520paper%2520seeks%2520to%2520address%2520this%2520gap%2520in%2520the%2520state-of-the-art.%2520This%250Apaper%2520provides%252C%2520to%2520the%2520authors%2527%2520knowledge%252C%2520the%2520first%2520comprehensive%2520survey%250Aexploring%2520established%2520relationships%2520of%2520Knowledge%2520Graph%2520Embedding%2520Models%2520and%250AGraph%2520structure%2520in%2520the%2520literature.%2520It%2520is%2520the%2520hope%2520of%2520the%2520authors%2520that%2520this%2520work%250Awill%2520inspire%2520further%2520studies%2520in%2520this%2520area%252C%2520and%2520contribute%2520to%2520a%2520more%2520holistic%250Aunderstanding%2520of%2520KGs%252C%2520KGEMs%252C%2520and%2520the%2520link%2520prediction%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Knowledge%20Graph%20Structure%20and%20Knowledge%20Graph%20Embeddings&entry.906535625=Jeffrey%20Sardina%20and%20John%20D.%20Kelleher%20and%20Declan%20O%27Sullivan&entry.1292438233=%20%20Knowledge%20Graphs%20%28KGs%29%20and%20their%20machine%20learning%20counterpart%2C%20Knowledge%0AGraph%20Embedding%20Models%20%28KGEMs%29%2C%20have%20seen%20ever-increasing%20use%20in%20a%20wide%20variety%0Aof%20academic%20and%20applied%20settings.%20In%20particular%2C%20KGEMs%20are%20typically%20applied%20to%0AKGs%20to%20solve%20the%20link%20prediction%20task%3B%20i.e.%20to%20predict%20new%20facts%20in%20the%20domain%0Aof%20a%20KG%20based%20on%20existing%2C%20observed%20facts.%20While%20this%20approach%20has%20been%20shown%0Asubstantial%20power%20in%20many%20end-use%20cases%2C%20it%20remains%20incompletely%20characterised%0Ain%20terms%20of%20how%20KGEMs%20react%20differently%20to%20KG%20structure.%20This%20is%20of%20particular%0Aconcern%20in%20light%20of%20recent%20studies%20showing%20that%20KG%20structure%20can%20be%20a%0Asignificant%20source%20of%20bias%20as%20well%20as%20partially%20determinant%20of%20overall%20KGEM%0Aperformance.%20This%20paper%20seeks%20to%20address%20this%20gap%20in%20the%20state-of-the-art.%20This%0Apaper%20provides%2C%20to%20the%20authors%27%20knowledge%2C%20the%20first%20comprehensive%20survey%0Aexploring%20established%20relationships%20of%20Knowledge%20Graph%20Embedding%20Models%20and%0AGraph%20structure%20in%20the%20literature.%20It%20is%20the%20hope%20of%20the%20authors%20that%20this%20work%0Awill%20inspire%20further%20studies%20in%20this%20area%2C%20and%20contribute%20to%20a%20more%20holistic%0Aunderstanding%20of%20KGs%2C%20KGEMs%2C%20and%20the%20link%20prediction%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10092v1&entry.124074799=Read"},
{"title": "Filter or Compensate: Towards Invariant Representation from Distribution\n  Shift for Anomaly Detection", "author": "Zining Chen and Xingshuang Luo and Weiqiu Wang and Zhicheng Zhao and Fei Su and Aidong Men", "abstract": "  Recent Anomaly Detection (AD) methods have achieved great success with\nIn-Distribution (ID) data. However, real-world data often exhibits distribution\nshift, causing huge performance decay on traditional AD methods. From this\nperspective, few previous work has explored AD with distribution shift, and the\ndistribution-invariant normality learning has been proposed based on the\nReverse Distillation (RD) framework. However, we observe the misalignment issue\nbetween the teacher and the student network that causes detection failure,\nthereby propose FiCo, Filter or Compensate, to address the distribution shift\nissue in AD. FiCo firstly compensates the distribution-specific information to\nreduce the misalignment between the teacher and student network via the\nDistribution-Specific Compensation (DiSCo) module, and secondly filters all\nabnormal information to capture distribution-invariant normality with the\nDistribution-Invariant Filter (DiIFi) module. Extensive experiments on three\ndifferent AD benchmarks demonstrate the effectiveness of FiCo, which\noutperforms all existing state-of-the-art (SOTA) methods, and even achieves\nbetter results on the ID scenario compared with RD-based methods. Our code is\navailable at https://github.com/znchen666/FiCo.\n", "link": "http://arxiv.org/abs/2412.10115v1", "date": "2024-12-13", "relevancy": 2.3679, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4781}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4732}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Filter%20or%20Compensate%3A%20Towards%20Invariant%20Representation%20from%20Distribution%0A%20%20Shift%20for%20Anomaly%20Detection&body=Title%3A%20Filter%20or%20Compensate%3A%20Towards%20Invariant%20Representation%20from%20Distribution%0A%20%20Shift%20for%20Anomaly%20Detection%0AAuthor%3A%20Zining%20Chen%20and%20Xingshuang%20Luo%20and%20Weiqiu%20Wang%20and%20Zhicheng%20Zhao%20and%20Fei%20Su%20and%20Aidong%20Men%0AAbstract%3A%20%20%20Recent%20Anomaly%20Detection%20%28AD%29%20methods%20have%20achieved%20great%20success%20with%0AIn-Distribution%20%28ID%29%20data.%20However%2C%20real-world%20data%20often%20exhibits%20distribution%0Ashift%2C%20causing%20huge%20performance%20decay%20on%20traditional%20AD%20methods.%20From%20this%0Aperspective%2C%20few%20previous%20work%20has%20explored%20AD%20with%20distribution%20shift%2C%20and%20the%0Adistribution-invariant%20normality%20learning%20has%20been%20proposed%20based%20on%20the%0AReverse%20Distillation%20%28RD%29%20framework.%20However%2C%20we%20observe%20the%20misalignment%20issue%0Abetween%20the%20teacher%20and%20the%20student%20network%20that%20causes%20detection%20failure%2C%0Athereby%20propose%20FiCo%2C%20Filter%20or%20Compensate%2C%20to%20address%20the%20distribution%20shift%0Aissue%20in%20AD.%20FiCo%20firstly%20compensates%20the%20distribution-specific%20information%20to%0Areduce%20the%20misalignment%20between%20the%20teacher%20and%20student%20network%20via%20the%0ADistribution-Specific%20Compensation%20%28DiSCo%29%20module%2C%20and%20secondly%20filters%20all%0Aabnormal%20information%20to%20capture%20distribution-invariant%20normality%20with%20the%0ADistribution-Invariant%20Filter%20%28DiIFi%29%20module.%20Extensive%20experiments%20on%20three%0Adifferent%20AD%20benchmarks%20demonstrate%20the%20effectiveness%20of%20FiCo%2C%20which%0Aoutperforms%20all%20existing%20state-of-the-art%20%28SOTA%29%20methods%2C%20and%20even%20achieves%0Abetter%20results%20on%20the%20ID%20scenario%20compared%20with%20RD-based%20methods.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/znchen666/FiCo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10115v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFilter%2520or%2520Compensate%253A%2520Towards%2520Invariant%2520Representation%2520from%2520Distribution%250A%2520%2520Shift%2520for%2520Anomaly%2520Detection%26entry.906535625%3DZining%2520Chen%2520and%2520Xingshuang%2520Luo%2520and%2520Weiqiu%2520Wang%2520and%2520Zhicheng%2520Zhao%2520and%2520Fei%2520Su%2520and%2520Aidong%2520Men%26entry.1292438233%3D%2520%2520Recent%2520Anomaly%2520Detection%2520%2528AD%2529%2520methods%2520have%2520achieved%2520great%2520success%2520with%250AIn-Distribution%2520%2528ID%2529%2520data.%2520However%252C%2520real-world%2520data%2520often%2520exhibits%2520distribution%250Ashift%252C%2520causing%2520huge%2520performance%2520decay%2520on%2520traditional%2520AD%2520methods.%2520From%2520this%250Aperspective%252C%2520few%2520previous%2520work%2520has%2520explored%2520AD%2520with%2520distribution%2520shift%252C%2520and%2520the%250Adistribution-invariant%2520normality%2520learning%2520has%2520been%2520proposed%2520based%2520on%2520the%250AReverse%2520Distillation%2520%2528RD%2529%2520framework.%2520However%252C%2520we%2520observe%2520the%2520misalignment%2520issue%250Abetween%2520the%2520teacher%2520and%2520the%2520student%2520network%2520that%2520causes%2520detection%2520failure%252C%250Athereby%2520propose%2520FiCo%252C%2520Filter%2520or%2520Compensate%252C%2520to%2520address%2520the%2520distribution%2520shift%250Aissue%2520in%2520AD.%2520FiCo%2520firstly%2520compensates%2520the%2520distribution-specific%2520information%2520to%250Areduce%2520the%2520misalignment%2520between%2520the%2520teacher%2520and%2520student%2520network%2520via%2520the%250ADistribution-Specific%2520Compensation%2520%2528DiSCo%2529%2520module%252C%2520and%2520secondly%2520filters%2520all%250Aabnormal%2520information%2520to%2520capture%2520distribution-invariant%2520normality%2520with%2520the%250ADistribution-Invariant%2520Filter%2520%2528DiIFi%2529%2520module.%2520Extensive%2520experiments%2520on%2520three%250Adifferent%2520AD%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520FiCo%252C%2520which%250Aoutperforms%2520all%2520existing%2520state-of-the-art%2520%2528SOTA%2529%2520methods%252C%2520and%2520even%2520achieves%250Abetter%2520results%2520on%2520the%2520ID%2520scenario%2520compared%2520with%2520RD-based%2520methods.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/znchen666/FiCo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10115v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Filter%20or%20Compensate%3A%20Towards%20Invariant%20Representation%20from%20Distribution%0A%20%20Shift%20for%20Anomaly%20Detection&entry.906535625=Zining%20Chen%20and%20Xingshuang%20Luo%20and%20Weiqiu%20Wang%20and%20Zhicheng%20Zhao%20and%20Fei%20Su%20and%20Aidong%20Men&entry.1292438233=%20%20Recent%20Anomaly%20Detection%20%28AD%29%20methods%20have%20achieved%20great%20success%20with%0AIn-Distribution%20%28ID%29%20data.%20However%2C%20real-world%20data%20often%20exhibits%20distribution%0Ashift%2C%20causing%20huge%20performance%20decay%20on%20traditional%20AD%20methods.%20From%20this%0Aperspective%2C%20few%20previous%20work%20has%20explored%20AD%20with%20distribution%20shift%2C%20and%20the%0Adistribution-invariant%20normality%20learning%20has%20been%20proposed%20based%20on%20the%0AReverse%20Distillation%20%28RD%29%20framework.%20However%2C%20we%20observe%20the%20misalignment%20issue%0Abetween%20the%20teacher%20and%20the%20student%20network%20that%20causes%20detection%20failure%2C%0Athereby%20propose%20FiCo%2C%20Filter%20or%20Compensate%2C%20to%20address%20the%20distribution%20shift%0Aissue%20in%20AD.%20FiCo%20firstly%20compensates%20the%20distribution-specific%20information%20to%0Areduce%20the%20misalignment%20between%20the%20teacher%20and%20student%20network%20via%20the%0ADistribution-Specific%20Compensation%20%28DiSCo%29%20module%2C%20and%20secondly%20filters%20all%0Aabnormal%20information%20to%20capture%20distribution-invariant%20normality%20with%20the%0ADistribution-Invariant%20Filter%20%28DiIFi%29%20module.%20Extensive%20experiments%20on%20three%0Adifferent%20AD%20benchmarks%20demonstrate%20the%20effectiveness%20of%20FiCo%2C%20which%0Aoutperforms%20all%20existing%20state-of-the-art%20%28SOTA%29%20methods%2C%20and%20even%20achieves%0Abetter%20results%20on%20the%20ID%20scenario%20compared%20with%20RD-based%20methods.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/znchen666/FiCo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10115v1&entry.124074799=Read"},
{"title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through\n  Tree Planning", "author": "Xiang Li and Yunshi Lan and Chao Yang", "abstract": "  Recently, numerous new benchmarks have been established to evaluate the\nperformance of large language models (LLMs) via either computing a holistic\nscore or employing another LLM as a judge. However, these approaches suffer\nfrom data leakage due to the open access of the benchmark and inflexible\nevaluation process. To address this issue, we introduce $\\textbf{TreeEval}$, a\nbenchmark-free evaluation method for LLMs that let a high-performance LLM host\nan irreproducible evaluation session and essentially avoids the data leakage.\nMoreover, this LLM performs as an examiner to raise up a series of questions\nunder a topic with a tree planing strategy, which considers the current\nevaluation status to decide the next question generation and ensures the\ncompleteness and efficiency of the evaluation process. We evaluate $6$ models\nof different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately\nachieved the highest correlation coefficient with AlpacaEval2.0 using only\naround $45$ questions. We also conduct more analysis to show the robustness and\nreliability of TreeEval. Our code can be accessed via the provided\nhttps://github.com/Ashura5/TreeEval.\n", "link": "http://arxiv.org/abs/2402.13125v2", "date": "2024-12-13", "relevancy": 2.3666, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TreeEval%3A%20Benchmark-Free%20Evaluation%20of%20Large%20Language%20Models%20through%0A%20%20Tree%20Planning&body=Title%3A%20TreeEval%3A%20Benchmark-Free%20Evaluation%20of%20Large%20Language%20Models%20through%0A%20%20Tree%20Planning%0AAuthor%3A%20Xiang%20Li%20and%20Yunshi%20Lan%20and%20Chao%20Yang%0AAbstract%3A%20%20%20Recently%2C%20numerous%20new%20benchmarks%20have%20been%20established%20to%20evaluate%20the%0Aperformance%20of%20large%20language%20models%20%28LLMs%29%20via%20either%20computing%20a%20holistic%0Ascore%20or%20employing%20another%20LLM%20as%20a%20judge.%20However%2C%20these%20approaches%20suffer%0Afrom%20data%20leakage%20due%20to%20the%20open%20access%20of%20the%20benchmark%20and%20inflexible%0Aevaluation%20process.%20To%20address%20this%20issue%2C%20we%20introduce%20%24%5Ctextbf%7BTreeEval%7D%24%2C%20a%0Abenchmark-free%20evaluation%20method%20for%20LLMs%20that%20let%20a%20high-performance%20LLM%20host%0Aan%20irreproducible%20evaluation%20session%20and%20essentially%20avoids%20the%20data%20leakage.%0AMoreover%2C%20this%20LLM%20performs%20as%20an%20examiner%20to%20raise%20up%20a%20series%20of%20questions%0Aunder%20a%20topic%20with%20a%20tree%20planing%20strategy%2C%20which%20considers%20the%20current%0Aevaluation%20status%20to%20decide%20the%20next%20question%20generation%20and%20ensures%20the%0Acompleteness%20and%20efficiency%20of%20the%20evaluation%20process.%20We%20evaluate%20%246%24%20models%0Aof%20different%20parameter%20sizes%2C%20including%20%247%24B%2C%20%2413%24B%2C%20and%20%2433%24B%2C%20and%20ultimately%0Aachieved%20the%20highest%20correlation%20coefficient%20with%20AlpacaEval2.0%20using%20only%0Aaround%20%2445%24%20questions.%20We%20also%20conduct%20more%20analysis%20to%20show%20the%20robustness%20and%0Areliability%20of%20TreeEval.%20Our%20code%20can%20be%20accessed%20via%20the%20provided%0Ahttps%3A//github.com/Ashura5/TreeEval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13125v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTreeEval%253A%2520Benchmark-Free%2520Evaluation%2520of%2520Large%2520Language%2520Models%2520through%250A%2520%2520Tree%2520Planning%26entry.906535625%3DXiang%2520Li%2520and%2520Yunshi%2520Lan%2520and%2520Chao%2520Yang%26entry.1292438233%3D%2520%2520Recently%252C%2520numerous%2520new%2520benchmarks%2520have%2520been%2520established%2520to%2520evaluate%2520the%250Aperformance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520via%2520either%2520computing%2520a%2520holistic%250Ascore%2520or%2520employing%2520another%2520LLM%2520as%2520a%2520judge.%2520However%252C%2520these%2520approaches%2520suffer%250Afrom%2520data%2520leakage%2520due%2520to%2520the%2520open%2520access%2520of%2520the%2520benchmark%2520and%2520inflexible%250Aevaluation%2520process.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520%2524%255Ctextbf%257BTreeEval%257D%2524%252C%2520a%250Abenchmark-free%2520evaluation%2520method%2520for%2520LLMs%2520that%2520let%2520a%2520high-performance%2520LLM%2520host%250Aan%2520irreproducible%2520evaluation%2520session%2520and%2520essentially%2520avoids%2520the%2520data%2520leakage.%250AMoreover%252C%2520this%2520LLM%2520performs%2520as%2520an%2520examiner%2520to%2520raise%2520up%2520a%2520series%2520of%2520questions%250Aunder%2520a%2520topic%2520with%2520a%2520tree%2520planing%2520strategy%252C%2520which%2520considers%2520the%2520current%250Aevaluation%2520status%2520to%2520decide%2520the%2520next%2520question%2520generation%2520and%2520ensures%2520the%250Acompleteness%2520and%2520efficiency%2520of%2520the%2520evaluation%2520process.%2520We%2520evaluate%2520%25246%2524%2520models%250Aof%2520different%2520parameter%2520sizes%252C%2520including%2520%25247%2524B%252C%2520%252413%2524B%252C%2520and%2520%252433%2524B%252C%2520and%2520ultimately%250Aachieved%2520the%2520highest%2520correlation%2520coefficient%2520with%2520AlpacaEval2.0%2520using%2520only%250Aaround%2520%252445%2524%2520questions.%2520We%2520also%2520conduct%2520more%2520analysis%2520to%2520show%2520the%2520robustness%2520and%250Areliability%2520of%2520TreeEval.%2520Our%2520code%2520can%2520be%2520accessed%2520via%2520the%2520provided%250Ahttps%253A//github.com/Ashura5/TreeEval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13125v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TreeEval%3A%20Benchmark-Free%20Evaluation%20of%20Large%20Language%20Models%20through%0A%20%20Tree%20Planning&entry.906535625=Xiang%20Li%20and%20Yunshi%20Lan%20and%20Chao%20Yang&entry.1292438233=%20%20Recently%2C%20numerous%20new%20benchmarks%20have%20been%20established%20to%20evaluate%20the%0Aperformance%20of%20large%20language%20models%20%28LLMs%29%20via%20either%20computing%20a%20holistic%0Ascore%20or%20employing%20another%20LLM%20as%20a%20judge.%20However%2C%20these%20approaches%20suffer%0Afrom%20data%20leakage%20due%20to%20the%20open%20access%20of%20the%20benchmark%20and%20inflexible%0Aevaluation%20process.%20To%20address%20this%20issue%2C%20we%20introduce%20%24%5Ctextbf%7BTreeEval%7D%24%2C%20a%0Abenchmark-free%20evaluation%20method%20for%20LLMs%20that%20let%20a%20high-performance%20LLM%20host%0Aan%20irreproducible%20evaluation%20session%20and%20essentially%20avoids%20the%20data%20leakage.%0AMoreover%2C%20this%20LLM%20performs%20as%20an%20examiner%20to%20raise%20up%20a%20series%20of%20questions%0Aunder%20a%20topic%20with%20a%20tree%20planing%20strategy%2C%20which%20considers%20the%20current%0Aevaluation%20status%20to%20decide%20the%20next%20question%20generation%20and%20ensures%20the%0Acompleteness%20and%20efficiency%20of%20the%20evaluation%20process.%20We%20evaluate%20%246%24%20models%0Aof%20different%20parameter%20sizes%2C%20including%20%247%24B%2C%20%2413%24B%2C%20and%20%2433%24B%2C%20and%20ultimately%0Aachieved%20the%20highest%20correlation%20coefficient%20with%20AlpacaEval2.0%20using%20only%0Aaround%20%2445%24%20questions.%20We%20also%20conduct%20more%20analysis%20to%20show%20the%20robustness%20and%0Areliability%20of%20TreeEval.%20Our%20code%20can%20be%20accessed%20via%20the%20provided%0Ahttps%3A//github.com/Ashura5/TreeEval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13125v2&entry.124074799=Read"},
{"title": "Data Pruning Can Do More: A Comprehensive Data Pruning Approach for\n  Object Re-identification", "author": "Zi Yang and Haojin Yang and Soumajit Majumder and Jorge Cardoso and Guillermo Gallego", "abstract": "  Previous studies have demonstrated that not each sample in a dataset is of\nequal importance during training. Data pruning aims to remove less important or\ninformative samples while still achieving comparable results as training on the\noriginal (untruncated) dataset, thereby reducing storage and training costs.\nHowever, the majority of data pruning methods are applied to image\nclassification tasks. To our knowledge, this work is the first to explore the\nfeasibility of these pruning methods applied to object re-identification (ReID)\ntasks, while also presenting a more comprehensive data pruning approach. By\nfully leveraging the logit history during training, our approach offers a more\naccurate and comprehensive metric for quantifying sample importance, as well as\ncorrecting mislabeled samples and recognizing outliers. Furthermore, our\napproach is highly efficient, reducing the cost of importance score estimation\nby 10 times compared to existing methods. Our approach is a plug-and-play,\narchitecture-agnostic framework that can eliminate/reduce 35%, 30%, and 5% of\nsamples/training time on the VeRi, MSMT17 and Market1501 datasets,\nrespectively, with negligible loss in accuracy (< 0.1%). The lists of\nimportant, mislabeled, and outlier samples from these ReID datasets are\navailable at https://github.com/Zi-Y/data-pruning-reid.\n", "link": "http://arxiv.org/abs/2412.10091v1", "date": "2024-12-13", "relevancy": 2.3648, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4876}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4746}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Pruning%20Can%20Do%20More%3A%20A%20Comprehensive%20Data%20Pruning%20Approach%20for%0A%20%20Object%20Re-identification&body=Title%3A%20Data%20Pruning%20Can%20Do%20More%3A%20A%20Comprehensive%20Data%20Pruning%20Approach%20for%0A%20%20Object%20Re-identification%0AAuthor%3A%20Zi%20Yang%20and%20Haojin%20Yang%20and%20Soumajit%20Majumder%20and%20Jorge%20Cardoso%20and%20Guillermo%20Gallego%0AAbstract%3A%20%20%20Previous%20studies%20have%20demonstrated%20that%20not%20each%20sample%20in%20a%20dataset%20is%20of%0Aequal%20importance%20during%20training.%20Data%20pruning%20aims%20to%20remove%20less%20important%20or%0Ainformative%20samples%20while%20still%20achieving%20comparable%20results%20as%20training%20on%20the%0Aoriginal%20%28untruncated%29%20dataset%2C%20thereby%20reducing%20storage%20and%20training%20costs.%0AHowever%2C%20the%20majority%20of%20data%20pruning%20methods%20are%20applied%20to%20image%0Aclassification%20tasks.%20To%20our%20knowledge%2C%20this%20work%20is%20the%20first%20to%20explore%20the%0Afeasibility%20of%20these%20pruning%20methods%20applied%20to%20object%20re-identification%20%28ReID%29%0Atasks%2C%20while%20also%20presenting%20a%20more%20comprehensive%20data%20pruning%20approach.%20By%0Afully%20leveraging%20the%20logit%20history%20during%20training%2C%20our%20approach%20offers%20a%20more%0Aaccurate%20and%20comprehensive%20metric%20for%20quantifying%20sample%20importance%2C%20as%20well%20as%0Acorrecting%20mislabeled%20samples%20and%20recognizing%20outliers.%20Furthermore%2C%20our%0Aapproach%20is%20highly%20efficient%2C%20reducing%20the%20cost%20of%20importance%20score%20estimation%0Aby%2010%20times%20compared%20to%20existing%20methods.%20Our%20approach%20is%20a%20plug-and-play%2C%0Aarchitecture-agnostic%20framework%20that%20can%20eliminate/reduce%2035%25%2C%2030%25%2C%20and%205%25%20of%0Asamples/training%20time%20on%20the%20VeRi%2C%20MSMT17%20and%20Market1501%20datasets%2C%0Arespectively%2C%20with%20negligible%20loss%20in%20accuracy%20%28%3C%200.1%25%29.%20The%20lists%20of%0Aimportant%2C%20mislabeled%2C%20and%20outlier%20samples%20from%20these%20ReID%20datasets%20are%0Aavailable%20at%20https%3A//github.com/Zi-Y/data-pruning-reid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Pruning%2520Can%2520Do%2520More%253A%2520A%2520Comprehensive%2520Data%2520Pruning%2520Approach%2520for%250A%2520%2520Object%2520Re-identification%26entry.906535625%3DZi%2520Yang%2520and%2520Haojin%2520Yang%2520and%2520Soumajit%2520Majumder%2520and%2520Jorge%2520Cardoso%2520and%2520Guillermo%2520Gallego%26entry.1292438233%3D%2520%2520Previous%2520studies%2520have%2520demonstrated%2520that%2520not%2520each%2520sample%2520in%2520a%2520dataset%2520is%2520of%250Aequal%2520importance%2520during%2520training.%2520Data%2520pruning%2520aims%2520to%2520remove%2520less%2520important%2520or%250Ainformative%2520samples%2520while%2520still%2520achieving%2520comparable%2520results%2520as%2520training%2520on%2520the%250Aoriginal%2520%2528untruncated%2529%2520dataset%252C%2520thereby%2520reducing%2520storage%2520and%2520training%2520costs.%250AHowever%252C%2520the%2520majority%2520of%2520data%2520pruning%2520methods%2520are%2520applied%2520to%2520image%250Aclassification%2520tasks.%2520To%2520our%2520knowledge%252C%2520this%2520work%2520is%2520the%2520first%2520to%2520explore%2520the%250Afeasibility%2520of%2520these%2520pruning%2520methods%2520applied%2520to%2520object%2520re-identification%2520%2528ReID%2529%250Atasks%252C%2520while%2520also%2520presenting%2520a%2520more%2520comprehensive%2520data%2520pruning%2520approach.%2520By%250Afully%2520leveraging%2520the%2520logit%2520history%2520during%2520training%252C%2520our%2520approach%2520offers%2520a%2520more%250Aaccurate%2520and%2520comprehensive%2520metric%2520for%2520quantifying%2520sample%2520importance%252C%2520as%2520well%2520as%250Acorrecting%2520mislabeled%2520samples%2520and%2520recognizing%2520outliers.%2520Furthermore%252C%2520our%250Aapproach%2520is%2520highly%2520efficient%252C%2520reducing%2520the%2520cost%2520of%2520importance%2520score%2520estimation%250Aby%252010%2520times%2520compared%2520to%2520existing%2520methods.%2520Our%2520approach%2520is%2520a%2520plug-and-play%252C%250Aarchitecture-agnostic%2520framework%2520that%2520can%2520eliminate/reduce%252035%2525%252C%252030%2525%252C%2520and%25205%2525%2520of%250Asamples/training%2520time%2520on%2520the%2520VeRi%252C%2520MSMT17%2520and%2520Market1501%2520datasets%252C%250Arespectively%252C%2520with%2520negligible%2520loss%2520in%2520accuracy%2520%2528%253C%25200.1%2525%2529.%2520The%2520lists%2520of%250Aimportant%252C%2520mislabeled%252C%2520and%2520outlier%2520samples%2520from%2520these%2520ReID%2520datasets%2520are%250Aavailable%2520at%2520https%253A//github.com/Zi-Y/data-pruning-reid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Pruning%20Can%20Do%20More%3A%20A%20Comprehensive%20Data%20Pruning%20Approach%20for%0A%20%20Object%20Re-identification&entry.906535625=Zi%20Yang%20and%20Haojin%20Yang%20and%20Soumajit%20Majumder%20and%20Jorge%20Cardoso%20and%20Guillermo%20Gallego&entry.1292438233=%20%20Previous%20studies%20have%20demonstrated%20that%20not%20each%20sample%20in%20a%20dataset%20is%20of%0Aequal%20importance%20during%20training.%20Data%20pruning%20aims%20to%20remove%20less%20important%20or%0Ainformative%20samples%20while%20still%20achieving%20comparable%20results%20as%20training%20on%20the%0Aoriginal%20%28untruncated%29%20dataset%2C%20thereby%20reducing%20storage%20and%20training%20costs.%0AHowever%2C%20the%20majority%20of%20data%20pruning%20methods%20are%20applied%20to%20image%0Aclassification%20tasks.%20To%20our%20knowledge%2C%20this%20work%20is%20the%20first%20to%20explore%20the%0Afeasibility%20of%20these%20pruning%20methods%20applied%20to%20object%20re-identification%20%28ReID%29%0Atasks%2C%20while%20also%20presenting%20a%20more%20comprehensive%20data%20pruning%20approach.%20By%0Afully%20leveraging%20the%20logit%20history%20during%20training%2C%20our%20approach%20offers%20a%20more%0Aaccurate%20and%20comprehensive%20metric%20for%20quantifying%20sample%20importance%2C%20as%20well%20as%0Acorrecting%20mislabeled%20samples%20and%20recognizing%20outliers.%20Furthermore%2C%20our%0Aapproach%20is%20highly%20efficient%2C%20reducing%20the%20cost%20of%20importance%20score%20estimation%0Aby%2010%20times%20compared%20to%20existing%20methods.%20Our%20approach%20is%20a%20plug-and-play%2C%0Aarchitecture-agnostic%20framework%20that%20can%20eliminate/reduce%2035%25%2C%2030%25%2C%20and%205%25%20of%0Asamples/training%20time%20on%20the%20VeRi%2C%20MSMT17%20and%20Market1501%20datasets%2C%0Arespectively%2C%20with%20negligible%20loss%20in%20accuracy%20%28%3C%200.1%25%29.%20The%20lists%20of%0Aimportant%2C%20mislabeled%2C%20and%20outlier%20samples%20from%20these%20ReID%20datasets%20are%0Aavailable%20at%20https%3A//github.com/Zi-Y/data-pruning-reid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10091v1&entry.124074799=Read"},
{"title": "ProbeSDF: Light Field Probes for Neural Surface Reconstruction", "author": "Briac Toussaint and Diego Thomas and Jean-S\u00e9bastien Franco", "abstract": "  SDF-based differential rendering frameworks have achieved state-of-the-art\nmultiview 3D shape reconstruction. In this work, we re-examine this family of\napproaches by minimally reformulating its core appearance model in a way that\nsimultaneously yields faster computation and increased performance. To this\ngoal, we exhibit a physically-inspired minimal radiance parametrization\ndecoupling angular and spatial contributions, by encoding them with a small\nnumber of features stored in two respective volumetric grids of different\nresolutions. Requiring as little as four parameters per voxel, and a tiny MLP\ncall inside a single fully fused kernel, our approach allows to enhance\nperformance with both surface and image (PSNR) metrics, while providing a\nsignificant training speedup and real-time rendering. We show this performance\nto be consistently achieved on real data over two widely different and popular\napplication fields, generic object and human subject shape reconstruction,\nusing four representative and challenging datasets.\n", "link": "http://arxiv.org/abs/2412.10084v1", "date": "2024-12-13", "relevancy": 2.3335, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5842}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5842}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProbeSDF%3A%20Light%20Field%20Probes%20for%20Neural%20Surface%20Reconstruction&body=Title%3A%20ProbeSDF%3A%20Light%20Field%20Probes%20for%20Neural%20Surface%20Reconstruction%0AAuthor%3A%20Briac%20Toussaint%20and%20Diego%20Thomas%20and%20Jean-S%C3%A9bastien%20Franco%0AAbstract%3A%20%20%20SDF-based%20differential%20rendering%20frameworks%20have%20achieved%20state-of-the-art%0Amultiview%203D%20shape%20reconstruction.%20In%20this%20work%2C%20we%20re-examine%20this%20family%20of%0Aapproaches%20by%20minimally%20reformulating%20its%20core%20appearance%20model%20in%20a%20way%20that%0Asimultaneously%20yields%20faster%20computation%20and%20increased%20performance.%20To%20this%0Agoal%2C%20we%20exhibit%20a%20physically-inspired%20minimal%20radiance%20parametrization%0Adecoupling%20angular%20and%20spatial%20contributions%2C%20by%20encoding%20them%20with%20a%20small%0Anumber%20of%20features%20stored%20in%20two%20respective%20volumetric%20grids%20of%20different%0Aresolutions.%20Requiring%20as%20little%20as%20four%20parameters%20per%20voxel%2C%20and%20a%20tiny%20MLP%0Acall%20inside%20a%20single%20fully%20fused%20kernel%2C%20our%20approach%20allows%20to%20enhance%0Aperformance%20with%20both%20surface%20and%20image%20%28PSNR%29%20metrics%2C%20while%20providing%20a%0Asignificant%20training%20speedup%20and%20real-time%20rendering.%20We%20show%20this%20performance%0Ato%20be%20consistently%20achieved%20on%20real%20data%20over%20two%20widely%20different%20and%20popular%0Aapplication%20fields%2C%20generic%20object%20and%20human%20subject%20shape%20reconstruction%2C%0Ausing%20four%20representative%20and%20challenging%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbeSDF%253A%2520Light%2520Field%2520Probes%2520for%2520Neural%2520Surface%2520Reconstruction%26entry.906535625%3DBriac%2520Toussaint%2520and%2520Diego%2520Thomas%2520and%2520Jean-S%25C3%25A9bastien%2520Franco%26entry.1292438233%3D%2520%2520SDF-based%2520differential%2520rendering%2520frameworks%2520have%2520achieved%2520state-of-the-art%250Amultiview%25203D%2520shape%2520reconstruction.%2520In%2520this%2520work%252C%2520we%2520re-examine%2520this%2520family%2520of%250Aapproaches%2520by%2520minimally%2520reformulating%2520its%2520core%2520appearance%2520model%2520in%2520a%2520way%2520that%250Asimultaneously%2520yields%2520faster%2520computation%2520and%2520increased%2520performance.%2520To%2520this%250Agoal%252C%2520we%2520exhibit%2520a%2520physically-inspired%2520minimal%2520radiance%2520parametrization%250Adecoupling%2520angular%2520and%2520spatial%2520contributions%252C%2520by%2520encoding%2520them%2520with%2520a%2520small%250Anumber%2520of%2520features%2520stored%2520in%2520two%2520respective%2520volumetric%2520grids%2520of%2520different%250Aresolutions.%2520Requiring%2520as%2520little%2520as%2520four%2520parameters%2520per%2520voxel%252C%2520and%2520a%2520tiny%2520MLP%250Acall%2520inside%2520a%2520single%2520fully%2520fused%2520kernel%252C%2520our%2520approach%2520allows%2520to%2520enhance%250Aperformance%2520with%2520both%2520surface%2520and%2520image%2520%2528PSNR%2529%2520metrics%252C%2520while%2520providing%2520a%250Asignificant%2520training%2520speedup%2520and%2520real-time%2520rendering.%2520We%2520show%2520this%2520performance%250Ato%2520be%2520consistently%2520achieved%2520on%2520real%2520data%2520over%2520two%2520widely%2520different%2520and%2520popular%250Aapplication%2520fields%252C%2520generic%2520object%2520and%2520human%2520subject%2520shape%2520reconstruction%252C%250Ausing%2520four%2520representative%2520and%2520challenging%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProbeSDF%3A%20Light%20Field%20Probes%20for%20Neural%20Surface%20Reconstruction&entry.906535625=Briac%20Toussaint%20and%20Diego%20Thomas%20and%20Jean-S%C3%A9bastien%20Franco&entry.1292438233=%20%20SDF-based%20differential%20rendering%20frameworks%20have%20achieved%20state-of-the-art%0Amultiview%203D%20shape%20reconstruction.%20In%20this%20work%2C%20we%20re-examine%20this%20family%20of%0Aapproaches%20by%20minimally%20reformulating%20its%20core%20appearance%20model%20in%20a%20way%20that%0Asimultaneously%20yields%20faster%20computation%20and%20increased%20performance.%20To%20this%0Agoal%2C%20we%20exhibit%20a%20physically-inspired%20minimal%20radiance%20parametrization%0Adecoupling%20angular%20and%20spatial%20contributions%2C%20by%20encoding%20them%20with%20a%20small%0Anumber%20of%20features%20stored%20in%20two%20respective%20volumetric%20grids%20of%20different%0Aresolutions.%20Requiring%20as%20little%20as%20four%20parameters%20per%20voxel%2C%20and%20a%20tiny%20MLP%0Acall%20inside%20a%20single%20fully%20fused%20kernel%2C%20our%20approach%20allows%20to%20enhance%0Aperformance%20with%20both%20surface%20and%20image%20%28PSNR%29%20metrics%2C%20while%20providing%20a%0Asignificant%20training%20speedup%20and%20real-time%20rendering.%20We%20show%20this%20performance%0Ato%20be%20consistently%20achieved%20on%20real%20data%20over%20two%20widely%20different%20and%20popular%0Aapplication%20fields%2C%20generic%20object%20and%20human%20subject%20shape%20reconstruction%2C%0Ausing%20four%20representative%20and%20challenging%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10084v1&entry.124074799=Read"},
{"title": "UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for\n  Diverse Medical Imaging Modalities", "author": "Muhammad Uzair Khattak and Shahina Kunhimon and Muzammal Naseer and Salman Khan and Fahad Shahbaz Khan", "abstract": "  Vision-Language Models (VLMs) trained via contrastive learning have achieved\nnotable success in natural image tasks. However, their application in the\nmedical domain remains limited due to the scarcity of openly accessible,\nlarge-scale medical image-text datasets. Existing medical VLMs either train on\nclosed-source proprietary or relatively small open-source datasets that do not\ngeneralize well. Similarly, most models remain specific to a single or limited\nnumber of medical imaging domains, again restricting their applicability to\nother modalities. To address this gap, we introduce UniMed, a large-scale,\nopen-source multi-modal medical dataset comprising over 5.3 million image-text\npairs across six diverse imaging modalities: X-ray, CT, MRI, Ultrasound,\nPathology, and Fundus. UniMed is developed using a data-collection framework\nthat leverages Large Language Models (LLMs) to transform modality-specific\nclassification datasets into image-text formats while incorporating existing\nimage-text data from the medical domain, facilitating scalable VLM pretraining.\nUsing UniMed, we trained UniMed-CLIP, a unified VLM for six modalities that\nsignificantly outperforms existing generalist VLMs and matches\nmodality-specific medical VLMs, achieving notable gains in zero-shot\nevaluations. For instance, UniMed-CLIP improves over BiomedCLIP (trained on\nproprietary data) by an absolute gain of +12.61, averaged over 21 datasets,\nwhile using 3x less training data. To facilitate future research, we release\nUniMed dataset, training codes, and models at\nhttps://github.com/mbzuai-oryx/UniMed-CLIP.\n", "link": "http://arxiv.org/abs/2412.10372v1", "date": "2024-12-13", "relevancy": 2.3282, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.625}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5702}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniMed-CLIP%3A%20Towards%20a%20Unified%20Image-Text%20Pretraining%20Paradigm%20for%0A%20%20Diverse%20Medical%20Imaging%20Modalities&body=Title%3A%20UniMed-CLIP%3A%20Towards%20a%20Unified%20Image-Text%20Pretraining%20Paradigm%20for%0A%20%20Diverse%20Medical%20Imaging%20Modalities%0AAuthor%3A%20Muhammad%20Uzair%20Khattak%20and%20Shahina%20Kunhimon%20and%20Muzammal%20Naseer%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20trained%20via%20contrastive%20learning%20have%20achieved%0Anotable%20success%20in%20natural%20image%20tasks.%20However%2C%20their%20application%20in%20the%0Amedical%20domain%20remains%20limited%20due%20to%20the%20scarcity%20of%20openly%20accessible%2C%0Alarge-scale%20medical%20image-text%20datasets.%20Existing%20medical%20VLMs%20either%20train%20on%0Aclosed-source%20proprietary%20or%20relatively%20small%20open-source%20datasets%20that%20do%20not%0Ageneralize%20well.%20Similarly%2C%20most%20models%20remain%20specific%20to%20a%20single%20or%20limited%0Anumber%20of%20medical%20imaging%20domains%2C%20again%20restricting%20their%20applicability%20to%0Aother%20modalities.%20To%20address%20this%20gap%2C%20we%20introduce%20UniMed%2C%20a%20large-scale%2C%0Aopen-source%20multi-modal%20medical%20dataset%20comprising%20over%205.3%20million%20image-text%0Apairs%20across%20six%20diverse%20imaging%20modalities%3A%20X-ray%2C%20CT%2C%20MRI%2C%20Ultrasound%2C%0APathology%2C%20and%20Fundus.%20UniMed%20is%20developed%20using%20a%20data-collection%20framework%0Athat%20leverages%20Large%20Language%20Models%20%28LLMs%29%20to%20transform%20modality-specific%0Aclassification%20datasets%20into%20image-text%20formats%20while%20incorporating%20existing%0Aimage-text%20data%20from%20the%20medical%20domain%2C%20facilitating%20scalable%20VLM%20pretraining.%0AUsing%20UniMed%2C%20we%20trained%20UniMed-CLIP%2C%20a%20unified%20VLM%20for%20six%20modalities%20that%0Asignificantly%20outperforms%20existing%20generalist%20VLMs%20and%20matches%0Amodality-specific%20medical%20VLMs%2C%20achieving%20notable%20gains%20in%20zero-shot%0Aevaluations.%20For%20instance%2C%20UniMed-CLIP%20improves%20over%20BiomedCLIP%20%28trained%20on%0Aproprietary%20data%29%20by%20an%20absolute%20gain%20of%20%2B12.61%2C%20averaged%20over%2021%20datasets%2C%0Awhile%20using%203x%20less%20training%20data.%20To%20facilitate%20future%20research%2C%20we%20release%0AUniMed%20dataset%2C%20training%20codes%2C%20and%20models%20at%0Ahttps%3A//github.com/mbzuai-oryx/UniMed-CLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniMed-CLIP%253A%2520Towards%2520a%2520Unified%2520Image-Text%2520Pretraining%2520Paradigm%2520for%250A%2520%2520Diverse%2520Medical%2520Imaging%2520Modalities%26entry.906535625%3DMuhammad%2520Uzair%2520Khattak%2520and%2520Shahina%2520Kunhimon%2520and%2520Muzammal%2520Naseer%2520and%2520Salman%2520Khan%2520and%2520Fahad%2520Shahbaz%2520Khan%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520trained%2520via%2520contrastive%2520learning%2520have%2520achieved%250Anotable%2520success%2520in%2520natural%2520image%2520tasks.%2520However%252C%2520their%2520application%2520in%2520the%250Amedical%2520domain%2520remains%2520limited%2520due%2520to%2520the%2520scarcity%2520of%2520openly%2520accessible%252C%250Alarge-scale%2520medical%2520image-text%2520datasets.%2520Existing%2520medical%2520VLMs%2520either%2520train%2520on%250Aclosed-source%2520proprietary%2520or%2520relatively%2520small%2520open-source%2520datasets%2520that%2520do%2520not%250Ageneralize%2520well.%2520Similarly%252C%2520most%2520models%2520remain%2520specific%2520to%2520a%2520single%2520or%2520limited%250Anumber%2520of%2520medical%2520imaging%2520domains%252C%2520again%2520restricting%2520their%2520applicability%2520to%250Aother%2520modalities.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520UniMed%252C%2520a%2520large-scale%252C%250Aopen-source%2520multi-modal%2520medical%2520dataset%2520comprising%2520over%25205.3%2520million%2520image-text%250Apairs%2520across%2520six%2520diverse%2520imaging%2520modalities%253A%2520X-ray%252C%2520CT%252C%2520MRI%252C%2520Ultrasound%252C%250APathology%252C%2520and%2520Fundus.%2520UniMed%2520is%2520developed%2520using%2520a%2520data-collection%2520framework%250Athat%2520leverages%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520transform%2520modality-specific%250Aclassification%2520datasets%2520into%2520image-text%2520formats%2520while%2520incorporating%2520existing%250Aimage-text%2520data%2520from%2520the%2520medical%2520domain%252C%2520facilitating%2520scalable%2520VLM%2520pretraining.%250AUsing%2520UniMed%252C%2520we%2520trained%2520UniMed-CLIP%252C%2520a%2520unified%2520VLM%2520for%2520six%2520modalities%2520that%250Asignificantly%2520outperforms%2520existing%2520generalist%2520VLMs%2520and%2520matches%250Amodality-specific%2520medical%2520VLMs%252C%2520achieving%2520notable%2520gains%2520in%2520zero-shot%250Aevaluations.%2520For%2520instance%252C%2520UniMed-CLIP%2520improves%2520over%2520BiomedCLIP%2520%2528trained%2520on%250Aproprietary%2520data%2529%2520by%2520an%2520absolute%2520gain%2520of%2520%252B12.61%252C%2520averaged%2520over%252021%2520datasets%252C%250Awhile%2520using%25203x%2520less%2520training%2520data.%2520To%2520facilitate%2520future%2520research%252C%2520we%2520release%250AUniMed%2520dataset%252C%2520training%2520codes%252C%2520and%2520models%2520at%250Ahttps%253A//github.com/mbzuai-oryx/UniMed-CLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniMed-CLIP%3A%20Towards%20a%20Unified%20Image-Text%20Pretraining%20Paradigm%20for%0A%20%20Diverse%20Medical%20Imaging%20Modalities&entry.906535625=Muhammad%20Uzair%20Khattak%20and%20Shahina%20Kunhimon%20and%20Muzammal%20Naseer%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20trained%20via%20contrastive%20learning%20have%20achieved%0Anotable%20success%20in%20natural%20image%20tasks.%20However%2C%20their%20application%20in%20the%0Amedical%20domain%20remains%20limited%20due%20to%20the%20scarcity%20of%20openly%20accessible%2C%0Alarge-scale%20medical%20image-text%20datasets.%20Existing%20medical%20VLMs%20either%20train%20on%0Aclosed-source%20proprietary%20or%20relatively%20small%20open-source%20datasets%20that%20do%20not%0Ageneralize%20well.%20Similarly%2C%20most%20models%20remain%20specific%20to%20a%20single%20or%20limited%0Anumber%20of%20medical%20imaging%20domains%2C%20again%20restricting%20their%20applicability%20to%0Aother%20modalities.%20To%20address%20this%20gap%2C%20we%20introduce%20UniMed%2C%20a%20large-scale%2C%0Aopen-source%20multi-modal%20medical%20dataset%20comprising%20over%205.3%20million%20image-text%0Apairs%20across%20six%20diverse%20imaging%20modalities%3A%20X-ray%2C%20CT%2C%20MRI%2C%20Ultrasound%2C%0APathology%2C%20and%20Fundus.%20UniMed%20is%20developed%20using%20a%20data-collection%20framework%0Athat%20leverages%20Large%20Language%20Models%20%28LLMs%29%20to%20transform%20modality-specific%0Aclassification%20datasets%20into%20image-text%20formats%20while%20incorporating%20existing%0Aimage-text%20data%20from%20the%20medical%20domain%2C%20facilitating%20scalable%20VLM%20pretraining.%0AUsing%20UniMed%2C%20we%20trained%20UniMed-CLIP%2C%20a%20unified%20VLM%20for%20six%20modalities%20that%0Asignificantly%20outperforms%20existing%20generalist%20VLMs%20and%20matches%0Amodality-specific%20medical%20VLMs%2C%20achieving%20notable%20gains%20in%20zero-shot%0Aevaluations.%20For%20instance%2C%20UniMed-CLIP%20improves%20over%20BiomedCLIP%20%28trained%20on%0Aproprietary%20data%29%20by%20an%20absolute%20gain%20of%20%2B12.61%2C%20averaged%20over%2021%20datasets%2C%0Awhile%20using%203x%20less%20training%20data.%20To%20facilitate%20future%20research%2C%20we%20release%0AUniMed%20dataset%2C%20training%20codes%2C%20and%20models%20at%0Ahttps%3A//github.com/mbzuai-oryx/UniMed-CLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10372v1&entry.124074799=Read"},
{"title": "Robust Monocular Visual Odometry using Curriculum Learning", "author": "Assaf Lahiany and Oren Gal", "abstract": "  Curriculum Learning (CL), drawing inspiration from natural learning patterns\nobserved in humans and animals, employs a systematic approach of gradually\nintroducing increasingly complex training data during model development. Our\nwork applies innovative CL methodologies to address the challenging geometric\nproblem of monocular Visual Odometry (VO) estimation, which is essential for\nrobot navigation in constrained environments. The primary objective of our\nresearch is to push the boundaries of current state-of-the-art (SOTA)\nbenchmarks in monocular VO by investigating various curriculum learning\nstrategies. We enhance the end-to-end Deep-Patch-Visual Odometry (DPVO)\nframework through the integration of novel CL approaches, with the goal of\ndeveloping more resilient models capable of maintaining high performance across\nchallenging environments and complex motion scenarios. Our research encompasses\nseveral distinctive CL strategies. We develop methods to evaluate sample\ndifficulty based on trajectory motion characteristics, implement sophisticated\nadaptive scheduling through self-paced weighted loss mechanisms, and utilize\nreinforcement learning agents for dynamic adjustment of training emphasis.\nThrough comprehensive evaluation on the diverse synthetic TartanAir dataset and\ncomplex real-world benchmarks such as EuRoC and TUM-RGBD, our Curriculum\nLearning-based Deep-Patch-Visual Odometry (CL-DPVO) demonstrates superior\nperformance compared to existing SOTA methods, including both feature-based and\nlearning-based VO approaches. The results validate the effectiveness of\nintegrating curriculum learning principles into visual odometry systems.\n", "link": "http://arxiv.org/abs/2411.13438v2", "date": "2024-12-13", "relevancy": 2.3174, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5872}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5797}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Monocular%20Visual%20Odometry%20using%20Curriculum%20Learning&body=Title%3A%20Robust%20Monocular%20Visual%20Odometry%20using%20Curriculum%20Learning%0AAuthor%3A%20Assaf%20Lahiany%20and%20Oren%20Gal%0AAbstract%3A%20%20%20Curriculum%20Learning%20%28CL%29%2C%20drawing%20inspiration%20from%20natural%20learning%20patterns%0Aobserved%20in%20humans%20and%20animals%2C%20employs%20a%20systematic%20approach%20of%20gradually%0Aintroducing%20increasingly%20complex%20training%20data%20during%20model%20development.%20Our%0Awork%20applies%20innovative%20CL%20methodologies%20to%20address%20the%20challenging%20geometric%0Aproblem%20of%20monocular%20Visual%20Odometry%20%28VO%29%20estimation%2C%20which%20is%20essential%20for%0Arobot%20navigation%20in%20constrained%20environments.%20The%20primary%20objective%20of%20our%0Aresearch%20is%20to%20push%20the%20boundaries%20of%20current%20state-of-the-art%20%28SOTA%29%0Abenchmarks%20in%20monocular%20VO%20by%20investigating%20various%20curriculum%20learning%0Astrategies.%20We%20enhance%20the%20end-to-end%20Deep-Patch-Visual%20Odometry%20%28DPVO%29%0Aframework%20through%20the%20integration%20of%20novel%20CL%20approaches%2C%20with%20the%20goal%20of%0Adeveloping%20more%20resilient%20models%20capable%20of%20maintaining%20high%20performance%20across%0Achallenging%20environments%20and%20complex%20motion%20scenarios.%20Our%20research%20encompasses%0Aseveral%20distinctive%20CL%20strategies.%20We%20develop%20methods%20to%20evaluate%20sample%0Adifficulty%20based%20on%20trajectory%20motion%20characteristics%2C%20implement%20sophisticated%0Aadaptive%20scheduling%20through%20self-paced%20weighted%20loss%20mechanisms%2C%20and%20utilize%0Areinforcement%20learning%20agents%20for%20dynamic%20adjustment%20of%20training%20emphasis.%0AThrough%20comprehensive%20evaluation%20on%20the%20diverse%20synthetic%20TartanAir%20dataset%20and%0Acomplex%20real-world%20benchmarks%20such%20as%20EuRoC%20and%20TUM-RGBD%2C%20our%20Curriculum%0ALearning-based%20Deep-Patch-Visual%20Odometry%20%28CL-DPVO%29%20demonstrates%20superior%0Aperformance%20compared%20to%20existing%20SOTA%20methods%2C%20including%20both%20feature-based%20and%0Alearning-based%20VO%20approaches.%20The%20results%20validate%20the%20effectiveness%20of%0Aintegrating%20curriculum%20learning%20principles%20into%20visual%20odometry%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13438v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Monocular%2520Visual%2520Odometry%2520using%2520Curriculum%2520Learning%26entry.906535625%3DAssaf%2520Lahiany%2520and%2520Oren%2520Gal%26entry.1292438233%3D%2520%2520Curriculum%2520Learning%2520%2528CL%2529%252C%2520drawing%2520inspiration%2520from%2520natural%2520learning%2520patterns%250Aobserved%2520in%2520humans%2520and%2520animals%252C%2520employs%2520a%2520systematic%2520approach%2520of%2520gradually%250Aintroducing%2520increasingly%2520complex%2520training%2520data%2520during%2520model%2520development.%2520Our%250Awork%2520applies%2520innovative%2520CL%2520methodologies%2520to%2520address%2520the%2520challenging%2520geometric%250Aproblem%2520of%2520monocular%2520Visual%2520Odometry%2520%2528VO%2529%2520estimation%252C%2520which%2520is%2520essential%2520for%250Arobot%2520navigation%2520in%2520constrained%2520environments.%2520The%2520primary%2520objective%2520of%2520our%250Aresearch%2520is%2520to%2520push%2520the%2520boundaries%2520of%2520current%2520state-of-the-art%2520%2528SOTA%2529%250Abenchmarks%2520in%2520monocular%2520VO%2520by%2520investigating%2520various%2520curriculum%2520learning%250Astrategies.%2520We%2520enhance%2520the%2520end-to-end%2520Deep-Patch-Visual%2520Odometry%2520%2528DPVO%2529%250Aframework%2520through%2520the%2520integration%2520of%2520novel%2520CL%2520approaches%252C%2520with%2520the%2520goal%2520of%250Adeveloping%2520more%2520resilient%2520models%2520capable%2520of%2520maintaining%2520high%2520performance%2520across%250Achallenging%2520environments%2520and%2520complex%2520motion%2520scenarios.%2520Our%2520research%2520encompasses%250Aseveral%2520distinctive%2520CL%2520strategies.%2520We%2520develop%2520methods%2520to%2520evaluate%2520sample%250Adifficulty%2520based%2520on%2520trajectory%2520motion%2520characteristics%252C%2520implement%2520sophisticated%250Aadaptive%2520scheduling%2520through%2520self-paced%2520weighted%2520loss%2520mechanisms%252C%2520and%2520utilize%250Areinforcement%2520learning%2520agents%2520for%2520dynamic%2520adjustment%2520of%2520training%2520emphasis.%250AThrough%2520comprehensive%2520evaluation%2520on%2520the%2520diverse%2520synthetic%2520TartanAir%2520dataset%2520and%250Acomplex%2520real-world%2520benchmarks%2520such%2520as%2520EuRoC%2520and%2520TUM-RGBD%252C%2520our%2520Curriculum%250ALearning-based%2520Deep-Patch-Visual%2520Odometry%2520%2528CL-DPVO%2529%2520demonstrates%2520superior%250Aperformance%2520compared%2520to%2520existing%2520SOTA%2520methods%252C%2520including%2520both%2520feature-based%2520and%250Alearning-based%2520VO%2520approaches.%2520The%2520results%2520validate%2520the%2520effectiveness%2520of%250Aintegrating%2520curriculum%2520learning%2520principles%2520into%2520visual%2520odometry%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13438v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Monocular%20Visual%20Odometry%20using%20Curriculum%20Learning&entry.906535625=Assaf%20Lahiany%20and%20Oren%20Gal&entry.1292438233=%20%20Curriculum%20Learning%20%28CL%29%2C%20drawing%20inspiration%20from%20natural%20learning%20patterns%0Aobserved%20in%20humans%20and%20animals%2C%20employs%20a%20systematic%20approach%20of%20gradually%0Aintroducing%20increasingly%20complex%20training%20data%20during%20model%20development.%20Our%0Awork%20applies%20innovative%20CL%20methodologies%20to%20address%20the%20challenging%20geometric%0Aproblem%20of%20monocular%20Visual%20Odometry%20%28VO%29%20estimation%2C%20which%20is%20essential%20for%0Arobot%20navigation%20in%20constrained%20environments.%20The%20primary%20objective%20of%20our%0Aresearch%20is%20to%20push%20the%20boundaries%20of%20current%20state-of-the-art%20%28SOTA%29%0Abenchmarks%20in%20monocular%20VO%20by%20investigating%20various%20curriculum%20learning%0Astrategies.%20We%20enhance%20the%20end-to-end%20Deep-Patch-Visual%20Odometry%20%28DPVO%29%0Aframework%20through%20the%20integration%20of%20novel%20CL%20approaches%2C%20with%20the%20goal%20of%0Adeveloping%20more%20resilient%20models%20capable%20of%20maintaining%20high%20performance%20across%0Achallenging%20environments%20and%20complex%20motion%20scenarios.%20Our%20research%20encompasses%0Aseveral%20distinctive%20CL%20strategies.%20We%20develop%20methods%20to%20evaluate%20sample%0Adifficulty%20based%20on%20trajectory%20motion%20characteristics%2C%20implement%20sophisticated%0Aadaptive%20scheduling%20through%20self-paced%20weighted%20loss%20mechanisms%2C%20and%20utilize%0Areinforcement%20learning%20agents%20for%20dynamic%20adjustment%20of%20training%20emphasis.%0AThrough%20comprehensive%20evaluation%20on%20the%20diverse%20synthetic%20TartanAir%20dataset%20and%0Acomplex%20real-world%20benchmarks%20such%20as%20EuRoC%20and%20TUM-RGBD%2C%20our%20Curriculum%0ALearning-based%20Deep-Patch-Visual%20Odometry%20%28CL-DPVO%29%20demonstrates%20superior%0Aperformance%20compared%20to%20existing%20SOTA%20methods%2C%20including%20both%20feature-based%20and%0Alearning-based%20VO%20approaches.%20The%20results%20validate%20the%20effectiveness%20of%0Aintegrating%20curriculum%20learning%20principles%20into%20visual%20odometry%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13438v2&entry.124074799=Read"},
{"title": "Sims: An Interactive Tool for Geospatial Matching and Clustering", "author": "Akram Zaytar and Girmaw Abebe Tadesse and Caleb Robinson and Eduardo G. Bendito and Medha Devare and Meklit Chernet and Gilles Q. Hacheme and Rahul Dodhia and Juan M. Lavista Ferres", "abstract": "  Acquiring, processing, and visualizing geospatial data requires significant\ncomputing resources, especially for large spatio-temporal domains. This\nchallenge hinders the rapid discovery of predictive features, which is\nessential for advancing geospatial modeling. To address this, we developed\nSimilarity Search (Sims), a no-code web tool that allows users to visualize,\ncompare, cluster, and perform similarity search over defined regions of\ninterest using Google Earth Engine as a backend. Sims is designed to complement\nexisting modeling tools by focusing on feature exploration rather than model\ncreation. We demonstrate the utility of Sims through a case study analyzing\nsimulated maize yield data in Rwanda, where we evaluate how different\ncombinations of soil, weather, and agronomic features affect the clustering of\nyield response zones. Sims is open source and available at\nhttps://github.com/microsoft/Sims\n", "link": "http://arxiv.org/abs/2412.10184v1", "date": "2024-12-13", "relevancy": 2.2984, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4678}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4595}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sims%3A%20An%20Interactive%20Tool%20for%20Geospatial%20Matching%20and%20Clustering&body=Title%3A%20Sims%3A%20An%20Interactive%20Tool%20for%20Geospatial%20Matching%20and%20Clustering%0AAuthor%3A%20Akram%20Zaytar%20and%20Girmaw%20Abebe%20Tadesse%20and%20Caleb%20Robinson%20and%20Eduardo%20G.%20Bendito%20and%20Medha%20Devare%20and%20Meklit%20Chernet%20and%20Gilles%20Q.%20Hacheme%20and%20Rahul%20Dodhia%20and%20Juan%20M.%20Lavista%20Ferres%0AAbstract%3A%20%20%20Acquiring%2C%20processing%2C%20and%20visualizing%20geospatial%20data%20requires%20significant%0Acomputing%20resources%2C%20especially%20for%20large%20spatio-temporal%20domains.%20This%0Achallenge%20hinders%20the%20rapid%20discovery%20of%20predictive%20features%2C%20which%20is%0Aessential%20for%20advancing%20geospatial%20modeling.%20To%20address%20this%2C%20we%20developed%0ASimilarity%20Search%20%28Sims%29%2C%20a%20no-code%20web%20tool%20that%20allows%20users%20to%20visualize%2C%0Acompare%2C%20cluster%2C%20and%20perform%20similarity%20search%20over%20defined%20regions%20of%0Ainterest%20using%20Google%20Earth%20Engine%20as%20a%20backend.%20Sims%20is%20designed%20to%20complement%0Aexisting%20modeling%20tools%20by%20focusing%20on%20feature%20exploration%20rather%20than%20model%0Acreation.%20We%20demonstrate%20the%20utility%20of%20Sims%20through%20a%20case%20study%20analyzing%0Asimulated%20maize%20yield%20data%20in%20Rwanda%2C%20where%20we%20evaluate%20how%20different%0Acombinations%20of%20soil%2C%20weather%2C%20and%20agronomic%20features%20affect%20the%20clustering%20of%0Ayield%20response%20zones.%20Sims%20is%20open%20source%20and%20available%20at%0Ahttps%3A//github.com/microsoft/Sims%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10184v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSims%253A%2520An%2520Interactive%2520Tool%2520for%2520Geospatial%2520Matching%2520and%2520Clustering%26entry.906535625%3DAkram%2520Zaytar%2520and%2520Girmaw%2520Abebe%2520Tadesse%2520and%2520Caleb%2520Robinson%2520and%2520Eduardo%2520G.%2520Bendito%2520and%2520Medha%2520Devare%2520and%2520Meklit%2520Chernet%2520and%2520Gilles%2520Q.%2520Hacheme%2520and%2520Rahul%2520Dodhia%2520and%2520Juan%2520M.%2520Lavista%2520Ferres%26entry.1292438233%3D%2520%2520Acquiring%252C%2520processing%252C%2520and%2520visualizing%2520geospatial%2520data%2520requires%2520significant%250Acomputing%2520resources%252C%2520especially%2520for%2520large%2520spatio-temporal%2520domains.%2520This%250Achallenge%2520hinders%2520the%2520rapid%2520discovery%2520of%2520predictive%2520features%252C%2520which%2520is%250Aessential%2520for%2520advancing%2520geospatial%2520modeling.%2520To%2520address%2520this%252C%2520we%2520developed%250ASimilarity%2520Search%2520%2528Sims%2529%252C%2520a%2520no-code%2520web%2520tool%2520that%2520allows%2520users%2520to%2520visualize%252C%250Acompare%252C%2520cluster%252C%2520and%2520perform%2520similarity%2520search%2520over%2520defined%2520regions%2520of%250Ainterest%2520using%2520Google%2520Earth%2520Engine%2520as%2520a%2520backend.%2520Sims%2520is%2520designed%2520to%2520complement%250Aexisting%2520modeling%2520tools%2520by%2520focusing%2520on%2520feature%2520exploration%2520rather%2520than%2520model%250Acreation.%2520We%2520demonstrate%2520the%2520utility%2520of%2520Sims%2520through%2520a%2520case%2520study%2520analyzing%250Asimulated%2520maize%2520yield%2520data%2520in%2520Rwanda%252C%2520where%2520we%2520evaluate%2520how%2520different%250Acombinations%2520of%2520soil%252C%2520weather%252C%2520and%2520agronomic%2520features%2520affect%2520the%2520clustering%2520of%250Ayield%2520response%2520zones.%2520Sims%2520is%2520open%2520source%2520and%2520available%2520at%250Ahttps%253A//github.com/microsoft/Sims%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10184v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sims%3A%20An%20Interactive%20Tool%20for%20Geospatial%20Matching%20and%20Clustering&entry.906535625=Akram%20Zaytar%20and%20Girmaw%20Abebe%20Tadesse%20and%20Caleb%20Robinson%20and%20Eduardo%20G.%20Bendito%20and%20Medha%20Devare%20and%20Meklit%20Chernet%20and%20Gilles%20Q.%20Hacheme%20and%20Rahul%20Dodhia%20and%20Juan%20M.%20Lavista%20Ferres&entry.1292438233=%20%20Acquiring%2C%20processing%2C%20and%20visualizing%20geospatial%20data%20requires%20significant%0Acomputing%20resources%2C%20especially%20for%20large%20spatio-temporal%20domains.%20This%0Achallenge%20hinders%20the%20rapid%20discovery%20of%20predictive%20features%2C%20which%20is%0Aessential%20for%20advancing%20geospatial%20modeling.%20To%20address%20this%2C%20we%20developed%0ASimilarity%20Search%20%28Sims%29%2C%20a%20no-code%20web%20tool%20that%20allows%20users%20to%20visualize%2C%0Acompare%2C%20cluster%2C%20and%20perform%20similarity%20search%20over%20defined%20regions%20of%0Ainterest%20using%20Google%20Earth%20Engine%20as%20a%20backend.%20Sims%20is%20designed%20to%20complement%0Aexisting%20modeling%20tools%20by%20focusing%20on%20feature%20exploration%20rather%20than%20model%0Acreation.%20We%20demonstrate%20the%20utility%20of%20Sims%20through%20a%20case%20study%20analyzing%0Asimulated%20maize%20yield%20data%20in%20Rwanda%2C%20where%20we%20evaluate%20how%20different%0Acombinations%20of%20soil%2C%20weather%2C%20and%20agronomic%20features%20affect%20the%20clustering%20of%0Ayield%20response%20zones.%20Sims%20is%20open%20source%20and%20available%20at%0Ahttps%3A//github.com/microsoft/Sims%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10184v1&entry.124074799=Read"},
{"title": "Adaptive Dual-Headway Unicycle Pose Control and Motion Prediction for\n  Optimal Sampling-Based Feedback Motion Planning", "author": "Aykut \u0130\u015fleyen and Abhidnya Kadu and Ren\u00e9 van de Molengraft and \u00d6m\u00fcr Arslan", "abstract": "  Safe, smooth, and optimal motion planning for nonholonomically constrained\nmobile robots and autonomous vehicles is essential for achieving reliable,\nseamless, and efficient autonomy in logistics, mobility, and service\nindustries. In many such application settings, nonholonomic robots, like\nunicycles with restricted motion, require precise planning and control of both\ntranslational and orientational motion to approach specific locations in a\ndesignated orientation, such as for approaching changing, parking, and loading\nareas. In this paper, we introduce a new dual-headway unicycle pose control\nmethod by leveraging an adaptively placed headway point in front of the\nunicycle pose and a tailway point behind the goal pose. In summary, the\nunicycle robot continuously follows its headway point, which chases the tailway\npoint of the goal pose and the asymptotic motion of the tailway point towards\nthe goal position guides the unicycle robot to approach the goal location with\nthe correct orientation. The simple and intuitive geometric construction of\ndual-headway unicycle pose control enables an explicit convex feedback motion\nprediction bound on the closed-loop unicycle motion trajectory for fast and\naccurate safety verification. We present an application of dual-headway\nunicycle control for optimal sampling-based motion planning around obstacles.\nIn numerical simulations, we show that optimal unicycle motion planning using\ndual-headway translation and orientation distances significantly outperforms\nEuclidean translation and cosine orientation distances in generating smooth\nmotion with minimal travel and turning effort.\n", "link": "http://arxiv.org/abs/2412.10350v1", "date": "2024-12-13", "relevancy": 2.2964, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6061}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5835}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Dual-Headway%20Unicycle%20Pose%20Control%20and%20Motion%20Prediction%20for%0A%20%20Optimal%20Sampling-Based%20Feedback%20Motion%20Planning&body=Title%3A%20Adaptive%20Dual-Headway%20Unicycle%20Pose%20Control%20and%20Motion%20Prediction%20for%0A%20%20Optimal%20Sampling-Based%20Feedback%20Motion%20Planning%0AAuthor%3A%20Aykut%20%C4%B0%C5%9Fleyen%20and%20Abhidnya%20Kadu%20and%20Ren%C3%A9%20van%20de%20Molengraft%20and%20%C3%96m%C3%BCr%20Arslan%0AAbstract%3A%20%20%20Safe%2C%20smooth%2C%20and%20optimal%20motion%20planning%20for%20nonholonomically%20constrained%0Amobile%20robots%20and%20autonomous%20vehicles%20is%20essential%20for%20achieving%20reliable%2C%0Aseamless%2C%20and%20efficient%20autonomy%20in%20logistics%2C%20mobility%2C%20and%20service%0Aindustries.%20In%20many%20such%20application%20settings%2C%20nonholonomic%20robots%2C%20like%0Aunicycles%20with%20restricted%20motion%2C%20require%20precise%20planning%20and%20control%20of%20both%0Atranslational%20and%20orientational%20motion%20to%20approach%20specific%20locations%20in%20a%0Adesignated%20orientation%2C%20such%20as%20for%20approaching%20changing%2C%20parking%2C%20and%20loading%0Aareas.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20dual-headway%20unicycle%20pose%20control%0Amethod%20by%20leveraging%20an%20adaptively%20placed%20headway%20point%20in%20front%20of%20the%0Aunicycle%20pose%20and%20a%20tailway%20point%20behind%20the%20goal%20pose.%20In%20summary%2C%20the%0Aunicycle%20robot%20continuously%20follows%20its%20headway%20point%2C%20which%20chases%20the%20tailway%0Apoint%20of%20the%20goal%20pose%20and%20the%20asymptotic%20motion%20of%20the%20tailway%20point%20towards%0Athe%20goal%20position%20guides%20the%20unicycle%20robot%20to%20approach%20the%20goal%20location%20with%0Athe%20correct%20orientation.%20The%20simple%20and%20intuitive%20geometric%20construction%20of%0Adual-headway%20unicycle%20pose%20control%20enables%20an%20explicit%20convex%20feedback%20motion%0Aprediction%20bound%20on%20the%20closed-loop%20unicycle%20motion%20trajectory%20for%20fast%20and%0Aaccurate%20safety%20verification.%20We%20present%20an%20application%20of%20dual-headway%0Aunicycle%20control%20for%20optimal%20sampling-based%20motion%20planning%20around%20obstacles.%0AIn%20numerical%20simulations%2C%20we%20show%20that%20optimal%20unicycle%20motion%20planning%20using%0Adual-headway%20translation%20and%20orientation%20distances%20significantly%20outperforms%0AEuclidean%20translation%20and%20cosine%20orientation%20distances%20in%20generating%20smooth%0Amotion%20with%20minimal%20travel%20and%20turning%20effort.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Dual-Headway%2520Unicycle%2520Pose%2520Control%2520and%2520Motion%2520Prediction%2520for%250A%2520%2520Optimal%2520Sampling-Based%2520Feedback%2520Motion%2520Planning%26entry.906535625%3DAykut%2520%25C4%25B0%25C5%259Fleyen%2520and%2520Abhidnya%2520Kadu%2520and%2520Ren%25C3%25A9%2520van%2520de%2520Molengraft%2520and%2520%25C3%2596m%25C3%25BCr%2520Arslan%26entry.1292438233%3D%2520%2520Safe%252C%2520smooth%252C%2520and%2520optimal%2520motion%2520planning%2520for%2520nonholonomically%2520constrained%250Amobile%2520robots%2520and%2520autonomous%2520vehicles%2520is%2520essential%2520for%2520achieving%2520reliable%252C%250Aseamless%252C%2520and%2520efficient%2520autonomy%2520in%2520logistics%252C%2520mobility%252C%2520and%2520service%250Aindustries.%2520In%2520many%2520such%2520application%2520settings%252C%2520nonholonomic%2520robots%252C%2520like%250Aunicycles%2520with%2520restricted%2520motion%252C%2520require%2520precise%2520planning%2520and%2520control%2520of%2520both%250Atranslational%2520and%2520orientational%2520motion%2520to%2520approach%2520specific%2520locations%2520in%2520a%250Adesignated%2520orientation%252C%2520such%2520as%2520for%2520approaching%2520changing%252C%2520parking%252C%2520and%2520loading%250Aareas.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520dual-headway%2520unicycle%2520pose%2520control%250Amethod%2520by%2520leveraging%2520an%2520adaptively%2520placed%2520headway%2520point%2520in%2520front%2520of%2520the%250Aunicycle%2520pose%2520and%2520a%2520tailway%2520point%2520behind%2520the%2520goal%2520pose.%2520In%2520summary%252C%2520the%250Aunicycle%2520robot%2520continuously%2520follows%2520its%2520headway%2520point%252C%2520which%2520chases%2520the%2520tailway%250Apoint%2520of%2520the%2520goal%2520pose%2520and%2520the%2520asymptotic%2520motion%2520of%2520the%2520tailway%2520point%2520towards%250Athe%2520goal%2520position%2520guides%2520the%2520unicycle%2520robot%2520to%2520approach%2520the%2520goal%2520location%2520with%250Athe%2520correct%2520orientation.%2520The%2520simple%2520and%2520intuitive%2520geometric%2520construction%2520of%250Adual-headway%2520unicycle%2520pose%2520control%2520enables%2520an%2520explicit%2520convex%2520feedback%2520motion%250Aprediction%2520bound%2520on%2520the%2520closed-loop%2520unicycle%2520motion%2520trajectory%2520for%2520fast%2520and%250Aaccurate%2520safety%2520verification.%2520We%2520present%2520an%2520application%2520of%2520dual-headway%250Aunicycle%2520control%2520for%2520optimal%2520sampling-based%2520motion%2520planning%2520around%2520obstacles.%250AIn%2520numerical%2520simulations%252C%2520we%2520show%2520that%2520optimal%2520unicycle%2520motion%2520planning%2520using%250Adual-headway%2520translation%2520and%2520orientation%2520distances%2520significantly%2520outperforms%250AEuclidean%2520translation%2520and%2520cosine%2520orientation%2520distances%2520in%2520generating%2520smooth%250Amotion%2520with%2520minimal%2520travel%2520and%2520turning%2520effort.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Dual-Headway%20Unicycle%20Pose%20Control%20and%20Motion%20Prediction%20for%0A%20%20Optimal%20Sampling-Based%20Feedback%20Motion%20Planning&entry.906535625=Aykut%20%C4%B0%C5%9Fleyen%20and%20Abhidnya%20Kadu%20and%20Ren%C3%A9%20van%20de%20Molengraft%20and%20%C3%96m%C3%BCr%20Arslan&entry.1292438233=%20%20Safe%2C%20smooth%2C%20and%20optimal%20motion%20planning%20for%20nonholonomically%20constrained%0Amobile%20robots%20and%20autonomous%20vehicles%20is%20essential%20for%20achieving%20reliable%2C%0Aseamless%2C%20and%20efficient%20autonomy%20in%20logistics%2C%20mobility%2C%20and%20service%0Aindustries.%20In%20many%20such%20application%20settings%2C%20nonholonomic%20robots%2C%20like%0Aunicycles%20with%20restricted%20motion%2C%20require%20precise%20planning%20and%20control%20of%20both%0Atranslational%20and%20orientational%20motion%20to%20approach%20specific%20locations%20in%20a%0Adesignated%20orientation%2C%20such%20as%20for%20approaching%20changing%2C%20parking%2C%20and%20loading%0Aareas.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20dual-headway%20unicycle%20pose%20control%0Amethod%20by%20leveraging%20an%20adaptively%20placed%20headway%20point%20in%20front%20of%20the%0Aunicycle%20pose%20and%20a%20tailway%20point%20behind%20the%20goal%20pose.%20In%20summary%2C%20the%0Aunicycle%20robot%20continuously%20follows%20its%20headway%20point%2C%20which%20chases%20the%20tailway%0Apoint%20of%20the%20goal%20pose%20and%20the%20asymptotic%20motion%20of%20the%20tailway%20point%20towards%0Athe%20goal%20position%20guides%20the%20unicycle%20robot%20to%20approach%20the%20goal%20location%20with%0Athe%20correct%20orientation.%20The%20simple%20and%20intuitive%20geometric%20construction%20of%0Adual-headway%20unicycle%20pose%20control%20enables%20an%20explicit%20convex%20feedback%20motion%0Aprediction%20bound%20on%20the%20closed-loop%20unicycle%20motion%20trajectory%20for%20fast%20and%0Aaccurate%20safety%20verification.%20We%20present%20an%20application%20of%20dual-headway%0Aunicycle%20control%20for%20optimal%20sampling-based%20motion%20planning%20around%20obstacles.%0AIn%20numerical%20simulations%2C%20we%20show%20that%20optimal%20unicycle%20motion%20planning%20using%0Adual-headway%20translation%20and%20orientation%20distances%20significantly%20outperforms%0AEuclidean%20translation%20and%20cosine%20orientation%20distances%20in%20generating%20smooth%0Amotion%20with%20minimal%20travel%20and%20turning%20effort.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10350v1&entry.124074799=Read"},
{"title": "Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous\n  Environments", "author": "Kehan Chen and Dong An and Yan Huang and Rongtao Xu and Yifei Su and Yonggen Ling and Ian Reid and Liang Wang", "abstract": "  We address the task of Vision-Language Navigation in Continuous Environments\n(VLN-CE) under the zero-shot setting. Zero-shot VLN-CE is particularly\nchallenging due to the absence of expert demonstrations for training and\nminimal environment structural prior to guide navigation. To confront these\nchallenges, we propose a Constraint-Aware Navigator (CA-Nav), which reframes\nzero-shot VLN-CE as a sequential, constraint-aware sub-instruction completion\nprocess. CA-Nav continuously translates sub-instructions into navigation plans\nusing two core modules: the Constraint-Aware Sub-instruction Manager (CSM) and\nthe Constraint-Aware Value Mapper (CVM). CSM defines the completion criteria\nfor decomposed sub-instructions as constraints and tracks navigation progress\nby switching sub-instructions in a constraint-aware manner. CVM, guided by\nCSM's constraints, generates a value map on the fly and refines it using\nsuperpixel clustering to improve navigation stability. CA-Nav achieves the\nstate-of-the-art performance on two VLN-CE benchmarks, surpassing the previous\nbest method by 12 percent and 13 percent in Success Rate on the validation\nunseen splits of R2R-CE and RxR-CE, respectively. Moreover, CA-Nav demonstrates\nits effectiveness in real-world robot deployments across various indoor scenes\nand instructions.\n", "link": "http://arxiv.org/abs/2412.10137v1", "date": "2024-12-13", "relevancy": 2.2602, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5786}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5554}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constraint-Aware%20Zero-Shot%20Vision-Language%20Navigation%20in%20Continuous%0A%20%20Environments&body=Title%3A%20Constraint-Aware%20Zero-Shot%20Vision-Language%20Navigation%20in%20Continuous%0A%20%20Environments%0AAuthor%3A%20Kehan%20Chen%20and%20Dong%20An%20and%20Yan%20Huang%20and%20Rongtao%20Xu%20and%20Yifei%20Su%20and%20Yonggen%20Ling%20and%20Ian%20Reid%20and%20Liang%20Wang%0AAbstract%3A%20%20%20We%20address%20the%20task%20of%20Vision-Language%20Navigation%20in%20Continuous%20Environments%0A%28VLN-CE%29%20under%20the%20zero-shot%20setting.%20Zero-shot%20VLN-CE%20is%20particularly%0Achallenging%20due%20to%20the%20absence%20of%20expert%20demonstrations%20for%20training%20and%0Aminimal%20environment%20structural%20prior%20to%20guide%20navigation.%20To%20confront%20these%0Achallenges%2C%20we%20propose%20a%20Constraint-Aware%20Navigator%20%28CA-Nav%29%2C%20which%20reframes%0Azero-shot%20VLN-CE%20as%20a%20sequential%2C%20constraint-aware%20sub-instruction%20completion%0Aprocess.%20CA-Nav%20continuously%20translates%20sub-instructions%20into%20navigation%20plans%0Ausing%20two%20core%20modules%3A%20the%20Constraint-Aware%20Sub-instruction%20Manager%20%28CSM%29%20and%0Athe%20Constraint-Aware%20Value%20Mapper%20%28CVM%29.%20CSM%20defines%20the%20completion%20criteria%0Afor%20decomposed%20sub-instructions%20as%20constraints%20and%20tracks%20navigation%20progress%0Aby%20switching%20sub-instructions%20in%20a%20constraint-aware%20manner.%20CVM%2C%20guided%20by%0ACSM%27s%20constraints%2C%20generates%20a%20value%20map%20on%20the%20fly%20and%20refines%20it%20using%0Asuperpixel%20clustering%20to%20improve%20navigation%20stability.%20CA-Nav%20achieves%20the%0Astate-of-the-art%20performance%20on%20two%20VLN-CE%20benchmarks%2C%20surpassing%20the%20previous%0Abest%20method%20by%2012%20percent%20and%2013%20percent%20in%20Success%20Rate%20on%20the%20validation%0Aunseen%20splits%20of%20R2R-CE%20and%20RxR-CE%2C%20respectively.%20Moreover%2C%20CA-Nav%20demonstrates%0Aits%20effectiveness%20in%20real-world%20robot%20deployments%20across%20various%20indoor%20scenes%0Aand%20instructions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstraint-Aware%2520Zero-Shot%2520Vision-Language%2520Navigation%2520in%2520Continuous%250A%2520%2520Environments%26entry.906535625%3DKehan%2520Chen%2520and%2520Dong%2520An%2520and%2520Yan%2520Huang%2520and%2520Rongtao%2520Xu%2520and%2520Yifei%2520Su%2520and%2520Yonggen%2520Ling%2520and%2520Ian%2520Reid%2520and%2520Liang%2520Wang%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520task%2520of%2520Vision-Language%2520Navigation%2520in%2520Continuous%2520Environments%250A%2528VLN-CE%2529%2520under%2520the%2520zero-shot%2520setting.%2520Zero-shot%2520VLN-CE%2520is%2520particularly%250Achallenging%2520due%2520to%2520the%2520absence%2520of%2520expert%2520demonstrations%2520for%2520training%2520and%250Aminimal%2520environment%2520structural%2520prior%2520to%2520guide%2520navigation.%2520To%2520confront%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520Constraint-Aware%2520Navigator%2520%2528CA-Nav%2529%252C%2520which%2520reframes%250Azero-shot%2520VLN-CE%2520as%2520a%2520sequential%252C%2520constraint-aware%2520sub-instruction%2520completion%250Aprocess.%2520CA-Nav%2520continuously%2520translates%2520sub-instructions%2520into%2520navigation%2520plans%250Ausing%2520two%2520core%2520modules%253A%2520the%2520Constraint-Aware%2520Sub-instruction%2520Manager%2520%2528CSM%2529%2520and%250Athe%2520Constraint-Aware%2520Value%2520Mapper%2520%2528CVM%2529.%2520CSM%2520defines%2520the%2520completion%2520criteria%250Afor%2520decomposed%2520sub-instructions%2520as%2520constraints%2520and%2520tracks%2520navigation%2520progress%250Aby%2520switching%2520sub-instructions%2520in%2520a%2520constraint-aware%2520manner.%2520CVM%252C%2520guided%2520by%250ACSM%2527s%2520constraints%252C%2520generates%2520a%2520value%2520map%2520on%2520the%2520fly%2520and%2520refines%2520it%2520using%250Asuperpixel%2520clustering%2520to%2520improve%2520navigation%2520stability.%2520CA-Nav%2520achieves%2520the%250Astate-of-the-art%2520performance%2520on%2520two%2520VLN-CE%2520benchmarks%252C%2520surpassing%2520the%2520previous%250Abest%2520method%2520by%252012%2520percent%2520and%252013%2520percent%2520in%2520Success%2520Rate%2520on%2520the%2520validation%250Aunseen%2520splits%2520of%2520R2R-CE%2520and%2520RxR-CE%252C%2520respectively.%2520Moreover%252C%2520CA-Nav%2520demonstrates%250Aits%2520effectiveness%2520in%2520real-world%2520robot%2520deployments%2520across%2520various%2520indoor%2520scenes%250Aand%2520instructions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constraint-Aware%20Zero-Shot%20Vision-Language%20Navigation%20in%20Continuous%0A%20%20Environments&entry.906535625=Kehan%20Chen%20and%20Dong%20An%20and%20Yan%20Huang%20and%20Rongtao%20Xu%20and%20Yifei%20Su%20and%20Yonggen%20Ling%20and%20Ian%20Reid%20and%20Liang%20Wang&entry.1292438233=%20%20We%20address%20the%20task%20of%20Vision-Language%20Navigation%20in%20Continuous%20Environments%0A%28VLN-CE%29%20under%20the%20zero-shot%20setting.%20Zero-shot%20VLN-CE%20is%20particularly%0Achallenging%20due%20to%20the%20absence%20of%20expert%20demonstrations%20for%20training%20and%0Aminimal%20environment%20structural%20prior%20to%20guide%20navigation.%20To%20confront%20these%0Achallenges%2C%20we%20propose%20a%20Constraint-Aware%20Navigator%20%28CA-Nav%29%2C%20which%20reframes%0Azero-shot%20VLN-CE%20as%20a%20sequential%2C%20constraint-aware%20sub-instruction%20completion%0Aprocess.%20CA-Nav%20continuously%20translates%20sub-instructions%20into%20navigation%20plans%0Ausing%20two%20core%20modules%3A%20the%20Constraint-Aware%20Sub-instruction%20Manager%20%28CSM%29%20and%0Athe%20Constraint-Aware%20Value%20Mapper%20%28CVM%29.%20CSM%20defines%20the%20completion%20criteria%0Afor%20decomposed%20sub-instructions%20as%20constraints%20and%20tracks%20navigation%20progress%0Aby%20switching%20sub-instructions%20in%20a%20constraint-aware%20manner.%20CVM%2C%20guided%20by%0ACSM%27s%20constraints%2C%20generates%20a%20value%20map%20on%20the%20fly%20and%20refines%20it%20using%0Asuperpixel%20clustering%20to%20improve%20navigation%20stability.%20CA-Nav%20achieves%20the%0Astate-of-the-art%20performance%20on%20two%20VLN-CE%20benchmarks%2C%20surpassing%20the%20previous%0Abest%20method%20by%2012%20percent%20and%2013%20percent%20in%20Success%20Rate%20on%20the%20validation%0Aunseen%20splits%20of%20R2R-CE%20and%20RxR-CE%2C%20respectively.%20Moreover%2C%20CA-Nav%20demonstrates%0Aits%20effectiveness%20in%20real-world%20robot%20deployments%20across%20various%20indoor%20scenes%0Aand%20instructions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10137v1&entry.124074799=Read"},
{"title": "UN-DETR: Promoting Objectness Learning via Joint Supervision for Unknown\n  Object Detection", "author": "Haomiao Liu and Hao Xu and Chuhuai Yue and Bo Ma", "abstract": "  Unknown Object Detection (UOD) aims to identify objects of unseen categories,\ndiffering from the traditional detection paradigm limited by the closed-world\nassumption. A key component of UOD is learning a generalized representation,\ni.e. objectness for both known and unknown categories to distinguish and\nlocalize objects from the background in a class-agnostic manner. However,\nprevious methods obtain supervision signals for learning objectness in\nisolation from either localization or classification information, leading to\npoor performance for UOD. To address this issue, we propose a transformer-based\nUOD framework, UN-DETR. Based on this, we craft Instance Presence Score (IPS)\nto represent the probability of an object's presence. For the purpose of\ninformation complementarity, IPS employs a strategy of joint supervised\nlearning, integrating attributes representing general objectness from the\npositional and the categorical latent space as supervision signals. To enhance\nIPS learning, we introduce a one-to-many assignment strategy to incorporate\nmore supervision. Then, we propose Unbiased Query Selection to provide premium\ninitial query vectors for the decoder. Additionally, we propose an IPS-guided\npost-process strategy to filter redundant boxes and correct classification\npredictions for known and unknown objects. Finally, we pretrain the entire\nUN-DETR in an unsupervised manner, in order to obtain objectness prior. Our\nUN-DETR is comprehensively evaluated on multiple UOD and known detection\nbenchmarks, demonstrating its effectiveness and achieving state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2412.10176v1", "date": "2024-12-13", "relevancy": 2.2579, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.62}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5659}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UN-DETR%3A%20Promoting%20Objectness%20Learning%20via%20Joint%20Supervision%20for%20Unknown%0A%20%20Object%20Detection&body=Title%3A%20UN-DETR%3A%20Promoting%20Objectness%20Learning%20via%20Joint%20Supervision%20for%20Unknown%0A%20%20Object%20Detection%0AAuthor%3A%20Haomiao%20Liu%20and%20Hao%20Xu%20and%20Chuhuai%20Yue%20and%20Bo%20Ma%0AAbstract%3A%20%20%20Unknown%20Object%20Detection%20%28UOD%29%20aims%20to%20identify%20objects%20of%20unseen%20categories%2C%0Adiffering%20from%20the%20traditional%20detection%20paradigm%20limited%20by%20the%20closed-world%0Aassumption.%20A%20key%20component%20of%20UOD%20is%20learning%20a%20generalized%20representation%2C%0Ai.e.%20objectness%20for%20both%20known%20and%20unknown%20categories%20to%20distinguish%20and%0Alocalize%20objects%20from%20the%20background%20in%20a%20class-agnostic%20manner.%20However%2C%0Aprevious%20methods%20obtain%20supervision%20signals%20for%20learning%20objectness%20in%0Aisolation%20from%20either%20localization%20or%20classification%20information%2C%20leading%20to%0Apoor%20performance%20for%20UOD.%20To%20address%20this%20issue%2C%20we%20propose%20a%20transformer-based%0AUOD%20framework%2C%20UN-DETR.%20Based%20on%20this%2C%20we%20craft%20Instance%20Presence%20Score%20%28IPS%29%0Ato%20represent%20the%20probability%20of%20an%20object%27s%20presence.%20For%20the%20purpose%20of%0Ainformation%20complementarity%2C%20IPS%20employs%20a%20strategy%20of%20joint%20supervised%0Alearning%2C%20integrating%20attributes%20representing%20general%20objectness%20from%20the%0Apositional%20and%20the%20categorical%20latent%20space%20as%20supervision%20signals.%20To%20enhance%0AIPS%20learning%2C%20we%20introduce%20a%20one-to-many%20assignment%20strategy%20to%20incorporate%0Amore%20supervision.%20Then%2C%20we%20propose%20Unbiased%20Query%20Selection%20to%20provide%20premium%0Ainitial%20query%20vectors%20for%20the%20decoder.%20Additionally%2C%20we%20propose%20an%20IPS-guided%0Apost-process%20strategy%20to%20filter%20redundant%20boxes%20and%20correct%20classification%0Apredictions%20for%20known%20and%20unknown%20objects.%20Finally%2C%20we%20pretrain%20the%20entire%0AUN-DETR%20in%20an%20unsupervised%20manner%2C%20in%20order%20to%20obtain%20objectness%20prior.%20Our%0AUN-DETR%20is%20comprehensively%20evaluated%20on%20multiple%20UOD%20and%20known%20detection%0Abenchmarks%2C%20demonstrating%20its%20effectiveness%20and%20achieving%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUN-DETR%253A%2520Promoting%2520Objectness%2520Learning%2520via%2520Joint%2520Supervision%2520for%2520Unknown%250A%2520%2520Object%2520Detection%26entry.906535625%3DHaomiao%2520Liu%2520and%2520Hao%2520Xu%2520and%2520Chuhuai%2520Yue%2520and%2520Bo%2520Ma%26entry.1292438233%3D%2520%2520Unknown%2520Object%2520Detection%2520%2528UOD%2529%2520aims%2520to%2520identify%2520objects%2520of%2520unseen%2520categories%252C%250Adiffering%2520from%2520the%2520traditional%2520detection%2520paradigm%2520limited%2520by%2520the%2520closed-world%250Aassumption.%2520A%2520key%2520component%2520of%2520UOD%2520is%2520learning%2520a%2520generalized%2520representation%252C%250Ai.e.%2520objectness%2520for%2520both%2520known%2520and%2520unknown%2520categories%2520to%2520distinguish%2520and%250Alocalize%2520objects%2520from%2520the%2520background%2520in%2520a%2520class-agnostic%2520manner.%2520However%252C%250Aprevious%2520methods%2520obtain%2520supervision%2520signals%2520for%2520learning%2520objectness%2520in%250Aisolation%2520from%2520either%2520localization%2520or%2520classification%2520information%252C%2520leading%2520to%250Apoor%2520performance%2520for%2520UOD.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520transformer-based%250AUOD%2520framework%252C%2520UN-DETR.%2520Based%2520on%2520this%252C%2520we%2520craft%2520Instance%2520Presence%2520Score%2520%2528IPS%2529%250Ato%2520represent%2520the%2520probability%2520of%2520an%2520object%2527s%2520presence.%2520For%2520the%2520purpose%2520of%250Ainformation%2520complementarity%252C%2520IPS%2520employs%2520a%2520strategy%2520of%2520joint%2520supervised%250Alearning%252C%2520integrating%2520attributes%2520representing%2520general%2520objectness%2520from%2520the%250Apositional%2520and%2520the%2520categorical%2520latent%2520space%2520as%2520supervision%2520signals.%2520To%2520enhance%250AIPS%2520learning%252C%2520we%2520introduce%2520a%2520one-to-many%2520assignment%2520strategy%2520to%2520incorporate%250Amore%2520supervision.%2520Then%252C%2520we%2520propose%2520Unbiased%2520Query%2520Selection%2520to%2520provide%2520premium%250Ainitial%2520query%2520vectors%2520for%2520the%2520decoder.%2520Additionally%252C%2520we%2520propose%2520an%2520IPS-guided%250Apost-process%2520strategy%2520to%2520filter%2520redundant%2520boxes%2520and%2520correct%2520classification%250Apredictions%2520for%2520known%2520and%2520unknown%2520objects.%2520Finally%252C%2520we%2520pretrain%2520the%2520entire%250AUN-DETR%2520in%2520an%2520unsupervised%2520manner%252C%2520in%2520order%2520to%2520obtain%2520objectness%2520prior.%2520Our%250AUN-DETR%2520is%2520comprehensively%2520evaluated%2520on%2520multiple%2520UOD%2520and%2520known%2520detection%250Abenchmarks%252C%2520demonstrating%2520its%2520effectiveness%2520and%2520achieving%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UN-DETR%3A%20Promoting%20Objectness%20Learning%20via%20Joint%20Supervision%20for%20Unknown%0A%20%20Object%20Detection&entry.906535625=Haomiao%20Liu%20and%20Hao%20Xu%20and%20Chuhuai%20Yue%20and%20Bo%20Ma&entry.1292438233=%20%20Unknown%20Object%20Detection%20%28UOD%29%20aims%20to%20identify%20objects%20of%20unseen%20categories%2C%0Adiffering%20from%20the%20traditional%20detection%20paradigm%20limited%20by%20the%20closed-world%0Aassumption.%20A%20key%20component%20of%20UOD%20is%20learning%20a%20generalized%20representation%2C%0Ai.e.%20objectness%20for%20both%20known%20and%20unknown%20categories%20to%20distinguish%20and%0Alocalize%20objects%20from%20the%20background%20in%20a%20class-agnostic%20manner.%20However%2C%0Aprevious%20methods%20obtain%20supervision%20signals%20for%20learning%20objectness%20in%0Aisolation%20from%20either%20localization%20or%20classification%20information%2C%20leading%20to%0Apoor%20performance%20for%20UOD.%20To%20address%20this%20issue%2C%20we%20propose%20a%20transformer-based%0AUOD%20framework%2C%20UN-DETR.%20Based%20on%20this%2C%20we%20craft%20Instance%20Presence%20Score%20%28IPS%29%0Ato%20represent%20the%20probability%20of%20an%20object%27s%20presence.%20For%20the%20purpose%20of%0Ainformation%20complementarity%2C%20IPS%20employs%20a%20strategy%20of%20joint%20supervised%0Alearning%2C%20integrating%20attributes%20representing%20general%20objectness%20from%20the%0Apositional%20and%20the%20categorical%20latent%20space%20as%20supervision%20signals.%20To%20enhance%0AIPS%20learning%2C%20we%20introduce%20a%20one-to-many%20assignment%20strategy%20to%20incorporate%0Amore%20supervision.%20Then%2C%20we%20propose%20Unbiased%20Query%20Selection%20to%20provide%20premium%0Ainitial%20query%20vectors%20for%20the%20decoder.%20Additionally%2C%20we%20propose%20an%20IPS-guided%0Apost-process%20strategy%20to%20filter%20redundant%20boxes%20and%20correct%20classification%0Apredictions%20for%20known%20and%20unknown%20objects.%20Finally%2C%20we%20pretrain%20the%20entire%0AUN-DETR%20in%20an%20unsupervised%20manner%2C%20in%20order%20to%20obtain%20objectness%20prior.%20Our%0AUN-DETR%20is%20comprehensively%20evaluated%20on%20multiple%20UOD%20and%20known%20detection%0Abenchmarks%2C%20demonstrating%20its%20effectiveness%20and%20achieving%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10176v1&entry.124074799=Read"},
{"title": "Exploring the Frontiers of Animation Video Generation in the Sora Era:\n  Method, Dataset and Benchmark", "author": "Yudong Jiang and Baohan Xu and Siqian Yang and Mingyu Yin and Jing Liu and Chao Xu and Siqi Wang and Yidi Wu and Bingwen Zhu and Jixuan Xu and Yue Zhang and Jinlong Hou and Huyang Sun", "abstract": "  Animation has gained significant interest in the recent film and TV industry.\nDespite the success of advanced video generation models like Sora, Kling, and\nCogVideoX in generating natural videos, they lack the same effectiveness in\nhandling animation videos. Evaluating animation video generation is also a\ngreat challenge due to its unique artist styles, violating the laws of physics\nand exaggerated motions. In this paper, we present a comprehensive system,\nAniSora, designed for animation video generation, which includes a data\nprocessing pipeline, a controllable generation model, and an evaluation\ndataset. Supported by the data processing pipeline with over 10M high-quality\ndata, the generation model incorporates a spatiotemporal mask module to\nfacilitate key animation production functions such as image-to-video\ngeneration, frame interpolation, and localized image-guided animation. We also\ncollect an evaluation benchmark of 948 various animation videos, the evaluation\non VBench and human double-blind test demonstrates consistency in character and\nmotion, achieving state-of-the-art results in animation video generation. %We\nalso collect an evaluation benchmark of 948 various animation videos, with\nspecifically developed metrics for animation video generation. Our model access\nAPI and evaluation benchmark will be publicly available.\n", "link": "http://arxiv.org/abs/2412.10255v1", "date": "2024-12-13", "relevancy": 2.2443, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5744}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5609}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Frontiers%20of%20Animation%20Video%20Generation%20in%20the%20Sora%20Era%3A%0A%20%20Method%2C%20Dataset%20and%20Benchmark&body=Title%3A%20Exploring%20the%20Frontiers%20of%20Animation%20Video%20Generation%20in%20the%20Sora%20Era%3A%0A%20%20Method%2C%20Dataset%20and%20Benchmark%0AAuthor%3A%20Yudong%20Jiang%20and%20Baohan%20Xu%20and%20Siqian%20Yang%20and%20Mingyu%20Yin%20and%20Jing%20Liu%20and%20Chao%20Xu%20and%20Siqi%20Wang%20and%20Yidi%20Wu%20and%20Bingwen%20Zhu%20and%20Jixuan%20Xu%20and%20Yue%20Zhang%20and%20Jinlong%20Hou%20and%20Huyang%20Sun%0AAbstract%3A%20%20%20Animation%20has%20gained%20significant%20interest%20in%20the%20recent%20film%20and%20TV%20industry.%0ADespite%20the%20success%20of%20advanced%20video%20generation%20models%20like%20Sora%2C%20Kling%2C%20and%0ACogVideoX%20in%20generating%20natural%20videos%2C%20they%20lack%20the%20same%20effectiveness%20in%0Ahandling%20animation%20videos.%20Evaluating%20animation%20video%20generation%20is%20also%20a%0Agreat%20challenge%20due%20to%20its%20unique%20artist%20styles%2C%20violating%20the%20laws%20of%20physics%0Aand%20exaggerated%20motions.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20system%2C%0AAniSora%2C%20designed%20for%20animation%20video%20generation%2C%20which%20includes%20a%20data%0Aprocessing%20pipeline%2C%20a%20controllable%20generation%20model%2C%20and%20an%20evaluation%0Adataset.%20Supported%20by%20the%20data%20processing%20pipeline%20with%20over%2010M%20high-quality%0Adata%2C%20the%20generation%20model%20incorporates%20a%20spatiotemporal%20mask%20module%20to%0Afacilitate%20key%20animation%20production%20functions%20such%20as%20image-to-video%0Ageneration%2C%20frame%20interpolation%2C%20and%20localized%20image-guided%20animation.%20We%20also%0Acollect%20an%20evaluation%20benchmark%20of%20948%20various%20animation%20videos%2C%20the%20evaluation%0Aon%20VBench%20and%20human%20double-blind%20test%20demonstrates%20consistency%20in%20character%20and%0Amotion%2C%20achieving%20state-of-the-art%20results%20in%20animation%20video%20generation.%20%25We%0Aalso%20collect%20an%20evaluation%20benchmark%20of%20948%20various%20animation%20videos%2C%20with%0Aspecifically%20developed%20metrics%20for%20animation%20video%20generation.%20Our%20model%20access%0AAPI%20and%20evaluation%20benchmark%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Frontiers%2520of%2520Animation%2520Video%2520Generation%2520in%2520the%2520Sora%2520Era%253A%250A%2520%2520Method%252C%2520Dataset%2520and%2520Benchmark%26entry.906535625%3DYudong%2520Jiang%2520and%2520Baohan%2520Xu%2520and%2520Siqian%2520Yang%2520and%2520Mingyu%2520Yin%2520and%2520Jing%2520Liu%2520and%2520Chao%2520Xu%2520and%2520Siqi%2520Wang%2520and%2520Yidi%2520Wu%2520and%2520Bingwen%2520Zhu%2520and%2520Jixuan%2520Xu%2520and%2520Yue%2520Zhang%2520and%2520Jinlong%2520Hou%2520and%2520Huyang%2520Sun%26entry.1292438233%3D%2520%2520Animation%2520has%2520gained%2520significant%2520interest%2520in%2520the%2520recent%2520film%2520and%2520TV%2520industry.%250ADespite%2520the%2520success%2520of%2520advanced%2520video%2520generation%2520models%2520like%2520Sora%252C%2520Kling%252C%2520and%250ACogVideoX%2520in%2520generating%2520natural%2520videos%252C%2520they%2520lack%2520the%2520same%2520effectiveness%2520in%250Ahandling%2520animation%2520videos.%2520Evaluating%2520animation%2520video%2520generation%2520is%2520also%2520a%250Agreat%2520challenge%2520due%2520to%2520its%2520unique%2520artist%2520styles%252C%2520violating%2520the%2520laws%2520of%2520physics%250Aand%2520exaggerated%2520motions.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520system%252C%250AAniSora%252C%2520designed%2520for%2520animation%2520video%2520generation%252C%2520which%2520includes%2520a%2520data%250Aprocessing%2520pipeline%252C%2520a%2520controllable%2520generation%2520model%252C%2520and%2520an%2520evaluation%250Adataset.%2520Supported%2520by%2520the%2520data%2520processing%2520pipeline%2520with%2520over%252010M%2520high-quality%250Adata%252C%2520the%2520generation%2520model%2520incorporates%2520a%2520spatiotemporal%2520mask%2520module%2520to%250Afacilitate%2520key%2520animation%2520production%2520functions%2520such%2520as%2520image-to-video%250Ageneration%252C%2520frame%2520interpolation%252C%2520and%2520localized%2520image-guided%2520animation.%2520We%2520also%250Acollect%2520an%2520evaluation%2520benchmark%2520of%2520948%2520various%2520animation%2520videos%252C%2520the%2520evaluation%250Aon%2520VBench%2520and%2520human%2520double-blind%2520test%2520demonstrates%2520consistency%2520in%2520character%2520and%250Amotion%252C%2520achieving%2520state-of-the-art%2520results%2520in%2520animation%2520video%2520generation.%2520%2525We%250Aalso%2520collect%2520an%2520evaluation%2520benchmark%2520of%2520948%2520various%2520animation%2520videos%252C%2520with%250Aspecifically%2520developed%2520metrics%2520for%2520animation%2520video%2520generation.%2520Our%2520model%2520access%250AAPI%2520and%2520evaluation%2520benchmark%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Frontiers%20of%20Animation%20Video%20Generation%20in%20the%20Sora%20Era%3A%0A%20%20Method%2C%20Dataset%20and%20Benchmark&entry.906535625=Yudong%20Jiang%20and%20Baohan%20Xu%20and%20Siqian%20Yang%20and%20Mingyu%20Yin%20and%20Jing%20Liu%20and%20Chao%20Xu%20and%20Siqi%20Wang%20and%20Yidi%20Wu%20and%20Bingwen%20Zhu%20and%20Jixuan%20Xu%20and%20Yue%20Zhang%20and%20Jinlong%20Hou%20and%20Huyang%20Sun&entry.1292438233=%20%20Animation%20has%20gained%20significant%20interest%20in%20the%20recent%20film%20and%20TV%20industry.%0ADespite%20the%20success%20of%20advanced%20video%20generation%20models%20like%20Sora%2C%20Kling%2C%20and%0ACogVideoX%20in%20generating%20natural%20videos%2C%20they%20lack%20the%20same%20effectiveness%20in%0Ahandling%20animation%20videos.%20Evaluating%20animation%20video%20generation%20is%20also%20a%0Agreat%20challenge%20due%20to%20its%20unique%20artist%20styles%2C%20violating%20the%20laws%20of%20physics%0Aand%20exaggerated%20motions.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20system%2C%0AAniSora%2C%20designed%20for%20animation%20video%20generation%2C%20which%20includes%20a%20data%0Aprocessing%20pipeline%2C%20a%20controllable%20generation%20model%2C%20and%20an%20evaluation%0Adataset.%20Supported%20by%20the%20data%20processing%20pipeline%20with%20over%2010M%20high-quality%0Adata%2C%20the%20generation%20model%20incorporates%20a%20spatiotemporal%20mask%20module%20to%0Afacilitate%20key%20animation%20production%20functions%20such%20as%20image-to-video%0Ageneration%2C%20frame%20interpolation%2C%20and%20localized%20image-guided%20animation.%20We%20also%0Acollect%20an%20evaluation%20benchmark%20of%20948%20various%20animation%20videos%2C%20the%20evaluation%0Aon%20VBench%20and%20human%20double-blind%20test%20demonstrates%20consistency%20in%20character%20and%0Amotion%2C%20achieving%20state-of-the-art%20results%20in%20animation%20video%20generation.%20%25We%0Aalso%20collect%20an%20evaluation%20benchmark%20of%20948%20various%20animation%20videos%2C%20with%0Aspecifically%20developed%20metrics%20for%20animation%20video%20generation.%20Our%20model%20access%0AAPI%20and%20evaluation%20benchmark%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10255v1&entry.124074799=Read"},
{"title": "Prompt-Guided Mask Proposal for Two-Stage Open-Vocabulary Segmentation", "author": "Yu-Jhe Li and Xinyang Zhang and Kun Wan and Lantao Yu and Ajinkya Kale and Xin Lu", "abstract": "  We tackle the challenge of open-vocabulary segmentation, where we need to\nidentify objects from a wide range of categories in different environments,\nusing text prompts as our input. To overcome this challenge, existing methods\noften use multi-modal models like CLIP, which combine image and text features\nin a shared embedding space to bridge the gap between limited and extensive\nvocabulary recognition, resulting in a two-stage approach: In the first stage,\na mask generator takes an input image to generate mask proposals, and the in\nthe second stage the target mask is picked based on the query. However, the\nexpected target mask may not exist in the generated mask proposals, which leads\nto an unexpected output mask. In our work, we propose a novel approach named\nPrompt-guided Mask Proposal (PMP) where the mask generator takes the input text\nprompts and generates masks guided by these prompts. Compared with mask\nproposals generated without input prompts, masks generated by PMP are better\naligned with the input prompts. To realize PMP, we designed a cross-attention\nmechanism between text tokens and query tokens which is capable of generating\nprompt-guided mask proposals after each decoding. We combined our PMP with\nseveral existing works employing a query-based segmentation backbone and the\nexperiments on five benchmark datasets demonstrate the effectiveness of this\napproach, showcasing significant improvements over the current two-stage models\n(1% ~ 3% absolute performance gain in terms of mIOU). The steady improvement in\nperformance across these benchmarks indicates the effective generalization of\nour proposed lightweight prompt-aware method.\n", "link": "http://arxiv.org/abs/2412.10292v1", "date": "2024-12-13", "relevancy": 2.2336, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5807}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-Guided%20Mask%20Proposal%20for%20Two-Stage%20Open-Vocabulary%20Segmentation&body=Title%3A%20Prompt-Guided%20Mask%20Proposal%20for%20Two-Stage%20Open-Vocabulary%20Segmentation%0AAuthor%3A%20Yu-Jhe%20Li%20and%20Xinyang%20Zhang%20and%20Kun%20Wan%20and%20Lantao%20Yu%20and%20Ajinkya%20Kale%20and%20Xin%20Lu%0AAbstract%3A%20%20%20We%20tackle%20the%20challenge%20of%20open-vocabulary%20segmentation%2C%20where%20we%20need%20to%0Aidentify%20objects%20from%20a%20wide%20range%20of%20categories%20in%20different%20environments%2C%0Ausing%20text%20prompts%20as%20our%20input.%20To%20overcome%20this%20challenge%2C%20existing%20methods%0Aoften%20use%20multi-modal%20models%20like%20CLIP%2C%20which%20combine%20image%20and%20text%20features%0Ain%20a%20shared%20embedding%20space%20to%20bridge%20the%20gap%20between%20limited%20and%20extensive%0Avocabulary%20recognition%2C%20resulting%20in%20a%20two-stage%20approach%3A%20In%20the%20first%20stage%2C%0Aa%20mask%20generator%20takes%20an%20input%20image%20to%20generate%20mask%20proposals%2C%20and%20the%20in%0Athe%20second%20stage%20the%20target%20mask%20is%20picked%20based%20on%20the%20query.%20However%2C%20the%0Aexpected%20target%20mask%20may%20not%20exist%20in%20the%20generated%20mask%20proposals%2C%20which%20leads%0Ato%20an%20unexpected%20output%20mask.%20In%20our%20work%2C%20we%20propose%20a%20novel%20approach%20named%0APrompt-guided%20Mask%20Proposal%20%28PMP%29%20where%20the%20mask%20generator%20takes%20the%20input%20text%0Aprompts%20and%20generates%20masks%20guided%20by%20these%20prompts.%20Compared%20with%20mask%0Aproposals%20generated%20without%20input%20prompts%2C%20masks%20generated%20by%20PMP%20are%20better%0Aaligned%20with%20the%20input%20prompts.%20To%20realize%20PMP%2C%20we%20designed%20a%20cross-attention%0Amechanism%20between%20text%20tokens%20and%20query%20tokens%20which%20is%20capable%20of%20generating%0Aprompt-guided%20mask%20proposals%20after%20each%20decoding.%20We%20combined%20our%20PMP%20with%0Aseveral%20existing%20works%20employing%20a%20query-based%20segmentation%20backbone%20and%20the%0Aexperiments%20on%20five%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20this%0Aapproach%2C%20showcasing%20significant%20improvements%20over%20the%20current%20two-stage%20models%0A%281%25%20~%203%25%20absolute%20performance%20gain%20in%20terms%20of%20mIOU%29.%20The%20steady%20improvement%20in%0Aperformance%20across%20these%20benchmarks%20indicates%20the%20effective%20generalization%20of%0Aour%20proposed%20lightweight%20prompt-aware%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-Guided%2520Mask%2520Proposal%2520for%2520Two-Stage%2520Open-Vocabulary%2520Segmentation%26entry.906535625%3DYu-Jhe%2520Li%2520and%2520Xinyang%2520Zhang%2520and%2520Kun%2520Wan%2520and%2520Lantao%2520Yu%2520and%2520Ajinkya%2520Kale%2520and%2520Xin%2520Lu%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520challenge%2520of%2520open-vocabulary%2520segmentation%252C%2520where%2520we%2520need%2520to%250Aidentify%2520objects%2520from%2520a%2520wide%2520range%2520of%2520categories%2520in%2520different%2520environments%252C%250Ausing%2520text%2520prompts%2520as%2520our%2520input.%2520To%2520overcome%2520this%2520challenge%252C%2520existing%2520methods%250Aoften%2520use%2520multi-modal%2520models%2520like%2520CLIP%252C%2520which%2520combine%2520image%2520and%2520text%2520features%250Ain%2520a%2520shared%2520embedding%2520space%2520to%2520bridge%2520the%2520gap%2520between%2520limited%2520and%2520extensive%250Avocabulary%2520recognition%252C%2520resulting%2520in%2520a%2520two-stage%2520approach%253A%2520In%2520the%2520first%2520stage%252C%250Aa%2520mask%2520generator%2520takes%2520an%2520input%2520image%2520to%2520generate%2520mask%2520proposals%252C%2520and%2520the%2520in%250Athe%2520second%2520stage%2520the%2520target%2520mask%2520is%2520picked%2520based%2520on%2520the%2520query.%2520However%252C%2520the%250Aexpected%2520target%2520mask%2520may%2520not%2520exist%2520in%2520the%2520generated%2520mask%2520proposals%252C%2520which%2520leads%250Ato%2520an%2520unexpected%2520output%2520mask.%2520In%2520our%2520work%252C%2520we%2520propose%2520a%2520novel%2520approach%2520named%250APrompt-guided%2520Mask%2520Proposal%2520%2528PMP%2529%2520where%2520the%2520mask%2520generator%2520takes%2520the%2520input%2520text%250Aprompts%2520and%2520generates%2520masks%2520guided%2520by%2520these%2520prompts.%2520Compared%2520with%2520mask%250Aproposals%2520generated%2520without%2520input%2520prompts%252C%2520masks%2520generated%2520by%2520PMP%2520are%2520better%250Aaligned%2520with%2520the%2520input%2520prompts.%2520To%2520realize%2520PMP%252C%2520we%2520designed%2520a%2520cross-attention%250Amechanism%2520between%2520text%2520tokens%2520and%2520query%2520tokens%2520which%2520is%2520capable%2520of%2520generating%250Aprompt-guided%2520mask%2520proposals%2520after%2520each%2520decoding.%2520We%2520combined%2520our%2520PMP%2520with%250Aseveral%2520existing%2520works%2520employing%2520a%2520query-based%2520segmentation%2520backbone%2520and%2520the%250Aexperiments%2520on%2520five%2520benchmark%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520this%250Aapproach%252C%2520showcasing%2520significant%2520improvements%2520over%2520the%2520current%2520two-stage%2520models%250A%25281%2525%2520~%25203%2525%2520absolute%2520performance%2520gain%2520in%2520terms%2520of%2520mIOU%2529.%2520The%2520steady%2520improvement%2520in%250Aperformance%2520across%2520these%2520benchmarks%2520indicates%2520the%2520effective%2520generalization%2520of%250Aour%2520proposed%2520lightweight%2520prompt-aware%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-Guided%20Mask%20Proposal%20for%20Two-Stage%20Open-Vocabulary%20Segmentation&entry.906535625=Yu-Jhe%20Li%20and%20Xinyang%20Zhang%20and%20Kun%20Wan%20and%20Lantao%20Yu%20and%20Ajinkya%20Kale%20and%20Xin%20Lu&entry.1292438233=%20%20We%20tackle%20the%20challenge%20of%20open-vocabulary%20segmentation%2C%20where%20we%20need%20to%0Aidentify%20objects%20from%20a%20wide%20range%20of%20categories%20in%20different%20environments%2C%0Ausing%20text%20prompts%20as%20our%20input.%20To%20overcome%20this%20challenge%2C%20existing%20methods%0Aoften%20use%20multi-modal%20models%20like%20CLIP%2C%20which%20combine%20image%20and%20text%20features%0Ain%20a%20shared%20embedding%20space%20to%20bridge%20the%20gap%20between%20limited%20and%20extensive%0Avocabulary%20recognition%2C%20resulting%20in%20a%20two-stage%20approach%3A%20In%20the%20first%20stage%2C%0Aa%20mask%20generator%20takes%20an%20input%20image%20to%20generate%20mask%20proposals%2C%20and%20the%20in%0Athe%20second%20stage%20the%20target%20mask%20is%20picked%20based%20on%20the%20query.%20However%2C%20the%0Aexpected%20target%20mask%20may%20not%20exist%20in%20the%20generated%20mask%20proposals%2C%20which%20leads%0Ato%20an%20unexpected%20output%20mask.%20In%20our%20work%2C%20we%20propose%20a%20novel%20approach%20named%0APrompt-guided%20Mask%20Proposal%20%28PMP%29%20where%20the%20mask%20generator%20takes%20the%20input%20text%0Aprompts%20and%20generates%20masks%20guided%20by%20these%20prompts.%20Compared%20with%20mask%0Aproposals%20generated%20without%20input%20prompts%2C%20masks%20generated%20by%20PMP%20are%20better%0Aaligned%20with%20the%20input%20prompts.%20To%20realize%20PMP%2C%20we%20designed%20a%20cross-attention%0Amechanism%20between%20text%20tokens%20and%20query%20tokens%20which%20is%20capable%20of%20generating%0Aprompt-guided%20mask%20proposals%20after%20each%20decoding.%20We%20combined%20our%20PMP%20with%0Aseveral%20existing%20works%20employing%20a%20query-based%20segmentation%20backbone%20and%20the%0Aexperiments%20on%20five%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20this%0Aapproach%2C%20showcasing%20significant%20improvements%20over%20the%20current%20two-stage%20models%0A%281%25%20~%203%25%20absolute%20performance%20gain%20in%20terms%20of%20mIOU%29.%20The%20steady%20improvement%20in%0Aperformance%20across%20these%20benchmarks%20indicates%20the%20effective%20generalization%20of%0Aour%20proposed%20lightweight%20prompt-aware%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10292v1&entry.124074799=Read"},
{"title": "TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for\n  Generalist Robotic Policies", "author": "Ruijie Zheng and Yongyuan Liang and Shuaiyi Huang and Jianfeng Gao and Hal Daum\u00e9 III and Andrey Kolobov and Furong Huang and Jianwei Yang", "abstract": "  Although large vision-language-action (VLA) models pretrained on extensive\nrobot datasets offer promising generalist policies for robotic learning, they\nstill struggle with spatial-temporal dynamics in interactive robotics, making\nthem less effective in handling complex tasks, such as manipulation. In this\nwork, we introduce visual trace prompting, a simple yet effective approach to\nfacilitate VLA models' spatial-temporal awareness for action prediction by\nencoding state-action trajectories visually. We develop a new TraceVLA model by\nfinetuning OpenVLA on our own collected dataset of 150K robot manipulation\ntrajectories using visual trace prompting. Evaluations of TraceVLA across 137\nconfigurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstrate\nstate-of-the-art performance, outperforming OpenVLA by 10% on SimplerEnv and\n3.5x on real-robot tasks and exhibiting robust generalization across diverse\nembodiments and scenarios. To further validate the effectiveness and generality\nof our method, we present a compact VLA model based on 4B Phi-3-Vision,\npretrained on the Open-X-Embodiment and finetuned on our dataset, rivals the 7B\nOpenVLA baseline while significantly improving inference efficiency.\n", "link": "http://arxiv.org/abs/2412.10345v1", "date": "2024-12-13", "relevancy": 2.2225, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TraceVLA%3A%20Visual%20Trace%20Prompting%20Enhances%20Spatial-Temporal%20Awareness%20for%0A%20%20Generalist%20Robotic%20Policies&body=Title%3A%20TraceVLA%3A%20Visual%20Trace%20Prompting%20Enhances%20Spatial-Temporal%20Awareness%20for%0A%20%20Generalist%20Robotic%20Policies%0AAuthor%3A%20Ruijie%20Zheng%20and%20Yongyuan%20Liang%20and%20Shuaiyi%20Huang%20and%20Jianfeng%20Gao%20and%20Hal%20Daum%C3%A9%20III%20and%20Andrey%20Kolobov%20and%20Furong%20Huang%20and%20Jianwei%20Yang%0AAbstract%3A%20%20%20Although%20large%20vision-language-action%20%28VLA%29%20models%20pretrained%20on%20extensive%0Arobot%20datasets%20offer%20promising%20generalist%20policies%20for%20robotic%20learning%2C%20they%0Astill%20struggle%20with%20spatial-temporal%20dynamics%20in%20interactive%20robotics%2C%20making%0Athem%20less%20effective%20in%20handling%20complex%20tasks%2C%20such%20as%20manipulation.%20In%20this%0Awork%2C%20we%20introduce%20visual%20trace%20prompting%2C%20a%20simple%20yet%20effective%20approach%20to%0Afacilitate%20VLA%20models%27%20spatial-temporal%20awareness%20for%20action%20prediction%20by%0Aencoding%20state-action%20trajectories%20visually.%20We%20develop%20a%20new%20TraceVLA%20model%20by%0Afinetuning%20OpenVLA%20on%20our%20own%20collected%20dataset%20of%20150K%20robot%20manipulation%0Atrajectories%20using%20visual%20trace%20prompting.%20Evaluations%20of%20TraceVLA%20across%20137%0Aconfigurations%20in%20SimplerEnv%20and%204%20tasks%20on%20a%20physical%20WidowX%20robot%20demonstrate%0Astate-of-the-art%20performance%2C%20outperforming%20OpenVLA%20by%2010%25%20on%20SimplerEnv%20and%0A3.5x%20on%20real-robot%20tasks%20and%20exhibiting%20robust%20generalization%20across%20diverse%0Aembodiments%20and%20scenarios.%20To%20further%20validate%20the%20effectiveness%20and%20generality%0Aof%20our%20method%2C%20we%20present%20a%20compact%20VLA%20model%20based%20on%204B%20Phi-3-Vision%2C%0Apretrained%20on%20the%20Open-X-Embodiment%20and%20finetuned%20on%20our%20dataset%2C%20rivals%20the%207B%0AOpenVLA%20baseline%20while%20significantly%20improving%20inference%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraceVLA%253A%2520Visual%2520Trace%2520Prompting%2520Enhances%2520Spatial-Temporal%2520Awareness%2520for%250A%2520%2520Generalist%2520Robotic%2520Policies%26entry.906535625%3DRuijie%2520Zheng%2520and%2520Yongyuan%2520Liang%2520and%2520Shuaiyi%2520Huang%2520and%2520Jianfeng%2520Gao%2520and%2520Hal%2520Daum%25C3%25A9%2520III%2520and%2520Andrey%2520Kolobov%2520and%2520Furong%2520Huang%2520and%2520Jianwei%2520Yang%26entry.1292438233%3D%2520%2520Although%2520large%2520vision-language-action%2520%2528VLA%2529%2520models%2520pretrained%2520on%2520extensive%250Arobot%2520datasets%2520offer%2520promising%2520generalist%2520policies%2520for%2520robotic%2520learning%252C%2520they%250Astill%2520struggle%2520with%2520spatial-temporal%2520dynamics%2520in%2520interactive%2520robotics%252C%2520making%250Athem%2520less%2520effective%2520in%2520handling%2520complex%2520tasks%252C%2520such%2520as%2520manipulation.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520visual%2520trace%2520prompting%252C%2520a%2520simple%2520yet%2520effective%2520approach%2520to%250Afacilitate%2520VLA%2520models%2527%2520spatial-temporal%2520awareness%2520for%2520action%2520prediction%2520by%250Aencoding%2520state-action%2520trajectories%2520visually.%2520We%2520develop%2520a%2520new%2520TraceVLA%2520model%2520by%250Afinetuning%2520OpenVLA%2520on%2520our%2520own%2520collected%2520dataset%2520of%2520150K%2520robot%2520manipulation%250Atrajectories%2520using%2520visual%2520trace%2520prompting.%2520Evaluations%2520of%2520TraceVLA%2520across%2520137%250Aconfigurations%2520in%2520SimplerEnv%2520and%25204%2520tasks%2520on%2520a%2520physical%2520WidowX%2520robot%2520demonstrate%250Astate-of-the-art%2520performance%252C%2520outperforming%2520OpenVLA%2520by%252010%2525%2520on%2520SimplerEnv%2520and%250A3.5x%2520on%2520real-robot%2520tasks%2520and%2520exhibiting%2520robust%2520generalization%2520across%2520diverse%250Aembodiments%2520and%2520scenarios.%2520To%2520further%2520validate%2520the%2520effectiveness%2520and%2520generality%250Aof%2520our%2520method%252C%2520we%2520present%2520a%2520compact%2520VLA%2520model%2520based%2520on%25204B%2520Phi-3-Vision%252C%250Apretrained%2520on%2520the%2520Open-X-Embodiment%2520and%2520finetuned%2520on%2520our%2520dataset%252C%2520rivals%2520the%25207B%250AOpenVLA%2520baseline%2520while%2520significantly%2520improving%2520inference%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TraceVLA%3A%20Visual%20Trace%20Prompting%20Enhances%20Spatial-Temporal%20Awareness%20for%0A%20%20Generalist%20Robotic%20Policies&entry.906535625=Ruijie%20Zheng%20and%20Yongyuan%20Liang%20and%20Shuaiyi%20Huang%20and%20Jianfeng%20Gao%20and%20Hal%20Daum%C3%A9%20III%20and%20Andrey%20Kolobov%20and%20Furong%20Huang%20and%20Jianwei%20Yang&entry.1292438233=%20%20Although%20large%20vision-language-action%20%28VLA%29%20models%20pretrained%20on%20extensive%0Arobot%20datasets%20offer%20promising%20generalist%20policies%20for%20robotic%20learning%2C%20they%0Astill%20struggle%20with%20spatial-temporal%20dynamics%20in%20interactive%20robotics%2C%20making%0Athem%20less%20effective%20in%20handling%20complex%20tasks%2C%20such%20as%20manipulation.%20In%20this%0Awork%2C%20we%20introduce%20visual%20trace%20prompting%2C%20a%20simple%20yet%20effective%20approach%20to%0Afacilitate%20VLA%20models%27%20spatial-temporal%20awareness%20for%20action%20prediction%20by%0Aencoding%20state-action%20trajectories%20visually.%20We%20develop%20a%20new%20TraceVLA%20model%20by%0Afinetuning%20OpenVLA%20on%20our%20own%20collected%20dataset%20of%20150K%20robot%20manipulation%0Atrajectories%20using%20visual%20trace%20prompting.%20Evaluations%20of%20TraceVLA%20across%20137%0Aconfigurations%20in%20SimplerEnv%20and%204%20tasks%20on%20a%20physical%20WidowX%20robot%20demonstrate%0Astate-of-the-art%20performance%2C%20outperforming%20OpenVLA%20by%2010%25%20on%20SimplerEnv%20and%0A3.5x%20on%20real-robot%20tasks%20and%20exhibiting%20robust%20generalization%20across%20diverse%0Aembodiments%20and%20scenarios.%20To%20further%20validate%20the%20effectiveness%20and%20generality%0Aof%20our%20method%2C%20we%20present%20a%20compact%20VLA%20model%20based%20on%204B%20Phi-3-Vision%2C%0Apretrained%20on%20the%20Open-X-Embodiment%20and%20finetuned%20on%20our%20dataset%2C%20rivals%20the%207B%0AOpenVLA%20baseline%20while%20significantly%20improving%20inference%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10345v1&entry.124074799=Read"},
{"title": "The Correlated Gaussian Sparse Histogram Mechanism", "author": "Christian Janos Lebeda and Lukas Retschmeier", "abstract": "  We consider the problem of releasing a sparse histogram under $(\\varepsilon,\n\\delta)$-differential privacy. The stability histogram independently adds noise\nfrom a Laplace or Gaussian distribution to the non-zero entries and removes\nthose noisy counts below a threshold.\n  Thereby, the introduction of new non-zero values between neighboring\nhistograms is only revealed with probability at most $\\delta$, and typically,\nthe value of the threshold dominates the error of the mechanism. We consider\nthe variant of the stability histogram with Gaussian noise.\n  Recent works ([Joseph and Yu, COLT '24] and [Lebeda, SOSA '25]) reduced the\nerror for private histograms using correlated Gaussian noise. However, these\ntechniques can not be directly applied in the very sparse setting. Instead, we\nadopt Lebeda's technique and show that adding correlated noise to the non-zero\ncounts only allows us to reduce the magnitude of noise when we have a sparsity\nbound. This, in turn, allows us to use a lower threshold by up to a factor of\n$1/2$ compared to the non-correlated noise mechanism. We then extend our\nmechanism to a setting without a known bound on sparsity. Additionally, we show\nthat correlated noise can give a similar improvement for the more practical\ndiscrete Gaussian mechanism.\n", "link": "http://arxiv.org/abs/2412.10357v1", "date": "2024-12-13", "relevancy": 2.2161, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4538}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.447}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Correlated%20Gaussian%20Sparse%20Histogram%20Mechanism&body=Title%3A%20The%20Correlated%20Gaussian%20Sparse%20Histogram%20Mechanism%0AAuthor%3A%20Christian%20Janos%20Lebeda%20and%20Lukas%20Retschmeier%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20releasing%20a%20sparse%20histogram%20under%20%24%28%5Cvarepsilon%2C%0A%5Cdelta%29%24-differential%20privacy.%20The%20stability%20histogram%20independently%20adds%20noise%0Afrom%20a%20Laplace%20or%20Gaussian%20distribution%20to%20the%20non-zero%20entries%20and%20removes%0Athose%20noisy%20counts%20below%20a%20threshold.%0A%20%20Thereby%2C%20the%20introduction%20of%20new%20non-zero%20values%20between%20neighboring%0Ahistograms%20is%20only%20revealed%20with%20probability%20at%20most%20%24%5Cdelta%24%2C%20and%20typically%2C%0Athe%20value%20of%20the%20threshold%20dominates%20the%20error%20of%20the%20mechanism.%20We%20consider%0Athe%20variant%20of%20the%20stability%20histogram%20with%20Gaussian%20noise.%0A%20%20Recent%20works%20%28%5BJoseph%20and%20Yu%2C%20COLT%20%2724%5D%20and%20%5BLebeda%2C%20SOSA%20%2725%5D%29%20reduced%20the%0Aerror%20for%20private%20histograms%20using%20correlated%20Gaussian%20noise.%20However%2C%20these%0Atechniques%20can%20not%20be%20directly%20applied%20in%20the%20very%20sparse%20setting.%20Instead%2C%20we%0Aadopt%20Lebeda%27s%20technique%20and%20show%20that%20adding%20correlated%20noise%20to%20the%20non-zero%0Acounts%20only%20allows%20us%20to%20reduce%20the%20magnitude%20of%20noise%20when%20we%20have%20a%20sparsity%0Abound.%20This%2C%20in%20turn%2C%20allows%20us%20to%20use%20a%20lower%20threshold%20by%20up%20to%20a%20factor%20of%0A%241/2%24%20compared%20to%20the%20non-correlated%20noise%20mechanism.%20We%20then%20extend%20our%0Amechanism%20to%20a%20setting%20without%20a%20known%20bound%20on%20sparsity.%20Additionally%2C%20we%20show%0Athat%20correlated%20noise%20can%20give%20a%20similar%20improvement%20for%20the%20more%20practical%0Adiscrete%20Gaussian%20mechanism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Correlated%2520Gaussian%2520Sparse%2520Histogram%2520Mechanism%26entry.906535625%3DChristian%2520Janos%2520Lebeda%2520and%2520Lukas%2520Retschmeier%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520releasing%2520a%2520sparse%2520histogram%2520under%2520%2524%2528%255Cvarepsilon%252C%250A%255Cdelta%2529%2524-differential%2520privacy.%2520The%2520stability%2520histogram%2520independently%2520adds%2520noise%250Afrom%2520a%2520Laplace%2520or%2520Gaussian%2520distribution%2520to%2520the%2520non-zero%2520entries%2520and%2520removes%250Athose%2520noisy%2520counts%2520below%2520a%2520threshold.%250A%2520%2520Thereby%252C%2520the%2520introduction%2520of%2520new%2520non-zero%2520values%2520between%2520neighboring%250Ahistograms%2520is%2520only%2520revealed%2520with%2520probability%2520at%2520most%2520%2524%255Cdelta%2524%252C%2520and%2520typically%252C%250Athe%2520value%2520of%2520the%2520threshold%2520dominates%2520the%2520error%2520of%2520the%2520mechanism.%2520We%2520consider%250Athe%2520variant%2520of%2520the%2520stability%2520histogram%2520with%2520Gaussian%2520noise.%250A%2520%2520Recent%2520works%2520%2528%255BJoseph%2520and%2520Yu%252C%2520COLT%2520%252724%255D%2520and%2520%255BLebeda%252C%2520SOSA%2520%252725%255D%2529%2520reduced%2520the%250Aerror%2520for%2520private%2520histograms%2520using%2520correlated%2520Gaussian%2520noise.%2520However%252C%2520these%250Atechniques%2520can%2520not%2520be%2520directly%2520applied%2520in%2520the%2520very%2520sparse%2520setting.%2520Instead%252C%2520we%250Aadopt%2520Lebeda%2527s%2520technique%2520and%2520show%2520that%2520adding%2520correlated%2520noise%2520to%2520the%2520non-zero%250Acounts%2520only%2520allows%2520us%2520to%2520reduce%2520the%2520magnitude%2520of%2520noise%2520when%2520we%2520have%2520a%2520sparsity%250Abound.%2520This%252C%2520in%2520turn%252C%2520allows%2520us%2520to%2520use%2520a%2520lower%2520threshold%2520by%2520up%2520to%2520a%2520factor%2520of%250A%25241/2%2524%2520compared%2520to%2520the%2520non-correlated%2520noise%2520mechanism.%2520We%2520then%2520extend%2520our%250Amechanism%2520to%2520a%2520setting%2520without%2520a%2520known%2520bound%2520on%2520sparsity.%2520Additionally%252C%2520we%2520show%250Athat%2520correlated%2520noise%2520can%2520give%2520a%2520similar%2520improvement%2520for%2520the%2520more%2520practical%250Adiscrete%2520Gaussian%2520mechanism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Correlated%20Gaussian%20Sparse%20Histogram%20Mechanism&entry.906535625=Christian%20Janos%20Lebeda%20and%20Lukas%20Retschmeier&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20releasing%20a%20sparse%20histogram%20under%20%24%28%5Cvarepsilon%2C%0A%5Cdelta%29%24-differential%20privacy.%20The%20stability%20histogram%20independently%20adds%20noise%0Afrom%20a%20Laplace%20or%20Gaussian%20distribution%20to%20the%20non-zero%20entries%20and%20removes%0Athose%20noisy%20counts%20below%20a%20threshold.%0A%20%20Thereby%2C%20the%20introduction%20of%20new%20non-zero%20values%20between%20neighboring%0Ahistograms%20is%20only%20revealed%20with%20probability%20at%20most%20%24%5Cdelta%24%2C%20and%20typically%2C%0Athe%20value%20of%20the%20threshold%20dominates%20the%20error%20of%20the%20mechanism.%20We%20consider%0Athe%20variant%20of%20the%20stability%20histogram%20with%20Gaussian%20noise.%0A%20%20Recent%20works%20%28%5BJoseph%20and%20Yu%2C%20COLT%20%2724%5D%20and%20%5BLebeda%2C%20SOSA%20%2725%5D%29%20reduced%20the%0Aerror%20for%20private%20histograms%20using%20correlated%20Gaussian%20noise.%20However%2C%20these%0Atechniques%20can%20not%20be%20directly%20applied%20in%20the%20very%20sparse%20setting.%20Instead%2C%20we%0Aadopt%20Lebeda%27s%20technique%20and%20show%20that%20adding%20correlated%20noise%20to%20the%20non-zero%0Acounts%20only%20allows%20us%20to%20reduce%20the%20magnitude%20of%20noise%20when%20we%20have%20a%20sparsity%0Abound.%20This%2C%20in%20turn%2C%20allows%20us%20to%20use%20a%20lower%20threshold%20by%20up%20to%20a%20factor%20of%0A%241/2%24%20compared%20to%20the%20non-correlated%20noise%20mechanism.%20We%20then%20extend%20our%0Amechanism%20to%20a%20setting%20without%20a%20known%20bound%20on%20sparsity.%20Additionally%2C%20we%20show%0Athat%20correlated%20noise%20can%20give%20a%20similar%20improvement%20for%20the%20more%20practical%0Adiscrete%20Gaussian%20mechanism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10357v1&entry.124074799=Read"},
{"title": "Iterating the Transient Light Transport Matrix for Non-Line-of-Sight\n  Imaging", "author": "Talha Sultan and Eric Brandt and Khadijeh Masumnia-Bisheh and Simone Riccardo and Pavel Polynkin and Alberto Tosi and Andreas Velten", "abstract": "  Active imaging systems sample the Transient Light Transport Matrix (TLTM) for\na scene by sequentially illuminating various positions in this scene using a\ncontrollable light source, and then measuring the resulting spatiotemporal\nlight transport with time of flight (ToF) sensors. Time-resolved\nNon-line-of-sight (NLOS) imaging employs an active imaging system that measures\npart of the TLTM of an intermediary relay surface, and uses the indirect\nreflections of light encoded within this TLTM to \"see around corners\". Such\nimaging systems have applications in diverse areas such as disaster response,\nremote surveillance, and autonomous navigation. While existing NLOS imaging\nsystems usually measure a subset of the full TLTM, development of customized\ngated Single Photon Avalanche Diode (SPAD) arrays\n\\cite{riccardo_fast-gated_2022} has made it feasible to probe the full\nmeasurement space. In this work, we demonstrate that the full TLTM on the relay\nsurface can be processed with efficient algorithms to computationally focus and\ndetect our illumination in different parts of the hidden scene, turning the\nrelay surface into a second-order active imaging system. These algorithms allow\nus to iterate on the measured, first-order TLTM, and extract a \\textbf{second\norder TLTM for surfaces in the hidden scene}. We showcase three applications of\nTLTMs in NLOS imaging: (1) Scene Relighting with novel illumination, (2)\nSeparation of direct and indirect components of light transport in the hidden\nscene, and (3) Dual Photography. Additionally, we empirically demonstrate that\nSPAD arrays enable parallel acquisition of photons, effectively mitigating long\nacquisition times.\n", "link": "http://arxiv.org/abs/2412.10300v1", "date": "2024-12-13", "relevancy": 2.2091, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5748}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5405}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterating%20the%20Transient%20Light%20Transport%20Matrix%20for%20Non-Line-of-Sight%0A%20%20Imaging&body=Title%3A%20Iterating%20the%20Transient%20Light%20Transport%20Matrix%20for%20Non-Line-of-Sight%0A%20%20Imaging%0AAuthor%3A%20Talha%20Sultan%20and%20Eric%20Brandt%20and%20Khadijeh%20Masumnia-Bisheh%20and%20Simone%20Riccardo%20and%20Pavel%20Polynkin%20and%20Alberto%20Tosi%20and%20Andreas%20Velten%0AAbstract%3A%20%20%20Active%20imaging%20systems%20sample%20the%20Transient%20Light%20Transport%20Matrix%20%28TLTM%29%20for%0Aa%20scene%20by%20sequentially%20illuminating%20various%20positions%20in%20this%20scene%20using%20a%0Acontrollable%20light%20source%2C%20and%20then%20measuring%20the%20resulting%20spatiotemporal%0Alight%20transport%20with%20time%20of%20flight%20%28ToF%29%20sensors.%20Time-resolved%0ANon-line-of-sight%20%28NLOS%29%20imaging%20employs%20an%20active%20imaging%20system%20that%20measures%0Apart%20of%20the%20TLTM%20of%20an%20intermediary%20relay%20surface%2C%20and%20uses%20the%20indirect%0Areflections%20of%20light%20encoded%20within%20this%20TLTM%20to%20%22see%20around%20corners%22.%20Such%0Aimaging%20systems%20have%20applications%20in%20diverse%20areas%20such%20as%20disaster%20response%2C%0Aremote%20surveillance%2C%20and%20autonomous%20navigation.%20While%20existing%20NLOS%20imaging%0Asystems%20usually%20measure%20a%20subset%20of%20the%20full%20TLTM%2C%20development%20of%20customized%0Agated%20Single%20Photon%20Avalanche%20Diode%20%28SPAD%29%20arrays%0A%5Ccite%7Briccardo_fast-gated_2022%7D%20has%20made%20it%20feasible%20to%20probe%20the%20full%0Ameasurement%20space.%20In%20this%20work%2C%20we%20demonstrate%20that%20the%20full%20TLTM%20on%20the%20relay%0Asurface%20can%20be%20processed%20with%20efficient%20algorithms%20to%20computationally%20focus%20and%0Adetect%20our%20illumination%20in%20different%20parts%20of%20the%20hidden%20scene%2C%20turning%20the%0Arelay%20surface%20into%20a%20second-order%20active%20imaging%20system.%20These%20algorithms%20allow%0Aus%20to%20iterate%20on%20the%20measured%2C%20first-order%20TLTM%2C%20and%20extract%20a%20%5Ctextbf%7Bsecond%0Aorder%20TLTM%20for%20surfaces%20in%20the%20hidden%20scene%7D.%20We%20showcase%20three%20applications%20of%0ATLTMs%20in%20NLOS%20imaging%3A%20%281%29%20Scene%20Relighting%20with%20novel%20illumination%2C%20%282%29%0ASeparation%20of%20direct%20and%20indirect%20components%20of%20light%20transport%20in%20the%20hidden%0Ascene%2C%20and%20%283%29%20Dual%20Photography.%20Additionally%2C%20we%20empirically%20demonstrate%20that%0ASPAD%20arrays%20enable%20parallel%20acquisition%20of%20photons%2C%20effectively%20mitigating%20long%0Aacquisition%20times.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterating%2520the%2520Transient%2520Light%2520Transport%2520Matrix%2520for%2520Non-Line-of-Sight%250A%2520%2520Imaging%26entry.906535625%3DTalha%2520Sultan%2520and%2520Eric%2520Brandt%2520and%2520Khadijeh%2520Masumnia-Bisheh%2520and%2520Simone%2520Riccardo%2520and%2520Pavel%2520Polynkin%2520and%2520Alberto%2520Tosi%2520and%2520Andreas%2520Velten%26entry.1292438233%3D%2520%2520Active%2520imaging%2520systems%2520sample%2520the%2520Transient%2520Light%2520Transport%2520Matrix%2520%2528TLTM%2529%2520for%250Aa%2520scene%2520by%2520sequentially%2520illuminating%2520various%2520positions%2520in%2520this%2520scene%2520using%2520a%250Acontrollable%2520light%2520source%252C%2520and%2520then%2520measuring%2520the%2520resulting%2520spatiotemporal%250Alight%2520transport%2520with%2520time%2520of%2520flight%2520%2528ToF%2529%2520sensors.%2520Time-resolved%250ANon-line-of-sight%2520%2528NLOS%2529%2520imaging%2520employs%2520an%2520active%2520imaging%2520system%2520that%2520measures%250Apart%2520of%2520the%2520TLTM%2520of%2520an%2520intermediary%2520relay%2520surface%252C%2520and%2520uses%2520the%2520indirect%250Areflections%2520of%2520light%2520encoded%2520within%2520this%2520TLTM%2520to%2520%2522see%2520around%2520corners%2522.%2520Such%250Aimaging%2520systems%2520have%2520applications%2520in%2520diverse%2520areas%2520such%2520as%2520disaster%2520response%252C%250Aremote%2520surveillance%252C%2520and%2520autonomous%2520navigation.%2520While%2520existing%2520NLOS%2520imaging%250Asystems%2520usually%2520measure%2520a%2520subset%2520of%2520the%2520full%2520TLTM%252C%2520development%2520of%2520customized%250Agated%2520Single%2520Photon%2520Avalanche%2520Diode%2520%2528SPAD%2529%2520arrays%250A%255Ccite%257Briccardo_fast-gated_2022%257D%2520has%2520made%2520it%2520feasible%2520to%2520probe%2520the%2520full%250Ameasurement%2520space.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520the%2520full%2520TLTM%2520on%2520the%2520relay%250Asurface%2520can%2520be%2520processed%2520with%2520efficient%2520algorithms%2520to%2520computationally%2520focus%2520and%250Adetect%2520our%2520illumination%2520in%2520different%2520parts%2520of%2520the%2520hidden%2520scene%252C%2520turning%2520the%250Arelay%2520surface%2520into%2520a%2520second-order%2520active%2520imaging%2520system.%2520These%2520algorithms%2520allow%250Aus%2520to%2520iterate%2520on%2520the%2520measured%252C%2520first-order%2520TLTM%252C%2520and%2520extract%2520a%2520%255Ctextbf%257Bsecond%250Aorder%2520TLTM%2520for%2520surfaces%2520in%2520the%2520hidden%2520scene%257D.%2520We%2520showcase%2520three%2520applications%2520of%250ATLTMs%2520in%2520NLOS%2520imaging%253A%2520%25281%2529%2520Scene%2520Relighting%2520with%2520novel%2520illumination%252C%2520%25282%2529%250ASeparation%2520of%2520direct%2520and%2520indirect%2520components%2520of%2520light%2520transport%2520in%2520the%2520hidden%250Ascene%252C%2520and%2520%25283%2529%2520Dual%2520Photography.%2520Additionally%252C%2520we%2520empirically%2520demonstrate%2520that%250ASPAD%2520arrays%2520enable%2520parallel%2520acquisition%2520of%2520photons%252C%2520effectively%2520mitigating%2520long%250Aacquisition%2520times.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterating%20the%20Transient%20Light%20Transport%20Matrix%20for%20Non-Line-of-Sight%0A%20%20Imaging&entry.906535625=Talha%20Sultan%20and%20Eric%20Brandt%20and%20Khadijeh%20Masumnia-Bisheh%20and%20Simone%20Riccardo%20and%20Pavel%20Polynkin%20and%20Alberto%20Tosi%20and%20Andreas%20Velten&entry.1292438233=%20%20Active%20imaging%20systems%20sample%20the%20Transient%20Light%20Transport%20Matrix%20%28TLTM%29%20for%0Aa%20scene%20by%20sequentially%20illuminating%20various%20positions%20in%20this%20scene%20using%20a%0Acontrollable%20light%20source%2C%20and%20then%20measuring%20the%20resulting%20spatiotemporal%0Alight%20transport%20with%20time%20of%20flight%20%28ToF%29%20sensors.%20Time-resolved%0ANon-line-of-sight%20%28NLOS%29%20imaging%20employs%20an%20active%20imaging%20system%20that%20measures%0Apart%20of%20the%20TLTM%20of%20an%20intermediary%20relay%20surface%2C%20and%20uses%20the%20indirect%0Areflections%20of%20light%20encoded%20within%20this%20TLTM%20to%20%22see%20around%20corners%22.%20Such%0Aimaging%20systems%20have%20applications%20in%20diverse%20areas%20such%20as%20disaster%20response%2C%0Aremote%20surveillance%2C%20and%20autonomous%20navigation.%20While%20existing%20NLOS%20imaging%0Asystems%20usually%20measure%20a%20subset%20of%20the%20full%20TLTM%2C%20development%20of%20customized%0Agated%20Single%20Photon%20Avalanche%20Diode%20%28SPAD%29%20arrays%0A%5Ccite%7Briccardo_fast-gated_2022%7D%20has%20made%20it%20feasible%20to%20probe%20the%20full%0Ameasurement%20space.%20In%20this%20work%2C%20we%20demonstrate%20that%20the%20full%20TLTM%20on%20the%20relay%0Asurface%20can%20be%20processed%20with%20efficient%20algorithms%20to%20computationally%20focus%20and%0Adetect%20our%20illumination%20in%20different%20parts%20of%20the%20hidden%20scene%2C%20turning%20the%0Arelay%20surface%20into%20a%20second-order%20active%20imaging%20system.%20These%20algorithms%20allow%0Aus%20to%20iterate%20on%20the%20measured%2C%20first-order%20TLTM%2C%20and%20extract%20a%20%5Ctextbf%7Bsecond%0Aorder%20TLTM%20for%20surfaces%20in%20the%20hidden%20scene%7D.%20We%20showcase%20three%20applications%20of%0ATLTMs%20in%20NLOS%20imaging%3A%20%281%29%20Scene%20Relighting%20with%20novel%20illumination%2C%20%282%29%0ASeparation%20of%20direct%20and%20indirect%20components%20of%20light%20transport%20in%20the%20hidden%0Ascene%2C%20and%20%283%29%20Dual%20Photography.%20Additionally%2C%20we%20empirically%20demonstrate%20that%0ASPAD%20arrays%20enable%20parallel%20acquisition%20of%20photons%2C%20effectively%20mitigating%20long%0Aacquisition%20times.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10300v1&entry.124074799=Read"},
{"title": "NetOrchLLM: Mastering Wireless Network Orchestration with Large Language\n  Models", "author": "Asmaa Abdallah and Abdullatif Albaseer and Abdulkadir Celik and Mohamed Abdallah and Ahmed M. Eltawil", "abstract": "  The transition to 6G networks promises unprecedented advancements in wireless\ncommunication, with increased data rates, ultra-low latency, and enhanced\ncapacity. However, the complexity of managing and optimizing these\nnext-generation networks presents significant challenges. The advent of large\nlanguage models (LLMs) has revolutionized various domains by leveraging their\nsophisticated natural language understanding capabilities. However, the\npractical application of LLMs in wireless network orchestration and management\nremains largely unexplored. Existing literature predominantly offers visionary\nperspectives without concrete implementations, leaving a significant gap in the\nfield. To address this gap, this paper presents NETORCHLLM, a wireless NETwork\nORCHestrator LLM framework that uses LLMs to seamlessly orchestrate diverse\nwireless-specific models from wireless communication communities using their\nlanguage understanding and generation capabilities. A comprehensive framework\nis introduced, demonstrating the practical viability of our approach and\nshowcasing how LLMs can be effectively harnessed to optimize dense network\noperations, manage dynamic environments, and improve overall network\nperformance. NETORCHLLM bridges the theoretical aspirations of prior research\nwith practical, actionable solutions, paving the way for future advancements in\nintegrating generative AI technologies within the wireless communications\nsector.\n", "link": "http://arxiv.org/abs/2412.10107v1", "date": "2024-12-13", "relevancy": 2.1978, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.452}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.452}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NetOrchLLM%3A%20Mastering%20Wireless%20Network%20Orchestration%20with%20Large%20Language%0A%20%20Models&body=Title%3A%20NetOrchLLM%3A%20Mastering%20Wireless%20Network%20Orchestration%20with%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Asmaa%20Abdallah%20and%20Abdullatif%20Albaseer%20and%20Abdulkadir%20Celik%20and%20Mohamed%20Abdallah%20and%20Ahmed%20M.%20Eltawil%0AAbstract%3A%20%20%20The%20transition%20to%206G%20networks%20promises%20unprecedented%20advancements%20in%20wireless%0Acommunication%2C%20with%20increased%20data%20rates%2C%20ultra-low%20latency%2C%20and%20enhanced%0Acapacity.%20However%2C%20the%20complexity%20of%20managing%20and%20optimizing%20these%0Anext-generation%20networks%20presents%20significant%20challenges.%20The%20advent%20of%20large%0Alanguage%20models%20%28LLMs%29%20has%20revolutionized%20various%20domains%20by%20leveraging%20their%0Asophisticated%20natural%20language%20understanding%20capabilities.%20However%2C%20the%0Apractical%20application%20of%20LLMs%20in%20wireless%20network%20orchestration%20and%20management%0Aremains%20largely%20unexplored.%20Existing%20literature%20predominantly%20offers%20visionary%0Aperspectives%20without%20concrete%20implementations%2C%20leaving%20a%20significant%20gap%20in%20the%0Afield.%20To%20address%20this%20gap%2C%20this%20paper%20presents%20NETORCHLLM%2C%20a%20wireless%20NETwork%0AORCHestrator%20LLM%20framework%20that%20uses%20LLMs%20to%20seamlessly%20orchestrate%20diverse%0Awireless-specific%20models%20from%20wireless%20communication%20communities%20using%20their%0Alanguage%20understanding%20and%20generation%20capabilities.%20A%20comprehensive%20framework%0Ais%20introduced%2C%20demonstrating%20the%20practical%20viability%20of%20our%20approach%20and%0Ashowcasing%20how%20LLMs%20can%20be%20effectively%20harnessed%20to%20optimize%20dense%20network%0Aoperations%2C%20manage%20dynamic%20environments%2C%20and%20improve%20overall%20network%0Aperformance.%20NETORCHLLM%20bridges%20the%20theoretical%20aspirations%20of%20prior%20research%0Awith%20practical%2C%20actionable%20solutions%2C%20paving%20the%20way%20for%20future%20advancements%20in%0Aintegrating%20generative%20AI%20technologies%20within%20the%20wireless%20communications%0Asector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNetOrchLLM%253A%2520Mastering%2520Wireless%2520Network%2520Orchestration%2520with%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DAsmaa%2520Abdallah%2520and%2520Abdullatif%2520Albaseer%2520and%2520Abdulkadir%2520Celik%2520and%2520Mohamed%2520Abdallah%2520and%2520Ahmed%2520M.%2520Eltawil%26entry.1292438233%3D%2520%2520The%2520transition%2520to%25206G%2520networks%2520promises%2520unprecedented%2520advancements%2520in%2520wireless%250Acommunication%252C%2520with%2520increased%2520data%2520rates%252C%2520ultra-low%2520latency%252C%2520and%2520enhanced%250Acapacity.%2520However%252C%2520the%2520complexity%2520of%2520managing%2520and%2520optimizing%2520these%250Anext-generation%2520networks%2520presents%2520significant%2520challenges.%2520The%2520advent%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520has%2520revolutionized%2520various%2520domains%2520by%2520leveraging%2520their%250Asophisticated%2520natural%2520language%2520understanding%2520capabilities.%2520However%252C%2520the%250Apractical%2520application%2520of%2520LLMs%2520in%2520wireless%2520network%2520orchestration%2520and%2520management%250Aremains%2520largely%2520unexplored.%2520Existing%2520literature%2520predominantly%2520offers%2520visionary%250Aperspectives%2520without%2520concrete%2520implementations%252C%2520leaving%2520a%2520significant%2520gap%2520in%2520the%250Afield.%2520To%2520address%2520this%2520gap%252C%2520this%2520paper%2520presents%2520NETORCHLLM%252C%2520a%2520wireless%2520NETwork%250AORCHestrator%2520LLM%2520framework%2520that%2520uses%2520LLMs%2520to%2520seamlessly%2520orchestrate%2520diverse%250Awireless-specific%2520models%2520from%2520wireless%2520communication%2520communities%2520using%2520their%250Alanguage%2520understanding%2520and%2520generation%2520capabilities.%2520A%2520comprehensive%2520framework%250Ais%2520introduced%252C%2520demonstrating%2520the%2520practical%2520viability%2520of%2520our%2520approach%2520and%250Ashowcasing%2520how%2520LLMs%2520can%2520be%2520effectively%2520harnessed%2520to%2520optimize%2520dense%2520network%250Aoperations%252C%2520manage%2520dynamic%2520environments%252C%2520and%2520improve%2520overall%2520network%250Aperformance.%2520NETORCHLLM%2520bridges%2520the%2520theoretical%2520aspirations%2520of%2520prior%2520research%250Awith%2520practical%252C%2520actionable%2520solutions%252C%2520paving%2520the%2520way%2520for%2520future%2520advancements%2520in%250Aintegrating%2520generative%2520AI%2520technologies%2520within%2520the%2520wireless%2520communications%250Asector.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NetOrchLLM%3A%20Mastering%20Wireless%20Network%20Orchestration%20with%20Large%20Language%0A%20%20Models&entry.906535625=Asmaa%20Abdallah%20and%20Abdullatif%20Albaseer%20and%20Abdulkadir%20Celik%20and%20Mohamed%20Abdallah%20and%20Ahmed%20M.%20Eltawil&entry.1292438233=%20%20The%20transition%20to%206G%20networks%20promises%20unprecedented%20advancements%20in%20wireless%0Acommunication%2C%20with%20increased%20data%20rates%2C%20ultra-low%20latency%2C%20and%20enhanced%0Acapacity.%20However%2C%20the%20complexity%20of%20managing%20and%20optimizing%20these%0Anext-generation%20networks%20presents%20significant%20challenges.%20The%20advent%20of%20large%0Alanguage%20models%20%28LLMs%29%20has%20revolutionized%20various%20domains%20by%20leveraging%20their%0Asophisticated%20natural%20language%20understanding%20capabilities.%20However%2C%20the%0Apractical%20application%20of%20LLMs%20in%20wireless%20network%20orchestration%20and%20management%0Aremains%20largely%20unexplored.%20Existing%20literature%20predominantly%20offers%20visionary%0Aperspectives%20without%20concrete%20implementations%2C%20leaving%20a%20significant%20gap%20in%20the%0Afield.%20To%20address%20this%20gap%2C%20this%20paper%20presents%20NETORCHLLM%2C%20a%20wireless%20NETwork%0AORCHestrator%20LLM%20framework%20that%20uses%20LLMs%20to%20seamlessly%20orchestrate%20diverse%0Awireless-specific%20models%20from%20wireless%20communication%20communities%20using%20their%0Alanguage%20understanding%20and%20generation%20capabilities.%20A%20comprehensive%20framework%0Ais%20introduced%2C%20demonstrating%20the%20practical%20viability%20of%20our%20approach%20and%0Ashowcasing%20how%20LLMs%20can%20be%20effectively%20harnessed%20to%20optimize%20dense%20network%0Aoperations%2C%20manage%20dynamic%20environments%2C%20and%20improve%20overall%20network%0Aperformance.%20NETORCHLLM%20bridges%20the%20theoretical%20aspirations%20of%20prior%20research%0Awith%20practical%2C%20actionable%20solutions%2C%20paving%20the%20way%20for%20future%20advancements%20in%0Aintegrating%20generative%20AI%20technologies%20within%20the%20wireless%20communications%0Asector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10107v1&entry.124074799=Read"},
{"title": "MOREL: Enhancing Adversarial Robustness through Multi-Objective\n  Representation Learning", "author": "Sedjro Salomon Hotegni and Sebastian Peitz", "abstract": "  Extensive research has shown that deep neural networks (DNNs) are vulnerable\nto slight adversarial perturbations$-$small changes to the input data that\nappear insignificant but cause the model to produce drastically different\noutputs. In addition to augmenting training data with adversarial examples\ngenerated from a specific attack method, most of the current defense strategies\nnecessitate modifying the original model architecture components to improve\nrobustness or performing test-time data purification to handle adversarial\nattacks. In this work, we demonstrate that strong feature representation\nlearning during training can significantly enhance the original model's\nrobustness. We propose MOREL, a multi-objective feature representation learning\napproach, encouraging classification models to produce similar features for\ninputs within the same class, despite perturbations. Our training method\ninvolves an embedding space where cosine similarity loss and multi-positive\ncontrastive loss are used to align natural and adversarial features from the\nmodel encoder and ensure tight clustering. Concurrently, the classifier is\nmotivated to achieve accurate predictions. Through extensive experiments, we\ndemonstrate that our approach significantly enhances the robustness of DNNs\nagainst white-box and black-box adversarial attacks, outperforming other\nmethods that similarly require no architectural changes or test-time data\npurification. Our code is available at https://github.com/salomonhotegni/MOREL\n", "link": "http://arxiv.org/abs/2410.01697v3", "date": "2024-12-13", "relevancy": 2.1937, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5652}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.544}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOREL%3A%20Enhancing%20Adversarial%20Robustness%20through%20Multi-Objective%0A%20%20Representation%20Learning&body=Title%3A%20MOREL%3A%20Enhancing%20Adversarial%20Robustness%20through%20Multi-Objective%0A%20%20Representation%20Learning%0AAuthor%3A%20Sedjro%20Salomon%20Hotegni%20and%20Sebastian%20Peitz%0AAbstract%3A%20%20%20Extensive%20research%20has%20shown%20that%20deep%20neural%20networks%20%28DNNs%29%20are%20vulnerable%0Ato%20slight%20adversarial%20perturbations%24-%24small%20changes%20to%20the%20input%20data%20that%0Aappear%20insignificant%20but%20cause%20the%20model%20to%20produce%20drastically%20different%0Aoutputs.%20In%20addition%20to%20augmenting%20training%20data%20with%20adversarial%20examples%0Agenerated%20from%20a%20specific%20attack%20method%2C%20most%20of%20the%20current%20defense%20strategies%0Anecessitate%20modifying%20the%20original%20model%20architecture%20components%20to%20improve%0Arobustness%20or%20performing%20test-time%20data%20purification%20to%20handle%20adversarial%0Aattacks.%20In%20this%20work%2C%20we%20demonstrate%20that%20strong%20feature%20representation%0Alearning%20during%20training%20can%20significantly%20enhance%20the%20original%20model%27s%0Arobustness.%20We%20propose%20MOREL%2C%20a%20multi-objective%20feature%20representation%20learning%0Aapproach%2C%20encouraging%20classification%20models%20to%20produce%20similar%20features%20for%0Ainputs%20within%20the%20same%20class%2C%20despite%20perturbations.%20Our%20training%20method%0Ainvolves%20an%20embedding%20space%20where%20cosine%20similarity%20loss%20and%20multi-positive%0Acontrastive%20loss%20are%20used%20to%20align%20natural%20and%20adversarial%20features%20from%20the%0Amodel%20encoder%20and%20ensure%20tight%20clustering.%20Concurrently%2C%20the%20classifier%20is%0Amotivated%20to%20achieve%20accurate%20predictions.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20our%20approach%20significantly%20enhances%20the%20robustness%20of%20DNNs%0Aagainst%20white-box%20and%20black-box%20adversarial%20attacks%2C%20outperforming%20other%0Amethods%20that%20similarly%20require%20no%20architectural%20changes%20or%20test-time%20data%0Apurification.%20Our%20code%20is%20available%20at%20https%3A//github.com/salomonhotegni/MOREL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01697v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOREL%253A%2520Enhancing%2520Adversarial%2520Robustness%2520through%2520Multi-Objective%250A%2520%2520Representation%2520Learning%26entry.906535625%3DSedjro%2520Salomon%2520Hotegni%2520and%2520Sebastian%2520Peitz%26entry.1292438233%3D%2520%2520Extensive%2520research%2520has%2520shown%2520that%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520vulnerable%250Ato%2520slight%2520adversarial%2520perturbations%2524-%2524small%2520changes%2520to%2520the%2520input%2520data%2520that%250Aappear%2520insignificant%2520but%2520cause%2520the%2520model%2520to%2520produce%2520drastically%2520different%250Aoutputs.%2520In%2520addition%2520to%2520augmenting%2520training%2520data%2520with%2520adversarial%2520examples%250Agenerated%2520from%2520a%2520specific%2520attack%2520method%252C%2520most%2520of%2520the%2520current%2520defense%2520strategies%250Anecessitate%2520modifying%2520the%2520original%2520model%2520architecture%2520components%2520to%2520improve%250Arobustness%2520or%2520performing%2520test-time%2520data%2520purification%2520to%2520handle%2520adversarial%250Aattacks.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520strong%2520feature%2520representation%250Alearning%2520during%2520training%2520can%2520significantly%2520enhance%2520the%2520original%2520model%2527s%250Arobustness.%2520We%2520propose%2520MOREL%252C%2520a%2520multi-objective%2520feature%2520representation%2520learning%250Aapproach%252C%2520encouraging%2520classification%2520models%2520to%2520produce%2520similar%2520features%2520for%250Ainputs%2520within%2520the%2520same%2520class%252C%2520despite%2520perturbations.%2520Our%2520training%2520method%250Ainvolves%2520an%2520embedding%2520space%2520where%2520cosine%2520similarity%2520loss%2520and%2520multi-positive%250Acontrastive%2520loss%2520are%2520used%2520to%2520align%2520natural%2520and%2520adversarial%2520features%2520from%2520the%250Amodel%2520encoder%2520and%2520ensure%2520tight%2520clustering.%2520Concurrently%252C%2520the%2520classifier%2520is%250Amotivated%2520to%2520achieve%2520accurate%2520predictions.%2520Through%2520extensive%2520experiments%252C%2520we%250Ademonstrate%2520that%2520our%2520approach%2520significantly%2520enhances%2520the%2520robustness%2520of%2520DNNs%250Aagainst%2520white-box%2520and%2520black-box%2520adversarial%2520attacks%252C%2520outperforming%2520other%250Amethods%2520that%2520similarly%2520require%2520no%2520architectural%2520changes%2520or%2520test-time%2520data%250Apurification.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/salomonhotegni/MOREL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01697v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOREL%3A%20Enhancing%20Adversarial%20Robustness%20through%20Multi-Objective%0A%20%20Representation%20Learning&entry.906535625=Sedjro%20Salomon%20Hotegni%20and%20Sebastian%20Peitz&entry.1292438233=%20%20Extensive%20research%20has%20shown%20that%20deep%20neural%20networks%20%28DNNs%29%20are%20vulnerable%0Ato%20slight%20adversarial%20perturbations%24-%24small%20changes%20to%20the%20input%20data%20that%0Aappear%20insignificant%20but%20cause%20the%20model%20to%20produce%20drastically%20different%0Aoutputs.%20In%20addition%20to%20augmenting%20training%20data%20with%20adversarial%20examples%0Agenerated%20from%20a%20specific%20attack%20method%2C%20most%20of%20the%20current%20defense%20strategies%0Anecessitate%20modifying%20the%20original%20model%20architecture%20components%20to%20improve%0Arobustness%20or%20performing%20test-time%20data%20purification%20to%20handle%20adversarial%0Aattacks.%20In%20this%20work%2C%20we%20demonstrate%20that%20strong%20feature%20representation%0Alearning%20during%20training%20can%20significantly%20enhance%20the%20original%20model%27s%0Arobustness.%20We%20propose%20MOREL%2C%20a%20multi-objective%20feature%20representation%20learning%0Aapproach%2C%20encouraging%20classification%20models%20to%20produce%20similar%20features%20for%0Ainputs%20within%20the%20same%20class%2C%20despite%20perturbations.%20Our%20training%20method%0Ainvolves%20an%20embedding%20space%20where%20cosine%20similarity%20loss%20and%20multi-positive%0Acontrastive%20loss%20are%20used%20to%20align%20natural%20and%20adversarial%20features%20from%20the%0Amodel%20encoder%20and%20ensure%20tight%20clustering.%20Concurrently%2C%20the%20classifier%20is%0Amotivated%20to%20achieve%20accurate%20predictions.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20our%20approach%20significantly%20enhances%20the%20robustness%20of%20DNNs%0Aagainst%20white-box%20and%20black-box%20adversarial%20attacks%2C%20outperforming%20other%0Amethods%20that%20similarly%20require%20no%20architectural%20changes%20or%20test-time%20data%0Apurification.%20Our%20code%20is%20available%20at%20https%3A//github.com/salomonhotegni/MOREL%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01697v3&entry.124074799=Read"},
{"title": "SVGDreamer++: Advancing Editability and Diversity in Text-Guided SVG\n  Generation", "author": "Ximing Xing and Qian Yu and Chuang Wang and Haitao Zhou and Jing Zhang and Dong Xu", "abstract": "  Recently, text-guided scalable vector graphics (SVG) synthesis has\ndemonstrated significant potential in domains such as iconography and\nsketching. However, SVGs generated from existing Text-to-SVG methods often lack\neditability and exhibit deficiencies in visual quality and diversity. In this\npaper, we propose a novel text-guided vector graphics synthesis method to\naddress these limitations. To enhance the editability of output SVGs, we\nintroduce a Hierarchical Image VEctorization (HIVE) framework that operates at\nthe semantic object level and supervises the optimization of components within\nthe vector object. This approach facilitates the decoupling of vector graphics\ninto distinct objects and component levels. Our proposed HIVE algorithm,\ninformed by image segmentation priors, not only ensures a more precise\nrepresentation of vector graphics but also enables fine-grained editing\ncapabilities within vector objects. To improve the diversity of output SVGs, we\npresent a Vectorized Particle-based Score Distillation (VPSD) approach. VPSD\naddresses over-saturation issues in existing methods and enhances sample\ndiversity. A pre-trained reward model is incorporated to re-weight vector\nparticles, improving aesthetic appeal and enabling faster convergence.\nAdditionally, we design a novel adaptive vector primitives control strategy,\nwhich allows for the dynamic adjustment of the number of primitives, thereby\nenhancing the presentation of graphic details. Extensive experiments validate\nthe effectiveness of the proposed method, demonstrating its superiority over\nbaseline methods in terms of editability, visual quality, and diversity. We\nalso show that our new method supports up to six distinct vector styles,\ncapable of generating high-quality vector assets suitable for stylized vector\ndesign and poster design. Code and demo will be released at:\nhttp://ximinng.github.io/SVGDreamerV2Project/\n", "link": "http://arxiv.org/abs/2411.17832v2", "date": "2024-12-13", "relevancy": 2.1841, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5758}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5274}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SVGDreamer%2B%2B%3A%20Advancing%20Editability%20and%20Diversity%20in%20Text-Guided%20SVG%0A%20%20Generation&body=Title%3A%20SVGDreamer%2B%2B%3A%20Advancing%20Editability%20and%20Diversity%20in%20Text-Guided%20SVG%0A%20%20Generation%0AAuthor%3A%20Ximing%20Xing%20and%20Qian%20Yu%20and%20Chuang%20Wang%20and%20Haitao%20Zhou%20and%20Jing%20Zhang%20and%20Dong%20Xu%0AAbstract%3A%20%20%20Recently%2C%20text-guided%20scalable%20vector%20graphics%20%28SVG%29%20synthesis%20has%0Ademonstrated%20significant%20potential%20in%20domains%20such%20as%20iconography%20and%0Asketching.%20However%2C%20SVGs%20generated%20from%20existing%20Text-to-SVG%20methods%20often%20lack%0Aeditability%20and%20exhibit%20deficiencies%20in%20visual%20quality%20and%20diversity.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20text-guided%20vector%20graphics%20synthesis%20method%20to%0Aaddress%20these%20limitations.%20To%20enhance%20the%20editability%20of%20output%20SVGs%2C%20we%0Aintroduce%20a%20Hierarchical%20Image%20VEctorization%20%28HIVE%29%20framework%20that%20operates%20at%0Athe%20semantic%20object%20level%20and%20supervises%20the%20optimization%20of%20components%20within%0Athe%20vector%20object.%20This%20approach%20facilitates%20the%20decoupling%20of%20vector%20graphics%0Ainto%20distinct%20objects%20and%20component%20levels.%20Our%20proposed%20HIVE%20algorithm%2C%0Ainformed%20by%20image%20segmentation%20priors%2C%20not%20only%20ensures%20a%20more%20precise%0Arepresentation%20of%20vector%20graphics%20but%20also%20enables%20fine-grained%20editing%0Acapabilities%20within%20vector%20objects.%20To%20improve%20the%20diversity%20of%20output%20SVGs%2C%20we%0Apresent%20a%20Vectorized%20Particle-based%20Score%20Distillation%20%28VPSD%29%20approach.%20VPSD%0Aaddresses%20over-saturation%20issues%20in%20existing%20methods%20and%20enhances%20sample%0Adiversity.%20A%20pre-trained%20reward%20model%20is%20incorporated%20to%20re-weight%20vector%0Aparticles%2C%20improving%20aesthetic%20appeal%20and%20enabling%20faster%20convergence.%0AAdditionally%2C%20we%20design%20a%20novel%20adaptive%20vector%20primitives%20control%20strategy%2C%0Awhich%20allows%20for%20the%20dynamic%20adjustment%20of%20the%20number%20of%20primitives%2C%20thereby%0Aenhancing%20the%20presentation%20of%20graphic%20details.%20Extensive%20experiments%20validate%0Athe%20effectiveness%20of%20the%20proposed%20method%2C%20demonstrating%20its%20superiority%20over%0Abaseline%20methods%20in%20terms%20of%20editability%2C%20visual%20quality%2C%20and%20diversity.%20We%0Aalso%20show%20that%20our%20new%20method%20supports%20up%20to%20six%20distinct%20vector%20styles%2C%0Acapable%20of%20generating%20high-quality%20vector%20assets%20suitable%20for%20stylized%20vector%0Adesign%20and%20poster%20design.%20Code%20and%20demo%20will%20be%20released%20at%3A%0Ahttp%3A//ximinng.github.io/SVGDreamerV2Project/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17832v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSVGDreamer%252B%252B%253A%2520Advancing%2520Editability%2520and%2520Diversity%2520in%2520Text-Guided%2520SVG%250A%2520%2520Generation%26entry.906535625%3DXiming%2520Xing%2520and%2520Qian%2520Yu%2520and%2520Chuang%2520Wang%2520and%2520Haitao%2520Zhou%2520and%2520Jing%2520Zhang%2520and%2520Dong%2520Xu%26entry.1292438233%3D%2520%2520Recently%252C%2520text-guided%2520scalable%2520vector%2520graphics%2520%2528SVG%2529%2520synthesis%2520has%250Ademonstrated%2520significant%2520potential%2520in%2520domains%2520such%2520as%2520iconography%2520and%250Asketching.%2520However%252C%2520SVGs%2520generated%2520from%2520existing%2520Text-to-SVG%2520methods%2520often%2520lack%250Aeditability%2520and%2520exhibit%2520deficiencies%2520in%2520visual%2520quality%2520and%2520diversity.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520text-guided%2520vector%2520graphics%2520synthesis%2520method%2520to%250Aaddress%2520these%2520limitations.%2520To%2520enhance%2520the%2520editability%2520of%2520output%2520SVGs%252C%2520we%250Aintroduce%2520a%2520Hierarchical%2520Image%2520VEctorization%2520%2528HIVE%2529%2520framework%2520that%2520operates%2520at%250Athe%2520semantic%2520object%2520level%2520and%2520supervises%2520the%2520optimization%2520of%2520components%2520within%250Athe%2520vector%2520object.%2520This%2520approach%2520facilitates%2520the%2520decoupling%2520of%2520vector%2520graphics%250Ainto%2520distinct%2520objects%2520and%2520component%2520levels.%2520Our%2520proposed%2520HIVE%2520algorithm%252C%250Ainformed%2520by%2520image%2520segmentation%2520priors%252C%2520not%2520only%2520ensures%2520a%2520more%2520precise%250Arepresentation%2520of%2520vector%2520graphics%2520but%2520also%2520enables%2520fine-grained%2520editing%250Acapabilities%2520within%2520vector%2520objects.%2520To%2520improve%2520the%2520diversity%2520of%2520output%2520SVGs%252C%2520we%250Apresent%2520a%2520Vectorized%2520Particle-based%2520Score%2520Distillation%2520%2528VPSD%2529%2520approach.%2520VPSD%250Aaddresses%2520over-saturation%2520issues%2520in%2520existing%2520methods%2520and%2520enhances%2520sample%250Adiversity.%2520A%2520pre-trained%2520reward%2520model%2520is%2520incorporated%2520to%2520re-weight%2520vector%250Aparticles%252C%2520improving%2520aesthetic%2520appeal%2520and%2520enabling%2520faster%2520convergence.%250AAdditionally%252C%2520we%2520design%2520a%2520novel%2520adaptive%2520vector%2520primitives%2520control%2520strategy%252C%250Awhich%2520allows%2520for%2520the%2520dynamic%2520adjustment%2520of%2520the%2520number%2520of%2520primitives%252C%2520thereby%250Aenhancing%2520the%2520presentation%2520of%2520graphic%2520details.%2520Extensive%2520experiments%2520validate%250Athe%2520effectiveness%2520of%2520the%2520proposed%2520method%252C%2520demonstrating%2520its%2520superiority%2520over%250Abaseline%2520methods%2520in%2520terms%2520of%2520editability%252C%2520visual%2520quality%252C%2520and%2520diversity.%2520We%250Aalso%2520show%2520that%2520our%2520new%2520method%2520supports%2520up%2520to%2520six%2520distinct%2520vector%2520styles%252C%250Acapable%2520of%2520generating%2520high-quality%2520vector%2520assets%2520suitable%2520for%2520stylized%2520vector%250Adesign%2520and%2520poster%2520design.%2520Code%2520and%2520demo%2520will%2520be%2520released%2520at%253A%250Ahttp%253A//ximinng.github.io/SVGDreamerV2Project/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17832v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SVGDreamer%2B%2B%3A%20Advancing%20Editability%20and%20Diversity%20in%20Text-Guided%20SVG%0A%20%20Generation&entry.906535625=Ximing%20Xing%20and%20Qian%20Yu%20and%20Chuang%20Wang%20and%20Haitao%20Zhou%20and%20Jing%20Zhang%20and%20Dong%20Xu&entry.1292438233=%20%20Recently%2C%20text-guided%20scalable%20vector%20graphics%20%28SVG%29%20synthesis%20has%0Ademonstrated%20significant%20potential%20in%20domains%20such%20as%20iconography%20and%0Asketching.%20However%2C%20SVGs%20generated%20from%20existing%20Text-to-SVG%20methods%20often%20lack%0Aeditability%20and%20exhibit%20deficiencies%20in%20visual%20quality%20and%20diversity.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20text-guided%20vector%20graphics%20synthesis%20method%20to%0Aaddress%20these%20limitations.%20To%20enhance%20the%20editability%20of%20output%20SVGs%2C%20we%0Aintroduce%20a%20Hierarchical%20Image%20VEctorization%20%28HIVE%29%20framework%20that%20operates%20at%0Athe%20semantic%20object%20level%20and%20supervises%20the%20optimization%20of%20components%20within%0Athe%20vector%20object.%20This%20approach%20facilitates%20the%20decoupling%20of%20vector%20graphics%0Ainto%20distinct%20objects%20and%20component%20levels.%20Our%20proposed%20HIVE%20algorithm%2C%0Ainformed%20by%20image%20segmentation%20priors%2C%20not%20only%20ensures%20a%20more%20precise%0Arepresentation%20of%20vector%20graphics%20but%20also%20enables%20fine-grained%20editing%0Acapabilities%20within%20vector%20objects.%20To%20improve%20the%20diversity%20of%20output%20SVGs%2C%20we%0Apresent%20a%20Vectorized%20Particle-based%20Score%20Distillation%20%28VPSD%29%20approach.%20VPSD%0Aaddresses%20over-saturation%20issues%20in%20existing%20methods%20and%20enhances%20sample%0Adiversity.%20A%20pre-trained%20reward%20model%20is%20incorporated%20to%20re-weight%20vector%0Aparticles%2C%20improving%20aesthetic%20appeal%20and%20enabling%20faster%20convergence.%0AAdditionally%2C%20we%20design%20a%20novel%20adaptive%20vector%20primitives%20control%20strategy%2C%0Awhich%20allows%20for%20the%20dynamic%20adjustment%20of%20the%20number%20of%20primitives%2C%20thereby%0Aenhancing%20the%20presentation%20of%20graphic%20details.%20Extensive%20experiments%20validate%0Athe%20effectiveness%20of%20the%20proposed%20method%2C%20demonstrating%20its%20superiority%20over%0Abaseline%20methods%20in%20terms%20of%20editability%2C%20visual%20quality%2C%20and%20diversity.%20We%0Aalso%20show%20that%20our%20new%20method%20supports%20up%20to%20six%20distinct%20vector%20styles%2C%0Acapable%20of%20generating%20high-quality%20vector%20assets%20suitable%20for%20stylized%20vector%0Adesign%20and%20poster%20design.%20Code%20and%20demo%20will%20be%20released%20at%3A%0Ahttp%3A//ximinng.github.io/SVGDreamerV2Project/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17832v2&entry.124074799=Read"},
{"title": "Detecting LLM Hallucination Through Layer-wise Information Deficiency:\n  Analysis of Unanswerable Questions and Ambiguous Prompts", "author": "Hazel Kim and Adel Bibi and Philip Torr and Yarin Gal", "abstract": "  Large language models (LLMs) frequently generate confident yet inaccurate\nresponses, introducing significant risks for deployment in safety-critical\ndomains. We present a novel approach to detecting model hallucination through\nsystematic analysis of information flow across model layers when processing\ninputs with insufficient or ambiguous context. Our investigation reveals that\nhallucination manifests as usable information deficiencies in inter-layer\ntransmissions. While existing approaches primarily focus on final-layer output\nanalysis, we demonstrate that tracking cross-layer information dynamics\n($\\mathcal{L}$I) provides robust indicators of model reliability, accounting\nfor both information gain and loss during computation. $\\mathcal{L}$I improves\nmodel reliability by immediately integrating with universal LLMs without\nadditional training or architectural modifications.\n", "link": "http://arxiv.org/abs/2412.10246v1", "date": "2024-12-13", "relevancy": 2.1713, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5583}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20LLM%20Hallucination%20Through%20Layer-wise%20Information%20Deficiency%3A%0A%20%20Analysis%20of%20Unanswerable%20Questions%20and%20Ambiguous%20Prompts&body=Title%3A%20Detecting%20LLM%20Hallucination%20Through%20Layer-wise%20Information%20Deficiency%3A%0A%20%20Analysis%20of%20Unanswerable%20Questions%20and%20Ambiguous%20Prompts%0AAuthor%3A%20Hazel%20Kim%20and%20Adel%20Bibi%20and%20Philip%20Torr%20and%20Yarin%20Gal%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20frequently%20generate%20confident%20yet%20inaccurate%0Aresponses%2C%20introducing%20significant%20risks%20for%20deployment%20in%20safety-critical%0Adomains.%20We%20present%20a%20novel%20approach%20to%20detecting%20model%20hallucination%20through%0Asystematic%20analysis%20of%20information%20flow%20across%20model%20layers%20when%20processing%0Ainputs%20with%20insufficient%20or%20ambiguous%20context.%20Our%20investigation%20reveals%20that%0Ahallucination%20manifests%20as%20usable%20information%20deficiencies%20in%20inter-layer%0Atransmissions.%20While%20existing%20approaches%20primarily%20focus%20on%20final-layer%20output%0Aanalysis%2C%20we%20demonstrate%20that%20tracking%20cross-layer%20information%20dynamics%0A%28%24%5Cmathcal%7BL%7D%24I%29%20provides%20robust%20indicators%20of%20model%20reliability%2C%20accounting%0Afor%20both%20information%20gain%20and%20loss%20during%20computation.%20%24%5Cmathcal%7BL%7D%24I%20improves%0Amodel%20reliability%20by%20immediately%20integrating%20with%20universal%20LLMs%20without%0Aadditional%20training%20or%20architectural%20modifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520LLM%2520Hallucination%2520Through%2520Layer-wise%2520Information%2520Deficiency%253A%250A%2520%2520Analysis%2520of%2520Unanswerable%2520Questions%2520and%2520Ambiguous%2520Prompts%26entry.906535625%3DHazel%2520Kim%2520and%2520Adel%2520Bibi%2520and%2520Philip%2520Torr%2520and%2520Yarin%2520Gal%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520frequently%2520generate%2520confident%2520yet%2520inaccurate%250Aresponses%252C%2520introducing%2520significant%2520risks%2520for%2520deployment%2520in%2520safety-critical%250Adomains.%2520We%2520present%2520a%2520novel%2520approach%2520to%2520detecting%2520model%2520hallucination%2520through%250Asystematic%2520analysis%2520of%2520information%2520flow%2520across%2520model%2520layers%2520when%2520processing%250Ainputs%2520with%2520insufficient%2520or%2520ambiguous%2520context.%2520Our%2520investigation%2520reveals%2520that%250Ahallucination%2520manifests%2520as%2520usable%2520information%2520deficiencies%2520in%2520inter-layer%250Atransmissions.%2520While%2520existing%2520approaches%2520primarily%2520focus%2520on%2520final-layer%2520output%250Aanalysis%252C%2520we%2520demonstrate%2520that%2520tracking%2520cross-layer%2520information%2520dynamics%250A%2528%2524%255Cmathcal%257BL%257D%2524I%2529%2520provides%2520robust%2520indicators%2520of%2520model%2520reliability%252C%2520accounting%250Afor%2520both%2520information%2520gain%2520and%2520loss%2520during%2520computation.%2520%2524%255Cmathcal%257BL%257D%2524I%2520improves%250Amodel%2520reliability%2520by%2520immediately%2520integrating%2520with%2520universal%2520LLMs%2520without%250Aadditional%2520training%2520or%2520architectural%2520modifications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20LLM%20Hallucination%20Through%20Layer-wise%20Information%20Deficiency%3A%0A%20%20Analysis%20of%20Unanswerable%20Questions%20and%20Ambiguous%20Prompts&entry.906535625=Hazel%20Kim%20and%20Adel%20Bibi%20and%20Philip%20Torr%20and%20Yarin%20Gal&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20frequently%20generate%20confident%20yet%20inaccurate%0Aresponses%2C%20introducing%20significant%20risks%20for%20deployment%20in%20safety-critical%0Adomains.%20We%20present%20a%20novel%20approach%20to%20detecting%20model%20hallucination%20through%0Asystematic%20analysis%20of%20information%20flow%20across%20model%20layers%20when%20processing%0Ainputs%20with%20insufficient%20or%20ambiguous%20context.%20Our%20investigation%20reveals%20that%0Ahallucination%20manifests%20as%20usable%20information%20deficiencies%20in%20inter-layer%0Atransmissions.%20While%20existing%20approaches%20primarily%20focus%20on%20final-layer%20output%0Aanalysis%2C%20we%20demonstrate%20that%20tracking%20cross-layer%20information%20dynamics%0A%28%24%5Cmathcal%7BL%7D%24I%29%20provides%20robust%20indicators%20of%20model%20reliability%2C%20accounting%0Afor%20both%20information%20gain%20and%20loss%20during%20computation.%20%24%5Cmathcal%7BL%7D%24I%20improves%0Amodel%20reliability%20by%20immediately%20integrating%20with%20universal%20LLMs%20without%0Aadditional%20training%20or%20architectural%20modifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10246v1&entry.124074799=Read"},
{"title": "Low-Latency Scalable Streaming for Event-Based Vision", "author": "Andrew Hamara and Benjamin Kilpatrick and Alex Baratta and Brendon Kofink and Andrew C. Freeman", "abstract": "  Recently, we have witnessed the rise of novel ``event-based'' camera sensors\nfor high-speed, low-power video capture. Rather than recording discrete image\nframes, these sensors output asynchronous ``event'' tuples with microsecond\nprecision, only when the brightness change of a given pixel exceeds a certain\nthreshold. Although these sensors have enabled compelling new computer vision\napplications, these applications often require expensive, power-hungry GPU\nsystems, rendering them incompatible for deployment on the low-power devices\nfor which event cameras are optimized. Whereas receiver-driven rate adaptation\nis a crucial feature of modern video streaming solutions, this topic is\nunderexplored in the realm of event-based vision systems. On a real-world event\ncamera dataset, we first demonstrate that a state-of-the-art object detection\napplication is resilient to dramatic data loss, and that this loss may be\nweighted towards the end of each temporal window. We then propose a scalable\nstreaming method for event-based data based on Media Over QUIC, prioritizing\nobject detection performance and low latency. The application server can\nreceive complementary event data across several streams simultaneously, and\ndrop streams as needed to maintain a certain latency. With a latency target of\n5 ms for end-to-end transmission across a small network, we observe an average\nreduction in detection mAP as low as 0.36. With a more relaxed latency target\nof 50 ms, we observe an average mAP reduction as low as 0.19.\n", "link": "http://arxiv.org/abs/2412.07889v2", "date": "2024-12-13", "relevancy": 2.1564, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5709}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5442}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Latency%20Scalable%20Streaming%20for%20Event-Based%20Vision&body=Title%3A%20Low-Latency%20Scalable%20Streaming%20for%20Event-Based%20Vision%0AAuthor%3A%20Andrew%20Hamara%20and%20Benjamin%20Kilpatrick%20and%20Alex%20Baratta%20and%20Brendon%20Kofink%20and%20Andrew%20C.%20Freeman%0AAbstract%3A%20%20%20Recently%2C%20we%20have%20witnessed%20the%20rise%20of%20novel%20%60%60event-based%27%27%20camera%20sensors%0Afor%20high-speed%2C%20low-power%20video%20capture.%20Rather%20than%20recording%20discrete%20image%0Aframes%2C%20these%20sensors%20output%20asynchronous%20%60%60event%27%27%20tuples%20with%20microsecond%0Aprecision%2C%20only%20when%20the%20brightness%20change%20of%20a%20given%20pixel%20exceeds%20a%20certain%0Athreshold.%20Although%20these%20sensors%20have%20enabled%20compelling%20new%20computer%20vision%0Aapplications%2C%20these%20applications%20often%20require%20expensive%2C%20power-hungry%20GPU%0Asystems%2C%20rendering%20them%20incompatible%20for%20deployment%20on%20the%20low-power%20devices%0Afor%20which%20event%20cameras%20are%20optimized.%20Whereas%20receiver-driven%20rate%20adaptation%0Ais%20a%20crucial%20feature%20of%20modern%20video%20streaming%20solutions%2C%20this%20topic%20is%0Aunderexplored%20in%20the%20realm%20of%20event-based%20vision%20systems.%20On%20a%20real-world%20event%0Acamera%20dataset%2C%20we%20first%20demonstrate%20that%20a%20state-of-the-art%20object%20detection%0Aapplication%20is%20resilient%20to%20dramatic%20data%20loss%2C%20and%20that%20this%20loss%20may%20be%0Aweighted%20towards%20the%20end%20of%20each%20temporal%20window.%20We%20then%20propose%20a%20scalable%0Astreaming%20method%20for%20event-based%20data%20based%20on%20Media%20Over%20QUIC%2C%20prioritizing%0Aobject%20detection%20performance%20and%20low%20latency.%20The%20application%20server%20can%0Areceive%20complementary%20event%20data%20across%20several%20streams%20simultaneously%2C%20and%0Adrop%20streams%20as%20needed%20to%20maintain%20a%20certain%20latency.%20With%20a%20latency%20target%20of%0A5%20ms%20for%20end-to-end%20transmission%20across%20a%20small%20network%2C%20we%20observe%20an%20average%0Areduction%20in%20detection%20mAP%20as%20low%20as%200.36.%20With%20a%20more%20relaxed%20latency%20target%0Aof%2050%20ms%2C%20we%20observe%20an%20average%20mAP%20reduction%20as%20low%20as%200.19.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07889v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Latency%2520Scalable%2520Streaming%2520for%2520Event-Based%2520Vision%26entry.906535625%3DAndrew%2520Hamara%2520and%2520Benjamin%2520Kilpatrick%2520and%2520Alex%2520Baratta%2520and%2520Brendon%2520Kofink%2520and%2520Andrew%2520C.%2520Freeman%26entry.1292438233%3D%2520%2520Recently%252C%2520we%2520have%2520witnessed%2520the%2520rise%2520of%2520novel%2520%2560%2560event-based%2527%2527%2520camera%2520sensors%250Afor%2520high-speed%252C%2520low-power%2520video%2520capture.%2520Rather%2520than%2520recording%2520discrete%2520image%250Aframes%252C%2520these%2520sensors%2520output%2520asynchronous%2520%2560%2560event%2527%2527%2520tuples%2520with%2520microsecond%250Aprecision%252C%2520only%2520when%2520the%2520brightness%2520change%2520of%2520a%2520given%2520pixel%2520exceeds%2520a%2520certain%250Athreshold.%2520Although%2520these%2520sensors%2520have%2520enabled%2520compelling%2520new%2520computer%2520vision%250Aapplications%252C%2520these%2520applications%2520often%2520require%2520expensive%252C%2520power-hungry%2520GPU%250Asystems%252C%2520rendering%2520them%2520incompatible%2520for%2520deployment%2520on%2520the%2520low-power%2520devices%250Afor%2520which%2520event%2520cameras%2520are%2520optimized.%2520Whereas%2520receiver-driven%2520rate%2520adaptation%250Ais%2520a%2520crucial%2520feature%2520of%2520modern%2520video%2520streaming%2520solutions%252C%2520this%2520topic%2520is%250Aunderexplored%2520in%2520the%2520realm%2520of%2520event-based%2520vision%2520systems.%2520On%2520a%2520real-world%2520event%250Acamera%2520dataset%252C%2520we%2520first%2520demonstrate%2520that%2520a%2520state-of-the-art%2520object%2520detection%250Aapplication%2520is%2520resilient%2520to%2520dramatic%2520data%2520loss%252C%2520and%2520that%2520this%2520loss%2520may%2520be%250Aweighted%2520towards%2520the%2520end%2520of%2520each%2520temporal%2520window.%2520We%2520then%2520propose%2520a%2520scalable%250Astreaming%2520method%2520for%2520event-based%2520data%2520based%2520on%2520Media%2520Over%2520QUIC%252C%2520prioritizing%250Aobject%2520detection%2520performance%2520and%2520low%2520latency.%2520The%2520application%2520server%2520can%250Areceive%2520complementary%2520event%2520data%2520across%2520several%2520streams%2520simultaneously%252C%2520and%250Adrop%2520streams%2520as%2520needed%2520to%2520maintain%2520a%2520certain%2520latency.%2520With%2520a%2520latency%2520target%2520of%250A5%2520ms%2520for%2520end-to-end%2520transmission%2520across%2520a%2520small%2520network%252C%2520we%2520observe%2520an%2520average%250Areduction%2520in%2520detection%2520mAP%2520as%2520low%2520as%25200.36.%2520With%2520a%2520more%2520relaxed%2520latency%2520target%250Aof%252050%2520ms%252C%2520we%2520observe%2520an%2520average%2520mAP%2520reduction%2520as%2520low%2520as%25200.19.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07889v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Latency%20Scalable%20Streaming%20for%20Event-Based%20Vision&entry.906535625=Andrew%20Hamara%20and%20Benjamin%20Kilpatrick%20and%20Alex%20Baratta%20and%20Brendon%20Kofink%20and%20Andrew%20C.%20Freeman&entry.1292438233=%20%20Recently%2C%20we%20have%20witnessed%20the%20rise%20of%20novel%20%60%60event-based%27%27%20camera%20sensors%0Afor%20high-speed%2C%20low-power%20video%20capture.%20Rather%20than%20recording%20discrete%20image%0Aframes%2C%20these%20sensors%20output%20asynchronous%20%60%60event%27%27%20tuples%20with%20microsecond%0Aprecision%2C%20only%20when%20the%20brightness%20change%20of%20a%20given%20pixel%20exceeds%20a%20certain%0Athreshold.%20Although%20these%20sensors%20have%20enabled%20compelling%20new%20computer%20vision%0Aapplications%2C%20these%20applications%20often%20require%20expensive%2C%20power-hungry%20GPU%0Asystems%2C%20rendering%20them%20incompatible%20for%20deployment%20on%20the%20low-power%20devices%0Afor%20which%20event%20cameras%20are%20optimized.%20Whereas%20receiver-driven%20rate%20adaptation%0Ais%20a%20crucial%20feature%20of%20modern%20video%20streaming%20solutions%2C%20this%20topic%20is%0Aunderexplored%20in%20the%20realm%20of%20event-based%20vision%20systems.%20On%20a%20real-world%20event%0Acamera%20dataset%2C%20we%20first%20demonstrate%20that%20a%20state-of-the-art%20object%20detection%0Aapplication%20is%20resilient%20to%20dramatic%20data%20loss%2C%20and%20that%20this%20loss%20may%20be%0Aweighted%20towards%20the%20end%20of%20each%20temporal%20window.%20We%20then%20propose%20a%20scalable%0Astreaming%20method%20for%20event-based%20data%20based%20on%20Media%20Over%20QUIC%2C%20prioritizing%0Aobject%20detection%20performance%20and%20low%20latency.%20The%20application%20server%20can%0Areceive%20complementary%20event%20data%20across%20several%20streams%20simultaneously%2C%20and%0Adrop%20streams%20as%20needed%20to%20maintain%20a%20certain%20latency.%20With%20a%20latency%20target%20of%0A5%20ms%20for%20end-to-end%20transmission%20across%20a%20small%20network%2C%20we%20observe%20an%20average%0Areduction%20in%20detection%20mAP%20as%20low%20as%200.36.%20With%20a%20more%20relaxed%20latency%20target%0Aof%2050%20ms%2C%20we%20observe%20an%20average%20mAP%20reduction%20as%20low%20as%200.19.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07889v2&entry.124074799=Read"},
{"title": "Timealign: A multi-modal object detection method for time misalignment\n  fusing in autonomous driving", "author": "Zhihang Song and Lihui Peng and Jianming Hu and Danya Yao and Yi Zhang", "abstract": "  The multi-modal perception methods are thriving in the autonomous driving\nfield due to their better usage of complementary data from different sensors.\nSuch methods depend on calibration and synchronization between sensors to get\naccurate environmental information. There have already been studies about\nspace-alignment robustness in autonomous driving object detection process,\nhowever, the research for time-alignment is relatively few. As in reality\nexperiments, LiDAR point clouds are more challenging for real-time data\ntransfer, our study used historical frames of LiDAR to better align features\nwhen the LiDAR data lags exist. We designed a Timealign module to predict and\ncombine LiDAR features with observation to tackle such time misalignment based\non SOTA GraphBEV framework.\n", "link": "http://arxiv.org/abs/2412.10033v1", "date": "2024-12-13", "relevancy": 2.1556, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5477}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5472}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Timealign%3A%20A%20multi-modal%20object%20detection%20method%20for%20time%20misalignment%0A%20%20fusing%20in%20autonomous%20driving&body=Title%3A%20Timealign%3A%20A%20multi-modal%20object%20detection%20method%20for%20time%20misalignment%0A%20%20fusing%20in%20autonomous%20driving%0AAuthor%3A%20Zhihang%20Song%20and%20Lihui%20Peng%20and%20Jianming%20Hu%20and%20Danya%20Yao%20and%20Yi%20Zhang%0AAbstract%3A%20%20%20The%20multi-modal%20perception%20methods%20are%20thriving%20in%20the%20autonomous%20driving%0Afield%20due%20to%20their%20better%20usage%20of%20complementary%20data%20from%20different%20sensors.%0ASuch%20methods%20depend%20on%20calibration%20and%20synchronization%20between%20sensors%20to%20get%0Aaccurate%20environmental%20information.%20There%20have%20already%20been%20studies%20about%0Aspace-alignment%20robustness%20in%20autonomous%20driving%20object%20detection%20process%2C%0Ahowever%2C%20the%20research%20for%20time-alignment%20is%20relatively%20few.%20As%20in%20reality%0Aexperiments%2C%20LiDAR%20point%20clouds%20are%20more%20challenging%20for%20real-time%20data%0Atransfer%2C%20our%20study%20used%20historical%20frames%20of%20LiDAR%20to%20better%20align%20features%0Awhen%20the%20LiDAR%20data%20lags%20exist.%20We%20designed%20a%20Timealign%20module%20to%20predict%20and%0Acombine%20LiDAR%20features%20with%20observation%20to%20tackle%20such%20time%20misalignment%20based%0Aon%20SOTA%20GraphBEV%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimealign%253A%2520A%2520multi-modal%2520object%2520detection%2520method%2520for%2520time%2520misalignment%250A%2520%2520fusing%2520in%2520autonomous%2520driving%26entry.906535625%3DZhihang%2520Song%2520and%2520Lihui%2520Peng%2520and%2520Jianming%2520Hu%2520and%2520Danya%2520Yao%2520and%2520Yi%2520Zhang%26entry.1292438233%3D%2520%2520The%2520multi-modal%2520perception%2520methods%2520are%2520thriving%2520in%2520the%2520autonomous%2520driving%250Afield%2520due%2520to%2520their%2520better%2520usage%2520of%2520complementary%2520data%2520from%2520different%2520sensors.%250ASuch%2520methods%2520depend%2520on%2520calibration%2520and%2520synchronization%2520between%2520sensors%2520to%2520get%250Aaccurate%2520environmental%2520information.%2520There%2520have%2520already%2520been%2520studies%2520about%250Aspace-alignment%2520robustness%2520in%2520autonomous%2520driving%2520object%2520detection%2520process%252C%250Ahowever%252C%2520the%2520research%2520for%2520time-alignment%2520is%2520relatively%2520few.%2520As%2520in%2520reality%250Aexperiments%252C%2520LiDAR%2520point%2520clouds%2520are%2520more%2520challenging%2520for%2520real-time%2520data%250Atransfer%252C%2520our%2520study%2520used%2520historical%2520frames%2520of%2520LiDAR%2520to%2520better%2520align%2520features%250Awhen%2520the%2520LiDAR%2520data%2520lags%2520exist.%2520We%2520designed%2520a%2520Timealign%2520module%2520to%2520predict%2520and%250Acombine%2520LiDAR%2520features%2520with%2520observation%2520to%2520tackle%2520such%2520time%2520misalignment%2520based%250Aon%2520SOTA%2520GraphBEV%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Timealign%3A%20A%20multi-modal%20object%20detection%20method%20for%20time%20misalignment%0A%20%20fusing%20in%20autonomous%20driving&entry.906535625=Zhihang%20Song%20and%20Lihui%20Peng%20and%20Jianming%20Hu%20and%20Danya%20Yao%20and%20Yi%20Zhang&entry.1292438233=%20%20The%20multi-modal%20perception%20methods%20are%20thriving%20in%20the%20autonomous%20driving%0Afield%20due%20to%20their%20better%20usage%20of%20complementary%20data%20from%20different%20sensors.%0ASuch%20methods%20depend%20on%20calibration%20and%20synchronization%20between%20sensors%20to%20get%0Aaccurate%20environmental%20information.%20There%20have%20already%20been%20studies%20about%0Aspace-alignment%20robustness%20in%20autonomous%20driving%20object%20detection%20process%2C%0Ahowever%2C%20the%20research%20for%20time-alignment%20is%20relatively%20few.%20As%20in%20reality%0Aexperiments%2C%20LiDAR%20point%20clouds%20are%20more%20challenging%20for%20real-time%20data%0Atransfer%2C%20our%20study%20used%20historical%20frames%20of%20LiDAR%20to%20better%20align%20features%0Awhen%20the%20LiDAR%20data%20lags%20exist.%20We%20designed%20a%20Timealign%20module%20to%20predict%20and%0Acombine%20LiDAR%20features%20with%20observation%20to%20tackle%20such%20time%20misalignment%20based%0Aon%20SOTA%20GraphBEV%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10033v1&entry.124074799=Read"},
{"title": "Mr. DETR: Instructive Multi-Route Training for Detection Transformers", "author": "Chang-Bin Zhang and Yujie Zhong and Kai Han", "abstract": "  Existing methods enhance the training of detection transformers by\nincorporating an auxiliary one-to-many assignment. In this work, we treat the\nmodel as a multi-task framework, simultaneously performing one-to-one and\none-to-many predictions. We investigate the roles of each component in the\ntransformer decoder across these two training targets, including\nself-attention, cross-attention, and feed-forward network. Our empirical\nresults demonstrate that any independent component in the decoder can\neffectively learn both targets simultaneously, even when other components are\nshared. This finding leads us to propose a multi-route training mechanism,\nfeaturing a primary route for one-to-one prediction and two auxiliary training\nroutes for one-to-many prediction. We enhance the training mechanism with a\nnovel instructive self-attention that dynamically and flexibly guides object\nqueries for one-to-many prediction. The auxiliary routes are removed during\ninference, ensuring no impact on model architecture or inference cost. We\nconduct extensive experiments on various baselines, achieving consistent\nimprovements as shown in Figure 1.\n", "link": "http://arxiv.org/abs/2412.10028v1", "date": "2024-12-13", "relevancy": 2.1356, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5395}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5313}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mr.%20DETR%3A%20Instructive%20Multi-Route%20Training%20for%20Detection%20Transformers&body=Title%3A%20Mr.%20DETR%3A%20Instructive%20Multi-Route%20Training%20for%20Detection%20Transformers%0AAuthor%3A%20Chang-Bin%20Zhang%20and%20Yujie%20Zhong%20and%20Kai%20Han%0AAbstract%3A%20%20%20Existing%20methods%20enhance%20the%20training%20of%20detection%20transformers%20by%0Aincorporating%20an%20auxiliary%20one-to-many%20assignment.%20In%20this%20work%2C%20we%20treat%20the%0Amodel%20as%20a%20multi-task%20framework%2C%20simultaneously%20performing%20one-to-one%20and%0Aone-to-many%20predictions.%20We%20investigate%20the%20roles%20of%20each%20component%20in%20the%0Atransformer%20decoder%20across%20these%20two%20training%20targets%2C%20including%0Aself-attention%2C%20cross-attention%2C%20and%20feed-forward%20network.%20Our%20empirical%0Aresults%20demonstrate%20that%20any%20independent%20component%20in%20the%20decoder%20can%0Aeffectively%20learn%20both%20targets%20simultaneously%2C%20even%20when%20other%20components%20are%0Ashared.%20This%20finding%20leads%20us%20to%20propose%20a%20multi-route%20training%20mechanism%2C%0Afeaturing%20a%20primary%20route%20for%20one-to-one%20prediction%20and%20two%20auxiliary%20training%0Aroutes%20for%20one-to-many%20prediction.%20We%20enhance%20the%20training%20mechanism%20with%20a%0Anovel%20instructive%20self-attention%20that%20dynamically%20and%20flexibly%20guides%20object%0Aqueries%20for%20one-to-many%20prediction.%20The%20auxiliary%20routes%20are%20removed%20during%0Ainference%2C%20ensuring%20no%20impact%20on%20model%20architecture%20or%20inference%20cost.%20We%0Aconduct%20extensive%20experiments%20on%20various%20baselines%2C%20achieving%20consistent%0Aimprovements%20as%20shown%20in%20Figure%201.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMr.%2520DETR%253A%2520Instructive%2520Multi-Route%2520Training%2520for%2520Detection%2520Transformers%26entry.906535625%3DChang-Bin%2520Zhang%2520and%2520Yujie%2520Zhong%2520and%2520Kai%2520Han%26entry.1292438233%3D%2520%2520Existing%2520methods%2520enhance%2520the%2520training%2520of%2520detection%2520transformers%2520by%250Aincorporating%2520an%2520auxiliary%2520one-to-many%2520assignment.%2520In%2520this%2520work%252C%2520we%2520treat%2520the%250Amodel%2520as%2520a%2520multi-task%2520framework%252C%2520simultaneously%2520performing%2520one-to-one%2520and%250Aone-to-many%2520predictions.%2520We%2520investigate%2520the%2520roles%2520of%2520each%2520component%2520in%2520the%250Atransformer%2520decoder%2520across%2520these%2520two%2520training%2520targets%252C%2520including%250Aself-attention%252C%2520cross-attention%252C%2520and%2520feed-forward%2520network.%2520Our%2520empirical%250Aresults%2520demonstrate%2520that%2520any%2520independent%2520component%2520in%2520the%2520decoder%2520can%250Aeffectively%2520learn%2520both%2520targets%2520simultaneously%252C%2520even%2520when%2520other%2520components%2520are%250Ashared.%2520This%2520finding%2520leads%2520us%2520to%2520propose%2520a%2520multi-route%2520training%2520mechanism%252C%250Afeaturing%2520a%2520primary%2520route%2520for%2520one-to-one%2520prediction%2520and%2520two%2520auxiliary%2520training%250Aroutes%2520for%2520one-to-many%2520prediction.%2520We%2520enhance%2520the%2520training%2520mechanism%2520with%2520a%250Anovel%2520instructive%2520self-attention%2520that%2520dynamically%2520and%2520flexibly%2520guides%2520object%250Aqueries%2520for%2520one-to-many%2520prediction.%2520The%2520auxiliary%2520routes%2520are%2520removed%2520during%250Ainference%252C%2520ensuring%2520no%2520impact%2520on%2520model%2520architecture%2520or%2520inference%2520cost.%2520We%250Aconduct%2520extensive%2520experiments%2520on%2520various%2520baselines%252C%2520achieving%2520consistent%250Aimprovements%2520as%2520shown%2520in%2520Figure%25201.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mr.%20DETR%3A%20Instructive%20Multi-Route%20Training%20for%20Detection%20Transformers&entry.906535625=Chang-Bin%20Zhang%20and%20Yujie%20Zhong%20and%20Kai%20Han&entry.1292438233=%20%20Existing%20methods%20enhance%20the%20training%20of%20detection%20transformers%20by%0Aincorporating%20an%20auxiliary%20one-to-many%20assignment.%20In%20this%20work%2C%20we%20treat%20the%0Amodel%20as%20a%20multi-task%20framework%2C%20simultaneously%20performing%20one-to-one%20and%0Aone-to-many%20predictions.%20We%20investigate%20the%20roles%20of%20each%20component%20in%20the%0Atransformer%20decoder%20across%20these%20two%20training%20targets%2C%20including%0Aself-attention%2C%20cross-attention%2C%20and%20feed-forward%20network.%20Our%20empirical%0Aresults%20demonstrate%20that%20any%20independent%20component%20in%20the%20decoder%20can%0Aeffectively%20learn%20both%20targets%20simultaneously%2C%20even%20when%20other%20components%20are%0Ashared.%20This%20finding%20leads%20us%20to%20propose%20a%20multi-route%20training%20mechanism%2C%0Afeaturing%20a%20primary%20route%20for%20one-to-one%20prediction%20and%20two%20auxiliary%20training%0Aroutes%20for%20one-to-many%20prediction.%20We%20enhance%20the%20training%20mechanism%20with%20a%0Anovel%20instructive%20self-attention%20that%20dynamically%20and%20flexibly%20guides%20object%0Aqueries%20for%20one-to-many%20prediction.%20The%20auxiliary%20routes%20are%20removed%20during%0Ainference%2C%20ensuring%20no%20impact%20on%20model%20architecture%20or%20inference%20cost.%20We%0Aconduct%20extensive%20experiments%20on%20various%20baselines%2C%20achieving%20consistent%0Aimprovements%20as%20shown%20in%20Figure%201.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10028v1&entry.124074799=Read"},
{"title": "Ultra-High Resolution Segmentation via Boundary-Enhanced Patch-Merging\n  Transformer", "author": "Haopeng Sun", "abstract": "  Segmentation of ultra-high resolution (UHR) images is a critical task with\nnumerous applications, yet it poses significant challenges due to high spatial\nresolution and rich fine details. Recent approaches adopt a dual-branch\narchitecture, where a global branch learns long-range contextual information\nand a local branch captures fine details. However, they struggle to handle the\nconflict between global and local information while adding significant extra\ncomputational cost. Inspired by the human visual system's ability to rapidly\norient attention to important areas with fine details and filter out irrelevant\ninformation, we propose a novel UHR segmentation method called\nBoundary-enhanced Patch-merging Transformer (BPT). BPT consists of two key\ncomponents: (1) Patch-Merging Transformer (PMT) for dynamically allocating\ntokens to informative regions to acquire global and local representations, and\n(2) Boundary-Enhanced Module (BEM) that leverages boundary information to\nenrich fine details. Extensive experiments on multiple UHR image segmentation\nbenchmarks demonstrate that our BPT outperforms previous state-of-the-art\nmethods without introducing extra computational overhead. Codes will be\nreleased to facilitate research.\n", "link": "http://arxiv.org/abs/2412.10181v1", "date": "2024-12-13", "relevancy": 2.1346, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5376}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5312}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultra-High%20Resolution%20Segmentation%20via%20Boundary-Enhanced%20Patch-Merging%0A%20%20Transformer&body=Title%3A%20Ultra-High%20Resolution%20Segmentation%20via%20Boundary-Enhanced%20Patch-Merging%0A%20%20Transformer%0AAuthor%3A%20Haopeng%20Sun%0AAbstract%3A%20%20%20Segmentation%20of%20ultra-high%20resolution%20%28UHR%29%20images%20is%20a%20critical%20task%20with%0Anumerous%20applications%2C%20yet%20it%20poses%20significant%20challenges%20due%20to%20high%20spatial%0Aresolution%20and%20rich%20fine%20details.%20Recent%20approaches%20adopt%20a%20dual-branch%0Aarchitecture%2C%20where%20a%20global%20branch%20learns%20long-range%20contextual%20information%0Aand%20a%20local%20branch%20captures%20fine%20details.%20However%2C%20they%20struggle%20to%20handle%20the%0Aconflict%20between%20global%20and%20local%20information%20while%20adding%20significant%20extra%0Acomputational%20cost.%20Inspired%20by%20the%20human%20visual%20system%27s%20ability%20to%20rapidly%0Aorient%20attention%20to%20important%20areas%20with%20fine%20details%20and%20filter%20out%20irrelevant%0Ainformation%2C%20we%20propose%20a%20novel%20UHR%20segmentation%20method%20called%0ABoundary-enhanced%20Patch-merging%20Transformer%20%28BPT%29.%20BPT%20consists%20of%20two%20key%0Acomponents%3A%20%281%29%20Patch-Merging%20Transformer%20%28PMT%29%20for%20dynamically%20allocating%0Atokens%20to%20informative%20regions%20to%20acquire%20global%20and%20local%20representations%2C%20and%0A%282%29%20Boundary-Enhanced%20Module%20%28BEM%29%20that%20leverages%20boundary%20information%20to%0Aenrich%20fine%20details.%20Extensive%20experiments%20on%20multiple%20UHR%20image%20segmentation%0Abenchmarks%20demonstrate%20that%20our%20BPT%20outperforms%20previous%20state-of-the-art%0Amethods%20without%20introducing%20extra%20computational%20overhead.%20Codes%20will%20be%0Areleased%20to%20facilitate%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltra-High%2520Resolution%2520Segmentation%2520via%2520Boundary-Enhanced%2520Patch-Merging%250A%2520%2520Transformer%26entry.906535625%3DHaopeng%2520Sun%26entry.1292438233%3D%2520%2520Segmentation%2520of%2520ultra-high%2520resolution%2520%2528UHR%2529%2520images%2520is%2520a%2520critical%2520task%2520with%250Anumerous%2520applications%252C%2520yet%2520it%2520poses%2520significant%2520challenges%2520due%2520to%2520high%2520spatial%250Aresolution%2520and%2520rich%2520fine%2520details.%2520Recent%2520approaches%2520adopt%2520a%2520dual-branch%250Aarchitecture%252C%2520where%2520a%2520global%2520branch%2520learns%2520long-range%2520contextual%2520information%250Aand%2520a%2520local%2520branch%2520captures%2520fine%2520details.%2520However%252C%2520they%2520struggle%2520to%2520handle%2520the%250Aconflict%2520between%2520global%2520and%2520local%2520information%2520while%2520adding%2520significant%2520extra%250Acomputational%2520cost.%2520Inspired%2520by%2520the%2520human%2520visual%2520system%2527s%2520ability%2520to%2520rapidly%250Aorient%2520attention%2520to%2520important%2520areas%2520with%2520fine%2520details%2520and%2520filter%2520out%2520irrelevant%250Ainformation%252C%2520we%2520propose%2520a%2520novel%2520UHR%2520segmentation%2520method%2520called%250ABoundary-enhanced%2520Patch-merging%2520Transformer%2520%2528BPT%2529.%2520BPT%2520consists%2520of%2520two%2520key%250Acomponents%253A%2520%25281%2529%2520Patch-Merging%2520Transformer%2520%2528PMT%2529%2520for%2520dynamically%2520allocating%250Atokens%2520to%2520informative%2520regions%2520to%2520acquire%2520global%2520and%2520local%2520representations%252C%2520and%250A%25282%2529%2520Boundary-Enhanced%2520Module%2520%2528BEM%2529%2520that%2520leverages%2520boundary%2520information%2520to%250Aenrich%2520fine%2520details.%2520Extensive%2520experiments%2520on%2520multiple%2520UHR%2520image%2520segmentation%250Abenchmarks%2520demonstrate%2520that%2520our%2520BPT%2520outperforms%2520previous%2520state-of-the-art%250Amethods%2520without%2520introducing%2520extra%2520computational%2520overhead.%2520Codes%2520will%2520be%250Areleased%2520to%2520facilitate%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra-High%20Resolution%20Segmentation%20via%20Boundary-Enhanced%20Patch-Merging%0A%20%20Transformer&entry.906535625=Haopeng%20Sun&entry.1292438233=%20%20Segmentation%20of%20ultra-high%20resolution%20%28UHR%29%20images%20is%20a%20critical%20task%20with%0Anumerous%20applications%2C%20yet%20it%20poses%20significant%20challenges%20due%20to%20high%20spatial%0Aresolution%20and%20rich%20fine%20details.%20Recent%20approaches%20adopt%20a%20dual-branch%0Aarchitecture%2C%20where%20a%20global%20branch%20learns%20long-range%20contextual%20information%0Aand%20a%20local%20branch%20captures%20fine%20details.%20However%2C%20they%20struggle%20to%20handle%20the%0Aconflict%20between%20global%20and%20local%20information%20while%20adding%20significant%20extra%0Acomputational%20cost.%20Inspired%20by%20the%20human%20visual%20system%27s%20ability%20to%20rapidly%0Aorient%20attention%20to%20important%20areas%20with%20fine%20details%20and%20filter%20out%20irrelevant%0Ainformation%2C%20we%20propose%20a%20novel%20UHR%20segmentation%20method%20called%0ABoundary-enhanced%20Patch-merging%20Transformer%20%28BPT%29.%20BPT%20consists%20of%20two%20key%0Acomponents%3A%20%281%29%20Patch-Merging%20Transformer%20%28PMT%29%20for%20dynamically%20allocating%0Atokens%20to%20informative%20regions%20to%20acquire%20global%20and%20local%20representations%2C%20and%0A%282%29%20Boundary-Enhanced%20Module%20%28BEM%29%20that%20leverages%20boundary%20information%20to%0Aenrich%20fine%20details.%20Extensive%20experiments%20on%20multiple%20UHR%20image%20segmentation%0Abenchmarks%20demonstrate%20that%20our%20BPT%20outperforms%20previous%20state-of-the-art%0Amethods%20without%20introducing%20extra%20computational%20overhead.%20Codes%20will%20be%0Areleased%20to%20facilitate%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10181v1&entry.124074799=Read"},
{"title": "A Library for Learning Neural Operators", "author": "Jean Kossaifi and Nikola Kovachki and Zongyi Li and Davit Pitt and Miguel Liu-Schiaffini and Robert Joseph George and Boris Bonev and Kamyar Azizzadenesheli and Julius Berner and Anima Anandkumar", "abstract": "  We present NeuralOperator, an open-source Python library for operator\nlearning. Neural operators generalize neural networks to maps between function\nspaces instead of finite-dimensional Euclidean spaces. They can be trained and\ninferenced on input and output functions given at various discretizations,\nsatisfying a discretization convergence properties. Built on top of PyTorch,\nNeuralOperator provides all the tools for training and deploying neural\noperator models, as well as developing new ones, in a high-quality, tested,\nopen-source package. It combines cutting-edge models and customizability with a\ngentle learning curve and simple user interface for newcomers.\n", "link": "http://arxiv.org/abs/2412.10354v1", "date": "2024-12-13", "relevancy": 2.1304, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4365}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4217}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.42}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Library%20for%20Learning%20Neural%20Operators&body=Title%3A%20A%20Library%20for%20Learning%20Neural%20Operators%0AAuthor%3A%20Jean%20Kossaifi%20and%20Nikola%20Kovachki%20and%20Zongyi%20Li%20and%20Davit%20Pitt%20and%20Miguel%20Liu-Schiaffini%20and%20Robert%20Joseph%20George%20and%20Boris%20Bonev%20and%20Kamyar%20Azizzadenesheli%20and%20Julius%20Berner%20and%20Anima%20Anandkumar%0AAbstract%3A%20%20%20We%20present%20NeuralOperator%2C%20an%20open-source%20Python%20library%20for%20operator%0Alearning.%20Neural%20operators%20generalize%20neural%20networks%20to%20maps%20between%20function%0Aspaces%20instead%20of%20finite-dimensional%20Euclidean%20spaces.%20They%20can%20be%20trained%20and%0Ainferenced%20on%20input%20and%20output%20functions%20given%20at%20various%20discretizations%2C%0Asatisfying%20a%20discretization%20convergence%20properties.%20Built%20on%20top%20of%20PyTorch%2C%0ANeuralOperator%20provides%20all%20the%20tools%20for%20training%20and%20deploying%20neural%0Aoperator%20models%2C%20as%20well%20as%20developing%20new%20ones%2C%20in%20a%20high-quality%2C%20tested%2C%0Aopen-source%20package.%20It%20combines%20cutting-edge%20models%20and%20customizability%20with%20a%0Agentle%20learning%20curve%20and%20simple%20user%20interface%20for%20newcomers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Library%2520for%2520Learning%2520Neural%2520Operators%26entry.906535625%3DJean%2520Kossaifi%2520and%2520Nikola%2520Kovachki%2520and%2520Zongyi%2520Li%2520and%2520Davit%2520Pitt%2520and%2520Miguel%2520Liu-Schiaffini%2520and%2520Robert%2520Joseph%2520George%2520and%2520Boris%2520Bonev%2520and%2520Kamyar%2520Azizzadenesheli%2520and%2520Julius%2520Berner%2520and%2520Anima%2520Anandkumar%26entry.1292438233%3D%2520%2520We%2520present%2520NeuralOperator%252C%2520an%2520open-source%2520Python%2520library%2520for%2520operator%250Alearning.%2520Neural%2520operators%2520generalize%2520neural%2520networks%2520to%2520maps%2520between%2520function%250Aspaces%2520instead%2520of%2520finite-dimensional%2520Euclidean%2520spaces.%2520They%2520can%2520be%2520trained%2520and%250Ainferenced%2520on%2520input%2520and%2520output%2520functions%2520given%2520at%2520various%2520discretizations%252C%250Asatisfying%2520a%2520discretization%2520convergence%2520properties.%2520Built%2520on%2520top%2520of%2520PyTorch%252C%250ANeuralOperator%2520provides%2520all%2520the%2520tools%2520for%2520training%2520and%2520deploying%2520neural%250Aoperator%2520models%252C%2520as%2520well%2520as%2520developing%2520new%2520ones%252C%2520in%2520a%2520high-quality%252C%2520tested%252C%250Aopen-source%2520package.%2520It%2520combines%2520cutting-edge%2520models%2520and%2520customizability%2520with%2520a%250Agentle%2520learning%2520curve%2520and%2520simple%2520user%2520interface%2520for%2520newcomers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Library%20for%20Learning%20Neural%20Operators&entry.906535625=Jean%20Kossaifi%20and%20Nikola%20Kovachki%20and%20Zongyi%20Li%20and%20Davit%20Pitt%20and%20Miguel%20Liu-Schiaffini%20and%20Robert%20Joseph%20George%20and%20Boris%20Bonev%20and%20Kamyar%20Azizzadenesheli%20and%20Julius%20Berner%20and%20Anima%20Anandkumar&entry.1292438233=%20%20We%20present%20NeuralOperator%2C%20an%20open-source%20Python%20library%20for%20operator%0Alearning.%20Neural%20operators%20generalize%20neural%20networks%20to%20maps%20between%20function%0Aspaces%20instead%20of%20finite-dimensional%20Euclidean%20spaces.%20They%20can%20be%20trained%20and%0Ainferenced%20on%20input%20and%20output%20functions%20given%20at%20various%20discretizations%2C%0Asatisfying%20a%20discretization%20convergence%20properties.%20Built%20on%20top%20of%20PyTorch%2C%0ANeuralOperator%20provides%20all%20the%20tools%20for%20training%20and%20deploying%20neural%0Aoperator%20models%2C%20as%20well%20as%20developing%20new%20ones%2C%20in%20a%20high-quality%2C%20tested%2C%0Aopen-source%20package.%20It%20combines%20cutting-edge%20models%20and%20customizability%20with%20a%0Agentle%20learning%20curve%20and%20simple%20user%20interface%20for%20newcomers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10354v1&entry.124074799=Read"},
{"title": "Hidden Biases of End-to-End Driving Datasets", "author": "Julian Zimmerlin and Jens Bei\u00dfwenger and Bernhard Jaeger and Andreas Geiger and Kashyap Chitta", "abstract": "  End-to-end driving systems have made rapid progress, but have so far not been\napplied to the challenging new CARLA Leaderboard 2.0. Further, while there is a\nlarge body of literature on end-to-end architectures and training strategies,\nthe impact of the training dataset is often overlooked. In this work, we make a\nfirst attempt at end-to-end driving for Leaderboard 2.0. Instead of\ninvestigating architectures, we systematically analyze the training dataset,\nleading to new insights: (1) Expert style significantly affects downstream\npolicy performance. (2) In complex data sets, the frames should not be weighted\non the basis of simplistic criteria such as class frequencies. (3) Instead,\nestimating whether a frame changes the target labels compared to previous\nframes can reduce the size of the dataset without removing important\ninformation. By incorporating these findings, our model ranks first and second\nrespectively on the map and sensors tracks of the 2024 CARLA Challenge, and\nsets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncover\na design flaw in the current evaluation metrics and propose a modification for\nfuture challenges. Our dataset, code, and pre-trained models are publicly\navailable at https://github.com/autonomousvision/carla_garage.\n", "link": "http://arxiv.org/abs/2412.09602v2", "date": "2024-12-13", "relevancy": 2.1144, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5427}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5195}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hidden%20Biases%20of%20End-to-End%20Driving%20Datasets&body=Title%3A%20Hidden%20Biases%20of%20End-to-End%20Driving%20Datasets%0AAuthor%3A%20Julian%20Zimmerlin%20and%20Jens%20Bei%C3%9Fwenger%20and%20Bernhard%20Jaeger%20and%20Andreas%20Geiger%20and%20Kashyap%20Chitta%0AAbstract%3A%20%20%20End-to-end%20driving%20systems%20have%20made%20rapid%20progress%2C%20but%20have%20so%20far%20not%20been%0Aapplied%20to%20the%20challenging%20new%20CARLA%20Leaderboard%202.0.%20Further%2C%20while%20there%20is%20a%0Alarge%20body%20of%20literature%20on%20end-to-end%20architectures%20and%20training%20strategies%2C%0Athe%20impact%20of%20the%20training%20dataset%20is%20often%20overlooked.%20In%20this%20work%2C%20we%20make%20a%0Afirst%20attempt%20at%20end-to-end%20driving%20for%20Leaderboard%202.0.%20Instead%20of%0Ainvestigating%20architectures%2C%20we%20systematically%20analyze%20the%20training%20dataset%2C%0Aleading%20to%20new%20insights%3A%20%281%29%20Expert%20style%20significantly%20affects%20downstream%0Apolicy%20performance.%20%282%29%20In%20complex%20data%20sets%2C%20the%20frames%20should%20not%20be%20weighted%0Aon%20the%20basis%20of%20simplistic%20criteria%20such%20as%20class%20frequencies.%20%283%29%20Instead%2C%0Aestimating%20whether%20a%20frame%20changes%20the%20target%20labels%20compared%20to%20previous%0Aframes%20can%20reduce%20the%20size%20of%20the%20dataset%20without%20removing%20important%0Ainformation.%20By%20incorporating%20these%20findings%2C%20our%20model%20ranks%20first%20and%20second%0Arespectively%20on%20the%20map%20and%20sensors%20tracks%20of%20the%202024%20CARLA%20Challenge%2C%20and%0Asets%20a%20new%20state-of-the-art%20on%20the%20Bench2Drive%20test%20routes.%20Finally%2C%20we%20uncover%0Aa%20design%20flaw%20in%20the%20current%20evaluation%20metrics%20and%20propose%20a%20modification%20for%0Afuture%20challenges.%20Our%20dataset%2C%20code%2C%20and%20pre-trained%20models%20are%20publicly%0Aavailable%20at%20https%3A//github.com/autonomousvision/carla_garage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09602v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHidden%2520Biases%2520of%2520End-to-End%2520Driving%2520Datasets%26entry.906535625%3DJulian%2520Zimmerlin%2520and%2520Jens%2520Bei%25C3%259Fwenger%2520and%2520Bernhard%2520Jaeger%2520and%2520Andreas%2520Geiger%2520and%2520Kashyap%2520Chitta%26entry.1292438233%3D%2520%2520End-to-end%2520driving%2520systems%2520have%2520made%2520rapid%2520progress%252C%2520but%2520have%2520so%2520far%2520not%2520been%250Aapplied%2520to%2520the%2520challenging%2520new%2520CARLA%2520Leaderboard%25202.0.%2520Further%252C%2520while%2520there%2520is%2520a%250Alarge%2520body%2520of%2520literature%2520on%2520end-to-end%2520architectures%2520and%2520training%2520strategies%252C%250Athe%2520impact%2520of%2520the%2520training%2520dataset%2520is%2520often%2520overlooked.%2520In%2520this%2520work%252C%2520we%2520make%2520a%250Afirst%2520attempt%2520at%2520end-to-end%2520driving%2520for%2520Leaderboard%25202.0.%2520Instead%2520of%250Ainvestigating%2520architectures%252C%2520we%2520systematically%2520analyze%2520the%2520training%2520dataset%252C%250Aleading%2520to%2520new%2520insights%253A%2520%25281%2529%2520Expert%2520style%2520significantly%2520affects%2520downstream%250Apolicy%2520performance.%2520%25282%2529%2520In%2520complex%2520data%2520sets%252C%2520the%2520frames%2520should%2520not%2520be%2520weighted%250Aon%2520the%2520basis%2520of%2520simplistic%2520criteria%2520such%2520as%2520class%2520frequencies.%2520%25283%2529%2520Instead%252C%250Aestimating%2520whether%2520a%2520frame%2520changes%2520the%2520target%2520labels%2520compared%2520to%2520previous%250Aframes%2520can%2520reduce%2520the%2520size%2520of%2520the%2520dataset%2520without%2520removing%2520important%250Ainformation.%2520By%2520incorporating%2520these%2520findings%252C%2520our%2520model%2520ranks%2520first%2520and%2520second%250Arespectively%2520on%2520the%2520map%2520and%2520sensors%2520tracks%2520of%2520the%25202024%2520CARLA%2520Challenge%252C%2520and%250Asets%2520a%2520new%2520state-of-the-art%2520on%2520the%2520Bench2Drive%2520test%2520routes.%2520Finally%252C%2520we%2520uncover%250Aa%2520design%2520flaw%2520in%2520the%2520current%2520evaluation%2520metrics%2520and%2520propose%2520a%2520modification%2520for%250Afuture%2520challenges.%2520Our%2520dataset%252C%2520code%252C%2520and%2520pre-trained%2520models%2520are%2520publicly%250Aavailable%2520at%2520https%253A//github.com/autonomousvision/carla_garage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09602v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hidden%20Biases%20of%20End-to-End%20Driving%20Datasets&entry.906535625=Julian%20Zimmerlin%20and%20Jens%20Bei%C3%9Fwenger%20and%20Bernhard%20Jaeger%20and%20Andreas%20Geiger%20and%20Kashyap%20Chitta&entry.1292438233=%20%20End-to-end%20driving%20systems%20have%20made%20rapid%20progress%2C%20but%20have%20so%20far%20not%20been%0Aapplied%20to%20the%20challenging%20new%20CARLA%20Leaderboard%202.0.%20Further%2C%20while%20there%20is%20a%0Alarge%20body%20of%20literature%20on%20end-to-end%20architectures%20and%20training%20strategies%2C%0Athe%20impact%20of%20the%20training%20dataset%20is%20often%20overlooked.%20In%20this%20work%2C%20we%20make%20a%0Afirst%20attempt%20at%20end-to-end%20driving%20for%20Leaderboard%202.0.%20Instead%20of%0Ainvestigating%20architectures%2C%20we%20systematically%20analyze%20the%20training%20dataset%2C%0Aleading%20to%20new%20insights%3A%20%281%29%20Expert%20style%20significantly%20affects%20downstream%0Apolicy%20performance.%20%282%29%20In%20complex%20data%20sets%2C%20the%20frames%20should%20not%20be%20weighted%0Aon%20the%20basis%20of%20simplistic%20criteria%20such%20as%20class%20frequencies.%20%283%29%20Instead%2C%0Aestimating%20whether%20a%20frame%20changes%20the%20target%20labels%20compared%20to%20previous%0Aframes%20can%20reduce%20the%20size%20of%20the%20dataset%20without%20removing%20important%0Ainformation.%20By%20incorporating%20these%20findings%2C%20our%20model%20ranks%20first%20and%20second%0Arespectively%20on%20the%20map%20and%20sensors%20tracks%20of%20the%202024%20CARLA%20Challenge%2C%20and%0Asets%20a%20new%20state-of-the-art%20on%20the%20Bench2Drive%20test%20routes.%20Finally%2C%20we%20uncover%0Aa%20design%20flaw%20in%20the%20current%20evaluation%20metrics%20and%20propose%20a%20modification%20for%0Afuture%20challenges.%20Our%20dataset%2C%20code%2C%20and%20pre-trained%20models%20are%20publicly%0Aavailable%20at%20https%3A//github.com/autonomousvision/carla_garage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09602v2&entry.124074799=Read"},
{"title": "Interlocking-free Selective Rationalization Through Genetic-based\n  Learning", "author": "Federico Ruggeri and Gaetano Signorelli", "abstract": "  A popular end-to-end architecture for selective rationalization is the\nselect-then-predict pipeline, comprising a generator to extract highlights fed\nto a predictor. Such a cooperative system suffers from suboptimal equilibrium\nminima due to the dominance of one of the two modules, a phenomenon known as\ninterlocking. While several contributions aimed at addressing interlocking,\nthey only mitigate its effect, often by introducing feature-based heuristics,\nsampling, and ad-hoc regularizations. We present GenSPP, the first\ninterlocking-free architecture for selective rationalization that does not\nrequire any learning overhead, as the above-mentioned. GenSPP avoids\ninterlocking by performing disjoint training of the generator and predictor via\ngenetic global search. Experiments on a synthetic and a real-world benchmark\nshow that our model outperforms several state-of-the-art competitors.\n", "link": "http://arxiv.org/abs/2412.10312v1", "date": "2024-12-13", "relevancy": 2.0906, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5432}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5124}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interlocking-free%20Selective%20Rationalization%20Through%20Genetic-based%0A%20%20Learning&body=Title%3A%20Interlocking-free%20Selective%20Rationalization%20Through%20Genetic-based%0A%20%20Learning%0AAuthor%3A%20Federico%20Ruggeri%20and%20Gaetano%20Signorelli%0AAbstract%3A%20%20%20A%20popular%20end-to-end%20architecture%20for%20selective%20rationalization%20is%20the%0Aselect-then-predict%20pipeline%2C%20comprising%20a%20generator%20to%20extract%20highlights%20fed%0Ato%20a%20predictor.%20Such%20a%20cooperative%20system%20suffers%20from%20suboptimal%20equilibrium%0Aminima%20due%20to%20the%20dominance%20of%20one%20of%20the%20two%20modules%2C%20a%20phenomenon%20known%20as%0Ainterlocking.%20While%20several%20contributions%20aimed%20at%20addressing%20interlocking%2C%0Athey%20only%20mitigate%20its%20effect%2C%20often%20by%20introducing%20feature-based%20heuristics%2C%0Asampling%2C%20and%20ad-hoc%20regularizations.%20We%20present%20GenSPP%2C%20the%20first%0Ainterlocking-free%20architecture%20for%20selective%20rationalization%20that%20does%20not%0Arequire%20any%20learning%20overhead%2C%20as%20the%20above-mentioned.%20GenSPP%20avoids%0Ainterlocking%20by%20performing%20disjoint%20training%20of%20the%20generator%20and%20predictor%20via%0Agenetic%20global%20search.%20Experiments%20on%20a%20synthetic%20and%20a%20real-world%20benchmark%0Ashow%20that%20our%20model%20outperforms%20several%20state-of-the-art%20competitors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterlocking-free%2520Selective%2520Rationalization%2520Through%2520Genetic-based%250A%2520%2520Learning%26entry.906535625%3DFederico%2520Ruggeri%2520and%2520Gaetano%2520Signorelli%26entry.1292438233%3D%2520%2520A%2520popular%2520end-to-end%2520architecture%2520for%2520selective%2520rationalization%2520is%2520the%250Aselect-then-predict%2520pipeline%252C%2520comprising%2520a%2520generator%2520to%2520extract%2520highlights%2520fed%250Ato%2520a%2520predictor.%2520Such%2520a%2520cooperative%2520system%2520suffers%2520from%2520suboptimal%2520equilibrium%250Aminima%2520due%2520to%2520the%2520dominance%2520of%2520one%2520of%2520the%2520two%2520modules%252C%2520a%2520phenomenon%2520known%2520as%250Ainterlocking.%2520While%2520several%2520contributions%2520aimed%2520at%2520addressing%2520interlocking%252C%250Athey%2520only%2520mitigate%2520its%2520effect%252C%2520often%2520by%2520introducing%2520feature-based%2520heuristics%252C%250Asampling%252C%2520and%2520ad-hoc%2520regularizations.%2520We%2520present%2520GenSPP%252C%2520the%2520first%250Ainterlocking-free%2520architecture%2520for%2520selective%2520rationalization%2520that%2520does%2520not%250Arequire%2520any%2520learning%2520overhead%252C%2520as%2520the%2520above-mentioned.%2520GenSPP%2520avoids%250Ainterlocking%2520by%2520performing%2520disjoint%2520training%2520of%2520the%2520generator%2520and%2520predictor%2520via%250Agenetic%2520global%2520search.%2520Experiments%2520on%2520a%2520synthetic%2520and%2520a%2520real-world%2520benchmark%250Ashow%2520that%2520our%2520model%2520outperforms%2520several%2520state-of-the-art%2520competitors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interlocking-free%20Selective%20Rationalization%20Through%20Genetic-based%0A%20%20Learning&entry.906535625=Federico%20Ruggeri%20and%20Gaetano%20Signorelli&entry.1292438233=%20%20A%20popular%20end-to-end%20architecture%20for%20selective%20rationalization%20is%20the%0Aselect-then-predict%20pipeline%2C%20comprising%20a%20generator%20to%20extract%20highlights%20fed%0Ato%20a%20predictor.%20Such%20a%20cooperative%20system%20suffers%20from%20suboptimal%20equilibrium%0Aminima%20due%20to%20the%20dominance%20of%20one%20of%20the%20two%20modules%2C%20a%20phenomenon%20known%20as%0Ainterlocking.%20While%20several%20contributions%20aimed%20at%20addressing%20interlocking%2C%0Athey%20only%20mitigate%20its%20effect%2C%20often%20by%20introducing%20feature-based%20heuristics%2C%0Asampling%2C%20and%20ad-hoc%20regularizations.%20We%20present%20GenSPP%2C%20the%20first%0Ainterlocking-free%20architecture%20for%20selective%20rationalization%20that%20does%20not%0Arequire%20any%20learning%20overhead%2C%20as%20the%20above-mentioned.%20GenSPP%20avoids%0Ainterlocking%20by%20performing%20disjoint%20training%20of%20the%20generator%20and%20predictor%20via%0Agenetic%20global%20search.%20Experiments%20on%20a%20synthetic%20and%20a%20real-world%20benchmark%0Ashow%20that%20our%20model%20outperforms%20several%20state-of-the-art%20competitors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10312v1&entry.124074799=Read"},
{"title": "Large Action Models: From Inception to Implementation", "author": "Lu Wang and Fangkai Yang and Chaoyun Zhang and Junting Lu and Jiaxu Qian and Shilin He and Pu Zhao and Bo Qiao and Ray Huang and Si Qin and Qisheng Su and Jiayi Ye and Yudi Zhang and Jian-Guang Lou and Qingwei Lin and Saravan Rajmohan and Dongmei Zhang and Qi Zhang", "abstract": "  As AI continues to advance, there is a growing demand for systems that go\nbeyond language-based assistance and move toward intelligent agents capable of\nperforming real-world actions. This evolution requires the transition from\ntraditional Large Language Models (LLMs), which excel at generating textual\nresponses, to Large Action Models (LAMs), designed for action generation and\nexecution within dynamic environments. Enabled by agent systems, LAMs hold the\npotential to transform AI from passive language understanding to active task\ncompletion, marking a significant milestone in the progression toward\nartificial general intelligence.\n  In this paper, we present a comprehensive framework for developing LAMs,\noffering a systematic approach to their creation, from inception to deployment.\nWe begin with an overview of LAMs, highlighting their unique characteristics\nand delineating their differences from LLMs. Using a Windows OS-based agent as\na case study, we provide a detailed, step-by-step guide on the key stages of\nLAM development, including data collection, model training, environment\nintegration, grounding, and evaluation. This generalizable workflow can serve\nas a blueprint for creating functional LAMs in various application domains. We\nconclude by identifying the current limitations of LAMs and discussing\ndirections for future research and industrial deployment, emphasizing the\nchallenges and opportunities that lie ahead in realizing the full potential of\nLAMs in real-world applications.\n  The code for the data collection process utilized in this paper is publicly\navailable at: https://github.com/microsoft/UFO/tree/main/dataflow, and\ncomprehensive documentation can be found at\nhttps://microsoft.github.io/UFO/dataflow/overview/.\n", "link": "http://arxiv.org/abs/2412.10047v1", "date": "2024-12-13", "relevancy": 2.0862, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5493}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5168}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Action%20Models%3A%20From%20Inception%20to%20Implementation&body=Title%3A%20Large%20Action%20Models%3A%20From%20Inception%20to%20Implementation%0AAuthor%3A%20Lu%20Wang%20and%20Fangkai%20Yang%20and%20Chaoyun%20Zhang%20and%20Junting%20Lu%20and%20Jiaxu%20Qian%20and%20Shilin%20He%20and%20Pu%20Zhao%20and%20Bo%20Qiao%20and%20Ray%20Huang%20and%20Si%20Qin%20and%20Qisheng%20Su%20and%20Jiayi%20Ye%20and%20Yudi%20Zhang%20and%20Jian-Guang%20Lou%20and%20Qingwei%20Lin%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%20and%20Qi%20Zhang%0AAbstract%3A%20%20%20As%20AI%20continues%20to%20advance%2C%20there%20is%20a%20growing%20demand%20for%20systems%20that%20go%0Abeyond%20language-based%20assistance%20and%20move%20toward%20intelligent%20agents%20capable%20of%0Aperforming%20real-world%20actions.%20This%20evolution%20requires%20the%20transition%20from%0Atraditional%20Large%20Language%20Models%20%28LLMs%29%2C%20which%20excel%20at%20generating%20textual%0Aresponses%2C%20to%20Large%20Action%20Models%20%28LAMs%29%2C%20designed%20for%20action%20generation%20and%0Aexecution%20within%20dynamic%20environments.%20Enabled%20by%20agent%20systems%2C%20LAMs%20hold%20the%0Apotential%20to%20transform%20AI%20from%20passive%20language%20understanding%20to%20active%20task%0Acompletion%2C%20marking%20a%20significant%20milestone%20in%20the%20progression%20toward%0Aartificial%20general%20intelligence.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20framework%20for%20developing%20LAMs%2C%0Aoffering%20a%20systematic%20approach%20to%20their%20creation%2C%20from%20inception%20to%20deployment.%0AWe%20begin%20with%20an%20overview%20of%20LAMs%2C%20highlighting%20their%20unique%20characteristics%0Aand%20delineating%20their%20differences%20from%20LLMs.%20Using%20a%20Windows%20OS-based%20agent%20as%0Aa%20case%20study%2C%20we%20provide%20a%20detailed%2C%20step-by-step%20guide%20on%20the%20key%20stages%20of%0ALAM%20development%2C%20including%20data%20collection%2C%20model%20training%2C%20environment%0Aintegration%2C%20grounding%2C%20and%20evaluation.%20This%20generalizable%20workflow%20can%20serve%0Aas%20a%20blueprint%20for%20creating%20functional%20LAMs%20in%20various%20application%20domains.%20We%0Aconclude%20by%20identifying%20the%20current%20limitations%20of%20LAMs%20and%20discussing%0Adirections%20for%20future%20research%20and%20industrial%20deployment%2C%20emphasizing%20the%0Achallenges%20and%20opportunities%20that%20lie%20ahead%20in%20realizing%20the%20full%20potential%20of%0ALAMs%20in%20real-world%20applications.%0A%20%20The%20code%20for%20the%20data%20collection%20process%20utilized%20in%20this%20paper%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/microsoft/UFO/tree/main/dataflow%2C%20and%0Acomprehensive%20documentation%20can%20be%20found%20at%0Ahttps%3A//microsoft.github.io/UFO/dataflow/overview/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Action%2520Models%253A%2520From%2520Inception%2520to%2520Implementation%26entry.906535625%3DLu%2520Wang%2520and%2520Fangkai%2520Yang%2520and%2520Chaoyun%2520Zhang%2520and%2520Junting%2520Lu%2520and%2520Jiaxu%2520Qian%2520and%2520Shilin%2520He%2520and%2520Pu%2520Zhao%2520and%2520Bo%2520Qiao%2520and%2520Ray%2520Huang%2520and%2520Si%2520Qin%2520and%2520Qisheng%2520Su%2520and%2520Jiayi%2520Ye%2520and%2520Yudi%2520Zhang%2520and%2520Jian-Guang%2520Lou%2520and%2520Qingwei%2520Lin%2520and%2520Saravan%2520Rajmohan%2520and%2520Dongmei%2520Zhang%2520and%2520Qi%2520Zhang%26entry.1292438233%3D%2520%2520As%2520AI%2520continues%2520to%2520advance%252C%2520there%2520is%2520a%2520growing%2520demand%2520for%2520systems%2520that%2520go%250Abeyond%2520language-based%2520assistance%2520and%2520move%2520toward%2520intelligent%2520agents%2520capable%2520of%250Aperforming%2520real-world%2520actions.%2520This%2520evolution%2520requires%2520the%2520transition%2520from%250Atraditional%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520which%2520excel%2520at%2520generating%2520textual%250Aresponses%252C%2520to%2520Large%2520Action%2520Models%2520%2528LAMs%2529%252C%2520designed%2520for%2520action%2520generation%2520and%250Aexecution%2520within%2520dynamic%2520environments.%2520Enabled%2520by%2520agent%2520systems%252C%2520LAMs%2520hold%2520the%250Apotential%2520to%2520transform%2520AI%2520from%2520passive%2520language%2520understanding%2520to%2520active%2520task%250Acompletion%252C%2520marking%2520a%2520significant%2520milestone%2520in%2520the%2520progression%2520toward%250Aartificial%2520general%2520intelligence.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520framework%2520for%2520developing%2520LAMs%252C%250Aoffering%2520a%2520systematic%2520approach%2520to%2520their%2520creation%252C%2520from%2520inception%2520to%2520deployment.%250AWe%2520begin%2520with%2520an%2520overview%2520of%2520LAMs%252C%2520highlighting%2520their%2520unique%2520characteristics%250Aand%2520delineating%2520their%2520differences%2520from%2520LLMs.%2520Using%2520a%2520Windows%2520OS-based%2520agent%2520as%250Aa%2520case%2520study%252C%2520we%2520provide%2520a%2520detailed%252C%2520step-by-step%2520guide%2520on%2520the%2520key%2520stages%2520of%250ALAM%2520development%252C%2520including%2520data%2520collection%252C%2520model%2520training%252C%2520environment%250Aintegration%252C%2520grounding%252C%2520and%2520evaluation.%2520This%2520generalizable%2520workflow%2520can%2520serve%250Aas%2520a%2520blueprint%2520for%2520creating%2520functional%2520LAMs%2520in%2520various%2520application%2520domains.%2520We%250Aconclude%2520by%2520identifying%2520the%2520current%2520limitations%2520of%2520LAMs%2520and%2520discussing%250Adirections%2520for%2520future%2520research%2520and%2520industrial%2520deployment%252C%2520emphasizing%2520the%250Achallenges%2520and%2520opportunities%2520that%2520lie%2520ahead%2520in%2520realizing%2520the%2520full%2520potential%2520of%250ALAMs%2520in%2520real-world%2520applications.%250A%2520%2520The%2520code%2520for%2520the%2520data%2520collection%2520process%2520utilized%2520in%2520this%2520paper%2520is%2520publicly%250Aavailable%2520at%253A%2520https%253A//github.com/microsoft/UFO/tree/main/dataflow%252C%2520and%250Acomprehensive%2520documentation%2520can%2520be%2520found%2520at%250Ahttps%253A//microsoft.github.io/UFO/dataflow/overview/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Action%20Models%3A%20From%20Inception%20to%20Implementation&entry.906535625=Lu%20Wang%20and%20Fangkai%20Yang%20and%20Chaoyun%20Zhang%20and%20Junting%20Lu%20and%20Jiaxu%20Qian%20and%20Shilin%20He%20and%20Pu%20Zhao%20and%20Bo%20Qiao%20and%20Ray%20Huang%20and%20Si%20Qin%20and%20Qisheng%20Su%20and%20Jiayi%20Ye%20and%20Yudi%20Zhang%20and%20Jian-Guang%20Lou%20and%20Qingwei%20Lin%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%20and%20Qi%20Zhang&entry.1292438233=%20%20As%20AI%20continues%20to%20advance%2C%20there%20is%20a%20growing%20demand%20for%20systems%20that%20go%0Abeyond%20language-based%20assistance%20and%20move%20toward%20intelligent%20agents%20capable%20of%0Aperforming%20real-world%20actions.%20This%20evolution%20requires%20the%20transition%20from%0Atraditional%20Large%20Language%20Models%20%28LLMs%29%2C%20which%20excel%20at%20generating%20textual%0Aresponses%2C%20to%20Large%20Action%20Models%20%28LAMs%29%2C%20designed%20for%20action%20generation%20and%0Aexecution%20within%20dynamic%20environments.%20Enabled%20by%20agent%20systems%2C%20LAMs%20hold%20the%0Apotential%20to%20transform%20AI%20from%20passive%20language%20understanding%20to%20active%20task%0Acompletion%2C%20marking%20a%20significant%20milestone%20in%20the%20progression%20toward%0Aartificial%20general%20intelligence.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20framework%20for%20developing%20LAMs%2C%0Aoffering%20a%20systematic%20approach%20to%20their%20creation%2C%20from%20inception%20to%20deployment.%0AWe%20begin%20with%20an%20overview%20of%20LAMs%2C%20highlighting%20their%20unique%20characteristics%0Aand%20delineating%20their%20differences%20from%20LLMs.%20Using%20a%20Windows%20OS-based%20agent%20as%0Aa%20case%20study%2C%20we%20provide%20a%20detailed%2C%20step-by-step%20guide%20on%20the%20key%20stages%20of%0ALAM%20development%2C%20including%20data%20collection%2C%20model%20training%2C%20environment%0Aintegration%2C%20grounding%2C%20and%20evaluation.%20This%20generalizable%20workflow%20can%20serve%0Aas%20a%20blueprint%20for%20creating%20functional%20LAMs%20in%20various%20application%20domains.%20We%0Aconclude%20by%20identifying%20the%20current%20limitations%20of%20LAMs%20and%20discussing%0Adirections%20for%20future%20research%20and%20industrial%20deployment%2C%20emphasizing%20the%0Achallenges%20and%20opportunities%20that%20lie%20ahead%20in%20realizing%20the%20full%20potential%20of%0ALAMs%20in%20real-world%20applications.%0A%20%20The%20code%20for%20the%20data%20collection%20process%20utilized%20in%20this%20paper%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/microsoft/UFO/tree/main/dataflow%2C%20and%0Acomprehensive%20documentation%20can%20be%20found%20at%0Ahttps%3A//microsoft.github.io/UFO/dataflow/overview/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10047v1&entry.124074799=Read"},
{"title": "Guidance Not Obstruction: A Conjugate Consistent Enhanced Strategy for\n  Domain Generalization", "author": "Meng Cao and Songcan Chen", "abstract": "  Domain generalization addresses domain shift in real-world applications. Most\napproaches adopt a domain angle, seeking invariant representation across\ndomains by aligning their marginal distributions, irrespective of individual\nclasses, naturally leading to insufficient exploration of discriminative\ninformation. Switching to a class angle, we find that multiple domain-related\npeaks or clusters within the same individual classes must emerge due to\ndistribution shift. In other words, marginal alignment does not guarantee\nconditional alignment, leading to suboptimal generalization. Therefore, we\nargue that acquiring discriminative generalization between classes within\ndomains is crucial. In contrast to seeking distribution alignment, we endeavor\nto safeguard domain-related between-class discrimination. To this end, we\ndevise a novel Conjugate Consistent Enhanced Module, namely Con2EM, based on a\ndistribution over domains, i.e., a meta-distribution. Specifically, we employ a\nnovel distribution-level Universum strategy to generate supplementary diverse\ndomain-related class-conditional distributions, thereby enhancing\ngeneralization. This allows us to resample from these generated distributions\nto provide feedback to the primordial instance-level classifier, further\nimproving its adaptability to the target-agnostic. To ensure generation\naccuracy, we establish an additional distribution-level classifier to\nregularize these conditional distributions. Extensive experiments have been\nconducted to demonstrate its effectiveness and low computational cost compared\nto SOTAs.\n", "link": "http://arxiv.org/abs/2412.10089v1", "date": "2024-12-13", "relevancy": 2.0762, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5434}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5203}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guidance%20Not%20Obstruction%3A%20A%20Conjugate%20Consistent%20Enhanced%20Strategy%20for%0A%20%20Domain%20Generalization&body=Title%3A%20Guidance%20Not%20Obstruction%3A%20A%20Conjugate%20Consistent%20Enhanced%20Strategy%20for%0A%20%20Domain%20Generalization%0AAuthor%3A%20Meng%20Cao%20and%20Songcan%20Chen%0AAbstract%3A%20%20%20Domain%20generalization%20addresses%20domain%20shift%20in%20real-world%20applications.%20Most%0Aapproaches%20adopt%20a%20domain%20angle%2C%20seeking%20invariant%20representation%20across%0Adomains%20by%20aligning%20their%20marginal%20distributions%2C%20irrespective%20of%20individual%0Aclasses%2C%20naturally%20leading%20to%20insufficient%20exploration%20of%20discriminative%0Ainformation.%20Switching%20to%20a%20class%20angle%2C%20we%20find%20that%20multiple%20domain-related%0Apeaks%20or%20clusters%20within%20the%20same%20individual%20classes%20must%20emerge%20due%20to%0Adistribution%20shift.%20In%20other%20words%2C%20marginal%20alignment%20does%20not%20guarantee%0Aconditional%20alignment%2C%20leading%20to%20suboptimal%20generalization.%20Therefore%2C%20we%0Aargue%20that%20acquiring%20discriminative%20generalization%20between%20classes%20within%0Adomains%20is%20crucial.%20In%20contrast%20to%20seeking%20distribution%20alignment%2C%20we%20endeavor%0Ato%20safeguard%20domain-related%20between-class%20discrimination.%20To%20this%20end%2C%20we%0Adevise%20a%20novel%20Conjugate%20Consistent%20Enhanced%20Module%2C%20namely%20Con2EM%2C%20based%20on%20a%0Adistribution%20over%20domains%2C%20i.e.%2C%20a%20meta-distribution.%20Specifically%2C%20we%20employ%20a%0Anovel%20distribution-level%20Universum%20strategy%20to%20generate%20supplementary%20diverse%0Adomain-related%20class-conditional%20distributions%2C%20thereby%20enhancing%0Ageneralization.%20This%20allows%20us%20to%20resample%20from%20these%20generated%20distributions%0Ato%20provide%20feedback%20to%20the%20primordial%20instance-level%20classifier%2C%20further%0Aimproving%20its%20adaptability%20to%20the%20target-agnostic.%20To%20ensure%20generation%0Aaccuracy%2C%20we%20establish%20an%20additional%20distribution-level%20classifier%20to%0Aregularize%20these%20conditional%20distributions.%20Extensive%20experiments%20have%20been%0Aconducted%20to%20demonstrate%20its%20effectiveness%20and%20low%20computational%20cost%20compared%0Ato%20SOTAs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuidance%2520Not%2520Obstruction%253A%2520A%2520Conjugate%2520Consistent%2520Enhanced%2520Strategy%2520for%250A%2520%2520Domain%2520Generalization%26entry.906535625%3DMeng%2520Cao%2520and%2520Songcan%2520Chen%26entry.1292438233%3D%2520%2520Domain%2520generalization%2520addresses%2520domain%2520shift%2520in%2520real-world%2520applications.%2520Most%250Aapproaches%2520adopt%2520a%2520domain%2520angle%252C%2520seeking%2520invariant%2520representation%2520across%250Adomains%2520by%2520aligning%2520their%2520marginal%2520distributions%252C%2520irrespective%2520of%2520individual%250Aclasses%252C%2520naturally%2520leading%2520to%2520insufficient%2520exploration%2520of%2520discriminative%250Ainformation.%2520Switching%2520to%2520a%2520class%2520angle%252C%2520we%2520find%2520that%2520multiple%2520domain-related%250Apeaks%2520or%2520clusters%2520within%2520the%2520same%2520individual%2520classes%2520must%2520emerge%2520due%2520to%250Adistribution%2520shift.%2520In%2520other%2520words%252C%2520marginal%2520alignment%2520does%2520not%2520guarantee%250Aconditional%2520alignment%252C%2520leading%2520to%2520suboptimal%2520generalization.%2520Therefore%252C%2520we%250Aargue%2520that%2520acquiring%2520discriminative%2520generalization%2520between%2520classes%2520within%250Adomains%2520is%2520crucial.%2520In%2520contrast%2520to%2520seeking%2520distribution%2520alignment%252C%2520we%2520endeavor%250Ato%2520safeguard%2520domain-related%2520between-class%2520discrimination.%2520To%2520this%2520end%252C%2520we%250Adevise%2520a%2520novel%2520Conjugate%2520Consistent%2520Enhanced%2520Module%252C%2520namely%2520Con2EM%252C%2520based%2520on%2520a%250Adistribution%2520over%2520domains%252C%2520i.e.%252C%2520a%2520meta-distribution.%2520Specifically%252C%2520we%2520employ%2520a%250Anovel%2520distribution-level%2520Universum%2520strategy%2520to%2520generate%2520supplementary%2520diverse%250Adomain-related%2520class-conditional%2520distributions%252C%2520thereby%2520enhancing%250Ageneralization.%2520This%2520allows%2520us%2520to%2520resample%2520from%2520these%2520generated%2520distributions%250Ato%2520provide%2520feedback%2520to%2520the%2520primordial%2520instance-level%2520classifier%252C%2520further%250Aimproving%2520its%2520adaptability%2520to%2520the%2520target-agnostic.%2520To%2520ensure%2520generation%250Aaccuracy%252C%2520we%2520establish%2520an%2520additional%2520distribution-level%2520classifier%2520to%250Aregularize%2520these%2520conditional%2520distributions.%2520Extensive%2520experiments%2520have%2520been%250Aconducted%2520to%2520demonstrate%2520its%2520effectiveness%2520and%2520low%2520computational%2520cost%2520compared%250Ato%2520SOTAs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guidance%20Not%20Obstruction%3A%20A%20Conjugate%20Consistent%20Enhanced%20Strategy%20for%0A%20%20Domain%20Generalization&entry.906535625=Meng%20Cao%20and%20Songcan%20Chen&entry.1292438233=%20%20Domain%20generalization%20addresses%20domain%20shift%20in%20real-world%20applications.%20Most%0Aapproaches%20adopt%20a%20domain%20angle%2C%20seeking%20invariant%20representation%20across%0Adomains%20by%20aligning%20their%20marginal%20distributions%2C%20irrespective%20of%20individual%0Aclasses%2C%20naturally%20leading%20to%20insufficient%20exploration%20of%20discriminative%0Ainformation.%20Switching%20to%20a%20class%20angle%2C%20we%20find%20that%20multiple%20domain-related%0Apeaks%20or%20clusters%20within%20the%20same%20individual%20classes%20must%20emerge%20due%20to%0Adistribution%20shift.%20In%20other%20words%2C%20marginal%20alignment%20does%20not%20guarantee%0Aconditional%20alignment%2C%20leading%20to%20suboptimal%20generalization.%20Therefore%2C%20we%0Aargue%20that%20acquiring%20discriminative%20generalization%20between%20classes%20within%0Adomains%20is%20crucial.%20In%20contrast%20to%20seeking%20distribution%20alignment%2C%20we%20endeavor%0Ato%20safeguard%20domain-related%20between-class%20discrimination.%20To%20this%20end%2C%20we%0Adevise%20a%20novel%20Conjugate%20Consistent%20Enhanced%20Module%2C%20namely%20Con2EM%2C%20based%20on%20a%0Adistribution%20over%20domains%2C%20i.e.%2C%20a%20meta-distribution.%20Specifically%2C%20we%20employ%20a%0Anovel%20distribution-level%20Universum%20strategy%20to%20generate%20supplementary%20diverse%0Adomain-related%20class-conditional%20distributions%2C%20thereby%20enhancing%0Ageneralization.%20This%20allows%20us%20to%20resample%20from%20these%20generated%20distributions%0Ato%20provide%20feedback%20to%20the%20primordial%20instance-level%20classifier%2C%20further%0Aimproving%20its%20adaptability%20to%20the%20target-agnostic.%20To%20ensure%20generation%0Aaccuracy%2C%20we%20establish%20an%20additional%20distribution-level%20classifier%20to%0Aregularize%20these%20conditional%20distributions.%20Extensive%20experiments%20have%20been%0Aconducted%20to%20demonstrate%20its%20effectiveness%20and%20low%20computational%20cost%20compared%0Ato%20SOTAs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10089v1&entry.124074799=Read"},
{"title": "MVQ:Towards Efficient DNN Compression and Acceleration with Masked\n  Vector Quantization", "author": "Shuaiting Li and Chengxuan Wang and Juncan Deng and Zeyu Wang and Zewen Ye and Zongsheng Wang and Haibin Shen and Kejie Huang", "abstract": "  Vector quantization(VQ) is a hardware-friendly DNN compression method that\ncan reduce the storage cost and weight-loading datawidth of hardware\naccelerators. However, conventional VQ techniques lead to significant accuracy\nloss because the important weights are not well preserved. To tackle this\nproblem, a novel approach called MVQ is proposed, which aims at better\napproximating important weights with a limited number of codewords. At the\nalgorithm level, our approach removes the less important weights through N:M\npruning and then minimizes the vector clustering error between the remaining\nweights and codewords by the masked k-means algorithm. Only distances between\nthe unpruned weights and the codewords are computed, which are then used to\nupdate the codewords. At the architecture level, our accelerator implements\nvector quantization on an EWS (Enhanced weight stationary) CNN accelerator and\nproposes a sparse systolic array design to maximize the benefits brought by\nmasked vector quantization.\\\\ Our algorithm is validated on various models for\nimage classification, object detection, and segmentation tasks. Experimental\nresults demonstrate that MVQ not only outperforms conventional vector\nquantization methods at comparable compression ratios but also reduces FLOPs.\nUnder ASIC evaluation, our MVQ accelerator boosts energy efficiency by\n2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared\nwith the base EWS accelerator. Compared to the previous sparse accelerators,\nMVQ achieves 1.73$\\times$ higher energy efficiency.\n", "link": "http://arxiv.org/abs/2412.10261v1", "date": "2024-12-13", "relevancy": 2.0734, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5612}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5247}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVQ%3ATowards%20Efficient%20DNN%20Compression%20and%20Acceleration%20with%20Masked%0A%20%20Vector%20Quantization&body=Title%3A%20MVQ%3ATowards%20Efficient%20DNN%20Compression%20and%20Acceleration%20with%20Masked%0A%20%20Vector%20Quantization%0AAuthor%3A%20Shuaiting%20Li%20and%20Chengxuan%20Wang%20and%20Juncan%20Deng%20and%20Zeyu%20Wang%20and%20Zewen%20Ye%20and%20Zongsheng%20Wang%20and%20Haibin%20Shen%20and%20Kejie%20Huang%0AAbstract%3A%20%20%20Vector%20quantization%28VQ%29%20is%20a%20hardware-friendly%20DNN%20compression%20method%20that%0Acan%20reduce%20the%20storage%20cost%20and%20weight-loading%20datawidth%20of%20hardware%0Aaccelerators.%20However%2C%20conventional%20VQ%20techniques%20lead%20to%20significant%20accuracy%0Aloss%20because%20the%20important%20weights%20are%20not%20well%20preserved.%20To%20tackle%20this%0Aproblem%2C%20a%20novel%20approach%20called%20MVQ%20is%20proposed%2C%20which%20aims%20at%20better%0Aapproximating%20important%20weights%20with%20a%20limited%20number%20of%20codewords.%20At%20the%0Aalgorithm%20level%2C%20our%20approach%20removes%20the%20less%20important%20weights%20through%20N%3AM%0Apruning%20and%20then%20minimizes%20the%20vector%20clustering%20error%20between%20the%20remaining%0Aweights%20and%20codewords%20by%20the%20masked%20k-means%20algorithm.%20Only%20distances%20between%0Athe%20unpruned%20weights%20and%20the%20codewords%20are%20computed%2C%20which%20are%20then%20used%20to%0Aupdate%20the%20codewords.%20At%20the%20architecture%20level%2C%20our%20accelerator%20implements%0Avector%20quantization%20on%20an%20EWS%20%28Enhanced%20weight%20stationary%29%20CNN%20accelerator%20and%0Aproposes%20a%20sparse%20systolic%20array%20design%20to%20maximize%20the%20benefits%20brought%20by%0Amasked%20vector%20quantization.%5C%5C%20Our%20algorithm%20is%20validated%20on%20various%20models%20for%0Aimage%20classification%2C%20object%20detection%2C%20and%20segmentation%20tasks.%20Experimental%0Aresults%20demonstrate%20that%20MVQ%20not%20only%20outperforms%20conventional%20vector%0Aquantization%20methods%20at%20comparable%20compression%20ratios%20but%20also%20reduces%20FLOPs.%0AUnder%20ASIC%20evaluation%2C%20our%20MVQ%20accelerator%20boosts%20energy%20efficiency%20by%0A2.3%24%5Ctimes%24%20and%20reduces%20the%20size%20of%20the%20systolic%20array%20by%2055%5C%25%20when%20compared%0Awith%20the%20base%20EWS%20accelerator.%20Compared%20to%20the%20previous%20sparse%20accelerators%2C%0AMVQ%20achieves%201.73%24%5Ctimes%24%20higher%20energy%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVQ%253ATowards%2520Efficient%2520DNN%2520Compression%2520and%2520Acceleration%2520with%2520Masked%250A%2520%2520Vector%2520Quantization%26entry.906535625%3DShuaiting%2520Li%2520and%2520Chengxuan%2520Wang%2520and%2520Juncan%2520Deng%2520and%2520Zeyu%2520Wang%2520and%2520Zewen%2520Ye%2520and%2520Zongsheng%2520Wang%2520and%2520Haibin%2520Shen%2520and%2520Kejie%2520Huang%26entry.1292438233%3D%2520%2520Vector%2520quantization%2528VQ%2529%2520is%2520a%2520hardware-friendly%2520DNN%2520compression%2520method%2520that%250Acan%2520reduce%2520the%2520storage%2520cost%2520and%2520weight-loading%2520datawidth%2520of%2520hardware%250Aaccelerators.%2520However%252C%2520conventional%2520VQ%2520techniques%2520lead%2520to%2520significant%2520accuracy%250Aloss%2520because%2520the%2520important%2520weights%2520are%2520not%2520well%2520preserved.%2520To%2520tackle%2520this%250Aproblem%252C%2520a%2520novel%2520approach%2520called%2520MVQ%2520is%2520proposed%252C%2520which%2520aims%2520at%2520better%250Aapproximating%2520important%2520weights%2520with%2520a%2520limited%2520number%2520of%2520codewords.%2520At%2520the%250Aalgorithm%2520level%252C%2520our%2520approach%2520removes%2520the%2520less%2520important%2520weights%2520through%2520N%253AM%250Apruning%2520and%2520then%2520minimizes%2520the%2520vector%2520clustering%2520error%2520between%2520the%2520remaining%250Aweights%2520and%2520codewords%2520by%2520the%2520masked%2520k-means%2520algorithm.%2520Only%2520distances%2520between%250Athe%2520unpruned%2520weights%2520and%2520the%2520codewords%2520are%2520computed%252C%2520which%2520are%2520then%2520used%2520to%250Aupdate%2520the%2520codewords.%2520At%2520the%2520architecture%2520level%252C%2520our%2520accelerator%2520implements%250Avector%2520quantization%2520on%2520an%2520EWS%2520%2528Enhanced%2520weight%2520stationary%2529%2520CNN%2520accelerator%2520and%250Aproposes%2520a%2520sparse%2520systolic%2520array%2520design%2520to%2520maximize%2520the%2520benefits%2520brought%2520by%250Amasked%2520vector%2520quantization.%255C%255C%2520Our%2520algorithm%2520is%2520validated%2520on%2520various%2520models%2520for%250Aimage%2520classification%252C%2520object%2520detection%252C%2520and%2520segmentation%2520tasks.%2520Experimental%250Aresults%2520demonstrate%2520that%2520MVQ%2520not%2520only%2520outperforms%2520conventional%2520vector%250Aquantization%2520methods%2520at%2520comparable%2520compression%2520ratios%2520but%2520also%2520reduces%2520FLOPs.%250AUnder%2520ASIC%2520evaluation%252C%2520our%2520MVQ%2520accelerator%2520boosts%2520energy%2520efficiency%2520by%250A2.3%2524%255Ctimes%2524%2520and%2520reduces%2520the%2520size%2520of%2520the%2520systolic%2520array%2520by%252055%255C%2525%2520when%2520compared%250Awith%2520the%2520base%2520EWS%2520accelerator.%2520Compared%2520to%2520the%2520previous%2520sparse%2520accelerators%252C%250AMVQ%2520achieves%25201.73%2524%255Ctimes%2524%2520higher%2520energy%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVQ%3ATowards%20Efficient%20DNN%20Compression%20and%20Acceleration%20with%20Masked%0A%20%20Vector%20Quantization&entry.906535625=Shuaiting%20Li%20and%20Chengxuan%20Wang%20and%20Juncan%20Deng%20and%20Zeyu%20Wang%20and%20Zewen%20Ye%20and%20Zongsheng%20Wang%20and%20Haibin%20Shen%20and%20Kejie%20Huang&entry.1292438233=%20%20Vector%20quantization%28VQ%29%20is%20a%20hardware-friendly%20DNN%20compression%20method%20that%0Acan%20reduce%20the%20storage%20cost%20and%20weight-loading%20datawidth%20of%20hardware%0Aaccelerators.%20However%2C%20conventional%20VQ%20techniques%20lead%20to%20significant%20accuracy%0Aloss%20because%20the%20important%20weights%20are%20not%20well%20preserved.%20To%20tackle%20this%0Aproblem%2C%20a%20novel%20approach%20called%20MVQ%20is%20proposed%2C%20which%20aims%20at%20better%0Aapproximating%20important%20weights%20with%20a%20limited%20number%20of%20codewords.%20At%20the%0Aalgorithm%20level%2C%20our%20approach%20removes%20the%20less%20important%20weights%20through%20N%3AM%0Apruning%20and%20then%20minimizes%20the%20vector%20clustering%20error%20between%20the%20remaining%0Aweights%20and%20codewords%20by%20the%20masked%20k-means%20algorithm.%20Only%20distances%20between%0Athe%20unpruned%20weights%20and%20the%20codewords%20are%20computed%2C%20which%20are%20then%20used%20to%0Aupdate%20the%20codewords.%20At%20the%20architecture%20level%2C%20our%20accelerator%20implements%0Avector%20quantization%20on%20an%20EWS%20%28Enhanced%20weight%20stationary%29%20CNN%20accelerator%20and%0Aproposes%20a%20sparse%20systolic%20array%20design%20to%20maximize%20the%20benefits%20brought%20by%0Amasked%20vector%20quantization.%5C%5C%20Our%20algorithm%20is%20validated%20on%20various%20models%20for%0Aimage%20classification%2C%20object%20detection%2C%20and%20segmentation%20tasks.%20Experimental%0Aresults%20demonstrate%20that%20MVQ%20not%20only%20outperforms%20conventional%20vector%0Aquantization%20methods%20at%20comparable%20compression%20ratios%20but%20also%20reduces%20FLOPs.%0AUnder%20ASIC%20evaluation%2C%20our%20MVQ%20accelerator%20boosts%20energy%20efficiency%20by%0A2.3%24%5Ctimes%24%20and%20reduces%20the%20size%20of%20the%20systolic%20array%20by%2055%5C%25%20when%20compared%0Awith%20the%20base%20EWS%20accelerator.%20Compared%20to%20the%20previous%20sparse%20accelerators%2C%0AMVQ%20achieves%201.73%24%5Ctimes%24%20higher%20energy%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10261v1&entry.124074799=Read"},
{"title": "COMET: Benchmark for Comprehensive Biological Multi-omics Evaluation\n  Tasks and Language Models", "author": "Yuchen Ren and Wenwei Han and Qianyuan Zhang and Yining Tang and Weiqiang Bai and Yuchen Cai and Lifeng Qiao and Hao Jiang and Dong Yuan and Tao Chen and Siqi Sun and Pan Tan and Wanli Ouyang and Nanqing Dong and Xinzhu Ma and Peng Ye", "abstract": "  As key elements within the central dogma, DNA, RNA, and proteins play crucial\nroles in maintaining life by guaranteeing accurate genetic expression and\nimplementation. Although research on these molecules has profoundly impacted\nfields like medicine, agriculture, and industry, the diversity of machine\nlearning approaches-from traditional statistical methods to deep learning\nmodels and large language models-poses challenges for researchers in choosing\nthe most suitable models for specific tasks, especially for cross-omics and\nmulti-omics tasks due to the lack of comprehensive benchmarks. To address this,\nwe introduce the first comprehensive multi-omics benchmark COMET (Benchmark for\nBiological COmprehensive Multi-omics Evaluation Tasks and Language Models),\ndesigned to evaluate models across single-omics, cross-omics, and multi-omics\ntasks. First, we curate and develop a diverse collection of downstream tasks\nand datasets covering key structural and functional aspects in DNA, RNA, and\nproteins, including tasks that span multiple omics levels. Then, we evaluate\nexisting foundational language models for DNA, RNA, and proteins, as well as\nthe newly proposed multi-omics method, offering valuable insights into their\nperformance in integrating and analyzing data from different biological\nmodalities. This benchmark aims to define critical issues in multi-omics\nresearch and guide future directions, ultimately promoting advancements in\nunderstanding biological processes through integrated and different omics data\nanalysis.\n", "link": "http://arxiv.org/abs/2412.10347v1", "date": "2024-12-13", "relevancy": 2.059, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5292}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5292}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COMET%3A%20Benchmark%20for%20Comprehensive%20Biological%20Multi-omics%20Evaluation%0A%20%20Tasks%20and%20Language%20Models&body=Title%3A%20COMET%3A%20Benchmark%20for%20Comprehensive%20Biological%20Multi-omics%20Evaluation%0A%20%20Tasks%20and%20Language%20Models%0AAuthor%3A%20Yuchen%20Ren%20and%20Wenwei%20Han%20and%20Qianyuan%20Zhang%20and%20Yining%20Tang%20and%20Weiqiang%20Bai%20and%20Yuchen%20Cai%20and%20Lifeng%20Qiao%20and%20Hao%20Jiang%20and%20Dong%20Yuan%20and%20Tao%20Chen%20and%20Siqi%20Sun%20and%20Pan%20Tan%20and%20Wanli%20Ouyang%20and%20Nanqing%20Dong%20and%20Xinzhu%20Ma%20and%20Peng%20Ye%0AAbstract%3A%20%20%20As%20key%20elements%20within%20the%20central%20dogma%2C%20DNA%2C%20RNA%2C%20and%20proteins%20play%20crucial%0Aroles%20in%20maintaining%20life%20by%20guaranteeing%20accurate%20genetic%20expression%20and%0Aimplementation.%20Although%20research%20on%20these%20molecules%20has%20profoundly%20impacted%0Afields%20like%20medicine%2C%20agriculture%2C%20and%20industry%2C%20the%20diversity%20of%20machine%0Alearning%20approaches-from%20traditional%20statistical%20methods%20to%20deep%20learning%0Amodels%20and%20large%20language%20models-poses%20challenges%20for%20researchers%20in%20choosing%0Athe%20most%20suitable%20models%20for%20specific%20tasks%2C%20especially%20for%20cross-omics%20and%0Amulti-omics%20tasks%20due%20to%20the%20lack%20of%20comprehensive%20benchmarks.%20To%20address%20this%2C%0Awe%20introduce%20the%20first%20comprehensive%20multi-omics%20benchmark%20COMET%20%28Benchmark%20for%0ABiological%20COmprehensive%20Multi-omics%20Evaluation%20Tasks%20and%20Language%20Models%29%2C%0Adesigned%20to%20evaluate%20models%20across%20single-omics%2C%20cross-omics%2C%20and%20multi-omics%0Atasks.%20First%2C%20we%20curate%20and%20develop%20a%20diverse%20collection%20of%20downstream%20tasks%0Aand%20datasets%20covering%20key%20structural%20and%20functional%20aspects%20in%20DNA%2C%20RNA%2C%20and%0Aproteins%2C%20including%20tasks%20that%20span%20multiple%20omics%20levels.%20Then%2C%20we%20evaluate%0Aexisting%20foundational%20language%20models%20for%20DNA%2C%20RNA%2C%20and%20proteins%2C%20as%20well%20as%0Athe%20newly%20proposed%20multi-omics%20method%2C%20offering%20valuable%20insights%20into%20their%0Aperformance%20in%20integrating%20and%20analyzing%20data%20from%20different%20biological%0Amodalities.%20This%20benchmark%20aims%20to%20define%20critical%20issues%20in%20multi-omics%0Aresearch%20and%20guide%20future%20directions%2C%20ultimately%20promoting%20advancements%20in%0Aunderstanding%20biological%20processes%20through%20integrated%20and%20different%20omics%20data%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOMET%253A%2520Benchmark%2520for%2520Comprehensive%2520Biological%2520Multi-omics%2520Evaluation%250A%2520%2520Tasks%2520and%2520Language%2520Models%26entry.906535625%3DYuchen%2520Ren%2520and%2520Wenwei%2520Han%2520and%2520Qianyuan%2520Zhang%2520and%2520Yining%2520Tang%2520and%2520Weiqiang%2520Bai%2520and%2520Yuchen%2520Cai%2520and%2520Lifeng%2520Qiao%2520and%2520Hao%2520Jiang%2520and%2520Dong%2520Yuan%2520and%2520Tao%2520Chen%2520and%2520Siqi%2520Sun%2520and%2520Pan%2520Tan%2520and%2520Wanli%2520Ouyang%2520and%2520Nanqing%2520Dong%2520and%2520Xinzhu%2520Ma%2520and%2520Peng%2520Ye%26entry.1292438233%3D%2520%2520As%2520key%2520elements%2520within%2520the%2520central%2520dogma%252C%2520DNA%252C%2520RNA%252C%2520and%2520proteins%2520play%2520crucial%250Aroles%2520in%2520maintaining%2520life%2520by%2520guaranteeing%2520accurate%2520genetic%2520expression%2520and%250Aimplementation.%2520Although%2520research%2520on%2520these%2520molecules%2520has%2520profoundly%2520impacted%250Afields%2520like%2520medicine%252C%2520agriculture%252C%2520and%2520industry%252C%2520the%2520diversity%2520of%2520machine%250Alearning%2520approaches-from%2520traditional%2520statistical%2520methods%2520to%2520deep%2520learning%250Amodels%2520and%2520large%2520language%2520models-poses%2520challenges%2520for%2520researchers%2520in%2520choosing%250Athe%2520most%2520suitable%2520models%2520for%2520specific%2520tasks%252C%2520especially%2520for%2520cross-omics%2520and%250Amulti-omics%2520tasks%2520due%2520to%2520the%2520lack%2520of%2520comprehensive%2520benchmarks.%2520To%2520address%2520this%252C%250Awe%2520introduce%2520the%2520first%2520comprehensive%2520multi-omics%2520benchmark%2520COMET%2520%2528Benchmark%2520for%250ABiological%2520COmprehensive%2520Multi-omics%2520Evaluation%2520Tasks%2520and%2520Language%2520Models%2529%252C%250Adesigned%2520to%2520evaluate%2520models%2520across%2520single-omics%252C%2520cross-omics%252C%2520and%2520multi-omics%250Atasks.%2520First%252C%2520we%2520curate%2520and%2520develop%2520a%2520diverse%2520collection%2520of%2520downstream%2520tasks%250Aand%2520datasets%2520covering%2520key%2520structural%2520and%2520functional%2520aspects%2520in%2520DNA%252C%2520RNA%252C%2520and%250Aproteins%252C%2520including%2520tasks%2520that%2520span%2520multiple%2520omics%2520levels.%2520Then%252C%2520we%2520evaluate%250Aexisting%2520foundational%2520language%2520models%2520for%2520DNA%252C%2520RNA%252C%2520and%2520proteins%252C%2520as%2520well%2520as%250Athe%2520newly%2520proposed%2520multi-omics%2520method%252C%2520offering%2520valuable%2520insights%2520into%2520their%250Aperformance%2520in%2520integrating%2520and%2520analyzing%2520data%2520from%2520different%2520biological%250Amodalities.%2520This%2520benchmark%2520aims%2520to%2520define%2520critical%2520issues%2520in%2520multi-omics%250Aresearch%2520and%2520guide%2520future%2520directions%252C%2520ultimately%2520promoting%2520advancements%2520in%250Aunderstanding%2520biological%2520processes%2520through%2520integrated%2520and%2520different%2520omics%2520data%250Aanalysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COMET%3A%20Benchmark%20for%20Comprehensive%20Biological%20Multi-omics%20Evaluation%0A%20%20Tasks%20and%20Language%20Models&entry.906535625=Yuchen%20Ren%20and%20Wenwei%20Han%20and%20Qianyuan%20Zhang%20and%20Yining%20Tang%20and%20Weiqiang%20Bai%20and%20Yuchen%20Cai%20and%20Lifeng%20Qiao%20and%20Hao%20Jiang%20and%20Dong%20Yuan%20and%20Tao%20Chen%20and%20Siqi%20Sun%20and%20Pan%20Tan%20and%20Wanli%20Ouyang%20and%20Nanqing%20Dong%20and%20Xinzhu%20Ma%20and%20Peng%20Ye&entry.1292438233=%20%20As%20key%20elements%20within%20the%20central%20dogma%2C%20DNA%2C%20RNA%2C%20and%20proteins%20play%20crucial%0Aroles%20in%20maintaining%20life%20by%20guaranteeing%20accurate%20genetic%20expression%20and%0Aimplementation.%20Although%20research%20on%20these%20molecules%20has%20profoundly%20impacted%0Afields%20like%20medicine%2C%20agriculture%2C%20and%20industry%2C%20the%20diversity%20of%20machine%0Alearning%20approaches-from%20traditional%20statistical%20methods%20to%20deep%20learning%0Amodels%20and%20large%20language%20models-poses%20challenges%20for%20researchers%20in%20choosing%0Athe%20most%20suitable%20models%20for%20specific%20tasks%2C%20especially%20for%20cross-omics%20and%0Amulti-omics%20tasks%20due%20to%20the%20lack%20of%20comprehensive%20benchmarks.%20To%20address%20this%2C%0Awe%20introduce%20the%20first%20comprehensive%20multi-omics%20benchmark%20COMET%20%28Benchmark%20for%0ABiological%20COmprehensive%20Multi-omics%20Evaluation%20Tasks%20and%20Language%20Models%29%2C%0Adesigned%20to%20evaluate%20models%20across%20single-omics%2C%20cross-omics%2C%20and%20multi-omics%0Atasks.%20First%2C%20we%20curate%20and%20develop%20a%20diverse%20collection%20of%20downstream%20tasks%0Aand%20datasets%20covering%20key%20structural%20and%20functional%20aspects%20in%20DNA%2C%20RNA%2C%20and%0Aproteins%2C%20including%20tasks%20that%20span%20multiple%20omics%20levels.%20Then%2C%20we%20evaluate%0Aexisting%20foundational%20language%20models%20for%20DNA%2C%20RNA%2C%20and%20proteins%2C%20as%20well%20as%0Athe%20newly%20proposed%20multi-omics%20method%2C%20offering%20valuable%20insights%20into%20their%0Aperformance%20in%20integrating%20and%20analyzing%20data%20from%20different%20biological%0Amodalities.%20This%20benchmark%20aims%20to%20define%20critical%20issues%20in%20multi-omics%0Aresearch%20and%20guide%20future%20directions%2C%20ultimately%20promoting%20advancements%20in%0Aunderstanding%20biological%20processes%20through%20integrated%20and%20different%20omics%20data%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10347v1&entry.124074799=Read"},
{"title": "Multi-Head Encoding for Extreme Label Classification", "author": "Daojun Liang and Haixia Zhang and Dongfeng Yuan and Minggao Zhang", "abstract": "  The number of categories of instances in the real world is normally huge, and\neach instance may contain multiple labels. To distinguish these massive labels\nutilizing machine learning, eXtreme Label Classification (XLC) has been\nestablished. However, as the number of categories increases, the number of\nparameters and nonlinear operations in the classifier also rises. This results\nin a Classifier Computational Overload Problem (CCOP). To address this, we\npropose a Multi-Head Encoding (MHE) mechanism, which replaces the vanilla\nclassifier with a multi-head classifier. During the training process, MHE\ndecomposes extreme labels into the product of multiple short local labels, with\neach head trained on these local labels. During testing, the predicted labels\ncan be directly calculated from the local predictions of each head. This\nreduces the computational load geometrically. Then, according to the\ncharacteristics of different XLC tasks, e.g., single-label, multi-label, and\nmodel pretraining tasks, three MHE-based implementations, i.e., Multi-Head\nProduct, Multi-Head Cascade, and Multi-Head Sampling, are proposed to more\neffectively cope with CCOP. Moreover, we theoretically demonstrate that MHE can\nachieve performance approximately equivalent to that of the vanilla classifier\nby generalizing the low-rank approximation problem from Frobenius-norm to\nCross-Entropy. Experimental results show that the proposed methods achieve\nstate-of-the-art performance while significantly streamlining the training and\ninference processes of XLC tasks. The source code has been made public at\nhttps://github.com/Anoise/MHE.\n", "link": "http://arxiv.org/abs/2412.10182v1", "date": "2024-12-13", "relevancy": 2.0458, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5139}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5112}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Head%20Encoding%20for%20Extreme%20Label%20Classification&body=Title%3A%20Multi-Head%20Encoding%20for%20Extreme%20Label%20Classification%0AAuthor%3A%20Daojun%20Liang%20and%20Haixia%20Zhang%20and%20Dongfeng%20Yuan%20and%20Minggao%20Zhang%0AAbstract%3A%20%20%20The%20number%20of%20categories%20of%20instances%20in%20the%20real%20world%20is%20normally%20huge%2C%20and%0Aeach%20instance%20may%20contain%20multiple%20labels.%20To%20distinguish%20these%20massive%20labels%0Autilizing%20machine%20learning%2C%20eXtreme%20Label%20Classification%20%28XLC%29%20has%20been%0Aestablished.%20However%2C%20as%20the%20number%20of%20categories%20increases%2C%20the%20number%20of%0Aparameters%20and%20nonlinear%20operations%20in%20the%20classifier%20also%20rises.%20This%20results%0Ain%20a%20Classifier%20Computational%20Overload%20Problem%20%28CCOP%29.%20To%20address%20this%2C%20we%0Apropose%20a%20Multi-Head%20Encoding%20%28MHE%29%20mechanism%2C%20which%20replaces%20the%20vanilla%0Aclassifier%20with%20a%20multi-head%20classifier.%20During%20the%20training%20process%2C%20MHE%0Adecomposes%20extreme%20labels%20into%20the%20product%20of%20multiple%20short%20local%20labels%2C%20with%0Aeach%20head%20trained%20on%20these%20local%20labels.%20During%20testing%2C%20the%20predicted%20labels%0Acan%20be%20directly%20calculated%20from%20the%20local%20predictions%20of%20each%20head.%20This%0Areduces%20the%20computational%20load%20geometrically.%20Then%2C%20according%20to%20the%0Acharacteristics%20of%20different%20XLC%20tasks%2C%20e.g.%2C%20single-label%2C%20multi-label%2C%20and%0Amodel%20pretraining%20tasks%2C%20three%20MHE-based%20implementations%2C%20i.e.%2C%20Multi-Head%0AProduct%2C%20Multi-Head%20Cascade%2C%20and%20Multi-Head%20Sampling%2C%20are%20proposed%20to%20more%0Aeffectively%20cope%20with%20CCOP.%20Moreover%2C%20we%20theoretically%20demonstrate%20that%20MHE%20can%0Aachieve%20performance%20approximately%20equivalent%20to%20that%20of%20the%20vanilla%20classifier%0Aby%20generalizing%20the%20low-rank%20approximation%20problem%20from%20Frobenius-norm%20to%0ACross-Entropy.%20Experimental%20results%20show%20that%20the%20proposed%20methods%20achieve%0Astate-of-the-art%20performance%20while%20significantly%20streamlining%20the%20training%20and%0Ainference%20processes%20of%20XLC%20tasks.%20The%20source%20code%20has%20been%20made%20public%20at%0Ahttps%3A//github.com/Anoise/MHE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Head%2520Encoding%2520for%2520Extreme%2520Label%2520Classification%26entry.906535625%3DDaojun%2520Liang%2520and%2520Haixia%2520Zhang%2520and%2520Dongfeng%2520Yuan%2520and%2520Minggao%2520Zhang%26entry.1292438233%3D%2520%2520The%2520number%2520of%2520categories%2520of%2520instances%2520in%2520the%2520real%2520world%2520is%2520normally%2520huge%252C%2520and%250Aeach%2520instance%2520may%2520contain%2520multiple%2520labels.%2520To%2520distinguish%2520these%2520massive%2520labels%250Autilizing%2520machine%2520learning%252C%2520eXtreme%2520Label%2520Classification%2520%2528XLC%2529%2520has%2520been%250Aestablished.%2520However%252C%2520as%2520the%2520number%2520of%2520categories%2520increases%252C%2520the%2520number%2520of%250Aparameters%2520and%2520nonlinear%2520operations%2520in%2520the%2520classifier%2520also%2520rises.%2520This%2520results%250Ain%2520a%2520Classifier%2520Computational%2520Overload%2520Problem%2520%2528CCOP%2529.%2520To%2520address%2520this%252C%2520we%250Apropose%2520a%2520Multi-Head%2520Encoding%2520%2528MHE%2529%2520mechanism%252C%2520which%2520replaces%2520the%2520vanilla%250Aclassifier%2520with%2520a%2520multi-head%2520classifier.%2520During%2520the%2520training%2520process%252C%2520MHE%250Adecomposes%2520extreme%2520labels%2520into%2520the%2520product%2520of%2520multiple%2520short%2520local%2520labels%252C%2520with%250Aeach%2520head%2520trained%2520on%2520these%2520local%2520labels.%2520During%2520testing%252C%2520the%2520predicted%2520labels%250Acan%2520be%2520directly%2520calculated%2520from%2520the%2520local%2520predictions%2520of%2520each%2520head.%2520This%250Areduces%2520the%2520computational%2520load%2520geometrically.%2520Then%252C%2520according%2520to%2520the%250Acharacteristics%2520of%2520different%2520XLC%2520tasks%252C%2520e.g.%252C%2520single-label%252C%2520multi-label%252C%2520and%250Amodel%2520pretraining%2520tasks%252C%2520three%2520MHE-based%2520implementations%252C%2520i.e.%252C%2520Multi-Head%250AProduct%252C%2520Multi-Head%2520Cascade%252C%2520and%2520Multi-Head%2520Sampling%252C%2520are%2520proposed%2520to%2520more%250Aeffectively%2520cope%2520with%2520CCOP.%2520Moreover%252C%2520we%2520theoretically%2520demonstrate%2520that%2520MHE%2520can%250Aachieve%2520performance%2520approximately%2520equivalent%2520to%2520that%2520of%2520the%2520vanilla%2520classifier%250Aby%2520generalizing%2520the%2520low-rank%2520approximation%2520problem%2520from%2520Frobenius-norm%2520to%250ACross-Entropy.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%2520methods%2520achieve%250Astate-of-the-art%2520performance%2520while%2520significantly%2520streamlining%2520the%2520training%2520and%250Ainference%2520processes%2520of%2520XLC%2520tasks.%2520The%2520source%2520code%2520has%2520been%2520made%2520public%2520at%250Ahttps%253A//github.com/Anoise/MHE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Head%20Encoding%20for%20Extreme%20Label%20Classification&entry.906535625=Daojun%20Liang%20and%20Haixia%20Zhang%20and%20Dongfeng%20Yuan%20and%20Minggao%20Zhang&entry.1292438233=%20%20The%20number%20of%20categories%20of%20instances%20in%20the%20real%20world%20is%20normally%20huge%2C%20and%0Aeach%20instance%20may%20contain%20multiple%20labels.%20To%20distinguish%20these%20massive%20labels%0Autilizing%20machine%20learning%2C%20eXtreme%20Label%20Classification%20%28XLC%29%20has%20been%0Aestablished.%20However%2C%20as%20the%20number%20of%20categories%20increases%2C%20the%20number%20of%0Aparameters%20and%20nonlinear%20operations%20in%20the%20classifier%20also%20rises.%20This%20results%0Ain%20a%20Classifier%20Computational%20Overload%20Problem%20%28CCOP%29.%20To%20address%20this%2C%20we%0Apropose%20a%20Multi-Head%20Encoding%20%28MHE%29%20mechanism%2C%20which%20replaces%20the%20vanilla%0Aclassifier%20with%20a%20multi-head%20classifier.%20During%20the%20training%20process%2C%20MHE%0Adecomposes%20extreme%20labels%20into%20the%20product%20of%20multiple%20short%20local%20labels%2C%20with%0Aeach%20head%20trained%20on%20these%20local%20labels.%20During%20testing%2C%20the%20predicted%20labels%0Acan%20be%20directly%20calculated%20from%20the%20local%20predictions%20of%20each%20head.%20This%0Areduces%20the%20computational%20load%20geometrically.%20Then%2C%20according%20to%20the%0Acharacteristics%20of%20different%20XLC%20tasks%2C%20e.g.%2C%20single-label%2C%20multi-label%2C%20and%0Amodel%20pretraining%20tasks%2C%20three%20MHE-based%20implementations%2C%20i.e.%2C%20Multi-Head%0AProduct%2C%20Multi-Head%20Cascade%2C%20and%20Multi-Head%20Sampling%2C%20are%20proposed%20to%20more%0Aeffectively%20cope%20with%20CCOP.%20Moreover%2C%20we%20theoretically%20demonstrate%20that%20MHE%20can%0Aachieve%20performance%20approximately%20equivalent%20to%20that%20of%20the%20vanilla%20classifier%0Aby%20generalizing%20the%20low-rank%20approximation%20problem%20from%20Frobenius-norm%20to%0ACross-Entropy.%20Experimental%20results%20show%20that%20the%20proposed%20methods%20achieve%0Astate-of-the-art%20performance%20while%20significantly%20streamlining%20the%20training%20and%0Ainference%20processes%20of%20XLC%20tasks.%20The%20source%20code%20has%20been%20made%20public%20at%0Ahttps%3A//github.com/Anoise/MHE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10182v1&entry.124074799=Read"},
{"title": "Structural Entropy Guided Probabilistic Coding", "author": "Xiang Huang and Hao Peng and Li Sun and Hui Lin and Chunyang Liu and Jiang Cao and Philip S. Yu", "abstract": "  Probabilistic embeddings have several advantages over deterministic\nembeddings as they map each data point to a distribution, which better\ndescribes the uncertainty and complexity of data. Many works focus on adjusting\nthe distribution constraint under the Information Bottleneck (IB) principle to\nenhance representation learning. However, these proposed regularization terms\nonly consider the constraint of each latent variable, omitting the structural\ninformation between latent variables. In this paper, we propose a novel\nstructural entropy-guided probabilistic coding model, named SEPC. Specifically,\nwe incorporate the relationship between latent variables into the optimization\nby proposing a structural entropy regularization loss. Besides, as traditional\nstructural information theory is not well-suited for regression tasks, we\npropose a probabilistic encoding tree, transferring regression tasks to\nclassification tasks while diminishing the influence of the transformation.\nExperimental results across 12 natural language understanding tasks, including\nboth classification and regression tasks, demonstrate the superior performance\nof SEPC compared to other state-of-the-art models in terms of effectiveness,\ngeneralization capability, and robustness to label noise. The codes and\ndatasets are available at https://github.com/SELGroup/SEPC.\n", "link": "http://arxiv.org/abs/2412.08841v2", "date": "2024-12-13", "relevancy": 2.0433, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5017}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural%20Entropy%20Guided%20Probabilistic%20Coding&body=Title%3A%20Structural%20Entropy%20Guided%20Probabilistic%20Coding%0AAuthor%3A%20Xiang%20Huang%20and%20Hao%20Peng%20and%20Li%20Sun%20and%20Hui%20Lin%20and%20Chunyang%20Liu%20and%20Jiang%20Cao%20and%20Philip%20S.%20Yu%0AAbstract%3A%20%20%20Probabilistic%20embeddings%20have%20several%20advantages%20over%20deterministic%0Aembeddings%20as%20they%20map%20each%20data%20point%20to%20a%20distribution%2C%20which%20better%0Adescribes%20the%20uncertainty%20and%20complexity%20of%20data.%20Many%20works%20focus%20on%20adjusting%0Athe%20distribution%20constraint%20under%20the%20Information%20Bottleneck%20%28IB%29%20principle%20to%0Aenhance%20representation%20learning.%20However%2C%20these%20proposed%20regularization%20terms%0Aonly%20consider%20the%20constraint%20of%20each%20latent%20variable%2C%20omitting%20the%20structural%0Ainformation%20between%20latent%20variables.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Astructural%20entropy-guided%20probabilistic%20coding%20model%2C%20named%20SEPC.%20Specifically%2C%0Awe%20incorporate%20the%20relationship%20between%20latent%20variables%20into%20the%20optimization%0Aby%20proposing%20a%20structural%20entropy%20regularization%20loss.%20Besides%2C%20as%20traditional%0Astructural%20information%20theory%20is%20not%20well-suited%20for%20regression%20tasks%2C%20we%0Apropose%20a%20probabilistic%20encoding%20tree%2C%20transferring%20regression%20tasks%20to%0Aclassification%20tasks%20while%20diminishing%20the%20influence%20of%20the%20transformation.%0AExperimental%20results%20across%2012%20natural%20language%20understanding%20tasks%2C%20including%0Aboth%20classification%20and%20regression%20tasks%2C%20demonstrate%20the%20superior%20performance%0Aof%20SEPC%20compared%20to%20other%20state-of-the-art%20models%20in%20terms%20of%20effectiveness%2C%0Ageneralization%20capability%2C%20and%20robustness%20to%20label%20noise.%20The%20codes%20and%0Adatasets%20are%20available%20at%20https%3A//github.com/SELGroup/SEPC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08841v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural%2520Entropy%2520Guided%2520Probabilistic%2520Coding%26entry.906535625%3DXiang%2520Huang%2520and%2520Hao%2520Peng%2520and%2520Li%2520Sun%2520and%2520Hui%2520Lin%2520and%2520Chunyang%2520Liu%2520and%2520Jiang%2520Cao%2520and%2520Philip%2520S.%2520Yu%26entry.1292438233%3D%2520%2520Probabilistic%2520embeddings%2520have%2520several%2520advantages%2520over%2520deterministic%250Aembeddings%2520as%2520they%2520map%2520each%2520data%2520point%2520to%2520a%2520distribution%252C%2520which%2520better%250Adescribes%2520the%2520uncertainty%2520and%2520complexity%2520of%2520data.%2520Many%2520works%2520focus%2520on%2520adjusting%250Athe%2520distribution%2520constraint%2520under%2520the%2520Information%2520Bottleneck%2520%2528IB%2529%2520principle%2520to%250Aenhance%2520representation%2520learning.%2520However%252C%2520these%2520proposed%2520regularization%2520terms%250Aonly%2520consider%2520the%2520constraint%2520of%2520each%2520latent%2520variable%252C%2520omitting%2520the%2520structural%250Ainformation%2520between%2520latent%2520variables.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Astructural%2520entropy-guided%2520probabilistic%2520coding%2520model%252C%2520named%2520SEPC.%2520Specifically%252C%250Awe%2520incorporate%2520the%2520relationship%2520between%2520latent%2520variables%2520into%2520the%2520optimization%250Aby%2520proposing%2520a%2520structural%2520entropy%2520regularization%2520loss.%2520Besides%252C%2520as%2520traditional%250Astructural%2520information%2520theory%2520is%2520not%2520well-suited%2520for%2520regression%2520tasks%252C%2520we%250Apropose%2520a%2520probabilistic%2520encoding%2520tree%252C%2520transferring%2520regression%2520tasks%2520to%250Aclassification%2520tasks%2520while%2520diminishing%2520the%2520influence%2520of%2520the%2520transformation.%250AExperimental%2520results%2520across%252012%2520natural%2520language%2520understanding%2520tasks%252C%2520including%250Aboth%2520classification%2520and%2520regression%2520tasks%252C%2520demonstrate%2520the%2520superior%2520performance%250Aof%2520SEPC%2520compared%2520to%2520other%2520state-of-the-art%2520models%2520in%2520terms%2520of%2520effectiveness%252C%250Ageneralization%2520capability%252C%2520and%2520robustness%2520to%2520label%2520noise.%2520The%2520codes%2520and%250Adatasets%2520are%2520available%2520at%2520https%253A//github.com/SELGroup/SEPC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08841v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural%20Entropy%20Guided%20Probabilistic%20Coding&entry.906535625=Xiang%20Huang%20and%20Hao%20Peng%20and%20Li%20Sun%20and%20Hui%20Lin%20and%20Chunyang%20Liu%20and%20Jiang%20Cao%20and%20Philip%20S.%20Yu&entry.1292438233=%20%20Probabilistic%20embeddings%20have%20several%20advantages%20over%20deterministic%0Aembeddings%20as%20they%20map%20each%20data%20point%20to%20a%20distribution%2C%20which%20better%0Adescribes%20the%20uncertainty%20and%20complexity%20of%20data.%20Many%20works%20focus%20on%20adjusting%0Athe%20distribution%20constraint%20under%20the%20Information%20Bottleneck%20%28IB%29%20principle%20to%0Aenhance%20representation%20learning.%20However%2C%20these%20proposed%20regularization%20terms%0Aonly%20consider%20the%20constraint%20of%20each%20latent%20variable%2C%20omitting%20the%20structural%0Ainformation%20between%20latent%20variables.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Astructural%20entropy-guided%20probabilistic%20coding%20model%2C%20named%20SEPC.%20Specifically%2C%0Awe%20incorporate%20the%20relationship%20between%20latent%20variables%20into%20the%20optimization%0Aby%20proposing%20a%20structural%20entropy%20regularization%20loss.%20Besides%2C%20as%20traditional%0Astructural%20information%20theory%20is%20not%20well-suited%20for%20regression%20tasks%2C%20we%0Apropose%20a%20probabilistic%20encoding%20tree%2C%20transferring%20regression%20tasks%20to%0Aclassification%20tasks%20while%20diminishing%20the%20influence%20of%20the%20transformation.%0AExperimental%20results%20across%2012%20natural%20language%20understanding%20tasks%2C%20including%0Aboth%20classification%20and%20regression%20tasks%2C%20demonstrate%20the%20superior%20performance%0Aof%20SEPC%20compared%20to%20other%20state-of-the-art%20models%20in%20terms%20of%20effectiveness%2C%0Ageneralization%20capability%2C%20and%20robustness%20to%20label%20noise.%20The%20codes%20and%0Adatasets%20are%20available%20at%20https%3A//github.com/SELGroup/SEPC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08841v2&entry.124074799=Read"},
{"title": "Can LLMs Convert Graphs to Text-Attributed Graphs?", "author": "Zehong Wang and Sidney Liu and Zheyuan Zhang and Tianyi Ma and Chuxu Zhang and Yanfang Ye", "abstract": "  Graphs are ubiquitous data structures found in numerous real-world\napplications, such as drug discovery, recommender systems, and social network\nanalysis. Graph neural networks (GNNs) have become a popular tool to learn node\nembeddings through message passing on these structures. However, a significant\nchallenge arises when applying GNNs to multiple graphs with different feature\nspaces, as existing GNN architectures are not designed for cross-graph feature\nalignment. To address this, recent approaches introduce text-attributed graphs,\nwhere each node is associated with a textual description, enabling the use of a\nshared textual encoder to project nodes from different graphs into a unified\nfeature space. While promising, this method relies heavily on the availability\nof text-attributed data, which can be difficult to obtain in practice. To\nbridge this gap, we propose a novel method named Topology-Aware Node\ndescription Synthesis (TANS), which leverages large language models (LLMs) to\nautomatically convert existing graphs into text-attributed graphs. The key idea\nis to integrate topological information with each node's properties, enhancing\nthe LLMs' ability to explain how graph topology influences node semantics. We\nevaluate our TANS on text-rich, text-limited, and text-free graphs,\ndemonstrating that it enables a single GNN to operate across diverse graphs.\nNotably, on text-free graphs, our method significantly outperforms existing\napproaches that manually design node features, showcasing the potential of LLMs\nfor preprocessing graph-structured data, even in the absence of textual\ninformation. The code and data are available at\nhttps://github.com/Zehong-Wang/TANS.\n", "link": "http://arxiv.org/abs/2412.10136v1", "date": "2024-12-13", "relevancy": 2.0377, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5152}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.513}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20Convert%20Graphs%20to%20Text-Attributed%20Graphs%3F&body=Title%3A%20Can%20LLMs%20Convert%20Graphs%20to%20Text-Attributed%20Graphs%3F%0AAuthor%3A%20Zehong%20Wang%20and%20Sidney%20Liu%20and%20Zheyuan%20Zhang%20and%20Tianyi%20Ma%20and%20Chuxu%20Zhang%20and%20Yanfang%20Ye%0AAbstract%3A%20%20%20Graphs%20are%20ubiquitous%20data%20structures%20found%20in%20numerous%20real-world%0Aapplications%2C%20such%20as%20drug%20discovery%2C%20recommender%20systems%2C%20and%20social%20network%0Aanalysis.%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20a%20popular%20tool%20to%20learn%20node%0Aembeddings%20through%20message%20passing%20on%20these%20structures.%20However%2C%20a%20significant%0Achallenge%20arises%20when%20applying%20GNNs%20to%20multiple%20graphs%20with%20different%20feature%0Aspaces%2C%20as%20existing%20GNN%20architectures%20are%20not%20designed%20for%20cross-graph%20feature%0Aalignment.%20To%20address%20this%2C%20recent%20approaches%20introduce%20text-attributed%20graphs%2C%0Awhere%20each%20node%20is%20associated%20with%20a%20textual%20description%2C%20enabling%20the%20use%20of%20a%0Ashared%20textual%20encoder%20to%20project%20nodes%20from%20different%20graphs%20into%20a%20unified%0Afeature%20space.%20While%20promising%2C%20this%20method%20relies%20heavily%20on%20the%20availability%0Aof%20text-attributed%20data%2C%20which%20can%20be%20difficult%20to%20obtain%20in%20practice.%20To%0Abridge%20this%20gap%2C%20we%20propose%20a%20novel%20method%20named%20Topology-Aware%20Node%0Adescription%20Synthesis%20%28TANS%29%2C%20which%20leverages%20large%20language%20models%20%28LLMs%29%20to%0Aautomatically%20convert%20existing%20graphs%20into%20text-attributed%20graphs.%20The%20key%20idea%0Ais%20to%20integrate%20topological%20information%20with%20each%20node%27s%20properties%2C%20enhancing%0Athe%20LLMs%27%20ability%20to%20explain%20how%20graph%20topology%20influences%20node%20semantics.%20We%0Aevaluate%20our%20TANS%20on%20text-rich%2C%20text-limited%2C%20and%20text-free%20graphs%2C%0Ademonstrating%20that%20it%20enables%20a%20single%20GNN%20to%20operate%20across%20diverse%20graphs.%0ANotably%2C%20on%20text-free%20graphs%2C%20our%20method%20significantly%20outperforms%20existing%0Aapproaches%20that%20manually%20design%20node%20features%2C%20showcasing%20the%20potential%20of%20LLMs%0Afor%20preprocessing%20graph-structured%20data%2C%20even%20in%20the%20absence%20of%20textual%0Ainformation.%20The%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/Zehong-Wang/TANS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520Convert%2520Graphs%2520to%2520Text-Attributed%2520Graphs%253F%26entry.906535625%3DZehong%2520Wang%2520and%2520Sidney%2520Liu%2520and%2520Zheyuan%2520Zhang%2520and%2520Tianyi%2520Ma%2520and%2520Chuxu%2520Zhang%2520and%2520Yanfang%2520Ye%26entry.1292438233%3D%2520%2520Graphs%2520are%2520ubiquitous%2520data%2520structures%2520found%2520in%2520numerous%2520real-world%250Aapplications%252C%2520such%2520as%2520drug%2520discovery%252C%2520recommender%2520systems%252C%2520and%2520social%2520network%250Aanalysis.%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520become%2520a%2520popular%2520tool%2520to%2520learn%2520node%250Aembeddings%2520through%2520message%2520passing%2520on%2520these%2520structures.%2520However%252C%2520a%2520significant%250Achallenge%2520arises%2520when%2520applying%2520GNNs%2520to%2520multiple%2520graphs%2520with%2520different%2520feature%250Aspaces%252C%2520as%2520existing%2520GNN%2520architectures%2520are%2520not%2520designed%2520for%2520cross-graph%2520feature%250Aalignment.%2520To%2520address%2520this%252C%2520recent%2520approaches%2520introduce%2520text-attributed%2520graphs%252C%250Awhere%2520each%2520node%2520is%2520associated%2520with%2520a%2520textual%2520description%252C%2520enabling%2520the%2520use%2520of%2520a%250Ashared%2520textual%2520encoder%2520to%2520project%2520nodes%2520from%2520different%2520graphs%2520into%2520a%2520unified%250Afeature%2520space.%2520While%2520promising%252C%2520this%2520method%2520relies%2520heavily%2520on%2520the%2520availability%250Aof%2520text-attributed%2520data%252C%2520which%2520can%2520be%2520difficult%2520to%2520obtain%2520in%2520practice.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520novel%2520method%2520named%2520Topology-Aware%2520Node%250Adescription%2520Synthesis%2520%2528TANS%2529%252C%2520which%2520leverages%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%250Aautomatically%2520convert%2520existing%2520graphs%2520into%2520text-attributed%2520graphs.%2520The%2520key%2520idea%250Ais%2520to%2520integrate%2520topological%2520information%2520with%2520each%2520node%2527s%2520properties%252C%2520enhancing%250Athe%2520LLMs%2527%2520ability%2520to%2520explain%2520how%2520graph%2520topology%2520influences%2520node%2520semantics.%2520We%250Aevaluate%2520our%2520TANS%2520on%2520text-rich%252C%2520text-limited%252C%2520and%2520text-free%2520graphs%252C%250Ademonstrating%2520that%2520it%2520enables%2520a%2520single%2520GNN%2520to%2520operate%2520across%2520diverse%2520graphs.%250ANotably%252C%2520on%2520text-free%2520graphs%252C%2520our%2520method%2520significantly%2520outperforms%2520existing%250Aapproaches%2520that%2520manually%2520design%2520node%2520features%252C%2520showcasing%2520the%2520potential%2520of%2520LLMs%250Afor%2520preprocessing%2520graph-structured%2520data%252C%2520even%2520in%2520the%2520absence%2520of%2520textual%250Ainformation.%2520The%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/Zehong-Wang/TANS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20Convert%20Graphs%20to%20Text-Attributed%20Graphs%3F&entry.906535625=Zehong%20Wang%20and%20Sidney%20Liu%20and%20Zheyuan%20Zhang%20and%20Tianyi%20Ma%20and%20Chuxu%20Zhang%20and%20Yanfang%20Ye&entry.1292438233=%20%20Graphs%20are%20ubiquitous%20data%20structures%20found%20in%20numerous%20real-world%0Aapplications%2C%20such%20as%20drug%20discovery%2C%20recommender%20systems%2C%20and%20social%20network%0Aanalysis.%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20a%20popular%20tool%20to%20learn%20node%0Aembeddings%20through%20message%20passing%20on%20these%20structures.%20However%2C%20a%20significant%0Achallenge%20arises%20when%20applying%20GNNs%20to%20multiple%20graphs%20with%20different%20feature%0Aspaces%2C%20as%20existing%20GNN%20architectures%20are%20not%20designed%20for%20cross-graph%20feature%0Aalignment.%20To%20address%20this%2C%20recent%20approaches%20introduce%20text-attributed%20graphs%2C%0Awhere%20each%20node%20is%20associated%20with%20a%20textual%20description%2C%20enabling%20the%20use%20of%20a%0Ashared%20textual%20encoder%20to%20project%20nodes%20from%20different%20graphs%20into%20a%20unified%0Afeature%20space.%20While%20promising%2C%20this%20method%20relies%20heavily%20on%20the%20availability%0Aof%20text-attributed%20data%2C%20which%20can%20be%20difficult%20to%20obtain%20in%20practice.%20To%0Abridge%20this%20gap%2C%20we%20propose%20a%20novel%20method%20named%20Topology-Aware%20Node%0Adescription%20Synthesis%20%28TANS%29%2C%20which%20leverages%20large%20language%20models%20%28LLMs%29%20to%0Aautomatically%20convert%20existing%20graphs%20into%20text-attributed%20graphs.%20The%20key%20idea%0Ais%20to%20integrate%20topological%20information%20with%20each%20node%27s%20properties%2C%20enhancing%0Athe%20LLMs%27%20ability%20to%20explain%20how%20graph%20topology%20influences%20node%20semantics.%20We%0Aevaluate%20our%20TANS%20on%20text-rich%2C%20text-limited%2C%20and%20text-free%20graphs%2C%0Ademonstrating%20that%20it%20enables%20a%20single%20GNN%20to%20operate%20across%20diverse%20graphs.%0ANotably%2C%20on%20text-free%20graphs%2C%20our%20method%20significantly%20outperforms%20existing%0Aapproaches%20that%20manually%20design%20node%20features%2C%20showcasing%20the%20potential%20of%20LLMs%0Afor%20preprocessing%20graph-structured%20data%2C%20even%20in%20the%20absence%20of%20textual%0Ainformation.%20The%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/Zehong-Wang/TANS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10136v1&entry.124074799=Read"},
{"title": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving", "author": "Yuhan Liu and Yuyang Huang and Jiayi Yao and Zhuohan Gu and Kuntai Du and Hanchen Li and Yihua Cheng and Junchen Jiang and Shan Lu and Madan Musuvathi and Esha Choukse", "abstract": "  Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.\n", "link": "http://arxiv.org/abs/2411.02820v2", "date": "2024-12-13", "relevancy": 2.0281, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DroidSpeak%3A%20KV%20Cache%20Sharing%20for%20Efficient%20Multi-LLM%20Serving&body=Title%3A%20DroidSpeak%3A%20KV%20Cache%20Sharing%20for%20Efficient%20Multi-LLM%20Serving%0AAuthor%3A%20Yuhan%20Liu%20and%20Yuyang%20Huang%20and%20Jiayi%20Yao%20and%20Zhuohan%20Gu%20and%20Kuntai%20Du%20and%20Hanchen%20Li%20and%20Yihua%20Cheng%20and%20Junchen%20Jiang%20and%20Shan%20Lu%20and%20Madan%20Musuvathi%20and%20Esha%20Choukse%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20employed%20in%20complex%20workflows%2C%0Awhere%20different%20LLMs%20and%20fine-tuned%20variants%20collaboratively%20address%20complex%0Atasks.%20However%2C%20these%20systems%20face%20significant%20inefficiencies%20due%20to%20redundant%0Acontext%20processing%20of%20the%20shared%20context.%20We%20propose%20DroidSpeak%2C%20a%20framework%0Athat%20optimizes%20context%20sharing%20between%20fine-tuned%20LLMs%20derived%20from%20the%20same%0Afoundational%20model.%20DroidSpeak%20identifies%20critical%20layers%20in%20the%20KV%20cache%20and%0Aselectively%20recomputes%20them%2C%20enabling%20effective%20reuse%20of%20intermediate%20data%0Awhile%20maintaining%20high%20accuracy.%0A%20%20Our%20approach%20balances%20computational%20efficiency%20and%20task%20fidelity%2C%0Asignificantly%20reducing%20inference%20latency%20and%20throughput%20bottlenecks.%0AExperiments%20on%20diverse%20datasets%20and%20model%20pairs%20demonstrate%20that%20DroidSpeak%0Aachieves%20up%20to%203x%20higher%20throughputs%20and%202.6x%20faster%20prefill%20times%20with%0Anegligible%20accuracy%20loss%20compared%20to%20full%20recomputation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02820v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDroidSpeak%253A%2520KV%2520Cache%2520Sharing%2520for%2520Efficient%2520Multi-LLM%2520Serving%26entry.906535625%3DYuhan%2520Liu%2520and%2520Yuyang%2520Huang%2520and%2520Jiayi%2520Yao%2520and%2520Zhuohan%2520Gu%2520and%2520Kuntai%2520Du%2520and%2520Hanchen%2520Li%2520and%2520Yihua%2520Cheng%2520and%2520Junchen%2520Jiang%2520and%2520Shan%2520Lu%2520and%2520Madan%2520Musuvathi%2520and%2520Esha%2520Choukse%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520employed%2520in%2520complex%2520workflows%252C%250Awhere%2520different%2520LLMs%2520and%2520fine-tuned%2520variants%2520collaboratively%2520address%2520complex%250Atasks.%2520However%252C%2520these%2520systems%2520face%2520significant%2520inefficiencies%2520due%2520to%2520redundant%250Acontext%2520processing%2520of%2520the%2520shared%2520context.%2520We%2520propose%2520DroidSpeak%252C%2520a%2520framework%250Athat%2520optimizes%2520context%2520sharing%2520between%2520fine-tuned%2520LLMs%2520derived%2520from%2520the%2520same%250Afoundational%2520model.%2520DroidSpeak%2520identifies%2520critical%2520layers%2520in%2520the%2520KV%2520cache%2520and%250Aselectively%2520recomputes%2520them%252C%2520enabling%2520effective%2520reuse%2520of%2520intermediate%2520data%250Awhile%2520maintaining%2520high%2520accuracy.%250A%2520%2520Our%2520approach%2520balances%2520computational%2520efficiency%2520and%2520task%2520fidelity%252C%250Asignificantly%2520reducing%2520inference%2520latency%2520and%2520throughput%2520bottlenecks.%250AExperiments%2520on%2520diverse%2520datasets%2520and%2520model%2520pairs%2520demonstrate%2520that%2520DroidSpeak%250Aachieves%2520up%2520to%25203x%2520higher%2520throughputs%2520and%25202.6x%2520faster%2520prefill%2520times%2520with%250Anegligible%2520accuracy%2520loss%2520compared%2520to%2520full%2520recomputation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02820v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DroidSpeak%3A%20KV%20Cache%20Sharing%20for%20Efficient%20Multi-LLM%20Serving&entry.906535625=Yuhan%20Liu%20and%20Yuyang%20Huang%20and%20Jiayi%20Yao%20and%20Zhuohan%20Gu%20and%20Kuntai%20Du%20and%20Hanchen%20Li%20and%20Yihua%20Cheng%20and%20Junchen%20Jiang%20and%20Shan%20Lu%20and%20Madan%20Musuvathi%20and%20Esha%20Choukse&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20employed%20in%20complex%20workflows%2C%0Awhere%20different%20LLMs%20and%20fine-tuned%20variants%20collaboratively%20address%20complex%0Atasks.%20However%2C%20these%20systems%20face%20significant%20inefficiencies%20due%20to%20redundant%0Acontext%20processing%20of%20the%20shared%20context.%20We%20propose%20DroidSpeak%2C%20a%20framework%0Athat%20optimizes%20context%20sharing%20between%20fine-tuned%20LLMs%20derived%20from%20the%20same%0Afoundational%20model.%20DroidSpeak%20identifies%20critical%20layers%20in%20the%20KV%20cache%20and%0Aselectively%20recomputes%20them%2C%20enabling%20effective%20reuse%20of%20intermediate%20data%0Awhile%20maintaining%20high%20accuracy.%0A%20%20Our%20approach%20balances%20computational%20efficiency%20and%20task%20fidelity%2C%0Asignificantly%20reducing%20inference%20latency%20and%20throughput%20bottlenecks.%0AExperiments%20on%20diverse%20datasets%20and%20model%20pairs%20demonstrate%20that%20DroidSpeak%0Aachieves%20up%20to%203x%20higher%20throughputs%20and%202.6x%20faster%20prefill%20times%20with%0Anegligible%20accuracy%20loss%20compared%20to%20full%20recomputation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02820v2&entry.124074799=Read"},
{"title": "Inverse Reinforcement Learning by Estimating Expertise of Demonstrators", "author": "Mark Beliaev and Ramtin Pedarsani", "abstract": "  In Imitation Learning (IL), utilizing suboptimal and heterogeneous\ndemonstrations presents a substantial challenge due to the varied nature of\nreal-world data. However, standard IL algorithms consider these datasets as\nhomogeneous, thereby inheriting the deficiencies of suboptimal demonstrators.\nPrevious approaches to this issue rely on impractical assumptions like\nhigh-quality data subsets, confidence rankings, or explicit environmental\nknowledge. This paper introduces IRLEED, Inverse Reinforcement Learning by\nEstimating Expertise of Demonstrators, a novel framework that overcomes these\nhurdles without prior knowledge of demonstrator expertise. IRLEED enhances\nexisting Inverse Reinforcement Learning (IRL) algorithms by combining a general\nmodel for demonstrator suboptimality to address reward bias and action\nvariance, with a Maximum Entropy IRL framework to efficiently derive the\noptimal policy from diverse, suboptimal demonstrations. Experiments in both\nonline and offline IL settings, with simulated and human-generated data,\ndemonstrate IRLEED's adaptability and effectiveness, making it a versatile\nsolution for learning from suboptimal demonstrations.\n", "link": "http://arxiv.org/abs/2402.01886v2", "date": "2024-12-13", "relevancy": 2.0153, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5206}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5044}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20Reinforcement%20Learning%20by%20Estimating%20Expertise%20of%20Demonstrators&body=Title%3A%20Inverse%20Reinforcement%20Learning%20by%20Estimating%20Expertise%20of%20Demonstrators%0AAuthor%3A%20Mark%20Beliaev%20and%20Ramtin%20Pedarsani%0AAbstract%3A%20%20%20In%20Imitation%20Learning%20%28IL%29%2C%20utilizing%20suboptimal%20and%20heterogeneous%0Ademonstrations%20presents%20a%20substantial%20challenge%20due%20to%20the%20varied%20nature%20of%0Areal-world%20data.%20However%2C%20standard%20IL%20algorithms%20consider%20these%20datasets%20as%0Ahomogeneous%2C%20thereby%20inheriting%20the%20deficiencies%20of%20suboptimal%20demonstrators.%0APrevious%20approaches%20to%20this%20issue%20rely%20on%20impractical%20assumptions%20like%0Ahigh-quality%20data%20subsets%2C%20confidence%20rankings%2C%20or%20explicit%20environmental%0Aknowledge.%20This%20paper%20introduces%20IRLEED%2C%20Inverse%20Reinforcement%20Learning%20by%0AEstimating%20Expertise%20of%20Demonstrators%2C%20a%20novel%20framework%20that%20overcomes%20these%0Ahurdles%20without%20prior%20knowledge%20of%20demonstrator%20expertise.%20IRLEED%20enhances%0Aexisting%20Inverse%20Reinforcement%20Learning%20%28IRL%29%20algorithms%20by%20combining%20a%20general%0Amodel%20for%20demonstrator%20suboptimality%20to%20address%20reward%20bias%20and%20action%0Avariance%2C%20with%20a%20Maximum%20Entropy%20IRL%20framework%20to%20efficiently%20derive%20the%0Aoptimal%20policy%20from%20diverse%2C%20suboptimal%20demonstrations.%20Experiments%20in%20both%0Aonline%20and%20offline%20IL%20settings%2C%20with%20simulated%20and%20human-generated%20data%2C%0Ademonstrate%20IRLEED%27s%20adaptability%20and%20effectiveness%2C%20making%20it%20a%20versatile%0Asolution%20for%20learning%20from%20suboptimal%20demonstrations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01886v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520Reinforcement%2520Learning%2520by%2520Estimating%2520Expertise%2520of%2520Demonstrators%26entry.906535625%3DMark%2520Beliaev%2520and%2520Ramtin%2520Pedarsani%26entry.1292438233%3D%2520%2520In%2520Imitation%2520Learning%2520%2528IL%2529%252C%2520utilizing%2520suboptimal%2520and%2520heterogeneous%250Ademonstrations%2520presents%2520a%2520substantial%2520challenge%2520due%2520to%2520the%2520varied%2520nature%2520of%250Areal-world%2520data.%2520However%252C%2520standard%2520IL%2520algorithms%2520consider%2520these%2520datasets%2520as%250Ahomogeneous%252C%2520thereby%2520inheriting%2520the%2520deficiencies%2520of%2520suboptimal%2520demonstrators.%250APrevious%2520approaches%2520to%2520this%2520issue%2520rely%2520on%2520impractical%2520assumptions%2520like%250Ahigh-quality%2520data%2520subsets%252C%2520confidence%2520rankings%252C%2520or%2520explicit%2520environmental%250Aknowledge.%2520This%2520paper%2520introduces%2520IRLEED%252C%2520Inverse%2520Reinforcement%2520Learning%2520by%250AEstimating%2520Expertise%2520of%2520Demonstrators%252C%2520a%2520novel%2520framework%2520that%2520overcomes%2520these%250Ahurdles%2520without%2520prior%2520knowledge%2520of%2520demonstrator%2520expertise.%2520IRLEED%2520enhances%250Aexisting%2520Inverse%2520Reinforcement%2520Learning%2520%2528IRL%2529%2520algorithms%2520by%2520combining%2520a%2520general%250Amodel%2520for%2520demonstrator%2520suboptimality%2520to%2520address%2520reward%2520bias%2520and%2520action%250Avariance%252C%2520with%2520a%2520Maximum%2520Entropy%2520IRL%2520framework%2520to%2520efficiently%2520derive%2520the%250Aoptimal%2520policy%2520from%2520diverse%252C%2520suboptimal%2520demonstrations.%2520Experiments%2520in%2520both%250Aonline%2520and%2520offline%2520IL%2520settings%252C%2520with%2520simulated%2520and%2520human-generated%2520data%252C%250Ademonstrate%2520IRLEED%2527s%2520adaptability%2520and%2520effectiveness%252C%2520making%2520it%2520a%2520versatile%250Asolution%2520for%2520learning%2520from%2520suboptimal%2520demonstrations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01886v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Reinforcement%20Learning%20by%20Estimating%20Expertise%20of%20Demonstrators&entry.906535625=Mark%20Beliaev%20and%20Ramtin%20Pedarsani&entry.1292438233=%20%20In%20Imitation%20Learning%20%28IL%29%2C%20utilizing%20suboptimal%20and%20heterogeneous%0Ademonstrations%20presents%20a%20substantial%20challenge%20due%20to%20the%20varied%20nature%20of%0Areal-world%20data.%20However%2C%20standard%20IL%20algorithms%20consider%20these%20datasets%20as%0Ahomogeneous%2C%20thereby%20inheriting%20the%20deficiencies%20of%20suboptimal%20demonstrators.%0APrevious%20approaches%20to%20this%20issue%20rely%20on%20impractical%20assumptions%20like%0Ahigh-quality%20data%20subsets%2C%20confidence%20rankings%2C%20or%20explicit%20environmental%0Aknowledge.%20This%20paper%20introduces%20IRLEED%2C%20Inverse%20Reinforcement%20Learning%20by%0AEstimating%20Expertise%20of%20Demonstrators%2C%20a%20novel%20framework%20that%20overcomes%20these%0Ahurdles%20without%20prior%20knowledge%20of%20demonstrator%20expertise.%20IRLEED%20enhances%0Aexisting%20Inverse%20Reinforcement%20Learning%20%28IRL%29%20algorithms%20by%20combining%20a%20general%0Amodel%20for%20demonstrator%20suboptimality%20to%20address%20reward%20bias%20and%20action%0Avariance%2C%20with%20a%20Maximum%20Entropy%20IRL%20framework%20to%20efficiently%20derive%20the%0Aoptimal%20policy%20from%20diverse%2C%20suboptimal%20demonstrations.%20Experiments%20in%20both%0Aonline%20and%20offline%20IL%20settings%2C%20with%20simulated%20and%20human-generated%20data%2C%0Ademonstrate%20IRLEED%27s%20adaptability%20and%20effectiveness%2C%20making%20it%20a%20versatile%0Asolution%20for%20learning%20from%20suboptimal%20demonstrations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01886v2&entry.124074799=Read"},
{"title": "Panacea: Novel DNN Accelerator using Accuracy-Preserving Asymmetric\n  Quantization and Energy-Saving Bit-Slice Sparsity", "author": "Dongyun Kam and Myeongji Yun and Sunwoo Yoo and Seungwoo Hong and Zhengya Zhang and Youngjoo Lee", "abstract": "  Low bit-precisions and their bit-slice sparsity have recently been studied to\naccelerate general matrix-multiplications (GEMM) during large-scale deep neural\nnetwork (DNN) inferences. While the conventional symmetric quantization\nfacilitates low-resolution processing with bit-slice sparsity for both weight\nand activation, its accuracy loss caused by the activation's asymmetric\ndistributions cannot be acceptable, especially for large-scale DNNs. In efforts\nto mitigate this accuracy loss, recent studies have actively utilized\nasymmetric quantization for activations without requiring additional\noperations. However, the cutting-edge asymmetric quantization produces numerous\nnonzero slices that cannot be compressed and skipped by recent bit-slice GEMM\naccelerators, naturally consuming more processing energy to handle the\nquantized DNN models.\n  To simultaneously achieve high accuracy and hardware efficiency for\nlarge-scale DNN inferences, this paper proposes an Asymmetrically-Quantized\nbit-Slice GEMM (AQS-GEMM) for the first time. In contrast to the previous\nbit-slice computing, which only skips operations of zero slices, the AQS-GEMM\ncompresses frequent nonzero slices, generated by asymmetric quantization, and\nskips their operations. To increase the slice-level sparsity of activations, we\nalso introduce two algorithm-hardware co-optimization methods: a zero-point\nmanipulation and a distribution-based bit-slicing. To support the proposed\nAQS-GEMM and optimizations at the hardware-level, we newly introduce a DNN\naccelerator, Panacea, which efficiently handles sparse/dense workloads of the\ntiled AQS-GEMM to increase data reuse and utilization. Panacea supports a\nspecialized dataflow and run-length encoding to maximize data reuse and\nminimize external memory accesses, significantly improving its hardware\nefficiency. Our benchmark evaluations show Panacea outperforms existing DNN\naccelerators.\n", "link": "http://arxiv.org/abs/2412.10059v1", "date": "2024-12-13", "relevancy": 2.0143, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5332}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5088}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Panacea%3A%20Novel%20DNN%20Accelerator%20using%20Accuracy-Preserving%20Asymmetric%0A%20%20Quantization%20and%20Energy-Saving%20Bit-Slice%20Sparsity&body=Title%3A%20Panacea%3A%20Novel%20DNN%20Accelerator%20using%20Accuracy-Preserving%20Asymmetric%0A%20%20Quantization%20and%20Energy-Saving%20Bit-Slice%20Sparsity%0AAuthor%3A%20Dongyun%20Kam%20and%20Myeongji%20Yun%20and%20Sunwoo%20Yoo%20and%20Seungwoo%20Hong%20and%20Zhengya%20Zhang%20and%20Youngjoo%20Lee%0AAbstract%3A%20%20%20Low%20bit-precisions%20and%20their%20bit-slice%20sparsity%20have%20recently%20been%20studied%20to%0Aaccelerate%20general%20matrix-multiplications%20%28GEMM%29%20during%20large-scale%20deep%20neural%0Anetwork%20%28DNN%29%20inferences.%20While%20the%20conventional%20symmetric%20quantization%0Afacilitates%20low-resolution%20processing%20with%20bit-slice%20sparsity%20for%20both%20weight%0Aand%20activation%2C%20its%20accuracy%20loss%20caused%20by%20the%20activation%27s%20asymmetric%0Adistributions%20cannot%20be%20acceptable%2C%20especially%20for%20large-scale%20DNNs.%20In%20efforts%0Ato%20mitigate%20this%20accuracy%20loss%2C%20recent%20studies%20have%20actively%20utilized%0Aasymmetric%20quantization%20for%20activations%20without%20requiring%20additional%0Aoperations.%20However%2C%20the%20cutting-edge%20asymmetric%20quantization%20produces%20numerous%0Anonzero%20slices%20that%20cannot%20be%20compressed%20and%20skipped%20by%20recent%20bit-slice%20GEMM%0Aaccelerators%2C%20naturally%20consuming%20more%20processing%20energy%20to%20handle%20the%0Aquantized%20DNN%20models.%0A%20%20To%20simultaneously%20achieve%20high%20accuracy%20and%20hardware%20efficiency%20for%0Alarge-scale%20DNN%20inferences%2C%20this%20paper%20proposes%20an%20Asymmetrically-Quantized%0Abit-Slice%20GEMM%20%28AQS-GEMM%29%20for%20the%20first%20time.%20In%20contrast%20to%20the%20previous%0Abit-slice%20computing%2C%20which%20only%20skips%20operations%20of%20zero%20slices%2C%20the%20AQS-GEMM%0Acompresses%20frequent%20nonzero%20slices%2C%20generated%20by%20asymmetric%20quantization%2C%20and%0Askips%20their%20operations.%20To%20increase%20the%20slice-level%20sparsity%20of%20activations%2C%20we%0Aalso%20introduce%20two%20algorithm-hardware%20co-optimization%20methods%3A%20a%20zero-point%0Amanipulation%20and%20a%20distribution-based%20bit-slicing.%20To%20support%20the%20proposed%0AAQS-GEMM%20and%20optimizations%20at%20the%20hardware-level%2C%20we%20newly%20introduce%20a%20DNN%0Aaccelerator%2C%20Panacea%2C%20which%20efficiently%20handles%20sparse/dense%20workloads%20of%20the%0Atiled%20AQS-GEMM%20to%20increase%20data%20reuse%20and%20utilization.%20Panacea%20supports%20a%0Aspecialized%20dataflow%20and%20run-length%20encoding%20to%20maximize%20data%20reuse%20and%0Aminimize%20external%20memory%20accesses%2C%20significantly%20improving%20its%20hardware%0Aefficiency.%20Our%20benchmark%20evaluations%20show%20Panacea%20outperforms%20existing%20DNN%0Aaccelerators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanacea%253A%2520Novel%2520DNN%2520Accelerator%2520using%2520Accuracy-Preserving%2520Asymmetric%250A%2520%2520Quantization%2520and%2520Energy-Saving%2520Bit-Slice%2520Sparsity%26entry.906535625%3DDongyun%2520Kam%2520and%2520Myeongji%2520Yun%2520and%2520Sunwoo%2520Yoo%2520and%2520Seungwoo%2520Hong%2520and%2520Zhengya%2520Zhang%2520and%2520Youngjoo%2520Lee%26entry.1292438233%3D%2520%2520Low%2520bit-precisions%2520and%2520their%2520bit-slice%2520sparsity%2520have%2520recently%2520been%2520studied%2520to%250Aaccelerate%2520general%2520matrix-multiplications%2520%2528GEMM%2529%2520during%2520large-scale%2520deep%2520neural%250Anetwork%2520%2528DNN%2529%2520inferences.%2520While%2520the%2520conventional%2520symmetric%2520quantization%250Afacilitates%2520low-resolution%2520processing%2520with%2520bit-slice%2520sparsity%2520for%2520both%2520weight%250Aand%2520activation%252C%2520its%2520accuracy%2520loss%2520caused%2520by%2520the%2520activation%2527s%2520asymmetric%250Adistributions%2520cannot%2520be%2520acceptable%252C%2520especially%2520for%2520large-scale%2520DNNs.%2520In%2520efforts%250Ato%2520mitigate%2520this%2520accuracy%2520loss%252C%2520recent%2520studies%2520have%2520actively%2520utilized%250Aasymmetric%2520quantization%2520for%2520activations%2520without%2520requiring%2520additional%250Aoperations.%2520However%252C%2520the%2520cutting-edge%2520asymmetric%2520quantization%2520produces%2520numerous%250Anonzero%2520slices%2520that%2520cannot%2520be%2520compressed%2520and%2520skipped%2520by%2520recent%2520bit-slice%2520GEMM%250Aaccelerators%252C%2520naturally%2520consuming%2520more%2520processing%2520energy%2520to%2520handle%2520the%250Aquantized%2520DNN%2520models.%250A%2520%2520To%2520simultaneously%2520achieve%2520high%2520accuracy%2520and%2520hardware%2520efficiency%2520for%250Alarge-scale%2520DNN%2520inferences%252C%2520this%2520paper%2520proposes%2520an%2520Asymmetrically-Quantized%250Abit-Slice%2520GEMM%2520%2528AQS-GEMM%2529%2520for%2520the%2520first%2520time.%2520In%2520contrast%2520to%2520the%2520previous%250Abit-slice%2520computing%252C%2520which%2520only%2520skips%2520operations%2520of%2520zero%2520slices%252C%2520the%2520AQS-GEMM%250Acompresses%2520frequent%2520nonzero%2520slices%252C%2520generated%2520by%2520asymmetric%2520quantization%252C%2520and%250Askips%2520their%2520operations.%2520To%2520increase%2520the%2520slice-level%2520sparsity%2520of%2520activations%252C%2520we%250Aalso%2520introduce%2520two%2520algorithm-hardware%2520co-optimization%2520methods%253A%2520a%2520zero-point%250Amanipulation%2520and%2520a%2520distribution-based%2520bit-slicing.%2520To%2520support%2520the%2520proposed%250AAQS-GEMM%2520and%2520optimizations%2520at%2520the%2520hardware-level%252C%2520we%2520newly%2520introduce%2520a%2520DNN%250Aaccelerator%252C%2520Panacea%252C%2520which%2520efficiently%2520handles%2520sparse/dense%2520workloads%2520of%2520the%250Atiled%2520AQS-GEMM%2520to%2520increase%2520data%2520reuse%2520and%2520utilization.%2520Panacea%2520supports%2520a%250Aspecialized%2520dataflow%2520and%2520run-length%2520encoding%2520to%2520maximize%2520data%2520reuse%2520and%250Aminimize%2520external%2520memory%2520accesses%252C%2520significantly%2520improving%2520its%2520hardware%250Aefficiency.%2520Our%2520benchmark%2520evaluations%2520show%2520Panacea%2520outperforms%2520existing%2520DNN%250Aaccelerators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Panacea%3A%20Novel%20DNN%20Accelerator%20using%20Accuracy-Preserving%20Asymmetric%0A%20%20Quantization%20and%20Energy-Saving%20Bit-Slice%20Sparsity&entry.906535625=Dongyun%20Kam%20and%20Myeongji%20Yun%20and%20Sunwoo%20Yoo%20and%20Seungwoo%20Hong%20and%20Zhengya%20Zhang%20and%20Youngjoo%20Lee&entry.1292438233=%20%20Low%20bit-precisions%20and%20their%20bit-slice%20sparsity%20have%20recently%20been%20studied%20to%0Aaccelerate%20general%20matrix-multiplications%20%28GEMM%29%20during%20large-scale%20deep%20neural%0Anetwork%20%28DNN%29%20inferences.%20While%20the%20conventional%20symmetric%20quantization%0Afacilitates%20low-resolution%20processing%20with%20bit-slice%20sparsity%20for%20both%20weight%0Aand%20activation%2C%20its%20accuracy%20loss%20caused%20by%20the%20activation%27s%20asymmetric%0Adistributions%20cannot%20be%20acceptable%2C%20especially%20for%20large-scale%20DNNs.%20In%20efforts%0Ato%20mitigate%20this%20accuracy%20loss%2C%20recent%20studies%20have%20actively%20utilized%0Aasymmetric%20quantization%20for%20activations%20without%20requiring%20additional%0Aoperations.%20However%2C%20the%20cutting-edge%20asymmetric%20quantization%20produces%20numerous%0Anonzero%20slices%20that%20cannot%20be%20compressed%20and%20skipped%20by%20recent%20bit-slice%20GEMM%0Aaccelerators%2C%20naturally%20consuming%20more%20processing%20energy%20to%20handle%20the%0Aquantized%20DNN%20models.%0A%20%20To%20simultaneously%20achieve%20high%20accuracy%20and%20hardware%20efficiency%20for%0Alarge-scale%20DNN%20inferences%2C%20this%20paper%20proposes%20an%20Asymmetrically-Quantized%0Abit-Slice%20GEMM%20%28AQS-GEMM%29%20for%20the%20first%20time.%20In%20contrast%20to%20the%20previous%0Abit-slice%20computing%2C%20which%20only%20skips%20operations%20of%20zero%20slices%2C%20the%20AQS-GEMM%0Acompresses%20frequent%20nonzero%20slices%2C%20generated%20by%20asymmetric%20quantization%2C%20and%0Askips%20their%20operations.%20To%20increase%20the%20slice-level%20sparsity%20of%20activations%2C%20we%0Aalso%20introduce%20two%20algorithm-hardware%20co-optimization%20methods%3A%20a%20zero-point%0Amanipulation%20and%20a%20distribution-based%20bit-slicing.%20To%20support%20the%20proposed%0AAQS-GEMM%20and%20optimizations%20at%20the%20hardware-level%2C%20we%20newly%20introduce%20a%20DNN%0Aaccelerator%2C%20Panacea%2C%20which%20efficiently%20handles%20sparse/dense%20workloads%20of%20the%0Atiled%20AQS-GEMM%20to%20increase%20data%20reuse%20and%20utilization.%20Panacea%20supports%20a%0Aspecialized%20dataflow%20and%20run-length%20encoding%20to%20maximize%20data%20reuse%20and%0Aminimize%20external%20memory%20accesses%2C%20significantly%20improving%20its%20hardware%0Aefficiency.%20Our%20benchmark%20evaluations%20show%20Panacea%20outperforms%20existing%20DNN%0Aaccelerators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10059v1&entry.124074799=Read"},
{"title": "RAID-Database: human Responses to Affine Image Distortions", "author": "Paula Daud\u00e9n-Oliver and David Agost-Beltran and Emilio Sansano-Sansano and Valero Laparra and Jes\u00fas Malo and Marina Mart\u00ednez-Garcia", "abstract": "  Image quality databases are used to train models for predicting subjective\nhuman perception. However, most existing databases focus on distortions\ncommonly found in digital media and not in natural conditions. Affine\ntransformations are particularly relevant to study, as they are among the most\ncommonly encountered by human observers in everyday life. This Data Descriptor\npresents a set of human responses to suprathreshold affine image transforms\n(rotation, translation, scaling) and Gaussian noise as convenient reference to\ncompare with previously existing image quality databases. The responses were\nmeasured using well established psychophysics: the Maximum Likelihood\nDifference Scaling method. The set contains responses to 864 distorted images.\nThe experiments involved 105 observers and more than 20000 comparisons of\nquadruples of images. The quality of the dataset is ensured because (a) it\nreproduces the classical Pi\\'eron's law, (b) it reproduces classical absolute\ndetection thresholds, and (c) it is consistent with conventional image quality\ndatabases but improves them according to Group-MAD experiments.\n", "link": "http://arxiv.org/abs/2412.10211v1", "date": "2024-12-13", "relevancy": 2.0114, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5106}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4983}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAID-Database%3A%20human%20Responses%20to%20Affine%20Image%20Distortions&body=Title%3A%20RAID-Database%3A%20human%20Responses%20to%20Affine%20Image%20Distortions%0AAuthor%3A%20Paula%20Daud%C3%A9n-Oliver%20and%20David%20Agost-Beltran%20and%20Emilio%20Sansano-Sansano%20and%20Valero%20Laparra%20and%20Jes%C3%BAs%20Malo%20and%20Marina%20Mart%C3%ADnez-Garcia%0AAbstract%3A%20%20%20Image%20quality%20databases%20are%20used%20to%20train%20models%20for%20predicting%20subjective%0Ahuman%20perception.%20However%2C%20most%20existing%20databases%20focus%20on%20distortions%0Acommonly%20found%20in%20digital%20media%20and%20not%20in%20natural%20conditions.%20Affine%0Atransformations%20are%20particularly%20relevant%20to%20study%2C%20as%20they%20are%20among%20the%20most%0Acommonly%20encountered%20by%20human%20observers%20in%20everyday%20life.%20This%20Data%20Descriptor%0Apresents%20a%20set%20of%20human%20responses%20to%20suprathreshold%20affine%20image%20transforms%0A%28rotation%2C%20translation%2C%20scaling%29%20and%20Gaussian%20noise%20as%20convenient%20reference%20to%0Acompare%20with%20previously%20existing%20image%20quality%20databases.%20The%20responses%20were%0Ameasured%20using%20well%20established%20psychophysics%3A%20the%20Maximum%20Likelihood%0ADifference%20Scaling%20method.%20The%20set%20contains%20responses%20to%20864%20distorted%20images.%0AThe%20experiments%20involved%20105%20observers%20and%20more%20than%2020000%20comparisons%20of%0Aquadruples%20of%20images.%20The%20quality%20of%20the%20dataset%20is%20ensured%20because%20%28a%29%20it%0Areproduces%20the%20classical%20Pi%5C%27eron%27s%20law%2C%20%28b%29%20it%20reproduces%20classical%20absolute%0Adetection%20thresholds%2C%20and%20%28c%29%20it%20is%20consistent%20with%20conventional%20image%20quality%0Adatabases%20but%20improves%20them%20according%20to%20Group-MAD%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAID-Database%253A%2520human%2520Responses%2520to%2520Affine%2520Image%2520Distortions%26entry.906535625%3DPaula%2520Daud%25C3%25A9n-Oliver%2520and%2520David%2520Agost-Beltran%2520and%2520Emilio%2520Sansano-Sansano%2520and%2520Valero%2520Laparra%2520and%2520Jes%25C3%25BAs%2520Malo%2520and%2520Marina%2520Mart%25C3%25ADnez-Garcia%26entry.1292438233%3D%2520%2520Image%2520quality%2520databases%2520are%2520used%2520to%2520train%2520models%2520for%2520predicting%2520subjective%250Ahuman%2520perception.%2520However%252C%2520most%2520existing%2520databases%2520focus%2520on%2520distortions%250Acommonly%2520found%2520in%2520digital%2520media%2520and%2520not%2520in%2520natural%2520conditions.%2520Affine%250Atransformations%2520are%2520particularly%2520relevant%2520to%2520study%252C%2520as%2520they%2520are%2520among%2520the%2520most%250Acommonly%2520encountered%2520by%2520human%2520observers%2520in%2520everyday%2520life.%2520This%2520Data%2520Descriptor%250Apresents%2520a%2520set%2520of%2520human%2520responses%2520to%2520suprathreshold%2520affine%2520image%2520transforms%250A%2528rotation%252C%2520translation%252C%2520scaling%2529%2520and%2520Gaussian%2520noise%2520as%2520convenient%2520reference%2520to%250Acompare%2520with%2520previously%2520existing%2520image%2520quality%2520databases.%2520The%2520responses%2520were%250Ameasured%2520using%2520well%2520established%2520psychophysics%253A%2520the%2520Maximum%2520Likelihood%250ADifference%2520Scaling%2520method.%2520The%2520set%2520contains%2520responses%2520to%2520864%2520distorted%2520images.%250AThe%2520experiments%2520involved%2520105%2520observers%2520and%2520more%2520than%252020000%2520comparisons%2520of%250Aquadruples%2520of%2520images.%2520The%2520quality%2520of%2520the%2520dataset%2520is%2520ensured%2520because%2520%2528a%2529%2520it%250Areproduces%2520the%2520classical%2520Pi%255C%2527eron%2527s%2520law%252C%2520%2528b%2529%2520it%2520reproduces%2520classical%2520absolute%250Adetection%2520thresholds%252C%2520and%2520%2528c%2529%2520it%2520is%2520consistent%2520with%2520conventional%2520image%2520quality%250Adatabases%2520but%2520improves%2520them%2520according%2520to%2520Group-MAD%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAID-Database%3A%20human%20Responses%20to%20Affine%20Image%20Distortions&entry.906535625=Paula%20Daud%C3%A9n-Oliver%20and%20David%20Agost-Beltran%20and%20Emilio%20Sansano-Sansano%20and%20Valero%20Laparra%20and%20Jes%C3%BAs%20Malo%20and%20Marina%20Mart%C3%ADnez-Garcia&entry.1292438233=%20%20Image%20quality%20databases%20are%20used%20to%20train%20models%20for%20predicting%20subjective%0Ahuman%20perception.%20However%2C%20most%20existing%20databases%20focus%20on%20distortions%0Acommonly%20found%20in%20digital%20media%20and%20not%20in%20natural%20conditions.%20Affine%0Atransformations%20are%20particularly%20relevant%20to%20study%2C%20as%20they%20are%20among%20the%20most%0Acommonly%20encountered%20by%20human%20observers%20in%20everyday%20life.%20This%20Data%20Descriptor%0Apresents%20a%20set%20of%20human%20responses%20to%20suprathreshold%20affine%20image%20transforms%0A%28rotation%2C%20translation%2C%20scaling%29%20and%20Gaussian%20noise%20as%20convenient%20reference%20to%0Acompare%20with%20previously%20existing%20image%20quality%20databases.%20The%20responses%20were%0Ameasured%20using%20well%20established%20psychophysics%3A%20the%20Maximum%20Likelihood%0ADifference%20Scaling%20method.%20The%20set%20contains%20responses%20to%20864%20distorted%20images.%0AThe%20experiments%20involved%20105%20observers%20and%20more%20than%2020000%20comparisons%20of%0Aquadruples%20of%20images.%20The%20quality%20of%20the%20dataset%20is%20ensured%20because%20%28a%29%20it%0Areproduces%20the%20classical%20Pi%5C%27eron%27s%20law%2C%20%28b%29%20it%20reproduces%20classical%20absolute%0Adetection%20thresholds%2C%20and%20%28c%29%20it%20is%20consistent%20with%20conventional%20image%20quality%0Adatabases%20but%20improves%20them%20according%20to%20Group-MAD%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10211v1&entry.124074799=Read"},
{"title": "Generative AI in Medicine", "author": "Divya Shanmugam and Monica Agrawal and Rajiv Movva and Irene Y. Chen and Marzyeh Ghassemi and Emma Pierson", "abstract": "  The increased capabilities of generative AI have dramatically expanded its\npossible use cases in medicine. We provide a comprehensive overview of\ngenerative AI use cases for clinicians, patients, clinical trial organizers,\nresearchers, and trainees. We then discuss the many challenges -- including\nmaintaining privacy and security, improving transparency and interpretability,\nupholding equity, and rigorously evaluating models -- which must be overcome to\nrealize this potential, and the open research directions they give rise to.\n", "link": "http://arxiv.org/abs/2412.10337v1", "date": "2024-12-13", "relevancy": 2.0046, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5402}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4978}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20in%20Medicine&body=Title%3A%20Generative%20AI%20in%20Medicine%0AAuthor%3A%20Divya%20Shanmugam%20and%20Monica%20Agrawal%20and%20Rajiv%20Movva%20and%20Irene%20Y.%20Chen%20and%20Marzyeh%20Ghassemi%20and%20Emma%20Pierson%0AAbstract%3A%20%20%20The%20increased%20capabilities%20of%20generative%20AI%20have%20dramatically%20expanded%20its%0Apossible%20use%20cases%20in%20medicine.%20We%20provide%20a%20comprehensive%20overview%20of%0Agenerative%20AI%20use%20cases%20for%20clinicians%2C%20patients%2C%20clinical%20trial%20organizers%2C%0Aresearchers%2C%20and%20trainees.%20We%20then%20discuss%20the%20many%20challenges%20--%20including%0Amaintaining%20privacy%20and%20security%2C%20improving%20transparency%20and%20interpretability%2C%0Aupholding%20equity%2C%20and%20rigorously%20evaluating%20models%20--%20which%20must%20be%20overcome%20to%0Arealize%20this%20potential%2C%20and%20the%20open%20research%20directions%20they%20give%20rise%20to.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520in%2520Medicine%26entry.906535625%3DDivya%2520Shanmugam%2520and%2520Monica%2520Agrawal%2520and%2520Rajiv%2520Movva%2520and%2520Irene%2520Y.%2520Chen%2520and%2520Marzyeh%2520Ghassemi%2520and%2520Emma%2520Pierson%26entry.1292438233%3D%2520%2520The%2520increased%2520capabilities%2520of%2520generative%2520AI%2520have%2520dramatically%2520expanded%2520its%250Apossible%2520use%2520cases%2520in%2520medicine.%2520We%2520provide%2520a%2520comprehensive%2520overview%2520of%250Agenerative%2520AI%2520use%2520cases%2520for%2520clinicians%252C%2520patients%252C%2520clinical%2520trial%2520organizers%252C%250Aresearchers%252C%2520and%2520trainees.%2520We%2520then%2520discuss%2520the%2520many%2520challenges%2520--%2520including%250Amaintaining%2520privacy%2520and%2520security%252C%2520improving%2520transparency%2520and%2520interpretability%252C%250Aupholding%2520equity%252C%2520and%2520rigorously%2520evaluating%2520models%2520--%2520which%2520must%2520be%2520overcome%2520to%250Arealize%2520this%2520potential%252C%2520and%2520the%2520open%2520research%2520directions%2520they%2520give%2520rise%2520to.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20in%20Medicine&entry.906535625=Divya%20Shanmugam%20and%20Monica%20Agrawal%20and%20Rajiv%20Movva%20and%20Irene%20Y.%20Chen%20and%20Marzyeh%20Ghassemi%20and%20Emma%20Pierson&entry.1292438233=%20%20The%20increased%20capabilities%20of%20generative%20AI%20have%20dramatically%20expanded%20its%0Apossible%20use%20cases%20in%20medicine.%20We%20provide%20a%20comprehensive%20overview%20of%0Agenerative%20AI%20use%20cases%20for%20clinicians%2C%20patients%2C%20clinical%20trial%20organizers%2C%0Aresearchers%2C%20and%20trainees.%20We%20then%20discuss%20the%20many%20challenges%20--%20including%0Amaintaining%20privacy%20and%20security%2C%20improving%20transparency%20and%20interpretability%2C%0Aupholding%20equity%2C%20and%20rigorously%20evaluating%20models%20--%20which%20must%20be%20overcome%20to%0Arealize%20this%20potential%2C%20and%20the%20open%20research%20directions%20they%20give%20rise%20to.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10337v1&entry.124074799=Read"},
{"title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods", "author": "Yucheng Li and Huiqiang Jiang and Qianhui Wu and Xufang Luo and Surin Ahn and Chengruidong Zhang and Amir H. Abdi and Dongsheng Li and Jianfeng Gao and Yuqing Yang and Lili Qiu", "abstract": "  Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.\n", "link": "http://arxiv.org/abs/2412.10319v1", "date": "2024-12-13", "relevancy": 2.0033, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5096}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5096}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCBench%3A%20A%20KV%20Cache-Centric%20Analysis%20of%20Long-Context%20Methods&body=Title%3A%20SCBench%3A%20A%20KV%20Cache-Centric%20Analysis%20of%20Long-Context%20Methods%0AAuthor%3A%20Yucheng%20Li%20and%20Huiqiang%20Jiang%20and%20Qianhui%20Wu%20and%20Xufang%20Luo%20and%20Surin%20Ahn%20and%20Chengruidong%20Zhang%20and%20Amir%20H.%20Abdi%20and%20Dongsheng%20Li%20and%20Jianfeng%20Gao%20and%20Yuqing%20Yang%20and%20Lili%20Qiu%0AAbstract%3A%20%20%20Long-context%20LLMs%20have%20enabled%20numerous%20downstream%20applications%20but%20also%0Aintroduced%20significant%20challenges%20related%20to%20computational%20and%20memory%0Aefficiency.%20To%20address%20these%20challenges%2C%20optimizations%20for%20long-context%0Ainference%20have%20been%20developed%2C%20centered%20around%20the%20KV%20cache.%20However%2C%20existing%0Abenchmarks%20often%20evaluate%20in%20single-request%2C%20neglecting%20the%20full%20lifecycle%20of%0Athe%20KV%20cache%20in%20real-world%20use.%20This%20oversight%20is%20particularly%20critical%2C%20as%20KV%0Acache%20reuse%20has%20become%20widely%20adopted%20in%20LLMs%20inference%20frameworks%2C%20such%20as%0AvLLM%20and%20SGLang%2C%20as%20well%20as%20by%20LLM%20providers%2C%20including%20OpenAI%2C%20Microsoft%2C%0AGoogle%2C%20and%20Anthropic.%20To%20address%20this%20gap%2C%20we%20introduce%0ASCBench%28SharedContextBench%29%2C%20a%20comprehensive%20benchmark%20for%20evaluating%0Along-context%20methods%20from%20a%20KV%20cachecentric%20perspective%3A%201%29%20KV%20cache%0Ageneration%2C%202%29%20KV%20cache%20compression%2C%203%29%20KV%20cache%20retrieval%2C%204%29%20KV%20cache%0Aloading.%20Specifically%2C%20SCBench%20uses%20test%20examples%20with%20shared%20context%2C%20ranging%0A12%20tasks%20with%20two%20shared%20context%20modes%2C%20covering%20four%20categories%20of%0Along-context%20capabilities%3A%20string%20retrieval%2C%20semantic%20retrieval%2C%20global%0Ainformation%2C%20and%20multi-task.%20With%20it%2C%20we%20provide%20an%20extensive%20KV%20cache-centric%0Aanalysis%20of%20eight%20categories%20long-context%20solutions%2C%20including%20Gated%20Linear%0ARNNs%2C%20Mamba-Attention%20hybrids%2C%20and%20efficient%20methods%20such%20as%20sparse%20attention%2C%0AKV%20cache%20dropping%2C%20quantization%2C%20retrieval%2C%20loading%2C%20and%20prompt%20compression.%0AThe%20evaluation%20is%20conducted%20on%208%20long-context%20LLMs.%20Our%20findings%20show%20that%0Asub-O%28n%29%20memory%20methods%20suffer%20in%20multi-turn%20scenarios%2C%20while%20sparse%20encoding%0Awith%20O%28n%29%20memory%20and%20sub-O%28n%5E2%29%20pre-filling%20computation%20perform%20robustly.%0ADynamic%20sparsity%20yields%20more%20expressive%20KV%20caches%20than%20static%20patterns%2C%20and%0Alayer-level%20sparsity%20in%20hybrid%20architectures%20reduces%20memory%20usage%20with%20strong%0Aperformance.%20Additionally%2C%20we%20identify%20attention%20distribution%20shift%20issues%20in%0Along-generation%20scenarios.%20https%3A//aka.ms/SCBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCBench%253A%2520A%2520KV%2520Cache-Centric%2520Analysis%2520of%2520Long-Context%2520Methods%26entry.906535625%3DYucheng%2520Li%2520and%2520Huiqiang%2520Jiang%2520and%2520Qianhui%2520Wu%2520and%2520Xufang%2520Luo%2520and%2520Surin%2520Ahn%2520and%2520Chengruidong%2520Zhang%2520and%2520Amir%2520H.%2520Abdi%2520and%2520Dongsheng%2520Li%2520and%2520Jianfeng%2520Gao%2520and%2520Yuqing%2520Yang%2520and%2520Lili%2520Qiu%26entry.1292438233%3D%2520%2520Long-context%2520LLMs%2520have%2520enabled%2520numerous%2520downstream%2520applications%2520but%2520also%250Aintroduced%2520significant%2520challenges%2520related%2520to%2520computational%2520and%2520memory%250Aefficiency.%2520To%2520address%2520these%2520challenges%252C%2520optimizations%2520for%2520long-context%250Ainference%2520have%2520been%2520developed%252C%2520centered%2520around%2520the%2520KV%2520cache.%2520However%252C%2520existing%250Abenchmarks%2520often%2520evaluate%2520in%2520single-request%252C%2520neglecting%2520the%2520full%2520lifecycle%2520of%250Athe%2520KV%2520cache%2520in%2520real-world%2520use.%2520This%2520oversight%2520is%2520particularly%2520critical%252C%2520as%2520KV%250Acache%2520reuse%2520has%2520become%2520widely%2520adopted%2520in%2520LLMs%2520inference%2520frameworks%252C%2520such%2520as%250AvLLM%2520and%2520SGLang%252C%2520as%2520well%2520as%2520by%2520LLM%2520providers%252C%2520including%2520OpenAI%252C%2520Microsoft%252C%250AGoogle%252C%2520and%2520Anthropic.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250ASCBench%2528SharedContextBench%2529%252C%2520a%2520comprehensive%2520benchmark%2520for%2520evaluating%250Along-context%2520methods%2520from%2520a%2520KV%2520cachecentric%2520perspective%253A%25201%2529%2520KV%2520cache%250Ageneration%252C%25202%2529%2520KV%2520cache%2520compression%252C%25203%2529%2520KV%2520cache%2520retrieval%252C%25204%2529%2520KV%2520cache%250Aloading.%2520Specifically%252C%2520SCBench%2520uses%2520test%2520examples%2520with%2520shared%2520context%252C%2520ranging%250A12%2520tasks%2520with%2520two%2520shared%2520context%2520modes%252C%2520covering%2520four%2520categories%2520of%250Along-context%2520capabilities%253A%2520string%2520retrieval%252C%2520semantic%2520retrieval%252C%2520global%250Ainformation%252C%2520and%2520multi-task.%2520With%2520it%252C%2520we%2520provide%2520an%2520extensive%2520KV%2520cache-centric%250Aanalysis%2520of%2520eight%2520categories%2520long-context%2520solutions%252C%2520including%2520Gated%2520Linear%250ARNNs%252C%2520Mamba-Attention%2520hybrids%252C%2520and%2520efficient%2520methods%2520such%2520as%2520sparse%2520attention%252C%250AKV%2520cache%2520dropping%252C%2520quantization%252C%2520retrieval%252C%2520loading%252C%2520and%2520prompt%2520compression.%250AThe%2520evaluation%2520is%2520conducted%2520on%25208%2520long-context%2520LLMs.%2520Our%2520findings%2520show%2520that%250Asub-O%2528n%2529%2520memory%2520methods%2520suffer%2520in%2520multi-turn%2520scenarios%252C%2520while%2520sparse%2520encoding%250Awith%2520O%2528n%2529%2520memory%2520and%2520sub-O%2528n%255E2%2529%2520pre-filling%2520computation%2520perform%2520robustly.%250ADynamic%2520sparsity%2520yields%2520more%2520expressive%2520KV%2520caches%2520than%2520static%2520patterns%252C%2520and%250Alayer-level%2520sparsity%2520in%2520hybrid%2520architectures%2520reduces%2520memory%2520usage%2520with%2520strong%250Aperformance.%2520Additionally%252C%2520we%2520identify%2520attention%2520distribution%2520shift%2520issues%2520in%250Along-generation%2520scenarios.%2520https%253A//aka.ms/SCBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCBench%3A%20A%20KV%20Cache-Centric%20Analysis%20of%20Long-Context%20Methods&entry.906535625=Yucheng%20Li%20and%20Huiqiang%20Jiang%20and%20Qianhui%20Wu%20and%20Xufang%20Luo%20and%20Surin%20Ahn%20and%20Chengruidong%20Zhang%20and%20Amir%20H.%20Abdi%20and%20Dongsheng%20Li%20and%20Jianfeng%20Gao%20and%20Yuqing%20Yang%20and%20Lili%20Qiu&entry.1292438233=%20%20Long-context%20LLMs%20have%20enabled%20numerous%20downstream%20applications%20but%20also%0Aintroduced%20significant%20challenges%20related%20to%20computational%20and%20memory%0Aefficiency.%20To%20address%20these%20challenges%2C%20optimizations%20for%20long-context%0Ainference%20have%20been%20developed%2C%20centered%20around%20the%20KV%20cache.%20However%2C%20existing%0Abenchmarks%20often%20evaluate%20in%20single-request%2C%20neglecting%20the%20full%20lifecycle%20of%0Athe%20KV%20cache%20in%20real-world%20use.%20This%20oversight%20is%20particularly%20critical%2C%20as%20KV%0Acache%20reuse%20has%20become%20widely%20adopted%20in%20LLMs%20inference%20frameworks%2C%20such%20as%0AvLLM%20and%20SGLang%2C%20as%20well%20as%20by%20LLM%20providers%2C%20including%20OpenAI%2C%20Microsoft%2C%0AGoogle%2C%20and%20Anthropic.%20To%20address%20this%20gap%2C%20we%20introduce%0ASCBench%28SharedContextBench%29%2C%20a%20comprehensive%20benchmark%20for%20evaluating%0Along-context%20methods%20from%20a%20KV%20cachecentric%20perspective%3A%201%29%20KV%20cache%0Ageneration%2C%202%29%20KV%20cache%20compression%2C%203%29%20KV%20cache%20retrieval%2C%204%29%20KV%20cache%0Aloading.%20Specifically%2C%20SCBench%20uses%20test%20examples%20with%20shared%20context%2C%20ranging%0A12%20tasks%20with%20two%20shared%20context%20modes%2C%20covering%20four%20categories%20of%0Along-context%20capabilities%3A%20string%20retrieval%2C%20semantic%20retrieval%2C%20global%0Ainformation%2C%20and%20multi-task.%20With%20it%2C%20we%20provide%20an%20extensive%20KV%20cache-centric%0Aanalysis%20of%20eight%20categories%20long-context%20solutions%2C%20including%20Gated%20Linear%0ARNNs%2C%20Mamba-Attention%20hybrids%2C%20and%20efficient%20methods%20such%20as%20sparse%20attention%2C%0AKV%20cache%20dropping%2C%20quantization%2C%20retrieval%2C%20loading%2C%20and%20prompt%20compression.%0AThe%20evaluation%20is%20conducted%20on%208%20long-context%20LLMs.%20Our%20findings%20show%20that%0Asub-O%28n%29%20memory%20methods%20suffer%20in%20multi-turn%20scenarios%2C%20while%20sparse%20encoding%0Awith%20O%28n%29%20memory%20and%20sub-O%28n%5E2%29%20pre-filling%20computation%20perform%20robustly.%0ADynamic%20sparsity%20yields%20more%20expressive%20KV%20caches%20than%20static%20patterns%2C%20and%0Alayer-level%20sparsity%20in%20hybrid%20architectures%20reduces%20memory%20usage%20with%20strong%0Aperformance.%20Additionally%2C%20we%20identify%20attention%20distribution%20shift%20issues%20in%0Along-generation%20scenarios.%20https%3A//aka.ms/SCBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10319v1&entry.124074799=Read"},
{"title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language\n  Models", "author": "Zhihao Du and Yuxuan Wang and Qian Chen and Xian Shi and Xiang Lv and Tianyu Zhao and Zhifu Gao and Yexin Yang and Changfeng Gao and Hui Wang and Fan Yu and Huadai Liu and Zhengyan Sheng and Yue Gu and Chong Deng and Wen Wang and Shiliang Zhang and Zhijie Yan and Jingren Zhou", "abstract": "  In our previous work, we introduced CosyVoice, a multilingual speech\nsynthesis model based on supervised discrete speech tokens. By employing\nprogressive semantic decoding with two popular generative models, language\nmodels (LMs) and Flow Matching, CosyVoice demonstrated high prosody\nnaturalness, content consistency, and speaker similarity in speech in-context\nlearning. Recently, significant progress has been made in multi-modal large\nlanguage models (LLMs), where the response latency and real-time factor of\nspeech synthesis play a crucial role in the interactive experience. Therefore,\nin this report, we present an improved streaming speech synthesis model,\nCosyVoice 2, which incorporates comprehensive and systematic optimizations.\nSpecifically, we introduce finite-scalar quantization to improve the codebook\nutilization of speech tokens. For the text-speech LM, we streamline the model\narchitecture to allow direct use of a pre-trained LLM as the backbone. In\naddition, we develop a chunk-aware causal flow matching model to support\nvarious synthesis scenarios, enabling both streaming and non-streaming\nsynthesis within a single model. By training on a large-scale multilingual\ndataset, CosyVoice 2 achieves human-parity naturalness, minimal response\nlatency, and virtually lossless synthesis quality in the streaming mode. We\ninvite readers to listen to the demos at\nhttps://funaudiollm.github.io/cosyvoice2.\n", "link": "http://arxiv.org/abs/2412.10117v1", "date": "2024-12-13", "relevancy": 1.9966, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5265}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CosyVoice%202%3A%20Scalable%20Streaming%20Speech%20Synthesis%20with%20Large%20Language%0A%20%20Models&body=Title%3A%20CosyVoice%202%3A%20Scalable%20Streaming%20Speech%20Synthesis%20with%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Zhihao%20Du%20and%20Yuxuan%20Wang%20and%20Qian%20Chen%20and%20Xian%20Shi%20and%20Xiang%20Lv%20and%20Tianyu%20Zhao%20and%20Zhifu%20Gao%20and%20Yexin%20Yang%20and%20Changfeng%20Gao%20and%20Hui%20Wang%20and%20Fan%20Yu%20and%20Huadai%20Liu%20and%20Zhengyan%20Sheng%20and%20Yue%20Gu%20and%20Chong%20Deng%20and%20Wen%20Wang%20and%20Shiliang%20Zhang%20and%20Zhijie%20Yan%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20In%20our%20previous%20work%2C%20we%20introduced%20CosyVoice%2C%20a%20multilingual%20speech%0Asynthesis%20model%20based%20on%20supervised%20discrete%20speech%20tokens.%20By%20employing%0Aprogressive%20semantic%20decoding%20with%20two%20popular%20generative%20models%2C%20language%0Amodels%20%28LMs%29%20and%20Flow%20Matching%2C%20CosyVoice%20demonstrated%20high%20prosody%0Anaturalness%2C%20content%20consistency%2C%20and%20speaker%20similarity%20in%20speech%20in-context%0Alearning.%20Recently%2C%20significant%20progress%20has%20been%20made%20in%20multi-modal%20large%0Alanguage%20models%20%28LLMs%29%2C%20where%20the%20response%20latency%20and%20real-time%20factor%20of%0Aspeech%20synthesis%20play%20a%20crucial%20role%20in%20the%20interactive%20experience.%20Therefore%2C%0Ain%20this%20report%2C%20we%20present%20an%20improved%20streaming%20speech%20synthesis%20model%2C%0ACosyVoice%202%2C%20which%20incorporates%20comprehensive%20and%20systematic%20optimizations.%0ASpecifically%2C%20we%20introduce%20finite-scalar%20quantization%20to%20improve%20the%20codebook%0Autilization%20of%20speech%20tokens.%20For%20the%20text-speech%20LM%2C%20we%20streamline%20the%20model%0Aarchitecture%20to%20allow%20direct%20use%20of%20a%20pre-trained%20LLM%20as%20the%20backbone.%20In%0Aaddition%2C%20we%20develop%20a%20chunk-aware%20causal%20flow%20matching%20model%20to%20support%0Avarious%20synthesis%20scenarios%2C%20enabling%20both%20streaming%20and%20non-streaming%0Asynthesis%20within%20a%20single%20model.%20By%20training%20on%20a%20large-scale%20multilingual%0Adataset%2C%20CosyVoice%202%20achieves%20human-parity%20naturalness%2C%20minimal%20response%0Alatency%2C%20and%20virtually%20lossless%20synthesis%20quality%20in%20the%20streaming%20mode.%20We%0Ainvite%20readers%20to%20listen%20to%20the%20demos%20at%0Ahttps%3A//funaudiollm.github.io/cosyvoice2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCosyVoice%25202%253A%2520Scalable%2520Streaming%2520Speech%2520Synthesis%2520with%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DZhihao%2520Du%2520and%2520Yuxuan%2520Wang%2520and%2520Qian%2520Chen%2520and%2520Xian%2520Shi%2520and%2520Xiang%2520Lv%2520and%2520Tianyu%2520Zhao%2520and%2520Zhifu%2520Gao%2520and%2520Yexin%2520Yang%2520and%2520Changfeng%2520Gao%2520and%2520Hui%2520Wang%2520and%2520Fan%2520Yu%2520and%2520Huadai%2520Liu%2520and%2520Zhengyan%2520Sheng%2520and%2520Yue%2520Gu%2520and%2520Chong%2520Deng%2520and%2520Wen%2520Wang%2520and%2520Shiliang%2520Zhang%2520and%2520Zhijie%2520Yan%2520and%2520Jingren%2520Zhou%26entry.1292438233%3D%2520%2520In%2520our%2520previous%2520work%252C%2520we%2520introduced%2520CosyVoice%252C%2520a%2520multilingual%2520speech%250Asynthesis%2520model%2520based%2520on%2520supervised%2520discrete%2520speech%2520tokens.%2520By%2520employing%250Aprogressive%2520semantic%2520decoding%2520with%2520two%2520popular%2520generative%2520models%252C%2520language%250Amodels%2520%2528LMs%2529%2520and%2520Flow%2520Matching%252C%2520CosyVoice%2520demonstrated%2520high%2520prosody%250Anaturalness%252C%2520content%2520consistency%252C%2520and%2520speaker%2520similarity%2520in%2520speech%2520in-context%250Alearning.%2520Recently%252C%2520significant%2520progress%2520has%2520been%2520made%2520in%2520multi-modal%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520where%2520the%2520response%2520latency%2520and%2520real-time%2520factor%2520of%250Aspeech%2520synthesis%2520play%2520a%2520crucial%2520role%2520in%2520the%2520interactive%2520experience.%2520Therefore%252C%250Ain%2520this%2520report%252C%2520we%2520present%2520an%2520improved%2520streaming%2520speech%2520synthesis%2520model%252C%250ACosyVoice%25202%252C%2520which%2520incorporates%2520comprehensive%2520and%2520systematic%2520optimizations.%250ASpecifically%252C%2520we%2520introduce%2520finite-scalar%2520quantization%2520to%2520improve%2520the%2520codebook%250Autilization%2520of%2520speech%2520tokens.%2520For%2520the%2520text-speech%2520LM%252C%2520we%2520streamline%2520the%2520model%250Aarchitecture%2520to%2520allow%2520direct%2520use%2520of%2520a%2520pre-trained%2520LLM%2520as%2520the%2520backbone.%2520In%250Aaddition%252C%2520we%2520develop%2520a%2520chunk-aware%2520causal%2520flow%2520matching%2520model%2520to%2520support%250Avarious%2520synthesis%2520scenarios%252C%2520enabling%2520both%2520streaming%2520and%2520non-streaming%250Asynthesis%2520within%2520a%2520single%2520model.%2520By%2520training%2520on%2520a%2520large-scale%2520multilingual%250Adataset%252C%2520CosyVoice%25202%2520achieves%2520human-parity%2520naturalness%252C%2520minimal%2520response%250Alatency%252C%2520and%2520virtually%2520lossless%2520synthesis%2520quality%2520in%2520the%2520streaming%2520mode.%2520We%250Ainvite%2520readers%2520to%2520listen%2520to%2520the%2520demos%2520at%250Ahttps%253A//funaudiollm.github.io/cosyvoice2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CosyVoice%202%3A%20Scalable%20Streaming%20Speech%20Synthesis%20with%20Large%20Language%0A%20%20Models&entry.906535625=Zhihao%20Du%20and%20Yuxuan%20Wang%20and%20Qian%20Chen%20and%20Xian%20Shi%20and%20Xiang%20Lv%20and%20Tianyu%20Zhao%20and%20Zhifu%20Gao%20and%20Yexin%20Yang%20and%20Changfeng%20Gao%20and%20Hui%20Wang%20and%20Fan%20Yu%20and%20Huadai%20Liu%20and%20Zhengyan%20Sheng%20and%20Yue%20Gu%20and%20Chong%20Deng%20and%20Wen%20Wang%20and%20Shiliang%20Zhang%20and%20Zhijie%20Yan%20and%20Jingren%20Zhou&entry.1292438233=%20%20In%20our%20previous%20work%2C%20we%20introduced%20CosyVoice%2C%20a%20multilingual%20speech%0Asynthesis%20model%20based%20on%20supervised%20discrete%20speech%20tokens.%20By%20employing%0Aprogressive%20semantic%20decoding%20with%20two%20popular%20generative%20models%2C%20language%0Amodels%20%28LMs%29%20and%20Flow%20Matching%2C%20CosyVoice%20demonstrated%20high%20prosody%0Anaturalness%2C%20content%20consistency%2C%20and%20speaker%20similarity%20in%20speech%20in-context%0Alearning.%20Recently%2C%20significant%20progress%20has%20been%20made%20in%20multi-modal%20large%0Alanguage%20models%20%28LLMs%29%2C%20where%20the%20response%20latency%20and%20real-time%20factor%20of%0Aspeech%20synthesis%20play%20a%20crucial%20role%20in%20the%20interactive%20experience.%20Therefore%2C%0Ain%20this%20report%2C%20we%20present%20an%20improved%20streaming%20speech%20synthesis%20model%2C%0ACosyVoice%202%2C%20which%20incorporates%20comprehensive%20and%20systematic%20optimizations.%0ASpecifically%2C%20we%20introduce%20finite-scalar%20quantization%20to%20improve%20the%20codebook%0Autilization%20of%20speech%20tokens.%20For%20the%20text-speech%20LM%2C%20we%20streamline%20the%20model%0Aarchitecture%20to%20allow%20direct%20use%20of%20a%20pre-trained%20LLM%20as%20the%20backbone.%20In%0Aaddition%2C%20we%20develop%20a%20chunk-aware%20causal%20flow%20matching%20model%20to%20support%0Avarious%20synthesis%20scenarios%2C%20enabling%20both%20streaming%20and%20non-streaming%0Asynthesis%20within%20a%20single%20model.%20By%20training%20on%20a%20large-scale%20multilingual%0Adataset%2C%20CosyVoice%202%20achieves%20human-parity%20naturalness%2C%20minimal%20response%0Alatency%2C%20and%20virtually%20lossless%20synthesis%20quality%20in%20the%20streaming%20mode.%20We%0Ainvite%20readers%20to%20listen%20to%20the%20demos%20at%0Ahttps%3A//funaudiollm.github.io/cosyvoice2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10117v1&entry.124074799=Read"},
{"title": "BatDeck -- Ultra Low-power Ultrasonic Ego-velocity Estimation and\n  Obstacle Avoidance on Nano-drones", "author": "Hanna M\u00fcller and Victor Kartsch and Michele Magno and Luca Benini", "abstract": "  Nano-drones, with their small, lightweight design, are ideal for\nconfined-space rescue missions and inherently safe for human interaction.\nHowever, their limited payload restricts the critical sensing needed for\nego-velocity estimation and obstacle detection to single-bean laser-based\ntime-of-flight (ToF) and low-resolution optical sensors. Although those sensors\nhave demonstrated good performance, they fail in some complex real-world\nscenarios, especially when facing transparent or reflective surfaces (ToFs) or\nwhen lacking visual features (optical-flow sensors). Taking inspiration from\nbats, this paper proposes a novel two-way ranging-based method for ego-velocity\nestimation and obstacle avoidance based on down-and-forward facing\nultra-low-power ultrasonic sensors, which improve the performance when the\ndrone faces reflective materials or navigates in complete darkness. Our results\ndemonstrate that our new sensing system achieves a mean square error of 0.019\nm/s on ego-velocity estimation and allows exploration for a flight time of 8\nminutes while covering 136 m on average in a challenging environment with\ntransparent and reflective obstacles. We also compare ultrasonic and\nlaser-based ToF sensing techniques for obstacle avoidance, as well as optical\nflow and ultrasonic-based techniques for ego-velocity estimation, denoting how\nthese systems and methods can be complemented to enhance the robustness of\nnano-drone operations.\n", "link": "http://arxiv.org/abs/2412.10048v1", "date": "2024-12-13", "relevancy": 1.992, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5111}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4958}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BatDeck%20--%20Ultra%20Low-power%20Ultrasonic%20Ego-velocity%20Estimation%20and%0A%20%20Obstacle%20Avoidance%20on%20Nano-drones&body=Title%3A%20BatDeck%20--%20Ultra%20Low-power%20Ultrasonic%20Ego-velocity%20Estimation%20and%0A%20%20Obstacle%20Avoidance%20on%20Nano-drones%0AAuthor%3A%20Hanna%20M%C3%BCller%20and%20Victor%20Kartsch%20and%20Michele%20Magno%20and%20Luca%20Benini%0AAbstract%3A%20%20%20Nano-drones%2C%20with%20their%20small%2C%20lightweight%20design%2C%20are%20ideal%20for%0Aconfined-space%20rescue%20missions%20and%20inherently%20safe%20for%20human%20interaction.%0AHowever%2C%20their%20limited%20payload%20restricts%20the%20critical%20sensing%20needed%20for%0Aego-velocity%20estimation%20and%20obstacle%20detection%20to%20single-bean%20laser-based%0Atime-of-flight%20%28ToF%29%20and%20low-resolution%20optical%20sensors.%20Although%20those%20sensors%0Ahave%20demonstrated%20good%20performance%2C%20they%20fail%20in%20some%20complex%20real-world%0Ascenarios%2C%20especially%20when%20facing%20transparent%20or%20reflective%20surfaces%20%28ToFs%29%20or%0Awhen%20lacking%20visual%20features%20%28optical-flow%20sensors%29.%20Taking%20inspiration%20from%0Abats%2C%20this%20paper%20proposes%20a%20novel%20two-way%20ranging-based%20method%20for%20ego-velocity%0Aestimation%20and%20obstacle%20avoidance%20based%20on%20down-and-forward%20facing%0Aultra-low-power%20ultrasonic%20sensors%2C%20which%20improve%20the%20performance%20when%20the%0Adrone%20faces%20reflective%20materials%20or%20navigates%20in%20complete%20darkness.%20Our%20results%0Ademonstrate%20that%20our%20new%20sensing%20system%20achieves%20a%20mean%20square%20error%20of%200.019%0Am/s%20on%20ego-velocity%20estimation%20and%20allows%20exploration%20for%20a%20flight%20time%20of%208%0Aminutes%20while%20covering%20136%20m%20on%20average%20in%20a%20challenging%20environment%20with%0Atransparent%20and%20reflective%20obstacles.%20We%20also%20compare%20ultrasonic%20and%0Alaser-based%20ToF%20sensing%20techniques%20for%20obstacle%20avoidance%2C%20as%20well%20as%20optical%0Aflow%20and%20ultrasonic-based%20techniques%20for%20ego-velocity%20estimation%2C%20denoting%20how%0Athese%20systems%20and%20methods%20can%20be%20complemented%20to%20enhance%20the%20robustness%20of%0Anano-drone%20operations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBatDeck%2520--%2520Ultra%2520Low-power%2520Ultrasonic%2520Ego-velocity%2520Estimation%2520and%250A%2520%2520Obstacle%2520Avoidance%2520on%2520Nano-drones%26entry.906535625%3DHanna%2520M%25C3%25BCller%2520and%2520Victor%2520Kartsch%2520and%2520Michele%2520Magno%2520and%2520Luca%2520Benini%26entry.1292438233%3D%2520%2520Nano-drones%252C%2520with%2520their%2520small%252C%2520lightweight%2520design%252C%2520are%2520ideal%2520for%250Aconfined-space%2520rescue%2520missions%2520and%2520inherently%2520safe%2520for%2520human%2520interaction.%250AHowever%252C%2520their%2520limited%2520payload%2520restricts%2520the%2520critical%2520sensing%2520needed%2520for%250Aego-velocity%2520estimation%2520and%2520obstacle%2520detection%2520to%2520single-bean%2520laser-based%250Atime-of-flight%2520%2528ToF%2529%2520and%2520low-resolution%2520optical%2520sensors.%2520Although%2520those%2520sensors%250Ahave%2520demonstrated%2520good%2520performance%252C%2520they%2520fail%2520in%2520some%2520complex%2520real-world%250Ascenarios%252C%2520especially%2520when%2520facing%2520transparent%2520or%2520reflective%2520surfaces%2520%2528ToFs%2529%2520or%250Awhen%2520lacking%2520visual%2520features%2520%2528optical-flow%2520sensors%2529.%2520Taking%2520inspiration%2520from%250Abats%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520two-way%2520ranging-based%2520method%2520for%2520ego-velocity%250Aestimation%2520and%2520obstacle%2520avoidance%2520based%2520on%2520down-and-forward%2520facing%250Aultra-low-power%2520ultrasonic%2520sensors%252C%2520which%2520improve%2520the%2520performance%2520when%2520the%250Adrone%2520faces%2520reflective%2520materials%2520or%2520navigates%2520in%2520complete%2520darkness.%2520Our%2520results%250Ademonstrate%2520that%2520our%2520new%2520sensing%2520system%2520achieves%2520a%2520mean%2520square%2520error%2520of%25200.019%250Am/s%2520on%2520ego-velocity%2520estimation%2520and%2520allows%2520exploration%2520for%2520a%2520flight%2520time%2520of%25208%250Aminutes%2520while%2520covering%2520136%2520m%2520on%2520average%2520in%2520a%2520challenging%2520environment%2520with%250Atransparent%2520and%2520reflective%2520obstacles.%2520We%2520also%2520compare%2520ultrasonic%2520and%250Alaser-based%2520ToF%2520sensing%2520techniques%2520for%2520obstacle%2520avoidance%252C%2520as%2520well%2520as%2520optical%250Aflow%2520and%2520ultrasonic-based%2520techniques%2520for%2520ego-velocity%2520estimation%252C%2520denoting%2520how%250Athese%2520systems%2520and%2520methods%2520can%2520be%2520complemented%2520to%2520enhance%2520the%2520robustness%2520of%250Anano-drone%2520operations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BatDeck%20--%20Ultra%20Low-power%20Ultrasonic%20Ego-velocity%20Estimation%20and%0A%20%20Obstacle%20Avoidance%20on%20Nano-drones&entry.906535625=Hanna%20M%C3%BCller%20and%20Victor%20Kartsch%20and%20Michele%20Magno%20and%20Luca%20Benini&entry.1292438233=%20%20Nano-drones%2C%20with%20their%20small%2C%20lightweight%20design%2C%20are%20ideal%20for%0Aconfined-space%20rescue%20missions%20and%20inherently%20safe%20for%20human%20interaction.%0AHowever%2C%20their%20limited%20payload%20restricts%20the%20critical%20sensing%20needed%20for%0Aego-velocity%20estimation%20and%20obstacle%20detection%20to%20single-bean%20laser-based%0Atime-of-flight%20%28ToF%29%20and%20low-resolution%20optical%20sensors.%20Although%20those%20sensors%0Ahave%20demonstrated%20good%20performance%2C%20they%20fail%20in%20some%20complex%20real-world%0Ascenarios%2C%20especially%20when%20facing%20transparent%20or%20reflective%20surfaces%20%28ToFs%29%20or%0Awhen%20lacking%20visual%20features%20%28optical-flow%20sensors%29.%20Taking%20inspiration%20from%0Abats%2C%20this%20paper%20proposes%20a%20novel%20two-way%20ranging-based%20method%20for%20ego-velocity%0Aestimation%20and%20obstacle%20avoidance%20based%20on%20down-and-forward%20facing%0Aultra-low-power%20ultrasonic%20sensors%2C%20which%20improve%20the%20performance%20when%20the%0Adrone%20faces%20reflective%20materials%20or%20navigates%20in%20complete%20darkness.%20Our%20results%0Ademonstrate%20that%20our%20new%20sensing%20system%20achieves%20a%20mean%20square%20error%20of%200.019%0Am/s%20on%20ego-velocity%20estimation%20and%20allows%20exploration%20for%20a%20flight%20time%20of%208%0Aminutes%20while%20covering%20136%20m%20on%20average%20in%20a%20challenging%20environment%20with%0Atransparent%20and%20reflective%20obstacles.%20We%20also%20compare%20ultrasonic%20and%0Alaser-based%20ToF%20sensing%20techniques%20for%20obstacle%20avoidance%2C%20as%20well%20as%20optical%0Aflow%20and%20ultrasonic-based%20techniques%20for%20ego-velocity%20estimation%2C%20denoting%20how%0Athese%20systems%20and%20methods%20can%20be%20complemented%20to%20enhance%20the%20robustness%20of%0Anano-drone%20operations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10048v1&entry.124074799=Read"},
{"title": "VibrantVS: A high-resolution multi-task transformer for forest canopy\n  height estimation", "author": "Tony Chang and Kiarie Ndegwa and Andreas Gros and Vincent A. Landau and Luke J. Zachmann and Bogdan State and Mitchell A. Gritts and Colton W. Miller and Nathan E. Rutenbeck and Scott Conway and Guy Bayes", "abstract": "  This paper explores the application of a novel multi-task vision transformer\n(ViT) model for the estimation of canopy height models (CHMs) using 4-band\nNational Agriculture Imagery Program (NAIP) imagery across the western United\nStates. We compare the effectiveness of this model in terms of accuracy and\nprecision aggregated across ecoregions and class heights versus three other\nbenchmark peer-reviewed models. Key findings suggest that, while other\nbenchmark models can provide high precision in localized areas, the VibrantVS\nmodel has substantial advantages across a broad reach of ecoregions in the\nwestern United States with higher accuracy, higher precision, the ability to\ngenerate updated inference at a cadence of three years or less, and high\nspatial resolution. The VibrantVS model provides significant value for\necological monitoring and land management decisions for wildfire mitigation.\n", "link": "http://arxiv.org/abs/2412.10351v1", "date": "2024-12-13", "relevancy": 1.9858, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5044}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5001}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VibrantVS%3A%20A%20high-resolution%20multi-task%20transformer%20for%20forest%20canopy%0A%20%20height%20estimation&body=Title%3A%20VibrantVS%3A%20A%20high-resolution%20multi-task%20transformer%20for%20forest%20canopy%0A%20%20height%20estimation%0AAuthor%3A%20Tony%20Chang%20and%20Kiarie%20Ndegwa%20and%20Andreas%20Gros%20and%20Vincent%20A.%20Landau%20and%20Luke%20J.%20Zachmann%20and%20Bogdan%20State%20and%20Mitchell%20A.%20Gritts%20and%20Colton%20W.%20Miller%20and%20Nathan%20E.%20Rutenbeck%20and%20Scott%20Conway%20and%20Guy%20Bayes%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20application%20of%20a%20novel%20multi-task%20vision%20transformer%0A%28ViT%29%20model%20for%20the%20estimation%20of%20canopy%20height%20models%20%28CHMs%29%20using%204-band%0ANational%20Agriculture%20Imagery%20Program%20%28NAIP%29%20imagery%20across%20the%20western%20United%0AStates.%20We%20compare%20the%20effectiveness%20of%20this%20model%20in%20terms%20of%20accuracy%20and%0Aprecision%20aggregated%20across%20ecoregions%20and%20class%20heights%20versus%20three%20other%0Abenchmark%20peer-reviewed%20models.%20Key%20findings%20suggest%20that%2C%20while%20other%0Abenchmark%20models%20can%20provide%20high%20precision%20in%20localized%20areas%2C%20the%20VibrantVS%0Amodel%20has%20substantial%20advantages%20across%20a%20broad%20reach%20of%20ecoregions%20in%20the%0Awestern%20United%20States%20with%20higher%20accuracy%2C%20higher%20precision%2C%20the%20ability%20to%0Agenerate%20updated%20inference%20at%20a%20cadence%20of%20three%20years%20or%20less%2C%20and%20high%0Aspatial%20resolution.%20The%20VibrantVS%20model%20provides%20significant%20value%20for%0Aecological%20monitoring%20and%20land%20management%20decisions%20for%20wildfire%20mitigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVibrantVS%253A%2520A%2520high-resolution%2520multi-task%2520transformer%2520for%2520forest%2520canopy%250A%2520%2520height%2520estimation%26entry.906535625%3DTony%2520Chang%2520and%2520Kiarie%2520Ndegwa%2520and%2520Andreas%2520Gros%2520and%2520Vincent%2520A.%2520Landau%2520and%2520Luke%2520J.%2520Zachmann%2520and%2520Bogdan%2520State%2520and%2520Mitchell%2520A.%2520Gritts%2520and%2520Colton%2520W.%2520Miller%2520and%2520Nathan%2520E.%2520Rutenbeck%2520and%2520Scott%2520Conway%2520and%2520Guy%2520Bayes%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520application%2520of%2520a%2520novel%2520multi-task%2520vision%2520transformer%250A%2528ViT%2529%2520model%2520for%2520the%2520estimation%2520of%2520canopy%2520height%2520models%2520%2528CHMs%2529%2520using%25204-band%250ANational%2520Agriculture%2520Imagery%2520Program%2520%2528NAIP%2529%2520imagery%2520across%2520the%2520western%2520United%250AStates.%2520We%2520compare%2520the%2520effectiveness%2520of%2520this%2520model%2520in%2520terms%2520of%2520accuracy%2520and%250Aprecision%2520aggregated%2520across%2520ecoregions%2520and%2520class%2520heights%2520versus%2520three%2520other%250Abenchmark%2520peer-reviewed%2520models.%2520Key%2520findings%2520suggest%2520that%252C%2520while%2520other%250Abenchmark%2520models%2520can%2520provide%2520high%2520precision%2520in%2520localized%2520areas%252C%2520the%2520VibrantVS%250Amodel%2520has%2520substantial%2520advantages%2520across%2520a%2520broad%2520reach%2520of%2520ecoregions%2520in%2520the%250Awestern%2520United%2520States%2520with%2520higher%2520accuracy%252C%2520higher%2520precision%252C%2520the%2520ability%2520to%250Agenerate%2520updated%2520inference%2520at%2520a%2520cadence%2520of%2520three%2520years%2520or%2520less%252C%2520and%2520high%250Aspatial%2520resolution.%2520The%2520VibrantVS%2520model%2520provides%2520significant%2520value%2520for%250Aecological%2520monitoring%2520and%2520land%2520management%2520decisions%2520for%2520wildfire%2520mitigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VibrantVS%3A%20A%20high-resolution%20multi-task%20transformer%20for%20forest%20canopy%0A%20%20height%20estimation&entry.906535625=Tony%20Chang%20and%20Kiarie%20Ndegwa%20and%20Andreas%20Gros%20and%20Vincent%20A.%20Landau%20and%20Luke%20J.%20Zachmann%20and%20Bogdan%20State%20and%20Mitchell%20A.%20Gritts%20and%20Colton%20W.%20Miller%20and%20Nathan%20E.%20Rutenbeck%20and%20Scott%20Conway%20and%20Guy%20Bayes&entry.1292438233=%20%20This%20paper%20explores%20the%20application%20of%20a%20novel%20multi-task%20vision%20transformer%0A%28ViT%29%20model%20for%20the%20estimation%20of%20canopy%20height%20models%20%28CHMs%29%20using%204-band%0ANational%20Agriculture%20Imagery%20Program%20%28NAIP%29%20imagery%20across%20the%20western%20United%0AStates.%20We%20compare%20the%20effectiveness%20of%20this%20model%20in%20terms%20of%20accuracy%20and%0Aprecision%20aggregated%20across%20ecoregions%20and%20class%20heights%20versus%20three%20other%0Abenchmark%20peer-reviewed%20models.%20Key%20findings%20suggest%20that%2C%20while%20other%0Abenchmark%20models%20can%20provide%20high%20precision%20in%20localized%20areas%2C%20the%20VibrantVS%0Amodel%20has%20substantial%20advantages%20across%20a%20broad%20reach%20of%20ecoregions%20in%20the%0Awestern%20United%20States%20with%20higher%20accuracy%2C%20higher%20precision%2C%20the%20ability%20to%0Agenerate%20updated%20inference%20at%20a%20cadence%20of%20three%20years%20or%20less%2C%20and%20high%0Aspatial%20resolution.%20The%20VibrantVS%20model%20provides%20significant%20value%20for%0Aecological%20monitoring%20and%20land%20management%20decisions%20for%20wildfire%20mitigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10351v1&entry.124074799=Read"},
{"title": "Text2Cypher: Bridging Natural Language and Graph Databases", "author": "Makbule Gulcin Ozsoy and Leila Messallem and Jon Besga and Gianandrea Minneci", "abstract": "  Knowledge graphs use nodes, relationships, and properties to represent\narbitrarily complex data. When stored in a graph database, the Cypher query\nlanguage enables efficient modeling and querying of knowledge graphs. However,\nusing Cypher requires specialized knowledge, which can present a challenge for\nnon-expert users. Our work Text2Cypher aims to bridge this gap by translating\nnatural language queries into Cypher query language and extending the utility\nof knowledge graphs to non-technical expert users.\n  While large language models (LLMs) can be used for this purpose, they often\nstruggle to capture complex nuances, resulting in incomplete or incorrect\noutputs. Fine-tuning LLMs on domain-specific datasets has proven to be a more\npromising approach, but the limited availability of high-quality, publicly\navailable Text2Cypher datasets makes this challenging. In this work, we show\nhow we combined, cleaned and organized several publicly available datasets into\na total of 44,387 instances, enabling effective fine-tuning and evaluation.\nModels fine-tuned on this dataset showed significant performance gains, with\nimprovements in Google-BLEU and Exact Match scores over baseline models,\nhighlighting the importance of high-quality datasets and fine-tuning in\nimproving Text2Cypher performance.\n", "link": "http://arxiv.org/abs/2412.10064v1", "date": "2024-12-13", "relevancy": 1.9841, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2Cypher%3A%20Bridging%20Natural%20Language%20and%20Graph%20Databases&body=Title%3A%20Text2Cypher%3A%20Bridging%20Natural%20Language%20and%20Graph%20Databases%0AAuthor%3A%20Makbule%20Gulcin%20Ozsoy%20and%20Leila%20Messallem%20and%20Jon%20Besga%20and%20Gianandrea%20Minneci%0AAbstract%3A%20%20%20Knowledge%20graphs%20use%20nodes%2C%20relationships%2C%20and%20properties%20to%20represent%0Aarbitrarily%20complex%20data.%20When%20stored%20in%20a%20graph%20database%2C%20the%20Cypher%20query%0Alanguage%20enables%20efficient%20modeling%20and%20querying%20of%20knowledge%20graphs.%20However%2C%0Ausing%20Cypher%20requires%20specialized%20knowledge%2C%20which%20can%20present%20a%20challenge%20for%0Anon-expert%20users.%20Our%20work%20Text2Cypher%20aims%20to%20bridge%20this%20gap%20by%20translating%0Anatural%20language%20queries%20into%20Cypher%20query%20language%20and%20extending%20the%20utility%0Aof%20knowledge%20graphs%20to%20non-technical%20expert%20users.%0A%20%20While%20large%20language%20models%20%28LLMs%29%20can%20be%20used%20for%20this%20purpose%2C%20they%20often%0Astruggle%20to%20capture%20complex%20nuances%2C%20resulting%20in%20incomplete%20or%20incorrect%0Aoutputs.%20Fine-tuning%20LLMs%20on%20domain-specific%20datasets%20has%20proven%20to%20be%20a%20more%0Apromising%20approach%2C%20but%20the%20limited%20availability%20of%20high-quality%2C%20publicly%0Aavailable%20Text2Cypher%20datasets%20makes%20this%20challenging.%20In%20this%20work%2C%20we%20show%0Ahow%20we%20combined%2C%20cleaned%20and%20organized%20several%20publicly%20available%20datasets%20into%0Aa%20total%20of%2044%2C387%20instances%2C%20enabling%20effective%20fine-tuning%20and%20evaluation.%0AModels%20fine-tuned%20on%20this%20dataset%20showed%20significant%20performance%20gains%2C%20with%0Aimprovements%20in%20Google-BLEU%20and%20Exact%20Match%20scores%20over%20baseline%20models%2C%0Ahighlighting%20the%20importance%20of%20high-quality%20datasets%20and%20fine-tuning%20in%0Aimproving%20Text2Cypher%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10064v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2Cypher%253A%2520Bridging%2520Natural%2520Language%2520and%2520Graph%2520Databases%26entry.906535625%3DMakbule%2520Gulcin%2520Ozsoy%2520and%2520Leila%2520Messallem%2520and%2520Jon%2520Besga%2520and%2520Gianandrea%2520Minneci%26entry.1292438233%3D%2520%2520Knowledge%2520graphs%2520use%2520nodes%252C%2520relationships%252C%2520and%2520properties%2520to%2520represent%250Aarbitrarily%2520complex%2520data.%2520When%2520stored%2520in%2520a%2520graph%2520database%252C%2520the%2520Cypher%2520query%250Alanguage%2520enables%2520efficient%2520modeling%2520and%2520querying%2520of%2520knowledge%2520graphs.%2520However%252C%250Ausing%2520Cypher%2520requires%2520specialized%2520knowledge%252C%2520which%2520can%2520present%2520a%2520challenge%2520for%250Anon-expert%2520users.%2520Our%2520work%2520Text2Cypher%2520aims%2520to%2520bridge%2520this%2520gap%2520by%2520translating%250Anatural%2520language%2520queries%2520into%2520Cypher%2520query%2520language%2520and%2520extending%2520the%2520utility%250Aof%2520knowledge%2520graphs%2520to%2520non-technical%2520expert%2520users.%250A%2520%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520can%2520be%2520used%2520for%2520this%2520purpose%252C%2520they%2520often%250Astruggle%2520to%2520capture%2520complex%2520nuances%252C%2520resulting%2520in%2520incomplete%2520or%2520incorrect%250Aoutputs.%2520Fine-tuning%2520LLMs%2520on%2520domain-specific%2520datasets%2520has%2520proven%2520to%2520be%2520a%2520more%250Apromising%2520approach%252C%2520but%2520the%2520limited%2520availability%2520of%2520high-quality%252C%2520publicly%250Aavailable%2520Text2Cypher%2520datasets%2520makes%2520this%2520challenging.%2520In%2520this%2520work%252C%2520we%2520show%250Ahow%2520we%2520combined%252C%2520cleaned%2520and%2520organized%2520several%2520publicly%2520available%2520datasets%2520into%250Aa%2520total%2520of%252044%252C387%2520instances%252C%2520enabling%2520effective%2520fine-tuning%2520and%2520evaluation.%250AModels%2520fine-tuned%2520on%2520this%2520dataset%2520showed%2520significant%2520performance%2520gains%252C%2520with%250Aimprovements%2520in%2520Google-BLEU%2520and%2520Exact%2520Match%2520scores%2520over%2520baseline%2520models%252C%250Ahighlighting%2520the%2520importance%2520of%2520high-quality%2520datasets%2520and%2520fine-tuning%2520in%250Aimproving%2520Text2Cypher%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10064v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2Cypher%3A%20Bridging%20Natural%20Language%20and%20Graph%20Databases&entry.906535625=Makbule%20Gulcin%20Ozsoy%20and%20Leila%20Messallem%20and%20Jon%20Besga%20and%20Gianandrea%20Minneci&entry.1292438233=%20%20Knowledge%20graphs%20use%20nodes%2C%20relationships%2C%20and%20properties%20to%20represent%0Aarbitrarily%20complex%20data.%20When%20stored%20in%20a%20graph%20database%2C%20the%20Cypher%20query%0Alanguage%20enables%20efficient%20modeling%20and%20querying%20of%20knowledge%20graphs.%20However%2C%0Ausing%20Cypher%20requires%20specialized%20knowledge%2C%20which%20can%20present%20a%20challenge%20for%0Anon-expert%20users.%20Our%20work%20Text2Cypher%20aims%20to%20bridge%20this%20gap%20by%20translating%0Anatural%20language%20queries%20into%20Cypher%20query%20language%20and%20extending%20the%20utility%0Aof%20knowledge%20graphs%20to%20non-technical%20expert%20users.%0A%20%20While%20large%20language%20models%20%28LLMs%29%20can%20be%20used%20for%20this%20purpose%2C%20they%20often%0Astruggle%20to%20capture%20complex%20nuances%2C%20resulting%20in%20incomplete%20or%20incorrect%0Aoutputs.%20Fine-tuning%20LLMs%20on%20domain-specific%20datasets%20has%20proven%20to%20be%20a%20more%0Apromising%20approach%2C%20but%20the%20limited%20availability%20of%20high-quality%2C%20publicly%0Aavailable%20Text2Cypher%20datasets%20makes%20this%20challenging.%20In%20this%20work%2C%20we%20show%0Ahow%20we%20combined%2C%20cleaned%20and%20organized%20several%20publicly%20available%20datasets%20into%0Aa%20total%20of%2044%2C387%20instances%2C%20enabling%20effective%20fine-tuning%20and%20evaluation.%0AModels%20fine-tuned%20on%20this%20dataset%20showed%20significant%20performance%20gains%2C%20with%0Aimprovements%20in%20Google-BLEU%20and%20Exact%20Match%20scores%20over%20baseline%20models%2C%0Ahighlighting%20the%20importance%20of%20high-quality%20datasets%20and%20fine-tuning%20in%0Aimproving%20Text2Cypher%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10064v1&entry.124074799=Read"},
{"title": "Copy-Move Detection in Optical Microscopy: A Segmentation Network and A\n  Dataset", "author": "Hao-Chiang Shao and Yuan-Rong Liao and Tse-Yu Tseng and Yen-Liang Chuo and Fong-Yi Lin", "abstract": "  With increasing revelations of academic fraud, detecting forged experimental\nimages in the biomedical field has become a public concern. The challenge lies\nin the fact that copy-move targets can include background tissue, small\nforeground objects, or both, which may be out of the training domain and\nsubject to unseen attacks, rendering standard object-detection-based approaches\nless effective. To address this, we reformulate the problem of detecting\nbiomedical copy-move forgery regions as an intra-image co-saliency detection\ntask and propose CMSeg-Net, a copy-move forgery segmentation network capable of\nidentifying unseen duplicated areas. Built on a multi-resolution\nencoder-decoder architecture, CMSeg-Net incorporates self-correlation and\ncorrelation-assisted spatial-attention modules to detect intra-image regional\nsimilarities within feature tensors at each observation scale. This design\nhelps distinguish even small copy-move targets in complex microscopic images\nfrom other similar objects. Furthermore, we created a copy-move forgery dataset\nof optical microscopic images, named FakeParaEgg, using open data from the ICIP\n2022 Challenge to support CMSeg-Net's development and verify its performance.\nExtensive experiments demonstrate that our approach outperforms previous\nstate-of-the-art methods on the FakeParaEgg dataset and other open copy-move\ndetection datasets, including CASIA-CMFD, CoMoFoD, and CMF. The FakeParaEgg\ndataset, our source code, and the CMF dataset with our manually defined\nsegmentation ground truths available at\n``https://github.com/YoursEver/FakeParaEgg''.\n", "link": "http://arxiv.org/abs/2412.10258v1", "date": "2024-12-13", "relevancy": 1.9821, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5039}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4905}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Copy-Move%20Detection%20in%20Optical%20Microscopy%3A%20A%20Segmentation%20Network%20and%20A%0A%20%20Dataset&body=Title%3A%20Copy-Move%20Detection%20in%20Optical%20Microscopy%3A%20A%20Segmentation%20Network%20and%20A%0A%20%20Dataset%0AAuthor%3A%20Hao-Chiang%20Shao%20and%20Yuan-Rong%20Liao%20and%20Tse-Yu%20Tseng%20and%20Yen-Liang%20Chuo%20and%20Fong-Yi%20Lin%0AAbstract%3A%20%20%20With%20increasing%20revelations%20of%20academic%20fraud%2C%20detecting%20forged%20experimental%0Aimages%20in%20the%20biomedical%20field%20has%20become%20a%20public%20concern.%20The%20challenge%20lies%0Ain%20the%20fact%20that%20copy-move%20targets%20can%20include%20background%20tissue%2C%20small%0Aforeground%20objects%2C%20or%20both%2C%20which%20may%20be%20out%20of%20the%20training%20domain%20and%0Asubject%20to%20unseen%20attacks%2C%20rendering%20standard%20object-detection-based%20approaches%0Aless%20effective.%20To%20address%20this%2C%20we%20reformulate%20the%20problem%20of%20detecting%0Abiomedical%20copy-move%20forgery%20regions%20as%20an%20intra-image%20co-saliency%20detection%0Atask%20and%20propose%20CMSeg-Net%2C%20a%20copy-move%20forgery%20segmentation%20network%20capable%20of%0Aidentifying%20unseen%20duplicated%20areas.%20Built%20on%20a%20multi-resolution%0Aencoder-decoder%20architecture%2C%20CMSeg-Net%20incorporates%20self-correlation%20and%0Acorrelation-assisted%20spatial-attention%20modules%20to%20detect%20intra-image%20regional%0Asimilarities%20within%20feature%20tensors%20at%20each%20observation%20scale.%20This%20design%0Ahelps%20distinguish%20even%20small%20copy-move%20targets%20in%20complex%20microscopic%20images%0Afrom%20other%20similar%20objects.%20Furthermore%2C%20we%20created%20a%20copy-move%20forgery%20dataset%0Aof%20optical%20microscopic%20images%2C%20named%20FakeParaEgg%2C%20using%20open%20data%20from%20the%20ICIP%0A2022%20Challenge%20to%20support%20CMSeg-Net%27s%20development%20and%20verify%20its%20performance.%0AExtensive%20experiments%20demonstrate%20that%20our%20approach%20outperforms%20previous%0Astate-of-the-art%20methods%20on%20the%20FakeParaEgg%20dataset%20and%20other%20open%20copy-move%0Adetection%20datasets%2C%20including%20CASIA-CMFD%2C%20CoMoFoD%2C%20and%20CMF.%20The%20FakeParaEgg%0Adataset%2C%20our%20source%20code%2C%20and%20the%20CMF%20dataset%20with%20our%20manually%20defined%0Asegmentation%20ground%20truths%20available%20at%0A%60%60https%3A//github.com/YoursEver/FakeParaEgg%27%27.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCopy-Move%2520Detection%2520in%2520Optical%2520Microscopy%253A%2520A%2520Segmentation%2520Network%2520and%2520A%250A%2520%2520Dataset%26entry.906535625%3DHao-Chiang%2520Shao%2520and%2520Yuan-Rong%2520Liao%2520and%2520Tse-Yu%2520Tseng%2520and%2520Yen-Liang%2520Chuo%2520and%2520Fong-Yi%2520Lin%26entry.1292438233%3D%2520%2520With%2520increasing%2520revelations%2520of%2520academic%2520fraud%252C%2520detecting%2520forged%2520experimental%250Aimages%2520in%2520the%2520biomedical%2520field%2520has%2520become%2520a%2520public%2520concern.%2520The%2520challenge%2520lies%250Ain%2520the%2520fact%2520that%2520copy-move%2520targets%2520can%2520include%2520background%2520tissue%252C%2520small%250Aforeground%2520objects%252C%2520or%2520both%252C%2520which%2520may%2520be%2520out%2520of%2520the%2520training%2520domain%2520and%250Asubject%2520to%2520unseen%2520attacks%252C%2520rendering%2520standard%2520object-detection-based%2520approaches%250Aless%2520effective.%2520To%2520address%2520this%252C%2520we%2520reformulate%2520the%2520problem%2520of%2520detecting%250Abiomedical%2520copy-move%2520forgery%2520regions%2520as%2520an%2520intra-image%2520co-saliency%2520detection%250Atask%2520and%2520propose%2520CMSeg-Net%252C%2520a%2520copy-move%2520forgery%2520segmentation%2520network%2520capable%2520of%250Aidentifying%2520unseen%2520duplicated%2520areas.%2520Built%2520on%2520a%2520multi-resolution%250Aencoder-decoder%2520architecture%252C%2520CMSeg-Net%2520incorporates%2520self-correlation%2520and%250Acorrelation-assisted%2520spatial-attention%2520modules%2520to%2520detect%2520intra-image%2520regional%250Asimilarities%2520within%2520feature%2520tensors%2520at%2520each%2520observation%2520scale.%2520This%2520design%250Ahelps%2520distinguish%2520even%2520small%2520copy-move%2520targets%2520in%2520complex%2520microscopic%2520images%250Afrom%2520other%2520similar%2520objects.%2520Furthermore%252C%2520we%2520created%2520a%2520copy-move%2520forgery%2520dataset%250Aof%2520optical%2520microscopic%2520images%252C%2520named%2520FakeParaEgg%252C%2520using%2520open%2520data%2520from%2520the%2520ICIP%250A2022%2520Challenge%2520to%2520support%2520CMSeg-Net%2527s%2520development%2520and%2520verify%2520its%2520performance.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520previous%250Astate-of-the-art%2520methods%2520on%2520the%2520FakeParaEgg%2520dataset%2520and%2520other%2520open%2520copy-move%250Adetection%2520datasets%252C%2520including%2520CASIA-CMFD%252C%2520CoMoFoD%252C%2520and%2520CMF.%2520The%2520FakeParaEgg%250Adataset%252C%2520our%2520source%2520code%252C%2520and%2520the%2520CMF%2520dataset%2520with%2520our%2520manually%2520defined%250Asegmentation%2520ground%2520truths%2520available%2520at%250A%2560%2560https%253A//github.com/YoursEver/FakeParaEgg%2527%2527.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Copy-Move%20Detection%20in%20Optical%20Microscopy%3A%20A%20Segmentation%20Network%20and%20A%0A%20%20Dataset&entry.906535625=Hao-Chiang%20Shao%20and%20Yuan-Rong%20Liao%20and%20Tse-Yu%20Tseng%20and%20Yen-Liang%20Chuo%20and%20Fong-Yi%20Lin&entry.1292438233=%20%20With%20increasing%20revelations%20of%20academic%20fraud%2C%20detecting%20forged%20experimental%0Aimages%20in%20the%20biomedical%20field%20has%20become%20a%20public%20concern.%20The%20challenge%20lies%0Ain%20the%20fact%20that%20copy-move%20targets%20can%20include%20background%20tissue%2C%20small%0Aforeground%20objects%2C%20or%20both%2C%20which%20may%20be%20out%20of%20the%20training%20domain%20and%0Asubject%20to%20unseen%20attacks%2C%20rendering%20standard%20object-detection-based%20approaches%0Aless%20effective.%20To%20address%20this%2C%20we%20reformulate%20the%20problem%20of%20detecting%0Abiomedical%20copy-move%20forgery%20regions%20as%20an%20intra-image%20co-saliency%20detection%0Atask%20and%20propose%20CMSeg-Net%2C%20a%20copy-move%20forgery%20segmentation%20network%20capable%20of%0Aidentifying%20unseen%20duplicated%20areas.%20Built%20on%20a%20multi-resolution%0Aencoder-decoder%20architecture%2C%20CMSeg-Net%20incorporates%20self-correlation%20and%0Acorrelation-assisted%20spatial-attention%20modules%20to%20detect%20intra-image%20regional%0Asimilarities%20within%20feature%20tensors%20at%20each%20observation%20scale.%20This%20design%0Ahelps%20distinguish%20even%20small%20copy-move%20targets%20in%20complex%20microscopic%20images%0Afrom%20other%20similar%20objects.%20Furthermore%2C%20we%20created%20a%20copy-move%20forgery%20dataset%0Aof%20optical%20microscopic%20images%2C%20named%20FakeParaEgg%2C%20using%20open%20data%20from%20the%20ICIP%0A2022%20Challenge%20to%20support%20CMSeg-Net%27s%20development%20and%20verify%20its%20performance.%0AExtensive%20experiments%20demonstrate%20that%20our%20approach%20outperforms%20previous%0Astate-of-the-art%20methods%20on%20the%20FakeParaEgg%20dataset%20and%20other%20open%20copy-move%0Adetection%20datasets%2C%20including%20CASIA-CMFD%2C%20CoMoFoD%2C%20and%20CMF.%20The%20FakeParaEgg%0Adataset%2C%20our%20source%20code%2C%20and%20the%20CMF%20dataset%20with%20our%20manually%20defined%0Asegmentation%20ground%20truths%20available%20at%0A%60%60https%3A//github.com/YoursEver/FakeParaEgg%27%27.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10258v1&entry.124074799=Read"},
{"title": "Geometric sparsification in recurrent neural networks", "author": "Wyatt Mackey and Ioannis Schizas and Jared Deighton and David L. Boothe, Jr. and Vasileios Maroulas", "abstract": "  A common technique for ameliorating the computational costs of running large\nneural models is sparsification, or the pruning of neural connections during\ntraining. Sparse models are capable of maintaining the high accuracy of state\nof the art models, while functioning at the cost of more parsimonious models.\nThe structures which underlie sparse architectures are, however, poorly\nunderstood and not consistent between differently trained models and\nsparsification schemes. In this paper, we propose a new technique for\nsparsification of recurrent neural nets (RNNs), called moduli regularization,\nin combination with magnitude pruning. Moduli regularization leverages the\ndynamical system induced by the recurrent structure to induce a geometric\nrelationship between neurons in the hidden state of the RNN. By making our\nregularizing term explicitly geometric, we provide the first, to our knowledge,\na priori description of the desired sparse architecture of our neural net, as\nwell as explicit end-to-end learning of RNN geometry. We verify the\neffectiveness of our scheme under diverse conditions, testing in navigation,\nnatural language processing, and addition RNNs. Navigation is a structurally\ngeometric task, for which there are known moduli spaces, and we show that\nregularization can be used to reach 90% sparsity while maintaining model\nperformance only when coefficients are chosen in accordance with a suitable\nmoduli space. Natural language processing and addition, however, have no known\nmoduli space in which computations are performed. Nevertheless, we show that\nmoduli regularization induces more stable recurrent neural nets, and achieves\nhigh fidelity models above 90% sparsity.\n", "link": "http://arxiv.org/abs/2406.06290v2", "date": "2024-12-13", "relevancy": 1.9793, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.507}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4861}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20sparsification%20in%20recurrent%20neural%20networks&body=Title%3A%20Geometric%20sparsification%20in%20recurrent%20neural%20networks%0AAuthor%3A%20Wyatt%20Mackey%20and%20Ioannis%20Schizas%20and%20Jared%20Deighton%20and%20David%20L.%20Boothe%2C%20Jr.%20and%20Vasileios%20Maroulas%0AAbstract%3A%20%20%20A%20common%20technique%20for%20ameliorating%20the%20computational%20costs%20of%20running%20large%0Aneural%20models%20is%20sparsification%2C%20or%20the%20pruning%20of%20neural%20connections%20during%0Atraining.%20Sparse%20models%20are%20capable%20of%20maintaining%20the%20high%20accuracy%20of%20state%0Aof%20the%20art%20models%2C%20while%20functioning%20at%20the%20cost%20of%20more%20parsimonious%20models.%0AThe%20structures%20which%20underlie%20sparse%20architectures%20are%2C%20however%2C%20poorly%0Aunderstood%20and%20not%20consistent%20between%20differently%20trained%20models%20and%0Asparsification%20schemes.%20In%20this%20paper%2C%20we%20propose%20a%20new%20technique%20for%0Asparsification%20of%20recurrent%20neural%20nets%20%28RNNs%29%2C%20called%20moduli%20regularization%2C%0Ain%20combination%20with%20magnitude%20pruning.%20Moduli%20regularization%20leverages%20the%0Adynamical%20system%20induced%20by%20the%20recurrent%20structure%20to%20induce%20a%20geometric%0Arelationship%20between%20neurons%20in%20the%20hidden%20state%20of%20the%20RNN.%20By%20making%20our%0Aregularizing%20term%20explicitly%20geometric%2C%20we%20provide%20the%20first%2C%20to%20our%20knowledge%2C%0Aa%20priori%20description%20of%20the%20desired%20sparse%20architecture%20of%20our%20neural%20net%2C%20as%0Awell%20as%20explicit%20end-to-end%20learning%20of%20RNN%20geometry.%20We%20verify%20the%0Aeffectiveness%20of%20our%20scheme%20under%20diverse%20conditions%2C%20testing%20in%20navigation%2C%0Anatural%20language%20processing%2C%20and%20addition%20RNNs.%20Navigation%20is%20a%20structurally%0Ageometric%20task%2C%20for%20which%20there%20are%20known%20moduli%20spaces%2C%20and%20we%20show%20that%0Aregularization%20can%20be%20used%20to%20reach%2090%25%20sparsity%20while%20maintaining%20model%0Aperformance%20only%20when%20coefficients%20are%20chosen%20in%20accordance%20with%20a%20suitable%0Amoduli%20space.%20Natural%20language%20processing%20and%20addition%2C%20however%2C%20have%20no%20known%0Amoduli%20space%20in%20which%20computations%20are%20performed.%20Nevertheless%2C%20we%20show%20that%0Amoduli%20regularization%20induces%20more%20stable%20recurrent%20neural%20nets%2C%20and%20achieves%0Ahigh%20fidelity%20models%20above%2090%25%20sparsity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06290v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520sparsification%2520in%2520recurrent%2520neural%2520networks%26entry.906535625%3DWyatt%2520Mackey%2520and%2520Ioannis%2520Schizas%2520and%2520Jared%2520Deighton%2520and%2520David%2520L.%2520Boothe%252C%2520Jr.%2520and%2520Vasileios%2520Maroulas%26entry.1292438233%3D%2520%2520A%2520common%2520technique%2520for%2520ameliorating%2520the%2520computational%2520costs%2520of%2520running%2520large%250Aneural%2520models%2520is%2520sparsification%252C%2520or%2520the%2520pruning%2520of%2520neural%2520connections%2520during%250Atraining.%2520Sparse%2520models%2520are%2520capable%2520of%2520maintaining%2520the%2520high%2520accuracy%2520of%2520state%250Aof%2520the%2520art%2520models%252C%2520while%2520functioning%2520at%2520the%2520cost%2520of%2520more%2520parsimonious%2520models.%250AThe%2520structures%2520which%2520underlie%2520sparse%2520architectures%2520are%252C%2520however%252C%2520poorly%250Aunderstood%2520and%2520not%2520consistent%2520between%2520differently%2520trained%2520models%2520and%250Asparsification%2520schemes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520technique%2520for%250Asparsification%2520of%2520recurrent%2520neural%2520nets%2520%2528RNNs%2529%252C%2520called%2520moduli%2520regularization%252C%250Ain%2520combination%2520with%2520magnitude%2520pruning.%2520Moduli%2520regularization%2520leverages%2520the%250Adynamical%2520system%2520induced%2520by%2520the%2520recurrent%2520structure%2520to%2520induce%2520a%2520geometric%250Arelationship%2520between%2520neurons%2520in%2520the%2520hidden%2520state%2520of%2520the%2520RNN.%2520By%2520making%2520our%250Aregularizing%2520term%2520explicitly%2520geometric%252C%2520we%2520provide%2520the%2520first%252C%2520to%2520our%2520knowledge%252C%250Aa%2520priori%2520description%2520of%2520the%2520desired%2520sparse%2520architecture%2520of%2520our%2520neural%2520net%252C%2520as%250Awell%2520as%2520explicit%2520end-to-end%2520learning%2520of%2520RNN%2520geometry.%2520We%2520verify%2520the%250Aeffectiveness%2520of%2520our%2520scheme%2520under%2520diverse%2520conditions%252C%2520testing%2520in%2520navigation%252C%250Anatural%2520language%2520processing%252C%2520and%2520addition%2520RNNs.%2520Navigation%2520is%2520a%2520structurally%250Ageometric%2520task%252C%2520for%2520which%2520there%2520are%2520known%2520moduli%2520spaces%252C%2520and%2520we%2520show%2520that%250Aregularization%2520can%2520be%2520used%2520to%2520reach%252090%2525%2520sparsity%2520while%2520maintaining%2520model%250Aperformance%2520only%2520when%2520coefficients%2520are%2520chosen%2520in%2520accordance%2520with%2520a%2520suitable%250Amoduli%2520space.%2520Natural%2520language%2520processing%2520and%2520addition%252C%2520however%252C%2520have%2520no%2520known%250Amoduli%2520space%2520in%2520which%2520computations%2520are%2520performed.%2520Nevertheless%252C%2520we%2520show%2520that%250Amoduli%2520regularization%2520induces%2520more%2520stable%2520recurrent%2520neural%2520nets%252C%2520and%2520achieves%250Ahigh%2520fidelity%2520models%2520above%252090%2525%2520sparsity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06290v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20sparsification%20in%20recurrent%20neural%20networks&entry.906535625=Wyatt%20Mackey%20and%20Ioannis%20Schizas%20and%20Jared%20Deighton%20and%20David%20L.%20Boothe%2C%20Jr.%20and%20Vasileios%20Maroulas&entry.1292438233=%20%20A%20common%20technique%20for%20ameliorating%20the%20computational%20costs%20of%20running%20large%0Aneural%20models%20is%20sparsification%2C%20or%20the%20pruning%20of%20neural%20connections%20during%0Atraining.%20Sparse%20models%20are%20capable%20of%20maintaining%20the%20high%20accuracy%20of%20state%0Aof%20the%20art%20models%2C%20while%20functioning%20at%20the%20cost%20of%20more%20parsimonious%20models.%0AThe%20structures%20which%20underlie%20sparse%20architectures%20are%2C%20however%2C%20poorly%0Aunderstood%20and%20not%20consistent%20between%20differently%20trained%20models%20and%0Asparsification%20schemes.%20In%20this%20paper%2C%20we%20propose%20a%20new%20technique%20for%0Asparsification%20of%20recurrent%20neural%20nets%20%28RNNs%29%2C%20called%20moduli%20regularization%2C%0Ain%20combination%20with%20magnitude%20pruning.%20Moduli%20regularization%20leverages%20the%0Adynamical%20system%20induced%20by%20the%20recurrent%20structure%20to%20induce%20a%20geometric%0Arelationship%20between%20neurons%20in%20the%20hidden%20state%20of%20the%20RNN.%20By%20making%20our%0Aregularizing%20term%20explicitly%20geometric%2C%20we%20provide%20the%20first%2C%20to%20our%20knowledge%2C%0Aa%20priori%20description%20of%20the%20desired%20sparse%20architecture%20of%20our%20neural%20net%2C%20as%0Awell%20as%20explicit%20end-to-end%20learning%20of%20RNN%20geometry.%20We%20verify%20the%0Aeffectiveness%20of%20our%20scheme%20under%20diverse%20conditions%2C%20testing%20in%20navigation%2C%0Anatural%20language%20processing%2C%20and%20addition%20RNNs.%20Navigation%20is%20a%20structurally%0Ageometric%20task%2C%20for%20which%20there%20are%20known%20moduli%20spaces%2C%20and%20we%20show%20that%0Aregularization%20can%20be%20used%20to%20reach%2090%25%20sparsity%20while%20maintaining%20model%0Aperformance%20only%20when%20coefficients%20are%20chosen%20in%20accordance%20with%20a%20suitable%0Amoduli%20space.%20Natural%20language%20processing%20and%20addition%2C%20however%2C%20have%20no%20known%0Amoduli%20space%20in%20which%20computations%20are%20performed.%20Nevertheless%2C%20we%20show%20that%0Amoduli%20regularization%20induces%20more%20stable%20recurrent%20neural%20nets%2C%20and%20achieves%0Ahigh%20fidelity%20models%20above%2090%25%20sparsity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06290v2&entry.124074799=Read"},
{"title": "IGNITE: Individualized GeNeration of Imputations in Time-series\n  Electronic health records", "author": "Ghadeer O. Ghosheh and Jin Li and Tingting Zhu", "abstract": "  Electronic Health Records present a valuable modality for driving\npersonalized medicine, where treatment is tailored to fit individual-level\ndifferences. For this purpose, many data-driven machine learning and\nstatistical models rely on the wealth of longitudinal EHRs to study patients'\nphysiological and treatment effects. However, longitudinal EHRs tend to be\nsparse and highly missing, where missingness could also be informative and\nreflect the underlying patient's health status. Therefore, the success of\ndata-driven models for personalized medicine highly depends on how the EHR data\nis represented from physiological data, treatments, and the missing values in\nthe data. To this end, we propose a novel deep-learning model that learns the\nunderlying patient dynamics over time across multivariate data to generate\npersonalized realistic values conditioning on an individual's demographic\ncharacteristics and treatments. Our proposed model, IGNITE (Individualized\nGeNeration of Imputations in Time-series Electronic health records), utilises a\nconditional dual-variational autoencoder augmented with dual-stage attention to\ngenerate missing values for an individual. In IGNITE, we further propose a\nnovel individualized missingness mask (IMM), which helps our model generate\nvalues based on the individual's observed data and missingness patterns. We\nfurther extend the use of IGNITE from imputing missingness to a personalized\ndata synthesizer, where it generates missing EHRs that were never observed\nprior or even generates new patients for various applications. We validate our\nmodel on three large publicly available datasets and show that IGNITE\noutperforms state-of-the-art approaches in missing data reconstruction and task\nprediction.\n", "link": "http://arxiv.org/abs/2401.04402v2", "date": "2024-12-13", "relevancy": 1.9785, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5016}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4947}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IGNITE%3A%20Individualized%20GeNeration%20of%20Imputations%20in%20Time-series%0A%20%20Electronic%20health%20records&body=Title%3A%20IGNITE%3A%20Individualized%20GeNeration%20of%20Imputations%20in%20Time-series%0A%20%20Electronic%20health%20records%0AAuthor%3A%20Ghadeer%20O.%20Ghosheh%20and%20Jin%20Li%20and%20Tingting%20Zhu%0AAbstract%3A%20%20%20Electronic%20Health%20Records%20present%20a%20valuable%20modality%20for%20driving%0Apersonalized%20medicine%2C%20where%20treatment%20is%20tailored%20to%20fit%20individual-level%0Adifferences.%20For%20this%20purpose%2C%20many%20data-driven%20machine%20learning%20and%0Astatistical%20models%20rely%20on%20the%20wealth%20of%20longitudinal%20EHRs%20to%20study%20patients%27%0Aphysiological%20and%20treatment%20effects.%20However%2C%20longitudinal%20EHRs%20tend%20to%20be%0Asparse%20and%20highly%20missing%2C%20where%20missingness%20could%20also%20be%20informative%20and%0Areflect%20the%20underlying%20patient%27s%20health%20status.%20Therefore%2C%20the%20success%20of%0Adata-driven%20models%20for%20personalized%20medicine%20highly%20depends%20on%20how%20the%20EHR%20data%0Ais%20represented%20from%20physiological%20data%2C%20treatments%2C%20and%20the%20missing%20values%20in%0Athe%20data.%20To%20this%20end%2C%20we%20propose%20a%20novel%20deep-learning%20model%20that%20learns%20the%0Aunderlying%20patient%20dynamics%20over%20time%20across%20multivariate%20data%20to%20generate%0Apersonalized%20realistic%20values%20conditioning%20on%20an%20individual%27s%20demographic%0Acharacteristics%20and%20treatments.%20Our%20proposed%20model%2C%20IGNITE%20%28Individualized%0AGeNeration%20of%20Imputations%20in%20Time-series%20Electronic%20health%20records%29%2C%20utilises%20a%0Aconditional%20dual-variational%20autoencoder%20augmented%20with%20dual-stage%20attention%20to%0Agenerate%20missing%20values%20for%20an%20individual.%20In%20IGNITE%2C%20we%20further%20propose%20a%0Anovel%20individualized%20missingness%20mask%20%28IMM%29%2C%20which%20helps%20our%20model%20generate%0Avalues%20based%20on%20the%20individual%27s%20observed%20data%20and%20missingness%20patterns.%20We%0Afurther%20extend%20the%20use%20of%20IGNITE%20from%20imputing%20missingness%20to%20a%20personalized%0Adata%20synthesizer%2C%20where%20it%20generates%20missing%20EHRs%20that%20were%20never%20observed%0Aprior%20or%20even%20generates%20new%20patients%20for%20various%20applications.%20We%20validate%20our%0Amodel%20on%20three%20large%20publicly%20available%20datasets%20and%20show%20that%20IGNITE%0Aoutperforms%20state-of-the-art%20approaches%20in%20missing%20data%20reconstruction%20and%20task%0Aprediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04402v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIGNITE%253A%2520Individualized%2520GeNeration%2520of%2520Imputations%2520in%2520Time-series%250A%2520%2520Electronic%2520health%2520records%26entry.906535625%3DGhadeer%2520O.%2520Ghosheh%2520and%2520Jin%2520Li%2520and%2520Tingting%2520Zhu%26entry.1292438233%3D%2520%2520Electronic%2520Health%2520Records%2520present%2520a%2520valuable%2520modality%2520for%2520driving%250Apersonalized%2520medicine%252C%2520where%2520treatment%2520is%2520tailored%2520to%2520fit%2520individual-level%250Adifferences.%2520For%2520this%2520purpose%252C%2520many%2520data-driven%2520machine%2520learning%2520and%250Astatistical%2520models%2520rely%2520on%2520the%2520wealth%2520of%2520longitudinal%2520EHRs%2520to%2520study%2520patients%2527%250Aphysiological%2520and%2520treatment%2520effects.%2520However%252C%2520longitudinal%2520EHRs%2520tend%2520to%2520be%250Asparse%2520and%2520highly%2520missing%252C%2520where%2520missingness%2520could%2520also%2520be%2520informative%2520and%250Areflect%2520the%2520underlying%2520patient%2527s%2520health%2520status.%2520Therefore%252C%2520the%2520success%2520of%250Adata-driven%2520models%2520for%2520personalized%2520medicine%2520highly%2520depends%2520on%2520how%2520the%2520EHR%2520data%250Ais%2520represented%2520from%2520physiological%2520data%252C%2520treatments%252C%2520and%2520the%2520missing%2520values%2520in%250Athe%2520data.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520deep-learning%2520model%2520that%2520learns%2520the%250Aunderlying%2520patient%2520dynamics%2520over%2520time%2520across%2520multivariate%2520data%2520to%2520generate%250Apersonalized%2520realistic%2520values%2520conditioning%2520on%2520an%2520individual%2527s%2520demographic%250Acharacteristics%2520and%2520treatments.%2520Our%2520proposed%2520model%252C%2520IGNITE%2520%2528Individualized%250AGeNeration%2520of%2520Imputations%2520in%2520Time-series%2520Electronic%2520health%2520records%2529%252C%2520utilises%2520a%250Aconditional%2520dual-variational%2520autoencoder%2520augmented%2520with%2520dual-stage%2520attention%2520to%250Agenerate%2520missing%2520values%2520for%2520an%2520individual.%2520In%2520IGNITE%252C%2520we%2520further%2520propose%2520a%250Anovel%2520individualized%2520missingness%2520mask%2520%2528IMM%2529%252C%2520which%2520helps%2520our%2520model%2520generate%250Avalues%2520based%2520on%2520the%2520individual%2527s%2520observed%2520data%2520and%2520missingness%2520patterns.%2520We%250Afurther%2520extend%2520the%2520use%2520of%2520IGNITE%2520from%2520imputing%2520missingness%2520to%2520a%2520personalized%250Adata%2520synthesizer%252C%2520where%2520it%2520generates%2520missing%2520EHRs%2520that%2520were%2520never%2520observed%250Aprior%2520or%2520even%2520generates%2520new%2520patients%2520for%2520various%2520applications.%2520We%2520validate%2520our%250Amodel%2520on%2520three%2520large%2520publicly%2520available%2520datasets%2520and%2520show%2520that%2520IGNITE%250Aoutperforms%2520state-of-the-art%2520approaches%2520in%2520missing%2520data%2520reconstruction%2520and%2520task%250Aprediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.04402v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IGNITE%3A%20Individualized%20GeNeration%20of%20Imputations%20in%20Time-series%0A%20%20Electronic%20health%20records&entry.906535625=Ghadeer%20O.%20Ghosheh%20and%20Jin%20Li%20and%20Tingting%20Zhu&entry.1292438233=%20%20Electronic%20Health%20Records%20present%20a%20valuable%20modality%20for%20driving%0Apersonalized%20medicine%2C%20where%20treatment%20is%20tailored%20to%20fit%20individual-level%0Adifferences.%20For%20this%20purpose%2C%20many%20data-driven%20machine%20learning%20and%0Astatistical%20models%20rely%20on%20the%20wealth%20of%20longitudinal%20EHRs%20to%20study%20patients%27%0Aphysiological%20and%20treatment%20effects.%20However%2C%20longitudinal%20EHRs%20tend%20to%20be%0Asparse%20and%20highly%20missing%2C%20where%20missingness%20could%20also%20be%20informative%20and%0Areflect%20the%20underlying%20patient%27s%20health%20status.%20Therefore%2C%20the%20success%20of%0Adata-driven%20models%20for%20personalized%20medicine%20highly%20depends%20on%20how%20the%20EHR%20data%0Ais%20represented%20from%20physiological%20data%2C%20treatments%2C%20and%20the%20missing%20values%20in%0Athe%20data.%20To%20this%20end%2C%20we%20propose%20a%20novel%20deep-learning%20model%20that%20learns%20the%0Aunderlying%20patient%20dynamics%20over%20time%20across%20multivariate%20data%20to%20generate%0Apersonalized%20realistic%20values%20conditioning%20on%20an%20individual%27s%20demographic%0Acharacteristics%20and%20treatments.%20Our%20proposed%20model%2C%20IGNITE%20%28Individualized%0AGeNeration%20of%20Imputations%20in%20Time-series%20Electronic%20health%20records%29%2C%20utilises%20a%0Aconditional%20dual-variational%20autoencoder%20augmented%20with%20dual-stage%20attention%20to%0Agenerate%20missing%20values%20for%20an%20individual.%20In%20IGNITE%2C%20we%20further%20propose%20a%0Anovel%20individualized%20missingness%20mask%20%28IMM%29%2C%20which%20helps%20our%20model%20generate%0Avalues%20based%20on%20the%20individual%27s%20observed%20data%20and%20missingness%20patterns.%20We%0Afurther%20extend%20the%20use%20of%20IGNITE%20from%20imputing%20missingness%20to%20a%20personalized%0Adata%20synthesizer%2C%20where%20it%20generates%20missing%20EHRs%20that%20were%20never%20observed%0Aprior%20or%20even%20generates%20new%20patients%20for%20various%20applications.%20We%20validate%20our%0Amodel%20on%20three%20large%20publicly%20available%20datasets%20and%20show%20that%20IGNITE%0Aoutperforms%20state-of-the-art%20approaches%20in%20missing%20data%20reconstruction%20and%20task%0Aprediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04402v2&entry.124074799=Read"},
{"title": "Crack-EdgeSAM Self-Prompting Crack Segmentation System for Edge Devices", "author": "Yingchu Wang and Ji He and Shijie Yu", "abstract": "  Structural health monitoring (SHM) is essential for the early detection of\ninfrastructure defects, such as cracks in concrete bridge pier. but often faces\nchallenges in efficiency and accuracy in complex environments. Although the\nSegment Anything Model (SAM) achieves excellent segmentation performance, its\ncomputational demands limit its suitability for real-time applications on edge\ndevices. To address these challenges, this paper proposes Crack-EdgeSAM, a\nself-prompting crack segmentation system that integrates YOLOv8 for generating\nprompt boxes and a fine-tuned EdgeSAM model for crack segmentation. To ensure\ncomputational efficiency, the method employs ConvLoRA, a Parameter-Efficient\nFine-Tuning (PEFT) technique, along with DiceFocalLoss to fine-tune the EdgeSAM\nmodel. Our experimental results on public datasets and the climbing robot\nautomatic inspections demonstrate that the system achieves high segmentation\naccuracy and significantly enhanced inference speed compared to the most recent\nmethods. Notably, the system processes 1024 x 1024 pixels images at 46 FPS on\nour PC and 8 FPS on Jetson Orin Nano.\n", "link": "http://arxiv.org/abs/2412.07205v2", "date": "2024-12-13", "relevancy": 1.9769, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5136}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5048}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Crack-EdgeSAM%20Self-Prompting%20Crack%20Segmentation%20System%20for%20Edge%20Devices&body=Title%3A%20Crack-EdgeSAM%20Self-Prompting%20Crack%20Segmentation%20System%20for%20Edge%20Devices%0AAuthor%3A%20Yingchu%20Wang%20and%20Ji%20He%20and%20Shijie%20Yu%0AAbstract%3A%20%20%20Structural%20health%20monitoring%20%28SHM%29%20is%20essential%20for%20the%20early%20detection%20of%0Ainfrastructure%20defects%2C%20such%20as%20cracks%20in%20concrete%20bridge%20pier.%20but%20often%20faces%0Achallenges%20in%20efficiency%20and%20accuracy%20in%20complex%20environments.%20Although%20the%0ASegment%20Anything%20Model%20%28SAM%29%20achieves%20excellent%20segmentation%20performance%2C%20its%0Acomputational%20demands%20limit%20its%20suitability%20for%20real-time%20applications%20on%20edge%0Adevices.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20Crack-EdgeSAM%2C%20a%0Aself-prompting%20crack%20segmentation%20system%20that%20integrates%20YOLOv8%20for%20generating%0Aprompt%20boxes%20and%20a%20fine-tuned%20EdgeSAM%20model%20for%20crack%20segmentation.%20To%20ensure%0Acomputational%20efficiency%2C%20the%20method%20employs%20ConvLoRA%2C%20a%20Parameter-Efficient%0AFine-Tuning%20%28PEFT%29%20technique%2C%20along%20with%20DiceFocalLoss%20to%20fine-tune%20the%20EdgeSAM%0Amodel.%20Our%20experimental%20results%20on%20public%20datasets%20and%20the%20climbing%20robot%0Aautomatic%20inspections%20demonstrate%20that%20the%20system%20achieves%20high%20segmentation%0Aaccuracy%20and%20significantly%20enhanced%20inference%20speed%20compared%20to%20the%20most%20recent%0Amethods.%20Notably%2C%20the%20system%20processes%201024%20x%201024%20pixels%20images%20at%2046%20FPS%20on%0Aour%20PC%20and%208%20FPS%20on%20Jetson%20Orin%20Nano.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07205v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrack-EdgeSAM%2520Self-Prompting%2520Crack%2520Segmentation%2520System%2520for%2520Edge%2520Devices%26entry.906535625%3DYingchu%2520Wang%2520and%2520Ji%2520He%2520and%2520Shijie%2520Yu%26entry.1292438233%3D%2520%2520Structural%2520health%2520monitoring%2520%2528SHM%2529%2520is%2520essential%2520for%2520the%2520early%2520detection%2520of%250Ainfrastructure%2520defects%252C%2520such%2520as%2520cracks%2520in%2520concrete%2520bridge%2520pier.%2520but%2520often%2520faces%250Achallenges%2520in%2520efficiency%2520and%2520accuracy%2520in%2520complex%2520environments.%2520Although%2520the%250ASegment%2520Anything%2520Model%2520%2528SAM%2529%2520achieves%2520excellent%2520segmentation%2520performance%252C%2520its%250Acomputational%2520demands%2520limit%2520its%2520suitability%2520for%2520real-time%2520applications%2520on%2520edge%250Adevices.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520Crack-EdgeSAM%252C%2520a%250Aself-prompting%2520crack%2520segmentation%2520system%2520that%2520integrates%2520YOLOv8%2520for%2520generating%250Aprompt%2520boxes%2520and%2520a%2520fine-tuned%2520EdgeSAM%2520model%2520for%2520crack%2520segmentation.%2520To%2520ensure%250Acomputational%2520efficiency%252C%2520the%2520method%2520employs%2520ConvLoRA%252C%2520a%2520Parameter-Efficient%250AFine-Tuning%2520%2528PEFT%2529%2520technique%252C%2520along%2520with%2520DiceFocalLoss%2520to%2520fine-tune%2520the%2520EdgeSAM%250Amodel.%2520Our%2520experimental%2520results%2520on%2520public%2520datasets%2520and%2520the%2520climbing%2520robot%250Aautomatic%2520inspections%2520demonstrate%2520that%2520the%2520system%2520achieves%2520high%2520segmentation%250Aaccuracy%2520and%2520significantly%2520enhanced%2520inference%2520speed%2520compared%2520to%2520the%2520most%2520recent%250Amethods.%2520Notably%252C%2520the%2520system%2520processes%25201024%2520x%25201024%2520pixels%2520images%2520at%252046%2520FPS%2520on%250Aour%2520PC%2520and%25208%2520FPS%2520on%2520Jetson%2520Orin%2520Nano.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07205v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Crack-EdgeSAM%20Self-Prompting%20Crack%20Segmentation%20System%20for%20Edge%20Devices&entry.906535625=Yingchu%20Wang%20and%20Ji%20He%20and%20Shijie%20Yu&entry.1292438233=%20%20Structural%20health%20monitoring%20%28SHM%29%20is%20essential%20for%20the%20early%20detection%20of%0Ainfrastructure%20defects%2C%20such%20as%20cracks%20in%20concrete%20bridge%20pier.%20but%20often%20faces%0Achallenges%20in%20efficiency%20and%20accuracy%20in%20complex%20environments.%20Although%20the%0ASegment%20Anything%20Model%20%28SAM%29%20achieves%20excellent%20segmentation%20performance%2C%20its%0Acomputational%20demands%20limit%20its%20suitability%20for%20real-time%20applications%20on%20edge%0Adevices.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20Crack-EdgeSAM%2C%20a%0Aself-prompting%20crack%20segmentation%20system%20that%20integrates%20YOLOv8%20for%20generating%0Aprompt%20boxes%20and%20a%20fine-tuned%20EdgeSAM%20model%20for%20crack%20segmentation.%20To%20ensure%0Acomputational%20efficiency%2C%20the%20method%20employs%20ConvLoRA%2C%20a%20Parameter-Efficient%0AFine-Tuning%20%28PEFT%29%20technique%2C%20along%20with%20DiceFocalLoss%20to%20fine-tune%20the%20EdgeSAM%0Amodel.%20Our%20experimental%20results%20on%20public%20datasets%20and%20the%20climbing%20robot%0Aautomatic%20inspections%20demonstrate%20that%20the%20system%20achieves%20high%20segmentation%0Aaccuracy%20and%20significantly%20enhanced%20inference%20speed%20compared%20to%20the%20most%20recent%0Amethods.%20Notably%2C%20the%20system%20processes%201024%20x%201024%20pixels%20images%20at%2046%20FPS%20on%0Aour%20PC%20and%208%20FPS%20on%20Jetson%20Orin%20Nano.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07205v2&entry.124074799=Read"},
{"title": "GAOKAO-Eval: Does high scores truly reflect strong capabilities in LLMs?", "author": "Zhikai Lei and Tianyi Liang and Hanglei Hu and Jin Zhang and Yunhua Zhou and Yunfan Shao and Linyang Li and Chenchui Li and Changbo Wang and Hang Yan and Qipeng Guo", "abstract": "  Large Language Models (LLMs) are commonly evaluated using human-crafted\nbenchmarks, under the premise that higher scores implicitly reflect stronger\nhuman-like performance. However, there is growing concern that LLMs may ``game\"\nthese benchmarks due to data leakage, achieving high scores while struggling\nwith tasks simple for humans. To substantively address the problem, we create\nGAOKAO-Eval, a comprehensive benchmark based on China's National College\nEntrance Examination (Gaokao), and conduct ``closed-book\" evaluations for\nrepresentative models released prior to Gaokao. Contrary to prevailing\nconsensus, even after addressing data leakage and comprehensiveness,\nGAOKAO-Eval reveals that high scores still fail to truly reflect human-aligned\ncapabilities. To better understand this mismatch, We introduce the Rasch model\nfrom cognitive psychology to analyze LLM scoring patterns and identify two key\ndiscrepancies: 1) anomalous consistent performance across various question\ndifficulties, and 2) high variance in performance on questions of similar\ndifficulty. In addition, We identified inconsistent grading of LLM-generated\nanswers among teachers and recurring mistake patterns. we find that the\nphenomenons are well-grounded in the motivations behind OpenAI o1, and o1's\nreasoning-as-difficulties can mitigate the mismatch. These results show that\nGAOKAO-Eval can reveal limitations in LLM capabilities not captured by current\nbenchmarks and highlight the need for more LLM-aligned difficulty analysis.\n", "link": "http://arxiv.org/abs/2412.10056v1", "date": "2024-12-13", "relevancy": 1.9698, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAOKAO-Eval%3A%20Does%20high%20scores%20truly%20reflect%20strong%20capabilities%20in%20LLMs%3F&body=Title%3A%20GAOKAO-Eval%3A%20Does%20high%20scores%20truly%20reflect%20strong%20capabilities%20in%20LLMs%3F%0AAuthor%3A%20Zhikai%20Lei%20and%20Tianyi%20Liang%20and%20Hanglei%20Hu%20and%20Jin%20Zhang%20and%20Yunhua%20Zhou%20and%20Yunfan%20Shao%20and%20Linyang%20Li%20and%20Chenchui%20Li%20and%20Changbo%20Wang%20and%20Hang%20Yan%20and%20Qipeng%20Guo%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20commonly%20evaluated%20using%20human-crafted%0Abenchmarks%2C%20under%20the%20premise%20that%20higher%20scores%20implicitly%20reflect%20stronger%0Ahuman-like%20performance.%20However%2C%20there%20is%20growing%20concern%20that%20LLMs%20may%20%60%60game%22%0Athese%20benchmarks%20due%20to%20data%20leakage%2C%20achieving%20high%20scores%20while%20struggling%0Awith%20tasks%20simple%20for%20humans.%20To%20substantively%20address%20the%20problem%2C%20we%20create%0AGAOKAO-Eval%2C%20a%20comprehensive%20benchmark%20based%20on%20China%27s%20National%20College%0AEntrance%20Examination%20%28Gaokao%29%2C%20and%20conduct%20%60%60closed-book%22%20evaluations%20for%0Arepresentative%20models%20released%20prior%20to%20Gaokao.%20Contrary%20to%20prevailing%0Aconsensus%2C%20even%20after%20addressing%20data%20leakage%20and%20comprehensiveness%2C%0AGAOKAO-Eval%20reveals%20that%20high%20scores%20still%20fail%20to%20truly%20reflect%20human-aligned%0Acapabilities.%20To%20better%20understand%20this%20mismatch%2C%20We%20introduce%20the%20Rasch%20model%0Afrom%20cognitive%20psychology%20to%20analyze%20LLM%20scoring%20patterns%20and%20identify%20two%20key%0Adiscrepancies%3A%201%29%20anomalous%20consistent%20performance%20across%20various%20question%0Adifficulties%2C%20and%202%29%20high%20variance%20in%20performance%20on%20questions%20of%20similar%0Adifficulty.%20In%20addition%2C%20We%20identified%20inconsistent%20grading%20of%20LLM-generated%0Aanswers%20among%20teachers%20and%20recurring%20mistake%20patterns.%20we%20find%20that%20the%0Aphenomenons%20are%20well-grounded%20in%20the%20motivations%20behind%20OpenAI%20o1%2C%20and%20o1%27s%0Areasoning-as-difficulties%20can%20mitigate%20the%20mismatch.%20These%20results%20show%20that%0AGAOKAO-Eval%20can%20reveal%20limitations%20in%20LLM%20capabilities%20not%20captured%20by%20current%0Abenchmarks%20and%20highlight%20the%20need%20for%20more%20LLM-aligned%20difficulty%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAOKAO-Eval%253A%2520Does%2520high%2520scores%2520truly%2520reflect%2520strong%2520capabilities%2520in%2520LLMs%253F%26entry.906535625%3DZhikai%2520Lei%2520and%2520Tianyi%2520Liang%2520and%2520Hanglei%2520Hu%2520and%2520Jin%2520Zhang%2520and%2520Yunhua%2520Zhou%2520and%2520Yunfan%2520Shao%2520and%2520Linyang%2520Li%2520and%2520Chenchui%2520Li%2520and%2520Changbo%2520Wang%2520and%2520Hang%2520Yan%2520and%2520Qipeng%2520Guo%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520commonly%2520evaluated%2520using%2520human-crafted%250Abenchmarks%252C%2520under%2520the%2520premise%2520that%2520higher%2520scores%2520implicitly%2520reflect%2520stronger%250Ahuman-like%2520performance.%2520However%252C%2520there%2520is%2520growing%2520concern%2520that%2520LLMs%2520may%2520%2560%2560game%2522%250Athese%2520benchmarks%2520due%2520to%2520data%2520leakage%252C%2520achieving%2520high%2520scores%2520while%2520struggling%250Awith%2520tasks%2520simple%2520for%2520humans.%2520To%2520substantively%2520address%2520the%2520problem%252C%2520we%2520create%250AGAOKAO-Eval%252C%2520a%2520comprehensive%2520benchmark%2520based%2520on%2520China%2527s%2520National%2520College%250AEntrance%2520Examination%2520%2528Gaokao%2529%252C%2520and%2520conduct%2520%2560%2560closed-book%2522%2520evaluations%2520for%250Arepresentative%2520models%2520released%2520prior%2520to%2520Gaokao.%2520Contrary%2520to%2520prevailing%250Aconsensus%252C%2520even%2520after%2520addressing%2520data%2520leakage%2520and%2520comprehensiveness%252C%250AGAOKAO-Eval%2520reveals%2520that%2520high%2520scores%2520still%2520fail%2520to%2520truly%2520reflect%2520human-aligned%250Acapabilities.%2520To%2520better%2520understand%2520this%2520mismatch%252C%2520We%2520introduce%2520the%2520Rasch%2520model%250Afrom%2520cognitive%2520psychology%2520to%2520analyze%2520LLM%2520scoring%2520patterns%2520and%2520identify%2520two%2520key%250Adiscrepancies%253A%25201%2529%2520anomalous%2520consistent%2520performance%2520across%2520various%2520question%250Adifficulties%252C%2520and%25202%2529%2520high%2520variance%2520in%2520performance%2520on%2520questions%2520of%2520similar%250Adifficulty.%2520In%2520addition%252C%2520We%2520identified%2520inconsistent%2520grading%2520of%2520LLM-generated%250Aanswers%2520among%2520teachers%2520and%2520recurring%2520mistake%2520patterns.%2520we%2520find%2520that%2520the%250Aphenomenons%2520are%2520well-grounded%2520in%2520the%2520motivations%2520behind%2520OpenAI%2520o1%252C%2520and%2520o1%2527s%250Areasoning-as-difficulties%2520can%2520mitigate%2520the%2520mismatch.%2520These%2520results%2520show%2520that%250AGAOKAO-Eval%2520can%2520reveal%2520limitations%2520in%2520LLM%2520capabilities%2520not%2520captured%2520by%2520current%250Abenchmarks%2520and%2520highlight%2520the%2520need%2520for%2520more%2520LLM-aligned%2520difficulty%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAOKAO-Eval%3A%20Does%20high%20scores%20truly%20reflect%20strong%20capabilities%20in%20LLMs%3F&entry.906535625=Zhikai%20Lei%20and%20Tianyi%20Liang%20and%20Hanglei%20Hu%20and%20Jin%20Zhang%20and%20Yunhua%20Zhou%20and%20Yunfan%20Shao%20and%20Linyang%20Li%20and%20Chenchui%20Li%20and%20Changbo%20Wang%20and%20Hang%20Yan%20and%20Qipeng%20Guo&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20commonly%20evaluated%20using%20human-crafted%0Abenchmarks%2C%20under%20the%20premise%20that%20higher%20scores%20implicitly%20reflect%20stronger%0Ahuman-like%20performance.%20However%2C%20there%20is%20growing%20concern%20that%20LLMs%20may%20%60%60game%22%0Athese%20benchmarks%20due%20to%20data%20leakage%2C%20achieving%20high%20scores%20while%20struggling%0Awith%20tasks%20simple%20for%20humans.%20To%20substantively%20address%20the%20problem%2C%20we%20create%0AGAOKAO-Eval%2C%20a%20comprehensive%20benchmark%20based%20on%20China%27s%20National%20College%0AEntrance%20Examination%20%28Gaokao%29%2C%20and%20conduct%20%60%60closed-book%22%20evaluations%20for%0Arepresentative%20models%20released%20prior%20to%20Gaokao.%20Contrary%20to%20prevailing%0Aconsensus%2C%20even%20after%20addressing%20data%20leakage%20and%20comprehensiveness%2C%0AGAOKAO-Eval%20reveals%20that%20high%20scores%20still%20fail%20to%20truly%20reflect%20human-aligned%0Acapabilities.%20To%20better%20understand%20this%20mismatch%2C%20We%20introduce%20the%20Rasch%20model%0Afrom%20cognitive%20psychology%20to%20analyze%20LLM%20scoring%20patterns%20and%20identify%20two%20key%0Adiscrepancies%3A%201%29%20anomalous%20consistent%20performance%20across%20various%20question%0Adifficulties%2C%20and%202%29%20high%20variance%20in%20performance%20on%20questions%20of%20similar%0Adifficulty.%20In%20addition%2C%20We%20identified%20inconsistent%20grading%20of%20LLM-generated%0Aanswers%20among%20teachers%20and%20recurring%20mistake%20patterns.%20we%20find%20that%20the%0Aphenomenons%20are%20well-grounded%20in%20the%20motivations%20behind%20OpenAI%20o1%2C%20and%20o1%27s%0Areasoning-as-difficulties%20can%20mitigate%20the%20mismatch.%20These%20results%20show%20that%0AGAOKAO-Eval%20can%20reveal%20limitations%20in%20LLM%20capabilities%20not%20captured%20by%20current%0Abenchmarks%20and%20highlight%20the%20need%20for%20more%20LLM-aligned%20difficulty%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10056v1&entry.124074799=Read"},
{"title": "Adversarial Robustness of Bottleneck Injected Deep Neural Networks for\n  Task-Oriented Communication", "author": "Alireza Furutanpey and Pantelis A. Frangoudis and Patrik Szabo and Schahram Dustdar", "abstract": "  This paper investigates the adversarial robustness of Deep Neural Networks\n(DNNs) using Information Bottleneck (IB) objectives for task-oriented\ncommunication systems. We empirically demonstrate that while IB-based\napproaches provide baseline resilience against attacks targeting downstream\ntasks, the reliance on generative models for task-oriented communication\nintroduces new vulnerabilities. Through extensive experiments on several\ndatasets, we analyze how bottleneck depth and task complexity influence\nadversarial robustness. Our key findings show that Shallow Variational\nBottleneck Injection (SVBI) provides less adversarial robustness compared to\nDeep Variational Information Bottleneck (DVIB) approaches, with the gap\nwidening for more complex tasks. Additionally, we reveal that IB-based\nobjectives exhibit stronger robustness against attacks focusing on salient\npixels with high intensity compared to those perturbing many pixels with lower\nintensity. Lastly, we demonstrate that task-oriented communication systems that\nrely on generative models to extract and recover salient information have an\nincreased attack surface. The results highlight important security\nconsiderations for next-generation communication systems that leverage neural\nnetworks for goal-oriented compression.\n", "link": "http://arxiv.org/abs/2412.10265v1", "date": "2024-12-13", "relevancy": 1.9694, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5034}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4891}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Robustness%20of%20Bottleneck%20Injected%20Deep%20Neural%20Networks%20for%0A%20%20Task-Oriented%20Communication&body=Title%3A%20Adversarial%20Robustness%20of%20Bottleneck%20Injected%20Deep%20Neural%20Networks%20for%0A%20%20Task-Oriented%20Communication%0AAuthor%3A%20Alireza%20Furutanpey%20and%20Pantelis%20A.%20Frangoudis%20and%20Patrik%20Szabo%20and%20Schahram%20Dustdar%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20adversarial%20robustness%20of%20Deep%20Neural%20Networks%0A%28DNNs%29%20using%20Information%20Bottleneck%20%28IB%29%20objectives%20for%20task-oriented%0Acommunication%20systems.%20We%20empirically%20demonstrate%20that%20while%20IB-based%0Aapproaches%20provide%20baseline%20resilience%20against%20attacks%20targeting%20downstream%0Atasks%2C%20the%20reliance%20on%20generative%20models%20for%20task-oriented%20communication%0Aintroduces%20new%20vulnerabilities.%20Through%20extensive%20experiments%20on%20several%0Adatasets%2C%20we%20analyze%20how%20bottleneck%20depth%20and%20task%20complexity%20influence%0Aadversarial%20robustness.%20Our%20key%20findings%20show%20that%20Shallow%20Variational%0ABottleneck%20Injection%20%28SVBI%29%20provides%20less%20adversarial%20robustness%20compared%20to%0ADeep%20Variational%20Information%20Bottleneck%20%28DVIB%29%20approaches%2C%20with%20the%20gap%0Awidening%20for%20more%20complex%20tasks.%20Additionally%2C%20we%20reveal%20that%20IB-based%0Aobjectives%20exhibit%20stronger%20robustness%20against%20attacks%20focusing%20on%20salient%0Apixels%20with%20high%20intensity%20compared%20to%20those%20perturbing%20many%20pixels%20with%20lower%0Aintensity.%20Lastly%2C%20we%20demonstrate%20that%20task-oriented%20communication%20systems%20that%0Arely%20on%20generative%20models%20to%20extract%20and%20recover%20salient%20information%20have%20an%0Aincreased%20attack%20surface.%20The%20results%20highlight%20important%20security%0Aconsiderations%20for%20next-generation%20communication%20systems%20that%20leverage%20neural%0Anetworks%20for%20goal-oriented%20compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Robustness%2520of%2520Bottleneck%2520Injected%2520Deep%2520Neural%2520Networks%2520for%250A%2520%2520Task-Oriented%2520Communication%26entry.906535625%3DAlireza%2520Furutanpey%2520and%2520Pantelis%2520A.%2520Frangoudis%2520and%2520Patrik%2520Szabo%2520and%2520Schahram%2520Dustdar%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520adversarial%2520robustness%2520of%2520Deep%2520Neural%2520Networks%250A%2528DNNs%2529%2520using%2520Information%2520Bottleneck%2520%2528IB%2529%2520objectives%2520for%2520task-oriented%250Acommunication%2520systems.%2520We%2520empirically%2520demonstrate%2520that%2520while%2520IB-based%250Aapproaches%2520provide%2520baseline%2520resilience%2520against%2520attacks%2520targeting%2520downstream%250Atasks%252C%2520the%2520reliance%2520on%2520generative%2520models%2520for%2520task-oriented%2520communication%250Aintroduces%2520new%2520vulnerabilities.%2520Through%2520extensive%2520experiments%2520on%2520several%250Adatasets%252C%2520we%2520analyze%2520how%2520bottleneck%2520depth%2520and%2520task%2520complexity%2520influence%250Aadversarial%2520robustness.%2520Our%2520key%2520findings%2520show%2520that%2520Shallow%2520Variational%250ABottleneck%2520Injection%2520%2528SVBI%2529%2520provides%2520less%2520adversarial%2520robustness%2520compared%2520to%250ADeep%2520Variational%2520Information%2520Bottleneck%2520%2528DVIB%2529%2520approaches%252C%2520with%2520the%2520gap%250Awidening%2520for%2520more%2520complex%2520tasks.%2520Additionally%252C%2520we%2520reveal%2520that%2520IB-based%250Aobjectives%2520exhibit%2520stronger%2520robustness%2520against%2520attacks%2520focusing%2520on%2520salient%250Apixels%2520with%2520high%2520intensity%2520compared%2520to%2520those%2520perturbing%2520many%2520pixels%2520with%2520lower%250Aintensity.%2520Lastly%252C%2520we%2520demonstrate%2520that%2520task-oriented%2520communication%2520systems%2520that%250Arely%2520on%2520generative%2520models%2520to%2520extract%2520and%2520recover%2520salient%2520information%2520have%2520an%250Aincreased%2520attack%2520surface.%2520The%2520results%2520highlight%2520important%2520security%250Aconsiderations%2520for%2520next-generation%2520communication%2520systems%2520that%2520leverage%2520neural%250Anetworks%2520for%2520goal-oriented%2520compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Robustness%20of%20Bottleneck%20Injected%20Deep%20Neural%20Networks%20for%0A%20%20Task-Oriented%20Communication&entry.906535625=Alireza%20Furutanpey%20and%20Pantelis%20A.%20Frangoudis%20and%20Patrik%20Szabo%20and%20Schahram%20Dustdar&entry.1292438233=%20%20This%20paper%20investigates%20the%20adversarial%20robustness%20of%20Deep%20Neural%20Networks%0A%28DNNs%29%20using%20Information%20Bottleneck%20%28IB%29%20objectives%20for%20task-oriented%0Acommunication%20systems.%20We%20empirically%20demonstrate%20that%20while%20IB-based%0Aapproaches%20provide%20baseline%20resilience%20against%20attacks%20targeting%20downstream%0Atasks%2C%20the%20reliance%20on%20generative%20models%20for%20task-oriented%20communication%0Aintroduces%20new%20vulnerabilities.%20Through%20extensive%20experiments%20on%20several%0Adatasets%2C%20we%20analyze%20how%20bottleneck%20depth%20and%20task%20complexity%20influence%0Aadversarial%20robustness.%20Our%20key%20findings%20show%20that%20Shallow%20Variational%0ABottleneck%20Injection%20%28SVBI%29%20provides%20less%20adversarial%20robustness%20compared%20to%0ADeep%20Variational%20Information%20Bottleneck%20%28DVIB%29%20approaches%2C%20with%20the%20gap%0Awidening%20for%20more%20complex%20tasks.%20Additionally%2C%20we%20reveal%20that%20IB-based%0Aobjectives%20exhibit%20stronger%20robustness%20against%20attacks%20focusing%20on%20salient%0Apixels%20with%20high%20intensity%20compared%20to%20those%20perturbing%20many%20pixels%20with%20lower%0Aintensity.%20Lastly%2C%20we%20demonstrate%20that%20task-oriented%20communication%20systems%20that%0Arely%20on%20generative%20models%20to%20extract%20and%20recover%20salient%20information%20have%20an%0Aincreased%20attack%20surface.%20The%20results%20highlight%20important%20security%0Aconsiderations%20for%20next-generation%20communication%20systems%20that%20leverage%20neural%0Anetworks%20for%20goal-oriented%20compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10265v1&entry.124074799=Read"},
{"title": "Efficient Sign-Based Optimization: Accelerating Convergence via Variance\n  Reduction", "author": "Wei Jiang and Sifan Yang and Wenhao Yang and Lijun Zhang", "abstract": "  Sign stochastic gradient descent (signSGD) is a communication-efficient\nmethod that transmits only the sign of stochastic gradients for parameter\nupdating. Existing literature has demonstrated that signSGD can achieve a\nconvergence rate of $\\mathcal{O}(d^{1/2}T^{-1/4})$, where $d$ represents the\ndimension and $T$ is the iteration number. In this paper, we improve this\nconvergence rate to $\\mathcal{O}(d^{1/2}T^{-1/3})$ by introducing the\nSign-based Stochastic Variance Reduction (SSVR) method, which employs variance\nreduction estimators to track gradients and leverages their signs to update.\nFor finite-sum problems, our method can be further enhanced to achieve a\nconvergence rate of $\\mathcal{O}(m^{1/4}d^{1/2}T^{-1/2})$, where $m$ denotes\nthe number of component functions. Furthermore, we investigate the\nheterogeneous majority vote in distributed settings and introduce two novel\nalgorithms that attain improved convergence rates of\n$\\mathcal{O}(d^{1/2}T^{-1/2} + dn^{-1/2})$ and $\\mathcal{O}(d^{1/4}T^{-1/4})$\nrespectively, outperforming the previous results of $\\mathcal{O}(dT^{-1/4} +\ndn^{-1/2})$ and $\\mathcal{O}(d^{3/8}T^{-1/8})$, where $n$ represents the number\nof nodes. Numerical experiments across different tasks validate the\neffectiveness of our proposed methods.\n", "link": "http://arxiv.org/abs/2406.00489v3", "date": "2024-12-13", "relevancy": 1.9613, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5143}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5018}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Sign-Based%20Optimization%3A%20Accelerating%20Convergence%20via%20Variance%0A%20%20Reduction&body=Title%3A%20Efficient%20Sign-Based%20Optimization%3A%20Accelerating%20Convergence%20via%20Variance%0A%20%20Reduction%0AAuthor%3A%20Wei%20Jiang%20and%20Sifan%20Yang%20and%20Wenhao%20Yang%20and%20Lijun%20Zhang%0AAbstract%3A%20%20%20Sign%20stochastic%20gradient%20descent%20%28signSGD%29%20is%20a%20communication-efficient%0Amethod%20that%20transmits%20only%20the%20sign%20of%20stochastic%20gradients%20for%20parameter%0Aupdating.%20Existing%20literature%20has%20demonstrated%20that%20signSGD%20can%20achieve%20a%0Aconvergence%20rate%20of%20%24%5Cmathcal%7BO%7D%28d%5E%7B1/2%7DT%5E%7B-1/4%7D%29%24%2C%20where%20%24d%24%20represents%20the%0Adimension%20and%20%24T%24%20is%20the%20iteration%20number.%20In%20this%20paper%2C%20we%20improve%20this%0Aconvergence%20rate%20to%20%24%5Cmathcal%7BO%7D%28d%5E%7B1/2%7DT%5E%7B-1/3%7D%29%24%20by%20introducing%20the%0ASign-based%20Stochastic%20Variance%20Reduction%20%28SSVR%29%20method%2C%20which%20employs%20variance%0Areduction%20estimators%20to%20track%20gradients%20and%20leverages%20their%20signs%20to%20update.%0AFor%20finite-sum%20problems%2C%20our%20method%20can%20be%20further%20enhanced%20to%20achieve%20a%0Aconvergence%20rate%20of%20%24%5Cmathcal%7BO%7D%28m%5E%7B1/4%7Dd%5E%7B1/2%7DT%5E%7B-1/2%7D%29%24%2C%20where%20%24m%24%20denotes%0Athe%20number%20of%20component%20functions.%20Furthermore%2C%20we%20investigate%20the%0Aheterogeneous%20majority%20vote%20in%20distributed%20settings%20and%20introduce%20two%20novel%0Aalgorithms%20that%20attain%20improved%20convergence%20rates%20of%0A%24%5Cmathcal%7BO%7D%28d%5E%7B1/2%7DT%5E%7B-1/2%7D%20%2B%20dn%5E%7B-1/2%7D%29%24%20and%20%24%5Cmathcal%7BO%7D%28d%5E%7B1/4%7DT%5E%7B-1/4%7D%29%24%0Arespectively%2C%20outperforming%20the%20previous%20results%20of%20%24%5Cmathcal%7BO%7D%28dT%5E%7B-1/4%7D%20%2B%0Adn%5E%7B-1/2%7D%29%24%20and%20%24%5Cmathcal%7BO%7D%28d%5E%7B3/8%7DT%5E%7B-1/8%7D%29%24%2C%20where%20%24n%24%20represents%20the%20number%0Aof%20nodes.%20Numerical%20experiments%20across%20different%20tasks%20validate%20the%0Aeffectiveness%20of%20our%20proposed%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00489v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Sign-Based%2520Optimization%253A%2520Accelerating%2520Convergence%2520via%2520Variance%250A%2520%2520Reduction%26entry.906535625%3DWei%2520Jiang%2520and%2520Sifan%2520Yang%2520and%2520Wenhao%2520Yang%2520and%2520Lijun%2520Zhang%26entry.1292438233%3D%2520%2520Sign%2520stochastic%2520gradient%2520descent%2520%2528signSGD%2529%2520is%2520a%2520communication-efficient%250Amethod%2520that%2520transmits%2520only%2520the%2520sign%2520of%2520stochastic%2520gradients%2520for%2520parameter%250Aupdating.%2520Existing%2520literature%2520has%2520demonstrated%2520that%2520signSGD%2520can%2520achieve%2520a%250Aconvergence%2520rate%2520of%2520%2524%255Cmathcal%257BO%257D%2528d%255E%257B1/2%257DT%255E%257B-1/4%257D%2529%2524%252C%2520where%2520%2524d%2524%2520represents%2520the%250Adimension%2520and%2520%2524T%2524%2520is%2520the%2520iteration%2520number.%2520In%2520this%2520paper%252C%2520we%2520improve%2520this%250Aconvergence%2520rate%2520to%2520%2524%255Cmathcal%257BO%257D%2528d%255E%257B1/2%257DT%255E%257B-1/3%257D%2529%2524%2520by%2520introducing%2520the%250ASign-based%2520Stochastic%2520Variance%2520Reduction%2520%2528SSVR%2529%2520method%252C%2520which%2520employs%2520variance%250Areduction%2520estimators%2520to%2520track%2520gradients%2520and%2520leverages%2520their%2520signs%2520to%2520update.%250AFor%2520finite-sum%2520problems%252C%2520our%2520method%2520can%2520be%2520further%2520enhanced%2520to%2520achieve%2520a%250Aconvergence%2520rate%2520of%2520%2524%255Cmathcal%257BO%257D%2528m%255E%257B1/4%257Dd%255E%257B1/2%257DT%255E%257B-1/2%257D%2529%2524%252C%2520where%2520%2524m%2524%2520denotes%250Athe%2520number%2520of%2520component%2520functions.%2520Furthermore%252C%2520we%2520investigate%2520the%250Aheterogeneous%2520majority%2520vote%2520in%2520distributed%2520settings%2520and%2520introduce%2520two%2520novel%250Aalgorithms%2520that%2520attain%2520improved%2520convergence%2520rates%2520of%250A%2524%255Cmathcal%257BO%257D%2528d%255E%257B1/2%257DT%255E%257B-1/2%257D%2520%252B%2520dn%255E%257B-1/2%257D%2529%2524%2520and%2520%2524%255Cmathcal%257BO%257D%2528d%255E%257B1/4%257DT%255E%257B-1/4%257D%2529%2524%250Arespectively%252C%2520outperforming%2520the%2520previous%2520results%2520of%2520%2524%255Cmathcal%257BO%257D%2528dT%255E%257B-1/4%257D%2520%252B%250Adn%255E%257B-1/2%257D%2529%2524%2520and%2520%2524%255Cmathcal%257BO%257D%2528d%255E%257B3/8%257DT%255E%257B-1/8%257D%2529%2524%252C%2520where%2520%2524n%2524%2520represents%2520the%2520number%250Aof%2520nodes.%2520Numerical%2520experiments%2520across%2520different%2520tasks%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00489v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Sign-Based%20Optimization%3A%20Accelerating%20Convergence%20via%20Variance%0A%20%20Reduction&entry.906535625=Wei%20Jiang%20and%20Sifan%20Yang%20and%20Wenhao%20Yang%20and%20Lijun%20Zhang&entry.1292438233=%20%20Sign%20stochastic%20gradient%20descent%20%28signSGD%29%20is%20a%20communication-efficient%0Amethod%20that%20transmits%20only%20the%20sign%20of%20stochastic%20gradients%20for%20parameter%0Aupdating.%20Existing%20literature%20has%20demonstrated%20that%20signSGD%20can%20achieve%20a%0Aconvergence%20rate%20of%20%24%5Cmathcal%7BO%7D%28d%5E%7B1/2%7DT%5E%7B-1/4%7D%29%24%2C%20where%20%24d%24%20represents%20the%0Adimension%20and%20%24T%24%20is%20the%20iteration%20number.%20In%20this%20paper%2C%20we%20improve%20this%0Aconvergence%20rate%20to%20%24%5Cmathcal%7BO%7D%28d%5E%7B1/2%7DT%5E%7B-1/3%7D%29%24%20by%20introducing%20the%0ASign-based%20Stochastic%20Variance%20Reduction%20%28SSVR%29%20method%2C%20which%20employs%20variance%0Areduction%20estimators%20to%20track%20gradients%20and%20leverages%20their%20signs%20to%20update.%0AFor%20finite-sum%20problems%2C%20our%20method%20can%20be%20further%20enhanced%20to%20achieve%20a%0Aconvergence%20rate%20of%20%24%5Cmathcal%7BO%7D%28m%5E%7B1/4%7Dd%5E%7B1/2%7DT%5E%7B-1/2%7D%29%24%2C%20where%20%24m%24%20denotes%0Athe%20number%20of%20component%20functions.%20Furthermore%2C%20we%20investigate%20the%0Aheterogeneous%20majority%20vote%20in%20distributed%20settings%20and%20introduce%20two%20novel%0Aalgorithms%20that%20attain%20improved%20convergence%20rates%20of%0A%24%5Cmathcal%7BO%7D%28d%5E%7B1/2%7DT%5E%7B-1/2%7D%20%2B%20dn%5E%7B-1/2%7D%29%24%20and%20%24%5Cmathcal%7BO%7D%28d%5E%7B1/4%7DT%5E%7B-1/4%7D%29%24%0Arespectively%2C%20outperforming%20the%20previous%20results%20of%20%24%5Cmathcal%7BO%7D%28dT%5E%7B-1/4%7D%20%2B%0Adn%5E%7B-1/2%7D%29%24%20and%20%24%5Cmathcal%7BO%7D%28d%5E%7B3/8%7DT%5E%7B-1/8%7D%29%24%2C%20where%20%24n%24%20represents%20the%20number%0Aof%20nodes.%20Numerical%20experiments%20across%20different%20tasks%20validate%20the%0Aeffectiveness%20of%20our%20proposed%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00489v3&entry.124074799=Read"},
{"title": "Cultural Evolution of Cooperation among LLM Agents", "author": "Aron Vallinder and Edward Hughes", "abstract": "  Large language models (LLMs) provide a compelling foundation for building\ngenerally-capable AI agents. These agents may soon be deployed at scale in the\nreal world, representing the interests of individual humans (e.g., AI\nassistants) or groups of humans (e.g., AI-accelerated corporations). At\npresent, relatively little is known about the dynamics of multiple LLM agents\ninteracting over many generations of iterative deployment. In this paper, we\nexamine whether a \"society\" of LLM agents can learn mutually beneficial social\nnorms in the face of incentives to defect, a distinctive feature of human\nsociality that is arguably crucial to the success of civilization. In\nparticular, we study the evolution of indirect reciprocity across generations\nof LLM agents playing a classic iterated Donor Game in which agents can observe\nthe recent behavior of their peers. We find that the evolution of cooperation\ndiffers markedly across base models, with societies of Claude 3.5 Sonnet agents\nachieving significantly higher average scores than Gemini 1.5 Flash, which, in\nturn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an\nadditional mechanism for costly punishment to achieve yet higher scores, while\nGemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also\nobserve variation in emergent behavior across random seeds, suggesting an\nunderstudied sensitive dependence on initial conditions. We suggest that our\nevaluation regime could inspire an inexpensive and informative new class of LLM\nbenchmarks, focussed on the implications of LLM agent deployment for the\ncooperative infrastructure of society.\n", "link": "http://arxiv.org/abs/2412.10270v1", "date": "2024-12-13", "relevancy": 1.9277, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4942}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cultural%20Evolution%20of%20Cooperation%20among%20LLM%20Agents&body=Title%3A%20Cultural%20Evolution%20of%20Cooperation%20among%20LLM%20Agents%0AAuthor%3A%20Aron%20Vallinder%20and%20Edward%20Hughes%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20provide%20a%20compelling%20foundation%20for%20building%0Agenerally-capable%20AI%20agents.%20These%20agents%20may%20soon%20be%20deployed%20at%20scale%20in%20the%0Areal%20world%2C%20representing%20the%20interests%20of%20individual%20humans%20%28e.g.%2C%20AI%0Aassistants%29%20or%20groups%20of%20humans%20%28e.g.%2C%20AI-accelerated%20corporations%29.%20At%0Apresent%2C%20relatively%20little%20is%20known%20about%20the%20dynamics%20of%20multiple%20LLM%20agents%0Ainteracting%20over%20many%20generations%20of%20iterative%20deployment.%20In%20this%20paper%2C%20we%0Aexamine%20whether%20a%20%22society%22%20of%20LLM%20agents%20can%20learn%20mutually%20beneficial%20social%0Anorms%20in%20the%20face%20of%20incentives%20to%20defect%2C%20a%20distinctive%20feature%20of%20human%0Asociality%20that%20is%20arguably%20crucial%20to%20the%20success%20of%20civilization.%20In%0Aparticular%2C%20we%20study%20the%20evolution%20of%20indirect%20reciprocity%20across%20generations%0Aof%20LLM%20agents%20playing%20a%20classic%20iterated%20Donor%20Game%20in%20which%20agents%20can%20observe%0Athe%20recent%20behavior%20of%20their%20peers.%20We%20find%20that%20the%20evolution%20of%20cooperation%0Adiffers%20markedly%20across%20base%20models%2C%20with%20societies%20of%20Claude%203.5%20Sonnet%20agents%0Aachieving%20significantly%20higher%20average%20scores%20than%20Gemini%201.5%20Flash%2C%20which%2C%20in%0Aturn%2C%20outperforms%20GPT-4o.%20Further%2C%20Claude%203.5%20Sonnet%20can%20make%20use%20of%20an%0Aadditional%20mechanism%20for%20costly%20punishment%20to%20achieve%20yet%20higher%20scores%2C%20while%0AGemini%201.5%20Flash%20and%20GPT-4o%20fail%20to%20do%20so.%20For%20each%20model%20class%2C%20we%20also%0Aobserve%20variation%20in%20emergent%20behavior%20across%20random%20seeds%2C%20suggesting%20an%0Aunderstudied%20sensitive%20dependence%20on%20initial%20conditions.%20We%20suggest%20that%20our%0Aevaluation%20regime%20could%20inspire%20an%20inexpensive%20and%20informative%20new%20class%20of%20LLM%0Abenchmarks%2C%20focussed%20on%20the%20implications%20of%20LLM%20agent%20deployment%20for%20the%0Acooperative%20infrastructure%20of%20society.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCultural%2520Evolution%2520of%2520Cooperation%2520among%2520LLM%2520Agents%26entry.906535625%3DAron%2520Vallinder%2520and%2520Edward%2520Hughes%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520provide%2520a%2520compelling%2520foundation%2520for%2520building%250Agenerally-capable%2520AI%2520agents.%2520These%2520agents%2520may%2520soon%2520be%2520deployed%2520at%2520scale%2520in%2520the%250Areal%2520world%252C%2520representing%2520the%2520interests%2520of%2520individual%2520humans%2520%2528e.g.%252C%2520AI%250Aassistants%2529%2520or%2520groups%2520of%2520humans%2520%2528e.g.%252C%2520AI-accelerated%2520corporations%2529.%2520At%250Apresent%252C%2520relatively%2520little%2520is%2520known%2520about%2520the%2520dynamics%2520of%2520multiple%2520LLM%2520agents%250Ainteracting%2520over%2520many%2520generations%2520of%2520iterative%2520deployment.%2520In%2520this%2520paper%252C%2520we%250Aexamine%2520whether%2520a%2520%2522society%2522%2520of%2520LLM%2520agents%2520can%2520learn%2520mutually%2520beneficial%2520social%250Anorms%2520in%2520the%2520face%2520of%2520incentives%2520to%2520defect%252C%2520a%2520distinctive%2520feature%2520of%2520human%250Asociality%2520that%2520is%2520arguably%2520crucial%2520to%2520the%2520success%2520of%2520civilization.%2520In%250Aparticular%252C%2520we%2520study%2520the%2520evolution%2520of%2520indirect%2520reciprocity%2520across%2520generations%250Aof%2520LLM%2520agents%2520playing%2520a%2520classic%2520iterated%2520Donor%2520Game%2520in%2520which%2520agents%2520can%2520observe%250Athe%2520recent%2520behavior%2520of%2520their%2520peers.%2520We%2520find%2520that%2520the%2520evolution%2520of%2520cooperation%250Adiffers%2520markedly%2520across%2520base%2520models%252C%2520with%2520societies%2520of%2520Claude%25203.5%2520Sonnet%2520agents%250Aachieving%2520significantly%2520higher%2520average%2520scores%2520than%2520Gemini%25201.5%2520Flash%252C%2520which%252C%2520in%250Aturn%252C%2520outperforms%2520GPT-4o.%2520Further%252C%2520Claude%25203.5%2520Sonnet%2520can%2520make%2520use%2520of%2520an%250Aadditional%2520mechanism%2520for%2520costly%2520punishment%2520to%2520achieve%2520yet%2520higher%2520scores%252C%2520while%250AGemini%25201.5%2520Flash%2520and%2520GPT-4o%2520fail%2520to%2520do%2520so.%2520For%2520each%2520model%2520class%252C%2520we%2520also%250Aobserve%2520variation%2520in%2520emergent%2520behavior%2520across%2520random%2520seeds%252C%2520suggesting%2520an%250Aunderstudied%2520sensitive%2520dependence%2520on%2520initial%2520conditions.%2520We%2520suggest%2520that%2520our%250Aevaluation%2520regime%2520could%2520inspire%2520an%2520inexpensive%2520and%2520informative%2520new%2520class%2520of%2520LLM%250Abenchmarks%252C%2520focussed%2520on%2520the%2520implications%2520of%2520LLM%2520agent%2520deployment%2520for%2520the%250Acooperative%2520infrastructure%2520of%2520society.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cultural%20Evolution%20of%20Cooperation%20among%20LLM%20Agents&entry.906535625=Aron%20Vallinder%20and%20Edward%20Hughes&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20provide%20a%20compelling%20foundation%20for%20building%0Agenerally-capable%20AI%20agents.%20These%20agents%20may%20soon%20be%20deployed%20at%20scale%20in%20the%0Areal%20world%2C%20representing%20the%20interests%20of%20individual%20humans%20%28e.g.%2C%20AI%0Aassistants%29%20or%20groups%20of%20humans%20%28e.g.%2C%20AI-accelerated%20corporations%29.%20At%0Apresent%2C%20relatively%20little%20is%20known%20about%20the%20dynamics%20of%20multiple%20LLM%20agents%0Ainteracting%20over%20many%20generations%20of%20iterative%20deployment.%20In%20this%20paper%2C%20we%0Aexamine%20whether%20a%20%22society%22%20of%20LLM%20agents%20can%20learn%20mutually%20beneficial%20social%0Anorms%20in%20the%20face%20of%20incentives%20to%20defect%2C%20a%20distinctive%20feature%20of%20human%0Asociality%20that%20is%20arguably%20crucial%20to%20the%20success%20of%20civilization.%20In%0Aparticular%2C%20we%20study%20the%20evolution%20of%20indirect%20reciprocity%20across%20generations%0Aof%20LLM%20agents%20playing%20a%20classic%20iterated%20Donor%20Game%20in%20which%20agents%20can%20observe%0Athe%20recent%20behavior%20of%20their%20peers.%20We%20find%20that%20the%20evolution%20of%20cooperation%0Adiffers%20markedly%20across%20base%20models%2C%20with%20societies%20of%20Claude%203.5%20Sonnet%20agents%0Aachieving%20significantly%20higher%20average%20scores%20than%20Gemini%201.5%20Flash%2C%20which%2C%20in%0Aturn%2C%20outperforms%20GPT-4o.%20Further%2C%20Claude%203.5%20Sonnet%20can%20make%20use%20of%20an%0Aadditional%20mechanism%20for%20costly%20punishment%20to%20achieve%20yet%20higher%20scores%2C%20while%0AGemini%201.5%20Flash%20and%20GPT-4o%20fail%20to%20do%20so.%20For%20each%20model%20class%2C%20we%20also%0Aobserve%20variation%20in%20emergent%20behavior%20across%20random%20seeds%2C%20suggesting%20an%0Aunderstudied%20sensitive%20dependence%20on%20initial%20conditions.%20We%20suggest%20that%20our%0Aevaluation%20regime%20could%20inspire%20an%20inexpensive%20and%20informative%20new%20class%20of%20LLM%0Abenchmarks%2C%20focussed%20on%20the%20implications%20of%20LLM%20agent%20deployment%20for%20the%0Acooperative%20infrastructure%20of%20society.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10270v1&entry.124074799=Read"},
{"title": "AgentMixer: Multi-Agent Correlated Policy Factorization", "author": "Zhiyuan Li and Wenshuai Zhao and Lijun Wu and Joni Pajarinen", "abstract": "  In multi-agent reinforcement learning, centralized training with\ndecentralized execution (CTDE) methods typically assume that agents make\ndecisions based on their local observations independently, which may not lead\nto a correlated joint policy with coordination. Coordination can be explicitly\nencouraged during training and individual policies can be trained to imitate\nthe correlated joint policy. However, this may lead to an \\textit{asymmetric\nlearning failure} due to the observation mismatch between the joint and\nindividual policies. Inspired by the concept of correlated equilibrium, we\nintroduce a \\textit{strategy modification} called AgentMixer that allows agents\nto correlate their policies. AgentMixer combines individual partially\nobservable policies into a joint fully observable policy non-linearly. To\nenable decentralized execution, we introduce\n\\textit{Individual-Global-Consistency} to guarantee mode consistency during\njoint training of the centralized and decentralized policies and prove that\nAgentMixer converges to an $\\epsilon$-approximate Correlated Equilibrium. In\nthe Multi-Agent MuJoCo, SMAC-v2, Matrix Game, and Predator-Prey benchmarks,\nAgentMixer outperforms or matches state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2401.08728v3", "date": "2024-12-13", "relevancy": 1.4496, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5042}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4835}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentMixer%3A%20Multi-Agent%20Correlated%20Policy%20Factorization&body=Title%3A%20AgentMixer%3A%20Multi-Agent%20Correlated%20Policy%20Factorization%0AAuthor%3A%20Zhiyuan%20Li%20and%20Wenshuai%20Zhao%20and%20Lijun%20Wu%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20In%20multi-agent%20reinforcement%20learning%2C%20centralized%20training%20with%0Adecentralized%20execution%20%28CTDE%29%20methods%20typically%20assume%20that%20agents%20make%0Adecisions%20based%20on%20their%20local%20observations%20independently%2C%20which%20may%20not%20lead%0Ato%20a%20correlated%20joint%20policy%20with%20coordination.%20Coordination%20can%20be%20explicitly%0Aencouraged%20during%20training%20and%20individual%20policies%20can%20be%20trained%20to%20imitate%0Athe%20correlated%20joint%20policy.%20However%2C%20this%20may%20lead%20to%20an%20%5Ctextit%7Basymmetric%0Alearning%20failure%7D%20due%20to%20the%20observation%20mismatch%20between%20the%20joint%20and%0Aindividual%20policies.%20Inspired%20by%20the%20concept%20of%20correlated%20equilibrium%2C%20we%0Aintroduce%20a%20%5Ctextit%7Bstrategy%20modification%7D%20called%20AgentMixer%20that%20allows%20agents%0Ato%20correlate%20their%20policies.%20AgentMixer%20combines%20individual%20partially%0Aobservable%20policies%20into%20a%20joint%20fully%20observable%20policy%20non-linearly.%20To%0Aenable%20decentralized%20execution%2C%20we%20introduce%0A%5Ctextit%7BIndividual-Global-Consistency%7D%20to%20guarantee%20mode%20consistency%20during%0Ajoint%20training%20of%20the%20centralized%20and%20decentralized%20policies%20and%20prove%20that%0AAgentMixer%20converges%20to%20an%20%24%5Cepsilon%24-approximate%20Correlated%20Equilibrium.%20In%0Athe%20Multi-Agent%20MuJoCo%2C%20SMAC-v2%2C%20Matrix%20Game%2C%20and%20Predator-Prey%20benchmarks%2C%0AAgentMixer%20outperforms%20or%20matches%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08728v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentMixer%253A%2520Multi-Agent%2520Correlated%2520Policy%2520Factorization%26entry.906535625%3DZhiyuan%2520Li%2520and%2520Wenshuai%2520Zhao%2520and%2520Lijun%2520Wu%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520In%2520multi-agent%2520reinforcement%2520learning%252C%2520centralized%2520training%2520with%250Adecentralized%2520execution%2520%2528CTDE%2529%2520methods%2520typically%2520assume%2520that%2520agents%2520make%250Adecisions%2520based%2520on%2520their%2520local%2520observations%2520independently%252C%2520which%2520may%2520not%2520lead%250Ato%2520a%2520correlated%2520joint%2520policy%2520with%2520coordination.%2520Coordination%2520can%2520be%2520explicitly%250Aencouraged%2520during%2520training%2520and%2520individual%2520policies%2520can%2520be%2520trained%2520to%2520imitate%250Athe%2520correlated%2520joint%2520policy.%2520However%252C%2520this%2520may%2520lead%2520to%2520an%2520%255Ctextit%257Basymmetric%250Alearning%2520failure%257D%2520due%2520to%2520the%2520observation%2520mismatch%2520between%2520the%2520joint%2520and%250Aindividual%2520policies.%2520Inspired%2520by%2520the%2520concept%2520of%2520correlated%2520equilibrium%252C%2520we%250Aintroduce%2520a%2520%255Ctextit%257Bstrategy%2520modification%257D%2520called%2520AgentMixer%2520that%2520allows%2520agents%250Ato%2520correlate%2520their%2520policies.%2520AgentMixer%2520combines%2520individual%2520partially%250Aobservable%2520policies%2520into%2520a%2520joint%2520fully%2520observable%2520policy%2520non-linearly.%2520To%250Aenable%2520decentralized%2520execution%252C%2520we%2520introduce%250A%255Ctextit%257BIndividual-Global-Consistency%257D%2520to%2520guarantee%2520mode%2520consistency%2520during%250Ajoint%2520training%2520of%2520the%2520centralized%2520and%2520decentralized%2520policies%2520and%2520prove%2520that%250AAgentMixer%2520converges%2520to%2520an%2520%2524%255Cepsilon%2524-approximate%2520Correlated%2520Equilibrium.%2520In%250Athe%2520Multi-Agent%2520MuJoCo%252C%2520SMAC-v2%252C%2520Matrix%2520Game%252C%2520and%2520Predator-Prey%2520benchmarks%252C%250AAgentMixer%2520outperforms%2520or%2520matches%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.08728v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentMixer%3A%20Multi-Agent%20Correlated%20Policy%20Factorization&entry.906535625=Zhiyuan%20Li%20and%20Wenshuai%20Zhao%20and%20Lijun%20Wu%20and%20Joni%20Pajarinen&entry.1292438233=%20%20In%20multi-agent%20reinforcement%20learning%2C%20centralized%20training%20with%0Adecentralized%20execution%20%28CTDE%29%20methods%20typically%20assume%20that%20agents%20make%0Adecisions%20based%20on%20their%20local%20observations%20independently%2C%20which%20may%20not%20lead%0Ato%20a%20correlated%20joint%20policy%20with%20coordination.%20Coordination%20can%20be%20explicitly%0Aencouraged%20during%20training%20and%20individual%20policies%20can%20be%20trained%20to%20imitate%0Athe%20correlated%20joint%20policy.%20However%2C%20this%20may%20lead%20to%20an%20%5Ctextit%7Basymmetric%0Alearning%20failure%7D%20due%20to%20the%20observation%20mismatch%20between%20the%20joint%20and%0Aindividual%20policies.%20Inspired%20by%20the%20concept%20of%20correlated%20equilibrium%2C%20we%0Aintroduce%20a%20%5Ctextit%7Bstrategy%20modification%7D%20called%20AgentMixer%20that%20allows%20agents%0Ato%20correlate%20their%20policies.%20AgentMixer%20combines%20individual%20partially%0Aobservable%20policies%20into%20a%20joint%20fully%20observable%20policy%20non-linearly.%20To%0Aenable%20decentralized%20execution%2C%20we%20introduce%0A%5Ctextit%7BIndividual-Global-Consistency%7D%20to%20guarantee%20mode%20consistency%20during%0Ajoint%20training%20of%20the%20centralized%20and%20decentralized%20policies%20and%20prove%20that%0AAgentMixer%20converges%20to%20an%20%24%5Cepsilon%24-approximate%20Correlated%20Equilibrium.%20In%0Athe%20Multi-Agent%20MuJoCo%2C%20SMAC-v2%2C%20Matrix%20Game%2C%20and%20Predator-Prey%20benchmarks%2C%0AAgentMixer%20outperforms%20or%20matches%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08728v3&entry.124074799=Read"},
{"title": "BiCert: A Bilinear Mixed Integer Programming Formulation for Precise\n  Certified Bounds Against Data Poisoning Attacks", "author": "Tobias Lorenz and Marta Kwiatkowska and Mario Fritz", "abstract": "  Data poisoning attacks pose one of the biggest threats to modern AI systems,\nnecessitating robust defenses. While extensive efforts have been made to\ndevelop empirical defenses, attackers continue to evolve, creating\nsophisticated methods to circumvent these measures. To address this, we must\nmove beyond empirical defenses and establish provable certification methods\nthat guarantee robustness. This paper introduces a novel certification\napproach, BiCert, using Bilinear Mixed Integer Programming (BMIP) to compute\nsound deterministic bounds that provide such provable robustness. Using BMIP,\nwe compute the reachable set of parameters that could result from training with\npotentially manipulated data. A key element to make this computation feasible\nis to relax the reachable parameter set to a convex set between training\niterations. At test time, this parameter set allows us to predict all possible\noutcomes, guaranteeing robustness. BiCert is more precise than previous\nmethods, which rely solely on interval and polyhedral bounds. Crucially, our\napproach overcomes the fundamental limitation of prior approaches where\nparameter bounds could only grow, often uncontrollably. We show that BiCert's\ntighter bounds eliminate a key source of divergence issues, resulting in more\nstable training and higher certified accuracy.\n", "link": "http://arxiv.org/abs/2412.10186v1", "date": "2024-12-13", "relevancy": 1.3866, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4849}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4685}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiCert%3A%20A%20Bilinear%20Mixed%20Integer%20Programming%20Formulation%20for%20Precise%0A%20%20Certified%20Bounds%20Against%20Data%20Poisoning%20Attacks&body=Title%3A%20BiCert%3A%20A%20Bilinear%20Mixed%20Integer%20Programming%20Formulation%20for%20Precise%0A%20%20Certified%20Bounds%20Against%20Data%20Poisoning%20Attacks%0AAuthor%3A%20Tobias%20Lorenz%20and%20Marta%20Kwiatkowska%20and%20Mario%20Fritz%0AAbstract%3A%20%20%20Data%20poisoning%20attacks%20pose%20one%20of%20the%20biggest%20threats%20to%20modern%20AI%20systems%2C%0Anecessitating%20robust%20defenses.%20While%20extensive%20efforts%20have%20been%20made%20to%0Adevelop%20empirical%20defenses%2C%20attackers%20continue%20to%20evolve%2C%20creating%0Asophisticated%20methods%20to%20circumvent%20these%20measures.%20To%20address%20this%2C%20we%20must%0Amove%20beyond%20empirical%20defenses%20and%20establish%20provable%20certification%20methods%0Athat%20guarantee%20robustness.%20This%20paper%20introduces%20a%20novel%20certification%0Aapproach%2C%20BiCert%2C%20using%20Bilinear%20Mixed%20Integer%20Programming%20%28BMIP%29%20to%20compute%0Asound%20deterministic%20bounds%20that%20provide%20such%20provable%20robustness.%20Using%20BMIP%2C%0Awe%20compute%20the%20reachable%20set%20of%20parameters%20that%20could%20result%20from%20training%20with%0Apotentially%20manipulated%20data.%20A%20key%20element%20to%20make%20this%20computation%20feasible%0Ais%20to%20relax%20the%20reachable%20parameter%20set%20to%20a%20convex%20set%20between%20training%0Aiterations.%20At%20test%20time%2C%20this%20parameter%20set%20allows%20us%20to%20predict%20all%20possible%0Aoutcomes%2C%20guaranteeing%20robustness.%20BiCert%20is%20more%20precise%20than%20previous%0Amethods%2C%20which%20rely%20solely%20on%20interval%20and%20polyhedral%20bounds.%20Crucially%2C%20our%0Aapproach%20overcomes%20the%20fundamental%20limitation%20of%20prior%20approaches%20where%0Aparameter%20bounds%20could%20only%20grow%2C%20often%20uncontrollably.%20We%20show%20that%20BiCert%27s%0Atighter%20bounds%20eliminate%20a%20key%20source%20of%20divergence%20issues%2C%20resulting%20in%20more%0Astable%20training%20and%20higher%20certified%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiCert%253A%2520A%2520Bilinear%2520Mixed%2520Integer%2520Programming%2520Formulation%2520for%2520Precise%250A%2520%2520Certified%2520Bounds%2520Against%2520Data%2520Poisoning%2520Attacks%26entry.906535625%3DTobias%2520Lorenz%2520and%2520Marta%2520Kwiatkowska%2520and%2520Mario%2520Fritz%26entry.1292438233%3D%2520%2520Data%2520poisoning%2520attacks%2520pose%2520one%2520of%2520the%2520biggest%2520threats%2520to%2520modern%2520AI%2520systems%252C%250Anecessitating%2520robust%2520defenses.%2520While%2520extensive%2520efforts%2520have%2520been%2520made%2520to%250Adevelop%2520empirical%2520defenses%252C%2520attackers%2520continue%2520to%2520evolve%252C%2520creating%250Asophisticated%2520methods%2520to%2520circumvent%2520these%2520measures.%2520To%2520address%2520this%252C%2520we%2520must%250Amove%2520beyond%2520empirical%2520defenses%2520and%2520establish%2520provable%2520certification%2520methods%250Athat%2520guarantee%2520robustness.%2520This%2520paper%2520introduces%2520a%2520novel%2520certification%250Aapproach%252C%2520BiCert%252C%2520using%2520Bilinear%2520Mixed%2520Integer%2520Programming%2520%2528BMIP%2529%2520to%2520compute%250Asound%2520deterministic%2520bounds%2520that%2520provide%2520such%2520provable%2520robustness.%2520Using%2520BMIP%252C%250Awe%2520compute%2520the%2520reachable%2520set%2520of%2520parameters%2520that%2520could%2520result%2520from%2520training%2520with%250Apotentially%2520manipulated%2520data.%2520A%2520key%2520element%2520to%2520make%2520this%2520computation%2520feasible%250Ais%2520to%2520relax%2520the%2520reachable%2520parameter%2520set%2520to%2520a%2520convex%2520set%2520between%2520training%250Aiterations.%2520At%2520test%2520time%252C%2520this%2520parameter%2520set%2520allows%2520us%2520to%2520predict%2520all%2520possible%250Aoutcomes%252C%2520guaranteeing%2520robustness.%2520BiCert%2520is%2520more%2520precise%2520than%2520previous%250Amethods%252C%2520which%2520rely%2520solely%2520on%2520interval%2520and%2520polyhedral%2520bounds.%2520Crucially%252C%2520our%250Aapproach%2520overcomes%2520the%2520fundamental%2520limitation%2520of%2520prior%2520approaches%2520where%250Aparameter%2520bounds%2520could%2520only%2520grow%252C%2520often%2520uncontrollably.%2520We%2520show%2520that%2520BiCert%2527s%250Atighter%2520bounds%2520eliminate%2520a%2520key%2520source%2520of%2520divergence%2520issues%252C%2520resulting%2520in%2520more%250Astable%2520training%2520and%2520higher%2520certified%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiCert%3A%20A%20Bilinear%20Mixed%20Integer%20Programming%20Formulation%20for%20Precise%0A%20%20Certified%20Bounds%20Against%20Data%20Poisoning%20Attacks&entry.906535625=Tobias%20Lorenz%20and%20Marta%20Kwiatkowska%20and%20Mario%20Fritz&entry.1292438233=%20%20Data%20poisoning%20attacks%20pose%20one%20of%20the%20biggest%20threats%20to%20modern%20AI%20systems%2C%0Anecessitating%20robust%20defenses.%20While%20extensive%20efforts%20have%20been%20made%20to%0Adevelop%20empirical%20defenses%2C%20attackers%20continue%20to%20evolve%2C%20creating%0Asophisticated%20methods%20to%20circumvent%20these%20measures.%20To%20address%20this%2C%20we%20must%0Amove%20beyond%20empirical%20defenses%20and%20establish%20provable%20certification%20methods%0Athat%20guarantee%20robustness.%20This%20paper%20introduces%20a%20novel%20certification%0Aapproach%2C%20BiCert%2C%20using%20Bilinear%20Mixed%20Integer%20Programming%20%28BMIP%29%20to%20compute%0Asound%20deterministic%20bounds%20that%20provide%20such%20provable%20robustness.%20Using%20BMIP%2C%0Awe%20compute%20the%20reachable%20set%20of%20parameters%20that%20could%20result%20from%20training%20with%0Apotentially%20manipulated%20data.%20A%20key%20element%20to%20make%20this%20computation%20feasible%0Ais%20to%20relax%20the%20reachable%20parameter%20set%20to%20a%20convex%20set%20between%20training%0Aiterations.%20At%20test%20time%2C%20this%20parameter%20set%20allows%20us%20to%20predict%20all%20possible%0Aoutcomes%2C%20guaranteeing%20robustness.%20BiCert%20is%20more%20precise%20than%20previous%0Amethods%2C%20which%20rely%20solely%20on%20interval%20and%20polyhedral%20bounds.%20Crucially%2C%20our%0Aapproach%20overcomes%20the%20fundamental%20limitation%20of%20prior%20approaches%20where%0Aparameter%20bounds%20could%20only%20grow%2C%20often%20uncontrollably.%20We%20show%20that%20BiCert%27s%0Atighter%20bounds%20eliminate%20a%20key%20source%20of%20divergence%20issues%2C%20resulting%20in%20more%0Astable%20training%20and%20higher%20certified%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10186v1&entry.124074799=Read"},
{"title": "Data Integration with Fusion Searchlight: Classifying Brain States from\n  Resting-state fMRI", "author": "Simon Wein and Marco Riebel and Lisa-Marie Brunner and Caroline Nothdurfter and Rainer Rupprecht and Jens V. Schwarzbach", "abstract": "  Spontaneous neural activity observed in resting-state fMRI is characterized\nby complex spatio-temporal dynamics. Different measures related to local and\nglobal brain connectivity and fluctuations in low-frequency amplitudes can\nquantify individual aspects of these neural dynamics. Even though such measures\nare derived from the same functional signals, they are often evaluated\nseparately, neglecting their interrelations and potentially reducing the\nanalysis sensitivity. In our study, we present a fusion searchlight (FuSL)\nframework to combine the complementary information contained in different\nresting-state fMRI metrics and demonstrate how this can improve the decoding of\nbrain states. Moreover, we show how explainable AI allows us to reconstruct the\ndifferential impact of each metric on the decoding, which additionally\nincreases spatial specificity of searchlight analysis. In general, this\nframework can be adapted to combine information derived from different imaging\nmodalities or experimental conditions, offering a versatile and interpretable\ntool for data fusion in neuroimaging.\n", "link": "http://arxiv.org/abs/2412.10161v1", "date": "2024-12-13", "relevancy": 1.913, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4823}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Integration%20with%20Fusion%20Searchlight%3A%20Classifying%20Brain%20States%20from%0A%20%20Resting-state%20fMRI&body=Title%3A%20Data%20Integration%20with%20Fusion%20Searchlight%3A%20Classifying%20Brain%20States%20from%0A%20%20Resting-state%20fMRI%0AAuthor%3A%20Simon%20Wein%20and%20Marco%20Riebel%20and%20Lisa-Marie%20Brunner%20and%20Caroline%20Nothdurfter%20and%20Rainer%20Rupprecht%20and%20Jens%20V.%20Schwarzbach%0AAbstract%3A%20%20%20Spontaneous%20neural%20activity%20observed%20in%20resting-state%20fMRI%20is%20characterized%0Aby%20complex%20spatio-temporal%20dynamics.%20Different%20measures%20related%20to%20local%20and%0Aglobal%20brain%20connectivity%20and%20fluctuations%20in%20low-frequency%20amplitudes%20can%0Aquantify%20individual%20aspects%20of%20these%20neural%20dynamics.%20Even%20though%20such%20measures%0Aare%20derived%20from%20the%20same%20functional%20signals%2C%20they%20are%20often%20evaluated%0Aseparately%2C%20neglecting%20their%20interrelations%20and%20potentially%20reducing%20the%0Aanalysis%20sensitivity.%20In%20our%20study%2C%20we%20present%20a%20fusion%20searchlight%20%28FuSL%29%0Aframework%20to%20combine%20the%20complementary%20information%20contained%20in%20different%0Aresting-state%20fMRI%20metrics%20and%20demonstrate%20how%20this%20can%20improve%20the%20decoding%20of%0Abrain%20states.%20Moreover%2C%20we%20show%20how%20explainable%20AI%20allows%20us%20to%20reconstruct%20the%0Adifferential%20impact%20of%20each%20metric%20on%20the%20decoding%2C%20which%20additionally%0Aincreases%20spatial%20specificity%20of%20searchlight%20analysis.%20In%20general%2C%20this%0Aframework%20can%20be%20adapted%20to%20combine%20information%20derived%20from%20different%20imaging%0Amodalities%20or%20experimental%20conditions%2C%20offering%20a%20versatile%20and%20interpretable%0Atool%20for%20data%20fusion%20in%20neuroimaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10161v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Integration%2520with%2520Fusion%2520Searchlight%253A%2520Classifying%2520Brain%2520States%2520from%250A%2520%2520Resting-state%2520fMRI%26entry.906535625%3DSimon%2520Wein%2520and%2520Marco%2520Riebel%2520and%2520Lisa-Marie%2520Brunner%2520and%2520Caroline%2520Nothdurfter%2520and%2520Rainer%2520Rupprecht%2520and%2520Jens%2520V.%2520Schwarzbach%26entry.1292438233%3D%2520%2520Spontaneous%2520neural%2520activity%2520observed%2520in%2520resting-state%2520fMRI%2520is%2520characterized%250Aby%2520complex%2520spatio-temporal%2520dynamics.%2520Different%2520measures%2520related%2520to%2520local%2520and%250Aglobal%2520brain%2520connectivity%2520and%2520fluctuations%2520in%2520low-frequency%2520amplitudes%2520can%250Aquantify%2520individual%2520aspects%2520of%2520these%2520neural%2520dynamics.%2520Even%2520though%2520such%2520measures%250Aare%2520derived%2520from%2520the%2520same%2520functional%2520signals%252C%2520they%2520are%2520often%2520evaluated%250Aseparately%252C%2520neglecting%2520their%2520interrelations%2520and%2520potentially%2520reducing%2520the%250Aanalysis%2520sensitivity.%2520In%2520our%2520study%252C%2520we%2520present%2520a%2520fusion%2520searchlight%2520%2528FuSL%2529%250Aframework%2520to%2520combine%2520the%2520complementary%2520information%2520contained%2520in%2520different%250Aresting-state%2520fMRI%2520metrics%2520and%2520demonstrate%2520how%2520this%2520can%2520improve%2520the%2520decoding%2520of%250Abrain%2520states.%2520Moreover%252C%2520we%2520show%2520how%2520explainable%2520AI%2520allows%2520us%2520to%2520reconstruct%2520the%250Adifferential%2520impact%2520of%2520each%2520metric%2520on%2520the%2520decoding%252C%2520which%2520additionally%250Aincreases%2520spatial%2520specificity%2520of%2520searchlight%2520analysis.%2520In%2520general%252C%2520this%250Aframework%2520can%2520be%2520adapted%2520to%2520combine%2520information%2520derived%2520from%2520different%2520imaging%250Amodalities%2520or%2520experimental%2520conditions%252C%2520offering%2520a%2520versatile%2520and%2520interpretable%250Atool%2520for%2520data%2520fusion%2520in%2520neuroimaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10161v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Integration%20with%20Fusion%20Searchlight%3A%20Classifying%20Brain%20States%20from%0A%20%20Resting-state%20fMRI&entry.906535625=Simon%20Wein%20and%20Marco%20Riebel%20and%20Lisa-Marie%20Brunner%20and%20Caroline%20Nothdurfter%20and%20Rainer%20Rupprecht%20and%20Jens%20V.%20Schwarzbach&entry.1292438233=%20%20Spontaneous%20neural%20activity%20observed%20in%20resting-state%20fMRI%20is%20characterized%0Aby%20complex%20spatio-temporal%20dynamics.%20Different%20measures%20related%20to%20local%20and%0Aglobal%20brain%20connectivity%20and%20fluctuations%20in%20low-frequency%20amplitudes%20can%0Aquantify%20individual%20aspects%20of%20these%20neural%20dynamics.%20Even%20though%20such%20measures%0Aare%20derived%20from%20the%20same%20functional%20signals%2C%20they%20are%20often%20evaluated%0Aseparately%2C%20neglecting%20their%20interrelations%20and%20potentially%20reducing%20the%0Aanalysis%20sensitivity.%20In%20our%20study%2C%20we%20present%20a%20fusion%20searchlight%20%28FuSL%29%0Aframework%20to%20combine%20the%20complementary%20information%20contained%20in%20different%0Aresting-state%20fMRI%20metrics%20and%20demonstrate%20how%20this%20can%20improve%20the%20decoding%20of%0Abrain%20states.%20Moreover%2C%20we%20show%20how%20explainable%20AI%20allows%20us%20to%20reconstruct%20the%0Adifferential%20impact%20of%20each%20metric%20on%20the%20decoding%2C%20which%20additionally%0Aincreases%20spatial%20specificity%20of%20searchlight%20analysis.%20In%20general%2C%20this%0Aframework%20can%20be%20adapted%20to%20combine%20information%20derived%20from%20different%20imaging%0Amodalities%20or%20experimental%20conditions%2C%20offering%20a%20versatile%20and%20interpretable%0Atool%20for%20data%20fusion%20in%20neuroimaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10161v1&entry.124074799=Read"},
{"title": "Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining", "author": "Zhiqi Ge and Juncheng Li and Xinglei Pang and Minghe Gao and Kaihang Pan and Wang Lin and Hao Fei and Wenqiao Zhang and Siliang Tang and Yueting Zhuang", "abstract": "  Digital agents are increasingly employed to automate tasks in interactive\ndigital environments such as web pages, software applications, and operating\nsystems. While text-based agents built on Large Language Models (LLMs) often\nrequire frequent updates due to platform-specific APIs, visual agents\nleveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability\nby interacting directly with Graphical User Interfaces (GUIs). However, these\nagents face significant challenges in visual perception, particularly when\nhandling high-resolution, visually complex digital environments. This paper\nintroduces Iris, a foundational visual agent that addresses these challenges\nthrough two key innovations: Information-Sensitive Cropping (ISC) and\nSelf-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes\nvisually dense regions using a edge detection algorithm, enabling efficient\nprocessing by allocating more computational resources to areas with higher\ninformation density. SRDL enhances the agent's ability to handle complex tasks\nby leveraging a dual-learning loop, where improvements in referring (describing\nUI elements) reinforce grounding (locating elements) and vice versa, all\nwithout requiring additional annotated data. Empirical evaluations demonstrate\nthat Iris achieves state-of-the-art performance across multiple benchmarks with\nonly 850K GUI annotations, outperforming methods using 10x more training data.\nThese improvements further translate to significant gains in both web and OS\nagent downstream tasks.\n", "link": "http://arxiv.org/abs/2412.10342v1", "date": "2024-12-13", "relevancy": 1.5915, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5562}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5232}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iris%3A%20Breaking%20GUI%20Complexity%20with%20Adaptive%20Focus%20and%20Self-Refining&body=Title%3A%20Iris%3A%20Breaking%20GUI%20Complexity%20with%20Adaptive%20Focus%20and%20Self-Refining%0AAuthor%3A%20Zhiqi%20Ge%20and%20Juncheng%20Li%20and%20Xinglei%20Pang%20and%20Minghe%20Gao%20and%20Kaihang%20Pan%20and%20Wang%20Lin%20and%20Hao%20Fei%20and%20Wenqiao%20Zhang%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Digital%20agents%20are%20increasingly%20employed%20to%20automate%20tasks%20in%20interactive%0Adigital%20environments%20such%20as%20web%20pages%2C%20software%20applications%2C%20and%20operating%0Asystems.%20While%20text-based%20agents%20built%20on%20Large%20Language%20Models%20%28LLMs%29%20often%0Arequire%20frequent%20updates%20due%20to%20platform-specific%20APIs%2C%20visual%20agents%0Aleveraging%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20offer%20enhanced%20adaptability%0Aby%20interacting%20directly%20with%20Graphical%20User%20Interfaces%20%28GUIs%29.%20However%2C%20these%0Aagents%20face%20significant%20challenges%20in%20visual%20perception%2C%20particularly%20when%0Ahandling%20high-resolution%2C%20visually%20complex%20digital%20environments.%20This%20paper%0Aintroduces%20Iris%2C%20a%20foundational%20visual%20agent%20that%20addresses%20these%20challenges%0Athrough%20two%20key%20innovations%3A%20Information-Sensitive%20Cropping%20%28ISC%29%20and%0ASelf-Refining%20Dual%20Learning%20%28SRDL%29.%20ISC%20dynamically%20identifies%20and%20prioritizes%0Avisually%20dense%20regions%20using%20a%20edge%20detection%20algorithm%2C%20enabling%20efficient%0Aprocessing%20by%20allocating%20more%20computational%20resources%20to%20areas%20with%20higher%0Ainformation%20density.%20SRDL%20enhances%20the%20agent%27s%20ability%20to%20handle%20complex%20tasks%0Aby%20leveraging%20a%20dual-learning%20loop%2C%20where%20improvements%20in%20referring%20%28describing%0AUI%20elements%29%20reinforce%20grounding%20%28locating%20elements%29%20and%20vice%20versa%2C%20all%0Awithout%20requiring%20additional%20annotated%20data.%20Empirical%20evaluations%20demonstrate%0Athat%20Iris%20achieves%20state-of-the-art%20performance%20across%20multiple%20benchmarks%20with%0Aonly%20850K%20GUI%20annotations%2C%20outperforming%20methods%20using%2010x%20more%20training%20data.%0AThese%20improvements%20further%20translate%20to%20significant%20gains%20in%20both%20web%20and%20OS%0Aagent%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIris%253A%2520Breaking%2520GUI%2520Complexity%2520with%2520Adaptive%2520Focus%2520and%2520Self-Refining%26entry.906535625%3DZhiqi%2520Ge%2520and%2520Juncheng%2520Li%2520and%2520Xinglei%2520Pang%2520and%2520Minghe%2520Gao%2520and%2520Kaihang%2520Pan%2520and%2520Wang%2520Lin%2520and%2520Hao%2520Fei%2520and%2520Wenqiao%2520Zhang%2520and%2520Siliang%2520Tang%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Digital%2520agents%2520are%2520increasingly%2520employed%2520to%2520automate%2520tasks%2520in%2520interactive%250Adigital%2520environments%2520such%2520as%2520web%2520pages%252C%2520software%2520applications%252C%2520and%2520operating%250Asystems.%2520While%2520text-based%2520agents%2520built%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520often%250Arequire%2520frequent%2520updates%2520due%2520to%2520platform-specific%2520APIs%252C%2520visual%2520agents%250Aleveraging%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520offer%2520enhanced%2520adaptability%250Aby%2520interacting%2520directly%2520with%2520Graphical%2520User%2520Interfaces%2520%2528GUIs%2529.%2520However%252C%2520these%250Aagents%2520face%2520significant%2520challenges%2520in%2520visual%2520perception%252C%2520particularly%2520when%250Ahandling%2520high-resolution%252C%2520visually%2520complex%2520digital%2520environments.%2520This%2520paper%250Aintroduces%2520Iris%252C%2520a%2520foundational%2520visual%2520agent%2520that%2520addresses%2520these%2520challenges%250Athrough%2520two%2520key%2520innovations%253A%2520Information-Sensitive%2520Cropping%2520%2528ISC%2529%2520and%250ASelf-Refining%2520Dual%2520Learning%2520%2528SRDL%2529.%2520ISC%2520dynamically%2520identifies%2520and%2520prioritizes%250Avisually%2520dense%2520regions%2520using%2520a%2520edge%2520detection%2520algorithm%252C%2520enabling%2520efficient%250Aprocessing%2520by%2520allocating%2520more%2520computational%2520resources%2520to%2520areas%2520with%2520higher%250Ainformation%2520density.%2520SRDL%2520enhances%2520the%2520agent%2527s%2520ability%2520to%2520handle%2520complex%2520tasks%250Aby%2520leveraging%2520a%2520dual-learning%2520loop%252C%2520where%2520improvements%2520in%2520referring%2520%2528describing%250AUI%2520elements%2529%2520reinforce%2520grounding%2520%2528locating%2520elements%2529%2520and%2520vice%2520versa%252C%2520all%250Awithout%2520requiring%2520additional%2520annotated%2520data.%2520Empirical%2520evaluations%2520demonstrate%250Athat%2520Iris%2520achieves%2520state-of-the-art%2520performance%2520across%2520multiple%2520benchmarks%2520with%250Aonly%2520850K%2520GUI%2520annotations%252C%2520outperforming%2520methods%2520using%252010x%2520more%2520training%2520data.%250AThese%2520improvements%2520further%2520translate%2520to%2520significant%2520gains%2520in%2520both%2520web%2520and%2520OS%250Aagent%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iris%3A%20Breaking%20GUI%20Complexity%20with%20Adaptive%20Focus%20and%20Self-Refining&entry.906535625=Zhiqi%20Ge%20and%20Juncheng%20Li%20and%20Xinglei%20Pang%20and%20Minghe%20Gao%20and%20Kaihang%20Pan%20and%20Wang%20Lin%20and%20Hao%20Fei%20and%20Wenqiao%20Zhang%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Digital%20agents%20are%20increasingly%20employed%20to%20automate%20tasks%20in%20interactive%0Adigital%20environments%20such%20as%20web%20pages%2C%20software%20applications%2C%20and%20operating%0Asystems.%20While%20text-based%20agents%20built%20on%20Large%20Language%20Models%20%28LLMs%29%20often%0Arequire%20frequent%20updates%20due%20to%20platform-specific%20APIs%2C%20visual%20agents%0Aleveraging%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20offer%20enhanced%20adaptability%0Aby%20interacting%20directly%20with%20Graphical%20User%20Interfaces%20%28GUIs%29.%20However%2C%20these%0Aagents%20face%20significant%20challenges%20in%20visual%20perception%2C%20particularly%20when%0Ahandling%20high-resolution%2C%20visually%20complex%20digital%20environments.%20This%20paper%0Aintroduces%20Iris%2C%20a%20foundational%20visual%20agent%20that%20addresses%20these%20challenges%0Athrough%20two%20key%20innovations%3A%20Information-Sensitive%20Cropping%20%28ISC%29%20and%0ASelf-Refining%20Dual%20Learning%20%28SRDL%29.%20ISC%20dynamically%20identifies%20and%20prioritizes%0Avisually%20dense%20regions%20using%20a%20edge%20detection%20algorithm%2C%20enabling%20efficient%0Aprocessing%20by%20allocating%20more%20computational%20resources%20to%20areas%20with%20higher%0Ainformation%20density.%20SRDL%20enhances%20the%20agent%27s%20ability%20to%20handle%20complex%20tasks%0Aby%20leveraging%20a%20dual-learning%20loop%2C%20where%20improvements%20in%20referring%20%28describing%0AUI%20elements%29%20reinforce%20grounding%20%28locating%20elements%29%20and%20vice%20versa%2C%20all%0Awithout%20requiring%20additional%20annotated%20data.%20Empirical%20evaluations%20demonstrate%0Athat%20Iris%20achieves%20state-of-the-art%20performance%20across%20multiple%20benchmarks%20with%0Aonly%20850K%20GUI%20annotations%2C%20outperforming%20methods%20using%2010x%20more%20training%20data.%0AThese%20improvements%20further%20translate%20to%20significant%20gains%20in%20both%20web%20and%20OS%0Aagent%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10342v1&entry.124074799=Read"},
{"title": "A Universal Degradation-based Bridging Technique for Domain Adaptive\n  Semantic Segmentation", "author": "Wangkai Li and Rui Sun and Tianzhu Zhang", "abstract": "  Semantic segmentation often suffers from significant performance degradation\nwhen the trained network is applied to a different domain. To address this\nissue, unsupervised domain adaptation (UDA) has been extensively studied.\nExisting methods introduce the domain bridging techniques to mitigate\nsubstantial domain gap, which construct intermediate domains to facilitate the\ngradual transfer of knowledge across different domains. However, these\nstrategies often require dataset-specific designs and may generate unnatural\nintermediate distributions that lead to semantic shift. In this paper, we\npropose DiDA, a universal degradation-based bridging technique formalized as a\ndiffusion forward process. DiDA consists of two key modules: (1)\nDegradation-based Intermediate Domain Construction, which creates continuous\nintermediate domains through simple image degradation operations to encourage\nlearning domain-invariant features as domain differences gradually diminish;\n(2) Semantic Shift Compensation, which leverages a diffusion encoder to encode\nand compensate for semantic shift information with degraded time-steps,\npreserving discriminative representations in the intermediate domains. As a\nplug-and-play solution, DiDA supports various degradation operations and\nseamlessly integrates with existing UDA methods. Extensive experiments on\nprevalent synthetic-to-real semantic segmentation benchmarks demonstrate that\nDiDA consistently improves performance across different settings and achieves\nnew state-of-the-art results when combined with existing methods.\n", "link": "http://arxiv.org/abs/2412.10339v1", "date": "2024-12-13", "relevancy": 1.6214, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5837}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5373}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Universal%20Degradation-based%20Bridging%20Technique%20for%20Domain%20Adaptive%0A%20%20Semantic%20Segmentation&body=Title%3A%20A%20Universal%20Degradation-based%20Bridging%20Technique%20for%20Domain%20Adaptive%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Wangkai%20Li%20and%20Rui%20Sun%20and%20Tianzhu%20Zhang%0AAbstract%3A%20%20%20Semantic%20segmentation%20often%20suffers%20from%20significant%20performance%20degradation%0Awhen%20the%20trained%20network%20is%20applied%20to%20a%20different%20domain.%20To%20address%20this%0Aissue%2C%20unsupervised%20domain%20adaptation%20%28UDA%29%20has%20been%20extensively%20studied.%0AExisting%20methods%20introduce%20the%20domain%20bridging%20techniques%20to%20mitigate%0Asubstantial%20domain%20gap%2C%20which%20construct%20intermediate%20domains%20to%20facilitate%20the%0Agradual%20transfer%20of%20knowledge%20across%20different%20domains.%20However%2C%20these%0Astrategies%20often%20require%20dataset-specific%20designs%20and%20may%20generate%20unnatural%0Aintermediate%20distributions%20that%20lead%20to%20semantic%20shift.%20In%20this%20paper%2C%20we%0Apropose%20DiDA%2C%20a%20universal%20degradation-based%20bridging%20technique%20formalized%20as%20a%0Adiffusion%20forward%20process.%20DiDA%20consists%20of%20two%20key%20modules%3A%20%281%29%0ADegradation-based%20Intermediate%20Domain%20Construction%2C%20which%20creates%20continuous%0Aintermediate%20domains%20through%20simple%20image%20degradation%20operations%20to%20encourage%0Alearning%20domain-invariant%20features%20as%20domain%20differences%20gradually%20diminish%3B%0A%282%29%20Semantic%20Shift%20Compensation%2C%20which%20leverages%20a%20diffusion%20encoder%20to%20encode%0Aand%20compensate%20for%20semantic%20shift%20information%20with%20degraded%20time-steps%2C%0Apreserving%20discriminative%20representations%20in%20the%20intermediate%20domains.%20As%20a%0Aplug-and-play%20solution%2C%20DiDA%20supports%20various%20degradation%20operations%20and%0Aseamlessly%20integrates%20with%20existing%20UDA%20methods.%20Extensive%20experiments%20on%0Aprevalent%20synthetic-to-real%20semantic%20segmentation%20benchmarks%20demonstrate%20that%0ADiDA%20consistently%20improves%20performance%20across%20different%20settings%20and%20achieves%0Anew%20state-of-the-art%20results%20when%20combined%20with%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Universal%2520Degradation-based%2520Bridging%2520Technique%2520for%2520Domain%2520Adaptive%250A%2520%2520Semantic%2520Segmentation%26entry.906535625%3DWangkai%2520Li%2520and%2520Rui%2520Sun%2520and%2520Tianzhu%2520Zhang%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520often%2520suffers%2520from%2520significant%2520performance%2520degradation%250Awhen%2520the%2520trained%2520network%2520is%2520applied%2520to%2520a%2520different%2520domain.%2520To%2520address%2520this%250Aissue%252C%2520unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520has%2520been%2520extensively%2520studied.%250AExisting%2520methods%2520introduce%2520the%2520domain%2520bridging%2520techniques%2520to%2520mitigate%250Asubstantial%2520domain%2520gap%252C%2520which%2520construct%2520intermediate%2520domains%2520to%2520facilitate%2520the%250Agradual%2520transfer%2520of%2520knowledge%2520across%2520different%2520domains.%2520However%252C%2520these%250Astrategies%2520often%2520require%2520dataset-specific%2520designs%2520and%2520may%2520generate%2520unnatural%250Aintermediate%2520distributions%2520that%2520lead%2520to%2520semantic%2520shift.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520DiDA%252C%2520a%2520universal%2520degradation-based%2520bridging%2520technique%2520formalized%2520as%2520a%250Adiffusion%2520forward%2520process.%2520DiDA%2520consists%2520of%2520two%2520key%2520modules%253A%2520%25281%2529%250ADegradation-based%2520Intermediate%2520Domain%2520Construction%252C%2520which%2520creates%2520continuous%250Aintermediate%2520domains%2520through%2520simple%2520image%2520degradation%2520operations%2520to%2520encourage%250Alearning%2520domain-invariant%2520features%2520as%2520domain%2520differences%2520gradually%2520diminish%253B%250A%25282%2529%2520Semantic%2520Shift%2520Compensation%252C%2520which%2520leverages%2520a%2520diffusion%2520encoder%2520to%2520encode%250Aand%2520compensate%2520for%2520semantic%2520shift%2520information%2520with%2520degraded%2520time-steps%252C%250Apreserving%2520discriminative%2520representations%2520in%2520the%2520intermediate%2520domains.%2520As%2520a%250Aplug-and-play%2520solution%252C%2520DiDA%2520supports%2520various%2520degradation%2520operations%2520and%250Aseamlessly%2520integrates%2520with%2520existing%2520UDA%2520methods.%2520Extensive%2520experiments%2520on%250Aprevalent%2520synthetic-to-real%2520semantic%2520segmentation%2520benchmarks%2520demonstrate%2520that%250ADiDA%2520consistently%2520improves%2520performance%2520across%2520different%2520settings%2520and%2520achieves%250Anew%2520state-of-the-art%2520results%2520when%2520combined%2520with%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Universal%20Degradation-based%20Bridging%20Technique%20for%20Domain%20Adaptive%0A%20%20Semantic%20Segmentation&entry.906535625=Wangkai%20Li%20and%20Rui%20Sun%20and%20Tianzhu%20Zhang&entry.1292438233=%20%20Semantic%20segmentation%20often%20suffers%20from%20significant%20performance%20degradation%0Awhen%20the%20trained%20network%20is%20applied%20to%20a%20different%20domain.%20To%20address%20this%0Aissue%2C%20unsupervised%20domain%20adaptation%20%28UDA%29%20has%20been%20extensively%20studied.%0AExisting%20methods%20introduce%20the%20domain%20bridging%20techniques%20to%20mitigate%0Asubstantial%20domain%20gap%2C%20which%20construct%20intermediate%20domains%20to%20facilitate%20the%0Agradual%20transfer%20of%20knowledge%20across%20different%20domains.%20However%2C%20these%0Astrategies%20often%20require%20dataset-specific%20designs%20and%20may%20generate%20unnatural%0Aintermediate%20distributions%20that%20lead%20to%20semantic%20shift.%20In%20this%20paper%2C%20we%0Apropose%20DiDA%2C%20a%20universal%20degradation-based%20bridging%20technique%20formalized%20as%20a%0Adiffusion%20forward%20process.%20DiDA%20consists%20of%20two%20key%20modules%3A%20%281%29%0ADegradation-based%20Intermediate%20Domain%20Construction%2C%20which%20creates%20continuous%0Aintermediate%20domains%20through%20simple%20image%20degradation%20operations%20to%20encourage%0Alearning%20domain-invariant%20features%20as%20domain%20differences%20gradually%20diminish%3B%0A%282%29%20Semantic%20Shift%20Compensation%2C%20which%20leverages%20a%20diffusion%20encoder%20to%20encode%0Aand%20compensate%20for%20semantic%20shift%20information%20with%20degraded%20time-steps%2C%0Apreserving%20discriminative%20representations%20in%20the%20intermediate%20domains.%20As%20a%0Aplug-and-play%20solution%2C%20DiDA%20supports%20various%20degradation%20operations%20and%0Aseamlessly%20integrates%20with%20existing%20UDA%20methods.%20Extensive%20experiments%20on%0Aprevalent%20synthetic-to-real%20semantic%20segmentation%20benchmarks%20demonstrate%20that%0ADiDA%20consistently%20improves%20performance%20across%20different%20settings%20and%20achieves%0Anew%20state-of-the-art%20results%20when%20combined%20with%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10339v1&entry.124074799=Read"},
{"title": "A Gated Residual Kolmogorov-Arnold Networks for Mixtures of Experts", "author": "Hugo Inzirillo and Remi Genet", "abstract": "  This paper introduces KAMoE, a novel Mixture of Experts (MoE) framework based\non Gated Residual Kolmogorov-Arnold Networks (GRKAN). We propose GRKAN as an\nalternative to the traditional gating function, aiming to enhance efficiency\nand interpretability in MoE modeling. Through extensive experiments on digital\nasset markets and real estate valuation, we demonstrate that KAMoE consistently\noutperforms traditional MoE architectures across various tasks and model types.\nOur results show that GRKAN exhibits superior performance compared to standard\nGating Residual Networks, particularly in LSTM-based models for sequential\ntasks. We also provide insights into the trade-offs between model complexity\nand performance gains in MoE and KAMoE architectures.\n", "link": "http://arxiv.org/abs/2409.15161v2", "date": "2024-12-13", "relevancy": 1.4092, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4728}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4687}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Gated%20Residual%20Kolmogorov-Arnold%20Networks%20for%20Mixtures%20of%20Experts&body=Title%3A%20A%20Gated%20Residual%20Kolmogorov-Arnold%20Networks%20for%20Mixtures%20of%20Experts%0AAuthor%3A%20Hugo%20Inzirillo%20and%20Remi%20Genet%0AAbstract%3A%20%20%20This%20paper%20introduces%20KAMoE%2C%20a%20novel%20Mixture%20of%20Experts%20%28MoE%29%20framework%20based%0Aon%20Gated%20Residual%20Kolmogorov-Arnold%20Networks%20%28GRKAN%29.%20We%20propose%20GRKAN%20as%20an%0Aalternative%20to%20the%20traditional%20gating%20function%2C%20aiming%20to%20enhance%20efficiency%0Aand%20interpretability%20in%20MoE%20modeling.%20Through%20extensive%20experiments%20on%20digital%0Aasset%20markets%20and%20real%20estate%20valuation%2C%20we%20demonstrate%20that%20KAMoE%20consistently%0Aoutperforms%20traditional%20MoE%20architectures%20across%20various%20tasks%20and%20model%20types.%0AOur%20results%20show%20that%20GRKAN%20exhibits%20superior%20performance%20compared%20to%20standard%0AGating%20Residual%20Networks%2C%20particularly%20in%20LSTM-based%20models%20for%20sequential%0Atasks.%20We%20also%20provide%20insights%20into%20the%20trade-offs%20between%20model%20complexity%0Aand%20performance%20gains%20in%20MoE%20and%20KAMoE%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15161v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Gated%2520Residual%2520Kolmogorov-Arnold%2520Networks%2520for%2520Mixtures%2520of%2520Experts%26entry.906535625%3DHugo%2520Inzirillo%2520and%2520Remi%2520Genet%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520KAMoE%252C%2520a%2520novel%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520framework%2520based%250Aon%2520Gated%2520Residual%2520Kolmogorov-Arnold%2520Networks%2520%2528GRKAN%2529.%2520We%2520propose%2520GRKAN%2520as%2520an%250Aalternative%2520to%2520the%2520traditional%2520gating%2520function%252C%2520aiming%2520to%2520enhance%2520efficiency%250Aand%2520interpretability%2520in%2520MoE%2520modeling.%2520Through%2520extensive%2520experiments%2520on%2520digital%250Aasset%2520markets%2520and%2520real%2520estate%2520valuation%252C%2520we%2520demonstrate%2520that%2520KAMoE%2520consistently%250Aoutperforms%2520traditional%2520MoE%2520architectures%2520across%2520various%2520tasks%2520and%2520model%2520types.%250AOur%2520results%2520show%2520that%2520GRKAN%2520exhibits%2520superior%2520performance%2520compared%2520to%2520standard%250AGating%2520Residual%2520Networks%252C%2520particularly%2520in%2520LSTM-based%2520models%2520for%2520sequential%250Atasks.%2520We%2520also%2520provide%2520insights%2520into%2520the%2520trade-offs%2520between%2520model%2520complexity%250Aand%2520performance%2520gains%2520in%2520MoE%2520and%2520KAMoE%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15161v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Gated%20Residual%20Kolmogorov-Arnold%20Networks%20for%20Mixtures%20of%20Experts&entry.906535625=Hugo%20Inzirillo%20and%20Remi%20Genet&entry.1292438233=%20%20This%20paper%20introduces%20KAMoE%2C%20a%20novel%20Mixture%20of%20Experts%20%28MoE%29%20framework%20based%0Aon%20Gated%20Residual%20Kolmogorov-Arnold%20Networks%20%28GRKAN%29.%20We%20propose%20GRKAN%20as%20an%0Aalternative%20to%20the%20traditional%20gating%20function%2C%20aiming%20to%20enhance%20efficiency%0Aand%20interpretability%20in%20MoE%20modeling.%20Through%20extensive%20experiments%20on%20digital%0Aasset%20markets%20and%20real%20estate%20valuation%2C%20we%20demonstrate%20that%20KAMoE%20consistently%0Aoutperforms%20traditional%20MoE%20architectures%20across%20various%20tasks%20and%20model%20types.%0AOur%20results%20show%20that%20GRKAN%20exhibits%20superior%20performance%20compared%20to%20standard%0AGating%20Residual%20Networks%2C%20particularly%20in%20LSTM-based%20models%20for%20sequential%0Atasks.%20We%20also%20provide%20insights%20into%20the%20trade-offs%20between%20model%20complexity%0Aand%20performance%20gains%20in%20MoE%20and%20KAMoE%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15161v2&entry.124074799=Read"},
{"title": "MST-R: Multi-Stage Tuning for Retrieval Systems and Metric Evaluation", "author": "Yash Malviya and Karan Dhingra and Maneesh Singh", "abstract": "  Regulatory documents are rich in nuanced terminology and specialized\nsemantics. FRAG systems: Frozen retrieval-augmented generators utilizing\npre-trained (or, frozen) components face consequent challenges with both\nretriever and answering performance. We present a system that adapts the\nretriever performance to the target domain using a multi-stage tuning (MST)\nstrategy. Our retrieval approach, called MST-R (a) first fine-tunes encoders\nused in vector stores using hard negative mining, (b) then uses a hybrid\nretriever, combining sparse and dense retrievers using reciprocal rank fusion,\nand then (c) adapts the cross-attention encoder by fine-tuning only the top-k\nretrieved results. We benchmark the system performance on the dataset released\nfor the RIRAG challenge (as part of the RegNLP workshop at COLING 2025). We\nachieve significant performance gains obtaining a top rank on the RegNLP\nchallenge leaderboard. We also show that a trivial answering approach games the\nRePASs metric outscoring all baselines and a pre-trained Llama model. Analyzing\nthis anomaly, we present important takeaways for future research.\n", "link": "http://arxiv.org/abs/2412.10313v1", "date": "2024-12-13", "relevancy": 1.4714, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5002}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4949}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MST-R%3A%20Multi-Stage%20Tuning%20for%20Retrieval%20Systems%20and%20Metric%20Evaluation&body=Title%3A%20MST-R%3A%20Multi-Stage%20Tuning%20for%20Retrieval%20Systems%20and%20Metric%20Evaluation%0AAuthor%3A%20Yash%20Malviya%20and%20Karan%20Dhingra%20and%20Maneesh%20Singh%0AAbstract%3A%20%20%20Regulatory%20documents%20are%20rich%20in%20nuanced%20terminology%20and%20specialized%0Asemantics.%20FRAG%20systems%3A%20Frozen%20retrieval-augmented%20generators%20utilizing%0Apre-trained%20%28or%2C%20frozen%29%20components%20face%20consequent%20challenges%20with%20both%0Aretriever%20and%20answering%20performance.%20We%20present%20a%20system%20that%20adapts%20the%0Aretriever%20performance%20to%20the%20target%20domain%20using%20a%20multi-stage%20tuning%20%28MST%29%0Astrategy.%20Our%20retrieval%20approach%2C%20called%20MST-R%20%28a%29%20first%20fine-tunes%20encoders%0Aused%20in%20vector%20stores%20using%20hard%20negative%20mining%2C%20%28b%29%20then%20uses%20a%20hybrid%0Aretriever%2C%20combining%20sparse%20and%20dense%20retrievers%20using%20reciprocal%20rank%20fusion%2C%0Aand%20then%20%28c%29%20adapts%20the%20cross-attention%20encoder%20by%20fine-tuning%20only%20the%20top-k%0Aretrieved%20results.%20We%20benchmark%20the%20system%20performance%20on%20the%20dataset%20released%0Afor%20the%20RIRAG%20challenge%20%28as%20part%20of%20the%20RegNLP%20workshop%20at%20COLING%202025%29.%20We%0Aachieve%20significant%20performance%20gains%20obtaining%20a%20top%20rank%20on%20the%20RegNLP%0Achallenge%20leaderboard.%20We%20also%20show%20that%20a%20trivial%20answering%20approach%20games%20the%0ARePASs%20metric%20outscoring%20all%20baselines%20and%20a%20pre-trained%20Llama%20model.%20Analyzing%0Athis%20anomaly%2C%20we%20present%20important%20takeaways%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMST-R%253A%2520Multi-Stage%2520Tuning%2520for%2520Retrieval%2520Systems%2520and%2520Metric%2520Evaluation%26entry.906535625%3DYash%2520Malviya%2520and%2520Karan%2520Dhingra%2520and%2520Maneesh%2520Singh%26entry.1292438233%3D%2520%2520Regulatory%2520documents%2520are%2520rich%2520in%2520nuanced%2520terminology%2520and%2520specialized%250Asemantics.%2520FRAG%2520systems%253A%2520Frozen%2520retrieval-augmented%2520generators%2520utilizing%250Apre-trained%2520%2528or%252C%2520frozen%2529%2520components%2520face%2520consequent%2520challenges%2520with%2520both%250Aretriever%2520and%2520answering%2520performance.%2520We%2520present%2520a%2520system%2520that%2520adapts%2520the%250Aretriever%2520performance%2520to%2520the%2520target%2520domain%2520using%2520a%2520multi-stage%2520tuning%2520%2528MST%2529%250Astrategy.%2520Our%2520retrieval%2520approach%252C%2520called%2520MST-R%2520%2528a%2529%2520first%2520fine-tunes%2520encoders%250Aused%2520in%2520vector%2520stores%2520using%2520hard%2520negative%2520mining%252C%2520%2528b%2529%2520then%2520uses%2520a%2520hybrid%250Aretriever%252C%2520combining%2520sparse%2520and%2520dense%2520retrievers%2520using%2520reciprocal%2520rank%2520fusion%252C%250Aand%2520then%2520%2528c%2529%2520adapts%2520the%2520cross-attention%2520encoder%2520by%2520fine-tuning%2520only%2520the%2520top-k%250Aretrieved%2520results.%2520We%2520benchmark%2520the%2520system%2520performance%2520on%2520the%2520dataset%2520released%250Afor%2520the%2520RIRAG%2520challenge%2520%2528as%2520part%2520of%2520the%2520RegNLP%2520workshop%2520at%2520COLING%25202025%2529.%2520We%250Aachieve%2520significant%2520performance%2520gains%2520obtaining%2520a%2520top%2520rank%2520on%2520the%2520RegNLP%250Achallenge%2520leaderboard.%2520We%2520also%2520show%2520that%2520a%2520trivial%2520answering%2520approach%2520games%2520the%250ARePASs%2520metric%2520outscoring%2520all%2520baselines%2520and%2520a%2520pre-trained%2520Llama%2520model.%2520Analyzing%250Athis%2520anomaly%252C%2520we%2520present%2520important%2520takeaways%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MST-R%3A%20Multi-Stage%20Tuning%20for%20Retrieval%20Systems%20and%20Metric%20Evaluation&entry.906535625=Yash%20Malviya%20and%20Karan%20Dhingra%20and%20Maneesh%20Singh&entry.1292438233=%20%20Regulatory%20documents%20are%20rich%20in%20nuanced%20terminology%20and%20specialized%0Asemantics.%20FRAG%20systems%3A%20Frozen%20retrieval-augmented%20generators%20utilizing%0Apre-trained%20%28or%2C%20frozen%29%20components%20face%20consequent%20challenges%20with%20both%0Aretriever%20and%20answering%20performance.%20We%20present%20a%20system%20that%20adapts%20the%0Aretriever%20performance%20to%20the%20target%20domain%20using%20a%20multi-stage%20tuning%20%28MST%29%0Astrategy.%20Our%20retrieval%20approach%2C%20called%20MST-R%20%28a%29%20first%20fine-tunes%20encoders%0Aused%20in%20vector%20stores%20using%20hard%20negative%20mining%2C%20%28b%29%20then%20uses%20a%20hybrid%0Aretriever%2C%20combining%20sparse%20and%20dense%20retrievers%20using%20reciprocal%20rank%20fusion%2C%0Aand%20then%20%28c%29%20adapts%20the%20cross-attention%20encoder%20by%20fine-tuning%20only%20the%20top-k%0Aretrieved%20results.%20We%20benchmark%20the%20system%20performance%20on%20the%20dataset%20released%0Afor%20the%20RIRAG%20challenge%20%28as%20part%20of%20the%20RegNLP%20workshop%20at%20COLING%202025%29.%20We%0Aachieve%20significant%20performance%20gains%20obtaining%20a%20top%20rank%20on%20the%20RegNLP%0Achallenge%20leaderboard.%20We%20also%20show%20that%20a%20trivial%20answering%20approach%20games%20the%0ARePASs%20metric%20outscoring%20all%20baselines%20and%20a%20pre-trained%20Llama%20model.%20Analyzing%0Athis%20anomaly%2C%20we%20present%20important%20takeaways%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10313v1&entry.124074799=Read"},
{"title": "Learning payoffs while routing in skill-based queues", "author": "Sanne van Kempen and Jaron Sanders and Fiona Sloothaak and Maarten G. Wolf", "abstract": "  Motivated by applications in service systems, we consider queueing systems\nwhere each customer must be handled by a server with the right skill set. We\nfocus on optimizing the routing of customers to servers in order to maximize\nthe total payoff of customer--server matches. In addition, customer--server\ndependent payoff parameters are assumed to be unknown a priori. We construct a\nmachine learning algorithm that adaptively learns the payoff parameters while\nmaximizing the total payoff and prove that it achieves polylogarithmic regret.\nMoreover, we show that the algorithm is asymptotically optimal up to\nlogarithmic terms by deriving a regret lower bound. The algorithm leverages the\nbasic feasible solutions of a static linear program as the action space. The\nregret analysis overcomes the complex interplay between queueing and learning\nby analyzing the convergence of the queue length process to its stationary\nbehavior. We also demonstrate the performance of the algorithm numerically, and\nhave included an experiment with time-varying parameters highlighting the\npotential of the algorithm in non-static environments.\n", "link": "http://arxiv.org/abs/2412.10168v1", "date": "2024-12-13", "relevancy": 1.6142, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.423}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.411}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20payoffs%20while%20routing%20in%20skill-based%20queues&body=Title%3A%20Learning%20payoffs%20while%20routing%20in%20skill-based%20queues%0AAuthor%3A%20Sanne%20van%20Kempen%20and%20Jaron%20Sanders%20and%20Fiona%20Sloothaak%20and%20Maarten%20G.%20Wolf%0AAbstract%3A%20%20%20Motivated%20by%20applications%20in%20service%20systems%2C%20we%20consider%20queueing%20systems%0Awhere%20each%20customer%20must%20be%20handled%20by%20a%20server%20with%20the%20right%20skill%20set.%20We%0Afocus%20on%20optimizing%20the%20routing%20of%20customers%20to%20servers%20in%20order%20to%20maximize%0Athe%20total%20payoff%20of%20customer--server%20matches.%20In%20addition%2C%20customer--server%0Adependent%20payoff%20parameters%20are%20assumed%20to%20be%20unknown%20a%20priori.%20We%20construct%20a%0Amachine%20learning%20algorithm%20that%20adaptively%20learns%20the%20payoff%20parameters%20while%0Amaximizing%20the%20total%20payoff%20and%20prove%20that%20it%20achieves%20polylogarithmic%20regret.%0AMoreover%2C%20we%20show%20that%20the%20algorithm%20is%20asymptotically%20optimal%20up%20to%0Alogarithmic%20terms%20by%20deriving%20a%20regret%20lower%20bound.%20The%20algorithm%20leverages%20the%0Abasic%20feasible%20solutions%20of%20a%20static%20linear%20program%20as%20the%20action%20space.%20The%0Aregret%20analysis%20overcomes%20the%20complex%20interplay%20between%20queueing%20and%20learning%0Aby%20analyzing%20the%20convergence%20of%20the%20queue%20length%20process%20to%20its%20stationary%0Abehavior.%20We%20also%20demonstrate%20the%20performance%20of%20the%20algorithm%20numerically%2C%20and%0Ahave%20included%20an%20experiment%20with%20time-varying%20parameters%20highlighting%20the%0Apotential%20of%20the%20algorithm%20in%20non-static%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520payoffs%2520while%2520routing%2520in%2520skill-based%2520queues%26entry.906535625%3DSanne%2520van%2520Kempen%2520and%2520Jaron%2520Sanders%2520and%2520Fiona%2520Sloothaak%2520and%2520Maarten%2520G.%2520Wolf%26entry.1292438233%3D%2520%2520Motivated%2520by%2520applications%2520in%2520service%2520systems%252C%2520we%2520consider%2520queueing%2520systems%250Awhere%2520each%2520customer%2520must%2520be%2520handled%2520by%2520a%2520server%2520with%2520the%2520right%2520skill%2520set.%2520We%250Afocus%2520on%2520optimizing%2520the%2520routing%2520of%2520customers%2520to%2520servers%2520in%2520order%2520to%2520maximize%250Athe%2520total%2520payoff%2520of%2520customer--server%2520matches.%2520In%2520addition%252C%2520customer--server%250Adependent%2520payoff%2520parameters%2520are%2520assumed%2520to%2520be%2520unknown%2520a%2520priori.%2520We%2520construct%2520a%250Amachine%2520learning%2520algorithm%2520that%2520adaptively%2520learns%2520the%2520payoff%2520parameters%2520while%250Amaximizing%2520the%2520total%2520payoff%2520and%2520prove%2520that%2520it%2520achieves%2520polylogarithmic%2520regret.%250AMoreover%252C%2520we%2520show%2520that%2520the%2520algorithm%2520is%2520asymptotically%2520optimal%2520up%2520to%250Alogarithmic%2520terms%2520by%2520deriving%2520a%2520regret%2520lower%2520bound.%2520The%2520algorithm%2520leverages%2520the%250Abasic%2520feasible%2520solutions%2520of%2520a%2520static%2520linear%2520program%2520as%2520the%2520action%2520space.%2520The%250Aregret%2520analysis%2520overcomes%2520the%2520complex%2520interplay%2520between%2520queueing%2520and%2520learning%250Aby%2520analyzing%2520the%2520convergence%2520of%2520the%2520queue%2520length%2520process%2520to%2520its%2520stationary%250Abehavior.%2520We%2520also%2520demonstrate%2520the%2520performance%2520of%2520the%2520algorithm%2520numerically%252C%2520and%250Ahave%2520included%2520an%2520experiment%2520with%2520time-varying%2520parameters%2520highlighting%2520the%250Apotential%2520of%2520the%2520algorithm%2520in%2520non-static%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20payoffs%20while%20routing%20in%20skill-based%20queues&entry.906535625=Sanne%20van%20Kempen%20and%20Jaron%20Sanders%20and%20Fiona%20Sloothaak%20and%20Maarten%20G.%20Wolf&entry.1292438233=%20%20Motivated%20by%20applications%20in%20service%20systems%2C%20we%20consider%20queueing%20systems%0Awhere%20each%20customer%20must%20be%20handled%20by%20a%20server%20with%20the%20right%20skill%20set.%20We%0Afocus%20on%20optimizing%20the%20routing%20of%20customers%20to%20servers%20in%20order%20to%20maximize%0Athe%20total%20payoff%20of%20customer--server%20matches.%20In%20addition%2C%20customer--server%0Adependent%20payoff%20parameters%20are%20assumed%20to%20be%20unknown%20a%20priori.%20We%20construct%20a%0Amachine%20learning%20algorithm%20that%20adaptively%20learns%20the%20payoff%20parameters%20while%0Amaximizing%20the%20total%20payoff%20and%20prove%20that%20it%20achieves%20polylogarithmic%20regret.%0AMoreover%2C%20we%20show%20that%20the%20algorithm%20is%20asymptotically%20optimal%20up%20to%0Alogarithmic%20terms%20by%20deriving%20a%20regret%20lower%20bound.%20The%20algorithm%20leverages%20the%0Abasic%20feasible%20solutions%20of%20a%20static%20linear%20program%20as%20the%20action%20space.%20The%0Aregret%20analysis%20overcomes%20the%20complex%20interplay%20between%20queueing%20and%20learning%0Aby%20analyzing%20the%20convergence%20of%20the%20queue%20length%20process%20to%20its%20stationary%0Abehavior.%20We%20also%20demonstrate%20the%20performance%20of%20the%20algorithm%20numerically%2C%20and%0Ahave%20included%20an%20experiment%20with%20time-varying%20parameters%20highlighting%20the%0Apotential%20of%20the%20algorithm%20in%20non-static%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10168v1&entry.124074799=Read"},
{"title": "A Clinical Tuning Framework for Continuous Kinematic and Impedance\n  Control of a Powered Knee-Ankle Prosthesis", "author": "Emma Reznick and T. Kevin Best and Robert Gregg", "abstract": "  Objective: Configuring a prosthetic leg is an integral part of the fitting\nprocess, but the personalization of a multi-modal powered knee-ankle prosthesis\nis often too complex to realize in a clinical environment. This paper develops\nboth the technical means to individualize a hybrid kinematic-impedance\ncontroller for variable-incline walking and sit-stand transitions, and an\nintuitive Clinical Tuning Interface (CTI) that allows prosthetists to directly\nmodify the controller behavior.\n  Methods: Utilizing an established method for predicting kinematic gait\nindividuality alongside a new parallel approach for kinetic individuality, we\napplied tuned characteristics exclusively from level-ground walking to\npersonalize continuous-phase/task models of joint kinematics and impedance. To\ntake advantage of this method, we developed a CTI that translates common\nclinical tuning parameters into model adjustments. We then conducted a case\nstudy involving an above-knee amputee participant where a prosthetist\niteratively tuned the prosthesis in a simulated clinical session involving\nwalking and sit-stand transitions.\n  Results: The prosthetist fully tuned the multi-activity prosthesis controller\nin under 20 min. Each iteration of tuning (i.e., observation, parameter\nadjustment, and model reprocessing) took 2 min on average for walking and 1 min\non average for sit-stand. The tuned behavior changes were appropriately\nmanifested in the commanded prosthesis torques, both at the tuned tasks and\nacross untuned tasks (inclines).\n  Conclusion: The CTI leveraged able-bodied trends to efficiently personalize a\nwide array of walking tasks and sit-stand transitions. A case-study validated\nthe CTI tuning method and demonstrated the efficiency necessary for powered\nknee-ankle prostheses to become clinically viable.\n", "link": "http://arxiv.org/abs/2412.10154v1", "date": "2024-12-13", "relevancy": 1.5195, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5335}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5162}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Clinical%20Tuning%20Framework%20for%20Continuous%20Kinematic%20and%20Impedance%0A%20%20Control%20of%20a%20Powered%20Knee-Ankle%20Prosthesis&body=Title%3A%20A%20Clinical%20Tuning%20Framework%20for%20Continuous%20Kinematic%20and%20Impedance%0A%20%20Control%20of%20a%20Powered%20Knee-Ankle%20Prosthesis%0AAuthor%3A%20Emma%20Reznick%20and%20T.%20Kevin%20Best%20and%20Robert%20Gregg%0AAbstract%3A%20%20%20Objective%3A%20Configuring%20a%20prosthetic%20leg%20is%20an%20integral%20part%20of%20the%20fitting%0Aprocess%2C%20but%20the%20personalization%20of%20a%20multi-modal%20powered%20knee-ankle%20prosthesis%0Ais%20often%20too%20complex%20to%20realize%20in%20a%20clinical%20environment.%20This%20paper%20develops%0Aboth%20the%20technical%20means%20to%20individualize%20a%20hybrid%20kinematic-impedance%0Acontroller%20for%20variable-incline%20walking%20and%20sit-stand%20transitions%2C%20and%20an%0Aintuitive%20Clinical%20Tuning%20Interface%20%28CTI%29%20that%20allows%20prosthetists%20to%20directly%0Amodify%20the%20controller%20behavior.%0A%20%20Methods%3A%20Utilizing%20an%20established%20method%20for%20predicting%20kinematic%20gait%0Aindividuality%20alongside%20a%20new%20parallel%20approach%20for%20kinetic%20individuality%2C%20we%0Aapplied%20tuned%20characteristics%20exclusively%20from%20level-ground%20walking%20to%0Apersonalize%20continuous-phase/task%20models%20of%20joint%20kinematics%20and%20impedance.%20To%0Atake%20advantage%20of%20this%20method%2C%20we%20developed%20a%20CTI%20that%20translates%20common%0Aclinical%20tuning%20parameters%20into%20model%20adjustments.%20We%20then%20conducted%20a%20case%0Astudy%20involving%20an%20above-knee%20amputee%20participant%20where%20a%20prosthetist%0Aiteratively%20tuned%20the%20prosthesis%20in%20a%20simulated%20clinical%20session%20involving%0Awalking%20and%20sit-stand%20transitions.%0A%20%20Results%3A%20The%20prosthetist%20fully%20tuned%20the%20multi-activity%20prosthesis%20controller%0Ain%20under%2020%20min.%20Each%20iteration%20of%20tuning%20%28i.e.%2C%20observation%2C%20parameter%0Aadjustment%2C%20and%20model%20reprocessing%29%20took%202%20min%20on%20average%20for%20walking%20and%201%20min%0Aon%20average%20for%20sit-stand.%20The%20tuned%20behavior%20changes%20were%20appropriately%0Amanifested%20in%20the%20commanded%20prosthesis%20torques%2C%20both%20at%20the%20tuned%20tasks%20and%0Aacross%20untuned%20tasks%20%28inclines%29.%0A%20%20Conclusion%3A%20The%20CTI%20leveraged%20able-bodied%20trends%20to%20efficiently%20personalize%20a%0Awide%20array%20of%20walking%20tasks%20and%20sit-stand%20transitions.%20A%20case-study%20validated%0Athe%20CTI%20tuning%20method%20and%20demonstrated%20the%20efficiency%20necessary%20for%20powered%0Aknee-ankle%20prostheses%20to%20become%20clinically%20viable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Clinical%2520Tuning%2520Framework%2520for%2520Continuous%2520Kinematic%2520and%2520Impedance%250A%2520%2520Control%2520of%2520a%2520Powered%2520Knee-Ankle%2520Prosthesis%26entry.906535625%3DEmma%2520Reznick%2520and%2520T.%2520Kevin%2520Best%2520and%2520Robert%2520Gregg%26entry.1292438233%3D%2520%2520Objective%253A%2520Configuring%2520a%2520prosthetic%2520leg%2520is%2520an%2520integral%2520part%2520of%2520the%2520fitting%250Aprocess%252C%2520but%2520the%2520personalization%2520of%2520a%2520multi-modal%2520powered%2520knee-ankle%2520prosthesis%250Ais%2520often%2520too%2520complex%2520to%2520realize%2520in%2520a%2520clinical%2520environment.%2520This%2520paper%2520develops%250Aboth%2520the%2520technical%2520means%2520to%2520individualize%2520a%2520hybrid%2520kinematic-impedance%250Acontroller%2520for%2520variable-incline%2520walking%2520and%2520sit-stand%2520transitions%252C%2520and%2520an%250Aintuitive%2520Clinical%2520Tuning%2520Interface%2520%2528CTI%2529%2520that%2520allows%2520prosthetists%2520to%2520directly%250Amodify%2520the%2520controller%2520behavior.%250A%2520%2520Methods%253A%2520Utilizing%2520an%2520established%2520method%2520for%2520predicting%2520kinematic%2520gait%250Aindividuality%2520alongside%2520a%2520new%2520parallel%2520approach%2520for%2520kinetic%2520individuality%252C%2520we%250Aapplied%2520tuned%2520characteristics%2520exclusively%2520from%2520level-ground%2520walking%2520to%250Apersonalize%2520continuous-phase/task%2520models%2520of%2520joint%2520kinematics%2520and%2520impedance.%2520To%250Atake%2520advantage%2520of%2520this%2520method%252C%2520we%2520developed%2520a%2520CTI%2520that%2520translates%2520common%250Aclinical%2520tuning%2520parameters%2520into%2520model%2520adjustments.%2520We%2520then%2520conducted%2520a%2520case%250Astudy%2520involving%2520an%2520above-knee%2520amputee%2520participant%2520where%2520a%2520prosthetist%250Aiteratively%2520tuned%2520the%2520prosthesis%2520in%2520a%2520simulated%2520clinical%2520session%2520involving%250Awalking%2520and%2520sit-stand%2520transitions.%250A%2520%2520Results%253A%2520The%2520prosthetist%2520fully%2520tuned%2520the%2520multi-activity%2520prosthesis%2520controller%250Ain%2520under%252020%2520min.%2520Each%2520iteration%2520of%2520tuning%2520%2528i.e.%252C%2520observation%252C%2520parameter%250Aadjustment%252C%2520and%2520model%2520reprocessing%2529%2520took%25202%2520min%2520on%2520average%2520for%2520walking%2520and%25201%2520min%250Aon%2520average%2520for%2520sit-stand.%2520The%2520tuned%2520behavior%2520changes%2520were%2520appropriately%250Amanifested%2520in%2520the%2520commanded%2520prosthesis%2520torques%252C%2520both%2520at%2520the%2520tuned%2520tasks%2520and%250Aacross%2520untuned%2520tasks%2520%2528inclines%2529.%250A%2520%2520Conclusion%253A%2520The%2520CTI%2520leveraged%2520able-bodied%2520trends%2520to%2520efficiently%2520personalize%2520a%250Awide%2520array%2520of%2520walking%2520tasks%2520and%2520sit-stand%2520transitions.%2520A%2520case-study%2520validated%250Athe%2520CTI%2520tuning%2520method%2520and%2520demonstrated%2520the%2520efficiency%2520necessary%2520for%2520powered%250Aknee-ankle%2520prostheses%2520to%2520become%2520clinically%2520viable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Clinical%20Tuning%20Framework%20for%20Continuous%20Kinematic%20and%20Impedance%0A%20%20Control%20of%20a%20Powered%20Knee-Ankle%20Prosthesis&entry.906535625=Emma%20Reznick%20and%20T.%20Kevin%20Best%20and%20Robert%20Gregg&entry.1292438233=%20%20Objective%3A%20Configuring%20a%20prosthetic%20leg%20is%20an%20integral%20part%20of%20the%20fitting%0Aprocess%2C%20but%20the%20personalization%20of%20a%20multi-modal%20powered%20knee-ankle%20prosthesis%0Ais%20often%20too%20complex%20to%20realize%20in%20a%20clinical%20environment.%20This%20paper%20develops%0Aboth%20the%20technical%20means%20to%20individualize%20a%20hybrid%20kinematic-impedance%0Acontroller%20for%20variable-incline%20walking%20and%20sit-stand%20transitions%2C%20and%20an%0Aintuitive%20Clinical%20Tuning%20Interface%20%28CTI%29%20that%20allows%20prosthetists%20to%20directly%0Amodify%20the%20controller%20behavior.%0A%20%20Methods%3A%20Utilizing%20an%20established%20method%20for%20predicting%20kinematic%20gait%0Aindividuality%20alongside%20a%20new%20parallel%20approach%20for%20kinetic%20individuality%2C%20we%0Aapplied%20tuned%20characteristics%20exclusively%20from%20level-ground%20walking%20to%0Apersonalize%20continuous-phase/task%20models%20of%20joint%20kinematics%20and%20impedance.%20To%0Atake%20advantage%20of%20this%20method%2C%20we%20developed%20a%20CTI%20that%20translates%20common%0Aclinical%20tuning%20parameters%20into%20model%20adjustments.%20We%20then%20conducted%20a%20case%0Astudy%20involving%20an%20above-knee%20amputee%20participant%20where%20a%20prosthetist%0Aiteratively%20tuned%20the%20prosthesis%20in%20a%20simulated%20clinical%20session%20involving%0Awalking%20and%20sit-stand%20transitions.%0A%20%20Results%3A%20The%20prosthetist%20fully%20tuned%20the%20multi-activity%20prosthesis%20controller%0Ain%20under%2020%20min.%20Each%20iteration%20of%20tuning%20%28i.e.%2C%20observation%2C%20parameter%0Aadjustment%2C%20and%20model%20reprocessing%29%20took%202%20min%20on%20average%20for%20walking%20and%201%20min%0Aon%20average%20for%20sit-stand.%20The%20tuned%20behavior%20changes%20were%20appropriately%0Amanifested%20in%20the%20commanded%20prosthesis%20torques%2C%20both%20at%20the%20tuned%20tasks%20and%0Aacross%20untuned%20tasks%20%28inclines%29.%0A%20%20Conclusion%3A%20The%20CTI%20leveraged%20able-bodied%20trends%20to%20efficiently%20personalize%20a%0Awide%20array%20of%20walking%20tasks%20and%20sit-stand%20transitions.%20A%20case-study%20validated%0Athe%20CTI%20tuning%20method%20and%20demonstrated%20the%20efficiency%20necessary%20for%20powered%0Aknee-ankle%20prostheses%20to%20become%20clinically%20viable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10154v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


