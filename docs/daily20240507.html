<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240506.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review", "author": "Anurag Dalal and Daniel Hagen and Kjell G. Robbersmyr and Kristian Muri Knausg\u00e5rd", "abstract": "  Image-based 3D reconstruction is a challenging task that involves inferring\nthe 3D shape of an object or scene from a set of input images. Learning-based\nmethods have gained attention for their ability to directly estimate 3D shapes.\nThis review paper focuses on state-of-the-art techniques for 3D reconstruction,\nincluding the generation of novel, unseen views. An overview of recent\ndevelopments in the Gaussian Splatting method is provided, covering input\ntypes, model structures, output representations, and training strategies.\nUnresolved challenges and future directions are also discussed. Given the rapid\nprogress in this domain and the numerous opportunities for enhancing 3D\nreconstruction methods, a comprehensive examination of algorithms appears\nessential. Consequently, this study offers a thorough overview of the latest\nadvancements in Gaussian Splatting.\n", "link": "http://arxiv.org/abs/2405.03417v1", "date": "2024-05-06", "relevancy": 3.1808, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7032}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6306}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Splatting%3A%203D%20Reconstruction%20and%20Novel%20View%20Synthesis%2C%20a%20Review&body=Title%3A%20Gaussian%20Splatting%3A%203D%20Reconstruction%20and%20Novel%20View%20Synthesis%2C%20a%20Review%0AAuthor%3A%20Anurag%20Dalal%20and%20Daniel%20Hagen%20and%20Kjell%20G.%20Robbersmyr%20and%20Kristian%20Muri%20Knausg%C3%A5rd%0AAbstract%3A%20%20%20Image-based%203D%20reconstruction%20is%20a%20challenging%20task%20that%20involves%20inferring%0Athe%203D%20shape%20of%20an%20object%20or%20scene%20from%20a%20set%20of%20input%20images.%20Learning-based%0Amethods%20have%20gained%20attention%20for%20their%20ability%20to%20directly%20estimate%203D%20shapes.%0AThis%20review%20paper%20focuses%20on%20state-of-the-art%20techniques%20for%203D%20reconstruction%2C%0Aincluding%20the%20generation%20of%20novel%2C%20unseen%20views.%20An%20overview%20of%20recent%0Adevelopments%20in%20the%20Gaussian%20Splatting%20method%20is%20provided%2C%20covering%20input%0Atypes%2C%20model%20structures%2C%20output%20representations%2C%20and%20training%20strategies.%0AUnresolved%20challenges%20and%20future%20directions%20are%20also%20discussed.%20Given%20the%20rapid%0Aprogress%20in%20this%20domain%20and%20the%20numerous%20opportunities%20for%20enhancing%203D%0Areconstruction%20methods%2C%20a%20comprehensive%20examination%20of%20algorithms%20appears%0Aessential.%20Consequently%2C%20this%20study%20offers%20a%20thorough%20overview%20of%20the%20latest%0Aadvancements%20in%20Gaussian%20Splatting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Splatting%253A%25203D%2520Reconstruction%2520and%2520Novel%2520View%2520Synthesis%252C%2520a%2520Review%26entry.906535625%3DAnurag%2520Dalal%2520and%2520Daniel%2520Hagen%2520and%2520Kjell%2520G.%2520Robbersmyr%2520and%2520Kristian%2520Muri%2520Knausg%25C3%25A5rd%26entry.1292438233%3D%2520%2520Image-based%25203D%2520reconstruction%2520is%2520a%2520challenging%2520task%2520that%2520involves%2520inferring%250Athe%25203D%2520shape%2520of%2520an%2520object%2520or%2520scene%2520from%2520a%2520set%2520of%2520input%2520images.%2520Learning-based%250Amethods%2520have%2520gained%2520attention%2520for%2520their%2520ability%2520to%2520directly%2520estimate%25203D%2520shapes.%250AThis%2520review%2520paper%2520focuses%2520on%2520state-of-the-art%2520techniques%2520for%25203D%2520reconstruction%252C%250Aincluding%2520the%2520generation%2520of%2520novel%252C%2520unseen%2520views.%2520An%2520overview%2520of%2520recent%250Adevelopments%2520in%2520the%2520Gaussian%2520Splatting%2520method%2520is%2520provided%252C%2520covering%2520input%250Atypes%252C%2520model%2520structures%252C%2520output%2520representations%252C%2520and%2520training%2520strategies.%250AUnresolved%2520challenges%2520and%2520future%2520directions%2520are%2520also%2520discussed.%2520Given%2520the%2520rapid%250Aprogress%2520in%2520this%2520domain%2520and%2520the%2520numerous%2520opportunities%2520for%2520enhancing%25203D%250Areconstruction%2520methods%252C%2520a%2520comprehensive%2520examination%2520of%2520algorithms%2520appears%250Aessential.%2520Consequently%252C%2520this%2520study%2520offers%2520a%2520thorough%2520overview%2520of%2520the%2520latest%250Aadvancements%2520in%2520Gaussian%2520Splatting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Splatting%3A%203D%20Reconstruction%20and%20Novel%20View%20Synthesis%2C%20a%20Review&entry.906535625=Anurag%20Dalal%20and%20Daniel%20Hagen%20and%20Kjell%20G.%20Robbersmyr%20and%20Kristian%20Muri%20Knausg%C3%A5rd&entry.1292438233=%20%20Image-based%203D%20reconstruction%20is%20a%20challenging%20task%20that%20involves%20inferring%0Athe%203D%20shape%20of%20an%20object%20or%20scene%20from%20a%20set%20of%20input%20images.%20Learning-based%0Amethods%20have%20gained%20attention%20for%20their%20ability%20to%20directly%20estimate%203D%20shapes.%0AThis%20review%20paper%20focuses%20on%20state-of-the-art%20techniques%20for%203D%20reconstruction%2C%0Aincluding%20the%20generation%20of%20novel%2C%20unseen%20views.%20An%20overview%20of%20recent%0Adevelopments%20in%20the%20Gaussian%20Splatting%20method%20is%20provided%2C%20covering%20input%0Atypes%2C%20model%20structures%2C%20output%20representations%2C%20and%20training%20strategies.%0AUnresolved%20challenges%20and%20future%20directions%20are%20also%20discussed.%20Given%20the%20rapid%0Aprogress%20in%20this%20domain%20and%20the%20numerous%20opportunities%20for%20enhancing%203D%0Areconstruction%20methods%2C%20a%20comprehensive%20examination%20of%20algorithms%20appears%0Aessential.%20Consequently%2C%20this%20study%20offers%20a%20thorough%20overview%20of%20the%20latest%0Aadvancements%20in%20Gaussian%20Splatting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03417v1&entry.124074799=Read"},
{"title": "SL-SLAM: A robust visual-inertial SLAM based deep feature extraction and\n  matching", "author": "Zhang Xiao and Shuaixin Li", "abstract": "  This paper explores how deep learning techniques can improve visual-based\nSLAM performance in challenging environments. By combining deep feature\nextraction and deep matching methods, we introduce a versatile hybrid visual\nSLAM system designed to enhance adaptability in challenging scenarios, such as\nlow-light conditions, dynamic lighting, weak-texture areas, and severe jitter.\nOur system supports multiple modes, including monocular, stereo,\nmonocular-inertial, and stereo-inertial configurations. We also perform\nanalysis how to combine visual SLAM with deep learning methods to enlighten\nother researches. Through extensive experiments on both public datasets and\nself-sampled data, we demonstrate the superiority of the SL-SLAM system over\ntraditional approaches. The experimental results show that SL-SLAM outperforms\nstate-of-the-art SLAM algorithms in terms of localization accuracy and tracking\nrobustness. For the benefit of community, we make public the source code at\nhttps://github.com/zzzzxxxx111/SLslam.\n", "link": "http://arxiv.org/abs/2405.03413v1", "date": "2024-05-06", "relevancy": 3.1171, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6426}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6256}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SL-SLAM%3A%20A%20robust%20visual-inertial%20SLAM%20based%20deep%20feature%20extraction%20and%0A%20%20matching&body=Title%3A%20SL-SLAM%3A%20A%20robust%20visual-inertial%20SLAM%20based%20deep%20feature%20extraction%20and%0A%20%20matching%0AAuthor%3A%20Zhang%20Xiao%20and%20Shuaixin%20Li%0AAbstract%3A%20%20%20This%20paper%20explores%20how%20deep%20learning%20techniques%20can%20improve%20visual-based%0ASLAM%20performance%20in%20challenging%20environments.%20By%20combining%20deep%20feature%0Aextraction%20and%20deep%20matching%20methods%2C%20we%20introduce%20a%20versatile%20hybrid%20visual%0ASLAM%20system%20designed%20to%20enhance%20adaptability%20in%20challenging%20scenarios%2C%20such%20as%0Alow-light%20conditions%2C%20dynamic%20lighting%2C%20weak-texture%20areas%2C%20and%20severe%20jitter.%0AOur%20system%20supports%20multiple%20modes%2C%20including%20monocular%2C%20stereo%2C%0Amonocular-inertial%2C%20and%20stereo-inertial%20configurations.%20We%20also%20perform%0Aanalysis%20how%20to%20combine%20visual%20SLAM%20with%20deep%20learning%20methods%20to%20enlighten%0Aother%20researches.%20Through%20extensive%20experiments%20on%20both%20public%20datasets%20and%0Aself-sampled%20data%2C%20we%20demonstrate%20the%20superiority%20of%20the%20SL-SLAM%20system%20over%0Atraditional%20approaches.%20The%20experimental%20results%20show%20that%20SL-SLAM%20outperforms%0Astate-of-the-art%20SLAM%20algorithms%20in%20terms%20of%20localization%20accuracy%20and%20tracking%0Arobustness.%20For%20the%20benefit%20of%20community%2C%20we%20make%20public%20the%20source%20code%20at%0Ahttps%3A//github.com/zzzzxxxx111/SLslam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSL-SLAM%253A%2520A%2520robust%2520visual-inertial%2520SLAM%2520based%2520deep%2520feature%2520extraction%2520and%250A%2520%2520matching%26entry.906535625%3DZhang%2520Xiao%2520and%2520Shuaixin%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520how%2520deep%2520learning%2520techniques%2520can%2520improve%2520visual-based%250ASLAM%2520performance%2520in%2520challenging%2520environments.%2520By%2520combining%2520deep%2520feature%250Aextraction%2520and%2520deep%2520matching%2520methods%252C%2520we%2520introduce%2520a%2520versatile%2520hybrid%2520visual%250ASLAM%2520system%2520designed%2520to%2520enhance%2520adaptability%2520in%2520challenging%2520scenarios%252C%2520such%2520as%250Alow-light%2520conditions%252C%2520dynamic%2520lighting%252C%2520weak-texture%2520areas%252C%2520and%2520severe%2520jitter.%250AOur%2520system%2520supports%2520multiple%2520modes%252C%2520including%2520monocular%252C%2520stereo%252C%250Amonocular-inertial%252C%2520and%2520stereo-inertial%2520configurations.%2520We%2520also%2520perform%250Aanalysis%2520how%2520to%2520combine%2520visual%2520SLAM%2520with%2520deep%2520learning%2520methods%2520to%2520enlighten%250Aother%2520researches.%2520Through%2520extensive%2520experiments%2520on%2520both%2520public%2520datasets%2520and%250Aself-sampled%2520data%252C%2520we%2520demonstrate%2520the%2520superiority%2520of%2520the%2520SL-SLAM%2520system%2520over%250Atraditional%2520approaches.%2520The%2520experimental%2520results%2520show%2520that%2520SL-SLAM%2520outperforms%250Astate-of-the-art%2520SLAM%2520algorithms%2520in%2520terms%2520of%2520localization%2520accuracy%2520and%2520tracking%250Arobustness.%2520For%2520the%2520benefit%2520of%2520community%252C%2520we%2520make%2520public%2520the%2520source%2520code%2520at%250Ahttps%253A//github.com/zzzzxxxx111/SLslam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SL-SLAM%3A%20A%20robust%20visual-inertial%20SLAM%20based%20deep%20feature%20extraction%20and%0A%20%20matching&entry.906535625=Zhang%20Xiao%20and%20Shuaixin%20Li&entry.1292438233=%20%20This%20paper%20explores%20how%20deep%20learning%20techniques%20can%20improve%20visual-based%0ASLAM%20performance%20in%20challenging%20environments.%20By%20combining%20deep%20feature%0Aextraction%20and%20deep%20matching%20methods%2C%20we%20introduce%20a%20versatile%20hybrid%20visual%0ASLAM%20system%20designed%20to%20enhance%20adaptability%20in%20challenging%20scenarios%2C%20such%20as%0Alow-light%20conditions%2C%20dynamic%20lighting%2C%20weak-texture%20areas%2C%20and%20severe%20jitter.%0AOur%20system%20supports%20multiple%20modes%2C%20including%20monocular%2C%20stereo%2C%0Amonocular-inertial%2C%20and%20stereo-inertial%20configurations.%20We%20also%20perform%0Aanalysis%20how%20to%20combine%20visual%20SLAM%20with%20deep%20learning%20methods%20to%20enlighten%0Aother%20researches.%20Through%20extensive%20experiments%20on%20both%20public%20datasets%20and%0Aself-sampled%20data%2C%20we%20demonstrate%20the%20superiority%20of%20the%20SL-SLAM%20system%20over%0Atraditional%20approaches.%20The%20experimental%20results%20show%20that%20SL-SLAM%20outperforms%0Astate-of-the-art%20SLAM%20algorithms%20in%20terms%20of%20localization%20accuracy%20and%20tracking%0Arobustness.%20For%20the%20benefit%20of%20community%2C%20we%20make%20public%20the%20source%20code%20at%0Ahttps%3A//github.com/zzzzxxxx111/SLslam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03413v1&entry.124074799=Read"},
{"title": "3D LiDAR Mapping in Dynamic Environments Using a 4D Implicit Neural\n  Representation", "author": "Xingguang Zhong and Yue Pan and Cyrill Stachniss and Jens Behley", "abstract": "  Building accurate maps is a key building block to enable reliable\nlocalization, planning, and navigation of autonomous vehicles. We propose a\nnovel approach for building accurate maps of dynamic environments utilizing a\nsequence of LiDAR scans. To this end, we propose encoding the 4D scene into a\nnovel spatio-temporal implicit neural map representation by fitting a\ntime-dependent truncated signed distance function to each point. Using our\nrepresentation, we extract the static map by filtering the dynamic parts. Our\nneural representation is based on sparse feature grids, a globally shared\ndecoder, and time-dependent basis functions, which we jointly optimize in an\nunsupervised fashion. To learn this representation from a sequence of LiDAR\nscans, we design a simple yet efficient loss function to supervise the map\noptimization in a piecewise way. We evaluate our approach on various scenes\ncontaining moving objects in terms of the reconstruction quality of static maps\nand the segmentation of dynamic point clouds. The experimental results\ndemonstrate that our method is capable of removing the dynamic part of the\ninput point clouds while reconstructing accurate and complete 3D maps,\noutperforming several state-of-the-art methods. Codes are available at:\nhttps://github.com/PRBonn/4dNDF\n", "link": "http://arxiv.org/abs/2405.03388v1", "date": "2024-05-06", "relevancy": 3.1061, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6321}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.631}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20LiDAR%20Mapping%20in%20Dynamic%20Environments%20Using%20a%204D%20Implicit%20Neural%0A%20%20Representation&body=Title%3A%203D%20LiDAR%20Mapping%20in%20Dynamic%20Environments%20Using%20a%204D%20Implicit%20Neural%0A%20%20Representation%0AAuthor%3A%20Xingguang%20Zhong%20and%20Yue%20Pan%20and%20Cyrill%20Stachniss%20and%20Jens%20Behley%0AAbstract%3A%20%20%20Building%20accurate%20maps%20is%20a%20key%20building%20block%20to%20enable%20reliable%0Alocalization%2C%20planning%2C%20and%20navigation%20of%20autonomous%20vehicles.%20We%20propose%20a%0Anovel%20approach%20for%20building%20accurate%20maps%20of%20dynamic%20environments%20utilizing%20a%0Asequence%20of%20LiDAR%20scans.%20To%20this%20end%2C%20we%20propose%20encoding%20the%204D%20scene%20into%20a%0Anovel%20spatio-temporal%20implicit%20neural%20map%20representation%20by%20fitting%20a%0Atime-dependent%20truncated%20signed%20distance%20function%20to%20each%20point.%20Using%20our%0Arepresentation%2C%20we%20extract%20the%20static%20map%20by%20filtering%20the%20dynamic%20parts.%20Our%0Aneural%20representation%20is%20based%20on%20sparse%20feature%20grids%2C%20a%20globally%20shared%0Adecoder%2C%20and%20time-dependent%20basis%20functions%2C%20which%20we%20jointly%20optimize%20in%20an%0Aunsupervised%20fashion.%20To%20learn%20this%20representation%20from%20a%20sequence%20of%20LiDAR%0Ascans%2C%20we%20design%20a%20simple%20yet%20efficient%20loss%20function%20to%20supervise%20the%20map%0Aoptimization%20in%20a%20piecewise%20way.%20We%20evaluate%20our%20approach%20on%20various%20scenes%0Acontaining%20moving%20objects%20in%20terms%20of%20the%20reconstruction%20quality%20of%20static%20maps%0Aand%20the%20segmentation%20of%20dynamic%20point%20clouds.%20The%20experimental%20results%0Ademonstrate%20that%20our%20method%20is%20capable%20of%20removing%20the%20dynamic%20part%20of%20the%0Ainput%20point%20clouds%20while%20reconstructing%20accurate%20and%20complete%203D%20maps%2C%0Aoutperforming%20several%20state-of-the-art%20methods.%20Codes%20are%20available%20at%3A%0Ahttps%3A//github.com/PRBonn/4dNDF%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03388v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520LiDAR%2520Mapping%2520in%2520Dynamic%2520Environments%2520Using%2520a%25204D%2520Implicit%2520Neural%250A%2520%2520Representation%26entry.906535625%3DXingguang%2520Zhong%2520and%2520Yue%2520Pan%2520and%2520Cyrill%2520Stachniss%2520and%2520Jens%2520Behley%26entry.1292438233%3D%2520%2520Building%2520accurate%2520maps%2520is%2520a%2520key%2520building%2520block%2520to%2520enable%2520reliable%250Alocalization%252C%2520planning%252C%2520and%2520navigation%2520of%2520autonomous%2520vehicles.%2520We%2520propose%2520a%250Anovel%2520approach%2520for%2520building%2520accurate%2520maps%2520of%2520dynamic%2520environments%2520utilizing%2520a%250Asequence%2520of%2520LiDAR%2520scans.%2520To%2520this%2520end%252C%2520we%2520propose%2520encoding%2520the%25204D%2520scene%2520into%2520a%250Anovel%2520spatio-temporal%2520implicit%2520neural%2520map%2520representation%2520by%2520fitting%2520a%250Atime-dependent%2520truncated%2520signed%2520distance%2520function%2520to%2520each%2520point.%2520Using%2520our%250Arepresentation%252C%2520we%2520extract%2520the%2520static%2520map%2520by%2520filtering%2520the%2520dynamic%2520parts.%2520Our%250Aneural%2520representation%2520is%2520based%2520on%2520sparse%2520feature%2520grids%252C%2520a%2520globally%2520shared%250Adecoder%252C%2520and%2520time-dependent%2520basis%2520functions%252C%2520which%2520we%2520jointly%2520optimize%2520in%2520an%250Aunsupervised%2520fashion.%2520To%2520learn%2520this%2520representation%2520from%2520a%2520sequence%2520of%2520LiDAR%250Ascans%252C%2520we%2520design%2520a%2520simple%2520yet%2520efficient%2520loss%2520function%2520to%2520supervise%2520the%2520map%250Aoptimization%2520in%2520a%2520piecewise%2520way.%2520We%2520evaluate%2520our%2520approach%2520on%2520various%2520scenes%250Acontaining%2520moving%2520objects%2520in%2520terms%2520of%2520the%2520reconstruction%2520quality%2520of%2520static%2520maps%250Aand%2520the%2520segmentation%2520of%2520dynamic%2520point%2520clouds.%2520The%2520experimental%2520results%250Ademonstrate%2520that%2520our%2520method%2520is%2520capable%2520of%2520removing%2520the%2520dynamic%2520part%2520of%2520the%250Ainput%2520point%2520clouds%2520while%2520reconstructing%2520accurate%2520and%2520complete%25203D%2520maps%252C%250Aoutperforming%2520several%2520state-of-the-art%2520methods.%2520Codes%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/PRBonn/4dNDF%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03388v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20LiDAR%20Mapping%20in%20Dynamic%20Environments%20Using%20a%204D%20Implicit%20Neural%0A%20%20Representation&entry.906535625=Xingguang%20Zhong%20and%20Yue%20Pan%20and%20Cyrill%20Stachniss%20and%20Jens%20Behley&entry.1292438233=%20%20Building%20accurate%20maps%20is%20a%20key%20building%20block%20to%20enable%20reliable%0Alocalization%2C%20planning%2C%20and%20navigation%20of%20autonomous%20vehicles.%20We%20propose%20a%0Anovel%20approach%20for%20building%20accurate%20maps%20of%20dynamic%20environments%20utilizing%20a%0Asequence%20of%20LiDAR%20scans.%20To%20this%20end%2C%20we%20propose%20encoding%20the%204D%20scene%20into%20a%0Anovel%20spatio-temporal%20implicit%20neural%20map%20representation%20by%20fitting%20a%0Atime-dependent%20truncated%20signed%20distance%20function%20to%20each%20point.%20Using%20our%0Arepresentation%2C%20we%20extract%20the%20static%20map%20by%20filtering%20the%20dynamic%20parts.%20Our%0Aneural%20representation%20is%20based%20on%20sparse%20feature%20grids%2C%20a%20globally%20shared%0Adecoder%2C%20and%20time-dependent%20basis%20functions%2C%20which%20we%20jointly%20optimize%20in%20an%0Aunsupervised%20fashion.%20To%20learn%20this%20representation%20from%20a%20sequence%20of%20LiDAR%0Ascans%2C%20we%20design%20a%20simple%20yet%20efficient%20loss%20function%20to%20supervise%20the%20map%0Aoptimization%20in%20a%20piecewise%20way.%20We%20evaluate%20our%20approach%20on%20various%20scenes%0Acontaining%20moving%20objects%20in%20terms%20of%20the%20reconstruction%20quality%20of%20static%20maps%0Aand%20the%20segmentation%20of%20dynamic%20point%20clouds.%20The%20experimental%20results%0Ademonstrate%20that%20our%20method%20is%20capable%20of%20removing%20the%20dynamic%20part%20of%20the%0Ainput%20point%20clouds%20while%20reconstructing%20accurate%20and%20complete%203D%20maps%2C%0Aoutperforming%20several%20state-of-the-art%20methods.%20Codes%20are%20available%20at%3A%0Ahttps%3A//github.com/PRBonn/4dNDF%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03388v1&entry.124074799=Read"},
{"title": "A Construct-Optimize Approach to Sparse View Synthesis without Camera\n  Pose", "author": "Kaiwen Jiang and Yang Fu and Mukund Varma T and Yash Belhe and Xiaolong Wang and Hao Su and Ravi Ramamoorthi", "abstract": "  Novel view synthesis from a sparse set of input images is a challenging\nproblem of great practical interest, especially when camera poses are absent or\ninaccurate. Direct optimization of camera poses and usage of estimated depths\nin neural radiance field algorithms usually do not produce good results because\nof the coupling between poses and depths, and inaccuracies in monocular depth\nestimation. In this paper, we leverage the recent 3D Gaussian splatting method\nto develop a novel construct-and-optimize method for sparse view synthesis\nwithout camera poses. Specifically, we construct a solution progressively by\nusing monocular depth and projecting pixels back into the 3D world. During\nconstruction, we optimize the solution by detecting 2D correspondences between\ntraining views and the corresponding rendered images. We develop a unified\ndifferentiable pipeline for camera registration and adjustment of both camera\nposes and depths, followed by back-projection. We also introduce a novel notion\nof an expected surface in Gaussian splatting, which is critical to our\noptimization. These steps enable a coarse solution, which can then be low-pass\nfiltered and refined using standard optimization methods. We demonstrate\nresults on the Tanks and Temples and Static Hikes datasets with as few as three\nwidely-spaced views, showing significantly better quality than competing\nmethods, including those with approximate camera pose information. Moreover,\nour results improve with more views and outperform previous InstantNGP and\nGaussian Splatting algorithms even when using half the dataset.\n", "link": "http://arxiv.org/abs/2405.03659v1", "date": "2024-05-06", "relevancy": 3.0622, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6748}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6073}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Construct-Optimize%20Approach%20to%20Sparse%20View%20Synthesis%20without%20Camera%0A%20%20Pose&body=Title%3A%20A%20Construct-Optimize%20Approach%20to%20Sparse%20View%20Synthesis%20without%20Camera%0A%20%20Pose%0AAuthor%3A%20Kaiwen%20Jiang%20and%20Yang%20Fu%20and%20Mukund%20Varma%20T%20and%20Yash%20Belhe%20and%20Xiaolong%20Wang%20and%20Hao%20Su%20and%20Ravi%20Ramamoorthi%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20from%20a%20sparse%20set%20of%20input%20images%20is%20a%20challenging%0Aproblem%20of%20great%20practical%20interest%2C%20especially%20when%20camera%20poses%20are%20absent%20or%0Ainaccurate.%20Direct%20optimization%20of%20camera%20poses%20and%20usage%20of%20estimated%20depths%0Ain%20neural%20radiance%20field%20algorithms%20usually%20do%20not%20produce%20good%20results%20because%0Aof%20the%20coupling%20between%20poses%20and%20depths%2C%20and%20inaccuracies%20in%20monocular%20depth%0Aestimation.%20In%20this%20paper%2C%20we%20leverage%20the%20recent%203D%20Gaussian%20splatting%20method%0Ato%20develop%20a%20novel%20construct-and-optimize%20method%20for%20sparse%20view%20synthesis%0Awithout%20camera%20poses.%20Specifically%2C%20we%20construct%20a%20solution%20progressively%20by%0Ausing%20monocular%20depth%20and%20projecting%20pixels%20back%20into%20the%203D%20world.%20During%0Aconstruction%2C%20we%20optimize%20the%20solution%20by%20detecting%202D%20correspondences%20between%0Atraining%20views%20and%20the%20corresponding%20rendered%20images.%20We%20develop%20a%20unified%0Adifferentiable%20pipeline%20for%20camera%20registration%20and%20adjustment%20of%20both%20camera%0Aposes%20and%20depths%2C%20followed%20by%20back-projection.%20We%20also%20introduce%20a%20novel%20notion%0Aof%20an%20expected%20surface%20in%20Gaussian%20splatting%2C%20which%20is%20critical%20to%20our%0Aoptimization.%20These%20steps%20enable%20a%20coarse%20solution%2C%20which%20can%20then%20be%20low-pass%0Afiltered%20and%20refined%20using%20standard%20optimization%20methods.%20We%20demonstrate%0Aresults%20on%20the%20Tanks%20and%20Temples%20and%20Static%20Hikes%20datasets%20with%20as%20few%20as%20three%0Awidely-spaced%20views%2C%20showing%20significantly%20better%20quality%20than%20competing%0Amethods%2C%20including%20those%20with%20approximate%20camera%20pose%20information.%20Moreover%2C%0Aour%20results%20improve%20with%20more%20views%20and%20outperform%20previous%20InstantNGP%20and%0AGaussian%20Splatting%20algorithms%20even%20when%20using%20half%20the%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Construct-Optimize%2520Approach%2520to%2520Sparse%2520View%2520Synthesis%2520without%2520Camera%250A%2520%2520Pose%26entry.906535625%3DKaiwen%2520Jiang%2520and%2520Yang%2520Fu%2520and%2520Mukund%2520Varma%2520T%2520and%2520Yash%2520Belhe%2520and%2520Xiaolong%2520Wang%2520and%2520Hao%2520Su%2520and%2520Ravi%2520Ramamoorthi%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520from%2520a%2520sparse%2520set%2520of%2520input%2520images%2520is%2520a%2520challenging%250Aproblem%2520of%2520great%2520practical%2520interest%252C%2520especially%2520when%2520camera%2520poses%2520are%2520absent%2520or%250Ainaccurate.%2520Direct%2520optimization%2520of%2520camera%2520poses%2520and%2520usage%2520of%2520estimated%2520depths%250Ain%2520neural%2520radiance%2520field%2520algorithms%2520usually%2520do%2520not%2520produce%2520good%2520results%2520because%250Aof%2520the%2520coupling%2520between%2520poses%2520and%2520depths%252C%2520and%2520inaccuracies%2520in%2520monocular%2520depth%250Aestimation.%2520In%2520this%2520paper%252C%2520we%2520leverage%2520the%2520recent%25203D%2520Gaussian%2520splatting%2520method%250Ato%2520develop%2520a%2520novel%2520construct-and-optimize%2520method%2520for%2520sparse%2520view%2520synthesis%250Awithout%2520camera%2520poses.%2520Specifically%252C%2520we%2520construct%2520a%2520solution%2520progressively%2520by%250Ausing%2520monocular%2520depth%2520and%2520projecting%2520pixels%2520back%2520into%2520the%25203D%2520world.%2520During%250Aconstruction%252C%2520we%2520optimize%2520the%2520solution%2520by%2520detecting%25202D%2520correspondences%2520between%250Atraining%2520views%2520and%2520the%2520corresponding%2520rendered%2520images.%2520We%2520develop%2520a%2520unified%250Adifferentiable%2520pipeline%2520for%2520camera%2520registration%2520and%2520adjustment%2520of%2520both%2520camera%250Aposes%2520and%2520depths%252C%2520followed%2520by%2520back-projection.%2520We%2520also%2520introduce%2520a%2520novel%2520notion%250Aof%2520an%2520expected%2520surface%2520in%2520Gaussian%2520splatting%252C%2520which%2520is%2520critical%2520to%2520our%250Aoptimization.%2520These%2520steps%2520enable%2520a%2520coarse%2520solution%252C%2520which%2520can%2520then%2520be%2520low-pass%250Afiltered%2520and%2520refined%2520using%2520standard%2520optimization%2520methods.%2520We%2520demonstrate%250Aresults%2520on%2520the%2520Tanks%2520and%2520Temples%2520and%2520Static%2520Hikes%2520datasets%2520with%2520as%2520few%2520as%2520three%250Awidely-spaced%2520views%252C%2520showing%2520significantly%2520better%2520quality%2520than%2520competing%250Amethods%252C%2520including%2520those%2520with%2520approximate%2520camera%2520pose%2520information.%2520Moreover%252C%250Aour%2520results%2520improve%2520with%2520more%2520views%2520and%2520outperform%2520previous%2520InstantNGP%2520and%250AGaussian%2520Splatting%2520algorithms%2520even%2520when%2520using%2520half%2520the%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Construct-Optimize%20Approach%20to%20Sparse%20View%20Synthesis%20without%20Camera%0A%20%20Pose&entry.906535625=Kaiwen%20Jiang%20and%20Yang%20Fu%20and%20Mukund%20Varma%20T%20and%20Yash%20Belhe%20and%20Xiaolong%20Wang%20and%20Hao%20Su%20and%20Ravi%20Ramamoorthi&entry.1292438233=%20%20Novel%20view%20synthesis%20from%20a%20sparse%20set%20of%20input%20images%20is%20a%20challenging%0Aproblem%20of%20great%20practical%20interest%2C%20especially%20when%20camera%20poses%20are%20absent%20or%0Ainaccurate.%20Direct%20optimization%20of%20camera%20poses%20and%20usage%20of%20estimated%20depths%0Ain%20neural%20radiance%20field%20algorithms%20usually%20do%20not%20produce%20good%20results%20because%0Aof%20the%20coupling%20between%20poses%20and%20depths%2C%20and%20inaccuracies%20in%20monocular%20depth%0Aestimation.%20In%20this%20paper%2C%20we%20leverage%20the%20recent%203D%20Gaussian%20splatting%20method%0Ato%20develop%20a%20novel%20construct-and-optimize%20method%20for%20sparse%20view%20synthesis%0Awithout%20camera%20poses.%20Specifically%2C%20we%20construct%20a%20solution%20progressively%20by%0Ausing%20monocular%20depth%20and%20projecting%20pixels%20back%20into%20the%203D%20world.%20During%0Aconstruction%2C%20we%20optimize%20the%20solution%20by%20detecting%202D%20correspondences%20between%0Atraining%20views%20and%20the%20corresponding%20rendered%20images.%20We%20develop%20a%20unified%0Adifferentiable%20pipeline%20for%20camera%20registration%20and%20adjustment%20of%20both%20camera%0Aposes%20and%20depths%2C%20followed%20by%20back-projection.%20We%20also%20introduce%20a%20novel%20notion%0Aof%20an%20expected%20surface%20in%20Gaussian%20splatting%2C%20which%20is%20critical%20to%20our%0Aoptimization.%20These%20steps%20enable%20a%20coarse%20solution%2C%20which%20can%20then%20be%20low-pass%0Afiltered%20and%20refined%20using%20standard%20optimization%20methods.%20We%20demonstrate%0Aresults%20on%20the%20Tanks%20and%20Temples%20and%20Static%20Hikes%20datasets%20with%20as%20few%20as%20three%0Awidely-spaced%20views%2C%20showing%20significantly%20better%20quality%20than%20competing%0Amethods%2C%20including%20those%20with%20approximate%20camera%20pose%20information.%20Moreover%2C%0Aour%20results%20improve%20with%20more%20views%20and%20outperform%20previous%20InstantNGP%20and%0AGaussian%20Splatting%20algorithms%20even%20when%20using%20half%20the%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03659v1&entry.124074799=Read"},
{"title": "Generated Contents Enrichment", "author": "Mahdi Naseri and Jiayan Qiu and Zhou Wang", "abstract": "  In this paper, we investigate a novel artificial intelligence generation\ntask, termed as generated contents enrichment (GCE). Different from\nconventional artificial intelligence contents generation task that enriches the\ngiven textual description implicitly with limited semantics for generating\nvisually real content, our proposed GCE strives to perform content enrichment\nexplicitly on both the visual and textual domain, from which the enriched\ncontents are visually real, structurally reasonable, and semantically abundant.\nTowards to solve GCE, we propose a deep end-to-end method that explicitly\nexplores the semantics and inter-semantic relationships during the enrichment.\nSpecifically, we first model the input description as a semantic graph, wherein\neach node represents an object and each edge corresponds to the inter-object\nrelationship. We then adopt Graph Convolutional Networks on top of the input\nscene description to predict the enriching objects and their relationships with\nthe input objects. Finally, the enriched graph is fed into an image synthesis\nmodel to carry out the visual contents generation. Our experiments conducted on\nthe Visual Genome dataset exhibit promising and visually plausible results.\n", "link": "http://arxiv.org/abs/2405.03650v1", "date": "2024-05-06", "relevancy": 2.9386, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6503}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5583}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generated%20Contents%20Enrichment&body=Title%3A%20Generated%20Contents%20Enrichment%0AAuthor%3A%20Mahdi%20Naseri%20and%20Jiayan%20Qiu%20and%20Zhou%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20a%20novel%20artificial%20intelligence%20generation%0Atask%2C%20termed%20as%20generated%20contents%20enrichment%20%28GCE%29.%20Different%20from%0Aconventional%20artificial%20intelligence%20contents%20generation%20task%20that%20enriches%20the%0Agiven%20textual%20description%20implicitly%20with%20limited%20semantics%20for%20generating%0Avisually%20real%20content%2C%20our%20proposed%20GCE%20strives%20to%20perform%20content%20enrichment%0Aexplicitly%20on%20both%20the%20visual%20and%20textual%20domain%2C%20from%20which%20the%20enriched%0Acontents%20are%20visually%20real%2C%20structurally%20reasonable%2C%20and%20semantically%20abundant.%0ATowards%20to%20solve%20GCE%2C%20we%20propose%20a%20deep%20end-to-end%20method%20that%20explicitly%0Aexplores%20the%20semantics%20and%20inter-semantic%20relationships%20during%20the%20enrichment.%0ASpecifically%2C%20we%20first%20model%20the%20input%20description%20as%20a%20semantic%20graph%2C%20wherein%0Aeach%20node%20represents%20an%20object%20and%20each%20edge%20corresponds%20to%20the%20inter-object%0Arelationship.%20We%20then%20adopt%20Graph%20Convolutional%20Networks%20on%20top%20of%20the%20input%0Ascene%20description%20to%20predict%20the%20enriching%20objects%20and%20their%20relationships%20with%0Athe%20input%20objects.%20Finally%2C%20the%20enriched%20graph%20is%20fed%20into%20an%20image%20synthesis%0Amodel%20to%20carry%20out%20the%20visual%20contents%20generation.%20Our%20experiments%20conducted%20on%0Athe%20Visual%20Genome%20dataset%20exhibit%20promising%20and%20visually%20plausible%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerated%2520Contents%2520Enrichment%26entry.906535625%3DMahdi%2520Naseri%2520and%2520Jiayan%2520Qiu%2520and%2520Zhou%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520a%2520novel%2520artificial%2520intelligence%2520generation%250Atask%252C%2520termed%2520as%2520generated%2520contents%2520enrichment%2520%2528GCE%2529.%2520Different%2520from%250Aconventional%2520artificial%2520intelligence%2520contents%2520generation%2520task%2520that%2520enriches%2520the%250Agiven%2520textual%2520description%2520implicitly%2520with%2520limited%2520semantics%2520for%2520generating%250Avisually%2520real%2520content%252C%2520our%2520proposed%2520GCE%2520strives%2520to%2520perform%2520content%2520enrichment%250Aexplicitly%2520on%2520both%2520the%2520visual%2520and%2520textual%2520domain%252C%2520from%2520which%2520the%2520enriched%250Acontents%2520are%2520visually%2520real%252C%2520structurally%2520reasonable%252C%2520and%2520semantically%2520abundant.%250ATowards%2520to%2520solve%2520GCE%252C%2520we%2520propose%2520a%2520deep%2520end-to-end%2520method%2520that%2520explicitly%250Aexplores%2520the%2520semantics%2520and%2520inter-semantic%2520relationships%2520during%2520the%2520enrichment.%250ASpecifically%252C%2520we%2520first%2520model%2520the%2520input%2520description%2520as%2520a%2520semantic%2520graph%252C%2520wherein%250Aeach%2520node%2520represents%2520an%2520object%2520and%2520each%2520edge%2520corresponds%2520to%2520the%2520inter-object%250Arelationship.%2520We%2520then%2520adopt%2520Graph%2520Convolutional%2520Networks%2520on%2520top%2520of%2520the%2520input%250Ascene%2520description%2520to%2520predict%2520the%2520enriching%2520objects%2520and%2520their%2520relationships%2520with%250Athe%2520input%2520objects.%2520Finally%252C%2520the%2520enriched%2520graph%2520is%2520fed%2520into%2520an%2520image%2520synthesis%250Amodel%2520to%2520carry%2520out%2520the%2520visual%2520contents%2520generation.%2520Our%2520experiments%2520conducted%2520on%250Athe%2520Visual%2520Genome%2520dataset%2520exhibit%2520promising%2520and%2520visually%2520plausible%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generated%20Contents%20Enrichment&entry.906535625=Mahdi%20Naseri%20and%20Jiayan%20Qiu%20and%20Zhou%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20a%20novel%20artificial%20intelligence%20generation%0Atask%2C%20termed%20as%20generated%20contents%20enrichment%20%28GCE%29.%20Different%20from%0Aconventional%20artificial%20intelligence%20contents%20generation%20task%20that%20enriches%20the%0Agiven%20textual%20description%20implicitly%20with%20limited%20semantics%20for%20generating%0Avisually%20real%20content%2C%20our%20proposed%20GCE%20strives%20to%20perform%20content%20enrichment%0Aexplicitly%20on%20both%20the%20visual%20and%20textual%20domain%2C%20from%20which%20the%20enriched%0Acontents%20are%20visually%20real%2C%20structurally%20reasonable%2C%20and%20semantically%20abundant.%0ATowards%20to%20solve%20GCE%2C%20we%20propose%20a%20deep%20end-to-end%20method%20that%20explicitly%0Aexplores%20the%20semantics%20and%20inter-semantic%20relationships%20during%20the%20enrichment.%0ASpecifically%2C%20we%20first%20model%20the%20input%20description%20as%20a%20semantic%20graph%2C%20wherein%0Aeach%20node%20represents%20an%20object%20and%20each%20edge%20corresponds%20to%20the%20inter-object%0Arelationship.%20We%20then%20adopt%20Graph%20Convolutional%20Networks%20on%20top%20of%20the%20input%0Ascene%20description%20to%20predict%20the%20enriching%20objects%20and%20their%20relationships%20with%0Athe%20input%20objects.%20Finally%2C%20the%20enriched%20graph%20is%20fed%20into%20an%20image%20synthesis%0Amodel%20to%20carry%20out%20the%20visual%20contents%20generation.%20Our%20experiments%20conducted%20on%0Athe%20Visual%20Genome%20dataset%20exhibit%20promising%20and%20visually%20plausible%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03650v1&entry.124074799=Read"},
{"title": "SceneTracker: Long-term Scene Flow Estimation Network", "author": "Bo Wang and Jian Li and Yang Yu and Li Liu and Zhenping Sun and Dewen Hu", "abstract": "  Considering the complementarity of scene flow estimation in the spatial\ndomain's focusing capability and 3D object tracking in the temporal domain's\ncoherence, this study aims to address a comprehensive new task that can\nsimultaneously capture fine-grained and long-term 3D motion in an online\nmanner: long-term scene flow estimation (LSFE). We introduce SceneTracker, a\nnovel learning-based LSFE network that adopts an iterative approach to\napproximate the optimal trajectory. Besides, it dynamically indexes and\nconstructs appearance and depth correlation features simultaneously and employs\nthe Transformer to explore and utilize long-range connections within and\nbetween trajectories. With detailed experiments, SceneTracker shows superior\ncapabilities in handling 3D spatial occlusion and depth noise interference,\nhighly tailored to the LSFE task's needs. Finally, we build the first\nreal-world evaluation dataset, LSFDriving, further substantiating\nSceneTracker's commendable generalization capacity. The code and data for\nSceneTracker is available at https://github.com/wwsource/SceneTracker.\n", "link": "http://arxiv.org/abs/2403.19924v3", "date": "2024-05-06", "relevancy": 2.7138, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5452}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5444}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneTracker%3A%20Long-term%20Scene%20Flow%20Estimation%20Network&body=Title%3A%20SceneTracker%3A%20Long-term%20Scene%20Flow%20Estimation%20Network%0AAuthor%3A%20Bo%20Wang%20and%20Jian%20Li%20and%20Yang%20Yu%20and%20Li%20Liu%20and%20Zhenping%20Sun%20and%20Dewen%20Hu%0AAbstract%3A%20%20%20Considering%20the%20complementarity%20of%20scene%20flow%20estimation%20in%20the%20spatial%0Adomain%27s%20focusing%20capability%20and%203D%20object%20tracking%20in%20the%20temporal%20domain%27s%0Acoherence%2C%20this%20study%20aims%20to%20address%20a%20comprehensive%20new%20task%20that%20can%0Asimultaneously%20capture%20fine-grained%20and%20long-term%203D%20motion%20in%20an%20online%0Amanner%3A%20long-term%20scene%20flow%20estimation%20%28LSFE%29.%20We%20introduce%20SceneTracker%2C%20a%0Anovel%20learning-based%20LSFE%20network%20that%20adopts%20an%20iterative%20approach%20to%0Aapproximate%20the%20optimal%20trajectory.%20Besides%2C%20it%20dynamically%20indexes%20and%0Aconstructs%20appearance%20and%20depth%20correlation%20features%20simultaneously%20and%20employs%0Athe%20Transformer%20to%20explore%20and%20utilize%20long-range%20connections%20within%20and%0Abetween%20trajectories.%20With%20detailed%20experiments%2C%20SceneTracker%20shows%20superior%0Acapabilities%20in%20handling%203D%20spatial%20occlusion%20and%20depth%20noise%20interference%2C%0Ahighly%20tailored%20to%20the%20LSFE%20task%27s%20needs.%20Finally%2C%20we%20build%20the%20first%0Areal-world%20evaluation%20dataset%2C%20LSFDriving%2C%20further%20substantiating%0ASceneTracker%27s%20commendable%20generalization%20capacity.%20The%20code%20and%20data%20for%0ASceneTracker%20is%20available%20at%20https%3A//github.com/wwsource/SceneTracker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19924v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneTracker%253A%2520Long-term%2520Scene%2520Flow%2520Estimation%2520Network%26entry.906535625%3DBo%2520Wang%2520and%2520Jian%2520Li%2520and%2520Yang%2520Yu%2520and%2520Li%2520Liu%2520and%2520Zhenping%2520Sun%2520and%2520Dewen%2520Hu%26entry.1292438233%3D%2520%2520Considering%2520the%2520complementarity%2520of%2520scene%2520flow%2520estimation%2520in%2520the%2520spatial%250Adomain%2527s%2520focusing%2520capability%2520and%25203D%2520object%2520tracking%2520in%2520the%2520temporal%2520domain%2527s%250Acoherence%252C%2520this%2520study%2520aims%2520to%2520address%2520a%2520comprehensive%2520new%2520task%2520that%2520can%250Asimultaneously%2520capture%2520fine-grained%2520and%2520long-term%25203D%2520motion%2520in%2520an%2520online%250Amanner%253A%2520long-term%2520scene%2520flow%2520estimation%2520%2528LSFE%2529.%2520We%2520introduce%2520SceneTracker%252C%2520a%250Anovel%2520learning-based%2520LSFE%2520network%2520that%2520adopts%2520an%2520iterative%2520approach%2520to%250Aapproximate%2520the%2520optimal%2520trajectory.%2520Besides%252C%2520it%2520dynamically%2520indexes%2520and%250Aconstructs%2520appearance%2520and%2520depth%2520correlation%2520features%2520simultaneously%2520and%2520employs%250Athe%2520Transformer%2520to%2520explore%2520and%2520utilize%2520long-range%2520connections%2520within%2520and%250Abetween%2520trajectories.%2520With%2520detailed%2520experiments%252C%2520SceneTracker%2520shows%2520superior%250Acapabilities%2520in%2520handling%25203D%2520spatial%2520occlusion%2520and%2520depth%2520noise%2520interference%252C%250Ahighly%2520tailored%2520to%2520the%2520LSFE%2520task%2527s%2520needs.%2520Finally%252C%2520we%2520build%2520the%2520first%250Areal-world%2520evaluation%2520dataset%252C%2520LSFDriving%252C%2520further%2520substantiating%250ASceneTracker%2527s%2520commendable%2520generalization%2520capacity.%2520The%2520code%2520and%2520data%2520for%250ASceneTracker%2520is%2520available%2520at%2520https%253A//github.com/wwsource/SceneTracker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19924v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneTracker%3A%20Long-term%20Scene%20Flow%20Estimation%20Network&entry.906535625=Bo%20Wang%20and%20Jian%20Li%20and%20Yang%20Yu%20and%20Li%20Liu%20and%20Zhenping%20Sun%20and%20Dewen%20Hu&entry.1292438233=%20%20Considering%20the%20complementarity%20of%20scene%20flow%20estimation%20in%20the%20spatial%0Adomain%27s%20focusing%20capability%20and%203D%20object%20tracking%20in%20the%20temporal%20domain%27s%0Acoherence%2C%20this%20study%20aims%20to%20address%20a%20comprehensive%20new%20task%20that%20can%0Asimultaneously%20capture%20fine-grained%20and%20long-term%203D%20motion%20in%20an%20online%0Amanner%3A%20long-term%20scene%20flow%20estimation%20%28LSFE%29.%20We%20introduce%20SceneTracker%2C%20a%0Anovel%20learning-based%20LSFE%20network%20that%20adopts%20an%20iterative%20approach%20to%0Aapproximate%20the%20optimal%20trajectory.%20Besides%2C%20it%20dynamically%20indexes%20and%0Aconstructs%20appearance%20and%20depth%20correlation%20features%20simultaneously%20and%20employs%0Athe%20Transformer%20to%20explore%20and%20utilize%20long-range%20connections%20within%20and%0Abetween%20trajectories.%20With%20detailed%20experiments%2C%20SceneTracker%20shows%20superior%0Acapabilities%20in%20handling%203D%20spatial%20occlusion%20and%20depth%20noise%20interference%2C%0Ahighly%20tailored%20to%20the%20LSFE%20task%27s%20needs.%20Finally%2C%20we%20build%20the%20first%0Areal-world%20evaluation%20dataset%2C%20LSFDriving%2C%20further%20substantiating%0ASceneTracker%27s%20commendable%20generalization%20capacity.%20The%20code%20and%20data%20for%0ASceneTracker%20is%20available%20at%20https%3A//github.com/wwsource/SceneTracker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19924v3&entry.124074799=Read"},
{"title": "An Image Quality Evaluation and Masking Algorithm Based On Pre-trained\n  Deep Neural Networks", "author": "Peng Jia and Yu Song and Jiameng Lv and Runyu Ning", "abstract": "  With the growing amount of astronomical data, there is an increasing need for\nautomated data processing pipelines, which can extract scientific information\nfrom observation data without human interventions. A critical aspect of these\npipelines is the image quality evaluation and masking algorithm, which\nevaluates image qualities based on various factors such as cloud coverage, sky\nbrightness, scattering light from the optical system, point spread function\nsize and shape, and read-out noise. Occasionally, the algorithm requires\nmasking of areas severely affected by noise. However, the algorithm often\nnecessitates significant human interventions, reducing data processing\nefficiency. In this study, we present a deep learning based image quality\nevaluation algorithm that uses an autoencoder to learn features of high quality\nastronomical images. The trained autoencoder enables automatic evaluation of\nimage quality and masking of noise affected areas. We have evaluated the\nperformance of our algorithm using two test cases: images with point spread\nfunctions of varying full width half magnitude, and images with complex\nbackgrounds. In the first scenario, our algorithm could effectively identify\nvariations of the point spread functions, which can provide valuable reference\ninformation for photometry. In the second scenario, our method could\nsuccessfully mask regions affected by complex regions, which could\nsignificantly increase the photometry accuracy. Our algorithm can be employed\nto automatically evaluate image quality obtained by different sky surveying\nprojects, further increasing the speed and robustness of data processing\npipelines.\n", "link": "http://arxiv.org/abs/2405.03408v1", "date": "2024-05-06", "relevancy": 2.7014, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5473}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5387}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Image%20Quality%20Evaluation%20and%20Masking%20Algorithm%20Based%20On%20Pre-trained%0A%20%20Deep%20Neural%20Networks&body=Title%3A%20An%20Image%20Quality%20Evaluation%20and%20Masking%20Algorithm%20Based%20On%20Pre-trained%0A%20%20Deep%20Neural%20Networks%0AAuthor%3A%20Peng%20Jia%20and%20Yu%20Song%20and%20Jiameng%20Lv%20and%20Runyu%20Ning%0AAbstract%3A%20%20%20With%20the%20growing%20amount%20of%20astronomical%20data%2C%20there%20is%20an%20increasing%20need%20for%0Aautomated%20data%20processing%20pipelines%2C%20which%20can%20extract%20scientific%20information%0Afrom%20observation%20data%20without%20human%20interventions.%20A%20critical%20aspect%20of%20these%0Apipelines%20is%20the%20image%20quality%20evaluation%20and%20masking%20algorithm%2C%20which%0Aevaluates%20image%20qualities%20based%20on%20various%20factors%20such%20as%20cloud%20coverage%2C%20sky%0Abrightness%2C%20scattering%20light%20from%20the%20optical%20system%2C%20point%20spread%20function%0Asize%20and%20shape%2C%20and%20read-out%20noise.%20Occasionally%2C%20the%20algorithm%20requires%0Amasking%20of%20areas%20severely%20affected%20by%20noise.%20However%2C%20the%20algorithm%20often%0Anecessitates%20significant%20human%20interventions%2C%20reducing%20data%20processing%0Aefficiency.%20In%20this%20study%2C%20we%20present%20a%20deep%20learning%20based%20image%20quality%0Aevaluation%20algorithm%20that%20uses%20an%20autoencoder%20to%20learn%20features%20of%20high%20quality%0Aastronomical%20images.%20The%20trained%20autoencoder%20enables%20automatic%20evaluation%20of%0Aimage%20quality%20and%20masking%20of%20noise%20affected%20areas.%20We%20have%20evaluated%20the%0Aperformance%20of%20our%20algorithm%20using%20two%20test%20cases%3A%20images%20with%20point%20spread%0Afunctions%20of%20varying%20full%20width%20half%20magnitude%2C%20and%20images%20with%20complex%0Abackgrounds.%20In%20the%20first%20scenario%2C%20our%20algorithm%20could%20effectively%20identify%0Avariations%20of%20the%20point%20spread%20functions%2C%20which%20can%20provide%20valuable%20reference%0Ainformation%20for%20photometry.%20In%20the%20second%20scenario%2C%20our%20method%20could%0Asuccessfully%20mask%20regions%20affected%20by%20complex%20regions%2C%20which%20could%0Asignificantly%20increase%20the%20photometry%20accuracy.%20Our%20algorithm%20can%20be%20employed%0Ato%20automatically%20evaluate%20image%20quality%20obtained%20by%20different%20sky%20surveying%0Aprojects%2C%20further%20increasing%20the%20speed%20and%20robustness%20of%20data%20processing%0Apipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Image%2520Quality%2520Evaluation%2520and%2520Masking%2520Algorithm%2520Based%2520On%2520Pre-trained%250A%2520%2520Deep%2520Neural%2520Networks%26entry.906535625%3DPeng%2520Jia%2520and%2520Yu%2520Song%2520and%2520Jiameng%2520Lv%2520and%2520Runyu%2520Ning%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520amount%2520of%2520astronomical%2520data%252C%2520there%2520is%2520an%2520increasing%2520need%2520for%250Aautomated%2520data%2520processing%2520pipelines%252C%2520which%2520can%2520extract%2520scientific%2520information%250Afrom%2520observation%2520data%2520without%2520human%2520interventions.%2520A%2520critical%2520aspect%2520of%2520these%250Apipelines%2520is%2520the%2520image%2520quality%2520evaluation%2520and%2520masking%2520algorithm%252C%2520which%250Aevaluates%2520image%2520qualities%2520based%2520on%2520various%2520factors%2520such%2520as%2520cloud%2520coverage%252C%2520sky%250Abrightness%252C%2520scattering%2520light%2520from%2520the%2520optical%2520system%252C%2520point%2520spread%2520function%250Asize%2520and%2520shape%252C%2520and%2520read-out%2520noise.%2520Occasionally%252C%2520the%2520algorithm%2520requires%250Amasking%2520of%2520areas%2520severely%2520affected%2520by%2520noise.%2520However%252C%2520the%2520algorithm%2520often%250Anecessitates%2520significant%2520human%2520interventions%252C%2520reducing%2520data%2520processing%250Aefficiency.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520deep%2520learning%2520based%2520image%2520quality%250Aevaluation%2520algorithm%2520that%2520uses%2520an%2520autoencoder%2520to%2520learn%2520features%2520of%2520high%2520quality%250Aastronomical%2520images.%2520The%2520trained%2520autoencoder%2520enables%2520automatic%2520evaluation%2520of%250Aimage%2520quality%2520and%2520masking%2520of%2520noise%2520affected%2520areas.%2520We%2520have%2520evaluated%2520the%250Aperformance%2520of%2520our%2520algorithm%2520using%2520two%2520test%2520cases%253A%2520images%2520with%2520point%2520spread%250Afunctions%2520of%2520varying%2520full%2520width%2520half%2520magnitude%252C%2520and%2520images%2520with%2520complex%250Abackgrounds.%2520In%2520the%2520first%2520scenario%252C%2520our%2520algorithm%2520could%2520effectively%2520identify%250Avariations%2520of%2520the%2520point%2520spread%2520functions%252C%2520which%2520can%2520provide%2520valuable%2520reference%250Ainformation%2520for%2520photometry.%2520In%2520the%2520second%2520scenario%252C%2520our%2520method%2520could%250Asuccessfully%2520mask%2520regions%2520affected%2520by%2520complex%2520regions%252C%2520which%2520could%250Asignificantly%2520increase%2520the%2520photometry%2520accuracy.%2520Our%2520algorithm%2520can%2520be%2520employed%250Ato%2520automatically%2520evaluate%2520image%2520quality%2520obtained%2520by%2520different%2520sky%2520surveying%250Aprojects%252C%2520further%2520increasing%2520the%2520speed%2520and%2520robustness%2520of%2520data%2520processing%250Apipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Image%20Quality%20Evaluation%20and%20Masking%20Algorithm%20Based%20On%20Pre-trained%0A%20%20Deep%20Neural%20Networks&entry.906535625=Peng%20Jia%20and%20Yu%20Song%20and%20Jiameng%20Lv%20and%20Runyu%20Ning&entry.1292438233=%20%20With%20the%20growing%20amount%20of%20astronomical%20data%2C%20there%20is%20an%20increasing%20need%20for%0Aautomated%20data%20processing%20pipelines%2C%20which%20can%20extract%20scientific%20information%0Afrom%20observation%20data%20without%20human%20interventions.%20A%20critical%20aspect%20of%20these%0Apipelines%20is%20the%20image%20quality%20evaluation%20and%20masking%20algorithm%2C%20which%0Aevaluates%20image%20qualities%20based%20on%20various%20factors%20such%20as%20cloud%20coverage%2C%20sky%0Abrightness%2C%20scattering%20light%20from%20the%20optical%20system%2C%20point%20spread%20function%0Asize%20and%20shape%2C%20and%20read-out%20noise.%20Occasionally%2C%20the%20algorithm%20requires%0Amasking%20of%20areas%20severely%20affected%20by%20noise.%20However%2C%20the%20algorithm%20often%0Anecessitates%20significant%20human%20interventions%2C%20reducing%20data%20processing%0Aefficiency.%20In%20this%20study%2C%20we%20present%20a%20deep%20learning%20based%20image%20quality%0Aevaluation%20algorithm%20that%20uses%20an%20autoencoder%20to%20learn%20features%20of%20high%20quality%0Aastronomical%20images.%20The%20trained%20autoencoder%20enables%20automatic%20evaluation%20of%0Aimage%20quality%20and%20masking%20of%20noise%20affected%20areas.%20We%20have%20evaluated%20the%0Aperformance%20of%20our%20algorithm%20using%20two%20test%20cases%3A%20images%20with%20point%20spread%0Afunctions%20of%20varying%20full%20width%20half%20magnitude%2C%20and%20images%20with%20complex%0Abackgrounds.%20In%20the%20first%20scenario%2C%20our%20algorithm%20could%20effectively%20identify%0Avariations%20of%20the%20point%20spread%20functions%2C%20which%20can%20provide%20valuable%20reference%0Ainformation%20for%20photometry.%20In%20the%20second%20scenario%2C%20our%20method%20could%0Asuccessfully%20mask%20regions%20affected%20by%20complex%20regions%2C%20which%20could%0Asignificantly%20increase%20the%20photometry%20accuracy.%20Our%20algorithm%20can%20be%20employed%0Ato%20automatically%20evaluate%20image%20quality%20obtained%20by%20different%20sky%20surveying%0Aprojects%2C%20further%20increasing%20the%20speed%20and%20robustness%20of%20data%20processing%0Apipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03408v1&entry.124074799=Read"},
{"title": "Dual Relation Mining Network for Zero-Shot Learning", "author": "Jinwei Han and Yingguo Gao and Zhiwen Lin and Ke Yan and Shouhong Ding and Yuan Gao and Gui-Song Xia", "abstract": "  Zero-shot learning (ZSL) aims to recognize novel classes through transferring\nshared semantic knowledge (e.g., attributes) from seen classes to unseen\nclasses. Recently, attention-based methods have exhibited significant progress\nwhich align visual features and attributes via a spatial attention mechanism.\nHowever, these methods only explore visual-semantic relationship in the spatial\ndimension, which can lead to classification ambiguity when different attributes\nshare similar attention regions, and semantic relationship between attributes\nis rarely discussed. To alleviate the above problems, we propose a Dual\nRelation Mining Network (DRMN) to enable more effective visual-semantic\ninteractions and learn semantic relationship among attributes for knowledge\ntransfer. Specifically, we introduce a Dual Attention Block (DAB) for\nvisual-semantic relationship mining, which enriches visual information by\nmulti-level feature fusion and conducts spatial attention for visual to\nsemantic embedding. Moreover, an attribute-guided channel attention is utilized\nto decouple entangled semantic features. For semantic relationship modeling, we\nutilize a Semantic Interaction Transformer (SIT) to enhance the generalization\nof attribute representations among images. Additionally, a global\nclassification branch is introduced as a complement to human-defined semantic\nattributes, and we then combine the results with attribute-based\nclassification. Extensive experiments demonstrate that the proposed DRMN leads\nto new state-of-the-art performances on three standard ZSL benchmarks, i.e.,\nCUB, SUN, and AwA2.\n", "link": "http://arxiv.org/abs/2405.03613v1", "date": "2024-05-06", "relevancy": 2.6992, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5565}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5393}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Relation%20Mining%20Network%20for%20Zero-Shot%20Learning&body=Title%3A%20Dual%20Relation%20Mining%20Network%20for%20Zero-Shot%20Learning%0AAuthor%3A%20Jinwei%20Han%20and%20Yingguo%20Gao%20and%20Zhiwen%20Lin%20and%20Ke%20Yan%20and%20Shouhong%20Ding%20and%20Yuan%20Gao%20and%20Gui-Song%20Xia%0AAbstract%3A%20%20%20Zero-shot%20learning%20%28ZSL%29%20aims%20to%20recognize%20novel%20classes%20through%20transferring%0Ashared%20semantic%20knowledge%20%28e.g.%2C%20attributes%29%20from%20seen%20classes%20to%20unseen%0Aclasses.%20Recently%2C%20attention-based%20methods%20have%20exhibited%20significant%20progress%0Awhich%20align%20visual%20features%20and%20attributes%20via%20a%20spatial%20attention%20mechanism.%0AHowever%2C%20these%20methods%20only%20explore%20visual-semantic%20relationship%20in%20the%20spatial%0Adimension%2C%20which%20can%20lead%20to%20classification%20ambiguity%20when%20different%20attributes%0Ashare%20similar%20attention%20regions%2C%20and%20semantic%20relationship%20between%20attributes%0Ais%20rarely%20discussed.%20To%20alleviate%20the%20above%20problems%2C%20we%20propose%20a%20Dual%0ARelation%20Mining%20Network%20%28DRMN%29%20to%20enable%20more%20effective%20visual-semantic%0Ainteractions%20and%20learn%20semantic%20relationship%20among%20attributes%20for%20knowledge%0Atransfer.%20Specifically%2C%20we%20introduce%20a%20Dual%20Attention%20Block%20%28DAB%29%20for%0Avisual-semantic%20relationship%20mining%2C%20which%20enriches%20visual%20information%20by%0Amulti-level%20feature%20fusion%20and%20conducts%20spatial%20attention%20for%20visual%20to%0Asemantic%20embedding.%20Moreover%2C%20an%20attribute-guided%20channel%20attention%20is%20utilized%0Ato%20decouple%20entangled%20semantic%20features.%20For%20semantic%20relationship%20modeling%2C%20we%0Autilize%20a%20Semantic%20Interaction%20Transformer%20%28SIT%29%20to%20enhance%20the%20generalization%0Aof%20attribute%20representations%20among%20images.%20Additionally%2C%20a%20global%0Aclassification%20branch%20is%20introduced%20as%20a%20complement%20to%20human-defined%20semantic%0Aattributes%2C%20and%20we%20then%20combine%20the%20results%20with%20attribute-based%0Aclassification.%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20DRMN%20leads%0Ato%20new%20state-of-the-art%20performances%20on%20three%20standard%20ZSL%20benchmarks%2C%20i.e.%2C%0ACUB%2C%20SUN%2C%20and%20AwA2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Relation%2520Mining%2520Network%2520for%2520Zero-Shot%2520Learning%26entry.906535625%3DJinwei%2520Han%2520and%2520Yingguo%2520Gao%2520and%2520Zhiwen%2520Lin%2520and%2520Ke%2520Yan%2520and%2520Shouhong%2520Ding%2520and%2520Yuan%2520Gao%2520and%2520Gui-Song%2520Xia%26entry.1292438233%3D%2520%2520Zero-shot%2520learning%2520%2528ZSL%2529%2520aims%2520to%2520recognize%2520novel%2520classes%2520through%2520transferring%250Ashared%2520semantic%2520knowledge%2520%2528e.g.%252C%2520attributes%2529%2520from%2520seen%2520classes%2520to%2520unseen%250Aclasses.%2520Recently%252C%2520attention-based%2520methods%2520have%2520exhibited%2520significant%2520progress%250Awhich%2520align%2520visual%2520features%2520and%2520attributes%2520via%2520a%2520spatial%2520attention%2520mechanism.%250AHowever%252C%2520these%2520methods%2520only%2520explore%2520visual-semantic%2520relationship%2520in%2520the%2520spatial%250Adimension%252C%2520which%2520can%2520lead%2520to%2520classification%2520ambiguity%2520when%2520different%2520attributes%250Ashare%2520similar%2520attention%2520regions%252C%2520and%2520semantic%2520relationship%2520between%2520attributes%250Ais%2520rarely%2520discussed.%2520To%2520alleviate%2520the%2520above%2520problems%252C%2520we%2520propose%2520a%2520Dual%250ARelation%2520Mining%2520Network%2520%2528DRMN%2529%2520to%2520enable%2520more%2520effective%2520visual-semantic%250Ainteractions%2520and%2520learn%2520semantic%2520relationship%2520among%2520attributes%2520for%2520knowledge%250Atransfer.%2520Specifically%252C%2520we%2520introduce%2520a%2520Dual%2520Attention%2520Block%2520%2528DAB%2529%2520for%250Avisual-semantic%2520relationship%2520mining%252C%2520which%2520enriches%2520visual%2520information%2520by%250Amulti-level%2520feature%2520fusion%2520and%2520conducts%2520spatial%2520attention%2520for%2520visual%2520to%250Asemantic%2520embedding.%2520Moreover%252C%2520an%2520attribute-guided%2520channel%2520attention%2520is%2520utilized%250Ato%2520decouple%2520entangled%2520semantic%2520features.%2520For%2520semantic%2520relationship%2520modeling%252C%2520we%250Autilize%2520a%2520Semantic%2520Interaction%2520Transformer%2520%2528SIT%2529%2520to%2520enhance%2520the%2520generalization%250Aof%2520attribute%2520representations%2520among%2520images.%2520Additionally%252C%2520a%2520global%250Aclassification%2520branch%2520is%2520introduced%2520as%2520a%2520complement%2520to%2520human-defined%2520semantic%250Aattributes%252C%2520and%2520we%2520then%2520combine%2520the%2520results%2520with%2520attribute-based%250Aclassification.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520DRMN%2520leads%250Ato%2520new%2520state-of-the-art%2520performances%2520on%2520three%2520standard%2520ZSL%2520benchmarks%252C%2520i.e.%252C%250ACUB%252C%2520SUN%252C%2520and%2520AwA2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Relation%20Mining%20Network%20for%20Zero-Shot%20Learning&entry.906535625=Jinwei%20Han%20and%20Yingguo%20Gao%20and%20Zhiwen%20Lin%20and%20Ke%20Yan%20and%20Shouhong%20Ding%20and%20Yuan%20Gao%20and%20Gui-Song%20Xia&entry.1292438233=%20%20Zero-shot%20learning%20%28ZSL%29%20aims%20to%20recognize%20novel%20classes%20through%20transferring%0Ashared%20semantic%20knowledge%20%28e.g.%2C%20attributes%29%20from%20seen%20classes%20to%20unseen%0Aclasses.%20Recently%2C%20attention-based%20methods%20have%20exhibited%20significant%20progress%0Awhich%20align%20visual%20features%20and%20attributes%20via%20a%20spatial%20attention%20mechanism.%0AHowever%2C%20these%20methods%20only%20explore%20visual-semantic%20relationship%20in%20the%20spatial%0Adimension%2C%20which%20can%20lead%20to%20classification%20ambiguity%20when%20different%20attributes%0Ashare%20similar%20attention%20regions%2C%20and%20semantic%20relationship%20between%20attributes%0Ais%20rarely%20discussed.%20To%20alleviate%20the%20above%20problems%2C%20we%20propose%20a%20Dual%0ARelation%20Mining%20Network%20%28DRMN%29%20to%20enable%20more%20effective%20visual-semantic%0Ainteractions%20and%20learn%20semantic%20relationship%20among%20attributes%20for%20knowledge%0Atransfer.%20Specifically%2C%20we%20introduce%20a%20Dual%20Attention%20Block%20%28DAB%29%20for%0Avisual-semantic%20relationship%20mining%2C%20which%20enriches%20visual%20information%20by%0Amulti-level%20feature%20fusion%20and%20conducts%20spatial%20attention%20for%20visual%20to%0Asemantic%20embedding.%20Moreover%2C%20an%20attribute-guided%20channel%20attention%20is%20utilized%0Ato%20decouple%20entangled%20semantic%20features.%20For%20semantic%20relationship%20modeling%2C%20we%0Autilize%20a%20Semantic%20Interaction%20Transformer%20%28SIT%29%20to%20enhance%20the%20generalization%0Aof%20attribute%20representations%20among%20images.%20Additionally%2C%20a%20global%0Aclassification%20branch%20is%20introduced%20as%20a%20complement%20to%20human-defined%20semantic%0Aattributes%2C%20and%20we%20then%20combine%20the%20results%20with%20attribute-based%0Aclassification.%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20DRMN%20leads%0Ato%20new%20state-of-the-art%20performances%20on%20three%20standard%20ZSL%20benchmarks%2C%20i.e.%2C%0ACUB%2C%20SUN%2C%20and%20AwA2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03613v1&entry.124074799=Read"},
{"title": "ShadowNav: Autonomous Global Localization for Lunar Navigation in\n  Darkness", "author": "Deegan Atha and R. Michael Swan and Abhishek Cauligi and Anne Bettens and Edwin Goh and Dima Kogan and Larry Matthies and Masahiro Ono", "abstract": "  The ability to determine the pose of a rover in an inertial frame\nautonomously is a crucial capability necessary for the next generation of\nsurface rover missions on other planetary bodies. Currently, most on-going\nrover missions utilize ground-in-the-loop interventions to manually correct for\ndrift in the pose estimate and this human supervision bottlenecks the distance\nover which rovers can operate autonomously and carry out scientific\nmeasurements. In this paper, we present ShadowNav, an autonomous approach for\nglobal localization on the Moon with an emphasis on driving in darkness and at\nnighttime. Our approach uses the leading edge of Lunar craters as landmarks and\na particle filtering approach is used to associate detected craters with known\nones on an offboard map. We discuss the key design decisions in developing the\nShadowNav framework for use with a Lunar rover concept equipped with a stereo\ncamera and an external illumination source. Finally, we demonstrate the\nefficacy of our proposed approach in both a Lunar simulation environment and on\ndata collected during a field test at Cinder Lakes, Arizona.\n", "link": "http://arxiv.org/abs/2405.01673v2", "date": "2024-05-06", "relevancy": 2.6865, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5858}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5268}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShadowNav%3A%20Autonomous%20Global%20Localization%20for%20Lunar%20Navigation%20in%0A%20%20Darkness&body=Title%3A%20ShadowNav%3A%20Autonomous%20Global%20Localization%20for%20Lunar%20Navigation%20in%0A%20%20Darkness%0AAuthor%3A%20Deegan%20Atha%20and%20R.%20Michael%20Swan%20and%20Abhishek%20Cauligi%20and%20Anne%20Bettens%20and%20Edwin%20Goh%20and%20Dima%20Kogan%20and%20Larry%20Matthies%20and%20Masahiro%20Ono%0AAbstract%3A%20%20%20The%20ability%20to%20determine%20the%20pose%20of%20a%20rover%20in%20an%20inertial%20frame%0Aautonomously%20is%20a%20crucial%20capability%20necessary%20for%20the%20next%20generation%20of%0Asurface%20rover%20missions%20on%20other%20planetary%20bodies.%20Currently%2C%20most%20on-going%0Arover%20missions%20utilize%20ground-in-the-loop%20interventions%20to%20manually%20correct%20for%0Adrift%20in%20the%20pose%20estimate%20and%20this%20human%20supervision%20bottlenecks%20the%20distance%0Aover%20which%20rovers%20can%20operate%20autonomously%20and%20carry%20out%20scientific%0Ameasurements.%20In%20this%20paper%2C%20we%20present%20ShadowNav%2C%20an%20autonomous%20approach%20for%0Aglobal%20localization%20on%20the%20Moon%20with%20an%20emphasis%20on%20driving%20in%20darkness%20and%20at%0Anighttime.%20Our%20approach%20uses%20the%20leading%20edge%20of%20Lunar%20craters%20as%20landmarks%20and%0Aa%20particle%20filtering%20approach%20is%20used%20to%20associate%20detected%20craters%20with%20known%0Aones%20on%20an%20offboard%20map.%20We%20discuss%20the%20key%20design%20decisions%20in%20developing%20the%0AShadowNav%20framework%20for%20use%20with%20a%20Lunar%20rover%20concept%20equipped%20with%20a%20stereo%0Acamera%20and%20an%20external%20illumination%20source.%20Finally%2C%20we%20demonstrate%20the%0Aefficacy%20of%20our%20proposed%20approach%20in%20both%20a%20Lunar%20simulation%20environment%20and%20on%0Adata%20collected%20during%20a%20field%20test%20at%20Cinder%20Lakes%2C%20Arizona.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01673v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShadowNav%253A%2520Autonomous%2520Global%2520Localization%2520for%2520Lunar%2520Navigation%2520in%250A%2520%2520Darkness%26entry.906535625%3DDeegan%2520Atha%2520and%2520R.%2520Michael%2520Swan%2520and%2520Abhishek%2520Cauligi%2520and%2520Anne%2520Bettens%2520and%2520Edwin%2520Goh%2520and%2520Dima%2520Kogan%2520and%2520Larry%2520Matthies%2520and%2520Masahiro%2520Ono%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520determine%2520the%2520pose%2520of%2520a%2520rover%2520in%2520an%2520inertial%2520frame%250Aautonomously%2520is%2520a%2520crucial%2520capability%2520necessary%2520for%2520the%2520next%2520generation%2520of%250Asurface%2520rover%2520missions%2520on%2520other%2520planetary%2520bodies.%2520Currently%252C%2520most%2520on-going%250Arover%2520missions%2520utilize%2520ground-in-the-loop%2520interventions%2520to%2520manually%2520correct%2520for%250Adrift%2520in%2520the%2520pose%2520estimate%2520and%2520this%2520human%2520supervision%2520bottlenecks%2520the%2520distance%250Aover%2520which%2520rovers%2520can%2520operate%2520autonomously%2520and%2520carry%2520out%2520scientific%250Ameasurements.%2520In%2520this%2520paper%252C%2520we%2520present%2520ShadowNav%252C%2520an%2520autonomous%2520approach%2520for%250Aglobal%2520localization%2520on%2520the%2520Moon%2520with%2520an%2520emphasis%2520on%2520driving%2520in%2520darkness%2520and%2520at%250Anighttime.%2520Our%2520approach%2520uses%2520the%2520leading%2520edge%2520of%2520Lunar%2520craters%2520as%2520landmarks%2520and%250Aa%2520particle%2520filtering%2520approach%2520is%2520used%2520to%2520associate%2520detected%2520craters%2520with%2520known%250Aones%2520on%2520an%2520offboard%2520map.%2520We%2520discuss%2520the%2520key%2520design%2520decisions%2520in%2520developing%2520the%250AShadowNav%2520framework%2520for%2520use%2520with%2520a%2520Lunar%2520rover%2520concept%2520equipped%2520with%2520a%2520stereo%250Acamera%2520and%2520an%2520external%2520illumination%2520source.%2520Finally%252C%2520we%2520demonstrate%2520the%250Aefficacy%2520of%2520our%2520proposed%2520approach%2520in%2520both%2520a%2520Lunar%2520simulation%2520environment%2520and%2520on%250Adata%2520collected%2520during%2520a%2520field%2520test%2520at%2520Cinder%2520Lakes%252C%2520Arizona.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01673v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShadowNav%3A%20Autonomous%20Global%20Localization%20for%20Lunar%20Navigation%20in%0A%20%20Darkness&entry.906535625=Deegan%20Atha%20and%20R.%20Michael%20Swan%20and%20Abhishek%20Cauligi%20and%20Anne%20Bettens%20and%20Edwin%20Goh%20and%20Dima%20Kogan%20and%20Larry%20Matthies%20and%20Masahiro%20Ono&entry.1292438233=%20%20The%20ability%20to%20determine%20the%20pose%20of%20a%20rover%20in%20an%20inertial%20frame%0Aautonomously%20is%20a%20crucial%20capability%20necessary%20for%20the%20next%20generation%20of%0Asurface%20rover%20missions%20on%20other%20planetary%20bodies.%20Currently%2C%20most%20on-going%0Arover%20missions%20utilize%20ground-in-the-loop%20interventions%20to%20manually%20correct%20for%0Adrift%20in%20the%20pose%20estimate%20and%20this%20human%20supervision%20bottlenecks%20the%20distance%0Aover%20which%20rovers%20can%20operate%20autonomously%20and%20carry%20out%20scientific%0Ameasurements.%20In%20this%20paper%2C%20we%20present%20ShadowNav%2C%20an%20autonomous%20approach%20for%0Aglobal%20localization%20on%20the%20Moon%20with%20an%20emphasis%20on%20driving%20in%20darkness%20and%20at%0Anighttime.%20Our%20approach%20uses%20the%20leading%20edge%20of%20Lunar%20craters%20as%20landmarks%20and%0Aa%20particle%20filtering%20approach%20is%20used%20to%20associate%20detected%20craters%20with%20known%0Aones%20on%20an%20offboard%20map.%20We%20discuss%20the%20key%20design%20decisions%20in%20developing%20the%0AShadowNav%20framework%20for%20use%20with%20a%20Lunar%20rover%20concept%20equipped%20with%20a%20stereo%0Acamera%20and%20an%20external%20illumination%20source.%20Finally%2C%20we%20demonstrate%20the%0Aefficacy%20of%20our%20proposed%20approach%20in%20both%20a%20Lunar%20simulation%20environment%20and%20on%0Adata%20collected%20during%20a%20field%20test%20at%20Cinder%20Lakes%2C%20Arizona.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01673v2&entry.124074799=Read"},
{"title": "A Linear Time and Space Local Point Cloud Geometry Encoder via\n  Vectorized Kernel Mixture (VecKM)", "author": "Dehao Yuan and Cornelia Ferm\u00fcller and Tahseen Rabbani and Furong Huang and Yiannis Aloimonos", "abstract": "  We propose VecKM, a local point cloud geometry encoder that is descriptive\nand efficient to compute. VecKM leverages a unique approach by vectorizing a\nkernel mixture to represent the local point cloud. Such representation's\ndescriptiveness is supported by two theorems that validate its ability to\nreconstruct and preserve the similarity of the local shape. Unlike existing\nencoders downsampling the local point cloud, VecKM constructs the local\ngeometry encoding using all neighboring points, producing a more descriptive\nencoding.\n  Moreover, VecKM is efficient to compute and scalable to large point cloud\ninputs: VecKM reduces the memory cost from $(n^2+nKd)$ to $(nd+np)$; and\nreduces the major runtime cost from computing $nK$ MLPs to $n$ MLPs, where $n$\nis the size of the point cloud, $K$ is the neighborhood size, $d$ is the\nencoding dimension, and $p$ is a marginal factor. The efficiency is due to\nVecKM's unique factorizable property that eliminates the need of explicitly\ngrouping points into neighbors.\n  In the normal estimation task, VecKM demonstrates not only 100x faster\ninference speed but also highest accuracy and strongest robustness. In\nclassification and segmentation tasks, integrating VecKM as a preprocessing\nmodule achieves consistently better performance than the PointNet, PointNet++,\nand point transformer baselines, and runs consistently faster by up to 10\ntimes.\n", "link": "http://arxiv.org/abs/2404.01568v2", "date": "2024-05-06", "relevancy": 2.6505, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5469}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5235}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Linear%20Time%20and%20Space%20Local%20Point%20Cloud%20Geometry%20Encoder%20via%0A%20%20Vectorized%20Kernel%20Mixture%20%28VecKM%29&body=Title%3A%20A%20Linear%20Time%20and%20Space%20Local%20Point%20Cloud%20Geometry%20Encoder%20via%0A%20%20Vectorized%20Kernel%20Mixture%20%28VecKM%29%0AAuthor%3A%20Dehao%20Yuan%20and%20Cornelia%20Ferm%C3%BCller%20and%20Tahseen%20Rabbani%20and%20Furong%20Huang%20and%20Yiannis%20Aloimonos%0AAbstract%3A%20%20%20We%20propose%20VecKM%2C%20a%20local%20point%20cloud%20geometry%20encoder%20that%20is%20descriptive%0Aand%20efficient%20to%20compute.%20VecKM%20leverages%20a%20unique%20approach%20by%20vectorizing%20a%0Akernel%20mixture%20to%20represent%20the%20local%20point%20cloud.%20Such%20representation%27s%0Adescriptiveness%20is%20supported%20by%20two%20theorems%20that%20validate%20its%20ability%20to%0Areconstruct%20and%20preserve%20the%20similarity%20of%20the%20local%20shape.%20Unlike%20existing%0Aencoders%20downsampling%20the%20local%20point%20cloud%2C%20VecKM%20constructs%20the%20local%0Ageometry%20encoding%20using%20all%20neighboring%20points%2C%20producing%20a%20more%20descriptive%0Aencoding.%0A%20%20Moreover%2C%20VecKM%20is%20efficient%20to%20compute%20and%20scalable%20to%20large%20point%20cloud%0Ainputs%3A%20VecKM%20reduces%20the%20memory%20cost%20from%20%24%28n%5E2%2BnKd%29%24%20to%20%24%28nd%2Bnp%29%24%3B%20and%0Areduces%20the%20major%20runtime%20cost%20from%20computing%20%24nK%24%20MLPs%20to%20%24n%24%20MLPs%2C%20where%20%24n%24%0Ais%20the%20size%20of%20the%20point%20cloud%2C%20%24K%24%20is%20the%20neighborhood%20size%2C%20%24d%24%20is%20the%0Aencoding%20dimension%2C%20and%20%24p%24%20is%20a%20marginal%20factor.%20The%20efficiency%20is%20due%20to%0AVecKM%27s%20unique%20factorizable%20property%20that%20eliminates%20the%20need%20of%20explicitly%0Agrouping%20points%20into%20neighbors.%0A%20%20In%20the%20normal%20estimation%20task%2C%20VecKM%20demonstrates%20not%20only%20100x%20faster%0Ainference%20speed%20but%20also%20highest%20accuracy%20and%20strongest%20robustness.%20In%0Aclassification%20and%20segmentation%20tasks%2C%20integrating%20VecKM%20as%20a%20preprocessing%0Amodule%20achieves%20consistently%20better%20performance%20than%20the%20PointNet%2C%20PointNet%2B%2B%2C%0Aand%20point%20transformer%20baselines%2C%20and%20runs%20consistently%20faster%20by%20up%20to%2010%0Atimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01568v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Linear%2520Time%2520and%2520Space%2520Local%2520Point%2520Cloud%2520Geometry%2520Encoder%2520via%250A%2520%2520Vectorized%2520Kernel%2520Mixture%2520%2528VecKM%2529%26entry.906535625%3DDehao%2520Yuan%2520and%2520Cornelia%2520Ferm%25C3%25BCller%2520and%2520Tahseen%2520Rabbani%2520and%2520Furong%2520Huang%2520and%2520Yiannis%2520Aloimonos%26entry.1292438233%3D%2520%2520We%2520propose%2520VecKM%252C%2520a%2520local%2520point%2520cloud%2520geometry%2520encoder%2520that%2520is%2520descriptive%250Aand%2520efficient%2520to%2520compute.%2520VecKM%2520leverages%2520a%2520unique%2520approach%2520by%2520vectorizing%2520a%250Akernel%2520mixture%2520to%2520represent%2520the%2520local%2520point%2520cloud.%2520Such%2520representation%2527s%250Adescriptiveness%2520is%2520supported%2520by%2520two%2520theorems%2520that%2520validate%2520its%2520ability%2520to%250Areconstruct%2520and%2520preserve%2520the%2520similarity%2520of%2520the%2520local%2520shape.%2520Unlike%2520existing%250Aencoders%2520downsampling%2520the%2520local%2520point%2520cloud%252C%2520VecKM%2520constructs%2520the%2520local%250Ageometry%2520encoding%2520using%2520all%2520neighboring%2520points%252C%2520producing%2520a%2520more%2520descriptive%250Aencoding.%250A%2520%2520Moreover%252C%2520VecKM%2520is%2520efficient%2520to%2520compute%2520and%2520scalable%2520to%2520large%2520point%2520cloud%250Ainputs%253A%2520VecKM%2520reduces%2520the%2520memory%2520cost%2520from%2520%2524%2528n%255E2%252BnKd%2529%2524%2520to%2520%2524%2528nd%252Bnp%2529%2524%253B%2520and%250Areduces%2520the%2520major%2520runtime%2520cost%2520from%2520computing%2520%2524nK%2524%2520MLPs%2520to%2520%2524n%2524%2520MLPs%252C%2520where%2520%2524n%2524%250Ais%2520the%2520size%2520of%2520the%2520point%2520cloud%252C%2520%2524K%2524%2520is%2520the%2520neighborhood%2520size%252C%2520%2524d%2524%2520is%2520the%250Aencoding%2520dimension%252C%2520and%2520%2524p%2524%2520is%2520a%2520marginal%2520factor.%2520The%2520efficiency%2520is%2520due%2520to%250AVecKM%2527s%2520unique%2520factorizable%2520property%2520that%2520eliminates%2520the%2520need%2520of%2520explicitly%250Agrouping%2520points%2520into%2520neighbors.%250A%2520%2520In%2520the%2520normal%2520estimation%2520task%252C%2520VecKM%2520demonstrates%2520not%2520only%2520100x%2520faster%250Ainference%2520speed%2520but%2520also%2520highest%2520accuracy%2520and%2520strongest%2520robustness.%2520In%250Aclassification%2520and%2520segmentation%2520tasks%252C%2520integrating%2520VecKM%2520as%2520a%2520preprocessing%250Amodule%2520achieves%2520consistently%2520better%2520performance%2520than%2520the%2520PointNet%252C%2520PointNet%252B%252B%252C%250Aand%2520point%2520transformer%2520baselines%252C%2520and%2520runs%2520consistently%2520faster%2520by%2520up%2520to%252010%250Atimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01568v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Linear%20Time%20and%20Space%20Local%20Point%20Cloud%20Geometry%20Encoder%20via%0A%20%20Vectorized%20Kernel%20Mixture%20%28VecKM%29&entry.906535625=Dehao%20Yuan%20and%20Cornelia%20Ferm%C3%BCller%20and%20Tahseen%20Rabbani%20and%20Furong%20Huang%20and%20Yiannis%20Aloimonos&entry.1292438233=%20%20We%20propose%20VecKM%2C%20a%20local%20point%20cloud%20geometry%20encoder%20that%20is%20descriptive%0Aand%20efficient%20to%20compute.%20VecKM%20leverages%20a%20unique%20approach%20by%20vectorizing%20a%0Akernel%20mixture%20to%20represent%20the%20local%20point%20cloud.%20Such%20representation%27s%0Adescriptiveness%20is%20supported%20by%20two%20theorems%20that%20validate%20its%20ability%20to%0Areconstruct%20and%20preserve%20the%20similarity%20of%20the%20local%20shape.%20Unlike%20existing%0Aencoders%20downsampling%20the%20local%20point%20cloud%2C%20VecKM%20constructs%20the%20local%0Ageometry%20encoding%20using%20all%20neighboring%20points%2C%20producing%20a%20more%20descriptive%0Aencoding.%0A%20%20Moreover%2C%20VecKM%20is%20efficient%20to%20compute%20and%20scalable%20to%20large%20point%20cloud%0Ainputs%3A%20VecKM%20reduces%20the%20memory%20cost%20from%20%24%28n%5E2%2BnKd%29%24%20to%20%24%28nd%2Bnp%29%24%3B%20and%0Areduces%20the%20major%20runtime%20cost%20from%20computing%20%24nK%24%20MLPs%20to%20%24n%24%20MLPs%2C%20where%20%24n%24%0Ais%20the%20size%20of%20the%20point%20cloud%2C%20%24K%24%20is%20the%20neighborhood%20size%2C%20%24d%24%20is%20the%0Aencoding%20dimension%2C%20and%20%24p%24%20is%20a%20marginal%20factor.%20The%20efficiency%20is%20due%20to%0AVecKM%27s%20unique%20factorizable%20property%20that%20eliminates%20the%20need%20of%20explicitly%0Agrouping%20points%20into%20neighbors.%0A%20%20In%20the%20normal%20estimation%20task%2C%20VecKM%20demonstrates%20not%20only%20100x%20faster%0Ainference%20speed%20but%20also%20highest%20accuracy%20and%20strongest%20robustness.%20In%0Aclassification%20and%20segmentation%20tasks%2C%20integrating%20VecKM%20as%20a%20preprocessing%0Amodule%20achieves%20consistently%20better%20performance%20than%20the%20PointNet%2C%20PointNet%2B%2B%2C%0Aand%20point%20transformer%20baselines%2C%20and%20runs%20consistently%20faster%20by%20up%20to%2010%0Atimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01568v2&entry.124074799=Read"},
{"title": "Generate Point Clouds with Multiscale Details from Graph-Represented\n  Structures", "author": "Ximing Yang and Zhibo Zhang and Zhengfu He and Cheng Jin", "abstract": "  As details are missing in most representations of structures, the lack of\ncontrollability to more information is one of the major weaknesses in\nstructure-based controllable point cloud generation. It is observable that\ndefinitions of details and structures are subjective. Details can be treated as\nstructures on small scales. To represent structures in different scales at the\nsame time, we present a graph-based representation of structures called the\nMultiscale Structure Graph (MSG). Given structures in multiple scales, similar\npatterns of local structures can be found at different scales, positions, and\nangles. The knowledge learned from a regional structure pattern shall be\ntransferred to other similar patterns. An encoding and generation mechanism,\nnamely the Multiscale Structure-based Point Cloud Generator (MSPCG) is\nproposed, which can simultaneously learn point cloud generation from local\npatterns with miscellaneous spatial properties. The proposed method supports\nmultiscale editions on point clouds by editing the MSG. By generating point\nclouds from local structures and learning simultaneously in multiple scales,\nour MSPCG has better generalization ability and scalability. Trained on the\nShapeNet, our MSPCG can generate point clouds from a given structure for unseen\ncategories and indoor scenes. The experimental results show that our method\nsignificantly outperforms baseline methods.\n", "link": "http://arxiv.org/abs/2112.06433v3", "date": "2024-05-06", "relevancy": 2.6489, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.56}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5333}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generate%20Point%20Clouds%20with%20Multiscale%20Details%20from%20Graph-Represented%0A%20%20Structures&body=Title%3A%20Generate%20Point%20Clouds%20with%20Multiscale%20Details%20from%20Graph-Represented%0A%20%20Structures%0AAuthor%3A%20Ximing%20Yang%20and%20Zhibo%20Zhang%20and%20Zhengfu%20He%20and%20Cheng%20Jin%0AAbstract%3A%20%20%20As%20details%20are%20missing%20in%20most%20representations%20of%20structures%2C%20the%20lack%20of%0Acontrollability%20to%20more%20information%20is%20one%20of%20the%20major%20weaknesses%20in%0Astructure-based%20controllable%20point%20cloud%20generation.%20It%20is%20observable%20that%0Adefinitions%20of%20details%20and%20structures%20are%20subjective.%20Details%20can%20be%20treated%20as%0Astructures%20on%20small%20scales.%20To%20represent%20structures%20in%20different%20scales%20at%20the%0Asame%20time%2C%20we%20present%20a%20graph-based%20representation%20of%20structures%20called%20the%0AMultiscale%20Structure%20Graph%20%28MSG%29.%20Given%20structures%20in%20multiple%20scales%2C%20similar%0Apatterns%20of%20local%20structures%20can%20be%20found%20at%20different%20scales%2C%20positions%2C%20and%0Aangles.%20The%20knowledge%20learned%20from%20a%20regional%20structure%20pattern%20shall%20be%0Atransferred%20to%20other%20similar%20patterns.%20An%20encoding%20and%20generation%20mechanism%2C%0Anamely%20the%20Multiscale%20Structure-based%20Point%20Cloud%20Generator%20%28MSPCG%29%20is%0Aproposed%2C%20which%20can%20simultaneously%20learn%20point%20cloud%20generation%20from%20local%0Apatterns%20with%20miscellaneous%20spatial%20properties.%20The%20proposed%20method%20supports%0Amultiscale%20editions%20on%20point%20clouds%20by%20editing%20the%20MSG.%20By%20generating%20point%0Aclouds%20from%20local%20structures%20and%20learning%20simultaneously%20in%20multiple%20scales%2C%0Aour%20MSPCG%20has%20better%20generalization%20ability%20and%20scalability.%20Trained%20on%20the%0AShapeNet%2C%20our%20MSPCG%20can%20generate%20point%20clouds%20from%20a%20given%20structure%20for%20unseen%0Acategories%20and%20indoor%20scenes.%20The%20experimental%20results%20show%20that%20our%20method%0Asignificantly%20outperforms%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2112.06433v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerate%2520Point%2520Clouds%2520with%2520Multiscale%2520Details%2520from%2520Graph-Represented%250A%2520%2520Structures%26entry.906535625%3DXiming%2520Yang%2520and%2520Zhibo%2520Zhang%2520and%2520Zhengfu%2520He%2520and%2520Cheng%2520Jin%26entry.1292438233%3D%2520%2520As%2520details%2520are%2520missing%2520in%2520most%2520representations%2520of%2520structures%252C%2520the%2520lack%2520of%250Acontrollability%2520to%2520more%2520information%2520is%2520one%2520of%2520the%2520major%2520weaknesses%2520in%250Astructure-based%2520controllable%2520point%2520cloud%2520generation.%2520It%2520is%2520observable%2520that%250Adefinitions%2520of%2520details%2520and%2520structures%2520are%2520subjective.%2520Details%2520can%2520be%2520treated%2520as%250Astructures%2520on%2520small%2520scales.%2520To%2520represent%2520structures%2520in%2520different%2520scales%2520at%2520the%250Asame%2520time%252C%2520we%2520present%2520a%2520graph-based%2520representation%2520of%2520structures%2520called%2520the%250AMultiscale%2520Structure%2520Graph%2520%2528MSG%2529.%2520Given%2520structures%2520in%2520multiple%2520scales%252C%2520similar%250Apatterns%2520of%2520local%2520structures%2520can%2520be%2520found%2520at%2520different%2520scales%252C%2520positions%252C%2520and%250Aangles.%2520The%2520knowledge%2520learned%2520from%2520a%2520regional%2520structure%2520pattern%2520shall%2520be%250Atransferred%2520to%2520other%2520similar%2520patterns.%2520An%2520encoding%2520and%2520generation%2520mechanism%252C%250Anamely%2520the%2520Multiscale%2520Structure-based%2520Point%2520Cloud%2520Generator%2520%2528MSPCG%2529%2520is%250Aproposed%252C%2520which%2520can%2520simultaneously%2520learn%2520point%2520cloud%2520generation%2520from%2520local%250Apatterns%2520with%2520miscellaneous%2520spatial%2520properties.%2520The%2520proposed%2520method%2520supports%250Amultiscale%2520editions%2520on%2520point%2520clouds%2520by%2520editing%2520the%2520MSG.%2520By%2520generating%2520point%250Aclouds%2520from%2520local%2520structures%2520and%2520learning%2520simultaneously%2520in%2520multiple%2520scales%252C%250Aour%2520MSPCG%2520has%2520better%2520generalization%2520ability%2520and%2520scalability.%2520Trained%2520on%2520the%250AShapeNet%252C%2520our%2520MSPCG%2520can%2520generate%2520point%2520clouds%2520from%2520a%2520given%2520structure%2520for%2520unseen%250Acategories%2520and%2520indoor%2520scenes.%2520The%2520experimental%2520results%2520show%2520that%2520our%2520method%250Asignificantly%2520outperforms%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2112.06433v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generate%20Point%20Clouds%20with%20Multiscale%20Details%20from%20Graph-Represented%0A%20%20Structures&entry.906535625=Ximing%20Yang%20and%20Zhibo%20Zhang%20and%20Zhengfu%20He%20and%20Cheng%20Jin&entry.1292438233=%20%20As%20details%20are%20missing%20in%20most%20representations%20of%20structures%2C%20the%20lack%20of%0Acontrollability%20to%20more%20information%20is%20one%20of%20the%20major%20weaknesses%20in%0Astructure-based%20controllable%20point%20cloud%20generation.%20It%20is%20observable%20that%0Adefinitions%20of%20details%20and%20structures%20are%20subjective.%20Details%20can%20be%20treated%20as%0Astructures%20on%20small%20scales.%20To%20represent%20structures%20in%20different%20scales%20at%20the%0Asame%20time%2C%20we%20present%20a%20graph-based%20representation%20of%20structures%20called%20the%0AMultiscale%20Structure%20Graph%20%28MSG%29.%20Given%20structures%20in%20multiple%20scales%2C%20similar%0Apatterns%20of%20local%20structures%20can%20be%20found%20at%20different%20scales%2C%20positions%2C%20and%0Aangles.%20The%20knowledge%20learned%20from%20a%20regional%20structure%20pattern%20shall%20be%0Atransferred%20to%20other%20similar%20patterns.%20An%20encoding%20and%20generation%20mechanism%2C%0Anamely%20the%20Multiscale%20Structure-based%20Point%20Cloud%20Generator%20%28MSPCG%29%20is%0Aproposed%2C%20which%20can%20simultaneously%20learn%20point%20cloud%20generation%20from%20local%0Apatterns%20with%20miscellaneous%20spatial%20properties.%20The%20proposed%20method%20supports%0Amultiscale%20editions%20on%20point%20clouds%20by%20editing%20the%20MSG.%20By%20generating%20point%0Aclouds%20from%20local%20structures%20and%20learning%20simultaneously%20in%20multiple%20scales%2C%0Aour%20MSPCG%20has%20better%20generalization%20ability%20and%20scalability.%20Trained%20on%20the%0AShapeNet%2C%20our%20MSPCG%20can%20generate%20point%20clouds%20from%20a%20given%20structure%20for%20unseen%0Acategories%20and%20indoor%20scenes.%20The%20experimental%20results%20show%20that%20our%20method%0Asignificantly%20outperforms%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2112.06433v3&entry.124074799=Read"},
{"title": "Classification of Breast Cancer Histopathology Images using a Modified\n  Supervised Contrastive Learning Method", "author": "Matina Mahdizadeh Sani and Ali Royat and Mahdieh Soleymani Baghshah", "abstract": "  Deep neural networks have reached remarkable achievements in medical image\nprocessing tasks, specifically classifying and detecting various diseases.\nHowever, when confronted with limited data, these networks face a critical\nvulnerability, often succumbing to overfitting by excessively memorizing the\nlimited information available. This work addresses the challenge mentioned\nabove by improving the supervised contrastive learning method to reduce the\nimpact of false positives. Unlike most existing methods that rely predominantly\non fully supervised learning, our approach leverages the advantages of\nself-supervised learning in conjunction with employing the available labeled\ndata. We evaluate our method on the BreakHis dataset, which consists of breast\ncancer histopathology images, and demonstrate an increase in classification\naccuracy by 1.45% at the image level and 1.42% at the patient level compared to\nthe state-of-the-art method. This improvement corresponds to 93.63% absolute\naccuracy, highlighting our approach's effectiveness in leveraging data\nproperties to learn more appropriate representation space.\n", "link": "http://arxiv.org/abs/2405.03642v1", "date": "2024-05-06", "relevancy": 2.6318, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5821}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5002}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20of%20Breast%20Cancer%20Histopathology%20Images%20using%20a%20Modified%0A%20%20Supervised%20Contrastive%20Learning%20Method&body=Title%3A%20Classification%20of%20Breast%20Cancer%20Histopathology%20Images%20using%20a%20Modified%0A%20%20Supervised%20Contrastive%20Learning%20Method%0AAuthor%3A%20Matina%20Mahdizadeh%20Sani%20and%20Ali%20Royat%20and%20Mahdieh%20Soleymani%20Baghshah%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20reached%20remarkable%20achievements%20in%20medical%20image%0Aprocessing%20tasks%2C%20specifically%20classifying%20and%20detecting%20various%20diseases.%0AHowever%2C%20when%20confronted%20with%20limited%20data%2C%20these%20networks%20face%20a%20critical%0Avulnerability%2C%20often%20succumbing%20to%20overfitting%20by%20excessively%20memorizing%20the%0Alimited%20information%20available.%20This%20work%20addresses%20the%20challenge%20mentioned%0Aabove%20by%20improving%20the%20supervised%20contrastive%20learning%20method%20to%20reduce%20the%0Aimpact%20of%20false%20positives.%20Unlike%20most%20existing%20methods%20that%20rely%20predominantly%0Aon%20fully%20supervised%20learning%2C%20our%20approach%20leverages%20the%20advantages%20of%0Aself-supervised%20learning%20in%20conjunction%20with%20employing%20the%20available%20labeled%0Adata.%20We%20evaluate%20our%20method%20on%20the%20BreakHis%20dataset%2C%20which%20consists%20of%20breast%0Acancer%20histopathology%20images%2C%20and%20demonstrate%20an%20increase%20in%20classification%0Aaccuracy%20by%201.45%25%20at%20the%20image%20level%20and%201.42%25%20at%20the%20patient%20level%20compared%20to%0Athe%20state-of-the-art%20method.%20This%20improvement%20corresponds%20to%2093.63%25%20absolute%0Aaccuracy%2C%20highlighting%20our%20approach%27s%20effectiveness%20in%20leveraging%20data%0Aproperties%20to%20learn%20more%20appropriate%20representation%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520of%2520Breast%2520Cancer%2520Histopathology%2520Images%2520using%2520a%2520Modified%250A%2520%2520Supervised%2520Contrastive%2520Learning%2520Method%26entry.906535625%3DMatina%2520Mahdizadeh%2520Sani%2520and%2520Ali%2520Royat%2520and%2520Mahdieh%2520Soleymani%2520Baghshah%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520reached%2520remarkable%2520achievements%2520in%2520medical%2520image%250Aprocessing%2520tasks%252C%2520specifically%2520classifying%2520and%2520detecting%2520various%2520diseases.%250AHowever%252C%2520when%2520confronted%2520with%2520limited%2520data%252C%2520these%2520networks%2520face%2520a%2520critical%250Avulnerability%252C%2520often%2520succumbing%2520to%2520overfitting%2520by%2520excessively%2520memorizing%2520the%250Alimited%2520information%2520available.%2520This%2520work%2520addresses%2520the%2520challenge%2520mentioned%250Aabove%2520by%2520improving%2520the%2520supervised%2520contrastive%2520learning%2520method%2520to%2520reduce%2520the%250Aimpact%2520of%2520false%2520positives.%2520Unlike%2520most%2520existing%2520methods%2520that%2520rely%2520predominantly%250Aon%2520fully%2520supervised%2520learning%252C%2520our%2520approach%2520leverages%2520the%2520advantages%2520of%250Aself-supervised%2520learning%2520in%2520conjunction%2520with%2520employing%2520the%2520available%2520labeled%250Adata.%2520We%2520evaluate%2520our%2520method%2520on%2520the%2520BreakHis%2520dataset%252C%2520which%2520consists%2520of%2520breast%250Acancer%2520histopathology%2520images%252C%2520and%2520demonstrate%2520an%2520increase%2520in%2520classification%250Aaccuracy%2520by%25201.45%2525%2520at%2520the%2520image%2520level%2520and%25201.42%2525%2520at%2520the%2520patient%2520level%2520compared%2520to%250Athe%2520state-of-the-art%2520method.%2520This%2520improvement%2520corresponds%2520to%252093.63%2525%2520absolute%250Aaccuracy%252C%2520highlighting%2520our%2520approach%2527s%2520effectiveness%2520in%2520leveraging%2520data%250Aproperties%2520to%2520learn%2520more%2520appropriate%2520representation%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20of%20Breast%20Cancer%20Histopathology%20Images%20using%20a%20Modified%0A%20%20Supervised%20Contrastive%20Learning%20Method&entry.906535625=Matina%20Mahdizadeh%20Sani%20and%20Ali%20Royat%20and%20Mahdieh%20Soleymani%20Baghshah&entry.1292438233=%20%20Deep%20neural%20networks%20have%20reached%20remarkable%20achievements%20in%20medical%20image%0Aprocessing%20tasks%2C%20specifically%20classifying%20and%20detecting%20various%20diseases.%0AHowever%2C%20when%20confronted%20with%20limited%20data%2C%20these%20networks%20face%20a%20critical%0Avulnerability%2C%20often%20succumbing%20to%20overfitting%20by%20excessively%20memorizing%20the%0Alimited%20information%20available.%20This%20work%20addresses%20the%20challenge%20mentioned%0Aabove%20by%20improving%20the%20supervised%20contrastive%20learning%20method%20to%20reduce%20the%0Aimpact%20of%20false%20positives.%20Unlike%20most%20existing%20methods%20that%20rely%20predominantly%0Aon%20fully%20supervised%20learning%2C%20our%20approach%20leverages%20the%20advantages%20of%0Aself-supervised%20learning%20in%20conjunction%20with%20employing%20the%20available%20labeled%0Adata.%20We%20evaluate%20our%20method%20on%20the%20BreakHis%20dataset%2C%20which%20consists%20of%20breast%0Acancer%20histopathology%20images%2C%20and%20demonstrate%20an%20increase%20in%20classification%0Aaccuracy%20by%201.45%25%20at%20the%20image%20level%20and%201.42%25%20at%20the%20patient%20level%20compared%20to%0Athe%20state-of-the-art%20method.%20This%20improvement%20corresponds%20to%2093.63%25%20absolute%0Aaccuracy%2C%20highlighting%20our%20approach%27s%20effectiveness%20in%20leveraging%20data%0Aproperties%20to%20learn%20more%20appropriate%20representation%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03642v1&entry.124074799=Read"},
{"title": "CICA: Content-Injected Contrastive Alignment for Zero-Shot Document\n  Image Classification", "author": "Sankalp Sinha and Muhammad Saif Ullah Khan and Talha Uddin Sheikh and Didier Stricker and Muhammad Zeshan Afzal", "abstract": "  Zero-shot learning has been extensively investigated in the broader field of\nvisual recognition, attracting significant interest recently. However, the\ncurrent work on zero-shot learning in document image classification remains\nscarce. The existing studies either focus exclusively on zero-shot inference,\nor their evaluation does not align with the established criteria of zero-shot\nevaluation in the visual recognition domain. We provide a comprehensive\ndocument image classification analysis in Zero-Shot Learning (ZSL) and\nGeneralized Zero-Shot Learning (GZSL) settings to address this gap. Our\nmethodology and evaluation align with the established practices of this domain.\nAdditionally, we propose zero-shot splits for the RVL-CDIP dataset.\nFurthermore, we introduce CICA (pronounced 'ki-ka'), a framework that enhances\nthe zero-shot learning capabilities of CLIP. CICA consists of a novel 'content\nmodule' designed to leverage any generic document-related textual information.\nThe discriminative features extracted by this module are aligned with CLIP's\ntext and image features using a novel 'coupled-contrastive' loss. Our module\nimproves CLIP's ZSL top-1 accuracy by 6.7% and GZSL harmonic mean by 24% on the\nRVL-CDIP dataset. Our module is lightweight and adds only 3.3% more parameters\nto CLIP. Our work sets the direction for future research in zero-shot document\nclassification.\n", "link": "http://arxiv.org/abs/2405.03660v1", "date": "2024-05-06", "relevancy": 2.6121, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5392}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5149}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CICA%3A%20Content-Injected%20Contrastive%20Alignment%20for%20Zero-Shot%20Document%0A%20%20Image%20Classification&body=Title%3A%20CICA%3A%20Content-Injected%20Contrastive%20Alignment%20for%20Zero-Shot%20Document%0A%20%20Image%20Classification%0AAuthor%3A%20Sankalp%20Sinha%20and%20Muhammad%20Saif%20Ullah%20Khan%20and%20Talha%20Uddin%20Sheikh%20and%20Didier%20Stricker%20and%20Muhammad%20Zeshan%20Afzal%0AAbstract%3A%20%20%20Zero-shot%20learning%20has%20been%20extensively%20investigated%20in%20the%20broader%20field%20of%0Avisual%20recognition%2C%20attracting%20significant%20interest%20recently.%20However%2C%20the%0Acurrent%20work%20on%20zero-shot%20learning%20in%20document%20image%20classification%20remains%0Ascarce.%20The%20existing%20studies%20either%20focus%20exclusively%20on%20zero-shot%20inference%2C%0Aor%20their%20evaluation%20does%20not%20align%20with%20the%20established%20criteria%20of%20zero-shot%0Aevaluation%20in%20the%20visual%20recognition%20domain.%20We%20provide%20a%20comprehensive%0Adocument%20image%20classification%20analysis%20in%20Zero-Shot%20Learning%20%28ZSL%29%20and%0AGeneralized%20Zero-Shot%20Learning%20%28GZSL%29%20settings%20to%20address%20this%20gap.%20Our%0Amethodology%20and%20evaluation%20align%20with%20the%20established%20practices%20of%20this%20domain.%0AAdditionally%2C%20we%20propose%20zero-shot%20splits%20for%20the%20RVL-CDIP%20dataset.%0AFurthermore%2C%20we%20introduce%20CICA%20%28pronounced%20%27ki-ka%27%29%2C%20a%20framework%20that%20enhances%0Athe%20zero-shot%20learning%20capabilities%20of%20CLIP.%20CICA%20consists%20of%20a%20novel%20%27content%0Amodule%27%20designed%20to%20leverage%20any%20generic%20document-related%20textual%20information.%0AThe%20discriminative%20features%20extracted%20by%20this%20module%20are%20aligned%20with%20CLIP%27s%0Atext%20and%20image%20features%20using%20a%20novel%20%27coupled-contrastive%27%20loss.%20Our%20module%0Aimproves%20CLIP%27s%20ZSL%20top-1%20accuracy%20by%206.7%25%20and%20GZSL%20harmonic%20mean%20by%2024%25%20on%20the%0ARVL-CDIP%20dataset.%20Our%20module%20is%20lightweight%20and%20adds%20only%203.3%25%20more%20parameters%0Ato%20CLIP.%20Our%20work%20sets%20the%20direction%20for%20future%20research%20in%20zero-shot%20document%0Aclassification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCICA%253A%2520Content-Injected%2520Contrastive%2520Alignment%2520for%2520Zero-Shot%2520Document%250A%2520%2520Image%2520Classification%26entry.906535625%3DSankalp%2520Sinha%2520and%2520Muhammad%2520Saif%2520Ullah%2520Khan%2520and%2520Talha%2520Uddin%2520Sheikh%2520and%2520Didier%2520Stricker%2520and%2520Muhammad%2520Zeshan%2520Afzal%26entry.1292438233%3D%2520%2520Zero-shot%2520learning%2520has%2520been%2520extensively%2520investigated%2520in%2520the%2520broader%2520field%2520of%250Avisual%2520recognition%252C%2520attracting%2520significant%2520interest%2520recently.%2520However%252C%2520the%250Acurrent%2520work%2520on%2520zero-shot%2520learning%2520in%2520document%2520image%2520classification%2520remains%250Ascarce.%2520The%2520existing%2520studies%2520either%2520focus%2520exclusively%2520on%2520zero-shot%2520inference%252C%250Aor%2520their%2520evaluation%2520does%2520not%2520align%2520with%2520the%2520established%2520criteria%2520of%2520zero-shot%250Aevaluation%2520in%2520the%2520visual%2520recognition%2520domain.%2520We%2520provide%2520a%2520comprehensive%250Adocument%2520image%2520classification%2520analysis%2520in%2520Zero-Shot%2520Learning%2520%2528ZSL%2529%2520and%250AGeneralized%2520Zero-Shot%2520Learning%2520%2528GZSL%2529%2520settings%2520to%2520address%2520this%2520gap.%2520Our%250Amethodology%2520and%2520evaluation%2520align%2520with%2520the%2520established%2520practices%2520of%2520this%2520domain.%250AAdditionally%252C%2520we%2520propose%2520zero-shot%2520splits%2520for%2520the%2520RVL-CDIP%2520dataset.%250AFurthermore%252C%2520we%2520introduce%2520CICA%2520%2528pronounced%2520%2527ki-ka%2527%2529%252C%2520a%2520framework%2520that%2520enhances%250Athe%2520zero-shot%2520learning%2520capabilities%2520of%2520CLIP.%2520CICA%2520consists%2520of%2520a%2520novel%2520%2527content%250Amodule%2527%2520designed%2520to%2520leverage%2520any%2520generic%2520document-related%2520textual%2520information.%250AThe%2520discriminative%2520features%2520extracted%2520by%2520this%2520module%2520are%2520aligned%2520with%2520CLIP%2527s%250Atext%2520and%2520image%2520features%2520using%2520a%2520novel%2520%2527coupled-contrastive%2527%2520loss.%2520Our%2520module%250Aimproves%2520CLIP%2527s%2520ZSL%2520top-1%2520accuracy%2520by%25206.7%2525%2520and%2520GZSL%2520harmonic%2520mean%2520by%252024%2525%2520on%2520the%250ARVL-CDIP%2520dataset.%2520Our%2520module%2520is%2520lightweight%2520and%2520adds%2520only%25203.3%2525%2520more%2520parameters%250Ato%2520CLIP.%2520Our%2520work%2520sets%2520the%2520direction%2520for%2520future%2520research%2520in%2520zero-shot%2520document%250Aclassification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CICA%3A%20Content-Injected%20Contrastive%20Alignment%20for%20Zero-Shot%20Document%0A%20%20Image%20Classification&entry.906535625=Sankalp%20Sinha%20and%20Muhammad%20Saif%20Ullah%20Khan%20and%20Talha%20Uddin%20Sheikh%20and%20Didier%20Stricker%20and%20Muhammad%20Zeshan%20Afzal&entry.1292438233=%20%20Zero-shot%20learning%20has%20been%20extensively%20investigated%20in%20the%20broader%20field%20of%0Avisual%20recognition%2C%20attracting%20significant%20interest%20recently.%20However%2C%20the%0Acurrent%20work%20on%20zero-shot%20learning%20in%20document%20image%20classification%20remains%0Ascarce.%20The%20existing%20studies%20either%20focus%20exclusively%20on%20zero-shot%20inference%2C%0Aor%20their%20evaluation%20does%20not%20align%20with%20the%20established%20criteria%20of%20zero-shot%0Aevaluation%20in%20the%20visual%20recognition%20domain.%20We%20provide%20a%20comprehensive%0Adocument%20image%20classification%20analysis%20in%20Zero-Shot%20Learning%20%28ZSL%29%20and%0AGeneralized%20Zero-Shot%20Learning%20%28GZSL%29%20settings%20to%20address%20this%20gap.%20Our%0Amethodology%20and%20evaluation%20align%20with%20the%20established%20practices%20of%20this%20domain.%0AAdditionally%2C%20we%20propose%20zero-shot%20splits%20for%20the%20RVL-CDIP%20dataset.%0AFurthermore%2C%20we%20introduce%20CICA%20%28pronounced%20%27ki-ka%27%29%2C%20a%20framework%20that%20enhances%0Athe%20zero-shot%20learning%20capabilities%20of%20CLIP.%20CICA%20consists%20of%20a%20novel%20%27content%0Amodule%27%20designed%20to%20leverage%20any%20generic%20document-related%20textual%20information.%0AThe%20discriminative%20features%20extracted%20by%20this%20module%20are%20aligned%20with%20CLIP%27s%0Atext%20and%20image%20features%20using%20a%20novel%20%27coupled-contrastive%27%20loss.%20Our%20module%0Aimproves%20CLIP%27s%20ZSL%20top-1%20accuracy%20by%206.7%25%20and%20GZSL%20harmonic%20mean%20by%2024%25%20on%20the%0ARVL-CDIP%20dataset.%20Our%20module%20is%20lightweight%20and%20adds%20only%203.3%25%20more%20parameters%0Ato%20CLIP.%20Our%20work%20sets%20the%20direction%20for%20future%20research%20in%20zero-shot%20document%0Aclassification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03660v1&entry.124074799=Read"},
{"title": "E2GNN: Efficient Graph Neural Network Ensembles for Semi-Supervised\n  Classification", "author": "Xin Zhang and Daochen Zha and Qiaoyu Tan", "abstract": "  This work studies ensemble learning for graph neural networks (GNNs) under\nthe popular semi-supervised setting. Ensemble learning has shown superiority in\nimproving the accuracy and robustness of traditional machine learning by\ncombining the outputs of multiple weak learners. However, adopting a similar\nidea to integrate different GNN models is challenging because of two reasons.\nFirst, GNN is notorious for its poor inference ability, so naively assembling\nmultiple GNN models would deteriorate the inference efficiency. Second, when\nGNN models are trained with few labeled nodes, their performance are limited.\nIn this case, the vanilla ensemble approach, e.g., majority vote, may be\nsub-optimal since most base models, i.e., GNNs, may make the wrong predictions.\nTo this end, in this paper, we propose an efficient ensemble learner--E2GNN to\nassemble multiple GNNs in a learnable way by leveraging both labeled and\nunlabeled nodes. Specifically, we first pre-train different GNN models on a\ngiven data scenario according to the labeled nodes. Next, instead of directly\ncombing their outputs for label inference, we train a simple multi-layer\nperceptron--MLP model to mimic their predictions on both labeled and unlabeled\nnodes. Then the unified MLP model is deployed to infer labels for unlabeled or\nnew nodes. Since the predictions of unlabeled nodes from different GNN models\nmay be incorrect, we develop a reinforced discriminator to effectively filter\nout those wrongly predicted nodes to boost the performance of MLP. By doing\nthis, we suggest a principled approach to tackle the inference issues of GNN\nensembles and maintain the merit of ensemble learning: improved performance.\nComprehensive experiments over both transductive and inductive settings, across\ndifferent GNN backbones and 8 benchmark datasets, demonstrate the superiority\nof E2GNN.\n", "link": "http://arxiv.org/abs/2405.03401v1", "date": "2024-05-06", "relevancy": 2.6016, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5598}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5154}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E2GNN%3A%20Efficient%20Graph%20Neural%20Network%20Ensembles%20for%20Semi-Supervised%0A%20%20Classification&body=Title%3A%20E2GNN%3A%20Efficient%20Graph%20Neural%20Network%20Ensembles%20for%20Semi-Supervised%0A%20%20Classification%0AAuthor%3A%20Xin%20Zhang%20and%20Daochen%20Zha%20and%20Qiaoyu%20Tan%0AAbstract%3A%20%20%20This%20work%20studies%20ensemble%20learning%20for%20graph%20neural%20networks%20%28GNNs%29%20under%0Athe%20popular%20semi-supervised%20setting.%20Ensemble%20learning%20has%20shown%20superiority%20in%0Aimproving%20the%20accuracy%20and%20robustness%20of%20traditional%20machine%20learning%20by%0Acombining%20the%20outputs%20of%20multiple%20weak%20learners.%20However%2C%20adopting%20a%20similar%0Aidea%20to%20integrate%20different%20GNN%20models%20is%20challenging%20because%20of%20two%20reasons.%0AFirst%2C%20GNN%20is%20notorious%20for%20its%20poor%20inference%20ability%2C%20so%20naively%20assembling%0Amultiple%20GNN%20models%20would%20deteriorate%20the%20inference%20efficiency.%20Second%2C%20when%0AGNN%20models%20are%20trained%20with%20few%20labeled%20nodes%2C%20their%20performance%20are%20limited.%0AIn%20this%20case%2C%20the%20vanilla%20ensemble%20approach%2C%20e.g.%2C%20majority%20vote%2C%20may%20be%0Asub-optimal%20since%20most%20base%20models%2C%20i.e.%2C%20GNNs%2C%20may%20make%20the%20wrong%20predictions.%0ATo%20this%20end%2C%20in%20this%20paper%2C%20we%20propose%20an%20efficient%20ensemble%20learner--E2GNN%20to%0Aassemble%20multiple%20GNNs%20in%20a%20learnable%20way%20by%20leveraging%20both%20labeled%20and%0Aunlabeled%20nodes.%20Specifically%2C%20we%20first%20pre-train%20different%20GNN%20models%20on%20a%0Agiven%20data%20scenario%20according%20to%20the%20labeled%20nodes.%20Next%2C%20instead%20of%20directly%0Acombing%20their%20outputs%20for%20label%20inference%2C%20we%20train%20a%20simple%20multi-layer%0Aperceptron--MLP%20model%20to%20mimic%20their%20predictions%20on%20both%20labeled%20and%20unlabeled%0Anodes.%20Then%20the%20unified%20MLP%20model%20is%20deployed%20to%20infer%20labels%20for%20unlabeled%20or%0Anew%20nodes.%20Since%20the%20predictions%20of%20unlabeled%20nodes%20from%20different%20GNN%20models%0Amay%20be%20incorrect%2C%20we%20develop%20a%20reinforced%20discriminator%20to%20effectively%20filter%0Aout%20those%20wrongly%20predicted%20nodes%20to%20boost%20the%20performance%20of%20MLP.%20By%20doing%0Athis%2C%20we%20suggest%20a%20principled%20approach%20to%20tackle%20the%20inference%20issues%20of%20GNN%0Aensembles%20and%20maintain%20the%20merit%20of%20ensemble%20learning%3A%20improved%20performance.%0AComprehensive%20experiments%20over%20both%20transductive%20and%20inductive%20settings%2C%20across%0Adifferent%20GNN%20backbones%20and%208%20benchmark%20datasets%2C%20demonstrate%20the%20superiority%0Aof%20E2GNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE2GNN%253A%2520Efficient%2520Graph%2520Neural%2520Network%2520Ensembles%2520for%2520Semi-Supervised%250A%2520%2520Classification%26entry.906535625%3DXin%2520Zhang%2520and%2520Daochen%2520Zha%2520and%2520Qiaoyu%2520Tan%26entry.1292438233%3D%2520%2520This%2520work%2520studies%2520ensemble%2520learning%2520for%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520under%250Athe%2520popular%2520semi-supervised%2520setting.%2520Ensemble%2520learning%2520has%2520shown%2520superiority%2520in%250Aimproving%2520the%2520accuracy%2520and%2520robustness%2520of%2520traditional%2520machine%2520learning%2520by%250Acombining%2520the%2520outputs%2520of%2520multiple%2520weak%2520learners.%2520However%252C%2520adopting%2520a%2520similar%250Aidea%2520to%2520integrate%2520different%2520GNN%2520models%2520is%2520challenging%2520because%2520of%2520two%2520reasons.%250AFirst%252C%2520GNN%2520is%2520notorious%2520for%2520its%2520poor%2520inference%2520ability%252C%2520so%2520naively%2520assembling%250Amultiple%2520GNN%2520models%2520would%2520deteriorate%2520the%2520inference%2520efficiency.%2520Second%252C%2520when%250AGNN%2520models%2520are%2520trained%2520with%2520few%2520labeled%2520nodes%252C%2520their%2520performance%2520are%2520limited.%250AIn%2520this%2520case%252C%2520the%2520vanilla%2520ensemble%2520approach%252C%2520e.g.%252C%2520majority%2520vote%252C%2520may%2520be%250Asub-optimal%2520since%2520most%2520base%2520models%252C%2520i.e.%252C%2520GNNs%252C%2520may%2520make%2520the%2520wrong%2520predictions.%250ATo%2520this%2520end%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520an%2520efficient%2520ensemble%2520learner--E2GNN%2520to%250Aassemble%2520multiple%2520GNNs%2520in%2520a%2520learnable%2520way%2520by%2520leveraging%2520both%2520labeled%2520and%250Aunlabeled%2520nodes.%2520Specifically%252C%2520we%2520first%2520pre-train%2520different%2520GNN%2520models%2520on%2520a%250Agiven%2520data%2520scenario%2520according%2520to%2520the%2520labeled%2520nodes.%2520Next%252C%2520instead%2520of%2520directly%250Acombing%2520their%2520outputs%2520for%2520label%2520inference%252C%2520we%2520train%2520a%2520simple%2520multi-layer%250Aperceptron--MLP%2520model%2520to%2520mimic%2520their%2520predictions%2520on%2520both%2520labeled%2520and%2520unlabeled%250Anodes.%2520Then%2520the%2520unified%2520MLP%2520model%2520is%2520deployed%2520to%2520infer%2520labels%2520for%2520unlabeled%2520or%250Anew%2520nodes.%2520Since%2520the%2520predictions%2520of%2520unlabeled%2520nodes%2520from%2520different%2520GNN%2520models%250Amay%2520be%2520incorrect%252C%2520we%2520develop%2520a%2520reinforced%2520discriminator%2520to%2520effectively%2520filter%250Aout%2520those%2520wrongly%2520predicted%2520nodes%2520to%2520boost%2520the%2520performance%2520of%2520MLP.%2520By%2520doing%250Athis%252C%2520we%2520suggest%2520a%2520principled%2520approach%2520to%2520tackle%2520the%2520inference%2520issues%2520of%2520GNN%250Aensembles%2520and%2520maintain%2520the%2520merit%2520of%2520ensemble%2520learning%253A%2520improved%2520performance.%250AComprehensive%2520experiments%2520over%2520both%2520transductive%2520and%2520inductive%2520settings%252C%2520across%250Adifferent%2520GNN%2520backbones%2520and%25208%2520benchmark%2520datasets%252C%2520demonstrate%2520the%2520superiority%250Aof%2520E2GNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E2GNN%3A%20Efficient%20Graph%20Neural%20Network%20Ensembles%20for%20Semi-Supervised%0A%20%20Classification&entry.906535625=Xin%20Zhang%20and%20Daochen%20Zha%20and%20Qiaoyu%20Tan&entry.1292438233=%20%20This%20work%20studies%20ensemble%20learning%20for%20graph%20neural%20networks%20%28GNNs%29%20under%0Athe%20popular%20semi-supervised%20setting.%20Ensemble%20learning%20has%20shown%20superiority%20in%0Aimproving%20the%20accuracy%20and%20robustness%20of%20traditional%20machine%20learning%20by%0Acombining%20the%20outputs%20of%20multiple%20weak%20learners.%20However%2C%20adopting%20a%20similar%0Aidea%20to%20integrate%20different%20GNN%20models%20is%20challenging%20because%20of%20two%20reasons.%0AFirst%2C%20GNN%20is%20notorious%20for%20its%20poor%20inference%20ability%2C%20so%20naively%20assembling%0Amultiple%20GNN%20models%20would%20deteriorate%20the%20inference%20efficiency.%20Second%2C%20when%0AGNN%20models%20are%20trained%20with%20few%20labeled%20nodes%2C%20their%20performance%20are%20limited.%0AIn%20this%20case%2C%20the%20vanilla%20ensemble%20approach%2C%20e.g.%2C%20majority%20vote%2C%20may%20be%0Asub-optimal%20since%20most%20base%20models%2C%20i.e.%2C%20GNNs%2C%20may%20make%20the%20wrong%20predictions.%0ATo%20this%20end%2C%20in%20this%20paper%2C%20we%20propose%20an%20efficient%20ensemble%20learner--E2GNN%20to%0Aassemble%20multiple%20GNNs%20in%20a%20learnable%20way%20by%20leveraging%20both%20labeled%20and%0Aunlabeled%20nodes.%20Specifically%2C%20we%20first%20pre-train%20different%20GNN%20models%20on%20a%0Agiven%20data%20scenario%20according%20to%20the%20labeled%20nodes.%20Next%2C%20instead%20of%20directly%0Acombing%20their%20outputs%20for%20label%20inference%2C%20we%20train%20a%20simple%20multi-layer%0Aperceptron--MLP%20model%20to%20mimic%20their%20predictions%20on%20both%20labeled%20and%20unlabeled%0Anodes.%20Then%20the%20unified%20MLP%20model%20is%20deployed%20to%20infer%20labels%20for%20unlabeled%20or%0Anew%20nodes.%20Since%20the%20predictions%20of%20unlabeled%20nodes%20from%20different%20GNN%20models%0Amay%20be%20incorrect%2C%20we%20develop%20a%20reinforced%20discriminator%20to%20effectively%20filter%0Aout%20those%20wrongly%20predicted%20nodes%20to%20boost%20the%20performance%20of%20MLP.%20By%20doing%0Athis%2C%20we%20suggest%20a%20principled%20approach%20to%20tackle%20the%20inference%20issues%20of%20GNN%0Aensembles%20and%20maintain%20the%20merit%20of%20ensemble%20learning%3A%20improved%20performance.%0AComprehensive%20experiments%20over%20both%20transductive%20and%20inductive%20settings%2C%20across%0Adifferent%20GNN%20backbones%20and%208%20benchmark%20datasets%2C%20demonstrate%20the%20superiority%0Aof%20E2GNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03401v1&entry.124074799=Read"},
{"title": "GLIP: Electromagnetic Field Exposure Map Completion by Deep Generative\n  Networks", "author": "Mohammed Mallik and Davy P. Gaillot and Laurent Clavier", "abstract": "  In Spectrum cartography (SC), the generation of exposure maps for radio\nfrequency electromagnetic fields (RF-EMF) spans dimensions of frequency, space,\nand time, which relies on a sparse collection of sensor data, posing a\nchallenging ill-posed inverse problem. Cartography methods based on models\nintegrate designed priors, such as sparsity and low-rank structures, to refine\nthe solution of this inverse problem. In our previous work, EMF exposure map\nreconstruction was achieved by Generative Adversarial Networks (GANs) where\nphysical laws or structural constraints were employed as a prior, but they\nrequire a large amount of labeled data or simulated full maps for training to\nproduce efficient results. In this paper, we present a method to reconstruct\nEMF exposure maps using only the generator network in GANs which does not\nrequire explicit training, thus overcoming the limitations of GANs, such as\nusing reference full exposure maps. This approach uses a prior from sensor data\nas Local Image Prior (LIP) captured by deep convolutional generative networks\nindependent of learning the network parameters from images in an urban\nenvironment. Experimental results show that, even when only sparse sensor data\nare available, our method can produce accurate estimates.\n", "link": "http://arxiv.org/abs/2405.03384v1", "date": "2024-05-06", "relevancy": 2.5558, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5169}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5161}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLIP%3A%20Electromagnetic%20Field%20Exposure%20Map%20Completion%20by%20Deep%20Generative%0A%20%20Networks&body=Title%3A%20GLIP%3A%20Electromagnetic%20Field%20Exposure%20Map%20Completion%20by%20Deep%20Generative%0A%20%20Networks%0AAuthor%3A%20Mohammed%20Mallik%20and%20Davy%20P.%20Gaillot%20and%20Laurent%20Clavier%0AAbstract%3A%20%20%20In%20Spectrum%20cartography%20%28SC%29%2C%20the%20generation%20of%20exposure%20maps%20for%20radio%0Afrequency%20electromagnetic%20fields%20%28RF-EMF%29%20spans%20dimensions%20of%20frequency%2C%20space%2C%0Aand%20time%2C%20which%20relies%20on%20a%20sparse%20collection%20of%20sensor%20data%2C%20posing%20a%0Achallenging%20ill-posed%20inverse%20problem.%20Cartography%20methods%20based%20on%20models%0Aintegrate%20designed%20priors%2C%20such%20as%20sparsity%20and%20low-rank%20structures%2C%20to%20refine%0Athe%20solution%20of%20this%20inverse%20problem.%20In%20our%20previous%20work%2C%20EMF%20exposure%20map%0Areconstruction%20was%20achieved%20by%20Generative%20Adversarial%20Networks%20%28GANs%29%20where%0Aphysical%20laws%20or%20structural%20constraints%20were%20employed%20as%20a%20prior%2C%20but%20they%0Arequire%20a%20large%20amount%20of%20labeled%20data%20or%20simulated%20full%20maps%20for%20training%20to%0Aproduce%20efficient%20results.%20In%20this%20paper%2C%20we%20present%20a%20method%20to%20reconstruct%0AEMF%20exposure%20maps%20using%20only%20the%20generator%20network%20in%20GANs%20which%20does%20not%0Arequire%20explicit%20training%2C%20thus%20overcoming%20the%20limitations%20of%20GANs%2C%20such%20as%0Ausing%20reference%20full%20exposure%20maps.%20This%20approach%20uses%20a%20prior%20from%20sensor%20data%0Aas%20Local%20Image%20Prior%20%28LIP%29%20captured%20by%20deep%20convolutional%20generative%20networks%0Aindependent%20of%20learning%20the%20network%20parameters%20from%20images%20in%20an%20urban%0Aenvironment.%20Experimental%20results%20show%20that%2C%20even%20when%20only%20sparse%20sensor%20data%0Aare%20available%2C%20our%20method%20can%20produce%20accurate%20estimates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03384v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLIP%253A%2520Electromagnetic%2520Field%2520Exposure%2520Map%2520Completion%2520by%2520Deep%2520Generative%250A%2520%2520Networks%26entry.906535625%3DMohammed%2520Mallik%2520and%2520Davy%2520P.%2520Gaillot%2520and%2520Laurent%2520Clavier%26entry.1292438233%3D%2520%2520In%2520Spectrum%2520cartography%2520%2528SC%2529%252C%2520the%2520generation%2520of%2520exposure%2520maps%2520for%2520radio%250Afrequency%2520electromagnetic%2520fields%2520%2528RF-EMF%2529%2520spans%2520dimensions%2520of%2520frequency%252C%2520space%252C%250Aand%2520time%252C%2520which%2520relies%2520on%2520a%2520sparse%2520collection%2520of%2520sensor%2520data%252C%2520posing%2520a%250Achallenging%2520ill-posed%2520inverse%2520problem.%2520Cartography%2520methods%2520based%2520on%2520models%250Aintegrate%2520designed%2520priors%252C%2520such%2520as%2520sparsity%2520and%2520low-rank%2520structures%252C%2520to%2520refine%250Athe%2520solution%2520of%2520this%2520inverse%2520problem.%2520In%2520our%2520previous%2520work%252C%2520EMF%2520exposure%2520map%250Areconstruction%2520was%2520achieved%2520by%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520where%250Aphysical%2520laws%2520or%2520structural%2520constraints%2520were%2520employed%2520as%2520a%2520prior%252C%2520but%2520they%250Arequire%2520a%2520large%2520amount%2520of%2520labeled%2520data%2520or%2520simulated%2520full%2520maps%2520for%2520training%2520to%250Aproduce%2520efficient%2520results.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520method%2520to%2520reconstruct%250AEMF%2520exposure%2520maps%2520using%2520only%2520the%2520generator%2520network%2520in%2520GANs%2520which%2520does%2520not%250Arequire%2520explicit%2520training%252C%2520thus%2520overcoming%2520the%2520limitations%2520of%2520GANs%252C%2520such%2520as%250Ausing%2520reference%2520full%2520exposure%2520maps.%2520This%2520approach%2520uses%2520a%2520prior%2520from%2520sensor%2520data%250Aas%2520Local%2520Image%2520Prior%2520%2528LIP%2529%2520captured%2520by%2520deep%2520convolutional%2520generative%2520networks%250Aindependent%2520of%2520learning%2520the%2520network%2520parameters%2520from%2520images%2520in%2520an%2520urban%250Aenvironment.%2520Experimental%2520results%2520show%2520that%252C%2520even%2520when%2520only%2520sparse%2520sensor%2520data%250Aare%2520available%252C%2520our%2520method%2520can%2520produce%2520accurate%2520estimates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03384v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLIP%3A%20Electromagnetic%20Field%20Exposure%20Map%20Completion%20by%20Deep%20Generative%0A%20%20Networks&entry.906535625=Mohammed%20Mallik%20and%20Davy%20P.%20Gaillot%20and%20Laurent%20Clavier&entry.1292438233=%20%20In%20Spectrum%20cartography%20%28SC%29%2C%20the%20generation%20of%20exposure%20maps%20for%20radio%0Afrequency%20electromagnetic%20fields%20%28RF-EMF%29%20spans%20dimensions%20of%20frequency%2C%20space%2C%0Aand%20time%2C%20which%20relies%20on%20a%20sparse%20collection%20of%20sensor%20data%2C%20posing%20a%0Achallenging%20ill-posed%20inverse%20problem.%20Cartography%20methods%20based%20on%20models%0Aintegrate%20designed%20priors%2C%20such%20as%20sparsity%20and%20low-rank%20structures%2C%20to%20refine%0Athe%20solution%20of%20this%20inverse%20problem.%20In%20our%20previous%20work%2C%20EMF%20exposure%20map%0Areconstruction%20was%20achieved%20by%20Generative%20Adversarial%20Networks%20%28GANs%29%20where%0Aphysical%20laws%20or%20structural%20constraints%20were%20employed%20as%20a%20prior%2C%20but%20they%0Arequire%20a%20large%20amount%20of%20labeled%20data%20or%20simulated%20full%20maps%20for%20training%20to%0Aproduce%20efficient%20results.%20In%20this%20paper%2C%20we%20present%20a%20method%20to%20reconstruct%0AEMF%20exposure%20maps%20using%20only%20the%20generator%20network%20in%20GANs%20which%20does%20not%0Arequire%20explicit%20training%2C%20thus%20overcoming%20the%20limitations%20of%20GANs%2C%20such%20as%0Ausing%20reference%20full%20exposure%20maps.%20This%20approach%20uses%20a%20prior%20from%20sensor%20data%0Aas%20Local%20Image%20Prior%20%28LIP%29%20captured%20by%20deep%20convolutional%20generative%20networks%0Aindependent%20of%20learning%20the%20network%20parameters%20from%20images%20in%20an%20urban%0Aenvironment.%20Experimental%20results%20show%20that%2C%20even%20when%20only%20sparse%20sensor%20data%0Aare%20available%2C%20our%20method%20can%20produce%20accurate%20estimates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03384v1&entry.124074799=Read"},
{"title": "Liberating Seen Classes: Boosting Few-Shot and Zero-Shot Text\n  Classification via Anchor Generation and Classification Reframing", "author": "Han Liu and Siyang Zhao and Xiaotong Zhang and Feng Zhang and Wei Wang and Fenglong Ma and Hongyang Chen and Hong Yu and Xianchao Zhang", "abstract": "  Few-shot and zero-shot text classification aim to recognize samples from\nnovel classes with limited labeled samples or no labeled samples at all. While\nprevailing methods have shown promising performance via transferring knowledge\nfrom seen classes to unseen classes, they are still limited by (1) Inherent\ndissimilarities among classes make the transformation of features learned from\nseen classes to unseen classes both difficult and inefficient. (2) Rare labeled\nnovel samples usually cannot provide enough supervision signals to enable the\nmodel to adjust from the source distribution to the target distribution,\nespecially for complicated scenarios. To alleviate the above issues, we propose\na simple and effective strategy for few-shot and zero-shot text classification.\nWe aim to liberate the model from the confines of seen classes, thereby\nenabling it to predict unseen categories without the necessity of training on\nseen classes. Specifically, for mining more related unseen category knowledge,\nwe utilize a large pre-trained language model to generate pseudo novel samples,\nand select the most representative ones as category anchors. After that, we\nconvert the multi-class classification task into a binary classification task\nand use the similarities of query-anchor pairs for prediction to fully leverage\nthe limited supervision signals. Extensive experiments on six widely used\npublic datasets show that our proposed method can outperform other strong\nbaselines significantly in few-shot and zero-shot tasks, even without using any\nseen class samples.\n", "link": "http://arxiv.org/abs/2405.03565v1", "date": "2024-05-06", "relevancy": 2.5535, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5248}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5116}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Liberating%20Seen%20Classes%3A%20Boosting%20Few-Shot%20and%20Zero-Shot%20Text%0A%20%20Classification%20via%20Anchor%20Generation%20and%20Classification%20Reframing&body=Title%3A%20Liberating%20Seen%20Classes%3A%20Boosting%20Few-Shot%20and%20Zero-Shot%20Text%0A%20%20Classification%20via%20Anchor%20Generation%20and%20Classification%20Reframing%0AAuthor%3A%20Han%20Liu%20and%20Siyang%20Zhao%20and%20Xiaotong%20Zhang%20and%20Feng%20Zhang%20and%20Wei%20Wang%20and%20Fenglong%20Ma%20and%20Hongyang%20Chen%20and%20Hong%20Yu%20and%20Xianchao%20Zhang%0AAbstract%3A%20%20%20Few-shot%20and%20zero-shot%20text%20classification%20aim%20to%20recognize%20samples%20from%0Anovel%20classes%20with%20limited%20labeled%20samples%20or%20no%20labeled%20samples%20at%20all.%20While%0Aprevailing%20methods%20have%20shown%20promising%20performance%20via%20transferring%20knowledge%0Afrom%20seen%20classes%20to%20unseen%20classes%2C%20they%20are%20still%20limited%20by%20%281%29%20Inherent%0Adissimilarities%20among%20classes%20make%20the%20transformation%20of%20features%20learned%20from%0Aseen%20classes%20to%20unseen%20classes%20both%20difficult%20and%20inefficient.%20%282%29%20Rare%20labeled%0Anovel%20samples%20usually%20cannot%20provide%20enough%20supervision%20signals%20to%20enable%20the%0Amodel%20to%20adjust%20from%20the%20source%20distribution%20to%20the%20target%20distribution%2C%0Aespecially%20for%20complicated%20scenarios.%20To%20alleviate%20the%20above%20issues%2C%20we%20propose%0Aa%20simple%20and%20effective%20strategy%20for%20few-shot%20and%20zero-shot%20text%20classification.%0AWe%20aim%20to%20liberate%20the%20model%20from%20the%20confines%20of%20seen%20classes%2C%20thereby%0Aenabling%20it%20to%20predict%20unseen%20categories%20without%20the%20necessity%20of%20training%20on%0Aseen%20classes.%20Specifically%2C%20for%20mining%20more%20related%20unseen%20category%20knowledge%2C%0Awe%20utilize%20a%20large%20pre-trained%20language%20model%20to%20generate%20pseudo%20novel%20samples%2C%0Aand%20select%20the%20most%20representative%20ones%20as%20category%20anchors.%20After%20that%2C%20we%0Aconvert%20the%20multi-class%20classification%20task%20into%20a%20binary%20classification%20task%0Aand%20use%20the%20similarities%20of%20query-anchor%20pairs%20for%20prediction%20to%20fully%20leverage%0Athe%20limited%20supervision%20signals.%20Extensive%20experiments%20on%20six%20widely%20used%0Apublic%20datasets%20show%20that%20our%20proposed%20method%20can%20outperform%20other%20strong%0Abaselines%20significantly%20in%20few-shot%20and%20zero-shot%20tasks%2C%20even%20without%20using%20any%0Aseen%20class%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiberating%2520Seen%2520Classes%253A%2520Boosting%2520Few-Shot%2520and%2520Zero-Shot%2520Text%250A%2520%2520Classification%2520via%2520Anchor%2520Generation%2520and%2520Classification%2520Reframing%26entry.906535625%3DHan%2520Liu%2520and%2520Siyang%2520Zhao%2520and%2520Xiaotong%2520Zhang%2520and%2520Feng%2520Zhang%2520and%2520Wei%2520Wang%2520and%2520Fenglong%2520Ma%2520and%2520Hongyang%2520Chen%2520and%2520Hong%2520Yu%2520and%2520Xianchao%2520Zhang%26entry.1292438233%3D%2520%2520Few-shot%2520and%2520zero-shot%2520text%2520classification%2520aim%2520to%2520recognize%2520samples%2520from%250Anovel%2520classes%2520with%2520limited%2520labeled%2520samples%2520or%2520no%2520labeled%2520samples%2520at%2520all.%2520While%250Aprevailing%2520methods%2520have%2520shown%2520promising%2520performance%2520via%2520transferring%2520knowledge%250Afrom%2520seen%2520classes%2520to%2520unseen%2520classes%252C%2520they%2520are%2520still%2520limited%2520by%2520%25281%2529%2520Inherent%250Adissimilarities%2520among%2520classes%2520make%2520the%2520transformation%2520of%2520features%2520learned%2520from%250Aseen%2520classes%2520to%2520unseen%2520classes%2520both%2520difficult%2520and%2520inefficient.%2520%25282%2529%2520Rare%2520labeled%250Anovel%2520samples%2520usually%2520cannot%2520provide%2520enough%2520supervision%2520signals%2520to%2520enable%2520the%250Amodel%2520to%2520adjust%2520from%2520the%2520source%2520distribution%2520to%2520the%2520target%2520distribution%252C%250Aespecially%2520for%2520complicated%2520scenarios.%2520To%2520alleviate%2520the%2520above%2520issues%252C%2520we%2520propose%250Aa%2520simple%2520and%2520effective%2520strategy%2520for%2520few-shot%2520and%2520zero-shot%2520text%2520classification.%250AWe%2520aim%2520to%2520liberate%2520the%2520model%2520from%2520the%2520confines%2520of%2520seen%2520classes%252C%2520thereby%250Aenabling%2520it%2520to%2520predict%2520unseen%2520categories%2520without%2520the%2520necessity%2520of%2520training%2520on%250Aseen%2520classes.%2520Specifically%252C%2520for%2520mining%2520more%2520related%2520unseen%2520category%2520knowledge%252C%250Awe%2520utilize%2520a%2520large%2520pre-trained%2520language%2520model%2520to%2520generate%2520pseudo%2520novel%2520samples%252C%250Aand%2520select%2520the%2520most%2520representative%2520ones%2520as%2520category%2520anchors.%2520After%2520that%252C%2520we%250Aconvert%2520the%2520multi-class%2520classification%2520task%2520into%2520a%2520binary%2520classification%2520task%250Aand%2520use%2520the%2520similarities%2520of%2520query-anchor%2520pairs%2520for%2520prediction%2520to%2520fully%2520leverage%250Athe%2520limited%2520supervision%2520signals.%2520Extensive%2520experiments%2520on%2520six%2520widely%2520used%250Apublic%2520datasets%2520show%2520that%2520our%2520proposed%2520method%2520can%2520outperform%2520other%2520strong%250Abaselines%2520significantly%2520in%2520few-shot%2520and%2520zero-shot%2520tasks%252C%2520even%2520without%2520using%2520any%250Aseen%2520class%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Liberating%20Seen%20Classes%3A%20Boosting%20Few-Shot%20and%20Zero-Shot%20Text%0A%20%20Classification%20via%20Anchor%20Generation%20and%20Classification%20Reframing&entry.906535625=Han%20Liu%20and%20Siyang%20Zhao%20and%20Xiaotong%20Zhang%20and%20Feng%20Zhang%20and%20Wei%20Wang%20and%20Fenglong%20Ma%20and%20Hongyang%20Chen%20and%20Hong%20Yu%20and%20Xianchao%20Zhang&entry.1292438233=%20%20Few-shot%20and%20zero-shot%20text%20classification%20aim%20to%20recognize%20samples%20from%0Anovel%20classes%20with%20limited%20labeled%20samples%20or%20no%20labeled%20samples%20at%20all.%20While%0Aprevailing%20methods%20have%20shown%20promising%20performance%20via%20transferring%20knowledge%0Afrom%20seen%20classes%20to%20unseen%20classes%2C%20they%20are%20still%20limited%20by%20%281%29%20Inherent%0Adissimilarities%20among%20classes%20make%20the%20transformation%20of%20features%20learned%20from%0Aseen%20classes%20to%20unseen%20classes%20both%20difficult%20and%20inefficient.%20%282%29%20Rare%20labeled%0Anovel%20samples%20usually%20cannot%20provide%20enough%20supervision%20signals%20to%20enable%20the%0Amodel%20to%20adjust%20from%20the%20source%20distribution%20to%20the%20target%20distribution%2C%0Aespecially%20for%20complicated%20scenarios.%20To%20alleviate%20the%20above%20issues%2C%20we%20propose%0Aa%20simple%20and%20effective%20strategy%20for%20few-shot%20and%20zero-shot%20text%20classification.%0AWe%20aim%20to%20liberate%20the%20model%20from%20the%20confines%20of%20seen%20classes%2C%20thereby%0Aenabling%20it%20to%20predict%20unseen%20categories%20without%20the%20necessity%20of%20training%20on%0Aseen%20classes.%20Specifically%2C%20for%20mining%20more%20related%20unseen%20category%20knowledge%2C%0Awe%20utilize%20a%20large%20pre-trained%20language%20model%20to%20generate%20pseudo%20novel%20samples%2C%0Aand%20select%20the%20most%20representative%20ones%20as%20category%20anchors.%20After%20that%2C%20we%0Aconvert%20the%20multi-class%20classification%20task%20into%20a%20binary%20classification%20task%0Aand%20use%20the%20similarities%20of%20query-anchor%20pairs%20for%20prediction%20to%20fully%20leverage%0Athe%20limited%20supervision%20signals.%20Extensive%20experiments%20on%20six%20widely%20used%0Apublic%20datasets%20show%20that%20our%20proposed%20method%20can%20outperform%20other%20strong%0Abaselines%20significantly%20in%20few-shot%20and%20zero-shot%20tasks%2C%20even%20without%20using%20any%0Aseen%20class%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03565v1&entry.124074799=Read"},
{"title": "Denoising of Geodetic Time Series Using Spatiotemporal Graph Neural\n  Networks: Application to Slow Slip Event Extraction", "author": "Giuseppe Costantino and Sophie Giffard-Roisin and Mauro Dalla Mura and Anne Socquet", "abstract": "  Geospatial data has been transformative for the monitoring of the Earth, yet,\nas in the case of (geo)physical monitoring, the measurements can have variable\nspatial and temporal sampling and may be associated with a significant level of\nperturbations degrading the signal quality. Denoising geospatial data is,\ntherefore, essential, yet often challenging because the observations may\ncomprise noise coming from different origins, including both environmental\nsignals and instrumental artifacts, which are spatially and temporally\ncorrelated, thus hard to disentangle. This study addresses the denoising of\nmultivariate time series acquired by irregularly distributed networks of\nsensors, requiring specific methods to handle the spatiotemporal correlation of\nthe noise and the signal of interest. Specifically, our method focuses on the\ndenoising of geodetic position time series, used to monitor ground displacement\nworldwide with centimeter- to-millimeter precision. Among the signals affecting\nGNSS data, slow slip events (SSEs) are of interest to seismologists. These are\ntransients of deformation that are weakly emerging compared to other signals.\nHere, we design SSEdenoiser, a multi-station spatiotemporal graph-based\nattentive denoiser that learns latent characteristics of GNSS noise to reveal\nSSE-related displacement with sub-millimeter precision. It is based on the key\ncombination of graph recurrent networks and spatiotemporal Transformers. The\nproposed method is applied to the Cascadia subduction zone, where SSEs occur\nalong with bursts of tectonic tremors, a seismic rumbling identified from\nindependent seismic recordings. The extracted events match the spatiotemporal\nevolution of tremors. This good space-time correlation of the denoised GNSS\nsignals with the tremors validates the proposed denoising procedure.\n", "link": "http://arxiv.org/abs/2405.03320v1", "date": "2024-05-06", "relevancy": 2.5517, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5392}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5038}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Denoising%20of%20Geodetic%20Time%20Series%20Using%20Spatiotemporal%20Graph%20Neural%0A%20%20Networks%3A%20Application%20to%20Slow%20Slip%20Event%20Extraction&body=Title%3A%20Denoising%20of%20Geodetic%20Time%20Series%20Using%20Spatiotemporal%20Graph%20Neural%0A%20%20Networks%3A%20Application%20to%20Slow%20Slip%20Event%20Extraction%0AAuthor%3A%20Giuseppe%20Costantino%20and%20Sophie%20Giffard-Roisin%20and%20Mauro%20Dalla%20Mura%20and%20Anne%20Socquet%0AAbstract%3A%20%20%20Geospatial%20data%20has%20been%20transformative%20for%20the%20monitoring%20of%20the%20Earth%2C%20yet%2C%0Aas%20in%20the%20case%20of%20%28geo%29physical%20monitoring%2C%20the%20measurements%20can%20have%20variable%0Aspatial%20and%20temporal%20sampling%20and%20may%20be%20associated%20with%20a%20significant%20level%20of%0Aperturbations%20degrading%20the%20signal%20quality.%20Denoising%20geospatial%20data%20is%2C%0Atherefore%2C%20essential%2C%20yet%20often%20challenging%20because%20the%20observations%20may%0Acomprise%20noise%20coming%20from%20different%20origins%2C%20including%20both%20environmental%0Asignals%20and%20instrumental%20artifacts%2C%20which%20are%20spatially%20and%20temporally%0Acorrelated%2C%20thus%20hard%20to%20disentangle.%20This%20study%20addresses%20the%20denoising%20of%0Amultivariate%20time%20series%20acquired%20by%20irregularly%20distributed%20networks%20of%0Asensors%2C%20requiring%20specific%20methods%20to%20handle%20the%20spatiotemporal%20correlation%20of%0Athe%20noise%20and%20the%20signal%20of%20interest.%20Specifically%2C%20our%20method%20focuses%20on%20the%0Adenoising%20of%20geodetic%20position%20time%20series%2C%20used%20to%20monitor%20ground%20displacement%0Aworldwide%20with%20centimeter-%20to-millimeter%20precision.%20Among%20the%20signals%20affecting%0AGNSS%20data%2C%20slow%20slip%20events%20%28SSEs%29%20are%20of%20interest%20to%20seismologists.%20These%20are%0Atransients%20of%20deformation%20that%20are%20weakly%20emerging%20compared%20to%20other%20signals.%0AHere%2C%20we%20design%20SSEdenoiser%2C%20a%20multi-station%20spatiotemporal%20graph-based%0Aattentive%20denoiser%20that%20learns%20latent%20characteristics%20of%20GNSS%20noise%20to%20reveal%0ASSE-related%20displacement%20with%20sub-millimeter%20precision.%20It%20is%20based%20on%20the%20key%0Acombination%20of%20graph%20recurrent%20networks%20and%20spatiotemporal%20Transformers.%20The%0Aproposed%20method%20is%20applied%20to%20the%20Cascadia%20subduction%20zone%2C%20where%20SSEs%20occur%0Aalong%20with%20bursts%20of%20tectonic%20tremors%2C%20a%20seismic%20rumbling%20identified%20from%0Aindependent%20seismic%20recordings.%20The%20extracted%20events%20match%20the%20spatiotemporal%0Aevolution%20of%20tremors.%20This%20good%20space-time%20correlation%20of%20the%20denoised%20GNSS%0Asignals%20with%20the%20tremors%20validates%20the%20proposed%20denoising%20procedure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenoising%2520of%2520Geodetic%2520Time%2520Series%2520Using%2520Spatiotemporal%2520Graph%2520Neural%250A%2520%2520Networks%253A%2520Application%2520to%2520Slow%2520Slip%2520Event%2520Extraction%26entry.906535625%3DGiuseppe%2520Costantino%2520and%2520Sophie%2520Giffard-Roisin%2520and%2520Mauro%2520Dalla%2520Mura%2520and%2520Anne%2520Socquet%26entry.1292438233%3D%2520%2520Geospatial%2520data%2520has%2520been%2520transformative%2520for%2520the%2520monitoring%2520of%2520the%2520Earth%252C%2520yet%252C%250Aas%2520in%2520the%2520case%2520of%2520%2528geo%2529physical%2520monitoring%252C%2520the%2520measurements%2520can%2520have%2520variable%250Aspatial%2520and%2520temporal%2520sampling%2520and%2520may%2520be%2520associated%2520with%2520a%2520significant%2520level%2520of%250Aperturbations%2520degrading%2520the%2520signal%2520quality.%2520Denoising%2520geospatial%2520data%2520is%252C%250Atherefore%252C%2520essential%252C%2520yet%2520often%2520challenging%2520because%2520the%2520observations%2520may%250Acomprise%2520noise%2520coming%2520from%2520different%2520origins%252C%2520including%2520both%2520environmental%250Asignals%2520and%2520instrumental%2520artifacts%252C%2520which%2520are%2520spatially%2520and%2520temporally%250Acorrelated%252C%2520thus%2520hard%2520to%2520disentangle.%2520This%2520study%2520addresses%2520the%2520denoising%2520of%250Amultivariate%2520time%2520series%2520acquired%2520by%2520irregularly%2520distributed%2520networks%2520of%250Asensors%252C%2520requiring%2520specific%2520methods%2520to%2520handle%2520the%2520spatiotemporal%2520correlation%2520of%250Athe%2520noise%2520and%2520the%2520signal%2520of%2520interest.%2520Specifically%252C%2520our%2520method%2520focuses%2520on%2520the%250Adenoising%2520of%2520geodetic%2520position%2520time%2520series%252C%2520used%2520to%2520monitor%2520ground%2520displacement%250Aworldwide%2520with%2520centimeter-%2520to-millimeter%2520precision.%2520Among%2520the%2520signals%2520affecting%250AGNSS%2520data%252C%2520slow%2520slip%2520events%2520%2528SSEs%2529%2520are%2520of%2520interest%2520to%2520seismologists.%2520These%2520are%250Atransients%2520of%2520deformation%2520that%2520are%2520weakly%2520emerging%2520compared%2520to%2520other%2520signals.%250AHere%252C%2520we%2520design%2520SSEdenoiser%252C%2520a%2520multi-station%2520spatiotemporal%2520graph-based%250Aattentive%2520denoiser%2520that%2520learns%2520latent%2520characteristics%2520of%2520GNSS%2520noise%2520to%2520reveal%250ASSE-related%2520displacement%2520with%2520sub-millimeter%2520precision.%2520It%2520is%2520based%2520on%2520the%2520key%250Acombination%2520of%2520graph%2520recurrent%2520networks%2520and%2520spatiotemporal%2520Transformers.%2520The%250Aproposed%2520method%2520is%2520applied%2520to%2520the%2520Cascadia%2520subduction%2520zone%252C%2520where%2520SSEs%2520occur%250Aalong%2520with%2520bursts%2520of%2520tectonic%2520tremors%252C%2520a%2520seismic%2520rumbling%2520identified%2520from%250Aindependent%2520seismic%2520recordings.%2520The%2520extracted%2520events%2520match%2520the%2520spatiotemporal%250Aevolution%2520of%2520tremors.%2520This%2520good%2520space-time%2520correlation%2520of%2520the%2520denoised%2520GNSS%250Asignals%2520with%2520the%2520tremors%2520validates%2520the%2520proposed%2520denoising%2520procedure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Denoising%20of%20Geodetic%20Time%20Series%20Using%20Spatiotemporal%20Graph%20Neural%0A%20%20Networks%3A%20Application%20to%20Slow%20Slip%20Event%20Extraction&entry.906535625=Giuseppe%20Costantino%20and%20Sophie%20Giffard-Roisin%20and%20Mauro%20Dalla%20Mura%20and%20Anne%20Socquet&entry.1292438233=%20%20Geospatial%20data%20has%20been%20transformative%20for%20the%20monitoring%20of%20the%20Earth%2C%20yet%2C%0Aas%20in%20the%20case%20of%20%28geo%29physical%20monitoring%2C%20the%20measurements%20can%20have%20variable%0Aspatial%20and%20temporal%20sampling%20and%20may%20be%20associated%20with%20a%20significant%20level%20of%0Aperturbations%20degrading%20the%20signal%20quality.%20Denoising%20geospatial%20data%20is%2C%0Atherefore%2C%20essential%2C%20yet%20often%20challenging%20because%20the%20observations%20may%0Acomprise%20noise%20coming%20from%20different%20origins%2C%20including%20both%20environmental%0Asignals%20and%20instrumental%20artifacts%2C%20which%20are%20spatially%20and%20temporally%0Acorrelated%2C%20thus%20hard%20to%20disentangle.%20This%20study%20addresses%20the%20denoising%20of%0Amultivariate%20time%20series%20acquired%20by%20irregularly%20distributed%20networks%20of%0Asensors%2C%20requiring%20specific%20methods%20to%20handle%20the%20spatiotemporal%20correlation%20of%0Athe%20noise%20and%20the%20signal%20of%20interest.%20Specifically%2C%20our%20method%20focuses%20on%20the%0Adenoising%20of%20geodetic%20position%20time%20series%2C%20used%20to%20monitor%20ground%20displacement%0Aworldwide%20with%20centimeter-%20to-millimeter%20precision.%20Among%20the%20signals%20affecting%0AGNSS%20data%2C%20slow%20slip%20events%20%28SSEs%29%20are%20of%20interest%20to%20seismologists.%20These%20are%0Atransients%20of%20deformation%20that%20are%20weakly%20emerging%20compared%20to%20other%20signals.%0AHere%2C%20we%20design%20SSEdenoiser%2C%20a%20multi-station%20spatiotemporal%20graph-based%0Aattentive%20denoiser%20that%20learns%20latent%20characteristics%20of%20GNSS%20noise%20to%20reveal%0ASSE-related%20displacement%20with%20sub-millimeter%20precision.%20It%20is%20based%20on%20the%20key%0Acombination%20of%20graph%20recurrent%20networks%20and%20spatiotemporal%20Transformers.%20The%0Aproposed%20method%20is%20applied%20to%20the%20Cascadia%20subduction%20zone%2C%20where%20SSEs%20occur%0Aalong%20with%20bursts%20of%20tectonic%20tremors%2C%20a%20seismic%20rumbling%20identified%20from%0Aindependent%20seismic%20recordings.%20The%20extracted%20events%20match%20the%20spatiotemporal%0Aevolution%20of%20tremors.%20This%20good%20space-time%20correlation%20of%20the%20denoised%20GNSS%0Asignals%20with%20the%20tremors%20validates%20the%20proposed%20denoising%20procedure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03320v1&entry.124074799=Read"},
{"title": "Deep Clustering with Self-Supervision using Pairwise Similarities", "author": "Mohammadreza Sadeghi and Narges Armanfard", "abstract": "  Deep clustering incorporates embedding into clustering to find a\nlower-dimensional space appropriate for clustering. In this paper, we propose a\nnovel deep clustering framework with self-supervision using pairwise\nsimilarities (DCSS). The proposed method consists of two successive phases. In\nthe first phase, we propose to form hypersphere-like groups of similar data\npoints, i.e. one hypersphere per cluster, employing an autoencoder that is\ntrained using cluster-specific losses. The hyper-spheres are formed in the\nautoencoder's latent space. In the second phase, we propose to employ pairwise\nsimilarities to create a $K$-dimensional space that is capable of accommodating\nmore complex cluster distributions, hence providing more accurate clustering\nperformance. $K$ is the number of clusters. The autoencoder's latent space\nobtained in the first phase is used as the input of the second phase. The\neffectiveness of both phases is demonstrated on seven benchmark datasets by\nconducting a rigorous set of experiments.\n", "link": "http://arxiv.org/abs/2405.03590v1", "date": "2024-05-06", "relevancy": 2.5418, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5408}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5166}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Clustering%20with%20Self-Supervision%20using%20Pairwise%20Similarities&body=Title%3A%20Deep%20Clustering%20with%20Self-Supervision%20using%20Pairwise%20Similarities%0AAuthor%3A%20Mohammadreza%20Sadeghi%20and%20Narges%20Armanfard%0AAbstract%3A%20%20%20Deep%20clustering%20incorporates%20embedding%20into%20clustering%20to%20find%20a%0Alower-dimensional%20space%20appropriate%20for%20clustering.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20deep%20clustering%20framework%20with%20self-supervision%20using%20pairwise%0Asimilarities%20%28DCSS%29.%20The%20proposed%20method%20consists%20of%20two%20successive%20phases.%20In%0Athe%20first%20phase%2C%20we%20propose%20to%20form%20hypersphere-like%20groups%20of%20similar%20data%0Apoints%2C%20i.e.%20one%20hypersphere%20per%20cluster%2C%20employing%20an%20autoencoder%20that%20is%0Atrained%20using%20cluster-specific%20losses.%20The%20hyper-spheres%20are%20formed%20in%20the%0Aautoencoder%27s%20latent%20space.%20In%20the%20second%20phase%2C%20we%20propose%20to%20employ%20pairwise%0Asimilarities%20to%20create%20a%20%24K%24-dimensional%20space%20that%20is%20capable%20of%20accommodating%0Amore%20complex%20cluster%20distributions%2C%20hence%20providing%20more%20accurate%20clustering%0Aperformance.%20%24K%24%20is%20the%20number%20of%20clusters.%20The%20autoencoder%27s%20latent%20space%0Aobtained%20in%20the%20first%20phase%20is%20used%20as%20the%20input%20of%20the%20second%20phase.%20The%0Aeffectiveness%20of%20both%20phases%20is%20demonstrated%20on%20seven%20benchmark%20datasets%20by%0Aconducting%20a%20rigorous%20set%20of%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Clustering%2520with%2520Self-Supervision%2520using%2520Pairwise%2520Similarities%26entry.906535625%3DMohammadreza%2520Sadeghi%2520and%2520Narges%2520Armanfard%26entry.1292438233%3D%2520%2520Deep%2520clustering%2520incorporates%2520embedding%2520into%2520clustering%2520to%2520find%2520a%250Alower-dimensional%2520space%2520appropriate%2520for%2520clustering.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520deep%2520clustering%2520framework%2520with%2520self-supervision%2520using%2520pairwise%250Asimilarities%2520%2528DCSS%2529.%2520The%2520proposed%2520method%2520consists%2520of%2520two%2520successive%2520phases.%2520In%250Athe%2520first%2520phase%252C%2520we%2520propose%2520to%2520form%2520hypersphere-like%2520groups%2520of%2520similar%2520data%250Apoints%252C%2520i.e.%2520one%2520hypersphere%2520per%2520cluster%252C%2520employing%2520an%2520autoencoder%2520that%2520is%250Atrained%2520using%2520cluster-specific%2520losses.%2520The%2520hyper-spheres%2520are%2520formed%2520in%2520the%250Aautoencoder%2527s%2520latent%2520space.%2520In%2520the%2520second%2520phase%252C%2520we%2520propose%2520to%2520employ%2520pairwise%250Asimilarities%2520to%2520create%2520a%2520%2524K%2524-dimensional%2520space%2520that%2520is%2520capable%2520of%2520accommodating%250Amore%2520complex%2520cluster%2520distributions%252C%2520hence%2520providing%2520more%2520accurate%2520clustering%250Aperformance.%2520%2524K%2524%2520is%2520the%2520number%2520of%2520clusters.%2520The%2520autoencoder%2527s%2520latent%2520space%250Aobtained%2520in%2520the%2520first%2520phase%2520is%2520used%2520as%2520the%2520input%2520of%2520the%2520second%2520phase.%2520The%250Aeffectiveness%2520of%2520both%2520phases%2520is%2520demonstrated%2520on%2520seven%2520benchmark%2520datasets%2520by%250Aconducting%2520a%2520rigorous%2520set%2520of%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Clustering%20with%20Self-Supervision%20using%20Pairwise%20Similarities&entry.906535625=Mohammadreza%20Sadeghi%20and%20Narges%20Armanfard&entry.1292438233=%20%20Deep%20clustering%20incorporates%20embedding%20into%20clustering%20to%20find%20a%0Alower-dimensional%20space%20appropriate%20for%20clustering.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20deep%20clustering%20framework%20with%20self-supervision%20using%20pairwise%0Asimilarities%20%28DCSS%29.%20The%20proposed%20method%20consists%20of%20two%20successive%20phases.%20In%0Athe%20first%20phase%2C%20we%20propose%20to%20form%20hypersphere-like%20groups%20of%20similar%20data%0Apoints%2C%20i.e.%20one%20hypersphere%20per%20cluster%2C%20employing%20an%20autoencoder%20that%20is%0Atrained%20using%20cluster-specific%20losses.%20The%20hyper-spheres%20are%20formed%20in%20the%0Aautoencoder%27s%20latent%20space.%20In%20the%20second%20phase%2C%20we%20propose%20to%20employ%20pairwise%0Asimilarities%20to%20create%20a%20%24K%24-dimensional%20space%20that%20is%20capable%20of%20accommodating%0Amore%20complex%20cluster%20distributions%2C%20hence%20providing%20more%20accurate%20clustering%0Aperformance.%20%24K%24%20is%20the%20number%20of%20clusters.%20The%20autoencoder%27s%20latent%20space%0Aobtained%20in%20the%20first%20phase%20is%20used%20as%20the%20input%20of%20the%20second%20phase.%20The%0Aeffectiveness%20of%20both%20phases%20is%20demonstrated%20on%20seven%20benchmark%20datasets%20by%0Aconducting%20a%20rigorous%20set%20of%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03590v1&entry.124074799=Read"},
{"title": "An Empty Room is All We Want: Automatic Defurnishing of Indoor Panoramas", "author": "Mira Slavcheva and Dave Gausebeck and Kevin Chen and David Buchhofer and Azwad Sabik and Chen Ma and Sachal Dhillon and Olaf Brandt and Alan Dolhasz", "abstract": "  We propose a pipeline that leverages Stable Diffusion to improve inpainting\nresults in the context of defurnishing -- the removal of furniture items from\nindoor panorama images. Specifically, we illustrate how increased context,\ndomain-specific model fine-tuning, and improved image blending can produce\nhigh-fidelity inpaints that are geometrically plausible without needing to rely\non room layout estimation. We demonstrate qualitative and quantitative\nimprovements over other furniture removal techniques.\n", "link": "http://arxiv.org/abs/2405.03682v1", "date": "2024-05-06", "relevancy": 2.517, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5125}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empty%20Room%20is%20All%20We%20Want%3A%20Automatic%20Defurnishing%20of%20Indoor%20Panoramas&body=Title%3A%20An%20Empty%20Room%20is%20All%20We%20Want%3A%20Automatic%20Defurnishing%20of%20Indoor%20Panoramas%0AAuthor%3A%20Mira%20Slavcheva%20and%20Dave%20Gausebeck%20and%20Kevin%20Chen%20and%20David%20Buchhofer%20and%20Azwad%20Sabik%20and%20Chen%20Ma%20and%20Sachal%20Dhillon%20and%20Olaf%20Brandt%20and%20Alan%20Dolhasz%0AAbstract%3A%20%20%20We%20propose%20a%20pipeline%20that%20leverages%20Stable%20Diffusion%20to%20improve%20inpainting%0Aresults%20in%20the%20context%20of%20defurnishing%20--%20the%20removal%20of%20furniture%20items%20from%0Aindoor%20panorama%20images.%20Specifically%2C%20we%20illustrate%20how%20increased%20context%2C%0Adomain-specific%20model%20fine-tuning%2C%20and%20improved%20image%20blending%20can%20produce%0Ahigh-fidelity%20inpaints%20that%20are%20geometrically%20plausible%20without%20needing%20to%20rely%0Aon%20room%20layout%20estimation.%20We%20demonstrate%20qualitative%20and%20quantitative%0Aimprovements%20over%20other%20furniture%20removal%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empty%2520Room%2520is%2520All%2520We%2520Want%253A%2520Automatic%2520Defurnishing%2520of%2520Indoor%2520Panoramas%26entry.906535625%3DMira%2520Slavcheva%2520and%2520Dave%2520Gausebeck%2520and%2520Kevin%2520Chen%2520and%2520David%2520Buchhofer%2520and%2520Azwad%2520Sabik%2520and%2520Chen%2520Ma%2520and%2520Sachal%2520Dhillon%2520and%2520Olaf%2520Brandt%2520and%2520Alan%2520Dolhasz%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520pipeline%2520that%2520leverages%2520Stable%2520Diffusion%2520to%2520improve%2520inpainting%250Aresults%2520in%2520the%2520context%2520of%2520defurnishing%2520--%2520the%2520removal%2520of%2520furniture%2520items%2520from%250Aindoor%2520panorama%2520images.%2520Specifically%252C%2520we%2520illustrate%2520how%2520increased%2520context%252C%250Adomain-specific%2520model%2520fine-tuning%252C%2520and%2520improved%2520image%2520blending%2520can%2520produce%250Ahigh-fidelity%2520inpaints%2520that%2520are%2520geometrically%2520plausible%2520without%2520needing%2520to%2520rely%250Aon%2520room%2520layout%2520estimation.%2520We%2520demonstrate%2520qualitative%2520and%2520quantitative%250Aimprovements%2520over%2520other%2520furniture%2520removal%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empty%20Room%20is%20All%20We%20Want%3A%20Automatic%20Defurnishing%20of%20Indoor%20Panoramas&entry.906535625=Mira%20Slavcheva%20and%20Dave%20Gausebeck%20and%20Kevin%20Chen%20and%20David%20Buchhofer%20and%20Azwad%20Sabik%20and%20Chen%20Ma%20and%20Sachal%20Dhillon%20and%20Olaf%20Brandt%20and%20Alan%20Dolhasz&entry.1292438233=%20%20We%20propose%20a%20pipeline%20that%20leverages%20Stable%20Diffusion%20to%20improve%20inpainting%0Aresults%20in%20the%20context%20of%20defurnishing%20--%20the%20removal%20of%20furniture%20items%20from%0Aindoor%20panorama%20images.%20Specifically%2C%20we%20illustrate%20how%20increased%20context%2C%0Adomain-specific%20model%20fine-tuning%2C%20and%20improved%20image%20blending%20can%20produce%0Ahigh-fidelity%20inpaints%20that%20are%20geometrically%20plausible%20without%20needing%20to%20rely%0Aon%20room%20layout%20estimation.%20We%20demonstrate%20qualitative%20and%20quantitative%0Aimprovements%20over%20other%20furniture%20removal%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03682v1&entry.124074799=Read"},
{"title": "DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D\n  Classification", "author": "Sitian Shen and Zilin Zhu and Linqian Fan and Harry Zhang and Xinxiao Wu", "abstract": "  Large pre-trained models have had a significant impact on computer vision by\nenabling multi-modal learning, where the CLIP model has achieved impressive\nresults in image classification, object detection, and semantic segmentation.\nHowever, the model's performance on 3D point cloud processing tasks is limited\ndue to the domain gap between depth maps from 3D projection and training images\nof CLIP. This paper proposes DiffCLIP, a new pre-training framework that\nincorporates stable diffusion with ControlNet to minimize the domain gap in the\nvisual branch. Additionally, a style-prompt generation module is introduced for\nfew-shot tasks in the textual branch. Extensive experiments on the ModelNet10,\nModelNet40, and ScanObjectNN datasets show that DiffCLIP has strong abilities\nfor 3D understanding. By using stable diffusion and style-prompt generation,\nDiffCLIP achieves an accuracy of 43.2\\% for zero-shot classification on OBJ\\_BG\nof ScanObjectNN, which is state-of-the-art performance, and an accuracy of\n80.6\\% for zero-shot classification on ModelNet10, which is comparable to\nstate-of-the-art performance.\n", "link": "http://arxiv.org/abs/2305.15957v3", "date": "2024-05-06", "relevancy": 2.4869, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6501}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6114}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffCLIP%3A%20Leveraging%20Stable%20Diffusion%20for%20Language%20Grounded%203D%0A%20%20Classification&body=Title%3A%20DiffCLIP%3A%20Leveraging%20Stable%20Diffusion%20for%20Language%20Grounded%203D%0A%20%20Classification%0AAuthor%3A%20Sitian%20Shen%20and%20Zilin%20Zhu%20and%20Linqian%20Fan%20and%20Harry%20Zhang%20and%20Xinxiao%20Wu%0AAbstract%3A%20%20%20Large%20pre-trained%20models%20have%20had%20a%20significant%20impact%20on%20computer%20vision%20by%0Aenabling%20multi-modal%20learning%2C%20where%20the%20CLIP%20model%20has%20achieved%20impressive%0Aresults%20in%20image%20classification%2C%20object%20detection%2C%20and%20semantic%20segmentation.%0AHowever%2C%20the%20model%27s%20performance%20on%203D%20point%20cloud%20processing%20tasks%20is%20limited%0Adue%20to%20the%20domain%20gap%20between%20depth%20maps%20from%203D%20projection%20and%20training%20images%0Aof%20CLIP.%20This%20paper%20proposes%20DiffCLIP%2C%20a%20new%20pre-training%20framework%20that%0Aincorporates%20stable%20diffusion%20with%20ControlNet%20to%20minimize%20the%20domain%20gap%20in%20the%0Avisual%20branch.%20Additionally%2C%20a%20style-prompt%20generation%20module%20is%20introduced%20for%0Afew-shot%20tasks%20in%20the%20textual%20branch.%20Extensive%20experiments%20on%20the%20ModelNet10%2C%0AModelNet40%2C%20and%20ScanObjectNN%20datasets%20show%20that%20DiffCLIP%20has%20strong%20abilities%0Afor%203D%20understanding.%20By%20using%20stable%20diffusion%20and%20style-prompt%20generation%2C%0ADiffCLIP%20achieves%20an%20accuracy%20of%2043.2%5C%25%20for%20zero-shot%20classification%20on%20OBJ%5C_BG%0Aof%20ScanObjectNN%2C%20which%20is%20state-of-the-art%20performance%2C%20and%20an%20accuracy%20of%0A80.6%5C%25%20for%20zero-shot%20classification%20on%20ModelNet10%2C%20which%20is%20comparable%20to%0Astate-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15957v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffCLIP%253A%2520Leveraging%2520Stable%2520Diffusion%2520for%2520Language%2520Grounded%25203D%250A%2520%2520Classification%26entry.906535625%3DSitian%2520Shen%2520and%2520Zilin%2520Zhu%2520and%2520Linqian%2520Fan%2520and%2520Harry%2520Zhang%2520and%2520Xinxiao%2520Wu%26entry.1292438233%3D%2520%2520Large%2520pre-trained%2520models%2520have%2520had%2520a%2520significant%2520impact%2520on%2520computer%2520vision%2520by%250Aenabling%2520multi-modal%2520learning%252C%2520where%2520the%2520CLIP%2520model%2520has%2520achieved%2520impressive%250Aresults%2520in%2520image%2520classification%252C%2520object%2520detection%252C%2520and%2520semantic%2520segmentation.%250AHowever%252C%2520the%2520model%2527s%2520performance%2520on%25203D%2520point%2520cloud%2520processing%2520tasks%2520is%2520limited%250Adue%2520to%2520the%2520domain%2520gap%2520between%2520depth%2520maps%2520from%25203D%2520projection%2520and%2520training%2520images%250Aof%2520CLIP.%2520This%2520paper%2520proposes%2520DiffCLIP%252C%2520a%2520new%2520pre-training%2520framework%2520that%250Aincorporates%2520stable%2520diffusion%2520with%2520ControlNet%2520to%2520minimize%2520the%2520domain%2520gap%2520in%2520the%250Avisual%2520branch.%2520Additionally%252C%2520a%2520style-prompt%2520generation%2520module%2520is%2520introduced%2520for%250Afew-shot%2520tasks%2520in%2520the%2520textual%2520branch.%2520Extensive%2520experiments%2520on%2520the%2520ModelNet10%252C%250AModelNet40%252C%2520and%2520ScanObjectNN%2520datasets%2520show%2520that%2520DiffCLIP%2520has%2520strong%2520abilities%250Afor%25203D%2520understanding.%2520By%2520using%2520stable%2520diffusion%2520and%2520style-prompt%2520generation%252C%250ADiffCLIP%2520achieves%2520an%2520accuracy%2520of%252043.2%255C%2525%2520for%2520zero-shot%2520classification%2520on%2520OBJ%255C_BG%250Aof%2520ScanObjectNN%252C%2520which%2520is%2520state-of-the-art%2520performance%252C%2520and%2520an%2520accuracy%2520of%250A80.6%255C%2525%2520for%2520zero-shot%2520classification%2520on%2520ModelNet10%252C%2520which%2520is%2520comparable%2520to%250Astate-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.15957v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffCLIP%3A%20Leveraging%20Stable%20Diffusion%20for%20Language%20Grounded%203D%0A%20%20Classification&entry.906535625=Sitian%20Shen%20and%20Zilin%20Zhu%20and%20Linqian%20Fan%20and%20Harry%20Zhang%20and%20Xinxiao%20Wu&entry.1292438233=%20%20Large%20pre-trained%20models%20have%20had%20a%20significant%20impact%20on%20computer%20vision%20by%0Aenabling%20multi-modal%20learning%2C%20where%20the%20CLIP%20model%20has%20achieved%20impressive%0Aresults%20in%20image%20classification%2C%20object%20detection%2C%20and%20semantic%20segmentation.%0AHowever%2C%20the%20model%27s%20performance%20on%203D%20point%20cloud%20processing%20tasks%20is%20limited%0Adue%20to%20the%20domain%20gap%20between%20depth%20maps%20from%203D%20projection%20and%20training%20images%0Aof%20CLIP.%20This%20paper%20proposes%20DiffCLIP%2C%20a%20new%20pre-training%20framework%20that%0Aincorporates%20stable%20diffusion%20with%20ControlNet%20to%20minimize%20the%20domain%20gap%20in%20the%0Avisual%20branch.%20Additionally%2C%20a%20style-prompt%20generation%20module%20is%20introduced%20for%0Afew-shot%20tasks%20in%20the%20textual%20branch.%20Extensive%20experiments%20on%20the%20ModelNet10%2C%0AModelNet40%2C%20and%20ScanObjectNN%20datasets%20show%20that%20DiffCLIP%20has%20strong%20abilities%0Afor%203D%20understanding.%20By%20using%20stable%20diffusion%20and%20style-prompt%20generation%2C%0ADiffCLIP%20achieves%20an%20accuracy%20of%2043.2%5C%25%20for%20zero-shot%20classification%20on%20OBJ%5C_BG%0Aof%20ScanObjectNN%2C%20which%20is%20state-of-the-art%20performance%2C%20and%20an%20accuracy%20of%0A80.6%5C%25%20for%20zero-shot%20classification%20on%20ModelNet10%2C%20which%20is%20comparable%20to%0Astate-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15957v3&entry.124074799=Read"},
{"title": "SCULPT: Shape-Conditioned Unpaired Learning of Pose-dependent Clothed\n  and Textured Human Meshes", "author": "Soubhik Sanyal and Partha Ghosh and Jinlong Yang and Michael J. Black and Justus Thies and Timo Bolkart", "abstract": "  We present SCULPT, a novel 3D generative model for clothed and textured 3D\nmeshes of humans. Specifically, we devise a deep neural network that learns to\nrepresent the geometry and appearance distribution of clothed human bodies.\nTraining such a model is challenging, as datasets of textured 3D meshes for\nhumans are limited in size and accessibility. Our key observation is that there\nexist medium-sized 3D scan datasets like CAPE, as well as large-scale 2D image\ndatasets of clothed humans and multiple appearances can be mapped to a single\ngeometry. To effectively learn from the two data modalities, we propose an\nunpaired learning procedure for pose-dependent clothed and textured human\nmeshes. Specifically, we learn a pose-dependent geometry space from 3D scan\ndata. We represent this as per vertex displacements w.r.t. the SMPL model.\nNext, we train a geometry conditioned texture generator in an unsupervised way\nusing the 2D image data. We use intermediate activations of the learned\ngeometry model to condition our texture generator. To alleviate entanglement\nbetween pose and clothing type, and pose and clothing appearance, we condition\nboth the texture and geometry generators with attribute labels such as clothing\ntypes for the geometry, and clothing colors for the texture generator. We\nautomatically generated these conditioning labels for the 2D images based on\nthe visual question answering model BLIP and CLIP. We validate our method on\nthe SCULPT dataset, and compare to state-of-the-art 3D generative models for\nclothed human bodies. Our code and data can be found at\nhttps://sculpt.is.tue.mpg.de.\n", "link": "http://arxiv.org/abs/2308.10638v2", "date": "2024-05-06", "relevancy": 2.4515, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6261}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6057}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCULPT%3A%20Shape-Conditioned%20Unpaired%20Learning%20of%20Pose-dependent%20Clothed%0A%20%20and%20Textured%20Human%20Meshes&body=Title%3A%20SCULPT%3A%20Shape-Conditioned%20Unpaired%20Learning%20of%20Pose-dependent%20Clothed%0A%20%20and%20Textured%20Human%20Meshes%0AAuthor%3A%20Soubhik%20Sanyal%20and%20Partha%20Ghosh%20and%20Jinlong%20Yang%20and%20Michael%20J.%20Black%20and%20Justus%20Thies%20and%20Timo%20Bolkart%0AAbstract%3A%20%20%20We%20present%20SCULPT%2C%20a%20novel%203D%20generative%20model%20for%20clothed%20and%20textured%203D%0Ameshes%20of%20humans.%20Specifically%2C%20we%20devise%20a%20deep%20neural%20network%20that%20learns%20to%0Arepresent%20the%20geometry%20and%20appearance%20distribution%20of%20clothed%20human%20bodies.%0ATraining%20such%20a%20model%20is%20challenging%2C%20as%20datasets%20of%20textured%203D%20meshes%20for%0Ahumans%20are%20limited%20in%20size%20and%20accessibility.%20Our%20key%20observation%20is%20that%20there%0Aexist%20medium-sized%203D%20scan%20datasets%20like%20CAPE%2C%20as%20well%20as%20large-scale%202D%20image%0Adatasets%20of%20clothed%20humans%20and%20multiple%20appearances%20can%20be%20mapped%20to%20a%20single%0Ageometry.%20To%20effectively%20learn%20from%20the%20two%20data%20modalities%2C%20we%20propose%20an%0Aunpaired%20learning%20procedure%20for%20pose-dependent%20clothed%20and%20textured%20human%0Ameshes.%20Specifically%2C%20we%20learn%20a%20pose-dependent%20geometry%20space%20from%203D%20scan%0Adata.%20We%20represent%20this%20as%20per%20vertex%20displacements%20w.r.t.%20the%20SMPL%20model.%0ANext%2C%20we%20train%20a%20geometry%20conditioned%20texture%20generator%20in%20an%20unsupervised%20way%0Ausing%20the%202D%20image%20data.%20We%20use%20intermediate%20activations%20of%20the%20learned%0Ageometry%20model%20to%20condition%20our%20texture%20generator.%20To%20alleviate%20entanglement%0Abetween%20pose%20and%20clothing%20type%2C%20and%20pose%20and%20clothing%20appearance%2C%20we%20condition%0Aboth%20the%20texture%20and%20geometry%20generators%20with%20attribute%20labels%20such%20as%20clothing%0Atypes%20for%20the%20geometry%2C%20and%20clothing%20colors%20for%20the%20texture%20generator.%20We%0Aautomatically%20generated%20these%20conditioning%20labels%20for%20the%202D%20images%20based%20on%0Athe%20visual%20question%20answering%20model%20BLIP%20and%20CLIP.%20We%20validate%20our%20method%20on%0Athe%20SCULPT%20dataset%2C%20and%20compare%20to%20state-of-the-art%203D%20generative%20models%20for%0Aclothed%20human%20bodies.%20Our%20code%20and%20data%20can%20be%20found%20at%0Ahttps%3A//sculpt.is.tue.mpg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.10638v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCULPT%253A%2520Shape-Conditioned%2520Unpaired%2520Learning%2520of%2520Pose-dependent%2520Clothed%250A%2520%2520and%2520Textured%2520Human%2520Meshes%26entry.906535625%3DSoubhik%2520Sanyal%2520and%2520Partha%2520Ghosh%2520and%2520Jinlong%2520Yang%2520and%2520Michael%2520J.%2520Black%2520and%2520Justus%2520Thies%2520and%2520Timo%2520Bolkart%26entry.1292438233%3D%2520%2520We%2520present%2520SCULPT%252C%2520a%2520novel%25203D%2520generative%2520model%2520for%2520clothed%2520and%2520textured%25203D%250Ameshes%2520of%2520humans.%2520Specifically%252C%2520we%2520devise%2520a%2520deep%2520neural%2520network%2520that%2520learns%2520to%250Arepresent%2520the%2520geometry%2520and%2520appearance%2520distribution%2520of%2520clothed%2520human%2520bodies.%250ATraining%2520such%2520a%2520model%2520is%2520challenging%252C%2520as%2520datasets%2520of%2520textured%25203D%2520meshes%2520for%250Ahumans%2520are%2520limited%2520in%2520size%2520and%2520accessibility.%2520Our%2520key%2520observation%2520is%2520that%2520there%250Aexist%2520medium-sized%25203D%2520scan%2520datasets%2520like%2520CAPE%252C%2520as%2520well%2520as%2520large-scale%25202D%2520image%250Adatasets%2520of%2520clothed%2520humans%2520and%2520multiple%2520appearances%2520can%2520be%2520mapped%2520to%2520a%2520single%250Ageometry.%2520To%2520effectively%2520learn%2520from%2520the%2520two%2520data%2520modalities%252C%2520we%2520propose%2520an%250Aunpaired%2520learning%2520procedure%2520for%2520pose-dependent%2520clothed%2520and%2520textured%2520human%250Ameshes.%2520Specifically%252C%2520we%2520learn%2520a%2520pose-dependent%2520geometry%2520space%2520from%25203D%2520scan%250Adata.%2520We%2520represent%2520this%2520as%2520per%2520vertex%2520displacements%2520w.r.t.%2520the%2520SMPL%2520model.%250ANext%252C%2520we%2520train%2520a%2520geometry%2520conditioned%2520texture%2520generator%2520in%2520an%2520unsupervised%2520way%250Ausing%2520the%25202D%2520image%2520data.%2520We%2520use%2520intermediate%2520activations%2520of%2520the%2520learned%250Ageometry%2520model%2520to%2520condition%2520our%2520texture%2520generator.%2520To%2520alleviate%2520entanglement%250Abetween%2520pose%2520and%2520clothing%2520type%252C%2520and%2520pose%2520and%2520clothing%2520appearance%252C%2520we%2520condition%250Aboth%2520the%2520texture%2520and%2520geometry%2520generators%2520with%2520attribute%2520labels%2520such%2520as%2520clothing%250Atypes%2520for%2520the%2520geometry%252C%2520and%2520clothing%2520colors%2520for%2520the%2520texture%2520generator.%2520We%250Aautomatically%2520generated%2520these%2520conditioning%2520labels%2520for%2520the%25202D%2520images%2520based%2520on%250Athe%2520visual%2520question%2520answering%2520model%2520BLIP%2520and%2520CLIP.%2520We%2520validate%2520our%2520method%2520on%250Athe%2520SCULPT%2520dataset%252C%2520and%2520compare%2520to%2520state-of-the-art%25203D%2520generative%2520models%2520for%250Aclothed%2520human%2520bodies.%2520Our%2520code%2520and%2520data%2520can%2520be%2520found%2520at%250Ahttps%253A//sculpt.is.tue.mpg.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.10638v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCULPT%3A%20Shape-Conditioned%20Unpaired%20Learning%20of%20Pose-dependent%20Clothed%0A%20%20and%20Textured%20Human%20Meshes&entry.906535625=Soubhik%20Sanyal%20and%20Partha%20Ghosh%20and%20Jinlong%20Yang%20and%20Michael%20J.%20Black%20and%20Justus%20Thies%20and%20Timo%20Bolkart&entry.1292438233=%20%20We%20present%20SCULPT%2C%20a%20novel%203D%20generative%20model%20for%20clothed%20and%20textured%203D%0Ameshes%20of%20humans.%20Specifically%2C%20we%20devise%20a%20deep%20neural%20network%20that%20learns%20to%0Arepresent%20the%20geometry%20and%20appearance%20distribution%20of%20clothed%20human%20bodies.%0ATraining%20such%20a%20model%20is%20challenging%2C%20as%20datasets%20of%20textured%203D%20meshes%20for%0Ahumans%20are%20limited%20in%20size%20and%20accessibility.%20Our%20key%20observation%20is%20that%20there%0Aexist%20medium-sized%203D%20scan%20datasets%20like%20CAPE%2C%20as%20well%20as%20large-scale%202D%20image%0Adatasets%20of%20clothed%20humans%20and%20multiple%20appearances%20can%20be%20mapped%20to%20a%20single%0Ageometry.%20To%20effectively%20learn%20from%20the%20two%20data%20modalities%2C%20we%20propose%20an%0Aunpaired%20learning%20procedure%20for%20pose-dependent%20clothed%20and%20textured%20human%0Ameshes.%20Specifically%2C%20we%20learn%20a%20pose-dependent%20geometry%20space%20from%203D%20scan%0Adata.%20We%20represent%20this%20as%20per%20vertex%20displacements%20w.r.t.%20the%20SMPL%20model.%0ANext%2C%20we%20train%20a%20geometry%20conditioned%20texture%20generator%20in%20an%20unsupervised%20way%0Ausing%20the%202D%20image%20data.%20We%20use%20intermediate%20activations%20of%20the%20learned%0Ageometry%20model%20to%20condition%20our%20texture%20generator.%20To%20alleviate%20entanglement%0Abetween%20pose%20and%20clothing%20type%2C%20and%20pose%20and%20clothing%20appearance%2C%20we%20condition%0Aboth%20the%20texture%20and%20geometry%20generators%20with%20attribute%20labels%20such%20as%20clothing%0Atypes%20for%20the%20geometry%2C%20and%20clothing%20colors%20for%20the%20texture%20generator.%20We%0Aautomatically%20generated%20these%20conditioning%20labels%20for%20the%202D%20images%20based%20on%0Athe%20visual%20question%20answering%20model%20BLIP%20and%20CLIP.%20We%20validate%20our%20method%20on%0Athe%20SCULPT%20dataset%2C%20and%20compare%20to%20state-of-the-art%203D%20generative%20models%20for%0Aclothed%20human%20bodies.%20Our%20code%20and%20data%20can%20be%20found%20at%0Ahttps%3A//sculpt.is.tue.mpg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.10638v2&entry.124074799=Read"},
{"title": "Language-Image Models with 3D Understanding", "author": "Jang Hyun Cho and Boris Ivanovic and Yulong Cao and Edward Schmerling and Yue Wang and Xinshuo Weng and Boyi Li and Yurong You and Philipp Kr\u00e4henb\u00fchl and Yan Wang and Marco Pavone", "abstract": "  Multi-modal large language models (MLLMs) have shown incredible capabilities\nin a variety of 2D vision and language tasks. We extend MLLMs' perceptual\ncapabilities to ground and reason about images in 3-dimensional space. To that\nend, we first develop a large-scale pre-training dataset for 2D and 3D called\nLV3D by combining multiple existing 2D and 3D recognition datasets under a\ncommon task formulation: as multi-turn question-answering. Next, we introduce a\nnew MLLM named Cube-LLM and pre-train it on LV3D. We show that pure data\nscaling makes a strong 3D perception capability without 3D specific\narchitectural design or training objective. Cube-LLM exhibits intriguing\nproperties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting\nto improve 3D understanding from 2D context information. (2) Cube-LLM can\nfollow complex and diverse instructions and adapt to versatile input and output\nformats. (3) Cube-LLM can be visually prompted such as 2D box or a set of\ncandidate 3D boxes from specialists. Our experiments on outdoor benchmarks\ndemonstrate that Cube-LLM significantly outperforms existing baselines by 21.3\npoints of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7\npoints on the DriveLM dataset for complex reasoning about driving scenarios,\nrespectively. Cube-LLM also shows competitive results in general MLLM\nbenchmarks such as refCOCO for 2D grounding with (87.0) average score, as well\nas visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for\ncomplex reasoning. Our project is available at\nhttps://janghyuncho.github.io/Cube-LLM.\n", "link": "http://arxiv.org/abs/2405.03685v1", "date": "2024-05-06", "relevancy": 2.4282, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6555}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6064}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Image%20Models%20with%203D%20Understanding&body=Title%3A%20Language-Image%20Models%20with%203D%20Understanding%0AAuthor%3A%20Jang%20Hyun%20Cho%20and%20Boris%20Ivanovic%20and%20Yulong%20Cao%20and%20Edward%20Schmerling%20and%20Yue%20Wang%20and%20Xinshuo%20Weng%20and%20Boyi%20Li%20and%20Yurong%20You%20and%20Philipp%20Kr%C3%A4henb%C3%BChl%20and%20Yan%20Wang%20and%20Marco%20Pavone%0AAbstract%3A%20%20%20Multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20shown%20incredible%20capabilities%0Ain%20a%20variety%20of%202D%20vision%20and%20language%20tasks.%20We%20extend%20MLLMs%27%20perceptual%0Acapabilities%20to%20ground%20and%20reason%20about%20images%20in%203-dimensional%20space.%20To%20that%0Aend%2C%20we%20first%20develop%20a%20large-scale%20pre-training%20dataset%20for%202D%20and%203D%20called%0ALV3D%20by%20combining%20multiple%20existing%202D%20and%203D%20recognition%20datasets%20under%20a%0Acommon%20task%20formulation%3A%20as%20multi-turn%20question-answering.%20Next%2C%20we%20introduce%20a%0Anew%20MLLM%20named%20Cube-LLM%20and%20pre-train%20it%20on%20LV3D.%20We%20show%20that%20pure%20data%0Ascaling%20makes%20a%20strong%203D%20perception%20capability%20without%203D%20specific%0Aarchitectural%20design%20or%20training%20objective.%20Cube-LLM%20exhibits%20intriguing%0Aproperties%20similar%20to%20LLMs%3A%20%281%29%20Cube-LLM%20can%20apply%20chain-of-thought%20prompting%0Ato%20improve%203D%20understanding%20from%202D%20context%20information.%20%282%29%20Cube-LLM%20can%0Afollow%20complex%20and%20diverse%20instructions%20and%20adapt%20to%20versatile%20input%20and%20output%0Aformats.%20%283%29%20Cube-LLM%20can%20be%20visually%20prompted%20such%20as%202D%20box%20or%20a%20set%20of%0Acandidate%203D%20boxes%20from%20specialists.%20Our%20experiments%20on%20outdoor%20benchmarks%0Ademonstrate%20that%20Cube-LLM%20significantly%20outperforms%20existing%20baselines%20by%2021.3%0Apoints%20of%20AP-BEV%20on%20the%20Talk2Car%20dataset%20for%203D%20grounded%20reasoning%20and%2017.7%0Apoints%20on%20the%20DriveLM%20dataset%20for%20complex%20reasoning%20about%20driving%20scenarios%2C%0Arespectively.%20Cube-LLM%20also%20shows%20competitive%20results%20in%20general%20MLLM%0Abenchmarks%20such%20as%20refCOCO%20for%202D%20grounding%20with%20%2887.0%29%20average%20score%2C%20as%20well%0Aas%20visual%20question%20answering%20benchmarks%20such%20as%20VQAv2%2C%20GQA%2C%20SQA%2C%20POPE%2C%20etc.%20for%0Acomplex%20reasoning.%20Our%20project%20is%20available%20at%0Ahttps%3A//janghyuncho.github.io/Cube-LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Image%2520Models%2520with%25203D%2520Understanding%26entry.906535625%3DJang%2520Hyun%2520Cho%2520and%2520Boris%2520Ivanovic%2520and%2520Yulong%2520Cao%2520and%2520Edward%2520Schmerling%2520and%2520Yue%2520Wang%2520and%2520Xinshuo%2520Weng%2520and%2520Boyi%2520Li%2520and%2520Yurong%2520You%2520and%2520Philipp%2520Kr%25C3%25A4henb%25C3%25BChl%2520and%2520Yan%2520Wang%2520and%2520Marco%2520Pavone%26entry.1292438233%3D%2520%2520Multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520shown%2520incredible%2520capabilities%250Ain%2520a%2520variety%2520of%25202D%2520vision%2520and%2520language%2520tasks.%2520We%2520extend%2520MLLMs%2527%2520perceptual%250Acapabilities%2520to%2520ground%2520and%2520reason%2520about%2520images%2520in%25203-dimensional%2520space.%2520To%2520that%250Aend%252C%2520we%2520first%2520develop%2520a%2520large-scale%2520pre-training%2520dataset%2520for%25202D%2520and%25203D%2520called%250ALV3D%2520by%2520combining%2520multiple%2520existing%25202D%2520and%25203D%2520recognition%2520datasets%2520under%2520a%250Acommon%2520task%2520formulation%253A%2520as%2520multi-turn%2520question-answering.%2520Next%252C%2520we%2520introduce%2520a%250Anew%2520MLLM%2520named%2520Cube-LLM%2520and%2520pre-train%2520it%2520on%2520LV3D.%2520We%2520show%2520that%2520pure%2520data%250Ascaling%2520makes%2520a%2520strong%25203D%2520perception%2520capability%2520without%25203D%2520specific%250Aarchitectural%2520design%2520or%2520training%2520objective.%2520Cube-LLM%2520exhibits%2520intriguing%250Aproperties%2520similar%2520to%2520LLMs%253A%2520%25281%2529%2520Cube-LLM%2520can%2520apply%2520chain-of-thought%2520prompting%250Ato%2520improve%25203D%2520understanding%2520from%25202D%2520context%2520information.%2520%25282%2529%2520Cube-LLM%2520can%250Afollow%2520complex%2520and%2520diverse%2520instructions%2520and%2520adapt%2520to%2520versatile%2520input%2520and%2520output%250Aformats.%2520%25283%2529%2520Cube-LLM%2520can%2520be%2520visually%2520prompted%2520such%2520as%25202D%2520box%2520or%2520a%2520set%2520of%250Acandidate%25203D%2520boxes%2520from%2520specialists.%2520Our%2520experiments%2520on%2520outdoor%2520benchmarks%250Ademonstrate%2520that%2520Cube-LLM%2520significantly%2520outperforms%2520existing%2520baselines%2520by%252021.3%250Apoints%2520of%2520AP-BEV%2520on%2520the%2520Talk2Car%2520dataset%2520for%25203D%2520grounded%2520reasoning%2520and%252017.7%250Apoints%2520on%2520the%2520DriveLM%2520dataset%2520for%2520complex%2520reasoning%2520about%2520driving%2520scenarios%252C%250Arespectively.%2520Cube-LLM%2520also%2520shows%2520competitive%2520results%2520in%2520general%2520MLLM%250Abenchmarks%2520such%2520as%2520refCOCO%2520for%25202D%2520grounding%2520with%2520%252887.0%2529%2520average%2520score%252C%2520as%2520well%250Aas%2520visual%2520question%2520answering%2520benchmarks%2520such%2520as%2520VQAv2%252C%2520GQA%252C%2520SQA%252C%2520POPE%252C%2520etc.%2520for%250Acomplex%2520reasoning.%2520Our%2520project%2520is%2520available%2520at%250Ahttps%253A//janghyuncho.github.io/Cube-LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Image%20Models%20with%203D%20Understanding&entry.906535625=Jang%20Hyun%20Cho%20and%20Boris%20Ivanovic%20and%20Yulong%20Cao%20and%20Edward%20Schmerling%20and%20Yue%20Wang%20and%20Xinshuo%20Weng%20and%20Boyi%20Li%20and%20Yurong%20You%20and%20Philipp%20Kr%C3%A4henb%C3%BChl%20and%20Yan%20Wang%20and%20Marco%20Pavone&entry.1292438233=%20%20Multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20shown%20incredible%20capabilities%0Ain%20a%20variety%20of%202D%20vision%20and%20language%20tasks.%20We%20extend%20MLLMs%27%20perceptual%0Acapabilities%20to%20ground%20and%20reason%20about%20images%20in%203-dimensional%20space.%20To%20that%0Aend%2C%20we%20first%20develop%20a%20large-scale%20pre-training%20dataset%20for%202D%20and%203D%20called%0ALV3D%20by%20combining%20multiple%20existing%202D%20and%203D%20recognition%20datasets%20under%20a%0Acommon%20task%20formulation%3A%20as%20multi-turn%20question-answering.%20Next%2C%20we%20introduce%20a%0Anew%20MLLM%20named%20Cube-LLM%20and%20pre-train%20it%20on%20LV3D.%20We%20show%20that%20pure%20data%0Ascaling%20makes%20a%20strong%203D%20perception%20capability%20without%203D%20specific%0Aarchitectural%20design%20or%20training%20objective.%20Cube-LLM%20exhibits%20intriguing%0Aproperties%20similar%20to%20LLMs%3A%20%281%29%20Cube-LLM%20can%20apply%20chain-of-thought%20prompting%0Ato%20improve%203D%20understanding%20from%202D%20context%20information.%20%282%29%20Cube-LLM%20can%0Afollow%20complex%20and%20diverse%20instructions%20and%20adapt%20to%20versatile%20input%20and%20output%0Aformats.%20%283%29%20Cube-LLM%20can%20be%20visually%20prompted%20such%20as%202D%20box%20or%20a%20set%20of%0Acandidate%203D%20boxes%20from%20specialists.%20Our%20experiments%20on%20outdoor%20benchmarks%0Ademonstrate%20that%20Cube-LLM%20significantly%20outperforms%20existing%20baselines%20by%2021.3%0Apoints%20of%20AP-BEV%20on%20the%20Talk2Car%20dataset%20for%203D%20grounded%20reasoning%20and%2017.7%0Apoints%20on%20the%20DriveLM%20dataset%20for%20complex%20reasoning%20about%20driving%20scenarios%2C%0Arespectively.%20Cube-LLM%20also%20shows%20competitive%20results%20in%20general%20MLLM%0Abenchmarks%20such%20as%20refCOCO%20for%202D%20grounding%20with%20%2887.0%29%20average%20score%2C%20as%20well%0Aas%20visual%20question%20answering%20benchmarks%20such%20as%20VQAv2%2C%20GQA%2C%20SQA%2C%20POPE%2C%20etc.%20for%0Acomplex%20reasoning.%20Our%20project%20is%20available%20at%0Ahttps%3A//janghyuncho.github.io/Cube-LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03685v1&entry.124074799=Read"},
{"title": "Neural Graph Mapping for Dense SLAM with Efficient Loop Closure", "author": "Leonard Bruns and Jun Zhang and Patric Jensfelt", "abstract": "  Existing neural field-based SLAM methods typically employ a single monolithic\nfield as their scene representation. This prevents efficient incorporation of\nloop closure constraints and limits scalability. To address these shortcomings,\nwe propose a neural mapping framework which anchors lightweight neural fields\nto the pose graph of a sparse visual SLAM system. Our approach shows the\nability to integrate large-scale loop closures, while limiting necessary\nreintegration. Furthermore, we verify the scalability of our approach by\ndemonstrating successful building-scale mapping taking multiple loop closures\ninto account during the optimization, and show that our method outperforms\nexisting state-of-the-art approaches on large scenes in terms of quality and\nruntime. Our code is available at\nhttps://kth-rpl.github.io/neural_graph_mapping/.\n", "link": "http://arxiv.org/abs/2405.03633v1", "date": "2024-05-06", "relevancy": 2.4259, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.62}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6105}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Graph%20Mapping%20for%20Dense%20SLAM%20with%20Efficient%20Loop%20Closure&body=Title%3A%20Neural%20Graph%20Mapping%20for%20Dense%20SLAM%20with%20Efficient%20Loop%20Closure%0AAuthor%3A%20Leonard%20Bruns%20and%20Jun%20Zhang%20and%20Patric%20Jensfelt%0AAbstract%3A%20%20%20Existing%20neural%20field-based%20SLAM%20methods%20typically%20employ%20a%20single%20monolithic%0Afield%20as%20their%20scene%20representation.%20This%20prevents%20efficient%20incorporation%20of%0Aloop%20closure%20constraints%20and%20limits%20scalability.%20To%20address%20these%20shortcomings%2C%0Awe%20propose%20a%20neural%20mapping%20framework%20which%20anchors%20lightweight%20neural%20fields%0Ato%20the%20pose%20graph%20of%20a%20sparse%20visual%20SLAM%20system.%20Our%20approach%20shows%20the%0Aability%20to%20integrate%20large-scale%20loop%20closures%2C%20while%20limiting%20necessary%0Areintegration.%20Furthermore%2C%20we%20verify%20the%20scalability%20of%20our%20approach%20by%0Ademonstrating%20successful%20building-scale%20mapping%20taking%20multiple%20loop%20closures%0Ainto%20account%20during%20the%20optimization%2C%20and%20show%20that%20our%20method%20outperforms%0Aexisting%20state-of-the-art%20approaches%20on%20large%20scenes%20in%20terms%20of%20quality%20and%0Aruntime.%20Our%20code%20is%20available%20at%0Ahttps%3A//kth-rpl.github.io/neural_graph_mapping/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Graph%2520Mapping%2520for%2520Dense%2520SLAM%2520with%2520Efficient%2520Loop%2520Closure%26entry.906535625%3DLeonard%2520Bruns%2520and%2520Jun%2520Zhang%2520and%2520Patric%2520Jensfelt%26entry.1292438233%3D%2520%2520Existing%2520neural%2520field-based%2520SLAM%2520methods%2520typically%2520employ%2520a%2520single%2520monolithic%250Afield%2520as%2520their%2520scene%2520representation.%2520This%2520prevents%2520efficient%2520incorporation%2520of%250Aloop%2520closure%2520constraints%2520and%2520limits%2520scalability.%2520To%2520address%2520these%2520shortcomings%252C%250Awe%2520propose%2520a%2520neural%2520mapping%2520framework%2520which%2520anchors%2520lightweight%2520neural%2520fields%250Ato%2520the%2520pose%2520graph%2520of%2520a%2520sparse%2520visual%2520SLAM%2520system.%2520Our%2520approach%2520shows%2520the%250Aability%2520to%2520integrate%2520large-scale%2520loop%2520closures%252C%2520while%2520limiting%2520necessary%250Areintegration.%2520Furthermore%252C%2520we%2520verify%2520the%2520scalability%2520of%2520our%2520approach%2520by%250Ademonstrating%2520successful%2520building-scale%2520mapping%2520taking%2520multiple%2520loop%2520closures%250Ainto%2520account%2520during%2520the%2520optimization%252C%2520and%2520show%2520that%2520our%2520method%2520outperforms%250Aexisting%2520state-of-the-art%2520approaches%2520on%2520large%2520scenes%2520in%2520terms%2520of%2520quality%2520and%250Aruntime.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//kth-rpl.github.io/neural_graph_mapping/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Graph%20Mapping%20for%20Dense%20SLAM%20with%20Efficient%20Loop%20Closure&entry.906535625=Leonard%20Bruns%20and%20Jun%20Zhang%20and%20Patric%20Jensfelt&entry.1292438233=%20%20Existing%20neural%20field-based%20SLAM%20methods%20typically%20employ%20a%20single%20monolithic%0Afield%20as%20their%20scene%20representation.%20This%20prevents%20efficient%20incorporation%20of%0Aloop%20closure%20constraints%20and%20limits%20scalability.%20To%20address%20these%20shortcomings%2C%0Awe%20propose%20a%20neural%20mapping%20framework%20which%20anchors%20lightweight%20neural%20fields%0Ato%20the%20pose%20graph%20of%20a%20sparse%20visual%20SLAM%20system.%20Our%20approach%20shows%20the%0Aability%20to%20integrate%20large-scale%20loop%20closures%2C%20while%20limiting%20necessary%0Areintegration.%20Furthermore%2C%20we%20verify%20the%20scalability%20of%20our%20approach%20by%0Ademonstrating%20successful%20building-scale%20mapping%20taking%20multiple%20loop%20closures%0Ainto%20account%20during%20the%20optimization%2C%20and%20show%20that%20our%20method%20outperforms%0Aexisting%20state-of-the-art%20approaches%20on%20large%20scenes%20in%20terms%20of%20quality%20and%0Aruntime.%20Our%20code%20is%20available%20at%0Ahttps%3A//kth-rpl.github.io/neural_graph_mapping/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03633v1&entry.124074799=Read"},
{"title": "LightTR: A Lightweight Framework for Federated Trajectory Recovery", "author": "Ziqiao Liu and Hao Miao and Yan Zhao and Chenxi Liu and Kai Zheng and Huan Li", "abstract": "  With the proliferation of GPS-equipped edge devices, huge trajectory data is\ngenerated and accumulated in various domains, motivating a variety of urban\napplications. Due to the limited acquisition capabilities of edge devices, a\nlot of trajectories are recorded at a low sampling rate, which may lead to the\neffectiveness drop of urban applications. We aim to recover a high-sampled\ntrajectory based on the low-sampled trajectory in free space, i.e., without\nroad network information, to enhance the usability of trajectory data and\nsupport urban applications more effectively. Recent proposals targeting\ntrajectory recovery often assume that trajectories are available at a central\nlocation, which fail to handle the decentralized trajectories and hurt privacy.\nTo bridge the gap between decentralized training and trajectory recovery, we\npropose a lightweight framework, LightTR, for federated trajectory recovery\nbased on a client-server architecture, while keeping the data decentralized and\nprivate in each client/platform center (e.g., each data center of a company).\nSpecifically, considering the limited processing capabilities of edge devices,\nLightTR encompasses a light local trajectory embedding module that offers\nimproved computational efficiency without compromising its feature extraction\ncapabilities. LightTR also features a meta-knowledge enhanced local-global\ntraining scheme to reduce communication costs between the server and clients\nand thus further offer efficiency improvement. Extensive experiments\ndemonstrate the effectiveness and efficiency of the proposed framework.\n", "link": "http://arxiv.org/abs/2405.03409v1", "date": "2024-05-06", "relevancy": 2.4054, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.501}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4772}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightTR%3A%20A%20Lightweight%20Framework%20for%20Federated%20Trajectory%20Recovery&body=Title%3A%20LightTR%3A%20A%20Lightweight%20Framework%20for%20Federated%20Trajectory%20Recovery%0AAuthor%3A%20Ziqiao%20Liu%20and%20Hao%20Miao%20and%20Yan%20Zhao%20and%20Chenxi%20Liu%20and%20Kai%20Zheng%20and%20Huan%20Li%0AAbstract%3A%20%20%20With%20the%20proliferation%20of%20GPS-equipped%20edge%20devices%2C%20huge%20trajectory%20data%20is%0Agenerated%20and%20accumulated%20in%20various%20domains%2C%20motivating%20a%20variety%20of%20urban%0Aapplications.%20Due%20to%20the%20limited%20acquisition%20capabilities%20of%20edge%20devices%2C%20a%0Alot%20of%20trajectories%20are%20recorded%20at%20a%20low%20sampling%20rate%2C%20which%20may%20lead%20to%20the%0Aeffectiveness%20drop%20of%20urban%20applications.%20We%20aim%20to%20recover%20a%20high-sampled%0Atrajectory%20based%20on%20the%20low-sampled%20trajectory%20in%20free%20space%2C%20i.e.%2C%20without%0Aroad%20network%20information%2C%20to%20enhance%20the%20usability%20of%20trajectory%20data%20and%0Asupport%20urban%20applications%20more%20effectively.%20Recent%20proposals%20targeting%0Atrajectory%20recovery%20often%20assume%20that%20trajectories%20are%20available%20at%20a%20central%0Alocation%2C%20which%20fail%20to%20handle%20the%20decentralized%20trajectories%20and%20hurt%20privacy.%0ATo%20bridge%20the%20gap%20between%20decentralized%20training%20and%20trajectory%20recovery%2C%20we%0Apropose%20a%20lightweight%20framework%2C%20LightTR%2C%20for%20federated%20trajectory%20recovery%0Abased%20on%20a%20client-server%20architecture%2C%20while%20keeping%20the%20data%20decentralized%20and%0Aprivate%20in%20each%20client/platform%20center%20%28e.g.%2C%20each%20data%20center%20of%20a%20company%29.%0ASpecifically%2C%20considering%20the%20limited%20processing%20capabilities%20of%20edge%20devices%2C%0ALightTR%20encompasses%20a%20light%20local%20trajectory%20embedding%20module%20that%20offers%0Aimproved%20computational%20efficiency%20without%20compromising%20its%20feature%20extraction%0Acapabilities.%20LightTR%20also%20features%20a%20meta-knowledge%20enhanced%20local-global%0Atraining%20scheme%20to%20reduce%20communication%20costs%20between%20the%20server%20and%20clients%0Aand%20thus%20further%20offer%20efficiency%20improvement.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20and%20efficiency%20of%20the%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightTR%253A%2520A%2520Lightweight%2520Framework%2520for%2520Federated%2520Trajectory%2520Recovery%26entry.906535625%3DZiqiao%2520Liu%2520and%2520Hao%2520Miao%2520and%2520Yan%2520Zhao%2520and%2520Chenxi%2520Liu%2520and%2520Kai%2520Zheng%2520and%2520Huan%2520Li%26entry.1292438233%3D%2520%2520With%2520the%2520proliferation%2520of%2520GPS-equipped%2520edge%2520devices%252C%2520huge%2520trajectory%2520data%2520is%250Agenerated%2520and%2520accumulated%2520in%2520various%2520domains%252C%2520motivating%2520a%2520variety%2520of%2520urban%250Aapplications.%2520Due%2520to%2520the%2520limited%2520acquisition%2520capabilities%2520of%2520edge%2520devices%252C%2520a%250Alot%2520of%2520trajectories%2520are%2520recorded%2520at%2520a%2520low%2520sampling%2520rate%252C%2520which%2520may%2520lead%2520to%2520the%250Aeffectiveness%2520drop%2520of%2520urban%2520applications.%2520We%2520aim%2520to%2520recover%2520a%2520high-sampled%250Atrajectory%2520based%2520on%2520the%2520low-sampled%2520trajectory%2520in%2520free%2520space%252C%2520i.e.%252C%2520without%250Aroad%2520network%2520information%252C%2520to%2520enhance%2520the%2520usability%2520of%2520trajectory%2520data%2520and%250Asupport%2520urban%2520applications%2520more%2520effectively.%2520Recent%2520proposals%2520targeting%250Atrajectory%2520recovery%2520often%2520assume%2520that%2520trajectories%2520are%2520available%2520at%2520a%2520central%250Alocation%252C%2520which%2520fail%2520to%2520handle%2520the%2520decentralized%2520trajectories%2520and%2520hurt%2520privacy.%250ATo%2520bridge%2520the%2520gap%2520between%2520decentralized%2520training%2520and%2520trajectory%2520recovery%252C%2520we%250Apropose%2520a%2520lightweight%2520framework%252C%2520LightTR%252C%2520for%2520federated%2520trajectory%2520recovery%250Abased%2520on%2520a%2520client-server%2520architecture%252C%2520while%2520keeping%2520the%2520data%2520decentralized%2520and%250Aprivate%2520in%2520each%2520client/platform%2520center%2520%2528e.g.%252C%2520each%2520data%2520center%2520of%2520a%2520company%2529.%250ASpecifically%252C%2520considering%2520the%2520limited%2520processing%2520capabilities%2520of%2520edge%2520devices%252C%250ALightTR%2520encompasses%2520a%2520light%2520local%2520trajectory%2520embedding%2520module%2520that%2520offers%250Aimproved%2520computational%2520efficiency%2520without%2520compromising%2520its%2520feature%2520extraction%250Acapabilities.%2520LightTR%2520also%2520features%2520a%2520meta-knowledge%2520enhanced%2520local-global%250Atraining%2520scheme%2520to%2520reduce%2520communication%2520costs%2520between%2520the%2520server%2520and%2520clients%250Aand%2520thus%2520further%2520offer%2520efficiency%2520improvement.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520the%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightTR%3A%20A%20Lightweight%20Framework%20for%20Federated%20Trajectory%20Recovery&entry.906535625=Ziqiao%20Liu%20and%20Hao%20Miao%20and%20Yan%20Zhao%20and%20Chenxi%20Liu%20and%20Kai%20Zheng%20and%20Huan%20Li&entry.1292438233=%20%20With%20the%20proliferation%20of%20GPS-equipped%20edge%20devices%2C%20huge%20trajectory%20data%20is%0Agenerated%20and%20accumulated%20in%20various%20domains%2C%20motivating%20a%20variety%20of%20urban%0Aapplications.%20Due%20to%20the%20limited%20acquisition%20capabilities%20of%20edge%20devices%2C%20a%0Alot%20of%20trajectories%20are%20recorded%20at%20a%20low%20sampling%20rate%2C%20which%20may%20lead%20to%20the%0Aeffectiveness%20drop%20of%20urban%20applications.%20We%20aim%20to%20recover%20a%20high-sampled%0Atrajectory%20based%20on%20the%20low-sampled%20trajectory%20in%20free%20space%2C%20i.e.%2C%20without%0Aroad%20network%20information%2C%20to%20enhance%20the%20usability%20of%20trajectory%20data%20and%0Asupport%20urban%20applications%20more%20effectively.%20Recent%20proposals%20targeting%0Atrajectory%20recovery%20often%20assume%20that%20trajectories%20are%20available%20at%20a%20central%0Alocation%2C%20which%20fail%20to%20handle%20the%20decentralized%20trajectories%20and%20hurt%20privacy.%0ATo%20bridge%20the%20gap%20between%20decentralized%20training%20and%20trajectory%20recovery%2C%20we%0Apropose%20a%20lightweight%20framework%2C%20LightTR%2C%20for%20federated%20trajectory%20recovery%0Abased%20on%20a%20client-server%20architecture%2C%20while%20keeping%20the%20data%20decentralized%20and%0Aprivate%20in%20each%20client/platform%20center%20%28e.g.%2C%20each%20data%20center%20of%20a%20company%29.%0ASpecifically%2C%20considering%20the%20limited%20processing%20capabilities%20of%20edge%20devices%2C%0ALightTR%20encompasses%20a%20light%20local%20trajectory%20embedding%20module%20that%20offers%0Aimproved%20computational%20efficiency%20without%20compromising%20its%20feature%20extraction%0Acapabilities.%20LightTR%20also%20features%20a%20meta-knowledge%20enhanced%20local-global%0Atraining%20scheme%20to%20reduce%20communication%20costs%20between%20the%20server%20and%20clients%0Aand%20thus%20further%20offer%20efficiency%20improvement.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20and%20efficiency%20of%20the%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03409v1&entry.124074799=Read"},
{"title": "RepVGG-GELAN: Enhanced GELAN with VGG-STYLE ConvNets for Brain Tumour\n  Detection", "author": "Thennarasi Balakrishnan and Sandeep Singh Sengar", "abstract": "  Object detection algorithms particularly those based on YOLO have\ndemonstrated remarkable efficiency in balancing speed and accuracy. However,\ntheir application in brain tumour detection remains underexplored. This study\nproposes RepVGG-GELAN, a novel YOLO architecture enhanced with RepVGG, a\nreparameterized convolutional approach for object detection tasks particularly\nfocusing on brain tumour detection within medical images. RepVGG-GELAN\nleverages the RepVGG architecture to improve both speed and accuracy in\ndetecting brain tumours. Integrating RepVGG into the YOLO framework aims to\nachieve a balance between computational efficiency and detection performance.\nThis study includes a spatial pyramid pooling-based Generalized Efficient Layer\nAggregation Network (GELAN) architecture which further enhances the capability\nof RepVGG. Experimental evaluation conducted on a brain tumour dataset\ndemonstrates the effectiveness of RepVGG-GELAN surpassing existing RCS-YOLO in\nterms of precision and speed. Specifically, RepVGG-GELAN achieves an increased\nprecision of 4.91% and an increased AP50 of 2.54% over the latest existing\napproach while operating at 240.7 GFLOPs. The proposed RepVGG-GELAN with GELAN\narchitecture presents promising results establishing itself as a\nstate-of-the-art solution for accurate and efficient brain tumour detection in\nmedical images. The implementation code is publicly available at\nhttps://github.com/ThensiB/RepVGG-GELAN.\n", "link": "http://arxiv.org/abs/2405.03541v1", "date": "2024-05-06", "relevancy": 2.3896, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.502}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.469}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RepVGG-GELAN%3A%20Enhanced%20GELAN%20with%20VGG-STYLE%20ConvNets%20for%20Brain%20Tumour%0A%20%20Detection&body=Title%3A%20RepVGG-GELAN%3A%20Enhanced%20GELAN%20with%20VGG-STYLE%20ConvNets%20for%20Brain%20Tumour%0A%20%20Detection%0AAuthor%3A%20Thennarasi%20Balakrishnan%20and%20Sandeep%20Singh%20Sengar%0AAbstract%3A%20%20%20Object%20detection%20algorithms%20particularly%20those%20based%20on%20YOLO%20have%0Ademonstrated%20remarkable%20efficiency%20in%20balancing%20speed%20and%20accuracy.%20However%2C%0Atheir%20application%20in%20brain%20tumour%20detection%20remains%20underexplored.%20This%20study%0Aproposes%20RepVGG-GELAN%2C%20a%20novel%20YOLO%20architecture%20enhanced%20with%20RepVGG%2C%20a%0Areparameterized%20convolutional%20approach%20for%20object%20detection%20tasks%20particularly%0Afocusing%20on%20brain%20tumour%20detection%20within%20medical%20images.%20RepVGG-GELAN%0Aleverages%20the%20RepVGG%20architecture%20to%20improve%20both%20speed%20and%20accuracy%20in%0Adetecting%20brain%20tumours.%20Integrating%20RepVGG%20into%20the%20YOLO%20framework%20aims%20to%0Aachieve%20a%20balance%20between%20computational%20efficiency%20and%20detection%20performance.%0AThis%20study%20includes%20a%20spatial%20pyramid%20pooling-based%20Generalized%20Efficient%20Layer%0AAggregation%20Network%20%28GELAN%29%20architecture%20which%20further%20enhances%20the%20capability%0Aof%20RepVGG.%20Experimental%20evaluation%20conducted%20on%20a%20brain%20tumour%20dataset%0Ademonstrates%20the%20effectiveness%20of%20RepVGG-GELAN%20surpassing%20existing%20RCS-YOLO%20in%0Aterms%20of%20precision%20and%20speed.%20Specifically%2C%20RepVGG-GELAN%20achieves%20an%20increased%0Aprecision%20of%204.91%25%20and%20an%20increased%20AP50%20of%202.54%25%20over%20the%20latest%20existing%0Aapproach%20while%20operating%20at%20240.7%20GFLOPs.%20The%20proposed%20RepVGG-GELAN%20with%20GELAN%0Aarchitecture%20presents%20promising%20results%20establishing%20itself%20as%20a%0Astate-of-the-art%20solution%20for%20accurate%20and%20efficient%20brain%20tumour%20detection%20in%0Amedical%20images.%20The%20implementation%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/ThensiB/RepVGG-GELAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepVGG-GELAN%253A%2520Enhanced%2520GELAN%2520with%2520VGG-STYLE%2520ConvNets%2520for%2520Brain%2520Tumour%250A%2520%2520Detection%26entry.906535625%3DThennarasi%2520Balakrishnan%2520and%2520Sandeep%2520Singh%2520Sengar%26entry.1292438233%3D%2520%2520Object%2520detection%2520algorithms%2520particularly%2520those%2520based%2520on%2520YOLO%2520have%250Ademonstrated%2520remarkable%2520efficiency%2520in%2520balancing%2520speed%2520and%2520accuracy.%2520However%252C%250Atheir%2520application%2520in%2520brain%2520tumour%2520detection%2520remains%2520underexplored.%2520This%2520study%250Aproposes%2520RepVGG-GELAN%252C%2520a%2520novel%2520YOLO%2520architecture%2520enhanced%2520with%2520RepVGG%252C%2520a%250Areparameterized%2520convolutional%2520approach%2520for%2520object%2520detection%2520tasks%2520particularly%250Afocusing%2520on%2520brain%2520tumour%2520detection%2520within%2520medical%2520images.%2520RepVGG-GELAN%250Aleverages%2520the%2520RepVGG%2520architecture%2520to%2520improve%2520both%2520speed%2520and%2520accuracy%2520in%250Adetecting%2520brain%2520tumours.%2520Integrating%2520RepVGG%2520into%2520the%2520YOLO%2520framework%2520aims%2520to%250Aachieve%2520a%2520balance%2520between%2520computational%2520efficiency%2520and%2520detection%2520performance.%250AThis%2520study%2520includes%2520a%2520spatial%2520pyramid%2520pooling-based%2520Generalized%2520Efficient%2520Layer%250AAggregation%2520Network%2520%2528GELAN%2529%2520architecture%2520which%2520further%2520enhances%2520the%2520capability%250Aof%2520RepVGG.%2520Experimental%2520evaluation%2520conducted%2520on%2520a%2520brain%2520tumour%2520dataset%250Ademonstrates%2520the%2520effectiveness%2520of%2520RepVGG-GELAN%2520surpassing%2520existing%2520RCS-YOLO%2520in%250Aterms%2520of%2520precision%2520and%2520speed.%2520Specifically%252C%2520RepVGG-GELAN%2520achieves%2520an%2520increased%250Aprecision%2520of%25204.91%2525%2520and%2520an%2520increased%2520AP50%2520of%25202.54%2525%2520over%2520the%2520latest%2520existing%250Aapproach%2520while%2520operating%2520at%2520240.7%2520GFLOPs.%2520The%2520proposed%2520RepVGG-GELAN%2520with%2520GELAN%250Aarchitecture%2520presents%2520promising%2520results%2520establishing%2520itself%2520as%2520a%250Astate-of-the-art%2520solution%2520for%2520accurate%2520and%2520efficient%2520brain%2520tumour%2520detection%2520in%250Amedical%2520images.%2520The%2520implementation%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/ThensiB/RepVGG-GELAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RepVGG-GELAN%3A%20Enhanced%20GELAN%20with%20VGG-STYLE%20ConvNets%20for%20Brain%20Tumour%0A%20%20Detection&entry.906535625=Thennarasi%20Balakrishnan%20and%20Sandeep%20Singh%20Sengar&entry.1292438233=%20%20Object%20detection%20algorithms%20particularly%20those%20based%20on%20YOLO%20have%0Ademonstrated%20remarkable%20efficiency%20in%20balancing%20speed%20and%20accuracy.%20However%2C%0Atheir%20application%20in%20brain%20tumour%20detection%20remains%20underexplored.%20This%20study%0Aproposes%20RepVGG-GELAN%2C%20a%20novel%20YOLO%20architecture%20enhanced%20with%20RepVGG%2C%20a%0Areparameterized%20convolutional%20approach%20for%20object%20detection%20tasks%20particularly%0Afocusing%20on%20brain%20tumour%20detection%20within%20medical%20images.%20RepVGG-GELAN%0Aleverages%20the%20RepVGG%20architecture%20to%20improve%20both%20speed%20and%20accuracy%20in%0Adetecting%20brain%20tumours.%20Integrating%20RepVGG%20into%20the%20YOLO%20framework%20aims%20to%0Aachieve%20a%20balance%20between%20computational%20efficiency%20and%20detection%20performance.%0AThis%20study%20includes%20a%20spatial%20pyramid%20pooling-based%20Generalized%20Efficient%20Layer%0AAggregation%20Network%20%28GELAN%29%20architecture%20which%20further%20enhances%20the%20capability%0Aof%20RepVGG.%20Experimental%20evaluation%20conducted%20on%20a%20brain%20tumour%20dataset%0Ademonstrates%20the%20effectiveness%20of%20RepVGG-GELAN%20surpassing%20existing%20RCS-YOLO%20in%0Aterms%20of%20precision%20and%20speed.%20Specifically%2C%20RepVGG-GELAN%20achieves%20an%20increased%0Aprecision%20of%204.91%25%20and%20an%20increased%20AP50%20of%202.54%25%20over%20the%20latest%20existing%0Aapproach%20while%20operating%20at%20240.7%20GFLOPs.%20The%20proposed%20RepVGG-GELAN%20with%20GELAN%0Aarchitecture%20presents%20promising%20results%20establishing%20itself%20as%20a%0Astate-of-the-art%20solution%20for%20accurate%20and%20efficient%20brain%20tumour%20detection%20in%0Amedical%20images.%20The%20implementation%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/ThensiB/RepVGG-GELAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03541v1&entry.124074799=Read"},
{"title": "DBDH: A Dual-Branch Dual-Head Neural Network for Invisible Embedded\n  Regions Localization", "author": "Chengxin Zhao and Hefei Ling and Sijing Xie and Nan Sun and Zongyi Li and Yuxuan Shi and Jiazhong Chen", "abstract": "  Embedding invisible hyperlinks or hidden codes in images to replace QR codes\nhas become a hot topic recently. This technology requires first localizing the\nembedded region in the captured photos before decoding. Existing methods that\ntrain models to find the invisible embedded region struggle to obtain accurate\nlocalization results, leading to degraded decoding accuracy. This limitation is\nprimarily because the CNN network is sensitive to low-frequency signals, while\nthe embedded signal is typically in the high-frequency form. Based on this,\nthis paper proposes a Dual-Branch Dual-Head (DBDH) neural network tailored for\nthe precise localization of invisible embedded regions. Specifically, DBDH uses\na low-level texture branch containing 62 high-pass filters to capture the\nhigh-frequency signals induced by embedding. A high-level context branch is\nused to extract discriminative features between the embedded and normal\nregions. DBDH employs a detection head to directly detect the four vertices of\nthe embedding region. In addition, we introduce an extra segmentation head to\nsegment the mask of the embedding region during training. The segmentation head\nprovides pixel-level supervision for model learning, facilitating better\nlearning of the embedded signals. Based on two state-of-the-art invisible\noffline-to-online messaging methods, we construct two datasets and augmentation\nstrategies for training and testing localization models. Extensive experiments\ndemonstrate the superior performance of the proposed DBDH over existing\nmethods.\n", "link": "http://arxiv.org/abs/2405.03436v1", "date": "2024-05-06", "relevancy": 2.3625, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6246}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5687}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DBDH%3A%20A%20Dual-Branch%20Dual-Head%20Neural%20Network%20for%20Invisible%20Embedded%0A%20%20Regions%20Localization&body=Title%3A%20DBDH%3A%20A%20Dual-Branch%20Dual-Head%20Neural%20Network%20for%20Invisible%20Embedded%0A%20%20Regions%20Localization%0AAuthor%3A%20Chengxin%20Zhao%20and%20Hefei%20Ling%20and%20Sijing%20Xie%20and%20Nan%20Sun%20and%20Zongyi%20Li%20and%20Yuxuan%20Shi%20and%20Jiazhong%20Chen%0AAbstract%3A%20%20%20Embedding%20invisible%20hyperlinks%20or%20hidden%20codes%20in%20images%20to%20replace%20QR%20codes%0Ahas%20become%20a%20hot%20topic%20recently.%20This%20technology%20requires%20first%20localizing%20the%0Aembedded%20region%20in%20the%20captured%20photos%20before%20decoding.%20Existing%20methods%20that%0Atrain%20models%20to%20find%20the%20invisible%20embedded%20region%20struggle%20to%20obtain%20accurate%0Alocalization%20results%2C%20leading%20to%20degraded%20decoding%20accuracy.%20This%20limitation%20is%0Aprimarily%20because%20the%20CNN%20network%20is%20sensitive%20to%20low-frequency%20signals%2C%20while%0Athe%20embedded%20signal%20is%20typically%20in%20the%20high-frequency%20form.%20Based%20on%20this%2C%0Athis%20paper%20proposes%20a%20Dual-Branch%20Dual-Head%20%28DBDH%29%20neural%20network%20tailored%20for%0Athe%20precise%20localization%20of%20invisible%20embedded%20regions.%20Specifically%2C%20DBDH%20uses%0Aa%20low-level%20texture%20branch%20containing%2062%20high-pass%20filters%20to%20capture%20the%0Ahigh-frequency%20signals%20induced%20by%20embedding.%20A%20high-level%20context%20branch%20is%0Aused%20to%20extract%20discriminative%20features%20between%20the%20embedded%20and%20normal%0Aregions.%20DBDH%20employs%20a%20detection%20head%20to%20directly%20detect%20the%20four%20vertices%20of%0Athe%20embedding%20region.%20In%20addition%2C%20we%20introduce%20an%20extra%20segmentation%20head%20to%0Asegment%20the%20mask%20of%20the%20embedding%20region%20during%20training.%20The%20segmentation%20head%0Aprovides%20pixel-level%20supervision%20for%20model%20learning%2C%20facilitating%20better%0Alearning%20of%20the%20embedded%20signals.%20Based%20on%20two%20state-of-the-art%20invisible%0Aoffline-to-online%20messaging%20methods%2C%20we%20construct%20two%20datasets%20and%20augmentation%0Astrategies%20for%20training%20and%20testing%20localization%20models.%20Extensive%20experiments%0Ademonstrate%20the%20superior%20performance%20of%20the%20proposed%20DBDH%20over%20existing%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDBDH%253A%2520A%2520Dual-Branch%2520Dual-Head%2520Neural%2520Network%2520for%2520Invisible%2520Embedded%250A%2520%2520Regions%2520Localization%26entry.906535625%3DChengxin%2520Zhao%2520and%2520Hefei%2520Ling%2520and%2520Sijing%2520Xie%2520and%2520Nan%2520Sun%2520and%2520Zongyi%2520Li%2520and%2520Yuxuan%2520Shi%2520and%2520Jiazhong%2520Chen%26entry.1292438233%3D%2520%2520Embedding%2520invisible%2520hyperlinks%2520or%2520hidden%2520codes%2520in%2520images%2520to%2520replace%2520QR%2520codes%250Ahas%2520become%2520a%2520hot%2520topic%2520recently.%2520This%2520technology%2520requires%2520first%2520localizing%2520the%250Aembedded%2520region%2520in%2520the%2520captured%2520photos%2520before%2520decoding.%2520Existing%2520methods%2520that%250Atrain%2520models%2520to%2520find%2520the%2520invisible%2520embedded%2520region%2520struggle%2520to%2520obtain%2520accurate%250Alocalization%2520results%252C%2520leading%2520to%2520degraded%2520decoding%2520accuracy.%2520This%2520limitation%2520is%250Aprimarily%2520because%2520the%2520CNN%2520network%2520is%2520sensitive%2520to%2520low-frequency%2520signals%252C%2520while%250Athe%2520embedded%2520signal%2520is%2520typically%2520in%2520the%2520high-frequency%2520form.%2520Based%2520on%2520this%252C%250Athis%2520paper%2520proposes%2520a%2520Dual-Branch%2520Dual-Head%2520%2528DBDH%2529%2520neural%2520network%2520tailored%2520for%250Athe%2520precise%2520localization%2520of%2520invisible%2520embedded%2520regions.%2520Specifically%252C%2520DBDH%2520uses%250Aa%2520low-level%2520texture%2520branch%2520containing%252062%2520high-pass%2520filters%2520to%2520capture%2520the%250Ahigh-frequency%2520signals%2520induced%2520by%2520embedding.%2520A%2520high-level%2520context%2520branch%2520is%250Aused%2520to%2520extract%2520discriminative%2520features%2520between%2520the%2520embedded%2520and%2520normal%250Aregions.%2520DBDH%2520employs%2520a%2520detection%2520head%2520to%2520directly%2520detect%2520the%2520four%2520vertices%2520of%250Athe%2520embedding%2520region.%2520In%2520addition%252C%2520we%2520introduce%2520an%2520extra%2520segmentation%2520head%2520to%250Asegment%2520the%2520mask%2520of%2520the%2520embedding%2520region%2520during%2520training.%2520The%2520segmentation%2520head%250Aprovides%2520pixel-level%2520supervision%2520for%2520model%2520learning%252C%2520facilitating%2520better%250Alearning%2520of%2520the%2520embedded%2520signals.%2520Based%2520on%2520two%2520state-of-the-art%2520invisible%250Aoffline-to-online%2520messaging%2520methods%252C%2520we%2520construct%2520two%2520datasets%2520and%2520augmentation%250Astrategies%2520for%2520training%2520and%2520testing%2520localization%2520models.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520the%2520proposed%2520DBDH%2520over%2520existing%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DBDH%3A%20A%20Dual-Branch%20Dual-Head%20Neural%20Network%20for%20Invisible%20Embedded%0A%20%20Regions%20Localization&entry.906535625=Chengxin%20Zhao%20and%20Hefei%20Ling%20and%20Sijing%20Xie%20and%20Nan%20Sun%20and%20Zongyi%20Li%20and%20Yuxuan%20Shi%20and%20Jiazhong%20Chen&entry.1292438233=%20%20Embedding%20invisible%20hyperlinks%20or%20hidden%20codes%20in%20images%20to%20replace%20QR%20codes%0Ahas%20become%20a%20hot%20topic%20recently.%20This%20technology%20requires%20first%20localizing%20the%0Aembedded%20region%20in%20the%20captured%20photos%20before%20decoding.%20Existing%20methods%20that%0Atrain%20models%20to%20find%20the%20invisible%20embedded%20region%20struggle%20to%20obtain%20accurate%0Alocalization%20results%2C%20leading%20to%20degraded%20decoding%20accuracy.%20This%20limitation%20is%0Aprimarily%20because%20the%20CNN%20network%20is%20sensitive%20to%20low-frequency%20signals%2C%20while%0Athe%20embedded%20signal%20is%20typically%20in%20the%20high-frequency%20form.%20Based%20on%20this%2C%0Athis%20paper%20proposes%20a%20Dual-Branch%20Dual-Head%20%28DBDH%29%20neural%20network%20tailored%20for%0Athe%20precise%20localization%20of%20invisible%20embedded%20regions.%20Specifically%2C%20DBDH%20uses%0Aa%20low-level%20texture%20branch%20containing%2062%20high-pass%20filters%20to%20capture%20the%0Ahigh-frequency%20signals%20induced%20by%20embedding.%20A%20high-level%20context%20branch%20is%0Aused%20to%20extract%20discriminative%20features%20between%20the%20embedded%20and%20normal%0Aregions.%20DBDH%20employs%20a%20detection%20head%20to%20directly%20detect%20the%20four%20vertices%20of%0Athe%20embedding%20region.%20In%20addition%2C%20we%20introduce%20an%20extra%20segmentation%20head%20to%0Asegment%20the%20mask%20of%20the%20embedding%20region%20during%20training.%20The%20segmentation%20head%0Aprovides%20pixel-level%20supervision%20for%20model%20learning%2C%20facilitating%20better%0Alearning%20of%20the%20embedded%20signals.%20Based%20on%20two%20state-of-the-art%20invisible%0Aoffline-to-online%20messaging%20methods%2C%20we%20construct%20two%20datasets%20and%20augmentation%0Astrategies%20for%20training%20and%20testing%20localization%20models.%20Extensive%20experiments%0Ademonstrate%20the%20superior%20performance%20of%20the%20proposed%20DBDH%20over%20existing%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03436v1&entry.124074799=Read"},
{"title": "Recent Trends in 3D Reconstruction of General Non-Rigid Scenes", "author": "Raza Yunus and Jan Eric Lenssen and Michael Niemeyer and Yiyi Liao and Christian Rupprecht and Christian Theobalt and Gerard Pons-Moll and Jia-Bin Huang and Vladislav Golyanik and Eddy Ilg", "abstract": "  Reconstructing models of the real world, including 3D geometry, appearance,\nand motion of real scenes, is essential for computer graphics and computer\nvision. It enables the synthesizing of photorealistic novel views, useful for\nthe movie industry and AR/VR applications. It also facilitates the content\ncreation necessary in computer games and AR/VR by avoiding laborious manual\ndesign processes. Further, such models are fundamental for intelligent\ncomputing systems that need to interpret real-world scenes and actions to act\nand interact safely with the human world. Notably, the world surrounding us is\ndynamic, and reconstructing models of dynamic, non-rigidly moving scenes is a\nseverely underconstrained and challenging problem. This state-of-the-art report\n(STAR) offers the reader a comprehensive summary of state-of-the-art techniques\nwith monocular and multi-view inputs such as data from RGB and RGB-D sensors,\namong others, conveying an understanding of different approaches, their\npotential applications, and promising further research directions. The report\ncovers 3D reconstruction of general non-rigid scenes and further addresses the\ntechniques for scene decomposition, editing and controlling, and generalizable\nand generative modeling. More specifically, we first review the common and\nfundamental concepts necessary to understand and navigate the field and then\ndiscuss the state-of-the-art techniques by reviewing recent approaches that use\ntraditional and machine-learning-based neural representations, including a\ndiscussion on the newly enabled applications. The STAR is concluded with a\ndiscussion of the remaining limitations and open challenges.\n", "link": "http://arxiv.org/abs/2403.15064v2", "date": "2024-05-06", "relevancy": 2.3572, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6266}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5704}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recent%20Trends%20in%203D%20Reconstruction%20of%20General%20Non-Rigid%20Scenes&body=Title%3A%20Recent%20Trends%20in%203D%20Reconstruction%20of%20General%20Non-Rigid%20Scenes%0AAuthor%3A%20Raza%20Yunus%20and%20Jan%20Eric%20Lenssen%20and%20Michael%20Niemeyer%20and%20Yiyi%20Liao%20and%20Christian%20Rupprecht%20and%20Christian%20Theobalt%20and%20Gerard%20Pons-Moll%20and%20Jia-Bin%20Huang%20and%20Vladislav%20Golyanik%20and%20Eddy%20Ilg%0AAbstract%3A%20%20%20Reconstructing%20models%20of%20the%20real%20world%2C%20including%203D%20geometry%2C%20appearance%2C%0Aand%20motion%20of%20real%20scenes%2C%20is%20essential%20for%20computer%20graphics%20and%20computer%0Avision.%20It%20enables%20the%20synthesizing%20of%20photorealistic%20novel%20views%2C%20useful%20for%0Athe%20movie%20industry%20and%20AR/VR%20applications.%20It%20also%20facilitates%20the%20content%0Acreation%20necessary%20in%20computer%20games%20and%20AR/VR%20by%20avoiding%20laborious%20manual%0Adesign%20processes.%20Further%2C%20such%20models%20are%20fundamental%20for%20intelligent%0Acomputing%20systems%20that%20need%20to%20interpret%20real-world%20scenes%20and%20actions%20to%20act%0Aand%20interact%20safely%20with%20the%20human%20world.%20Notably%2C%20the%20world%20surrounding%20us%20is%0Adynamic%2C%20and%20reconstructing%20models%20of%20dynamic%2C%20non-rigidly%20moving%20scenes%20is%20a%0Aseverely%20underconstrained%20and%20challenging%20problem.%20This%20state-of-the-art%20report%0A%28STAR%29%20offers%20the%20reader%20a%20comprehensive%20summary%20of%20state-of-the-art%20techniques%0Awith%20monocular%20and%20multi-view%20inputs%20such%20as%20data%20from%20RGB%20and%20RGB-D%20sensors%2C%0Aamong%20others%2C%20conveying%20an%20understanding%20of%20different%20approaches%2C%20their%0Apotential%20applications%2C%20and%20promising%20further%20research%20directions.%20The%20report%0Acovers%203D%20reconstruction%20of%20general%20non-rigid%20scenes%20and%20further%20addresses%20the%0Atechniques%20for%20scene%20decomposition%2C%20editing%20and%20controlling%2C%20and%20generalizable%0Aand%20generative%20modeling.%20More%20specifically%2C%20we%20first%20review%20the%20common%20and%0Afundamental%20concepts%20necessary%20to%20understand%20and%20navigate%20the%20field%20and%20then%0Adiscuss%20the%20state-of-the-art%20techniques%20by%20reviewing%20recent%20approaches%20that%20use%0Atraditional%20and%20machine-learning-based%20neural%20representations%2C%20including%20a%0Adiscussion%20on%20the%20newly%20enabled%20applications.%20The%20STAR%20is%20concluded%20with%20a%0Adiscussion%20of%20the%20remaining%20limitations%20and%20open%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15064v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecent%2520Trends%2520in%25203D%2520Reconstruction%2520of%2520General%2520Non-Rigid%2520Scenes%26entry.906535625%3DRaza%2520Yunus%2520and%2520Jan%2520Eric%2520Lenssen%2520and%2520Michael%2520Niemeyer%2520and%2520Yiyi%2520Liao%2520and%2520Christian%2520Rupprecht%2520and%2520Christian%2520Theobalt%2520and%2520Gerard%2520Pons-Moll%2520and%2520Jia-Bin%2520Huang%2520and%2520Vladislav%2520Golyanik%2520and%2520Eddy%2520Ilg%26entry.1292438233%3D%2520%2520Reconstructing%2520models%2520of%2520the%2520real%2520world%252C%2520including%25203D%2520geometry%252C%2520appearance%252C%250Aand%2520motion%2520of%2520real%2520scenes%252C%2520is%2520essential%2520for%2520computer%2520graphics%2520and%2520computer%250Avision.%2520It%2520enables%2520the%2520synthesizing%2520of%2520photorealistic%2520novel%2520views%252C%2520useful%2520for%250Athe%2520movie%2520industry%2520and%2520AR/VR%2520applications.%2520It%2520also%2520facilitates%2520the%2520content%250Acreation%2520necessary%2520in%2520computer%2520games%2520and%2520AR/VR%2520by%2520avoiding%2520laborious%2520manual%250Adesign%2520processes.%2520Further%252C%2520such%2520models%2520are%2520fundamental%2520for%2520intelligent%250Acomputing%2520systems%2520that%2520need%2520to%2520interpret%2520real-world%2520scenes%2520and%2520actions%2520to%2520act%250Aand%2520interact%2520safely%2520with%2520the%2520human%2520world.%2520Notably%252C%2520the%2520world%2520surrounding%2520us%2520is%250Adynamic%252C%2520and%2520reconstructing%2520models%2520of%2520dynamic%252C%2520non-rigidly%2520moving%2520scenes%2520is%2520a%250Aseverely%2520underconstrained%2520and%2520challenging%2520problem.%2520This%2520state-of-the-art%2520report%250A%2528STAR%2529%2520offers%2520the%2520reader%2520a%2520comprehensive%2520summary%2520of%2520state-of-the-art%2520techniques%250Awith%2520monocular%2520and%2520multi-view%2520inputs%2520such%2520as%2520data%2520from%2520RGB%2520and%2520RGB-D%2520sensors%252C%250Aamong%2520others%252C%2520conveying%2520an%2520understanding%2520of%2520different%2520approaches%252C%2520their%250Apotential%2520applications%252C%2520and%2520promising%2520further%2520research%2520directions.%2520The%2520report%250Acovers%25203D%2520reconstruction%2520of%2520general%2520non-rigid%2520scenes%2520and%2520further%2520addresses%2520the%250Atechniques%2520for%2520scene%2520decomposition%252C%2520editing%2520and%2520controlling%252C%2520and%2520generalizable%250Aand%2520generative%2520modeling.%2520More%2520specifically%252C%2520we%2520first%2520review%2520the%2520common%2520and%250Afundamental%2520concepts%2520necessary%2520to%2520understand%2520and%2520navigate%2520the%2520field%2520and%2520then%250Adiscuss%2520the%2520state-of-the-art%2520techniques%2520by%2520reviewing%2520recent%2520approaches%2520that%2520use%250Atraditional%2520and%2520machine-learning-based%2520neural%2520representations%252C%2520including%2520a%250Adiscussion%2520on%2520the%2520newly%2520enabled%2520applications.%2520The%2520STAR%2520is%2520concluded%2520with%2520a%250Adiscussion%2520of%2520the%2520remaining%2520limitations%2520and%2520open%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15064v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recent%20Trends%20in%203D%20Reconstruction%20of%20General%20Non-Rigid%20Scenes&entry.906535625=Raza%20Yunus%20and%20Jan%20Eric%20Lenssen%20and%20Michael%20Niemeyer%20and%20Yiyi%20Liao%20and%20Christian%20Rupprecht%20and%20Christian%20Theobalt%20and%20Gerard%20Pons-Moll%20and%20Jia-Bin%20Huang%20and%20Vladislav%20Golyanik%20and%20Eddy%20Ilg&entry.1292438233=%20%20Reconstructing%20models%20of%20the%20real%20world%2C%20including%203D%20geometry%2C%20appearance%2C%0Aand%20motion%20of%20real%20scenes%2C%20is%20essential%20for%20computer%20graphics%20and%20computer%0Avision.%20It%20enables%20the%20synthesizing%20of%20photorealistic%20novel%20views%2C%20useful%20for%0Athe%20movie%20industry%20and%20AR/VR%20applications.%20It%20also%20facilitates%20the%20content%0Acreation%20necessary%20in%20computer%20games%20and%20AR/VR%20by%20avoiding%20laborious%20manual%0Adesign%20processes.%20Further%2C%20such%20models%20are%20fundamental%20for%20intelligent%0Acomputing%20systems%20that%20need%20to%20interpret%20real-world%20scenes%20and%20actions%20to%20act%0Aand%20interact%20safely%20with%20the%20human%20world.%20Notably%2C%20the%20world%20surrounding%20us%20is%0Adynamic%2C%20and%20reconstructing%20models%20of%20dynamic%2C%20non-rigidly%20moving%20scenes%20is%20a%0Aseverely%20underconstrained%20and%20challenging%20problem.%20This%20state-of-the-art%20report%0A%28STAR%29%20offers%20the%20reader%20a%20comprehensive%20summary%20of%20state-of-the-art%20techniques%0Awith%20monocular%20and%20multi-view%20inputs%20such%20as%20data%20from%20RGB%20and%20RGB-D%20sensors%2C%0Aamong%20others%2C%20conveying%20an%20understanding%20of%20different%20approaches%2C%20their%0Apotential%20applications%2C%20and%20promising%20further%20research%20directions.%20The%20report%0Acovers%203D%20reconstruction%20of%20general%20non-rigid%20scenes%20and%20further%20addresses%20the%0Atechniques%20for%20scene%20decomposition%2C%20editing%20and%20controlling%2C%20and%20generalizable%0Aand%20generative%20modeling.%20More%20specifically%2C%20we%20first%20review%20the%20common%20and%0Afundamental%20concepts%20necessary%20to%20understand%20and%20navigate%20the%20field%20and%20then%0Adiscuss%20the%20state-of-the-art%20techniques%20by%20reviewing%20recent%20approaches%20that%20use%0Atraditional%20and%20machine-learning-based%20neural%20representations%2C%20including%20a%0Adiscussion%20on%20the%20newly%20enabled%20applications.%20The%20STAR%20is%20concluded%20with%20a%0Adiscussion%20of%20the%20remaining%20limitations%20and%20open%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15064v2&entry.124074799=Read"},
{"title": "Adaptive Retrieval and Scalable Indexing for k-NN Search with\n  Cross-Encoders", "author": "Nishant Yadav and Nicholas Monath and Manzil Zaheer and Rob Fergus and Andrew McCallum", "abstract": "  Cross-encoder (CE) models which compute similarity by jointly encoding a\nquery-item pair perform better than embedding-based models (dual-encoders) at\nestimating query-item relevance. Existing approaches perform k-NN search with\nCE by approximating the CE similarity with a vector embedding space fit either\nwith dual-encoders (DE) or CUR matrix factorization. DE-based\nretrieve-and-rerank approaches suffer from poor recall on new domains and the\nretrieval with DE is decoupled from the CE. While CUR-based approaches can be\nmore accurate than the DE-based approach, they require a prohibitively large\nnumber of CE calls to compute item embeddings, thus making it impractical for\ndeployment at scale. In this paper, we address these shortcomings with our\nproposed sparse-matrix factorization based method that efficiently computes\nlatent query and item embeddings to approximate CE scores and performs k-NN\nsearch with the approximate CE similarity. We compute item embeddings offline\nby factorizing a sparse matrix containing query-item CE scores for a set of\ntrain queries. Our method produces a high-quality approximation while requiring\nonly a fraction of CE calls as compared to CUR-based methods, and allows for\nleveraging DE to initialize the embedding space while avoiding compute- and\nresource-intensive finetuning of DE via distillation. At test time, the item\nembeddings remain fixed and retrieval occurs over rounds, alternating between\na) estimating the test query embedding by minimizing error in approximating CE\nscores of items retrieved thus far, and b) using the updated test query\nembedding for retrieving more items. Our k-NN search method improves recall by\nup to 5% (k=1) and 54% (k=100) over DE-based approaches. Additionally, our\nindexing approach achieves a speedup of up to 100x over CUR-based and 5x over\nDE distillation methods, while matching or improving k-NN search recall over\nbaselines.\n", "link": "http://arxiv.org/abs/2405.03651v1", "date": "2024-05-06", "relevancy": 2.3366, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4816}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4734}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Retrieval%20and%20Scalable%20Indexing%20for%20k-NN%20Search%20with%0A%20%20Cross-Encoders&body=Title%3A%20Adaptive%20Retrieval%20and%20Scalable%20Indexing%20for%20k-NN%20Search%20with%0A%20%20Cross-Encoders%0AAuthor%3A%20Nishant%20Yadav%20and%20Nicholas%20Monath%20and%20Manzil%20Zaheer%20and%20Rob%20Fergus%20and%20Andrew%20McCallum%0AAbstract%3A%20%20%20Cross-encoder%20%28CE%29%20models%20which%20compute%20similarity%20by%20jointly%20encoding%20a%0Aquery-item%20pair%20perform%20better%20than%20embedding-based%20models%20%28dual-encoders%29%20at%0Aestimating%20query-item%20relevance.%20Existing%20approaches%20perform%20k-NN%20search%20with%0ACE%20by%20approximating%20the%20CE%20similarity%20with%20a%20vector%20embedding%20space%20fit%20either%0Awith%20dual-encoders%20%28DE%29%20or%20CUR%20matrix%20factorization.%20DE-based%0Aretrieve-and-rerank%20approaches%20suffer%20from%20poor%20recall%20on%20new%20domains%20and%20the%0Aretrieval%20with%20DE%20is%20decoupled%20from%20the%20CE.%20While%20CUR-based%20approaches%20can%20be%0Amore%20accurate%20than%20the%20DE-based%20approach%2C%20they%20require%20a%20prohibitively%20large%0Anumber%20of%20CE%20calls%20to%20compute%20item%20embeddings%2C%20thus%20making%20it%20impractical%20for%0Adeployment%20at%20scale.%20In%20this%20paper%2C%20we%20address%20these%20shortcomings%20with%20our%0Aproposed%20sparse-matrix%20factorization%20based%20method%20that%20efficiently%20computes%0Alatent%20query%20and%20item%20embeddings%20to%20approximate%20CE%20scores%20and%20performs%20k-NN%0Asearch%20with%20the%20approximate%20CE%20similarity.%20We%20compute%20item%20embeddings%20offline%0Aby%20factorizing%20a%20sparse%20matrix%20containing%20query-item%20CE%20scores%20for%20a%20set%20of%0Atrain%20queries.%20Our%20method%20produces%20a%20high-quality%20approximation%20while%20requiring%0Aonly%20a%20fraction%20of%20CE%20calls%20as%20compared%20to%20CUR-based%20methods%2C%20and%20allows%20for%0Aleveraging%20DE%20to%20initialize%20the%20embedding%20space%20while%20avoiding%20compute-%20and%0Aresource-intensive%20finetuning%20of%20DE%20via%20distillation.%20At%20test%20time%2C%20the%20item%0Aembeddings%20remain%20fixed%20and%20retrieval%20occurs%20over%20rounds%2C%20alternating%20between%0Aa%29%20estimating%20the%20test%20query%20embedding%20by%20minimizing%20error%20in%20approximating%20CE%0Ascores%20of%20items%20retrieved%20thus%20far%2C%20and%20b%29%20using%20the%20updated%20test%20query%0Aembedding%20for%20retrieving%20more%20items.%20Our%20k-NN%20search%20method%20improves%20recall%20by%0Aup%20to%205%25%20%28k%3D1%29%20and%2054%25%20%28k%3D100%29%20over%20DE-based%20approaches.%20Additionally%2C%20our%0Aindexing%20approach%20achieves%20a%20speedup%20of%20up%20to%20100x%20over%20CUR-based%20and%205x%20over%0ADE%20distillation%20methods%2C%20while%20matching%20or%20improving%20k-NN%20search%20recall%20over%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Retrieval%2520and%2520Scalable%2520Indexing%2520for%2520k-NN%2520Search%2520with%250A%2520%2520Cross-Encoders%26entry.906535625%3DNishant%2520Yadav%2520and%2520Nicholas%2520Monath%2520and%2520Manzil%2520Zaheer%2520and%2520Rob%2520Fergus%2520and%2520Andrew%2520McCallum%26entry.1292438233%3D%2520%2520Cross-encoder%2520%2528CE%2529%2520models%2520which%2520compute%2520similarity%2520by%2520jointly%2520encoding%2520a%250Aquery-item%2520pair%2520perform%2520better%2520than%2520embedding-based%2520models%2520%2528dual-encoders%2529%2520at%250Aestimating%2520query-item%2520relevance.%2520Existing%2520approaches%2520perform%2520k-NN%2520search%2520with%250ACE%2520by%2520approximating%2520the%2520CE%2520similarity%2520with%2520a%2520vector%2520embedding%2520space%2520fit%2520either%250Awith%2520dual-encoders%2520%2528DE%2529%2520or%2520CUR%2520matrix%2520factorization.%2520DE-based%250Aretrieve-and-rerank%2520approaches%2520suffer%2520from%2520poor%2520recall%2520on%2520new%2520domains%2520and%2520the%250Aretrieval%2520with%2520DE%2520is%2520decoupled%2520from%2520the%2520CE.%2520While%2520CUR-based%2520approaches%2520can%2520be%250Amore%2520accurate%2520than%2520the%2520DE-based%2520approach%252C%2520they%2520require%2520a%2520prohibitively%2520large%250Anumber%2520of%2520CE%2520calls%2520to%2520compute%2520item%2520embeddings%252C%2520thus%2520making%2520it%2520impractical%2520for%250Adeployment%2520at%2520scale.%2520In%2520this%2520paper%252C%2520we%2520address%2520these%2520shortcomings%2520with%2520our%250Aproposed%2520sparse-matrix%2520factorization%2520based%2520method%2520that%2520efficiently%2520computes%250Alatent%2520query%2520and%2520item%2520embeddings%2520to%2520approximate%2520CE%2520scores%2520and%2520performs%2520k-NN%250Asearch%2520with%2520the%2520approximate%2520CE%2520similarity.%2520We%2520compute%2520item%2520embeddings%2520offline%250Aby%2520factorizing%2520a%2520sparse%2520matrix%2520containing%2520query-item%2520CE%2520scores%2520for%2520a%2520set%2520of%250Atrain%2520queries.%2520Our%2520method%2520produces%2520a%2520high-quality%2520approximation%2520while%2520requiring%250Aonly%2520a%2520fraction%2520of%2520CE%2520calls%2520as%2520compared%2520to%2520CUR-based%2520methods%252C%2520and%2520allows%2520for%250Aleveraging%2520DE%2520to%2520initialize%2520the%2520embedding%2520space%2520while%2520avoiding%2520compute-%2520and%250Aresource-intensive%2520finetuning%2520of%2520DE%2520via%2520distillation.%2520At%2520test%2520time%252C%2520the%2520item%250Aembeddings%2520remain%2520fixed%2520and%2520retrieval%2520occurs%2520over%2520rounds%252C%2520alternating%2520between%250Aa%2529%2520estimating%2520the%2520test%2520query%2520embedding%2520by%2520minimizing%2520error%2520in%2520approximating%2520CE%250Ascores%2520of%2520items%2520retrieved%2520thus%2520far%252C%2520and%2520b%2529%2520using%2520the%2520updated%2520test%2520query%250Aembedding%2520for%2520retrieving%2520more%2520items.%2520Our%2520k-NN%2520search%2520method%2520improves%2520recall%2520by%250Aup%2520to%25205%2525%2520%2528k%253D1%2529%2520and%252054%2525%2520%2528k%253D100%2529%2520over%2520DE-based%2520approaches.%2520Additionally%252C%2520our%250Aindexing%2520approach%2520achieves%2520a%2520speedup%2520of%2520up%2520to%2520100x%2520over%2520CUR-based%2520and%25205x%2520over%250ADE%2520distillation%2520methods%252C%2520while%2520matching%2520or%2520improving%2520k-NN%2520search%2520recall%2520over%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Retrieval%20and%20Scalable%20Indexing%20for%20k-NN%20Search%20with%0A%20%20Cross-Encoders&entry.906535625=Nishant%20Yadav%20and%20Nicholas%20Monath%20and%20Manzil%20Zaheer%20and%20Rob%20Fergus%20and%20Andrew%20McCallum&entry.1292438233=%20%20Cross-encoder%20%28CE%29%20models%20which%20compute%20similarity%20by%20jointly%20encoding%20a%0Aquery-item%20pair%20perform%20better%20than%20embedding-based%20models%20%28dual-encoders%29%20at%0Aestimating%20query-item%20relevance.%20Existing%20approaches%20perform%20k-NN%20search%20with%0ACE%20by%20approximating%20the%20CE%20similarity%20with%20a%20vector%20embedding%20space%20fit%20either%0Awith%20dual-encoders%20%28DE%29%20or%20CUR%20matrix%20factorization.%20DE-based%0Aretrieve-and-rerank%20approaches%20suffer%20from%20poor%20recall%20on%20new%20domains%20and%20the%0Aretrieval%20with%20DE%20is%20decoupled%20from%20the%20CE.%20While%20CUR-based%20approaches%20can%20be%0Amore%20accurate%20than%20the%20DE-based%20approach%2C%20they%20require%20a%20prohibitively%20large%0Anumber%20of%20CE%20calls%20to%20compute%20item%20embeddings%2C%20thus%20making%20it%20impractical%20for%0Adeployment%20at%20scale.%20In%20this%20paper%2C%20we%20address%20these%20shortcomings%20with%20our%0Aproposed%20sparse-matrix%20factorization%20based%20method%20that%20efficiently%20computes%0Alatent%20query%20and%20item%20embeddings%20to%20approximate%20CE%20scores%20and%20performs%20k-NN%0Asearch%20with%20the%20approximate%20CE%20similarity.%20We%20compute%20item%20embeddings%20offline%0Aby%20factorizing%20a%20sparse%20matrix%20containing%20query-item%20CE%20scores%20for%20a%20set%20of%0Atrain%20queries.%20Our%20method%20produces%20a%20high-quality%20approximation%20while%20requiring%0Aonly%20a%20fraction%20of%20CE%20calls%20as%20compared%20to%20CUR-based%20methods%2C%20and%20allows%20for%0Aleveraging%20DE%20to%20initialize%20the%20embedding%20space%20while%20avoiding%20compute-%20and%0Aresource-intensive%20finetuning%20of%20DE%20via%20distillation.%20At%20test%20time%2C%20the%20item%0Aembeddings%20remain%20fixed%20and%20retrieval%20occurs%20over%20rounds%2C%20alternating%20between%0Aa%29%20estimating%20the%20test%20query%20embedding%20by%20minimizing%20error%20in%20approximating%20CE%0Ascores%20of%20items%20retrieved%20thus%20far%2C%20and%20b%29%20using%20the%20updated%20test%20query%0Aembedding%20for%20retrieving%20more%20items.%20Our%20k-NN%20search%20method%20improves%20recall%20by%0Aup%20to%205%25%20%28k%3D1%29%20and%2054%25%20%28k%3D100%29%20over%20DE-based%20approaches.%20Additionally%2C%20our%0Aindexing%20approach%20achieves%20a%20speedup%20of%20up%20to%20100x%20over%20CUR-based%20and%205x%20over%0ADE%20distillation%20methods%2C%20while%20matching%20or%20improving%20k-NN%20search%20recall%20over%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03651v1&entry.124074799=Read"},
{"title": "Exploring the Efficacy of Federated-Continual Learning Nodes with\n  Attention-Based Classifier for Robust Web Phishing Detection: An Empirical\n  Investigation", "author": "Jesher Joshua M and Adhithya R and Sree Dananjay S and M Revathi", "abstract": "  Web phishing poses a dynamic threat, requiring detection systems to quickly\nadapt to the latest tactics. Traditional approaches of accumulating data and\nperiodically retraining models are outpaced. We propose a novel paradigm\ncombining federated learning and continual learning, enabling distributed nodes\nto continually update models on streams of new phishing data, without\naccumulating data. These locally adapted models are then aggregated at a\ncentral server via federated learning. To enhance detection, we introduce a\ncustom attention-based classifier model with residual connections, tailored for\nweb phishing, leveraging attention mechanisms to capture intricate phishing\npatterns. We evaluate our hybrid learning paradigm across continual learning\nstrategies (cumulative, replay, MIR, LwF) and model architectures through an\nempirical investigation. Our main contributions are: (1) a new hybrid\nfederated-continual learning paradigm for robust web phishing detection, and\n(2) a novel attention + residual connections based model explicitly designed\nfor this task, attaining 0.93 accuracy, 0.90 precision, 0.96 recall and 0.93\nf1-score with the LwF strategy, outperforming traditional approaches in\ndetecting emerging phishing threats while retaining past knowledge.\n", "link": "http://arxiv.org/abs/2405.03537v1", "date": "2024-05-06", "relevancy": 2.3364, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4707}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4671}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Efficacy%20of%20Federated-Continual%20Learning%20Nodes%20with%0A%20%20Attention-Based%20Classifier%20for%20Robust%20Web%20Phishing%20Detection%3A%20An%20Empirical%0A%20%20Investigation&body=Title%3A%20Exploring%20the%20Efficacy%20of%20Federated-Continual%20Learning%20Nodes%20with%0A%20%20Attention-Based%20Classifier%20for%20Robust%20Web%20Phishing%20Detection%3A%20An%20Empirical%0A%20%20Investigation%0AAuthor%3A%20Jesher%20Joshua%20M%20and%20Adhithya%20R%20and%20Sree%20Dananjay%20S%20and%20M%20Revathi%0AAbstract%3A%20%20%20Web%20phishing%20poses%20a%20dynamic%20threat%2C%20requiring%20detection%20systems%20to%20quickly%0Aadapt%20to%20the%20latest%20tactics.%20Traditional%20approaches%20of%20accumulating%20data%20and%0Aperiodically%20retraining%20models%20are%20outpaced.%20We%20propose%20a%20novel%20paradigm%0Acombining%20federated%20learning%20and%20continual%20learning%2C%20enabling%20distributed%20nodes%0Ato%20continually%20update%20models%20on%20streams%20of%20new%20phishing%20data%2C%20without%0Aaccumulating%20data.%20These%20locally%20adapted%20models%20are%20then%20aggregated%20at%20a%0Acentral%20server%20via%20federated%20learning.%20To%20enhance%20detection%2C%20we%20introduce%20a%0Acustom%20attention-based%20classifier%20model%20with%20residual%20connections%2C%20tailored%20for%0Aweb%20phishing%2C%20leveraging%20attention%20mechanisms%20to%20capture%20intricate%20phishing%0Apatterns.%20We%20evaluate%20our%20hybrid%20learning%20paradigm%20across%20continual%20learning%0Astrategies%20%28cumulative%2C%20replay%2C%20MIR%2C%20LwF%29%20and%20model%20architectures%20through%20an%0Aempirical%20investigation.%20Our%20main%20contributions%20are%3A%20%281%29%20a%20new%20hybrid%0Afederated-continual%20learning%20paradigm%20for%20robust%20web%20phishing%20detection%2C%20and%0A%282%29%20a%20novel%20attention%20%2B%20residual%20connections%20based%20model%20explicitly%20designed%0Afor%20this%20task%2C%20attaining%200.93%20accuracy%2C%200.90%20precision%2C%200.96%20recall%20and%200.93%0Af1-score%20with%20the%20LwF%20strategy%2C%20outperforming%20traditional%20approaches%20in%0Adetecting%20emerging%20phishing%20threats%20while%20retaining%20past%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Efficacy%2520of%2520Federated-Continual%2520Learning%2520Nodes%2520with%250A%2520%2520Attention-Based%2520Classifier%2520for%2520Robust%2520Web%2520Phishing%2520Detection%253A%2520An%2520Empirical%250A%2520%2520Investigation%26entry.906535625%3DJesher%2520Joshua%2520M%2520and%2520Adhithya%2520R%2520and%2520Sree%2520Dananjay%2520S%2520and%2520M%2520Revathi%26entry.1292438233%3D%2520%2520Web%2520phishing%2520poses%2520a%2520dynamic%2520threat%252C%2520requiring%2520detection%2520systems%2520to%2520quickly%250Aadapt%2520to%2520the%2520latest%2520tactics.%2520Traditional%2520approaches%2520of%2520accumulating%2520data%2520and%250Aperiodically%2520retraining%2520models%2520are%2520outpaced.%2520We%2520propose%2520a%2520novel%2520paradigm%250Acombining%2520federated%2520learning%2520and%2520continual%2520learning%252C%2520enabling%2520distributed%2520nodes%250Ato%2520continually%2520update%2520models%2520on%2520streams%2520of%2520new%2520phishing%2520data%252C%2520without%250Aaccumulating%2520data.%2520These%2520locally%2520adapted%2520models%2520are%2520then%2520aggregated%2520at%2520a%250Acentral%2520server%2520via%2520federated%2520learning.%2520To%2520enhance%2520detection%252C%2520we%2520introduce%2520a%250Acustom%2520attention-based%2520classifier%2520model%2520with%2520residual%2520connections%252C%2520tailored%2520for%250Aweb%2520phishing%252C%2520leveraging%2520attention%2520mechanisms%2520to%2520capture%2520intricate%2520phishing%250Apatterns.%2520We%2520evaluate%2520our%2520hybrid%2520learning%2520paradigm%2520across%2520continual%2520learning%250Astrategies%2520%2528cumulative%252C%2520replay%252C%2520MIR%252C%2520LwF%2529%2520and%2520model%2520architectures%2520through%2520an%250Aempirical%2520investigation.%2520Our%2520main%2520contributions%2520are%253A%2520%25281%2529%2520a%2520new%2520hybrid%250Afederated-continual%2520learning%2520paradigm%2520for%2520robust%2520web%2520phishing%2520detection%252C%2520and%250A%25282%2529%2520a%2520novel%2520attention%2520%252B%2520residual%2520connections%2520based%2520model%2520explicitly%2520designed%250Afor%2520this%2520task%252C%2520attaining%25200.93%2520accuracy%252C%25200.90%2520precision%252C%25200.96%2520recall%2520and%25200.93%250Af1-score%2520with%2520the%2520LwF%2520strategy%252C%2520outperforming%2520traditional%2520approaches%2520in%250Adetecting%2520emerging%2520phishing%2520threats%2520while%2520retaining%2520past%2520knowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Efficacy%20of%20Federated-Continual%20Learning%20Nodes%20with%0A%20%20Attention-Based%20Classifier%20for%20Robust%20Web%20Phishing%20Detection%3A%20An%20Empirical%0A%20%20Investigation&entry.906535625=Jesher%20Joshua%20M%20and%20Adhithya%20R%20and%20Sree%20Dananjay%20S%20and%20M%20Revathi&entry.1292438233=%20%20Web%20phishing%20poses%20a%20dynamic%20threat%2C%20requiring%20detection%20systems%20to%20quickly%0Aadapt%20to%20the%20latest%20tactics.%20Traditional%20approaches%20of%20accumulating%20data%20and%0Aperiodically%20retraining%20models%20are%20outpaced.%20We%20propose%20a%20novel%20paradigm%0Acombining%20federated%20learning%20and%20continual%20learning%2C%20enabling%20distributed%20nodes%0Ato%20continually%20update%20models%20on%20streams%20of%20new%20phishing%20data%2C%20without%0Aaccumulating%20data.%20These%20locally%20adapted%20models%20are%20then%20aggregated%20at%20a%0Acentral%20server%20via%20federated%20learning.%20To%20enhance%20detection%2C%20we%20introduce%20a%0Acustom%20attention-based%20classifier%20model%20with%20residual%20connections%2C%20tailored%20for%0Aweb%20phishing%2C%20leveraging%20attention%20mechanisms%20to%20capture%20intricate%20phishing%0Apatterns.%20We%20evaluate%20our%20hybrid%20learning%20paradigm%20across%20continual%20learning%0Astrategies%20%28cumulative%2C%20replay%2C%20MIR%2C%20LwF%29%20and%20model%20architectures%20through%20an%0Aempirical%20investigation.%20Our%20main%20contributions%20are%3A%20%281%29%20a%20new%20hybrid%0Afederated-continual%20learning%20paradigm%20for%20robust%20web%20phishing%20detection%2C%20and%0A%282%29%20a%20novel%20attention%20%2B%20residual%20connections%20based%20model%20explicitly%20designed%0Afor%20this%20task%2C%20attaining%200.93%20accuracy%2C%200.90%20precision%2C%200.96%20recall%20and%200.93%0Af1-score%20with%20the%20LwF%20strategy%2C%20outperforming%20traditional%20approaches%20in%0Adetecting%20emerging%20phishing%20threats%20while%20retaining%20past%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03537v1&entry.124074799=Read"},
{"title": "Motion Planning under Uncertainty: Integrating Learning-Based\n  Multi-Modal Predictors into Branch Model Predictive Control", "author": "Mohamed-Khalil Bouzidi and Bojan Derajic and Daniel Goehring and Joerg Reichardt", "abstract": "  In complex traffic environments, autonomous vehicles face multi-modal\nuncertainty about other agents' future behavior. To address this, recent\nadvancements in learningbased motion predictors output multi-modal predictions.\nWe present our novel framework that leverages Branch Model Predictive\nControl(BMPC) to account for these predictions. The framework includes an\nonline scenario-selection process guided by topology and collision risk\ncriteria. This efficiently selects a minimal set of predictions, rendering the\nBMPC realtime capable. Additionally, we introduce an adaptive decision\npostponing strategy that delays the planner's commitment to a single scenario\nuntil the uncertainty is resolved. Our comprehensive evaluations in traffic\nintersection and random highway merging scenarios demonstrate enhanced comfort\nand safety through our method.\n", "link": "http://arxiv.org/abs/2405.03470v1", "date": "2024-05-06", "relevancy": 2.295, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6352}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5907}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion%20Planning%20under%20Uncertainty%3A%20Integrating%20Learning-Based%0A%20%20Multi-Modal%20Predictors%20into%20Branch%20Model%20Predictive%20Control&body=Title%3A%20Motion%20Planning%20under%20Uncertainty%3A%20Integrating%20Learning-Based%0A%20%20Multi-Modal%20Predictors%20into%20Branch%20Model%20Predictive%20Control%0AAuthor%3A%20Mohamed-Khalil%20Bouzidi%20and%20Bojan%20Derajic%20and%20Daniel%20Goehring%20and%20Joerg%20Reichardt%0AAbstract%3A%20%20%20In%20complex%20traffic%20environments%2C%20autonomous%20vehicles%20face%20multi-modal%0Auncertainty%20about%20other%20agents%27%20future%20behavior.%20To%20address%20this%2C%20recent%0Aadvancements%20in%20learningbased%20motion%20predictors%20output%20multi-modal%20predictions.%0AWe%20present%20our%20novel%20framework%20that%20leverages%20Branch%20Model%20Predictive%0AControl%28BMPC%29%20to%20account%20for%20these%20predictions.%20The%20framework%20includes%20an%0Aonline%20scenario-selection%20process%20guided%20by%20topology%20and%20collision%20risk%0Acriteria.%20This%20efficiently%20selects%20a%20minimal%20set%20of%20predictions%2C%20rendering%20the%0ABMPC%20realtime%20capable.%20Additionally%2C%20we%20introduce%20an%20adaptive%20decision%0Apostponing%20strategy%20that%20delays%20the%20planner%27s%20commitment%20to%20a%20single%20scenario%0Auntil%20the%20uncertainty%20is%20resolved.%20Our%20comprehensive%20evaluations%20in%20traffic%0Aintersection%20and%20random%20highway%20merging%20scenarios%20demonstrate%20enhanced%20comfort%0Aand%20safety%20through%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion%2520Planning%2520under%2520Uncertainty%253A%2520Integrating%2520Learning-Based%250A%2520%2520Multi-Modal%2520Predictors%2520into%2520Branch%2520Model%2520Predictive%2520Control%26entry.906535625%3DMohamed-Khalil%2520Bouzidi%2520and%2520Bojan%2520Derajic%2520and%2520Daniel%2520Goehring%2520and%2520Joerg%2520Reichardt%26entry.1292438233%3D%2520%2520In%2520complex%2520traffic%2520environments%252C%2520autonomous%2520vehicles%2520face%2520multi-modal%250Auncertainty%2520about%2520other%2520agents%2527%2520future%2520behavior.%2520To%2520address%2520this%252C%2520recent%250Aadvancements%2520in%2520learningbased%2520motion%2520predictors%2520output%2520multi-modal%2520predictions.%250AWe%2520present%2520our%2520novel%2520framework%2520that%2520leverages%2520Branch%2520Model%2520Predictive%250AControl%2528BMPC%2529%2520to%2520account%2520for%2520these%2520predictions.%2520The%2520framework%2520includes%2520an%250Aonline%2520scenario-selection%2520process%2520guided%2520by%2520topology%2520and%2520collision%2520risk%250Acriteria.%2520This%2520efficiently%2520selects%2520a%2520minimal%2520set%2520of%2520predictions%252C%2520rendering%2520the%250ABMPC%2520realtime%2520capable.%2520Additionally%252C%2520we%2520introduce%2520an%2520adaptive%2520decision%250Apostponing%2520strategy%2520that%2520delays%2520the%2520planner%2527s%2520commitment%2520to%2520a%2520single%2520scenario%250Auntil%2520the%2520uncertainty%2520is%2520resolved.%2520Our%2520comprehensive%2520evaluations%2520in%2520traffic%250Aintersection%2520and%2520random%2520highway%2520merging%2520scenarios%2520demonstrate%2520enhanced%2520comfort%250Aand%2520safety%2520through%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion%20Planning%20under%20Uncertainty%3A%20Integrating%20Learning-Based%0A%20%20Multi-Modal%20Predictors%20into%20Branch%20Model%20Predictive%20Control&entry.906535625=Mohamed-Khalil%20Bouzidi%20and%20Bojan%20Derajic%20and%20Daniel%20Goehring%20and%20Joerg%20Reichardt&entry.1292438233=%20%20In%20complex%20traffic%20environments%2C%20autonomous%20vehicles%20face%20multi-modal%0Auncertainty%20about%20other%20agents%27%20future%20behavior.%20To%20address%20this%2C%20recent%0Aadvancements%20in%20learningbased%20motion%20predictors%20output%20multi-modal%20predictions.%0AWe%20present%20our%20novel%20framework%20that%20leverages%20Branch%20Model%20Predictive%0AControl%28BMPC%29%20to%20account%20for%20these%20predictions.%20The%20framework%20includes%20an%0Aonline%20scenario-selection%20process%20guided%20by%20topology%20and%20collision%20risk%0Acriteria.%20This%20efficiently%20selects%20a%20minimal%20set%20of%20predictions%2C%20rendering%20the%0ABMPC%20realtime%20capable.%20Additionally%2C%20we%20introduce%20an%20adaptive%20decision%0Apostponing%20strategy%20that%20delays%20the%20planner%27s%20commitment%20to%20a%20single%20scenario%0Auntil%20the%20uncertainty%20is%20resolved.%20Our%20comprehensive%20evaluations%20in%20traffic%0Aintersection%20and%20random%20highway%20merging%20scenarios%20demonstrate%20enhanced%20comfort%0Aand%20safety%20through%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03470v1&entry.124074799=Read"},
{"title": "Don't Waste Your Time: Early Stopping Cross-Validation", "author": "Edward Bergman and Lennart Purucker and Frank Hutter", "abstract": "  State-of-the-art automated machine learning systems for tabular data often\nemploy cross-validation; ensuring that measured performances generalize to\nunseen data, or that subsequent ensembling does not overfit. However, using\nk-fold cross-validation instead of holdout validation drastically increases the\ncomputational cost of validating a single configuration. While ensuring better\ngeneralization and, by extension, better performance, the additional cost is\noften prohibitive for effective model selection within a time budget. We aim to\nmake model selection with cross-validation more effective. Therefore, we study\nearly stopping the process of cross-validation during model selection. We\ninvestigate the impact of early stopping on random search for two algorithms,\nMLP and random forest, across 36 classification datasets. We further analyze\nthe impact of the number of folds by considering 3-, 5-, and 10-folds. In\naddition, we investigate the impact of early stopping with Bayesian\noptimization instead of random search and also repeated cross-validation. Our\nexploratory study shows that even a simple-to-understand and easy-to-implement\nmethod consistently allows model selection to converge faster; in ~94% of all\ndatasets, on average by ~214%. Moreover, stopping cross-validation enables\nmodel selection to explore the search space more exhaustively by considering\n+167% configurations on average within one hour, while also obtaining better\noverall performance.\n", "link": "http://arxiv.org/abs/2405.03389v1", "date": "2024-05-06", "relevancy": 2.2924, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4626}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4575}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Waste%20Your%20Time%3A%20Early%20Stopping%20Cross-Validation&body=Title%3A%20Don%27t%20Waste%20Your%20Time%3A%20Early%20Stopping%20Cross-Validation%0AAuthor%3A%20Edward%20Bergman%20and%20Lennart%20Purucker%20and%20Frank%20Hutter%0AAbstract%3A%20%20%20State-of-the-art%20automated%20machine%20learning%20systems%20for%20tabular%20data%20often%0Aemploy%20cross-validation%3B%20ensuring%20that%20measured%20performances%20generalize%20to%0Aunseen%20data%2C%20or%20that%20subsequent%20ensembling%20does%20not%20overfit.%20However%2C%20using%0Ak-fold%20cross-validation%20instead%20of%20holdout%20validation%20drastically%20increases%20the%0Acomputational%20cost%20of%20validating%20a%20single%20configuration.%20While%20ensuring%20better%0Ageneralization%20and%2C%20by%20extension%2C%20better%20performance%2C%20the%20additional%20cost%20is%0Aoften%20prohibitive%20for%20effective%20model%20selection%20within%20a%20time%20budget.%20We%20aim%20to%0Amake%20model%20selection%20with%20cross-validation%20more%20effective.%20Therefore%2C%20we%20study%0Aearly%20stopping%20the%20process%20of%20cross-validation%20during%20model%20selection.%20We%0Ainvestigate%20the%20impact%20of%20early%20stopping%20on%20random%20search%20for%20two%20algorithms%2C%0AMLP%20and%20random%20forest%2C%20across%2036%20classification%20datasets.%20We%20further%20analyze%0Athe%20impact%20of%20the%20number%20of%20folds%20by%20considering%203-%2C%205-%2C%20and%2010-folds.%20In%0Aaddition%2C%20we%20investigate%20the%20impact%20of%20early%20stopping%20with%20Bayesian%0Aoptimization%20instead%20of%20random%20search%20and%20also%20repeated%20cross-validation.%20Our%0Aexploratory%20study%20shows%20that%20even%20a%20simple-to-understand%20and%20easy-to-implement%0Amethod%20consistently%20allows%20model%20selection%20to%20converge%20faster%3B%20in%20~94%25%20of%20all%0Adatasets%2C%20on%20average%20by%20~214%25.%20Moreover%2C%20stopping%20cross-validation%20enables%0Amodel%20selection%20to%20explore%20the%20search%20space%20more%20exhaustively%20by%20considering%0A%2B167%25%20configurations%20on%20average%20within%20one%20hour%2C%20while%20also%20obtaining%20better%0Aoverall%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Waste%2520Your%2520Time%253A%2520Early%2520Stopping%2520Cross-Validation%26entry.906535625%3DEdward%2520Bergman%2520and%2520Lennart%2520Purucker%2520and%2520Frank%2520Hutter%26entry.1292438233%3D%2520%2520State-of-the-art%2520automated%2520machine%2520learning%2520systems%2520for%2520tabular%2520data%2520often%250Aemploy%2520cross-validation%253B%2520ensuring%2520that%2520measured%2520performances%2520generalize%2520to%250Aunseen%2520data%252C%2520or%2520that%2520subsequent%2520ensembling%2520does%2520not%2520overfit.%2520However%252C%2520using%250Ak-fold%2520cross-validation%2520instead%2520of%2520holdout%2520validation%2520drastically%2520increases%2520the%250Acomputational%2520cost%2520of%2520validating%2520a%2520single%2520configuration.%2520While%2520ensuring%2520better%250Ageneralization%2520and%252C%2520by%2520extension%252C%2520better%2520performance%252C%2520the%2520additional%2520cost%2520is%250Aoften%2520prohibitive%2520for%2520effective%2520model%2520selection%2520within%2520a%2520time%2520budget.%2520We%2520aim%2520to%250Amake%2520model%2520selection%2520with%2520cross-validation%2520more%2520effective.%2520Therefore%252C%2520we%2520study%250Aearly%2520stopping%2520the%2520process%2520of%2520cross-validation%2520during%2520model%2520selection.%2520We%250Ainvestigate%2520the%2520impact%2520of%2520early%2520stopping%2520on%2520random%2520search%2520for%2520two%2520algorithms%252C%250AMLP%2520and%2520random%2520forest%252C%2520across%252036%2520classification%2520datasets.%2520We%2520further%2520analyze%250Athe%2520impact%2520of%2520the%2520number%2520of%2520folds%2520by%2520considering%25203-%252C%25205-%252C%2520and%252010-folds.%2520In%250Aaddition%252C%2520we%2520investigate%2520the%2520impact%2520of%2520early%2520stopping%2520with%2520Bayesian%250Aoptimization%2520instead%2520of%2520random%2520search%2520and%2520also%2520repeated%2520cross-validation.%2520Our%250Aexploratory%2520study%2520shows%2520that%2520even%2520a%2520simple-to-understand%2520and%2520easy-to-implement%250Amethod%2520consistently%2520allows%2520model%2520selection%2520to%2520converge%2520faster%253B%2520in%2520~94%2525%2520of%2520all%250Adatasets%252C%2520on%2520average%2520by%2520~214%2525.%2520Moreover%252C%2520stopping%2520cross-validation%2520enables%250Amodel%2520selection%2520to%2520explore%2520the%2520search%2520space%2520more%2520exhaustively%2520by%2520considering%250A%252B167%2525%2520configurations%2520on%2520average%2520within%2520one%2520hour%252C%2520while%2520also%2520obtaining%2520better%250Aoverall%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Waste%20Your%20Time%3A%20Early%20Stopping%20Cross-Validation&entry.906535625=Edward%20Bergman%20and%20Lennart%20Purucker%20and%20Frank%20Hutter&entry.1292438233=%20%20State-of-the-art%20automated%20machine%20learning%20systems%20for%20tabular%20data%20often%0Aemploy%20cross-validation%3B%20ensuring%20that%20measured%20performances%20generalize%20to%0Aunseen%20data%2C%20or%20that%20subsequent%20ensembling%20does%20not%20overfit.%20However%2C%20using%0Ak-fold%20cross-validation%20instead%20of%20holdout%20validation%20drastically%20increases%20the%0Acomputational%20cost%20of%20validating%20a%20single%20configuration.%20While%20ensuring%20better%0Ageneralization%20and%2C%20by%20extension%2C%20better%20performance%2C%20the%20additional%20cost%20is%0Aoften%20prohibitive%20for%20effective%20model%20selection%20within%20a%20time%20budget.%20We%20aim%20to%0Amake%20model%20selection%20with%20cross-validation%20more%20effective.%20Therefore%2C%20we%20study%0Aearly%20stopping%20the%20process%20of%20cross-validation%20during%20model%20selection.%20We%0Ainvestigate%20the%20impact%20of%20early%20stopping%20on%20random%20search%20for%20two%20algorithms%2C%0AMLP%20and%20random%20forest%2C%20across%2036%20classification%20datasets.%20We%20further%20analyze%0Athe%20impact%20of%20the%20number%20of%20folds%20by%20considering%203-%2C%205-%2C%20and%2010-folds.%20In%0Aaddition%2C%20we%20investigate%20the%20impact%20of%20early%20stopping%20with%20Bayesian%0Aoptimization%20instead%20of%20random%20search%20and%20also%20repeated%20cross-validation.%20Our%0Aexploratory%20study%20shows%20that%20even%20a%20simple-to-understand%20and%20easy-to-implement%0Amethod%20consistently%20allows%20model%20selection%20to%20converge%20faster%3B%20in%20~94%25%20of%20all%0Adatasets%2C%20on%20average%20by%20~214%25.%20Moreover%2C%20stopping%20cross-validation%20enables%0Amodel%20selection%20to%20explore%20the%20search%20space%20more%20exhaustively%20by%20considering%0A%2B167%25%20configurations%20on%20average%20within%20one%20hour%2C%20while%20also%20obtaining%20better%0Aoverall%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03389v1&entry.124074799=Read"},
{"title": "On the Theory of Cross-Modality Distillation with Contrastive Learning", "author": "Hangyu Lin and Chen Liu and Chengming Xu and Zhengqi Gao and Yanwei Fu and Yuan Yao", "abstract": "  Cross-modality distillation arises as an important topic for data modalities\ncontaining limited knowledge such as depth maps and high-quality sketches. Such\ntechniques are of great importance, especially for memory and\nprivacy-restricted scenarios where labeled training data is generally\nunavailable. To solve the problem, existing label-free methods leverage a few\npairwise unlabeled data to distill the knowledge by aligning features or\nstatistics between the source and target modalities. For instance, one\ntypically aims to minimize the L2 distance or contrastive loss between the\nlearned features of pairs of samples in the source (e.g. image) and the target\n(e.g. sketch) modalities. However, most algorithms in this domain only focus on\nthe experimental results but lack theoretical insight. To bridge the gap\nbetween the theory and practical method of cross-modality distillation, we\nfirst formulate a general framework of cross-modality contrastive distillation\n(CMCD), built upon contrastive learning that leverages both positive and\nnegative correspondence, towards a better distillation of generalizable\nfeatures. Furthermore, we establish a thorough convergence analysis that\nreveals that the distance between source and target modalities significantly\nimpacts the test error on downstream tasks within the target modality which is\nalso validated by the empirical results. Extensive experimental results show\nthat our algorithm outperforms existing algorithms consistently by a margin of\n2-3\\% across diverse modalities and tasks, covering modalities of image,\nsketch, depth map, and audio and tasks of recognition and segmentation.\n", "link": "http://arxiv.org/abs/2405.03355v1", "date": "2024-05-06", "relevancy": 2.2732, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6002}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5599}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Theory%20of%20Cross-Modality%20Distillation%20with%20Contrastive%20Learning&body=Title%3A%20On%20the%20Theory%20of%20Cross-Modality%20Distillation%20with%20Contrastive%20Learning%0AAuthor%3A%20Hangyu%20Lin%20and%20Chen%20Liu%20and%20Chengming%20Xu%20and%20Zhengqi%20Gao%20and%20Yanwei%20Fu%20and%20Yuan%20Yao%0AAbstract%3A%20%20%20Cross-modality%20distillation%20arises%20as%20an%20important%20topic%20for%20data%20modalities%0Acontaining%20limited%20knowledge%20such%20as%20depth%20maps%20and%20high-quality%20sketches.%20Such%0Atechniques%20are%20of%20great%20importance%2C%20especially%20for%20memory%20and%0Aprivacy-restricted%20scenarios%20where%20labeled%20training%20data%20is%20generally%0Aunavailable.%20To%20solve%20the%20problem%2C%20existing%20label-free%20methods%20leverage%20a%20few%0Apairwise%20unlabeled%20data%20to%20distill%20the%20knowledge%20by%20aligning%20features%20or%0Astatistics%20between%20the%20source%20and%20target%20modalities.%20For%20instance%2C%20one%0Atypically%20aims%20to%20minimize%20the%20L2%20distance%20or%20contrastive%20loss%20between%20the%0Alearned%20features%20of%20pairs%20of%20samples%20in%20the%20source%20%28e.g.%20image%29%20and%20the%20target%0A%28e.g.%20sketch%29%20modalities.%20However%2C%20most%20algorithms%20in%20this%20domain%20only%20focus%20on%0Athe%20experimental%20results%20but%20lack%20theoretical%20insight.%20To%20bridge%20the%20gap%0Abetween%20the%20theory%20and%20practical%20method%20of%20cross-modality%20distillation%2C%20we%0Afirst%20formulate%20a%20general%20framework%20of%20cross-modality%20contrastive%20distillation%0A%28CMCD%29%2C%20built%20upon%20contrastive%20learning%20that%20leverages%20both%20positive%20and%0Anegative%20correspondence%2C%20towards%20a%20better%20distillation%20of%20generalizable%0Afeatures.%20Furthermore%2C%20we%20establish%20a%20thorough%20convergence%20analysis%20that%0Areveals%20that%20the%20distance%20between%20source%20and%20target%20modalities%20significantly%0Aimpacts%20the%20test%20error%20on%20downstream%20tasks%20within%20the%20target%20modality%20which%20is%0Aalso%20validated%20by%20the%20empirical%20results.%20Extensive%20experimental%20results%20show%0Athat%20our%20algorithm%20outperforms%20existing%20algorithms%20consistently%20by%20a%20margin%20of%0A2-3%5C%25%20across%20diverse%20modalities%20and%20tasks%2C%20covering%20modalities%20of%20image%2C%0Asketch%2C%20depth%20map%2C%20and%20audio%20and%20tasks%20of%20recognition%20and%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Theory%2520of%2520Cross-Modality%2520Distillation%2520with%2520Contrastive%2520Learning%26entry.906535625%3DHangyu%2520Lin%2520and%2520Chen%2520Liu%2520and%2520Chengming%2520Xu%2520and%2520Zhengqi%2520Gao%2520and%2520Yanwei%2520Fu%2520and%2520Yuan%2520Yao%26entry.1292438233%3D%2520%2520Cross-modality%2520distillation%2520arises%2520as%2520an%2520important%2520topic%2520for%2520data%2520modalities%250Acontaining%2520limited%2520knowledge%2520such%2520as%2520depth%2520maps%2520and%2520high-quality%2520sketches.%2520Such%250Atechniques%2520are%2520of%2520great%2520importance%252C%2520especially%2520for%2520memory%2520and%250Aprivacy-restricted%2520scenarios%2520where%2520labeled%2520training%2520data%2520is%2520generally%250Aunavailable.%2520To%2520solve%2520the%2520problem%252C%2520existing%2520label-free%2520methods%2520leverage%2520a%2520few%250Apairwise%2520unlabeled%2520data%2520to%2520distill%2520the%2520knowledge%2520by%2520aligning%2520features%2520or%250Astatistics%2520between%2520the%2520source%2520and%2520target%2520modalities.%2520For%2520instance%252C%2520one%250Atypically%2520aims%2520to%2520minimize%2520the%2520L2%2520distance%2520or%2520contrastive%2520loss%2520between%2520the%250Alearned%2520features%2520of%2520pairs%2520of%2520samples%2520in%2520the%2520source%2520%2528e.g.%2520image%2529%2520and%2520the%2520target%250A%2528e.g.%2520sketch%2529%2520modalities.%2520However%252C%2520most%2520algorithms%2520in%2520this%2520domain%2520only%2520focus%2520on%250Athe%2520experimental%2520results%2520but%2520lack%2520theoretical%2520insight.%2520To%2520bridge%2520the%2520gap%250Abetween%2520the%2520theory%2520and%2520practical%2520method%2520of%2520cross-modality%2520distillation%252C%2520we%250Afirst%2520formulate%2520a%2520general%2520framework%2520of%2520cross-modality%2520contrastive%2520distillation%250A%2528CMCD%2529%252C%2520built%2520upon%2520contrastive%2520learning%2520that%2520leverages%2520both%2520positive%2520and%250Anegative%2520correspondence%252C%2520towards%2520a%2520better%2520distillation%2520of%2520generalizable%250Afeatures.%2520Furthermore%252C%2520we%2520establish%2520a%2520thorough%2520convergence%2520analysis%2520that%250Areveals%2520that%2520the%2520distance%2520between%2520source%2520and%2520target%2520modalities%2520significantly%250Aimpacts%2520the%2520test%2520error%2520on%2520downstream%2520tasks%2520within%2520the%2520target%2520modality%2520which%2520is%250Aalso%2520validated%2520by%2520the%2520empirical%2520results.%2520Extensive%2520experimental%2520results%2520show%250Athat%2520our%2520algorithm%2520outperforms%2520existing%2520algorithms%2520consistently%2520by%2520a%2520margin%2520of%250A2-3%255C%2525%2520across%2520diverse%2520modalities%2520and%2520tasks%252C%2520covering%2520modalities%2520of%2520image%252C%250Asketch%252C%2520depth%2520map%252C%2520and%2520audio%2520and%2520tasks%2520of%2520recognition%2520and%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Theory%20of%20Cross-Modality%20Distillation%20with%20Contrastive%20Learning&entry.906535625=Hangyu%20Lin%20and%20Chen%20Liu%20and%20Chengming%20Xu%20and%20Zhengqi%20Gao%20and%20Yanwei%20Fu%20and%20Yuan%20Yao&entry.1292438233=%20%20Cross-modality%20distillation%20arises%20as%20an%20important%20topic%20for%20data%20modalities%0Acontaining%20limited%20knowledge%20such%20as%20depth%20maps%20and%20high-quality%20sketches.%20Such%0Atechniques%20are%20of%20great%20importance%2C%20especially%20for%20memory%20and%0Aprivacy-restricted%20scenarios%20where%20labeled%20training%20data%20is%20generally%0Aunavailable.%20To%20solve%20the%20problem%2C%20existing%20label-free%20methods%20leverage%20a%20few%0Apairwise%20unlabeled%20data%20to%20distill%20the%20knowledge%20by%20aligning%20features%20or%0Astatistics%20between%20the%20source%20and%20target%20modalities.%20For%20instance%2C%20one%0Atypically%20aims%20to%20minimize%20the%20L2%20distance%20or%20contrastive%20loss%20between%20the%0Alearned%20features%20of%20pairs%20of%20samples%20in%20the%20source%20%28e.g.%20image%29%20and%20the%20target%0A%28e.g.%20sketch%29%20modalities.%20However%2C%20most%20algorithms%20in%20this%20domain%20only%20focus%20on%0Athe%20experimental%20results%20but%20lack%20theoretical%20insight.%20To%20bridge%20the%20gap%0Abetween%20the%20theory%20and%20practical%20method%20of%20cross-modality%20distillation%2C%20we%0Afirst%20formulate%20a%20general%20framework%20of%20cross-modality%20contrastive%20distillation%0A%28CMCD%29%2C%20built%20upon%20contrastive%20learning%20that%20leverages%20both%20positive%20and%0Anegative%20correspondence%2C%20towards%20a%20better%20distillation%20of%20generalizable%0Afeatures.%20Furthermore%2C%20we%20establish%20a%20thorough%20convergence%20analysis%20that%0Areveals%20that%20the%20distance%20between%20source%20and%20target%20modalities%20significantly%0Aimpacts%20the%20test%20error%20on%20downstream%20tasks%20within%20the%20target%20modality%20which%20is%0Aalso%20validated%20by%20the%20empirical%20results.%20Extensive%20experimental%20results%20show%0Athat%20our%20algorithm%20outperforms%20existing%20algorithms%20consistently%20by%20a%20margin%20of%0A2-3%5C%25%20across%20diverse%20modalities%20and%20tasks%2C%20covering%20modalities%20of%20image%2C%0Asketch%2C%20depth%20map%2C%20and%20audio%20and%20tasks%20of%20recognition%20and%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03355v1&entry.124074799=Read"},
{"title": "Graph Convolutional Neural Networks Sensitivity under Probabilistic\n  Error Model", "author": "Xinjue Wang and Esa Ollila and Sergiy A. Vorobyov", "abstract": "  Graph Neural Networks (GNNs), particularly Graph Convolutional Neural\nNetworks (GCNNs), have emerged as pivotal instruments in machine learning and\nsignal processing for processing graph-structured data. This paper proposes an\nanalysis framework to investigate the sensitivity of GCNNs to probabilistic\ngraph perturbations, directly impacting the graph shift operator (GSO). Our\nstudy establishes tight expected GSO error bounds, which are explicitly linked\nto the error model parameters, and reveals a linear relationship between GSO\nperturbations and the resulting output differences at each layer of GCNNs. This\nlinearity demonstrates that a single-layer GCNN maintains stability under graph\nedge perturbations, provided that the GSO errors remain bounded, regardless of\nthe perturbation scale. For multilayer GCNNs, the dependency of system's output\ndifference on GSO perturbations is shown to be a recursion of linearity.\nFinally, we exemplify the framework with the Graph Isomorphism Network (GIN)\nand Simple Graph Convolution Network (SGCN). Experiments validate our\ntheoretical derivations and the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2203.07831v4", "date": "2024-05-06", "relevancy": 2.2709, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4609}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4538}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Convolutional%20Neural%20Networks%20Sensitivity%20under%20Probabilistic%0A%20%20Error%20Model&body=Title%3A%20Graph%20Convolutional%20Neural%20Networks%20Sensitivity%20under%20Probabilistic%0A%20%20Error%20Model%0AAuthor%3A%20Xinjue%20Wang%20and%20Esa%20Ollila%20and%20Sergiy%20A.%20Vorobyov%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20particularly%20Graph%20Convolutional%20Neural%0ANetworks%20%28GCNNs%29%2C%20have%20emerged%20as%20pivotal%20instruments%20in%20machine%20learning%20and%0Asignal%20processing%20for%20processing%20graph-structured%20data.%20This%20paper%20proposes%20an%0Aanalysis%20framework%20to%20investigate%20the%20sensitivity%20of%20GCNNs%20to%20probabilistic%0Agraph%20perturbations%2C%20directly%20impacting%20the%20graph%20shift%20operator%20%28GSO%29.%20Our%0Astudy%20establishes%20tight%20expected%20GSO%20error%20bounds%2C%20which%20are%20explicitly%20linked%0Ato%20the%20error%20model%20parameters%2C%20and%20reveals%20a%20linear%20relationship%20between%20GSO%0Aperturbations%20and%20the%20resulting%20output%20differences%20at%20each%20layer%20of%20GCNNs.%20This%0Alinearity%20demonstrates%20that%20a%20single-layer%20GCNN%20maintains%20stability%20under%20graph%0Aedge%20perturbations%2C%20provided%20that%20the%20GSO%20errors%20remain%20bounded%2C%20regardless%20of%0Athe%20perturbation%20scale.%20For%20multilayer%20GCNNs%2C%20the%20dependency%20of%20system%27s%20output%0Adifference%20on%20GSO%20perturbations%20is%20shown%20to%20be%20a%20recursion%20of%20linearity.%0AFinally%2C%20we%20exemplify%20the%20framework%20with%20the%20Graph%20Isomorphism%20Network%20%28GIN%29%0Aand%20Simple%20Graph%20Convolution%20Network%20%28SGCN%29.%20Experiments%20validate%20our%0Atheoretical%20derivations%20and%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.07831v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Convolutional%2520Neural%2520Networks%2520Sensitivity%2520under%2520Probabilistic%250A%2520%2520Error%2520Model%26entry.906535625%3DXinjue%2520Wang%2520and%2520Esa%2520Ollila%2520and%2520Sergiy%2520A.%2520Vorobyov%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520particularly%2520Graph%2520Convolutional%2520Neural%250ANetworks%2520%2528GCNNs%2529%252C%2520have%2520emerged%2520as%2520pivotal%2520instruments%2520in%2520machine%2520learning%2520and%250Asignal%2520processing%2520for%2520processing%2520graph-structured%2520data.%2520This%2520paper%2520proposes%2520an%250Aanalysis%2520framework%2520to%2520investigate%2520the%2520sensitivity%2520of%2520GCNNs%2520to%2520probabilistic%250Agraph%2520perturbations%252C%2520directly%2520impacting%2520the%2520graph%2520shift%2520operator%2520%2528GSO%2529.%2520Our%250Astudy%2520establishes%2520tight%2520expected%2520GSO%2520error%2520bounds%252C%2520which%2520are%2520explicitly%2520linked%250Ato%2520the%2520error%2520model%2520parameters%252C%2520and%2520reveals%2520a%2520linear%2520relationship%2520between%2520GSO%250Aperturbations%2520and%2520the%2520resulting%2520output%2520differences%2520at%2520each%2520layer%2520of%2520GCNNs.%2520This%250Alinearity%2520demonstrates%2520that%2520a%2520single-layer%2520GCNN%2520maintains%2520stability%2520under%2520graph%250Aedge%2520perturbations%252C%2520provided%2520that%2520the%2520GSO%2520errors%2520remain%2520bounded%252C%2520regardless%2520of%250Athe%2520perturbation%2520scale.%2520For%2520multilayer%2520GCNNs%252C%2520the%2520dependency%2520of%2520system%2527s%2520output%250Adifference%2520on%2520GSO%2520perturbations%2520is%2520shown%2520to%2520be%2520a%2520recursion%2520of%2520linearity.%250AFinally%252C%2520we%2520exemplify%2520the%2520framework%2520with%2520the%2520Graph%2520Isomorphism%2520Network%2520%2528GIN%2529%250Aand%2520Simple%2520Graph%2520Convolution%2520Network%2520%2528SGCN%2529.%2520Experiments%2520validate%2520our%250Atheoretical%2520derivations%2520and%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2203.07831v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Convolutional%20Neural%20Networks%20Sensitivity%20under%20Probabilistic%0A%20%20Error%20Model&entry.906535625=Xinjue%20Wang%20and%20Esa%20Ollila%20and%20Sergiy%20A.%20Vorobyov&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20particularly%20Graph%20Convolutional%20Neural%0ANetworks%20%28GCNNs%29%2C%20have%20emerged%20as%20pivotal%20instruments%20in%20machine%20learning%20and%0Asignal%20processing%20for%20processing%20graph-structured%20data.%20This%20paper%20proposes%20an%0Aanalysis%20framework%20to%20investigate%20the%20sensitivity%20of%20GCNNs%20to%20probabilistic%0Agraph%20perturbations%2C%20directly%20impacting%20the%20graph%20shift%20operator%20%28GSO%29.%20Our%0Astudy%20establishes%20tight%20expected%20GSO%20error%20bounds%2C%20which%20are%20explicitly%20linked%0Ato%20the%20error%20model%20parameters%2C%20and%20reveals%20a%20linear%20relationship%20between%20GSO%0Aperturbations%20and%20the%20resulting%20output%20differences%20at%20each%20layer%20of%20GCNNs.%20This%0Alinearity%20demonstrates%20that%20a%20single-layer%20GCNN%20maintains%20stability%20under%20graph%0Aedge%20perturbations%2C%20provided%20that%20the%20GSO%20errors%20remain%20bounded%2C%20regardless%20of%0Athe%20perturbation%20scale.%20For%20multilayer%20GCNNs%2C%20the%20dependency%20of%20system%27s%20output%0Adifference%20on%20GSO%20perturbations%20is%20shown%20to%20be%20a%20recursion%20of%20linearity.%0AFinally%2C%20we%20exemplify%20the%20framework%20with%20the%20Graph%20Isomorphism%20Network%20%28GIN%29%0Aand%20Simple%20Graph%20Convolution%20Network%20%28SGCN%29.%20Experiments%20validate%20our%0Atheoretical%20derivations%20and%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.07831v4&entry.124074799=Read"},
{"title": "LGTM: Local-to-Global Text-Driven Human Motion Diffusion Model", "author": "Haowen Sun and Ruikun Zheng and Haibin Huang and Chongyang Ma and Hui Huang and Ruizhen Hu", "abstract": "  In this paper, we introduce LGTM, a novel Local-to-Global pipeline for\nText-to-Motion generation. LGTM utilizes a diffusion-based architecture and\naims to address the challenge of accurately translating textual descriptions\ninto semantically coherent human motion in computer animation. Specifically,\ntraditional methods often struggle with semantic discrepancies, particularly in\naligning specific motions to the correct body parts. To address this issue, we\npropose a two-stage pipeline to overcome this challenge: it first employs large\nlanguage models (LLMs) to decompose global motion descriptions into\npart-specific narratives, which are then processed by independent body-part\nmotion encoders to ensure precise local semantic alignment. Finally, an\nattention-based full-body optimizer refines the motion generation results and\nguarantees the overall coherence. Our experiments demonstrate that LGTM gains\nsignificant improvements in generating locally accurate, semantically-aligned\nhuman motion, marking a notable advancement in text-to-motion applications.\nCode and data for this paper are available at https://github.com/L-Sun/LGTM\n", "link": "http://arxiv.org/abs/2405.03485v1", "date": "2024-05-06", "relevancy": 2.2392, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6294}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5494}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LGTM%3A%20Local-to-Global%20Text-Driven%20Human%20Motion%20Diffusion%20Model&body=Title%3A%20LGTM%3A%20Local-to-Global%20Text-Driven%20Human%20Motion%20Diffusion%20Model%0AAuthor%3A%20Haowen%20Sun%20and%20Ruikun%20Zheng%20and%20Haibin%20Huang%20and%20Chongyang%20Ma%20and%20Hui%20Huang%20and%20Ruizhen%20Hu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20LGTM%2C%20a%20novel%20Local-to-Global%20pipeline%20for%0AText-to-Motion%20generation.%20LGTM%20utilizes%20a%20diffusion-based%20architecture%20and%0Aaims%20to%20address%20the%20challenge%20of%20accurately%20translating%20textual%20descriptions%0Ainto%20semantically%20coherent%20human%20motion%20in%20computer%20animation.%20Specifically%2C%0Atraditional%20methods%20often%20struggle%20with%20semantic%20discrepancies%2C%20particularly%20in%0Aaligning%20specific%20motions%20to%20the%20correct%20body%20parts.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20two-stage%20pipeline%20to%20overcome%20this%20challenge%3A%20it%20first%20employs%20large%0Alanguage%20models%20%28LLMs%29%20to%20decompose%20global%20motion%20descriptions%20into%0Apart-specific%20narratives%2C%20which%20are%20then%20processed%20by%20independent%20body-part%0Amotion%20encoders%20to%20ensure%20precise%20local%20semantic%20alignment.%20Finally%2C%20an%0Aattention-based%20full-body%20optimizer%20refines%20the%20motion%20generation%20results%20and%0Aguarantees%20the%20overall%20coherence.%20Our%20experiments%20demonstrate%20that%20LGTM%20gains%0Asignificant%20improvements%20in%20generating%20locally%20accurate%2C%20semantically-aligned%0Ahuman%20motion%2C%20marking%20a%20notable%20advancement%20in%20text-to-motion%20applications.%0ACode%20and%20data%20for%20this%20paper%20are%20available%20at%20https%3A//github.com/L-Sun/LGTM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLGTM%253A%2520Local-to-Global%2520Text-Driven%2520Human%2520Motion%2520Diffusion%2520Model%26entry.906535625%3DHaowen%2520Sun%2520and%2520Ruikun%2520Zheng%2520and%2520Haibin%2520Huang%2520and%2520Chongyang%2520Ma%2520and%2520Hui%2520Huang%2520and%2520Ruizhen%2520Hu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520LGTM%252C%2520a%2520novel%2520Local-to-Global%2520pipeline%2520for%250AText-to-Motion%2520generation.%2520LGTM%2520utilizes%2520a%2520diffusion-based%2520architecture%2520and%250Aaims%2520to%2520address%2520the%2520challenge%2520of%2520accurately%2520translating%2520textual%2520descriptions%250Ainto%2520semantically%2520coherent%2520human%2520motion%2520in%2520computer%2520animation.%2520Specifically%252C%250Atraditional%2520methods%2520often%2520struggle%2520with%2520semantic%2520discrepancies%252C%2520particularly%2520in%250Aaligning%2520specific%2520motions%2520to%2520the%2520correct%2520body%2520parts.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520a%2520two-stage%2520pipeline%2520to%2520overcome%2520this%2520challenge%253A%2520it%2520first%2520employs%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520decompose%2520global%2520motion%2520descriptions%2520into%250Apart-specific%2520narratives%252C%2520which%2520are%2520then%2520processed%2520by%2520independent%2520body-part%250Amotion%2520encoders%2520to%2520ensure%2520precise%2520local%2520semantic%2520alignment.%2520Finally%252C%2520an%250Aattention-based%2520full-body%2520optimizer%2520refines%2520the%2520motion%2520generation%2520results%2520and%250Aguarantees%2520the%2520overall%2520coherence.%2520Our%2520experiments%2520demonstrate%2520that%2520LGTM%2520gains%250Asignificant%2520improvements%2520in%2520generating%2520locally%2520accurate%252C%2520semantically-aligned%250Ahuman%2520motion%252C%2520marking%2520a%2520notable%2520advancement%2520in%2520text-to-motion%2520applications.%250ACode%2520and%2520data%2520for%2520this%2520paper%2520are%2520available%2520at%2520https%253A//github.com/L-Sun/LGTM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LGTM%3A%20Local-to-Global%20Text-Driven%20Human%20Motion%20Diffusion%20Model&entry.906535625=Haowen%20Sun%20and%20Ruikun%20Zheng%20and%20Haibin%20Huang%20and%20Chongyang%20Ma%20and%20Hui%20Huang%20and%20Ruizhen%20Hu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20LGTM%2C%20a%20novel%20Local-to-Global%20pipeline%20for%0AText-to-Motion%20generation.%20LGTM%20utilizes%20a%20diffusion-based%20architecture%20and%0Aaims%20to%20address%20the%20challenge%20of%20accurately%20translating%20textual%20descriptions%0Ainto%20semantically%20coherent%20human%20motion%20in%20computer%20animation.%20Specifically%2C%0Atraditional%20methods%20often%20struggle%20with%20semantic%20discrepancies%2C%20particularly%20in%0Aaligning%20specific%20motions%20to%20the%20correct%20body%20parts.%20To%20address%20this%20issue%2C%20we%0Apropose%20a%20two-stage%20pipeline%20to%20overcome%20this%20challenge%3A%20it%20first%20employs%20large%0Alanguage%20models%20%28LLMs%29%20to%20decompose%20global%20motion%20descriptions%20into%0Apart-specific%20narratives%2C%20which%20are%20then%20processed%20by%20independent%20body-part%0Amotion%20encoders%20to%20ensure%20precise%20local%20semantic%20alignment.%20Finally%2C%20an%0Aattention-based%20full-body%20optimizer%20refines%20the%20motion%20generation%20results%20and%0Aguarantees%20the%20overall%20coherence.%20Our%20experiments%20demonstrate%20that%20LGTM%20gains%0Asignificant%20improvements%20in%20generating%20locally%20accurate%2C%20semantically-aligned%0Ahuman%20motion%2C%20marking%20a%20notable%20advancement%20in%20text-to-motion%20applications.%0ACode%20and%20data%20for%20this%20paper%20are%20available%20at%20https%3A//github.com/L-Sun/LGTM%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03485v1&entry.124074799=Read"},
{"title": "Towards a Safe Real-Time Motion Planning Framework for Autonomous\n  Driving Systems: An MPPI Approach", "author": "Mehdi Testouri and Gamal Elghazaly and Raphael Frank", "abstract": "  Planning safe trajectories in Autonomous Driving Systems (ADS) is a complex\nproblem to solve in real-time. The main challenge to solve this problem arises\nfrom the various conditions and constraints imposed by road geometry, semantics\nand traffic rules, as well as the presence of dynamic agents. Recently, Model\nPredictive Path Integral (MPPI) has shown to be an effective framework for\noptimal motion planning and control in robot navigation in unstructured and\nhighly uncertain environments. In this paper, we formulate the motion planning\nproblem in ADS as a nonlinear stochastic dynamic optimization problem that can\nbe solved using an MPPI strategy. The main technical contribution of this work\nis a method to handle obstacles within the MPPI formulation safely. In this\nmethod, obstacles are approximated by circles that can be easily integrated\ninto the MPPI cost formulation while considering safety margins. The proposed\nMPPI framework has been efficiently implemented in our autonomous vehicle and\nexperimentally validated using three different primitive scenarios.\nExperimental results show that generated trajectories are safe, feasible and\nperfectly achieve the planning objective. The video results as well as the\nopen-source implementation are available at:\nhttps://gitlab.uni.lu/360lab-public/mppi\n", "link": "http://arxiv.org/abs/2308.01654v4", "date": "2024-05-06", "relevancy": 2.2237, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5664}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5631}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Safe%20Real-Time%20Motion%20Planning%20Framework%20for%20Autonomous%0A%20%20Driving%20Systems%3A%20An%20MPPI%20Approach&body=Title%3A%20Towards%20a%20Safe%20Real-Time%20Motion%20Planning%20Framework%20for%20Autonomous%0A%20%20Driving%20Systems%3A%20An%20MPPI%20Approach%0AAuthor%3A%20Mehdi%20Testouri%20and%20Gamal%20Elghazaly%20and%20Raphael%20Frank%0AAbstract%3A%20%20%20Planning%20safe%20trajectories%20in%20Autonomous%20Driving%20Systems%20%28ADS%29%20is%20a%20complex%0Aproblem%20to%20solve%20in%20real-time.%20The%20main%20challenge%20to%20solve%20this%20problem%20arises%0Afrom%20the%20various%20conditions%20and%20constraints%20imposed%20by%20road%20geometry%2C%20semantics%0Aand%20traffic%20rules%2C%20as%20well%20as%20the%20presence%20of%20dynamic%20agents.%20Recently%2C%20Model%0APredictive%20Path%20Integral%20%28MPPI%29%20has%20shown%20to%20be%20an%20effective%20framework%20for%0Aoptimal%20motion%20planning%20and%20control%20in%20robot%20navigation%20in%20unstructured%20and%0Ahighly%20uncertain%20environments.%20In%20this%20paper%2C%20we%20formulate%20the%20motion%20planning%0Aproblem%20in%20ADS%20as%20a%20nonlinear%20stochastic%20dynamic%20optimization%20problem%20that%20can%0Abe%20solved%20using%20an%20MPPI%20strategy.%20The%20main%20technical%20contribution%20of%20this%20work%0Ais%20a%20method%20to%20handle%20obstacles%20within%20the%20MPPI%20formulation%20safely.%20In%20this%0Amethod%2C%20obstacles%20are%20approximated%20by%20circles%20that%20can%20be%20easily%20integrated%0Ainto%20the%20MPPI%20cost%20formulation%20while%20considering%20safety%20margins.%20The%20proposed%0AMPPI%20framework%20has%20been%20efficiently%20implemented%20in%20our%20autonomous%20vehicle%20and%0Aexperimentally%20validated%20using%20three%20different%20primitive%20scenarios.%0AExperimental%20results%20show%20that%20generated%20trajectories%20are%20safe%2C%20feasible%20and%0Aperfectly%20achieve%20the%20planning%20objective.%20The%20video%20results%20as%20well%20as%20the%0Aopen-source%20implementation%20are%20available%20at%3A%0Ahttps%3A//gitlab.uni.lu/360lab-public/mppi%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.01654v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Safe%2520Real-Time%2520Motion%2520Planning%2520Framework%2520for%2520Autonomous%250A%2520%2520Driving%2520Systems%253A%2520An%2520MPPI%2520Approach%26entry.906535625%3DMehdi%2520Testouri%2520and%2520Gamal%2520Elghazaly%2520and%2520Raphael%2520Frank%26entry.1292438233%3D%2520%2520Planning%2520safe%2520trajectories%2520in%2520Autonomous%2520Driving%2520Systems%2520%2528ADS%2529%2520is%2520a%2520complex%250Aproblem%2520to%2520solve%2520in%2520real-time.%2520The%2520main%2520challenge%2520to%2520solve%2520this%2520problem%2520arises%250Afrom%2520the%2520various%2520conditions%2520and%2520constraints%2520imposed%2520by%2520road%2520geometry%252C%2520semantics%250Aand%2520traffic%2520rules%252C%2520as%2520well%2520as%2520the%2520presence%2520of%2520dynamic%2520agents.%2520Recently%252C%2520Model%250APredictive%2520Path%2520Integral%2520%2528MPPI%2529%2520has%2520shown%2520to%2520be%2520an%2520effective%2520framework%2520for%250Aoptimal%2520motion%2520planning%2520and%2520control%2520in%2520robot%2520navigation%2520in%2520unstructured%2520and%250Ahighly%2520uncertain%2520environments.%2520In%2520this%2520paper%252C%2520we%2520formulate%2520the%2520motion%2520planning%250Aproblem%2520in%2520ADS%2520as%2520a%2520nonlinear%2520stochastic%2520dynamic%2520optimization%2520problem%2520that%2520can%250Abe%2520solved%2520using%2520an%2520MPPI%2520strategy.%2520The%2520main%2520technical%2520contribution%2520of%2520this%2520work%250Ais%2520a%2520method%2520to%2520handle%2520obstacles%2520within%2520the%2520MPPI%2520formulation%2520safely.%2520In%2520this%250Amethod%252C%2520obstacles%2520are%2520approximated%2520by%2520circles%2520that%2520can%2520be%2520easily%2520integrated%250Ainto%2520the%2520MPPI%2520cost%2520formulation%2520while%2520considering%2520safety%2520margins.%2520The%2520proposed%250AMPPI%2520framework%2520has%2520been%2520efficiently%2520implemented%2520in%2520our%2520autonomous%2520vehicle%2520and%250Aexperimentally%2520validated%2520using%2520three%2520different%2520primitive%2520scenarios.%250AExperimental%2520results%2520show%2520that%2520generated%2520trajectories%2520are%2520safe%252C%2520feasible%2520and%250Aperfectly%2520achieve%2520the%2520planning%2520objective.%2520The%2520video%2520results%2520as%2520well%2520as%2520the%250Aopen-source%2520implementation%2520are%2520available%2520at%253A%250Ahttps%253A//gitlab.uni.lu/360lab-public/mppi%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.01654v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Safe%20Real-Time%20Motion%20Planning%20Framework%20for%20Autonomous%0A%20%20Driving%20Systems%3A%20An%20MPPI%20Approach&entry.906535625=Mehdi%20Testouri%20and%20Gamal%20Elghazaly%20and%20Raphael%20Frank&entry.1292438233=%20%20Planning%20safe%20trajectories%20in%20Autonomous%20Driving%20Systems%20%28ADS%29%20is%20a%20complex%0Aproblem%20to%20solve%20in%20real-time.%20The%20main%20challenge%20to%20solve%20this%20problem%20arises%0Afrom%20the%20various%20conditions%20and%20constraints%20imposed%20by%20road%20geometry%2C%20semantics%0Aand%20traffic%20rules%2C%20as%20well%20as%20the%20presence%20of%20dynamic%20agents.%20Recently%2C%20Model%0APredictive%20Path%20Integral%20%28MPPI%29%20has%20shown%20to%20be%20an%20effective%20framework%20for%0Aoptimal%20motion%20planning%20and%20control%20in%20robot%20navigation%20in%20unstructured%20and%0Ahighly%20uncertain%20environments.%20In%20this%20paper%2C%20we%20formulate%20the%20motion%20planning%0Aproblem%20in%20ADS%20as%20a%20nonlinear%20stochastic%20dynamic%20optimization%20problem%20that%20can%0Abe%20solved%20using%20an%20MPPI%20strategy.%20The%20main%20technical%20contribution%20of%20this%20work%0Ais%20a%20method%20to%20handle%20obstacles%20within%20the%20MPPI%20formulation%20safely.%20In%20this%0Amethod%2C%20obstacles%20are%20approximated%20by%20circles%20that%20can%20be%20easily%20integrated%0Ainto%20the%20MPPI%20cost%20formulation%20while%20considering%20safety%20margins.%20The%20proposed%0AMPPI%20framework%20has%20been%20efficiently%20implemented%20in%20our%20autonomous%20vehicle%20and%0Aexperimentally%20validated%20using%20three%20different%20primitive%20scenarios.%0AExperimental%20results%20show%20that%20generated%20trajectories%20are%20safe%2C%20feasible%20and%0Aperfectly%20achieve%20the%20planning%20objective.%20The%20video%20results%20as%20well%20as%20the%0Aopen-source%20implementation%20are%20available%20at%3A%0Ahttps%3A//gitlab.uni.lu/360lab-public/mppi%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.01654v4&entry.124074799=Read"},
{"title": "Task-conditioned adaptation of visual features in multi-task policy\n  learning", "author": "Pierre Marza and Laetitia Matignon and Olivier Simonin and Christian Wolf", "abstract": "  Successfully addressing a wide variety of tasks is a core ability of\nautonomous agents, requiring flexibly adapting the underlying decision-making\nstrategies and, as we argue in this work, also adapting the perception modules.\nAn analogical argument would be the human visual system, which uses top-down\nsignals to focus attention determined by the current task. Similarly, we adapt\npre-trained large vision models conditioned on specific downstream tasks in the\ncontext of multi-task policy learning. We introduce task-conditioned adapters\nthat do not require finetuning any pre-trained weights, combined with a single\npolicy trained with behavior cloning and capable of addressing multiple tasks.\nWe condition the visual adapters on task embeddings, which can be selected at\ninference if the task is known, or alternatively inferred from a set of example\ndemonstrations. To this end, we propose a new optimization-based estimator. We\nevaluate the method on a wide variety of tasks from the CortexBench benchmark\nand show that, compared to existing work, it can be addressed with a single\npolicy. In particular, we demonstrate that adapting visual features is a key\ndesign choice and that the method generalizes to unseen tasks given a few\ndemonstrations.\n", "link": "http://arxiv.org/abs/2402.07739v4", "date": "2024-05-06", "relevancy": 2.2044, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5814}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5707}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-conditioned%20adaptation%20of%20visual%20features%20in%20multi-task%20policy%0A%20%20learning&body=Title%3A%20Task-conditioned%20adaptation%20of%20visual%20features%20in%20multi-task%20policy%0A%20%20learning%0AAuthor%3A%20Pierre%20Marza%20and%20Laetitia%20Matignon%20and%20Olivier%20Simonin%20and%20Christian%20Wolf%0AAbstract%3A%20%20%20Successfully%20addressing%20a%20wide%20variety%20of%20tasks%20is%20a%20core%20ability%20of%0Aautonomous%20agents%2C%20requiring%20flexibly%20adapting%20the%20underlying%20decision-making%0Astrategies%20and%2C%20as%20we%20argue%20in%20this%20work%2C%20also%20adapting%20the%20perception%20modules.%0AAn%20analogical%20argument%20would%20be%20the%20human%20visual%20system%2C%20which%20uses%20top-down%0Asignals%20to%20focus%20attention%20determined%20by%20the%20current%20task.%20Similarly%2C%20we%20adapt%0Apre-trained%20large%20vision%20models%20conditioned%20on%20specific%20downstream%20tasks%20in%20the%0Acontext%20of%20multi-task%20policy%20learning.%20We%20introduce%20task-conditioned%20adapters%0Athat%20do%20not%20require%20finetuning%20any%20pre-trained%20weights%2C%20combined%20with%20a%20single%0Apolicy%20trained%20with%20behavior%20cloning%20and%20capable%20of%20addressing%20multiple%20tasks.%0AWe%20condition%20the%20visual%20adapters%20on%20task%20embeddings%2C%20which%20can%20be%20selected%20at%0Ainference%20if%20the%20task%20is%20known%2C%20or%20alternatively%20inferred%20from%20a%20set%20of%20example%0Ademonstrations.%20To%20this%20end%2C%20we%20propose%20a%20new%20optimization-based%20estimator.%20We%0Aevaluate%20the%20method%20on%20a%20wide%20variety%20of%20tasks%20from%20the%20CortexBench%20benchmark%0Aand%20show%20that%2C%20compared%20to%20existing%20work%2C%20it%20can%20be%20addressed%20with%20a%20single%0Apolicy.%20In%20particular%2C%20we%20demonstrate%20that%20adapting%20visual%20features%20is%20a%20key%0Adesign%20choice%20and%20that%20the%20method%20generalizes%20to%20unseen%20tasks%20given%20a%20few%0Ademonstrations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07739v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-conditioned%2520adaptation%2520of%2520visual%2520features%2520in%2520multi-task%2520policy%250A%2520%2520learning%26entry.906535625%3DPierre%2520Marza%2520and%2520Laetitia%2520Matignon%2520and%2520Olivier%2520Simonin%2520and%2520Christian%2520Wolf%26entry.1292438233%3D%2520%2520Successfully%2520addressing%2520a%2520wide%2520variety%2520of%2520tasks%2520is%2520a%2520core%2520ability%2520of%250Aautonomous%2520agents%252C%2520requiring%2520flexibly%2520adapting%2520the%2520underlying%2520decision-making%250Astrategies%2520and%252C%2520as%2520we%2520argue%2520in%2520this%2520work%252C%2520also%2520adapting%2520the%2520perception%2520modules.%250AAn%2520analogical%2520argument%2520would%2520be%2520the%2520human%2520visual%2520system%252C%2520which%2520uses%2520top-down%250Asignals%2520to%2520focus%2520attention%2520determined%2520by%2520the%2520current%2520task.%2520Similarly%252C%2520we%2520adapt%250Apre-trained%2520large%2520vision%2520models%2520conditioned%2520on%2520specific%2520downstream%2520tasks%2520in%2520the%250Acontext%2520of%2520multi-task%2520policy%2520learning.%2520We%2520introduce%2520task-conditioned%2520adapters%250Athat%2520do%2520not%2520require%2520finetuning%2520any%2520pre-trained%2520weights%252C%2520combined%2520with%2520a%2520single%250Apolicy%2520trained%2520with%2520behavior%2520cloning%2520and%2520capable%2520of%2520addressing%2520multiple%2520tasks.%250AWe%2520condition%2520the%2520visual%2520adapters%2520on%2520task%2520embeddings%252C%2520which%2520can%2520be%2520selected%2520at%250Ainference%2520if%2520the%2520task%2520is%2520known%252C%2520or%2520alternatively%2520inferred%2520from%2520a%2520set%2520of%2520example%250Ademonstrations.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520new%2520optimization-based%2520estimator.%2520We%250Aevaluate%2520the%2520method%2520on%2520a%2520wide%2520variety%2520of%2520tasks%2520from%2520the%2520CortexBench%2520benchmark%250Aand%2520show%2520that%252C%2520compared%2520to%2520existing%2520work%252C%2520it%2520can%2520be%2520addressed%2520with%2520a%2520single%250Apolicy.%2520In%2520particular%252C%2520we%2520demonstrate%2520that%2520adapting%2520visual%2520features%2520is%2520a%2520key%250Adesign%2520choice%2520and%2520that%2520the%2520method%2520generalizes%2520to%2520unseen%2520tasks%2520given%2520a%2520few%250Ademonstrations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07739v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-conditioned%20adaptation%20of%20visual%20features%20in%20multi-task%20policy%0A%20%20learning&entry.906535625=Pierre%20Marza%20and%20Laetitia%20Matignon%20and%20Olivier%20Simonin%20and%20Christian%20Wolf&entry.1292438233=%20%20Successfully%20addressing%20a%20wide%20variety%20of%20tasks%20is%20a%20core%20ability%20of%0Aautonomous%20agents%2C%20requiring%20flexibly%20adapting%20the%20underlying%20decision-making%0Astrategies%20and%2C%20as%20we%20argue%20in%20this%20work%2C%20also%20adapting%20the%20perception%20modules.%0AAn%20analogical%20argument%20would%20be%20the%20human%20visual%20system%2C%20which%20uses%20top-down%0Asignals%20to%20focus%20attention%20determined%20by%20the%20current%20task.%20Similarly%2C%20we%20adapt%0Apre-trained%20large%20vision%20models%20conditioned%20on%20specific%20downstream%20tasks%20in%20the%0Acontext%20of%20multi-task%20policy%20learning.%20We%20introduce%20task-conditioned%20adapters%0Athat%20do%20not%20require%20finetuning%20any%20pre-trained%20weights%2C%20combined%20with%20a%20single%0Apolicy%20trained%20with%20behavior%20cloning%20and%20capable%20of%20addressing%20multiple%20tasks.%0AWe%20condition%20the%20visual%20adapters%20on%20task%20embeddings%2C%20which%20can%20be%20selected%20at%0Ainference%20if%20the%20task%20is%20known%2C%20or%20alternatively%20inferred%20from%20a%20set%20of%20example%0Ademonstrations.%20To%20this%20end%2C%20we%20propose%20a%20new%20optimization-based%20estimator.%20We%0Aevaluate%20the%20method%20on%20a%20wide%20variety%20of%20tasks%20from%20the%20CortexBench%20benchmark%0Aand%20show%20that%2C%20compared%20to%20existing%20work%2C%20it%20can%20be%20addressed%20with%20a%20single%0Apolicy.%20In%20particular%2C%20we%20demonstrate%20that%20adapting%20visual%20features%20is%20a%20key%0Adesign%20choice%20and%20that%20the%20method%20generalizes%20to%20unseen%20tasks%20given%20a%20few%0Ademonstrations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07739v4&entry.124074799=Read"},
{"title": "GPLaSDI: Gaussian Process-based Interpretable Latent Space Dynamics\n  Identification through Deep Autoencoder", "author": "Christophe Bonneville and Youngsoo Choi and Debojyoti Ghosh and Jonathan L. Belof", "abstract": "  Numerically solving partial differential equations (PDEs) can be challenging\nand computationally expensive. This has led to the development of reduced-order\nmodels (ROMs) that are accurate but faster than full order models (FOMs).\nRecently, machine learning advances have enabled the creation of non-linear\nprojection methods, such as Latent Space Dynamics Identification (LaSDI). LaSDI\nmaps full-order PDE solutions to a latent space using autoencoders and learns\nthe system of ODEs governing the latent space dynamics. By interpolating and\nsolving the ODE system in the reduced latent space, fast and accurate ROM\npredictions can be made by feeding the predicted latent space dynamics into the\ndecoder. In this paper, we introduce GPLaSDI, a novel LaSDI-based framework\nthat relies on Gaussian process (GP) for latent space ODE interpolations. Using\nGPs offers two significant advantages. First, it enables the quantification of\nuncertainty over the ROM predictions. Second, leveraging this prediction\nuncertainty allows for efficient adaptive training through a greedy selection\nof additional training data points. This approach does not require prior\nknowledge of the underlying PDEs. Consequently, GPLaSDI is inherently\nnon-intrusive and can be applied to problems without a known PDE or its\nresidual. We demonstrate the effectiveness of our approach on the Burgers\nequation, Vlasov equation for plasma physics, and a rising thermal bubble\nproblem. Our proposed method achieves between 200 and 100,000 times speed-up,\nwith up to 7% relative error.\n", "link": "http://arxiv.org/abs/2308.05882v2", "date": "2024-05-06", "relevancy": 2.2028, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5663}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5479}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPLaSDI%3A%20Gaussian%20Process-based%20Interpretable%20Latent%20Space%20Dynamics%0A%20%20Identification%20through%20Deep%20Autoencoder&body=Title%3A%20GPLaSDI%3A%20Gaussian%20Process-based%20Interpretable%20Latent%20Space%20Dynamics%0A%20%20Identification%20through%20Deep%20Autoencoder%0AAuthor%3A%20Christophe%20Bonneville%20and%20Youngsoo%20Choi%20and%20Debojyoti%20Ghosh%20and%20Jonathan%20L.%20Belof%0AAbstract%3A%20%20%20Numerically%20solving%20partial%20differential%20equations%20%28PDEs%29%20can%20be%20challenging%0Aand%20computationally%20expensive.%20This%20has%20led%20to%20the%20development%20of%20reduced-order%0Amodels%20%28ROMs%29%20that%20are%20accurate%20but%20faster%20than%20full%20order%20models%20%28FOMs%29.%0ARecently%2C%20machine%20learning%20advances%20have%20enabled%20the%20creation%20of%20non-linear%0Aprojection%20methods%2C%20such%20as%20Latent%20Space%20Dynamics%20Identification%20%28LaSDI%29.%20LaSDI%0Amaps%20full-order%20PDE%20solutions%20to%20a%20latent%20space%20using%20autoencoders%20and%20learns%0Athe%20system%20of%20ODEs%20governing%20the%20latent%20space%20dynamics.%20By%20interpolating%20and%0Asolving%20the%20ODE%20system%20in%20the%20reduced%20latent%20space%2C%20fast%20and%20accurate%20ROM%0Apredictions%20can%20be%20made%20by%20feeding%20the%20predicted%20latent%20space%20dynamics%20into%20the%0Adecoder.%20In%20this%20paper%2C%20we%20introduce%20GPLaSDI%2C%20a%20novel%20LaSDI-based%20framework%0Athat%20relies%20on%20Gaussian%20process%20%28GP%29%20for%20latent%20space%20ODE%20interpolations.%20Using%0AGPs%20offers%20two%20significant%20advantages.%20First%2C%20it%20enables%20the%20quantification%20of%0Auncertainty%20over%20the%20ROM%20predictions.%20Second%2C%20leveraging%20this%20prediction%0Auncertainty%20allows%20for%20efficient%20adaptive%20training%20through%20a%20greedy%20selection%0Aof%20additional%20training%20data%20points.%20This%20approach%20does%20not%20require%20prior%0Aknowledge%20of%20the%20underlying%20PDEs.%20Consequently%2C%20GPLaSDI%20is%20inherently%0Anon-intrusive%20and%20can%20be%20applied%20to%20problems%20without%20a%20known%20PDE%20or%20its%0Aresidual.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%20the%20Burgers%0Aequation%2C%20Vlasov%20equation%20for%20plasma%20physics%2C%20and%20a%20rising%20thermal%20bubble%0Aproblem.%20Our%20proposed%20method%20achieves%20between%20200%20and%20100%2C000%20times%20speed-up%2C%0Awith%20up%20to%207%25%20relative%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.05882v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPLaSDI%253A%2520Gaussian%2520Process-based%2520Interpretable%2520Latent%2520Space%2520Dynamics%250A%2520%2520Identification%2520through%2520Deep%2520Autoencoder%26entry.906535625%3DChristophe%2520Bonneville%2520and%2520Youngsoo%2520Choi%2520and%2520Debojyoti%2520Ghosh%2520and%2520Jonathan%2520L.%2520Belof%26entry.1292438233%3D%2520%2520Numerically%2520solving%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520can%2520be%2520challenging%250Aand%2520computationally%2520expensive.%2520This%2520has%2520led%2520to%2520the%2520development%2520of%2520reduced-order%250Amodels%2520%2528ROMs%2529%2520that%2520are%2520accurate%2520but%2520faster%2520than%2520full%2520order%2520models%2520%2528FOMs%2529.%250ARecently%252C%2520machine%2520learning%2520advances%2520have%2520enabled%2520the%2520creation%2520of%2520non-linear%250Aprojection%2520methods%252C%2520such%2520as%2520Latent%2520Space%2520Dynamics%2520Identification%2520%2528LaSDI%2529.%2520LaSDI%250Amaps%2520full-order%2520PDE%2520solutions%2520to%2520a%2520latent%2520space%2520using%2520autoencoders%2520and%2520learns%250Athe%2520system%2520of%2520ODEs%2520governing%2520the%2520latent%2520space%2520dynamics.%2520By%2520interpolating%2520and%250Asolving%2520the%2520ODE%2520system%2520in%2520the%2520reduced%2520latent%2520space%252C%2520fast%2520and%2520accurate%2520ROM%250Apredictions%2520can%2520be%2520made%2520by%2520feeding%2520the%2520predicted%2520latent%2520space%2520dynamics%2520into%2520the%250Adecoder.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520GPLaSDI%252C%2520a%2520novel%2520LaSDI-based%2520framework%250Athat%2520relies%2520on%2520Gaussian%2520process%2520%2528GP%2529%2520for%2520latent%2520space%2520ODE%2520interpolations.%2520Using%250AGPs%2520offers%2520two%2520significant%2520advantages.%2520First%252C%2520it%2520enables%2520the%2520quantification%2520of%250Auncertainty%2520over%2520the%2520ROM%2520predictions.%2520Second%252C%2520leveraging%2520this%2520prediction%250Auncertainty%2520allows%2520for%2520efficient%2520adaptive%2520training%2520through%2520a%2520greedy%2520selection%250Aof%2520additional%2520training%2520data%2520points.%2520This%2520approach%2520does%2520not%2520require%2520prior%250Aknowledge%2520of%2520the%2520underlying%2520PDEs.%2520Consequently%252C%2520GPLaSDI%2520is%2520inherently%250Anon-intrusive%2520and%2520can%2520be%2520applied%2520to%2520problems%2520without%2520a%2520known%2520PDE%2520or%2520its%250Aresidual.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520on%2520the%2520Burgers%250Aequation%252C%2520Vlasov%2520equation%2520for%2520plasma%2520physics%252C%2520and%2520a%2520rising%2520thermal%2520bubble%250Aproblem.%2520Our%2520proposed%2520method%2520achieves%2520between%2520200%2520and%2520100%252C000%2520times%2520speed-up%252C%250Awith%2520up%2520to%25207%2525%2520relative%2520error.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.05882v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPLaSDI%3A%20Gaussian%20Process-based%20Interpretable%20Latent%20Space%20Dynamics%0A%20%20Identification%20through%20Deep%20Autoencoder&entry.906535625=Christophe%20Bonneville%20and%20Youngsoo%20Choi%20and%20Debojyoti%20Ghosh%20and%20Jonathan%20L.%20Belof&entry.1292438233=%20%20Numerically%20solving%20partial%20differential%20equations%20%28PDEs%29%20can%20be%20challenging%0Aand%20computationally%20expensive.%20This%20has%20led%20to%20the%20development%20of%20reduced-order%0Amodels%20%28ROMs%29%20that%20are%20accurate%20but%20faster%20than%20full%20order%20models%20%28FOMs%29.%0ARecently%2C%20machine%20learning%20advances%20have%20enabled%20the%20creation%20of%20non-linear%0Aprojection%20methods%2C%20such%20as%20Latent%20Space%20Dynamics%20Identification%20%28LaSDI%29.%20LaSDI%0Amaps%20full-order%20PDE%20solutions%20to%20a%20latent%20space%20using%20autoencoders%20and%20learns%0Athe%20system%20of%20ODEs%20governing%20the%20latent%20space%20dynamics.%20By%20interpolating%20and%0Asolving%20the%20ODE%20system%20in%20the%20reduced%20latent%20space%2C%20fast%20and%20accurate%20ROM%0Apredictions%20can%20be%20made%20by%20feeding%20the%20predicted%20latent%20space%20dynamics%20into%20the%0Adecoder.%20In%20this%20paper%2C%20we%20introduce%20GPLaSDI%2C%20a%20novel%20LaSDI-based%20framework%0Athat%20relies%20on%20Gaussian%20process%20%28GP%29%20for%20latent%20space%20ODE%20interpolations.%20Using%0AGPs%20offers%20two%20significant%20advantages.%20First%2C%20it%20enables%20the%20quantification%20of%0Auncertainty%20over%20the%20ROM%20predictions.%20Second%2C%20leveraging%20this%20prediction%0Auncertainty%20allows%20for%20efficient%20adaptive%20training%20through%20a%20greedy%20selection%0Aof%20additional%20training%20data%20points.%20This%20approach%20does%20not%20require%20prior%0Aknowledge%20of%20the%20underlying%20PDEs.%20Consequently%2C%20GPLaSDI%20is%20inherently%0Anon-intrusive%20and%20can%20be%20applied%20to%20problems%20without%20a%20known%20PDE%20or%20its%0Aresidual.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%20the%20Burgers%0Aequation%2C%20Vlasov%20equation%20for%20plasma%20physics%2C%20and%20a%20rising%20thermal%20bubble%0Aproblem.%20Our%20proposed%20method%20achieves%20between%20200%20and%20100%2C000%20times%20speed-up%2C%0Awith%20up%20to%207%25%20relative%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.05882v2&entry.124074799=Read"},
{"title": "ReinWiFi: A Reinforcement-Learning-Based Framework for the\n  Application-Layer QoS Optimization of WiFi Networks", "author": "Qianren Li and Bojie Lv and Yuncong Hong and Rui Wang", "abstract": "  In this paper, a reinforcement-learning-based scheduling framework is\nproposed and implemented to optimize the application-layer quality-of-service\n(QoS) of a practical wireless local area network (WLAN) suffering from unknown\ninterference. Particularly, application-layer tasks of file delivery and\ndelay-sensitive communication, e.g., screen projection, in a WLAN with enhanced\ndistributed channel access (EDCA) mechanism, are jointly scheduled by adjusting\nthe contention window sizes and application-layer throughput limitation, such\nthat their QoS, including the throughput of file delivery and the round trip\ntime of the delay-sensitive communication, can be optimized. Due to the unknown\ninterference and vendor-dependent implementation of the network interface card,\nthe relation between the scheduling policy and the system QoS is unknown.\nHence, a reinforcement learning method is proposed, in which a novel Q-network\nis trained to map from the historical scheduling parameters and QoS\nobservations to the current scheduling action. It is demonstrated on a testbed\nthat the proposed framework can achieve a significantly better QoS than the\nconventional EDCA mechanism.\n", "link": "http://arxiv.org/abs/2405.03526v1", "date": "2024-05-06", "relevancy": 2.1911, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4535}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4345}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReinWiFi%3A%20A%20Reinforcement-Learning-Based%20Framework%20for%20the%0A%20%20Application-Layer%20QoS%20Optimization%20of%20WiFi%20Networks&body=Title%3A%20ReinWiFi%3A%20A%20Reinforcement-Learning-Based%20Framework%20for%20the%0A%20%20Application-Layer%20QoS%20Optimization%20of%20WiFi%20Networks%0AAuthor%3A%20Qianren%20Li%20and%20Bojie%20Lv%20and%20Yuncong%20Hong%20and%20Rui%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20a%20reinforcement-learning-based%20scheduling%20framework%20is%0Aproposed%20and%20implemented%20to%20optimize%20the%20application-layer%20quality-of-service%0A%28QoS%29%20of%20a%20practical%20wireless%20local%20area%20network%20%28WLAN%29%20suffering%20from%20unknown%0Ainterference.%20Particularly%2C%20application-layer%20tasks%20of%20file%20delivery%20and%0Adelay-sensitive%20communication%2C%20e.g.%2C%20screen%20projection%2C%20in%20a%20WLAN%20with%20enhanced%0Adistributed%20channel%20access%20%28EDCA%29%20mechanism%2C%20are%20jointly%20scheduled%20by%20adjusting%0Athe%20contention%20window%20sizes%20and%20application-layer%20throughput%20limitation%2C%20such%0Athat%20their%20QoS%2C%20including%20the%20throughput%20of%20file%20delivery%20and%20the%20round%20trip%0Atime%20of%20the%20delay-sensitive%20communication%2C%20can%20be%20optimized.%20Due%20to%20the%20unknown%0Ainterference%20and%20vendor-dependent%20implementation%20of%20the%20network%20interface%20card%2C%0Athe%20relation%20between%20the%20scheduling%20policy%20and%20the%20system%20QoS%20is%20unknown.%0AHence%2C%20a%20reinforcement%20learning%20method%20is%20proposed%2C%20in%20which%20a%20novel%20Q-network%0Ais%20trained%20to%20map%20from%20the%20historical%20scheduling%20parameters%20and%20QoS%0Aobservations%20to%20the%20current%20scheduling%20action.%20It%20is%20demonstrated%20on%20a%20testbed%0Athat%20the%20proposed%20framework%20can%20achieve%20a%20significantly%20better%20QoS%20than%20the%0Aconventional%20EDCA%20mechanism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinWiFi%253A%2520A%2520Reinforcement-Learning-Based%2520Framework%2520for%2520the%250A%2520%2520Application-Layer%2520QoS%2520Optimization%2520of%2520WiFi%2520Networks%26entry.906535625%3DQianren%2520Li%2520and%2520Bojie%2520Lv%2520and%2520Yuncong%2520Hong%2520and%2520Rui%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520a%2520reinforcement-learning-based%2520scheduling%2520framework%2520is%250Aproposed%2520and%2520implemented%2520to%2520optimize%2520the%2520application-layer%2520quality-of-service%250A%2528QoS%2529%2520of%2520a%2520practical%2520wireless%2520local%2520area%2520network%2520%2528WLAN%2529%2520suffering%2520from%2520unknown%250Ainterference.%2520Particularly%252C%2520application-layer%2520tasks%2520of%2520file%2520delivery%2520and%250Adelay-sensitive%2520communication%252C%2520e.g.%252C%2520screen%2520projection%252C%2520in%2520a%2520WLAN%2520with%2520enhanced%250Adistributed%2520channel%2520access%2520%2528EDCA%2529%2520mechanism%252C%2520are%2520jointly%2520scheduled%2520by%2520adjusting%250Athe%2520contention%2520window%2520sizes%2520and%2520application-layer%2520throughput%2520limitation%252C%2520such%250Athat%2520their%2520QoS%252C%2520including%2520the%2520throughput%2520of%2520file%2520delivery%2520and%2520the%2520round%2520trip%250Atime%2520of%2520the%2520delay-sensitive%2520communication%252C%2520can%2520be%2520optimized.%2520Due%2520to%2520the%2520unknown%250Ainterference%2520and%2520vendor-dependent%2520implementation%2520of%2520the%2520network%2520interface%2520card%252C%250Athe%2520relation%2520between%2520the%2520scheduling%2520policy%2520and%2520the%2520system%2520QoS%2520is%2520unknown.%250AHence%252C%2520a%2520reinforcement%2520learning%2520method%2520is%2520proposed%252C%2520in%2520which%2520a%2520novel%2520Q-network%250Ais%2520trained%2520to%2520map%2520from%2520the%2520historical%2520scheduling%2520parameters%2520and%2520QoS%250Aobservations%2520to%2520the%2520current%2520scheduling%2520action.%2520It%2520is%2520demonstrated%2520on%2520a%2520testbed%250Athat%2520the%2520proposed%2520framework%2520can%2520achieve%2520a%2520significantly%2520better%2520QoS%2520than%2520the%250Aconventional%2520EDCA%2520mechanism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReinWiFi%3A%20A%20Reinforcement-Learning-Based%20Framework%20for%20the%0A%20%20Application-Layer%20QoS%20Optimization%20of%20WiFi%20Networks&entry.906535625=Qianren%20Li%20and%20Bojie%20Lv%20and%20Yuncong%20Hong%20and%20Rui%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20a%20reinforcement-learning-based%20scheduling%20framework%20is%0Aproposed%20and%20implemented%20to%20optimize%20the%20application-layer%20quality-of-service%0A%28QoS%29%20of%20a%20practical%20wireless%20local%20area%20network%20%28WLAN%29%20suffering%20from%20unknown%0Ainterference.%20Particularly%2C%20application-layer%20tasks%20of%20file%20delivery%20and%0Adelay-sensitive%20communication%2C%20e.g.%2C%20screen%20projection%2C%20in%20a%20WLAN%20with%20enhanced%0Adistributed%20channel%20access%20%28EDCA%29%20mechanism%2C%20are%20jointly%20scheduled%20by%20adjusting%0Athe%20contention%20window%20sizes%20and%20application-layer%20throughput%20limitation%2C%20such%0Athat%20their%20QoS%2C%20including%20the%20throughput%20of%20file%20delivery%20and%20the%20round%20trip%0Atime%20of%20the%20delay-sensitive%20communication%2C%20can%20be%20optimized.%20Due%20to%20the%20unknown%0Ainterference%20and%20vendor-dependent%20implementation%20of%20the%20network%20interface%20card%2C%0Athe%20relation%20between%20the%20scheduling%20policy%20and%20the%20system%20QoS%20is%20unknown.%0AHence%2C%20a%20reinforcement%20learning%20method%20is%20proposed%2C%20in%20which%20a%20novel%20Q-network%0Ais%20trained%20to%20map%20from%20the%20historical%20scheduling%20parameters%20and%20QoS%0Aobservations%20to%20the%20current%20scheduling%20action.%20It%20is%20demonstrated%20on%20a%20testbed%0Athat%20the%20proposed%20framework%20can%20achieve%20a%20significantly%20better%20QoS%20than%20the%0Aconventional%20EDCA%20mechanism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03526v1&entry.124074799=Read"},
{"title": "Towards Efficient Replay in Federated Incremental Learning", "author": "Yichen Li and Qunwei Li and Haozhao Wang and Ruixuan Li and Wenliang Zhong and Guannan Zhang", "abstract": "  In Federated Learning (FL), the data in each client is typically assumed\nfixed or static. However, data often comes in an incremental manner in\nreal-world applications, where the data domain may increase dynamically. In\nthis work, we study catastrophic forgetting with data heterogeneity in\nFederated Incremental Learning (FIL) scenarios where edge clients may lack\nenough storage space to retain full data. We propose to employ a simple,\ngeneric framework for FIL named Re-Fed, which can coordinate each client to\ncache important samples for replay. More specifically, when a new task arrives,\neach client first caches selected previous samples based on their global and\nlocal importance. Then, the client trains the local model with both the cached\nsamples and the samples from the new task. Theoretically, we analyze the\nability of Re-Fed to discover important samples for replay thus alleviating the\ncatastrophic forgetting problem. Moreover, we empirically show that Re-Fed\nachieves competitive performance compared to state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.05890v2", "date": "2024-05-06", "relevancy": 2.1876, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4484}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4321}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Efficient%20Replay%20in%20Federated%20Incremental%20Learning&body=Title%3A%20Towards%20Efficient%20Replay%20in%20Federated%20Incremental%20Learning%0AAuthor%3A%20Yichen%20Li%20and%20Qunwei%20Li%20and%20Haozhao%20Wang%20and%20Ruixuan%20Li%20and%20Wenliang%20Zhong%20and%20Guannan%20Zhang%0AAbstract%3A%20%20%20In%20Federated%20Learning%20%28FL%29%2C%20the%20data%20in%20each%20client%20is%20typically%20assumed%0Afixed%20or%20static.%20However%2C%20data%20often%20comes%20in%20an%20incremental%20manner%20in%0Areal-world%20applications%2C%20where%20the%20data%20domain%20may%20increase%20dynamically.%20In%0Athis%20work%2C%20we%20study%20catastrophic%20forgetting%20with%20data%20heterogeneity%20in%0AFederated%20Incremental%20Learning%20%28FIL%29%20scenarios%20where%20edge%20clients%20may%20lack%0Aenough%20storage%20space%20to%20retain%20full%20data.%20We%20propose%20to%20employ%20a%20simple%2C%0Ageneric%20framework%20for%20FIL%20named%20Re-Fed%2C%20which%20can%20coordinate%20each%20client%20to%0Acache%20important%20samples%20for%20replay.%20More%20specifically%2C%20when%20a%20new%20task%20arrives%2C%0Aeach%20client%20first%20caches%20selected%20previous%20samples%20based%20on%20their%20global%20and%0Alocal%20importance.%20Then%2C%20the%20client%20trains%20the%20local%20model%20with%20both%20the%20cached%0Asamples%20and%20the%20samples%20from%20the%20new%20task.%20Theoretically%2C%20we%20analyze%20the%0Aability%20of%20Re-Fed%20to%20discover%20important%20samples%20for%20replay%20thus%20alleviating%20the%0Acatastrophic%20forgetting%20problem.%20Moreover%2C%20we%20empirically%20show%20that%20Re-Fed%0Aachieves%20competitive%20performance%20compared%20to%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05890v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Efficient%2520Replay%2520in%2520Federated%2520Incremental%2520Learning%26entry.906535625%3DYichen%2520Li%2520and%2520Qunwei%2520Li%2520and%2520Haozhao%2520Wang%2520and%2520Ruixuan%2520Li%2520and%2520Wenliang%2520Zhong%2520and%2520Guannan%2520Zhang%26entry.1292438233%3D%2520%2520In%2520Federated%2520Learning%2520%2528FL%2529%252C%2520the%2520data%2520in%2520each%2520client%2520is%2520typically%2520assumed%250Afixed%2520or%2520static.%2520However%252C%2520data%2520often%2520comes%2520in%2520an%2520incremental%2520manner%2520in%250Areal-world%2520applications%252C%2520where%2520the%2520data%2520domain%2520may%2520increase%2520dynamically.%2520In%250Athis%2520work%252C%2520we%2520study%2520catastrophic%2520forgetting%2520with%2520data%2520heterogeneity%2520in%250AFederated%2520Incremental%2520Learning%2520%2528FIL%2529%2520scenarios%2520where%2520edge%2520clients%2520may%2520lack%250Aenough%2520storage%2520space%2520to%2520retain%2520full%2520data.%2520We%2520propose%2520to%2520employ%2520a%2520simple%252C%250Ageneric%2520framework%2520for%2520FIL%2520named%2520Re-Fed%252C%2520which%2520can%2520coordinate%2520each%2520client%2520to%250Acache%2520important%2520samples%2520for%2520replay.%2520More%2520specifically%252C%2520when%2520a%2520new%2520task%2520arrives%252C%250Aeach%2520client%2520first%2520caches%2520selected%2520previous%2520samples%2520based%2520on%2520their%2520global%2520and%250Alocal%2520importance.%2520Then%252C%2520the%2520client%2520trains%2520the%2520local%2520model%2520with%2520both%2520the%2520cached%250Asamples%2520and%2520the%2520samples%2520from%2520the%2520new%2520task.%2520Theoretically%252C%2520we%2520analyze%2520the%250Aability%2520of%2520Re-Fed%2520to%2520discover%2520important%2520samples%2520for%2520replay%2520thus%2520alleviating%2520the%250Acatastrophic%2520forgetting%2520problem.%2520Moreover%252C%2520we%2520empirically%2520show%2520that%2520Re-Fed%250Aachieves%2520competitive%2520performance%2520compared%2520to%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05890v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Efficient%20Replay%20in%20Federated%20Incremental%20Learning&entry.906535625=Yichen%20Li%20and%20Qunwei%20Li%20and%20Haozhao%20Wang%20and%20Ruixuan%20Li%20and%20Wenliang%20Zhong%20and%20Guannan%20Zhang&entry.1292438233=%20%20In%20Federated%20Learning%20%28FL%29%2C%20the%20data%20in%20each%20client%20is%20typically%20assumed%0Afixed%20or%20static.%20However%2C%20data%20often%20comes%20in%20an%20incremental%20manner%20in%0Areal-world%20applications%2C%20where%20the%20data%20domain%20may%20increase%20dynamically.%20In%0Athis%20work%2C%20we%20study%20catastrophic%20forgetting%20with%20data%20heterogeneity%20in%0AFederated%20Incremental%20Learning%20%28FIL%29%20scenarios%20where%20edge%20clients%20may%20lack%0Aenough%20storage%20space%20to%20retain%20full%20data.%20We%20propose%20to%20employ%20a%20simple%2C%0Ageneric%20framework%20for%20FIL%20named%20Re-Fed%2C%20which%20can%20coordinate%20each%20client%20to%0Acache%20important%20samples%20for%20replay.%20More%20specifically%2C%20when%20a%20new%20task%20arrives%2C%0Aeach%20client%20first%20caches%20selected%20previous%20samples%20based%20on%20their%20global%20and%0Alocal%20importance.%20Then%2C%20the%20client%20trains%20the%20local%20model%20with%20both%20the%20cached%0Asamples%20and%20the%20samples%20from%20the%20new%20task.%20Theoretically%2C%20we%20analyze%20the%0Aability%20of%20Re-Fed%20to%20discover%20important%20samples%20for%20replay%20thus%20alleviating%20the%0Acatastrophic%20forgetting%20problem.%20Moreover%2C%20we%20empirically%20show%20that%20Re-Fed%0Aachieves%20competitive%20performance%20compared%20to%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05890v2&entry.124074799=Read"},
{"title": "HawkDrive: A Transformer-driven Visual Perception System for Autonomous\n  Driving in Night Scene", "author": "Ziang Guo and Stepan Perminov and Mikhail Konenkov and Dzmitry Tsetserukou", "abstract": "  Many established vision perception systems for autonomous driving scenarios\nignore the influence of light conditions, one of the key elements for driving\nsafety. To address this problem, we present HawkDrive, a novel perception\nsystem with hardware and software solutions. Hardware that utilizes stereo\nvision perception, which has been demonstrated to be a more reliable way of\nestimating depth information than monocular vision, is partnered with the edge\ncomputing device Nvidia Jetson Xavier AGX. Our software for low light\nenhancement, depth estimation, and semantic segmentation tasks, is a\ntransformer-based neural network. Our software stack, which enables fast\ninference and noise reduction, is packaged into system modules in Robot\nOperating System 2 (ROS2). Our experimental results have shown that the\nproposed end-to-end system is effective in improving the depth estimation and\nsemantic segmentation performance. Our dataset and codes will be released at\nhttps://github.com/ZionGo6/HawkDrive.\n", "link": "http://arxiv.org/abs/2404.04653v2", "date": "2024-05-06", "relevancy": 2.1765, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5531}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5405}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HawkDrive%3A%20A%20Transformer-driven%20Visual%20Perception%20System%20for%20Autonomous%0A%20%20Driving%20in%20Night%20Scene&body=Title%3A%20HawkDrive%3A%20A%20Transformer-driven%20Visual%20Perception%20System%20for%20Autonomous%0A%20%20Driving%20in%20Night%20Scene%0AAuthor%3A%20Ziang%20Guo%20and%20Stepan%20Perminov%20and%20Mikhail%20Konenkov%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20Many%20established%20vision%20perception%20systems%20for%20autonomous%20driving%20scenarios%0Aignore%20the%20influence%20of%20light%20conditions%2C%20one%20of%20the%20key%20elements%20for%20driving%0Asafety.%20To%20address%20this%20problem%2C%20we%20present%20HawkDrive%2C%20a%20novel%20perception%0Asystem%20with%20hardware%20and%20software%20solutions.%20Hardware%20that%20utilizes%20stereo%0Avision%20perception%2C%20which%20has%20been%20demonstrated%20to%20be%20a%20more%20reliable%20way%20of%0Aestimating%20depth%20information%20than%20monocular%20vision%2C%20is%20partnered%20with%20the%20edge%0Acomputing%20device%20Nvidia%20Jetson%20Xavier%20AGX.%20Our%20software%20for%20low%20light%0Aenhancement%2C%20depth%20estimation%2C%20and%20semantic%20segmentation%20tasks%2C%20is%20a%0Atransformer-based%20neural%20network.%20Our%20software%20stack%2C%20which%20enables%20fast%0Ainference%20and%20noise%20reduction%2C%20is%20packaged%20into%20system%20modules%20in%20Robot%0AOperating%20System%202%20%28ROS2%29.%20Our%20experimental%20results%20have%20shown%20that%20the%0Aproposed%20end-to-end%20system%20is%20effective%20in%20improving%20the%20depth%20estimation%20and%0Asemantic%20segmentation%20performance.%20Our%20dataset%20and%20codes%20will%20be%20released%20at%0Ahttps%3A//github.com/ZionGo6/HawkDrive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04653v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHawkDrive%253A%2520A%2520Transformer-driven%2520Visual%2520Perception%2520System%2520for%2520Autonomous%250A%2520%2520Driving%2520in%2520Night%2520Scene%26entry.906535625%3DZiang%2520Guo%2520and%2520Stepan%2520Perminov%2520and%2520Mikhail%2520Konenkov%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520Many%2520established%2520vision%2520perception%2520systems%2520for%2520autonomous%2520driving%2520scenarios%250Aignore%2520the%2520influence%2520of%2520light%2520conditions%252C%2520one%2520of%2520the%2520key%2520elements%2520for%2520driving%250Asafety.%2520To%2520address%2520this%2520problem%252C%2520we%2520present%2520HawkDrive%252C%2520a%2520novel%2520perception%250Asystem%2520with%2520hardware%2520and%2520software%2520solutions.%2520Hardware%2520that%2520utilizes%2520stereo%250Avision%2520perception%252C%2520which%2520has%2520been%2520demonstrated%2520to%2520be%2520a%2520more%2520reliable%2520way%2520of%250Aestimating%2520depth%2520information%2520than%2520monocular%2520vision%252C%2520is%2520partnered%2520with%2520the%2520edge%250Acomputing%2520device%2520Nvidia%2520Jetson%2520Xavier%2520AGX.%2520Our%2520software%2520for%2520low%2520light%250Aenhancement%252C%2520depth%2520estimation%252C%2520and%2520semantic%2520segmentation%2520tasks%252C%2520is%2520a%250Atransformer-based%2520neural%2520network.%2520Our%2520software%2520stack%252C%2520which%2520enables%2520fast%250Ainference%2520and%2520noise%2520reduction%252C%2520is%2520packaged%2520into%2520system%2520modules%2520in%2520Robot%250AOperating%2520System%25202%2520%2528ROS2%2529.%2520Our%2520experimental%2520results%2520have%2520shown%2520that%2520the%250Aproposed%2520end-to-end%2520system%2520is%2520effective%2520in%2520improving%2520the%2520depth%2520estimation%2520and%250Asemantic%2520segmentation%2520performance.%2520Our%2520dataset%2520and%2520codes%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/ZionGo6/HawkDrive.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04653v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HawkDrive%3A%20A%20Transformer-driven%20Visual%20Perception%20System%20for%20Autonomous%0A%20%20Driving%20in%20Night%20Scene&entry.906535625=Ziang%20Guo%20and%20Stepan%20Perminov%20and%20Mikhail%20Konenkov%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20Many%20established%20vision%20perception%20systems%20for%20autonomous%20driving%20scenarios%0Aignore%20the%20influence%20of%20light%20conditions%2C%20one%20of%20the%20key%20elements%20for%20driving%0Asafety.%20To%20address%20this%20problem%2C%20we%20present%20HawkDrive%2C%20a%20novel%20perception%0Asystem%20with%20hardware%20and%20software%20solutions.%20Hardware%20that%20utilizes%20stereo%0Avision%20perception%2C%20which%20has%20been%20demonstrated%20to%20be%20a%20more%20reliable%20way%20of%0Aestimating%20depth%20information%20than%20monocular%20vision%2C%20is%20partnered%20with%20the%20edge%0Acomputing%20device%20Nvidia%20Jetson%20Xavier%20AGX.%20Our%20software%20for%20low%20light%0Aenhancement%2C%20depth%20estimation%2C%20and%20semantic%20segmentation%20tasks%2C%20is%20a%0Atransformer-based%20neural%20network.%20Our%20software%20stack%2C%20which%20enables%20fast%0Ainference%20and%20noise%20reduction%2C%20is%20packaged%20into%20system%20modules%20in%20Robot%0AOperating%20System%202%20%28ROS2%29.%20Our%20experimental%20results%20have%20shown%20that%20the%0Aproposed%20end-to-end%20system%20is%20effective%20in%20improving%20the%20depth%20estimation%20and%0Asemantic%20segmentation%20performance.%20Our%20dataset%20and%20codes%20will%20be%20released%20at%0Ahttps%3A//github.com/ZionGo6/HawkDrive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04653v2&entry.124074799=Read"},
{"title": "Collecting Consistently High Quality Object Tracks with Minimal Human\n  Involvement by Using Self-Supervised Learning to Detect Tracker Errors", "author": "Samreen Anjum and Suyog Jain and Danna Gurari", "abstract": "  We propose a hybrid framework for consistently producing high-quality object\ntracks by combining an automated object tracker with little human input. The\nkey idea is to tailor a module for each dataset to intelligently decide when an\nobject tracker is failing and so humans should be brought in to re-localize an\nobject for continued tracking. Our approach leverages self-supervised learning\non unlabeled videos to learn a tailored representation for a target object that\nis then used to actively monitor its tracked region and decide when the tracker\nfails. Since labeled data is not needed, our approach can be applied to novel\nobject categories. Experiments on three datasets demonstrate our method\noutperforms existing approaches, especially for small, fast moving, or occluded\nobjects.\n", "link": "http://arxiv.org/abs/2405.03643v1", "date": "2024-05-06", "relevancy": 2.1741, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5496}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5398}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collecting%20Consistently%20High%20Quality%20Object%20Tracks%20with%20Minimal%20Human%0A%20%20Involvement%20by%20Using%20Self-Supervised%20Learning%20to%20Detect%20Tracker%20Errors&body=Title%3A%20Collecting%20Consistently%20High%20Quality%20Object%20Tracks%20with%20Minimal%20Human%0A%20%20Involvement%20by%20Using%20Self-Supervised%20Learning%20to%20Detect%20Tracker%20Errors%0AAuthor%3A%20Samreen%20Anjum%20and%20Suyog%20Jain%20and%20Danna%20Gurari%0AAbstract%3A%20%20%20We%20propose%20a%20hybrid%20framework%20for%20consistently%20producing%20high-quality%20object%0Atracks%20by%20combining%20an%20automated%20object%20tracker%20with%20little%20human%20input.%20The%0Akey%20idea%20is%20to%20tailor%20a%20module%20for%20each%20dataset%20to%20intelligently%20decide%20when%20an%0Aobject%20tracker%20is%20failing%20and%20so%20humans%20should%20be%20brought%20in%20to%20re-localize%20an%0Aobject%20for%20continued%20tracking.%20Our%20approach%20leverages%20self-supervised%20learning%0Aon%20unlabeled%20videos%20to%20learn%20a%20tailored%20representation%20for%20a%20target%20object%20that%0Ais%20then%20used%20to%20actively%20monitor%20its%20tracked%20region%20and%20decide%20when%20the%20tracker%0Afails.%20Since%20labeled%20data%20is%20not%20needed%2C%20our%20approach%20can%20be%20applied%20to%20novel%0Aobject%20categories.%20Experiments%20on%20three%20datasets%20demonstrate%20our%20method%0Aoutperforms%20existing%20approaches%2C%20especially%20for%20small%2C%20fast%20moving%2C%20or%20occluded%0Aobjects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollecting%2520Consistently%2520High%2520Quality%2520Object%2520Tracks%2520with%2520Minimal%2520Human%250A%2520%2520Involvement%2520by%2520Using%2520Self-Supervised%2520Learning%2520to%2520Detect%2520Tracker%2520Errors%26entry.906535625%3DSamreen%2520Anjum%2520and%2520Suyog%2520Jain%2520and%2520Danna%2520Gurari%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520hybrid%2520framework%2520for%2520consistently%2520producing%2520high-quality%2520object%250Atracks%2520by%2520combining%2520an%2520automated%2520object%2520tracker%2520with%2520little%2520human%2520input.%2520The%250Akey%2520idea%2520is%2520to%2520tailor%2520a%2520module%2520for%2520each%2520dataset%2520to%2520intelligently%2520decide%2520when%2520an%250Aobject%2520tracker%2520is%2520failing%2520and%2520so%2520humans%2520should%2520be%2520brought%2520in%2520to%2520re-localize%2520an%250Aobject%2520for%2520continued%2520tracking.%2520Our%2520approach%2520leverages%2520self-supervised%2520learning%250Aon%2520unlabeled%2520videos%2520to%2520learn%2520a%2520tailored%2520representation%2520for%2520a%2520target%2520object%2520that%250Ais%2520then%2520used%2520to%2520actively%2520monitor%2520its%2520tracked%2520region%2520and%2520decide%2520when%2520the%2520tracker%250Afails.%2520Since%2520labeled%2520data%2520is%2520not%2520needed%252C%2520our%2520approach%2520can%2520be%2520applied%2520to%2520novel%250Aobject%2520categories.%2520Experiments%2520on%2520three%2520datasets%2520demonstrate%2520our%2520method%250Aoutperforms%2520existing%2520approaches%252C%2520especially%2520for%2520small%252C%2520fast%2520moving%252C%2520or%2520occluded%250Aobjects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collecting%20Consistently%20High%20Quality%20Object%20Tracks%20with%20Minimal%20Human%0A%20%20Involvement%20by%20Using%20Self-Supervised%20Learning%20to%20Detect%20Tracker%20Errors&entry.906535625=Samreen%20Anjum%20and%20Suyog%20Jain%20and%20Danna%20Gurari&entry.1292438233=%20%20We%20propose%20a%20hybrid%20framework%20for%20consistently%20producing%20high-quality%20object%0Atracks%20by%20combining%20an%20automated%20object%20tracker%20with%20little%20human%20input.%20The%0Akey%20idea%20is%20to%20tailor%20a%20module%20for%20each%20dataset%20to%20intelligently%20decide%20when%20an%0Aobject%20tracker%20is%20failing%20and%20so%20humans%20should%20be%20brought%20in%20to%20re-localize%20an%0Aobject%20for%20continued%20tracking.%20Our%20approach%20leverages%20self-supervised%20learning%0Aon%20unlabeled%20videos%20to%20learn%20a%20tailored%20representation%20for%20a%20target%20object%20that%0Ais%20then%20used%20to%20actively%20monitor%20its%20tracked%20region%20and%20decide%20when%20the%20tracker%0Afails.%20Since%20labeled%20data%20is%20not%20needed%2C%20our%20approach%20can%20be%20applied%20to%20novel%0Aobject%20categories.%20Experiments%20on%20three%20datasets%20demonstrate%20our%20method%0Aoutperforms%20existing%20approaches%2C%20especially%20for%20small%2C%20fast%20moving%2C%20or%20occluded%0Aobjects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03643v1&entry.124074799=Read"},
{"title": "Development of Ultra-Portable 3D Mapping Systems for Emergency Services", "author": "Charles Hamesse and Timoth\u00e9e Fr\u00e9ville and Juha Saarinen and Michiel Vlaminck and Hiep Luong and Rob Haelterman", "abstract": "  Miniaturization of cameras and LiDAR sensors has enabled the development of\nwearable 3D mapping systems for emergency responders. These systems have the\npotential to revolutionize response capabilities by providing real-time,\nhigh-fidelity maps of dynamic and hazardous environments. We present our recent\nefforts towards the development of such ultra-portable 3D mapping systems. We\nreview four different sensor configurations, either helmet-mounted or\nbody-worn, with two different mapping algorithms that were implemented and\nevaluated during field trials. The paper discusses the experimental results\nwith the aim to stimulate further discussion within the portable 3D mapping\nresearch community.\n", "link": "http://arxiv.org/abs/2405.03514v1", "date": "2024-05-06", "relevancy": 2.1737, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5746}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5239}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Development%20of%20Ultra-Portable%203D%20Mapping%20Systems%20for%20Emergency%20Services&body=Title%3A%20Development%20of%20Ultra-Portable%203D%20Mapping%20Systems%20for%20Emergency%20Services%0AAuthor%3A%20Charles%20Hamesse%20and%20Timoth%C3%A9e%20Fr%C3%A9ville%20and%20Juha%20Saarinen%20and%20Michiel%20Vlaminck%20and%20Hiep%20Luong%20and%20Rob%20Haelterman%0AAbstract%3A%20%20%20Miniaturization%20of%20cameras%20and%20LiDAR%20sensors%20has%20enabled%20the%20development%20of%0Awearable%203D%20mapping%20systems%20for%20emergency%20responders.%20These%20systems%20have%20the%0Apotential%20to%20revolutionize%20response%20capabilities%20by%20providing%20real-time%2C%0Ahigh-fidelity%20maps%20of%20dynamic%20and%20hazardous%20environments.%20We%20present%20our%20recent%0Aefforts%20towards%20the%20development%20of%20such%20ultra-portable%203D%20mapping%20systems.%20We%0Areview%20four%20different%20sensor%20configurations%2C%20either%20helmet-mounted%20or%0Abody-worn%2C%20with%20two%20different%20mapping%20algorithms%20that%20were%20implemented%20and%0Aevaluated%20during%20field%20trials.%20The%20paper%20discusses%20the%20experimental%20results%0Awith%20the%20aim%20to%20stimulate%20further%20discussion%20within%20the%20portable%203D%20mapping%0Aresearch%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDevelopment%2520of%2520Ultra-Portable%25203D%2520Mapping%2520Systems%2520for%2520Emergency%2520Services%26entry.906535625%3DCharles%2520Hamesse%2520and%2520Timoth%25C3%25A9e%2520Fr%25C3%25A9ville%2520and%2520Juha%2520Saarinen%2520and%2520Michiel%2520Vlaminck%2520and%2520Hiep%2520Luong%2520and%2520Rob%2520Haelterman%26entry.1292438233%3D%2520%2520Miniaturization%2520of%2520cameras%2520and%2520LiDAR%2520sensors%2520has%2520enabled%2520the%2520development%2520of%250Awearable%25203D%2520mapping%2520systems%2520for%2520emergency%2520responders.%2520These%2520systems%2520have%2520the%250Apotential%2520to%2520revolutionize%2520response%2520capabilities%2520by%2520providing%2520real-time%252C%250Ahigh-fidelity%2520maps%2520of%2520dynamic%2520and%2520hazardous%2520environments.%2520We%2520present%2520our%2520recent%250Aefforts%2520towards%2520the%2520development%2520of%2520such%2520ultra-portable%25203D%2520mapping%2520systems.%2520We%250Areview%2520four%2520different%2520sensor%2520configurations%252C%2520either%2520helmet-mounted%2520or%250Abody-worn%252C%2520with%2520two%2520different%2520mapping%2520algorithms%2520that%2520were%2520implemented%2520and%250Aevaluated%2520during%2520field%2520trials.%2520The%2520paper%2520discusses%2520the%2520experimental%2520results%250Awith%2520the%2520aim%2520to%2520stimulate%2520further%2520discussion%2520within%2520the%2520portable%25203D%2520mapping%250Aresearch%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Development%20of%20Ultra-Portable%203D%20Mapping%20Systems%20for%20Emergency%20Services&entry.906535625=Charles%20Hamesse%20and%20Timoth%C3%A9e%20Fr%C3%A9ville%20and%20Juha%20Saarinen%20and%20Michiel%20Vlaminck%20and%20Hiep%20Luong%20and%20Rob%20Haelterman&entry.1292438233=%20%20Miniaturization%20of%20cameras%20and%20LiDAR%20sensors%20has%20enabled%20the%20development%20of%0Awearable%203D%20mapping%20systems%20for%20emergency%20responders.%20These%20systems%20have%20the%0Apotential%20to%20revolutionize%20response%20capabilities%20by%20providing%20real-time%2C%0Ahigh-fidelity%20maps%20of%20dynamic%20and%20hazardous%20environments.%20We%20present%20our%20recent%0Aefforts%20towards%20the%20development%20of%20such%20ultra-portable%203D%20mapping%20systems.%20We%0Areview%20four%20different%20sensor%20configurations%2C%20either%20helmet-mounted%20or%0Abody-worn%2C%20with%20two%20different%20mapping%20algorithms%20that%20were%20implemented%20and%0Aevaluated%20during%20field%20trials.%20The%20paper%20discusses%20the%20experimental%20results%0Awith%20the%20aim%20to%20stimulate%20further%20discussion%20within%20the%20portable%203D%20mapping%0Aresearch%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03514v1&entry.124074799=Read"},
{"title": "PEM: Prototype-based Efficient MaskFormer for Image Segmentation", "author": "Niccol\u00f2 Cavagnero and Gabriele Rosi and Claudia Cuttano and Francesca Pistilli and Marco Ciccone and Giuseppe Averta and Fabio Cermelli", "abstract": "  Recent transformer-based architectures have shown impressive results in the\nfield of image segmentation. Thanks to their flexibility, they obtain\noutstanding performance in multiple segmentation tasks, such as semantic and\npanoptic, under a single unified framework. To achieve such impressive\nperformance, these architectures employ intensive operations and require\nsubstantial computational resources, which are often not available, especially\non edge devices. To fill this gap, we propose Prototype-based Efficient\nMaskFormer (PEM), an efficient transformer-based architecture that can operate\nin multiple segmentation tasks. PEM proposes a novel prototype-based\ncross-attention which leverages the redundancy of visual features to restrict\nthe computation and improve the efficiency without harming the performance. In\naddition, PEM introduces an efficient multi-scale feature pyramid network,\ncapable of extracting features that have high semantic content in an efficient\nway, thanks to the combination of deformable convolutions and context-based\nself-modulation. We benchmark the proposed PEM architecture on two tasks,\nsemantic and panoptic segmentation, evaluated on two different datasets,\nCityscapes and ADE20K. PEM demonstrates outstanding performance on every task\nand dataset, outperforming task-specific architectures while being comparable\nand even better than computationally-expensive baselines.\n", "link": "http://arxiv.org/abs/2402.19422v3", "date": "2024-05-06", "relevancy": 2.1655, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.558}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5312}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PEM%3A%20Prototype-based%20Efficient%20MaskFormer%20for%20Image%20Segmentation&body=Title%3A%20PEM%3A%20Prototype-based%20Efficient%20MaskFormer%20for%20Image%20Segmentation%0AAuthor%3A%20Niccol%C3%B2%20Cavagnero%20and%20Gabriele%20Rosi%20and%20Claudia%20Cuttano%20and%20Francesca%20Pistilli%20and%20Marco%20Ciccone%20and%20Giuseppe%20Averta%20and%20Fabio%20Cermelli%0AAbstract%3A%20%20%20Recent%20transformer-based%20architectures%20have%20shown%20impressive%20results%20in%20the%0Afield%20of%20image%20segmentation.%20Thanks%20to%20their%20flexibility%2C%20they%20obtain%0Aoutstanding%20performance%20in%20multiple%20segmentation%20tasks%2C%20such%20as%20semantic%20and%0Apanoptic%2C%20under%20a%20single%20unified%20framework.%20To%20achieve%20such%20impressive%0Aperformance%2C%20these%20architectures%20employ%20intensive%20operations%20and%20require%0Asubstantial%20computational%20resources%2C%20which%20are%20often%20not%20available%2C%20especially%0Aon%20edge%20devices.%20To%20fill%20this%20gap%2C%20we%20propose%20Prototype-based%20Efficient%0AMaskFormer%20%28PEM%29%2C%20an%20efficient%20transformer-based%20architecture%20that%20can%20operate%0Ain%20multiple%20segmentation%20tasks.%20PEM%20proposes%20a%20novel%20prototype-based%0Across-attention%20which%20leverages%20the%20redundancy%20of%20visual%20features%20to%20restrict%0Athe%20computation%20and%20improve%20the%20efficiency%20without%20harming%20the%20performance.%20In%0Aaddition%2C%20PEM%20introduces%20an%20efficient%20multi-scale%20feature%20pyramid%20network%2C%0Acapable%20of%20extracting%20features%20that%20have%20high%20semantic%20content%20in%20an%20efficient%0Away%2C%20thanks%20to%20the%20combination%20of%20deformable%20convolutions%20and%20context-based%0Aself-modulation.%20We%20benchmark%20the%20proposed%20PEM%20architecture%20on%20two%20tasks%2C%0Asemantic%20and%20panoptic%20segmentation%2C%20evaluated%20on%20two%20different%20datasets%2C%0ACityscapes%20and%20ADE20K.%20PEM%20demonstrates%20outstanding%20performance%20on%20every%20task%0Aand%20dataset%2C%20outperforming%20task-specific%20architectures%20while%20being%20comparable%0Aand%20even%20better%20than%20computationally-expensive%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19422v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPEM%253A%2520Prototype-based%2520Efficient%2520MaskFormer%2520for%2520Image%2520Segmentation%26entry.906535625%3DNiccol%25C3%25B2%2520Cavagnero%2520and%2520Gabriele%2520Rosi%2520and%2520Claudia%2520Cuttano%2520and%2520Francesca%2520Pistilli%2520and%2520Marco%2520Ciccone%2520and%2520Giuseppe%2520Averta%2520and%2520Fabio%2520Cermelli%26entry.1292438233%3D%2520%2520Recent%2520transformer-based%2520architectures%2520have%2520shown%2520impressive%2520results%2520in%2520the%250Afield%2520of%2520image%2520segmentation.%2520Thanks%2520to%2520their%2520flexibility%252C%2520they%2520obtain%250Aoutstanding%2520performance%2520in%2520multiple%2520segmentation%2520tasks%252C%2520such%2520as%2520semantic%2520and%250Apanoptic%252C%2520under%2520a%2520single%2520unified%2520framework.%2520To%2520achieve%2520such%2520impressive%250Aperformance%252C%2520these%2520architectures%2520employ%2520intensive%2520operations%2520and%2520require%250Asubstantial%2520computational%2520resources%252C%2520which%2520are%2520often%2520not%2520available%252C%2520especially%250Aon%2520edge%2520devices.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%2520Prototype-based%2520Efficient%250AMaskFormer%2520%2528PEM%2529%252C%2520an%2520efficient%2520transformer-based%2520architecture%2520that%2520can%2520operate%250Ain%2520multiple%2520segmentation%2520tasks.%2520PEM%2520proposes%2520a%2520novel%2520prototype-based%250Across-attention%2520which%2520leverages%2520the%2520redundancy%2520of%2520visual%2520features%2520to%2520restrict%250Athe%2520computation%2520and%2520improve%2520the%2520efficiency%2520without%2520harming%2520the%2520performance.%2520In%250Aaddition%252C%2520PEM%2520introduces%2520an%2520efficient%2520multi-scale%2520feature%2520pyramid%2520network%252C%250Acapable%2520of%2520extracting%2520features%2520that%2520have%2520high%2520semantic%2520content%2520in%2520an%2520efficient%250Away%252C%2520thanks%2520to%2520the%2520combination%2520of%2520deformable%2520convolutions%2520and%2520context-based%250Aself-modulation.%2520We%2520benchmark%2520the%2520proposed%2520PEM%2520architecture%2520on%2520two%2520tasks%252C%250Asemantic%2520and%2520panoptic%2520segmentation%252C%2520evaluated%2520on%2520two%2520different%2520datasets%252C%250ACityscapes%2520and%2520ADE20K.%2520PEM%2520demonstrates%2520outstanding%2520performance%2520on%2520every%2520task%250Aand%2520dataset%252C%2520outperforming%2520task-specific%2520architectures%2520while%2520being%2520comparable%250Aand%2520even%2520better%2520than%2520computationally-expensive%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19422v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEM%3A%20Prototype-based%20Efficient%20MaskFormer%20for%20Image%20Segmentation&entry.906535625=Niccol%C3%B2%20Cavagnero%20and%20Gabriele%20Rosi%20and%20Claudia%20Cuttano%20and%20Francesca%20Pistilli%20and%20Marco%20Ciccone%20and%20Giuseppe%20Averta%20and%20Fabio%20Cermelli&entry.1292438233=%20%20Recent%20transformer-based%20architectures%20have%20shown%20impressive%20results%20in%20the%0Afield%20of%20image%20segmentation.%20Thanks%20to%20their%20flexibility%2C%20they%20obtain%0Aoutstanding%20performance%20in%20multiple%20segmentation%20tasks%2C%20such%20as%20semantic%20and%0Apanoptic%2C%20under%20a%20single%20unified%20framework.%20To%20achieve%20such%20impressive%0Aperformance%2C%20these%20architectures%20employ%20intensive%20operations%20and%20require%0Asubstantial%20computational%20resources%2C%20which%20are%20often%20not%20available%2C%20especially%0Aon%20edge%20devices.%20To%20fill%20this%20gap%2C%20we%20propose%20Prototype-based%20Efficient%0AMaskFormer%20%28PEM%29%2C%20an%20efficient%20transformer-based%20architecture%20that%20can%20operate%0Ain%20multiple%20segmentation%20tasks.%20PEM%20proposes%20a%20novel%20prototype-based%0Across-attention%20which%20leverages%20the%20redundancy%20of%20visual%20features%20to%20restrict%0Athe%20computation%20and%20improve%20the%20efficiency%20without%20harming%20the%20performance.%20In%0Aaddition%2C%20PEM%20introduces%20an%20efficient%20multi-scale%20feature%20pyramid%20network%2C%0Acapable%20of%20extracting%20features%20that%20have%20high%20semantic%20content%20in%20an%20efficient%0Away%2C%20thanks%20to%20the%20combination%20of%20deformable%20convolutions%20and%20context-based%0Aself-modulation.%20We%20benchmark%20the%20proposed%20PEM%20architecture%20on%20two%20tasks%2C%0Asemantic%20and%20panoptic%20segmentation%2C%20evaluated%20on%20two%20different%20datasets%2C%0ACityscapes%20and%20ADE20K.%20PEM%20demonstrates%20outstanding%20performance%20on%20every%20task%0Aand%20dataset%2C%20outperforming%20task-specific%20architectures%20while%20being%20comparable%0Aand%20even%20better%20than%20computationally-expensive%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19422v3&entry.124074799=Read"},
{"title": "DUCK: Distance-based Unlearning via Centroid Kinematics", "author": "Marco Cotogni and Jacopo Bonato and Luigi Sabetta and Francesco Pelosin and Alessandro Nicolosi", "abstract": "  Machine Unlearning is rising as a new field, driven by the pressing necessity\nof ensuring privacy in modern artificial intelligence models. This technique\nprimarily aims to eradicate any residual influence of a specific subset of data\nfrom the knowledge acquired by a neural model during its training. This work\nintroduces a novel unlearning algorithm, denoted as Distance-based Unlearning\nvia Centroid Kinematics (DUCK), which employs metric learning to guide the\nremoval of samples matching the nearest incorrect centroid in the embedding\nspace. Evaluation of the algorithm's performance is conducted across various\nbenchmark datasets in two distinct scenarios, class removal, and homogeneous\nsampling removal, obtaining state-of-the-art performance. We also introduce a\nnovel metric, called Adaptive Unlearning Score (AUS), encompassing not only the\nefficacy of the unlearning process in forgetting target data but also\nquantifying the performance loss relative to the original model. Additionally,\nwe conducted a thorough investigation of the unlearning mechanism in DUCK,\nexamining its impact on the organization of the feature space and employing\nexplainable AI techniques for deeper insights.\n", "link": "http://arxiv.org/abs/2312.02052v2", "date": "2024-05-06", "relevancy": 2.1392, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5614}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5362}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DUCK%3A%20Distance-based%20Unlearning%20via%20Centroid%20Kinematics&body=Title%3A%20DUCK%3A%20Distance-based%20Unlearning%20via%20Centroid%20Kinematics%0AAuthor%3A%20Marco%20Cotogni%20and%20Jacopo%20Bonato%20and%20Luigi%20Sabetta%20and%20Francesco%20Pelosin%20and%20Alessandro%20Nicolosi%0AAbstract%3A%20%20%20Machine%20Unlearning%20is%20rising%20as%20a%20new%20field%2C%20driven%20by%20the%20pressing%20necessity%0Aof%20ensuring%20privacy%20in%20modern%20artificial%20intelligence%20models.%20This%20technique%0Aprimarily%20aims%20to%20eradicate%20any%20residual%20influence%20of%20a%20specific%20subset%20of%20data%0Afrom%20the%20knowledge%20acquired%20by%20a%20neural%20model%20during%20its%20training.%20This%20work%0Aintroduces%20a%20novel%20unlearning%20algorithm%2C%20denoted%20as%20Distance-based%20Unlearning%0Avia%20Centroid%20Kinematics%20%28DUCK%29%2C%20which%20employs%20metric%20learning%20to%20guide%20the%0Aremoval%20of%20samples%20matching%20the%20nearest%20incorrect%20centroid%20in%20the%20embedding%0Aspace.%20Evaluation%20of%20the%20algorithm%27s%20performance%20is%20conducted%20across%20various%0Abenchmark%20datasets%20in%20two%20distinct%20scenarios%2C%20class%20removal%2C%20and%20homogeneous%0Asampling%20removal%2C%20obtaining%20state-of-the-art%20performance.%20We%20also%20introduce%20a%0Anovel%20metric%2C%20called%20Adaptive%20Unlearning%20Score%20%28AUS%29%2C%20encompassing%20not%20only%20the%0Aefficacy%20of%20the%20unlearning%20process%20in%20forgetting%20target%20data%20but%20also%0Aquantifying%20the%20performance%20loss%20relative%20to%20the%20original%20model.%20Additionally%2C%0Awe%20conducted%20a%20thorough%20investigation%20of%20the%20unlearning%20mechanism%20in%20DUCK%2C%0Aexamining%20its%20impact%20on%20the%20organization%20of%20the%20feature%20space%20and%20employing%0Aexplainable%20AI%20techniques%20for%20deeper%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02052v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDUCK%253A%2520Distance-based%2520Unlearning%2520via%2520Centroid%2520Kinematics%26entry.906535625%3DMarco%2520Cotogni%2520and%2520Jacopo%2520Bonato%2520and%2520Luigi%2520Sabetta%2520and%2520Francesco%2520Pelosin%2520and%2520Alessandro%2520Nicolosi%26entry.1292438233%3D%2520%2520Machine%2520Unlearning%2520is%2520rising%2520as%2520a%2520new%2520field%252C%2520driven%2520by%2520the%2520pressing%2520necessity%250Aof%2520ensuring%2520privacy%2520in%2520modern%2520artificial%2520intelligence%2520models.%2520This%2520technique%250Aprimarily%2520aims%2520to%2520eradicate%2520any%2520residual%2520influence%2520of%2520a%2520specific%2520subset%2520of%2520data%250Afrom%2520the%2520knowledge%2520acquired%2520by%2520a%2520neural%2520model%2520during%2520its%2520training.%2520This%2520work%250Aintroduces%2520a%2520novel%2520unlearning%2520algorithm%252C%2520denoted%2520as%2520Distance-based%2520Unlearning%250Avia%2520Centroid%2520Kinematics%2520%2528DUCK%2529%252C%2520which%2520employs%2520metric%2520learning%2520to%2520guide%2520the%250Aremoval%2520of%2520samples%2520matching%2520the%2520nearest%2520incorrect%2520centroid%2520in%2520the%2520embedding%250Aspace.%2520Evaluation%2520of%2520the%2520algorithm%2527s%2520performance%2520is%2520conducted%2520across%2520various%250Abenchmark%2520datasets%2520in%2520two%2520distinct%2520scenarios%252C%2520class%2520removal%252C%2520and%2520homogeneous%250Asampling%2520removal%252C%2520obtaining%2520state-of-the-art%2520performance.%2520We%2520also%2520introduce%2520a%250Anovel%2520metric%252C%2520called%2520Adaptive%2520Unlearning%2520Score%2520%2528AUS%2529%252C%2520encompassing%2520not%2520only%2520the%250Aefficacy%2520of%2520the%2520unlearning%2520process%2520in%2520forgetting%2520target%2520data%2520but%2520also%250Aquantifying%2520the%2520performance%2520loss%2520relative%2520to%2520the%2520original%2520model.%2520Additionally%252C%250Awe%2520conducted%2520a%2520thorough%2520investigation%2520of%2520the%2520unlearning%2520mechanism%2520in%2520DUCK%252C%250Aexamining%2520its%2520impact%2520on%2520the%2520organization%2520of%2520the%2520feature%2520space%2520and%2520employing%250Aexplainable%2520AI%2520techniques%2520for%2520deeper%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02052v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DUCK%3A%20Distance-based%20Unlearning%20via%20Centroid%20Kinematics&entry.906535625=Marco%20Cotogni%20and%20Jacopo%20Bonato%20and%20Luigi%20Sabetta%20and%20Francesco%20Pelosin%20and%20Alessandro%20Nicolosi&entry.1292438233=%20%20Machine%20Unlearning%20is%20rising%20as%20a%20new%20field%2C%20driven%20by%20the%20pressing%20necessity%0Aof%20ensuring%20privacy%20in%20modern%20artificial%20intelligence%20models.%20This%20technique%0Aprimarily%20aims%20to%20eradicate%20any%20residual%20influence%20of%20a%20specific%20subset%20of%20data%0Afrom%20the%20knowledge%20acquired%20by%20a%20neural%20model%20during%20its%20training.%20This%20work%0Aintroduces%20a%20novel%20unlearning%20algorithm%2C%20denoted%20as%20Distance-based%20Unlearning%0Avia%20Centroid%20Kinematics%20%28DUCK%29%2C%20which%20employs%20metric%20learning%20to%20guide%20the%0Aremoval%20of%20samples%20matching%20the%20nearest%20incorrect%20centroid%20in%20the%20embedding%0Aspace.%20Evaluation%20of%20the%20algorithm%27s%20performance%20is%20conducted%20across%20various%0Abenchmark%20datasets%20in%20two%20distinct%20scenarios%2C%20class%20removal%2C%20and%20homogeneous%0Asampling%20removal%2C%20obtaining%20state-of-the-art%20performance.%20We%20also%20introduce%20a%0Anovel%20metric%2C%20called%20Adaptive%20Unlearning%20Score%20%28AUS%29%2C%20encompassing%20not%20only%20the%0Aefficacy%20of%20the%20unlearning%20process%20in%20forgetting%20target%20data%20but%20also%0Aquantifying%20the%20performance%20loss%20relative%20to%20the%20original%20model.%20Additionally%2C%0Awe%20conducted%20a%20thorough%20investigation%20of%20the%20unlearning%20mechanism%20in%20DUCK%2C%0Aexamining%20its%20impact%20on%20the%20organization%20of%20the%20feature%20space%20and%20employing%0Aexplainable%20AI%20techniques%20for%20deeper%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02052v2&entry.124074799=Read"},
{"title": "Pose Priors from Language Models", "author": "Sanjay Subramanian and Evonne Ng and Lea M\u00fcller and Dan Klein and Shiry Ginosar and Trevor Darrell", "abstract": "  We present a zero-shot pose optimization method that enforces accurate\nphysical contact constraints when estimating the 3D pose of humans. Our central\ninsight is that since language is often used to describe physical interaction,\nlarge pretrained text-based models can act as priors on pose estimation.\n  We can thus leverage this insight to improve pose estimation by converting\nnatural language descriptors, generated by a large multimodal model (LMM), into\ntractable losses to constrain the 3D pose optimization. Despite its simplicity,\nour method produces surprisingly compelling pose reconstructions of people in\nclose contact, correctly capturing the semantics of the social and physical\ninteractions. We demonstrate that our method rivals more complex\nstate-of-the-art approaches that require expensive human annotation of contact\npoints and training specialized models. Moreover, unlike previous approaches,\nour method provides a unified framework for resolving self-contact and\nperson-to-person contact.\n", "link": "http://arxiv.org/abs/2405.03689v1", "date": "2024-05-06", "relevancy": 2.1366, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.561}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.518}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pose%20Priors%20from%20Language%20Models&body=Title%3A%20Pose%20Priors%20from%20Language%20Models%0AAuthor%3A%20Sanjay%20Subramanian%20and%20Evonne%20Ng%20and%20Lea%20M%C3%BCller%20and%20Dan%20Klein%20and%20Shiry%20Ginosar%20and%20Trevor%20Darrell%0AAbstract%3A%20%20%20We%20present%20a%20zero-shot%20pose%20optimization%20method%20that%20enforces%20accurate%0Aphysical%20contact%20constraints%20when%20estimating%20the%203D%20pose%20of%20humans.%20Our%20central%0Ainsight%20is%20that%20since%20language%20is%20often%20used%20to%20describe%20physical%20interaction%2C%0Alarge%20pretrained%20text-based%20models%20can%20act%20as%20priors%20on%20pose%20estimation.%0A%20%20We%20can%20thus%20leverage%20this%20insight%20to%20improve%20pose%20estimation%20by%20converting%0Anatural%20language%20descriptors%2C%20generated%20by%20a%20large%20multimodal%20model%20%28LMM%29%2C%20into%0Atractable%20losses%20to%20constrain%20the%203D%20pose%20optimization.%20Despite%20its%20simplicity%2C%0Aour%20method%20produces%20surprisingly%20compelling%20pose%20reconstructions%20of%20people%20in%0Aclose%20contact%2C%20correctly%20capturing%20the%20semantics%20of%20the%20social%20and%20physical%0Ainteractions.%20We%20demonstrate%20that%20our%20method%20rivals%20more%20complex%0Astate-of-the-art%20approaches%20that%20require%20expensive%20human%20annotation%20of%20contact%0Apoints%20and%20training%20specialized%20models.%20Moreover%2C%20unlike%20previous%20approaches%2C%0Aour%20method%20provides%20a%20unified%20framework%20for%20resolving%20self-contact%20and%0Aperson-to-person%20contact.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPose%2520Priors%2520from%2520Language%2520Models%26entry.906535625%3DSanjay%2520Subramanian%2520and%2520Evonne%2520Ng%2520and%2520Lea%2520M%25C3%25BCller%2520and%2520Dan%2520Klein%2520and%2520Shiry%2520Ginosar%2520and%2520Trevor%2520Darrell%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520zero-shot%2520pose%2520optimization%2520method%2520that%2520enforces%2520accurate%250Aphysical%2520contact%2520constraints%2520when%2520estimating%2520the%25203D%2520pose%2520of%2520humans.%2520Our%2520central%250Ainsight%2520is%2520that%2520since%2520language%2520is%2520often%2520used%2520to%2520describe%2520physical%2520interaction%252C%250Alarge%2520pretrained%2520text-based%2520models%2520can%2520act%2520as%2520priors%2520on%2520pose%2520estimation.%250A%2520%2520We%2520can%2520thus%2520leverage%2520this%2520insight%2520to%2520improve%2520pose%2520estimation%2520by%2520converting%250Anatural%2520language%2520descriptors%252C%2520generated%2520by%2520a%2520large%2520multimodal%2520model%2520%2528LMM%2529%252C%2520into%250Atractable%2520losses%2520to%2520constrain%2520the%25203D%2520pose%2520optimization.%2520Despite%2520its%2520simplicity%252C%250Aour%2520method%2520produces%2520surprisingly%2520compelling%2520pose%2520reconstructions%2520of%2520people%2520in%250Aclose%2520contact%252C%2520correctly%2520capturing%2520the%2520semantics%2520of%2520the%2520social%2520and%2520physical%250Ainteractions.%2520We%2520demonstrate%2520that%2520our%2520method%2520rivals%2520more%2520complex%250Astate-of-the-art%2520approaches%2520that%2520require%2520expensive%2520human%2520annotation%2520of%2520contact%250Apoints%2520and%2520training%2520specialized%2520models.%2520Moreover%252C%2520unlike%2520previous%2520approaches%252C%250Aour%2520method%2520provides%2520a%2520unified%2520framework%2520for%2520resolving%2520self-contact%2520and%250Aperson-to-person%2520contact.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pose%20Priors%20from%20Language%20Models&entry.906535625=Sanjay%20Subramanian%20and%20Evonne%20Ng%20and%20Lea%20M%C3%BCller%20and%20Dan%20Klein%20and%20Shiry%20Ginosar%20and%20Trevor%20Darrell&entry.1292438233=%20%20We%20present%20a%20zero-shot%20pose%20optimization%20method%20that%20enforces%20accurate%0Aphysical%20contact%20constraints%20when%20estimating%20the%203D%20pose%20of%20humans.%20Our%20central%0Ainsight%20is%20that%20since%20language%20is%20often%20used%20to%20describe%20physical%20interaction%2C%0Alarge%20pretrained%20text-based%20models%20can%20act%20as%20priors%20on%20pose%20estimation.%0A%20%20We%20can%20thus%20leverage%20this%20insight%20to%20improve%20pose%20estimation%20by%20converting%0Anatural%20language%20descriptors%2C%20generated%20by%20a%20large%20multimodal%20model%20%28LMM%29%2C%20into%0Atractable%20losses%20to%20constrain%20the%203D%20pose%20optimization.%20Despite%20its%20simplicity%2C%0Aour%20method%20produces%20surprisingly%20compelling%20pose%20reconstructions%20of%20people%20in%0Aclose%20contact%2C%20correctly%20capturing%20the%20semantics%20of%20the%20social%20and%20physical%0Ainteractions.%20We%20demonstrate%20that%20our%20method%20rivals%20more%20complex%0Astate-of-the-art%20approaches%20that%20require%20expensive%20human%20annotation%20of%20contact%0Apoints%20and%20training%20specialized%20models.%20Moreover%2C%20unlike%20previous%20approaches%2C%0Aour%20method%20provides%20a%20unified%20framework%20for%20resolving%20self-contact%20and%0Aperson-to-person%20contact.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03689v1&entry.124074799=Read"},
{"title": "Statistical Edge Detection And UDF Learning For Shape Representation", "author": "Virgile Foy and Fabrice Gamboa and Reda Chhaibi", "abstract": "  In the field of computer vision, the numerical encoding of 3D surfaces is\ncrucial. It is classical to represent surfaces with their Signed Distance\nFunctions (SDFs) or Unsigned Distance Functions (UDFs). For tasks like\nrepresentation learning, surface classification, or surface reconstruction,\nthis function can be learned by a neural network, called Neural Distance\nFunction. This network, and in particular its weights, may serve as a\nparametric and implicit representation for the surface. The network must\nrepresent the surface as accurately as possible. In this paper, we propose a\nmethod for learning UDFs that improves the fidelity of the obtained Neural UDF\nto the original 3D surface. The key idea of our method is to concentrate the\nlearning effort of the Neural UDF on surface edges. More precisely, we show\nthat sampling more training points around surface edges allows better local\naccuracy of the trained Neural UDF, and thus improves the global expressiveness\nof the Neural UDF in terms of Hausdorff distance. To detect surface edges, we\npropose a new statistical method based on the calculation of a $p$-value at\neach point on the surface. Our method is shown to detect surface edges more\naccurately than a commonly used local geometric descriptor.\n", "link": "http://arxiv.org/abs/2405.03381v1", "date": "2024-05-06", "relevancy": 2.1278, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5484}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5434}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Statistical%20Edge%20Detection%20And%20UDF%20Learning%20For%20Shape%20Representation&body=Title%3A%20Statistical%20Edge%20Detection%20And%20UDF%20Learning%20For%20Shape%20Representation%0AAuthor%3A%20Virgile%20Foy%20and%20Fabrice%20Gamboa%20and%20Reda%20Chhaibi%0AAbstract%3A%20%20%20In%20the%20field%20of%20computer%20vision%2C%20the%20numerical%20encoding%20of%203D%20surfaces%20is%0Acrucial.%20It%20is%20classical%20to%20represent%20surfaces%20with%20their%20Signed%20Distance%0AFunctions%20%28SDFs%29%20or%20Unsigned%20Distance%20Functions%20%28UDFs%29.%20For%20tasks%20like%0Arepresentation%20learning%2C%20surface%20classification%2C%20or%20surface%20reconstruction%2C%0Athis%20function%20can%20be%20learned%20by%20a%20neural%20network%2C%20called%20Neural%20Distance%0AFunction.%20This%20network%2C%20and%20in%20particular%20its%20weights%2C%20may%20serve%20as%20a%0Aparametric%20and%20implicit%20representation%20for%20the%20surface.%20The%20network%20must%0Arepresent%20the%20surface%20as%20accurately%20as%20possible.%20In%20this%20paper%2C%20we%20propose%20a%0Amethod%20for%20learning%20UDFs%20that%20improves%20the%20fidelity%20of%20the%20obtained%20Neural%20UDF%0Ato%20the%20original%203D%20surface.%20The%20key%20idea%20of%20our%20method%20is%20to%20concentrate%20the%0Alearning%20effort%20of%20the%20Neural%20UDF%20on%20surface%20edges.%20More%20precisely%2C%20we%20show%0Athat%20sampling%20more%20training%20points%20around%20surface%20edges%20allows%20better%20local%0Aaccuracy%20of%20the%20trained%20Neural%20UDF%2C%20and%20thus%20improves%20the%20global%20expressiveness%0Aof%20the%20Neural%20UDF%20in%20terms%20of%20Hausdorff%20distance.%20To%20detect%20surface%20edges%2C%20we%0Apropose%20a%20new%20statistical%20method%20based%20on%20the%20calculation%20of%20a%20%24p%24-value%20at%0Aeach%20point%20on%20the%20surface.%20Our%20method%20is%20shown%20to%20detect%20surface%20edges%20more%0Aaccurately%20than%20a%20commonly%20used%20local%20geometric%20descriptor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatistical%2520Edge%2520Detection%2520And%2520UDF%2520Learning%2520For%2520Shape%2520Representation%26entry.906535625%3DVirgile%2520Foy%2520and%2520Fabrice%2520Gamboa%2520and%2520Reda%2520Chhaibi%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520computer%2520vision%252C%2520the%2520numerical%2520encoding%2520of%25203D%2520surfaces%2520is%250Acrucial.%2520It%2520is%2520classical%2520to%2520represent%2520surfaces%2520with%2520their%2520Signed%2520Distance%250AFunctions%2520%2528SDFs%2529%2520or%2520Unsigned%2520Distance%2520Functions%2520%2528UDFs%2529.%2520For%2520tasks%2520like%250Arepresentation%2520learning%252C%2520surface%2520classification%252C%2520or%2520surface%2520reconstruction%252C%250Athis%2520function%2520can%2520be%2520learned%2520by%2520a%2520neural%2520network%252C%2520called%2520Neural%2520Distance%250AFunction.%2520This%2520network%252C%2520and%2520in%2520particular%2520its%2520weights%252C%2520may%2520serve%2520as%2520a%250Aparametric%2520and%2520implicit%2520representation%2520for%2520the%2520surface.%2520The%2520network%2520must%250Arepresent%2520the%2520surface%2520as%2520accurately%2520as%2520possible.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Amethod%2520for%2520learning%2520UDFs%2520that%2520improves%2520the%2520fidelity%2520of%2520the%2520obtained%2520Neural%2520UDF%250Ato%2520the%2520original%25203D%2520surface.%2520The%2520key%2520idea%2520of%2520our%2520method%2520is%2520to%2520concentrate%2520the%250Alearning%2520effort%2520of%2520the%2520Neural%2520UDF%2520on%2520surface%2520edges.%2520More%2520precisely%252C%2520we%2520show%250Athat%2520sampling%2520more%2520training%2520points%2520around%2520surface%2520edges%2520allows%2520better%2520local%250Aaccuracy%2520of%2520the%2520trained%2520Neural%2520UDF%252C%2520and%2520thus%2520improves%2520the%2520global%2520expressiveness%250Aof%2520the%2520Neural%2520UDF%2520in%2520terms%2520of%2520Hausdorff%2520distance.%2520To%2520detect%2520surface%2520edges%252C%2520we%250Apropose%2520a%2520new%2520statistical%2520method%2520based%2520on%2520the%2520calculation%2520of%2520a%2520%2524p%2524-value%2520at%250Aeach%2520point%2520on%2520the%2520surface.%2520Our%2520method%2520is%2520shown%2520to%2520detect%2520surface%2520edges%2520more%250Aaccurately%2520than%2520a%2520commonly%2520used%2520local%2520geometric%2520descriptor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Statistical%20Edge%20Detection%20And%20UDF%20Learning%20For%20Shape%20Representation&entry.906535625=Virgile%20Foy%20and%20Fabrice%20Gamboa%20and%20Reda%20Chhaibi&entry.1292438233=%20%20In%20the%20field%20of%20computer%20vision%2C%20the%20numerical%20encoding%20of%203D%20surfaces%20is%0Acrucial.%20It%20is%20classical%20to%20represent%20surfaces%20with%20their%20Signed%20Distance%0AFunctions%20%28SDFs%29%20or%20Unsigned%20Distance%20Functions%20%28UDFs%29.%20For%20tasks%20like%0Arepresentation%20learning%2C%20surface%20classification%2C%20or%20surface%20reconstruction%2C%0Athis%20function%20can%20be%20learned%20by%20a%20neural%20network%2C%20called%20Neural%20Distance%0AFunction.%20This%20network%2C%20and%20in%20particular%20its%20weights%2C%20may%20serve%20as%20a%0Aparametric%20and%20implicit%20representation%20for%20the%20surface.%20The%20network%20must%0Arepresent%20the%20surface%20as%20accurately%20as%20possible.%20In%20this%20paper%2C%20we%20propose%20a%0Amethod%20for%20learning%20UDFs%20that%20improves%20the%20fidelity%20of%20the%20obtained%20Neural%20UDF%0Ato%20the%20original%203D%20surface.%20The%20key%20idea%20of%20our%20method%20is%20to%20concentrate%20the%0Alearning%20effort%20of%20the%20Neural%20UDF%20on%20surface%20edges.%20More%20precisely%2C%20we%20show%0Athat%20sampling%20more%20training%20points%20around%20surface%20edges%20allows%20better%20local%0Aaccuracy%20of%20the%20trained%20Neural%20UDF%2C%20and%20thus%20improves%20the%20global%20expressiveness%0Aof%20the%20Neural%20UDF%20in%20terms%20of%20Hausdorff%20distance.%20To%20detect%20surface%20edges%2C%20we%0Apropose%20a%20new%20statistical%20method%20based%20on%20the%20calculation%20of%20a%20%24p%24-value%20at%0Aeach%20point%20on%20the%20surface.%20Our%20method%20is%20shown%20to%20detect%20surface%20edges%20more%0Aaccurately%20than%20a%20commonly%20used%20local%20geometric%20descriptor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03381v1&entry.124074799=Read"},
{"title": "Light-VQA+: A Video Quality Assessment Model for Exposure Correction\n  with Vision-Language Guidance", "author": "Xunchu Zhou and Xiaohong Liu and Yunlong Dong and Tengchuan Kou and Yixuan Gao and Zicheng Zhang and Chunyi Li and Haoning Wu and Guangtao Zhai", "abstract": "  Recently, User-Generated Content (UGC) videos have gained popularity in our\ndaily lives. However, UGC videos often suffer from poor exposure due to the\nlimitations of photographic equipment and techniques. Therefore, Video Exposure\nCorrection (VEC) algorithms have been proposed, Low-Light Video Enhancement\n(LLVE) and Over-Exposed Video Recovery (OEVR) included. Equally important to\nthe VEC is the Video Quality Assessment (VQA). Unfortunately, almost all\nexisting VQA models are built generally, measuring the quality of a video from\na comprehensive perspective. As a result, Light-VQA, trained on LLVE-QA, is\nproposed for assessing LLVE. We extend the work of Light-VQA by expanding the\nLLVE-QA dataset into Video Exposure Correction Quality Assessment (VEC-QA)\ndataset with over-exposed videos and their corresponding corrected versions. In\naddition, we propose Light-VQA+, a VQA model specialized in assessing VEC.\nLight-VQA+ differs from Light-VQA mainly from the usage of the CLIP model and\nthe vision-language guidance during the feature extraction, followed by a new\nmodule referring to the Human Visual System (HVS) for more accurate assessment.\nExtensive experimental results show that our model achieves the best\nperformance against the current State-Of-The-Art (SOTA) VQA models on the\nVEC-QA dataset and other public datasets.\n", "link": "http://arxiv.org/abs/2405.03333v1", "date": "2024-05-06", "relevancy": 2.1248, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5396}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5259}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Light-VQA%2B%3A%20A%20Video%20Quality%20Assessment%20Model%20for%20Exposure%20Correction%0A%20%20with%20Vision-Language%20Guidance&body=Title%3A%20Light-VQA%2B%3A%20A%20Video%20Quality%20Assessment%20Model%20for%20Exposure%20Correction%0A%20%20with%20Vision-Language%20Guidance%0AAuthor%3A%20Xunchu%20Zhou%20and%20Xiaohong%20Liu%20and%20Yunlong%20Dong%20and%20Tengchuan%20Kou%20and%20Yixuan%20Gao%20and%20Zicheng%20Zhang%20and%20Chunyi%20Li%20and%20Haoning%20Wu%20and%20Guangtao%20Zhai%0AAbstract%3A%20%20%20Recently%2C%20User-Generated%20Content%20%28UGC%29%20videos%20have%20gained%20popularity%20in%20our%0Adaily%20lives.%20However%2C%20UGC%20videos%20often%20suffer%20from%20poor%20exposure%20due%20to%20the%0Alimitations%20of%20photographic%20equipment%20and%20techniques.%20Therefore%2C%20Video%20Exposure%0ACorrection%20%28VEC%29%20algorithms%20have%20been%20proposed%2C%20Low-Light%20Video%20Enhancement%0A%28LLVE%29%20and%20Over-Exposed%20Video%20Recovery%20%28OEVR%29%20included.%20Equally%20important%20to%0Athe%20VEC%20is%20the%20Video%20Quality%20Assessment%20%28VQA%29.%20Unfortunately%2C%20almost%20all%0Aexisting%20VQA%20models%20are%20built%20generally%2C%20measuring%20the%20quality%20of%20a%20video%20from%0Aa%20comprehensive%20perspective.%20As%20a%20result%2C%20Light-VQA%2C%20trained%20on%20LLVE-QA%2C%20is%0Aproposed%20for%20assessing%20LLVE.%20We%20extend%20the%20work%20of%20Light-VQA%20by%20expanding%20the%0ALLVE-QA%20dataset%20into%20Video%20Exposure%20Correction%20Quality%20Assessment%20%28VEC-QA%29%0Adataset%20with%20over-exposed%20videos%20and%20their%20corresponding%20corrected%20versions.%20In%0Aaddition%2C%20we%20propose%20Light-VQA%2B%2C%20a%20VQA%20model%20specialized%20in%20assessing%20VEC.%0ALight-VQA%2B%20differs%20from%20Light-VQA%20mainly%20from%20the%20usage%20of%20the%20CLIP%20model%20and%0Athe%20vision-language%20guidance%20during%20the%20feature%20extraction%2C%20followed%20by%20a%20new%0Amodule%20referring%20to%20the%20Human%20Visual%20System%20%28HVS%29%20for%20more%20accurate%20assessment.%0AExtensive%20experimental%20results%20show%20that%20our%20model%20achieves%20the%20best%0Aperformance%20against%20the%20current%20State-Of-The-Art%20%28SOTA%29%20VQA%20models%20on%20the%0AVEC-QA%20dataset%20and%20other%20public%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLight-VQA%252B%253A%2520A%2520Video%2520Quality%2520Assessment%2520Model%2520for%2520Exposure%2520Correction%250A%2520%2520with%2520Vision-Language%2520Guidance%26entry.906535625%3DXunchu%2520Zhou%2520and%2520Xiaohong%2520Liu%2520and%2520Yunlong%2520Dong%2520and%2520Tengchuan%2520Kou%2520and%2520Yixuan%2520Gao%2520and%2520Zicheng%2520Zhang%2520and%2520Chunyi%2520Li%2520and%2520Haoning%2520Wu%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3D%2520%2520Recently%252C%2520User-Generated%2520Content%2520%2528UGC%2529%2520videos%2520have%2520gained%2520popularity%2520in%2520our%250Adaily%2520lives.%2520However%252C%2520UGC%2520videos%2520often%2520suffer%2520from%2520poor%2520exposure%2520due%2520to%2520the%250Alimitations%2520of%2520photographic%2520equipment%2520and%2520techniques.%2520Therefore%252C%2520Video%2520Exposure%250ACorrection%2520%2528VEC%2529%2520algorithms%2520have%2520been%2520proposed%252C%2520Low-Light%2520Video%2520Enhancement%250A%2528LLVE%2529%2520and%2520Over-Exposed%2520Video%2520Recovery%2520%2528OEVR%2529%2520included.%2520Equally%2520important%2520to%250Athe%2520VEC%2520is%2520the%2520Video%2520Quality%2520Assessment%2520%2528VQA%2529.%2520Unfortunately%252C%2520almost%2520all%250Aexisting%2520VQA%2520models%2520are%2520built%2520generally%252C%2520measuring%2520the%2520quality%2520of%2520a%2520video%2520from%250Aa%2520comprehensive%2520perspective.%2520As%2520a%2520result%252C%2520Light-VQA%252C%2520trained%2520on%2520LLVE-QA%252C%2520is%250Aproposed%2520for%2520assessing%2520LLVE.%2520We%2520extend%2520the%2520work%2520of%2520Light-VQA%2520by%2520expanding%2520the%250ALLVE-QA%2520dataset%2520into%2520Video%2520Exposure%2520Correction%2520Quality%2520Assessment%2520%2528VEC-QA%2529%250Adataset%2520with%2520over-exposed%2520videos%2520and%2520their%2520corresponding%2520corrected%2520versions.%2520In%250Aaddition%252C%2520we%2520propose%2520Light-VQA%252B%252C%2520a%2520VQA%2520model%2520specialized%2520in%2520assessing%2520VEC.%250ALight-VQA%252B%2520differs%2520from%2520Light-VQA%2520mainly%2520from%2520the%2520usage%2520of%2520the%2520CLIP%2520model%2520and%250Athe%2520vision-language%2520guidance%2520during%2520the%2520feature%2520extraction%252C%2520followed%2520by%2520a%2520new%250Amodule%2520referring%2520to%2520the%2520Human%2520Visual%2520System%2520%2528HVS%2529%2520for%2520more%2520accurate%2520assessment.%250AExtensive%2520experimental%2520results%2520show%2520that%2520our%2520model%2520achieves%2520the%2520best%250Aperformance%2520against%2520the%2520current%2520State-Of-The-Art%2520%2528SOTA%2529%2520VQA%2520models%2520on%2520the%250AVEC-QA%2520dataset%2520and%2520other%2520public%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Light-VQA%2B%3A%20A%20Video%20Quality%20Assessment%20Model%20for%20Exposure%20Correction%0A%20%20with%20Vision-Language%20Guidance&entry.906535625=Xunchu%20Zhou%20and%20Xiaohong%20Liu%20and%20Yunlong%20Dong%20and%20Tengchuan%20Kou%20and%20Yixuan%20Gao%20and%20Zicheng%20Zhang%20and%20Chunyi%20Li%20and%20Haoning%20Wu%20and%20Guangtao%20Zhai&entry.1292438233=%20%20Recently%2C%20User-Generated%20Content%20%28UGC%29%20videos%20have%20gained%20popularity%20in%20our%0Adaily%20lives.%20However%2C%20UGC%20videos%20often%20suffer%20from%20poor%20exposure%20due%20to%20the%0Alimitations%20of%20photographic%20equipment%20and%20techniques.%20Therefore%2C%20Video%20Exposure%0ACorrection%20%28VEC%29%20algorithms%20have%20been%20proposed%2C%20Low-Light%20Video%20Enhancement%0A%28LLVE%29%20and%20Over-Exposed%20Video%20Recovery%20%28OEVR%29%20included.%20Equally%20important%20to%0Athe%20VEC%20is%20the%20Video%20Quality%20Assessment%20%28VQA%29.%20Unfortunately%2C%20almost%20all%0Aexisting%20VQA%20models%20are%20built%20generally%2C%20measuring%20the%20quality%20of%20a%20video%20from%0Aa%20comprehensive%20perspective.%20As%20a%20result%2C%20Light-VQA%2C%20trained%20on%20LLVE-QA%2C%20is%0Aproposed%20for%20assessing%20LLVE.%20We%20extend%20the%20work%20of%20Light-VQA%20by%20expanding%20the%0ALLVE-QA%20dataset%20into%20Video%20Exposure%20Correction%20Quality%20Assessment%20%28VEC-QA%29%0Adataset%20with%20over-exposed%20videos%20and%20their%20corresponding%20corrected%20versions.%20In%0Aaddition%2C%20we%20propose%20Light-VQA%2B%2C%20a%20VQA%20model%20specialized%20in%20assessing%20VEC.%0ALight-VQA%2B%20differs%20from%20Light-VQA%20mainly%20from%20the%20usage%20of%20the%20CLIP%20model%20and%0Athe%20vision-language%20guidance%20during%20the%20feature%20extraction%2C%20followed%20by%20a%20new%0Amodule%20referring%20to%20the%20Human%20Visual%20System%20%28HVS%29%20for%20more%20accurate%20assessment.%0AExtensive%20experimental%20results%20show%20that%20our%20model%20achieves%20the%20best%0Aperformance%20against%20the%20current%20State-Of-The-Art%20%28SOTA%29%20VQA%20models%20on%20the%0AVEC-QA%20dataset%20and%20other%20public%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03333v1&entry.124074799=Read"},
{"title": "Enhancing DETRs Variants through Improved Content Query and Similar\n  Query Aggregation", "author": "Yingying Zhang and Chuangji Shi and Xin Guo and Jiangwei Lao and Jian Wang and Jiaotuan Wang and Jingdong Chen", "abstract": "  The design of the query is crucial for the performance of DETR and its\nvariants. Each query consists of two components: a content part and a\npositional one. Traditionally, the content query is initialized with a zero or\nlearnable embedding, lacking essential content information and resulting in\nsub-optimal performance. In this paper, we introduce a novel plug-and-play\nmodule, Self-Adaptive Content Query (SACQ), to address this limitation. The\nSACQ module utilizes features from the transformer encoder to generate content\nqueries via self-attention pooling. This allows candidate queries to adapt to\nthe input image, resulting in a more comprehensive content prior and better\nfocus on target objects. However, this improved concentration poses a challenge\nfor the training process that utilizes the Hungarian matching, which selects\nonly a single candidate and suppresses other similar ones. To overcome this, we\npropose a query aggregation strategy to cooperate with SACQ. It merges similar\npredicted candidates from different queries, easing the optimization. Our\nextensive experiments on the COCO dataset demonstrate the effectiveness of our\nproposed approaches across six different DETR's variants with multiple\nconfigurations, achieving an average improvement of over 1.0 AP.\n", "link": "http://arxiv.org/abs/2405.03318v1", "date": "2024-05-06", "relevancy": 2.1061, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5448}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5165}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20DETRs%20Variants%20through%20Improved%20Content%20Query%20and%20Similar%0A%20%20Query%20Aggregation&body=Title%3A%20Enhancing%20DETRs%20Variants%20through%20Improved%20Content%20Query%20and%20Similar%0A%20%20Query%20Aggregation%0AAuthor%3A%20Yingying%20Zhang%20and%20Chuangji%20Shi%20and%20Xin%20Guo%20and%20Jiangwei%20Lao%20and%20Jian%20Wang%20and%20Jiaotuan%20Wang%20and%20Jingdong%20Chen%0AAbstract%3A%20%20%20The%20design%20of%20the%20query%20is%20crucial%20for%20the%20performance%20of%20DETR%20and%20its%0Avariants.%20Each%20query%20consists%20of%20two%20components%3A%20a%20content%20part%20and%20a%0Apositional%20one.%20Traditionally%2C%20the%20content%20query%20is%20initialized%20with%20a%20zero%20or%0Alearnable%20embedding%2C%20lacking%20essential%20content%20information%20and%20resulting%20in%0Asub-optimal%20performance.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20plug-and-play%0Amodule%2C%20Self-Adaptive%20Content%20Query%20%28SACQ%29%2C%20to%20address%20this%20limitation.%20The%0ASACQ%20module%20utilizes%20features%20from%20the%20transformer%20encoder%20to%20generate%20content%0Aqueries%20via%20self-attention%20pooling.%20This%20allows%20candidate%20queries%20to%20adapt%20to%0Athe%20input%20image%2C%20resulting%20in%20a%20more%20comprehensive%20content%20prior%20and%20better%0Afocus%20on%20target%20objects.%20However%2C%20this%20improved%20concentration%20poses%20a%20challenge%0Afor%20the%20training%20process%20that%20utilizes%20the%20Hungarian%20matching%2C%20which%20selects%0Aonly%20a%20single%20candidate%20and%20suppresses%20other%20similar%20ones.%20To%20overcome%20this%2C%20we%0Apropose%20a%20query%20aggregation%20strategy%20to%20cooperate%20with%20SACQ.%20It%20merges%20similar%0Apredicted%20candidates%20from%20different%20queries%2C%20easing%20the%20optimization.%20Our%0Aextensive%20experiments%20on%20the%20COCO%20dataset%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20approaches%20across%20six%20different%20DETR%27s%20variants%20with%20multiple%0Aconfigurations%2C%20achieving%20an%20average%20improvement%20of%20over%201.0%20AP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520DETRs%2520Variants%2520through%2520Improved%2520Content%2520Query%2520and%2520Similar%250A%2520%2520Query%2520Aggregation%26entry.906535625%3DYingying%2520Zhang%2520and%2520Chuangji%2520Shi%2520and%2520Xin%2520Guo%2520and%2520Jiangwei%2520Lao%2520and%2520Jian%2520Wang%2520and%2520Jiaotuan%2520Wang%2520and%2520Jingdong%2520Chen%26entry.1292438233%3D%2520%2520The%2520design%2520of%2520the%2520query%2520is%2520crucial%2520for%2520the%2520performance%2520of%2520DETR%2520and%2520its%250Avariants.%2520Each%2520query%2520consists%2520of%2520two%2520components%253A%2520a%2520content%2520part%2520and%2520a%250Apositional%2520one.%2520Traditionally%252C%2520the%2520content%2520query%2520is%2520initialized%2520with%2520a%2520zero%2520or%250Alearnable%2520embedding%252C%2520lacking%2520essential%2520content%2520information%2520and%2520resulting%2520in%250Asub-optimal%2520performance.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520plug-and-play%250Amodule%252C%2520Self-Adaptive%2520Content%2520Query%2520%2528SACQ%2529%252C%2520to%2520address%2520this%2520limitation.%2520The%250ASACQ%2520module%2520utilizes%2520features%2520from%2520the%2520transformer%2520encoder%2520to%2520generate%2520content%250Aqueries%2520via%2520self-attention%2520pooling.%2520This%2520allows%2520candidate%2520queries%2520to%2520adapt%2520to%250Athe%2520input%2520image%252C%2520resulting%2520in%2520a%2520more%2520comprehensive%2520content%2520prior%2520and%2520better%250Afocus%2520on%2520target%2520objects.%2520However%252C%2520this%2520improved%2520concentration%2520poses%2520a%2520challenge%250Afor%2520the%2520training%2520process%2520that%2520utilizes%2520the%2520Hungarian%2520matching%252C%2520which%2520selects%250Aonly%2520a%2520single%2520candidate%2520and%2520suppresses%2520other%2520similar%2520ones.%2520To%2520overcome%2520this%252C%2520we%250Apropose%2520a%2520query%2520aggregation%2520strategy%2520to%2520cooperate%2520with%2520SACQ.%2520It%2520merges%2520similar%250Apredicted%2520candidates%2520from%2520different%2520queries%252C%2520easing%2520the%2520optimization.%2520Our%250Aextensive%2520experiments%2520on%2520the%2520COCO%2520dataset%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520approaches%2520across%2520six%2520different%2520DETR%2527s%2520variants%2520with%2520multiple%250Aconfigurations%252C%2520achieving%2520an%2520average%2520improvement%2520of%2520over%25201.0%2520AP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20DETRs%20Variants%20through%20Improved%20Content%20Query%20and%20Similar%0A%20%20Query%20Aggregation&entry.906535625=Yingying%20Zhang%20and%20Chuangji%20Shi%20and%20Xin%20Guo%20and%20Jiangwei%20Lao%20and%20Jian%20Wang%20and%20Jiaotuan%20Wang%20and%20Jingdong%20Chen&entry.1292438233=%20%20The%20design%20of%20the%20query%20is%20crucial%20for%20the%20performance%20of%20DETR%20and%20its%0Avariants.%20Each%20query%20consists%20of%20two%20components%3A%20a%20content%20part%20and%20a%0Apositional%20one.%20Traditionally%2C%20the%20content%20query%20is%20initialized%20with%20a%20zero%20or%0Alearnable%20embedding%2C%20lacking%20essential%20content%20information%20and%20resulting%20in%0Asub-optimal%20performance.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20plug-and-play%0Amodule%2C%20Self-Adaptive%20Content%20Query%20%28SACQ%29%2C%20to%20address%20this%20limitation.%20The%0ASACQ%20module%20utilizes%20features%20from%20the%20transformer%20encoder%20to%20generate%20content%0Aqueries%20via%20self-attention%20pooling.%20This%20allows%20candidate%20queries%20to%20adapt%20to%0Athe%20input%20image%2C%20resulting%20in%20a%20more%20comprehensive%20content%20prior%20and%20better%0Afocus%20on%20target%20objects.%20However%2C%20this%20improved%20concentration%20poses%20a%20challenge%0Afor%20the%20training%20process%20that%20utilizes%20the%20Hungarian%20matching%2C%20which%20selects%0Aonly%20a%20single%20candidate%20and%20suppresses%20other%20similar%20ones.%20To%20overcome%20this%2C%20we%0Apropose%20a%20query%20aggregation%20strategy%20to%20cooperate%20with%20SACQ.%20It%20merges%20similar%0Apredicted%20candidates%20from%20different%20queries%2C%20easing%20the%20optimization.%20Our%0Aextensive%20experiments%20on%20the%20COCO%20dataset%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20approaches%20across%20six%20different%20DETR%27s%20variants%20with%20multiple%0Aconfigurations%2C%20achieving%20an%20average%20improvement%20of%20over%201.0%20AP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03318v1&entry.124074799=Read"},
{"title": "Federated Learning Across Decentralized and Unshared Archives for Remote\n  Sensing Image Classification", "author": "Bar\u0131\u015f B\u00fcy\u00fckta\u015f and Gencer Sumbul and Beg\u00fcm Demir", "abstract": "  Federated learning (FL) enables the collaboration of multiple deep learning\nmodels to learn from decentralized data archives (i.e., clients) without\naccessing data on clients. Although FL offers ample opportunities in knowledge\ndiscovery from distributed image archives, it is seldom considered in remote\nsensing (RS). In this paper, as a first time in RS, we present a comparative\nstudy of state-of-the-art FL algorithms for RS image classification problems.\nTo this end, we initially provide a systematic review of the FL algorithms\npresented in the computer vision and machine learning communities. Then, we\nselect several state-of-the-art FL algorithms based on their effectiveness with\nrespect to training data heterogeneity across clients (known as non-IID data).\nAfter presenting an extensive overview of the selected algorithms, a\ntheoretical comparison of the algorithms is conducted based on their: 1) local\ntraining complexity; 2) aggregation complexity; 3) learning efficiency; 4)\ncommunication cost; and 5) scalability in terms of number of clients. After the\ntheoretical comparison, experimental analyses are presented to compare them\nunder different decentralization scenarios. For the experimental analyses, we\nfocus our attention on multi-label image classification problems in RS. Based\non our comprehensive analyses, we finally derive a guideline for selecting\nsuitable FL algorithms in RS. The code of this work will be publicly available\nat https://git.tu-berlin.de/rsim/FL-RS.\n", "link": "http://arxiv.org/abs/2311.06141v2", "date": "2024-05-06", "relevancy": 2.1011, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5406}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5158}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Learning%20Across%20Decentralized%20and%20Unshared%20Archives%20for%20Remote%0A%20%20Sensing%20Image%20Classification&body=Title%3A%20Federated%20Learning%20Across%20Decentralized%20and%20Unshared%20Archives%20for%20Remote%0A%20%20Sensing%20Image%20Classification%0AAuthor%3A%20Bar%C4%B1%C5%9F%20B%C3%BCy%C3%BCkta%C5%9F%20and%20Gencer%20Sumbul%20and%20Beg%C3%BCm%20Demir%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20enables%20the%20collaboration%20of%20multiple%20deep%20learning%0Amodels%20to%20learn%20from%20decentralized%20data%20archives%20%28i.e.%2C%20clients%29%20without%0Aaccessing%20data%20on%20clients.%20Although%20FL%20offers%20ample%20opportunities%20in%20knowledge%0Adiscovery%20from%20distributed%20image%20archives%2C%20it%20is%20seldom%20considered%20in%20remote%0Asensing%20%28RS%29.%20In%20this%20paper%2C%20as%20a%20first%20time%20in%20RS%2C%20we%20present%20a%20comparative%0Astudy%20of%20state-of-the-art%20FL%20algorithms%20for%20RS%20image%20classification%20problems.%0ATo%20this%20end%2C%20we%20initially%20provide%20a%20systematic%20review%20of%20the%20FL%20algorithms%0Apresented%20in%20the%20computer%20vision%20and%20machine%20learning%20communities.%20Then%2C%20we%0Aselect%20several%20state-of-the-art%20FL%20algorithms%20based%20on%20their%20effectiveness%20with%0Arespect%20to%20training%20data%20heterogeneity%20across%20clients%20%28known%20as%20non-IID%20data%29.%0AAfter%20presenting%20an%20extensive%20overview%20of%20the%20selected%20algorithms%2C%20a%0Atheoretical%20comparison%20of%20the%20algorithms%20is%20conducted%20based%20on%20their%3A%201%29%20local%0Atraining%20complexity%3B%202%29%20aggregation%20complexity%3B%203%29%20learning%20efficiency%3B%204%29%0Acommunication%20cost%3B%20and%205%29%20scalability%20in%20terms%20of%20number%20of%20clients.%20After%20the%0Atheoretical%20comparison%2C%20experimental%20analyses%20are%20presented%20to%20compare%20them%0Aunder%20different%20decentralization%20scenarios.%20For%20the%20experimental%20analyses%2C%20we%0Afocus%20our%20attention%20on%20multi-label%20image%20classification%20problems%20in%20RS.%20Based%0Aon%20our%20comprehensive%20analyses%2C%20we%20finally%20derive%20a%20guideline%20for%20selecting%0Asuitable%20FL%20algorithms%20in%20RS.%20The%20code%20of%20this%20work%20will%20be%20publicly%20available%0Aat%20https%3A//git.tu-berlin.de/rsim/FL-RS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.06141v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Learning%2520Across%2520Decentralized%2520and%2520Unshared%2520Archives%2520for%2520Remote%250A%2520%2520Sensing%2520Image%2520Classification%26entry.906535625%3DBar%25C4%25B1%25C5%259F%2520B%25C3%25BCy%25C3%25BCkta%25C5%259F%2520and%2520Gencer%2520Sumbul%2520and%2520Beg%25C3%25BCm%2520Demir%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520enables%2520the%2520collaboration%2520of%2520multiple%2520deep%2520learning%250Amodels%2520to%2520learn%2520from%2520decentralized%2520data%2520archives%2520%2528i.e.%252C%2520clients%2529%2520without%250Aaccessing%2520data%2520on%2520clients.%2520Although%2520FL%2520offers%2520ample%2520opportunities%2520in%2520knowledge%250Adiscovery%2520from%2520distributed%2520image%2520archives%252C%2520it%2520is%2520seldom%2520considered%2520in%2520remote%250Asensing%2520%2528RS%2529.%2520In%2520this%2520paper%252C%2520as%2520a%2520first%2520time%2520in%2520RS%252C%2520we%2520present%2520a%2520comparative%250Astudy%2520of%2520state-of-the-art%2520FL%2520algorithms%2520for%2520RS%2520image%2520classification%2520problems.%250ATo%2520this%2520end%252C%2520we%2520initially%2520provide%2520a%2520systematic%2520review%2520of%2520the%2520FL%2520algorithms%250Apresented%2520in%2520the%2520computer%2520vision%2520and%2520machine%2520learning%2520communities.%2520Then%252C%2520we%250Aselect%2520several%2520state-of-the-art%2520FL%2520algorithms%2520based%2520on%2520their%2520effectiveness%2520with%250Arespect%2520to%2520training%2520data%2520heterogeneity%2520across%2520clients%2520%2528known%2520as%2520non-IID%2520data%2529.%250AAfter%2520presenting%2520an%2520extensive%2520overview%2520of%2520the%2520selected%2520algorithms%252C%2520a%250Atheoretical%2520comparison%2520of%2520the%2520algorithms%2520is%2520conducted%2520based%2520on%2520their%253A%25201%2529%2520local%250Atraining%2520complexity%253B%25202%2529%2520aggregation%2520complexity%253B%25203%2529%2520learning%2520efficiency%253B%25204%2529%250Acommunication%2520cost%253B%2520and%25205%2529%2520scalability%2520in%2520terms%2520of%2520number%2520of%2520clients.%2520After%2520the%250Atheoretical%2520comparison%252C%2520experimental%2520analyses%2520are%2520presented%2520to%2520compare%2520them%250Aunder%2520different%2520decentralization%2520scenarios.%2520For%2520the%2520experimental%2520analyses%252C%2520we%250Afocus%2520our%2520attention%2520on%2520multi-label%2520image%2520classification%2520problems%2520in%2520RS.%2520Based%250Aon%2520our%2520comprehensive%2520analyses%252C%2520we%2520finally%2520derive%2520a%2520guideline%2520for%2520selecting%250Asuitable%2520FL%2520algorithms%2520in%2520RS.%2520The%2520code%2520of%2520this%2520work%2520will%2520be%2520publicly%2520available%250Aat%2520https%253A//git.tu-berlin.de/rsim/FL-RS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.06141v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20Across%20Decentralized%20and%20Unshared%20Archives%20for%20Remote%0A%20%20Sensing%20Image%20Classification&entry.906535625=Bar%C4%B1%C5%9F%20B%C3%BCy%C3%BCkta%C5%9F%20and%20Gencer%20Sumbul%20and%20Beg%C3%BCm%20Demir&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20enables%20the%20collaboration%20of%20multiple%20deep%20learning%0Amodels%20to%20learn%20from%20decentralized%20data%20archives%20%28i.e.%2C%20clients%29%20without%0Aaccessing%20data%20on%20clients.%20Although%20FL%20offers%20ample%20opportunities%20in%20knowledge%0Adiscovery%20from%20distributed%20image%20archives%2C%20it%20is%20seldom%20considered%20in%20remote%0Asensing%20%28RS%29.%20In%20this%20paper%2C%20as%20a%20first%20time%20in%20RS%2C%20we%20present%20a%20comparative%0Astudy%20of%20state-of-the-art%20FL%20algorithms%20for%20RS%20image%20classification%20problems.%0ATo%20this%20end%2C%20we%20initially%20provide%20a%20systematic%20review%20of%20the%20FL%20algorithms%0Apresented%20in%20the%20computer%20vision%20and%20machine%20learning%20communities.%20Then%2C%20we%0Aselect%20several%20state-of-the-art%20FL%20algorithms%20based%20on%20their%20effectiveness%20with%0Arespect%20to%20training%20data%20heterogeneity%20across%20clients%20%28known%20as%20non-IID%20data%29.%0AAfter%20presenting%20an%20extensive%20overview%20of%20the%20selected%20algorithms%2C%20a%0Atheoretical%20comparison%20of%20the%20algorithms%20is%20conducted%20based%20on%20their%3A%201%29%20local%0Atraining%20complexity%3B%202%29%20aggregation%20complexity%3B%203%29%20learning%20efficiency%3B%204%29%0Acommunication%20cost%3B%20and%205%29%20scalability%20in%20terms%20of%20number%20of%20clients.%20After%20the%0Atheoretical%20comparison%2C%20experimental%20analyses%20are%20presented%20to%20compare%20them%0Aunder%20different%20decentralization%20scenarios.%20For%20the%20experimental%20analyses%2C%20we%0Afocus%20our%20attention%20on%20multi-label%20image%20classification%20problems%20in%20RS.%20Based%0Aon%20our%20comprehensive%20analyses%2C%20we%20finally%20derive%20a%20guideline%20for%20selecting%0Asuitable%20FL%20algorithms%20in%20RS.%20The%20code%20of%20this%20work%20will%20be%20publicly%20available%0Aat%20https%3A//git.tu-berlin.de/rsim/FL-RS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.06141v2&entry.124074799=Read"},
{"title": "Model- and Data-Based Control of Self-Balancing Robots: Practical\n  Educational Approach with LabVIEW and Arduino", "author": "Abdelrahman Abdelgawad and Tarek Shohdy and Ayman Nada", "abstract": "  A two-wheeled self-balancing robot (TWSBR) is non-linear and unstable system.\nThis study compares the performance of model-based and data-based control\nstrategies for TWSBRs, with an explicit practical educational approach.\nModel-based control (MBC) algorithms such as Lead-Lag and PID control require a\nproficient dynamic modeling and mathematical manipulation to drive the\nlinearized equations of motions and develop the appropriate controller. On the\nother side, data-based control (DBC) methods, like fuzzy control, provide a\nsimpler and quicker approach to designing effective controllers without needing\nin-depth understanding of the system model. In this paper, the advantages and\ndisadvantages of both MBC and DBC using a TWSBR are illustrated. All\ncontrollers were implemented and tested on the OSOYOO self-balancing kit,\nincluding an Arduino microcontroller, MPU-6050 sensor, and DC motors. The\ncontrol law and the user interface are constructed using the LabVIEW-LINX\ntoolkit. A real-time hardware-in-loop experiment validates the results,\nhighlighting controllers that can be implemented on a cost-effective platform.\n", "link": "http://arxiv.org/abs/2405.03561v1", "date": "2024-05-06", "relevancy": 2.0989, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5535}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5434}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model-%20and%20Data-Based%20Control%20of%20Self-Balancing%20Robots%3A%20Practical%0A%20%20Educational%20Approach%20with%20LabVIEW%20and%20Arduino&body=Title%3A%20Model-%20and%20Data-Based%20Control%20of%20Self-Balancing%20Robots%3A%20Practical%0A%20%20Educational%20Approach%20with%20LabVIEW%20and%20Arduino%0AAuthor%3A%20Abdelrahman%20Abdelgawad%20and%20Tarek%20Shohdy%20and%20Ayman%20Nada%0AAbstract%3A%20%20%20A%20two-wheeled%20self-balancing%20robot%20%28TWSBR%29%20is%20non-linear%20and%20unstable%20system.%0AThis%20study%20compares%20the%20performance%20of%20model-based%20and%20data-based%20control%0Astrategies%20for%20TWSBRs%2C%20with%20an%20explicit%20practical%20educational%20approach.%0AModel-based%20control%20%28MBC%29%20algorithms%20such%20as%20Lead-Lag%20and%20PID%20control%20require%20a%0Aproficient%20dynamic%20modeling%20and%20mathematical%20manipulation%20to%20drive%20the%0Alinearized%20equations%20of%20motions%20and%20develop%20the%20appropriate%20controller.%20On%20the%0Aother%20side%2C%20data-based%20control%20%28DBC%29%20methods%2C%20like%20fuzzy%20control%2C%20provide%20a%0Asimpler%20and%20quicker%20approach%20to%20designing%20effective%20controllers%20without%20needing%0Ain-depth%20understanding%20of%20the%20system%20model.%20In%20this%20paper%2C%20the%20advantages%20and%0Adisadvantages%20of%20both%20MBC%20and%20DBC%20using%20a%20TWSBR%20are%20illustrated.%20All%0Acontrollers%20were%20implemented%20and%20tested%20on%20the%20OSOYOO%20self-balancing%20kit%2C%0Aincluding%20an%20Arduino%20microcontroller%2C%20MPU-6050%20sensor%2C%20and%20DC%20motors.%20The%0Acontrol%20law%20and%20the%20user%20interface%20are%20constructed%20using%20the%20LabVIEW-LINX%0Atoolkit.%20A%20real-time%20hardware-in-loop%20experiment%20validates%20the%20results%2C%0Ahighlighting%20controllers%20that%20can%20be%20implemented%20on%20a%20cost-effective%20platform.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel-%2520and%2520Data-Based%2520Control%2520of%2520Self-Balancing%2520Robots%253A%2520Practical%250A%2520%2520Educational%2520Approach%2520with%2520LabVIEW%2520and%2520Arduino%26entry.906535625%3DAbdelrahman%2520Abdelgawad%2520and%2520Tarek%2520Shohdy%2520and%2520Ayman%2520Nada%26entry.1292438233%3D%2520%2520A%2520two-wheeled%2520self-balancing%2520robot%2520%2528TWSBR%2529%2520is%2520non-linear%2520and%2520unstable%2520system.%250AThis%2520study%2520compares%2520the%2520performance%2520of%2520model-based%2520and%2520data-based%2520control%250Astrategies%2520for%2520TWSBRs%252C%2520with%2520an%2520explicit%2520practical%2520educational%2520approach.%250AModel-based%2520control%2520%2528MBC%2529%2520algorithms%2520such%2520as%2520Lead-Lag%2520and%2520PID%2520control%2520require%2520a%250Aproficient%2520dynamic%2520modeling%2520and%2520mathematical%2520manipulation%2520to%2520drive%2520the%250Alinearized%2520equations%2520of%2520motions%2520and%2520develop%2520the%2520appropriate%2520controller.%2520On%2520the%250Aother%2520side%252C%2520data-based%2520control%2520%2528DBC%2529%2520methods%252C%2520like%2520fuzzy%2520control%252C%2520provide%2520a%250Asimpler%2520and%2520quicker%2520approach%2520to%2520designing%2520effective%2520controllers%2520without%2520needing%250Ain-depth%2520understanding%2520of%2520the%2520system%2520model.%2520In%2520this%2520paper%252C%2520the%2520advantages%2520and%250Adisadvantages%2520of%2520both%2520MBC%2520and%2520DBC%2520using%2520a%2520TWSBR%2520are%2520illustrated.%2520All%250Acontrollers%2520were%2520implemented%2520and%2520tested%2520on%2520the%2520OSOYOO%2520self-balancing%2520kit%252C%250Aincluding%2520an%2520Arduino%2520microcontroller%252C%2520MPU-6050%2520sensor%252C%2520and%2520DC%2520motors.%2520The%250Acontrol%2520law%2520and%2520the%2520user%2520interface%2520are%2520constructed%2520using%2520the%2520LabVIEW-LINX%250Atoolkit.%2520A%2520real-time%2520hardware-in-loop%2520experiment%2520validates%2520the%2520results%252C%250Ahighlighting%2520controllers%2520that%2520can%2520be%2520implemented%2520on%2520a%2520cost-effective%2520platform.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-%20and%20Data-Based%20Control%20of%20Self-Balancing%20Robots%3A%20Practical%0A%20%20Educational%20Approach%20with%20LabVIEW%20and%20Arduino&entry.906535625=Abdelrahman%20Abdelgawad%20and%20Tarek%20Shohdy%20and%20Ayman%20Nada&entry.1292438233=%20%20A%20two-wheeled%20self-balancing%20robot%20%28TWSBR%29%20is%20non-linear%20and%20unstable%20system.%0AThis%20study%20compares%20the%20performance%20of%20model-based%20and%20data-based%20control%0Astrategies%20for%20TWSBRs%2C%20with%20an%20explicit%20practical%20educational%20approach.%0AModel-based%20control%20%28MBC%29%20algorithms%20such%20as%20Lead-Lag%20and%20PID%20control%20require%20a%0Aproficient%20dynamic%20modeling%20and%20mathematical%20manipulation%20to%20drive%20the%0Alinearized%20equations%20of%20motions%20and%20develop%20the%20appropriate%20controller.%20On%20the%0Aother%20side%2C%20data-based%20control%20%28DBC%29%20methods%2C%20like%20fuzzy%20control%2C%20provide%20a%0Asimpler%20and%20quicker%20approach%20to%20designing%20effective%20controllers%20without%20needing%0Ain-depth%20understanding%20of%20the%20system%20model.%20In%20this%20paper%2C%20the%20advantages%20and%0Adisadvantages%20of%20both%20MBC%20and%20DBC%20using%20a%20TWSBR%20are%20illustrated.%20All%0Acontrollers%20were%20implemented%20and%20tested%20on%20the%20OSOYOO%20self-balancing%20kit%2C%0Aincluding%20an%20Arduino%20microcontroller%2C%20MPU-6050%20sensor%2C%20and%20DC%20motors.%20The%0Acontrol%20law%20and%20the%20user%20interface%20are%20constructed%20using%20the%20LabVIEW-LINX%0Atoolkit.%20A%20real-time%20hardware-in-loop%20experiment%20validates%20the%20results%2C%0Ahighlighting%20controllers%20that%20can%20be%20implemented%20on%20a%20cost-effective%20platform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03561v1&entry.124074799=Read"},
{"title": "AnchorGT: Efficient and Flexible Attention Architecture for Scalable\n  Graph Transformers", "author": "Wenhao Zhu and Guojie Song and Liang Wang and Shaoguo Liu", "abstract": "  Graph Transformers (GTs) have significantly advanced the field of graph\nrepresentation learning by overcoming the limitations of message-passing graph\nneural networks (GNNs) and demonstrating promising performance and expressive\npower. However, the quadratic complexity of self-attention mechanism in GTs has\nlimited their scalability, and previous approaches to address this issue often\nsuffer from expressiveness degradation or lack of versatility. To address this\nissue, we propose AnchorGT, a novel attention architecture for GTs with global\nreceptive field and almost linear complexity, which serves as a flexible\nbuilding block to improve the scalability of a wide range of GT models.\nInspired by anchor-based GNNs, we employ structurally important $k$-dominating\nnode set as anchors and design an attention mechanism that focuses on the\nrelationship between individual nodes and anchors, while retaining the global\nreceptive field for all nodes. With its intuitive design, AnchorGT can easily\nreplace the attention module in various GT models with different network\narchitectures and structural encodings, resulting in reduced computational\noverhead without sacrificing performance. In addition, we theoretically prove\nthat AnchorGT attention can be strictly more expressive than Weisfeiler-Lehman\ntest, showing its superiority in representing graph structures. Our experiments\non three state-of-the-art GT models demonstrate that their AnchorGT variants\ncan achieve better results while being faster and significantly more memory\nefficient.\n", "link": "http://arxiv.org/abs/2405.03481v1", "date": "2024-05-06", "relevancy": 2.098, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5449}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5262}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnchorGT%3A%20Efficient%20and%20Flexible%20Attention%20Architecture%20for%20Scalable%0A%20%20Graph%20Transformers&body=Title%3A%20AnchorGT%3A%20Efficient%20and%20Flexible%20Attention%20Architecture%20for%20Scalable%0A%20%20Graph%20Transformers%0AAuthor%3A%20Wenhao%20Zhu%20and%20Guojie%20Song%20and%20Liang%20Wang%20and%20Shaoguo%20Liu%0AAbstract%3A%20%20%20Graph%20Transformers%20%28GTs%29%20have%20significantly%20advanced%20the%20field%20of%20graph%0Arepresentation%20learning%20by%20overcoming%20the%20limitations%20of%20message-passing%20graph%0Aneural%20networks%20%28GNNs%29%20and%20demonstrating%20promising%20performance%20and%20expressive%0Apower.%20However%2C%20the%20quadratic%20complexity%20of%20self-attention%20mechanism%20in%20GTs%20has%0Alimited%20their%20scalability%2C%20and%20previous%20approaches%20to%20address%20this%20issue%20often%0Asuffer%20from%20expressiveness%20degradation%20or%20lack%20of%20versatility.%20To%20address%20this%0Aissue%2C%20we%20propose%20AnchorGT%2C%20a%20novel%20attention%20architecture%20for%20GTs%20with%20global%0Areceptive%20field%20and%20almost%20linear%20complexity%2C%20which%20serves%20as%20a%20flexible%0Abuilding%20block%20to%20improve%20the%20scalability%20of%20a%20wide%20range%20of%20GT%20models.%0AInspired%20by%20anchor-based%20GNNs%2C%20we%20employ%20structurally%20important%20%24k%24-dominating%0Anode%20set%20as%20anchors%20and%20design%20an%20attention%20mechanism%20that%20focuses%20on%20the%0Arelationship%20between%20individual%20nodes%20and%20anchors%2C%20while%20retaining%20the%20global%0Areceptive%20field%20for%20all%20nodes.%20With%20its%20intuitive%20design%2C%20AnchorGT%20can%20easily%0Areplace%20the%20attention%20module%20in%20various%20GT%20models%20with%20different%20network%0Aarchitectures%20and%20structural%20encodings%2C%20resulting%20in%20reduced%20computational%0Aoverhead%20without%20sacrificing%20performance.%20In%20addition%2C%20we%20theoretically%20prove%0Athat%20AnchorGT%20attention%20can%20be%20strictly%20more%20expressive%20than%20Weisfeiler-Lehman%0Atest%2C%20showing%20its%20superiority%20in%20representing%20graph%20structures.%20Our%20experiments%0Aon%20three%20state-of-the-art%20GT%20models%20demonstrate%20that%20their%20AnchorGT%20variants%0Acan%20achieve%20better%20results%20while%20being%20faster%20and%20significantly%20more%20memory%0Aefficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnchorGT%253A%2520Efficient%2520and%2520Flexible%2520Attention%2520Architecture%2520for%2520Scalable%250A%2520%2520Graph%2520Transformers%26entry.906535625%3DWenhao%2520Zhu%2520and%2520Guojie%2520Song%2520and%2520Liang%2520Wang%2520and%2520Shaoguo%2520Liu%26entry.1292438233%3D%2520%2520Graph%2520Transformers%2520%2528GTs%2529%2520have%2520significantly%2520advanced%2520the%2520field%2520of%2520graph%250Arepresentation%2520learning%2520by%2520overcoming%2520the%2520limitations%2520of%2520message-passing%2520graph%250Aneural%2520networks%2520%2528GNNs%2529%2520and%2520demonstrating%2520promising%2520performance%2520and%2520expressive%250Apower.%2520However%252C%2520the%2520quadratic%2520complexity%2520of%2520self-attention%2520mechanism%2520in%2520GTs%2520has%250Alimited%2520their%2520scalability%252C%2520and%2520previous%2520approaches%2520to%2520address%2520this%2520issue%2520often%250Asuffer%2520from%2520expressiveness%2520degradation%2520or%2520lack%2520of%2520versatility.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520AnchorGT%252C%2520a%2520novel%2520attention%2520architecture%2520for%2520GTs%2520with%2520global%250Areceptive%2520field%2520and%2520almost%2520linear%2520complexity%252C%2520which%2520serves%2520as%2520a%2520flexible%250Abuilding%2520block%2520to%2520improve%2520the%2520scalability%2520of%2520a%2520wide%2520range%2520of%2520GT%2520models.%250AInspired%2520by%2520anchor-based%2520GNNs%252C%2520we%2520employ%2520structurally%2520important%2520%2524k%2524-dominating%250Anode%2520set%2520as%2520anchors%2520and%2520design%2520an%2520attention%2520mechanism%2520that%2520focuses%2520on%2520the%250Arelationship%2520between%2520individual%2520nodes%2520and%2520anchors%252C%2520while%2520retaining%2520the%2520global%250Areceptive%2520field%2520for%2520all%2520nodes.%2520With%2520its%2520intuitive%2520design%252C%2520AnchorGT%2520can%2520easily%250Areplace%2520the%2520attention%2520module%2520in%2520various%2520GT%2520models%2520with%2520different%2520network%250Aarchitectures%2520and%2520structural%2520encodings%252C%2520resulting%2520in%2520reduced%2520computational%250Aoverhead%2520without%2520sacrificing%2520performance.%2520In%2520addition%252C%2520we%2520theoretically%2520prove%250Athat%2520AnchorGT%2520attention%2520can%2520be%2520strictly%2520more%2520expressive%2520than%2520Weisfeiler-Lehman%250Atest%252C%2520showing%2520its%2520superiority%2520in%2520representing%2520graph%2520structures.%2520Our%2520experiments%250Aon%2520three%2520state-of-the-art%2520GT%2520models%2520demonstrate%2520that%2520their%2520AnchorGT%2520variants%250Acan%2520achieve%2520better%2520results%2520while%2520being%2520faster%2520and%2520significantly%2520more%2520memory%250Aefficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnchorGT%3A%20Efficient%20and%20Flexible%20Attention%20Architecture%20for%20Scalable%0A%20%20Graph%20Transformers&entry.906535625=Wenhao%20Zhu%20and%20Guojie%20Song%20and%20Liang%20Wang%20and%20Shaoguo%20Liu&entry.1292438233=%20%20Graph%20Transformers%20%28GTs%29%20have%20significantly%20advanced%20the%20field%20of%20graph%0Arepresentation%20learning%20by%20overcoming%20the%20limitations%20of%20message-passing%20graph%0Aneural%20networks%20%28GNNs%29%20and%20demonstrating%20promising%20performance%20and%20expressive%0Apower.%20However%2C%20the%20quadratic%20complexity%20of%20self-attention%20mechanism%20in%20GTs%20has%0Alimited%20their%20scalability%2C%20and%20previous%20approaches%20to%20address%20this%20issue%20often%0Asuffer%20from%20expressiveness%20degradation%20or%20lack%20of%20versatility.%20To%20address%20this%0Aissue%2C%20we%20propose%20AnchorGT%2C%20a%20novel%20attention%20architecture%20for%20GTs%20with%20global%0Areceptive%20field%20and%20almost%20linear%20complexity%2C%20which%20serves%20as%20a%20flexible%0Abuilding%20block%20to%20improve%20the%20scalability%20of%20a%20wide%20range%20of%20GT%20models.%0AInspired%20by%20anchor-based%20GNNs%2C%20we%20employ%20structurally%20important%20%24k%24-dominating%0Anode%20set%20as%20anchors%20and%20design%20an%20attention%20mechanism%20that%20focuses%20on%20the%0Arelationship%20between%20individual%20nodes%20and%20anchors%2C%20while%20retaining%20the%20global%0Areceptive%20field%20for%20all%20nodes.%20With%20its%20intuitive%20design%2C%20AnchorGT%20can%20easily%0Areplace%20the%20attention%20module%20in%20various%20GT%20models%20with%20different%20network%0Aarchitectures%20and%20structural%20encodings%2C%20resulting%20in%20reduced%20computational%0Aoverhead%20without%20sacrificing%20performance.%20In%20addition%2C%20we%20theoretically%20prove%0Athat%20AnchorGT%20attention%20can%20be%20strictly%20more%20expressive%20than%20Weisfeiler-Lehman%0Atest%2C%20showing%20its%20superiority%20in%20representing%20graph%20structures.%20Our%20experiments%0Aon%20three%20state-of-the-art%20GT%20models%20demonstrate%20that%20their%20AnchorGT%20variants%0Acan%20achieve%20better%20results%20while%20being%20faster%20and%20significantly%20more%20memory%0Aefficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03481v1&entry.124074799=Read"},
{"title": "A Lightweight Neural Architecture Search Model for Medical Image\n  Classification", "author": "Lunchen Xie and Eugenio Lomurno and Matteo Gambella and Danilo Ardagna and Manuel Roveri and Matteo Matteucci and Qingjiang Shi", "abstract": "  Accurate classification of medical images is essential for modern\ndiagnostics. Deep learning advancements led clinicians to increasingly use\nsophisticated models to make faster and more accurate decisions, sometimes\nreplacing human judgment. However, model development is costly and repetitive.\nNeural Architecture Search (NAS) provides solutions by automating the design of\ndeep learning architectures. This paper presents ZO-DARTS+, a differentiable\nNAS algorithm that improves search efficiency through a novel method of\ngenerating sparse probabilities by bi-level optimization. Experiments on five\npublic medical datasets show that ZO-DARTS+ matches the accuracy of\nstate-of-the-art solutions while reducing search times by up to three times.\n", "link": "http://arxiv.org/abs/2405.03462v1", "date": "2024-05-06", "relevancy": 2.0925, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.563}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5277}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lightweight%20Neural%20Architecture%20Search%20Model%20for%20Medical%20Image%0A%20%20Classification&body=Title%3A%20A%20Lightweight%20Neural%20Architecture%20Search%20Model%20for%20Medical%20Image%0A%20%20Classification%0AAuthor%3A%20Lunchen%20Xie%20and%20Eugenio%20Lomurno%20and%20Matteo%20Gambella%20and%20Danilo%20Ardagna%20and%20Manuel%20Roveri%20and%20Matteo%20Matteucci%20and%20Qingjiang%20Shi%0AAbstract%3A%20%20%20Accurate%20classification%20of%20medical%20images%20is%20essential%20for%20modern%0Adiagnostics.%20Deep%20learning%20advancements%20led%20clinicians%20to%20increasingly%20use%0Asophisticated%20models%20to%20make%20faster%20and%20more%20accurate%20decisions%2C%20sometimes%0Areplacing%20human%20judgment.%20However%2C%20model%20development%20is%20costly%20and%20repetitive.%0ANeural%20Architecture%20Search%20%28NAS%29%20provides%20solutions%20by%20automating%20the%20design%20of%0Adeep%20learning%20architectures.%20This%20paper%20presents%20ZO-DARTS%2B%2C%20a%20differentiable%0ANAS%20algorithm%20that%20improves%20search%20efficiency%20through%20a%20novel%20method%20of%0Agenerating%20sparse%20probabilities%20by%20bi-level%20optimization.%20Experiments%20on%20five%0Apublic%20medical%20datasets%20show%20that%20ZO-DARTS%2B%20matches%20the%20accuracy%20of%0Astate-of-the-art%20solutions%20while%20reducing%20search%20times%20by%20up%20to%20three%20times.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lightweight%2520Neural%2520Architecture%2520Search%2520Model%2520for%2520Medical%2520Image%250A%2520%2520Classification%26entry.906535625%3DLunchen%2520Xie%2520and%2520Eugenio%2520Lomurno%2520and%2520Matteo%2520Gambella%2520and%2520Danilo%2520Ardagna%2520and%2520Manuel%2520Roveri%2520and%2520Matteo%2520Matteucci%2520and%2520Qingjiang%2520Shi%26entry.1292438233%3D%2520%2520Accurate%2520classification%2520of%2520medical%2520images%2520is%2520essential%2520for%2520modern%250Adiagnostics.%2520Deep%2520learning%2520advancements%2520led%2520clinicians%2520to%2520increasingly%2520use%250Asophisticated%2520models%2520to%2520make%2520faster%2520and%2520more%2520accurate%2520decisions%252C%2520sometimes%250Areplacing%2520human%2520judgment.%2520However%252C%2520model%2520development%2520is%2520costly%2520and%2520repetitive.%250ANeural%2520Architecture%2520Search%2520%2528NAS%2529%2520provides%2520solutions%2520by%2520automating%2520the%2520design%2520of%250Adeep%2520learning%2520architectures.%2520This%2520paper%2520presents%2520ZO-DARTS%252B%252C%2520a%2520differentiable%250ANAS%2520algorithm%2520that%2520improves%2520search%2520efficiency%2520through%2520a%2520novel%2520method%2520of%250Agenerating%2520sparse%2520probabilities%2520by%2520bi-level%2520optimization.%2520Experiments%2520on%2520five%250Apublic%2520medical%2520datasets%2520show%2520that%2520ZO-DARTS%252B%2520matches%2520the%2520accuracy%2520of%250Astate-of-the-art%2520solutions%2520while%2520reducing%2520search%2520times%2520by%2520up%2520to%2520three%2520times.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lightweight%20Neural%20Architecture%20Search%20Model%20for%20Medical%20Image%0A%20%20Classification&entry.906535625=Lunchen%20Xie%20and%20Eugenio%20Lomurno%20and%20Matteo%20Gambella%20and%20Danilo%20Ardagna%20and%20Manuel%20Roveri%20and%20Matteo%20Matteucci%20and%20Qingjiang%20Shi&entry.1292438233=%20%20Accurate%20classification%20of%20medical%20images%20is%20essential%20for%20modern%0Adiagnostics.%20Deep%20learning%20advancements%20led%20clinicians%20to%20increasingly%20use%0Asophisticated%20models%20to%20make%20faster%20and%20more%20accurate%20decisions%2C%20sometimes%0Areplacing%20human%20judgment.%20However%2C%20model%20development%20is%20costly%20and%20repetitive.%0ANeural%20Architecture%20Search%20%28NAS%29%20provides%20solutions%20by%20automating%20the%20design%20of%0Adeep%20learning%20architectures.%20This%20paper%20presents%20ZO-DARTS%2B%2C%20a%20differentiable%0ANAS%20algorithm%20that%20improves%20search%20efficiency%20through%20a%20novel%20method%20of%0Agenerating%20sparse%20probabilities%20by%20bi-level%20optimization.%20Experiments%20on%20five%0Apublic%20medical%20datasets%20show%20that%20ZO-DARTS%2B%20matches%20the%20accuracy%20of%0Astate-of-the-art%20solutions%20while%20reducing%20search%20times%20by%20up%20to%20three%20times.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03462v1&entry.124074799=Read"},
{"title": "Biased-MPPI: Informing Sampling-Based Model Predictive Control by Fusing\n  Ancillary Controllers", "author": "Elia Trevisan and Javier Alonso-Mora", "abstract": "  Motion planning for autonomous robots in dynamic environments poses numerous\nchallenges due to uncertainties in the robot's dynamics and interaction with\nother agents. Sampling-based MPC approaches, such as Model Predictive Path\nIntegral (MPPI) control, have shown promise in addressing these complex motion\nplanning problems. However, the performance of MPPI relies heavily on the\nchoice of sampling distribution. Existing literature often uses the previously\ncomputed input sequence as the mean of a Gaussian distribution for sampling,\nleading to potential failures and local minima. In this paper, we propose a\nnovel derivation of MPPI that allows for arbitrary sampling distributions to\nenhance efficiency, robustness, and convergence while alleviating the problem\nof local minima. We present an efficient importance sampling scheme that\ncombines classical and learning-based ancillary controllers simultaneously,\nresulting in more informative sampling and control fusion. Several simulated\nand real-world demonstrate the validity of our approach.\n", "link": "http://arxiv.org/abs/2401.09241v2", "date": "2024-05-06", "relevancy": 2.0921, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5801}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5323}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biased-MPPI%3A%20Informing%20Sampling-Based%20Model%20Predictive%20Control%20by%20Fusing%0A%20%20Ancillary%20Controllers&body=Title%3A%20Biased-MPPI%3A%20Informing%20Sampling-Based%20Model%20Predictive%20Control%20by%20Fusing%0A%20%20Ancillary%20Controllers%0AAuthor%3A%20Elia%20Trevisan%20and%20Javier%20Alonso-Mora%0AAbstract%3A%20%20%20Motion%20planning%20for%20autonomous%20robots%20in%20dynamic%20environments%20poses%20numerous%0Achallenges%20due%20to%20uncertainties%20in%20the%20robot%27s%20dynamics%20and%20interaction%20with%0Aother%20agents.%20Sampling-based%20MPC%20approaches%2C%20such%20as%20Model%20Predictive%20Path%0AIntegral%20%28MPPI%29%20control%2C%20have%20shown%20promise%20in%20addressing%20these%20complex%20motion%0Aplanning%20problems.%20However%2C%20the%20performance%20of%20MPPI%20relies%20heavily%20on%20the%0Achoice%20of%20sampling%20distribution.%20Existing%20literature%20often%20uses%20the%20previously%0Acomputed%20input%20sequence%20as%20the%20mean%20of%20a%20Gaussian%20distribution%20for%20sampling%2C%0Aleading%20to%20potential%20failures%20and%20local%20minima.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20derivation%20of%20MPPI%20that%20allows%20for%20arbitrary%20sampling%20distributions%20to%0Aenhance%20efficiency%2C%20robustness%2C%20and%20convergence%20while%20alleviating%20the%20problem%0Aof%20local%20minima.%20We%20present%20an%20efficient%20importance%20sampling%20scheme%20that%0Acombines%20classical%20and%20learning-based%20ancillary%20controllers%20simultaneously%2C%0Aresulting%20in%20more%20informative%20sampling%20and%20control%20fusion.%20Several%20simulated%0Aand%20real-world%20demonstrate%20the%20validity%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09241v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiased-MPPI%253A%2520Informing%2520Sampling-Based%2520Model%2520Predictive%2520Control%2520by%2520Fusing%250A%2520%2520Ancillary%2520Controllers%26entry.906535625%3DElia%2520Trevisan%2520and%2520Javier%2520Alonso-Mora%26entry.1292438233%3D%2520%2520Motion%2520planning%2520for%2520autonomous%2520robots%2520in%2520dynamic%2520environments%2520poses%2520numerous%250Achallenges%2520due%2520to%2520uncertainties%2520in%2520the%2520robot%2527s%2520dynamics%2520and%2520interaction%2520with%250Aother%2520agents.%2520Sampling-based%2520MPC%2520approaches%252C%2520such%2520as%2520Model%2520Predictive%2520Path%250AIntegral%2520%2528MPPI%2529%2520control%252C%2520have%2520shown%2520promise%2520in%2520addressing%2520these%2520complex%2520motion%250Aplanning%2520problems.%2520However%252C%2520the%2520performance%2520of%2520MPPI%2520relies%2520heavily%2520on%2520the%250Achoice%2520of%2520sampling%2520distribution.%2520Existing%2520literature%2520often%2520uses%2520the%2520previously%250Acomputed%2520input%2520sequence%2520as%2520the%2520mean%2520of%2520a%2520Gaussian%2520distribution%2520for%2520sampling%252C%250Aleading%2520to%2520potential%2520failures%2520and%2520local%2520minima.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520derivation%2520of%2520MPPI%2520that%2520allows%2520for%2520arbitrary%2520sampling%2520distributions%2520to%250Aenhance%2520efficiency%252C%2520robustness%252C%2520and%2520convergence%2520while%2520alleviating%2520the%2520problem%250Aof%2520local%2520minima.%2520We%2520present%2520an%2520efficient%2520importance%2520sampling%2520scheme%2520that%250Acombines%2520classical%2520and%2520learning-based%2520ancillary%2520controllers%2520simultaneously%252C%250Aresulting%2520in%2520more%2520informative%2520sampling%2520and%2520control%2520fusion.%2520Several%2520simulated%250Aand%2520real-world%2520demonstrate%2520the%2520validity%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.09241v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biased-MPPI%3A%20Informing%20Sampling-Based%20Model%20Predictive%20Control%20by%20Fusing%0A%20%20Ancillary%20Controllers&entry.906535625=Elia%20Trevisan%20and%20Javier%20Alonso-Mora&entry.1292438233=%20%20Motion%20planning%20for%20autonomous%20robots%20in%20dynamic%20environments%20poses%20numerous%0Achallenges%20due%20to%20uncertainties%20in%20the%20robot%27s%20dynamics%20and%20interaction%20with%0Aother%20agents.%20Sampling-based%20MPC%20approaches%2C%20such%20as%20Model%20Predictive%20Path%0AIntegral%20%28MPPI%29%20control%2C%20have%20shown%20promise%20in%20addressing%20these%20complex%20motion%0Aplanning%20problems.%20However%2C%20the%20performance%20of%20MPPI%20relies%20heavily%20on%20the%0Achoice%20of%20sampling%20distribution.%20Existing%20literature%20often%20uses%20the%20previously%0Acomputed%20input%20sequence%20as%20the%20mean%20of%20a%20Gaussian%20distribution%20for%20sampling%2C%0Aleading%20to%20potential%20failures%20and%20local%20minima.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20derivation%20of%20MPPI%20that%20allows%20for%20arbitrary%20sampling%20distributions%20to%0Aenhance%20efficiency%2C%20robustness%2C%20and%20convergence%20while%20alleviating%20the%20problem%0Aof%20local%20minima.%20We%20present%20an%20efficient%20importance%20sampling%20scheme%20that%0Acombines%20classical%20and%20learning-based%20ancillary%20controllers%20simultaneously%2C%0Aresulting%20in%20more%20informative%20sampling%20and%20control%20fusion.%20Several%20simulated%0Aand%20real-world%20demonstrate%20the%20validity%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09241v2&entry.124074799=Read"},
{"title": "Uncertainty Quantification in Multivariable Regression for Material\n  Property Prediction with Bayesian Neural Networks", "author": "Longze Li and Jiang Chang and Aleksandar Vakanski and Yachun Wang and Tiankai Yao and Min Xian", "abstract": "  With the increased use of data-driven approaches and machine learning-based\nmethods in material science, the importance of reliable uncertainty\nquantification (UQ) of the predicted variables for informed decision-making\ncannot be overstated. UQ in material property prediction poses unique\nchallenges, including the multi-scale and multi-physics nature of advanced\nmaterials, intricate interactions between numerous factors, limited\navailability of large curated datasets for model training, etc. Recently,\nBayesian Neural Networks (BNNs) have emerged as a promising approach for UQ,\noffering a probabilistic framework for capturing uncertainties within neural\nnetworks. In this work, we introduce an approach for UQ within physics-informed\nBNNs, which integrates knowledge from governing laws in material modeling to\nguide the models toward physically consistent predictions. To evaluate the\neffectiveness of this approach, we present case studies for predicting the\ncreep rupture life of steel alloys. Experimental validation with three datasets\nof collected measurements from creep tests demonstrates the ability of BNNs to\nproduce accurate point and uncertainty estimates that are competitive or exceed\nthe performance of the conventional method of Gaussian Process Regression.\nSimilarly, we evaluated the suitability of BNNs for UQ in an active learning\napplication and reported competitive performance. The most promising framework\nfor creep life prediction is BNNs based on Markov Chain Monte Carlo\napproximation of the posterior distribution of network parameters, as it\nprovided more reliable results in comparison to BNNs based on variational\ninference approximation or related NNs with probabilistic outputs. The codes\nare available at:\nhttps://github.com/avakanski/Creep-uncertainty-quantification.\n", "link": "http://arxiv.org/abs/2311.02495v3", "date": "2024-05-06", "relevancy": 2.0843, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6094}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5205}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Quantification%20in%20Multivariable%20Regression%20for%20Material%0A%20%20Property%20Prediction%20with%20Bayesian%20Neural%20Networks&body=Title%3A%20Uncertainty%20Quantification%20in%20Multivariable%20Regression%20for%20Material%0A%20%20Property%20Prediction%20with%20Bayesian%20Neural%20Networks%0AAuthor%3A%20Longze%20Li%20and%20Jiang%20Chang%20and%20Aleksandar%20Vakanski%20and%20Yachun%20Wang%20and%20Tiankai%20Yao%20and%20Min%20Xian%0AAbstract%3A%20%20%20With%20the%20increased%20use%20of%20data-driven%20approaches%20and%20machine%20learning-based%0Amethods%20in%20material%20science%2C%20the%20importance%20of%20reliable%20uncertainty%0Aquantification%20%28UQ%29%20of%20the%20predicted%20variables%20for%20informed%20decision-making%0Acannot%20be%20overstated.%20UQ%20in%20material%20property%20prediction%20poses%20unique%0Achallenges%2C%20including%20the%20multi-scale%20and%20multi-physics%20nature%20of%20advanced%0Amaterials%2C%20intricate%20interactions%20between%20numerous%20factors%2C%20limited%0Aavailability%20of%20large%20curated%20datasets%20for%20model%20training%2C%20etc.%20Recently%2C%0ABayesian%20Neural%20Networks%20%28BNNs%29%20have%20emerged%20as%20a%20promising%20approach%20for%20UQ%2C%0Aoffering%20a%20probabilistic%20framework%20for%20capturing%20uncertainties%20within%20neural%0Anetworks.%20In%20this%20work%2C%20we%20introduce%20an%20approach%20for%20UQ%20within%20physics-informed%0ABNNs%2C%20which%20integrates%20knowledge%20from%20governing%20laws%20in%20material%20modeling%20to%0Aguide%20the%20models%20toward%20physically%20consistent%20predictions.%20To%20evaluate%20the%0Aeffectiveness%20of%20this%20approach%2C%20we%20present%20case%20studies%20for%20predicting%20the%0Acreep%20rupture%20life%20of%20steel%20alloys.%20Experimental%20validation%20with%20three%20datasets%0Aof%20collected%20measurements%20from%20creep%20tests%20demonstrates%20the%20ability%20of%20BNNs%20to%0Aproduce%20accurate%20point%20and%20uncertainty%20estimates%20that%20are%20competitive%20or%20exceed%0Athe%20performance%20of%20the%20conventional%20method%20of%20Gaussian%20Process%20Regression.%0ASimilarly%2C%20we%20evaluated%20the%20suitability%20of%20BNNs%20for%20UQ%20in%20an%20active%20learning%0Aapplication%20and%20reported%20competitive%20performance.%20The%20most%20promising%20framework%0Afor%20creep%20life%20prediction%20is%20BNNs%20based%20on%20Markov%20Chain%20Monte%20Carlo%0Aapproximation%20of%20the%20posterior%20distribution%20of%20network%20parameters%2C%20as%20it%0Aprovided%20more%20reliable%20results%20in%20comparison%20to%20BNNs%20based%20on%20variational%0Ainference%20approximation%20or%20related%20NNs%20with%20probabilistic%20outputs.%20The%20codes%0Aare%20available%20at%3A%0Ahttps%3A//github.com/avakanski/Creep-uncertainty-quantification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.02495v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Quantification%2520in%2520Multivariable%2520Regression%2520for%2520Material%250A%2520%2520Property%2520Prediction%2520with%2520Bayesian%2520Neural%2520Networks%26entry.906535625%3DLongze%2520Li%2520and%2520Jiang%2520Chang%2520and%2520Aleksandar%2520Vakanski%2520and%2520Yachun%2520Wang%2520and%2520Tiankai%2520Yao%2520and%2520Min%2520Xian%26entry.1292438233%3D%2520%2520With%2520the%2520increased%2520use%2520of%2520data-driven%2520approaches%2520and%2520machine%2520learning-based%250Amethods%2520in%2520material%2520science%252C%2520the%2520importance%2520of%2520reliable%2520uncertainty%250Aquantification%2520%2528UQ%2529%2520of%2520the%2520predicted%2520variables%2520for%2520informed%2520decision-making%250Acannot%2520be%2520overstated.%2520UQ%2520in%2520material%2520property%2520prediction%2520poses%2520unique%250Achallenges%252C%2520including%2520the%2520multi-scale%2520and%2520multi-physics%2520nature%2520of%2520advanced%250Amaterials%252C%2520intricate%2520interactions%2520between%2520numerous%2520factors%252C%2520limited%250Aavailability%2520of%2520large%2520curated%2520datasets%2520for%2520model%2520training%252C%2520etc.%2520Recently%252C%250ABayesian%2520Neural%2520Networks%2520%2528BNNs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520UQ%252C%250Aoffering%2520a%2520probabilistic%2520framework%2520for%2520capturing%2520uncertainties%2520within%2520neural%250Anetworks.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520approach%2520for%2520UQ%2520within%2520physics-informed%250ABNNs%252C%2520which%2520integrates%2520knowledge%2520from%2520governing%2520laws%2520in%2520material%2520modeling%2520to%250Aguide%2520the%2520models%2520toward%2520physically%2520consistent%2520predictions.%2520To%2520evaluate%2520the%250Aeffectiveness%2520of%2520this%2520approach%252C%2520we%2520present%2520case%2520studies%2520for%2520predicting%2520the%250Acreep%2520rupture%2520life%2520of%2520steel%2520alloys.%2520Experimental%2520validation%2520with%2520three%2520datasets%250Aof%2520collected%2520measurements%2520from%2520creep%2520tests%2520demonstrates%2520the%2520ability%2520of%2520BNNs%2520to%250Aproduce%2520accurate%2520point%2520and%2520uncertainty%2520estimates%2520that%2520are%2520competitive%2520or%2520exceed%250Athe%2520performance%2520of%2520the%2520conventional%2520method%2520of%2520Gaussian%2520Process%2520Regression.%250ASimilarly%252C%2520we%2520evaluated%2520the%2520suitability%2520of%2520BNNs%2520for%2520UQ%2520in%2520an%2520active%2520learning%250Aapplication%2520and%2520reported%2520competitive%2520performance.%2520The%2520most%2520promising%2520framework%250Afor%2520creep%2520life%2520prediction%2520is%2520BNNs%2520based%2520on%2520Markov%2520Chain%2520Monte%2520Carlo%250Aapproximation%2520of%2520the%2520posterior%2520distribution%2520of%2520network%2520parameters%252C%2520as%2520it%250Aprovided%2520more%2520reliable%2520results%2520in%2520comparison%2520to%2520BNNs%2520based%2520on%2520variational%250Ainference%2520approximation%2520or%2520related%2520NNs%2520with%2520probabilistic%2520outputs.%2520The%2520codes%250Aare%2520available%2520at%253A%250Ahttps%253A//github.com/avakanski/Creep-uncertainty-quantification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.02495v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Quantification%20in%20Multivariable%20Regression%20for%20Material%0A%20%20Property%20Prediction%20with%20Bayesian%20Neural%20Networks&entry.906535625=Longze%20Li%20and%20Jiang%20Chang%20and%20Aleksandar%20Vakanski%20and%20Yachun%20Wang%20and%20Tiankai%20Yao%20and%20Min%20Xian&entry.1292438233=%20%20With%20the%20increased%20use%20of%20data-driven%20approaches%20and%20machine%20learning-based%0Amethods%20in%20material%20science%2C%20the%20importance%20of%20reliable%20uncertainty%0Aquantification%20%28UQ%29%20of%20the%20predicted%20variables%20for%20informed%20decision-making%0Acannot%20be%20overstated.%20UQ%20in%20material%20property%20prediction%20poses%20unique%0Achallenges%2C%20including%20the%20multi-scale%20and%20multi-physics%20nature%20of%20advanced%0Amaterials%2C%20intricate%20interactions%20between%20numerous%20factors%2C%20limited%0Aavailability%20of%20large%20curated%20datasets%20for%20model%20training%2C%20etc.%20Recently%2C%0ABayesian%20Neural%20Networks%20%28BNNs%29%20have%20emerged%20as%20a%20promising%20approach%20for%20UQ%2C%0Aoffering%20a%20probabilistic%20framework%20for%20capturing%20uncertainties%20within%20neural%0Anetworks.%20In%20this%20work%2C%20we%20introduce%20an%20approach%20for%20UQ%20within%20physics-informed%0ABNNs%2C%20which%20integrates%20knowledge%20from%20governing%20laws%20in%20material%20modeling%20to%0Aguide%20the%20models%20toward%20physically%20consistent%20predictions.%20To%20evaluate%20the%0Aeffectiveness%20of%20this%20approach%2C%20we%20present%20case%20studies%20for%20predicting%20the%0Acreep%20rupture%20life%20of%20steel%20alloys.%20Experimental%20validation%20with%20three%20datasets%0Aof%20collected%20measurements%20from%20creep%20tests%20demonstrates%20the%20ability%20of%20BNNs%20to%0Aproduce%20accurate%20point%20and%20uncertainty%20estimates%20that%20are%20competitive%20or%20exceed%0Athe%20performance%20of%20the%20conventional%20method%20of%20Gaussian%20Process%20Regression.%0ASimilarly%2C%20we%20evaluated%20the%20suitability%20of%20BNNs%20for%20UQ%20in%20an%20active%20learning%0Aapplication%20and%20reported%20competitive%20performance.%20The%20most%20promising%20framework%0Afor%20creep%20life%20prediction%20is%20BNNs%20based%20on%20Markov%20Chain%20Monte%20Carlo%0Aapproximation%20of%20the%20posterior%20distribution%20of%20network%20parameters%2C%20as%20it%0Aprovided%20more%20reliable%20results%20in%20comparison%20to%20BNNs%20based%20on%20variational%0Ainference%20approximation%20or%20related%20NNs%20with%20probabilistic%20outputs.%20The%20codes%0Aare%20available%20at%3A%0Ahttps%3A//github.com/avakanski/Creep-uncertainty-quantification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.02495v3&entry.124074799=Read"},
{"title": "An Optimized Ensemble Deep Learning Model For Brain Tumor Classification", "author": "Md. Alamin Talukder and Md. Manowarul Islam and Md Ashraf Uddin", "abstract": "  Brain tumors present a grave risk to human life, demanding precise and timely\ndiagnosis for effective treatment. Inaccurate identification of brain tumors\ncan significantly diminish life expectancy, underscoring the critical need for\nprecise diagnostic methods. Manual identification of brain tumors within vast\nMagnetic Resonance Imaging (MRI) image datasets is arduous and time-consuming.\nThus, the development of a reliable deep learning (DL) model is essential to\nenhance diagnostic accuracy and ultimately save lives. This study introduces an\ninnovative optimization-based deep ensemble approach employing transfer\nlearning (TL) to efficiently classify brain tumors. Our methodology includes\nmeticulous preprocessing, reconstruction of TL architectures, fine-tuning, and\nensemble DL models utilizing weighted optimization techniques such as Genetic\nAlgorithm-based Weight Optimization (GAWO) and Grid Search-based Weight\nOptimization (GSWO). Experimentation is conducted on the Figshare\nContrast-Enhanced MRI (CE-MRI) brain tumor dataset, comprising 3064 images. Our\napproach achieves notable accuracy scores, with Xception, ResNet50V2,\nResNet152V2, InceptionResNetV2, GAWO, and GSWO attaining 99.42%, 98.37%,\n98.22%, 98.26%, 99.71%, and 99.76% accuracy, respectively. Notably, GSWO\ndemonstrates superior accuracy, averaging 99.76\\% accuracy across five folds on\nthe Figshare CE-MRI brain tumor dataset. The comparative analysis highlights\nthe significant performance enhancement of our proposed model over existing\ncounterparts. In conclusion, our optimized deep ensemble model exhibits\nexceptional accuracy in swiftly classifying brain tumors. Furthermore, it has\nthe potential to assist neurologists and clinicians in making accurate and\nimmediate diagnostic decisions.\n", "link": "http://arxiv.org/abs/2305.12844v2", "date": "2024-05-06", "relevancy": 2.0685, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5438}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5181}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Optimized%20Ensemble%20Deep%20Learning%20Model%20For%20Brain%20Tumor%20Classification&body=Title%3A%20An%20Optimized%20Ensemble%20Deep%20Learning%20Model%20For%20Brain%20Tumor%20Classification%0AAuthor%3A%20Md.%20Alamin%20Talukder%20and%20Md.%20Manowarul%20Islam%20and%20Md%20Ashraf%20Uddin%0AAbstract%3A%20%20%20Brain%20tumors%20present%20a%20grave%20risk%20to%20human%20life%2C%20demanding%20precise%20and%20timely%0Adiagnosis%20for%20effective%20treatment.%20Inaccurate%20identification%20of%20brain%20tumors%0Acan%20significantly%20diminish%20life%20expectancy%2C%20underscoring%20the%20critical%20need%20for%0Aprecise%20diagnostic%20methods.%20Manual%20identification%20of%20brain%20tumors%20within%20vast%0AMagnetic%20Resonance%20Imaging%20%28MRI%29%20image%20datasets%20is%20arduous%20and%20time-consuming.%0AThus%2C%20the%20development%20of%20a%20reliable%20deep%20learning%20%28DL%29%20model%20is%20essential%20to%0Aenhance%20diagnostic%20accuracy%20and%20ultimately%20save%20lives.%20This%20study%20introduces%20an%0Ainnovative%20optimization-based%20deep%20ensemble%20approach%20employing%20transfer%0Alearning%20%28TL%29%20to%20efficiently%20classify%20brain%20tumors.%20Our%20methodology%20includes%0Ameticulous%20preprocessing%2C%20reconstruction%20of%20TL%20architectures%2C%20fine-tuning%2C%20and%0Aensemble%20DL%20models%20utilizing%20weighted%20optimization%20techniques%20such%20as%20Genetic%0AAlgorithm-based%20Weight%20Optimization%20%28GAWO%29%20and%20Grid%20Search-based%20Weight%0AOptimization%20%28GSWO%29.%20Experimentation%20is%20conducted%20on%20the%20Figshare%0AContrast-Enhanced%20MRI%20%28CE-MRI%29%20brain%20tumor%20dataset%2C%20comprising%203064%20images.%20Our%0Aapproach%20achieves%20notable%20accuracy%20scores%2C%20with%20Xception%2C%20ResNet50V2%2C%0AResNet152V2%2C%20InceptionResNetV2%2C%20GAWO%2C%20and%20GSWO%20attaining%2099.42%25%2C%2098.37%25%2C%0A98.22%25%2C%2098.26%25%2C%2099.71%25%2C%20and%2099.76%25%20accuracy%2C%20respectively.%20Notably%2C%20GSWO%0Ademonstrates%20superior%20accuracy%2C%20averaging%2099.76%5C%25%20accuracy%20across%20five%20folds%20on%0Athe%20Figshare%20CE-MRI%20brain%20tumor%20dataset.%20The%20comparative%20analysis%20highlights%0Athe%20significant%20performance%20enhancement%20of%20our%20proposed%20model%20over%20existing%0Acounterparts.%20In%20conclusion%2C%20our%20optimized%20deep%20ensemble%20model%20exhibits%0Aexceptional%20accuracy%20in%20swiftly%20classifying%20brain%20tumors.%20Furthermore%2C%20it%20has%0Athe%20potential%20to%20assist%20neurologists%20and%20clinicians%20in%20making%20accurate%20and%0Aimmediate%20diagnostic%20decisions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.12844v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Optimized%2520Ensemble%2520Deep%2520Learning%2520Model%2520For%2520Brain%2520Tumor%2520Classification%26entry.906535625%3DMd.%2520Alamin%2520Talukder%2520and%2520Md.%2520Manowarul%2520Islam%2520and%2520Md%2520Ashraf%2520Uddin%26entry.1292438233%3D%2520%2520Brain%2520tumors%2520present%2520a%2520grave%2520risk%2520to%2520human%2520life%252C%2520demanding%2520precise%2520and%2520timely%250Adiagnosis%2520for%2520effective%2520treatment.%2520Inaccurate%2520identification%2520of%2520brain%2520tumors%250Acan%2520significantly%2520diminish%2520life%2520expectancy%252C%2520underscoring%2520the%2520critical%2520need%2520for%250Aprecise%2520diagnostic%2520methods.%2520Manual%2520identification%2520of%2520brain%2520tumors%2520within%2520vast%250AMagnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520image%2520datasets%2520is%2520arduous%2520and%2520time-consuming.%250AThus%252C%2520the%2520development%2520of%2520a%2520reliable%2520deep%2520learning%2520%2528DL%2529%2520model%2520is%2520essential%2520to%250Aenhance%2520diagnostic%2520accuracy%2520and%2520ultimately%2520save%2520lives.%2520This%2520study%2520introduces%2520an%250Ainnovative%2520optimization-based%2520deep%2520ensemble%2520approach%2520employing%2520transfer%250Alearning%2520%2528TL%2529%2520to%2520efficiently%2520classify%2520brain%2520tumors.%2520Our%2520methodology%2520includes%250Ameticulous%2520preprocessing%252C%2520reconstruction%2520of%2520TL%2520architectures%252C%2520fine-tuning%252C%2520and%250Aensemble%2520DL%2520models%2520utilizing%2520weighted%2520optimization%2520techniques%2520such%2520as%2520Genetic%250AAlgorithm-based%2520Weight%2520Optimization%2520%2528GAWO%2529%2520and%2520Grid%2520Search-based%2520Weight%250AOptimization%2520%2528GSWO%2529.%2520Experimentation%2520is%2520conducted%2520on%2520the%2520Figshare%250AContrast-Enhanced%2520MRI%2520%2528CE-MRI%2529%2520brain%2520tumor%2520dataset%252C%2520comprising%25203064%2520images.%2520Our%250Aapproach%2520achieves%2520notable%2520accuracy%2520scores%252C%2520with%2520Xception%252C%2520ResNet50V2%252C%250AResNet152V2%252C%2520InceptionResNetV2%252C%2520GAWO%252C%2520and%2520GSWO%2520attaining%252099.42%2525%252C%252098.37%2525%252C%250A98.22%2525%252C%252098.26%2525%252C%252099.71%2525%252C%2520and%252099.76%2525%2520accuracy%252C%2520respectively.%2520Notably%252C%2520GSWO%250Ademonstrates%2520superior%2520accuracy%252C%2520averaging%252099.76%255C%2525%2520accuracy%2520across%2520five%2520folds%2520on%250Athe%2520Figshare%2520CE-MRI%2520brain%2520tumor%2520dataset.%2520The%2520comparative%2520analysis%2520highlights%250Athe%2520significant%2520performance%2520enhancement%2520of%2520our%2520proposed%2520model%2520over%2520existing%250Acounterparts.%2520In%2520conclusion%252C%2520our%2520optimized%2520deep%2520ensemble%2520model%2520exhibits%250Aexceptional%2520accuracy%2520in%2520swiftly%2520classifying%2520brain%2520tumors.%2520Furthermore%252C%2520it%2520has%250Athe%2520potential%2520to%2520assist%2520neurologists%2520and%2520clinicians%2520in%2520making%2520accurate%2520and%250Aimmediate%2520diagnostic%2520decisions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.12844v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Optimized%20Ensemble%20Deep%20Learning%20Model%20For%20Brain%20Tumor%20Classification&entry.906535625=Md.%20Alamin%20Talukder%20and%20Md.%20Manowarul%20Islam%20and%20Md%20Ashraf%20Uddin&entry.1292438233=%20%20Brain%20tumors%20present%20a%20grave%20risk%20to%20human%20life%2C%20demanding%20precise%20and%20timely%0Adiagnosis%20for%20effective%20treatment.%20Inaccurate%20identification%20of%20brain%20tumors%0Acan%20significantly%20diminish%20life%20expectancy%2C%20underscoring%20the%20critical%20need%20for%0Aprecise%20diagnostic%20methods.%20Manual%20identification%20of%20brain%20tumors%20within%20vast%0AMagnetic%20Resonance%20Imaging%20%28MRI%29%20image%20datasets%20is%20arduous%20and%20time-consuming.%0AThus%2C%20the%20development%20of%20a%20reliable%20deep%20learning%20%28DL%29%20model%20is%20essential%20to%0Aenhance%20diagnostic%20accuracy%20and%20ultimately%20save%20lives.%20This%20study%20introduces%20an%0Ainnovative%20optimization-based%20deep%20ensemble%20approach%20employing%20transfer%0Alearning%20%28TL%29%20to%20efficiently%20classify%20brain%20tumors.%20Our%20methodology%20includes%0Ameticulous%20preprocessing%2C%20reconstruction%20of%20TL%20architectures%2C%20fine-tuning%2C%20and%0Aensemble%20DL%20models%20utilizing%20weighted%20optimization%20techniques%20such%20as%20Genetic%0AAlgorithm-based%20Weight%20Optimization%20%28GAWO%29%20and%20Grid%20Search-based%20Weight%0AOptimization%20%28GSWO%29.%20Experimentation%20is%20conducted%20on%20the%20Figshare%0AContrast-Enhanced%20MRI%20%28CE-MRI%29%20brain%20tumor%20dataset%2C%20comprising%203064%20images.%20Our%0Aapproach%20achieves%20notable%20accuracy%20scores%2C%20with%20Xception%2C%20ResNet50V2%2C%0AResNet152V2%2C%20InceptionResNetV2%2C%20GAWO%2C%20and%20GSWO%20attaining%2099.42%25%2C%2098.37%25%2C%0A98.22%25%2C%2098.26%25%2C%2099.71%25%2C%20and%2099.76%25%20accuracy%2C%20respectively.%20Notably%2C%20GSWO%0Ademonstrates%20superior%20accuracy%2C%20averaging%2099.76%5C%25%20accuracy%20across%20five%20folds%20on%0Athe%20Figshare%20CE-MRI%20brain%20tumor%20dataset.%20The%20comparative%20analysis%20highlights%0Athe%20significant%20performance%20enhancement%20of%20our%20proposed%20model%20over%20existing%0Acounterparts.%20In%20conclusion%2C%20our%20optimized%20deep%20ensemble%20model%20exhibits%0Aexceptional%20accuracy%20in%20swiftly%20classifying%20brain%20tumors.%20Furthermore%2C%20it%20has%0Athe%20potential%20to%20assist%20neurologists%20and%20clinicians%20in%20making%20accurate%20and%0Aimmediate%20diagnostic%20decisions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.12844v2&entry.124074799=Read"},
{"title": "Exploring Interactive Semantic Alignment for Efficient HOI Detection\n  with Vision-language Model", "author": "Jihao Dong and Renjie Pan and Hua Yang", "abstract": "  Human-Object Interaction (HOI) detection aims to localize human-object pairs\nand comprehend their interactions. Recently, two-stage transformer-based\nmethods have demonstrated competitive performance. However, these methods\nfrequently focus on object appearance features and ignore global contextual\ninformation. Besides, vision-language model CLIP which effectively aligns\nvisual and text embeddings has shown great potential in zero-shot HOI\ndetection. Based on the former facts, We introduce a novel HOI detector named\nISA-HOI, which extensively leverages knowledge from CLIP, aligning interactive\nsemantics between visual and textual features. We first extract global context\nof image and local features of object to Improve interaction Features in images\n(IF). On the other hand, we propose a Verb Semantic Improvement (VSI) module to\nenhance textual features of verb labels via cross-modal fusion. Ultimately, our\nmethod achieves competitive results on the HICO-DET and V-COCO benchmarks with\nmuch fewer training epochs, and outperforms the state-of-the-art under\nzero-shot settings.\n", "link": "http://arxiv.org/abs/2404.12678v2", "date": "2024-05-06", "relevancy": 2.0675, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5418}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5242}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Interactive%20Semantic%20Alignment%20for%20Efficient%20HOI%20Detection%0A%20%20with%20Vision-language%20Model&body=Title%3A%20Exploring%20Interactive%20Semantic%20Alignment%20for%20Efficient%20HOI%20Detection%0A%20%20with%20Vision-language%20Model%0AAuthor%3A%20Jihao%20Dong%20and%20Renjie%20Pan%20and%20Hua%20Yang%0AAbstract%3A%20%20%20Human-Object%20Interaction%20%28HOI%29%20detection%20aims%20to%20localize%20human-object%20pairs%0Aand%20comprehend%20their%20interactions.%20Recently%2C%20two-stage%20transformer-based%0Amethods%20have%20demonstrated%20competitive%20performance.%20However%2C%20these%20methods%0Afrequently%20focus%20on%20object%20appearance%20features%20and%20ignore%20global%20contextual%0Ainformation.%20Besides%2C%20vision-language%20model%20CLIP%20which%20effectively%20aligns%0Avisual%20and%20text%20embeddings%20has%20shown%20great%20potential%20in%20zero-shot%20HOI%0Adetection.%20Based%20on%20the%20former%20facts%2C%20We%20introduce%20a%20novel%20HOI%20detector%20named%0AISA-HOI%2C%20which%20extensively%20leverages%20knowledge%20from%20CLIP%2C%20aligning%20interactive%0Asemantics%20between%20visual%20and%20textual%20features.%20We%20first%20extract%20global%20context%0Aof%20image%20and%20local%20features%20of%20object%20to%20Improve%20interaction%20Features%20in%20images%0A%28IF%29.%20On%20the%20other%20hand%2C%20we%20propose%20a%20Verb%20Semantic%20Improvement%20%28VSI%29%20module%20to%0Aenhance%20textual%20features%20of%20verb%20labels%20via%20cross-modal%20fusion.%20Ultimately%2C%20our%0Amethod%20achieves%20competitive%20results%20on%20the%20HICO-DET%20and%20V-COCO%20benchmarks%20with%0Amuch%20fewer%20training%20epochs%2C%20and%20outperforms%20the%20state-of-the-art%20under%0Azero-shot%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12678v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Interactive%2520Semantic%2520Alignment%2520for%2520Efficient%2520HOI%2520Detection%250A%2520%2520with%2520Vision-language%2520Model%26entry.906535625%3DJihao%2520Dong%2520and%2520Renjie%2520Pan%2520and%2520Hua%2520Yang%26entry.1292438233%3D%2520%2520Human-Object%2520Interaction%2520%2528HOI%2529%2520detection%2520aims%2520to%2520localize%2520human-object%2520pairs%250Aand%2520comprehend%2520their%2520interactions.%2520Recently%252C%2520two-stage%2520transformer-based%250Amethods%2520have%2520demonstrated%2520competitive%2520performance.%2520However%252C%2520these%2520methods%250Afrequently%2520focus%2520on%2520object%2520appearance%2520features%2520and%2520ignore%2520global%2520contextual%250Ainformation.%2520Besides%252C%2520vision-language%2520model%2520CLIP%2520which%2520effectively%2520aligns%250Avisual%2520and%2520text%2520embeddings%2520has%2520shown%2520great%2520potential%2520in%2520zero-shot%2520HOI%250Adetection.%2520Based%2520on%2520the%2520former%2520facts%252C%2520We%2520introduce%2520a%2520novel%2520HOI%2520detector%2520named%250AISA-HOI%252C%2520which%2520extensively%2520leverages%2520knowledge%2520from%2520CLIP%252C%2520aligning%2520interactive%250Asemantics%2520between%2520visual%2520and%2520textual%2520features.%2520We%2520first%2520extract%2520global%2520context%250Aof%2520image%2520and%2520local%2520features%2520of%2520object%2520to%2520Improve%2520interaction%2520Features%2520in%2520images%250A%2528IF%2529.%2520On%2520the%2520other%2520hand%252C%2520we%2520propose%2520a%2520Verb%2520Semantic%2520Improvement%2520%2528VSI%2529%2520module%2520to%250Aenhance%2520textual%2520features%2520of%2520verb%2520labels%2520via%2520cross-modal%2520fusion.%2520Ultimately%252C%2520our%250Amethod%2520achieves%2520competitive%2520results%2520on%2520the%2520HICO-DET%2520and%2520V-COCO%2520benchmarks%2520with%250Amuch%2520fewer%2520training%2520epochs%252C%2520and%2520outperforms%2520the%2520state-of-the-art%2520under%250Azero-shot%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12678v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Interactive%20Semantic%20Alignment%20for%20Efficient%20HOI%20Detection%0A%20%20with%20Vision-language%20Model&entry.906535625=Jihao%20Dong%20and%20Renjie%20Pan%20and%20Hua%20Yang&entry.1292438233=%20%20Human-Object%20Interaction%20%28HOI%29%20detection%20aims%20to%20localize%20human-object%20pairs%0Aand%20comprehend%20their%20interactions.%20Recently%2C%20two-stage%20transformer-based%0Amethods%20have%20demonstrated%20competitive%20performance.%20However%2C%20these%20methods%0Afrequently%20focus%20on%20object%20appearance%20features%20and%20ignore%20global%20contextual%0Ainformation.%20Besides%2C%20vision-language%20model%20CLIP%20which%20effectively%20aligns%0Avisual%20and%20text%20embeddings%20has%20shown%20great%20potential%20in%20zero-shot%20HOI%0Adetection.%20Based%20on%20the%20former%20facts%2C%20We%20introduce%20a%20novel%20HOI%20detector%20named%0AISA-HOI%2C%20which%20extensively%20leverages%20knowledge%20from%20CLIP%2C%20aligning%20interactive%0Asemantics%20between%20visual%20and%20textual%20features.%20We%20first%20extract%20global%20context%0Aof%20image%20and%20local%20features%20of%20object%20to%20Improve%20interaction%20Features%20in%20images%0A%28IF%29.%20On%20the%20other%20hand%2C%20we%20propose%20a%20Verb%20Semantic%20Improvement%20%28VSI%29%20module%20to%0Aenhance%20textual%20features%20of%20verb%20labels%20via%20cross-modal%20fusion.%20Ultimately%2C%20our%0Amethod%20achieves%20competitive%20results%20on%20the%20HICO-DET%20and%20V-COCO%20benchmarks%20with%0Amuch%20fewer%20training%20epochs%2C%20and%20outperforms%20the%20state-of-the-art%20under%0Azero-shot%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12678v2&entry.124074799=Read"},
{"title": "World Models for Autonomous Driving: An Initial Survey", "author": "Yanchen Guan and Haicheng Liao and Zhenning Li and Guohui Zhang and Chengzhong Xu", "abstract": "  In the rapidly evolving landscape of autonomous driving, the capability to\naccurately predict future events and assess their implications is paramount for\nboth safety and efficiency, critically aiding the decision-making process.\nWorld models have emerged as a transformative approach, enabling autonomous\ndriving systems to synthesize and interpret vast amounts of sensor data,\nthereby predicting potential future scenarios and compensating for information\ngaps. This paper provides an initial review of the current state and\nprospective advancements of world models in autonomous driving, spanning their\ntheoretical underpinnings, practical applications, and the ongoing research\nefforts aimed at overcoming existing limitations. Highlighting the significant\nrole of world models in advancing autonomous driving technologies, this survey\naspires to serve as a foundational reference for the research community,\nfacilitating swift access to and comprehension of this burgeoning field, and\ninspiring continued innovation and exploration.\n", "link": "http://arxiv.org/abs/2403.02622v2", "date": "2024-05-06", "relevancy": 2.0599, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5207}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5163}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20World%20Models%20for%20Autonomous%20Driving%3A%20An%20Initial%20Survey&body=Title%3A%20World%20Models%20for%20Autonomous%20Driving%3A%20An%20Initial%20Survey%0AAuthor%3A%20Yanchen%20Guan%20and%20Haicheng%20Liao%20and%20Zhenning%20Li%20and%20Guohui%20Zhang%20and%20Chengzhong%20Xu%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20landscape%20of%20autonomous%20driving%2C%20the%20capability%20to%0Aaccurately%20predict%20future%20events%20and%20assess%20their%20implications%20is%20paramount%20for%0Aboth%20safety%20and%20efficiency%2C%20critically%20aiding%20the%20decision-making%20process.%0AWorld%20models%20have%20emerged%20as%20a%20transformative%20approach%2C%20enabling%20autonomous%0Adriving%20systems%20to%20synthesize%20and%20interpret%20vast%20amounts%20of%20sensor%20data%2C%0Athereby%20predicting%20potential%20future%20scenarios%20and%20compensating%20for%20information%0Agaps.%20This%20paper%20provides%20an%20initial%20review%20of%20the%20current%20state%20and%0Aprospective%20advancements%20of%20world%20models%20in%20autonomous%20driving%2C%20spanning%20their%0Atheoretical%20underpinnings%2C%20practical%20applications%2C%20and%20the%20ongoing%20research%0Aefforts%20aimed%20at%20overcoming%20existing%20limitations.%20Highlighting%20the%20significant%0Arole%20of%20world%20models%20in%20advancing%20autonomous%20driving%20technologies%2C%20this%20survey%0Aaspires%20to%20serve%20as%20a%20foundational%20reference%20for%20the%20research%20community%2C%0Afacilitating%20swift%20access%20to%20and%20comprehension%20of%20this%20burgeoning%20field%2C%20and%0Ainspiring%20continued%20innovation%20and%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02622v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorld%2520Models%2520for%2520Autonomous%2520Driving%253A%2520An%2520Initial%2520Survey%26entry.906535625%3DYanchen%2520Guan%2520and%2520Haicheng%2520Liao%2520and%2520Zhenning%2520Li%2520and%2520Guohui%2520Zhang%2520and%2520Chengzhong%2520Xu%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520landscape%2520of%2520autonomous%2520driving%252C%2520the%2520capability%2520to%250Aaccurately%2520predict%2520future%2520events%2520and%2520assess%2520their%2520implications%2520is%2520paramount%2520for%250Aboth%2520safety%2520and%2520efficiency%252C%2520critically%2520aiding%2520the%2520decision-making%2520process.%250AWorld%2520models%2520have%2520emerged%2520as%2520a%2520transformative%2520approach%252C%2520enabling%2520autonomous%250Adriving%2520systems%2520to%2520synthesize%2520and%2520interpret%2520vast%2520amounts%2520of%2520sensor%2520data%252C%250Athereby%2520predicting%2520potential%2520future%2520scenarios%2520and%2520compensating%2520for%2520information%250Agaps.%2520This%2520paper%2520provides%2520an%2520initial%2520review%2520of%2520the%2520current%2520state%2520and%250Aprospective%2520advancements%2520of%2520world%2520models%2520in%2520autonomous%2520driving%252C%2520spanning%2520their%250Atheoretical%2520underpinnings%252C%2520practical%2520applications%252C%2520and%2520the%2520ongoing%2520research%250Aefforts%2520aimed%2520at%2520overcoming%2520existing%2520limitations.%2520Highlighting%2520the%2520significant%250Arole%2520of%2520world%2520models%2520in%2520advancing%2520autonomous%2520driving%2520technologies%252C%2520this%2520survey%250Aaspires%2520to%2520serve%2520as%2520a%2520foundational%2520reference%2520for%2520the%2520research%2520community%252C%250Afacilitating%2520swift%2520access%2520to%2520and%2520comprehension%2520of%2520this%2520burgeoning%2520field%252C%2520and%250Ainspiring%2520continued%2520innovation%2520and%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02622v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=World%20Models%20for%20Autonomous%20Driving%3A%20An%20Initial%20Survey&entry.906535625=Yanchen%20Guan%20and%20Haicheng%20Liao%20and%20Zhenning%20Li%20and%20Guohui%20Zhang%20and%20Chengzhong%20Xu&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20landscape%20of%20autonomous%20driving%2C%20the%20capability%20to%0Aaccurately%20predict%20future%20events%20and%20assess%20their%20implications%20is%20paramount%20for%0Aboth%20safety%20and%20efficiency%2C%20critically%20aiding%20the%20decision-making%20process.%0AWorld%20models%20have%20emerged%20as%20a%20transformative%20approach%2C%20enabling%20autonomous%0Adriving%20systems%20to%20synthesize%20and%20interpret%20vast%20amounts%20of%20sensor%20data%2C%0Athereby%20predicting%20potential%20future%20scenarios%20and%20compensating%20for%20information%0Agaps.%20This%20paper%20provides%20an%20initial%20review%20of%20the%20current%20state%20and%0Aprospective%20advancements%20of%20world%20models%20in%20autonomous%20driving%2C%20spanning%20their%0Atheoretical%20underpinnings%2C%20practical%20applications%2C%20and%20the%20ongoing%20research%0Aefforts%20aimed%20at%20overcoming%20existing%20limitations.%20Highlighting%20the%20significant%0Arole%20of%20world%20models%20in%20advancing%20autonomous%20driving%20technologies%2C%20this%20survey%0Aaspires%20to%20serve%20as%20a%20foundational%20reference%20for%20the%20research%20community%2C%0Afacilitating%20swift%20access%20to%20and%20comprehension%20of%20this%20burgeoning%20field%2C%20and%0Ainspiring%20continued%20innovation%20and%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02622v2&entry.124074799=Read"},
{"title": "Understanding the Vulnerability of Skeleton-based Human Activity\n  Recognition via Black-box Attack", "author": "Yunfeng Diao and He Wang and Tianjia Shao and Yong-Liang Yang and Kun Zhou and David Hogg and Meng Wang", "abstract": "  Human Activity Recognition (HAR) has been employed in a wide range of\napplications, e.g. self-driving cars, where safety and lives are at stake.\nRecently, the robustness of skeleton-based HAR methods have been questioned due\nto their vulnerability to adversarial attacks. However, the proposed attacks\nrequire the full-knowledge of the attacked classifier, which is overly\nrestrictive. In this paper, we show such threats indeed exist, even when the\nattacker only has access to the input/output of the model. To this end, we\npropose the very first black-box adversarial attack approach in skeleton-based\nHAR called BASAR. BASAR explores the interplay between the classification\nboundary and the natural motion manifold. To our best knowledge, this is the\nfirst time data manifold is introduced in adversarial attacks on time series.\nVia BASAR, we find on-manifold adversarial samples are extremely deceitful and\nrather common in skeletal motions, in contrast to the common belief that\nadversarial samples only exist off-manifold. Through exhaustive evaluation, we\nshow that BASAR can deliver successful attacks across classifiers, datasets,\nand attack modes. By attack, BASAR helps identify the potential causes of the\nmodel vulnerability and provides insights on possible improvements. Finally, to\nmitigate the newly identified threat, we propose a new adversarial training\napproach by leveraging the sophisticated distributions of on/off-manifold\nadversarial samples, called mixed manifold-based adversarial training (MMAT).\nMMAT can successfully help defend against adversarial attacks without\ncompromising classification accuracy.\n", "link": "http://arxiv.org/abs/2211.11312v2", "date": "2024-05-06", "relevancy": 2.0591, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.543}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4947}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20the%20Vulnerability%20of%20Skeleton-based%20Human%20Activity%0A%20%20Recognition%20via%20Black-box%20Attack&body=Title%3A%20Understanding%20the%20Vulnerability%20of%20Skeleton-based%20Human%20Activity%0A%20%20Recognition%20via%20Black-box%20Attack%0AAuthor%3A%20Yunfeng%20Diao%20and%20He%20Wang%20and%20Tianjia%20Shao%20and%20Yong-Liang%20Yang%20and%20Kun%20Zhou%20and%20David%20Hogg%20and%20Meng%20Wang%0AAbstract%3A%20%20%20Human%20Activity%20Recognition%20%28HAR%29%20has%20been%20employed%20in%20a%20wide%20range%20of%0Aapplications%2C%20e.g.%20self-driving%20cars%2C%20where%20safety%20and%20lives%20are%20at%20stake.%0ARecently%2C%20the%20robustness%20of%20skeleton-based%20HAR%20methods%20have%20been%20questioned%20due%0Ato%20their%20vulnerability%20to%20adversarial%20attacks.%20However%2C%20the%20proposed%20attacks%0Arequire%20the%20full-knowledge%20of%20the%20attacked%20classifier%2C%20which%20is%20overly%0Arestrictive.%20In%20this%20paper%2C%20we%20show%20such%20threats%20indeed%20exist%2C%20even%20when%20the%0Aattacker%20only%20has%20access%20to%20the%20input/output%20of%20the%20model.%20To%20this%20end%2C%20we%0Apropose%20the%20very%20first%20black-box%20adversarial%20attack%20approach%20in%20skeleton-based%0AHAR%20called%20BASAR.%20BASAR%20explores%20the%20interplay%20between%20the%20classification%0Aboundary%20and%20the%20natural%20motion%20manifold.%20To%20our%20best%20knowledge%2C%20this%20is%20the%0Afirst%20time%20data%20manifold%20is%20introduced%20in%20adversarial%20attacks%20on%20time%20series.%0AVia%20BASAR%2C%20we%20find%20on-manifold%20adversarial%20samples%20are%20extremely%20deceitful%20and%0Arather%20common%20in%20skeletal%20motions%2C%20in%20contrast%20to%20the%20common%20belief%20that%0Aadversarial%20samples%20only%20exist%20off-manifold.%20Through%20exhaustive%20evaluation%2C%20we%0Ashow%20that%20BASAR%20can%20deliver%20successful%20attacks%20across%20classifiers%2C%20datasets%2C%0Aand%20attack%20modes.%20By%20attack%2C%20BASAR%20helps%20identify%20the%20potential%20causes%20of%20the%0Amodel%20vulnerability%20and%20provides%20insights%20on%20possible%20improvements.%20Finally%2C%20to%0Amitigate%20the%20newly%20identified%20threat%2C%20we%20propose%20a%20new%20adversarial%20training%0Aapproach%20by%20leveraging%20the%20sophisticated%20distributions%20of%20on/off-manifold%0Aadversarial%20samples%2C%20called%20mixed%20manifold-based%20adversarial%20training%20%28MMAT%29.%0AMMAT%20can%20successfully%20help%20defend%20against%20adversarial%20attacks%20without%0Acompromising%20classification%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.11312v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520the%2520Vulnerability%2520of%2520Skeleton-based%2520Human%2520Activity%250A%2520%2520Recognition%2520via%2520Black-box%2520Attack%26entry.906535625%3DYunfeng%2520Diao%2520and%2520He%2520Wang%2520and%2520Tianjia%2520Shao%2520and%2520Yong-Liang%2520Yang%2520and%2520Kun%2520Zhou%2520and%2520David%2520Hogg%2520and%2520Meng%2520Wang%26entry.1292438233%3D%2520%2520Human%2520Activity%2520Recognition%2520%2528HAR%2529%2520has%2520been%2520employed%2520in%2520a%2520wide%2520range%2520of%250Aapplications%252C%2520e.g.%2520self-driving%2520cars%252C%2520where%2520safety%2520and%2520lives%2520are%2520at%2520stake.%250ARecently%252C%2520the%2520robustness%2520of%2520skeleton-based%2520HAR%2520methods%2520have%2520been%2520questioned%2520due%250Ato%2520their%2520vulnerability%2520to%2520adversarial%2520attacks.%2520However%252C%2520the%2520proposed%2520attacks%250Arequire%2520the%2520full-knowledge%2520of%2520the%2520attacked%2520classifier%252C%2520which%2520is%2520overly%250Arestrictive.%2520In%2520this%2520paper%252C%2520we%2520show%2520such%2520threats%2520indeed%2520exist%252C%2520even%2520when%2520the%250Aattacker%2520only%2520has%2520access%2520to%2520the%2520input/output%2520of%2520the%2520model.%2520To%2520this%2520end%252C%2520we%250Apropose%2520the%2520very%2520first%2520black-box%2520adversarial%2520attack%2520approach%2520in%2520skeleton-based%250AHAR%2520called%2520BASAR.%2520BASAR%2520explores%2520the%2520interplay%2520between%2520the%2520classification%250Aboundary%2520and%2520the%2520natural%2520motion%2520manifold.%2520To%2520our%2520best%2520knowledge%252C%2520this%2520is%2520the%250Afirst%2520time%2520data%2520manifold%2520is%2520introduced%2520in%2520adversarial%2520attacks%2520on%2520time%2520series.%250AVia%2520BASAR%252C%2520we%2520find%2520on-manifold%2520adversarial%2520samples%2520are%2520extremely%2520deceitful%2520and%250Arather%2520common%2520in%2520skeletal%2520motions%252C%2520in%2520contrast%2520to%2520the%2520common%2520belief%2520that%250Aadversarial%2520samples%2520only%2520exist%2520off-manifold.%2520Through%2520exhaustive%2520evaluation%252C%2520we%250Ashow%2520that%2520BASAR%2520can%2520deliver%2520successful%2520attacks%2520across%2520classifiers%252C%2520datasets%252C%250Aand%2520attack%2520modes.%2520By%2520attack%252C%2520BASAR%2520helps%2520identify%2520the%2520potential%2520causes%2520of%2520the%250Amodel%2520vulnerability%2520and%2520provides%2520insights%2520on%2520possible%2520improvements.%2520Finally%252C%2520to%250Amitigate%2520the%2520newly%2520identified%2520threat%252C%2520we%2520propose%2520a%2520new%2520adversarial%2520training%250Aapproach%2520by%2520leveraging%2520the%2520sophisticated%2520distributions%2520of%2520on/off-manifold%250Aadversarial%2520samples%252C%2520called%2520mixed%2520manifold-based%2520adversarial%2520training%2520%2528MMAT%2529.%250AMMAT%2520can%2520successfully%2520help%2520defend%2520against%2520adversarial%2520attacks%2520without%250Acompromising%2520classification%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.11312v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20the%20Vulnerability%20of%20Skeleton-based%20Human%20Activity%0A%20%20Recognition%20via%20Black-box%20Attack&entry.906535625=Yunfeng%20Diao%20and%20He%20Wang%20and%20Tianjia%20Shao%20and%20Yong-Liang%20Yang%20and%20Kun%20Zhou%20and%20David%20Hogg%20and%20Meng%20Wang&entry.1292438233=%20%20Human%20Activity%20Recognition%20%28HAR%29%20has%20been%20employed%20in%20a%20wide%20range%20of%0Aapplications%2C%20e.g.%20self-driving%20cars%2C%20where%20safety%20and%20lives%20are%20at%20stake.%0ARecently%2C%20the%20robustness%20of%20skeleton-based%20HAR%20methods%20have%20been%20questioned%20due%0Ato%20their%20vulnerability%20to%20adversarial%20attacks.%20However%2C%20the%20proposed%20attacks%0Arequire%20the%20full-knowledge%20of%20the%20attacked%20classifier%2C%20which%20is%20overly%0Arestrictive.%20In%20this%20paper%2C%20we%20show%20such%20threats%20indeed%20exist%2C%20even%20when%20the%0Aattacker%20only%20has%20access%20to%20the%20input/output%20of%20the%20model.%20To%20this%20end%2C%20we%0Apropose%20the%20very%20first%20black-box%20adversarial%20attack%20approach%20in%20skeleton-based%0AHAR%20called%20BASAR.%20BASAR%20explores%20the%20interplay%20between%20the%20classification%0Aboundary%20and%20the%20natural%20motion%20manifold.%20To%20our%20best%20knowledge%2C%20this%20is%20the%0Afirst%20time%20data%20manifold%20is%20introduced%20in%20adversarial%20attacks%20on%20time%20series.%0AVia%20BASAR%2C%20we%20find%20on-manifold%20adversarial%20samples%20are%20extremely%20deceitful%20and%0Arather%20common%20in%20skeletal%20motions%2C%20in%20contrast%20to%20the%20common%20belief%20that%0Aadversarial%20samples%20only%20exist%20off-manifold.%20Through%20exhaustive%20evaluation%2C%20we%0Ashow%20that%20BASAR%20can%20deliver%20successful%20attacks%20across%20classifiers%2C%20datasets%2C%0Aand%20attack%20modes.%20By%20attack%2C%20BASAR%20helps%20identify%20the%20potential%20causes%20of%20the%0Amodel%20vulnerability%20and%20provides%20insights%20on%20possible%20improvements.%20Finally%2C%20to%0Amitigate%20the%20newly%20identified%20threat%2C%20we%20propose%20a%20new%20adversarial%20training%0Aapproach%20by%20leveraging%20the%20sophisticated%20distributions%20of%20on/off-manifold%0Aadversarial%20samples%2C%20called%20mixed%20manifold-based%20adversarial%20training%20%28MMAT%29.%0AMMAT%20can%20successfully%20help%20defend%20against%20adversarial%20attacks%20without%0Acompromising%20classification%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.11312v2&entry.124074799=Read"},
{"title": "Retinexmamba: Retinex-based Mamba for Low-light Image Enhancement", "author": "Jiesong Bai and Yuhao Yin and Qiyuan He", "abstract": "  In the field of low-light image enhancement, both traditional Retinex methods\nand advanced deep learning techniques such as Retinexformer have shown distinct\nadvantages and limitations. Traditional Retinex methods, designed to mimic the\nhuman eye's perception of brightness and color, decompose images into\nillumination and reflection components but struggle with noise management and\ndetail preservation under low light conditions. Retinexformer enhances\nillumination estimation through traditional self-attention mechanisms, but\nfaces challenges with insufficient interpretability and suboptimal enhancement\neffects. To overcome these limitations, this paper introduces the RetinexMamba\narchitecture. RetinexMamba not only captures the physical intuitiveness of\ntraditional Retinex methods but also integrates the deep learning framework of\nRetinexformer, leveraging the computational efficiency of State Space Models\n(SSMs) to enhance processing speed. This architecture features innovative\nillumination estimators and damage restorer mechanisms that maintain image\nquality during enhancement. Moreover, RetinexMamba replaces the IG-MSA\n(Illumination-Guided Multi-Head Attention) in Retinexformer with a\nFused-Attention mechanism, improving the model's interpretability. Experimental\nevaluations on the LOL dataset show that RetinexMamba outperforms existing deep\nlearning approaches based on Retinex theory in both quantitative and\nqualitative metrics, confirming its effectiveness and superiority in enhancing\nlow-light images.\n", "link": "http://arxiv.org/abs/2405.03349v1", "date": "2024-05-06", "relevancy": 2.0534, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5349}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5185}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retinexmamba%3A%20Retinex-based%20Mamba%20for%20Low-light%20Image%20Enhancement&body=Title%3A%20Retinexmamba%3A%20Retinex-based%20Mamba%20for%20Low-light%20Image%20Enhancement%0AAuthor%3A%20Jiesong%20Bai%20and%20Yuhao%20Yin%20and%20Qiyuan%20He%0AAbstract%3A%20%20%20In%20the%20field%20of%20low-light%20image%20enhancement%2C%20both%20traditional%20Retinex%20methods%0Aand%20advanced%20deep%20learning%20techniques%20such%20as%20Retinexformer%20have%20shown%20distinct%0Aadvantages%20and%20limitations.%20Traditional%20Retinex%20methods%2C%20designed%20to%20mimic%20the%0Ahuman%20eye%27s%20perception%20of%20brightness%20and%20color%2C%20decompose%20images%20into%0Aillumination%20and%20reflection%20components%20but%20struggle%20with%20noise%20management%20and%0Adetail%20preservation%20under%20low%20light%20conditions.%20Retinexformer%20enhances%0Aillumination%20estimation%20through%20traditional%20self-attention%20mechanisms%2C%20but%0Afaces%20challenges%20with%20insufficient%20interpretability%20and%20suboptimal%20enhancement%0Aeffects.%20To%20overcome%20these%20limitations%2C%20this%20paper%20introduces%20the%20RetinexMamba%0Aarchitecture.%20RetinexMamba%20not%20only%20captures%20the%20physical%20intuitiveness%20of%0Atraditional%20Retinex%20methods%20but%20also%20integrates%20the%20deep%20learning%20framework%20of%0ARetinexformer%2C%20leveraging%20the%20computational%20efficiency%20of%20State%20Space%20Models%0A%28SSMs%29%20to%20enhance%20processing%20speed.%20This%20architecture%20features%20innovative%0Aillumination%20estimators%20and%20damage%20restorer%20mechanisms%20that%20maintain%20image%0Aquality%20during%20enhancement.%20Moreover%2C%20RetinexMamba%20replaces%20the%20IG-MSA%0A%28Illumination-Guided%20Multi-Head%20Attention%29%20in%20Retinexformer%20with%20a%0AFused-Attention%20mechanism%2C%20improving%20the%20model%27s%20interpretability.%20Experimental%0Aevaluations%20on%20the%20LOL%20dataset%20show%20that%20RetinexMamba%20outperforms%20existing%20deep%0Alearning%20approaches%20based%20on%20Retinex%20theory%20in%20both%20quantitative%20and%0Aqualitative%20metrics%2C%20confirming%20its%20effectiveness%20and%20superiority%20in%20enhancing%0Alow-light%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetinexmamba%253A%2520Retinex-based%2520Mamba%2520for%2520Low-light%2520Image%2520Enhancement%26entry.906535625%3DJiesong%2520Bai%2520and%2520Yuhao%2520Yin%2520and%2520Qiyuan%2520He%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520low-light%2520image%2520enhancement%252C%2520both%2520traditional%2520Retinex%2520methods%250Aand%2520advanced%2520deep%2520learning%2520techniques%2520such%2520as%2520Retinexformer%2520have%2520shown%2520distinct%250Aadvantages%2520and%2520limitations.%2520Traditional%2520Retinex%2520methods%252C%2520designed%2520to%2520mimic%2520the%250Ahuman%2520eye%2527s%2520perception%2520of%2520brightness%2520and%2520color%252C%2520decompose%2520images%2520into%250Aillumination%2520and%2520reflection%2520components%2520but%2520struggle%2520with%2520noise%2520management%2520and%250Adetail%2520preservation%2520under%2520low%2520light%2520conditions.%2520Retinexformer%2520enhances%250Aillumination%2520estimation%2520through%2520traditional%2520self-attention%2520mechanisms%252C%2520but%250Afaces%2520challenges%2520with%2520insufficient%2520interpretability%2520and%2520suboptimal%2520enhancement%250Aeffects.%2520To%2520overcome%2520these%2520limitations%252C%2520this%2520paper%2520introduces%2520the%2520RetinexMamba%250Aarchitecture.%2520RetinexMamba%2520not%2520only%2520captures%2520the%2520physical%2520intuitiveness%2520of%250Atraditional%2520Retinex%2520methods%2520but%2520also%2520integrates%2520the%2520deep%2520learning%2520framework%2520of%250ARetinexformer%252C%2520leveraging%2520the%2520computational%2520efficiency%2520of%2520State%2520Space%2520Models%250A%2528SSMs%2529%2520to%2520enhance%2520processing%2520speed.%2520This%2520architecture%2520features%2520innovative%250Aillumination%2520estimators%2520and%2520damage%2520restorer%2520mechanisms%2520that%2520maintain%2520image%250Aquality%2520during%2520enhancement.%2520Moreover%252C%2520RetinexMamba%2520replaces%2520the%2520IG-MSA%250A%2528Illumination-Guided%2520Multi-Head%2520Attention%2529%2520in%2520Retinexformer%2520with%2520a%250AFused-Attention%2520mechanism%252C%2520improving%2520the%2520model%2527s%2520interpretability.%2520Experimental%250Aevaluations%2520on%2520the%2520LOL%2520dataset%2520show%2520that%2520RetinexMamba%2520outperforms%2520existing%2520deep%250Alearning%2520approaches%2520based%2520on%2520Retinex%2520theory%2520in%2520both%2520quantitative%2520and%250Aqualitative%2520metrics%252C%2520confirming%2520its%2520effectiveness%2520and%2520superiority%2520in%2520enhancing%250Alow-light%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retinexmamba%3A%20Retinex-based%20Mamba%20for%20Low-light%20Image%20Enhancement&entry.906535625=Jiesong%20Bai%20and%20Yuhao%20Yin%20and%20Qiyuan%20He&entry.1292438233=%20%20In%20the%20field%20of%20low-light%20image%20enhancement%2C%20both%20traditional%20Retinex%20methods%0Aand%20advanced%20deep%20learning%20techniques%20such%20as%20Retinexformer%20have%20shown%20distinct%0Aadvantages%20and%20limitations.%20Traditional%20Retinex%20methods%2C%20designed%20to%20mimic%20the%0Ahuman%20eye%27s%20perception%20of%20brightness%20and%20color%2C%20decompose%20images%20into%0Aillumination%20and%20reflection%20components%20but%20struggle%20with%20noise%20management%20and%0Adetail%20preservation%20under%20low%20light%20conditions.%20Retinexformer%20enhances%0Aillumination%20estimation%20through%20traditional%20self-attention%20mechanisms%2C%20but%0Afaces%20challenges%20with%20insufficient%20interpretability%20and%20suboptimal%20enhancement%0Aeffects.%20To%20overcome%20these%20limitations%2C%20this%20paper%20introduces%20the%20RetinexMamba%0Aarchitecture.%20RetinexMamba%20not%20only%20captures%20the%20physical%20intuitiveness%20of%0Atraditional%20Retinex%20methods%20but%20also%20integrates%20the%20deep%20learning%20framework%20of%0ARetinexformer%2C%20leveraging%20the%20computational%20efficiency%20of%20State%20Space%20Models%0A%28SSMs%29%20to%20enhance%20processing%20speed.%20This%20architecture%20features%20innovative%0Aillumination%20estimators%20and%20damage%20restorer%20mechanisms%20that%20maintain%20image%0Aquality%20during%20enhancement.%20Moreover%2C%20RetinexMamba%20replaces%20the%20IG-MSA%0A%28Illumination-Guided%20Multi-Head%20Attention%29%20in%20Retinexformer%20with%20a%0AFused-Attention%20mechanism%2C%20improving%20the%20model%27s%20interpretability.%20Experimental%0Aevaluations%20on%20the%20LOL%20dataset%20show%20that%20RetinexMamba%20outperforms%20existing%20deep%0Alearning%20approaches%20based%20on%20Retinex%20theory%20in%20both%20quantitative%20and%0Aqualitative%20metrics%2C%20confirming%20its%20effectiveness%20and%20superiority%20in%20enhancing%0Alow-light%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03349v1&entry.124074799=Read"},
{"title": "Improved Forward-Forward Contrastive Learning", "author": "Gananath R", "abstract": "  The backpropagation algorithm, or backprop, is a widely utilized optimization\ntechnique in deep learning. While there's growing evidence suggesting that\nmodels trained with backprop can accurately explain neuronal data, no\nbackprop-like method has yet been discovered in the biological brain for\nlearning. Moreover, employing a naive implementation of backprop in the brain\nhas several drawbacks. In 2022, Geoffrey Hinton proposed a biologically\nplausible learning method known as the Forward-Forward (FF) algorithm. Shortly\nafter this paper, a modified version called FFCL was introduced. However, FFCL\nhad limitations, notably being a three-stage learning system where the final\nstage still relied on regular backpropagation. In our approach, we address\nthese drawbacks by eliminating the last two stages of FFCL and completely\nremoving regular backpropagation. Instead, we rely solely on local updates,\noffering a more biologically plausible alternative.\n", "link": "http://arxiv.org/abs/2405.03432v1", "date": "2024-05-06", "relevancy": 2.0511, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5334}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4989}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Forward-Forward%20Contrastive%20Learning&body=Title%3A%20Improved%20Forward-Forward%20Contrastive%20Learning%0AAuthor%3A%20Gananath%20R%0AAbstract%3A%20%20%20The%20backpropagation%20algorithm%2C%20or%20backprop%2C%20is%20a%20widely%20utilized%20optimization%0Atechnique%20in%20deep%20learning.%20While%20there%27s%20growing%20evidence%20suggesting%20that%0Amodels%20trained%20with%20backprop%20can%20accurately%20explain%20neuronal%20data%2C%20no%0Abackprop-like%20method%20has%20yet%20been%20discovered%20in%20the%20biological%20brain%20for%0Alearning.%20Moreover%2C%20employing%20a%20naive%20implementation%20of%20backprop%20in%20the%20brain%0Ahas%20several%20drawbacks.%20In%202022%2C%20Geoffrey%20Hinton%20proposed%20a%20biologically%0Aplausible%20learning%20method%20known%20as%20the%20Forward-Forward%20%28FF%29%20algorithm.%20Shortly%0Aafter%20this%20paper%2C%20a%20modified%20version%20called%20FFCL%20was%20introduced.%20However%2C%20FFCL%0Ahad%20limitations%2C%20notably%20being%20a%20three-stage%20learning%20system%20where%20the%20final%0Astage%20still%20relied%20on%20regular%20backpropagation.%20In%20our%20approach%2C%20we%20address%0Athese%20drawbacks%20by%20eliminating%20the%20last%20two%20stages%20of%20FFCL%20and%20completely%0Aremoving%20regular%20backpropagation.%20Instead%2C%20we%20rely%20solely%20on%20local%20updates%2C%0Aoffering%20a%20more%20biologically%20plausible%20alternative.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Forward-Forward%2520Contrastive%2520Learning%26entry.906535625%3DGananath%2520R%26entry.1292438233%3D%2520%2520The%2520backpropagation%2520algorithm%252C%2520or%2520backprop%252C%2520is%2520a%2520widely%2520utilized%2520optimization%250Atechnique%2520in%2520deep%2520learning.%2520While%2520there%2527s%2520growing%2520evidence%2520suggesting%2520that%250Amodels%2520trained%2520with%2520backprop%2520can%2520accurately%2520explain%2520neuronal%2520data%252C%2520no%250Abackprop-like%2520method%2520has%2520yet%2520been%2520discovered%2520in%2520the%2520biological%2520brain%2520for%250Alearning.%2520Moreover%252C%2520employing%2520a%2520naive%2520implementation%2520of%2520backprop%2520in%2520the%2520brain%250Ahas%2520several%2520drawbacks.%2520In%25202022%252C%2520Geoffrey%2520Hinton%2520proposed%2520a%2520biologically%250Aplausible%2520learning%2520method%2520known%2520as%2520the%2520Forward-Forward%2520%2528FF%2529%2520algorithm.%2520Shortly%250Aafter%2520this%2520paper%252C%2520a%2520modified%2520version%2520called%2520FFCL%2520was%2520introduced.%2520However%252C%2520FFCL%250Ahad%2520limitations%252C%2520notably%2520being%2520a%2520three-stage%2520learning%2520system%2520where%2520the%2520final%250Astage%2520still%2520relied%2520on%2520regular%2520backpropagation.%2520In%2520our%2520approach%252C%2520we%2520address%250Athese%2520drawbacks%2520by%2520eliminating%2520the%2520last%2520two%2520stages%2520of%2520FFCL%2520and%2520completely%250Aremoving%2520regular%2520backpropagation.%2520Instead%252C%2520we%2520rely%2520solely%2520on%2520local%2520updates%252C%250Aoffering%2520a%2520more%2520biologically%2520plausible%2520alternative.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Forward-Forward%20Contrastive%20Learning&entry.906535625=Gananath%20R&entry.1292438233=%20%20The%20backpropagation%20algorithm%2C%20or%20backprop%2C%20is%20a%20widely%20utilized%20optimization%0Atechnique%20in%20deep%20learning.%20While%20there%27s%20growing%20evidence%20suggesting%20that%0Amodels%20trained%20with%20backprop%20can%20accurately%20explain%20neuronal%20data%2C%20no%0Abackprop-like%20method%20has%20yet%20been%20discovered%20in%20the%20biological%20brain%20for%0Alearning.%20Moreover%2C%20employing%20a%20naive%20implementation%20of%20backprop%20in%20the%20brain%0Ahas%20several%20drawbacks.%20In%202022%2C%20Geoffrey%20Hinton%20proposed%20a%20biologically%0Aplausible%20learning%20method%20known%20as%20the%20Forward-Forward%20%28FF%29%20algorithm.%20Shortly%0Aafter%20this%20paper%2C%20a%20modified%20version%20called%20FFCL%20was%20introduced.%20However%2C%20FFCL%0Ahad%20limitations%2C%20notably%20being%20a%20three-stage%20learning%20system%20where%20the%20final%0Astage%20still%20relied%20on%20regular%20backpropagation.%20In%20our%20approach%2C%20we%20address%0Athese%20drawbacks%20by%20eliminating%20the%20last%20two%20stages%20of%20FFCL%20and%20completely%0Aremoving%20regular%20backpropagation.%20Instead%2C%20we%20rely%20solely%20on%20local%20updates%2C%0Aoffering%20a%20more%20biologically%20plausible%20alternative.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03432v1&entry.124074799=Read"},
{"title": "Telextiles: End-to-end Remote Transmission of Fabric Tactile Sensation", "author": "Takekazu Kitagishi and Yuichi Hiroi and Yuna Watanabe and Yuta Itoh and Jun Rekimoto", "abstract": "  The tactile sensation of textiles is critical in determining the comfort of\nclothing. For remote use, such as online shopping, users cannot physically\ntouch the textile of clothes, making it difficult to evaluate its tactile\nsensation. Tactile sensing and actuation devices are required to transmit the\ntactile sensation of textiles. The sensing device needs to recognize different\ngarments, even with hand-held sensors. In addition, the existing actuation\ndevice can only present a limited number of known patterns and cannot transmit\nunknown tactile sensations of textiles. To address these issues, we propose\nTelextiles, an interface that can remotely transmit tactile sensations of\ntextiles by creating a latent space that reflects the proximity of textiles\nthrough contrastive self-supervised learning. We confirm that textiles with\nsimilar tactile features are located close to each other in the latent space\nthrough a two-dimensional plot. We then compress the latent features for known\ntextile samples into the 1D distance and apply the 16 textile samples to the\nrollers in the order of the distance. The roller is rotated to select the\ntextile with the closest feature if an unknown textile is detected.\n", "link": "http://arxiv.org/abs/2405.03363v1", "date": "2024-05-06", "relevancy": 2.0482, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5496}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5378}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Telextiles%3A%20End-to-end%20Remote%20Transmission%20of%20Fabric%20Tactile%20Sensation&body=Title%3A%20Telextiles%3A%20End-to-end%20Remote%20Transmission%20of%20Fabric%20Tactile%20Sensation%0AAuthor%3A%20Takekazu%20Kitagishi%20and%20Yuichi%20Hiroi%20and%20Yuna%20Watanabe%20and%20Yuta%20Itoh%20and%20Jun%20Rekimoto%0AAbstract%3A%20%20%20The%20tactile%20sensation%20of%20textiles%20is%20critical%20in%20determining%20the%20comfort%20of%0Aclothing.%20For%20remote%20use%2C%20such%20as%20online%20shopping%2C%20users%20cannot%20physically%0Atouch%20the%20textile%20of%20clothes%2C%20making%20it%20difficult%20to%20evaluate%20its%20tactile%0Asensation.%20Tactile%20sensing%20and%20actuation%20devices%20are%20required%20to%20transmit%20the%0Atactile%20sensation%20of%20textiles.%20The%20sensing%20device%20needs%20to%20recognize%20different%0Agarments%2C%20even%20with%20hand-held%20sensors.%20In%20addition%2C%20the%20existing%20actuation%0Adevice%20can%20only%20present%20a%20limited%20number%20of%20known%20patterns%20and%20cannot%20transmit%0Aunknown%20tactile%20sensations%20of%20textiles.%20To%20address%20these%20issues%2C%20we%20propose%0ATelextiles%2C%20an%20interface%20that%20can%20remotely%20transmit%20tactile%20sensations%20of%0Atextiles%20by%20creating%20a%20latent%20space%20that%20reflects%20the%20proximity%20of%20textiles%0Athrough%20contrastive%20self-supervised%20learning.%20We%20confirm%20that%20textiles%20with%0Asimilar%20tactile%20features%20are%20located%20close%20to%20each%20other%20in%20the%20latent%20space%0Athrough%20a%20two-dimensional%20plot.%20We%20then%20compress%20the%20latent%20features%20for%20known%0Atextile%20samples%20into%20the%201D%20distance%20and%20apply%20the%2016%20textile%20samples%20to%20the%0Arollers%20in%20the%20order%20of%20the%20distance.%20The%20roller%20is%20rotated%20to%20select%20the%0Atextile%20with%20the%20closest%20feature%20if%20an%20unknown%20textile%20is%20detected.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03363v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTelextiles%253A%2520End-to-end%2520Remote%2520Transmission%2520of%2520Fabric%2520Tactile%2520Sensation%26entry.906535625%3DTakekazu%2520Kitagishi%2520and%2520Yuichi%2520Hiroi%2520and%2520Yuna%2520Watanabe%2520and%2520Yuta%2520Itoh%2520and%2520Jun%2520Rekimoto%26entry.1292438233%3D%2520%2520The%2520tactile%2520sensation%2520of%2520textiles%2520is%2520critical%2520in%2520determining%2520the%2520comfort%2520of%250Aclothing.%2520For%2520remote%2520use%252C%2520such%2520as%2520online%2520shopping%252C%2520users%2520cannot%2520physically%250Atouch%2520the%2520textile%2520of%2520clothes%252C%2520making%2520it%2520difficult%2520to%2520evaluate%2520its%2520tactile%250Asensation.%2520Tactile%2520sensing%2520and%2520actuation%2520devices%2520are%2520required%2520to%2520transmit%2520the%250Atactile%2520sensation%2520of%2520textiles.%2520The%2520sensing%2520device%2520needs%2520to%2520recognize%2520different%250Agarments%252C%2520even%2520with%2520hand-held%2520sensors.%2520In%2520addition%252C%2520the%2520existing%2520actuation%250Adevice%2520can%2520only%2520present%2520a%2520limited%2520number%2520of%2520known%2520patterns%2520and%2520cannot%2520transmit%250Aunknown%2520tactile%2520sensations%2520of%2520textiles.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250ATelextiles%252C%2520an%2520interface%2520that%2520can%2520remotely%2520transmit%2520tactile%2520sensations%2520of%250Atextiles%2520by%2520creating%2520a%2520latent%2520space%2520that%2520reflects%2520the%2520proximity%2520of%2520textiles%250Athrough%2520contrastive%2520self-supervised%2520learning.%2520We%2520confirm%2520that%2520textiles%2520with%250Asimilar%2520tactile%2520features%2520are%2520located%2520close%2520to%2520each%2520other%2520in%2520the%2520latent%2520space%250Athrough%2520a%2520two-dimensional%2520plot.%2520We%2520then%2520compress%2520the%2520latent%2520features%2520for%2520known%250Atextile%2520samples%2520into%2520the%25201D%2520distance%2520and%2520apply%2520the%252016%2520textile%2520samples%2520to%2520the%250Arollers%2520in%2520the%2520order%2520of%2520the%2520distance.%2520The%2520roller%2520is%2520rotated%2520to%2520select%2520the%250Atextile%2520with%2520the%2520closest%2520feature%2520if%2520an%2520unknown%2520textile%2520is%2520detected.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03363v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Telextiles%3A%20End-to-end%20Remote%20Transmission%20of%20Fabric%20Tactile%20Sensation&entry.906535625=Takekazu%20Kitagishi%20and%20Yuichi%20Hiroi%20and%20Yuna%20Watanabe%20and%20Yuta%20Itoh%20and%20Jun%20Rekimoto&entry.1292438233=%20%20The%20tactile%20sensation%20of%20textiles%20is%20critical%20in%20determining%20the%20comfort%20of%0Aclothing.%20For%20remote%20use%2C%20such%20as%20online%20shopping%2C%20users%20cannot%20physically%0Atouch%20the%20textile%20of%20clothes%2C%20making%20it%20difficult%20to%20evaluate%20its%20tactile%0Asensation.%20Tactile%20sensing%20and%20actuation%20devices%20are%20required%20to%20transmit%20the%0Atactile%20sensation%20of%20textiles.%20The%20sensing%20device%20needs%20to%20recognize%20different%0Agarments%2C%20even%20with%20hand-held%20sensors.%20In%20addition%2C%20the%20existing%20actuation%0Adevice%20can%20only%20present%20a%20limited%20number%20of%20known%20patterns%20and%20cannot%20transmit%0Aunknown%20tactile%20sensations%20of%20textiles.%20To%20address%20these%20issues%2C%20we%20propose%0ATelextiles%2C%20an%20interface%20that%20can%20remotely%20transmit%20tactile%20sensations%20of%0Atextiles%20by%20creating%20a%20latent%20space%20that%20reflects%20the%20proximity%20of%20textiles%0Athrough%20contrastive%20self-supervised%20learning.%20We%20confirm%20that%20textiles%20with%0Asimilar%20tactile%20features%20are%20located%20close%20to%20each%20other%20in%20the%20latent%20space%0Athrough%20a%20two-dimensional%20plot.%20We%20then%20compress%20the%20latent%20features%20for%20known%0Atextile%20samples%20into%20the%201D%20distance%20and%20apply%20the%2016%20textile%20samples%20to%20the%0Arollers%20in%20the%20order%20of%20the%20distance.%20The%20roller%20is%20rotated%20to%20select%20the%0Atextile%20with%20the%20closest%20feature%20if%20an%20unknown%20textile%20is%20detected.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03363v1&entry.124074799=Read"},
{"title": "Implantable Adaptive Cells: differentiable architecture search to\n  improve the performance of any trained U-shaped network", "author": "Emil Benedykciuk and Marcin Denkowski and Grzegorz W\u00f3jcik", "abstract": "  This paper introduces a novel approach to enhance the performance of\npre-trained neural networks in medical image segmentation using Neural\nArchitecture Search (NAS) methods, specifically Differentiable Architecture\nSearch (DARTS). We present the concept of Implantable Adaptive Cell (IAC),\nsmall but powerful modules identified through Partially-Connected DARTS,\ndesigned to be injected into the skip connections of an existing and already\ntrained U-shaped model. Our strategy allows for the seamless integration of the\nIAC into the pre-existing architecture, thereby enhancing its performance\nwithout necessitating a complete retraining from scratch. The empirical\nstudies, focusing on medical image segmentation tasks, demonstrate the efficacy\nof this method. The integration of specialized IAC cells into various\nconfigurations of the U-Net model increases segmentation accuracy by almost 2\\%\npoints on average for the validation dataset and over 3\\% points for the\ntraining dataset. The findings of this study not only offer a cost-effective\nalternative to the complete overhaul of complex models for performance upgrades\nbut also indicate the potential applicability of our method to other\narchitectures and problem domains.\n", "link": "http://arxiv.org/abs/2405.03420v1", "date": "2024-05-06", "relevancy": 2.0454, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5292}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5129}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implantable%20Adaptive%20Cells%3A%20differentiable%20architecture%20search%20to%0A%20%20improve%20the%20performance%20of%20any%20trained%20U-shaped%20network&body=Title%3A%20Implantable%20Adaptive%20Cells%3A%20differentiable%20architecture%20search%20to%0A%20%20improve%20the%20performance%20of%20any%20trained%20U-shaped%20network%0AAuthor%3A%20Emil%20Benedykciuk%20and%20Marcin%20Denkowski%20and%20Grzegorz%20W%C3%B3jcik%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20approach%20to%20enhance%20the%20performance%20of%0Apre-trained%20neural%20networks%20in%20medical%20image%20segmentation%20using%20Neural%0AArchitecture%20Search%20%28NAS%29%20methods%2C%20specifically%20Differentiable%20Architecture%0ASearch%20%28DARTS%29.%20We%20present%20the%20concept%20of%20Implantable%20Adaptive%20Cell%20%28IAC%29%2C%0Asmall%20but%20powerful%20modules%20identified%20through%20Partially-Connected%20DARTS%2C%0Adesigned%20to%20be%20injected%20into%20the%20skip%20connections%20of%20an%20existing%20and%20already%0Atrained%20U-shaped%20model.%20Our%20strategy%20allows%20for%20the%20seamless%20integration%20of%20the%0AIAC%20into%20the%20pre-existing%20architecture%2C%20thereby%20enhancing%20its%20performance%0Awithout%20necessitating%20a%20complete%20retraining%20from%20scratch.%20The%20empirical%0Astudies%2C%20focusing%20on%20medical%20image%20segmentation%20tasks%2C%20demonstrate%20the%20efficacy%0Aof%20this%20method.%20The%20integration%20of%20specialized%20IAC%20cells%20into%20various%0Aconfigurations%20of%20the%20U-Net%20model%20increases%20segmentation%20accuracy%20by%20almost%202%5C%25%0Apoints%20on%20average%20for%20the%20validation%20dataset%20and%20over%203%5C%25%20points%20for%20the%0Atraining%20dataset.%20The%20findings%20of%20this%20study%20not%20only%20offer%20a%20cost-effective%0Aalternative%20to%20the%20complete%20overhaul%20of%20complex%20models%20for%20performance%20upgrades%0Abut%20also%20indicate%20the%20potential%20applicability%20of%20our%20method%20to%20other%0Aarchitectures%20and%20problem%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplantable%2520Adaptive%2520Cells%253A%2520differentiable%2520architecture%2520search%2520to%250A%2520%2520improve%2520the%2520performance%2520of%2520any%2520trained%2520U-shaped%2520network%26entry.906535625%3DEmil%2520Benedykciuk%2520and%2520Marcin%2520Denkowski%2520and%2520Grzegorz%2520W%25C3%25B3jcik%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%2520enhance%2520the%2520performance%2520of%250Apre-trained%2520neural%2520networks%2520in%2520medical%2520image%2520segmentation%2520using%2520Neural%250AArchitecture%2520Search%2520%2528NAS%2529%2520methods%252C%2520specifically%2520Differentiable%2520Architecture%250ASearch%2520%2528DARTS%2529.%2520We%2520present%2520the%2520concept%2520of%2520Implantable%2520Adaptive%2520Cell%2520%2528IAC%2529%252C%250Asmall%2520but%2520powerful%2520modules%2520identified%2520through%2520Partially-Connected%2520DARTS%252C%250Adesigned%2520to%2520be%2520injected%2520into%2520the%2520skip%2520connections%2520of%2520an%2520existing%2520and%2520already%250Atrained%2520U-shaped%2520model.%2520Our%2520strategy%2520allows%2520for%2520the%2520seamless%2520integration%2520of%2520the%250AIAC%2520into%2520the%2520pre-existing%2520architecture%252C%2520thereby%2520enhancing%2520its%2520performance%250Awithout%2520necessitating%2520a%2520complete%2520retraining%2520from%2520scratch.%2520The%2520empirical%250Astudies%252C%2520focusing%2520on%2520medical%2520image%2520segmentation%2520tasks%252C%2520demonstrate%2520the%2520efficacy%250Aof%2520this%2520method.%2520The%2520integration%2520of%2520specialized%2520IAC%2520cells%2520into%2520various%250Aconfigurations%2520of%2520the%2520U-Net%2520model%2520increases%2520segmentation%2520accuracy%2520by%2520almost%25202%255C%2525%250Apoints%2520on%2520average%2520for%2520the%2520validation%2520dataset%2520and%2520over%25203%255C%2525%2520points%2520for%2520the%250Atraining%2520dataset.%2520The%2520findings%2520of%2520this%2520study%2520not%2520only%2520offer%2520a%2520cost-effective%250Aalternative%2520to%2520the%2520complete%2520overhaul%2520of%2520complex%2520models%2520for%2520performance%2520upgrades%250Abut%2520also%2520indicate%2520the%2520potential%2520applicability%2520of%2520our%2520method%2520to%2520other%250Aarchitectures%2520and%2520problem%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implantable%20Adaptive%20Cells%3A%20differentiable%20architecture%20search%20to%0A%20%20improve%20the%20performance%20of%20any%20trained%20U-shaped%20network&entry.906535625=Emil%20Benedykciuk%20and%20Marcin%20Denkowski%20and%20Grzegorz%20W%C3%B3jcik&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20approach%20to%20enhance%20the%20performance%20of%0Apre-trained%20neural%20networks%20in%20medical%20image%20segmentation%20using%20Neural%0AArchitecture%20Search%20%28NAS%29%20methods%2C%20specifically%20Differentiable%20Architecture%0ASearch%20%28DARTS%29.%20We%20present%20the%20concept%20of%20Implantable%20Adaptive%20Cell%20%28IAC%29%2C%0Asmall%20but%20powerful%20modules%20identified%20through%20Partially-Connected%20DARTS%2C%0Adesigned%20to%20be%20injected%20into%20the%20skip%20connections%20of%20an%20existing%20and%20already%0Atrained%20U-shaped%20model.%20Our%20strategy%20allows%20for%20the%20seamless%20integration%20of%20the%0AIAC%20into%20the%20pre-existing%20architecture%2C%20thereby%20enhancing%20its%20performance%0Awithout%20necessitating%20a%20complete%20retraining%20from%20scratch.%20The%20empirical%0Astudies%2C%20focusing%20on%20medical%20image%20segmentation%20tasks%2C%20demonstrate%20the%20efficacy%0Aof%20this%20method.%20The%20integration%20of%20specialized%20IAC%20cells%20into%20various%0Aconfigurations%20of%20the%20U-Net%20model%20increases%20segmentation%20accuracy%20by%20almost%202%5C%25%0Apoints%20on%20average%20for%20the%20validation%20dataset%20and%20over%203%5C%25%20points%20for%20the%0Atraining%20dataset.%20The%20findings%20of%20this%20study%20not%20only%20offer%20a%20cost-effective%0Aalternative%20to%20the%20complete%20overhaul%20of%20complex%20models%20for%20performance%20upgrades%0Abut%20also%20indicate%20the%20potential%20applicability%20of%20our%20method%20to%20other%0Aarchitectures%20and%20problem%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03420v1&entry.124074799=Read"},
{"title": "Collaborative Learning for Cyberattack Detection in Blockchain Networks", "author": "Tran Viet Khoa and Do Hai Son and Dinh Thai Hoang and Nguyen Linh Trung and Tran Thi Thuy Quynh and Diep N. Nguyen and Nguyen Viet Ha and Eryk Dutkiewicz", "abstract": "  This article aims to study intrusion attacks and then develop a novel\ncyberattack detection framework to detect cyberattacks at the network layer\n(e.g., Brute Password and Flooding of Transactions) of blockchain networks.\nSpecifically, we first design and implement a blockchain network in our\nlaboratory. This blockchain network will serve two purposes, i.e., to generate\nthe real traffic data (including both normal data and attack data) for our\nlearning models and to implement real-time experiments to evaluate the\nperformance of our proposed intrusion detection framework. To the best of our\nknowledge, this is the first dataset that is synthesized in a laboratory for\ncyberattacks in a blockchain network. We then propose a novel collaborative\nlearning model that allows efficient deployment in the blockchain network to\ndetect attacks. The main idea of the proposed learning model is to enable\nblockchain nodes to actively collect data, learn the knowledge from data using\nthe Deep Belief Network, and then share the knowledge learned from its data\nwith other blockchain nodes in the network. In this way, we can not only\nleverage the knowledge from all the nodes in the network but also do not need\nto gather all raw data for training at a centralized node like conventional\ncentralized learning solutions. Such a framework can also avoid the risk of\nexposing local data's privacy as well as excessive network overhead/congestion.\nBoth intensive simulations and real-time experiments clearly show that our\nproposed intrusion detection framework can achieve an accuracy of up to 98.6%\nin detecting attacks.\n", "link": "http://arxiv.org/abs/2203.11076v4", "date": "2024-05-06", "relevancy": 2.0451, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4151}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4091}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Learning%20for%20Cyberattack%20Detection%20in%20Blockchain%20Networks&body=Title%3A%20Collaborative%20Learning%20for%20Cyberattack%20Detection%20in%20Blockchain%20Networks%0AAuthor%3A%20Tran%20Viet%20Khoa%20and%20Do%20Hai%20Son%20and%20Dinh%20Thai%20Hoang%20and%20Nguyen%20Linh%20Trung%20and%20Tran%20Thi%20Thuy%20Quynh%20and%20Diep%20N.%20Nguyen%20and%20Nguyen%20Viet%20Ha%20and%20Eryk%20Dutkiewicz%0AAbstract%3A%20%20%20This%20article%20aims%20to%20study%20intrusion%20attacks%20and%20then%20develop%20a%20novel%0Acyberattack%20detection%20framework%20to%20detect%20cyberattacks%20at%20the%20network%20layer%0A%28e.g.%2C%20Brute%20Password%20and%20Flooding%20of%20Transactions%29%20of%20blockchain%20networks.%0ASpecifically%2C%20we%20first%20design%20and%20implement%20a%20blockchain%20network%20in%20our%0Alaboratory.%20This%20blockchain%20network%20will%20serve%20two%20purposes%2C%20i.e.%2C%20to%20generate%0Athe%20real%20traffic%20data%20%28including%20both%20normal%20data%20and%20attack%20data%29%20for%20our%0Alearning%20models%20and%20to%20implement%20real-time%20experiments%20to%20evaluate%20the%0Aperformance%20of%20our%20proposed%20intrusion%20detection%20framework.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20dataset%20that%20is%20synthesized%20in%20a%20laboratory%20for%0Acyberattacks%20in%20a%20blockchain%20network.%20We%20then%20propose%20a%20novel%20collaborative%0Alearning%20model%20that%20allows%20efficient%20deployment%20in%20the%20blockchain%20network%20to%0Adetect%20attacks.%20The%20main%20idea%20of%20the%20proposed%20learning%20model%20is%20to%20enable%0Ablockchain%20nodes%20to%20actively%20collect%20data%2C%20learn%20the%20knowledge%20from%20data%20using%0Athe%20Deep%20Belief%20Network%2C%20and%20then%20share%20the%20knowledge%20learned%20from%20its%20data%0Awith%20other%20blockchain%20nodes%20in%20the%20network.%20In%20this%20way%2C%20we%20can%20not%20only%0Aleverage%20the%20knowledge%20from%20all%20the%20nodes%20in%20the%20network%20but%20also%20do%20not%20need%0Ato%20gather%20all%20raw%20data%20for%20training%20at%20a%20centralized%20node%20like%20conventional%0Acentralized%20learning%20solutions.%20Such%20a%20framework%20can%20also%20avoid%20the%20risk%20of%0Aexposing%20local%20data%27s%20privacy%20as%20well%20as%20excessive%20network%20overhead/congestion.%0ABoth%20intensive%20simulations%20and%20real-time%20experiments%20clearly%20show%20that%20our%0Aproposed%20intrusion%20detection%20framework%20can%20achieve%20an%20accuracy%20of%20up%20to%2098.6%25%0Ain%20detecting%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.11076v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Learning%2520for%2520Cyberattack%2520Detection%2520in%2520Blockchain%2520Networks%26entry.906535625%3DTran%2520Viet%2520Khoa%2520and%2520Do%2520Hai%2520Son%2520and%2520Dinh%2520Thai%2520Hoang%2520and%2520Nguyen%2520Linh%2520Trung%2520and%2520Tran%2520Thi%2520Thuy%2520Quynh%2520and%2520Diep%2520N.%2520Nguyen%2520and%2520Nguyen%2520Viet%2520Ha%2520and%2520Eryk%2520Dutkiewicz%26entry.1292438233%3D%2520%2520This%2520article%2520aims%2520to%2520study%2520intrusion%2520attacks%2520and%2520then%2520develop%2520a%2520novel%250Acyberattack%2520detection%2520framework%2520to%2520detect%2520cyberattacks%2520at%2520the%2520network%2520layer%250A%2528e.g.%252C%2520Brute%2520Password%2520and%2520Flooding%2520of%2520Transactions%2529%2520of%2520blockchain%2520networks.%250ASpecifically%252C%2520we%2520first%2520design%2520and%2520implement%2520a%2520blockchain%2520network%2520in%2520our%250Alaboratory.%2520This%2520blockchain%2520network%2520will%2520serve%2520two%2520purposes%252C%2520i.e.%252C%2520to%2520generate%250Athe%2520real%2520traffic%2520data%2520%2528including%2520both%2520normal%2520data%2520and%2520attack%2520data%2529%2520for%2520our%250Alearning%2520models%2520and%2520to%2520implement%2520real-time%2520experiments%2520to%2520evaluate%2520the%250Aperformance%2520of%2520our%2520proposed%2520intrusion%2520detection%2520framework.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520dataset%2520that%2520is%2520synthesized%2520in%2520a%2520laboratory%2520for%250Acyberattacks%2520in%2520a%2520blockchain%2520network.%2520We%2520then%2520propose%2520a%2520novel%2520collaborative%250Alearning%2520model%2520that%2520allows%2520efficient%2520deployment%2520in%2520the%2520blockchain%2520network%2520to%250Adetect%2520attacks.%2520The%2520main%2520idea%2520of%2520the%2520proposed%2520learning%2520model%2520is%2520to%2520enable%250Ablockchain%2520nodes%2520to%2520actively%2520collect%2520data%252C%2520learn%2520the%2520knowledge%2520from%2520data%2520using%250Athe%2520Deep%2520Belief%2520Network%252C%2520and%2520then%2520share%2520the%2520knowledge%2520learned%2520from%2520its%2520data%250Awith%2520other%2520blockchain%2520nodes%2520in%2520the%2520network.%2520In%2520this%2520way%252C%2520we%2520can%2520not%2520only%250Aleverage%2520the%2520knowledge%2520from%2520all%2520the%2520nodes%2520in%2520the%2520network%2520but%2520also%2520do%2520not%2520need%250Ato%2520gather%2520all%2520raw%2520data%2520for%2520training%2520at%2520a%2520centralized%2520node%2520like%2520conventional%250Acentralized%2520learning%2520solutions.%2520Such%2520a%2520framework%2520can%2520also%2520avoid%2520the%2520risk%2520of%250Aexposing%2520local%2520data%2527s%2520privacy%2520as%2520well%2520as%2520excessive%2520network%2520overhead/congestion.%250ABoth%2520intensive%2520simulations%2520and%2520real-time%2520experiments%2520clearly%2520show%2520that%2520our%250Aproposed%2520intrusion%2520detection%2520framework%2520can%2520achieve%2520an%2520accuracy%2520of%2520up%2520to%252098.6%2525%250Ain%2520detecting%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2203.11076v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Learning%20for%20Cyberattack%20Detection%20in%20Blockchain%20Networks&entry.906535625=Tran%20Viet%20Khoa%20and%20Do%20Hai%20Son%20and%20Dinh%20Thai%20Hoang%20and%20Nguyen%20Linh%20Trung%20and%20Tran%20Thi%20Thuy%20Quynh%20and%20Diep%20N.%20Nguyen%20and%20Nguyen%20Viet%20Ha%20and%20Eryk%20Dutkiewicz&entry.1292438233=%20%20This%20article%20aims%20to%20study%20intrusion%20attacks%20and%20then%20develop%20a%20novel%0Acyberattack%20detection%20framework%20to%20detect%20cyberattacks%20at%20the%20network%20layer%0A%28e.g.%2C%20Brute%20Password%20and%20Flooding%20of%20Transactions%29%20of%20blockchain%20networks.%0ASpecifically%2C%20we%20first%20design%20and%20implement%20a%20blockchain%20network%20in%20our%0Alaboratory.%20This%20blockchain%20network%20will%20serve%20two%20purposes%2C%20i.e.%2C%20to%20generate%0Athe%20real%20traffic%20data%20%28including%20both%20normal%20data%20and%20attack%20data%29%20for%20our%0Alearning%20models%20and%20to%20implement%20real-time%20experiments%20to%20evaluate%20the%0Aperformance%20of%20our%20proposed%20intrusion%20detection%20framework.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20dataset%20that%20is%20synthesized%20in%20a%20laboratory%20for%0Acyberattacks%20in%20a%20blockchain%20network.%20We%20then%20propose%20a%20novel%20collaborative%0Alearning%20model%20that%20allows%20efficient%20deployment%20in%20the%20blockchain%20network%20to%0Adetect%20attacks.%20The%20main%20idea%20of%20the%20proposed%20learning%20model%20is%20to%20enable%0Ablockchain%20nodes%20to%20actively%20collect%20data%2C%20learn%20the%20knowledge%20from%20data%20using%0Athe%20Deep%20Belief%20Network%2C%20and%20then%20share%20the%20knowledge%20learned%20from%20its%20data%0Awith%20other%20blockchain%20nodes%20in%20the%20network.%20In%20this%20way%2C%20we%20can%20not%20only%0Aleverage%20the%20knowledge%20from%20all%20the%20nodes%20in%20the%20network%20but%20also%20do%20not%20need%0Ato%20gather%20all%20raw%20data%20for%20training%20at%20a%20centralized%20node%20like%20conventional%0Acentralized%20learning%20solutions.%20Such%20a%20framework%20can%20also%20avoid%20the%20risk%20of%0Aexposing%20local%20data%27s%20privacy%20as%20well%20as%20excessive%20network%20overhead/congestion.%0ABoth%20intensive%20simulations%20and%20real-time%20experiments%20clearly%20show%20that%20our%0Aproposed%20intrusion%20detection%20framework%20can%20achieve%20an%20accuracy%20of%20up%20to%2098.6%25%0Ain%20detecting%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.11076v4&entry.124074799=Read"},
{"title": "PopulAtion Parameter Averaging (PAPA)", "author": "Alexia Jolicoeur-Martineau and Emy Gervais and Kilian Fatras and Yan Zhang and Simon Lacoste-Julien", "abstract": "  Ensemble methods combine the predictions of multiple models to improve\nperformance, but they require significantly higher computation costs at\ninference time. To avoid these costs, multiple neural networks can be combined\ninto one by averaging their weights. However, this usually performs\nsignificantly worse than ensembling. Weight averaging is only beneficial when\ndifferent enough to benefit from combining them, but similar enough to average\nwell. Based on this idea, we propose PopulAtion Parameter Averaging (PAPA): a\nmethod that combines the generality of ensembling with the efficiency of weight\naveraging. PAPA leverages a population of diverse models (trained on different\ndata orders, augmentations, and regularizations) while slowly pushing the\nweights of the networks toward the population average of the weights. We also\npropose PAPA variants (PAPA-all, and PAPA-2) that average weights rarely rather\nthan continuously; all methods increase generalization, but PAPA tends to\nperform best. PAPA reduces the performance gap between averaging and\nensembling, increasing the average accuracy of a population of models by up to\n0.8% on CIFAR-10, 1.9% on CIFAR-100, and 1.6% on ImageNet when compared to\ntraining independent (non-averaged) models.\n", "link": "http://arxiv.org/abs/2304.03094v4", "date": "2024-05-06", "relevancy": 2.0448, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4183}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4113}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PopulAtion%20Parameter%20Averaging%20%28PAPA%29&body=Title%3A%20PopulAtion%20Parameter%20Averaging%20%28PAPA%29%0AAuthor%3A%20Alexia%20Jolicoeur-Martineau%20and%20Emy%20Gervais%20and%20Kilian%20Fatras%20and%20Yan%20Zhang%20and%20Simon%20Lacoste-Julien%0AAbstract%3A%20%20%20Ensemble%20methods%20combine%20the%20predictions%20of%20multiple%20models%20to%20improve%0Aperformance%2C%20but%20they%20require%20significantly%20higher%20computation%20costs%20at%0Ainference%20time.%20To%20avoid%20these%20costs%2C%20multiple%20neural%20networks%20can%20be%20combined%0Ainto%20one%20by%20averaging%20their%20weights.%20However%2C%20this%20usually%20performs%0Asignificantly%20worse%20than%20ensembling.%20Weight%20averaging%20is%20only%20beneficial%20when%0Adifferent%20enough%20to%20benefit%20from%20combining%20them%2C%20but%20similar%20enough%20to%20average%0Awell.%20Based%20on%20this%20idea%2C%20we%20propose%20PopulAtion%20Parameter%20Averaging%20%28PAPA%29%3A%20a%0Amethod%20that%20combines%20the%20generality%20of%20ensembling%20with%20the%20efficiency%20of%20weight%0Aaveraging.%20PAPA%20leverages%20a%20population%20of%20diverse%20models%20%28trained%20on%20different%0Adata%20orders%2C%20augmentations%2C%20and%20regularizations%29%20while%20slowly%20pushing%20the%0Aweights%20of%20the%20networks%20toward%20the%20population%20average%20of%20the%20weights.%20We%20also%0Apropose%20PAPA%20variants%20%28PAPA-all%2C%20and%20PAPA-2%29%20that%20average%20weights%20rarely%20rather%0Athan%20continuously%3B%20all%20methods%20increase%20generalization%2C%20but%20PAPA%20tends%20to%0Aperform%20best.%20PAPA%20reduces%20the%20performance%20gap%20between%20averaging%20and%0Aensembling%2C%20increasing%20the%20average%20accuracy%20of%20a%20population%20of%20models%20by%20up%20to%0A0.8%25%20on%20CIFAR-10%2C%201.9%25%20on%20CIFAR-100%2C%20and%201.6%25%20on%20ImageNet%20when%20compared%20to%0Atraining%20independent%20%28non-averaged%29%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.03094v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPopulAtion%2520Parameter%2520Averaging%2520%2528PAPA%2529%26entry.906535625%3DAlexia%2520Jolicoeur-Martineau%2520and%2520Emy%2520Gervais%2520and%2520Kilian%2520Fatras%2520and%2520Yan%2520Zhang%2520and%2520Simon%2520Lacoste-Julien%26entry.1292438233%3D%2520%2520Ensemble%2520methods%2520combine%2520the%2520predictions%2520of%2520multiple%2520models%2520to%2520improve%250Aperformance%252C%2520but%2520they%2520require%2520significantly%2520higher%2520computation%2520costs%2520at%250Ainference%2520time.%2520To%2520avoid%2520these%2520costs%252C%2520multiple%2520neural%2520networks%2520can%2520be%2520combined%250Ainto%2520one%2520by%2520averaging%2520their%2520weights.%2520However%252C%2520this%2520usually%2520performs%250Asignificantly%2520worse%2520than%2520ensembling.%2520Weight%2520averaging%2520is%2520only%2520beneficial%2520when%250Adifferent%2520enough%2520to%2520benefit%2520from%2520combining%2520them%252C%2520but%2520similar%2520enough%2520to%2520average%250Awell.%2520Based%2520on%2520this%2520idea%252C%2520we%2520propose%2520PopulAtion%2520Parameter%2520Averaging%2520%2528PAPA%2529%253A%2520a%250Amethod%2520that%2520combines%2520the%2520generality%2520of%2520ensembling%2520with%2520the%2520efficiency%2520of%2520weight%250Aaveraging.%2520PAPA%2520leverages%2520a%2520population%2520of%2520diverse%2520models%2520%2528trained%2520on%2520different%250Adata%2520orders%252C%2520augmentations%252C%2520and%2520regularizations%2529%2520while%2520slowly%2520pushing%2520the%250Aweights%2520of%2520the%2520networks%2520toward%2520the%2520population%2520average%2520of%2520the%2520weights.%2520We%2520also%250Apropose%2520PAPA%2520variants%2520%2528PAPA-all%252C%2520and%2520PAPA-2%2529%2520that%2520average%2520weights%2520rarely%2520rather%250Athan%2520continuously%253B%2520all%2520methods%2520increase%2520generalization%252C%2520but%2520PAPA%2520tends%2520to%250Aperform%2520best.%2520PAPA%2520reduces%2520the%2520performance%2520gap%2520between%2520averaging%2520and%250Aensembling%252C%2520increasing%2520the%2520average%2520accuracy%2520of%2520a%2520population%2520of%2520models%2520by%2520up%2520to%250A0.8%2525%2520on%2520CIFAR-10%252C%25201.9%2525%2520on%2520CIFAR-100%252C%2520and%25201.6%2525%2520on%2520ImageNet%2520when%2520compared%2520to%250Atraining%2520independent%2520%2528non-averaged%2529%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.03094v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PopulAtion%20Parameter%20Averaging%20%28PAPA%29&entry.906535625=Alexia%20Jolicoeur-Martineau%20and%20Emy%20Gervais%20and%20Kilian%20Fatras%20and%20Yan%20Zhang%20and%20Simon%20Lacoste-Julien&entry.1292438233=%20%20Ensemble%20methods%20combine%20the%20predictions%20of%20multiple%20models%20to%20improve%0Aperformance%2C%20but%20they%20require%20significantly%20higher%20computation%20costs%20at%0Ainference%20time.%20To%20avoid%20these%20costs%2C%20multiple%20neural%20networks%20can%20be%20combined%0Ainto%20one%20by%20averaging%20their%20weights.%20However%2C%20this%20usually%20performs%0Asignificantly%20worse%20than%20ensembling.%20Weight%20averaging%20is%20only%20beneficial%20when%0Adifferent%20enough%20to%20benefit%20from%20combining%20them%2C%20but%20similar%20enough%20to%20average%0Awell.%20Based%20on%20this%20idea%2C%20we%20propose%20PopulAtion%20Parameter%20Averaging%20%28PAPA%29%3A%20a%0Amethod%20that%20combines%20the%20generality%20of%20ensembling%20with%20the%20efficiency%20of%20weight%0Aaveraging.%20PAPA%20leverages%20a%20population%20of%20diverse%20models%20%28trained%20on%20different%0Adata%20orders%2C%20augmentations%2C%20and%20regularizations%29%20while%20slowly%20pushing%20the%0Aweights%20of%20the%20networks%20toward%20the%20population%20average%20of%20the%20weights.%20We%20also%0Apropose%20PAPA%20variants%20%28PAPA-all%2C%20and%20PAPA-2%29%20that%20average%20weights%20rarely%20rather%0Athan%20continuously%3B%20all%20methods%20increase%20generalization%2C%20but%20PAPA%20tends%20to%0Aperform%20best.%20PAPA%20reduces%20the%20performance%20gap%20between%20averaging%20and%0Aensembling%2C%20increasing%20the%20average%20accuracy%20of%20a%20population%20of%20models%20by%20up%20to%0A0.8%25%20on%20CIFAR-10%2C%201.9%25%20on%20CIFAR-100%2C%20and%201.6%25%20on%20ImageNet%20when%20compared%20to%0Atraining%20independent%20%28non-averaged%29%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.03094v4&entry.124074799=Read"},
{"title": "Enhancing Sign Language Teaching: A Mixed Reality Approach for Immersive\n  Learning and Multi-Dimensional Feedback", "author": "Hongli Wen and Yang Xu and Lin Li and Xudong Ru and Xingce Wang and Zhongke Wu", "abstract": "  Traditional sign language teaching methods face challenges such as limited\nfeedback and diverse learning scenarios. Although 2D resources lack real-time\nfeedback, classroom teaching is constrained by a scarcity of teacher. Methods\nbased on VR and AR have relatively primitive interaction feedback mechanisms.\nThis study proposes an innovative teaching model that uses real-time monocular\nvision and mixed reality technology. First, we introduce an improved\nhand-posture reconstruction method to achieve sign language semantic retention\nand real-time feedback. Second, a ternary system evaluation algorithm is\nproposed for a comprehensive assessment, maintaining good consistency with\nexperts in sign language. Furthermore, we use mixed reality technology to\nconstruct a scenario-based 3D sign language classroom and explore the user\nexperience of scenario teaching. Overall, this paper presents a novel teaching\nmethod that provides an immersive learning experience, advanced posture\nreconstruction, and precise feedback, achieving positive feedback on user\nexperience and learning effectiveness.\n", "link": "http://arxiv.org/abs/2404.10490v2", "date": "2024-05-06", "relevancy": 2.0396, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5307}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5267}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Sign%20Language%20Teaching%3A%20A%20Mixed%20Reality%20Approach%20for%20Immersive%0A%20%20Learning%20and%20Multi-Dimensional%20Feedback&body=Title%3A%20Enhancing%20Sign%20Language%20Teaching%3A%20A%20Mixed%20Reality%20Approach%20for%20Immersive%0A%20%20Learning%20and%20Multi-Dimensional%20Feedback%0AAuthor%3A%20Hongli%20Wen%20and%20Yang%20Xu%20and%20Lin%20Li%20and%20Xudong%20Ru%20and%20Xingce%20Wang%20and%20Zhongke%20Wu%0AAbstract%3A%20%20%20Traditional%20sign%20language%20teaching%20methods%20face%20challenges%20such%20as%20limited%0Afeedback%20and%20diverse%20learning%20scenarios.%20Although%202D%20resources%20lack%20real-time%0Afeedback%2C%20classroom%20teaching%20is%20constrained%20by%20a%20scarcity%20of%20teacher.%20Methods%0Abased%20on%20VR%20and%20AR%20have%20relatively%20primitive%20interaction%20feedback%20mechanisms.%0AThis%20study%20proposes%20an%20innovative%20teaching%20model%20that%20uses%20real-time%20monocular%0Avision%20and%20mixed%20reality%20technology.%20First%2C%20we%20introduce%20an%20improved%0Ahand-posture%20reconstruction%20method%20to%20achieve%20sign%20language%20semantic%20retention%0Aand%20real-time%20feedback.%20Second%2C%20a%20ternary%20system%20evaluation%20algorithm%20is%0Aproposed%20for%20a%20comprehensive%20assessment%2C%20maintaining%20good%20consistency%20with%0Aexperts%20in%20sign%20language.%20Furthermore%2C%20we%20use%20mixed%20reality%20technology%20to%0Aconstruct%20a%20scenario-based%203D%20sign%20language%20classroom%20and%20explore%20the%20user%0Aexperience%20of%20scenario%20teaching.%20Overall%2C%20this%20paper%20presents%20a%20novel%20teaching%0Amethod%20that%20provides%20an%20immersive%20learning%20experience%2C%20advanced%20posture%0Areconstruction%2C%20and%20precise%20feedback%2C%20achieving%20positive%20feedback%20on%20user%0Aexperience%20and%20learning%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10490v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Sign%2520Language%2520Teaching%253A%2520A%2520Mixed%2520Reality%2520Approach%2520for%2520Immersive%250A%2520%2520Learning%2520and%2520Multi-Dimensional%2520Feedback%26entry.906535625%3DHongli%2520Wen%2520and%2520Yang%2520Xu%2520and%2520Lin%2520Li%2520and%2520Xudong%2520Ru%2520and%2520Xingce%2520Wang%2520and%2520Zhongke%2520Wu%26entry.1292438233%3D%2520%2520Traditional%2520sign%2520language%2520teaching%2520methods%2520face%2520challenges%2520such%2520as%2520limited%250Afeedback%2520and%2520diverse%2520learning%2520scenarios.%2520Although%25202D%2520resources%2520lack%2520real-time%250Afeedback%252C%2520classroom%2520teaching%2520is%2520constrained%2520by%2520a%2520scarcity%2520of%2520teacher.%2520Methods%250Abased%2520on%2520VR%2520and%2520AR%2520have%2520relatively%2520primitive%2520interaction%2520feedback%2520mechanisms.%250AThis%2520study%2520proposes%2520an%2520innovative%2520teaching%2520model%2520that%2520uses%2520real-time%2520monocular%250Avision%2520and%2520mixed%2520reality%2520technology.%2520First%252C%2520we%2520introduce%2520an%2520improved%250Ahand-posture%2520reconstruction%2520method%2520to%2520achieve%2520sign%2520language%2520semantic%2520retention%250Aand%2520real-time%2520feedback.%2520Second%252C%2520a%2520ternary%2520system%2520evaluation%2520algorithm%2520is%250Aproposed%2520for%2520a%2520comprehensive%2520assessment%252C%2520maintaining%2520good%2520consistency%2520with%250Aexperts%2520in%2520sign%2520language.%2520Furthermore%252C%2520we%2520use%2520mixed%2520reality%2520technology%2520to%250Aconstruct%2520a%2520scenario-based%25203D%2520sign%2520language%2520classroom%2520and%2520explore%2520the%2520user%250Aexperience%2520of%2520scenario%2520teaching.%2520Overall%252C%2520this%2520paper%2520presents%2520a%2520novel%2520teaching%250Amethod%2520that%2520provides%2520an%2520immersive%2520learning%2520experience%252C%2520advanced%2520posture%250Areconstruction%252C%2520and%2520precise%2520feedback%252C%2520achieving%2520positive%2520feedback%2520on%2520user%250Aexperience%2520and%2520learning%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10490v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Sign%20Language%20Teaching%3A%20A%20Mixed%20Reality%20Approach%20for%20Immersive%0A%20%20Learning%20and%20Multi-Dimensional%20Feedback&entry.906535625=Hongli%20Wen%20and%20Yang%20Xu%20and%20Lin%20Li%20and%20Xudong%20Ru%20and%20Xingce%20Wang%20and%20Zhongke%20Wu&entry.1292438233=%20%20Traditional%20sign%20language%20teaching%20methods%20face%20challenges%20such%20as%20limited%0Afeedback%20and%20diverse%20learning%20scenarios.%20Although%202D%20resources%20lack%20real-time%0Afeedback%2C%20classroom%20teaching%20is%20constrained%20by%20a%20scarcity%20of%20teacher.%20Methods%0Abased%20on%20VR%20and%20AR%20have%20relatively%20primitive%20interaction%20feedback%20mechanisms.%0AThis%20study%20proposes%20an%20innovative%20teaching%20model%20that%20uses%20real-time%20monocular%0Avision%20and%20mixed%20reality%20technology.%20First%2C%20we%20introduce%20an%20improved%0Ahand-posture%20reconstruction%20method%20to%20achieve%20sign%20language%20semantic%20retention%0Aand%20real-time%20feedback.%20Second%2C%20a%20ternary%20system%20evaluation%20algorithm%20is%0Aproposed%20for%20a%20comprehensive%20assessment%2C%20maintaining%20good%20consistency%20with%0Aexperts%20in%20sign%20language.%20Furthermore%2C%20we%20use%20mixed%20reality%20technology%20to%0Aconstruct%20a%20scenario-based%203D%20sign%20language%20classroom%20and%20explore%20the%20user%0Aexperience%20of%20scenario%20teaching.%20Overall%2C%20this%20paper%20presents%20a%20novel%20teaching%0Amethod%20that%20provides%20an%20immersive%20learning%20experience%2C%20advanced%20posture%0Areconstruction%2C%20and%20precise%20feedback%2C%20achieving%20positive%20feedback%20on%20user%0Aexperience%20and%20learning%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10490v2&entry.124074799=Read"},
{"title": "UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and\n  AI-Generated Images", "author": "Yiting Qu and Xinyue Shen and Yixin Wu and Michael Backes and Savvas Zannettou and Yang Zhang", "abstract": "  Image safety classifiers play an important role in identifying and mitigating\nthe spread of unsafe images online (e.g., images including violence, hateful\nrhetoric, etc.). At the same time, with the advent of text-to-image models and\nincreasing concerns about the safety of AI models, developers are increasingly\nrelying on image safety classifiers to safeguard their models. Yet, the\nperformance of current image safety classifiers remains unknown for real-world\nand AI-generated images. To bridge this research gap, in this work, we propose\nUnsafeBench, a benchmarking framework that evaluates the effectiveness and\nrobustness of image safety classifiers. First, we curate a large dataset of 10K\nreal-world and AI-generated images that are annotated as safe or unsafe based\non a set of 11 unsafe categories of images (sexual, violent, hateful, etc.).\nThen, we evaluate the effectiveness and robustness of five popular image safety\nclassifiers, as well as three classifiers that are powered by general-purpose\nvisual language models. Our assessment indicates that existing image safety\nclassifiers are not comprehensive and effective enough in mitigating the\nmultifaceted problem of unsafe images. Also, we find that classifiers trained\nonly on real-world images tend to have degraded performance when applied to\nAI-generated images. Motivated by these findings, we design and implement a\ncomprehensive image moderation tool called PerspectiveVision, which effectively\nidentifies 11 categories of real-world and AI-generated unsafe images. The best\nPerspectiveVision model achieves an overall F1-Score of 0.810 on six evaluation\ndatasets, which is comparable with closed-source and expensive state-of-the-art\nmodels like GPT-4V. UnsafeBench and PerspectiveVision can aid the research\ncommunity in better understanding the landscape of image safety classification\nin the era of generative AI.\n", "link": "http://arxiv.org/abs/2405.03486v1", "date": "2024-05-06", "relevancy": 2.0395, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5214}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5088}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnsafeBench%3A%20Benchmarking%20Image%20Safety%20Classifiers%20on%20Real-World%20and%0A%20%20AI-Generated%20Images&body=Title%3A%20UnsafeBench%3A%20Benchmarking%20Image%20Safety%20Classifiers%20on%20Real-World%20and%0A%20%20AI-Generated%20Images%0AAuthor%3A%20Yiting%20Qu%20and%20Xinyue%20Shen%20and%20Yixin%20Wu%20and%20Michael%20Backes%20and%20Savvas%20Zannettou%20and%20Yang%20Zhang%0AAbstract%3A%20%20%20Image%20safety%20classifiers%20play%20an%20important%20role%20in%20identifying%20and%20mitigating%0Athe%20spread%20of%20unsafe%20images%20online%20%28e.g.%2C%20images%20including%20violence%2C%20hateful%0Arhetoric%2C%20etc.%29.%20At%20the%20same%20time%2C%20with%20the%20advent%20of%20text-to-image%20models%20and%0Aincreasing%20concerns%20about%20the%20safety%20of%20AI%20models%2C%20developers%20are%20increasingly%0Arelying%20on%20image%20safety%20classifiers%20to%20safeguard%20their%20models.%20Yet%2C%20the%0Aperformance%20of%20current%20image%20safety%20classifiers%20remains%20unknown%20for%20real-world%0Aand%20AI-generated%20images.%20To%20bridge%20this%20research%20gap%2C%20in%20this%20work%2C%20we%20propose%0AUnsafeBench%2C%20a%20benchmarking%20framework%20that%20evaluates%20the%20effectiveness%20and%0Arobustness%20of%20image%20safety%20classifiers.%20First%2C%20we%20curate%20a%20large%20dataset%20of%2010K%0Areal-world%20and%20AI-generated%20images%20that%20are%20annotated%20as%20safe%20or%20unsafe%20based%0Aon%20a%20set%20of%2011%20unsafe%20categories%20of%20images%20%28sexual%2C%20violent%2C%20hateful%2C%20etc.%29.%0AThen%2C%20we%20evaluate%20the%20effectiveness%20and%20robustness%20of%20five%20popular%20image%20safety%0Aclassifiers%2C%20as%20well%20as%20three%20classifiers%20that%20are%20powered%20by%20general-purpose%0Avisual%20language%20models.%20Our%20assessment%20indicates%20that%20existing%20image%20safety%0Aclassifiers%20are%20not%20comprehensive%20and%20effective%20enough%20in%20mitigating%20the%0Amultifaceted%20problem%20of%20unsafe%20images.%20Also%2C%20we%20find%20that%20classifiers%20trained%0Aonly%20on%20real-world%20images%20tend%20to%20have%20degraded%20performance%20when%20applied%20to%0AAI-generated%20images.%20Motivated%20by%20these%20findings%2C%20we%20design%20and%20implement%20a%0Acomprehensive%20image%20moderation%20tool%20called%20PerspectiveVision%2C%20which%20effectively%0Aidentifies%2011%20categories%20of%20real-world%20and%20AI-generated%20unsafe%20images.%20The%20best%0APerspectiveVision%20model%20achieves%20an%20overall%20F1-Score%20of%200.810%20on%20six%20evaluation%0Adatasets%2C%20which%20is%20comparable%20with%20closed-source%20and%20expensive%20state-of-the-art%0Amodels%20like%20GPT-4V.%20UnsafeBench%20and%20PerspectiveVision%20can%20aid%20the%20research%0Acommunity%20in%20better%20understanding%20the%20landscape%20of%20image%20safety%20classification%0Ain%20the%20era%20of%20generative%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsafeBench%253A%2520Benchmarking%2520Image%2520Safety%2520Classifiers%2520on%2520Real-World%2520and%250A%2520%2520AI-Generated%2520Images%26entry.906535625%3DYiting%2520Qu%2520and%2520Xinyue%2520Shen%2520and%2520Yixin%2520Wu%2520and%2520Michael%2520Backes%2520and%2520Savvas%2520Zannettou%2520and%2520Yang%2520Zhang%26entry.1292438233%3D%2520%2520Image%2520safety%2520classifiers%2520play%2520an%2520important%2520role%2520in%2520identifying%2520and%2520mitigating%250Athe%2520spread%2520of%2520unsafe%2520images%2520online%2520%2528e.g.%252C%2520images%2520including%2520violence%252C%2520hateful%250Arhetoric%252C%2520etc.%2529.%2520At%2520the%2520same%2520time%252C%2520with%2520the%2520advent%2520of%2520text-to-image%2520models%2520and%250Aincreasing%2520concerns%2520about%2520the%2520safety%2520of%2520AI%2520models%252C%2520developers%2520are%2520increasingly%250Arelying%2520on%2520image%2520safety%2520classifiers%2520to%2520safeguard%2520their%2520models.%2520Yet%252C%2520the%250Aperformance%2520of%2520current%2520image%2520safety%2520classifiers%2520remains%2520unknown%2520for%2520real-world%250Aand%2520AI-generated%2520images.%2520To%2520bridge%2520this%2520research%2520gap%252C%2520in%2520this%2520work%252C%2520we%2520propose%250AUnsafeBench%252C%2520a%2520benchmarking%2520framework%2520that%2520evaluates%2520the%2520effectiveness%2520and%250Arobustness%2520of%2520image%2520safety%2520classifiers.%2520First%252C%2520we%2520curate%2520a%2520large%2520dataset%2520of%252010K%250Areal-world%2520and%2520AI-generated%2520images%2520that%2520are%2520annotated%2520as%2520safe%2520or%2520unsafe%2520based%250Aon%2520a%2520set%2520of%252011%2520unsafe%2520categories%2520of%2520images%2520%2528sexual%252C%2520violent%252C%2520hateful%252C%2520etc.%2529.%250AThen%252C%2520we%2520evaluate%2520the%2520effectiveness%2520and%2520robustness%2520of%2520five%2520popular%2520image%2520safety%250Aclassifiers%252C%2520as%2520well%2520as%2520three%2520classifiers%2520that%2520are%2520powered%2520by%2520general-purpose%250Avisual%2520language%2520models.%2520Our%2520assessment%2520indicates%2520that%2520existing%2520image%2520safety%250Aclassifiers%2520are%2520not%2520comprehensive%2520and%2520effective%2520enough%2520in%2520mitigating%2520the%250Amultifaceted%2520problem%2520of%2520unsafe%2520images.%2520Also%252C%2520we%2520find%2520that%2520classifiers%2520trained%250Aonly%2520on%2520real-world%2520images%2520tend%2520to%2520have%2520degraded%2520performance%2520when%2520applied%2520to%250AAI-generated%2520images.%2520Motivated%2520by%2520these%2520findings%252C%2520we%2520design%2520and%2520implement%2520a%250Acomprehensive%2520image%2520moderation%2520tool%2520called%2520PerspectiveVision%252C%2520which%2520effectively%250Aidentifies%252011%2520categories%2520of%2520real-world%2520and%2520AI-generated%2520unsafe%2520images.%2520The%2520best%250APerspectiveVision%2520model%2520achieves%2520an%2520overall%2520F1-Score%2520of%25200.810%2520on%2520six%2520evaluation%250Adatasets%252C%2520which%2520is%2520comparable%2520with%2520closed-source%2520and%2520expensive%2520state-of-the-art%250Amodels%2520like%2520GPT-4V.%2520UnsafeBench%2520and%2520PerspectiveVision%2520can%2520aid%2520the%2520research%250Acommunity%2520in%2520better%2520understanding%2520the%2520landscape%2520of%2520image%2520safety%2520classification%250Ain%2520the%2520era%2520of%2520generative%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnsafeBench%3A%20Benchmarking%20Image%20Safety%20Classifiers%20on%20Real-World%20and%0A%20%20AI-Generated%20Images&entry.906535625=Yiting%20Qu%20and%20Xinyue%20Shen%20and%20Yixin%20Wu%20and%20Michael%20Backes%20and%20Savvas%20Zannettou%20and%20Yang%20Zhang&entry.1292438233=%20%20Image%20safety%20classifiers%20play%20an%20important%20role%20in%20identifying%20and%20mitigating%0Athe%20spread%20of%20unsafe%20images%20online%20%28e.g.%2C%20images%20including%20violence%2C%20hateful%0Arhetoric%2C%20etc.%29.%20At%20the%20same%20time%2C%20with%20the%20advent%20of%20text-to-image%20models%20and%0Aincreasing%20concerns%20about%20the%20safety%20of%20AI%20models%2C%20developers%20are%20increasingly%0Arelying%20on%20image%20safety%20classifiers%20to%20safeguard%20their%20models.%20Yet%2C%20the%0Aperformance%20of%20current%20image%20safety%20classifiers%20remains%20unknown%20for%20real-world%0Aand%20AI-generated%20images.%20To%20bridge%20this%20research%20gap%2C%20in%20this%20work%2C%20we%20propose%0AUnsafeBench%2C%20a%20benchmarking%20framework%20that%20evaluates%20the%20effectiveness%20and%0Arobustness%20of%20image%20safety%20classifiers.%20First%2C%20we%20curate%20a%20large%20dataset%20of%2010K%0Areal-world%20and%20AI-generated%20images%20that%20are%20annotated%20as%20safe%20or%20unsafe%20based%0Aon%20a%20set%20of%2011%20unsafe%20categories%20of%20images%20%28sexual%2C%20violent%2C%20hateful%2C%20etc.%29.%0AThen%2C%20we%20evaluate%20the%20effectiveness%20and%20robustness%20of%20five%20popular%20image%20safety%0Aclassifiers%2C%20as%20well%20as%20three%20classifiers%20that%20are%20powered%20by%20general-purpose%0Avisual%20language%20models.%20Our%20assessment%20indicates%20that%20existing%20image%20safety%0Aclassifiers%20are%20not%20comprehensive%20and%20effective%20enough%20in%20mitigating%20the%0Amultifaceted%20problem%20of%20unsafe%20images.%20Also%2C%20we%20find%20that%20classifiers%20trained%0Aonly%20on%20real-world%20images%20tend%20to%20have%20degraded%20performance%20when%20applied%20to%0AAI-generated%20images.%20Motivated%20by%20these%20findings%2C%20we%20design%20and%20implement%20a%0Acomprehensive%20image%20moderation%20tool%20called%20PerspectiveVision%2C%20which%20effectively%0Aidentifies%2011%20categories%20of%20real-world%20and%20AI-generated%20unsafe%20images.%20The%20best%0APerspectiveVision%20model%20achieves%20an%20overall%20F1-Score%20of%200.810%20on%20six%20evaluation%0Adatasets%2C%20which%20is%20comparable%20with%20closed-source%20and%20expensive%20state-of-the-art%0Amodels%20like%20GPT-4V.%20UnsafeBench%20and%20PerspectiveVision%20can%20aid%20the%20research%0Acommunity%20in%20better%20understanding%20the%20landscape%20of%20image%20safety%20classification%0Ain%20the%20era%20of%20generative%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03486v1&entry.124074799=Read"},
{"title": "Enabling High-Sparsity Foundational Llama Models with Efficient\n  Pretraining and Deployment", "author": "Abhinav Agarwalla and Abhay Gupta and Alexandre Marques and Shubhra Pandit and Michael Goin and Eldar Kurtic and Kevin Leong and Tuan Nguyen and Mahmoud Salem and Dan Alistarh and Sean Lie and Mark Kurtz", "abstract": "  Large language models (LLMs) have revolutionized Natural Language Processing\n(NLP), but their size creates computational bottlenecks. We introduce a novel\napproach to create accurate, sparse foundational versions of performant LLMs\nthat achieve full accuracy recovery for fine-tuning tasks at up to 70%\nsparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT\none-shot pruning method and sparse pretraining of those models on a subset of\nthe SlimPajama dataset mixed with a Python subset of The Stack dataset. We\nexhibit training acceleration due to sparsity on Cerebras CS-3 chips that\nclosely matches theoretical scaling. In addition, we establish inference\nacceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine\nand 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are\nrealized via sparsity alone, thus enabling further gains through additional use\nof quantization. Specifically, we show a total speedup on CPUs for\nsparse-quantized LLaMA models of up to 8.6x. We demonstrate these results\nacross diverse, challenging tasks, including chat, instruction following, code\ngeneration, arithmetic reasoning, and summarization to prove their generality.\nThis work paves the way for rapidly creating smaller and faster LLMs without\nsacrificing accuracy.\n", "link": "http://arxiv.org/abs/2405.03594v1", "date": "2024-05-06", "relevancy": 2.0326, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5147}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5097}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enabling%20High-Sparsity%20Foundational%20Llama%20Models%20with%20Efficient%0A%20%20Pretraining%20and%20Deployment&body=Title%3A%20Enabling%20High-Sparsity%20Foundational%20Llama%20Models%20with%20Efficient%0A%20%20Pretraining%20and%20Deployment%0AAuthor%3A%20Abhinav%20Agarwalla%20and%20Abhay%20Gupta%20and%20Alexandre%20Marques%20and%20Shubhra%20Pandit%20and%20Michael%20Goin%20and%20Eldar%20Kurtic%20and%20Kevin%20Leong%20and%20Tuan%20Nguyen%20and%20Mahmoud%20Salem%20and%20Dan%20Alistarh%20and%20Sean%20Lie%20and%20Mark%20Kurtz%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20Natural%20Language%20Processing%0A%28NLP%29%2C%20but%20their%20size%20creates%20computational%20bottlenecks.%20We%20introduce%20a%20novel%0Aapproach%20to%20create%20accurate%2C%20sparse%20foundational%20versions%20of%20performant%20LLMs%0Athat%20achieve%20full%20accuracy%20recovery%20for%20fine-tuning%20tasks%20at%20up%20to%2070%25%0Asparsity.%20We%20achieve%20this%20for%20the%20LLaMA-2%207B%20model%20by%20combining%20the%20SparseGPT%0Aone-shot%20pruning%20method%20and%20sparse%20pretraining%20of%20those%20models%20on%20a%20subset%20of%0Athe%20SlimPajama%20dataset%20mixed%20with%20a%20Python%20subset%20of%20The%20Stack%20dataset.%20We%0Aexhibit%20training%20acceleration%20due%20to%20sparsity%20on%20Cerebras%20CS-3%20chips%20that%0Aclosely%20matches%20theoretical%20scaling.%20In%20addition%2C%20we%20establish%20inference%0Aacceleration%20of%20up%20to%203x%20on%20CPUs%20by%20utilizing%20Neural%20Magic%27s%20DeepSparse%20engine%0Aand%201.7x%20on%20GPUs%20through%20Neural%20Magic%27s%20nm-vllm%20engine.%20The%20above%20gains%20are%0Arealized%20via%20sparsity%20alone%2C%20thus%20enabling%20further%20gains%20through%20additional%20use%0Aof%20quantization.%20Specifically%2C%20we%20show%20a%20total%20speedup%20on%20CPUs%20for%0Asparse-quantized%20LLaMA%20models%20of%20up%20to%208.6x.%20We%20demonstrate%20these%20results%0Aacross%20diverse%2C%20challenging%20tasks%2C%20including%20chat%2C%20instruction%20following%2C%20code%0Ageneration%2C%20arithmetic%20reasoning%2C%20and%20summarization%20to%20prove%20their%20generality.%0AThis%20work%20paves%20the%20way%20for%20rapidly%20creating%20smaller%20and%20faster%20LLMs%20without%0Asacrificing%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnabling%2520High-Sparsity%2520Foundational%2520Llama%2520Models%2520with%2520Efficient%250A%2520%2520Pretraining%2520and%2520Deployment%26entry.906535625%3DAbhinav%2520Agarwalla%2520and%2520Abhay%2520Gupta%2520and%2520Alexandre%2520Marques%2520and%2520Shubhra%2520Pandit%2520and%2520Michael%2520Goin%2520and%2520Eldar%2520Kurtic%2520and%2520Kevin%2520Leong%2520and%2520Tuan%2520Nguyen%2520and%2520Mahmoud%2520Salem%2520and%2520Dan%2520Alistarh%2520and%2520Sean%2520Lie%2520and%2520Mark%2520Kurtz%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520revolutionized%2520Natural%2520Language%2520Processing%250A%2528NLP%2529%252C%2520but%2520their%2520size%2520creates%2520computational%2520bottlenecks.%2520We%2520introduce%2520a%2520novel%250Aapproach%2520to%2520create%2520accurate%252C%2520sparse%2520foundational%2520versions%2520of%2520performant%2520LLMs%250Athat%2520achieve%2520full%2520accuracy%2520recovery%2520for%2520fine-tuning%2520tasks%2520at%2520up%2520to%252070%2525%250Asparsity.%2520We%2520achieve%2520this%2520for%2520the%2520LLaMA-2%25207B%2520model%2520by%2520combining%2520the%2520SparseGPT%250Aone-shot%2520pruning%2520method%2520and%2520sparse%2520pretraining%2520of%2520those%2520models%2520on%2520a%2520subset%2520of%250Athe%2520SlimPajama%2520dataset%2520mixed%2520with%2520a%2520Python%2520subset%2520of%2520The%2520Stack%2520dataset.%2520We%250Aexhibit%2520training%2520acceleration%2520due%2520to%2520sparsity%2520on%2520Cerebras%2520CS-3%2520chips%2520that%250Aclosely%2520matches%2520theoretical%2520scaling.%2520In%2520addition%252C%2520we%2520establish%2520inference%250Aacceleration%2520of%2520up%2520to%25203x%2520on%2520CPUs%2520by%2520utilizing%2520Neural%2520Magic%2527s%2520DeepSparse%2520engine%250Aand%25201.7x%2520on%2520GPUs%2520through%2520Neural%2520Magic%2527s%2520nm-vllm%2520engine.%2520The%2520above%2520gains%2520are%250Arealized%2520via%2520sparsity%2520alone%252C%2520thus%2520enabling%2520further%2520gains%2520through%2520additional%2520use%250Aof%2520quantization.%2520Specifically%252C%2520we%2520show%2520a%2520total%2520speedup%2520on%2520CPUs%2520for%250Asparse-quantized%2520LLaMA%2520models%2520of%2520up%2520to%25208.6x.%2520We%2520demonstrate%2520these%2520results%250Aacross%2520diverse%252C%2520challenging%2520tasks%252C%2520including%2520chat%252C%2520instruction%2520following%252C%2520code%250Ageneration%252C%2520arithmetic%2520reasoning%252C%2520and%2520summarization%2520to%2520prove%2520their%2520generality.%250AThis%2520work%2520paves%2520the%2520way%2520for%2520rapidly%2520creating%2520smaller%2520and%2520faster%2520LLMs%2520without%250Asacrificing%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20High-Sparsity%20Foundational%20Llama%20Models%20with%20Efficient%0A%20%20Pretraining%20and%20Deployment&entry.906535625=Abhinav%20Agarwalla%20and%20Abhay%20Gupta%20and%20Alexandre%20Marques%20and%20Shubhra%20Pandit%20and%20Michael%20Goin%20and%20Eldar%20Kurtic%20and%20Kevin%20Leong%20and%20Tuan%20Nguyen%20and%20Mahmoud%20Salem%20and%20Dan%20Alistarh%20and%20Sean%20Lie%20and%20Mark%20Kurtz&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20Natural%20Language%20Processing%0A%28NLP%29%2C%20but%20their%20size%20creates%20computational%20bottlenecks.%20We%20introduce%20a%20novel%0Aapproach%20to%20create%20accurate%2C%20sparse%20foundational%20versions%20of%20performant%20LLMs%0Athat%20achieve%20full%20accuracy%20recovery%20for%20fine-tuning%20tasks%20at%20up%20to%2070%25%0Asparsity.%20We%20achieve%20this%20for%20the%20LLaMA-2%207B%20model%20by%20combining%20the%20SparseGPT%0Aone-shot%20pruning%20method%20and%20sparse%20pretraining%20of%20those%20models%20on%20a%20subset%20of%0Athe%20SlimPajama%20dataset%20mixed%20with%20a%20Python%20subset%20of%20The%20Stack%20dataset.%20We%0Aexhibit%20training%20acceleration%20due%20to%20sparsity%20on%20Cerebras%20CS-3%20chips%20that%0Aclosely%20matches%20theoretical%20scaling.%20In%20addition%2C%20we%20establish%20inference%0Aacceleration%20of%20up%20to%203x%20on%20CPUs%20by%20utilizing%20Neural%20Magic%27s%20DeepSparse%20engine%0Aand%201.7x%20on%20GPUs%20through%20Neural%20Magic%27s%20nm-vllm%20engine.%20The%20above%20gains%20are%0Arealized%20via%20sparsity%20alone%2C%20thus%20enabling%20further%20gains%20through%20additional%20use%0Aof%20quantization.%20Specifically%2C%20we%20show%20a%20total%20speedup%20on%20CPUs%20for%0Asparse-quantized%20LLaMA%20models%20of%20up%20to%208.6x.%20We%20demonstrate%20these%20results%0Aacross%20diverse%2C%20challenging%20tasks%2C%20including%20chat%2C%20instruction%20following%2C%20code%0Ageneration%2C%20arithmetic%20reasoning%2C%20and%20summarization%20to%20prove%20their%20generality.%0AThis%20work%20paves%20the%20way%20for%20rapidly%20creating%20smaller%20and%20faster%20LLMs%20without%0Asacrificing%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03594v1&entry.124074799=Read"},
{"title": "Learning Robust Classifiers with Self-Guided Spurious Correlation\n  Mitigation", "author": "Guangtao Zheng and Wenqian Ye and Aidong Zhang", "abstract": "  Deep neural classifiers tend to rely on spurious correlations between\nspurious attributes of inputs and targets to make predictions, which could\njeopardize their generalization capability. Training classifiers robust to\nspurious correlations typically relies on annotations of spurious correlations\nin data, which are often expensive to get. In this paper, we tackle an\nannotation-free setting and propose a self-guided spurious correlation\nmitigation framework. Our framework automatically constructs fine-grained\ntraining labels tailored for a classifier obtained with empirical risk\nminimization to improve its robustness against spurious correlations. The\nfine-grained training labels are formulated with different prediction behaviors\nof the classifier identified in a novel spuriousness embedding space. We\nconstruct the space with automatically detected conceptual attributes and a\nnovel spuriousness metric which measures how likely a class-attribute\ncorrelation is exploited for predictions. We demonstrate that training the\nclassifier to distinguish different prediction behaviors reduces its reliance\non spurious correlations without knowing them a priori and outperforms prior\nmethods on five real-world datasets.\n", "link": "http://arxiv.org/abs/2405.03649v1", "date": "2024-05-06", "relevancy": 2.0259, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5171}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5066}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Robust%20Classifiers%20with%20Self-Guided%20Spurious%20Correlation%0A%20%20Mitigation&body=Title%3A%20Learning%20Robust%20Classifiers%20with%20Self-Guided%20Spurious%20Correlation%0A%20%20Mitigation%0AAuthor%3A%20Guangtao%20Zheng%20and%20Wenqian%20Ye%20and%20Aidong%20Zhang%0AAbstract%3A%20%20%20Deep%20neural%20classifiers%20tend%20to%20rely%20on%20spurious%20correlations%20between%0Aspurious%20attributes%20of%20inputs%20and%20targets%20to%20make%20predictions%2C%20which%20could%0Ajeopardize%20their%20generalization%20capability.%20Training%20classifiers%20robust%20to%0Aspurious%20correlations%20typically%20relies%20on%20annotations%20of%20spurious%20correlations%0Ain%20data%2C%20which%20are%20often%20expensive%20to%20get.%20In%20this%20paper%2C%20we%20tackle%20an%0Aannotation-free%20setting%20and%20propose%20a%20self-guided%20spurious%20correlation%0Amitigation%20framework.%20Our%20framework%20automatically%20constructs%20fine-grained%0Atraining%20labels%20tailored%20for%20a%20classifier%20obtained%20with%20empirical%20risk%0Aminimization%20to%20improve%20its%20robustness%20against%20spurious%20correlations.%20The%0Afine-grained%20training%20labels%20are%20formulated%20with%20different%20prediction%20behaviors%0Aof%20the%20classifier%20identified%20in%20a%20novel%20spuriousness%20embedding%20space.%20We%0Aconstruct%20the%20space%20with%20automatically%20detected%20conceptual%20attributes%20and%20a%0Anovel%20spuriousness%20metric%20which%20measures%20how%20likely%20a%20class-attribute%0Acorrelation%20is%20exploited%20for%20predictions.%20We%20demonstrate%20that%20training%20the%0Aclassifier%20to%20distinguish%20different%20prediction%20behaviors%20reduces%20its%20reliance%0Aon%20spurious%20correlations%20without%20knowing%20them%20a%20priori%20and%20outperforms%20prior%0Amethods%20on%20five%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Robust%2520Classifiers%2520with%2520Self-Guided%2520Spurious%2520Correlation%250A%2520%2520Mitigation%26entry.906535625%3DGuangtao%2520Zheng%2520and%2520Wenqian%2520Ye%2520and%2520Aidong%2520Zhang%26entry.1292438233%3D%2520%2520Deep%2520neural%2520classifiers%2520tend%2520to%2520rely%2520on%2520spurious%2520correlations%2520between%250Aspurious%2520attributes%2520of%2520inputs%2520and%2520targets%2520to%2520make%2520predictions%252C%2520which%2520could%250Ajeopardize%2520their%2520generalization%2520capability.%2520Training%2520classifiers%2520robust%2520to%250Aspurious%2520correlations%2520typically%2520relies%2520on%2520annotations%2520of%2520spurious%2520correlations%250Ain%2520data%252C%2520which%2520are%2520often%2520expensive%2520to%2520get.%2520In%2520this%2520paper%252C%2520we%2520tackle%2520an%250Aannotation-free%2520setting%2520and%2520propose%2520a%2520self-guided%2520spurious%2520correlation%250Amitigation%2520framework.%2520Our%2520framework%2520automatically%2520constructs%2520fine-grained%250Atraining%2520labels%2520tailored%2520for%2520a%2520classifier%2520obtained%2520with%2520empirical%2520risk%250Aminimization%2520to%2520improve%2520its%2520robustness%2520against%2520spurious%2520correlations.%2520The%250Afine-grained%2520training%2520labels%2520are%2520formulated%2520with%2520different%2520prediction%2520behaviors%250Aof%2520the%2520classifier%2520identified%2520in%2520a%2520novel%2520spuriousness%2520embedding%2520space.%2520We%250Aconstruct%2520the%2520space%2520with%2520automatically%2520detected%2520conceptual%2520attributes%2520and%2520a%250Anovel%2520spuriousness%2520metric%2520which%2520measures%2520how%2520likely%2520a%2520class-attribute%250Acorrelation%2520is%2520exploited%2520for%2520predictions.%2520We%2520demonstrate%2520that%2520training%2520the%250Aclassifier%2520to%2520distinguish%2520different%2520prediction%2520behaviors%2520reduces%2520its%2520reliance%250Aon%2520spurious%2520correlations%2520without%2520knowing%2520them%2520a%2520priori%2520and%2520outperforms%2520prior%250Amethods%2520on%2520five%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Robust%20Classifiers%20with%20Self-Guided%20Spurious%20Correlation%0A%20%20Mitigation&entry.906535625=Guangtao%20Zheng%20and%20Wenqian%20Ye%20and%20Aidong%20Zhang&entry.1292438233=%20%20Deep%20neural%20classifiers%20tend%20to%20rely%20on%20spurious%20correlations%20between%0Aspurious%20attributes%20of%20inputs%20and%20targets%20to%20make%20predictions%2C%20which%20could%0Ajeopardize%20their%20generalization%20capability.%20Training%20classifiers%20robust%20to%0Aspurious%20correlations%20typically%20relies%20on%20annotations%20of%20spurious%20correlations%0Ain%20data%2C%20which%20are%20often%20expensive%20to%20get.%20In%20this%20paper%2C%20we%20tackle%20an%0Aannotation-free%20setting%20and%20propose%20a%20self-guided%20spurious%20correlation%0Amitigation%20framework.%20Our%20framework%20automatically%20constructs%20fine-grained%0Atraining%20labels%20tailored%20for%20a%20classifier%20obtained%20with%20empirical%20risk%0Aminimization%20to%20improve%20its%20robustness%20against%20spurious%20correlations.%20The%0Afine-grained%20training%20labels%20are%20formulated%20with%20different%20prediction%20behaviors%0Aof%20the%20classifier%20identified%20in%20a%20novel%20spuriousness%20embedding%20space.%20We%0Aconstruct%20the%20space%20with%20automatically%20detected%20conceptual%20attributes%20and%20a%0Anovel%20spuriousness%20metric%20which%20measures%20how%20likely%20a%20class-attribute%0Acorrelation%20is%20exploited%20for%20predictions.%20We%20demonstrate%20that%20training%20the%0Aclassifier%20to%20distinguish%20different%20prediction%20behaviors%20reduces%20its%20reliance%0Aon%20spurious%20correlations%20without%20knowing%20them%20a%20priori%20and%20outperforms%20prior%0Amethods%20on%20five%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03649v1&entry.124074799=Read"},
{"title": "Graph Transformers without Positional Encodings", "author": "Ayush Garg", "abstract": "  Recently, Transformers for graph representation learning have become\nincreasingly popular, achieving state-of-the-art performance on a wide-variety\nof graph datasets, either alone or in combination with message-passing graph\nneural networks (MP-GNNs). Infusing graph inductive-biases in the innately\nstructure-agnostic transformer architecture in the form of structural or\npositional encodings (PEs) is key to achieving these impressive results.\nHowever, designing such encodings is tricky and disparate attempts have been\nmade to engineer such encodings including Laplacian eigenvectors, relative\nrandom-walk probabilities (RRWP), spatial encodings, centrality encodings, edge\nencodings etc. In this work, we argue that such encodings may not be required\nat all, provided the attention mechanism itself incorporates information about\nthe graph structure. We introduce Eigenformer, a Graph Transformer employing a\nnovel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of\nthe graph, and empirically show that it achieves performance competetive with\nSOTA Graph Transformers on a number of standard GNN benchmarks. Additionally,\nwe theoretically prove that Eigenformer can express various graph structural\nconnectivity matrices, which is particularly essential when learning over\nsmaller graphs.\n", "link": "http://arxiv.org/abs/2401.17791v3", "date": "2024-05-06", "relevancy": 2.0132, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.543}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5187}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Transformers%20without%20Positional%20Encodings&body=Title%3A%20Graph%20Transformers%20without%20Positional%20Encodings%0AAuthor%3A%20Ayush%20Garg%0AAbstract%3A%20%20%20Recently%2C%20Transformers%20for%20graph%20representation%20learning%20have%20become%0Aincreasingly%20popular%2C%20achieving%20state-of-the-art%20performance%20on%20a%20wide-variety%0Aof%20graph%20datasets%2C%20either%20alone%20or%20in%20combination%20with%20message-passing%20graph%0Aneural%20networks%20%28MP-GNNs%29.%20Infusing%20graph%20inductive-biases%20in%20the%20innately%0Astructure-agnostic%20transformer%20architecture%20in%20the%20form%20of%20structural%20or%0Apositional%20encodings%20%28PEs%29%20is%20key%20to%20achieving%20these%20impressive%20results.%0AHowever%2C%20designing%20such%20encodings%20is%20tricky%20and%20disparate%20attempts%20have%20been%0Amade%20to%20engineer%20such%20encodings%20including%20Laplacian%20eigenvectors%2C%20relative%0Arandom-walk%20probabilities%20%28RRWP%29%2C%20spatial%20encodings%2C%20centrality%20encodings%2C%20edge%0Aencodings%20etc.%20In%20this%20work%2C%20we%20argue%20that%20such%20encodings%20may%20not%20be%20required%0Aat%20all%2C%20provided%20the%20attention%20mechanism%20itself%20incorporates%20information%20about%0Athe%20graph%20structure.%20We%20introduce%20Eigenformer%2C%20a%20Graph%20Transformer%20employing%20a%0Anovel%20spectrum-aware%20attention%20mechanism%20cognizant%20of%20the%20Laplacian%20spectrum%20of%0Athe%20graph%2C%20and%20empirically%20show%20that%20it%20achieves%20performance%20competetive%20with%0ASOTA%20Graph%20Transformers%20on%20a%20number%20of%20standard%20GNN%20benchmarks.%20Additionally%2C%0Awe%20theoretically%20prove%20that%20Eigenformer%20can%20express%20various%20graph%20structural%0Aconnectivity%20matrices%2C%20which%20is%20particularly%20essential%20when%20learning%20over%0Asmaller%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.17791v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Transformers%2520without%2520Positional%2520Encodings%26entry.906535625%3DAyush%2520Garg%26entry.1292438233%3D%2520%2520Recently%252C%2520Transformers%2520for%2520graph%2520representation%2520learning%2520have%2520become%250Aincreasingly%2520popular%252C%2520achieving%2520state-of-the-art%2520performance%2520on%2520a%2520wide-variety%250Aof%2520graph%2520datasets%252C%2520either%2520alone%2520or%2520in%2520combination%2520with%2520message-passing%2520graph%250Aneural%2520networks%2520%2528MP-GNNs%2529.%2520Infusing%2520graph%2520inductive-biases%2520in%2520the%2520innately%250Astructure-agnostic%2520transformer%2520architecture%2520in%2520the%2520form%2520of%2520structural%2520or%250Apositional%2520encodings%2520%2528PEs%2529%2520is%2520key%2520to%2520achieving%2520these%2520impressive%2520results.%250AHowever%252C%2520designing%2520such%2520encodings%2520is%2520tricky%2520and%2520disparate%2520attempts%2520have%2520been%250Amade%2520to%2520engineer%2520such%2520encodings%2520including%2520Laplacian%2520eigenvectors%252C%2520relative%250Arandom-walk%2520probabilities%2520%2528RRWP%2529%252C%2520spatial%2520encodings%252C%2520centrality%2520encodings%252C%2520edge%250Aencodings%2520etc.%2520In%2520this%2520work%252C%2520we%2520argue%2520that%2520such%2520encodings%2520may%2520not%2520be%2520required%250Aat%2520all%252C%2520provided%2520the%2520attention%2520mechanism%2520itself%2520incorporates%2520information%2520about%250Athe%2520graph%2520structure.%2520We%2520introduce%2520Eigenformer%252C%2520a%2520Graph%2520Transformer%2520employing%2520a%250Anovel%2520spectrum-aware%2520attention%2520mechanism%2520cognizant%2520of%2520the%2520Laplacian%2520spectrum%2520of%250Athe%2520graph%252C%2520and%2520empirically%2520show%2520that%2520it%2520achieves%2520performance%2520competetive%2520with%250ASOTA%2520Graph%2520Transformers%2520on%2520a%2520number%2520of%2520standard%2520GNN%2520benchmarks.%2520Additionally%252C%250Awe%2520theoretically%2520prove%2520that%2520Eigenformer%2520can%2520express%2520various%2520graph%2520structural%250Aconnectivity%2520matrices%252C%2520which%2520is%2520particularly%2520essential%2520when%2520learning%2520over%250Asmaller%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.17791v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Transformers%20without%20Positional%20Encodings&entry.906535625=Ayush%20Garg&entry.1292438233=%20%20Recently%2C%20Transformers%20for%20graph%20representation%20learning%20have%20become%0Aincreasingly%20popular%2C%20achieving%20state-of-the-art%20performance%20on%20a%20wide-variety%0Aof%20graph%20datasets%2C%20either%20alone%20or%20in%20combination%20with%20message-passing%20graph%0Aneural%20networks%20%28MP-GNNs%29.%20Infusing%20graph%20inductive-biases%20in%20the%20innately%0Astructure-agnostic%20transformer%20architecture%20in%20the%20form%20of%20structural%20or%0Apositional%20encodings%20%28PEs%29%20is%20key%20to%20achieving%20these%20impressive%20results.%0AHowever%2C%20designing%20such%20encodings%20is%20tricky%20and%20disparate%20attempts%20have%20been%0Amade%20to%20engineer%20such%20encodings%20including%20Laplacian%20eigenvectors%2C%20relative%0Arandom-walk%20probabilities%20%28RRWP%29%2C%20spatial%20encodings%2C%20centrality%20encodings%2C%20edge%0Aencodings%20etc.%20In%20this%20work%2C%20we%20argue%20that%20such%20encodings%20may%20not%20be%20required%0Aat%20all%2C%20provided%20the%20attention%20mechanism%20itself%20incorporates%20information%20about%0Athe%20graph%20structure.%20We%20introduce%20Eigenformer%2C%20a%20Graph%20Transformer%20employing%20a%0Anovel%20spectrum-aware%20attention%20mechanism%20cognizant%20of%20the%20Laplacian%20spectrum%20of%0Athe%20graph%2C%20and%20empirically%20show%20that%20it%20achieves%20performance%20competetive%20with%0ASOTA%20Graph%20Transformers%20on%20a%20number%20of%20standard%20GNN%20benchmarks.%20Additionally%2C%0Awe%20theoretically%20prove%20that%20Eigenformer%20can%20express%20various%20graph%20structural%0Aconnectivity%20matrices%2C%20which%20is%20particularly%20essential%20when%20learning%20over%0Asmaller%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.17791v3&entry.124074799=Read"},
{"title": "Reverse Forward Curriculum Learning for Extreme Sample and Demonstration\n  Efficiency in Reinforcement Learning", "author": "Stone Tao and Arth Shukla and Tse-kai Chan and Hao Su", "abstract": "  Reinforcement learning (RL) presents a promising framework to learn policies\nthrough environment interaction, but often requires an infeasible amount of\ninteraction data to solve complex tasks from sparse rewards. One direction\nincludes augmenting RL with offline data demonstrating desired tasks, but past\nwork often require a lot of high-quality demonstration data that is difficult\nto obtain, especially for domains such as robotics. Our approach consists of a\nreverse curriculum followed by a forward curriculum. Unique to our approach\ncompared to past work is the ability to efficiently leverage more than one\ndemonstration via a per-demonstration reverse curriculum generated via state\nresets. The result of our reverse curriculum is an initial policy that performs\nwell on a narrow initial state distribution and helps overcome difficult\nexploration problems. A forward curriculum is then used to accelerate the\ntraining of the initial policy to perform well on the full initial state\ndistribution of the task and improve demonstration and sample efficiency. We\nshow how the combination of a reverse curriculum and forward curriculum in our\nmethod, RFCL, enables significant improvements in demonstration and sample\nefficiency compared against various state-of-the-art\nlearning-from-demonstration baselines, even solving previously unsolvable tasks\nthat require high precision and control.\n", "link": "http://arxiv.org/abs/2405.03379v1", "date": "2024-05-06", "relevancy": 2.0126, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5375}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5024}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reverse%20Forward%20Curriculum%20Learning%20for%20Extreme%20Sample%20and%20Demonstration%0A%20%20Efficiency%20in%20Reinforcement%20Learning&body=Title%3A%20Reverse%20Forward%20Curriculum%20Learning%20for%20Extreme%20Sample%20and%20Demonstration%0A%20%20Efficiency%20in%20Reinforcement%20Learning%0AAuthor%3A%20Stone%20Tao%20and%20Arth%20Shukla%20and%20Tse-kai%20Chan%20and%20Hao%20Su%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20presents%20a%20promising%20framework%20to%20learn%20policies%0Athrough%20environment%20interaction%2C%20but%20often%20requires%20an%20infeasible%20amount%20of%0Ainteraction%20data%20to%20solve%20complex%20tasks%20from%20sparse%20rewards.%20One%20direction%0Aincludes%20augmenting%20RL%20with%20offline%20data%20demonstrating%20desired%20tasks%2C%20but%20past%0Awork%20often%20require%20a%20lot%20of%20high-quality%20demonstration%20data%20that%20is%20difficult%0Ato%20obtain%2C%20especially%20for%20domains%20such%20as%20robotics.%20Our%20approach%20consists%20of%20a%0Areverse%20curriculum%20followed%20by%20a%20forward%20curriculum.%20Unique%20to%20our%20approach%0Acompared%20to%20past%20work%20is%20the%20ability%20to%20efficiently%20leverage%20more%20than%20one%0Ademonstration%20via%20a%20per-demonstration%20reverse%20curriculum%20generated%20via%20state%0Aresets.%20The%20result%20of%20our%20reverse%20curriculum%20is%20an%20initial%20policy%20that%20performs%0Awell%20on%20a%20narrow%20initial%20state%20distribution%20and%20helps%20overcome%20difficult%0Aexploration%20problems.%20A%20forward%20curriculum%20is%20then%20used%20to%20accelerate%20the%0Atraining%20of%20the%20initial%20policy%20to%20perform%20well%20on%20the%20full%20initial%20state%0Adistribution%20of%20the%20task%20and%20improve%20demonstration%20and%20sample%20efficiency.%20We%0Ashow%20how%20the%20combination%20of%20a%20reverse%20curriculum%20and%20forward%20curriculum%20in%20our%0Amethod%2C%20RFCL%2C%20enables%20significant%20improvements%20in%20demonstration%20and%20sample%0Aefficiency%20compared%20against%20various%20state-of-the-art%0Alearning-from-demonstration%20baselines%2C%20even%20solving%20previously%20unsolvable%20tasks%0Athat%20require%20high%20precision%20and%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReverse%2520Forward%2520Curriculum%2520Learning%2520for%2520Extreme%2520Sample%2520and%2520Demonstration%250A%2520%2520Efficiency%2520in%2520Reinforcement%2520Learning%26entry.906535625%3DStone%2520Tao%2520and%2520Arth%2520Shukla%2520and%2520Tse-kai%2520Chan%2520and%2520Hao%2520Su%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520presents%2520a%2520promising%2520framework%2520to%2520learn%2520policies%250Athrough%2520environment%2520interaction%252C%2520but%2520often%2520requires%2520an%2520infeasible%2520amount%2520of%250Ainteraction%2520data%2520to%2520solve%2520complex%2520tasks%2520from%2520sparse%2520rewards.%2520One%2520direction%250Aincludes%2520augmenting%2520RL%2520with%2520offline%2520data%2520demonstrating%2520desired%2520tasks%252C%2520but%2520past%250Awork%2520often%2520require%2520a%2520lot%2520of%2520high-quality%2520demonstration%2520data%2520that%2520is%2520difficult%250Ato%2520obtain%252C%2520especially%2520for%2520domains%2520such%2520as%2520robotics.%2520Our%2520approach%2520consists%2520of%2520a%250Areverse%2520curriculum%2520followed%2520by%2520a%2520forward%2520curriculum.%2520Unique%2520to%2520our%2520approach%250Acompared%2520to%2520past%2520work%2520is%2520the%2520ability%2520to%2520efficiently%2520leverage%2520more%2520than%2520one%250Ademonstration%2520via%2520a%2520per-demonstration%2520reverse%2520curriculum%2520generated%2520via%2520state%250Aresets.%2520The%2520result%2520of%2520our%2520reverse%2520curriculum%2520is%2520an%2520initial%2520policy%2520that%2520performs%250Awell%2520on%2520a%2520narrow%2520initial%2520state%2520distribution%2520and%2520helps%2520overcome%2520difficult%250Aexploration%2520problems.%2520A%2520forward%2520curriculum%2520is%2520then%2520used%2520to%2520accelerate%2520the%250Atraining%2520of%2520the%2520initial%2520policy%2520to%2520perform%2520well%2520on%2520the%2520full%2520initial%2520state%250Adistribution%2520of%2520the%2520task%2520and%2520improve%2520demonstration%2520and%2520sample%2520efficiency.%2520We%250Ashow%2520how%2520the%2520combination%2520of%2520a%2520reverse%2520curriculum%2520and%2520forward%2520curriculum%2520in%2520our%250Amethod%252C%2520RFCL%252C%2520enables%2520significant%2520improvements%2520in%2520demonstration%2520and%2520sample%250Aefficiency%2520compared%2520against%2520various%2520state-of-the-art%250Alearning-from-demonstration%2520baselines%252C%2520even%2520solving%2520previously%2520unsolvable%2520tasks%250Athat%2520require%2520high%2520precision%2520and%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reverse%20Forward%20Curriculum%20Learning%20for%20Extreme%20Sample%20and%20Demonstration%0A%20%20Efficiency%20in%20Reinforcement%20Learning&entry.906535625=Stone%20Tao%20and%20Arth%20Shukla%20and%20Tse-kai%20Chan%20and%20Hao%20Su&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20presents%20a%20promising%20framework%20to%20learn%20policies%0Athrough%20environment%20interaction%2C%20but%20often%20requires%20an%20infeasible%20amount%20of%0Ainteraction%20data%20to%20solve%20complex%20tasks%20from%20sparse%20rewards.%20One%20direction%0Aincludes%20augmenting%20RL%20with%20offline%20data%20demonstrating%20desired%20tasks%2C%20but%20past%0Awork%20often%20require%20a%20lot%20of%20high-quality%20demonstration%20data%20that%20is%20difficult%0Ato%20obtain%2C%20especially%20for%20domains%20such%20as%20robotics.%20Our%20approach%20consists%20of%20a%0Areverse%20curriculum%20followed%20by%20a%20forward%20curriculum.%20Unique%20to%20our%20approach%0Acompared%20to%20past%20work%20is%20the%20ability%20to%20efficiently%20leverage%20more%20than%20one%0Ademonstration%20via%20a%20per-demonstration%20reverse%20curriculum%20generated%20via%20state%0Aresets.%20The%20result%20of%20our%20reverse%20curriculum%20is%20an%20initial%20policy%20that%20performs%0Awell%20on%20a%20narrow%20initial%20state%20distribution%20and%20helps%20overcome%20difficult%0Aexploration%20problems.%20A%20forward%20curriculum%20is%20then%20used%20to%20accelerate%20the%0Atraining%20of%20the%20initial%20policy%20to%20perform%20well%20on%20the%20full%20initial%20state%0Adistribution%20of%20the%20task%20and%20improve%20demonstration%20and%20sample%20efficiency.%20We%0Ashow%20how%20the%20combination%20of%20a%20reverse%20curriculum%20and%20forward%20curriculum%20in%20our%0Amethod%2C%20RFCL%2C%20enables%20significant%20improvements%20in%20demonstration%20and%20sample%0Aefficiency%20compared%20against%20various%20state-of-the-art%0Alearning-from-demonstration%20baselines%2C%20even%20solving%20previously%20unsolvable%20tasks%0Athat%20require%20high%20precision%20and%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03379v1&entry.124074799=Read"},
{"title": "Optimizing Hand Region Detection in MediaPipe Holistic Full-Body Pose\n  Estimation to Improve Accuracy and Avoid Downstream Errors", "author": "Amit Moryossef", "abstract": "  This paper addresses a critical flaw in MediaPipe Holistic's hand Region of\nInterest (ROI) prediction, which struggles with non-ideal hand orientations,\naffecting sign language recognition accuracy. We propose a data-driven approach\nto enhance ROI estimation, leveraging an enriched feature set including\nadditional hand keypoints and the z-dimension. Our results demonstrate better\nestimates, with higher Intersection-over-Union compared to the current method.\nOur code and optimizations are available at\nhttps://github.com/sign-language-processing/mediapipe-hand-crop-fix.\n", "link": "http://arxiv.org/abs/2405.03545v1", "date": "2024-05-06", "relevancy": 2.0024, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5075}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.499}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Hand%20Region%20Detection%20in%20MediaPipe%20Holistic%20Full-Body%20Pose%0A%20%20Estimation%20to%20Improve%20Accuracy%20and%20Avoid%20Downstream%20Errors&body=Title%3A%20Optimizing%20Hand%20Region%20Detection%20in%20MediaPipe%20Holistic%20Full-Body%20Pose%0A%20%20Estimation%20to%20Improve%20Accuracy%20and%20Avoid%20Downstream%20Errors%0AAuthor%3A%20Amit%20Moryossef%0AAbstract%3A%20%20%20This%20paper%20addresses%20a%20critical%20flaw%20in%20MediaPipe%20Holistic%27s%20hand%20Region%20of%0AInterest%20%28ROI%29%20prediction%2C%20which%20struggles%20with%20non-ideal%20hand%20orientations%2C%0Aaffecting%20sign%20language%20recognition%20accuracy.%20We%20propose%20a%20data-driven%20approach%0Ato%20enhance%20ROI%20estimation%2C%20leveraging%20an%20enriched%20feature%20set%20including%0Aadditional%20hand%20keypoints%20and%20the%20z-dimension.%20Our%20results%20demonstrate%20better%0Aestimates%2C%20with%20higher%20Intersection-over-Union%20compared%20to%20the%20current%20method.%0AOur%20code%20and%20optimizations%20are%20available%20at%0Ahttps%3A//github.com/sign-language-processing/mediapipe-hand-crop-fix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Hand%2520Region%2520Detection%2520in%2520MediaPipe%2520Holistic%2520Full-Body%2520Pose%250A%2520%2520Estimation%2520to%2520Improve%2520Accuracy%2520and%2520Avoid%2520Downstream%2520Errors%26entry.906535625%3DAmit%2520Moryossef%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520a%2520critical%2520flaw%2520in%2520MediaPipe%2520Holistic%2527s%2520hand%2520Region%2520of%250AInterest%2520%2528ROI%2529%2520prediction%252C%2520which%2520struggles%2520with%2520non-ideal%2520hand%2520orientations%252C%250Aaffecting%2520sign%2520language%2520recognition%2520accuracy.%2520We%2520propose%2520a%2520data-driven%2520approach%250Ato%2520enhance%2520ROI%2520estimation%252C%2520leveraging%2520an%2520enriched%2520feature%2520set%2520including%250Aadditional%2520hand%2520keypoints%2520and%2520the%2520z-dimension.%2520Our%2520results%2520demonstrate%2520better%250Aestimates%252C%2520with%2520higher%2520Intersection-over-Union%2520compared%2520to%2520the%2520current%2520method.%250AOur%2520code%2520and%2520optimizations%2520are%2520available%2520at%250Ahttps%253A//github.com/sign-language-processing/mediapipe-hand-crop-fix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Hand%20Region%20Detection%20in%20MediaPipe%20Holistic%20Full-Body%20Pose%0A%20%20Estimation%20to%20Improve%20Accuracy%20and%20Avoid%20Downstream%20Errors&entry.906535625=Amit%20Moryossef&entry.1292438233=%20%20This%20paper%20addresses%20a%20critical%20flaw%20in%20MediaPipe%20Holistic%27s%20hand%20Region%20of%0AInterest%20%28ROI%29%20prediction%2C%20which%20struggles%20with%20non-ideal%20hand%20orientations%2C%0Aaffecting%20sign%20language%20recognition%20accuracy.%20We%20propose%20a%20data-driven%20approach%0Ato%20enhance%20ROI%20estimation%2C%20leveraging%20an%20enriched%20feature%20set%20including%0Aadditional%20hand%20keypoints%20and%20the%20z-dimension.%20Our%20results%20demonstrate%20better%0Aestimates%2C%20with%20higher%20Intersection-over-Union%20compared%20to%20the%20current%20method.%0AOur%20code%20and%20optimizations%20are%20available%20at%0Ahttps%3A//github.com/sign-language-processing/mediapipe-hand-crop-fix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03545v1&entry.124074799=Read"},
{"title": "ILILT: Implicit Learning of Inverse Lithography Technologies", "author": "Haoyu Yang and Haoxing Ren", "abstract": "  Lithography, transferring chip design masks to the silicon wafer, is the most\nimportant phase in modern semiconductor manufacturing flow. Due to the\nlimitations of lithography systems, Extensive design optimizations are required\nto tackle the design and silicon mismatch. Inverse lithography technology (ILT)\nis one of the promising solutions to perform pre-fabrication optimization,\ntermed mask optimization. Because of mask optimization problems' constrained\nnon-convexity, numerical ILT solvers rely heavily on good initialization to\navoid getting stuck on sub-optimal solutions. Machine learning (ML) techniques\nare hence proposed to generate mask initialization for ILT solvers with\none-shot inference, targeting faster and better convergence during ILT. This\npaper addresses the question of \\textit{whether ML models can directly generate\nhigh-quality optimized masks without engaging ILT solvers in the loop}. We\npropose an implicit learning ILT framework: ILILT, which leverages the implicit\nlayer learning method and lithography-conditioned inputs to ground the model.\nTrained to understand the ILT optimization procedure, ILILT can outperform the\nstate-of-the-art machine learning solutions, significantly improving efficiency\nand quality.\n", "link": "http://arxiv.org/abs/2405.03574v1", "date": "2024-05-06", "relevancy": 1.9994, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.508}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.503}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ILILT%3A%20Implicit%20Learning%20of%20Inverse%20Lithography%20Technologies&body=Title%3A%20ILILT%3A%20Implicit%20Learning%20of%20Inverse%20Lithography%20Technologies%0AAuthor%3A%20Haoyu%20Yang%20and%20Haoxing%20Ren%0AAbstract%3A%20%20%20Lithography%2C%20transferring%20chip%20design%20masks%20to%20the%20silicon%20wafer%2C%20is%20the%20most%0Aimportant%20phase%20in%20modern%20semiconductor%20manufacturing%20flow.%20Due%20to%20the%0Alimitations%20of%20lithography%20systems%2C%20Extensive%20design%20optimizations%20are%20required%0Ato%20tackle%20the%20design%20and%20silicon%20mismatch.%20Inverse%20lithography%20technology%20%28ILT%29%0Ais%20one%20of%20the%20promising%20solutions%20to%20perform%20pre-fabrication%20optimization%2C%0Atermed%20mask%20optimization.%20Because%20of%20mask%20optimization%20problems%27%20constrained%0Anon-convexity%2C%20numerical%20ILT%20solvers%20rely%20heavily%20on%20good%20initialization%20to%0Aavoid%20getting%20stuck%20on%20sub-optimal%20solutions.%20Machine%20learning%20%28ML%29%20techniques%0Aare%20hence%20proposed%20to%20generate%20mask%20initialization%20for%20ILT%20solvers%20with%0Aone-shot%20inference%2C%20targeting%20faster%20and%20better%20convergence%20during%20ILT.%20This%0Apaper%20addresses%20the%20question%20of%20%5Ctextit%7Bwhether%20ML%20models%20can%20directly%20generate%0Ahigh-quality%20optimized%20masks%20without%20engaging%20ILT%20solvers%20in%20the%20loop%7D.%20We%0Apropose%20an%20implicit%20learning%20ILT%20framework%3A%20ILILT%2C%20which%20leverages%20the%20implicit%0Alayer%20learning%20method%20and%20lithography-conditioned%20inputs%20to%20ground%20the%20model.%0ATrained%20to%20understand%20the%20ILT%20optimization%20procedure%2C%20ILILT%20can%20outperform%20the%0Astate-of-the-art%20machine%20learning%20solutions%2C%20significantly%20improving%20efficiency%0Aand%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DILILT%253A%2520Implicit%2520Learning%2520of%2520Inverse%2520Lithography%2520Technologies%26entry.906535625%3DHaoyu%2520Yang%2520and%2520Haoxing%2520Ren%26entry.1292438233%3D%2520%2520Lithography%252C%2520transferring%2520chip%2520design%2520masks%2520to%2520the%2520silicon%2520wafer%252C%2520is%2520the%2520most%250Aimportant%2520phase%2520in%2520modern%2520semiconductor%2520manufacturing%2520flow.%2520Due%2520to%2520the%250Alimitations%2520of%2520lithography%2520systems%252C%2520Extensive%2520design%2520optimizations%2520are%2520required%250Ato%2520tackle%2520the%2520design%2520and%2520silicon%2520mismatch.%2520Inverse%2520lithography%2520technology%2520%2528ILT%2529%250Ais%2520one%2520of%2520the%2520promising%2520solutions%2520to%2520perform%2520pre-fabrication%2520optimization%252C%250Atermed%2520mask%2520optimization.%2520Because%2520of%2520mask%2520optimization%2520problems%2527%2520constrained%250Anon-convexity%252C%2520numerical%2520ILT%2520solvers%2520rely%2520heavily%2520on%2520good%2520initialization%2520to%250Aavoid%2520getting%2520stuck%2520on%2520sub-optimal%2520solutions.%2520Machine%2520learning%2520%2528ML%2529%2520techniques%250Aare%2520hence%2520proposed%2520to%2520generate%2520mask%2520initialization%2520for%2520ILT%2520solvers%2520with%250Aone-shot%2520inference%252C%2520targeting%2520faster%2520and%2520better%2520convergence%2520during%2520ILT.%2520This%250Apaper%2520addresses%2520the%2520question%2520of%2520%255Ctextit%257Bwhether%2520ML%2520models%2520can%2520directly%2520generate%250Ahigh-quality%2520optimized%2520masks%2520without%2520engaging%2520ILT%2520solvers%2520in%2520the%2520loop%257D.%2520We%250Apropose%2520an%2520implicit%2520learning%2520ILT%2520framework%253A%2520ILILT%252C%2520which%2520leverages%2520the%2520implicit%250Alayer%2520learning%2520method%2520and%2520lithography-conditioned%2520inputs%2520to%2520ground%2520the%2520model.%250ATrained%2520to%2520understand%2520the%2520ILT%2520optimization%2520procedure%252C%2520ILILT%2520can%2520outperform%2520the%250Astate-of-the-art%2520machine%2520learning%2520solutions%252C%2520significantly%2520improving%2520efficiency%250Aand%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ILILT%3A%20Implicit%20Learning%20of%20Inverse%20Lithography%20Technologies&entry.906535625=Haoyu%20Yang%20and%20Haoxing%20Ren&entry.1292438233=%20%20Lithography%2C%20transferring%20chip%20design%20masks%20to%20the%20silicon%20wafer%2C%20is%20the%20most%0Aimportant%20phase%20in%20modern%20semiconductor%20manufacturing%20flow.%20Due%20to%20the%0Alimitations%20of%20lithography%20systems%2C%20Extensive%20design%20optimizations%20are%20required%0Ato%20tackle%20the%20design%20and%20silicon%20mismatch.%20Inverse%20lithography%20technology%20%28ILT%29%0Ais%20one%20of%20the%20promising%20solutions%20to%20perform%20pre-fabrication%20optimization%2C%0Atermed%20mask%20optimization.%20Because%20of%20mask%20optimization%20problems%27%20constrained%0Anon-convexity%2C%20numerical%20ILT%20solvers%20rely%20heavily%20on%20good%20initialization%20to%0Aavoid%20getting%20stuck%20on%20sub-optimal%20solutions.%20Machine%20learning%20%28ML%29%20techniques%0Aare%20hence%20proposed%20to%20generate%20mask%20initialization%20for%20ILT%20solvers%20with%0Aone-shot%20inference%2C%20targeting%20faster%20and%20better%20convergence%20during%20ILT.%20This%0Apaper%20addresses%20the%20question%20of%20%5Ctextit%7Bwhether%20ML%20models%20can%20directly%20generate%0Ahigh-quality%20optimized%20masks%20without%20engaging%20ILT%20solvers%20in%20the%20loop%7D.%20We%0Apropose%20an%20implicit%20learning%20ILT%20framework%3A%20ILILT%2C%20which%20leverages%20the%20implicit%0Alayer%20learning%20method%20and%20lithography-conditioned%20inputs%20to%20ground%20the%20model.%0ATrained%20to%20understand%20the%20ILT%20optimization%20procedure%2C%20ILILT%20can%20outperform%20the%0Astate-of-the-art%20machine%20learning%20solutions%2C%20significantly%20improving%20efficiency%0Aand%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03574v1&entry.124074799=Read"},
{"title": "Geometry-aware framework for deep energy method: an application to\n  structural mechanics with hyperelastic materials", "author": "Thi Nguyen Khoa Nguyen and Thibault Dairay and Rapha\u00ebl Meunier and Christophe Millet and Mathilde Mougeot", "abstract": "  Physics-Informed Neural Networks (PINNs) have gained considerable interest in\ndiverse engineering domains thanks to their capacity to integrate physical laws\ninto deep learning models. Recently, geometry-aware PINN-based approaches that\nemploy the strong form of underlying physical system equations have been\ndeveloped with the aim of integrating geometric information into PINNs. Despite\nongoing research, the assessment of PINNs in problems with various geometries\nremains an active area of investigation. In this work, we introduce a novel\nphysics-informed framework named the Geometry-Aware Deep Energy Method (GADEM)\nfor solving structural mechanics problems on different geometries. As the weak\nform of the physical system equation (or the energy-based approach) has\ndemonstrated clear advantages compared to the strong form for solving solid\nmechanics problems, GADEM employs the weak form and aims to infer the solution\non multiple shapes of geometries. Integrating a geometry-aware framework into\nan energy-based method results in an effective physics-informed deep learning\nmodel in terms of accuracy and computational cost. Different ways to represent\nthe geometric information and to encode the geometric latent vectors are\ninvestigated in this work. We introduce a loss function of GADEM which is\nminimized based on the potential energy of all considered geometries. An\nadaptive learning method is also employed for the sampling of collocation\npoints to enhance the performance of GADEM. We present some applications of\nGADEM to solve solid mechanics problems, including a loading simulation of a\ntoy tire involving contact mechanics and large deformation hyperelasticity. The\nnumerical results of this work demonstrate the remarkable capability of GADEM\nto infer the solution on various and new shapes of geometries using only one\ntrained model.\n", "link": "http://arxiv.org/abs/2405.03427v1", "date": "2024-05-06", "relevancy": 1.9952, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5423}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4929}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-aware%20framework%20for%20deep%20energy%20method%3A%20an%20application%20to%0A%20%20structural%20mechanics%20with%20hyperelastic%20materials&body=Title%3A%20Geometry-aware%20framework%20for%20deep%20energy%20method%3A%20an%20application%20to%0A%20%20structural%20mechanics%20with%20hyperelastic%20materials%0AAuthor%3A%20Thi%20Nguyen%20Khoa%20Nguyen%20and%20Thibault%20Dairay%20and%20Rapha%C3%ABl%20Meunier%20and%20Christophe%20Millet%20and%20Mathilde%20Mougeot%0AAbstract%3A%20%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20have%20gained%20considerable%20interest%20in%0Adiverse%20engineering%20domains%20thanks%20to%20their%20capacity%20to%20integrate%20physical%20laws%0Ainto%20deep%20learning%20models.%20Recently%2C%20geometry-aware%20PINN-based%20approaches%20that%0Aemploy%20the%20strong%20form%20of%20underlying%20physical%20system%20equations%20have%20been%0Adeveloped%20with%20the%20aim%20of%20integrating%20geometric%20information%20into%20PINNs.%20Despite%0Aongoing%20research%2C%20the%20assessment%20of%20PINNs%20in%20problems%20with%20various%20geometries%0Aremains%20an%20active%20area%20of%20investigation.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aphysics-informed%20framework%20named%20the%20Geometry-Aware%20Deep%20Energy%20Method%20%28GADEM%29%0Afor%20solving%20structural%20mechanics%20problems%20on%20different%20geometries.%20As%20the%20weak%0Aform%20of%20the%20physical%20system%20equation%20%28or%20the%20energy-based%20approach%29%20has%0Ademonstrated%20clear%20advantages%20compared%20to%20the%20strong%20form%20for%20solving%20solid%0Amechanics%20problems%2C%20GADEM%20employs%20the%20weak%20form%20and%20aims%20to%20infer%20the%20solution%0Aon%20multiple%20shapes%20of%20geometries.%20Integrating%20a%20geometry-aware%20framework%20into%0Aan%20energy-based%20method%20results%20in%20an%20effective%20physics-informed%20deep%20learning%0Amodel%20in%20terms%20of%20accuracy%20and%20computational%20cost.%20Different%20ways%20to%20represent%0Athe%20geometric%20information%20and%20to%20encode%20the%20geometric%20latent%20vectors%20are%0Ainvestigated%20in%20this%20work.%20We%20introduce%20a%20loss%20function%20of%20GADEM%20which%20is%0Aminimized%20based%20on%20the%20potential%20energy%20of%20all%20considered%20geometries.%20An%0Aadaptive%20learning%20method%20is%20also%20employed%20for%20the%20sampling%20of%20collocation%0Apoints%20to%20enhance%20the%20performance%20of%20GADEM.%20We%20present%20some%20applications%20of%0AGADEM%20to%20solve%20solid%20mechanics%20problems%2C%20including%20a%20loading%20simulation%20of%20a%0Atoy%20tire%20involving%20contact%20mechanics%20and%20large%20deformation%20hyperelasticity.%20The%0Anumerical%20results%20of%20this%20work%20demonstrate%20the%20remarkable%20capability%20of%20GADEM%0Ato%20infer%20the%20solution%20on%20various%20and%20new%20shapes%20of%20geometries%20using%20only%20one%0Atrained%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-aware%2520framework%2520for%2520deep%2520energy%2520method%253A%2520an%2520application%2520to%250A%2520%2520structural%2520mechanics%2520with%2520hyperelastic%2520materials%26entry.906535625%3DThi%2520Nguyen%2520Khoa%2520Nguyen%2520and%2520Thibault%2520Dairay%2520and%2520Rapha%25C3%25ABl%2520Meunier%2520and%2520Christophe%2520Millet%2520and%2520Mathilde%2520Mougeot%26entry.1292438233%3D%2520%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520have%2520gained%2520considerable%2520interest%2520in%250Adiverse%2520engineering%2520domains%2520thanks%2520to%2520their%2520capacity%2520to%2520integrate%2520physical%2520laws%250Ainto%2520deep%2520learning%2520models.%2520Recently%252C%2520geometry-aware%2520PINN-based%2520approaches%2520that%250Aemploy%2520the%2520strong%2520form%2520of%2520underlying%2520physical%2520system%2520equations%2520have%2520been%250Adeveloped%2520with%2520the%2520aim%2520of%2520integrating%2520geometric%2520information%2520into%2520PINNs.%2520Despite%250Aongoing%2520research%252C%2520the%2520assessment%2520of%2520PINNs%2520in%2520problems%2520with%2520various%2520geometries%250Aremains%2520an%2520active%2520area%2520of%2520investigation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%250Aphysics-informed%2520framework%2520named%2520the%2520Geometry-Aware%2520Deep%2520Energy%2520Method%2520%2528GADEM%2529%250Afor%2520solving%2520structural%2520mechanics%2520problems%2520on%2520different%2520geometries.%2520As%2520the%2520weak%250Aform%2520of%2520the%2520physical%2520system%2520equation%2520%2528or%2520the%2520energy-based%2520approach%2529%2520has%250Ademonstrated%2520clear%2520advantages%2520compared%2520to%2520the%2520strong%2520form%2520for%2520solving%2520solid%250Amechanics%2520problems%252C%2520GADEM%2520employs%2520the%2520weak%2520form%2520and%2520aims%2520to%2520infer%2520the%2520solution%250Aon%2520multiple%2520shapes%2520of%2520geometries.%2520Integrating%2520a%2520geometry-aware%2520framework%2520into%250Aan%2520energy-based%2520method%2520results%2520in%2520an%2520effective%2520physics-informed%2520deep%2520learning%250Amodel%2520in%2520terms%2520of%2520accuracy%2520and%2520computational%2520cost.%2520Different%2520ways%2520to%2520represent%250Athe%2520geometric%2520information%2520and%2520to%2520encode%2520the%2520geometric%2520latent%2520vectors%2520are%250Ainvestigated%2520in%2520this%2520work.%2520We%2520introduce%2520a%2520loss%2520function%2520of%2520GADEM%2520which%2520is%250Aminimized%2520based%2520on%2520the%2520potential%2520energy%2520of%2520all%2520considered%2520geometries.%2520An%250Aadaptive%2520learning%2520method%2520is%2520also%2520employed%2520for%2520the%2520sampling%2520of%2520collocation%250Apoints%2520to%2520enhance%2520the%2520performance%2520of%2520GADEM.%2520We%2520present%2520some%2520applications%2520of%250AGADEM%2520to%2520solve%2520solid%2520mechanics%2520problems%252C%2520including%2520a%2520loading%2520simulation%2520of%2520a%250Atoy%2520tire%2520involving%2520contact%2520mechanics%2520and%2520large%2520deformation%2520hyperelasticity.%2520The%250Anumerical%2520results%2520of%2520this%2520work%2520demonstrate%2520the%2520remarkable%2520capability%2520of%2520GADEM%250Ato%2520infer%2520the%2520solution%2520on%2520various%2520and%2520new%2520shapes%2520of%2520geometries%2520using%2520only%2520one%250Atrained%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-aware%20framework%20for%20deep%20energy%20method%3A%20an%20application%20to%0A%20%20structural%20mechanics%20with%20hyperelastic%20materials&entry.906535625=Thi%20Nguyen%20Khoa%20Nguyen%20and%20Thibault%20Dairay%20and%20Rapha%C3%ABl%20Meunier%20and%20Christophe%20Millet%20and%20Mathilde%20Mougeot&entry.1292438233=%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20have%20gained%20considerable%20interest%20in%0Adiverse%20engineering%20domains%20thanks%20to%20their%20capacity%20to%20integrate%20physical%20laws%0Ainto%20deep%20learning%20models.%20Recently%2C%20geometry-aware%20PINN-based%20approaches%20that%0Aemploy%20the%20strong%20form%20of%20underlying%20physical%20system%20equations%20have%20been%0Adeveloped%20with%20the%20aim%20of%20integrating%20geometric%20information%20into%20PINNs.%20Despite%0Aongoing%20research%2C%20the%20assessment%20of%20PINNs%20in%20problems%20with%20various%20geometries%0Aremains%20an%20active%20area%20of%20investigation.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aphysics-informed%20framework%20named%20the%20Geometry-Aware%20Deep%20Energy%20Method%20%28GADEM%29%0Afor%20solving%20structural%20mechanics%20problems%20on%20different%20geometries.%20As%20the%20weak%0Aform%20of%20the%20physical%20system%20equation%20%28or%20the%20energy-based%20approach%29%20has%0Ademonstrated%20clear%20advantages%20compared%20to%20the%20strong%20form%20for%20solving%20solid%0Amechanics%20problems%2C%20GADEM%20employs%20the%20weak%20form%20and%20aims%20to%20infer%20the%20solution%0Aon%20multiple%20shapes%20of%20geometries.%20Integrating%20a%20geometry-aware%20framework%20into%0Aan%20energy-based%20method%20results%20in%20an%20effective%20physics-informed%20deep%20learning%0Amodel%20in%20terms%20of%20accuracy%20and%20computational%20cost.%20Different%20ways%20to%20represent%0Athe%20geometric%20information%20and%20to%20encode%20the%20geometric%20latent%20vectors%20are%0Ainvestigated%20in%20this%20work.%20We%20introduce%20a%20loss%20function%20of%20GADEM%20which%20is%0Aminimized%20based%20on%20the%20potential%20energy%20of%20all%20considered%20geometries.%20An%0Aadaptive%20learning%20method%20is%20also%20employed%20for%20the%20sampling%20of%20collocation%0Apoints%20to%20enhance%20the%20performance%20of%20GADEM.%20We%20present%20some%20applications%20of%0AGADEM%20to%20solve%20solid%20mechanics%20problems%2C%20including%20a%20loading%20simulation%20of%20a%0Atoy%20tire%20involving%20contact%20mechanics%20and%20large%20deformation%20hyperelasticity.%20The%0Anumerical%20results%20of%20this%20work%20demonstrate%20the%20remarkable%20capability%20of%20GADEM%0Ato%20infer%20the%20solution%20on%20various%20and%20new%20shapes%20of%20geometries%20using%20only%20one%0Atrained%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03427v1&entry.124074799=Read"},
{"title": "Efficient and Near-Optimal Noise Generation for Streaming Differential\n  Privacy", "author": "Krishnamurthy Dvijotham and H. Brendan McMahan and Krishna Pillutla and Thomas Steinke and Abhradeep Thakurta", "abstract": "  In the task of differentially private (DP) continual counting, we receive a\nstream of increments and our goal is to output an approximate running total of\nthese increments, without revealing too much about any specific increment.\nDespite its simplicity, differentially private continual counting has attracted\nsignificant attention both in theory and in practice. Existing algorithms for\ndifferentially private continual counting are either inefficient in terms of\ntheir space usage or add an excessive amount of noise, inducing suboptimal\nutility.\n  The most practical DP continual counting algorithms add carefully correlated\nGaussian noise to the values. The task of choosing the covariance for this\nnoise can be expressed in terms of factoring the lower-triangular matrix of\nones (which computes prefix sums). We present two approaches from this class\n(for different parameter regimes) that achieve near-optimal utility for DP\ncontinual counting and only require logarithmic or polylogarithmic space (and\ntime).\n  Our first approach is based on a space-efficient streaming matrix\nmultiplication algorithm for a class of Toeplitz matrices. We show that to\ninstantiate this algorithm for DP continual counting, it is sufficient to find\na low-degree rational function that approximates the square root on a circle in\nthe complex plane. We then apply and extend tools from approximation theory to\nachieve this. We also derive efficient closed-forms for the objective function\nfor arbitrarily many steps, and show direct numerical optimization yields a\nhighly practical solution to the problem. Our second approach combines our\nfirst approach with a recursive construction similar to the binary tree\nmechanism.\n", "link": "http://arxiv.org/abs/2404.16706v3", "date": "2024-05-06", "relevancy": 1.9916, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5046}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4953}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Near-Optimal%20Noise%20Generation%20for%20Streaming%20Differential%0A%20%20Privacy&body=Title%3A%20Efficient%20and%20Near-Optimal%20Noise%20Generation%20for%20Streaming%20Differential%0A%20%20Privacy%0AAuthor%3A%20Krishnamurthy%20Dvijotham%20and%20H.%20Brendan%20McMahan%20and%20Krishna%20Pillutla%20and%20Thomas%20Steinke%20and%20Abhradeep%20Thakurta%0AAbstract%3A%20%20%20In%20the%20task%20of%20differentially%20private%20%28DP%29%20continual%20counting%2C%20we%20receive%20a%0Astream%20of%20increments%20and%20our%20goal%20is%20to%20output%20an%20approximate%20running%20total%20of%0Athese%20increments%2C%20without%20revealing%20too%20much%20about%20any%20specific%20increment.%0ADespite%20its%20simplicity%2C%20differentially%20private%20continual%20counting%20has%20attracted%0Asignificant%20attention%20both%20in%20theory%20and%20in%20practice.%20Existing%20algorithms%20for%0Adifferentially%20private%20continual%20counting%20are%20either%20inefficient%20in%20terms%20of%0Atheir%20space%20usage%20or%20add%20an%20excessive%20amount%20of%20noise%2C%20inducing%20suboptimal%0Autility.%0A%20%20The%20most%20practical%20DP%20continual%20counting%20algorithms%20add%20carefully%20correlated%0AGaussian%20noise%20to%20the%20values.%20The%20task%20of%20choosing%20the%20covariance%20for%20this%0Anoise%20can%20be%20expressed%20in%20terms%20of%20factoring%20the%20lower-triangular%20matrix%20of%0Aones%20%28which%20computes%20prefix%20sums%29.%20We%20present%20two%20approaches%20from%20this%20class%0A%28for%20different%20parameter%20regimes%29%20that%20achieve%20near-optimal%20utility%20for%20DP%0Acontinual%20counting%20and%20only%20require%20logarithmic%20or%20polylogarithmic%20space%20%28and%0Atime%29.%0A%20%20Our%20first%20approach%20is%20based%20on%20a%20space-efficient%20streaming%20matrix%0Amultiplication%20algorithm%20for%20a%20class%20of%20Toeplitz%20matrices.%20We%20show%20that%20to%0Ainstantiate%20this%20algorithm%20for%20DP%20continual%20counting%2C%20it%20is%20sufficient%20to%20find%0Aa%20low-degree%20rational%20function%20that%20approximates%20the%20square%20root%20on%20a%20circle%20in%0Athe%20complex%20plane.%20We%20then%20apply%20and%20extend%20tools%20from%20approximation%20theory%20to%0Aachieve%20this.%20We%20also%20derive%20efficient%20closed-forms%20for%20the%20objective%20function%0Afor%20arbitrarily%20many%20steps%2C%20and%20show%20direct%20numerical%20optimization%20yields%20a%0Ahighly%20practical%20solution%20to%20the%20problem.%20Our%20second%20approach%20combines%20our%0Afirst%20approach%20with%20a%20recursive%20construction%20similar%20to%20the%20binary%20tree%0Amechanism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16706v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520and%2520Near-Optimal%2520Noise%2520Generation%2520for%2520Streaming%2520Differential%250A%2520%2520Privacy%26entry.906535625%3DKrishnamurthy%2520Dvijotham%2520and%2520H.%2520Brendan%2520McMahan%2520and%2520Krishna%2520Pillutla%2520and%2520Thomas%2520Steinke%2520and%2520Abhradeep%2520Thakurta%26entry.1292438233%3D%2520%2520In%2520the%2520task%2520of%2520differentially%2520private%2520%2528DP%2529%2520continual%2520counting%252C%2520we%2520receive%2520a%250Astream%2520of%2520increments%2520and%2520our%2520goal%2520is%2520to%2520output%2520an%2520approximate%2520running%2520total%2520of%250Athese%2520increments%252C%2520without%2520revealing%2520too%2520much%2520about%2520any%2520specific%2520increment.%250ADespite%2520its%2520simplicity%252C%2520differentially%2520private%2520continual%2520counting%2520has%2520attracted%250Asignificant%2520attention%2520both%2520in%2520theory%2520and%2520in%2520practice.%2520Existing%2520algorithms%2520for%250Adifferentially%2520private%2520continual%2520counting%2520are%2520either%2520inefficient%2520in%2520terms%2520of%250Atheir%2520space%2520usage%2520or%2520add%2520an%2520excessive%2520amount%2520of%2520noise%252C%2520inducing%2520suboptimal%250Autility.%250A%2520%2520The%2520most%2520practical%2520DP%2520continual%2520counting%2520algorithms%2520add%2520carefully%2520correlated%250AGaussian%2520noise%2520to%2520the%2520values.%2520The%2520task%2520of%2520choosing%2520the%2520covariance%2520for%2520this%250Anoise%2520can%2520be%2520expressed%2520in%2520terms%2520of%2520factoring%2520the%2520lower-triangular%2520matrix%2520of%250Aones%2520%2528which%2520computes%2520prefix%2520sums%2529.%2520We%2520present%2520two%2520approaches%2520from%2520this%2520class%250A%2528for%2520different%2520parameter%2520regimes%2529%2520that%2520achieve%2520near-optimal%2520utility%2520for%2520DP%250Acontinual%2520counting%2520and%2520only%2520require%2520logarithmic%2520or%2520polylogarithmic%2520space%2520%2528and%250Atime%2529.%250A%2520%2520Our%2520first%2520approach%2520is%2520based%2520on%2520a%2520space-efficient%2520streaming%2520matrix%250Amultiplication%2520algorithm%2520for%2520a%2520class%2520of%2520Toeplitz%2520matrices.%2520We%2520show%2520that%2520to%250Ainstantiate%2520this%2520algorithm%2520for%2520DP%2520continual%2520counting%252C%2520it%2520is%2520sufficient%2520to%2520find%250Aa%2520low-degree%2520rational%2520function%2520that%2520approximates%2520the%2520square%2520root%2520on%2520a%2520circle%2520in%250Athe%2520complex%2520plane.%2520We%2520then%2520apply%2520and%2520extend%2520tools%2520from%2520approximation%2520theory%2520to%250Aachieve%2520this.%2520We%2520also%2520derive%2520efficient%2520closed-forms%2520for%2520the%2520objective%2520function%250Afor%2520arbitrarily%2520many%2520steps%252C%2520and%2520show%2520direct%2520numerical%2520optimization%2520yields%2520a%250Ahighly%2520practical%2520solution%2520to%2520the%2520problem.%2520Our%2520second%2520approach%2520combines%2520our%250Afirst%2520approach%2520with%2520a%2520recursive%2520construction%2520similar%2520to%2520the%2520binary%2520tree%250Amechanism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16706v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Near-Optimal%20Noise%20Generation%20for%20Streaming%20Differential%0A%20%20Privacy&entry.906535625=Krishnamurthy%20Dvijotham%20and%20H.%20Brendan%20McMahan%20and%20Krishna%20Pillutla%20and%20Thomas%20Steinke%20and%20Abhradeep%20Thakurta&entry.1292438233=%20%20In%20the%20task%20of%20differentially%20private%20%28DP%29%20continual%20counting%2C%20we%20receive%20a%0Astream%20of%20increments%20and%20our%20goal%20is%20to%20output%20an%20approximate%20running%20total%20of%0Athese%20increments%2C%20without%20revealing%20too%20much%20about%20any%20specific%20increment.%0ADespite%20its%20simplicity%2C%20differentially%20private%20continual%20counting%20has%20attracted%0Asignificant%20attention%20both%20in%20theory%20and%20in%20practice.%20Existing%20algorithms%20for%0Adifferentially%20private%20continual%20counting%20are%20either%20inefficient%20in%20terms%20of%0Atheir%20space%20usage%20or%20add%20an%20excessive%20amount%20of%20noise%2C%20inducing%20suboptimal%0Autility.%0A%20%20The%20most%20practical%20DP%20continual%20counting%20algorithms%20add%20carefully%20correlated%0AGaussian%20noise%20to%20the%20values.%20The%20task%20of%20choosing%20the%20covariance%20for%20this%0Anoise%20can%20be%20expressed%20in%20terms%20of%20factoring%20the%20lower-triangular%20matrix%20of%0Aones%20%28which%20computes%20prefix%20sums%29.%20We%20present%20two%20approaches%20from%20this%20class%0A%28for%20different%20parameter%20regimes%29%20that%20achieve%20near-optimal%20utility%20for%20DP%0Acontinual%20counting%20and%20only%20require%20logarithmic%20or%20polylogarithmic%20space%20%28and%0Atime%29.%0A%20%20Our%20first%20approach%20is%20based%20on%20a%20space-efficient%20streaming%20matrix%0Amultiplication%20algorithm%20for%20a%20class%20of%20Toeplitz%20matrices.%20We%20show%20that%20to%0Ainstantiate%20this%20algorithm%20for%20DP%20continual%20counting%2C%20it%20is%20sufficient%20to%20find%0Aa%20low-degree%20rational%20function%20that%20approximates%20the%20square%20root%20on%20a%20circle%20in%0Athe%20complex%20plane.%20We%20then%20apply%20and%20extend%20tools%20from%20approximation%20theory%20to%0Aachieve%20this.%20We%20also%20derive%20efficient%20closed-forms%20for%20the%20objective%20function%0Afor%20arbitrarily%20many%20steps%2C%20and%20show%20direct%20numerical%20optimization%20yields%20a%0Ahighly%20practical%20solution%20to%20the%20problem.%20Our%20second%20approach%20combines%20our%0Afirst%20approach%20with%20a%20recursive%20construction%20similar%20to%20the%20binary%20tree%0Amechanism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16706v3&entry.124074799=Read"},
{"title": "Annot-Mix: Learning with Noisy Class Labels from Multiple Annotators via\n  a Mixup Extension", "author": "Marek Herde and Lukas L\u00fchrs and Denis Huseljic and Bernhard Sick", "abstract": "  Training with noisy class labels impairs neural networks' generalization\nperformance. In this context, mixup is a popular regularization technique to\nimprove training robustness by making memorizing false class labels more\ndifficult. However, mixup neglects that, typically, multiple annotators, e.g.,\ncrowdworkers, provide class labels. Therefore, we propose an extension of\nmixup, which handles multiple class labels per instance while considering which\nclass label originates from which annotator. Integrated into our\nmulti-annotator classification framework annot-mix, it performs superiorly to\neight state-of-the-art approaches on eleven datasets with noisy class labels\nprovided either by human or simulated annotators. Our code is publicly\navailable through our repository at https://github.com/ies-research/annot-mix.\n", "link": "http://arxiv.org/abs/2405.03386v1", "date": "2024-05-06", "relevancy": 1.9896, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.525}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4998}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Annot-Mix%3A%20Learning%20with%20Noisy%20Class%20Labels%20from%20Multiple%20Annotators%20via%0A%20%20a%20Mixup%20Extension&body=Title%3A%20Annot-Mix%3A%20Learning%20with%20Noisy%20Class%20Labels%20from%20Multiple%20Annotators%20via%0A%20%20a%20Mixup%20Extension%0AAuthor%3A%20Marek%20Herde%20and%20Lukas%20L%C3%BChrs%20and%20Denis%20Huseljic%20and%20Bernhard%20Sick%0AAbstract%3A%20%20%20Training%20with%20noisy%20class%20labels%20impairs%20neural%20networks%27%20generalization%0Aperformance.%20In%20this%20context%2C%20mixup%20is%20a%20popular%20regularization%20technique%20to%0Aimprove%20training%20robustness%20by%20making%20memorizing%20false%20class%20labels%20more%0Adifficult.%20However%2C%20mixup%20neglects%20that%2C%20typically%2C%20multiple%20annotators%2C%20e.g.%2C%0Acrowdworkers%2C%20provide%20class%20labels.%20Therefore%2C%20we%20propose%20an%20extension%20of%0Amixup%2C%20which%20handles%20multiple%20class%20labels%20per%20instance%20while%20considering%20which%0Aclass%20label%20originates%20from%20which%20annotator.%20Integrated%20into%20our%0Amulti-annotator%20classification%20framework%20annot-mix%2C%20it%20performs%20superiorly%20to%0Aeight%20state-of-the-art%20approaches%20on%20eleven%20datasets%20with%20noisy%20class%20labels%0Aprovided%20either%20by%20human%20or%20simulated%20annotators.%20Our%20code%20is%20publicly%0Aavailable%20through%20our%20repository%20at%20https%3A//github.com/ies-research/annot-mix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnnot-Mix%253A%2520Learning%2520with%2520Noisy%2520Class%2520Labels%2520from%2520Multiple%2520Annotators%2520via%250A%2520%2520a%2520Mixup%2520Extension%26entry.906535625%3DMarek%2520Herde%2520and%2520Lukas%2520L%25C3%25BChrs%2520and%2520Denis%2520Huseljic%2520and%2520Bernhard%2520Sick%26entry.1292438233%3D%2520%2520Training%2520with%2520noisy%2520class%2520labels%2520impairs%2520neural%2520networks%2527%2520generalization%250Aperformance.%2520In%2520this%2520context%252C%2520mixup%2520is%2520a%2520popular%2520regularization%2520technique%2520to%250Aimprove%2520training%2520robustness%2520by%2520making%2520memorizing%2520false%2520class%2520labels%2520more%250Adifficult.%2520However%252C%2520mixup%2520neglects%2520that%252C%2520typically%252C%2520multiple%2520annotators%252C%2520e.g.%252C%250Acrowdworkers%252C%2520provide%2520class%2520labels.%2520Therefore%252C%2520we%2520propose%2520an%2520extension%2520of%250Amixup%252C%2520which%2520handles%2520multiple%2520class%2520labels%2520per%2520instance%2520while%2520considering%2520which%250Aclass%2520label%2520originates%2520from%2520which%2520annotator.%2520Integrated%2520into%2520our%250Amulti-annotator%2520classification%2520framework%2520annot-mix%252C%2520it%2520performs%2520superiorly%2520to%250Aeight%2520state-of-the-art%2520approaches%2520on%2520eleven%2520datasets%2520with%2520noisy%2520class%2520labels%250Aprovided%2520either%2520by%2520human%2520or%2520simulated%2520annotators.%2520Our%2520code%2520is%2520publicly%250Aavailable%2520through%2520our%2520repository%2520at%2520https%253A//github.com/ies-research/annot-mix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Annot-Mix%3A%20Learning%20with%20Noisy%20Class%20Labels%20from%20Multiple%20Annotators%20via%0A%20%20a%20Mixup%20Extension&entry.906535625=Marek%20Herde%20and%20Lukas%20L%C3%BChrs%20and%20Denis%20Huseljic%20and%20Bernhard%20Sick&entry.1292438233=%20%20Training%20with%20noisy%20class%20labels%20impairs%20neural%20networks%27%20generalization%0Aperformance.%20In%20this%20context%2C%20mixup%20is%20a%20popular%20regularization%20technique%20to%0Aimprove%20training%20robustness%20by%20making%20memorizing%20false%20class%20labels%20more%0Adifficult.%20However%2C%20mixup%20neglects%20that%2C%20typically%2C%20multiple%20annotators%2C%20e.g.%2C%0Acrowdworkers%2C%20provide%20class%20labels.%20Therefore%2C%20we%20propose%20an%20extension%20of%0Amixup%2C%20which%20handles%20multiple%20class%20labels%20per%20instance%20while%20considering%20which%0Aclass%20label%20originates%20from%20which%20annotator.%20Integrated%20into%20our%0Amulti-annotator%20classification%20framework%20annot-mix%2C%20it%20performs%20superiorly%20to%0Aeight%20state-of-the-art%20approaches%20on%20eleven%20datasets%20with%20noisy%20class%20labels%0Aprovided%20either%20by%20human%20or%20simulated%20annotators.%20Our%20code%20is%20publicly%0Aavailable%20through%20our%20repository%20at%20https%3A//github.com/ies-research/annot-mix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03386v1&entry.124074799=Read"},
{"title": "SHE-Net: Syntax-Hierarchy-Enhanced Text-Video Retrieval", "author": "Xuzheng Yu and Chen Jiang and Xingning Dong and Tian Gan and Ming Yang and Qingpei Guo", "abstract": "  The user base of short video apps has experienced unprecedented growth in\nrecent years, resulting in a significant demand for video content analysis. In\nparticular, text-video retrieval, which aims to find the top matching videos\ngiven text descriptions from a vast video corpus, is an essential function, the\nprimary challenge of which is to bridge the modality gap. Nevertheless, most\nexisting approaches treat texts merely as discrete tokens and neglect their\nsyntax structures. Moreover, the abundant spatial and temporal clues in videos\nare often underutilized due to the lack of interaction with text. To address\nthese issues, we argue that using texts as guidance to focus on relevant\ntemporal frames and spatial regions within videos is beneficial. In this paper,\nwe propose a novel Syntax-Hierarchy-Enhanced text-video retrieval method\n(SHE-Net) that exploits the inherent semantic and syntax hierarchy of texts to\nbridge the modality gap from two perspectives. First, to facilitate a more\nfine-grained integration of visual content, we employ the text syntax\nhierarchy, which reveals the grammatical structure of text descriptions, to\nguide the visual representations. Second, to further enhance the multi-modal\ninteraction and alignment, we also utilize the syntax hierarchy to guide the\nsimilarity calculation. We evaluated our method on four public text-video\nretrieval datasets of MSR-VTT, MSVD, DiDeMo, and ActivityNet. The experimental\nresults and ablation studies confirm the advantages of our proposed method.\n", "link": "http://arxiv.org/abs/2404.14066v2", "date": "2024-05-06", "relevancy": 1.9888, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5172}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5067}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHE-Net%3A%20Syntax-Hierarchy-Enhanced%20Text-Video%20Retrieval&body=Title%3A%20SHE-Net%3A%20Syntax-Hierarchy-Enhanced%20Text-Video%20Retrieval%0AAuthor%3A%20Xuzheng%20Yu%20and%20Chen%20Jiang%20and%20Xingning%20Dong%20and%20Tian%20Gan%20and%20Ming%20Yang%20and%20Qingpei%20Guo%0AAbstract%3A%20%20%20The%20user%20base%20of%20short%20video%20apps%20has%20experienced%20unprecedented%20growth%20in%0Arecent%20years%2C%20resulting%20in%20a%20significant%20demand%20for%20video%20content%20analysis.%20In%0Aparticular%2C%20text-video%20retrieval%2C%20which%20aims%20to%20find%20the%20top%20matching%20videos%0Agiven%20text%20descriptions%20from%20a%20vast%20video%20corpus%2C%20is%20an%20essential%20function%2C%20the%0Aprimary%20challenge%20of%20which%20is%20to%20bridge%20the%20modality%20gap.%20Nevertheless%2C%20most%0Aexisting%20approaches%20treat%20texts%20merely%20as%20discrete%20tokens%20and%20neglect%20their%0Asyntax%20structures.%20Moreover%2C%20the%20abundant%20spatial%20and%20temporal%20clues%20in%20videos%0Aare%20often%20underutilized%20due%20to%20the%20lack%20of%20interaction%20with%20text.%20To%20address%0Athese%20issues%2C%20we%20argue%20that%20using%20texts%20as%20guidance%20to%20focus%20on%20relevant%0Atemporal%20frames%20and%20spatial%20regions%20within%20videos%20is%20beneficial.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20Syntax-Hierarchy-Enhanced%20text-video%20retrieval%20method%0A%28SHE-Net%29%20that%20exploits%20the%20inherent%20semantic%20and%20syntax%20hierarchy%20of%20texts%20to%0Abridge%20the%20modality%20gap%20from%20two%20perspectives.%20First%2C%20to%20facilitate%20a%20more%0Afine-grained%20integration%20of%20visual%20content%2C%20we%20employ%20the%20text%20syntax%0Ahierarchy%2C%20which%20reveals%20the%20grammatical%20structure%20of%20text%20descriptions%2C%20to%0Aguide%20the%20visual%20representations.%20Second%2C%20to%20further%20enhance%20the%20multi-modal%0Ainteraction%20and%20alignment%2C%20we%20also%20utilize%20the%20syntax%20hierarchy%20to%20guide%20the%0Asimilarity%20calculation.%20We%20evaluated%20our%20method%20on%20four%20public%20text-video%0Aretrieval%20datasets%20of%20MSR-VTT%2C%20MSVD%2C%20DiDeMo%2C%20and%20ActivityNet.%20The%20experimental%0Aresults%20and%20ablation%20studies%20confirm%20the%20advantages%20of%20our%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14066v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHE-Net%253A%2520Syntax-Hierarchy-Enhanced%2520Text-Video%2520Retrieval%26entry.906535625%3DXuzheng%2520Yu%2520and%2520Chen%2520Jiang%2520and%2520Xingning%2520Dong%2520and%2520Tian%2520Gan%2520and%2520Ming%2520Yang%2520and%2520Qingpei%2520Guo%26entry.1292438233%3D%2520%2520The%2520user%2520base%2520of%2520short%2520video%2520apps%2520has%2520experienced%2520unprecedented%2520growth%2520in%250Arecent%2520years%252C%2520resulting%2520in%2520a%2520significant%2520demand%2520for%2520video%2520content%2520analysis.%2520In%250Aparticular%252C%2520text-video%2520retrieval%252C%2520which%2520aims%2520to%2520find%2520the%2520top%2520matching%2520videos%250Agiven%2520text%2520descriptions%2520from%2520a%2520vast%2520video%2520corpus%252C%2520is%2520an%2520essential%2520function%252C%2520the%250Aprimary%2520challenge%2520of%2520which%2520is%2520to%2520bridge%2520the%2520modality%2520gap.%2520Nevertheless%252C%2520most%250Aexisting%2520approaches%2520treat%2520texts%2520merely%2520as%2520discrete%2520tokens%2520and%2520neglect%2520their%250Asyntax%2520structures.%2520Moreover%252C%2520the%2520abundant%2520spatial%2520and%2520temporal%2520clues%2520in%2520videos%250Aare%2520often%2520underutilized%2520due%2520to%2520the%2520lack%2520of%2520interaction%2520with%2520text.%2520To%2520address%250Athese%2520issues%252C%2520we%2520argue%2520that%2520using%2520texts%2520as%2520guidance%2520to%2520focus%2520on%2520relevant%250Atemporal%2520frames%2520and%2520spatial%2520regions%2520within%2520videos%2520is%2520beneficial.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520novel%2520Syntax-Hierarchy-Enhanced%2520text-video%2520retrieval%2520method%250A%2528SHE-Net%2529%2520that%2520exploits%2520the%2520inherent%2520semantic%2520and%2520syntax%2520hierarchy%2520of%2520texts%2520to%250Abridge%2520the%2520modality%2520gap%2520from%2520two%2520perspectives.%2520First%252C%2520to%2520facilitate%2520a%2520more%250Afine-grained%2520integration%2520of%2520visual%2520content%252C%2520we%2520employ%2520the%2520text%2520syntax%250Ahierarchy%252C%2520which%2520reveals%2520the%2520grammatical%2520structure%2520of%2520text%2520descriptions%252C%2520to%250Aguide%2520the%2520visual%2520representations.%2520Second%252C%2520to%2520further%2520enhance%2520the%2520multi-modal%250Ainteraction%2520and%2520alignment%252C%2520we%2520also%2520utilize%2520the%2520syntax%2520hierarchy%2520to%2520guide%2520the%250Asimilarity%2520calculation.%2520We%2520evaluated%2520our%2520method%2520on%2520four%2520public%2520text-video%250Aretrieval%2520datasets%2520of%2520MSR-VTT%252C%2520MSVD%252C%2520DiDeMo%252C%2520and%2520ActivityNet.%2520The%2520experimental%250Aresults%2520and%2520ablation%2520studies%2520confirm%2520the%2520advantages%2520of%2520our%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14066v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHE-Net%3A%20Syntax-Hierarchy-Enhanced%20Text-Video%20Retrieval&entry.906535625=Xuzheng%20Yu%20and%20Chen%20Jiang%20and%20Xingning%20Dong%20and%20Tian%20Gan%20and%20Ming%20Yang%20and%20Qingpei%20Guo&entry.1292438233=%20%20The%20user%20base%20of%20short%20video%20apps%20has%20experienced%20unprecedented%20growth%20in%0Arecent%20years%2C%20resulting%20in%20a%20significant%20demand%20for%20video%20content%20analysis.%20In%0Aparticular%2C%20text-video%20retrieval%2C%20which%20aims%20to%20find%20the%20top%20matching%20videos%0Agiven%20text%20descriptions%20from%20a%20vast%20video%20corpus%2C%20is%20an%20essential%20function%2C%20the%0Aprimary%20challenge%20of%20which%20is%20to%20bridge%20the%20modality%20gap.%20Nevertheless%2C%20most%0Aexisting%20approaches%20treat%20texts%20merely%20as%20discrete%20tokens%20and%20neglect%20their%0Asyntax%20structures.%20Moreover%2C%20the%20abundant%20spatial%20and%20temporal%20clues%20in%20videos%0Aare%20often%20underutilized%20due%20to%20the%20lack%20of%20interaction%20with%20text.%20To%20address%0Athese%20issues%2C%20we%20argue%20that%20using%20texts%20as%20guidance%20to%20focus%20on%20relevant%0Atemporal%20frames%20and%20spatial%20regions%20within%20videos%20is%20beneficial.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20Syntax-Hierarchy-Enhanced%20text-video%20retrieval%20method%0A%28SHE-Net%29%20that%20exploits%20the%20inherent%20semantic%20and%20syntax%20hierarchy%20of%20texts%20to%0Abridge%20the%20modality%20gap%20from%20two%20perspectives.%20First%2C%20to%20facilitate%20a%20more%0Afine-grained%20integration%20of%20visual%20content%2C%20we%20employ%20the%20text%20syntax%0Ahierarchy%2C%20which%20reveals%20the%20grammatical%20structure%20of%20text%20descriptions%2C%20to%0Aguide%20the%20visual%20representations.%20Second%2C%20to%20further%20enhance%20the%20multi-modal%0Ainteraction%20and%20alignment%2C%20we%20also%20utilize%20the%20syntax%20hierarchy%20to%20guide%20the%0Asimilarity%20calculation.%20We%20evaluated%20our%20method%20on%20four%20public%20text-video%0Aretrieval%20datasets%20of%20MSR-VTT%2C%20MSVD%2C%20DiDeMo%2C%20and%20ActivityNet.%20The%20experimental%0Aresults%20and%20ablation%20studies%20confirm%20the%20advantages%20of%20our%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14066v2&entry.124074799=Read"},
{"title": "Collage: Light-Weight Low-Precision Strategy for LLM Training", "author": "Tao Yu and Gaurav Gupta and Karthick Gopalswamy and Amith Mamidala and Hao Zhou and Jeffrey Huynh and Youngsuk Park and Ron Diamant and Anoop Deoras and Luke Huan", "abstract": "  Large models training is plagued by the intense compute cost and limited\nhardware memory. A practical solution is low-precision representation but is\ntroubled by loss in numerical accuracy and unstable training rendering the\nmodel less useful. We argue that low-precision floating points can perform well\nprovided the error is properly compensated at the critical locations in the\ntraining process. We propose Collage which utilizes multi-component float\nrepresentation in low-precision to accurately perform operations with numerical\nerrors accounted. To understand the impact of imprecision to training, we\npropose a simple and novel metric which tracks the lost information during\ntraining as well as differentiates various precision strategies. Our method\nworks with commonly used low-precision such as half-precision ($16$-bit\nfloating points) and can be naturally extended to work with even lower\nprecision such as $8$-bit. Experimental results show that pre-training using\nCollage removes the requirement of using $32$-bit floating-point copies of the\nmodel and attains similar/better training performance compared to $(16,\n32)$-bit mixed-precision strategy, with up to $3.7\\times$ speedup and $\\sim\n15\\%$ to $23\\%$ less memory usage in practice.\n", "link": "http://arxiv.org/abs/2405.03637v1", "date": "2024-05-06", "relevancy": 1.9662, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4992}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4937}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collage%3A%20Light-Weight%20Low-Precision%20Strategy%20for%20LLM%20Training&body=Title%3A%20Collage%3A%20Light-Weight%20Low-Precision%20Strategy%20for%20LLM%20Training%0AAuthor%3A%20Tao%20Yu%20and%20Gaurav%20Gupta%20and%20Karthick%20Gopalswamy%20and%20Amith%20Mamidala%20and%20Hao%20Zhou%20and%20Jeffrey%20Huynh%20and%20Youngsuk%20Park%20and%20Ron%20Diamant%20and%20Anoop%20Deoras%20and%20Luke%20Huan%0AAbstract%3A%20%20%20Large%20models%20training%20is%20plagued%20by%20the%20intense%20compute%20cost%20and%20limited%0Ahardware%20memory.%20A%20practical%20solution%20is%20low-precision%20representation%20but%20is%0Atroubled%20by%20loss%20in%20numerical%20accuracy%20and%20unstable%20training%20rendering%20the%0Amodel%20less%20useful.%20We%20argue%20that%20low-precision%20floating%20points%20can%20perform%20well%0Aprovided%20the%20error%20is%20properly%20compensated%20at%20the%20critical%20locations%20in%20the%0Atraining%20process.%20We%20propose%20Collage%20which%20utilizes%20multi-component%20float%0Arepresentation%20in%20low-precision%20to%20accurately%20perform%20operations%20with%20numerical%0Aerrors%20accounted.%20To%20understand%20the%20impact%20of%20imprecision%20to%20training%2C%20we%0Apropose%20a%20simple%20and%20novel%20metric%20which%20tracks%20the%20lost%20information%20during%0Atraining%20as%20well%20as%20differentiates%20various%20precision%20strategies.%20Our%20method%0Aworks%20with%20commonly%20used%20low-precision%20such%20as%20half-precision%20%28%2416%24-bit%0Afloating%20points%29%20and%20can%20be%20naturally%20extended%20to%20work%20with%20even%20lower%0Aprecision%20such%20as%20%248%24-bit.%20Experimental%20results%20show%20that%20pre-training%20using%0ACollage%20removes%20the%20requirement%20of%20using%20%2432%24-bit%20floating-point%20copies%20of%20the%0Amodel%20and%20attains%20similar/better%20training%20performance%20compared%20to%20%24%2816%2C%0A32%29%24-bit%20mixed-precision%20strategy%2C%20with%20up%20to%20%243.7%5Ctimes%24%20speedup%20and%20%24%5Csim%0A15%5C%25%24%20to%20%2423%5C%25%24%20less%20memory%20usage%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollage%253A%2520Light-Weight%2520Low-Precision%2520Strategy%2520for%2520LLM%2520Training%26entry.906535625%3DTao%2520Yu%2520and%2520Gaurav%2520Gupta%2520and%2520Karthick%2520Gopalswamy%2520and%2520Amith%2520Mamidala%2520and%2520Hao%2520Zhou%2520and%2520Jeffrey%2520Huynh%2520and%2520Youngsuk%2520Park%2520and%2520Ron%2520Diamant%2520and%2520Anoop%2520Deoras%2520and%2520Luke%2520Huan%26entry.1292438233%3D%2520%2520Large%2520models%2520training%2520is%2520plagued%2520by%2520the%2520intense%2520compute%2520cost%2520and%2520limited%250Ahardware%2520memory.%2520A%2520practical%2520solution%2520is%2520low-precision%2520representation%2520but%2520is%250Atroubled%2520by%2520loss%2520in%2520numerical%2520accuracy%2520and%2520unstable%2520training%2520rendering%2520the%250Amodel%2520less%2520useful.%2520We%2520argue%2520that%2520low-precision%2520floating%2520points%2520can%2520perform%2520well%250Aprovided%2520the%2520error%2520is%2520properly%2520compensated%2520at%2520the%2520critical%2520locations%2520in%2520the%250Atraining%2520process.%2520We%2520propose%2520Collage%2520which%2520utilizes%2520multi-component%2520float%250Arepresentation%2520in%2520low-precision%2520to%2520accurately%2520perform%2520operations%2520with%2520numerical%250Aerrors%2520accounted.%2520To%2520understand%2520the%2520impact%2520of%2520imprecision%2520to%2520training%252C%2520we%250Apropose%2520a%2520simple%2520and%2520novel%2520metric%2520which%2520tracks%2520the%2520lost%2520information%2520during%250Atraining%2520as%2520well%2520as%2520differentiates%2520various%2520precision%2520strategies.%2520Our%2520method%250Aworks%2520with%2520commonly%2520used%2520low-precision%2520such%2520as%2520half-precision%2520%2528%252416%2524-bit%250Afloating%2520points%2529%2520and%2520can%2520be%2520naturally%2520extended%2520to%2520work%2520with%2520even%2520lower%250Aprecision%2520such%2520as%2520%25248%2524-bit.%2520Experimental%2520results%2520show%2520that%2520pre-training%2520using%250ACollage%2520removes%2520the%2520requirement%2520of%2520using%2520%252432%2524-bit%2520floating-point%2520copies%2520of%2520the%250Amodel%2520and%2520attains%2520similar/better%2520training%2520performance%2520compared%2520to%2520%2524%252816%252C%250A32%2529%2524-bit%2520mixed-precision%2520strategy%252C%2520with%2520up%2520to%2520%25243.7%255Ctimes%2524%2520speedup%2520and%2520%2524%255Csim%250A15%255C%2525%2524%2520to%2520%252423%255C%2525%2524%2520less%2520memory%2520usage%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collage%3A%20Light-Weight%20Low-Precision%20Strategy%20for%20LLM%20Training&entry.906535625=Tao%20Yu%20and%20Gaurav%20Gupta%20and%20Karthick%20Gopalswamy%20and%20Amith%20Mamidala%20and%20Hao%20Zhou%20and%20Jeffrey%20Huynh%20and%20Youngsuk%20Park%20and%20Ron%20Diamant%20and%20Anoop%20Deoras%20and%20Luke%20Huan&entry.1292438233=%20%20Large%20models%20training%20is%20plagued%20by%20the%20intense%20compute%20cost%20and%20limited%0Ahardware%20memory.%20A%20practical%20solution%20is%20low-precision%20representation%20but%20is%0Atroubled%20by%20loss%20in%20numerical%20accuracy%20and%20unstable%20training%20rendering%20the%0Amodel%20less%20useful.%20We%20argue%20that%20low-precision%20floating%20points%20can%20perform%20well%0Aprovided%20the%20error%20is%20properly%20compensated%20at%20the%20critical%20locations%20in%20the%0Atraining%20process.%20We%20propose%20Collage%20which%20utilizes%20multi-component%20float%0Arepresentation%20in%20low-precision%20to%20accurately%20perform%20operations%20with%20numerical%0Aerrors%20accounted.%20To%20understand%20the%20impact%20of%20imprecision%20to%20training%2C%20we%0Apropose%20a%20simple%20and%20novel%20metric%20which%20tracks%20the%20lost%20information%20during%0Atraining%20as%20well%20as%20differentiates%20various%20precision%20strategies.%20Our%20method%0Aworks%20with%20commonly%20used%20low-precision%20such%20as%20half-precision%20%28%2416%24-bit%0Afloating%20points%29%20and%20can%20be%20naturally%20extended%20to%20work%20with%20even%20lower%0Aprecision%20such%20as%20%248%24-bit.%20Experimental%20results%20show%20that%20pre-training%20using%0ACollage%20removes%20the%20requirement%20of%20using%20%2432%24-bit%20floating-point%20copies%20of%20the%0Amodel%20and%20attains%20similar/better%20training%20performance%20compared%20to%20%24%2816%2C%0A32%29%24-bit%20mixed-precision%20strategy%2C%20with%20up%20to%20%243.7%5Ctimes%24%20speedup%20and%20%24%5Csim%0A15%5C%25%24%20to%20%2423%5C%25%24%20less%20memory%20usage%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03637v1&entry.124074799=Read"},
{"title": "Doubly Robust Causal Effect Estimation under Networked Interference via\n  Targeted Learning", "author": "Weilin Chen and Ruichu Cai and Zeqin Yang and Jie Qiao and Yuguang Yan and Zijian Li and Zhifeng Hao", "abstract": "  Causal effect estimation under networked interference is an important but\nchallenging problem. Available parametric methods are limited in their model\nspace, while previous semiparametric methods, e.g., leveraging neural networks\nto fit only one single nuisance function, may still encounter misspecification\nproblems under networked interference without appropriate assumptions on the\ndata generation process. To mitigate bias stemming from misspecification, we\npropose a novel doubly robust causal effect estimator under networked\ninterference, by adapting the targeted learning technique to the training of\nneural networks. Specifically, we generalize the targeted learning technique\ninto the networked interference setting and establish the condition under which\nan estimator achieves double robustness. Based on the condition, we devise an\nend-to-end causal effect estimator by transforming the identified theoretical\ncondition into a targeted loss. Moreover, we provide a theoretical analysis of\nour designed estimator, revealing a faster convergence rate compared to a\nsingle nuisance model. Extensive experimental results on two real-world\nnetworks with semisynthetic data demonstrate the effectiveness of our proposed\nestimators.\n", "link": "http://arxiv.org/abs/2405.03342v1", "date": "2024-05-06", "relevancy": 1.9599, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5245}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4893}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Doubly%20Robust%20Causal%20Effect%20Estimation%20under%20Networked%20Interference%20via%0A%20%20Targeted%20Learning&body=Title%3A%20Doubly%20Robust%20Causal%20Effect%20Estimation%20under%20Networked%20Interference%20via%0A%20%20Targeted%20Learning%0AAuthor%3A%20Weilin%20Chen%20and%20Ruichu%20Cai%20and%20Zeqin%20Yang%20and%20Jie%20Qiao%20and%20Yuguang%20Yan%20and%20Zijian%20Li%20and%20Zhifeng%20Hao%0AAbstract%3A%20%20%20Causal%20effect%20estimation%20under%20networked%20interference%20is%20an%20important%20but%0Achallenging%20problem.%20Available%20parametric%20methods%20are%20limited%20in%20their%20model%0Aspace%2C%20while%20previous%20semiparametric%20methods%2C%20e.g.%2C%20leveraging%20neural%20networks%0Ato%20fit%20only%20one%20single%20nuisance%20function%2C%20may%20still%20encounter%20misspecification%0Aproblems%20under%20networked%20interference%20without%20appropriate%20assumptions%20on%20the%0Adata%20generation%20process.%20To%20mitigate%20bias%20stemming%20from%20misspecification%2C%20we%0Apropose%20a%20novel%20doubly%20robust%20causal%20effect%20estimator%20under%20networked%0Ainterference%2C%20by%20adapting%20the%20targeted%20learning%20technique%20to%20the%20training%20of%0Aneural%20networks.%20Specifically%2C%20we%20generalize%20the%20targeted%20learning%20technique%0Ainto%20the%20networked%20interference%20setting%20and%20establish%20the%20condition%20under%20which%0Aan%20estimator%20achieves%20double%20robustness.%20Based%20on%20the%20condition%2C%20we%20devise%20an%0Aend-to-end%20causal%20effect%20estimator%20by%20transforming%20the%20identified%20theoretical%0Acondition%20into%20a%20targeted%20loss.%20Moreover%2C%20we%20provide%20a%20theoretical%20analysis%20of%0Aour%20designed%20estimator%2C%20revealing%20a%20faster%20convergence%20rate%20compared%20to%20a%0Asingle%20nuisance%20model.%20Extensive%20experimental%20results%20on%20two%20real-world%0Anetworks%20with%20semisynthetic%20data%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Aestimators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoubly%2520Robust%2520Causal%2520Effect%2520Estimation%2520under%2520Networked%2520Interference%2520via%250A%2520%2520Targeted%2520Learning%26entry.906535625%3DWeilin%2520Chen%2520and%2520Ruichu%2520Cai%2520and%2520Zeqin%2520Yang%2520and%2520Jie%2520Qiao%2520and%2520Yuguang%2520Yan%2520and%2520Zijian%2520Li%2520and%2520Zhifeng%2520Hao%26entry.1292438233%3D%2520%2520Causal%2520effect%2520estimation%2520under%2520networked%2520interference%2520is%2520an%2520important%2520but%250Achallenging%2520problem.%2520Available%2520parametric%2520methods%2520are%2520limited%2520in%2520their%2520model%250Aspace%252C%2520while%2520previous%2520semiparametric%2520methods%252C%2520e.g.%252C%2520leveraging%2520neural%2520networks%250Ato%2520fit%2520only%2520one%2520single%2520nuisance%2520function%252C%2520may%2520still%2520encounter%2520misspecification%250Aproblems%2520under%2520networked%2520interference%2520without%2520appropriate%2520assumptions%2520on%2520the%250Adata%2520generation%2520process.%2520To%2520mitigate%2520bias%2520stemming%2520from%2520misspecification%252C%2520we%250Apropose%2520a%2520novel%2520doubly%2520robust%2520causal%2520effect%2520estimator%2520under%2520networked%250Ainterference%252C%2520by%2520adapting%2520the%2520targeted%2520learning%2520technique%2520to%2520the%2520training%2520of%250Aneural%2520networks.%2520Specifically%252C%2520we%2520generalize%2520the%2520targeted%2520learning%2520technique%250Ainto%2520the%2520networked%2520interference%2520setting%2520and%2520establish%2520the%2520condition%2520under%2520which%250Aan%2520estimator%2520achieves%2520double%2520robustness.%2520Based%2520on%2520the%2520condition%252C%2520we%2520devise%2520an%250Aend-to-end%2520causal%2520effect%2520estimator%2520by%2520transforming%2520the%2520identified%2520theoretical%250Acondition%2520into%2520a%2520targeted%2520loss.%2520Moreover%252C%2520we%2520provide%2520a%2520theoretical%2520analysis%2520of%250Aour%2520designed%2520estimator%252C%2520revealing%2520a%2520faster%2520convergence%2520rate%2520compared%2520to%2520a%250Asingle%2520nuisance%2520model.%2520Extensive%2520experimental%2520results%2520on%2520two%2520real-world%250Anetworks%2520with%2520semisynthetic%2520data%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%250Aestimators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Doubly%20Robust%20Causal%20Effect%20Estimation%20under%20Networked%20Interference%20via%0A%20%20Targeted%20Learning&entry.906535625=Weilin%20Chen%20and%20Ruichu%20Cai%20and%20Zeqin%20Yang%20and%20Jie%20Qiao%20and%20Yuguang%20Yan%20and%20Zijian%20Li%20and%20Zhifeng%20Hao&entry.1292438233=%20%20Causal%20effect%20estimation%20under%20networked%20interference%20is%20an%20important%20but%0Achallenging%20problem.%20Available%20parametric%20methods%20are%20limited%20in%20their%20model%0Aspace%2C%20while%20previous%20semiparametric%20methods%2C%20e.g.%2C%20leveraging%20neural%20networks%0Ato%20fit%20only%20one%20single%20nuisance%20function%2C%20may%20still%20encounter%20misspecification%0Aproblems%20under%20networked%20interference%20without%20appropriate%20assumptions%20on%20the%0Adata%20generation%20process.%20To%20mitigate%20bias%20stemming%20from%20misspecification%2C%20we%0Apropose%20a%20novel%20doubly%20robust%20causal%20effect%20estimator%20under%20networked%0Ainterference%2C%20by%20adapting%20the%20targeted%20learning%20technique%20to%20the%20training%20of%0Aneural%20networks.%20Specifically%2C%20we%20generalize%20the%20targeted%20learning%20technique%0Ainto%20the%20networked%20interference%20setting%20and%20establish%20the%20condition%20under%20which%0Aan%20estimator%20achieves%20double%20robustness.%20Based%20on%20the%20condition%2C%20we%20devise%20an%0Aend-to-end%20causal%20effect%20estimator%20by%20transforming%20the%20identified%20theoretical%0Acondition%20into%20a%20targeted%20loss.%20Moreover%2C%20we%20provide%20a%20theoretical%20analysis%20of%0Aour%20designed%20estimator%2C%20revealing%20a%20faster%20convergence%20rate%20compared%20to%20a%0Asingle%20nuisance%20model.%20Extensive%20experimental%20results%20on%20two%20real-world%0Anetworks%20with%20semisynthetic%20data%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Aestimators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03342v1&entry.124074799=Read"},
{"title": "From Molecules to Materials: Pre-training Large Generalizable Models for\n  Atomic Property Prediction", "author": "Nima Shoghi and Adeesh Kolluru and John R. Kitchin and Zachary W. Ulissi and C. Lawrence Zitnick and Brandon M. Wood", "abstract": "  Foundation models have been transformational in machine learning fields such\nas natural language processing and computer vision. Similar success in atomic\nproperty prediction has been limited due to the challenges of training\neffective models across multiple chemical domains. To address this, we\nintroduce Joint Multi-domain Pre-training (JMP), a supervised pre-training\nstrategy that simultaneously trains on multiple datasets from different\nchemical domains, treating each dataset as a unique pre-training task within a\nmulti-task framework. Our combined training dataset consists of $\\sim$120M\nsystems from OC20, OC22, ANI-1x, and Transition-1x. We evaluate performance and\ngeneralization by fine-tuning over a diverse set of downstream tasks and\ndatasets including: QM9, rMD17, MatBench, QMOF, SPICE, and MD22. JMP\ndemonstrates an average improvement of 59% over training from scratch, and\nmatches or sets state-of-the-art on 34 out of 40 tasks. Our work highlights the\npotential of pre-training strategies that utilize diverse data to advance\nproperty prediction across chemical domains, especially for low-data tasks.\nPlease visit https://nima.sh/jmp for further information.\n", "link": "http://arxiv.org/abs/2310.16802v2", "date": "2024-05-06", "relevancy": 1.9589, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5616}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4851}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Molecules%20to%20Materials%3A%20Pre-training%20Large%20Generalizable%20Models%20for%0A%20%20Atomic%20Property%20Prediction&body=Title%3A%20From%20Molecules%20to%20Materials%3A%20Pre-training%20Large%20Generalizable%20Models%20for%0A%20%20Atomic%20Property%20Prediction%0AAuthor%3A%20Nima%20Shoghi%20and%20Adeesh%20Kolluru%20and%20John%20R.%20Kitchin%20and%20Zachary%20W.%20Ulissi%20and%20C.%20Lawrence%20Zitnick%20and%20Brandon%20M.%20Wood%0AAbstract%3A%20%20%20Foundation%20models%20have%20been%20transformational%20in%20machine%20learning%20fields%20such%0Aas%20natural%20language%20processing%20and%20computer%20vision.%20Similar%20success%20in%20atomic%0Aproperty%20prediction%20has%20been%20limited%20due%20to%20the%20challenges%20of%20training%0Aeffective%20models%20across%20multiple%20chemical%20domains.%20To%20address%20this%2C%20we%0Aintroduce%20Joint%20Multi-domain%20Pre-training%20%28JMP%29%2C%20a%20supervised%20pre-training%0Astrategy%20that%20simultaneously%20trains%20on%20multiple%20datasets%20from%20different%0Achemical%20domains%2C%20treating%20each%20dataset%20as%20a%20unique%20pre-training%20task%20within%20a%0Amulti-task%20framework.%20Our%20combined%20training%20dataset%20consists%20of%20%24%5Csim%24120M%0Asystems%20from%20OC20%2C%20OC22%2C%20ANI-1x%2C%20and%20Transition-1x.%20We%20evaluate%20performance%20and%0Ageneralization%20by%20fine-tuning%20over%20a%20diverse%20set%20of%20downstream%20tasks%20and%0Adatasets%20including%3A%20QM9%2C%20rMD17%2C%20MatBench%2C%20QMOF%2C%20SPICE%2C%20and%20MD22.%20JMP%0Ademonstrates%20an%20average%20improvement%20of%2059%25%20over%20training%20from%20scratch%2C%20and%0Amatches%20or%20sets%20state-of-the-art%20on%2034%20out%20of%2040%20tasks.%20Our%20work%20highlights%20the%0Apotential%20of%20pre-training%20strategies%20that%20utilize%20diverse%20data%20to%20advance%0Aproperty%20prediction%20across%20chemical%20domains%2C%20especially%20for%20low-data%20tasks.%0APlease%20visit%20https%3A//nima.sh/jmp%20for%20further%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.16802v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Molecules%2520to%2520Materials%253A%2520Pre-training%2520Large%2520Generalizable%2520Models%2520for%250A%2520%2520Atomic%2520Property%2520Prediction%26entry.906535625%3DNima%2520Shoghi%2520and%2520Adeesh%2520Kolluru%2520and%2520John%2520R.%2520Kitchin%2520and%2520Zachary%2520W.%2520Ulissi%2520and%2520C.%2520Lawrence%2520Zitnick%2520and%2520Brandon%2520M.%2520Wood%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520been%2520transformational%2520in%2520machine%2520learning%2520fields%2520such%250Aas%2520natural%2520language%2520processing%2520and%2520computer%2520vision.%2520Similar%2520success%2520in%2520atomic%250Aproperty%2520prediction%2520has%2520been%2520limited%2520due%2520to%2520the%2520challenges%2520of%2520training%250Aeffective%2520models%2520across%2520multiple%2520chemical%2520domains.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520Joint%2520Multi-domain%2520Pre-training%2520%2528JMP%2529%252C%2520a%2520supervised%2520pre-training%250Astrategy%2520that%2520simultaneously%2520trains%2520on%2520multiple%2520datasets%2520from%2520different%250Achemical%2520domains%252C%2520treating%2520each%2520dataset%2520as%2520a%2520unique%2520pre-training%2520task%2520within%2520a%250Amulti-task%2520framework.%2520Our%2520combined%2520training%2520dataset%2520consists%2520of%2520%2524%255Csim%2524120M%250Asystems%2520from%2520OC20%252C%2520OC22%252C%2520ANI-1x%252C%2520and%2520Transition-1x.%2520We%2520evaluate%2520performance%2520and%250Ageneralization%2520by%2520fine-tuning%2520over%2520a%2520diverse%2520set%2520of%2520downstream%2520tasks%2520and%250Adatasets%2520including%253A%2520QM9%252C%2520rMD17%252C%2520MatBench%252C%2520QMOF%252C%2520SPICE%252C%2520and%2520MD22.%2520JMP%250Ademonstrates%2520an%2520average%2520improvement%2520of%252059%2525%2520over%2520training%2520from%2520scratch%252C%2520and%250Amatches%2520or%2520sets%2520state-of-the-art%2520on%252034%2520out%2520of%252040%2520tasks.%2520Our%2520work%2520highlights%2520the%250Apotential%2520of%2520pre-training%2520strategies%2520that%2520utilize%2520diverse%2520data%2520to%2520advance%250Aproperty%2520prediction%2520across%2520chemical%2520domains%252C%2520especially%2520for%2520low-data%2520tasks.%250APlease%2520visit%2520https%253A//nima.sh/jmp%2520for%2520further%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.16802v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Molecules%20to%20Materials%3A%20Pre-training%20Large%20Generalizable%20Models%20for%0A%20%20Atomic%20Property%20Prediction&entry.906535625=Nima%20Shoghi%20and%20Adeesh%20Kolluru%20and%20John%20R.%20Kitchin%20and%20Zachary%20W.%20Ulissi%20and%20C.%20Lawrence%20Zitnick%20and%20Brandon%20M.%20Wood&entry.1292438233=%20%20Foundation%20models%20have%20been%20transformational%20in%20machine%20learning%20fields%20such%0Aas%20natural%20language%20processing%20and%20computer%20vision.%20Similar%20success%20in%20atomic%0Aproperty%20prediction%20has%20been%20limited%20due%20to%20the%20challenges%20of%20training%0Aeffective%20models%20across%20multiple%20chemical%20domains.%20To%20address%20this%2C%20we%0Aintroduce%20Joint%20Multi-domain%20Pre-training%20%28JMP%29%2C%20a%20supervised%20pre-training%0Astrategy%20that%20simultaneously%20trains%20on%20multiple%20datasets%20from%20different%0Achemical%20domains%2C%20treating%20each%20dataset%20as%20a%20unique%20pre-training%20task%20within%20a%0Amulti-task%20framework.%20Our%20combined%20training%20dataset%20consists%20of%20%24%5Csim%24120M%0Asystems%20from%20OC20%2C%20OC22%2C%20ANI-1x%2C%20and%20Transition-1x.%20We%20evaluate%20performance%20and%0Ageneralization%20by%20fine-tuning%20over%20a%20diverse%20set%20of%20downstream%20tasks%20and%0Adatasets%20including%3A%20QM9%2C%20rMD17%2C%20MatBench%2C%20QMOF%2C%20SPICE%2C%20and%20MD22.%20JMP%0Ademonstrates%20an%20average%20improvement%20of%2059%25%20over%20training%20from%20scratch%2C%20and%0Amatches%20or%20sets%20state-of-the-art%20on%2034%20out%20of%2040%20tasks.%20Our%20work%20highlights%20the%0Apotential%20of%20pre-training%20strategies%20that%20utilize%20diverse%20data%20to%20advance%0Aproperty%20prediction%20across%20chemical%20domains%2C%20especially%20for%20low-data%20tasks.%0APlease%20visit%20https%3A//nima.sh/jmp%20for%20further%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.16802v2&entry.124074799=Read"},
{"title": "DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D\n  Generation", "author": "Yukun Huang and Jianan Wang and Yukai Shi and Boshi Tang and Xianbiao Qi and Lei Zhang", "abstract": "  Text-to-image diffusion models pre-trained on billions of image-text pairs\nhave recently enabled 3D content creation by optimizing a randomly initialized\ndifferentiable 3D representation with score distillation. However, the\noptimization process suffers slow convergence and the resultant 3D models often\nexhibit two limitations: (a) quality concerns such as missing attributes and\ndistorted shape and texture; (b) extremely low diversity comparing to\ntext-guided image synthesis. In this paper, we show that the conflict between\nthe 3D optimization process and uniform timestep sampling in score distillation\nis the main reason for these limitations. To resolve this conflict, we propose\nto prioritize timestep sampling with monotonically non-increasing functions,\nwhich aligns the 3D optimization process with the sampling process of diffusion\nmodel. Extensive experiments show that our simple redesign significantly\nimproves 3D content creation with faster convergence, better quality and\ndiversity.\n", "link": "http://arxiv.org/abs/2306.12422v2", "date": "2024-05-06", "relevancy": 1.9447, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.657}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6473}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamTime%3A%20An%20Improved%20Optimization%20Strategy%20for%20Diffusion-Guided%203D%0A%20%20Generation&body=Title%3A%20DreamTime%3A%20An%20Improved%20Optimization%20Strategy%20for%20Diffusion-Guided%203D%0A%20%20Generation%0AAuthor%3A%20Yukun%20Huang%20and%20Jianan%20Wang%20and%20Yukai%20Shi%20and%20Boshi%20Tang%20and%20Xianbiao%20Qi%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20pre-trained%20on%20billions%20of%20image-text%20pairs%0Ahave%20recently%20enabled%203D%20content%20creation%20by%20optimizing%20a%20randomly%20initialized%0Adifferentiable%203D%20representation%20with%20score%20distillation.%20However%2C%20the%0Aoptimization%20process%20suffers%20slow%20convergence%20and%20the%20resultant%203D%20models%20often%0Aexhibit%20two%20limitations%3A%20%28a%29%20quality%20concerns%20such%20as%20missing%20attributes%20and%0Adistorted%20shape%20and%20texture%3B%20%28b%29%20extremely%20low%20diversity%20comparing%20to%0Atext-guided%20image%20synthesis.%20In%20this%20paper%2C%20we%20show%20that%20the%20conflict%20between%0Athe%203D%20optimization%20process%20and%20uniform%20timestep%20sampling%20in%20score%20distillation%0Ais%20the%20main%20reason%20for%20these%20limitations.%20To%20resolve%20this%20conflict%2C%20we%20propose%0Ato%20prioritize%20timestep%20sampling%20with%20monotonically%20non-increasing%20functions%2C%0Awhich%20aligns%20the%203D%20optimization%20process%20with%20the%20sampling%20process%20of%20diffusion%0Amodel.%20Extensive%20experiments%20show%20that%20our%20simple%20redesign%20significantly%0Aimproves%203D%20content%20creation%20with%20faster%20convergence%2C%20better%20quality%20and%0Adiversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.12422v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamTime%253A%2520An%2520Improved%2520Optimization%2520Strategy%2520for%2520Diffusion-Guided%25203D%250A%2520%2520Generation%26entry.906535625%3DYukun%2520Huang%2520and%2520Jianan%2520Wang%2520and%2520Yukai%2520Shi%2520and%2520Boshi%2520Tang%2520and%2520Xianbiao%2520Qi%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520models%2520pre-trained%2520on%2520billions%2520of%2520image-text%2520pairs%250Ahave%2520recently%2520enabled%25203D%2520content%2520creation%2520by%2520optimizing%2520a%2520randomly%2520initialized%250Adifferentiable%25203D%2520representation%2520with%2520score%2520distillation.%2520However%252C%2520the%250Aoptimization%2520process%2520suffers%2520slow%2520convergence%2520and%2520the%2520resultant%25203D%2520models%2520often%250Aexhibit%2520two%2520limitations%253A%2520%2528a%2529%2520quality%2520concerns%2520such%2520as%2520missing%2520attributes%2520and%250Adistorted%2520shape%2520and%2520texture%253B%2520%2528b%2529%2520extremely%2520low%2520diversity%2520comparing%2520to%250Atext-guided%2520image%2520synthesis.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520the%2520conflict%2520between%250Athe%25203D%2520optimization%2520process%2520and%2520uniform%2520timestep%2520sampling%2520in%2520score%2520distillation%250Ais%2520the%2520main%2520reason%2520for%2520these%2520limitations.%2520To%2520resolve%2520this%2520conflict%252C%2520we%2520propose%250Ato%2520prioritize%2520timestep%2520sampling%2520with%2520monotonically%2520non-increasing%2520functions%252C%250Awhich%2520aligns%2520the%25203D%2520optimization%2520process%2520with%2520the%2520sampling%2520process%2520of%2520diffusion%250Amodel.%2520Extensive%2520experiments%2520show%2520that%2520our%2520simple%2520redesign%2520significantly%250Aimproves%25203D%2520content%2520creation%2520with%2520faster%2520convergence%252C%2520better%2520quality%2520and%250Adiversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.12422v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamTime%3A%20An%20Improved%20Optimization%20Strategy%20for%20Diffusion-Guided%203D%0A%20%20Generation&entry.906535625=Yukun%20Huang%20and%20Jianan%20Wang%20and%20Yukai%20Shi%20and%20Boshi%20Tang%20and%20Xianbiao%20Qi%20and%20Lei%20Zhang&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20pre-trained%20on%20billions%20of%20image-text%20pairs%0Ahave%20recently%20enabled%203D%20content%20creation%20by%20optimizing%20a%20randomly%20initialized%0Adifferentiable%203D%20representation%20with%20score%20distillation.%20However%2C%20the%0Aoptimization%20process%20suffers%20slow%20convergence%20and%20the%20resultant%203D%20models%20often%0Aexhibit%20two%20limitations%3A%20%28a%29%20quality%20concerns%20such%20as%20missing%20attributes%20and%0Adistorted%20shape%20and%20texture%3B%20%28b%29%20extremely%20low%20diversity%20comparing%20to%0Atext-guided%20image%20synthesis.%20In%20this%20paper%2C%20we%20show%20that%20the%20conflict%20between%0Athe%203D%20optimization%20process%20and%20uniform%20timestep%20sampling%20in%20score%20distillation%0Ais%20the%20main%20reason%20for%20these%20limitations.%20To%20resolve%20this%20conflict%2C%20we%20propose%0Ato%20prioritize%20timestep%20sampling%20with%20monotonically%20non-increasing%20functions%2C%0Awhich%20aligns%20the%203D%20optimization%20process%20with%20the%20sampling%20process%20of%20diffusion%0Amodel.%20Extensive%20experiments%20show%20that%20our%20simple%20redesign%20significantly%0Aimproves%203D%20content%20creation%20with%20faster%20convergence%2C%20better%20quality%20and%0Adiversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.12422v2&entry.124074799=Read"},
{"title": "Automated Metaheuristic Algorithm Design with Autoregressive Learning", "author": "Qi Zhao and Tengfei Liu and Bai Yan and Qiqi Duan and Jian Yang and Yuhui Shi", "abstract": "  Automated design of metaheuristic algorithms offers an attractive avenue to\nreduce human effort and gain enhanced performance beyond human intuition.\nCurrent automated methods design algorithms within a fixed structure and\noperate from scratch. This poses a clear gap towards fully discovering\npotentials over the metaheuristic family and fertilizing from prior design\nexperience. To bridge the gap, this paper proposes an autoregressive\nlearning-based designer for automated design of metaheuristic algorithms. Our\ndesigner formulates metaheuristic algorithm design as a sequence generation\ntask, and harnesses an autoregressive generative network to handle the task.\nThis offers two advances. First, through autoregressive inference, the designer\ngenerates algorithms with diverse lengths and structures, enabling to fully\ndiscover potentials over the metaheuristic family. Second, prior design\nknowledge learned and accumulated in neurons of the designer can be retrieved\nfor designing algorithms for future problems, paving the way to continual\ndesign of algorithms for open-ended problem-solving. Extensive experiments on\nnumeral benchmarks and real-world problems reveal that the proposed designer\ngenerates algorithms that outperform all human-created baselines on 24 out of\n25 test problems. The generated algorithms display various structures and\nbehaviors, reasonably fitting for different problem-solving contexts. Code will\nbe released after paper publication.\n", "link": "http://arxiv.org/abs/2405.03419v1", "date": "2024-05-06", "relevancy": 1.9412, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4887}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4871}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Metaheuristic%20Algorithm%20Design%20with%20Autoregressive%20Learning&body=Title%3A%20Automated%20Metaheuristic%20Algorithm%20Design%20with%20Autoregressive%20Learning%0AAuthor%3A%20Qi%20Zhao%20and%20Tengfei%20Liu%20and%20Bai%20Yan%20and%20Qiqi%20Duan%20and%20Jian%20Yang%20and%20Yuhui%20Shi%0AAbstract%3A%20%20%20Automated%20design%20of%20metaheuristic%20algorithms%20offers%20an%20attractive%20avenue%20to%0Areduce%20human%20effort%20and%20gain%20enhanced%20performance%20beyond%20human%20intuition.%0ACurrent%20automated%20methods%20design%20algorithms%20within%20a%20fixed%20structure%20and%0Aoperate%20from%20scratch.%20This%20poses%20a%20clear%20gap%20towards%20fully%20discovering%0Apotentials%20over%20the%20metaheuristic%20family%20and%20fertilizing%20from%20prior%20design%0Aexperience.%20To%20bridge%20the%20gap%2C%20this%20paper%20proposes%20an%20autoregressive%0Alearning-based%20designer%20for%20automated%20design%20of%20metaheuristic%20algorithms.%20Our%0Adesigner%20formulates%20metaheuristic%20algorithm%20design%20as%20a%20sequence%20generation%0Atask%2C%20and%20harnesses%20an%20autoregressive%20generative%20network%20to%20handle%20the%20task.%0AThis%20offers%20two%20advances.%20First%2C%20through%20autoregressive%20inference%2C%20the%20designer%0Agenerates%20algorithms%20with%20diverse%20lengths%20and%20structures%2C%20enabling%20to%20fully%0Adiscover%20potentials%20over%20the%20metaheuristic%20family.%20Second%2C%20prior%20design%0Aknowledge%20learned%20and%20accumulated%20in%20neurons%20of%20the%20designer%20can%20be%20retrieved%0Afor%20designing%20algorithms%20for%20future%20problems%2C%20paving%20the%20way%20to%20continual%0Adesign%20of%20algorithms%20for%20open-ended%20problem-solving.%20Extensive%20experiments%20on%0Anumeral%20benchmarks%20and%20real-world%20problems%20reveal%20that%20the%20proposed%20designer%0Agenerates%20algorithms%20that%20outperform%20all%20human-created%20baselines%20on%2024%20out%20of%0A25%20test%20problems.%20The%20generated%20algorithms%20display%20various%20structures%20and%0Abehaviors%2C%20reasonably%20fitting%20for%20different%20problem-solving%20contexts.%20Code%20will%0Abe%20released%20after%20paper%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Metaheuristic%2520Algorithm%2520Design%2520with%2520Autoregressive%2520Learning%26entry.906535625%3DQi%2520Zhao%2520and%2520Tengfei%2520Liu%2520and%2520Bai%2520Yan%2520and%2520Qiqi%2520Duan%2520and%2520Jian%2520Yang%2520and%2520Yuhui%2520Shi%26entry.1292438233%3D%2520%2520Automated%2520design%2520of%2520metaheuristic%2520algorithms%2520offers%2520an%2520attractive%2520avenue%2520to%250Areduce%2520human%2520effort%2520and%2520gain%2520enhanced%2520performance%2520beyond%2520human%2520intuition.%250ACurrent%2520automated%2520methods%2520design%2520algorithms%2520within%2520a%2520fixed%2520structure%2520and%250Aoperate%2520from%2520scratch.%2520This%2520poses%2520a%2520clear%2520gap%2520towards%2520fully%2520discovering%250Apotentials%2520over%2520the%2520metaheuristic%2520family%2520and%2520fertilizing%2520from%2520prior%2520design%250Aexperience.%2520To%2520bridge%2520the%2520gap%252C%2520this%2520paper%2520proposes%2520an%2520autoregressive%250Alearning-based%2520designer%2520for%2520automated%2520design%2520of%2520metaheuristic%2520algorithms.%2520Our%250Adesigner%2520formulates%2520metaheuristic%2520algorithm%2520design%2520as%2520a%2520sequence%2520generation%250Atask%252C%2520and%2520harnesses%2520an%2520autoregressive%2520generative%2520network%2520to%2520handle%2520the%2520task.%250AThis%2520offers%2520two%2520advances.%2520First%252C%2520through%2520autoregressive%2520inference%252C%2520the%2520designer%250Agenerates%2520algorithms%2520with%2520diverse%2520lengths%2520and%2520structures%252C%2520enabling%2520to%2520fully%250Adiscover%2520potentials%2520over%2520the%2520metaheuristic%2520family.%2520Second%252C%2520prior%2520design%250Aknowledge%2520learned%2520and%2520accumulated%2520in%2520neurons%2520of%2520the%2520designer%2520can%2520be%2520retrieved%250Afor%2520designing%2520algorithms%2520for%2520future%2520problems%252C%2520paving%2520the%2520way%2520to%2520continual%250Adesign%2520of%2520algorithms%2520for%2520open-ended%2520problem-solving.%2520Extensive%2520experiments%2520on%250Anumeral%2520benchmarks%2520and%2520real-world%2520problems%2520reveal%2520that%2520the%2520proposed%2520designer%250Agenerates%2520algorithms%2520that%2520outperform%2520all%2520human-created%2520baselines%2520on%252024%2520out%2520of%250A25%2520test%2520problems.%2520The%2520generated%2520algorithms%2520display%2520various%2520structures%2520and%250Abehaviors%252C%2520reasonably%2520fitting%2520for%2520different%2520problem-solving%2520contexts.%2520Code%2520will%250Abe%2520released%2520after%2520paper%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Metaheuristic%20Algorithm%20Design%20with%20Autoregressive%20Learning&entry.906535625=Qi%20Zhao%20and%20Tengfei%20Liu%20and%20Bai%20Yan%20and%20Qiqi%20Duan%20and%20Jian%20Yang%20and%20Yuhui%20Shi&entry.1292438233=%20%20Automated%20design%20of%20metaheuristic%20algorithms%20offers%20an%20attractive%20avenue%20to%0Areduce%20human%20effort%20and%20gain%20enhanced%20performance%20beyond%20human%20intuition.%0ACurrent%20automated%20methods%20design%20algorithms%20within%20a%20fixed%20structure%20and%0Aoperate%20from%20scratch.%20This%20poses%20a%20clear%20gap%20towards%20fully%20discovering%0Apotentials%20over%20the%20metaheuristic%20family%20and%20fertilizing%20from%20prior%20design%0Aexperience.%20To%20bridge%20the%20gap%2C%20this%20paper%20proposes%20an%20autoregressive%0Alearning-based%20designer%20for%20automated%20design%20of%20metaheuristic%20algorithms.%20Our%0Adesigner%20formulates%20metaheuristic%20algorithm%20design%20as%20a%20sequence%20generation%0Atask%2C%20and%20harnesses%20an%20autoregressive%20generative%20network%20to%20handle%20the%20task.%0AThis%20offers%20two%20advances.%20First%2C%20through%20autoregressive%20inference%2C%20the%20designer%0Agenerates%20algorithms%20with%20diverse%20lengths%20and%20structures%2C%20enabling%20to%20fully%0Adiscover%20potentials%20over%20the%20metaheuristic%20family.%20Second%2C%20prior%20design%0Aknowledge%20learned%20and%20accumulated%20in%20neurons%20of%20the%20designer%20can%20be%20retrieved%0Afor%20designing%20algorithms%20for%20future%20problems%2C%20paving%20the%20way%20to%20continual%0Adesign%20of%20algorithms%20for%20open-ended%20problem-solving.%20Extensive%20experiments%20on%0Anumeral%20benchmarks%20and%20real-world%20problems%20reveal%20that%20the%20proposed%20designer%0Agenerates%20algorithms%20that%20outperform%20all%20human-created%20baselines%20on%2024%20out%20of%0A25%20test%20problems.%20The%20generated%20algorithms%20display%20various%20structures%20and%0Abehaviors%2C%20reasonably%20fitting%20for%20different%20problem-solving%20contexts.%20Code%20will%0Abe%20released%20after%20paper%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03419v1&entry.124074799=Read"},
{"title": "ExeDec: Execution Decomposition for Compositional Generalization in\n  Neural Program Synthesis", "author": "Kensen Shi and Joey Hong and Yinlin Deng and Pengcheng Yin and Manzil Zaheer and Charles Sutton", "abstract": "  When writing programs, people have the ability to tackle a new complex task\nby decomposing it into smaller and more familiar subtasks. While it is\ndifficult to measure whether neural program synthesis methods have similar\ncapabilities, we can measure whether they compositionally generalize, that is,\nwhether a model that has been trained on the simpler subtasks is subsequently\nable to solve more complex tasks. In this paper, we characterize several\ndifferent forms of compositional generalization that are desirable in program\nsynthesis, forming a meta-benchmark which we use to create generalization tasks\nfor two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a\nnovel decomposition-based synthesis strategy that predicts execution subgoals\nto solve problems step-by-step informed by program execution at each step. When\nused with Transformer models trained from scratch, ExeDec has better synthesis\nperformance and greatly improved compositional generalization ability compared\nto baselines. Finally, we use our benchmarks to demonstrate that LLMs struggle\nto compositionally generalize when asked to do programming-by-example in a\nfew-shot setting, but an ExeDec-style prompting approach can improve the\ngeneralization ability and overall performance.\n", "link": "http://arxiv.org/abs/2307.13883v2", "date": "2024-05-06", "relevancy": 1.9395, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5147}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4864}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExeDec%3A%20Execution%20Decomposition%20for%20Compositional%20Generalization%20in%0A%20%20Neural%20Program%20Synthesis&body=Title%3A%20ExeDec%3A%20Execution%20Decomposition%20for%20Compositional%20Generalization%20in%0A%20%20Neural%20Program%20Synthesis%0AAuthor%3A%20Kensen%20Shi%20and%20Joey%20Hong%20and%20Yinlin%20Deng%20and%20Pengcheng%20Yin%20and%20Manzil%20Zaheer%20and%20Charles%20Sutton%0AAbstract%3A%20%20%20When%20writing%20programs%2C%20people%20have%20the%20ability%20to%20tackle%20a%20new%20complex%20task%0Aby%20decomposing%20it%20into%20smaller%20and%20more%20familiar%20subtasks.%20While%20it%20is%0Adifficult%20to%20measure%20whether%20neural%20program%20synthesis%20methods%20have%20similar%0Acapabilities%2C%20we%20can%20measure%20whether%20they%20compositionally%20generalize%2C%20that%20is%2C%0Awhether%20a%20model%20that%20has%20been%20trained%20on%20the%20simpler%20subtasks%20is%20subsequently%0Aable%20to%20solve%20more%20complex%20tasks.%20In%20this%20paper%2C%20we%20characterize%20several%0Adifferent%20forms%20of%20compositional%20generalization%20that%20are%20desirable%20in%20program%0Asynthesis%2C%20forming%20a%20meta-benchmark%20which%20we%20use%20to%20create%20generalization%20tasks%0Afor%20two%20popular%20datasets%2C%20RobustFill%20and%20DeepCoder.%20We%20then%20propose%20ExeDec%2C%20a%0Anovel%20decomposition-based%20synthesis%20strategy%20that%20predicts%20execution%20subgoals%0Ato%20solve%20problems%20step-by-step%20informed%20by%20program%20execution%20at%20each%20step.%20When%0Aused%20with%20Transformer%20models%20trained%20from%20scratch%2C%20ExeDec%20has%20better%20synthesis%0Aperformance%20and%20greatly%20improved%20compositional%20generalization%20ability%20compared%0Ato%20baselines.%20Finally%2C%20we%20use%20our%20benchmarks%20to%20demonstrate%20that%20LLMs%20struggle%0Ato%20compositionally%20generalize%20when%20asked%20to%20do%20programming-by-example%20in%20a%0Afew-shot%20setting%2C%20but%20an%20ExeDec-style%20prompting%20approach%20can%20improve%20the%0Ageneralization%20ability%20and%20overall%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.13883v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExeDec%253A%2520Execution%2520Decomposition%2520for%2520Compositional%2520Generalization%2520in%250A%2520%2520Neural%2520Program%2520Synthesis%26entry.906535625%3DKensen%2520Shi%2520and%2520Joey%2520Hong%2520and%2520Yinlin%2520Deng%2520and%2520Pengcheng%2520Yin%2520and%2520Manzil%2520Zaheer%2520and%2520Charles%2520Sutton%26entry.1292438233%3D%2520%2520When%2520writing%2520programs%252C%2520people%2520have%2520the%2520ability%2520to%2520tackle%2520a%2520new%2520complex%2520task%250Aby%2520decomposing%2520it%2520into%2520smaller%2520and%2520more%2520familiar%2520subtasks.%2520While%2520it%2520is%250Adifficult%2520to%2520measure%2520whether%2520neural%2520program%2520synthesis%2520methods%2520have%2520similar%250Acapabilities%252C%2520we%2520can%2520measure%2520whether%2520they%2520compositionally%2520generalize%252C%2520that%2520is%252C%250Awhether%2520a%2520model%2520that%2520has%2520been%2520trained%2520on%2520the%2520simpler%2520subtasks%2520is%2520subsequently%250Aable%2520to%2520solve%2520more%2520complex%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520characterize%2520several%250Adifferent%2520forms%2520of%2520compositional%2520generalization%2520that%2520are%2520desirable%2520in%2520program%250Asynthesis%252C%2520forming%2520a%2520meta-benchmark%2520which%2520we%2520use%2520to%2520create%2520generalization%2520tasks%250Afor%2520two%2520popular%2520datasets%252C%2520RobustFill%2520and%2520DeepCoder.%2520We%2520then%2520propose%2520ExeDec%252C%2520a%250Anovel%2520decomposition-based%2520synthesis%2520strategy%2520that%2520predicts%2520execution%2520subgoals%250Ato%2520solve%2520problems%2520step-by-step%2520informed%2520by%2520program%2520execution%2520at%2520each%2520step.%2520When%250Aused%2520with%2520Transformer%2520models%2520trained%2520from%2520scratch%252C%2520ExeDec%2520has%2520better%2520synthesis%250Aperformance%2520and%2520greatly%2520improved%2520compositional%2520generalization%2520ability%2520compared%250Ato%2520baselines.%2520Finally%252C%2520we%2520use%2520our%2520benchmarks%2520to%2520demonstrate%2520that%2520LLMs%2520struggle%250Ato%2520compositionally%2520generalize%2520when%2520asked%2520to%2520do%2520programming-by-example%2520in%2520a%250Afew-shot%2520setting%252C%2520but%2520an%2520ExeDec-style%2520prompting%2520approach%2520can%2520improve%2520the%250Ageneralization%2520ability%2520and%2520overall%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.13883v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExeDec%3A%20Execution%20Decomposition%20for%20Compositional%20Generalization%20in%0A%20%20Neural%20Program%20Synthesis&entry.906535625=Kensen%20Shi%20and%20Joey%20Hong%20and%20Yinlin%20Deng%20and%20Pengcheng%20Yin%20and%20Manzil%20Zaheer%20and%20Charles%20Sutton&entry.1292438233=%20%20When%20writing%20programs%2C%20people%20have%20the%20ability%20to%20tackle%20a%20new%20complex%20task%0Aby%20decomposing%20it%20into%20smaller%20and%20more%20familiar%20subtasks.%20While%20it%20is%0Adifficult%20to%20measure%20whether%20neural%20program%20synthesis%20methods%20have%20similar%0Acapabilities%2C%20we%20can%20measure%20whether%20they%20compositionally%20generalize%2C%20that%20is%2C%0Awhether%20a%20model%20that%20has%20been%20trained%20on%20the%20simpler%20subtasks%20is%20subsequently%0Aable%20to%20solve%20more%20complex%20tasks.%20In%20this%20paper%2C%20we%20characterize%20several%0Adifferent%20forms%20of%20compositional%20generalization%20that%20are%20desirable%20in%20program%0Asynthesis%2C%20forming%20a%20meta-benchmark%20which%20we%20use%20to%20create%20generalization%20tasks%0Afor%20two%20popular%20datasets%2C%20RobustFill%20and%20DeepCoder.%20We%20then%20propose%20ExeDec%2C%20a%0Anovel%20decomposition-based%20synthesis%20strategy%20that%20predicts%20execution%20subgoals%0Ato%20solve%20problems%20step-by-step%20informed%20by%20program%20execution%20at%20each%20step.%20When%0Aused%20with%20Transformer%20models%20trained%20from%20scratch%2C%20ExeDec%20has%20better%20synthesis%0Aperformance%20and%20greatly%20improved%20compositional%20generalization%20ability%20compared%0Ato%20baselines.%20Finally%2C%20we%20use%20our%20benchmarks%20to%20demonstrate%20that%20LLMs%20struggle%0Ato%20compositionally%20generalize%20when%20asked%20to%20do%20programming-by-example%20in%20a%0Afew-shot%20setting%2C%20but%20an%20ExeDec-style%20prompting%20approach%20can%20improve%20the%0Ageneralization%20ability%20and%20overall%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.13883v2&entry.124074799=Read"},
{"title": "ReCycle: Fast and Efficient Long Time Series Forecasting with Residual\n  Cyclic Transformers", "author": "Arvid Weyrauch and Thomas Steens and Oskar Taubert and Benedikt Hanke and Aslan Eqbal and Ewa G\u00f6tz and Achim Streit and Markus G\u00f6tz and Charlotte Debus", "abstract": "  Transformers have recently gained prominence in long time series forecasting\nby elevating accuracies in a variety of use cases. Regrettably, in the race for\nbetter predictive performance the overhead of model architectures has grown\nonerous, leading to models with computational demand infeasible for most\npractical applications. To bridge the gap between high method complexity and\nrealistic computational resources, we introduce the Residual Cyclic\nTransformer, ReCycle. ReCycle utilizes primary cycle compression to address the\ncomputational complexity of the attention mechanism in long time series. By\nlearning residuals from refined smoothing average techniques, ReCycle surpasses\nstate-of-the-art accuracy in a variety of application use cases. The reliable\nand explainable fallback behavior ensured by simple, yet robust, smoothing\naverage techniques additionally lowers the barrier for user acceptance. At the\nsame time, our approach reduces the run time and energy consumption by more\nthan an order of magnitude, making both training and inference feasible on\nlow-performance, low-power and edge computing devices. Code is available at\nhttps://github.com/Helmholtz-AI-Energy/ReCycle\n", "link": "http://arxiv.org/abs/2405.03429v1", "date": "2024-05-06", "relevancy": 1.9365, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5717}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.472}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReCycle%3A%20Fast%20and%20Efficient%20Long%20Time%20Series%20Forecasting%20with%20Residual%0A%20%20Cyclic%20Transformers&body=Title%3A%20ReCycle%3A%20Fast%20and%20Efficient%20Long%20Time%20Series%20Forecasting%20with%20Residual%0A%20%20Cyclic%20Transformers%0AAuthor%3A%20Arvid%20Weyrauch%20and%20Thomas%20Steens%20and%20Oskar%20Taubert%20and%20Benedikt%20Hanke%20and%20Aslan%20Eqbal%20and%20Ewa%20G%C3%B6tz%20and%20Achim%20Streit%20and%20Markus%20G%C3%B6tz%20and%20Charlotte%20Debus%0AAbstract%3A%20%20%20Transformers%20have%20recently%20gained%20prominence%20in%20long%20time%20series%20forecasting%0Aby%20elevating%20accuracies%20in%20a%20variety%20of%20use%20cases.%20Regrettably%2C%20in%20the%20race%20for%0Abetter%20predictive%20performance%20the%20overhead%20of%20model%20architectures%20has%20grown%0Aonerous%2C%20leading%20to%20models%20with%20computational%20demand%20infeasible%20for%20most%0Apractical%20applications.%20To%20bridge%20the%20gap%20between%20high%20method%20complexity%20and%0Arealistic%20computational%20resources%2C%20we%20introduce%20the%20Residual%20Cyclic%0ATransformer%2C%20ReCycle.%20ReCycle%20utilizes%20primary%20cycle%20compression%20to%20address%20the%0Acomputational%20complexity%20of%20the%20attention%20mechanism%20in%20long%20time%20series.%20By%0Alearning%20residuals%20from%20refined%20smoothing%20average%20techniques%2C%20ReCycle%20surpasses%0Astate-of-the-art%20accuracy%20in%20a%20variety%20of%20application%20use%20cases.%20The%20reliable%0Aand%20explainable%20fallback%20behavior%20ensured%20by%20simple%2C%20yet%20robust%2C%20smoothing%0Aaverage%20techniques%20additionally%20lowers%20the%20barrier%20for%20user%20acceptance.%20At%20the%0Asame%20time%2C%20our%20approach%20reduces%20the%20run%20time%20and%20energy%20consumption%20by%20more%0Athan%20an%20order%20of%20magnitude%2C%20making%20both%20training%20and%20inference%20feasible%20on%0Alow-performance%2C%20low-power%20and%20edge%20computing%20devices.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Helmholtz-AI-Energy/ReCycle%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReCycle%253A%2520Fast%2520and%2520Efficient%2520Long%2520Time%2520Series%2520Forecasting%2520with%2520Residual%250A%2520%2520Cyclic%2520Transformers%26entry.906535625%3DArvid%2520Weyrauch%2520and%2520Thomas%2520Steens%2520and%2520Oskar%2520Taubert%2520and%2520Benedikt%2520Hanke%2520and%2520Aslan%2520Eqbal%2520and%2520Ewa%2520G%25C3%25B6tz%2520and%2520Achim%2520Streit%2520and%2520Markus%2520G%25C3%25B6tz%2520and%2520Charlotte%2520Debus%26entry.1292438233%3D%2520%2520Transformers%2520have%2520recently%2520gained%2520prominence%2520in%2520long%2520time%2520series%2520forecasting%250Aby%2520elevating%2520accuracies%2520in%2520a%2520variety%2520of%2520use%2520cases.%2520Regrettably%252C%2520in%2520the%2520race%2520for%250Abetter%2520predictive%2520performance%2520the%2520overhead%2520of%2520model%2520architectures%2520has%2520grown%250Aonerous%252C%2520leading%2520to%2520models%2520with%2520computational%2520demand%2520infeasible%2520for%2520most%250Apractical%2520applications.%2520To%2520bridge%2520the%2520gap%2520between%2520high%2520method%2520complexity%2520and%250Arealistic%2520computational%2520resources%252C%2520we%2520introduce%2520the%2520Residual%2520Cyclic%250ATransformer%252C%2520ReCycle.%2520ReCycle%2520utilizes%2520primary%2520cycle%2520compression%2520to%2520address%2520the%250Acomputational%2520complexity%2520of%2520the%2520attention%2520mechanism%2520in%2520long%2520time%2520series.%2520By%250Alearning%2520residuals%2520from%2520refined%2520smoothing%2520average%2520techniques%252C%2520ReCycle%2520surpasses%250Astate-of-the-art%2520accuracy%2520in%2520a%2520variety%2520of%2520application%2520use%2520cases.%2520The%2520reliable%250Aand%2520explainable%2520fallback%2520behavior%2520ensured%2520by%2520simple%252C%2520yet%2520robust%252C%2520smoothing%250Aaverage%2520techniques%2520additionally%2520lowers%2520the%2520barrier%2520for%2520user%2520acceptance.%2520At%2520the%250Asame%2520time%252C%2520our%2520approach%2520reduces%2520the%2520run%2520time%2520and%2520energy%2520consumption%2520by%2520more%250Athan%2520an%2520order%2520of%2520magnitude%252C%2520making%2520both%2520training%2520and%2520inference%2520feasible%2520on%250Alow-performance%252C%2520low-power%2520and%2520edge%2520computing%2520devices.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Helmholtz-AI-Energy/ReCycle%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReCycle%3A%20Fast%20and%20Efficient%20Long%20Time%20Series%20Forecasting%20with%20Residual%0A%20%20Cyclic%20Transformers&entry.906535625=Arvid%20Weyrauch%20and%20Thomas%20Steens%20and%20Oskar%20Taubert%20and%20Benedikt%20Hanke%20and%20Aslan%20Eqbal%20and%20Ewa%20G%C3%B6tz%20and%20Achim%20Streit%20and%20Markus%20G%C3%B6tz%20and%20Charlotte%20Debus&entry.1292438233=%20%20Transformers%20have%20recently%20gained%20prominence%20in%20long%20time%20series%20forecasting%0Aby%20elevating%20accuracies%20in%20a%20variety%20of%20use%20cases.%20Regrettably%2C%20in%20the%20race%20for%0Abetter%20predictive%20performance%20the%20overhead%20of%20model%20architectures%20has%20grown%0Aonerous%2C%20leading%20to%20models%20with%20computational%20demand%20infeasible%20for%20most%0Apractical%20applications.%20To%20bridge%20the%20gap%20between%20high%20method%20complexity%20and%0Arealistic%20computational%20resources%2C%20we%20introduce%20the%20Residual%20Cyclic%0ATransformer%2C%20ReCycle.%20ReCycle%20utilizes%20primary%20cycle%20compression%20to%20address%20the%0Acomputational%20complexity%20of%20the%20attention%20mechanism%20in%20long%20time%20series.%20By%0Alearning%20residuals%20from%20refined%20smoothing%20average%20techniques%2C%20ReCycle%20surpasses%0Astate-of-the-art%20accuracy%20in%20a%20variety%20of%20application%20use%20cases.%20The%20reliable%0Aand%20explainable%20fallback%20behavior%20ensured%20by%20simple%2C%20yet%20robust%2C%20smoothing%0Aaverage%20techniques%20additionally%20lowers%20the%20barrier%20for%20user%20acceptance.%20At%20the%0Asame%20time%2C%20our%20approach%20reduces%20the%20run%20time%20and%20energy%20consumption%20by%20more%0Athan%20an%20order%20of%20magnitude%2C%20making%20both%20training%20and%20inference%20feasible%20on%0Alow-performance%2C%20low-power%20and%20edge%20computing%20devices.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Helmholtz-AI-Energy/ReCycle%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03429v1&entry.124074799=Read"},
{"title": "A Minimum-Jerk Approach to Handle Singularities in Virtual Fixtures", "author": "Giovanni Braglia and Sylvain Calinon and Luigi Biagiotti", "abstract": "  Implementing virtual fixtures in guiding tasks constrains the movement of the\nrobot's end effector to specific curves within its workspace. However,\nincorporating guiding frameworks may encounter discontinuities when optimizing\nthe reference target position to the nearest point relative to the current\nrobot position. This article aims to give a geometric interpretation of such\ndiscontinuities, with specific reference to the commonly adopted Gauss-Newton\nalgorithm. The effect of such discontinuities, defined as Euclidean Distance\nSingularities, is experimentally proved. We then propose a solution that is\nbased on a Linear Quadratic Tracking problem with minimum jerk command, then\ncompare and validate the performances of the proposed framework in two\ndifferent human-robot interaction scenarios.\n", "link": "http://arxiv.org/abs/2405.03473v1", "date": "2024-05-06", "relevancy": 1.9315, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4895}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4839}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Minimum-Jerk%20Approach%20to%20Handle%20Singularities%20in%20Virtual%20Fixtures&body=Title%3A%20A%20Minimum-Jerk%20Approach%20to%20Handle%20Singularities%20in%20Virtual%20Fixtures%0AAuthor%3A%20Giovanni%20Braglia%20and%20Sylvain%20Calinon%20and%20Luigi%20Biagiotti%0AAbstract%3A%20%20%20Implementing%20virtual%20fixtures%20in%20guiding%20tasks%20constrains%20the%20movement%20of%20the%0Arobot%27s%20end%20effector%20to%20specific%20curves%20within%20its%20workspace.%20However%2C%0Aincorporating%20guiding%20frameworks%20may%20encounter%20discontinuities%20when%20optimizing%0Athe%20reference%20target%20position%20to%20the%20nearest%20point%20relative%20to%20the%20current%0Arobot%20position.%20This%20article%20aims%20to%20give%20a%20geometric%20interpretation%20of%20such%0Adiscontinuities%2C%20with%20specific%20reference%20to%20the%20commonly%20adopted%20Gauss-Newton%0Aalgorithm.%20The%20effect%20of%20such%20discontinuities%2C%20defined%20as%20Euclidean%20Distance%0ASingularities%2C%20is%20experimentally%20proved.%20We%20then%20propose%20a%20solution%20that%20is%0Abased%20on%20a%20Linear%20Quadratic%20Tracking%20problem%20with%20minimum%20jerk%20command%2C%20then%0Acompare%20and%20validate%20the%20performances%20of%20the%20proposed%20framework%20in%20two%0Adifferent%20human-robot%20interaction%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Minimum-Jerk%2520Approach%2520to%2520Handle%2520Singularities%2520in%2520Virtual%2520Fixtures%26entry.906535625%3DGiovanni%2520Braglia%2520and%2520Sylvain%2520Calinon%2520and%2520Luigi%2520Biagiotti%26entry.1292438233%3D%2520%2520Implementing%2520virtual%2520fixtures%2520in%2520guiding%2520tasks%2520constrains%2520the%2520movement%2520of%2520the%250Arobot%2527s%2520end%2520effector%2520to%2520specific%2520curves%2520within%2520its%2520workspace.%2520However%252C%250Aincorporating%2520guiding%2520frameworks%2520may%2520encounter%2520discontinuities%2520when%2520optimizing%250Athe%2520reference%2520target%2520position%2520to%2520the%2520nearest%2520point%2520relative%2520to%2520the%2520current%250Arobot%2520position.%2520This%2520article%2520aims%2520to%2520give%2520a%2520geometric%2520interpretation%2520of%2520such%250Adiscontinuities%252C%2520with%2520specific%2520reference%2520to%2520the%2520commonly%2520adopted%2520Gauss-Newton%250Aalgorithm.%2520The%2520effect%2520of%2520such%2520discontinuities%252C%2520defined%2520as%2520Euclidean%2520Distance%250ASingularities%252C%2520is%2520experimentally%2520proved.%2520We%2520then%2520propose%2520a%2520solution%2520that%2520is%250Abased%2520on%2520a%2520Linear%2520Quadratic%2520Tracking%2520problem%2520with%2520minimum%2520jerk%2520command%252C%2520then%250Acompare%2520and%2520validate%2520the%2520performances%2520of%2520the%2520proposed%2520framework%2520in%2520two%250Adifferent%2520human-robot%2520interaction%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Minimum-Jerk%20Approach%20to%20Handle%20Singularities%20in%20Virtual%20Fixtures&entry.906535625=Giovanni%20Braglia%20and%20Sylvain%20Calinon%20and%20Luigi%20Biagiotti&entry.1292438233=%20%20Implementing%20virtual%20fixtures%20in%20guiding%20tasks%20constrains%20the%20movement%20of%20the%0Arobot%27s%20end%20effector%20to%20specific%20curves%20within%20its%20workspace.%20However%2C%0Aincorporating%20guiding%20frameworks%20may%20encounter%20discontinuities%20when%20optimizing%0Athe%20reference%20target%20position%20to%20the%20nearest%20point%20relative%20to%20the%20current%0Arobot%20position.%20This%20article%20aims%20to%20give%20a%20geometric%20interpretation%20of%20such%0Adiscontinuities%2C%20with%20specific%20reference%20to%20the%20commonly%20adopted%20Gauss-Newton%0Aalgorithm.%20The%20effect%20of%20such%20discontinuities%2C%20defined%20as%20Euclidean%20Distance%0ASingularities%2C%20is%20experimentally%20proved.%20We%20then%20propose%20a%20solution%20that%20is%0Abased%20on%20a%20Linear%20Quadratic%20Tracking%20problem%20with%20minimum%20jerk%20command%2C%20then%0Acompare%20and%20validate%20the%20performances%20of%20the%20proposed%20framework%20in%20two%0Adifferent%20human-robot%20interaction%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03473v1&entry.124074799=Read"},
{"title": "SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling\n  with Backtracking", "author": "Chris Cundy and Stefano Ermon", "abstract": "  In many domains, autoregressive models can attain high likelihood on the task\nof predicting the next observation. However, this maximum-likelihood (MLE)\nobjective does not necessarily match a downstream use-case of autoregressively\ngenerating high-quality sequences. The MLE objective weights sequences\nproportionally to their frequency under the data distribution, with no guidance\nfor the model's behaviour out of distribution (OOD): leading to compounding\nerror during autoregressive generation. In order to address this compounding\nerror problem, we formulate sequence generation as an imitation learning (IL)\nproblem. This allows us to minimize a variety of divergences between the\ndistribution of sequences generated by an autoregressive model and sequences\nfrom a dataset, including divergences with weight on OOD generated sequences.\nThe IL framework also allows us to incorporate backtracking by introducing a\nbackspace action into the generation process. This further mitigates the\ncompounding error problem by allowing the model to revert a sampled token if it\ntakes the sequence OOD. Our resulting method, SequenceMatch, can be implemented\nwithout adversarial training or architectural changes. We identify the\nSequenceMatch-$\\chi^2$ divergence as a more suitable training objective for\nautoregressive models which are used for generation. We show that empirically,\nSequenceMatch training leads to improvements over MLE on text generation with\nlanguage models and arithmetic.\n", "link": "http://arxiv.org/abs/2306.05426v3", "date": "2024-05-06", "relevancy": 1.918, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.489}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.475}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SequenceMatch%3A%20Imitation%20Learning%20for%20Autoregressive%20Sequence%20Modelling%0A%20%20with%20Backtracking&body=Title%3A%20SequenceMatch%3A%20Imitation%20Learning%20for%20Autoregressive%20Sequence%20Modelling%0A%20%20with%20Backtracking%0AAuthor%3A%20Chris%20Cundy%20and%20Stefano%20Ermon%0AAbstract%3A%20%20%20In%20many%20domains%2C%20autoregressive%20models%20can%20attain%20high%20likelihood%20on%20the%20task%0Aof%20predicting%20the%20next%20observation.%20However%2C%20this%20maximum-likelihood%20%28MLE%29%0Aobjective%20does%20not%20necessarily%20match%20a%20downstream%20use-case%20of%20autoregressively%0Agenerating%20high-quality%20sequences.%20The%20MLE%20objective%20weights%20sequences%0Aproportionally%20to%20their%20frequency%20under%20the%20data%20distribution%2C%20with%20no%20guidance%0Afor%20the%20model%27s%20behaviour%20out%20of%20distribution%20%28OOD%29%3A%20leading%20to%20compounding%0Aerror%20during%20autoregressive%20generation.%20In%20order%20to%20address%20this%20compounding%0Aerror%20problem%2C%20we%20formulate%20sequence%20generation%20as%20an%20imitation%20learning%20%28IL%29%0Aproblem.%20This%20allows%20us%20to%20minimize%20a%20variety%20of%20divergences%20between%20the%0Adistribution%20of%20sequences%20generated%20by%20an%20autoregressive%20model%20and%20sequences%0Afrom%20a%20dataset%2C%20including%20divergences%20with%20weight%20on%20OOD%20generated%20sequences.%0AThe%20IL%20framework%20also%20allows%20us%20to%20incorporate%20backtracking%20by%20introducing%20a%0Abackspace%20action%20into%20the%20generation%20process.%20This%20further%20mitigates%20the%0Acompounding%20error%20problem%20by%20allowing%20the%20model%20to%20revert%20a%20sampled%20token%20if%20it%0Atakes%20the%20sequence%20OOD.%20Our%20resulting%20method%2C%20SequenceMatch%2C%20can%20be%20implemented%0Awithout%20adversarial%20training%20or%20architectural%20changes.%20We%20identify%20the%0ASequenceMatch-%24%5Cchi%5E2%24%20divergence%20as%20a%20more%20suitable%20training%20objective%20for%0Aautoregressive%20models%20which%20are%20used%20for%20generation.%20We%20show%20that%20empirically%2C%0ASequenceMatch%20training%20leads%20to%20improvements%20over%20MLE%20on%20text%20generation%20with%0Alanguage%20models%20and%20arithmetic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.05426v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSequenceMatch%253A%2520Imitation%2520Learning%2520for%2520Autoregressive%2520Sequence%2520Modelling%250A%2520%2520with%2520Backtracking%26entry.906535625%3DChris%2520Cundy%2520and%2520Stefano%2520Ermon%26entry.1292438233%3D%2520%2520In%2520many%2520domains%252C%2520autoregressive%2520models%2520can%2520attain%2520high%2520likelihood%2520on%2520the%2520task%250Aof%2520predicting%2520the%2520next%2520observation.%2520However%252C%2520this%2520maximum-likelihood%2520%2528MLE%2529%250Aobjective%2520does%2520not%2520necessarily%2520match%2520a%2520downstream%2520use-case%2520of%2520autoregressively%250Agenerating%2520high-quality%2520sequences.%2520The%2520MLE%2520objective%2520weights%2520sequences%250Aproportionally%2520to%2520their%2520frequency%2520under%2520the%2520data%2520distribution%252C%2520with%2520no%2520guidance%250Afor%2520the%2520model%2527s%2520behaviour%2520out%2520of%2520distribution%2520%2528OOD%2529%253A%2520leading%2520to%2520compounding%250Aerror%2520during%2520autoregressive%2520generation.%2520In%2520order%2520to%2520address%2520this%2520compounding%250Aerror%2520problem%252C%2520we%2520formulate%2520sequence%2520generation%2520as%2520an%2520imitation%2520learning%2520%2528IL%2529%250Aproblem.%2520This%2520allows%2520us%2520to%2520minimize%2520a%2520variety%2520of%2520divergences%2520between%2520the%250Adistribution%2520of%2520sequences%2520generated%2520by%2520an%2520autoregressive%2520model%2520and%2520sequences%250Afrom%2520a%2520dataset%252C%2520including%2520divergences%2520with%2520weight%2520on%2520OOD%2520generated%2520sequences.%250AThe%2520IL%2520framework%2520also%2520allows%2520us%2520to%2520incorporate%2520backtracking%2520by%2520introducing%2520a%250Abackspace%2520action%2520into%2520the%2520generation%2520process.%2520This%2520further%2520mitigates%2520the%250Acompounding%2520error%2520problem%2520by%2520allowing%2520the%2520model%2520to%2520revert%2520a%2520sampled%2520token%2520if%2520it%250Atakes%2520the%2520sequence%2520OOD.%2520Our%2520resulting%2520method%252C%2520SequenceMatch%252C%2520can%2520be%2520implemented%250Awithout%2520adversarial%2520training%2520or%2520architectural%2520changes.%2520We%2520identify%2520the%250ASequenceMatch-%2524%255Cchi%255E2%2524%2520divergence%2520as%2520a%2520more%2520suitable%2520training%2520objective%2520for%250Aautoregressive%2520models%2520which%2520are%2520used%2520for%2520generation.%2520We%2520show%2520that%2520empirically%252C%250ASequenceMatch%2520training%2520leads%2520to%2520improvements%2520over%2520MLE%2520on%2520text%2520generation%2520with%250Alanguage%2520models%2520and%2520arithmetic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.05426v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SequenceMatch%3A%20Imitation%20Learning%20for%20Autoregressive%20Sequence%20Modelling%0A%20%20with%20Backtracking&entry.906535625=Chris%20Cundy%20and%20Stefano%20Ermon&entry.1292438233=%20%20In%20many%20domains%2C%20autoregressive%20models%20can%20attain%20high%20likelihood%20on%20the%20task%0Aof%20predicting%20the%20next%20observation.%20However%2C%20this%20maximum-likelihood%20%28MLE%29%0Aobjective%20does%20not%20necessarily%20match%20a%20downstream%20use-case%20of%20autoregressively%0Agenerating%20high-quality%20sequences.%20The%20MLE%20objective%20weights%20sequences%0Aproportionally%20to%20their%20frequency%20under%20the%20data%20distribution%2C%20with%20no%20guidance%0Afor%20the%20model%27s%20behaviour%20out%20of%20distribution%20%28OOD%29%3A%20leading%20to%20compounding%0Aerror%20during%20autoregressive%20generation.%20In%20order%20to%20address%20this%20compounding%0Aerror%20problem%2C%20we%20formulate%20sequence%20generation%20as%20an%20imitation%20learning%20%28IL%29%0Aproblem.%20This%20allows%20us%20to%20minimize%20a%20variety%20of%20divergences%20between%20the%0Adistribution%20of%20sequences%20generated%20by%20an%20autoregressive%20model%20and%20sequences%0Afrom%20a%20dataset%2C%20including%20divergences%20with%20weight%20on%20OOD%20generated%20sequences.%0AThe%20IL%20framework%20also%20allows%20us%20to%20incorporate%20backtracking%20by%20introducing%20a%0Abackspace%20action%20into%20the%20generation%20process.%20This%20further%20mitigates%20the%0Acompounding%20error%20problem%20by%20allowing%20the%20model%20to%20revert%20a%20sampled%20token%20if%20it%0Atakes%20the%20sequence%20OOD.%20Our%20resulting%20method%2C%20SequenceMatch%2C%20can%20be%20implemented%0Awithout%20adversarial%20training%20or%20architectural%20changes.%20We%20identify%20the%0ASequenceMatch-%24%5Cchi%5E2%24%20divergence%20as%20a%20more%20suitable%20training%20objective%20for%0Aautoregressive%20models%20which%20are%20used%20for%20generation.%20We%20show%20that%20empirically%2C%0ASequenceMatch%20training%20leads%20to%20improvements%20over%20MLE%20on%20text%20generation%20with%0Alanguage%20models%20and%20arithmetic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.05426v3&entry.124074799=Read"},
{"title": "Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient\n  for Subgraph Representation Learning", "author": "Dongkwan Kim and Alice Oh", "abstract": "  Subgraph representation learning has emerged as an important problem, but it\nis by default approached with specialized graph neural networks on a large\nglobal graph. These models demand extensive memory and computational resources\nbut challenge modeling hierarchical structures of subgraphs. In this paper, we\npropose Subgraph-To-Node (S2N) translation, a novel formulation for learning\nrepresentations of subgraphs. Specifically, given a set of subgraphs in the\nglobal graph, we construct a new graph by coarsely transforming subgraphs into\nnodes. Demonstrating both theoretical and empirical evidence, S2N not only\nsignificantly reduces memory and computational costs compared to\nstate-of-the-art models but also outperforms them by capturing both local and\nglobal structures of the subgraph. By leveraging graph coarsening methods, our\nmethod outperforms baselines even in a data-scarce setting with insufficient\nsubgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned\nmodels with S2N translation can process 183 -- 711 times more subgraph samples\nthan state-of-the-art models at a better or similar performance level.\n", "link": "http://arxiv.org/abs/2204.04510v3", "date": "2024-05-06", "relevancy": 1.9013, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5048}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4585}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Translating%20Subgraphs%20to%20Nodes%20Makes%20Simple%20GNNs%20Strong%20and%20Efficient%0A%20%20for%20Subgraph%20Representation%20Learning&body=Title%3A%20Translating%20Subgraphs%20to%20Nodes%20Makes%20Simple%20GNNs%20Strong%20and%20Efficient%0A%20%20for%20Subgraph%20Representation%20Learning%0AAuthor%3A%20Dongkwan%20Kim%20and%20Alice%20Oh%0AAbstract%3A%20%20%20Subgraph%20representation%20learning%20has%20emerged%20as%20an%20important%20problem%2C%20but%20it%0Ais%20by%20default%20approached%20with%20specialized%20graph%20neural%20networks%20on%20a%20large%0Aglobal%20graph.%20These%20models%20demand%20extensive%20memory%20and%20computational%20resources%0Abut%20challenge%20modeling%20hierarchical%20structures%20of%20subgraphs.%20In%20this%20paper%2C%20we%0Apropose%20Subgraph-To-Node%20%28S2N%29%20translation%2C%20a%20novel%20formulation%20for%20learning%0Arepresentations%20of%20subgraphs.%20Specifically%2C%20given%20a%20set%20of%20subgraphs%20in%20the%0Aglobal%20graph%2C%20we%20construct%20a%20new%20graph%20by%20coarsely%20transforming%20subgraphs%20into%0Anodes.%20Demonstrating%20both%20theoretical%20and%20empirical%20evidence%2C%20S2N%20not%20only%0Asignificantly%20reduces%20memory%20and%20computational%20costs%20compared%20to%0Astate-of-the-art%20models%20but%20also%20outperforms%20them%20by%20capturing%20both%20local%20and%0Aglobal%20structures%20of%20the%20subgraph.%20By%20leveraging%20graph%20coarsening%20methods%2C%20our%0Amethod%20outperforms%20baselines%20even%20in%20a%20data-scarce%20setting%20with%20insufficient%0Asubgraphs.%20Our%20experiments%20on%20eight%20benchmarks%20demonstrate%20that%20fined-tuned%0Amodels%20with%20S2N%20translation%20can%20process%20183%20--%20711%20times%20more%20subgraph%20samples%0Athan%20state-of-the-art%20models%20at%20a%20better%20or%20similar%20performance%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2204.04510v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTranslating%2520Subgraphs%2520to%2520Nodes%2520Makes%2520Simple%2520GNNs%2520Strong%2520and%2520Efficient%250A%2520%2520for%2520Subgraph%2520Representation%2520Learning%26entry.906535625%3DDongkwan%2520Kim%2520and%2520Alice%2520Oh%26entry.1292438233%3D%2520%2520Subgraph%2520representation%2520learning%2520has%2520emerged%2520as%2520an%2520important%2520problem%252C%2520but%2520it%250Ais%2520by%2520default%2520approached%2520with%2520specialized%2520graph%2520neural%2520networks%2520on%2520a%2520large%250Aglobal%2520graph.%2520These%2520models%2520demand%2520extensive%2520memory%2520and%2520computational%2520resources%250Abut%2520challenge%2520modeling%2520hierarchical%2520structures%2520of%2520subgraphs.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Subgraph-To-Node%2520%2528S2N%2529%2520translation%252C%2520a%2520novel%2520formulation%2520for%2520learning%250Arepresentations%2520of%2520subgraphs.%2520Specifically%252C%2520given%2520a%2520set%2520of%2520subgraphs%2520in%2520the%250Aglobal%2520graph%252C%2520we%2520construct%2520a%2520new%2520graph%2520by%2520coarsely%2520transforming%2520subgraphs%2520into%250Anodes.%2520Demonstrating%2520both%2520theoretical%2520and%2520empirical%2520evidence%252C%2520S2N%2520not%2520only%250Asignificantly%2520reduces%2520memory%2520and%2520computational%2520costs%2520compared%2520to%250Astate-of-the-art%2520models%2520but%2520also%2520outperforms%2520them%2520by%2520capturing%2520both%2520local%2520and%250Aglobal%2520structures%2520of%2520the%2520subgraph.%2520By%2520leveraging%2520graph%2520coarsening%2520methods%252C%2520our%250Amethod%2520outperforms%2520baselines%2520even%2520in%2520a%2520data-scarce%2520setting%2520with%2520insufficient%250Asubgraphs.%2520Our%2520experiments%2520on%2520eight%2520benchmarks%2520demonstrate%2520that%2520fined-tuned%250Amodels%2520with%2520S2N%2520translation%2520can%2520process%2520183%2520--%2520711%2520times%2520more%2520subgraph%2520samples%250Athan%2520state-of-the-art%2520models%2520at%2520a%2520better%2520or%2520similar%2520performance%2520level.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2204.04510v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Translating%20Subgraphs%20to%20Nodes%20Makes%20Simple%20GNNs%20Strong%20and%20Efficient%0A%20%20for%20Subgraph%20Representation%20Learning&entry.906535625=Dongkwan%20Kim%20and%20Alice%20Oh&entry.1292438233=%20%20Subgraph%20representation%20learning%20has%20emerged%20as%20an%20important%20problem%2C%20but%20it%0Ais%20by%20default%20approached%20with%20specialized%20graph%20neural%20networks%20on%20a%20large%0Aglobal%20graph.%20These%20models%20demand%20extensive%20memory%20and%20computational%20resources%0Abut%20challenge%20modeling%20hierarchical%20structures%20of%20subgraphs.%20In%20this%20paper%2C%20we%0Apropose%20Subgraph-To-Node%20%28S2N%29%20translation%2C%20a%20novel%20formulation%20for%20learning%0Arepresentations%20of%20subgraphs.%20Specifically%2C%20given%20a%20set%20of%20subgraphs%20in%20the%0Aglobal%20graph%2C%20we%20construct%20a%20new%20graph%20by%20coarsely%20transforming%20subgraphs%20into%0Anodes.%20Demonstrating%20both%20theoretical%20and%20empirical%20evidence%2C%20S2N%20not%20only%0Asignificantly%20reduces%20memory%20and%20computational%20costs%20compared%20to%0Astate-of-the-art%20models%20but%20also%20outperforms%20them%20by%20capturing%20both%20local%20and%0Aglobal%20structures%20of%20the%20subgraph.%20By%20leveraging%20graph%20coarsening%20methods%2C%20our%0Amethod%20outperforms%20baselines%20even%20in%20a%20data-scarce%20setting%20with%20insufficient%0Asubgraphs.%20Our%20experiments%20on%20eight%20benchmarks%20demonstrate%20that%20fined-tuned%0Amodels%20with%20S2N%20translation%20can%20process%20183%20--%20711%20times%20more%20subgraph%20samples%0Athan%20state-of-the-art%20models%20at%20a%20better%20or%20similar%20performance%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2204.04510v3&entry.124074799=Read"},
{"title": "Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy\n  Gradient Methods", "author": "Sara Klein and Simon Weissmann and Leif D\u00f6ring", "abstract": "  Markov Decision Processes (MDPs) are a formal framework for modeling and\nsolving sequential decision-making problems. In finite-time horizons such\nproblems are relevant for instance for optimal stopping or specific supply\nchain problems, but also in the training of large language models. In contrast\nto infinite horizon MDPs optimal policies are not stationary, policies must be\nlearned for every single epoch. In practice all parameters are often trained\nsimultaneously, ignoring the inherent structure suggested by dynamic\nprogramming. This paper introduces a combination of dynamic programming and\npolicy gradient called dynamic policy gradient, where the parameters are\ntrained backwards in time. For the tabular softmax parametrisation we carry out\nthe convergence analysis for simultaneous and dynamic policy gradient towards\nglobal optima, both in the exact and sampled gradient settings without\nregularisation. It turns out that the use of dynamic policy gradient training\nmuch better exploits the structure of finite- time problems which is reflected\nin improved convergence bounds.\n", "link": "http://arxiv.org/abs/2310.02671v2", "date": "2024-05-06", "relevancy": 1.9007, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4898}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4749}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Stationarity%3A%20Convergence%20Analysis%20of%20Stochastic%20Softmax%20Policy%0A%20%20Gradient%20Methods&body=Title%3A%20Beyond%20Stationarity%3A%20Convergence%20Analysis%20of%20Stochastic%20Softmax%20Policy%0A%20%20Gradient%20Methods%0AAuthor%3A%20Sara%20Klein%20and%20Simon%20Weissmann%20and%20Leif%20D%C3%B6ring%0AAbstract%3A%20%20%20Markov%20Decision%20Processes%20%28MDPs%29%20are%20a%20formal%20framework%20for%20modeling%20and%0Asolving%20sequential%20decision-making%20problems.%20In%20finite-time%20horizons%20such%0Aproblems%20are%20relevant%20for%20instance%20for%20optimal%20stopping%20or%20specific%20supply%0Achain%20problems%2C%20but%20also%20in%20the%20training%20of%20large%20language%20models.%20In%20contrast%0Ato%20infinite%20horizon%20MDPs%20optimal%20policies%20are%20not%20stationary%2C%20policies%20must%20be%0Alearned%20for%20every%20single%20epoch.%20In%20practice%20all%20parameters%20are%20often%20trained%0Asimultaneously%2C%20ignoring%20the%20inherent%20structure%20suggested%20by%20dynamic%0Aprogramming.%20This%20paper%20introduces%20a%20combination%20of%20dynamic%20programming%20and%0Apolicy%20gradient%20called%20dynamic%20policy%20gradient%2C%20where%20the%20parameters%20are%0Atrained%20backwards%20in%20time.%20For%20the%20tabular%20softmax%20parametrisation%20we%20carry%20out%0Athe%20convergence%20analysis%20for%20simultaneous%20and%20dynamic%20policy%20gradient%20towards%0Aglobal%20optima%2C%20both%20in%20the%20exact%20and%20sampled%20gradient%20settings%20without%0Aregularisation.%20It%20turns%20out%20that%20the%20use%20of%20dynamic%20policy%20gradient%20training%0Amuch%20better%20exploits%20the%20structure%20of%20finite-%20time%20problems%20which%20is%20reflected%0Ain%20improved%20convergence%20bounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02671v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Stationarity%253A%2520Convergence%2520Analysis%2520of%2520Stochastic%2520Softmax%2520Policy%250A%2520%2520Gradient%2520Methods%26entry.906535625%3DSara%2520Klein%2520and%2520Simon%2520Weissmann%2520and%2520Leif%2520D%25C3%25B6ring%26entry.1292438233%3D%2520%2520Markov%2520Decision%2520Processes%2520%2528MDPs%2529%2520are%2520a%2520formal%2520framework%2520for%2520modeling%2520and%250Asolving%2520sequential%2520decision-making%2520problems.%2520In%2520finite-time%2520horizons%2520such%250Aproblems%2520are%2520relevant%2520for%2520instance%2520for%2520optimal%2520stopping%2520or%2520specific%2520supply%250Achain%2520problems%252C%2520but%2520also%2520in%2520the%2520training%2520of%2520large%2520language%2520models.%2520In%2520contrast%250Ato%2520infinite%2520horizon%2520MDPs%2520optimal%2520policies%2520are%2520not%2520stationary%252C%2520policies%2520must%2520be%250Alearned%2520for%2520every%2520single%2520epoch.%2520In%2520practice%2520all%2520parameters%2520are%2520often%2520trained%250Asimultaneously%252C%2520ignoring%2520the%2520inherent%2520structure%2520suggested%2520by%2520dynamic%250Aprogramming.%2520This%2520paper%2520introduces%2520a%2520combination%2520of%2520dynamic%2520programming%2520and%250Apolicy%2520gradient%2520called%2520dynamic%2520policy%2520gradient%252C%2520where%2520the%2520parameters%2520are%250Atrained%2520backwards%2520in%2520time.%2520For%2520the%2520tabular%2520softmax%2520parametrisation%2520we%2520carry%2520out%250Athe%2520convergence%2520analysis%2520for%2520simultaneous%2520and%2520dynamic%2520policy%2520gradient%2520towards%250Aglobal%2520optima%252C%2520both%2520in%2520the%2520exact%2520and%2520sampled%2520gradient%2520settings%2520without%250Aregularisation.%2520It%2520turns%2520out%2520that%2520the%2520use%2520of%2520dynamic%2520policy%2520gradient%2520training%250Amuch%2520better%2520exploits%2520the%2520structure%2520of%2520finite-%2520time%2520problems%2520which%2520is%2520reflected%250Ain%2520improved%2520convergence%2520bounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02671v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Stationarity%3A%20Convergence%20Analysis%20of%20Stochastic%20Softmax%20Policy%0A%20%20Gradient%20Methods&entry.906535625=Sara%20Klein%20and%20Simon%20Weissmann%20and%20Leif%20D%C3%B6ring&entry.1292438233=%20%20Markov%20Decision%20Processes%20%28MDPs%29%20are%20a%20formal%20framework%20for%20modeling%20and%0Asolving%20sequential%20decision-making%20problems.%20In%20finite-time%20horizons%20such%0Aproblems%20are%20relevant%20for%20instance%20for%20optimal%20stopping%20or%20specific%20supply%0Achain%20problems%2C%20but%20also%20in%20the%20training%20of%20large%20language%20models.%20In%20contrast%0Ato%20infinite%20horizon%20MDPs%20optimal%20policies%20are%20not%20stationary%2C%20policies%20must%20be%0Alearned%20for%20every%20single%20epoch.%20In%20practice%20all%20parameters%20are%20often%20trained%0Asimultaneously%2C%20ignoring%20the%20inherent%20structure%20suggested%20by%20dynamic%0Aprogramming.%20This%20paper%20introduces%20a%20combination%20of%20dynamic%20programming%20and%0Apolicy%20gradient%20called%20dynamic%20policy%20gradient%2C%20where%20the%20parameters%20are%0Atrained%20backwards%20in%20time.%20For%20the%20tabular%20softmax%20parametrisation%20we%20carry%20out%0Athe%20convergence%20analysis%20for%20simultaneous%20and%20dynamic%20policy%20gradient%20towards%0Aglobal%20optima%2C%20both%20in%20the%20exact%20and%20sampled%20gradient%20settings%20without%0Aregularisation.%20It%20turns%20out%20that%20the%20use%20of%20dynamic%20policy%20gradient%20training%0Amuch%20better%20exploits%20the%20structure%20of%20finite-%20time%20problems%20which%20is%20reflected%0Ain%20improved%20convergence%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02671v2&entry.124074799=Read"},
{"title": "Federated Learning Privacy: Attacks, Defenses, Applications, and Policy\n  Landscape - A Survey", "author": "Joshua C. Zhao and Saurabh Bagchi and Salman Avestimehr and Kevin S. Chan and Somali Chaterji and Dimitris Dimitriadis and Jiacheng Li and Ninghui Li and Arash Nourian and Holger R. Roth", "abstract": "  Deep learning has shown incredible potential across a vast array of tasks and\naccompanying this growth has been an insatiable appetite for data. However, a\nlarge amount of data needed for enabling deep learning is stored on personal\ndevices and recent concerns on privacy have further highlighted challenges for\naccessing such data. As a result, federated learning (FL) has emerged as an\nimportant privacy-preserving technology enabling collaborative training of\nmachine learning models without the need to send the raw, potentially\nsensitive, data to a central server. However, the fundamental premise that\nsending model updates to a server is privacy-preserving only holds if the\nupdates cannot be \"reverse engineered\" to infer information about the private\ntraining data. It has been shown under a wide variety of settings that this\npremise for privacy does {\\em not} hold.\n  In this survey paper, we provide a comprehensive literature review of the\ndifferent privacy attacks and defense methods in FL. We identify the current\nlimitations of these attacks and highlight the settings in which FL client\nprivacy can be broken. We dissect some of the successful industry applications\nof FL and draw lessons for future successful adoption. We survey the emerging\nlandscape of privacy regulation for FL. We conclude with future directions for\ntaking FL toward the cherished goal of generating accurate models while\npreserving the privacy of the data from its participants.\n", "link": "http://arxiv.org/abs/2405.03636v1", "date": "2024-05-06", "relevancy": 1.8894, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4998}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4589}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Learning%20Privacy%3A%20Attacks%2C%20Defenses%2C%20Applications%2C%20and%20Policy%0A%20%20Landscape%20-%20A%20Survey&body=Title%3A%20Federated%20Learning%20Privacy%3A%20Attacks%2C%20Defenses%2C%20Applications%2C%20and%20Policy%0A%20%20Landscape%20-%20A%20Survey%0AAuthor%3A%20Joshua%20C.%20Zhao%20and%20Saurabh%20Bagchi%20and%20Salman%20Avestimehr%20and%20Kevin%20S.%20Chan%20and%20Somali%20Chaterji%20and%20Dimitris%20Dimitriadis%20and%20Jiacheng%20Li%20and%20Ninghui%20Li%20and%20Arash%20Nourian%20and%20Holger%20R.%20Roth%0AAbstract%3A%20%20%20Deep%20learning%20has%20shown%20incredible%20potential%20across%20a%20vast%20array%20of%20tasks%20and%0Aaccompanying%20this%20growth%20has%20been%20an%20insatiable%20appetite%20for%20data.%20However%2C%20a%0Alarge%20amount%20of%20data%20needed%20for%20enabling%20deep%20learning%20is%20stored%20on%20personal%0Adevices%20and%20recent%20concerns%20on%20privacy%20have%20further%20highlighted%20challenges%20for%0Aaccessing%20such%20data.%20As%20a%20result%2C%20federated%20learning%20%28FL%29%20has%20emerged%20as%20an%0Aimportant%20privacy-preserving%20technology%20enabling%20collaborative%20training%20of%0Amachine%20learning%20models%20without%20the%20need%20to%20send%20the%20raw%2C%20potentially%0Asensitive%2C%20data%20to%20a%20central%20server.%20However%2C%20the%20fundamental%20premise%20that%0Asending%20model%20updates%20to%20a%20server%20is%20privacy-preserving%20only%20holds%20if%20the%0Aupdates%20cannot%20be%20%22reverse%20engineered%22%20to%20infer%20information%20about%20the%20private%0Atraining%20data.%20It%20has%20been%20shown%20under%20a%20wide%20variety%20of%20settings%20that%20this%0Apremise%20for%20privacy%20does%20%7B%5Cem%20not%7D%20hold.%0A%20%20In%20this%20survey%20paper%2C%20we%20provide%20a%20comprehensive%20literature%20review%20of%20the%0Adifferent%20privacy%20attacks%20and%20defense%20methods%20in%20FL.%20We%20identify%20the%20current%0Alimitations%20of%20these%20attacks%20and%20highlight%20the%20settings%20in%20which%20FL%20client%0Aprivacy%20can%20be%20broken.%20We%20dissect%20some%20of%20the%20successful%20industry%20applications%0Aof%20FL%20and%20draw%20lessons%20for%20future%20successful%20adoption.%20We%20survey%20the%20emerging%0Alandscape%20of%20privacy%20regulation%20for%20FL.%20We%20conclude%20with%20future%20directions%20for%0Ataking%20FL%20toward%20the%20cherished%20goal%20of%20generating%20accurate%20models%20while%0Apreserving%20the%20privacy%20of%20the%20data%20from%20its%20participants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Learning%2520Privacy%253A%2520Attacks%252C%2520Defenses%252C%2520Applications%252C%2520and%2520Policy%250A%2520%2520Landscape%2520-%2520A%2520Survey%26entry.906535625%3DJoshua%2520C.%2520Zhao%2520and%2520Saurabh%2520Bagchi%2520and%2520Salman%2520Avestimehr%2520and%2520Kevin%2520S.%2520Chan%2520and%2520Somali%2520Chaterji%2520and%2520Dimitris%2520Dimitriadis%2520and%2520Jiacheng%2520Li%2520and%2520Ninghui%2520Li%2520and%2520Arash%2520Nourian%2520and%2520Holger%2520R.%2520Roth%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520shown%2520incredible%2520potential%2520across%2520a%2520vast%2520array%2520of%2520tasks%2520and%250Aaccompanying%2520this%2520growth%2520has%2520been%2520an%2520insatiable%2520appetite%2520for%2520data.%2520However%252C%2520a%250Alarge%2520amount%2520of%2520data%2520needed%2520for%2520enabling%2520deep%2520learning%2520is%2520stored%2520on%2520personal%250Adevices%2520and%2520recent%2520concerns%2520on%2520privacy%2520have%2520further%2520highlighted%2520challenges%2520for%250Aaccessing%2520such%2520data.%2520As%2520a%2520result%252C%2520federated%2520learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520an%250Aimportant%2520privacy-preserving%2520technology%2520enabling%2520collaborative%2520training%2520of%250Amachine%2520learning%2520models%2520without%2520the%2520need%2520to%2520send%2520the%2520raw%252C%2520potentially%250Asensitive%252C%2520data%2520to%2520a%2520central%2520server.%2520However%252C%2520the%2520fundamental%2520premise%2520that%250Asending%2520model%2520updates%2520to%2520a%2520server%2520is%2520privacy-preserving%2520only%2520holds%2520if%2520the%250Aupdates%2520cannot%2520be%2520%2522reverse%2520engineered%2522%2520to%2520infer%2520information%2520about%2520the%2520private%250Atraining%2520data.%2520It%2520has%2520been%2520shown%2520under%2520a%2520wide%2520variety%2520of%2520settings%2520that%2520this%250Apremise%2520for%2520privacy%2520does%2520%257B%255Cem%2520not%257D%2520hold.%250A%2520%2520In%2520this%2520survey%2520paper%252C%2520we%2520provide%2520a%2520comprehensive%2520literature%2520review%2520of%2520the%250Adifferent%2520privacy%2520attacks%2520and%2520defense%2520methods%2520in%2520FL.%2520We%2520identify%2520the%2520current%250Alimitations%2520of%2520these%2520attacks%2520and%2520highlight%2520the%2520settings%2520in%2520which%2520FL%2520client%250Aprivacy%2520can%2520be%2520broken.%2520We%2520dissect%2520some%2520of%2520the%2520successful%2520industry%2520applications%250Aof%2520FL%2520and%2520draw%2520lessons%2520for%2520future%2520successful%2520adoption.%2520We%2520survey%2520the%2520emerging%250Alandscape%2520of%2520privacy%2520regulation%2520for%2520FL.%2520We%2520conclude%2520with%2520future%2520directions%2520for%250Ataking%2520FL%2520toward%2520the%2520cherished%2520goal%2520of%2520generating%2520accurate%2520models%2520while%250Apreserving%2520the%2520privacy%2520of%2520the%2520data%2520from%2520its%2520participants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20Privacy%3A%20Attacks%2C%20Defenses%2C%20Applications%2C%20and%20Policy%0A%20%20Landscape%20-%20A%20Survey&entry.906535625=Joshua%20C.%20Zhao%20and%20Saurabh%20Bagchi%20and%20Salman%20Avestimehr%20and%20Kevin%20S.%20Chan%20and%20Somali%20Chaterji%20and%20Dimitris%20Dimitriadis%20and%20Jiacheng%20Li%20and%20Ninghui%20Li%20and%20Arash%20Nourian%20and%20Holger%20R.%20Roth&entry.1292438233=%20%20Deep%20learning%20has%20shown%20incredible%20potential%20across%20a%20vast%20array%20of%20tasks%20and%0Aaccompanying%20this%20growth%20has%20been%20an%20insatiable%20appetite%20for%20data.%20However%2C%20a%0Alarge%20amount%20of%20data%20needed%20for%20enabling%20deep%20learning%20is%20stored%20on%20personal%0Adevices%20and%20recent%20concerns%20on%20privacy%20have%20further%20highlighted%20challenges%20for%0Aaccessing%20such%20data.%20As%20a%20result%2C%20federated%20learning%20%28FL%29%20has%20emerged%20as%20an%0Aimportant%20privacy-preserving%20technology%20enabling%20collaborative%20training%20of%0Amachine%20learning%20models%20without%20the%20need%20to%20send%20the%20raw%2C%20potentially%0Asensitive%2C%20data%20to%20a%20central%20server.%20However%2C%20the%20fundamental%20premise%20that%0Asending%20model%20updates%20to%20a%20server%20is%20privacy-preserving%20only%20holds%20if%20the%0Aupdates%20cannot%20be%20%22reverse%20engineered%22%20to%20infer%20information%20about%20the%20private%0Atraining%20data.%20It%20has%20been%20shown%20under%20a%20wide%20variety%20of%20settings%20that%20this%0Apremise%20for%20privacy%20does%20%7B%5Cem%20not%7D%20hold.%0A%20%20In%20this%20survey%20paper%2C%20we%20provide%20a%20comprehensive%20literature%20review%20of%20the%0Adifferent%20privacy%20attacks%20and%20defense%20methods%20in%20FL.%20We%20identify%20the%20current%0Alimitations%20of%20these%20attacks%20and%20highlight%20the%20settings%20in%20which%20FL%20client%0Aprivacy%20can%20be%20broken.%20We%20dissect%20some%20of%20the%20successful%20industry%20applications%0Aof%20FL%20and%20draw%20lessons%20for%20future%20successful%20adoption.%20We%20survey%20the%20emerging%0Alandscape%20of%20privacy%20regulation%20for%20FL.%20We%20conclude%20with%20future%20directions%20for%0Ataking%20FL%20toward%20the%20cherished%20goal%20of%20generating%20accurate%20models%20while%0Apreserving%20the%20privacy%20of%20the%20data%20from%20its%20participants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03636v1&entry.124074799=Read"},
{"title": "Snake Learning: A Communication- and Computation-Efficient Distributed\n  Learning Framework for 6G", "author": "Xiaoxue Yu and Xingfu Yi and Rongpeng Li and Fei Wang and Chenghui Peng and Zhifeng Zhao and Honggang Zhang", "abstract": "  In the evolution towards 6G, integrating Artificial Intelligence (AI) with\nadvanced network infrastructure emerges as a pivotal strategy for enhancing\nnetwork intelligence and resource utilization. Existing distributed learning\nframeworks like Federated Learning and Split Learning often struggle with\nsignificant challenges in dynamic network environments including high\nsynchronization demands, costly communication overheads, severe computing\nresource consumption, and data heterogeneity across network nodes. These\nobstacles hinder the applications of ubiquitous computing capabilities of 6G\nnetworks, especially in light of the trend of escalating model parameters and\ntraining data volumes. To address these challenges effectively, this paper\nintroduces \"Snake Learning\", a cost-effective distributed learning framework.\nSpecifically, Snake Learning respects the heterogeneity of inter-node computing\ncapability and local data distribution in 6G networks, and sequentially trains\nthe designated part of model layers on individual nodes. This layer-by-layer\nserpentine update mechanism contributes to significantly reducing the\nrequirements for storage, memory and communication during the model training\nphase, and demonstrates superior adaptability and efficiency for both Computer\nVision (CV) training and Large Language Model (LLM) fine-tuning tasks across\nhomogeneous and heterogeneous data distributions.\n", "link": "http://arxiv.org/abs/2405.03372v1", "date": "2024-05-06", "relevancy": 1.887, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4784}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4725}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Snake%20Learning%3A%20A%20Communication-%20and%20Computation-Efficient%20Distributed%0A%20%20Learning%20Framework%20for%206G&body=Title%3A%20Snake%20Learning%3A%20A%20Communication-%20and%20Computation-Efficient%20Distributed%0A%20%20Learning%20Framework%20for%206G%0AAuthor%3A%20Xiaoxue%20Yu%20and%20Xingfu%20Yi%20and%20Rongpeng%20Li%20and%20Fei%20Wang%20and%20Chenghui%20Peng%20and%20Zhifeng%20Zhao%20and%20Honggang%20Zhang%0AAbstract%3A%20%20%20In%20the%20evolution%20towards%206G%2C%20integrating%20Artificial%20Intelligence%20%28AI%29%20with%0Aadvanced%20network%20infrastructure%20emerges%20as%20a%20pivotal%20strategy%20for%20enhancing%0Anetwork%20intelligence%20and%20resource%20utilization.%20Existing%20distributed%20learning%0Aframeworks%20like%20Federated%20Learning%20and%20Split%20Learning%20often%20struggle%20with%0Asignificant%20challenges%20in%20dynamic%20network%20environments%20including%20high%0Asynchronization%20demands%2C%20costly%20communication%20overheads%2C%20severe%20computing%0Aresource%20consumption%2C%20and%20data%20heterogeneity%20across%20network%20nodes.%20These%0Aobstacles%20hinder%20the%20applications%20of%20ubiquitous%20computing%20capabilities%20of%206G%0Anetworks%2C%20especially%20in%20light%20of%20the%20trend%20of%20escalating%20model%20parameters%20and%0Atraining%20data%20volumes.%20To%20address%20these%20challenges%20effectively%2C%20this%20paper%0Aintroduces%20%22Snake%20Learning%22%2C%20a%20cost-effective%20distributed%20learning%20framework.%0ASpecifically%2C%20Snake%20Learning%20respects%20the%20heterogeneity%20of%20inter-node%20computing%0Acapability%20and%20local%20data%20distribution%20in%206G%20networks%2C%20and%20sequentially%20trains%0Athe%20designated%20part%20of%20model%20layers%20on%20individual%20nodes.%20This%20layer-by-layer%0Aserpentine%20update%20mechanism%20contributes%20to%20significantly%20reducing%20the%0Arequirements%20for%20storage%2C%20memory%20and%20communication%20during%20the%20model%20training%0Aphase%2C%20and%20demonstrates%20superior%20adaptability%20and%20efficiency%20for%20both%20Computer%0AVision%20%28CV%29%20training%20and%20Large%20Language%20Model%20%28LLM%29%20fine-tuning%20tasks%20across%0Ahomogeneous%20and%20heterogeneous%20data%20distributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSnake%2520Learning%253A%2520A%2520Communication-%2520and%2520Computation-Efficient%2520Distributed%250A%2520%2520Learning%2520Framework%2520for%25206G%26entry.906535625%3DXiaoxue%2520Yu%2520and%2520Xingfu%2520Yi%2520and%2520Rongpeng%2520Li%2520and%2520Fei%2520Wang%2520and%2520Chenghui%2520Peng%2520and%2520Zhifeng%2520Zhao%2520and%2520Honggang%2520Zhang%26entry.1292438233%3D%2520%2520In%2520the%2520evolution%2520towards%25206G%252C%2520integrating%2520Artificial%2520Intelligence%2520%2528AI%2529%2520with%250Aadvanced%2520network%2520infrastructure%2520emerges%2520as%2520a%2520pivotal%2520strategy%2520for%2520enhancing%250Anetwork%2520intelligence%2520and%2520resource%2520utilization.%2520Existing%2520distributed%2520learning%250Aframeworks%2520like%2520Federated%2520Learning%2520and%2520Split%2520Learning%2520often%2520struggle%2520with%250Asignificant%2520challenges%2520in%2520dynamic%2520network%2520environments%2520including%2520high%250Asynchronization%2520demands%252C%2520costly%2520communication%2520overheads%252C%2520severe%2520computing%250Aresource%2520consumption%252C%2520and%2520data%2520heterogeneity%2520across%2520network%2520nodes.%2520These%250Aobstacles%2520hinder%2520the%2520applications%2520of%2520ubiquitous%2520computing%2520capabilities%2520of%25206G%250Anetworks%252C%2520especially%2520in%2520light%2520of%2520the%2520trend%2520of%2520escalating%2520model%2520parameters%2520and%250Atraining%2520data%2520volumes.%2520To%2520address%2520these%2520challenges%2520effectively%252C%2520this%2520paper%250Aintroduces%2520%2522Snake%2520Learning%2522%252C%2520a%2520cost-effective%2520distributed%2520learning%2520framework.%250ASpecifically%252C%2520Snake%2520Learning%2520respects%2520the%2520heterogeneity%2520of%2520inter-node%2520computing%250Acapability%2520and%2520local%2520data%2520distribution%2520in%25206G%2520networks%252C%2520and%2520sequentially%2520trains%250Athe%2520designated%2520part%2520of%2520model%2520layers%2520on%2520individual%2520nodes.%2520This%2520layer-by-layer%250Aserpentine%2520update%2520mechanism%2520contributes%2520to%2520significantly%2520reducing%2520the%250Arequirements%2520for%2520storage%252C%2520memory%2520and%2520communication%2520during%2520the%2520model%2520training%250Aphase%252C%2520and%2520demonstrates%2520superior%2520adaptability%2520and%2520efficiency%2520for%2520both%2520Computer%250AVision%2520%2528CV%2529%2520training%2520and%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520fine-tuning%2520tasks%2520across%250Ahomogeneous%2520and%2520heterogeneous%2520data%2520distributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Snake%20Learning%3A%20A%20Communication-%20and%20Computation-Efficient%20Distributed%0A%20%20Learning%20Framework%20for%206G&entry.906535625=Xiaoxue%20Yu%20and%20Xingfu%20Yi%20and%20Rongpeng%20Li%20and%20Fei%20Wang%20and%20Chenghui%20Peng%20and%20Zhifeng%20Zhao%20and%20Honggang%20Zhang&entry.1292438233=%20%20In%20the%20evolution%20towards%206G%2C%20integrating%20Artificial%20Intelligence%20%28AI%29%20with%0Aadvanced%20network%20infrastructure%20emerges%20as%20a%20pivotal%20strategy%20for%20enhancing%0Anetwork%20intelligence%20and%20resource%20utilization.%20Existing%20distributed%20learning%0Aframeworks%20like%20Federated%20Learning%20and%20Split%20Learning%20often%20struggle%20with%0Asignificant%20challenges%20in%20dynamic%20network%20environments%20including%20high%0Asynchronization%20demands%2C%20costly%20communication%20overheads%2C%20severe%20computing%0Aresource%20consumption%2C%20and%20data%20heterogeneity%20across%20network%20nodes.%20These%0Aobstacles%20hinder%20the%20applications%20of%20ubiquitous%20computing%20capabilities%20of%206G%0Anetworks%2C%20especially%20in%20light%20of%20the%20trend%20of%20escalating%20model%20parameters%20and%0Atraining%20data%20volumes.%20To%20address%20these%20challenges%20effectively%2C%20this%20paper%0Aintroduces%20%22Snake%20Learning%22%2C%20a%20cost-effective%20distributed%20learning%20framework.%0ASpecifically%2C%20Snake%20Learning%20respects%20the%20heterogeneity%20of%20inter-node%20computing%0Acapability%20and%20local%20data%20distribution%20in%206G%20networks%2C%20and%20sequentially%20trains%0Athe%20designated%20part%20of%20model%20layers%20on%20individual%20nodes.%20This%20layer-by-layer%0Aserpentine%20update%20mechanism%20contributes%20to%20significantly%20reducing%20the%0Arequirements%20for%20storage%2C%20memory%20and%20communication%20during%20the%20model%20training%0Aphase%2C%20and%20demonstrates%20superior%20adaptability%20and%20efficiency%20for%20both%20Computer%0AVision%20%28CV%29%20training%20and%20Large%20Language%20Model%20%28LLM%29%20fine-tuning%20tasks%20across%0Ahomogeneous%20and%20heterogeneous%20data%20distributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03372v1&entry.124074799=Read"},
{"title": "Why is SAM Robust to Label Noise?", "author": "Christina Baek and Zico Kolter and Aditi Raghunathan", "abstract": "  Sharpness-Aware Minimization (SAM) is most known for achieving state-of\nthe-art performances on natural image and language tasks. However, its most\npronounced improvements (of tens of percent) is rather in the presence of label\nnoise. Understanding SAM's label noise robustness requires a departure from\ncharacterizing the robustness of minimas lying in \"flatter\" regions of the loss\nlandscape. In particular, the peak performance under label noise occurs with\nearly stopping, far before the loss converges. We decompose SAM's robustness\ninto two effects: one induced by changes to the logit term and the other\ninduced by changes to the network Jacobian. The first can be observed in linear\nlogistic regression where SAM provably up-weights the gradient contribution\nfrom clean examples. Although this explicit up-weighting is also observable in\nneural networks, when we intervene and modify SAM to remove this effect,\nsurprisingly, we see no visible degradation in performance. We infer that SAM's\neffect in deeper networks is instead explained entirely by the effect SAM has\non the network Jacobian. We theoretically derive the implicit regularization\ninduced by this Jacobian effect in two layer linear networks. Motivated by our\nanalysis, we see that cheaper alternatives to SAM that explicitly induce these\nregularization effects largely recover the benefits in deep networks trained on\nreal-world datasets.\n", "link": "http://arxiv.org/abs/2405.03676v1", "date": "2024-05-06", "relevancy": 1.8864, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4737}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4722}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20is%20SAM%20Robust%20to%20Label%20Noise%3F&body=Title%3A%20Why%20is%20SAM%20Robust%20to%20Label%20Noise%3F%0AAuthor%3A%20Christina%20Baek%20and%20Zico%20Kolter%20and%20Aditi%20Raghunathan%0AAbstract%3A%20%20%20Sharpness-Aware%20Minimization%20%28SAM%29%20is%20most%20known%20for%20achieving%20state-of%0Athe-art%20performances%20on%20natural%20image%20and%20language%20tasks.%20However%2C%20its%20most%0Apronounced%20improvements%20%28of%20tens%20of%20percent%29%20is%20rather%20in%20the%20presence%20of%20label%0Anoise.%20Understanding%20SAM%27s%20label%20noise%20robustness%20requires%20a%20departure%20from%0Acharacterizing%20the%20robustness%20of%20minimas%20lying%20in%20%22flatter%22%20regions%20of%20the%20loss%0Alandscape.%20In%20particular%2C%20the%20peak%20performance%20under%20label%20noise%20occurs%20with%0Aearly%20stopping%2C%20far%20before%20the%20loss%20converges.%20We%20decompose%20SAM%27s%20robustness%0Ainto%20two%20effects%3A%20one%20induced%20by%20changes%20to%20the%20logit%20term%20and%20the%20other%0Ainduced%20by%20changes%20to%20the%20network%20Jacobian.%20The%20first%20can%20be%20observed%20in%20linear%0Alogistic%20regression%20where%20SAM%20provably%20up-weights%20the%20gradient%20contribution%0Afrom%20clean%20examples.%20Although%20this%20explicit%20up-weighting%20is%20also%20observable%20in%0Aneural%20networks%2C%20when%20we%20intervene%20and%20modify%20SAM%20to%20remove%20this%20effect%2C%0Asurprisingly%2C%20we%20see%20no%20visible%20degradation%20in%20performance.%20We%20infer%20that%20SAM%27s%0Aeffect%20in%20deeper%20networks%20is%20instead%20explained%20entirely%20by%20the%20effect%20SAM%20has%0Aon%20the%20network%20Jacobian.%20We%20theoretically%20derive%20the%20implicit%20regularization%0Ainduced%20by%20this%20Jacobian%20effect%20in%20two%20layer%20linear%20networks.%20Motivated%20by%20our%0Aanalysis%2C%20we%20see%20that%20cheaper%20alternatives%20to%20SAM%20that%20explicitly%20induce%20these%0Aregularization%20effects%20largely%20recover%20the%20benefits%20in%20deep%20networks%20trained%20on%0Areal-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520is%2520SAM%2520Robust%2520to%2520Label%2520Noise%253F%26entry.906535625%3DChristina%2520Baek%2520and%2520Zico%2520Kolter%2520and%2520Aditi%2520Raghunathan%26entry.1292438233%3D%2520%2520Sharpness-Aware%2520Minimization%2520%2528SAM%2529%2520is%2520most%2520known%2520for%2520achieving%2520state-of%250Athe-art%2520performances%2520on%2520natural%2520image%2520and%2520language%2520tasks.%2520However%252C%2520its%2520most%250Apronounced%2520improvements%2520%2528of%2520tens%2520of%2520percent%2529%2520is%2520rather%2520in%2520the%2520presence%2520of%2520label%250Anoise.%2520Understanding%2520SAM%2527s%2520label%2520noise%2520robustness%2520requires%2520a%2520departure%2520from%250Acharacterizing%2520the%2520robustness%2520of%2520minimas%2520lying%2520in%2520%2522flatter%2522%2520regions%2520of%2520the%2520loss%250Alandscape.%2520In%2520particular%252C%2520the%2520peak%2520performance%2520under%2520label%2520noise%2520occurs%2520with%250Aearly%2520stopping%252C%2520far%2520before%2520the%2520loss%2520converges.%2520We%2520decompose%2520SAM%2527s%2520robustness%250Ainto%2520two%2520effects%253A%2520one%2520induced%2520by%2520changes%2520to%2520the%2520logit%2520term%2520and%2520the%2520other%250Ainduced%2520by%2520changes%2520to%2520the%2520network%2520Jacobian.%2520The%2520first%2520can%2520be%2520observed%2520in%2520linear%250Alogistic%2520regression%2520where%2520SAM%2520provably%2520up-weights%2520the%2520gradient%2520contribution%250Afrom%2520clean%2520examples.%2520Although%2520this%2520explicit%2520up-weighting%2520is%2520also%2520observable%2520in%250Aneural%2520networks%252C%2520when%2520we%2520intervene%2520and%2520modify%2520SAM%2520to%2520remove%2520this%2520effect%252C%250Asurprisingly%252C%2520we%2520see%2520no%2520visible%2520degradation%2520in%2520performance.%2520We%2520infer%2520that%2520SAM%2527s%250Aeffect%2520in%2520deeper%2520networks%2520is%2520instead%2520explained%2520entirely%2520by%2520the%2520effect%2520SAM%2520has%250Aon%2520the%2520network%2520Jacobian.%2520We%2520theoretically%2520derive%2520the%2520implicit%2520regularization%250Ainduced%2520by%2520this%2520Jacobian%2520effect%2520in%2520two%2520layer%2520linear%2520networks.%2520Motivated%2520by%2520our%250Aanalysis%252C%2520we%2520see%2520that%2520cheaper%2520alternatives%2520to%2520SAM%2520that%2520explicitly%2520induce%2520these%250Aregularization%2520effects%2520largely%2520recover%2520the%2520benefits%2520in%2520deep%2520networks%2520trained%2520on%250Areal-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20is%20SAM%20Robust%20to%20Label%20Noise%3F&entry.906535625=Christina%20Baek%20and%20Zico%20Kolter%20and%20Aditi%20Raghunathan&entry.1292438233=%20%20Sharpness-Aware%20Minimization%20%28SAM%29%20is%20most%20known%20for%20achieving%20state-of%0Athe-art%20performances%20on%20natural%20image%20and%20language%20tasks.%20However%2C%20its%20most%0Apronounced%20improvements%20%28of%20tens%20of%20percent%29%20is%20rather%20in%20the%20presence%20of%20label%0Anoise.%20Understanding%20SAM%27s%20label%20noise%20robustness%20requires%20a%20departure%20from%0Acharacterizing%20the%20robustness%20of%20minimas%20lying%20in%20%22flatter%22%20regions%20of%20the%20loss%0Alandscape.%20In%20particular%2C%20the%20peak%20performance%20under%20label%20noise%20occurs%20with%0Aearly%20stopping%2C%20far%20before%20the%20loss%20converges.%20We%20decompose%20SAM%27s%20robustness%0Ainto%20two%20effects%3A%20one%20induced%20by%20changes%20to%20the%20logit%20term%20and%20the%20other%0Ainduced%20by%20changes%20to%20the%20network%20Jacobian.%20The%20first%20can%20be%20observed%20in%20linear%0Alogistic%20regression%20where%20SAM%20provably%20up-weights%20the%20gradient%20contribution%0Afrom%20clean%20examples.%20Although%20this%20explicit%20up-weighting%20is%20also%20observable%20in%0Aneural%20networks%2C%20when%20we%20intervene%20and%20modify%20SAM%20to%20remove%20this%20effect%2C%0Asurprisingly%2C%20we%20see%20no%20visible%20degradation%20in%20performance.%20We%20infer%20that%20SAM%27s%0Aeffect%20in%20deeper%20networks%20is%20instead%20explained%20entirely%20by%20the%20effect%20SAM%20has%0Aon%20the%20network%20Jacobian.%20We%20theoretically%20derive%20the%20implicit%20regularization%0Ainduced%20by%20this%20Jacobian%20effect%20in%20two%20layer%20linear%20networks.%20Motivated%20by%20our%0Aanalysis%2C%20we%20see%20that%20cheaper%20alternatives%20to%20SAM%20that%20explicitly%20induce%20these%0Aregularization%20effects%20largely%20recover%20the%20benefits%20in%20deep%20networks%20trained%20on%0Areal-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03676v1&entry.124074799=Read"},
{"title": "GI-SMN: Gradient Inversion Attack against Federated Learning without\n  Prior Knowledge", "author": "Jin Qian and Kaimin Wei and Yongdong Wu and Jilian Zhang and Jipeng Chen and Huan Bao", "abstract": "  Federated learning (FL) has emerged as a privacy-preserving machine learning\napproach where multiple parties share gradient information rather than original\nuser data. Recent work has demonstrated that gradient inversion attacks can\nexploit the gradients of FL to recreate the original user data, posing\nsignificant privacy risks. However, these attacks make strong assumptions about\nthe attacker, such as altering the model structure or parameters, gaining batch\nnormalization statistics, or acquiring prior knowledge of the original training\nset, etc. Consequently, these attacks are not possible in real-world scenarios.\nTo end it, we propose a novel Gradient Inversion attack based on Style\nMigration Network (GI-SMN), which breaks through the strong assumptions made by\nprevious gradient inversion attacks. The optimization space is reduced by the\nrefinement of the latent code and the use of regular terms to facilitate\ngradient matching. GI-SMN enables the reconstruction of user data with high\nsimilarity in batches. Experimental results have demonstrated that GI-SMN\noutperforms state-of-the-art gradient inversion attacks in both visual effect\nand similarity metrics. Additionally, it also can overcome gradient pruning and\ndifferential privacy defenses.\n", "link": "http://arxiv.org/abs/2405.03516v1", "date": "2024-05-06", "relevancy": 1.4402, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4857}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4747}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GI-SMN%3A%20Gradient%20Inversion%20Attack%20against%20Federated%20Learning%20without%0A%20%20Prior%20Knowledge&body=Title%3A%20GI-SMN%3A%20Gradient%20Inversion%20Attack%20against%20Federated%20Learning%20without%0A%20%20Prior%20Knowledge%0AAuthor%3A%20Jin%20Qian%20and%20Kaimin%20Wei%20and%20Yongdong%20Wu%20and%20Jilian%20Zhang%20and%20Jipeng%20Chen%20and%20Huan%20Bao%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20has%20emerged%20as%20a%20privacy-preserving%20machine%20learning%0Aapproach%20where%20multiple%20parties%20share%20gradient%20information%20rather%20than%20original%0Auser%20data.%20Recent%20work%20has%20demonstrated%20that%20gradient%20inversion%20attacks%20can%0Aexploit%20the%20gradients%20of%20FL%20to%20recreate%20the%20original%20user%20data%2C%20posing%0Asignificant%20privacy%20risks.%20However%2C%20these%20attacks%20make%20strong%20assumptions%20about%0Athe%20attacker%2C%20such%20as%20altering%20the%20model%20structure%20or%20parameters%2C%20gaining%20batch%0Anormalization%20statistics%2C%20or%20acquiring%20prior%20knowledge%20of%20the%20original%20training%0Aset%2C%20etc.%20Consequently%2C%20these%20attacks%20are%20not%20possible%20in%20real-world%20scenarios.%0ATo%20end%20it%2C%20we%20propose%20a%20novel%20Gradient%20Inversion%20attack%20based%20on%20Style%0AMigration%20Network%20%28GI-SMN%29%2C%20which%20breaks%20through%20the%20strong%20assumptions%20made%20by%0Aprevious%20gradient%20inversion%20attacks.%20The%20optimization%20space%20is%20reduced%20by%20the%0Arefinement%20of%20the%20latent%20code%20and%20the%20use%20of%20regular%20terms%20to%20facilitate%0Agradient%20matching.%20GI-SMN%20enables%20the%20reconstruction%20of%20user%20data%20with%20high%0Asimilarity%20in%20batches.%20Experimental%20results%20have%20demonstrated%20that%20GI-SMN%0Aoutperforms%20state-of-the-art%20gradient%20inversion%20attacks%20in%20both%20visual%20effect%0Aand%20similarity%20metrics.%20Additionally%2C%20it%20also%20can%20overcome%20gradient%20pruning%20and%0Adifferential%20privacy%20defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGI-SMN%253A%2520Gradient%2520Inversion%2520Attack%2520against%2520Federated%2520Learning%2520without%250A%2520%2520Prior%2520Knowledge%26entry.906535625%3DJin%2520Qian%2520and%2520Kaimin%2520Wei%2520and%2520Yongdong%2520Wu%2520and%2520Jilian%2520Zhang%2520and%2520Jipeng%2520Chen%2520and%2520Huan%2520Bao%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520a%2520privacy-preserving%2520machine%2520learning%250Aapproach%2520where%2520multiple%2520parties%2520share%2520gradient%2520information%2520rather%2520than%2520original%250Auser%2520data.%2520Recent%2520work%2520has%2520demonstrated%2520that%2520gradient%2520inversion%2520attacks%2520can%250Aexploit%2520the%2520gradients%2520of%2520FL%2520to%2520recreate%2520the%2520original%2520user%2520data%252C%2520posing%250Asignificant%2520privacy%2520risks.%2520However%252C%2520these%2520attacks%2520make%2520strong%2520assumptions%2520about%250Athe%2520attacker%252C%2520such%2520as%2520altering%2520the%2520model%2520structure%2520or%2520parameters%252C%2520gaining%2520batch%250Anormalization%2520statistics%252C%2520or%2520acquiring%2520prior%2520knowledge%2520of%2520the%2520original%2520training%250Aset%252C%2520etc.%2520Consequently%252C%2520these%2520attacks%2520are%2520not%2520possible%2520in%2520real-world%2520scenarios.%250ATo%2520end%2520it%252C%2520we%2520propose%2520a%2520novel%2520Gradient%2520Inversion%2520attack%2520based%2520on%2520Style%250AMigration%2520Network%2520%2528GI-SMN%2529%252C%2520which%2520breaks%2520through%2520the%2520strong%2520assumptions%2520made%2520by%250Aprevious%2520gradient%2520inversion%2520attacks.%2520The%2520optimization%2520space%2520is%2520reduced%2520by%2520the%250Arefinement%2520of%2520the%2520latent%2520code%2520and%2520the%2520use%2520of%2520regular%2520terms%2520to%2520facilitate%250Agradient%2520matching.%2520GI-SMN%2520enables%2520the%2520reconstruction%2520of%2520user%2520data%2520with%2520high%250Asimilarity%2520in%2520batches.%2520Experimental%2520results%2520have%2520demonstrated%2520that%2520GI-SMN%250Aoutperforms%2520state-of-the-art%2520gradient%2520inversion%2520attacks%2520in%2520both%2520visual%2520effect%250Aand%2520similarity%2520metrics.%2520Additionally%252C%2520it%2520also%2520can%2520overcome%2520gradient%2520pruning%2520and%250Adifferential%2520privacy%2520defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GI-SMN%3A%20Gradient%20Inversion%20Attack%20against%20Federated%20Learning%20without%0A%20%20Prior%20Knowledge&entry.906535625=Jin%20Qian%20and%20Kaimin%20Wei%20and%20Yongdong%20Wu%20and%20Jilian%20Zhang%20and%20Jipeng%20Chen%20and%20Huan%20Bao&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20has%20emerged%20as%20a%20privacy-preserving%20machine%20learning%0Aapproach%20where%20multiple%20parties%20share%20gradient%20information%20rather%20than%20original%0Auser%20data.%20Recent%20work%20has%20demonstrated%20that%20gradient%20inversion%20attacks%20can%0Aexploit%20the%20gradients%20of%20FL%20to%20recreate%20the%20original%20user%20data%2C%20posing%0Asignificant%20privacy%20risks.%20However%2C%20these%20attacks%20make%20strong%20assumptions%20about%0Athe%20attacker%2C%20such%20as%20altering%20the%20model%20structure%20or%20parameters%2C%20gaining%20batch%0Anormalization%20statistics%2C%20or%20acquiring%20prior%20knowledge%20of%20the%20original%20training%0Aset%2C%20etc.%20Consequently%2C%20these%20attacks%20are%20not%20possible%20in%20real-world%20scenarios.%0ATo%20end%20it%2C%20we%20propose%20a%20novel%20Gradient%20Inversion%20attack%20based%20on%20Style%0AMigration%20Network%20%28GI-SMN%29%2C%20which%20breaks%20through%20the%20strong%20assumptions%20made%20by%0Aprevious%20gradient%20inversion%20attacks.%20The%20optimization%20space%20is%20reduced%20by%20the%0Arefinement%20of%20the%20latent%20code%20and%20the%20use%20of%20regular%20terms%20to%20facilitate%0Agradient%20matching.%20GI-SMN%20enables%20the%20reconstruction%20of%20user%20data%20with%20high%0Asimilarity%20in%20batches.%20Experimental%20results%20have%20demonstrated%20that%20GI-SMN%0Aoutperforms%20state-of-the-art%20gradient%20inversion%20attacks%20in%20both%20visual%20effect%0Aand%20similarity%20metrics.%20Additionally%2C%20it%20also%20can%20overcome%20gradient%20pruning%20and%0Adifferential%20privacy%20defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03516v1&entry.124074799=Read"},
{"title": "$\u03b5$-Policy Gradient for Online Pricing", "author": "Lukasz Szpruch and Tanut Treetanthiploet and Yufei Zhang", "abstract": "  Combining model-based and model-free reinforcement learning approaches, this\npaper proposes and analyzes an $\\epsilon$-policy gradient algorithm for the\nonline pricing learning task. The algorithm extends $\\epsilon$-greedy algorithm\nby replacing greedy exploitation with gradient descent step and facilitates\nlearning via model inference. We optimize the regret of the proposed algorithm\nby quantifying the exploration cost in terms of the exploration probability\n$\\epsilon$ and the exploitation cost in terms of the gradient descent\noptimization and gradient estimation errors. The algorithm achieves an expected\nregret of order $\\mathcal{O}(\\sqrt{T})$ (up to a logarithmic factor) over $T$\ntrials.\n", "link": "http://arxiv.org/abs/2405.03624v1", "date": "2024-05-06", "relevancy": 1.6881, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4365}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.433}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%CE%B5%24-Policy%20Gradient%20for%20Online%20Pricing&body=Title%3A%20%24%CE%B5%24-Policy%20Gradient%20for%20Online%20Pricing%0AAuthor%3A%20Lukasz%20Szpruch%20and%20Tanut%20Treetanthiploet%20and%20Yufei%20Zhang%0AAbstract%3A%20%20%20Combining%20model-based%20and%20model-free%20reinforcement%20learning%20approaches%2C%20this%0Apaper%20proposes%20and%20analyzes%20an%20%24%5Cepsilon%24-policy%20gradient%20algorithm%20for%20the%0Aonline%20pricing%20learning%20task.%20The%20algorithm%20extends%20%24%5Cepsilon%24-greedy%20algorithm%0Aby%20replacing%20greedy%20exploitation%20with%20gradient%20descent%20step%20and%20facilitates%0Alearning%20via%20model%20inference.%20We%20optimize%20the%20regret%20of%20the%20proposed%20algorithm%0Aby%20quantifying%20the%20exploration%20cost%20in%20terms%20of%20the%20exploration%20probability%0A%24%5Cepsilon%24%20and%20the%20exploitation%20cost%20in%20terms%20of%20the%20gradient%20descent%0Aoptimization%20and%20gradient%20estimation%20errors.%20The%20algorithm%20achieves%20an%20expected%0Aregret%20of%20order%20%24%5Cmathcal%7BO%7D%28%5Csqrt%7BT%7D%29%24%20%28up%20to%20a%20logarithmic%20factor%29%20over%20%24T%24%0Atrials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%25CE%25B5%2524-Policy%2520Gradient%2520for%2520Online%2520Pricing%26entry.906535625%3DLukasz%2520Szpruch%2520and%2520Tanut%2520Treetanthiploet%2520and%2520Yufei%2520Zhang%26entry.1292438233%3D%2520%2520Combining%2520model-based%2520and%2520model-free%2520reinforcement%2520learning%2520approaches%252C%2520this%250Apaper%2520proposes%2520and%2520analyzes%2520an%2520%2524%255Cepsilon%2524-policy%2520gradient%2520algorithm%2520for%2520the%250Aonline%2520pricing%2520learning%2520task.%2520The%2520algorithm%2520extends%2520%2524%255Cepsilon%2524-greedy%2520algorithm%250Aby%2520replacing%2520greedy%2520exploitation%2520with%2520gradient%2520descent%2520step%2520and%2520facilitates%250Alearning%2520via%2520model%2520inference.%2520We%2520optimize%2520the%2520regret%2520of%2520the%2520proposed%2520algorithm%250Aby%2520quantifying%2520the%2520exploration%2520cost%2520in%2520terms%2520of%2520the%2520exploration%2520probability%250A%2524%255Cepsilon%2524%2520and%2520the%2520exploitation%2520cost%2520in%2520terms%2520of%2520the%2520gradient%2520descent%250Aoptimization%2520and%2520gradient%2520estimation%2520errors.%2520The%2520algorithm%2520achieves%2520an%2520expected%250Aregret%2520of%2520order%2520%2524%255Cmathcal%257BO%257D%2528%255Csqrt%257BT%257D%2529%2524%2520%2528up%2520to%2520a%2520logarithmic%2520factor%2529%2520over%2520%2524T%2524%250Atrials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%CE%B5%24-Policy%20Gradient%20for%20Online%20Pricing&entry.906535625=Lukasz%20Szpruch%20and%20Tanut%20Treetanthiploet%20and%20Yufei%20Zhang&entry.1292438233=%20%20Combining%20model-based%20and%20model-free%20reinforcement%20learning%20approaches%2C%20this%0Apaper%20proposes%20and%20analyzes%20an%20%24%5Cepsilon%24-policy%20gradient%20algorithm%20for%20the%0Aonline%20pricing%20learning%20task.%20The%20algorithm%20extends%20%24%5Cepsilon%24-greedy%20algorithm%0Aby%20replacing%20greedy%20exploitation%20with%20gradient%20descent%20step%20and%20facilitates%0Alearning%20via%20model%20inference.%20We%20optimize%20the%20regret%20of%20the%20proposed%20algorithm%0Aby%20quantifying%20the%20exploration%20cost%20in%20terms%20of%20the%20exploration%20probability%0A%24%5Cepsilon%24%20and%20the%20exploitation%20cost%20in%20terms%20of%20the%20gradient%20descent%0Aoptimization%20and%20gradient%20estimation%20errors.%20The%20algorithm%20achieves%20an%20expected%0Aregret%20of%20order%20%24%5Cmathcal%7BO%7D%28%5Csqrt%7BT%7D%29%24%20%28up%20to%20a%20logarithmic%20factor%29%20over%20%24T%24%0Atrials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03624v1&entry.124074799=Read"},
{"title": "Spice-E : Structural Priors in 3D Diffusion using Cross-Entity Attention", "author": "Etai Sella and Gal Fiebelman and Noam Atia and Hadar Averbuch-Elor", "abstract": "  We are witnessing rapid progress in automatically generating and manipulating\n3D assets due to the availability of pretrained text-image diffusion models.\nHowever, time-consuming optimization procedures are required for synthesizing\neach sample, hindering their potential for democratizing 3D content creation.\nConversely, 3D diffusion models now train on million-scale 3D datasets,\nyielding high-quality text-conditional 3D samples within seconds. In this work,\nwe present Spice-E - a neural network that adds structural guidance to 3D\ndiffusion models, extending their usage beyond text-conditional generation. At\nits core, our framework introduces a cross-entity attention mechanism that\nallows for multiple entities (in particular, paired input and guidance 3D\nshapes) to interact via their internal representations within the denoising\nnetwork. We utilize this mechanism for learning task-specific structural priors\nin 3D diffusion models from auxiliary guidance shapes. We show that our\napproach supports a variety of applications, including 3D stylization, semantic\nshape editing and text-conditional abstraction-to-3D, which transforms\nprimitive-based abstractions into highly-expressive shapes. Extensive\nexperiments demonstrate that Spice-E achieves SOTA performance over these tasks\nwhile often being considerably faster than alternative methods. Importantly,\nthis is accomplished without tailoring our approach for any specific task.\n", "link": "http://arxiv.org/abs/2311.17834v3", "date": "2024-05-06", "relevancy": 1.7313, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6017}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5701}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spice-E%20%3A%20Structural%20Priors%20in%203D%20Diffusion%20using%20Cross-Entity%20Attention&body=Title%3A%20Spice-E%20%3A%20Structural%20Priors%20in%203D%20Diffusion%20using%20Cross-Entity%20Attention%0AAuthor%3A%20Etai%20Sella%20and%20Gal%20Fiebelman%20and%20Noam%20Atia%20and%20Hadar%20Averbuch-Elor%0AAbstract%3A%20%20%20We%20are%20witnessing%20rapid%20progress%20in%20automatically%20generating%20and%20manipulating%0A3D%20assets%20due%20to%20the%20availability%20of%20pretrained%20text-image%20diffusion%20models.%0AHowever%2C%20time-consuming%20optimization%20procedures%20are%20required%20for%20synthesizing%0Aeach%20sample%2C%20hindering%20their%20potential%20for%20democratizing%203D%20content%20creation.%0AConversely%2C%203D%20diffusion%20models%20now%20train%20on%20million-scale%203D%20datasets%2C%0Ayielding%20high-quality%20text-conditional%203D%20samples%20within%20seconds.%20In%20this%20work%2C%0Awe%20present%20Spice-E%20-%20a%20neural%20network%20that%20adds%20structural%20guidance%20to%203D%0Adiffusion%20models%2C%20extending%20their%20usage%20beyond%20text-conditional%20generation.%20At%0Aits%20core%2C%20our%20framework%20introduces%20a%20cross-entity%20attention%20mechanism%20that%0Aallows%20for%20multiple%20entities%20%28in%20particular%2C%20paired%20input%20and%20guidance%203D%0Ashapes%29%20to%20interact%20via%20their%20internal%20representations%20within%20the%20denoising%0Anetwork.%20We%20utilize%20this%20mechanism%20for%20learning%20task-specific%20structural%20priors%0Ain%203D%20diffusion%20models%20from%20auxiliary%20guidance%20shapes.%20We%20show%20that%20our%0Aapproach%20supports%20a%20variety%20of%20applications%2C%20including%203D%20stylization%2C%20semantic%0Ashape%20editing%20and%20text-conditional%20abstraction-to-3D%2C%20which%20transforms%0Aprimitive-based%20abstractions%20into%20highly-expressive%20shapes.%20Extensive%0Aexperiments%20demonstrate%20that%20Spice-E%20achieves%20SOTA%20performance%20over%20these%20tasks%0Awhile%20often%20being%20considerably%20faster%20than%20alternative%20methods.%20Importantly%2C%0Athis%20is%20accomplished%20without%20tailoring%20our%20approach%20for%20any%20specific%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17834v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpice-E%2520%253A%2520Structural%2520Priors%2520in%25203D%2520Diffusion%2520using%2520Cross-Entity%2520Attention%26entry.906535625%3DEtai%2520Sella%2520and%2520Gal%2520Fiebelman%2520and%2520Noam%2520Atia%2520and%2520Hadar%2520Averbuch-Elor%26entry.1292438233%3D%2520%2520We%2520are%2520witnessing%2520rapid%2520progress%2520in%2520automatically%2520generating%2520and%2520manipulating%250A3D%2520assets%2520due%2520to%2520the%2520availability%2520of%2520pretrained%2520text-image%2520diffusion%2520models.%250AHowever%252C%2520time-consuming%2520optimization%2520procedures%2520are%2520required%2520for%2520synthesizing%250Aeach%2520sample%252C%2520hindering%2520their%2520potential%2520for%2520democratizing%25203D%2520content%2520creation.%250AConversely%252C%25203D%2520diffusion%2520models%2520now%2520train%2520on%2520million-scale%25203D%2520datasets%252C%250Ayielding%2520high-quality%2520text-conditional%25203D%2520samples%2520within%2520seconds.%2520In%2520this%2520work%252C%250Awe%2520present%2520Spice-E%2520-%2520a%2520neural%2520network%2520that%2520adds%2520structural%2520guidance%2520to%25203D%250Adiffusion%2520models%252C%2520extending%2520their%2520usage%2520beyond%2520text-conditional%2520generation.%2520At%250Aits%2520core%252C%2520our%2520framework%2520introduces%2520a%2520cross-entity%2520attention%2520mechanism%2520that%250Aallows%2520for%2520multiple%2520entities%2520%2528in%2520particular%252C%2520paired%2520input%2520and%2520guidance%25203D%250Ashapes%2529%2520to%2520interact%2520via%2520their%2520internal%2520representations%2520within%2520the%2520denoising%250Anetwork.%2520We%2520utilize%2520this%2520mechanism%2520for%2520learning%2520task-specific%2520structural%2520priors%250Ain%25203D%2520diffusion%2520models%2520from%2520auxiliary%2520guidance%2520shapes.%2520We%2520show%2520that%2520our%250Aapproach%2520supports%2520a%2520variety%2520of%2520applications%252C%2520including%25203D%2520stylization%252C%2520semantic%250Ashape%2520editing%2520and%2520text-conditional%2520abstraction-to-3D%252C%2520which%2520transforms%250Aprimitive-based%2520abstractions%2520into%2520highly-expressive%2520shapes.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520Spice-E%2520achieves%2520SOTA%2520performance%2520over%2520these%2520tasks%250Awhile%2520often%2520being%2520considerably%2520faster%2520than%2520alternative%2520methods.%2520Importantly%252C%250Athis%2520is%2520accomplished%2520without%2520tailoring%2520our%2520approach%2520for%2520any%2520specific%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17834v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spice-E%20%3A%20Structural%20Priors%20in%203D%20Diffusion%20using%20Cross-Entity%20Attention&entry.906535625=Etai%20Sella%20and%20Gal%20Fiebelman%20and%20Noam%20Atia%20and%20Hadar%20Averbuch-Elor&entry.1292438233=%20%20We%20are%20witnessing%20rapid%20progress%20in%20automatically%20generating%20and%20manipulating%0A3D%20assets%20due%20to%20the%20availability%20of%20pretrained%20text-image%20diffusion%20models.%0AHowever%2C%20time-consuming%20optimization%20procedures%20are%20required%20for%20synthesizing%0Aeach%20sample%2C%20hindering%20their%20potential%20for%20democratizing%203D%20content%20creation.%0AConversely%2C%203D%20diffusion%20models%20now%20train%20on%20million-scale%203D%20datasets%2C%0Ayielding%20high-quality%20text-conditional%203D%20samples%20within%20seconds.%20In%20this%20work%2C%0Awe%20present%20Spice-E%20-%20a%20neural%20network%20that%20adds%20structural%20guidance%20to%203D%0Adiffusion%20models%2C%20extending%20their%20usage%20beyond%20text-conditional%20generation.%20At%0Aits%20core%2C%20our%20framework%20introduces%20a%20cross-entity%20attention%20mechanism%20that%0Aallows%20for%20multiple%20entities%20%28in%20particular%2C%20paired%20input%20and%20guidance%203D%0Ashapes%29%20to%20interact%20via%20their%20internal%20representations%20within%20the%20denoising%0Anetwork.%20We%20utilize%20this%20mechanism%20for%20learning%20task-specific%20structural%20priors%0Ain%203D%20diffusion%20models%20from%20auxiliary%20guidance%20shapes.%20We%20show%20that%20our%0Aapproach%20supports%20a%20variety%20of%20applications%2C%20including%203D%20stylization%2C%20semantic%0Ashape%20editing%20and%20text-conditional%20abstraction-to-3D%2C%20which%20transforms%0Aprimitive-based%20abstractions%20into%20highly-expressive%20shapes.%20Extensive%0Aexperiments%20demonstrate%20that%20Spice-E%20achieves%20SOTA%20performance%20over%20these%20tasks%0Awhile%20often%20being%20considerably%20faster%20than%20alternative%20methods.%20Importantly%2C%0Athis%20is%20accomplished%20without%20tailoring%20our%20approach%20for%20any%20specific%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17834v3&entry.124074799=Read"},
{"title": "On-site scale factor linearity calibration of MEMS triaxial gyroscopes", "author": "Yaqi Li and Li Wang and Zhitao Wang and Xiangqing Li and Jiaojiao Li and Steven weidong Su", "abstract": "  The calibration of MEMS triaxial gyroscopes is crucial for achieving precise\nattitude estimation for various wearable health monitoring applications.\nHowever, gyroscope calibration poses greater challenges compared to\naccelerometers and magnetometers. This paper introduces an efficient method for\ncalibrating MEMS triaxial gyroscopes via only a servo motor, making it\nwell-suited for field environments. The core strategy of the method involves\nutilizing the fact that the dot product of the measured gravity and the\nrotational speed in a fixed frame remains constant. To eliminate the influence\nof rotating centrifugal force on the accelerometer, the accelerometer data is\nmeasured while stationary. The proposed calibration experiment scheme, which\nallows gyroscopic measurements when operating each axis at a specific rotation\nspeed, making it easier to evaluate the linearity across a related speed range\nconstituted by a series of rotation speeds. Moreover, solely the classical\nleast squares algorithm proves adequate for estimating the scale factor,\nnotably streamlining the analysis of the calibration process. Extensive\nnumerical simulations were conducted to analyze the proposed method's\nperformance in calibrating a triaxial gyroscope model. Experimental validation\nwas also carried out using a commercially available MEMS inertial measurement\nunit (LSM9DS1 from Arduino nano 33 BLE SENSE) and a servo motor capable of\ncontrolling precise speed. The experimental results effectively demonstrate the\nefficacy of the proposed calibration approach.\n", "link": "http://arxiv.org/abs/2405.03393v1", "date": "2024-05-06", "relevancy": 1.7624, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4527}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4383}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On-site%20scale%20factor%20linearity%20calibration%20of%20MEMS%20triaxial%20gyroscopes&body=Title%3A%20On-site%20scale%20factor%20linearity%20calibration%20of%20MEMS%20triaxial%20gyroscopes%0AAuthor%3A%20Yaqi%20Li%20and%20Li%20Wang%20and%20Zhitao%20Wang%20and%20Xiangqing%20Li%20and%20Jiaojiao%20Li%20and%20Steven%20weidong%20Su%0AAbstract%3A%20%20%20The%20calibration%20of%20MEMS%20triaxial%20gyroscopes%20is%20crucial%20for%20achieving%20precise%0Aattitude%20estimation%20for%20various%20wearable%20health%20monitoring%20applications.%0AHowever%2C%20gyroscope%20calibration%20poses%20greater%20challenges%20compared%20to%0Aaccelerometers%20and%20magnetometers.%20This%20paper%20introduces%20an%20efficient%20method%20for%0Acalibrating%20MEMS%20triaxial%20gyroscopes%20via%20only%20a%20servo%20motor%2C%20making%20it%0Awell-suited%20for%20field%20environments.%20The%20core%20strategy%20of%20the%20method%20involves%0Autilizing%20the%20fact%20that%20the%20dot%20product%20of%20the%20measured%20gravity%20and%20the%0Arotational%20speed%20in%20a%20fixed%20frame%20remains%20constant.%20To%20eliminate%20the%20influence%0Aof%20rotating%20centrifugal%20force%20on%20the%20accelerometer%2C%20the%20accelerometer%20data%20is%0Ameasured%20while%20stationary.%20The%20proposed%20calibration%20experiment%20scheme%2C%20which%0Aallows%20gyroscopic%20measurements%20when%20operating%20each%20axis%20at%20a%20specific%20rotation%0Aspeed%2C%20making%20it%20easier%20to%20evaluate%20the%20linearity%20across%20a%20related%20speed%20range%0Aconstituted%20by%20a%20series%20of%20rotation%20speeds.%20Moreover%2C%20solely%20the%20classical%0Aleast%20squares%20algorithm%20proves%20adequate%20for%20estimating%20the%20scale%20factor%2C%0Anotably%20streamlining%20the%20analysis%20of%20the%20calibration%20process.%20Extensive%0Anumerical%20simulations%20were%20conducted%20to%20analyze%20the%20proposed%20method%27s%0Aperformance%20in%20calibrating%20a%20triaxial%20gyroscope%20model.%20Experimental%20validation%0Awas%20also%20carried%20out%20using%20a%20commercially%20available%20MEMS%20inertial%20measurement%0Aunit%20%28LSM9DS1%20from%20Arduino%20nano%2033%20BLE%20SENSE%29%20and%20a%20servo%20motor%20capable%20of%0Acontrolling%20precise%20speed.%20The%20experimental%20results%20effectively%20demonstrate%20the%0Aefficacy%20of%20the%20proposed%20calibration%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03393v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn-site%2520scale%2520factor%2520linearity%2520calibration%2520of%2520MEMS%2520triaxial%2520gyroscopes%26entry.906535625%3DYaqi%2520Li%2520and%2520Li%2520Wang%2520and%2520Zhitao%2520Wang%2520and%2520Xiangqing%2520Li%2520and%2520Jiaojiao%2520Li%2520and%2520Steven%2520weidong%2520Su%26entry.1292438233%3D%2520%2520The%2520calibration%2520of%2520MEMS%2520triaxial%2520gyroscopes%2520is%2520crucial%2520for%2520achieving%2520precise%250Aattitude%2520estimation%2520for%2520various%2520wearable%2520health%2520monitoring%2520applications.%250AHowever%252C%2520gyroscope%2520calibration%2520poses%2520greater%2520challenges%2520compared%2520to%250Aaccelerometers%2520and%2520magnetometers.%2520This%2520paper%2520introduces%2520an%2520efficient%2520method%2520for%250Acalibrating%2520MEMS%2520triaxial%2520gyroscopes%2520via%2520only%2520a%2520servo%2520motor%252C%2520making%2520it%250Awell-suited%2520for%2520field%2520environments.%2520The%2520core%2520strategy%2520of%2520the%2520method%2520involves%250Autilizing%2520the%2520fact%2520that%2520the%2520dot%2520product%2520of%2520the%2520measured%2520gravity%2520and%2520the%250Arotational%2520speed%2520in%2520a%2520fixed%2520frame%2520remains%2520constant.%2520To%2520eliminate%2520the%2520influence%250Aof%2520rotating%2520centrifugal%2520force%2520on%2520the%2520accelerometer%252C%2520the%2520accelerometer%2520data%2520is%250Ameasured%2520while%2520stationary.%2520The%2520proposed%2520calibration%2520experiment%2520scheme%252C%2520which%250Aallows%2520gyroscopic%2520measurements%2520when%2520operating%2520each%2520axis%2520at%2520a%2520specific%2520rotation%250Aspeed%252C%2520making%2520it%2520easier%2520to%2520evaluate%2520the%2520linearity%2520across%2520a%2520related%2520speed%2520range%250Aconstituted%2520by%2520a%2520series%2520of%2520rotation%2520speeds.%2520Moreover%252C%2520solely%2520the%2520classical%250Aleast%2520squares%2520algorithm%2520proves%2520adequate%2520for%2520estimating%2520the%2520scale%2520factor%252C%250Anotably%2520streamlining%2520the%2520analysis%2520of%2520the%2520calibration%2520process.%2520Extensive%250Anumerical%2520simulations%2520were%2520conducted%2520to%2520analyze%2520the%2520proposed%2520method%2527s%250Aperformance%2520in%2520calibrating%2520a%2520triaxial%2520gyroscope%2520model.%2520Experimental%2520validation%250Awas%2520also%2520carried%2520out%2520using%2520a%2520commercially%2520available%2520MEMS%2520inertial%2520measurement%250Aunit%2520%2528LSM9DS1%2520from%2520Arduino%2520nano%252033%2520BLE%2520SENSE%2529%2520and%2520a%2520servo%2520motor%2520capable%2520of%250Acontrolling%2520precise%2520speed.%2520The%2520experimental%2520results%2520effectively%2520demonstrate%2520the%250Aefficacy%2520of%2520the%2520proposed%2520calibration%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03393v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-site%20scale%20factor%20linearity%20calibration%20of%20MEMS%20triaxial%20gyroscopes&entry.906535625=Yaqi%20Li%20and%20Li%20Wang%20and%20Zhitao%20Wang%20and%20Xiangqing%20Li%20and%20Jiaojiao%20Li%20and%20Steven%20weidong%20Su&entry.1292438233=%20%20The%20calibration%20of%20MEMS%20triaxial%20gyroscopes%20is%20crucial%20for%20achieving%20precise%0Aattitude%20estimation%20for%20various%20wearable%20health%20monitoring%20applications.%0AHowever%2C%20gyroscope%20calibration%20poses%20greater%20challenges%20compared%20to%0Aaccelerometers%20and%20magnetometers.%20This%20paper%20introduces%20an%20efficient%20method%20for%0Acalibrating%20MEMS%20triaxial%20gyroscopes%20via%20only%20a%20servo%20motor%2C%20making%20it%0Awell-suited%20for%20field%20environments.%20The%20core%20strategy%20of%20the%20method%20involves%0Autilizing%20the%20fact%20that%20the%20dot%20product%20of%20the%20measured%20gravity%20and%20the%0Arotational%20speed%20in%20a%20fixed%20frame%20remains%20constant.%20To%20eliminate%20the%20influence%0Aof%20rotating%20centrifugal%20force%20on%20the%20accelerometer%2C%20the%20accelerometer%20data%20is%0Ameasured%20while%20stationary.%20The%20proposed%20calibration%20experiment%20scheme%2C%20which%0Aallows%20gyroscopic%20measurements%20when%20operating%20each%20axis%20at%20a%20specific%20rotation%0Aspeed%2C%20making%20it%20easier%20to%20evaluate%20the%20linearity%20across%20a%20related%20speed%20range%0Aconstituted%20by%20a%20series%20of%20rotation%20speeds.%20Moreover%2C%20solely%20the%20classical%0Aleast%20squares%20algorithm%20proves%20adequate%20for%20estimating%20the%20scale%20factor%2C%0Anotably%20streamlining%20the%20analysis%20of%20the%20calibration%20process.%20Extensive%0Anumerical%20simulations%20were%20conducted%20to%20analyze%20the%20proposed%20method%27s%0Aperformance%20in%20calibrating%20a%20triaxial%20gyroscope%20model.%20Experimental%20validation%0Awas%20also%20carried%20out%20using%20a%20commercially%20available%20MEMS%20inertial%20measurement%0Aunit%20%28LSM9DS1%20from%20Arduino%20nano%2033%20BLE%20SENSE%29%20and%20a%20servo%20motor%20capable%20of%0Acontrolling%20precise%20speed.%20The%20experimental%20results%20effectively%20demonstrate%20the%0Aefficacy%20of%20the%20proposed%20calibration%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03393v1&entry.124074799=Read"},
{"title": "AlphaMath Almost Zero: process Supervision without process", "author": "Guoxin Chen and Minpeng Liao and Chengxi Li and Kai Fan", "abstract": "  Recent advancements in large language models (LLMs) have substantially\nenhanced their mathematical reasoning abilities. However, these models still\nstruggle with complex problems that require multiple reasoning steps,\nfrequently leading to logical or numerical errors. While numerical mistakes can\nlargely be addressed by integrating a code interpreter, identifying logical\nerrors within intermediate steps is more challenging. Moreover, manually\nannotating these steps for training is not only expensive but also demands\nspecialized expertise. In this study, we introduce an innovative approach that\neliminates the need for manual annotation by leveraging the Monte Carlo Tree\nSearch (MCTS) framework to generate both the process supervision and evaluation\nsignals automatically. Essentially, when a LLM is well pre-trained, only the\nmathematical questions and their final answers are required to generate our\ntraining data, without requiring the solutions. We proceed to train a\nstep-level value model designed to improve the LLM's inference process in\nmathematical domains. Our experiments indicate that using automatically\ngenerated solutions by LLMs enhanced with MCTS significantly improves the\nmodel's proficiency in dealing with intricate mathematical reasoning tasks.\n", "link": "http://arxiv.org/abs/2405.03553v1", "date": "2024-05-06", "relevancy": 0.9636, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4835}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4825}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlphaMath%20Almost%20Zero%3A%20process%20Supervision%20without%20process&body=Title%3A%20AlphaMath%20Almost%20Zero%3A%20process%20Supervision%20without%20process%0AAuthor%3A%20Guoxin%20Chen%20and%20Minpeng%20Liao%20and%20Chengxi%20Li%20and%20Kai%20Fan%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20substantially%0Aenhanced%20their%20mathematical%20reasoning%20abilities.%20However%2C%20these%20models%20still%0Astruggle%20with%20complex%20problems%20that%20require%20multiple%20reasoning%20steps%2C%0Afrequently%20leading%20to%20logical%20or%20numerical%20errors.%20While%20numerical%20mistakes%20can%0Alargely%20be%20addressed%20by%20integrating%20a%20code%20interpreter%2C%20identifying%20logical%0Aerrors%20within%20intermediate%20steps%20is%20more%20challenging.%20Moreover%2C%20manually%0Aannotating%20these%20steps%20for%20training%20is%20not%20only%20expensive%20but%20also%20demands%0Aspecialized%20expertise.%20In%20this%20study%2C%20we%20introduce%20an%20innovative%20approach%20that%0Aeliminates%20the%20need%20for%20manual%20annotation%20by%20leveraging%20the%20Monte%20Carlo%20Tree%0ASearch%20%28MCTS%29%20framework%20to%20generate%20both%20the%20process%20supervision%20and%20evaluation%0Asignals%20automatically.%20Essentially%2C%20when%20a%20LLM%20is%20well%20pre-trained%2C%20only%20the%0Amathematical%20questions%20and%20their%20final%20answers%20are%20required%20to%20generate%20our%0Atraining%20data%2C%20without%20requiring%20the%20solutions.%20We%20proceed%20to%20train%20a%0Astep-level%20value%20model%20designed%20to%20improve%20the%20LLM%27s%20inference%20process%20in%0Amathematical%20domains.%20Our%20experiments%20indicate%20that%20using%20automatically%0Agenerated%20solutions%20by%20LLMs%20enhanced%20with%20MCTS%20significantly%20improves%20the%0Amodel%27s%20proficiency%20in%20dealing%20with%20intricate%20mathematical%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlphaMath%2520Almost%2520Zero%253A%2520process%2520Supervision%2520without%2520process%26entry.906535625%3DGuoxin%2520Chen%2520and%2520Minpeng%2520Liao%2520and%2520Chengxi%2520Li%2520and%2520Kai%2520Fan%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520substantially%250Aenhanced%2520their%2520mathematical%2520reasoning%2520abilities.%2520However%252C%2520these%2520models%2520still%250Astruggle%2520with%2520complex%2520problems%2520that%2520require%2520multiple%2520reasoning%2520steps%252C%250Afrequently%2520leading%2520to%2520logical%2520or%2520numerical%2520errors.%2520While%2520numerical%2520mistakes%2520can%250Alargely%2520be%2520addressed%2520by%2520integrating%2520a%2520code%2520interpreter%252C%2520identifying%2520logical%250Aerrors%2520within%2520intermediate%2520steps%2520is%2520more%2520challenging.%2520Moreover%252C%2520manually%250Aannotating%2520these%2520steps%2520for%2520training%2520is%2520not%2520only%2520expensive%2520but%2520also%2520demands%250Aspecialized%2520expertise.%2520In%2520this%2520study%252C%2520we%2520introduce%2520an%2520innovative%2520approach%2520that%250Aeliminates%2520the%2520need%2520for%2520manual%2520annotation%2520by%2520leveraging%2520the%2520Monte%2520Carlo%2520Tree%250ASearch%2520%2528MCTS%2529%2520framework%2520to%2520generate%2520both%2520the%2520process%2520supervision%2520and%2520evaluation%250Asignals%2520automatically.%2520Essentially%252C%2520when%2520a%2520LLM%2520is%2520well%2520pre-trained%252C%2520only%2520the%250Amathematical%2520questions%2520and%2520their%2520final%2520answers%2520are%2520required%2520to%2520generate%2520our%250Atraining%2520data%252C%2520without%2520requiring%2520the%2520solutions.%2520We%2520proceed%2520to%2520train%2520a%250Astep-level%2520value%2520model%2520designed%2520to%2520improve%2520the%2520LLM%2527s%2520inference%2520process%2520in%250Amathematical%2520domains.%2520Our%2520experiments%2520indicate%2520that%2520using%2520automatically%250Agenerated%2520solutions%2520by%2520LLMs%2520enhanced%2520with%2520MCTS%2520significantly%2520improves%2520the%250Amodel%2527s%2520proficiency%2520in%2520dealing%2520with%2520intricate%2520mathematical%2520reasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlphaMath%20Almost%20Zero%3A%20process%20Supervision%20without%20process&entry.906535625=Guoxin%20Chen%20and%20Minpeng%20Liao%20and%20Chengxi%20Li%20and%20Kai%20Fan&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20substantially%0Aenhanced%20their%20mathematical%20reasoning%20abilities.%20However%2C%20these%20models%20still%0Astruggle%20with%20complex%20problems%20that%20require%20multiple%20reasoning%20steps%2C%0Afrequently%20leading%20to%20logical%20or%20numerical%20errors.%20While%20numerical%20mistakes%20can%0Alargely%20be%20addressed%20by%20integrating%20a%20code%20interpreter%2C%20identifying%20logical%0Aerrors%20within%20intermediate%20steps%20is%20more%20challenging.%20Moreover%2C%20manually%0Aannotating%20these%20steps%20for%20training%20is%20not%20only%20expensive%20but%20also%20demands%0Aspecialized%20expertise.%20In%20this%20study%2C%20we%20introduce%20an%20innovative%20approach%20that%0Aeliminates%20the%20need%20for%20manual%20annotation%20by%20leveraging%20the%20Monte%20Carlo%20Tree%0ASearch%20%28MCTS%29%20framework%20to%20generate%20both%20the%20process%20supervision%20and%20evaluation%0Asignals%20automatically.%20Essentially%2C%20when%20a%20LLM%20is%20well%20pre-trained%2C%20only%20the%0Amathematical%20questions%20and%20their%20final%20answers%20are%20required%20to%20generate%20our%0Atraining%20data%2C%20without%20requiring%20the%20solutions.%20We%20proceed%20to%20train%20a%0Astep-level%20value%20model%20designed%20to%20improve%20the%20LLM%27s%20inference%20process%20in%0Amathematical%20domains.%20Our%20experiments%20indicate%20that%20using%20automatically%0Agenerated%20solutions%20by%20LLMs%20enhanced%20with%20MCTS%20significantly%20improves%20the%0Amodel%27s%20proficiency%20in%20dealing%20with%20intricate%20mathematical%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03553v1&entry.124074799=Read"},
{"title": "Can LLMs Deeply Detect Complex Malicious Queries? A Framework for\n  Jailbreaking via Obfuscating Intent", "author": "Shang Shang and Xinqiang Zhao and Zhongjiang Yao and Yepeng Yao and Liya Su and Zijing Fan and Xiaodan Zhang and Zhengwei Jiang", "abstract": "  To demonstrate and address the underlying maliciousness, we propose a\ntheoretical hypothesis and analytical approach, and introduce a new black-box\njailbreak attack methodology named IntentObfuscator, exploiting this identified\nflaw by obfuscating the true intentions behind user prompts.This approach\ncompels LLMs to inadvertently generate restricted content, bypassing their\nbuilt-in content security measures. We detail two implementations under this\nframework: \"Obscure Intention\" and \"Create Ambiguity\", which manipulate query\ncomplexity and ambiguity to evade malicious intent detection effectively. We\nempirically validate the effectiveness of the IntentObfuscator method across\nseveral models, including ChatGPT-3.5, ChatGPT-4, Qwen and Baichuan, achieving\nan average jailbreak success rate of 69.21\\%. Notably, our tests on\nChatGPT-3.5, which claims 100 million weekly active users, achieved a\nremarkable success rate of 83.65\\%. We also extend our validation to diverse\ntypes of sensitive content like graphic violence, racism, sexism, political\nsensitivity, cybersecurity threats, and criminal skills, further proving the\nsubstantial impact of our findings on enhancing 'Red Team' strategies against\nLLM content security frameworks.\n", "link": "http://arxiv.org/abs/2405.03654v1", "date": "2024-05-06", "relevancy": 1.5293, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4009}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3799}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20Deeply%20Detect%20Complex%20Malicious%20Queries%3F%20A%20Framework%20for%0A%20%20Jailbreaking%20via%20Obfuscating%20Intent&body=Title%3A%20Can%20LLMs%20Deeply%20Detect%20Complex%20Malicious%20Queries%3F%20A%20Framework%20for%0A%20%20Jailbreaking%20via%20Obfuscating%20Intent%0AAuthor%3A%20Shang%20Shang%20and%20Xinqiang%20Zhao%20and%20Zhongjiang%20Yao%20and%20Yepeng%20Yao%20and%20Liya%20Su%20and%20Zijing%20Fan%20and%20Xiaodan%20Zhang%20and%20Zhengwei%20Jiang%0AAbstract%3A%20%20%20To%20demonstrate%20and%20address%20the%20underlying%20maliciousness%2C%20we%20propose%20a%0Atheoretical%20hypothesis%20and%20analytical%20approach%2C%20and%20introduce%20a%20new%20black-box%0Ajailbreak%20attack%20methodology%20named%20IntentObfuscator%2C%20exploiting%20this%20identified%0Aflaw%20by%20obfuscating%20the%20true%20intentions%20behind%20user%20prompts.This%20approach%0Acompels%20LLMs%20to%20inadvertently%20generate%20restricted%20content%2C%20bypassing%20their%0Abuilt-in%20content%20security%20measures.%20We%20detail%20two%20implementations%20under%20this%0Aframework%3A%20%22Obscure%20Intention%22%20and%20%22Create%20Ambiguity%22%2C%20which%20manipulate%20query%0Acomplexity%20and%20ambiguity%20to%20evade%20malicious%20intent%20detection%20effectively.%20We%0Aempirically%20validate%20the%20effectiveness%20of%20the%20IntentObfuscator%20method%20across%0Aseveral%20models%2C%20including%20ChatGPT-3.5%2C%20ChatGPT-4%2C%20Qwen%20and%20Baichuan%2C%20achieving%0Aan%20average%20jailbreak%20success%20rate%20of%2069.21%5C%25.%20Notably%2C%20our%20tests%20on%0AChatGPT-3.5%2C%20which%20claims%20100%20million%20weekly%20active%20users%2C%20achieved%20a%0Aremarkable%20success%20rate%20of%2083.65%5C%25.%20We%20also%20extend%20our%20validation%20to%20diverse%0Atypes%20of%20sensitive%20content%20like%20graphic%20violence%2C%20racism%2C%20sexism%2C%20political%0Asensitivity%2C%20cybersecurity%20threats%2C%20and%20criminal%20skills%2C%20further%20proving%20the%0Asubstantial%20impact%20of%20our%20findings%20on%20enhancing%20%27Red%20Team%27%20strategies%20against%0ALLM%20content%20security%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520Deeply%2520Detect%2520Complex%2520Malicious%2520Queries%253F%2520A%2520Framework%2520for%250A%2520%2520Jailbreaking%2520via%2520Obfuscating%2520Intent%26entry.906535625%3DShang%2520Shang%2520and%2520Xinqiang%2520Zhao%2520and%2520Zhongjiang%2520Yao%2520and%2520Yepeng%2520Yao%2520and%2520Liya%2520Su%2520and%2520Zijing%2520Fan%2520and%2520Xiaodan%2520Zhang%2520and%2520Zhengwei%2520Jiang%26entry.1292438233%3D%2520%2520To%2520demonstrate%2520and%2520address%2520the%2520underlying%2520maliciousness%252C%2520we%2520propose%2520a%250Atheoretical%2520hypothesis%2520and%2520analytical%2520approach%252C%2520and%2520introduce%2520a%2520new%2520black-box%250Ajailbreak%2520attack%2520methodology%2520named%2520IntentObfuscator%252C%2520exploiting%2520this%2520identified%250Aflaw%2520by%2520obfuscating%2520the%2520true%2520intentions%2520behind%2520user%2520prompts.This%2520approach%250Acompels%2520LLMs%2520to%2520inadvertently%2520generate%2520restricted%2520content%252C%2520bypassing%2520their%250Abuilt-in%2520content%2520security%2520measures.%2520We%2520detail%2520two%2520implementations%2520under%2520this%250Aframework%253A%2520%2522Obscure%2520Intention%2522%2520and%2520%2522Create%2520Ambiguity%2522%252C%2520which%2520manipulate%2520query%250Acomplexity%2520and%2520ambiguity%2520to%2520evade%2520malicious%2520intent%2520detection%2520effectively.%2520We%250Aempirically%2520validate%2520the%2520effectiveness%2520of%2520the%2520IntentObfuscator%2520method%2520across%250Aseveral%2520models%252C%2520including%2520ChatGPT-3.5%252C%2520ChatGPT-4%252C%2520Qwen%2520and%2520Baichuan%252C%2520achieving%250Aan%2520average%2520jailbreak%2520success%2520rate%2520of%252069.21%255C%2525.%2520Notably%252C%2520our%2520tests%2520on%250AChatGPT-3.5%252C%2520which%2520claims%2520100%2520million%2520weekly%2520active%2520users%252C%2520achieved%2520a%250Aremarkable%2520success%2520rate%2520of%252083.65%255C%2525.%2520We%2520also%2520extend%2520our%2520validation%2520to%2520diverse%250Atypes%2520of%2520sensitive%2520content%2520like%2520graphic%2520violence%252C%2520racism%252C%2520sexism%252C%2520political%250Asensitivity%252C%2520cybersecurity%2520threats%252C%2520and%2520criminal%2520skills%252C%2520further%2520proving%2520the%250Asubstantial%2520impact%2520of%2520our%2520findings%2520on%2520enhancing%2520%2527Red%2520Team%2527%2520strategies%2520against%250ALLM%2520content%2520security%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20Deeply%20Detect%20Complex%20Malicious%20Queries%3F%20A%20Framework%20for%0A%20%20Jailbreaking%20via%20Obfuscating%20Intent&entry.906535625=Shang%20Shang%20and%20Xinqiang%20Zhao%20and%20Zhongjiang%20Yao%20and%20Yepeng%20Yao%20and%20Liya%20Su%20and%20Zijing%20Fan%20and%20Xiaodan%20Zhang%20and%20Zhengwei%20Jiang&entry.1292438233=%20%20To%20demonstrate%20and%20address%20the%20underlying%20maliciousness%2C%20we%20propose%20a%0Atheoretical%20hypothesis%20and%20analytical%20approach%2C%20and%20introduce%20a%20new%20black-box%0Ajailbreak%20attack%20methodology%20named%20IntentObfuscator%2C%20exploiting%20this%20identified%0Aflaw%20by%20obfuscating%20the%20true%20intentions%20behind%20user%20prompts.This%20approach%0Acompels%20LLMs%20to%20inadvertently%20generate%20restricted%20content%2C%20bypassing%20their%0Abuilt-in%20content%20security%20measures.%20We%20detail%20two%20implementations%20under%20this%0Aframework%3A%20%22Obscure%20Intention%22%20and%20%22Create%20Ambiguity%22%2C%20which%20manipulate%20query%0Acomplexity%20and%20ambiguity%20to%20evade%20malicious%20intent%20detection%20effectively.%20We%0Aempirically%20validate%20the%20effectiveness%20of%20the%20IntentObfuscator%20method%20across%0Aseveral%20models%2C%20including%20ChatGPT-3.5%2C%20ChatGPT-4%2C%20Qwen%20and%20Baichuan%2C%20achieving%0Aan%20average%20jailbreak%20success%20rate%20of%2069.21%5C%25.%20Notably%2C%20our%20tests%20on%0AChatGPT-3.5%2C%20which%20claims%20100%20million%20weekly%20active%20users%2C%20achieved%20a%0Aremarkable%20success%20rate%20of%2083.65%5C%25.%20We%20also%20extend%20our%20validation%20to%20diverse%0Atypes%20of%20sensitive%20content%20like%20graphic%20violence%2C%20racism%2C%20sexism%2C%20political%0Asensitivity%2C%20cybersecurity%20threats%2C%20and%20criminal%20skills%2C%20further%20proving%20the%0Asubstantial%20impact%20of%20our%20findings%20on%20enhancing%20%27Red%20Team%27%20strategies%20against%0ALLM%20content%20security%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03654v1&entry.124074799=Read"},
{"title": "A review on data-driven constitutive laws for solids", "author": "Jan Niklas Fuhg and Govinda Anantha Padmanabha and Nikolaos Bouklas and Bahador Bahmani and WaiChing Sun and Nikolaos N. Vlassis and Moritz Flaschel and Pietro Carrara and Laura De Lorenzis", "abstract": "  This review article highlights state-of-the-art data-driven techniques to\ndiscover, encode, surrogate, or emulate constitutive laws that describe the\npath-independent and path-dependent response of solids. Our objective is to\nprovide an organized taxonomy to a large spectrum of methodologies developed in\nthe past decades and to discuss the benefits and drawbacks of the various\ntechniques for interpreting and forecasting mechanics behavior across different\nscales. Distinguishing between machine-learning-based and model-free methods,\nwe further categorize approaches based on their interpretability and on their\nlearning process/type of required data, while discussing the key problems of\ngeneralization and trustworthiness. We attempt to provide a road map of how\nthese can be reconciled in a data-availability-aware context. We also touch\nupon relevant aspects such as data sampling techniques, design of experiments,\nverification, and validation.\n", "link": "http://arxiv.org/abs/2405.03658v1", "date": "2024-05-06", "relevancy": 1.6617, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4452}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4202}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20review%20on%20data-driven%20constitutive%20laws%20for%20solids&body=Title%3A%20A%20review%20on%20data-driven%20constitutive%20laws%20for%20solids%0AAuthor%3A%20Jan%20Niklas%20Fuhg%20and%20Govinda%20Anantha%20Padmanabha%20and%20Nikolaos%20Bouklas%20and%20Bahador%20Bahmani%20and%20WaiChing%20Sun%20and%20Nikolaos%20N.%20Vlassis%20and%20Moritz%20Flaschel%20and%20Pietro%20Carrara%20and%20Laura%20De%20Lorenzis%0AAbstract%3A%20%20%20This%20review%20article%20highlights%20state-of-the-art%20data-driven%20techniques%20to%0Adiscover%2C%20encode%2C%20surrogate%2C%20or%20emulate%20constitutive%20laws%20that%20describe%20the%0Apath-independent%20and%20path-dependent%20response%20of%20solids.%20Our%20objective%20is%20to%0Aprovide%20an%20organized%20taxonomy%20to%20a%20large%20spectrum%20of%20methodologies%20developed%20in%0Athe%20past%20decades%20and%20to%20discuss%20the%20benefits%20and%20drawbacks%20of%20the%20various%0Atechniques%20for%20interpreting%20and%20forecasting%20mechanics%20behavior%20across%20different%0Ascales.%20Distinguishing%20between%20machine-learning-based%20and%20model-free%20methods%2C%0Awe%20further%20categorize%20approaches%20based%20on%20their%20interpretability%20and%20on%20their%0Alearning%20process/type%20of%20required%20data%2C%20while%20discussing%20the%20key%20problems%20of%0Ageneralization%20and%20trustworthiness.%20We%20attempt%20to%20provide%20a%20road%20map%20of%20how%0Athese%20can%20be%20reconciled%20in%20a%20data-availability-aware%20context.%20We%20also%20touch%0Aupon%20relevant%20aspects%20such%20as%20data%20sampling%20techniques%2C%20design%20of%20experiments%2C%0Averification%2C%20and%20validation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03658v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520review%2520on%2520data-driven%2520constitutive%2520laws%2520for%2520solids%26entry.906535625%3DJan%2520Niklas%2520Fuhg%2520and%2520Govinda%2520Anantha%2520Padmanabha%2520and%2520Nikolaos%2520Bouklas%2520and%2520Bahador%2520Bahmani%2520and%2520WaiChing%2520Sun%2520and%2520Nikolaos%2520N.%2520Vlassis%2520and%2520Moritz%2520Flaschel%2520and%2520Pietro%2520Carrara%2520and%2520Laura%2520De%2520Lorenzis%26entry.1292438233%3D%2520%2520This%2520review%2520article%2520highlights%2520state-of-the-art%2520data-driven%2520techniques%2520to%250Adiscover%252C%2520encode%252C%2520surrogate%252C%2520or%2520emulate%2520constitutive%2520laws%2520that%2520describe%2520the%250Apath-independent%2520and%2520path-dependent%2520response%2520of%2520solids.%2520Our%2520objective%2520is%2520to%250Aprovide%2520an%2520organized%2520taxonomy%2520to%2520a%2520large%2520spectrum%2520of%2520methodologies%2520developed%2520in%250Athe%2520past%2520decades%2520and%2520to%2520discuss%2520the%2520benefits%2520and%2520drawbacks%2520of%2520the%2520various%250Atechniques%2520for%2520interpreting%2520and%2520forecasting%2520mechanics%2520behavior%2520across%2520different%250Ascales.%2520Distinguishing%2520between%2520machine-learning-based%2520and%2520model-free%2520methods%252C%250Awe%2520further%2520categorize%2520approaches%2520based%2520on%2520their%2520interpretability%2520and%2520on%2520their%250Alearning%2520process/type%2520of%2520required%2520data%252C%2520while%2520discussing%2520the%2520key%2520problems%2520of%250Ageneralization%2520and%2520trustworthiness.%2520We%2520attempt%2520to%2520provide%2520a%2520road%2520map%2520of%2520how%250Athese%2520can%2520be%2520reconciled%2520in%2520a%2520data-availability-aware%2520context.%2520We%2520also%2520touch%250Aupon%2520relevant%2520aspects%2520such%2520as%2520data%2520sampling%2520techniques%252C%2520design%2520of%2520experiments%252C%250Averification%252C%2520and%2520validation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03658v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20review%20on%20data-driven%20constitutive%20laws%20for%20solids&entry.906535625=Jan%20Niklas%20Fuhg%20and%20Govinda%20Anantha%20Padmanabha%20and%20Nikolaos%20Bouklas%20and%20Bahador%20Bahmani%20and%20WaiChing%20Sun%20and%20Nikolaos%20N.%20Vlassis%20and%20Moritz%20Flaschel%20and%20Pietro%20Carrara%20and%20Laura%20De%20Lorenzis&entry.1292438233=%20%20This%20review%20article%20highlights%20state-of-the-art%20data-driven%20techniques%20to%0Adiscover%2C%20encode%2C%20surrogate%2C%20or%20emulate%20constitutive%20laws%20that%20describe%20the%0Apath-independent%20and%20path-dependent%20response%20of%20solids.%20Our%20objective%20is%20to%0Aprovide%20an%20organized%20taxonomy%20to%20a%20large%20spectrum%20of%20methodologies%20developed%20in%0Athe%20past%20decades%20and%20to%20discuss%20the%20benefits%20and%20drawbacks%20of%20the%20various%0Atechniques%20for%20interpreting%20and%20forecasting%20mechanics%20behavior%20across%20different%0Ascales.%20Distinguishing%20between%20machine-learning-based%20and%20model-free%20methods%2C%0Awe%20further%20categorize%20approaches%20based%20on%20their%20interpretability%20and%20on%20their%0Alearning%20process/type%20of%20required%20data%2C%20while%20discussing%20the%20key%20problems%20of%0Ageneralization%20and%20trustworthiness.%20We%20attempt%20to%20provide%20a%20road%20map%20of%20how%0Athese%20can%20be%20reconciled%20in%20a%20data-availability-aware%20context.%20We%20also%20touch%0Aupon%20relevant%20aspects%20such%20as%20data%20sampling%20techniques%2C%20design%20of%20experiments%2C%0Averification%2C%20and%20validation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03658v1&entry.124074799=Read"},
{"title": "Data-Copilot: Bridging Billions of Data and Humans with Autonomous\n  Workflow", "author": "Wenqi Zhang and Yongliang Shen and Weiming Lu and Yueting Zhuang", "abstract": "  Various industries such as finance, meteorology, and energy produce vast\namounts of heterogeneous data every day. There is a natural demand for humans\nto manage, process, and display data efficiently. However, it necessitates\nlabor-intensive efforts and a high level of expertise for these data-related\ntasks. Considering large language models (LLMs) showcase promising capabilities\nin semantic understanding and reasoning, we advocate that the deployment of\nLLMs could autonomously manage and process massive amounts of data while\ninteracting and displaying in a human-friendly manner. Based on this, we\npropose Data-Copilot, an LLM-based system that connects numerous data sources\non one end and caters to diverse human demands on the other end. Acting as an\nexperienced expert, Data-Copilot autonomously transforms raw data into\nmulti-form output that best matches the user's intent. Specifically, it first\ndesigns multiple universal interfaces to satisfy diverse data-related requests,\nlike querying, analysis, prediction, and visualization. In real-time response,\nit automatically deploys a concise workflow by invoking corresponding\ninterfaces. The whole process is fully controlled by Data-Copilot, without\nhuman assistance. We release Data-Copilot-1.0 using massive Chinese financial\ndata, e.g., stocks, funds, and news. Experiments indicate it achieves reliable\nperformance with lower token consumption, showing promising application\nprospects.\n", "link": "http://arxiv.org/abs/2306.07209v3", "date": "2024-05-06", "relevancy": 0.9686, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4938}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4814}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Copilot%3A%20Bridging%20Billions%20of%20Data%20and%20Humans%20with%20Autonomous%0A%20%20Workflow&body=Title%3A%20Data-Copilot%3A%20Bridging%20Billions%20of%20Data%20and%20Humans%20with%20Autonomous%0A%20%20Workflow%0AAuthor%3A%20Wenqi%20Zhang%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Various%20industries%20such%20as%20finance%2C%20meteorology%2C%20and%20energy%20produce%20vast%0Aamounts%20of%20heterogeneous%20data%20every%20day.%20There%20is%20a%20natural%20demand%20for%20humans%0Ato%20manage%2C%20process%2C%20and%20display%20data%20efficiently.%20However%2C%20it%20necessitates%0Alabor-intensive%20efforts%20and%20a%20high%20level%20of%20expertise%20for%20these%20data-related%0Atasks.%20Considering%20large%20language%20models%20%28LLMs%29%20showcase%20promising%20capabilities%0Ain%20semantic%20understanding%20and%20reasoning%2C%20we%20advocate%20that%20the%20deployment%20of%0ALLMs%20could%20autonomously%20manage%20and%20process%20massive%20amounts%20of%20data%20while%0Ainteracting%20and%20displaying%20in%20a%20human-friendly%20manner.%20Based%20on%20this%2C%20we%0Apropose%20Data-Copilot%2C%20an%20LLM-based%20system%20that%20connects%20numerous%20data%20sources%0Aon%20one%20end%20and%20caters%20to%20diverse%20human%20demands%20on%20the%20other%20end.%20Acting%20as%20an%0Aexperienced%20expert%2C%20Data-Copilot%20autonomously%20transforms%20raw%20data%20into%0Amulti-form%20output%20that%20best%20matches%20the%20user%27s%20intent.%20Specifically%2C%20it%20first%0Adesigns%20multiple%20universal%20interfaces%20to%20satisfy%20diverse%20data-related%20requests%2C%0Alike%20querying%2C%20analysis%2C%20prediction%2C%20and%20visualization.%20In%20real-time%20response%2C%0Ait%20automatically%20deploys%20a%20concise%20workflow%20by%20invoking%20corresponding%0Ainterfaces.%20The%20whole%20process%20is%20fully%20controlled%20by%20Data-Copilot%2C%20without%0Ahuman%20assistance.%20We%20release%20Data-Copilot-1.0%20using%20massive%20Chinese%20financial%0Adata%2C%20e.g.%2C%20stocks%2C%20funds%2C%20and%20news.%20Experiments%20indicate%20it%20achieves%20reliable%0Aperformance%20with%20lower%20token%20consumption%2C%20showing%20promising%20application%0Aprospects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.07209v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Copilot%253A%2520Bridging%2520Billions%2520of%2520Data%2520and%2520Humans%2520with%2520Autonomous%250A%2520%2520Workflow%26entry.906535625%3DWenqi%2520Zhang%2520and%2520Yongliang%2520Shen%2520and%2520Weiming%2520Lu%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Various%2520industries%2520such%2520as%2520finance%252C%2520meteorology%252C%2520and%2520energy%2520produce%2520vast%250Aamounts%2520of%2520heterogeneous%2520data%2520every%2520day.%2520There%2520is%2520a%2520natural%2520demand%2520for%2520humans%250Ato%2520manage%252C%2520process%252C%2520and%2520display%2520data%2520efficiently.%2520However%252C%2520it%2520necessitates%250Alabor-intensive%2520efforts%2520and%2520a%2520high%2520level%2520of%2520expertise%2520for%2520these%2520data-related%250Atasks.%2520Considering%2520large%2520language%2520models%2520%2528LLMs%2529%2520showcase%2520promising%2520capabilities%250Ain%2520semantic%2520understanding%2520and%2520reasoning%252C%2520we%2520advocate%2520that%2520the%2520deployment%2520of%250ALLMs%2520could%2520autonomously%2520manage%2520and%2520process%2520massive%2520amounts%2520of%2520data%2520while%250Ainteracting%2520and%2520displaying%2520in%2520a%2520human-friendly%2520manner.%2520Based%2520on%2520this%252C%2520we%250Apropose%2520Data-Copilot%252C%2520an%2520LLM-based%2520system%2520that%2520connects%2520numerous%2520data%2520sources%250Aon%2520one%2520end%2520and%2520caters%2520to%2520diverse%2520human%2520demands%2520on%2520the%2520other%2520end.%2520Acting%2520as%2520an%250Aexperienced%2520expert%252C%2520Data-Copilot%2520autonomously%2520transforms%2520raw%2520data%2520into%250Amulti-form%2520output%2520that%2520best%2520matches%2520the%2520user%2527s%2520intent.%2520Specifically%252C%2520it%2520first%250Adesigns%2520multiple%2520universal%2520interfaces%2520to%2520satisfy%2520diverse%2520data-related%2520requests%252C%250Alike%2520querying%252C%2520analysis%252C%2520prediction%252C%2520and%2520visualization.%2520In%2520real-time%2520response%252C%250Ait%2520automatically%2520deploys%2520a%2520concise%2520workflow%2520by%2520invoking%2520corresponding%250Ainterfaces.%2520The%2520whole%2520process%2520is%2520fully%2520controlled%2520by%2520Data-Copilot%252C%2520without%250Ahuman%2520assistance.%2520We%2520release%2520Data-Copilot-1.0%2520using%2520massive%2520Chinese%2520financial%250Adata%252C%2520e.g.%252C%2520stocks%252C%2520funds%252C%2520and%2520news.%2520Experiments%2520indicate%2520it%2520achieves%2520reliable%250Aperformance%2520with%2520lower%2520token%2520consumption%252C%2520showing%2520promising%2520application%250Aprospects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.07209v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Copilot%3A%20Bridging%20Billions%20of%20Data%20and%20Humans%20with%20Autonomous%0A%20%20Workflow&entry.906535625=Wenqi%20Zhang%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Various%20industries%20such%20as%20finance%2C%20meteorology%2C%20and%20energy%20produce%20vast%0Aamounts%20of%20heterogeneous%20data%20every%20day.%20There%20is%20a%20natural%20demand%20for%20humans%0Ato%20manage%2C%20process%2C%20and%20display%20data%20efficiently.%20However%2C%20it%20necessitates%0Alabor-intensive%20efforts%20and%20a%20high%20level%20of%20expertise%20for%20these%20data-related%0Atasks.%20Considering%20large%20language%20models%20%28LLMs%29%20showcase%20promising%20capabilities%0Ain%20semantic%20understanding%20and%20reasoning%2C%20we%20advocate%20that%20the%20deployment%20of%0ALLMs%20could%20autonomously%20manage%20and%20process%20massive%20amounts%20of%20data%20while%0Ainteracting%20and%20displaying%20in%20a%20human-friendly%20manner.%20Based%20on%20this%2C%20we%0Apropose%20Data-Copilot%2C%20an%20LLM-based%20system%20that%20connects%20numerous%20data%20sources%0Aon%20one%20end%20and%20caters%20to%20diverse%20human%20demands%20on%20the%20other%20end.%20Acting%20as%20an%0Aexperienced%20expert%2C%20Data-Copilot%20autonomously%20transforms%20raw%20data%20into%0Amulti-form%20output%20that%20best%20matches%20the%20user%27s%20intent.%20Specifically%2C%20it%20first%0Adesigns%20multiple%20universal%20interfaces%20to%20satisfy%20diverse%20data-related%20requests%2C%0Alike%20querying%2C%20analysis%2C%20prediction%2C%20and%20visualization.%20In%20real-time%20response%2C%0Ait%20automatically%20deploys%20a%20concise%20workflow%20by%20invoking%20corresponding%0Ainterfaces.%20The%20whole%20process%20is%20fully%20controlled%20by%20Data-Copilot%2C%20without%0Ahuman%20assistance.%20We%20release%20Data-Copilot-1.0%20using%20massive%20Chinese%20financial%0Adata%2C%20e.g.%2C%20stocks%2C%20funds%2C%20and%20news.%20Experiments%20indicate%20it%20achieves%20reliable%0Aperformance%20with%20lower%20token%20consumption%2C%20showing%20promising%20application%0Aprospects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.07209v3&entry.124074799=Read"},
{"title": "Competitive strategies to use \"warm start\" algorithms with predictions", "author": "Vaidehi Srinivas and Avrim Blum", "abstract": "  We consider the problem of learning and using predictions for warm start\nalgorithms with predictions. In this setting, an algorithm is given an instance\nof a problem, and a prediction of the solution. The runtime of the algorithm is\nbounded by the distance from the predicted solution to the true solution of the\ninstance. Previous work has shown that when instances are drawn iid from some\ndistribution, it is possible to learn an approximately optimal fixed prediction\n(Dinitz et al, NeurIPS 2021), and in the adversarial online case, it is\npossible to compete with the best fixed prediction in hindsight (Khodak et al,\nNeurIPS 2022).\n  In this work we give competitive guarantees against stronger benchmarks that\nconsider a set of $k$ predictions $\\mathbf{P}$. That is, the \"optimal offline\ncost\" to solve an instance with respect to $\\mathbf{P}$ is the distance from\nthe true solution to the closest member of $\\mathbf{P}$. This is analogous to\nthe $k$-medians objective function. In the distributional setting, we show a\nsimple strategy that incurs cost that is at most an $O(k)$ factor worse than\nthe optimal offline cost. We then show a way to leverage learnable coarse\ninformation, in the form of partitions of the instance space into groups of\n\"similar\" instances, that allows us to potentially avoid this $O(k)$ factor.\n  Finally, we consider an online version of the problem, where we compete\nagainst offline strategies that are allowed to maintain a moving set of $k$\npredictions or \"trajectories,\" and are charged for how much the predictions\nmove. We give an algorithm that does at most $O(k^4 \\ln^2 k)$ times as much\nwork as any offline strategy of $k$ trajectories. This algorithm is\ndeterministic (robust to an adaptive adversary), and oblivious to the setting\nof $k$. Thus the guarantee holds for all $k$ simultaneously.\n", "link": "http://arxiv.org/abs/2405.03661v1", "date": "2024-05-06", "relevancy": 1.8521, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4853}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4777}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Competitive%20strategies%20to%20use%20%22warm%20start%22%20algorithms%20with%20predictions&body=Title%3A%20Competitive%20strategies%20to%20use%20%22warm%20start%22%20algorithms%20with%20predictions%0AAuthor%3A%20Vaidehi%20Srinivas%20and%20Avrim%20Blum%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20learning%20and%20using%20predictions%20for%20warm%20start%0Aalgorithms%20with%20predictions.%20In%20this%20setting%2C%20an%20algorithm%20is%20given%20an%20instance%0Aof%20a%20problem%2C%20and%20a%20prediction%20of%20the%20solution.%20The%20runtime%20of%20the%20algorithm%20is%0Abounded%20by%20the%20distance%20from%20the%20predicted%20solution%20to%20the%20true%20solution%20of%20the%0Ainstance.%20Previous%20work%20has%20shown%20that%20when%20instances%20are%20drawn%20iid%20from%20some%0Adistribution%2C%20it%20is%20possible%20to%20learn%20an%20approximately%20optimal%20fixed%20prediction%0A%28Dinitz%20et%20al%2C%20NeurIPS%202021%29%2C%20and%20in%20the%20adversarial%20online%20case%2C%20it%20is%0Apossible%20to%20compete%20with%20the%20best%20fixed%20prediction%20in%20hindsight%20%28Khodak%20et%20al%2C%0ANeurIPS%202022%29.%0A%20%20In%20this%20work%20we%20give%20competitive%20guarantees%20against%20stronger%20benchmarks%20that%0Aconsider%20a%20set%20of%20%24k%24%20predictions%20%24%5Cmathbf%7BP%7D%24.%20That%20is%2C%20the%20%22optimal%20offline%0Acost%22%20to%20solve%20an%20instance%20with%20respect%20to%20%24%5Cmathbf%7BP%7D%24%20is%20the%20distance%20from%0Athe%20true%20solution%20to%20the%20closest%20member%20of%20%24%5Cmathbf%7BP%7D%24.%20This%20is%20analogous%20to%0Athe%20%24k%24-medians%20objective%20function.%20In%20the%20distributional%20setting%2C%20we%20show%20a%0Asimple%20strategy%20that%20incurs%20cost%20that%20is%20at%20most%20an%20%24O%28k%29%24%20factor%20worse%20than%0Athe%20optimal%20offline%20cost.%20We%20then%20show%20a%20way%20to%20leverage%20learnable%20coarse%0Ainformation%2C%20in%20the%20form%20of%20partitions%20of%20the%20instance%20space%20into%20groups%20of%0A%22similar%22%20instances%2C%20that%20allows%20us%20to%20potentially%20avoid%20this%20%24O%28k%29%24%20factor.%0A%20%20Finally%2C%20we%20consider%20an%20online%20version%20of%20the%20problem%2C%20where%20we%20compete%0Aagainst%20offline%20strategies%20that%20are%20allowed%20to%20maintain%20a%20moving%20set%20of%20%24k%24%0Apredictions%20or%20%22trajectories%2C%22%20and%20are%20charged%20for%20how%20much%20the%20predictions%0Amove.%20We%20give%20an%20algorithm%20that%20does%20at%20most%20%24O%28k%5E4%20%5Cln%5E2%20k%29%24%20times%20as%20much%0Awork%20as%20any%20offline%20strategy%20of%20%24k%24%20trajectories.%20This%20algorithm%20is%0Adeterministic%20%28robust%20to%20an%20adaptive%20adversary%29%2C%20and%20oblivious%20to%20the%20setting%0Aof%20%24k%24.%20Thus%20the%20guarantee%20holds%20for%20all%20%24k%24%20simultaneously.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompetitive%2520strategies%2520to%2520use%2520%2522warm%2520start%2522%2520algorithms%2520with%2520predictions%26entry.906535625%3DVaidehi%2520Srinivas%2520and%2520Avrim%2520Blum%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520learning%2520and%2520using%2520predictions%2520for%2520warm%2520start%250Aalgorithms%2520with%2520predictions.%2520In%2520this%2520setting%252C%2520an%2520algorithm%2520is%2520given%2520an%2520instance%250Aof%2520a%2520problem%252C%2520and%2520a%2520prediction%2520of%2520the%2520solution.%2520The%2520runtime%2520of%2520the%2520algorithm%2520is%250Abounded%2520by%2520the%2520distance%2520from%2520the%2520predicted%2520solution%2520to%2520the%2520true%2520solution%2520of%2520the%250Ainstance.%2520Previous%2520work%2520has%2520shown%2520that%2520when%2520instances%2520are%2520drawn%2520iid%2520from%2520some%250Adistribution%252C%2520it%2520is%2520possible%2520to%2520learn%2520an%2520approximately%2520optimal%2520fixed%2520prediction%250A%2528Dinitz%2520et%2520al%252C%2520NeurIPS%25202021%2529%252C%2520and%2520in%2520the%2520adversarial%2520online%2520case%252C%2520it%2520is%250Apossible%2520to%2520compete%2520with%2520the%2520best%2520fixed%2520prediction%2520in%2520hindsight%2520%2528Khodak%2520et%2520al%252C%250ANeurIPS%25202022%2529.%250A%2520%2520In%2520this%2520work%2520we%2520give%2520competitive%2520guarantees%2520against%2520stronger%2520benchmarks%2520that%250Aconsider%2520a%2520set%2520of%2520%2524k%2524%2520predictions%2520%2524%255Cmathbf%257BP%257D%2524.%2520That%2520is%252C%2520the%2520%2522optimal%2520offline%250Acost%2522%2520to%2520solve%2520an%2520instance%2520with%2520respect%2520to%2520%2524%255Cmathbf%257BP%257D%2524%2520is%2520the%2520distance%2520from%250Athe%2520true%2520solution%2520to%2520the%2520closest%2520member%2520of%2520%2524%255Cmathbf%257BP%257D%2524.%2520This%2520is%2520analogous%2520to%250Athe%2520%2524k%2524-medians%2520objective%2520function.%2520In%2520the%2520distributional%2520setting%252C%2520we%2520show%2520a%250Asimple%2520strategy%2520that%2520incurs%2520cost%2520that%2520is%2520at%2520most%2520an%2520%2524O%2528k%2529%2524%2520factor%2520worse%2520than%250Athe%2520optimal%2520offline%2520cost.%2520We%2520then%2520show%2520a%2520way%2520to%2520leverage%2520learnable%2520coarse%250Ainformation%252C%2520in%2520the%2520form%2520of%2520partitions%2520of%2520the%2520instance%2520space%2520into%2520groups%2520of%250A%2522similar%2522%2520instances%252C%2520that%2520allows%2520us%2520to%2520potentially%2520avoid%2520this%2520%2524O%2528k%2529%2524%2520factor.%250A%2520%2520Finally%252C%2520we%2520consider%2520an%2520online%2520version%2520of%2520the%2520problem%252C%2520where%2520we%2520compete%250Aagainst%2520offline%2520strategies%2520that%2520are%2520allowed%2520to%2520maintain%2520a%2520moving%2520set%2520of%2520%2524k%2524%250Apredictions%2520or%2520%2522trajectories%252C%2522%2520and%2520are%2520charged%2520for%2520how%2520much%2520the%2520predictions%250Amove.%2520We%2520give%2520an%2520algorithm%2520that%2520does%2520at%2520most%2520%2524O%2528k%255E4%2520%255Cln%255E2%2520k%2529%2524%2520times%2520as%2520much%250Awork%2520as%2520any%2520offline%2520strategy%2520of%2520%2524k%2524%2520trajectories.%2520This%2520algorithm%2520is%250Adeterministic%2520%2528robust%2520to%2520an%2520adaptive%2520adversary%2529%252C%2520and%2520oblivious%2520to%2520the%2520setting%250Aof%2520%2524k%2524.%2520Thus%2520the%2520guarantee%2520holds%2520for%2520all%2520%2524k%2524%2520simultaneously.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Competitive%20strategies%20to%20use%20%22warm%20start%22%20algorithms%20with%20predictions&entry.906535625=Vaidehi%20Srinivas%20and%20Avrim%20Blum&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20learning%20and%20using%20predictions%20for%20warm%20start%0Aalgorithms%20with%20predictions.%20In%20this%20setting%2C%20an%20algorithm%20is%20given%20an%20instance%0Aof%20a%20problem%2C%20and%20a%20prediction%20of%20the%20solution.%20The%20runtime%20of%20the%20algorithm%20is%0Abounded%20by%20the%20distance%20from%20the%20predicted%20solution%20to%20the%20true%20solution%20of%20the%0Ainstance.%20Previous%20work%20has%20shown%20that%20when%20instances%20are%20drawn%20iid%20from%20some%0Adistribution%2C%20it%20is%20possible%20to%20learn%20an%20approximately%20optimal%20fixed%20prediction%0A%28Dinitz%20et%20al%2C%20NeurIPS%202021%29%2C%20and%20in%20the%20adversarial%20online%20case%2C%20it%20is%0Apossible%20to%20compete%20with%20the%20best%20fixed%20prediction%20in%20hindsight%20%28Khodak%20et%20al%2C%0ANeurIPS%202022%29.%0A%20%20In%20this%20work%20we%20give%20competitive%20guarantees%20against%20stronger%20benchmarks%20that%0Aconsider%20a%20set%20of%20%24k%24%20predictions%20%24%5Cmathbf%7BP%7D%24.%20That%20is%2C%20the%20%22optimal%20offline%0Acost%22%20to%20solve%20an%20instance%20with%20respect%20to%20%24%5Cmathbf%7BP%7D%24%20is%20the%20distance%20from%0Athe%20true%20solution%20to%20the%20closest%20member%20of%20%24%5Cmathbf%7BP%7D%24.%20This%20is%20analogous%20to%0Athe%20%24k%24-medians%20objective%20function.%20In%20the%20distributional%20setting%2C%20we%20show%20a%0Asimple%20strategy%20that%20incurs%20cost%20that%20is%20at%20most%20an%20%24O%28k%29%24%20factor%20worse%20than%0Athe%20optimal%20offline%20cost.%20We%20then%20show%20a%20way%20to%20leverage%20learnable%20coarse%0Ainformation%2C%20in%20the%20form%20of%20partitions%20of%20the%20instance%20space%20into%20groups%20of%0A%22similar%22%20instances%2C%20that%20allows%20us%20to%20potentially%20avoid%20this%20%24O%28k%29%24%20factor.%0A%20%20Finally%2C%20we%20consider%20an%20online%20version%20of%20the%20problem%2C%20where%20we%20compete%0Aagainst%20offline%20strategies%20that%20are%20allowed%20to%20maintain%20a%20moving%20set%20of%20%24k%24%0Apredictions%20or%20%22trajectories%2C%22%20and%20are%20charged%20for%20how%20much%20the%20predictions%0Amove.%20We%20give%20an%20algorithm%20that%20does%20at%20most%20%24O%28k%5E4%20%5Cln%5E2%20k%29%24%20times%20as%20much%0Awork%20as%20any%20offline%20strategy%20of%20%24k%24%20trajectories.%20This%20algorithm%20is%0Adeterministic%20%28robust%20to%20an%20adaptive%20adversary%29%2C%20and%20oblivious%20to%20the%20setting%0Aof%20%24k%24.%20Thus%20the%20guarantee%20holds%20for%20all%20%24k%24%20simultaneously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03661v1&entry.124074799=Read"},
{"title": "Can Language Model Moderators Improve the Health of Online Discourse?", "author": "Hyundong Cho and Shuai Liu and Taiwei Shi and Darpan Jain and Basem Rizk and Yuyang Huang and Zixun Lu and Nuan Wen and Jonathan Gratch and Emilio Ferrara and Jonathan May", "abstract": "  Conversational moderation of online communities is crucial to maintaining\ncivility for a constructive environment, but it is challenging to scale and\nharmful to moderators. The inclusion of sophisticated natural language\ngeneration modules as a force multiplier to aid human moderators is a\ntantalizing prospect, but adequate evaluation approaches have so far been\nelusive. In this paper, we establish a systematic definition of conversational\nmoderation effectiveness grounded on moderation literature and establish design\ncriteria for conducting realistic yet safe evaluation. We then propose a\ncomprehensive evaluation framework to assess models' moderation capabilities\nindependently of human intervention. With our framework, we conduct the first\nknown study of language models as conversational moderators, finding that\nappropriately prompted models that incorporate insights from social science can\nprovide specific and fair feedback on toxic behavior but struggle to influence\nusers to increase their levels of respect and cooperation.\n", "link": "http://arxiv.org/abs/2311.10781v2", "date": "2024-05-06", "relevancy": 0.8918, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4686}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4409}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Language%20Model%20Moderators%20Improve%20the%20Health%20of%20Online%20Discourse%3F&body=Title%3A%20Can%20Language%20Model%20Moderators%20Improve%20the%20Health%20of%20Online%20Discourse%3F%0AAuthor%3A%20Hyundong%20Cho%20and%20Shuai%20Liu%20and%20Taiwei%20Shi%20and%20Darpan%20Jain%20and%20Basem%20Rizk%20and%20Yuyang%20Huang%20and%20Zixun%20Lu%20and%20Nuan%20Wen%20and%20Jonathan%20Gratch%20and%20Emilio%20Ferrara%20and%20Jonathan%20May%0AAbstract%3A%20%20%20Conversational%20moderation%20of%20online%20communities%20is%20crucial%20to%20maintaining%0Acivility%20for%20a%20constructive%20environment%2C%20but%20it%20is%20challenging%20to%20scale%20and%0Aharmful%20to%20moderators.%20The%20inclusion%20of%20sophisticated%20natural%20language%0Ageneration%20modules%20as%20a%20force%20multiplier%20to%20aid%20human%20moderators%20is%20a%0Atantalizing%20prospect%2C%20but%20adequate%20evaluation%20approaches%20have%20so%20far%20been%0Aelusive.%20In%20this%20paper%2C%20we%20establish%20a%20systematic%20definition%20of%20conversational%0Amoderation%20effectiveness%20grounded%20on%20moderation%20literature%20and%20establish%20design%0Acriteria%20for%20conducting%20realistic%20yet%20safe%20evaluation.%20We%20then%20propose%20a%0Acomprehensive%20evaluation%20framework%20to%20assess%20models%27%20moderation%20capabilities%0Aindependently%20of%20human%20intervention.%20With%20our%20framework%2C%20we%20conduct%20the%20first%0Aknown%20study%20of%20language%20models%20as%20conversational%20moderators%2C%20finding%20that%0Aappropriately%20prompted%20models%20that%20incorporate%20insights%20from%20social%20science%20can%0Aprovide%20specific%20and%20fair%20feedback%20on%20toxic%20behavior%20but%20struggle%20to%20influence%0Ausers%20to%20increase%20their%20levels%20of%20respect%20and%20cooperation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10781v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Language%2520Model%2520Moderators%2520Improve%2520the%2520Health%2520of%2520Online%2520Discourse%253F%26entry.906535625%3DHyundong%2520Cho%2520and%2520Shuai%2520Liu%2520and%2520Taiwei%2520Shi%2520and%2520Darpan%2520Jain%2520and%2520Basem%2520Rizk%2520and%2520Yuyang%2520Huang%2520and%2520Zixun%2520Lu%2520and%2520Nuan%2520Wen%2520and%2520Jonathan%2520Gratch%2520and%2520Emilio%2520Ferrara%2520and%2520Jonathan%2520May%26entry.1292438233%3D%2520%2520Conversational%2520moderation%2520of%2520online%2520communities%2520is%2520crucial%2520to%2520maintaining%250Acivility%2520for%2520a%2520constructive%2520environment%252C%2520but%2520it%2520is%2520challenging%2520to%2520scale%2520and%250Aharmful%2520to%2520moderators.%2520The%2520inclusion%2520of%2520sophisticated%2520natural%2520language%250Ageneration%2520modules%2520as%2520a%2520force%2520multiplier%2520to%2520aid%2520human%2520moderators%2520is%2520a%250Atantalizing%2520prospect%252C%2520but%2520adequate%2520evaluation%2520approaches%2520have%2520so%2520far%2520been%250Aelusive.%2520In%2520this%2520paper%252C%2520we%2520establish%2520a%2520systematic%2520definition%2520of%2520conversational%250Amoderation%2520effectiveness%2520grounded%2520on%2520moderation%2520literature%2520and%2520establish%2520design%250Acriteria%2520for%2520conducting%2520realistic%2520yet%2520safe%2520evaluation.%2520We%2520then%2520propose%2520a%250Acomprehensive%2520evaluation%2520framework%2520to%2520assess%2520models%2527%2520moderation%2520capabilities%250Aindependently%2520of%2520human%2520intervention.%2520With%2520our%2520framework%252C%2520we%2520conduct%2520the%2520first%250Aknown%2520study%2520of%2520language%2520models%2520as%2520conversational%2520moderators%252C%2520finding%2520that%250Aappropriately%2520prompted%2520models%2520that%2520incorporate%2520insights%2520from%2520social%2520science%2520can%250Aprovide%2520specific%2520and%2520fair%2520feedback%2520on%2520toxic%2520behavior%2520but%2520struggle%2520to%2520influence%250Ausers%2520to%2520increase%2520their%2520levels%2520of%2520respect%2520and%2520cooperation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.10781v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Language%20Model%20Moderators%20Improve%20the%20Health%20of%20Online%20Discourse%3F&entry.906535625=Hyundong%20Cho%20and%20Shuai%20Liu%20and%20Taiwei%20Shi%20and%20Darpan%20Jain%20and%20Basem%20Rizk%20and%20Yuyang%20Huang%20and%20Zixun%20Lu%20and%20Nuan%20Wen%20and%20Jonathan%20Gratch%20and%20Emilio%20Ferrara%20and%20Jonathan%20May&entry.1292438233=%20%20Conversational%20moderation%20of%20online%20communities%20is%20crucial%20to%20maintaining%0Acivility%20for%20a%20constructive%20environment%2C%20but%20it%20is%20challenging%20to%20scale%20and%0Aharmful%20to%20moderators.%20The%20inclusion%20of%20sophisticated%20natural%20language%0Ageneration%20modules%20as%20a%20force%20multiplier%20to%20aid%20human%20moderators%20is%20a%0Atantalizing%20prospect%2C%20but%20adequate%20evaluation%20approaches%20have%20so%20far%20been%0Aelusive.%20In%20this%20paper%2C%20we%20establish%20a%20systematic%20definition%20of%20conversational%0Amoderation%20effectiveness%20grounded%20on%20moderation%20literature%20and%20establish%20design%0Acriteria%20for%20conducting%20realistic%20yet%20safe%20evaluation.%20We%20then%20propose%20a%0Acomprehensive%20evaluation%20framework%20to%20assess%20models%27%20moderation%20capabilities%0Aindependently%20of%20human%20intervention.%20With%20our%20framework%2C%20we%20conduct%20the%20first%0Aknown%20study%20of%20language%20models%20as%20conversational%20moderators%2C%20finding%20that%0Aappropriately%20prompted%20models%20that%20incorporate%20insights%20from%20social%20science%20can%0Aprovide%20specific%20and%20fair%20feedback%20on%20toxic%20behavior%20but%20struggle%20to%20influence%0Ausers%20to%20increase%20their%20levels%20of%20respect%20and%20cooperation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10781v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


