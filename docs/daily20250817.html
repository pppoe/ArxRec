<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250814.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MinD-3D++: Advancing fMRI-Based 3D Reconstruction with High-Quality\n  Textured Mesh Generation and a Comprehensive Dataset", "author": "Jianxiong Gao and Yanwei Fu and Yuqian Fu and Yun Wang and Xuelin Qian and Jianfeng Feng", "abstract": "  Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI)\ndata, introduced as Recon3DMind, is of significant interest to both cognitive\nneuroscience and computer vision. To advance this task, we present the fMRI-3D\ndataset, which includes data from 15 participants and showcases a total of\n4,768 3D objects. The dataset consists of two components: fMRI-Shape,\npreviously introduced and available at\nhttps://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape, and fMRI-Objaverse,\nproposed in this paper and available at\nhttps://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverse\nincludes data from 5 subjects, 4 of whom are also part of the core set in\nfMRI-Shape. Each subject views 3,142 3D objects across 117 categories, all\naccompanied by text captions. This significantly enhances the diversity and\npotential applications of the dataset. Moreover, we propose MinD-3D++, a novel\nframework for decoding textured 3D visual information from fMRI signals. The\nframework evaluates the feasibility of not only reconstructing 3D objects from\nthe human mind but also generating, for the first time, 3D textured meshes with\ndetailed textures from fMRI data. We establish new benchmarks by designing\nmetrics at the semantic, structural, and textured levels to evaluate model\nperformance. Furthermore, we assess the model's effectiveness in\nout-of-distribution settings and analyze the attribution of the proposed 3D\npari fMRI dataset in visual regions of interest (ROIs) in fMRI signals. Our\nexperiments demonstrate that MinD-3D++ not only reconstructs 3D objects with\nhigh semantic and spatial accuracy but also provides deeper insights into how\nthe human brain processes 3D visual information. Project page:\nhttps://jianxgao.github.io/MinD-3D.\n", "link": "http://arxiv.org/abs/2409.11315v3", "date": "2025-08-14", "relevancy": 3.1871, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6591}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6591}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MinD-3D%2B%2B%3A%20Advancing%20fMRI-Based%203D%20Reconstruction%20with%20High-Quality%0A%20%20Textured%20Mesh%20Generation%20and%20a%20Comprehensive%20Dataset&body=Title%3A%20MinD-3D%2B%2B%3A%20Advancing%20fMRI-Based%203D%20Reconstruction%20with%20High-Quality%0A%20%20Textured%20Mesh%20Generation%20and%20a%20Comprehensive%20Dataset%0AAuthor%3A%20Jianxiong%20Gao%20and%20Yanwei%20Fu%20and%20Yuqian%20Fu%20and%20Yun%20Wang%20and%20Xuelin%20Qian%20and%20Jianfeng%20Feng%0AAbstract%3A%20%20%20Reconstructing%203D%20visuals%20from%20functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%0Adata%2C%20introduced%20as%20Recon3DMind%2C%20is%20of%20significant%20interest%20to%20both%20cognitive%0Aneuroscience%20and%20computer%20vision.%20To%20advance%20this%20task%2C%20we%20present%20the%20fMRI-3D%0Adataset%2C%20which%20includes%20data%20from%2015%20participants%20and%20showcases%20a%20total%20of%0A4%2C768%203D%20objects.%20The%20dataset%20consists%20of%20two%20components%3A%20fMRI-Shape%2C%0Apreviously%20introduced%20and%20available%20at%0Ahttps%3A//huggingface.co/datasets/Fudan-fMRI/fMRI-Shape%2C%20and%20fMRI-Objaverse%2C%0Aproposed%20in%20this%20paper%20and%20available%20at%0Ahttps%3A//huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse.%20fMRI-Objaverse%0Aincludes%20data%20from%205%20subjects%2C%204%20of%20whom%20are%20also%20part%20of%20the%20core%20set%20in%0AfMRI-Shape.%20Each%20subject%20views%203%2C142%203D%20objects%20across%20117%20categories%2C%20all%0Aaccompanied%20by%20text%20captions.%20This%20significantly%20enhances%20the%20diversity%20and%0Apotential%20applications%20of%20the%20dataset.%20Moreover%2C%20we%20propose%20MinD-3D%2B%2B%2C%20a%20novel%0Aframework%20for%20decoding%20textured%203D%20visual%20information%20from%20fMRI%20signals.%20The%0Aframework%20evaluates%20the%20feasibility%20of%20not%20only%20reconstructing%203D%20objects%20from%0Athe%20human%20mind%20but%20also%20generating%2C%20for%20the%20first%20time%2C%203D%20textured%20meshes%20with%0Adetailed%20textures%20from%20fMRI%20data.%20We%20establish%20new%20benchmarks%20by%20designing%0Ametrics%20at%20the%20semantic%2C%20structural%2C%20and%20textured%20levels%20to%20evaluate%20model%0Aperformance.%20Furthermore%2C%20we%20assess%20the%20model%27s%20effectiveness%20in%0Aout-of-distribution%20settings%20and%20analyze%20the%20attribution%20of%20the%20proposed%203D%0Apari%20fMRI%20dataset%20in%20visual%20regions%20of%20interest%20%28ROIs%29%20in%20fMRI%20signals.%20Our%0Aexperiments%20demonstrate%20that%20MinD-3D%2B%2B%20not%20only%20reconstructs%203D%20objects%20with%0Ahigh%20semantic%20and%20spatial%20accuracy%20but%20also%20provides%20deeper%20insights%20into%20how%0Athe%20human%20brain%20processes%203D%20visual%20information.%20Project%20page%3A%0Ahttps%3A//jianxgao.github.io/MinD-3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11315v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinD-3D%252B%252B%253A%2520Advancing%2520fMRI-Based%25203D%2520Reconstruction%2520with%2520High-Quality%250A%2520%2520Textured%2520Mesh%2520Generation%2520and%2520a%2520Comprehensive%2520Dataset%26entry.906535625%3DJianxiong%2520Gao%2520and%2520Yanwei%2520Fu%2520and%2520Yuqian%2520Fu%2520and%2520Yun%2520Wang%2520and%2520Xuelin%2520Qian%2520and%2520Jianfeng%2520Feng%26entry.1292438233%3D%2520%2520Reconstructing%25203D%2520visuals%2520from%2520functional%2520Magnetic%2520Resonance%2520Imaging%2520%2528fMRI%2529%250Adata%252C%2520introduced%2520as%2520Recon3DMind%252C%2520is%2520of%2520significant%2520interest%2520to%2520both%2520cognitive%250Aneuroscience%2520and%2520computer%2520vision.%2520To%2520advance%2520this%2520task%252C%2520we%2520present%2520the%2520fMRI-3D%250Adataset%252C%2520which%2520includes%2520data%2520from%252015%2520participants%2520and%2520showcases%2520a%2520total%2520of%250A4%252C768%25203D%2520objects.%2520The%2520dataset%2520consists%2520of%2520two%2520components%253A%2520fMRI-Shape%252C%250Apreviously%2520introduced%2520and%2520available%2520at%250Ahttps%253A//huggingface.co/datasets/Fudan-fMRI/fMRI-Shape%252C%2520and%2520fMRI-Objaverse%252C%250Aproposed%2520in%2520this%2520paper%2520and%2520available%2520at%250Ahttps%253A//huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse.%2520fMRI-Objaverse%250Aincludes%2520data%2520from%25205%2520subjects%252C%25204%2520of%2520whom%2520are%2520also%2520part%2520of%2520the%2520core%2520set%2520in%250AfMRI-Shape.%2520Each%2520subject%2520views%25203%252C142%25203D%2520objects%2520across%2520117%2520categories%252C%2520all%250Aaccompanied%2520by%2520text%2520captions.%2520This%2520significantly%2520enhances%2520the%2520diversity%2520and%250Apotential%2520applications%2520of%2520the%2520dataset.%2520Moreover%252C%2520we%2520propose%2520MinD-3D%252B%252B%252C%2520a%2520novel%250Aframework%2520for%2520decoding%2520textured%25203D%2520visual%2520information%2520from%2520fMRI%2520signals.%2520The%250Aframework%2520evaluates%2520the%2520feasibility%2520of%2520not%2520only%2520reconstructing%25203D%2520objects%2520from%250Athe%2520human%2520mind%2520but%2520also%2520generating%252C%2520for%2520the%2520first%2520time%252C%25203D%2520textured%2520meshes%2520with%250Adetailed%2520textures%2520from%2520fMRI%2520data.%2520We%2520establish%2520new%2520benchmarks%2520by%2520designing%250Ametrics%2520at%2520the%2520semantic%252C%2520structural%252C%2520and%2520textured%2520levels%2520to%2520evaluate%2520model%250Aperformance.%2520Furthermore%252C%2520we%2520assess%2520the%2520model%2527s%2520effectiveness%2520in%250Aout-of-distribution%2520settings%2520and%2520analyze%2520the%2520attribution%2520of%2520the%2520proposed%25203D%250Apari%2520fMRI%2520dataset%2520in%2520visual%2520regions%2520of%2520interest%2520%2528ROIs%2529%2520in%2520fMRI%2520signals.%2520Our%250Aexperiments%2520demonstrate%2520that%2520MinD-3D%252B%252B%2520not%2520only%2520reconstructs%25203D%2520objects%2520with%250Ahigh%2520semantic%2520and%2520spatial%2520accuracy%2520but%2520also%2520provides%2520deeper%2520insights%2520into%2520how%250Athe%2520human%2520brain%2520processes%25203D%2520visual%2520information.%2520Project%2520page%253A%250Ahttps%253A//jianxgao.github.io/MinD-3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11315v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MinD-3D%2B%2B%3A%20Advancing%20fMRI-Based%203D%20Reconstruction%20with%20High-Quality%0A%20%20Textured%20Mesh%20Generation%20and%20a%20Comprehensive%20Dataset&entry.906535625=Jianxiong%20Gao%20and%20Yanwei%20Fu%20and%20Yuqian%20Fu%20and%20Yun%20Wang%20and%20Xuelin%20Qian%20and%20Jianfeng%20Feng&entry.1292438233=%20%20Reconstructing%203D%20visuals%20from%20functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%0Adata%2C%20introduced%20as%20Recon3DMind%2C%20is%20of%20significant%20interest%20to%20both%20cognitive%0Aneuroscience%20and%20computer%20vision.%20To%20advance%20this%20task%2C%20we%20present%20the%20fMRI-3D%0Adataset%2C%20which%20includes%20data%20from%2015%20participants%20and%20showcases%20a%20total%20of%0A4%2C768%203D%20objects.%20The%20dataset%20consists%20of%20two%20components%3A%20fMRI-Shape%2C%0Apreviously%20introduced%20and%20available%20at%0Ahttps%3A//huggingface.co/datasets/Fudan-fMRI/fMRI-Shape%2C%20and%20fMRI-Objaverse%2C%0Aproposed%20in%20this%20paper%20and%20available%20at%0Ahttps%3A//huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse.%20fMRI-Objaverse%0Aincludes%20data%20from%205%20subjects%2C%204%20of%20whom%20are%20also%20part%20of%20the%20core%20set%20in%0AfMRI-Shape.%20Each%20subject%20views%203%2C142%203D%20objects%20across%20117%20categories%2C%20all%0Aaccompanied%20by%20text%20captions.%20This%20significantly%20enhances%20the%20diversity%20and%0Apotential%20applications%20of%20the%20dataset.%20Moreover%2C%20we%20propose%20MinD-3D%2B%2B%2C%20a%20novel%0Aframework%20for%20decoding%20textured%203D%20visual%20information%20from%20fMRI%20signals.%20The%0Aframework%20evaluates%20the%20feasibility%20of%20not%20only%20reconstructing%203D%20objects%20from%0Athe%20human%20mind%20but%20also%20generating%2C%20for%20the%20first%20time%2C%203D%20textured%20meshes%20with%0Adetailed%20textures%20from%20fMRI%20data.%20We%20establish%20new%20benchmarks%20by%20designing%0Ametrics%20at%20the%20semantic%2C%20structural%2C%20and%20textured%20levels%20to%20evaluate%20model%0Aperformance.%20Furthermore%2C%20we%20assess%20the%20model%27s%20effectiveness%20in%0Aout-of-distribution%20settings%20and%20analyze%20the%20attribution%20of%20the%20proposed%203D%0Apari%20fMRI%20dataset%20in%20visual%20regions%20of%20interest%20%28ROIs%29%20in%20fMRI%20signals.%20Our%0Aexperiments%20demonstrate%20that%20MinD-3D%2B%2B%20not%20only%20reconstructs%203D%20objects%20with%0Ahigh%20semantic%20and%20spatial%20accuracy%20but%20also%20provides%20deeper%20insights%20into%20how%0Athe%20human%20brain%20processes%203D%20visual%20information.%20Project%20page%3A%0Ahttps%3A//jianxgao.github.io/MinD-3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11315v3&entry.124074799=Read"},
{"title": "CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian\n  Splatting", "author": "Lei Tian and Xiaomin Li and Liqian Ma and Hao Yin and Zirui Zheng and Hefei Huang and Taiqing Li and Huchuan Lu and Xu Jia", "abstract": "  Recent advances in 3D reconstruction techniques and vision-language models\nhave fueled significant progress in 3D semantic understanding, a capability\ncritical to robotics, autonomous driving, and virtual/augmented reality.\nHowever, methods that rely on 2D priors are prone to a critical challenge:\ncross-view semantic inconsistencies induced by occlusion, image blur, and\nview-dependent variations. These inconsistencies, when propagated via\nprojection supervision, deteriorate the quality of 3D Gaussian semantic fields\nand introduce artifacts in the rendered outputs. To mitigate this limitation,\nwe propose CCL-LGS, a novel framework that enforces view-consistent semantic\nsupervision by integrating multi-view semantic cues. Specifically, our approach\nfirst employs a zero-shot tracker to align a set of SAM-generated 2D masks and\nreliably identify their corresponding categories. Next, we utilize CLIP to\nextract robust semantic encodings across views. Finally, our Contrastive\nCodebook Learning (CCL) module distills discriminative semantic features by\nenforcing intra-class compactness and inter-class distinctiveness. In contrast\nto previous methods that directly apply CLIP to imperfect masks, our framework\nexplicitly resolves semantic conflicts while preserving category\ndiscriminability. Extensive experiments demonstrate that CCL-LGS outperforms\nprevious state-of-the-art methods. Our project page is available at\nhttps://epsilontl.github.io/CCL-LGS/.\n", "link": "http://arxiv.org/abs/2505.20469v2", "date": "2025-08-14", "relevancy": 3.1113, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6353}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6247}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CCL-LGS%3A%20Contrastive%20Codebook%20Learning%20for%203D%20Language%20Gaussian%0A%20%20Splatting&body=Title%3A%20CCL-LGS%3A%20Contrastive%20Codebook%20Learning%20for%203D%20Language%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Lei%20Tian%20and%20Xiaomin%20Li%20and%20Liqian%20Ma%20and%20Hao%20Yin%20and%20Zirui%20Zheng%20and%20Hefei%20Huang%20and%20Taiqing%20Li%20and%20Huchuan%20Lu%20and%20Xu%20Jia%0AAbstract%3A%20%20%20Recent%20advances%20in%203D%20reconstruction%20techniques%20and%20vision-language%20models%0Ahave%20fueled%20significant%20progress%20in%203D%20semantic%20understanding%2C%20a%20capability%0Acritical%20to%20robotics%2C%20autonomous%20driving%2C%20and%20virtual/augmented%20reality.%0AHowever%2C%20methods%20that%20rely%20on%202D%20priors%20are%20prone%20to%20a%20critical%20challenge%3A%0Across-view%20semantic%20inconsistencies%20induced%20by%20occlusion%2C%20image%20blur%2C%20and%0Aview-dependent%20variations.%20These%20inconsistencies%2C%20when%20propagated%20via%0Aprojection%20supervision%2C%20deteriorate%20the%20quality%20of%203D%20Gaussian%20semantic%20fields%0Aand%20introduce%20artifacts%20in%20the%20rendered%20outputs.%20To%20mitigate%20this%20limitation%2C%0Awe%20propose%20CCL-LGS%2C%20a%20novel%20framework%20that%20enforces%20view-consistent%20semantic%0Asupervision%20by%20integrating%20multi-view%20semantic%20cues.%20Specifically%2C%20our%20approach%0Afirst%20employs%20a%20zero-shot%20tracker%20to%20align%20a%20set%20of%20SAM-generated%202D%20masks%20and%0Areliably%20identify%20their%20corresponding%20categories.%20Next%2C%20we%20utilize%20CLIP%20to%0Aextract%20robust%20semantic%20encodings%20across%20views.%20Finally%2C%20our%20Contrastive%0ACodebook%20Learning%20%28CCL%29%20module%20distills%20discriminative%20semantic%20features%20by%0Aenforcing%20intra-class%20compactness%20and%20inter-class%20distinctiveness.%20In%20contrast%0Ato%20previous%20methods%20that%20directly%20apply%20CLIP%20to%20imperfect%20masks%2C%20our%20framework%0Aexplicitly%20resolves%20semantic%20conflicts%20while%20preserving%20category%0Adiscriminability.%20Extensive%20experiments%20demonstrate%20that%20CCL-LGS%20outperforms%0Aprevious%20state-of-the-art%20methods.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//epsilontl.github.io/CCL-LGS/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20469v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCCL-LGS%253A%2520Contrastive%2520Codebook%2520Learning%2520for%25203D%2520Language%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DLei%2520Tian%2520and%2520Xiaomin%2520Li%2520and%2520Liqian%2520Ma%2520and%2520Hao%2520Yin%2520and%2520Zirui%2520Zheng%2520and%2520Hefei%2520Huang%2520and%2520Taiqing%2520Li%2520and%2520Huchuan%2520Lu%2520and%2520Xu%2520Jia%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%25203D%2520reconstruction%2520techniques%2520and%2520vision-language%2520models%250Ahave%2520fueled%2520significant%2520progress%2520in%25203D%2520semantic%2520understanding%252C%2520a%2520capability%250Acritical%2520to%2520robotics%252C%2520autonomous%2520driving%252C%2520and%2520virtual/augmented%2520reality.%250AHowever%252C%2520methods%2520that%2520rely%2520on%25202D%2520priors%2520are%2520prone%2520to%2520a%2520critical%2520challenge%253A%250Across-view%2520semantic%2520inconsistencies%2520induced%2520by%2520occlusion%252C%2520image%2520blur%252C%2520and%250Aview-dependent%2520variations.%2520These%2520inconsistencies%252C%2520when%2520propagated%2520via%250Aprojection%2520supervision%252C%2520deteriorate%2520the%2520quality%2520of%25203D%2520Gaussian%2520semantic%2520fields%250Aand%2520introduce%2520artifacts%2520in%2520the%2520rendered%2520outputs.%2520To%2520mitigate%2520this%2520limitation%252C%250Awe%2520propose%2520CCL-LGS%252C%2520a%2520novel%2520framework%2520that%2520enforces%2520view-consistent%2520semantic%250Asupervision%2520by%2520integrating%2520multi-view%2520semantic%2520cues.%2520Specifically%252C%2520our%2520approach%250Afirst%2520employs%2520a%2520zero-shot%2520tracker%2520to%2520align%2520a%2520set%2520of%2520SAM-generated%25202D%2520masks%2520and%250Areliably%2520identify%2520their%2520corresponding%2520categories.%2520Next%252C%2520we%2520utilize%2520CLIP%2520to%250Aextract%2520robust%2520semantic%2520encodings%2520across%2520views.%2520Finally%252C%2520our%2520Contrastive%250ACodebook%2520Learning%2520%2528CCL%2529%2520module%2520distills%2520discriminative%2520semantic%2520features%2520by%250Aenforcing%2520intra-class%2520compactness%2520and%2520inter-class%2520distinctiveness.%2520In%2520contrast%250Ato%2520previous%2520methods%2520that%2520directly%2520apply%2520CLIP%2520to%2520imperfect%2520masks%252C%2520our%2520framework%250Aexplicitly%2520resolves%2520semantic%2520conflicts%2520while%2520preserving%2520category%250Adiscriminability.%2520Extensive%2520experiments%2520demonstrate%2520that%2520CCL-LGS%2520outperforms%250Aprevious%2520state-of-the-art%2520methods.%2520Our%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//epsilontl.github.io/CCL-LGS/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20469v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CCL-LGS%3A%20Contrastive%20Codebook%20Learning%20for%203D%20Language%20Gaussian%0A%20%20Splatting&entry.906535625=Lei%20Tian%20and%20Xiaomin%20Li%20and%20Liqian%20Ma%20and%20Hao%20Yin%20and%20Zirui%20Zheng%20and%20Hefei%20Huang%20and%20Taiqing%20Li%20and%20Huchuan%20Lu%20and%20Xu%20Jia&entry.1292438233=%20%20Recent%20advances%20in%203D%20reconstruction%20techniques%20and%20vision-language%20models%0Ahave%20fueled%20significant%20progress%20in%203D%20semantic%20understanding%2C%20a%20capability%0Acritical%20to%20robotics%2C%20autonomous%20driving%2C%20and%20virtual/augmented%20reality.%0AHowever%2C%20methods%20that%20rely%20on%202D%20priors%20are%20prone%20to%20a%20critical%20challenge%3A%0Across-view%20semantic%20inconsistencies%20induced%20by%20occlusion%2C%20image%20blur%2C%20and%0Aview-dependent%20variations.%20These%20inconsistencies%2C%20when%20propagated%20via%0Aprojection%20supervision%2C%20deteriorate%20the%20quality%20of%203D%20Gaussian%20semantic%20fields%0Aand%20introduce%20artifacts%20in%20the%20rendered%20outputs.%20To%20mitigate%20this%20limitation%2C%0Awe%20propose%20CCL-LGS%2C%20a%20novel%20framework%20that%20enforces%20view-consistent%20semantic%0Asupervision%20by%20integrating%20multi-view%20semantic%20cues.%20Specifically%2C%20our%20approach%0Afirst%20employs%20a%20zero-shot%20tracker%20to%20align%20a%20set%20of%20SAM-generated%202D%20masks%20and%0Areliably%20identify%20their%20corresponding%20categories.%20Next%2C%20we%20utilize%20CLIP%20to%0Aextract%20robust%20semantic%20encodings%20across%20views.%20Finally%2C%20our%20Contrastive%0ACodebook%20Learning%20%28CCL%29%20module%20distills%20discriminative%20semantic%20features%20by%0Aenforcing%20intra-class%20compactness%20and%20inter-class%20distinctiveness.%20In%20contrast%0Ato%20previous%20methods%20that%20directly%20apply%20CLIP%20to%20imperfect%20masks%2C%20our%20framework%0Aexplicitly%20resolves%20semantic%20conflicts%20while%20preserving%20category%0Adiscriminability.%20Extensive%20experiments%20demonstrate%20that%20CCL-LGS%20outperforms%0Aprevious%20state-of-the-art%20methods.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//epsilontl.github.io/CCL-LGS/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20469v2&entry.124074799=Read"},
{"title": "HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head\n  Synthesis", "author": "Shiyu Liu and Kui Jiang and Xianming Liu and Hongxun Yao and Xiaocheng Feng", "abstract": "  Audio-driven talking head video generation enhances user engagement in\nhuman-computer interaction. However, current methods frequently produce videos\nwith motion blur and lip jitter, primarily due to their reliance on implicit\nmodeling of audio-facial motion correlations--an approach lacking explicit\narticulatory priors (i.e., anatomical guidance for speech-related facial\nmovements). To overcome this limitation, we propose HM-Talker, a novel\nframework for generating high-fidelity, temporally coherent talking heads.\nHM-Talker leverages a hybrid motion representation combining both implicit and\nexplicit motion cues. Explicit cues use Action Units (AUs), anatomically\ndefined facial muscle movements, alongside implicit features to minimize\nphoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement\nModule (CMDM) extracts complementary implicit/explicit motion features while\npredicting AUs directly from audio input aligned to visual cues. To mitigate\nidentity-dependent biases in explicit features and enhance cross-subject\ngeneralization, we introduce the Hybrid Motion Modeling Module (HMMM). This\nmodule dynamically merges randomly paired implicit/explicit features, enforcing\nidentity-agnostic learning. Together, these components enable robust lip\nsynchronization across diverse identities, advancing personalized talking head\nsynthesis. Extensive experiments demonstrate HM-Talker's superiority over\nstate-of-the-art methods in visual quality and lip-sync accuracy.\n", "link": "http://arxiv.org/abs/2508.10566v1", "date": "2025-08-14", "relevancy": 3.068, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6395}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6007}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HM-Talker%3A%20Hybrid%20Motion%20Modeling%20for%20High-Fidelity%20Talking%20Head%0A%20%20Synthesis&body=Title%3A%20HM-Talker%3A%20Hybrid%20Motion%20Modeling%20for%20High-Fidelity%20Talking%20Head%0A%20%20Synthesis%0AAuthor%3A%20Shiyu%20Liu%20and%20Kui%20Jiang%20and%20Xianming%20Liu%20and%20Hongxun%20Yao%20and%20Xiaocheng%20Feng%0AAbstract%3A%20%20%20Audio-driven%20talking%20head%20video%20generation%20enhances%20user%20engagement%20in%0Ahuman-computer%20interaction.%20However%2C%20current%20methods%20frequently%20produce%20videos%0Awith%20motion%20blur%20and%20lip%20jitter%2C%20primarily%20due%20to%20their%20reliance%20on%20implicit%0Amodeling%20of%20audio-facial%20motion%20correlations--an%20approach%20lacking%20explicit%0Aarticulatory%20priors%20%28i.e.%2C%20anatomical%20guidance%20for%20speech-related%20facial%0Amovements%29.%20To%20overcome%20this%20limitation%2C%20we%20propose%20HM-Talker%2C%20a%20novel%0Aframework%20for%20generating%20high-fidelity%2C%20temporally%20coherent%20talking%20heads.%0AHM-Talker%20leverages%20a%20hybrid%20motion%20representation%20combining%20both%20implicit%20and%0Aexplicit%20motion%20cues.%20Explicit%20cues%20use%20Action%20Units%20%28AUs%29%2C%20anatomically%0Adefined%20facial%20muscle%20movements%2C%20alongside%20implicit%20features%20to%20minimize%0Aphoneme-viseme%20misalignment.%20Specifically%2C%20our%20Cross-Modal%20Disentanglement%0AModule%20%28CMDM%29%20extracts%20complementary%20implicit/explicit%20motion%20features%20while%0Apredicting%20AUs%20directly%20from%20audio%20input%20aligned%20to%20visual%20cues.%20To%20mitigate%0Aidentity-dependent%20biases%20in%20explicit%20features%20and%20enhance%20cross-subject%0Ageneralization%2C%20we%20introduce%20the%20Hybrid%20Motion%20Modeling%20Module%20%28HMMM%29.%20This%0Amodule%20dynamically%20merges%20randomly%20paired%20implicit/explicit%20features%2C%20enforcing%0Aidentity-agnostic%20learning.%20Together%2C%20these%20components%20enable%20robust%20lip%0Asynchronization%20across%20diverse%20identities%2C%20advancing%20personalized%20talking%20head%0Asynthesis.%20Extensive%20experiments%20demonstrate%20HM-Talker%27s%20superiority%20over%0Astate-of-the-art%20methods%20in%20visual%20quality%20and%20lip-sync%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHM-Talker%253A%2520Hybrid%2520Motion%2520Modeling%2520for%2520High-Fidelity%2520Talking%2520Head%250A%2520%2520Synthesis%26entry.906535625%3DShiyu%2520Liu%2520and%2520Kui%2520Jiang%2520and%2520Xianming%2520Liu%2520and%2520Hongxun%2520Yao%2520and%2520Xiaocheng%2520Feng%26entry.1292438233%3D%2520%2520Audio-driven%2520talking%2520head%2520video%2520generation%2520enhances%2520user%2520engagement%2520in%250Ahuman-computer%2520interaction.%2520However%252C%2520current%2520methods%2520frequently%2520produce%2520videos%250Awith%2520motion%2520blur%2520and%2520lip%2520jitter%252C%2520primarily%2520due%2520to%2520their%2520reliance%2520on%2520implicit%250Amodeling%2520of%2520audio-facial%2520motion%2520correlations--an%2520approach%2520lacking%2520explicit%250Aarticulatory%2520priors%2520%2528i.e.%252C%2520anatomical%2520guidance%2520for%2520speech-related%2520facial%250Amovements%2529.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520HM-Talker%252C%2520a%2520novel%250Aframework%2520for%2520generating%2520high-fidelity%252C%2520temporally%2520coherent%2520talking%2520heads.%250AHM-Talker%2520leverages%2520a%2520hybrid%2520motion%2520representation%2520combining%2520both%2520implicit%2520and%250Aexplicit%2520motion%2520cues.%2520Explicit%2520cues%2520use%2520Action%2520Units%2520%2528AUs%2529%252C%2520anatomically%250Adefined%2520facial%2520muscle%2520movements%252C%2520alongside%2520implicit%2520features%2520to%2520minimize%250Aphoneme-viseme%2520misalignment.%2520Specifically%252C%2520our%2520Cross-Modal%2520Disentanglement%250AModule%2520%2528CMDM%2529%2520extracts%2520complementary%2520implicit/explicit%2520motion%2520features%2520while%250Apredicting%2520AUs%2520directly%2520from%2520audio%2520input%2520aligned%2520to%2520visual%2520cues.%2520To%2520mitigate%250Aidentity-dependent%2520biases%2520in%2520explicit%2520features%2520and%2520enhance%2520cross-subject%250Ageneralization%252C%2520we%2520introduce%2520the%2520Hybrid%2520Motion%2520Modeling%2520Module%2520%2528HMMM%2529.%2520This%250Amodule%2520dynamically%2520merges%2520randomly%2520paired%2520implicit/explicit%2520features%252C%2520enforcing%250Aidentity-agnostic%2520learning.%2520Together%252C%2520these%2520components%2520enable%2520robust%2520lip%250Asynchronization%2520across%2520diverse%2520identities%252C%2520advancing%2520personalized%2520talking%2520head%250Asynthesis.%2520Extensive%2520experiments%2520demonstrate%2520HM-Talker%2527s%2520superiority%2520over%250Astate-of-the-art%2520methods%2520in%2520visual%2520quality%2520and%2520lip-sync%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HM-Talker%3A%20Hybrid%20Motion%20Modeling%20for%20High-Fidelity%20Talking%20Head%0A%20%20Synthesis&entry.906535625=Shiyu%20Liu%20and%20Kui%20Jiang%20and%20Xianming%20Liu%20and%20Hongxun%20Yao%20and%20Xiaocheng%20Feng&entry.1292438233=%20%20Audio-driven%20talking%20head%20video%20generation%20enhances%20user%20engagement%20in%0Ahuman-computer%20interaction.%20However%2C%20current%20methods%20frequently%20produce%20videos%0Awith%20motion%20blur%20and%20lip%20jitter%2C%20primarily%20due%20to%20their%20reliance%20on%20implicit%0Amodeling%20of%20audio-facial%20motion%20correlations--an%20approach%20lacking%20explicit%0Aarticulatory%20priors%20%28i.e.%2C%20anatomical%20guidance%20for%20speech-related%20facial%0Amovements%29.%20To%20overcome%20this%20limitation%2C%20we%20propose%20HM-Talker%2C%20a%20novel%0Aframework%20for%20generating%20high-fidelity%2C%20temporally%20coherent%20talking%20heads.%0AHM-Talker%20leverages%20a%20hybrid%20motion%20representation%20combining%20both%20implicit%20and%0Aexplicit%20motion%20cues.%20Explicit%20cues%20use%20Action%20Units%20%28AUs%29%2C%20anatomically%0Adefined%20facial%20muscle%20movements%2C%20alongside%20implicit%20features%20to%20minimize%0Aphoneme-viseme%20misalignment.%20Specifically%2C%20our%20Cross-Modal%20Disentanglement%0AModule%20%28CMDM%29%20extracts%20complementary%20implicit/explicit%20motion%20features%20while%0Apredicting%20AUs%20directly%20from%20audio%20input%20aligned%20to%20visual%20cues.%20To%20mitigate%0Aidentity-dependent%20biases%20in%20explicit%20features%20and%20enhance%20cross-subject%0Ageneralization%2C%20we%20introduce%20the%20Hybrid%20Motion%20Modeling%20Module%20%28HMMM%29.%20This%0Amodule%20dynamically%20merges%20randomly%20paired%20implicit/explicit%20features%2C%20enforcing%0Aidentity-agnostic%20learning.%20Together%2C%20these%20components%20enable%20robust%20lip%0Asynchronization%20across%20diverse%20identities%2C%20advancing%20personalized%20talking%20head%0Asynthesis.%20Extensive%20experiments%20demonstrate%20HM-Talker%27s%20superiority%20over%0Astate-of-the-art%20methods%20in%20visual%20quality%20and%20lip-sync%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10566v1&entry.124074799=Read"},
{"title": "Puppeteer: Rig and Animate Your 3D Models", "author": "Chaoyue Song and Xiu Li and Fan Yang and Zhongcong Xu and Jiacheng Wei and Fayao Liu and Jiashi Feng and Guosheng Lin and Jianfeng Zhang", "abstract": "  Modern interactive applications increasingly demand dynamic 3D content, yet\nthe transformation of static 3D models into animated assets constitutes a\nsignificant bottleneck in content creation pipelines. While recent advances in\ngenerative AI have revolutionized static 3D model creation, rigging and\nanimation continue to depend heavily on expert intervention. We present\nPuppeteer, a comprehensive framework that addresses both automatic rigging and\nanimation for diverse 3D objects. Our system first predicts plausible skeletal\nstructures via an auto-regressive transformer that introduces a joint-based\ntokenization strategy for compact representation and a hierarchical ordering\nmethodology with stochastic perturbation that enhances bidirectional learning\ncapabilities. It then infers skinning weights via an attention-based\narchitecture incorporating topology-aware joint attention that explicitly\nencodes inter-joint relationships based on skeletal graph distances. Finally,\nwe complement these rigging advances with a differentiable optimization-based\nanimation pipeline that generates stable, high-fidelity animations while being\ncomputationally more efficient than existing approaches. Extensive evaluations\nacross multiple benchmarks demonstrate that our method significantly\noutperforms state-of-the-art techniques in both skeletal prediction accuracy\nand skinning quality. The system robustly processes diverse 3D content, ranging\nfrom professionally designed game assets to AI-generated shapes, producing\ntemporally coherent animations that eliminate the jittering issues common in\nexisting methods.\n", "link": "http://arxiv.org/abs/2508.10898v1", "date": "2025-08-14", "relevancy": 3.0615, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.661}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6123}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Puppeteer%3A%20Rig%20and%20Animate%20Your%203D%20Models&body=Title%3A%20Puppeteer%3A%20Rig%20and%20Animate%20Your%203D%20Models%0AAuthor%3A%20Chaoyue%20Song%20and%20Xiu%20Li%20and%20Fan%20Yang%20and%20Zhongcong%20Xu%20and%20Jiacheng%20Wei%20and%20Fayao%20Liu%20and%20Jiashi%20Feng%20and%20Guosheng%20Lin%20and%20Jianfeng%20Zhang%0AAbstract%3A%20%20%20Modern%20interactive%20applications%20increasingly%20demand%20dynamic%203D%20content%2C%20yet%0Athe%20transformation%20of%20static%203D%20models%20into%20animated%20assets%20constitutes%20a%0Asignificant%20bottleneck%20in%20content%20creation%20pipelines.%20While%20recent%20advances%20in%0Agenerative%20AI%20have%20revolutionized%20static%203D%20model%20creation%2C%20rigging%20and%0Aanimation%20continue%20to%20depend%20heavily%20on%20expert%20intervention.%20We%20present%0APuppeteer%2C%20a%20comprehensive%20framework%20that%20addresses%20both%20automatic%20rigging%20and%0Aanimation%20for%20diverse%203D%20objects.%20Our%20system%20first%20predicts%20plausible%20skeletal%0Astructures%20via%20an%20auto-regressive%20transformer%20that%20introduces%20a%20joint-based%0Atokenization%20strategy%20for%20compact%20representation%20and%20a%20hierarchical%20ordering%0Amethodology%20with%20stochastic%20perturbation%20that%20enhances%20bidirectional%20learning%0Acapabilities.%20It%20then%20infers%20skinning%20weights%20via%20an%20attention-based%0Aarchitecture%20incorporating%20topology-aware%20joint%20attention%20that%20explicitly%0Aencodes%20inter-joint%20relationships%20based%20on%20skeletal%20graph%20distances.%20Finally%2C%0Awe%20complement%20these%20rigging%20advances%20with%20a%20differentiable%20optimization-based%0Aanimation%20pipeline%20that%20generates%20stable%2C%20high-fidelity%20animations%20while%20being%0Acomputationally%20more%20efficient%20than%20existing%20approaches.%20Extensive%20evaluations%0Aacross%20multiple%20benchmarks%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20state-of-the-art%20techniques%20in%20both%20skeletal%20prediction%20accuracy%0Aand%20skinning%20quality.%20The%20system%20robustly%20processes%20diverse%203D%20content%2C%20ranging%0Afrom%20professionally%20designed%20game%20assets%20to%20AI-generated%20shapes%2C%20producing%0Atemporally%20coherent%20animations%20that%20eliminate%20the%20jittering%20issues%20common%20in%0Aexisting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10898v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPuppeteer%253A%2520Rig%2520and%2520Animate%2520Your%25203D%2520Models%26entry.906535625%3DChaoyue%2520Song%2520and%2520Xiu%2520Li%2520and%2520Fan%2520Yang%2520and%2520Zhongcong%2520Xu%2520and%2520Jiacheng%2520Wei%2520and%2520Fayao%2520Liu%2520and%2520Jiashi%2520Feng%2520and%2520Guosheng%2520Lin%2520and%2520Jianfeng%2520Zhang%26entry.1292438233%3D%2520%2520Modern%2520interactive%2520applications%2520increasingly%2520demand%2520dynamic%25203D%2520content%252C%2520yet%250Athe%2520transformation%2520of%2520static%25203D%2520models%2520into%2520animated%2520assets%2520constitutes%2520a%250Asignificant%2520bottleneck%2520in%2520content%2520creation%2520pipelines.%2520While%2520recent%2520advances%2520in%250Agenerative%2520AI%2520have%2520revolutionized%2520static%25203D%2520model%2520creation%252C%2520rigging%2520and%250Aanimation%2520continue%2520to%2520depend%2520heavily%2520on%2520expert%2520intervention.%2520We%2520present%250APuppeteer%252C%2520a%2520comprehensive%2520framework%2520that%2520addresses%2520both%2520automatic%2520rigging%2520and%250Aanimation%2520for%2520diverse%25203D%2520objects.%2520Our%2520system%2520first%2520predicts%2520plausible%2520skeletal%250Astructures%2520via%2520an%2520auto-regressive%2520transformer%2520that%2520introduces%2520a%2520joint-based%250Atokenization%2520strategy%2520for%2520compact%2520representation%2520and%2520a%2520hierarchical%2520ordering%250Amethodology%2520with%2520stochastic%2520perturbation%2520that%2520enhances%2520bidirectional%2520learning%250Acapabilities.%2520It%2520then%2520infers%2520skinning%2520weights%2520via%2520an%2520attention-based%250Aarchitecture%2520incorporating%2520topology-aware%2520joint%2520attention%2520that%2520explicitly%250Aencodes%2520inter-joint%2520relationships%2520based%2520on%2520skeletal%2520graph%2520distances.%2520Finally%252C%250Awe%2520complement%2520these%2520rigging%2520advances%2520with%2520a%2520differentiable%2520optimization-based%250Aanimation%2520pipeline%2520that%2520generates%2520stable%252C%2520high-fidelity%2520animations%2520while%2520being%250Acomputationally%2520more%2520efficient%2520than%2520existing%2520approaches.%2520Extensive%2520evaluations%250Aacross%2520multiple%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520significantly%250Aoutperforms%2520state-of-the-art%2520techniques%2520in%2520both%2520skeletal%2520prediction%2520accuracy%250Aand%2520skinning%2520quality.%2520The%2520system%2520robustly%2520processes%2520diverse%25203D%2520content%252C%2520ranging%250Afrom%2520professionally%2520designed%2520game%2520assets%2520to%2520AI-generated%2520shapes%252C%2520producing%250Atemporally%2520coherent%2520animations%2520that%2520eliminate%2520the%2520jittering%2520issues%2520common%2520in%250Aexisting%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10898v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Puppeteer%3A%20Rig%20and%20Animate%20Your%203D%20Models&entry.906535625=Chaoyue%20Song%20and%20Xiu%20Li%20and%20Fan%20Yang%20and%20Zhongcong%20Xu%20and%20Jiacheng%20Wei%20and%20Fayao%20Liu%20and%20Jiashi%20Feng%20and%20Guosheng%20Lin%20and%20Jianfeng%20Zhang&entry.1292438233=%20%20Modern%20interactive%20applications%20increasingly%20demand%20dynamic%203D%20content%2C%20yet%0Athe%20transformation%20of%20static%203D%20models%20into%20animated%20assets%20constitutes%20a%0Asignificant%20bottleneck%20in%20content%20creation%20pipelines.%20While%20recent%20advances%20in%0Agenerative%20AI%20have%20revolutionized%20static%203D%20model%20creation%2C%20rigging%20and%0Aanimation%20continue%20to%20depend%20heavily%20on%20expert%20intervention.%20We%20present%0APuppeteer%2C%20a%20comprehensive%20framework%20that%20addresses%20both%20automatic%20rigging%20and%0Aanimation%20for%20diverse%203D%20objects.%20Our%20system%20first%20predicts%20plausible%20skeletal%0Astructures%20via%20an%20auto-regressive%20transformer%20that%20introduces%20a%20joint-based%0Atokenization%20strategy%20for%20compact%20representation%20and%20a%20hierarchical%20ordering%0Amethodology%20with%20stochastic%20perturbation%20that%20enhances%20bidirectional%20learning%0Acapabilities.%20It%20then%20infers%20skinning%20weights%20via%20an%20attention-based%0Aarchitecture%20incorporating%20topology-aware%20joint%20attention%20that%20explicitly%0Aencodes%20inter-joint%20relationships%20based%20on%20skeletal%20graph%20distances.%20Finally%2C%0Awe%20complement%20these%20rigging%20advances%20with%20a%20differentiable%20optimization-based%0Aanimation%20pipeline%20that%20generates%20stable%2C%20high-fidelity%20animations%20while%20being%0Acomputationally%20more%20efficient%20than%20existing%20approaches.%20Extensive%20evaluations%0Aacross%20multiple%20benchmarks%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20state-of-the-art%20techniques%20in%20both%20skeletal%20prediction%20accuracy%0Aand%20skinning%20quality.%20The%20system%20robustly%20processes%20diverse%203D%20content%2C%20ranging%0Afrom%20professionally%20designed%20game%20assets%20to%20AI-generated%20shapes%2C%20producing%0Atemporally%20coherent%20animations%20that%20eliminate%20the%20jittering%20issues%20common%20in%0Aexisting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10898v1&entry.124074799=Read"},
{"title": "Serial Over Parallel: Learning Continual Unification for Multi-Modal\n  Visual Object Tracking and Benchmarking", "author": "Zhangyong Tang and Tianyang Xu and Xuefeng Zhu and Chunyang Cheng and Tao Zhou and Xiaojun Wu and Josef Kittler", "abstract": "  Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws\nincreasing attention due to the complementary nature of different modalities in\nbuilding robust tracking systems. Existing practices mix all data sensor types\nin a single training procedure, structuring a parallel paradigm from the\ndata-centric perspective and aiming for a global optimum on the joint\ndistribution of the involved tasks. However, the absence of a unified benchmark\nwhere all types of data coexist forces evaluations on separated benchmarks,\ncausing \\textit{inconsistency} between training and testing, thus leading to\nperformance \\textit{degradation}. To address these issues, this work advances\nin two aspects: \\ding{182} A unified benchmark, coined as UniBench300, is\nintroduced to bridge the inconsistency by incorporating multiple task data,\nreducing inference passes from three to one and cutting time consumption by\n27\\%. \\ding{183} The unification process is reformulated in a serial format,\nprogressively integrating new tasks. In this way, the performance degradation\ncan be specified as knowledge forgetting of previous tasks, which naturally\naligns with the philosophy of continual learning (CL), motivating further\nexploration of injecting CL into the unification process. Extensive experiments\nconducted on two baselines and four benchmarks demonstrate the significance of\nUniBench300 and the superiority of CL in supporting a stable unification\nprocess. Moreover, while conducting dedicated analyses, the performance\ndegradation is found to be negatively correlated with network capacity.\nAdditionally, modality discrepancies contribute to varying degradation levels\nacross tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for\nfuture multi-modal vision research. Source codes and the proposed benchmark is\navailable at \\textit{https://github.com/Zhangyong-Tang/UniBench300}.\n", "link": "http://arxiv.org/abs/2508.10655v1", "date": "2025-08-14", "relevancy": 3.0161, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6151}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Serial%20Over%20Parallel%3A%20Learning%20Continual%20Unification%20for%20Multi-Modal%0A%20%20Visual%20Object%20Tracking%20and%20Benchmarking&body=Title%3A%20Serial%20Over%20Parallel%3A%20Learning%20Continual%20Unification%20for%20Multi-Modal%0A%20%20Visual%20Object%20Tracking%20and%20Benchmarking%0AAuthor%3A%20Zhangyong%20Tang%20and%20Tianyang%20Xu%20and%20Xuefeng%20Zhu%20and%20Chunyang%20Cheng%20and%20Tao%20Zhou%20and%20Xiaojun%20Wu%20and%20Josef%20Kittler%0AAbstract%3A%20%20%20Unifying%20multiple%20multi-modal%20visual%20object%20tracking%20%28MMVOT%29%20tasks%20draws%0Aincreasing%20attention%20due%20to%20the%20complementary%20nature%20of%20different%20modalities%20in%0Abuilding%20robust%20tracking%20systems.%20Existing%20practices%20mix%20all%20data%20sensor%20types%0Ain%20a%20single%20training%20procedure%2C%20structuring%20a%20parallel%20paradigm%20from%20the%0Adata-centric%20perspective%20and%20aiming%20for%20a%20global%20optimum%20on%20the%20joint%0Adistribution%20of%20the%20involved%20tasks.%20However%2C%20the%20absence%20of%20a%20unified%20benchmark%0Awhere%20all%20types%20of%20data%20coexist%20forces%20evaluations%20on%20separated%20benchmarks%2C%0Acausing%20%5Ctextit%7Binconsistency%7D%20between%20training%20and%20testing%2C%20thus%20leading%20to%0Aperformance%20%5Ctextit%7Bdegradation%7D.%20To%20address%20these%20issues%2C%20this%20work%20advances%0Ain%20two%20aspects%3A%20%5Cding%7B182%7D%20A%20unified%20benchmark%2C%20coined%20as%20UniBench300%2C%20is%0Aintroduced%20to%20bridge%20the%20inconsistency%20by%20incorporating%20multiple%20task%20data%2C%0Areducing%20inference%20passes%20from%20three%20to%20one%20and%20cutting%20time%20consumption%20by%0A27%5C%25.%20%5Cding%7B183%7D%20The%20unification%20process%20is%20reformulated%20in%20a%20serial%20format%2C%0Aprogressively%20integrating%20new%20tasks.%20In%20this%20way%2C%20the%20performance%20degradation%0Acan%20be%20specified%20as%20knowledge%20forgetting%20of%20previous%20tasks%2C%20which%20naturally%0Aaligns%20with%20the%20philosophy%20of%20continual%20learning%20%28CL%29%2C%20motivating%20further%0Aexploration%20of%20injecting%20CL%20into%20the%20unification%20process.%20Extensive%20experiments%0Aconducted%20on%20two%20baselines%20and%20four%20benchmarks%20demonstrate%20the%20significance%20of%0AUniBench300%20and%20the%20superiority%20of%20CL%20in%20supporting%20a%20stable%20unification%0Aprocess.%20Moreover%2C%20while%20conducting%20dedicated%20analyses%2C%20the%20performance%0Adegradation%20is%20found%20to%20be%20negatively%20correlated%20with%20network%20capacity.%0AAdditionally%2C%20modality%20discrepancies%20contribute%20to%20varying%20degradation%20levels%0Aacross%20tasks%20%28RGBT%20%3E%20RGBD%20%3E%20RGBE%20in%20MMVOT%29%2C%20offering%20valuable%20insights%20for%0Afuture%20multi-modal%20vision%20research.%20Source%20codes%20and%20the%20proposed%20benchmark%20is%0Aavailable%20at%20%5Ctextit%7Bhttps%3A//github.com/Zhangyong-Tang/UniBench300%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSerial%2520Over%2520Parallel%253A%2520Learning%2520Continual%2520Unification%2520for%2520Multi-Modal%250A%2520%2520Visual%2520Object%2520Tracking%2520and%2520Benchmarking%26entry.906535625%3DZhangyong%2520Tang%2520and%2520Tianyang%2520Xu%2520and%2520Xuefeng%2520Zhu%2520and%2520Chunyang%2520Cheng%2520and%2520Tao%2520Zhou%2520and%2520Xiaojun%2520Wu%2520and%2520Josef%2520Kittler%26entry.1292438233%3D%2520%2520Unifying%2520multiple%2520multi-modal%2520visual%2520object%2520tracking%2520%2528MMVOT%2529%2520tasks%2520draws%250Aincreasing%2520attention%2520due%2520to%2520the%2520complementary%2520nature%2520of%2520different%2520modalities%2520in%250Abuilding%2520robust%2520tracking%2520systems.%2520Existing%2520practices%2520mix%2520all%2520data%2520sensor%2520types%250Ain%2520a%2520single%2520training%2520procedure%252C%2520structuring%2520a%2520parallel%2520paradigm%2520from%2520the%250Adata-centric%2520perspective%2520and%2520aiming%2520for%2520a%2520global%2520optimum%2520on%2520the%2520joint%250Adistribution%2520of%2520the%2520involved%2520tasks.%2520However%252C%2520the%2520absence%2520of%2520a%2520unified%2520benchmark%250Awhere%2520all%2520types%2520of%2520data%2520coexist%2520forces%2520evaluations%2520on%2520separated%2520benchmarks%252C%250Acausing%2520%255Ctextit%257Binconsistency%257D%2520between%2520training%2520and%2520testing%252C%2520thus%2520leading%2520to%250Aperformance%2520%255Ctextit%257Bdegradation%257D.%2520To%2520address%2520these%2520issues%252C%2520this%2520work%2520advances%250Ain%2520two%2520aspects%253A%2520%255Cding%257B182%257D%2520A%2520unified%2520benchmark%252C%2520coined%2520as%2520UniBench300%252C%2520is%250Aintroduced%2520to%2520bridge%2520the%2520inconsistency%2520by%2520incorporating%2520multiple%2520task%2520data%252C%250Areducing%2520inference%2520passes%2520from%2520three%2520to%2520one%2520and%2520cutting%2520time%2520consumption%2520by%250A27%255C%2525.%2520%255Cding%257B183%257D%2520The%2520unification%2520process%2520is%2520reformulated%2520in%2520a%2520serial%2520format%252C%250Aprogressively%2520integrating%2520new%2520tasks.%2520In%2520this%2520way%252C%2520the%2520performance%2520degradation%250Acan%2520be%2520specified%2520as%2520knowledge%2520forgetting%2520of%2520previous%2520tasks%252C%2520which%2520naturally%250Aaligns%2520with%2520the%2520philosophy%2520of%2520continual%2520learning%2520%2528CL%2529%252C%2520motivating%2520further%250Aexploration%2520of%2520injecting%2520CL%2520into%2520the%2520unification%2520process.%2520Extensive%2520experiments%250Aconducted%2520on%2520two%2520baselines%2520and%2520four%2520benchmarks%2520demonstrate%2520the%2520significance%2520of%250AUniBench300%2520and%2520the%2520superiority%2520of%2520CL%2520in%2520supporting%2520a%2520stable%2520unification%250Aprocess.%2520Moreover%252C%2520while%2520conducting%2520dedicated%2520analyses%252C%2520the%2520performance%250Adegradation%2520is%2520found%2520to%2520be%2520negatively%2520correlated%2520with%2520network%2520capacity.%250AAdditionally%252C%2520modality%2520discrepancies%2520contribute%2520to%2520varying%2520degradation%2520levels%250Aacross%2520tasks%2520%2528RGBT%2520%253E%2520RGBD%2520%253E%2520RGBE%2520in%2520MMVOT%2529%252C%2520offering%2520valuable%2520insights%2520for%250Afuture%2520multi-modal%2520vision%2520research.%2520Source%2520codes%2520and%2520the%2520proposed%2520benchmark%2520is%250Aavailable%2520at%2520%255Ctextit%257Bhttps%253A//github.com/Zhangyong-Tang/UniBench300%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Serial%20Over%20Parallel%3A%20Learning%20Continual%20Unification%20for%20Multi-Modal%0A%20%20Visual%20Object%20Tracking%20and%20Benchmarking&entry.906535625=Zhangyong%20Tang%20and%20Tianyang%20Xu%20and%20Xuefeng%20Zhu%20and%20Chunyang%20Cheng%20and%20Tao%20Zhou%20and%20Xiaojun%20Wu%20and%20Josef%20Kittler&entry.1292438233=%20%20Unifying%20multiple%20multi-modal%20visual%20object%20tracking%20%28MMVOT%29%20tasks%20draws%0Aincreasing%20attention%20due%20to%20the%20complementary%20nature%20of%20different%20modalities%20in%0Abuilding%20robust%20tracking%20systems.%20Existing%20practices%20mix%20all%20data%20sensor%20types%0Ain%20a%20single%20training%20procedure%2C%20structuring%20a%20parallel%20paradigm%20from%20the%0Adata-centric%20perspective%20and%20aiming%20for%20a%20global%20optimum%20on%20the%20joint%0Adistribution%20of%20the%20involved%20tasks.%20However%2C%20the%20absence%20of%20a%20unified%20benchmark%0Awhere%20all%20types%20of%20data%20coexist%20forces%20evaluations%20on%20separated%20benchmarks%2C%0Acausing%20%5Ctextit%7Binconsistency%7D%20between%20training%20and%20testing%2C%20thus%20leading%20to%0Aperformance%20%5Ctextit%7Bdegradation%7D.%20To%20address%20these%20issues%2C%20this%20work%20advances%0Ain%20two%20aspects%3A%20%5Cding%7B182%7D%20A%20unified%20benchmark%2C%20coined%20as%20UniBench300%2C%20is%0Aintroduced%20to%20bridge%20the%20inconsistency%20by%20incorporating%20multiple%20task%20data%2C%0Areducing%20inference%20passes%20from%20three%20to%20one%20and%20cutting%20time%20consumption%20by%0A27%5C%25.%20%5Cding%7B183%7D%20The%20unification%20process%20is%20reformulated%20in%20a%20serial%20format%2C%0Aprogressively%20integrating%20new%20tasks.%20In%20this%20way%2C%20the%20performance%20degradation%0Acan%20be%20specified%20as%20knowledge%20forgetting%20of%20previous%20tasks%2C%20which%20naturally%0Aaligns%20with%20the%20philosophy%20of%20continual%20learning%20%28CL%29%2C%20motivating%20further%0Aexploration%20of%20injecting%20CL%20into%20the%20unification%20process.%20Extensive%20experiments%0Aconducted%20on%20two%20baselines%20and%20four%20benchmarks%20demonstrate%20the%20significance%20of%0AUniBench300%20and%20the%20superiority%20of%20CL%20in%20supporting%20a%20stable%20unification%0Aprocess.%20Moreover%2C%20while%20conducting%20dedicated%20analyses%2C%20the%20performance%0Adegradation%20is%20found%20to%20be%20negatively%20correlated%20with%20network%20capacity.%0AAdditionally%2C%20modality%20discrepancies%20contribute%20to%20varying%20degradation%20levels%0Aacross%20tasks%20%28RGBT%20%3E%20RGBD%20%3E%20RGBE%20in%20MMVOT%29%2C%20offering%20valuable%20insights%20for%0Afuture%20multi-modal%20vision%20research.%20Source%20codes%20and%20the%20proposed%20benchmark%20is%0Aavailable%20at%20%5Ctextit%7Bhttps%3A//github.com/Zhangyong-Tang/UniBench300%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10655v1&entry.124074799=Read"},
{"title": "GLM-4.1V-Thinking and GLM-4.5V: Towards Versatile Multimodal Reasoning\n  with Scalable Reinforcement Learning", "author": "GLM-V Team and  : and Wenyi Hong and Wenmeng Yu and Xiaotao Gu and Guo Wang and Guobing Gan and Haomiao Tang and Jiale Cheng and Ji Qi and Junhui Ji and Lihang Pan and Shuaiqi Duan and Weihan Wang and Yan Wang and Yean Cheng and Zehai He and Zhe Su and Zhen Yang and Ziyang Pan and Aohan Zeng and Baoxu Wang and Bin Chen and Boyan Shi and Changyu Pang and Chenhui Zhang and Da Yin and Fan Yang and Guoqing Chen and Jiazheng Xu and Jiale Zhu and Jiali Chen and Jing Chen and Jinhao Chen and Jinghao Lin and Jinjiang Wang and Junjie Chen and Leqi Lei and Letian Gong and Leyi Pan and Mingdao Liu and Mingde Xu and Mingzhi Zhang and Qinkai Zheng and Sheng Yang and Shi Zhong and Shiyu Huang and Shuyuan Zhao and Siyan Xue and Shangqin Tu and Shengbiao Meng and Tianshu Zhang and Tianwei Luo and Tianxiang Hao and Tianyu Tong and Wenkai Li and Wei Jia and Xiao Liu and Xiaohan Zhang and Xin Lyu and Xinyue Fan and Xuancheng Huang and Yanling Wang and Yadong Xue and Yanfeng Wang and Yanzi Wang and Yifan An and Yifan Du and Yiming Shi and Yiheng Huang and Yilin Niu and Yuan Wang and Yuanchang Yue and Yuchen Li and Yutao Zhang and Yuting Wang and Yu Wang and Yuxuan Zhang and Zhao Xue and Zhenyu Hou and Zhengxiao Du and Zihan Wang and Peng Zhang and Debing Liu and Bin Xu and Juanzi Li and Minlie Huang and Yuxiao Dong and Jie Tang", "abstract": "  We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models\n(VLMs) designed to advance general-purpose multimodal understanding and\nreasoning. In this report, we share our key findings in the development of the\nreasoning-centric training framework. We first develop a capable vision\nfoundation model with significant potential through large-scale pre-training,\nwhich arguably sets the upper bound for the final performance. We then propose\nReinforcement Learning with Curriculum Sampling (RLCS) to unlock the full\npotential of the model, leading to comprehensive capability enhancement across\na diverse range of tasks, including STEM problem solving, video understanding,\ncontent recognition, coding, grounding, GUI-based agents, and long document\ninterpretation. In a comprehensive evaluation across 42 public benchmarks,\nGLM-4.5V achieves state-of-the-art performance on nearly all tasks among\nopen-source models of similar size, and demonstrates competitive or even\nsuperior results compared to closed-source models such as Gemini-2.5-Flash on\nchallenging tasks including Coding and GUI Agents. Meanwhile, the smaller\nGLM-4.1V-9B-Thinking remains highly competitive-achieving superior results to\nthe much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source both\nGLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information are\nreleased at https://github.com/zai-org/GLM-V.\n", "link": "http://arxiv.org/abs/2507.01006v4", "date": "2025-08-14", "relevancy": 2.9605, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6071}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLM-4.1V-Thinking%20and%20GLM-4.5V%3A%20Towards%20Versatile%20Multimodal%20Reasoning%0A%20%20with%20Scalable%20Reinforcement%20Learning&body=Title%3A%20GLM-4.1V-Thinking%20and%20GLM-4.5V%3A%20Towards%20Versatile%20Multimodal%20Reasoning%0A%20%20with%20Scalable%20Reinforcement%20Learning%0AAuthor%3A%20GLM-V%20Team%20and%20%20%3A%20and%20Wenyi%20Hong%20and%20Wenmeng%20Yu%20and%20Xiaotao%20Gu%20and%20Guo%20Wang%20and%20Guobing%20Gan%20and%20Haomiao%20Tang%20and%20Jiale%20Cheng%20and%20Ji%20Qi%20and%20Junhui%20Ji%20and%20Lihang%20Pan%20and%20Shuaiqi%20Duan%20and%20Weihan%20Wang%20and%20Yan%20Wang%20and%20Yean%20Cheng%20and%20Zehai%20He%20and%20Zhe%20Su%20and%20Zhen%20Yang%20and%20Ziyang%20Pan%20and%20Aohan%20Zeng%20and%20Baoxu%20Wang%20and%20Bin%20Chen%20and%20Boyan%20Shi%20and%20Changyu%20Pang%20and%20Chenhui%20Zhang%20and%20Da%20Yin%20and%20Fan%20Yang%20and%20Guoqing%20Chen%20and%20Jiazheng%20Xu%20and%20Jiale%20Zhu%20and%20Jiali%20Chen%20and%20Jing%20Chen%20and%20Jinhao%20Chen%20and%20Jinghao%20Lin%20and%20Jinjiang%20Wang%20and%20Junjie%20Chen%20and%20Leqi%20Lei%20and%20Letian%20Gong%20and%20Leyi%20Pan%20and%20Mingdao%20Liu%20and%20Mingde%20Xu%20and%20Mingzhi%20Zhang%20and%20Qinkai%20Zheng%20and%20Sheng%20Yang%20and%20Shi%20Zhong%20and%20Shiyu%20Huang%20and%20Shuyuan%20Zhao%20and%20Siyan%20Xue%20and%20Shangqin%20Tu%20and%20Shengbiao%20Meng%20and%20Tianshu%20Zhang%20and%20Tianwei%20Luo%20and%20Tianxiang%20Hao%20and%20Tianyu%20Tong%20and%20Wenkai%20Li%20and%20Wei%20Jia%20and%20Xiao%20Liu%20and%20Xiaohan%20Zhang%20and%20Xin%20Lyu%20and%20Xinyue%20Fan%20and%20Xuancheng%20Huang%20and%20Yanling%20Wang%20and%20Yadong%20Xue%20and%20Yanfeng%20Wang%20and%20Yanzi%20Wang%20and%20Yifan%20An%20and%20Yifan%20Du%20and%20Yiming%20Shi%20and%20Yiheng%20Huang%20and%20Yilin%20Niu%20and%20Yuan%20Wang%20and%20Yuanchang%20Yue%20and%20Yuchen%20Li%20and%20Yutao%20Zhang%20and%20Yuting%20Wang%20and%20Yu%20Wang%20and%20Yuxuan%20Zhang%20and%20Zhao%20Xue%20and%20Zhenyu%20Hou%20and%20Zhengxiao%20Du%20and%20Zihan%20Wang%20and%20Peng%20Zhang%20and%20Debing%20Liu%20and%20Bin%20Xu%20and%20Juanzi%20Li%20and%20Minlie%20Huang%20and%20Yuxiao%20Dong%20and%20Jie%20Tang%0AAbstract%3A%20%20%20We%20present%20GLM-4.1V-Thinking%20and%20GLM-4.5V%2C%20a%20family%20of%20vision-language%20models%0A%28VLMs%29%20designed%20to%20advance%20general-purpose%20multimodal%20understanding%20and%0Areasoning.%20In%20this%20report%2C%20we%20share%20our%20key%20findings%20in%20the%20development%20of%20the%0Areasoning-centric%20training%20framework.%20We%20first%20develop%20a%20capable%20vision%0Afoundation%20model%20with%20significant%20potential%20through%20large-scale%20pre-training%2C%0Awhich%20arguably%20sets%20the%20upper%20bound%20for%20the%20final%20performance.%20We%20then%20propose%0AReinforcement%20Learning%20with%20Curriculum%20Sampling%20%28RLCS%29%20to%20unlock%20the%20full%0Apotential%20of%20the%20model%2C%20leading%20to%20comprehensive%20capability%20enhancement%20across%0Aa%20diverse%20range%20of%20tasks%2C%20including%20STEM%20problem%20solving%2C%20video%20understanding%2C%0Acontent%20recognition%2C%20coding%2C%20grounding%2C%20GUI-based%20agents%2C%20and%20long%20document%0Ainterpretation.%20In%20a%20comprehensive%20evaluation%20across%2042%20public%20benchmarks%2C%0AGLM-4.5V%20achieves%20state-of-the-art%20performance%20on%20nearly%20all%20tasks%20among%0Aopen-source%20models%20of%20similar%20size%2C%20and%20demonstrates%20competitive%20or%20even%0Asuperior%20results%20compared%20to%20closed-source%20models%20such%20as%20Gemini-2.5-Flash%20on%0Achallenging%20tasks%20including%20Coding%20and%20GUI%20Agents.%20Meanwhile%2C%20the%20smaller%0AGLM-4.1V-9B-Thinking%20remains%20highly%20competitive-achieving%20superior%20results%20to%0Athe%20much%20larger%20Qwen2.5-VL-72B%20on%2029%20benchmarks.%20We%20open-source%20both%0AGLM-4.1V-9B-Thinking%20and%20GLM-4.5V.%20Code%2C%20models%20and%20more%20information%20are%0Areleased%20at%20https%3A//github.com/zai-org/GLM-V.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01006v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLM-4.1V-Thinking%2520and%2520GLM-4.5V%253A%2520Towards%2520Versatile%2520Multimodal%2520Reasoning%250A%2520%2520with%2520Scalable%2520Reinforcement%2520Learning%26entry.906535625%3DGLM-V%2520Team%2520and%2520%2520%253A%2520and%2520Wenyi%2520Hong%2520and%2520Wenmeng%2520Yu%2520and%2520Xiaotao%2520Gu%2520and%2520Guo%2520Wang%2520and%2520Guobing%2520Gan%2520and%2520Haomiao%2520Tang%2520and%2520Jiale%2520Cheng%2520and%2520Ji%2520Qi%2520and%2520Junhui%2520Ji%2520and%2520Lihang%2520Pan%2520and%2520Shuaiqi%2520Duan%2520and%2520Weihan%2520Wang%2520and%2520Yan%2520Wang%2520and%2520Yean%2520Cheng%2520and%2520Zehai%2520He%2520and%2520Zhe%2520Su%2520and%2520Zhen%2520Yang%2520and%2520Ziyang%2520Pan%2520and%2520Aohan%2520Zeng%2520and%2520Baoxu%2520Wang%2520and%2520Bin%2520Chen%2520and%2520Boyan%2520Shi%2520and%2520Changyu%2520Pang%2520and%2520Chenhui%2520Zhang%2520and%2520Da%2520Yin%2520and%2520Fan%2520Yang%2520and%2520Guoqing%2520Chen%2520and%2520Jiazheng%2520Xu%2520and%2520Jiale%2520Zhu%2520and%2520Jiali%2520Chen%2520and%2520Jing%2520Chen%2520and%2520Jinhao%2520Chen%2520and%2520Jinghao%2520Lin%2520and%2520Jinjiang%2520Wang%2520and%2520Junjie%2520Chen%2520and%2520Leqi%2520Lei%2520and%2520Letian%2520Gong%2520and%2520Leyi%2520Pan%2520and%2520Mingdao%2520Liu%2520and%2520Mingde%2520Xu%2520and%2520Mingzhi%2520Zhang%2520and%2520Qinkai%2520Zheng%2520and%2520Sheng%2520Yang%2520and%2520Shi%2520Zhong%2520and%2520Shiyu%2520Huang%2520and%2520Shuyuan%2520Zhao%2520and%2520Siyan%2520Xue%2520and%2520Shangqin%2520Tu%2520and%2520Shengbiao%2520Meng%2520and%2520Tianshu%2520Zhang%2520and%2520Tianwei%2520Luo%2520and%2520Tianxiang%2520Hao%2520and%2520Tianyu%2520Tong%2520and%2520Wenkai%2520Li%2520and%2520Wei%2520Jia%2520and%2520Xiao%2520Liu%2520and%2520Xiaohan%2520Zhang%2520and%2520Xin%2520Lyu%2520and%2520Xinyue%2520Fan%2520and%2520Xuancheng%2520Huang%2520and%2520Yanling%2520Wang%2520and%2520Yadong%2520Xue%2520and%2520Yanfeng%2520Wang%2520and%2520Yanzi%2520Wang%2520and%2520Yifan%2520An%2520and%2520Yifan%2520Du%2520and%2520Yiming%2520Shi%2520and%2520Yiheng%2520Huang%2520and%2520Yilin%2520Niu%2520and%2520Yuan%2520Wang%2520and%2520Yuanchang%2520Yue%2520and%2520Yuchen%2520Li%2520and%2520Yutao%2520Zhang%2520and%2520Yuting%2520Wang%2520and%2520Yu%2520Wang%2520and%2520Yuxuan%2520Zhang%2520and%2520Zhao%2520Xue%2520and%2520Zhenyu%2520Hou%2520and%2520Zhengxiao%2520Du%2520and%2520Zihan%2520Wang%2520and%2520Peng%2520Zhang%2520and%2520Debing%2520Liu%2520and%2520Bin%2520Xu%2520and%2520Juanzi%2520Li%2520and%2520Minlie%2520Huang%2520and%2520Yuxiao%2520Dong%2520and%2520Jie%2520Tang%26entry.1292438233%3D%2520%2520We%2520present%2520GLM-4.1V-Thinking%2520and%2520GLM-4.5V%252C%2520a%2520family%2520of%2520vision-language%2520models%250A%2528VLMs%2529%2520designed%2520to%2520advance%2520general-purpose%2520multimodal%2520understanding%2520and%250Areasoning.%2520In%2520this%2520report%252C%2520we%2520share%2520our%2520key%2520findings%2520in%2520the%2520development%2520of%2520the%250Areasoning-centric%2520training%2520framework.%2520We%2520first%2520develop%2520a%2520capable%2520vision%250Afoundation%2520model%2520with%2520significant%2520potential%2520through%2520large-scale%2520pre-training%252C%250Awhich%2520arguably%2520sets%2520the%2520upper%2520bound%2520for%2520the%2520final%2520performance.%2520We%2520then%2520propose%250AReinforcement%2520Learning%2520with%2520Curriculum%2520Sampling%2520%2528RLCS%2529%2520to%2520unlock%2520the%2520full%250Apotential%2520of%2520the%2520model%252C%2520leading%2520to%2520comprehensive%2520capability%2520enhancement%2520across%250Aa%2520diverse%2520range%2520of%2520tasks%252C%2520including%2520STEM%2520problem%2520solving%252C%2520video%2520understanding%252C%250Acontent%2520recognition%252C%2520coding%252C%2520grounding%252C%2520GUI-based%2520agents%252C%2520and%2520long%2520document%250Ainterpretation.%2520In%2520a%2520comprehensive%2520evaluation%2520across%252042%2520public%2520benchmarks%252C%250AGLM-4.5V%2520achieves%2520state-of-the-art%2520performance%2520on%2520nearly%2520all%2520tasks%2520among%250Aopen-source%2520models%2520of%2520similar%2520size%252C%2520and%2520demonstrates%2520competitive%2520or%2520even%250Asuperior%2520results%2520compared%2520to%2520closed-source%2520models%2520such%2520as%2520Gemini-2.5-Flash%2520on%250Achallenging%2520tasks%2520including%2520Coding%2520and%2520GUI%2520Agents.%2520Meanwhile%252C%2520the%2520smaller%250AGLM-4.1V-9B-Thinking%2520remains%2520highly%2520competitive-achieving%2520superior%2520results%2520to%250Athe%2520much%2520larger%2520Qwen2.5-VL-72B%2520on%252029%2520benchmarks.%2520We%2520open-source%2520both%250AGLM-4.1V-9B-Thinking%2520and%2520GLM-4.5V.%2520Code%252C%2520models%2520and%2520more%2520information%2520are%250Areleased%2520at%2520https%253A//github.com/zai-org/GLM-V.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01006v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLM-4.1V-Thinking%20and%20GLM-4.5V%3A%20Towards%20Versatile%20Multimodal%20Reasoning%0A%20%20with%20Scalable%20Reinforcement%20Learning&entry.906535625=GLM-V%20Team%20and%20%20%3A%20and%20Wenyi%20Hong%20and%20Wenmeng%20Yu%20and%20Xiaotao%20Gu%20and%20Guo%20Wang%20and%20Guobing%20Gan%20and%20Haomiao%20Tang%20and%20Jiale%20Cheng%20and%20Ji%20Qi%20and%20Junhui%20Ji%20and%20Lihang%20Pan%20and%20Shuaiqi%20Duan%20and%20Weihan%20Wang%20and%20Yan%20Wang%20and%20Yean%20Cheng%20and%20Zehai%20He%20and%20Zhe%20Su%20and%20Zhen%20Yang%20and%20Ziyang%20Pan%20and%20Aohan%20Zeng%20and%20Baoxu%20Wang%20and%20Bin%20Chen%20and%20Boyan%20Shi%20and%20Changyu%20Pang%20and%20Chenhui%20Zhang%20and%20Da%20Yin%20and%20Fan%20Yang%20and%20Guoqing%20Chen%20and%20Jiazheng%20Xu%20and%20Jiale%20Zhu%20and%20Jiali%20Chen%20and%20Jing%20Chen%20and%20Jinhao%20Chen%20and%20Jinghao%20Lin%20and%20Jinjiang%20Wang%20and%20Junjie%20Chen%20and%20Leqi%20Lei%20and%20Letian%20Gong%20and%20Leyi%20Pan%20and%20Mingdao%20Liu%20and%20Mingde%20Xu%20and%20Mingzhi%20Zhang%20and%20Qinkai%20Zheng%20and%20Sheng%20Yang%20and%20Shi%20Zhong%20and%20Shiyu%20Huang%20and%20Shuyuan%20Zhao%20and%20Siyan%20Xue%20and%20Shangqin%20Tu%20and%20Shengbiao%20Meng%20and%20Tianshu%20Zhang%20and%20Tianwei%20Luo%20and%20Tianxiang%20Hao%20and%20Tianyu%20Tong%20and%20Wenkai%20Li%20and%20Wei%20Jia%20and%20Xiao%20Liu%20and%20Xiaohan%20Zhang%20and%20Xin%20Lyu%20and%20Xinyue%20Fan%20and%20Xuancheng%20Huang%20and%20Yanling%20Wang%20and%20Yadong%20Xue%20and%20Yanfeng%20Wang%20and%20Yanzi%20Wang%20and%20Yifan%20An%20and%20Yifan%20Du%20and%20Yiming%20Shi%20and%20Yiheng%20Huang%20and%20Yilin%20Niu%20and%20Yuan%20Wang%20and%20Yuanchang%20Yue%20and%20Yuchen%20Li%20and%20Yutao%20Zhang%20and%20Yuting%20Wang%20and%20Yu%20Wang%20and%20Yuxuan%20Zhang%20and%20Zhao%20Xue%20and%20Zhenyu%20Hou%20and%20Zhengxiao%20Du%20and%20Zihan%20Wang%20and%20Peng%20Zhang%20and%20Debing%20Liu%20and%20Bin%20Xu%20and%20Juanzi%20Li%20and%20Minlie%20Huang%20and%20Yuxiao%20Dong%20and%20Jie%20Tang&entry.1292438233=%20%20We%20present%20GLM-4.1V-Thinking%20and%20GLM-4.5V%2C%20a%20family%20of%20vision-language%20models%0A%28VLMs%29%20designed%20to%20advance%20general-purpose%20multimodal%20understanding%20and%0Areasoning.%20In%20this%20report%2C%20we%20share%20our%20key%20findings%20in%20the%20development%20of%20the%0Areasoning-centric%20training%20framework.%20We%20first%20develop%20a%20capable%20vision%0Afoundation%20model%20with%20significant%20potential%20through%20large-scale%20pre-training%2C%0Awhich%20arguably%20sets%20the%20upper%20bound%20for%20the%20final%20performance.%20We%20then%20propose%0AReinforcement%20Learning%20with%20Curriculum%20Sampling%20%28RLCS%29%20to%20unlock%20the%20full%0Apotential%20of%20the%20model%2C%20leading%20to%20comprehensive%20capability%20enhancement%20across%0Aa%20diverse%20range%20of%20tasks%2C%20including%20STEM%20problem%20solving%2C%20video%20understanding%2C%0Acontent%20recognition%2C%20coding%2C%20grounding%2C%20GUI-based%20agents%2C%20and%20long%20document%0Ainterpretation.%20In%20a%20comprehensive%20evaluation%20across%2042%20public%20benchmarks%2C%0AGLM-4.5V%20achieves%20state-of-the-art%20performance%20on%20nearly%20all%20tasks%20among%0Aopen-source%20models%20of%20similar%20size%2C%20and%20demonstrates%20competitive%20or%20even%0Asuperior%20results%20compared%20to%20closed-source%20models%20such%20as%20Gemini-2.5-Flash%20on%0Achallenging%20tasks%20including%20Coding%20and%20GUI%20Agents.%20Meanwhile%2C%20the%20smaller%0AGLM-4.1V-9B-Thinking%20remains%20highly%20competitive-achieving%20superior%20results%20to%0Athe%20much%20larger%20Qwen2.5-VL-72B%20on%2029%20benchmarks.%20We%20open-source%20both%0AGLM-4.1V-9B-Thinking%20and%20GLM-4.5V.%20Code%2C%20models%20and%20more%20information%20are%0Areleased%20at%20https%3A//github.com/zai-org/GLM-V.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01006v4&entry.124074799=Read"},
{"title": "DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose\n  Reconstruction", "author": "Ben Kaye and Tomas Jakab and Shangzhe Wu and Christian Rupprecht and Andrea Vedaldi", "abstract": "  The choice of data representation is a key factor in the success of deep\nlearning in geometric tasks. For instance, DUSt3R recently introduced the\nconcept of viewpoint-invariant point maps, generalizing depth prediction and\nshowing that all key problems in the 3D reconstruction of static scenes can be\nreduced to predicting such point maps. In this paper, we develop an analogous\nconcept for a very different problem: the reconstruction of the 3D shape and\npose of deformable objects. To this end, we introduce Dual Point Maps (DualPM),\nwhere a pair of point maps is extracted from the same image-one associating\npixels to their 3D locations on the object and the other to a canonical version\nof the object in its rest pose. We also extend point maps to amodal\nreconstruction to recover the complete shape of the object, even through\nself-occlusions. We show that 3D reconstruction and 3D pose estimation can be\nreduced to the prediction of DualPMs. Empirically, we demonstrate that this\nrepresentation is a suitable target for deep networks to predict. Specifically,\nwe focus on modeling quadrupeds, showing that DualPMs can be trained purely on\nsynthetic 3D data, consisting of one or two models per category, while\ngeneralizing effectively to real images. With this approach, we achieve\nsignificant improvements over previous methods for the 3D analysis and\nreconstruction of such objects.\n", "link": "http://arxiv.org/abs/2412.04464v5", "date": "2025-08-14", "relevancy": 2.9519, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6018}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5976}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualPM%3A%20Dual%20Posed-Canonical%20Point%20Maps%20for%203D%20Shape%20and%20Pose%0A%20%20Reconstruction&body=Title%3A%20DualPM%3A%20Dual%20Posed-Canonical%20Point%20Maps%20for%203D%20Shape%20and%20Pose%0A%20%20Reconstruction%0AAuthor%3A%20Ben%20Kaye%20and%20Tomas%20Jakab%20and%20Shangzhe%20Wu%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi%0AAbstract%3A%20%20%20The%20choice%20of%20data%20representation%20is%20a%20key%20factor%20in%20the%20success%20of%20deep%0Alearning%20in%20geometric%20tasks.%20For%20instance%2C%20DUSt3R%20recently%20introduced%20the%0Aconcept%20of%20viewpoint-invariant%20point%20maps%2C%20generalizing%20depth%20prediction%20and%0Ashowing%20that%20all%20key%20problems%20in%20the%203D%20reconstruction%20of%20static%20scenes%20can%20be%0Areduced%20to%20predicting%20such%20point%20maps.%20In%20this%20paper%2C%20we%20develop%20an%20analogous%0Aconcept%20for%20a%20very%20different%20problem%3A%20the%20reconstruction%20of%20the%203D%20shape%20and%0Apose%20of%20deformable%20objects.%20To%20this%20end%2C%20we%20introduce%20Dual%20Point%20Maps%20%28DualPM%29%2C%0Awhere%20a%20pair%20of%20point%20maps%20is%20extracted%20from%20the%20same%20image-one%20associating%0Apixels%20to%20their%203D%20locations%20on%20the%20object%20and%20the%20other%20to%20a%20canonical%20version%0Aof%20the%20object%20in%20its%20rest%20pose.%20We%20also%20extend%20point%20maps%20to%20amodal%0Areconstruction%20to%20recover%20the%20complete%20shape%20of%20the%20object%2C%20even%20through%0Aself-occlusions.%20We%20show%20that%203D%20reconstruction%20and%203D%20pose%20estimation%20can%20be%0Areduced%20to%20the%20prediction%20of%20DualPMs.%20Empirically%2C%20we%20demonstrate%20that%20this%0Arepresentation%20is%20a%20suitable%20target%20for%20deep%20networks%20to%20predict.%20Specifically%2C%0Awe%20focus%20on%20modeling%20quadrupeds%2C%20showing%20that%20DualPMs%20can%20be%20trained%20purely%20on%0Asynthetic%203D%20data%2C%20consisting%20of%20one%20or%20two%20models%20per%20category%2C%20while%0Ageneralizing%20effectively%20to%20real%20images.%20With%20this%20approach%2C%20we%20achieve%0Asignificant%20improvements%20over%20previous%20methods%20for%20the%203D%20analysis%20and%0Areconstruction%20of%20such%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04464v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualPM%253A%2520Dual%2520Posed-Canonical%2520Point%2520Maps%2520for%25203D%2520Shape%2520and%2520Pose%250A%2520%2520Reconstruction%26entry.906535625%3DBen%2520Kaye%2520and%2520Tomas%2520Jakab%2520and%2520Shangzhe%2520Wu%2520and%2520Christian%2520Rupprecht%2520and%2520Andrea%2520Vedaldi%26entry.1292438233%3D%2520%2520The%2520choice%2520of%2520data%2520representation%2520is%2520a%2520key%2520factor%2520in%2520the%2520success%2520of%2520deep%250Alearning%2520in%2520geometric%2520tasks.%2520For%2520instance%252C%2520DUSt3R%2520recently%2520introduced%2520the%250Aconcept%2520of%2520viewpoint-invariant%2520point%2520maps%252C%2520generalizing%2520depth%2520prediction%2520and%250Ashowing%2520that%2520all%2520key%2520problems%2520in%2520the%25203D%2520reconstruction%2520of%2520static%2520scenes%2520can%2520be%250Areduced%2520to%2520predicting%2520such%2520point%2520maps.%2520In%2520this%2520paper%252C%2520we%2520develop%2520an%2520analogous%250Aconcept%2520for%2520a%2520very%2520different%2520problem%253A%2520the%2520reconstruction%2520of%2520the%25203D%2520shape%2520and%250Apose%2520of%2520deformable%2520objects.%2520To%2520this%2520end%252C%2520we%2520introduce%2520Dual%2520Point%2520Maps%2520%2528DualPM%2529%252C%250Awhere%2520a%2520pair%2520of%2520point%2520maps%2520is%2520extracted%2520from%2520the%2520same%2520image-one%2520associating%250Apixels%2520to%2520their%25203D%2520locations%2520on%2520the%2520object%2520and%2520the%2520other%2520to%2520a%2520canonical%2520version%250Aof%2520the%2520object%2520in%2520its%2520rest%2520pose.%2520We%2520also%2520extend%2520point%2520maps%2520to%2520amodal%250Areconstruction%2520to%2520recover%2520the%2520complete%2520shape%2520of%2520the%2520object%252C%2520even%2520through%250Aself-occlusions.%2520We%2520show%2520that%25203D%2520reconstruction%2520and%25203D%2520pose%2520estimation%2520can%2520be%250Areduced%2520to%2520the%2520prediction%2520of%2520DualPMs.%2520Empirically%252C%2520we%2520demonstrate%2520that%2520this%250Arepresentation%2520is%2520a%2520suitable%2520target%2520for%2520deep%2520networks%2520to%2520predict.%2520Specifically%252C%250Awe%2520focus%2520on%2520modeling%2520quadrupeds%252C%2520showing%2520that%2520DualPMs%2520can%2520be%2520trained%2520purely%2520on%250Asynthetic%25203D%2520data%252C%2520consisting%2520of%2520one%2520or%2520two%2520models%2520per%2520category%252C%2520while%250Ageneralizing%2520effectively%2520to%2520real%2520images.%2520With%2520this%2520approach%252C%2520we%2520achieve%250Asignificant%2520improvements%2520over%2520previous%2520methods%2520for%2520the%25203D%2520analysis%2520and%250Areconstruction%2520of%2520such%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04464v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualPM%3A%20Dual%20Posed-Canonical%20Point%20Maps%20for%203D%20Shape%20and%20Pose%0A%20%20Reconstruction&entry.906535625=Ben%20Kaye%20and%20Tomas%20Jakab%20and%20Shangzhe%20Wu%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi&entry.1292438233=%20%20The%20choice%20of%20data%20representation%20is%20a%20key%20factor%20in%20the%20success%20of%20deep%0Alearning%20in%20geometric%20tasks.%20For%20instance%2C%20DUSt3R%20recently%20introduced%20the%0Aconcept%20of%20viewpoint-invariant%20point%20maps%2C%20generalizing%20depth%20prediction%20and%0Ashowing%20that%20all%20key%20problems%20in%20the%203D%20reconstruction%20of%20static%20scenes%20can%20be%0Areduced%20to%20predicting%20such%20point%20maps.%20In%20this%20paper%2C%20we%20develop%20an%20analogous%0Aconcept%20for%20a%20very%20different%20problem%3A%20the%20reconstruction%20of%20the%203D%20shape%20and%0Apose%20of%20deformable%20objects.%20To%20this%20end%2C%20we%20introduce%20Dual%20Point%20Maps%20%28DualPM%29%2C%0Awhere%20a%20pair%20of%20point%20maps%20is%20extracted%20from%20the%20same%20image-one%20associating%0Apixels%20to%20their%203D%20locations%20on%20the%20object%20and%20the%20other%20to%20a%20canonical%20version%0Aof%20the%20object%20in%20its%20rest%20pose.%20We%20also%20extend%20point%20maps%20to%20amodal%0Areconstruction%20to%20recover%20the%20complete%20shape%20of%20the%20object%2C%20even%20through%0Aself-occlusions.%20We%20show%20that%203D%20reconstruction%20and%203D%20pose%20estimation%20can%20be%0Areduced%20to%20the%20prediction%20of%20DualPMs.%20Empirically%2C%20we%20demonstrate%20that%20this%0Arepresentation%20is%20a%20suitable%20target%20for%20deep%20networks%20to%20predict.%20Specifically%2C%0Awe%20focus%20on%20modeling%20quadrupeds%2C%20showing%20that%20DualPMs%20can%20be%20trained%20purely%20on%0Asynthetic%203D%20data%2C%20consisting%20of%20one%20or%20two%20models%20per%20category%2C%20while%0Ageneralizing%20effectively%20to%20real%20images.%20With%20this%20approach%2C%20we%20achieve%0Asignificant%20improvements%20over%20previous%20methods%20for%20the%203D%20analysis%20and%0Areconstruction%20of%20such%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04464v5&entry.124074799=Read"},
{"title": "Human-in-Context: Unified Cross-Domain 3D Human Motion Modeling via\n  In-Context Learning", "author": "Mengyuan Liu and Xinshun Wang and Zhongbin Fang and Deheng Ye and Xia Li and Tao Tang and Songtao Wu and Xiangtai Li and Ming-Hsuan Yang", "abstract": "  This paper aims to model 3D human motion across domains, where a single model\nis expected to handle multiple modalities, tasks, and datasets. Existing\ncross-domain models often rely on domain-specific components and multi-stage\ntraining, which limits their practicality and scalability. To overcome these\nchallenges, we propose a new setting to train a unified cross-domain model\nthrough a single process, eliminating the need for domain-specific components\nand multi-stage training. We first introduce Pose-in-Context (PiC), which\nleverages in-context learning to create a pose-centric cross-domain model.\nWhile PiC generalizes across multiple pose-based tasks and datasets, it\nencounters difficulties with modality diversity, prompting strategy, and\ncontextual dependency handling. We thus propose Human-in-Context (HiC), an\nextension of PiC that broadens generalization across modalities, tasks, and\ndatasets. HiC combines pose and mesh representations within a unified\nframework, expands task coverage, and incorporates larger-scale datasets.\nAdditionally, HiC introduces a max-min similarity prompt sampling strategy to\nenhance generalization across diverse domains and a network architecture with\ndual-branch context injection for improved handling of contextual dependencies.\nExtensive experimental results show that HiC performs better than PiC in terms\nof generalization, data scale, and performance across a wide range of domains.\nThese results demonstrate the potential of HiC for building a unified\ncross-domain 3D human motion model with improved flexibility and scalability.\nThe source codes and models are available at\nhttps://github.com/BradleyWang0416/Human-in-Context.\n", "link": "http://arxiv.org/abs/2508.10897v1", "date": "2025-08-14", "relevancy": 2.9432, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6364}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5718}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-in-Context%3A%20Unified%20Cross-Domain%203D%20Human%20Motion%20Modeling%20via%0A%20%20In-Context%20Learning&body=Title%3A%20Human-in-Context%3A%20Unified%20Cross-Domain%203D%20Human%20Motion%20Modeling%20via%0A%20%20In-Context%20Learning%0AAuthor%3A%20Mengyuan%20Liu%20and%20Xinshun%20Wang%20and%20Zhongbin%20Fang%20and%20Deheng%20Ye%20and%20Xia%20Li%20and%20Tao%20Tang%20and%20Songtao%20Wu%20and%20Xiangtai%20Li%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20model%203D%20human%20motion%20across%20domains%2C%20where%20a%20single%20model%0Ais%20expected%20to%20handle%20multiple%20modalities%2C%20tasks%2C%20and%20datasets.%20Existing%0Across-domain%20models%20often%20rely%20on%20domain-specific%20components%20and%20multi-stage%0Atraining%2C%20which%20limits%20their%20practicality%20and%20scalability.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20a%20new%20setting%20to%20train%20a%20unified%20cross-domain%20model%0Athrough%20a%20single%20process%2C%20eliminating%20the%20need%20for%20domain-specific%20components%0Aand%20multi-stage%20training.%20We%20first%20introduce%20Pose-in-Context%20%28PiC%29%2C%20which%0Aleverages%20in-context%20learning%20to%20create%20a%20pose-centric%20cross-domain%20model.%0AWhile%20PiC%20generalizes%20across%20multiple%20pose-based%20tasks%20and%20datasets%2C%20it%0Aencounters%20difficulties%20with%20modality%20diversity%2C%20prompting%20strategy%2C%20and%0Acontextual%20dependency%20handling.%20We%20thus%20propose%20Human-in-Context%20%28HiC%29%2C%20an%0Aextension%20of%20PiC%20that%20broadens%20generalization%20across%20modalities%2C%20tasks%2C%20and%0Adatasets.%20HiC%20combines%20pose%20and%20mesh%20representations%20within%20a%20unified%0Aframework%2C%20expands%20task%20coverage%2C%20and%20incorporates%20larger-scale%20datasets.%0AAdditionally%2C%20HiC%20introduces%20a%20max-min%20similarity%20prompt%20sampling%20strategy%20to%0Aenhance%20generalization%20across%20diverse%20domains%20and%20a%20network%20architecture%20with%0Adual-branch%20context%20injection%20for%20improved%20handling%20of%20contextual%20dependencies.%0AExtensive%20experimental%20results%20show%20that%20HiC%20performs%20better%20than%20PiC%20in%20terms%0Aof%20generalization%2C%20data%20scale%2C%20and%20performance%20across%20a%20wide%20range%20of%20domains.%0AThese%20results%20demonstrate%20the%20potential%20of%20HiC%20for%20building%20a%20unified%0Across-domain%203D%20human%20motion%20model%20with%20improved%20flexibility%20and%20scalability.%0AThe%20source%20codes%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/BradleyWang0416/Human-in-Context.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-in-Context%253A%2520Unified%2520Cross-Domain%25203D%2520Human%2520Motion%2520Modeling%2520via%250A%2520%2520In-Context%2520Learning%26entry.906535625%3DMengyuan%2520Liu%2520and%2520Xinshun%2520Wang%2520and%2520Zhongbin%2520Fang%2520and%2520Deheng%2520Ye%2520and%2520Xia%2520Li%2520and%2520Tao%2520Tang%2520and%2520Songtao%2520Wu%2520and%2520Xiangtai%2520Li%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520model%25203D%2520human%2520motion%2520across%2520domains%252C%2520where%2520a%2520single%2520model%250Ais%2520expected%2520to%2520handle%2520multiple%2520modalities%252C%2520tasks%252C%2520and%2520datasets.%2520Existing%250Across-domain%2520models%2520often%2520rely%2520on%2520domain-specific%2520components%2520and%2520multi-stage%250Atraining%252C%2520which%2520limits%2520their%2520practicality%2520and%2520scalability.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520new%2520setting%2520to%2520train%2520a%2520unified%2520cross-domain%2520model%250Athrough%2520a%2520single%2520process%252C%2520eliminating%2520the%2520need%2520for%2520domain-specific%2520components%250Aand%2520multi-stage%2520training.%2520We%2520first%2520introduce%2520Pose-in-Context%2520%2528PiC%2529%252C%2520which%250Aleverages%2520in-context%2520learning%2520to%2520create%2520a%2520pose-centric%2520cross-domain%2520model.%250AWhile%2520PiC%2520generalizes%2520across%2520multiple%2520pose-based%2520tasks%2520and%2520datasets%252C%2520it%250Aencounters%2520difficulties%2520with%2520modality%2520diversity%252C%2520prompting%2520strategy%252C%2520and%250Acontextual%2520dependency%2520handling.%2520We%2520thus%2520propose%2520Human-in-Context%2520%2528HiC%2529%252C%2520an%250Aextension%2520of%2520PiC%2520that%2520broadens%2520generalization%2520across%2520modalities%252C%2520tasks%252C%2520and%250Adatasets.%2520HiC%2520combines%2520pose%2520and%2520mesh%2520representations%2520within%2520a%2520unified%250Aframework%252C%2520expands%2520task%2520coverage%252C%2520and%2520incorporates%2520larger-scale%2520datasets.%250AAdditionally%252C%2520HiC%2520introduces%2520a%2520max-min%2520similarity%2520prompt%2520sampling%2520strategy%2520to%250Aenhance%2520generalization%2520across%2520diverse%2520domains%2520and%2520a%2520network%2520architecture%2520with%250Adual-branch%2520context%2520injection%2520for%2520improved%2520handling%2520of%2520contextual%2520dependencies.%250AExtensive%2520experimental%2520results%2520show%2520that%2520HiC%2520performs%2520better%2520than%2520PiC%2520in%2520terms%250Aof%2520generalization%252C%2520data%2520scale%252C%2520and%2520performance%2520across%2520a%2520wide%2520range%2520of%2520domains.%250AThese%2520results%2520demonstrate%2520the%2520potential%2520of%2520HiC%2520for%2520building%2520a%2520unified%250Across-domain%25203D%2520human%2520motion%2520model%2520with%2520improved%2520flexibility%2520and%2520scalability.%250AThe%2520source%2520codes%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/BradleyWang0416/Human-in-Context.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-in-Context%3A%20Unified%20Cross-Domain%203D%20Human%20Motion%20Modeling%20via%0A%20%20In-Context%20Learning&entry.906535625=Mengyuan%20Liu%20and%20Xinshun%20Wang%20and%20Zhongbin%20Fang%20and%20Deheng%20Ye%20and%20Xia%20Li%20and%20Tao%20Tang%20and%20Songtao%20Wu%20and%20Xiangtai%20Li%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20This%20paper%20aims%20to%20model%203D%20human%20motion%20across%20domains%2C%20where%20a%20single%20model%0Ais%20expected%20to%20handle%20multiple%20modalities%2C%20tasks%2C%20and%20datasets.%20Existing%0Across-domain%20models%20often%20rely%20on%20domain-specific%20components%20and%20multi-stage%0Atraining%2C%20which%20limits%20their%20practicality%20and%20scalability.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20a%20new%20setting%20to%20train%20a%20unified%20cross-domain%20model%0Athrough%20a%20single%20process%2C%20eliminating%20the%20need%20for%20domain-specific%20components%0Aand%20multi-stage%20training.%20We%20first%20introduce%20Pose-in-Context%20%28PiC%29%2C%20which%0Aleverages%20in-context%20learning%20to%20create%20a%20pose-centric%20cross-domain%20model.%0AWhile%20PiC%20generalizes%20across%20multiple%20pose-based%20tasks%20and%20datasets%2C%20it%0Aencounters%20difficulties%20with%20modality%20diversity%2C%20prompting%20strategy%2C%20and%0Acontextual%20dependency%20handling.%20We%20thus%20propose%20Human-in-Context%20%28HiC%29%2C%20an%0Aextension%20of%20PiC%20that%20broadens%20generalization%20across%20modalities%2C%20tasks%2C%20and%0Adatasets.%20HiC%20combines%20pose%20and%20mesh%20representations%20within%20a%20unified%0Aframework%2C%20expands%20task%20coverage%2C%20and%20incorporates%20larger-scale%20datasets.%0AAdditionally%2C%20HiC%20introduces%20a%20max-min%20similarity%20prompt%20sampling%20strategy%20to%0Aenhance%20generalization%20across%20diverse%20domains%20and%20a%20network%20architecture%20with%0Adual-branch%20context%20injection%20for%20improved%20handling%20of%20contextual%20dependencies.%0AExtensive%20experimental%20results%20show%20that%20HiC%20performs%20better%20than%20PiC%20in%20terms%0Aof%20generalization%2C%20data%20scale%2C%20and%20performance%20across%20a%20wide%20range%20of%20domains.%0AThese%20results%20demonstrate%20the%20potential%20of%20HiC%20for%20building%20a%20unified%0Across-domain%203D%20human%20motion%20model%20with%20improved%20flexibility%20and%20scalability.%0AThe%20source%20codes%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/BradleyWang0416/Human-in-Context.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10897v1&entry.124074799=Read"},
{"title": "ChatENV: An Interactive Vision-Language Model for Sensor-Guided\n  Environmental Monitoring and Scenario Simulation", "author": "Hosam Elgendy and Ahmed Sharshar and Ahmed Aboeitta and Mohsen Guizani", "abstract": "  Understanding environmental changes from aerial imagery is vital for climate\nresilience, urban planning, and ecosystem monitoring. Yet, current vision\nlanguage models (VLMs) overlook causal signals from environmental sensors, rely\non single-source captions prone to stylistic bias, and lack interactive\nscenario-based reasoning. We present ChatENV, the first interactive VLM that\njointly reasons over satellite image pairs and real-world sensor data. Our\nframework: (i) creates a 177k-image dataset forming 152k temporal pairs across\n62 land-use classes in 197 countries with rich sensor metadata (e.g.,\ntemperature, PM10, CO); (ii) annotates data using GPT- 4o and Gemini 2.0 for\nstylistic and semantic diversity; and (iii) fine-tunes Qwen-2.5-VL using\nefficient Low-Rank Adaptation (LoRA) adapters for chat purposes. ChatENV\nachieves strong performance in temporal and \"what-if\" reasoning (e.g., BERT-F1\n0.903) and rivals or outperforms state-of-the-art temporal models, while\nsupporting interactive scenario-based analysis. This positions ChatENV as a\npowerful tool for grounded, sensor-aware environmental monitoring.\n", "link": "http://arxiv.org/abs/2508.10635v1", "date": "2025-08-14", "relevancy": 2.9244, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5907}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5907}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatENV%3A%20An%20Interactive%20Vision-Language%20Model%20for%20Sensor-Guided%0A%20%20Environmental%20Monitoring%20and%20Scenario%20Simulation&body=Title%3A%20ChatENV%3A%20An%20Interactive%20Vision-Language%20Model%20for%20Sensor-Guided%0A%20%20Environmental%20Monitoring%20and%20Scenario%20Simulation%0AAuthor%3A%20Hosam%20Elgendy%20and%20Ahmed%20Sharshar%20and%20Ahmed%20Aboeitta%20and%20Mohsen%20Guizani%0AAbstract%3A%20%20%20Understanding%20environmental%20changes%20from%20aerial%20imagery%20is%20vital%20for%20climate%0Aresilience%2C%20urban%20planning%2C%20and%20ecosystem%20monitoring.%20Yet%2C%20current%20vision%0Alanguage%20models%20%28VLMs%29%20overlook%20causal%20signals%20from%20environmental%20sensors%2C%20rely%0Aon%20single-source%20captions%20prone%20to%20stylistic%20bias%2C%20and%20lack%20interactive%0Ascenario-based%20reasoning.%20We%20present%20ChatENV%2C%20the%20first%20interactive%20VLM%20that%0Ajointly%20reasons%20over%20satellite%20image%20pairs%20and%20real-world%20sensor%20data.%20Our%0Aframework%3A%20%28i%29%20creates%20a%20177k-image%20dataset%20forming%20152k%20temporal%20pairs%20across%0A62%20land-use%20classes%20in%20197%20countries%20with%20rich%20sensor%20metadata%20%28e.g.%2C%0Atemperature%2C%20PM10%2C%20CO%29%3B%20%28ii%29%20annotates%20data%20using%20GPT-%204o%20and%20Gemini%202.0%20for%0Astylistic%20and%20semantic%20diversity%3B%20and%20%28iii%29%20fine-tunes%20Qwen-2.5-VL%20using%0Aefficient%20Low-Rank%20Adaptation%20%28LoRA%29%20adapters%20for%20chat%20purposes.%20ChatENV%0Aachieves%20strong%20performance%20in%20temporal%20and%20%22what-if%22%20reasoning%20%28e.g.%2C%20BERT-F1%0A0.903%29%20and%20rivals%20or%20outperforms%20state-of-the-art%20temporal%20models%2C%20while%0Asupporting%20interactive%20scenario-based%20analysis.%20This%20positions%20ChatENV%20as%20a%0Apowerful%20tool%20for%20grounded%2C%20sensor-aware%20environmental%20monitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatENV%253A%2520An%2520Interactive%2520Vision-Language%2520Model%2520for%2520Sensor-Guided%250A%2520%2520Environmental%2520Monitoring%2520and%2520Scenario%2520Simulation%26entry.906535625%3DHosam%2520Elgendy%2520and%2520Ahmed%2520Sharshar%2520and%2520Ahmed%2520Aboeitta%2520and%2520Mohsen%2520Guizani%26entry.1292438233%3D%2520%2520Understanding%2520environmental%2520changes%2520from%2520aerial%2520imagery%2520is%2520vital%2520for%2520climate%250Aresilience%252C%2520urban%2520planning%252C%2520and%2520ecosystem%2520monitoring.%2520Yet%252C%2520current%2520vision%250Alanguage%2520models%2520%2528VLMs%2529%2520overlook%2520causal%2520signals%2520from%2520environmental%2520sensors%252C%2520rely%250Aon%2520single-source%2520captions%2520prone%2520to%2520stylistic%2520bias%252C%2520and%2520lack%2520interactive%250Ascenario-based%2520reasoning.%2520We%2520present%2520ChatENV%252C%2520the%2520first%2520interactive%2520VLM%2520that%250Ajointly%2520reasons%2520over%2520satellite%2520image%2520pairs%2520and%2520real-world%2520sensor%2520data.%2520Our%250Aframework%253A%2520%2528i%2529%2520creates%2520a%2520177k-image%2520dataset%2520forming%2520152k%2520temporal%2520pairs%2520across%250A62%2520land-use%2520classes%2520in%2520197%2520countries%2520with%2520rich%2520sensor%2520metadata%2520%2528e.g.%252C%250Atemperature%252C%2520PM10%252C%2520CO%2529%253B%2520%2528ii%2529%2520annotates%2520data%2520using%2520GPT-%25204o%2520and%2520Gemini%25202.0%2520for%250Astylistic%2520and%2520semantic%2520diversity%253B%2520and%2520%2528iii%2529%2520fine-tunes%2520Qwen-2.5-VL%2520using%250Aefficient%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520adapters%2520for%2520chat%2520purposes.%2520ChatENV%250Aachieves%2520strong%2520performance%2520in%2520temporal%2520and%2520%2522what-if%2522%2520reasoning%2520%2528e.g.%252C%2520BERT-F1%250A0.903%2529%2520and%2520rivals%2520or%2520outperforms%2520state-of-the-art%2520temporal%2520models%252C%2520while%250Asupporting%2520interactive%2520scenario-based%2520analysis.%2520This%2520positions%2520ChatENV%2520as%2520a%250Apowerful%2520tool%2520for%2520grounded%252C%2520sensor-aware%2520environmental%2520monitoring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatENV%3A%20An%20Interactive%20Vision-Language%20Model%20for%20Sensor-Guided%0A%20%20Environmental%20Monitoring%20and%20Scenario%20Simulation&entry.906535625=Hosam%20Elgendy%20and%20Ahmed%20Sharshar%20and%20Ahmed%20Aboeitta%20and%20Mohsen%20Guizani&entry.1292438233=%20%20Understanding%20environmental%20changes%20from%20aerial%20imagery%20is%20vital%20for%20climate%0Aresilience%2C%20urban%20planning%2C%20and%20ecosystem%20monitoring.%20Yet%2C%20current%20vision%0Alanguage%20models%20%28VLMs%29%20overlook%20causal%20signals%20from%20environmental%20sensors%2C%20rely%0Aon%20single-source%20captions%20prone%20to%20stylistic%20bias%2C%20and%20lack%20interactive%0Ascenario-based%20reasoning.%20We%20present%20ChatENV%2C%20the%20first%20interactive%20VLM%20that%0Ajointly%20reasons%20over%20satellite%20image%20pairs%20and%20real-world%20sensor%20data.%20Our%0Aframework%3A%20%28i%29%20creates%20a%20177k-image%20dataset%20forming%20152k%20temporal%20pairs%20across%0A62%20land-use%20classes%20in%20197%20countries%20with%20rich%20sensor%20metadata%20%28e.g.%2C%0Atemperature%2C%20PM10%2C%20CO%29%3B%20%28ii%29%20annotates%20data%20using%20GPT-%204o%20and%20Gemini%202.0%20for%0Astylistic%20and%20semantic%20diversity%3B%20and%20%28iii%29%20fine-tunes%20Qwen-2.5-VL%20using%0Aefficient%20Low-Rank%20Adaptation%20%28LoRA%29%20adapters%20for%20chat%20purposes.%20ChatENV%0Aachieves%20strong%20performance%20in%20temporal%20and%20%22what-if%22%20reasoning%20%28e.g.%2C%20BERT-F1%0A0.903%29%20and%20rivals%20or%20outperforms%20state-of-the-art%20temporal%20models%2C%20while%0Asupporting%20interactive%20scenario-based%20analysis.%20This%20positions%20ChatENV%20as%20a%0Apowerful%20tool%20for%20grounded%2C%20sensor-aware%20environmental%20monitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10635v1&entry.124074799=Read"},
{"title": "An Efficient Model-Driven Groupwise Approach for Atlas Construction", "author": "Ziwei Zou and Bei Zou and Xiaoyan Kui and Wenqi Lu and Haoran Dou and Arezoo Zakeri and Timothy Cootes and Alejandro F Frangi and Jinming Duan", "abstract": "  Atlas construction is fundamental to medical image analysis, offering a\nstandardized spatial reference for tasks such as population-level anatomical\nmodeling. While data-driven registration methods have recently shown promise in\npairwise settings, their reliance on large training datasets, limited\ngeneralizability, and lack of true inference phases in groupwise contexts\nhinder their practical use. In contrast, model-driven methods offer\ntraining-free, theoretically grounded, and data-efficient alternatives, though\nthey often face scalability and optimization challenges when applied to large\n3D datasets. In this work, we introduce DARC (Diffeomorphic Atlas Registration\nvia Coordinate descent), a novel model-driven groupwise registration framework\nfor atlas construction. DARC supports a broad range of image dissimilarity\nmetrics and efficiently handles arbitrary numbers of 3D images without\nincurring GPU memory issues. Through a coordinate descent strategy and a\ncentrality-enforcing activation function, DARC produces unbiased, diffeomorphic\natlases with high anatomical fidelity. Beyond atlas construction, we\ndemonstrate two key applications: (1) One-shot segmentation, where labels\nannotated only on the atlas are propagated to subjects via inverse\ndeformations, outperforming state-of-the-art few-shot methods; and (2) shape\nsynthesis, where new anatomical variants are generated by warping the atlas\nmesh using synthesized diffeomorphic deformation fields. Overall, DARC offers a\nflexible, generalizable, and resource-efficient framework for atlas\nconstruction and applications.\n", "link": "http://arxiv.org/abs/2508.10743v1", "date": "2025-08-14", "relevancy": 2.8865, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5907}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5907}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Model-Driven%20Groupwise%20Approach%20for%20Atlas%20Construction&body=Title%3A%20An%20Efficient%20Model-Driven%20Groupwise%20Approach%20for%20Atlas%20Construction%0AAuthor%3A%20Ziwei%20Zou%20and%20Bei%20Zou%20and%20Xiaoyan%20Kui%20and%20Wenqi%20Lu%20and%20Haoran%20Dou%20and%20Arezoo%20Zakeri%20and%20Timothy%20Cootes%20and%20Alejandro%20F%20Frangi%20and%20Jinming%20Duan%0AAbstract%3A%20%20%20Atlas%20construction%20is%20fundamental%20to%20medical%20image%20analysis%2C%20offering%20a%0Astandardized%20spatial%20reference%20for%20tasks%20such%20as%20population-level%20anatomical%0Amodeling.%20While%20data-driven%20registration%20methods%20have%20recently%20shown%20promise%20in%0Apairwise%20settings%2C%20their%20reliance%20on%20large%20training%20datasets%2C%20limited%0Ageneralizability%2C%20and%20lack%20of%20true%20inference%20phases%20in%20groupwise%20contexts%0Ahinder%20their%20practical%20use.%20In%20contrast%2C%20model-driven%20methods%20offer%0Atraining-free%2C%20theoretically%20grounded%2C%20and%20data-efficient%20alternatives%2C%20though%0Athey%20often%20face%20scalability%20and%20optimization%20challenges%20when%20applied%20to%20large%0A3D%20datasets.%20In%20this%20work%2C%20we%20introduce%20DARC%20%28Diffeomorphic%20Atlas%20Registration%0Avia%20Coordinate%20descent%29%2C%20a%20novel%20model-driven%20groupwise%20registration%20framework%0Afor%20atlas%20construction.%20DARC%20supports%20a%20broad%20range%20of%20image%20dissimilarity%0Ametrics%20and%20efficiently%20handles%20arbitrary%20numbers%20of%203D%20images%20without%0Aincurring%20GPU%20memory%20issues.%20Through%20a%20coordinate%20descent%20strategy%20and%20a%0Acentrality-enforcing%20activation%20function%2C%20DARC%20produces%20unbiased%2C%20diffeomorphic%0Aatlases%20with%20high%20anatomical%20fidelity.%20Beyond%20atlas%20construction%2C%20we%0Ademonstrate%20two%20key%20applications%3A%20%281%29%20One-shot%20segmentation%2C%20where%20labels%0Aannotated%20only%20on%20the%20atlas%20are%20propagated%20to%20subjects%20via%20inverse%0Adeformations%2C%20outperforming%20state-of-the-art%20few-shot%20methods%3B%20and%20%282%29%20shape%0Asynthesis%2C%20where%20new%20anatomical%20variants%20are%20generated%20by%20warping%20the%20atlas%0Amesh%20using%20synthesized%20diffeomorphic%20deformation%20fields.%20Overall%2C%20DARC%20offers%20a%0Aflexible%2C%20generalizable%2C%20and%20resource-efficient%20framework%20for%20atlas%0Aconstruction%20and%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Model-Driven%2520Groupwise%2520Approach%2520for%2520Atlas%2520Construction%26entry.906535625%3DZiwei%2520Zou%2520and%2520Bei%2520Zou%2520and%2520Xiaoyan%2520Kui%2520and%2520Wenqi%2520Lu%2520and%2520Haoran%2520Dou%2520and%2520Arezoo%2520Zakeri%2520and%2520Timothy%2520Cootes%2520and%2520Alejandro%2520F%2520Frangi%2520and%2520Jinming%2520Duan%26entry.1292438233%3D%2520%2520Atlas%2520construction%2520is%2520fundamental%2520to%2520medical%2520image%2520analysis%252C%2520offering%2520a%250Astandardized%2520spatial%2520reference%2520for%2520tasks%2520such%2520as%2520population-level%2520anatomical%250Amodeling.%2520While%2520data-driven%2520registration%2520methods%2520have%2520recently%2520shown%2520promise%2520in%250Apairwise%2520settings%252C%2520their%2520reliance%2520on%2520large%2520training%2520datasets%252C%2520limited%250Ageneralizability%252C%2520and%2520lack%2520of%2520true%2520inference%2520phases%2520in%2520groupwise%2520contexts%250Ahinder%2520their%2520practical%2520use.%2520In%2520contrast%252C%2520model-driven%2520methods%2520offer%250Atraining-free%252C%2520theoretically%2520grounded%252C%2520and%2520data-efficient%2520alternatives%252C%2520though%250Athey%2520often%2520face%2520scalability%2520and%2520optimization%2520challenges%2520when%2520applied%2520to%2520large%250A3D%2520datasets.%2520In%2520this%2520work%252C%2520we%2520introduce%2520DARC%2520%2528Diffeomorphic%2520Atlas%2520Registration%250Avia%2520Coordinate%2520descent%2529%252C%2520a%2520novel%2520model-driven%2520groupwise%2520registration%2520framework%250Afor%2520atlas%2520construction.%2520DARC%2520supports%2520a%2520broad%2520range%2520of%2520image%2520dissimilarity%250Ametrics%2520and%2520efficiently%2520handles%2520arbitrary%2520numbers%2520of%25203D%2520images%2520without%250Aincurring%2520GPU%2520memory%2520issues.%2520Through%2520a%2520coordinate%2520descent%2520strategy%2520and%2520a%250Acentrality-enforcing%2520activation%2520function%252C%2520DARC%2520produces%2520unbiased%252C%2520diffeomorphic%250Aatlases%2520with%2520high%2520anatomical%2520fidelity.%2520Beyond%2520atlas%2520construction%252C%2520we%250Ademonstrate%2520two%2520key%2520applications%253A%2520%25281%2529%2520One-shot%2520segmentation%252C%2520where%2520labels%250Aannotated%2520only%2520on%2520the%2520atlas%2520are%2520propagated%2520to%2520subjects%2520via%2520inverse%250Adeformations%252C%2520outperforming%2520state-of-the-art%2520few-shot%2520methods%253B%2520and%2520%25282%2529%2520shape%250Asynthesis%252C%2520where%2520new%2520anatomical%2520variants%2520are%2520generated%2520by%2520warping%2520the%2520atlas%250Amesh%2520using%2520synthesized%2520diffeomorphic%2520deformation%2520fields.%2520Overall%252C%2520DARC%2520offers%2520a%250Aflexible%252C%2520generalizable%252C%2520and%2520resource-efficient%2520framework%2520for%2520atlas%250Aconstruction%2520and%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Model-Driven%20Groupwise%20Approach%20for%20Atlas%20Construction&entry.906535625=Ziwei%20Zou%20and%20Bei%20Zou%20and%20Xiaoyan%20Kui%20and%20Wenqi%20Lu%20and%20Haoran%20Dou%20and%20Arezoo%20Zakeri%20and%20Timothy%20Cootes%20and%20Alejandro%20F%20Frangi%20and%20Jinming%20Duan&entry.1292438233=%20%20Atlas%20construction%20is%20fundamental%20to%20medical%20image%20analysis%2C%20offering%20a%0Astandardized%20spatial%20reference%20for%20tasks%20such%20as%20population-level%20anatomical%0Amodeling.%20While%20data-driven%20registration%20methods%20have%20recently%20shown%20promise%20in%0Apairwise%20settings%2C%20their%20reliance%20on%20large%20training%20datasets%2C%20limited%0Ageneralizability%2C%20and%20lack%20of%20true%20inference%20phases%20in%20groupwise%20contexts%0Ahinder%20their%20practical%20use.%20In%20contrast%2C%20model-driven%20methods%20offer%0Atraining-free%2C%20theoretically%20grounded%2C%20and%20data-efficient%20alternatives%2C%20though%0Athey%20often%20face%20scalability%20and%20optimization%20challenges%20when%20applied%20to%20large%0A3D%20datasets.%20In%20this%20work%2C%20we%20introduce%20DARC%20%28Diffeomorphic%20Atlas%20Registration%0Avia%20Coordinate%20descent%29%2C%20a%20novel%20model-driven%20groupwise%20registration%20framework%0Afor%20atlas%20construction.%20DARC%20supports%20a%20broad%20range%20of%20image%20dissimilarity%0Ametrics%20and%20efficiently%20handles%20arbitrary%20numbers%20of%203D%20images%20without%0Aincurring%20GPU%20memory%20issues.%20Through%20a%20coordinate%20descent%20strategy%20and%20a%0Acentrality-enforcing%20activation%20function%2C%20DARC%20produces%20unbiased%2C%20diffeomorphic%0Aatlases%20with%20high%20anatomical%20fidelity.%20Beyond%20atlas%20construction%2C%20we%0Ademonstrate%20two%20key%20applications%3A%20%281%29%20One-shot%20segmentation%2C%20where%20labels%0Aannotated%20only%20on%20the%20atlas%20are%20propagated%20to%20subjects%20via%20inverse%0Adeformations%2C%20outperforming%20state-of-the-art%20few-shot%20methods%3B%20and%20%282%29%20shape%0Asynthesis%2C%20where%20new%20anatomical%20variants%20are%20generated%20by%20warping%20the%20atlas%0Amesh%20using%20synthesized%20diffeomorphic%20deformation%20fields.%20Overall%2C%20DARC%20offers%20a%0Aflexible%2C%20generalizable%2C%20and%20resource-efficient%20framework%20for%20atlas%0Aconstruction%20and%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10743v1&entry.124074799=Read"},
{"title": "AddressVLM: Cross-view Alignment Tuning for Image Address Localization\n  using Large Vision-Language Models", "author": "Shixiong Xu and Chenghao Zhang and Lubin Fan and Yuan Zhou and Bin Fan and Shiming Xiang and Gaofeng Meng and Jieping Ye", "abstract": "  Large visual language models (LVLMs) have demonstrated impressive performance\nin coarse-grained geo-localization at the country or city level, but they\nstruggle with fine-grained street-level localization within urban areas. In\nthis paper, we explore integrating city-wide address localization capabilities\ninto LVLMs, facilitating flexible address-related question answering using\nstreet-view images. A key challenge is that the street-view visual\nquestion-and-answer (VQA) data provides only microscopic visual cues, leading\nto subpar performance in fine-tuned models. To tackle this issue, we\nincorporate perspective-invariant satellite images as macro cues and propose\ncross-view alignment tuning including a satellite-view and street-view image\ngrafting mechanism, along with an automatic label generation mechanism. Then\nLVLM's global understanding of street distribution is enhanced through\ncross-view matching. Our proposed model, named AddressVLM, consists of\ntwo-stage training protocols: cross-view alignment tuning and address\nlocalization tuning. Furthermore, we have constructed two street-view VQA\ndatasets based on image address localization datasets from Pittsburgh and San\nFrancisco. Qualitative and quantitative evaluations demonstrate that AddressVLM\noutperforms counterpart LVLMs by over 9% and 12% in average address\nlocalization accuracy on these two datasets, respectively.\n", "link": "http://arxiv.org/abs/2508.10667v1", "date": "2025-08-14", "relevancy": 2.8282, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5764}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AddressVLM%3A%20Cross-view%20Alignment%20Tuning%20for%20Image%20Address%20Localization%0A%20%20using%20Large%20Vision-Language%20Models&body=Title%3A%20AddressVLM%3A%20Cross-view%20Alignment%20Tuning%20for%20Image%20Address%20Localization%0A%20%20using%20Large%20Vision-Language%20Models%0AAuthor%3A%20Shixiong%20Xu%20and%20Chenghao%20Zhang%20and%20Lubin%20Fan%20and%20Yuan%20Zhou%20and%20Bin%20Fan%20and%20Shiming%20Xiang%20and%20Gaofeng%20Meng%20and%20Jieping%20Ye%0AAbstract%3A%20%20%20Large%20visual%20language%20models%20%28LVLMs%29%20have%20demonstrated%20impressive%20performance%0Ain%20coarse-grained%20geo-localization%20at%20the%20country%20or%20city%20level%2C%20but%20they%0Astruggle%20with%20fine-grained%20street-level%20localization%20within%20urban%20areas.%20In%0Athis%20paper%2C%20we%20explore%20integrating%20city-wide%20address%20localization%20capabilities%0Ainto%20LVLMs%2C%20facilitating%20flexible%20address-related%20question%20answering%20using%0Astreet-view%20images.%20A%20key%20challenge%20is%20that%20the%20street-view%20visual%0Aquestion-and-answer%20%28VQA%29%20data%20provides%20only%20microscopic%20visual%20cues%2C%20leading%0Ato%20subpar%20performance%20in%20fine-tuned%20models.%20To%20tackle%20this%20issue%2C%20we%0Aincorporate%20perspective-invariant%20satellite%20images%20as%20macro%20cues%20and%20propose%0Across-view%20alignment%20tuning%20including%20a%20satellite-view%20and%20street-view%20image%0Agrafting%20mechanism%2C%20along%20with%20an%20automatic%20label%20generation%20mechanism.%20Then%0ALVLM%27s%20global%20understanding%20of%20street%20distribution%20is%20enhanced%20through%0Across-view%20matching.%20Our%20proposed%20model%2C%20named%20AddressVLM%2C%20consists%20of%0Atwo-stage%20training%20protocols%3A%20cross-view%20alignment%20tuning%20and%20address%0Alocalization%20tuning.%20Furthermore%2C%20we%20have%20constructed%20two%20street-view%20VQA%0Adatasets%20based%20on%20image%20address%20localization%20datasets%20from%20Pittsburgh%20and%20San%0AFrancisco.%20Qualitative%20and%20quantitative%20evaluations%20demonstrate%20that%20AddressVLM%0Aoutperforms%20counterpart%20LVLMs%20by%20over%209%25%20and%2012%25%20in%20average%20address%0Alocalization%20accuracy%20on%20these%20two%20datasets%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressVLM%253A%2520Cross-view%2520Alignment%2520Tuning%2520for%2520Image%2520Address%2520Localization%250A%2520%2520using%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DShixiong%2520Xu%2520and%2520Chenghao%2520Zhang%2520and%2520Lubin%2520Fan%2520and%2520Yuan%2520Zhou%2520and%2520Bin%2520Fan%2520and%2520Shiming%2520Xiang%2520and%2520Gaofeng%2520Meng%2520and%2520Jieping%2520Ye%26entry.1292438233%3D%2520%2520Large%2520visual%2520language%2520models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520impressive%2520performance%250Ain%2520coarse-grained%2520geo-localization%2520at%2520the%2520country%2520or%2520city%2520level%252C%2520but%2520they%250Astruggle%2520with%2520fine-grained%2520street-level%2520localization%2520within%2520urban%2520areas.%2520In%250Athis%2520paper%252C%2520we%2520explore%2520integrating%2520city-wide%2520address%2520localization%2520capabilities%250Ainto%2520LVLMs%252C%2520facilitating%2520flexible%2520address-related%2520question%2520answering%2520using%250Astreet-view%2520images.%2520A%2520key%2520challenge%2520is%2520that%2520the%2520street-view%2520visual%250Aquestion-and-answer%2520%2528VQA%2529%2520data%2520provides%2520only%2520microscopic%2520visual%2520cues%252C%2520leading%250Ato%2520subpar%2520performance%2520in%2520fine-tuned%2520models.%2520To%2520tackle%2520this%2520issue%252C%2520we%250Aincorporate%2520perspective-invariant%2520satellite%2520images%2520as%2520macro%2520cues%2520and%2520propose%250Across-view%2520alignment%2520tuning%2520including%2520a%2520satellite-view%2520and%2520street-view%2520image%250Agrafting%2520mechanism%252C%2520along%2520with%2520an%2520automatic%2520label%2520generation%2520mechanism.%2520Then%250ALVLM%2527s%2520global%2520understanding%2520of%2520street%2520distribution%2520is%2520enhanced%2520through%250Across-view%2520matching.%2520Our%2520proposed%2520model%252C%2520named%2520AddressVLM%252C%2520consists%2520of%250Atwo-stage%2520training%2520protocols%253A%2520cross-view%2520alignment%2520tuning%2520and%2520address%250Alocalization%2520tuning.%2520Furthermore%252C%2520we%2520have%2520constructed%2520two%2520street-view%2520VQA%250Adatasets%2520based%2520on%2520image%2520address%2520localization%2520datasets%2520from%2520Pittsburgh%2520and%2520San%250AFrancisco.%2520Qualitative%2520and%2520quantitative%2520evaluations%2520demonstrate%2520that%2520AddressVLM%250Aoutperforms%2520counterpart%2520LVLMs%2520by%2520over%25209%2525%2520and%252012%2525%2520in%2520average%2520address%250Alocalization%2520accuracy%2520on%2520these%2520two%2520datasets%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AddressVLM%3A%20Cross-view%20Alignment%20Tuning%20for%20Image%20Address%20Localization%0A%20%20using%20Large%20Vision-Language%20Models&entry.906535625=Shixiong%20Xu%20and%20Chenghao%20Zhang%20and%20Lubin%20Fan%20and%20Yuan%20Zhou%20and%20Bin%20Fan%20and%20Shiming%20Xiang%20and%20Gaofeng%20Meng%20and%20Jieping%20Ye&entry.1292438233=%20%20Large%20visual%20language%20models%20%28LVLMs%29%20have%20demonstrated%20impressive%20performance%0Ain%20coarse-grained%20geo-localization%20at%20the%20country%20or%20city%20level%2C%20but%20they%0Astruggle%20with%20fine-grained%20street-level%20localization%20within%20urban%20areas.%20In%0Athis%20paper%2C%20we%20explore%20integrating%20city-wide%20address%20localization%20capabilities%0Ainto%20LVLMs%2C%20facilitating%20flexible%20address-related%20question%20answering%20using%0Astreet-view%20images.%20A%20key%20challenge%20is%20that%20the%20street-view%20visual%0Aquestion-and-answer%20%28VQA%29%20data%20provides%20only%20microscopic%20visual%20cues%2C%20leading%0Ato%20subpar%20performance%20in%20fine-tuned%20models.%20To%20tackle%20this%20issue%2C%20we%0Aincorporate%20perspective-invariant%20satellite%20images%20as%20macro%20cues%20and%20propose%0Across-view%20alignment%20tuning%20including%20a%20satellite-view%20and%20street-view%20image%0Agrafting%20mechanism%2C%20along%20with%20an%20automatic%20label%20generation%20mechanism.%20Then%0ALVLM%27s%20global%20understanding%20of%20street%20distribution%20is%20enhanced%20through%0Across-view%20matching.%20Our%20proposed%20model%2C%20named%20AddressVLM%2C%20consists%20of%0Atwo-stage%20training%20protocols%3A%20cross-view%20alignment%20tuning%20and%20address%0Alocalization%20tuning.%20Furthermore%2C%20we%20have%20constructed%20two%20street-view%20VQA%0Adatasets%20based%20on%20image%20address%20localization%20datasets%20from%20Pittsburgh%20and%20San%0AFrancisco.%20Qualitative%20and%20quantitative%20evaluations%20demonstrate%20that%20AddressVLM%0Aoutperforms%20counterpart%20LVLMs%20by%20over%209%25%20and%2012%25%20in%20average%20address%0Alocalization%20accuracy%20on%20these%20two%20datasets%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10667v1&entry.124074799=Read"},
{"title": "GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View\n  Stereo", "author": "Vibhas K. Vats and Sripad Joshi and David J. Crandall and Md. Alimoor Reza and Soon-heung Jung", "abstract": "  Traditional multi-view stereo (MVS) methods rely heavily on photometric and\ngeometric consistency constraints, but newer machine learning-based MVS methods\ncheck geometric consistency across multiple source views only as a\npost-processing step. In this paper, we present a novel approach that\nexplicitly encourages geometric consistency of reference view depth maps across\nmultiple source views at different scales during learning (see Fig. 1). We find\nthat adding this geometric consistency loss significantly accelerates learning\nby explicitly penalizing geometrically inconsistent pixels, reducing the\ntraining iteration requirements to nearly half that of other MVS methods. Our\nextensive experiments show that our approach achieves a new state-of-the-art on\nthe DTU and BlendedMVS datasets, and competitive results on the Tanks and\nTemples benchmark. To the best of our knowledge, GC-MVSNet is the first attempt\nto enforce multi-view, multi-scale geometric consistency during learning.\n", "link": "http://arxiv.org/abs/2310.19583v4", "date": "2025-08-14", "relevancy": 2.8206, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5913}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5521}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GC-MVSNet%3A%20Multi-View%2C%20Multi-Scale%2C%20Geometrically-Consistent%20Multi-View%0A%20%20Stereo&body=Title%3A%20GC-MVSNet%3A%20Multi-View%2C%20Multi-Scale%2C%20Geometrically-Consistent%20Multi-View%0A%20%20Stereo%0AAuthor%3A%20Vibhas%20K.%20Vats%20and%20Sripad%20Joshi%20and%20David%20J.%20Crandall%20and%20Md.%20Alimoor%20Reza%20and%20Soon-heung%20Jung%0AAbstract%3A%20%20%20Traditional%20multi-view%20stereo%20%28MVS%29%20methods%20rely%20heavily%20on%20photometric%20and%0Ageometric%20consistency%20constraints%2C%20but%20newer%20machine%20learning-based%20MVS%20methods%0Acheck%20geometric%20consistency%20across%20multiple%20source%20views%20only%20as%20a%0Apost-processing%20step.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20that%0Aexplicitly%20encourages%20geometric%20consistency%20of%20reference%20view%20depth%20maps%20across%0Amultiple%20source%20views%20at%20different%20scales%20during%20learning%20%28see%20Fig.%201%29.%20We%20find%0Athat%20adding%20this%20geometric%20consistency%20loss%20significantly%20accelerates%20learning%0Aby%20explicitly%20penalizing%20geometrically%20inconsistent%20pixels%2C%20reducing%20the%0Atraining%20iteration%20requirements%20to%20nearly%20half%20that%20of%20other%20MVS%20methods.%20Our%0Aextensive%20experiments%20show%20that%20our%20approach%20achieves%20a%20new%20state-of-the-art%20on%0Athe%20DTU%20and%20BlendedMVS%20datasets%2C%20and%20competitive%20results%20on%20the%20Tanks%20and%0ATemples%20benchmark.%20To%20the%20best%20of%20our%20knowledge%2C%20GC-MVSNet%20is%20the%20first%20attempt%0Ato%20enforce%20multi-view%2C%20multi-scale%20geometric%20consistency%20during%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19583v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGC-MVSNet%253A%2520Multi-View%252C%2520Multi-Scale%252C%2520Geometrically-Consistent%2520Multi-View%250A%2520%2520Stereo%26entry.906535625%3DVibhas%2520K.%2520Vats%2520and%2520Sripad%2520Joshi%2520and%2520David%2520J.%2520Crandall%2520and%2520Md.%2520Alimoor%2520Reza%2520and%2520Soon-heung%2520Jung%26entry.1292438233%3D%2520%2520Traditional%2520multi-view%2520stereo%2520%2528MVS%2529%2520methods%2520rely%2520heavily%2520on%2520photometric%2520and%250Ageometric%2520consistency%2520constraints%252C%2520but%2520newer%2520machine%2520learning-based%2520MVS%2520methods%250Acheck%2520geometric%2520consistency%2520across%2520multiple%2520source%2520views%2520only%2520as%2520a%250Apost-processing%2520step.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520approach%2520that%250Aexplicitly%2520encourages%2520geometric%2520consistency%2520of%2520reference%2520view%2520depth%2520maps%2520across%250Amultiple%2520source%2520views%2520at%2520different%2520scales%2520during%2520learning%2520%2528see%2520Fig.%25201%2529.%2520We%2520find%250Athat%2520adding%2520this%2520geometric%2520consistency%2520loss%2520significantly%2520accelerates%2520learning%250Aby%2520explicitly%2520penalizing%2520geometrically%2520inconsistent%2520pixels%252C%2520reducing%2520the%250Atraining%2520iteration%2520requirements%2520to%2520nearly%2520half%2520that%2520of%2520other%2520MVS%2520methods.%2520Our%250Aextensive%2520experiments%2520show%2520that%2520our%2520approach%2520achieves%2520a%2520new%2520state-of-the-art%2520on%250Athe%2520DTU%2520and%2520BlendedMVS%2520datasets%252C%2520and%2520competitive%2520results%2520on%2520the%2520Tanks%2520and%250ATemples%2520benchmark.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520GC-MVSNet%2520is%2520the%2520first%2520attempt%250Ato%2520enforce%2520multi-view%252C%2520multi-scale%2520geometric%2520consistency%2520during%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19583v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GC-MVSNet%3A%20Multi-View%2C%20Multi-Scale%2C%20Geometrically-Consistent%20Multi-View%0A%20%20Stereo&entry.906535625=Vibhas%20K.%20Vats%20and%20Sripad%20Joshi%20and%20David%20J.%20Crandall%20and%20Md.%20Alimoor%20Reza%20and%20Soon-heung%20Jung&entry.1292438233=%20%20Traditional%20multi-view%20stereo%20%28MVS%29%20methods%20rely%20heavily%20on%20photometric%20and%0Ageometric%20consistency%20constraints%2C%20but%20newer%20machine%20learning-based%20MVS%20methods%0Acheck%20geometric%20consistency%20across%20multiple%20source%20views%20only%20as%20a%0Apost-processing%20step.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20that%0Aexplicitly%20encourages%20geometric%20consistency%20of%20reference%20view%20depth%20maps%20across%0Amultiple%20source%20views%20at%20different%20scales%20during%20learning%20%28see%20Fig.%201%29.%20We%20find%0Athat%20adding%20this%20geometric%20consistency%20loss%20significantly%20accelerates%20learning%0Aby%20explicitly%20penalizing%20geometrically%20inconsistent%20pixels%2C%20reducing%20the%0Atraining%20iteration%20requirements%20to%20nearly%20half%20that%20of%20other%20MVS%20methods.%20Our%0Aextensive%20experiments%20show%20that%20our%20approach%20achieves%20a%20new%20state-of-the-art%20on%0Athe%20DTU%20and%20BlendedMVS%20datasets%2C%20and%20competitive%20results%20on%20the%20Tanks%20and%0ATemples%20benchmark.%20To%20the%20best%20of%20our%20knowledge%2C%20GC-MVSNet%20is%20the%20first%20attempt%0Ato%20enforce%20multi-view%2C%20multi-scale%20geometric%20consistency%20during%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19583v4&entry.124074799=Read"},
{"title": "GNN-based Unified Deep Learning", "author": "Furkan Pala and Islem Rekik", "abstract": "  Deep learning models often struggle to maintain generalizability in medical\nimaging, particularly under domain-fracture scenarios where distribution shifts\narise from varying imaging techniques, acquisition protocols, patient\npopulations, demographics, and equipment. In practice, each hospital may need\nto train distinct models - differing in learning task, width, and depth - to\nmatch local data. For example, one hospital may use Euclidean architectures\nsuch as MLPs and CNNs for tabular or grid-like image data, while another may\nrequire non-Euclidean architectures such as graph neural networks (GNNs) for\nirregular data like brain connectomes. How to train such heterogeneous models\ncoherently across datasets, while enhancing each model's generalizability,\nremains an open problem. We propose unified learning, a new paradigm that\nencodes each model into a graph representation, enabling unification in a\nshared graph learning space. A GNN then guides optimization of these unified\nmodels. By decoupling parameters of individual models and controlling them\nthrough a unified GNN (uGNN), our method supports parameter sharing and\nknowledge transfer across varying architectures (MLPs, CNNs, GNNs) and\ndistributions, improving generalizability. Evaluations on MorphoMNIST and two\nMedMNIST benchmarks - PneumoniaMNIST and BreastMNIST - show that unified\nlearning boosts performance when models are trained on unique distributions and\ntested on mixed ones, demonstrating strong robustness to unseen data with large\ndistribution shifts. Code and benchmarks: https://github.com/basiralab/uGNN\n", "link": "http://arxiv.org/abs/2508.10583v1", "date": "2025-08-14", "relevancy": 2.8138, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5844}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5575}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GNN-based%20Unified%20Deep%20Learning&body=Title%3A%20GNN-based%20Unified%20Deep%20Learning%0AAuthor%3A%20Furkan%20Pala%20and%20Islem%20Rekik%0AAbstract%3A%20%20%20Deep%20learning%20models%20often%20struggle%20to%20maintain%20generalizability%20in%20medical%0Aimaging%2C%20particularly%20under%20domain-fracture%20scenarios%20where%20distribution%20shifts%0Aarise%20from%20varying%20imaging%20techniques%2C%20acquisition%20protocols%2C%20patient%0Apopulations%2C%20demographics%2C%20and%20equipment.%20In%20practice%2C%20each%20hospital%20may%20need%0Ato%20train%20distinct%20models%20-%20differing%20in%20learning%20task%2C%20width%2C%20and%20depth%20-%20to%0Amatch%20local%20data.%20For%20example%2C%20one%20hospital%20may%20use%20Euclidean%20architectures%0Asuch%20as%20MLPs%20and%20CNNs%20for%20tabular%20or%20grid-like%20image%20data%2C%20while%20another%20may%0Arequire%20non-Euclidean%20architectures%20such%20as%20graph%20neural%20networks%20%28GNNs%29%20for%0Airregular%20data%20like%20brain%20connectomes.%20How%20to%20train%20such%20heterogeneous%20models%0Acoherently%20across%20datasets%2C%20while%20enhancing%20each%20model%27s%20generalizability%2C%0Aremains%20an%20open%20problem.%20We%20propose%20unified%20learning%2C%20a%20new%20paradigm%20that%0Aencodes%20each%20model%20into%20a%20graph%20representation%2C%20enabling%20unification%20in%20a%0Ashared%20graph%20learning%20space.%20A%20GNN%20then%20guides%20optimization%20of%20these%20unified%0Amodels.%20By%20decoupling%20parameters%20of%20individual%20models%20and%20controlling%20them%0Athrough%20a%20unified%20GNN%20%28uGNN%29%2C%20our%20method%20supports%20parameter%20sharing%20and%0Aknowledge%20transfer%20across%20varying%20architectures%20%28MLPs%2C%20CNNs%2C%20GNNs%29%20and%0Adistributions%2C%20improving%20generalizability.%20Evaluations%20on%20MorphoMNIST%20and%20two%0AMedMNIST%20benchmarks%20-%20PneumoniaMNIST%20and%20BreastMNIST%20-%20show%20that%20unified%0Alearning%20boosts%20performance%20when%20models%20are%20trained%20on%20unique%20distributions%20and%0Atested%20on%20mixed%20ones%2C%20demonstrating%20strong%20robustness%20to%20unseen%20data%20with%20large%0Adistribution%20shifts.%20Code%20and%20benchmarks%3A%20https%3A//github.com/basiralab/uGNN%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGNN-based%2520Unified%2520Deep%2520Learning%26entry.906535625%3DFurkan%2520Pala%2520and%2520Islem%2520Rekik%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520often%2520struggle%2520to%2520maintain%2520generalizability%2520in%2520medical%250Aimaging%252C%2520particularly%2520under%2520domain-fracture%2520scenarios%2520where%2520distribution%2520shifts%250Aarise%2520from%2520varying%2520imaging%2520techniques%252C%2520acquisition%2520protocols%252C%2520patient%250Apopulations%252C%2520demographics%252C%2520and%2520equipment.%2520In%2520practice%252C%2520each%2520hospital%2520may%2520need%250Ato%2520train%2520distinct%2520models%2520-%2520differing%2520in%2520learning%2520task%252C%2520width%252C%2520and%2520depth%2520-%2520to%250Amatch%2520local%2520data.%2520For%2520example%252C%2520one%2520hospital%2520may%2520use%2520Euclidean%2520architectures%250Asuch%2520as%2520MLPs%2520and%2520CNNs%2520for%2520tabular%2520or%2520grid-like%2520image%2520data%252C%2520while%2520another%2520may%250Arequire%2520non-Euclidean%2520architectures%2520such%2520as%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520for%250Airregular%2520data%2520like%2520brain%2520connectomes.%2520How%2520to%2520train%2520such%2520heterogeneous%2520models%250Acoherently%2520across%2520datasets%252C%2520while%2520enhancing%2520each%2520model%2527s%2520generalizability%252C%250Aremains%2520an%2520open%2520problem.%2520We%2520propose%2520unified%2520learning%252C%2520a%2520new%2520paradigm%2520that%250Aencodes%2520each%2520model%2520into%2520a%2520graph%2520representation%252C%2520enabling%2520unification%2520in%2520a%250Ashared%2520graph%2520learning%2520space.%2520A%2520GNN%2520then%2520guides%2520optimization%2520of%2520these%2520unified%250Amodels.%2520By%2520decoupling%2520parameters%2520of%2520individual%2520models%2520and%2520controlling%2520them%250Athrough%2520a%2520unified%2520GNN%2520%2528uGNN%2529%252C%2520our%2520method%2520supports%2520parameter%2520sharing%2520and%250Aknowledge%2520transfer%2520across%2520varying%2520architectures%2520%2528MLPs%252C%2520CNNs%252C%2520GNNs%2529%2520and%250Adistributions%252C%2520improving%2520generalizability.%2520Evaluations%2520on%2520MorphoMNIST%2520and%2520two%250AMedMNIST%2520benchmarks%2520-%2520PneumoniaMNIST%2520and%2520BreastMNIST%2520-%2520show%2520that%2520unified%250Alearning%2520boosts%2520performance%2520when%2520models%2520are%2520trained%2520on%2520unique%2520distributions%2520and%250Atested%2520on%2520mixed%2520ones%252C%2520demonstrating%2520strong%2520robustness%2520to%2520unseen%2520data%2520with%2520large%250Adistribution%2520shifts.%2520Code%2520and%2520benchmarks%253A%2520https%253A//github.com/basiralab/uGNN%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GNN-based%20Unified%20Deep%20Learning&entry.906535625=Furkan%20Pala%20and%20Islem%20Rekik&entry.1292438233=%20%20Deep%20learning%20models%20often%20struggle%20to%20maintain%20generalizability%20in%20medical%0Aimaging%2C%20particularly%20under%20domain-fracture%20scenarios%20where%20distribution%20shifts%0Aarise%20from%20varying%20imaging%20techniques%2C%20acquisition%20protocols%2C%20patient%0Apopulations%2C%20demographics%2C%20and%20equipment.%20In%20practice%2C%20each%20hospital%20may%20need%0Ato%20train%20distinct%20models%20-%20differing%20in%20learning%20task%2C%20width%2C%20and%20depth%20-%20to%0Amatch%20local%20data.%20For%20example%2C%20one%20hospital%20may%20use%20Euclidean%20architectures%0Asuch%20as%20MLPs%20and%20CNNs%20for%20tabular%20or%20grid-like%20image%20data%2C%20while%20another%20may%0Arequire%20non-Euclidean%20architectures%20such%20as%20graph%20neural%20networks%20%28GNNs%29%20for%0Airregular%20data%20like%20brain%20connectomes.%20How%20to%20train%20such%20heterogeneous%20models%0Acoherently%20across%20datasets%2C%20while%20enhancing%20each%20model%27s%20generalizability%2C%0Aremains%20an%20open%20problem.%20We%20propose%20unified%20learning%2C%20a%20new%20paradigm%20that%0Aencodes%20each%20model%20into%20a%20graph%20representation%2C%20enabling%20unification%20in%20a%0Ashared%20graph%20learning%20space.%20A%20GNN%20then%20guides%20optimization%20of%20these%20unified%0Amodels.%20By%20decoupling%20parameters%20of%20individual%20models%20and%20controlling%20them%0Athrough%20a%20unified%20GNN%20%28uGNN%29%2C%20our%20method%20supports%20parameter%20sharing%20and%0Aknowledge%20transfer%20across%20varying%20architectures%20%28MLPs%2C%20CNNs%2C%20GNNs%29%20and%0Adistributions%2C%20improving%20generalizability.%20Evaluations%20on%20MorphoMNIST%20and%20two%0AMedMNIST%20benchmarks%20-%20PneumoniaMNIST%20and%20BreastMNIST%20-%20show%20that%20unified%0Alearning%20boosts%20performance%20when%20models%20are%20trained%20on%20unique%20distributions%20and%0Atested%20on%20mixed%20ones%2C%20demonstrating%20strong%20robustness%20to%20unseen%20data%20with%20large%0Adistribution%20shifts.%20Code%20and%20benchmarks%3A%20https%3A//github.com/basiralab/uGNN%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10583v1&entry.124074799=Read"},
{"title": "Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote\n  Sensing Change Detection", "author": "Humza Naveed and Xina Zeng and Mitch Bryson and Nagita Mehrseresht", "abstract": "  Foundational models have achieved significant success in diverse domains of\ncomputer vision. They learn general representations that are easily\ntransferable to tasks not seen during training. One such foundational model is\nSegment anything model (SAM), which can accurately segment objects in images.\nWe propose adapting the SAM encoder via fine-tuning for remote sensing change\ndetection (RSCD) along with spatial-temporal feature enhancement (STFE) and\nmulti-scale decoder fusion (MSDF) to detect changes robustly at multiple\nscales. Additionally, we propose a novel cross-entropy masking (CEM) loss to\nhandle high class imbalance in change detection datasets. Our method\noutperforms state-of-the-art (SOTA) methods on four change detection datasets,\nLevir-CD, WHU-CD, CLCD, and S2Looking. We achieved 2.5% F1-score improvement on\na large complex S2Looking dataset. The code is available at:\nhttps://github.com/humza909/SAM-CEM-CD\n", "link": "http://arxiv.org/abs/2508.10568v1", "date": "2025-08-14", "relevancy": 2.7898, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5802}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5582}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20SAM%20via%20Cross-Entropy%20Masking%20for%20Class%20Imbalance%20in%20Remote%0A%20%20Sensing%20Change%20Detection&body=Title%3A%20Adapting%20SAM%20via%20Cross-Entropy%20Masking%20for%20Class%20Imbalance%20in%20Remote%0A%20%20Sensing%20Change%20Detection%0AAuthor%3A%20Humza%20Naveed%20and%20Xina%20Zeng%20and%20Mitch%20Bryson%20and%20Nagita%20Mehrseresht%0AAbstract%3A%20%20%20Foundational%20models%20have%20achieved%20significant%20success%20in%20diverse%20domains%20of%0Acomputer%20vision.%20They%20learn%20general%20representations%20that%20are%20easily%0Atransferable%20to%20tasks%20not%20seen%20during%20training.%20One%20such%20foundational%20model%20is%0ASegment%20anything%20model%20%28SAM%29%2C%20which%20can%20accurately%20segment%20objects%20in%20images.%0AWe%20propose%20adapting%20the%20SAM%20encoder%20via%20fine-tuning%20for%20remote%20sensing%20change%0Adetection%20%28RSCD%29%20along%20with%20spatial-temporal%20feature%20enhancement%20%28STFE%29%20and%0Amulti-scale%20decoder%20fusion%20%28MSDF%29%20to%20detect%20changes%20robustly%20at%20multiple%0Ascales.%20Additionally%2C%20we%20propose%20a%20novel%20cross-entropy%20masking%20%28CEM%29%20loss%20to%0Ahandle%20high%20class%20imbalance%20in%20change%20detection%20datasets.%20Our%20method%0Aoutperforms%20state-of-the-art%20%28SOTA%29%20methods%20on%20four%20change%20detection%20datasets%2C%0ALevir-CD%2C%20WHU-CD%2C%20CLCD%2C%20and%20S2Looking.%20We%20achieved%202.5%25%20F1-score%20improvement%20on%0Aa%20large%20complex%20S2Looking%20dataset.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/humza909/SAM-CEM-CD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520SAM%2520via%2520Cross-Entropy%2520Masking%2520for%2520Class%2520Imbalance%2520in%2520Remote%250A%2520%2520Sensing%2520Change%2520Detection%26entry.906535625%3DHumza%2520Naveed%2520and%2520Xina%2520Zeng%2520and%2520Mitch%2520Bryson%2520and%2520Nagita%2520Mehrseresht%26entry.1292438233%3D%2520%2520Foundational%2520models%2520have%2520achieved%2520significant%2520success%2520in%2520diverse%2520domains%2520of%250Acomputer%2520vision.%2520They%2520learn%2520general%2520representations%2520that%2520are%2520easily%250Atransferable%2520to%2520tasks%2520not%2520seen%2520during%2520training.%2520One%2520such%2520foundational%2520model%2520is%250ASegment%2520anything%2520model%2520%2528SAM%2529%252C%2520which%2520can%2520accurately%2520segment%2520objects%2520in%2520images.%250AWe%2520propose%2520adapting%2520the%2520SAM%2520encoder%2520via%2520fine-tuning%2520for%2520remote%2520sensing%2520change%250Adetection%2520%2528RSCD%2529%2520along%2520with%2520spatial-temporal%2520feature%2520enhancement%2520%2528STFE%2529%2520and%250Amulti-scale%2520decoder%2520fusion%2520%2528MSDF%2529%2520to%2520detect%2520changes%2520robustly%2520at%2520multiple%250Ascales.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%2520cross-entropy%2520masking%2520%2528CEM%2529%2520loss%2520to%250Ahandle%2520high%2520class%2520imbalance%2520in%2520change%2520detection%2520datasets.%2520Our%2520method%250Aoutperforms%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520on%2520four%2520change%2520detection%2520datasets%252C%250ALevir-CD%252C%2520WHU-CD%252C%2520CLCD%252C%2520and%2520S2Looking.%2520We%2520achieved%25202.5%2525%2520F1-score%2520improvement%2520on%250Aa%2520large%2520complex%2520S2Looking%2520dataset.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/humza909/SAM-CEM-CD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20SAM%20via%20Cross-Entropy%20Masking%20for%20Class%20Imbalance%20in%20Remote%0A%20%20Sensing%20Change%20Detection&entry.906535625=Humza%20Naveed%20and%20Xina%20Zeng%20and%20Mitch%20Bryson%20and%20Nagita%20Mehrseresht&entry.1292438233=%20%20Foundational%20models%20have%20achieved%20significant%20success%20in%20diverse%20domains%20of%0Acomputer%20vision.%20They%20learn%20general%20representations%20that%20are%20easily%0Atransferable%20to%20tasks%20not%20seen%20during%20training.%20One%20such%20foundational%20model%20is%0ASegment%20anything%20model%20%28SAM%29%2C%20which%20can%20accurately%20segment%20objects%20in%20images.%0AWe%20propose%20adapting%20the%20SAM%20encoder%20via%20fine-tuning%20for%20remote%20sensing%20change%0Adetection%20%28RSCD%29%20along%20with%20spatial-temporal%20feature%20enhancement%20%28STFE%29%20and%0Amulti-scale%20decoder%20fusion%20%28MSDF%29%20to%20detect%20changes%20robustly%20at%20multiple%0Ascales.%20Additionally%2C%20we%20propose%20a%20novel%20cross-entropy%20masking%20%28CEM%29%20loss%20to%0Ahandle%20high%20class%20imbalance%20in%20change%20detection%20datasets.%20Our%20method%0Aoutperforms%20state-of-the-art%20%28SOTA%29%20methods%20on%20four%20change%20detection%20datasets%2C%0ALevir-CD%2C%20WHU-CD%2C%20CLCD%2C%20and%20S2Looking.%20We%20achieved%202.5%25%20F1-score%20improvement%20on%0Aa%20large%20complex%20S2Looking%20dataset.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/humza909/SAM-CEM-CD%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10568v1&entry.124074799=Read"},
{"title": "TexVerse: A Universe of 3D Objects with High-Resolution Textures", "author": "Yibo Zhang and Li Zhang and Rui Ma and Nan Cao", "abstract": "  We introduce TexVerse, a large-scale 3D dataset featuring high-resolution\ntextures. While recent advances in large-scale 3D datasets have enhanced\nhigh-resolution geometry generation, creating high-resolution textures\nend-to-end remains underexplored due to the lack of suitable datasets. TexVerse\nfills this gap with a curated collection of over 858K unique high-resolution 3D\nmodels sourced from Sketchfab, including more than 158K models with physically\nbased rendering (PBR) materials. Each model encompasses all of its\nhigh-resolution variants, bringing the total to 1.6M 3D instances. TexVerse\nalso includes specialized subsets: TexVerse-Skeleton, with 69K rigged models,\nand TexVerse-Animation, with 54K animated models, both preserving original\nskeleton and animation data uploaded by the user. We also provide detailed\nmodel annotations describing overall characteristics, structural components,\nand intricate features. TexVerse offers a high-quality data resource with\nwide-ranging potential applications in texture synthesis, PBR material\ndevelopment, animation, and various 3D vision and graphics tasks.\n", "link": "http://arxiv.org/abs/2508.10868v1", "date": "2025-08-14", "relevancy": 2.7867, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5762}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5762}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TexVerse%3A%20A%20Universe%20of%203D%20Objects%20with%20High-Resolution%20Textures&body=Title%3A%20TexVerse%3A%20A%20Universe%20of%203D%20Objects%20with%20High-Resolution%20Textures%0AAuthor%3A%20Yibo%20Zhang%20and%20Li%20Zhang%20and%20Rui%20Ma%20and%20Nan%20Cao%0AAbstract%3A%20%20%20We%20introduce%20TexVerse%2C%20a%20large-scale%203D%20dataset%20featuring%20high-resolution%0Atextures.%20While%20recent%20advances%20in%20large-scale%203D%20datasets%20have%20enhanced%0Ahigh-resolution%20geometry%20generation%2C%20creating%20high-resolution%20textures%0Aend-to-end%20remains%20underexplored%20due%20to%20the%20lack%20of%20suitable%20datasets.%20TexVerse%0Afills%20this%20gap%20with%20a%20curated%20collection%20of%20over%20858K%20unique%20high-resolution%203D%0Amodels%20sourced%20from%20Sketchfab%2C%20including%20more%20than%20158K%20models%20with%20physically%0Abased%20rendering%20%28PBR%29%20materials.%20Each%20model%20encompasses%20all%20of%20its%0Ahigh-resolution%20variants%2C%20bringing%20the%20total%20to%201.6M%203D%20instances.%20TexVerse%0Aalso%20includes%20specialized%20subsets%3A%20TexVerse-Skeleton%2C%20with%2069K%20rigged%20models%2C%0Aand%20TexVerse-Animation%2C%20with%2054K%20animated%20models%2C%20both%20preserving%20original%0Askeleton%20and%20animation%20data%20uploaded%20by%20the%20user.%20We%20also%20provide%20detailed%0Amodel%20annotations%20describing%20overall%20characteristics%2C%20structural%20components%2C%0Aand%20intricate%20features.%20TexVerse%20offers%20a%20high-quality%20data%20resource%20with%0Awide-ranging%20potential%20applications%20in%20texture%20synthesis%2C%20PBR%20material%0Adevelopment%2C%20animation%2C%20and%20various%203D%20vision%20and%20graphics%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTexVerse%253A%2520A%2520Universe%2520of%25203D%2520Objects%2520with%2520High-Resolution%2520Textures%26entry.906535625%3DYibo%2520Zhang%2520and%2520Li%2520Zhang%2520and%2520Rui%2520Ma%2520and%2520Nan%2520Cao%26entry.1292438233%3D%2520%2520We%2520introduce%2520TexVerse%252C%2520a%2520large-scale%25203D%2520dataset%2520featuring%2520high-resolution%250Atextures.%2520While%2520recent%2520advances%2520in%2520large-scale%25203D%2520datasets%2520have%2520enhanced%250Ahigh-resolution%2520geometry%2520generation%252C%2520creating%2520high-resolution%2520textures%250Aend-to-end%2520remains%2520underexplored%2520due%2520to%2520the%2520lack%2520of%2520suitable%2520datasets.%2520TexVerse%250Afills%2520this%2520gap%2520with%2520a%2520curated%2520collection%2520of%2520over%2520858K%2520unique%2520high-resolution%25203D%250Amodels%2520sourced%2520from%2520Sketchfab%252C%2520including%2520more%2520than%2520158K%2520models%2520with%2520physically%250Abased%2520rendering%2520%2528PBR%2529%2520materials.%2520Each%2520model%2520encompasses%2520all%2520of%2520its%250Ahigh-resolution%2520variants%252C%2520bringing%2520the%2520total%2520to%25201.6M%25203D%2520instances.%2520TexVerse%250Aalso%2520includes%2520specialized%2520subsets%253A%2520TexVerse-Skeleton%252C%2520with%252069K%2520rigged%2520models%252C%250Aand%2520TexVerse-Animation%252C%2520with%252054K%2520animated%2520models%252C%2520both%2520preserving%2520original%250Askeleton%2520and%2520animation%2520data%2520uploaded%2520by%2520the%2520user.%2520We%2520also%2520provide%2520detailed%250Amodel%2520annotations%2520describing%2520overall%2520characteristics%252C%2520structural%2520components%252C%250Aand%2520intricate%2520features.%2520TexVerse%2520offers%2520a%2520high-quality%2520data%2520resource%2520with%250Awide-ranging%2520potential%2520applications%2520in%2520texture%2520synthesis%252C%2520PBR%2520material%250Adevelopment%252C%2520animation%252C%2520and%2520various%25203D%2520vision%2520and%2520graphics%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TexVerse%3A%20A%20Universe%20of%203D%20Objects%20with%20High-Resolution%20Textures&entry.906535625=Yibo%20Zhang%20and%20Li%20Zhang%20and%20Rui%20Ma%20and%20Nan%20Cao&entry.1292438233=%20%20We%20introduce%20TexVerse%2C%20a%20large-scale%203D%20dataset%20featuring%20high-resolution%0Atextures.%20While%20recent%20advances%20in%20large-scale%203D%20datasets%20have%20enhanced%0Ahigh-resolution%20geometry%20generation%2C%20creating%20high-resolution%20textures%0Aend-to-end%20remains%20underexplored%20due%20to%20the%20lack%20of%20suitable%20datasets.%20TexVerse%0Afills%20this%20gap%20with%20a%20curated%20collection%20of%20over%20858K%20unique%20high-resolution%203D%0Amodels%20sourced%20from%20Sketchfab%2C%20including%20more%20than%20158K%20models%20with%20physically%0Abased%20rendering%20%28PBR%29%20materials.%20Each%20model%20encompasses%20all%20of%20its%0Ahigh-resolution%20variants%2C%20bringing%20the%20total%20to%201.6M%203D%20instances.%20TexVerse%0Aalso%20includes%20specialized%20subsets%3A%20TexVerse-Skeleton%2C%20with%2069K%20rigged%20models%2C%0Aand%20TexVerse-Animation%2C%20with%2054K%20animated%20models%2C%20both%20preserving%20original%0Askeleton%20and%20animation%20data%20uploaded%20by%20the%20user.%20We%20also%20provide%20detailed%0Amodel%20annotations%20describing%20overall%20characteristics%2C%20structural%20components%2C%0Aand%20intricate%20features.%20TexVerse%20offers%20a%20high-quality%20data%20resource%20with%0Awide-ranging%20potential%20applications%20in%20texture%20synthesis%2C%20PBR%20material%0Adevelopment%2C%20animation%2C%20and%20various%203D%20vision%20and%20graphics%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10868v1&entry.124074799=Read"},
{"title": "SemPT: Semantic Prompt Tuning for Vision-Language Models", "author": "Xiao Shi and Yangjun Ou and Zhenzhong Chen", "abstract": "  Visual transfer learning for unseen categories presents an active research\ntopic yet a challenging task, due to the inherent conflict between preserving\ncategory-specific representations and acquiring transferable knowledge.\nVision-Language Models (VLMs) pre-trained on large amounts of image-text pairs\noffer a promising solution. However, existing prompt tuning methods rely on\nsparse category labels or disparate LLM-generated descriptions, which fragment\nknowledge representation and hinder transferability. To address this\nlimitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that\ntackles the generalization challenge by leveraging shared attribute-level\nknowledge across categories. Specifically, SemPT adopts a two-step prompting\nstrategy to guide LLM in extracting shared visual attributes and generating\nattribute-level descriptions, capturing transferable semantic cues beyond\nlabels while ensuring coherent structure. Then, visually guided weighting is\napplied to the embeddings of attribute-level descriptions to reduce noise from\nirrelevant attributes and enhance the text embeddings. Additionally, image\nembeddings are jointly aligned with both label and attribute-enhanced text\nembeddings, balancing discrimination for seen categories and transferability to\nunseen ones. Considering the availability of category exposure, our inference\ndynamically selects between standard label embeddings for seen categories and\nattribute-enhanced embeddings for unseen ones to ensure effective adaptation.\nExtensive experiments on 15 benchmark datasets demonstrate that SemPT achieves\nstate-of-the-art performance across various settings, including base-to-novel\ngeneralization, cross-dataset transfer, cross-domain transfer, and few-shot\nlearning.\n", "link": "http://arxiv.org/abs/2508.10645v1", "date": "2025-08-14", "relevancy": 2.7856, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5663}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SemPT%3A%20Semantic%20Prompt%20Tuning%20for%20Vision-Language%20Models&body=Title%3A%20SemPT%3A%20Semantic%20Prompt%20Tuning%20for%20Vision-Language%20Models%0AAuthor%3A%20Xiao%20Shi%20and%20Yangjun%20Ou%20and%20Zhenzhong%20Chen%0AAbstract%3A%20%20%20Visual%20transfer%20learning%20for%20unseen%20categories%20presents%20an%20active%20research%0Atopic%20yet%20a%20challenging%20task%2C%20due%20to%20the%20inherent%20conflict%20between%20preserving%0Acategory-specific%20representations%20and%20acquiring%20transferable%20knowledge.%0AVision-Language%20Models%20%28VLMs%29%20pre-trained%20on%20large%20amounts%20of%20image-text%20pairs%0Aoffer%20a%20promising%20solution.%20However%2C%20existing%20prompt%20tuning%20methods%20rely%20on%0Asparse%20category%20labels%20or%20disparate%20LLM-generated%20descriptions%2C%20which%20fragment%0Aknowledge%20representation%20and%20hinder%20transferability.%20To%20address%20this%0Alimitation%2C%20we%20introduce%20Semantic%20Prompt%20Tuning%20%28SemPT%29%2C%20a%20novel%20framework%20that%0Atackles%20the%20generalization%20challenge%20by%20leveraging%20shared%20attribute-level%0Aknowledge%20across%20categories.%20Specifically%2C%20SemPT%20adopts%20a%20two-step%20prompting%0Astrategy%20to%20guide%20LLM%20in%20extracting%20shared%20visual%20attributes%20and%20generating%0Aattribute-level%20descriptions%2C%20capturing%20transferable%20semantic%20cues%20beyond%0Alabels%20while%20ensuring%20coherent%20structure.%20Then%2C%20visually%20guided%20weighting%20is%0Aapplied%20to%20the%20embeddings%20of%20attribute-level%20descriptions%20to%20reduce%20noise%20from%0Airrelevant%20attributes%20and%20enhance%20the%20text%20embeddings.%20Additionally%2C%20image%0Aembeddings%20are%20jointly%20aligned%20with%20both%20label%20and%20attribute-enhanced%20text%0Aembeddings%2C%20balancing%20discrimination%20for%20seen%20categories%20and%20transferability%20to%0Aunseen%20ones.%20Considering%20the%20availability%20of%20category%20exposure%2C%20our%20inference%0Adynamically%20selects%20between%20standard%20label%20embeddings%20for%20seen%20categories%20and%0Aattribute-enhanced%20embeddings%20for%20unseen%20ones%20to%20ensure%20effective%20adaptation.%0AExtensive%20experiments%20on%2015%20benchmark%20datasets%20demonstrate%20that%20SemPT%20achieves%0Astate-of-the-art%20performance%20across%20various%20settings%2C%20including%20base-to-novel%0Ageneralization%2C%20cross-dataset%20transfer%2C%20cross-domain%20transfer%2C%20and%20few-shot%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemPT%253A%2520Semantic%2520Prompt%2520Tuning%2520for%2520Vision-Language%2520Models%26entry.906535625%3DXiao%2520Shi%2520and%2520Yangjun%2520Ou%2520and%2520Zhenzhong%2520Chen%26entry.1292438233%3D%2520%2520Visual%2520transfer%2520learning%2520for%2520unseen%2520categories%2520presents%2520an%2520active%2520research%250Atopic%2520yet%2520a%2520challenging%2520task%252C%2520due%2520to%2520the%2520inherent%2520conflict%2520between%2520preserving%250Acategory-specific%2520representations%2520and%2520acquiring%2520transferable%2520knowledge.%250AVision-Language%2520Models%2520%2528VLMs%2529%2520pre-trained%2520on%2520large%2520amounts%2520of%2520image-text%2520pairs%250Aoffer%2520a%2520promising%2520solution.%2520However%252C%2520existing%2520prompt%2520tuning%2520methods%2520rely%2520on%250Asparse%2520category%2520labels%2520or%2520disparate%2520LLM-generated%2520descriptions%252C%2520which%2520fragment%250Aknowledge%2520representation%2520and%2520hinder%2520transferability.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520introduce%2520Semantic%2520Prompt%2520Tuning%2520%2528SemPT%2529%252C%2520a%2520novel%2520framework%2520that%250Atackles%2520the%2520generalization%2520challenge%2520by%2520leveraging%2520shared%2520attribute-level%250Aknowledge%2520across%2520categories.%2520Specifically%252C%2520SemPT%2520adopts%2520a%2520two-step%2520prompting%250Astrategy%2520to%2520guide%2520LLM%2520in%2520extracting%2520shared%2520visual%2520attributes%2520and%2520generating%250Aattribute-level%2520descriptions%252C%2520capturing%2520transferable%2520semantic%2520cues%2520beyond%250Alabels%2520while%2520ensuring%2520coherent%2520structure.%2520Then%252C%2520visually%2520guided%2520weighting%2520is%250Aapplied%2520to%2520the%2520embeddings%2520of%2520attribute-level%2520descriptions%2520to%2520reduce%2520noise%2520from%250Airrelevant%2520attributes%2520and%2520enhance%2520the%2520text%2520embeddings.%2520Additionally%252C%2520image%250Aembeddings%2520are%2520jointly%2520aligned%2520with%2520both%2520label%2520and%2520attribute-enhanced%2520text%250Aembeddings%252C%2520balancing%2520discrimination%2520for%2520seen%2520categories%2520and%2520transferability%2520to%250Aunseen%2520ones.%2520Considering%2520the%2520availability%2520of%2520category%2520exposure%252C%2520our%2520inference%250Adynamically%2520selects%2520between%2520standard%2520label%2520embeddings%2520for%2520seen%2520categories%2520and%250Aattribute-enhanced%2520embeddings%2520for%2520unseen%2520ones%2520to%2520ensure%2520effective%2520adaptation.%250AExtensive%2520experiments%2520on%252015%2520benchmark%2520datasets%2520demonstrate%2520that%2520SemPT%2520achieves%250Astate-of-the-art%2520performance%2520across%2520various%2520settings%252C%2520including%2520base-to-novel%250Ageneralization%252C%2520cross-dataset%2520transfer%252C%2520cross-domain%2520transfer%252C%2520and%2520few-shot%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemPT%3A%20Semantic%20Prompt%20Tuning%20for%20Vision-Language%20Models&entry.906535625=Xiao%20Shi%20and%20Yangjun%20Ou%20and%20Zhenzhong%20Chen&entry.1292438233=%20%20Visual%20transfer%20learning%20for%20unseen%20categories%20presents%20an%20active%20research%0Atopic%20yet%20a%20challenging%20task%2C%20due%20to%20the%20inherent%20conflict%20between%20preserving%0Acategory-specific%20representations%20and%20acquiring%20transferable%20knowledge.%0AVision-Language%20Models%20%28VLMs%29%20pre-trained%20on%20large%20amounts%20of%20image-text%20pairs%0Aoffer%20a%20promising%20solution.%20However%2C%20existing%20prompt%20tuning%20methods%20rely%20on%0Asparse%20category%20labels%20or%20disparate%20LLM-generated%20descriptions%2C%20which%20fragment%0Aknowledge%20representation%20and%20hinder%20transferability.%20To%20address%20this%0Alimitation%2C%20we%20introduce%20Semantic%20Prompt%20Tuning%20%28SemPT%29%2C%20a%20novel%20framework%20that%0Atackles%20the%20generalization%20challenge%20by%20leveraging%20shared%20attribute-level%0Aknowledge%20across%20categories.%20Specifically%2C%20SemPT%20adopts%20a%20two-step%20prompting%0Astrategy%20to%20guide%20LLM%20in%20extracting%20shared%20visual%20attributes%20and%20generating%0Aattribute-level%20descriptions%2C%20capturing%20transferable%20semantic%20cues%20beyond%0Alabels%20while%20ensuring%20coherent%20structure.%20Then%2C%20visually%20guided%20weighting%20is%0Aapplied%20to%20the%20embeddings%20of%20attribute-level%20descriptions%20to%20reduce%20noise%20from%0Airrelevant%20attributes%20and%20enhance%20the%20text%20embeddings.%20Additionally%2C%20image%0Aembeddings%20are%20jointly%20aligned%20with%20both%20label%20and%20attribute-enhanced%20text%0Aembeddings%2C%20balancing%20discrimination%20for%20seen%20categories%20and%20transferability%20to%0Aunseen%20ones.%20Considering%20the%20availability%20of%20category%20exposure%2C%20our%20inference%0Adynamically%20selects%20between%20standard%20label%20embeddings%20for%20seen%20categories%20and%0Aattribute-enhanced%20embeddings%20for%20unseen%20ones%20to%20ensure%20effective%20adaptation.%0AExtensive%20experiments%20on%2015%20benchmark%20datasets%20demonstrate%20that%20SemPT%20achieves%0Astate-of-the-art%20performance%20across%20various%20settings%2C%20including%20base-to-novel%0Ageneralization%2C%20cross-dataset%20transfer%2C%20cross-domain%20transfer%2C%20and%20few-shot%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10645v1&entry.124074799=Read"},
{"title": "TD3Net: A temporal densely connected multi-dilated convolutional network\n  for lipreading", "author": "Byung Hoon Lee and Wooseok Shin and Sung Won Han", "abstract": "  The word-level lipreading approach typically employs a two-stage framework\nwith separate frontend and backend architectures to model dynamic lip\nmovements. Each component has been extensively studied, and in the backend\narchitecture, temporal convolutional networks (TCNs) have been widely adopted\nin state-of-the-art methods. Recently, dense skip connections have been\nintroduced in TCNs to mitigate the limited density of the receptive field,\nthereby improving the modeling of complex temporal representations. However,\ntheir performance remains constrained owing to potential information loss\nregarding the continuous nature of lip movements, caused by blind spots in the\nreceptive field. To address this limitation, we propose TD3Net, a temporal\ndensely connected multi-dilated convolutional network that combines dense skip\nconnections and multi-dilated temporal convolutions as the backend\narchitecture. TD3Net covers a wide and dense receptive field without blind\nspots by applying different dilation factors to skip-connected features.\nExperimental results on a word-level lipreading task using two large publicly\navailable datasets, Lip Reading in the Wild (LRW) and LRW-1000, indicate that\nthe proposed method achieves performance comparable to state-of-the-art\nmethods. It achieved higher accuracy with fewer parameters and lower\nfloating-point operations compared to existing TCN-based backend architectures.\nMoreover, visualization results suggest that our approach effectively utilizes\ndiverse temporal features while preserving temporal continuity, presenting\nnotable advantages in lipreading systems. The code is available at our GitHub\nrepository (https://github.com/Leebh-kor/TD3Net).\n", "link": "http://arxiv.org/abs/2506.16073v3", "date": "2025-08-14", "relevancy": 2.7837, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5982}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5473}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TD3Net%3A%20A%20temporal%20densely%20connected%20multi-dilated%20convolutional%20network%0A%20%20for%20lipreading&body=Title%3A%20TD3Net%3A%20A%20temporal%20densely%20connected%20multi-dilated%20convolutional%20network%0A%20%20for%20lipreading%0AAuthor%3A%20Byung%20Hoon%20Lee%20and%20Wooseok%20Shin%20and%20Sung%20Won%20Han%0AAbstract%3A%20%20%20The%20word-level%20lipreading%20approach%20typically%20employs%20a%20two-stage%20framework%0Awith%20separate%20frontend%20and%20backend%20architectures%20to%20model%20dynamic%20lip%0Amovements.%20Each%20component%20has%20been%20extensively%20studied%2C%20and%20in%20the%20backend%0Aarchitecture%2C%20temporal%20convolutional%20networks%20%28TCNs%29%20have%20been%20widely%20adopted%0Ain%20state-of-the-art%20methods.%20Recently%2C%20dense%20skip%20connections%20have%20been%0Aintroduced%20in%20TCNs%20to%20mitigate%20the%20limited%20density%20of%20the%20receptive%20field%2C%0Athereby%20improving%20the%20modeling%20of%20complex%20temporal%20representations.%20However%2C%0Atheir%20performance%20remains%20constrained%20owing%20to%20potential%20information%20loss%0Aregarding%20the%20continuous%20nature%20of%20lip%20movements%2C%20caused%20by%20blind%20spots%20in%20the%0Areceptive%20field.%20To%20address%20this%20limitation%2C%20we%20propose%20TD3Net%2C%20a%20temporal%0Adensely%20connected%20multi-dilated%20convolutional%20network%20that%20combines%20dense%20skip%0Aconnections%20and%20multi-dilated%20temporal%20convolutions%20as%20the%20backend%0Aarchitecture.%20TD3Net%20covers%20a%20wide%20and%20dense%20receptive%20field%20without%20blind%0Aspots%20by%20applying%20different%20dilation%20factors%20to%20skip-connected%20features.%0AExperimental%20results%20on%20a%20word-level%20lipreading%20task%20using%20two%20large%20publicly%0Aavailable%20datasets%2C%20Lip%20Reading%20in%20the%20Wild%20%28LRW%29%20and%20LRW-1000%2C%20indicate%20that%0Athe%20proposed%20method%20achieves%20performance%20comparable%20to%20state-of-the-art%0Amethods.%20It%20achieved%20higher%20accuracy%20with%20fewer%20parameters%20and%20lower%0Afloating-point%20operations%20compared%20to%20existing%20TCN-based%20backend%20architectures.%0AMoreover%2C%20visualization%20results%20suggest%20that%20our%20approach%20effectively%20utilizes%0Adiverse%20temporal%20features%20while%20preserving%20temporal%20continuity%2C%20presenting%0Anotable%20advantages%20in%20lipreading%20systems.%20The%20code%20is%20available%20at%20our%20GitHub%0Arepository%20%28https%3A//github.com/Leebh-kor/TD3Net%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.16073v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTD3Net%253A%2520A%2520temporal%2520densely%2520connected%2520multi-dilated%2520convolutional%2520network%250A%2520%2520for%2520lipreading%26entry.906535625%3DByung%2520Hoon%2520Lee%2520and%2520Wooseok%2520Shin%2520and%2520Sung%2520Won%2520Han%26entry.1292438233%3D%2520%2520The%2520word-level%2520lipreading%2520approach%2520typically%2520employs%2520a%2520two-stage%2520framework%250Awith%2520separate%2520frontend%2520and%2520backend%2520architectures%2520to%2520model%2520dynamic%2520lip%250Amovements.%2520Each%2520component%2520has%2520been%2520extensively%2520studied%252C%2520and%2520in%2520the%2520backend%250Aarchitecture%252C%2520temporal%2520convolutional%2520networks%2520%2528TCNs%2529%2520have%2520been%2520widely%2520adopted%250Ain%2520state-of-the-art%2520methods.%2520Recently%252C%2520dense%2520skip%2520connections%2520have%2520been%250Aintroduced%2520in%2520TCNs%2520to%2520mitigate%2520the%2520limited%2520density%2520of%2520the%2520receptive%2520field%252C%250Athereby%2520improving%2520the%2520modeling%2520of%2520complex%2520temporal%2520representations.%2520However%252C%250Atheir%2520performance%2520remains%2520constrained%2520owing%2520to%2520potential%2520information%2520loss%250Aregarding%2520the%2520continuous%2520nature%2520of%2520lip%2520movements%252C%2520caused%2520by%2520blind%2520spots%2520in%2520the%250Areceptive%2520field.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520TD3Net%252C%2520a%2520temporal%250Adensely%2520connected%2520multi-dilated%2520convolutional%2520network%2520that%2520combines%2520dense%2520skip%250Aconnections%2520and%2520multi-dilated%2520temporal%2520convolutions%2520as%2520the%2520backend%250Aarchitecture.%2520TD3Net%2520covers%2520a%2520wide%2520and%2520dense%2520receptive%2520field%2520without%2520blind%250Aspots%2520by%2520applying%2520different%2520dilation%2520factors%2520to%2520skip-connected%2520features.%250AExperimental%2520results%2520on%2520a%2520word-level%2520lipreading%2520task%2520using%2520two%2520large%2520publicly%250Aavailable%2520datasets%252C%2520Lip%2520Reading%2520in%2520the%2520Wild%2520%2528LRW%2529%2520and%2520LRW-1000%252C%2520indicate%2520that%250Athe%2520proposed%2520method%2520achieves%2520performance%2520comparable%2520to%2520state-of-the-art%250Amethods.%2520It%2520achieved%2520higher%2520accuracy%2520with%2520fewer%2520parameters%2520and%2520lower%250Afloating-point%2520operations%2520compared%2520to%2520existing%2520TCN-based%2520backend%2520architectures.%250AMoreover%252C%2520visualization%2520results%2520suggest%2520that%2520our%2520approach%2520effectively%2520utilizes%250Adiverse%2520temporal%2520features%2520while%2520preserving%2520temporal%2520continuity%252C%2520presenting%250Anotable%2520advantages%2520in%2520lipreading%2520systems.%2520The%2520code%2520is%2520available%2520at%2520our%2520GitHub%250Arepository%2520%2528https%253A//github.com/Leebh-kor/TD3Net%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.16073v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TD3Net%3A%20A%20temporal%20densely%20connected%20multi-dilated%20convolutional%20network%0A%20%20for%20lipreading&entry.906535625=Byung%20Hoon%20Lee%20and%20Wooseok%20Shin%20and%20Sung%20Won%20Han&entry.1292438233=%20%20The%20word-level%20lipreading%20approach%20typically%20employs%20a%20two-stage%20framework%0Awith%20separate%20frontend%20and%20backend%20architectures%20to%20model%20dynamic%20lip%0Amovements.%20Each%20component%20has%20been%20extensively%20studied%2C%20and%20in%20the%20backend%0Aarchitecture%2C%20temporal%20convolutional%20networks%20%28TCNs%29%20have%20been%20widely%20adopted%0Ain%20state-of-the-art%20methods.%20Recently%2C%20dense%20skip%20connections%20have%20been%0Aintroduced%20in%20TCNs%20to%20mitigate%20the%20limited%20density%20of%20the%20receptive%20field%2C%0Athereby%20improving%20the%20modeling%20of%20complex%20temporal%20representations.%20However%2C%0Atheir%20performance%20remains%20constrained%20owing%20to%20potential%20information%20loss%0Aregarding%20the%20continuous%20nature%20of%20lip%20movements%2C%20caused%20by%20blind%20spots%20in%20the%0Areceptive%20field.%20To%20address%20this%20limitation%2C%20we%20propose%20TD3Net%2C%20a%20temporal%0Adensely%20connected%20multi-dilated%20convolutional%20network%20that%20combines%20dense%20skip%0Aconnections%20and%20multi-dilated%20temporal%20convolutions%20as%20the%20backend%0Aarchitecture.%20TD3Net%20covers%20a%20wide%20and%20dense%20receptive%20field%20without%20blind%0Aspots%20by%20applying%20different%20dilation%20factors%20to%20skip-connected%20features.%0AExperimental%20results%20on%20a%20word-level%20lipreading%20task%20using%20two%20large%20publicly%0Aavailable%20datasets%2C%20Lip%20Reading%20in%20the%20Wild%20%28LRW%29%20and%20LRW-1000%2C%20indicate%20that%0Athe%20proposed%20method%20achieves%20performance%20comparable%20to%20state-of-the-art%0Amethods.%20It%20achieved%20higher%20accuracy%20with%20fewer%20parameters%20and%20lower%0Afloating-point%20operations%20compared%20to%20existing%20TCN-based%20backend%20architectures.%0AMoreover%2C%20visualization%20results%20suggest%20that%20our%20approach%20effectively%20utilizes%0Adiverse%20temporal%20features%20while%20preserving%20temporal%20continuity%2C%20presenting%0Anotable%20advantages%20in%20lipreading%20systems.%20The%20code%20is%20available%20at%20our%20GitHub%0Arepository%20%28https%3A//github.com/Leebh-kor/TD3Net%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.16073v3&entry.124074799=Read"},
{"title": "AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video\n  Sequences", "author": "Jieyu Li and Xin Zhang and Joey Tianyi Zhou", "abstract": "  Recent advances in AI-generated content have fueled the rise of highly\nrealistic synthetic videos, posing severe risks to societal trust and digital\nintegrity. Existing benchmarks for video authenticity detection typically\nsuffer from limited realism, insufficient scale, and inadequate complexity,\nfailing to effectively evaluate modern vision-language models against\nsophisticated forgeries. To address this critical gap, we introduce AEGIS, a\nnovel large-scale benchmark explicitly targeting the detection of\nhyper-realistic and semantically nuanced AI-generated videos. AEGIS comprises\nover 10,000 rigorously curated real and synthetic videos generated by diverse,\nstate-of-the-art generative models, including Stable Video Diffusion,\nCogVideoX-5B, KLing, and Sora, encompassing open-source and proprietary\narchitectures. In particular, AEGIS features specially constructed challenging\nsubsets enhanced with robustness evaluation. Furthermore, we provide multimodal\nannotations spanning Semantic-Authenticity Descriptions, Motion Features, and\nLow-level Visual Features, facilitating authenticity detection and supporting\ndownstream tasks such as multimodal fusion and forgery localization. Extensive\nexperiments using advanced vision-language models demonstrate limited detection\ncapabilities on the most challenging subsets of AEGIS, highlighting the\ndataset's unique complexity and realism beyond the current generalization\ncapabilities of existing models. In essence, AEGIS establishes an indispensable\nevaluation benchmark, fundamentally advancing research toward developing\ngenuinely robust, reliable, broadly generalizable video authenticity detection\nmethodologies capable of addressing real-world forgery threats. Our dataset is\navailable on https://huggingface.co/datasets/Clarifiedfish/AEGIS.\n", "link": "http://arxiv.org/abs/2508.10771v1", "date": "2025-08-14", "relevancy": 2.7809, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5913}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5409}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AEGIS%3A%20Authenticity%20Evaluation%20Benchmark%20for%20AI-Generated%20Video%0A%20%20Sequences&body=Title%3A%20AEGIS%3A%20Authenticity%20Evaluation%20Benchmark%20for%20AI-Generated%20Video%0A%20%20Sequences%0AAuthor%3A%20Jieyu%20Li%20and%20Xin%20Zhang%20and%20Joey%20Tianyi%20Zhou%0AAbstract%3A%20%20%20Recent%20advances%20in%20AI-generated%20content%20have%20fueled%20the%20rise%20of%20highly%0Arealistic%20synthetic%20videos%2C%20posing%20severe%20risks%20to%20societal%20trust%20and%20digital%0Aintegrity.%20Existing%20benchmarks%20for%20video%20authenticity%20detection%20typically%0Asuffer%20from%20limited%20realism%2C%20insufficient%20scale%2C%20and%20inadequate%20complexity%2C%0Afailing%20to%20effectively%20evaluate%20modern%20vision-language%20models%20against%0Asophisticated%20forgeries.%20To%20address%20this%20critical%20gap%2C%20we%20introduce%20AEGIS%2C%20a%0Anovel%20large-scale%20benchmark%20explicitly%20targeting%20the%20detection%20of%0Ahyper-realistic%20and%20semantically%20nuanced%20AI-generated%20videos.%20AEGIS%20comprises%0Aover%2010%2C000%20rigorously%20curated%20real%20and%20synthetic%20videos%20generated%20by%20diverse%2C%0Astate-of-the-art%20generative%20models%2C%20including%20Stable%20Video%20Diffusion%2C%0ACogVideoX-5B%2C%20KLing%2C%20and%20Sora%2C%20encompassing%20open-source%20and%20proprietary%0Aarchitectures.%20In%20particular%2C%20AEGIS%20features%20specially%20constructed%20challenging%0Asubsets%20enhanced%20with%20robustness%20evaluation.%20Furthermore%2C%20we%20provide%20multimodal%0Aannotations%20spanning%20Semantic-Authenticity%20Descriptions%2C%20Motion%20Features%2C%20and%0ALow-level%20Visual%20Features%2C%20facilitating%20authenticity%20detection%20and%20supporting%0Adownstream%20tasks%20such%20as%20multimodal%20fusion%20and%20forgery%20localization.%20Extensive%0Aexperiments%20using%20advanced%20vision-language%20models%20demonstrate%20limited%20detection%0Acapabilities%20on%20the%20most%20challenging%20subsets%20of%20AEGIS%2C%20highlighting%20the%0Adataset%27s%20unique%20complexity%20and%20realism%20beyond%20the%20current%20generalization%0Acapabilities%20of%20existing%20models.%20In%20essence%2C%20AEGIS%20establishes%20an%20indispensable%0Aevaluation%20benchmark%2C%20fundamentally%20advancing%20research%20toward%20developing%0Agenuinely%20robust%2C%20reliable%2C%20broadly%20generalizable%20video%20authenticity%20detection%0Amethodologies%20capable%20of%20addressing%20real-world%20forgery%20threats.%20Our%20dataset%20is%0Aavailable%20on%20https%3A//huggingface.co/datasets/Clarifiedfish/AEGIS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAEGIS%253A%2520Authenticity%2520Evaluation%2520Benchmark%2520for%2520AI-Generated%2520Video%250A%2520%2520Sequences%26entry.906535625%3DJieyu%2520Li%2520and%2520Xin%2520Zhang%2520and%2520Joey%2520Tianyi%2520Zhou%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520AI-generated%2520content%2520have%2520fueled%2520the%2520rise%2520of%2520highly%250Arealistic%2520synthetic%2520videos%252C%2520posing%2520severe%2520risks%2520to%2520societal%2520trust%2520and%2520digital%250Aintegrity.%2520Existing%2520benchmarks%2520for%2520video%2520authenticity%2520detection%2520typically%250Asuffer%2520from%2520limited%2520realism%252C%2520insufficient%2520scale%252C%2520and%2520inadequate%2520complexity%252C%250Afailing%2520to%2520effectively%2520evaluate%2520modern%2520vision-language%2520models%2520against%250Asophisticated%2520forgeries.%2520To%2520address%2520this%2520critical%2520gap%252C%2520we%2520introduce%2520AEGIS%252C%2520a%250Anovel%2520large-scale%2520benchmark%2520explicitly%2520targeting%2520the%2520detection%2520of%250Ahyper-realistic%2520and%2520semantically%2520nuanced%2520AI-generated%2520videos.%2520AEGIS%2520comprises%250Aover%252010%252C000%2520rigorously%2520curated%2520real%2520and%2520synthetic%2520videos%2520generated%2520by%2520diverse%252C%250Astate-of-the-art%2520generative%2520models%252C%2520including%2520Stable%2520Video%2520Diffusion%252C%250ACogVideoX-5B%252C%2520KLing%252C%2520and%2520Sora%252C%2520encompassing%2520open-source%2520and%2520proprietary%250Aarchitectures.%2520In%2520particular%252C%2520AEGIS%2520features%2520specially%2520constructed%2520challenging%250Asubsets%2520enhanced%2520with%2520robustness%2520evaluation.%2520Furthermore%252C%2520we%2520provide%2520multimodal%250Aannotations%2520spanning%2520Semantic-Authenticity%2520Descriptions%252C%2520Motion%2520Features%252C%2520and%250ALow-level%2520Visual%2520Features%252C%2520facilitating%2520authenticity%2520detection%2520and%2520supporting%250Adownstream%2520tasks%2520such%2520as%2520multimodal%2520fusion%2520and%2520forgery%2520localization.%2520Extensive%250Aexperiments%2520using%2520advanced%2520vision-language%2520models%2520demonstrate%2520limited%2520detection%250Acapabilities%2520on%2520the%2520most%2520challenging%2520subsets%2520of%2520AEGIS%252C%2520highlighting%2520the%250Adataset%2527s%2520unique%2520complexity%2520and%2520realism%2520beyond%2520the%2520current%2520generalization%250Acapabilities%2520of%2520existing%2520models.%2520In%2520essence%252C%2520AEGIS%2520establishes%2520an%2520indispensable%250Aevaluation%2520benchmark%252C%2520fundamentally%2520advancing%2520research%2520toward%2520developing%250Agenuinely%2520robust%252C%2520reliable%252C%2520broadly%2520generalizable%2520video%2520authenticity%2520detection%250Amethodologies%2520capable%2520of%2520addressing%2520real-world%2520forgery%2520threats.%2520Our%2520dataset%2520is%250Aavailable%2520on%2520https%253A//huggingface.co/datasets/Clarifiedfish/AEGIS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AEGIS%3A%20Authenticity%20Evaluation%20Benchmark%20for%20AI-Generated%20Video%0A%20%20Sequences&entry.906535625=Jieyu%20Li%20and%20Xin%20Zhang%20and%20Joey%20Tianyi%20Zhou&entry.1292438233=%20%20Recent%20advances%20in%20AI-generated%20content%20have%20fueled%20the%20rise%20of%20highly%0Arealistic%20synthetic%20videos%2C%20posing%20severe%20risks%20to%20societal%20trust%20and%20digital%0Aintegrity.%20Existing%20benchmarks%20for%20video%20authenticity%20detection%20typically%0Asuffer%20from%20limited%20realism%2C%20insufficient%20scale%2C%20and%20inadequate%20complexity%2C%0Afailing%20to%20effectively%20evaluate%20modern%20vision-language%20models%20against%0Asophisticated%20forgeries.%20To%20address%20this%20critical%20gap%2C%20we%20introduce%20AEGIS%2C%20a%0Anovel%20large-scale%20benchmark%20explicitly%20targeting%20the%20detection%20of%0Ahyper-realistic%20and%20semantically%20nuanced%20AI-generated%20videos.%20AEGIS%20comprises%0Aover%2010%2C000%20rigorously%20curated%20real%20and%20synthetic%20videos%20generated%20by%20diverse%2C%0Astate-of-the-art%20generative%20models%2C%20including%20Stable%20Video%20Diffusion%2C%0ACogVideoX-5B%2C%20KLing%2C%20and%20Sora%2C%20encompassing%20open-source%20and%20proprietary%0Aarchitectures.%20In%20particular%2C%20AEGIS%20features%20specially%20constructed%20challenging%0Asubsets%20enhanced%20with%20robustness%20evaluation.%20Furthermore%2C%20we%20provide%20multimodal%0Aannotations%20spanning%20Semantic-Authenticity%20Descriptions%2C%20Motion%20Features%2C%20and%0ALow-level%20Visual%20Features%2C%20facilitating%20authenticity%20detection%20and%20supporting%0Adownstream%20tasks%20such%20as%20multimodal%20fusion%20and%20forgery%20localization.%20Extensive%0Aexperiments%20using%20advanced%20vision-language%20models%20demonstrate%20limited%20detection%0Acapabilities%20on%20the%20most%20challenging%20subsets%20of%20AEGIS%2C%20highlighting%20the%0Adataset%27s%20unique%20complexity%20and%20realism%20beyond%20the%20current%20generalization%0Acapabilities%20of%20existing%20models.%20In%20essence%2C%20AEGIS%20establishes%20an%20indispensable%0Aevaluation%20benchmark%2C%20fundamentally%20advancing%20research%20toward%20developing%0Agenuinely%20robust%2C%20reliable%2C%20broadly%20generalizable%20video%20authenticity%20detection%0Amethodologies%20capable%20of%20addressing%20real-world%20forgery%20threats.%20Our%20dataset%20is%0Aavailable%20on%20https%3A//huggingface.co/datasets/Clarifiedfish/AEGIS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10771v1&entry.124074799=Read"},
{"title": "HyperTea: A Hypergraph-based Temporal Enhancement and Alignment Network\n  for Moving Infrared Small Target Detection", "author": "Zhaoyuan Qi and Weihua Gao and Wenlong Niu and Jie Tang and Yun Li and Xiaodong Peng", "abstract": "  In practical application scenarios, moving infrared small target detection\n(MIRSTD) remains highly challenging due to the target's small size, weak\nintensity, and complex motion pattern. Existing methods typically only model\nlow-order correlations between feature nodes and perform feature extraction and\nenhancement within a single temporal scale. Although hypergraphs have been\nwidely used for high-order correlation learning, they have received limited\nattention in MIRSTD. To explore the potential of hypergraphs and enhance\nmulti-timescale feature representation, we propose HyperTea, which integrates\nglobal and local temporal perspectives to effectively model high-order\nspatiotemporal correlations of features. HyperTea consists of three modules:\nthe global temporal enhancement module (GTEM) realizes global temporal context\nenhancement through semantic aggregation and propagation; the local temporal\nenhancement module (LTEM) is designed to capture local motion patterns between\nadjacent frames and then enhance local temporal context; additionally, we\nfurther develop a temporal alignment module (TAM) to address potential\ncross-scale feature misalignment. To our best knowledge, HyperTea is the first\nwork to integrate convolutional neural networks (CNNs), recurrent neural\nnetworks (RNNs), and hypergraph neural networks (HGNNs) for MIRSTD,\nsignificantly improving detection performance. Experiments on DAUB and IRDST\ndemonstrate its state-of-the-art (SOTA) performance. Our source codes are\navailable at https://github.com/Lurenjia-LRJ/HyperTea.\n", "link": "http://arxiv.org/abs/2508.10678v1", "date": "2025-08-14", "relevancy": 2.7769, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.561}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5528}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperTea%3A%20A%20Hypergraph-based%20Temporal%20Enhancement%20and%20Alignment%20Network%0A%20%20for%20Moving%20Infrared%20Small%20Target%20Detection&body=Title%3A%20HyperTea%3A%20A%20Hypergraph-based%20Temporal%20Enhancement%20and%20Alignment%20Network%0A%20%20for%20Moving%20Infrared%20Small%20Target%20Detection%0AAuthor%3A%20Zhaoyuan%20Qi%20and%20Weihua%20Gao%20and%20Wenlong%20Niu%20and%20Jie%20Tang%20and%20Yun%20Li%20and%20Xiaodong%20Peng%0AAbstract%3A%20%20%20In%20practical%20application%20scenarios%2C%20moving%20infrared%20small%20target%20detection%0A%28MIRSTD%29%20remains%20highly%20challenging%20due%20to%20the%20target%27s%20small%20size%2C%20weak%0Aintensity%2C%20and%20complex%20motion%20pattern.%20Existing%20methods%20typically%20only%20model%0Alow-order%20correlations%20between%20feature%20nodes%20and%20perform%20feature%20extraction%20and%0Aenhancement%20within%20a%20single%20temporal%20scale.%20Although%20hypergraphs%20have%20been%0Awidely%20used%20for%20high-order%20correlation%20learning%2C%20they%20have%20received%20limited%0Aattention%20in%20MIRSTD.%20To%20explore%20the%20potential%20of%20hypergraphs%20and%20enhance%0Amulti-timescale%20feature%20representation%2C%20we%20propose%20HyperTea%2C%20which%20integrates%0Aglobal%20and%20local%20temporal%20perspectives%20to%20effectively%20model%20high-order%0Aspatiotemporal%20correlations%20of%20features.%20HyperTea%20consists%20of%20three%20modules%3A%0Athe%20global%20temporal%20enhancement%20module%20%28GTEM%29%20realizes%20global%20temporal%20context%0Aenhancement%20through%20semantic%20aggregation%20and%20propagation%3B%20the%20local%20temporal%0Aenhancement%20module%20%28LTEM%29%20is%20designed%20to%20capture%20local%20motion%20patterns%20between%0Aadjacent%20frames%20and%20then%20enhance%20local%20temporal%20context%3B%20additionally%2C%20we%0Afurther%20develop%20a%20temporal%20alignment%20module%20%28TAM%29%20to%20address%20potential%0Across-scale%20feature%20misalignment.%20To%20our%20best%20knowledge%2C%20HyperTea%20is%20the%20first%0Awork%20to%20integrate%20convolutional%20neural%20networks%20%28CNNs%29%2C%20recurrent%20neural%0Anetworks%20%28RNNs%29%2C%20and%20hypergraph%20neural%20networks%20%28HGNNs%29%20for%20MIRSTD%2C%0Asignificantly%20improving%20detection%20performance.%20Experiments%20on%20DAUB%20and%20IRDST%0Ademonstrate%20its%20state-of-the-art%20%28SOTA%29%20performance.%20Our%20source%20codes%20are%0Aavailable%20at%20https%3A//github.com/Lurenjia-LRJ/HyperTea.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperTea%253A%2520A%2520Hypergraph-based%2520Temporal%2520Enhancement%2520and%2520Alignment%2520Network%250A%2520%2520for%2520Moving%2520Infrared%2520Small%2520Target%2520Detection%26entry.906535625%3DZhaoyuan%2520Qi%2520and%2520Weihua%2520Gao%2520and%2520Wenlong%2520Niu%2520and%2520Jie%2520Tang%2520and%2520Yun%2520Li%2520and%2520Xiaodong%2520Peng%26entry.1292438233%3D%2520%2520In%2520practical%2520application%2520scenarios%252C%2520moving%2520infrared%2520small%2520target%2520detection%250A%2528MIRSTD%2529%2520remains%2520highly%2520challenging%2520due%2520to%2520the%2520target%2527s%2520small%2520size%252C%2520weak%250Aintensity%252C%2520and%2520complex%2520motion%2520pattern.%2520Existing%2520methods%2520typically%2520only%2520model%250Alow-order%2520correlations%2520between%2520feature%2520nodes%2520and%2520perform%2520feature%2520extraction%2520and%250Aenhancement%2520within%2520a%2520single%2520temporal%2520scale.%2520Although%2520hypergraphs%2520have%2520been%250Awidely%2520used%2520for%2520high-order%2520correlation%2520learning%252C%2520they%2520have%2520received%2520limited%250Aattention%2520in%2520MIRSTD.%2520To%2520explore%2520the%2520potential%2520of%2520hypergraphs%2520and%2520enhance%250Amulti-timescale%2520feature%2520representation%252C%2520we%2520propose%2520HyperTea%252C%2520which%2520integrates%250Aglobal%2520and%2520local%2520temporal%2520perspectives%2520to%2520effectively%2520model%2520high-order%250Aspatiotemporal%2520correlations%2520of%2520features.%2520HyperTea%2520consists%2520of%2520three%2520modules%253A%250Athe%2520global%2520temporal%2520enhancement%2520module%2520%2528GTEM%2529%2520realizes%2520global%2520temporal%2520context%250Aenhancement%2520through%2520semantic%2520aggregation%2520and%2520propagation%253B%2520the%2520local%2520temporal%250Aenhancement%2520module%2520%2528LTEM%2529%2520is%2520designed%2520to%2520capture%2520local%2520motion%2520patterns%2520between%250Aadjacent%2520frames%2520and%2520then%2520enhance%2520local%2520temporal%2520context%253B%2520additionally%252C%2520we%250Afurther%2520develop%2520a%2520temporal%2520alignment%2520module%2520%2528TAM%2529%2520to%2520address%2520potential%250Across-scale%2520feature%2520misalignment.%2520To%2520our%2520best%2520knowledge%252C%2520HyperTea%2520is%2520the%2520first%250Awork%2520to%2520integrate%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%2520recurrent%2520neural%250Anetworks%2520%2528RNNs%2529%252C%2520and%2520hypergraph%2520neural%2520networks%2520%2528HGNNs%2529%2520for%2520MIRSTD%252C%250Asignificantly%2520improving%2520detection%2520performance.%2520Experiments%2520on%2520DAUB%2520and%2520IRDST%250Ademonstrate%2520its%2520state-of-the-art%2520%2528SOTA%2529%2520performance.%2520Our%2520source%2520codes%2520are%250Aavailable%2520at%2520https%253A//github.com/Lurenjia-LRJ/HyperTea.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperTea%3A%20A%20Hypergraph-based%20Temporal%20Enhancement%20and%20Alignment%20Network%0A%20%20for%20Moving%20Infrared%20Small%20Target%20Detection&entry.906535625=Zhaoyuan%20Qi%20and%20Weihua%20Gao%20and%20Wenlong%20Niu%20and%20Jie%20Tang%20and%20Yun%20Li%20and%20Xiaodong%20Peng&entry.1292438233=%20%20In%20practical%20application%20scenarios%2C%20moving%20infrared%20small%20target%20detection%0A%28MIRSTD%29%20remains%20highly%20challenging%20due%20to%20the%20target%27s%20small%20size%2C%20weak%0Aintensity%2C%20and%20complex%20motion%20pattern.%20Existing%20methods%20typically%20only%20model%0Alow-order%20correlations%20between%20feature%20nodes%20and%20perform%20feature%20extraction%20and%0Aenhancement%20within%20a%20single%20temporal%20scale.%20Although%20hypergraphs%20have%20been%0Awidely%20used%20for%20high-order%20correlation%20learning%2C%20they%20have%20received%20limited%0Aattention%20in%20MIRSTD.%20To%20explore%20the%20potential%20of%20hypergraphs%20and%20enhance%0Amulti-timescale%20feature%20representation%2C%20we%20propose%20HyperTea%2C%20which%20integrates%0Aglobal%20and%20local%20temporal%20perspectives%20to%20effectively%20model%20high-order%0Aspatiotemporal%20correlations%20of%20features.%20HyperTea%20consists%20of%20three%20modules%3A%0Athe%20global%20temporal%20enhancement%20module%20%28GTEM%29%20realizes%20global%20temporal%20context%0Aenhancement%20through%20semantic%20aggregation%20and%20propagation%3B%20the%20local%20temporal%0Aenhancement%20module%20%28LTEM%29%20is%20designed%20to%20capture%20local%20motion%20patterns%20between%0Aadjacent%20frames%20and%20then%20enhance%20local%20temporal%20context%3B%20additionally%2C%20we%0Afurther%20develop%20a%20temporal%20alignment%20module%20%28TAM%29%20to%20address%20potential%0Across-scale%20feature%20misalignment.%20To%20our%20best%20knowledge%2C%20HyperTea%20is%20the%20first%0Awork%20to%20integrate%20convolutional%20neural%20networks%20%28CNNs%29%2C%20recurrent%20neural%0Anetworks%20%28RNNs%29%2C%20and%20hypergraph%20neural%20networks%20%28HGNNs%29%20for%20MIRSTD%2C%0Asignificantly%20improving%20detection%20performance.%20Experiments%20on%20DAUB%20and%20IRDST%0Ademonstrate%20its%20state-of-the-art%20%28SOTA%29%20performance.%20Our%20source%20codes%20are%0Aavailable%20at%20https%3A//github.com/Lurenjia-LRJ/HyperTea.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10678v1&entry.124074799=Read"},
{"title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed\n  View Memory", "author": "Runjia Li and Philip Torr and Andrea Vedaldi and Tomas Jakab", "abstract": "  We propose a novel memory module for building video generators capable of\ninteractively exploring environments. Previous approaches have achieved similar\nresults either by out-painting 2D views of a scene while incrementally\nreconstructing its 3D geometry-which quickly accumulates errors-or by using\nvideo generators with a short context window, which struggle to maintain scene\ncoherence over the long term. To address these limitations, we introduce\nSurfel-Indexed View Memory (VMem), a memory module that remembers past views by\nindexing them geometrically based on the 3D surface elements (surfels) they\nhave observed. VMem enables efficient retrieval of the most relevant past views\nwhen generating new ones. By focusing only on these relevant views, our method\nproduces consistent explorations of imagined environments at a fraction of the\ncomputational cost required to use all past views as context. We evaluate our\napproach on challenging long-term scene synthesis benchmarks and demonstrate\nsuperior performance compared to existing methods in maintaining scene\ncoherence and camera control.\n", "link": "http://arxiv.org/abs/2506.18903v3", "date": "2025-08-14", "relevancy": 2.7742, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5655}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5495}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VMem%3A%20Consistent%20Interactive%20Video%20Scene%20Generation%20with%20Surfel-Indexed%0A%20%20View%20Memory&body=Title%3A%20VMem%3A%20Consistent%20Interactive%20Video%20Scene%20Generation%20with%20Surfel-Indexed%0A%20%20View%20Memory%0AAuthor%3A%20Runjia%20Li%20and%20Philip%20Torr%20and%20Andrea%20Vedaldi%20and%20Tomas%20Jakab%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20memory%20module%20for%20building%20video%20generators%20capable%20of%0Ainteractively%20exploring%20environments.%20Previous%20approaches%20have%20achieved%20similar%0Aresults%20either%20by%20out-painting%202D%20views%20of%20a%20scene%20while%20incrementally%0Areconstructing%20its%203D%20geometry-which%20quickly%20accumulates%20errors-or%20by%20using%0Avideo%20generators%20with%20a%20short%20context%20window%2C%20which%20struggle%20to%20maintain%20scene%0Acoherence%20over%20the%20long%20term.%20To%20address%20these%20limitations%2C%20we%20introduce%0ASurfel-Indexed%20View%20Memory%20%28VMem%29%2C%20a%20memory%20module%20that%20remembers%20past%20views%20by%0Aindexing%20them%20geometrically%20based%20on%20the%203D%20surface%20elements%20%28surfels%29%20they%0Ahave%20observed.%20VMem%20enables%20efficient%20retrieval%20of%20the%20most%20relevant%20past%20views%0Awhen%20generating%20new%20ones.%20By%20focusing%20only%20on%20these%20relevant%20views%2C%20our%20method%0Aproduces%20consistent%20explorations%20of%20imagined%20environments%20at%20a%20fraction%20of%20the%0Acomputational%20cost%20required%20to%20use%20all%20past%20views%20as%20context.%20We%20evaluate%20our%0Aapproach%20on%20challenging%20long-term%20scene%20synthesis%20benchmarks%20and%20demonstrate%0Asuperior%20performance%20compared%20to%20existing%20methods%20in%20maintaining%20scene%0Acoherence%20and%20camera%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.18903v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVMem%253A%2520Consistent%2520Interactive%2520Video%2520Scene%2520Generation%2520with%2520Surfel-Indexed%250A%2520%2520View%2520Memory%26entry.906535625%3DRunjia%2520Li%2520and%2520Philip%2520Torr%2520and%2520Andrea%2520Vedaldi%2520and%2520Tomas%2520Jakab%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520memory%2520module%2520for%2520building%2520video%2520generators%2520capable%2520of%250Ainteractively%2520exploring%2520environments.%2520Previous%2520approaches%2520have%2520achieved%2520similar%250Aresults%2520either%2520by%2520out-painting%25202D%2520views%2520of%2520a%2520scene%2520while%2520incrementally%250Areconstructing%2520its%25203D%2520geometry-which%2520quickly%2520accumulates%2520errors-or%2520by%2520using%250Avideo%2520generators%2520with%2520a%2520short%2520context%2520window%252C%2520which%2520struggle%2520to%2520maintain%2520scene%250Acoherence%2520over%2520the%2520long%2520term.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%250ASurfel-Indexed%2520View%2520Memory%2520%2528VMem%2529%252C%2520a%2520memory%2520module%2520that%2520remembers%2520past%2520views%2520by%250Aindexing%2520them%2520geometrically%2520based%2520on%2520the%25203D%2520surface%2520elements%2520%2528surfels%2529%2520they%250Ahave%2520observed.%2520VMem%2520enables%2520efficient%2520retrieval%2520of%2520the%2520most%2520relevant%2520past%2520views%250Awhen%2520generating%2520new%2520ones.%2520By%2520focusing%2520only%2520on%2520these%2520relevant%2520views%252C%2520our%2520method%250Aproduces%2520consistent%2520explorations%2520of%2520imagined%2520environments%2520at%2520a%2520fraction%2520of%2520the%250Acomputational%2520cost%2520required%2520to%2520use%2520all%2520past%2520views%2520as%2520context.%2520We%2520evaluate%2520our%250Aapproach%2520on%2520challenging%2520long-term%2520scene%2520synthesis%2520benchmarks%2520and%2520demonstrate%250Asuperior%2520performance%2520compared%2520to%2520existing%2520methods%2520in%2520maintaining%2520scene%250Acoherence%2520and%2520camera%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18903v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VMem%3A%20Consistent%20Interactive%20Video%20Scene%20Generation%20with%20Surfel-Indexed%0A%20%20View%20Memory&entry.906535625=Runjia%20Li%20and%20Philip%20Torr%20and%20Andrea%20Vedaldi%20and%20Tomas%20Jakab&entry.1292438233=%20%20We%20propose%20a%20novel%20memory%20module%20for%20building%20video%20generators%20capable%20of%0Ainteractively%20exploring%20environments.%20Previous%20approaches%20have%20achieved%20similar%0Aresults%20either%20by%20out-painting%202D%20views%20of%20a%20scene%20while%20incrementally%0Areconstructing%20its%203D%20geometry-which%20quickly%20accumulates%20errors-or%20by%20using%0Avideo%20generators%20with%20a%20short%20context%20window%2C%20which%20struggle%20to%20maintain%20scene%0Acoherence%20over%20the%20long%20term.%20To%20address%20these%20limitations%2C%20we%20introduce%0ASurfel-Indexed%20View%20Memory%20%28VMem%29%2C%20a%20memory%20module%20that%20remembers%20past%20views%20by%0Aindexing%20them%20geometrically%20based%20on%20the%203D%20surface%20elements%20%28surfels%29%20they%0Ahave%20observed.%20VMem%20enables%20efficient%20retrieval%20of%20the%20most%20relevant%20past%20views%0Awhen%20generating%20new%20ones.%20By%20focusing%20only%20on%20these%20relevant%20views%2C%20our%20method%0Aproduces%20consistent%20explorations%20of%20imagined%20environments%20at%20a%20fraction%20of%20the%0Acomputational%20cost%20required%20to%20use%20all%20past%20views%20as%20context.%20We%20evaluate%20our%0Aapproach%20on%20challenging%20long-term%20scene%20synthesis%20benchmarks%20and%20demonstrate%0Asuperior%20performance%20compared%20to%20existing%20methods%20in%20maintaining%20scene%0Acoherence%20and%20camera%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.18903v3&entry.124074799=Read"},
{"title": "Preacher: Paper-to-Video Agentic System", "author": "Jingwei Liu and Ling Yang and Hao Luo and Fan Wang and Hongyan Li and Mengdi Wang", "abstract": "  The paper-to-video task converts a research paper into a structured video\nabstract, distilling key concepts, methods, and conclusions into an accessible,\nwell-organized format. While state-of-the-art video generation models\ndemonstrate potential, they are constrained by limited context windows, rigid\nvideo duration constraints, limited stylistic diversity, and an inability to\nrepresent domain-specific knowledge. To address these limitations, we introduce\nPreacher, the first paper-to-video agentic system. Preacher employs a topdown\napproach to decompose, summarize, and reformulate the paper, followed by\nbottom-up video generation, synthesizing diverse video segments into a coherent\nabstract. To align cross-modal representations, we define key scenes and\nintroduce a Progressive Chain of Thought (P-CoT) for granular, iterative\nplanning. Preacher successfully generates high-quality video abstracts across\nfive research fields, demonstrating expertise beyond current video generation\nmodels. Code will be released at: https://github.com/GenVerse/Paper2Video\n", "link": "http://arxiv.org/abs/2508.09632v2", "date": "2025-08-14", "relevancy": 2.7077, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5584}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.534}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preacher%3A%20Paper-to-Video%20Agentic%20System&body=Title%3A%20Preacher%3A%20Paper-to-Video%20Agentic%20System%0AAuthor%3A%20Jingwei%20Liu%20and%20Ling%20Yang%20and%20Hao%20Luo%20and%20Fan%20Wang%20and%20Hongyan%20Li%20and%20Mengdi%20Wang%0AAbstract%3A%20%20%20The%20paper-to-video%20task%20converts%20a%20research%20paper%20into%20a%20structured%20video%0Aabstract%2C%20distilling%20key%20concepts%2C%20methods%2C%20and%20conclusions%20into%20an%20accessible%2C%0Awell-organized%20format.%20While%20state-of-the-art%20video%20generation%20models%0Ademonstrate%20potential%2C%20they%20are%20constrained%20by%20limited%20context%20windows%2C%20rigid%0Avideo%20duration%20constraints%2C%20limited%20stylistic%20diversity%2C%20and%20an%20inability%20to%0Arepresent%20domain-specific%20knowledge.%20To%20address%20these%20limitations%2C%20we%20introduce%0APreacher%2C%20the%20first%20paper-to-video%20agentic%20system.%20Preacher%20employs%20a%20topdown%0Aapproach%20to%20decompose%2C%20summarize%2C%20and%20reformulate%20the%20paper%2C%20followed%20by%0Abottom-up%20video%20generation%2C%20synthesizing%20diverse%20video%20segments%20into%20a%20coherent%0Aabstract.%20To%20align%20cross-modal%20representations%2C%20we%20define%20key%20scenes%20and%0Aintroduce%20a%20Progressive%20Chain%20of%20Thought%20%28P-CoT%29%20for%20granular%2C%20iterative%0Aplanning.%20Preacher%20successfully%20generates%20high-quality%20video%20abstracts%20across%0Afive%20research%20fields%2C%20demonstrating%20expertise%20beyond%20current%20video%20generation%0Amodels.%20Code%20will%20be%20released%20at%3A%20https%3A//github.com/GenVerse/Paper2Video%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09632v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreacher%253A%2520Paper-to-Video%2520Agentic%2520System%26entry.906535625%3DJingwei%2520Liu%2520and%2520Ling%2520Yang%2520and%2520Hao%2520Luo%2520and%2520Fan%2520Wang%2520and%2520Hongyan%2520Li%2520and%2520Mengdi%2520Wang%26entry.1292438233%3D%2520%2520The%2520paper-to-video%2520task%2520converts%2520a%2520research%2520paper%2520into%2520a%2520structured%2520video%250Aabstract%252C%2520distilling%2520key%2520concepts%252C%2520methods%252C%2520and%2520conclusions%2520into%2520an%2520accessible%252C%250Awell-organized%2520format.%2520While%2520state-of-the-art%2520video%2520generation%2520models%250Ademonstrate%2520potential%252C%2520they%2520are%2520constrained%2520by%2520limited%2520context%2520windows%252C%2520rigid%250Avideo%2520duration%2520constraints%252C%2520limited%2520stylistic%2520diversity%252C%2520and%2520an%2520inability%2520to%250Arepresent%2520domain-specific%2520knowledge.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%250APreacher%252C%2520the%2520first%2520paper-to-video%2520agentic%2520system.%2520Preacher%2520employs%2520a%2520topdown%250Aapproach%2520to%2520decompose%252C%2520summarize%252C%2520and%2520reformulate%2520the%2520paper%252C%2520followed%2520by%250Abottom-up%2520video%2520generation%252C%2520synthesizing%2520diverse%2520video%2520segments%2520into%2520a%2520coherent%250Aabstract.%2520To%2520align%2520cross-modal%2520representations%252C%2520we%2520define%2520key%2520scenes%2520and%250Aintroduce%2520a%2520Progressive%2520Chain%2520of%2520Thought%2520%2528P-CoT%2529%2520for%2520granular%252C%2520iterative%250Aplanning.%2520Preacher%2520successfully%2520generates%2520high-quality%2520video%2520abstracts%2520across%250Afive%2520research%2520fields%252C%2520demonstrating%2520expertise%2520beyond%2520current%2520video%2520generation%250Amodels.%2520Code%2520will%2520be%2520released%2520at%253A%2520https%253A//github.com/GenVerse/Paper2Video%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09632v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preacher%3A%20Paper-to-Video%20Agentic%20System&entry.906535625=Jingwei%20Liu%20and%20Ling%20Yang%20and%20Hao%20Luo%20and%20Fan%20Wang%20and%20Hongyan%20Li%20and%20Mengdi%20Wang&entry.1292438233=%20%20The%20paper-to-video%20task%20converts%20a%20research%20paper%20into%20a%20structured%20video%0Aabstract%2C%20distilling%20key%20concepts%2C%20methods%2C%20and%20conclusions%20into%20an%20accessible%2C%0Awell-organized%20format.%20While%20state-of-the-art%20video%20generation%20models%0Ademonstrate%20potential%2C%20they%20are%20constrained%20by%20limited%20context%20windows%2C%20rigid%0Avideo%20duration%20constraints%2C%20limited%20stylistic%20diversity%2C%20and%20an%20inability%20to%0Arepresent%20domain-specific%20knowledge.%20To%20address%20these%20limitations%2C%20we%20introduce%0APreacher%2C%20the%20first%20paper-to-video%20agentic%20system.%20Preacher%20employs%20a%20topdown%0Aapproach%20to%20decompose%2C%20summarize%2C%20and%20reformulate%20the%20paper%2C%20followed%20by%0Abottom-up%20video%20generation%2C%20synthesizing%20diverse%20video%20segments%20into%20a%20coherent%0Aabstract.%20To%20align%20cross-modal%20representations%2C%20we%20define%20key%20scenes%20and%0Aintroduce%20a%20Progressive%20Chain%20of%20Thought%20%28P-CoT%29%20for%20granular%2C%20iterative%0Aplanning.%20Preacher%20successfully%20generates%20high-quality%20video%20abstracts%20across%0Afive%20research%20fields%2C%20demonstrating%20expertise%20beyond%20current%20video%20generation%0Amodels.%20Code%20will%20be%20released%20at%3A%20https%3A//github.com/GenVerse/Paper2Video%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09632v2&entry.124074799=Read"},
{"title": "Unifying Self-Supervised Clustering and Energy-Based Models", "author": "Emanuele Sansone and Robin Manhaeve", "abstract": "  Self-supervised learning excels at learning representations from large\namounts of data. At the same time, generative models offer the complementary\nproperty of learning information about the underlying data generation process.\nIn this study, we aim at establishing a principled connection between these two\nparadigms and highlight the benefits of their complementarity. In particular,\nwe perform an analysis of self-supervised learning objectives, elucidating the\nunderlying probabilistic graphical models and presenting a standardized\nmethodology for their derivation from first principles. The analysis suggests a\nnatural means of integrating self-supervised learning with likelihood-based\ngenerative models. We instantiate this concept within the realm of\ncluster-based self-supervised learning and energy models, introducing a lower\nbound proven to reliably penalize the most important failure modes and\nunlocking full unification. Our theoretical findings are substantiated through\nexperiments on synthetic and real-world data, including SVHN, CIFAR10, and\nCIFAR100, demonstrating that our objective function allows to jointly train a\nbackbone network in a discriminative and generative fashion, consequently\noutperforming existing self-supervised learning strategies in terms of\nclustering, generation and out-of-distribution detection performance by a wide\nmargin. We also demonstrate that the solution can be integrated into a\nneuro-symbolic framework to tackle a simple yet non-trivial instantiation of\nthe symbol grounding problem. The code is publicly available at\nhttps://github.com/emsansone/GEDI.\n", "link": "http://arxiv.org/abs/2401.00873v5", "date": "2025-08-14", "relevancy": 2.6905, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.54}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5398}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unifying%20Self-Supervised%20Clustering%20and%20Energy-Based%20Models&body=Title%3A%20Unifying%20Self-Supervised%20Clustering%20and%20Energy-Based%20Models%0AAuthor%3A%20Emanuele%20Sansone%20and%20Robin%20Manhaeve%0AAbstract%3A%20%20%20Self-supervised%20learning%20excels%20at%20learning%20representations%20from%20large%0Aamounts%20of%20data.%20At%20the%20same%20time%2C%20generative%20models%20offer%20the%20complementary%0Aproperty%20of%20learning%20information%20about%20the%20underlying%20data%20generation%20process.%0AIn%20this%20study%2C%20we%20aim%20at%20establishing%20a%20principled%20connection%20between%20these%20two%0Aparadigms%20and%20highlight%20the%20benefits%20of%20their%20complementarity.%20In%20particular%2C%0Awe%20perform%20an%20analysis%20of%20self-supervised%20learning%20objectives%2C%20elucidating%20the%0Aunderlying%20probabilistic%20graphical%20models%20and%20presenting%20a%20standardized%0Amethodology%20for%20their%20derivation%20from%20first%20principles.%20The%20analysis%20suggests%20a%0Anatural%20means%20of%20integrating%20self-supervised%20learning%20with%20likelihood-based%0Agenerative%20models.%20We%20instantiate%20this%20concept%20within%20the%20realm%20of%0Acluster-based%20self-supervised%20learning%20and%20energy%20models%2C%20introducing%20a%20lower%0Abound%20proven%20to%20reliably%20penalize%20the%20most%20important%20failure%20modes%20and%0Aunlocking%20full%20unification.%20Our%20theoretical%20findings%20are%20substantiated%20through%0Aexperiments%20on%20synthetic%20and%20real-world%20data%2C%20including%20SVHN%2C%20CIFAR10%2C%20and%0ACIFAR100%2C%20demonstrating%20that%20our%20objective%20function%20allows%20to%20jointly%20train%20a%0Abackbone%20network%20in%20a%20discriminative%20and%20generative%20fashion%2C%20consequently%0Aoutperforming%20existing%20self-supervised%20learning%20strategies%20in%20terms%20of%0Aclustering%2C%20generation%20and%20out-of-distribution%20detection%20performance%20by%20a%20wide%0Amargin.%20We%20also%20demonstrate%20that%20the%20solution%20can%20be%20integrated%20into%20a%0Aneuro-symbolic%20framework%20to%20tackle%20a%20simple%20yet%20non-trivial%20instantiation%20of%0Athe%20symbol%20grounding%20problem.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/emsansone/GEDI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00873v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnifying%2520Self-Supervised%2520Clustering%2520and%2520Energy-Based%2520Models%26entry.906535625%3DEmanuele%2520Sansone%2520and%2520Robin%2520Manhaeve%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520excels%2520at%2520learning%2520representations%2520from%2520large%250Aamounts%2520of%2520data.%2520At%2520the%2520same%2520time%252C%2520generative%2520models%2520offer%2520the%2520complementary%250Aproperty%2520of%2520learning%2520information%2520about%2520the%2520underlying%2520data%2520generation%2520process.%250AIn%2520this%2520study%252C%2520we%2520aim%2520at%2520establishing%2520a%2520principled%2520connection%2520between%2520these%2520two%250Aparadigms%2520and%2520highlight%2520the%2520benefits%2520of%2520their%2520complementarity.%2520In%2520particular%252C%250Awe%2520perform%2520an%2520analysis%2520of%2520self-supervised%2520learning%2520objectives%252C%2520elucidating%2520the%250Aunderlying%2520probabilistic%2520graphical%2520models%2520and%2520presenting%2520a%2520standardized%250Amethodology%2520for%2520their%2520derivation%2520from%2520first%2520principles.%2520The%2520analysis%2520suggests%2520a%250Anatural%2520means%2520of%2520integrating%2520self-supervised%2520learning%2520with%2520likelihood-based%250Agenerative%2520models.%2520We%2520instantiate%2520this%2520concept%2520within%2520the%2520realm%2520of%250Acluster-based%2520self-supervised%2520learning%2520and%2520energy%2520models%252C%2520introducing%2520a%2520lower%250Abound%2520proven%2520to%2520reliably%2520penalize%2520the%2520most%2520important%2520failure%2520modes%2520and%250Aunlocking%2520full%2520unification.%2520Our%2520theoretical%2520findings%2520are%2520substantiated%2520through%250Aexperiments%2520on%2520synthetic%2520and%2520real-world%2520data%252C%2520including%2520SVHN%252C%2520CIFAR10%252C%2520and%250ACIFAR100%252C%2520demonstrating%2520that%2520our%2520objective%2520function%2520allows%2520to%2520jointly%2520train%2520a%250Abackbone%2520network%2520in%2520a%2520discriminative%2520and%2520generative%2520fashion%252C%2520consequently%250Aoutperforming%2520existing%2520self-supervised%2520learning%2520strategies%2520in%2520terms%2520of%250Aclustering%252C%2520generation%2520and%2520out-of-distribution%2520detection%2520performance%2520by%2520a%2520wide%250Amargin.%2520We%2520also%2520demonstrate%2520that%2520the%2520solution%2520can%2520be%2520integrated%2520into%2520a%250Aneuro-symbolic%2520framework%2520to%2520tackle%2520a%2520simple%2520yet%2520non-trivial%2520instantiation%2520of%250Athe%2520symbol%2520grounding%2520problem.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/emsansone/GEDI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.00873v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20Self-Supervised%20Clustering%20and%20Energy-Based%20Models&entry.906535625=Emanuele%20Sansone%20and%20Robin%20Manhaeve&entry.1292438233=%20%20Self-supervised%20learning%20excels%20at%20learning%20representations%20from%20large%0Aamounts%20of%20data.%20At%20the%20same%20time%2C%20generative%20models%20offer%20the%20complementary%0Aproperty%20of%20learning%20information%20about%20the%20underlying%20data%20generation%20process.%0AIn%20this%20study%2C%20we%20aim%20at%20establishing%20a%20principled%20connection%20between%20these%20two%0Aparadigms%20and%20highlight%20the%20benefits%20of%20their%20complementarity.%20In%20particular%2C%0Awe%20perform%20an%20analysis%20of%20self-supervised%20learning%20objectives%2C%20elucidating%20the%0Aunderlying%20probabilistic%20graphical%20models%20and%20presenting%20a%20standardized%0Amethodology%20for%20their%20derivation%20from%20first%20principles.%20The%20analysis%20suggests%20a%0Anatural%20means%20of%20integrating%20self-supervised%20learning%20with%20likelihood-based%0Agenerative%20models.%20We%20instantiate%20this%20concept%20within%20the%20realm%20of%0Acluster-based%20self-supervised%20learning%20and%20energy%20models%2C%20introducing%20a%20lower%0Abound%20proven%20to%20reliably%20penalize%20the%20most%20important%20failure%20modes%20and%0Aunlocking%20full%20unification.%20Our%20theoretical%20findings%20are%20substantiated%20through%0Aexperiments%20on%20synthetic%20and%20real-world%20data%2C%20including%20SVHN%2C%20CIFAR10%2C%20and%0ACIFAR100%2C%20demonstrating%20that%20our%20objective%20function%20allows%20to%20jointly%20train%20a%0Abackbone%20network%20in%20a%20discriminative%20and%20generative%20fashion%2C%20consequently%0Aoutperforming%20existing%20self-supervised%20learning%20strategies%20in%20terms%20of%0Aclustering%2C%20generation%20and%20out-of-distribution%20detection%20performance%20by%20a%20wide%0Amargin.%20We%20also%20demonstrate%20that%20the%20solution%20can%20be%20integrated%20into%20a%0Aneuro-symbolic%20framework%20to%20tackle%20a%20simple%20yet%20non-trivial%20instantiation%20of%0Athe%20symbol%20grounding%20problem.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/emsansone/GEDI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00873v5&entry.124074799=Read"},
{"title": "ESSENTIAL: Episodic and Semantic Memory Integration for Video\n  Class-Incremental Learning", "author": "Jongseo Lee and Kyungho Bae and Kyle Min and Gyeong-Moon Park and Jinwoo Choi", "abstract": "  In this work, we tackle the problem of video classincremental learning\n(VCIL). Many existing VCIL methods mitigate catastrophic forgetting by\nrehearsal training with a few temporally dense samples stored in episodic\nmemory, which is memory-inefficient. Alternatively, some methods store\ntemporally sparse samples, sacrificing essential temporal information and\nthereby resulting in inferior performance. To address this trade-off between\nmemory-efficiency and performance, we propose EpiSodic and SEmaNTIc memory\nintegrAtion for video class-incremental Learning (ESSENTIAL). ESSENTIAL\nconsists of episodic memory for storing temporally sparse features and semantic\nmemory for storing general knowledge represented by learnable prompts. We\nintroduce a novel memory retrieval (MR) module that integrates episodic memory\nand semantic prompts through cross-attention, enabling the retrieval of\ntemporally dense features from temporally sparse features. We rigorously\nvalidate ESSENTIAL on diverse datasets: UCF-101, HMDB51, and\nSomething-Something-V2 from the TCD benchmark and UCF-101, ActivityNet, and\nKinetics-400 from the vCLIMB benchmark. Remarkably, with significantly reduced\nmemory, ESSENTIAL achieves favorable performance on the benchmarks.\n", "link": "http://arxiv.org/abs/2508.10896v1", "date": "2025-08-14", "relevancy": 2.6877, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5343}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ESSENTIAL%3A%20Episodic%20and%20Semantic%20Memory%20Integration%20for%20Video%0A%20%20Class-Incremental%20Learning&body=Title%3A%20ESSENTIAL%3A%20Episodic%20and%20Semantic%20Memory%20Integration%20for%20Video%0A%20%20Class-Incremental%20Learning%0AAuthor%3A%20Jongseo%20Lee%20and%20Kyungho%20Bae%20and%20Kyle%20Min%20and%20Gyeong-Moon%20Park%20and%20Jinwoo%20Choi%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20tackle%20the%20problem%20of%20video%20classincremental%20learning%0A%28VCIL%29.%20Many%20existing%20VCIL%20methods%20mitigate%20catastrophic%20forgetting%20by%0Arehearsal%20training%20with%20a%20few%20temporally%20dense%20samples%20stored%20in%20episodic%0Amemory%2C%20which%20is%20memory-inefficient.%20Alternatively%2C%20some%20methods%20store%0Atemporally%20sparse%20samples%2C%20sacrificing%20essential%20temporal%20information%20and%0Athereby%20resulting%20in%20inferior%20performance.%20To%20address%20this%20trade-off%20between%0Amemory-efficiency%20and%20performance%2C%20we%20propose%20EpiSodic%20and%20SEmaNTIc%20memory%0AintegrAtion%20for%20video%20class-incremental%20Learning%20%28ESSENTIAL%29.%20ESSENTIAL%0Aconsists%20of%20episodic%20memory%20for%20storing%20temporally%20sparse%20features%20and%20semantic%0Amemory%20for%20storing%20general%20knowledge%20represented%20by%20learnable%20prompts.%20We%0Aintroduce%20a%20novel%20memory%20retrieval%20%28MR%29%20module%20that%20integrates%20episodic%20memory%0Aand%20semantic%20prompts%20through%20cross-attention%2C%20enabling%20the%20retrieval%20of%0Atemporally%20dense%20features%20from%20temporally%20sparse%20features.%20We%20rigorously%0Avalidate%20ESSENTIAL%20on%20diverse%20datasets%3A%20UCF-101%2C%20HMDB51%2C%20and%0ASomething-Something-V2%20from%20the%20TCD%20benchmark%20and%20UCF-101%2C%20ActivityNet%2C%20and%0AKinetics-400%20from%20the%20vCLIMB%20benchmark.%20Remarkably%2C%20with%20significantly%20reduced%0Amemory%2C%20ESSENTIAL%20achieves%20favorable%20performance%20on%20the%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DESSENTIAL%253A%2520Episodic%2520and%2520Semantic%2520Memory%2520Integration%2520for%2520Video%250A%2520%2520Class-Incremental%2520Learning%26entry.906535625%3DJongseo%2520Lee%2520and%2520Kyungho%2520Bae%2520and%2520Kyle%2520Min%2520and%2520Gyeong-Moon%2520Park%2520and%2520Jinwoo%2520Choi%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520tackle%2520the%2520problem%2520of%2520video%2520classincremental%2520learning%250A%2528VCIL%2529.%2520Many%2520existing%2520VCIL%2520methods%2520mitigate%2520catastrophic%2520forgetting%2520by%250Arehearsal%2520training%2520with%2520a%2520few%2520temporally%2520dense%2520samples%2520stored%2520in%2520episodic%250Amemory%252C%2520which%2520is%2520memory-inefficient.%2520Alternatively%252C%2520some%2520methods%2520store%250Atemporally%2520sparse%2520samples%252C%2520sacrificing%2520essential%2520temporal%2520information%2520and%250Athereby%2520resulting%2520in%2520inferior%2520performance.%2520To%2520address%2520this%2520trade-off%2520between%250Amemory-efficiency%2520and%2520performance%252C%2520we%2520propose%2520EpiSodic%2520and%2520SEmaNTIc%2520memory%250AintegrAtion%2520for%2520video%2520class-incremental%2520Learning%2520%2528ESSENTIAL%2529.%2520ESSENTIAL%250Aconsists%2520of%2520episodic%2520memory%2520for%2520storing%2520temporally%2520sparse%2520features%2520and%2520semantic%250Amemory%2520for%2520storing%2520general%2520knowledge%2520represented%2520by%2520learnable%2520prompts.%2520We%250Aintroduce%2520a%2520novel%2520memory%2520retrieval%2520%2528MR%2529%2520module%2520that%2520integrates%2520episodic%2520memory%250Aand%2520semantic%2520prompts%2520through%2520cross-attention%252C%2520enabling%2520the%2520retrieval%2520of%250Atemporally%2520dense%2520features%2520from%2520temporally%2520sparse%2520features.%2520We%2520rigorously%250Avalidate%2520ESSENTIAL%2520on%2520diverse%2520datasets%253A%2520UCF-101%252C%2520HMDB51%252C%2520and%250ASomething-Something-V2%2520from%2520the%2520TCD%2520benchmark%2520and%2520UCF-101%252C%2520ActivityNet%252C%2520and%250AKinetics-400%2520from%2520the%2520vCLIMB%2520benchmark.%2520Remarkably%252C%2520with%2520significantly%2520reduced%250Amemory%252C%2520ESSENTIAL%2520achieves%2520favorable%2520performance%2520on%2520the%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ESSENTIAL%3A%20Episodic%20and%20Semantic%20Memory%20Integration%20for%20Video%0A%20%20Class-Incremental%20Learning&entry.906535625=Jongseo%20Lee%20and%20Kyungho%20Bae%20and%20Kyle%20Min%20and%20Gyeong-Moon%20Park%20and%20Jinwoo%20Choi&entry.1292438233=%20%20In%20this%20work%2C%20we%20tackle%20the%20problem%20of%20video%20classincremental%20learning%0A%28VCIL%29.%20Many%20existing%20VCIL%20methods%20mitigate%20catastrophic%20forgetting%20by%0Arehearsal%20training%20with%20a%20few%20temporally%20dense%20samples%20stored%20in%20episodic%0Amemory%2C%20which%20is%20memory-inefficient.%20Alternatively%2C%20some%20methods%20store%0Atemporally%20sparse%20samples%2C%20sacrificing%20essential%20temporal%20information%20and%0Athereby%20resulting%20in%20inferior%20performance.%20To%20address%20this%20trade-off%20between%0Amemory-efficiency%20and%20performance%2C%20we%20propose%20EpiSodic%20and%20SEmaNTIc%20memory%0AintegrAtion%20for%20video%20class-incremental%20Learning%20%28ESSENTIAL%29.%20ESSENTIAL%0Aconsists%20of%20episodic%20memory%20for%20storing%20temporally%20sparse%20features%20and%20semantic%0Amemory%20for%20storing%20general%20knowledge%20represented%20by%20learnable%20prompts.%20We%0Aintroduce%20a%20novel%20memory%20retrieval%20%28MR%29%20module%20that%20integrates%20episodic%20memory%0Aand%20semantic%20prompts%20through%20cross-attention%2C%20enabling%20the%20retrieval%20of%0Atemporally%20dense%20features%20from%20temporally%20sparse%20features.%20We%20rigorously%0Avalidate%20ESSENTIAL%20on%20diverse%20datasets%3A%20UCF-101%2C%20HMDB51%2C%20and%0ASomething-Something-V2%20from%20the%20TCD%20benchmark%20and%20UCF-101%2C%20ActivityNet%2C%20and%0AKinetics-400%20from%20the%20vCLIMB%20benchmark.%20Remarkably%2C%20with%20significantly%20reduced%0Amemory%2C%20ESSENTIAL%20achieves%20favorable%20performance%20on%20the%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10896v1&entry.124074799=Read"},
{"title": "Quantum Visual Fields with Neural Amplitude Encoding", "author": "Shuteng Wang and Christian Theobalt and Vladislav Golyanik", "abstract": "  Quantum Implicit Neural Representations (QINRs) include components for\nlearning and execution on gate-based quantum computers. While QINRs recently\nemerged as a promising new paradigm, many challenges concerning their\narchitecture and ansatz design, the utility of quantum-mechanical properties,\ntraining efficiency and the interplay with classical modules remain. This paper\nadvances the field by introducing a new type of QINR for 2D image and 3D\ngeometric field learning, which we collectively refer to as Quantum Visual\nField (QVF). QVF encodes classical data into quantum statevectors using neural\namplitude encoding grounded in a learnable energy manifold, ensuring meaningful\nHilbert space embeddings. Our ansatz follows a fully entangled design of\nlearnable parametrised quantum circuits, with quantum (unitary) operations\nperformed in the real Hilbert space, resulting in numerically stable training\nwith fast convergence. QVF does not rely on classical post-processing -- in\ncontrast to the previous QINR learning approach -- and directly employs\nprojective measurement to extract learned signals encoded in the ansatz.\nExperiments on a quantum hardware simulator demonstrate that QVF outperforms\nthe existing quantum approach and widely used classical foundational baselines\nin terms of visual representation accuracy across various metrics and model\ncharacteristics, such as learning of high-frequency details. We also show\napplications of QVF in 2D and 3D field completion and 3D shape interpolation,\nhighlighting its practical potential.\n", "link": "http://arxiv.org/abs/2508.10900v1", "date": "2025-08-14", "relevancy": 2.6587, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Visual%20Fields%20with%20Neural%20Amplitude%20Encoding&body=Title%3A%20Quantum%20Visual%20Fields%20with%20Neural%20Amplitude%20Encoding%0AAuthor%3A%20Shuteng%20Wang%20and%20Christian%20Theobalt%20and%20Vladislav%20Golyanik%0AAbstract%3A%20%20%20Quantum%20Implicit%20Neural%20Representations%20%28QINRs%29%20include%20components%20for%0Alearning%20and%20execution%20on%20gate-based%20quantum%20computers.%20While%20QINRs%20recently%0Aemerged%20as%20a%20promising%20new%20paradigm%2C%20many%20challenges%20concerning%20their%0Aarchitecture%20and%20ansatz%20design%2C%20the%20utility%20of%20quantum-mechanical%20properties%2C%0Atraining%20efficiency%20and%20the%20interplay%20with%20classical%20modules%20remain.%20This%20paper%0Aadvances%20the%20field%20by%20introducing%20a%20new%20type%20of%20QINR%20for%202D%20image%20and%203D%0Ageometric%20field%20learning%2C%20which%20we%20collectively%20refer%20to%20as%20Quantum%20Visual%0AField%20%28QVF%29.%20QVF%20encodes%20classical%20data%20into%20quantum%20statevectors%20using%20neural%0Aamplitude%20encoding%20grounded%20in%20a%20learnable%20energy%20manifold%2C%20ensuring%20meaningful%0AHilbert%20space%20embeddings.%20Our%20ansatz%20follows%20a%20fully%20entangled%20design%20of%0Alearnable%20parametrised%20quantum%20circuits%2C%20with%20quantum%20%28unitary%29%20operations%0Aperformed%20in%20the%20real%20Hilbert%20space%2C%20resulting%20in%20numerically%20stable%20training%0Awith%20fast%20convergence.%20QVF%20does%20not%20rely%20on%20classical%20post-processing%20--%20in%0Acontrast%20to%20the%20previous%20QINR%20learning%20approach%20--%20and%20directly%20employs%0Aprojective%20measurement%20to%20extract%20learned%20signals%20encoded%20in%20the%20ansatz.%0AExperiments%20on%20a%20quantum%20hardware%20simulator%20demonstrate%20that%20QVF%20outperforms%0Athe%20existing%20quantum%20approach%20and%20widely%20used%20classical%20foundational%20baselines%0Ain%20terms%20of%20visual%20representation%20accuracy%20across%20various%20metrics%20and%20model%0Acharacteristics%2C%20such%20as%20learning%20of%20high-frequency%20details.%20We%20also%20show%0Aapplications%20of%20QVF%20in%202D%20and%203D%20field%20completion%20and%203D%20shape%20interpolation%2C%0Ahighlighting%20its%20practical%20potential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10900v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Visual%2520Fields%2520with%2520Neural%2520Amplitude%2520Encoding%26entry.906535625%3DShuteng%2520Wang%2520and%2520Christian%2520Theobalt%2520and%2520Vladislav%2520Golyanik%26entry.1292438233%3D%2520%2520Quantum%2520Implicit%2520Neural%2520Representations%2520%2528QINRs%2529%2520include%2520components%2520for%250Alearning%2520and%2520execution%2520on%2520gate-based%2520quantum%2520computers.%2520While%2520QINRs%2520recently%250Aemerged%2520as%2520a%2520promising%2520new%2520paradigm%252C%2520many%2520challenges%2520concerning%2520their%250Aarchitecture%2520and%2520ansatz%2520design%252C%2520the%2520utility%2520of%2520quantum-mechanical%2520properties%252C%250Atraining%2520efficiency%2520and%2520the%2520interplay%2520with%2520classical%2520modules%2520remain.%2520This%2520paper%250Aadvances%2520the%2520field%2520by%2520introducing%2520a%2520new%2520type%2520of%2520QINR%2520for%25202D%2520image%2520and%25203D%250Ageometric%2520field%2520learning%252C%2520which%2520we%2520collectively%2520refer%2520to%2520as%2520Quantum%2520Visual%250AField%2520%2528QVF%2529.%2520QVF%2520encodes%2520classical%2520data%2520into%2520quantum%2520statevectors%2520using%2520neural%250Aamplitude%2520encoding%2520grounded%2520in%2520a%2520learnable%2520energy%2520manifold%252C%2520ensuring%2520meaningful%250AHilbert%2520space%2520embeddings.%2520Our%2520ansatz%2520follows%2520a%2520fully%2520entangled%2520design%2520of%250Alearnable%2520parametrised%2520quantum%2520circuits%252C%2520with%2520quantum%2520%2528unitary%2529%2520operations%250Aperformed%2520in%2520the%2520real%2520Hilbert%2520space%252C%2520resulting%2520in%2520numerically%2520stable%2520training%250Awith%2520fast%2520convergence.%2520QVF%2520does%2520not%2520rely%2520on%2520classical%2520post-processing%2520--%2520in%250Acontrast%2520to%2520the%2520previous%2520QINR%2520learning%2520approach%2520--%2520and%2520directly%2520employs%250Aprojective%2520measurement%2520to%2520extract%2520learned%2520signals%2520encoded%2520in%2520the%2520ansatz.%250AExperiments%2520on%2520a%2520quantum%2520hardware%2520simulator%2520demonstrate%2520that%2520QVF%2520outperforms%250Athe%2520existing%2520quantum%2520approach%2520and%2520widely%2520used%2520classical%2520foundational%2520baselines%250Ain%2520terms%2520of%2520visual%2520representation%2520accuracy%2520across%2520various%2520metrics%2520and%2520model%250Acharacteristics%252C%2520such%2520as%2520learning%2520of%2520high-frequency%2520details.%2520We%2520also%2520show%250Aapplications%2520of%2520QVF%2520in%25202D%2520and%25203D%2520field%2520completion%2520and%25203D%2520shape%2520interpolation%252C%250Ahighlighting%2520its%2520practical%2520potential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10900v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Visual%20Fields%20with%20Neural%20Amplitude%20Encoding&entry.906535625=Shuteng%20Wang%20and%20Christian%20Theobalt%20and%20Vladislav%20Golyanik&entry.1292438233=%20%20Quantum%20Implicit%20Neural%20Representations%20%28QINRs%29%20include%20components%20for%0Alearning%20and%20execution%20on%20gate-based%20quantum%20computers.%20While%20QINRs%20recently%0Aemerged%20as%20a%20promising%20new%20paradigm%2C%20many%20challenges%20concerning%20their%0Aarchitecture%20and%20ansatz%20design%2C%20the%20utility%20of%20quantum-mechanical%20properties%2C%0Atraining%20efficiency%20and%20the%20interplay%20with%20classical%20modules%20remain.%20This%20paper%0Aadvances%20the%20field%20by%20introducing%20a%20new%20type%20of%20QINR%20for%202D%20image%20and%203D%0Ageometric%20field%20learning%2C%20which%20we%20collectively%20refer%20to%20as%20Quantum%20Visual%0AField%20%28QVF%29.%20QVF%20encodes%20classical%20data%20into%20quantum%20statevectors%20using%20neural%0Aamplitude%20encoding%20grounded%20in%20a%20learnable%20energy%20manifold%2C%20ensuring%20meaningful%0AHilbert%20space%20embeddings.%20Our%20ansatz%20follows%20a%20fully%20entangled%20design%20of%0Alearnable%20parametrised%20quantum%20circuits%2C%20with%20quantum%20%28unitary%29%20operations%0Aperformed%20in%20the%20real%20Hilbert%20space%2C%20resulting%20in%20numerically%20stable%20training%0Awith%20fast%20convergence.%20QVF%20does%20not%20rely%20on%20classical%20post-processing%20--%20in%0Acontrast%20to%20the%20previous%20QINR%20learning%20approach%20--%20and%20directly%20employs%0Aprojective%20measurement%20to%20extract%20learned%20signals%20encoded%20in%20the%20ansatz.%0AExperiments%20on%20a%20quantum%20hardware%20simulator%20demonstrate%20that%20QVF%20outperforms%0Athe%20existing%20quantum%20approach%20and%20widely%20used%20classical%20foundational%20baselines%0Ain%20terms%20of%20visual%20representation%20accuracy%20across%20various%20metrics%20and%20model%0Acharacteristics%2C%20such%20as%20learning%20of%20high-frequency%20details.%20We%20also%20show%0Aapplications%20of%20QVF%20in%202D%20and%203D%20field%20completion%20and%203D%20shape%20interpolation%2C%0Ahighlighting%20its%20practical%20potential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10900v1&entry.124074799=Read"},
{"title": "From Black Box to Transparency: Enhancing Automated Interpreting\n  Assessment with Explainable AI in College Classrooms", "author": "Zhaokun Jiang and Ziyin Zhang", "abstract": "  Recent advancements in machine learning have spurred growing interests in\nautomated interpreting quality assessment. Nevertheless, existing research\nsuffers from insufficient examination of language use quality, unsatisfactory\nmodeling effectiveness due to data scarcity and imbalance, and a lack of\nefforts to explain model predictions. To address these gaps, we propose a\nmulti-dimensional modeling framework that integrates feature engineering, data\naugmentation, and explainable machine learning. This approach prioritizes\nexplainability over ``black box'' predictions by utilizing only\nconstruct-relevant, transparent features and conducting Shapley Value (SHAP)\nanalysis. Our results demonstrate strong predictive performance on a novel\nEnglish-Chinese consecutive interpreting dataset, identifying BLEURT and\nCometKiwi scores to be the strongest predictive features for fidelity,\npause-related features for fluency, and Chinese-specific phraseological\ndiversity metrics for language use. Overall, by placing particular emphasis on\nexplainability, we present a scalable, reliable, and transparent alternative to\ntraditional human evaluation, facilitating the provision of detailed diagnostic\nfeedback for learners and supporting self-regulated learning advantages not\nafforded by automated scores in isolation.\n", "link": "http://arxiv.org/abs/2508.10860v1", "date": "2025-08-14", "relevancy": 2.6527, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5321}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5321}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Black%20Box%20to%20Transparency%3A%20Enhancing%20Automated%20Interpreting%0A%20%20Assessment%20with%20Explainable%20AI%20in%20College%20Classrooms&body=Title%3A%20From%20Black%20Box%20to%20Transparency%3A%20Enhancing%20Automated%20Interpreting%0A%20%20Assessment%20with%20Explainable%20AI%20in%20College%20Classrooms%0AAuthor%3A%20Zhaokun%20Jiang%20and%20Ziyin%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20machine%20learning%20have%20spurred%20growing%20interests%20in%0Aautomated%20interpreting%20quality%20assessment.%20Nevertheless%2C%20existing%20research%0Asuffers%20from%20insufficient%20examination%20of%20language%20use%20quality%2C%20unsatisfactory%0Amodeling%20effectiveness%20due%20to%20data%20scarcity%20and%20imbalance%2C%20and%20a%20lack%20of%0Aefforts%20to%20explain%20model%20predictions.%20To%20address%20these%20gaps%2C%20we%20propose%20a%0Amulti-dimensional%20modeling%20framework%20that%20integrates%20feature%20engineering%2C%20data%0Aaugmentation%2C%20and%20explainable%20machine%20learning.%20This%20approach%20prioritizes%0Aexplainability%20over%20%60%60black%20box%27%27%20predictions%20by%20utilizing%20only%0Aconstruct-relevant%2C%20transparent%20features%20and%20conducting%20Shapley%20Value%20%28SHAP%29%0Aanalysis.%20Our%20results%20demonstrate%20strong%20predictive%20performance%20on%20a%20novel%0AEnglish-Chinese%20consecutive%20interpreting%20dataset%2C%20identifying%20BLEURT%20and%0ACometKiwi%20scores%20to%20be%20the%20strongest%20predictive%20features%20for%20fidelity%2C%0Apause-related%20features%20for%20fluency%2C%20and%20Chinese-specific%20phraseological%0Adiversity%20metrics%20for%20language%20use.%20Overall%2C%20by%20placing%20particular%20emphasis%20on%0Aexplainability%2C%20we%20present%20a%20scalable%2C%20reliable%2C%20and%20transparent%20alternative%20to%0Atraditional%20human%20evaluation%2C%20facilitating%20the%20provision%20of%20detailed%20diagnostic%0Afeedback%20for%20learners%20and%20supporting%20self-regulated%20learning%20advantages%20not%0Aafforded%20by%20automated%20scores%20in%20isolation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Black%2520Box%2520to%2520Transparency%253A%2520Enhancing%2520Automated%2520Interpreting%250A%2520%2520Assessment%2520with%2520Explainable%2520AI%2520in%2520College%2520Classrooms%26entry.906535625%3DZhaokun%2520Jiang%2520and%2520Ziyin%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520machine%2520learning%2520have%2520spurred%2520growing%2520interests%2520in%250Aautomated%2520interpreting%2520quality%2520assessment.%2520Nevertheless%252C%2520existing%2520research%250Asuffers%2520from%2520insufficient%2520examination%2520of%2520language%2520use%2520quality%252C%2520unsatisfactory%250Amodeling%2520effectiveness%2520due%2520to%2520data%2520scarcity%2520and%2520imbalance%252C%2520and%2520a%2520lack%2520of%250Aefforts%2520to%2520explain%2520model%2520predictions.%2520To%2520address%2520these%2520gaps%252C%2520we%2520propose%2520a%250Amulti-dimensional%2520modeling%2520framework%2520that%2520integrates%2520feature%2520engineering%252C%2520data%250Aaugmentation%252C%2520and%2520explainable%2520machine%2520learning.%2520This%2520approach%2520prioritizes%250Aexplainability%2520over%2520%2560%2560black%2520box%2527%2527%2520predictions%2520by%2520utilizing%2520only%250Aconstruct-relevant%252C%2520transparent%2520features%2520and%2520conducting%2520Shapley%2520Value%2520%2528SHAP%2529%250Aanalysis.%2520Our%2520results%2520demonstrate%2520strong%2520predictive%2520performance%2520on%2520a%2520novel%250AEnglish-Chinese%2520consecutive%2520interpreting%2520dataset%252C%2520identifying%2520BLEURT%2520and%250ACometKiwi%2520scores%2520to%2520be%2520the%2520strongest%2520predictive%2520features%2520for%2520fidelity%252C%250Apause-related%2520features%2520for%2520fluency%252C%2520and%2520Chinese-specific%2520phraseological%250Adiversity%2520metrics%2520for%2520language%2520use.%2520Overall%252C%2520by%2520placing%2520particular%2520emphasis%2520on%250Aexplainability%252C%2520we%2520present%2520a%2520scalable%252C%2520reliable%252C%2520and%2520transparent%2520alternative%2520to%250Atraditional%2520human%2520evaluation%252C%2520facilitating%2520the%2520provision%2520of%2520detailed%2520diagnostic%250Afeedback%2520for%2520learners%2520and%2520supporting%2520self-regulated%2520learning%2520advantages%2520not%250Aafforded%2520by%2520automated%2520scores%2520in%2520isolation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Black%20Box%20to%20Transparency%3A%20Enhancing%20Automated%20Interpreting%0A%20%20Assessment%20with%20Explainable%20AI%20in%20College%20Classrooms&entry.906535625=Zhaokun%20Jiang%20and%20Ziyin%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%20machine%20learning%20have%20spurred%20growing%20interests%20in%0Aautomated%20interpreting%20quality%20assessment.%20Nevertheless%2C%20existing%20research%0Asuffers%20from%20insufficient%20examination%20of%20language%20use%20quality%2C%20unsatisfactory%0Amodeling%20effectiveness%20due%20to%20data%20scarcity%20and%20imbalance%2C%20and%20a%20lack%20of%0Aefforts%20to%20explain%20model%20predictions.%20To%20address%20these%20gaps%2C%20we%20propose%20a%0Amulti-dimensional%20modeling%20framework%20that%20integrates%20feature%20engineering%2C%20data%0Aaugmentation%2C%20and%20explainable%20machine%20learning.%20This%20approach%20prioritizes%0Aexplainability%20over%20%60%60black%20box%27%27%20predictions%20by%20utilizing%20only%0Aconstruct-relevant%2C%20transparent%20features%20and%20conducting%20Shapley%20Value%20%28SHAP%29%0Aanalysis.%20Our%20results%20demonstrate%20strong%20predictive%20performance%20on%20a%20novel%0AEnglish-Chinese%20consecutive%20interpreting%20dataset%2C%20identifying%20BLEURT%20and%0ACometKiwi%20scores%20to%20be%20the%20strongest%20predictive%20features%20for%20fidelity%2C%0Apause-related%20features%20for%20fluency%2C%20and%20Chinese-specific%20phraseological%0Adiversity%20metrics%20for%20language%20use.%20Overall%2C%20by%20placing%20particular%20emphasis%20on%0Aexplainability%2C%20we%20present%20a%20scalable%2C%20reliable%2C%20and%20transparent%20alternative%20to%0Atraditional%20human%20evaluation%2C%20facilitating%20the%20provision%20of%20detailed%20diagnostic%0Afeedback%20for%20learners%20and%20supporting%20self-regulated%20learning%20advantages%20not%0Aafforded%20by%20automated%20scores%20in%20isolation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10860v1&entry.124074799=Read"},
{"title": "OpenCUA: Open Foundations for Computer-Use Agents", "author": "Xinyuan Wang and Bowen Wang and Dunjie Lu and Junlin Yang and Tianbao Xie and Junli Wang and Jiaqi Deng and Xiaole Guo and Yiheng Xu and Chen Henry Wu and Zhennan Shen and Zhuokai Li and Ryan Li and Xiaochuan Li and Junda Chen and Boyuan Zheng and Peihang Li and Fangyu Lei and Ruisheng Cao and Yeqiao Fu and Dongchan Shin and Martin Shin and Jiarui Hu and Yuyan Wang and Jixuan Chen and Yuxiao Ye and Danyang Zhang and Dikang Du and Hao Hu and Huarong Chen and Zaida Zhou and Haotian Yao and Ziwei Chen and Qizheng Gu and Yipu Wang and Heng Wang and Diyi Yang and Victor Zhong and Flood Sung and Y. Charles and Zhilin Yang and Tao Yu", "abstract": "  Vision-language models have demonstrated impressive capabilities as\ncomputer-use agents (CUAs) capable of automating diverse computer tasks. As\ntheir commercial potential grows, critical details of the most capable CUA\nsystems remain closed. As these agents will increasingly mediate digital\ninteractions and execute consequential decisions on our behalf, the research\ncommunity needs access to open CUA frameworks to study their capabilities,\nlimitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive\nopen-source framework for scaling CUA data and foundation models. Our framework\nconsists of: (1) an annotation infrastructure that seamlessly captures human\ncomputer-use demonstrations; (2) AgentNet, the first large-scale computer-use\ntask dataset spanning 3 operating systems and 200+ applications and websites;\n(3) a scalable pipeline that transforms demonstrations into state-action pairs\nwith reflective long Chain-of-Thought reasoning that sustain robust performance\ngains as data scales. Our end-to-end agent models demonstrate strong\nperformance across CUA benchmarks. In particular, OpenCUA-32B achieves an\naverage success rate of 34.8% on OSWorld-Verified, establishing a new\nstate-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA\n(GPT-4o). Further analysis confirms that our approach generalizes well across\ndomains and benefits significantly from increased test-time computation. We\nrelease our annotation tool, datasets, code, and models to build open\nfoundations for further CUA research.\n", "link": "http://arxiv.org/abs/2508.09123v2", "date": "2025-08-14", "relevancy": 2.6525, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5399}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenCUA%3A%20Open%20Foundations%20for%20Computer-Use%20Agents&body=Title%3A%20OpenCUA%3A%20Open%20Foundations%20for%20Computer-Use%20Agents%0AAuthor%3A%20Xinyuan%20Wang%20and%20Bowen%20Wang%20and%20Dunjie%20Lu%20and%20Junlin%20Yang%20and%20Tianbao%20Xie%20and%20Junli%20Wang%20and%20Jiaqi%20Deng%20and%20Xiaole%20Guo%20and%20Yiheng%20Xu%20and%20Chen%20Henry%20Wu%20and%20Zhennan%20Shen%20and%20Zhuokai%20Li%20and%20Ryan%20Li%20and%20Xiaochuan%20Li%20and%20Junda%20Chen%20and%20Boyuan%20Zheng%20and%20Peihang%20Li%20and%20Fangyu%20Lei%20and%20Ruisheng%20Cao%20and%20Yeqiao%20Fu%20and%20Dongchan%20Shin%20and%20Martin%20Shin%20and%20Jiarui%20Hu%20and%20Yuyan%20Wang%20and%20Jixuan%20Chen%20and%20Yuxiao%20Ye%20and%20Danyang%20Zhang%20and%20Dikang%20Du%20and%20Hao%20Hu%20and%20Huarong%20Chen%20and%20Zaida%20Zhou%20and%20Haotian%20Yao%20and%20Ziwei%20Chen%20and%20Qizheng%20Gu%20and%20Yipu%20Wang%20and%20Heng%20Wang%20and%20Diyi%20Yang%20and%20Victor%20Zhong%20and%20Flood%20Sung%20and%20Y.%20Charles%20and%20Zhilin%20Yang%20and%20Tao%20Yu%0AAbstract%3A%20%20%20Vision-language%20models%20have%20demonstrated%20impressive%20capabilities%20as%0Acomputer-use%20agents%20%28CUAs%29%20capable%20of%20automating%20diverse%20computer%20tasks.%20As%0Atheir%20commercial%20potential%20grows%2C%20critical%20details%20of%20the%20most%20capable%20CUA%0Asystems%20remain%20closed.%20As%20these%20agents%20will%20increasingly%20mediate%20digital%0Ainteractions%20and%20execute%20consequential%20decisions%20on%20our%20behalf%2C%20the%20research%0Acommunity%20needs%20access%20to%20open%20CUA%20frameworks%20to%20study%20their%20capabilities%2C%0Alimitations%2C%20and%20risks.%20To%20bridge%20this%20gap%2C%20we%20propose%20OpenCUA%2C%20a%20comprehensive%0Aopen-source%20framework%20for%20scaling%20CUA%20data%20and%20foundation%20models.%20Our%20framework%0Aconsists%20of%3A%20%281%29%20an%20annotation%20infrastructure%20that%20seamlessly%20captures%20human%0Acomputer-use%20demonstrations%3B%20%282%29%20AgentNet%2C%20the%20first%20large-scale%20computer-use%0Atask%20dataset%20spanning%203%20operating%20systems%20and%20200%2B%20applications%20and%20websites%3B%0A%283%29%20a%20scalable%20pipeline%20that%20transforms%20demonstrations%20into%20state-action%20pairs%0Awith%20reflective%20long%20Chain-of-Thought%20reasoning%20that%20sustain%20robust%20performance%0Agains%20as%20data%20scales.%20Our%20end-to-end%20agent%20models%20demonstrate%20strong%0Aperformance%20across%20CUA%20benchmarks.%20In%20particular%2C%20OpenCUA-32B%20achieves%20an%0Aaverage%20success%20rate%20of%2034.8%25%20on%20OSWorld-Verified%2C%20establishing%20a%20new%0Astate-of-the-art%20%28SOTA%29%20among%20open-source%20models%20and%20surpassing%20OpenAI%20CUA%0A%28GPT-4o%29.%20Further%20analysis%20confirms%20that%20our%20approach%20generalizes%20well%20across%0Adomains%20and%20benefits%20significantly%20from%20increased%20test-time%20computation.%20We%0Arelease%20our%20annotation%20tool%2C%20datasets%2C%20code%2C%20and%20models%20to%20build%20open%0Afoundations%20for%20further%20CUA%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09123v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenCUA%253A%2520Open%2520Foundations%2520for%2520Computer-Use%2520Agents%26entry.906535625%3DXinyuan%2520Wang%2520and%2520Bowen%2520Wang%2520and%2520Dunjie%2520Lu%2520and%2520Junlin%2520Yang%2520and%2520Tianbao%2520Xie%2520and%2520Junli%2520Wang%2520and%2520Jiaqi%2520Deng%2520and%2520Xiaole%2520Guo%2520and%2520Yiheng%2520Xu%2520and%2520Chen%2520Henry%2520Wu%2520and%2520Zhennan%2520Shen%2520and%2520Zhuokai%2520Li%2520and%2520Ryan%2520Li%2520and%2520Xiaochuan%2520Li%2520and%2520Junda%2520Chen%2520and%2520Boyuan%2520Zheng%2520and%2520Peihang%2520Li%2520and%2520Fangyu%2520Lei%2520and%2520Ruisheng%2520Cao%2520and%2520Yeqiao%2520Fu%2520and%2520Dongchan%2520Shin%2520and%2520Martin%2520Shin%2520and%2520Jiarui%2520Hu%2520and%2520Yuyan%2520Wang%2520and%2520Jixuan%2520Chen%2520and%2520Yuxiao%2520Ye%2520and%2520Danyang%2520Zhang%2520and%2520Dikang%2520Du%2520and%2520Hao%2520Hu%2520and%2520Huarong%2520Chen%2520and%2520Zaida%2520Zhou%2520and%2520Haotian%2520Yao%2520and%2520Ziwei%2520Chen%2520and%2520Qizheng%2520Gu%2520and%2520Yipu%2520Wang%2520and%2520Heng%2520Wang%2520and%2520Diyi%2520Yang%2520and%2520Victor%2520Zhong%2520and%2520Flood%2520Sung%2520and%2520Y.%2520Charles%2520and%2520Zhilin%2520Yang%2520and%2520Tao%2520Yu%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520have%2520demonstrated%2520impressive%2520capabilities%2520as%250Acomputer-use%2520agents%2520%2528CUAs%2529%2520capable%2520of%2520automating%2520diverse%2520computer%2520tasks.%2520As%250Atheir%2520commercial%2520potential%2520grows%252C%2520critical%2520details%2520of%2520the%2520most%2520capable%2520CUA%250Asystems%2520remain%2520closed.%2520As%2520these%2520agents%2520will%2520increasingly%2520mediate%2520digital%250Ainteractions%2520and%2520execute%2520consequential%2520decisions%2520on%2520our%2520behalf%252C%2520the%2520research%250Acommunity%2520needs%2520access%2520to%2520open%2520CUA%2520frameworks%2520to%2520study%2520their%2520capabilities%252C%250Alimitations%252C%2520and%2520risks.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520OpenCUA%252C%2520a%2520comprehensive%250Aopen-source%2520framework%2520for%2520scaling%2520CUA%2520data%2520and%2520foundation%2520models.%2520Our%2520framework%250Aconsists%2520of%253A%2520%25281%2529%2520an%2520annotation%2520infrastructure%2520that%2520seamlessly%2520captures%2520human%250Acomputer-use%2520demonstrations%253B%2520%25282%2529%2520AgentNet%252C%2520the%2520first%2520large-scale%2520computer-use%250Atask%2520dataset%2520spanning%25203%2520operating%2520systems%2520and%2520200%252B%2520applications%2520and%2520websites%253B%250A%25283%2529%2520a%2520scalable%2520pipeline%2520that%2520transforms%2520demonstrations%2520into%2520state-action%2520pairs%250Awith%2520reflective%2520long%2520Chain-of-Thought%2520reasoning%2520that%2520sustain%2520robust%2520performance%250Agains%2520as%2520data%2520scales.%2520Our%2520end-to-end%2520agent%2520models%2520demonstrate%2520strong%250Aperformance%2520across%2520CUA%2520benchmarks.%2520In%2520particular%252C%2520OpenCUA-32B%2520achieves%2520an%250Aaverage%2520success%2520rate%2520of%252034.8%2525%2520on%2520OSWorld-Verified%252C%2520establishing%2520a%2520new%250Astate-of-the-art%2520%2528SOTA%2529%2520among%2520open-source%2520models%2520and%2520surpassing%2520OpenAI%2520CUA%250A%2528GPT-4o%2529.%2520Further%2520analysis%2520confirms%2520that%2520our%2520approach%2520generalizes%2520well%2520across%250Adomains%2520and%2520benefits%2520significantly%2520from%2520increased%2520test-time%2520computation.%2520We%250Arelease%2520our%2520annotation%2520tool%252C%2520datasets%252C%2520code%252C%2520and%2520models%2520to%2520build%2520open%250Afoundations%2520for%2520further%2520CUA%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09123v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenCUA%3A%20Open%20Foundations%20for%20Computer-Use%20Agents&entry.906535625=Xinyuan%20Wang%20and%20Bowen%20Wang%20and%20Dunjie%20Lu%20and%20Junlin%20Yang%20and%20Tianbao%20Xie%20and%20Junli%20Wang%20and%20Jiaqi%20Deng%20and%20Xiaole%20Guo%20and%20Yiheng%20Xu%20and%20Chen%20Henry%20Wu%20and%20Zhennan%20Shen%20and%20Zhuokai%20Li%20and%20Ryan%20Li%20and%20Xiaochuan%20Li%20and%20Junda%20Chen%20and%20Boyuan%20Zheng%20and%20Peihang%20Li%20and%20Fangyu%20Lei%20and%20Ruisheng%20Cao%20and%20Yeqiao%20Fu%20and%20Dongchan%20Shin%20and%20Martin%20Shin%20and%20Jiarui%20Hu%20and%20Yuyan%20Wang%20and%20Jixuan%20Chen%20and%20Yuxiao%20Ye%20and%20Danyang%20Zhang%20and%20Dikang%20Du%20and%20Hao%20Hu%20and%20Huarong%20Chen%20and%20Zaida%20Zhou%20and%20Haotian%20Yao%20and%20Ziwei%20Chen%20and%20Qizheng%20Gu%20and%20Yipu%20Wang%20and%20Heng%20Wang%20and%20Diyi%20Yang%20and%20Victor%20Zhong%20and%20Flood%20Sung%20and%20Y.%20Charles%20and%20Zhilin%20Yang%20and%20Tao%20Yu&entry.1292438233=%20%20Vision-language%20models%20have%20demonstrated%20impressive%20capabilities%20as%0Acomputer-use%20agents%20%28CUAs%29%20capable%20of%20automating%20diverse%20computer%20tasks.%20As%0Atheir%20commercial%20potential%20grows%2C%20critical%20details%20of%20the%20most%20capable%20CUA%0Asystems%20remain%20closed.%20As%20these%20agents%20will%20increasingly%20mediate%20digital%0Ainteractions%20and%20execute%20consequential%20decisions%20on%20our%20behalf%2C%20the%20research%0Acommunity%20needs%20access%20to%20open%20CUA%20frameworks%20to%20study%20their%20capabilities%2C%0Alimitations%2C%20and%20risks.%20To%20bridge%20this%20gap%2C%20we%20propose%20OpenCUA%2C%20a%20comprehensive%0Aopen-source%20framework%20for%20scaling%20CUA%20data%20and%20foundation%20models.%20Our%20framework%0Aconsists%20of%3A%20%281%29%20an%20annotation%20infrastructure%20that%20seamlessly%20captures%20human%0Acomputer-use%20demonstrations%3B%20%282%29%20AgentNet%2C%20the%20first%20large-scale%20computer-use%0Atask%20dataset%20spanning%203%20operating%20systems%20and%20200%2B%20applications%20and%20websites%3B%0A%283%29%20a%20scalable%20pipeline%20that%20transforms%20demonstrations%20into%20state-action%20pairs%0Awith%20reflective%20long%20Chain-of-Thought%20reasoning%20that%20sustain%20robust%20performance%0Agains%20as%20data%20scales.%20Our%20end-to-end%20agent%20models%20demonstrate%20strong%0Aperformance%20across%20CUA%20benchmarks.%20In%20particular%2C%20OpenCUA-32B%20achieves%20an%0Aaverage%20success%20rate%20of%2034.8%25%20on%20OSWorld-Verified%2C%20establishing%20a%20new%0Astate-of-the-art%20%28SOTA%29%20among%20open-source%20models%20and%20surpassing%20OpenAI%20CUA%0A%28GPT-4o%29.%20Further%20analysis%20confirms%20that%20our%20approach%20generalizes%20well%20across%0Adomains%20and%20benefits%20significantly%20from%20increased%20test-time%20computation.%20We%0Arelease%20our%20annotation%20tool%2C%20datasets%2C%20code%2C%20and%20models%20to%20build%20open%0Afoundations%20for%20further%20CUA%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09123v2&entry.124074799=Read"},
{"title": "Fourier-Guided Attention Upsampling for Image Super-Resolution", "author": "Daejune Choi and Youchan No and Jinhyung Lee and Duksu Kim", "abstract": "  We propose Frequency-Guided Attention (FGA), a lightweight upsampling module\nfor single image super-resolution. Conventional upsamplers, such as Sub-Pixel\nConvolution, are efficient but frequently fail to reconstruct high-frequency\ndetails and introduce aliasing artifacts. FGA addresses these issues by\nintegrating (1) a Fourier feature-based Multi-Layer Perceptron (MLP) for\npositional frequency encoding, (2) a cross-resolution Correlation Attention\nLayer for adaptive spatial alignment, and (3) a frequency-domain L1 loss for\nspectral fidelity supervision. Adding merely 0.3M parameters, FGA consistently\nenhances performance across five diverse super-resolution backbones in both\nlightweight and full-capacity scenarios. Experimental results demonstrate\naverage PSNR gains of 0.12~0.14 dB and improved frequency-domain consistency by\nup to 29%, particularly evident on texture-rich datasets. Visual and spectral\nevaluations confirm FGA's effectiveness in reducing aliasing and preserving\nfine details, establishing it as a practical, scalable alternative to\ntraditional upsampling methods.\n", "link": "http://arxiv.org/abs/2508.10616v1", "date": "2025-08-14", "relevancy": 2.6359, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.571}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5176}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fourier-Guided%20Attention%20Upsampling%20for%20Image%20Super-Resolution&body=Title%3A%20Fourier-Guided%20Attention%20Upsampling%20for%20Image%20Super-Resolution%0AAuthor%3A%20Daejune%20Choi%20and%20Youchan%20No%20and%20Jinhyung%20Lee%20and%20Duksu%20Kim%0AAbstract%3A%20%20%20We%20propose%20Frequency-Guided%20Attention%20%28FGA%29%2C%20a%20lightweight%20upsampling%20module%0Afor%20single%20image%20super-resolution.%20Conventional%20upsamplers%2C%20such%20as%20Sub-Pixel%0AConvolution%2C%20are%20efficient%20but%20frequently%20fail%20to%20reconstruct%20high-frequency%0Adetails%20and%20introduce%20aliasing%20artifacts.%20FGA%20addresses%20these%20issues%20by%0Aintegrating%20%281%29%20a%20Fourier%20feature-based%20Multi-Layer%20Perceptron%20%28MLP%29%20for%0Apositional%20frequency%20encoding%2C%20%282%29%20a%20cross-resolution%20Correlation%20Attention%0ALayer%20for%20adaptive%20spatial%20alignment%2C%20and%20%283%29%20a%20frequency-domain%20L1%20loss%20for%0Aspectral%20fidelity%20supervision.%20Adding%20merely%200.3M%20parameters%2C%20FGA%20consistently%0Aenhances%20performance%20across%20five%20diverse%20super-resolution%20backbones%20in%20both%0Alightweight%20and%20full-capacity%20scenarios.%20Experimental%20results%20demonstrate%0Aaverage%20PSNR%20gains%20of%200.12~0.14%20dB%20and%20improved%20frequency-domain%20consistency%20by%0Aup%20to%2029%25%2C%20particularly%20evident%20on%20texture-rich%20datasets.%20Visual%20and%20spectral%0Aevaluations%20confirm%20FGA%27s%20effectiveness%20in%20reducing%20aliasing%20and%20preserving%0Afine%20details%2C%20establishing%20it%20as%20a%20practical%2C%20scalable%20alternative%20to%0Atraditional%20upsampling%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFourier-Guided%2520Attention%2520Upsampling%2520for%2520Image%2520Super-Resolution%26entry.906535625%3DDaejune%2520Choi%2520and%2520Youchan%2520No%2520and%2520Jinhyung%2520Lee%2520and%2520Duksu%2520Kim%26entry.1292438233%3D%2520%2520We%2520propose%2520Frequency-Guided%2520Attention%2520%2528FGA%2529%252C%2520a%2520lightweight%2520upsampling%2520module%250Afor%2520single%2520image%2520super-resolution.%2520Conventional%2520upsamplers%252C%2520such%2520as%2520Sub-Pixel%250AConvolution%252C%2520are%2520efficient%2520but%2520frequently%2520fail%2520to%2520reconstruct%2520high-frequency%250Adetails%2520and%2520introduce%2520aliasing%2520artifacts.%2520FGA%2520addresses%2520these%2520issues%2520by%250Aintegrating%2520%25281%2529%2520a%2520Fourier%2520feature-based%2520Multi-Layer%2520Perceptron%2520%2528MLP%2529%2520for%250Apositional%2520frequency%2520encoding%252C%2520%25282%2529%2520a%2520cross-resolution%2520Correlation%2520Attention%250ALayer%2520for%2520adaptive%2520spatial%2520alignment%252C%2520and%2520%25283%2529%2520a%2520frequency-domain%2520L1%2520loss%2520for%250Aspectral%2520fidelity%2520supervision.%2520Adding%2520merely%25200.3M%2520parameters%252C%2520FGA%2520consistently%250Aenhances%2520performance%2520across%2520five%2520diverse%2520super-resolution%2520backbones%2520in%2520both%250Alightweight%2520and%2520full-capacity%2520scenarios.%2520Experimental%2520results%2520demonstrate%250Aaverage%2520PSNR%2520gains%2520of%25200.12~0.14%2520dB%2520and%2520improved%2520frequency-domain%2520consistency%2520by%250Aup%2520to%252029%2525%252C%2520particularly%2520evident%2520on%2520texture-rich%2520datasets.%2520Visual%2520and%2520spectral%250Aevaluations%2520confirm%2520FGA%2527s%2520effectiveness%2520in%2520reducing%2520aliasing%2520and%2520preserving%250Afine%2520details%252C%2520establishing%2520it%2520as%2520a%2520practical%252C%2520scalable%2520alternative%2520to%250Atraditional%2520upsampling%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fourier-Guided%20Attention%20Upsampling%20for%20Image%20Super-Resolution&entry.906535625=Daejune%20Choi%20and%20Youchan%20No%20and%20Jinhyung%20Lee%20and%20Duksu%20Kim&entry.1292438233=%20%20We%20propose%20Frequency-Guided%20Attention%20%28FGA%29%2C%20a%20lightweight%20upsampling%20module%0Afor%20single%20image%20super-resolution.%20Conventional%20upsamplers%2C%20such%20as%20Sub-Pixel%0AConvolution%2C%20are%20efficient%20but%20frequently%20fail%20to%20reconstruct%20high-frequency%0Adetails%20and%20introduce%20aliasing%20artifacts.%20FGA%20addresses%20these%20issues%20by%0Aintegrating%20%281%29%20a%20Fourier%20feature-based%20Multi-Layer%20Perceptron%20%28MLP%29%20for%0Apositional%20frequency%20encoding%2C%20%282%29%20a%20cross-resolution%20Correlation%20Attention%0ALayer%20for%20adaptive%20spatial%20alignment%2C%20and%20%283%29%20a%20frequency-domain%20L1%20loss%20for%0Aspectral%20fidelity%20supervision.%20Adding%20merely%200.3M%20parameters%2C%20FGA%20consistently%0Aenhances%20performance%20across%20five%20diverse%20super-resolution%20backbones%20in%20both%0Alightweight%20and%20full-capacity%20scenarios.%20Experimental%20results%20demonstrate%0Aaverage%20PSNR%20gains%20of%200.12~0.14%20dB%20and%20improved%20frequency-domain%20consistency%20by%0Aup%20to%2029%25%2C%20particularly%20evident%20on%20texture-rich%20datasets.%20Visual%20and%20spectral%0Aevaluations%20confirm%20FGA%27s%20effectiveness%20in%20reducing%20aliasing%20and%20preserving%0Afine%20details%2C%20establishing%20it%20as%20a%20practical%2C%20scalable%20alternative%20to%0Atraditional%20upsampling%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10616v1&entry.124074799=Read"},
{"title": "Video-BLADE: Block-Sparse Attention Meets Step Distillation for\n  Efficient Video Generation", "author": "Youping Gu and Xiaolong Li and Yuhao Hu and Bohan Zhuang", "abstract": "  Diffusion transformers currently lead the field in high-quality video\ngeneration, but their slow iterative denoising process and prohibitive\nquadratic attention costs for long sequences create significant inference\nbottlenecks. While both step distillation and sparse attention mechanisms have\nshown promise as independent acceleration strategies, effectively combining\nthese approaches presents critical challenges -- training-free integration\nyields suboptimal results, while separately training sparse attention after\nstep distillation requires prohibitively expensive high-quality video data. To\novercome these limitations, we propose BLADE, an innovative data-free joint\ntraining framework that introduces: (1) an Adaptive Block-Sparse Attention\n(ASA) mechanism for dynamically generating content-aware sparsity masks to\nfocus computation on salient spatiotemporal features, and (2) a sparsity-aware\nstep distillation paradigm built upon Trajectory Distribution Matching (TDM)\nthat directly incorporates sparsity into the distillation process rather than\ntreating it as a separate compression step, with fast convergence. We validate\nBLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework\ndemonstrates remarkable efficiency gains across different scales. On\nWan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a\n50-step baseline. Moreover, on models such as CogVideoX-5B with short video\nsequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the\nacceleration is accompanied by a consistent quality improvement. On the\nVBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from\n0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further\ncorroborated by superior ratings in human evaluations. Our code and model\nweights are publicly available at: http://ziplab.co/BLADE-Homepage/.\n", "link": "http://arxiv.org/abs/2508.10774v1", "date": "2025-08-14", "relevancy": 2.5895, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6621}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6481}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-BLADE%3A%20Block-Sparse%20Attention%20Meets%20Step%20Distillation%20for%0A%20%20Efficient%20Video%20Generation&body=Title%3A%20Video-BLADE%3A%20Block-Sparse%20Attention%20Meets%20Step%20Distillation%20for%0A%20%20Efficient%20Video%20Generation%0AAuthor%3A%20Youping%20Gu%20and%20Xiaolong%20Li%20and%20Yuhao%20Hu%20and%20Bohan%20Zhuang%0AAbstract%3A%20%20%20Diffusion%20transformers%20currently%20lead%20the%20field%20in%20high-quality%20video%0Ageneration%2C%20but%20their%20slow%20iterative%20denoising%20process%20and%20prohibitive%0Aquadratic%20attention%20costs%20for%20long%20sequences%20create%20significant%20inference%0Abottlenecks.%20While%20both%20step%20distillation%20and%20sparse%20attention%20mechanisms%20have%0Ashown%20promise%20as%20independent%20acceleration%20strategies%2C%20effectively%20combining%0Athese%20approaches%20presents%20critical%20challenges%20--%20training-free%20integration%0Ayields%20suboptimal%20results%2C%20while%20separately%20training%20sparse%20attention%20after%0Astep%20distillation%20requires%20prohibitively%20expensive%20high-quality%20video%20data.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20BLADE%2C%20an%20innovative%20data-free%20joint%0Atraining%20framework%20that%20introduces%3A%20%281%29%20an%20Adaptive%20Block-Sparse%20Attention%0A%28ASA%29%20mechanism%20for%20dynamically%20generating%20content-aware%20sparsity%20masks%20to%0Afocus%20computation%20on%20salient%20spatiotemporal%20features%2C%20and%20%282%29%20a%20sparsity-aware%0Astep%20distillation%20paradigm%20built%20upon%20Trajectory%20Distribution%20Matching%20%28TDM%29%0Athat%20directly%20incorporates%20sparsity%20into%20the%20distillation%20process%20rather%20than%0Atreating%20it%20as%20a%20separate%20compression%20step%2C%20with%20fast%20convergence.%20We%20validate%0ABLADE%20on%20text-to-video%20models%20like%20CogVideoX-5B%20and%20Wan2.1-1.3B.%20Our%20framework%0Ademonstrates%20remarkable%20efficiency%20gains%20across%20different%20scales.%20On%0AWan2.1-1.3B%2C%20BLADE%20achieves%20a%2014.10x%20end-to-end%20inference%20acceleration%20over%20a%0A50-step%20baseline.%20Moreover%2C%20on%20models%20such%20as%20CogVideoX-5B%20with%20short%20video%0Asequence%20lengths%2C%20our%20framework%20delivers%20a%20robust%208.89x%20speedup.%20Crucially%2C%20the%0Aacceleration%20is%20accompanied%20by%20a%20consistent%20quality%20improvement.%20On%20the%0AVBench-2.0%20benchmark%2C%20BLADE%20boosts%20the%20score%20of%20CogVideoX-5B%20to%200.569%20%28from%0A0.534%29%20and%20Wan2.1-1.3B%20to%200.570%20%28from%200.563%29%2C%20results%20that%20are%20further%0Acorroborated%20by%20superior%20ratings%20in%20human%20evaluations.%20Our%20code%20and%20model%0Aweights%20are%20publicly%20available%20at%3A%20http%3A//ziplab.co/BLADE-Homepage/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-BLADE%253A%2520Block-Sparse%2520Attention%2520Meets%2520Step%2520Distillation%2520for%250A%2520%2520Efficient%2520Video%2520Generation%26entry.906535625%3DYouping%2520Gu%2520and%2520Xiaolong%2520Li%2520and%2520Yuhao%2520Hu%2520and%2520Bohan%2520Zhuang%26entry.1292438233%3D%2520%2520Diffusion%2520transformers%2520currently%2520lead%2520the%2520field%2520in%2520high-quality%2520video%250Ageneration%252C%2520but%2520their%2520slow%2520iterative%2520denoising%2520process%2520and%2520prohibitive%250Aquadratic%2520attention%2520costs%2520for%2520long%2520sequences%2520create%2520significant%2520inference%250Abottlenecks.%2520While%2520both%2520step%2520distillation%2520and%2520sparse%2520attention%2520mechanisms%2520have%250Ashown%2520promise%2520as%2520independent%2520acceleration%2520strategies%252C%2520effectively%2520combining%250Athese%2520approaches%2520presents%2520critical%2520challenges%2520--%2520training-free%2520integration%250Ayields%2520suboptimal%2520results%252C%2520while%2520separately%2520training%2520sparse%2520attention%2520after%250Astep%2520distillation%2520requires%2520prohibitively%2520expensive%2520high-quality%2520video%2520data.%2520To%250Aovercome%2520these%2520limitations%252C%2520we%2520propose%2520BLADE%252C%2520an%2520innovative%2520data-free%2520joint%250Atraining%2520framework%2520that%2520introduces%253A%2520%25281%2529%2520an%2520Adaptive%2520Block-Sparse%2520Attention%250A%2528ASA%2529%2520mechanism%2520for%2520dynamically%2520generating%2520content-aware%2520sparsity%2520masks%2520to%250Afocus%2520computation%2520on%2520salient%2520spatiotemporal%2520features%252C%2520and%2520%25282%2529%2520a%2520sparsity-aware%250Astep%2520distillation%2520paradigm%2520built%2520upon%2520Trajectory%2520Distribution%2520Matching%2520%2528TDM%2529%250Athat%2520directly%2520incorporates%2520sparsity%2520into%2520the%2520distillation%2520process%2520rather%2520than%250Atreating%2520it%2520as%2520a%2520separate%2520compression%2520step%252C%2520with%2520fast%2520convergence.%2520We%2520validate%250ABLADE%2520on%2520text-to-video%2520models%2520like%2520CogVideoX-5B%2520and%2520Wan2.1-1.3B.%2520Our%2520framework%250Ademonstrates%2520remarkable%2520efficiency%2520gains%2520across%2520different%2520scales.%2520On%250AWan2.1-1.3B%252C%2520BLADE%2520achieves%2520a%252014.10x%2520end-to-end%2520inference%2520acceleration%2520over%2520a%250A50-step%2520baseline.%2520Moreover%252C%2520on%2520models%2520such%2520as%2520CogVideoX-5B%2520with%2520short%2520video%250Asequence%2520lengths%252C%2520our%2520framework%2520delivers%2520a%2520robust%25208.89x%2520speedup.%2520Crucially%252C%2520the%250Aacceleration%2520is%2520accompanied%2520by%2520a%2520consistent%2520quality%2520improvement.%2520On%2520the%250AVBench-2.0%2520benchmark%252C%2520BLADE%2520boosts%2520the%2520score%2520of%2520CogVideoX-5B%2520to%25200.569%2520%2528from%250A0.534%2529%2520and%2520Wan2.1-1.3B%2520to%25200.570%2520%2528from%25200.563%2529%252C%2520results%2520that%2520are%2520further%250Acorroborated%2520by%2520superior%2520ratings%2520in%2520human%2520evaluations.%2520Our%2520code%2520and%2520model%250Aweights%2520are%2520publicly%2520available%2520at%253A%2520http%253A//ziplab.co/BLADE-Homepage/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-BLADE%3A%20Block-Sparse%20Attention%20Meets%20Step%20Distillation%20for%0A%20%20Efficient%20Video%20Generation&entry.906535625=Youping%20Gu%20and%20Xiaolong%20Li%20and%20Yuhao%20Hu%20and%20Bohan%20Zhuang&entry.1292438233=%20%20Diffusion%20transformers%20currently%20lead%20the%20field%20in%20high-quality%20video%0Ageneration%2C%20but%20their%20slow%20iterative%20denoising%20process%20and%20prohibitive%0Aquadratic%20attention%20costs%20for%20long%20sequences%20create%20significant%20inference%0Abottlenecks.%20While%20both%20step%20distillation%20and%20sparse%20attention%20mechanisms%20have%0Ashown%20promise%20as%20independent%20acceleration%20strategies%2C%20effectively%20combining%0Athese%20approaches%20presents%20critical%20challenges%20--%20training-free%20integration%0Ayields%20suboptimal%20results%2C%20while%20separately%20training%20sparse%20attention%20after%0Astep%20distillation%20requires%20prohibitively%20expensive%20high-quality%20video%20data.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20BLADE%2C%20an%20innovative%20data-free%20joint%0Atraining%20framework%20that%20introduces%3A%20%281%29%20an%20Adaptive%20Block-Sparse%20Attention%0A%28ASA%29%20mechanism%20for%20dynamically%20generating%20content-aware%20sparsity%20masks%20to%0Afocus%20computation%20on%20salient%20spatiotemporal%20features%2C%20and%20%282%29%20a%20sparsity-aware%0Astep%20distillation%20paradigm%20built%20upon%20Trajectory%20Distribution%20Matching%20%28TDM%29%0Athat%20directly%20incorporates%20sparsity%20into%20the%20distillation%20process%20rather%20than%0Atreating%20it%20as%20a%20separate%20compression%20step%2C%20with%20fast%20convergence.%20We%20validate%0ABLADE%20on%20text-to-video%20models%20like%20CogVideoX-5B%20and%20Wan2.1-1.3B.%20Our%20framework%0Ademonstrates%20remarkable%20efficiency%20gains%20across%20different%20scales.%20On%0AWan2.1-1.3B%2C%20BLADE%20achieves%20a%2014.10x%20end-to-end%20inference%20acceleration%20over%20a%0A50-step%20baseline.%20Moreover%2C%20on%20models%20such%20as%20CogVideoX-5B%20with%20short%20video%0Asequence%20lengths%2C%20our%20framework%20delivers%20a%20robust%208.89x%20speedup.%20Crucially%2C%20the%0Aacceleration%20is%20accompanied%20by%20a%20consistent%20quality%20improvement.%20On%20the%0AVBench-2.0%20benchmark%2C%20BLADE%20boosts%20the%20score%20of%20CogVideoX-5B%20to%200.569%20%28from%0A0.534%29%20and%20Wan2.1-1.3B%20to%200.570%20%28from%200.563%29%2C%20results%20that%20are%20further%0Acorroborated%20by%20superior%20ratings%20in%20human%20evaluations.%20Our%20code%20and%20model%0Aweights%20are%20publicly%20available%20at%3A%20http%3A//ziplab.co/BLADE-Homepage/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10774v1&entry.124074799=Read"},
{"title": "AR Surgical Navigation With Surface Tracing: Comparing\n  In-SitVisualization with Tool-Tracking Guidance for Neurosurgical\n  Applications", "author": "Marc J. Fischer and Jeffrey Potts and Gabriel Urreola and Dax Jones and Paolo Palmisciano and E. Bradley Strong and Branden Cord and Andrew D. Hernandez and Julia D. Sharma and E. Brandon Strong", "abstract": "  Augmented Reality (AR) surgical navigation systems are emerging as the next\ngeneration of intraoperative surgical guidance, promising to overcome\nlimitations of traditional navigation systems. However, known issues with AR\ndepth perception due to vergence-accommodation conflict and occlusion handling\nlimitations of the currently commercially available display technology present\nacute challenges in surgical settings where precision is paramount. This study\npresents a novel methodology for utilizing AR guidance to register anatomical\ntargets and provide real-time instrument navigation using placement of\nsimulated external ventricular drain catheters on a phantom model as the\nclinical scenario. The system registers target positions to the patient through\na novel surface tracing method and uses real-time infrared tool tracking to aid\nin catheter placement, relying only on the onboard sensors of the Microsoft\nHoloLens 2. A group of intended users performed the procedure of simulated\ninsertions under two AR guidance conditions: static in-situ visualization,\nwhere planned trajectories are overlaid directly onto the patient anatomy, and\nreal-time tool-tracking guidance, where live feedback of the catheter's pose is\nprovided relative to the plan. Following the insertion tests, computed\ntomography scans of the phantom models were acquired, allowing for evaluation\nof insertion accuracy, target deviation, angular error, and depth precision.\nSystem Usability Scale surveys assessed user experience and cognitive workload.\nTool-tracking guidance improved performance metrics across all accuracy\nmeasures and was preferred by users in subjective evaluations. A free copy of\nthis paper and all supplemental materials are available at\nhttps://bit.ly/45l89Hq.\n", "link": "http://arxiv.org/abs/2508.10554v1", "date": "2025-08-14", "relevancy": 2.5739, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5467}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5017}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AR%20Surgical%20Navigation%20With%20Surface%20Tracing%3A%20Comparing%0A%20%20In-SitVisualization%20with%20Tool-Tracking%20Guidance%20for%20Neurosurgical%0A%20%20Applications&body=Title%3A%20AR%20Surgical%20Navigation%20With%20Surface%20Tracing%3A%20Comparing%0A%20%20In-SitVisualization%20with%20Tool-Tracking%20Guidance%20for%20Neurosurgical%0A%20%20Applications%0AAuthor%3A%20Marc%20J.%20Fischer%20and%20Jeffrey%20Potts%20and%20Gabriel%20Urreola%20and%20Dax%20Jones%20and%20Paolo%20Palmisciano%20and%20E.%20Bradley%20Strong%20and%20Branden%20Cord%20and%20Andrew%20D.%20Hernandez%20and%20Julia%20D.%20Sharma%20and%20E.%20Brandon%20Strong%0AAbstract%3A%20%20%20Augmented%20Reality%20%28AR%29%20surgical%20navigation%20systems%20are%20emerging%20as%20the%20next%0Ageneration%20of%20intraoperative%20surgical%20guidance%2C%20promising%20to%20overcome%0Alimitations%20of%20traditional%20navigation%20systems.%20However%2C%20known%20issues%20with%20AR%0Adepth%20perception%20due%20to%20vergence-accommodation%20conflict%20and%20occlusion%20handling%0Alimitations%20of%20the%20currently%20commercially%20available%20display%20technology%20present%0Aacute%20challenges%20in%20surgical%20settings%20where%20precision%20is%20paramount.%20This%20study%0Apresents%20a%20novel%20methodology%20for%20utilizing%20AR%20guidance%20to%20register%20anatomical%0Atargets%20and%20provide%20real-time%20instrument%20navigation%20using%20placement%20of%0Asimulated%20external%20ventricular%20drain%20catheters%20on%20a%20phantom%20model%20as%20the%0Aclinical%20scenario.%20The%20system%20registers%20target%20positions%20to%20the%20patient%20through%0Aa%20novel%20surface%20tracing%20method%20and%20uses%20real-time%20infrared%20tool%20tracking%20to%20aid%0Ain%20catheter%20placement%2C%20relying%20only%20on%20the%20onboard%20sensors%20of%20the%20Microsoft%0AHoloLens%202.%20A%20group%20of%20intended%20users%20performed%20the%20procedure%20of%20simulated%0Ainsertions%20under%20two%20AR%20guidance%20conditions%3A%20static%20in-situ%20visualization%2C%0Awhere%20planned%20trajectories%20are%20overlaid%20directly%20onto%20the%20patient%20anatomy%2C%20and%0Areal-time%20tool-tracking%20guidance%2C%20where%20live%20feedback%20of%20the%20catheter%27s%20pose%20is%0Aprovided%20relative%20to%20the%20plan.%20Following%20the%20insertion%20tests%2C%20computed%0Atomography%20scans%20of%20the%20phantom%20models%20were%20acquired%2C%20allowing%20for%20evaluation%0Aof%20insertion%20accuracy%2C%20target%20deviation%2C%20angular%20error%2C%20and%20depth%20precision.%0ASystem%20Usability%20Scale%20surveys%20assessed%20user%20experience%20and%20cognitive%20workload.%0ATool-tracking%20guidance%20improved%20performance%20metrics%20across%20all%20accuracy%0Ameasures%20and%20was%20preferred%20by%20users%20in%20subjective%20evaluations.%20A%20free%20copy%20of%0Athis%20paper%20and%20all%20supplemental%20materials%20are%20available%20at%0Ahttps%3A//bit.ly/45l89Hq.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAR%2520Surgical%2520Navigation%2520With%2520Surface%2520Tracing%253A%2520Comparing%250A%2520%2520In-SitVisualization%2520with%2520Tool-Tracking%2520Guidance%2520for%2520Neurosurgical%250A%2520%2520Applications%26entry.906535625%3DMarc%2520J.%2520Fischer%2520and%2520Jeffrey%2520Potts%2520and%2520Gabriel%2520Urreola%2520and%2520Dax%2520Jones%2520and%2520Paolo%2520Palmisciano%2520and%2520E.%2520Bradley%2520Strong%2520and%2520Branden%2520Cord%2520and%2520Andrew%2520D.%2520Hernandez%2520and%2520Julia%2520D.%2520Sharma%2520and%2520E.%2520Brandon%2520Strong%26entry.1292438233%3D%2520%2520Augmented%2520Reality%2520%2528AR%2529%2520surgical%2520navigation%2520systems%2520are%2520emerging%2520as%2520the%2520next%250Ageneration%2520of%2520intraoperative%2520surgical%2520guidance%252C%2520promising%2520to%2520overcome%250Alimitations%2520of%2520traditional%2520navigation%2520systems.%2520However%252C%2520known%2520issues%2520with%2520AR%250Adepth%2520perception%2520due%2520to%2520vergence-accommodation%2520conflict%2520and%2520occlusion%2520handling%250Alimitations%2520of%2520the%2520currently%2520commercially%2520available%2520display%2520technology%2520present%250Aacute%2520challenges%2520in%2520surgical%2520settings%2520where%2520precision%2520is%2520paramount.%2520This%2520study%250Apresents%2520a%2520novel%2520methodology%2520for%2520utilizing%2520AR%2520guidance%2520to%2520register%2520anatomical%250Atargets%2520and%2520provide%2520real-time%2520instrument%2520navigation%2520using%2520placement%2520of%250Asimulated%2520external%2520ventricular%2520drain%2520catheters%2520on%2520a%2520phantom%2520model%2520as%2520the%250Aclinical%2520scenario.%2520The%2520system%2520registers%2520target%2520positions%2520to%2520the%2520patient%2520through%250Aa%2520novel%2520surface%2520tracing%2520method%2520and%2520uses%2520real-time%2520infrared%2520tool%2520tracking%2520to%2520aid%250Ain%2520catheter%2520placement%252C%2520relying%2520only%2520on%2520the%2520onboard%2520sensors%2520of%2520the%2520Microsoft%250AHoloLens%25202.%2520A%2520group%2520of%2520intended%2520users%2520performed%2520the%2520procedure%2520of%2520simulated%250Ainsertions%2520under%2520two%2520AR%2520guidance%2520conditions%253A%2520static%2520in-situ%2520visualization%252C%250Awhere%2520planned%2520trajectories%2520are%2520overlaid%2520directly%2520onto%2520the%2520patient%2520anatomy%252C%2520and%250Areal-time%2520tool-tracking%2520guidance%252C%2520where%2520live%2520feedback%2520of%2520the%2520catheter%2527s%2520pose%2520is%250Aprovided%2520relative%2520to%2520the%2520plan.%2520Following%2520the%2520insertion%2520tests%252C%2520computed%250Atomography%2520scans%2520of%2520the%2520phantom%2520models%2520were%2520acquired%252C%2520allowing%2520for%2520evaluation%250Aof%2520insertion%2520accuracy%252C%2520target%2520deviation%252C%2520angular%2520error%252C%2520and%2520depth%2520precision.%250ASystem%2520Usability%2520Scale%2520surveys%2520assessed%2520user%2520experience%2520and%2520cognitive%2520workload.%250ATool-tracking%2520guidance%2520improved%2520performance%2520metrics%2520across%2520all%2520accuracy%250Ameasures%2520and%2520was%2520preferred%2520by%2520users%2520in%2520subjective%2520evaluations.%2520A%2520free%2520copy%2520of%250Athis%2520paper%2520and%2520all%2520supplemental%2520materials%2520are%2520available%2520at%250Ahttps%253A//bit.ly/45l89Hq.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AR%20Surgical%20Navigation%20With%20Surface%20Tracing%3A%20Comparing%0A%20%20In-SitVisualization%20with%20Tool-Tracking%20Guidance%20for%20Neurosurgical%0A%20%20Applications&entry.906535625=Marc%20J.%20Fischer%20and%20Jeffrey%20Potts%20and%20Gabriel%20Urreola%20and%20Dax%20Jones%20and%20Paolo%20Palmisciano%20and%20E.%20Bradley%20Strong%20and%20Branden%20Cord%20and%20Andrew%20D.%20Hernandez%20and%20Julia%20D.%20Sharma%20and%20E.%20Brandon%20Strong&entry.1292438233=%20%20Augmented%20Reality%20%28AR%29%20surgical%20navigation%20systems%20are%20emerging%20as%20the%20next%0Ageneration%20of%20intraoperative%20surgical%20guidance%2C%20promising%20to%20overcome%0Alimitations%20of%20traditional%20navigation%20systems.%20However%2C%20known%20issues%20with%20AR%0Adepth%20perception%20due%20to%20vergence-accommodation%20conflict%20and%20occlusion%20handling%0Alimitations%20of%20the%20currently%20commercially%20available%20display%20technology%20present%0Aacute%20challenges%20in%20surgical%20settings%20where%20precision%20is%20paramount.%20This%20study%0Apresents%20a%20novel%20methodology%20for%20utilizing%20AR%20guidance%20to%20register%20anatomical%0Atargets%20and%20provide%20real-time%20instrument%20navigation%20using%20placement%20of%0Asimulated%20external%20ventricular%20drain%20catheters%20on%20a%20phantom%20model%20as%20the%0Aclinical%20scenario.%20The%20system%20registers%20target%20positions%20to%20the%20patient%20through%0Aa%20novel%20surface%20tracing%20method%20and%20uses%20real-time%20infrared%20tool%20tracking%20to%20aid%0Ain%20catheter%20placement%2C%20relying%20only%20on%20the%20onboard%20sensors%20of%20the%20Microsoft%0AHoloLens%202.%20A%20group%20of%20intended%20users%20performed%20the%20procedure%20of%20simulated%0Ainsertions%20under%20two%20AR%20guidance%20conditions%3A%20static%20in-situ%20visualization%2C%0Awhere%20planned%20trajectories%20are%20overlaid%20directly%20onto%20the%20patient%20anatomy%2C%20and%0Areal-time%20tool-tracking%20guidance%2C%20where%20live%20feedback%20of%20the%20catheter%27s%20pose%20is%0Aprovided%20relative%20to%20the%20plan.%20Following%20the%20insertion%20tests%2C%20computed%0Atomography%20scans%20of%20the%20phantom%20models%20were%20acquired%2C%20allowing%20for%20evaluation%0Aof%20insertion%20accuracy%2C%20target%20deviation%2C%20angular%20error%2C%20and%20depth%20precision.%0ASystem%20Usability%20Scale%20surveys%20assessed%20user%20experience%20and%20cognitive%20workload.%0ATool-tracking%20guidance%20improved%20performance%20metrics%20across%20all%20accuracy%0Ameasures%20and%20was%20preferred%20by%20users%20in%20subjective%20evaluations.%20A%20free%20copy%20of%0Athis%20paper%20and%20all%20supplemental%20materials%20are%20available%20at%0Ahttps%3A//bit.ly/45l89Hq.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10554v1&entry.124074799=Read"},
{"title": "SHALE: A Scalable Benchmark for Fine-grained Hallucination Evaluation in\n  LVLMs", "author": "Bei Yan and Zhiyuan Chen and Yuecong Min and Jie Zhang and Jiahao Wang and Xiaozhen Wang and Shiguang Shan", "abstract": "  Despite rapid advances, Large Vision-Language Models (LVLMs) still suffer\nfrom hallucinations, i.e., generating content inconsistent with input or\nestablished world knowledge, which correspond to faithfulness and factuality\nhallucinations, respectively. Prior studies primarily evaluate faithfulness\nhallucination at a rather coarse level (e.g., object-level) and lack\nfine-grained analysis. Additionally, existing benchmarks often rely on costly\nmanual curation or reused public datasets, raising concerns about scalability\nand data leakage. To address these limitations, we propose an automated data\nconstruction pipeline that produces scalable, controllable, and diverse\nevaluation data. We also design a hierarchical hallucination induction\nframework with input perturbations to simulate realistic noisy scenarios.\nIntegrating these designs, we construct SHALE, a Scalable HALlucination\nEvaluation benchmark designed to assess both faithfulness and factuality\nhallucinations via a fine-grained hallucination categorization scheme. SHALE\ncomprises over 30K image-instruction pairs spanning 12 representative visual\nperception aspects for faithfulness and 6 knowledge domains for factuality,\nconsidering both clean and noisy scenarios. Extensive experiments on over 20\nmainstream LVLMs reveal significant factuality hallucinations and high\nsensitivity to semantic perturbations.\n", "link": "http://arxiv.org/abs/2508.09584v2", "date": "2025-08-14", "relevancy": 2.5551, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5128}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5128}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHALE%3A%20A%20Scalable%20Benchmark%20for%20Fine-grained%20Hallucination%20Evaluation%20in%0A%20%20LVLMs&body=Title%3A%20SHALE%3A%20A%20Scalable%20Benchmark%20for%20Fine-grained%20Hallucination%20Evaluation%20in%0A%20%20LVLMs%0AAuthor%3A%20Bei%20Yan%20and%20Zhiyuan%20Chen%20and%20Yuecong%20Min%20and%20Jie%20Zhang%20and%20Jiahao%20Wang%20and%20Xiaozhen%20Wang%20and%20Shiguang%20Shan%0AAbstract%3A%20%20%20Despite%20rapid%20advances%2C%20Large%20Vision-Language%20Models%20%28LVLMs%29%20still%20suffer%0Afrom%20hallucinations%2C%20i.e.%2C%20generating%20content%20inconsistent%20with%20input%20or%0Aestablished%20world%20knowledge%2C%20which%20correspond%20to%20faithfulness%20and%20factuality%0Ahallucinations%2C%20respectively.%20Prior%20studies%20primarily%20evaluate%20faithfulness%0Ahallucination%20at%20a%20rather%20coarse%20level%20%28e.g.%2C%20object-level%29%20and%20lack%0Afine-grained%20analysis.%20Additionally%2C%20existing%20benchmarks%20often%20rely%20on%20costly%0Amanual%20curation%20or%20reused%20public%20datasets%2C%20raising%20concerns%20about%20scalability%0Aand%20data%20leakage.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20automated%20data%0Aconstruction%20pipeline%20that%20produces%20scalable%2C%20controllable%2C%20and%20diverse%0Aevaluation%20data.%20We%20also%20design%20a%20hierarchical%20hallucination%20induction%0Aframework%20with%20input%20perturbations%20to%20simulate%20realistic%20noisy%20scenarios.%0AIntegrating%20these%20designs%2C%20we%20construct%20SHALE%2C%20a%20Scalable%20HALlucination%0AEvaluation%20benchmark%20designed%20to%20assess%20both%20faithfulness%20and%20factuality%0Ahallucinations%20via%20a%20fine-grained%20hallucination%20categorization%20scheme.%20SHALE%0Acomprises%20over%2030K%20image-instruction%20pairs%20spanning%2012%20representative%20visual%0Aperception%20aspects%20for%20faithfulness%20and%206%20knowledge%20domains%20for%20factuality%2C%0Aconsidering%20both%20clean%20and%20noisy%20scenarios.%20Extensive%20experiments%20on%20over%2020%0Amainstream%20LVLMs%20reveal%20significant%20factuality%20hallucinations%20and%20high%0Asensitivity%20to%20semantic%20perturbations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09584v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHALE%253A%2520A%2520Scalable%2520Benchmark%2520for%2520Fine-grained%2520Hallucination%2520Evaluation%2520in%250A%2520%2520LVLMs%26entry.906535625%3DBei%2520Yan%2520and%2520Zhiyuan%2520Chen%2520and%2520Yuecong%2520Min%2520and%2520Jie%2520Zhang%2520and%2520Jiahao%2520Wang%2520and%2520Xiaozhen%2520Wang%2520and%2520Shiguang%2520Shan%26entry.1292438233%3D%2520%2520Despite%2520rapid%2520advances%252C%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520still%2520suffer%250Afrom%2520hallucinations%252C%2520i.e.%252C%2520generating%2520content%2520inconsistent%2520with%2520input%2520or%250Aestablished%2520world%2520knowledge%252C%2520which%2520correspond%2520to%2520faithfulness%2520and%2520factuality%250Ahallucinations%252C%2520respectively.%2520Prior%2520studies%2520primarily%2520evaluate%2520faithfulness%250Ahallucination%2520at%2520a%2520rather%2520coarse%2520level%2520%2528e.g.%252C%2520object-level%2529%2520and%2520lack%250Afine-grained%2520analysis.%2520Additionally%252C%2520existing%2520benchmarks%2520often%2520rely%2520on%2520costly%250Amanual%2520curation%2520or%2520reused%2520public%2520datasets%252C%2520raising%2520concerns%2520about%2520scalability%250Aand%2520data%2520leakage.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520an%2520automated%2520data%250Aconstruction%2520pipeline%2520that%2520produces%2520scalable%252C%2520controllable%252C%2520and%2520diverse%250Aevaluation%2520data.%2520We%2520also%2520design%2520a%2520hierarchical%2520hallucination%2520induction%250Aframework%2520with%2520input%2520perturbations%2520to%2520simulate%2520realistic%2520noisy%2520scenarios.%250AIntegrating%2520these%2520designs%252C%2520we%2520construct%2520SHALE%252C%2520a%2520Scalable%2520HALlucination%250AEvaluation%2520benchmark%2520designed%2520to%2520assess%2520both%2520faithfulness%2520and%2520factuality%250Ahallucinations%2520via%2520a%2520fine-grained%2520hallucination%2520categorization%2520scheme.%2520SHALE%250Acomprises%2520over%252030K%2520image-instruction%2520pairs%2520spanning%252012%2520representative%2520visual%250Aperception%2520aspects%2520for%2520faithfulness%2520and%25206%2520knowledge%2520domains%2520for%2520factuality%252C%250Aconsidering%2520both%2520clean%2520and%2520noisy%2520scenarios.%2520Extensive%2520experiments%2520on%2520over%252020%250Amainstream%2520LVLMs%2520reveal%2520significant%2520factuality%2520hallucinations%2520and%2520high%250Asensitivity%2520to%2520semantic%2520perturbations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09584v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHALE%3A%20A%20Scalable%20Benchmark%20for%20Fine-grained%20Hallucination%20Evaluation%20in%0A%20%20LVLMs&entry.906535625=Bei%20Yan%20and%20Zhiyuan%20Chen%20and%20Yuecong%20Min%20and%20Jie%20Zhang%20and%20Jiahao%20Wang%20and%20Xiaozhen%20Wang%20and%20Shiguang%20Shan&entry.1292438233=%20%20Despite%20rapid%20advances%2C%20Large%20Vision-Language%20Models%20%28LVLMs%29%20still%20suffer%0Afrom%20hallucinations%2C%20i.e.%2C%20generating%20content%20inconsistent%20with%20input%20or%0Aestablished%20world%20knowledge%2C%20which%20correspond%20to%20faithfulness%20and%20factuality%0Ahallucinations%2C%20respectively.%20Prior%20studies%20primarily%20evaluate%20faithfulness%0Ahallucination%20at%20a%20rather%20coarse%20level%20%28e.g.%2C%20object-level%29%20and%20lack%0Afine-grained%20analysis.%20Additionally%2C%20existing%20benchmarks%20often%20rely%20on%20costly%0Amanual%20curation%20or%20reused%20public%20datasets%2C%20raising%20concerns%20about%20scalability%0Aand%20data%20leakage.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20automated%20data%0Aconstruction%20pipeline%20that%20produces%20scalable%2C%20controllable%2C%20and%20diverse%0Aevaluation%20data.%20We%20also%20design%20a%20hierarchical%20hallucination%20induction%0Aframework%20with%20input%20perturbations%20to%20simulate%20realistic%20noisy%20scenarios.%0AIntegrating%20these%20designs%2C%20we%20construct%20SHALE%2C%20a%20Scalable%20HALlucination%0AEvaluation%20benchmark%20designed%20to%20assess%20both%20faithfulness%20and%20factuality%0Ahallucinations%20via%20a%20fine-grained%20hallucination%20categorization%20scheme.%20SHALE%0Acomprises%20over%2030K%20image-instruction%20pairs%20spanning%2012%20representative%20visual%0Aperception%20aspects%20for%20faithfulness%20and%206%20knowledge%20domains%20for%20factuality%2C%0Aconsidering%20both%20clean%20and%20noisy%20scenarios.%20Extensive%20experiments%20on%20over%2020%0Amainstream%20LVLMs%20reveal%20significant%20factuality%20hallucinations%20and%20high%0Asensitivity%20to%20semantic%20perturbations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09584v2&entry.124074799=Read"},
{"title": "Novel View Synthesis using DDIM Inversion", "author": "Sehajdeep SIngh and A V Subramanyam", "abstract": "  Synthesizing novel views from a single input image is a challenging task. It\nrequires extrapolating the 3D structure of a scene while inferring details in\noccluded regions, and maintaining geometric consistency across viewpoints. Many\nexisting methods must fine-tune large diffusion backbones using multiple views\nor train a diffusion model from scratch, which is extremely expensive.\nAdditionally, they suffer from blurry reconstruction and poor generalization.\nThis gap presents the opportunity to explore an explicit lightweight view\ntranslation framework that can directly utilize the high-fidelity generative\ncapabilities of a pretrained diffusion model while reconstructing a scene from\na novel view. Given the DDIM-inverted latent of a single input image, we employ\na camera pose-conditioned translation U-Net, TUNet, to predict the inverted\nlatent corresponding to the desired target view. However, the image sampled\nusing the predicted latent may result in a blurry reconstruction. To this end,\nwe propose a novel fusion strategy that exploits the inherent noise correlation\nstructure observed in DDIM inversion. The proposed fusion strategy helps\npreserve the texture and fine-grained details. To synthesize the novel view, we\nuse the fused latent as the initial condition for DDIM sampling, leveraging the\ngenerative prior of the pretrained diffusion model. Extensive experiments on\nMVImgNet demonstrate that our method outperforms existing methods.\n", "link": "http://arxiv.org/abs/2508.10688v1", "date": "2025-08-14", "relevancy": 2.5387, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.636}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6344}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20View%20Synthesis%20using%20DDIM%20Inversion&body=Title%3A%20Novel%20View%20Synthesis%20using%20DDIM%20Inversion%0AAuthor%3A%20Sehajdeep%20SIngh%20and%20A%20V%20Subramanyam%0AAbstract%3A%20%20%20Synthesizing%20novel%20views%20from%20a%20single%20input%20image%20is%20a%20challenging%20task.%20It%0Arequires%20extrapolating%20the%203D%20structure%20of%20a%20scene%20while%20inferring%20details%20in%0Aoccluded%20regions%2C%20and%20maintaining%20geometric%20consistency%20across%20viewpoints.%20Many%0Aexisting%20methods%20must%20fine-tune%20large%20diffusion%20backbones%20using%20multiple%20views%0Aor%20train%20a%20diffusion%20model%20from%20scratch%2C%20which%20is%20extremely%20expensive.%0AAdditionally%2C%20they%20suffer%20from%20blurry%20reconstruction%20and%20poor%20generalization.%0AThis%20gap%20presents%20the%20opportunity%20to%20explore%20an%20explicit%20lightweight%20view%0Atranslation%20framework%20that%20can%20directly%20utilize%20the%20high-fidelity%20generative%0Acapabilities%20of%20a%20pretrained%20diffusion%20model%20while%20reconstructing%20a%20scene%20from%0Aa%20novel%20view.%20Given%20the%20DDIM-inverted%20latent%20of%20a%20single%20input%20image%2C%20we%20employ%0Aa%20camera%20pose-conditioned%20translation%20U-Net%2C%20TUNet%2C%20to%20predict%20the%20inverted%0Alatent%20corresponding%20to%20the%20desired%20target%20view.%20However%2C%20the%20image%20sampled%0Ausing%20the%20predicted%20latent%20may%20result%20in%20a%20blurry%20reconstruction.%20To%20this%20end%2C%0Awe%20propose%20a%20novel%20fusion%20strategy%20that%20exploits%20the%20inherent%20noise%20correlation%0Astructure%20observed%20in%20DDIM%20inversion.%20The%20proposed%20fusion%20strategy%20helps%0Apreserve%20the%20texture%20and%20fine-grained%20details.%20To%20synthesize%20the%20novel%20view%2C%20we%0Ause%20the%20fused%20latent%20as%20the%20initial%20condition%20for%20DDIM%20sampling%2C%20leveraging%20the%0Agenerative%20prior%20of%20the%20pretrained%20diffusion%20model.%20Extensive%20experiments%20on%0AMVImgNet%20demonstrate%20that%20our%20method%20outperforms%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520View%2520Synthesis%2520using%2520DDIM%2520Inversion%26entry.906535625%3DSehajdeep%2520SIngh%2520and%2520A%2520V%2520Subramanyam%26entry.1292438233%3D%2520%2520Synthesizing%2520novel%2520views%2520from%2520a%2520single%2520input%2520image%2520is%2520a%2520challenging%2520task.%2520It%250Arequires%2520extrapolating%2520the%25203D%2520structure%2520of%2520a%2520scene%2520while%2520inferring%2520details%2520in%250Aoccluded%2520regions%252C%2520and%2520maintaining%2520geometric%2520consistency%2520across%2520viewpoints.%2520Many%250Aexisting%2520methods%2520must%2520fine-tune%2520large%2520diffusion%2520backbones%2520using%2520multiple%2520views%250Aor%2520train%2520a%2520diffusion%2520model%2520from%2520scratch%252C%2520which%2520is%2520extremely%2520expensive.%250AAdditionally%252C%2520they%2520suffer%2520from%2520blurry%2520reconstruction%2520and%2520poor%2520generalization.%250AThis%2520gap%2520presents%2520the%2520opportunity%2520to%2520explore%2520an%2520explicit%2520lightweight%2520view%250Atranslation%2520framework%2520that%2520can%2520directly%2520utilize%2520the%2520high-fidelity%2520generative%250Acapabilities%2520of%2520a%2520pretrained%2520diffusion%2520model%2520while%2520reconstructing%2520a%2520scene%2520from%250Aa%2520novel%2520view.%2520Given%2520the%2520DDIM-inverted%2520latent%2520of%2520a%2520single%2520input%2520image%252C%2520we%2520employ%250Aa%2520camera%2520pose-conditioned%2520translation%2520U-Net%252C%2520TUNet%252C%2520to%2520predict%2520the%2520inverted%250Alatent%2520corresponding%2520to%2520the%2520desired%2520target%2520view.%2520However%252C%2520the%2520image%2520sampled%250Ausing%2520the%2520predicted%2520latent%2520may%2520result%2520in%2520a%2520blurry%2520reconstruction.%2520To%2520this%2520end%252C%250Awe%2520propose%2520a%2520novel%2520fusion%2520strategy%2520that%2520exploits%2520the%2520inherent%2520noise%2520correlation%250Astructure%2520observed%2520in%2520DDIM%2520inversion.%2520The%2520proposed%2520fusion%2520strategy%2520helps%250Apreserve%2520the%2520texture%2520and%2520fine-grained%2520details.%2520To%2520synthesize%2520the%2520novel%2520view%252C%2520we%250Ause%2520the%2520fused%2520latent%2520as%2520the%2520initial%2520condition%2520for%2520DDIM%2520sampling%252C%2520leveraging%2520the%250Agenerative%2520prior%2520of%2520the%2520pretrained%2520diffusion%2520model.%2520Extensive%2520experiments%2520on%250AMVImgNet%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20View%20Synthesis%20using%20DDIM%20Inversion&entry.906535625=Sehajdeep%20SIngh%20and%20A%20V%20Subramanyam&entry.1292438233=%20%20Synthesizing%20novel%20views%20from%20a%20single%20input%20image%20is%20a%20challenging%20task.%20It%0Arequires%20extrapolating%20the%203D%20structure%20of%20a%20scene%20while%20inferring%20details%20in%0Aoccluded%20regions%2C%20and%20maintaining%20geometric%20consistency%20across%20viewpoints.%20Many%0Aexisting%20methods%20must%20fine-tune%20large%20diffusion%20backbones%20using%20multiple%20views%0Aor%20train%20a%20diffusion%20model%20from%20scratch%2C%20which%20is%20extremely%20expensive.%0AAdditionally%2C%20they%20suffer%20from%20blurry%20reconstruction%20and%20poor%20generalization.%0AThis%20gap%20presents%20the%20opportunity%20to%20explore%20an%20explicit%20lightweight%20view%0Atranslation%20framework%20that%20can%20directly%20utilize%20the%20high-fidelity%20generative%0Acapabilities%20of%20a%20pretrained%20diffusion%20model%20while%20reconstructing%20a%20scene%20from%0Aa%20novel%20view.%20Given%20the%20DDIM-inverted%20latent%20of%20a%20single%20input%20image%2C%20we%20employ%0Aa%20camera%20pose-conditioned%20translation%20U-Net%2C%20TUNet%2C%20to%20predict%20the%20inverted%0Alatent%20corresponding%20to%20the%20desired%20target%20view.%20However%2C%20the%20image%20sampled%0Ausing%20the%20predicted%20latent%20may%20result%20in%20a%20blurry%20reconstruction.%20To%20this%20end%2C%0Awe%20propose%20a%20novel%20fusion%20strategy%20that%20exploits%20the%20inherent%20noise%20correlation%0Astructure%20observed%20in%20DDIM%20inversion.%20The%20proposed%20fusion%20strategy%20helps%0Apreserve%20the%20texture%20and%20fine-grained%20details.%20To%20synthesize%20the%20novel%20view%2C%20we%0Ause%20the%20fused%20latent%20as%20the%20initial%20condition%20for%20DDIM%20sampling%2C%20leveraging%20the%0Agenerative%20prior%20of%20the%20pretrained%20diffusion%20model.%20Extensive%20experiments%20on%0AMVImgNet%20demonstrate%20that%20our%20method%20outperforms%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10688v1&entry.124074799=Read"},
{"title": "From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in\n  Vision Language Models", "author": "Tiancheng Han and Yunfei Gao and Yong Li and Wuzhou Yu and Qiaosheng Zhang and Wenqi Shao", "abstract": "  Spatio-physical reasoning, a foundation capability for understanding the real\nphysics world, is a critical step towards building robust world models. While\nrecent vision language models (VLMs) have shown remarkable progress in\nspecialized domains like multimodal mathematics and pure spatial understanding,\ntheir capability for spatio-physical reasoning remains largely unexplored. This\npaper provides a comprehensive diagnostic analysis of mainstream VLMs,\nrevealing that current models perform inadequately on this crucial task.\nFurther detailed analysis shows that this underperformance is largely\nattributable to biases caused by human-like prior and a lack of deep reasoning.\nTo address these challenges, we apply supervised fine-tuning followed by\nrule-based reinforcement learning to Qwen2.5-VL-7B, resulting in significant\nimprovements in spatio-physical reasoning capabilities and surpassing leading\nproprietary models. Nevertheless, despite this success, the model's\ngeneralization to new physics scenarios remains limited -- underscoring the\npressing need for new approaches in spatio-physical reasoning.\n", "link": "http://arxiv.org/abs/2508.10770v1", "date": "2025-08-14", "relevancy": 2.5267, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6353}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Diagnosis%20to%20Improvement%3A%20Probing%20Spatio-Physical%20Reasoning%20in%0A%20%20Vision%20Language%20Models&body=Title%3A%20From%20Diagnosis%20to%20Improvement%3A%20Probing%20Spatio-Physical%20Reasoning%20in%0A%20%20Vision%20Language%20Models%0AAuthor%3A%20Tiancheng%20Han%20and%20Yunfei%20Gao%20and%20Yong%20Li%20and%20Wuzhou%20Yu%20and%20Qiaosheng%20Zhang%20and%20Wenqi%20Shao%0AAbstract%3A%20%20%20Spatio-physical%20reasoning%2C%20a%20foundation%20capability%20for%20understanding%20the%20real%0Aphysics%20world%2C%20is%20a%20critical%20step%20towards%20building%20robust%20world%20models.%20While%0Arecent%20vision%20language%20models%20%28VLMs%29%20have%20shown%20remarkable%20progress%20in%0Aspecialized%20domains%20like%20multimodal%20mathematics%20and%20pure%20spatial%20understanding%2C%0Atheir%20capability%20for%20spatio-physical%20reasoning%20remains%20largely%20unexplored.%20This%0Apaper%20provides%20a%20comprehensive%20diagnostic%20analysis%20of%20mainstream%20VLMs%2C%0Arevealing%20that%20current%20models%20perform%20inadequately%20on%20this%20crucial%20task.%0AFurther%20detailed%20analysis%20shows%20that%20this%20underperformance%20is%20largely%0Aattributable%20to%20biases%20caused%20by%20human-like%20prior%20and%20a%20lack%20of%20deep%20reasoning.%0ATo%20address%20these%20challenges%2C%20we%20apply%20supervised%20fine-tuning%20followed%20by%0Arule-based%20reinforcement%20learning%20to%20Qwen2.5-VL-7B%2C%20resulting%20in%20significant%0Aimprovements%20in%20spatio-physical%20reasoning%20capabilities%20and%20surpassing%20leading%0Aproprietary%20models.%20Nevertheless%2C%20despite%20this%20success%2C%20the%20model%27s%0Ageneralization%20to%20new%20physics%20scenarios%20remains%20limited%20--%20underscoring%20the%0Apressing%20need%20for%20new%20approaches%20in%20spatio-physical%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Diagnosis%2520to%2520Improvement%253A%2520Probing%2520Spatio-Physical%2520Reasoning%2520in%250A%2520%2520Vision%2520Language%2520Models%26entry.906535625%3DTiancheng%2520Han%2520and%2520Yunfei%2520Gao%2520and%2520Yong%2520Li%2520and%2520Wuzhou%2520Yu%2520and%2520Qiaosheng%2520Zhang%2520and%2520Wenqi%2520Shao%26entry.1292438233%3D%2520%2520Spatio-physical%2520reasoning%252C%2520a%2520foundation%2520capability%2520for%2520understanding%2520the%2520real%250Aphysics%2520world%252C%2520is%2520a%2520critical%2520step%2520towards%2520building%2520robust%2520world%2520models.%2520While%250Arecent%2520vision%2520language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520remarkable%2520progress%2520in%250Aspecialized%2520domains%2520like%2520multimodal%2520mathematics%2520and%2520pure%2520spatial%2520understanding%252C%250Atheir%2520capability%2520for%2520spatio-physical%2520reasoning%2520remains%2520largely%2520unexplored.%2520This%250Apaper%2520provides%2520a%2520comprehensive%2520diagnostic%2520analysis%2520of%2520mainstream%2520VLMs%252C%250Arevealing%2520that%2520current%2520models%2520perform%2520inadequately%2520on%2520this%2520crucial%2520task.%250AFurther%2520detailed%2520analysis%2520shows%2520that%2520this%2520underperformance%2520is%2520largely%250Aattributable%2520to%2520biases%2520caused%2520by%2520human-like%2520prior%2520and%2520a%2520lack%2520of%2520deep%2520reasoning.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520apply%2520supervised%2520fine-tuning%2520followed%2520by%250Arule-based%2520reinforcement%2520learning%2520to%2520Qwen2.5-VL-7B%252C%2520resulting%2520in%2520significant%250Aimprovements%2520in%2520spatio-physical%2520reasoning%2520capabilities%2520and%2520surpassing%2520leading%250Aproprietary%2520models.%2520Nevertheless%252C%2520despite%2520this%2520success%252C%2520the%2520model%2527s%250Ageneralization%2520to%2520new%2520physics%2520scenarios%2520remains%2520limited%2520--%2520underscoring%2520the%250Apressing%2520need%2520for%2520new%2520approaches%2520in%2520spatio-physical%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Diagnosis%20to%20Improvement%3A%20Probing%20Spatio-Physical%20Reasoning%20in%0A%20%20Vision%20Language%20Models&entry.906535625=Tiancheng%20Han%20and%20Yunfei%20Gao%20and%20Yong%20Li%20and%20Wuzhou%20Yu%20and%20Qiaosheng%20Zhang%20and%20Wenqi%20Shao&entry.1292438233=%20%20Spatio-physical%20reasoning%2C%20a%20foundation%20capability%20for%20understanding%20the%20real%0Aphysics%20world%2C%20is%20a%20critical%20step%20towards%20building%20robust%20world%20models.%20While%0Arecent%20vision%20language%20models%20%28VLMs%29%20have%20shown%20remarkable%20progress%20in%0Aspecialized%20domains%20like%20multimodal%20mathematics%20and%20pure%20spatial%20understanding%2C%0Atheir%20capability%20for%20spatio-physical%20reasoning%20remains%20largely%20unexplored.%20This%0Apaper%20provides%20a%20comprehensive%20diagnostic%20analysis%20of%20mainstream%20VLMs%2C%0Arevealing%20that%20current%20models%20perform%20inadequately%20on%20this%20crucial%20task.%0AFurther%20detailed%20analysis%20shows%20that%20this%20underperformance%20is%20largely%0Aattributable%20to%20biases%20caused%20by%20human-like%20prior%20and%20a%20lack%20of%20deep%20reasoning.%0ATo%20address%20these%20challenges%2C%20we%20apply%20supervised%20fine-tuning%20followed%20by%0Arule-based%20reinforcement%20learning%20to%20Qwen2.5-VL-7B%2C%20resulting%20in%20significant%0Aimprovements%20in%20spatio-physical%20reasoning%20capabilities%20and%20surpassing%20leading%0Aproprietary%20models.%20Nevertheless%2C%20despite%20this%20success%2C%20the%20model%27s%0Ageneralization%20to%20new%20physics%20scenarios%20remains%20limited%20--%20underscoring%20the%0Apressing%20need%20for%20new%20approaches%20in%20spatio-physical%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10770v1&entry.124074799=Read"},
{"title": "Enhancing Fairness in Autoencoders for Node-Level Graph Anomaly\n  Detection", "author": "Shouju Wang and Yuchen Song and Sheng'en Li and Dongmian Zou", "abstract": "  Graph anomaly detection (GAD) has become an increasingly important task\nacross various domains. With the rapid development of graph neural networks\n(GNNs), GAD methods have achieved significant performance improvements.\nHowever, fairness considerations in GAD remain largely underexplored. Indeed,\nGNN-based GAD models can inherit and amplify biases present in training data,\npotentially leading to unfair outcomes. While existing efforts have focused on\ndeveloping fair GNNs, most approaches target node classification tasks, where\nmodels often rely on simple layer architectures rather than autoencoder-based\nstructures, which are the most widely used architecturs for anomaly detection.\nTo address fairness in autoencoder-based GAD models, we propose\n\\textbf{D}is\\textbf{E}ntangled \\textbf{C}ounterfactual \\textbf{A}dversarial\n\\textbf{F}air (DECAF)-GAD, a framework that alleviates bias while preserving\nGAD performance. Specifically, we introduce a structural causal model (SCM) to\ndisentangle sensitive attributes from learned representations. Based on this\ncausal framework, we formulate a specialized autoencoder architecture along\nwith a fairness-guided loss function. Through extensive experiments on both\nsynthetic and real-world datasets, we demonstrate that DECAF-GAD not only\nachieves competitive anomaly detection performance but also significantly\nenhances fairness metrics compared to baseline GAD methods. Our code is\navailable at https://github.com/Tlhey/decaf_code.\n", "link": "http://arxiv.org/abs/2508.10785v1", "date": "2025-08-14", "relevancy": 2.5156, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.514}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5048}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Fairness%20in%20Autoencoders%20for%20Node-Level%20Graph%20Anomaly%0A%20%20Detection&body=Title%3A%20Enhancing%20Fairness%20in%20Autoencoders%20for%20Node-Level%20Graph%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Shouju%20Wang%20and%20Yuchen%20Song%20and%20Sheng%27en%20Li%20and%20Dongmian%20Zou%0AAbstract%3A%20%20%20Graph%20anomaly%20detection%20%28GAD%29%20has%20become%20an%20increasingly%20important%20task%0Aacross%20various%20domains.%20With%20the%20rapid%20development%20of%20graph%20neural%20networks%0A%28GNNs%29%2C%20GAD%20methods%20have%20achieved%20significant%20performance%20improvements.%0AHowever%2C%20fairness%20considerations%20in%20GAD%20remain%20largely%20underexplored.%20Indeed%2C%0AGNN-based%20GAD%20models%20can%20inherit%20and%20amplify%20biases%20present%20in%20training%20data%2C%0Apotentially%20leading%20to%20unfair%20outcomes.%20While%20existing%20efforts%20have%20focused%20on%0Adeveloping%20fair%20GNNs%2C%20most%20approaches%20target%20node%20classification%20tasks%2C%20where%0Amodels%20often%20rely%20on%20simple%20layer%20architectures%20rather%20than%20autoencoder-based%0Astructures%2C%20which%20are%20the%20most%20widely%20used%20architecturs%20for%20anomaly%20detection.%0ATo%20address%20fairness%20in%20autoencoder-based%20GAD%20models%2C%20we%20propose%0A%5Ctextbf%7BD%7Dis%5Ctextbf%7BE%7Dntangled%20%5Ctextbf%7BC%7Dounterfactual%20%5Ctextbf%7BA%7Ddversarial%0A%5Ctextbf%7BF%7Dair%20%28DECAF%29-GAD%2C%20a%20framework%20that%20alleviates%20bias%20while%20preserving%0AGAD%20performance.%20Specifically%2C%20we%20introduce%20a%20structural%20causal%20model%20%28SCM%29%20to%0Adisentangle%20sensitive%20attributes%20from%20learned%20representations.%20Based%20on%20this%0Acausal%20framework%2C%20we%20formulate%20a%20specialized%20autoencoder%20architecture%20along%0Awith%20a%20fairness-guided%20loss%20function.%20Through%20extensive%20experiments%20on%20both%0Asynthetic%20and%20real-world%20datasets%2C%20we%20demonstrate%20that%20DECAF-GAD%20not%20only%0Aachieves%20competitive%20anomaly%20detection%20performance%20but%20also%20significantly%0Aenhances%20fairness%20metrics%20compared%20to%20baseline%20GAD%20methods.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/Tlhey/decaf_code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Fairness%2520in%2520Autoencoders%2520for%2520Node-Level%2520Graph%2520Anomaly%250A%2520%2520Detection%26entry.906535625%3DShouju%2520Wang%2520and%2520Yuchen%2520Song%2520and%2520Sheng%2527en%2520Li%2520and%2520Dongmian%2520Zou%26entry.1292438233%3D%2520%2520Graph%2520anomaly%2520detection%2520%2528GAD%2529%2520has%2520become%2520an%2520increasingly%2520important%2520task%250Aacross%2520various%2520domains.%2520With%2520the%2520rapid%2520development%2520of%2520graph%2520neural%2520networks%250A%2528GNNs%2529%252C%2520GAD%2520methods%2520have%2520achieved%2520significant%2520performance%2520improvements.%250AHowever%252C%2520fairness%2520considerations%2520in%2520GAD%2520remain%2520largely%2520underexplored.%2520Indeed%252C%250AGNN-based%2520GAD%2520models%2520can%2520inherit%2520and%2520amplify%2520biases%2520present%2520in%2520training%2520data%252C%250Apotentially%2520leading%2520to%2520unfair%2520outcomes.%2520While%2520existing%2520efforts%2520have%2520focused%2520on%250Adeveloping%2520fair%2520GNNs%252C%2520most%2520approaches%2520target%2520node%2520classification%2520tasks%252C%2520where%250Amodels%2520often%2520rely%2520on%2520simple%2520layer%2520architectures%2520rather%2520than%2520autoencoder-based%250Astructures%252C%2520which%2520are%2520the%2520most%2520widely%2520used%2520architecturs%2520for%2520anomaly%2520detection.%250ATo%2520address%2520fairness%2520in%2520autoencoder-based%2520GAD%2520models%252C%2520we%2520propose%250A%255Ctextbf%257BD%257Dis%255Ctextbf%257BE%257Dntangled%2520%255Ctextbf%257BC%257Dounterfactual%2520%255Ctextbf%257BA%257Ddversarial%250A%255Ctextbf%257BF%257Dair%2520%2528DECAF%2529-GAD%252C%2520a%2520framework%2520that%2520alleviates%2520bias%2520while%2520preserving%250AGAD%2520performance.%2520Specifically%252C%2520we%2520introduce%2520a%2520structural%2520causal%2520model%2520%2528SCM%2529%2520to%250Adisentangle%2520sensitive%2520attributes%2520from%2520learned%2520representations.%2520Based%2520on%2520this%250Acausal%2520framework%252C%2520we%2520formulate%2520a%2520specialized%2520autoencoder%2520architecture%2520along%250Awith%2520a%2520fairness-guided%2520loss%2520function.%2520Through%2520extensive%2520experiments%2520on%2520both%250Asynthetic%2520and%2520real-world%2520datasets%252C%2520we%2520demonstrate%2520that%2520DECAF-GAD%2520not%2520only%250Aachieves%2520competitive%2520anomaly%2520detection%2520performance%2520but%2520also%2520significantly%250Aenhances%2520fairness%2520metrics%2520compared%2520to%2520baseline%2520GAD%2520methods.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/Tlhey/decaf_code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Fairness%20in%20Autoencoders%20for%20Node-Level%20Graph%20Anomaly%0A%20%20Detection&entry.906535625=Shouju%20Wang%20and%20Yuchen%20Song%20and%20Sheng%27en%20Li%20and%20Dongmian%20Zou&entry.1292438233=%20%20Graph%20anomaly%20detection%20%28GAD%29%20has%20become%20an%20increasingly%20important%20task%0Aacross%20various%20domains.%20With%20the%20rapid%20development%20of%20graph%20neural%20networks%0A%28GNNs%29%2C%20GAD%20methods%20have%20achieved%20significant%20performance%20improvements.%0AHowever%2C%20fairness%20considerations%20in%20GAD%20remain%20largely%20underexplored.%20Indeed%2C%0AGNN-based%20GAD%20models%20can%20inherit%20and%20amplify%20biases%20present%20in%20training%20data%2C%0Apotentially%20leading%20to%20unfair%20outcomes.%20While%20existing%20efforts%20have%20focused%20on%0Adeveloping%20fair%20GNNs%2C%20most%20approaches%20target%20node%20classification%20tasks%2C%20where%0Amodels%20often%20rely%20on%20simple%20layer%20architectures%20rather%20than%20autoencoder-based%0Astructures%2C%20which%20are%20the%20most%20widely%20used%20architecturs%20for%20anomaly%20detection.%0ATo%20address%20fairness%20in%20autoencoder-based%20GAD%20models%2C%20we%20propose%0A%5Ctextbf%7BD%7Dis%5Ctextbf%7BE%7Dntangled%20%5Ctextbf%7BC%7Dounterfactual%20%5Ctextbf%7BA%7Ddversarial%0A%5Ctextbf%7BF%7Dair%20%28DECAF%29-GAD%2C%20a%20framework%20that%20alleviates%20bias%20while%20preserving%0AGAD%20performance.%20Specifically%2C%20we%20introduce%20a%20structural%20causal%20model%20%28SCM%29%20to%0Adisentangle%20sensitive%20attributes%20from%20learned%20representations.%20Based%20on%20this%0Acausal%20framework%2C%20we%20formulate%20a%20specialized%20autoencoder%20architecture%20along%0Awith%20a%20fairness-guided%20loss%20function.%20Through%20extensive%20experiments%20on%20both%0Asynthetic%20and%20real-world%20datasets%2C%20we%20demonstrate%20that%20DECAF-GAD%20not%20only%0Aachieves%20competitive%20anomaly%20detection%20performance%20but%20also%20significantly%0Aenhances%20fairness%20metrics%20compared%20to%20baseline%20GAD%20methods.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/Tlhey/decaf_code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10785v1&entry.124074799=Read"},
{"title": "GenOM: Ontology Matching with Description Generation and Large Language\n  Model", "author": "Yiping Song and Jiaoyan Chen and Renate A. Schmidt", "abstract": "  Ontology matching (OM) plays an essential role in enabling semantic\ninteroperability and integration across heterogeneous knowledge sources,\nparticularly in the biomedical domain which contains numerous complex concepts\nrelated to diseases and pharmaceuticals. This paper introduces GenOM, a large\nlanguage model (LLM)-based ontology alignment framework, which enriches the\nsemantic representations of ontology concepts via generating textual\ndefinitions, retrieves alignment candidates with an embedding model, and\nincorporates exact matching-based tools to improve precision. Extensive\nexperiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often\nachieve competitive performance, surpassing many baselines including\ntraditional OM systems and recent LLM-based methods. Further ablation studies\nconfirm the effectiveness of semantic enrichment and few-shot prompting,\nhighlighting the framework's robustness and adaptability.\n", "link": "http://arxiv.org/abs/2508.10703v1", "date": "2025-08-14", "relevancy": 2.4982, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4981}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenOM%3A%20Ontology%20Matching%20with%20Description%20Generation%20and%20Large%20Language%0A%20%20Model&body=Title%3A%20GenOM%3A%20Ontology%20Matching%20with%20Description%20Generation%20and%20Large%20Language%0A%20%20Model%0AAuthor%3A%20Yiping%20Song%20and%20Jiaoyan%20Chen%20and%20Renate%20A.%20Schmidt%0AAbstract%3A%20%20%20Ontology%20matching%20%28OM%29%20plays%20an%20essential%20role%20in%20enabling%20semantic%0Ainteroperability%20and%20integration%20across%20heterogeneous%20knowledge%20sources%2C%0Aparticularly%20in%20the%20biomedical%20domain%20which%20contains%20numerous%20complex%20concepts%0Arelated%20to%20diseases%20and%20pharmaceuticals.%20This%20paper%20introduces%20GenOM%2C%20a%20large%0Alanguage%20model%20%28LLM%29-based%20ontology%20alignment%20framework%2C%20which%20enriches%20the%0Asemantic%20representations%20of%20ontology%20concepts%20via%20generating%20textual%0Adefinitions%2C%20retrieves%20alignment%20candidates%20with%20an%20embedding%20model%2C%20and%0Aincorporates%20exact%20matching-based%20tools%20to%20improve%20precision.%20Extensive%0Aexperiments%20conducted%20on%20the%20OAEI%20Bio-ML%20track%20demonstrate%20that%20GenOM%20can%20often%0Aachieve%20competitive%20performance%2C%20surpassing%20many%20baselines%20including%0Atraditional%20OM%20systems%20and%20recent%20LLM-based%20methods.%20Further%20ablation%20studies%0Aconfirm%20the%20effectiveness%20of%20semantic%20enrichment%20and%20few-shot%20prompting%2C%0Ahighlighting%20the%20framework%27s%20robustness%20and%20adaptability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenOM%253A%2520Ontology%2520Matching%2520with%2520Description%2520Generation%2520and%2520Large%2520Language%250A%2520%2520Model%26entry.906535625%3DYiping%2520Song%2520and%2520Jiaoyan%2520Chen%2520and%2520Renate%2520A.%2520Schmidt%26entry.1292438233%3D%2520%2520Ontology%2520matching%2520%2528OM%2529%2520plays%2520an%2520essential%2520role%2520in%2520enabling%2520semantic%250Ainteroperability%2520and%2520integration%2520across%2520heterogeneous%2520knowledge%2520sources%252C%250Aparticularly%2520in%2520the%2520biomedical%2520domain%2520which%2520contains%2520numerous%2520complex%2520concepts%250Arelated%2520to%2520diseases%2520and%2520pharmaceuticals.%2520This%2520paper%2520introduces%2520GenOM%252C%2520a%2520large%250Alanguage%2520model%2520%2528LLM%2529-based%2520ontology%2520alignment%2520framework%252C%2520which%2520enriches%2520the%250Asemantic%2520representations%2520of%2520ontology%2520concepts%2520via%2520generating%2520textual%250Adefinitions%252C%2520retrieves%2520alignment%2520candidates%2520with%2520an%2520embedding%2520model%252C%2520and%250Aincorporates%2520exact%2520matching-based%2520tools%2520to%2520improve%2520precision.%2520Extensive%250Aexperiments%2520conducted%2520on%2520the%2520OAEI%2520Bio-ML%2520track%2520demonstrate%2520that%2520GenOM%2520can%2520often%250Aachieve%2520competitive%2520performance%252C%2520surpassing%2520many%2520baselines%2520including%250Atraditional%2520OM%2520systems%2520and%2520recent%2520LLM-based%2520methods.%2520Further%2520ablation%2520studies%250Aconfirm%2520the%2520effectiveness%2520of%2520semantic%2520enrichment%2520and%2520few-shot%2520prompting%252C%250Ahighlighting%2520the%2520framework%2527s%2520robustness%2520and%2520adaptability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenOM%3A%20Ontology%20Matching%20with%20Description%20Generation%20and%20Large%20Language%0A%20%20Model&entry.906535625=Yiping%20Song%20and%20Jiaoyan%20Chen%20and%20Renate%20A.%20Schmidt&entry.1292438233=%20%20Ontology%20matching%20%28OM%29%20plays%20an%20essential%20role%20in%20enabling%20semantic%0Ainteroperability%20and%20integration%20across%20heterogeneous%20knowledge%20sources%2C%0Aparticularly%20in%20the%20biomedical%20domain%20which%20contains%20numerous%20complex%20concepts%0Arelated%20to%20diseases%20and%20pharmaceuticals.%20This%20paper%20introduces%20GenOM%2C%20a%20large%0Alanguage%20model%20%28LLM%29-based%20ontology%20alignment%20framework%2C%20which%20enriches%20the%0Asemantic%20representations%20of%20ontology%20concepts%20via%20generating%20textual%0Adefinitions%2C%20retrieves%20alignment%20candidates%20with%20an%20embedding%20model%2C%20and%0Aincorporates%20exact%20matching-based%20tools%20to%20improve%20precision.%20Extensive%0Aexperiments%20conducted%20on%20the%20OAEI%20Bio-ML%20track%20demonstrate%20that%20GenOM%20can%20often%0Aachieve%20competitive%20performance%2C%20surpassing%20many%20baselines%20including%0Atraditional%20OM%20systems%20and%20recent%20LLM-based%20methods.%20Further%20ablation%20studies%0Aconfirm%20the%20effectiveness%20of%20semantic%20enrichment%20and%20few-shot%20prompting%2C%0Ahighlighting%20the%20framework%27s%20robustness%20and%20adaptability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10703v1&entry.124074799=Read"},
{"title": "STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer", "author": "Yushi Lan and Yihang Luo and Fangzhou Hong and Shangchen Zhou and Honghua Chen and Zhaoyang Lyu and Shuai Yang and Bo Dai and Chen Change Loy and Xingang Pan", "abstract": "  We present STream3R, a novel approach to 3D reconstruction that reformulates\npointmap prediction as a decoder-only Transformer problem. Existing\nstate-of-the-art methods for multi-view reconstruction either depend on\nexpensive global optimization or rely on simplistic memory mechanisms that\nscale poorly with sequence length. In contrast, STream3R introduces an\nstreaming framework that processes image sequences efficiently using causal\nattention, inspired by advances in modern language modeling. By learning\ngeometric priors from large-scale 3D datasets, STream3R generalizes well to\ndiverse and challenging scenarios, including dynamic scenes where traditional\nmethods often fail. Extensive experiments show that our method consistently\noutperforms prior work across both static and dynamic scene benchmarks.\nMoreover, STream3R is inherently compatible with LLM-style training\ninfrastructure, enabling efficient large-scale pretraining and fine-tuning for\nvarious downstream 3D tasks. Our results underscore the potential of causal\nTransformer models for online 3D perception, paving the way for real-time 3D\nunderstanding in streaming environments. More details can be found in our\nproject page: https://nirvanalan.github.io/projects/stream3r.\n", "link": "http://arxiv.org/abs/2508.10893v1", "date": "2025-08-14", "relevancy": 2.4968, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6354}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.622}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STream3R%3A%20Scalable%20Sequential%203D%20Reconstruction%20with%20Causal%20Transformer&body=Title%3A%20STream3R%3A%20Scalable%20Sequential%203D%20Reconstruction%20with%20Causal%20Transformer%0AAuthor%3A%20Yushi%20Lan%20and%20Yihang%20Luo%20and%20Fangzhou%20Hong%20and%20Shangchen%20Zhou%20and%20Honghua%20Chen%20and%20Zhaoyang%20Lyu%20and%20Shuai%20Yang%20and%20Bo%20Dai%20and%20Chen%20Change%20Loy%20and%20Xingang%20Pan%0AAbstract%3A%20%20%20We%20present%20STream3R%2C%20a%20novel%20approach%20to%203D%20reconstruction%20that%20reformulates%0Apointmap%20prediction%20as%20a%20decoder-only%20Transformer%20problem.%20Existing%0Astate-of-the-art%20methods%20for%20multi-view%20reconstruction%20either%20depend%20on%0Aexpensive%20global%20optimization%20or%20rely%20on%20simplistic%20memory%20mechanisms%20that%0Ascale%20poorly%20with%20sequence%20length.%20In%20contrast%2C%20STream3R%20introduces%20an%0Astreaming%20framework%20that%20processes%20image%20sequences%20efficiently%20using%20causal%0Aattention%2C%20inspired%20by%20advances%20in%20modern%20language%20modeling.%20By%20learning%0Ageometric%20priors%20from%20large-scale%203D%20datasets%2C%20STream3R%20generalizes%20well%20to%0Adiverse%20and%20challenging%20scenarios%2C%20including%20dynamic%20scenes%20where%20traditional%0Amethods%20often%20fail.%20Extensive%20experiments%20show%20that%20our%20method%20consistently%0Aoutperforms%20prior%20work%20across%20both%20static%20and%20dynamic%20scene%20benchmarks.%0AMoreover%2C%20STream3R%20is%20inherently%20compatible%20with%20LLM-style%20training%0Ainfrastructure%2C%20enabling%20efficient%20large-scale%20pretraining%20and%20fine-tuning%20for%0Avarious%20downstream%203D%20tasks.%20Our%20results%20underscore%20the%20potential%20of%20causal%0ATransformer%20models%20for%20online%203D%20perception%2C%20paving%20the%20way%20for%20real-time%203D%0Aunderstanding%20in%20streaming%20environments.%20More%20details%20can%20be%20found%20in%20our%0Aproject%20page%3A%20https%3A//nirvanalan.github.io/projects/stream3r.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10893v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTream3R%253A%2520Scalable%2520Sequential%25203D%2520Reconstruction%2520with%2520Causal%2520Transformer%26entry.906535625%3DYushi%2520Lan%2520and%2520Yihang%2520Luo%2520and%2520Fangzhou%2520Hong%2520and%2520Shangchen%2520Zhou%2520and%2520Honghua%2520Chen%2520and%2520Zhaoyang%2520Lyu%2520and%2520Shuai%2520Yang%2520and%2520Bo%2520Dai%2520and%2520Chen%2520Change%2520Loy%2520and%2520Xingang%2520Pan%26entry.1292438233%3D%2520%2520We%2520present%2520STream3R%252C%2520a%2520novel%2520approach%2520to%25203D%2520reconstruction%2520that%2520reformulates%250Apointmap%2520prediction%2520as%2520a%2520decoder-only%2520Transformer%2520problem.%2520Existing%250Astate-of-the-art%2520methods%2520for%2520multi-view%2520reconstruction%2520either%2520depend%2520on%250Aexpensive%2520global%2520optimization%2520or%2520rely%2520on%2520simplistic%2520memory%2520mechanisms%2520that%250Ascale%2520poorly%2520with%2520sequence%2520length.%2520In%2520contrast%252C%2520STream3R%2520introduces%2520an%250Astreaming%2520framework%2520that%2520processes%2520image%2520sequences%2520efficiently%2520using%2520causal%250Aattention%252C%2520inspired%2520by%2520advances%2520in%2520modern%2520language%2520modeling.%2520By%2520learning%250Ageometric%2520priors%2520from%2520large-scale%25203D%2520datasets%252C%2520STream3R%2520generalizes%2520well%2520to%250Adiverse%2520and%2520challenging%2520scenarios%252C%2520including%2520dynamic%2520scenes%2520where%2520traditional%250Amethods%2520often%2520fail.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520consistently%250Aoutperforms%2520prior%2520work%2520across%2520both%2520static%2520and%2520dynamic%2520scene%2520benchmarks.%250AMoreover%252C%2520STream3R%2520is%2520inherently%2520compatible%2520with%2520LLM-style%2520training%250Ainfrastructure%252C%2520enabling%2520efficient%2520large-scale%2520pretraining%2520and%2520fine-tuning%2520for%250Avarious%2520downstream%25203D%2520tasks.%2520Our%2520results%2520underscore%2520the%2520potential%2520of%2520causal%250ATransformer%2520models%2520for%2520online%25203D%2520perception%252C%2520paving%2520the%2520way%2520for%2520real-time%25203D%250Aunderstanding%2520in%2520streaming%2520environments.%2520More%2520details%2520can%2520be%2520found%2520in%2520our%250Aproject%2520page%253A%2520https%253A//nirvanalan.github.io/projects/stream3r.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10893v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STream3R%3A%20Scalable%20Sequential%203D%20Reconstruction%20with%20Causal%20Transformer&entry.906535625=Yushi%20Lan%20and%20Yihang%20Luo%20and%20Fangzhou%20Hong%20and%20Shangchen%20Zhou%20and%20Honghua%20Chen%20and%20Zhaoyang%20Lyu%20and%20Shuai%20Yang%20and%20Bo%20Dai%20and%20Chen%20Change%20Loy%20and%20Xingang%20Pan&entry.1292438233=%20%20We%20present%20STream3R%2C%20a%20novel%20approach%20to%203D%20reconstruction%20that%20reformulates%0Apointmap%20prediction%20as%20a%20decoder-only%20Transformer%20problem.%20Existing%0Astate-of-the-art%20methods%20for%20multi-view%20reconstruction%20either%20depend%20on%0Aexpensive%20global%20optimization%20or%20rely%20on%20simplistic%20memory%20mechanisms%20that%0Ascale%20poorly%20with%20sequence%20length.%20In%20contrast%2C%20STream3R%20introduces%20an%0Astreaming%20framework%20that%20processes%20image%20sequences%20efficiently%20using%20causal%0Aattention%2C%20inspired%20by%20advances%20in%20modern%20language%20modeling.%20By%20learning%0Ageometric%20priors%20from%20large-scale%203D%20datasets%2C%20STream3R%20generalizes%20well%20to%0Adiverse%20and%20challenging%20scenarios%2C%20including%20dynamic%20scenes%20where%20traditional%0Amethods%20often%20fail.%20Extensive%20experiments%20show%20that%20our%20method%20consistently%0Aoutperforms%20prior%20work%20across%20both%20static%20and%20dynamic%20scene%20benchmarks.%0AMoreover%2C%20STream3R%20is%20inherently%20compatible%20with%20LLM-style%20training%0Ainfrastructure%2C%20enabling%20efficient%20large-scale%20pretraining%20and%20fine-tuning%20for%0Avarious%20downstream%203D%20tasks.%20Our%20results%20underscore%20the%20potential%20of%20causal%0ATransformer%20models%20for%20online%203D%20perception%2C%20paving%20the%20way%20for%20real-time%203D%0Aunderstanding%20in%20streaming%20environments.%20More%20details%20can%20be%20found%20in%20our%0Aproject%20page%3A%20https%3A//nirvanalan.github.io/projects/stream3r.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10893v1&entry.124074799=Read"},
{"title": "DiRW: Path-Aware Digraph Learning for Heterophily", "author": "Daohan Su and Xunkai Li and Zhenjun Li and Yinping Liao and Rong-Hua Li and Guoren Wang", "abstract": "  Recently, graph neural network (GNN) has emerged as a powerful representation\nlearning tool for graph-structured data. However, most approaches are tailored\nfor undirected graphs, neglecting the abundant information in the edges of\ndirected graphs (digraphs). In fact, digraphs are widely applied in the real\nworld and confirmed to address heterophily challenges. Despite recent\nadvancements, existing spatial- and spectral-based DiGNNs have limitations due\nto their complex learning mechanisms and reliance on high-quality topology,\nresulting in low efficiency and unstable performance. To address these issues,\nwe propose Directed Random Walk (DiRW), a plug-and-play strategy for most\nspatial-based DiGNNs and also an innovative model which offers a new digraph\nlearning paradigm. Specifically, it utilizes a direction-aware path sampler\noptimized from the perspectives of walk probability, length, and number in a\nweight-free manner by considering node profiles and topologies. Building upon\nthis, DiRW incorporates a node-wise learnable path aggregator for generalized\nnode representations. Extensive experiments on 9 datasets demonstrate that\nDiRW: (1) enhances most spatial-based methods as a plug-and-play strategy; (2)\nachieves SOTA performance as a new digraph learning paradigm. The source code\nand data are available at https://github.com/dhsiuu/DiRW.\n", "link": "http://arxiv.org/abs/2410.10320v2", "date": "2025-08-14", "relevancy": 2.4883, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5102}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5009}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiRW%3A%20Path-Aware%20Digraph%20Learning%20for%20Heterophily&body=Title%3A%20DiRW%3A%20Path-Aware%20Digraph%20Learning%20for%20Heterophily%0AAuthor%3A%20Daohan%20Su%20and%20Xunkai%20Li%20and%20Zhenjun%20Li%20and%20Yinping%20Liao%20and%20Rong-Hua%20Li%20and%20Guoren%20Wang%0AAbstract%3A%20%20%20Recently%2C%20graph%20neural%20network%20%28GNN%29%20has%20emerged%20as%20a%20powerful%20representation%0Alearning%20tool%20for%20graph-structured%20data.%20However%2C%20most%20approaches%20are%20tailored%0Afor%20undirected%20graphs%2C%20neglecting%20the%20abundant%20information%20in%20the%20edges%20of%0Adirected%20graphs%20%28digraphs%29.%20In%20fact%2C%20digraphs%20are%20widely%20applied%20in%20the%20real%0Aworld%20and%20confirmed%20to%20address%20heterophily%20challenges.%20Despite%20recent%0Aadvancements%2C%20existing%20spatial-%20and%20spectral-based%20DiGNNs%20have%20limitations%20due%0Ato%20their%20complex%20learning%20mechanisms%20and%20reliance%20on%20high-quality%20topology%2C%0Aresulting%20in%20low%20efficiency%20and%20unstable%20performance.%20To%20address%20these%20issues%2C%0Awe%20propose%20Directed%20Random%20Walk%20%28DiRW%29%2C%20a%20plug-and-play%20strategy%20for%20most%0Aspatial-based%20DiGNNs%20and%20also%20an%20innovative%20model%20which%20offers%20a%20new%20digraph%0Alearning%20paradigm.%20Specifically%2C%20it%20utilizes%20a%20direction-aware%20path%20sampler%0Aoptimized%20from%20the%20perspectives%20of%20walk%20probability%2C%20length%2C%20and%20number%20in%20a%0Aweight-free%20manner%20by%20considering%20node%20profiles%20and%20topologies.%20Building%20upon%0Athis%2C%20DiRW%20incorporates%20a%20node-wise%20learnable%20path%20aggregator%20for%20generalized%0Anode%20representations.%20Extensive%20experiments%20on%209%20datasets%20demonstrate%20that%0ADiRW%3A%20%281%29%20enhances%20most%20spatial-based%20methods%20as%20a%20plug-and-play%20strategy%3B%20%282%29%0Aachieves%20SOTA%20performance%20as%20a%20new%20digraph%20learning%20paradigm.%20The%20source%20code%0Aand%20data%20are%20available%20at%20https%3A//github.com/dhsiuu/DiRW.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10320v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiRW%253A%2520Path-Aware%2520Digraph%2520Learning%2520for%2520Heterophily%26entry.906535625%3DDaohan%2520Su%2520and%2520Xunkai%2520Li%2520and%2520Zhenjun%2520Li%2520and%2520Yinping%2520Liao%2520and%2520Rong-Hua%2520Li%2520and%2520Guoren%2520Wang%26entry.1292438233%3D%2520%2520Recently%252C%2520graph%2520neural%2520network%2520%2528GNN%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520representation%250Alearning%2520tool%2520for%2520graph-structured%2520data.%2520However%252C%2520most%2520approaches%2520are%2520tailored%250Afor%2520undirected%2520graphs%252C%2520neglecting%2520the%2520abundant%2520information%2520in%2520the%2520edges%2520of%250Adirected%2520graphs%2520%2528digraphs%2529.%2520In%2520fact%252C%2520digraphs%2520are%2520widely%2520applied%2520in%2520the%2520real%250Aworld%2520and%2520confirmed%2520to%2520address%2520heterophily%2520challenges.%2520Despite%2520recent%250Aadvancements%252C%2520existing%2520spatial-%2520and%2520spectral-based%2520DiGNNs%2520have%2520limitations%2520due%250Ato%2520their%2520complex%2520learning%2520mechanisms%2520and%2520reliance%2520on%2520high-quality%2520topology%252C%250Aresulting%2520in%2520low%2520efficiency%2520and%2520unstable%2520performance.%2520To%2520address%2520these%2520issues%252C%250Awe%2520propose%2520Directed%2520Random%2520Walk%2520%2528DiRW%2529%252C%2520a%2520plug-and-play%2520strategy%2520for%2520most%250Aspatial-based%2520DiGNNs%2520and%2520also%2520an%2520innovative%2520model%2520which%2520offers%2520a%2520new%2520digraph%250Alearning%2520paradigm.%2520Specifically%252C%2520it%2520utilizes%2520a%2520direction-aware%2520path%2520sampler%250Aoptimized%2520from%2520the%2520perspectives%2520of%2520walk%2520probability%252C%2520length%252C%2520and%2520number%2520in%2520a%250Aweight-free%2520manner%2520by%2520considering%2520node%2520profiles%2520and%2520topologies.%2520Building%2520upon%250Athis%252C%2520DiRW%2520incorporates%2520a%2520node-wise%2520learnable%2520path%2520aggregator%2520for%2520generalized%250Anode%2520representations.%2520Extensive%2520experiments%2520on%25209%2520datasets%2520demonstrate%2520that%250ADiRW%253A%2520%25281%2529%2520enhances%2520most%2520spatial-based%2520methods%2520as%2520a%2520plug-and-play%2520strategy%253B%2520%25282%2529%250Aachieves%2520SOTA%2520performance%2520as%2520a%2520new%2520digraph%2520learning%2520paradigm.%2520The%2520source%2520code%250Aand%2520data%2520are%2520available%2520at%2520https%253A//github.com/dhsiuu/DiRW.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10320v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiRW%3A%20Path-Aware%20Digraph%20Learning%20for%20Heterophily&entry.906535625=Daohan%20Su%20and%20Xunkai%20Li%20and%20Zhenjun%20Li%20and%20Yinping%20Liao%20and%20Rong-Hua%20Li%20and%20Guoren%20Wang&entry.1292438233=%20%20Recently%2C%20graph%20neural%20network%20%28GNN%29%20has%20emerged%20as%20a%20powerful%20representation%0Alearning%20tool%20for%20graph-structured%20data.%20However%2C%20most%20approaches%20are%20tailored%0Afor%20undirected%20graphs%2C%20neglecting%20the%20abundant%20information%20in%20the%20edges%20of%0Adirected%20graphs%20%28digraphs%29.%20In%20fact%2C%20digraphs%20are%20widely%20applied%20in%20the%20real%0Aworld%20and%20confirmed%20to%20address%20heterophily%20challenges.%20Despite%20recent%0Aadvancements%2C%20existing%20spatial-%20and%20spectral-based%20DiGNNs%20have%20limitations%20due%0Ato%20their%20complex%20learning%20mechanisms%20and%20reliance%20on%20high-quality%20topology%2C%0Aresulting%20in%20low%20efficiency%20and%20unstable%20performance.%20To%20address%20these%20issues%2C%0Awe%20propose%20Directed%20Random%20Walk%20%28DiRW%29%2C%20a%20plug-and-play%20strategy%20for%20most%0Aspatial-based%20DiGNNs%20and%20also%20an%20innovative%20model%20which%20offers%20a%20new%20digraph%0Alearning%20paradigm.%20Specifically%2C%20it%20utilizes%20a%20direction-aware%20path%20sampler%0Aoptimized%20from%20the%20perspectives%20of%20walk%20probability%2C%20length%2C%20and%20number%20in%20a%0Aweight-free%20manner%20by%20considering%20node%20profiles%20and%20topologies.%20Building%20upon%0Athis%2C%20DiRW%20incorporates%20a%20node-wise%20learnable%20path%20aggregator%20for%20generalized%0Anode%20representations.%20Extensive%20experiments%20on%209%20datasets%20demonstrate%20that%0ADiRW%3A%20%281%29%20enhances%20most%20spatial-based%20methods%20as%20a%20plug-and-play%20strategy%3B%20%282%29%0Aachieves%20SOTA%20performance%20as%20a%20new%20digraph%20learning%20paradigm.%20The%20source%20code%0Aand%20data%20are%20available%20at%20https%3A//github.com/dhsiuu/DiRW.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10320v2&entry.124074799=Read"},
{"title": "From Large Angles to Consistent Faces: Identity-Preserving Video\n  Generation via Mixture of Facial Experts", "author": "Yuji Wang and Moran Li and Xiaobin Hu and Ran Yi and Jiangning Zhang and Chengming Xu and Weijian Cao and Yabiao Wang and Chengjie Wang and Lizhuang Ma", "abstract": "  Current video generation models struggle with identity preservation under\nlarge facial angles, primarily facing two challenges: the difficulty in\nexploring an effective mechanism to integrate identity features into DiT\nstructure, and the lack of targeted coverage of large facial angles in existing\nopen-source video datasets. To address these, we present two key innovations.\nFirst, we introduce a Mixture of Facial Experts (MoFE) that dynamically\ncombines complementary cues from three specialized experts, each designed to\ncapture distinct but mutually reinforcing aspects of facial attributes. The\nidentity expert captures cross-pose identity-sensitive features, the semantic\nexpert extracts high-level visual semantxics, and the detail expert preserves\npixel-level features (e.g., skin texture, color gradients). Furthermore, to\nmitigate dataset limitations, we have tailored a data processing pipeline\ncentered on two key aspects: Face Constraints and Identity Consistency. Face\nConstraints ensure facial angle diversity and a high proportion of facial\nregions, while Identity Consistency preserves coherent person-specific features\nacross temporal sequences, collectively addressing the scarcity of large facial\nangles and identity-stable training data in existing datasets. Leveraging this\npipeline, we have curated and refined a Large Face Angles (LFA) Dataset from\nexisting open-source human video datasets, comprising 460K video clips with\nannotated facial angles. Experimental results on the LFA benchmark demonstrate\nthat our method, empowered by the LFA dataset, significantly outperforms prior\nSOTA methods in face similarity, face FID, and CLIP semantic alignment. The\ncode and dataset will be made publicly available at\nhttps://github.com/rain152/LFA-Video-Generation.\n", "link": "http://arxiv.org/abs/2508.09476v2", "date": "2025-08-14", "relevancy": 2.4711, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6678}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5868}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Large%20Angles%20to%20Consistent%20Faces%3A%20Identity-Preserving%20Video%0A%20%20Generation%20via%20Mixture%20of%20Facial%20Experts&body=Title%3A%20From%20Large%20Angles%20to%20Consistent%20Faces%3A%20Identity-Preserving%20Video%0A%20%20Generation%20via%20Mixture%20of%20Facial%20Experts%0AAuthor%3A%20Yuji%20Wang%20and%20Moran%20Li%20and%20Xiaobin%20Hu%20and%20Ran%20Yi%20and%20Jiangning%20Zhang%20and%20Chengming%20Xu%20and%20Weijian%20Cao%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20Current%20video%20generation%20models%20struggle%20with%20identity%20preservation%20under%0Alarge%20facial%20angles%2C%20primarily%20facing%20two%20challenges%3A%20the%20difficulty%20in%0Aexploring%20an%20effective%20mechanism%20to%20integrate%20identity%20features%20into%20DiT%0Astructure%2C%20and%20the%20lack%20of%20targeted%20coverage%20of%20large%20facial%20angles%20in%20existing%0Aopen-source%20video%20datasets.%20To%20address%20these%2C%20we%20present%20two%20key%20innovations.%0AFirst%2C%20we%20introduce%20a%20Mixture%20of%20Facial%20Experts%20%28MoFE%29%20that%20dynamically%0Acombines%20complementary%20cues%20from%20three%20specialized%20experts%2C%20each%20designed%20to%0Acapture%20distinct%20but%20mutually%20reinforcing%20aspects%20of%20facial%20attributes.%20The%0Aidentity%20expert%20captures%20cross-pose%20identity-sensitive%20features%2C%20the%20semantic%0Aexpert%20extracts%20high-level%20visual%20semantxics%2C%20and%20the%20detail%20expert%20preserves%0Apixel-level%20features%20%28e.g.%2C%20skin%20texture%2C%20color%20gradients%29.%20Furthermore%2C%20to%0Amitigate%20dataset%20limitations%2C%20we%20have%20tailored%20a%20data%20processing%20pipeline%0Acentered%20on%20two%20key%20aspects%3A%20Face%20Constraints%20and%20Identity%20Consistency.%20Face%0AConstraints%20ensure%20facial%20angle%20diversity%20and%20a%20high%20proportion%20of%20facial%0Aregions%2C%20while%20Identity%20Consistency%20preserves%20coherent%20person-specific%20features%0Aacross%20temporal%20sequences%2C%20collectively%20addressing%20the%20scarcity%20of%20large%20facial%0Aangles%20and%20identity-stable%20training%20data%20in%20existing%20datasets.%20Leveraging%20this%0Apipeline%2C%20we%20have%20curated%20and%20refined%20a%20Large%20Face%20Angles%20%28LFA%29%20Dataset%20from%0Aexisting%20open-source%20human%20video%20datasets%2C%20comprising%20460K%20video%20clips%20with%0Aannotated%20facial%20angles.%20Experimental%20results%20on%20the%20LFA%20benchmark%20demonstrate%0Athat%20our%20method%2C%20empowered%20by%20the%20LFA%20dataset%2C%20significantly%20outperforms%20prior%0ASOTA%20methods%20in%20face%20similarity%2C%20face%20FID%2C%20and%20CLIP%20semantic%20alignment.%20The%0Acode%20and%20dataset%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/rain152/LFA-Video-Generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09476v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Large%2520Angles%2520to%2520Consistent%2520Faces%253A%2520Identity-Preserving%2520Video%250A%2520%2520Generation%2520via%2520Mixture%2520of%2520Facial%2520Experts%26entry.906535625%3DYuji%2520Wang%2520and%2520Moran%2520Li%2520and%2520Xiaobin%2520Hu%2520and%2520Ran%2520Yi%2520and%2520Jiangning%2520Zhang%2520and%2520Chengming%2520Xu%2520and%2520Weijian%2520Cao%2520and%2520Yabiao%2520Wang%2520and%2520Chengjie%2520Wang%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3D%2520%2520Current%2520video%2520generation%2520models%2520struggle%2520with%2520identity%2520preservation%2520under%250Alarge%2520facial%2520angles%252C%2520primarily%2520facing%2520two%2520challenges%253A%2520the%2520difficulty%2520in%250Aexploring%2520an%2520effective%2520mechanism%2520to%2520integrate%2520identity%2520features%2520into%2520DiT%250Astructure%252C%2520and%2520the%2520lack%2520of%2520targeted%2520coverage%2520of%2520large%2520facial%2520angles%2520in%2520existing%250Aopen-source%2520video%2520datasets.%2520To%2520address%2520these%252C%2520we%2520present%2520two%2520key%2520innovations.%250AFirst%252C%2520we%2520introduce%2520a%2520Mixture%2520of%2520Facial%2520Experts%2520%2528MoFE%2529%2520that%2520dynamically%250Acombines%2520complementary%2520cues%2520from%2520three%2520specialized%2520experts%252C%2520each%2520designed%2520to%250Acapture%2520distinct%2520but%2520mutually%2520reinforcing%2520aspects%2520of%2520facial%2520attributes.%2520The%250Aidentity%2520expert%2520captures%2520cross-pose%2520identity-sensitive%2520features%252C%2520the%2520semantic%250Aexpert%2520extracts%2520high-level%2520visual%2520semantxics%252C%2520and%2520the%2520detail%2520expert%2520preserves%250Apixel-level%2520features%2520%2528e.g.%252C%2520skin%2520texture%252C%2520color%2520gradients%2529.%2520Furthermore%252C%2520to%250Amitigate%2520dataset%2520limitations%252C%2520we%2520have%2520tailored%2520a%2520data%2520processing%2520pipeline%250Acentered%2520on%2520two%2520key%2520aspects%253A%2520Face%2520Constraints%2520and%2520Identity%2520Consistency.%2520Face%250AConstraints%2520ensure%2520facial%2520angle%2520diversity%2520and%2520a%2520high%2520proportion%2520of%2520facial%250Aregions%252C%2520while%2520Identity%2520Consistency%2520preserves%2520coherent%2520person-specific%2520features%250Aacross%2520temporal%2520sequences%252C%2520collectively%2520addressing%2520the%2520scarcity%2520of%2520large%2520facial%250Aangles%2520and%2520identity-stable%2520training%2520data%2520in%2520existing%2520datasets.%2520Leveraging%2520this%250Apipeline%252C%2520we%2520have%2520curated%2520and%2520refined%2520a%2520Large%2520Face%2520Angles%2520%2528LFA%2529%2520Dataset%2520from%250Aexisting%2520open-source%2520human%2520video%2520datasets%252C%2520comprising%2520460K%2520video%2520clips%2520with%250Aannotated%2520facial%2520angles.%2520Experimental%2520results%2520on%2520the%2520LFA%2520benchmark%2520demonstrate%250Athat%2520our%2520method%252C%2520empowered%2520by%2520the%2520LFA%2520dataset%252C%2520significantly%2520outperforms%2520prior%250ASOTA%2520methods%2520in%2520face%2520similarity%252C%2520face%2520FID%252C%2520and%2520CLIP%2520semantic%2520alignment.%2520The%250Acode%2520and%2520dataset%2520will%2520be%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/rain152/LFA-Video-Generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09476v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Large%20Angles%20to%20Consistent%20Faces%3A%20Identity-Preserving%20Video%0A%20%20Generation%20via%20Mixture%20of%20Facial%20Experts&entry.906535625=Yuji%20Wang%20and%20Moran%20Li%20and%20Xiaobin%20Hu%20and%20Ran%20Yi%20and%20Jiangning%20Zhang%20and%20Chengming%20Xu%20and%20Weijian%20Cao%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Lizhuang%20Ma&entry.1292438233=%20%20Current%20video%20generation%20models%20struggle%20with%20identity%20preservation%20under%0Alarge%20facial%20angles%2C%20primarily%20facing%20two%20challenges%3A%20the%20difficulty%20in%0Aexploring%20an%20effective%20mechanism%20to%20integrate%20identity%20features%20into%20DiT%0Astructure%2C%20and%20the%20lack%20of%20targeted%20coverage%20of%20large%20facial%20angles%20in%20existing%0Aopen-source%20video%20datasets.%20To%20address%20these%2C%20we%20present%20two%20key%20innovations.%0AFirst%2C%20we%20introduce%20a%20Mixture%20of%20Facial%20Experts%20%28MoFE%29%20that%20dynamically%0Acombines%20complementary%20cues%20from%20three%20specialized%20experts%2C%20each%20designed%20to%0Acapture%20distinct%20but%20mutually%20reinforcing%20aspects%20of%20facial%20attributes.%20The%0Aidentity%20expert%20captures%20cross-pose%20identity-sensitive%20features%2C%20the%20semantic%0Aexpert%20extracts%20high-level%20visual%20semantxics%2C%20and%20the%20detail%20expert%20preserves%0Apixel-level%20features%20%28e.g.%2C%20skin%20texture%2C%20color%20gradients%29.%20Furthermore%2C%20to%0Amitigate%20dataset%20limitations%2C%20we%20have%20tailored%20a%20data%20processing%20pipeline%0Acentered%20on%20two%20key%20aspects%3A%20Face%20Constraints%20and%20Identity%20Consistency.%20Face%0AConstraints%20ensure%20facial%20angle%20diversity%20and%20a%20high%20proportion%20of%20facial%0Aregions%2C%20while%20Identity%20Consistency%20preserves%20coherent%20person-specific%20features%0Aacross%20temporal%20sequences%2C%20collectively%20addressing%20the%20scarcity%20of%20large%20facial%0Aangles%20and%20identity-stable%20training%20data%20in%20existing%20datasets.%20Leveraging%20this%0Apipeline%2C%20we%20have%20curated%20and%20refined%20a%20Large%20Face%20Angles%20%28LFA%29%20Dataset%20from%0Aexisting%20open-source%20human%20video%20datasets%2C%20comprising%20460K%20video%20clips%20with%0Aannotated%20facial%20angles.%20Experimental%20results%20on%20the%20LFA%20benchmark%20demonstrate%0Athat%20our%20method%2C%20empowered%20by%20the%20LFA%20dataset%2C%20significantly%20outperforms%20prior%0ASOTA%20methods%20in%20face%20similarity%2C%20face%20FID%2C%20and%20CLIP%20semantic%20alignment.%20The%0Acode%20and%20dataset%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/rain152/LFA-Video-Generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09476v2&entry.124074799=Read"},
{"title": "Comparison of Data Reduction Criteria for Online Gaussian Processes", "author": "Thore Wietzke and Knut Graichen", "abstract": "  Gaussian Processes (GPs) are widely used for regression and system\nidentification due to their flexibility and ability to quantify uncertainty.\nHowever, their computational complexity limits their applicability to small\ndatasets. Moreover in a streaming scenario, more and more datapoints accumulate\nwhich is intractable even for Sparse GPs. Online GPs aim to alleviate this\nproblem by e.g. defining a maximum budget of datapoints and removing redundant\ndatapoints. This work provides a unified comparison of several reduction\ncriteria, analyzing both their computational complexity and reduction behavior.\nThe criteria are evaluated on benchmark functions and real-world datasets,\nincluding dynamic system identification tasks. Additionally, acceptance\ncriteria are proposed to further filter out redundant datapoints. This work\nyields practical guidelines for choosing a suitable criterion for an online GP\nalgorithm.\n", "link": "http://arxiv.org/abs/2508.10815v1", "date": "2025-08-14", "relevancy": 2.4679, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5114}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4978}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparison%20of%20Data%20Reduction%20Criteria%20for%20Online%20Gaussian%20Processes&body=Title%3A%20Comparison%20of%20Data%20Reduction%20Criteria%20for%20Online%20Gaussian%20Processes%0AAuthor%3A%20Thore%20Wietzke%20and%20Knut%20Graichen%0AAbstract%3A%20%20%20Gaussian%20Processes%20%28GPs%29%20are%20widely%20used%20for%20regression%20and%20system%0Aidentification%20due%20to%20their%20flexibility%20and%20ability%20to%20quantify%20uncertainty.%0AHowever%2C%20their%20computational%20complexity%20limits%20their%20applicability%20to%20small%0Adatasets.%20Moreover%20in%20a%20streaming%20scenario%2C%20more%20and%20more%20datapoints%20accumulate%0Awhich%20is%20intractable%20even%20for%20Sparse%20GPs.%20Online%20GPs%20aim%20to%20alleviate%20this%0Aproblem%20by%20e.g.%20defining%20a%20maximum%20budget%20of%20datapoints%20and%20removing%20redundant%0Adatapoints.%20This%20work%20provides%20a%20unified%20comparison%20of%20several%20reduction%0Acriteria%2C%20analyzing%20both%20their%20computational%20complexity%20and%20reduction%20behavior.%0AThe%20criteria%20are%20evaluated%20on%20benchmark%20functions%20and%20real-world%20datasets%2C%0Aincluding%20dynamic%20system%20identification%20tasks.%20Additionally%2C%20acceptance%0Acriteria%20are%20proposed%20to%20further%20filter%20out%20redundant%20datapoints.%20This%20work%0Ayields%20practical%20guidelines%20for%20choosing%20a%20suitable%20criterion%20for%20an%20online%20GP%0Aalgorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparison%2520of%2520Data%2520Reduction%2520Criteria%2520for%2520Online%2520Gaussian%2520Processes%26entry.906535625%3DThore%2520Wietzke%2520and%2520Knut%2520Graichen%26entry.1292438233%3D%2520%2520Gaussian%2520Processes%2520%2528GPs%2529%2520are%2520widely%2520used%2520for%2520regression%2520and%2520system%250Aidentification%2520due%2520to%2520their%2520flexibility%2520and%2520ability%2520to%2520quantify%2520uncertainty.%250AHowever%252C%2520their%2520computational%2520complexity%2520limits%2520their%2520applicability%2520to%2520small%250Adatasets.%2520Moreover%2520in%2520a%2520streaming%2520scenario%252C%2520more%2520and%2520more%2520datapoints%2520accumulate%250Awhich%2520is%2520intractable%2520even%2520for%2520Sparse%2520GPs.%2520Online%2520GPs%2520aim%2520to%2520alleviate%2520this%250Aproblem%2520by%2520e.g.%2520defining%2520a%2520maximum%2520budget%2520of%2520datapoints%2520and%2520removing%2520redundant%250Adatapoints.%2520This%2520work%2520provides%2520a%2520unified%2520comparison%2520of%2520several%2520reduction%250Acriteria%252C%2520analyzing%2520both%2520their%2520computational%2520complexity%2520and%2520reduction%2520behavior.%250AThe%2520criteria%2520are%2520evaluated%2520on%2520benchmark%2520functions%2520and%2520real-world%2520datasets%252C%250Aincluding%2520dynamic%2520system%2520identification%2520tasks.%2520Additionally%252C%2520acceptance%250Acriteria%2520are%2520proposed%2520to%2520further%2520filter%2520out%2520redundant%2520datapoints.%2520This%2520work%250Ayields%2520practical%2520guidelines%2520for%2520choosing%2520a%2520suitable%2520criterion%2520for%2520an%2520online%2520GP%250Aalgorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparison%20of%20Data%20Reduction%20Criteria%20for%20Online%20Gaussian%20Processes&entry.906535625=Thore%20Wietzke%20and%20Knut%20Graichen&entry.1292438233=%20%20Gaussian%20Processes%20%28GPs%29%20are%20widely%20used%20for%20regression%20and%20system%0Aidentification%20due%20to%20their%20flexibility%20and%20ability%20to%20quantify%20uncertainty.%0AHowever%2C%20their%20computational%20complexity%20limits%20their%20applicability%20to%20small%0Adatasets.%20Moreover%20in%20a%20streaming%20scenario%2C%20more%20and%20more%20datapoints%20accumulate%0Awhich%20is%20intractable%20even%20for%20Sparse%20GPs.%20Online%20GPs%20aim%20to%20alleviate%20this%0Aproblem%20by%20e.g.%20defining%20a%20maximum%20budget%20of%20datapoints%20and%20removing%20redundant%0Adatapoints.%20This%20work%20provides%20a%20unified%20comparison%20of%20several%20reduction%0Acriteria%2C%20analyzing%20both%20their%20computational%20complexity%20and%20reduction%20behavior.%0AThe%20criteria%20are%20evaluated%20on%20benchmark%20functions%20and%20real-world%20datasets%2C%0Aincluding%20dynamic%20system%20identification%20tasks.%20Additionally%2C%20acceptance%0Acriteria%20are%20proposed%20to%20further%20filter%20out%20redundant%20datapoints.%20This%20work%0Ayields%20practical%20guidelines%20for%20choosing%20a%20suitable%20criterion%20for%20an%20online%20GP%0Aalgorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10815v1&entry.124074799=Read"},
{"title": "Revisiting Cross-View Localization from Image Matching", "author": "Panwang Xia and Qiong Wu and Lei Yu and Yi Liu and Mingtao Xiong and Lei Liang and Yongjun Zhang and Yi Wan", "abstract": "  Cross-view localization aims to estimate the 3 degrees of freedom pose of a\nground-view image by registering it to aerial or satellite imagery. It is\nessential in GNSS-denied environments such as urban canyons and disaster zones.\nExisting methods either regress poses directly or align features in a shared\nbird's-eye view (BEV) space, both built upon accurate spatial correspondences\nbetween perspectives. However, these methods fail to establish strict\ncross-view correspondences, yielding only coarse or geometrically inconsistent\nmatches. Consequently, fine-grained image matching between ground and aerial\nviews remains an unsolved problem, which in turn constrains the\ninterpretability of localization results. In this paper, we revisit cross-view\nlocalization from the perspective of cross-view image matching and propose a\nnovel framework that improves both matching and localization. Specifically, we\nintroduce a Surface Model to model visible regions for accurate BEV projection,\nand a SimRefiner module to refine the similarity matrix through local-global\nresidual correction, eliminating the reliance on post-processing like RANSAC.\nTo further support research in this area, we introduce CVFM, the first\nbenchmark with 32,509 cross-view image pairs annotated with pixel-level\ncorrespondences. Extensive experiments demonstrate that our approach\nsubstantially improves both localization accuracy and image matching quality,\nsetting new baselines under extreme viewpoint disparity.\n", "link": "http://arxiv.org/abs/2508.10716v1", "date": "2025-08-14", "relevancy": 2.4664, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6486}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5988}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Cross-View%20Localization%20from%20Image%20Matching&body=Title%3A%20Revisiting%20Cross-View%20Localization%20from%20Image%20Matching%0AAuthor%3A%20Panwang%20Xia%20and%20Qiong%20Wu%20and%20Lei%20Yu%20and%20Yi%20Liu%20and%20Mingtao%20Xiong%20and%20Lei%20Liang%20and%20Yongjun%20Zhang%20and%20Yi%20Wan%0AAbstract%3A%20%20%20Cross-view%20localization%20aims%20to%20estimate%20the%203%20degrees%20of%20freedom%20pose%20of%20a%0Aground-view%20image%20by%20registering%20it%20to%20aerial%20or%20satellite%20imagery.%20It%20is%0Aessential%20in%20GNSS-denied%20environments%20such%20as%20urban%20canyons%20and%20disaster%20zones.%0AExisting%20methods%20either%20regress%20poses%20directly%20or%20align%20features%20in%20a%20shared%0Abird%27s-eye%20view%20%28BEV%29%20space%2C%20both%20built%20upon%20accurate%20spatial%20correspondences%0Abetween%20perspectives.%20However%2C%20these%20methods%20fail%20to%20establish%20strict%0Across-view%20correspondences%2C%20yielding%20only%20coarse%20or%20geometrically%20inconsistent%0Amatches.%20Consequently%2C%20fine-grained%20image%20matching%20between%20ground%20and%20aerial%0Aviews%20remains%20an%20unsolved%20problem%2C%20which%20in%20turn%20constrains%20the%0Ainterpretability%20of%20localization%20results.%20In%20this%20paper%2C%20we%20revisit%20cross-view%0Alocalization%20from%20the%20perspective%20of%20cross-view%20image%20matching%20and%20propose%20a%0Anovel%20framework%20that%20improves%20both%20matching%20and%20localization.%20Specifically%2C%20we%0Aintroduce%20a%20Surface%20Model%20to%20model%20visible%20regions%20for%20accurate%20BEV%20projection%2C%0Aand%20a%20SimRefiner%20module%20to%20refine%20the%20similarity%20matrix%20through%20local-global%0Aresidual%20correction%2C%20eliminating%20the%20reliance%20on%20post-processing%20like%20RANSAC.%0ATo%20further%20support%20research%20in%20this%20area%2C%20we%20introduce%20CVFM%2C%20the%20first%0Abenchmark%20with%2032%2C509%20cross-view%20image%20pairs%20annotated%20with%20pixel-level%0Acorrespondences.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%0Asubstantially%20improves%20both%20localization%20accuracy%20and%20image%20matching%20quality%2C%0Asetting%20new%20baselines%20under%20extreme%20viewpoint%20disparity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Cross-View%2520Localization%2520from%2520Image%2520Matching%26entry.906535625%3DPanwang%2520Xia%2520and%2520Qiong%2520Wu%2520and%2520Lei%2520Yu%2520and%2520Yi%2520Liu%2520and%2520Mingtao%2520Xiong%2520and%2520Lei%2520Liang%2520and%2520Yongjun%2520Zhang%2520and%2520Yi%2520Wan%26entry.1292438233%3D%2520%2520Cross-view%2520localization%2520aims%2520to%2520estimate%2520the%25203%2520degrees%2520of%2520freedom%2520pose%2520of%2520a%250Aground-view%2520image%2520by%2520registering%2520it%2520to%2520aerial%2520or%2520satellite%2520imagery.%2520It%2520is%250Aessential%2520in%2520GNSS-denied%2520environments%2520such%2520as%2520urban%2520canyons%2520and%2520disaster%2520zones.%250AExisting%2520methods%2520either%2520regress%2520poses%2520directly%2520or%2520align%2520features%2520in%2520a%2520shared%250Abird%2527s-eye%2520view%2520%2528BEV%2529%2520space%252C%2520both%2520built%2520upon%2520accurate%2520spatial%2520correspondences%250Abetween%2520perspectives.%2520However%252C%2520these%2520methods%2520fail%2520to%2520establish%2520strict%250Across-view%2520correspondences%252C%2520yielding%2520only%2520coarse%2520or%2520geometrically%2520inconsistent%250Amatches.%2520Consequently%252C%2520fine-grained%2520image%2520matching%2520between%2520ground%2520and%2520aerial%250Aviews%2520remains%2520an%2520unsolved%2520problem%252C%2520which%2520in%2520turn%2520constrains%2520the%250Ainterpretability%2520of%2520localization%2520results.%2520In%2520this%2520paper%252C%2520we%2520revisit%2520cross-view%250Alocalization%2520from%2520the%2520perspective%2520of%2520cross-view%2520image%2520matching%2520and%2520propose%2520a%250Anovel%2520framework%2520that%2520improves%2520both%2520matching%2520and%2520localization.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520Surface%2520Model%2520to%2520model%2520visible%2520regions%2520for%2520accurate%2520BEV%2520projection%252C%250Aand%2520a%2520SimRefiner%2520module%2520to%2520refine%2520the%2520similarity%2520matrix%2520through%2520local-global%250Aresidual%2520correction%252C%2520eliminating%2520the%2520reliance%2520on%2520post-processing%2520like%2520RANSAC.%250ATo%2520further%2520support%2520research%2520in%2520this%2520area%252C%2520we%2520introduce%2520CVFM%252C%2520the%2520first%250Abenchmark%2520with%252032%252C509%2520cross-view%2520image%2520pairs%2520annotated%2520with%2520pixel-level%250Acorrespondences.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%250Asubstantially%2520improves%2520both%2520localization%2520accuracy%2520and%2520image%2520matching%2520quality%252C%250Asetting%2520new%2520baselines%2520under%2520extreme%2520viewpoint%2520disparity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Cross-View%20Localization%20from%20Image%20Matching&entry.906535625=Panwang%20Xia%20and%20Qiong%20Wu%20and%20Lei%20Yu%20and%20Yi%20Liu%20and%20Mingtao%20Xiong%20and%20Lei%20Liang%20and%20Yongjun%20Zhang%20and%20Yi%20Wan&entry.1292438233=%20%20Cross-view%20localization%20aims%20to%20estimate%20the%203%20degrees%20of%20freedom%20pose%20of%20a%0Aground-view%20image%20by%20registering%20it%20to%20aerial%20or%20satellite%20imagery.%20It%20is%0Aessential%20in%20GNSS-denied%20environments%20such%20as%20urban%20canyons%20and%20disaster%20zones.%0AExisting%20methods%20either%20regress%20poses%20directly%20or%20align%20features%20in%20a%20shared%0Abird%27s-eye%20view%20%28BEV%29%20space%2C%20both%20built%20upon%20accurate%20spatial%20correspondences%0Abetween%20perspectives.%20However%2C%20these%20methods%20fail%20to%20establish%20strict%0Across-view%20correspondences%2C%20yielding%20only%20coarse%20or%20geometrically%20inconsistent%0Amatches.%20Consequently%2C%20fine-grained%20image%20matching%20between%20ground%20and%20aerial%0Aviews%20remains%20an%20unsolved%20problem%2C%20which%20in%20turn%20constrains%20the%0Ainterpretability%20of%20localization%20results.%20In%20this%20paper%2C%20we%20revisit%20cross-view%0Alocalization%20from%20the%20perspective%20of%20cross-view%20image%20matching%20and%20propose%20a%0Anovel%20framework%20that%20improves%20both%20matching%20and%20localization.%20Specifically%2C%20we%0Aintroduce%20a%20Surface%20Model%20to%20model%20visible%20regions%20for%20accurate%20BEV%20projection%2C%0Aand%20a%20SimRefiner%20module%20to%20refine%20the%20similarity%20matrix%20through%20local-global%0Aresidual%20correction%2C%20eliminating%20the%20reliance%20on%20post-processing%20like%20RANSAC.%0ATo%20further%20support%20research%20in%20this%20area%2C%20we%20introduce%20CVFM%2C%20the%20first%0Abenchmark%20with%2032%2C509%20cross-view%20image%20pairs%20annotated%20with%20pixel-level%0Acorrespondences.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%0Asubstantially%20improves%20both%20localization%20accuracy%20and%20image%20matching%20quality%2C%0Asetting%20new%20baselines%20under%20extreme%20viewpoint%20disparity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10716v1&entry.124074799=Read"},
{"title": "Hierarchical Fine-grained Preference Optimization for Physically\n  Plausible Video Generation", "author": "Harold Haodong Chen and Haojian Huang and Qifeng Chen and Harry Yang and Ser-Nam Lim", "abstract": "  Recent advancements in video generation have enabled the creation of\nhigh-quality, visually compelling videos. However, generating videos that\nadhere to the laws of physics remains a critical challenge for applications\nrequiring realism and accuracy. In this work, we propose PhysHPO, a novel\nframework for Hierarchical Cross-Modal Direct Preference Optimization, to\ntackle this challenge by enabling fine-grained preference alignment for\nphysically plausible video generation. PhysHPO optimizes video alignment across\nfour hierarchical granularities: a) Instance Level, aligning the overall video\ncontent with the input prompt; b) State Level, ensuring temporal consistency\nusing boundary frames as anchors; c) Motion Level, modeling motion trajectories\nfor realistic dynamics; and d) Semantic Level, maintaining logical consistency\nbetween narrative and visuals. Recognizing that real-world videos are the best\nreflections of physical phenomena, we further introduce an automated data\nselection pipeline to efficiently identify and utilize \"good data\" from\nexisting large-scale text-video datasets, thereby eliminating the need for\ncostly and time-intensive dataset construction. Extensive experiments on both\nphysics-focused and general capability benchmarks demonstrate that PhysHPO\nsignificantly improves physical plausibility and overall video generation\nquality of advanced models. To the best of our knowledge, this is the first\nwork to explore fine-grained preference alignment and data selection for video\ngeneration, paving the way for more realistic and human-preferred video\ngeneration paradigms.\n", "link": "http://arxiv.org/abs/2508.10858v1", "date": "2025-08-14", "relevancy": 2.4643, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6749}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5741}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Fine-grained%20Preference%20Optimization%20for%20Physically%0A%20%20Plausible%20Video%20Generation&body=Title%3A%20Hierarchical%20Fine-grained%20Preference%20Optimization%20for%20Physically%0A%20%20Plausible%20Video%20Generation%0AAuthor%3A%20Harold%20Haodong%20Chen%20and%20Haojian%20Huang%20and%20Qifeng%20Chen%20and%20Harry%20Yang%20and%20Ser-Nam%20Lim%0AAbstract%3A%20%20%20Recent%20advancements%20in%20video%20generation%20have%20enabled%20the%20creation%20of%0Ahigh-quality%2C%20visually%20compelling%20videos.%20However%2C%20generating%20videos%20that%0Aadhere%20to%20the%20laws%20of%20physics%20remains%20a%20critical%20challenge%20for%20applications%0Arequiring%20realism%20and%20accuracy.%20In%20this%20work%2C%20we%20propose%20PhysHPO%2C%20a%20novel%0Aframework%20for%20Hierarchical%20Cross-Modal%20Direct%20Preference%20Optimization%2C%20to%0Atackle%20this%20challenge%20by%20enabling%20fine-grained%20preference%20alignment%20for%0Aphysically%20plausible%20video%20generation.%20PhysHPO%20optimizes%20video%20alignment%20across%0Afour%20hierarchical%20granularities%3A%20a%29%20Instance%20Level%2C%20aligning%20the%20overall%20video%0Acontent%20with%20the%20input%20prompt%3B%20b%29%20State%20Level%2C%20ensuring%20temporal%20consistency%0Ausing%20boundary%20frames%20as%20anchors%3B%20c%29%20Motion%20Level%2C%20modeling%20motion%20trajectories%0Afor%20realistic%20dynamics%3B%20and%20d%29%20Semantic%20Level%2C%20maintaining%20logical%20consistency%0Abetween%20narrative%20and%20visuals.%20Recognizing%20that%20real-world%20videos%20are%20the%20best%0Areflections%20of%20physical%20phenomena%2C%20we%20further%20introduce%20an%20automated%20data%0Aselection%20pipeline%20to%20efficiently%20identify%20and%20utilize%20%22good%20data%22%20from%0Aexisting%20large-scale%20text-video%20datasets%2C%20thereby%20eliminating%20the%20need%20for%0Acostly%20and%20time-intensive%20dataset%20construction.%20Extensive%20experiments%20on%20both%0Aphysics-focused%20and%20general%20capability%20benchmarks%20demonstrate%20that%20PhysHPO%0Asignificantly%20improves%20physical%20plausibility%20and%20overall%20video%20generation%0Aquality%20of%20advanced%20models.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Awork%20to%20explore%20fine-grained%20preference%20alignment%20and%20data%20selection%20for%20video%0Ageneration%2C%20paving%20the%20way%20for%20more%20realistic%20and%20human-preferred%20video%0Ageneration%20paradigms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10858v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Fine-grained%2520Preference%2520Optimization%2520for%2520Physically%250A%2520%2520Plausible%2520Video%2520Generation%26entry.906535625%3DHarold%2520Haodong%2520Chen%2520and%2520Haojian%2520Huang%2520and%2520Qifeng%2520Chen%2520and%2520Harry%2520Yang%2520and%2520Ser-Nam%2520Lim%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520video%2520generation%2520have%2520enabled%2520the%2520creation%2520of%250Ahigh-quality%252C%2520visually%2520compelling%2520videos.%2520However%252C%2520generating%2520videos%2520that%250Aadhere%2520to%2520the%2520laws%2520of%2520physics%2520remains%2520a%2520critical%2520challenge%2520for%2520applications%250Arequiring%2520realism%2520and%2520accuracy.%2520In%2520this%2520work%252C%2520we%2520propose%2520PhysHPO%252C%2520a%2520novel%250Aframework%2520for%2520Hierarchical%2520Cross-Modal%2520Direct%2520Preference%2520Optimization%252C%2520to%250Atackle%2520this%2520challenge%2520by%2520enabling%2520fine-grained%2520preference%2520alignment%2520for%250Aphysically%2520plausible%2520video%2520generation.%2520PhysHPO%2520optimizes%2520video%2520alignment%2520across%250Afour%2520hierarchical%2520granularities%253A%2520a%2529%2520Instance%2520Level%252C%2520aligning%2520the%2520overall%2520video%250Acontent%2520with%2520the%2520input%2520prompt%253B%2520b%2529%2520State%2520Level%252C%2520ensuring%2520temporal%2520consistency%250Ausing%2520boundary%2520frames%2520as%2520anchors%253B%2520c%2529%2520Motion%2520Level%252C%2520modeling%2520motion%2520trajectories%250Afor%2520realistic%2520dynamics%253B%2520and%2520d%2529%2520Semantic%2520Level%252C%2520maintaining%2520logical%2520consistency%250Abetween%2520narrative%2520and%2520visuals.%2520Recognizing%2520that%2520real-world%2520videos%2520are%2520the%2520best%250Areflections%2520of%2520physical%2520phenomena%252C%2520we%2520further%2520introduce%2520an%2520automated%2520data%250Aselection%2520pipeline%2520to%2520efficiently%2520identify%2520and%2520utilize%2520%2522good%2520data%2522%2520from%250Aexisting%2520large-scale%2520text-video%2520datasets%252C%2520thereby%2520eliminating%2520the%2520need%2520for%250Acostly%2520and%2520time-intensive%2520dataset%2520construction.%2520Extensive%2520experiments%2520on%2520both%250Aphysics-focused%2520and%2520general%2520capability%2520benchmarks%2520demonstrate%2520that%2520PhysHPO%250Asignificantly%2520improves%2520physical%2520plausibility%2520and%2520overall%2520video%2520generation%250Aquality%2520of%2520advanced%2520models.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Awork%2520to%2520explore%2520fine-grained%2520preference%2520alignment%2520and%2520data%2520selection%2520for%2520video%250Ageneration%252C%2520paving%2520the%2520way%2520for%2520more%2520realistic%2520and%2520human-preferred%2520video%250Ageneration%2520paradigms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10858v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Fine-grained%20Preference%20Optimization%20for%20Physically%0A%20%20Plausible%20Video%20Generation&entry.906535625=Harold%20Haodong%20Chen%20and%20Haojian%20Huang%20and%20Qifeng%20Chen%20and%20Harry%20Yang%20and%20Ser-Nam%20Lim&entry.1292438233=%20%20Recent%20advancements%20in%20video%20generation%20have%20enabled%20the%20creation%20of%0Ahigh-quality%2C%20visually%20compelling%20videos.%20However%2C%20generating%20videos%20that%0Aadhere%20to%20the%20laws%20of%20physics%20remains%20a%20critical%20challenge%20for%20applications%0Arequiring%20realism%20and%20accuracy.%20In%20this%20work%2C%20we%20propose%20PhysHPO%2C%20a%20novel%0Aframework%20for%20Hierarchical%20Cross-Modal%20Direct%20Preference%20Optimization%2C%20to%0Atackle%20this%20challenge%20by%20enabling%20fine-grained%20preference%20alignment%20for%0Aphysically%20plausible%20video%20generation.%20PhysHPO%20optimizes%20video%20alignment%20across%0Afour%20hierarchical%20granularities%3A%20a%29%20Instance%20Level%2C%20aligning%20the%20overall%20video%0Acontent%20with%20the%20input%20prompt%3B%20b%29%20State%20Level%2C%20ensuring%20temporal%20consistency%0Ausing%20boundary%20frames%20as%20anchors%3B%20c%29%20Motion%20Level%2C%20modeling%20motion%20trajectories%0Afor%20realistic%20dynamics%3B%20and%20d%29%20Semantic%20Level%2C%20maintaining%20logical%20consistency%0Abetween%20narrative%20and%20visuals.%20Recognizing%20that%20real-world%20videos%20are%20the%20best%0Areflections%20of%20physical%20phenomena%2C%20we%20further%20introduce%20an%20automated%20data%0Aselection%20pipeline%20to%20efficiently%20identify%20and%20utilize%20%22good%20data%22%20from%0Aexisting%20large-scale%20text-video%20datasets%2C%20thereby%20eliminating%20the%20need%20for%0Acostly%20and%20time-intensive%20dataset%20construction.%20Extensive%20experiments%20on%20both%0Aphysics-focused%20and%20general%20capability%20benchmarks%20demonstrate%20that%20PhysHPO%0Asignificantly%20improves%20physical%20plausibility%20and%20overall%20video%20generation%0Aquality%20of%20advanced%20models.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Awork%20to%20explore%20fine-grained%20preference%20alignment%20and%20data%20selection%20for%20video%0Ageneration%2C%20paving%20the%20way%20for%20more%20realistic%20and%20human-preferred%20video%0Ageneration%20paradigms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10858v1&entry.124074799=Read"},
{"title": "Iterative Volume Fusion for Asymmetric Stereo Matching", "author": "Yuanting Gao and Linghao Shen", "abstract": "  Stereo matching is vital in 3D computer vision, with most algorithms assuming\nsymmetric visual properties between binocular visions. However, the rise of\nasymmetric multi-camera systems (e.g., tele-wide cameras) challenges this\nassumption and complicates stereo matching. Visual asymmetry disrupts stereo\nmatching by affecting the crucial cost volume computation. To address this, we\nexplore the matching cost distribution of two established cost volume\nconstruction methods in asymmetric stereo. We find that each cost volume\nexperiences distinct information distortion, indicating that both should be\ncomprehensively utilized to solve the issue. Based on this, we propose the\ntwo-phase Iterative Volume Fusion network for Asymmetric Stereo matching\n(IVF-AStereo). Initially, the aggregated concatenation volume refines the\ncorrelation volume. Subsequently, both volumes are fused to enhance fine\ndetails. Our method excels in asymmetric scenarios and shows robust performance\nagainst significant visual asymmetry. Extensive comparative experiments on\nbenchmark datasets, along with ablation studies, confirm the effectiveness of\nour approach in asymmetric stereo with resolution and color degradation.\n", "link": "http://arxiv.org/abs/2508.09543v2", "date": "2025-08-14", "relevancy": 2.4633, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4989}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4937}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Volume%20Fusion%20for%20Asymmetric%20Stereo%20Matching&body=Title%3A%20Iterative%20Volume%20Fusion%20for%20Asymmetric%20Stereo%20Matching%0AAuthor%3A%20Yuanting%20Gao%20and%20Linghao%20Shen%0AAbstract%3A%20%20%20Stereo%20matching%20is%20vital%20in%203D%20computer%20vision%2C%20with%20most%20algorithms%20assuming%0Asymmetric%20visual%20properties%20between%20binocular%20visions.%20However%2C%20the%20rise%20of%0Aasymmetric%20multi-camera%20systems%20%28e.g.%2C%20tele-wide%20cameras%29%20challenges%20this%0Aassumption%20and%20complicates%20stereo%20matching.%20Visual%20asymmetry%20disrupts%20stereo%0Amatching%20by%20affecting%20the%20crucial%20cost%20volume%20computation.%20To%20address%20this%2C%20we%0Aexplore%20the%20matching%20cost%20distribution%20of%20two%20established%20cost%20volume%0Aconstruction%20methods%20in%20asymmetric%20stereo.%20We%20find%20that%20each%20cost%20volume%0Aexperiences%20distinct%20information%20distortion%2C%20indicating%20that%20both%20should%20be%0Acomprehensively%20utilized%20to%20solve%20the%20issue.%20Based%20on%20this%2C%20we%20propose%20the%0Atwo-phase%20Iterative%20Volume%20Fusion%20network%20for%20Asymmetric%20Stereo%20matching%0A%28IVF-AStereo%29.%20Initially%2C%20the%20aggregated%20concatenation%20volume%20refines%20the%0Acorrelation%20volume.%20Subsequently%2C%20both%20volumes%20are%20fused%20to%20enhance%20fine%0Adetails.%20Our%20method%20excels%20in%20asymmetric%20scenarios%20and%20shows%20robust%20performance%0Aagainst%20significant%20visual%20asymmetry.%20Extensive%20comparative%20experiments%20on%0Abenchmark%20datasets%2C%20along%20with%20ablation%20studies%2C%20confirm%20the%20effectiveness%20of%0Aour%20approach%20in%20asymmetric%20stereo%20with%20resolution%20and%20color%20degradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09543v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Volume%2520Fusion%2520for%2520Asymmetric%2520Stereo%2520Matching%26entry.906535625%3DYuanting%2520Gao%2520and%2520Linghao%2520Shen%26entry.1292438233%3D%2520%2520Stereo%2520matching%2520is%2520vital%2520in%25203D%2520computer%2520vision%252C%2520with%2520most%2520algorithms%2520assuming%250Asymmetric%2520visual%2520properties%2520between%2520binocular%2520visions.%2520However%252C%2520the%2520rise%2520of%250Aasymmetric%2520multi-camera%2520systems%2520%2528e.g.%252C%2520tele-wide%2520cameras%2529%2520challenges%2520this%250Aassumption%2520and%2520complicates%2520stereo%2520matching.%2520Visual%2520asymmetry%2520disrupts%2520stereo%250Amatching%2520by%2520affecting%2520the%2520crucial%2520cost%2520volume%2520computation.%2520To%2520address%2520this%252C%2520we%250Aexplore%2520the%2520matching%2520cost%2520distribution%2520of%2520two%2520established%2520cost%2520volume%250Aconstruction%2520methods%2520in%2520asymmetric%2520stereo.%2520We%2520find%2520that%2520each%2520cost%2520volume%250Aexperiences%2520distinct%2520information%2520distortion%252C%2520indicating%2520that%2520both%2520should%2520be%250Acomprehensively%2520utilized%2520to%2520solve%2520the%2520issue.%2520Based%2520on%2520this%252C%2520we%2520propose%2520the%250Atwo-phase%2520Iterative%2520Volume%2520Fusion%2520network%2520for%2520Asymmetric%2520Stereo%2520matching%250A%2528IVF-AStereo%2529.%2520Initially%252C%2520the%2520aggregated%2520concatenation%2520volume%2520refines%2520the%250Acorrelation%2520volume.%2520Subsequently%252C%2520both%2520volumes%2520are%2520fused%2520to%2520enhance%2520fine%250Adetails.%2520Our%2520method%2520excels%2520in%2520asymmetric%2520scenarios%2520and%2520shows%2520robust%2520performance%250Aagainst%2520significant%2520visual%2520asymmetry.%2520Extensive%2520comparative%2520experiments%2520on%250Abenchmark%2520datasets%252C%2520along%2520with%2520ablation%2520studies%252C%2520confirm%2520the%2520effectiveness%2520of%250Aour%2520approach%2520in%2520asymmetric%2520stereo%2520with%2520resolution%2520and%2520color%2520degradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09543v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Volume%20Fusion%20for%20Asymmetric%20Stereo%20Matching&entry.906535625=Yuanting%20Gao%20and%20Linghao%20Shen&entry.1292438233=%20%20Stereo%20matching%20is%20vital%20in%203D%20computer%20vision%2C%20with%20most%20algorithms%20assuming%0Asymmetric%20visual%20properties%20between%20binocular%20visions.%20However%2C%20the%20rise%20of%0Aasymmetric%20multi-camera%20systems%20%28e.g.%2C%20tele-wide%20cameras%29%20challenges%20this%0Aassumption%20and%20complicates%20stereo%20matching.%20Visual%20asymmetry%20disrupts%20stereo%0Amatching%20by%20affecting%20the%20crucial%20cost%20volume%20computation.%20To%20address%20this%2C%20we%0Aexplore%20the%20matching%20cost%20distribution%20of%20two%20established%20cost%20volume%0Aconstruction%20methods%20in%20asymmetric%20stereo.%20We%20find%20that%20each%20cost%20volume%0Aexperiences%20distinct%20information%20distortion%2C%20indicating%20that%20both%20should%20be%0Acomprehensively%20utilized%20to%20solve%20the%20issue.%20Based%20on%20this%2C%20we%20propose%20the%0Atwo-phase%20Iterative%20Volume%20Fusion%20network%20for%20Asymmetric%20Stereo%20matching%0A%28IVF-AStereo%29.%20Initially%2C%20the%20aggregated%20concatenation%20volume%20refines%20the%0Acorrelation%20volume.%20Subsequently%2C%20both%20volumes%20are%20fused%20to%20enhance%20fine%0Adetails.%20Our%20method%20excels%20in%20asymmetric%20scenarios%20and%20shows%20robust%20performance%0Aagainst%20significant%20visual%20asymmetry.%20Extensive%20comparative%20experiments%20on%0Abenchmark%20datasets%2C%20along%20with%20ablation%20studies%2C%20confirm%20the%20effectiveness%20of%0Aour%20approach%20in%20asymmetric%20stereo%20with%20resolution%20and%20color%20degradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09543v2&entry.124074799=Read"},
{"title": "On Understanding of the Dynamics of Model Capacity in Continual Learning", "author": "Supriyo Chakraborty and Krishnan Raghavan", "abstract": "  The stability-plasticity dilemma, closely related to a neural network's (NN)\ncapacity-its ability to represent tasks-is a fundamental challenge in continual\nlearning (CL). Within this context, we introduce CL's effective model capacity\n(CLEMC) that characterizes the dynamic behavior of the stability-plasticity\nbalance point. We develop a difference equation to model the evolution of the\ninterplay between the NN, task data, and optimization procedure. We then\nleverage CLEMC to demonstrate that the effective capacity-and, by extension,\nthe stability-plasticity balance point is inherently non-stationary. We show\nthat regardless of the NN architecture or optimization method, a NN's ability\nto represent new tasks diminishes when incoming task distributions differ from\nprevious ones. We conduct extensive experiments to support our theoretical\nfindings, spanning a range of architectures-from small feedforward network and\nconvolutional networks to medium-sized graph neural networks and\ntransformer-based large language models with millions of parameters.\n", "link": "http://arxiv.org/abs/2508.08052v2", "date": "2025-08-14", "relevancy": 2.4449, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5166}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4752}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Understanding%20of%20the%20Dynamics%20of%20Model%20Capacity%20in%20Continual%20Learning&body=Title%3A%20On%20Understanding%20of%20the%20Dynamics%20of%20Model%20Capacity%20in%20Continual%20Learning%0AAuthor%3A%20Supriyo%20Chakraborty%20and%20Krishnan%20Raghavan%0AAbstract%3A%20%20%20The%20stability-plasticity%20dilemma%2C%20closely%20related%20to%20a%20neural%20network%27s%20%28NN%29%0Acapacity-its%20ability%20to%20represent%20tasks-is%20a%20fundamental%20challenge%20in%20continual%0Alearning%20%28CL%29.%20Within%20this%20context%2C%20we%20introduce%20CL%27s%20effective%20model%20capacity%0A%28CLEMC%29%20that%20characterizes%20the%20dynamic%20behavior%20of%20the%20stability-plasticity%0Abalance%20point.%20We%20develop%20a%20difference%20equation%20to%20model%20the%20evolution%20of%20the%0Ainterplay%20between%20the%20NN%2C%20task%20data%2C%20and%20optimization%20procedure.%20We%20then%0Aleverage%20CLEMC%20to%20demonstrate%20that%20the%20effective%20capacity-and%2C%20by%20extension%2C%0Athe%20stability-plasticity%20balance%20point%20is%20inherently%20non-stationary.%20We%20show%0Athat%20regardless%20of%20the%20NN%20architecture%20or%20optimization%20method%2C%20a%20NN%27s%20ability%0Ato%20represent%20new%20tasks%20diminishes%20when%20incoming%20task%20distributions%20differ%20from%0Aprevious%20ones.%20We%20conduct%20extensive%20experiments%20to%20support%20our%20theoretical%0Afindings%2C%20spanning%20a%20range%20of%20architectures-from%20small%20feedforward%20network%20and%0Aconvolutional%20networks%20to%20medium-sized%20graph%20neural%20networks%20and%0Atransformer-based%20large%20language%20models%20with%20millions%20of%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08052v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Understanding%2520of%2520the%2520Dynamics%2520of%2520Model%2520Capacity%2520in%2520Continual%2520Learning%26entry.906535625%3DSupriyo%2520Chakraborty%2520and%2520Krishnan%2520Raghavan%26entry.1292438233%3D%2520%2520The%2520stability-plasticity%2520dilemma%252C%2520closely%2520related%2520to%2520a%2520neural%2520network%2527s%2520%2528NN%2529%250Acapacity-its%2520ability%2520to%2520represent%2520tasks-is%2520a%2520fundamental%2520challenge%2520in%2520continual%250Alearning%2520%2528CL%2529.%2520Within%2520this%2520context%252C%2520we%2520introduce%2520CL%2527s%2520effective%2520model%2520capacity%250A%2528CLEMC%2529%2520that%2520characterizes%2520the%2520dynamic%2520behavior%2520of%2520the%2520stability-plasticity%250Abalance%2520point.%2520We%2520develop%2520a%2520difference%2520equation%2520to%2520model%2520the%2520evolution%2520of%2520the%250Ainterplay%2520between%2520the%2520NN%252C%2520task%2520data%252C%2520and%2520optimization%2520procedure.%2520We%2520then%250Aleverage%2520CLEMC%2520to%2520demonstrate%2520that%2520the%2520effective%2520capacity-and%252C%2520by%2520extension%252C%250Athe%2520stability-plasticity%2520balance%2520point%2520is%2520inherently%2520non-stationary.%2520We%2520show%250Athat%2520regardless%2520of%2520the%2520NN%2520architecture%2520or%2520optimization%2520method%252C%2520a%2520NN%2527s%2520ability%250Ato%2520represent%2520new%2520tasks%2520diminishes%2520when%2520incoming%2520task%2520distributions%2520differ%2520from%250Aprevious%2520ones.%2520We%2520conduct%2520extensive%2520experiments%2520to%2520support%2520our%2520theoretical%250Afindings%252C%2520spanning%2520a%2520range%2520of%2520architectures-from%2520small%2520feedforward%2520network%2520and%250Aconvolutional%2520networks%2520to%2520medium-sized%2520graph%2520neural%2520networks%2520and%250Atransformer-based%2520large%2520language%2520models%2520with%2520millions%2520of%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08052v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Understanding%20of%20the%20Dynamics%20of%20Model%20Capacity%20in%20Continual%20Learning&entry.906535625=Supriyo%20Chakraborty%20and%20Krishnan%20Raghavan&entry.1292438233=%20%20The%20stability-plasticity%20dilemma%2C%20closely%20related%20to%20a%20neural%20network%27s%20%28NN%29%0Acapacity-its%20ability%20to%20represent%20tasks-is%20a%20fundamental%20challenge%20in%20continual%0Alearning%20%28CL%29.%20Within%20this%20context%2C%20we%20introduce%20CL%27s%20effective%20model%20capacity%0A%28CLEMC%29%20that%20characterizes%20the%20dynamic%20behavior%20of%20the%20stability-plasticity%0Abalance%20point.%20We%20develop%20a%20difference%20equation%20to%20model%20the%20evolution%20of%20the%0Ainterplay%20between%20the%20NN%2C%20task%20data%2C%20and%20optimization%20procedure.%20We%20then%0Aleverage%20CLEMC%20to%20demonstrate%20that%20the%20effective%20capacity-and%2C%20by%20extension%2C%0Athe%20stability-plasticity%20balance%20point%20is%20inherently%20non-stationary.%20We%20show%0Athat%20regardless%20of%20the%20NN%20architecture%20or%20optimization%20method%2C%20a%20NN%27s%20ability%0Ato%20represent%20new%20tasks%20diminishes%20when%20incoming%20task%20distributions%20differ%20from%0Aprevious%20ones.%20We%20conduct%20extensive%20experiments%20to%20support%20our%20theoretical%0Afindings%2C%20spanning%20a%20range%20of%20architectures-from%20small%20feedforward%20network%20and%0Aconvolutional%20networks%20to%20medium-sized%20graph%20neural%20networks%20and%0Atransformer-based%20large%20language%20models%20with%20millions%20of%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08052v2&entry.124074799=Read"},
{"title": "FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly\n  Detection", "author": "Yunfeng Zhao and Yixin Liu and Shiyuan Li and Qingfeng Chen and Yu Zheng and Shirui Pan", "abstract": "  Graph Anomaly Detection (GAD) aims to identify nodes that deviate from the\nmajority within a graph, playing a crucial role in applications such as social\nnetworks and e-commerce. Despite the current advancements in deep\nlearning-based GAD, existing approaches often suffer from high deployment costs\nand poor scalability due to their complex and resource-intensive training\nprocesses. Surprisingly, our empirical findings suggest that the training phase\nof deep GAD methods, commonly perceived as crucial, may actually contribute\nless to anomaly detection performance than expected. Inspired by this, we\npropose FreeGAD, a novel training-free yet effective GAD method. Specifically,\nit leverages an affinity-gated residual encoder to generate anomaly-aware\nrepresentations. Meanwhile, FreeGAD identifies anchor nodes as pseudo-normal\nand anomalous guides, followed by calculating anomaly scores through\nanchor-guided statistical deviations. Extensive experiments demonstrate that\nFreeGAD achieves superior anomaly detection performance, efficiency, and\nscalability on multiple benchmark datasets from diverse domains, without any\ntraining or iterative optimization.\n", "link": "http://arxiv.org/abs/2508.10594v1", "date": "2025-08-14", "relevancy": 2.4312, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4902}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4899}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeGAD%3A%20A%20Training-Free%20yet%20Effective%20Approach%20for%20Graph%20Anomaly%0A%20%20Detection&body=Title%3A%20FreeGAD%3A%20A%20Training-Free%20yet%20Effective%20Approach%20for%20Graph%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Yunfeng%20Zhao%20and%20Yixin%20Liu%20and%20Shiyuan%20Li%20and%20Qingfeng%20Chen%20and%20Yu%20Zheng%20and%20Shirui%20Pan%0AAbstract%3A%20%20%20Graph%20Anomaly%20Detection%20%28GAD%29%20aims%20to%20identify%20nodes%20that%20deviate%20from%20the%0Amajority%20within%20a%20graph%2C%20playing%20a%20crucial%20role%20in%20applications%20such%20as%20social%0Anetworks%20and%20e-commerce.%20Despite%20the%20current%20advancements%20in%20deep%0Alearning-based%20GAD%2C%20existing%20approaches%20often%20suffer%20from%20high%20deployment%20costs%0Aand%20poor%20scalability%20due%20to%20their%20complex%20and%20resource-intensive%20training%0Aprocesses.%20Surprisingly%2C%20our%20empirical%20findings%20suggest%20that%20the%20training%20phase%0Aof%20deep%20GAD%20methods%2C%20commonly%20perceived%20as%20crucial%2C%20may%20actually%20contribute%0Aless%20to%20anomaly%20detection%20performance%20than%20expected.%20Inspired%20by%20this%2C%20we%0Apropose%20FreeGAD%2C%20a%20novel%20training-free%20yet%20effective%20GAD%20method.%20Specifically%2C%0Ait%20leverages%20an%20affinity-gated%20residual%20encoder%20to%20generate%20anomaly-aware%0Arepresentations.%20Meanwhile%2C%20FreeGAD%20identifies%20anchor%20nodes%20as%20pseudo-normal%0Aand%20anomalous%20guides%2C%20followed%20by%20calculating%20anomaly%20scores%20through%0Aanchor-guided%20statistical%20deviations.%20Extensive%20experiments%20demonstrate%20that%0AFreeGAD%20achieves%20superior%20anomaly%20detection%20performance%2C%20efficiency%2C%20and%0Ascalability%20on%20multiple%20benchmark%20datasets%20from%20diverse%20domains%2C%20without%20any%0Atraining%20or%20iterative%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeGAD%253A%2520A%2520Training-Free%2520yet%2520Effective%2520Approach%2520for%2520Graph%2520Anomaly%250A%2520%2520Detection%26entry.906535625%3DYunfeng%2520Zhao%2520and%2520Yixin%2520Liu%2520and%2520Shiyuan%2520Li%2520and%2520Qingfeng%2520Chen%2520and%2520Yu%2520Zheng%2520and%2520Shirui%2520Pan%26entry.1292438233%3D%2520%2520Graph%2520Anomaly%2520Detection%2520%2528GAD%2529%2520aims%2520to%2520identify%2520nodes%2520that%2520deviate%2520from%2520the%250Amajority%2520within%2520a%2520graph%252C%2520playing%2520a%2520crucial%2520role%2520in%2520applications%2520such%2520as%2520social%250Anetworks%2520and%2520e-commerce.%2520Despite%2520the%2520current%2520advancements%2520in%2520deep%250Alearning-based%2520GAD%252C%2520existing%2520approaches%2520often%2520suffer%2520from%2520high%2520deployment%2520costs%250Aand%2520poor%2520scalability%2520due%2520to%2520their%2520complex%2520and%2520resource-intensive%2520training%250Aprocesses.%2520Surprisingly%252C%2520our%2520empirical%2520findings%2520suggest%2520that%2520the%2520training%2520phase%250Aof%2520deep%2520GAD%2520methods%252C%2520commonly%2520perceived%2520as%2520crucial%252C%2520may%2520actually%2520contribute%250Aless%2520to%2520anomaly%2520detection%2520performance%2520than%2520expected.%2520Inspired%2520by%2520this%252C%2520we%250Apropose%2520FreeGAD%252C%2520a%2520novel%2520training-free%2520yet%2520effective%2520GAD%2520method.%2520Specifically%252C%250Ait%2520leverages%2520an%2520affinity-gated%2520residual%2520encoder%2520to%2520generate%2520anomaly-aware%250Arepresentations.%2520Meanwhile%252C%2520FreeGAD%2520identifies%2520anchor%2520nodes%2520as%2520pseudo-normal%250Aand%2520anomalous%2520guides%252C%2520followed%2520by%2520calculating%2520anomaly%2520scores%2520through%250Aanchor-guided%2520statistical%2520deviations.%2520Extensive%2520experiments%2520demonstrate%2520that%250AFreeGAD%2520achieves%2520superior%2520anomaly%2520detection%2520performance%252C%2520efficiency%252C%2520and%250Ascalability%2520on%2520multiple%2520benchmark%2520datasets%2520from%2520diverse%2520domains%252C%2520without%2520any%250Atraining%2520or%2520iterative%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeGAD%3A%20A%20Training-Free%20yet%20Effective%20Approach%20for%20Graph%20Anomaly%0A%20%20Detection&entry.906535625=Yunfeng%20Zhao%20and%20Yixin%20Liu%20and%20Shiyuan%20Li%20and%20Qingfeng%20Chen%20and%20Yu%20Zheng%20and%20Shirui%20Pan&entry.1292438233=%20%20Graph%20Anomaly%20Detection%20%28GAD%29%20aims%20to%20identify%20nodes%20that%20deviate%20from%20the%0Amajority%20within%20a%20graph%2C%20playing%20a%20crucial%20role%20in%20applications%20such%20as%20social%0Anetworks%20and%20e-commerce.%20Despite%20the%20current%20advancements%20in%20deep%0Alearning-based%20GAD%2C%20existing%20approaches%20often%20suffer%20from%20high%20deployment%20costs%0Aand%20poor%20scalability%20due%20to%20their%20complex%20and%20resource-intensive%20training%0Aprocesses.%20Surprisingly%2C%20our%20empirical%20findings%20suggest%20that%20the%20training%20phase%0Aof%20deep%20GAD%20methods%2C%20commonly%20perceived%20as%20crucial%2C%20may%20actually%20contribute%0Aless%20to%20anomaly%20detection%20performance%20than%20expected.%20Inspired%20by%20this%2C%20we%0Apropose%20FreeGAD%2C%20a%20novel%20training-free%20yet%20effective%20GAD%20method.%20Specifically%2C%0Ait%20leverages%20an%20affinity-gated%20residual%20encoder%20to%20generate%20anomaly-aware%0Arepresentations.%20Meanwhile%2C%20FreeGAD%20identifies%20anchor%20nodes%20as%20pseudo-normal%0Aand%20anomalous%20guides%2C%20followed%20by%20calculating%20anomaly%20scores%20through%0Aanchor-guided%20statistical%20deviations.%20Extensive%20experiments%20demonstrate%20that%0AFreeGAD%20achieves%20superior%20anomaly%20detection%20performance%2C%20efficiency%2C%20and%0Ascalability%20on%20multiple%20benchmark%20datasets%20from%20diverse%20domains%2C%20without%20any%0Atraining%20or%20iterative%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10594v1&entry.124074799=Read"},
{"title": "Axis-level Symmetry Detection with Group-Equivariant Representation", "author": "Wongyun Yu and Ahyun Seo and Minsu Cho", "abstract": "  Symmetry is a fundamental concept that has been extensively studied, yet\ndetecting it in complex scenes remains a significant challenge in computer\nvision. Recent heatmap-based approaches can localize potential regions of\nsymmetry axes but often lack precision in identifying individual axes. In this\nwork, we propose a novel framework for axis-level detection of the two most\ncommon symmetry types-reflection and rotation-by representing them as explicit\ngeometric primitives, i.e. lines and points. Our method employs a dual-branch\narchitecture that is equivariant to the dihedral group, with each branch\nspecialized to exploit the structure of dihedral group-equivariant features for\nits respective symmetry type. For reflection symmetry, we introduce\norientational anchors, aligned with group components, to enable\norientation-specific detection, and a reflectional matching that measures\nsimilarity between patterns and their mirrored counterparts across candidate\naxes. For rotational symmetry, we propose a rotational matching that compares\npatterns at fixed angular intervals to identify rotational centers. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance,\noutperforming existing approaches.\n", "link": "http://arxiv.org/abs/2508.10740v1", "date": "2025-08-14", "relevancy": 2.4143, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5121}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4769}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Axis-level%20Symmetry%20Detection%20with%20Group-Equivariant%20Representation&body=Title%3A%20Axis-level%20Symmetry%20Detection%20with%20Group-Equivariant%20Representation%0AAuthor%3A%20Wongyun%20Yu%20and%20Ahyun%20Seo%20and%20Minsu%20Cho%0AAbstract%3A%20%20%20Symmetry%20is%20a%20fundamental%20concept%20that%20has%20been%20extensively%20studied%2C%20yet%0Adetecting%20it%20in%20complex%20scenes%20remains%20a%20significant%20challenge%20in%20computer%0Avision.%20Recent%20heatmap-based%20approaches%20can%20localize%20potential%20regions%20of%0Asymmetry%20axes%20but%20often%20lack%20precision%20in%20identifying%20individual%20axes.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20framework%20for%20axis-level%20detection%20of%20the%20two%20most%0Acommon%20symmetry%20types-reflection%20and%20rotation-by%20representing%20them%20as%20explicit%0Ageometric%20primitives%2C%20i.e.%20lines%20and%20points.%20Our%20method%20employs%20a%20dual-branch%0Aarchitecture%20that%20is%20equivariant%20to%20the%20dihedral%20group%2C%20with%20each%20branch%0Aspecialized%20to%20exploit%20the%20structure%20of%20dihedral%20group-equivariant%20features%20for%0Aits%20respective%20symmetry%20type.%20For%20reflection%20symmetry%2C%20we%20introduce%0Aorientational%20anchors%2C%20aligned%20with%20group%20components%2C%20to%20enable%0Aorientation-specific%20detection%2C%20and%20a%20reflectional%20matching%20that%20measures%0Asimilarity%20between%20patterns%20and%20their%20mirrored%20counterparts%20across%20candidate%0Aaxes.%20For%20rotational%20symmetry%2C%20we%20propose%20a%20rotational%20matching%20that%20compares%0Apatterns%20at%20fixed%20angular%20intervals%20to%20identify%20rotational%20centers.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%2C%0Aoutperforming%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAxis-level%2520Symmetry%2520Detection%2520with%2520Group-Equivariant%2520Representation%26entry.906535625%3DWongyun%2520Yu%2520and%2520Ahyun%2520Seo%2520and%2520Minsu%2520Cho%26entry.1292438233%3D%2520%2520Symmetry%2520is%2520a%2520fundamental%2520concept%2520that%2520has%2520been%2520extensively%2520studied%252C%2520yet%250Adetecting%2520it%2520in%2520complex%2520scenes%2520remains%2520a%2520significant%2520challenge%2520in%2520computer%250Avision.%2520Recent%2520heatmap-based%2520approaches%2520can%2520localize%2520potential%2520regions%2520of%250Asymmetry%2520axes%2520but%2520often%2520lack%2520precision%2520in%2520identifying%2520individual%2520axes.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520novel%2520framework%2520for%2520axis-level%2520detection%2520of%2520the%2520two%2520most%250Acommon%2520symmetry%2520types-reflection%2520and%2520rotation-by%2520representing%2520them%2520as%2520explicit%250Ageometric%2520primitives%252C%2520i.e.%2520lines%2520and%2520points.%2520Our%2520method%2520employs%2520a%2520dual-branch%250Aarchitecture%2520that%2520is%2520equivariant%2520to%2520the%2520dihedral%2520group%252C%2520with%2520each%2520branch%250Aspecialized%2520to%2520exploit%2520the%2520structure%2520of%2520dihedral%2520group-equivariant%2520features%2520for%250Aits%2520respective%2520symmetry%2520type.%2520For%2520reflection%2520symmetry%252C%2520we%2520introduce%250Aorientational%2520anchors%252C%2520aligned%2520with%2520group%2520components%252C%2520to%2520enable%250Aorientation-specific%2520detection%252C%2520and%2520a%2520reflectional%2520matching%2520that%2520measures%250Asimilarity%2520between%2520patterns%2520and%2520their%2520mirrored%2520counterparts%2520across%2520candidate%250Aaxes.%2520For%2520rotational%2520symmetry%252C%2520we%2520propose%2520a%2520rotational%2520matching%2520that%2520compares%250Apatterns%2520at%2520fixed%2520angular%2520intervals%2520to%2520identify%2520rotational%2520centers.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%252C%250Aoutperforming%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Axis-level%20Symmetry%20Detection%20with%20Group-Equivariant%20Representation&entry.906535625=Wongyun%20Yu%20and%20Ahyun%20Seo%20and%20Minsu%20Cho&entry.1292438233=%20%20Symmetry%20is%20a%20fundamental%20concept%20that%20has%20been%20extensively%20studied%2C%20yet%0Adetecting%20it%20in%20complex%20scenes%20remains%20a%20significant%20challenge%20in%20computer%0Avision.%20Recent%20heatmap-based%20approaches%20can%20localize%20potential%20regions%20of%0Asymmetry%20axes%20but%20often%20lack%20precision%20in%20identifying%20individual%20axes.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20framework%20for%20axis-level%20detection%20of%20the%20two%20most%0Acommon%20symmetry%20types-reflection%20and%20rotation-by%20representing%20them%20as%20explicit%0Ageometric%20primitives%2C%20i.e.%20lines%20and%20points.%20Our%20method%20employs%20a%20dual-branch%0Aarchitecture%20that%20is%20equivariant%20to%20the%20dihedral%20group%2C%20with%20each%20branch%0Aspecialized%20to%20exploit%20the%20structure%20of%20dihedral%20group-equivariant%20features%20for%0Aits%20respective%20symmetry%20type.%20For%20reflection%20symmetry%2C%20we%20introduce%0Aorientational%20anchors%2C%20aligned%20with%20group%20components%2C%20to%20enable%0Aorientation-specific%20detection%2C%20and%20a%20reflectional%20matching%20that%20measures%0Asimilarity%20between%20patterns%20and%20their%20mirrored%20counterparts%20across%20candidate%0Aaxes.%20For%20rotational%20symmetry%2C%20we%20propose%20a%20rotational%20matching%20that%20compares%0Apatterns%20at%20fixed%20angular%20intervals%20to%20identify%20rotational%20centers.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%2C%0Aoutperforming%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10740v1&entry.124074799=Read"},
{"title": "Towards Agentic AI for Multimodal-Guided Video Object Segmentation", "author": "Tuyen Tran and Thao Minh Le and Truyen Tran", "abstract": "  Referring-based Video Object Segmentation is a multimodal problem that\nrequires producing fine-grained segmentation results guided by external cues.\nTraditional approaches to this task typically involve training specialized\nmodels, which come with high computational complexity and manual annotation\neffort. Recent advances in vision-language foundation models open a promising\ndirection toward training-free approaches. Several studies have explored\nleveraging these general-purpose models for fine-grained segmentation,\nachieving performance comparable to that of fully supervised, task-specific\nmodels. However, existing methods rely on fixed pipelines that lack the\nflexibility needed to adapt to the dynamic nature of the task. To address this\nlimitation, we propose Multi-Modal Agent, a novel agentic system designed to\nsolve this task in a more flexible and adaptive manner. Specifically, our\nmethod leverages the reasoning capabilities of large language models (LLMs) to\ngenerate dynamic workflows tailored to each input. This adaptive procedure\niteratively interacts with a set of specialized tools designed for low-level\ntasks across different modalities to identify the target object described by\nthe multimodal cues. Our agentic approach demonstrates clear improvements over\nprior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS.\n", "link": "http://arxiv.org/abs/2508.10572v1", "date": "2025-08-14", "relevancy": 2.391, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6119}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Agentic%20AI%20for%20Multimodal-Guided%20Video%20Object%20Segmentation&body=Title%3A%20Towards%20Agentic%20AI%20for%20Multimodal-Guided%20Video%20Object%20Segmentation%0AAuthor%3A%20Tuyen%20Tran%20and%20Thao%20Minh%20Le%20and%20Truyen%20Tran%0AAbstract%3A%20%20%20Referring-based%20Video%20Object%20Segmentation%20is%20a%20multimodal%20problem%20that%0Arequires%20producing%20fine-grained%20segmentation%20results%20guided%20by%20external%20cues.%0ATraditional%20approaches%20to%20this%20task%20typically%20involve%20training%20specialized%0Amodels%2C%20which%20come%20with%20high%20computational%20complexity%20and%20manual%20annotation%0Aeffort.%20Recent%20advances%20in%20vision-language%20foundation%20models%20open%20a%20promising%0Adirection%20toward%20training-free%20approaches.%20Several%20studies%20have%20explored%0Aleveraging%20these%20general-purpose%20models%20for%20fine-grained%20segmentation%2C%0Aachieving%20performance%20comparable%20to%20that%20of%20fully%20supervised%2C%20task-specific%0Amodels.%20However%2C%20existing%20methods%20rely%20on%20fixed%20pipelines%20that%20lack%20the%0Aflexibility%20needed%20to%20adapt%20to%20the%20dynamic%20nature%20of%20the%20task.%20To%20address%20this%0Alimitation%2C%20we%20propose%20Multi-Modal%20Agent%2C%20a%20novel%20agentic%20system%20designed%20to%0Asolve%20this%20task%20in%20a%20more%20flexible%20and%20adaptive%20manner.%20Specifically%2C%20our%0Amethod%20leverages%20the%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20to%0Agenerate%20dynamic%20workflows%20tailored%20to%20each%20input.%20This%20adaptive%20procedure%0Aiteratively%20interacts%20with%20a%20set%20of%20specialized%20tools%20designed%20for%20low-level%0Atasks%20across%20different%20modalities%20to%20identify%20the%20target%20object%20described%20by%0Athe%20multimodal%20cues.%20Our%20agentic%20approach%20demonstrates%20clear%20improvements%20over%0Aprior%20methods%20on%20two%20multimodal-conditioned%20VOS%20tasks%3A%20RVOS%20and%20Ref-AVS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10572v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Agentic%2520AI%2520for%2520Multimodal-Guided%2520Video%2520Object%2520Segmentation%26entry.906535625%3DTuyen%2520Tran%2520and%2520Thao%2520Minh%2520Le%2520and%2520Truyen%2520Tran%26entry.1292438233%3D%2520%2520Referring-based%2520Video%2520Object%2520Segmentation%2520is%2520a%2520multimodal%2520problem%2520that%250Arequires%2520producing%2520fine-grained%2520segmentation%2520results%2520guided%2520by%2520external%2520cues.%250ATraditional%2520approaches%2520to%2520this%2520task%2520typically%2520involve%2520training%2520specialized%250Amodels%252C%2520which%2520come%2520with%2520high%2520computational%2520complexity%2520and%2520manual%2520annotation%250Aeffort.%2520Recent%2520advances%2520in%2520vision-language%2520foundation%2520models%2520open%2520a%2520promising%250Adirection%2520toward%2520training-free%2520approaches.%2520Several%2520studies%2520have%2520explored%250Aleveraging%2520these%2520general-purpose%2520models%2520for%2520fine-grained%2520segmentation%252C%250Aachieving%2520performance%2520comparable%2520to%2520that%2520of%2520fully%2520supervised%252C%2520task-specific%250Amodels.%2520However%252C%2520existing%2520methods%2520rely%2520on%2520fixed%2520pipelines%2520that%2520lack%2520the%250Aflexibility%2520needed%2520to%2520adapt%2520to%2520the%2520dynamic%2520nature%2520of%2520the%2520task.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520Multi-Modal%2520Agent%252C%2520a%2520novel%2520agentic%2520system%2520designed%2520to%250Asolve%2520this%2520task%2520in%2520a%2520more%2520flexible%2520and%2520adaptive%2520manner.%2520Specifically%252C%2520our%250Amethod%2520leverages%2520the%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%250Agenerate%2520dynamic%2520workflows%2520tailored%2520to%2520each%2520input.%2520This%2520adaptive%2520procedure%250Aiteratively%2520interacts%2520with%2520a%2520set%2520of%2520specialized%2520tools%2520designed%2520for%2520low-level%250Atasks%2520across%2520different%2520modalities%2520to%2520identify%2520the%2520target%2520object%2520described%2520by%250Athe%2520multimodal%2520cues.%2520Our%2520agentic%2520approach%2520demonstrates%2520clear%2520improvements%2520over%250Aprior%2520methods%2520on%2520two%2520multimodal-conditioned%2520VOS%2520tasks%253A%2520RVOS%2520and%2520Ref-AVS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10572v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Agentic%20AI%20for%20Multimodal-Guided%20Video%20Object%20Segmentation&entry.906535625=Tuyen%20Tran%20and%20Thao%20Minh%20Le%20and%20Truyen%20Tran&entry.1292438233=%20%20Referring-based%20Video%20Object%20Segmentation%20is%20a%20multimodal%20problem%20that%0Arequires%20producing%20fine-grained%20segmentation%20results%20guided%20by%20external%20cues.%0ATraditional%20approaches%20to%20this%20task%20typically%20involve%20training%20specialized%0Amodels%2C%20which%20come%20with%20high%20computational%20complexity%20and%20manual%20annotation%0Aeffort.%20Recent%20advances%20in%20vision-language%20foundation%20models%20open%20a%20promising%0Adirection%20toward%20training-free%20approaches.%20Several%20studies%20have%20explored%0Aleveraging%20these%20general-purpose%20models%20for%20fine-grained%20segmentation%2C%0Aachieving%20performance%20comparable%20to%20that%20of%20fully%20supervised%2C%20task-specific%0Amodels.%20However%2C%20existing%20methods%20rely%20on%20fixed%20pipelines%20that%20lack%20the%0Aflexibility%20needed%20to%20adapt%20to%20the%20dynamic%20nature%20of%20the%20task.%20To%20address%20this%0Alimitation%2C%20we%20propose%20Multi-Modal%20Agent%2C%20a%20novel%20agentic%20system%20designed%20to%0Asolve%20this%20task%20in%20a%20more%20flexible%20and%20adaptive%20manner.%20Specifically%2C%20our%0Amethod%20leverages%20the%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20to%0Agenerate%20dynamic%20workflows%20tailored%20to%20each%20input.%20This%20adaptive%20procedure%0Aiteratively%20interacts%20with%20a%20set%20of%20specialized%20tools%20designed%20for%20low-level%0Atasks%20across%20different%20modalities%20to%20identify%20the%20target%20object%20described%20by%0Athe%20multimodal%20cues.%20Our%20agentic%20approach%20demonstrates%20clear%20improvements%20over%0Aprior%20methods%20on%20two%20multimodal-conditioned%20VOS%20tasks%3A%20RVOS%20and%20Ref-AVS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10572v1&entry.124074799=Read"},
{"title": "Improved GUI Grounding via Iterative Narrowing", "author": "Anthony Nguyen", "abstract": "  Graphical User Interface (GUI) grounding plays a crucial role in enhancing\nthe capabilities of Vision-Language Model (VLM) agents. While general VLMs,\nsuch as GPT-4V, demonstrate strong performance across various tasks, their\nproficiency in GUI grounding remains suboptimal. Recent studies have focused on\nfine-tuning these models specifically for zero-shot GUI grounding, yielding\nsignificant improvements over baseline performance. We introduce a visual\nprompting framework that employs an iterative narrowing mechanism to further\nimprove the performance of both general and fine-tuned models in GUI grounding.\nFor evaluation, we tested our method on a comprehensive benchmark comprising\nvarious UI platforms and provided the code to reproduce our results.\n", "link": "http://arxiv.org/abs/2411.13591v6", "date": "2025-08-14", "relevancy": 2.3864, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4832}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4743}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20GUI%20Grounding%20via%20Iterative%20Narrowing&body=Title%3A%20Improved%20GUI%20Grounding%20via%20Iterative%20Narrowing%0AAuthor%3A%20Anthony%20Nguyen%0AAbstract%3A%20%20%20Graphical%20User%20Interface%20%28GUI%29%20grounding%20plays%20a%20crucial%20role%20in%20enhancing%0Athe%20capabilities%20of%20Vision-Language%20Model%20%28VLM%29%20agents.%20While%20general%20VLMs%2C%0Asuch%20as%20GPT-4V%2C%20demonstrate%20strong%20performance%20across%20various%20tasks%2C%20their%0Aproficiency%20in%20GUI%20grounding%20remains%20suboptimal.%20Recent%20studies%20have%20focused%20on%0Afine-tuning%20these%20models%20specifically%20for%20zero-shot%20GUI%20grounding%2C%20yielding%0Asignificant%20improvements%20over%20baseline%20performance.%20We%20introduce%20a%20visual%0Aprompting%20framework%20that%20employs%20an%20iterative%20narrowing%20mechanism%20to%20further%0Aimprove%20the%20performance%20of%20both%20general%20and%20fine-tuned%20models%20in%20GUI%20grounding.%0AFor%20evaluation%2C%20we%20tested%20our%20method%20on%20a%20comprehensive%20benchmark%20comprising%0Avarious%20UI%20platforms%20and%20provided%20the%20code%20to%20reproduce%20our%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13591v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520GUI%2520Grounding%2520via%2520Iterative%2520Narrowing%26entry.906535625%3DAnthony%2520Nguyen%26entry.1292438233%3D%2520%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520grounding%2520plays%2520a%2520crucial%2520role%2520in%2520enhancing%250Athe%2520capabilities%2520of%2520Vision-Language%2520Model%2520%2528VLM%2529%2520agents.%2520While%2520general%2520VLMs%252C%250Asuch%2520as%2520GPT-4V%252C%2520demonstrate%2520strong%2520performance%2520across%2520various%2520tasks%252C%2520their%250Aproficiency%2520in%2520GUI%2520grounding%2520remains%2520suboptimal.%2520Recent%2520studies%2520have%2520focused%2520on%250Afine-tuning%2520these%2520models%2520specifically%2520for%2520zero-shot%2520GUI%2520grounding%252C%2520yielding%250Asignificant%2520improvements%2520over%2520baseline%2520performance.%2520We%2520introduce%2520a%2520visual%250Aprompting%2520framework%2520that%2520employs%2520an%2520iterative%2520narrowing%2520mechanism%2520to%2520further%250Aimprove%2520the%2520performance%2520of%2520both%2520general%2520and%2520fine-tuned%2520models%2520in%2520GUI%2520grounding.%250AFor%2520evaluation%252C%2520we%2520tested%2520our%2520method%2520on%2520a%2520comprehensive%2520benchmark%2520comprising%250Avarious%2520UI%2520platforms%2520and%2520provided%2520the%2520code%2520to%2520reproduce%2520our%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13591v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20GUI%20Grounding%20via%20Iterative%20Narrowing&entry.906535625=Anthony%20Nguyen&entry.1292438233=%20%20Graphical%20User%20Interface%20%28GUI%29%20grounding%20plays%20a%20crucial%20role%20in%20enhancing%0Athe%20capabilities%20of%20Vision-Language%20Model%20%28VLM%29%20agents.%20While%20general%20VLMs%2C%0Asuch%20as%20GPT-4V%2C%20demonstrate%20strong%20performance%20across%20various%20tasks%2C%20their%0Aproficiency%20in%20GUI%20grounding%20remains%20suboptimal.%20Recent%20studies%20have%20focused%20on%0Afine-tuning%20these%20models%20specifically%20for%20zero-shot%20GUI%20grounding%2C%20yielding%0Asignificant%20improvements%20over%20baseline%20performance.%20We%20introduce%20a%20visual%0Aprompting%20framework%20that%20employs%20an%20iterative%20narrowing%20mechanism%20to%20further%0Aimprove%20the%20performance%20of%20both%20general%20and%20fine-tuned%20models%20in%20GUI%20grounding.%0AFor%20evaluation%2C%20we%20tested%20our%20method%20on%20a%20comprehensive%20benchmark%20comprising%0Avarious%20UI%20platforms%20and%20provided%20the%20code%20to%20reproduce%20our%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13591v6&entry.124074799=Read"},
{"title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "author": "Guangda Liu and Chengwei Li and Zhenyu Ning and Minyi Guo and Jieru Zhao", "abstract": "  Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.\n", "link": "http://arxiv.org/abs/2505.13109v2", "date": "2025-08-14", "relevancy": 2.3787, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4731}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeKV%3A%20Boosting%20KV%20Cache%20Retrieval%20for%20Efficient%20LLM%20Inference&body=Title%3A%20FreeKV%3A%20Boosting%20KV%20Cache%20Retrieval%20for%20Efficient%20LLM%20Inference%0AAuthor%3A%20Guangda%20Liu%20and%20Chengwei%20Li%20and%20Zhenyu%20Ning%20and%20Minyi%20Guo%20and%20Jieru%20Zhao%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20widely%20deployed%20with%20rapidly%20expanding%0Acontext%20windows%20to%20support%20increasingly%20demanding%20applications.%20However%2C%20long%0Acontexts%20pose%20significant%20deployment%20challenges%2C%20primarily%20due%20to%20the%20KV%20cache%0Awhose%20size%20grows%20proportionally%20with%20context%20length.%20While%20KV%20cache%20compression%0Amethods%20are%20proposed%20to%20address%20this%20issue%2C%20KV%20dropping%20methods%20incur%0Aconsiderable%20accuracy%20loss%2C%20and%20KV%20retrieval%20methods%20suffer%20from%20significant%0Aefficiency%20bottlenecks.%20We%20propose%20FreeKV%2C%20an%20algorithm-system%20co-optimization%0Aframework%20to%20enhance%20KV%20retrieval%20efficiency%20while%20preserving%20accuracy.%20On%20the%0Aalgorithm%20side%2C%20FreeKV%20introduces%20speculative%20retrieval%20to%20shift%20the%20KV%0Aselection%20and%20recall%20processes%20out%20of%20the%20critical%20path%2C%20combined%20with%0Afine-grained%20correction%20to%20ensure%20accuracy.%20On%20the%20system%20side%2C%20FreeKV%20employs%0Ahybrid%20KV%20layouts%20across%20CPU%20and%20GPU%20memory%20to%20eliminate%20fragmented%20data%0Atransfers%2C%20and%20leverages%20double-buffered%20streamed%20recall%20to%20further%20improve%0Aefficiency.%20Experiments%20demonstrate%20that%20FreeKV%20achieves%20near-lossless%20accuracy%0Aacross%20various%20scenarios%20and%20models%2C%20delivering%20up%20to%2013%24%5Ctimes%24%20speedup%0Acompared%20to%20SOTA%20KV%20retrieval%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13109v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeKV%253A%2520Boosting%2520KV%2520Cache%2520Retrieval%2520for%2520Efficient%2520LLM%2520Inference%26entry.906535625%3DGuangda%2520Liu%2520and%2520Chengwei%2520Li%2520and%2520Zhenyu%2520Ning%2520and%2520Minyi%2520Guo%2520and%2520Jieru%2520Zhao%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520widely%2520deployed%2520with%2520rapidly%2520expanding%250Acontext%2520windows%2520to%2520support%2520increasingly%2520demanding%2520applications.%2520However%252C%2520long%250Acontexts%2520pose%2520significant%2520deployment%2520challenges%252C%2520primarily%2520due%2520to%2520the%2520KV%2520cache%250Awhose%2520size%2520grows%2520proportionally%2520with%2520context%2520length.%2520While%2520KV%2520cache%2520compression%250Amethods%2520are%2520proposed%2520to%2520address%2520this%2520issue%252C%2520KV%2520dropping%2520methods%2520incur%250Aconsiderable%2520accuracy%2520loss%252C%2520and%2520KV%2520retrieval%2520methods%2520suffer%2520from%2520significant%250Aefficiency%2520bottlenecks.%2520We%2520propose%2520FreeKV%252C%2520an%2520algorithm-system%2520co-optimization%250Aframework%2520to%2520enhance%2520KV%2520retrieval%2520efficiency%2520while%2520preserving%2520accuracy.%2520On%2520the%250Aalgorithm%2520side%252C%2520FreeKV%2520introduces%2520speculative%2520retrieval%2520to%2520shift%2520the%2520KV%250Aselection%2520and%2520recall%2520processes%2520out%2520of%2520the%2520critical%2520path%252C%2520combined%2520with%250Afine-grained%2520correction%2520to%2520ensure%2520accuracy.%2520On%2520the%2520system%2520side%252C%2520FreeKV%2520employs%250Ahybrid%2520KV%2520layouts%2520across%2520CPU%2520and%2520GPU%2520memory%2520to%2520eliminate%2520fragmented%2520data%250Atransfers%252C%2520and%2520leverages%2520double-buffered%2520streamed%2520recall%2520to%2520further%2520improve%250Aefficiency.%2520Experiments%2520demonstrate%2520that%2520FreeKV%2520achieves%2520near-lossless%2520accuracy%250Aacross%2520various%2520scenarios%2520and%2520models%252C%2520delivering%2520up%2520to%252013%2524%255Ctimes%2524%2520speedup%250Acompared%2520to%2520SOTA%2520KV%2520retrieval%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13109v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeKV%3A%20Boosting%20KV%20Cache%20Retrieval%20for%20Efficient%20LLM%20Inference&entry.906535625=Guangda%20Liu%20and%20Chengwei%20Li%20and%20Zhenyu%20Ning%20and%20Minyi%20Guo%20and%20Jieru%20Zhao&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20widely%20deployed%20with%20rapidly%20expanding%0Acontext%20windows%20to%20support%20increasingly%20demanding%20applications.%20However%2C%20long%0Acontexts%20pose%20significant%20deployment%20challenges%2C%20primarily%20due%20to%20the%20KV%20cache%0Awhose%20size%20grows%20proportionally%20with%20context%20length.%20While%20KV%20cache%20compression%0Amethods%20are%20proposed%20to%20address%20this%20issue%2C%20KV%20dropping%20methods%20incur%0Aconsiderable%20accuracy%20loss%2C%20and%20KV%20retrieval%20methods%20suffer%20from%20significant%0Aefficiency%20bottlenecks.%20We%20propose%20FreeKV%2C%20an%20algorithm-system%20co-optimization%0Aframework%20to%20enhance%20KV%20retrieval%20efficiency%20while%20preserving%20accuracy.%20On%20the%0Aalgorithm%20side%2C%20FreeKV%20introduces%20speculative%20retrieval%20to%20shift%20the%20KV%0Aselection%20and%20recall%20processes%20out%20of%20the%20critical%20path%2C%20combined%20with%0Afine-grained%20correction%20to%20ensure%20accuracy.%20On%20the%20system%20side%2C%20FreeKV%20employs%0Ahybrid%20KV%20layouts%20across%20CPU%20and%20GPU%20memory%20to%20eliminate%20fragmented%20data%0Atransfers%2C%20and%20leverages%20double-buffered%20streamed%20recall%20to%20further%20improve%0Aefficiency.%20Experiments%20demonstrate%20that%20FreeKV%20achieves%20near-lossless%20accuracy%0Aacross%20various%20scenarios%20and%20models%2C%20delivering%20up%20to%2013%24%5Ctimes%24%20speedup%0Acompared%20to%20SOTA%20KV%20retrieval%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13109v2&entry.124074799=Read"},
{"title": "Tuning-Free Online Robust Principal Component Analysis through Implicit\n  Regularization", "author": "Lakshmi Jayalal and Gokularam Muthukrishnan and Sheetal Kalyani", "abstract": "  The performance of the standard Online Robust Principal Component Analysis\n(OR-PCA) technique depends on the optimum tuning of the explicit regularizers\nand this tuning is dataset sensitive. We aim to remove the dependency on these\ntuning parameters by using implicit regularization. We propose to use the\nimplicit regularization effect of various modified gradient descents to make\nOR-PCA tuning free. Our method incorporates three different versions of\nmodified gradient descent that separately but naturally encourage sparsity and\nlow-rank structures in the data. The proposed method performs comparable or\nbetter than the tuned OR-PCA for both simulated and real-world datasets.\nTuning-free ORPCA makes it more scalable for large datasets since we do not\nrequire dataset-dependent parameter tuning.\n", "link": "http://arxiv.org/abs/2409.07275v2", "date": "2025-08-14", "relevancy": 2.3779, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.507}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4631}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tuning-Free%20Online%20Robust%20Principal%20Component%20Analysis%20through%20Implicit%0A%20%20Regularization&body=Title%3A%20Tuning-Free%20Online%20Robust%20Principal%20Component%20Analysis%20through%20Implicit%0A%20%20Regularization%0AAuthor%3A%20Lakshmi%20Jayalal%20and%20Gokularam%20Muthukrishnan%20and%20Sheetal%20Kalyani%0AAbstract%3A%20%20%20The%20performance%20of%20the%20standard%20Online%20Robust%20Principal%20Component%20Analysis%0A%28OR-PCA%29%20technique%20depends%20on%20the%20optimum%20tuning%20of%20the%20explicit%20regularizers%0Aand%20this%20tuning%20is%20dataset%20sensitive.%20We%20aim%20to%20remove%20the%20dependency%20on%20these%0Atuning%20parameters%20by%20using%20implicit%20regularization.%20We%20propose%20to%20use%20the%0Aimplicit%20regularization%20effect%20of%20various%20modified%20gradient%20descents%20to%20make%0AOR-PCA%20tuning%20free.%20Our%20method%20incorporates%20three%20different%20versions%20of%0Amodified%20gradient%20descent%20that%20separately%20but%20naturally%20encourage%20sparsity%20and%0Alow-rank%20structures%20in%20the%20data.%20The%20proposed%20method%20performs%20comparable%20or%0Abetter%20than%20the%20tuned%20OR-PCA%20for%20both%20simulated%20and%20real-world%20datasets.%0ATuning-free%20ORPCA%20makes%20it%20more%20scalable%20for%20large%20datasets%20since%20we%20do%20not%0Arequire%20dataset-dependent%20parameter%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07275v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTuning-Free%2520Online%2520Robust%2520Principal%2520Component%2520Analysis%2520through%2520Implicit%250A%2520%2520Regularization%26entry.906535625%3DLakshmi%2520Jayalal%2520and%2520Gokularam%2520Muthukrishnan%2520and%2520Sheetal%2520Kalyani%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520the%2520standard%2520Online%2520Robust%2520Principal%2520Component%2520Analysis%250A%2528OR-PCA%2529%2520technique%2520depends%2520on%2520the%2520optimum%2520tuning%2520of%2520the%2520explicit%2520regularizers%250Aand%2520this%2520tuning%2520is%2520dataset%2520sensitive.%2520We%2520aim%2520to%2520remove%2520the%2520dependency%2520on%2520these%250Atuning%2520parameters%2520by%2520using%2520implicit%2520regularization.%2520We%2520propose%2520to%2520use%2520the%250Aimplicit%2520regularization%2520effect%2520of%2520various%2520modified%2520gradient%2520descents%2520to%2520make%250AOR-PCA%2520tuning%2520free.%2520Our%2520method%2520incorporates%2520three%2520different%2520versions%2520of%250Amodified%2520gradient%2520descent%2520that%2520separately%2520but%2520naturally%2520encourage%2520sparsity%2520and%250Alow-rank%2520structures%2520in%2520the%2520data.%2520The%2520proposed%2520method%2520performs%2520comparable%2520or%250Abetter%2520than%2520the%2520tuned%2520OR-PCA%2520for%2520both%2520simulated%2520and%2520real-world%2520datasets.%250ATuning-free%2520ORPCA%2520makes%2520it%2520more%2520scalable%2520for%2520large%2520datasets%2520since%2520we%2520do%2520not%250Arequire%2520dataset-dependent%2520parameter%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07275v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tuning-Free%20Online%20Robust%20Principal%20Component%20Analysis%20through%20Implicit%0A%20%20Regularization&entry.906535625=Lakshmi%20Jayalal%20and%20Gokularam%20Muthukrishnan%20and%20Sheetal%20Kalyani&entry.1292438233=%20%20The%20performance%20of%20the%20standard%20Online%20Robust%20Principal%20Component%20Analysis%0A%28OR-PCA%29%20technique%20depends%20on%20the%20optimum%20tuning%20of%20the%20explicit%20regularizers%0Aand%20this%20tuning%20is%20dataset%20sensitive.%20We%20aim%20to%20remove%20the%20dependency%20on%20these%0Atuning%20parameters%20by%20using%20implicit%20regularization.%20We%20propose%20to%20use%20the%0Aimplicit%20regularization%20effect%20of%20various%20modified%20gradient%20descents%20to%20make%0AOR-PCA%20tuning%20free.%20Our%20method%20incorporates%20three%20different%20versions%20of%0Amodified%20gradient%20descent%20that%20separately%20but%20naturally%20encourage%20sparsity%20and%0Alow-rank%20structures%20in%20the%20data.%20The%20proposed%20method%20performs%20comparable%20or%0Abetter%20than%20the%20tuned%20OR-PCA%20for%20both%20simulated%20and%20real-world%20datasets.%0ATuning-free%20ORPCA%20makes%20it%20more%20scalable%20for%20large%20datasets%20since%20we%20do%20not%0Arequire%20dataset-dependent%20parameter%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07275v2&entry.124074799=Read"},
{"title": "Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided\n  Adaptive Token Selection", "author": "Ayush K. Rai and Kyle Min and Tarun Krishna and Feiyan Hu and Alan F. Smeaton and Noel E. O'Connor", "abstract": "  Masked video modeling~(MVM) has emerged as a highly effective pre-training\nstrategy for visual foundation models, whereby the model reconstructs masked\nspatiotemporal tokens using information from visible tokens. However, a key\nchallenge in such approaches lies in selecting an appropriate masking strategy.\nPrevious studies have explored predefined masking techniques, including random\nand tube-based masking, as well as approaches that leverage key motion priors,\noptical flow and semantic cues from externally pre-trained models. In this\nwork, we introduce a novel and generalizable Trajectory-Aware Adaptive Token\nSampler (TATS), which models the motion dynamics of tokens and can be\nseamlessly integrated into the masked autoencoder (MAE) framework to select\nmotion-centric tokens in videos. Additionally, we propose a unified training\nstrategy that enables joint optimization of both MAE and TATS from scratch\nusing Proximal Policy Optimization (PPO). We show that our model allows for\naggressive masking without compromising performance on the downstream task of\naction recognition while also ensuring that the pre-training remains memory\nefficient. Extensive experiments of the proposed approach across four\nbenchmarks, including Something-Something v2, Kinetics-400, UCF101, and HMDB51,\ndemonstrate the effectiveness, transferability, generalization, and efficiency\nof our work compared to other state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2505.08561v2", "date": "2025-08-14", "relevancy": 2.3648, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6183}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.599}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20meets%20Masked%20Video%20Modeling%20%3A%20Trajectory-Guided%0A%20%20Adaptive%20Token%20Selection&body=Title%3A%20Reinforcement%20Learning%20meets%20Masked%20Video%20Modeling%20%3A%20Trajectory-Guided%0A%20%20Adaptive%20Token%20Selection%0AAuthor%3A%20Ayush%20K.%20Rai%20and%20Kyle%20Min%20and%20Tarun%20Krishna%20and%20Feiyan%20Hu%20and%20Alan%20F.%20Smeaton%20and%20Noel%20E.%20O%27Connor%0AAbstract%3A%20%20%20Masked%20video%20modeling~%28MVM%29%20has%20emerged%20as%20a%20highly%20effective%20pre-training%0Astrategy%20for%20visual%20foundation%20models%2C%20whereby%20the%20model%20reconstructs%20masked%0Aspatiotemporal%20tokens%20using%20information%20from%20visible%20tokens.%20However%2C%20a%20key%0Achallenge%20in%20such%20approaches%20lies%20in%20selecting%20an%20appropriate%20masking%20strategy.%0APrevious%20studies%20have%20explored%20predefined%20masking%20techniques%2C%20including%20random%0Aand%20tube-based%20masking%2C%20as%20well%20as%20approaches%20that%20leverage%20key%20motion%20priors%2C%0Aoptical%20flow%20and%20semantic%20cues%20from%20externally%20pre-trained%20models.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20and%20generalizable%20Trajectory-Aware%20Adaptive%20Token%0ASampler%20%28TATS%29%2C%20which%20models%20the%20motion%20dynamics%20of%20tokens%20and%20can%20be%0Aseamlessly%20integrated%20into%20the%20masked%20autoencoder%20%28MAE%29%20framework%20to%20select%0Amotion-centric%20tokens%20in%20videos.%20Additionally%2C%20we%20propose%20a%20unified%20training%0Astrategy%20that%20enables%20joint%20optimization%20of%20both%20MAE%20and%20TATS%20from%20scratch%0Ausing%20Proximal%20Policy%20Optimization%20%28PPO%29.%20We%20show%20that%20our%20model%20allows%20for%0Aaggressive%20masking%20without%20compromising%20performance%20on%20the%20downstream%20task%20of%0Aaction%20recognition%20while%20also%20ensuring%20that%20the%20pre-training%20remains%20memory%0Aefficient.%20Extensive%20experiments%20of%20the%20proposed%20approach%20across%20four%0Abenchmarks%2C%20including%20Something-Something%20v2%2C%20Kinetics-400%2C%20UCF101%2C%20and%20HMDB51%2C%0Ademonstrate%20the%20effectiveness%2C%20transferability%2C%20generalization%2C%20and%20efficiency%0Aof%20our%20work%20compared%20to%20other%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08561v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520meets%2520Masked%2520Video%2520Modeling%2520%253A%2520Trajectory-Guided%250A%2520%2520Adaptive%2520Token%2520Selection%26entry.906535625%3DAyush%2520K.%2520Rai%2520and%2520Kyle%2520Min%2520and%2520Tarun%2520Krishna%2520and%2520Feiyan%2520Hu%2520and%2520Alan%2520F.%2520Smeaton%2520and%2520Noel%2520E.%2520O%2527Connor%26entry.1292438233%3D%2520%2520Masked%2520video%2520modeling~%2528MVM%2529%2520has%2520emerged%2520as%2520a%2520highly%2520effective%2520pre-training%250Astrategy%2520for%2520visual%2520foundation%2520models%252C%2520whereby%2520the%2520model%2520reconstructs%2520masked%250Aspatiotemporal%2520tokens%2520using%2520information%2520from%2520visible%2520tokens.%2520However%252C%2520a%2520key%250Achallenge%2520in%2520such%2520approaches%2520lies%2520in%2520selecting%2520an%2520appropriate%2520masking%2520strategy.%250APrevious%2520studies%2520have%2520explored%2520predefined%2520masking%2520techniques%252C%2520including%2520random%250Aand%2520tube-based%2520masking%252C%2520as%2520well%2520as%2520approaches%2520that%2520leverage%2520key%2520motion%2520priors%252C%250Aoptical%2520flow%2520and%2520semantic%2520cues%2520from%2520externally%2520pre-trained%2520models.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520a%2520novel%2520and%2520generalizable%2520Trajectory-Aware%2520Adaptive%2520Token%250ASampler%2520%2528TATS%2529%252C%2520which%2520models%2520the%2520motion%2520dynamics%2520of%2520tokens%2520and%2520can%2520be%250Aseamlessly%2520integrated%2520into%2520the%2520masked%2520autoencoder%2520%2528MAE%2529%2520framework%2520to%2520select%250Amotion-centric%2520tokens%2520in%2520videos.%2520Additionally%252C%2520we%2520propose%2520a%2520unified%2520training%250Astrategy%2520that%2520enables%2520joint%2520optimization%2520of%2520both%2520MAE%2520and%2520TATS%2520from%2520scratch%250Ausing%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529.%2520We%2520show%2520that%2520our%2520model%2520allows%2520for%250Aaggressive%2520masking%2520without%2520compromising%2520performance%2520on%2520the%2520downstream%2520task%2520of%250Aaction%2520recognition%2520while%2520also%2520ensuring%2520that%2520the%2520pre-training%2520remains%2520memory%250Aefficient.%2520Extensive%2520experiments%2520of%2520the%2520proposed%2520approach%2520across%2520four%250Abenchmarks%252C%2520including%2520Something-Something%2520v2%252C%2520Kinetics-400%252C%2520UCF101%252C%2520and%2520HMDB51%252C%250Ademonstrate%2520the%2520effectiveness%252C%2520transferability%252C%2520generalization%252C%2520and%2520efficiency%250Aof%2520our%2520work%2520compared%2520to%2520other%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08561v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20meets%20Masked%20Video%20Modeling%20%3A%20Trajectory-Guided%0A%20%20Adaptive%20Token%20Selection&entry.906535625=Ayush%20K.%20Rai%20and%20Kyle%20Min%20and%20Tarun%20Krishna%20and%20Feiyan%20Hu%20and%20Alan%20F.%20Smeaton%20and%20Noel%20E.%20O%27Connor&entry.1292438233=%20%20Masked%20video%20modeling~%28MVM%29%20has%20emerged%20as%20a%20highly%20effective%20pre-training%0Astrategy%20for%20visual%20foundation%20models%2C%20whereby%20the%20model%20reconstructs%20masked%0Aspatiotemporal%20tokens%20using%20information%20from%20visible%20tokens.%20However%2C%20a%20key%0Achallenge%20in%20such%20approaches%20lies%20in%20selecting%20an%20appropriate%20masking%20strategy.%0APrevious%20studies%20have%20explored%20predefined%20masking%20techniques%2C%20including%20random%0Aand%20tube-based%20masking%2C%20as%20well%20as%20approaches%20that%20leverage%20key%20motion%20priors%2C%0Aoptical%20flow%20and%20semantic%20cues%20from%20externally%20pre-trained%20models.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20and%20generalizable%20Trajectory-Aware%20Adaptive%20Token%0ASampler%20%28TATS%29%2C%20which%20models%20the%20motion%20dynamics%20of%20tokens%20and%20can%20be%0Aseamlessly%20integrated%20into%20the%20masked%20autoencoder%20%28MAE%29%20framework%20to%20select%0Amotion-centric%20tokens%20in%20videos.%20Additionally%2C%20we%20propose%20a%20unified%20training%0Astrategy%20that%20enables%20joint%20optimization%20of%20both%20MAE%20and%20TATS%20from%20scratch%0Ausing%20Proximal%20Policy%20Optimization%20%28PPO%29.%20We%20show%20that%20our%20model%20allows%20for%0Aaggressive%20masking%20without%20compromising%20performance%20on%20the%20downstream%20task%20of%0Aaction%20recognition%20while%20also%20ensuring%20that%20the%20pre-training%20remains%20memory%0Aefficient.%20Extensive%20experiments%20of%20the%20proposed%20approach%20across%20four%0Abenchmarks%2C%20including%20Something-Something%20v2%2C%20Kinetics-400%2C%20UCF101%2C%20and%20HMDB51%2C%0Ademonstrate%20the%20effectiveness%2C%20transferability%2C%20generalization%2C%20and%20efficiency%0Aof%20our%20work%20compared%20to%20other%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08561v2&entry.124074799=Read"},
{"title": "Evaluation of Cultural Competence of Vision-Language Models", "author": "Srishti Yadav and Lauren Tilton and Maria Antoniak and Taylor Arnold and Jiaang Li and Siddhesh Milind Pawar and Antonia Karamolegkou and Stella Frank and Zhaochong An and Negar Rostamzadeh and Daniel Hershcovich and Serge Belongie and Ekaterina Shutova", "abstract": "  Modern vision-language models (VLMs) often fail at cultural competency\nevaluations and benchmarks. Given the diversity of applications built upon\nVLMs, there is renewed interest in understanding how they encode cultural\nnuances. While individual aspects of this problem have been studied, we still\nlack a comprehensive framework for systematically identifying and annotating\nthe nuanced cultural dimensions present in images for VLMs. This position paper\nargues that foundational methodologies from visual culture studies (cultural\nstudies, semiotics, and visual studies) are necessary for cultural analysis of\nimages. Building upon this review, we propose a set of five frameworks,\ncorresponding to cultural dimensions, that must be considered for a more\ncomplete analysis of the cultural competencies of VLMs.\n", "link": "http://arxiv.org/abs/2505.22793v2", "date": "2025-08-14", "relevancy": 2.3616, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6014}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6014}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Cultural%20Competence%20of%20Vision-Language%20Models&body=Title%3A%20Evaluation%20of%20Cultural%20Competence%20of%20Vision-Language%20Models%0AAuthor%3A%20Srishti%20Yadav%20and%20Lauren%20Tilton%20and%20Maria%20Antoniak%20and%20Taylor%20Arnold%20and%20Jiaang%20Li%20and%20Siddhesh%20Milind%20Pawar%20and%20Antonia%20Karamolegkou%20and%20Stella%20Frank%20and%20Zhaochong%20An%20and%20Negar%20Rostamzadeh%20and%20Daniel%20Hershcovich%20and%20Serge%20Belongie%20and%20Ekaterina%20Shutova%0AAbstract%3A%20%20%20Modern%20vision-language%20models%20%28VLMs%29%20often%20fail%20at%20cultural%20competency%0Aevaluations%20and%20benchmarks.%20Given%20the%20diversity%20of%20applications%20built%20upon%0AVLMs%2C%20there%20is%20renewed%20interest%20in%20understanding%20how%20they%20encode%20cultural%0Anuances.%20While%20individual%20aspects%20of%20this%20problem%20have%20been%20studied%2C%20we%20still%0Alack%20a%20comprehensive%20framework%20for%20systematically%20identifying%20and%20annotating%0Athe%20nuanced%20cultural%20dimensions%20present%20in%20images%20for%20VLMs.%20This%20position%20paper%0Aargues%20that%20foundational%20methodologies%20from%20visual%20culture%20studies%20%28cultural%0Astudies%2C%20semiotics%2C%20and%20visual%20studies%29%20are%20necessary%20for%20cultural%20analysis%20of%0Aimages.%20Building%20upon%20this%20review%2C%20we%20propose%20a%20set%20of%20five%20frameworks%2C%0Acorresponding%20to%20cultural%20dimensions%2C%20that%20must%20be%20considered%20for%20a%20more%0Acomplete%20analysis%20of%20the%20cultural%20competencies%20of%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22793v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520Cultural%2520Competence%2520of%2520Vision-Language%2520Models%26entry.906535625%3DSrishti%2520Yadav%2520and%2520Lauren%2520Tilton%2520and%2520Maria%2520Antoniak%2520and%2520Taylor%2520Arnold%2520and%2520Jiaang%2520Li%2520and%2520Siddhesh%2520Milind%2520Pawar%2520and%2520Antonia%2520Karamolegkou%2520and%2520Stella%2520Frank%2520and%2520Zhaochong%2520An%2520and%2520Negar%2520Rostamzadeh%2520and%2520Daniel%2520Hershcovich%2520and%2520Serge%2520Belongie%2520and%2520Ekaterina%2520Shutova%26entry.1292438233%3D%2520%2520Modern%2520vision-language%2520models%2520%2528VLMs%2529%2520often%2520fail%2520at%2520cultural%2520competency%250Aevaluations%2520and%2520benchmarks.%2520Given%2520the%2520diversity%2520of%2520applications%2520built%2520upon%250AVLMs%252C%2520there%2520is%2520renewed%2520interest%2520in%2520understanding%2520how%2520they%2520encode%2520cultural%250Anuances.%2520While%2520individual%2520aspects%2520of%2520this%2520problem%2520have%2520been%2520studied%252C%2520we%2520still%250Alack%2520a%2520comprehensive%2520framework%2520for%2520systematically%2520identifying%2520and%2520annotating%250Athe%2520nuanced%2520cultural%2520dimensions%2520present%2520in%2520images%2520for%2520VLMs.%2520This%2520position%2520paper%250Aargues%2520that%2520foundational%2520methodologies%2520from%2520visual%2520culture%2520studies%2520%2528cultural%250Astudies%252C%2520semiotics%252C%2520and%2520visual%2520studies%2529%2520are%2520necessary%2520for%2520cultural%2520analysis%2520of%250Aimages.%2520Building%2520upon%2520this%2520review%252C%2520we%2520propose%2520a%2520set%2520of%2520five%2520frameworks%252C%250Acorresponding%2520to%2520cultural%2520dimensions%252C%2520that%2520must%2520be%2520considered%2520for%2520a%2520more%250Acomplete%2520analysis%2520of%2520the%2520cultural%2520competencies%2520of%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22793v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Cultural%20Competence%20of%20Vision-Language%20Models&entry.906535625=Srishti%20Yadav%20and%20Lauren%20Tilton%20and%20Maria%20Antoniak%20and%20Taylor%20Arnold%20and%20Jiaang%20Li%20and%20Siddhesh%20Milind%20Pawar%20and%20Antonia%20Karamolegkou%20and%20Stella%20Frank%20and%20Zhaochong%20An%20and%20Negar%20Rostamzadeh%20and%20Daniel%20Hershcovich%20and%20Serge%20Belongie%20and%20Ekaterina%20Shutova&entry.1292438233=%20%20Modern%20vision-language%20models%20%28VLMs%29%20often%20fail%20at%20cultural%20competency%0Aevaluations%20and%20benchmarks.%20Given%20the%20diversity%20of%20applications%20built%20upon%0AVLMs%2C%20there%20is%20renewed%20interest%20in%20understanding%20how%20they%20encode%20cultural%0Anuances.%20While%20individual%20aspects%20of%20this%20problem%20have%20been%20studied%2C%20we%20still%0Alack%20a%20comprehensive%20framework%20for%20systematically%20identifying%20and%20annotating%0Athe%20nuanced%20cultural%20dimensions%20present%20in%20images%20for%20VLMs.%20This%20position%20paper%0Aargues%20that%20foundational%20methodologies%20from%20visual%20culture%20studies%20%28cultural%0Astudies%2C%20semiotics%2C%20and%20visual%20studies%29%20are%20necessary%20for%20cultural%20analysis%20of%0Aimages.%20Building%20upon%20this%20review%2C%20we%20propose%20a%20set%20of%20five%20frameworks%2C%0Acorresponding%20to%20cultural%20dimensions%2C%20that%20must%20be%20considered%20for%20a%20more%0Acomplete%20analysis%20of%20the%20cultural%20competencies%20of%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22793v2&entry.124074799=Read"},
{"title": "SPHENIC: Topology-Informed Multi-View Clustering for Spatial\n  Transcriptomics", "author": "Chenkai Guo and Yikai Zhu and Jing Yangum and Renxiang Guan and Por Lip Yee and Guangdun Peng and Dayu Hu", "abstract": "  By incorporating spatial location information, spatial-transcriptomics\nclustering yields more comprehensive insights into cell subpopulation\nidentification. Despite recent progress, existing methods have at least two\nlimitations: (i) topological learning typically considers only representations\nof individual cells or their interaction graphs; however, spatial\ntranscriptomic profiles are often noisy, making these approaches vulnerable to\nlow-quality topological signals, and (ii) insufficient modeling of spatial\nneighborhood information leads to low-quality spatial embeddings. To address\nthese limitations, we propose SPHENIC, a novel Spatial Persistent Homology\nEnhanced Neighborhood Integrative Clustering method. Specifically, SPHENIC\nincorporates invariant topological features into the clustering network to\nachieve stable representation learning. Additionally, to construct high-quality\nspatial embeddings that reflect the true cellular distribution, we design the\nSpatial Constraint and Distribution Optimization Module (SCDOM). This module\nincreases the similarity between a cell's embedding and those of its spatial\nneighbors, decreases similarity with non-neighboring cells, and thereby\nproduces clustering-friendly spatial embeddings. Extensive experiments on 14\nbenchmark spatial transcriptomic slices demonstrate that SPHENIC achieves\nsuperior performance on the spatial clustering task, outperforming existing\nstate-of-the-art methods by 3.31%-6.54% over the best alternative.\n", "link": "http://arxiv.org/abs/2508.10646v1", "date": "2025-08-14", "relevancy": 2.3591, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4814}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4689}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPHENIC%3A%20Topology-Informed%20Multi-View%20Clustering%20for%20Spatial%0A%20%20Transcriptomics&body=Title%3A%20SPHENIC%3A%20Topology-Informed%20Multi-View%20Clustering%20for%20Spatial%0A%20%20Transcriptomics%0AAuthor%3A%20Chenkai%20Guo%20and%20Yikai%20Zhu%20and%20Jing%20Yangum%20and%20Renxiang%20Guan%20and%20Por%20Lip%20Yee%20and%20Guangdun%20Peng%20and%20Dayu%20Hu%0AAbstract%3A%20%20%20By%20incorporating%20spatial%20location%20information%2C%20spatial-transcriptomics%0Aclustering%20yields%20more%20comprehensive%20insights%20into%20cell%20subpopulation%0Aidentification.%20Despite%20recent%20progress%2C%20existing%20methods%20have%20at%20least%20two%0Alimitations%3A%20%28i%29%20topological%20learning%20typically%20considers%20only%20representations%0Aof%20individual%20cells%20or%20their%20interaction%20graphs%3B%20however%2C%20spatial%0Atranscriptomic%20profiles%20are%20often%20noisy%2C%20making%20these%20approaches%20vulnerable%20to%0Alow-quality%20topological%20signals%2C%20and%20%28ii%29%20insufficient%20modeling%20of%20spatial%0Aneighborhood%20information%20leads%20to%20low-quality%20spatial%20embeddings.%20To%20address%0Athese%20limitations%2C%20we%20propose%20SPHENIC%2C%20a%20novel%20Spatial%20Persistent%20Homology%0AEnhanced%20Neighborhood%20Integrative%20Clustering%20method.%20Specifically%2C%20SPHENIC%0Aincorporates%20invariant%20topological%20features%20into%20the%20clustering%20network%20to%0Aachieve%20stable%20representation%20learning.%20Additionally%2C%20to%20construct%20high-quality%0Aspatial%20embeddings%20that%20reflect%20the%20true%20cellular%20distribution%2C%20we%20design%20the%0ASpatial%20Constraint%20and%20Distribution%20Optimization%20Module%20%28SCDOM%29.%20This%20module%0Aincreases%20the%20similarity%20between%20a%20cell%27s%20embedding%20and%20those%20of%20its%20spatial%0Aneighbors%2C%20decreases%20similarity%20with%20non-neighboring%20cells%2C%20and%20thereby%0Aproduces%20clustering-friendly%20spatial%20embeddings.%20Extensive%20experiments%20on%2014%0Abenchmark%20spatial%20transcriptomic%20slices%20demonstrate%20that%20SPHENIC%20achieves%0Asuperior%20performance%20on%20the%20spatial%20clustering%20task%2C%20outperforming%20existing%0Astate-of-the-art%20methods%20by%203.31%25-6.54%25%20over%20the%20best%20alternative.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPHENIC%253A%2520Topology-Informed%2520Multi-View%2520Clustering%2520for%2520Spatial%250A%2520%2520Transcriptomics%26entry.906535625%3DChenkai%2520Guo%2520and%2520Yikai%2520Zhu%2520and%2520Jing%2520Yangum%2520and%2520Renxiang%2520Guan%2520and%2520Por%2520Lip%2520Yee%2520and%2520Guangdun%2520Peng%2520and%2520Dayu%2520Hu%26entry.1292438233%3D%2520%2520By%2520incorporating%2520spatial%2520location%2520information%252C%2520spatial-transcriptomics%250Aclustering%2520yields%2520more%2520comprehensive%2520insights%2520into%2520cell%2520subpopulation%250Aidentification.%2520Despite%2520recent%2520progress%252C%2520existing%2520methods%2520have%2520at%2520least%2520two%250Alimitations%253A%2520%2528i%2529%2520topological%2520learning%2520typically%2520considers%2520only%2520representations%250Aof%2520individual%2520cells%2520or%2520their%2520interaction%2520graphs%253B%2520however%252C%2520spatial%250Atranscriptomic%2520profiles%2520are%2520often%2520noisy%252C%2520making%2520these%2520approaches%2520vulnerable%2520to%250Alow-quality%2520topological%2520signals%252C%2520and%2520%2528ii%2529%2520insufficient%2520modeling%2520of%2520spatial%250Aneighborhood%2520information%2520leads%2520to%2520low-quality%2520spatial%2520embeddings.%2520To%2520address%250Athese%2520limitations%252C%2520we%2520propose%2520SPHENIC%252C%2520a%2520novel%2520Spatial%2520Persistent%2520Homology%250AEnhanced%2520Neighborhood%2520Integrative%2520Clustering%2520method.%2520Specifically%252C%2520SPHENIC%250Aincorporates%2520invariant%2520topological%2520features%2520into%2520the%2520clustering%2520network%2520to%250Aachieve%2520stable%2520representation%2520learning.%2520Additionally%252C%2520to%2520construct%2520high-quality%250Aspatial%2520embeddings%2520that%2520reflect%2520the%2520true%2520cellular%2520distribution%252C%2520we%2520design%2520the%250ASpatial%2520Constraint%2520and%2520Distribution%2520Optimization%2520Module%2520%2528SCDOM%2529.%2520This%2520module%250Aincreases%2520the%2520similarity%2520between%2520a%2520cell%2527s%2520embedding%2520and%2520those%2520of%2520its%2520spatial%250Aneighbors%252C%2520decreases%2520similarity%2520with%2520non-neighboring%2520cells%252C%2520and%2520thereby%250Aproduces%2520clustering-friendly%2520spatial%2520embeddings.%2520Extensive%2520experiments%2520on%252014%250Abenchmark%2520spatial%2520transcriptomic%2520slices%2520demonstrate%2520that%2520SPHENIC%2520achieves%250Asuperior%2520performance%2520on%2520the%2520spatial%2520clustering%2520task%252C%2520outperforming%2520existing%250Astate-of-the-art%2520methods%2520by%25203.31%2525-6.54%2525%2520over%2520the%2520best%2520alternative.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPHENIC%3A%20Topology-Informed%20Multi-View%20Clustering%20for%20Spatial%0A%20%20Transcriptomics&entry.906535625=Chenkai%20Guo%20and%20Yikai%20Zhu%20and%20Jing%20Yangum%20and%20Renxiang%20Guan%20and%20Por%20Lip%20Yee%20and%20Guangdun%20Peng%20and%20Dayu%20Hu&entry.1292438233=%20%20By%20incorporating%20spatial%20location%20information%2C%20spatial-transcriptomics%0Aclustering%20yields%20more%20comprehensive%20insights%20into%20cell%20subpopulation%0Aidentification.%20Despite%20recent%20progress%2C%20existing%20methods%20have%20at%20least%20two%0Alimitations%3A%20%28i%29%20topological%20learning%20typically%20considers%20only%20representations%0Aof%20individual%20cells%20or%20their%20interaction%20graphs%3B%20however%2C%20spatial%0Atranscriptomic%20profiles%20are%20often%20noisy%2C%20making%20these%20approaches%20vulnerable%20to%0Alow-quality%20topological%20signals%2C%20and%20%28ii%29%20insufficient%20modeling%20of%20spatial%0Aneighborhood%20information%20leads%20to%20low-quality%20spatial%20embeddings.%20To%20address%0Athese%20limitations%2C%20we%20propose%20SPHENIC%2C%20a%20novel%20Spatial%20Persistent%20Homology%0AEnhanced%20Neighborhood%20Integrative%20Clustering%20method.%20Specifically%2C%20SPHENIC%0Aincorporates%20invariant%20topological%20features%20into%20the%20clustering%20network%20to%0Aachieve%20stable%20representation%20learning.%20Additionally%2C%20to%20construct%20high-quality%0Aspatial%20embeddings%20that%20reflect%20the%20true%20cellular%20distribution%2C%20we%20design%20the%0ASpatial%20Constraint%20and%20Distribution%20Optimization%20Module%20%28SCDOM%29.%20This%20module%0Aincreases%20the%20similarity%20between%20a%20cell%27s%20embedding%20and%20those%20of%20its%20spatial%0Aneighbors%2C%20decreases%20similarity%20with%20non-neighboring%20cells%2C%20and%20thereby%0Aproduces%20clustering-friendly%20spatial%20embeddings.%20Extensive%20experiments%20on%2014%0Abenchmark%20spatial%20transcriptomic%20slices%20demonstrate%20that%20SPHENIC%20achieves%0Asuperior%20performance%20on%20the%20spatial%20clustering%20task%2C%20outperforming%20existing%0Astate-of-the-art%20methods%20by%203.31%25-6.54%25%20over%20the%20best%20alternative.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10646v1&entry.124074799=Read"},
{"title": "SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous\n  Driving", "author": "Philipp Wolters and Johannes Gilg and Torben Teepe and Gerhard Rigoll", "abstract": "  End-to-end autonomous driving systems promise stronger performance through\nunified optimization of perception, motion forecasting, and planning. However,\nvision-based approaches face fundamental limitations in adverse weather\nconditions, partial occlusions, and precise velocity estimation - critical\nchallenges in safety-sensitive scenarios where accurate motion understanding\nand long-horizon trajectory prediction are essential for collision avoidance.\nTo address these limitations, we propose SpaRC-AD, a query-based end-to-end\ncamera-radar fusion framework for planning-oriented autonomous driving. Through\nsparse 3D feature alignment, and doppler-based velocity estimation, we achieve\nstrong 3D scene representations for refinement of agent anchors, map polylines\nand motion modelling. Our method achieves strong improvements over the\nstate-of-the-art vision-only baselines across multiple autonomous driving\ntasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA),\nonline mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory\nplanning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal\nconsistency on multiple challenging benchmarks, including real-world open-loop\nnuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We\nshow the effectiveness of radar-based fusion in safety-critical scenarios where\naccurate motion understanding and long-horizon trajectory prediction are\nessential for collision avoidance. The source code of all experiments is\navailable at https://phi-wol.github.io/sparcad/\n", "link": "http://arxiv.org/abs/2508.10567v1", "date": "2025-08-14", "relevancy": 2.3541, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.595}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5847}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaRC-AD%3A%20A%20Baseline%20for%20Radar-Camera%20Fusion%20in%20End-to-End%20Autonomous%0A%20%20Driving&body=Title%3A%20SpaRC-AD%3A%20A%20Baseline%20for%20Radar-Camera%20Fusion%20in%20End-to-End%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Philipp%20Wolters%20and%20Johannes%20Gilg%20and%20Torben%20Teepe%20and%20Gerhard%20Rigoll%0AAbstract%3A%20%20%20End-to-end%20autonomous%20driving%20systems%20promise%20stronger%20performance%20through%0Aunified%20optimization%20of%20perception%2C%20motion%20forecasting%2C%20and%20planning.%20However%2C%0Avision-based%20approaches%20face%20fundamental%20limitations%20in%20adverse%20weather%0Aconditions%2C%20partial%20occlusions%2C%20and%20precise%20velocity%20estimation%20-%20critical%0Achallenges%20in%20safety-sensitive%20scenarios%20where%20accurate%20motion%20understanding%0Aand%20long-horizon%20trajectory%20prediction%20are%20essential%20for%20collision%20avoidance.%0ATo%20address%20these%20limitations%2C%20we%20propose%20SpaRC-AD%2C%20a%20query-based%20end-to-end%0Acamera-radar%20fusion%20framework%20for%20planning-oriented%20autonomous%20driving.%20Through%0Asparse%203D%20feature%20alignment%2C%20and%20doppler-based%20velocity%20estimation%2C%20we%20achieve%0Astrong%203D%20scene%20representations%20for%20refinement%20of%20agent%20anchors%2C%20map%20polylines%0Aand%20motion%20modelling.%20Our%20method%20achieves%20strong%20improvements%20over%20the%0Astate-of-the-art%20vision-only%20baselines%20across%20multiple%20autonomous%20driving%0Atasks%2C%20including%203D%20detection%20%28%2B4.8%25%20mAP%29%2C%20multi-object%20tracking%20%28%2B8.3%25%20AMOTA%29%2C%0Aonline%20mapping%20%28%2B1.8%25%20mAP%29%2C%20motion%20prediction%20%28-4.0%25%20mADE%29%2C%20and%20trajectory%0Aplanning%20%28-0.1m%20L2%20and%20-9%25%20TPC%29.%20We%20achieve%20both%20spatial%20coherence%20and%20temporal%0Aconsistency%20on%20multiple%20challenging%20benchmarks%2C%20including%20real-world%20open-loop%0AnuScenes%2C%20long-horizon%20T-nuScenes%2C%20and%20closed-loop%20simulator%20Bench2Drive.%20We%0Ashow%20the%20effectiveness%20of%20radar-based%20fusion%20in%20safety-critical%20scenarios%20where%0Aaccurate%20motion%20understanding%20and%20long-horizon%20trajectory%20prediction%20are%0Aessential%20for%20collision%20avoidance.%20The%20source%20code%20of%20all%20experiments%20is%0Aavailable%20at%20https%3A//phi-wol.github.io/sparcad/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaRC-AD%253A%2520A%2520Baseline%2520for%2520Radar-Camera%2520Fusion%2520in%2520End-to-End%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DPhilipp%2520Wolters%2520and%2520Johannes%2520Gilg%2520and%2520Torben%2520Teepe%2520and%2520Gerhard%2520Rigoll%26entry.1292438233%3D%2520%2520End-to-end%2520autonomous%2520driving%2520systems%2520promise%2520stronger%2520performance%2520through%250Aunified%2520optimization%2520of%2520perception%252C%2520motion%2520forecasting%252C%2520and%2520planning.%2520However%252C%250Avision-based%2520approaches%2520face%2520fundamental%2520limitations%2520in%2520adverse%2520weather%250Aconditions%252C%2520partial%2520occlusions%252C%2520and%2520precise%2520velocity%2520estimation%2520-%2520critical%250Achallenges%2520in%2520safety-sensitive%2520scenarios%2520where%2520accurate%2520motion%2520understanding%250Aand%2520long-horizon%2520trajectory%2520prediction%2520are%2520essential%2520for%2520collision%2520avoidance.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520propose%2520SpaRC-AD%252C%2520a%2520query-based%2520end-to-end%250Acamera-radar%2520fusion%2520framework%2520for%2520planning-oriented%2520autonomous%2520driving.%2520Through%250Asparse%25203D%2520feature%2520alignment%252C%2520and%2520doppler-based%2520velocity%2520estimation%252C%2520we%2520achieve%250Astrong%25203D%2520scene%2520representations%2520for%2520refinement%2520of%2520agent%2520anchors%252C%2520map%2520polylines%250Aand%2520motion%2520modelling.%2520Our%2520method%2520achieves%2520strong%2520improvements%2520over%2520the%250Astate-of-the-art%2520vision-only%2520baselines%2520across%2520multiple%2520autonomous%2520driving%250Atasks%252C%2520including%25203D%2520detection%2520%2528%252B4.8%2525%2520mAP%2529%252C%2520multi-object%2520tracking%2520%2528%252B8.3%2525%2520AMOTA%2529%252C%250Aonline%2520mapping%2520%2528%252B1.8%2525%2520mAP%2529%252C%2520motion%2520prediction%2520%2528-4.0%2525%2520mADE%2529%252C%2520and%2520trajectory%250Aplanning%2520%2528-0.1m%2520L2%2520and%2520-9%2525%2520TPC%2529.%2520We%2520achieve%2520both%2520spatial%2520coherence%2520and%2520temporal%250Aconsistency%2520on%2520multiple%2520challenging%2520benchmarks%252C%2520including%2520real-world%2520open-loop%250AnuScenes%252C%2520long-horizon%2520T-nuScenes%252C%2520and%2520closed-loop%2520simulator%2520Bench2Drive.%2520We%250Ashow%2520the%2520effectiveness%2520of%2520radar-based%2520fusion%2520in%2520safety-critical%2520scenarios%2520where%250Aaccurate%2520motion%2520understanding%2520and%2520long-horizon%2520trajectory%2520prediction%2520are%250Aessential%2520for%2520collision%2520avoidance.%2520The%2520source%2520code%2520of%2520all%2520experiments%2520is%250Aavailable%2520at%2520https%253A//phi-wol.github.io/sparcad/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaRC-AD%3A%20A%20Baseline%20for%20Radar-Camera%20Fusion%20in%20End-to-End%20Autonomous%0A%20%20Driving&entry.906535625=Philipp%20Wolters%20and%20Johannes%20Gilg%20and%20Torben%20Teepe%20and%20Gerhard%20Rigoll&entry.1292438233=%20%20End-to-end%20autonomous%20driving%20systems%20promise%20stronger%20performance%20through%0Aunified%20optimization%20of%20perception%2C%20motion%20forecasting%2C%20and%20planning.%20However%2C%0Avision-based%20approaches%20face%20fundamental%20limitations%20in%20adverse%20weather%0Aconditions%2C%20partial%20occlusions%2C%20and%20precise%20velocity%20estimation%20-%20critical%0Achallenges%20in%20safety-sensitive%20scenarios%20where%20accurate%20motion%20understanding%0Aand%20long-horizon%20trajectory%20prediction%20are%20essential%20for%20collision%20avoidance.%0ATo%20address%20these%20limitations%2C%20we%20propose%20SpaRC-AD%2C%20a%20query-based%20end-to-end%0Acamera-radar%20fusion%20framework%20for%20planning-oriented%20autonomous%20driving.%20Through%0Asparse%203D%20feature%20alignment%2C%20and%20doppler-based%20velocity%20estimation%2C%20we%20achieve%0Astrong%203D%20scene%20representations%20for%20refinement%20of%20agent%20anchors%2C%20map%20polylines%0Aand%20motion%20modelling.%20Our%20method%20achieves%20strong%20improvements%20over%20the%0Astate-of-the-art%20vision-only%20baselines%20across%20multiple%20autonomous%20driving%0Atasks%2C%20including%203D%20detection%20%28%2B4.8%25%20mAP%29%2C%20multi-object%20tracking%20%28%2B8.3%25%20AMOTA%29%2C%0Aonline%20mapping%20%28%2B1.8%25%20mAP%29%2C%20motion%20prediction%20%28-4.0%25%20mADE%29%2C%20and%20trajectory%0Aplanning%20%28-0.1m%20L2%20and%20-9%25%20TPC%29.%20We%20achieve%20both%20spatial%20coherence%20and%20temporal%0Aconsistency%20on%20multiple%20challenging%20benchmarks%2C%20including%20real-world%20open-loop%0AnuScenes%2C%20long-horizon%20T-nuScenes%2C%20and%20closed-loop%20simulator%20Bench2Drive.%20We%0Ashow%20the%20effectiveness%20of%20radar-based%20fusion%20in%20safety-critical%20scenarios%20where%0Aaccurate%20motion%20understanding%20and%20long-horizon%20trajectory%20prediction%20are%0Aessential%20for%20collision%20avoidance.%20The%20source%20code%20of%20all%20experiments%20is%0Aavailable%20at%20https%3A//phi-wol.github.io/sparcad/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10567v1&entry.124074799=Read"},
{"title": "Quantum-Brain: Quantum-Inspired Neural Network Approach to Vision-Brain\n  Understanding", "author": "Hoang-Quan Nguyen and Xuan-Bac Nguyen and Hugh Churchill and Arabinda Kumar Choudhary and Pawan Sinha and Samee U. Khan and Khoa Luu", "abstract": "  Vision-brain understanding aims to extract semantic information about brain\nsignals from human perceptions. Existing deep learning methods for vision-brain\nunderstanding are usually introduced in a traditional learning paradigm missing\nthe ability to learn the connectivities between brain regions. Meanwhile, the\nquantum computing theory offers a new paradigm for designing deep learning\nmodels. Motivated by the connectivities in the brain signals and the\nentanglement properties in quantum computing, we propose a novel Quantum-Brain\napproach, a quantum-inspired neural network, to tackle the vision-brain\nunderstanding problem. To compute the connectivity between areas in brain\nsignals, we introduce a new Quantum-Inspired Voxel-Controlling module to learn\nthe impact of a brain voxel on others represented in the Hilbert space. To\neffectively learn connectivity, a novel Phase-Shifting module is presented to\ncalibrate the value of the brain signals. Finally, we introduce a new\nMeasurement-like Projection module to present the connectivity information from\nthe Hilbert space into the feature space. The proposed approach can learn to\nfind the connectivities between fMRI voxels and enhance the semantic\ninformation obtained from human perceptions. Our experimental results on the\nNatural Scene Dataset benchmarks illustrate the effectiveness of the proposed\nmethod with Top-1 accuracies of 95.1% and 95.6% on image and brain retrieval\ntasks and an Inception score of 95.3% on fMRI-to-image reconstruction task. Our\nproposed quantum-inspired network brings a potential paradigm to solving the\nvision-brain problems via the quantum computing theory.\n", "link": "http://arxiv.org/abs/2411.13378v2", "date": "2025-08-14", "relevancy": 2.3478, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5994}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum-Brain%3A%20Quantum-Inspired%20Neural%20Network%20Approach%20to%20Vision-Brain%0A%20%20Understanding&body=Title%3A%20Quantum-Brain%3A%20Quantum-Inspired%20Neural%20Network%20Approach%20to%20Vision-Brain%0A%20%20Understanding%0AAuthor%3A%20Hoang-Quan%20Nguyen%20and%20Xuan-Bac%20Nguyen%20and%20Hugh%20Churchill%20and%20Arabinda%20Kumar%20Choudhary%20and%20Pawan%20Sinha%20and%20Samee%20U.%20Khan%20and%20Khoa%20Luu%0AAbstract%3A%20%20%20Vision-brain%20understanding%20aims%20to%20extract%20semantic%20information%20about%20brain%0Asignals%20from%20human%20perceptions.%20Existing%20deep%20learning%20methods%20for%20vision-brain%0Aunderstanding%20are%20usually%20introduced%20in%20a%20traditional%20learning%20paradigm%20missing%0Athe%20ability%20to%20learn%20the%20connectivities%20between%20brain%20regions.%20Meanwhile%2C%20the%0Aquantum%20computing%20theory%20offers%20a%20new%20paradigm%20for%20designing%20deep%20learning%0Amodels.%20Motivated%20by%20the%20connectivities%20in%20the%20brain%20signals%20and%20the%0Aentanglement%20properties%20in%20quantum%20computing%2C%20we%20propose%20a%20novel%20Quantum-Brain%0Aapproach%2C%20a%20quantum-inspired%20neural%20network%2C%20to%20tackle%20the%20vision-brain%0Aunderstanding%20problem.%20To%20compute%20the%20connectivity%20between%20areas%20in%20brain%0Asignals%2C%20we%20introduce%20a%20new%20Quantum-Inspired%20Voxel-Controlling%20module%20to%20learn%0Athe%20impact%20of%20a%20brain%20voxel%20on%20others%20represented%20in%20the%20Hilbert%20space.%20To%0Aeffectively%20learn%20connectivity%2C%20a%20novel%20Phase-Shifting%20module%20is%20presented%20to%0Acalibrate%20the%20value%20of%20the%20brain%20signals.%20Finally%2C%20we%20introduce%20a%20new%0AMeasurement-like%20Projection%20module%20to%20present%20the%20connectivity%20information%20from%0Athe%20Hilbert%20space%20into%20the%20feature%20space.%20The%20proposed%20approach%20can%20learn%20to%0Afind%20the%20connectivities%20between%20fMRI%20voxels%20and%20enhance%20the%20semantic%0Ainformation%20obtained%20from%20human%20perceptions.%20Our%20experimental%20results%20on%20the%0ANatural%20Scene%20Dataset%20benchmarks%20illustrate%20the%20effectiveness%20of%20the%20proposed%0Amethod%20with%20Top-1%20accuracies%20of%2095.1%25%20and%2095.6%25%20on%20image%20and%20brain%20retrieval%0Atasks%20and%20an%20Inception%20score%20of%2095.3%25%20on%20fMRI-to-image%20reconstruction%20task.%20Our%0Aproposed%20quantum-inspired%20network%20brings%20a%20potential%20paradigm%20to%20solving%20the%0Avision-brain%20problems%20via%20the%20quantum%20computing%20theory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13378v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum-Brain%253A%2520Quantum-Inspired%2520Neural%2520Network%2520Approach%2520to%2520Vision-Brain%250A%2520%2520Understanding%26entry.906535625%3DHoang-Quan%2520Nguyen%2520and%2520Xuan-Bac%2520Nguyen%2520and%2520Hugh%2520Churchill%2520and%2520Arabinda%2520Kumar%2520Choudhary%2520and%2520Pawan%2520Sinha%2520and%2520Samee%2520U.%2520Khan%2520and%2520Khoa%2520Luu%26entry.1292438233%3D%2520%2520Vision-brain%2520understanding%2520aims%2520to%2520extract%2520semantic%2520information%2520about%2520brain%250Asignals%2520from%2520human%2520perceptions.%2520Existing%2520deep%2520learning%2520methods%2520for%2520vision-brain%250Aunderstanding%2520are%2520usually%2520introduced%2520in%2520a%2520traditional%2520learning%2520paradigm%2520missing%250Athe%2520ability%2520to%2520learn%2520the%2520connectivities%2520between%2520brain%2520regions.%2520Meanwhile%252C%2520the%250Aquantum%2520computing%2520theory%2520offers%2520a%2520new%2520paradigm%2520for%2520designing%2520deep%2520learning%250Amodels.%2520Motivated%2520by%2520the%2520connectivities%2520in%2520the%2520brain%2520signals%2520and%2520the%250Aentanglement%2520properties%2520in%2520quantum%2520computing%252C%2520we%2520propose%2520a%2520novel%2520Quantum-Brain%250Aapproach%252C%2520a%2520quantum-inspired%2520neural%2520network%252C%2520to%2520tackle%2520the%2520vision-brain%250Aunderstanding%2520problem.%2520To%2520compute%2520the%2520connectivity%2520between%2520areas%2520in%2520brain%250Asignals%252C%2520we%2520introduce%2520a%2520new%2520Quantum-Inspired%2520Voxel-Controlling%2520module%2520to%2520learn%250Athe%2520impact%2520of%2520a%2520brain%2520voxel%2520on%2520others%2520represented%2520in%2520the%2520Hilbert%2520space.%2520To%250Aeffectively%2520learn%2520connectivity%252C%2520a%2520novel%2520Phase-Shifting%2520module%2520is%2520presented%2520to%250Acalibrate%2520the%2520value%2520of%2520the%2520brain%2520signals.%2520Finally%252C%2520we%2520introduce%2520a%2520new%250AMeasurement-like%2520Projection%2520module%2520to%2520present%2520the%2520connectivity%2520information%2520from%250Athe%2520Hilbert%2520space%2520into%2520the%2520feature%2520space.%2520The%2520proposed%2520approach%2520can%2520learn%2520to%250Afind%2520the%2520connectivities%2520between%2520fMRI%2520voxels%2520and%2520enhance%2520the%2520semantic%250Ainformation%2520obtained%2520from%2520human%2520perceptions.%2520Our%2520experimental%2520results%2520on%2520the%250ANatural%2520Scene%2520Dataset%2520benchmarks%2520illustrate%2520the%2520effectiveness%2520of%2520the%2520proposed%250Amethod%2520with%2520Top-1%2520accuracies%2520of%252095.1%2525%2520and%252095.6%2525%2520on%2520image%2520and%2520brain%2520retrieval%250Atasks%2520and%2520an%2520Inception%2520score%2520of%252095.3%2525%2520on%2520fMRI-to-image%2520reconstruction%2520task.%2520Our%250Aproposed%2520quantum-inspired%2520network%2520brings%2520a%2520potential%2520paradigm%2520to%2520solving%2520the%250Avision-brain%2520problems%2520via%2520the%2520quantum%2520computing%2520theory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13378v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum-Brain%3A%20Quantum-Inspired%20Neural%20Network%20Approach%20to%20Vision-Brain%0A%20%20Understanding&entry.906535625=Hoang-Quan%20Nguyen%20and%20Xuan-Bac%20Nguyen%20and%20Hugh%20Churchill%20and%20Arabinda%20Kumar%20Choudhary%20and%20Pawan%20Sinha%20and%20Samee%20U.%20Khan%20and%20Khoa%20Luu&entry.1292438233=%20%20Vision-brain%20understanding%20aims%20to%20extract%20semantic%20information%20about%20brain%0Asignals%20from%20human%20perceptions.%20Existing%20deep%20learning%20methods%20for%20vision-brain%0Aunderstanding%20are%20usually%20introduced%20in%20a%20traditional%20learning%20paradigm%20missing%0Athe%20ability%20to%20learn%20the%20connectivities%20between%20brain%20regions.%20Meanwhile%2C%20the%0Aquantum%20computing%20theory%20offers%20a%20new%20paradigm%20for%20designing%20deep%20learning%0Amodels.%20Motivated%20by%20the%20connectivities%20in%20the%20brain%20signals%20and%20the%0Aentanglement%20properties%20in%20quantum%20computing%2C%20we%20propose%20a%20novel%20Quantum-Brain%0Aapproach%2C%20a%20quantum-inspired%20neural%20network%2C%20to%20tackle%20the%20vision-brain%0Aunderstanding%20problem.%20To%20compute%20the%20connectivity%20between%20areas%20in%20brain%0Asignals%2C%20we%20introduce%20a%20new%20Quantum-Inspired%20Voxel-Controlling%20module%20to%20learn%0Athe%20impact%20of%20a%20brain%20voxel%20on%20others%20represented%20in%20the%20Hilbert%20space.%20To%0Aeffectively%20learn%20connectivity%2C%20a%20novel%20Phase-Shifting%20module%20is%20presented%20to%0Acalibrate%20the%20value%20of%20the%20brain%20signals.%20Finally%2C%20we%20introduce%20a%20new%0AMeasurement-like%20Projection%20module%20to%20present%20the%20connectivity%20information%20from%0Athe%20Hilbert%20space%20into%20the%20feature%20space.%20The%20proposed%20approach%20can%20learn%20to%0Afind%20the%20connectivities%20between%20fMRI%20voxels%20and%20enhance%20the%20semantic%0Ainformation%20obtained%20from%20human%20perceptions.%20Our%20experimental%20results%20on%20the%0ANatural%20Scene%20Dataset%20benchmarks%20illustrate%20the%20effectiveness%20of%20the%20proposed%0Amethod%20with%20Top-1%20accuracies%20of%2095.1%25%20and%2095.6%25%20on%20image%20and%20brain%20retrieval%0Atasks%20and%20an%20Inception%20score%20of%2095.3%25%20on%20fMRI-to-image%20reconstruction%20task.%20Our%0Aproposed%20quantum-inspired%20network%20brings%20a%20potential%20paradigm%20to%20solving%20the%0Avision-brain%20problems%20via%20the%20quantum%20computing%20theory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13378v2&entry.124074799=Read"},
{"title": "HumanSense: From Multimodal Perception to Empathetic Context-Aware\n  Responses through Reasoning MLLMs", "author": "Zheng Qin and Ruobing Zheng and Yabing Wang and Tianqi Li and Yi Yuan and Jingdong Chen and Le Wang", "abstract": "  While Multimodal Large Language Models (MLLMs) show immense promise for\nachieving truly human-like interactions, progress is hindered by the lack of\nfine-grained evaluation frameworks for human-centered scenarios, encompassing\nboth the understanding of complex human intentions and the provision of\nempathetic, context-aware responses. Here we introduce HumanSense, a\ncomprehensive benchmark designed to evaluate the human-centered perception and\ninteraction capabilities of MLLMs, with a particular focus on deep\nunderstanding of extended multimodal contexts and the formulation of rational\nfeedback. Our evaluation reveals that leading MLLMs still have considerable\nroom for improvement, particularly for advanced interaction-oriented tasks.\nSupplementing visual input with audio and text information yields substantial\nimprovements, and Omni-modal models show advantages on these tasks.\nFurthermore, we argue that appropriate feedback stems from a contextual\nanalysis of the interlocutor's needs and emotions, with reasoning ability\nserving as the key to unlocking it. Accordingly, we employ a multi-stage,\nmodality-progressive reinforcement learning to enhance the reasoning abilities\nof an Omni model, achieving substantial gains on evaluation results.\nAdditionally, we observe that successful reasoning processes exhibit highly\nconsistent thought patterns. By designing corresponding prompts, we also\nenhance the performance of non-reasoning models in a training-free manner.\nProject page:\n\\textcolor{brightpink}https://digital-avatar.github.io/ai/HumanSense/\n", "link": "http://arxiv.org/abs/2508.10576v1", "date": "2025-08-14", "relevancy": 2.3326, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5835}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5835}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanSense%3A%20From%20Multimodal%20Perception%20to%20Empathetic%20Context-Aware%0A%20%20Responses%20through%20Reasoning%20MLLMs&body=Title%3A%20HumanSense%3A%20From%20Multimodal%20Perception%20to%20Empathetic%20Context-Aware%0A%20%20Responses%20through%20Reasoning%20MLLMs%0AAuthor%3A%20Zheng%20Qin%20and%20Ruobing%20Zheng%20and%20Yabing%20Wang%20and%20Tianqi%20Li%20and%20Yi%20Yuan%20and%20Jingdong%20Chen%20and%20Le%20Wang%0AAbstract%3A%20%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20show%20immense%20promise%20for%0Aachieving%20truly%20human-like%20interactions%2C%20progress%20is%20hindered%20by%20the%20lack%20of%0Afine-grained%20evaluation%20frameworks%20for%20human-centered%20scenarios%2C%20encompassing%0Aboth%20the%20understanding%20of%20complex%20human%20intentions%20and%20the%20provision%20of%0Aempathetic%2C%20context-aware%20responses.%20Here%20we%20introduce%20HumanSense%2C%20a%0Acomprehensive%20benchmark%20designed%20to%20evaluate%20the%20human-centered%20perception%20and%0Ainteraction%20capabilities%20of%20MLLMs%2C%20with%20a%20particular%20focus%20on%20deep%0Aunderstanding%20of%20extended%20multimodal%20contexts%20and%20the%20formulation%20of%20rational%0Afeedback.%20Our%20evaluation%20reveals%20that%20leading%20MLLMs%20still%20have%20considerable%0Aroom%20for%20improvement%2C%20particularly%20for%20advanced%20interaction-oriented%20tasks.%0ASupplementing%20visual%20input%20with%20audio%20and%20text%20information%20yields%20substantial%0Aimprovements%2C%20and%20Omni-modal%20models%20show%20advantages%20on%20these%20tasks.%0AFurthermore%2C%20we%20argue%20that%20appropriate%20feedback%20stems%20from%20a%20contextual%0Aanalysis%20of%20the%20interlocutor%27s%20needs%20and%20emotions%2C%20with%20reasoning%20ability%0Aserving%20as%20the%20key%20to%20unlocking%20it.%20Accordingly%2C%20we%20employ%20a%20multi-stage%2C%0Amodality-progressive%20reinforcement%20learning%20to%20enhance%20the%20reasoning%20abilities%0Aof%20an%20Omni%20model%2C%20achieving%20substantial%20gains%20on%20evaluation%20results.%0AAdditionally%2C%20we%20observe%20that%20successful%20reasoning%20processes%20exhibit%20highly%0Aconsistent%20thought%20patterns.%20By%20designing%20corresponding%20prompts%2C%20we%20also%0Aenhance%20the%20performance%20of%20non-reasoning%20models%20in%20a%20training-free%20manner.%0AProject%20page%3A%0A%5Ctextcolor%7Bbrightpink%7Dhttps%3A//digital-avatar.github.io/ai/HumanSense/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanSense%253A%2520From%2520Multimodal%2520Perception%2520to%2520Empathetic%2520Context-Aware%250A%2520%2520Responses%2520through%2520Reasoning%2520MLLMs%26entry.906535625%3DZheng%2520Qin%2520and%2520Ruobing%2520Zheng%2520and%2520Yabing%2520Wang%2520and%2520Tianqi%2520Li%2520and%2520Yi%2520Yuan%2520and%2520Jingdong%2520Chen%2520and%2520Le%2520Wang%26entry.1292438233%3D%2520%2520While%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520show%2520immense%2520promise%2520for%250Aachieving%2520truly%2520human-like%2520interactions%252C%2520progress%2520is%2520hindered%2520by%2520the%2520lack%2520of%250Afine-grained%2520evaluation%2520frameworks%2520for%2520human-centered%2520scenarios%252C%2520encompassing%250Aboth%2520the%2520understanding%2520of%2520complex%2520human%2520intentions%2520and%2520the%2520provision%2520of%250Aempathetic%252C%2520context-aware%2520responses.%2520Here%2520we%2520introduce%2520HumanSense%252C%2520a%250Acomprehensive%2520benchmark%2520designed%2520to%2520evaluate%2520the%2520human-centered%2520perception%2520and%250Ainteraction%2520capabilities%2520of%2520MLLMs%252C%2520with%2520a%2520particular%2520focus%2520on%2520deep%250Aunderstanding%2520of%2520extended%2520multimodal%2520contexts%2520and%2520the%2520formulation%2520of%2520rational%250Afeedback.%2520Our%2520evaluation%2520reveals%2520that%2520leading%2520MLLMs%2520still%2520have%2520considerable%250Aroom%2520for%2520improvement%252C%2520particularly%2520for%2520advanced%2520interaction-oriented%2520tasks.%250ASupplementing%2520visual%2520input%2520with%2520audio%2520and%2520text%2520information%2520yields%2520substantial%250Aimprovements%252C%2520and%2520Omni-modal%2520models%2520show%2520advantages%2520on%2520these%2520tasks.%250AFurthermore%252C%2520we%2520argue%2520that%2520appropriate%2520feedback%2520stems%2520from%2520a%2520contextual%250Aanalysis%2520of%2520the%2520interlocutor%2527s%2520needs%2520and%2520emotions%252C%2520with%2520reasoning%2520ability%250Aserving%2520as%2520the%2520key%2520to%2520unlocking%2520it.%2520Accordingly%252C%2520we%2520employ%2520a%2520multi-stage%252C%250Amodality-progressive%2520reinforcement%2520learning%2520to%2520enhance%2520the%2520reasoning%2520abilities%250Aof%2520an%2520Omni%2520model%252C%2520achieving%2520substantial%2520gains%2520on%2520evaluation%2520results.%250AAdditionally%252C%2520we%2520observe%2520that%2520successful%2520reasoning%2520processes%2520exhibit%2520highly%250Aconsistent%2520thought%2520patterns.%2520By%2520designing%2520corresponding%2520prompts%252C%2520we%2520also%250Aenhance%2520the%2520performance%2520of%2520non-reasoning%2520models%2520in%2520a%2520training-free%2520manner.%250AProject%2520page%253A%250A%255Ctextcolor%257Bbrightpink%257Dhttps%253A//digital-avatar.github.io/ai/HumanSense/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanSense%3A%20From%20Multimodal%20Perception%20to%20Empathetic%20Context-Aware%0A%20%20Responses%20through%20Reasoning%20MLLMs&entry.906535625=Zheng%20Qin%20and%20Ruobing%20Zheng%20and%20Yabing%20Wang%20and%20Tianqi%20Li%20and%20Yi%20Yuan%20and%20Jingdong%20Chen%20and%20Le%20Wang&entry.1292438233=%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20show%20immense%20promise%20for%0Aachieving%20truly%20human-like%20interactions%2C%20progress%20is%20hindered%20by%20the%20lack%20of%0Afine-grained%20evaluation%20frameworks%20for%20human-centered%20scenarios%2C%20encompassing%0Aboth%20the%20understanding%20of%20complex%20human%20intentions%20and%20the%20provision%20of%0Aempathetic%2C%20context-aware%20responses.%20Here%20we%20introduce%20HumanSense%2C%20a%0Acomprehensive%20benchmark%20designed%20to%20evaluate%20the%20human-centered%20perception%20and%0Ainteraction%20capabilities%20of%20MLLMs%2C%20with%20a%20particular%20focus%20on%20deep%0Aunderstanding%20of%20extended%20multimodal%20contexts%20and%20the%20formulation%20of%20rational%0Afeedback.%20Our%20evaluation%20reveals%20that%20leading%20MLLMs%20still%20have%20considerable%0Aroom%20for%20improvement%2C%20particularly%20for%20advanced%20interaction-oriented%20tasks.%0ASupplementing%20visual%20input%20with%20audio%20and%20text%20information%20yields%20substantial%0Aimprovements%2C%20and%20Omni-modal%20models%20show%20advantages%20on%20these%20tasks.%0AFurthermore%2C%20we%20argue%20that%20appropriate%20feedback%20stems%20from%20a%20contextual%0Aanalysis%20of%20the%20interlocutor%27s%20needs%20and%20emotions%2C%20with%20reasoning%20ability%0Aserving%20as%20the%20key%20to%20unlocking%20it.%20Accordingly%2C%20we%20employ%20a%20multi-stage%2C%0Amodality-progressive%20reinforcement%20learning%20to%20enhance%20the%20reasoning%20abilities%0Aof%20an%20Omni%20model%2C%20achieving%20substantial%20gains%20on%20evaluation%20results.%0AAdditionally%2C%20we%20observe%20that%20successful%20reasoning%20processes%20exhibit%20highly%0Aconsistent%20thought%20patterns.%20By%20designing%20corresponding%20prompts%2C%20we%20also%0Aenhance%20the%20performance%20of%20non-reasoning%20models%20in%20a%20training-free%20manner.%0AProject%20page%3A%0A%5Ctextcolor%7Bbrightpink%7Dhttps%3A//digital-avatar.github.io/ai/HumanSense/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10576v1&entry.124074799=Read"},
{"title": "Processing and acquisition traces in visual encoders: What does CLIP\n  know about your camera?", "author": "Ryan Ramos and Vladan Stojni\u0107 and Giorgos Kordopatis-Zilos and Yuta Nakashima and Giorgos Tolias and Noa Garcia", "abstract": "  Prior work has analyzed the robustness of visual encoders to image\ntransformations and corruptions, particularly in cases where such alterations\nare not seen during training. When this occurs, they introduce a form of\ndistribution shift at test time, often leading to performance degradation. The\nprimary focus has been on severe corruptions that, when applied aggressively,\ndistort useful signals necessary for accurate semantic predictions.\n  We take a different perspective by analyzing parameters of the image\nacquisition process and transformations that may be subtle or even\nimperceptible to the human eye. We find that such parameters are systematically\nencoded in the learned visual representations and can be easily recovered. More\nstrikingly, their presence can have a profound impact, either positively or\nnegatively, on semantic predictions. This effect depends on whether there is a\nstrong correlation or anti-correlation between semantic labels and these\nacquisition-based or processing-based labels. Our code and data are available\nat: https://github.com/ryan-caesar-ramos/visual-encoder-traces\n", "link": "http://arxiv.org/abs/2508.10637v1", "date": "2025-08-14", "relevancy": 2.3244, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5864}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Processing%20and%20acquisition%20traces%20in%20visual%20encoders%3A%20What%20does%20CLIP%0A%20%20know%20about%20your%20camera%3F&body=Title%3A%20Processing%20and%20acquisition%20traces%20in%20visual%20encoders%3A%20What%20does%20CLIP%0A%20%20know%20about%20your%20camera%3F%0AAuthor%3A%20Ryan%20Ramos%20and%20Vladan%20Stojni%C4%87%20and%20Giorgos%20Kordopatis-Zilos%20and%20Yuta%20Nakashima%20and%20Giorgos%20Tolias%20and%20Noa%20Garcia%0AAbstract%3A%20%20%20Prior%20work%20has%20analyzed%20the%20robustness%20of%20visual%20encoders%20to%20image%0Atransformations%20and%20corruptions%2C%20particularly%20in%20cases%20where%20such%20alterations%0Aare%20not%20seen%20during%20training.%20When%20this%20occurs%2C%20they%20introduce%20a%20form%20of%0Adistribution%20shift%20at%20test%20time%2C%20often%20leading%20to%20performance%20degradation.%20The%0Aprimary%20focus%20has%20been%20on%20severe%20corruptions%20that%2C%20when%20applied%20aggressively%2C%0Adistort%20useful%20signals%20necessary%20for%20accurate%20semantic%20predictions.%0A%20%20We%20take%20a%20different%20perspective%20by%20analyzing%20parameters%20of%20the%20image%0Aacquisition%20process%20and%20transformations%20that%20may%20be%20subtle%20or%20even%0Aimperceptible%20to%20the%20human%20eye.%20We%20find%20that%20such%20parameters%20are%20systematically%0Aencoded%20in%20the%20learned%20visual%20representations%20and%20can%20be%20easily%20recovered.%20More%0Astrikingly%2C%20their%20presence%20can%20have%20a%20profound%20impact%2C%20either%20positively%20or%0Anegatively%2C%20on%20semantic%20predictions.%20This%20effect%20depends%20on%20whether%20there%20is%20a%0Astrong%20correlation%20or%20anti-correlation%20between%20semantic%20labels%20and%20these%0Aacquisition-based%20or%20processing-based%20labels.%20Our%20code%20and%20data%20are%20available%0Aat%3A%20https%3A//github.com/ryan-caesar-ramos/visual-encoder-traces%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProcessing%2520and%2520acquisition%2520traces%2520in%2520visual%2520encoders%253A%2520What%2520does%2520CLIP%250A%2520%2520know%2520about%2520your%2520camera%253F%26entry.906535625%3DRyan%2520Ramos%2520and%2520Vladan%2520Stojni%25C4%2587%2520and%2520Giorgos%2520Kordopatis-Zilos%2520and%2520Yuta%2520Nakashima%2520and%2520Giorgos%2520Tolias%2520and%2520Noa%2520Garcia%26entry.1292438233%3D%2520%2520Prior%2520work%2520has%2520analyzed%2520the%2520robustness%2520of%2520visual%2520encoders%2520to%2520image%250Atransformations%2520and%2520corruptions%252C%2520particularly%2520in%2520cases%2520where%2520such%2520alterations%250Aare%2520not%2520seen%2520during%2520training.%2520When%2520this%2520occurs%252C%2520they%2520introduce%2520a%2520form%2520of%250Adistribution%2520shift%2520at%2520test%2520time%252C%2520often%2520leading%2520to%2520performance%2520degradation.%2520The%250Aprimary%2520focus%2520has%2520been%2520on%2520severe%2520corruptions%2520that%252C%2520when%2520applied%2520aggressively%252C%250Adistort%2520useful%2520signals%2520necessary%2520for%2520accurate%2520semantic%2520predictions.%250A%2520%2520We%2520take%2520a%2520different%2520perspective%2520by%2520analyzing%2520parameters%2520of%2520the%2520image%250Aacquisition%2520process%2520and%2520transformations%2520that%2520may%2520be%2520subtle%2520or%2520even%250Aimperceptible%2520to%2520the%2520human%2520eye.%2520We%2520find%2520that%2520such%2520parameters%2520are%2520systematically%250Aencoded%2520in%2520the%2520learned%2520visual%2520representations%2520and%2520can%2520be%2520easily%2520recovered.%2520More%250Astrikingly%252C%2520their%2520presence%2520can%2520have%2520a%2520profound%2520impact%252C%2520either%2520positively%2520or%250Anegatively%252C%2520on%2520semantic%2520predictions.%2520This%2520effect%2520depends%2520on%2520whether%2520there%2520is%2520a%250Astrong%2520correlation%2520or%2520anti-correlation%2520between%2520semantic%2520labels%2520and%2520these%250Aacquisition-based%2520or%2520processing-based%2520labels.%2520Our%2520code%2520and%2520data%2520are%2520available%250Aat%253A%2520https%253A//github.com/ryan-caesar-ramos/visual-encoder-traces%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Processing%20and%20acquisition%20traces%20in%20visual%20encoders%3A%20What%20does%20CLIP%0A%20%20know%20about%20your%20camera%3F&entry.906535625=Ryan%20Ramos%20and%20Vladan%20Stojni%C4%87%20and%20Giorgos%20Kordopatis-Zilos%20and%20Yuta%20Nakashima%20and%20Giorgos%20Tolias%20and%20Noa%20Garcia&entry.1292438233=%20%20Prior%20work%20has%20analyzed%20the%20robustness%20of%20visual%20encoders%20to%20image%0Atransformations%20and%20corruptions%2C%20particularly%20in%20cases%20where%20such%20alterations%0Aare%20not%20seen%20during%20training.%20When%20this%20occurs%2C%20they%20introduce%20a%20form%20of%0Adistribution%20shift%20at%20test%20time%2C%20often%20leading%20to%20performance%20degradation.%20The%0Aprimary%20focus%20has%20been%20on%20severe%20corruptions%20that%2C%20when%20applied%20aggressively%2C%0Adistort%20useful%20signals%20necessary%20for%20accurate%20semantic%20predictions.%0A%20%20We%20take%20a%20different%20perspective%20by%20analyzing%20parameters%20of%20the%20image%0Aacquisition%20process%20and%20transformations%20that%20may%20be%20subtle%20or%20even%0Aimperceptible%20to%20the%20human%20eye.%20We%20find%20that%20such%20parameters%20are%20systematically%0Aencoded%20in%20the%20learned%20visual%20representations%20and%20can%20be%20easily%20recovered.%20More%0Astrikingly%2C%20their%20presence%20can%20have%20a%20profound%20impact%2C%20either%20positively%20or%0Anegatively%2C%20on%20semantic%20predictions.%20This%20effect%20depends%20on%20whether%20there%20is%20a%0Astrong%20correlation%20or%20anti-correlation%20between%20semantic%20labels%20and%20these%0Aacquisition-based%20or%20processing-based%20labels.%20Our%20code%20and%20data%20are%20available%0Aat%3A%20https%3A//github.com/ryan-caesar-ramos/visual-encoder-traces%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10637v1&entry.124074799=Read"},
{"title": "Sample-efficient LLM Optimization with Reset Replay", "author": "Zichuan Liu and Jinyu Wang and Lei Song and Jiang Bian", "abstract": "  Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data.\n", "link": "http://arxiv.org/abs/2508.06412v2", "date": "2025-08-14", "relevancy": 2.3117, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4669}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4602}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample-efficient%20LLM%20Optimization%20with%20Reset%20Replay&body=Title%3A%20Sample-efficient%20LLM%20Optimization%20with%20Reset%20Replay%0AAuthor%3A%20Zichuan%20Liu%20and%20Jinyu%20Wang%20and%20Lei%20Song%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20Recent%20advancements%20in%20post-training%20Large%20Language%20Models%20%28LLMs%29%2C%0Aparticularly%20through%20Reinforcement%20Learning%20%28RL%29%20and%20preference%20optimization%0Amethods%2C%20are%20key%20drivers%20for%20enhancing%20their%20reasoning%20capabilities.%20However%2C%0Athese%20methods%20are%20often%20plagued%20by%20low%20sample%20efficiency%20and%20a%20susceptibility%0Ato%20primacy%20bias%2C%20where%20overfitting%20to%20initial%20experiences%20degrades%20policy%0Aquality%20and%20damages%20the%20learning%20process.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20LLM%20optimization%20with%20Reset%20Replay%20%28LoRR%29%2C%20a%20general%20and%20powerful%0Aplugin%20designed%20to%20enhance%20sample%20efficiency%20in%20any%20preference-based%0Aoptimization%20framework.%20LoRR%20core%20mechanism%20enables%20training%20at%20a%20high%20replay%0Anumber%2C%20maximizing%20the%20utility%20of%20each%20collected%20data%20batch.%20To%20counteract%20the%0Arisk%20of%20overfitting%20inherent%20in%20high-replay%20training%2C%20LoRR%20incorporates%20a%0Aperiodic%20reset%20strategy%20with%20reusing%20initial%20data%2C%20which%20preserves%20network%0Aplasticity.%20Furthermore%2C%20it%20leverages%20a%20hybrid%20optimization%20objective%2C%0Acombining%20supervised%20fine-tuning%20%28SFT%29%20and%20preference-based%20losses%20to%20further%0Abolster%20data%20exploitation.%20Our%20extensive%20experiments%20demonstrate%20that%20LoRR%0Asignificantly%20boosts%20the%20performance%20of%20various%20preference%20optimization%20methods%0Aon%20both%20mathematical%20and%20general%20reasoning%20benchmarks.%20Notably%2C%20an%20iterative%0ADPO%20approach%20augmented%20with%20LoRR%20achieves%20comparable%20performance%20on%20challenging%0Amath%20tasks%2C%20outperforming%20some%20complex%20and%20computationally%20intensive%20RL-based%0Aalgorithms.%20These%20findings%20highlight%20that%20LoRR%20offers%20a%20practical%2C%0Asample-efficient%2C%20and%20highly%20effective%20paradigm%20for%20LLM%20finetuning%2C%20unlocking%0Agreater%20performance%20from%20limited%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06412v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample-efficient%2520LLM%2520Optimization%2520with%2520Reset%2520Replay%26entry.906535625%3DZichuan%2520Liu%2520and%2520Jinyu%2520Wang%2520and%2520Lei%2520Song%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520post-training%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Aparticularly%2520through%2520Reinforcement%2520Learning%2520%2528RL%2529%2520and%2520preference%2520optimization%250Amethods%252C%2520are%2520key%2520drivers%2520for%2520enhancing%2520their%2520reasoning%2520capabilities.%2520However%252C%250Athese%2520methods%2520are%2520often%2520plagued%2520by%2520low%2520sample%2520efficiency%2520and%2520a%2520susceptibility%250Ato%2520primacy%2520bias%252C%2520where%2520overfitting%2520to%2520initial%2520experiences%2520degrades%2520policy%250Aquality%2520and%2520damages%2520the%2520learning%2520process.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520LLM%2520optimization%2520with%2520Reset%2520Replay%2520%2528LoRR%2529%252C%2520a%2520general%2520and%2520powerful%250Aplugin%2520designed%2520to%2520enhance%2520sample%2520efficiency%2520in%2520any%2520preference-based%250Aoptimization%2520framework.%2520LoRR%2520core%2520mechanism%2520enables%2520training%2520at%2520a%2520high%2520replay%250Anumber%252C%2520maximizing%2520the%2520utility%2520of%2520each%2520collected%2520data%2520batch.%2520To%2520counteract%2520the%250Arisk%2520of%2520overfitting%2520inherent%2520in%2520high-replay%2520training%252C%2520LoRR%2520incorporates%2520a%250Aperiodic%2520reset%2520strategy%2520with%2520reusing%2520initial%2520data%252C%2520which%2520preserves%2520network%250Aplasticity.%2520Furthermore%252C%2520it%2520leverages%2520a%2520hybrid%2520optimization%2520objective%252C%250Acombining%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520preference-based%2520losses%2520to%2520further%250Abolster%2520data%2520exploitation.%2520Our%2520extensive%2520experiments%2520demonstrate%2520that%2520LoRR%250Asignificantly%2520boosts%2520the%2520performance%2520of%2520various%2520preference%2520optimization%2520methods%250Aon%2520both%2520mathematical%2520and%2520general%2520reasoning%2520benchmarks.%2520Notably%252C%2520an%2520iterative%250ADPO%2520approach%2520augmented%2520with%2520LoRR%2520achieves%2520comparable%2520performance%2520on%2520challenging%250Amath%2520tasks%252C%2520outperforming%2520some%2520complex%2520and%2520computationally%2520intensive%2520RL-based%250Aalgorithms.%2520These%2520findings%2520highlight%2520that%2520LoRR%2520offers%2520a%2520practical%252C%250Asample-efficient%252C%2520and%2520highly%2520effective%2520paradigm%2520for%2520LLM%2520finetuning%252C%2520unlocking%250Agreater%2520performance%2520from%2520limited%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06412v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample-efficient%20LLM%20Optimization%20with%20Reset%20Replay&entry.906535625=Zichuan%20Liu%20and%20Jinyu%20Wang%20and%20Lei%20Song%20and%20Jiang%20Bian&entry.1292438233=%20%20Recent%20advancements%20in%20post-training%20Large%20Language%20Models%20%28LLMs%29%2C%0Aparticularly%20through%20Reinforcement%20Learning%20%28RL%29%20and%20preference%20optimization%0Amethods%2C%20are%20key%20drivers%20for%20enhancing%20their%20reasoning%20capabilities.%20However%2C%0Athese%20methods%20are%20often%20plagued%20by%20low%20sample%20efficiency%20and%20a%20susceptibility%0Ato%20primacy%20bias%2C%20where%20overfitting%20to%20initial%20experiences%20degrades%20policy%0Aquality%20and%20damages%20the%20learning%20process.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20LLM%20optimization%20with%20Reset%20Replay%20%28LoRR%29%2C%20a%20general%20and%20powerful%0Aplugin%20designed%20to%20enhance%20sample%20efficiency%20in%20any%20preference-based%0Aoptimization%20framework.%20LoRR%20core%20mechanism%20enables%20training%20at%20a%20high%20replay%0Anumber%2C%20maximizing%20the%20utility%20of%20each%20collected%20data%20batch.%20To%20counteract%20the%0Arisk%20of%20overfitting%20inherent%20in%20high-replay%20training%2C%20LoRR%20incorporates%20a%0Aperiodic%20reset%20strategy%20with%20reusing%20initial%20data%2C%20which%20preserves%20network%0Aplasticity.%20Furthermore%2C%20it%20leverages%20a%20hybrid%20optimization%20objective%2C%0Acombining%20supervised%20fine-tuning%20%28SFT%29%20and%20preference-based%20losses%20to%20further%0Abolster%20data%20exploitation.%20Our%20extensive%20experiments%20demonstrate%20that%20LoRR%0Asignificantly%20boosts%20the%20performance%20of%20various%20preference%20optimization%20methods%0Aon%20both%20mathematical%20and%20general%20reasoning%20benchmarks.%20Notably%2C%20an%20iterative%0ADPO%20approach%20augmented%20with%20LoRR%20achieves%20comparable%20performance%20on%20challenging%0Amath%20tasks%2C%20outperforming%20some%20complex%20and%20computationally%20intensive%20RL-based%0Aalgorithms.%20These%20findings%20highlight%20that%20LoRR%20offers%20a%20practical%2C%0Asample-efficient%2C%20and%20highly%20effective%20paradigm%20for%20LLM%20finetuning%2C%20unlocking%0Agreater%20performance%20from%20limited%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06412v2&entry.124074799=Read"},
{"title": "DIVA-VQA: Detecting Inter-frame Variations in UGC Video Quality", "author": "Xinyi Wang and Angeliki Katsenou and David Bull", "abstract": "  The rapid growth of user-generated (video) content (UGC) has driven increased\ndemand for research on no-reference (NR) perceptual video quality assessment\n(VQA). NR-VQA is a key component for large-scale video quality monitoring in\nsocial media and streaming applications where a pristine reference is not\navailable. This paper proposes a novel NR-VQA model based on spatio-temporal\nfragmentation driven by inter-frame variations. By leveraging these inter-frame\ndifferences, the model progressively analyses quality-sensitive regions at\nmultiple levels: frames, patches, and fragmented frames. It integrates frames,\nfragmented residuals, and fragmented frames aligned with residuals to\neffectively capture global and local information. The model extracts both 2D\nand 3D features in order to characterize these spatio-temporal variations.\nExperiments conducted on five UGC datasets and against state-of-the-art models\nranked our proposed method among the top 2 in terms of average rank correlation\n(DIVA-VQA-L: 0.898 and DIVA-VQA-B: 0.886). The improved performance is offered\nat a low runtime complexity, with DIVA-VQA-B ranked top and DIVA-VQA-L third on\naverage compared to the fastest existing NR-VQA method. Code and models are\npublicly available at: https://github.com/xinyiW915/DIVA-VQA.\n", "link": "http://arxiv.org/abs/2508.10605v1", "date": "2025-08-14", "relevancy": 2.3092, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5941}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5707}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIVA-VQA%3A%20Detecting%20Inter-frame%20Variations%20in%20UGC%20Video%20Quality&body=Title%3A%20DIVA-VQA%3A%20Detecting%20Inter-frame%20Variations%20in%20UGC%20Video%20Quality%0AAuthor%3A%20Xinyi%20Wang%20and%20Angeliki%20Katsenou%20and%20David%20Bull%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20user-generated%20%28video%29%20content%20%28UGC%29%20has%20driven%20increased%0Ademand%20for%20research%20on%20no-reference%20%28NR%29%20perceptual%20video%20quality%20assessment%0A%28VQA%29.%20NR-VQA%20is%20a%20key%20component%20for%20large-scale%20video%20quality%20monitoring%20in%0Asocial%20media%20and%20streaming%20applications%20where%20a%20pristine%20reference%20is%20not%0Aavailable.%20This%20paper%20proposes%20a%20novel%20NR-VQA%20model%20based%20on%20spatio-temporal%0Afragmentation%20driven%20by%20inter-frame%20variations.%20By%20leveraging%20these%20inter-frame%0Adifferences%2C%20the%20model%20progressively%20analyses%20quality-sensitive%20regions%20at%0Amultiple%20levels%3A%20frames%2C%20patches%2C%20and%20fragmented%20frames.%20It%20integrates%20frames%2C%0Afragmented%20residuals%2C%20and%20fragmented%20frames%20aligned%20with%20residuals%20to%0Aeffectively%20capture%20global%20and%20local%20information.%20The%20model%20extracts%20both%202D%0Aand%203D%20features%20in%20order%20to%20characterize%20these%20spatio-temporal%20variations.%0AExperiments%20conducted%20on%20five%20UGC%20datasets%20and%20against%20state-of-the-art%20models%0Aranked%20our%20proposed%20method%20among%20the%20top%202%20in%20terms%20of%20average%20rank%20correlation%0A%28DIVA-VQA-L%3A%200.898%20and%20DIVA-VQA-B%3A%200.886%29.%20The%20improved%20performance%20is%20offered%0Aat%20a%20low%20runtime%20complexity%2C%20with%20DIVA-VQA-B%20ranked%20top%20and%20DIVA-VQA-L%20third%20on%0Aaverage%20compared%20to%20the%20fastest%20existing%20NR-VQA%20method.%20Code%20and%20models%20are%0Apublicly%20available%20at%3A%20https%3A//github.com/xinyiW915/DIVA-VQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIVA-VQA%253A%2520Detecting%2520Inter-frame%2520Variations%2520in%2520UGC%2520Video%2520Quality%26entry.906535625%3DXinyi%2520Wang%2520and%2520Angeliki%2520Katsenou%2520and%2520David%2520Bull%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520user-generated%2520%2528video%2529%2520content%2520%2528UGC%2529%2520has%2520driven%2520increased%250Ademand%2520for%2520research%2520on%2520no-reference%2520%2528NR%2529%2520perceptual%2520video%2520quality%2520assessment%250A%2528VQA%2529.%2520NR-VQA%2520is%2520a%2520key%2520component%2520for%2520large-scale%2520video%2520quality%2520monitoring%2520in%250Asocial%2520media%2520and%2520streaming%2520applications%2520where%2520a%2520pristine%2520reference%2520is%2520not%250Aavailable.%2520This%2520paper%2520proposes%2520a%2520novel%2520NR-VQA%2520model%2520based%2520on%2520spatio-temporal%250Afragmentation%2520driven%2520by%2520inter-frame%2520variations.%2520By%2520leveraging%2520these%2520inter-frame%250Adifferences%252C%2520the%2520model%2520progressively%2520analyses%2520quality-sensitive%2520regions%2520at%250Amultiple%2520levels%253A%2520frames%252C%2520patches%252C%2520and%2520fragmented%2520frames.%2520It%2520integrates%2520frames%252C%250Afragmented%2520residuals%252C%2520and%2520fragmented%2520frames%2520aligned%2520with%2520residuals%2520to%250Aeffectively%2520capture%2520global%2520and%2520local%2520information.%2520The%2520model%2520extracts%2520both%25202D%250Aand%25203D%2520features%2520in%2520order%2520to%2520characterize%2520these%2520spatio-temporal%2520variations.%250AExperiments%2520conducted%2520on%2520five%2520UGC%2520datasets%2520and%2520against%2520state-of-the-art%2520models%250Aranked%2520our%2520proposed%2520method%2520among%2520the%2520top%25202%2520in%2520terms%2520of%2520average%2520rank%2520correlation%250A%2528DIVA-VQA-L%253A%25200.898%2520and%2520DIVA-VQA-B%253A%25200.886%2529.%2520The%2520improved%2520performance%2520is%2520offered%250Aat%2520a%2520low%2520runtime%2520complexity%252C%2520with%2520DIVA-VQA-B%2520ranked%2520top%2520and%2520DIVA-VQA-L%2520third%2520on%250Aaverage%2520compared%2520to%2520the%2520fastest%2520existing%2520NR-VQA%2520method.%2520Code%2520and%2520models%2520are%250Apublicly%2520available%2520at%253A%2520https%253A//github.com/xinyiW915/DIVA-VQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIVA-VQA%3A%20Detecting%20Inter-frame%20Variations%20in%20UGC%20Video%20Quality&entry.906535625=Xinyi%20Wang%20and%20Angeliki%20Katsenou%20and%20David%20Bull&entry.1292438233=%20%20The%20rapid%20growth%20of%20user-generated%20%28video%29%20content%20%28UGC%29%20has%20driven%20increased%0Ademand%20for%20research%20on%20no-reference%20%28NR%29%20perceptual%20video%20quality%20assessment%0A%28VQA%29.%20NR-VQA%20is%20a%20key%20component%20for%20large-scale%20video%20quality%20monitoring%20in%0Asocial%20media%20and%20streaming%20applications%20where%20a%20pristine%20reference%20is%20not%0Aavailable.%20This%20paper%20proposes%20a%20novel%20NR-VQA%20model%20based%20on%20spatio-temporal%0Afragmentation%20driven%20by%20inter-frame%20variations.%20By%20leveraging%20these%20inter-frame%0Adifferences%2C%20the%20model%20progressively%20analyses%20quality-sensitive%20regions%20at%0Amultiple%20levels%3A%20frames%2C%20patches%2C%20and%20fragmented%20frames.%20It%20integrates%20frames%2C%0Afragmented%20residuals%2C%20and%20fragmented%20frames%20aligned%20with%20residuals%20to%0Aeffectively%20capture%20global%20and%20local%20information.%20The%20model%20extracts%20both%202D%0Aand%203D%20features%20in%20order%20to%20characterize%20these%20spatio-temporal%20variations.%0AExperiments%20conducted%20on%20five%20UGC%20datasets%20and%20against%20state-of-the-art%20models%0Aranked%20our%20proposed%20method%20among%20the%20top%202%20in%20terms%20of%20average%20rank%20correlation%0A%28DIVA-VQA-L%3A%200.898%20and%20DIVA-VQA-B%3A%200.886%29.%20The%20improved%20performance%20is%20offered%0Aat%20a%20low%20runtime%20complexity%2C%20with%20DIVA-VQA-B%20ranked%20top%20and%20DIVA-VQA-L%20third%20on%0Aaverage%20compared%20to%20the%20fastest%20existing%20NR-VQA%20method.%20Code%20and%20models%20are%0Apublicly%20available%20at%3A%20https%3A//github.com/xinyiW915/DIVA-VQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10605v1&entry.124074799=Read"},
{"title": "IADGPT: Unified LVLM for Few-Shot Industrial Anomaly Detection,\n  Localization, and Reasoning via In-Context Learning", "author": "Mengyang Zhao and Teng Fu and Haiyang Yu and Ke Niu and Bin Li", "abstract": "  Few-Shot Industrial Anomaly Detection (FS-IAD) has important applications in\nautomating industrial quality inspection. Recently, some FS-IAD methods based\non Large Vision-Language Models (LVLMs) have been proposed with some\nachievements through prompt learning or fine-tuning. However, existing LVLMs\nfocus on general tasks but lack basic industrial knowledge and reasoning\ncapabilities related to FS-IAD, making these methods far from specialized human\nquality inspectors. To address these challenges, we propose a unified\nframework, IADGPT, designed to perform FS-IAD in a human-like manner, while\nalso handling associated localization and reasoning tasks, even for diverse and\nnovel industrial products. To this end, we introduce a three-stage progressive\ntraining strategy inspired by humans. Specifically, the first two stages\ngradually guide IADGPT in acquiring fundamental industrial knowledge and\ndiscrepancy awareness. In the third stage, we design an in-context\nlearning-based training paradigm, enabling IADGPT to leverage a few-shot image\nas the exemplars for improved generalization to novel products. In addition, we\ndesign a strategy that enables IADGPT to output image-level and pixel-level\nanomaly scores using the logits output and the attention map, respectively, in\nconjunction with the language output to accomplish anomaly reasoning. To\nsupport our training, we present a new dataset comprising 100K images across\n400 diverse industrial product categories with extensive attribute-level\ntextual annotations. Experiments indicate IADGPT achieves considerable\nperformance gains in anomaly detection and demonstrates competitiveness in\nanomaly localization and reasoning. We will release our dataset in\ncamera-ready.\n", "link": "http://arxiv.org/abs/2508.10681v1", "date": "2025-08-14", "relevancy": 2.2936, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5801}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5721}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IADGPT%3A%20Unified%20LVLM%20for%20Few-Shot%20Industrial%20Anomaly%20Detection%2C%0A%20%20Localization%2C%20and%20Reasoning%20via%20In-Context%20Learning&body=Title%3A%20IADGPT%3A%20Unified%20LVLM%20for%20Few-Shot%20Industrial%20Anomaly%20Detection%2C%0A%20%20Localization%2C%20and%20Reasoning%20via%20In-Context%20Learning%0AAuthor%3A%20Mengyang%20Zhao%20and%20Teng%20Fu%20and%20Haiyang%20Yu%20and%20Ke%20Niu%20and%20Bin%20Li%0AAbstract%3A%20%20%20Few-Shot%20Industrial%20Anomaly%20Detection%20%28FS-IAD%29%20has%20important%20applications%20in%0Aautomating%20industrial%20quality%20inspection.%20Recently%2C%20some%20FS-IAD%20methods%20based%0Aon%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20been%20proposed%20with%20some%0Aachievements%20through%20prompt%20learning%20or%20fine-tuning.%20However%2C%20existing%20LVLMs%0Afocus%20on%20general%20tasks%20but%20lack%20basic%20industrial%20knowledge%20and%20reasoning%0Acapabilities%20related%20to%20FS-IAD%2C%20making%20these%20methods%20far%20from%20specialized%20human%0Aquality%20inspectors.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20unified%0Aframework%2C%20IADGPT%2C%20designed%20to%20perform%20FS-IAD%20in%20a%20human-like%20manner%2C%20while%0Aalso%20handling%20associated%20localization%20and%20reasoning%20tasks%2C%20even%20for%20diverse%20and%0Anovel%20industrial%20products.%20To%20this%20end%2C%20we%20introduce%20a%20three-stage%20progressive%0Atraining%20strategy%20inspired%20by%20humans.%20Specifically%2C%20the%20first%20two%20stages%0Agradually%20guide%20IADGPT%20in%20acquiring%20fundamental%20industrial%20knowledge%20and%0Adiscrepancy%20awareness.%20In%20the%20third%20stage%2C%20we%20design%20an%20in-context%0Alearning-based%20training%20paradigm%2C%20enabling%20IADGPT%20to%20leverage%20a%20few-shot%20image%0Aas%20the%20exemplars%20for%20improved%20generalization%20to%20novel%20products.%20In%20addition%2C%20we%0Adesign%20a%20strategy%20that%20enables%20IADGPT%20to%20output%20image-level%20and%20pixel-level%0Aanomaly%20scores%20using%20the%20logits%20output%20and%20the%20attention%20map%2C%20respectively%2C%20in%0Aconjunction%20with%20the%20language%20output%20to%20accomplish%20anomaly%20reasoning.%20To%0Asupport%20our%20training%2C%20we%20present%20a%20new%20dataset%20comprising%20100K%20images%20across%0A400%20diverse%20industrial%20product%20categories%20with%20extensive%20attribute-level%0Atextual%20annotations.%20Experiments%20indicate%20IADGPT%20achieves%20considerable%0Aperformance%20gains%20in%20anomaly%20detection%20and%20demonstrates%20competitiveness%20in%0Aanomaly%20localization%20and%20reasoning.%20We%20will%20release%20our%20dataset%20in%0Acamera-ready.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIADGPT%253A%2520Unified%2520LVLM%2520for%2520Few-Shot%2520Industrial%2520Anomaly%2520Detection%252C%250A%2520%2520Localization%252C%2520and%2520Reasoning%2520via%2520In-Context%2520Learning%26entry.906535625%3DMengyang%2520Zhao%2520and%2520Teng%2520Fu%2520and%2520Haiyang%2520Yu%2520and%2520Ke%2520Niu%2520and%2520Bin%2520Li%26entry.1292438233%3D%2520%2520Few-Shot%2520Industrial%2520Anomaly%2520Detection%2520%2528FS-IAD%2529%2520has%2520important%2520applications%2520in%250Aautomating%2520industrial%2520quality%2520inspection.%2520Recently%252C%2520some%2520FS-IAD%2520methods%2520based%250Aon%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520been%2520proposed%2520with%2520some%250Aachievements%2520through%2520prompt%2520learning%2520or%2520fine-tuning.%2520However%252C%2520existing%2520LVLMs%250Afocus%2520on%2520general%2520tasks%2520but%2520lack%2520basic%2520industrial%2520knowledge%2520and%2520reasoning%250Acapabilities%2520related%2520to%2520FS-IAD%252C%2520making%2520these%2520methods%2520far%2520from%2520specialized%2520human%250Aquality%2520inspectors.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520unified%250Aframework%252C%2520IADGPT%252C%2520designed%2520to%2520perform%2520FS-IAD%2520in%2520a%2520human-like%2520manner%252C%2520while%250Aalso%2520handling%2520associated%2520localization%2520and%2520reasoning%2520tasks%252C%2520even%2520for%2520diverse%2520and%250Anovel%2520industrial%2520products.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520three-stage%2520progressive%250Atraining%2520strategy%2520inspired%2520by%2520humans.%2520Specifically%252C%2520the%2520first%2520two%2520stages%250Agradually%2520guide%2520IADGPT%2520in%2520acquiring%2520fundamental%2520industrial%2520knowledge%2520and%250Adiscrepancy%2520awareness.%2520In%2520the%2520third%2520stage%252C%2520we%2520design%2520an%2520in-context%250Alearning-based%2520training%2520paradigm%252C%2520enabling%2520IADGPT%2520to%2520leverage%2520a%2520few-shot%2520image%250Aas%2520the%2520exemplars%2520for%2520improved%2520generalization%2520to%2520novel%2520products.%2520In%2520addition%252C%2520we%250Adesign%2520a%2520strategy%2520that%2520enables%2520IADGPT%2520to%2520output%2520image-level%2520and%2520pixel-level%250Aanomaly%2520scores%2520using%2520the%2520logits%2520output%2520and%2520the%2520attention%2520map%252C%2520respectively%252C%2520in%250Aconjunction%2520with%2520the%2520language%2520output%2520to%2520accomplish%2520anomaly%2520reasoning.%2520To%250Asupport%2520our%2520training%252C%2520we%2520present%2520a%2520new%2520dataset%2520comprising%2520100K%2520images%2520across%250A400%2520diverse%2520industrial%2520product%2520categories%2520with%2520extensive%2520attribute-level%250Atextual%2520annotations.%2520Experiments%2520indicate%2520IADGPT%2520achieves%2520considerable%250Aperformance%2520gains%2520in%2520anomaly%2520detection%2520and%2520demonstrates%2520competitiveness%2520in%250Aanomaly%2520localization%2520and%2520reasoning.%2520We%2520will%2520release%2520our%2520dataset%2520in%250Acamera-ready.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IADGPT%3A%20Unified%20LVLM%20for%20Few-Shot%20Industrial%20Anomaly%20Detection%2C%0A%20%20Localization%2C%20and%20Reasoning%20via%20In-Context%20Learning&entry.906535625=Mengyang%20Zhao%20and%20Teng%20Fu%20and%20Haiyang%20Yu%20and%20Ke%20Niu%20and%20Bin%20Li&entry.1292438233=%20%20Few-Shot%20Industrial%20Anomaly%20Detection%20%28FS-IAD%29%20has%20important%20applications%20in%0Aautomating%20industrial%20quality%20inspection.%20Recently%2C%20some%20FS-IAD%20methods%20based%0Aon%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20been%20proposed%20with%20some%0Aachievements%20through%20prompt%20learning%20or%20fine-tuning.%20However%2C%20existing%20LVLMs%0Afocus%20on%20general%20tasks%20but%20lack%20basic%20industrial%20knowledge%20and%20reasoning%0Acapabilities%20related%20to%20FS-IAD%2C%20making%20these%20methods%20far%20from%20specialized%20human%0Aquality%20inspectors.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20unified%0Aframework%2C%20IADGPT%2C%20designed%20to%20perform%20FS-IAD%20in%20a%20human-like%20manner%2C%20while%0Aalso%20handling%20associated%20localization%20and%20reasoning%20tasks%2C%20even%20for%20diverse%20and%0Anovel%20industrial%20products.%20To%20this%20end%2C%20we%20introduce%20a%20three-stage%20progressive%0Atraining%20strategy%20inspired%20by%20humans.%20Specifically%2C%20the%20first%20two%20stages%0Agradually%20guide%20IADGPT%20in%20acquiring%20fundamental%20industrial%20knowledge%20and%0Adiscrepancy%20awareness.%20In%20the%20third%20stage%2C%20we%20design%20an%20in-context%0Alearning-based%20training%20paradigm%2C%20enabling%20IADGPT%20to%20leverage%20a%20few-shot%20image%0Aas%20the%20exemplars%20for%20improved%20generalization%20to%20novel%20products.%20In%20addition%2C%20we%0Adesign%20a%20strategy%20that%20enables%20IADGPT%20to%20output%20image-level%20and%20pixel-level%0Aanomaly%20scores%20using%20the%20logits%20output%20and%20the%20attention%20map%2C%20respectively%2C%20in%0Aconjunction%20with%20the%20language%20output%20to%20accomplish%20anomaly%20reasoning.%20To%0Asupport%20our%20training%2C%20we%20present%20a%20new%20dataset%20comprising%20100K%20images%20across%0A400%20diverse%20industrial%20product%20categories%20with%20extensive%20attribute-level%0Atextual%20annotations.%20Experiments%20indicate%20IADGPT%20achieves%20considerable%0Aperformance%20gains%20in%20anomaly%20detection%20and%20demonstrates%20competitiveness%20in%0Aanomaly%20localization%20and%20reasoning.%20We%20will%20release%20our%20dataset%20in%0Acamera-ready.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10681v1&entry.124074799=Read"},
{"title": "Stepwise Decomposition and Dual-stream Focus: A Novel Approach for\n  Training-free Camouflaged Object Segmentation", "author": "Chao Yin and Hao Li and Kequan Yang and Jide Li and Pinpin Zhu and Xiaoqiang Li", "abstract": "  While promptable segmentation (\\textit{e.g.}, SAM) has shown promise for\nvarious segmentation tasks, it still requires manual visual prompts for each\nobject to be segmented. In contrast, task-generic promptable segmentation aims\nto reduce the need for such detailed prompts by employing only a task-generic\nprompt to guide segmentation across all test samples. However, when applied to\nCamouflaged Object Segmentation (COS), current methods still face two critical\nissues: 1) \\textit{\\textbf{semantic ambiguity in getting instance-specific text\nprompts}}, which arises from insufficient discriminative cues in holistic\ncaptions, leading to foreground-background confusion; 2)\n\\textit{\\textbf{semantic discrepancy combined with spatial separation in\ngetting instance-specific visual prompts}}, which results from global\nbackground sampling far from object boundaries with low feature correlation,\ncausing SAM to segment irrelevant regions. To address the issues above, we\npropose \\textbf{RDVP-MSD}, a novel training-free test-time adaptation framework\nthat synergizes \\textbf{R}egion-constrained \\textbf{D}ual-stream\n\\textbf{V}isual \\textbf{P}rompting (RDVP) via \\textbf{M}ultimodal\n\\textbf{S}tepwise \\textbf{D}ecomposition Chain of Thought (MSD-CoT). MSD-CoT\nprogressively disentangles image captions to eliminate semantic ambiguity,\nwhile RDVP injects spatial constraints into visual prompting and independently\nsamples visual prompts for foreground and background points, effectively\nmitigating semantic discrepancy and spatial separation. Without requiring any\ntraining or supervision, RDVP-MSD achieves a state-of-the-art segmentation\nresult on multiple COS benchmarks and delivers a faster inference speed than\nprevious methods, demonstrating significantly improved accuracy and efficiency.\nThe codes will be available at\n\\href{https://github.com/ycyinchao/RDVP-MSD}{https://github.com/ycyinchao/RDVP-MSD}\n", "link": "http://arxiv.org/abs/2506.06818v3", "date": "2025-08-14", "relevancy": 2.2899, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5878}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stepwise%20Decomposition%20and%20Dual-stream%20Focus%3A%20A%20Novel%20Approach%20for%0A%20%20Training-free%20Camouflaged%20Object%20Segmentation&body=Title%3A%20Stepwise%20Decomposition%20and%20Dual-stream%20Focus%3A%20A%20Novel%20Approach%20for%0A%20%20Training-free%20Camouflaged%20Object%20Segmentation%0AAuthor%3A%20Chao%20Yin%20and%20Hao%20Li%20and%20Kequan%20Yang%20and%20Jide%20Li%20and%20Pinpin%20Zhu%20and%20Xiaoqiang%20Li%0AAbstract%3A%20%20%20While%20promptable%20segmentation%20%28%5Ctextit%7Be.g.%7D%2C%20SAM%29%20has%20shown%20promise%20for%0Avarious%20segmentation%20tasks%2C%20it%20still%20requires%20manual%20visual%20prompts%20for%20each%0Aobject%20to%20be%20segmented.%20In%20contrast%2C%20task-generic%20promptable%20segmentation%20aims%0Ato%20reduce%20the%20need%20for%20such%20detailed%20prompts%20by%20employing%20only%20a%20task-generic%0Aprompt%20to%20guide%20segmentation%20across%20all%20test%20samples.%20However%2C%20when%20applied%20to%0ACamouflaged%20Object%20Segmentation%20%28COS%29%2C%20current%20methods%20still%20face%20two%20critical%0Aissues%3A%201%29%20%5Ctextit%7B%5Ctextbf%7Bsemantic%20ambiguity%20in%20getting%20instance-specific%20text%0Aprompts%7D%7D%2C%20which%20arises%20from%20insufficient%20discriminative%20cues%20in%20holistic%0Acaptions%2C%20leading%20to%20foreground-background%20confusion%3B%202%29%0A%5Ctextit%7B%5Ctextbf%7Bsemantic%20discrepancy%20combined%20with%20spatial%20separation%20in%0Agetting%20instance-specific%20visual%20prompts%7D%7D%2C%20which%20results%20from%20global%0Abackground%20sampling%20far%20from%20object%20boundaries%20with%20low%20feature%20correlation%2C%0Acausing%20SAM%20to%20segment%20irrelevant%20regions.%20To%20address%20the%20issues%20above%2C%20we%0Apropose%20%5Ctextbf%7BRDVP-MSD%7D%2C%20a%20novel%20training-free%20test-time%20adaptation%20framework%0Athat%20synergizes%20%5Ctextbf%7BR%7Degion-constrained%20%5Ctextbf%7BD%7Dual-stream%0A%5Ctextbf%7BV%7Disual%20%5Ctextbf%7BP%7Drompting%20%28RDVP%29%20via%20%5Ctextbf%7BM%7Dultimodal%0A%5Ctextbf%7BS%7Dtepwise%20%5Ctextbf%7BD%7Decomposition%20Chain%20of%20Thought%20%28MSD-CoT%29.%20MSD-CoT%0Aprogressively%20disentangles%20image%20captions%20to%20eliminate%20semantic%20ambiguity%2C%0Awhile%20RDVP%20injects%20spatial%20constraints%20into%20visual%20prompting%20and%20independently%0Asamples%20visual%20prompts%20for%20foreground%20and%20background%20points%2C%20effectively%0Amitigating%20semantic%20discrepancy%20and%20spatial%20separation.%20Without%20requiring%20any%0Atraining%20or%20supervision%2C%20RDVP-MSD%20achieves%20a%20state-of-the-art%20segmentation%0Aresult%20on%20multiple%20COS%20benchmarks%20and%20delivers%20a%20faster%20inference%20speed%20than%0Aprevious%20methods%2C%20demonstrating%20significantly%20improved%20accuracy%20and%20efficiency.%0AThe%20codes%20will%20be%20available%20at%0A%5Chref%7Bhttps%3A//github.com/ycyinchao/RDVP-MSD%7D%7Bhttps%3A//github.com/ycyinchao/RDVP-MSD%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06818v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStepwise%2520Decomposition%2520and%2520Dual-stream%2520Focus%253A%2520A%2520Novel%2520Approach%2520for%250A%2520%2520Training-free%2520Camouflaged%2520Object%2520Segmentation%26entry.906535625%3DChao%2520Yin%2520and%2520Hao%2520Li%2520and%2520Kequan%2520Yang%2520and%2520Jide%2520Li%2520and%2520Pinpin%2520Zhu%2520and%2520Xiaoqiang%2520Li%26entry.1292438233%3D%2520%2520While%2520promptable%2520segmentation%2520%2528%255Ctextit%257Be.g.%257D%252C%2520SAM%2529%2520has%2520shown%2520promise%2520for%250Avarious%2520segmentation%2520tasks%252C%2520it%2520still%2520requires%2520manual%2520visual%2520prompts%2520for%2520each%250Aobject%2520to%2520be%2520segmented.%2520In%2520contrast%252C%2520task-generic%2520promptable%2520segmentation%2520aims%250Ato%2520reduce%2520the%2520need%2520for%2520such%2520detailed%2520prompts%2520by%2520employing%2520only%2520a%2520task-generic%250Aprompt%2520to%2520guide%2520segmentation%2520across%2520all%2520test%2520samples.%2520However%252C%2520when%2520applied%2520to%250ACamouflaged%2520Object%2520Segmentation%2520%2528COS%2529%252C%2520current%2520methods%2520still%2520face%2520two%2520critical%250Aissues%253A%25201%2529%2520%255Ctextit%257B%255Ctextbf%257Bsemantic%2520ambiguity%2520in%2520getting%2520instance-specific%2520text%250Aprompts%257D%257D%252C%2520which%2520arises%2520from%2520insufficient%2520discriminative%2520cues%2520in%2520holistic%250Acaptions%252C%2520leading%2520to%2520foreground-background%2520confusion%253B%25202%2529%250A%255Ctextit%257B%255Ctextbf%257Bsemantic%2520discrepancy%2520combined%2520with%2520spatial%2520separation%2520in%250Agetting%2520instance-specific%2520visual%2520prompts%257D%257D%252C%2520which%2520results%2520from%2520global%250Abackground%2520sampling%2520far%2520from%2520object%2520boundaries%2520with%2520low%2520feature%2520correlation%252C%250Acausing%2520SAM%2520to%2520segment%2520irrelevant%2520regions.%2520To%2520address%2520the%2520issues%2520above%252C%2520we%250Apropose%2520%255Ctextbf%257BRDVP-MSD%257D%252C%2520a%2520novel%2520training-free%2520test-time%2520adaptation%2520framework%250Athat%2520synergizes%2520%255Ctextbf%257BR%257Degion-constrained%2520%255Ctextbf%257BD%257Dual-stream%250A%255Ctextbf%257BV%257Disual%2520%255Ctextbf%257BP%257Drompting%2520%2528RDVP%2529%2520via%2520%255Ctextbf%257BM%257Dultimodal%250A%255Ctextbf%257BS%257Dtepwise%2520%255Ctextbf%257BD%257Decomposition%2520Chain%2520of%2520Thought%2520%2528MSD-CoT%2529.%2520MSD-CoT%250Aprogressively%2520disentangles%2520image%2520captions%2520to%2520eliminate%2520semantic%2520ambiguity%252C%250Awhile%2520RDVP%2520injects%2520spatial%2520constraints%2520into%2520visual%2520prompting%2520and%2520independently%250Asamples%2520visual%2520prompts%2520for%2520foreground%2520and%2520background%2520points%252C%2520effectively%250Amitigating%2520semantic%2520discrepancy%2520and%2520spatial%2520separation.%2520Without%2520requiring%2520any%250Atraining%2520or%2520supervision%252C%2520RDVP-MSD%2520achieves%2520a%2520state-of-the-art%2520segmentation%250Aresult%2520on%2520multiple%2520COS%2520benchmarks%2520and%2520delivers%2520a%2520faster%2520inference%2520speed%2520than%250Aprevious%2520methods%252C%2520demonstrating%2520significantly%2520improved%2520accuracy%2520and%2520efficiency.%250AThe%2520codes%2520will%2520be%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/ycyinchao/RDVP-MSD%257D%257Bhttps%253A//github.com/ycyinchao/RDVP-MSD%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06818v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stepwise%20Decomposition%20and%20Dual-stream%20Focus%3A%20A%20Novel%20Approach%20for%0A%20%20Training-free%20Camouflaged%20Object%20Segmentation&entry.906535625=Chao%20Yin%20and%20Hao%20Li%20and%20Kequan%20Yang%20and%20Jide%20Li%20and%20Pinpin%20Zhu%20and%20Xiaoqiang%20Li&entry.1292438233=%20%20While%20promptable%20segmentation%20%28%5Ctextit%7Be.g.%7D%2C%20SAM%29%20has%20shown%20promise%20for%0Avarious%20segmentation%20tasks%2C%20it%20still%20requires%20manual%20visual%20prompts%20for%20each%0Aobject%20to%20be%20segmented.%20In%20contrast%2C%20task-generic%20promptable%20segmentation%20aims%0Ato%20reduce%20the%20need%20for%20such%20detailed%20prompts%20by%20employing%20only%20a%20task-generic%0Aprompt%20to%20guide%20segmentation%20across%20all%20test%20samples.%20However%2C%20when%20applied%20to%0ACamouflaged%20Object%20Segmentation%20%28COS%29%2C%20current%20methods%20still%20face%20two%20critical%0Aissues%3A%201%29%20%5Ctextit%7B%5Ctextbf%7Bsemantic%20ambiguity%20in%20getting%20instance-specific%20text%0Aprompts%7D%7D%2C%20which%20arises%20from%20insufficient%20discriminative%20cues%20in%20holistic%0Acaptions%2C%20leading%20to%20foreground-background%20confusion%3B%202%29%0A%5Ctextit%7B%5Ctextbf%7Bsemantic%20discrepancy%20combined%20with%20spatial%20separation%20in%0Agetting%20instance-specific%20visual%20prompts%7D%7D%2C%20which%20results%20from%20global%0Abackground%20sampling%20far%20from%20object%20boundaries%20with%20low%20feature%20correlation%2C%0Acausing%20SAM%20to%20segment%20irrelevant%20regions.%20To%20address%20the%20issues%20above%2C%20we%0Apropose%20%5Ctextbf%7BRDVP-MSD%7D%2C%20a%20novel%20training-free%20test-time%20adaptation%20framework%0Athat%20synergizes%20%5Ctextbf%7BR%7Degion-constrained%20%5Ctextbf%7BD%7Dual-stream%0A%5Ctextbf%7BV%7Disual%20%5Ctextbf%7BP%7Drompting%20%28RDVP%29%20via%20%5Ctextbf%7BM%7Dultimodal%0A%5Ctextbf%7BS%7Dtepwise%20%5Ctextbf%7BD%7Decomposition%20Chain%20of%20Thought%20%28MSD-CoT%29.%20MSD-CoT%0Aprogressively%20disentangles%20image%20captions%20to%20eliminate%20semantic%20ambiguity%2C%0Awhile%20RDVP%20injects%20spatial%20constraints%20into%20visual%20prompting%20and%20independently%0Asamples%20visual%20prompts%20for%20foreground%20and%20background%20points%2C%20effectively%0Amitigating%20semantic%20discrepancy%20and%20spatial%20separation.%20Without%20requiring%20any%0Atraining%20or%20supervision%2C%20RDVP-MSD%20achieves%20a%20state-of-the-art%20segmentation%0Aresult%20on%20multiple%20COS%20benchmarks%20and%20delivers%20a%20faster%20inference%20speed%20than%0Aprevious%20methods%2C%20demonstrating%20significantly%20improved%20accuracy%20and%20efficiency.%0AThe%20codes%20will%20be%20available%20at%0A%5Chref%7Bhttps%3A//github.com/ycyinchao/RDVP-MSD%7D%7Bhttps%3A//github.com/ycyinchao/RDVP-MSD%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06818v3&entry.124074799=Read"},
{"title": "ToonComposer: Streamlining Cartoon Production with Generative\n  Post-Keyframing", "author": "Lingen Li and Guangzhi Wang and Zhaoyang Zhang and Yaowei Li and Xiaoyu Li and Qi Dou and Jinwei Gu and Tianfan Xue and Ying Shan", "abstract": "  Traditional cartoon and anime production involves keyframing, inbetweening,\nand colorization stages, which require intensive manual effort. Despite recent\nadvances in AI, existing methods often handle these stages separately, leading\nto error accumulation and artifacts. For instance, inbetweening approaches\nstruggle with large motions, while colorization methods require dense per-frame\nsketches. To address this, we introduce ToonComposer, a generative model that\nunifies inbetweening and colorization into a single post-keyframing stage.\nToonComposer employs a sparse sketch injection mechanism to provide precise\ncontrol using keyframe sketches. Additionally, it uses a cartoon adaptation\nmethod with the spatial low-rank adapter to tailor a modern video foundation\nmodel to the cartoon domain while keeping its temporal prior intact. Requiring\nas few as a single sketch and a colored reference frame, ToonComposer excels\nwith sparse inputs, while also supporting multiple sketches at any temporal\nlocation for more precise motion control. This dual capability reduces manual\nworkload and improves flexibility, empowering artists in real-world scenarios.\nTo evaluate our model, we further created PKBench, a benchmark featuring\nhuman-drawn sketches that simulate real-world use cases. Our evaluation\ndemonstrates that ToonComposer outperforms existing methods in visual quality,\nmotion consistency, and production efficiency, offering a superior and more\nflexible solution for AI-assisted cartoon production.\n", "link": "http://arxiv.org/abs/2508.10881v1", "date": "2025-08-14", "relevancy": 2.2773, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6373}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5653}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToonComposer%3A%20Streamlining%20Cartoon%20Production%20with%20Generative%0A%20%20Post-Keyframing&body=Title%3A%20ToonComposer%3A%20Streamlining%20Cartoon%20Production%20with%20Generative%0A%20%20Post-Keyframing%0AAuthor%3A%20Lingen%20Li%20and%20Guangzhi%20Wang%20and%20Zhaoyang%20Zhang%20and%20Yaowei%20Li%20and%20Xiaoyu%20Li%20and%20Qi%20Dou%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Traditional%20cartoon%20and%20anime%20production%20involves%20keyframing%2C%20inbetweening%2C%0Aand%20colorization%20stages%2C%20which%20require%20intensive%20manual%20effort.%20Despite%20recent%0Aadvances%20in%20AI%2C%20existing%20methods%20often%20handle%20these%20stages%20separately%2C%20leading%0Ato%20error%20accumulation%20and%20artifacts.%20For%20instance%2C%20inbetweening%20approaches%0Astruggle%20with%20large%20motions%2C%20while%20colorization%20methods%20require%20dense%20per-frame%0Asketches.%20To%20address%20this%2C%20we%20introduce%20ToonComposer%2C%20a%20generative%20model%20that%0Aunifies%20inbetweening%20and%20colorization%20into%20a%20single%20post-keyframing%20stage.%0AToonComposer%20employs%20a%20sparse%20sketch%20injection%20mechanism%20to%20provide%20precise%0Acontrol%20using%20keyframe%20sketches.%20Additionally%2C%20it%20uses%20a%20cartoon%20adaptation%0Amethod%20with%20the%20spatial%20low-rank%20adapter%20to%20tailor%20a%20modern%20video%20foundation%0Amodel%20to%20the%20cartoon%20domain%20while%20keeping%20its%20temporal%20prior%20intact.%20Requiring%0Aas%20few%20as%20a%20single%20sketch%20and%20a%20colored%20reference%20frame%2C%20ToonComposer%20excels%0Awith%20sparse%20inputs%2C%20while%20also%20supporting%20multiple%20sketches%20at%20any%20temporal%0Alocation%20for%20more%20precise%20motion%20control.%20This%20dual%20capability%20reduces%20manual%0Aworkload%20and%20improves%20flexibility%2C%20empowering%20artists%20in%20real-world%20scenarios.%0ATo%20evaluate%20our%20model%2C%20we%20further%20created%20PKBench%2C%20a%20benchmark%20featuring%0Ahuman-drawn%20sketches%20that%20simulate%20real-world%20use%20cases.%20Our%20evaluation%0Ademonstrates%20that%20ToonComposer%20outperforms%20existing%20methods%20in%20visual%20quality%2C%0Amotion%20consistency%2C%20and%20production%20efficiency%2C%20offering%20a%20superior%20and%20more%0Aflexible%20solution%20for%20AI-assisted%20cartoon%20production.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToonComposer%253A%2520Streamlining%2520Cartoon%2520Production%2520with%2520Generative%250A%2520%2520Post-Keyframing%26entry.906535625%3DLingen%2520Li%2520and%2520Guangzhi%2520Wang%2520and%2520Zhaoyang%2520Zhang%2520and%2520Yaowei%2520Li%2520and%2520Xiaoyu%2520Li%2520and%2520Qi%2520Dou%2520and%2520Jinwei%2520Gu%2520and%2520Tianfan%2520Xue%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Traditional%2520cartoon%2520and%2520anime%2520production%2520involves%2520keyframing%252C%2520inbetweening%252C%250Aand%2520colorization%2520stages%252C%2520which%2520require%2520intensive%2520manual%2520effort.%2520Despite%2520recent%250Aadvances%2520in%2520AI%252C%2520existing%2520methods%2520often%2520handle%2520these%2520stages%2520separately%252C%2520leading%250Ato%2520error%2520accumulation%2520and%2520artifacts.%2520For%2520instance%252C%2520inbetweening%2520approaches%250Astruggle%2520with%2520large%2520motions%252C%2520while%2520colorization%2520methods%2520require%2520dense%2520per-frame%250Asketches.%2520To%2520address%2520this%252C%2520we%2520introduce%2520ToonComposer%252C%2520a%2520generative%2520model%2520that%250Aunifies%2520inbetweening%2520and%2520colorization%2520into%2520a%2520single%2520post-keyframing%2520stage.%250AToonComposer%2520employs%2520a%2520sparse%2520sketch%2520injection%2520mechanism%2520to%2520provide%2520precise%250Acontrol%2520using%2520keyframe%2520sketches.%2520Additionally%252C%2520it%2520uses%2520a%2520cartoon%2520adaptation%250Amethod%2520with%2520the%2520spatial%2520low-rank%2520adapter%2520to%2520tailor%2520a%2520modern%2520video%2520foundation%250Amodel%2520to%2520the%2520cartoon%2520domain%2520while%2520keeping%2520its%2520temporal%2520prior%2520intact.%2520Requiring%250Aas%2520few%2520as%2520a%2520single%2520sketch%2520and%2520a%2520colored%2520reference%2520frame%252C%2520ToonComposer%2520excels%250Awith%2520sparse%2520inputs%252C%2520while%2520also%2520supporting%2520multiple%2520sketches%2520at%2520any%2520temporal%250Alocation%2520for%2520more%2520precise%2520motion%2520control.%2520This%2520dual%2520capability%2520reduces%2520manual%250Aworkload%2520and%2520improves%2520flexibility%252C%2520empowering%2520artists%2520in%2520real-world%2520scenarios.%250ATo%2520evaluate%2520our%2520model%252C%2520we%2520further%2520created%2520PKBench%252C%2520a%2520benchmark%2520featuring%250Ahuman-drawn%2520sketches%2520that%2520simulate%2520real-world%2520use%2520cases.%2520Our%2520evaluation%250Ademonstrates%2520that%2520ToonComposer%2520outperforms%2520existing%2520methods%2520in%2520visual%2520quality%252C%250Amotion%2520consistency%252C%2520and%2520production%2520efficiency%252C%2520offering%2520a%2520superior%2520and%2520more%250Aflexible%2520solution%2520for%2520AI-assisted%2520cartoon%2520production.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToonComposer%3A%20Streamlining%20Cartoon%20Production%20with%20Generative%0A%20%20Post-Keyframing&entry.906535625=Lingen%20Li%20and%20Guangzhi%20Wang%20and%20Zhaoyang%20Zhang%20and%20Yaowei%20Li%20and%20Xiaoyu%20Li%20and%20Qi%20Dou%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue%20and%20Ying%20Shan&entry.1292438233=%20%20Traditional%20cartoon%20and%20anime%20production%20involves%20keyframing%2C%20inbetweening%2C%0Aand%20colorization%20stages%2C%20which%20require%20intensive%20manual%20effort.%20Despite%20recent%0Aadvances%20in%20AI%2C%20existing%20methods%20often%20handle%20these%20stages%20separately%2C%20leading%0Ato%20error%20accumulation%20and%20artifacts.%20For%20instance%2C%20inbetweening%20approaches%0Astruggle%20with%20large%20motions%2C%20while%20colorization%20methods%20require%20dense%20per-frame%0Asketches.%20To%20address%20this%2C%20we%20introduce%20ToonComposer%2C%20a%20generative%20model%20that%0Aunifies%20inbetweening%20and%20colorization%20into%20a%20single%20post-keyframing%20stage.%0AToonComposer%20employs%20a%20sparse%20sketch%20injection%20mechanism%20to%20provide%20precise%0Acontrol%20using%20keyframe%20sketches.%20Additionally%2C%20it%20uses%20a%20cartoon%20adaptation%0Amethod%20with%20the%20spatial%20low-rank%20adapter%20to%20tailor%20a%20modern%20video%20foundation%0Amodel%20to%20the%20cartoon%20domain%20while%20keeping%20its%20temporal%20prior%20intact.%20Requiring%0Aas%20few%20as%20a%20single%20sketch%20and%20a%20colored%20reference%20frame%2C%20ToonComposer%20excels%0Awith%20sparse%20inputs%2C%20while%20also%20supporting%20multiple%20sketches%20at%20any%20temporal%0Alocation%20for%20more%20precise%20motion%20control.%20This%20dual%20capability%20reduces%20manual%0Aworkload%20and%20improves%20flexibility%2C%20empowering%20artists%20in%20real-world%20scenarios.%0ATo%20evaluate%20our%20model%2C%20we%20further%20created%20PKBench%2C%20a%20benchmark%20featuring%0Ahuman-drawn%20sketches%20that%20simulate%20real-world%20use%20cases.%20Our%20evaluation%0Ademonstrates%20that%20ToonComposer%20outperforms%20existing%20methods%20in%20visual%20quality%2C%0Amotion%20consistency%2C%20and%20production%20efficiency%2C%20offering%20a%20superior%20and%20more%0Aflexible%20solution%20for%20AI-assisted%20cartoon%20production.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10881v1&entry.124074799=Read"},
{"title": "Exploiting Discriminative Codebook Prior for Autoregressive Image\n  Generation", "author": "Longxiang Tang and Ruihang Chu and Xiang Wang and Yujin Han and Pingyu Wu and Chunming He and Yingya Zhang and Shiwei Zhang and Jiaya Jia", "abstract": "  Advanced discrete token-based autoregressive image generation systems first\ntokenize images into sequences of token indices with a codebook, and then model\nthese sequences in an autoregressive paradigm. While autoregressive generative\nmodels are trained only on index values, the prior encoded in the codebook,\nwhich contains rich token similarity information, is not exploited. Recent\nstudies have attempted to incorporate this prior by performing naive k-means\nclustering on the tokens, helping to facilitate the training of generative\nmodels with a reduced codebook. However, we reveal that k-means clustering\nperforms poorly in the codebook feature space due to inherent issues, including\ntoken space disparity and centroid distance inaccuracy. In this work, we\npropose the Discriminative Codebook Prior Extractor (DCPE) as an alternative to\nk-means clustering for more effectively mining and utilizing the token\nsimilarity information embedded in the codebook. DCPE replaces the commonly\nused centroid-based distance, which is found to be unsuitable and inaccurate\nfor the token feature space, with a more reasonable instance-based distance.\nUsing an agglomerative merging technique, it further addresses the token space\ndisparity issue by avoiding splitting high-density regions and aggregating\nlow-density ones. Extensive experiments demonstrate that DCPE is plug-and-play\nand integrates seamlessly with existing codebook prior-based paradigms. With\nthe discriminative prior extracted, DCPE accelerates the training of\nautoregressive models by 42% on LlamaGen-B and improves final FID and IS\nperformance.\n", "link": "http://arxiv.org/abs/2508.10719v1", "date": "2025-08-14", "relevancy": 2.2685, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5712}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5696}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Discriminative%20Codebook%20Prior%20for%20Autoregressive%20Image%0A%20%20Generation&body=Title%3A%20Exploiting%20Discriminative%20Codebook%20Prior%20for%20Autoregressive%20Image%0A%20%20Generation%0AAuthor%3A%20Longxiang%20Tang%20and%20Ruihang%20Chu%20and%20Xiang%20Wang%20and%20Yujin%20Han%20and%20Pingyu%20Wu%20and%20Chunming%20He%20and%20Yingya%20Zhang%20and%20Shiwei%20Zhang%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20Advanced%20discrete%20token-based%20autoregressive%20image%20generation%20systems%20first%0Atokenize%20images%20into%20sequences%20of%20token%20indices%20with%20a%20codebook%2C%20and%20then%20model%0Athese%20sequences%20in%20an%20autoregressive%20paradigm.%20While%20autoregressive%20generative%0Amodels%20are%20trained%20only%20on%20index%20values%2C%20the%20prior%20encoded%20in%20the%20codebook%2C%0Awhich%20contains%20rich%20token%20similarity%20information%2C%20is%20not%20exploited.%20Recent%0Astudies%20have%20attempted%20to%20incorporate%20this%20prior%20by%20performing%20naive%20k-means%0Aclustering%20on%20the%20tokens%2C%20helping%20to%20facilitate%20the%20training%20of%20generative%0Amodels%20with%20a%20reduced%20codebook.%20However%2C%20we%20reveal%20that%20k-means%20clustering%0Aperforms%20poorly%20in%20the%20codebook%20feature%20space%20due%20to%20inherent%20issues%2C%20including%0Atoken%20space%20disparity%20and%20centroid%20distance%20inaccuracy.%20In%20this%20work%2C%20we%0Apropose%20the%20Discriminative%20Codebook%20Prior%20Extractor%20%28DCPE%29%20as%20an%20alternative%20to%0Ak-means%20clustering%20for%20more%20effectively%20mining%20and%20utilizing%20the%20token%0Asimilarity%20information%20embedded%20in%20the%20codebook.%20DCPE%20replaces%20the%20commonly%0Aused%20centroid-based%20distance%2C%20which%20is%20found%20to%20be%20unsuitable%20and%20inaccurate%0Afor%20the%20token%20feature%20space%2C%20with%20a%20more%20reasonable%20instance-based%20distance.%0AUsing%20an%20agglomerative%20merging%20technique%2C%20it%20further%20addresses%20the%20token%20space%0Adisparity%20issue%20by%20avoiding%20splitting%20high-density%20regions%20and%20aggregating%0Alow-density%20ones.%20Extensive%20experiments%20demonstrate%20that%20DCPE%20is%20plug-and-play%0Aand%20integrates%20seamlessly%20with%20existing%20codebook%20prior-based%20paradigms.%20With%0Athe%20discriminative%20prior%20extracted%2C%20DCPE%20accelerates%20the%20training%20of%0Aautoregressive%20models%20by%2042%25%20on%20LlamaGen-B%20and%20improves%20final%20FID%20and%20IS%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Discriminative%2520Codebook%2520Prior%2520for%2520Autoregressive%2520Image%250A%2520%2520Generation%26entry.906535625%3DLongxiang%2520Tang%2520and%2520Ruihang%2520Chu%2520and%2520Xiang%2520Wang%2520and%2520Yujin%2520Han%2520and%2520Pingyu%2520Wu%2520and%2520Chunming%2520He%2520and%2520Yingya%2520Zhang%2520and%2520Shiwei%2520Zhang%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520Advanced%2520discrete%2520token-based%2520autoregressive%2520image%2520generation%2520systems%2520first%250Atokenize%2520images%2520into%2520sequences%2520of%2520token%2520indices%2520with%2520a%2520codebook%252C%2520and%2520then%2520model%250Athese%2520sequences%2520in%2520an%2520autoregressive%2520paradigm.%2520While%2520autoregressive%2520generative%250Amodels%2520are%2520trained%2520only%2520on%2520index%2520values%252C%2520the%2520prior%2520encoded%2520in%2520the%2520codebook%252C%250Awhich%2520contains%2520rich%2520token%2520similarity%2520information%252C%2520is%2520not%2520exploited.%2520Recent%250Astudies%2520have%2520attempted%2520to%2520incorporate%2520this%2520prior%2520by%2520performing%2520naive%2520k-means%250Aclustering%2520on%2520the%2520tokens%252C%2520helping%2520to%2520facilitate%2520the%2520training%2520of%2520generative%250Amodels%2520with%2520a%2520reduced%2520codebook.%2520However%252C%2520we%2520reveal%2520that%2520k-means%2520clustering%250Aperforms%2520poorly%2520in%2520the%2520codebook%2520feature%2520space%2520due%2520to%2520inherent%2520issues%252C%2520including%250Atoken%2520space%2520disparity%2520and%2520centroid%2520distance%2520inaccuracy.%2520In%2520this%2520work%252C%2520we%250Apropose%2520the%2520Discriminative%2520Codebook%2520Prior%2520Extractor%2520%2528DCPE%2529%2520as%2520an%2520alternative%2520to%250Ak-means%2520clustering%2520for%2520more%2520effectively%2520mining%2520and%2520utilizing%2520the%2520token%250Asimilarity%2520information%2520embedded%2520in%2520the%2520codebook.%2520DCPE%2520replaces%2520the%2520commonly%250Aused%2520centroid-based%2520distance%252C%2520which%2520is%2520found%2520to%2520be%2520unsuitable%2520and%2520inaccurate%250Afor%2520the%2520token%2520feature%2520space%252C%2520with%2520a%2520more%2520reasonable%2520instance-based%2520distance.%250AUsing%2520an%2520agglomerative%2520merging%2520technique%252C%2520it%2520further%2520addresses%2520the%2520token%2520space%250Adisparity%2520issue%2520by%2520avoiding%2520splitting%2520high-density%2520regions%2520and%2520aggregating%250Alow-density%2520ones.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DCPE%2520is%2520plug-and-play%250Aand%2520integrates%2520seamlessly%2520with%2520existing%2520codebook%2520prior-based%2520paradigms.%2520With%250Athe%2520discriminative%2520prior%2520extracted%252C%2520DCPE%2520accelerates%2520the%2520training%2520of%250Aautoregressive%2520models%2520by%252042%2525%2520on%2520LlamaGen-B%2520and%2520improves%2520final%2520FID%2520and%2520IS%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Discriminative%20Codebook%20Prior%20for%20Autoregressive%20Image%0A%20%20Generation&entry.906535625=Longxiang%20Tang%20and%20Ruihang%20Chu%20and%20Xiang%20Wang%20and%20Yujin%20Han%20and%20Pingyu%20Wu%20and%20Chunming%20He%20and%20Yingya%20Zhang%20and%20Shiwei%20Zhang%20and%20Jiaya%20Jia&entry.1292438233=%20%20Advanced%20discrete%20token-based%20autoregressive%20image%20generation%20systems%20first%0Atokenize%20images%20into%20sequences%20of%20token%20indices%20with%20a%20codebook%2C%20and%20then%20model%0Athese%20sequences%20in%20an%20autoregressive%20paradigm.%20While%20autoregressive%20generative%0Amodels%20are%20trained%20only%20on%20index%20values%2C%20the%20prior%20encoded%20in%20the%20codebook%2C%0Awhich%20contains%20rich%20token%20similarity%20information%2C%20is%20not%20exploited.%20Recent%0Astudies%20have%20attempted%20to%20incorporate%20this%20prior%20by%20performing%20naive%20k-means%0Aclustering%20on%20the%20tokens%2C%20helping%20to%20facilitate%20the%20training%20of%20generative%0Amodels%20with%20a%20reduced%20codebook.%20However%2C%20we%20reveal%20that%20k-means%20clustering%0Aperforms%20poorly%20in%20the%20codebook%20feature%20space%20due%20to%20inherent%20issues%2C%20including%0Atoken%20space%20disparity%20and%20centroid%20distance%20inaccuracy.%20In%20this%20work%2C%20we%0Apropose%20the%20Discriminative%20Codebook%20Prior%20Extractor%20%28DCPE%29%20as%20an%20alternative%20to%0Ak-means%20clustering%20for%20more%20effectively%20mining%20and%20utilizing%20the%20token%0Asimilarity%20information%20embedded%20in%20the%20codebook.%20DCPE%20replaces%20the%20commonly%0Aused%20centroid-based%20distance%2C%20which%20is%20found%20to%20be%20unsuitable%20and%20inaccurate%0Afor%20the%20token%20feature%20space%2C%20with%20a%20more%20reasonable%20instance-based%20distance.%0AUsing%20an%20agglomerative%20merging%20technique%2C%20it%20further%20addresses%20the%20token%20space%0Adisparity%20issue%20by%20avoiding%20splitting%20high-density%20regions%20and%20aggregating%0Alow-density%20ones.%20Extensive%20experiments%20demonstrate%20that%20DCPE%20is%20plug-and-play%0Aand%20integrates%20seamlessly%20with%20existing%20codebook%20prior-based%20paradigms.%20With%0Athe%20discriminative%20prior%20extracted%2C%20DCPE%20accelerates%20the%20training%20of%0Aautoregressive%20models%20by%2042%25%20on%20LlamaGen-B%20and%20improves%20final%20FID%20and%20IS%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10719v1&entry.124074799=Read"},
{"title": "MedVLThinker: Simple Baselines for Multimodal Medical Reasoning", "author": "Xiaoke Huang and Juncheng Wu and Hui Liu and Xianfeng Tang and Yuyin Zhou", "abstract": "  Large Reasoning Models (LRMs) have introduced a new paradigm in AI by\nenabling models to ``think before responding\" via chain-of-thought reasoning.\nHowever, the absence of open and reproducible recipes for building\nreasoning-centric medical LMMs hinders community-wide research, analysis, and\ncomparison. In this paper, we present MedVLThinker, a suite of simple yet\nstrong baselines. Our fully open recipe consists of: (1) systematic data\ncuration for both text-only and image-text medical data, filtered according to\nvarying levels of reasoning difficulty, and (2) two training paradigms:\nSupervised Fine-Tuning (SFT) on distilled reasoning traces and Reinforcement\nLearning with Verifiable Rewards (RLVR) based on final answer correctness.\nAcross extensive experiments on the Qwen2.5-VL model family (3B, 7B) and six\nmedical QA benchmarks, we find that RLVR consistently and significantly\noutperforms SFT. Additionally, under the RLVR framework, a key,\ncounter-intuitive finding is that training on our curated text-only reasoning\ndata provides a more substantial performance boost than training on multimodal\nimage-text data. Our best open 7B model, trained using the RLVR recipe on\ntext-only data, establishes a new state-of-the-art on existing public VQA\nbenchmarks, surpassing all previous open-source medical LMMs. Furthermore,\nscaling our model to 32B achieves performance on par with the proprietary\nGPT-4o. We release all curated data, models, and code to provide the community\nwith a strong, open foundation for future research in multimodal medical\nreasoning.\n", "link": "http://arxiv.org/abs/2508.02669v2", "date": "2025-08-14", "relevancy": 2.253, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5681}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5681}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedVLThinker%3A%20Simple%20Baselines%20for%20Multimodal%20Medical%20Reasoning&body=Title%3A%20MedVLThinker%3A%20Simple%20Baselines%20for%20Multimodal%20Medical%20Reasoning%0AAuthor%3A%20Xiaoke%20Huang%20and%20Juncheng%20Wu%20and%20Hui%20Liu%20and%20Xianfeng%20Tang%20and%20Yuyin%20Zhou%0AAbstract%3A%20%20%20Large%20Reasoning%20Models%20%28LRMs%29%20have%20introduced%20a%20new%20paradigm%20in%20AI%20by%0Aenabling%20models%20to%20%60%60think%20before%20responding%22%20via%20chain-of-thought%20reasoning.%0AHowever%2C%20the%20absence%20of%20open%20and%20reproducible%20recipes%20for%20building%0Areasoning-centric%20medical%20LMMs%20hinders%20community-wide%20research%2C%20analysis%2C%20and%0Acomparison.%20In%20this%20paper%2C%20we%20present%20MedVLThinker%2C%20a%20suite%20of%20simple%20yet%0Astrong%20baselines.%20Our%20fully%20open%20recipe%20consists%20of%3A%20%281%29%20systematic%20data%0Acuration%20for%20both%20text-only%20and%20image-text%20medical%20data%2C%20filtered%20according%20to%0Avarying%20levels%20of%20reasoning%20difficulty%2C%20and%20%282%29%20two%20training%20paradigms%3A%0ASupervised%20Fine-Tuning%20%28SFT%29%20on%20distilled%20reasoning%20traces%20and%20Reinforcement%0ALearning%20with%20Verifiable%20Rewards%20%28RLVR%29%20based%20on%20final%20answer%20correctness.%0AAcross%20extensive%20experiments%20on%20the%20Qwen2.5-VL%20model%20family%20%283B%2C%207B%29%20and%20six%0Amedical%20QA%20benchmarks%2C%20we%20find%20that%20RLVR%20consistently%20and%20significantly%0Aoutperforms%20SFT.%20Additionally%2C%20under%20the%20RLVR%20framework%2C%20a%20key%2C%0Acounter-intuitive%20finding%20is%20that%20training%20on%20our%20curated%20text-only%20reasoning%0Adata%20provides%20a%20more%20substantial%20performance%20boost%20than%20training%20on%20multimodal%0Aimage-text%20data.%20Our%20best%20open%207B%20model%2C%20trained%20using%20the%20RLVR%20recipe%20on%0Atext-only%20data%2C%20establishes%20a%20new%20state-of-the-art%20on%20existing%20public%20VQA%0Abenchmarks%2C%20surpassing%20all%20previous%20open-source%20medical%20LMMs.%20Furthermore%2C%0Ascaling%20our%20model%20to%2032B%20achieves%20performance%20on%20par%20with%20the%20proprietary%0AGPT-4o.%20We%20release%20all%20curated%20data%2C%20models%2C%20and%20code%20to%20provide%20the%20community%0Awith%20a%20strong%2C%20open%20foundation%20for%20future%20research%20in%20multimodal%20medical%0Areasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02669v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedVLThinker%253A%2520Simple%2520Baselines%2520for%2520Multimodal%2520Medical%2520Reasoning%26entry.906535625%3DXiaoke%2520Huang%2520and%2520Juncheng%2520Wu%2520and%2520Hui%2520Liu%2520and%2520Xianfeng%2520Tang%2520and%2520Yuyin%2520Zhou%26entry.1292438233%3D%2520%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529%2520have%2520introduced%2520a%2520new%2520paradigm%2520in%2520AI%2520by%250Aenabling%2520models%2520to%2520%2560%2560think%2520before%2520responding%2522%2520via%2520chain-of-thought%2520reasoning.%250AHowever%252C%2520the%2520absence%2520of%2520open%2520and%2520reproducible%2520recipes%2520for%2520building%250Areasoning-centric%2520medical%2520LMMs%2520hinders%2520community-wide%2520research%252C%2520analysis%252C%2520and%250Acomparison.%2520In%2520this%2520paper%252C%2520we%2520present%2520MedVLThinker%252C%2520a%2520suite%2520of%2520simple%2520yet%250Astrong%2520baselines.%2520Our%2520fully%2520open%2520recipe%2520consists%2520of%253A%2520%25281%2529%2520systematic%2520data%250Acuration%2520for%2520both%2520text-only%2520and%2520image-text%2520medical%2520data%252C%2520filtered%2520according%2520to%250Avarying%2520levels%2520of%2520reasoning%2520difficulty%252C%2520and%2520%25282%2529%2520two%2520training%2520paradigms%253A%250ASupervised%2520Fine-Tuning%2520%2528SFT%2529%2520on%2520distilled%2520reasoning%2520traces%2520and%2520Reinforcement%250ALearning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520based%2520on%2520final%2520answer%2520correctness.%250AAcross%2520extensive%2520experiments%2520on%2520the%2520Qwen2.5-VL%2520model%2520family%2520%25283B%252C%25207B%2529%2520and%2520six%250Amedical%2520QA%2520benchmarks%252C%2520we%2520find%2520that%2520RLVR%2520consistently%2520and%2520significantly%250Aoutperforms%2520SFT.%2520Additionally%252C%2520under%2520the%2520RLVR%2520framework%252C%2520a%2520key%252C%250Acounter-intuitive%2520finding%2520is%2520that%2520training%2520on%2520our%2520curated%2520text-only%2520reasoning%250Adata%2520provides%2520a%2520more%2520substantial%2520performance%2520boost%2520than%2520training%2520on%2520multimodal%250Aimage-text%2520data.%2520Our%2520best%2520open%25207B%2520model%252C%2520trained%2520using%2520the%2520RLVR%2520recipe%2520on%250Atext-only%2520data%252C%2520establishes%2520a%2520new%2520state-of-the-art%2520on%2520existing%2520public%2520VQA%250Abenchmarks%252C%2520surpassing%2520all%2520previous%2520open-source%2520medical%2520LMMs.%2520Furthermore%252C%250Ascaling%2520our%2520model%2520to%252032B%2520achieves%2520performance%2520on%2520par%2520with%2520the%2520proprietary%250AGPT-4o.%2520We%2520release%2520all%2520curated%2520data%252C%2520models%252C%2520and%2520code%2520to%2520provide%2520the%2520community%250Awith%2520a%2520strong%252C%2520open%2520foundation%2520for%2520future%2520research%2520in%2520multimodal%2520medical%250Areasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02669v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedVLThinker%3A%20Simple%20Baselines%20for%20Multimodal%20Medical%20Reasoning&entry.906535625=Xiaoke%20Huang%20and%20Juncheng%20Wu%20and%20Hui%20Liu%20and%20Xianfeng%20Tang%20and%20Yuyin%20Zhou&entry.1292438233=%20%20Large%20Reasoning%20Models%20%28LRMs%29%20have%20introduced%20a%20new%20paradigm%20in%20AI%20by%0Aenabling%20models%20to%20%60%60think%20before%20responding%22%20via%20chain-of-thought%20reasoning.%0AHowever%2C%20the%20absence%20of%20open%20and%20reproducible%20recipes%20for%20building%0Areasoning-centric%20medical%20LMMs%20hinders%20community-wide%20research%2C%20analysis%2C%20and%0Acomparison.%20In%20this%20paper%2C%20we%20present%20MedVLThinker%2C%20a%20suite%20of%20simple%20yet%0Astrong%20baselines.%20Our%20fully%20open%20recipe%20consists%20of%3A%20%281%29%20systematic%20data%0Acuration%20for%20both%20text-only%20and%20image-text%20medical%20data%2C%20filtered%20according%20to%0Avarying%20levels%20of%20reasoning%20difficulty%2C%20and%20%282%29%20two%20training%20paradigms%3A%0ASupervised%20Fine-Tuning%20%28SFT%29%20on%20distilled%20reasoning%20traces%20and%20Reinforcement%0ALearning%20with%20Verifiable%20Rewards%20%28RLVR%29%20based%20on%20final%20answer%20correctness.%0AAcross%20extensive%20experiments%20on%20the%20Qwen2.5-VL%20model%20family%20%283B%2C%207B%29%20and%20six%0Amedical%20QA%20benchmarks%2C%20we%20find%20that%20RLVR%20consistently%20and%20significantly%0Aoutperforms%20SFT.%20Additionally%2C%20under%20the%20RLVR%20framework%2C%20a%20key%2C%0Acounter-intuitive%20finding%20is%20that%20training%20on%20our%20curated%20text-only%20reasoning%0Adata%20provides%20a%20more%20substantial%20performance%20boost%20than%20training%20on%20multimodal%0Aimage-text%20data.%20Our%20best%20open%207B%20model%2C%20trained%20using%20the%20RLVR%20recipe%20on%0Atext-only%20data%2C%20establishes%20a%20new%20state-of-the-art%20on%20existing%20public%20VQA%0Abenchmarks%2C%20surpassing%20all%20previous%20open-source%20medical%20LMMs.%20Furthermore%2C%0Ascaling%20our%20model%20to%2032B%20achieves%20performance%20on%20par%20with%20the%20proprietary%0AGPT-4o.%20We%20release%20all%20curated%20data%2C%20models%2C%20and%20code%20to%20provide%20the%20community%0Awith%20a%20strong%2C%20open%20foundation%20for%20future%20research%20in%20multimodal%20medical%0Areasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02669v2&entry.124074799=Read"},
{"title": "Reinforcement Learning in Vision: A Survey", "author": "Weijia Wu and Chen Gao and Joya Chen and Kevin Qinghong Lin and Qingwei Meng and Yiming Zhang and Yuke Qiu and Hong Zhou and Mike Zheng Shou", "abstract": "  Recent advances at the intersection of reinforcement learning (RL) and visual\nintelligence have enabled agents that not only perceive complex visual scenes\nbut also reason, generate, and act within them. This survey offers a critical\nand up-to-date synthesis of the field. We first formalize visual RL problems\nand trace the evolution of policy-optimization strategies from RLHF to\nverifiable reward paradigms, and from Proximal Policy Optimization to Group\nRelative Policy Optimization. We then organize more than 200 representative\nworks into four thematic pillars: multi-modal large language models, visual\ngeneration, unified model frameworks, and vision-language-action models. For\neach pillar we examine algorithmic design, reward engineering, benchmark\nprogress, and we distill trends such as curriculum-driven training,\npreference-aligned diffusion, and unified reward modeling. Finally, we review\nevaluation protocols spanning set-level fidelity, sample-level preference, and\nstate-level stability, and we identify open challenges that include sample\nefficiency, generalization, and safe deployment. Our goal is to provide\nresearchers and practitioners with a coherent map of the rapidly expanding\nlandscape of visual RL and to highlight promising directions for future\ninquiry. Resources are available at:\nhttps://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.\n", "link": "http://arxiv.org/abs/2508.08189v2", "date": "2025-08-14", "relevancy": 2.2522, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5695}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5695}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20in%20Vision%3A%20A%20Survey&body=Title%3A%20Reinforcement%20Learning%20in%20Vision%3A%20A%20Survey%0AAuthor%3A%20Weijia%20Wu%20and%20Chen%20Gao%20and%20Joya%20Chen%20and%20Kevin%20Qinghong%20Lin%20and%20Qingwei%20Meng%20and%20Yiming%20Zhang%20and%20Yuke%20Qiu%20and%20Hong%20Zhou%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Recent%20advances%20at%20the%20intersection%20of%20reinforcement%20learning%20%28RL%29%20and%20visual%0Aintelligence%20have%20enabled%20agents%20that%20not%20only%20perceive%20complex%20visual%20scenes%0Abut%20also%20reason%2C%20generate%2C%20and%20act%20within%20them.%20This%20survey%20offers%20a%20critical%0Aand%20up-to-date%20synthesis%20of%20the%20field.%20We%20first%20formalize%20visual%20RL%20problems%0Aand%20trace%20the%20evolution%20of%20policy-optimization%20strategies%20from%20RLHF%20to%0Averifiable%20reward%20paradigms%2C%20and%20from%20Proximal%20Policy%20Optimization%20to%20Group%0ARelative%20Policy%20Optimization.%20We%20then%20organize%20more%20than%20200%20representative%0Aworks%20into%20four%20thematic%20pillars%3A%20multi-modal%20large%20language%20models%2C%20visual%0Ageneration%2C%20unified%20model%20frameworks%2C%20and%20vision-language-action%20models.%20For%0Aeach%20pillar%20we%20examine%20algorithmic%20design%2C%20reward%20engineering%2C%20benchmark%0Aprogress%2C%20and%20we%20distill%20trends%20such%20as%20curriculum-driven%20training%2C%0Apreference-aligned%20diffusion%2C%20and%20unified%20reward%20modeling.%20Finally%2C%20we%20review%0Aevaluation%20protocols%20spanning%20set-level%20fidelity%2C%20sample-level%20preference%2C%20and%0Astate-level%20stability%2C%20and%20we%20identify%20open%20challenges%20that%20include%20sample%0Aefficiency%2C%20generalization%2C%20and%20safe%20deployment.%20Our%20goal%20is%20to%20provide%0Aresearchers%20and%20practitioners%20with%20a%20coherent%20map%20of%20the%20rapidly%20expanding%0Alandscape%20of%20visual%20RL%20and%20to%20highlight%20promising%20directions%20for%20future%0Ainquiry.%20Resources%20are%20available%20at%3A%0Ahttps%3A//github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520in%2520Vision%253A%2520A%2520Survey%26entry.906535625%3DWeijia%2520Wu%2520and%2520Chen%2520Gao%2520and%2520Joya%2520Chen%2520and%2520Kevin%2520Qinghong%2520Lin%2520and%2520Qingwei%2520Meng%2520and%2520Yiming%2520Zhang%2520and%2520Yuke%2520Qiu%2520and%2520Hong%2520Zhou%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Recent%2520advances%2520at%2520the%2520intersection%2520of%2520reinforcement%2520learning%2520%2528RL%2529%2520and%2520visual%250Aintelligence%2520have%2520enabled%2520agents%2520that%2520not%2520only%2520perceive%2520complex%2520visual%2520scenes%250Abut%2520also%2520reason%252C%2520generate%252C%2520and%2520act%2520within%2520them.%2520This%2520survey%2520offers%2520a%2520critical%250Aand%2520up-to-date%2520synthesis%2520of%2520the%2520field.%2520We%2520first%2520formalize%2520visual%2520RL%2520problems%250Aand%2520trace%2520the%2520evolution%2520of%2520policy-optimization%2520strategies%2520from%2520RLHF%2520to%250Averifiable%2520reward%2520paradigms%252C%2520and%2520from%2520Proximal%2520Policy%2520Optimization%2520to%2520Group%250ARelative%2520Policy%2520Optimization.%2520We%2520then%2520organize%2520more%2520than%2520200%2520representative%250Aworks%2520into%2520four%2520thematic%2520pillars%253A%2520multi-modal%2520large%2520language%2520models%252C%2520visual%250Ageneration%252C%2520unified%2520model%2520frameworks%252C%2520and%2520vision-language-action%2520models.%2520For%250Aeach%2520pillar%2520we%2520examine%2520algorithmic%2520design%252C%2520reward%2520engineering%252C%2520benchmark%250Aprogress%252C%2520and%2520we%2520distill%2520trends%2520such%2520as%2520curriculum-driven%2520training%252C%250Apreference-aligned%2520diffusion%252C%2520and%2520unified%2520reward%2520modeling.%2520Finally%252C%2520we%2520review%250Aevaluation%2520protocols%2520spanning%2520set-level%2520fidelity%252C%2520sample-level%2520preference%252C%2520and%250Astate-level%2520stability%252C%2520and%2520we%2520identify%2520open%2520challenges%2520that%2520include%2520sample%250Aefficiency%252C%2520generalization%252C%2520and%2520safe%2520deployment.%2520Our%2520goal%2520is%2520to%2520provide%250Aresearchers%2520and%2520practitioners%2520with%2520a%2520coherent%2520map%2520of%2520the%2520rapidly%2520expanding%250Alandscape%2520of%2520visual%2520RL%2520and%2520to%2520highlight%2520promising%2520directions%2520for%2520future%250Ainquiry.%2520Resources%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20in%20Vision%3A%20A%20Survey&entry.906535625=Weijia%20Wu%20and%20Chen%20Gao%20and%20Joya%20Chen%20and%20Kevin%20Qinghong%20Lin%20and%20Qingwei%20Meng%20and%20Yiming%20Zhang%20and%20Yuke%20Qiu%20and%20Hong%20Zhou%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Recent%20advances%20at%20the%20intersection%20of%20reinforcement%20learning%20%28RL%29%20and%20visual%0Aintelligence%20have%20enabled%20agents%20that%20not%20only%20perceive%20complex%20visual%20scenes%0Abut%20also%20reason%2C%20generate%2C%20and%20act%20within%20them.%20This%20survey%20offers%20a%20critical%0Aand%20up-to-date%20synthesis%20of%20the%20field.%20We%20first%20formalize%20visual%20RL%20problems%0Aand%20trace%20the%20evolution%20of%20policy-optimization%20strategies%20from%20RLHF%20to%0Averifiable%20reward%20paradigms%2C%20and%20from%20Proximal%20Policy%20Optimization%20to%20Group%0ARelative%20Policy%20Optimization.%20We%20then%20organize%20more%20than%20200%20representative%0Aworks%20into%20four%20thematic%20pillars%3A%20multi-modal%20large%20language%20models%2C%20visual%0Ageneration%2C%20unified%20model%20frameworks%2C%20and%20vision-language-action%20models.%20For%0Aeach%20pillar%20we%20examine%20algorithmic%20design%2C%20reward%20engineering%2C%20benchmark%0Aprogress%2C%20and%20we%20distill%20trends%20such%20as%20curriculum-driven%20training%2C%0Apreference-aligned%20diffusion%2C%20and%20unified%20reward%20modeling.%20Finally%2C%20we%20review%0Aevaluation%20protocols%20spanning%20set-level%20fidelity%2C%20sample-level%20preference%2C%20and%0Astate-level%20stability%2C%20and%20we%20identify%20open%20challenges%20that%20include%20sample%0Aefficiency%2C%20generalization%2C%20and%20safe%20deployment.%20Our%20goal%20is%20to%20provide%0Aresearchers%20and%20practitioners%20with%20a%20coherent%20map%20of%20the%20rapidly%20expanding%0Alandscape%20of%20visual%20RL%20and%20to%20highlight%20promising%20directions%20for%20future%0Ainquiry.%20Resources%20are%20available%20at%3A%0Ahttps%3A//github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08189v2&entry.124074799=Read"},
{"title": "Curse of High Dimensionality Issue in Transformer for Long-context\n  Modeling", "author": "Shuhai Zhang and Zeng You and Yaofo Chen and Zhiquan Wen and Qianyue Wang and Zhijie Qiu and Yuanqing Li and Mingkui Tan", "abstract": "  Transformer-based large language models (LLMs) excel in natural language\nprocessing tasks by capturing long-range dependencies through self-attention\nmechanisms. However, long-context modeling faces significant computational\ninefficiencies due to \\textit{redundant} attention computations: while\nattention weights are often \\textit{sparse}, all tokens consume \\textit{equal}\ncomputational resources. In this paper, we reformulate traditional\nprobabilistic sequence modeling as a \\textit{supervised learning task},\nenabling the separation of relevant and irrelevant tokens and providing a\nclearer understanding of redundancy. Based on this reformulation, we\ntheoretically analyze attention sparsity, revealing that only a few tokens\nsignificantly contribute to predictions. Building on this, we formulate\nattention optimization as a linear coding problem and propose a \\textit{group\ncoding strategy}, theoretically showing its ability to improve robustness\nagainst random noise and enhance learning efficiency. Motivated by this, we\npropose \\textit{Dynamic Group Attention} (DGA), which leverages the group\ncoding to explicitly reduce redundancy by aggregating less important tokens\nduring attention computation. Empirical results show that our DGA significantly\nreduces computational costs while maintaining competitive performance.Code is\navailable at https://github.com/bolixinyu/DynamicGroupAttention.\n", "link": "http://arxiv.org/abs/2505.22107v4", "date": "2025-08-14", "relevancy": 2.2483, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5812}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5587}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curse%20of%20High%20Dimensionality%20Issue%20in%20Transformer%20for%20Long-context%0A%20%20Modeling&body=Title%3A%20Curse%20of%20High%20Dimensionality%20Issue%20in%20Transformer%20for%20Long-context%0A%20%20Modeling%0AAuthor%3A%20Shuhai%20Zhang%20and%20Zeng%20You%20and%20Yaofo%20Chen%20and%20Zhiquan%20Wen%20and%20Qianyue%20Wang%20and%20Zhijie%20Qiu%20and%20Yuanqing%20Li%20and%20Mingkui%20Tan%0AAbstract%3A%20%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20excel%20in%20natural%20language%0Aprocessing%20tasks%20by%20capturing%20long-range%20dependencies%20through%20self-attention%0Amechanisms.%20However%2C%20long-context%20modeling%20faces%20significant%20computational%0Ainefficiencies%20due%20to%20%5Ctextit%7Bredundant%7D%20attention%20computations%3A%20while%0Aattention%20weights%20are%20often%20%5Ctextit%7Bsparse%7D%2C%20all%20tokens%20consume%20%5Ctextit%7Bequal%7D%0Acomputational%20resources.%20In%20this%20paper%2C%20we%20reformulate%20traditional%0Aprobabilistic%20sequence%20modeling%20as%20a%20%5Ctextit%7Bsupervised%20learning%20task%7D%2C%0Aenabling%20the%20separation%20of%20relevant%20and%20irrelevant%20tokens%20and%20providing%20a%0Aclearer%20understanding%20of%20redundancy.%20Based%20on%20this%20reformulation%2C%20we%0Atheoretically%20analyze%20attention%20sparsity%2C%20revealing%20that%20only%20a%20few%20tokens%0Asignificantly%20contribute%20to%20predictions.%20Building%20on%20this%2C%20we%20formulate%0Aattention%20optimization%20as%20a%20linear%20coding%20problem%20and%20propose%20a%20%5Ctextit%7Bgroup%0Acoding%20strategy%7D%2C%20theoretically%20showing%20its%20ability%20to%20improve%20robustness%0Aagainst%20random%20noise%20and%20enhance%20learning%20efficiency.%20Motivated%20by%20this%2C%20we%0Apropose%20%5Ctextit%7BDynamic%20Group%20Attention%7D%20%28DGA%29%2C%20which%20leverages%20the%20group%0Acoding%20to%20explicitly%20reduce%20redundancy%20by%20aggregating%20less%20important%20tokens%0Aduring%20attention%20computation.%20Empirical%20results%20show%20that%20our%20DGA%20significantly%0Areduces%20computational%20costs%20while%20maintaining%20competitive%20performance.Code%20is%0Aavailable%20at%20https%3A//github.com/bolixinyu/DynamicGroupAttention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22107v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurse%2520of%2520High%2520Dimensionality%2520Issue%2520in%2520Transformer%2520for%2520Long-context%250A%2520%2520Modeling%26entry.906535625%3DShuhai%2520Zhang%2520and%2520Zeng%2520You%2520and%2520Yaofo%2520Chen%2520and%2520Zhiquan%2520Wen%2520and%2520Qianyue%2520Wang%2520and%2520Zhijie%2520Qiu%2520and%2520Yuanqing%2520Li%2520and%2520Mingkui%2520Tan%26entry.1292438233%3D%2520%2520Transformer-based%2520large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520in%2520natural%2520language%250Aprocessing%2520tasks%2520by%2520capturing%2520long-range%2520dependencies%2520through%2520self-attention%250Amechanisms.%2520However%252C%2520long-context%2520modeling%2520faces%2520significant%2520computational%250Ainefficiencies%2520due%2520to%2520%255Ctextit%257Bredundant%257D%2520attention%2520computations%253A%2520while%250Aattention%2520weights%2520are%2520often%2520%255Ctextit%257Bsparse%257D%252C%2520all%2520tokens%2520consume%2520%255Ctextit%257Bequal%257D%250Acomputational%2520resources.%2520In%2520this%2520paper%252C%2520we%2520reformulate%2520traditional%250Aprobabilistic%2520sequence%2520modeling%2520as%2520a%2520%255Ctextit%257Bsupervised%2520learning%2520task%257D%252C%250Aenabling%2520the%2520separation%2520of%2520relevant%2520and%2520irrelevant%2520tokens%2520and%2520providing%2520a%250Aclearer%2520understanding%2520of%2520redundancy.%2520Based%2520on%2520this%2520reformulation%252C%2520we%250Atheoretically%2520analyze%2520attention%2520sparsity%252C%2520revealing%2520that%2520only%2520a%2520few%2520tokens%250Asignificantly%2520contribute%2520to%2520predictions.%2520Building%2520on%2520this%252C%2520we%2520formulate%250Aattention%2520optimization%2520as%2520a%2520linear%2520coding%2520problem%2520and%2520propose%2520a%2520%255Ctextit%257Bgroup%250Acoding%2520strategy%257D%252C%2520theoretically%2520showing%2520its%2520ability%2520to%2520improve%2520robustness%250Aagainst%2520random%2520noise%2520and%2520enhance%2520learning%2520efficiency.%2520Motivated%2520by%2520this%252C%2520we%250Apropose%2520%255Ctextit%257BDynamic%2520Group%2520Attention%257D%2520%2528DGA%2529%252C%2520which%2520leverages%2520the%2520group%250Acoding%2520to%2520explicitly%2520reduce%2520redundancy%2520by%2520aggregating%2520less%2520important%2520tokens%250Aduring%2520attention%2520computation.%2520Empirical%2520results%2520show%2520that%2520our%2520DGA%2520significantly%250Areduces%2520computational%2520costs%2520while%2520maintaining%2520competitive%2520performance.Code%2520is%250Aavailable%2520at%2520https%253A//github.com/bolixinyu/DynamicGroupAttention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22107v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curse%20of%20High%20Dimensionality%20Issue%20in%20Transformer%20for%20Long-context%0A%20%20Modeling&entry.906535625=Shuhai%20Zhang%20and%20Zeng%20You%20and%20Yaofo%20Chen%20and%20Zhiquan%20Wen%20and%20Qianyue%20Wang%20and%20Zhijie%20Qiu%20and%20Yuanqing%20Li%20and%20Mingkui%20Tan&entry.1292438233=%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20excel%20in%20natural%20language%0Aprocessing%20tasks%20by%20capturing%20long-range%20dependencies%20through%20self-attention%0Amechanisms.%20However%2C%20long-context%20modeling%20faces%20significant%20computational%0Ainefficiencies%20due%20to%20%5Ctextit%7Bredundant%7D%20attention%20computations%3A%20while%0Aattention%20weights%20are%20often%20%5Ctextit%7Bsparse%7D%2C%20all%20tokens%20consume%20%5Ctextit%7Bequal%7D%0Acomputational%20resources.%20In%20this%20paper%2C%20we%20reformulate%20traditional%0Aprobabilistic%20sequence%20modeling%20as%20a%20%5Ctextit%7Bsupervised%20learning%20task%7D%2C%0Aenabling%20the%20separation%20of%20relevant%20and%20irrelevant%20tokens%20and%20providing%20a%0Aclearer%20understanding%20of%20redundancy.%20Based%20on%20this%20reformulation%2C%20we%0Atheoretically%20analyze%20attention%20sparsity%2C%20revealing%20that%20only%20a%20few%20tokens%0Asignificantly%20contribute%20to%20predictions.%20Building%20on%20this%2C%20we%20formulate%0Aattention%20optimization%20as%20a%20linear%20coding%20problem%20and%20propose%20a%20%5Ctextit%7Bgroup%0Acoding%20strategy%7D%2C%20theoretically%20showing%20its%20ability%20to%20improve%20robustness%0Aagainst%20random%20noise%20and%20enhance%20learning%20efficiency.%20Motivated%20by%20this%2C%20we%0Apropose%20%5Ctextit%7BDynamic%20Group%20Attention%7D%20%28DGA%29%2C%20which%20leverages%20the%20group%0Acoding%20to%20explicitly%20reduce%20redundancy%20by%20aggregating%20less%20important%20tokens%0Aduring%20attention%20computation.%20Empirical%20results%20show%20that%20our%20DGA%20significantly%0Areduces%20computational%20costs%20while%20maintaining%20competitive%20performance.Code%20is%0Aavailable%20at%20https%3A//github.com/bolixinyu/DynamicGroupAttention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22107v4&entry.124074799=Read"},
{"title": "Self-Supervised Temporal Super-Resolution of Energy Data using\n  Generative Adversarial Transformer", "author": "Xuanhao Mu and G\u00f6khan Demirel and Yuzhe Zhang and Jianlei Liu and Thorsten Schlachter and Veit Hagenmeyer", "abstract": "  To bridge the temporal granularity gap in energy network design and operation\nbased on Energy System Models, resampling of time series is required. While\nconventional upsampling methods are computationally efficient, they often\nresult in significant information loss or increased noise. Advanced models such\nas time series generation models, Super-Resolution models and imputation models\nshow potential, but also face fundamental challenges. The goal of time series\ngenerative models is to learn the distribution of the original data to generate\nhigh-resolution series with similar statistical characteristics. This is not\nentirely consistent with the definition of upsampling. Time series\nSuper-Resolution models or imputation models can degrade the accuracy of\nupsampling because the input low-resolution time series are sparse and may have\ninsufficient context. Moreover, such models usually rely on supervised learning\nparadigms. This presents a fundamental application paradox: their training\nrequires the high-resolution time series that is intrinsically absent in\nupsampling application scenarios. To address the mentioned upsampling issue,\nthis paper introduces a new method utilizing Generative Adversarial\nTransformers (GATs), which can be trained without access to any ground-truth\nhigh-resolution data. Compared with conventional interpolation methods, the\nintroduced method can reduce the root mean square error (RMSE) of upsampling\ntasks by 9%, and the accuracy of a model predictive control (MPC) application\nscenario is improved by 13%.\n", "link": "http://arxiv.org/abs/2508.10587v1", "date": "2025-08-14", "relevancy": 2.2413, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6024}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5723}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Temporal%20Super-Resolution%20of%20Energy%20Data%20using%0A%20%20Generative%20Adversarial%20Transformer&body=Title%3A%20Self-Supervised%20Temporal%20Super-Resolution%20of%20Energy%20Data%20using%0A%20%20Generative%20Adversarial%20Transformer%0AAuthor%3A%20Xuanhao%20Mu%20and%20G%C3%B6khan%20Demirel%20and%20Yuzhe%20Zhang%20and%20Jianlei%20Liu%20and%20Thorsten%20Schlachter%20and%20Veit%20Hagenmeyer%0AAbstract%3A%20%20%20To%20bridge%20the%20temporal%20granularity%20gap%20in%20energy%20network%20design%20and%20operation%0Abased%20on%20Energy%20System%20Models%2C%20resampling%20of%20time%20series%20is%20required.%20While%0Aconventional%20upsampling%20methods%20are%20computationally%20efficient%2C%20they%20often%0Aresult%20in%20significant%20information%20loss%20or%20increased%20noise.%20Advanced%20models%20such%0Aas%20time%20series%20generation%20models%2C%20Super-Resolution%20models%20and%20imputation%20models%0Ashow%20potential%2C%20but%20also%20face%20fundamental%20challenges.%20The%20goal%20of%20time%20series%0Agenerative%20models%20is%20to%20learn%20the%20distribution%20of%20the%20original%20data%20to%20generate%0Ahigh-resolution%20series%20with%20similar%20statistical%20characteristics.%20This%20is%20not%0Aentirely%20consistent%20with%20the%20definition%20of%20upsampling.%20Time%20series%0ASuper-Resolution%20models%20or%20imputation%20models%20can%20degrade%20the%20accuracy%20of%0Aupsampling%20because%20the%20input%20low-resolution%20time%20series%20are%20sparse%20and%20may%20have%0Ainsufficient%20context.%20Moreover%2C%20such%20models%20usually%20rely%20on%20supervised%20learning%0Aparadigms.%20This%20presents%20a%20fundamental%20application%20paradox%3A%20their%20training%0Arequires%20the%20high-resolution%20time%20series%20that%20is%20intrinsically%20absent%20in%0Aupsampling%20application%20scenarios.%20To%20address%20the%20mentioned%20upsampling%20issue%2C%0Athis%20paper%20introduces%20a%20new%20method%20utilizing%20Generative%20Adversarial%0ATransformers%20%28GATs%29%2C%20which%20can%20be%20trained%20without%20access%20to%20any%20ground-truth%0Ahigh-resolution%20data.%20Compared%20with%20conventional%20interpolation%20methods%2C%20the%0Aintroduced%20method%20can%20reduce%20the%20root%20mean%20square%20error%20%28RMSE%29%20of%20upsampling%0Atasks%20by%209%25%2C%20and%20the%20accuracy%20of%20a%20model%20predictive%20control%20%28MPC%29%20application%0Ascenario%20is%20improved%20by%2013%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Temporal%2520Super-Resolution%2520of%2520Energy%2520Data%2520using%250A%2520%2520Generative%2520Adversarial%2520Transformer%26entry.906535625%3DXuanhao%2520Mu%2520and%2520G%25C3%25B6khan%2520Demirel%2520and%2520Yuzhe%2520Zhang%2520and%2520Jianlei%2520Liu%2520and%2520Thorsten%2520Schlachter%2520and%2520Veit%2520Hagenmeyer%26entry.1292438233%3D%2520%2520To%2520bridge%2520the%2520temporal%2520granularity%2520gap%2520in%2520energy%2520network%2520design%2520and%2520operation%250Abased%2520on%2520Energy%2520System%2520Models%252C%2520resampling%2520of%2520time%2520series%2520is%2520required.%2520While%250Aconventional%2520upsampling%2520methods%2520are%2520computationally%2520efficient%252C%2520they%2520often%250Aresult%2520in%2520significant%2520information%2520loss%2520or%2520increased%2520noise.%2520Advanced%2520models%2520such%250Aas%2520time%2520series%2520generation%2520models%252C%2520Super-Resolution%2520models%2520and%2520imputation%2520models%250Ashow%2520potential%252C%2520but%2520also%2520face%2520fundamental%2520challenges.%2520The%2520goal%2520of%2520time%2520series%250Agenerative%2520models%2520is%2520to%2520learn%2520the%2520distribution%2520of%2520the%2520original%2520data%2520to%2520generate%250Ahigh-resolution%2520series%2520with%2520similar%2520statistical%2520characteristics.%2520This%2520is%2520not%250Aentirely%2520consistent%2520with%2520the%2520definition%2520of%2520upsampling.%2520Time%2520series%250ASuper-Resolution%2520models%2520or%2520imputation%2520models%2520can%2520degrade%2520the%2520accuracy%2520of%250Aupsampling%2520because%2520the%2520input%2520low-resolution%2520time%2520series%2520are%2520sparse%2520and%2520may%2520have%250Ainsufficient%2520context.%2520Moreover%252C%2520such%2520models%2520usually%2520rely%2520on%2520supervised%2520learning%250Aparadigms.%2520This%2520presents%2520a%2520fundamental%2520application%2520paradox%253A%2520their%2520training%250Arequires%2520the%2520high-resolution%2520time%2520series%2520that%2520is%2520intrinsically%2520absent%2520in%250Aupsampling%2520application%2520scenarios.%2520To%2520address%2520the%2520mentioned%2520upsampling%2520issue%252C%250Athis%2520paper%2520introduces%2520a%2520new%2520method%2520utilizing%2520Generative%2520Adversarial%250ATransformers%2520%2528GATs%2529%252C%2520which%2520can%2520be%2520trained%2520without%2520access%2520to%2520any%2520ground-truth%250Ahigh-resolution%2520data.%2520Compared%2520with%2520conventional%2520interpolation%2520methods%252C%2520the%250Aintroduced%2520method%2520can%2520reduce%2520the%2520root%2520mean%2520square%2520error%2520%2528RMSE%2529%2520of%2520upsampling%250Atasks%2520by%25209%2525%252C%2520and%2520the%2520accuracy%2520of%2520a%2520model%2520predictive%2520control%2520%2528MPC%2529%2520application%250Ascenario%2520is%2520improved%2520by%252013%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Temporal%20Super-Resolution%20of%20Energy%20Data%20using%0A%20%20Generative%20Adversarial%20Transformer&entry.906535625=Xuanhao%20Mu%20and%20G%C3%B6khan%20Demirel%20and%20Yuzhe%20Zhang%20and%20Jianlei%20Liu%20and%20Thorsten%20Schlachter%20and%20Veit%20Hagenmeyer&entry.1292438233=%20%20To%20bridge%20the%20temporal%20granularity%20gap%20in%20energy%20network%20design%20and%20operation%0Abased%20on%20Energy%20System%20Models%2C%20resampling%20of%20time%20series%20is%20required.%20While%0Aconventional%20upsampling%20methods%20are%20computationally%20efficient%2C%20they%20often%0Aresult%20in%20significant%20information%20loss%20or%20increased%20noise.%20Advanced%20models%20such%0Aas%20time%20series%20generation%20models%2C%20Super-Resolution%20models%20and%20imputation%20models%0Ashow%20potential%2C%20but%20also%20face%20fundamental%20challenges.%20The%20goal%20of%20time%20series%0Agenerative%20models%20is%20to%20learn%20the%20distribution%20of%20the%20original%20data%20to%20generate%0Ahigh-resolution%20series%20with%20similar%20statistical%20characteristics.%20This%20is%20not%0Aentirely%20consistent%20with%20the%20definition%20of%20upsampling.%20Time%20series%0ASuper-Resolution%20models%20or%20imputation%20models%20can%20degrade%20the%20accuracy%20of%0Aupsampling%20because%20the%20input%20low-resolution%20time%20series%20are%20sparse%20and%20may%20have%0Ainsufficient%20context.%20Moreover%2C%20such%20models%20usually%20rely%20on%20supervised%20learning%0Aparadigms.%20This%20presents%20a%20fundamental%20application%20paradox%3A%20their%20training%0Arequires%20the%20high-resolution%20time%20series%20that%20is%20intrinsically%20absent%20in%0Aupsampling%20application%20scenarios.%20To%20address%20the%20mentioned%20upsampling%20issue%2C%0Athis%20paper%20introduces%20a%20new%20method%20utilizing%20Generative%20Adversarial%0ATransformers%20%28GATs%29%2C%20which%20can%20be%20trained%20without%20access%20to%20any%20ground-truth%0Ahigh-resolution%20data.%20Compared%20with%20conventional%20interpolation%20methods%2C%20the%0Aintroduced%20method%20can%20reduce%20the%20root%20mean%20square%20error%20%28RMSE%29%20of%20upsampling%0Atasks%20by%209%25%2C%20and%20the%20accuracy%20of%20a%20model%20predictive%20control%20%28MPC%29%20application%0Ascenario%20is%20improved%20by%2013%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10587v1&entry.124074799=Read"},
{"title": "NAVER: A Neuro-Symbolic Compositional Automaton for Visual Grounding\n  with Explicit Logic Reasoning", "author": "Zhixi Cai and Fucai Ke and Simindokht Jahangard and Maria Garcia de la Banda and Reza Haffari and Peter J. Stuckey and Hamid Rezatofighi", "abstract": "  Visual Grounding (VG) tasks, such as referring expression detection and\nsegmentation tasks are important for linking visual entities to context,\nespecially in complex reasoning tasks that require detailed query\ninterpretation. This paper explores VG beyond basic perception, highlighting\nchallenges for methods that require reasoning like human cognition. Recent\nadvances in large language methods (LLMs) and Vision-Language methods (VLMs)\nhave improved abilities for visual comprehension, contextual understanding, and\nreasoning. These methods are mainly split into end-to-end and compositional\nmethods, with the latter offering more flexibility. Compositional approaches\nthat integrate LLMs and foundation models show promising performance but still\nstruggle with complex reasoning with language-based logical representations. To\naddress these limitations, we propose NAVER, a compositional visual grounding\nmethod that integrates explicit probabilistic logic reasoning within a\nfinite-state automaton, equipped with a self-correcting mechanism. This design\nimproves robustness and interpretability in inference through explicit logic\nreasoning. Our results show that NAVER achieves SoTA performance comparing to\nrecent end-to-end and compositional baselines. The code is available at\nhttps://github.com/ControlNet/NAVER .\n", "link": "http://arxiv.org/abs/2502.00372v3", "date": "2025-08-14", "relevancy": 2.2349, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.564}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NAVER%3A%20A%20Neuro-Symbolic%20Compositional%20Automaton%20for%20Visual%20Grounding%0A%20%20with%20Explicit%20Logic%20Reasoning&body=Title%3A%20NAVER%3A%20A%20Neuro-Symbolic%20Compositional%20Automaton%20for%20Visual%20Grounding%0A%20%20with%20Explicit%20Logic%20Reasoning%0AAuthor%3A%20Zhixi%20Cai%20and%20Fucai%20Ke%20and%20Simindokht%20Jahangard%20and%20Maria%20Garcia%20de%20la%20Banda%20and%20Reza%20Haffari%20and%20Peter%20J.%20Stuckey%20and%20Hamid%20Rezatofighi%0AAbstract%3A%20%20%20Visual%20Grounding%20%28VG%29%20tasks%2C%20such%20as%20referring%20expression%20detection%20and%0Asegmentation%20tasks%20are%20important%20for%20linking%20visual%20entities%20to%20context%2C%0Aespecially%20in%20complex%20reasoning%20tasks%20that%20require%20detailed%20query%0Ainterpretation.%20This%20paper%20explores%20VG%20beyond%20basic%20perception%2C%20highlighting%0Achallenges%20for%20methods%20that%20require%20reasoning%20like%20human%20cognition.%20Recent%0Aadvances%20in%20large%20language%20methods%20%28LLMs%29%20and%20Vision-Language%20methods%20%28VLMs%29%0Ahave%20improved%20abilities%20for%20visual%20comprehension%2C%20contextual%20understanding%2C%20and%0Areasoning.%20These%20methods%20are%20mainly%20split%20into%20end-to-end%20and%20compositional%0Amethods%2C%20with%20the%20latter%20offering%20more%20flexibility.%20Compositional%20approaches%0Athat%20integrate%20LLMs%20and%20foundation%20models%20show%20promising%20performance%20but%20still%0Astruggle%20with%20complex%20reasoning%20with%20language-based%20logical%20representations.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20NAVER%2C%20a%20compositional%20visual%20grounding%0Amethod%20that%20integrates%20explicit%20probabilistic%20logic%20reasoning%20within%20a%0Afinite-state%20automaton%2C%20equipped%20with%20a%20self-correcting%20mechanism.%20This%20design%0Aimproves%20robustness%20and%20interpretability%20in%20inference%20through%20explicit%20logic%0Areasoning.%20Our%20results%20show%20that%20NAVER%20achieves%20SoTA%20performance%20comparing%20to%0Arecent%20end-to-end%20and%20compositional%20baselines.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ControlNet/NAVER%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00372v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNAVER%253A%2520A%2520Neuro-Symbolic%2520Compositional%2520Automaton%2520for%2520Visual%2520Grounding%250A%2520%2520with%2520Explicit%2520Logic%2520Reasoning%26entry.906535625%3DZhixi%2520Cai%2520and%2520Fucai%2520Ke%2520and%2520Simindokht%2520Jahangard%2520and%2520Maria%2520Garcia%2520de%2520la%2520Banda%2520and%2520Reza%2520Haffari%2520and%2520Peter%2520J.%2520Stuckey%2520and%2520Hamid%2520Rezatofighi%26entry.1292438233%3D%2520%2520Visual%2520Grounding%2520%2528VG%2529%2520tasks%252C%2520such%2520as%2520referring%2520expression%2520detection%2520and%250Asegmentation%2520tasks%2520are%2520important%2520for%2520linking%2520visual%2520entities%2520to%2520context%252C%250Aespecially%2520in%2520complex%2520reasoning%2520tasks%2520that%2520require%2520detailed%2520query%250Ainterpretation.%2520This%2520paper%2520explores%2520VG%2520beyond%2520basic%2520perception%252C%2520highlighting%250Achallenges%2520for%2520methods%2520that%2520require%2520reasoning%2520like%2520human%2520cognition.%2520Recent%250Aadvances%2520in%2520large%2520language%2520methods%2520%2528LLMs%2529%2520and%2520Vision-Language%2520methods%2520%2528VLMs%2529%250Ahave%2520improved%2520abilities%2520for%2520visual%2520comprehension%252C%2520contextual%2520understanding%252C%2520and%250Areasoning.%2520These%2520methods%2520are%2520mainly%2520split%2520into%2520end-to-end%2520and%2520compositional%250Amethods%252C%2520with%2520the%2520latter%2520offering%2520more%2520flexibility.%2520Compositional%2520approaches%250Athat%2520integrate%2520LLMs%2520and%2520foundation%2520models%2520show%2520promising%2520performance%2520but%2520still%250Astruggle%2520with%2520complex%2520reasoning%2520with%2520language-based%2520logical%2520representations.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520NAVER%252C%2520a%2520compositional%2520visual%2520grounding%250Amethod%2520that%2520integrates%2520explicit%2520probabilistic%2520logic%2520reasoning%2520within%2520a%250Afinite-state%2520automaton%252C%2520equipped%2520with%2520a%2520self-correcting%2520mechanism.%2520This%2520design%250Aimproves%2520robustness%2520and%2520interpretability%2520in%2520inference%2520through%2520explicit%2520logic%250Areasoning.%2520Our%2520results%2520show%2520that%2520NAVER%2520achieves%2520SoTA%2520performance%2520comparing%2520to%250Arecent%2520end-to-end%2520and%2520compositional%2520baselines.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ControlNet/NAVER%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00372v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NAVER%3A%20A%20Neuro-Symbolic%20Compositional%20Automaton%20for%20Visual%20Grounding%0A%20%20with%20Explicit%20Logic%20Reasoning&entry.906535625=Zhixi%20Cai%20and%20Fucai%20Ke%20and%20Simindokht%20Jahangard%20and%20Maria%20Garcia%20de%20la%20Banda%20and%20Reza%20Haffari%20and%20Peter%20J.%20Stuckey%20and%20Hamid%20Rezatofighi&entry.1292438233=%20%20Visual%20Grounding%20%28VG%29%20tasks%2C%20such%20as%20referring%20expression%20detection%20and%0Asegmentation%20tasks%20are%20important%20for%20linking%20visual%20entities%20to%20context%2C%0Aespecially%20in%20complex%20reasoning%20tasks%20that%20require%20detailed%20query%0Ainterpretation.%20This%20paper%20explores%20VG%20beyond%20basic%20perception%2C%20highlighting%0Achallenges%20for%20methods%20that%20require%20reasoning%20like%20human%20cognition.%20Recent%0Aadvances%20in%20large%20language%20methods%20%28LLMs%29%20and%20Vision-Language%20methods%20%28VLMs%29%0Ahave%20improved%20abilities%20for%20visual%20comprehension%2C%20contextual%20understanding%2C%20and%0Areasoning.%20These%20methods%20are%20mainly%20split%20into%20end-to-end%20and%20compositional%0Amethods%2C%20with%20the%20latter%20offering%20more%20flexibility.%20Compositional%20approaches%0Athat%20integrate%20LLMs%20and%20foundation%20models%20show%20promising%20performance%20but%20still%0Astruggle%20with%20complex%20reasoning%20with%20language-based%20logical%20representations.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20NAVER%2C%20a%20compositional%20visual%20grounding%0Amethod%20that%20integrates%20explicit%20probabilistic%20logic%20reasoning%20within%20a%0Afinite-state%20automaton%2C%20equipped%20with%20a%20self-correcting%20mechanism.%20This%20design%0Aimproves%20robustness%20and%20interpretability%20in%20inference%20through%20explicit%20logic%0Areasoning.%20Our%20results%20show%20that%20NAVER%20achieves%20SoTA%20performance%20comparing%20to%0Arecent%20end-to-end%20and%20compositional%20baselines.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ControlNet/NAVER%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00372v3&entry.124074799=Read"},
{"title": "Self-Supervised Stereo Matching with Multi-Baseline Contrastive Learning", "author": "Peng Xu and Zhiyu Xiang and Jingyun Fu and Tianyu Pu and Kai Wang and Chaojie Ji and Tingming Bai and Eryun Liu", "abstract": "  Current self-supervised stereo matching relies on the photometric consistency\nassumption, which breaks down in occluded regions due to ill-posed\ncorrespondences. To address this issue, we propose BaCon-Stereo, a simple yet\neffective contrastive learning framework for self-supervised stereo network\ntraining in both non-occluded and occluded regions. We adopt a teacher-student\nparadigm with multi-baseline inputs, in which the stereo pairs fed into the\nteacher and student share the same reference view but differ in target views.\nGeometrically, regions occluded in the student's target view are often visible\nin the teacher's, making it easier for the teacher to predict in these regions.\nThe teacher's prediction is rescaled to match the student's baseline and then\nused to supervise the student. We also introduce an occlusion-aware attention\nmap to better guide the student in learning occlusion completion. To support\ntraining, we synthesize a multi-baseline dataset BaCon-20k. Extensive\nexperiments demonstrate that BaCon-Stereo improves prediction in both occluded\nand non-occluded regions, achieves strong generalization and robustness, and\noutperforms state-of-the-art self-supervised methods on both KITTI 2015 and\n2012 benchmarks. Our code and dataset will be released upon paper acceptance.\n", "link": "http://arxiv.org/abs/2508.10838v1", "date": "2025-08-14", "relevancy": 2.23, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5654}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5606}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Stereo%20Matching%20with%20Multi-Baseline%20Contrastive%20Learning&body=Title%3A%20Self-Supervised%20Stereo%20Matching%20with%20Multi-Baseline%20Contrastive%20Learning%0AAuthor%3A%20Peng%20Xu%20and%20Zhiyu%20Xiang%20and%20Jingyun%20Fu%20and%20Tianyu%20Pu%20and%20Kai%20Wang%20and%20Chaojie%20Ji%20and%20Tingming%20Bai%20and%20Eryun%20Liu%0AAbstract%3A%20%20%20Current%20self-supervised%20stereo%20matching%20relies%20on%20the%20photometric%20consistency%0Aassumption%2C%20which%20breaks%20down%20in%20occluded%20regions%20due%20to%20ill-posed%0Acorrespondences.%20To%20address%20this%20issue%2C%20we%20propose%20BaCon-Stereo%2C%20a%20simple%20yet%0Aeffective%20contrastive%20learning%20framework%20for%20self-supervised%20stereo%20network%0Atraining%20in%20both%20non-occluded%20and%20occluded%20regions.%20We%20adopt%20a%20teacher-student%0Aparadigm%20with%20multi-baseline%20inputs%2C%20in%20which%20the%20stereo%20pairs%20fed%20into%20the%0Ateacher%20and%20student%20share%20the%20same%20reference%20view%20but%20differ%20in%20target%20views.%0AGeometrically%2C%20regions%20occluded%20in%20the%20student%27s%20target%20view%20are%20often%20visible%0Ain%20the%20teacher%27s%2C%20making%20it%20easier%20for%20the%20teacher%20to%20predict%20in%20these%20regions.%0AThe%20teacher%27s%20prediction%20is%20rescaled%20to%20match%20the%20student%27s%20baseline%20and%20then%0Aused%20to%20supervise%20the%20student.%20We%20also%20introduce%20an%20occlusion-aware%20attention%0Amap%20to%20better%20guide%20the%20student%20in%20learning%20occlusion%20completion.%20To%20support%0Atraining%2C%20we%20synthesize%20a%20multi-baseline%20dataset%20BaCon-20k.%20Extensive%0Aexperiments%20demonstrate%20that%20BaCon-Stereo%20improves%20prediction%20in%20both%20occluded%0Aand%20non-occluded%20regions%2C%20achieves%20strong%20generalization%20and%20robustness%2C%20and%0Aoutperforms%20state-of-the-art%20self-supervised%20methods%20on%20both%20KITTI%202015%20and%0A2012%20benchmarks.%20Our%20code%20and%20dataset%20will%20be%20released%20upon%20paper%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Stereo%2520Matching%2520with%2520Multi-Baseline%2520Contrastive%2520Learning%26entry.906535625%3DPeng%2520Xu%2520and%2520Zhiyu%2520Xiang%2520and%2520Jingyun%2520Fu%2520and%2520Tianyu%2520Pu%2520and%2520Kai%2520Wang%2520and%2520Chaojie%2520Ji%2520and%2520Tingming%2520Bai%2520and%2520Eryun%2520Liu%26entry.1292438233%3D%2520%2520Current%2520self-supervised%2520stereo%2520matching%2520relies%2520on%2520the%2520photometric%2520consistency%250Aassumption%252C%2520which%2520breaks%2520down%2520in%2520occluded%2520regions%2520due%2520to%2520ill-posed%250Acorrespondences.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520BaCon-Stereo%252C%2520a%2520simple%2520yet%250Aeffective%2520contrastive%2520learning%2520framework%2520for%2520self-supervised%2520stereo%2520network%250Atraining%2520in%2520both%2520non-occluded%2520and%2520occluded%2520regions.%2520We%2520adopt%2520a%2520teacher-student%250Aparadigm%2520with%2520multi-baseline%2520inputs%252C%2520in%2520which%2520the%2520stereo%2520pairs%2520fed%2520into%2520the%250Ateacher%2520and%2520student%2520share%2520the%2520same%2520reference%2520view%2520but%2520differ%2520in%2520target%2520views.%250AGeometrically%252C%2520regions%2520occluded%2520in%2520the%2520student%2527s%2520target%2520view%2520are%2520often%2520visible%250Ain%2520the%2520teacher%2527s%252C%2520making%2520it%2520easier%2520for%2520the%2520teacher%2520to%2520predict%2520in%2520these%2520regions.%250AThe%2520teacher%2527s%2520prediction%2520is%2520rescaled%2520to%2520match%2520the%2520student%2527s%2520baseline%2520and%2520then%250Aused%2520to%2520supervise%2520the%2520student.%2520We%2520also%2520introduce%2520an%2520occlusion-aware%2520attention%250Amap%2520to%2520better%2520guide%2520the%2520student%2520in%2520learning%2520occlusion%2520completion.%2520To%2520support%250Atraining%252C%2520we%2520synthesize%2520a%2520multi-baseline%2520dataset%2520BaCon-20k.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520BaCon-Stereo%2520improves%2520prediction%2520in%2520both%2520occluded%250Aand%2520non-occluded%2520regions%252C%2520achieves%2520strong%2520generalization%2520and%2520robustness%252C%2520and%250Aoutperforms%2520state-of-the-art%2520self-supervised%2520methods%2520on%2520both%2520KITTI%25202015%2520and%250A2012%2520benchmarks.%2520Our%2520code%2520and%2520dataset%2520will%2520be%2520released%2520upon%2520paper%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Stereo%20Matching%20with%20Multi-Baseline%20Contrastive%20Learning&entry.906535625=Peng%20Xu%20and%20Zhiyu%20Xiang%20and%20Jingyun%20Fu%20and%20Tianyu%20Pu%20and%20Kai%20Wang%20and%20Chaojie%20Ji%20and%20Tingming%20Bai%20and%20Eryun%20Liu&entry.1292438233=%20%20Current%20self-supervised%20stereo%20matching%20relies%20on%20the%20photometric%20consistency%0Aassumption%2C%20which%20breaks%20down%20in%20occluded%20regions%20due%20to%20ill-posed%0Acorrespondences.%20To%20address%20this%20issue%2C%20we%20propose%20BaCon-Stereo%2C%20a%20simple%20yet%0Aeffective%20contrastive%20learning%20framework%20for%20self-supervised%20stereo%20network%0Atraining%20in%20both%20non-occluded%20and%20occluded%20regions.%20We%20adopt%20a%20teacher-student%0Aparadigm%20with%20multi-baseline%20inputs%2C%20in%20which%20the%20stereo%20pairs%20fed%20into%20the%0Ateacher%20and%20student%20share%20the%20same%20reference%20view%20but%20differ%20in%20target%20views.%0AGeometrically%2C%20regions%20occluded%20in%20the%20student%27s%20target%20view%20are%20often%20visible%0Ain%20the%20teacher%27s%2C%20making%20it%20easier%20for%20the%20teacher%20to%20predict%20in%20these%20regions.%0AThe%20teacher%27s%20prediction%20is%20rescaled%20to%20match%20the%20student%27s%20baseline%20and%20then%0Aused%20to%20supervise%20the%20student.%20We%20also%20introduce%20an%20occlusion-aware%20attention%0Amap%20to%20better%20guide%20the%20student%20in%20learning%20occlusion%20completion.%20To%20support%0Atraining%2C%20we%20synthesize%20a%20multi-baseline%20dataset%20BaCon-20k.%20Extensive%0Aexperiments%20demonstrate%20that%20BaCon-Stereo%20improves%20prediction%20in%20both%20occluded%0Aand%20non-occluded%20regions%2C%20achieves%20strong%20generalization%20and%20robustness%2C%20and%0Aoutperforms%20state-of-the-art%20self-supervised%20methods%20on%20both%20KITTI%202015%20and%0A2012%20benchmarks.%20Our%20code%20and%20dataset%20will%20be%20released%20upon%20paper%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10838v1&entry.124074799=Read"},
{"title": "A Linear N-Point Solver for Structure and Motion from Asynchronous\n  Tracks", "author": "Hang Su and Yunlong Feng and Daniel Gehrig and Panfeng Jiang and Ling Gao and Xavier Lagorce and Laurent Kneip", "abstract": "  Structure and continuous motion estimation from point correspondences is a\nfundamental problem in computer vision that has been powered by well-known\nalgorithms such as the familiar 5-point or 8-point algorithm. However, despite\ntheir acclaim, these algorithms are limited to processing point correspondences\noriginating from a pair of views each one representing an instantaneous capture\nof the scene. Yet, in the case of rolling shutter cameras, or more recently,\nevent cameras, this synchronization breaks down. In this work, we present a\nunified approach for structure and linear motion estimation from 2D point\ncorrespondences with arbitrary timestamps, from an arbitrary set of views. By\nformulating the problem in terms of first-order dynamics and leveraging a\nconstant velocity motion model, we derive a novel, linear point incidence\nrelation allowing for the efficient recovery of both linear velocity and 3D\npoints with predictable degeneracies and solution multiplicities. Owing to its\ngeneral formulation, it can handle correspondences from a wide range of sensing\nmodalities such as global shutter, rolling shutter, and event cameras, and can\neven combine correspondences from different collocated sensors. We validate the\neffectiveness of our solver on both simulated and real-world data, where we\nshow consistent improvement across all modalities when compared to recent\napproaches. We believe our work opens the door to efficient structure and\nmotion estimation from asynchronous data. Code can be found at\nhttps://github.com/suhang99/AsyncTrack-Motion-Solver.\n", "link": "http://arxiv.org/abs/2507.22733v2", "date": "2025-08-14", "relevancy": 2.2258, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5687}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5592}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Linear%20N-Point%20Solver%20for%20Structure%20and%20Motion%20from%20Asynchronous%0A%20%20Tracks&body=Title%3A%20A%20Linear%20N-Point%20Solver%20for%20Structure%20and%20Motion%20from%20Asynchronous%0A%20%20Tracks%0AAuthor%3A%20Hang%20Su%20and%20Yunlong%20Feng%20and%20Daniel%20Gehrig%20and%20Panfeng%20Jiang%20and%20Ling%20Gao%20and%20Xavier%20Lagorce%20and%20Laurent%20Kneip%0AAbstract%3A%20%20%20Structure%20and%20continuous%20motion%20estimation%20from%20point%20correspondences%20is%20a%0Afundamental%20problem%20in%20computer%20vision%20that%20has%20been%20powered%20by%20well-known%0Aalgorithms%20such%20as%20the%20familiar%205-point%20or%208-point%20algorithm.%20However%2C%20despite%0Atheir%20acclaim%2C%20these%20algorithms%20are%20limited%20to%20processing%20point%20correspondences%0Aoriginating%20from%20a%20pair%20of%20views%20each%20one%20representing%20an%20instantaneous%20capture%0Aof%20the%20scene.%20Yet%2C%20in%20the%20case%20of%20rolling%20shutter%20cameras%2C%20or%20more%20recently%2C%0Aevent%20cameras%2C%20this%20synchronization%20breaks%20down.%20In%20this%20work%2C%20we%20present%20a%0Aunified%20approach%20for%20structure%20and%20linear%20motion%20estimation%20from%202D%20point%0Acorrespondences%20with%20arbitrary%20timestamps%2C%20from%20an%20arbitrary%20set%20of%20views.%20By%0Aformulating%20the%20problem%20in%20terms%20of%20first-order%20dynamics%20and%20leveraging%20a%0Aconstant%20velocity%20motion%20model%2C%20we%20derive%20a%20novel%2C%20linear%20point%20incidence%0Arelation%20allowing%20for%20the%20efficient%20recovery%20of%20both%20linear%20velocity%20and%203D%0Apoints%20with%20predictable%20degeneracies%20and%20solution%20multiplicities.%20Owing%20to%20its%0Ageneral%20formulation%2C%20it%20can%20handle%20correspondences%20from%20a%20wide%20range%20of%20sensing%0Amodalities%20such%20as%20global%20shutter%2C%20rolling%20shutter%2C%20and%20event%20cameras%2C%20and%20can%0Aeven%20combine%20correspondences%20from%20different%20collocated%20sensors.%20We%20validate%20the%0Aeffectiveness%20of%20our%20solver%20on%20both%20simulated%20and%20real-world%20data%2C%20where%20we%0Ashow%20consistent%20improvement%20across%20all%20modalities%20when%20compared%20to%20recent%0Aapproaches.%20We%20believe%20our%20work%20opens%20the%20door%20to%20efficient%20structure%20and%0Amotion%20estimation%20from%20asynchronous%20data.%20Code%20can%20be%20found%20at%0Ahttps%3A//github.com/suhang99/AsyncTrack-Motion-Solver.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22733v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Linear%2520N-Point%2520Solver%2520for%2520Structure%2520and%2520Motion%2520from%2520Asynchronous%250A%2520%2520Tracks%26entry.906535625%3DHang%2520Su%2520and%2520Yunlong%2520Feng%2520and%2520Daniel%2520Gehrig%2520and%2520Panfeng%2520Jiang%2520and%2520Ling%2520Gao%2520and%2520Xavier%2520Lagorce%2520and%2520Laurent%2520Kneip%26entry.1292438233%3D%2520%2520Structure%2520and%2520continuous%2520motion%2520estimation%2520from%2520point%2520correspondences%2520is%2520a%250Afundamental%2520problem%2520in%2520computer%2520vision%2520that%2520has%2520been%2520powered%2520by%2520well-known%250Aalgorithms%2520such%2520as%2520the%2520familiar%25205-point%2520or%25208-point%2520algorithm.%2520However%252C%2520despite%250Atheir%2520acclaim%252C%2520these%2520algorithms%2520are%2520limited%2520to%2520processing%2520point%2520correspondences%250Aoriginating%2520from%2520a%2520pair%2520of%2520views%2520each%2520one%2520representing%2520an%2520instantaneous%2520capture%250Aof%2520the%2520scene.%2520Yet%252C%2520in%2520the%2520case%2520of%2520rolling%2520shutter%2520cameras%252C%2520or%2520more%2520recently%252C%250Aevent%2520cameras%252C%2520this%2520synchronization%2520breaks%2520down.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Aunified%2520approach%2520for%2520structure%2520and%2520linear%2520motion%2520estimation%2520from%25202D%2520point%250Acorrespondences%2520with%2520arbitrary%2520timestamps%252C%2520from%2520an%2520arbitrary%2520set%2520of%2520views.%2520By%250Aformulating%2520the%2520problem%2520in%2520terms%2520of%2520first-order%2520dynamics%2520and%2520leveraging%2520a%250Aconstant%2520velocity%2520motion%2520model%252C%2520we%2520derive%2520a%2520novel%252C%2520linear%2520point%2520incidence%250Arelation%2520allowing%2520for%2520the%2520efficient%2520recovery%2520of%2520both%2520linear%2520velocity%2520and%25203D%250Apoints%2520with%2520predictable%2520degeneracies%2520and%2520solution%2520multiplicities.%2520Owing%2520to%2520its%250Ageneral%2520formulation%252C%2520it%2520can%2520handle%2520correspondences%2520from%2520a%2520wide%2520range%2520of%2520sensing%250Amodalities%2520such%2520as%2520global%2520shutter%252C%2520rolling%2520shutter%252C%2520and%2520event%2520cameras%252C%2520and%2520can%250Aeven%2520combine%2520correspondences%2520from%2520different%2520collocated%2520sensors.%2520We%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520solver%2520on%2520both%2520simulated%2520and%2520real-world%2520data%252C%2520where%2520we%250Ashow%2520consistent%2520improvement%2520across%2520all%2520modalities%2520when%2520compared%2520to%2520recent%250Aapproaches.%2520We%2520believe%2520our%2520work%2520opens%2520the%2520door%2520to%2520efficient%2520structure%2520and%250Amotion%2520estimation%2520from%2520asynchronous%2520data.%2520Code%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/suhang99/AsyncTrack-Motion-Solver.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22733v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Linear%20N-Point%20Solver%20for%20Structure%20and%20Motion%20from%20Asynchronous%0A%20%20Tracks&entry.906535625=Hang%20Su%20and%20Yunlong%20Feng%20and%20Daniel%20Gehrig%20and%20Panfeng%20Jiang%20and%20Ling%20Gao%20and%20Xavier%20Lagorce%20and%20Laurent%20Kneip&entry.1292438233=%20%20Structure%20and%20continuous%20motion%20estimation%20from%20point%20correspondences%20is%20a%0Afundamental%20problem%20in%20computer%20vision%20that%20has%20been%20powered%20by%20well-known%0Aalgorithms%20such%20as%20the%20familiar%205-point%20or%208-point%20algorithm.%20However%2C%20despite%0Atheir%20acclaim%2C%20these%20algorithms%20are%20limited%20to%20processing%20point%20correspondences%0Aoriginating%20from%20a%20pair%20of%20views%20each%20one%20representing%20an%20instantaneous%20capture%0Aof%20the%20scene.%20Yet%2C%20in%20the%20case%20of%20rolling%20shutter%20cameras%2C%20or%20more%20recently%2C%0Aevent%20cameras%2C%20this%20synchronization%20breaks%20down.%20In%20this%20work%2C%20we%20present%20a%0Aunified%20approach%20for%20structure%20and%20linear%20motion%20estimation%20from%202D%20point%0Acorrespondences%20with%20arbitrary%20timestamps%2C%20from%20an%20arbitrary%20set%20of%20views.%20By%0Aformulating%20the%20problem%20in%20terms%20of%20first-order%20dynamics%20and%20leveraging%20a%0Aconstant%20velocity%20motion%20model%2C%20we%20derive%20a%20novel%2C%20linear%20point%20incidence%0Arelation%20allowing%20for%20the%20efficient%20recovery%20of%20both%20linear%20velocity%20and%203D%0Apoints%20with%20predictable%20degeneracies%20and%20solution%20multiplicities.%20Owing%20to%20its%0Ageneral%20formulation%2C%20it%20can%20handle%20correspondences%20from%20a%20wide%20range%20of%20sensing%0Amodalities%20such%20as%20global%20shutter%2C%20rolling%20shutter%2C%20and%20event%20cameras%2C%20and%20can%0Aeven%20combine%20correspondences%20from%20different%20collocated%20sensors.%20We%20validate%20the%0Aeffectiveness%20of%20our%20solver%20on%20both%20simulated%20and%20real-world%20data%2C%20where%20we%0Ashow%20consistent%20improvement%20across%20all%20modalities%20when%20compared%20to%20recent%0Aapproaches.%20We%20believe%20our%20work%20opens%20the%20door%20to%20efficient%20structure%20and%0Amotion%20estimation%20from%20asynchronous%20data.%20Code%20can%20be%20found%20at%0Ahttps%3A//github.com/suhang99/AsyncTrack-Motion-Solver.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22733v2&entry.124074799=Read"},
{"title": "EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain\n  Egocentric Video Question Answering", "author": "Yanjun Li and Yuqian Fu and Tianwen Qian and Qi'ao Xu and Silong Dai and Danda Pani Paudel and Luc Van Gool and Xiaoling Wang", "abstract": "  Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly pushed the frontier of egocentric video question answering\n(EgocentricQA). However, existing benchmarks and studies are mainly limited to\ncommon daily activities such as cooking and cleaning. In contrast, real-world\ndeployment inevitably encounters domain shifts, where target domains differ\nsubstantially in both visual style and semantic content. To bridge this gap, we\nintroduce \\textbf{EgoCross}, a comprehensive benchmark designed to evaluate the\ncross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four\ndiverse and challenging domains, including surgery, industry, extreme sports,\nand animal perspective, representing realistic and high-impact application\nscenarios. It comprises approximately 1,000 QA pairs across 798 video clips,\nspanning four key QA tasks: prediction, recognition, localization, and\ncounting. Each QA pair provides both OpenQA and CloseQA formats to support\nfine-grained evaluation. Extensive experiments show that most existing MLLMs,\nwhether general-purpose or egocentric-specialized, struggle to generalize to\ndomains beyond daily life, highlighting the limitations of current models.\nFurthermore, we conduct several pilot studies, \\eg, fine-tuning and\nreinforcement learning, to explore potential improvements. We hope EgoCross and\nour accompanying analysis will serve as a foundation for advancing\ndomain-adaptive, robust egocentric video understanding. Data and codes will be\nreleased at:\n\\href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}\n", "link": "http://arxiv.org/abs/2508.10729v1", "date": "2025-08-14", "relevancy": 2.2229, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5642}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.554}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoCross%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Cross-Domain%0A%20%20Egocentric%20Video%20Question%20Answering&body=Title%3A%20EgoCross%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Cross-Domain%0A%20%20Egocentric%20Video%20Question%20Answering%0AAuthor%3A%20Yanjun%20Li%20and%20Yuqian%20Fu%20and%20Tianwen%20Qian%20and%20Qi%27ao%20Xu%20and%20Silong%20Dai%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool%20and%20Xiaoling%20Wang%0AAbstract%3A%20%20%20Recent%20advances%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%0Asignificantly%20pushed%20the%20frontier%20of%20egocentric%20video%20question%20answering%0A%28EgocentricQA%29.%20However%2C%20existing%20benchmarks%20and%20studies%20are%20mainly%20limited%20to%0Acommon%20daily%20activities%20such%20as%20cooking%20and%20cleaning.%20In%20contrast%2C%20real-world%0Adeployment%20inevitably%20encounters%20domain%20shifts%2C%20where%20target%20domains%20differ%0Asubstantially%20in%20both%20visual%20style%20and%20semantic%20content.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20%5Ctextbf%7BEgoCross%7D%2C%20a%20comprehensive%20benchmark%20designed%20to%20evaluate%20the%0Across-domain%20generalization%20of%20MLLMs%20in%20EgocentricQA.%20EgoCross%20covers%20four%0Adiverse%20and%20challenging%20domains%2C%20including%20surgery%2C%20industry%2C%20extreme%20sports%2C%0Aand%20animal%20perspective%2C%20representing%20realistic%20and%20high-impact%20application%0Ascenarios.%20It%20comprises%20approximately%201%2C000%20QA%20pairs%20across%20798%20video%20clips%2C%0Aspanning%20four%20key%20QA%20tasks%3A%20prediction%2C%20recognition%2C%20localization%2C%20and%0Acounting.%20Each%20QA%20pair%20provides%20both%20OpenQA%20and%20CloseQA%20formats%20to%20support%0Afine-grained%20evaluation.%20Extensive%20experiments%20show%20that%20most%20existing%20MLLMs%2C%0Awhether%20general-purpose%20or%20egocentric-specialized%2C%20struggle%20to%20generalize%20to%0Adomains%20beyond%20daily%20life%2C%20highlighting%20the%20limitations%20of%20current%20models.%0AFurthermore%2C%20we%20conduct%20several%20pilot%20studies%2C%20%5Ceg%2C%20fine-tuning%20and%0Areinforcement%20learning%2C%20to%20explore%20potential%20improvements.%20We%20hope%20EgoCross%20and%0Aour%20accompanying%20analysis%20will%20serve%20as%20a%20foundation%20for%20advancing%0Adomain-adaptive%2C%20robust%20egocentric%20video%20understanding.%20Data%20and%20codes%20will%20be%0Areleased%20at%3A%0A%5Chref%7Bhttps%3A//github.com/MyUniverse0726/EgoCross%7D%7Bhttps%3A//github.com/MyUniverse0726/EgoCross.%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoCross%253A%2520Benchmarking%2520Multimodal%2520Large%2520Language%2520Models%2520for%2520Cross-Domain%250A%2520%2520Egocentric%2520Video%2520Question%2520Answering%26entry.906535625%3DYanjun%2520Li%2520and%2520Yuqian%2520Fu%2520and%2520Tianwen%2520Qian%2520and%2520Qi%2527ao%2520Xu%2520and%2520Silong%2520Dai%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Luc%2520Van%2520Gool%2520and%2520Xiaoling%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%250Asignificantly%2520pushed%2520the%2520frontier%2520of%2520egocentric%2520video%2520question%2520answering%250A%2528EgocentricQA%2529.%2520However%252C%2520existing%2520benchmarks%2520and%2520studies%2520are%2520mainly%2520limited%2520to%250Acommon%2520daily%2520activities%2520such%2520as%2520cooking%2520and%2520cleaning.%2520In%2520contrast%252C%2520real-world%250Adeployment%2520inevitably%2520encounters%2520domain%2520shifts%252C%2520where%2520target%2520domains%2520differ%250Asubstantially%2520in%2520both%2520visual%2520style%2520and%2520semantic%2520content.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aintroduce%2520%255Ctextbf%257BEgoCross%257D%252C%2520a%2520comprehensive%2520benchmark%2520designed%2520to%2520evaluate%2520the%250Across-domain%2520generalization%2520of%2520MLLMs%2520in%2520EgocentricQA.%2520EgoCross%2520covers%2520four%250Adiverse%2520and%2520challenging%2520domains%252C%2520including%2520surgery%252C%2520industry%252C%2520extreme%2520sports%252C%250Aand%2520animal%2520perspective%252C%2520representing%2520realistic%2520and%2520high-impact%2520application%250Ascenarios.%2520It%2520comprises%2520approximately%25201%252C000%2520QA%2520pairs%2520across%2520798%2520video%2520clips%252C%250Aspanning%2520four%2520key%2520QA%2520tasks%253A%2520prediction%252C%2520recognition%252C%2520localization%252C%2520and%250Acounting.%2520Each%2520QA%2520pair%2520provides%2520both%2520OpenQA%2520and%2520CloseQA%2520formats%2520to%2520support%250Afine-grained%2520evaluation.%2520Extensive%2520experiments%2520show%2520that%2520most%2520existing%2520MLLMs%252C%250Awhether%2520general-purpose%2520or%2520egocentric-specialized%252C%2520struggle%2520to%2520generalize%2520to%250Adomains%2520beyond%2520daily%2520life%252C%2520highlighting%2520the%2520limitations%2520of%2520current%2520models.%250AFurthermore%252C%2520we%2520conduct%2520several%2520pilot%2520studies%252C%2520%255Ceg%252C%2520fine-tuning%2520and%250Areinforcement%2520learning%252C%2520to%2520explore%2520potential%2520improvements.%2520We%2520hope%2520EgoCross%2520and%250Aour%2520accompanying%2520analysis%2520will%2520serve%2520as%2520a%2520foundation%2520for%2520advancing%250Adomain-adaptive%252C%2520robust%2520egocentric%2520video%2520understanding.%2520Data%2520and%2520codes%2520will%2520be%250Areleased%2520at%253A%250A%255Chref%257Bhttps%253A//github.com/MyUniverse0726/EgoCross%257D%257Bhttps%253A//github.com/MyUniverse0726/EgoCross.%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoCross%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Cross-Domain%0A%20%20Egocentric%20Video%20Question%20Answering&entry.906535625=Yanjun%20Li%20and%20Yuqian%20Fu%20and%20Tianwen%20Qian%20and%20Qi%27ao%20Xu%20and%20Silong%20Dai%20and%20Danda%20Pani%20Paudel%20and%20Luc%20Van%20Gool%20and%20Xiaoling%20Wang&entry.1292438233=%20%20Recent%20advances%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%0Asignificantly%20pushed%20the%20frontier%20of%20egocentric%20video%20question%20answering%0A%28EgocentricQA%29.%20However%2C%20existing%20benchmarks%20and%20studies%20are%20mainly%20limited%20to%0Acommon%20daily%20activities%20such%20as%20cooking%20and%20cleaning.%20In%20contrast%2C%20real-world%0Adeployment%20inevitably%20encounters%20domain%20shifts%2C%20where%20target%20domains%20differ%0Asubstantially%20in%20both%20visual%20style%20and%20semantic%20content.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20%5Ctextbf%7BEgoCross%7D%2C%20a%20comprehensive%20benchmark%20designed%20to%20evaluate%20the%0Across-domain%20generalization%20of%20MLLMs%20in%20EgocentricQA.%20EgoCross%20covers%20four%0Adiverse%20and%20challenging%20domains%2C%20including%20surgery%2C%20industry%2C%20extreme%20sports%2C%0Aand%20animal%20perspective%2C%20representing%20realistic%20and%20high-impact%20application%0Ascenarios.%20It%20comprises%20approximately%201%2C000%20QA%20pairs%20across%20798%20video%20clips%2C%0Aspanning%20four%20key%20QA%20tasks%3A%20prediction%2C%20recognition%2C%20localization%2C%20and%0Acounting.%20Each%20QA%20pair%20provides%20both%20OpenQA%20and%20CloseQA%20formats%20to%20support%0Afine-grained%20evaluation.%20Extensive%20experiments%20show%20that%20most%20existing%20MLLMs%2C%0Awhether%20general-purpose%20or%20egocentric-specialized%2C%20struggle%20to%20generalize%20to%0Adomains%20beyond%20daily%20life%2C%20highlighting%20the%20limitations%20of%20current%20models.%0AFurthermore%2C%20we%20conduct%20several%20pilot%20studies%2C%20%5Ceg%2C%20fine-tuning%20and%0Areinforcement%20learning%2C%20to%20explore%20potential%20improvements.%20We%20hope%20EgoCross%20and%0Aour%20accompanying%20analysis%20will%20serve%20as%20a%20foundation%20for%20advancing%0Adomain-adaptive%2C%20robust%20egocentric%20video%20understanding.%20Data%20and%20codes%20will%20be%0Areleased%20at%3A%0A%5Chref%7Bhttps%3A//github.com/MyUniverse0726/EgoCross%7D%7Bhttps%3A//github.com/MyUniverse0726/EgoCross.%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10729v1&entry.124074799=Read"},
{"title": "TAR: Teacher-Aligned Representations via Contrastive Learning for\n  Quadrupedal Locomotion", "author": "Amr Mousa and Neil Karavis and Michele Caprio and Wei Pan and Richard Allmendinger", "abstract": "  Quadrupedal locomotion via Reinforcement Learning (RL) is commonly addressed\nusing the teacher-student paradigm, where a privileged teacher guides a\nproprioceptive student policy. However, key challenges such as representation\nmisalignment between privileged teacher and proprioceptive-only student,\ncovariate shift due to behavioral cloning, and lack of deployable adaptation;\nlead to poor generalization in real-world scenarios. We propose Teacher-Aligned\nRepresentations via Contrastive Learning (TAR), a framework that leverages\nprivileged information with self-supervised contrastive learning to bridge this\ngap. By aligning representations to a privileged teacher in simulation via\ncontrastive objectives, our student policy learns structured latent spaces and\nexhibits robust generalization to Out-of-Distribution (OOD) scenarios,\nsurpassing the fully privileged \"Teacher\". Results showed accelerated training\nby 2x compared to state-of-the-art baselines to achieve peak performance. OOD\nscenarios showed better generalization by 40% on average compared to existing\nmethods. Moreover, TAR transitions seamlessly into learning during deployment\nwithout requiring privileged states, setting a new benchmark in\nsample-efficient, adaptive locomotion and enabling continual fine-tuning in\nreal-world scenarios. Open-source code and videos are available at\nhttps://amrmousa.com/TARLoco/.\n", "link": "http://arxiv.org/abs/2503.20839v2", "date": "2025-08-14", "relevancy": 2.2198, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5741}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5734}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAR%3A%20Teacher-Aligned%20Representations%20via%20Contrastive%20Learning%20for%0A%20%20Quadrupedal%20Locomotion&body=Title%3A%20TAR%3A%20Teacher-Aligned%20Representations%20via%20Contrastive%20Learning%20for%0A%20%20Quadrupedal%20Locomotion%0AAuthor%3A%20Amr%20Mousa%20and%20Neil%20Karavis%20and%20Michele%20Caprio%20and%20Wei%20Pan%20and%20Richard%20Allmendinger%0AAbstract%3A%20%20%20Quadrupedal%20locomotion%20via%20Reinforcement%20Learning%20%28RL%29%20is%20commonly%20addressed%0Ausing%20the%20teacher-student%20paradigm%2C%20where%20a%20privileged%20teacher%20guides%20a%0Aproprioceptive%20student%20policy.%20However%2C%20key%20challenges%20such%20as%20representation%0Amisalignment%20between%20privileged%20teacher%20and%20proprioceptive-only%20student%2C%0Acovariate%20shift%20due%20to%20behavioral%20cloning%2C%20and%20lack%20of%20deployable%20adaptation%3B%0Alead%20to%20poor%20generalization%20in%20real-world%20scenarios.%20We%20propose%20Teacher-Aligned%0ARepresentations%20via%20Contrastive%20Learning%20%28TAR%29%2C%20a%20framework%20that%20leverages%0Aprivileged%20information%20with%20self-supervised%20contrastive%20learning%20to%20bridge%20this%0Agap.%20By%20aligning%20representations%20to%20a%20privileged%20teacher%20in%20simulation%20via%0Acontrastive%20objectives%2C%20our%20student%20policy%20learns%20structured%20latent%20spaces%20and%0Aexhibits%20robust%20generalization%20to%20Out-of-Distribution%20%28OOD%29%20scenarios%2C%0Asurpassing%20the%20fully%20privileged%20%22Teacher%22.%20Results%20showed%20accelerated%20training%0Aby%202x%20compared%20to%20state-of-the-art%20baselines%20to%20achieve%20peak%20performance.%20OOD%0Ascenarios%20showed%20better%20generalization%20by%2040%25%20on%20average%20compared%20to%20existing%0Amethods.%20Moreover%2C%20TAR%20transitions%20seamlessly%20into%20learning%20during%20deployment%0Awithout%20requiring%20privileged%20states%2C%20setting%20a%20new%20benchmark%20in%0Asample-efficient%2C%20adaptive%20locomotion%20and%20enabling%20continual%20fine-tuning%20in%0Areal-world%20scenarios.%20Open-source%20code%20and%20videos%20are%20available%20at%0Ahttps%3A//amrmousa.com/TARLoco/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.20839v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAR%253A%2520Teacher-Aligned%2520Representations%2520via%2520Contrastive%2520Learning%2520for%250A%2520%2520Quadrupedal%2520Locomotion%26entry.906535625%3DAmr%2520Mousa%2520and%2520Neil%2520Karavis%2520and%2520Michele%2520Caprio%2520and%2520Wei%2520Pan%2520and%2520Richard%2520Allmendinger%26entry.1292438233%3D%2520%2520Quadrupedal%2520locomotion%2520via%2520Reinforcement%2520Learning%2520%2528RL%2529%2520is%2520commonly%2520addressed%250Ausing%2520the%2520teacher-student%2520paradigm%252C%2520where%2520a%2520privileged%2520teacher%2520guides%2520a%250Aproprioceptive%2520student%2520policy.%2520However%252C%2520key%2520challenges%2520such%2520as%2520representation%250Amisalignment%2520between%2520privileged%2520teacher%2520and%2520proprioceptive-only%2520student%252C%250Acovariate%2520shift%2520due%2520to%2520behavioral%2520cloning%252C%2520and%2520lack%2520of%2520deployable%2520adaptation%253B%250Alead%2520to%2520poor%2520generalization%2520in%2520real-world%2520scenarios.%2520We%2520propose%2520Teacher-Aligned%250ARepresentations%2520via%2520Contrastive%2520Learning%2520%2528TAR%2529%252C%2520a%2520framework%2520that%2520leverages%250Aprivileged%2520information%2520with%2520self-supervised%2520contrastive%2520learning%2520to%2520bridge%2520this%250Agap.%2520By%2520aligning%2520representations%2520to%2520a%2520privileged%2520teacher%2520in%2520simulation%2520via%250Acontrastive%2520objectives%252C%2520our%2520student%2520policy%2520learns%2520structured%2520latent%2520spaces%2520and%250Aexhibits%2520robust%2520generalization%2520to%2520Out-of-Distribution%2520%2528OOD%2529%2520scenarios%252C%250Asurpassing%2520the%2520fully%2520privileged%2520%2522Teacher%2522.%2520Results%2520showed%2520accelerated%2520training%250Aby%25202x%2520compared%2520to%2520state-of-the-art%2520baselines%2520to%2520achieve%2520peak%2520performance.%2520OOD%250Ascenarios%2520showed%2520better%2520generalization%2520by%252040%2525%2520on%2520average%2520compared%2520to%2520existing%250Amethods.%2520Moreover%252C%2520TAR%2520transitions%2520seamlessly%2520into%2520learning%2520during%2520deployment%250Awithout%2520requiring%2520privileged%2520states%252C%2520setting%2520a%2520new%2520benchmark%2520in%250Asample-efficient%252C%2520adaptive%2520locomotion%2520and%2520enabling%2520continual%2520fine-tuning%2520in%250Areal-world%2520scenarios.%2520Open-source%2520code%2520and%2520videos%2520are%2520available%2520at%250Ahttps%253A//amrmousa.com/TARLoco/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.20839v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAR%3A%20Teacher-Aligned%20Representations%20via%20Contrastive%20Learning%20for%0A%20%20Quadrupedal%20Locomotion&entry.906535625=Amr%20Mousa%20and%20Neil%20Karavis%20and%20Michele%20Caprio%20and%20Wei%20Pan%20and%20Richard%20Allmendinger&entry.1292438233=%20%20Quadrupedal%20locomotion%20via%20Reinforcement%20Learning%20%28RL%29%20is%20commonly%20addressed%0Ausing%20the%20teacher-student%20paradigm%2C%20where%20a%20privileged%20teacher%20guides%20a%0Aproprioceptive%20student%20policy.%20However%2C%20key%20challenges%20such%20as%20representation%0Amisalignment%20between%20privileged%20teacher%20and%20proprioceptive-only%20student%2C%0Acovariate%20shift%20due%20to%20behavioral%20cloning%2C%20and%20lack%20of%20deployable%20adaptation%3B%0Alead%20to%20poor%20generalization%20in%20real-world%20scenarios.%20We%20propose%20Teacher-Aligned%0ARepresentations%20via%20Contrastive%20Learning%20%28TAR%29%2C%20a%20framework%20that%20leverages%0Aprivileged%20information%20with%20self-supervised%20contrastive%20learning%20to%20bridge%20this%0Agap.%20By%20aligning%20representations%20to%20a%20privileged%20teacher%20in%20simulation%20via%0Acontrastive%20objectives%2C%20our%20student%20policy%20learns%20structured%20latent%20spaces%20and%0Aexhibits%20robust%20generalization%20to%20Out-of-Distribution%20%28OOD%29%20scenarios%2C%0Asurpassing%20the%20fully%20privileged%20%22Teacher%22.%20Results%20showed%20accelerated%20training%0Aby%202x%20compared%20to%20state-of-the-art%20baselines%20to%20achieve%20peak%20performance.%20OOD%0Ascenarios%20showed%20better%20generalization%20by%2040%25%20on%20average%20compared%20to%20existing%0Amethods.%20Moreover%2C%20TAR%20transitions%20seamlessly%20into%20learning%20during%20deployment%0Awithout%20requiring%20privileged%20states%2C%20setting%20a%20new%20benchmark%20in%0Asample-efficient%2C%20adaptive%20locomotion%20and%20enabling%20continual%20fine-tuning%20in%0Areal-world%20scenarios.%20Open-source%20code%20and%20videos%20are%20available%20at%0Ahttps%3A//amrmousa.com/TARLoco/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.20839v2&entry.124074799=Read"},
{"title": "Towards Powerful and Practical Patch Attacks for 2D Object Detection in\n  Autonomous Driving", "author": "Yuxin Cao and Yedi Zhang and Wentao He and Yifan Liao and Yan Xiao and Chang Li and Zhiyong Huang and Jin Song Dong", "abstract": "  Learning-based autonomous driving systems remain critically vulnerable to\nadversarial patches, posing serious safety and security risks in their\nreal-world deployment. Black-box attacks, notable for their high attack success\nrate without model knowledge, are especially concerning, with their\ntransferability extensively studied to reduce computational costs compared to\nquery-based attacks. Previous transferability-based black-box attacks typically\nadopt mean Average Precision (mAP) as the evaluation metric and design training\nloss accordingly. However, due to the presence of multiple detected bounding\nboxes and the relatively lenient Intersection over Union (IoU) thresholds, the\nattack effectiveness of these approaches is often overestimated, resulting in\nreduced success rates in practical attacking scenarios. Furthermore, patches\ntrained on low-resolution data often fail to maintain effectiveness on\nhigh-resolution images, limiting their transferability to autonomous driving\ndatasets. To fill this gap, we propose P$^3$A, a Powerful and Practical Patch\nAttack framework for 2D object detection in autonomous driving, specifically\noptimized for high-resolution datasets. First, we introduce a novel metric,\nPractical Attack Success Rate (PASR), to more accurately quantify attack\neffectiveness with greater relevance for pedestrian safety. Second, we present\na tailored Localization-Confidence Suppression Loss (LCSL) to improve attack\ntransferability under PASR. Finally, to maintain the transferability for\nhigh-resolution datasets, we further incorporate the Probabilistic\nScale-Preserving Padding (PSPP) into the patch attack pipeline as a data\npreprocessing step. Extensive experiments show that P$^3$A outperforms\nstate-of-the-art attacks on unseen models and unseen high-resolution datasets,\nboth under the proposed practical IoU-based evaluation metric and the previous\nmAP-based metrics.\n", "link": "http://arxiv.org/abs/2508.10600v1", "date": "2025-08-14", "relevancy": 2.2135, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5775}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5457}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Powerful%20and%20Practical%20Patch%20Attacks%20for%202D%20Object%20Detection%20in%0A%20%20Autonomous%20Driving&body=Title%3A%20Towards%20Powerful%20and%20Practical%20Patch%20Attacks%20for%202D%20Object%20Detection%20in%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Yuxin%20Cao%20and%20Yedi%20Zhang%20and%20Wentao%20He%20and%20Yifan%20Liao%20and%20Yan%20Xiao%20and%20Chang%20Li%20and%20Zhiyong%20Huang%20and%20Jin%20Song%20Dong%0AAbstract%3A%20%20%20Learning-based%20autonomous%20driving%20systems%20remain%20critically%20vulnerable%20to%0Aadversarial%20patches%2C%20posing%20serious%20safety%20and%20security%20risks%20in%20their%0Areal-world%20deployment.%20Black-box%20attacks%2C%20notable%20for%20their%20high%20attack%20success%0Arate%20without%20model%20knowledge%2C%20are%20especially%20concerning%2C%20with%20their%0Atransferability%20extensively%20studied%20to%20reduce%20computational%20costs%20compared%20to%0Aquery-based%20attacks.%20Previous%20transferability-based%20black-box%20attacks%20typically%0Aadopt%20mean%20Average%20Precision%20%28mAP%29%20as%20the%20evaluation%20metric%20and%20design%20training%0Aloss%20accordingly.%20However%2C%20due%20to%20the%20presence%20of%20multiple%20detected%20bounding%0Aboxes%20and%20the%20relatively%20lenient%20Intersection%20over%20Union%20%28IoU%29%20thresholds%2C%20the%0Aattack%20effectiveness%20of%20these%20approaches%20is%20often%20overestimated%2C%20resulting%20in%0Areduced%20success%20rates%20in%20practical%20attacking%20scenarios.%20Furthermore%2C%20patches%0Atrained%20on%20low-resolution%20data%20often%20fail%20to%20maintain%20effectiveness%20on%0Ahigh-resolution%20images%2C%20limiting%20their%20transferability%20to%20autonomous%20driving%0Adatasets.%20To%20fill%20this%20gap%2C%20we%20propose%20P%24%5E3%24A%2C%20a%20Powerful%20and%20Practical%20Patch%0AAttack%20framework%20for%202D%20object%20detection%20in%20autonomous%20driving%2C%20specifically%0Aoptimized%20for%20high-resolution%20datasets.%20First%2C%20we%20introduce%20a%20novel%20metric%2C%0APractical%20Attack%20Success%20Rate%20%28PASR%29%2C%20to%20more%20accurately%20quantify%20attack%0Aeffectiveness%20with%20greater%20relevance%20for%20pedestrian%20safety.%20Second%2C%20we%20present%0Aa%20tailored%20Localization-Confidence%20Suppression%20Loss%20%28LCSL%29%20to%20improve%20attack%0Atransferability%20under%20PASR.%20Finally%2C%20to%20maintain%20the%20transferability%20for%0Ahigh-resolution%20datasets%2C%20we%20further%20incorporate%20the%20Probabilistic%0AScale-Preserving%20Padding%20%28PSPP%29%20into%20the%20patch%20attack%20pipeline%20as%20a%20data%0Apreprocessing%20step.%20Extensive%20experiments%20show%20that%20P%24%5E3%24A%20outperforms%0Astate-of-the-art%20attacks%20on%20unseen%20models%20and%20unseen%20high-resolution%20datasets%2C%0Aboth%20under%20the%20proposed%20practical%20IoU-based%20evaluation%20metric%20and%20the%20previous%0AmAP-based%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Powerful%2520and%2520Practical%2520Patch%2520Attacks%2520for%25202D%2520Object%2520Detection%2520in%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DYuxin%2520Cao%2520and%2520Yedi%2520Zhang%2520and%2520Wentao%2520He%2520and%2520Yifan%2520Liao%2520and%2520Yan%2520Xiao%2520and%2520Chang%2520Li%2520and%2520Zhiyong%2520Huang%2520and%2520Jin%2520Song%2520Dong%26entry.1292438233%3D%2520%2520Learning-based%2520autonomous%2520driving%2520systems%2520remain%2520critically%2520vulnerable%2520to%250Aadversarial%2520patches%252C%2520posing%2520serious%2520safety%2520and%2520security%2520risks%2520in%2520their%250Areal-world%2520deployment.%2520Black-box%2520attacks%252C%2520notable%2520for%2520their%2520high%2520attack%2520success%250Arate%2520without%2520model%2520knowledge%252C%2520are%2520especially%2520concerning%252C%2520with%2520their%250Atransferability%2520extensively%2520studied%2520to%2520reduce%2520computational%2520costs%2520compared%2520to%250Aquery-based%2520attacks.%2520Previous%2520transferability-based%2520black-box%2520attacks%2520typically%250Aadopt%2520mean%2520Average%2520Precision%2520%2528mAP%2529%2520as%2520the%2520evaluation%2520metric%2520and%2520design%2520training%250Aloss%2520accordingly.%2520However%252C%2520due%2520to%2520the%2520presence%2520of%2520multiple%2520detected%2520bounding%250Aboxes%2520and%2520the%2520relatively%2520lenient%2520Intersection%2520over%2520Union%2520%2528IoU%2529%2520thresholds%252C%2520the%250Aattack%2520effectiveness%2520of%2520these%2520approaches%2520is%2520often%2520overestimated%252C%2520resulting%2520in%250Areduced%2520success%2520rates%2520in%2520practical%2520attacking%2520scenarios.%2520Furthermore%252C%2520patches%250Atrained%2520on%2520low-resolution%2520data%2520often%2520fail%2520to%2520maintain%2520effectiveness%2520on%250Ahigh-resolution%2520images%252C%2520limiting%2520their%2520transferability%2520to%2520autonomous%2520driving%250Adatasets.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%2520P%2524%255E3%2524A%252C%2520a%2520Powerful%2520and%2520Practical%2520Patch%250AAttack%2520framework%2520for%25202D%2520object%2520detection%2520in%2520autonomous%2520driving%252C%2520specifically%250Aoptimized%2520for%2520high-resolution%2520datasets.%2520First%252C%2520we%2520introduce%2520a%2520novel%2520metric%252C%250APractical%2520Attack%2520Success%2520Rate%2520%2528PASR%2529%252C%2520to%2520more%2520accurately%2520quantify%2520attack%250Aeffectiveness%2520with%2520greater%2520relevance%2520for%2520pedestrian%2520safety.%2520Second%252C%2520we%2520present%250Aa%2520tailored%2520Localization-Confidence%2520Suppression%2520Loss%2520%2528LCSL%2529%2520to%2520improve%2520attack%250Atransferability%2520under%2520PASR.%2520Finally%252C%2520to%2520maintain%2520the%2520transferability%2520for%250Ahigh-resolution%2520datasets%252C%2520we%2520further%2520incorporate%2520the%2520Probabilistic%250AScale-Preserving%2520Padding%2520%2528PSPP%2529%2520into%2520the%2520patch%2520attack%2520pipeline%2520as%2520a%2520data%250Apreprocessing%2520step.%2520Extensive%2520experiments%2520show%2520that%2520P%2524%255E3%2524A%2520outperforms%250Astate-of-the-art%2520attacks%2520on%2520unseen%2520models%2520and%2520unseen%2520high-resolution%2520datasets%252C%250Aboth%2520under%2520the%2520proposed%2520practical%2520IoU-based%2520evaluation%2520metric%2520and%2520the%2520previous%250AmAP-based%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Powerful%20and%20Practical%20Patch%20Attacks%20for%202D%20Object%20Detection%20in%0A%20%20Autonomous%20Driving&entry.906535625=Yuxin%20Cao%20and%20Yedi%20Zhang%20and%20Wentao%20He%20and%20Yifan%20Liao%20and%20Yan%20Xiao%20and%20Chang%20Li%20and%20Zhiyong%20Huang%20and%20Jin%20Song%20Dong&entry.1292438233=%20%20Learning-based%20autonomous%20driving%20systems%20remain%20critically%20vulnerable%20to%0Aadversarial%20patches%2C%20posing%20serious%20safety%20and%20security%20risks%20in%20their%0Areal-world%20deployment.%20Black-box%20attacks%2C%20notable%20for%20their%20high%20attack%20success%0Arate%20without%20model%20knowledge%2C%20are%20especially%20concerning%2C%20with%20their%0Atransferability%20extensively%20studied%20to%20reduce%20computational%20costs%20compared%20to%0Aquery-based%20attacks.%20Previous%20transferability-based%20black-box%20attacks%20typically%0Aadopt%20mean%20Average%20Precision%20%28mAP%29%20as%20the%20evaluation%20metric%20and%20design%20training%0Aloss%20accordingly.%20However%2C%20due%20to%20the%20presence%20of%20multiple%20detected%20bounding%0Aboxes%20and%20the%20relatively%20lenient%20Intersection%20over%20Union%20%28IoU%29%20thresholds%2C%20the%0Aattack%20effectiveness%20of%20these%20approaches%20is%20often%20overestimated%2C%20resulting%20in%0Areduced%20success%20rates%20in%20practical%20attacking%20scenarios.%20Furthermore%2C%20patches%0Atrained%20on%20low-resolution%20data%20often%20fail%20to%20maintain%20effectiveness%20on%0Ahigh-resolution%20images%2C%20limiting%20their%20transferability%20to%20autonomous%20driving%0Adatasets.%20To%20fill%20this%20gap%2C%20we%20propose%20P%24%5E3%24A%2C%20a%20Powerful%20and%20Practical%20Patch%0AAttack%20framework%20for%202D%20object%20detection%20in%20autonomous%20driving%2C%20specifically%0Aoptimized%20for%20high-resolution%20datasets.%20First%2C%20we%20introduce%20a%20novel%20metric%2C%0APractical%20Attack%20Success%20Rate%20%28PASR%29%2C%20to%20more%20accurately%20quantify%20attack%0Aeffectiveness%20with%20greater%20relevance%20for%20pedestrian%20safety.%20Second%2C%20we%20present%0Aa%20tailored%20Localization-Confidence%20Suppression%20Loss%20%28LCSL%29%20to%20improve%20attack%0Atransferability%20under%20PASR.%20Finally%2C%20to%20maintain%20the%20transferability%20for%0Ahigh-resolution%20datasets%2C%20we%20further%20incorporate%20the%20Probabilistic%0AScale-Preserving%20Padding%20%28PSPP%29%20into%20the%20patch%20attack%20pipeline%20as%20a%20data%0Apreprocessing%20step.%20Extensive%20experiments%20show%20that%20P%24%5E3%24A%20outperforms%0Astate-of-the-art%20attacks%20on%20unseen%20models%20and%20unseen%20high-resolution%20datasets%2C%0Aboth%20under%20the%20proposed%20practical%20IoU-based%20evaluation%20metric%20and%20the%20previous%0AmAP-based%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10600v1&entry.124074799=Read"},
{"title": "PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D\n  Perception Tasks", "author": "Xinhao Wang and Zhiwei Lin and Zhongyu Xia and Yongtao Wang", "abstract": "  Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)\nrepresent two mainstream model quantization approaches. However, PTQ often\nleads to unacceptable performance degradation in quantized models, while QAT\nimposes substantial GPU memory requirements and extended training time due to\nweight fine-tuning.In this paper, we propose PTQAT, a novel general hybrid\nquantization algorithm for the efficient deployment of 3D perception networks.\nTo address the speed accuracy trade-off between PTQ and QAT, our method selects\ncritical layers for QAT fine-tuning and performs PTQ on the remaining layers.\nContrary to intuition, fine-tuning the layers with smaller output discrepancies\nbefore and after quantization, rather than those with larger discrepancies,\nactually leads to greater improvements in the model's quantization accuracy.\nThis means we better compensate for quantization errors during their\npropagation, rather than addressing them at the point where they occur. The\nproposed PTQAT achieves similar performance to QAT with more efficiency by\nfreezing nearly 50% of quantifiable layers. Additionally, PTQAT is a universal\nquantization method that supports various quantization bit widths (4 bits) as\nwell as different model architectures, including CNNs and Transformers. The\nexperimental results on nuScenes across diverse 3D perception tasks, including\nobject detection, semantic segmentation, and occupancy prediction, show that\nour method consistently outperforms QAT-only baselines. Notably, it achieves\n0.2%-0.9% NDS and 0.3%-1.0% mAP gains in object detection, 0.3%-2.0% mIoU gains\nin semantic segmentation and occupancy prediction while fine-tuning fewer\nweights.\n", "link": "http://arxiv.org/abs/2508.10557v1", "date": "2025-08-14", "relevancy": 2.1883, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5706}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5315}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PTQAT%3A%20A%20Hybrid%20Parameter-Efficient%20Quantization%20Algorithm%20for%203D%0A%20%20Perception%20Tasks&body=Title%3A%20PTQAT%3A%20A%20Hybrid%20Parameter-Efficient%20Quantization%20Algorithm%20for%203D%0A%20%20Perception%20Tasks%0AAuthor%3A%20Xinhao%20Wang%20and%20Zhiwei%20Lin%20and%20Zhongyu%20Xia%20and%20Yongtao%20Wang%0AAbstract%3A%20%20%20Post-Training%20Quantization%20%28PTQ%29%20and%20Quantization-Aware%20Training%20%28QAT%29%0Arepresent%20two%20mainstream%20model%20quantization%20approaches.%20However%2C%20PTQ%20often%0Aleads%20to%20unacceptable%20performance%20degradation%20in%20quantized%20models%2C%20while%20QAT%0Aimposes%20substantial%20GPU%20memory%20requirements%20and%20extended%20training%20time%20due%20to%0Aweight%20fine-tuning.In%20this%20paper%2C%20we%20propose%20PTQAT%2C%20a%20novel%20general%20hybrid%0Aquantization%20algorithm%20for%20the%20efficient%20deployment%20of%203D%20perception%20networks.%0ATo%20address%20the%20speed%20accuracy%20trade-off%20between%20PTQ%20and%20QAT%2C%20our%20method%20selects%0Acritical%20layers%20for%20QAT%20fine-tuning%20and%20performs%20PTQ%20on%20the%20remaining%20layers.%0AContrary%20to%20intuition%2C%20fine-tuning%20the%20layers%20with%20smaller%20output%20discrepancies%0Abefore%20and%20after%20quantization%2C%20rather%20than%20those%20with%20larger%20discrepancies%2C%0Aactually%20leads%20to%20greater%20improvements%20in%20the%20model%27s%20quantization%20accuracy.%0AThis%20means%20we%20better%20compensate%20for%20quantization%20errors%20during%20their%0Apropagation%2C%20rather%20than%20addressing%20them%20at%20the%20point%20where%20they%20occur.%20The%0Aproposed%20PTQAT%20achieves%20similar%20performance%20to%20QAT%20with%20more%20efficiency%20by%0Afreezing%20nearly%2050%25%20of%20quantifiable%20layers.%20Additionally%2C%20PTQAT%20is%20a%20universal%0Aquantization%20method%20that%20supports%20various%20quantization%20bit%20widths%20%284%20bits%29%20as%0Awell%20as%20different%20model%20architectures%2C%20including%20CNNs%20and%20Transformers.%20The%0Aexperimental%20results%20on%20nuScenes%20across%20diverse%203D%20perception%20tasks%2C%20including%0Aobject%20detection%2C%20semantic%20segmentation%2C%20and%20occupancy%20prediction%2C%20show%20that%0Aour%20method%20consistently%20outperforms%20QAT-only%20baselines.%20Notably%2C%20it%20achieves%0A0.2%25-0.9%25%20NDS%20and%200.3%25-1.0%25%20mAP%20gains%20in%20object%20detection%2C%200.3%25-2.0%25%20mIoU%20gains%0Ain%20semantic%20segmentation%20and%20occupancy%20prediction%20while%20fine-tuning%20fewer%0Aweights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPTQAT%253A%2520A%2520Hybrid%2520Parameter-Efficient%2520Quantization%2520Algorithm%2520for%25203D%250A%2520%2520Perception%2520Tasks%26entry.906535625%3DXinhao%2520Wang%2520and%2520Zhiwei%2520Lin%2520and%2520Zhongyu%2520Xia%2520and%2520Yongtao%2520Wang%26entry.1292438233%3D%2520%2520Post-Training%2520Quantization%2520%2528PTQ%2529%2520and%2520Quantization-Aware%2520Training%2520%2528QAT%2529%250Arepresent%2520two%2520mainstream%2520model%2520quantization%2520approaches.%2520However%252C%2520PTQ%2520often%250Aleads%2520to%2520unacceptable%2520performance%2520degradation%2520in%2520quantized%2520models%252C%2520while%2520QAT%250Aimposes%2520substantial%2520GPU%2520memory%2520requirements%2520and%2520extended%2520training%2520time%2520due%2520to%250Aweight%2520fine-tuning.In%2520this%2520paper%252C%2520we%2520propose%2520PTQAT%252C%2520a%2520novel%2520general%2520hybrid%250Aquantization%2520algorithm%2520for%2520the%2520efficient%2520deployment%2520of%25203D%2520perception%2520networks.%250ATo%2520address%2520the%2520speed%2520accuracy%2520trade-off%2520between%2520PTQ%2520and%2520QAT%252C%2520our%2520method%2520selects%250Acritical%2520layers%2520for%2520QAT%2520fine-tuning%2520and%2520performs%2520PTQ%2520on%2520the%2520remaining%2520layers.%250AContrary%2520to%2520intuition%252C%2520fine-tuning%2520the%2520layers%2520with%2520smaller%2520output%2520discrepancies%250Abefore%2520and%2520after%2520quantization%252C%2520rather%2520than%2520those%2520with%2520larger%2520discrepancies%252C%250Aactually%2520leads%2520to%2520greater%2520improvements%2520in%2520the%2520model%2527s%2520quantization%2520accuracy.%250AThis%2520means%2520we%2520better%2520compensate%2520for%2520quantization%2520errors%2520during%2520their%250Apropagation%252C%2520rather%2520than%2520addressing%2520them%2520at%2520the%2520point%2520where%2520they%2520occur.%2520The%250Aproposed%2520PTQAT%2520achieves%2520similar%2520performance%2520to%2520QAT%2520with%2520more%2520efficiency%2520by%250Afreezing%2520nearly%252050%2525%2520of%2520quantifiable%2520layers.%2520Additionally%252C%2520PTQAT%2520is%2520a%2520universal%250Aquantization%2520method%2520that%2520supports%2520various%2520quantization%2520bit%2520widths%2520%25284%2520bits%2529%2520as%250Awell%2520as%2520different%2520model%2520architectures%252C%2520including%2520CNNs%2520and%2520Transformers.%2520The%250Aexperimental%2520results%2520on%2520nuScenes%2520across%2520diverse%25203D%2520perception%2520tasks%252C%2520including%250Aobject%2520detection%252C%2520semantic%2520segmentation%252C%2520and%2520occupancy%2520prediction%252C%2520show%2520that%250Aour%2520method%2520consistently%2520outperforms%2520QAT-only%2520baselines.%2520Notably%252C%2520it%2520achieves%250A0.2%2525-0.9%2525%2520NDS%2520and%25200.3%2525-1.0%2525%2520mAP%2520gains%2520in%2520object%2520detection%252C%25200.3%2525-2.0%2525%2520mIoU%2520gains%250Ain%2520semantic%2520segmentation%2520and%2520occupancy%2520prediction%2520while%2520fine-tuning%2520fewer%250Aweights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PTQAT%3A%20A%20Hybrid%20Parameter-Efficient%20Quantization%20Algorithm%20for%203D%0A%20%20Perception%20Tasks&entry.906535625=Xinhao%20Wang%20and%20Zhiwei%20Lin%20and%20Zhongyu%20Xia%20and%20Yongtao%20Wang&entry.1292438233=%20%20Post-Training%20Quantization%20%28PTQ%29%20and%20Quantization-Aware%20Training%20%28QAT%29%0Arepresent%20two%20mainstream%20model%20quantization%20approaches.%20However%2C%20PTQ%20often%0Aleads%20to%20unacceptable%20performance%20degradation%20in%20quantized%20models%2C%20while%20QAT%0Aimposes%20substantial%20GPU%20memory%20requirements%20and%20extended%20training%20time%20due%20to%0Aweight%20fine-tuning.In%20this%20paper%2C%20we%20propose%20PTQAT%2C%20a%20novel%20general%20hybrid%0Aquantization%20algorithm%20for%20the%20efficient%20deployment%20of%203D%20perception%20networks.%0ATo%20address%20the%20speed%20accuracy%20trade-off%20between%20PTQ%20and%20QAT%2C%20our%20method%20selects%0Acritical%20layers%20for%20QAT%20fine-tuning%20and%20performs%20PTQ%20on%20the%20remaining%20layers.%0AContrary%20to%20intuition%2C%20fine-tuning%20the%20layers%20with%20smaller%20output%20discrepancies%0Abefore%20and%20after%20quantization%2C%20rather%20than%20those%20with%20larger%20discrepancies%2C%0Aactually%20leads%20to%20greater%20improvements%20in%20the%20model%27s%20quantization%20accuracy.%0AThis%20means%20we%20better%20compensate%20for%20quantization%20errors%20during%20their%0Apropagation%2C%20rather%20than%20addressing%20them%20at%20the%20point%20where%20they%20occur.%20The%0Aproposed%20PTQAT%20achieves%20similar%20performance%20to%20QAT%20with%20more%20efficiency%20by%0Afreezing%20nearly%2050%25%20of%20quantifiable%20layers.%20Additionally%2C%20PTQAT%20is%20a%20universal%0Aquantization%20method%20that%20supports%20various%20quantization%20bit%20widths%20%284%20bits%29%20as%0Awell%20as%20different%20model%20architectures%2C%20including%20CNNs%20and%20Transformers.%20The%0Aexperimental%20results%20on%20nuScenes%20across%20diverse%203D%20perception%20tasks%2C%20including%0Aobject%20detection%2C%20semantic%20segmentation%2C%20and%20occupancy%20prediction%2C%20show%20that%0Aour%20method%20consistently%20outperforms%20QAT-only%20baselines.%20Notably%2C%20it%20achieves%0A0.2%25-0.9%25%20NDS%20and%200.3%25-1.0%25%20mAP%20gains%20in%20object%20detection%2C%200.3%25-2.0%25%20mIoU%20gains%0Ain%20semantic%20segmentation%20and%20occupancy%20prediction%20while%20fine-tuning%20fewer%0Aweights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10557v1&entry.124074799=Read"},
{"title": "Increasing the Utility of Synthetic Images through Chamfer Guidance", "author": "Nicola Dall'Asen and Xiaofeng Zhang and Reyhane Askari Hemmat and Melissa Hall and Jakob Verbeek and Adriana Romero-Soriano and Michal Drozdzal", "abstract": "  Conditional image generative models hold considerable promise to produce\ninfinite amounts of synthetic training data. Yet, recent progress in generation\nquality has come at the expense of generation diversity, limiting the utility\nof these models as a source of synthetic training data. Although guidance-based\napproaches have been introduced to improve the utility of generated data by\nfocusing on quality or diversity, the (implicit or explicit) utility functions\noftentimes disregard the potential distribution shift between synthetic and\nreal data. In this work, we introduce Chamfer Guidance: a training-free\nguidance approach which leverages a handful of real exemplar images to\ncharacterize the quality and diversity of synthetic data. We show that by\nleveraging the proposed Chamfer Guidance, we can boost the diversity of the\ngenerations w.r.t. a dataset of real images while maintaining or improving the\ngeneration quality on ImageNet-1k and standard geo-diversity benchmarks. Our\napproach achieves state-of-the-art few-shot performance with as little as 2\nexemplar real images, obtaining 96.4\\% in terms of precision, and 86.4\\% in\nterms of distributional coverage, which increase to 97.5\\% and 92.7\\%,\nrespectively, when using 32 real images. We showcase the benefits of the\nChamfer Guidance generation by training downstream image classifiers on\nsynthetic data, achieving accuracy boost of up to 15\\% for in-distribution over\nthe baselines, and up to 16\\% in out-of-distribution. Furthermore, our approach\ndoes not require using the unconditional model, and thus obtains a 31\\%\nreduction in FLOPs w.r.t. classifier-free-guidance-based approaches at sampling\ntime.\n", "link": "http://arxiv.org/abs/2508.10631v1", "date": "2025-08-14", "relevancy": 2.1651, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5791}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5377}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Increasing%20the%20Utility%20of%20Synthetic%20Images%20through%20Chamfer%20Guidance&body=Title%3A%20Increasing%20the%20Utility%20of%20Synthetic%20Images%20through%20Chamfer%20Guidance%0AAuthor%3A%20Nicola%20Dall%27Asen%20and%20Xiaofeng%20Zhang%20and%20Reyhane%20Askari%20Hemmat%20and%20Melissa%20Hall%20and%20Jakob%20Verbeek%20and%20Adriana%20Romero-Soriano%20and%20Michal%20Drozdzal%0AAbstract%3A%20%20%20Conditional%20image%20generative%20models%20hold%20considerable%20promise%20to%20produce%0Ainfinite%20amounts%20of%20synthetic%20training%20data.%20Yet%2C%20recent%20progress%20in%20generation%0Aquality%20has%20come%20at%20the%20expense%20of%20generation%20diversity%2C%20limiting%20the%20utility%0Aof%20these%20models%20as%20a%20source%20of%20synthetic%20training%20data.%20Although%20guidance-based%0Aapproaches%20have%20been%20introduced%20to%20improve%20the%20utility%20of%20generated%20data%20by%0Afocusing%20on%20quality%20or%20diversity%2C%20the%20%28implicit%20or%20explicit%29%20utility%20functions%0Aoftentimes%20disregard%20the%20potential%20distribution%20shift%20between%20synthetic%20and%0Areal%20data.%20In%20this%20work%2C%20we%20introduce%20Chamfer%20Guidance%3A%20a%20training-free%0Aguidance%20approach%20which%20leverages%20a%20handful%20of%20real%20exemplar%20images%20to%0Acharacterize%20the%20quality%20and%20diversity%20of%20synthetic%20data.%20We%20show%20that%20by%0Aleveraging%20the%20proposed%20Chamfer%20Guidance%2C%20we%20can%20boost%20the%20diversity%20of%20the%0Agenerations%20w.r.t.%20a%20dataset%20of%20real%20images%20while%20maintaining%20or%20improving%20the%0Ageneration%20quality%20on%20ImageNet-1k%20and%20standard%20geo-diversity%20benchmarks.%20Our%0Aapproach%20achieves%20state-of-the-art%20few-shot%20performance%20with%20as%20little%20as%202%0Aexemplar%20real%20images%2C%20obtaining%2096.4%5C%25%20in%20terms%20of%20precision%2C%20and%2086.4%5C%25%20in%0Aterms%20of%20distributional%20coverage%2C%20which%20increase%20to%2097.5%5C%25%20and%2092.7%5C%25%2C%0Arespectively%2C%20when%20using%2032%20real%20images.%20We%20showcase%20the%20benefits%20of%20the%0AChamfer%20Guidance%20generation%20by%20training%20downstream%20image%20classifiers%20on%0Asynthetic%20data%2C%20achieving%20accuracy%20boost%20of%20up%20to%2015%5C%25%20for%20in-distribution%20over%0Athe%20baselines%2C%20and%20up%20to%2016%5C%25%20in%20out-of-distribution.%20Furthermore%2C%20our%20approach%0Adoes%20not%20require%20using%20the%20unconditional%20model%2C%20and%20thus%20obtains%20a%2031%5C%25%0Areduction%20in%20FLOPs%20w.r.t.%20classifier-free-guidance-based%20approaches%20at%20sampling%0Atime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncreasing%2520the%2520Utility%2520of%2520Synthetic%2520Images%2520through%2520Chamfer%2520Guidance%26entry.906535625%3DNicola%2520Dall%2527Asen%2520and%2520Xiaofeng%2520Zhang%2520and%2520Reyhane%2520Askari%2520Hemmat%2520and%2520Melissa%2520Hall%2520and%2520Jakob%2520Verbeek%2520and%2520Adriana%2520Romero-Soriano%2520and%2520Michal%2520Drozdzal%26entry.1292438233%3D%2520%2520Conditional%2520image%2520generative%2520models%2520hold%2520considerable%2520promise%2520to%2520produce%250Ainfinite%2520amounts%2520of%2520synthetic%2520training%2520data.%2520Yet%252C%2520recent%2520progress%2520in%2520generation%250Aquality%2520has%2520come%2520at%2520the%2520expense%2520of%2520generation%2520diversity%252C%2520limiting%2520the%2520utility%250Aof%2520these%2520models%2520as%2520a%2520source%2520of%2520synthetic%2520training%2520data.%2520Although%2520guidance-based%250Aapproaches%2520have%2520been%2520introduced%2520to%2520improve%2520the%2520utility%2520of%2520generated%2520data%2520by%250Afocusing%2520on%2520quality%2520or%2520diversity%252C%2520the%2520%2528implicit%2520or%2520explicit%2529%2520utility%2520functions%250Aoftentimes%2520disregard%2520the%2520potential%2520distribution%2520shift%2520between%2520synthetic%2520and%250Areal%2520data.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Chamfer%2520Guidance%253A%2520a%2520training-free%250Aguidance%2520approach%2520which%2520leverages%2520a%2520handful%2520of%2520real%2520exemplar%2520images%2520to%250Acharacterize%2520the%2520quality%2520and%2520diversity%2520of%2520synthetic%2520data.%2520We%2520show%2520that%2520by%250Aleveraging%2520the%2520proposed%2520Chamfer%2520Guidance%252C%2520we%2520can%2520boost%2520the%2520diversity%2520of%2520the%250Agenerations%2520w.r.t.%2520a%2520dataset%2520of%2520real%2520images%2520while%2520maintaining%2520or%2520improving%2520the%250Ageneration%2520quality%2520on%2520ImageNet-1k%2520and%2520standard%2520geo-diversity%2520benchmarks.%2520Our%250Aapproach%2520achieves%2520state-of-the-art%2520few-shot%2520performance%2520with%2520as%2520little%2520as%25202%250Aexemplar%2520real%2520images%252C%2520obtaining%252096.4%255C%2525%2520in%2520terms%2520of%2520precision%252C%2520and%252086.4%255C%2525%2520in%250Aterms%2520of%2520distributional%2520coverage%252C%2520which%2520increase%2520to%252097.5%255C%2525%2520and%252092.7%255C%2525%252C%250Arespectively%252C%2520when%2520using%252032%2520real%2520images.%2520We%2520showcase%2520the%2520benefits%2520of%2520the%250AChamfer%2520Guidance%2520generation%2520by%2520training%2520downstream%2520image%2520classifiers%2520on%250Asynthetic%2520data%252C%2520achieving%2520accuracy%2520boost%2520of%2520up%2520to%252015%255C%2525%2520for%2520in-distribution%2520over%250Athe%2520baselines%252C%2520and%2520up%2520to%252016%255C%2525%2520in%2520out-of-distribution.%2520Furthermore%252C%2520our%2520approach%250Adoes%2520not%2520require%2520using%2520the%2520unconditional%2520model%252C%2520and%2520thus%2520obtains%2520a%252031%255C%2525%250Areduction%2520in%2520FLOPs%2520w.r.t.%2520classifier-free-guidance-based%2520approaches%2520at%2520sampling%250Atime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Increasing%20the%20Utility%20of%20Synthetic%20Images%20through%20Chamfer%20Guidance&entry.906535625=Nicola%20Dall%27Asen%20and%20Xiaofeng%20Zhang%20and%20Reyhane%20Askari%20Hemmat%20and%20Melissa%20Hall%20and%20Jakob%20Verbeek%20and%20Adriana%20Romero-Soriano%20and%20Michal%20Drozdzal&entry.1292438233=%20%20Conditional%20image%20generative%20models%20hold%20considerable%20promise%20to%20produce%0Ainfinite%20amounts%20of%20synthetic%20training%20data.%20Yet%2C%20recent%20progress%20in%20generation%0Aquality%20has%20come%20at%20the%20expense%20of%20generation%20diversity%2C%20limiting%20the%20utility%0Aof%20these%20models%20as%20a%20source%20of%20synthetic%20training%20data.%20Although%20guidance-based%0Aapproaches%20have%20been%20introduced%20to%20improve%20the%20utility%20of%20generated%20data%20by%0Afocusing%20on%20quality%20or%20diversity%2C%20the%20%28implicit%20or%20explicit%29%20utility%20functions%0Aoftentimes%20disregard%20the%20potential%20distribution%20shift%20between%20synthetic%20and%0Areal%20data.%20In%20this%20work%2C%20we%20introduce%20Chamfer%20Guidance%3A%20a%20training-free%0Aguidance%20approach%20which%20leverages%20a%20handful%20of%20real%20exemplar%20images%20to%0Acharacterize%20the%20quality%20and%20diversity%20of%20synthetic%20data.%20We%20show%20that%20by%0Aleveraging%20the%20proposed%20Chamfer%20Guidance%2C%20we%20can%20boost%20the%20diversity%20of%20the%0Agenerations%20w.r.t.%20a%20dataset%20of%20real%20images%20while%20maintaining%20or%20improving%20the%0Ageneration%20quality%20on%20ImageNet-1k%20and%20standard%20geo-diversity%20benchmarks.%20Our%0Aapproach%20achieves%20state-of-the-art%20few-shot%20performance%20with%20as%20little%20as%202%0Aexemplar%20real%20images%2C%20obtaining%2096.4%5C%25%20in%20terms%20of%20precision%2C%20and%2086.4%5C%25%20in%0Aterms%20of%20distributional%20coverage%2C%20which%20increase%20to%2097.5%5C%25%20and%2092.7%5C%25%2C%0Arespectively%2C%20when%20using%2032%20real%20images.%20We%20showcase%20the%20benefits%20of%20the%0AChamfer%20Guidance%20generation%20by%20training%20downstream%20image%20classifiers%20on%0Asynthetic%20data%2C%20achieving%20accuracy%20boost%20of%20up%20to%2015%5C%25%20for%20in-distribution%20over%0Athe%20baselines%2C%20and%20up%20to%2016%5C%25%20in%20out-of-distribution.%20Furthermore%2C%20our%20approach%0Adoes%20not%20require%20using%20the%20unconditional%20model%2C%20and%20thus%20obtains%20a%2031%5C%25%0Areduction%20in%20FLOPs%20w.r.t.%20classifier-free-guidance-based%20approaches%20at%20sampling%0Atime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10631v1&entry.124074799=Read"},
{"title": "Cooperative Face Liveness Detection from Optical Flow", "author": "Artem Sokolov and Mikhail Nikitin and Anton Konushin", "abstract": "  In this work, we proposed a novel cooperative video-based face liveness\ndetection method based on a new user interaction scenario where participants\nare instructed to slowly move their frontal-oriented face closer to the camera.\nThis controlled approaching face protocol, combined with optical flow analysis,\nrepresents the core innovation of our approach. By designing a system where\nusers follow this specific movement pattern, we enable robust extraction of\nfacial volume information through neural optical flow estimation, significantly\nimproving discrimination between genuine faces and various presentation attacks\n(including printed photos, screen displays, masks, and video replays). Our\nmethod processes both the predicted optical flows and RGB frames through a\nneural classifier, effectively leveraging spatial-temporal features for more\nreliable liveness detection compared to passive methods.\n", "link": "http://arxiv.org/abs/2508.10786v1", "date": "2025-08-14", "relevancy": 2.1645, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5672}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5267}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cooperative%20Face%20Liveness%20Detection%20from%20Optical%20Flow&body=Title%3A%20Cooperative%20Face%20Liveness%20Detection%20from%20Optical%20Flow%0AAuthor%3A%20Artem%20Sokolov%20and%20Mikhail%20Nikitin%20and%20Anton%20Konushin%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20proposed%20a%20novel%20cooperative%20video-based%20face%20liveness%0Adetection%20method%20based%20on%20a%20new%20user%20interaction%20scenario%20where%20participants%0Aare%20instructed%20to%20slowly%20move%20their%20frontal-oriented%20face%20closer%20to%20the%20camera.%0AThis%20controlled%20approaching%20face%20protocol%2C%20combined%20with%20optical%20flow%20analysis%2C%0Arepresents%20the%20core%20innovation%20of%20our%20approach.%20By%20designing%20a%20system%20where%0Ausers%20follow%20this%20specific%20movement%20pattern%2C%20we%20enable%20robust%20extraction%20of%0Afacial%20volume%20information%20through%20neural%20optical%20flow%20estimation%2C%20significantly%0Aimproving%20discrimination%20between%20genuine%20faces%20and%20various%20presentation%20attacks%0A%28including%20printed%20photos%2C%20screen%20displays%2C%20masks%2C%20and%20video%20replays%29.%20Our%0Amethod%20processes%20both%20the%20predicted%20optical%20flows%20and%20RGB%20frames%20through%20a%0Aneural%20classifier%2C%20effectively%20leveraging%20spatial-temporal%20features%20for%20more%0Areliable%20liveness%20detection%20compared%20to%20passive%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCooperative%2520Face%2520Liveness%2520Detection%2520from%2520Optical%2520Flow%26entry.906535625%3DArtem%2520Sokolov%2520and%2520Mikhail%2520Nikitin%2520and%2520Anton%2520Konushin%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520proposed%2520a%2520novel%2520cooperative%2520video-based%2520face%2520liveness%250Adetection%2520method%2520based%2520on%2520a%2520new%2520user%2520interaction%2520scenario%2520where%2520participants%250Aare%2520instructed%2520to%2520slowly%2520move%2520their%2520frontal-oriented%2520face%2520closer%2520to%2520the%2520camera.%250AThis%2520controlled%2520approaching%2520face%2520protocol%252C%2520combined%2520with%2520optical%2520flow%2520analysis%252C%250Arepresents%2520the%2520core%2520innovation%2520of%2520our%2520approach.%2520By%2520designing%2520a%2520system%2520where%250Ausers%2520follow%2520this%2520specific%2520movement%2520pattern%252C%2520we%2520enable%2520robust%2520extraction%2520of%250Afacial%2520volume%2520information%2520through%2520neural%2520optical%2520flow%2520estimation%252C%2520significantly%250Aimproving%2520discrimination%2520between%2520genuine%2520faces%2520and%2520various%2520presentation%2520attacks%250A%2528including%2520printed%2520photos%252C%2520screen%2520displays%252C%2520masks%252C%2520and%2520video%2520replays%2529.%2520Our%250Amethod%2520processes%2520both%2520the%2520predicted%2520optical%2520flows%2520and%2520RGB%2520frames%2520through%2520a%250Aneural%2520classifier%252C%2520effectively%2520leveraging%2520spatial-temporal%2520features%2520for%2520more%250Areliable%2520liveness%2520detection%2520compared%2520to%2520passive%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cooperative%20Face%20Liveness%20Detection%20from%20Optical%20Flow&entry.906535625=Artem%20Sokolov%20and%20Mikhail%20Nikitin%20and%20Anton%20Konushin&entry.1292438233=%20%20In%20this%20work%2C%20we%20proposed%20a%20novel%20cooperative%20video-based%20face%20liveness%0Adetection%20method%20based%20on%20a%20new%20user%20interaction%20scenario%20where%20participants%0Aare%20instructed%20to%20slowly%20move%20their%20frontal-oriented%20face%20closer%20to%20the%20camera.%0AThis%20controlled%20approaching%20face%20protocol%2C%20combined%20with%20optical%20flow%20analysis%2C%0Arepresents%20the%20core%20innovation%20of%20our%20approach.%20By%20designing%20a%20system%20where%0Ausers%20follow%20this%20specific%20movement%20pattern%2C%20we%20enable%20robust%20extraction%20of%0Afacial%20volume%20information%20through%20neural%20optical%20flow%20estimation%2C%20significantly%0Aimproving%20discrimination%20between%20genuine%20faces%20and%20various%20presentation%20attacks%0A%28including%20printed%20photos%2C%20screen%20displays%2C%20masks%2C%20and%20video%20replays%29.%20Our%0Amethod%20processes%20both%20the%20predicted%20optical%20flows%20and%20RGB%20frames%20through%20a%0Aneural%20classifier%2C%20effectively%20leveraging%20spatial-temporal%20features%20for%20more%0Areliable%20liveness%20detection%20compared%20to%20passive%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10786v1&entry.124074799=Read"},
{"title": "Natively Trainable Sparse Attention for Hierarchical Point Cloud\n  Datasets", "author": "Nicolas Lapautre and Maria Marchenko and Carlos Miguel Pati\u00f1o and Xin Zhou", "abstract": "  Unlocking the potential of transformers on datasets of large physical systems\ndepends on overcoming the quadratic scaling of the attention mechanism. This\nwork explores combining the Erwin architecture with the Native Sparse Attention\n(NSA) mechanism to improve the efficiency and receptive field of transformer\nmodels for large-scale physical systems, addressing the challenge of quadratic\nattention complexity. We adapt the NSA mechanism for non-sequential data,\nimplement the Erwin NSA model, and evaluate it on three datasets from the\nphysical sciences -- cosmology simulations, molecular dynamics, and air\npressure modeling -- achieving performance that matches or exceeds that of the\noriginal Erwin model. Additionally, we reproduce the experimental results from\nthe Erwin paper to validate their implementation.\n", "link": "http://arxiv.org/abs/2508.10758v1", "date": "2025-08-14", "relevancy": 2.1641, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5471}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.537}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Natively%20Trainable%20Sparse%20Attention%20for%20Hierarchical%20Point%20Cloud%0A%20%20Datasets&body=Title%3A%20Natively%20Trainable%20Sparse%20Attention%20for%20Hierarchical%20Point%20Cloud%0A%20%20Datasets%0AAuthor%3A%20Nicolas%20Lapautre%20and%20Maria%20Marchenko%20and%20Carlos%20Miguel%20Pati%C3%B1o%20and%20Xin%20Zhou%0AAbstract%3A%20%20%20Unlocking%20the%20potential%20of%20transformers%20on%20datasets%20of%20large%20physical%20systems%0Adepends%20on%20overcoming%20the%20quadratic%20scaling%20of%20the%20attention%20mechanism.%20This%0Awork%20explores%20combining%20the%20Erwin%20architecture%20with%20the%20Native%20Sparse%20Attention%0A%28NSA%29%20mechanism%20to%20improve%20the%20efficiency%20and%20receptive%20field%20of%20transformer%0Amodels%20for%20large-scale%20physical%20systems%2C%20addressing%20the%20challenge%20of%20quadratic%0Aattention%20complexity.%20We%20adapt%20the%20NSA%20mechanism%20for%20non-sequential%20data%2C%0Aimplement%20the%20Erwin%20NSA%20model%2C%20and%20evaluate%20it%20on%20three%20datasets%20from%20the%0Aphysical%20sciences%20--%20cosmology%20simulations%2C%20molecular%20dynamics%2C%20and%20air%0Apressure%20modeling%20--%20achieving%20performance%20that%20matches%20or%20exceeds%20that%20of%20the%0Aoriginal%20Erwin%20model.%20Additionally%2C%20we%20reproduce%20the%20experimental%20results%20from%0Athe%20Erwin%20paper%20to%20validate%20their%20implementation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNatively%2520Trainable%2520Sparse%2520Attention%2520for%2520Hierarchical%2520Point%2520Cloud%250A%2520%2520Datasets%26entry.906535625%3DNicolas%2520Lapautre%2520and%2520Maria%2520Marchenko%2520and%2520Carlos%2520Miguel%2520Pati%25C3%25B1o%2520and%2520Xin%2520Zhou%26entry.1292438233%3D%2520%2520Unlocking%2520the%2520potential%2520of%2520transformers%2520on%2520datasets%2520of%2520large%2520physical%2520systems%250Adepends%2520on%2520overcoming%2520the%2520quadratic%2520scaling%2520of%2520the%2520attention%2520mechanism.%2520This%250Awork%2520explores%2520combining%2520the%2520Erwin%2520architecture%2520with%2520the%2520Native%2520Sparse%2520Attention%250A%2528NSA%2529%2520mechanism%2520to%2520improve%2520the%2520efficiency%2520and%2520receptive%2520field%2520of%2520transformer%250Amodels%2520for%2520large-scale%2520physical%2520systems%252C%2520addressing%2520the%2520challenge%2520of%2520quadratic%250Aattention%2520complexity.%2520We%2520adapt%2520the%2520NSA%2520mechanism%2520for%2520non-sequential%2520data%252C%250Aimplement%2520the%2520Erwin%2520NSA%2520model%252C%2520and%2520evaluate%2520it%2520on%2520three%2520datasets%2520from%2520the%250Aphysical%2520sciences%2520--%2520cosmology%2520simulations%252C%2520molecular%2520dynamics%252C%2520and%2520air%250Apressure%2520modeling%2520--%2520achieving%2520performance%2520that%2520matches%2520or%2520exceeds%2520that%2520of%2520the%250Aoriginal%2520Erwin%2520model.%2520Additionally%252C%2520we%2520reproduce%2520the%2520experimental%2520results%2520from%250Athe%2520Erwin%2520paper%2520to%2520validate%2520their%2520implementation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Natively%20Trainable%20Sparse%20Attention%20for%20Hierarchical%20Point%20Cloud%0A%20%20Datasets&entry.906535625=Nicolas%20Lapautre%20and%20Maria%20Marchenko%20and%20Carlos%20Miguel%20Pati%C3%B1o%20and%20Xin%20Zhou&entry.1292438233=%20%20Unlocking%20the%20potential%20of%20transformers%20on%20datasets%20of%20large%20physical%20systems%0Adepends%20on%20overcoming%20the%20quadratic%20scaling%20of%20the%20attention%20mechanism.%20This%0Awork%20explores%20combining%20the%20Erwin%20architecture%20with%20the%20Native%20Sparse%20Attention%0A%28NSA%29%20mechanism%20to%20improve%20the%20efficiency%20and%20receptive%20field%20of%20transformer%0Amodels%20for%20large-scale%20physical%20systems%2C%20addressing%20the%20challenge%20of%20quadratic%0Aattention%20complexity.%20We%20adapt%20the%20NSA%20mechanism%20for%20non-sequential%20data%2C%0Aimplement%20the%20Erwin%20NSA%20model%2C%20and%20evaluate%20it%20on%20three%20datasets%20from%20the%0Aphysical%20sciences%20--%20cosmology%20simulations%2C%20molecular%20dynamics%2C%20and%20air%0Apressure%20modeling%20--%20achieving%20performance%20that%20matches%20or%20exceeds%20that%20of%20the%0Aoriginal%20Erwin%20model.%20Additionally%2C%20we%20reproduce%20the%20experimental%20results%20from%0Athe%20Erwin%20paper%20to%20validate%20their%20implementation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10758v1&entry.124074799=Read"},
{"title": "Personalized Feature Translation for Expression Recognition: An\n  Efficient Source-Free Domain Adaptation Method", "author": "Masoumeh Sharafi and Soufiane Belharbi and Houssem Ben Salem and Ali Etemad and Alessandro Lameiras Koerich and Marco Pedersoli and Simon Bacon and Eric Granger", "abstract": "  Facial expression recognition (FER) models are employed in many video-based\naffective computing applications, such as human-computer interaction and\nhealthcare monitoring. However, deep FER models often struggle with subtle\nexpressions and high inter-subject variability, limiting their performance in\nreal-world applications. To improve their performance, source-free domain\nadaptation (SFDA) methods have been proposed to personalize a pretrained source\nmodel using only unlabeled target domain data, thereby avoiding data privacy,\nstorage, and transmission constraints. This paper addresses a challenging\nscenario where source data is unavailable for adaptation, and only unlabeled\ntarget data consisting solely of neutral expressions is available. SFDA methods\nare not typically designed to adapt using target data from only a single class.\nFurther, using models to generate facial images with non-neutral expressions\ncan be unstable and computationally intensive. In this paper, personalized\nfeature translation (PFT) is proposed for SFDA. Unlike current image\ntranslation methods for SFDA, our lightweight method operates in the latent\nspace. We first pre-train the translator on the source domain data to transform\nthe subject-specific style features from one source subject into another.\nExpression information is preserved by optimizing a combination of expression\nconsistency and style-aware objectives. Then, the translator is adapted on\nneutral target data, without using source data or image synthesis. By\ntranslating in the latent space, PFT avoids the complexity and noise of face\nexpression generation, producing discriminative embeddings optimized for\nclassification. Using PFT eliminates the need for image synthesis, reduces\ncomputational overhead (using a lightweight translator), and only adapts part\nof the model, making the method efficient compared to image-based translation.\n", "link": "http://arxiv.org/abs/2508.09202v2", "date": "2025-08-14", "relevancy": 2.1536, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5475}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5377}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Feature%20Translation%20for%20Expression%20Recognition%3A%20An%0A%20%20Efficient%20Source-Free%20Domain%20Adaptation%20Method&body=Title%3A%20Personalized%20Feature%20Translation%20for%20Expression%20Recognition%3A%20An%0A%20%20Efficient%20Source-Free%20Domain%20Adaptation%20Method%0AAuthor%3A%20Masoumeh%20Sharafi%20and%20Soufiane%20Belharbi%20and%20Houssem%20Ben%20Salem%20and%20Ali%20Etemad%20and%20Alessandro%20Lameiras%20Koerich%20and%20Marco%20Pedersoli%20and%20Simon%20Bacon%20and%20Eric%20Granger%0AAbstract%3A%20%20%20Facial%20expression%20recognition%20%28FER%29%20models%20are%20employed%20in%20many%20video-based%0Aaffective%20computing%20applications%2C%20such%20as%20human-computer%20interaction%20and%0Ahealthcare%20monitoring.%20However%2C%20deep%20FER%20models%20often%20struggle%20with%20subtle%0Aexpressions%20and%20high%20inter-subject%20variability%2C%20limiting%20their%20performance%20in%0Areal-world%20applications.%20To%20improve%20their%20performance%2C%20source-free%20domain%0Aadaptation%20%28SFDA%29%20methods%20have%20been%20proposed%20to%20personalize%20a%20pretrained%20source%0Amodel%20using%20only%20unlabeled%20target%20domain%20data%2C%20thereby%20avoiding%20data%20privacy%2C%0Astorage%2C%20and%20transmission%20constraints.%20This%20paper%20addresses%20a%20challenging%0Ascenario%20where%20source%20data%20is%20unavailable%20for%20adaptation%2C%20and%20only%20unlabeled%0Atarget%20data%20consisting%20solely%20of%20neutral%20expressions%20is%20available.%20SFDA%20methods%0Aare%20not%20typically%20designed%20to%20adapt%20using%20target%20data%20from%20only%20a%20single%20class.%0AFurther%2C%20using%20models%20to%20generate%20facial%20images%20with%20non-neutral%20expressions%0Acan%20be%20unstable%20and%20computationally%20intensive.%20In%20this%20paper%2C%20personalized%0Afeature%20translation%20%28PFT%29%20is%20proposed%20for%20SFDA.%20Unlike%20current%20image%0Atranslation%20methods%20for%20SFDA%2C%20our%20lightweight%20method%20operates%20in%20the%20latent%0Aspace.%20We%20first%20pre-train%20the%20translator%20on%20the%20source%20domain%20data%20to%20transform%0Athe%20subject-specific%20style%20features%20from%20one%20source%20subject%20into%20another.%0AExpression%20information%20is%20preserved%20by%20optimizing%20a%20combination%20of%20expression%0Aconsistency%20and%20style-aware%20objectives.%20Then%2C%20the%20translator%20is%20adapted%20on%0Aneutral%20target%20data%2C%20without%20using%20source%20data%20or%20image%20synthesis.%20By%0Atranslating%20in%20the%20latent%20space%2C%20PFT%20avoids%20the%20complexity%20and%20noise%20of%20face%0Aexpression%20generation%2C%20producing%20discriminative%20embeddings%20optimized%20for%0Aclassification.%20Using%20PFT%20eliminates%20the%20need%20for%20image%20synthesis%2C%20reduces%0Acomputational%20overhead%20%28using%20a%20lightweight%20translator%29%2C%20and%20only%20adapts%20part%0Aof%20the%20model%2C%20making%20the%20method%20efficient%20compared%20to%20image-based%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09202v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Feature%2520Translation%2520for%2520Expression%2520Recognition%253A%2520An%250A%2520%2520Efficient%2520Source-Free%2520Domain%2520Adaptation%2520Method%26entry.906535625%3DMasoumeh%2520Sharafi%2520and%2520Soufiane%2520Belharbi%2520and%2520Houssem%2520Ben%2520Salem%2520and%2520Ali%2520Etemad%2520and%2520Alessandro%2520Lameiras%2520Koerich%2520and%2520Marco%2520Pedersoli%2520and%2520Simon%2520Bacon%2520and%2520Eric%2520Granger%26entry.1292438233%3D%2520%2520Facial%2520expression%2520recognition%2520%2528FER%2529%2520models%2520are%2520employed%2520in%2520many%2520video-based%250Aaffective%2520computing%2520applications%252C%2520such%2520as%2520human-computer%2520interaction%2520and%250Ahealthcare%2520monitoring.%2520However%252C%2520deep%2520FER%2520models%2520often%2520struggle%2520with%2520subtle%250Aexpressions%2520and%2520high%2520inter-subject%2520variability%252C%2520limiting%2520their%2520performance%2520in%250Areal-world%2520applications.%2520To%2520improve%2520their%2520performance%252C%2520source-free%2520domain%250Aadaptation%2520%2528SFDA%2529%2520methods%2520have%2520been%2520proposed%2520to%2520personalize%2520a%2520pretrained%2520source%250Amodel%2520using%2520only%2520unlabeled%2520target%2520domain%2520data%252C%2520thereby%2520avoiding%2520data%2520privacy%252C%250Astorage%252C%2520and%2520transmission%2520constraints.%2520This%2520paper%2520addresses%2520a%2520challenging%250Ascenario%2520where%2520source%2520data%2520is%2520unavailable%2520for%2520adaptation%252C%2520and%2520only%2520unlabeled%250Atarget%2520data%2520consisting%2520solely%2520of%2520neutral%2520expressions%2520is%2520available.%2520SFDA%2520methods%250Aare%2520not%2520typically%2520designed%2520to%2520adapt%2520using%2520target%2520data%2520from%2520only%2520a%2520single%2520class.%250AFurther%252C%2520using%2520models%2520to%2520generate%2520facial%2520images%2520with%2520non-neutral%2520expressions%250Acan%2520be%2520unstable%2520and%2520computationally%2520intensive.%2520In%2520this%2520paper%252C%2520personalized%250Afeature%2520translation%2520%2528PFT%2529%2520is%2520proposed%2520for%2520SFDA.%2520Unlike%2520current%2520image%250Atranslation%2520methods%2520for%2520SFDA%252C%2520our%2520lightweight%2520method%2520operates%2520in%2520the%2520latent%250Aspace.%2520We%2520first%2520pre-train%2520the%2520translator%2520on%2520the%2520source%2520domain%2520data%2520to%2520transform%250Athe%2520subject-specific%2520style%2520features%2520from%2520one%2520source%2520subject%2520into%2520another.%250AExpression%2520information%2520is%2520preserved%2520by%2520optimizing%2520a%2520combination%2520of%2520expression%250Aconsistency%2520and%2520style-aware%2520objectives.%2520Then%252C%2520the%2520translator%2520is%2520adapted%2520on%250Aneutral%2520target%2520data%252C%2520without%2520using%2520source%2520data%2520or%2520image%2520synthesis.%2520By%250Atranslating%2520in%2520the%2520latent%2520space%252C%2520PFT%2520avoids%2520the%2520complexity%2520and%2520noise%2520of%2520face%250Aexpression%2520generation%252C%2520producing%2520discriminative%2520embeddings%2520optimized%2520for%250Aclassification.%2520Using%2520PFT%2520eliminates%2520the%2520need%2520for%2520image%2520synthesis%252C%2520reduces%250Acomputational%2520overhead%2520%2528using%2520a%2520lightweight%2520translator%2529%252C%2520and%2520only%2520adapts%2520part%250Aof%2520the%2520model%252C%2520making%2520the%2520method%2520efficient%2520compared%2520to%2520image-based%2520translation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09202v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Feature%20Translation%20for%20Expression%20Recognition%3A%20An%0A%20%20Efficient%20Source-Free%20Domain%20Adaptation%20Method&entry.906535625=Masoumeh%20Sharafi%20and%20Soufiane%20Belharbi%20and%20Houssem%20Ben%20Salem%20and%20Ali%20Etemad%20and%20Alessandro%20Lameiras%20Koerich%20and%20Marco%20Pedersoli%20and%20Simon%20Bacon%20and%20Eric%20Granger&entry.1292438233=%20%20Facial%20expression%20recognition%20%28FER%29%20models%20are%20employed%20in%20many%20video-based%0Aaffective%20computing%20applications%2C%20such%20as%20human-computer%20interaction%20and%0Ahealthcare%20monitoring.%20However%2C%20deep%20FER%20models%20often%20struggle%20with%20subtle%0Aexpressions%20and%20high%20inter-subject%20variability%2C%20limiting%20their%20performance%20in%0Areal-world%20applications.%20To%20improve%20their%20performance%2C%20source-free%20domain%0Aadaptation%20%28SFDA%29%20methods%20have%20been%20proposed%20to%20personalize%20a%20pretrained%20source%0Amodel%20using%20only%20unlabeled%20target%20domain%20data%2C%20thereby%20avoiding%20data%20privacy%2C%0Astorage%2C%20and%20transmission%20constraints.%20This%20paper%20addresses%20a%20challenging%0Ascenario%20where%20source%20data%20is%20unavailable%20for%20adaptation%2C%20and%20only%20unlabeled%0Atarget%20data%20consisting%20solely%20of%20neutral%20expressions%20is%20available.%20SFDA%20methods%0Aare%20not%20typically%20designed%20to%20adapt%20using%20target%20data%20from%20only%20a%20single%20class.%0AFurther%2C%20using%20models%20to%20generate%20facial%20images%20with%20non-neutral%20expressions%0Acan%20be%20unstable%20and%20computationally%20intensive.%20In%20this%20paper%2C%20personalized%0Afeature%20translation%20%28PFT%29%20is%20proposed%20for%20SFDA.%20Unlike%20current%20image%0Atranslation%20methods%20for%20SFDA%2C%20our%20lightweight%20method%20operates%20in%20the%20latent%0Aspace.%20We%20first%20pre-train%20the%20translator%20on%20the%20source%20domain%20data%20to%20transform%0Athe%20subject-specific%20style%20features%20from%20one%20source%20subject%20into%20another.%0AExpression%20information%20is%20preserved%20by%20optimizing%20a%20combination%20of%20expression%0Aconsistency%20and%20style-aware%20objectives.%20Then%2C%20the%20translator%20is%20adapted%20on%0Aneutral%20target%20data%2C%20without%20using%20source%20data%20or%20image%20synthesis.%20By%0Atranslating%20in%20the%20latent%20space%2C%20PFT%20avoids%20the%20complexity%20and%20noise%20of%20face%0Aexpression%20generation%2C%20producing%20discriminative%20embeddings%20optimized%20for%0Aclassification.%20Using%20PFT%20eliminates%20the%20need%20for%20image%20synthesis%2C%20reduces%0Acomputational%20overhead%20%28using%20a%20lightweight%20translator%29%2C%20and%20only%20adapts%20part%0Aof%20the%20model%2C%20making%20the%20method%20efficient%20compared%20to%20image-based%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09202v2&entry.124074799=Read"},
{"title": "Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based\n  Generalized Planning", "author": "Sangwoo Jeon and Juchul Shin and Gyeong-Tae Kim and YeonJe Cho and Seongwoo Kim", "abstract": "  Generalized planning using deep reinforcement learning (RL) combined with\ngraph neural networks (GNNs) has shown promising results in various symbolic\nplanning domains described by PDDL. However, existing approaches typically\nrepresent planning states as fully connected graphs, leading to a combinatorial\nexplosion in edge information and substantial sparsity as problem scales grow,\nespecially evident in large grid-based environments. This dense representation\nresults in diluted node-level information, exponentially increases memory\nrequirements, and ultimately makes learning infeasible for larger-scale\nproblems. To address these challenges, we propose a sparse, goal-aware GNN\nrepresentation that selectively encodes relevant local relationships and\nexplicitly integrates spatial features related to the goal. We validate our\napproach by designing novel drone mission scenarios based on PDDL within a grid\nworld, effectively simulating realistic mission execution environments. Our\nexperimental results demonstrate that our method scales effectively to larger\ngrid sizes previously infeasible with dense graph representations and\nsubstantially improves policy generalization and success rates. Our findings\nprovide a practical foundation for addressing realistic, large-scale\ngeneralized planning tasks.\n", "link": "http://arxiv.org/abs/2508.10747v1", "date": "2025-08-14", "relevancy": 2.1481, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5509}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5457}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Up%20without%20Fading%20Out%3A%20Goal-Aware%20Sparse%20GNN%20for%20RL-based%0A%20%20Generalized%20Planning&body=Title%3A%20Scaling%20Up%20without%20Fading%20Out%3A%20Goal-Aware%20Sparse%20GNN%20for%20RL-based%0A%20%20Generalized%20Planning%0AAuthor%3A%20Sangwoo%20Jeon%20and%20Juchul%20Shin%20and%20Gyeong-Tae%20Kim%20and%20YeonJe%20Cho%20and%20Seongwoo%20Kim%0AAbstract%3A%20%20%20Generalized%20planning%20using%20deep%20reinforcement%20learning%20%28RL%29%20combined%20with%0Agraph%20neural%20networks%20%28GNNs%29%20has%20shown%20promising%20results%20in%20various%20symbolic%0Aplanning%20domains%20described%20by%20PDDL.%20However%2C%20existing%20approaches%20typically%0Arepresent%20planning%20states%20as%20fully%20connected%20graphs%2C%20leading%20to%20a%20combinatorial%0Aexplosion%20in%20edge%20information%20and%20substantial%20sparsity%20as%20problem%20scales%20grow%2C%0Aespecially%20evident%20in%20large%20grid-based%20environments.%20This%20dense%20representation%0Aresults%20in%20diluted%20node-level%20information%2C%20exponentially%20increases%20memory%0Arequirements%2C%20and%20ultimately%20makes%20learning%20infeasible%20for%20larger-scale%0Aproblems.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20sparse%2C%20goal-aware%20GNN%0Arepresentation%20that%20selectively%20encodes%20relevant%20local%20relationships%20and%0Aexplicitly%20integrates%20spatial%20features%20related%20to%20the%20goal.%20We%20validate%20our%0Aapproach%20by%20designing%20novel%20drone%20mission%20scenarios%20based%20on%20PDDL%20within%20a%20grid%0Aworld%2C%20effectively%20simulating%20realistic%20mission%20execution%20environments.%20Our%0Aexperimental%20results%20demonstrate%20that%20our%20method%20scales%20effectively%20to%20larger%0Agrid%20sizes%20previously%20infeasible%20with%20dense%20graph%20representations%20and%0Asubstantially%20improves%20policy%20generalization%20and%20success%20rates.%20Our%20findings%0Aprovide%20a%20practical%20foundation%20for%20addressing%20realistic%2C%20large-scale%0Ageneralized%20planning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Up%2520without%2520Fading%2520Out%253A%2520Goal-Aware%2520Sparse%2520GNN%2520for%2520RL-based%250A%2520%2520Generalized%2520Planning%26entry.906535625%3DSangwoo%2520Jeon%2520and%2520Juchul%2520Shin%2520and%2520Gyeong-Tae%2520Kim%2520and%2520YeonJe%2520Cho%2520and%2520Seongwoo%2520Kim%26entry.1292438233%3D%2520%2520Generalized%2520planning%2520using%2520deep%2520reinforcement%2520learning%2520%2528RL%2529%2520combined%2520with%250Agraph%2520neural%2520networks%2520%2528GNNs%2529%2520has%2520shown%2520promising%2520results%2520in%2520various%2520symbolic%250Aplanning%2520domains%2520described%2520by%2520PDDL.%2520However%252C%2520existing%2520approaches%2520typically%250Arepresent%2520planning%2520states%2520as%2520fully%2520connected%2520graphs%252C%2520leading%2520to%2520a%2520combinatorial%250Aexplosion%2520in%2520edge%2520information%2520and%2520substantial%2520sparsity%2520as%2520problem%2520scales%2520grow%252C%250Aespecially%2520evident%2520in%2520large%2520grid-based%2520environments.%2520This%2520dense%2520representation%250Aresults%2520in%2520diluted%2520node-level%2520information%252C%2520exponentially%2520increases%2520memory%250Arequirements%252C%2520and%2520ultimately%2520makes%2520learning%2520infeasible%2520for%2520larger-scale%250Aproblems.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520sparse%252C%2520goal-aware%2520GNN%250Arepresentation%2520that%2520selectively%2520encodes%2520relevant%2520local%2520relationships%2520and%250Aexplicitly%2520integrates%2520spatial%2520features%2520related%2520to%2520the%2520goal.%2520We%2520validate%2520our%250Aapproach%2520by%2520designing%2520novel%2520drone%2520mission%2520scenarios%2520based%2520on%2520PDDL%2520within%2520a%2520grid%250Aworld%252C%2520effectively%2520simulating%2520realistic%2520mission%2520execution%2520environments.%2520Our%250Aexperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520scales%2520effectively%2520to%2520larger%250Agrid%2520sizes%2520previously%2520infeasible%2520with%2520dense%2520graph%2520representations%2520and%250Asubstantially%2520improves%2520policy%2520generalization%2520and%2520success%2520rates.%2520Our%2520findings%250Aprovide%2520a%2520practical%2520foundation%2520for%2520addressing%2520realistic%252C%2520large-scale%250Ageneralized%2520planning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Up%20without%20Fading%20Out%3A%20Goal-Aware%20Sparse%20GNN%20for%20RL-based%0A%20%20Generalized%20Planning&entry.906535625=Sangwoo%20Jeon%20and%20Juchul%20Shin%20and%20Gyeong-Tae%20Kim%20and%20YeonJe%20Cho%20and%20Seongwoo%20Kim&entry.1292438233=%20%20Generalized%20planning%20using%20deep%20reinforcement%20learning%20%28RL%29%20combined%20with%0Agraph%20neural%20networks%20%28GNNs%29%20has%20shown%20promising%20results%20in%20various%20symbolic%0Aplanning%20domains%20described%20by%20PDDL.%20However%2C%20existing%20approaches%20typically%0Arepresent%20planning%20states%20as%20fully%20connected%20graphs%2C%20leading%20to%20a%20combinatorial%0Aexplosion%20in%20edge%20information%20and%20substantial%20sparsity%20as%20problem%20scales%20grow%2C%0Aespecially%20evident%20in%20large%20grid-based%20environments.%20This%20dense%20representation%0Aresults%20in%20diluted%20node-level%20information%2C%20exponentially%20increases%20memory%0Arequirements%2C%20and%20ultimately%20makes%20learning%20infeasible%20for%20larger-scale%0Aproblems.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20sparse%2C%20goal-aware%20GNN%0Arepresentation%20that%20selectively%20encodes%20relevant%20local%20relationships%20and%0Aexplicitly%20integrates%20spatial%20features%20related%20to%20the%20goal.%20We%20validate%20our%0Aapproach%20by%20designing%20novel%20drone%20mission%20scenarios%20based%20on%20PDDL%20within%20a%20grid%0Aworld%2C%20effectively%20simulating%20realistic%20mission%20execution%20environments.%20Our%0Aexperimental%20results%20demonstrate%20that%20our%20method%20scales%20effectively%20to%20larger%0Agrid%20sizes%20previously%20infeasible%20with%20dense%20graph%20representations%20and%0Asubstantially%20improves%20policy%20generalization%20and%20success%20rates.%20Our%20findings%0Aprovide%20a%20practical%20foundation%20for%20addressing%20realistic%2C%20large-scale%0Ageneralized%20planning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10747v1&entry.124074799=Read"},
{"title": "Oops!... They Stole it Again: Attacks on Split Learning", "author": "Tanveer Khan and Antonis Michalas", "abstract": "  Split Learning (SL) is a collaborative learning approach that improves\nprivacy by keeping data on the client-side while sharing only the intermediate\noutput with a server. However, the distributed nature of SL introduces new\nsecurity challenges, necessitating a comprehensive exploration of potential\nattacks. This paper systematically reviews various attacks on SL, classifying\nthem based on factors such as the attacker's role, the type of privacy risks,\nwhen data leaks occur, and where vulnerabilities exist. We also analyze\nexisting defense methods, including cryptographic methods, data modification\napproaches, distributed techniques, and hybrid solutions. Our findings reveal\nsecurity gaps, highlighting the effectiveness and limitations of existing\ndefenses. By identifying open challenges and future directions, this work\nprovides valuable information to improve SL privacy issues and guide further\nresearch.\n", "link": "http://arxiv.org/abs/2508.10598v1", "date": "2025-08-14", "relevancy": 2.1292, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4314}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4307}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Oops%21...%20They%20Stole%20it%20Again%3A%20Attacks%20on%20Split%20Learning&body=Title%3A%20Oops%21...%20They%20Stole%20it%20Again%3A%20Attacks%20on%20Split%20Learning%0AAuthor%3A%20Tanveer%20Khan%20and%20Antonis%20Michalas%0AAbstract%3A%20%20%20Split%20Learning%20%28SL%29%20is%20a%20collaborative%20learning%20approach%20that%20improves%0Aprivacy%20by%20keeping%20data%20on%20the%20client-side%20while%20sharing%20only%20the%20intermediate%0Aoutput%20with%20a%20server.%20However%2C%20the%20distributed%20nature%20of%20SL%20introduces%20new%0Asecurity%20challenges%2C%20necessitating%20a%20comprehensive%20exploration%20of%20potential%0Aattacks.%20This%20paper%20systematically%20reviews%20various%20attacks%20on%20SL%2C%20classifying%0Athem%20based%20on%20factors%20such%20as%20the%20attacker%27s%20role%2C%20the%20type%20of%20privacy%20risks%2C%0Awhen%20data%20leaks%20occur%2C%20and%20where%20vulnerabilities%20exist.%20We%20also%20analyze%0Aexisting%20defense%20methods%2C%20including%20cryptographic%20methods%2C%20data%20modification%0Aapproaches%2C%20distributed%20techniques%2C%20and%20hybrid%20solutions.%20Our%20findings%20reveal%0Asecurity%20gaps%2C%20highlighting%20the%20effectiveness%20and%20limitations%20of%20existing%0Adefenses.%20By%20identifying%20open%20challenges%20and%20future%20directions%2C%20this%20work%0Aprovides%20valuable%20information%20to%20improve%20SL%20privacy%20issues%20and%20guide%20further%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOops%2521...%2520They%2520Stole%2520it%2520Again%253A%2520Attacks%2520on%2520Split%2520Learning%26entry.906535625%3DTanveer%2520Khan%2520and%2520Antonis%2520Michalas%26entry.1292438233%3D%2520%2520Split%2520Learning%2520%2528SL%2529%2520is%2520a%2520collaborative%2520learning%2520approach%2520that%2520improves%250Aprivacy%2520by%2520keeping%2520data%2520on%2520the%2520client-side%2520while%2520sharing%2520only%2520the%2520intermediate%250Aoutput%2520with%2520a%2520server.%2520However%252C%2520the%2520distributed%2520nature%2520of%2520SL%2520introduces%2520new%250Asecurity%2520challenges%252C%2520necessitating%2520a%2520comprehensive%2520exploration%2520of%2520potential%250Aattacks.%2520This%2520paper%2520systematically%2520reviews%2520various%2520attacks%2520on%2520SL%252C%2520classifying%250Athem%2520based%2520on%2520factors%2520such%2520as%2520the%2520attacker%2527s%2520role%252C%2520the%2520type%2520of%2520privacy%2520risks%252C%250Awhen%2520data%2520leaks%2520occur%252C%2520and%2520where%2520vulnerabilities%2520exist.%2520We%2520also%2520analyze%250Aexisting%2520defense%2520methods%252C%2520including%2520cryptographic%2520methods%252C%2520data%2520modification%250Aapproaches%252C%2520distributed%2520techniques%252C%2520and%2520hybrid%2520solutions.%2520Our%2520findings%2520reveal%250Asecurity%2520gaps%252C%2520highlighting%2520the%2520effectiveness%2520and%2520limitations%2520of%2520existing%250Adefenses.%2520By%2520identifying%2520open%2520challenges%2520and%2520future%2520directions%252C%2520this%2520work%250Aprovides%2520valuable%2520information%2520to%2520improve%2520SL%2520privacy%2520issues%2520and%2520guide%2520further%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Oops%21...%20They%20Stole%20it%20Again%3A%20Attacks%20on%20Split%20Learning&entry.906535625=Tanveer%20Khan%20and%20Antonis%20Michalas&entry.1292438233=%20%20Split%20Learning%20%28SL%29%20is%20a%20collaborative%20learning%20approach%20that%20improves%0Aprivacy%20by%20keeping%20data%20on%20the%20client-side%20while%20sharing%20only%20the%20intermediate%0Aoutput%20with%20a%20server.%20However%2C%20the%20distributed%20nature%20of%20SL%20introduces%20new%0Asecurity%20challenges%2C%20necessitating%20a%20comprehensive%20exploration%20of%20potential%0Aattacks.%20This%20paper%20systematically%20reviews%20various%20attacks%20on%20SL%2C%20classifying%0Athem%20based%20on%20factors%20such%20as%20the%20attacker%27s%20role%2C%20the%20type%20of%20privacy%20risks%2C%0Awhen%20data%20leaks%20occur%2C%20and%20where%20vulnerabilities%20exist.%20We%20also%20analyze%0Aexisting%20defense%20methods%2C%20including%20cryptographic%20methods%2C%20data%20modification%0Aapproaches%2C%20distributed%20techniques%2C%20and%20hybrid%20solutions.%20Our%20findings%20reveal%0Asecurity%20gaps%2C%20highlighting%20the%20effectiveness%20and%20limitations%20of%20existing%0Adefenses.%20By%20identifying%20open%20challenges%20and%20future%20directions%2C%20this%20work%0Aprovides%20valuable%20information%20to%20improve%20SL%20privacy%20issues%20and%20guide%20further%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10598v1&entry.124074799=Read"},
{"title": "Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for\n  Reliable Operation of Wheeled Mobile Robots", "author": "Mehdi Heydari Shahna and Jouni Mattila", "abstract": "  Deep neural networks (DNNs) can enable precise control while maintaining low\ncomputational costs by circumventing the need for dynamic modeling. However,\nthe deployment of such black-box approaches remains challenging for heavy-duty\nwheeled mobile robots (WMRs), which are subject to strict international\nstandards and prone to faults and disturbances. We designed a hierarchical\ncontrol policy for heavy-duty WMRs, monitored by two safety layers with\ndiffering levels of authority. To this end, a DNN policy was trained and\ndeployed as the primary control strategy, providing high-precision performance\nunder nominal operating conditions. When external disturbances arise and reach\na level of intensity such that the system performance falls below a predefined\nthreshold, a low-level safety layer intervenes by deactivating the primary\ncontrol policy and activating a model-free robust adaptive control (RAC)\npolicy. This transition enables the system to continue operating while ensuring\nstability by effectively managing the inherent trade-off between system\nrobustness and responsiveness. Regardless of the control policy in use, a\nhigh-level safety layer continuously monitors system performance during\noperation. It initiates a shutdown only when disturbances become sufficiently\nsevere such that compensation is no longer viable and continued operation would\njeopardize the system or its environment. The proposed synthesis of DNN and RAC\npolicy guarantees uniform exponential stability of the entire WMR system while\nadhering to safety standards to some extent. The effectiveness of the proposed\napproach was further validated through real-time experiments using a 6,000 kg\nWMR.\n", "link": "http://arxiv.org/abs/2508.10634v1", "date": "2025-08-14", "relevancy": 2.1275, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5362}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5288}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthesis%20of%20Deep%20Neural%20Networks%20with%20Safe%20Robust%20Adaptive%20Control%20for%0A%20%20Reliable%20Operation%20of%20Wheeled%20Mobile%20Robots&body=Title%3A%20Synthesis%20of%20Deep%20Neural%20Networks%20with%20Safe%20Robust%20Adaptive%20Control%20for%0A%20%20Reliable%20Operation%20of%20Wheeled%20Mobile%20Robots%0AAuthor%3A%20Mehdi%20Heydari%20Shahna%20and%20Jouni%20Mattila%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20can%20enable%20precise%20control%20while%20maintaining%20low%0Acomputational%20costs%20by%20circumventing%20the%20need%20for%20dynamic%20modeling.%20However%2C%0Athe%20deployment%20of%20such%20black-box%20approaches%20remains%20challenging%20for%20heavy-duty%0Awheeled%20mobile%20robots%20%28WMRs%29%2C%20which%20are%20subject%20to%20strict%20international%0Astandards%20and%20prone%20to%20faults%20and%20disturbances.%20We%20designed%20a%20hierarchical%0Acontrol%20policy%20for%20heavy-duty%20WMRs%2C%20monitored%20by%20two%20safety%20layers%20with%0Adiffering%20levels%20of%20authority.%20To%20this%20end%2C%20a%20DNN%20policy%20was%20trained%20and%0Adeployed%20as%20the%20primary%20control%20strategy%2C%20providing%20high-precision%20performance%0Aunder%20nominal%20operating%20conditions.%20When%20external%20disturbances%20arise%20and%20reach%0Aa%20level%20of%20intensity%20such%20that%20the%20system%20performance%20falls%20below%20a%20predefined%0Athreshold%2C%20a%20low-level%20safety%20layer%20intervenes%20by%20deactivating%20the%20primary%0Acontrol%20policy%20and%20activating%20a%20model-free%20robust%20adaptive%20control%20%28RAC%29%0Apolicy.%20This%20transition%20enables%20the%20system%20to%20continue%20operating%20while%20ensuring%0Astability%20by%20effectively%20managing%20the%20inherent%20trade-off%20between%20system%0Arobustness%20and%20responsiveness.%20Regardless%20of%20the%20control%20policy%20in%20use%2C%20a%0Ahigh-level%20safety%20layer%20continuously%20monitors%20system%20performance%20during%0Aoperation.%20It%20initiates%20a%20shutdown%20only%20when%20disturbances%20become%20sufficiently%0Asevere%20such%20that%20compensation%20is%20no%20longer%20viable%20and%20continued%20operation%20would%0Ajeopardize%20the%20system%20or%20its%20environment.%20The%20proposed%20synthesis%20of%20DNN%20and%20RAC%0Apolicy%20guarantees%20uniform%20exponential%20stability%20of%20the%20entire%20WMR%20system%20while%0Aadhering%20to%20safety%20standards%20to%20some%20extent.%20The%20effectiveness%20of%20the%20proposed%0Aapproach%20was%20further%20validated%20through%20real-time%20experiments%20using%20a%206%2C000%20kg%0AWMR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesis%2520of%2520Deep%2520Neural%2520Networks%2520with%2520Safe%2520Robust%2520Adaptive%2520Control%2520for%250A%2520%2520Reliable%2520Operation%2520of%2520Wheeled%2520Mobile%2520Robots%26entry.906535625%3DMehdi%2520Heydari%2520Shahna%2520and%2520Jouni%2520Mattila%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520can%2520enable%2520precise%2520control%2520while%2520maintaining%2520low%250Acomputational%2520costs%2520by%2520circumventing%2520the%2520need%2520for%2520dynamic%2520modeling.%2520However%252C%250Athe%2520deployment%2520of%2520such%2520black-box%2520approaches%2520remains%2520challenging%2520for%2520heavy-duty%250Awheeled%2520mobile%2520robots%2520%2528WMRs%2529%252C%2520which%2520are%2520subject%2520to%2520strict%2520international%250Astandards%2520and%2520prone%2520to%2520faults%2520and%2520disturbances.%2520We%2520designed%2520a%2520hierarchical%250Acontrol%2520policy%2520for%2520heavy-duty%2520WMRs%252C%2520monitored%2520by%2520two%2520safety%2520layers%2520with%250Adiffering%2520levels%2520of%2520authority.%2520To%2520this%2520end%252C%2520a%2520DNN%2520policy%2520was%2520trained%2520and%250Adeployed%2520as%2520the%2520primary%2520control%2520strategy%252C%2520providing%2520high-precision%2520performance%250Aunder%2520nominal%2520operating%2520conditions.%2520When%2520external%2520disturbances%2520arise%2520and%2520reach%250Aa%2520level%2520of%2520intensity%2520such%2520that%2520the%2520system%2520performance%2520falls%2520below%2520a%2520predefined%250Athreshold%252C%2520a%2520low-level%2520safety%2520layer%2520intervenes%2520by%2520deactivating%2520the%2520primary%250Acontrol%2520policy%2520and%2520activating%2520a%2520model-free%2520robust%2520adaptive%2520control%2520%2528RAC%2529%250Apolicy.%2520This%2520transition%2520enables%2520the%2520system%2520to%2520continue%2520operating%2520while%2520ensuring%250Astability%2520by%2520effectively%2520managing%2520the%2520inherent%2520trade-off%2520between%2520system%250Arobustness%2520and%2520responsiveness.%2520Regardless%2520of%2520the%2520control%2520policy%2520in%2520use%252C%2520a%250Ahigh-level%2520safety%2520layer%2520continuously%2520monitors%2520system%2520performance%2520during%250Aoperation.%2520It%2520initiates%2520a%2520shutdown%2520only%2520when%2520disturbances%2520become%2520sufficiently%250Asevere%2520such%2520that%2520compensation%2520is%2520no%2520longer%2520viable%2520and%2520continued%2520operation%2520would%250Ajeopardize%2520the%2520system%2520or%2520its%2520environment.%2520The%2520proposed%2520synthesis%2520of%2520DNN%2520and%2520RAC%250Apolicy%2520guarantees%2520uniform%2520exponential%2520stability%2520of%2520the%2520entire%2520WMR%2520system%2520while%250Aadhering%2520to%2520safety%2520standards%2520to%2520some%2520extent.%2520The%2520effectiveness%2520of%2520the%2520proposed%250Aapproach%2520was%2520further%2520validated%2520through%2520real-time%2520experiments%2520using%2520a%25206%252C000%2520kg%250AWMR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesis%20of%20Deep%20Neural%20Networks%20with%20Safe%20Robust%20Adaptive%20Control%20for%0A%20%20Reliable%20Operation%20of%20Wheeled%20Mobile%20Robots&entry.906535625=Mehdi%20Heydari%20Shahna%20and%20Jouni%20Mattila&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20can%20enable%20precise%20control%20while%20maintaining%20low%0Acomputational%20costs%20by%20circumventing%20the%20need%20for%20dynamic%20modeling.%20However%2C%0Athe%20deployment%20of%20such%20black-box%20approaches%20remains%20challenging%20for%20heavy-duty%0Awheeled%20mobile%20robots%20%28WMRs%29%2C%20which%20are%20subject%20to%20strict%20international%0Astandards%20and%20prone%20to%20faults%20and%20disturbances.%20We%20designed%20a%20hierarchical%0Acontrol%20policy%20for%20heavy-duty%20WMRs%2C%20monitored%20by%20two%20safety%20layers%20with%0Adiffering%20levels%20of%20authority.%20To%20this%20end%2C%20a%20DNN%20policy%20was%20trained%20and%0Adeployed%20as%20the%20primary%20control%20strategy%2C%20providing%20high-precision%20performance%0Aunder%20nominal%20operating%20conditions.%20When%20external%20disturbances%20arise%20and%20reach%0Aa%20level%20of%20intensity%20such%20that%20the%20system%20performance%20falls%20below%20a%20predefined%0Athreshold%2C%20a%20low-level%20safety%20layer%20intervenes%20by%20deactivating%20the%20primary%0Acontrol%20policy%20and%20activating%20a%20model-free%20robust%20adaptive%20control%20%28RAC%29%0Apolicy.%20This%20transition%20enables%20the%20system%20to%20continue%20operating%20while%20ensuring%0Astability%20by%20effectively%20managing%20the%20inherent%20trade-off%20between%20system%0Arobustness%20and%20responsiveness.%20Regardless%20of%20the%20control%20policy%20in%20use%2C%20a%0Ahigh-level%20safety%20layer%20continuously%20monitors%20system%20performance%20during%0Aoperation.%20It%20initiates%20a%20shutdown%20only%20when%20disturbances%20become%20sufficiently%0Asevere%20such%20that%20compensation%20is%20no%20longer%20viable%20and%20continued%20operation%20would%0Ajeopardize%20the%20system%20or%20its%20environment.%20The%20proposed%20synthesis%20of%20DNN%20and%20RAC%0Apolicy%20guarantees%20uniform%20exponential%20stability%20of%20the%20entire%20WMR%20system%20while%0Aadhering%20to%20safety%20standards%20to%20some%20extent.%20The%20effectiveness%20of%20the%20proposed%0Aapproach%20was%20further%20validated%20through%20real-time%20experiments%20using%20a%206%2C000%20kg%0AWMR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10634v1&entry.124074799=Read"},
{"title": "MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and\n  Multispectral Earth Observation Data", "author": "Antoine Labatie and Michael Vaccaro and Nina Lardiere and Anatol Garioud and Nicolas Gonthier", "abstract": "  Self-supervised learning holds great promise for remote sensing, but standard\nself-supervised methods must be adapted to the unique characteristics of Earth\nobservation data. We take a step in this direction by conducting a\ncomprehensive benchmark of fusion strategies and reconstruction target\nnormalization schemes for multimodal, multitemporal, and multispectral Earth\nobservation data. Based on our findings, we propose MAESTRO, a novel adaptation\nof the Masked Autoencoder, featuring optimized fusion strategies and a tailored\ntarget normalization scheme that introduces a spectral prior as a\nself-supervisory signal. Evaluated on four Earth observation datasets, MAESTRO\nsets a new state-of-the-art on tasks that strongly rely on multitemporal\ndynamics, while remaining highly competitive on tasks dominated by a single\nmono-temporal modality. Code to reproduce all our experiments is available at\nhttps://github.com/ignf/maestro.\n", "link": "http://arxiv.org/abs/2508.10894v1", "date": "2025-08-14", "relevancy": 2.1263, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5423}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5253}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAESTRO%3A%20Masked%20AutoEncoders%20for%20Multimodal%2C%20Multitemporal%2C%20and%0A%20%20Multispectral%20Earth%20Observation%20Data&body=Title%3A%20MAESTRO%3A%20Masked%20AutoEncoders%20for%20Multimodal%2C%20Multitemporal%2C%20and%0A%20%20Multispectral%20Earth%20Observation%20Data%0AAuthor%3A%20Antoine%20Labatie%20and%20Michael%20Vaccaro%20and%20Nina%20Lardiere%20and%20Anatol%20Garioud%20and%20Nicolas%20Gonthier%0AAbstract%3A%20%20%20Self-supervised%20learning%20holds%20great%20promise%20for%20remote%20sensing%2C%20but%20standard%0Aself-supervised%20methods%20must%20be%20adapted%20to%20the%20unique%20characteristics%20of%20Earth%0Aobservation%20data.%20We%20take%20a%20step%20in%20this%20direction%20by%20conducting%20a%0Acomprehensive%20benchmark%20of%20fusion%20strategies%20and%20reconstruction%20target%0Anormalization%20schemes%20for%20multimodal%2C%20multitemporal%2C%20and%20multispectral%20Earth%0Aobservation%20data.%20Based%20on%20our%20findings%2C%20we%20propose%20MAESTRO%2C%20a%20novel%20adaptation%0Aof%20the%20Masked%20Autoencoder%2C%20featuring%20optimized%20fusion%20strategies%20and%20a%20tailored%0Atarget%20normalization%20scheme%20that%20introduces%20a%20spectral%20prior%20as%20a%0Aself-supervisory%20signal.%20Evaluated%20on%20four%20Earth%20observation%20datasets%2C%20MAESTRO%0Asets%20a%20new%20state-of-the-art%20on%20tasks%20that%20strongly%20rely%20on%20multitemporal%0Adynamics%2C%20while%20remaining%20highly%20competitive%20on%20tasks%20dominated%20by%20a%20single%0Amono-temporal%20modality.%20Code%20to%20reproduce%20all%20our%20experiments%20is%20available%20at%0Ahttps%3A//github.com/ignf/maestro.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAESTRO%253A%2520Masked%2520AutoEncoders%2520for%2520Multimodal%252C%2520Multitemporal%252C%2520and%250A%2520%2520Multispectral%2520Earth%2520Observation%2520Data%26entry.906535625%3DAntoine%2520Labatie%2520and%2520Michael%2520Vaccaro%2520and%2520Nina%2520Lardiere%2520and%2520Anatol%2520Garioud%2520and%2520Nicolas%2520Gonthier%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520holds%2520great%2520promise%2520for%2520remote%2520sensing%252C%2520but%2520standard%250Aself-supervised%2520methods%2520must%2520be%2520adapted%2520to%2520the%2520unique%2520characteristics%2520of%2520Earth%250Aobservation%2520data.%2520We%2520take%2520a%2520step%2520in%2520this%2520direction%2520by%2520conducting%2520a%250Acomprehensive%2520benchmark%2520of%2520fusion%2520strategies%2520and%2520reconstruction%2520target%250Anormalization%2520schemes%2520for%2520multimodal%252C%2520multitemporal%252C%2520and%2520multispectral%2520Earth%250Aobservation%2520data.%2520Based%2520on%2520our%2520findings%252C%2520we%2520propose%2520MAESTRO%252C%2520a%2520novel%2520adaptation%250Aof%2520the%2520Masked%2520Autoencoder%252C%2520featuring%2520optimized%2520fusion%2520strategies%2520and%2520a%2520tailored%250Atarget%2520normalization%2520scheme%2520that%2520introduces%2520a%2520spectral%2520prior%2520as%2520a%250Aself-supervisory%2520signal.%2520Evaluated%2520on%2520four%2520Earth%2520observation%2520datasets%252C%2520MAESTRO%250Asets%2520a%2520new%2520state-of-the-art%2520on%2520tasks%2520that%2520strongly%2520rely%2520on%2520multitemporal%250Adynamics%252C%2520while%2520remaining%2520highly%2520competitive%2520on%2520tasks%2520dominated%2520by%2520a%2520single%250Amono-temporal%2520modality.%2520Code%2520to%2520reproduce%2520all%2520our%2520experiments%2520is%2520available%2520at%250Ahttps%253A//github.com/ignf/maestro.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAESTRO%3A%20Masked%20AutoEncoders%20for%20Multimodal%2C%20Multitemporal%2C%20and%0A%20%20Multispectral%20Earth%20Observation%20Data&entry.906535625=Antoine%20Labatie%20and%20Michael%20Vaccaro%20and%20Nina%20Lardiere%20and%20Anatol%20Garioud%20and%20Nicolas%20Gonthier&entry.1292438233=%20%20Self-supervised%20learning%20holds%20great%20promise%20for%20remote%20sensing%2C%20but%20standard%0Aself-supervised%20methods%20must%20be%20adapted%20to%20the%20unique%20characteristics%20of%20Earth%0Aobservation%20data.%20We%20take%20a%20step%20in%20this%20direction%20by%20conducting%20a%0Acomprehensive%20benchmark%20of%20fusion%20strategies%20and%20reconstruction%20target%0Anormalization%20schemes%20for%20multimodal%2C%20multitemporal%2C%20and%20multispectral%20Earth%0Aobservation%20data.%20Based%20on%20our%20findings%2C%20we%20propose%20MAESTRO%2C%20a%20novel%20adaptation%0Aof%20the%20Masked%20Autoencoder%2C%20featuring%20optimized%20fusion%20strategies%20and%20a%20tailored%0Atarget%20normalization%20scheme%20that%20introduces%20a%20spectral%20prior%20as%20a%0Aself-supervisory%20signal.%20Evaluated%20on%20four%20Earth%20observation%20datasets%2C%20MAESTRO%0Asets%20a%20new%20state-of-the-art%20on%20tasks%20that%20strongly%20rely%20on%20multitemporal%0Adynamics%2C%20while%20remaining%20highly%20competitive%20on%20tasks%20dominated%20by%20a%20single%0Amono-temporal%20modality.%20Code%20to%20reproduce%20all%20our%20experiments%20is%20available%20at%0Ahttps%3A//github.com/ignf/maestro.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10894v1&entry.124074799=Read"},
{"title": "MDNS: Masked Diffusion Neural Sampler via Stochastic Optimal Control", "author": "Yuchen Zhu and Wei Guo and Jaemoo Choi and Guan-Horng Liu and Yongxin Chen and Molei Tao", "abstract": "  We study the problem of learning a neural sampler to generate samples from\ndiscrete state spaces where the target probability mass function\n$\\pi\\propto\\mathrm{e}^{-U}$ is known up to a normalizing constant, which is an\nimportant task in fields such as statistical physics, machine learning,\ncombinatorial optimization, etc. To better address this challenging task when\nthe state space has a large cardinality and the distribution is multi-modal, we\npropose $\\textbf{M}$asked $\\textbf{D}$iffusion $\\textbf{N}$eural\n$\\textbf{S}$ampler ($\\textbf{MDNS}$), a novel framework for training discrete\nneural samplers by aligning two path measures through a family of learning\nobjectives, theoretically grounded in the stochastic optimal control of the\ncontinuous-time Markov chains. We validate the efficiency and scalability of\nMDNS through extensive experiments on various distributions with distinct\nstatistical properties, where MDNS learns to accurately sample from the target\ndistributions despite the extremely high problem dimensions and outperforms\nother learning-based baselines by a large margin. A comprehensive study of\nablations and extensions is also provided to demonstrate the efficacy and\npotential of the proposed framework.\n", "link": "http://arxiv.org/abs/2508.10684v1", "date": "2025-08-14", "relevancy": 2.1135, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5746}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5214}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MDNS%3A%20Masked%20Diffusion%20Neural%20Sampler%20via%20Stochastic%20Optimal%20Control&body=Title%3A%20MDNS%3A%20Masked%20Diffusion%20Neural%20Sampler%20via%20Stochastic%20Optimal%20Control%0AAuthor%3A%20Yuchen%20Zhu%20and%20Wei%20Guo%20and%20Jaemoo%20Choi%20and%20Guan-Horng%20Liu%20and%20Yongxin%20Chen%20and%20Molei%20Tao%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20learning%20a%20neural%20sampler%20to%20generate%20samples%20from%0Adiscrete%20state%20spaces%20where%20the%20target%20probability%20mass%20function%0A%24%5Cpi%5Cpropto%5Cmathrm%7Be%7D%5E%7B-U%7D%24%20is%20known%20up%20to%20a%20normalizing%20constant%2C%20which%20is%20an%0Aimportant%20task%20in%20fields%20such%20as%20statistical%20physics%2C%20machine%20learning%2C%0Acombinatorial%20optimization%2C%20etc.%20To%20better%20address%20this%20challenging%20task%20when%0Athe%20state%20space%20has%20a%20large%20cardinality%20and%20the%20distribution%20is%20multi-modal%2C%20we%0Apropose%20%24%5Ctextbf%7BM%7D%24asked%20%24%5Ctextbf%7BD%7D%24iffusion%20%24%5Ctextbf%7BN%7D%24eural%0A%24%5Ctextbf%7BS%7D%24ampler%20%28%24%5Ctextbf%7BMDNS%7D%24%29%2C%20a%20novel%20framework%20for%20training%20discrete%0Aneural%20samplers%20by%20aligning%20two%20path%20measures%20through%20a%20family%20of%20learning%0Aobjectives%2C%20theoretically%20grounded%20in%20the%20stochastic%20optimal%20control%20of%20the%0Acontinuous-time%20Markov%20chains.%20We%20validate%20the%20efficiency%20and%20scalability%20of%0AMDNS%20through%20extensive%20experiments%20on%20various%20distributions%20with%20distinct%0Astatistical%20properties%2C%20where%20MDNS%20learns%20to%20accurately%20sample%20from%20the%20target%0Adistributions%20despite%20the%20extremely%20high%20problem%20dimensions%20and%20outperforms%0Aother%20learning-based%20baselines%20by%20a%20large%20margin.%20A%20comprehensive%20study%20of%0Aablations%20and%20extensions%20is%20also%20provided%20to%20demonstrate%20the%20efficacy%20and%0Apotential%20of%20the%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMDNS%253A%2520Masked%2520Diffusion%2520Neural%2520Sampler%2520via%2520Stochastic%2520Optimal%2520Control%26entry.906535625%3DYuchen%2520Zhu%2520and%2520Wei%2520Guo%2520and%2520Jaemoo%2520Choi%2520and%2520Guan-Horng%2520Liu%2520and%2520Yongxin%2520Chen%2520and%2520Molei%2520Tao%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520learning%2520a%2520neural%2520sampler%2520to%2520generate%2520samples%2520from%250Adiscrete%2520state%2520spaces%2520where%2520the%2520target%2520probability%2520mass%2520function%250A%2524%255Cpi%255Cpropto%255Cmathrm%257Be%257D%255E%257B-U%257D%2524%2520is%2520known%2520up%2520to%2520a%2520normalizing%2520constant%252C%2520which%2520is%2520an%250Aimportant%2520task%2520in%2520fields%2520such%2520as%2520statistical%2520physics%252C%2520machine%2520learning%252C%250Acombinatorial%2520optimization%252C%2520etc.%2520To%2520better%2520address%2520this%2520challenging%2520task%2520when%250Athe%2520state%2520space%2520has%2520a%2520large%2520cardinality%2520and%2520the%2520distribution%2520is%2520multi-modal%252C%2520we%250Apropose%2520%2524%255Ctextbf%257BM%257D%2524asked%2520%2524%255Ctextbf%257BD%257D%2524iffusion%2520%2524%255Ctextbf%257BN%257D%2524eural%250A%2524%255Ctextbf%257BS%257D%2524ampler%2520%2528%2524%255Ctextbf%257BMDNS%257D%2524%2529%252C%2520a%2520novel%2520framework%2520for%2520training%2520discrete%250Aneural%2520samplers%2520by%2520aligning%2520two%2520path%2520measures%2520through%2520a%2520family%2520of%2520learning%250Aobjectives%252C%2520theoretically%2520grounded%2520in%2520the%2520stochastic%2520optimal%2520control%2520of%2520the%250Acontinuous-time%2520Markov%2520chains.%2520We%2520validate%2520the%2520efficiency%2520and%2520scalability%2520of%250AMDNS%2520through%2520extensive%2520experiments%2520on%2520various%2520distributions%2520with%2520distinct%250Astatistical%2520properties%252C%2520where%2520MDNS%2520learns%2520to%2520accurately%2520sample%2520from%2520the%2520target%250Adistributions%2520despite%2520the%2520extremely%2520high%2520problem%2520dimensions%2520and%2520outperforms%250Aother%2520learning-based%2520baselines%2520by%2520a%2520large%2520margin.%2520A%2520comprehensive%2520study%2520of%250Aablations%2520and%2520extensions%2520is%2520also%2520provided%2520to%2520demonstrate%2520the%2520efficacy%2520and%250Apotential%2520of%2520the%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MDNS%3A%20Masked%20Diffusion%20Neural%20Sampler%20via%20Stochastic%20Optimal%20Control&entry.906535625=Yuchen%20Zhu%20and%20Wei%20Guo%20and%20Jaemoo%20Choi%20and%20Guan-Horng%20Liu%20and%20Yongxin%20Chen%20and%20Molei%20Tao&entry.1292438233=%20%20We%20study%20the%20problem%20of%20learning%20a%20neural%20sampler%20to%20generate%20samples%20from%0Adiscrete%20state%20spaces%20where%20the%20target%20probability%20mass%20function%0A%24%5Cpi%5Cpropto%5Cmathrm%7Be%7D%5E%7B-U%7D%24%20is%20known%20up%20to%20a%20normalizing%20constant%2C%20which%20is%20an%0Aimportant%20task%20in%20fields%20such%20as%20statistical%20physics%2C%20machine%20learning%2C%0Acombinatorial%20optimization%2C%20etc.%20To%20better%20address%20this%20challenging%20task%20when%0Athe%20state%20space%20has%20a%20large%20cardinality%20and%20the%20distribution%20is%20multi-modal%2C%20we%0Apropose%20%24%5Ctextbf%7BM%7D%24asked%20%24%5Ctextbf%7BD%7D%24iffusion%20%24%5Ctextbf%7BN%7D%24eural%0A%24%5Ctextbf%7BS%7D%24ampler%20%28%24%5Ctextbf%7BMDNS%7D%24%29%2C%20a%20novel%20framework%20for%20training%20discrete%0Aneural%20samplers%20by%20aligning%20two%20path%20measures%20through%20a%20family%20of%20learning%0Aobjectives%2C%20theoretically%20grounded%20in%20the%20stochastic%20optimal%20control%20of%20the%0Acontinuous-time%20Markov%20chains.%20We%20validate%20the%20efficiency%20and%20scalability%20of%0AMDNS%20through%20extensive%20experiments%20on%20various%20distributions%20with%20distinct%0Astatistical%20properties%2C%20where%20MDNS%20learns%20to%20accurately%20sample%20from%20the%20target%0Adistributions%20despite%20the%20extremely%20high%20problem%20dimensions%20and%20outperforms%0Aother%20learning-based%20baselines%20by%20a%20large%20margin.%20A%20comprehensive%20study%20of%0Aablations%20and%20extensions%20is%20also%20provided%20to%20demonstrate%20the%20efficacy%20and%0Apotential%20of%20the%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10684v1&entry.124074799=Read"},
{"title": "WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer", "author": "Junyu Wu and Weiming Chang and Xiaotao Liu and Guanyou He and Tingfeng Xian and Haoqiang Hong and Boqi Chen and Haotao Tian and Tao Yang and Yunsheng Shi and Feng Lin and Ting Yao", "abstract": "  Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent\nparadigm for training large language models and multimodal systems. Despite\nnotable advances enabled by existing RLHF training frameworks, significant\nchallenges remain in scaling to complex multimodal workflows and adapting to\ndynamic workloads. In particular, current systems often encounter limitations\nrelated to controller scalability when managing large models, as well as\ninefficiencies in orchestrating intricate RLHF pipelines, especially in\nscenarios that require dynamic sampling and resource allocation. In this paper,\nwe introduce WeChat-YATT (Yet Another Transformer Trainer in WeChat), a simple,\nscalable, and balanced RLHF training framework specifically designed to address\nthese challenges. WeChat-YATT features a parallel controller programming model\nthat enables flexible and efficient orchestration of complex RLHF workflows,\neffectively mitigating the bottlenecks associated with centralized controller\narchitectures and facilitating scalability in large-scale data scenarios. In\naddition, we propose a dynamic placement schema that adaptively partitions\ncomputational resources and schedules workloads, thereby significantly reducing\nhardware idle time and improving GPU utilization under variable training\nconditions. We evaluate WeChat-YATT across a range of experimental scenarios,\ndemonstrating that it achieves substantial improvements in throughput compared\nto state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been\nsuccessfully deployed to train models supporting WeChat product features for a\nlarge-scale user base, underscoring its effectiveness and robustness in\nreal-world applications.We have open-source WeChat-YATT at\nhttps://www.github.com/tencent/WeChat-YATT.\n", "link": "http://arxiv.org/abs/2508.07970v2", "date": "2025-08-14", "relevancy": 2.1011, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.545}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5312}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WeChat-YATT%3A%20A%20Simple%2C%20Scalable%20and%20Balanced%20RLHF%20Trainer&body=Title%3A%20WeChat-YATT%3A%20A%20Simple%2C%20Scalable%20and%20Balanced%20RLHF%20Trainer%0AAuthor%3A%20Junyu%20Wu%20and%20Weiming%20Chang%20and%20Xiaotao%20Liu%20and%20Guanyou%20He%20and%20Tingfeng%20Xian%20and%20Haoqiang%20Hong%20and%20Boqi%20Chen%20and%20Haotao%20Tian%20and%20Tao%20Yang%20and%20Yunsheng%20Shi%20and%20Feng%20Lin%20and%20Ting%20Yao%0AAbstract%3A%20%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20has%20emerged%20as%20a%20prominent%0Aparadigm%20for%20training%20large%20language%20models%20and%20multimodal%20systems.%20Despite%0Anotable%20advances%20enabled%20by%20existing%20RLHF%20training%20frameworks%2C%20significant%0Achallenges%20remain%20in%20scaling%20to%20complex%20multimodal%20workflows%20and%20adapting%20to%0Adynamic%20workloads.%20In%20particular%2C%20current%20systems%20often%20encounter%20limitations%0Arelated%20to%20controller%20scalability%20when%20managing%20large%20models%2C%20as%20well%20as%0Ainefficiencies%20in%20orchestrating%20intricate%20RLHF%20pipelines%2C%20especially%20in%0Ascenarios%20that%20require%20dynamic%20sampling%20and%20resource%20allocation.%20In%20this%20paper%2C%0Awe%20introduce%20WeChat-YATT%20%28Yet%20Another%20Transformer%20Trainer%20in%20WeChat%29%2C%20a%20simple%2C%0Ascalable%2C%20and%20balanced%20RLHF%20training%20framework%20specifically%20designed%20to%20address%0Athese%20challenges.%20WeChat-YATT%20features%20a%20parallel%20controller%20programming%20model%0Athat%20enables%20flexible%20and%20efficient%20orchestration%20of%20complex%20RLHF%20workflows%2C%0Aeffectively%20mitigating%20the%20bottlenecks%20associated%20with%20centralized%20controller%0Aarchitectures%20and%20facilitating%20scalability%20in%20large-scale%20data%20scenarios.%20In%0Aaddition%2C%20we%20propose%20a%20dynamic%20placement%20schema%20that%20adaptively%20partitions%0Acomputational%20resources%20and%20schedules%20workloads%2C%20thereby%20significantly%20reducing%0Ahardware%20idle%20time%20and%20improving%20GPU%20utilization%20under%20variable%20training%0Aconditions.%20We%20evaluate%20WeChat-YATT%20across%20a%20range%20of%20experimental%20scenarios%2C%0Ademonstrating%20that%20it%20achieves%20substantial%20improvements%20in%20throughput%20compared%0Ato%20state-of-the-art%20RLHF%20training%20frameworks.%20Furthermore%2C%20WeChat-YATT%20has%20been%0Asuccessfully%20deployed%20to%20train%20models%20supporting%20WeChat%20product%20features%20for%20a%0Alarge-scale%20user%20base%2C%20underscoring%20its%20effectiveness%20and%20robustness%20in%0Areal-world%20applications.We%20have%20open-source%20WeChat-YATT%20at%0Ahttps%3A//www.github.com/tencent/WeChat-YATT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07970v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeChat-YATT%253A%2520A%2520Simple%252C%2520Scalable%2520and%2520Balanced%2520RLHF%2520Trainer%26entry.906535625%3DJunyu%2520Wu%2520and%2520Weiming%2520Chang%2520and%2520Xiaotao%2520Liu%2520and%2520Guanyou%2520He%2520and%2520Tingfeng%2520Xian%2520and%2520Haoqiang%2520Hong%2520and%2520Boqi%2520Chen%2520and%2520Haotao%2520Tian%2520and%2520Tao%2520Yang%2520and%2520Yunsheng%2520Shi%2520and%2520Feng%2520Lin%2520and%2520Ting%2520Yao%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520has%2520emerged%2520as%2520a%2520prominent%250Aparadigm%2520for%2520training%2520large%2520language%2520models%2520and%2520multimodal%2520systems.%2520Despite%250Anotable%2520advances%2520enabled%2520by%2520existing%2520RLHF%2520training%2520frameworks%252C%2520significant%250Achallenges%2520remain%2520in%2520scaling%2520to%2520complex%2520multimodal%2520workflows%2520and%2520adapting%2520to%250Adynamic%2520workloads.%2520In%2520particular%252C%2520current%2520systems%2520often%2520encounter%2520limitations%250Arelated%2520to%2520controller%2520scalability%2520when%2520managing%2520large%2520models%252C%2520as%2520well%2520as%250Ainefficiencies%2520in%2520orchestrating%2520intricate%2520RLHF%2520pipelines%252C%2520especially%2520in%250Ascenarios%2520that%2520require%2520dynamic%2520sampling%2520and%2520resource%2520allocation.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520WeChat-YATT%2520%2528Yet%2520Another%2520Transformer%2520Trainer%2520in%2520WeChat%2529%252C%2520a%2520simple%252C%250Ascalable%252C%2520and%2520balanced%2520RLHF%2520training%2520framework%2520specifically%2520designed%2520to%2520address%250Athese%2520challenges.%2520WeChat-YATT%2520features%2520a%2520parallel%2520controller%2520programming%2520model%250Athat%2520enables%2520flexible%2520and%2520efficient%2520orchestration%2520of%2520complex%2520RLHF%2520workflows%252C%250Aeffectively%2520mitigating%2520the%2520bottlenecks%2520associated%2520with%2520centralized%2520controller%250Aarchitectures%2520and%2520facilitating%2520scalability%2520in%2520large-scale%2520data%2520scenarios.%2520In%250Aaddition%252C%2520we%2520propose%2520a%2520dynamic%2520placement%2520schema%2520that%2520adaptively%2520partitions%250Acomputational%2520resources%2520and%2520schedules%2520workloads%252C%2520thereby%2520significantly%2520reducing%250Ahardware%2520idle%2520time%2520and%2520improving%2520GPU%2520utilization%2520under%2520variable%2520training%250Aconditions.%2520We%2520evaluate%2520WeChat-YATT%2520across%2520a%2520range%2520of%2520experimental%2520scenarios%252C%250Ademonstrating%2520that%2520it%2520achieves%2520substantial%2520improvements%2520in%2520throughput%2520compared%250Ato%2520state-of-the-art%2520RLHF%2520training%2520frameworks.%2520Furthermore%252C%2520WeChat-YATT%2520has%2520been%250Asuccessfully%2520deployed%2520to%2520train%2520models%2520supporting%2520WeChat%2520product%2520features%2520for%2520a%250Alarge-scale%2520user%2520base%252C%2520underscoring%2520its%2520effectiveness%2520and%2520robustness%2520in%250Areal-world%2520applications.We%2520have%2520open-source%2520WeChat-YATT%2520at%250Ahttps%253A//www.github.com/tencent/WeChat-YATT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07970v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WeChat-YATT%3A%20A%20Simple%2C%20Scalable%20and%20Balanced%20RLHF%20Trainer&entry.906535625=Junyu%20Wu%20and%20Weiming%20Chang%20and%20Xiaotao%20Liu%20and%20Guanyou%20He%20and%20Tingfeng%20Xian%20and%20Haoqiang%20Hong%20and%20Boqi%20Chen%20and%20Haotao%20Tian%20and%20Tao%20Yang%20and%20Yunsheng%20Shi%20and%20Feng%20Lin%20and%20Ting%20Yao&entry.1292438233=%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20has%20emerged%20as%20a%20prominent%0Aparadigm%20for%20training%20large%20language%20models%20and%20multimodal%20systems.%20Despite%0Anotable%20advances%20enabled%20by%20existing%20RLHF%20training%20frameworks%2C%20significant%0Achallenges%20remain%20in%20scaling%20to%20complex%20multimodal%20workflows%20and%20adapting%20to%0Adynamic%20workloads.%20In%20particular%2C%20current%20systems%20often%20encounter%20limitations%0Arelated%20to%20controller%20scalability%20when%20managing%20large%20models%2C%20as%20well%20as%0Ainefficiencies%20in%20orchestrating%20intricate%20RLHF%20pipelines%2C%20especially%20in%0Ascenarios%20that%20require%20dynamic%20sampling%20and%20resource%20allocation.%20In%20this%20paper%2C%0Awe%20introduce%20WeChat-YATT%20%28Yet%20Another%20Transformer%20Trainer%20in%20WeChat%29%2C%20a%20simple%2C%0Ascalable%2C%20and%20balanced%20RLHF%20training%20framework%20specifically%20designed%20to%20address%0Athese%20challenges.%20WeChat-YATT%20features%20a%20parallel%20controller%20programming%20model%0Athat%20enables%20flexible%20and%20efficient%20orchestration%20of%20complex%20RLHF%20workflows%2C%0Aeffectively%20mitigating%20the%20bottlenecks%20associated%20with%20centralized%20controller%0Aarchitectures%20and%20facilitating%20scalability%20in%20large-scale%20data%20scenarios.%20In%0Aaddition%2C%20we%20propose%20a%20dynamic%20placement%20schema%20that%20adaptively%20partitions%0Acomputational%20resources%20and%20schedules%20workloads%2C%20thereby%20significantly%20reducing%0Ahardware%20idle%20time%20and%20improving%20GPU%20utilization%20under%20variable%20training%0Aconditions.%20We%20evaluate%20WeChat-YATT%20across%20a%20range%20of%20experimental%20scenarios%2C%0Ademonstrating%20that%20it%20achieves%20substantial%20improvements%20in%20throughput%20compared%0Ato%20state-of-the-art%20RLHF%20training%20frameworks.%20Furthermore%2C%20WeChat-YATT%20has%20been%0Asuccessfully%20deployed%20to%20train%20models%20supporting%20WeChat%20product%20features%20for%20a%0Alarge-scale%20user%20base%2C%20underscoring%20its%20effectiveness%20and%20robustness%20in%0Areal-world%20applications.We%20have%20open-source%20WeChat-YATT%20at%0Ahttps%3A//www.github.com/tencent/WeChat-YATT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07970v2&entry.124074799=Read"},
{"title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache", "author": "Dayou Du and Shijie Cao and Jianyi Cheng and Luo Mai and Ting Cao and Mao Yang", "abstract": "  The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding.\n", "link": "http://arxiv.org/abs/2503.18773v2", "date": "2025-08-14", "relevancy": 2.0961, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BitDecoding%3A%20Unlocking%20Tensor%20Cores%20for%20Long-Context%20LLMs%20with%20Low-Bit%0A%20%20KV%20Cache&body=Title%3A%20BitDecoding%3A%20Unlocking%20Tensor%20Cores%20for%20Long-Context%20LLMs%20with%20Low-Bit%0A%20%20KV%20Cache%0AAuthor%3A%20Dayou%20Du%20and%20Shijie%20Cao%20and%20Jianyi%20Cheng%20and%20Luo%20Mai%20and%20Ting%20Cao%20and%20Mao%20Yang%0AAbstract%3A%20%20%20The%20rise%20of%20long-context%20Large%20Language%20Models%20%28LLMs%29%20amplifies%20memory%20and%0Abandwidth%20demands%20during%20autoregressive%20decoding%2C%20as%20the%20Key-Value%20%28KV%29%20cache%0Agrows%20with%20each%20generated%20token.%20Low-bit%20KV-cache%20quantization%20%28e.g.%2C%204-bit%20or%0A2-bit%29%20can%20reduce%20memory%20footprint%20while%20preserving%20accuracy%2C%20but%20existing%0Asystems%20suffer%20from%20slow%20decoding%20due%20to%20their%20exclusive%20reliance%20on%20CUDA%0Acores%2C%20neglecting%20Tensor%20Cores%20%28the%20primary%20source%20of%20compute%20on%20modern%20GPUs%29.%0AWe%20present%20BitDecoding%2C%20a%20new%20long-context%20LLM%20inference%20system%20with%20a%20low-bit%0AKV%20cache.%20BitDecoding%20enables%20efficient%20low-bit%20KV-cache%20decoding%20by%0Acooperatively%20leveraging%20CUDA%20cores%20and%20Tensor%20Cores.%20It%20introduces%20methods%20for%0Aautomatically%20inducing%20optimized%20layouts%20to%20exploit%20Tensor%20Cores%2C%20along%20with%0Awarp-level%20parallelization%20strategies%20for%20dequantization.%20For%20unified%20system%0Asupport%2C%20BitDecoding%20includes%20a%20query%20transformation%20module%20supporting%20diverse%0Aattention%20variants%2C%20a%20quantization%20kernel%20that%20supports%20both%20tensor-wise%20and%0Achannel-wise%20scaling%20used%20in%20various%20quantization%20algorithms%20with%20high%0Aperformance%2C%20and%20a%20dequantization%20kernel%20with%20a%20software-defined%20pipeline%20to%0Acoordinate%20CUDA%20and%20Tensor%20Cores%20execution%20for%20mixed-precision%20operations.%0AEvaluated%20on%20RTX%204090%2C%20A100%2C%20and%20H100%2C%20BitDecoding%20accelerates%20decoding%20by%20up%0Ato%207.5x%2C%204.8x%2C%20and%208.9x%2C%20respectively%2C%20over%20FP16%20FlashDecoding-v2%2C%20and%0Asurpasses%20the%20state-of-the-art%20low-bit%20system%20QServe%20by%20up%20to%204.3x.%20On%0ALLaMA-3.1-8B%20with%20a%20128K%20context%2C%20BitDecoding%20reduces%20single-batch%20decoding%0Alatency%20by%203x%2C%20showing%20substantial%20improvements%20for%20long-context%20generation.%0AThe%20code%20is%20available%20at%20https%3A//github.com/DD-DuDa/BitDecoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18773v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBitDecoding%253A%2520Unlocking%2520Tensor%2520Cores%2520for%2520Long-Context%2520LLMs%2520with%2520Low-Bit%250A%2520%2520KV%2520Cache%26entry.906535625%3DDayou%2520Du%2520and%2520Shijie%2520Cao%2520and%2520Jianyi%2520Cheng%2520and%2520Luo%2520Mai%2520and%2520Ting%2520Cao%2520and%2520Mao%2520Yang%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520long-context%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520amplifies%2520memory%2520and%250Abandwidth%2520demands%2520during%2520autoregressive%2520decoding%252C%2520as%2520the%2520Key-Value%2520%2528KV%2529%2520cache%250Agrows%2520with%2520each%2520generated%2520token.%2520Low-bit%2520KV-cache%2520quantization%2520%2528e.g.%252C%25204-bit%2520or%250A2-bit%2529%2520can%2520reduce%2520memory%2520footprint%2520while%2520preserving%2520accuracy%252C%2520but%2520existing%250Asystems%2520suffer%2520from%2520slow%2520decoding%2520due%2520to%2520their%2520exclusive%2520reliance%2520on%2520CUDA%250Acores%252C%2520neglecting%2520Tensor%2520Cores%2520%2528the%2520primary%2520source%2520of%2520compute%2520on%2520modern%2520GPUs%2529.%250AWe%2520present%2520BitDecoding%252C%2520a%2520new%2520long-context%2520LLM%2520inference%2520system%2520with%2520a%2520low-bit%250AKV%2520cache.%2520BitDecoding%2520enables%2520efficient%2520low-bit%2520KV-cache%2520decoding%2520by%250Acooperatively%2520leveraging%2520CUDA%2520cores%2520and%2520Tensor%2520Cores.%2520It%2520introduces%2520methods%2520for%250Aautomatically%2520inducing%2520optimized%2520layouts%2520to%2520exploit%2520Tensor%2520Cores%252C%2520along%2520with%250Awarp-level%2520parallelization%2520strategies%2520for%2520dequantization.%2520For%2520unified%2520system%250Asupport%252C%2520BitDecoding%2520includes%2520a%2520query%2520transformation%2520module%2520supporting%2520diverse%250Aattention%2520variants%252C%2520a%2520quantization%2520kernel%2520that%2520supports%2520both%2520tensor-wise%2520and%250Achannel-wise%2520scaling%2520used%2520in%2520various%2520quantization%2520algorithms%2520with%2520high%250Aperformance%252C%2520and%2520a%2520dequantization%2520kernel%2520with%2520a%2520software-defined%2520pipeline%2520to%250Acoordinate%2520CUDA%2520and%2520Tensor%2520Cores%2520execution%2520for%2520mixed-precision%2520operations.%250AEvaluated%2520on%2520RTX%25204090%252C%2520A100%252C%2520and%2520H100%252C%2520BitDecoding%2520accelerates%2520decoding%2520by%2520up%250Ato%25207.5x%252C%25204.8x%252C%2520and%25208.9x%252C%2520respectively%252C%2520over%2520FP16%2520FlashDecoding-v2%252C%2520and%250Asurpasses%2520the%2520state-of-the-art%2520low-bit%2520system%2520QServe%2520by%2520up%2520to%25204.3x.%2520On%250ALLaMA-3.1-8B%2520with%2520a%2520128K%2520context%252C%2520BitDecoding%2520reduces%2520single-batch%2520decoding%250Alatency%2520by%25203x%252C%2520showing%2520substantial%2520improvements%2520for%2520long-context%2520generation.%250AThe%2520code%2520is%2520available%2520at%2520https%253A//github.com/DD-DuDa/BitDecoding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18773v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BitDecoding%3A%20Unlocking%20Tensor%20Cores%20for%20Long-Context%20LLMs%20with%20Low-Bit%0A%20%20KV%20Cache&entry.906535625=Dayou%20Du%20and%20Shijie%20Cao%20and%20Jianyi%20Cheng%20and%20Luo%20Mai%20and%20Ting%20Cao%20and%20Mao%20Yang&entry.1292438233=%20%20The%20rise%20of%20long-context%20Large%20Language%20Models%20%28LLMs%29%20amplifies%20memory%20and%0Abandwidth%20demands%20during%20autoregressive%20decoding%2C%20as%20the%20Key-Value%20%28KV%29%20cache%0Agrows%20with%20each%20generated%20token.%20Low-bit%20KV-cache%20quantization%20%28e.g.%2C%204-bit%20or%0A2-bit%29%20can%20reduce%20memory%20footprint%20while%20preserving%20accuracy%2C%20but%20existing%0Asystems%20suffer%20from%20slow%20decoding%20due%20to%20their%20exclusive%20reliance%20on%20CUDA%0Acores%2C%20neglecting%20Tensor%20Cores%20%28the%20primary%20source%20of%20compute%20on%20modern%20GPUs%29.%0AWe%20present%20BitDecoding%2C%20a%20new%20long-context%20LLM%20inference%20system%20with%20a%20low-bit%0AKV%20cache.%20BitDecoding%20enables%20efficient%20low-bit%20KV-cache%20decoding%20by%0Acooperatively%20leveraging%20CUDA%20cores%20and%20Tensor%20Cores.%20It%20introduces%20methods%20for%0Aautomatically%20inducing%20optimized%20layouts%20to%20exploit%20Tensor%20Cores%2C%20along%20with%0Awarp-level%20parallelization%20strategies%20for%20dequantization.%20For%20unified%20system%0Asupport%2C%20BitDecoding%20includes%20a%20query%20transformation%20module%20supporting%20diverse%0Aattention%20variants%2C%20a%20quantization%20kernel%20that%20supports%20both%20tensor-wise%20and%0Achannel-wise%20scaling%20used%20in%20various%20quantization%20algorithms%20with%20high%0Aperformance%2C%20and%20a%20dequantization%20kernel%20with%20a%20software-defined%20pipeline%20to%0Acoordinate%20CUDA%20and%20Tensor%20Cores%20execution%20for%20mixed-precision%20operations.%0AEvaluated%20on%20RTX%204090%2C%20A100%2C%20and%20H100%2C%20BitDecoding%20accelerates%20decoding%20by%20up%0Ato%207.5x%2C%204.8x%2C%20and%208.9x%2C%20respectively%2C%20over%20FP16%20FlashDecoding-v2%2C%20and%0Asurpasses%20the%20state-of-the-art%20low-bit%20system%20QServe%20by%20up%20to%204.3x.%20On%0ALLaMA-3.1-8B%20with%20a%20128K%20context%2C%20BitDecoding%20reduces%20single-batch%20decoding%0Alatency%20by%203x%2C%20showing%20substantial%20improvements%20for%20long-context%20generation.%0AThe%20code%20is%20available%20at%20https%3A//github.com/DD-DuDa/BitDecoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18773v2&entry.124074799=Read"},
{"title": "Physics-Informed Joint Multi-TE Super-Resolution with Implicit Neural\n  Representation for Robust Fetal T2 Mapping", "author": "Busra Bulut and Maik Dannecker and Thomas Sanchez and Sara Neves Silva and Vladyslav Zalevskyi and Steven Jia and Jean-Baptiste Ledoux and Guillaume Auzias and Fran\u00e7ois Rousseau and Jana Hutter and Daniel Rueckert and Meritxell Bach Cuadra", "abstract": "  T2 mapping in fetal brain MRI has the potential to improve characterization\nof the developing brain, especially at mid-field (0.55T), where T2 decay is\nslower. However, this is challenging as fetal MRI acquisition relies on\nmultiple motion-corrupted stacks of thick slices, requiring slice-to-volume\nreconstruction (SVR) to estimate a high-resolution (HR) 3D volume. Currently,\nT2 mapping involves repeated acquisitions of these stacks at each echo time\n(TE), leading to long scan times and high sensitivity to motion. We tackle this\nchallenge with a method that jointly reconstructs data across TEs, addressing\nsevere motion. Our approach combines implicit neural representations with a\nphysics-informed regularization that models T2 decay, enabling information\nsharing across TEs while preserving anatomical and quantitative T2 fidelity. We\ndemonstrate state-of-the-art performance on simulated fetal brain and in vivo\nadult datasets with fetal-like motion. We also present the first in vivo fetal\nT2 mapping results at 0.55T. Our study shows potential for reducing the number\nof stacks per TE in T2 mapping by leveraging anatomical redundancy.\n", "link": "http://arxiv.org/abs/2508.10680v1", "date": "2025-08-14", "relevancy": 2.0886, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5283}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5186}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Informed%20Joint%20Multi-TE%20Super-Resolution%20with%20Implicit%20Neural%0A%20%20Representation%20for%20Robust%20Fetal%20T2%20Mapping&body=Title%3A%20Physics-Informed%20Joint%20Multi-TE%20Super-Resolution%20with%20Implicit%20Neural%0A%20%20Representation%20for%20Robust%20Fetal%20T2%20Mapping%0AAuthor%3A%20Busra%20Bulut%20and%20Maik%20Dannecker%20and%20Thomas%20Sanchez%20and%20Sara%20Neves%20Silva%20and%20Vladyslav%20Zalevskyi%20and%20Steven%20Jia%20and%20Jean-Baptiste%20Ledoux%20and%20Guillaume%20Auzias%20and%20Fran%C3%A7ois%20Rousseau%20and%20Jana%20Hutter%20and%20Daniel%20Rueckert%20and%20Meritxell%20Bach%20Cuadra%0AAbstract%3A%20%20%20T2%20mapping%20in%20fetal%20brain%20MRI%20has%20the%20potential%20to%20improve%20characterization%0Aof%20the%20developing%20brain%2C%20especially%20at%20mid-field%20%280.55T%29%2C%20where%20T2%20decay%20is%0Aslower.%20However%2C%20this%20is%20challenging%20as%20fetal%20MRI%20acquisition%20relies%20on%0Amultiple%20motion-corrupted%20stacks%20of%20thick%20slices%2C%20requiring%20slice-to-volume%0Areconstruction%20%28SVR%29%20to%20estimate%20a%20high-resolution%20%28HR%29%203D%20volume.%20Currently%2C%0AT2%20mapping%20involves%20repeated%20acquisitions%20of%20these%20stacks%20at%20each%20echo%20time%0A%28TE%29%2C%20leading%20to%20long%20scan%20times%20and%20high%20sensitivity%20to%20motion.%20We%20tackle%20this%0Achallenge%20with%20a%20method%20that%20jointly%20reconstructs%20data%20across%20TEs%2C%20addressing%0Asevere%20motion.%20Our%20approach%20combines%20implicit%20neural%20representations%20with%20a%0Aphysics-informed%20regularization%20that%20models%20T2%20decay%2C%20enabling%20information%0Asharing%20across%20TEs%20while%20preserving%20anatomical%20and%20quantitative%20T2%20fidelity.%20We%0Ademonstrate%20state-of-the-art%20performance%20on%20simulated%20fetal%20brain%20and%20in%20vivo%0Aadult%20datasets%20with%20fetal-like%20motion.%20We%20also%20present%20the%20first%20in%20vivo%20fetal%0AT2%20mapping%20results%20at%200.55T.%20Our%20study%20shows%20potential%20for%20reducing%20the%20number%0Aof%20stacks%20per%20TE%20in%20T2%20mapping%20by%20leveraging%20anatomical%20redundancy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Informed%2520Joint%2520Multi-TE%2520Super-Resolution%2520with%2520Implicit%2520Neural%250A%2520%2520Representation%2520for%2520Robust%2520Fetal%2520T2%2520Mapping%26entry.906535625%3DBusra%2520Bulut%2520and%2520Maik%2520Dannecker%2520and%2520Thomas%2520Sanchez%2520and%2520Sara%2520Neves%2520Silva%2520and%2520Vladyslav%2520Zalevskyi%2520and%2520Steven%2520Jia%2520and%2520Jean-Baptiste%2520Ledoux%2520and%2520Guillaume%2520Auzias%2520and%2520Fran%25C3%25A7ois%2520Rousseau%2520and%2520Jana%2520Hutter%2520and%2520Daniel%2520Rueckert%2520and%2520Meritxell%2520Bach%2520Cuadra%26entry.1292438233%3D%2520%2520T2%2520mapping%2520in%2520fetal%2520brain%2520MRI%2520has%2520the%2520potential%2520to%2520improve%2520characterization%250Aof%2520the%2520developing%2520brain%252C%2520especially%2520at%2520mid-field%2520%25280.55T%2529%252C%2520where%2520T2%2520decay%2520is%250Aslower.%2520However%252C%2520this%2520is%2520challenging%2520as%2520fetal%2520MRI%2520acquisition%2520relies%2520on%250Amultiple%2520motion-corrupted%2520stacks%2520of%2520thick%2520slices%252C%2520requiring%2520slice-to-volume%250Areconstruction%2520%2528SVR%2529%2520to%2520estimate%2520a%2520high-resolution%2520%2528HR%2529%25203D%2520volume.%2520Currently%252C%250AT2%2520mapping%2520involves%2520repeated%2520acquisitions%2520of%2520these%2520stacks%2520at%2520each%2520echo%2520time%250A%2528TE%2529%252C%2520leading%2520to%2520long%2520scan%2520times%2520and%2520high%2520sensitivity%2520to%2520motion.%2520We%2520tackle%2520this%250Achallenge%2520with%2520a%2520method%2520that%2520jointly%2520reconstructs%2520data%2520across%2520TEs%252C%2520addressing%250Asevere%2520motion.%2520Our%2520approach%2520combines%2520implicit%2520neural%2520representations%2520with%2520a%250Aphysics-informed%2520regularization%2520that%2520models%2520T2%2520decay%252C%2520enabling%2520information%250Asharing%2520across%2520TEs%2520while%2520preserving%2520anatomical%2520and%2520quantitative%2520T2%2520fidelity.%2520We%250Ademonstrate%2520state-of-the-art%2520performance%2520on%2520simulated%2520fetal%2520brain%2520and%2520in%2520vivo%250Aadult%2520datasets%2520with%2520fetal-like%2520motion.%2520We%2520also%2520present%2520the%2520first%2520in%2520vivo%2520fetal%250AT2%2520mapping%2520results%2520at%25200.55T.%2520Our%2520study%2520shows%2520potential%2520for%2520reducing%2520the%2520number%250Aof%2520stacks%2520per%2520TE%2520in%2520T2%2520mapping%2520by%2520leveraging%2520anatomical%2520redundancy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Informed%20Joint%20Multi-TE%20Super-Resolution%20with%20Implicit%20Neural%0A%20%20Representation%20for%20Robust%20Fetal%20T2%20Mapping&entry.906535625=Busra%20Bulut%20and%20Maik%20Dannecker%20and%20Thomas%20Sanchez%20and%20Sara%20Neves%20Silva%20and%20Vladyslav%20Zalevskyi%20and%20Steven%20Jia%20and%20Jean-Baptiste%20Ledoux%20and%20Guillaume%20Auzias%20and%20Fran%C3%A7ois%20Rousseau%20and%20Jana%20Hutter%20and%20Daniel%20Rueckert%20and%20Meritxell%20Bach%20Cuadra&entry.1292438233=%20%20T2%20mapping%20in%20fetal%20brain%20MRI%20has%20the%20potential%20to%20improve%20characterization%0Aof%20the%20developing%20brain%2C%20especially%20at%20mid-field%20%280.55T%29%2C%20where%20T2%20decay%20is%0Aslower.%20However%2C%20this%20is%20challenging%20as%20fetal%20MRI%20acquisition%20relies%20on%0Amultiple%20motion-corrupted%20stacks%20of%20thick%20slices%2C%20requiring%20slice-to-volume%0Areconstruction%20%28SVR%29%20to%20estimate%20a%20high-resolution%20%28HR%29%203D%20volume.%20Currently%2C%0AT2%20mapping%20involves%20repeated%20acquisitions%20of%20these%20stacks%20at%20each%20echo%20time%0A%28TE%29%2C%20leading%20to%20long%20scan%20times%20and%20high%20sensitivity%20to%20motion.%20We%20tackle%20this%0Achallenge%20with%20a%20method%20that%20jointly%20reconstructs%20data%20across%20TEs%2C%20addressing%0Asevere%20motion.%20Our%20approach%20combines%20implicit%20neural%20representations%20with%20a%0Aphysics-informed%20regularization%20that%20models%20T2%20decay%2C%20enabling%20information%0Asharing%20across%20TEs%20while%20preserving%20anatomical%20and%20quantitative%20T2%20fidelity.%20We%0Ademonstrate%20state-of-the-art%20performance%20on%20simulated%20fetal%20brain%20and%20in%20vivo%0Aadult%20datasets%20with%20fetal-like%20motion.%20We%20also%20present%20the%20first%20in%20vivo%20fetal%0AT2%20mapping%20results%20at%200.55T.%20Our%20study%20shows%20potential%20for%20reducing%20the%20number%0Aof%20stacks%20per%20TE%20in%20T2%20mapping%20by%20leveraging%20anatomical%20redundancy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10680v1&entry.124074799=Read"},
{"title": "Insights from the Algonauts 2025 Winners", "author": "Paul S. Scotti and Mihir Tripathy", "abstract": "  The Algonauts 2025 Challenge just wrapped up a few weeks ago. It is a\nbiennial challenge in computational neuroscience in which teams attempt to\nbuild models that predict human brain activity from carefully curated stimuli.\nPrevious editions (2019, 2021, 2023) focused on still images and short videos;\nthe 2025 edition, which concluded last month (late July), pushed the field\nfurther by using long, multimodal movies. Teams were tasked with predicting\nfMRI responses across 1,000 whole-brain parcels across four participants in the\ndataset who were scanned while watching nearly 80 hours of naturalistic movie\nstimuli. These recordings came from the CNeuroMod project and included 65 hours\nof training data, about 55 hours of Friends (seasons 1-6) plus four feature\nfilms (The Bourne Supremacy, Hidden Figures, Life, and The Wolf of Wall\nStreet). The remaining data were used for validation: Season 7 of Friends for\nin-distribution tests, and the final winners for the Challenge were those who\ncould best predict brain activity for six films in their held-out\nout-of-distribution (OOD) set. The winners were just announced and the top team\nreports are now publicly available. As members of the MedARC team which placed\n4th in the competition, we reflect on the approaches that worked, what they\nreveal about the current state of brain encoding, and what might come next.\n", "link": "http://arxiv.org/abs/2508.10784v1", "date": "2025-08-14", "relevancy": 2.0878, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Insights%20from%20the%20Algonauts%202025%20Winners&body=Title%3A%20Insights%20from%20the%20Algonauts%202025%20Winners%0AAuthor%3A%20Paul%20S.%20Scotti%20and%20Mihir%20Tripathy%0AAbstract%3A%20%20%20The%20Algonauts%202025%20Challenge%20just%20wrapped%20up%20a%20few%20weeks%20ago.%20It%20is%20a%0Abiennial%20challenge%20in%20computational%20neuroscience%20in%20which%20teams%20attempt%20to%0Abuild%20models%20that%20predict%20human%20brain%20activity%20from%20carefully%20curated%20stimuli.%0APrevious%20editions%20%282019%2C%202021%2C%202023%29%20focused%20on%20still%20images%20and%20short%20videos%3B%0Athe%202025%20edition%2C%20which%20concluded%20last%20month%20%28late%20July%29%2C%20pushed%20the%20field%0Afurther%20by%20using%20long%2C%20multimodal%20movies.%20Teams%20were%20tasked%20with%20predicting%0AfMRI%20responses%20across%201%2C000%20whole-brain%20parcels%20across%20four%20participants%20in%20the%0Adataset%20who%20were%20scanned%20while%20watching%20nearly%2080%20hours%20of%20naturalistic%20movie%0Astimuli.%20These%20recordings%20came%20from%20the%20CNeuroMod%20project%20and%20included%2065%20hours%0Aof%20training%20data%2C%20about%2055%20hours%20of%20Friends%20%28seasons%201-6%29%20plus%20four%20feature%0Afilms%20%28The%20Bourne%20Supremacy%2C%20Hidden%20Figures%2C%20Life%2C%20and%20The%20Wolf%20of%20Wall%0AStreet%29.%20The%20remaining%20data%20were%20used%20for%20validation%3A%20Season%207%20of%20Friends%20for%0Ain-distribution%20tests%2C%20and%20the%20final%20winners%20for%20the%20Challenge%20were%20those%20who%0Acould%20best%20predict%20brain%20activity%20for%20six%20films%20in%20their%20held-out%0Aout-of-distribution%20%28OOD%29%20set.%20The%20winners%20were%20just%20announced%20and%20the%20top%20team%0Areports%20are%20now%20publicly%20available.%20As%20members%20of%20the%20MedARC%20team%20which%20placed%0A4th%20in%20the%20competition%2C%20we%20reflect%20on%20the%20approaches%20that%20worked%2C%20what%20they%0Areveal%20about%20the%20current%20state%20of%20brain%20encoding%2C%20and%20what%20might%20come%20next.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsights%2520from%2520the%2520Algonauts%25202025%2520Winners%26entry.906535625%3DPaul%2520S.%2520Scotti%2520and%2520Mihir%2520Tripathy%26entry.1292438233%3D%2520%2520The%2520Algonauts%25202025%2520Challenge%2520just%2520wrapped%2520up%2520a%2520few%2520weeks%2520ago.%2520It%2520is%2520a%250Abiennial%2520challenge%2520in%2520computational%2520neuroscience%2520in%2520which%2520teams%2520attempt%2520to%250Abuild%2520models%2520that%2520predict%2520human%2520brain%2520activity%2520from%2520carefully%2520curated%2520stimuli.%250APrevious%2520editions%2520%25282019%252C%25202021%252C%25202023%2529%2520focused%2520on%2520still%2520images%2520and%2520short%2520videos%253B%250Athe%25202025%2520edition%252C%2520which%2520concluded%2520last%2520month%2520%2528late%2520July%2529%252C%2520pushed%2520the%2520field%250Afurther%2520by%2520using%2520long%252C%2520multimodal%2520movies.%2520Teams%2520were%2520tasked%2520with%2520predicting%250AfMRI%2520responses%2520across%25201%252C000%2520whole-brain%2520parcels%2520across%2520four%2520participants%2520in%2520the%250Adataset%2520who%2520were%2520scanned%2520while%2520watching%2520nearly%252080%2520hours%2520of%2520naturalistic%2520movie%250Astimuli.%2520These%2520recordings%2520came%2520from%2520the%2520CNeuroMod%2520project%2520and%2520included%252065%2520hours%250Aof%2520training%2520data%252C%2520about%252055%2520hours%2520of%2520Friends%2520%2528seasons%25201-6%2529%2520plus%2520four%2520feature%250Afilms%2520%2528The%2520Bourne%2520Supremacy%252C%2520Hidden%2520Figures%252C%2520Life%252C%2520and%2520The%2520Wolf%2520of%2520Wall%250AStreet%2529.%2520The%2520remaining%2520data%2520were%2520used%2520for%2520validation%253A%2520Season%25207%2520of%2520Friends%2520for%250Ain-distribution%2520tests%252C%2520and%2520the%2520final%2520winners%2520for%2520the%2520Challenge%2520were%2520those%2520who%250Acould%2520best%2520predict%2520brain%2520activity%2520for%2520six%2520films%2520in%2520their%2520held-out%250Aout-of-distribution%2520%2528OOD%2529%2520set.%2520The%2520winners%2520were%2520just%2520announced%2520and%2520the%2520top%2520team%250Areports%2520are%2520now%2520publicly%2520available.%2520As%2520members%2520of%2520the%2520MedARC%2520team%2520which%2520placed%250A4th%2520in%2520the%2520competition%252C%2520we%2520reflect%2520on%2520the%2520approaches%2520that%2520worked%252C%2520what%2520they%250Areveal%2520about%2520the%2520current%2520state%2520of%2520brain%2520encoding%252C%2520and%2520what%2520might%2520come%2520next.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Insights%20from%20the%20Algonauts%202025%20Winners&entry.906535625=Paul%20S.%20Scotti%20and%20Mihir%20Tripathy&entry.1292438233=%20%20The%20Algonauts%202025%20Challenge%20just%20wrapped%20up%20a%20few%20weeks%20ago.%20It%20is%20a%0Abiennial%20challenge%20in%20computational%20neuroscience%20in%20which%20teams%20attempt%20to%0Abuild%20models%20that%20predict%20human%20brain%20activity%20from%20carefully%20curated%20stimuli.%0APrevious%20editions%20%282019%2C%202021%2C%202023%29%20focused%20on%20still%20images%20and%20short%20videos%3B%0Athe%202025%20edition%2C%20which%20concluded%20last%20month%20%28late%20July%29%2C%20pushed%20the%20field%0Afurther%20by%20using%20long%2C%20multimodal%20movies.%20Teams%20were%20tasked%20with%20predicting%0AfMRI%20responses%20across%201%2C000%20whole-brain%20parcels%20across%20four%20participants%20in%20the%0Adataset%20who%20were%20scanned%20while%20watching%20nearly%2080%20hours%20of%20naturalistic%20movie%0Astimuli.%20These%20recordings%20came%20from%20the%20CNeuroMod%20project%20and%20included%2065%20hours%0Aof%20training%20data%2C%20about%2055%20hours%20of%20Friends%20%28seasons%201-6%29%20plus%20four%20feature%0Afilms%20%28The%20Bourne%20Supremacy%2C%20Hidden%20Figures%2C%20Life%2C%20and%20The%20Wolf%20of%20Wall%0AStreet%29.%20The%20remaining%20data%20were%20used%20for%20validation%3A%20Season%207%20of%20Friends%20for%0Ain-distribution%20tests%2C%20and%20the%20final%20winners%20for%20the%20Challenge%20were%20those%20who%0Acould%20best%20predict%20brain%20activity%20for%20six%20films%20in%20their%20held-out%0Aout-of-distribution%20%28OOD%29%20set.%20The%20winners%20were%20just%20announced%20and%20the%20top%20team%0Areports%20are%20now%20publicly%20available.%20As%20members%20of%20the%20MedARC%20team%20which%20placed%0A4th%20in%20the%20competition%2C%20we%20reflect%20on%20the%20approaches%20that%20worked%2C%20what%20they%0Areveal%20about%20the%20current%20state%20of%20brain%20encoding%2C%20and%20what%20might%20come%20next.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10784v1&entry.124074799=Read"},
{"title": "MAP Estimation with Denoisers: Convergence Rates and Guarantees", "author": "Scott Pesme and Giacomo Meanti and Michael Arbel and Julien Mairal", "abstract": "  Denoiser models have become powerful tools for inverse problems, enabling the\nuse of pretrained networks to approximate the score of a smoothed prior\ndistribution. These models are often used in heuristic iterative schemes aimed\nat solving Maximum a Posteriori (MAP) optimisation problems, where the proximal\noperator of the negative log-prior plays a central role. In practice, this\noperator is intractable, and practitioners plug in a pretrained denoiser as a\nsurrogate-despite the lack of general theoretical justification for this\nsubstitution. In this work, we show that a simple algorithm, closely related to\nseveral used in practice, provably converges to the proximal operator under a\nlog-concavity assumption on the prior $p$. We show that this algorithm can be\ninterpreted as a gradient descent on smoothed proximal objectives. Our analysis\nthus provides a theoretical foundation for a class of empirically successful\nbut previously heuristic methods.\n", "link": "http://arxiv.org/abs/2507.15397v2", "date": "2025-08-14", "relevancy": 2.0839, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5383}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5113}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAP%20Estimation%20with%20Denoisers%3A%20Convergence%20Rates%20and%20Guarantees&body=Title%3A%20MAP%20Estimation%20with%20Denoisers%3A%20Convergence%20Rates%20and%20Guarantees%0AAuthor%3A%20Scott%20Pesme%20and%20Giacomo%20Meanti%20and%20Michael%20Arbel%20and%20Julien%20Mairal%0AAbstract%3A%20%20%20Denoiser%20models%20have%20become%20powerful%20tools%20for%20inverse%20problems%2C%20enabling%20the%0Ause%20of%20pretrained%20networks%20to%20approximate%20the%20score%20of%20a%20smoothed%20prior%0Adistribution.%20These%20models%20are%20often%20used%20in%20heuristic%20iterative%20schemes%20aimed%0Aat%20solving%20Maximum%20a%20Posteriori%20%28MAP%29%20optimisation%20problems%2C%20where%20the%20proximal%0Aoperator%20of%20the%20negative%20log-prior%20plays%20a%20central%20role.%20In%20practice%2C%20this%0Aoperator%20is%20intractable%2C%20and%20practitioners%20plug%20in%20a%20pretrained%20denoiser%20as%20a%0Asurrogate-despite%20the%20lack%20of%20general%20theoretical%20justification%20for%20this%0Asubstitution.%20In%20this%20work%2C%20we%20show%20that%20a%20simple%20algorithm%2C%20closely%20related%20to%0Aseveral%20used%20in%20practice%2C%20provably%20converges%20to%20the%20proximal%20operator%20under%20a%0Alog-concavity%20assumption%20on%20the%20prior%20%24p%24.%20We%20show%20that%20this%20algorithm%20can%20be%0Ainterpreted%20as%20a%20gradient%20descent%20on%20smoothed%20proximal%20objectives.%20Our%20analysis%0Athus%20provides%20a%20theoretical%20foundation%20for%20a%20class%20of%20empirically%20successful%0Abut%20previously%20heuristic%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15397v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAP%2520Estimation%2520with%2520Denoisers%253A%2520Convergence%2520Rates%2520and%2520Guarantees%26entry.906535625%3DScott%2520Pesme%2520and%2520Giacomo%2520Meanti%2520and%2520Michael%2520Arbel%2520and%2520Julien%2520Mairal%26entry.1292438233%3D%2520%2520Denoiser%2520models%2520have%2520become%2520powerful%2520tools%2520for%2520inverse%2520problems%252C%2520enabling%2520the%250Ause%2520of%2520pretrained%2520networks%2520to%2520approximate%2520the%2520score%2520of%2520a%2520smoothed%2520prior%250Adistribution.%2520These%2520models%2520are%2520often%2520used%2520in%2520heuristic%2520iterative%2520schemes%2520aimed%250Aat%2520solving%2520Maximum%2520a%2520Posteriori%2520%2528MAP%2529%2520optimisation%2520problems%252C%2520where%2520the%2520proximal%250Aoperator%2520of%2520the%2520negative%2520log-prior%2520plays%2520a%2520central%2520role.%2520In%2520practice%252C%2520this%250Aoperator%2520is%2520intractable%252C%2520and%2520practitioners%2520plug%2520in%2520a%2520pretrained%2520denoiser%2520as%2520a%250Asurrogate-despite%2520the%2520lack%2520of%2520general%2520theoretical%2520justification%2520for%2520this%250Asubstitution.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520a%2520simple%2520algorithm%252C%2520closely%2520related%2520to%250Aseveral%2520used%2520in%2520practice%252C%2520provably%2520converges%2520to%2520the%2520proximal%2520operator%2520under%2520a%250Alog-concavity%2520assumption%2520on%2520the%2520prior%2520%2524p%2524.%2520We%2520show%2520that%2520this%2520algorithm%2520can%2520be%250Ainterpreted%2520as%2520a%2520gradient%2520descent%2520on%2520smoothed%2520proximal%2520objectives.%2520Our%2520analysis%250Athus%2520provides%2520a%2520theoretical%2520foundation%2520for%2520a%2520class%2520of%2520empirically%2520successful%250Abut%2520previously%2520heuristic%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15397v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAP%20Estimation%20with%20Denoisers%3A%20Convergence%20Rates%20and%20Guarantees&entry.906535625=Scott%20Pesme%20and%20Giacomo%20Meanti%20and%20Michael%20Arbel%20and%20Julien%20Mairal&entry.1292438233=%20%20Denoiser%20models%20have%20become%20powerful%20tools%20for%20inverse%20problems%2C%20enabling%20the%0Ause%20of%20pretrained%20networks%20to%20approximate%20the%20score%20of%20a%20smoothed%20prior%0Adistribution.%20These%20models%20are%20often%20used%20in%20heuristic%20iterative%20schemes%20aimed%0Aat%20solving%20Maximum%20a%20Posteriori%20%28MAP%29%20optimisation%20problems%2C%20where%20the%20proximal%0Aoperator%20of%20the%20negative%20log-prior%20plays%20a%20central%20role.%20In%20practice%2C%20this%0Aoperator%20is%20intractable%2C%20and%20practitioners%20plug%20in%20a%20pretrained%20denoiser%20as%20a%0Asurrogate-despite%20the%20lack%20of%20general%20theoretical%20justification%20for%20this%0Asubstitution.%20In%20this%20work%2C%20we%20show%20that%20a%20simple%20algorithm%2C%20closely%20related%20to%0Aseveral%20used%20in%20practice%2C%20provably%20converges%20to%20the%20proximal%20operator%20under%20a%0Alog-concavity%20assumption%20on%20the%20prior%20%24p%24.%20We%20show%20that%20this%20algorithm%20can%20be%0Ainterpreted%20as%20a%20gradient%20descent%20on%20smoothed%20proximal%20objectives.%20Our%20analysis%0Athus%20provides%20a%20theoretical%20foundation%20for%20a%20class%20of%20empirically%20successful%0Abut%20previously%20heuristic%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15397v2&entry.124074799=Read"},
{"title": "VasoMIM: Vascular Anatomy-Aware Masked Image Modeling for Vessel\n  Segmentation", "author": "De-Xing Huang and Xiao-Hu Zhou and Mei-Jiang Gui and Xiao-Liang Xie and Shi-Qi Liu and Shuang-Yi Wang and Tian-Yu Xiang and Rui-Ze Ma and Nu-Fang Xiao and Zeng-Guang Hou", "abstract": "  Accurate vessel segmentation in X-ray angiograms is crucial for numerous\nclinical applications. However, the scarcity of annotated data presents a\nsignificant challenge, which has driven the adoption of self-supervised\nlearning (SSL) methods such as masked image modeling (MIM) to leverage\nlarge-scale unlabeled data for learning transferable representations.\nUnfortunately, conventional MIM often fails to capture vascular anatomy because\nof the severe class imbalance between vessel and background pixels, leading to\nweak vascular representations. To address this, we introduce Vascular\nanatomy-aware Masked Image Modeling (VasoMIM), a novel MIM framework tailored\nfor X-ray angiograms that explicitly integrates anatomical knowledge into the\npre-training process. Specifically, it comprises two complementary components:\nanatomy-guided masking strategy and anatomical consistency loss. The former\npreferentially masks vessel-containing patches to focus the model on\nreconstructing vessel-relevant regions. The latter enforces consistency in\nvascular semantics between the original and reconstructed images, thereby\nimproving the discriminability of vascular representations. Empirically,\nVasoMIM achieves state-of-the-art performance across three datasets. These\nfindings highlight its potential to facilitate X-ray angiogram analysis.\n", "link": "http://arxiv.org/abs/2508.10794v1", "date": "2025-08-14", "relevancy": 2.0698, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5586}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4968}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VasoMIM%3A%20Vascular%20Anatomy-Aware%20Masked%20Image%20Modeling%20for%20Vessel%0A%20%20Segmentation&body=Title%3A%20VasoMIM%3A%20Vascular%20Anatomy-Aware%20Masked%20Image%20Modeling%20for%20Vessel%0A%20%20Segmentation%0AAuthor%3A%20De-Xing%20Huang%20and%20Xiao-Hu%20Zhou%20and%20Mei-Jiang%20Gui%20and%20Xiao-Liang%20Xie%20and%20Shi-Qi%20Liu%20and%20Shuang-Yi%20Wang%20and%20Tian-Yu%20Xiang%20and%20Rui-Ze%20Ma%20and%20Nu-Fang%20Xiao%20and%20Zeng-Guang%20Hou%0AAbstract%3A%20%20%20Accurate%20vessel%20segmentation%20in%20X-ray%20angiograms%20is%20crucial%20for%20numerous%0Aclinical%20applications.%20However%2C%20the%20scarcity%20of%20annotated%20data%20presents%20a%0Asignificant%20challenge%2C%20which%20has%20driven%20the%20adoption%20of%20self-supervised%0Alearning%20%28SSL%29%20methods%20such%20as%20masked%20image%20modeling%20%28MIM%29%20to%20leverage%0Alarge-scale%20unlabeled%20data%20for%20learning%20transferable%20representations.%0AUnfortunately%2C%20conventional%20MIM%20often%20fails%20to%20capture%20vascular%20anatomy%20because%0Aof%20the%20severe%20class%20imbalance%20between%20vessel%20and%20background%20pixels%2C%20leading%20to%0Aweak%20vascular%20representations.%20To%20address%20this%2C%20we%20introduce%20Vascular%0Aanatomy-aware%20Masked%20Image%20Modeling%20%28VasoMIM%29%2C%20a%20novel%20MIM%20framework%20tailored%0Afor%20X-ray%20angiograms%20that%20explicitly%20integrates%20anatomical%20knowledge%20into%20the%0Apre-training%20process.%20Specifically%2C%20it%20comprises%20two%20complementary%20components%3A%0Aanatomy-guided%20masking%20strategy%20and%20anatomical%20consistency%20loss.%20The%20former%0Apreferentially%20masks%20vessel-containing%20patches%20to%20focus%20the%20model%20on%0Areconstructing%20vessel-relevant%20regions.%20The%20latter%20enforces%20consistency%20in%0Avascular%20semantics%20between%20the%20original%20and%20reconstructed%20images%2C%20thereby%0Aimproving%20the%20discriminability%20of%20vascular%20representations.%20Empirically%2C%0AVasoMIM%20achieves%20state-of-the-art%20performance%20across%20three%20datasets.%20These%0Afindings%20highlight%20its%20potential%20to%20facilitate%20X-ray%20angiogram%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVasoMIM%253A%2520Vascular%2520Anatomy-Aware%2520Masked%2520Image%2520Modeling%2520for%2520Vessel%250A%2520%2520Segmentation%26entry.906535625%3DDe-Xing%2520Huang%2520and%2520Xiao-Hu%2520Zhou%2520and%2520Mei-Jiang%2520Gui%2520and%2520Xiao-Liang%2520Xie%2520and%2520Shi-Qi%2520Liu%2520and%2520Shuang-Yi%2520Wang%2520and%2520Tian-Yu%2520Xiang%2520and%2520Rui-Ze%2520Ma%2520and%2520Nu-Fang%2520Xiao%2520and%2520Zeng-Guang%2520Hou%26entry.1292438233%3D%2520%2520Accurate%2520vessel%2520segmentation%2520in%2520X-ray%2520angiograms%2520is%2520crucial%2520for%2520numerous%250Aclinical%2520applications.%2520However%252C%2520the%2520scarcity%2520of%2520annotated%2520data%2520presents%2520a%250Asignificant%2520challenge%252C%2520which%2520has%2520driven%2520the%2520adoption%2520of%2520self-supervised%250Alearning%2520%2528SSL%2529%2520methods%2520such%2520as%2520masked%2520image%2520modeling%2520%2528MIM%2529%2520to%2520leverage%250Alarge-scale%2520unlabeled%2520data%2520for%2520learning%2520transferable%2520representations.%250AUnfortunately%252C%2520conventional%2520MIM%2520often%2520fails%2520to%2520capture%2520vascular%2520anatomy%2520because%250Aof%2520the%2520severe%2520class%2520imbalance%2520between%2520vessel%2520and%2520background%2520pixels%252C%2520leading%2520to%250Aweak%2520vascular%2520representations.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Vascular%250Aanatomy-aware%2520Masked%2520Image%2520Modeling%2520%2528VasoMIM%2529%252C%2520a%2520novel%2520MIM%2520framework%2520tailored%250Afor%2520X-ray%2520angiograms%2520that%2520explicitly%2520integrates%2520anatomical%2520knowledge%2520into%2520the%250Apre-training%2520process.%2520Specifically%252C%2520it%2520comprises%2520two%2520complementary%2520components%253A%250Aanatomy-guided%2520masking%2520strategy%2520and%2520anatomical%2520consistency%2520loss.%2520The%2520former%250Apreferentially%2520masks%2520vessel-containing%2520patches%2520to%2520focus%2520the%2520model%2520on%250Areconstructing%2520vessel-relevant%2520regions.%2520The%2520latter%2520enforces%2520consistency%2520in%250Avascular%2520semantics%2520between%2520the%2520original%2520and%2520reconstructed%2520images%252C%2520thereby%250Aimproving%2520the%2520discriminability%2520of%2520vascular%2520representations.%2520Empirically%252C%250AVasoMIM%2520achieves%2520state-of-the-art%2520performance%2520across%2520three%2520datasets.%2520These%250Afindings%2520highlight%2520its%2520potential%2520to%2520facilitate%2520X-ray%2520angiogram%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VasoMIM%3A%20Vascular%20Anatomy-Aware%20Masked%20Image%20Modeling%20for%20Vessel%0A%20%20Segmentation&entry.906535625=De-Xing%20Huang%20and%20Xiao-Hu%20Zhou%20and%20Mei-Jiang%20Gui%20and%20Xiao-Liang%20Xie%20and%20Shi-Qi%20Liu%20and%20Shuang-Yi%20Wang%20and%20Tian-Yu%20Xiang%20and%20Rui-Ze%20Ma%20and%20Nu-Fang%20Xiao%20and%20Zeng-Guang%20Hou&entry.1292438233=%20%20Accurate%20vessel%20segmentation%20in%20X-ray%20angiograms%20is%20crucial%20for%20numerous%0Aclinical%20applications.%20However%2C%20the%20scarcity%20of%20annotated%20data%20presents%20a%0Asignificant%20challenge%2C%20which%20has%20driven%20the%20adoption%20of%20self-supervised%0Alearning%20%28SSL%29%20methods%20such%20as%20masked%20image%20modeling%20%28MIM%29%20to%20leverage%0Alarge-scale%20unlabeled%20data%20for%20learning%20transferable%20representations.%0AUnfortunately%2C%20conventional%20MIM%20often%20fails%20to%20capture%20vascular%20anatomy%20because%0Aof%20the%20severe%20class%20imbalance%20between%20vessel%20and%20background%20pixels%2C%20leading%20to%0Aweak%20vascular%20representations.%20To%20address%20this%2C%20we%20introduce%20Vascular%0Aanatomy-aware%20Masked%20Image%20Modeling%20%28VasoMIM%29%2C%20a%20novel%20MIM%20framework%20tailored%0Afor%20X-ray%20angiograms%20that%20explicitly%20integrates%20anatomical%20knowledge%20into%20the%0Apre-training%20process.%20Specifically%2C%20it%20comprises%20two%20complementary%20components%3A%0Aanatomy-guided%20masking%20strategy%20and%20anatomical%20consistency%20loss.%20The%20former%0Apreferentially%20masks%20vessel-containing%20patches%20to%20focus%20the%20model%20on%0Areconstructing%20vessel-relevant%20regions.%20The%20latter%20enforces%20consistency%20in%0Avascular%20semantics%20between%20the%20original%20and%20reconstructed%20images%2C%20thereby%0Aimproving%20the%20discriminability%20of%20vascular%20representations.%20Empirically%2C%0AVasoMIM%20achieves%20state-of-the-art%20performance%20across%20three%20datasets.%20These%0Afindings%20highlight%20its%20potential%20to%20facilitate%20X-ray%20angiogram%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10794v1&entry.124074799=Read"},
{"title": "FRUGAL: Memory-Efficient Optimization by Reducing State Overhead for\n  Scalable Training", "author": "Philip Zmushko and Aleksandr Beznosikov and Martin Tak\u00e1\u010d and Samuel Horv\u00e1th", "abstract": "  With the increase in the number of parameters in large language models, the\nprocess of pre-training and fine-tuning increasingly demands larger volumes of\nGPU memory. A significant portion of this memory is typically consumed by the\noptimizer state. To overcome this challenge, recent approaches such as low-rank\nadaptation (LoRA (Hu et al., 2021)), low-rank gradient projection (GaLore (Zhao\net al., 2024)), and blockwise optimization (BAdam (Luo et al., 2024)) have been\nproposed. However, in all these algorithms, the $\\textit{effective rank of the\nweight updates remains low-rank}$, which can lead to a substantial loss of\ninformation from the gradient. This loss can be critically important,\nespecially during the pre-training stage. In this paper, we introduce\n$\\texttt{FRUGAL}$ ($\\textbf{F}$ull-$\\textbf{R}$ank $\\textbf{U}$pdates with\n$\\textbf{G}$r$\\textbf{A}$dient sp$\\textbf{L}$itting), a new memory-efficient\noptimization framework. $\\texttt{FRUGAL}$ leverages gradient splitting to\nperform low-dimensional updates using advanced algorithms (such as Adam), while\nupdates along the remaining directions are executed via state-free methods like\nSGD or signSGD (Bernstein et al., 2018). Our framework can be integrated with\nvarious low-rank update selection techniques, including GaLore and BAdam. We\nprovide theoretical convergence guarantees for our framework when using SGDM\nfor low-dimensional updates and SGD for state-free updates. Additionally, our\nmethod consistently outperforms concurrent approaches across various fixed\nmemory budgets, achieving state-of-the-art results in pre-training and\nfine-tuning tasks while balancing memory efficiency and performance metrics.\n", "link": "http://arxiv.org/abs/2411.07837v3", "date": "2025-08-14", "relevancy": 2.0691, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.527}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5197}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRUGAL%3A%20Memory-Efficient%20Optimization%20by%20Reducing%20State%20Overhead%20for%0A%20%20Scalable%20Training&body=Title%3A%20FRUGAL%3A%20Memory-Efficient%20Optimization%20by%20Reducing%20State%20Overhead%20for%0A%20%20Scalable%20Training%0AAuthor%3A%20Philip%20Zmushko%20and%20Aleksandr%20Beznosikov%20and%20Martin%20Tak%C3%A1%C4%8D%20and%20Samuel%20Horv%C3%A1th%0AAbstract%3A%20%20%20With%20the%20increase%20in%20the%20number%20of%20parameters%20in%20large%20language%20models%2C%20the%0Aprocess%20of%20pre-training%20and%20fine-tuning%20increasingly%20demands%20larger%20volumes%20of%0AGPU%20memory.%20A%20significant%20portion%20of%20this%20memory%20is%20typically%20consumed%20by%20the%0Aoptimizer%20state.%20To%20overcome%20this%20challenge%2C%20recent%20approaches%20such%20as%20low-rank%0Aadaptation%20%28LoRA%20%28Hu%20et%20al.%2C%202021%29%29%2C%20low-rank%20gradient%20projection%20%28GaLore%20%28Zhao%0Aet%20al.%2C%202024%29%29%2C%20and%20blockwise%20optimization%20%28BAdam%20%28Luo%20et%20al.%2C%202024%29%29%20have%20been%0Aproposed.%20However%2C%20in%20all%20these%20algorithms%2C%20the%20%24%5Ctextit%7Beffective%20rank%20of%20the%0Aweight%20updates%20remains%20low-rank%7D%24%2C%20which%20can%20lead%20to%20a%20substantial%20loss%20of%0Ainformation%20from%20the%20gradient.%20This%20loss%20can%20be%20critically%20important%2C%0Aespecially%20during%20the%20pre-training%20stage.%20In%20this%20paper%2C%20we%20introduce%0A%24%5Ctexttt%7BFRUGAL%7D%24%20%28%24%5Ctextbf%7BF%7D%24ull-%24%5Ctextbf%7BR%7D%24ank%20%24%5Ctextbf%7BU%7D%24pdates%20with%0A%24%5Ctextbf%7BG%7D%24r%24%5Ctextbf%7BA%7D%24dient%20sp%24%5Ctextbf%7BL%7D%24itting%29%2C%20a%20new%20memory-efficient%0Aoptimization%20framework.%20%24%5Ctexttt%7BFRUGAL%7D%24%20leverages%20gradient%20splitting%20to%0Aperform%20low-dimensional%20updates%20using%20advanced%20algorithms%20%28such%20as%20Adam%29%2C%20while%0Aupdates%20along%20the%20remaining%20directions%20are%20executed%20via%20state-free%20methods%20like%0ASGD%20or%20signSGD%20%28Bernstein%20et%20al.%2C%202018%29.%20Our%20framework%20can%20be%20integrated%20with%0Avarious%20low-rank%20update%20selection%20techniques%2C%20including%20GaLore%20and%20BAdam.%20We%0Aprovide%20theoretical%20convergence%20guarantees%20for%20our%20framework%20when%20using%20SGDM%0Afor%20low-dimensional%20updates%20and%20SGD%20for%20state-free%20updates.%20Additionally%2C%20our%0Amethod%20consistently%20outperforms%20concurrent%20approaches%20across%20various%20fixed%0Amemory%20budgets%2C%20achieving%20state-of-the-art%20results%20in%20pre-training%20and%0Afine-tuning%20tasks%20while%20balancing%20memory%20efficiency%20and%20performance%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07837v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRUGAL%253A%2520Memory-Efficient%2520Optimization%2520by%2520Reducing%2520State%2520Overhead%2520for%250A%2520%2520Scalable%2520Training%26entry.906535625%3DPhilip%2520Zmushko%2520and%2520Aleksandr%2520Beznosikov%2520and%2520Martin%2520Tak%25C3%25A1%25C4%258D%2520and%2520Samuel%2520Horv%25C3%25A1th%26entry.1292438233%3D%2520%2520With%2520the%2520increase%2520in%2520the%2520number%2520of%2520parameters%2520in%2520large%2520language%2520models%252C%2520the%250Aprocess%2520of%2520pre-training%2520and%2520fine-tuning%2520increasingly%2520demands%2520larger%2520volumes%2520of%250AGPU%2520memory.%2520A%2520significant%2520portion%2520of%2520this%2520memory%2520is%2520typically%2520consumed%2520by%2520the%250Aoptimizer%2520state.%2520To%2520overcome%2520this%2520challenge%252C%2520recent%2520approaches%2520such%2520as%2520low-rank%250Aadaptation%2520%2528LoRA%2520%2528Hu%2520et%2520al.%252C%25202021%2529%2529%252C%2520low-rank%2520gradient%2520projection%2520%2528GaLore%2520%2528Zhao%250Aet%2520al.%252C%25202024%2529%2529%252C%2520and%2520blockwise%2520optimization%2520%2528BAdam%2520%2528Luo%2520et%2520al.%252C%25202024%2529%2529%2520have%2520been%250Aproposed.%2520However%252C%2520in%2520all%2520these%2520algorithms%252C%2520the%2520%2524%255Ctextit%257Beffective%2520rank%2520of%2520the%250Aweight%2520updates%2520remains%2520low-rank%257D%2524%252C%2520which%2520can%2520lead%2520to%2520a%2520substantial%2520loss%2520of%250Ainformation%2520from%2520the%2520gradient.%2520This%2520loss%2520can%2520be%2520critically%2520important%252C%250Aespecially%2520during%2520the%2520pre-training%2520stage.%2520In%2520this%2520paper%252C%2520we%2520introduce%250A%2524%255Ctexttt%257BFRUGAL%257D%2524%2520%2528%2524%255Ctextbf%257BF%257D%2524ull-%2524%255Ctextbf%257BR%257D%2524ank%2520%2524%255Ctextbf%257BU%257D%2524pdates%2520with%250A%2524%255Ctextbf%257BG%257D%2524r%2524%255Ctextbf%257BA%257D%2524dient%2520sp%2524%255Ctextbf%257BL%257D%2524itting%2529%252C%2520a%2520new%2520memory-efficient%250Aoptimization%2520framework.%2520%2524%255Ctexttt%257BFRUGAL%257D%2524%2520leverages%2520gradient%2520splitting%2520to%250Aperform%2520low-dimensional%2520updates%2520using%2520advanced%2520algorithms%2520%2528such%2520as%2520Adam%2529%252C%2520while%250Aupdates%2520along%2520the%2520remaining%2520directions%2520are%2520executed%2520via%2520state-free%2520methods%2520like%250ASGD%2520or%2520signSGD%2520%2528Bernstein%2520et%2520al.%252C%25202018%2529.%2520Our%2520framework%2520can%2520be%2520integrated%2520with%250Avarious%2520low-rank%2520update%2520selection%2520techniques%252C%2520including%2520GaLore%2520and%2520BAdam.%2520We%250Aprovide%2520theoretical%2520convergence%2520guarantees%2520for%2520our%2520framework%2520when%2520using%2520SGDM%250Afor%2520low-dimensional%2520updates%2520and%2520SGD%2520for%2520state-free%2520updates.%2520Additionally%252C%2520our%250Amethod%2520consistently%2520outperforms%2520concurrent%2520approaches%2520across%2520various%2520fixed%250Amemory%2520budgets%252C%2520achieving%2520state-of-the-art%2520results%2520in%2520pre-training%2520and%250Afine-tuning%2520tasks%2520while%2520balancing%2520memory%2520efficiency%2520and%2520performance%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07837v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRUGAL%3A%20Memory-Efficient%20Optimization%20by%20Reducing%20State%20Overhead%20for%0A%20%20Scalable%20Training&entry.906535625=Philip%20Zmushko%20and%20Aleksandr%20Beznosikov%20and%20Martin%20Tak%C3%A1%C4%8D%20and%20Samuel%20Horv%C3%A1th&entry.1292438233=%20%20With%20the%20increase%20in%20the%20number%20of%20parameters%20in%20large%20language%20models%2C%20the%0Aprocess%20of%20pre-training%20and%20fine-tuning%20increasingly%20demands%20larger%20volumes%20of%0AGPU%20memory.%20A%20significant%20portion%20of%20this%20memory%20is%20typically%20consumed%20by%20the%0Aoptimizer%20state.%20To%20overcome%20this%20challenge%2C%20recent%20approaches%20such%20as%20low-rank%0Aadaptation%20%28LoRA%20%28Hu%20et%20al.%2C%202021%29%29%2C%20low-rank%20gradient%20projection%20%28GaLore%20%28Zhao%0Aet%20al.%2C%202024%29%29%2C%20and%20blockwise%20optimization%20%28BAdam%20%28Luo%20et%20al.%2C%202024%29%29%20have%20been%0Aproposed.%20However%2C%20in%20all%20these%20algorithms%2C%20the%20%24%5Ctextit%7Beffective%20rank%20of%20the%0Aweight%20updates%20remains%20low-rank%7D%24%2C%20which%20can%20lead%20to%20a%20substantial%20loss%20of%0Ainformation%20from%20the%20gradient.%20This%20loss%20can%20be%20critically%20important%2C%0Aespecially%20during%20the%20pre-training%20stage.%20In%20this%20paper%2C%20we%20introduce%0A%24%5Ctexttt%7BFRUGAL%7D%24%20%28%24%5Ctextbf%7BF%7D%24ull-%24%5Ctextbf%7BR%7D%24ank%20%24%5Ctextbf%7BU%7D%24pdates%20with%0A%24%5Ctextbf%7BG%7D%24r%24%5Ctextbf%7BA%7D%24dient%20sp%24%5Ctextbf%7BL%7D%24itting%29%2C%20a%20new%20memory-efficient%0Aoptimization%20framework.%20%24%5Ctexttt%7BFRUGAL%7D%24%20leverages%20gradient%20splitting%20to%0Aperform%20low-dimensional%20updates%20using%20advanced%20algorithms%20%28such%20as%20Adam%29%2C%20while%0Aupdates%20along%20the%20remaining%20directions%20are%20executed%20via%20state-free%20methods%20like%0ASGD%20or%20signSGD%20%28Bernstein%20et%20al.%2C%202018%29.%20Our%20framework%20can%20be%20integrated%20with%0Avarious%20low-rank%20update%20selection%20techniques%2C%20including%20GaLore%20and%20BAdam.%20We%0Aprovide%20theoretical%20convergence%20guarantees%20for%20our%20framework%20when%20using%20SGDM%0Afor%20low-dimensional%20updates%20and%20SGD%20for%20state-free%20updates.%20Additionally%2C%20our%0Amethod%20consistently%20outperforms%20concurrent%20approaches%20across%20various%20fixed%0Amemory%20budgets%2C%20achieving%20state-of-the-art%20results%20in%20pre-training%20and%0Afine-tuning%20tasks%20while%20balancing%20memory%20efficiency%20and%20performance%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07837v3&entry.124074799=Read"},
{"title": "CVIRO: A Consistent and Tightly-Coupled Visual-Inertial-Ranging Odometry\n  on Lie Groups", "author": "Yizhi Zhou and Ziwei Kang and Jiawei Xia and Xuan Wang", "abstract": "  Ultra Wideband (UWB) is widely used to mitigate drift in visual-inertial\nodometry (VIO) systems. Consistency is crucial for ensuring the estimation\naccuracy of a UWBaided VIO system. An inconsistent estimator can degrade\nlocalization performance, where the inconsistency primarily arises from two\nmain factors: (1) the estimator fails to preserve the correct system\nobservability, and (2) UWB anchor positions are assumed to be known, leading to\nimproper neglect of calibration uncertainty. In this paper, we propose a\nconsistent and tightly-coupled visual-inertial-ranging odometry (CVIRO) system\nbased on the Lie group. Our method incorporates the UWB anchor state into the\nsystem state, explicitly accounting for UWB calibration uncertainty and\nenabling the joint and consistent estimation of both robot and anchor states.\nFurthermore, observability consistency is ensured by leveraging the invariant\nerror properties of the Lie group. We analytically prove that the CVIRO\nalgorithm naturally maintains the system's correct unobservable subspace,\nthereby preserving estimation consistency. Extensive simulations and\nexperiments demonstrate that CVIRO achieves superior localization accuracy and\nconsistency compared to existing methods.\n", "link": "http://arxiv.org/abs/2508.10867v1", "date": "2025-08-14", "relevancy": 2.063, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5466}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4968}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CVIRO%3A%20A%20Consistent%20and%20Tightly-Coupled%20Visual-Inertial-Ranging%20Odometry%0A%20%20on%20Lie%20Groups&body=Title%3A%20CVIRO%3A%20A%20Consistent%20and%20Tightly-Coupled%20Visual-Inertial-Ranging%20Odometry%0A%20%20on%20Lie%20Groups%0AAuthor%3A%20Yizhi%20Zhou%20and%20Ziwei%20Kang%20and%20Jiawei%20Xia%20and%20Xuan%20Wang%0AAbstract%3A%20%20%20Ultra%20Wideband%20%28UWB%29%20is%20widely%20used%20to%20mitigate%20drift%20in%20visual-inertial%0Aodometry%20%28VIO%29%20systems.%20Consistency%20is%20crucial%20for%20ensuring%20the%20estimation%0Aaccuracy%20of%20a%20UWBaided%20VIO%20system.%20An%20inconsistent%20estimator%20can%20degrade%0Alocalization%20performance%2C%20where%20the%20inconsistency%20primarily%20arises%20from%20two%0Amain%20factors%3A%20%281%29%20the%20estimator%20fails%20to%20preserve%20the%20correct%20system%0Aobservability%2C%20and%20%282%29%20UWB%20anchor%20positions%20are%20assumed%20to%20be%20known%2C%20leading%20to%0Aimproper%20neglect%20of%20calibration%20uncertainty.%20In%20this%20paper%2C%20we%20propose%20a%0Aconsistent%20and%20tightly-coupled%20visual-inertial-ranging%20odometry%20%28CVIRO%29%20system%0Abased%20on%20the%20Lie%20group.%20Our%20method%20incorporates%20the%20UWB%20anchor%20state%20into%20the%0Asystem%20state%2C%20explicitly%20accounting%20for%20UWB%20calibration%20uncertainty%20and%0Aenabling%20the%20joint%20and%20consistent%20estimation%20of%20both%20robot%20and%20anchor%20states.%0AFurthermore%2C%20observability%20consistency%20is%20ensured%20by%20leveraging%20the%20invariant%0Aerror%20properties%20of%20the%20Lie%20group.%20We%20analytically%20prove%20that%20the%20CVIRO%0Aalgorithm%20naturally%20maintains%20the%20system%27s%20correct%20unobservable%20subspace%2C%0Athereby%20preserving%20estimation%20consistency.%20Extensive%20simulations%20and%0Aexperiments%20demonstrate%20that%20CVIRO%20achieves%20superior%20localization%20accuracy%20and%0Aconsistency%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCVIRO%253A%2520A%2520Consistent%2520and%2520Tightly-Coupled%2520Visual-Inertial-Ranging%2520Odometry%250A%2520%2520on%2520Lie%2520Groups%26entry.906535625%3DYizhi%2520Zhou%2520and%2520Ziwei%2520Kang%2520and%2520Jiawei%2520Xia%2520and%2520Xuan%2520Wang%26entry.1292438233%3D%2520%2520Ultra%2520Wideband%2520%2528UWB%2529%2520is%2520widely%2520used%2520to%2520mitigate%2520drift%2520in%2520visual-inertial%250Aodometry%2520%2528VIO%2529%2520systems.%2520Consistency%2520is%2520crucial%2520for%2520ensuring%2520the%2520estimation%250Aaccuracy%2520of%2520a%2520UWBaided%2520VIO%2520system.%2520An%2520inconsistent%2520estimator%2520can%2520degrade%250Alocalization%2520performance%252C%2520where%2520the%2520inconsistency%2520primarily%2520arises%2520from%2520two%250Amain%2520factors%253A%2520%25281%2529%2520the%2520estimator%2520fails%2520to%2520preserve%2520the%2520correct%2520system%250Aobservability%252C%2520and%2520%25282%2529%2520UWB%2520anchor%2520positions%2520are%2520assumed%2520to%2520be%2520known%252C%2520leading%2520to%250Aimproper%2520neglect%2520of%2520calibration%2520uncertainty.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Aconsistent%2520and%2520tightly-coupled%2520visual-inertial-ranging%2520odometry%2520%2528CVIRO%2529%2520system%250Abased%2520on%2520the%2520Lie%2520group.%2520Our%2520method%2520incorporates%2520the%2520UWB%2520anchor%2520state%2520into%2520the%250Asystem%2520state%252C%2520explicitly%2520accounting%2520for%2520UWB%2520calibration%2520uncertainty%2520and%250Aenabling%2520the%2520joint%2520and%2520consistent%2520estimation%2520of%2520both%2520robot%2520and%2520anchor%2520states.%250AFurthermore%252C%2520observability%2520consistency%2520is%2520ensured%2520by%2520leveraging%2520the%2520invariant%250Aerror%2520properties%2520of%2520the%2520Lie%2520group.%2520We%2520analytically%2520prove%2520that%2520the%2520CVIRO%250Aalgorithm%2520naturally%2520maintains%2520the%2520system%2527s%2520correct%2520unobservable%2520subspace%252C%250Athereby%2520preserving%2520estimation%2520consistency.%2520Extensive%2520simulations%2520and%250Aexperiments%2520demonstrate%2520that%2520CVIRO%2520achieves%2520superior%2520localization%2520accuracy%2520and%250Aconsistency%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CVIRO%3A%20A%20Consistent%20and%20Tightly-Coupled%20Visual-Inertial-Ranging%20Odometry%0A%20%20on%20Lie%20Groups&entry.906535625=Yizhi%20Zhou%20and%20Ziwei%20Kang%20and%20Jiawei%20Xia%20and%20Xuan%20Wang&entry.1292438233=%20%20Ultra%20Wideband%20%28UWB%29%20is%20widely%20used%20to%20mitigate%20drift%20in%20visual-inertial%0Aodometry%20%28VIO%29%20systems.%20Consistency%20is%20crucial%20for%20ensuring%20the%20estimation%0Aaccuracy%20of%20a%20UWBaided%20VIO%20system.%20An%20inconsistent%20estimator%20can%20degrade%0Alocalization%20performance%2C%20where%20the%20inconsistency%20primarily%20arises%20from%20two%0Amain%20factors%3A%20%281%29%20the%20estimator%20fails%20to%20preserve%20the%20correct%20system%0Aobservability%2C%20and%20%282%29%20UWB%20anchor%20positions%20are%20assumed%20to%20be%20known%2C%20leading%20to%0Aimproper%20neglect%20of%20calibration%20uncertainty.%20In%20this%20paper%2C%20we%20propose%20a%0Aconsistent%20and%20tightly-coupled%20visual-inertial-ranging%20odometry%20%28CVIRO%29%20system%0Abased%20on%20the%20Lie%20group.%20Our%20method%20incorporates%20the%20UWB%20anchor%20state%20into%20the%0Asystem%20state%2C%20explicitly%20accounting%20for%20UWB%20calibration%20uncertainty%20and%0Aenabling%20the%20joint%20and%20consistent%20estimation%20of%20both%20robot%20and%20anchor%20states.%0AFurthermore%2C%20observability%20consistency%20is%20ensured%20by%20leveraging%20the%20invariant%0Aerror%20properties%20of%20the%20Lie%20group.%20We%20analytically%20prove%20that%20the%20CVIRO%0Aalgorithm%20naturally%20maintains%20the%20system%27s%20correct%20unobservable%20subspace%2C%0Athereby%20preserving%20estimation%20consistency.%20Extensive%20simulations%20and%0Aexperiments%20demonstrate%20that%20CVIRO%20achieves%20superior%20localization%20accuracy%20and%0Aconsistency%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10867v1&entry.124074799=Read"},
{"title": "IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection", "author": "Yanhui Li and Yunkang Cao and Chengliang Liu and Yuan Xiong and Xinghui Dong and Chao Huang", "abstract": "  Industrial anomaly detection is a critical component of modern manufacturing,\nyet the scarcity of defective samples restricts traditional detection methods\nto scenario-specific applications. Although Vision-Language Models (VLMs)\ndemonstrate significant advantages in generalization capabilities, their\nperformance in industrial anomaly detection remains limited. To address this\nchallenge, we propose IAD-R1, a universal post-training framework applicable to\nVLMs of different architectures and parameter scales, which substantially\nenhances their anomaly detection capabilities. IAD-R1 employs a two-stage\ntraining strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT)\nstage utilizes a meticulously constructed high-quality Chain-of-Thought dataset\n(Expert-AD) for training, enhancing anomaly perception capabilities and\nestablishing reasoning-to-answer correlations; the Structured Control Group\nRelative Policy Optimization (SC-GRPO) stage employs carefully designed reward\nfunctions to achieve a capability leap from \"Anomaly Perception\" to \"Anomaly\nInterpretation\". Experimental results demonstrate that IAD-R1 achieves\nsignificant improvements across 7 VLMs, the largest improvement was on the DAGM\ndataset, with average accuracy 43.3% higher than the 0.5B baseline. Notably,\nthe 0.5B parameter model trained with IAD-R1 surpasses commercial models\nincluding GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the\neffectiveness and superiority of IAD-R1. The dataset, code, and all model\nweights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.\n", "link": "http://arxiv.org/abs/2508.09178v2", "date": "2025-08-14", "relevancy": 2.0599, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5176}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5145}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IAD-R1%3A%20Reinforcing%20Consistent%20Reasoning%20in%20Industrial%20Anomaly%20Detection&body=Title%3A%20IAD-R1%3A%20Reinforcing%20Consistent%20Reasoning%20in%20Industrial%20Anomaly%20Detection%0AAuthor%3A%20Yanhui%20Li%20and%20Yunkang%20Cao%20and%20Chengliang%20Liu%20and%20Yuan%20Xiong%20and%20Xinghui%20Dong%20and%20Chao%20Huang%0AAbstract%3A%20%20%20Industrial%20anomaly%20detection%20is%20a%20critical%20component%20of%20modern%20manufacturing%2C%0Ayet%20the%20scarcity%20of%20defective%20samples%20restricts%20traditional%20detection%20methods%0Ato%20scenario-specific%20applications.%20Although%20Vision-Language%20Models%20%28VLMs%29%0Ademonstrate%20significant%20advantages%20in%20generalization%20capabilities%2C%20their%0Aperformance%20in%20industrial%20anomaly%20detection%20remains%20limited.%20To%20address%20this%0Achallenge%2C%20we%20propose%20IAD-R1%2C%20a%20universal%20post-training%20framework%20applicable%20to%0AVLMs%20of%20different%20architectures%20and%20parameter%20scales%2C%20which%20substantially%0Aenhances%20their%20anomaly%20detection%20capabilities.%20IAD-R1%20employs%20a%20two-stage%0Atraining%20strategy%3A%20the%20Perception%20Activation%20Supervised%20Fine-Tuning%20%28PA-SFT%29%0Astage%20utilizes%20a%20meticulously%20constructed%20high-quality%20Chain-of-Thought%20dataset%0A%28Expert-AD%29%20for%20training%2C%20enhancing%20anomaly%20perception%20capabilities%20and%0Aestablishing%20reasoning-to-answer%20correlations%3B%20the%20Structured%20Control%20Group%0ARelative%20Policy%20Optimization%20%28SC-GRPO%29%20stage%20employs%20carefully%20designed%20reward%0Afunctions%20to%20achieve%20a%20capability%20leap%20from%20%22Anomaly%20Perception%22%20to%20%22Anomaly%0AInterpretation%22.%20Experimental%20results%20demonstrate%20that%20IAD-R1%20achieves%0Asignificant%20improvements%20across%207%20VLMs%2C%20the%20largest%20improvement%20was%20on%20the%20DAGM%0Adataset%2C%20with%20average%20accuracy%2043.3%25%20higher%20than%20the%200.5B%20baseline.%20Notably%2C%0Athe%200.5B%20parameter%20model%20trained%20with%20IAD-R1%20surpasses%20commercial%20models%0Aincluding%20GPT-4.1%20and%20Claude-Sonnet-4%20in%20zero-shot%20settings%2C%20demonstrating%20the%0Aeffectiveness%20and%20superiority%20of%20IAD-R1.%20The%20dataset%2C%20code%2C%20and%20all%20model%0Aweights%20will%20be%20publicly%20available%20at%20https%3A//github.com/Yanhui-Lee/IAD-R1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09178v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIAD-R1%253A%2520Reinforcing%2520Consistent%2520Reasoning%2520in%2520Industrial%2520Anomaly%2520Detection%26entry.906535625%3DYanhui%2520Li%2520and%2520Yunkang%2520Cao%2520and%2520Chengliang%2520Liu%2520and%2520Yuan%2520Xiong%2520and%2520Xinghui%2520Dong%2520and%2520Chao%2520Huang%26entry.1292438233%3D%2520%2520Industrial%2520anomaly%2520detection%2520is%2520a%2520critical%2520component%2520of%2520modern%2520manufacturing%252C%250Ayet%2520the%2520scarcity%2520of%2520defective%2520samples%2520restricts%2520traditional%2520detection%2520methods%250Ato%2520scenario-specific%2520applications.%2520Although%2520Vision-Language%2520Models%2520%2528VLMs%2529%250Ademonstrate%2520significant%2520advantages%2520in%2520generalization%2520capabilities%252C%2520their%250Aperformance%2520in%2520industrial%2520anomaly%2520detection%2520remains%2520limited.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520propose%2520IAD-R1%252C%2520a%2520universal%2520post-training%2520framework%2520applicable%2520to%250AVLMs%2520of%2520different%2520architectures%2520and%2520parameter%2520scales%252C%2520which%2520substantially%250Aenhances%2520their%2520anomaly%2520detection%2520capabilities.%2520IAD-R1%2520employs%2520a%2520two-stage%250Atraining%2520strategy%253A%2520the%2520Perception%2520Activation%2520Supervised%2520Fine-Tuning%2520%2528PA-SFT%2529%250Astage%2520utilizes%2520a%2520meticulously%2520constructed%2520high-quality%2520Chain-of-Thought%2520dataset%250A%2528Expert-AD%2529%2520for%2520training%252C%2520enhancing%2520anomaly%2520perception%2520capabilities%2520and%250Aestablishing%2520reasoning-to-answer%2520correlations%253B%2520the%2520Structured%2520Control%2520Group%250ARelative%2520Policy%2520Optimization%2520%2528SC-GRPO%2529%2520stage%2520employs%2520carefully%2520designed%2520reward%250Afunctions%2520to%2520achieve%2520a%2520capability%2520leap%2520from%2520%2522Anomaly%2520Perception%2522%2520to%2520%2522Anomaly%250AInterpretation%2522.%2520Experimental%2520results%2520demonstrate%2520that%2520IAD-R1%2520achieves%250Asignificant%2520improvements%2520across%25207%2520VLMs%252C%2520the%2520largest%2520improvement%2520was%2520on%2520the%2520DAGM%250Adataset%252C%2520with%2520average%2520accuracy%252043.3%2525%2520higher%2520than%2520the%25200.5B%2520baseline.%2520Notably%252C%250Athe%25200.5B%2520parameter%2520model%2520trained%2520with%2520IAD-R1%2520surpasses%2520commercial%2520models%250Aincluding%2520GPT-4.1%2520and%2520Claude-Sonnet-4%2520in%2520zero-shot%2520settings%252C%2520demonstrating%2520the%250Aeffectiveness%2520and%2520superiority%2520of%2520IAD-R1.%2520The%2520dataset%252C%2520code%252C%2520and%2520all%2520model%250Aweights%2520will%2520be%2520publicly%2520available%2520at%2520https%253A//github.com/Yanhui-Lee/IAD-R1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09178v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IAD-R1%3A%20Reinforcing%20Consistent%20Reasoning%20in%20Industrial%20Anomaly%20Detection&entry.906535625=Yanhui%20Li%20and%20Yunkang%20Cao%20and%20Chengliang%20Liu%20and%20Yuan%20Xiong%20and%20Xinghui%20Dong%20and%20Chao%20Huang&entry.1292438233=%20%20Industrial%20anomaly%20detection%20is%20a%20critical%20component%20of%20modern%20manufacturing%2C%0Ayet%20the%20scarcity%20of%20defective%20samples%20restricts%20traditional%20detection%20methods%0Ato%20scenario-specific%20applications.%20Although%20Vision-Language%20Models%20%28VLMs%29%0Ademonstrate%20significant%20advantages%20in%20generalization%20capabilities%2C%20their%0Aperformance%20in%20industrial%20anomaly%20detection%20remains%20limited.%20To%20address%20this%0Achallenge%2C%20we%20propose%20IAD-R1%2C%20a%20universal%20post-training%20framework%20applicable%20to%0AVLMs%20of%20different%20architectures%20and%20parameter%20scales%2C%20which%20substantially%0Aenhances%20their%20anomaly%20detection%20capabilities.%20IAD-R1%20employs%20a%20two-stage%0Atraining%20strategy%3A%20the%20Perception%20Activation%20Supervised%20Fine-Tuning%20%28PA-SFT%29%0Astage%20utilizes%20a%20meticulously%20constructed%20high-quality%20Chain-of-Thought%20dataset%0A%28Expert-AD%29%20for%20training%2C%20enhancing%20anomaly%20perception%20capabilities%20and%0Aestablishing%20reasoning-to-answer%20correlations%3B%20the%20Structured%20Control%20Group%0ARelative%20Policy%20Optimization%20%28SC-GRPO%29%20stage%20employs%20carefully%20designed%20reward%0Afunctions%20to%20achieve%20a%20capability%20leap%20from%20%22Anomaly%20Perception%22%20to%20%22Anomaly%0AInterpretation%22.%20Experimental%20results%20demonstrate%20that%20IAD-R1%20achieves%0Asignificant%20improvements%20across%207%20VLMs%2C%20the%20largest%20improvement%20was%20on%20the%20DAGM%0Adataset%2C%20with%20average%20accuracy%2043.3%25%20higher%20than%20the%200.5B%20baseline.%20Notably%2C%0Athe%200.5B%20parameter%20model%20trained%20with%20IAD-R1%20surpasses%20commercial%20models%0Aincluding%20GPT-4.1%20and%20Claude-Sonnet-4%20in%20zero-shot%20settings%2C%20demonstrating%20the%0Aeffectiveness%20and%20superiority%20of%20IAD-R1.%20The%20dataset%2C%20code%2C%20and%20all%20model%0Aweights%20will%20be%20publicly%20available%20at%20https%3A//github.com/Yanhui-Lee/IAD-R1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09178v2&entry.124074799=Read"},
{"title": "Mobile-Friendly Deep Learning for Plant Disease Detection: A Lightweight\n  CNN Benchmark Across 101 Classes of 33 Crops", "author": "Anand Kumar and Harminder Pal Monga and Tapasi Brahma and Satyam Kalra and Navas Sherif", "abstract": "  Plant diseases are a major threat to food security globally. It is important\nto develop early detection systems which can accurately detect. The advancement\nin computer vision techniques has the potential to solve this challenge. We\nhave developed a mobile-friendly solution which can accurately classify 101\nplant diseases across 33 crops. We built a comprehensive dataset by combining\ndifferent datasets, Plant Doc, PlantVillage, and PlantWild, all of which are\nfor the same purpose. We evaluated performance across several lightweight\narchitectures - MobileNetV2, MobileNetV3, MobileNetV3-Large, and\nEfficientNet-B0, B1 - specifically chosen for their efficiency on\nresource-constrained devices. The results were promising, with EfficientNet-B1\ndelivering our best performance at 94.7% classification accuracy. This\narchitecture struck an optimal balance between accuracy and computational\nefficiency, making it well-suited for real-world deployment on mobile devices.\n", "link": "http://arxiv.org/abs/2508.10817v1", "date": "2025-08-14", "relevancy": 1.8971, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4874}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4826}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mobile-Friendly%20Deep%20Learning%20for%20Plant%20Disease%20Detection%3A%20A%20Lightweight%0A%20%20CNN%20Benchmark%20Across%20101%20Classes%20of%2033%20Crops&body=Title%3A%20Mobile-Friendly%20Deep%20Learning%20for%20Plant%20Disease%20Detection%3A%20A%20Lightweight%0A%20%20CNN%20Benchmark%20Across%20101%20Classes%20of%2033%20Crops%0AAuthor%3A%20Anand%20Kumar%20and%20Harminder%20Pal%20Monga%20and%20Tapasi%20Brahma%20and%20Satyam%20Kalra%20and%20Navas%20Sherif%0AAbstract%3A%20%20%20Plant%20diseases%20are%20a%20major%20threat%20to%20food%20security%20globally.%20It%20is%20important%0Ato%20develop%20early%20detection%20systems%20which%20can%20accurately%20detect.%20The%20advancement%0Ain%20computer%20vision%20techniques%20has%20the%20potential%20to%20solve%20this%20challenge.%20We%0Ahave%20developed%20a%20mobile-friendly%20solution%20which%20can%20accurately%20classify%20101%0Aplant%20diseases%20across%2033%20crops.%20We%20built%20a%20comprehensive%20dataset%20by%20combining%0Adifferent%20datasets%2C%20Plant%20Doc%2C%20PlantVillage%2C%20and%20PlantWild%2C%20all%20of%20which%20are%0Afor%20the%20same%20purpose.%20We%20evaluated%20performance%20across%20several%20lightweight%0Aarchitectures%20-%20MobileNetV2%2C%20MobileNetV3%2C%20MobileNetV3-Large%2C%20and%0AEfficientNet-B0%2C%20B1%20-%20specifically%20chosen%20for%20their%20efficiency%20on%0Aresource-constrained%20devices.%20The%20results%20were%20promising%2C%20with%20EfficientNet-B1%0Adelivering%20our%20best%20performance%20at%2094.7%25%20classification%20accuracy.%20This%0Aarchitecture%20struck%20an%20optimal%20balance%20between%20accuracy%20and%20computational%0Aefficiency%2C%20making%20it%20well-suited%20for%20real-world%20deployment%20on%20mobile%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMobile-Friendly%2520Deep%2520Learning%2520for%2520Plant%2520Disease%2520Detection%253A%2520A%2520Lightweight%250A%2520%2520CNN%2520Benchmark%2520Across%2520101%2520Classes%2520of%252033%2520Crops%26entry.906535625%3DAnand%2520Kumar%2520and%2520Harminder%2520Pal%2520Monga%2520and%2520Tapasi%2520Brahma%2520and%2520Satyam%2520Kalra%2520and%2520Navas%2520Sherif%26entry.1292438233%3D%2520%2520Plant%2520diseases%2520are%2520a%2520major%2520threat%2520to%2520food%2520security%2520globally.%2520It%2520is%2520important%250Ato%2520develop%2520early%2520detection%2520systems%2520which%2520can%2520accurately%2520detect.%2520The%2520advancement%250Ain%2520computer%2520vision%2520techniques%2520has%2520the%2520potential%2520to%2520solve%2520this%2520challenge.%2520We%250Ahave%2520developed%2520a%2520mobile-friendly%2520solution%2520which%2520can%2520accurately%2520classify%2520101%250Aplant%2520diseases%2520across%252033%2520crops.%2520We%2520built%2520a%2520comprehensive%2520dataset%2520by%2520combining%250Adifferent%2520datasets%252C%2520Plant%2520Doc%252C%2520PlantVillage%252C%2520and%2520PlantWild%252C%2520all%2520of%2520which%2520are%250Afor%2520the%2520same%2520purpose.%2520We%2520evaluated%2520performance%2520across%2520several%2520lightweight%250Aarchitectures%2520-%2520MobileNetV2%252C%2520MobileNetV3%252C%2520MobileNetV3-Large%252C%2520and%250AEfficientNet-B0%252C%2520B1%2520-%2520specifically%2520chosen%2520for%2520their%2520efficiency%2520on%250Aresource-constrained%2520devices.%2520The%2520results%2520were%2520promising%252C%2520with%2520EfficientNet-B1%250Adelivering%2520our%2520best%2520performance%2520at%252094.7%2525%2520classification%2520accuracy.%2520This%250Aarchitecture%2520struck%2520an%2520optimal%2520balance%2520between%2520accuracy%2520and%2520computational%250Aefficiency%252C%2520making%2520it%2520well-suited%2520for%2520real-world%2520deployment%2520on%2520mobile%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mobile-Friendly%20Deep%20Learning%20for%20Plant%20Disease%20Detection%3A%20A%20Lightweight%0A%20%20CNN%20Benchmark%20Across%20101%20Classes%20of%2033%20Crops&entry.906535625=Anand%20Kumar%20and%20Harminder%20Pal%20Monga%20and%20Tapasi%20Brahma%20and%20Satyam%20Kalra%20and%20Navas%20Sherif&entry.1292438233=%20%20Plant%20diseases%20are%20a%20major%20threat%20to%20food%20security%20globally.%20It%20is%20important%0Ato%20develop%20early%20detection%20systems%20which%20can%20accurately%20detect.%20The%20advancement%0Ain%20computer%20vision%20techniques%20has%20the%20potential%20to%20solve%20this%20challenge.%20We%0Ahave%20developed%20a%20mobile-friendly%20solution%20which%20can%20accurately%20classify%20101%0Aplant%20diseases%20across%2033%20crops.%20We%20built%20a%20comprehensive%20dataset%20by%20combining%0Adifferent%20datasets%2C%20Plant%20Doc%2C%20PlantVillage%2C%20and%20PlantWild%2C%20all%20of%20which%20are%0Afor%20the%20same%20purpose.%20We%20evaluated%20performance%20across%20several%20lightweight%0Aarchitectures%20-%20MobileNetV2%2C%20MobileNetV3%2C%20MobileNetV3-Large%2C%20and%0AEfficientNet-B0%2C%20B1%20-%20specifically%20chosen%20for%20their%20efficiency%20on%0Aresource-constrained%20devices.%20The%20results%20were%20promising%2C%20with%20EfficientNet-B1%0Adelivering%20our%20best%20performance%20at%2094.7%25%20classification%20accuracy.%20This%0Aarchitecture%20struck%20an%20optimal%20balance%20between%20accuracy%20and%20computational%0Aefficiency%2C%20making%20it%20well-suited%20for%20real-world%20deployment%20on%20mobile%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10817v1&entry.124074799=Read"},
{"title": "Learning from Natural Language Feedback for Personalized Question\n  Answering", "author": "Alireza Salemi and Hamed Zamani", "abstract": "  Personalization is crucial for enhancing both the effectiveness and user\nsatisfaction of language technologies, particularly in information-seeking\ntasks like question answering. Current approaches for personalizing large\nlanguage models (LLMs) often rely on retrieval-augmented generation (RAG),\nfollowed by reinforcement learning with scalar reward signals to teach models\nhow to use retrieved personal context. We believe that these scalar rewards\nsometimes provide weak, non-instructive feedback, limiting learning efficiency\nand personalization quality. We introduce VAC, a novel framework for\npersonalized response generation that replaces scalar rewards with natural\nlanguage feedback (NLF) that are generated conditioned on the user profiles and\nthe question narratives. NLF serves as a rich and actionable supervision\nsignal, allowing the policy model to iteratively refine its outputs and\ninternalize effective personalization strategies. Training alternates between\noptimizing the feedback model and fine-tuning the policy model on the improved\nresponses, resulting in a policy model that no longer requires feedback at\ninference. Evaluation on the LaMP-QA benchmark that consists of three diverse\ndomains demonstrates consistent and significant improvements over the\nstate-of-the-art results. Human evaluations further confirm the superior\nquality of the generated responses. These results demonstrate that NLF provides\nmore effective signals for optimizing personalized question answering.\n", "link": "http://arxiv.org/abs/2508.10695v1", "date": "2025-08-14", "relevancy": 1.4029, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4786}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.466}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Natural%20Language%20Feedback%20for%20Personalized%20Question%0A%20%20Answering&body=Title%3A%20Learning%20from%20Natural%20Language%20Feedback%20for%20Personalized%20Question%0A%20%20Answering%0AAuthor%3A%20Alireza%20Salemi%20and%20Hamed%20Zamani%0AAbstract%3A%20%20%20Personalization%20is%20crucial%20for%20enhancing%20both%20the%20effectiveness%20and%20user%0Asatisfaction%20of%20language%20technologies%2C%20particularly%20in%20information-seeking%0Atasks%20like%20question%20answering.%20Current%20approaches%20for%20personalizing%20large%0Alanguage%20models%20%28LLMs%29%20often%20rely%20on%20retrieval-augmented%20generation%20%28RAG%29%2C%0Afollowed%20by%20reinforcement%20learning%20with%20scalar%20reward%20signals%20to%20teach%20models%0Ahow%20to%20use%20retrieved%20personal%20context.%20We%20believe%20that%20these%20scalar%20rewards%0Asometimes%20provide%20weak%2C%20non-instructive%20feedback%2C%20limiting%20learning%20efficiency%0Aand%20personalization%20quality.%20We%20introduce%20VAC%2C%20a%20novel%20framework%20for%0Apersonalized%20response%20generation%20that%20replaces%20scalar%20rewards%20with%20natural%0Alanguage%20feedback%20%28NLF%29%20that%20are%20generated%20conditioned%20on%20the%20user%20profiles%20and%0Athe%20question%20narratives.%20NLF%20serves%20as%20a%20rich%20and%20actionable%20supervision%0Asignal%2C%20allowing%20the%20policy%20model%20to%20iteratively%20refine%20its%20outputs%20and%0Ainternalize%20effective%20personalization%20strategies.%20Training%20alternates%20between%0Aoptimizing%20the%20feedback%20model%20and%20fine-tuning%20the%20policy%20model%20on%20the%20improved%0Aresponses%2C%20resulting%20in%20a%20policy%20model%20that%20no%20longer%20requires%20feedback%20at%0Ainference.%20Evaluation%20on%20the%20LaMP-QA%20benchmark%20that%20consists%20of%20three%20diverse%0Adomains%20demonstrates%20consistent%20and%20significant%20improvements%20over%20the%0Astate-of-the-art%20results.%20Human%20evaluations%20further%20confirm%20the%20superior%0Aquality%20of%20the%20generated%20responses.%20These%20results%20demonstrate%20that%20NLF%20provides%0Amore%20effective%20signals%20for%20optimizing%20personalized%20question%20answering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Natural%2520Language%2520Feedback%2520for%2520Personalized%2520Question%250A%2520%2520Answering%26entry.906535625%3DAlireza%2520Salemi%2520and%2520Hamed%2520Zamani%26entry.1292438233%3D%2520%2520Personalization%2520is%2520crucial%2520for%2520enhancing%2520both%2520the%2520effectiveness%2520and%2520user%250Asatisfaction%2520of%2520language%2520technologies%252C%2520particularly%2520in%2520information-seeking%250Atasks%2520like%2520question%2520answering.%2520Current%2520approaches%2520for%2520personalizing%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520often%2520rely%2520on%2520retrieval-augmented%2520generation%2520%2528RAG%2529%252C%250Afollowed%2520by%2520reinforcement%2520learning%2520with%2520scalar%2520reward%2520signals%2520to%2520teach%2520models%250Ahow%2520to%2520use%2520retrieved%2520personal%2520context.%2520We%2520believe%2520that%2520these%2520scalar%2520rewards%250Asometimes%2520provide%2520weak%252C%2520non-instructive%2520feedback%252C%2520limiting%2520learning%2520efficiency%250Aand%2520personalization%2520quality.%2520We%2520introduce%2520VAC%252C%2520a%2520novel%2520framework%2520for%250Apersonalized%2520response%2520generation%2520that%2520replaces%2520scalar%2520rewards%2520with%2520natural%250Alanguage%2520feedback%2520%2528NLF%2529%2520that%2520are%2520generated%2520conditioned%2520on%2520the%2520user%2520profiles%2520and%250Athe%2520question%2520narratives.%2520NLF%2520serves%2520as%2520a%2520rich%2520and%2520actionable%2520supervision%250Asignal%252C%2520allowing%2520the%2520policy%2520model%2520to%2520iteratively%2520refine%2520its%2520outputs%2520and%250Ainternalize%2520effective%2520personalization%2520strategies.%2520Training%2520alternates%2520between%250Aoptimizing%2520the%2520feedback%2520model%2520and%2520fine-tuning%2520the%2520policy%2520model%2520on%2520the%2520improved%250Aresponses%252C%2520resulting%2520in%2520a%2520policy%2520model%2520that%2520no%2520longer%2520requires%2520feedback%2520at%250Ainference.%2520Evaluation%2520on%2520the%2520LaMP-QA%2520benchmark%2520that%2520consists%2520of%2520three%2520diverse%250Adomains%2520demonstrates%2520consistent%2520and%2520significant%2520improvements%2520over%2520the%250Astate-of-the-art%2520results.%2520Human%2520evaluations%2520further%2520confirm%2520the%2520superior%250Aquality%2520of%2520the%2520generated%2520responses.%2520These%2520results%2520demonstrate%2520that%2520NLF%2520provides%250Amore%2520effective%2520signals%2520for%2520optimizing%2520personalized%2520question%2520answering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Natural%20Language%20Feedback%20for%20Personalized%20Question%0A%20%20Answering&entry.906535625=Alireza%20Salemi%20and%20Hamed%20Zamani&entry.1292438233=%20%20Personalization%20is%20crucial%20for%20enhancing%20both%20the%20effectiveness%20and%20user%0Asatisfaction%20of%20language%20technologies%2C%20particularly%20in%20information-seeking%0Atasks%20like%20question%20answering.%20Current%20approaches%20for%20personalizing%20large%0Alanguage%20models%20%28LLMs%29%20often%20rely%20on%20retrieval-augmented%20generation%20%28RAG%29%2C%0Afollowed%20by%20reinforcement%20learning%20with%20scalar%20reward%20signals%20to%20teach%20models%0Ahow%20to%20use%20retrieved%20personal%20context.%20We%20believe%20that%20these%20scalar%20rewards%0Asometimes%20provide%20weak%2C%20non-instructive%20feedback%2C%20limiting%20learning%20efficiency%0Aand%20personalization%20quality.%20We%20introduce%20VAC%2C%20a%20novel%20framework%20for%0Apersonalized%20response%20generation%20that%20replaces%20scalar%20rewards%20with%20natural%0Alanguage%20feedback%20%28NLF%29%20that%20are%20generated%20conditioned%20on%20the%20user%20profiles%20and%0Athe%20question%20narratives.%20NLF%20serves%20as%20a%20rich%20and%20actionable%20supervision%0Asignal%2C%20allowing%20the%20policy%20model%20to%20iteratively%20refine%20its%20outputs%20and%0Ainternalize%20effective%20personalization%20strategies.%20Training%20alternates%20between%0Aoptimizing%20the%20feedback%20model%20and%20fine-tuning%20the%20policy%20model%20on%20the%20improved%0Aresponses%2C%20resulting%20in%20a%20policy%20model%20that%20no%20longer%20requires%20feedback%20at%0Ainference.%20Evaluation%20on%20the%20LaMP-QA%20benchmark%20that%20consists%20of%20three%20diverse%0Adomains%20demonstrates%20consistent%20and%20significant%20improvements%20over%20the%0Astate-of-the-art%20results.%20Human%20evaluations%20further%20confirm%20the%20superior%0Aquality%20of%20the%20generated%20responses.%20These%20results%20demonstrate%20that%20NLF%20provides%0Amore%20effective%20signals%20for%20optimizing%20personalized%20question%20answering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10695v1&entry.124074799=Read"},
{"title": "Accelerating exoplanet climate modelling: A machine learning approach to\n  complement 3D GCM grid simulations", "author": "Alexander Plaschzug and Amit Reza and Ludmila Carone and Sebastian Gernjak and Christiane Helling", "abstract": "  With the development of ever-improving telescopes capable of observing\nexoplanet atmospheres in greater detail and number, there is a growing demand\nfor enhanced 3D climate models to support and help interpret observational data\nfrom space missions like CHEOPS, TESS, JWST, PLATO, and Ariel. However, the\ncomputationally intensive and time-consuming nature of general circulation\nmodels (GCMs) poses significant challenges in simulating a wide range of\nexoplanetary atmospheres. This study aims to determine whether machine learning\n(ML) algorithms can be used to predict the 3D temperature and wind structure of\narbitrary tidally-locked gaseous exoplanets in a range of planetary parameters.\nA new 3D GCM grid with 60 inflated hot Jupiters orbiting A, F, G, K, and M-type\nhost stars modelled with Exorad has been introduced. A dense neural network\n(DNN) and a decision tree algorithm (XGBoost) are trained on this grid to\npredict local gas temperatures along with horizontal and vertical winds. To\nensure the reliability and quality of the ML model predictions, WASP-121 b,\nHATS-42 b, NGTS-17 b, WASP-23 b, and NGTS-1 b-like planets, which are all\ntargets for PLATO observation, are selected and modelled with ExoRad and the\ntwo ML methods as test cases. The DNN predictions for the gas temperatures are\nto such a degree that the calculated spectra agree within 32 ppm for all but\none planet, for which only one single HCN feature reaches a 100 ppm difference.\nThe developed ML emulators can reliably predict the complete 3D temperature\nfield of an inflated warm to ultra-hot tidally locked Jupiter around A to\nM-type host stars. It provides a fast tool to complement and extend traditional\nGCM grids for exoplanet ensemble studies. The quality of the predictions is\nsuch that no or minimal effects on the gas phase chemistry, hence on the cloud\nformation and transmission spectra, are to be expected.\n", "link": "http://arxiv.org/abs/2508.10827v1", "date": "2025-08-14", "relevancy": 1.7706, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4483}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4458}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20exoplanet%20climate%20modelling%3A%20A%20machine%20learning%20approach%20to%0A%20%20complement%203D%20GCM%20grid%20simulations&body=Title%3A%20Accelerating%20exoplanet%20climate%20modelling%3A%20A%20machine%20learning%20approach%20to%0A%20%20complement%203D%20GCM%20grid%20simulations%0AAuthor%3A%20Alexander%20Plaschzug%20and%20Amit%20Reza%20and%20Ludmila%20Carone%20and%20Sebastian%20Gernjak%20and%20Christiane%20Helling%0AAbstract%3A%20%20%20With%20the%20development%20of%20ever-improving%20telescopes%20capable%20of%20observing%0Aexoplanet%20atmospheres%20in%20greater%20detail%20and%20number%2C%20there%20is%20a%20growing%20demand%0Afor%20enhanced%203D%20climate%20models%20to%20support%20and%20help%20interpret%20observational%20data%0Afrom%20space%20missions%20like%20CHEOPS%2C%20TESS%2C%20JWST%2C%20PLATO%2C%20and%20Ariel.%20However%2C%20the%0Acomputationally%20intensive%20and%20time-consuming%20nature%20of%20general%20circulation%0Amodels%20%28GCMs%29%20poses%20significant%20challenges%20in%20simulating%20a%20wide%20range%20of%0Aexoplanetary%20atmospheres.%20This%20study%20aims%20to%20determine%20whether%20machine%20learning%0A%28ML%29%20algorithms%20can%20be%20used%20to%20predict%20the%203D%20temperature%20and%20wind%20structure%20of%0Aarbitrary%20tidally-locked%20gaseous%20exoplanets%20in%20a%20range%20of%20planetary%20parameters.%0AA%20new%203D%20GCM%20grid%20with%2060%20inflated%20hot%20Jupiters%20orbiting%20A%2C%20F%2C%20G%2C%20K%2C%20and%20M-type%0Ahost%20stars%20modelled%20with%20Exorad%20has%20been%20introduced.%20A%20dense%20neural%20network%0A%28DNN%29%20and%20a%20decision%20tree%20algorithm%20%28XGBoost%29%20are%20trained%20on%20this%20grid%20to%0Apredict%20local%20gas%20temperatures%20along%20with%20horizontal%20and%20vertical%20winds.%20To%0Aensure%20the%20reliability%20and%20quality%20of%20the%20ML%20model%20predictions%2C%20WASP-121%20b%2C%0AHATS-42%20b%2C%20NGTS-17%20b%2C%20WASP-23%20b%2C%20and%20NGTS-1%20b-like%20planets%2C%20which%20are%20all%0Atargets%20for%20PLATO%20observation%2C%20are%20selected%20and%20modelled%20with%20ExoRad%20and%20the%0Atwo%20ML%20methods%20as%20test%20cases.%20The%20DNN%20predictions%20for%20the%20gas%20temperatures%20are%0Ato%20such%20a%20degree%20that%20the%20calculated%20spectra%20agree%20within%2032%20ppm%20for%20all%20but%0Aone%20planet%2C%20for%20which%20only%20one%20single%20HCN%20feature%20reaches%20a%20100%20ppm%20difference.%0AThe%20developed%20ML%20emulators%20can%20reliably%20predict%20the%20complete%203D%20temperature%0Afield%20of%20an%20inflated%20warm%20to%20ultra-hot%20tidally%20locked%20Jupiter%20around%20A%20to%0AM-type%20host%20stars.%20It%20provides%20a%20fast%20tool%20to%20complement%20and%20extend%20traditional%0AGCM%20grids%20for%20exoplanet%20ensemble%20studies.%20The%20quality%20of%20the%20predictions%20is%0Asuch%20that%20no%20or%20minimal%20effects%20on%20the%20gas%20phase%20chemistry%2C%20hence%20on%20the%20cloud%0Aformation%20and%20transmission%20spectra%2C%20are%20to%20be%20expected.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520exoplanet%2520climate%2520modelling%253A%2520A%2520machine%2520learning%2520approach%2520to%250A%2520%2520complement%25203D%2520GCM%2520grid%2520simulations%26entry.906535625%3DAlexander%2520Plaschzug%2520and%2520Amit%2520Reza%2520and%2520Ludmila%2520Carone%2520and%2520Sebastian%2520Gernjak%2520and%2520Christiane%2520Helling%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520of%2520ever-improving%2520telescopes%2520capable%2520of%2520observing%250Aexoplanet%2520atmospheres%2520in%2520greater%2520detail%2520and%2520number%252C%2520there%2520is%2520a%2520growing%2520demand%250Afor%2520enhanced%25203D%2520climate%2520models%2520to%2520support%2520and%2520help%2520interpret%2520observational%2520data%250Afrom%2520space%2520missions%2520like%2520CHEOPS%252C%2520TESS%252C%2520JWST%252C%2520PLATO%252C%2520and%2520Ariel.%2520However%252C%2520the%250Acomputationally%2520intensive%2520and%2520time-consuming%2520nature%2520of%2520general%2520circulation%250Amodels%2520%2528GCMs%2529%2520poses%2520significant%2520challenges%2520in%2520simulating%2520a%2520wide%2520range%2520of%250Aexoplanetary%2520atmospheres.%2520This%2520study%2520aims%2520to%2520determine%2520whether%2520machine%2520learning%250A%2528ML%2529%2520algorithms%2520can%2520be%2520used%2520to%2520predict%2520the%25203D%2520temperature%2520and%2520wind%2520structure%2520of%250Aarbitrary%2520tidally-locked%2520gaseous%2520exoplanets%2520in%2520a%2520range%2520of%2520planetary%2520parameters.%250AA%2520new%25203D%2520GCM%2520grid%2520with%252060%2520inflated%2520hot%2520Jupiters%2520orbiting%2520A%252C%2520F%252C%2520G%252C%2520K%252C%2520and%2520M-type%250Ahost%2520stars%2520modelled%2520with%2520Exorad%2520has%2520been%2520introduced.%2520A%2520dense%2520neural%2520network%250A%2528DNN%2529%2520and%2520a%2520decision%2520tree%2520algorithm%2520%2528XGBoost%2529%2520are%2520trained%2520on%2520this%2520grid%2520to%250Apredict%2520local%2520gas%2520temperatures%2520along%2520with%2520horizontal%2520and%2520vertical%2520winds.%2520To%250Aensure%2520the%2520reliability%2520and%2520quality%2520of%2520the%2520ML%2520model%2520predictions%252C%2520WASP-121%2520b%252C%250AHATS-42%2520b%252C%2520NGTS-17%2520b%252C%2520WASP-23%2520b%252C%2520and%2520NGTS-1%2520b-like%2520planets%252C%2520which%2520are%2520all%250Atargets%2520for%2520PLATO%2520observation%252C%2520are%2520selected%2520and%2520modelled%2520with%2520ExoRad%2520and%2520the%250Atwo%2520ML%2520methods%2520as%2520test%2520cases.%2520The%2520DNN%2520predictions%2520for%2520the%2520gas%2520temperatures%2520are%250Ato%2520such%2520a%2520degree%2520that%2520the%2520calculated%2520spectra%2520agree%2520within%252032%2520ppm%2520for%2520all%2520but%250Aone%2520planet%252C%2520for%2520which%2520only%2520one%2520single%2520HCN%2520feature%2520reaches%2520a%2520100%2520ppm%2520difference.%250AThe%2520developed%2520ML%2520emulators%2520can%2520reliably%2520predict%2520the%2520complete%25203D%2520temperature%250Afield%2520of%2520an%2520inflated%2520warm%2520to%2520ultra-hot%2520tidally%2520locked%2520Jupiter%2520around%2520A%2520to%250AM-type%2520host%2520stars.%2520It%2520provides%2520a%2520fast%2520tool%2520to%2520complement%2520and%2520extend%2520traditional%250AGCM%2520grids%2520for%2520exoplanet%2520ensemble%2520studies.%2520The%2520quality%2520of%2520the%2520predictions%2520is%250Asuch%2520that%2520no%2520or%2520minimal%2520effects%2520on%2520the%2520gas%2520phase%2520chemistry%252C%2520hence%2520on%2520the%2520cloud%250Aformation%2520and%2520transmission%2520spectra%252C%2520are%2520to%2520be%2520expected.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20exoplanet%20climate%20modelling%3A%20A%20machine%20learning%20approach%20to%0A%20%20complement%203D%20GCM%20grid%20simulations&entry.906535625=Alexander%20Plaschzug%20and%20Amit%20Reza%20and%20Ludmila%20Carone%20and%20Sebastian%20Gernjak%20and%20Christiane%20Helling&entry.1292438233=%20%20With%20the%20development%20of%20ever-improving%20telescopes%20capable%20of%20observing%0Aexoplanet%20atmospheres%20in%20greater%20detail%20and%20number%2C%20there%20is%20a%20growing%20demand%0Afor%20enhanced%203D%20climate%20models%20to%20support%20and%20help%20interpret%20observational%20data%0Afrom%20space%20missions%20like%20CHEOPS%2C%20TESS%2C%20JWST%2C%20PLATO%2C%20and%20Ariel.%20However%2C%20the%0Acomputationally%20intensive%20and%20time-consuming%20nature%20of%20general%20circulation%0Amodels%20%28GCMs%29%20poses%20significant%20challenges%20in%20simulating%20a%20wide%20range%20of%0Aexoplanetary%20atmospheres.%20This%20study%20aims%20to%20determine%20whether%20machine%20learning%0A%28ML%29%20algorithms%20can%20be%20used%20to%20predict%20the%203D%20temperature%20and%20wind%20structure%20of%0Aarbitrary%20tidally-locked%20gaseous%20exoplanets%20in%20a%20range%20of%20planetary%20parameters.%0AA%20new%203D%20GCM%20grid%20with%2060%20inflated%20hot%20Jupiters%20orbiting%20A%2C%20F%2C%20G%2C%20K%2C%20and%20M-type%0Ahost%20stars%20modelled%20with%20Exorad%20has%20been%20introduced.%20A%20dense%20neural%20network%0A%28DNN%29%20and%20a%20decision%20tree%20algorithm%20%28XGBoost%29%20are%20trained%20on%20this%20grid%20to%0Apredict%20local%20gas%20temperatures%20along%20with%20horizontal%20and%20vertical%20winds.%20To%0Aensure%20the%20reliability%20and%20quality%20of%20the%20ML%20model%20predictions%2C%20WASP-121%20b%2C%0AHATS-42%20b%2C%20NGTS-17%20b%2C%20WASP-23%20b%2C%20and%20NGTS-1%20b-like%20planets%2C%20which%20are%20all%0Atargets%20for%20PLATO%20observation%2C%20are%20selected%20and%20modelled%20with%20ExoRad%20and%20the%0Atwo%20ML%20methods%20as%20test%20cases.%20The%20DNN%20predictions%20for%20the%20gas%20temperatures%20are%0Ato%20such%20a%20degree%20that%20the%20calculated%20spectra%20agree%20within%2032%20ppm%20for%20all%20but%0Aone%20planet%2C%20for%20which%20only%20one%20single%20HCN%20feature%20reaches%20a%20100%20ppm%20difference.%0AThe%20developed%20ML%20emulators%20can%20reliably%20predict%20the%20complete%203D%20temperature%0Afield%20of%20an%20inflated%20warm%20to%20ultra-hot%20tidally%20locked%20Jupiter%20around%20A%20to%0AM-type%20host%20stars.%20It%20provides%20a%20fast%20tool%20to%20complement%20and%20extend%20traditional%0AGCM%20grids%20for%20exoplanet%20ensemble%20studies.%20The%20quality%20of%20the%20predictions%20is%0Asuch%20that%20no%20or%20minimal%20effects%20on%20the%20gas%20phase%20chemistry%2C%20hence%20on%20the%20cloud%0Aformation%20and%20transmission%20spectra%2C%20are%20to%20be%20expected.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10827v1&entry.124074799=Read"},
{"title": "The SET Perceptual Factors Framework: Towards Assured Perception for\n  Autonomous Systems", "author": "Troi Williams", "abstract": "  Future autonomous systems promise significant societal benefits, yet their\ndeployment raises concerns about safety and trustworthiness. A key concern is\nassuring the reliability of robot perception, as perception seeds safe\ndecision-making. Failures in perception are often due to complex yet common\nenvironmental factors and can lead to accidents that erode public trust. To\naddress this concern, we introduce the SET (Self, Environment, and Target)\nPerceptual Factors Framework. We designed the framework to systematically\nanalyze how factors such as weather, occlusion, or sensor limitations\nnegatively impact perception. To achieve this, the framework employs SET State\nTrees to categorize where such factors originate and SET Factor Trees to model\nhow these sources and factors impact perceptual tasks like object detection or\npose estimation. Next, we develop Perceptual Factor Models using both trees to\nquantify the uncertainty for a given task. Our framework aims to promote\nrigorous safety assurances and cultivate greater public understanding and trust\nin autonomous systems by offering a transparent and standardized method for\nidentifying, modeling, and communicating perceptual risks.\n", "link": "http://arxiv.org/abs/2508.10798v1", "date": "2025-08-14", "relevancy": 1.5987, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5458}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5429}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20SET%20Perceptual%20Factors%20Framework%3A%20Towards%20Assured%20Perception%20for%0A%20%20Autonomous%20Systems&body=Title%3A%20The%20SET%20Perceptual%20Factors%20Framework%3A%20Towards%20Assured%20Perception%20for%0A%20%20Autonomous%20Systems%0AAuthor%3A%20Troi%20Williams%0AAbstract%3A%20%20%20Future%20autonomous%20systems%20promise%20significant%20societal%20benefits%2C%20yet%20their%0Adeployment%20raises%20concerns%20about%20safety%20and%20trustworthiness.%20A%20key%20concern%20is%0Aassuring%20the%20reliability%20of%20robot%20perception%2C%20as%20perception%20seeds%20safe%0Adecision-making.%20Failures%20in%20perception%20are%20often%20due%20to%20complex%20yet%20common%0Aenvironmental%20factors%20and%20can%20lead%20to%20accidents%20that%20erode%20public%20trust.%20To%0Aaddress%20this%20concern%2C%20we%20introduce%20the%20SET%20%28Self%2C%20Environment%2C%20and%20Target%29%0APerceptual%20Factors%20Framework.%20We%20designed%20the%20framework%20to%20systematically%0Aanalyze%20how%20factors%20such%20as%20weather%2C%20occlusion%2C%20or%20sensor%20limitations%0Anegatively%20impact%20perception.%20To%20achieve%20this%2C%20the%20framework%20employs%20SET%20State%0ATrees%20to%20categorize%20where%20such%20factors%20originate%20and%20SET%20Factor%20Trees%20to%20model%0Ahow%20these%20sources%20and%20factors%20impact%20perceptual%20tasks%20like%20object%20detection%20or%0Apose%20estimation.%20Next%2C%20we%20develop%20Perceptual%20Factor%20Models%20using%20both%20trees%20to%0Aquantify%20the%20uncertainty%20for%20a%20given%20task.%20Our%20framework%20aims%20to%20promote%0Arigorous%20safety%20assurances%20and%20cultivate%20greater%20public%20understanding%20and%20trust%0Ain%20autonomous%20systems%20by%20offering%20a%20transparent%20and%20standardized%20method%20for%0Aidentifying%2C%20modeling%2C%20and%20communicating%20perceptual%20risks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520SET%2520Perceptual%2520Factors%2520Framework%253A%2520Towards%2520Assured%2520Perception%2520for%250A%2520%2520Autonomous%2520Systems%26entry.906535625%3DTroi%2520Williams%26entry.1292438233%3D%2520%2520Future%2520autonomous%2520systems%2520promise%2520significant%2520societal%2520benefits%252C%2520yet%2520their%250Adeployment%2520raises%2520concerns%2520about%2520safety%2520and%2520trustworthiness.%2520A%2520key%2520concern%2520is%250Aassuring%2520the%2520reliability%2520of%2520robot%2520perception%252C%2520as%2520perception%2520seeds%2520safe%250Adecision-making.%2520Failures%2520in%2520perception%2520are%2520often%2520due%2520to%2520complex%2520yet%2520common%250Aenvironmental%2520factors%2520and%2520can%2520lead%2520to%2520accidents%2520that%2520erode%2520public%2520trust.%2520To%250Aaddress%2520this%2520concern%252C%2520we%2520introduce%2520the%2520SET%2520%2528Self%252C%2520Environment%252C%2520and%2520Target%2529%250APerceptual%2520Factors%2520Framework.%2520We%2520designed%2520the%2520framework%2520to%2520systematically%250Aanalyze%2520how%2520factors%2520such%2520as%2520weather%252C%2520occlusion%252C%2520or%2520sensor%2520limitations%250Anegatively%2520impact%2520perception.%2520To%2520achieve%2520this%252C%2520the%2520framework%2520employs%2520SET%2520State%250ATrees%2520to%2520categorize%2520where%2520such%2520factors%2520originate%2520and%2520SET%2520Factor%2520Trees%2520to%2520model%250Ahow%2520these%2520sources%2520and%2520factors%2520impact%2520perceptual%2520tasks%2520like%2520object%2520detection%2520or%250Apose%2520estimation.%2520Next%252C%2520we%2520develop%2520Perceptual%2520Factor%2520Models%2520using%2520both%2520trees%2520to%250Aquantify%2520the%2520uncertainty%2520for%2520a%2520given%2520task.%2520Our%2520framework%2520aims%2520to%2520promote%250Arigorous%2520safety%2520assurances%2520and%2520cultivate%2520greater%2520public%2520understanding%2520and%2520trust%250Ain%2520autonomous%2520systems%2520by%2520offering%2520a%2520transparent%2520and%2520standardized%2520method%2520for%250Aidentifying%252C%2520modeling%252C%2520and%2520communicating%2520perceptual%2520risks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20SET%20Perceptual%20Factors%20Framework%3A%20Towards%20Assured%20Perception%20for%0A%20%20Autonomous%20Systems&entry.906535625=Troi%20Williams&entry.1292438233=%20%20Future%20autonomous%20systems%20promise%20significant%20societal%20benefits%2C%20yet%20their%0Adeployment%20raises%20concerns%20about%20safety%20and%20trustworthiness.%20A%20key%20concern%20is%0Aassuring%20the%20reliability%20of%20robot%20perception%2C%20as%20perception%20seeds%20safe%0Adecision-making.%20Failures%20in%20perception%20are%20often%20due%20to%20complex%20yet%20common%0Aenvironmental%20factors%20and%20can%20lead%20to%20accidents%20that%20erode%20public%20trust.%20To%0Aaddress%20this%20concern%2C%20we%20introduce%20the%20SET%20%28Self%2C%20Environment%2C%20and%20Target%29%0APerceptual%20Factors%20Framework.%20We%20designed%20the%20framework%20to%20systematically%0Aanalyze%20how%20factors%20such%20as%20weather%2C%20occlusion%2C%20or%20sensor%20limitations%0Anegatively%20impact%20perception.%20To%20achieve%20this%2C%20the%20framework%20employs%20SET%20State%0ATrees%20to%20categorize%20where%20such%20factors%20originate%20and%20SET%20Factor%20Trees%20to%20model%0Ahow%20these%20sources%20and%20factors%20impact%20perceptual%20tasks%20like%20object%20detection%20or%0Apose%20estimation.%20Next%2C%20we%20develop%20Perceptual%20Factor%20Models%20using%20both%20trees%20to%0Aquantify%20the%20uncertainty%20for%20a%20given%20task.%20Our%20framework%20aims%20to%20promote%0Arigorous%20safety%20assurances%20and%20cultivate%20greater%20public%20understanding%20and%20trust%0Ain%20autonomous%20systems%20by%20offering%20a%20transparent%20and%20standardized%20method%20for%0Aidentifying%2C%20modeling%2C%20and%20communicating%20perceptual%20risks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10798v1&entry.124074799=Read"},
{"title": "15,500 Seconds: Lean UAV Classification Using EfficientNet and\n  Lightweight Fine-Tuning", "author": "Andrew P. Berg and Qian Zhang and Mia Y. Wang", "abstract": "  As unmanned aerial vehicles (UAVs) become increasingly prevalent in both\nconsumer and defense applications, the need for reliable, modality-specific\nclassification systems grows in urgency. This paper addresses the challenge of\ndata scarcity in UAV audio classification by expanding on prior work through\nthe integration of pre-trained deep learning models, parameter-efficient\nfine-tuning (PEFT) strategies, and targeted data augmentation techniques. Using\na custom dataset of 3,100 UAV audio clips (15,500 seconds) spanning 31 distinct\ndrone types, we evaluate the performance of transformer-based and convolutional\nneural network (CNN) architectures under various fine-tuning configurations.\nExperiments were conducted with five-fold cross-validation, assessing accuracy,\ntraining efficiency, and robustness. Results show that full fine-tuning of the\nEfficientNet-B0 model with three augmentations achieved the highest validation\naccuracy (95.95), outperforming both the custom CNN and transformer-based\nmodels like AST. These findings suggest that combining lightweight\narchitectures with PEFT and well-chosen augmentations provides an effective\nstrategy for UAV audio classification on limited datasets. Future work will\nextend this framework to multimodal UAV classification using visual and radar\ntelemetry.\n", "link": "http://arxiv.org/abs/2506.11049v4", "date": "2025-08-14", "relevancy": 2.0476, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5132}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5131}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%2015%2C500%20Seconds%3A%20Lean%20UAV%20Classification%20Using%20EfficientNet%20and%0A%20%20Lightweight%20Fine-Tuning&body=Title%3A%2015%2C500%20Seconds%3A%20Lean%20UAV%20Classification%20Using%20EfficientNet%20and%0A%20%20Lightweight%20Fine-Tuning%0AAuthor%3A%20Andrew%20P.%20Berg%20and%20Qian%20Zhang%20and%20Mia%20Y.%20Wang%0AAbstract%3A%20%20%20As%20unmanned%20aerial%20vehicles%20%28UAVs%29%20become%20increasingly%20prevalent%20in%20both%0Aconsumer%20and%20defense%20applications%2C%20the%20need%20for%20reliable%2C%20modality-specific%0Aclassification%20systems%20grows%20in%20urgency.%20This%20paper%20addresses%20the%20challenge%20of%0Adata%20scarcity%20in%20UAV%20audio%20classification%20by%20expanding%20on%20prior%20work%20through%0Athe%20integration%20of%20pre-trained%20deep%20learning%20models%2C%20parameter-efficient%0Afine-tuning%20%28PEFT%29%20strategies%2C%20and%20targeted%20data%20augmentation%20techniques.%20Using%0Aa%20custom%20dataset%20of%203%2C100%20UAV%20audio%20clips%20%2815%2C500%20seconds%29%20spanning%2031%20distinct%0Adrone%20types%2C%20we%20evaluate%20the%20performance%20of%20transformer-based%20and%20convolutional%0Aneural%20network%20%28CNN%29%20architectures%20under%20various%20fine-tuning%20configurations.%0AExperiments%20were%20conducted%20with%20five-fold%20cross-validation%2C%20assessing%20accuracy%2C%0Atraining%20efficiency%2C%20and%20robustness.%20Results%20show%20that%20full%20fine-tuning%20of%20the%0AEfficientNet-B0%20model%20with%20three%20augmentations%20achieved%20the%20highest%20validation%0Aaccuracy%20%2895.95%29%2C%20outperforming%20both%20the%20custom%20CNN%20and%20transformer-based%0Amodels%20like%20AST.%20These%20findings%20suggest%20that%20combining%20lightweight%0Aarchitectures%20with%20PEFT%20and%20well-chosen%20augmentations%20provides%20an%20effective%0Astrategy%20for%20UAV%20audio%20classification%20on%20limited%20datasets.%20Future%20work%20will%0Aextend%20this%20framework%20to%20multimodal%20UAV%20classification%20using%20visual%20and%20radar%0Atelemetry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11049v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D15%252C500%2520Seconds%253A%2520Lean%2520UAV%2520Classification%2520Using%2520EfficientNet%2520and%250A%2520%2520Lightweight%2520Fine-Tuning%26entry.906535625%3DAndrew%2520P.%2520Berg%2520and%2520Qian%2520Zhang%2520and%2520Mia%2520Y.%2520Wang%26entry.1292438233%3D%2520%2520As%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520become%2520increasingly%2520prevalent%2520in%2520both%250Aconsumer%2520and%2520defense%2520applications%252C%2520the%2520need%2520for%2520reliable%252C%2520modality-specific%250Aclassification%2520systems%2520grows%2520in%2520urgency.%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%250Adata%2520scarcity%2520in%2520UAV%2520audio%2520classification%2520by%2520expanding%2520on%2520prior%2520work%2520through%250Athe%2520integration%2520of%2520pre-trained%2520deep%2520learning%2520models%252C%2520parameter-efficient%250Afine-tuning%2520%2528PEFT%2529%2520strategies%252C%2520and%2520targeted%2520data%2520augmentation%2520techniques.%2520Using%250Aa%2520custom%2520dataset%2520of%25203%252C100%2520UAV%2520audio%2520clips%2520%252815%252C500%2520seconds%2529%2520spanning%252031%2520distinct%250Adrone%2520types%252C%2520we%2520evaluate%2520the%2520performance%2520of%2520transformer-based%2520and%2520convolutional%250Aneural%2520network%2520%2528CNN%2529%2520architectures%2520under%2520various%2520fine-tuning%2520configurations.%250AExperiments%2520were%2520conducted%2520with%2520five-fold%2520cross-validation%252C%2520assessing%2520accuracy%252C%250Atraining%2520efficiency%252C%2520and%2520robustness.%2520Results%2520show%2520that%2520full%2520fine-tuning%2520of%2520the%250AEfficientNet-B0%2520model%2520with%2520three%2520augmentations%2520achieved%2520the%2520highest%2520validation%250Aaccuracy%2520%252895.95%2529%252C%2520outperforming%2520both%2520the%2520custom%2520CNN%2520and%2520transformer-based%250Amodels%2520like%2520AST.%2520These%2520findings%2520suggest%2520that%2520combining%2520lightweight%250Aarchitectures%2520with%2520PEFT%2520and%2520well-chosen%2520augmentations%2520provides%2520an%2520effective%250Astrategy%2520for%2520UAV%2520audio%2520classification%2520on%2520limited%2520datasets.%2520Future%2520work%2520will%250Aextend%2520this%2520framework%2520to%2520multimodal%2520UAV%2520classification%2520using%2520visual%2520and%2520radar%250Atelemetry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11049v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=15%2C500%20Seconds%3A%20Lean%20UAV%20Classification%20Using%20EfficientNet%20and%0A%20%20Lightweight%20Fine-Tuning&entry.906535625=Andrew%20P.%20Berg%20and%20Qian%20Zhang%20and%20Mia%20Y.%20Wang&entry.1292438233=%20%20As%20unmanned%20aerial%20vehicles%20%28UAVs%29%20become%20increasingly%20prevalent%20in%20both%0Aconsumer%20and%20defense%20applications%2C%20the%20need%20for%20reliable%2C%20modality-specific%0Aclassification%20systems%20grows%20in%20urgency.%20This%20paper%20addresses%20the%20challenge%20of%0Adata%20scarcity%20in%20UAV%20audio%20classification%20by%20expanding%20on%20prior%20work%20through%0Athe%20integration%20of%20pre-trained%20deep%20learning%20models%2C%20parameter-efficient%0Afine-tuning%20%28PEFT%29%20strategies%2C%20and%20targeted%20data%20augmentation%20techniques.%20Using%0Aa%20custom%20dataset%20of%203%2C100%20UAV%20audio%20clips%20%2815%2C500%20seconds%29%20spanning%2031%20distinct%0Adrone%20types%2C%20we%20evaluate%20the%20performance%20of%20transformer-based%20and%20convolutional%0Aneural%20network%20%28CNN%29%20architectures%20under%20various%20fine-tuning%20configurations.%0AExperiments%20were%20conducted%20with%20five-fold%20cross-validation%2C%20assessing%20accuracy%2C%0Atraining%20efficiency%2C%20and%20robustness.%20Results%20show%20that%20full%20fine-tuning%20of%20the%0AEfficientNet-B0%20model%20with%20three%20augmentations%20achieved%20the%20highest%20validation%0Aaccuracy%20%2895.95%29%2C%20outperforming%20both%20the%20custom%20CNN%20and%20transformer-based%0Amodels%20like%20AST.%20These%20findings%20suggest%20that%20combining%20lightweight%0Aarchitectures%20with%20PEFT%20and%20well-chosen%20augmentations%20provides%20an%20effective%0Astrategy%20for%20UAV%20audio%20classification%20on%20limited%20datasets.%20Future%20work%20will%0Aextend%20this%20framework%20to%20multimodal%20UAV%20classification%20using%20visual%20and%20radar%0Atelemetry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11049v4&entry.124074799=Read"},
{"title": "Variance Reduced Policy Gradient Method for Multi-Objective\n  Reinforcement Learning", "author": "Davide Guidobene and Lorenzo Benedetti and Diego Arapovic", "abstract": "  Multi-Objective Reinforcement Learning (MORL) is a generalization of\ntraditional Reinforcement Learning (RL) that aims to optimize multiple, often\nconflicting objectives simultaneously rather than focusing on a single reward.\nThis approach is crucial in complex decision-making scenarios where agents must\nbalance trade-offs between various goals, such as maximizing performance while\nminimizing costs. We consider the problem of MORL where the objectives are\ncombined using a non-linear scalarization function. Just like in standard RL,\npolicy gradient methods (PGMs) are amongst the most effective for handling\nlarge and continuous state-action spaces in MORL. However, existing PGMs for\nMORL suffer from high sample inefficiency, requiring large amounts of data to\nbe effective. Previous attempts to solve this problem rely on overly strict\nassumptions, losing PGMs' benefits in scalability to large state-action spaces.\nIn this work, we address the issue of sample efficiency by implementing\nvariance-reduction techniques to reduce the sample complexity of policy\ngradients while maintaining general assumptions.\n", "link": "http://arxiv.org/abs/2508.10608v1", "date": "2025-08-14", "relevancy": 1.7468, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4995}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4262}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variance%20Reduced%20Policy%20Gradient%20Method%20for%20Multi-Objective%0A%20%20Reinforcement%20Learning&body=Title%3A%20Variance%20Reduced%20Policy%20Gradient%20Method%20for%20Multi-Objective%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Davide%20Guidobene%20and%20Lorenzo%20Benedetti%20and%20Diego%20Arapovic%0AAbstract%3A%20%20%20Multi-Objective%20Reinforcement%20Learning%20%28MORL%29%20is%20a%20generalization%20of%0Atraditional%20Reinforcement%20Learning%20%28RL%29%20that%20aims%20to%20optimize%20multiple%2C%20often%0Aconflicting%20objectives%20simultaneously%20rather%20than%20focusing%20on%20a%20single%20reward.%0AThis%20approach%20is%20crucial%20in%20complex%20decision-making%20scenarios%20where%20agents%20must%0Abalance%20trade-offs%20between%20various%20goals%2C%20such%20as%20maximizing%20performance%20while%0Aminimizing%20costs.%20We%20consider%20the%20problem%20of%20MORL%20where%20the%20objectives%20are%0Acombined%20using%20a%20non-linear%20scalarization%20function.%20Just%20like%20in%20standard%20RL%2C%0Apolicy%20gradient%20methods%20%28PGMs%29%20are%20amongst%20the%20most%20effective%20for%20handling%0Alarge%20and%20continuous%20state-action%20spaces%20in%20MORL.%20However%2C%20existing%20PGMs%20for%0AMORL%20suffer%20from%20high%20sample%20inefficiency%2C%20requiring%20large%20amounts%20of%20data%20to%0Abe%20effective.%20Previous%20attempts%20to%20solve%20this%20problem%20rely%20on%20overly%20strict%0Aassumptions%2C%20losing%20PGMs%27%20benefits%20in%20scalability%20to%20large%20state-action%20spaces.%0AIn%20this%20work%2C%20we%20address%20the%20issue%20of%20sample%20efficiency%20by%20implementing%0Avariance-reduction%20techniques%20to%20reduce%20the%20sample%20complexity%20of%20policy%0Agradients%20while%20maintaining%20general%20assumptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariance%2520Reduced%2520Policy%2520Gradient%2520Method%2520for%2520Multi-Objective%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DDavide%2520Guidobene%2520and%2520Lorenzo%2520Benedetti%2520and%2520Diego%2520Arapovic%26entry.1292438233%3D%2520%2520Multi-Objective%2520Reinforcement%2520Learning%2520%2528MORL%2529%2520is%2520a%2520generalization%2520of%250Atraditional%2520Reinforcement%2520Learning%2520%2528RL%2529%2520that%2520aims%2520to%2520optimize%2520multiple%252C%2520often%250Aconflicting%2520objectives%2520simultaneously%2520rather%2520than%2520focusing%2520on%2520a%2520single%2520reward.%250AThis%2520approach%2520is%2520crucial%2520in%2520complex%2520decision-making%2520scenarios%2520where%2520agents%2520must%250Abalance%2520trade-offs%2520between%2520various%2520goals%252C%2520such%2520as%2520maximizing%2520performance%2520while%250Aminimizing%2520costs.%2520We%2520consider%2520the%2520problem%2520of%2520MORL%2520where%2520the%2520objectives%2520are%250Acombined%2520using%2520a%2520non-linear%2520scalarization%2520function.%2520Just%2520like%2520in%2520standard%2520RL%252C%250Apolicy%2520gradient%2520methods%2520%2528PGMs%2529%2520are%2520amongst%2520the%2520most%2520effective%2520for%2520handling%250Alarge%2520and%2520continuous%2520state-action%2520spaces%2520in%2520MORL.%2520However%252C%2520existing%2520PGMs%2520for%250AMORL%2520suffer%2520from%2520high%2520sample%2520inefficiency%252C%2520requiring%2520large%2520amounts%2520of%2520data%2520to%250Abe%2520effective.%2520Previous%2520attempts%2520to%2520solve%2520this%2520problem%2520rely%2520on%2520overly%2520strict%250Aassumptions%252C%2520losing%2520PGMs%2527%2520benefits%2520in%2520scalability%2520to%2520large%2520state-action%2520spaces.%250AIn%2520this%2520work%252C%2520we%2520address%2520the%2520issue%2520of%2520sample%2520efficiency%2520by%2520implementing%250Avariance-reduction%2520techniques%2520to%2520reduce%2520the%2520sample%2520complexity%2520of%2520policy%250Agradients%2520while%2520maintaining%2520general%2520assumptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variance%20Reduced%20Policy%20Gradient%20Method%20for%20Multi-Objective%0A%20%20Reinforcement%20Learning&entry.906535625=Davide%20Guidobene%20and%20Lorenzo%20Benedetti%20and%20Diego%20Arapovic&entry.1292438233=%20%20Multi-Objective%20Reinforcement%20Learning%20%28MORL%29%20is%20a%20generalization%20of%0Atraditional%20Reinforcement%20Learning%20%28RL%29%20that%20aims%20to%20optimize%20multiple%2C%20often%0Aconflicting%20objectives%20simultaneously%20rather%20than%20focusing%20on%20a%20single%20reward.%0AThis%20approach%20is%20crucial%20in%20complex%20decision-making%20scenarios%20where%20agents%20must%0Abalance%20trade-offs%20between%20various%20goals%2C%20such%20as%20maximizing%20performance%20while%0Aminimizing%20costs.%20We%20consider%20the%20problem%20of%20MORL%20where%20the%20objectives%20are%0Acombined%20using%20a%20non-linear%20scalarization%20function.%20Just%20like%20in%20standard%20RL%2C%0Apolicy%20gradient%20methods%20%28PGMs%29%20are%20amongst%20the%20most%20effective%20for%20handling%0Alarge%20and%20continuous%20state-action%20spaces%20in%20MORL.%20However%2C%20existing%20PGMs%20for%0AMORL%20suffer%20from%20high%20sample%20inefficiency%2C%20requiring%20large%20amounts%20of%20data%20to%0Abe%20effective.%20Previous%20attempts%20to%20solve%20this%20problem%20rely%20on%20overly%20strict%0Aassumptions%2C%20losing%20PGMs%27%20benefits%20in%20scalability%20to%20large%20state-action%20spaces.%0AIn%20this%20work%2C%20we%20address%20the%20issue%20of%20sample%20efficiency%20by%20implementing%0Avariance-reduction%20techniques%20to%20reduce%20the%20sample%20complexity%20of%20policy%0Agradients%20while%20maintaining%20general%20assumptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10608v1&entry.124074799=Read"},
{"title": "SoK: Data Minimization in Machine Learning", "author": "Robin Staab and Nikola Jovanovi\u0107 and Kimberly Mai and Prakhar Ganesh and Martin Vechev and Ferdinando Fioretto and Matthew Jagielski", "abstract": "  Data minimization (DM) describes the principle of collecting only the data\nstrictly necessary for a given task. It is a foundational principle across\nmajor data protection regulations like GDPR and CPRA. Violations of this\nprinciple have substantial real-world consequences, with regulatory actions\nresulting in fines reaching hundreds of millions of dollars. Notably, the\nrelevance of data minimization is particularly pronounced in machine learning\n(ML) applications, which typically rely on large datasets, resulting in an\nemerging research area known as Data Minimization in Machine Learning (DMML).\nAt the same time, existing work on other ML privacy and security topics often\naddresses concerns relevant to DMML without explicitly acknowledging the\nconnection. This disconnect leads to confusion among practitioners,\ncomplicating their efforts to implement DM principles and interpret the\nterminology, metrics, and evaluation criteria used across different research\ncommunities. To address this gap, our work introduces a comprehensive framework\nfor DMML, including a unified data pipeline, adversaries, and points of\nminimization. This framework allows us to systematically review the literature\non data minimization and \\emph{DM-adjacent} methodologies, for the first time\npresenting a structured overview designed to help practitioners and researchers\neffectively apply DM principles. Our work facilitates a unified DM-centric\nunderstanding and broader adoption of data minimization strategies in AI/ML.\n", "link": "http://arxiv.org/abs/2508.10836v1", "date": "2025-08-14", "relevancy": 1.3348, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4547}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4434}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoK%3A%20Data%20Minimization%20in%20Machine%20Learning&body=Title%3A%20SoK%3A%20Data%20Minimization%20in%20Machine%20Learning%0AAuthor%3A%20Robin%20Staab%20and%20Nikola%20Jovanovi%C4%87%20and%20Kimberly%20Mai%20and%20Prakhar%20Ganesh%20and%20Martin%20Vechev%20and%20Ferdinando%20Fioretto%20and%20Matthew%20Jagielski%0AAbstract%3A%20%20%20Data%20minimization%20%28DM%29%20describes%20the%20principle%20of%20collecting%20only%20the%20data%0Astrictly%20necessary%20for%20a%20given%20task.%20It%20is%20a%20foundational%20principle%20across%0Amajor%20data%20protection%20regulations%20like%20GDPR%20and%20CPRA.%20Violations%20of%20this%0Aprinciple%20have%20substantial%20real-world%20consequences%2C%20with%20regulatory%20actions%0Aresulting%20in%20fines%20reaching%20hundreds%20of%20millions%20of%20dollars.%20Notably%2C%20the%0Arelevance%20of%20data%20minimization%20is%20particularly%20pronounced%20in%20machine%20learning%0A%28ML%29%20applications%2C%20which%20typically%20rely%20on%20large%20datasets%2C%20resulting%20in%20an%0Aemerging%20research%20area%20known%20as%20Data%20Minimization%20in%20Machine%20Learning%20%28DMML%29.%0AAt%20the%20same%20time%2C%20existing%20work%20on%20other%20ML%20privacy%20and%20security%20topics%20often%0Aaddresses%20concerns%20relevant%20to%20DMML%20without%20explicitly%20acknowledging%20the%0Aconnection.%20This%20disconnect%20leads%20to%20confusion%20among%20practitioners%2C%0Acomplicating%20their%20efforts%20to%20implement%20DM%20principles%20and%20interpret%20the%0Aterminology%2C%20metrics%2C%20and%20evaluation%20criteria%20used%20across%20different%20research%0Acommunities.%20To%20address%20this%20gap%2C%20our%20work%20introduces%20a%20comprehensive%20framework%0Afor%20DMML%2C%20including%20a%20unified%20data%20pipeline%2C%20adversaries%2C%20and%20points%20of%0Aminimization.%20This%20framework%20allows%20us%20to%20systematically%20review%20the%20literature%0Aon%20data%20minimization%20and%20%5Cemph%7BDM-adjacent%7D%20methodologies%2C%20for%20the%20first%20time%0Apresenting%20a%20structured%20overview%20designed%20to%20help%20practitioners%20and%20researchers%0Aeffectively%20apply%20DM%20principles.%20Our%20work%20facilitates%20a%20unified%20DM-centric%0Aunderstanding%20and%20broader%20adoption%20of%20data%20minimization%20strategies%20in%20AI/ML.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoK%253A%2520Data%2520Minimization%2520in%2520Machine%2520Learning%26entry.906535625%3DRobin%2520Staab%2520and%2520Nikola%2520Jovanovi%25C4%2587%2520and%2520Kimberly%2520Mai%2520and%2520Prakhar%2520Ganesh%2520and%2520Martin%2520Vechev%2520and%2520Ferdinando%2520Fioretto%2520and%2520Matthew%2520Jagielski%26entry.1292438233%3D%2520%2520Data%2520minimization%2520%2528DM%2529%2520describes%2520the%2520principle%2520of%2520collecting%2520only%2520the%2520data%250Astrictly%2520necessary%2520for%2520a%2520given%2520task.%2520It%2520is%2520a%2520foundational%2520principle%2520across%250Amajor%2520data%2520protection%2520regulations%2520like%2520GDPR%2520and%2520CPRA.%2520Violations%2520of%2520this%250Aprinciple%2520have%2520substantial%2520real-world%2520consequences%252C%2520with%2520regulatory%2520actions%250Aresulting%2520in%2520fines%2520reaching%2520hundreds%2520of%2520millions%2520of%2520dollars.%2520Notably%252C%2520the%250Arelevance%2520of%2520data%2520minimization%2520is%2520particularly%2520pronounced%2520in%2520machine%2520learning%250A%2528ML%2529%2520applications%252C%2520which%2520typically%2520rely%2520on%2520large%2520datasets%252C%2520resulting%2520in%2520an%250Aemerging%2520research%2520area%2520known%2520as%2520Data%2520Minimization%2520in%2520Machine%2520Learning%2520%2528DMML%2529.%250AAt%2520the%2520same%2520time%252C%2520existing%2520work%2520on%2520other%2520ML%2520privacy%2520and%2520security%2520topics%2520often%250Aaddresses%2520concerns%2520relevant%2520to%2520DMML%2520without%2520explicitly%2520acknowledging%2520the%250Aconnection.%2520This%2520disconnect%2520leads%2520to%2520confusion%2520among%2520practitioners%252C%250Acomplicating%2520their%2520efforts%2520to%2520implement%2520DM%2520principles%2520and%2520interpret%2520the%250Aterminology%252C%2520metrics%252C%2520and%2520evaluation%2520criteria%2520used%2520across%2520different%2520research%250Acommunities.%2520To%2520address%2520this%2520gap%252C%2520our%2520work%2520introduces%2520a%2520comprehensive%2520framework%250Afor%2520DMML%252C%2520including%2520a%2520unified%2520data%2520pipeline%252C%2520adversaries%252C%2520and%2520points%2520of%250Aminimization.%2520This%2520framework%2520allows%2520us%2520to%2520systematically%2520review%2520the%2520literature%250Aon%2520data%2520minimization%2520and%2520%255Cemph%257BDM-adjacent%257D%2520methodologies%252C%2520for%2520the%2520first%2520time%250Apresenting%2520a%2520structured%2520overview%2520designed%2520to%2520help%2520practitioners%2520and%2520researchers%250Aeffectively%2520apply%2520DM%2520principles.%2520Our%2520work%2520facilitates%2520a%2520unified%2520DM-centric%250Aunderstanding%2520and%2520broader%2520adoption%2520of%2520data%2520minimization%2520strategies%2520in%2520AI/ML.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoK%3A%20Data%20Minimization%20in%20Machine%20Learning&entry.906535625=Robin%20Staab%20and%20Nikola%20Jovanovi%C4%87%20and%20Kimberly%20Mai%20and%20Prakhar%20Ganesh%20and%20Martin%20Vechev%20and%20Ferdinando%20Fioretto%20and%20Matthew%20Jagielski&entry.1292438233=%20%20Data%20minimization%20%28DM%29%20describes%20the%20principle%20of%20collecting%20only%20the%20data%0Astrictly%20necessary%20for%20a%20given%20task.%20It%20is%20a%20foundational%20principle%20across%0Amajor%20data%20protection%20regulations%20like%20GDPR%20and%20CPRA.%20Violations%20of%20this%0Aprinciple%20have%20substantial%20real-world%20consequences%2C%20with%20regulatory%20actions%0Aresulting%20in%20fines%20reaching%20hundreds%20of%20millions%20of%20dollars.%20Notably%2C%20the%0Arelevance%20of%20data%20minimization%20is%20particularly%20pronounced%20in%20machine%20learning%0A%28ML%29%20applications%2C%20which%20typically%20rely%20on%20large%20datasets%2C%20resulting%20in%20an%0Aemerging%20research%20area%20known%20as%20Data%20Minimization%20in%20Machine%20Learning%20%28DMML%29.%0AAt%20the%20same%20time%2C%20existing%20work%20on%20other%20ML%20privacy%20and%20security%20topics%20often%0Aaddresses%20concerns%20relevant%20to%20DMML%20without%20explicitly%20acknowledging%20the%0Aconnection.%20This%20disconnect%20leads%20to%20confusion%20among%20practitioners%2C%0Acomplicating%20their%20efforts%20to%20implement%20DM%20principles%20and%20interpret%20the%0Aterminology%2C%20metrics%2C%20and%20evaluation%20criteria%20used%20across%20different%20research%0Acommunities.%20To%20address%20this%20gap%2C%20our%20work%20introduces%20a%20comprehensive%20framework%0Afor%20DMML%2C%20including%20a%20unified%20data%20pipeline%2C%20adversaries%2C%20and%20points%20of%0Aminimization.%20This%20framework%20allows%20us%20to%20systematically%20review%20the%20literature%0Aon%20data%20minimization%20and%20%5Cemph%7BDM-adjacent%7D%20methodologies%2C%20for%20the%20first%20time%0Apresenting%20a%20structured%20overview%20designed%20to%20help%20practitioners%20and%20researchers%0Aeffectively%20apply%20DM%20principles.%20Our%20work%20facilitates%20a%20unified%20DM-centric%0Aunderstanding%20and%20broader%20adoption%20of%20data%20minimization%20strategies%20in%20AI/ML.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10836v1&entry.124074799=Read"},
{"title": "RobustDexGrasp: Robust Dexterous Grasping of General Objects", "author": "Hui Zhang and Zijian Wu and Linyi Huang and Sammy Christen and Jie Song", "abstract": "  The ability to robustly grasp a variety of objects is essential for dexterous\nrobots. In this paper, we present a framework for zero-shot dynamic dexterous\ngrasping using single-view visual inputs, designed to be resilient to various\ndisturbances. Our approach utilizes a hand-centric object shape representation\nbased on dynamic distance vectors between finger joints and object surfaces.\nThis representation captures the local shape around potential contact regions\nrather than focusing on detailed global object geometry, thereby enhancing\ngeneralization to shape variations and uncertainties. To address perception\nlimitations, we integrate a privileged teacher policy with a mixed curriculum\nlearning approach, allowing the student policy to effectively distill grasping\ncapabilities and explore for adaptation to disturbances. Trained in simulation,\nour method achieves success rates of 97.0% across 247,786 simulated objects and\n94.6% across 512 real objects, demonstrating remarkable generalization.\nQuantitative and qualitative results validate the robustness of our policy\nagainst various disturbances.\n", "link": "http://arxiv.org/abs/2504.05287v3", "date": "2025-08-14", "relevancy": 1.9349, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6991}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6055}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RobustDexGrasp%3A%20Robust%20Dexterous%20Grasping%20of%20General%20Objects&body=Title%3A%20RobustDexGrasp%3A%20Robust%20Dexterous%20Grasping%20of%20General%20Objects%0AAuthor%3A%20Hui%20Zhang%20and%20Zijian%20Wu%20and%20Linyi%20Huang%20and%20Sammy%20Christen%20and%20Jie%20Song%0AAbstract%3A%20%20%20The%20ability%20to%20robustly%20grasp%20a%20variety%20of%20objects%20is%20essential%20for%20dexterous%0Arobots.%20In%20this%20paper%2C%20we%20present%20a%20framework%20for%20zero-shot%20dynamic%20dexterous%0Agrasping%20using%20single-view%20visual%20inputs%2C%20designed%20to%20be%20resilient%20to%20various%0Adisturbances.%20Our%20approach%20utilizes%20a%20hand-centric%20object%20shape%20representation%0Abased%20on%20dynamic%20distance%20vectors%20between%20finger%20joints%20and%20object%20surfaces.%0AThis%20representation%20captures%20the%20local%20shape%20around%20potential%20contact%20regions%0Arather%20than%20focusing%20on%20detailed%20global%20object%20geometry%2C%20thereby%20enhancing%0Ageneralization%20to%20shape%20variations%20and%20uncertainties.%20To%20address%20perception%0Alimitations%2C%20we%20integrate%20a%20privileged%20teacher%20policy%20with%20a%20mixed%20curriculum%0Alearning%20approach%2C%20allowing%20the%20student%20policy%20to%20effectively%20distill%20grasping%0Acapabilities%20and%20explore%20for%20adaptation%20to%20disturbances.%20Trained%20in%20simulation%2C%0Aour%20method%20achieves%20success%20rates%20of%2097.0%25%20across%20247%2C786%20simulated%20objects%20and%0A94.6%25%20across%20512%20real%20objects%2C%20demonstrating%20remarkable%20generalization.%0AQuantitative%20and%20qualitative%20results%20validate%20the%20robustness%20of%20our%20policy%0Aagainst%20various%20disturbances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.05287v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustDexGrasp%253A%2520Robust%2520Dexterous%2520Grasping%2520of%2520General%2520Objects%26entry.906535625%3DHui%2520Zhang%2520and%2520Zijian%2520Wu%2520and%2520Linyi%2520Huang%2520and%2520Sammy%2520Christen%2520and%2520Jie%2520Song%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520robustly%2520grasp%2520a%2520variety%2520of%2520objects%2520is%2520essential%2520for%2520dexterous%250Arobots.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520framework%2520for%2520zero-shot%2520dynamic%2520dexterous%250Agrasping%2520using%2520single-view%2520visual%2520inputs%252C%2520designed%2520to%2520be%2520resilient%2520to%2520various%250Adisturbances.%2520Our%2520approach%2520utilizes%2520a%2520hand-centric%2520object%2520shape%2520representation%250Abased%2520on%2520dynamic%2520distance%2520vectors%2520between%2520finger%2520joints%2520and%2520object%2520surfaces.%250AThis%2520representation%2520captures%2520the%2520local%2520shape%2520around%2520potential%2520contact%2520regions%250Arather%2520than%2520focusing%2520on%2520detailed%2520global%2520object%2520geometry%252C%2520thereby%2520enhancing%250Ageneralization%2520to%2520shape%2520variations%2520and%2520uncertainties.%2520To%2520address%2520perception%250Alimitations%252C%2520we%2520integrate%2520a%2520privileged%2520teacher%2520policy%2520with%2520a%2520mixed%2520curriculum%250Alearning%2520approach%252C%2520allowing%2520the%2520student%2520policy%2520to%2520effectively%2520distill%2520grasping%250Acapabilities%2520and%2520explore%2520for%2520adaptation%2520to%2520disturbances.%2520Trained%2520in%2520simulation%252C%250Aour%2520method%2520achieves%2520success%2520rates%2520of%252097.0%2525%2520across%2520247%252C786%2520simulated%2520objects%2520and%250A94.6%2525%2520across%2520512%2520real%2520objects%252C%2520demonstrating%2520remarkable%2520generalization.%250AQuantitative%2520and%2520qualitative%2520results%2520validate%2520the%2520robustness%2520of%2520our%2520policy%250Aagainst%2520various%2520disturbances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.05287v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RobustDexGrasp%3A%20Robust%20Dexterous%20Grasping%20of%20General%20Objects&entry.906535625=Hui%20Zhang%20and%20Zijian%20Wu%20and%20Linyi%20Huang%20and%20Sammy%20Christen%20and%20Jie%20Song&entry.1292438233=%20%20The%20ability%20to%20robustly%20grasp%20a%20variety%20of%20objects%20is%20essential%20for%20dexterous%0Arobots.%20In%20this%20paper%2C%20we%20present%20a%20framework%20for%20zero-shot%20dynamic%20dexterous%0Agrasping%20using%20single-view%20visual%20inputs%2C%20designed%20to%20be%20resilient%20to%20various%0Adisturbances.%20Our%20approach%20utilizes%20a%20hand-centric%20object%20shape%20representation%0Abased%20on%20dynamic%20distance%20vectors%20between%20finger%20joints%20and%20object%20surfaces.%0AThis%20representation%20captures%20the%20local%20shape%20around%20potential%20contact%20regions%0Arather%20than%20focusing%20on%20detailed%20global%20object%20geometry%2C%20thereby%20enhancing%0Ageneralization%20to%20shape%20variations%20and%20uncertainties.%20To%20address%20perception%0Alimitations%2C%20we%20integrate%20a%20privileged%20teacher%20policy%20with%20a%20mixed%20curriculum%0Alearning%20approach%2C%20allowing%20the%20student%20policy%20to%20effectively%20distill%20grasping%0Acapabilities%20and%20explore%20for%20adaptation%20to%20disturbances.%20Trained%20in%20simulation%2C%0Aour%20method%20achieves%20success%20rates%20of%2097.0%25%20across%20247%2C786%20simulated%20objects%20and%0A94.6%25%20across%20512%20real%20objects%2C%20demonstrating%20remarkable%20generalization.%0AQuantitative%20and%20qualitative%20results%20validate%20the%20robustness%20of%20our%20policy%0Aagainst%20various%20disturbances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.05287v3&entry.124074799=Read"},
{"title": "An Open-Source User-Friendly Interface for Simulating Magnetic Soft\n  Robots using Simulation Open Framework Architecture (SOFA)", "author": "Carla Wehner and Finn Schubert and Heiko Hellkamp and Julius Hahnewald and Kilian Scheafer and Muhammad Bilal Khan and Oliver Gutfleisch", "abstract": "  Soft robots, particularly magnetic soft robots, require specialized\nsimulation tools to accurately model their deformation under external magnetic\nfields. However, existing platforms often lack dedicated support for magnetic\nmaterials, making them difficult to use for researchers at different expertise\nlevels. This work introduces an open-source, user-friendly simulation interface\nusing the Simulation Open Framework Architecture (SOFA), specifically designed\nto model magnetic soft robots. The tool enables users to define material\nproperties, apply magnetic fields, and observe resulting deformations in real\ntime. By integrating intuitive controls and stress analysis capabilities, it\naims to bridge the gap between theoretical modeling and practical design. Four\nbenchmark models - a beam, three- and four-finger grippers, and a butterfly -\ndemonstrate its functionality. The software's ease of use makes it accessible\nto both beginners and advanced researchers. Future improvements will refine\naccuracy through experimental validation and comparison with industry-standard\nfinite element solvers, ensuring realistic and predictive simulations of\nmagnetic soft robots.\n", "link": "http://arxiv.org/abs/2508.10686v1", "date": "2025-08-14", "relevancy": 1.4131, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4946}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.494}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Open-Source%20User-Friendly%20Interface%20for%20Simulating%20Magnetic%20Soft%0A%20%20Robots%20using%20Simulation%20Open%20Framework%20Architecture%20%28SOFA%29&body=Title%3A%20An%20Open-Source%20User-Friendly%20Interface%20for%20Simulating%20Magnetic%20Soft%0A%20%20Robots%20using%20Simulation%20Open%20Framework%20Architecture%20%28SOFA%29%0AAuthor%3A%20Carla%20Wehner%20and%20Finn%20Schubert%20and%20Heiko%20Hellkamp%20and%20Julius%20Hahnewald%20and%20Kilian%20Scheafer%20and%20Muhammad%20Bilal%20Khan%20and%20Oliver%20Gutfleisch%0AAbstract%3A%20%20%20Soft%20robots%2C%20particularly%20magnetic%20soft%20robots%2C%20require%20specialized%0Asimulation%20tools%20to%20accurately%20model%20their%20deformation%20under%20external%20magnetic%0Afields.%20However%2C%20existing%20platforms%20often%20lack%20dedicated%20support%20for%20magnetic%0Amaterials%2C%20making%20them%20difficult%20to%20use%20for%20researchers%20at%20different%20expertise%0Alevels.%20This%20work%20introduces%20an%20open-source%2C%20user-friendly%20simulation%20interface%0Ausing%20the%20Simulation%20Open%20Framework%20Architecture%20%28SOFA%29%2C%20specifically%20designed%0Ato%20model%20magnetic%20soft%20robots.%20The%20tool%20enables%20users%20to%20define%20material%0Aproperties%2C%20apply%20magnetic%20fields%2C%20and%20observe%20resulting%20deformations%20in%20real%0Atime.%20By%20integrating%20intuitive%20controls%20and%20stress%20analysis%20capabilities%2C%20it%0Aaims%20to%20bridge%20the%20gap%20between%20theoretical%20modeling%20and%20practical%20design.%20Four%0Abenchmark%20models%20-%20a%20beam%2C%20three-%20and%20four-finger%20grippers%2C%20and%20a%20butterfly%20-%0Ademonstrate%20its%20functionality.%20The%20software%27s%20ease%20of%20use%20makes%20it%20accessible%0Ato%20both%20beginners%20and%20advanced%20researchers.%20Future%20improvements%20will%20refine%0Aaccuracy%20through%20experimental%20validation%20and%20comparison%20with%20industry-standard%0Afinite%20element%20solvers%2C%20ensuring%20realistic%20and%20predictive%20simulations%20of%0Amagnetic%20soft%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Open-Source%2520User-Friendly%2520Interface%2520for%2520Simulating%2520Magnetic%2520Soft%250A%2520%2520Robots%2520using%2520Simulation%2520Open%2520Framework%2520Architecture%2520%2528SOFA%2529%26entry.906535625%3DCarla%2520Wehner%2520and%2520Finn%2520Schubert%2520and%2520Heiko%2520Hellkamp%2520and%2520Julius%2520Hahnewald%2520and%2520Kilian%2520Scheafer%2520and%2520Muhammad%2520Bilal%2520Khan%2520and%2520Oliver%2520Gutfleisch%26entry.1292438233%3D%2520%2520Soft%2520robots%252C%2520particularly%2520magnetic%2520soft%2520robots%252C%2520require%2520specialized%250Asimulation%2520tools%2520to%2520accurately%2520model%2520their%2520deformation%2520under%2520external%2520magnetic%250Afields.%2520However%252C%2520existing%2520platforms%2520often%2520lack%2520dedicated%2520support%2520for%2520magnetic%250Amaterials%252C%2520making%2520them%2520difficult%2520to%2520use%2520for%2520researchers%2520at%2520different%2520expertise%250Alevels.%2520This%2520work%2520introduces%2520an%2520open-source%252C%2520user-friendly%2520simulation%2520interface%250Ausing%2520the%2520Simulation%2520Open%2520Framework%2520Architecture%2520%2528SOFA%2529%252C%2520specifically%2520designed%250Ato%2520model%2520magnetic%2520soft%2520robots.%2520The%2520tool%2520enables%2520users%2520to%2520define%2520material%250Aproperties%252C%2520apply%2520magnetic%2520fields%252C%2520and%2520observe%2520resulting%2520deformations%2520in%2520real%250Atime.%2520By%2520integrating%2520intuitive%2520controls%2520and%2520stress%2520analysis%2520capabilities%252C%2520it%250Aaims%2520to%2520bridge%2520the%2520gap%2520between%2520theoretical%2520modeling%2520and%2520practical%2520design.%2520Four%250Abenchmark%2520models%2520-%2520a%2520beam%252C%2520three-%2520and%2520four-finger%2520grippers%252C%2520and%2520a%2520butterfly%2520-%250Ademonstrate%2520its%2520functionality.%2520The%2520software%2527s%2520ease%2520of%2520use%2520makes%2520it%2520accessible%250Ato%2520both%2520beginners%2520and%2520advanced%2520researchers.%2520Future%2520improvements%2520will%2520refine%250Aaccuracy%2520through%2520experimental%2520validation%2520and%2520comparison%2520with%2520industry-standard%250Afinite%2520element%2520solvers%252C%2520ensuring%2520realistic%2520and%2520predictive%2520simulations%2520of%250Amagnetic%2520soft%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Open-Source%20User-Friendly%20Interface%20for%20Simulating%20Magnetic%20Soft%0A%20%20Robots%20using%20Simulation%20Open%20Framework%20Architecture%20%28SOFA%29&entry.906535625=Carla%20Wehner%20and%20Finn%20Schubert%20and%20Heiko%20Hellkamp%20and%20Julius%20Hahnewald%20and%20Kilian%20Scheafer%20and%20Muhammad%20Bilal%20Khan%20and%20Oliver%20Gutfleisch&entry.1292438233=%20%20Soft%20robots%2C%20particularly%20magnetic%20soft%20robots%2C%20require%20specialized%0Asimulation%20tools%20to%20accurately%20model%20their%20deformation%20under%20external%20magnetic%0Afields.%20However%2C%20existing%20platforms%20often%20lack%20dedicated%20support%20for%20magnetic%0Amaterials%2C%20making%20them%20difficult%20to%20use%20for%20researchers%20at%20different%20expertise%0Alevels.%20This%20work%20introduces%20an%20open-source%2C%20user-friendly%20simulation%20interface%0Ausing%20the%20Simulation%20Open%20Framework%20Architecture%20%28SOFA%29%2C%20specifically%20designed%0Ato%20model%20magnetic%20soft%20robots.%20The%20tool%20enables%20users%20to%20define%20material%0Aproperties%2C%20apply%20magnetic%20fields%2C%20and%20observe%20resulting%20deformations%20in%20real%0Atime.%20By%20integrating%20intuitive%20controls%20and%20stress%20analysis%20capabilities%2C%20it%0Aaims%20to%20bridge%20the%20gap%20between%20theoretical%20modeling%20and%20practical%20design.%20Four%0Abenchmark%20models%20-%20a%20beam%2C%20three-%20and%20four-finger%20grippers%2C%20and%20a%20butterfly%20-%0Ademonstrate%20its%20functionality.%20The%20software%27s%20ease%20of%20use%20makes%20it%20accessible%0Ato%20both%20beginners%20and%20advanced%20researchers.%20Future%20improvements%20will%20refine%0Aaccuracy%20through%20experimental%20validation%20and%20comparison%20with%20industry-standard%0Afinite%20element%20solvers%2C%20ensuring%20realistic%20and%20predictive%20simulations%20of%0Amagnetic%20soft%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10686v1&entry.124074799=Read"},
{"title": "Parity Cross-Resonance: A Multiqubit Gate", "author": "Xuexin Xu and Siyu Wang and Radhika Joshi and Rihan Hai and Mohammad H. Ansari", "abstract": "  We present a native three-qubit entangling gate that exploits engineered\ninteractions to realize control-control-target and control-target-target\noperations in a single coherent step. Unlike conventional decompositions into\nmultiple two-qubit gates, our hybrid optimization approach selectively\namplifies desired interactions while suppressing unwanted couplings, yielding\nrobust performance across the computational subspace and beyond. The new gate\ncan be classified as a cross-resonance gate. We show it can be utilized in\nseveral ways, for example, in GHZ triplet state preparation, Toffoli-class\nlogic demonstrations with many-body interactions, and in implementing a\ncontrolled-ZZ gate. The latter maps the parity of two data qubits directly onto\na measurement qubit, enabling faster and higher-fidelity stabilizer\nmeasurements in surface-code quantum error correction. In all these examples,\nwe show that the three-qubit gate performance remains robust across Hilbert\nspace sizes, as confirmed by testing under increasing total excitation numbers.\nThis work lays the foundation for co-designing circuit architectures and\ncontrol protocols that leverage native multiqubit interactions as core elements\nof next-generation superconducting quantum processors.\n", "link": "http://arxiv.org/abs/2508.10807v1", "date": "2025-08-14", "relevancy": 1.4711, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3823}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.3711}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parity%20Cross-Resonance%3A%20A%20Multiqubit%20Gate&body=Title%3A%20Parity%20Cross-Resonance%3A%20A%20Multiqubit%20Gate%0AAuthor%3A%20Xuexin%20Xu%20and%20Siyu%20Wang%20and%20Radhika%20Joshi%20and%20Rihan%20Hai%20and%20Mohammad%20H.%20Ansari%0AAbstract%3A%20%20%20We%20present%20a%20native%20three-qubit%20entangling%20gate%20that%20exploits%20engineered%0Ainteractions%20to%20realize%20control-control-target%20and%20control-target-target%0Aoperations%20in%20a%20single%20coherent%20step.%20Unlike%20conventional%20decompositions%20into%0Amultiple%20two-qubit%20gates%2C%20our%20hybrid%20optimization%20approach%20selectively%0Aamplifies%20desired%20interactions%20while%20suppressing%20unwanted%20couplings%2C%20yielding%0Arobust%20performance%20across%20the%20computational%20subspace%20and%20beyond.%20The%20new%20gate%0Acan%20be%20classified%20as%20a%20cross-resonance%20gate.%20We%20show%20it%20can%20be%20utilized%20in%0Aseveral%20ways%2C%20for%20example%2C%20in%20GHZ%20triplet%20state%20preparation%2C%20Toffoli-class%0Alogic%20demonstrations%20with%20many-body%20interactions%2C%20and%20in%20implementing%20a%0Acontrolled-ZZ%20gate.%20The%20latter%20maps%20the%20parity%20of%20two%20data%20qubits%20directly%20onto%0Aa%20measurement%20qubit%2C%20enabling%20faster%20and%20higher-fidelity%20stabilizer%0Ameasurements%20in%20surface-code%20quantum%20error%20correction.%20In%20all%20these%20examples%2C%0Awe%20show%20that%20the%20three-qubit%20gate%20performance%20remains%20robust%20across%20Hilbert%0Aspace%20sizes%2C%20as%20confirmed%20by%20testing%20under%20increasing%20total%20excitation%20numbers.%0AThis%20work%20lays%20the%20foundation%20for%20co-designing%20circuit%20architectures%20and%0Acontrol%20protocols%20that%20leverage%20native%20multiqubit%20interactions%20as%20core%20elements%0Aof%20next-generation%20superconducting%20quantum%20processors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParity%2520Cross-Resonance%253A%2520A%2520Multiqubit%2520Gate%26entry.906535625%3DXuexin%2520Xu%2520and%2520Siyu%2520Wang%2520and%2520Radhika%2520Joshi%2520and%2520Rihan%2520Hai%2520and%2520Mohammad%2520H.%2520Ansari%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520native%2520three-qubit%2520entangling%2520gate%2520that%2520exploits%2520engineered%250Ainteractions%2520to%2520realize%2520control-control-target%2520and%2520control-target-target%250Aoperations%2520in%2520a%2520single%2520coherent%2520step.%2520Unlike%2520conventional%2520decompositions%2520into%250Amultiple%2520two-qubit%2520gates%252C%2520our%2520hybrid%2520optimization%2520approach%2520selectively%250Aamplifies%2520desired%2520interactions%2520while%2520suppressing%2520unwanted%2520couplings%252C%2520yielding%250Arobust%2520performance%2520across%2520the%2520computational%2520subspace%2520and%2520beyond.%2520The%2520new%2520gate%250Acan%2520be%2520classified%2520as%2520a%2520cross-resonance%2520gate.%2520We%2520show%2520it%2520can%2520be%2520utilized%2520in%250Aseveral%2520ways%252C%2520for%2520example%252C%2520in%2520GHZ%2520triplet%2520state%2520preparation%252C%2520Toffoli-class%250Alogic%2520demonstrations%2520with%2520many-body%2520interactions%252C%2520and%2520in%2520implementing%2520a%250Acontrolled-ZZ%2520gate.%2520The%2520latter%2520maps%2520the%2520parity%2520of%2520two%2520data%2520qubits%2520directly%2520onto%250Aa%2520measurement%2520qubit%252C%2520enabling%2520faster%2520and%2520higher-fidelity%2520stabilizer%250Ameasurements%2520in%2520surface-code%2520quantum%2520error%2520correction.%2520In%2520all%2520these%2520examples%252C%250Awe%2520show%2520that%2520the%2520three-qubit%2520gate%2520performance%2520remains%2520robust%2520across%2520Hilbert%250Aspace%2520sizes%252C%2520as%2520confirmed%2520by%2520testing%2520under%2520increasing%2520total%2520excitation%2520numbers.%250AThis%2520work%2520lays%2520the%2520foundation%2520for%2520co-designing%2520circuit%2520architectures%2520and%250Acontrol%2520protocols%2520that%2520leverage%2520native%2520multiqubit%2520interactions%2520as%2520core%2520elements%250Aof%2520next-generation%2520superconducting%2520quantum%2520processors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parity%20Cross-Resonance%3A%20A%20Multiqubit%20Gate&entry.906535625=Xuexin%20Xu%20and%20Siyu%20Wang%20and%20Radhika%20Joshi%20and%20Rihan%20Hai%20and%20Mohammad%20H.%20Ansari&entry.1292438233=%20%20We%20present%20a%20native%20three-qubit%20entangling%20gate%20that%20exploits%20engineered%0Ainteractions%20to%20realize%20control-control-target%20and%20control-target-target%0Aoperations%20in%20a%20single%20coherent%20step.%20Unlike%20conventional%20decompositions%20into%0Amultiple%20two-qubit%20gates%2C%20our%20hybrid%20optimization%20approach%20selectively%0Aamplifies%20desired%20interactions%20while%20suppressing%20unwanted%20couplings%2C%20yielding%0Arobust%20performance%20across%20the%20computational%20subspace%20and%20beyond.%20The%20new%20gate%0Acan%20be%20classified%20as%20a%20cross-resonance%20gate.%20We%20show%20it%20can%20be%20utilized%20in%0Aseveral%20ways%2C%20for%20example%2C%20in%20GHZ%20triplet%20state%20preparation%2C%20Toffoli-class%0Alogic%20demonstrations%20with%20many-body%20interactions%2C%20and%20in%20implementing%20a%0Acontrolled-ZZ%20gate.%20The%20latter%20maps%20the%20parity%20of%20two%20data%20qubits%20directly%20onto%0Aa%20measurement%20qubit%2C%20enabling%20faster%20and%20higher-fidelity%20stabilizer%0Ameasurements%20in%20surface-code%20quantum%20error%20correction.%20In%20all%20these%20examples%2C%0Awe%20show%20that%20the%20three-qubit%20gate%20performance%20remains%20robust%20across%20Hilbert%0Aspace%20sizes%2C%20as%20confirmed%20by%20testing%20under%20increasing%20total%20excitation%20numbers.%0AThis%20work%20lays%20the%20foundation%20for%20co-designing%20circuit%20architectures%20and%0Acontrol%20protocols%20that%20leverage%20native%20multiqubit%20interactions%20as%20core%20elements%0Aof%20next-generation%20superconducting%20quantum%20processors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10807v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


