<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250821.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Enhancing Novel View Synthesis from extremely sparse views with SfM-free\n  3D Gaussian Splatting Framework", "author": "Zongqi He and Hanmin Li and Kin-Chung Chan and Yushen Zuo and Hao Xie and Zhe Xiao and Jun Xiao and Kin-Man Lam", "abstract": "  3D Gaussian Splatting (3DGS) has demonstrated remarkable real-time\nperformance in novel view synthesis, yet its effectiveness relies heavily on\ndense multi-view inputs with precisely known camera poses, which are rarely\navailable in real-world scenarios. When input views become extremely sparse,\nthe Structure-from-Motion (SfM) method that 3DGS depends on for initialization\nfails to accurately reconstruct the 3D geometric structures of scenes,\nresulting in degraded rendering quality. In this paper, we propose a novel\nSfM-free 3DGS-based method that jointly estimates camera poses and reconstructs\n3D scenes from extremely sparse-view inputs. Specifically, instead of SfM, we\npropose a dense stereo module to progressively estimates camera pose\ninformation and reconstructs a global dense point cloud for initialization. To\naddress the inherent problem of information scarcity in extremely sparse-view\nsettings, we propose a coherent view interpolation module that interpolates\ncamera poses based on training view pairs and generates viewpoint-consistent\ncontent as additional supervision signals for training. Furthermore, we\nintroduce multi-scale Laplacian consistent regularization and adaptive\nspatial-aware multi-scale geometry regularization to enhance the quality of\ngeometrical structures and rendered content. Experiments show that our method\nsignificantly outperforms other state-of-the-art 3DGS-based approaches,\nachieving a remarkable 2.75dB improvement in PSNR under extremely sparse-view\nconditions (using only 2 training views). The images synthesized by our method\nexhibit minimal distortion while preserving rich high-frequency details,\nresulting in superior visual quality compared to existing techniques.\n", "link": "http://arxiv.org/abs/2508.15457v1", "date": "2025-08-21", "relevancy": 3.3785, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7035}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6704}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Novel%20View%20Synthesis%20from%20extremely%20sparse%20views%20with%20SfM-free%0A%20%203D%20Gaussian%20Splatting%20Framework&body=Title%3A%20Enhancing%20Novel%20View%20Synthesis%20from%20extremely%20sparse%20views%20with%20SfM-free%0A%20%203D%20Gaussian%20Splatting%20Framework%0AAuthor%3A%20Zongqi%20He%20and%20Hanmin%20Li%20and%20Kin-Chung%20Chan%20and%20Yushen%20Zuo%20and%20Hao%20Xie%20and%20Zhe%20Xiao%20and%20Jun%20Xiao%20and%20Kin-Man%20Lam%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20remarkable%20real-time%0Aperformance%20in%20novel%20view%20synthesis%2C%20yet%20its%20effectiveness%20relies%20heavily%20on%0Adense%20multi-view%20inputs%20with%20precisely%20known%20camera%20poses%2C%20which%20are%20rarely%0Aavailable%20in%20real-world%20scenarios.%20When%20input%20views%20become%20extremely%20sparse%2C%0Athe%20Structure-from-Motion%20%28SfM%29%20method%20that%203DGS%20depends%20on%20for%20initialization%0Afails%20to%20accurately%20reconstruct%20the%203D%20geometric%20structures%20of%20scenes%2C%0Aresulting%20in%20degraded%20rendering%20quality.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0ASfM-free%203DGS-based%20method%20that%20jointly%20estimates%20camera%20poses%20and%20reconstructs%0A3D%20scenes%20from%20extremely%20sparse-view%20inputs.%20Specifically%2C%20instead%20of%20SfM%2C%20we%0Apropose%20a%20dense%20stereo%20module%20to%20progressively%20estimates%20camera%20pose%0Ainformation%20and%20reconstructs%20a%20global%20dense%20point%20cloud%20for%20initialization.%20To%0Aaddress%20the%20inherent%20problem%20of%20information%20scarcity%20in%20extremely%20sparse-view%0Asettings%2C%20we%20propose%20a%20coherent%20view%20interpolation%20module%20that%20interpolates%0Acamera%20poses%20based%20on%20training%20view%20pairs%20and%20generates%20viewpoint-consistent%0Acontent%20as%20additional%20supervision%20signals%20for%20training.%20Furthermore%2C%20we%0Aintroduce%20multi-scale%20Laplacian%20consistent%20regularization%20and%20adaptive%0Aspatial-aware%20multi-scale%20geometry%20regularization%20to%20enhance%20the%20quality%20of%0Ageometrical%20structures%20and%20rendered%20content.%20Experiments%20show%20that%20our%20method%0Asignificantly%20outperforms%20other%20state-of-the-art%203DGS-based%20approaches%2C%0Aachieving%20a%20remarkable%202.75dB%20improvement%20in%20PSNR%20under%20extremely%20sparse-view%0Aconditions%20%28using%20only%202%20training%20views%29.%20The%20images%20synthesized%20by%20our%20method%0Aexhibit%20minimal%20distortion%20while%20preserving%20rich%20high-frequency%20details%2C%0Aresulting%20in%20superior%20visual%20quality%20compared%20to%20existing%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Novel%2520View%2520Synthesis%2520from%2520extremely%2520sparse%2520views%2520with%2520SfM-free%250A%2520%25203D%2520Gaussian%2520Splatting%2520Framework%26entry.906535625%3DZongqi%2520He%2520and%2520Hanmin%2520Li%2520and%2520Kin-Chung%2520Chan%2520and%2520Yushen%2520Zuo%2520and%2520Hao%2520Xie%2520and%2520Zhe%2520Xiao%2520and%2520Jun%2520Xiao%2520and%2520Kin-Man%2520Lam%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520demonstrated%2520remarkable%2520real-time%250Aperformance%2520in%2520novel%2520view%2520synthesis%252C%2520yet%2520its%2520effectiveness%2520relies%2520heavily%2520on%250Adense%2520multi-view%2520inputs%2520with%2520precisely%2520known%2520camera%2520poses%252C%2520which%2520are%2520rarely%250Aavailable%2520in%2520real-world%2520scenarios.%2520When%2520input%2520views%2520become%2520extremely%2520sparse%252C%250Athe%2520Structure-from-Motion%2520%2528SfM%2529%2520method%2520that%25203DGS%2520depends%2520on%2520for%2520initialization%250Afails%2520to%2520accurately%2520reconstruct%2520the%25203D%2520geometric%2520structures%2520of%2520scenes%252C%250Aresulting%2520in%2520degraded%2520rendering%2520quality.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250ASfM-free%25203DGS-based%2520method%2520that%2520jointly%2520estimates%2520camera%2520poses%2520and%2520reconstructs%250A3D%2520scenes%2520from%2520extremely%2520sparse-view%2520inputs.%2520Specifically%252C%2520instead%2520of%2520SfM%252C%2520we%250Apropose%2520a%2520dense%2520stereo%2520module%2520to%2520progressively%2520estimates%2520camera%2520pose%250Ainformation%2520and%2520reconstructs%2520a%2520global%2520dense%2520point%2520cloud%2520for%2520initialization.%2520To%250Aaddress%2520the%2520inherent%2520problem%2520of%2520information%2520scarcity%2520in%2520extremely%2520sparse-view%250Asettings%252C%2520we%2520propose%2520a%2520coherent%2520view%2520interpolation%2520module%2520that%2520interpolates%250Acamera%2520poses%2520based%2520on%2520training%2520view%2520pairs%2520and%2520generates%2520viewpoint-consistent%250Acontent%2520as%2520additional%2520supervision%2520signals%2520for%2520training.%2520Furthermore%252C%2520we%250Aintroduce%2520multi-scale%2520Laplacian%2520consistent%2520regularization%2520and%2520adaptive%250Aspatial-aware%2520multi-scale%2520geometry%2520regularization%2520to%2520enhance%2520the%2520quality%2520of%250Ageometrical%2520structures%2520and%2520rendered%2520content.%2520Experiments%2520show%2520that%2520our%2520method%250Asignificantly%2520outperforms%2520other%2520state-of-the-art%25203DGS-based%2520approaches%252C%250Aachieving%2520a%2520remarkable%25202.75dB%2520improvement%2520in%2520PSNR%2520under%2520extremely%2520sparse-view%250Aconditions%2520%2528using%2520only%25202%2520training%2520views%2529.%2520The%2520images%2520synthesized%2520by%2520our%2520method%250Aexhibit%2520minimal%2520distortion%2520while%2520preserving%2520rich%2520high-frequency%2520details%252C%250Aresulting%2520in%2520superior%2520visual%2520quality%2520compared%2520to%2520existing%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Novel%20View%20Synthesis%20from%20extremely%20sparse%20views%20with%20SfM-free%0A%20%203D%20Gaussian%20Splatting%20Framework&entry.906535625=Zongqi%20He%20and%20Hanmin%20Li%20and%20Kin-Chung%20Chan%20and%20Yushen%20Zuo%20and%20Hao%20Xie%20and%20Zhe%20Xiao%20and%20Jun%20Xiao%20and%20Kin-Man%20Lam&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20remarkable%20real-time%0Aperformance%20in%20novel%20view%20synthesis%2C%20yet%20its%20effectiveness%20relies%20heavily%20on%0Adense%20multi-view%20inputs%20with%20precisely%20known%20camera%20poses%2C%20which%20are%20rarely%0Aavailable%20in%20real-world%20scenarios.%20When%20input%20views%20become%20extremely%20sparse%2C%0Athe%20Structure-from-Motion%20%28SfM%29%20method%20that%203DGS%20depends%20on%20for%20initialization%0Afails%20to%20accurately%20reconstruct%20the%203D%20geometric%20structures%20of%20scenes%2C%0Aresulting%20in%20degraded%20rendering%20quality.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0ASfM-free%203DGS-based%20method%20that%20jointly%20estimates%20camera%20poses%20and%20reconstructs%0A3D%20scenes%20from%20extremely%20sparse-view%20inputs.%20Specifically%2C%20instead%20of%20SfM%2C%20we%0Apropose%20a%20dense%20stereo%20module%20to%20progressively%20estimates%20camera%20pose%0Ainformation%20and%20reconstructs%20a%20global%20dense%20point%20cloud%20for%20initialization.%20To%0Aaddress%20the%20inherent%20problem%20of%20information%20scarcity%20in%20extremely%20sparse-view%0Asettings%2C%20we%20propose%20a%20coherent%20view%20interpolation%20module%20that%20interpolates%0Acamera%20poses%20based%20on%20training%20view%20pairs%20and%20generates%20viewpoint-consistent%0Acontent%20as%20additional%20supervision%20signals%20for%20training.%20Furthermore%2C%20we%0Aintroduce%20multi-scale%20Laplacian%20consistent%20regularization%20and%20adaptive%0Aspatial-aware%20multi-scale%20geometry%20regularization%20to%20enhance%20the%20quality%20of%0Ageometrical%20structures%20and%20rendered%20content.%20Experiments%20show%20that%20our%20method%0Asignificantly%20outperforms%20other%20state-of-the-art%203DGS-based%20approaches%2C%0Aachieving%20a%20remarkable%202.75dB%20improvement%20in%20PSNR%20under%20extremely%20sparse-view%0Aconditions%20%28using%20only%202%20training%20views%29.%20The%20images%20synthesized%20by%20our%20method%0Aexhibit%20minimal%20distortion%20while%20preserving%20rich%20high-frequency%20details%2C%0Aresulting%20in%20superior%20visual%20quality%20compared%20to%20existing%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15457v1&entry.124074799=Read"},
{"title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt", "author": "Lukas H\u00f6llein and Alja\u017e Bo\u017ei\u010d and Michael Zollh\u00f6fer and Matthias Nie\u00dfner", "abstract": "  We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 20% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.\n", "link": "http://arxiv.org/abs/2409.12892v2", "date": "2025-08-21", "relevancy": 3.2754, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6985}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6446}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DGS-LM%3A%20Faster%20Gaussian-Splatting%20Optimization%20with%20Levenberg-Marquardt&body=Title%3A%203DGS-LM%3A%20Faster%20Gaussian-Splatting%20Optimization%20with%20Levenberg-Marquardt%0AAuthor%3A%20Lukas%20H%C3%B6llein%20and%20Alja%C5%BE%20Bo%C5%BEi%C4%8D%20and%20Michael%20Zollh%C3%B6fer%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20%20%20We%20present%203DGS-LM%2C%20a%20new%20method%20that%20accelerates%20the%20reconstruction%20of%203D%0AGaussian%20Splatting%20%283DGS%29%20by%20replacing%20its%20ADAM%20optimizer%20with%20a%20tailored%0ALevenberg-Marquardt%20%28LM%29.%20Existing%20methods%20reduce%20the%20optimization%20time%20by%0Adecreasing%20the%20number%20of%20Gaussians%20or%20by%20improving%20the%20implementation%20of%20the%0Adifferentiable%20rasterizer.%20However%2C%20they%20still%20rely%20on%20the%20ADAM%20optimizer%20to%0Afit%20Gaussian%20parameters%20of%20a%20scene%20in%20thousands%20of%20iterations%2C%20which%20can%20take%0Aup%20to%20an%20hour.%20To%20this%20end%2C%20we%20change%20the%20optimizer%20to%20LM%20that%20runs%20in%0Aconjunction%20with%20the%203DGS%20differentiable%20rasterizer.%20For%20efficient%20GPU%0Aparallization%2C%20we%20propose%20a%20caching%20data%20structure%20for%20intermediate%20gradients%0Athat%20allows%20us%20to%20efficiently%20calculate%20Jacobian-vector%20products%20in%20custom%20CUDA%0Akernels.%20In%20every%20LM%20iteration%2C%20we%20calculate%20update%20directions%20from%20multiple%0Aimage%20subsets%20using%20these%20kernels%20and%20combine%20them%20in%20a%20weighted%20mean.%20Overall%2C%0Aour%20method%20is%2020%25%20faster%20than%20the%20original%203DGS%20while%20obtaining%20the%20same%0Areconstruction%20quality.%20Our%20optimization%20is%20also%20agnostic%20to%20other%20methods%20that%0Aacclerate%203DGS%2C%20thus%20enabling%20even%20faster%20speedups%20compared%20to%20vanilla%203DGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12892v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DGS-LM%253A%2520Faster%2520Gaussian-Splatting%2520Optimization%2520with%2520Levenberg-Marquardt%26entry.906535625%3DLukas%2520H%25C3%25B6llein%2520and%2520Alja%25C5%25BE%2520Bo%25C5%25BEi%25C4%258D%2520and%2520Michael%2520Zollh%25C3%25B6fer%2520and%2520Matthias%2520Nie%25C3%259Fner%26entry.1292438233%3D%2520%2520We%2520present%25203DGS-LM%252C%2520a%2520new%2520method%2520that%2520accelerates%2520the%2520reconstruction%2520of%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529%2520by%2520replacing%2520its%2520ADAM%2520optimizer%2520with%2520a%2520tailored%250ALevenberg-Marquardt%2520%2528LM%2529.%2520Existing%2520methods%2520reduce%2520the%2520optimization%2520time%2520by%250Adecreasing%2520the%2520number%2520of%2520Gaussians%2520or%2520by%2520improving%2520the%2520implementation%2520of%2520the%250Adifferentiable%2520rasterizer.%2520However%252C%2520they%2520still%2520rely%2520on%2520the%2520ADAM%2520optimizer%2520to%250Afit%2520Gaussian%2520parameters%2520of%2520a%2520scene%2520in%2520thousands%2520of%2520iterations%252C%2520which%2520can%2520take%250Aup%2520to%2520an%2520hour.%2520To%2520this%2520end%252C%2520we%2520change%2520the%2520optimizer%2520to%2520LM%2520that%2520runs%2520in%250Aconjunction%2520with%2520the%25203DGS%2520differentiable%2520rasterizer.%2520For%2520efficient%2520GPU%250Aparallization%252C%2520we%2520propose%2520a%2520caching%2520data%2520structure%2520for%2520intermediate%2520gradients%250Athat%2520allows%2520us%2520to%2520efficiently%2520calculate%2520Jacobian-vector%2520products%2520in%2520custom%2520CUDA%250Akernels.%2520In%2520every%2520LM%2520iteration%252C%2520we%2520calculate%2520update%2520directions%2520from%2520multiple%250Aimage%2520subsets%2520using%2520these%2520kernels%2520and%2520combine%2520them%2520in%2520a%2520weighted%2520mean.%2520Overall%252C%250Aour%2520method%2520is%252020%2525%2520faster%2520than%2520the%2520original%25203DGS%2520while%2520obtaining%2520the%2520same%250Areconstruction%2520quality.%2520Our%2520optimization%2520is%2520also%2520agnostic%2520to%2520other%2520methods%2520that%250Aacclerate%25203DGS%252C%2520thus%2520enabling%2520even%2520faster%2520speedups%2520compared%2520to%2520vanilla%25203DGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12892v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DGS-LM%3A%20Faster%20Gaussian-Splatting%20Optimization%20with%20Levenberg-Marquardt&entry.906535625=Lukas%20H%C3%B6llein%20and%20Alja%C5%BE%20Bo%C5%BEi%C4%8D%20and%20Michael%20Zollh%C3%B6fer%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=%20%20We%20present%203DGS-LM%2C%20a%20new%20method%20that%20accelerates%20the%20reconstruction%20of%203D%0AGaussian%20Splatting%20%283DGS%29%20by%20replacing%20its%20ADAM%20optimizer%20with%20a%20tailored%0ALevenberg-Marquardt%20%28LM%29.%20Existing%20methods%20reduce%20the%20optimization%20time%20by%0Adecreasing%20the%20number%20of%20Gaussians%20or%20by%20improving%20the%20implementation%20of%20the%0Adifferentiable%20rasterizer.%20However%2C%20they%20still%20rely%20on%20the%20ADAM%20optimizer%20to%0Afit%20Gaussian%20parameters%20of%20a%20scene%20in%20thousands%20of%20iterations%2C%20which%20can%20take%0Aup%20to%20an%20hour.%20To%20this%20end%2C%20we%20change%20the%20optimizer%20to%20LM%20that%20runs%20in%0Aconjunction%20with%20the%203DGS%20differentiable%20rasterizer.%20For%20efficient%20GPU%0Aparallization%2C%20we%20propose%20a%20caching%20data%20structure%20for%20intermediate%20gradients%0Athat%20allows%20us%20to%20efficiently%20calculate%20Jacobian-vector%20products%20in%20custom%20CUDA%0Akernels.%20In%20every%20LM%20iteration%2C%20we%20calculate%20update%20directions%20from%20multiple%0Aimage%20subsets%20using%20these%20kernels%20and%20combine%20them%20in%20a%20weighted%20mean.%20Overall%2C%0Aour%20method%20is%2020%25%20faster%20than%20the%20original%203DGS%20while%20obtaining%20the%20same%0Areconstruction%20quality.%20Our%20optimization%20is%20also%20agnostic%20to%20other%20methods%20that%0Aacclerate%203DGS%2C%20thus%20enabling%20even%20faster%20speedups%20compared%20to%20vanilla%203DGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12892v2&entry.124074799=Read"},
{"title": "TiP4GEN: Text to Immersive Panorama 4D Scene Generation", "author": "Ke Xing and Hanwen Liang and Dejia Xu and Yuyang Yin and Konstantinos N. Plataniotis and Yao Zhao and Yunchao Wei", "abstract": "  With the rapid advancement and widespread adoption of VR/AR technologies,\nthere is a growing demand for the creation of high-quality, immersive dynamic\nscenes. However, existing generation works predominantly concentrate on the\ncreation of static scenes or narrow perspective-view dynamic scenes, falling\nshort of delivering a truly 360-degree immersive experience from any viewpoint.\nIn this paper, we introduce \\textbf{TiP4GEN}, an advanced text-to-dynamic\npanorama scene generation framework that enables fine-grained content control\nand synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN\nintegrates panorama video generation and dynamic scene reconstruction to create\n360-degree immersive virtual environments. For video generation, we introduce a\n\\textbf{Dual-branch Generation Model} consisting of a panorama branch and a\nperspective branch, responsible for global and local view generation,\nrespectively. A bidirectional cross-attention mechanism facilitates\ncomprehensive information exchange between the branches. For scene\nreconstruction, we propose a \\textbf{Geometry-aligned Reconstruction Model}\nbased on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using\nmetric depth maps and initializing scene cameras with estimated poses, our\nmethod ensures geometric consistency and temporal coherence for the\nreconstructed scenes. Extensive experiments demonstrate the effectiveness of\nour proposed designs and the superiority of TiP4GEN in generating visually\ncompelling and motion-coherent dynamic panoramic scenes. Our project page is at\nhttps://ke-xing.github.io/TiP4GEN/.\n", "link": "http://arxiv.org/abs/2508.12415v2", "date": "2025-08-21", "relevancy": 3.2167, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6469}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6469}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TiP4GEN%3A%20Text%20to%20Immersive%20Panorama%204D%20Scene%20Generation&body=Title%3A%20TiP4GEN%3A%20Text%20to%20Immersive%20Panorama%204D%20Scene%20Generation%0AAuthor%3A%20Ke%20Xing%20and%20Hanwen%20Liang%20and%20Dejia%20Xu%20and%20Yuyang%20Yin%20and%20Konstantinos%20N.%20Plataniotis%20and%20Yao%20Zhao%20and%20Yunchao%20Wei%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20and%20widespread%20adoption%20of%20VR/AR%20technologies%2C%0Athere%20is%20a%20growing%20demand%20for%20the%20creation%20of%20high-quality%2C%20immersive%20dynamic%0Ascenes.%20However%2C%20existing%20generation%20works%20predominantly%20concentrate%20on%20the%0Acreation%20of%20static%20scenes%20or%20narrow%20perspective-view%20dynamic%20scenes%2C%20falling%0Ashort%20of%20delivering%20a%20truly%20360-degree%20immersive%20experience%20from%20any%20viewpoint.%0AIn%20this%20paper%2C%20we%20introduce%20%5Ctextbf%7BTiP4GEN%7D%2C%20an%20advanced%20text-to-dynamic%0Apanorama%20scene%20generation%20framework%20that%20enables%20fine-grained%20content%20control%0Aand%20synthesizes%20motion-rich%2C%20geometry-consistent%20panoramic%204D%20scenes.%20TiP4GEN%0Aintegrates%20panorama%20video%20generation%20and%20dynamic%20scene%20reconstruction%20to%20create%0A360-degree%20immersive%20virtual%20environments.%20For%20video%20generation%2C%20we%20introduce%20a%0A%5Ctextbf%7BDual-branch%20Generation%20Model%7D%20consisting%20of%20a%20panorama%20branch%20and%20a%0Aperspective%20branch%2C%20responsible%20for%20global%20and%20local%20view%20generation%2C%0Arespectively.%20A%20bidirectional%20cross-attention%20mechanism%20facilitates%0Acomprehensive%20information%20exchange%20between%20the%20branches.%20For%20scene%0Areconstruction%2C%20we%20propose%20a%20%5Ctextbf%7BGeometry-aligned%20Reconstruction%20Model%7D%0Abased%20on%203D%20Gaussian%20Splatting.%20By%20aligning%20spatial-temporal%20point%20clouds%20using%0Ametric%20depth%20maps%20and%20initializing%20scene%20cameras%20with%20estimated%20poses%2C%20our%0Amethod%20ensures%20geometric%20consistency%20and%20temporal%20coherence%20for%20the%0Areconstructed%20scenes.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%0Aour%20proposed%20designs%20and%20the%20superiority%20of%20TiP4GEN%20in%20generating%20visually%0Acompelling%20and%20motion-coherent%20dynamic%20panoramic%20scenes.%20Our%20project%20page%20is%20at%0Ahttps%3A//ke-xing.github.io/TiP4GEN/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12415v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiP4GEN%253A%2520Text%2520to%2520Immersive%2520Panorama%25204D%2520Scene%2520Generation%26entry.906535625%3DKe%2520Xing%2520and%2520Hanwen%2520Liang%2520and%2520Dejia%2520Xu%2520and%2520Yuyang%2520Yin%2520and%2520Konstantinos%2520N.%2520Plataniotis%2520and%2520Yao%2520Zhao%2520and%2520Yunchao%2520Wei%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520and%2520widespread%2520adoption%2520of%2520VR/AR%2520technologies%252C%250Athere%2520is%2520a%2520growing%2520demand%2520for%2520the%2520creation%2520of%2520high-quality%252C%2520immersive%2520dynamic%250Ascenes.%2520However%252C%2520existing%2520generation%2520works%2520predominantly%2520concentrate%2520on%2520the%250Acreation%2520of%2520static%2520scenes%2520or%2520narrow%2520perspective-view%2520dynamic%2520scenes%252C%2520falling%250Ashort%2520of%2520delivering%2520a%2520truly%2520360-degree%2520immersive%2520experience%2520from%2520any%2520viewpoint.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520%255Ctextbf%257BTiP4GEN%257D%252C%2520an%2520advanced%2520text-to-dynamic%250Apanorama%2520scene%2520generation%2520framework%2520that%2520enables%2520fine-grained%2520content%2520control%250Aand%2520synthesizes%2520motion-rich%252C%2520geometry-consistent%2520panoramic%25204D%2520scenes.%2520TiP4GEN%250Aintegrates%2520panorama%2520video%2520generation%2520and%2520dynamic%2520scene%2520reconstruction%2520to%2520create%250A360-degree%2520immersive%2520virtual%2520environments.%2520For%2520video%2520generation%252C%2520we%2520introduce%2520a%250A%255Ctextbf%257BDual-branch%2520Generation%2520Model%257D%2520consisting%2520of%2520a%2520panorama%2520branch%2520and%2520a%250Aperspective%2520branch%252C%2520responsible%2520for%2520global%2520and%2520local%2520view%2520generation%252C%250Arespectively.%2520A%2520bidirectional%2520cross-attention%2520mechanism%2520facilitates%250Acomprehensive%2520information%2520exchange%2520between%2520the%2520branches.%2520For%2520scene%250Areconstruction%252C%2520we%2520propose%2520a%2520%255Ctextbf%257BGeometry-aligned%2520Reconstruction%2520Model%257D%250Abased%2520on%25203D%2520Gaussian%2520Splatting.%2520By%2520aligning%2520spatial-temporal%2520point%2520clouds%2520using%250Ametric%2520depth%2520maps%2520and%2520initializing%2520scene%2520cameras%2520with%2520estimated%2520poses%252C%2520our%250Amethod%2520ensures%2520geometric%2520consistency%2520and%2520temporal%2520coherence%2520for%2520the%250Areconstructed%2520scenes.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520proposed%2520designs%2520and%2520the%2520superiority%2520of%2520TiP4GEN%2520in%2520generating%2520visually%250Acompelling%2520and%2520motion-coherent%2520dynamic%2520panoramic%2520scenes.%2520Our%2520project%2520page%2520is%2520at%250Ahttps%253A//ke-xing.github.io/TiP4GEN/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12415v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TiP4GEN%3A%20Text%20to%20Immersive%20Panorama%204D%20Scene%20Generation&entry.906535625=Ke%20Xing%20and%20Hanwen%20Liang%20and%20Dejia%20Xu%20and%20Yuyang%20Yin%20and%20Konstantinos%20N.%20Plataniotis%20and%20Yao%20Zhao%20and%20Yunchao%20Wei&entry.1292438233=%20%20With%20the%20rapid%20advancement%20and%20widespread%20adoption%20of%20VR/AR%20technologies%2C%0Athere%20is%20a%20growing%20demand%20for%20the%20creation%20of%20high-quality%2C%20immersive%20dynamic%0Ascenes.%20However%2C%20existing%20generation%20works%20predominantly%20concentrate%20on%20the%0Acreation%20of%20static%20scenes%20or%20narrow%20perspective-view%20dynamic%20scenes%2C%20falling%0Ashort%20of%20delivering%20a%20truly%20360-degree%20immersive%20experience%20from%20any%20viewpoint.%0AIn%20this%20paper%2C%20we%20introduce%20%5Ctextbf%7BTiP4GEN%7D%2C%20an%20advanced%20text-to-dynamic%0Apanorama%20scene%20generation%20framework%20that%20enables%20fine-grained%20content%20control%0Aand%20synthesizes%20motion-rich%2C%20geometry-consistent%20panoramic%204D%20scenes.%20TiP4GEN%0Aintegrates%20panorama%20video%20generation%20and%20dynamic%20scene%20reconstruction%20to%20create%0A360-degree%20immersive%20virtual%20environments.%20For%20video%20generation%2C%20we%20introduce%20a%0A%5Ctextbf%7BDual-branch%20Generation%20Model%7D%20consisting%20of%20a%20panorama%20branch%20and%20a%0Aperspective%20branch%2C%20responsible%20for%20global%20and%20local%20view%20generation%2C%0Arespectively.%20A%20bidirectional%20cross-attention%20mechanism%20facilitates%0Acomprehensive%20information%20exchange%20between%20the%20branches.%20For%20scene%0Areconstruction%2C%20we%20propose%20a%20%5Ctextbf%7BGeometry-aligned%20Reconstruction%20Model%7D%0Abased%20on%203D%20Gaussian%20Splatting.%20By%20aligning%20spatial-temporal%20point%20clouds%20using%0Ametric%20depth%20maps%20and%20initializing%20scene%20cameras%20with%20estimated%20poses%2C%20our%0Amethod%20ensures%20geometric%20consistency%20and%20temporal%20coherence%20for%20the%0Areconstructed%20scenes.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%0Aour%20proposed%20designs%20and%20the%20superiority%20of%20TiP4GEN%20in%20generating%20visually%0Acompelling%20and%20motion-coherent%20dynamic%20panoramic%20scenes.%20Our%20project%20page%20is%20at%0Ahttps%3A//ke-xing.github.io/TiP4GEN/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12415v2&entry.124074799=Read"},
{"title": "DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning\n  in Complex 3D Situated Reasoning Tasks", "author": "Jiayi Song and Rui Wan and Lipeng Ma and Weidong Yang and Qingyuan Zhou and Yixuan Li and Ben Fei", "abstract": "  This work enhances the ability of large language models (LLMs) to perform\ncomplex reasoning in 3D scenes. Recent work has addressed the 3D situated\nreasoning task by invoking tool usage through large language models. Large\nlanguage models call tools via APIs and integrate the generated programs\nthrough a chain of thought to solve problems based on the program results.\nHowever, due to the simplicity of the questions in the dataset, the generated\nprogram reasoning chains are relatively short. To solve this main challenge, in\nthis paper, we introduce DeepThink3D to enhance the tool usage of LLMs in\ncomplex 3D situated reasoning tasks. Our work proposes a combinatorial and\niterative evolutionary approach on the SQA3D benchmark to generate more complex\nquestions. Building on this foundation, we fine-tune the large language model\nto make it more proficient in using 3D tools. By employing Direct Preference\nOptimization (DPO), we directly optimize the toolchain strategies generated by\nmodels, thereby enhancing their accuracy in complex tasks.\n", "link": "http://arxiv.org/abs/2508.15548v1", "date": "2025-08-21", "relevancy": 3.1312, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6554}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6554}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepThink3D%3A%20Enhancing%20Large%20Language%20Models%20with%20Programmatic%20Reasoning%0A%20%20in%20Complex%203D%20Situated%20Reasoning%20Tasks&body=Title%3A%20DeepThink3D%3A%20Enhancing%20Large%20Language%20Models%20with%20Programmatic%20Reasoning%0A%20%20in%20Complex%203D%20Situated%20Reasoning%20Tasks%0AAuthor%3A%20Jiayi%20Song%20and%20Rui%20Wan%20and%20Lipeng%20Ma%20and%20Weidong%20Yang%20and%20Qingyuan%20Zhou%20and%20Yixuan%20Li%20and%20Ben%20Fei%0AAbstract%3A%20%20%20This%20work%20enhances%20the%20ability%20of%20large%20language%20models%20%28LLMs%29%20to%20perform%0Acomplex%20reasoning%20in%203D%20scenes.%20Recent%20work%20has%20addressed%20the%203D%20situated%0Areasoning%20task%20by%20invoking%20tool%20usage%20through%20large%20language%20models.%20Large%0Alanguage%20models%20call%20tools%20via%20APIs%20and%20integrate%20the%20generated%20programs%0Athrough%20a%20chain%20of%20thought%20to%20solve%20problems%20based%20on%20the%20program%20results.%0AHowever%2C%20due%20to%20the%20simplicity%20of%20the%20questions%20in%20the%20dataset%2C%20the%20generated%0Aprogram%20reasoning%20chains%20are%20relatively%20short.%20To%20solve%20this%20main%20challenge%2C%20in%0Athis%20paper%2C%20we%20introduce%20DeepThink3D%20to%20enhance%20the%20tool%20usage%20of%20LLMs%20in%0Acomplex%203D%20situated%20reasoning%20tasks.%20Our%20work%20proposes%20a%20combinatorial%20and%0Aiterative%20evolutionary%20approach%20on%20the%20SQA3D%20benchmark%20to%20generate%20more%20complex%0Aquestions.%20Building%20on%20this%20foundation%2C%20we%20fine-tune%20the%20large%20language%20model%0Ato%20make%20it%20more%20proficient%20in%20using%203D%20tools.%20By%20employing%20Direct%20Preference%0AOptimization%20%28DPO%29%2C%20we%20directly%20optimize%20the%20toolchain%20strategies%20generated%20by%0Amodels%2C%20thereby%20enhancing%20their%20accuracy%20in%20complex%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepThink3D%253A%2520Enhancing%2520Large%2520Language%2520Models%2520with%2520Programmatic%2520Reasoning%250A%2520%2520in%2520Complex%25203D%2520Situated%2520Reasoning%2520Tasks%26entry.906535625%3DJiayi%2520Song%2520and%2520Rui%2520Wan%2520and%2520Lipeng%2520Ma%2520and%2520Weidong%2520Yang%2520and%2520Qingyuan%2520Zhou%2520and%2520Yixuan%2520Li%2520and%2520Ben%2520Fei%26entry.1292438233%3D%2520%2520This%2520work%2520enhances%2520the%2520ability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520perform%250Acomplex%2520reasoning%2520in%25203D%2520scenes.%2520Recent%2520work%2520has%2520addressed%2520the%25203D%2520situated%250Areasoning%2520task%2520by%2520invoking%2520tool%2520usage%2520through%2520large%2520language%2520models.%2520Large%250Alanguage%2520models%2520call%2520tools%2520via%2520APIs%2520and%2520integrate%2520the%2520generated%2520programs%250Athrough%2520a%2520chain%2520of%2520thought%2520to%2520solve%2520problems%2520based%2520on%2520the%2520program%2520results.%250AHowever%252C%2520due%2520to%2520the%2520simplicity%2520of%2520the%2520questions%2520in%2520the%2520dataset%252C%2520the%2520generated%250Aprogram%2520reasoning%2520chains%2520are%2520relatively%2520short.%2520To%2520solve%2520this%2520main%2520challenge%252C%2520in%250Athis%2520paper%252C%2520we%2520introduce%2520DeepThink3D%2520to%2520enhance%2520the%2520tool%2520usage%2520of%2520LLMs%2520in%250Acomplex%25203D%2520situated%2520reasoning%2520tasks.%2520Our%2520work%2520proposes%2520a%2520combinatorial%2520and%250Aiterative%2520evolutionary%2520approach%2520on%2520the%2520SQA3D%2520benchmark%2520to%2520generate%2520more%2520complex%250Aquestions.%2520Building%2520on%2520this%2520foundation%252C%2520we%2520fine-tune%2520the%2520large%2520language%2520model%250Ato%2520make%2520it%2520more%2520proficient%2520in%2520using%25203D%2520tools.%2520By%2520employing%2520Direct%2520Preference%250AOptimization%2520%2528DPO%2529%252C%2520we%2520directly%2520optimize%2520the%2520toolchain%2520strategies%2520generated%2520by%250Amodels%252C%2520thereby%2520enhancing%2520their%2520accuracy%2520in%2520complex%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepThink3D%3A%20Enhancing%20Large%20Language%20Models%20with%20Programmatic%20Reasoning%0A%20%20in%20Complex%203D%20Situated%20Reasoning%20Tasks&entry.906535625=Jiayi%20Song%20and%20Rui%20Wan%20and%20Lipeng%20Ma%20and%20Weidong%20Yang%20and%20Qingyuan%20Zhou%20and%20Yixuan%20Li%20and%20Ben%20Fei&entry.1292438233=%20%20This%20work%20enhances%20the%20ability%20of%20large%20language%20models%20%28LLMs%29%20to%20perform%0Acomplex%20reasoning%20in%203D%20scenes.%20Recent%20work%20has%20addressed%20the%203D%20situated%0Areasoning%20task%20by%20invoking%20tool%20usage%20through%20large%20language%20models.%20Large%0Alanguage%20models%20call%20tools%20via%20APIs%20and%20integrate%20the%20generated%20programs%0Athrough%20a%20chain%20of%20thought%20to%20solve%20problems%20based%20on%20the%20program%20results.%0AHowever%2C%20due%20to%20the%20simplicity%20of%20the%20questions%20in%20the%20dataset%2C%20the%20generated%0Aprogram%20reasoning%20chains%20are%20relatively%20short.%20To%20solve%20this%20main%20challenge%2C%20in%0Athis%20paper%2C%20we%20introduce%20DeepThink3D%20to%20enhance%20the%20tool%20usage%20of%20LLMs%20in%0Acomplex%203D%20situated%20reasoning%20tasks.%20Our%20work%20proposes%20a%20combinatorial%20and%0Aiterative%20evolutionary%20approach%20on%20the%20SQA3D%20benchmark%20to%20generate%20more%20complex%0Aquestions.%20Building%20on%20this%20foundation%2C%20we%20fine-tune%20the%20large%20language%20model%0Ato%20make%20it%20more%20proficient%20in%20using%203D%20tools.%20By%20employing%20Direct%20Preference%0AOptimization%20%28DPO%29%2C%20we%20directly%20optimize%20the%20toolchain%20strategies%20generated%20by%0Amodels%2C%20thereby%20enhancing%20their%20accuracy%20in%20complex%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15548v1&entry.124074799=Read"},
{"title": "WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception", "author": "Zhiheng Liu and Xueqing Deng and Shoufa Chen and Angtian Wang and Qiushan Guo and Mingfei Han and Zeyue Xue and Mengzhao Chen and Ping Luo and Linjie Yang", "abstract": "  Generative video modeling has made significant strides, yet ensuring\nstructural and temporal consistency over long sequences remains a challenge.\nCurrent methods predominantly rely on RGB signals, leading to accumulated\nerrors in object structure and motion over extended durations. To address these\nissues, we introduce WorldWeaver, a robust framework for long video generation\nthat jointly models RGB frames and perceptual conditions within a unified\nlong-horizon modeling scheme. Our training framework offers three key\nadvantages. First, by jointly predicting perceptual conditions and color\ninformation from a unified representation, it significantly enhances temporal\nconsistency and motion dynamics. Second, by leveraging depth cues, which we\nobserve to be more resistant to drift than RGB, we construct a memory bank that\npreserves clearer contextual information, improving quality in long-horizon\nvideo generation. Third, we employ segmented noise scheduling for training\nprediction groups, which further mitigates drift and reduces computational\ncost. Extensive experiments on both diffusion- and rectified flow-based models\ndemonstrate the effectiveness of WorldWeaver in reducing temporal drift and\nimproving the fidelity of generated videos.\n", "link": "http://arxiv.org/abs/2508.15720v1", "date": "2025-08-21", "relevancy": 3.0623, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6282}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.606}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WorldWeaver%3A%20Generating%20Long-Horizon%20Video%20Worlds%20via%20Rich%20Perception&body=Title%3A%20WorldWeaver%3A%20Generating%20Long-Horizon%20Video%20Worlds%20via%20Rich%20Perception%0AAuthor%3A%20Zhiheng%20Liu%20and%20Xueqing%20Deng%20and%20Shoufa%20Chen%20and%20Angtian%20Wang%20and%20Qiushan%20Guo%20and%20Mingfei%20Han%20and%20Zeyue%20Xue%20and%20Mengzhao%20Chen%20and%20Ping%20Luo%20and%20Linjie%20Yang%0AAbstract%3A%20%20%20Generative%20video%20modeling%20has%20made%20significant%20strides%2C%20yet%20ensuring%0Astructural%20and%20temporal%20consistency%20over%20long%20sequences%20remains%20a%20challenge.%0ACurrent%20methods%20predominantly%20rely%20on%20RGB%20signals%2C%20leading%20to%20accumulated%0Aerrors%20in%20object%20structure%20and%20motion%20over%20extended%20durations.%20To%20address%20these%0Aissues%2C%20we%20introduce%20WorldWeaver%2C%20a%20robust%20framework%20for%20long%20video%20generation%0Athat%20jointly%20models%20RGB%20frames%20and%20perceptual%20conditions%20within%20a%20unified%0Along-horizon%20modeling%20scheme.%20Our%20training%20framework%20offers%20three%20key%0Aadvantages.%20First%2C%20by%20jointly%20predicting%20perceptual%20conditions%20and%20color%0Ainformation%20from%20a%20unified%20representation%2C%20it%20significantly%20enhances%20temporal%0Aconsistency%20and%20motion%20dynamics.%20Second%2C%20by%20leveraging%20depth%20cues%2C%20which%20we%0Aobserve%20to%20be%20more%20resistant%20to%20drift%20than%20RGB%2C%20we%20construct%20a%20memory%20bank%20that%0Apreserves%20clearer%20contextual%20information%2C%20improving%20quality%20in%20long-horizon%0Avideo%20generation.%20Third%2C%20we%20employ%20segmented%20noise%20scheduling%20for%20training%0Aprediction%20groups%2C%20which%20further%20mitigates%20drift%20and%20reduces%20computational%0Acost.%20Extensive%20experiments%20on%20both%20diffusion-%20and%20rectified%20flow-based%20models%0Ademonstrate%20the%20effectiveness%20of%20WorldWeaver%20in%20reducing%20temporal%20drift%20and%0Aimproving%20the%20fidelity%20of%20generated%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorldWeaver%253A%2520Generating%2520Long-Horizon%2520Video%2520Worlds%2520via%2520Rich%2520Perception%26entry.906535625%3DZhiheng%2520Liu%2520and%2520Xueqing%2520Deng%2520and%2520Shoufa%2520Chen%2520and%2520Angtian%2520Wang%2520and%2520Qiushan%2520Guo%2520and%2520Mingfei%2520Han%2520and%2520Zeyue%2520Xue%2520and%2520Mengzhao%2520Chen%2520and%2520Ping%2520Luo%2520and%2520Linjie%2520Yang%26entry.1292438233%3D%2520%2520Generative%2520video%2520modeling%2520has%2520made%2520significant%2520strides%252C%2520yet%2520ensuring%250Astructural%2520and%2520temporal%2520consistency%2520over%2520long%2520sequences%2520remains%2520a%2520challenge.%250ACurrent%2520methods%2520predominantly%2520rely%2520on%2520RGB%2520signals%252C%2520leading%2520to%2520accumulated%250Aerrors%2520in%2520object%2520structure%2520and%2520motion%2520over%2520extended%2520durations.%2520To%2520address%2520these%250Aissues%252C%2520we%2520introduce%2520WorldWeaver%252C%2520a%2520robust%2520framework%2520for%2520long%2520video%2520generation%250Athat%2520jointly%2520models%2520RGB%2520frames%2520and%2520perceptual%2520conditions%2520within%2520a%2520unified%250Along-horizon%2520modeling%2520scheme.%2520Our%2520training%2520framework%2520offers%2520three%2520key%250Aadvantages.%2520First%252C%2520by%2520jointly%2520predicting%2520perceptual%2520conditions%2520and%2520color%250Ainformation%2520from%2520a%2520unified%2520representation%252C%2520it%2520significantly%2520enhances%2520temporal%250Aconsistency%2520and%2520motion%2520dynamics.%2520Second%252C%2520by%2520leveraging%2520depth%2520cues%252C%2520which%2520we%250Aobserve%2520to%2520be%2520more%2520resistant%2520to%2520drift%2520than%2520RGB%252C%2520we%2520construct%2520a%2520memory%2520bank%2520that%250Apreserves%2520clearer%2520contextual%2520information%252C%2520improving%2520quality%2520in%2520long-horizon%250Avideo%2520generation.%2520Third%252C%2520we%2520employ%2520segmented%2520noise%2520scheduling%2520for%2520training%250Aprediction%2520groups%252C%2520which%2520further%2520mitigates%2520drift%2520and%2520reduces%2520computational%250Acost.%2520Extensive%2520experiments%2520on%2520both%2520diffusion-%2520and%2520rectified%2520flow-based%2520models%250Ademonstrate%2520the%2520effectiveness%2520of%2520WorldWeaver%2520in%2520reducing%2520temporal%2520drift%2520and%250Aimproving%2520the%2520fidelity%2520of%2520generated%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WorldWeaver%3A%20Generating%20Long-Horizon%20Video%20Worlds%20via%20Rich%20Perception&entry.906535625=Zhiheng%20Liu%20and%20Xueqing%20Deng%20and%20Shoufa%20Chen%20and%20Angtian%20Wang%20and%20Qiushan%20Guo%20and%20Mingfei%20Han%20and%20Zeyue%20Xue%20and%20Mengzhao%20Chen%20and%20Ping%20Luo%20and%20Linjie%20Yang&entry.1292438233=%20%20Generative%20video%20modeling%20has%20made%20significant%20strides%2C%20yet%20ensuring%0Astructural%20and%20temporal%20consistency%20over%20long%20sequences%20remains%20a%20challenge.%0ACurrent%20methods%20predominantly%20rely%20on%20RGB%20signals%2C%20leading%20to%20accumulated%0Aerrors%20in%20object%20structure%20and%20motion%20over%20extended%20durations.%20To%20address%20these%0Aissues%2C%20we%20introduce%20WorldWeaver%2C%20a%20robust%20framework%20for%20long%20video%20generation%0Athat%20jointly%20models%20RGB%20frames%20and%20perceptual%20conditions%20within%20a%20unified%0Along-horizon%20modeling%20scheme.%20Our%20training%20framework%20offers%20three%20key%0Aadvantages.%20First%2C%20by%20jointly%20predicting%20perceptual%20conditions%20and%20color%0Ainformation%20from%20a%20unified%20representation%2C%20it%20significantly%20enhances%20temporal%0Aconsistency%20and%20motion%20dynamics.%20Second%2C%20by%20leveraging%20depth%20cues%2C%20which%20we%0Aobserve%20to%20be%20more%20resistant%20to%20drift%20than%20RGB%2C%20we%20construct%20a%20memory%20bank%20that%0Apreserves%20clearer%20contextual%20information%2C%20improving%20quality%20in%20long-horizon%0Avideo%20generation.%20Third%2C%20we%20employ%20segmented%20noise%20scheduling%20for%20training%0Aprediction%20groups%2C%20which%20further%20mitigates%20drift%20and%20reduces%20computational%0Acost.%20Extensive%20experiments%20on%20both%20diffusion-%20and%20rectified%20flow-based%20models%0Ademonstrate%20the%20effectiveness%20of%20WorldWeaver%20in%20reducing%20temporal%20drift%20and%0Aimproving%20the%20fidelity%20of%20generated%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15720v1&entry.124074799=Read"},
{"title": "MExECON: Multi-view Extended Explicit Clothed humans Optimized via\n  Normal integration", "author": "Fulden Ece U\u011fur and Rafael Redondo and Albert Barreiro and Stefan Hristov and Roger Mar\u00ed", "abstract": "  This work presents MExECON, a novel pipeline for 3D reconstruction of clothed\nhuman avatars from sparse multi-view RGB images. Building on the single-view\nmethod ECON, MExECON extends its capabilities to leverage multiple viewpoints,\nimproving geometry and body pose estimation. At the core of the pipeline is the\nproposed Joint Multi-view Body Optimization (JMBO) algorithm, which fits a\nsingle SMPL-X body model jointly across all input views, enforcing multi-view\nconsistency. The optimized body model serves as a low-frequency prior that\nguides the subsequent surface reconstruction, where geometric details are added\nvia normal map integration. MExECON integrates normal maps from both front and\nback views to accurately capture fine-grained surface details such as clothing\nfolds and hairstyles. All multi-view gains are achieved without requiring any\nnetwork re-training. Experimental results show that MExECON consistently\nimproves fidelity over the single-view baseline and achieves competitive\nperformance compared to modern few-shot 3D reconstruction methods.\n", "link": "http://arxiv.org/abs/2508.15500v1", "date": "2025-08-21", "relevancy": 3.0556, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.623}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6194}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MExECON%3A%20Multi-view%20Extended%20Explicit%20Clothed%20humans%20Optimized%20via%0A%20%20Normal%20integration&body=Title%3A%20MExECON%3A%20Multi-view%20Extended%20Explicit%20Clothed%20humans%20Optimized%20via%0A%20%20Normal%20integration%0AAuthor%3A%20Fulden%20Ece%20U%C4%9Fur%20and%20Rafael%20Redondo%20and%20Albert%20Barreiro%20and%20Stefan%20Hristov%20and%20Roger%20Mar%C3%AD%0AAbstract%3A%20%20%20This%20work%20presents%20MExECON%2C%20a%20novel%20pipeline%20for%203D%20reconstruction%20of%20clothed%0Ahuman%20avatars%20from%20sparse%20multi-view%20RGB%20images.%20Building%20on%20the%20single-view%0Amethod%20ECON%2C%20MExECON%20extends%20its%20capabilities%20to%20leverage%20multiple%20viewpoints%2C%0Aimproving%20geometry%20and%20body%20pose%20estimation.%20At%20the%20core%20of%20the%20pipeline%20is%20the%0Aproposed%20Joint%20Multi-view%20Body%20Optimization%20%28JMBO%29%20algorithm%2C%20which%20fits%20a%0Asingle%20SMPL-X%20body%20model%20jointly%20across%20all%20input%20views%2C%20enforcing%20multi-view%0Aconsistency.%20The%20optimized%20body%20model%20serves%20as%20a%20low-frequency%20prior%20that%0Aguides%20the%20subsequent%20surface%20reconstruction%2C%20where%20geometric%20details%20are%20added%0Avia%20normal%20map%20integration.%20MExECON%20integrates%20normal%20maps%20from%20both%20front%20and%0Aback%20views%20to%20accurately%20capture%20fine-grained%20surface%20details%20such%20as%20clothing%0Afolds%20and%20hairstyles.%20All%20multi-view%20gains%20are%20achieved%20without%20requiring%20any%0Anetwork%20re-training.%20Experimental%20results%20show%20that%20MExECON%20consistently%0Aimproves%20fidelity%20over%20the%20single-view%20baseline%20and%20achieves%20competitive%0Aperformance%20compared%20to%20modern%20few-shot%203D%20reconstruction%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMExECON%253A%2520Multi-view%2520Extended%2520Explicit%2520Clothed%2520humans%2520Optimized%2520via%250A%2520%2520Normal%2520integration%26entry.906535625%3DFulden%2520Ece%2520U%25C4%259Fur%2520and%2520Rafael%2520Redondo%2520and%2520Albert%2520Barreiro%2520and%2520Stefan%2520Hristov%2520and%2520Roger%2520Mar%25C3%25AD%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520MExECON%252C%2520a%2520novel%2520pipeline%2520for%25203D%2520reconstruction%2520of%2520clothed%250Ahuman%2520avatars%2520from%2520sparse%2520multi-view%2520RGB%2520images.%2520Building%2520on%2520the%2520single-view%250Amethod%2520ECON%252C%2520MExECON%2520extends%2520its%2520capabilities%2520to%2520leverage%2520multiple%2520viewpoints%252C%250Aimproving%2520geometry%2520and%2520body%2520pose%2520estimation.%2520At%2520the%2520core%2520of%2520the%2520pipeline%2520is%2520the%250Aproposed%2520Joint%2520Multi-view%2520Body%2520Optimization%2520%2528JMBO%2529%2520algorithm%252C%2520which%2520fits%2520a%250Asingle%2520SMPL-X%2520body%2520model%2520jointly%2520across%2520all%2520input%2520views%252C%2520enforcing%2520multi-view%250Aconsistency.%2520The%2520optimized%2520body%2520model%2520serves%2520as%2520a%2520low-frequency%2520prior%2520that%250Aguides%2520the%2520subsequent%2520surface%2520reconstruction%252C%2520where%2520geometric%2520details%2520are%2520added%250Avia%2520normal%2520map%2520integration.%2520MExECON%2520integrates%2520normal%2520maps%2520from%2520both%2520front%2520and%250Aback%2520views%2520to%2520accurately%2520capture%2520fine-grained%2520surface%2520details%2520such%2520as%2520clothing%250Afolds%2520and%2520hairstyles.%2520All%2520multi-view%2520gains%2520are%2520achieved%2520without%2520requiring%2520any%250Anetwork%2520re-training.%2520Experimental%2520results%2520show%2520that%2520MExECON%2520consistently%250Aimproves%2520fidelity%2520over%2520the%2520single-view%2520baseline%2520and%2520achieves%2520competitive%250Aperformance%2520compared%2520to%2520modern%2520few-shot%25203D%2520reconstruction%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MExECON%3A%20Multi-view%20Extended%20Explicit%20Clothed%20humans%20Optimized%20via%0A%20%20Normal%20integration&entry.906535625=Fulden%20Ece%20U%C4%9Fur%20and%20Rafael%20Redondo%20and%20Albert%20Barreiro%20and%20Stefan%20Hristov%20and%20Roger%20Mar%C3%AD&entry.1292438233=%20%20This%20work%20presents%20MExECON%2C%20a%20novel%20pipeline%20for%203D%20reconstruction%20of%20clothed%0Ahuman%20avatars%20from%20sparse%20multi-view%20RGB%20images.%20Building%20on%20the%20single-view%0Amethod%20ECON%2C%20MExECON%20extends%20its%20capabilities%20to%20leverage%20multiple%20viewpoints%2C%0Aimproving%20geometry%20and%20body%20pose%20estimation.%20At%20the%20core%20of%20the%20pipeline%20is%20the%0Aproposed%20Joint%20Multi-view%20Body%20Optimization%20%28JMBO%29%20algorithm%2C%20which%20fits%20a%0Asingle%20SMPL-X%20body%20model%20jointly%20across%20all%20input%20views%2C%20enforcing%20multi-view%0Aconsistency.%20The%20optimized%20body%20model%20serves%20as%20a%20low-frequency%20prior%20that%0Aguides%20the%20subsequent%20surface%20reconstruction%2C%20where%20geometric%20details%20are%20added%0Avia%20normal%20map%20integration.%20MExECON%20integrates%20normal%20maps%20from%20both%20front%20and%0Aback%20views%20to%20accurately%20capture%20fine-grained%20surface%20details%20such%20as%20clothing%0Afolds%20and%20hairstyles.%20All%20multi-view%20gains%20are%20achieved%20without%20requiring%20any%0Anetwork%20re-training.%20Experimental%20results%20show%20that%20MExECON%20consistently%0Aimproves%20fidelity%20over%20the%20single-view%20baseline%20and%20achieves%20competitive%0Aperformance%20compared%20to%20modern%20few-shot%203D%20reconstruction%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15500v1&entry.124074799=Read"},
{"title": "ATLAS: Decoupling Skeletal and Shape Parameters for Expressive\n  Parametric Human Modeling", "author": "Jinhyung Park and Javier Romero and Shunsuke Saito and Fabian Prada and Takaaki Shiratori and Yichen Xu and Federica Bogo and Shoou-I Yu and Kris Kitani and Rawal Khirodkar", "abstract": "  Parametric body models offer expressive 3D representation of humans across a\nwide range of poses, shapes, and facial expressions, typically derived by\nlearning a basis over registered 3D meshes. However, existing human mesh\nmodeling approaches struggle to capture detailed variations across diverse body\nposes and shapes, largely due to limited training data diversity and\nrestrictive modeling assumptions. Moreover, the common paradigm first optimizes\nthe external body surface using a linear basis, then regresses internal\nskeletal joints from surface vertices. This approach introduces problematic\ndependencies between internal skeleton and outer soft tissue, limiting direct\ncontrol over body height and bone lengths. To address these issues, we present\nATLAS, a high-fidelity body model learned from 600k high-resolution scans\ncaptured using 240 synchronized cameras. Unlike previous methods, we explicitly\ndecouple the shape and skeleton bases by grounding our mesh representation in\nthe human skeleton. This decoupling enables enhanced shape expressivity,\nfine-grained customization of body attributes, and keypoint fitting independent\nof external soft-tissue characteristics. ATLAS outperforms existing methods by\nfitting unseen subjects in diverse poses more accurately, and quantitative\nevaluations show that our non-linear pose correctives more effectively capture\ncomplex poses compared to linear models.\n", "link": "http://arxiv.org/abs/2508.15767v1", "date": "2025-08-21", "relevancy": 3.0335, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.622}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6075}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ATLAS%3A%20Decoupling%20Skeletal%20and%20Shape%20Parameters%20for%20Expressive%0A%20%20Parametric%20Human%20Modeling&body=Title%3A%20ATLAS%3A%20Decoupling%20Skeletal%20and%20Shape%20Parameters%20for%20Expressive%0A%20%20Parametric%20Human%20Modeling%0AAuthor%3A%20Jinhyung%20Park%20and%20Javier%20Romero%20and%20Shunsuke%20Saito%20and%20Fabian%20Prada%20and%20Takaaki%20Shiratori%20and%20Yichen%20Xu%20and%20Federica%20Bogo%20and%20Shoou-I%20Yu%20and%20Kris%20Kitani%20and%20Rawal%20Khirodkar%0AAbstract%3A%20%20%20Parametric%20body%20models%20offer%20expressive%203D%20representation%20of%20humans%20across%20a%0Awide%20range%20of%20poses%2C%20shapes%2C%20and%20facial%20expressions%2C%20typically%20derived%20by%0Alearning%20a%20basis%20over%20registered%203D%20meshes.%20However%2C%20existing%20human%20mesh%0Amodeling%20approaches%20struggle%20to%20capture%20detailed%20variations%20across%20diverse%20body%0Aposes%20and%20shapes%2C%20largely%20due%20to%20limited%20training%20data%20diversity%20and%0Arestrictive%20modeling%20assumptions.%20Moreover%2C%20the%20common%20paradigm%20first%20optimizes%0Athe%20external%20body%20surface%20using%20a%20linear%20basis%2C%20then%20regresses%20internal%0Askeletal%20joints%20from%20surface%20vertices.%20This%20approach%20introduces%20problematic%0Adependencies%20between%20internal%20skeleton%20and%20outer%20soft%20tissue%2C%20limiting%20direct%0Acontrol%20over%20body%20height%20and%20bone%20lengths.%20To%20address%20these%20issues%2C%20we%20present%0AATLAS%2C%20a%20high-fidelity%20body%20model%20learned%20from%20600k%20high-resolution%20scans%0Acaptured%20using%20240%20synchronized%20cameras.%20Unlike%20previous%20methods%2C%20we%20explicitly%0Adecouple%20the%20shape%20and%20skeleton%20bases%20by%20grounding%20our%20mesh%20representation%20in%0Athe%20human%20skeleton.%20This%20decoupling%20enables%20enhanced%20shape%20expressivity%2C%0Afine-grained%20customization%20of%20body%20attributes%2C%20and%20keypoint%20fitting%20independent%0Aof%20external%20soft-tissue%20characteristics.%20ATLAS%20outperforms%20existing%20methods%20by%0Afitting%20unseen%20subjects%20in%20diverse%20poses%20more%20accurately%2C%20and%20quantitative%0Aevaluations%20show%20that%20our%20non-linear%20pose%20correctives%20more%20effectively%20capture%0Acomplex%20poses%20compared%20to%20linear%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15767v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DATLAS%253A%2520Decoupling%2520Skeletal%2520and%2520Shape%2520Parameters%2520for%2520Expressive%250A%2520%2520Parametric%2520Human%2520Modeling%26entry.906535625%3DJinhyung%2520Park%2520and%2520Javier%2520Romero%2520and%2520Shunsuke%2520Saito%2520and%2520Fabian%2520Prada%2520and%2520Takaaki%2520Shiratori%2520and%2520Yichen%2520Xu%2520and%2520Federica%2520Bogo%2520and%2520Shoou-I%2520Yu%2520and%2520Kris%2520Kitani%2520and%2520Rawal%2520Khirodkar%26entry.1292438233%3D%2520%2520Parametric%2520body%2520models%2520offer%2520expressive%25203D%2520representation%2520of%2520humans%2520across%2520a%250Awide%2520range%2520of%2520poses%252C%2520shapes%252C%2520and%2520facial%2520expressions%252C%2520typically%2520derived%2520by%250Alearning%2520a%2520basis%2520over%2520registered%25203D%2520meshes.%2520However%252C%2520existing%2520human%2520mesh%250Amodeling%2520approaches%2520struggle%2520to%2520capture%2520detailed%2520variations%2520across%2520diverse%2520body%250Aposes%2520and%2520shapes%252C%2520largely%2520due%2520to%2520limited%2520training%2520data%2520diversity%2520and%250Arestrictive%2520modeling%2520assumptions.%2520Moreover%252C%2520the%2520common%2520paradigm%2520first%2520optimizes%250Athe%2520external%2520body%2520surface%2520using%2520a%2520linear%2520basis%252C%2520then%2520regresses%2520internal%250Askeletal%2520joints%2520from%2520surface%2520vertices.%2520This%2520approach%2520introduces%2520problematic%250Adependencies%2520between%2520internal%2520skeleton%2520and%2520outer%2520soft%2520tissue%252C%2520limiting%2520direct%250Acontrol%2520over%2520body%2520height%2520and%2520bone%2520lengths.%2520To%2520address%2520these%2520issues%252C%2520we%2520present%250AATLAS%252C%2520a%2520high-fidelity%2520body%2520model%2520learned%2520from%2520600k%2520high-resolution%2520scans%250Acaptured%2520using%2520240%2520synchronized%2520cameras.%2520Unlike%2520previous%2520methods%252C%2520we%2520explicitly%250Adecouple%2520the%2520shape%2520and%2520skeleton%2520bases%2520by%2520grounding%2520our%2520mesh%2520representation%2520in%250Athe%2520human%2520skeleton.%2520This%2520decoupling%2520enables%2520enhanced%2520shape%2520expressivity%252C%250Afine-grained%2520customization%2520of%2520body%2520attributes%252C%2520and%2520keypoint%2520fitting%2520independent%250Aof%2520external%2520soft-tissue%2520characteristics.%2520ATLAS%2520outperforms%2520existing%2520methods%2520by%250Afitting%2520unseen%2520subjects%2520in%2520diverse%2520poses%2520more%2520accurately%252C%2520and%2520quantitative%250Aevaluations%2520show%2520that%2520our%2520non-linear%2520pose%2520correctives%2520more%2520effectively%2520capture%250Acomplex%2520poses%2520compared%2520to%2520linear%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15767v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ATLAS%3A%20Decoupling%20Skeletal%20and%20Shape%20Parameters%20for%20Expressive%0A%20%20Parametric%20Human%20Modeling&entry.906535625=Jinhyung%20Park%20and%20Javier%20Romero%20and%20Shunsuke%20Saito%20and%20Fabian%20Prada%20and%20Takaaki%20Shiratori%20and%20Yichen%20Xu%20and%20Federica%20Bogo%20and%20Shoou-I%20Yu%20and%20Kris%20Kitani%20and%20Rawal%20Khirodkar&entry.1292438233=%20%20Parametric%20body%20models%20offer%20expressive%203D%20representation%20of%20humans%20across%20a%0Awide%20range%20of%20poses%2C%20shapes%2C%20and%20facial%20expressions%2C%20typically%20derived%20by%0Alearning%20a%20basis%20over%20registered%203D%20meshes.%20However%2C%20existing%20human%20mesh%0Amodeling%20approaches%20struggle%20to%20capture%20detailed%20variations%20across%20diverse%20body%0Aposes%20and%20shapes%2C%20largely%20due%20to%20limited%20training%20data%20diversity%20and%0Arestrictive%20modeling%20assumptions.%20Moreover%2C%20the%20common%20paradigm%20first%20optimizes%0Athe%20external%20body%20surface%20using%20a%20linear%20basis%2C%20then%20regresses%20internal%0Askeletal%20joints%20from%20surface%20vertices.%20This%20approach%20introduces%20problematic%0Adependencies%20between%20internal%20skeleton%20and%20outer%20soft%20tissue%2C%20limiting%20direct%0Acontrol%20over%20body%20height%20and%20bone%20lengths.%20To%20address%20these%20issues%2C%20we%20present%0AATLAS%2C%20a%20high-fidelity%20body%20model%20learned%20from%20600k%20high-resolution%20scans%0Acaptured%20using%20240%20synchronized%20cameras.%20Unlike%20previous%20methods%2C%20we%20explicitly%0Adecouple%20the%20shape%20and%20skeleton%20bases%20by%20grounding%20our%20mesh%20representation%20in%0Athe%20human%20skeleton.%20This%20decoupling%20enables%20enhanced%20shape%20expressivity%2C%0Afine-grained%20customization%20of%20body%20attributes%2C%20and%20keypoint%20fitting%20independent%0Aof%20external%20soft-tissue%20characteristics.%20ATLAS%20outperforms%20existing%20methods%20by%0Afitting%20unseen%20subjects%20in%20diverse%20poses%20more%20accurately%2C%20and%20quantitative%0Aevaluations%20show%20that%20our%20non-linear%20pose%20correctives%20more%20effectively%20capture%0Acomplex%20poses%20compared%20to%20linear%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15767v1&entry.124074799=Read"},
{"title": "CM2LoD3: Reconstructing LoD3 Building Models Using Semantic Conflict\n  Maps", "author": "Franz Hanke and Antonia Bieringer and Olaf Wysocki and Boris Jutzi", "abstract": "  Detailed 3D building models are crucial for urban planning, digital twins,\nand disaster management applications. While Level of Detail 1 (LoD)1 and LoD2\nbuilding models are widely available, they lack detailed facade elements\nessential for advanced urban analysis. In contrast, LoD3 models address this\nlimitation by incorporating facade elements such as windows, doors, and\nunderpasses. However, their generation has traditionally required manual\nmodeling, making large-scale adoption challenging. In this contribution,\nCM2LoD3, we present a novel method for reconstructing LoD3 building models\nleveraging Conflict Maps (CMs) obtained from ray-to-model-prior analysis.\nUnlike previous works, we concentrate on semantically segmenting real-world CMs\nwith synthetically generated CMs from our developed Semantic Conflict Map\nGenerator (SCMG). We also observe that additional segmentation of textured\nmodels can be fused with CMs using confidence scores to further increase\nsegmentation performance and thus increase 3D reconstruction accuracy.\nExperimental results demonstrate the effectiveness of our CM2LoD3 method in\nsegmenting and reconstructing building openings, with the 61% performance with\nuncertainty-aware fusion of segmented building textures. This research\ncontributes to the advancement of automated LoD3 model reconstruction, paving\nthe way for scalable and efficient 3D city modeling. Our project is available:\nhttps://github.com/InFraHank/CM2LoD3\n", "link": "http://arxiv.org/abs/2508.15672v1", "date": "2025-08-21", "relevancy": 2.9904, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6016}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6016}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CM2LoD3%3A%20Reconstructing%20LoD3%20Building%20Models%20Using%20Semantic%20Conflict%0A%20%20Maps&body=Title%3A%20CM2LoD3%3A%20Reconstructing%20LoD3%20Building%20Models%20Using%20Semantic%20Conflict%0A%20%20Maps%0AAuthor%3A%20Franz%20Hanke%20and%20Antonia%20Bieringer%20and%20Olaf%20Wysocki%20and%20Boris%20Jutzi%0AAbstract%3A%20%20%20Detailed%203D%20building%20models%20are%20crucial%20for%20urban%20planning%2C%20digital%20twins%2C%0Aand%20disaster%20management%20applications.%20While%20Level%20of%20Detail%201%20%28LoD%291%20and%20LoD2%0Abuilding%20models%20are%20widely%20available%2C%20they%20lack%20detailed%20facade%20elements%0Aessential%20for%20advanced%20urban%20analysis.%20In%20contrast%2C%20LoD3%20models%20address%20this%0Alimitation%20by%20incorporating%20facade%20elements%20such%20as%20windows%2C%20doors%2C%20and%0Aunderpasses.%20However%2C%20their%20generation%20has%20traditionally%20required%20manual%0Amodeling%2C%20making%20large-scale%20adoption%20challenging.%20In%20this%20contribution%2C%0ACM2LoD3%2C%20we%20present%20a%20novel%20method%20for%20reconstructing%20LoD3%20building%20models%0Aleveraging%20Conflict%20Maps%20%28CMs%29%20obtained%20from%20ray-to-model-prior%20analysis.%0AUnlike%20previous%20works%2C%20we%20concentrate%20on%20semantically%20segmenting%20real-world%20CMs%0Awith%20synthetically%20generated%20CMs%20from%20our%20developed%20Semantic%20Conflict%20Map%0AGenerator%20%28SCMG%29.%20We%20also%20observe%20that%20additional%20segmentation%20of%20textured%0Amodels%20can%20be%20fused%20with%20CMs%20using%20confidence%20scores%20to%20further%20increase%0Asegmentation%20performance%20and%20thus%20increase%203D%20reconstruction%20accuracy.%0AExperimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20CM2LoD3%20method%20in%0Asegmenting%20and%20reconstructing%20building%20openings%2C%20with%20the%2061%25%20performance%20with%0Auncertainty-aware%20fusion%20of%20segmented%20building%20textures.%20This%20research%0Acontributes%20to%20the%20advancement%20of%20automated%20LoD3%20model%20reconstruction%2C%20paving%0Athe%20way%20for%20scalable%20and%20efficient%203D%20city%20modeling.%20Our%20project%20is%20available%3A%0Ahttps%3A//github.com/InFraHank/CM2LoD3%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCM2LoD3%253A%2520Reconstructing%2520LoD3%2520Building%2520Models%2520Using%2520Semantic%2520Conflict%250A%2520%2520Maps%26entry.906535625%3DFranz%2520Hanke%2520and%2520Antonia%2520Bieringer%2520and%2520Olaf%2520Wysocki%2520and%2520Boris%2520Jutzi%26entry.1292438233%3D%2520%2520Detailed%25203D%2520building%2520models%2520are%2520crucial%2520for%2520urban%2520planning%252C%2520digital%2520twins%252C%250Aand%2520disaster%2520management%2520applications.%2520While%2520Level%2520of%2520Detail%25201%2520%2528LoD%25291%2520and%2520LoD2%250Abuilding%2520models%2520are%2520widely%2520available%252C%2520they%2520lack%2520detailed%2520facade%2520elements%250Aessential%2520for%2520advanced%2520urban%2520analysis.%2520In%2520contrast%252C%2520LoD3%2520models%2520address%2520this%250Alimitation%2520by%2520incorporating%2520facade%2520elements%2520such%2520as%2520windows%252C%2520doors%252C%2520and%250Aunderpasses.%2520However%252C%2520their%2520generation%2520has%2520traditionally%2520required%2520manual%250Amodeling%252C%2520making%2520large-scale%2520adoption%2520challenging.%2520In%2520this%2520contribution%252C%250ACM2LoD3%252C%2520we%2520present%2520a%2520novel%2520method%2520for%2520reconstructing%2520LoD3%2520building%2520models%250Aleveraging%2520Conflict%2520Maps%2520%2528CMs%2529%2520obtained%2520from%2520ray-to-model-prior%2520analysis.%250AUnlike%2520previous%2520works%252C%2520we%2520concentrate%2520on%2520semantically%2520segmenting%2520real-world%2520CMs%250Awith%2520synthetically%2520generated%2520CMs%2520from%2520our%2520developed%2520Semantic%2520Conflict%2520Map%250AGenerator%2520%2528SCMG%2529.%2520We%2520also%2520observe%2520that%2520additional%2520segmentation%2520of%2520textured%250Amodels%2520can%2520be%2520fused%2520with%2520CMs%2520using%2520confidence%2520scores%2520to%2520further%2520increase%250Asegmentation%2520performance%2520and%2520thus%2520increase%25203D%2520reconstruction%2520accuracy.%250AExperimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520CM2LoD3%2520method%2520in%250Asegmenting%2520and%2520reconstructing%2520building%2520openings%252C%2520with%2520the%252061%2525%2520performance%2520with%250Auncertainty-aware%2520fusion%2520of%2520segmented%2520building%2520textures.%2520This%2520research%250Acontributes%2520to%2520the%2520advancement%2520of%2520automated%2520LoD3%2520model%2520reconstruction%252C%2520paving%250Athe%2520way%2520for%2520scalable%2520and%2520efficient%25203D%2520city%2520modeling.%2520Our%2520project%2520is%2520available%253A%250Ahttps%253A//github.com/InFraHank/CM2LoD3%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CM2LoD3%3A%20Reconstructing%20LoD3%20Building%20Models%20Using%20Semantic%20Conflict%0A%20%20Maps&entry.906535625=Franz%20Hanke%20and%20Antonia%20Bieringer%20and%20Olaf%20Wysocki%20and%20Boris%20Jutzi&entry.1292438233=%20%20Detailed%203D%20building%20models%20are%20crucial%20for%20urban%20planning%2C%20digital%20twins%2C%0Aand%20disaster%20management%20applications.%20While%20Level%20of%20Detail%201%20%28LoD%291%20and%20LoD2%0Abuilding%20models%20are%20widely%20available%2C%20they%20lack%20detailed%20facade%20elements%0Aessential%20for%20advanced%20urban%20analysis.%20In%20contrast%2C%20LoD3%20models%20address%20this%0Alimitation%20by%20incorporating%20facade%20elements%20such%20as%20windows%2C%20doors%2C%20and%0Aunderpasses.%20However%2C%20their%20generation%20has%20traditionally%20required%20manual%0Amodeling%2C%20making%20large-scale%20adoption%20challenging.%20In%20this%20contribution%2C%0ACM2LoD3%2C%20we%20present%20a%20novel%20method%20for%20reconstructing%20LoD3%20building%20models%0Aleveraging%20Conflict%20Maps%20%28CMs%29%20obtained%20from%20ray-to-model-prior%20analysis.%0AUnlike%20previous%20works%2C%20we%20concentrate%20on%20semantically%20segmenting%20real-world%20CMs%0Awith%20synthetically%20generated%20CMs%20from%20our%20developed%20Semantic%20Conflict%20Map%0AGenerator%20%28SCMG%29.%20We%20also%20observe%20that%20additional%20segmentation%20of%20textured%0Amodels%20can%20be%20fused%20with%20CMs%20using%20confidence%20scores%20to%20further%20increase%0Asegmentation%20performance%20and%20thus%20increase%203D%20reconstruction%20accuracy.%0AExperimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20CM2LoD3%20method%20in%0Asegmenting%20and%20reconstructing%20building%20openings%2C%20with%20the%2061%25%20performance%20with%0Auncertainty-aware%20fusion%20of%20segmented%20building%20textures.%20This%20research%0Acontributes%20to%20the%20advancement%20of%20automated%20LoD3%20model%20reconstruction%2C%20paving%0Athe%20way%20for%20scalable%20and%20efficient%203D%20city%20modeling.%20Our%20project%20is%20available%3A%0Ahttps%3A//github.com/InFraHank/CM2LoD3%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15672v1&entry.124074799=Read"},
{"title": "Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling\n  Representation and Dynamically Fusing Features in CLIP", "author": "Ke Ma and Jun Long and Hongxiao Fei and Liujie Hua and Yiran Qian and Zhen Dai and Yueyi Luo", "abstract": "  Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap\nwhen applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of\nlocal inductive biases for dense prediction and their reliance on inflexible\nfeature fusion paradigms. We address these limitations through an Architectural\nCo-Design framework that jointly refines feature representation and cross-modal\nfusion. Our method integrates a parameter-efficient Convolutional Low-Rank\nAdaptation (Conv-LoRA) adapter to inject local inductive biases for\nfine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that\nleverages visual context to adaptively modulate text prompts, enabling a\npowerful bidirectional fusion. Extensive experiments on diverse industrial and\nmedical benchmarks demonstrate superior accuracy and robustness, validating\nthat this synergistic co-design is critical for robustly adapting foundation\nmodels to dense perception tasks.\n", "link": "http://arxiv.org/abs/2508.07819v2", "date": "2025-08-21", "relevancy": 2.97, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Architectural%20Co-Design%20for%20Zero-Shot%20Anomaly%20Detection%3A%20Decoupling%0A%20%20Representation%20and%20Dynamically%20Fusing%20Features%20in%20CLIP&body=Title%3A%20Architectural%20Co-Design%20for%20Zero-Shot%20Anomaly%20Detection%3A%20Decoupling%0A%20%20Representation%20and%20Dynamically%20Fusing%20Features%20in%20CLIP%0AAuthor%3A%20Ke%20Ma%20and%20Jun%20Long%20and%20Hongxiao%20Fei%20and%20Liujie%20Hua%20and%20Yiran%20Qian%20and%20Zhen%20Dai%20and%20Yueyi%20Luo%0AAbstract%3A%20%20%20Pre-trained%20Vision-Language%20Models%20%28VLMs%29%20face%20a%20significant%20adaptation%20gap%0Awhen%20applied%20to%20Zero-Shot%20Anomaly%20Detection%20%28ZSAD%29%2C%20stemming%20from%20their%20lack%20of%0Alocal%20inductive%20biases%20for%20dense%20prediction%20and%20their%20reliance%20on%20inflexible%0Afeature%20fusion%20paradigms.%20We%20address%20these%20limitations%20through%20an%20Architectural%0ACo-Design%20framework%20that%20jointly%20refines%20feature%20representation%20and%20cross-modal%0Afusion.%20Our%20method%20integrates%20a%20parameter-efficient%20Convolutional%20Low-Rank%0AAdaptation%20%28Conv-LoRA%29%20adapter%20to%20inject%20local%20inductive%20biases%20for%0Afine-grained%20representation%2C%20and%20introduces%20a%20Dynamic%20Fusion%20Gateway%20%28DFG%29%20that%0Aleverages%20visual%20context%20to%20adaptively%20modulate%20text%20prompts%2C%20enabling%20a%0Apowerful%20bidirectional%20fusion.%20Extensive%20experiments%20on%20diverse%20industrial%20and%0Amedical%20benchmarks%20demonstrate%20superior%20accuracy%20and%20robustness%2C%20validating%0Athat%20this%20synergistic%20co-design%20is%20critical%20for%20robustly%20adapting%20foundation%0Amodels%20to%20dense%20perception%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07819v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArchitectural%2520Co-Design%2520for%2520Zero-Shot%2520Anomaly%2520Detection%253A%2520Decoupling%250A%2520%2520Representation%2520and%2520Dynamically%2520Fusing%2520Features%2520in%2520CLIP%26entry.906535625%3DKe%2520Ma%2520and%2520Jun%2520Long%2520and%2520Hongxiao%2520Fei%2520and%2520Liujie%2520Hua%2520and%2520Yiran%2520Qian%2520and%2520Zhen%2520Dai%2520and%2520Yueyi%2520Luo%26entry.1292438233%3D%2520%2520Pre-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520face%2520a%2520significant%2520adaptation%2520gap%250Awhen%2520applied%2520to%2520Zero-Shot%2520Anomaly%2520Detection%2520%2528ZSAD%2529%252C%2520stemming%2520from%2520their%2520lack%2520of%250Alocal%2520inductive%2520biases%2520for%2520dense%2520prediction%2520and%2520their%2520reliance%2520on%2520inflexible%250Afeature%2520fusion%2520paradigms.%2520We%2520address%2520these%2520limitations%2520through%2520an%2520Architectural%250ACo-Design%2520framework%2520that%2520jointly%2520refines%2520feature%2520representation%2520and%2520cross-modal%250Afusion.%2520Our%2520method%2520integrates%2520a%2520parameter-efficient%2520Convolutional%2520Low-Rank%250AAdaptation%2520%2528Conv-LoRA%2529%2520adapter%2520to%2520inject%2520local%2520inductive%2520biases%2520for%250Afine-grained%2520representation%252C%2520and%2520introduces%2520a%2520Dynamic%2520Fusion%2520Gateway%2520%2528DFG%2529%2520that%250Aleverages%2520visual%2520context%2520to%2520adaptively%2520modulate%2520text%2520prompts%252C%2520enabling%2520a%250Apowerful%2520bidirectional%2520fusion.%2520Extensive%2520experiments%2520on%2520diverse%2520industrial%2520and%250Amedical%2520benchmarks%2520demonstrate%2520superior%2520accuracy%2520and%2520robustness%252C%2520validating%250Athat%2520this%2520synergistic%2520co-design%2520is%2520critical%2520for%2520robustly%2520adapting%2520foundation%250Amodels%2520to%2520dense%2520perception%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07819v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Architectural%20Co-Design%20for%20Zero-Shot%20Anomaly%20Detection%3A%20Decoupling%0A%20%20Representation%20and%20Dynamically%20Fusing%20Features%20in%20CLIP&entry.906535625=Ke%20Ma%20and%20Jun%20Long%20and%20Hongxiao%20Fei%20and%20Liujie%20Hua%20and%20Yiran%20Qian%20and%20Zhen%20Dai%20and%20Yueyi%20Luo&entry.1292438233=%20%20Pre-trained%20Vision-Language%20Models%20%28VLMs%29%20face%20a%20significant%20adaptation%20gap%0Awhen%20applied%20to%20Zero-Shot%20Anomaly%20Detection%20%28ZSAD%29%2C%20stemming%20from%20their%20lack%20of%0Alocal%20inductive%20biases%20for%20dense%20prediction%20and%20their%20reliance%20on%20inflexible%0Afeature%20fusion%20paradigms.%20We%20address%20these%20limitations%20through%20an%20Architectural%0ACo-Design%20framework%20that%20jointly%20refines%20feature%20representation%20and%20cross-modal%0Afusion.%20Our%20method%20integrates%20a%20parameter-efficient%20Convolutional%20Low-Rank%0AAdaptation%20%28Conv-LoRA%29%20adapter%20to%20inject%20local%20inductive%20biases%20for%0Afine-grained%20representation%2C%20and%20introduces%20a%20Dynamic%20Fusion%20Gateway%20%28DFG%29%20that%0Aleverages%20visual%20context%20to%20adaptively%20modulate%20text%20prompts%2C%20enabling%20a%0Apowerful%20bidirectional%20fusion.%20Extensive%20experiments%20on%20diverse%20industrial%20and%0Amedical%20benchmarks%20demonstrate%20superior%20accuracy%20and%20robustness%2C%20validating%0Athat%20this%20synergistic%20co-design%20is%20critical%20for%20robustly%20adapting%20foundation%0Amodels%20to%20dense%20perception%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07819v2&entry.124074799=Read"},
{"title": "LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning\n  under Long-Tailed Distributions", "author": "Yongju Jia and Jiarui Ma and Xiangxian Li and Baiqiao Zhang and Xianhui Cao and Juan Liu and Yulong Bian", "abstract": "  Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated\nimpressive capability in visual tasks, but their fine-tuning often suffers from\nbias in class-imbalanced scene. Recent works have introduced large language\nmodels (LLMs) to enhance VLM fine-tuning with supplementing semantic\ninformation. However, they often overlook inherent class imbalance in VLMs'\npre-training, which may lead to bias accumulation in downstream tasks. To\naddress this problem, this paper proposes a Multi-dimensional Dynamic Prompt\nRouting (MDPR) framework. MDPR constructs a comprehensive knowledge base for\nclasses, spanning five visual-semantic dimensions. During fine-tuning, the\ndynamic routing mechanism aligns global visual classes, retrieves optimal\nprompts, and balances fine-grained semantics, yielding stable predictions\nthrough logits fusion. Extensive experiments on long-tailed benchmarks,\nincluding CIFAR-LT, ImageNet-LT, and Places-LT, demonstrate that MDPR achieves\ncomparable results with current SOTA methods. Ablation studies further confirm\nthe effectiveness of our semantic library for tail classes, and show that our\ndynamic routing incurs minimal computational overhead, making MDPR a flexible\nand efficient enhancement for VLM fine-tuning under data imbalance.\n", "link": "http://arxiv.org/abs/2508.15688v1", "date": "2025-08-21", "relevancy": 2.953, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5989}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5865}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-empowered%20Dynamic%20Prompt%20Routing%20for%20Vision-Language%20Models%20Tuning%0A%20%20under%20Long-Tailed%20Distributions&body=Title%3A%20LLM-empowered%20Dynamic%20Prompt%20Routing%20for%20Vision-Language%20Models%20Tuning%0A%20%20under%20Long-Tailed%20Distributions%0AAuthor%3A%20Yongju%20Jia%20and%20Jiarui%20Ma%20and%20Xiangxian%20Li%20and%20Baiqiao%20Zhang%20and%20Xianhui%20Cao%20and%20Juan%20Liu%20and%20Yulong%20Bian%0AAbstract%3A%20%20%20Pre-trained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%20demonstrated%0Aimpressive%20capability%20in%20visual%20tasks%2C%20but%20their%20fine-tuning%20often%20suffers%20from%0Abias%20in%20class-imbalanced%20scene.%20Recent%20works%20have%20introduced%20large%20language%0Amodels%20%28LLMs%29%20to%20enhance%20VLM%20fine-tuning%20with%20supplementing%20semantic%0Ainformation.%20However%2C%20they%20often%20overlook%20inherent%20class%20imbalance%20in%20VLMs%27%0Apre-training%2C%20which%20may%20lead%20to%20bias%20accumulation%20in%20downstream%20tasks.%20To%0Aaddress%20this%20problem%2C%20this%20paper%20proposes%20a%20Multi-dimensional%20Dynamic%20Prompt%0ARouting%20%28MDPR%29%20framework.%20MDPR%20constructs%20a%20comprehensive%20knowledge%20base%20for%0Aclasses%2C%20spanning%20five%20visual-semantic%20dimensions.%20During%20fine-tuning%2C%20the%0Adynamic%20routing%20mechanism%20aligns%20global%20visual%20classes%2C%20retrieves%20optimal%0Aprompts%2C%20and%20balances%20fine-grained%20semantics%2C%20yielding%20stable%20predictions%0Athrough%20logits%20fusion.%20Extensive%20experiments%20on%20long-tailed%20benchmarks%2C%0Aincluding%20CIFAR-LT%2C%20ImageNet-LT%2C%20and%20Places-LT%2C%20demonstrate%20that%20MDPR%20achieves%0Acomparable%20results%20with%20current%20SOTA%20methods.%20Ablation%20studies%20further%20confirm%0Athe%20effectiveness%20of%20our%20semantic%20library%20for%20tail%20classes%2C%20and%20show%20that%20our%0Adynamic%20routing%20incurs%20minimal%20computational%20overhead%2C%20making%20MDPR%20a%20flexible%0Aand%20efficient%20enhancement%20for%20VLM%20fine-tuning%20under%20data%20imbalance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-empowered%2520Dynamic%2520Prompt%2520Routing%2520for%2520Vision-Language%2520Models%2520Tuning%250A%2520%2520under%2520Long-Tailed%2520Distributions%26entry.906535625%3DYongju%2520Jia%2520and%2520Jiarui%2520Ma%2520and%2520Xiangxian%2520Li%2520and%2520Baiqiao%2520Zhang%2520and%2520Xianhui%2520Cao%2520and%2520Juan%2520Liu%2520and%2520Yulong%2520Bian%26entry.1292438233%3D%2520%2520Pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%252C%2520have%2520demonstrated%250Aimpressive%2520capability%2520in%2520visual%2520tasks%252C%2520but%2520their%2520fine-tuning%2520often%2520suffers%2520from%250Abias%2520in%2520class-imbalanced%2520scene.%2520Recent%2520works%2520have%2520introduced%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520to%2520enhance%2520VLM%2520fine-tuning%2520with%2520supplementing%2520semantic%250Ainformation.%2520However%252C%2520they%2520often%2520overlook%2520inherent%2520class%2520imbalance%2520in%2520VLMs%2527%250Apre-training%252C%2520which%2520may%2520lead%2520to%2520bias%2520accumulation%2520in%2520downstream%2520tasks.%2520To%250Aaddress%2520this%2520problem%252C%2520this%2520paper%2520proposes%2520a%2520Multi-dimensional%2520Dynamic%2520Prompt%250ARouting%2520%2528MDPR%2529%2520framework.%2520MDPR%2520constructs%2520a%2520comprehensive%2520knowledge%2520base%2520for%250Aclasses%252C%2520spanning%2520five%2520visual-semantic%2520dimensions.%2520During%2520fine-tuning%252C%2520the%250Adynamic%2520routing%2520mechanism%2520aligns%2520global%2520visual%2520classes%252C%2520retrieves%2520optimal%250Aprompts%252C%2520and%2520balances%2520fine-grained%2520semantics%252C%2520yielding%2520stable%2520predictions%250Athrough%2520logits%2520fusion.%2520Extensive%2520experiments%2520on%2520long-tailed%2520benchmarks%252C%250Aincluding%2520CIFAR-LT%252C%2520ImageNet-LT%252C%2520and%2520Places-LT%252C%2520demonstrate%2520that%2520MDPR%2520achieves%250Acomparable%2520results%2520with%2520current%2520SOTA%2520methods.%2520Ablation%2520studies%2520further%2520confirm%250Athe%2520effectiveness%2520of%2520our%2520semantic%2520library%2520for%2520tail%2520classes%252C%2520and%2520show%2520that%2520our%250Adynamic%2520routing%2520incurs%2520minimal%2520computational%2520overhead%252C%2520making%2520MDPR%2520a%2520flexible%250Aand%2520efficient%2520enhancement%2520for%2520VLM%2520fine-tuning%2520under%2520data%2520imbalance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-empowered%20Dynamic%20Prompt%20Routing%20for%20Vision-Language%20Models%20Tuning%0A%20%20under%20Long-Tailed%20Distributions&entry.906535625=Yongju%20Jia%20and%20Jiarui%20Ma%20and%20Xiangxian%20Li%20and%20Baiqiao%20Zhang%20and%20Xianhui%20Cao%20and%20Juan%20Liu%20and%20Yulong%20Bian&entry.1292438233=%20%20Pre-trained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%20demonstrated%0Aimpressive%20capability%20in%20visual%20tasks%2C%20but%20their%20fine-tuning%20often%20suffers%20from%0Abias%20in%20class-imbalanced%20scene.%20Recent%20works%20have%20introduced%20large%20language%0Amodels%20%28LLMs%29%20to%20enhance%20VLM%20fine-tuning%20with%20supplementing%20semantic%0Ainformation.%20However%2C%20they%20often%20overlook%20inherent%20class%20imbalance%20in%20VLMs%27%0Apre-training%2C%20which%20may%20lead%20to%20bias%20accumulation%20in%20downstream%20tasks.%20To%0Aaddress%20this%20problem%2C%20this%20paper%20proposes%20a%20Multi-dimensional%20Dynamic%20Prompt%0ARouting%20%28MDPR%29%20framework.%20MDPR%20constructs%20a%20comprehensive%20knowledge%20base%20for%0Aclasses%2C%20spanning%20five%20visual-semantic%20dimensions.%20During%20fine-tuning%2C%20the%0Adynamic%20routing%20mechanism%20aligns%20global%20visual%20classes%2C%20retrieves%20optimal%0Aprompts%2C%20and%20balances%20fine-grained%20semantics%2C%20yielding%20stable%20predictions%0Athrough%20logits%20fusion.%20Extensive%20experiments%20on%20long-tailed%20benchmarks%2C%0Aincluding%20CIFAR-LT%2C%20ImageNet-LT%2C%20and%20Places-LT%2C%20demonstrate%20that%20MDPR%20achieves%0Acomparable%20results%20with%20current%20SOTA%20methods.%20Ablation%20studies%20further%20confirm%0Athe%20effectiveness%20of%20our%20semantic%20library%20for%20tail%20classes%2C%20and%20show%20that%20our%0Adynamic%20routing%20incurs%20minimal%20computational%20overhead%2C%20making%20MDPR%20a%20flexible%0Aand%20efficient%20enhancement%20for%20VLM%20fine-tuning%20under%20data%20imbalance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15688v1&entry.124074799=Read"},
{"title": "High-Frequency First: A Two-Stage Approach for Improving Image INR", "author": "Sumit Kumar Dam and Mrityunjoy Gain and Eui-Nam Huh and Choong Seon Hong", "abstract": "  Implicit Neural Representations (INRs) have emerged as a powerful alternative\nto traditional pixel-based formats by modeling images as continuous functions\nover spatial coordinates. A key challenge, however, lies in the spectral bias\nof neural networks, which tend to favor low-frequency components while\nstruggling to capture high-frequency (HF) details such as sharp edges and fine\ntextures. While prior approaches have addressed this limitation through\narchitectural modifications or specialized activation functions, we propose an\northogonal direction by directly guiding the training process. Specifically, we\nintroduce a two-stage training strategy where a neighbor-aware soft mask\nadaptively assigns higher weights to pixels with strong local variations,\nencouraging early focus on fine details. The model then transitions to\nfull-image training. Experimental results show that our approach consistently\nimproves reconstruction quality and complements existing INR methods. As a\npioneering attempt to assign frequency-aware importance to pixels in image INR,\nour work offers a new avenue for mitigating the spectral bias problem.\n", "link": "http://arxiv.org/abs/2508.15582v1", "date": "2025-08-21", "relevancy": 2.9325, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6108}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5766}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Frequency%20First%3A%20A%20Two-Stage%20Approach%20for%20Improving%20Image%20INR&body=Title%3A%20High-Frequency%20First%3A%20A%20Two-Stage%20Approach%20for%20Improving%20Image%20INR%0AAuthor%3A%20Sumit%20Kumar%20Dam%20and%20Mrityunjoy%20Gain%20and%20Eui-Nam%20Huh%20and%20Choong%20Seon%20Hong%0AAbstract%3A%20%20%20Implicit%20Neural%20Representations%20%28INRs%29%20have%20emerged%20as%20a%20powerful%20alternative%0Ato%20traditional%20pixel-based%20formats%20by%20modeling%20images%20as%20continuous%20functions%0Aover%20spatial%20coordinates.%20A%20key%20challenge%2C%20however%2C%20lies%20in%20the%20spectral%20bias%0Aof%20neural%20networks%2C%20which%20tend%20to%20favor%20low-frequency%20components%20while%0Astruggling%20to%20capture%20high-frequency%20%28HF%29%20details%20such%20as%20sharp%20edges%20and%20fine%0Atextures.%20While%20prior%20approaches%20have%20addressed%20this%20limitation%20through%0Aarchitectural%20modifications%20or%20specialized%20activation%20functions%2C%20we%20propose%20an%0Aorthogonal%20direction%20by%20directly%20guiding%20the%20training%20process.%20Specifically%2C%20we%0Aintroduce%20a%20two-stage%20training%20strategy%20where%20a%20neighbor-aware%20soft%20mask%0Aadaptively%20assigns%20higher%20weights%20to%20pixels%20with%20strong%20local%20variations%2C%0Aencouraging%20early%20focus%20on%20fine%20details.%20The%20model%20then%20transitions%20to%0Afull-image%20training.%20Experimental%20results%20show%20that%20our%20approach%20consistently%0Aimproves%20reconstruction%20quality%20and%20complements%20existing%20INR%20methods.%20As%20a%0Apioneering%20attempt%20to%20assign%20frequency-aware%20importance%20to%20pixels%20in%20image%20INR%2C%0Aour%20work%20offers%20a%20new%20avenue%20for%20mitigating%20the%20spectral%20bias%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Frequency%2520First%253A%2520A%2520Two-Stage%2520Approach%2520for%2520Improving%2520Image%2520INR%26entry.906535625%3DSumit%2520Kumar%2520Dam%2520and%2520Mrityunjoy%2520Gain%2520and%2520Eui-Nam%2520Huh%2520and%2520Choong%2520Seon%2520Hong%26entry.1292438233%3D%2520%2520Implicit%2520Neural%2520Representations%2520%2528INRs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520alternative%250Ato%2520traditional%2520pixel-based%2520formats%2520by%2520modeling%2520images%2520as%2520continuous%2520functions%250Aover%2520spatial%2520coordinates.%2520A%2520key%2520challenge%252C%2520however%252C%2520lies%2520in%2520the%2520spectral%2520bias%250Aof%2520neural%2520networks%252C%2520which%2520tend%2520to%2520favor%2520low-frequency%2520components%2520while%250Astruggling%2520to%2520capture%2520high-frequency%2520%2528HF%2529%2520details%2520such%2520as%2520sharp%2520edges%2520and%2520fine%250Atextures.%2520While%2520prior%2520approaches%2520have%2520addressed%2520this%2520limitation%2520through%250Aarchitectural%2520modifications%2520or%2520specialized%2520activation%2520functions%252C%2520we%2520propose%2520an%250Aorthogonal%2520direction%2520by%2520directly%2520guiding%2520the%2520training%2520process.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520two-stage%2520training%2520strategy%2520where%2520a%2520neighbor-aware%2520soft%2520mask%250Aadaptively%2520assigns%2520higher%2520weights%2520to%2520pixels%2520with%2520strong%2520local%2520variations%252C%250Aencouraging%2520early%2520focus%2520on%2520fine%2520details.%2520The%2520model%2520then%2520transitions%2520to%250Afull-image%2520training.%2520Experimental%2520results%2520show%2520that%2520our%2520approach%2520consistently%250Aimproves%2520reconstruction%2520quality%2520and%2520complements%2520existing%2520INR%2520methods.%2520As%2520a%250Apioneering%2520attempt%2520to%2520assign%2520frequency-aware%2520importance%2520to%2520pixels%2520in%2520image%2520INR%252C%250Aour%2520work%2520offers%2520a%2520new%2520avenue%2520for%2520mitigating%2520the%2520spectral%2520bias%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Frequency%20First%3A%20A%20Two-Stage%20Approach%20for%20Improving%20Image%20INR&entry.906535625=Sumit%20Kumar%20Dam%20and%20Mrityunjoy%20Gain%20and%20Eui-Nam%20Huh%20and%20Choong%20Seon%20Hong&entry.1292438233=%20%20Implicit%20Neural%20Representations%20%28INRs%29%20have%20emerged%20as%20a%20powerful%20alternative%0Ato%20traditional%20pixel-based%20formats%20by%20modeling%20images%20as%20continuous%20functions%0Aover%20spatial%20coordinates.%20A%20key%20challenge%2C%20however%2C%20lies%20in%20the%20spectral%20bias%0Aof%20neural%20networks%2C%20which%20tend%20to%20favor%20low-frequency%20components%20while%0Astruggling%20to%20capture%20high-frequency%20%28HF%29%20details%20such%20as%20sharp%20edges%20and%20fine%0Atextures.%20While%20prior%20approaches%20have%20addressed%20this%20limitation%20through%0Aarchitectural%20modifications%20or%20specialized%20activation%20functions%2C%20we%20propose%20an%0Aorthogonal%20direction%20by%20directly%20guiding%20the%20training%20process.%20Specifically%2C%20we%0Aintroduce%20a%20two-stage%20training%20strategy%20where%20a%20neighbor-aware%20soft%20mask%0Aadaptively%20assigns%20higher%20weights%20to%20pixels%20with%20strong%20local%20variations%2C%0Aencouraging%20early%20focus%20on%20fine%20details.%20The%20model%20then%20transitions%20to%0Afull-image%20training.%20Experimental%20results%20show%20that%20our%20approach%20consistently%0Aimproves%20reconstruction%20quality%20and%20complements%20existing%20INR%20methods.%20As%20a%0Apioneering%20attempt%20to%20assign%20frequency-aware%20importance%20to%20pixels%20in%20image%20INR%2C%0Aour%20work%20offers%20a%20new%20avenue%20for%20mitigating%20the%20spectral%20bias%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15582v1&entry.124074799=Read"},
{"title": "When and What: Diffusion-Grounded VideoLLM with Entity Aware\n  Segmentation for Long Video Understanding", "author": "Pengcheng Fang and Yuxia Chen and Rui Guo", "abstract": "  Understanding videos requires more than answering open ended questions, it\ndemands the ability to pinpoint when events occur and how entities interact\nacross time. While recent Video LLMs have achieved remarkable progress in\nholistic reasoning, they remain coarse in temporal perception: timestamps are\nencoded only implicitly, frame level features are weak in capturing continuity,\nand language vision alignment often drifts from the entities of interest. In\nthis paper, we present Grounded VideoDiT, a Video LLM designed to overcome\nthese limitations by introducing three key innovations. First, a Diffusion\nTemporal Latent (DTL) encoder enhances boundary sensitivity and maintains\ntemporal consistency. Second, object grounded representations explicitly bind\nquery entities to localized visual evidence, strengthening alignment. Third, a\nmixed token scheme with discrete temporal tokens provides explicit timestamp\nmodeling, enabling fine grained temporal reasoning. Together, these designs\nequip Grounded VideoDiT with robust grounding capabilities, as validated by\nstate of the art results on Charades STA, NExT GQA, and multiple VideoQA\nbenchmarks.\n", "link": "http://arxiv.org/abs/2508.15641v1", "date": "2025-08-21", "relevancy": 2.9309, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5901}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5901}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20and%20What%3A%20Diffusion-Grounded%20VideoLLM%20with%20Entity%20Aware%0A%20%20Segmentation%20for%20Long%20Video%20Understanding&body=Title%3A%20When%20and%20What%3A%20Diffusion-Grounded%20VideoLLM%20with%20Entity%20Aware%0A%20%20Segmentation%20for%20Long%20Video%20Understanding%0AAuthor%3A%20Pengcheng%20Fang%20and%20Yuxia%20Chen%20and%20Rui%20Guo%0AAbstract%3A%20%20%20Understanding%20videos%20requires%20more%20than%20answering%20open%20ended%20questions%2C%20it%0Ademands%20the%20ability%20to%20pinpoint%20when%20events%20occur%20and%20how%20entities%20interact%0Aacross%20time.%20While%20recent%20Video%20LLMs%20have%20achieved%20remarkable%20progress%20in%0Aholistic%20reasoning%2C%20they%20remain%20coarse%20in%20temporal%20perception%3A%20timestamps%20are%0Aencoded%20only%20implicitly%2C%20frame%20level%20features%20are%20weak%20in%20capturing%20continuity%2C%0Aand%20language%20vision%20alignment%20often%20drifts%20from%20the%20entities%20of%20interest.%20In%0Athis%20paper%2C%20we%20present%20Grounded%20VideoDiT%2C%20a%20Video%20LLM%20designed%20to%20overcome%0Athese%20limitations%20by%20introducing%20three%20key%20innovations.%20First%2C%20a%20Diffusion%0ATemporal%20Latent%20%28DTL%29%20encoder%20enhances%20boundary%20sensitivity%20and%20maintains%0Atemporal%20consistency.%20Second%2C%20object%20grounded%20representations%20explicitly%20bind%0Aquery%20entities%20to%20localized%20visual%20evidence%2C%20strengthening%20alignment.%20Third%2C%20a%0Amixed%20token%20scheme%20with%20discrete%20temporal%20tokens%20provides%20explicit%20timestamp%0Amodeling%2C%20enabling%20fine%20grained%20temporal%20reasoning.%20Together%2C%20these%20designs%0Aequip%20Grounded%20VideoDiT%20with%20robust%20grounding%20capabilities%2C%20as%20validated%20by%0Astate%20of%20the%20art%20results%20on%20Charades%20STA%2C%20NExT%20GQA%2C%20and%20multiple%20VideoQA%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520and%2520What%253A%2520Diffusion-Grounded%2520VideoLLM%2520with%2520Entity%2520Aware%250A%2520%2520Segmentation%2520for%2520Long%2520Video%2520Understanding%26entry.906535625%3DPengcheng%2520Fang%2520and%2520Yuxia%2520Chen%2520and%2520Rui%2520Guo%26entry.1292438233%3D%2520%2520Understanding%2520videos%2520requires%2520more%2520than%2520answering%2520open%2520ended%2520questions%252C%2520it%250Ademands%2520the%2520ability%2520to%2520pinpoint%2520when%2520events%2520occur%2520and%2520how%2520entities%2520interact%250Aacross%2520time.%2520While%2520recent%2520Video%2520LLMs%2520have%2520achieved%2520remarkable%2520progress%2520in%250Aholistic%2520reasoning%252C%2520they%2520remain%2520coarse%2520in%2520temporal%2520perception%253A%2520timestamps%2520are%250Aencoded%2520only%2520implicitly%252C%2520frame%2520level%2520features%2520are%2520weak%2520in%2520capturing%2520continuity%252C%250Aand%2520language%2520vision%2520alignment%2520often%2520drifts%2520from%2520the%2520entities%2520of%2520interest.%2520In%250Athis%2520paper%252C%2520we%2520present%2520Grounded%2520VideoDiT%252C%2520a%2520Video%2520LLM%2520designed%2520to%2520overcome%250Athese%2520limitations%2520by%2520introducing%2520three%2520key%2520innovations.%2520First%252C%2520a%2520Diffusion%250ATemporal%2520Latent%2520%2528DTL%2529%2520encoder%2520enhances%2520boundary%2520sensitivity%2520and%2520maintains%250Atemporal%2520consistency.%2520Second%252C%2520object%2520grounded%2520representations%2520explicitly%2520bind%250Aquery%2520entities%2520to%2520localized%2520visual%2520evidence%252C%2520strengthening%2520alignment.%2520Third%252C%2520a%250Amixed%2520token%2520scheme%2520with%2520discrete%2520temporal%2520tokens%2520provides%2520explicit%2520timestamp%250Amodeling%252C%2520enabling%2520fine%2520grained%2520temporal%2520reasoning.%2520Together%252C%2520these%2520designs%250Aequip%2520Grounded%2520VideoDiT%2520with%2520robust%2520grounding%2520capabilities%252C%2520as%2520validated%2520by%250Astate%2520of%2520the%2520art%2520results%2520on%2520Charades%2520STA%252C%2520NExT%2520GQA%252C%2520and%2520multiple%2520VideoQA%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20and%20What%3A%20Diffusion-Grounded%20VideoLLM%20with%20Entity%20Aware%0A%20%20Segmentation%20for%20Long%20Video%20Understanding&entry.906535625=Pengcheng%20Fang%20and%20Yuxia%20Chen%20and%20Rui%20Guo&entry.1292438233=%20%20Understanding%20videos%20requires%20more%20than%20answering%20open%20ended%20questions%2C%20it%0Ademands%20the%20ability%20to%20pinpoint%20when%20events%20occur%20and%20how%20entities%20interact%0Aacross%20time.%20While%20recent%20Video%20LLMs%20have%20achieved%20remarkable%20progress%20in%0Aholistic%20reasoning%2C%20they%20remain%20coarse%20in%20temporal%20perception%3A%20timestamps%20are%0Aencoded%20only%20implicitly%2C%20frame%20level%20features%20are%20weak%20in%20capturing%20continuity%2C%0Aand%20language%20vision%20alignment%20often%20drifts%20from%20the%20entities%20of%20interest.%20In%0Athis%20paper%2C%20we%20present%20Grounded%20VideoDiT%2C%20a%20Video%20LLM%20designed%20to%20overcome%0Athese%20limitations%20by%20introducing%20three%20key%20innovations.%20First%2C%20a%20Diffusion%0ATemporal%20Latent%20%28DTL%29%20encoder%20enhances%20boundary%20sensitivity%20and%20maintains%0Atemporal%20consistency.%20Second%2C%20object%20grounded%20representations%20explicitly%20bind%0Aquery%20entities%20to%20localized%20visual%20evidence%2C%20strengthening%20alignment.%20Third%2C%20a%0Amixed%20token%20scheme%20with%20discrete%20temporal%20tokens%20provides%20explicit%20timestamp%0Amodeling%2C%20enabling%20fine%20grained%20temporal%20reasoning.%20Together%2C%20these%20designs%0Aequip%20Grounded%20VideoDiT%20with%20robust%20grounding%20capabilities%2C%20as%20validated%20by%0Astate%20of%20the%20art%20results%20on%20Charades%20STA%2C%20NExT%20GQA%2C%20and%20multiple%20VideoQA%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15641v1&entry.124074799=Read"},
{"title": "D3FNet: A Differential Attention Fusion Network for Fine-Grained Road\n  Structure Extraction in Remote Perception Systems", "author": "Chang Liu and Yang Xu and Tamas Sziranyi", "abstract": "  Extracting narrow roads from high-resolution remote sensing imagery remains a\nsignificant challenge due to their limited width, fragmented topology, and\nfrequent occlusions. To address these issues, we propose D3FNet, a Dilated\nDual-Stream Differential Attention Fusion Network designed for fine-grained\nroad structure segmentation in remote perception systems. Built upon the\nencoder-decoder backbone of D-LinkNet, D3FNet introduces three key\ninnovations:(1) a Differential Attention Dilation Extraction (DADE) module that\nenhances subtle road features while suppressing background noise at the\nbottleneck; (2) a Dual-stream Decoding Fusion Mechanism (DDFM) that integrates\noriginal and attention-modulated features to balance spatial precision with\nsemantic context; and (3) a multi-scale dilation strategy (rates 1, 3, 5, 9)\nthat mitigates gridding artifacts and improves continuity in narrow road\nprediction. Unlike conventional models that overfit to generic road widths,\nD3FNet specifically targets fine-grained, occluded, and low-contrast road\nsegments. Extensive experiments on the DeepGlobe and CHN6-CUG benchmarks show\nthat D3FNet achieves superior IoU and recall on challenging road regions,\noutperforming state-of-the-art baselines. Ablation studies further verify the\ncomplementary synergy of attention-guided encoding and dual-path decoding.\nThese results confirm D3FNet as a robust solution for fine-grained narrow road\nextraction in complex remote and cooperative perception scenarios.\n", "link": "http://arxiv.org/abs/2508.15537v1", "date": "2025-08-21", "relevancy": 2.9267, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6011}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5774}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D3FNet%3A%20A%20Differential%20Attention%20Fusion%20Network%20for%20Fine-Grained%20Road%0A%20%20Structure%20Extraction%20in%20Remote%20Perception%20Systems&body=Title%3A%20D3FNet%3A%20A%20Differential%20Attention%20Fusion%20Network%20for%20Fine-Grained%20Road%0A%20%20Structure%20Extraction%20in%20Remote%20Perception%20Systems%0AAuthor%3A%20Chang%20Liu%20and%20Yang%20Xu%20and%20Tamas%20Sziranyi%0AAbstract%3A%20%20%20Extracting%20narrow%20roads%20from%20high-resolution%20remote%20sensing%20imagery%20remains%20a%0Asignificant%20challenge%20due%20to%20their%20limited%20width%2C%20fragmented%20topology%2C%20and%0Afrequent%20occlusions.%20To%20address%20these%20issues%2C%20we%20propose%20D3FNet%2C%20a%20Dilated%0ADual-Stream%20Differential%20Attention%20Fusion%20Network%20designed%20for%20fine-grained%0Aroad%20structure%20segmentation%20in%20remote%20perception%20systems.%20Built%20upon%20the%0Aencoder-decoder%20backbone%20of%20D-LinkNet%2C%20D3FNet%20introduces%20three%20key%0Ainnovations%3A%281%29%20a%20Differential%20Attention%20Dilation%20Extraction%20%28DADE%29%20module%20that%0Aenhances%20subtle%20road%20features%20while%20suppressing%20background%20noise%20at%20the%0Abottleneck%3B%20%282%29%20a%20Dual-stream%20Decoding%20Fusion%20Mechanism%20%28DDFM%29%20that%20integrates%0Aoriginal%20and%20attention-modulated%20features%20to%20balance%20spatial%20precision%20with%0Asemantic%20context%3B%20and%20%283%29%20a%20multi-scale%20dilation%20strategy%20%28rates%201%2C%203%2C%205%2C%209%29%0Athat%20mitigates%20gridding%20artifacts%20and%20improves%20continuity%20in%20narrow%20road%0Aprediction.%20Unlike%20conventional%20models%20that%20overfit%20to%20generic%20road%20widths%2C%0AD3FNet%20specifically%20targets%20fine-grained%2C%20occluded%2C%20and%20low-contrast%20road%0Asegments.%20Extensive%20experiments%20on%20the%20DeepGlobe%20and%20CHN6-CUG%20benchmarks%20show%0Athat%20D3FNet%20achieves%20superior%20IoU%20and%20recall%20on%20challenging%20road%20regions%2C%0Aoutperforming%20state-of-the-art%20baselines.%20Ablation%20studies%20further%20verify%20the%0Acomplementary%20synergy%20of%20attention-guided%20encoding%20and%20dual-path%20decoding.%0AThese%20results%20confirm%20D3FNet%20as%20a%20robust%20solution%20for%20fine-grained%20narrow%20road%0Aextraction%20in%20complex%20remote%20and%20cooperative%20perception%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD3FNet%253A%2520A%2520Differential%2520Attention%2520Fusion%2520Network%2520for%2520Fine-Grained%2520Road%250A%2520%2520Structure%2520Extraction%2520in%2520Remote%2520Perception%2520Systems%26entry.906535625%3DChang%2520Liu%2520and%2520Yang%2520Xu%2520and%2520Tamas%2520Sziranyi%26entry.1292438233%3D%2520%2520Extracting%2520narrow%2520roads%2520from%2520high-resolution%2520remote%2520sensing%2520imagery%2520remains%2520a%250Asignificant%2520challenge%2520due%2520to%2520their%2520limited%2520width%252C%2520fragmented%2520topology%252C%2520and%250Afrequent%2520occlusions.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520D3FNet%252C%2520a%2520Dilated%250ADual-Stream%2520Differential%2520Attention%2520Fusion%2520Network%2520designed%2520for%2520fine-grained%250Aroad%2520structure%2520segmentation%2520in%2520remote%2520perception%2520systems.%2520Built%2520upon%2520the%250Aencoder-decoder%2520backbone%2520of%2520D-LinkNet%252C%2520D3FNet%2520introduces%2520three%2520key%250Ainnovations%253A%25281%2529%2520a%2520Differential%2520Attention%2520Dilation%2520Extraction%2520%2528DADE%2529%2520module%2520that%250Aenhances%2520subtle%2520road%2520features%2520while%2520suppressing%2520background%2520noise%2520at%2520the%250Abottleneck%253B%2520%25282%2529%2520a%2520Dual-stream%2520Decoding%2520Fusion%2520Mechanism%2520%2528DDFM%2529%2520that%2520integrates%250Aoriginal%2520and%2520attention-modulated%2520features%2520to%2520balance%2520spatial%2520precision%2520with%250Asemantic%2520context%253B%2520and%2520%25283%2529%2520a%2520multi-scale%2520dilation%2520strategy%2520%2528rates%25201%252C%25203%252C%25205%252C%25209%2529%250Athat%2520mitigates%2520gridding%2520artifacts%2520and%2520improves%2520continuity%2520in%2520narrow%2520road%250Aprediction.%2520Unlike%2520conventional%2520models%2520that%2520overfit%2520to%2520generic%2520road%2520widths%252C%250AD3FNet%2520specifically%2520targets%2520fine-grained%252C%2520occluded%252C%2520and%2520low-contrast%2520road%250Asegments.%2520Extensive%2520experiments%2520on%2520the%2520DeepGlobe%2520and%2520CHN6-CUG%2520benchmarks%2520show%250Athat%2520D3FNet%2520achieves%2520superior%2520IoU%2520and%2520recall%2520on%2520challenging%2520road%2520regions%252C%250Aoutperforming%2520state-of-the-art%2520baselines.%2520Ablation%2520studies%2520further%2520verify%2520the%250Acomplementary%2520synergy%2520of%2520attention-guided%2520encoding%2520and%2520dual-path%2520decoding.%250AThese%2520results%2520confirm%2520D3FNet%2520as%2520a%2520robust%2520solution%2520for%2520fine-grained%2520narrow%2520road%250Aextraction%2520in%2520complex%2520remote%2520and%2520cooperative%2520perception%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D3FNet%3A%20A%20Differential%20Attention%20Fusion%20Network%20for%20Fine-Grained%20Road%0A%20%20Structure%20Extraction%20in%20Remote%20Perception%20Systems&entry.906535625=Chang%20Liu%20and%20Yang%20Xu%20and%20Tamas%20Sziranyi&entry.1292438233=%20%20Extracting%20narrow%20roads%20from%20high-resolution%20remote%20sensing%20imagery%20remains%20a%0Asignificant%20challenge%20due%20to%20their%20limited%20width%2C%20fragmented%20topology%2C%20and%0Afrequent%20occlusions.%20To%20address%20these%20issues%2C%20we%20propose%20D3FNet%2C%20a%20Dilated%0ADual-Stream%20Differential%20Attention%20Fusion%20Network%20designed%20for%20fine-grained%0Aroad%20structure%20segmentation%20in%20remote%20perception%20systems.%20Built%20upon%20the%0Aencoder-decoder%20backbone%20of%20D-LinkNet%2C%20D3FNet%20introduces%20three%20key%0Ainnovations%3A%281%29%20a%20Differential%20Attention%20Dilation%20Extraction%20%28DADE%29%20module%20that%0Aenhances%20subtle%20road%20features%20while%20suppressing%20background%20noise%20at%20the%0Abottleneck%3B%20%282%29%20a%20Dual-stream%20Decoding%20Fusion%20Mechanism%20%28DDFM%29%20that%20integrates%0Aoriginal%20and%20attention-modulated%20features%20to%20balance%20spatial%20precision%20with%0Asemantic%20context%3B%20and%20%283%29%20a%20multi-scale%20dilation%20strategy%20%28rates%201%2C%203%2C%205%2C%209%29%0Athat%20mitigates%20gridding%20artifacts%20and%20improves%20continuity%20in%20narrow%20road%0Aprediction.%20Unlike%20conventional%20models%20that%20overfit%20to%20generic%20road%20widths%2C%0AD3FNet%20specifically%20targets%20fine-grained%2C%20occluded%2C%20and%20low-contrast%20road%0Asegments.%20Extensive%20experiments%20on%20the%20DeepGlobe%20and%20CHN6-CUG%20benchmarks%20show%0Athat%20D3FNet%20achieves%20superior%20IoU%20and%20recall%20on%20challenging%20road%20regions%2C%0Aoutperforming%20state-of-the-art%20baselines.%20Ablation%20studies%20further%20verify%20the%0Acomplementary%20synergy%20of%20attention-guided%20encoding%20and%20dual-path%20decoding.%0AThese%20results%20confirm%20D3FNet%20as%20a%20robust%20solution%20for%20fine-grained%20narrow%20road%0Aextraction%20in%20complex%20remote%20and%20cooperative%20perception%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15537v1&entry.124074799=Read"},
{"title": "ExtraGS: Geometric-Aware Trajectory Extrapolation with\n  Uncertainty-Guided Generative Priors", "author": "Kaiyuan Tan and Yingying Shen and Haohui Zhu and Zhiwei Zhan and Shan Zhao and Mingfei Tu and Hongcheng Luo and Haiyang Sun and Bing Wang and Guang Chen and Hangjun Ye", "abstract": "  Synthesizing extrapolated views from recorded driving logs is critical for\nsimulating driving scenes for autonomous driving vehicles, yet it remains a\nchallenging task. Recent methods leverage generative priors as pseudo ground\ntruth, but often lead to poor geometric consistency and over-smoothed\nrenderings. To address these limitations, we propose ExtraGS, a holistic\nframework for trajectory extrapolation that integrates both geometric and\ngenerative priors. At the core of ExtraGS is a novel Road Surface Gaussian(RSG)\nrepresentation based on a hybrid Gaussian-Signed Distance Function (SDF)\ndesign, and Far Field Gaussians (FFG) that use learnable scaling factors to\nefficiently handle distant objects. Furthermore, we develop a self-supervised\nuncertainty estimation framework based on spherical harmonics that enables\nselective integration of generative priors only where extrapolation artifacts\noccur. Extensive experiments on multiple datasets, diverse multi-camera setups,\nand various generative priors demonstrate that ExtraGS significantly enhances\nthe realism and geometric consistency of extrapolated views, while preserving\nhigh fidelity along the original trajectory.\n", "link": "http://arxiv.org/abs/2508.15529v1", "date": "2025-08-21", "relevancy": 2.909, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5979}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5813}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExtraGS%3A%20Geometric-Aware%20Trajectory%20Extrapolation%20with%0A%20%20Uncertainty-Guided%20Generative%20Priors&body=Title%3A%20ExtraGS%3A%20Geometric-Aware%20Trajectory%20Extrapolation%20with%0A%20%20Uncertainty-Guided%20Generative%20Priors%0AAuthor%3A%20Kaiyuan%20Tan%20and%20Yingying%20Shen%20and%20Haohui%20Zhu%20and%20Zhiwei%20Zhan%20and%20Shan%20Zhao%20and%20Mingfei%20Tu%20and%20Hongcheng%20Luo%20and%20Haiyang%20Sun%20and%20Bing%20Wang%20and%20Guang%20Chen%20and%20Hangjun%20Ye%0AAbstract%3A%20%20%20Synthesizing%20extrapolated%20views%20from%20recorded%20driving%20logs%20is%20critical%20for%0Asimulating%20driving%20scenes%20for%20autonomous%20driving%20vehicles%2C%20yet%20it%20remains%20a%0Achallenging%20task.%20Recent%20methods%20leverage%20generative%20priors%20as%20pseudo%20ground%0Atruth%2C%20but%20often%20lead%20to%20poor%20geometric%20consistency%20and%20over-smoothed%0Arenderings.%20To%20address%20these%20limitations%2C%20we%20propose%20ExtraGS%2C%20a%20holistic%0Aframework%20for%20trajectory%20extrapolation%20that%20integrates%20both%20geometric%20and%0Agenerative%20priors.%20At%20the%20core%20of%20ExtraGS%20is%20a%20novel%20Road%20Surface%20Gaussian%28RSG%29%0Arepresentation%20based%20on%20a%20hybrid%20Gaussian-Signed%20Distance%20Function%20%28SDF%29%0Adesign%2C%20and%20Far%20Field%20Gaussians%20%28FFG%29%20that%20use%20learnable%20scaling%20factors%20to%0Aefficiently%20handle%20distant%20objects.%20Furthermore%2C%20we%20develop%20a%20self-supervised%0Auncertainty%20estimation%20framework%20based%20on%20spherical%20harmonics%20that%20enables%0Aselective%20integration%20of%20generative%20priors%20only%20where%20extrapolation%20artifacts%0Aoccur.%20Extensive%20experiments%20on%20multiple%20datasets%2C%20diverse%20multi-camera%20setups%2C%0Aand%20various%20generative%20priors%20demonstrate%20that%20ExtraGS%20significantly%20enhances%0Athe%20realism%20and%20geometric%20consistency%20of%20extrapolated%20views%2C%20while%20preserving%0Ahigh%20fidelity%20along%20the%20original%20trajectory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtraGS%253A%2520Geometric-Aware%2520Trajectory%2520Extrapolation%2520with%250A%2520%2520Uncertainty-Guided%2520Generative%2520Priors%26entry.906535625%3DKaiyuan%2520Tan%2520and%2520Yingying%2520Shen%2520and%2520Haohui%2520Zhu%2520and%2520Zhiwei%2520Zhan%2520and%2520Shan%2520Zhao%2520and%2520Mingfei%2520Tu%2520and%2520Hongcheng%2520Luo%2520and%2520Haiyang%2520Sun%2520and%2520Bing%2520Wang%2520and%2520Guang%2520Chen%2520and%2520Hangjun%2520Ye%26entry.1292438233%3D%2520%2520Synthesizing%2520extrapolated%2520views%2520from%2520recorded%2520driving%2520logs%2520is%2520critical%2520for%250Asimulating%2520driving%2520scenes%2520for%2520autonomous%2520driving%2520vehicles%252C%2520yet%2520it%2520remains%2520a%250Achallenging%2520task.%2520Recent%2520methods%2520leverage%2520generative%2520priors%2520as%2520pseudo%2520ground%250Atruth%252C%2520but%2520often%2520lead%2520to%2520poor%2520geometric%2520consistency%2520and%2520over-smoothed%250Arenderings.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520ExtraGS%252C%2520a%2520holistic%250Aframework%2520for%2520trajectory%2520extrapolation%2520that%2520integrates%2520both%2520geometric%2520and%250Agenerative%2520priors.%2520At%2520the%2520core%2520of%2520ExtraGS%2520is%2520a%2520novel%2520Road%2520Surface%2520Gaussian%2528RSG%2529%250Arepresentation%2520based%2520on%2520a%2520hybrid%2520Gaussian-Signed%2520Distance%2520Function%2520%2528SDF%2529%250Adesign%252C%2520and%2520Far%2520Field%2520Gaussians%2520%2528FFG%2529%2520that%2520use%2520learnable%2520scaling%2520factors%2520to%250Aefficiently%2520handle%2520distant%2520objects.%2520Furthermore%252C%2520we%2520develop%2520a%2520self-supervised%250Auncertainty%2520estimation%2520framework%2520based%2520on%2520spherical%2520harmonics%2520that%2520enables%250Aselective%2520integration%2520of%2520generative%2520priors%2520only%2520where%2520extrapolation%2520artifacts%250Aoccur.%2520Extensive%2520experiments%2520on%2520multiple%2520datasets%252C%2520diverse%2520multi-camera%2520setups%252C%250Aand%2520various%2520generative%2520priors%2520demonstrate%2520that%2520ExtraGS%2520significantly%2520enhances%250Athe%2520realism%2520and%2520geometric%2520consistency%2520of%2520extrapolated%2520views%252C%2520while%2520preserving%250Ahigh%2520fidelity%2520along%2520the%2520original%2520trajectory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExtraGS%3A%20Geometric-Aware%20Trajectory%20Extrapolation%20with%0A%20%20Uncertainty-Guided%20Generative%20Priors&entry.906535625=Kaiyuan%20Tan%20and%20Yingying%20Shen%20and%20Haohui%20Zhu%20and%20Zhiwei%20Zhan%20and%20Shan%20Zhao%20and%20Mingfei%20Tu%20and%20Hongcheng%20Luo%20and%20Haiyang%20Sun%20and%20Bing%20Wang%20and%20Guang%20Chen%20and%20Hangjun%20Ye&entry.1292438233=%20%20Synthesizing%20extrapolated%20views%20from%20recorded%20driving%20logs%20is%20critical%20for%0Asimulating%20driving%20scenes%20for%20autonomous%20driving%20vehicles%2C%20yet%20it%20remains%20a%0Achallenging%20task.%20Recent%20methods%20leverage%20generative%20priors%20as%20pseudo%20ground%0Atruth%2C%20but%20often%20lead%20to%20poor%20geometric%20consistency%20and%20over-smoothed%0Arenderings.%20To%20address%20these%20limitations%2C%20we%20propose%20ExtraGS%2C%20a%20holistic%0Aframework%20for%20trajectory%20extrapolation%20that%20integrates%20both%20geometric%20and%0Agenerative%20priors.%20At%20the%20core%20of%20ExtraGS%20is%20a%20novel%20Road%20Surface%20Gaussian%28RSG%29%0Arepresentation%20based%20on%20a%20hybrid%20Gaussian-Signed%20Distance%20Function%20%28SDF%29%0Adesign%2C%20and%20Far%20Field%20Gaussians%20%28FFG%29%20that%20use%20learnable%20scaling%20factors%20to%0Aefficiently%20handle%20distant%20objects.%20Furthermore%2C%20we%20develop%20a%20self-supervised%0Auncertainty%20estimation%20framework%20based%20on%20spherical%20harmonics%20that%20enables%0Aselective%20integration%20of%20generative%20priors%20only%20where%20extrapolation%20artifacts%0Aoccur.%20Extensive%20experiments%20on%20multiple%20datasets%2C%20diverse%20multi-camera%20setups%2C%0Aand%20various%20generative%20priors%20demonstrate%20that%20ExtraGS%20significantly%20enhances%0Athe%20realism%20and%20geometric%20consistency%20of%20extrapolated%20views%2C%20while%20preserving%0Ahigh%20fidelity%20along%20the%20original%20trajectory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15529v1&entry.124074799=Read"},
{"title": "Large Language Models Encode Semantics in Low-Dimensional Linear\n  Subspaces", "author": "Baturay Saglam and Paul Kassianik and Blaine Nelson and Sajana Weerawardhena and Yaron Singer and Amin Karbasi", "abstract": "  Understanding the latent space geometry of large language models (LLMs) is\nkey to interpreting their behavior and improving alignment. However, it remains\nunclear to what extent LLMs internally organize representations related to\nsemantic understanding. To explore this, we conduct a large-scale empirical\nstudy of hidden representations in 11 autoregressive models across 6 scientific\ntopics. We find that high-level semantic information consistently resides in\nlow-dimensional subspaces that form linearly separable representations across\ndomains. This separability becomes more pronounced in deeper layers and under\nprompts that elicit structured reasoning or alignment\nbehavior$\\unicode{x2013}$even when surface content remains unchanged. These\nfindings support geometry-aware tools that operate directly in latent space to\ndetect and mitigate harmful or adversarial content. As a proof of concept, we\ntrain an MLP probe on final-layer hidden states to act as a lightweight\nlatent-space guardrail. This approach substantially improves refusal rates on\nmalicious queries and prompt injections that bypass both the model's built-in\nsafety alignment and external token-level filters.\n", "link": "http://arxiv.org/abs/2507.09709v2", "date": "2025-08-21", "relevancy": 2.9021, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.605}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Encode%20Semantics%20in%20Low-Dimensional%20Linear%0A%20%20Subspaces&body=Title%3A%20Large%20Language%20Models%20Encode%20Semantics%20in%20Low-Dimensional%20Linear%0A%20%20Subspaces%0AAuthor%3A%20Baturay%20Saglam%20and%20Paul%20Kassianik%20and%20Blaine%20Nelson%20and%20Sajana%20Weerawardhena%20and%20Yaron%20Singer%20and%20Amin%20Karbasi%0AAbstract%3A%20%20%20Understanding%20the%20latent%20space%20geometry%20of%20large%20language%20models%20%28LLMs%29%20is%0Akey%20to%20interpreting%20their%20behavior%20and%20improving%20alignment.%20However%2C%20it%20remains%0Aunclear%20to%20what%20extent%20LLMs%20internally%20organize%20representations%20related%20to%0Asemantic%20understanding.%20To%20explore%20this%2C%20we%20conduct%20a%20large-scale%20empirical%0Astudy%20of%20hidden%20representations%20in%2011%20autoregressive%20models%20across%206%20scientific%0Atopics.%20We%20find%20that%20high-level%20semantic%20information%20consistently%20resides%20in%0Alow-dimensional%20subspaces%20that%20form%20linearly%20separable%20representations%20across%0Adomains.%20This%20separability%20becomes%20more%20pronounced%20in%20deeper%20layers%20and%20under%0Aprompts%20that%20elicit%20structured%20reasoning%20or%20alignment%0Abehavior%24%5Cunicode%7Bx2013%7D%24even%20when%20surface%20content%20remains%20unchanged.%20These%0Afindings%20support%20geometry-aware%20tools%20that%20operate%20directly%20in%20latent%20space%20to%0Adetect%20and%20mitigate%20harmful%20or%20adversarial%20content.%20As%20a%20proof%20of%20concept%2C%20we%0Atrain%20an%20MLP%20probe%20on%20final-layer%20hidden%20states%20to%20act%20as%20a%20lightweight%0Alatent-space%20guardrail.%20This%20approach%20substantially%20improves%20refusal%20rates%20on%0Amalicious%20queries%20and%20prompt%20injections%20that%20bypass%20both%20the%20model%27s%20built-in%0Asafety%20alignment%20and%20external%20token-level%20filters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09709v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Encode%2520Semantics%2520in%2520Low-Dimensional%2520Linear%250A%2520%2520Subspaces%26entry.906535625%3DBaturay%2520Saglam%2520and%2520Paul%2520Kassianik%2520and%2520Blaine%2520Nelson%2520and%2520Sajana%2520Weerawardhena%2520and%2520Yaron%2520Singer%2520and%2520Amin%2520Karbasi%26entry.1292438233%3D%2520%2520Understanding%2520the%2520latent%2520space%2520geometry%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%250Akey%2520to%2520interpreting%2520their%2520behavior%2520and%2520improving%2520alignment.%2520However%252C%2520it%2520remains%250Aunclear%2520to%2520what%2520extent%2520LLMs%2520internally%2520organize%2520representations%2520related%2520to%250Asemantic%2520understanding.%2520To%2520explore%2520this%252C%2520we%2520conduct%2520a%2520large-scale%2520empirical%250Astudy%2520of%2520hidden%2520representations%2520in%252011%2520autoregressive%2520models%2520across%25206%2520scientific%250Atopics.%2520We%2520find%2520that%2520high-level%2520semantic%2520information%2520consistently%2520resides%2520in%250Alow-dimensional%2520subspaces%2520that%2520form%2520linearly%2520separable%2520representations%2520across%250Adomains.%2520This%2520separability%2520becomes%2520more%2520pronounced%2520in%2520deeper%2520layers%2520and%2520under%250Aprompts%2520that%2520elicit%2520structured%2520reasoning%2520or%2520alignment%250Abehavior%2524%255Cunicode%257Bx2013%257D%2524even%2520when%2520surface%2520content%2520remains%2520unchanged.%2520These%250Afindings%2520support%2520geometry-aware%2520tools%2520that%2520operate%2520directly%2520in%2520latent%2520space%2520to%250Adetect%2520and%2520mitigate%2520harmful%2520or%2520adversarial%2520content.%2520As%2520a%2520proof%2520of%2520concept%252C%2520we%250Atrain%2520an%2520MLP%2520probe%2520on%2520final-layer%2520hidden%2520states%2520to%2520act%2520as%2520a%2520lightweight%250Alatent-space%2520guardrail.%2520This%2520approach%2520substantially%2520improves%2520refusal%2520rates%2520on%250Amalicious%2520queries%2520and%2520prompt%2520injections%2520that%2520bypass%2520both%2520the%2520model%2527s%2520built-in%250Asafety%2520alignment%2520and%2520external%2520token-level%2520filters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09709v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Encode%20Semantics%20in%20Low-Dimensional%20Linear%0A%20%20Subspaces&entry.906535625=Baturay%20Saglam%20and%20Paul%20Kassianik%20and%20Blaine%20Nelson%20and%20Sajana%20Weerawardhena%20and%20Yaron%20Singer%20and%20Amin%20Karbasi&entry.1292438233=%20%20Understanding%20the%20latent%20space%20geometry%20of%20large%20language%20models%20%28LLMs%29%20is%0Akey%20to%20interpreting%20their%20behavior%20and%20improving%20alignment.%20However%2C%20it%20remains%0Aunclear%20to%20what%20extent%20LLMs%20internally%20organize%20representations%20related%20to%0Asemantic%20understanding.%20To%20explore%20this%2C%20we%20conduct%20a%20large-scale%20empirical%0Astudy%20of%20hidden%20representations%20in%2011%20autoregressive%20models%20across%206%20scientific%0Atopics.%20We%20find%20that%20high-level%20semantic%20information%20consistently%20resides%20in%0Alow-dimensional%20subspaces%20that%20form%20linearly%20separable%20representations%20across%0Adomains.%20This%20separability%20becomes%20more%20pronounced%20in%20deeper%20layers%20and%20under%0Aprompts%20that%20elicit%20structured%20reasoning%20or%20alignment%0Abehavior%24%5Cunicode%7Bx2013%7D%24even%20when%20surface%20content%20remains%20unchanged.%20These%0Afindings%20support%20geometry-aware%20tools%20that%20operate%20directly%20in%20latent%20space%20to%0Adetect%20and%20mitigate%20harmful%20or%20adversarial%20content.%20As%20a%20proof%20of%20concept%2C%20we%0Atrain%20an%20MLP%20probe%20on%20final-layer%20hidden%20states%20to%20act%20as%20a%20lightweight%0Alatent-space%20guardrail.%20This%20approach%20substantially%20improves%20refusal%20rates%20on%0Amalicious%20queries%20and%20prompt%20injections%20that%20bypass%20both%20the%20model%27s%20built-in%0Asafety%20alignment%20and%20external%20token-level%20filters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09709v2&entry.124074799=Read"},
{"title": "The Devil is in the EOS: Sequence Training for Detailed Image Captioning", "author": "Abdelrahman Mohamed and Yova Kementchedjhieva", "abstract": "  Despite significant advances in vision-language models (VLMs), image\ncaptioning often suffers from a lack of detail, with base models producing\nshort, generic captions. This limitation persists even though VLMs are equipped\nwith strong vision and language backbones. While supervised data and complex\nreward functions have been proposed to improve detailed image captioning, we\nidentify a simpler underlying issue: a bias towards the end-of-sequence (EOS)\ntoken, which is introduced during cross-entropy training. We propose an\nunsupervised method to debias the model's tendency to predict the EOS token\nprematurely. By reducing this bias, we encourage the generation of longer, more\ndetailed captions without the need for intricate reward functions or\nsupervision. Our approach is straightforward, effective, and easily applicable\nto any pretrained model. We demonstrate its effectiveness through experiments\nwith three VLMs and on three detailed captioning benchmarks. Our results show a\nsubstantial increase in caption length and relevant details, albeit with an\nexpected increase in the rate of hallucinations.\n", "link": "http://arxiv.org/abs/2507.20077v2", "date": "2025-08-21", "relevancy": 2.8734, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5935}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5935}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Devil%20is%20in%20the%20EOS%3A%20Sequence%20Training%20for%20Detailed%20Image%20Captioning&body=Title%3A%20The%20Devil%20is%20in%20the%20EOS%3A%20Sequence%20Training%20for%20Detailed%20Image%20Captioning%0AAuthor%3A%20Abdelrahman%20Mohamed%20and%20Yova%20Kementchedjhieva%0AAbstract%3A%20%20%20Despite%20significant%20advances%20in%20vision-language%20models%20%28VLMs%29%2C%20image%0Acaptioning%20often%20suffers%20from%20a%20lack%20of%20detail%2C%20with%20base%20models%20producing%0Ashort%2C%20generic%20captions.%20This%20limitation%20persists%20even%20though%20VLMs%20are%20equipped%0Awith%20strong%20vision%20and%20language%20backbones.%20While%20supervised%20data%20and%20complex%0Areward%20functions%20have%20been%20proposed%20to%20improve%20detailed%20image%20captioning%2C%20we%0Aidentify%20a%20simpler%20underlying%20issue%3A%20a%20bias%20towards%20the%20end-of-sequence%20%28EOS%29%0Atoken%2C%20which%20is%20introduced%20during%20cross-entropy%20training.%20We%20propose%20an%0Aunsupervised%20method%20to%20debias%20the%20model%27s%20tendency%20to%20predict%20the%20EOS%20token%0Aprematurely.%20By%20reducing%20this%20bias%2C%20we%20encourage%20the%20generation%20of%20longer%2C%20more%0Adetailed%20captions%20without%20the%20need%20for%20intricate%20reward%20functions%20or%0Asupervision.%20Our%20approach%20is%20straightforward%2C%20effective%2C%20and%20easily%20applicable%0Ato%20any%20pretrained%20model.%20We%20demonstrate%20its%20effectiveness%20through%20experiments%0Awith%20three%20VLMs%20and%20on%20three%20detailed%20captioning%20benchmarks.%20Our%20results%20show%20a%0Asubstantial%20increase%20in%20caption%20length%20and%20relevant%20details%2C%20albeit%20with%20an%0Aexpected%20increase%20in%20the%20rate%20of%20hallucinations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20077v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Devil%2520is%2520in%2520the%2520EOS%253A%2520Sequence%2520Training%2520for%2520Detailed%2520Image%2520Captioning%26entry.906535625%3DAbdelrahman%2520Mohamed%2520and%2520Yova%2520Kementchedjhieva%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advances%2520in%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520image%250Acaptioning%2520often%2520suffers%2520from%2520a%2520lack%2520of%2520detail%252C%2520with%2520base%2520models%2520producing%250Ashort%252C%2520generic%2520captions.%2520This%2520limitation%2520persists%2520even%2520though%2520VLMs%2520are%2520equipped%250Awith%2520strong%2520vision%2520and%2520language%2520backbones.%2520While%2520supervised%2520data%2520and%2520complex%250Areward%2520functions%2520have%2520been%2520proposed%2520to%2520improve%2520detailed%2520image%2520captioning%252C%2520we%250Aidentify%2520a%2520simpler%2520underlying%2520issue%253A%2520a%2520bias%2520towards%2520the%2520end-of-sequence%2520%2528EOS%2529%250Atoken%252C%2520which%2520is%2520introduced%2520during%2520cross-entropy%2520training.%2520We%2520propose%2520an%250Aunsupervised%2520method%2520to%2520debias%2520the%2520model%2527s%2520tendency%2520to%2520predict%2520the%2520EOS%2520token%250Aprematurely.%2520By%2520reducing%2520this%2520bias%252C%2520we%2520encourage%2520the%2520generation%2520of%2520longer%252C%2520more%250Adetailed%2520captions%2520without%2520the%2520need%2520for%2520intricate%2520reward%2520functions%2520or%250Asupervision.%2520Our%2520approach%2520is%2520straightforward%252C%2520effective%252C%2520and%2520easily%2520applicable%250Ato%2520any%2520pretrained%2520model.%2520We%2520demonstrate%2520its%2520effectiveness%2520through%2520experiments%250Awith%2520three%2520VLMs%2520and%2520on%2520three%2520detailed%2520captioning%2520benchmarks.%2520Our%2520results%2520show%2520a%250Asubstantial%2520increase%2520in%2520caption%2520length%2520and%2520relevant%2520details%252C%2520albeit%2520with%2520an%250Aexpected%2520increase%2520in%2520the%2520rate%2520of%2520hallucinations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20077v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Devil%20is%20in%20the%20EOS%3A%20Sequence%20Training%20for%20Detailed%20Image%20Captioning&entry.906535625=Abdelrahman%20Mohamed%20and%20Yova%20Kementchedjhieva&entry.1292438233=%20%20Despite%20significant%20advances%20in%20vision-language%20models%20%28VLMs%29%2C%20image%0Acaptioning%20often%20suffers%20from%20a%20lack%20of%20detail%2C%20with%20base%20models%20producing%0Ashort%2C%20generic%20captions.%20This%20limitation%20persists%20even%20though%20VLMs%20are%20equipped%0Awith%20strong%20vision%20and%20language%20backbones.%20While%20supervised%20data%20and%20complex%0Areward%20functions%20have%20been%20proposed%20to%20improve%20detailed%20image%20captioning%2C%20we%0Aidentify%20a%20simpler%20underlying%20issue%3A%20a%20bias%20towards%20the%20end-of-sequence%20%28EOS%29%0Atoken%2C%20which%20is%20introduced%20during%20cross-entropy%20training.%20We%20propose%20an%0Aunsupervised%20method%20to%20debias%20the%20model%27s%20tendency%20to%20predict%20the%20EOS%20token%0Aprematurely.%20By%20reducing%20this%20bias%2C%20we%20encourage%20the%20generation%20of%20longer%2C%20more%0Adetailed%20captions%20without%20the%20need%20for%20intricate%20reward%20functions%20or%0Asupervision.%20Our%20approach%20is%20straightforward%2C%20effective%2C%20and%20easily%20applicable%0Ato%20any%20pretrained%20model.%20We%20demonstrate%20its%20effectiveness%20through%20experiments%0Awith%20three%20VLMs%20and%20on%20three%20detailed%20captioning%20benchmarks.%20Our%20results%20show%20a%0Asubstantial%20increase%20in%20caption%20length%20and%20relevant%20details%2C%20albeit%20with%20an%0Aexpected%20increase%20in%20the%20rate%20of%20hallucinations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20077v2&entry.124074799=Read"},
{"title": "MapKD: Unlocking Prior Knowledge with Cross-Modal Distillation for\n  Efficient Online HD Map Construction", "author": "Ziyang Yan and Ruikai Li and Zhiyong Cui and Bohan Li and Han Jiang and Yilong Ren and Aoyong Li and Zhenning Li and Sijia Wen and Haiyang Yu", "abstract": "  Online HD map construction is a fundamental task in autonomous driving\nsystems, aiming to acquire semantic information of map elements around the ego\nvehicle based on real-time sensor inputs. Recently, several approaches have\nachieved promising results by incorporating offline priors such as SD maps and\nHD maps or by fusing multi-modal data. However, these methods depend on stale\noffline maps and multi-modal sensor suites, resulting in avoidable\ncomputational overhead at inference. To address these limitations, we employ a\nknowledge distillation strategy to transfer knowledge from multimodal models\nwith prior knowledge to an efficient, low-cost, and vision-centric student\nmodel. Specifically, we propose MapKD, a novel multi-level cross-modal\nknowledge distillation framework with an innovative Teacher-Coach-Student (TCS)\nparadigm. This framework consists of: (1) a camera-LiDAR fusion model with\nSD/HD map priors serving as the teacher; (2) a vision-centric coach model with\nprior knowledge and simulated LiDAR to bridge the cross-modal knowledge\ntransfer gap; and (3) a lightweight vision-based student model. Additionally,\nwe introduce two targeted knowledge distillation strategies: Token-Guided 2D\nPatch Distillation (TGPD) for bird's eye view feature alignment and Masked\nSemantic Response Distillation (MSRD) for semantic learning guidance. Extensive\nexperiments on the challenging nuScenes dataset demonstrate that MapKD improves\nthe student model by +6.68 mIoU and +10.94 mAP while simultaneously\naccelerating inference speed. The code is available\nat:https://github.com/2004yan/MapKD2026.\n", "link": "http://arxiv.org/abs/2508.15653v1", "date": "2025-08-21", "relevancy": 2.8408, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5751}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5651}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MapKD%3A%20Unlocking%20Prior%20Knowledge%20with%20Cross-Modal%20Distillation%20for%0A%20%20Efficient%20Online%20HD%20Map%20Construction&body=Title%3A%20MapKD%3A%20Unlocking%20Prior%20Knowledge%20with%20Cross-Modal%20Distillation%20for%0A%20%20Efficient%20Online%20HD%20Map%20Construction%0AAuthor%3A%20Ziyang%20Yan%20and%20Ruikai%20Li%20and%20Zhiyong%20Cui%20and%20Bohan%20Li%20and%20Han%20Jiang%20and%20Yilong%20Ren%20and%20Aoyong%20Li%20and%20Zhenning%20Li%20and%20Sijia%20Wen%20and%20Haiyang%20Yu%0AAbstract%3A%20%20%20Online%20HD%20map%20construction%20is%20a%20fundamental%20task%20in%20autonomous%20driving%0Asystems%2C%20aiming%20to%20acquire%20semantic%20information%20of%20map%20elements%20around%20the%20ego%0Avehicle%20based%20on%20real-time%20sensor%20inputs.%20Recently%2C%20several%20approaches%20have%0Aachieved%20promising%20results%20by%20incorporating%20offline%20priors%20such%20as%20SD%20maps%20and%0AHD%20maps%20or%20by%20fusing%20multi-modal%20data.%20However%2C%20these%20methods%20depend%20on%20stale%0Aoffline%20maps%20and%20multi-modal%20sensor%20suites%2C%20resulting%20in%20avoidable%0Acomputational%20overhead%20at%20inference.%20To%20address%20these%20limitations%2C%20we%20employ%20a%0Aknowledge%20distillation%20strategy%20to%20transfer%20knowledge%20from%20multimodal%20models%0Awith%20prior%20knowledge%20to%20an%20efficient%2C%20low-cost%2C%20and%20vision-centric%20student%0Amodel.%20Specifically%2C%20we%20propose%20MapKD%2C%20a%20novel%20multi-level%20cross-modal%0Aknowledge%20distillation%20framework%20with%20an%20innovative%20Teacher-Coach-Student%20%28TCS%29%0Aparadigm.%20This%20framework%20consists%20of%3A%20%281%29%20a%20camera-LiDAR%20fusion%20model%20with%0ASD/HD%20map%20priors%20serving%20as%20the%20teacher%3B%20%282%29%20a%20vision-centric%20coach%20model%20with%0Aprior%20knowledge%20and%20simulated%20LiDAR%20to%20bridge%20the%20cross-modal%20knowledge%0Atransfer%20gap%3B%20and%20%283%29%20a%20lightweight%20vision-based%20student%20model.%20Additionally%2C%0Awe%20introduce%20two%20targeted%20knowledge%20distillation%20strategies%3A%20Token-Guided%202D%0APatch%20Distillation%20%28TGPD%29%20for%20bird%27s%20eye%20view%20feature%20alignment%20and%20Masked%0ASemantic%20Response%20Distillation%20%28MSRD%29%20for%20semantic%20learning%20guidance.%20Extensive%0Aexperiments%20on%20the%20challenging%20nuScenes%20dataset%20demonstrate%20that%20MapKD%20improves%0Athe%20student%20model%20by%20%2B6.68%20mIoU%20and%20%2B10.94%20mAP%20while%20simultaneously%0Aaccelerating%20inference%20speed.%20The%20code%20is%20available%0Aat%3Ahttps%3A//github.com/2004yan/MapKD2026.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapKD%253A%2520Unlocking%2520Prior%2520Knowledge%2520with%2520Cross-Modal%2520Distillation%2520for%250A%2520%2520Efficient%2520Online%2520HD%2520Map%2520Construction%26entry.906535625%3DZiyang%2520Yan%2520and%2520Ruikai%2520Li%2520and%2520Zhiyong%2520Cui%2520and%2520Bohan%2520Li%2520and%2520Han%2520Jiang%2520and%2520Yilong%2520Ren%2520and%2520Aoyong%2520Li%2520and%2520Zhenning%2520Li%2520and%2520Sijia%2520Wen%2520and%2520Haiyang%2520Yu%26entry.1292438233%3D%2520%2520Online%2520HD%2520map%2520construction%2520is%2520a%2520fundamental%2520task%2520in%2520autonomous%2520driving%250Asystems%252C%2520aiming%2520to%2520acquire%2520semantic%2520information%2520of%2520map%2520elements%2520around%2520the%2520ego%250Avehicle%2520based%2520on%2520real-time%2520sensor%2520inputs.%2520Recently%252C%2520several%2520approaches%2520have%250Aachieved%2520promising%2520results%2520by%2520incorporating%2520offline%2520priors%2520such%2520as%2520SD%2520maps%2520and%250AHD%2520maps%2520or%2520by%2520fusing%2520multi-modal%2520data.%2520However%252C%2520these%2520methods%2520depend%2520on%2520stale%250Aoffline%2520maps%2520and%2520multi-modal%2520sensor%2520suites%252C%2520resulting%2520in%2520avoidable%250Acomputational%2520overhead%2520at%2520inference.%2520To%2520address%2520these%2520limitations%252C%2520we%2520employ%2520a%250Aknowledge%2520distillation%2520strategy%2520to%2520transfer%2520knowledge%2520from%2520multimodal%2520models%250Awith%2520prior%2520knowledge%2520to%2520an%2520efficient%252C%2520low-cost%252C%2520and%2520vision-centric%2520student%250Amodel.%2520Specifically%252C%2520we%2520propose%2520MapKD%252C%2520a%2520novel%2520multi-level%2520cross-modal%250Aknowledge%2520distillation%2520framework%2520with%2520an%2520innovative%2520Teacher-Coach-Student%2520%2528TCS%2529%250Aparadigm.%2520This%2520framework%2520consists%2520of%253A%2520%25281%2529%2520a%2520camera-LiDAR%2520fusion%2520model%2520with%250ASD/HD%2520map%2520priors%2520serving%2520as%2520the%2520teacher%253B%2520%25282%2529%2520a%2520vision-centric%2520coach%2520model%2520with%250Aprior%2520knowledge%2520and%2520simulated%2520LiDAR%2520to%2520bridge%2520the%2520cross-modal%2520knowledge%250Atransfer%2520gap%253B%2520and%2520%25283%2529%2520a%2520lightweight%2520vision-based%2520student%2520model.%2520Additionally%252C%250Awe%2520introduce%2520two%2520targeted%2520knowledge%2520distillation%2520strategies%253A%2520Token-Guided%25202D%250APatch%2520Distillation%2520%2528TGPD%2529%2520for%2520bird%2527s%2520eye%2520view%2520feature%2520alignment%2520and%2520Masked%250ASemantic%2520Response%2520Distillation%2520%2528MSRD%2529%2520for%2520semantic%2520learning%2520guidance.%2520Extensive%250Aexperiments%2520on%2520the%2520challenging%2520nuScenes%2520dataset%2520demonstrate%2520that%2520MapKD%2520improves%250Athe%2520student%2520model%2520by%2520%252B6.68%2520mIoU%2520and%2520%252B10.94%2520mAP%2520while%2520simultaneously%250Aaccelerating%2520inference%2520speed.%2520The%2520code%2520is%2520available%250Aat%253Ahttps%253A//github.com/2004yan/MapKD2026.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MapKD%3A%20Unlocking%20Prior%20Knowledge%20with%20Cross-Modal%20Distillation%20for%0A%20%20Efficient%20Online%20HD%20Map%20Construction&entry.906535625=Ziyang%20Yan%20and%20Ruikai%20Li%20and%20Zhiyong%20Cui%20and%20Bohan%20Li%20and%20Han%20Jiang%20and%20Yilong%20Ren%20and%20Aoyong%20Li%20and%20Zhenning%20Li%20and%20Sijia%20Wen%20and%20Haiyang%20Yu&entry.1292438233=%20%20Online%20HD%20map%20construction%20is%20a%20fundamental%20task%20in%20autonomous%20driving%0Asystems%2C%20aiming%20to%20acquire%20semantic%20information%20of%20map%20elements%20around%20the%20ego%0Avehicle%20based%20on%20real-time%20sensor%20inputs.%20Recently%2C%20several%20approaches%20have%0Aachieved%20promising%20results%20by%20incorporating%20offline%20priors%20such%20as%20SD%20maps%20and%0AHD%20maps%20or%20by%20fusing%20multi-modal%20data.%20However%2C%20these%20methods%20depend%20on%20stale%0Aoffline%20maps%20and%20multi-modal%20sensor%20suites%2C%20resulting%20in%20avoidable%0Acomputational%20overhead%20at%20inference.%20To%20address%20these%20limitations%2C%20we%20employ%20a%0Aknowledge%20distillation%20strategy%20to%20transfer%20knowledge%20from%20multimodal%20models%0Awith%20prior%20knowledge%20to%20an%20efficient%2C%20low-cost%2C%20and%20vision-centric%20student%0Amodel.%20Specifically%2C%20we%20propose%20MapKD%2C%20a%20novel%20multi-level%20cross-modal%0Aknowledge%20distillation%20framework%20with%20an%20innovative%20Teacher-Coach-Student%20%28TCS%29%0Aparadigm.%20This%20framework%20consists%20of%3A%20%281%29%20a%20camera-LiDAR%20fusion%20model%20with%0ASD/HD%20map%20priors%20serving%20as%20the%20teacher%3B%20%282%29%20a%20vision-centric%20coach%20model%20with%0Aprior%20knowledge%20and%20simulated%20LiDAR%20to%20bridge%20the%20cross-modal%20knowledge%0Atransfer%20gap%3B%20and%20%283%29%20a%20lightweight%20vision-based%20student%20model.%20Additionally%2C%0Awe%20introduce%20two%20targeted%20knowledge%20distillation%20strategies%3A%20Token-Guided%202D%0APatch%20Distillation%20%28TGPD%29%20for%20bird%27s%20eye%20view%20feature%20alignment%20and%20Masked%0ASemantic%20Response%20Distillation%20%28MSRD%29%20for%20semantic%20learning%20guidance.%20Extensive%0Aexperiments%20on%20the%20challenging%20nuScenes%20dataset%20demonstrate%20that%20MapKD%20improves%0Athe%20student%20model%20by%20%2B6.68%20mIoU%20and%20%2B10.94%20mAP%20while%20simultaneously%0Aaccelerating%20inference%20speed.%20The%20code%20is%20available%0Aat%3Ahttps%3A//github.com/2004yan/MapKD2026.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15653v1&entry.124074799=Read"},
{"title": "From Points to Spheres: A Geometric Reinterpretation of Variational\n  Autoencoders", "author": "Songxuan Shi", "abstract": "  Variational Autoencoder is typically understood from the perspective of\nprobabilistic inference. In this work, we propose a new geometric\nreinterpretation which complements the probabilistic view and enhances its\nintuitiveness. We demonstrate that the proper construction of semantic\nmanifolds arises primarily from the constraining effect of the KL divergence on\nthe encoder. We view the latent representations as a Gaussian ball rather than\ndeterministic points. Under the constraint of KL divergence, Gaussian ball\nregularizes the latent space, promoting a more uniform distribution of\nencodings. Furthermore, we show that reparameterization establishes a critical\ncontractual mechanism between the encoder and decoder, enabling the decoder to\nlearn how to reconstruct from these stochastic regions. We further connect this\nviewpoint with VQ-VAE, offering a unified perspective: VQ-VAE can be seen as an\nautoencoder where encodings are constrained to a set of cluster centers, with\nits generative capability arising from the compactness rather than its\nstochasticity. This geometric framework provides a new lens for understanding\nhow VAE shapes the latent geometry to enable effective generation.\n", "link": "http://arxiv.org/abs/2507.17255v2", "date": "2025-08-21", "relevancy": 2.7466, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6124}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5298}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Points%20to%20Spheres%3A%20A%20Geometric%20Reinterpretation%20of%20Variational%0A%20%20Autoencoders&body=Title%3A%20From%20Points%20to%20Spheres%3A%20A%20Geometric%20Reinterpretation%20of%20Variational%0A%20%20Autoencoders%0AAuthor%3A%20Songxuan%20Shi%0AAbstract%3A%20%20%20Variational%20Autoencoder%20is%20typically%20understood%20from%20the%20perspective%20of%0Aprobabilistic%20inference.%20In%20this%20work%2C%20we%20propose%20a%20new%20geometric%0Areinterpretation%20which%20complements%20the%20probabilistic%20view%20and%20enhances%20its%0Aintuitiveness.%20We%20demonstrate%20that%20the%20proper%20construction%20of%20semantic%0Amanifolds%20arises%20primarily%20from%20the%20constraining%20effect%20of%20the%20KL%20divergence%20on%0Athe%20encoder.%20We%20view%20the%20latent%20representations%20as%20a%20Gaussian%20ball%20rather%20than%0Adeterministic%20points.%20Under%20the%20constraint%20of%20KL%20divergence%2C%20Gaussian%20ball%0Aregularizes%20the%20latent%20space%2C%20promoting%20a%20more%20uniform%20distribution%20of%0Aencodings.%20Furthermore%2C%20we%20show%20that%20reparameterization%20establishes%20a%20critical%0Acontractual%20mechanism%20between%20the%20encoder%20and%20decoder%2C%20enabling%20the%20decoder%20to%0Alearn%20how%20to%20reconstruct%20from%20these%20stochastic%20regions.%20We%20further%20connect%20this%0Aviewpoint%20with%20VQ-VAE%2C%20offering%20a%20unified%20perspective%3A%20VQ-VAE%20can%20be%20seen%20as%20an%0Aautoencoder%20where%20encodings%20are%20constrained%20to%20a%20set%20of%20cluster%20centers%2C%20with%0Aits%20generative%20capability%20arising%20from%20the%20compactness%20rather%20than%20its%0Astochasticity.%20This%20geometric%20framework%20provides%20a%20new%20lens%20for%20understanding%0Ahow%20VAE%20shapes%20the%20latent%20geometry%20to%20enable%20effective%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17255v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Points%2520to%2520Spheres%253A%2520A%2520Geometric%2520Reinterpretation%2520of%2520Variational%250A%2520%2520Autoencoders%26entry.906535625%3DSongxuan%2520Shi%26entry.1292438233%3D%2520%2520Variational%2520Autoencoder%2520is%2520typically%2520understood%2520from%2520the%2520perspective%2520of%250Aprobabilistic%2520inference.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520geometric%250Areinterpretation%2520which%2520complements%2520the%2520probabilistic%2520view%2520and%2520enhances%2520its%250Aintuitiveness.%2520We%2520demonstrate%2520that%2520the%2520proper%2520construction%2520of%2520semantic%250Amanifolds%2520arises%2520primarily%2520from%2520the%2520constraining%2520effect%2520of%2520the%2520KL%2520divergence%2520on%250Athe%2520encoder.%2520We%2520view%2520the%2520latent%2520representations%2520as%2520a%2520Gaussian%2520ball%2520rather%2520than%250Adeterministic%2520points.%2520Under%2520the%2520constraint%2520of%2520KL%2520divergence%252C%2520Gaussian%2520ball%250Aregularizes%2520the%2520latent%2520space%252C%2520promoting%2520a%2520more%2520uniform%2520distribution%2520of%250Aencodings.%2520Furthermore%252C%2520we%2520show%2520that%2520reparameterization%2520establishes%2520a%2520critical%250Acontractual%2520mechanism%2520between%2520the%2520encoder%2520and%2520decoder%252C%2520enabling%2520the%2520decoder%2520to%250Alearn%2520how%2520to%2520reconstruct%2520from%2520these%2520stochastic%2520regions.%2520We%2520further%2520connect%2520this%250Aviewpoint%2520with%2520VQ-VAE%252C%2520offering%2520a%2520unified%2520perspective%253A%2520VQ-VAE%2520can%2520be%2520seen%2520as%2520an%250Aautoencoder%2520where%2520encodings%2520are%2520constrained%2520to%2520a%2520set%2520of%2520cluster%2520centers%252C%2520with%250Aits%2520generative%2520capability%2520arising%2520from%2520the%2520compactness%2520rather%2520than%2520its%250Astochasticity.%2520This%2520geometric%2520framework%2520provides%2520a%2520new%2520lens%2520for%2520understanding%250Ahow%2520VAE%2520shapes%2520the%2520latent%2520geometry%2520to%2520enable%2520effective%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17255v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Points%20to%20Spheres%3A%20A%20Geometric%20Reinterpretation%20of%20Variational%0A%20%20Autoencoders&entry.906535625=Songxuan%20Shi&entry.1292438233=%20%20Variational%20Autoencoder%20is%20typically%20understood%20from%20the%20perspective%20of%0Aprobabilistic%20inference.%20In%20this%20work%2C%20we%20propose%20a%20new%20geometric%0Areinterpretation%20which%20complements%20the%20probabilistic%20view%20and%20enhances%20its%0Aintuitiveness.%20We%20demonstrate%20that%20the%20proper%20construction%20of%20semantic%0Amanifolds%20arises%20primarily%20from%20the%20constraining%20effect%20of%20the%20KL%20divergence%20on%0Athe%20encoder.%20We%20view%20the%20latent%20representations%20as%20a%20Gaussian%20ball%20rather%20than%0Adeterministic%20points.%20Under%20the%20constraint%20of%20KL%20divergence%2C%20Gaussian%20ball%0Aregularizes%20the%20latent%20space%2C%20promoting%20a%20more%20uniform%20distribution%20of%0Aencodings.%20Furthermore%2C%20we%20show%20that%20reparameterization%20establishes%20a%20critical%0Acontractual%20mechanism%20between%20the%20encoder%20and%20decoder%2C%20enabling%20the%20decoder%20to%0Alearn%20how%20to%20reconstruct%20from%20these%20stochastic%20regions.%20We%20further%20connect%20this%0Aviewpoint%20with%20VQ-VAE%2C%20offering%20a%20unified%20perspective%3A%20VQ-VAE%20can%20be%20seen%20as%20an%0Aautoencoder%20where%20encodings%20are%20constrained%20to%20a%20set%20of%20cluster%20centers%2C%20with%0Aits%20generative%20capability%20arising%20from%20the%20compactness%20rather%20than%20its%0Astochasticity.%20This%20geometric%20framework%20provides%20a%20new%20lens%20for%20understanding%0Ahow%20VAE%20shapes%20the%20latent%20geometry%20to%20enable%20effective%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17255v2&entry.124074799=Read"},
{"title": "Intern-S1: A Scientific Multimodal Foundation Model", "author": "Lei Bai and Zhongrui Cai and Maosong Cao and Weihan Cao and Chiyu Chen and Haojiong Chen and Kai Chen and Pengcheng Chen and Ying Chen and Yongkang Chen and Yu Cheng and Yu Cheng and Pei Chu and Tao Chu and Erfei Cui and Ganqu Cui and Long Cui and Ziyun Cui and Nianchen Deng and Ning Ding and Nanqin Dong and Peijie Dong and Shihan Dou and Sinan Du and Haodong Duan and Caihua Fan and Ben Gao and Changjiang Gao and Jianfei Gao and Songyang Gao and Yang Gao and Zhangwei Gao and Jiaye Ge and Qiming Ge and Lixin Gu and Yuzhe Gu and Aijia Guo and Qipeng Guo and Xu Guo and Conghui He and Junjun He and Yili Hong and Siyuan Hou and Caiyu Hu and Hanglei Hu and Jucheng Hu and Ming Hu and Zhouqi Hua and Haian Huang and Junhao Huang and Xu Huang and Zixian Huang and Zhe Jiang and Lingkai Kong and Linyang Li and Peiji Li and Pengze Li and Shuaibin Li and Tianbin Li and Wei Li and Yuqiang Li and Dahua Lin and Junyao Lin and Tianyi Lin and Zhishan Lin and Hongwei Liu and Jiangning Liu and Jiyao Liu and Junnan Liu and Kai Liu and Kaiwen Liu and Kuikun Liu and Shichun Liu and Shudong Liu and Wei Liu and Xinyao Liu and Yuhong Liu and Zhan Liu and Yinquan Lu and Haijun Lv and Hongxia Lv and Huijie Lv and Qidang Lv and Ying Lv and Chengqi Lyu and Chenglong Ma and Jianpeng Ma and Ren Ma and Runmin Ma and Runyuan Ma and Xinzhu Ma and Yichuan Ma and Zihan Ma and Sixuan Mi and Junzhi Ning and Wenchang Ning and Xinle Pang and Jiahui Peng and Runyu Peng and Yu Qiao and Jiantao Qiu and Xiaoye Qu and Yuan Qu and Yuchen Ren and Fukai Shang and Wenqi Shao and Junhao Shen and Shuaike Shen and Chunfeng Song and Demin Song and Diping Song and Chenlin Su and Weijie Su and Weigao Sun and Yu Sun and Qian Tan and Cheng Tang and Huanze Tang and Kexian Tang and Shixiang Tang and Jian Tong and Aoran Wang and Bin Wang and Dong Wang and Lintao Wang and Rui Wang and Weiyun Wang and Wenhai Wang and Yi Wang and Ziyi Wang and Ling-I Wu and Wen Wu and Yue Wu and Zijian Wu and Linchen Xiao and Shuhao Xing and Chao Xu and Huihui Xu and Jun Xu and Ruiliang Xu and Wanghan Xu and GanLin Yang and Yuming Yang and Haochen Ye and Jin Ye and Shenglong Ye and Jia Yu and Jiashuo Yu and Jing Yu and Fei Yuan and Bo Zhang and Chao Zhang and Chen Zhang and Hongjie Zhang and Jin Zhang and Qiaosheng Zhang and Qiuyinzhe Zhang and Songyang Zhang and Taolin Zhang and Wenlong Zhang and Wenwei Zhang and Yechen Zhang and Ziyang Zhang and Haiteng Zhao and Qian Zhao and Xiangyu Zhao and Xiangyu Zhao and Bowen Zhou and Dongzhan Zhou and Peiheng Zhou and Yuhao Zhou and Yunhua Zhou and Dongsheng Zhu and Lin Zhu and Yicheng Zou", "abstract": "  In recent years, a plethora of open-source foundation models have emerged,\nachieving remarkable progress in some widely attended fields, with performance\nbeing quite close to that of closed-source models. However, in high-value but\nmore challenging scientific professional fields, either the fields still rely\non expert models, or the progress of general foundation models lags\nsignificantly compared to those in popular areas, far from sufficient for\ntransforming scientific research and leaving substantial gap between\nopen-source models and closed-source models in these scientific domains. To\nmitigate this gap and explore a step further toward Artificial General\nIntelligence (AGI), we introduce Intern-S1, a specialized generalist equipped\nwith general understanding and reasoning capabilities with expertise to analyze\nmultiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)\nmodel with 28 billion activated parameters and 241 billion total parameters,\ncontinually pre-trained on 5T tokens, including over 2.5T tokens from\nscientific domains. In the post-training stage, Intern-S1 undergoes offline and\nthen online reinforcement learning (RL) in InternBootCamp, where we propose\nMixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks\nsimultaneously. Through integrated innovations in algorithms, data, and\ntraining systems, Intern-S1 achieved top-tier performance in online RL\ntraining.On comprehensive evaluation benchmarks, Intern-S1 demonstrates\ncompetitive performance on general reasoning tasks among open-source models and\nsignificantly outperforms open-source models in scientific domains, surpassing\nclosed-source state-of-the-art models in professional tasks, such as molecular\nsynthesis planning, reaction condition prediction, predicting thermodynamic\nstabilities for crystals. Our models are available at\nhttps://huggingface.co/internlm/Intern-S1.\n", "link": "http://arxiv.org/abs/2508.15763v1", "date": "2025-08-21", "relevancy": 2.7449, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5545}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intern-S1%3A%20A%20Scientific%20Multimodal%20Foundation%20Model&body=Title%3A%20Intern-S1%3A%20A%20Scientific%20Multimodal%20Foundation%20Model%0AAuthor%3A%20Lei%20Bai%20and%20Zhongrui%20Cai%20and%20Maosong%20Cao%20and%20Weihan%20Cao%20and%20Chiyu%20Chen%20and%20Haojiong%20Chen%20and%20Kai%20Chen%20and%20Pengcheng%20Chen%20and%20Ying%20Chen%20and%20Yongkang%20Chen%20and%20Yu%20Cheng%20and%20Yu%20Cheng%20and%20Pei%20Chu%20and%20Tao%20Chu%20and%20Erfei%20Cui%20and%20Ganqu%20Cui%20and%20Long%20Cui%20and%20Ziyun%20Cui%20and%20Nianchen%20Deng%20and%20Ning%20Ding%20and%20Nanqin%20Dong%20and%20Peijie%20Dong%20and%20Shihan%20Dou%20and%20Sinan%20Du%20and%20Haodong%20Duan%20and%20Caihua%20Fan%20and%20Ben%20Gao%20and%20Changjiang%20Gao%20and%20Jianfei%20Gao%20and%20Songyang%20Gao%20and%20Yang%20Gao%20and%20Zhangwei%20Gao%20and%20Jiaye%20Ge%20and%20Qiming%20Ge%20and%20Lixin%20Gu%20and%20Yuzhe%20Gu%20and%20Aijia%20Guo%20and%20Qipeng%20Guo%20and%20Xu%20Guo%20and%20Conghui%20He%20and%20Junjun%20He%20and%20Yili%20Hong%20and%20Siyuan%20Hou%20and%20Caiyu%20Hu%20and%20Hanglei%20Hu%20and%20Jucheng%20Hu%20and%20Ming%20Hu%20and%20Zhouqi%20Hua%20and%20Haian%20Huang%20and%20Junhao%20Huang%20and%20Xu%20Huang%20and%20Zixian%20Huang%20and%20Zhe%20Jiang%20and%20Lingkai%20Kong%20and%20Linyang%20Li%20and%20Peiji%20Li%20and%20Pengze%20Li%20and%20Shuaibin%20Li%20and%20Tianbin%20Li%20and%20Wei%20Li%20and%20Yuqiang%20Li%20and%20Dahua%20Lin%20and%20Junyao%20Lin%20and%20Tianyi%20Lin%20and%20Zhishan%20Lin%20and%20Hongwei%20Liu%20and%20Jiangning%20Liu%20and%20Jiyao%20Liu%20and%20Junnan%20Liu%20and%20Kai%20Liu%20and%20Kaiwen%20Liu%20and%20Kuikun%20Liu%20and%20Shichun%20Liu%20and%20Shudong%20Liu%20and%20Wei%20Liu%20and%20Xinyao%20Liu%20and%20Yuhong%20Liu%20and%20Zhan%20Liu%20and%20Yinquan%20Lu%20and%20Haijun%20Lv%20and%20Hongxia%20Lv%20and%20Huijie%20Lv%20and%20Qidang%20Lv%20and%20Ying%20Lv%20and%20Chengqi%20Lyu%20and%20Chenglong%20Ma%20and%20Jianpeng%20Ma%20and%20Ren%20Ma%20and%20Runmin%20Ma%20and%20Runyuan%20Ma%20and%20Xinzhu%20Ma%20and%20Yichuan%20Ma%20and%20Zihan%20Ma%20and%20Sixuan%20Mi%20and%20Junzhi%20Ning%20and%20Wenchang%20Ning%20and%20Xinle%20Pang%20and%20Jiahui%20Peng%20and%20Runyu%20Peng%20and%20Yu%20Qiao%20and%20Jiantao%20Qiu%20and%20Xiaoye%20Qu%20and%20Yuan%20Qu%20and%20Yuchen%20Ren%20and%20Fukai%20Shang%20and%20Wenqi%20Shao%20and%20Junhao%20Shen%20and%20Shuaike%20Shen%20and%20Chunfeng%20Song%20and%20Demin%20Song%20and%20Diping%20Song%20and%20Chenlin%20Su%20and%20Weijie%20Su%20and%20Weigao%20Sun%20and%20Yu%20Sun%20and%20Qian%20Tan%20and%20Cheng%20Tang%20and%20Huanze%20Tang%20and%20Kexian%20Tang%20and%20Shixiang%20Tang%20and%20Jian%20Tong%20and%20Aoran%20Wang%20and%20Bin%20Wang%20and%20Dong%20Wang%20and%20Lintao%20Wang%20and%20Rui%20Wang%20and%20Weiyun%20Wang%20and%20Wenhai%20Wang%20and%20Yi%20Wang%20and%20Ziyi%20Wang%20and%20Ling-I%20Wu%20and%20Wen%20Wu%20and%20Yue%20Wu%20and%20Zijian%20Wu%20and%20Linchen%20Xiao%20and%20Shuhao%20Xing%20and%20Chao%20Xu%20and%20Huihui%20Xu%20and%20Jun%20Xu%20and%20Ruiliang%20Xu%20and%20Wanghan%20Xu%20and%20GanLin%20Yang%20and%20Yuming%20Yang%20and%20Haochen%20Ye%20and%20Jin%20Ye%20and%20Shenglong%20Ye%20and%20Jia%20Yu%20and%20Jiashuo%20Yu%20and%20Jing%20Yu%20and%20Fei%20Yuan%20and%20Bo%20Zhang%20and%20Chao%20Zhang%20and%20Chen%20Zhang%20and%20Hongjie%20Zhang%20and%20Jin%20Zhang%20and%20Qiaosheng%20Zhang%20and%20Qiuyinzhe%20Zhang%20and%20Songyang%20Zhang%20and%20Taolin%20Zhang%20and%20Wenlong%20Zhang%20and%20Wenwei%20Zhang%20and%20Yechen%20Zhang%20and%20Ziyang%20Zhang%20and%20Haiteng%20Zhao%20and%20Qian%20Zhao%20and%20Xiangyu%20Zhao%20and%20Xiangyu%20Zhao%20and%20Bowen%20Zhou%20and%20Dongzhan%20Zhou%20and%20Peiheng%20Zhou%20and%20Yuhao%20Zhou%20and%20Yunhua%20Zhou%20and%20Dongsheng%20Zhu%20and%20Lin%20Zhu%20and%20Yicheng%20Zou%0AAbstract%3A%20%20%20In%20recent%20years%2C%20a%20plethora%20of%20open-source%20foundation%20models%20have%20emerged%2C%0Aachieving%20remarkable%20progress%20in%20some%20widely%20attended%20fields%2C%20with%20performance%0Abeing%20quite%20close%20to%20that%20of%20closed-source%20models.%20However%2C%20in%20high-value%20but%0Amore%20challenging%20scientific%20professional%20fields%2C%20either%20the%20fields%20still%20rely%0Aon%20expert%20models%2C%20or%20the%20progress%20of%20general%20foundation%20models%20lags%0Asignificantly%20compared%20to%20those%20in%20popular%20areas%2C%20far%20from%20sufficient%20for%0Atransforming%20scientific%20research%20and%20leaving%20substantial%20gap%20between%0Aopen-source%20models%20and%20closed-source%20models%20in%20these%20scientific%20domains.%20To%0Amitigate%20this%20gap%20and%20explore%20a%20step%20further%20toward%20Artificial%20General%0AIntelligence%20%28AGI%29%2C%20we%20introduce%20Intern-S1%2C%20a%20specialized%20generalist%20equipped%0Awith%20general%20understanding%20and%20reasoning%20capabilities%20with%20expertise%20to%20analyze%0Amultiple%20science%20modal%20data.%20Intern-S1%20is%20a%20multimodal%20Mixture-of-Experts%20%28MoE%29%0Amodel%20with%2028%20billion%20activated%20parameters%20and%20241%20billion%20total%20parameters%2C%0Acontinually%20pre-trained%20on%205T%20tokens%2C%20including%20over%202.5T%20tokens%20from%0Ascientific%20domains.%20In%20the%20post-training%20stage%2C%20Intern-S1%20undergoes%20offline%20and%0Athen%20online%20reinforcement%20learning%20%28RL%29%20in%20InternBootCamp%2C%20where%20we%20propose%0AMixture-of-Rewards%20%28MoR%29%20to%20synergize%20the%20RL%20training%20on%20more%20than%201000%20tasks%0Asimultaneously.%20Through%20integrated%20innovations%20in%20algorithms%2C%20data%2C%20and%0Atraining%20systems%2C%20Intern-S1%20achieved%20top-tier%20performance%20in%20online%20RL%0Atraining.On%20comprehensive%20evaluation%20benchmarks%2C%20Intern-S1%20demonstrates%0Acompetitive%20performance%20on%20general%20reasoning%20tasks%20among%20open-source%20models%20and%0Asignificantly%20outperforms%20open-source%20models%20in%20scientific%20domains%2C%20surpassing%0Aclosed-source%20state-of-the-art%20models%20in%20professional%20tasks%2C%20such%20as%20molecular%0Asynthesis%20planning%2C%20reaction%20condition%20prediction%2C%20predicting%20thermodynamic%0Astabilities%20for%20crystals.%20Our%20models%20are%20available%20at%0Ahttps%3A//huggingface.co/internlm/Intern-S1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntern-S1%253A%2520A%2520Scientific%2520Multimodal%2520Foundation%2520Model%26entry.906535625%3DLei%2520Bai%2520and%2520Zhongrui%2520Cai%2520and%2520Maosong%2520Cao%2520and%2520Weihan%2520Cao%2520and%2520Chiyu%2520Chen%2520and%2520Haojiong%2520Chen%2520and%2520Kai%2520Chen%2520and%2520Pengcheng%2520Chen%2520and%2520Ying%2520Chen%2520and%2520Yongkang%2520Chen%2520and%2520Yu%2520Cheng%2520and%2520Yu%2520Cheng%2520and%2520Pei%2520Chu%2520and%2520Tao%2520Chu%2520and%2520Erfei%2520Cui%2520and%2520Ganqu%2520Cui%2520and%2520Long%2520Cui%2520and%2520Ziyun%2520Cui%2520and%2520Nianchen%2520Deng%2520and%2520Ning%2520Ding%2520and%2520Nanqin%2520Dong%2520and%2520Peijie%2520Dong%2520and%2520Shihan%2520Dou%2520and%2520Sinan%2520Du%2520and%2520Haodong%2520Duan%2520and%2520Caihua%2520Fan%2520and%2520Ben%2520Gao%2520and%2520Changjiang%2520Gao%2520and%2520Jianfei%2520Gao%2520and%2520Songyang%2520Gao%2520and%2520Yang%2520Gao%2520and%2520Zhangwei%2520Gao%2520and%2520Jiaye%2520Ge%2520and%2520Qiming%2520Ge%2520and%2520Lixin%2520Gu%2520and%2520Yuzhe%2520Gu%2520and%2520Aijia%2520Guo%2520and%2520Qipeng%2520Guo%2520and%2520Xu%2520Guo%2520and%2520Conghui%2520He%2520and%2520Junjun%2520He%2520and%2520Yili%2520Hong%2520and%2520Siyuan%2520Hou%2520and%2520Caiyu%2520Hu%2520and%2520Hanglei%2520Hu%2520and%2520Jucheng%2520Hu%2520and%2520Ming%2520Hu%2520and%2520Zhouqi%2520Hua%2520and%2520Haian%2520Huang%2520and%2520Junhao%2520Huang%2520and%2520Xu%2520Huang%2520and%2520Zixian%2520Huang%2520and%2520Zhe%2520Jiang%2520and%2520Lingkai%2520Kong%2520and%2520Linyang%2520Li%2520and%2520Peiji%2520Li%2520and%2520Pengze%2520Li%2520and%2520Shuaibin%2520Li%2520and%2520Tianbin%2520Li%2520and%2520Wei%2520Li%2520and%2520Yuqiang%2520Li%2520and%2520Dahua%2520Lin%2520and%2520Junyao%2520Lin%2520and%2520Tianyi%2520Lin%2520and%2520Zhishan%2520Lin%2520and%2520Hongwei%2520Liu%2520and%2520Jiangning%2520Liu%2520and%2520Jiyao%2520Liu%2520and%2520Junnan%2520Liu%2520and%2520Kai%2520Liu%2520and%2520Kaiwen%2520Liu%2520and%2520Kuikun%2520Liu%2520and%2520Shichun%2520Liu%2520and%2520Shudong%2520Liu%2520and%2520Wei%2520Liu%2520and%2520Xinyao%2520Liu%2520and%2520Yuhong%2520Liu%2520and%2520Zhan%2520Liu%2520and%2520Yinquan%2520Lu%2520and%2520Haijun%2520Lv%2520and%2520Hongxia%2520Lv%2520and%2520Huijie%2520Lv%2520and%2520Qidang%2520Lv%2520and%2520Ying%2520Lv%2520and%2520Chengqi%2520Lyu%2520and%2520Chenglong%2520Ma%2520and%2520Jianpeng%2520Ma%2520and%2520Ren%2520Ma%2520and%2520Runmin%2520Ma%2520and%2520Runyuan%2520Ma%2520and%2520Xinzhu%2520Ma%2520and%2520Yichuan%2520Ma%2520and%2520Zihan%2520Ma%2520and%2520Sixuan%2520Mi%2520and%2520Junzhi%2520Ning%2520and%2520Wenchang%2520Ning%2520and%2520Xinle%2520Pang%2520and%2520Jiahui%2520Peng%2520and%2520Runyu%2520Peng%2520and%2520Yu%2520Qiao%2520and%2520Jiantao%2520Qiu%2520and%2520Xiaoye%2520Qu%2520and%2520Yuan%2520Qu%2520and%2520Yuchen%2520Ren%2520and%2520Fukai%2520Shang%2520and%2520Wenqi%2520Shao%2520and%2520Junhao%2520Shen%2520and%2520Shuaike%2520Shen%2520and%2520Chunfeng%2520Song%2520and%2520Demin%2520Song%2520and%2520Diping%2520Song%2520and%2520Chenlin%2520Su%2520and%2520Weijie%2520Su%2520and%2520Weigao%2520Sun%2520and%2520Yu%2520Sun%2520and%2520Qian%2520Tan%2520and%2520Cheng%2520Tang%2520and%2520Huanze%2520Tang%2520and%2520Kexian%2520Tang%2520and%2520Shixiang%2520Tang%2520and%2520Jian%2520Tong%2520and%2520Aoran%2520Wang%2520and%2520Bin%2520Wang%2520and%2520Dong%2520Wang%2520and%2520Lintao%2520Wang%2520and%2520Rui%2520Wang%2520and%2520Weiyun%2520Wang%2520and%2520Wenhai%2520Wang%2520and%2520Yi%2520Wang%2520and%2520Ziyi%2520Wang%2520and%2520Ling-I%2520Wu%2520and%2520Wen%2520Wu%2520and%2520Yue%2520Wu%2520and%2520Zijian%2520Wu%2520and%2520Linchen%2520Xiao%2520and%2520Shuhao%2520Xing%2520and%2520Chao%2520Xu%2520and%2520Huihui%2520Xu%2520and%2520Jun%2520Xu%2520and%2520Ruiliang%2520Xu%2520and%2520Wanghan%2520Xu%2520and%2520GanLin%2520Yang%2520and%2520Yuming%2520Yang%2520and%2520Haochen%2520Ye%2520and%2520Jin%2520Ye%2520and%2520Shenglong%2520Ye%2520and%2520Jia%2520Yu%2520and%2520Jiashuo%2520Yu%2520and%2520Jing%2520Yu%2520and%2520Fei%2520Yuan%2520and%2520Bo%2520Zhang%2520and%2520Chao%2520Zhang%2520and%2520Chen%2520Zhang%2520and%2520Hongjie%2520Zhang%2520and%2520Jin%2520Zhang%2520and%2520Qiaosheng%2520Zhang%2520and%2520Qiuyinzhe%2520Zhang%2520and%2520Songyang%2520Zhang%2520and%2520Taolin%2520Zhang%2520and%2520Wenlong%2520Zhang%2520and%2520Wenwei%2520Zhang%2520and%2520Yechen%2520Zhang%2520and%2520Ziyang%2520Zhang%2520and%2520Haiteng%2520Zhao%2520and%2520Qian%2520Zhao%2520and%2520Xiangyu%2520Zhao%2520and%2520Xiangyu%2520Zhao%2520and%2520Bowen%2520Zhou%2520and%2520Dongzhan%2520Zhou%2520and%2520Peiheng%2520Zhou%2520and%2520Yuhao%2520Zhou%2520and%2520Yunhua%2520Zhou%2520and%2520Dongsheng%2520Zhu%2520and%2520Lin%2520Zhu%2520and%2520Yicheng%2520Zou%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520a%2520plethora%2520of%2520open-source%2520foundation%2520models%2520have%2520emerged%252C%250Aachieving%2520remarkable%2520progress%2520in%2520some%2520widely%2520attended%2520fields%252C%2520with%2520performance%250Abeing%2520quite%2520close%2520to%2520that%2520of%2520closed-source%2520models.%2520However%252C%2520in%2520high-value%2520but%250Amore%2520challenging%2520scientific%2520professional%2520fields%252C%2520either%2520the%2520fields%2520still%2520rely%250Aon%2520expert%2520models%252C%2520or%2520the%2520progress%2520of%2520general%2520foundation%2520models%2520lags%250Asignificantly%2520compared%2520to%2520those%2520in%2520popular%2520areas%252C%2520far%2520from%2520sufficient%2520for%250Atransforming%2520scientific%2520research%2520and%2520leaving%2520substantial%2520gap%2520between%250Aopen-source%2520models%2520and%2520closed-source%2520models%2520in%2520these%2520scientific%2520domains.%2520To%250Amitigate%2520this%2520gap%2520and%2520explore%2520a%2520step%2520further%2520toward%2520Artificial%2520General%250AIntelligence%2520%2528AGI%2529%252C%2520we%2520introduce%2520Intern-S1%252C%2520a%2520specialized%2520generalist%2520equipped%250Awith%2520general%2520understanding%2520and%2520reasoning%2520capabilities%2520with%2520expertise%2520to%2520analyze%250Amultiple%2520science%2520modal%2520data.%2520Intern-S1%2520is%2520a%2520multimodal%2520Mixture-of-Experts%2520%2528MoE%2529%250Amodel%2520with%252028%2520billion%2520activated%2520parameters%2520and%2520241%2520billion%2520total%2520parameters%252C%250Acontinually%2520pre-trained%2520on%25205T%2520tokens%252C%2520including%2520over%25202.5T%2520tokens%2520from%250Ascientific%2520domains.%2520In%2520the%2520post-training%2520stage%252C%2520Intern-S1%2520undergoes%2520offline%2520and%250Athen%2520online%2520reinforcement%2520learning%2520%2528RL%2529%2520in%2520InternBootCamp%252C%2520where%2520we%2520propose%250AMixture-of-Rewards%2520%2528MoR%2529%2520to%2520synergize%2520the%2520RL%2520training%2520on%2520more%2520than%25201000%2520tasks%250Asimultaneously.%2520Through%2520integrated%2520innovations%2520in%2520algorithms%252C%2520data%252C%2520and%250Atraining%2520systems%252C%2520Intern-S1%2520achieved%2520top-tier%2520performance%2520in%2520online%2520RL%250Atraining.On%2520comprehensive%2520evaluation%2520benchmarks%252C%2520Intern-S1%2520demonstrates%250Acompetitive%2520performance%2520on%2520general%2520reasoning%2520tasks%2520among%2520open-source%2520models%2520and%250Asignificantly%2520outperforms%2520open-source%2520models%2520in%2520scientific%2520domains%252C%2520surpassing%250Aclosed-source%2520state-of-the-art%2520models%2520in%2520professional%2520tasks%252C%2520such%2520as%2520molecular%250Asynthesis%2520planning%252C%2520reaction%2520condition%2520prediction%252C%2520predicting%2520thermodynamic%250Astabilities%2520for%2520crystals.%2520Our%2520models%2520are%2520available%2520at%250Ahttps%253A//huggingface.co/internlm/Intern-S1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intern-S1%3A%20A%20Scientific%20Multimodal%20Foundation%20Model&entry.906535625=Lei%20Bai%20and%20Zhongrui%20Cai%20and%20Maosong%20Cao%20and%20Weihan%20Cao%20and%20Chiyu%20Chen%20and%20Haojiong%20Chen%20and%20Kai%20Chen%20and%20Pengcheng%20Chen%20and%20Ying%20Chen%20and%20Yongkang%20Chen%20and%20Yu%20Cheng%20and%20Yu%20Cheng%20and%20Pei%20Chu%20and%20Tao%20Chu%20and%20Erfei%20Cui%20and%20Ganqu%20Cui%20and%20Long%20Cui%20and%20Ziyun%20Cui%20and%20Nianchen%20Deng%20and%20Ning%20Ding%20and%20Nanqin%20Dong%20and%20Peijie%20Dong%20and%20Shihan%20Dou%20and%20Sinan%20Du%20and%20Haodong%20Duan%20and%20Caihua%20Fan%20and%20Ben%20Gao%20and%20Changjiang%20Gao%20and%20Jianfei%20Gao%20and%20Songyang%20Gao%20and%20Yang%20Gao%20and%20Zhangwei%20Gao%20and%20Jiaye%20Ge%20and%20Qiming%20Ge%20and%20Lixin%20Gu%20and%20Yuzhe%20Gu%20and%20Aijia%20Guo%20and%20Qipeng%20Guo%20and%20Xu%20Guo%20and%20Conghui%20He%20and%20Junjun%20He%20and%20Yili%20Hong%20and%20Siyuan%20Hou%20and%20Caiyu%20Hu%20and%20Hanglei%20Hu%20and%20Jucheng%20Hu%20and%20Ming%20Hu%20and%20Zhouqi%20Hua%20and%20Haian%20Huang%20and%20Junhao%20Huang%20and%20Xu%20Huang%20and%20Zixian%20Huang%20and%20Zhe%20Jiang%20and%20Lingkai%20Kong%20and%20Linyang%20Li%20and%20Peiji%20Li%20and%20Pengze%20Li%20and%20Shuaibin%20Li%20and%20Tianbin%20Li%20and%20Wei%20Li%20and%20Yuqiang%20Li%20and%20Dahua%20Lin%20and%20Junyao%20Lin%20and%20Tianyi%20Lin%20and%20Zhishan%20Lin%20and%20Hongwei%20Liu%20and%20Jiangning%20Liu%20and%20Jiyao%20Liu%20and%20Junnan%20Liu%20and%20Kai%20Liu%20and%20Kaiwen%20Liu%20and%20Kuikun%20Liu%20and%20Shichun%20Liu%20and%20Shudong%20Liu%20and%20Wei%20Liu%20and%20Xinyao%20Liu%20and%20Yuhong%20Liu%20and%20Zhan%20Liu%20and%20Yinquan%20Lu%20and%20Haijun%20Lv%20and%20Hongxia%20Lv%20and%20Huijie%20Lv%20and%20Qidang%20Lv%20and%20Ying%20Lv%20and%20Chengqi%20Lyu%20and%20Chenglong%20Ma%20and%20Jianpeng%20Ma%20and%20Ren%20Ma%20and%20Runmin%20Ma%20and%20Runyuan%20Ma%20and%20Xinzhu%20Ma%20and%20Yichuan%20Ma%20and%20Zihan%20Ma%20and%20Sixuan%20Mi%20and%20Junzhi%20Ning%20and%20Wenchang%20Ning%20and%20Xinle%20Pang%20and%20Jiahui%20Peng%20and%20Runyu%20Peng%20and%20Yu%20Qiao%20and%20Jiantao%20Qiu%20and%20Xiaoye%20Qu%20and%20Yuan%20Qu%20and%20Yuchen%20Ren%20and%20Fukai%20Shang%20and%20Wenqi%20Shao%20and%20Junhao%20Shen%20and%20Shuaike%20Shen%20and%20Chunfeng%20Song%20and%20Demin%20Song%20and%20Diping%20Song%20and%20Chenlin%20Su%20and%20Weijie%20Su%20and%20Weigao%20Sun%20and%20Yu%20Sun%20and%20Qian%20Tan%20and%20Cheng%20Tang%20and%20Huanze%20Tang%20and%20Kexian%20Tang%20and%20Shixiang%20Tang%20and%20Jian%20Tong%20and%20Aoran%20Wang%20and%20Bin%20Wang%20and%20Dong%20Wang%20and%20Lintao%20Wang%20and%20Rui%20Wang%20and%20Weiyun%20Wang%20and%20Wenhai%20Wang%20and%20Yi%20Wang%20and%20Ziyi%20Wang%20and%20Ling-I%20Wu%20and%20Wen%20Wu%20and%20Yue%20Wu%20and%20Zijian%20Wu%20and%20Linchen%20Xiao%20and%20Shuhao%20Xing%20and%20Chao%20Xu%20and%20Huihui%20Xu%20and%20Jun%20Xu%20and%20Ruiliang%20Xu%20and%20Wanghan%20Xu%20and%20GanLin%20Yang%20and%20Yuming%20Yang%20and%20Haochen%20Ye%20and%20Jin%20Ye%20and%20Shenglong%20Ye%20and%20Jia%20Yu%20and%20Jiashuo%20Yu%20and%20Jing%20Yu%20and%20Fei%20Yuan%20and%20Bo%20Zhang%20and%20Chao%20Zhang%20and%20Chen%20Zhang%20and%20Hongjie%20Zhang%20and%20Jin%20Zhang%20and%20Qiaosheng%20Zhang%20and%20Qiuyinzhe%20Zhang%20and%20Songyang%20Zhang%20and%20Taolin%20Zhang%20and%20Wenlong%20Zhang%20and%20Wenwei%20Zhang%20and%20Yechen%20Zhang%20and%20Ziyang%20Zhang%20and%20Haiteng%20Zhao%20and%20Qian%20Zhao%20and%20Xiangyu%20Zhao%20and%20Xiangyu%20Zhao%20and%20Bowen%20Zhou%20and%20Dongzhan%20Zhou%20and%20Peiheng%20Zhou%20and%20Yuhao%20Zhou%20and%20Yunhua%20Zhou%20and%20Dongsheng%20Zhu%20and%20Lin%20Zhu%20and%20Yicheng%20Zou&entry.1292438233=%20%20In%20recent%20years%2C%20a%20plethora%20of%20open-source%20foundation%20models%20have%20emerged%2C%0Aachieving%20remarkable%20progress%20in%20some%20widely%20attended%20fields%2C%20with%20performance%0Abeing%20quite%20close%20to%20that%20of%20closed-source%20models.%20However%2C%20in%20high-value%20but%0Amore%20challenging%20scientific%20professional%20fields%2C%20either%20the%20fields%20still%20rely%0Aon%20expert%20models%2C%20or%20the%20progress%20of%20general%20foundation%20models%20lags%0Asignificantly%20compared%20to%20those%20in%20popular%20areas%2C%20far%20from%20sufficient%20for%0Atransforming%20scientific%20research%20and%20leaving%20substantial%20gap%20between%0Aopen-source%20models%20and%20closed-source%20models%20in%20these%20scientific%20domains.%20To%0Amitigate%20this%20gap%20and%20explore%20a%20step%20further%20toward%20Artificial%20General%0AIntelligence%20%28AGI%29%2C%20we%20introduce%20Intern-S1%2C%20a%20specialized%20generalist%20equipped%0Awith%20general%20understanding%20and%20reasoning%20capabilities%20with%20expertise%20to%20analyze%0Amultiple%20science%20modal%20data.%20Intern-S1%20is%20a%20multimodal%20Mixture-of-Experts%20%28MoE%29%0Amodel%20with%2028%20billion%20activated%20parameters%20and%20241%20billion%20total%20parameters%2C%0Acontinually%20pre-trained%20on%205T%20tokens%2C%20including%20over%202.5T%20tokens%20from%0Ascientific%20domains.%20In%20the%20post-training%20stage%2C%20Intern-S1%20undergoes%20offline%20and%0Athen%20online%20reinforcement%20learning%20%28RL%29%20in%20InternBootCamp%2C%20where%20we%20propose%0AMixture-of-Rewards%20%28MoR%29%20to%20synergize%20the%20RL%20training%20on%20more%20than%201000%20tasks%0Asimultaneously.%20Through%20integrated%20innovations%20in%20algorithms%2C%20data%2C%20and%0Atraining%20systems%2C%20Intern-S1%20achieved%20top-tier%20performance%20in%20online%20RL%0Atraining.On%20comprehensive%20evaluation%20benchmarks%2C%20Intern-S1%20demonstrates%0Acompetitive%20performance%20on%20general%20reasoning%20tasks%20among%20open-source%20models%20and%0Asignificantly%20outperforms%20open-source%20models%20in%20scientific%20domains%2C%20surpassing%0Aclosed-source%20state-of-the-art%20models%20in%20professional%20tasks%2C%20such%20as%20molecular%0Asynthesis%20planning%2C%20reaction%20condition%20prediction%2C%20predicting%20thermodynamic%0Astabilities%20for%20crystals.%20Our%20models%20are%20available%20at%0Ahttps%3A//huggingface.co/internlm/Intern-S1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15763v1&entry.124074799=Read"},
{"title": "From Linearity to Non-Linearity: How Masked Autoencoders Capture Spatial\n  Correlations", "author": "Anthony Bisulco and Rahul Ramesh and Randall Balestriero and Pratik Chaudhari", "abstract": "  Masked Autoencoders (MAEs) have emerged as a powerful pretraining technique\nfor vision foundation models. Despite their effectiveness, they require\nextensive hyperparameter tuning (masking ratio, patch size, encoder/decoder\nlayers) when applied to novel datasets. While prior theoretical works have\nanalyzed MAEs in terms of their attention patterns and hierarchical latent\nvariable models, the connection between MAE hyperparameters and performance on\ndownstream tasks is relatively unexplored. This work investigates how MAEs\nlearn spatial correlations in the input image. We analytically derive the\nfeatures learned by a linear MAE and show that masking ratio and patch size can\nbe used to select for features that capture short- and long-range spatial\ncorrelations. We extend this analysis to non-linear MAEs to show that MAE\nrepresentations adapt to spatial correlations in the dataset, beyond\nsecond-order statistics. Finally, we discuss some insights on how to select MAE\nhyper-parameters in practice.\n", "link": "http://arxiv.org/abs/2508.15404v1", "date": "2025-08-21", "relevancy": 2.7433, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5439}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Linearity%20to%20Non-Linearity%3A%20How%20Masked%20Autoencoders%20Capture%20Spatial%0A%20%20Correlations&body=Title%3A%20From%20Linearity%20to%20Non-Linearity%3A%20How%20Masked%20Autoencoders%20Capture%20Spatial%0A%20%20Correlations%0AAuthor%3A%20Anthony%20Bisulco%20and%20Rahul%20Ramesh%20and%20Randall%20Balestriero%20and%20Pratik%20Chaudhari%0AAbstract%3A%20%20%20Masked%20Autoencoders%20%28MAEs%29%20have%20emerged%20as%20a%20powerful%20pretraining%20technique%0Afor%20vision%20foundation%20models.%20Despite%20their%20effectiveness%2C%20they%20require%0Aextensive%20hyperparameter%20tuning%20%28masking%20ratio%2C%20patch%20size%2C%20encoder/decoder%0Alayers%29%20when%20applied%20to%20novel%20datasets.%20While%20prior%20theoretical%20works%20have%0Aanalyzed%20MAEs%20in%20terms%20of%20their%20attention%20patterns%20and%20hierarchical%20latent%0Avariable%20models%2C%20the%20connection%20between%20MAE%20hyperparameters%20and%20performance%20on%0Adownstream%20tasks%20is%20relatively%20unexplored.%20This%20work%20investigates%20how%20MAEs%0Alearn%20spatial%20correlations%20in%20the%20input%20image.%20We%20analytically%20derive%20the%0Afeatures%20learned%20by%20a%20linear%20MAE%20and%20show%20that%20masking%20ratio%20and%20patch%20size%20can%0Abe%20used%20to%20select%20for%20features%20that%20capture%20short-%20and%20long-range%20spatial%0Acorrelations.%20We%20extend%20this%20analysis%20to%20non-linear%20MAEs%20to%20show%20that%20MAE%0Arepresentations%20adapt%20to%20spatial%20correlations%20in%20the%20dataset%2C%20beyond%0Asecond-order%20statistics.%20Finally%2C%20we%20discuss%20some%20insights%20on%20how%20to%20select%20MAE%0Ahyper-parameters%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Linearity%2520to%2520Non-Linearity%253A%2520How%2520Masked%2520Autoencoders%2520Capture%2520Spatial%250A%2520%2520Correlations%26entry.906535625%3DAnthony%2520Bisulco%2520and%2520Rahul%2520Ramesh%2520and%2520Randall%2520Balestriero%2520and%2520Pratik%2520Chaudhari%26entry.1292438233%3D%2520%2520Masked%2520Autoencoders%2520%2528MAEs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520pretraining%2520technique%250Afor%2520vision%2520foundation%2520models.%2520Despite%2520their%2520effectiveness%252C%2520they%2520require%250Aextensive%2520hyperparameter%2520tuning%2520%2528masking%2520ratio%252C%2520patch%2520size%252C%2520encoder/decoder%250Alayers%2529%2520when%2520applied%2520to%2520novel%2520datasets.%2520While%2520prior%2520theoretical%2520works%2520have%250Aanalyzed%2520MAEs%2520in%2520terms%2520of%2520their%2520attention%2520patterns%2520and%2520hierarchical%2520latent%250Avariable%2520models%252C%2520the%2520connection%2520between%2520MAE%2520hyperparameters%2520and%2520performance%2520on%250Adownstream%2520tasks%2520is%2520relatively%2520unexplored.%2520This%2520work%2520investigates%2520how%2520MAEs%250Alearn%2520spatial%2520correlations%2520in%2520the%2520input%2520image.%2520We%2520analytically%2520derive%2520the%250Afeatures%2520learned%2520by%2520a%2520linear%2520MAE%2520and%2520show%2520that%2520masking%2520ratio%2520and%2520patch%2520size%2520can%250Abe%2520used%2520to%2520select%2520for%2520features%2520that%2520capture%2520short-%2520and%2520long-range%2520spatial%250Acorrelations.%2520We%2520extend%2520this%2520analysis%2520to%2520non-linear%2520MAEs%2520to%2520show%2520that%2520MAE%250Arepresentations%2520adapt%2520to%2520spatial%2520correlations%2520in%2520the%2520dataset%252C%2520beyond%250Asecond-order%2520statistics.%2520Finally%252C%2520we%2520discuss%2520some%2520insights%2520on%2520how%2520to%2520select%2520MAE%250Ahyper-parameters%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Linearity%20to%20Non-Linearity%3A%20How%20Masked%20Autoencoders%20Capture%20Spatial%0A%20%20Correlations&entry.906535625=Anthony%20Bisulco%20and%20Rahul%20Ramesh%20and%20Randall%20Balestriero%20and%20Pratik%20Chaudhari&entry.1292438233=%20%20Masked%20Autoencoders%20%28MAEs%29%20have%20emerged%20as%20a%20powerful%20pretraining%20technique%0Afor%20vision%20foundation%20models.%20Despite%20their%20effectiveness%2C%20they%20require%0Aextensive%20hyperparameter%20tuning%20%28masking%20ratio%2C%20patch%20size%2C%20encoder/decoder%0Alayers%29%20when%20applied%20to%20novel%20datasets.%20While%20prior%20theoretical%20works%20have%0Aanalyzed%20MAEs%20in%20terms%20of%20their%20attention%20patterns%20and%20hierarchical%20latent%0Avariable%20models%2C%20the%20connection%20between%20MAE%20hyperparameters%20and%20performance%20on%0Adownstream%20tasks%20is%20relatively%20unexplored.%20This%20work%20investigates%20how%20MAEs%0Alearn%20spatial%20correlations%20in%20the%20input%20image.%20We%20analytically%20derive%20the%0Afeatures%20learned%20by%20a%20linear%20MAE%20and%20show%20that%20masking%20ratio%20and%20patch%20size%20can%0Abe%20used%20to%20select%20for%20features%20that%20capture%20short-%20and%20long-range%20spatial%0Acorrelations.%20We%20extend%20this%20analysis%20to%20non-linear%20MAEs%20to%20show%20that%20MAE%0Arepresentations%20adapt%20to%20spatial%20correlations%20in%20the%20dataset%2C%20beyond%0Asecond-order%20statistics.%20Finally%2C%20we%20discuss%20some%20insights%20on%20how%20to%20select%20MAE%0Ahyper-parameters%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15404v1&entry.124074799=Read"},
{"title": "Preacher: Paper-to-Video Agentic System", "author": "Jingwei Liu and Ling Yang and Hao Luo and Fan Wang and Hongyan Li and Mengdi Wang", "abstract": "  The paper-to-video task converts a research paper into a structured video\nabstract, distilling key concepts, methods, and conclusions into an accessible,\nwell-organized format. While state-of-the-art video generation models\ndemonstrate potential, they are constrained by limited context windows, rigid\nvideo duration constraints, limited stylistic diversity, and an inability to\nrepresent domain-specific knowledge. To address these limitations, we introduce\nPreacher, the first paper-to-video agentic system. Preacher employs a topdown\napproach to decompose, summarize, and reformulate the paper, followed by\nbottom-up video generation, synthesizing diverse video segments into a coherent\nabstract. To align cross-modal representations, we define key scenes and\nintroduce a Progressive Chain of Thought (P-CoT) for granular, iterative\nplanning. Preacher successfully generates high-quality video abstracts across\nfive research fields, demonstrating expertise beyond current video generation\nmodels. Code will be released at: https://github.com/GenVerse/Paper2Video\n", "link": "http://arxiv.org/abs/2508.09632v4", "date": "2025-08-21", "relevancy": 2.7077, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5584}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.534}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preacher%3A%20Paper-to-Video%20Agentic%20System&body=Title%3A%20Preacher%3A%20Paper-to-Video%20Agentic%20System%0AAuthor%3A%20Jingwei%20Liu%20and%20Ling%20Yang%20and%20Hao%20Luo%20and%20Fan%20Wang%20and%20Hongyan%20Li%20and%20Mengdi%20Wang%0AAbstract%3A%20%20%20The%20paper-to-video%20task%20converts%20a%20research%20paper%20into%20a%20structured%20video%0Aabstract%2C%20distilling%20key%20concepts%2C%20methods%2C%20and%20conclusions%20into%20an%20accessible%2C%0Awell-organized%20format.%20While%20state-of-the-art%20video%20generation%20models%0Ademonstrate%20potential%2C%20they%20are%20constrained%20by%20limited%20context%20windows%2C%20rigid%0Avideo%20duration%20constraints%2C%20limited%20stylistic%20diversity%2C%20and%20an%20inability%20to%0Arepresent%20domain-specific%20knowledge.%20To%20address%20these%20limitations%2C%20we%20introduce%0APreacher%2C%20the%20first%20paper-to-video%20agentic%20system.%20Preacher%20employs%20a%20topdown%0Aapproach%20to%20decompose%2C%20summarize%2C%20and%20reformulate%20the%20paper%2C%20followed%20by%0Abottom-up%20video%20generation%2C%20synthesizing%20diverse%20video%20segments%20into%20a%20coherent%0Aabstract.%20To%20align%20cross-modal%20representations%2C%20we%20define%20key%20scenes%20and%0Aintroduce%20a%20Progressive%20Chain%20of%20Thought%20%28P-CoT%29%20for%20granular%2C%20iterative%0Aplanning.%20Preacher%20successfully%20generates%20high-quality%20video%20abstracts%20across%0Afive%20research%20fields%2C%20demonstrating%20expertise%20beyond%20current%20video%20generation%0Amodels.%20Code%20will%20be%20released%20at%3A%20https%3A//github.com/GenVerse/Paper2Video%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09632v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreacher%253A%2520Paper-to-Video%2520Agentic%2520System%26entry.906535625%3DJingwei%2520Liu%2520and%2520Ling%2520Yang%2520and%2520Hao%2520Luo%2520and%2520Fan%2520Wang%2520and%2520Hongyan%2520Li%2520and%2520Mengdi%2520Wang%26entry.1292438233%3D%2520%2520The%2520paper-to-video%2520task%2520converts%2520a%2520research%2520paper%2520into%2520a%2520structured%2520video%250Aabstract%252C%2520distilling%2520key%2520concepts%252C%2520methods%252C%2520and%2520conclusions%2520into%2520an%2520accessible%252C%250Awell-organized%2520format.%2520While%2520state-of-the-art%2520video%2520generation%2520models%250Ademonstrate%2520potential%252C%2520they%2520are%2520constrained%2520by%2520limited%2520context%2520windows%252C%2520rigid%250Avideo%2520duration%2520constraints%252C%2520limited%2520stylistic%2520diversity%252C%2520and%2520an%2520inability%2520to%250Arepresent%2520domain-specific%2520knowledge.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%250APreacher%252C%2520the%2520first%2520paper-to-video%2520agentic%2520system.%2520Preacher%2520employs%2520a%2520topdown%250Aapproach%2520to%2520decompose%252C%2520summarize%252C%2520and%2520reformulate%2520the%2520paper%252C%2520followed%2520by%250Abottom-up%2520video%2520generation%252C%2520synthesizing%2520diverse%2520video%2520segments%2520into%2520a%2520coherent%250Aabstract.%2520To%2520align%2520cross-modal%2520representations%252C%2520we%2520define%2520key%2520scenes%2520and%250Aintroduce%2520a%2520Progressive%2520Chain%2520of%2520Thought%2520%2528P-CoT%2529%2520for%2520granular%252C%2520iterative%250Aplanning.%2520Preacher%2520successfully%2520generates%2520high-quality%2520video%2520abstracts%2520across%250Afive%2520research%2520fields%252C%2520demonstrating%2520expertise%2520beyond%2520current%2520video%2520generation%250Amodels.%2520Code%2520will%2520be%2520released%2520at%253A%2520https%253A//github.com/GenVerse/Paper2Video%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09632v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preacher%3A%20Paper-to-Video%20Agentic%20System&entry.906535625=Jingwei%20Liu%20and%20Ling%20Yang%20and%20Hao%20Luo%20and%20Fan%20Wang%20and%20Hongyan%20Li%20and%20Mengdi%20Wang&entry.1292438233=%20%20The%20paper-to-video%20task%20converts%20a%20research%20paper%20into%20a%20structured%20video%0Aabstract%2C%20distilling%20key%20concepts%2C%20methods%2C%20and%20conclusions%20into%20an%20accessible%2C%0Awell-organized%20format.%20While%20state-of-the-art%20video%20generation%20models%0Ademonstrate%20potential%2C%20they%20are%20constrained%20by%20limited%20context%20windows%2C%20rigid%0Avideo%20duration%20constraints%2C%20limited%20stylistic%20diversity%2C%20and%20an%20inability%20to%0Arepresent%20domain-specific%20knowledge.%20To%20address%20these%20limitations%2C%20we%20introduce%0APreacher%2C%20the%20first%20paper-to-video%20agentic%20system.%20Preacher%20employs%20a%20topdown%0Aapproach%20to%20decompose%2C%20summarize%2C%20and%20reformulate%20the%20paper%2C%20followed%20by%0Abottom-up%20video%20generation%2C%20synthesizing%20diverse%20video%20segments%20into%20a%20coherent%0Aabstract.%20To%20align%20cross-modal%20representations%2C%20we%20define%20key%20scenes%20and%0Aintroduce%20a%20Progressive%20Chain%20of%20Thought%20%28P-CoT%29%20for%20granular%2C%20iterative%0Aplanning.%20Preacher%20successfully%20generates%20high-quality%20video%20abstracts%20across%0Afive%20research%20fields%2C%20demonstrating%20expertise%20beyond%20current%20video%20generation%0Amodels.%20Code%20will%20be%20released%20at%3A%20https%3A//github.com/GenVerse/Paper2Video%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09632v4&entry.124074799=Read"},
{"title": "Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered\n  All-in-SAM Model", "author": "Xueyuan Li and Can Cui and Ruining Deng and Yucheng Tang and Quan Liu and Tianyuan Yao and Shunxing Bao and Naweed Chowdhury and Haichun Yang and Yuankai Huo", "abstract": "  Purpose: Recent developments in computational pathology have been driven by\nadvances in Vision Foundation Models, particularly the Segment Anything Model\n(SAM). This model facilitates nuclei segmentation through two primary methods:\nprompt-based zero-shot segmentation and the use of cell-specific SAM models for\ndirect segmentation. These approaches enable effective segmentation across a\nrange of nuclei and cells. However, general vision foundation models often face\nchallenges with fine-grained semantic segmentation, such as identifying\nspecific nuclei subtypes or particular cells. Approach: In this paper, we\npropose the molecular-empowered All-in-SAM Model to advance computational\npathology by leveraging the capabilities of vision foundation models. This\nmodel incorporates a full-stack approach, focusing on: (1) annotation-engaging\nlay annotators through molecular-empowered learning to reduce the need for\ndetailed pixel-level annotations, (2) learning-adapting the SAM model to\nemphasize specific semantics, which utilizes its strong generalizability with\nSAM adapter, and (3) refinement-enhancing segmentation accuracy by integrating\nMolecular-Oriented Corrective Learning (MOCL). Results: Experimental results\nfrom both in-house and public datasets show that the All-in-SAM model\nsignificantly improves cell classification performance, even when faced with\nvarying annotation quality. Conclusions: Our approach not only reduces the\nworkload for annotators but also extends the accessibility of precise\nbiomedical image analysis to resource-limited settings, thereby advancing\nmedical diagnostics and automating pathology image analysis.\n", "link": "http://arxiv.org/abs/2508.15751v1", "date": "2025-08-21", "relevancy": 2.6812, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5368}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5368}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-grained%20Multi-class%20Nuclei%20Segmentation%20with%20Molecular-empowered%0A%20%20All-in-SAM%20Model&body=Title%3A%20Fine-grained%20Multi-class%20Nuclei%20Segmentation%20with%20Molecular-empowered%0A%20%20All-in-SAM%20Model%0AAuthor%3A%20Xueyuan%20Li%20and%20Can%20Cui%20and%20Ruining%20Deng%20and%20Yucheng%20Tang%20and%20Quan%20Liu%20and%20Tianyuan%20Yao%20and%20Shunxing%20Bao%20and%20Naweed%20Chowdhury%20and%20Haichun%20Yang%20and%20Yuankai%20Huo%0AAbstract%3A%20%20%20Purpose%3A%20Recent%20developments%20in%20computational%20pathology%20have%20been%20driven%20by%0Aadvances%20in%20Vision%20Foundation%20Models%2C%20particularly%20the%20Segment%20Anything%20Model%0A%28SAM%29.%20This%20model%20facilitates%20nuclei%20segmentation%20through%20two%20primary%20methods%3A%0Aprompt-based%20zero-shot%20segmentation%20and%20the%20use%20of%20cell-specific%20SAM%20models%20for%0Adirect%20segmentation.%20These%20approaches%20enable%20effective%20segmentation%20across%20a%0Arange%20of%20nuclei%20and%20cells.%20However%2C%20general%20vision%20foundation%20models%20often%20face%0Achallenges%20with%20fine-grained%20semantic%20segmentation%2C%20such%20as%20identifying%0Aspecific%20nuclei%20subtypes%20or%20particular%20cells.%20Approach%3A%20In%20this%20paper%2C%20we%0Apropose%20the%20molecular-empowered%20All-in-SAM%20Model%20to%20advance%20computational%0Apathology%20by%20leveraging%20the%20capabilities%20of%20vision%20foundation%20models.%20This%0Amodel%20incorporates%20a%20full-stack%20approach%2C%20focusing%20on%3A%20%281%29%20annotation-engaging%0Alay%20annotators%20through%20molecular-empowered%20learning%20to%20reduce%20the%20need%20for%0Adetailed%20pixel-level%20annotations%2C%20%282%29%20learning-adapting%20the%20SAM%20model%20to%0Aemphasize%20specific%20semantics%2C%20which%20utilizes%20its%20strong%20generalizability%20with%0ASAM%20adapter%2C%20and%20%283%29%20refinement-enhancing%20segmentation%20accuracy%20by%20integrating%0AMolecular-Oriented%20Corrective%20Learning%20%28MOCL%29.%20Results%3A%20Experimental%20results%0Afrom%20both%20in-house%20and%20public%20datasets%20show%20that%20the%20All-in-SAM%20model%0Asignificantly%20improves%20cell%20classification%20performance%2C%20even%20when%20faced%20with%0Avarying%20annotation%20quality.%20Conclusions%3A%20Our%20approach%20not%20only%20reduces%20the%0Aworkload%20for%20annotators%20but%20also%20extends%20the%20accessibility%20of%20precise%0Abiomedical%20image%20analysis%20to%20resource-limited%20settings%2C%20thereby%20advancing%0Amedical%20diagnostics%20and%20automating%20pathology%20image%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-grained%2520Multi-class%2520Nuclei%2520Segmentation%2520with%2520Molecular-empowered%250A%2520%2520All-in-SAM%2520Model%26entry.906535625%3DXueyuan%2520Li%2520and%2520Can%2520Cui%2520and%2520Ruining%2520Deng%2520and%2520Yucheng%2520Tang%2520and%2520Quan%2520Liu%2520and%2520Tianyuan%2520Yao%2520and%2520Shunxing%2520Bao%2520and%2520Naweed%2520Chowdhury%2520and%2520Haichun%2520Yang%2520and%2520Yuankai%2520Huo%26entry.1292438233%3D%2520%2520Purpose%253A%2520Recent%2520developments%2520in%2520computational%2520pathology%2520have%2520been%2520driven%2520by%250Aadvances%2520in%2520Vision%2520Foundation%2520Models%252C%2520particularly%2520the%2520Segment%2520Anything%2520Model%250A%2528SAM%2529.%2520This%2520model%2520facilitates%2520nuclei%2520segmentation%2520through%2520two%2520primary%2520methods%253A%250Aprompt-based%2520zero-shot%2520segmentation%2520and%2520the%2520use%2520of%2520cell-specific%2520SAM%2520models%2520for%250Adirect%2520segmentation.%2520These%2520approaches%2520enable%2520effective%2520segmentation%2520across%2520a%250Arange%2520of%2520nuclei%2520and%2520cells.%2520However%252C%2520general%2520vision%2520foundation%2520models%2520often%2520face%250Achallenges%2520with%2520fine-grained%2520semantic%2520segmentation%252C%2520such%2520as%2520identifying%250Aspecific%2520nuclei%2520subtypes%2520or%2520particular%2520cells.%2520Approach%253A%2520In%2520this%2520paper%252C%2520we%250Apropose%2520the%2520molecular-empowered%2520All-in-SAM%2520Model%2520to%2520advance%2520computational%250Apathology%2520by%2520leveraging%2520the%2520capabilities%2520of%2520vision%2520foundation%2520models.%2520This%250Amodel%2520incorporates%2520a%2520full-stack%2520approach%252C%2520focusing%2520on%253A%2520%25281%2529%2520annotation-engaging%250Alay%2520annotators%2520through%2520molecular-empowered%2520learning%2520to%2520reduce%2520the%2520need%2520for%250Adetailed%2520pixel-level%2520annotations%252C%2520%25282%2529%2520learning-adapting%2520the%2520SAM%2520model%2520to%250Aemphasize%2520specific%2520semantics%252C%2520which%2520utilizes%2520its%2520strong%2520generalizability%2520with%250ASAM%2520adapter%252C%2520and%2520%25283%2529%2520refinement-enhancing%2520segmentation%2520accuracy%2520by%2520integrating%250AMolecular-Oriented%2520Corrective%2520Learning%2520%2528MOCL%2529.%2520Results%253A%2520Experimental%2520results%250Afrom%2520both%2520in-house%2520and%2520public%2520datasets%2520show%2520that%2520the%2520All-in-SAM%2520model%250Asignificantly%2520improves%2520cell%2520classification%2520performance%252C%2520even%2520when%2520faced%2520with%250Avarying%2520annotation%2520quality.%2520Conclusions%253A%2520Our%2520approach%2520not%2520only%2520reduces%2520the%250Aworkload%2520for%2520annotators%2520but%2520also%2520extends%2520the%2520accessibility%2520of%2520precise%250Abiomedical%2520image%2520analysis%2520to%2520resource-limited%2520settings%252C%2520thereby%2520advancing%250Amedical%2520diagnostics%2520and%2520automating%2520pathology%2520image%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-grained%20Multi-class%20Nuclei%20Segmentation%20with%20Molecular-empowered%0A%20%20All-in-SAM%20Model&entry.906535625=Xueyuan%20Li%20and%20Can%20Cui%20and%20Ruining%20Deng%20and%20Yucheng%20Tang%20and%20Quan%20Liu%20and%20Tianyuan%20Yao%20and%20Shunxing%20Bao%20and%20Naweed%20Chowdhury%20and%20Haichun%20Yang%20and%20Yuankai%20Huo&entry.1292438233=%20%20Purpose%3A%20Recent%20developments%20in%20computational%20pathology%20have%20been%20driven%20by%0Aadvances%20in%20Vision%20Foundation%20Models%2C%20particularly%20the%20Segment%20Anything%20Model%0A%28SAM%29.%20This%20model%20facilitates%20nuclei%20segmentation%20through%20two%20primary%20methods%3A%0Aprompt-based%20zero-shot%20segmentation%20and%20the%20use%20of%20cell-specific%20SAM%20models%20for%0Adirect%20segmentation.%20These%20approaches%20enable%20effective%20segmentation%20across%20a%0Arange%20of%20nuclei%20and%20cells.%20However%2C%20general%20vision%20foundation%20models%20often%20face%0Achallenges%20with%20fine-grained%20semantic%20segmentation%2C%20such%20as%20identifying%0Aspecific%20nuclei%20subtypes%20or%20particular%20cells.%20Approach%3A%20In%20this%20paper%2C%20we%0Apropose%20the%20molecular-empowered%20All-in-SAM%20Model%20to%20advance%20computational%0Apathology%20by%20leveraging%20the%20capabilities%20of%20vision%20foundation%20models.%20This%0Amodel%20incorporates%20a%20full-stack%20approach%2C%20focusing%20on%3A%20%281%29%20annotation-engaging%0Alay%20annotators%20through%20molecular-empowered%20learning%20to%20reduce%20the%20need%20for%0Adetailed%20pixel-level%20annotations%2C%20%282%29%20learning-adapting%20the%20SAM%20model%20to%0Aemphasize%20specific%20semantics%2C%20which%20utilizes%20its%20strong%20generalizability%20with%0ASAM%20adapter%2C%20and%20%283%29%20refinement-enhancing%20segmentation%20accuracy%20by%20integrating%0AMolecular-Oriented%20Corrective%20Learning%20%28MOCL%29.%20Results%3A%20Experimental%20results%0Afrom%20both%20in-house%20and%20public%20datasets%20show%20that%20the%20All-in-SAM%20model%0Asignificantly%20improves%20cell%20classification%20performance%2C%20even%20when%20faced%20with%0Avarying%20annotation%20quality.%20Conclusions%3A%20Our%20approach%20not%20only%20reduces%20the%0Aworkload%20for%20annotators%20but%20also%20extends%20the%20accessibility%20of%20precise%0Abiomedical%20image%20analysis%20to%20resource-limited%20settings%2C%20thereby%20advancing%0Amedical%20diagnostics%20and%20automating%20pathology%20image%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15751v1&entry.124074799=Read"},
{"title": "When Audio and Text Disagree: Revealing Text Bias in Large\n  Audio-Language Models", "author": "Cheng Wang and Gelei Deng and Xianglin Yang and Han Qiu and Tianwei Zhang", "abstract": "  Large Audio-Language Models (LALMs) are enhanced with audio perception\ncapabilities, enabling them to effectively process and understand multimodal\ninputs that combine audio and text. However, their performance in handling\nconflicting information between audio and text modalities remains largely\nunexamined. This paper introduces MCR-BENCH, the first comprehensive benchmark\nspecifically designed to evaluate how LALMs prioritize information when\npresented with inconsistent audio-text pairs. Through extensive evaluation\nacross diverse audio understanding tasks, we reveal a concerning phenomenon:\nwhen inconsistencies exist between modalities, LALMs display a significant bias\ntoward textual input, frequently disregarding audio evidence. This tendency\nleads to substantial performance degradation in audio-centric tasks and raises\nimportant reliability concerns for real-world applications. We further\ninvestigate the influencing factors of text bias, and explore mitigation\nstrategies through supervised finetuning, and analyze model confidence patterns\nthat reveal persistent overconfidence even with contradictory inputs. These\nfindings underscore the need for improved modality balance during training and\nmore sophisticated fusion mechanisms to enhance the robustness when handling\nconflicting multi-modal inputs. The project is available at\nhttps://github.com/WangCheng0116/MCR-BENCH.\n", "link": "http://arxiv.org/abs/2508.15407v1", "date": "2025-08-21", "relevancy": 2.6373, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5328}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5328}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Audio%20and%20Text%20Disagree%3A%20Revealing%20Text%20Bias%20in%20Large%0A%20%20Audio-Language%20Models&body=Title%3A%20When%20Audio%20and%20Text%20Disagree%3A%20Revealing%20Text%20Bias%20in%20Large%0A%20%20Audio-Language%20Models%0AAuthor%3A%20Cheng%20Wang%20and%20Gelei%20Deng%20and%20Xianglin%20Yang%20and%20Han%20Qiu%20and%20Tianwei%20Zhang%0AAbstract%3A%20%20%20Large%20Audio-Language%20Models%20%28LALMs%29%20are%20enhanced%20with%20audio%20perception%0Acapabilities%2C%20enabling%20them%20to%20effectively%20process%20and%20understand%20multimodal%0Ainputs%20that%20combine%20audio%20and%20text.%20However%2C%20their%20performance%20in%20handling%0Aconflicting%20information%20between%20audio%20and%20text%20modalities%20remains%20largely%0Aunexamined.%20This%20paper%20introduces%20MCR-BENCH%2C%20the%20first%20comprehensive%20benchmark%0Aspecifically%20designed%20to%20evaluate%20how%20LALMs%20prioritize%20information%20when%0Apresented%20with%20inconsistent%20audio-text%20pairs.%20Through%20extensive%20evaluation%0Aacross%20diverse%20audio%20understanding%20tasks%2C%20we%20reveal%20a%20concerning%20phenomenon%3A%0Awhen%20inconsistencies%20exist%20between%20modalities%2C%20LALMs%20display%20a%20significant%20bias%0Atoward%20textual%20input%2C%20frequently%20disregarding%20audio%20evidence.%20This%20tendency%0Aleads%20to%20substantial%20performance%20degradation%20in%20audio-centric%20tasks%20and%20raises%0Aimportant%20reliability%20concerns%20for%20real-world%20applications.%20We%20further%0Ainvestigate%20the%20influencing%20factors%20of%20text%20bias%2C%20and%20explore%20mitigation%0Astrategies%20through%20supervised%20finetuning%2C%20and%20analyze%20model%20confidence%20patterns%0Athat%20reveal%20persistent%20overconfidence%20even%20with%20contradictory%20inputs.%20These%0Afindings%20underscore%20the%20need%20for%20improved%20modality%20balance%20during%20training%20and%0Amore%20sophisticated%20fusion%20mechanisms%20to%20enhance%20the%20robustness%20when%20handling%0Aconflicting%20multi-modal%20inputs.%20The%20project%20is%20available%20at%0Ahttps%3A//github.com/WangCheng0116/MCR-BENCH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Audio%2520and%2520Text%2520Disagree%253A%2520Revealing%2520Text%2520Bias%2520in%2520Large%250A%2520%2520Audio-Language%2520Models%26entry.906535625%3DCheng%2520Wang%2520and%2520Gelei%2520Deng%2520and%2520Xianglin%2520Yang%2520and%2520Han%2520Qiu%2520and%2520Tianwei%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Audio-Language%2520Models%2520%2528LALMs%2529%2520are%2520enhanced%2520with%2520audio%2520perception%250Acapabilities%252C%2520enabling%2520them%2520to%2520effectively%2520process%2520and%2520understand%2520multimodal%250Ainputs%2520that%2520combine%2520audio%2520and%2520text.%2520However%252C%2520their%2520performance%2520in%2520handling%250Aconflicting%2520information%2520between%2520audio%2520and%2520text%2520modalities%2520remains%2520largely%250Aunexamined.%2520This%2520paper%2520introduces%2520MCR-BENCH%252C%2520the%2520first%2520comprehensive%2520benchmark%250Aspecifically%2520designed%2520to%2520evaluate%2520how%2520LALMs%2520prioritize%2520information%2520when%250Apresented%2520with%2520inconsistent%2520audio-text%2520pairs.%2520Through%2520extensive%2520evaluation%250Aacross%2520diverse%2520audio%2520understanding%2520tasks%252C%2520we%2520reveal%2520a%2520concerning%2520phenomenon%253A%250Awhen%2520inconsistencies%2520exist%2520between%2520modalities%252C%2520LALMs%2520display%2520a%2520significant%2520bias%250Atoward%2520textual%2520input%252C%2520frequently%2520disregarding%2520audio%2520evidence.%2520This%2520tendency%250Aleads%2520to%2520substantial%2520performance%2520degradation%2520in%2520audio-centric%2520tasks%2520and%2520raises%250Aimportant%2520reliability%2520concerns%2520for%2520real-world%2520applications.%2520We%2520further%250Ainvestigate%2520the%2520influencing%2520factors%2520of%2520text%2520bias%252C%2520and%2520explore%2520mitigation%250Astrategies%2520through%2520supervised%2520finetuning%252C%2520and%2520analyze%2520model%2520confidence%2520patterns%250Athat%2520reveal%2520persistent%2520overconfidence%2520even%2520with%2520contradictory%2520inputs.%2520These%250Afindings%2520underscore%2520the%2520need%2520for%2520improved%2520modality%2520balance%2520during%2520training%2520and%250Amore%2520sophisticated%2520fusion%2520mechanisms%2520to%2520enhance%2520the%2520robustness%2520when%2520handling%250Aconflicting%2520multi-modal%2520inputs.%2520The%2520project%2520is%2520available%2520at%250Ahttps%253A//github.com/WangCheng0116/MCR-BENCH.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Audio%20and%20Text%20Disagree%3A%20Revealing%20Text%20Bias%20in%20Large%0A%20%20Audio-Language%20Models&entry.906535625=Cheng%20Wang%20and%20Gelei%20Deng%20and%20Xianglin%20Yang%20and%20Han%20Qiu%20and%20Tianwei%20Zhang&entry.1292438233=%20%20Large%20Audio-Language%20Models%20%28LALMs%29%20are%20enhanced%20with%20audio%20perception%0Acapabilities%2C%20enabling%20them%20to%20effectively%20process%20and%20understand%20multimodal%0Ainputs%20that%20combine%20audio%20and%20text.%20However%2C%20their%20performance%20in%20handling%0Aconflicting%20information%20between%20audio%20and%20text%20modalities%20remains%20largely%0Aunexamined.%20This%20paper%20introduces%20MCR-BENCH%2C%20the%20first%20comprehensive%20benchmark%0Aspecifically%20designed%20to%20evaluate%20how%20LALMs%20prioritize%20information%20when%0Apresented%20with%20inconsistent%20audio-text%20pairs.%20Through%20extensive%20evaluation%0Aacross%20diverse%20audio%20understanding%20tasks%2C%20we%20reveal%20a%20concerning%20phenomenon%3A%0Awhen%20inconsistencies%20exist%20between%20modalities%2C%20LALMs%20display%20a%20significant%20bias%0Atoward%20textual%20input%2C%20frequently%20disregarding%20audio%20evidence.%20This%20tendency%0Aleads%20to%20substantial%20performance%20degradation%20in%20audio-centric%20tasks%20and%20raises%0Aimportant%20reliability%20concerns%20for%20real-world%20applications.%20We%20further%0Ainvestigate%20the%20influencing%20factors%20of%20text%20bias%2C%20and%20explore%20mitigation%0Astrategies%20through%20supervised%20finetuning%2C%20and%20analyze%20model%20confidence%20patterns%0Athat%20reveal%20persistent%20overconfidence%20even%20with%20contradictory%20inputs.%20These%0Afindings%20underscore%20the%20need%20for%20improved%20modality%20balance%20during%20training%20and%0Amore%20sophisticated%20fusion%20mechanisms%20to%20enhance%20the%20robustness%20when%20handling%0Aconflicting%20multi-modal%20inputs.%20The%20project%20is%20available%20at%0Ahttps%3A//github.com/WangCheng0116/MCR-BENCH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15407v1&entry.124074799=Read"},
{"title": "An Empirical Study of Knowledge Distillation for Code Understanding\n  Tasks", "author": "Ruiqi Wang and Zezhou Yang and Cuiyun Gao and Xin Xia and Qing Liao", "abstract": "  Pre-trained language models (PLMs) have emerged as powerful tools for code\nunderstanding. However, deploying these PLMs in large-scale applications faces\npractical challenges due to their computational intensity and inference\nlatency. Knowledge distillation (KD), a promising model compression and\nacceleration technique, addresses these limitations by transferring knowledge\nfrom large teacher models to compact student models, enabling efficient\ninference while preserving most of the teacher models' capabilities. While this\ntechnique has shown remarkable success in natural language processing and\ncomputer vision domains, its potential for code understanding tasks remains\nlargely underexplored.\n  In this paper, we systematically investigate the effectiveness and usage of\nKD in code understanding tasks. Our study encompasses two popular types of KD\nmethods, i.e., logit-based and feature-based KD methods, experimenting across\neight student models and two teacher PLMs from different domains on three\ndownstream tasks. The experimental results indicate that KD consistently offers\nnotable performance boosts across student models with different sizes compared\nwith standard fine-tuning. Notably, code-specific PLM demonstrates better\neffectiveness as the teacher model. Among all KD methods, the latest\nfeature-based KD methods exhibit superior performance, enabling student models\nto retain up to 98% teacher performance with merely 5% parameters. Regarding\nstudent architecture, our experiments reveal that similarity with teacher\narchitecture does not necessarily lead to better performance. We further\ndiscuss the efficiency and behaviors in the KD process and inference, summarize\nthe implications of findings, and identify promising future directions.\n", "link": "http://arxiv.org/abs/2508.15423v1", "date": "2025-08-21", "relevancy": 2.6193, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5346}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empirical%20Study%20of%20Knowledge%20Distillation%20for%20Code%20Understanding%0A%20%20Tasks&body=Title%3A%20An%20Empirical%20Study%20of%20Knowledge%20Distillation%20for%20Code%20Understanding%0A%20%20Tasks%0AAuthor%3A%20Ruiqi%20Wang%20and%20Zezhou%20Yang%20and%20Cuiyun%20Gao%20and%20Xin%20Xia%20and%20Qing%20Liao%0AAbstract%3A%20%20%20Pre-trained%20language%20models%20%28PLMs%29%20have%20emerged%20as%20powerful%20tools%20for%20code%0Aunderstanding.%20However%2C%20deploying%20these%20PLMs%20in%20large-scale%20applications%20faces%0Apractical%20challenges%20due%20to%20their%20computational%20intensity%20and%20inference%0Alatency.%20Knowledge%20distillation%20%28KD%29%2C%20a%20promising%20model%20compression%20and%0Aacceleration%20technique%2C%20addresses%20these%20limitations%20by%20transferring%20knowledge%0Afrom%20large%20teacher%20models%20to%20compact%20student%20models%2C%20enabling%20efficient%0Ainference%20while%20preserving%20most%20of%20the%20teacher%20models%27%20capabilities.%20While%20this%0Atechnique%20has%20shown%20remarkable%20success%20in%20natural%20language%20processing%20and%0Acomputer%20vision%20domains%2C%20its%20potential%20for%20code%20understanding%20tasks%20remains%0Alargely%20underexplored.%0A%20%20In%20this%20paper%2C%20we%20systematically%20investigate%20the%20effectiveness%20and%20usage%20of%0AKD%20in%20code%20understanding%20tasks.%20Our%20study%20encompasses%20two%20popular%20types%20of%20KD%0Amethods%2C%20i.e.%2C%20logit-based%20and%20feature-based%20KD%20methods%2C%20experimenting%20across%0Aeight%20student%20models%20and%20two%20teacher%20PLMs%20from%20different%20domains%20on%20three%0Adownstream%20tasks.%20The%20experimental%20results%20indicate%20that%20KD%20consistently%20offers%0Anotable%20performance%20boosts%20across%20student%20models%20with%20different%20sizes%20compared%0Awith%20standard%20fine-tuning.%20Notably%2C%20code-specific%20PLM%20demonstrates%20better%0Aeffectiveness%20as%20the%20teacher%20model.%20Among%20all%20KD%20methods%2C%20the%20latest%0Afeature-based%20KD%20methods%20exhibit%20superior%20performance%2C%20enabling%20student%20models%0Ato%20retain%20up%20to%2098%25%20teacher%20performance%20with%20merely%205%25%20parameters.%20Regarding%0Astudent%20architecture%2C%20our%20experiments%20reveal%20that%20similarity%20with%20teacher%0Aarchitecture%20does%20not%20necessarily%20lead%20to%20better%20performance.%20We%20further%0Adiscuss%20the%20efficiency%20and%20behaviors%20in%20the%20KD%20process%20and%20inference%2C%20summarize%0Athe%20implications%20of%20findings%2C%20and%20identify%20promising%20future%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empirical%2520Study%2520of%2520Knowledge%2520Distillation%2520for%2520Code%2520Understanding%250A%2520%2520Tasks%26entry.906535625%3DRuiqi%2520Wang%2520and%2520Zezhou%2520Yang%2520and%2520Cuiyun%2520Gao%2520and%2520Xin%2520Xia%2520and%2520Qing%2520Liao%26entry.1292438233%3D%2520%2520Pre-trained%2520language%2520models%2520%2528PLMs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520code%250Aunderstanding.%2520However%252C%2520deploying%2520these%2520PLMs%2520in%2520large-scale%2520applications%2520faces%250Apractical%2520challenges%2520due%2520to%2520their%2520computational%2520intensity%2520and%2520inference%250Alatency.%2520Knowledge%2520distillation%2520%2528KD%2529%252C%2520a%2520promising%2520model%2520compression%2520and%250Aacceleration%2520technique%252C%2520addresses%2520these%2520limitations%2520by%2520transferring%2520knowledge%250Afrom%2520large%2520teacher%2520models%2520to%2520compact%2520student%2520models%252C%2520enabling%2520efficient%250Ainference%2520while%2520preserving%2520most%2520of%2520the%2520teacher%2520models%2527%2520capabilities.%2520While%2520this%250Atechnique%2520has%2520shown%2520remarkable%2520success%2520in%2520natural%2520language%2520processing%2520and%250Acomputer%2520vision%2520domains%252C%2520its%2520potential%2520for%2520code%2520understanding%2520tasks%2520remains%250Alargely%2520underexplored.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520systematically%2520investigate%2520the%2520effectiveness%2520and%2520usage%2520of%250AKD%2520in%2520code%2520understanding%2520tasks.%2520Our%2520study%2520encompasses%2520two%2520popular%2520types%2520of%2520KD%250Amethods%252C%2520i.e.%252C%2520logit-based%2520and%2520feature-based%2520KD%2520methods%252C%2520experimenting%2520across%250Aeight%2520student%2520models%2520and%2520two%2520teacher%2520PLMs%2520from%2520different%2520domains%2520on%2520three%250Adownstream%2520tasks.%2520The%2520experimental%2520results%2520indicate%2520that%2520KD%2520consistently%2520offers%250Anotable%2520performance%2520boosts%2520across%2520student%2520models%2520with%2520different%2520sizes%2520compared%250Awith%2520standard%2520fine-tuning.%2520Notably%252C%2520code-specific%2520PLM%2520demonstrates%2520better%250Aeffectiveness%2520as%2520the%2520teacher%2520model.%2520Among%2520all%2520KD%2520methods%252C%2520the%2520latest%250Afeature-based%2520KD%2520methods%2520exhibit%2520superior%2520performance%252C%2520enabling%2520student%2520models%250Ato%2520retain%2520up%2520to%252098%2525%2520teacher%2520performance%2520with%2520merely%25205%2525%2520parameters.%2520Regarding%250Astudent%2520architecture%252C%2520our%2520experiments%2520reveal%2520that%2520similarity%2520with%2520teacher%250Aarchitecture%2520does%2520not%2520necessarily%2520lead%2520to%2520better%2520performance.%2520We%2520further%250Adiscuss%2520the%2520efficiency%2520and%2520behaviors%2520in%2520the%2520KD%2520process%2520and%2520inference%252C%2520summarize%250Athe%2520implications%2520of%2520findings%252C%2520and%2520identify%2520promising%2520future%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirical%20Study%20of%20Knowledge%20Distillation%20for%20Code%20Understanding%0A%20%20Tasks&entry.906535625=Ruiqi%20Wang%20and%20Zezhou%20Yang%20and%20Cuiyun%20Gao%20and%20Xin%20Xia%20and%20Qing%20Liao&entry.1292438233=%20%20Pre-trained%20language%20models%20%28PLMs%29%20have%20emerged%20as%20powerful%20tools%20for%20code%0Aunderstanding.%20However%2C%20deploying%20these%20PLMs%20in%20large-scale%20applications%20faces%0Apractical%20challenges%20due%20to%20their%20computational%20intensity%20and%20inference%0Alatency.%20Knowledge%20distillation%20%28KD%29%2C%20a%20promising%20model%20compression%20and%0Aacceleration%20technique%2C%20addresses%20these%20limitations%20by%20transferring%20knowledge%0Afrom%20large%20teacher%20models%20to%20compact%20student%20models%2C%20enabling%20efficient%0Ainference%20while%20preserving%20most%20of%20the%20teacher%20models%27%20capabilities.%20While%20this%0Atechnique%20has%20shown%20remarkable%20success%20in%20natural%20language%20processing%20and%0Acomputer%20vision%20domains%2C%20its%20potential%20for%20code%20understanding%20tasks%20remains%0Alargely%20underexplored.%0A%20%20In%20this%20paper%2C%20we%20systematically%20investigate%20the%20effectiveness%20and%20usage%20of%0AKD%20in%20code%20understanding%20tasks.%20Our%20study%20encompasses%20two%20popular%20types%20of%20KD%0Amethods%2C%20i.e.%2C%20logit-based%20and%20feature-based%20KD%20methods%2C%20experimenting%20across%0Aeight%20student%20models%20and%20two%20teacher%20PLMs%20from%20different%20domains%20on%20three%0Adownstream%20tasks.%20The%20experimental%20results%20indicate%20that%20KD%20consistently%20offers%0Anotable%20performance%20boosts%20across%20student%20models%20with%20different%20sizes%20compared%0Awith%20standard%20fine-tuning.%20Notably%2C%20code-specific%20PLM%20demonstrates%20better%0Aeffectiveness%20as%20the%20teacher%20model.%20Among%20all%20KD%20methods%2C%20the%20latest%0Afeature-based%20KD%20methods%20exhibit%20superior%20performance%2C%20enabling%20student%20models%0Ato%20retain%20up%20to%2098%25%20teacher%20performance%20with%20merely%205%25%20parameters.%20Regarding%0Astudent%20architecture%2C%20our%20experiments%20reveal%20that%20similarity%20with%20teacher%0Aarchitecture%20does%20not%20necessarily%20lead%20to%20better%20performance.%20We%20further%0Adiscuss%20the%20efficiency%20and%20behaviors%20in%20the%20KD%20process%20and%20inference%2C%20summarize%0Athe%20implications%20of%20findings%2C%20and%20identify%20promising%20future%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15423v1&entry.124074799=Read"},
{"title": "Tensorized Multi-Task Learning for Personalized Modeling of\n  Heterogeneous Individuals with High-Dimensional Data", "author": "Elif Konyar and Mostafa Reisi Gahrooei and Kamran Paynabar", "abstract": "  Effective modeling of heterogeneous subpopulations presents a significant\nchallenge due to variations in individual characteristics and behaviors. This\npaper proposes a novel approach to address this issue through multi-task\nlearning (MTL) and low-rank tensor decomposition techniques. Our MTL approach\naims to enhance personalized modeling by leveraging shared structures among\nsimilar tasks while accounting for distinct subpopulation-specific variations.\nWe introduce a framework where low-rank decomposition decomposes the collection\nof task model parameters into a low-rank structure that captures commonalities\nand variations across tasks and subpopulations. This approach allows for\nefficient learning of personalized models by sharing knowledge between similar\ntasks while preserving the unique characteristics of each subpopulation.\nExperimental results in simulation and case study datasets demonstrate the\nsuperior performance of the proposed method compared to several benchmarks,\nparticularly in scenarios with high variability among subpopulations. The\nproposed framework not only improves prediction accuracy but also enhances\ninterpretability by revealing underlying patterns that contribute to the\npersonalization of models.\n", "link": "http://arxiv.org/abs/2508.15676v1", "date": "2025-08-21", "relevancy": 2.6143, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5477}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5131}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tensorized%20Multi-Task%20Learning%20for%20Personalized%20Modeling%20of%0A%20%20Heterogeneous%20Individuals%20with%20High-Dimensional%20Data&body=Title%3A%20Tensorized%20Multi-Task%20Learning%20for%20Personalized%20Modeling%20of%0A%20%20Heterogeneous%20Individuals%20with%20High-Dimensional%20Data%0AAuthor%3A%20Elif%20Konyar%20and%20Mostafa%20Reisi%20Gahrooei%20and%20Kamran%20Paynabar%0AAbstract%3A%20%20%20Effective%20modeling%20of%20heterogeneous%20subpopulations%20presents%20a%20significant%0Achallenge%20due%20to%20variations%20in%20individual%20characteristics%20and%20behaviors.%20This%0Apaper%20proposes%20a%20novel%20approach%20to%20address%20this%20issue%20through%20multi-task%0Alearning%20%28MTL%29%20and%20low-rank%20tensor%20decomposition%20techniques.%20Our%20MTL%20approach%0Aaims%20to%20enhance%20personalized%20modeling%20by%20leveraging%20shared%20structures%20among%0Asimilar%20tasks%20while%20accounting%20for%20distinct%20subpopulation-specific%20variations.%0AWe%20introduce%20a%20framework%20where%20low-rank%20decomposition%20decomposes%20the%20collection%0Aof%20task%20model%20parameters%20into%20a%20low-rank%20structure%20that%20captures%20commonalities%0Aand%20variations%20across%20tasks%20and%20subpopulations.%20This%20approach%20allows%20for%0Aefficient%20learning%20of%20personalized%20models%20by%20sharing%20knowledge%20between%20similar%0Atasks%20while%20preserving%20the%20unique%20characteristics%20of%20each%20subpopulation.%0AExperimental%20results%20in%20simulation%20and%20case%20study%20datasets%20demonstrate%20the%0Asuperior%20performance%20of%20the%20proposed%20method%20compared%20to%20several%20benchmarks%2C%0Aparticularly%20in%20scenarios%20with%20high%20variability%20among%20subpopulations.%20The%0Aproposed%20framework%20not%20only%20improves%20prediction%20accuracy%20but%20also%20enhances%0Ainterpretability%20by%20revealing%20underlying%20patterns%20that%20contribute%20to%20the%0Apersonalization%20of%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTensorized%2520Multi-Task%2520Learning%2520for%2520Personalized%2520Modeling%2520of%250A%2520%2520Heterogeneous%2520Individuals%2520with%2520High-Dimensional%2520Data%26entry.906535625%3DElif%2520Konyar%2520and%2520Mostafa%2520Reisi%2520Gahrooei%2520and%2520Kamran%2520Paynabar%26entry.1292438233%3D%2520%2520Effective%2520modeling%2520of%2520heterogeneous%2520subpopulations%2520presents%2520a%2520significant%250Achallenge%2520due%2520to%2520variations%2520in%2520individual%2520characteristics%2520and%2520behaviors.%2520This%250Apaper%2520proposes%2520a%2520novel%2520approach%2520to%2520address%2520this%2520issue%2520through%2520multi-task%250Alearning%2520%2528MTL%2529%2520and%2520low-rank%2520tensor%2520decomposition%2520techniques.%2520Our%2520MTL%2520approach%250Aaims%2520to%2520enhance%2520personalized%2520modeling%2520by%2520leveraging%2520shared%2520structures%2520among%250Asimilar%2520tasks%2520while%2520accounting%2520for%2520distinct%2520subpopulation-specific%2520variations.%250AWe%2520introduce%2520a%2520framework%2520where%2520low-rank%2520decomposition%2520decomposes%2520the%2520collection%250Aof%2520task%2520model%2520parameters%2520into%2520a%2520low-rank%2520structure%2520that%2520captures%2520commonalities%250Aand%2520variations%2520across%2520tasks%2520and%2520subpopulations.%2520This%2520approach%2520allows%2520for%250Aefficient%2520learning%2520of%2520personalized%2520models%2520by%2520sharing%2520knowledge%2520between%2520similar%250Atasks%2520while%2520preserving%2520the%2520unique%2520characteristics%2520of%2520each%2520subpopulation.%250AExperimental%2520results%2520in%2520simulation%2520and%2520case%2520study%2520datasets%2520demonstrate%2520the%250Asuperior%2520performance%2520of%2520the%2520proposed%2520method%2520compared%2520to%2520several%2520benchmarks%252C%250Aparticularly%2520in%2520scenarios%2520with%2520high%2520variability%2520among%2520subpopulations.%2520The%250Aproposed%2520framework%2520not%2520only%2520improves%2520prediction%2520accuracy%2520but%2520also%2520enhances%250Ainterpretability%2520by%2520revealing%2520underlying%2520patterns%2520that%2520contribute%2520to%2520the%250Apersonalization%2520of%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tensorized%20Multi-Task%20Learning%20for%20Personalized%20Modeling%20of%0A%20%20Heterogeneous%20Individuals%20with%20High-Dimensional%20Data&entry.906535625=Elif%20Konyar%20and%20Mostafa%20Reisi%20Gahrooei%20and%20Kamran%20Paynabar&entry.1292438233=%20%20Effective%20modeling%20of%20heterogeneous%20subpopulations%20presents%20a%20significant%0Achallenge%20due%20to%20variations%20in%20individual%20characteristics%20and%20behaviors.%20This%0Apaper%20proposes%20a%20novel%20approach%20to%20address%20this%20issue%20through%20multi-task%0Alearning%20%28MTL%29%20and%20low-rank%20tensor%20decomposition%20techniques.%20Our%20MTL%20approach%0Aaims%20to%20enhance%20personalized%20modeling%20by%20leveraging%20shared%20structures%20among%0Asimilar%20tasks%20while%20accounting%20for%20distinct%20subpopulation-specific%20variations.%0AWe%20introduce%20a%20framework%20where%20low-rank%20decomposition%20decomposes%20the%20collection%0Aof%20task%20model%20parameters%20into%20a%20low-rank%20structure%20that%20captures%20commonalities%0Aand%20variations%20across%20tasks%20and%20subpopulations.%20This%20approach%20allows%20for%0Aefficient%20learning%20of%20personalized%20models%20by%20sharing%20knowledge%20between%20similar%0Atasks%20while%20preserving%20the%20unique%20characteristics%20of%20each%20subpopulation.%0AExperimental%20results%20in%20simulation%20and%20case%20study%20datasets%20demonstrate%20the%0Asuperior%20performance%20of%20the%20proposed%20method%20compared%20to%20several%20benchmarks%2C%0Aparticularly%20in%20scenarios%20with%20high%20variability%20among%20subpopulations.%20The%0Aproposed%20framework%20not%20only%20improves%20prediction%20accuracy%20but%20also%20enhances%0Ainterpretability%20by%20revealing%20underlying%20patterns%20that%20contribute%20to%20the%0Apersonalization%20of%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15676v1&entry.124074799=Read"},
{"title": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass", "author": "Yanxu Meng and Haoning Wu and Ya Zhang and Weidi Xie", "abstract": "  3D content generation has recently attracted significant research interest\ndue to its applications in VR/AR and embodied AI. In this work, we address the\nchallenging task of synthesizing multiple 3D assets within a single scene\nimage. Concretely, our contributions are fourfold: (i) we present SceneGen, a\nnovel framework that takes a scene image and corresponding object masks as\ninput, simultaneously producing multiple 3D assets with geometry and texture.\nNotably, SceneGen operates with no need for optimization or asset retrieval;\n(ii) we introduce a novel feature aggregation module that integrates local and\nglobal scene information from visual and geometric encoders within the feature\nextraction module. Coupled with a position head, this enables the generation of\n3D assets and their relative spatial positions in a single feedforward pass;\n(iii) we demonstrate SceneGen's direct extensibility to multi-image input\nscenarios. Despite being trained solely on single-image inputs, our\narchitectural design enables improved generation performance with multi-image\ninputs; and (iv) extensive quantitative and qualitative evaluations confirm the\nefficiency and robust generation abilities of our approach. We believe this\nparadigm offers a novel solution for high-quality 3D content generation,\npotentially advancing its practical applications in downstream tasks. The code\nand model will be publicly available at: https://mengmouxu.github.io/SceneGen.\n", "link": "http://arxiv.org/abs/2508.15769v1", "date": "2025-08-21", "relevancy": 2.6112, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6577}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6518}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneGen%3A%20Single-Image%203D%20Scene%20Generation%20in%20One%20Feedforward%20Pass&body=Title%3A%20SceneGen%3A%20Single-Image%203D%20Scene%20Generation%20in%20One%20Feedforward%20Pass%0AAuthor%3A%20Yanxu%20Meng%20and%20Haoning%20Wu%20and%20Ya%20Zhang%20and%20Weidi%20Xie%0AAbstract%3A%20%20%203D%20content%20generation%20has%20recently%20attracted%20significant%20research%20interest%0Adue%20to%20its%20applications%20in%20VR/AR%20and%20embodied%20AI.%20In%20this%20work%2C%20we%20address%20the%0Achallenging%20task%20of%20synthesizing%20multiple%203D%20assets%20within%20a%20single%20scene%0Aimage.%20Concretely%2C%20our%20contributions%20are%20fourfold%3A%20%28i%29%20we%20present%20SceneGen%2C%20a%0Anovel%20framework%20that%20takes%20a%20scene%20image%20and%20corresponding%20object%20masks%20as%0Ainput%2C%20simultaneously%20producing%20multiple%203D%20assets%20with%20geometry%20and%20texture.%0ANotably%2C%20SceneGen%20operates%20with%20no%20need%20for%20optimization%20or%20asset%20retrieval%3B%0A%28ii%29%20we%20introduce%20a%20novel%20feature%20aggregation%20module%20that%20integrates%20local%20and%0Aglobal%20scene%20information%20from%20visual%20and%20geometric%20encoders%20within%20the%20feature%0Aextraction%20module.%20Coupled%20with%20a%20position%20head%2C%20this%20enables%20the%20generation%20of%0A3D%20assets%20and%20their%20relative%20spatial%20positions%20in%20a%20single%20feedforward%20pass%3B%0A%28iii%29%20we%20demonstrate%20SceneGen%27s%20direct%20extensibility%20to%20multi-image%20input%0Ascenarios.%20Despite%20being%20trained%20solely%20on%20single-image%20inputs%2C%20our%0Aarchitectural%20design%20enables%20improved%20generation%20performance%20with%20multi-image%0Ainputs%3B%20and%20%28iv%29%20extensive%20quantitative%20and%20qualitative%20evaluations%20confirm%20the%0Aefficiency%20and%20robust%20generation%20abilities%20of%20our%20approach.%20We%20believe%20this%0Aparadigm%20offers%20a%20novel%20solution%20for%20high-quality%203D%20content%20generation%2C%0Apotentially%20advancing%20its%20practical%20applications%20in%20downstream%20tasks.%20The%20code%0Aand%20model%20will%20be%20publicly%20available%20at%3A%20https%3A//mengmouxu.github.io/SceneGen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneGen%253A%2520Single-Image%25203D%2520Scene%2520Generation%2520in%2520One%2520Feedforward%2520Pass%26entry.906535625%3DYanxu%2520Meng%2520and%2520Haoning%2520Wu%2520and%2520Ya%2520Zhang%2520and%2520Weidi%2520Xie%26entry.1292438233%3D%2520%25203D%2520content%2520generation%2520has%2520recently%2520attracted%2520significant%2520research%2520interest%250Adue%2520to%2520its%2520applications%2520in%2520VR/AR%2520and%2520embodied%2520AI.%2520In%2520this%2520work%252C%2520we%2520address%2520the%250Achallenging%2520task%2520of%2520synthesizing%2520multiple%25203D%2520assets%2520within%2520a%2520single%2520scene%250Aimage.%2520Concretely%252C%2520our%2520contributions%2520are%2520fourfold%253A%2520%2528i%2529%2520we%2520present%2520SceneGen%252C%2520a%250Anovel%2520framework%2520that%2520takes%2520a%2520scene%2520image%2520and%2520corresponding%2520object%2520masks%2520as%250Ainput%252C%2520simultaneously%2520producing%2520multiple%25203D%2520assets%2520with%2520geometry%2520and%2520texture.%250ANotably%252C%2520SceneGen%2520operates%2520with%2520no%2520need%2520for%2520optimization%2520or%2520asset%2520retrieval%253B%250A%2528ii%2529%2520we%2520introduce%2520a%2520novel%2520feature%2520aggregation%2520module%2520that%2520integrates%2520local%2520and%250Aglobal%2520scene%2520information%2520from%2520visual%2520and%2520geometric%2520encoders%2520within%2520the%2520feature%250Aextraction%2520module.%2520Coupled%2520with%2520a%2520position%2520head%252C%2520this%2520enables%2520the%2520generation%2520of%250A3D%2520assets%2520and%2520their%2520relative%2520spatial%2520positions%2520in%2520a%2520single%2520feedforward%2520pass%253B%250A%2528iii%2529%2520we%2520demonstrate%2520SceneGen%2527s%2520direct%2520extensibility%2520to%2520multi-image%2520input%250Ascenarios.%2520Despite%2520being%2520trained%2520solely%2520on%2520single-image%2520inputs%252C%2520our%250Aarchitectural%2520design%2520enables%2520improved%2520generation%2520performance%2520with%2520multi-image%250Ainputs%253B%2520and%2520%2528iv%2529%2520extensive%2520quantitative%2520and%2520qualitative%2520evaluations%2520confirm%2520the%250Aefficiency%2520and%2520robust%2520generation%2520abilities%2520of%2520our%2520approach.%2520We%2520believe%2520this%250Aparadigm%2520offers%2520a%2520novel%2520solution%2520for%2520high-quality%25203D%2520content%2520generation%252C%250Apotentially%2520advancing%2520its%2520practical%2520applications%2520in%2520downstream%2520tasks.%2520The%2520code%250Aand%2520model%2520will%2520be%2520publicly%2520available%2520at%253A%2520https%253A//mengmouxu.github.io/SceneGen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneGen%3A%20Single-Image%203D%20Scene%20Generation%20in%20One%20Feedforward%20Pass&entry.906535625=Yanxu%20Meng%20and%20Haoning%20Wu%20and%20Ya%20Zhang%20and%20Weidi%20Xie&entry.1292438233=%20%203D%20content%20generation%20has%20recently%20attracted%20significant%20research%20interest%0Adue%20to%20its%20applications%20in%20VR/AR%20and%20embodied%20AI.%20In%20this%20work%2C%20we%20address%20the%0Achallenging%20task%20of%20synthesizing%20multiple%203D%20assets%20within%20a%20single%20scene%0Aimage.%20Concretely%2C%20our%20contributions%20are%20fourfold%3A%20%28i%29%20we%20present%20SceneGen%2C%20a%0Anovel%20framework%20that%20takes%20a%20scene%20image%20and%20corresponding%20object%20masks%20as%0Ainput%2C%20simultaneously%20producing%20multiple%203D%20assets%20with%20geometry%20and%20texture.%0ANotably%2C%20SceneGen%20operates%20with%20no%20need%20for%20optimization%20or%20asset%20retrieval%3B%0A%28ii%29%20we%20introduce%20a%20novel%20feature%20aggregation%20module%20that%20integrates%20local%20and%0Aglobal%20scene%20information%20from%20visual%20and%20geometric%20encoders%20within%20the%20feature%0Aextraction%20module.%20Coupled%20with%20a%20position%20head%2C%20this%20enables%20the%20generation%20of%0A3D%20assets%20and%20their%20relative%20spatial%20positions%20in%20a%20single%20feedforward%20pass%3B%0A%28iii%29%20we%20demonstrate%20SceneGen%27s%20direct%20extensibility%20to%20multi-image%20input%0Ascenarios.%20Despite%20being%20trained%20solely%20on%20single-image%20inputs%2C%20our%0Aarchitectural%20design%20enables%20improved%20generation%20performance%20with%20multi-image%0Ainputs%3B%20and%20%28iv%29%20extensive%20quantitative%20and%20qualitative%20evaluations%20confirm%20the%0Aefficiency%20and%20robust%20generation%20abilities%20of%20our%20approach.%20We%20believe%20this%0Aparadigm%20offers%20a%20novel%20solution%20for%20high-quality%203D%20content%20generation%2C%0Apotentially%20advancing%20its%20practical%20applications%20in%20downstream%20tasks.%20The%20code%0Aand%20model%20will%20be%20publicly%20available%20at%3A%20https%3A//mengmouxu.github.io/SceneGen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15769v1&entry.124074799=Read"},
{"title": "Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis", "author": "Yufeng Zhao and Junnan Liu and Hongwei Liu and Dongsheng Zhu and Yuan Shen and Songyang Zhang and Kai Chen", "abstract": "  Large Language Models (LLMs) have made significant strides in reasoning tasks\nthrough methods like chain-of-thought (CoT) reasoning. However, they often fall\nshort in tasks requiring precise computations. Tool-Integrated Reasoning (TIR)\nhas emerged as a solution by incorporating external tools into the reasoning\nprocess. Nevertheless, the generalization of TIR in improving the reasoning\nability of LLM is still unclear. Additionally, whether TIR has improved the\nmodel's reasoning behavior and helped the model think remains to be studied. We\nintroduce ReasonZoo, a comprehensive benchmark encompassing nine diverse\nreasoning categories, to evaluate the effectiveness of TIR across various\ndomains. Additionally, we propose two novel metrics, Performance-Aware Cost\n(PAC) and Area Under the Performance-Cost Curve (AUC-PCC), to assess reasoning\nefficiency. Our empirical evaluation demonstrates that TIR-enabled models\nconsistently outperform their non-TIR counterparts in both mathematical and\nnon-mathematical tasks. Furthermore, TIR enhances reasoning efficiency, as\nevidenced by improved PAC and AUC-PCC, indicating reduced overthinking and more\nstreamlined reasoning. These findings underscore the domain-general benefits of\nTIR and its potential to advance LLM capabilities in complex reasoning tasks.\n", "link": "http://arxiv.org/abs/2508.15754v1", "date": "2025-08-21", "relevancy": 2.5794, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5292}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5292}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dissecting%20Tool-Integrated%20Reasoning%3A%20An%20Empirical%20Study%20and%20Analysis&body=Title%3A%20Dissecting%20Tool-Integrated%20Reasoning%3A%20An%20Empirical%20Study%20and%20Analysis%0AAuthor%3A%20Yufeng%20Zhao%20and%20Junnan%20Liu%20and%20Hongwei%20Liu%20and%20Dongsheng%20Zhu%20and%20Yuan%20Shen%20and%20Songyang%20Zhang%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20strides%20in%20reasoning%20tasks%0Athrough%20methods%20like%20chain-of-thought%20%28CoT%29%20reasoning.%20However%2C%20they%20often%20fall%0Ashort%20in%20tasks%20requiring%20precise%20computations.%20Tool-Integrated%20Reasoning%20%28TIR%29%0Ahas%20emerged%20as%20a%20solution%20by%20incorporating%20external%20tools%20into%20the%20reasoning%0Aprocess.%20Nevertheless%2C%20the%20generalization%20of%20TIR%20in%20improving%20the%20reasoning%0Aability%20of%20LLM%20is%20still%20unclear.%20Additionally%2C%20whether%20TIR%20has%20improved%20the%0Amodel%27s%20reasoning%20behavior%20and%20helped%20the%20model%20think%20remains%20to%20be%20studied.%20We%0Aintroduce%20ReasonZoo%2C%20a%20comprehensive%20benchmark%20encompassing%20nine%20diverse%0Areasoning%20categories%2C%20to%20evaluate%20the%20effectiveness%20of%20TIR%20across%20various%0Adomains.%20Additionally%2C%20we%20propose%20two%20novel%20metrics%2C%20Performance-Aware%20Cost%0A%28PAC%29%20and%20Area%20Under%20the%20Performance-Cost%20Curve%20%28AUC-PCC%29%2C%20to%20assess%20reasoning%0Aefficiency.%20Our%20empirical%20evaluation%20demonstrates%20that%20TIR-enabled%20models%0Aconsistently%20outperform%20their%20non-TIR%20counterparts%20in%20both%20mathematical%20and%0Anon-mathematical%20tasks.%20Furthermore%2C%20TIR%20enhances%20reasoning%20efficiency%2C%20as%0Aevidenced%20by%20improved%20PAC%20and%20AUC-PCC%2C%20indicating%20reduced%20overthinking%20and%20more%0Astreamlined%20reasoning.%20These%20findings%20underscore%20the%20domain-general%20benefits%20of%0ATIR%20and%20its%20potential%20to%20advance%20LLM%20capabilities%20in%20complex%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDissecting%2520Tool-Integrated%2520Reasoning%253A%2520An%2520Empirical%2520Study%2520and%2520Analysis%26entry.906535625%3DYufeng%2520Zhao%2520and%2520Junnan%2520Liu%2520and%2520Hongwei%2520Liu%2520and%2520Dongsheng%2520Zhu%2520and%2520Yuan%2520Shen%2520and%2520Songyang%2520Zhang%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520strides%2520in%2520reasoning%2520tasks%250Athrough%2520methods%2520like%2520chain-of-thought%2520%2528CoT%2529%2520reasoning.%2520However%252C%2520they%2520often%2520fall%250Ashort%2520in%2520tasks%2520requiring%2520precise%2520computations.%2520Tool-Integrated%2520Reasoning%2520%2528TIR%2529%250Ahas%2520emerged%2520as%2520a%2520solution%2520by%2520incorporating%2520external%2520tools%2520into%2520the%2520reasoning%250Aprocess.%2520Nevertheless%252C%2520the%2520generalization%2520of%2520TIR%2520in%2520improving%2520the%2520reasoning%250Aability%2520of%2520LLM%2520is%2520still%2520unclear.%2520Additionally%252C%2520whether%2520TIR%2520has%2520improved%2520the%250Amodel%2527s%2520reasoning%2520behavior%2520and%2520helped%2520the%2520model%2520think%2520remains%2520to%2520be%2520studied.%2520We%250Aintroduce%2520ReasonZoo%252C%2520a%2520comprehensive%2520benchmark%2520encompassing%2520nine%2520diverse%250Areasoning%2520categories%252C%2520to%2520evaluate%2520the%2520effectiveness%2520of%2520TIR%2520across%2520various%250Adomains.%2520Additionally%252C%2520we%2520propose%2520two%2520novel%2520metrics%252C%2520Performance-Aware%2520Cost%250A%2528PAC%2529%2520and%2520Area%2520Under%2520the%2520Performance-Cost%2520Curve%2520%2528AUC-PCC%2529%252C%2520to%2520assess%2520reasoning%250Aefficiency.%2520Our%2520empirical%2520evaluation%2520demonstrates%2520that%2520TIR-enabled%2520models%250Aconsistently%2520outperform%2520their%2520non-TIR%2520counterparts%2520in%2520both%2520mathematical%2520and%250Anon-mathematical%2520tasks.%2520Furthermore%252C%2520TIR%2520enhances%2520reasoning%2520efficiency%252C%2520as%250Aevidenced%2520by%2520improved%2520PAC%2520and%2520AUC-PCC%252C%2520indicating%2520reduced%2520overthinking%2520and%2520more%250Astreamlined%2520reasoning.%2520These%2520findings%2520underscore%2520the%2520domain-general%2520benefits%2520of%250ATIR%2520and%2520its%2520potential%2520to%2520advance%2520LLM%2520capabilities%2520in%2520complex%2520reasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dissecting%20Tool-Integrated%20Reasoning%3A%20An%20Empirical%20Study%20and%20Analysis&entry.906535625=Yufeng%20Zhao%20and%20Junnan%20Liu%20and%20Hongwei%20Liu%20and%20Dongsheng%20Zhu%20and%20Yuan%20Shen%20and%20Songyang%20Zhang%20and%20Kai%20Chen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20strides%20in%20reasoning%20tasks%0Athrough%20methods%20like%20chain-of-thought%20%28CoT%29%20reasoning.%20However%2C%20they%20often%20fall%0Ashort%20in%20tasks%20requiring%20precise%20computations.%20Tool-Integrated%20Reasoning%20%28TIR%29%0Ahas%20emerged%20as%20a%20solution%20by%20incorporating%20external%20tools%20into%20the%20reasoning%0Aprocess.%20Nevertheless%2C%20the%20generalization%20of%20TIR%20in%20improving%20the%20reasoning%0Aability%20of%20LLM%20is%20still%20unclear.%20Additionally%2C%20whether%20TIR%20has%20improved%20the%0Amodel%27s%20reasoning%20behavior%20and%20helped%20the%20model%20think%20remains%20to%20be%20studied.%20We%0Aintroduce%20ReasonZoo%2C%20a%20comprehensive%20benchmark%20encompassing%20nine%20diverse%0Areasoning%20categories%2C%20to%20evaluate%20the%20effectiveness%20of%20TIR%20across%20various%0Adomains.%20Additionally%2C%20we%20propose%20two%20novel%20metrics%2C%20Performance-Aware%20Cost%0A%28PAC%29%20and%20Area%20Under%20the%20Performance-Cost%20Curve%20%28AUC-PCC%29%2C%20to%20assess%20reasoning%0Aefficiency.%20Our%20empirical%20evaluation%20demonstrates%20that%20TIR-enabled%20models%0Aconsistently%20outperform%20their%20non-TIR%20counterparts%20in%20both%20mathematical%20and%0Anon-mathematical%20tasks.%20Furthermore%2C%20TIR%20enhances%20reasoning%20efficiency%2C%20as%0Aevidenced%20by%20improved%20PAC%20and%20AUC-PCC%2C%20indicating%20reduced%20overthinking%20and%20more%0Astreamlined%20reasoning.%20These%20findings%20underscore%20the%20domain-general%20benefits%20of%0ATIR%20and%20its%20potential%20to%20advance%20LLM%20capabilities%20in%20complex%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15754v1&entry.124074799=Read"},
{"title": "Weakly-Supervised Learning for Tree Instances Segmentation in Airborne\n  Lidar Point Clouds", "author": "Swann Emilien C\u00e9leste Destouches and Jesse Lahaye and Laurent Valentin Jospin and Jan Skaloud", "abstract": "  Tree instance segmentation of airborne laser scanning (ALS) data is of utmost\nimportance for forest monitoring, but remains challenging due to variations in\nthe data caused by factors such as sensor resolution, vegetation state at\nacquisition time, terrain characteristics, etc. Moreover, obtaining a\nsufficient amount of precisely labeled data to train fully supervised instance\nsegmentation methods is expensive. To address these challenges, we propose a\nweakly supervised approach where labels of an initial segmentation result\nobtained either by a non-finetuned model or a closed form algorithm are\nprovided as a quality rating by a human operator. The labels produced during\nthe quality assessment are then used to train a rating model, whose task is to\nclassify a segmentation output into the same classes as specified by the human\noperator. Finally, the segmentation model is finetuned using feedback from the\nrating model. This in turn improves the original segmentation model by 34\\% in\nterms of correctly identified tree instances while considerably reducing the\nnumber of non-tree instances predicted. Challenges still remain in data over\nsparsely forested regions characterized by small trees (less than two meters in\nheight) or within complex surroundings containing shrubs, boulders, etc. which\ncan be confused as trees where the performance of the proposed method is\nreduced.\n", "link": "http://arxiv.org/abs/2508.15646v1", "date": "2025-08-21", "relevancy": 2.571, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5275}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5194}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly-Supervised%20Learning%20for%20Tree%20Instances%20Segmentation%20in%20Airborne%0A%20%20Lidar%20Point%20Clouds&body=Title%3A%20Weakly-Supervised%20Learning%20for%20Tree%20Instances%20Segmentation%20in%20Airborne%0A%20%20Lidar%20Point%20Clouds%0AAuthor%3A%20Swann%20Emilien%20C%C3%A9leste%20Destouches%20and%20Jesse%20Lahaye%20and%20Laurent%20Valentin%20Jospin%20and%20Jan%20Skaloud%0AAbstract%3A%20%20%20Tree%20instance%20segmentation%20of%20airborne%20laser%20scanning%20%28ALS%29%20data%20is%20of%20utmost%0Aimportance%20for%20forest%20monitoring%2C%20but%20remains%20challenging%20due%20to%20variations%20in%0Athe%20data%20caused%20by%20factors%20such%20as%20sensor%20resolution%2C%20vegetation%20state%20at%0Aacquisition%20time%2C%20terrain%20characteristics%2C%20etc.%20Moreover%2C%20obtaining%20a%0Asufficient%20amount%20of%20precisely%20labeled%20data%20to%20train%20fully%20supervised%20instance%0Asegmentation%20methods%20is%20expensive.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Aweakly%20supervised%20approach%20where%20labels%20of%20an%20initial%20segmentation%20result%0Aobtained%20either%20by%20a%20non-finetuned%20model%20or%20a%20closed%20form%20algorithm%20are%0Aprovided%20as%20a%20quality%20rating%20by%20a%20human%20operator.%20The%20labels%20produced%20during%0Athe%20quality%20assessment%20are%20then%20used%20to%20train%20a%20rating%20model%2C%20whose%20task%20is%20to%0Aclassify%20a%20segmentation%20output%20into%20the%20same%20classes%20as%20specified%20by%20the%20human%0Aoperator.%20Finally%2C%20the%20segmentation%20model%20is%20finetuned%20using%20feedback%20from%20the%0Arating%20model.%20This%20in%20turn%20improves%20the%20original%20segmentation%20model%20by%2034%5C%25%20in%0Aterms%20of%20correctly%20identified%20tree%20instances%20while%20considerably%20reducing%20the%0Anumber%20of%20non-tree%20instances%20predicted.%20Challenges%20still%20remain%20in%20data%20over%0Asparsely%20forested%20regions%20characterized%20by%20small%20trees%20%28less%20than%20two%20meters%20in%0Aheight%29%20or%20within%20complex%20surroundings%20containing%20shrubs%2C%20boulders%2C%20etc.%20which%0Acan%20be%20confused%20as%20trees%20where%20the%20performance%20of%20the%20proposed%20method%20is%0Areduced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly-Supervised%2520Learning%2520for%2520Tree%2520Instances%2520Segmentation%2520in%2520Airborne%250A%2520%2520Lidar%2520Point%2520Clouds%26entry.906535625%3DSwann%2520Emilien%2520C%25C3%25A9leste%2520Destouches%2520and%2520Jesse%2520Lahaye%2520and%2520Laurent%2520Valentin%2520Jospin%2520and%2520Jan%2520Skaloud%26entry.1292438233%3D%2520%2520Tree%2520instance%2520segmentation%2520of%2520airborne%2520laser%2520scanning%2520%2528ALS%2529%2520data%2520is%2520of%2520utmost%250Aimportance%2520for%2520forest%2520monitoring%252C%2520but%2520remains%2520challenging%2520due%2520to%2520variations%2520in%250Athe%2520data%2520caused%2520by%2520factors%2520such%2520as%2520sensor%2520resolution%252C%2520vegetation%2520state%2520at%250Aacquisition%2520time%252C%2520terrain%2520characteristics%252C%2520etc.%2520Moreover%252C%2520obtaining%2520a%250Asufficient%2520amount%2520of%2520precisely%2520labeled%2520data%2520to%2520train%2520fully%2520supervised%2520instance%250Asegmentation%2520methods%2520is%2520expensive.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250Aweakly%2520supervised%2520approach%2520where%2520labels%2520of%2520an%2520initial%2520segmentation%2520result%250Aobtained%2520either%2520by%2520a%2520non-finetuned%2520model%2520or%2520a%2520closed%2520form%2520algorithm%2520are%250Aprovided%2520as%2520a%2520quality%2520rating%2520by%2520a%2520human%2520operator.%2520The%2520labels%2520produced%2520during%250Athe%2520quality%2520assessment%2520are%2520then%2520used%2520to%2520train%2520a%2520rating%2520model%252C%2520whose%2520task%2520is%2520to%250Aclassify%2520a%2520segmentation%2520output%2520into%2520the%2520same%2520classes%2520as%2520specified%2520by%2520the%2520human%250Aoperator.%2520Finally%252C%2520the%2520segmentation%2520model%2520is%2520finetuned%2520using%2520feedback%2520from%2520the%250Arating%2520model.%2520This%2520in%2520turn%2520improves%2520the%2520original%2520segmentation%2520model%2520by%252034%255C%2525%2520in%250Aterms%2520of%2520correctly%2520identified%2520tree%2520instances%2520while%2520considerably%2520reducing%2520the%250Anumber%2520of%2520non-tree%2520instances%2520predicted.%2520Challenges%2520still%2520remain%2520in%2520data%2520over%250Asparsely%2520forested%2520regions%2520characterized%2520by%2520small%2520trees%2520%2528less%2520than%2520two%2520meters%2520in%250Aheight%2529%2520or%2520within%2520complex%2520surroundings%2520containing%2520shrubs%252C%2520boulders%252C%2520etc.%2520which%250Acan%2520be%2520confused%2520as%2520trees%2520where%2520the%2520performance%2520of%2520the%2520proposed%2520method%2520is%250Areduced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly-Supervised%20Learning%20for%20Tree%20Instances%20Segmentation%20in%20Airborne%0A%20%20Lidar%20Point%20Clouds&entry.906535625=Swann%20Emilien%20C%C3%A9leste%20Destouches%20and%20Jesse%20Lahaye%20and%20Laurent%20Valentin%20Jospin%20and%20Jan%20Skaloud&entry.1292438233=%20%20Tree%20instance%20segmentation%20of%20airborne%20laser%20scanning%20%28ALS%29%20data%20is%20of%20utmost%0Aimportance%20for%20forest%20monitoring%2C%20but%20remains%20challenging%20due%20to%20variations%20in%0Athe%20data%20caused%20by%20factors%20such%20as%20sensor%20resolution%2C%20vegetation%20state%20at%0Aacquisition%20time%2C%20terrain%20characteristics%2C%20etc.%20Moreover%2C%20obtaining%20a%0Asufficient%20amount%20of%20precisely%20labeled%20data%20to%20train%20fully%20supervised%20instance%0Asegmentation%20methods%20is%20expensive.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Aweakly%20supervised%20approach%20where%20labels%20of%20an%20initial%20segmentation%20result%0Aobtained%20either%20by%20a%20non-finetuned%20model%20or%20a%20closed%20form%20algorithm%20are%0Aprovided%20as%20a%20quality%20rating%20by%20a%20human%20operator.%20The%20labels%20produced%20during%0Athe%20quality%20assessment%20are%20then%20used%20to%20train%20a%20rating%20model%2C%20whose%20task%20is%20to%0Aclassify%20a%20segmentation%20output%20into%20the%20same%20classes%20as%20specified%20by%20the%20human%0Aoperator.%20Finally%2C%20the%20segmentation%20model%20is%20finetuned%20using%20feedback%20from%20the%0Arating%20model.%20This%20in%20turn%20improves%20the%20original%20segmentation%20model%20by%2034%5C%25%20in%0Aterms%20of%20correctly%20identified%20tree%20instances%20while%20considerably%20reducing%20the%0Anumber%20of%20non-tree%20instances%20predicted.%20Challenges%20still%20remain%20in%20data%20over%0Asparsely%20forested%20regions%20characterized%20by%20small%20trees%20%28less%20than%20two%20meters%20in%0Aheight%29%20or%20within%20complex%20surroundings%20containing%20shrubs%2C%20boulders%2C%20etc.%20which%0Acan%20be%20confused%20as%20trees%20where%20the%20performance%20of%20the%20proposed%20method%20is%0Areduced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15646v1&entry.124074799=Read"},
{"title": "CineScale: Free Lunch in High-Resolution Cinematic Visual Generation", "author": "Haonan Qiu and Ning Yu and Ziqi Huang and Paul Debevec and Ziwei Liu", "abstract": "  Visual diffusion models achieve remarkable progress, yet they are typically\ntrained at limited resolutions due to the lack of high-resolution data and\nconstrained computation resources, hampering their ability to generate\nhigh-fidelity images or videos at higher resolutions. Recent efforts have\nexplored tuning-free strategies to exhibit the untapped potential\nhigher-resolution visual generation of pre-trained models. However, these\nmethods are still prone to producing low-quality visual content with repetitive\npatterns. The key obstacle lies in the inevitable increase in high-frequency\ninformation when the model generates visual content exceeding its training\nresolution, leading to undesirable repetitive patterns deriving from the\naccumulated errors. In this work, we propose CineScale, a novel inference\nparadigm to enable higher-resolution visual generation. To tackle the various\nissues introduced by the two types of video generation architectures, we\npropose dedicated variants tailored to each. Unlike existing baseline methods\nthat are confined to high-resolution T2I and T2V generation, CineScale broadens\nthe scope by enabling high-resolution I2V and V2V synthesis, built atop\nstate-of-the-art open-source video generation frameworks. Extensive experiments\nvalidate the superiority of our paradigm in extending the capabilities of\nhigher-resolution visual generation for both image and video models.\nRemarkably, our approach enables 8k image generation without any fine-tuning,\nand achieves 4k video generation with only minimal LoRA fine-tuning. Generated\nvideo samples are available at our website:\nhttps://eyeline-labs.github.io/CineScale/.\n", "link": "http://arxiv.org/abs/2508.15774v1", "date": "2025-08-21", "relevancy": 2.5701, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6672}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6378}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CineScale%3A%20Free%20Lunch%20in%20High-Resolution%20Cinematic%20Visual%20Generation&body=Title%3A%20CineScale%3A%20Free%20Lunch%20in%20High-Resolution%20Cinematic%20Visual%20Generation%0AAuthor%3A%20Haonan%20Qiu%20and%20Ning%20Yu%20and%20Ziqi%20Huang%20and%20Paul%20Debevec%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Visual%20diffusion%20models%20achieve%20remarkable%20progress%2C%20yet%20they%20are%20typically%0Atrained%20at%20limited%20resolutions%20due%20to%20the%20lack%20of%20high-resolution%20data%20and%0Aconstrained%20computation%20resources%2C%20hampering%20their%20ability%20to%20generate%0Ahigh-fidelity%20images%20or%20videos%20at%20higher%20resolutions.%20Recent%20efforts%20have%0Aexplored%20tuning-free%20strategies%20to%20exhibit%20the%20untapped%20potential%0Ahigher-resolution%20visual%20generation%20of%20pre-trained%20models.%20However%2C%20these%0Amethods%20are%20still%20prone%20to%20producing%20low-quality%20visual%20content%20with%20repetitive%0Apatterns.%20The%20key%20obstacle%20lies%20in%20the%20inevitable%20increase%20in%20high-frequency%0Ainformation%20when%20the%20model%20generates%20visual%20content%20exceeding%20its%20training%0Aresolution%2C%20leading%20to%20undesirable%20repetitive%20patterns%20deriving%20from%20the%0Aaccumulated%20errors.%20In%20this%20work%2C%20we%20propose%20CineScale%2C%20a%20novel%20inference%0Aparadigm%20to%20enable%20higher-resolution%20visual%20generation.%20To%20tackle%20the%20various%0Aissues%20introduced%20by%20the%20two%20types%20of%20video%20generation%20architectures%2C%20we%0Apropose%20dedicated%20variants%20tailored%20to%20each.%20Unlike%20existing%20baseline%20methods%0Athat%20are%20confined%20to%20high-resolution%20T2I%20and%20T2V%20generation%2C%20CineScale%20broadens%0Athe%20scope%20by%20enabling%20high-resolution%20I2V%20and%20V2V%20synthesis%2C%20built%20atop%0Astate-of-the-art%20open-source%20video%20generation%20frameworks.%20Extensive%20experiments%0Avalidate%20the%20superiority%20of%20our%20paradigm%20in%20extending%20the%20capabilities%20of%0Ahigher-resolution%20visual%20generation%20for%20both%20image%20and%20video%20models.%0ARemarkably%2C%20our%20approach%20enables%208k%20image%20generation%20without%20any%20fine-tuning%2C%0Aand%20achieves%204k%20video%20generation%20with%20only%20minimal%20LoRA%20fine-tuning.%20Generated%0Avideo%20samples%20are%20available%20at%20our%20website%3A%0Ahttps%3A//eyeline-labs.github.io/CineScale/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCineScale%253A%2520Free%2520Lunch%2520in%2520High-Resolution%2520Cinematic%2520Visual%2520Generation%26entry.906535625%3DHaonan%2520Qiu%2520and%2520Ning%2520Yu%2520and%2520Ziqi%2520Huang%2520and%2520Paul%2520Debevec%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Visual%2520diffusion%2520models%2520achieve%2520remarkable%2520progress%252C%2520yet%2520they%2520are%2520typically%250Atrained%2520at%2520limited%2520resolutions%2520due%2520to%2520the%2520lack%2520of%2520high-resolution%2520data%2520and%250Aconstrained%2520computation%2520resources%252C%2520hampering%2520their%2520ability%2520to%2520generate%250Ahigh-fidelity%2520images%2520or%2520videos%2520at%2520higher%2520resolutions.%2520Recent%2520efforts%2520have%250Aexplored%2520tuning-free%2520strategies%2520to%2520exhibit%2520the%2520untapped%2520potential%250Ahigher-resolution%2520visual%2520generation%2520of%2520pre-trained%2520models.%2520However%252C%2520these%250Amethods%2520are%2520still%2520prone%2520to%2520producing%2520low-quality%2520visual%2520content%2520with%2520repetitive%250Apatterns.%2520The%2520key%2520obstacle%2520lies%2520in%2520the%2520inevitable%2520increase%2520in%2520high-frequency%250Ainformation%2520when%2520the%2520model%2520generates%2520visual%2520content%2520exceeding%2520its%2520training%250Aresolution%252C%2520leading%2520to%2520undesirable%2520repetitive%2520patterns%2520deriving%2520from%2520the%250Aaccumulated%2520errors.%2520In%2520this%2520work%252C%2520we%2520propose%2520CineScale%252C%2520a%2520novel%2520inference%250Aparadigm%2520to%2520enable%2520higher-resolution%2520visual%2520generation.%2520To%2520tackle%2520the%2520various%250Aissues%2520introduced%2520by%2520the%2520two%2520types%2520of%2520video%2520generation%2520architectures%252C%2520we%250Apropose%2520dedicated%2520variants%2520tailored%2520to%2520each.%2520Unlike%2520existing%2520baseline%2520methods%250Athat%2520are%2520confined%2520to%2520high-resolution%2520T2I%2520and%2520T2V%2520generation%252C%2520CineScale%2520broadens%250Athe%2520scope%2520by%2520enabling%2520high-resolution%2520I2V%2520and%2520V2V%2520synthesis%252C%2520built%2520atop%250Astate-of-the-art%2520open-source%2520video%2520generation%2520frameworks.%2520Extensive%2520experiments%250Avalidate%2520the%2520superiority%2520of%2520our%2520paradigm%2520in%2520extending%2520the%2520capabilities%2520of%250Ahigher-resolution%2520visual%2520generation%2520for%2520both%2520image%2520and%2520video%2520models.%250ARemarkably%252C%2520our%2520approach%2520enables%25208k%2520image%2520generation%2520without%2520any%2520fine-tuning%252C%250Aand%2520achieves%25204k%2520video%2520generation%2520with%2520only%2520minimal%2520LoRA%2520fine-tuning.%2520Generated%250Avideo%2520samples%2520are%2520available%2520at%2520our%2520website%253A%250Ahttps%253A//eyeline-labs.github.io/CineScale/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CineScale%3A%20Free%20Lunch%20in%20High-Resolution%20Cinematic%20Visual%20Generation&entry.906535625=Haonan%20Qiu%20and%20Ning%20Yu%20and%20Ziqi%20Huang%20and%20Paul%20Debevec%20and%20Ziwei%20Liu&entry.1292438233=%20%20Visual%20diffusion%20models%20achieve%20remarkable%20progress%2C%20yet%20they%20are%20typically%0Atrained%20at%20limited%20resolutions%20due%20to%20the%20lack%20of%20high-resolution%20data%20and%0Aconstrained%20computation%20resources%2C%20hampering%20their%20ability%20to%20generate%0Ahigh-fidelity%20images%20or%20videos%20at%20higher%20resolutions.%20Recent%20efforts%20have%0Aexplored%20tuning-free%20strategies%20to%20exhibit%20the%20untapped%20potential%0Ahigher-resolution%20visual%20generation%20of%20pre-trained%20models.%20However%2C%20these%0Amethods%20are%20still%20prone%20to%20producing%20low-quality%20visual%20content%20with%20repetitive%0Apatterns.%20The%20key%20obstacle%20lies%20in%20the%20inevitable%20increase%20in%20high-frequency%0Ainformation%20when%20the%20model%20generates%20visual%20content%20exceeding%20its%20training%0Aresolution%2C%20leading%20to%20undesirable%20repetitive%20patterns%20deriving%20from%20the%0Aaccumulated%20errors.%20In%20this%20work%2C%20we%20propose%20CineScale%2C%20a%20novel%20inference%0Aparadigm%20to%20enable%20higher-resolution%20visual%20generation.%20To%20tackle%20the%20various%0Aissues%20introduced%20by%20the%20two%20types%20of%20video%20generation%20architectures%2C%20we%0Apropose%20dedicated%20variants%20tailored%20to%20each.%20Unlike%20existing%20baseline%20methods%0Athat%20are%20confined%20to%20high-resolution%20T2I%20and%20T2V%20generation%2C%20CineScale%20broadens%0Athe%20scope%20by%20enabling%20high-resolution%20I2V%20and%20V2V%20synthesis%2C%20built%20atop%0Astate-of-the-art%20open-source%20video%20generation%20frameworks.%20Extensive%20experiments%0Avalidate%20the%20superiority%20of%20our%20paradigm%20in%20extending%20the%20capabilities%20of%0Ahigher-resolution%20visual%20generation%20for%20both%20image%20and%20video%20models.%0ARemarkably%2C%20our%20approach%20enables%208k%20image%20generation%20without%20any%20fine-tuning%2C%0Aand%20achieves%204k%20video%20generation%20with%20only%20minimal%20LoRA%20fine-tuning.%20Generated%0Avideo%20samples%20are%20available%20at%20our%20website%3A%0Ahttps%3A//eyeline-labs.github.io/CineScale/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15774v1&entry.124074799=Read"},
{"title": "Fast globally optimal Truncated Least Squares point cloud registration\n  with fixed rotation axis", "author": "Ivo Ivanov and Carsten Markgraf", "abstract": "  Recent results showed that point cloud registration with given\ncorrespondences can be made robust to outlier rates of up to 95\\% using the\ntruncated least squares (TLS) formulation. However, solving this combinatorial\noptimization problem to global optimality is challenging. Provably globally\noptimal approaches using semidefinite programming (SDP) relaxations take\nhundreds of seconds for 100 points. In this paper, we propose a novel linear\ntime convex relaxation as well as a contractor method to speed up Branch and\nBound (BnB). Our solver can register two 3D point clouds with 100 points to\nprovable global optimality in less than half a second when the axis of rotation\nis provided. Although it currently cannot solve the full 6DoF problem, it is\ntwo orders of magnitude faster than the state-of-the-art SDP solver STRIDE when\nsolving the rotation-only TLS problem. In addition to providing a formal proof\nfor global optimality, we present empirical evidence of global optimality using\nadversarial instances with local minimas close to the global minimum.\n", "link": "http://arxiv.org/abs/2508.15613v1", "date": "2025-08-21", "relevancy": 2.5481, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5334}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5015}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20globally%20optimal%20Truncated%20Least%20Squares%20point%20cloud%20registration%0A%20%20with%20fixed%20rotation%20axis&body=Title%3A%20Fast%20globally%20optimal%20Truncated%20Least%20Squares%20point%20cloud%20registration%0A%20%20with%20fixed%20rotation%20axis%0AAuthor%3A%20Ivo%20Ivanov%20and%20Carsten%20Markgraf%0AAbstract%3A%20%20%20Recent%20results%20showed%20that%20point%20cloud%20registration%20with%20given%0Acorrespondences%20can%20be%20made%20robust%20to%20outlier%20rates%20of%20up%20to%2095%5C%25%20using%20the%0Atruncated%20least%20squares%20%28TLS%29%20formulation.%20However%2C%20solving%20this%20combinatorial%0Aoptimization%20problem%20to%20global%20optimality%20is%20challenging.%20Provably%20globally%0Aoptimal%20approaches%20using%20semidefinite%20programming%20%28SDP%29%20relaxations%20take%0Ahundreds%20of%20seconds%20for%20100%20points.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20linear%0Atime%20convex%20relaxation%20as%20well%20as%20a%20contractor%20method%20to%20speed%20up%20Branch%20and%0ABound%20%28BnB%29.%20Our%20solver%20can%20register%20two%203D%20point%20clouds%20with%20100%20points%20to%0Aprovable%20global%20optimality%20in%20less%20than%20half%20a%20second%20when%20the%20axis%20of%20rotation%0Ais%20provided.%20Although%20it%20currently%20cannot%20solve%20the%20full%206DoF%20problem%2C%20it%20is%0Atwo%20orders%20of%20magnitude%20faster%20than%20the%20state-of-the-art%20SDP%20solver%20STRIDE%20when%0Asolving%20the%20rotation-only%20TLS%20problem.%20In%20addition%20to%20providing%20a%20formal%20proof%0Afor%20global%20optimality%2C%20we%20present%20empirical%20evidence%20of%20global%20optimality%20using%0Aadversarial%20instances%20with%20local%20minimas%20close%20to%20the%20global%20minimum.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520globally%2520optimal%2520Truncated%2520Least%2520Squares%2520point%2520cloud%2520registration%250A%2520%2520with%2520fixed%2520rotation%2520axis%26entry.906535625%3DIvo%2520Ivanov%2520and%2520Carsten%2520Markgraf%26entry.1292438233%3D%2520%2520Recent%2520results%2520showed%2520that%2520point%2520cloud%2520registration%2520with%2520given%250Acorrespondences%2520can%2520be%2520made%2520robust%2520to%2520outlier%2520rates%2520of%2520up%2520to%252095%255C%2525%2520using%2520the%250Atruncated%2520least%2520squares%2520%2528TLS%2529%2520formulation.%2520However%252C%2520solving%2520this%2520combinatorial%250Aoptimization%2520problem%2520to%2520global%2520optimality%2520is%2520challenging.%2520Provably%2520globally%250Aoptimal%2520approaches%2520using%2520semidefinite%2520programming%2520%2528SDP%2529%2520relaxations%2520take%250Ahundreds%2520of%2520seconds%2520for%2520100%2520points.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520linear%250Atime%2520convex%2520relaxation%2520as%2520well%2520as%2520a%2520contractor%2520method%2520to%2520speed%2520up%2520Branch%2520and%250ABound%2520%2528BnB%2529.%2520Our%2520solver%2520can%2520register%2520two%25203D%2520point%2520clouds%2520with%2520100%2520points%2520to%250Aprovable%2520global%2520optimality%2520in%2520less%2520than%2520half%2520a%2520second%2520when%2520the%2520axis%2520of%2520rotation%250Ais%2520provided.%2520Although%2520it%2520currently%2520cannot%2520solve%2520the%2520full%25206DoF%2520problem%252C%2520it%2520is%250Atwo%2520orders%2520of%2520magnitude%2520faster%2520than%2520the%2520state-of-the-art%2520SDP%2520solver%2520STRIDE%2520when%250Asolving%2520the%2520rotation-only%2520TLS%2520problem.%2520In%2520addition%2520to%2520providing%2520a%2520formal%2520proof%250Afor%2520global%2520optimality%252C%2520we%2520present%2520empirical%2520evidence%2520of%2520global%2520optimality%2520using%250Aadversarial%2520instances%2520with%2520local%2520minimas%2520close%2520to%2520the%2520global%2520minimum.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20globally%20optimal%20Truncated%20Least%20Squares%20point%20cloud%20registration%0A%20%20with%20fixed%20rotation%20axis&entry.906535625=Ivo%20Ivanov%20and%20Carsten%20Markgraf&entry.1292438233=%20%20Recent%20results%20showed%20that%20point%20cloud%20registration%20with%20given%0Acorrespondences%20can%20be%20made%20robust%20to%20outlier%20rates%20of%20up%20to%2095%5C%25%20using%20the%0Atruncated%20least%20squares%20%28TLS%29%20formulation.%20However%2C%20solving%20this%20combinatorial%0Aoptimization%20problem%20to%20global%20optimality%20is%20challenging.%20Provably%20globally%0Aoptimal%20approaches%20using%20semidefinite%20programming%20%28SDP%29%20relaxations%20take%0Ahundreds%20of%20seconds%20for%20100%20points.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20linear%0Atime%20convex%20relaxation%20as%20well%20as%20a%20contractor%20method%20to%20speed%20up%20Branch%20and%0ABound%20%28BnB%29.%20Our%20solver%20can%20register%20two%203D%20point%20clouds%20with%20100%20points%20to%0Aprovable%20global%20optimality%20in%20less%20than%20half%20a%20second%20when%20the%20axis%20of%20rotation%0Ais%20provided.%20Although%20it%20currently%20cannot%20solve%20the%20full%206DoF%20problem%2C%20it%20is%0Atwo%20orders%20of%20magnitude%20faster%20than%20the%20state-of-the-art%20SDP%20solver%20STRIDE%20when%0Asolving%20the%20rotation-only%20TLS%20problem.%20In%20addition%20to%20providing%20a%20formal%20proof%0Afor%20global%20optimality%2C%20we%20present%20empirical%20evidence%20of%20global%20optimality%20using%0Aadversarial%20instances%20with%20local%20minimas%20close%20to%20the%20global%20minimum.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15613v1&entry.124074799=Read"},
{"title": "Subjective Behaviors and Preferences in LLM: Language of Browsing", "author": "Sai Sundaresan and Harshita Chopra and Atanu R. Sinha and Koustava Goswami and Nagasai Saketh Naidu and Raghav Karan and N Anushka", "abstract": "  A Large Language Model (LLM) offers versatility across domains and tasks,\npurportedly benefiting users with a wide variety of behaviors and preferences.\nWe question this perception about an LLM when users have inherently subjective\nbehaviors and preferences, as seen in their ubiquitous and idiosyncratic\nbrowsing of websites or apps. The sequential behavior logs of pages, thus\ngenerated, form something akin to each user's self-constructed \"language\",\nalbeit without the structure and grammar imbued in natural languages. We ask:\n(i) Can a small LM represent the \"language of browsing\" better than a large LM?\n(ii) Can an LM with a single set of parameters (or, single LM) adequately\ncapture myriad users' heterogeneous, subjective behaviors and preferences?\n(iii) Can a single LM with high average performance, yield low variance in\nperformance to make alignment good at user level? We introduce clusterwise LM\ntraining, HeTLM (Heterogeneity aware Training of Language Model), appropriate\nfor subjective behaviors. We find that (i) a small LM trained using a\npage-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM\nwith heterogeneous cluster specific set of parameters outperforms a single LM\nof the same family, controlling for the number of parameters; and (iii) a\nhigher mean and a lower variance in generation ensues, implying improved\nalignment.\n", "link": "http://arxiv.org/abs/2508.15474v1", "date": "2025-08-21", "relevancy": 2.5157, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5095}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subjective%20Behaviors%20and%20Preferences%20in%20LLM%3A%20Language%20of%20Browsing&body=Title%3A%20Subjective%20Behaviors%20and%20Preferences%20in%20LLM%3A%20Language%20of%20Browsing%0AAuthor%3A%20Sai%20Sundaresan%20and%20Harshita%20Chopra%20and%20Atanu%20R.%20Sinha%20and%20Koustava%20Goswami%20and%20Nagasai%20Saketh%20Naidu%20and%20Raghav%20Karan%20and%20N%20Anushka%0AAbstract%3A%20%20%20A%20Large%20Language%20Model%20%28LLM%29%20offers%20versatility%20across%20domains%20and%20tasks%2C%0Apurportedly%20benefiting%20users%20with%20a%20wide%20variety%20of%20behaviors%20and%20preferences.%0AWe%20question%20this%20perception%20about%20an%20LLM%20when%20users%20have%20inherently%20subjective%0Abehaviors%20and%20preferences%2C%20as%20seen%20in%20their%20ubiquitous%20and%20idiosyncratic%0Abrowsing%20of%20websites%20or%20apps.%20The%20sequential%20behavior%20logs%20of%20pages%2C%20thus%0Agenerated%2C%20form%20something%20akin%20to%20each%20user%27s%20self-constructed%20%22language%22%2C%0Aalbeit%20without%20the%20structure%20and%20grammar%20imbued%20in%20natural%20languages.%20We%20ask%3A%0A%28i%29%20Can%20a%20small%20LM%20represent%20the%20%22language%20of%20browsing%22%20better%20than%20a%20large%20LM%3F%0A%28ii%29%20Can%20an%20LM%20with%20a%20single%20set%20of%20parameters%20%28or%2C%20single%20LM%29%20adequately%0Acapture%20myriad%20users%27%20heterogeneous%2C%20subjective%20behaviors%20and%20preferences%3F%0A%28iii%29%20Can%20a%20single%20LM%20with%20high%20average%20performance%2C%20yield%20low%20variance%20in%0Aperformance%20to%20make%20alignment%20good%20at%20user%20level%3F%20We%20introduce%20clusterwise%20LM%0Atraining%2C%20HeTLM%20%28Heterogeneity%20aware%20Training%20of%20Language%20Model%29%2C%20appropriate%0Afor%20subjective%20behaviors.%20We%20find%20that%20%28i%29%20a%20small%20LM%20trained%20using%20a%0Apage-level%20tokenizer%20outperforms%20large%20pretrained%20or%20finetuned%20LMs%3B%20%28ii%29%20HeTLM%0Awith%20heterogeneous%20cluster%20specific%20set%20of%20parameters%20outperforms%20a%20single%20LM%0Aof%20the%20same%20family%2C%20controlling%20for%20the%20number%20of%20parameters%3B%20and%20%28iii%29%20a%0Ahigher%20mean%20and%20a%20lower%20variance%20in%20generation%20ensues%2C%20implying%20improved%0Aalignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubjective%2520Behaviors%2520and%2520Preferences%2520in%2520LLM%253A%2520Language%2520of%2520Browsing%26entry.906535625%3DSai%2520Sundaresan%2520and%2520Harshita%2520Chopra%2520and%2520Atanu%2520R.%2520Sinha%2520and%2520Koustava%2520Goswami%2520and%2520Nagasai%2520Saketh%2520Naidu%2520and%2520Raghav%2520Karan%2520and%2520N%2520Anushka%26entry.1292438233%3D%2520%2520A%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520offers%2520versatility%2520across%2520domains%2520and%2520tasks%252C%250Apurportedly%2520benefiting%2520users%2520with%2520a%2520wide%2520variety%2520of%2520behaviors%2520and%2520preferences.%250AWe%2520question%2520this%2520perception%2520about%2520an%2520LLM%2520when%2520users%2520have%2520inherently%2520subjective%250Abehaviors%2520and%2520preferences%252C%2520as%2520seen%2520in%2520their%2520ubiquitous%2520and%2520idiosyncratic%250Abrowsing%2520of%2520websites%2520or%2520apps.%2520The%2520sequential%2520behavior%2520logs%2520of%2520pages%252C%2520thus%250Agenerated%252C%2520form%2520something%2520akin%2520to%2520each%2520user%2527s%2520self-constructed%2520%2522language%2522%252C%250Aalbeit%2520without%2520the%2520structure%2520and%2520grammar%2520imbued%2520in%2520natural%2520languages.%2520We%2520ask%253A%250A%2528i%2529%2520Can%2520a%2520small%2520LM%2520represent%2520the%2520%2522language%2520of%2520browsing%2522%2520better%2520than%2520a%2520large%2520LM%253F%250A%2528ii%2529%2520Can%2520an%2520LM%2520with%2520a%2520single%2520set%2520of%2520parameters%2520%2528or%252C%2520single%2520LM%2529%2520adequately%250Acapture%2520myriad%2520users%2527%2520heterogeneous%252C%2520subjective%2520behaviors%2520and%2520preferences%253F%250A%2528iii%2529%2520Can%2520a%2520single%2520LM%2520with%2520high%2520average%2520performance%252C%2520yield%2520low%2520variance%2520in%250Aperformance%2520to%2520make%2520alignment%2520good%2520at%2520user%2520level%253F%2520We%2520introduce%2520clusterwise%2520LM%250Atraining%252C%2520HeTLM%2520%2528Heterogeneity%2520aware%2520Training%2520of%2520Language%2520Model%2529%252C%2520appropriate%250Afor%2520subjective%2520behaviors.%2520We%2520find%2520that%2520%2528i%2529%2520a%2520small%2520LM%2520trained%2520using%2520a%250Apage-level%2520tokenizer%2520outperforms%2520large%2520pretrained%2520or%2520finetuned%2520LMs%253B%2520%2528ii%2529%2520HeTLM%250Awith%2520heterogeneous%2520cluster%2520specific%2520set%2520of%2520parameters%2520outperforms%2520a%2520single%2520LM%250Aof%2520the%2520same%2520family%252C%2520controlling%2520for%2520the%2520number%2520of%2520parameters%253B%2520and%2520%2528iii%2529%2520a%250Ahigher%2520mean%2520and%2520a%2520lower%2520variance%2520in%2520generation%2520ensues%252C%2520implying%2520improved%250Aalignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subjective%20Behaviors%20and%20Preferences%20in%20LLM%3A%20Language%20of%20Browsing&entry.906535625=Sai%20Sundaresan%20and%20Harshita%20Chopra%20and%20Atanu%20R.%20Sinha%20and%20Koustava%20Goswami%20and%20Nagasai%20Saketh%20Naidu%20and%20Raghav%20Karan%20and%20N%20Anushka&entry.1292438233=%20%20A%20Large%20Language%20Model%20%28LLM%29%20offers%20versatility%20across%20domains%20and%20tasks%2C%0Apurportedly%20benefiting%20users%20with%20a%20wide%20variety%20of%20behaviors%20and%20preferences.%0AWe%20question%20this%20perception%20about%20an%20LLM%20when%20users%20have%20inherently%20subjective%0Abehaviors%20and%20preferences%2C%20as%20seen%20in%20their%20ubiquitous%20and%20idiosyncratic%0Abrowsing%20of%20websites%20or%20apps.%20The%20sequential%20behavior%20logs%20of%20pages%2C%20thus%0Agenerated%2C%20form%20something%20akin%20to%20each%20user%27s%20self-constructed%20%22language%22%2C%0Aalbeit%20without%20the%20structure%20and%20grammar%20imbued%20in%20natural%20languages.%20We%20ask%3A%0A%28i%29%20Can%20a%20small%20LM%20represent%20the%20%22language%20of%20browsing%22%20better%20than%20a%20large%20LM%3F%0A%28ii%29%20Can%20an%20LM%20with%20a%20single%20set%20of%20parameters%20%28or%2C%20single%20LM%29%20adequately%0Acapture%20myriad%20users%27%20heterogeneous%2C%20subjective%20behaviors%20and%20preferences%3F%0A%28iii%29%20Can%20a%20single%20LM%20with%20high%20average%20performance%2C%20yield%20low%20variance%20in%0Aperformance%20to%20make%20alignment%20good%20at%20user%20level%3F%20We%20introduce%20clusterwise%20LM%0Atraining%2C%20HeTLM%20%28Heterogeneity%20aware%20Training%20of%20Language%20Model%29%2C%20appropriate%0Afor%20subjective%20behaviors.%20We%20find%20that%20%28i%29%20a%20small%20LM%20trained%20using%20a%0Apage-level%20tokenizer%20outperforms%20large%20pretrained%20or%20finetuned%20LMs%3B%20%28ii%29%20HeTLM%0Awith%20heterogeneous%20cluster%20specific%20set%20of%20parameters%20outperforms%20a%20single%20LM%0Aof%20the%20same%20family%2C%20controlling%20for%20the%20number%20of%20parameters%3B%20and%20%28iii%29%20a%0Ahigher%20mean%20and%20a%20lower%20variance%20in%20generation%20ensues%2C%20implying%20improved%0Aalignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15474v1&entry.124074799=Read"},
{"title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters", "author": "Shanbo Cheng and Yu Bao and Qian Cao and Luyang Huang and Liyan Kang and Zhicheng Liu and Yu Lu and Wenhao Zhu and Jingwen Chen and Zhichao Huang and Tao Li and Yifu Li and Huiying Lin and Sitong Liu and Ningxin Peng and Shuaijie She and Lu Xu and Nuo Xu and Sen Yang and Runsheng Yu and Yiming Yu and Liehao Zou and Hang Li and Lu Lu and Yuxuan Wang and Yonghui Wu", "abstract": "  Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications.\n", "link": "http://arxiv.org/abs/2507.13618v4", "date": "2025-08-21", "relevancy": 2.5112, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seed-X%3A%20Building%20Strong%20Multilingual%20Translation%20LLM%20with%207B%20Parameters&body=Title%3A%20Seed-X%3A%20Building%20Strong%20Multilingual%20Translation%20LLM%20with%207B%20Parameters%0AAuthor%3A%20Shanbo%20Cheng%20and%20Yu%20Bao%20and%20Qian%20Cao%20and%20Luyang%20Huang%20and%20Liyan%20Kang%20and%20Zhicheng%20Liu%20and%20Yu%20Lu%20and%20Wenhao%20Zhu%20and%20Jingwen%20Chen%20and%20Zhichao%20Huang%20and%20Tao%20Li%20and%20Yifu%20Li%20and%20Huiying%20Lin%20and%20Sitong%20Liu%20and%20Ningxin%20Peng%20and%20Shuaijie%20She%20and%20Lu%20Xu%20and%20Nuo%20Xu%20and%20Sen%20Yang%20and%20Runsheng%20Yu%20and%20Yiming%20Yu%20and%20Liehao%20Zou%20and%20Hang%20Li%20and%20Lu%20Lu%20and%20Yuxuan%20Wang%20and%20Yonghui%20Wu%0AAbstract%3A%20%20%20Multilingual%20translation%20stands%20as%20a%20challenging%20task%20for%20large%20language%0Amodels%20%28LLMs%29%20to%20handle%20intricate%20language%20patterns%20and%20stilted%20translations%0Athat%20arise%20in%20automated%20translations.%20In%20this%20paper%2C%20we%20introduce%20Seed-X%2C%20a%0Afamily%20of%20open-source%20LLMs%20comprising%20instruct%20and%20reasoning%20models%2C%20pushing%0Athe%20limits%20of%20translation%20capability%20with%207B%20parameter%20size.%20The%20base%20model%20is%0Apre-trained%20on%20a%20diverse%2C%20high-quality%20dataset%20encompassing%20both%20monolingual%0Aand%20bilingual%20content%20across%2028%20languages%2C%20harnessing%20the%20full%20potential%20of%0Amultilingual%20data.%20The%20instruct%20model%20is%20then%20finetuned%20to%20translate%20by%0AChain-of-Thought%20%28CoT%29%20reasoning%20and%20further%20enhanced%20through%20reinforcement%0Alearning%20%28RL%29%20to%20achieve%20better%20generalization%20across%20diverse%20language%20pairs.%0ASeed-X%20achieves%20performance%20comparable%20to%20leading%20closed-source%20models%2C%0Aincluding%20Gemini-2.5%20and%20GPT-4o%2C%20across%2028%20languages%2C%20and%20significantly%0Aoutperforms%20larger%20open-source%20models%20in%20both%20automatic%20metrics%20and%20human%0Aevaluations.%20We%20share%20the%20best%20practices%20through%20our%20optimization%20process%2C%20and%0Amake%20the%20parameter%20public%20available%20for%20advancing%20translation%20research%20and%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13618v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeed-X%253A%2520Building%2520Strong%2520Multilingual%2520Translation%2520LLM%2520with%25207B%2520Parameters%26entry.906535625%3DShanbo%2520Cheng%2520and%2520Yu%2520Bao%2520and%2520Qian%2520Cao%2520and%2520Luyang%2520Huang%2520and%2520Liyan%2520Kang%2520and%2520Zhicheng%2520Liu%2520and%2520Yu%2520Lu%2520and%2520Wenhao%2520Zhu%2520and%2520Jingwen%2520Chen%2520and%2520Zhichao%2520Huang%2520and%2520Tao%2520Li%2520and%2520Yifu%2520Li%2520and%2520Huiying%2520Lin%2520and%2520Sitong%2520Liu%2520and%2520Ningxin%2520Peng%2520and%2520Shuaijie%2520She%2520and%2520Lu%2520Xu%2520and%2520Nuo%2520Xu%2520and%2520Sen%2520Yang%2520and%2520Runsheng%2520Yu%2520and%2520Yiming%2520Yu%2520and%2520Liehao%2520Zou%2520and%2520Hang%2520Li%2520and%2520Lu%2520Lu%2520and%2520Yuxuan%2520Wang%2520and%2520Yonghui%2520Wu%26entry.1292438233%3D%2520%2520Multilingual%2520translation%2520stands%2520as%2520a%2520challenging%2520task%2520for%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520to%2520handle%2520intricate%2520language%2520patterns%2520and%2520stilted%2520translations%250Athat%2520arise%2520in%2520automated%2520translations.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Seed-X%252C%2520a%250Afamily%2520of%2520open-source%2520LLMs%2520comprising%2520instruct%2520and%2520reasoning%2520models%252C%2520pushing%250Athe%2520limits%2520of%2520translation%2520capability%2520with%25207B%2520parameter%2520size.%2520The%2520base%2520model%2520is%250Apre-trained%2520on%2520a%2520diverse%252C%2520high-quality%2520dataset%2520encompassing%2520both%2520monolingual%250Aand%2520bilingual%2520content%2520across%252028%2520languages%252C%2520harnessing%2520the%2520full%2520potential%2520of%250Amultilingual%2520data.%2520The%2520instruct%2520model%2520is%2520then%2520finetuned%2520to%2520translate%2520by%250AChain-of-Thought%2520%2528CoT%2529%2520reasoning%2520and%2520further%2520enhanced%2520through%2520reinforcement%250Alearning%2520%2528RL%2529%2520to%2520achieve%2520better%2520generalization%2520across%2520diverse%2520language%2520pairs.%250ASeed-X%2520achieves%2520performance%2520comparable%2520to%2520leading%2520closed-source%2520models%252C%250Aincluding%2520Gemini-2.5%2520and%2520GPT-4o%252C%2520across%252028%2520languages%252C%2520and%2520significantly%250Aoutperforms%2520larger%2520open-source%2520models%2520in%2520both%2520automatic%2520metrics%2520and%2520human%250Aevaluations.%2520We%2520share%2520the%2520best%2520practices%2520through%2520our%2520optimization%2520process%252C%2520and%250Amake%2520the%2520parameter%2520public%2520available%2520for%2520advancing%2520translation%2520research%2520and%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13618v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seed-X%3A%20Building%20Strong%20Multilingual%20Translation%20LLM%20with%207B%20Parameters&entry.906535625=Shanbo%20Cheng%20and%20Yu%20Bao%20and%20Qian%20Cao%20and%20Luyang%20Huang%20and%20Liyan%20Kang%20and%20Zhicheng%20Liu%20and%20Yu%20Lu%20and%20Wenhao%20Zhu%20and%20Jingwen%20Chen%20and%20Zhichao%20Huang%20and%20Tao%20Li%20and%20Yifu%20Li%20and%20Huiying%20Lin%20and%20Sitong%20Liu%20and%20Ningxin%20Peng%20and%20Shuaijie%20She%20and%20Lu%20Xu%20and%20Nuo%20Xu%20and%20Sen%20Yang%20and%20Runsheng%20Yu%20and%20Yiming%20Yu%20and%20Liehao%20Zou%20and%20Hang%20Li%20and%20Lu%20Lu%20and%20Yuxuan%20Wang%20and%20Yonghui%20Wu&entry.1292438233=%20%20Multilingual%20translation%20stands%20as%20a%20challenging%20task%20for%20large%20language%0Amodels%20%28LLMs%29%20to%20handle%20intricate%20language%20patterns%20and%20stilted%20translations%0Athat%20arise%20in%20automated%20translations.%20In%20this%20paper%2C%20we%20introduce%20Seed-X%2C%20a%0Afamily%20of%20open-source%20LLMs%20comprising%20instruct%20and%20reasoning%20models%2C%20pushing%0Athe%20limits%20of%20translation%20capability%20with%207B%20parameter%20size.%20The%20base%20model%20is%0Apre-trained%20on%20a%20diverse%2C%20high-quality%20dataset%20encompassing%20both%20monolingual%0Aand%20bilingual%20content%20across%2028%20languages%2C%20harnessing%20the%20full%20potential%20of%0Amultilingual%20data.%20The%20instruct%20model%20is%20then%20finetuned%20to%20translate%20by%0AChain-of-Thought%20%28CoT%29%20reasoning%20and%20further%20enhanced%20through%20reinforcement%0Alearning%20%28RL%29%20to%20achieve%20better%20generalization%20across%20diverse%20language%20pairs.%0ASeed-X%20achieves%20performance%20comparable%20to%20leading%20closed-source%20models%2C%0Aincluding%20Gemini-2.5%20and%20GPT-4o%2C%20across%2028%20languages%2C%20and%20significantly%0Aoutperforms%20larger%20open-source%20models%20in%20both%20automatic%20metrics%20and%20human%0Aevaluations.%20We%20share%20the%20best%20practices%20through%20our%20optimization%20process%2C%20and%0Amake%20the%20parameter%20public%20available%20for%20advancing%20translation%20research%20and%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13618v4&entry.124074799=Read"},
{"title": "Fast-DDPM: Fast Denoising Diffusion Probabilistic Models for Medical\n  Image-to-Image Generation", "author": "Hongxu Jiang and Muhammad Imran and Teng Zhang and Yuyin Zhou and Muxuan Liang and Kuang Gong and Wei Shao", "abstract": "  Denoising diffusion probabilistic models (DDPMs) have achieved unprecedented\nsuccess in computer vision. However, they remain underutilized in medical\nimaging, a field crucial for disease diagnosis and treatment planning. This is\nprimarily due to the high computational cost associated with (1) the use of\nlarge number of time steps (e.g., 1,000) in diffusion processes and (2) the\nincreased dimensionality of medical images, which are often 3D or 4D. Training\na diffusion model on medical images typically takes days to weeks, while\nsampling each image volume takes minutes to hours. To address this challenge,\nwe introduce Fast-DDPM, a simple yet effective approach capable of improving\ntraining speed, sampling speed, and generation quality simultaneously. Unlike\nDDPM, which trains the image denoiser across 1,000 time steps, Fast-DDPM trains\nand samples using only 10 time steps. The key to our method lies in aligning\nthe training and sampling procedures to optimize time-step utilization.\nSpecifically, we introduced two efficient noise schedulers with 10 time steps:\none with uniform time step sampling and another with non-uniform sampling. We\nevaluated Fast-DDPM across three medical image-to-image generation tasks:\nmulti-image super-resolution, image denoising, and image-to-image translation.\nFast-DDPM outperformed DDPM and current state-of-the-art methods based on\nconvolutional networks and generative adversarial networks in all tasks.\nAdditionally, Fast-DDPM reduced the training time to 0.2x and the sampling time\nto 0.01x compared to DDPM. Our code is publicly available at:\nhttps://github.com/mirthAI/Fast-DDPM.\n", "link": "http://arxiv.org/abs/2405.14802v3", "date": "2025-08-21", "relevancy": 2.5101, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.683}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6322}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast-DDPM%3A%20Fast%20Denoising%20Diffusion%20Probabilistic%20Models%20for%20Medical%0A%20%20Image-to-Image%20Generation&body=Title%3A%20Fast-DDPM%3A%20Fast%20Denoising%20Diffusion%20Probabilistic%20Models%20for%20Medical%0A%20%20Image-to-Image%20Generation%0AAuthor%3A%20Hongxu%20Jiang%20and%20Muhammad%20Imran%20and%20Teng%20Zhang%20and%20Yuyin%20Zhou%20and%20Muxuan%20Liang%20and%20Kuang%20Gong%20and%20Wei%20Shao%0AAbstract%3A%20%20%20Denoising%20diffusion%20probabilistic%20models%20%28DDPMs%29%20have%20achieved%20unprecedented%0Asuccess%20in%20computer%20vision.%20However%2C%20they%20remain%20underutilized%20in%20medical%0Aimaging%2C%20a%20field%20crucial%20for%20disease%20diagnosis%20and%20treatment%20planning.%20This%20is%0Aprimarily%20due%20to%20the%20high%20computational%20cost%20associated%20with%20%281%29%20the%20use%20of%0Alarge%20number%20of%20time%20steps%20%28e.g.%2C%201%2C000%29%20in%20diffusion%20processes%20and%20%282%29%20the%0Aincreased%20dimensionality%20of%20medical%20images%2C%20which%20are%20often%203D%20or%204D.%20Training%0Aa%20diffusion%20model%20on%20medical%20images%20typically%20takes%20days%20to%20weeks%2C%20while%0Asampling%20each%20image%20volume%20takes%20minutes%20to%20hours.%20To%20address%20this%20challenge%2C%0Awe%20introduce%20Fast-DDPM%2C%20a%20simple%20yet%20effective%20approach%20capable%20of%20improving%0Atraining%20speed%2C%20sampling%20speed%2C%20and%20generation%20quality%20simultaneously.%20Unlike%0ADDPM%2C%20which%20trains%20the%20image%20denoiser%20across%201%2C000%20time%20steps%2C%20Fast-DDPM%20trains%0Aand%20samples%20using%20only%2010%20time%20steps.%20The%20key%20to%20our%20method%20lies%20in%20aligning%0Athe%20training%20and%20sampling%20procedures%20to%20optimize%20time-step%20utilization.%0ASpecifically%2C%20we%20introduced%20two%20efficient%20noise%20schedulers%20with%2010%20time%20steps%3A%0Aone%20with%20uniform%20time%20step%20sampling%20and%20another%20with%20non-uniform%20sampling.%20We%0Aevaluated%20Fast-DDPM%20across%20three%20medical%20image-to-image%20generation%20tasks%3A%0Amulti-image%20super-resolution%2C%20image%20denoising%2C%20and%20image-to-image%20translation.%0AFast-DDPM%20outperformed%20DDPM%20and%20current%20state-of-the-art%20methods%20based%20on%0Aconvolutional%20networks%20and%20generative%20adversarial%20networks%20in%20all%20tasks.%0AAdditionally%2C%20Fast-DDPM%20reduced%20the%20training%20time%20to%200.2x%20and%20the%20sampling%20time%0Ato%200.01x%20compared%20to%20DDPM.%20Our%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/mirthAI/Fast-DDPM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14802v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast-DDPM%253A%2520Fast%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520for%2520Medical%250A%2520%2520Image-to-Image%2520Generation%26entry.906535625%3DHongxu%2520Jiang%2520and%2520Muhammad%2520Imran%2520and%2520Teng%2520Zhang%2520and%2520Yuyin%2520Zhou%2520and%2520Muxuan%2520Liang%2520and%2520Kuang%2520Gong%2520and%2520Wei%2520Shao%26entry.1292438233%3D%2520%2520Denoising%2520diffusion%2520probabilistic%2520models%2520%2528DDPMs%2529%2520have%2520achieved%2520unprecedented%250Asuccess%2520in%2520computer%2520vision.%2520However%252C%2520they%2520remain%2520underutilized%2520in%2520medical%250Aimaging%252C%2520a%2520field%2520crucial%2520for%2520disease%2520diagnosis%2520and%2520treatment%2520planning.%2520This%2520is%250Aprimarily%2520due%2520to%2520the%2520high%2520computational%2520cost%2520associated%2520with%2520%25281%2529%2520the%2520use%2520of%250Alarge%2520number%2520of%2520time%2520steps%2520%2528e.g.%252C%25201%252C000%2529%2520in%2520diffusion%2520processes%2520and%2520%25282%2529%2520the%250Aincreased%2520dimensionality%2520of%2520medical%2520images%252C%2520which%2520are%2520often%25203D%2520or%25204D.%2520Training%250Aa%2520diffusion%2520model%2520on%2520medical%2520images%2520typically%2520takes%2520days%2520to%2520weeks%252C%2520while%250Asampling%2520each%2520image%2520volume%2520takes%2520minutes%2520to%2520hours.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520introduce%2520Fast-DDPM%252C%2520a%2520simple%2520yet%2520effective%2520approach%2520capable%2520of%2520improving%250Atraining%2520speed%252C%2520sampling%2520speed%252C%2520and%2520generation%2520quality%2520simultaneously.%2520Unlike%250ADDPM%252C%2520which%2520trains%2520the%2520image%2520denoiser%2520across%25201%252C000%2520time%2520steps%252C%2520Fast-DDPM%2520trains%250Aand%2520samples%2520using%2520only%252010%2520time%2520steps.%2520The%2520key%2520to%2520our%2520method%2520lies%2520in%2520aligning%250Athe%2520training%2520and%2520sampling%2520procedures%2520to%2520optimize%2520time-step%2520utilization.%250ASpecifically%252C%2520we%2520introduced%2520two%2520efficient%2520noise%2520schedulers%2520with%252010%2520time%2520steps%253A%250Aone%2520with%2520uniform%2520time%2520step%2520sampling%2520and%2520another%2520with%2520non-uniform%2520sampling.%2520We%250Aevaluated%2520Fast-DDPM%2520across%2520three%2520medical%2520image-to-image%2520generation%2520tasks%253A%250Amulti-image%2520super-resolution%252C%2520image%2520denoising%252C%2520and%2520image-to-image%2520translation.%250AFast-DDPM%2520outperformed%2520DDPM%2520and%2520current%2520state-of-the-art%2520methods%2520based%2520on%250Aconvolutional%2520networks%2520and%2520generative%2520adversarial%2520networks%2520in%2520all%2520tasks.%250AAdditionally%252C%2520Fast-DDPM%2520reduced%2520the%2520training%2520time%2520to%25200.2x%2520and%2520the%2520sampling%2520time%250Ato%25200.01x%2520compared%2520to%2520DDPM.%2520Our%2520code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/mirthAI/Fast-DDPM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14802v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast-DDPM%3A%20Fast%20Denoising%20Diffusion%20Probabilistic%20Models%20for%20Medical%0A%20%20Image-to-Image%20Generation&entry.906535625=Hongxu%20Jiang%20and%20Muhammad%20Imran%20and%20Teng%20Zhang%20and%20Yuyin%20Zhou%20and%20Muxuan%20Liang%20and%20Kuang%20Gong%20and%20Wei%20Shao&entry.1292438233=%20%20Denoising%20diffusion%20probabilistic%20models%20%28DDPMs%29%20have%20achieved%20unprecedented%0Asuccess%20in%20computer%20vision.%20However%2C%20they%20remain%20underutilized%20in%20medical%0Aimaging%2C%20a%20field%20crucial%20for%20disease%20diagnosis%20and%20treatment%20planning.%20This%20is%0Aprimarily%20due%20to%20the%20high%20computational%20cost%20associated%20with%20%281%29%20the%20use%20of%0Alarge%20number%20of%20time%20steps%20%28e.g.%2C%201%2C000%29%20in%20diffusion%20processes%20and%20%282%29%20the%0Aincreased%20dimensionality%20of%20medical%20images%2C%20which%20are%20often%203D%20or%204D.%20Training%0Aa%20diffusion%20model%20on%20medical%20images%20typically%20takes%20days%20to%20weeks%2C%20while%0Asampling%20each%20image%20volume%20takes%20minutes%20to%20hours.%20To%20address%20this%20challenge%2C%0Awe%20introduce%20Fast-DDPM%2C%20a%20simple%20yet%20effective%20approach%20capable%20of%20improving%0Atraining%20speed%2C%20sampling%20speed%2C%20and%20generation%20quality%20simultaneously.%20Unlike%0ADDPM%2C%20which%20trains%20the%20image%20denoiser%20across%201%2C000%20time%20steps%2C%20Fast-DDPM%20trains%0Aand%20samples%20using%20only%2010%20time%20steps.%20The%20key%20to%20our%20method%20lies%20in%20aligning%0Athe%20training%20and%20sampling%20procedures%20to%20optimize%20time-step%20utilization.%0ASpecifically%2C%20we%20introduced%20two%20efficient%20noise%20schedulers%20with%2010%20time%20steps%3A%0Aone%20with%20uniform%20time%20step%20sampling%20and%20another%20with%20non-uniform%20sampling.%20We%0Aevaluated%20Fast-DDPM%20across%20three%20medical%20image-to-image%20generation%20tasks%3A%0Amulti-image%20super-resolution%2C%20image%20denoising%2C%20and%20image-to-image%20translation.%0AFast-DDPM%20outperformed%20DDPM%20and%20current%20state-of-the-art%20methods%20based%20on%0Aconvolutional%20networks%20and%20generative%20adversarial%20networks%20in%20all%20tasks.%0AAdditionally%2C%20Fast-DDPM%20reduced%20the%20training%20time%20to%200.2x%20and%20the%20sampling%20time%0Ato%200.01x%20compared%20to%20DDPM.%20Our%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/mirthAI/Fast-DDPM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14802v3&entry.124074799=Read"},
{"title": "LATTE: Learning Aligned Transactions and Textual Embeddings for Bank\n  Clients", "author": "Egor Fadeev and Dzhambulat Mollaev and Aleksei Shestov and Dima Korolev and Omar Zoloev and Ivan Kireev and Andrey Savchenko and Maksim Makarenko", "abstract": "  Learning clients embeddings from sequences of their historic communications\nis central to financial applications. While large language models (LLMs) offer\ngeneral world knowledge, their direct use on long event sequences is\ncomputationally expensive and impractical in real-world pipelines. In this\npaper, we propose LATTE, a contrastive learning framework that aligns raw event\nembeddings with semantic embeddings from frozen LLMs. Behavioral features are\nsummarized into short prompts, embedded by the LLM, and used as supervision via\ncontrastive loss. The proposed approach significantly reduces inference cost\nand input size compared to conventional processing of complete sequence by LLM.\nWe experimentally show that our method outperforms state-of-the-art techniques\nfor learning event sequence representations on real-world financial datasets\nwhile remaining deployable in latency-sensitive environments.\n", "link": "http://arxiv.org/abs/2508.10021v2", "date": "2025-08-21", "relevancy": 2.5065, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5332}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4862}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LATTE%3A%20Learning%20Aligned%20Transactions%20and%20Textual%20Embeddings%20for%20Bank%0A%20%20Clients&body=Title%3A%20LATTE%3A%20Learning%20Aligned%20Transactions%20and%20Textual%20Embeddings%20for%20Bank%0A%20%20Clients%0AAuthor%3A%20Egor%20Fadeev%20and%20Dzhambulat%20Mollaev%20and%20Aleksei%20Shestov%20and%20Dima%20Korolev%20and%20Omar%20Zoloev%20and%20Ivan%20Kireev%20and%20Andrey%20Savchenko%20and%20Maksim%20Makarenko%0AAbstract%3A%20%20%20Learning%20clients%20embeddings%20from%20sequences%20of%20their%20historic%20communications%0Ais%20central%20to%20financial%20applications.%20While%20large%20language%20models%20%28LLMs%29%20offer%0Ageneral%20world%20knowledge%2C%20their%20direct%20use%20on%20long%20event%20sequences%20is%0Acomputationally%20expensive%20and%20impractical%20in%20real-world%20pipelines.%20In%20this%0Apaper%2C%20we%20propose%20LATTE%2C%20a%20contrastive%20learning%20framework%20that%20aligns%20raw%20event%0Aembeddings%20with%20semantic%20embeddings%20from%20frozen%20LLMs.%20Behavioral%20features%20are%0Asummarized%20into%20short%20prompts%2C%20embedded%20by%20the%20LLM%2C%20and%20used%20as%20supervision%20via%0Acontrastive%20loss.%20The%20proposed%20approach%20significantly%20reduces%20inference%20cost%0Aand%20input%20size%20compared%20to%20conventional%20processing%20of%20complete%20sequence%20by%20LLM.%0AWe%20experimentally%20show%20that%20our%20method%20outperforms%20state-of-the-art%20techniques%0Afor%20learning%20event%20sequence%20representations%20on%20real-world%20financial%20datasets%0Awhile%20remaining%20deployable%20in%20latency-sensitive%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10021v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLATTE%253A%2520Learning%2520Aligned%2520Transactions%2520and%2520Textual%2520Embeddings%2520for%2520Bank%250A%2520%2520Clients%26entry.906535625%3DEgor%2520Fadeev%2520and%2520Dzhambulat%2520Mollaev%2520and%2520Aleksei%2520Shestov%2520and%2520Dima%2520Korolev%2520and%2520Omar%2520Zoloev%2520and%2520Ivan%2520Kireev%2520and%2520Andrey%2520Savchenko%2520and%2520Maksim%2520Makarenko%26entry.1292438233%3D%2520%2520Learning%2520clients%2520embeddings%2520from%2520sequences%2520of%2520their%2520historic%2520communications%250Ais%2520central%2520to%2520financial%2520applications.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520offer%250Ageneral%2520world%2520knowledge%252C%2520their%2520direct%2520use%2520on%2520long%2520event%2520sequences%2520is%250Acomputationally%2520expensive%2520and%2520impractical%2520in%2520real-world%2520pipelines.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520LATTE%252C%2520a%2520contrastive%2520learning%2520framework%2520that%2520aligns%2520raw%2520event%250Aembeddings%2520with%2520semantic%2520embeddings%2520from%2520frozen%2520LLMs.%2520Behavioral%2520features%2520are%250Asummarized%2520into%2520short%2520prompts%252C%2520embedded%2520by%2520the%2520LLM%252C%2520and%2520used%2520as%2520supervision%2520via%250Acontrastive%2520loss.%2520The%2520proposed%2520approach%2520significantly%2520reduces%2520inference%2520cost%250Aand%2520input%2520size%2520compared%2520to%2520conventional%2520processing%2520of%2520complete%2520sequence%2520by%2520LLM.%250AWe%2520experimentally%2520show%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520techniques%250Afor%2520learning%2520event%2520sequence%2520representations%2520on%2520real-world%2520financial%2520datasets%250Awhile%2520remaining%2520deployable%2520in%2520latency-sensitive%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10021v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LATTE%3A%20Learning%20Aligned%20Transactions%20and%20Textual%20Embeddings%20for%20Bank%0A%20%20Clients&entry.906535625=Egor%20Fadeev%20and%20Dzhambulat%20Mollaev%20and%20Aleksei%20Shestov%20and%20Dima%20Korolev%20and%20Omar%20Zoloev%20and%20Ivan%20Kireev%20and%20Andrey%20Savchenko%20and%20Maksim%20Makarenko&entry.1292438233=%20%20Learning%20clients%20embeddings%20from%20sequences%20of%20their%20historic%20communications%0Ais%20central%20to%20financial%20applications.%20While%20large%20language%20models%20%28LLMs%29%20offer%0Ageneral%20world%20knowledge%2C%20their%20direct%20use%20on%20long%20event%20sequences%20is%0Acomputationally%20expensive%20and%20impractical%20in%20real-world%20pipelines.%20In%20this%0Apaper%2C%20we%20propose%20LATTE%2C%20a%20contrastive%20learning%20framework%20that%20aligns%20raw%20event%0Aembeddings%20with%20semantic%20embeddings%20from%20frozen%20LLMs.%20Behavioral%20features%20are%0Asummarized%20into%20short%20prompts%2C%20embedded%20by%20the%20LLM%2C%20and%20used%20as%20supervision%20via%0Acontrastive%20loss.%20The%20proposed%20approach%20significantly%20reduces%20inference%20cost%0Aand%20input%20size%20compared%20to%20conventional%20processing%20of%20complete%20sequence%20by%20LLM.%0AWe%20experimentally%20show%20that%20our%20method%20outperforms%20state-of-the-art%20techniques%0Afor%20learning%20event%20sequence%20representations%20on%20real-world%20financial%20datasets%0Awhile%20remaining%20deployable%20in%20latency-sensitive%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10021v2&entry.124074799=Read"},
{"title": "DoSReMC: Domain Shift Resilient Mammography Classification using Batch\n  Normalization Adaptation", "author": "U\u011furcan Aky\u00fcz and Deniz Katircioglu-\u00d6zt\u00fcrk and Emre K. S\u00fcsl\u00fc and Burhan Kele\u015f and Mete C. Kaya and Gamze Durhan and Meltem G. Akp\u0131nar and Figen B. Demirkaz\u0131k and G\u00f6zde B. Akar", "abstract": "  Numerous deep learning-based solutions have been developed for the automatic\nrecognition of breast cancer using mammography images. However, their\nperformance often declines when applied to data from different domains,\nprimarily due to domain shift -- the variation in data distributions between\nsource and target domains. This performance drop limits the safe and equitable\ndeployment of AI in real-world clinical settings. In this study, we present\nDoSReMC (Domain Shift Resilient Mammography Classification), a batch\nnormalization (BN) adaptation framework designed to enhance cross-domain\ngeneralization without retraining the entire model. Using three large-scale\nfull-field digital mammography (FFDM) datasets -- including HCTP, a newly\nintroduced, pathologically confirmed in-house dataset -- we conduct a\nsystematic cross-domain evaluation with convolutional neural networks (CNNs).\nOur results demonstrate that BN layers are a primary source of domain\ndependence: they perform effectively when training and testing occur within the\nsame domain, and they significantly impair model generalization under domain\nshift. DoSReMC addresses this limitation by fine-tuning only the BN and fully\nconnected (FC) layers, while preserving pretrained convolutional filters. We\nfurther integrate this targeted adaptation with an adversarial training scheme,\nyielding additional improvements in cross-domain generalizability. DoSReMC can\nbe readily incorporated into existing AI pipelines and applied across diverse\nclinical environments, providing a practical pathway toward more robust and\ngeneralizable mammography classification systems.\n", "link": "http://arxiv.org/abs/2508.15452v1", "date": "2025-08-21", "relevancy": 2.4597, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5057}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4857}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DoSReMC%3A%20Domain%20Shift%20Resilient%20Mammography%20Classification%20using%20Batch%0A%20%20Normalization%20Adaptation&body=Title%3A%20DoSReMC%3A%20Domain%20Shift%20Resilient%20Mammography%20Classification%20using%20Batch%0A%20%20Normalization%20Adaptation%0AAuthor%3A%20U%C4%9Furcan%20Aky%C3%BCz%20and%20Deniz%20Katircioglu-%C3%96zt%C3%BCrk%20and%20Emre%20K.%20S%C3%BCsl%C3%BC%20and%20Burhan%20Kele%C5%9F%20and%20Mete%20C.%20Kaya%20and%20Gamze%20Durhan%20and%20Meltem%20G.%20Akp%C4%B1nar%20and%20Figen%20B.%20Demirkaz%C4%B1k%20and%20G%C3%B6zde%20B.%20Akar%0AAbstract%3A%20%20%20Numerous%20deep%20learning-based%20solutions%20have%20been%20developed%20for%20the%20automatic%0Arecognition%20of%20breast%20cancer%20using%20mammography%20images.%20However%2C%20their%0Aperformance%20often%20declines%20when%20applied%20to%20data%20from%20different%20domains%2C%0Aprimarily%20due%20to%20domain%20shift%20--%20the%20variation%20in%20data%20distributions%20between%0Asource%20and%20target%20domains.%20This%20performance%20drop%20limits%20the%20safe%20and%20equitable%0Adeployment%20of%20AI%20in%20real-world%20clinical%20settings.%20In%20this%20study%2C%20we%20present%0ADoSReMC%20%28Domain%20Shift%20Resilient%20Mammography%20Classification%29%2C%20a%20batch%0Anormalization%20%28BN%29%20adaptation%20framework%20designed%20to%20enhance%20cross-domain%0Ageneralization%20without%20retraining%20the%20entire%20model.%20Using%20three%20large-scale%0Afull-field%20digital%20mammography%20%28FFDM%29%20datasets%20--%20including%20HCTP%2C%20a%20newly%0Aintroduced%2C%20pathologically%20confirmed%20in-house%20dataset%20--%20we%20conduct%20a%0Asystematic%20cross-domain%20evaluation%20with%20convolutional%20neural%20networks%20%28CNNs%29.%0AOur%20results%20demonstrate%20that%20BN%20layers%20are%20a%20primary%20source%20of%20domain%0Adependence%3A%20they%20perform%20effectively%20when%20training%20and%20testing%20occur%20within%20the%0Asame%20domain%2C%20and%20they%20significantly%20impair%20model%20generalization%20under%20domain%0Ashift.%20DoSReMC%20addresses%20this%20limitation%20by%20fine-tuning%20only%20the%20BN%20and%20fully%0Aconnected%20%28FC%29%20layers%2C%20while%20preserving%20pretrained%20convolutional%20filters.%20We%0Afurther%20integrate%20this%20targeted%20adaptation%20with%20an%20adversarial%20training%20scheme%2C%0Ayielding%20additional%20improvements%20in%20cross-domain%20generalizability.%20DoSReMC%20can%0Abe%20readily%20incorporated%20into%20existing%20AI%20pipelines%20and%20applied%20across%20diverse%0Aclinical%20environments%2C%20providing%20a%20practical%20pathway%20toward%20more%20robust%20and%0Ageneralizable%20mammography%20classification%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoSReMC%253A%2520Domain%2520Shift%2520Resilient%2520Mammography%2520Classification%2520using%2520Batch%250A%2520%2520Normalization%2520Adaptation%26entry.906535625%3DU%25C4%259Furcan%2520Aky%25C3%25BCz%2520and%2520Deniz%2520Katircioglu-%25C3%2596zt%25C3%25BCrk%2520and%2520Emre%2520K.%2520S%25C3%25BCsl%25C3%25BC%2520and%2520Burhan%2520Kele%25C5%259F%2520and%2520Mete%2520C.%2520Kaya%2520and%2520Gamze%2520Durhan%2520and%2520Meltem%2520G.%2520Akp%25C4%25B1nar%2520and%2520Figen%2520B.%2520Demirkaz%25C4%25B1k%2520and%2520G%25C3%25B6zde%2520B.%2520Akar%26entry.1292438233%3D%2520%2520Numerous%2520deep%2520learning-based%2520solutions%2520have%2520been%2520developed%2520for%2520the%2520automatic%250Arecognition%2520of%2520breast%2520cancer%2520using%2520mammography%2520images.%2520However%252C%2520their%250Aperformance%2520often%2520declines%2520when%2520applied%2520to%2520data%2520from%2520different%2520domains%252C%250Aprimarily%2520due%2520to%2520domain%2520shift%2520--%2520the%2520variation%2520in%2520data%2520distributions%2520between%250Asource%2520and%2520target%2520domains.%2520This%2520performance%2520drop%2520limits%2520the%2520safe%2520and%2520equitable%250Adeployment%2520of%2520AI%2520in%2520real-world%2520clinical%2520settings.%2520In%2520this%2520study%252C%2520we%2520present%250ADoSReMC%2520%2528Domain%2520Shift%2520Resilient%2520Mammography%2520Classification%2529%252C%2520a%2520batch%250Anormalization%2520%2528BN%2529%2520adaptation%2520framework%2520designed%2520to%2520enhance%2520cross-domain%250Ageneralization%2520without%2520retraining%2520the%2520entire%2520model.%2520Using%2520three%2520large-scale%250Afull-field%2520digital%2520mammography%2520%2528FFDM%2529%2520datasets%2520--%2520including%2520HCTP%252C%2520a%2520newly%250Aintroduced%252C%2520pathologically%2520confirmed%2520in-house%2520dataset%2520--%2520we%2520conduct%2520a%250Asystematic%2520cross-domain%2520evaluation%2520with%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529.%250AOur%2520results%2520demonstrate%2520that%2520BN%2520layers%2520are%2520a%2520primary%2520source%2520of%2520domain%250Adependence%253A%2520they%2520perform%2520effectively%2520when%2520training%2520and%2520testing%2520occur%2520within%2520the%250Asame%2520domain%252C%2520and%2520they%2520significantly%2520impair%2520model%2520generalization%2520under%2520domain%250Ashift.%2520DoSReMC%2520addresses%2520this%2520limitation%2520by%2520fine-tuning%2520only%2520the%2520BN%2520and%2520fully%250Aconnected%2520%2528FC%2529%2520layers%252C%2520while%2520preserving%2520pretrained%2520convolutional%2520filters.%2520We%250Afurther%2520integrate%2520this%2520targeted%2520adaptation%2520with%2520an%2520adversarial%2520training%2520scheme%252C%250Ayielding%2520additional%2520improvements%2520in%2520cross-domain%2520generalizability.%2520DoSReMC%2520can%250Abe%2520readily%2520incorporated%2520into%2520existing%2520AI%2520pipelines%2520and%2520applied%2520across%2520diverse%250Aclinical%2520environments%252C%2520providing%2520a%2520practical%2520pathway%2520toward%2520more%2520robust%2520and%250Ageneralizable%2520mammography%2520classification%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DoSReMC%3A%20Domain%20Shift%20Resilient%20Mammography%20Classification%20using%20Batch%0A%20%20Normalization%20Adaptation&entry.906535625=U%C4%9Furcan%20Aky%C3%BCz%20and%20Deniz%20Katircioglu-%C3%96zt%C3%BCrk%20and%20Emre%20K.%20S%C3%BCsl%C3%BC%20and%20Burhan%20Kele%C5%9F%20and%20Mete%20C.%20Kaya%20and%20Gamze%20Durhan%20and%20Meltem%20G.%20Akp%C4%B1nar%20and%20Figen%20B.%20Demirkaz%C4%B1k%20and%20G%C3%B6zde%20B.%20Akar&entry.1292438233=%20%20Numerous%20deep%20learning-based%20solutions%20have%20been%20developed%20for%20the%20automatic%0Arecognition%20of%20breast%20cancer%20using%20mammography%20images.%20However%2C%20their%0Aperformance%20often%20declines%20when%20applied%20to%20data%20from%20different%20domains%2C%0Aprimarily%20due%20to%20domain%20shift%20--%20the%20variation%20in%20data%20distributions%20between%0Asource%20and%20target%20domains.%20This%20performance%20drop%20limits%20the%20safe%20and%20equitable%0Adeployment%20of%20AI%20in%20real-world%20clinical%20settings.%20In%20this%20study%2C%20we%20present%0ADoSReMC%20%28Domain%20Shift%20Resilient%20Mammography%20Classification%29%2C%20a%20batch%0Anormalization%20%28BN%29%20adaptation%20framework%20designed%20to%20enhance%20cross-domain%0Ageneralization%20without%20retraining%20the%20entire%20model.%20Using%20three%20large-scale%0Afull-field%20digital%20mammography%20%28FFDM%29%20datasets%20--%20including%20HCTP%2C%20a%20newly%0Aintroduced%2C%20pathologically%20confirmed%20in-house%20dataset%20--%20we%20conduct%20a%0Asystematic%20cross-domain%20evaluation%20with%20convolutional%20neural%20networks%20%28CNNs%29.%0AOur%20results%20demonstrate%20that%20BN%20layers%20are%20a%20primary%20source%20of%20domain%0Adependence%3A%20they%20perform%20effectively%20when%20training%20and%20testing%20occur%20within%20the%0Asame%20domain%2C%20and%20they%20significantly%20impair%20model%20generalization%20under%20domain%0Ashift.%20DoSReMC%20addresses%20this%20limitation%20by%20fine-tuning%20only%20the%20BN%20and%20fully%0Aconnected%20%28FC%29%20layers%2C%20while%20preserving%20pretrained%20convolutional%20filters.%20We%0Afurther%20integrate%20this%20targeted%20adaptation%20with%20an%20adversarial%20training%20scheme%2C%0Ayielding%20additional%20improvements%20in%20cross-domain%20generalizability.%20DoSReMC%20can%0Abe%20readily%20incorporated%20into%20existing%20AI%20pipelines%20and%20applied%20across%20diverse%0Aclinical%20environments%2C%20providing%20a%20practical%20pathway%20toward%20more%20robust%20and%0Ageneralizable%20mammography%20classification%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15452v1&entry.124074799=Read"},
{"title": "Foundational Design Principles and Patterns for Building Robust and\n  Adaptive GenAI-Native Systems", "author": "Frederik Vandeputte", "abstract": "  Generative AI (GenAI) has emerged as a transformative technology,\ndemonstrating remarkable capabilities across diverse application domains.\nHowever, GenAI faces several major challenges in developing reliable and\nefficient GenAI-empowered systems due to its unpredictability and inefficiency.\nThis paper advocates for a paradigm shift: future GenAI-native systems should\nintegrate GenAI's cognitive capabilities with traditional software engineering\nprinciples to create robust, adaptive, and efficient systems.\n  We introduce foundational GenAI-native design principles centered around five\nkey pillars -- reliability, excellence, evolvability, self-reliance, and\nassurance -- and propose architectural patterns such as GenAI-native cells,\norganic substrates, and programmable routers to guide the creation of resilient\nand self-evolving systems. Additionally, we outline the key ingredients of a\nGenAI-native software stack and discuss the impact of these systems from\ntechnical, user adoption, economic, and legal perspectives, underscoring the\nneed for further validation and experimentation. Our work aims to inspire\nfuture research and encourage relevant communities to implement and refine this\nconceptual framework.\n", "link": "http://arxiv.org/abs/2508.15411v1", "date": "2025-08-21", "relevancy": 2.45, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.517}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5039}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundational%20Design%20Principles%20and%20Patterns%20for%20Building%20Robust%20and%0A%20%20Adaptive%20GenAI-Native%20Systems&body=Title%3A%20Foundational%20Design%20Principles%20and%20Patterns%20for%20Building%20Robust%20and%0A%20%20Adaptive%20GenAI-Native%20Systems%0AAuthor%3A%20Frederik%20Vandeputte%0AAbstract%3A%20%20%20Generative%20AI%20%28GenAI%29%20has%20emerged%20as%20a%20transformative%20technology%2C%0Ademonstrating%20remarkable%20capabilities%20across%20diverse%20application%20domains.%0AHowever%2C%20GenAI%20faces%20several%20major%20challenges%20in%20developing%20reliable%20and%0Aefficient%20GenAI-empowered%20systems%20due%20to%20its%20unpredictability%20and%20inefficiency.%0AThis%20paper%20advocates%20for%20a%20paradigm%20shift%3A%20future%20GenAI-native%20systems%20should%0Aintegrate%20GenAI%27s%20cognitive%20capabilities%20with%20traditional%20software%20engineering%0Aprinciples%20to%20create%20robust%2C%20adaptive%2C%20and%20efficient%20systems.%0A%20%20We%20introduce%20foundational%20GenAI-native%20design%20principles%20centered%20around%20five%0Akey%20pillars%20--%20reliability%2C%20excellence%2C%20evolvability%2C%20self-reliance%2C%20and%0Aassurance%20--%20and%20propose%20architectural%20patterns%20such%20as%20GenAI-native%20cells%2C%0Aorganic%20substrates%2C%20and%20programmable%20routers%20to%20guide%20the%20creation%20of%20resilient%0Aand%20self-evolving%20systems.%20Additionally%2C%20we%20outline%20the%20key%20ingredients%20of%20a%0AGenAI-native%20software%20stack%20and%20discuss%20the%20impact%20of%20these%20systems%20from%0Atechnical%2C%20user%20adoption%2C%20economic%2C%20and%20legal%20perspectives%2C%20underscoring%20the%0Aneed%20for%20further%20validation%20and%20experimentation.%20Our%20work%20aims%20to%20inspire%0Afuture%20research%20and%20encourage%20relevant%20communities%20to%20implement%20and%20refine%20this%0Aconceptual%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundational%2520Design%2520Principles%2520and%2520Patterns%2520for%2520Building%2520Robust%2520and%250A%2520%2520Adaptive%2520GenAI-Native%2520Systems%26entry.906535625%3DFrederik%2520Vandeputte%26entry.1292438233%3D%2520%2520Generative%2520AI%2520%2528GenAI%2529%2520has%2520emerged%2520as%2520a%2520transformative%2520technology%252C%250Ademonstrating%2520remarkable%2520capabilities%2520across%2520diverse%2520application%2520domains.%250AHowever%252C%2520GenAI%2520faces%2520several%2520major%2520challenges%2520in%2520developing%2520reliable%2520and%250Aefficient%2520GenAI-empowered%2520systems%2520due%2520to%2520its%2520unpredictability%2520and%2520inefficiency.%250AThis%2520paper%2520advocates%2520for%2520a%2520paradigm%2520shift%253A%2520future%2520GenAI-native%2520systems%2520should%250Aintegrate%2520GenAI%2527s%2520cognitive%2520capabilities%2520with%2520traditional%2520software%2520engineering%250Aprinciples%2520to%2520create%2520robust%252C%2520adaptive%252C%2520and%2520efficient%2520systems.%250A%2520%2520We%2520introduce%2520foundational%2520GenAI-native%2520design%2520principles%2520centered%2520around%2520five%250Akey%2520pillars%2520--%2520reliability%252C%2520excellence%252C%2520evolvability%252C%2520self-reliance%252C%2520and%250Aassurance%2520--%2520and%2520propose%2520architectural%2520patterns%2520such%2520as%2520GenAI-native%2520cells%252C%250Aorganic%2520substrates%252C%2520and%2520programmable%2520routers%2520to%2520guide%2520the%2520creation%2520of%2520resilient%250Aand%2520self-evolving%2520systems.%2520Additionally%252C%2520we%2520outline%2520the%2520key%2520ingredients%2520of%2520a%250AGenAI-native%2520software%2520stack%2520and%2520discuss%2520the%2520impact%2520of%2520these%2520systems%2520from%250Atechnical%252C%2520user%2520adoption%252C%2520economic%252C%2520and%2520legal%2520perspectives%252C%2520underscoring%2520the%250Aneed%2520for%2520further%2520validation%2520and%2520experimentation.%2520Our%2520work%2520aims%2520to%2520inspire%250Afuture%2520research%2520and%2520encourage%2520relevant%2520communities%2520to%2520implement%2520and%2520refine%2520this%250Aconceptual%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundational%20Design%20Principles%20and%20Patterns%20for%20Building%20Robust%20and%0A%20%20Adaptive%20GenAI-Native%20Systems&entry.906535625=Frederik%20Vandeputte&entry.1292438233=%20%20Generative%20AI%20%28GenAI%29%20has%20emerged%20as%20a%20transformative%20technology%2C%0Ademonstrating%20remarkable%20capabilities%20across%20diverse%20application%20domains.%0AHowever%2C%20GenAI%20faces%20several%20major%20challenges%20in%20developing%20reliable%20and%0Aefficient%20GenAI-empowered%20systems%20due%20to%20its%20unpredictability%20and%20inefficiency.%0AThis%20paper%20advocates%20for%20a%20paradigm%20shift%3A%20future%20GenAI-native%20systems%20should%0Aintegrate%20GenAI%27s%20cognitive%20capabilities%20with%20traditional%20software%20engineering%0Aprinciples%20to%20create%20robust%2C%20adaptive%2C%20and%20efficient%20systems.%0A%20%20We%20introduce%20foundational%20GenAI-native%20design%20principles%20centered%20around%20five%0Akey%20pillars%20--%20reliability%2C%20excellence%2C%20evolvability%2C%20self-reliance%2C%20and%0Aassurance%20--%20and%20propose%20architectural%20patterns%20such%20as%20GenAI-native%20cells%2C%0Aorganic%20substrates%2C%20and%20programmable%20routers%20to%20guide%20the%20creation%20of%20resilient%0Aand%20self-evolving%20systems.%20Additionally%2C%20we%20outline%20the%20key%20ingredients%20of%20a%0AGenAI-native%20software%20stack%20and%20discuss%20the%20impact%20of%20these%20systems%20from%0Atechnical%2C%20user%20adoption%2C%20economic%2C%20and%20legal%20perspectives%2C%20underscoring%20the%0Aneed%20for%20further%20validation%20and%20experimentation.%20Our%20work%20aims%20to%20inspire%0Afuture%20research%20and%20encourage%20relevant%20communities%20to%20implement%20and%20refine%20this%0Aconceptual%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15411v1&entry.124074799=Read"},
{"title": "GRASPED: Graph Anomaly Detection using Autoencoder with Spectral Encoder\n  and Decoder (Full Version)", "author": "Wei Herng Choong and Jixing Liu and Ching-Yu Kao and Philip Sperl", "abstract": "  Graph machine learning has been widely explored in various domains, such as\ncommunity detection, transaction analysis, and recommendation systems. In these\napplications, anomaly detection plays an important role. Recently, studies have\nshown that anomalies on graphs induce spectral shifts. Some supervised methods\nhave improved the utilization of such spectral domain information. However,\nthey remain limited by the scarcity of labeled data due to the nature of\nanomalies. On the other hand, existing unsupervised learning approaches\npredominantly rely on spatial information or only employ low-pass filters,\nthereby losing the capacity for multi-band analysis. In this paper, we propose\nGraph Autoencoder with Spectral Encoder and Spectral Decoder (GRASPED) for node\nanomaly detection. Our unsupervised learning model features an encoder based on\nGraph Wavelet Convolution, along with structural and attribute decoders. The\nGraph Wavelet Convolution-based encoder, combined with a Wiener Graph\nDeconvolution-based decoder, exhibits bandpass filter characteristics that\ncapture global and local graph information at multiple scales. This design\nallows for a learning-based reconstruction of node attributes, effectively\ncapturing anomaly information. Extensive experiments on several real-world\ngraph anomaly detection datasets demonstrate that GRASPED outperforms current\nstate-of-the-art models.\n", "link": "http://arxiv.org/abs/2508.15633v1", "date": "2025-08-21", "relevancy": 2.4204, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.503}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4804}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRASPED%3A%20Graph%20Anomaly%20Detection%20using%20Autoencoder%20with%20Spectral%20Encoder%0A%20%20and%20Decoder%20%28Full%20Version%29&body=Title%3A%20GRASPED%3A%20Graph%20Anomaly%20Detection%20using%20Autoencoder%20with%20Spectral%20Encoder%0A%20%20and%20Decoder%20%28Full%20Version%29%0AAuthor%3A%20Wei%20Herng%20Choong%20and%20Jixing%20Liu%20and%20Ching-Yu%20Kao%20and%20Philip%20Sperl%0AAbstract%3A%20%20%20Graph%20machine%20learning%20has%20been%20widely%20explored%20in%20various%20domains%2C%20such%20as%0Acommunity%20detection%2C%20transaction%20analysis%2C%20and%20recommendation%20systems.%20In%20these%0Aapplications%2C%20anomaly%20detection%20plays%20an%20important%20role.%20Recently%2C%20studies%20have%0Ashown%20that%20anomalies%20on%20graphs%20induce%20spectral%20shifts.%20Some%20supervised%20methods%0Ahave%20improved%20the%20utilization%20of%20such%20spectral%20domain%20information.%20However%2C%0Athey%20remain%20limited%20by%20the%20scarcity%20of%20labeled%20data%20due%20to%20the%20nature%20of%0Aanomalies.%20On%20the%20other%20hand%2C%20existing%20unsupervised%20learning%20approaches%0Apredominantly%20rely%20on%20spatial%20information%20or%20only%20employ%20low-pass%20filters%2C%0Athereby%20losing%20the%20capacity%20for%20multi-band%20analysis.%20In%20this%20paper%2C%20we%20propose%0AGraph%20Autoencoder%20with%20Spectral%20Encoder%20and%20Spectral%20Decoder%20%28GRASPED%29%20for%20node%0Aanomaly%20detection.%20Our%20unsupervised%20learning%20model%20features%20an%20encoder%20based%20on%0AGraph%20Wavelet%20Convolution%2C%20along%20with%20structural%20and%20attribute%20decoders.%20The%0AGraph%20Wavelet%20Convolution-based%20encoder%2C%20combined%20with%20a%20Wiener%20Graph%0ADeconvolution-based%20decoder%2C%20exhibits%20bandpass%20filter%20characteristics%20that%0Acapture%20global%20and%20local%20graph%20information%20at%20multiple%20scales.%20This%20design%0Aallows%20for%20a%20learning-based%20reconstruction%20of%20node%20attributes%2C%20effectively%0Acapturing%20anomaly%20information.%20Extensive%20experiments%20on%20several%20real-world%0Agraph%20anomaly%20detection%20datasets%20demonstrate%20that%20GRASPED%20outperforms%20current%0Astate-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRASPED%253A%2520Graph%2520Anomaly%2520Detection%2520using%2520Autoencoder%2520with%2520Spectral%2520Encoder%250A%2520%2520and%2520Decoder%2520%2528Full%2520Version%2529%26entry.906535625%3DWei%2520Herng%2520Choong%2520and%2520Jixing%2520Liu%2520and%2520Ching-Yu%2520Kao%2520and%2520Philip%2520Sperl%26entry.1292438233%3D%2520%2520Graph%2520machine%2520learning%2520has%2520been%2520widely%2520explored%2520in%2520various%2520domains%252C%2520such%2520as%250Acommunity%2520detection%252C%2520transaction%2520analysis%252C%2520and%2520recommendation%2520systems.%2520In%2520these%250Aapplications%252C%2520anomaly%2520detection%2520plays%2520an%2520important%2520role.%2520Recently%252C%2520studies%2520have%250Ashown%2520that%2520anomalies%2520on%2520graphs%2520induce%2520spectral%2520shifts.%2520Some%2520supervised%2520methods%250Ahave%2520improved%2520the%2520utilization%2520of%2520such%2520spectral%2520domain%2520information.%2520However%252C%250Athey%2520remain%2520limited%2520by%2520the%2520scarcity%2520of%2520labeled%2520data%2520due%2520to%2520the%2520nature%2520of%250Aanomalies.%2520On%2520the%2520other%2520hand%252C%2520existing%2520unsupervised%2520learning%2520approaches%250Apredominantly%2520rely%2520on%2520spatial%2520information%2520or%2520only%2520employ%2520low-pass%2520filters%252C%250Athereby%2520losing%2520the%2520capacity%2520for%2520multi-band%2520analysis.%2520In%2520this%2520paper%252C%2520we%2520propose%250AGraph%2520Autoencoder%2520with%2520Spectral%2520Encoder%2520and%2520Spectral%2520Decoder%2520%2528GRASPED%2529%2520for%2520node%250Aanomaly%2520detection.%2520Our%2520unsupervised%2520learning%2520model%2520features%2520an%2520encoder%2520based%2520on%250AGraph%2520Wavelet%2520Convolution%252C%2520along%2520with%2520structural%2520and%2520attribute%2520decoders.%2520The%250AGraph%2520Wavelet%2520Convolution-based%2520encoder%252C%2520combined%2520with%2520a%2520Wiener%2520Graph%250ADeconvolution-based%2520decoder%252C%2520exhibits%2520bandpass%2520filter%2520characteristics%2520that%250Acapture%2520global%2520and%2520local%2520graph%2520information%2520at%2520multiple%2520scales.%2520This%2520design%250Aallows%2520for%2520a%2520learning-based%2520reconstruction%2520of%2520node%2520attributes%252C%2520effectively%250Acapturing%2520anomaly%2520information.%2520Extensive%2520experiments%2520on%2520several%2520real-world%250Agraph%2520anomaly%2520detection%2520datasets%2520demonstrate%2520that%2520GRASPED%2520outperforms%2520current%250Astate-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRASPED%3A%20Graph%20Anomaly%20Detection%20using%20Autoencoder%20with%20Spectral%20Encoder%0A%20%20and%20Decoder%20%28Full%20Version%29&entry.906535625=Wei%20Herng%20Choong%20and%20Jixing%20Liu%20and%20Ching-Yu%20Kao%20and%20Philip%20Sperl&entry.1292438233=%20%20Graph%20machine%20learning%20has%20been%20widely%20explored%20in%20various%20domains%2C%20such%20as%0Acommunity%20detection%2C%20transaction%20analysis%2C%20and%20recommendation%20systems.%20In%20these%0Aapplications%2C%20anomaly%20detection%20plays%20an%20important%20role.%20Recently%2C%20studies%20have%0Ashown%20that%20anomalies%20on%20graphs%20induce%20spectral%20shifts.%20Some%20supervised%20methods%0Ahave%20improved%20the%20utilization%20of%20such%20spectral%20domain%20information.%20However%2C%0Athey%20remain%20limited%20by%20the%20scarcity%20of%20labeled%20data%20due%20to%20the%20nature%20of%0Aanomalies.%20On%20the%20other%20hand%2C%20existing%20unsupervised%20learning%20approaches%0Apredominantly%20rely%20on%20spatial%20information%20or%20only%20employ%20low-pass%20filters%2C%0Athereby%20losing%20the%20capacity%20for%20multi-band%20analysis.%20In%20this%20paper%2C%20we%20propose%0AGraph%20Autoencoder%20with%20Spectral%20Encoder%20and%20Spectral%20Decoder%20%28GRASPED%29%20for%20node%0Aanomaly%20detection.%20Our%20unsupervised%20learning%20model%20features%20an%20encoder%20based%20on%0AGraph%20Wavelet%20Convolution%2C%20along%20with%20structural%20and%20attribute%20decoders.%20The%0AGraph%20Wavelet%20Convolution-based%20encoder%2C%20combined%20with%20a%20Wiener%20Graph%0ADeconvolution-based%20decoder%2C%20exhibits%20bandpass%20filter%20characteristics%20that%0Acapture%20global%20and%20local%20graph%20information%20at%20multiple%20scales.%20This%20design%0Aallows%20for%20a%20learning-based%20reconstruction%20of%20node%20attributes%2C%20effectively%0Acapturing%20anomaly%20information.%20Extensive%20experiments%20on%20several%20real-world%0Agraph%20anomaly%20detection%20datasets%20demonstrate%20that%20GRASPED%20outperforms%20current%0Astate-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15633v1&entry.124074799=Read"},
{"title": "Waver: Wave Your Way to Lifelike Video Generation", "author": "Yifu Zhang and Hao Yang and Yuqi Zhang and Yifei Hu and Fengda Zhu and Chuang Lin and Xiaofeng Mei and Yi Jiang and Zehuan Yuan and Bingyue Peng", "abstract": "  We present Waver, a high-performance foundation model for unified image and\nvideo generation. Waver can directly generate videos with durations ranging\nfrom 5 to 10 seconds at a native resolution of 720p, which are subsequently\nupscaled to 1080p. The model simultaneously supports text-to-video (T2V),\nimage-to-video (I2V), and text-to-image (T2I) generation within a single,\nintegrated framework. We introduce a Hybrid Stream DiT architecture to enhance\nmodality alignment and accelerate training convergence. To ensure training data\nquality, we establish a comprehensive data curation pipeline and manually\nannotate and train an MLLM-based video quality model to filter for the\nhighest-quality samples. Furthermore, we provide detailed training and\ninference recipes to facilitate the generation of high-quality videos. Building\non these contributions, Waver excels at capturing complex motion, achieving\nsuperior motion amplitude and temporal consistency in video synthesis. Notably,\nit ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial\nAnalysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming\nexisting open-source models and matching or surpassing state-of-the-art\ncommercial solutions. We hope this technical report will help the community\nmore efficiently train high-quality video generation models and accelerate\nprogress in video generation technologies. Official page:\nhttps://github.com/FoundationVision/Waver.\n", "link": "http://arxiv.org/abs/2508.15761v1", "date": "2025-08-21", "relevancy": 2.4051, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6114}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5985}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Waver%3A%20Wave%20Your%20Way%20to%20Lifelike%20Video%20Generation&body=Title%3A%20Waver%3A%20Wave%20Your%20Way%20to%20Lifelike%20Video%20Generation%0AAuthor%3A%20Yifu%20Zhang%20and%20Hao%20Yang%20and%20Yuqi%20Zhang%20and%20Yifei%20Hu%20and%20Fengda%20Zhu%20and%20Chuang%20Lin%20and%20Xiaofeng%20Mei%20and%20Yi%20Jiang%20and%20Zehuan%20Yuan%20and%20Bingyue%20Peng%0AAbstract%3A%20%20%20We%20present%20Waver%2C%20a%20high-performance%20foundation%20model%20for%20unified%20image%20and%0Avideo%20generation.%20Waver%20can%20directly%20generate%20videos%20with%20durations%20ranging%0Afrom%205%20to%2010%20seconds%20at%20a%20native%20resolution%20of%20720p%2C%20which%20are%20subsequently%0Aupscaled%20to%201080p.%20The%20model%20simultaneously%20supports%20text-to-video%20%28T2V%29%2C%0Aimage-to-video%20%28I2V%29%2C%20and%20text-to-image%20%28T2I%29%20generation%20within%20a%20single%2C%0Aintegrated%20framework.%20We%20introduce%20a%20Hybrid%20Stream%20DiT%20architecture%20to%20enhance%0Amodality%20alignment%20and%20accelerate%20training%20convergence.%20To%20ensure%20training%20data%0Aquality%2C%20we%20establish%20a%20comprehensive%20data%20curation%20pipeline%20and%20manually%0Aannotate%20and%20train%20an%20MLLM-based%20video%20quality%20model%20to%20filter%20for%20the%0Ahighest-quality%20samples.%20Furthermore%2C%20we%20provide%20detailed%20training%20and%0Ainference%20recipes%20to%20facilitate%20the%20generation%20of%20high-quality%20videos.%20Building%0Aon%20these%20contributions%2C%20Waver%20excels%20at%20capturing%20complex%20motion%2C%20achieving%0Asuperior%20motion%20amplitude%20and%20temporal%20consistency%20in%20video%20synthesis.%20Notably%2C%0Ait%20ranks%20among%20the%20Top%203%20on%20both%20the%20T2V%20and%20I2V%20leaderboards%20at%20Artificial%0AAnalysis%20%28data%20as%20of%202025-07-30%2010%3A00%20GMT%2B8%29%2C%20consistently%20outperforming%0Aexisting%20open-source%20models%20and%20matching%20or%20surpassing%20state-of-the-art%0Acommercial%20solutions.%20We%20hope%20this%20technical%20report%20will%20help%20the%20community%0Amore%20efficiently%20train%20high-quality%20video%20generation%20models%20and%20accelerate%0Aprogress%20in%20video%20generation%20technologies.%20Official%20page%3A%0Ahttps%3A//github.com/FoundationVision/Waver.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWaver%253A%2520Wave%2520Your%2520Way%2520to%2520Lifelike%2520Video%2520Generation%26entry.906535625%3DYifu%2520Zhang%2520and%2520Hao%2520Yang%2520and%2520Yuqi%2520Zhang%2520and%2520Yifei%2520Hu%2520and%2520Fengda%2520Zhu%2520and%2520Chuang%2520Lin%2520and%2520Xiaofeng%2520Mei%2520and%2520Yi%2520Jiang%2520and%2520Zehuan%2520Yuan%2520and%2520Bingyue%2520Peng%26entry.1292438233%3D%2520%2520We%2520present%2520Waver%252C%2520a%2520high-performance%2520foundation%2520model%2520for%2520unified%2520image%2520and%250Avideo%2520generation.%2520Waver%2520can%2520directly%2520generate%2520videos%2520with%2520durations%2520ranging%250Afrom%25205%2520to%252010%2520seconds%2520at%2520a%2520native%2520resolution%2520of%2520720p%252C%2520which%2520are%2520subsequently%250Aupscaled%2520to%25201080p.%2520The%2520model%2520simultaneously%2520supports%2520text-to-video%2520%2528T2V%2529%252C%250Aimage-to-video%2520%2528I2V%2529%252C%2520and%2520text-to-image%2520%2528T2I%2529%2520generation%2520within%2520a%2520single%252C%250Aintegrated%2520framework.%2520We%2520introduce%2520a%2520Hybrid%2520Stream%2520DiT%2520architecture%2520to%2520enhance%250Amodality%2520alignment%2520and%2520accelerate%2520training%2520convergence.%2520To%2520ensure%2520training%2520data%250Aquality%252C%2520we%2520establish%2520a%2520comprehensive%2520data%2520curation%2520pipeline%2520and%2520manually%250Aannotate%2520and%2520train%2520an%2520MLLM-based%2520video%2520quality%2520model%2520to%2520filter%2520for%2520the%250Ahighest-quality%2520samples.%2520Furthermore%252C%2520we%2520provide%2520detailed%2520training%2520and%250Ainference%2520recipes%2520to%2520facilitate%2520the%2520generation%2520of%2520high-quality%2520videos.%2520Building%250Aon%2520these%2520contributions%252C%2520Waver%2520excels%2520at%2520capturing%2520complex%2520motion%252C%2520achieving%250Asuperior%2520motion%2520amplitude%2520and%2520temporal%2520consistency%2520in%2520video%2520synthesis.%2520Notably%252C%250Ait%2520ranks%2520among%2520the%2520Top%25203%2520on%2520both%2520the%2520T2V%2520and%2520I2V%2520leaderboards%2520at%2520Artificial%250AAnalysis%2520%2528data%2520as%2520of%25202025-07-30%252010%253A00%2520GMT%252B8%2529%252C%2520consistently%2520outperforming%250Aexisting%2520open-source%2520models%2520and%2520matching%2520or%2520surpassing%2520state-of-the-art%250Acommercial%2520solutions.%2520We%2520hope%2520this%2520technical%2520report%2520will%2520help%2520the%2520community%250Amore%2520efficiently%2520train%2520high-quality%2520video%2520generation%2520models%2520and%2520accelerate%250Aprogress%2520in%2520video%2520generation%2520technologies.%2520Official%2520page%253A%250Ahttps%253A//github.com/FoundationVision/Waver.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Waver%3A%20Wave%20Your%20Way%20to%20Lifelike%20Video%20Generation&entry.906535625=Yifu%20Zhang%20and%20Hao%20Yang%20and%20Yuqi%20Zhang%20and%20Yifei%20Hu%20and%20Fengda%20Zhu%20and%20Chuang%20Lin%20and%20Xiaofeng%20Mei%20and%20Yi%20Jiang%20and%20Zehuan%20Yuan%20and%20Bingyue%20Peng&entry.1292438233=%20%20We%20present%20Waver%2C%20a%20high-performance%20foundation%20model%20for%20unified%20image%20and%0Avideo%20generation.%20Waver%20can%20directly%20generate%20videos%20with%20durations%20ranging%0Afrom%205%20to%2010%20seconds%20at%20a%20native%20resolution%20of%20720p%2C%20which%20are%20subsequently%0Aupscaled%20to%201080p.%20The%20model%20simultaneously%20supports%20text-to-video%20%28T2V%29%2C%0Aimage-to-video%20%28I2V%29%2C%20and%20text-to-image%20%28T2I%29%20generation%20within%20a%20single%2C%0Aintegrated%20framework.%20We%20introduce%20a%20Hybrid%20Stream%20DiT%20architecture%20to%20enhance%0Amodality%20alignment%20and%20accelerate%20training%20convergence.%20To%20ensure%20training%20data%0Aquality%2C%20we%20establish%20a%20comprehensive%20data%20curation%20pipeline%20and%20manually%0Aannotate%20and%20train%20an%20MLLM-based%20video%20quality%20model%20to%20filter%20for%20the%0Ahighest-quality%20samples.%20Furthermore%2C%20we%20provide%20detailed%20training%20and%0Ainference%20recipes%20to%20facilitate%20the%20generation%20of%20high-quality%20videos.%20Building%0Aon%20these%20contributions%2C%20Waver%20excels%20at%20capturing%20complex%20motion%2C%20achieving%0Asuperior%20motion%20amplitude%20and%20temporal%20consistency%20in%20video%20synthesis.%20Notably%2C%0Ait%20ranks%20among%20the%20Top%203%20on%20both%20the%20T2V%20and%20I2V%20leaderboards%20at%20Artificial%0AAnalysis%20%28data%20as%20of%202025-07-30%2010%3A00%20GMT%2B8%29%2C%20consistently%20outperforming%0Aexisting%20open-source%20models%20and%20matching%20or%20surpassing%20state-of-the-art%0Acommercial%20solutions.%20We%20hope%20this%20technical%20report%20will%20help%20the%20community%0Amore%20efficiently%20train%20high-quality%20video%20generation%20models%20and%20accelerate%0Aprogress%20in%20video%20generation%20technologies.%20Official%20page%3A%0Ahttps%3A//github.com/FoundationVision/Waver.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15761v1&entry.124074799=Read"},
{"title": "Super-additive Cooperation in Language Model Agents", "author": "Filippo Tonini and Lukas Galke", "abstract": "  With the prospect of autonomous artificial intelligence (AI) agents, studying\ntheir tendency for cooperative behavior becomes an increasingly relevant topic.\nThis study is inspired by the super-additive cooperation theory, where the\ncombined effects of repeated interactions and inter-group rivalry have been\nargued to be the cause for cooperative tendencies found in humans. We devised a\nvirtual tournament where language model agents, grouped into teams, face each\nother in a Prisoner's Dilemma game. By simulating both internal team dynamics\nand external competition, we discovered that this blend substantially boosts\nboth overall and initial, one-shot cooperation levels (the tendency to\ncooperate in one-off interactions). This research provides a novel framework\nfor large language models to strategize and act in complex social scenarios and\noffers evidence for how intergroup competition can, counter-intuitively, result\nin more cooperative behavior. These insights are crucial for designing future\nmulti-agent AI systems that can effectively work together and better align with\nhuman values. Source code is available at\nhttps://github.com/pippot/Superadditive-cooperation-LLMs.\n", "link": "http://arxiv.org/abs/2508.15510v1", "date": "2025-08-21", "relevancy": 2.3978, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4654}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Super-additive%20Cooperation%20in%20Language%20Model%20Agents&body=Title%3A%20Super-additive%20Cooperation%20in%20Language%20Model%20Agents%0AAuthor%3A%20Filippo%20Tonini%20and%20Lukas%20Galke%0AAbstract%3A%20%20%20With%20the%20prospect%20of%20autonomous%20artificial%20intelligence%20%28AI%29%20agents%2C%20studying%0Atheir%20tendency%20for%20cooperative%20behavior%20becomes%20an%20increasingly%20relevant%20topic.%0AThis%20study%20is%20inspired%20by%20the%20super-additive%20cooperation%20theory%2C%20where%20the%0Acombined%20effects%20of%20repeated%20interactions%20and%20inter-group%20rivalry%20have%20been%0Aargued%20to%20be%20the%20cause%20for%20cooperative%20tendencies%20found%20in%20humans.%20We%20devised%20a%0Avirtual%20tournament%20where%20language%20model%20agents%2C%20grouped%20into%20teams%2C%20face%20each%0Aother%20in%20a%20Prisoner%27s%20Dilemma%20game.%20By%20simulating%20both%20internal%20team%20dynamics%0Aand%20external%20competition%2C%20we%20discovered%20that%20this%20blend%20substantially%20boosts%0Aboth%20overall%20and%20initial%2C%20one-shot%20cooperation%20levels%20%28the%20tendency%20to%0Acooperate%20in%20one-off%20interactions%29.%20This%20research%20provides%20a%20novel%20framework%0Afor%20large%20language%20models%20to%20strategize%20and%20act%20in%20complex%20social%20scenarios%20and%0Aoffers%20evidence%20for%20how%20intergroup%20competition%20can%2C%20counter-intuitively%2C%20result%0Ain%20more%20cooperative%20behavior.%20These%20insights%20are%20crucial%20for%20designing%20future%0Amulti-agent%20AI%20systems%20that%20can%20effectively%20work%20together%20and%20better%20align%20with%0Ahuman%20values.%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/pippot/Superadditive-cooperation-LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuper-additive%2520Cooperation%2520in%2520Language%2520Model%2520Agents%26entry.906535625%3DFilippo%2520Tonini%2520and%2520Lukas%2520Galke%26entry.1292438233%3D%2520%2520With%2520the%2520prospect%2520of%2520autonomous%2520artificial%2520intelligence%2520%2528AI%2529%2520agents%252C%2520studying%250Atheir%2520tendency%2520for%2520cooperative%2520behavior%2520becomes%2520an%2520increasingly%2520relevant%2520topic.%250AThis%2520study%2520is%2520inspired%2520by%2520the%2520super-additive%2520cooperation%2520theory%252C%2520where%2520the%250Acombined%2520effects%2520of%2520repeated%2520interactions%2520and%2520inter-group%2520rivalry%2520have%2520been%250Aargued%2520to%2520be%2520the%2520cause%2520for%2520cooperative%2520tendencies%2520found%2520in%2520humans.%2520We%2520devised%2520a%250Avirtual%2520tournament%2520where%2520language%2520model%2520agents%252C%2520grouped%2520into%2520teams%252C%2520face%2520each%250Aother%2520in%2520a%2520Prisoner%2527s%2520Dilemma%2520game.%2520By%2520simulating%2520both%2520internal%2520team%2520dynamics%250Aand%2520external%2520competition%252C%2520we%2520discovered%2520that%2520this%2520blend%2520substantially%2520boosts%250Aboth%2520overall%2520and%2520initial%252C%2520one-shot%2520cooperation%2520levels%2520%2528the%2520tendency%2520to%250Acooperate%2520in%2520one-off%2520interactions%2529.%2520This%2520research%2520provides%2520a%2520novel%2520framework%250Afor%2520large%2520language%2520models%2520to%2520strategize%2520and%2520act%2520in%2520complex%2520social%2520scenarios%2520and%250Aoffers%2520evidence%2520for%2520how%2520intergroup%2520competition%2520can%252C%2520counter-intuitively%252C%2520result%250Ain%2520more%2520cooperative%2520behavior.%2520These%2520insights%2520are%2520crucial%2520for%2520designing%2520future%250Amulti-agent%2520AI%2520systems%2520that%2520can%2520effectively%2520work%2520together%2520and%2520better%2520align%2520with%250Ahuman%2520values.%2520Source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/pippot/Superadditive-cooperation-LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Super-additive%20Cooperation%20in%20Language%20Model%20Agents&entry.906535625=Filippo%20Tonini%20and%20Lukas%20Galke&entry.1292438233=%20%20With%20the%20prospect%20of%20autonomous%20artificial%20intelligence%20%28AI%29%20agents%2C%20studying%0Atheir%20tendency%20for%20cooperative%20behavior%20becomes%20an%20increasingly%20relevant%20topic.%0AThis%20study%20is%20inspired%20by%20the%20super-additive%20cooperation%20theory%2C%20where%20the%0Acombined%20effects%20of%20repeated%20interactions%20and%20inter-group%20rivalry%20have%20been%0Aargued%20to%20be%20the%20cause%20for%20cooperative%20tendencies%20found%20in%20humans.%20We%20devised%20a%0Avirtual%20tournament%20where%20language%20model%20agents%2C%20grouped%20into%20teams%2C%20face%20each%0Aother%20in%20a%20Prisoner%27s%20Dilemma%20game.%20By%20simulating%20both%20internal%20team%20dynamics%0Aand%20external%20competition%2C%20we%20discovered%20that%20this%20blend%20substantially%20boosts%0Aboth%20overall%20and%20initial%2C%20one-shot%20cooperation%20levels%20%28the%20tendency%20to%0Acooperate%20in%20one-off%20interactions%29.%20This%20research%20provides%20a%20novel%20framework%0Afor%20large%20language%20models%20to%20strategize%20and%20act%20in%20complex%20social%20scenarios%20and%0Aoffers%20evidence%20for%20how%20intergroup%20competition%20can%2C%20counter-intuitively%2C%20result%0Ain%20more%20cooperative%20behavior.%20These%20insights%20are%20crucial%20for%20designing%20future%0Amulti-agent%20AI%20systems%20that%20can%20effectively%20work%20together%20and%20better%20align%20with%0Ahuman%20values.%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/pippot/Superadditive-cooperation-LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15510v1&entry.124074799=Read"},
{"title": "LORE: Latent Optimization for Precise Semantic Control in Rectified\n  Flow-based Image Editing", "author": "Liangyang Ouyang and Jiafeng Mao", "abstract": "  Text-driven image editing enables users to flexibly modify visual content\nthrough natural language instructions, and is widely applied to tasks such as\nsemantic object replacement, insertion, and removal. While recent\ninversion-based editing methods using rectified flow models have achieved\npromising results in image quality, we identify a structural limitation in\ntheir editing behavior: the semantic bias toward the source concept encoded in\nthe inverted noise tends to suppress attention to the target concept. This\nissue becomes particularly critical when the source and target semantics are\ndissimilar, where the attention mechanism inherently leads to editing failure\nor unintended modifications in non-target regions. In this paper, we\nsystematically analyze and validate this structural flaw, and introduce LORE, a\ntraining-free and efficient image editing method. LORE directly optimizes the\ninverted noise, addressing the core limitations in generalization and\ncontrollability of existing approaches, enabling stable, controllable, and\ngeneral-purpose concept replacement, without requiring architectural\nmodification or model fine-tuning. We conduct comprehensive evaluations on\nthree challenging benchmarks: PIEBench, SmartEdit, and GapEdit. Experimental\nresults show that LORE significantly outperforms strong baselines in terms of\nsemantic alignment, image quality, and background fidelity, demonstrating the\neffectiveness and scalability of latent-space optimization for general-purpose\nimage editing. Our implementation is available at\nhttps://github.com/oyly16/LORE.\n", "link": "http://arxiv.org/abs/2508.03144v2", "date": "2025-08-21", "relevancy": 2.3706, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6113}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5953}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LORE%3A%20Latent%20Optimization%20for%20Precise%20Semantic%20Control%20in%20Rectified%0A%20%20Flow-based%20Image%20Editing&body=Title%3A%20LORE%3A%20Latent%20Optimization%20for%20Precise%20Semantic%20Control%20in%20Rectified%0A%20%20Flow-based%20Image%20Editing%0AAuthor%3A%20Liangyang%20Ouyang%20and%20Jiafeng%20Mao%0AAbstract%3A%20%20%20Text-driven%20image%20editing%20enables%20users%20to%20flexibly%20modify%20visual%20content%0Athrough%20natural%20language%20instructions%2C%20and%20is%20widely%20applied%20to%20tasks%20such%20as%0Asemantic%20object%20replacement%2C%20insertion%2C%20and%20removal.%20While%20recent%0Ainversion-based%20editing%20methods%20using%20rectified%20flow%20models%20have%20achieved%0Apromising%20results%20in%20image%20quality%2C%20we%20identify%20a%20structural%20limitation%20in%0Atheir%20editing%20behavior%3A%20the%20semantic%20bias%20toward%20the%20source%20concept%20encoded%20in%0Athe%20inverted%20noise%20tends%20to%20suppress%20attention%20to%20the%20target%20concept.%20This%0Aissue%20becomes%20particularly%20critical%20when%20the%20source%20and%20target%20semantics%20are%0Adissimilar%2C%20where%20the%20attention%20mechanism%20inherently%20leads%20to%20editing%20failure%0Aor%20unintended%20modifications%20in%20non-target%20regions.%20In%20this%20paper%2C%20we%0Asystematically%20analyze%20and%20validate%20this%20structural%20flaw%2C%20and%20introduce%20LORE%2C%20a%0Atraining-free%20and%20efficient%20image%20editing%20method.%20LORE%20directly%20optimizes%20the%0Ainverted%20noise%2C%20addressing%20the%20core%20limitations%20in%20generalization%20and%0Acontrollability%20of%20existing%20approaches%2C%20enabling%20stable%2C%20controllable%2C%20and%0Ageneral-purpose%20concept%20replacement%2C%20without%20requiring%20architectural%0Amodification%20or%20model%20fine-tuning.%20We%20conduct%20comprehensive%20evaluations%20on%0Athree%20challenging%20benchmarks%3A%20PIEBench%2C%20SmartEdit%2C%20and%20GapEdit.%20Experimental%0Aresults%20show%20that%20LORE%20significantly%20outperforms%20strong%20baselines%20in%20terms%20of%0Asemantic%20alignment%2C%20image%20quality%2C%20and%20background%20fidelity%2C%20demonstrating%20the%0Aeffectiveness%20and%20scalability%20of%20latent-space%20optimization%20for%20general-purpose%0Aimage%20editing.%20Our%20implementation%20is%20available%20at%0Ahttps%3A//github.com/oyly16/LORE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03144v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLORE%253A%2520Latent%2520Optimization%2520for%2520Precise%2520Semantic%2520Control%2520in%2520Rectified%250A%2520%2520Flow-based%2520Image%2520Editing%26entry.906535625%3DLiangyang%2520Ouyang%2520and%2520Jiafeng%2520Mao%26entry.1292438233%3D%2520%2520Text-driven%2520image%2520editing%2520enables%2520users%2520to%2520flexibly%2520modify%2520visual%2520content%250Athrough%2520natural%2520language%2520instructions%252C%2520and%2520is%2520widely%2520applied%2520to%2520tasks%2520such%2520as%250Asemantic%2520object%2520replacement%252C%2520insertion%252C%2520and%2520removal.%2520While%2520recent%250Ainversion-based%2520editing%2520methods%2520using%2520rectified%2520flow%2520models%2520have%2520achieved%250Apromising%2520results%2520in%2520image%2520quality%252C%2520we%2520identify%2520a%2520structural%2520limitation%2520in%250Atheir%2520editing%2520behavior%253A%2520the%2520semantic%2520bias%2520toward%2520the%2520source%2520concept%2520encoded%2520in%250Athe%2520inverted%2520noise%2520tends%2520to%2520suppress%2520attention%2520to%2520the%2520target%2520concept.%2520This%250Aissue%2520becomes%2520particularly%2520critical%2520when%2520the%2520source%2520and%2520target%2520semantics%2520are%250Adissimilar%252C%2520where%2520the%2520attention%2520mechanism%2520inherently%2520leads%2520to%2520editing%2520failure%250Aor%2520unintended%2520modifications%2520in%2520non-target%2520regions.%2520In%2520this%2520paper%252C%2520we%250Asystematically%2520analyze%2520and%2520validate%2520this%2520structural%2520flaw%252C%2520and%2520introduce%2520LORE%252C%2520a%250Atraining-free%2520and%2520efficient%2520image%2520editing%2520method.%2520LORE%2520directly%2520optimizes%2520the%250Ainverted%2520noise%252C%2520addressing%2520the%2520core%2520limitations%2520in%2520generalization%2520and%250Acontrollability%2520of%2520existing%2520approaches%252C%2520enabling%2520stable%252C%2520controllable%252C%2520and%250Ageneral-purpose%2520concept%2520replacement%252C%2520without%2520requiring%2520architectural%250Amodification%2520or%2520model%2520fine-tuning.%2520We%2520conduct%2520comprehensive%2520evaluations%2520on%250Athree%2520challenging%2520benchmarks%253A%2520PIEBench%252C%2520SmartEdit%252C%2520and%2520GapEdit.%2520Experimental%250Aresults%2520show%2520that%2520LORE%2520significantly%2520outperforms%2520strong%2520baselines%2520in%2520terms%2520of%250Asemantic%2520alignment%252C%2520image%2520quality%252C%2520and%2520background%2520fidelity%252C%2520demonstrating%2520the%250Aeffectiveness%2520and%2520scalability%2520of%2520latent-space%2520optimization%2520for%2520general-purpose%250Aimage%2520editing.%2520Our%2520implementation%2520is%2520available%2520at%250Ahttps%253A//github.com/oyly16/LORE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03144v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LORE%3A%20Latent%20Optimization%20for%20Precise%20Semantic%20Control%20in%20Rectified%0A%20%20Flow-based%20Image%20Editing&entry.906535625=Liangyang%20Ouyang%20and%20Jiafeng%20Mao&entry.1292438233=%20%20Text-driven%20image%20editing%20enables%20users%20to%20flexibly%20modify%20visual%20content%0Athrough%20natural%20language%20instructions%2C%20and%20is%20widely%20applied%20to%20tasks%20such%20as%0Asemantic%20object%20replacement%2C%20insertion%2C%20and%20removal.%20While%20recent%0Ainversion-based%20editing%20methods%20using%20rectified%20flow%20models%20have%20achieved%0Apromising%20results%20in%20image%20quality%2C%20we%20identify%20a%20structural%20limitation%20in%0Atheir%20editing%20behavior%3A%20the%20semantic%20bias%20toward%20the%20source%20concept%20encoded%20in%0Athe%20inverted%20noise%20tends%20to%20suppress%20attention%20to%20the%20target%20concept.%20This%0Aissue%20becomes%20particularly%20critical%20when%20the%20source%20and%20target%20semantics%20are%0Adissimilar%2C%20where%20the%20attention%20mechanism%20inherently%20leads%20to%20editing%20failure%0Aor%20unintended%20modifications%20in%20non-target%20regions.%20In%20this%20paper%2C%20we%0Asystematically%20analyze%20and%20validate%20this%20structural%20flaw%2C%20and%20introduce%20LORE%2C%20a%0Atraining-free%20and%20efficient%20image%20editing%20method.%20LORE%20directly%20optimizes%20the%0Ainverted%20noise%2C%20addressing%20the%20core%20limitations%20in%20generalization%20and%0Acontrollability%20of%20existing%20approaches%2C%20enabling%20stable%2C%20controllable%2C%20and%0Ageneral-purpose%20concept%20replacement%2C%20without%20requiring%20architectural%0Amodification%20or%20model%20fine-tuning.%20We%20conduct%20comprehensive%20evaluations%20on%0Athree%20challenging%20benchmarks%3A%20PIEBench%2C%20SmartEdit%2C%20and%20GapEdit.%20Experimental%0Aresults%20show%20that%20LORE%20significantly%20outperforms%20strong%20baselines%20in%20terms%20of%0Asemantic%20alignment%2C%20image%20quality%2C%20and%20background%20fidelity%2C%20demonstrating%20the%0Aeffectiveness%20and%20scalability%20of%20latent-space%20optimization%20for%20general-purpose%0Aimage%20editing.%20Our%20implementation%20is%20available%20at%0Ahttps%3A//github.com/oyly16/LORE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03144v2&entry.124074799=Read"},
{"title": "Continual Neural Topic Model", "author": "Charu Karakkaparambil James and Waleed Mustafa and Marius Kloft and Sophie Fellenz", "abstract": "  In continual learning, our aim is to learn a new task without forgetting what\nwas learned previously. In topic models, this translates to learning new topic\nmodels without forgetting previously learned topics. Previous work either\nconsidered Dynamic Topic Models (DTMs), which learn the evolution of topics\nbased on the entire training corpus at once, or Online Topic Models, which are\nupdated continuously based on new data but do not have long-term memory. To\nfill this gap, we propose the Continual Neural Topic Model (CoNTM), which\ncontinuously learns topic models at subsequent time steps without forgetting\nwhat was previously learned. This is achieved using a global prior distribution\nthat is continuously updated. In our experiments, CoNTM consistently\noutperformed the dynamic topic model in terms of topic quality and predictive\nperplexity while being able to capture topic changes online. The analysis\nreveals that CoNTM can learn more diverse topics and better capture temporal\nchanges than existing methods.\n", "link": "http://arxiv.org/abs/2508.15612v1", "date": "2025-08-21", "relevancy": 2.3497, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4792}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4665}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Neural%20Topic%20Model&body=Title%3A%20Continual%20Neural%20Topic%20Model%0AAuthor%3A%20Charu%20Karakkaparambil%20James%20and%20Waleed%20Mustafa%20and%20Marius%20Kloft%20and%20Sophie%20Fellenz%0AAbstract%3A%20%20%20In%20continual%20learning%2C%20our%20aim%20is%20to%20learn%20a%20new%20task%20without%20forgetting%20what%0Awas%20learned%20previously.%20In%20topic%20models%2C%20this%20translates%20to%20learning%20new%20topic%0Amodels%20without%20forgetting%20previously%20learned%20topics.%20Previous%20work%20either%0Aconsidered%20Dynamic%20Topic%20Models%20%28DTMs%29%2C%20which%20learn%20the%20evolution%20of%20topics%0Abased%20on%20the%20entire%20training%20corpus%20at%20once%2C%20or%20Online%20Topic%20Models%2C%20which%20are%0Aupdated%20continuously%20based%20on%20new%20data%20but%20do%20not%20have%20long-term%20memory.%20To%0Afill%20this%20gap%2C%20we%20propose%20the%20Continual%20Neural%20Topic%20Model%20%28CoNTM%29%2C%20which%0Acontinuously%20learns%20topic%20models%20at%20subsequent%20time%20steps%20without%20forgetting%0Awhat%20was%20previously%20learned.%20This%20is%20achieved%20using%20a%20global%20prior%20distribution%0Athat%20is%20continuously%20updated.%20In%20our%20experiments%2C%20CoNTM%20consistently%0Aoutperformed%20the%20dynamic%20topic%20model%20in%20terms%20of%20topic%20quality%20and%20predictive%0Aperplexity%20while%20being%20able%20to%20capture%20topic%20changes%20online.%20The%20analysis%0Areveals%20that%20CoNTM%20can%20learn%20more%20diverse%20topics%20and%20better%20capture%20temporal%0Achanges%20than%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Neural%2520Topic%2520Model%26entry.906535625%3DCharu%2520Karakkaparambil%2520James%2520and%2520Waleed%2520Mustafa%2520and%2520Marius%2520Kloft%2520and%2520Sophie%2520Fellenz%26entry.1292438233%3D%2520%2520In%2520continual%2520learning%252C%2520our%2520aim%2520is%2520to%2520learn%2520a%2520new%2520task%2520without%2520forgetting%2520what%250Awas%2520learned%2520previously.%2520In%2520topic%2520models%252C%2520this%2520translates%2520to%2520learning%2520new%2520topic%250Amodels%2520without%2520forgetting%2520previously%2520learned%2520topics.%2520Previous%2520work%2520either%250Aconsidered%2520Dynamic%2520Topic%2520Models%2520%2528DTMs%2529%252C%2520which%2520learn%2520the%2520evolution%2520of%2520topics%250Abased%2520on%2520the%2520entire%2520training%2520corpus%2520at%2520once%252C%2520or%2520Online%2520Topic%2520Models%252C%2520which%2520are%250Aupdated%2520continuously%2520based%2520on%2520new%2520data%2520but%2520do%2520not%2520have%2520long-term%2520memory.%2520To%250Afill%2520this%2520gap%252C%2520we%2520propose%2520the%2520Continual%2520Neural%2520Topic%2520Model%2520%2528CoNTM%2529%252C%2520which%250Acontinuously%2520learns%2520topic%2520models%2520at%2520subsequent%2520time%2520steps%2520without%2520forgetting%250Awhat%2520was%2520previously%2520learned.%2520This%2520is%2520achieved%2520using%2520a%2520global%2520prior%2520distribution%250Athat%2520is%2520continuously%2520updated.%2520In%2520our%2520experiments%252C%2520CoNTM%2520consistently%250Aoutperformed%2520the%2520dynamic%2520topic%2520model%2520in%2520terms%2520of%2520topic%2520quality%2520and%2520predictive%250Aperplexity%2520while%2520being%2520able%2520to%2520capture%2520topic%2520changes%2520online.%2520The%2520analysis%250Areveals%2520that%2520CoNTM%2520can%2520learn%2520more%2520diverse%2520topics%2520and%2520better%2520capture%2520temporal%250Achanges%2520than%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Neural%20Topic%20Model&entry.906535625=Charu%20Karakkaparambil%20James%20and%20Waleed%20Mustafa%20and%20Marius%20Kloft%20and%20Sophie%20Fellenz&entry.1292438233=%20%20In%20continual%20learning%2C%20our%20aim%20is%20to%20learn%20a%20new%20task%20without%20forgetting%20what%0Awas%20learned%20previously.%20In%20topic%20models%2C%20this%20translates%20to%20learning%20new%20topic%0Amodels%20without%20forgetting%20previously%20learned%20topics.%20Previous%20work%20either%0Aconsidered%20Dynamic%20Topic%20Models%20%28DTMs%29%2C%20which%20learn%20the%20evolution%20of%20topics%0Abased%20on%20the%20entire%20training%20corpus%20at%20once%2C%20or%20Online%20Topic%20Models%2C%20which%20are%0Aupdated%20continuously%20based%20on%20new%20data%20but%20do%20not%20have%20long-term%20memory.%20To%0Afill%20this%20gap%2C%20we%20propose%20the%20Continual%20Neural%20Topic%20Model%20%28CoNTM%29%2C%20which%0Acontinuously%20learns%20topic%20models%20at%20subsequent%20time%20steps%20without%20forgetting%0Awhat%20was%20previously%20learned.%20This%20is%20achieved%20using%20a%20global%20prior%20distribution%0Athat%20is%20continuously%20updated.%20In%20our%20experiments%2C%20CoNTM%20consistently%0Aoutperformed%20the%20dynamic%20topic%20model%20in%20terms%20of%20topic%20quality%20and%20predictive%0Aperplexity%20while%20being%20able%20to%20capture%20topic%20changes%20online.%20The%20analysis%0Areveals%20that%20CoNTM%20can%20learn%20more%20diverse%20topics%20and%20better%20capture%20temporal%0Achanges%20than%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15612v1&entry.124074799=Read"},
{"title": "Continual Neural Topic Model", "author": "Charu Karakkaparambil James and Waleed Mustafa and Marius Kloft and Sophie Fellenz", "abstract": "  In continual learning, our aim is to learn a new task without forgetting what\nwas learned previously. In topic models, this translates to learning new topic\nmodels without forgetting previously learned topics. Previous work either\nconsidered Dynamic Topic Models (DTMs), which learn the evolution of topics\nbased on the entire training corpus at once, or Online Topic Models, which are\nupdated continuously based on new data but do not have long-term memory. To\nfill this gap, we propose the Continual Neural Topic Model (CoNTM), which\ncontinuously learns topic models at subsequent time steps without forgetting\nwhat was previously learned. This is achieved using a global prior distribution\nthat is continuously updated. In our experiments, CoNTM consistently\noutperformed the dynamic topic model in terms of topic quality and predictive\nperplexity while being able to capture topic changes online. The analysis\nreveals that CoNTM can learn more diverse topics and better capture temporal\nchanges than existing methods.\n", "link": "http://arxiv.org/abs/2508.15612v1", "date": "2025-08-21", "relevancy": 2.3497, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4792}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4665}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Neural%20Topic%20Model&body=Title%3A%20Continual%20Neural%20Topic%20Model%0AAuthor%3A%20Charu%20Karakkaparambil%20James%20and%20Waleed%20Mustafa%20and%20Marius%20Kloft%20and%20Sophie%20Fellenz%0AAbstract%3A%20%20%20In%20continual%20learning%2C%20our%20aim%20is%20to%20learn%20a%20new%20task%20without%20forgetting%20what%0Awas%20learned%20previously.%20In%20topic%20models%2C%20this%20translates%20to%20learning%20new%20topic%0Amodels%20without%20forgetting%20previously%20learned%20topics.%20Previous%20work%20either%0Aconsidered%20Dynamic%20Topic%20Models%20%28DTMs%29%2C%20which%20learn%20the%20evolution%20of%20topics%0Abased%20on%20the%20entire%20training%20corpus%20at%20once%2C%20or%20Online%20Topic%20Models%2C%20which%20are%0Aupdated%20continuously%20based%20on%20new%20data%20but%20do%20not%20have%20long-term%20memory.%20To%0Afill%20this%20gap%2C%20we%20propose%20the%20Continual%20Neural%20Topic%20Model%20%28CoNTM%29%2C%20which%0Acontinuously%20learns%20topic%20models%20at%20subsequent%20time%20steps%20without%20forgetting%0Awhat%20was%20previously%20learned.%20This%20is%20achieved%20using%20a%20global%20prior%20distribution%0Athat%20is%20continuously%20updated.%20In%20our%20experiments%2C%20CoNTM%20consistently%0Aoutperformed%20the%20dynamic%20topic%20model%20in%20terms%20of%20topic%20quality%20and%20predictive%0Aperplexity%20while%20being%20able%20to%20capture%20topic%20changes%20online.%20The%20analysis%0Areveals%20that%20CoNTM%20can%20learn%20more%20diverse%20topics%20and%20better%20capture%20temporal%0Achanges%20than%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Neural%2520Topic%2520Model%26entry.906535625%3DCharu%2520Karakkaparambil%2520James%2520and%2520Waleed%2520Mustafa%2520and%2520Marius%2520Kloft%2520and%2520Sophie%2520Fellenz%26entry.1292438233%3D%2520%2520In%2520continual%2520learning%252C%2520our%2520aim%2520is%2520to%2520learn%2520a%2520new%2520task%2520without%2520forgetting%2520what%250Awas%2520learned%2520previously.%2520In%2520topic%2520models%252C%2520this%2520translates%2520to%2520learning%2520new%2520topic%250Amodels%2520without%2520forgetting%2520previously%2520learned%2520topics.%2520Previous%2520work%2520either%250Aconsidered%2520Dynamic%2520Topic%2520Models%2520%2528DTMs%2529%252C%2520which%2520learn%2520the%2520evolution%2520of%2520topics%250Abased%2520on%2520the%2520entire%2520training%2520corpus%2520at%2520once%252C%2520or%2520Online%2520Topic%2520Models%252C%2520which%2520are%250Aupdated%2520continuously%2520based%2520on%2520new%2520data%2520but%2520do%2520not%2520have%2520long-term%2520memory.%2520To%250Afill%2520this%2520gap%252C%2520we%2520propose%2520the%2520Continual%2520Neural%2520Topic%2520Model%2520%2528CoNTM%2529%252C%2520which%250Acontinuously%2520learns%2520topic%2520models%2520at%2520subsequent%2520time%2520steps%2520without%2520forgetting%250Awhat%2520was%2520previously%2520learned.%2520This%2520is%2520achieved%2520using%2520a%2520global%2520prior%2520distribution%250Athat%2520is%2520continuously%2520updated.%2520In%2520our%2520experiments%252C%2520CoNTM%2520consistently%250Aoutperformed%2520the%2520dynamic%2520topic%2520model%2520in%2520terms%2520of%2520topic%2520quality%2520and%2520predictive%250Aperplexity%2520while%2520being%2520able%2520to%2520capture%2520topic%2520changes%2520online.%2520The%2520analysis%250Areveals%2520that%2520CoNTM%2520can%2520learn%2520more%2520diverse%2520topics%2520and%2520better%2520capture%2520temporal%250Achanges%2520than%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Neural%20Topic%20Model&entry.906535625=Charu%20Karakkaparambil%20James%20and%20Waleed%20Mustafa%20and%20Marius%20Kloft%20and%20Sophie%20Fellenz&entry.1292438233=%20%20In%20continual%20learning%2C%20our%20aim%20is%20to%20learn%20a%20new%20task%20without%20forgetting%20what%0Awas%20learned%20previously.%20In%20topic%20models%2C%20this%20translates%20to%20learning%20new%20topic%0Amodels%20without%20forgetting%20previously%20learned%20topics.%20Previous%20work%20either%0Aconsidered%20Dynamic%20Topic%20Models%20%28DTMs%29%2C%20which%20learn%20the%20evolution%20of%20topics%0Abased%20on%20the%20entire%20training%20corpus%20at%20once%2C%20or%20Online%20Topic%20Models%2C%20which%20are%0Aupdated%20continuously%20based%20on%20new%20data%20but%20do%20not%20have%20long-term%20memory.%20To%0Afill%20this%20gap%2C%20we%20propose%20the%20Continual%20Neural%20Topic%20Model%20%28CoNTM%29%2C%20which%0Acontinuously%20learns%20topic%20models%20at%20subsequent%20time%20steps%20without%20forgetting%0Awhat%20was%20previously%20learned.%20This%20is%20achieved%20using%20a%20global%20prior%20distribution%0Athat%20is%20continuously%20updated.%20In%20our%20experiments%2C%20CoNTM%20consistently%0Aoutperformed%20the%20dynamic%20topic%20model%20in%20terms%20of%20topic%20quality%20and%20predictive%0Aperplexity%20while%20being%20able%20to%20capture%20topic%20changes%20online.%20The%20analysis%0Areveals%20that%20CoNTM%20can%20learn%20more%20diverse%20topics%20and%20better%20capture%20temporal%0Achanges%20than%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15612v1&entry.124074799=Read"},
{"title": "On the Effectiveness of Graph Reordering for Accelerating Approximate\n  Nearest Neighbor Search on GPU", "author": "Yutaro Oguri and Mai Nishimura and Yusuke Matsui", "abstract": "  We present the first systematic investigation of graph reordering effects for\ngraph-based Approximate Nearest Neighbor Search (ANNS) on a GPU. While\ngraph-based ANNS has become the dominant paradigm for modern AI applications,\nrecent approaches focus on algorithmic innovations while neglecting memory\nlayout considerations that significantly affect execution time. Our unified\nevaluation framework enables comprehensive evaluation of diverse reordering\nstrategies across different graph indices through a graph adapter that converts\narbitrary graph topologies into a common representation and a GPU-optimized\ngraph traversal engine. We conduct a comprehensive analysis across diverse\ndatasets and state-of-the-art graph indices, introducing analysis metrics that\nquantify the relationship between structural properties and memory layout\neffectiveness. Our GPU-targeted reordering achieves up to 15$\\%$ QPS\nimprovements while preserving search accuracy, demonstrating that memory layout\noptimization operates orthogonally to existing algorithmic innovations. We will\nrelease all code upon publication to facilitate reproducibility and foster\nfurther research.\n", "link": "http://arxiv.org/abs/2508.15436v1", "date": "2025-08-21", "relevancy": 2.3436, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4951}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4712}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Effectiveness%20of%20Graph%20Reordering%20for%20Accelerating%20Approximate%0A%20%20Nearest%20Neighbor%20Search%20on%20GPU&body=Title%3A%20On%20the%20Effectiveness%20of%20Graph%20Reordering%20for%20Accelerating%20Approximate%0A%20%20Nearest%20Neighbor%20Search%20on%20GPU%0AAuthor%3A%20Yutaro%20Oguri%20and%20Mai%20Nishimura%20and%20Yusuke%20Matsui%0AAbstract%3A%20%20%20We%20present%20the%20first%20systematic%20investigation%20of%20graph%20reordering%20effects%20for%0Agraph-based%20Approximate%20Nearest%20Neighbor%20Search%20%28ANNS%29%20on%20a%20GPU.%20While%0Agraph-based%20ANNS%20has%20become%20the%20dominant%20paradigm%20for%20modern%20AI%20applications%2C%0Arecent%20approaches%20focus%20on%20algorithmic%20innovations%20while%20neglecting%20memory%0Alayout%20considerations%20that%20significantly%20affect%20execution%20time.%20Our%20unified%0Aevaluation%20framework%20enables%20comprehensive%20evaluation%20of%20diverse%20reordering%0Astrategies%20across%20different%20graph%20indices%20through%20a%20graph%20adapter%20that%20converts%0Aarbitrary%20graph%20topologies%20into%20a%20common%20representation%20and%20a%20GPU-optimized%0Agraph%20traversal%20engine.%20We%20conduct%20a%20comprehensive%20analysis%20across%20diverse%0Adatasets%20and%20state-of-the-art%20graph%20indices%2C%20introducing%20analysis%20metrics%20that%0Aquantify%20the%20relationship%20between%20structural%20properties%20and%20memory%20layout%0Aeffectiveness.%20Our%20GPU-targeted%20reordering%20achieves%20up%20to%2015%24%5C%25%24%20QPS%0Aimprovements%20while%20preserving%20search%20accuracy%2C%20demonstrating%20that%20memory%20layout%0Aoptimization%20operates%20orthogonally%20to%20existing%20algorithmic%20innovations.%20We%20will%0Arelease%20all%20code%20upon%20publication%20to%20facilitate%20reproducibility%20and%20foster%0Afurther%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Effectiveness%2520of%2520Graph%2520Reordering%2520for%2520Accelerating%2520Approximate%250A%2520%2520Nearest%2520Neighbor%2520Search%2520on%2520GPU%26entry.906535625%3DYutaro%2520Oguri%2520and%2520Mai%2520Nishimura%2520and%2520Yusuke%2520Matsui%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520first%2520systematic%2520investigation%2520of%2520graph%2520reordering%2520effects%2520for%250Agraph-based%2520Approximate%2520Nearest%2520Neighbor%2520Search%2520%2528ANNS%2529%2520on%2520a%2520GPU.%2520While%250Agraph-based%2520ANNS%2520has%2520become%2520the%2520dominant%2520paradigm%2520for%2520modern%2520AI%2520applications%252C%250Arecent%2520approaches%2520focus%2520on%2520algorithmic%2520innovations%2520while%2520neglecting%2520memory%250Alayout%2520considerations%2520that%2520significantly%2520affect%2520execution%2520time.%2520Our%2520unified%250Aevaluation%2520framework%2520enables%2520comprehensive%2520evaluation%2520of%2520diverse%2520reordering%250Astrategies%2520across%2520different%2520graph%2520indices%2520through%2520a%2520graph%2520adapter%2520that%2520converts%250Aarbitrary%2520graph%2520topologies%2520into%2520a%2520common%2520representation%2520and%2520a%2520GPU-optimized%250Agraph%2520traversal%2520engine.%2520We%2520conduct%2520a%2520comprehensive%2520analysis%2520across%2520diverse%250Adatasets%2520and%2520state-of-the-art%2520graph%2520indices%252C%2520introducing%2520analysis%2520metrics%2520that%250Aquantify%2520the%2520relationship%2520between%2520structural%2520properties%2520and%2520memory%2520layout%250Aeffectiveness.%2520Our%2520GPU-targeted%2520reordering%2520achieves%2520up%2520to%252015%2524%255C%2525%2524%2520QPS%250Aimprovements%2520while%2520preserving%2520search%2520accuracy%252C%2520demonstrating%2520that%2520memory%2520layout%250Aoptimization%2520operates%2520orthogonally%2520to%2520existing%2520algorithmic%2520innovations.%2520We%2520will%250Arelease%2520all%2520code%2520upon%2520publication%2520to%2520facilitate%2520reproducibility%2520and%2520foster%250Afurther%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Effectiveness%20of%20Graph%20Reordering%20for%20Accelerating%20Approximate%0A%20%20Nearest%20Neighbor%20Search%20on%20GPU&entry.906535625=Yutaro%20Oguri%20and%20Mai%20Nishimura%20and%20Yusuke%20Matsui&entry.1292438233=%20%20We%20present%20the%20first%20systematic%20investigation%20of%20graph%20reordering%20effects%20for%0Agraph-based%20Approximate%20Nearest%20Neighbor%20Search%20%28ANNS%29%20on%20a%20GPU.%20While%0Agraph-based%20ANNS%20has%20become%20the%20dominant%20paradigm%20for%20modern%20AI%20applications%2C%0Arecent%20approaches%20focus%20on%20algorithmic%20innovations%20while%20neglecting%20memory%0Alayout%20considerations%20that%20significantly%20affect%20execution%20time.%20Our%20unified%0Aevaluation%20framework%20enables%20comprehensive%20evaluation%20of%20diverse%20reordering%0Astrategies%20across%20different%20graph%20indices%20through%20a%20graph%20adapter%20that%20converts%0Aarbitrary%20graph%20topologies%20into%20a%20common%20representation%20and%20a%20GPU-optimized%0Agraph%20traversal%20engine.%20We%20conduct%20a%20comprehensive%20analysis%20across%20diverse%0Adatasets%20and%20state-of-the-art%20graph%20indices%2C%20introducing%20analysis%20metrics%20that%0Aquantify%20the%20relationship%20between%20structural%20properties%20and%20memory%20layout%0Aeffectiveness.%20Our%20GPU-targeted%20reordering%20achieves%20up%20to%2015%24%5C%25%24%20QPS%0Aimprovements%20while%20preserving%20search%20accuracy%2C%20demonstrating%20that%20memory%20layout%0Aoptimization%20operates%20orthogonally%20to%20existing%20algorithmic%20innovations.%20We%20will%0Arelease%20all%20code%20upon%20publication%20to%20facilitate%20reproducibility%20and%20foster%0Afurther%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15436v1&entry.124074799=Read"},
{"title": "Hessian-based lightweight neural network for brain vessel segmentation\n  on a minimal training dataset", "author": "Alexandra Bernadotte and Elfimov Nikita and Mikhail Shutov and Ivan Menshikov", "abstract": "  Accurate segmentation of blood vessels in brain magnetic resonance\nangiography (MRA) is essential for successful surgical procedures, such as\naneurysm repair or bypass surgery. Currently, annotation is primarily performed\nthrough manual segmentation or classical methods, such as the Frangi filter,\nwhich often lack sufficient accuracy. Neural networks have emerged as powerful\ntools for medical image segmentation, but their development depends on\nwell-annotated training datasets. However, there is a notable lack of publicly\navailable MRA datasets with detailed brain vessel annotations.\n  To address this gap, we propose a novel semi-supervised learning lightweight\nneural network with Hessian matrices on board for 3D segmentation of complex\nstructures such as tubular structures, which we named HessNet. The solution is\na Hessian-based neural network with only 6000 parameters. HessNet can run on\nthe CPU and significantly reduces the resource requirements for training neural\nnetworks. The accuracy of vessel segmentation on a minimal training dataset\nreaches state-of-the-art results. It helps us create a large, semi-manually\nannotated brain vessel dataset of brain MRA images based on the IXI dataset\n(annotated 200 images). Annotation was performed by three experts under the\nsupervision of three neurovascular surgeons after applying HessNet. It provides\nhigh accuracy of vessel segmentation and allows experts to focus only on the\nmost complex important cases. The dataset is available at\nhttps://git.scinalytics.com/terilat/VesselDatasetPartly.\n", "link": "http://arxiv.org/abs/2508.15660v1", "date": "2025-08-21", "relevancy": 2.3186, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4678}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4626}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hessian-based%20lightweight%20neural%20network%20for%20brain%20vessel%20segmentation%0A%20%20on%20a%20minimal%20training%20dataset&body=Title%3A%20Hessian-based%20lightweight%20neural%20network%20for%20brain%20vessel%20segmentation%0A%20%20on%20a%20minimal%20training%20dataset%0AAuthor%3A%20Alexandra%20Bernadotte%20and%20Elfimov%20Nikita%20and%20Mikhail%20Shutov%20and%20Ivan%20Menshikov%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20blood%20vessels%20in%20brain%20magnetic%20resonance%0Aangiography%20%28MRA%29%20is%20essential%20for%20successful%20surgical%20procedures%2C%20such%20as%0Aaneurysm%20repair%20or%20bypass%20surgery.%20Currently%2C%20annotation%20is%20primarily%20performed%0Athrough%20manual%20segmentation%20or%20classical%20methods%2C%20such%20as%20the%20Frangi%20filter%2C%0Awhich%20often%20lack%20sufficient%20accuracy.%20Neural%20networks%20have%20emerged%20as%20powerful%0Atools%20for%20medical%20image%20segmentation%2C%20but%20their%20development%20depends%20on%0Awell-annotated%20training%20datasets.%20However%2C%20there%20is%20a%20notable%20lack%20of%20publicly%0Aavailable%20MRA%20datasets%20with%20detailed%20brain%20vessel%20annotations.%0A%20%20To%20address%20this%20gap%2C%20we%20propose%20a%20novel%20semi-supervised%20learning%20lightweight%0Aneural%20network%20with%20Hessian%20matrices%20on%20board%20for%203D%20segmentation%20of%20complex%0Astructures%20such%20as%20tubular%20structures%2C%20which%20we%20named%20HessNet.%20The%20solution%20is%0Aa%20Hessian-based%20neural%20network%20with%20only%206000%20parameters.%20HessNet%20can%20run%20on%0Athe%20CPU%20and%20significantly%20reduces%20the%20resource%20requirements%20for%20training%20neural%0Anetworks.%20The%20accuracy%20of%20vessel%20segmentation%20on%20a%20minimal%20training%20dataset%0Areaches%20state-of-the-art%20results.%20It%20helps%20us%20create%20a%20large%2C%20semi-manually%0Aannotated%20brain%20vessel%20dataset%20of%20brain%20MRA%20images%20based%20on%20the%20IXI%20dataset%0A%28annotated%20200%20images%29.%20Annotation%20was%20performed%20by%20three%20experts%20under%20the%0Asupervision%20of%20three%20neurovascular%20surgeons%20after%20applying%20HessNet.%20It%20provides%0Ahigh%20accuracy%20of%20vessel%20segmentation%20and%20allows%20experts%20to%20focus%20only%20on%20the%0Amost%20complex%20important%20cases.%20The%20dataset%20is%20available%20at%0Ahttps%3A//git.scinalytics.com/terilat/VesselDatasetPartly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHessian-based%2520lightweight%2520neural%2520network%2520for%2520brain%2520vessel%2520segmentation%250A%2520%2520on%2520a%2520minimal%2520training%2520dataset%26entry.906535625%3DAlexandra%2520Bernadotte%2520and%2520Elfimov%2520Nikita%2520and%2520Mikhail%2520Shutov%2520and%2520Ivan%2520Menshikov%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520blood%2520vessels%2520in%2520brain%2520magnetic%2520resonance%250Aangiography%2520%2528MRA%2529%2520is%2520essential%2520for%2520successful%2520surgical%2520procedures%252C%2520such%2520as%250Aaneurysm%2520repair%2520or%2520bypass%2520surgery.%2520Currently%252C%2520annotation%2520is%2520primarily%2520performed%250Athrough%2520manual%2520segmentation%2520or%2520classical%2520methods%252C%2520such%2520as%2520the%2520Frangi%2520filter%252C%250Awhich%2520often%2520lack%2520sufficient%2520accuracy.%2520Neural%2520networks%2520have%2520emerged%2520as%2520powerful%250Atools%2520for%2520medical%2520image%2520segmentation%252C%2520but%2520their%2520development%2520depends%2520on%250Awell-annotated%2520training%2520datasets.%2520However%252C%2520there%2520is%2520a%2520notable%2520lack%2520of%2520publicly%250Aavailable%2520MRA%2520datasets%2520with%2520detailed%2520brain%2520vessel%2520annotations.%250A%2520%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520a%2520novel%2520semi-supervised%2520learning%2520lightweight%250Aneural%2520network%2520with%2520Hessian%2520matrices%2520on%2520board%2520for%25203D%2520segmentation%2520of%2520complex%250Astructures%2520such%2520as%2520tubular%2520structures%252C%2520which%2520we%2520named%2520HessNet.%2520The%2520solution%2520is%250Aa%2520Hessian-based%2520neural%2520network%2520with%2520only%25206000%2520parameters.%2520HessNet%2520can%2520run%2520on%250Athe%2520CPU%2520and%2520significantly%2520reduces%2520the%2520resource%2520requirements%2520for%2520training%2520neural%250Anetworks.%2520The%2520accuracy%2520of%2520vessel%2520segmentation%2520on%2520a%2520minimal%2520training%2520dataset%250Areaches%2520state-of-the-art%2520results.%2520It%2520helps%2520us%2520create%2520a%2520large%252C%2520semi-manually%250Aannotated%2520brain%2520vessel%2520dataset%2520of%2520brain%2520MRA%2520images%2520based%2520on%2520the%2520IXI%2520dataset%250A%2528annotated%2520200%2520images%2529.%2520Annotation%2520was%2520performed%2520by%2520three%2520experts%2520under%2520the%250Asupervision%2520of%2520three%2520neurovascular%2520surgeons%2520after%2520applying%2520HessNet.%2520It%2520provides%250Ahigh%2520accuracy%2520of%2520vessel%2520segmentation%2520and%2520allows%2520experts%2520to%2520focus%2520only%2520on%2520the%250Amost%2520complex%2520important%2520cases.%2520The%2520dataset%2520is%2520available%2520at%250Ahttps%253A//git.scinalytics.com/terilat/VesselDatasetPartly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hessian-based%20lightweight%20neural%20network%20for%20brain%20vessel%20segmentation%0A%20%20on%20a%20minimal%20training%20dataset&entry.906535625=Alexandra%20Bernadotte%20and%20Elfimov%20Nikita%20and%20Mikhail%20Shutov%20and%20Ivan%20Menshikov&entry.1292438233=%20%20Accurate%20segmentation%20of%20blood%20vessels%20in%20brain%20magnetic%20resonance%0Aangiography%20%28MRA%29%20is%20essential%20for%20successful%20surgical%20procedures%2C%20such%20as%0Aaneurysm%20repair%20or%20bypass%20surgery.%20Currently%2C%20annotation%20is%20primarily%20performed%0Athrough%20manual%20segmentation%20or%20classical%20methods%2C%20such%20as%20the%20Frangi%20filter%2C%0Awhich%20often%20lack%20sufficient%20accuracy.%20Neural%20networks%20have%20emerged%20as%20powerful%0Atools%20for%20medical%20image%20segmentation%2C%20but%20their%20development%20depends%20on%0Awell-annotated%20training%20datasets.%20However%2C%20there%20is%20a%20notable%20lack%20of%20publicly%0Aavailable%20MRA%20datasets%20with%20detailed%20brain%20vessel%20annotations.%0A%20%20To%20address%20this%20gap%2C%20we%20propose%20a%20novel%20semi-supervised%20learning%20lightweight%0Aneural%20network%20with%20Hessian%20matrices%20on%20board%20for%203D%20segmentation%20of%20complex%0Astructures%20such%20as%20tubular%20structures%2C%20which%20we%20named%20HessNet.%20The%20solution%20is%0Aa%20Hessian-based%20neural%20network%20with%20only%206000%20parameters.%20HessNet%20can%20run%20on%0Athe%20CPU%20and%20significantly%20reduces%20the%20resource%20requirements%20for%20training%20neural%0Anetworks.%20The%20accuracy%20of%20vessel%20segmentation%20on%20a%20minimal%20training%20dataset%0Areaches%20state-of-the-art%20results.%20It%20helps%20us%20create%20a%20large%2C%20semi-manually%0Aannotated%20brain%20vessel%20dataset%20of%20brain%20MRA%20images%20based%20on%20the%20IXI%20dataset%0A%28annotated%20200%20images%29.%20Annotation%20was%20performed%20by%20three%20experts%20under%20the%0Asupervision%20of%20three%20neurovascular%20surgeons%20after%20applying%20HessNet.%20It%20provides%0Ahigh%20accuracy%20of%20vessel%20segmentation%20and%20allows%20experts%20to%20focus%20only%20on%20the%0Amost%20complex%20important%20cases.%20The%20dataset%20is%20available%20at%0Ahttps%3A//git.scinalytics.com/terilat/VesselDatasetPartly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15660v1&entry.124074799=Read"},
{"title": "Foundation Models for Cross-Domain EEG Analysis Application: A Survey", "author": "Hongqi Li and Yitong Chen and Yujuan Wang and Weihang Ni and Haodong Zhang", "abstract": "  Electroencephalography (EEG) analysis stands at the forefront of neuroscience\nand artificial intelligence research, where foundation models are reshaping the\ntraditional EEG analysis paradigm by leveraging their powerful representational\ncapacity and cross-modal generalization. However, the rapid proliferation of\nthese techniques has led to a fragmented research landscape, characterized by\ndiverse model roles, inconsistent architectures, and a lack of systematic\ncategorization. To bridge this gap, this study presents the first comprehensive\nmodality-oriented taxonomy for foundation models in EEG analysis,\nsystematically organizing research advances based on output modalities of the\nnative EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodal\nframeworks. We rigorously analyze each category's research ideas, theoretical\nfoundations, and architectural innovations, while highlighting open challenges\nsuch as model interpretability, cross-domain generalization, and real-world\napplicability in EEG-based systems. By unifying this dispersed field, our work\nnot only provides a reference framework for future methodology development but\naccelerates the translation of EEG foundation models into scalable,\ninterpretable, and online actionable solutions.\n", "link": "http://arxiv.org/abs/2508.15716v1", "date": "2025-08-21", "relevancy": 2.3074, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5938}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5938}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Models%20for%20Cross-Domain%20EEG%20Analysis%20Application%3A%20A%20Survey&body=Title%3A%20Foundation%20Models%20for%20Cross-Domain%20EEG%20Analysis%20Application%3A%20A%20Survey%0AAuthor%3A%20Hongqi%20Li%20and%20Yitong%20Chen%20and%20Yujuan%20Wang%20and%20Weihang%20Ni%20and%20Haodong%20Zhang%0AAbstract%3A%20%20%20Electroencephalography%20%28EEG%29%20analysis%20stands%20at%20the%20forefront%20of%20neuroscience%0Aand%20artificial%20intelligence%20research%2C%20where%20foundation%20models%20are%20reshaping%20the%0Atraditional%20EEG%20analysis%20paradigm%20by%20leveraging%20their%20powerful%20representational%0Acapacity%20and%20cross-modal%20generalization.%20However%2C%20the%20rapid%20proliferation%20of%0Athese%20techniques%20has%20led%20to%20a%20fragmented%20research%20landscape%2C%20characterized%20by%0Adiverse%20model%20roles%2C%20inconsistent%20architectures%2C%20and%20a%20lack%20of%20systematic%0Acategorization.%20To%20bridge%20this%20gap%2C%20this%20study%20presents%20the%20first%20comprehensive%0Amodality-oriented%20taxonomy%20for%20foundation%20models%20in%20EEG%20analysis%2C%0Asystematically%20organizing%20research%20advances%20based%20on%20output%20modalities%20of%20the%0Anative%20EEG%20decoding%2C%20EEG-text%2C%20EEG-vision%2C%20EEG-audio%2C%20and%20broader%20multimodal%0Aframeworks.%20We%20rigorously%20analyze%20each%20category%27s%20research%20ideas%2C%20theoretical%0Afoundations%2C%20and%20architectural%20innovations%2C%20while%20highlighting%20open%20challenges%0Asuch%20as%20model%20interpretability%2C%20cross-domain%20generalization%2C%20and%20real-world%0Aapplicability%20in%20EEG-based%20systems.%20By%20unifying%20this%20dispersed%20field%2C%20our%20work%0Anot%20only%20provides%20a%20reference%20framework%20for%20future%20methodology%20development%20but%0Aaccelerates%20the%20translation%20of%20EEG%20foundation%20models%20into%20scalable%2C%0Ainterpretable%2C%20and%20online%20actionable%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Models%2520for%2520Cross-Domain%2520EEG%2520Analysis%2520Application%253A%2520A%2520Survey%26entry.906535625%3DHongqi%2520Li%2520and%2520Yitong%2520Chen%2520and%2520Yujuan%2520Wang%2520and%2520Weihang%2520Ni%2520and%2520Haodong%2520Zhang%26entry.1292438233%3D%2520%2520Electroencephalography%2520%2528EEG%2529%2520analysis%2520stands%2520at%2520the%2520forefront%2520of%2520neuroscience%250Aand%2520artificial%2520intelligence%2520research%252C%2520where%2520foundation%2520models%2520are%2520reshaping%2520the%250Atraditional%2520EEG%2520analysis%2520paradigm%2520by%2520leveraging%2520their%2520powerful%2520representational%250Acapacity%2520and%2520cross-modal%2520generalization.%2520However%252C%2520the%2520rapid%2520proliferation%2520of%250Athese%2520techniques%2520has%2520led%2520to%2520a%2520fragmented%2520research%2520landscape%252C%2520characterized%2520by%250Adiverse%2520model%2520roles%252C%2520inconsistent%2520architectures%252C%2520and%2520a%2520lack%2520of%2520systematic%250Acategorization.%2520To%2520bridge%2520this%2520gap%252C%2520this%2520study%2520presents%2520the%2520first%2520comprehensive%250Amodality-oriented%2520taxonomy%2520for%2520foundation%2520models%2520in%2520EEG%2520analysis%252C%250Asystematically%2520organizing%2520research%2520advances%2520based%2520on%2520output%2520modalities%2520of%2520the%250Anative%2520EEG%2520decoding%252C%2520EEG-text%252C%2520EEG-vision%252C%2520EEG-audio%252C%2520and%2520broader%2520multimodal%250Aframeworks.%2520We%2520rigorously%2520analyze%2520each%2520category%2527s%2520research%2520ideas%252C%2520theoretical%250Afoundations%252C%2520and%2520architectural%2520innovations%252C%2520while%2520highlighting%2520open%2520challenges%250Asuch%2520as%2520model%2520interpretability%252C%2520cross-domain%2520generalization%252C%2520and%2520real-world%250Aapplicability%2520in%2520EEG-based%2520systems.%2520By%2520unifying%2520this%2520dispersed%2520field%252C%2520our%2520work%250Anot%2520only%2520provides%2520a%2520reference%2520framework%2520for%2520future%2520methodology%2520development%2520but%250Aaccelerates%2520the%2520translation%2520of%2520EEG%2520foundation%2520models%2520into%2520scalable%252C%250Ainterpretable%252C%2520and%2520online%2520actionable%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Models%20for%20Cross-Domain%20EEG%20Analysis%20Application%3A%20A%20Survey&entry.906535625=Hongqi%20Li%20and%20Yitong%20Chen%20and%20Yujuan%20Wang%20and%20Weihang%20Ni%20and%20Haodong%20Zhang&entry.1292438233=%20%20Electroencephalography%20%28EEG%29%20analysis%20stands%20at%20the%20forefront%20of%20neuroscience%0Aand%20artificial%20intelligence%20research%2C%20where%20foundation%20models%20are%20reshaping%20the%0Atraditional%20EEG%20analysis%20paradigm%20by%20leveraging%20their%20powerful%20representational%0Acapacity%20and%20cross-modal%20generalization.%20However%2C%20the%20rapid%20proliferation%20of%0Athese%20techniques%20has%20led%20to%20a%20fragmented%20research%20landscape%2C%20characterized%20by%0Adiverse%20model%20roles%2C%20inconsistent%20architectures%2C%20and%20a%20lack%20of%20systematic%0Acategorization.%20To%20bridge%20this%20gap%2C%20this%20study%20presents%20the%20first%20comprehensive%0Amodality-oriented%20taxonomy%20for%20foundation%20models%20in%20EEG%20analysis%2C%0Asystematically%20organizing%20research%20advances%20based%20on%20output%20modalities%20of%20the%0Anative%20EEG%20decoding%2C%20EEG-text%2C%20EEG-vision%2C%20EEG-audio%2C%20and%20broader%20multimodal%0Aframeworks.%20We%20rigorously%20analyze%20each%20category%27s%20research%20ideas%2C%20theoretical%0Afoundations%2C%20and%20architectural%20innovations%2C%20while%20highlighting%20open%20challenges%0Asuch%20as%20model%20interpretability%2C%20cross-domain%20generalization%2C%20and%20real-world%0Aapplicability%20in%20EEG-based%20systems.%20By%20unifying%20this%20dispersed%20field%2C%20our%20work%0Anot%20only%20provides%20a%20reference%20framework%20for%20future%20methodology%20development%20but%0Aaccelerates%20the%20translation%20of%20EEG%20foundation%20models%20into%20scalable%2C%0Ainterpretable%2C%20and%20online%20actionable%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15716v1&entry.124074799=Read"},
{"title": "The Complexity Dynamics of Grokking", "author": "Branton DeMoss and Silvia Sapora and Jakob Foerster and Nick Hawes and Ingmar Posner", "abstract": "  We demonstrate the existence of a complexity phase transition in neural\nnetworks by studying the grokking phenomenon, where networks suddenly\ntransition from memorization to generalization long after overfitting their\ntraining data. To characterize this phase transition, we introduce a\ntheoretical framework for measuring complexity based on rate-distortion theory\nand Kolmogorov complexity, which can be understood as principled lossy\ncompression for networks. We find that properly regularized networks exhibit a\nsharp phase transition: complexity rises during memorization, then falls as the\nnetwork discovers a simpler underlying pattern that generalizes. In contrast,\nunregularized networks remain trapped in a high-complexity memorization phase.\nWe establish an explicit connection between our complexity measure and\ngeneralization bounds, providing a theoretical foundation for the link between\nlossy compression and generalization. Our framework achieves compression ratios\n30-40x better than na\\\"ive approaches, enabling precise tracking of complexity\ndynamics. Finally, we introduce a regularization method based on spectral\nentropy that encourages networks toward low-complexity representations by\npenalizing their intrinsic dimension.\n", "link": "http://arxiv.org/abs/2412.09810v2", "date": "2025-08-21", "relevancy": 2.3035, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4786}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Complexity%20Dynamics%20of%20Grokking&body=Title%3A%20The%20Complexity%20Dynamics%20of%20Grokking%0AAuthor%3A%20Branton%20DeMoss%20and%20Silvia%20Sapora%20and%20Jakob%20Foerster%20and%20Nick%20Hawes%20and%20Ingmar%20Posner%0AAbstract%3A%20%20%20We%20demonstrate%20the%20existence%20of%20a%20complexity%20phase%20transition%20in%20neural%0Anetworks%20by%20studying%20the%20grokking%20phenomenon%2C%20where%20networks%20suddenly%0Atransition%20from%20memorization%20to%20generalization%20long%20after%20overfitting%20their%0Atraining%20data.%20To%20characterize%20this%20phase%20transition%2C%20we%20introduce%20a%0Atheoretical%20framework%20for%20measuring%20complexity%20based%20on%20rate-distortion%20theory%0Aand%20Kolmogorov%20complexity%2C%20which%20can%20be%20understood%20as%20principled%20lossy%0Acompression%20for%20networks.%20We%20find%20that%20properly%20regularized%20networks%20exhibit%20a%0Asharp%20phase%20transition%3A%20complexity%20rises%20during%20memorization%2C%20then%20falls%20as%20the%0Anetwork%20discovers%20a%20simpler%20underlying%20pattern%20that%20generalizes.%20In%20contrast%2C%0Aunregularized%20networks%20remain%20trapped%20in%20a%20high-complexity%20memorization%20phase.%0AWe%20establish%20an%20explicit%20connection%20between%20our%20complexity%20measure%20and%0Ageneralization%20bounds%2C%20providing%20a%20theoretical%20foundation%20for%20the%20link%20between%0Alossy%20compression%20and%20generalization.%20Our%20framework%20achieves%20compression%20ratios%0A30-40x%20better%20than%20na%5C%22ive%20approaches%2C%20enabling%20precise%20tracking%20of%20complexity%0Adynamics.%20Finally%2C%20we%20introduce%20a%20regularization%20method%20based%20on%20spectral%0Aentropy%20that%20encourages%20networks%20toward%20low-complexity%20representations%20by%0Apenalizing%20their%20intrinsic%20dimension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09810v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Complexity%2520Dynamics%2520of%2520Grokking%26entry.906535625%3DBranton%2520DeMoss%2520and%2520Silvia%2520Sapora%2520and%2520Jakob%2520Foerster%2520and%2520Nick%2520Hawes%2520and%2520Ingmar%2520Posner%26entry.1292438233%3D%2520%2520We%2520demonstrate%2520the%2520existence%2520of%2520a%2520complexity%2520phase%2520transition%2520in%2520neural%250Anetworks%2520by%2520studying%2520the%2520grokking%2520phenomenon%252C%2520where%2520networks%2520suddenly%250Atransition%2520from%2520memorization%2520to%2520generalization%2520long%2520after%2520overfitting%2520their%250Atraining%2520data.%2520To%2520characterize%2520this%2520phase%2520transition%252C%2520we%2520introduce%2520a%250Atheoretical%2520framework%2520for%2520measuring%2520complexity%2520based%2520on%2520rate-distortion%2520theory%250Aand%2520Kolmogorov%2520complexity%252C%2520which%2520can%2520be%2520understood%2520as%2520principled%2520lossy%250Acompression%2520for%2520networks.%2520We%2520find%2520that%2520properly%2520regularized%2520networks%2520exhibit%2520a%250Asharp%2520phase%2520transition%253A%2520complexity%2520rises%2520during%2520memorization%252C%2520then%2520falls%2520as%2520the%250Anetwork%2520discovers%2520a%2520simpler%2520underlying%2520pattern%2520that%2520generalizes.%2520In%2520contrast%252C%250Aunregularized%2520networks%2520remain%2520trapped%2520in%2520a%2520high-complexity%2520memorization%2520phase.%250AWe%2520establish%2520an%2520explicit%2520connection%2520between%2520our%2520complexity%2520measure%2520and%250Ageneralization%2520bounds%252C%2520providing%2520a%2520theoretical%2520foundation%2520for%2520the%2520link%2520between%250Alossy%2520compression%2520and%2520generalization.%2520Our%2520framework%2520achieves%2520compression%2520ratios%250A30-40x%2520better%2520than%2520na%255C%2522ive%2520approaches%252C%2520enabling%2520precise%2520tracking%2520of%2520complexity%250Adynamics.%2520Finally%252C%2520we%2520introduce%2520a%2520regularization%2520method%2520based%2520on%2520spectral%250Aentropy%2520that%2520encourages%2520networks%2520toward%2520low-complexity%2520representations%2520by%250Apenalizing%2520their%2520intrinsic%2520dimension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09810v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Complexity%20Dynamics%20of%20Grokking&entry.906535625=Branton%20DeMoss%20and%20Silvia%20Sapora%20and%20Jakob%20Foerster%20and%20Nick%20Hawes%20and%20Ingmar%20Posner&entry.1292438233=%20%20We%20demonstrate%20the%20existence%20of%20a%20complexity%20phase%20transition%20in%20neural%0Anetworks%20by%20studying%20the%20grokking%20phenomenon%2C%20where%20networks%20suddenly%0Atransition%20from%20memorization%20to%20generalization%20long%20after%20overfitting%20their%0Atraining%20data.%20To%20characterize%20this%20phase%20transition%2C%20we%20introduce%20a%0Atheoretical%20framework%20for%20measuring%20complexity%20based%20on%20rate-distortion%20theory%0Aand%20Kolmogorov%20complexity%2C%20which%20can%20be%20understood%20as%20principled%20lossy%0Acompression%20for%20networks.%20We%20find%20that%20properly%20regularized%20networks%20exhibit%20a%0Asharp%20phase%20transition%3A%20complexity%20rises%20during%20memorization%2C%20then%20falls%20as%20the%0Anetwork%20discovers%20a%20simpler%20underlying%20pattern%20that%20generalizes.%20In%20contrast%2C%0Aunregularized%20networks%20remain%20trapped%20in%20a%20high-complexity%20memorization%20phase.%0AWe%20establish%20an%20explicit%20connection%20between%20our%20complexity%20measure%20and%0Ageneralization%20bounds%2C%20providing%20a%20theoretical%20foundation%20for%20the%20link%20between%0Alossy%20compression%20and%20generalization.%20Our%20framework%20achieves%20compression%20ratios%0A30-40x%20better%20than%20na%5C%22ive%20approaches%2C%20enabling%20precise%20tracking%20of%20complexity%0Adynamics.%20Finally%2C%20we%20introduce%20a%20regularization%20method%20based%20on%20spectral%0Aentropy%20that%20encourages%20networks%20toward%20low-complexity%20representations%20by%0Apenalizing%20their%20intrinsic%20dimension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09810v2&entry.124074799=Read"},
{"title": "RESfM: Robust Deep Equivariant Structure from Motion", "author": "Fadi Khatib and Yoni Kasten and Dror Moran and Meirav Galun and Ronen Basri", "abstract": "  Multiview Structure from Motion is a fundamental and challenging computer\nvision problem. A recent deep-based approach utilized matrix equivariant\narchitectures for simultaneous recovery of camera pose and 3D scene structure\nfrom large image collections. That work, however, made the unrealistic\nassumption that the point tracks given as input are almost clean of outliers.\nHere, we propose an architecture suited to dealing with outliers by adding a\nmultiview inlier/outlier classification module that respects the model\nequivariance and by utilizing a robust bundle adjustment step. Experiments\ndemonstrate that our method can be applied successfully in realistic settings\nthat include large image collections and point tracks extracted with common\nheuristics that include many outliers, achieving state-of-the-art accuracies in\nalmost all runs, superior to existing deep-based methods and on-par with\nleading classical (non-deep) sequential and global methods.\n", "link": "http://arxiv.org/abs/2404.14280v2", "date": "2025-08-21", "relevancy": 2.3005, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5971}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5612}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RESfM%3A%20Robust%20Deep%20Equivariant%20Structure%20from%20Motion&body=Title%3A%20RESfM%3A%20Robust%20Deep%20Equivariant%20Structure%20from%20Motion%0AAuthor%3A%20Fadi%20Khatib%20and%20Yoni%20Kasten%20and%20Dror%20Moran%20and%20Meirav%20Galun%20and%20Ronen%20Basri%0AAbstract%3A%20%20%20Multiview%20Structure%20from%20Motion%20is%20a%20fundamental%20and%20challenging%20computer%0Avision%20problem.%20A%20recent%20deep-based%20approach%20utilized%20matrix%20equivariant%0Aarchitectures%20for%20simultaneous%20recovery%20of%20camera%20pose%20and%203D%20scene%20structure%0Afrom%20large%20image%20collections.%20That%20work%2C%20however%2C%20made%20the%20unrealistic%0Aassumption%20that%20the%20point%20tracks%20given%20as%20input%20are%20almost%20clean%20of%20outliers.%0AHere%2C%20we%20propose%20an%20architecture%20suited%20to%20dealing%20with%20outliers%20by%20adding%20a%0Amultiview%20inlier/outlier%20classification%20module%20that%20respects%20the%20model%0Aequivariance%20and%20by%20utilizing%20a%20robust%20bundle%20adjustment%20step.%20Experiments%0Ademonstrate%20that%20our%20method%20can%20be%20applied%20successfully%20in%20realistic%20settings%0Athat%20include%20large%20image%20collections%20and%20point%20tracks%20extracted%20with%20common%0Aheuristics%20that%20include%20many%20outliers%2C%20achieving%20state-of-the-art%20accuracies%20in%0Aalmost%20all%20runs%2C%20superior%20to%20existing%20deep-based%20methods%20and%20on-par%20with%0Aleading%20classical%20%28non-deep%29%20sequential%20and%20global%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14280v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRESfM%253A%2520Robust%2520Deep%2520Equivariant%2520Structure%2520from%2520Motion%26entry.906535625%3DFadi%2520Khatib%2520and%2520Yoni%2520Kasten%2520and%2520Dror%2520Moran%2520and%2520Meirav%2520Galun%2520and%2520Ronen%2520Basri%26entry.1292438233%3D%2520%2520Multiview%2520Structure%2520from%2520Motion%2520is%2520a%2520fundamental%2520and%2520challenging%2520computer%250Avision%2520problem.%2520A%2520recent%2520deep-based%2520approach%2520utilized%2520matrix%2520equivariant%250Aarchitectures%2520for%2520simultaneous%2520recovery%2520of%2520camera%2520pose%2520and%25203D%2520scene%2520structure%250Afrom%2520large%2520image%2520collections.%2520That%2520work%252C%2520however%252C%2520made%2520the%2520unrealistic%250Aassumption%2520that%2520the%2520point%2520tracks%2520given%2520as%2520input%2520are%2520almost%2520clean%2520of%2520outliers.%250AHere%252C%2520we%2520propose%2520an%2520architecture%2520suited%2520to%2520dealing%2520with%2520outliers%2520by%2520adding%2520a%250Amultiview%2520inlier/outlier%2520classification%2520module%2520that%2520respects%2520the%2520model%250Aequivariance%2520and%2520by%2520utilizing%2520a%2520robust%2520bundle%2520adjustment%2520step.%2520Experiments%250Ademonstrate%2520that%2520our%2520method%2520can%2520be%2520applied%2520successfully%2520in%2520realistic%2520settings%250Athat%2520include%2520large%2520image%2520collections%2520and%2520point%2520tracks%2520extracted%2520with%2520common%250Aheuristics%2520that%2520include%2520many%2520outliers%252C%2520achieving%2520state-of-the-art%2520accuracies%2520in%250Aalmost%2520all%2520runs%252C%2520superior%2520to%2520existing%2520deep-based%2520methods%2520and%2520on-par%2520with%250Aleading%2520classical%2520%2528non-deep%2529%2520sequential%2520and%2520global%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14280v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RESfM%3A%20Robust%20Deep%20Equivariant%20Structure%20from%20Motion&entry.906535625=Fadi%20Khatib%20and%20Yoni%20Kasten%20and%20Dror%20Moran%20and%20Meirav%20Galun%20and%20Ronen%20Basri&entry.1292438233=%20%20Multiview%20Structure%20from%20Motion%20is%20a%20fundamental%20and%20challenging%20computer%0Avision%20problem.%20A%20recent%20deep-based%20approach%20utilized%20matrix%20equivariant%0Aarchitectures%20for%20simultaneous%20recovery%20of%20camera%20pose%20and%203D%20scene%20structure%0Afrom%20large%20image%20collections.%20That%20work%2C%20however%2C%20made%20the%20unrealistic%0Aassumption%20that%20the%20point%20tracks%20given%20as%20input%20are%20almost%20clean%20of%20outliers.%0AHere%2C%20we%20propose%20an%20architecture%20suited%20to%20dealing%20with%20outliers%20by%20adding%20a%0Amultiview%20inlier/outlier%20classification%20module%20that%20respects%20the%20model%0Aequivariance%20and%20by%20utilizing%20a%20robust%20bundle%20adjustment%20step.%20Experiments%0Ademonstrate%20that%20our%20method%20can%20be%20applied%20successfully%20in%20realistic%20settings%0Athat%20include%20large%20image%20collections%20and%20point%20tracks%20extracted%20with%20common%0Aheuristics%20that%20include%20many%20outliers%2C%20achieving%20state-of-the-art%20accuracies%20in%0Aalmost%20all%20runs%2C%20superior%20to%20existing%20deep-based%20methods%20and%20on-par%20with%0Aleading%20classical%20%28non-deep%29%20sequential%20and%20global%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14280v2&entry.124074799=Read"},
{"title": "Influence-driven Curriculum Learning for Pre-training on Limited Data", "author": "Loris Schoenegger and Lukas Thoma and Terra Blevins and Benjamin Roth", "abstract": "  Curriculum learning, a training technique where data is presented to the\nmodel in order of example difficulty (e.g., from simpler to more complex\ndocuments), has shown limited success for pre-training language models. In this\nwork, we investigate whether curriculum learning becomes competitive if we\nreplace conventional human-centered difficulty metrics with one that more\nclosely corresponds to example difficulty as observed during model training.\nSpecifically, we experiment with sorting training examples by their\n\\textit{training data influence}, a score which estimates the effect of\nindividual training examples on the model's output. Models trained on our\ncurricula are able to outperform ones trained in random order by over 10\npercentage points in benchmarks, confirming that curriculum learning is\nbeneficial for language model pre-training, as long as a more model-centric\nnotion of difficulty is adopted.\n", "link": "http://arxiv.org/abs/2508.15475v1", "date": "2025-08-21", "relevancy": 2.2981, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4643}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Influence-driven%20Curriculum%20Learning%20for%20Pre-training%20on%20Limited%20Data&body=Title%3A%20Influence-driven%20Curriculum%20Learning%20for%20Pre-training%20on%20Limited%20Data%0AAuthor%3A%20Loris%20Schoenegger%20and%20Lukas%20Thoma%20and%20Terra%20Blevins%20and%20Benjamin%20Roth%0AAbstract%3A%20%20%20Curriculum%20learning%2C%20a%20training%20technique%20where%20data%20is%20presented%20to%20the%0Amodel%20in%20order%20of%20example%20difficulty%20%28e.g.%2C%20from%20simpler%20to%20more%20complex%0Adocuments%29%2C%20has%20shown%20limited%20success%20for%20pre-training%20language%20models.%20In%20this%0Awork%2C%20we%20investigate%20whether%20curriculum%20learning%20becomes%20competitive%20if%20we%0Areplace%20conventional%20human-centered%20difficulty%20metrics%20with%20one%20that%20more%0Aclosely%20corresponds%20to%20example%20difficulty%20as%20observed%20during%20model%20training.%0ASpecifically%2C%20we%20experiment%20with%20sorting%20training%20examples%20by%20their%0A%5Ctextit%7Btraining%20data%20influence%7D%2C%20a%20score%20which%20estimates%20the%20effect%20of%0Aindividual%20training%20examples%20on%20the%20model%27s%20output.%20Models%20trained%20on%20our%0Acurricula%20are%20able%20to%20outperform%20ones%20trained%20in%20random%20order%20by%20over%2010%0Apercentage%20points%20in%20benchmarks%2C%20confirming%20that%20curriculum%20learning%20is%0Abeneficial%20for%20language%20model%20pre-training%2C%20as%20long%20as%20a%20more%20model-centric%0Anotion%20of%20difficulty%20is%20adopted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfluence-driven%2520Curriculum%2520Learning%2520for%2520Pre-training%2520on%2520Limited%2520Data%26entry.906535625%3DLoris%2520Schoenegger%2520and%2520Lukas%2520Thoma%2520and%2520Terra%2520Blevins%2520and%2520Benjamin%2520Roth%26entry.1292438233%3D%2520%2520Curriculum%2520learning%252C%2520a%2520training%2520technique%2520where%2520data%2520is%2520presented%2520to%2520the%250Amodel%2520in%2520order%2520of%2520example%2520difficulty%2520%2528e.g.%252C%2520from%2520simpler%2520to%2520more%2520complex%250Adocuments%2529%252C%2520has%2520shown%2520limited%2520success%2520for%2520pre-training%2520language%2520models.%2520In%2520this%250Awork%252C%2520we%2520investigate%2520whether%2520curriculum%2520learning%2520becomes%2520competitive%2520if%2520we%250Areplace%2520conventional%2520human-centered%2520difficulty%2520metrics%2520with%2520one%2520that%2520more%250Aclosely%2520corresponds%2520to%2520example%2520difficulty%2520as%2520observed%2520during%2520model%2520training.%250ASpecifically%252C%2520we%2520experiment%2520with%2520sorting%2520training%2520examples%2520by%2520their%250A%255Ctextit%257Btraining%2520data%2520influence%257D%252C%2520a%2520score%2520which%2520estimates%2520the%2520effect%2520of%250Aindividual%2520training%2520examples%2520on%2520the%2520model%2527s%2520output.%2520Models%2520trained%2520on%2520our%250Acurricula%2520are%2520able%2520to%2520outperform%2520ones%2520trained%2520in%2520random%2520order%2520by%2520over%252010%250Apercentage%2520points%2520in%2520benchmarks%252C%2520confirming%2520that%2520curriculum%2520learning%2520is%250Abeneficial%2520for%2520language%2520model%2520pre-training%252C%2520as%2520long%2520as%2520a%2520more%2520model-centric%250Anotion%2520of%2520difficulty%2520is%2520adopted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Influence-driven%20Curriculum%20Learning%20for%20Pre-training%20on%20Limited%20Data&entry.906535625=Loris%20Schoenegger%20and%20Lukas%20Thoma%20and%20Terra%20Blevins%20and%20Benjamin%20Roth&entry.1292438233=%20%20Curriculum%20learning%2C%20a%20training%20technique%20where%20data%20is%20presented%20to%20the%0Amodel%20in%20order%20of%20example%20difficulty%20%28e.g.%2C%20from%20simpler%20to%20more%20complex%0Adocuments%29%2C%20has%20shown%20limited%20success%20for%20pre-training%20language%20models.%20In%20this%0Awork%2C%20we%20investigate%20whether%20curriculum%20learning%20becomes%20competitive%20if%20we%0Areplace%20conventional%20human-centered%20difficulty%20metrics%20with%20one%20that%20more%0Aclosely%20corresponds%20to%20example%20difficulty%20as%20observed%20during%20model%20training.%0ASpecifically%2C%20we%20experiment%20with%20sorting%20training%20examples%20by%20their%0A%5Ctextit%7Btraining%20data%20influence%7D%2C%20a%20score%20which%20estimates%20the%20effect%20of%0Aindividual%20training%20examples%20on%20the%20model%27s%20output.%20Models%20trained%20on%20our%0Acurricula%20are%20able%20to%20outperform%20ones%20trained%20in%20random%20order%20by%20over%2010%0Apercentage%20points%20in%20benchmarks%2C%20confirming%20that%20curriculum%20learning%20is%0Abeneficial%20for%20language%20model%20pre-training%2C%20as%20long%20as%20a%20more%20model-centric%0Anotion%20of%20difficulty%20is%20adopted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15475v1&entry.124074799=Read"},
{"title": "Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors", "author": "Guotao Liang and Juncheng Hu and Ximing Xing and Jing Zhang and Qian Yu", "abstract": "  We introduce GroupSketch, a novel method for vector sketch animation that\neffectively handles multi-object interactions and complex motions. Existing\napproaches struggle with these scenarios, either being limited to single-object\ncases or suffering from temporal inconsistency and poor generalization. To\naddress these limitations, our method adopts a two-stage pipeline comprising\nMotion Initialization and Motion Refinement. In the first stage, the input\nsketch is interactively divided into semantic groups and key frames are\ndefined, enabling the generation of a coarse animation via interpolation. In\nthe second stage, we propose a Group-based Displacement Network (GDN), which\nrefines the coarse animation by predicting group-specific displacement fields,\nleveraging priors from a text-to-video model. GDN further incorporates\nspecialized modules, such as Context-conditioned Feature Enhancement (CCFE), to\nimprove temporal consistency. Extensive experiments demonstrate that our\napproach significantly outperforms existing methods in generating high-quality,\ntemporally consistent animations for complex, multi-object sketches, thus\nexpanding the practical applications of sketch animation.\n", "link": "http://arxiv.org/abs/2508.15535v1", "date": "2025-08-21", "relevancy": 2.2929, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.605}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5651}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Object%20Sketch%20Animation%20with%20Grouping%20and%20Motion%20Trajectory%20Priors&body=Title%3A%20Multi-Object%20Sketch%20Animation%20with%20Grouping%20and%20Motion%20Trajectory%20Priors%0AAuthor%3A%20Guotao%20Liang%20and%20Juncheng%20Hu%20and%20Ximing%20Xing%20and%20Jing%20Zhang%20and%20Qian%20Yu%0AAbstract%3A%20%20%20We%20introduce%20GroupSketch%2C%20a%20novel%20method%20for%20vector%20sketch%20animation%20that%0Aeffectively%20handles%20multi-object%20interactions%20and%20complex%20motions.%20Existing%0Aapproaches%20struggle%20with%20these%20scenarios%2C%20either%20being%20limited%20to%20single-object%0Acases%20or%20suffering%20from%20temporal%20inconsistency%20and%20poor%20generalization.%20To%0Aaddress%20these%20limitations%2C%20our%20method%20adopts%20a%20two-stage%20pipeline%20comprising%0AMotion%20Initialization%20and%20Motion%20Refinement.%20In%20the%20first%20stage%2C%20the%20input%0Asketch%20is%20interactively%20divided%20into%20semantic%20groups%20and%20key%20frames%20are%0Adefined%2C%20enabling%20the%20generation%20of%20a%20coarse%20animation%20via%20interpolation.%20In%0Athe%20second%20stage%2C%20we%20propose%20a%20Group-based%20Displacement%20Network%20%28GDN%29%2C%20which%0Arefines%20the%20coarse%20animation%20by%20predicting%20group-specific%20displacement%20fields%2C%0Aleveraging%20priors%20from%20a%20text-to-video%20model.%20GDN%20further%20incorporates%0Aspecialized%20modules%2C%20such%20as%20Context-conditioned%20Feature%20Enhancement%20%28CCFE%29%2C%20to%0Aimprove%20temporal%20consistency.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20significantly%20outperforms%20existing%20methods%20in%20generating%20high-quality%2C%0Atemporally%20consistent%20animations%20for%20complex%2C%20multi-object%20sketches%2C%20thus%0Aexpanding%20the%20practical%20applications%20of%20sketch%20animation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Object%2520Sketch%2520Animation%2520with%2520Grouping%2520and%2520Motion%2520Trajectory%2520Priors%26entry.906535625%3DGuotao%2520Liang%2520and%2520Juncheng%2520Hu%2520and%2520Ximing%2520Xing%2520and%2520Jing%2520Zhang%2520and%2520Qian%2520Yu%26entry.1292438233%3D%2520%2520We%2520introduce%2520GroupSketch%252C%2520a%2520novel%2520method%2520for%2520vector%2520sketch%2520animation%2520that%250Aeffectively%2520handles%2520multi-object%2520interactions%2520and%2520complex%2520motions.%2520Existing%250Aapproaches%2520struggle%2520with%2520these%2520scenarios%252C%2520either%2520being%2520limited%2520to%2520single-object%250Acases%2520or%2520suffering%2520from%2520temporal%2520inconsistency%2520and%2520poor%2520generalization.%2520To%250Aaddress%2520these%2520limitations%252C%2520our%2520method%2520adopts%2520a%2520two-stage%2520pipeline%2520comprising%250AMotion%2520Initialization%2520and%2520Motion%2520Refinement.%2520In%2520the%2520first%2520stage%252C%2520the%2520input%250Asketch%2520is%2520interactively%2520divided%2520into%2520semantic%2520groups%2520and%2520key%2520frames%2520are%250Adefined%252C%2520enabling%2520the%2520generation%2520of%2520a%2520coarse%2520animation%2520via%2520interpolation.%2520In%250Athe%2520second%2520stage%252C%2520we%2520propose%2520a%2520Group-based%2520Displacement%2520Network%2520%2528GDN%2529%252C%2520which%250Arefines%2520the%2520coarse%2520animation%2520by%2520predicting%2520group-specific%2520displacement%2520fields%252C%250Aleveraging%2520priors%2520from%2520a%2520text-to-video%2520model.%2520GDN%2520further%2520incorporates%250Aspecialized%2520modules%252C%2520such%2520as%2520Context-conditioned%2520Feature%2520Enhancement%2520%2528CCFE%2529%252C%2520to%250Aimprove%2520temporal%2520consistency.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aapproach%2520significantly%2520outperforms%2520existing%2520methods%2520in%2520generating%2520high-quality%252C%250Atemporally%2520consistent%2520animations%2520for%2520complex%252C%2520multi-object%2520sketches%252C%2520thus%250Aexpanding%2520the%2520practical%2520applications%2520of%2520sketch%2520animation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Object%20Sketch%20Animation%20with%20Grouping%20and%20Motion%20Trajectory%20Priors&entry.906535625=Guotao%20Liang%20and%20Juncheng%20Hu%20and%20Ximing%20Xing%20and%20Jing%20Zhang%20and%20Qian%20Yu&entry.1292438233=%20%20We%20introduce%20GroupSketch%2C%20a%20novel%20method%20for%20vector%20sketch%20animation%20that%0Aeffectively%20handles%20multi-object%20interactions%20and%20complex%20motions.%20Existing%0Aapproaches%20struggle%20with%20these%20scenarios%2C%20either%20being%20limited%20to%20single-object%0Acases%20or%20suffering%20from%20temporal%20inconsistency%20and%20poor%20generalization.%20To%0Aaddress%20these%20limitations%2C%20our%20method%20adopts%20a%20two-stage%20pipeline%20comprising%0AMotion%20Initialization%20and%20Motion%20Refinement.%20In%20the%20first%20stage%2C%20the%20input%0Asketch%20is%20interactively%20divided%20into%20semantic%20groups%20and%20key%20frames%20are%0Adefined%2C%20enabling%20the%20generation%20of%20a%20coarse%20animation%20via%20interpolation.%20In%0Athe%20second%20stage%2C%20we%20propose%20a%20Group-based%20Displacement%20Network%20%28GDN%29%2C%20which%0Arefines%20the%20coarse%20animation%20by%20predicting%20group-specific%20displacement%20fields%2C%0Aleveraging%20priors%20from%20a%20text-to-video%20model.%20GDN%20further%20incorporates%0Aspecialized%20modules%2C%20such%20as%20Context-conditioned%20Feature%20Enhancement%20%28CCFE%29%2C%20to%0Aimprove%20temporal%20consistency.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20significantly%20outperforms%20existing%20methods%20in%20generating%20high-quality%2C%0Atemporally%20consistent%20animations%20for%20complex%2C%20multi-object%20sketches%2C%20thus%0Aexpanding%20the%20practical%20applications%20of%20sketch%20animation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15535v1&entry.124074799=Read"},
{"title": "Let's Grow an Unbiased Community: Guiding the Fairness of Graphs via New\n  Links", "author": "Jiahua Lu and Huaxiao Liu and Shuotong Bai and Junjie Xu and Renqiang Luo and Enyan Dai", "abstract": "  Graph Neural Networks (GNNs) have achieved remarkable success across diverse\napplications. However, due to the biases in the graph structures, graph neural\nnetworks face significant challenges in fairness. Although the original user\ngraph structure is generally biased, it is promising to guide these existing\nstructures toward unbiased ones by introducing new links. The fairness guidance\nvia new links could foster unbiased communities, thereby enhancing fairness in\ndownstream applications. To address this issue, we propose a novel framework\nnamed FairGuide. Specifically, to ensure fairness in downstream tasks trained\non fairness-guided graphs, we introduce a differentiable community detection\ntask as a pseudo downstream task. Our theoretical analysis further demonstrates\nthat optimizing fairness within this pseudo task effectively enhances\nstructural fairness, promoting fairness generalization across diverse\ndownstream applications. Moreover, FairGuide employs an effective strategy\nwhich leverages meta-gradients derived from the fairness-guidance objective to\nidentify new links that significantly enhance structural fairness. Extensive\nexperimental results demonstrate the effectiveness and generalizability of our\nproposed method across a variety of graph-based fairness tasks.\n", "link": "http://arxiv.org/abs/2508.15499v1", "date": "2025-08-21", "relevancy": 2.2864, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.464}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4594}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%27s%20Grow%20an%20Unbiased%20Community%3A%20Guiding%20the%20Fairness%20of%20Graphs%20via%20New%0A%20%20Links&body=Title%3A%20Let%27s%20Grow%20an%20Unbiased%20Community%3A%20Guiding%20the%20Fairness%20of%20Graphs%20via%20New%0A%20%20Links%0AAuthor%3A%20Jiahua%20Lu%20and%20Huaxiao%20Liu%20and%20Shuotong%20Bai%20and%20Junjie%20Xu%20and%20Renqiang%20Luo%20and%20Enyan%20Dai%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20remarkable%20success%20across%20diverse%0Aapplications.%20However%2C%20due%20to%20the%20biases%20in%20the%20graph%20structures%2C%20graph%20neural%0Anetworks%20face%20significant%20challenges%20in%20fairness.%20Although%20the%20original%20user%0Agraph%20structure%20is%20generally%20biased%2C%20it%20is%20promising%20to%20guide%20these%20existing%0Astructures%20toward%20unbiased%20ones%20by%20introducing%20new%20links.%20The%20fairness%20guidance%0Avia%20new%20links%20could%20foster%20unbiased%20communities%2C%20thereby%20enhancing%20fairness%20in%0Adownstream%20applications.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20framework%0Anamed%20FairGuide.%20Specifically%2C%20to%20ensure%20fairness%20in%20downstream%20tasks%20trained%0Aon%20fairness-guided%20graphs%2C%20we%20introduce%20a%20differentiable%20community%20detection%0Atask%20as%20a%20pseudo%20downstream%20task.%20Our%20theoretical%20analysis%20further%20demonstrates%0Athat%20optimizing%20fairness%20within%20this%20pseudo%20task%20effectively%20enhances%0Astructural%20fairness%2C%20promoting%20fairness%20generalization%20across%20diverse%0Adownstream%20applications.%20Moreover%2C%20FairGuide%20employs%20an%20effective%20strategy%0Awhich%20leverages%20meta-gradients%20derived%20from%20the%20fairness-guidance%20objective%20to%0Aidentify%20new%20links%20that%20significantly%20enhance%20structural%20fairness.%20Extensive%0Aexperimental%20results%20demonstrate%20the%20effectiveness%20and%20generalizability%20of%20our%0Aproposed%20method%20across%20a%20variety%20of%20graph-based%20fairness%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2527s%2520Grow%2520an%2520Unbiased%2520Community%253A%2520Guiding%2520the%2520Fairness%2520of%2520Graphs%2520via%2520New%250A%2520%2520Links%26entry.906535625%3DJiahua%2520Lu%2520and%2520Huaxiao%2520Liu%2520and%2520Shuotong%2520Bai%2520and%2520Junjie%2520Xu%2520and%2520Renqiang%2520Luo%2520and%2520Enyan%2520Dai%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520achieved%2520remarkable%2520success%2520across%2520diverse%250Aapplications.%2520However%252C%2520due%2520to%2520the%2520biases%2520in%2520the%2520graph%2520structures%252C%2520graph%2520neural%250Anetworks%2520face%2520significant%2520challenges%2520in%2520fairness.%2520Although%2520the%2520original%2520user%250Agraph%2520structure%2520is%2520generally%2520biased%252C%2520it%2520is%2520promising%2520to%2520guide%2520these%2520existing%250Astructures%2520toward%2520unbiased%2520ones%2520by%2520introducing%2520new%2520links.%2520The%2520fairness%2520guidance%250Avia%2520new%2520links%2520could%2520foster%2520unbiased%2520communities%252C%2520thereby%2520enhancing%2520fairness%2520in%250Adownstream%2520applications.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520framework%250Anamed%2520FairGuide.%2520Specifically%252C%2520to%2520ensure%2520fairness%2520in%2520downstream%2520tasks%2520trained%250Aon%2520fairness-guided%2520graphs%252C%2520we%2520introduce%2520a%2520differentiable%2520community%2520detection%250Atask%2520as%2520a%2520pseudo%2520downstream%2520task.%2520Our%2520theoretical%2520analysis%2520further%2520demonstrates%250Athat%2520optimizing%2520fairness%2520within%2520this%2520pseudo%2520task%2520effectively%2520enhances%250Astructural%2520fairness%252C%2520promoting%2520fairness%2520generalization%2520across%2520diverse%250Adownstream%2520applications.%2520Moreover%252C%2520FairGuide%2520employs%2520an%2520effective%2520strategy%250Awhich%2520leverages%2520meta-gradients%2520derived%2520from%2520the%2520fairness-guidance%2520objective%2520to%250Aidentify%2520new%2520links%2520that%2520significantly%2520enhance%2520structural%2520fairness.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520the%2520effectiveness%2520and%2520generalizability%2520of%2520our%250Aproposed%2520method%2520across%2520a%2520variety%2520of%2520graph-based%2520fairness%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%27s%20Grow%20an%20Unbiased%20Community%3A%20Guiding%20the%20Fairness%20of%20Graphs%20via%20New%0A%20%20Links&entry.906535625=Jiahua%20Lu%20and%20Huaxiao%20Liu%20and%20Shuotong%20Bai%20and%20Junjie%20Xu%20and%20Renqiang%20Luo%20and%20Enyan%20Dai&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20remarkable%20success%20across%20diverse%0Aapplications.%20However%2C%20due%20to%20the%20biases%20in%20the%20graph%20structures%2C%20graph%20neural%0Anetworks%20face%20significant%20challenges%20in%20fairness.%20Although%20the%20original%20user%0Agraph%20structure%20is%20generally%20biased%2C%20it%20is%20promising%20to%20guide%20these%20existing%0Astructures%20toward%20unbiased%20ones%20by%20introducing%20new%20links.%20The%20fairness%20guidance%0Avia%20new%20links%20could%20foster%20unbiased%20communities%2C%20thereby%20enhancing%20fairness%20in%0Adownstream%20applications.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20framework%0Anamed%20FairGuide.%20Specifically%2C%20to%20ensure%20fairness%20in%20downstream%20tasks%20trained%0Aon%20fairness-guided%20graphs%2C%20we%20introduce%20a%20differentiable%20community%20detection%0Atask%20as%20a%20pseudo%20downstream%20task.%20Our%20theoretical%20analysis%20further%20demonstrates%0Athat%20optimizing%20fairness%20within%20this%20pseudo%20task%20effectively%20enhances%0Astructural%20fairness%2C%20promoting%20fairness%20generalization%20across%20diverse%0Adownstream%20applications.%20Moreover%2C%20FairGuide%20employs%20an%20effective%20strategy%0Awhich%20leverages%20meta-gradients%20derived%20from%20the%20fairness-guidance%20objective%20to%0Aidentify%20new%20links%20that%20significantly%20enhance%20structural%20fairness.%20Extensive%0Aexperimental%20results%20demonstrate%20the%20effectiveness%20and%20generalizability%20of%20our%0Aproposed%20method%20across%20a%20variety%20of%20graph-based%20fairness%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15499v1&entry.124074799=Read"},
{"title": "Label Uncertainty for Ultrasound Segmentation", "author": "Malini Shivaram and Gautam Rajendrakumar Gare and Laura Hutchins and Jacob Duplantis and Thomas Deiss and Thales Nogueira Gomes and Thong Tran and Keyur H. Patel and Thomas H Fox and Amita Krishnan and Deva Ramanan and Bennett DeBoisblanc and Ricardo Rodriguez and John Galeotti", "abstract": "  In medical imaging, inter-observer variability among radiologists often\nintroduces label uncertainty, particularly in modalities where visual\ninterpretation is subjective. Lung ultrasound (LUS) is a prime example-it\nfrequently presents a mixture of highly ambiguous regions and clearly\ndiscernible structures, making consistent annotation challenging even for\nexperienced clinicians. In this work, we introduce a novel approach to both\nlabeling and training AI models using expert-supplied, per-pixel confidence\nvalues. Rather than treating annotations as absolute ground truth, we design a\ndata annotation protocol that captures the confidence that radiologists have in\neach labeled region, modeling the inherent aleatoric uncertainty present in\nreal-world clinical data. We demonstrate that incorporating these confidence\nvalues during training leads to improved segmentation performance. More\nimportantly, we show that this enhanced segmentation quality translates into\nbetter performance on downstream clinically-critical tasks-specifically,\nestimating S/F oxygenation ratio values, classifying S/F ratio change, and\npredicting 30-day patient readmission. While we empirically evaluate many\nmethods for exposing the uncertainty to the learning model, we find that a\nsimple approach that trains a model on binarized labels obtained with a (60%)\nconfidence threshold works well. Importantly, high thresholds work far better\nthan a naive approach of a 50% threshold, indicating that training on very\nconfident pixels is far more effective. Our study systematically investigates\nthe impact of training with varying confidence thresholds, comparing not only\nsegmentation metrics but also downstream clinical outcomes. These results\nsuggest that label confidence is a valuable signal that, when properly\nleveraged, can significantly enhance the reliability and clinical utility of AI\nin medical imaging.\n", "link": "http://arxiv.org/abs/2508.15635v1", "date": "2025-08-21", "relevancy": 2.2858, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6679}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5729}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label%20Uncertainty%20for%20Ultrasound%20Segmentation&body=Title%3A%20Label%20Uncertainty%20for%20Ultrasound%20Segmentation%0AAuthor%3A%20Malini%20Shivaram%20and%20Gautam%20Rajendrakumar%20Gare%20and%20Laura%20Hutchins%20and%20Jacob%20Duplantis%20and%20Thomas%20Deiss%20and%20Thales%20Nogueira%20Gomes%20and%20Thong%20Tran%20and%20Keyur%20H.%20Patel%20and%20Thomas%20H%20Fox%20and%20Amita%20Krishnan%20and%20Deva%20Ramanan%20and%20Bennett%20DeBoisblanc%20and%20Ricardo%20Rodriguez%20and%20John%20Galeotti%0AAbstract%3A%20%20%20In%20medical%20imaging%2C%20inter-observer%20variability%20among%20radiologists%20often%0Aintroduces%20label%20uncertainty%2C%20particularly%20in%20modalities%20where%20visual%0Ainterpretation%20is%20subjective.%20Lung%20ultrasound%20%28LUS%29%20is%20a%20prime%20example-it%0Afrequently%20presents%20a%20mixture%20of%20highly%20ambiguous%20regions%20and%20clearly%0Adiscernible%20structures%2C%20making%20consistent%20annotation%20challenging%20even%20for%0Aexperienced%20clinicians.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20to%20both%0Alabeling%20and%20training%20AI%20models%20using%20expert-supplied%2C%20per-pixel%20confidence%0Avalues.%20Rather%20than%20treating%20annotations%20as%20absolute%20ground%20truth%2C%20we%20design%20a%0Adata%20annotation%20protocol%20that%20captures%20the%20confidence%20that%20radiologists%20have%20in%0Aeach%20labeled%20region%2C%20modeling%20the%20inherent%20aleatoric%20uncertainty%20present%20in%0Areal-world%20clinical%20data.%20We%20demonstrate%20that%20incorporating%20these%20confidence%0Avalues%20during%20training%20leads%20to%20improved%20segmentation%20performance.%20More%0Aimportantly%2C%20we%20show%20that%20this%20enhanced%20segmentation%20quality%20translates%20into%0Abetter%20performance%20on%20downstream%20clinically-critical%20tasks-specifically%2C%0Aestimating%20S/F%20oxygenation%20ratio%20values%2C%20classifying%20S/F%20ratio%20change%2C%20and%0Apredicting%2030-day%20patient%20readmission.%20While%20we%20empirically%20evaluate%20many%0Amethods%20for%20exposing%20the%20uncertainty%20to%20the%20learning%20model%2C%20we%20find%20that%20a%0Asimple%20approach%20that%20trains%20a%20model%20on%20binarized%20labels%20obtained%20with%20a%20%2860%25%29%0Aconfidence%20threshold%20works%20well.%20Importantly%2C%20high%20thresholds%20work%20far%20better%0Athan%20a%20naive%20approach%20of%20a%2050%25%20threshold%2C%20indicating%20that%20training%20on%20very%0Aconfident%20pixels%20is%20far%20more%20effective.%20Our%20study%20systematically%20investigates%0Athe%20impact%20of%20training%20with%20varying%20confidence%20thresholds%2C%20comparing%20not%20only%0Asegmentation%20metrics%20but%20also%20downstream%20clinical%20outcomes.%20These%20results%0Asuggest%20that%20label%20confidence%20is%20a%20valuable%20signal%20that%2C%20when%20properly%0Aleveraged%2C%20can%20significantly%20enhance%20the%20reliability%20and%20clinical%20utility%20of%20AI%0Ain%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel%2520Uncertainty%2520for%2520Ultrasound%2520Segmentation%26entry.906535625%3DMalini%2520Shivaram%2520and%2520Gautam%2520Rajendrakumar%2520Gare%2520and%2520Laura%2520Hutchins%2520and%2520Jacob%2520Duplantis%2520and%2520Thomas%2520Deiss%2520and%2520Thales%2520Nogueira%2520Gomes%2520and%2520Thong%2520Tran%2520and%2520Keyur%2520H.%2520Patel%2520and%2520Thomas%2520H%2520Fox%2520and%2520Amita%2520Krishnan%2520and%2520Deva%2520Ramanan%2520and%2520Bennett%2520DeBoisblanc%2520and%2520Ricardo%2520Rodriguez%2520and%2520John%2520Galeotti%26entry.1292438233%3D%2520%2520In%2520medical%2520imaging%252C%2520inter-observer%2520variability%2520among%2520radiologists%2520often%250Aintroduces%2520label%2520uncertainty%252C%2520particularly%2520in%2520modalities%2520where%2520visual%250Ainterpretation%2520is%2520subjective.%2520Lung%2520ultrasound%2520%2528LUS%2529%2520is%2520a%2520prime%2520example-it%250Afrequently%2520presents%2520a%2520mixture%2520of%2520highly%2520ambiguous%2520regions%2520and%2520clearly%250Adiscernible%2520structures%252C%2520making%2520consistent%2520annotation%2520challenging%2520even%2520for%250Aexperienced%2520clinicians.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520to%2520both%250Alabeling%2520and%2520training%2520AI%2520models%2520using%2520expert-supplied%252C%2520per-pixel%2520confidence%250Avalues.%2520Rather%2520than%2520treating%2520annotations%2520as%2520absolute%2520ground%2520truth%252C%2520we%2520design%2520a%250Adata%2520annotation%2520protocol%2520that%2520captures%2520the%2520confidence%2520that%2520radiologists%2520have%2520in%250Aeach%2520labeled%2520region%252C%2520modeling%2520the%2520inherent%2520aleatoric%2520uncertainty%2520present%2520in%250Areal-world%2520clinical%2520data.%2520We%2520demonstrate%2520that%2520incorporating%2520these%2520confidence%250Avalues%2520during%2520training%2520leads%2520to%2520improved%2520segmentation%2520performance.%2520More%250Aimportantly%252C%2520we%2520show%2520that%2520this%2520enhanced%2520segmentation%2520quality%2520translates%2520into%250Abetter%2520performance%2520on%2520downstream%2520clinically-critical%2520tasks-specifically%252C%250Aestimating%2520S/F%2520oxygenation%2520ratio%2520values%252C%2520classifying%2520S/F%2520ratio%2520change%252C%2520and%250Apredicting%252030-day%2520patient%2520readmission.%2520While%2520we%2520empirically%2520evaluate%2520many%250Amethods%2520for%2520exposing%2520the%2520uncertainty%2520to%2520the%2520learning%2520model%252C%2520we%2520find%2520that%2520a%250Asimple%2520approach%2520that%2520trains%2520a%2520model%2520on%2520binarized%2520labels%2520obtained%2520with%2520a%2520%252860%2525%2529%250Aconfidence%2520threshold%2520works%2520well.%2520Importantly%252C%2520high%2520thresholds%2520work%2520far%2520better%250Athan%2520a%2520naive%2520approach%2520of%2520a%252050%2525%2520threshold%252C%2520indicating%2520that%2520training%2520on%2520very%250Aconfident%2520pixels%2520is%2520far%2520more%2520effective.%2520Our%2520study%2520systematically%2520investigates%250Athe%2520impact%2520of%2520training%2520with%2520varying%2520confidence%2520thresholds%252C%2520comparing%2520not%2520only%250Asegmentation%2520metrics%2520but%2520also%2520downstream%2520clinical%2520outcomes.%2520These%2520results%250Asuggest%2520that%2520label%2520confidence%2520is%2520a%2520valuable%2520signal%2520that%252C%2520when%2520properly%250Aleveraged%252C%2520can%2520significantly%2520enhance%2520the%2520reliability%2520and%2520clinical%2520utility%2520of%2520AI%250Ain%2520medical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label%20Uncertainty%20for%20Ultrasound%20Segmentation&entry.906535625=Malini%20Shivaram%20and%20Gautam%20Rajendrakumar%20Gare%20and%20Laura%20Hutchins%20and%20Jacob%20Duplantis%20and%20Thomas%20Deiss%20and%20Thales%20Nogueira%20Gomes%20and%20Thong%20Tran%20and%20Keyur%20H.%20Patel%20and%20Thomas%20H%20Fox%20and%20Amita%20Krishnan%20and%20Deva%20Ramanan%20and%20Bennett%20DeBoisblanc%20and%20Ricardo%20Rodriguez%20and%20John%20Galeotti&entry.1292438233=%20%20In%20medical%20imaging%2C%20inter-observer%20variability%20among%20radiologists%20often%0Aintroduces%20label%20uncertainty%2C%20particularly%20in%20modalities%20where%20visual%0Ainterpretation%20is%20subjective.%20Lung%20ultrasound%20%28LUS%29%20is%20a%20prime%20example-it%0Afrequently%20presents%20a%20mixture%20of%20highly%20ambiguous%20regions%20and%20clearly%0Adiscernible%20structures%2C%20making%20consistent%20annotation%20challenging%20even%20for%0Aexperienced%20clinicians.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20to%20both%0Alabeling%20and%20training%20AI%20models%20using%20expert-supplied%2C%20per-pixel%20confidence%0Avalues.%20Rather%20than%20treating%20annotations%20as%20absolute%20ground%20truth%2C%20we%20design%20a%0Adata%20annotation%20protocol%20that%20captures%20the%20confidence%20that%20radiologists%20have%20in%0Aeach%20labeled%20region%2C%20modeling%20the%20inherent%20aleatoric%20uncertainty%20present%20in%0Areal-world%20clinical%20data.%20We%20demonstrate%20that%20incorporating%20these%20confidence%0Avalues%20during%20training%20leads%20to%20improved%20segmentation%20performance.%20More%0Aimportantly%2C%20we%20show%20that%20this%20enhanced%20segmentation%20quality%20translates%20into%0Abetter%20performance%20on%20downstream%20clinically-critical%20tasks-specifically%2C%0Aestimating%20S/F%20oxygenation%20ratio%20values%2C%20classifying%20S/F%20ratio%20change%2C%20and%0Apredicting%2030-day%20patient%20readmission.%20While%20we%20empirically%20evaluate%20many%0Amethods%20for%20exposing%20the%20uncertainty%20to%20the%20learning%20model%2C%20we%20find%20that%20a%0Asimple%20approach%20that%20trains%20a%20model%20on%20binarized%20labels%20obtained%20with%20a%20%2860%25%29%0Aconfidence%20threshold%20works%20well.%20Importantly%2C%20high%20thresholds%20work%20far%20better%0Athan%20a%20naive%20approach%20of%20a%2050%25%20threshold%2C%20indicating%20that%20training%20on%20very%0Aconfident%20pixels%20is%20far%20more%20effective.%20Our%20study%20systematically%20investigates%0Athe%20impact%20of%20training%20with%20varying%20confidence%20thresholds%2C%20comparing%20not%20only%0Asegmentation%20metrics%20but%20also%20downstream%20clinical%20outcomes.%20These%20results%0Asuggest%20that%20label%20confidence%20is%20a%20valuable%20signal%20that%2C%20when%20properly%0Aleveraged%2C%20can%20significantly%20enhance%20the%20reliability%20and%20clinical%20utility%20of%20AI%0Ain%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15635v1&entry.124074799=Read"},
{"title": "LGMSNet: Thinning a medical image segmentation model via dual-level\n  multiscale fusion", "author": "Chengqi Dong and Fenghe Tang and Rongge Mao and Xinpei Gao and S. Kevin Zhou", "abstract": "  Medical image segmentation plays a pivotal role in disease diagnosis and\ntreatment planning, particularly in resource-constrained clinical settings\nwhere lightweight and generalizable models are urgently needed. However,\nexisting lightweight models often compromise performance for efficiency and\nrarely adopt computationally expensive attention mechanisms, severely\nrestricting their global contextual perception capabilities. Additionally,\ncurrent architectures neglect the channel redundancy issue under the same\nconvolutional kernels in medical imaging, which hinders effective feature\nextraction. To address these challenges, we propose LGMSNet, a novel\nlightweight framework based on local and global dual multiscale that achieves\nstate-of-the-art performance with minimal computational overhead. LGMSNet\nemploys heterogeneous intra-layer kernels to extract local high-frequency\ninformation while mitigating channel redundancy. In addition, the model\nintegrates sparse transformer-convolutional hybrid branches to capture\nlow-frequency global information. Extensive experiments across six public\ndatasets demonstrate LGMSNet's superiority over existing state-of-the-art\nmethods. In particular, LGMSNet maintains exceptional performance in zero-shot\ngeneralization tests on four unseen datasets, underscoring its potential for\nreal-world deployment in resource-limited medical scenarios. The whole project\ncode is in https://github.com/cq-dong/LGMSNet.\n", "link": "http://arxiv.org/abs/2508.15476v1", "date": "2025-08-21", "relevancy": 2.2797, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5807}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5736}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LGMSNet%3A%20Thinning%20a%20medical%20image%20segmentation%20model%20via%20dual-level%0A%20%20multiscale%20fusion&body=Title%3A%20LGMSNet%3A%20Thinning%20a%20medical%20image%20segmentation%20model%20via%20dual-level%0A%20%20multiscale%20fusion%0AAuthor%3A%20Chengqi%20Dong%20and%20Fenghe%20Tang%20and%20Rongge%20Mao%20and%20Xinpei%20Gao%20and%20S.%20Kevin%20Zhou%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20plays%20a%20pivotal%20role%20in%20disease%20diagnosis%20and%0Atreatment%20planning%2C%20particularly%20in%20resource-constrained%20clinical%20settings%0Awhere%20lightweight%20and%20generalizable%20models%20are%20urgently%20needed.%20However%2C%0Aexisting%20lightweight%20models%20often%20compromise%20performance%20for%20efficiency%20and%0Ararely%20adopt%20computationally%20expensive%20attention%20mechanisms%2C%20severely%0Arestricting%20their%20global%20contextual%20perception%20capabilities.%20Additionally%2C%0Acurrent%20architectures%20neglect%20the%20channel%20redundancy%20issue%20under%20the%20same%0Aconvolutional%20kernels%20in%20medical%20imaging%2C%20which%20hinders%20effective%20feature%0Aextraction.%20To%20address%20these%20challenges%2C%20we%20propose%20LGMSNet%2C%20a%20novel%0Alightweight%20framework%20based%20on%20local%20and%20global%20dual%20multiscale%20that%20achieves%0Astate-of-the-art%20performance%20with%20minimal%20computational%20overhead.%20LGMSNet%0Aemploys%20heterogeneous%20intra-layer%20kernels%20to%20extract%20local%20high-frequency%0Ainformation%20while%20mitigating%20channel%20redundancy.%20In%20addition%2C%20the%20model%0Aintegrates%20sparse%20transformer-convolutional%20hybrid%20branches%20to%20capture%0Alow-frequency%20global%20information.%20Extensive%20experiments%20across%20six%20public%0Adatasets%20demonstrate%20LGMSNet%27s%20superiority%20over%20existing%20state-of-the-art%0Amethods.%20In%20particular%2C%20LGMSNet%20maintains%20exceptional%20performance%20in%20zero-shot%0Ageneralization%20tests%20on%20four%20unseen%20datasets%2C%20underscoring%20its%20potential%20for%0Areal-world%20deployment%20in%20resource-limited%20medical%20scenarios.%20The%20whole%20project%0Acode%20is%20in%20https%3A//github.com/cq-dong/LGMSNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLGMSNet%253A%2520Thinning%2520a%2520medical%2520image%2520segmentation%2520model%2520via%2520dual-level%250A%2520%2520multiscale%2520fusion%26entry.906535625%3DChengqi%2520Dong%2520and%2520Fenghe%2520Tang%2520and%2520Rongge%2520Mao%2520and%2520Xinpei%2520Gao%2520and%2520S.%2520Kevin%2520Zhou%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520plays%2520a%2520pivotal%2520role%2520in%2520disease%2520diagnosis%2520and%250Atreatment%2520planning%252C%2520particularly%2520in%2520resource-constrained%2520clinical%2520settings%250Awhere%2520lightweight%2520and%2520generalizable%2520models%2520are%2520urgently%2520needed.%2520However%252C%250Aexisting%2520lightweight%2520models%2520often%2520compromise%2520performance%2520for%2520efficiency%2520and%250Ararely%2520adopt%2520computationally%2520expensive%2520attention%2520mechanisms%252C%2520severely%250Arestricting%2520their%2520global%2520contextual%2520perception%2520capabilities.%2520Additionally%252C%250Acurrent%2520architectures%2520neglect%2520the%2520channel%2520redundancy%2520issue%2520under%2520the%2520same%250Aconvolutional%2520kernels%2520in%2520medical%2520imaging%252C%2520which%2520hinders%2520effective%2520feature%250Aextraction.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520LGMSNet%252C%2520a%2520novel%250Alightweight%2520framework%2520based%2520on%2520local%2520and%2520global%2520dual%2520multiscale%2520that%2520achieves%250Astate-of-the-art%2520performance%2520with%2520minimal%2520computational%2520overhead.%2520LGMSNet%250Aemploys%2520heterogeneous%2520intra-layer%2520kernels%2520to%2520extract%2520local%2520high-frequency%250Ainformation%2520while%2520mitigating%2520channel%2520redundancy.%2520In%2520addition%252C%2520the%2520model%250Aintegrates%2520sparse%2520transformer-convolutional%2520hybrid%2520branches%2520to%2520capture%250Alow-frequency%2520global%2520information.%2520Extensive%2520experiments%2520across%2520six%2520public%250Adatasets%2520demonstrate%2520LGMSNet%2527s%2520superiority%2520over%2520existing%2520state-of-the-art%250Amethods.%2520In%2520particular%252C%2520LGMSNet%2520maintains%2520exceptional%2520performance%2520in%2520zero-shot%250Ageneralization%2520tests%2520on%2520four%2520unseen%2520datasets%252C%2520underscoring%2520its%2520potential%2520for%250Areal-world%2520deployment%2520in%2520resource-limited%2520medical%2520scenarios.%2520The%2520whole%2520project%250Acode%2520is%2520in%2520https%253A//github.com/cq-dong/LGMSNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LGMSNet%3A%20Thinning%20a%20medical%20image%20segmentation%20model%20via%20dual-level%0A%20%20multiscale%20fusion&entry.906535625=Chengqi%20Dong%20and%20Fenghe%20Tang%20and%20Rongge%20Mao%20and%20Xinpei%20Gao%20and%20S.%20Kevin%20Zhou&entry.1292438233=%20%20Medical%20image%20segmentation%20plays%20a%20pivotal%20role%20in%20disease%20diagnosis%20and%0Atreatment%20planning%2C%20particularly%20in%20resource-constrained%20clinical%20settings%0Awhere%20lightweight%20and%20generalizable%20models%20are%20urgently%20needed.%20However%2C%0Aexisting%20lightweight%20models%20often%20compromise%20performance%20for%20efficiency%20and%0Ararely%20adopt%20computationally%20expensive%20attention%20mechanisms%2C%20severely%0Arestricting%20their%20global%20contextual%20perception%20capabilities.%20Additionally%2C%0Acurrent%20architectures%20neglect%20the%20channel%20redundancy%20issue%20under%20the%20same%0Aconvolutional%20kernels%20in%20medical%20imaging%2C%20which%20hinders%20effective%20feature%0Aextraction.%20To%20address%20these%20challenges%2C%20we%20propose%20LGMSNet%2C%20a%20novel%0Alightweight%20framework%20based%20on%20local%20and%20global%20dual%20multiscale%20that%20achieves%0Astate-of-the-art%20performance%20with%20minimal%20computational%20overhead.%20LGMSNet%0Aemploys%20heterogeneous%20intra-layer%20kernels%20to%20extract%20local%20high-frequency%0Ainformation%20while%20mitigating%20channel%20redundancy.%20In%20addition%2C%20the%20model%0Aintegrates%20sparse%20transformer-convolutional%20hybrid%20branches%20to%20capture%0Alow-frequency%20global%20information.%20Extensive%20experiments%20across%20six%20public%0Adatasets%20demonstrate%20LGMSNet%27s%20superiority%20over%20existing%20state-of-the-art%0Amethods.%20In%20particular%2C%20LGMSNet%20maintains%20exceptional%20performance%20in%20zero-shot%0Ageneralization%20tests%20on%20four%20unseen%20datasets%2C%20underscoring%20its%20potential%20for%0Areal-world%20deployment%20in%20resource-limited%20medical%20scenarios.%20The%20whole%20project%0Acode%20is%20in%20https%3A//github.com/cq-dong/LGMSNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15476v1&entry.124074799=Read"},
{"title": "EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal\n  E-Commerce Models", "author": "Xinyi Ling and Hanwen Du and Zhihui Zhu and Xia Ning", "abstract": "  E-commerce platforms are rich in multimodal data, featuring a variety of\nimages that depict product details. However, this raises an important question:\ndo these images always enhance product understanding, or can they sometimes\nintroduce redundancy or degrade performance? Existing datasets are limited in\nboth scale and design, making it difficult to systematically examine this\nquestion. To this end, we introduce EcomMMMU, an e-commerce multimodal\nmultitask understanding dataset with 406,190 samples and 8,989,510 images.\nEcomMMMU is comprised of multi-image visual-language data designed with 8\nessential tasks and a specialized VSS subset to benchmark the capability of\nmultimodal large language models (MLLMs) to effectively utilize visual content.\nAnalysis on EcomMMMU reveals that product images do not consistently improve\nperformance and can, in some cases, degrade it. This indicates that MLLMs may\nstruggle to effectively leverage rich visual content for e-commerce tasks.\nBuilding on these insights, we propose SUMEI, a data-driven method that\nstrategically utilizes multiple images via predicting visual utilities before\nusing them for downstream tasks. Comprehensive experiments demonstrate the\neffectiveness and robustness of SUMEI. The data and code are available through\nhttps://anonymous.4open.science/r/submission25.\n", "link": "http://arxiv.org/abs/2508.15721v1", "date": "2025-08-21", "relevancy": 2.279, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5948}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EcomMMMU%3A%20Strategic%20Utilization%20of%20Visuals%20for%20Robust%20Multimodal%0A%20%20E-Commerce%20Models&body=Title%3A%20EcomMMMU%3A%20Strategic%20Utilization%20of%20Visuals%20for%20Robust%20Multimodal%0A%20%20E-Commerce%20Models%0AAuthor%3A%20Xinyi%20Ling%20and%20Hanwen%20Du%20and%20Zhihui%20Zhu%20and%20Xia%20Ning%0AAbstract%3A%20%20%20E-commerce%20platforms%20are%20rich%20in%20multimodal%20data%2C%20featuring%20a%20variety%20of%0Aimages%20that%20depict%20product%20details.%20However%2C%20this%20raises%20an%20important%20question%3A%0Ado%20these%20images%20always%20enhance%20product%20understanding%2C%20or%20can%20they%20sometimes%0Aintroduce%20redundancy%20or%20degrade%20performance%3F%20Existing%20datasets%20are%20limited%20in%0Aboth%20scale%20and%20design%2C%20making%20it%20difficult%20to%20systematically%20examine%20this%0Aquestion.%20To%20this%20end%2C%20we%20introduce%20EcomMMMU%2C%20an%20e-commerce%20multimodal%0Amultitask%20understanding%20dataset%20with%20406%2C190%20samples%20and%208%2C989%2C510%20images.%0AEcomMMMU%20is%20comprised%20of%20multi-image%20visual-language%20data%20designed%20with%208%0Aessential%20tasks%20and%20a%20specialized%20VSS%20subset%20to%20benchmark%20the%20capability%20of%0Amultimodal%20large%20language%20models%20%28MLLMs%29%20to%20effectively%20utilize%20visual%20content.%0AAnalysis%20on%20EcomMMMU%20reveals%20that%20product%20images%20do%20not%20consistently%20improve%0Aperformance%20and%20can%2C%20in%20some%20cases%2C%20degrade%20it.%20This%20indicates%20that%20MLLMs%20may%0Astruggle%20to%20effectively%20leverage%20rich%20visual%20content%20for%20e-commerce%20tasks.%0ABuilding%20on%20these%20insights%2C%20we%20propose%20SUMEI%2C%20a%20data-driven%20method%20that%0Astrategically%20utilizes%20multiple%20images%20via%20predicting%20visual%20utilities%20before%0Ausing%20them%20for%20downstream%20tasks.%20Comprehensive%20experiments%20demonstrate%20the%0Aeffectiveness%20and%20robustness%20of%20SUMEI.%20The%20data%20and%20code%20are%20available%20through%0Ahttps%3A//anonymous.4open.science/r/submission25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEcomMMMU%253A%2520Strategic%2520Utilization%2520of%2520Visuals%2520for%2520Robust%2520Multimodal%250A%2520%2520E-Commerce%2520Models%26entry.906535625%3DXinyi%2520Ling%2520and%2520Hanwen%2520Du%2520and%2520Zhihui%2520Zhu%2520and%2520Xia%2520Ning%26entry.1292438233%3D%2520%2520E-commerce%2520platforms%2520are%2520rich%2520in%2520multimodal%2520data%252C%2520featuring%2520a%2520variety%2520of%250Aimages%2520that%2520depict%2520product%2520details.%2520However%252C%2520this%2520raises%2520an%2520important%2520question%253A%250Ado%2520these%2520images%2520always%2520enhance%2520product%2520understanding%252C%2520or%2520can%2520they%2520sometimes%250Aintroduce%2520redundancy%2520or%2520degrade%2520performance%253F%2520Existing%2520datasets%2520are%2520limited%2520in%250Aboth%2520scale%2520and%2520design%252C%2520making%2520it%2520difficult%2520to%2520systematically%2520examine%2520this%250Aquestion.%2520To%2520this%2520end%252C%2520we%2520introduce%2520EcomMMMU%252C%2520an%2520e-commerce%2520multimodal%250Amultitask%2520understanding%2520dataset%2520with%2520406%252C190%2520samples%2520and%25208%252C989%252C510%2520images.%250AEcomMMMU%2520is%2520comprised%2520of%2520multi-image%2520visual-language%2520data%2520designed%2520with%25208%250Aessential%2520tasks%2520and%2520a%2520specialized%2520VSS%2520subset%2520to%2520benchmark%2520the%2520capability%2520of%250Amultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520to%2520effectively%2520utilize%2520visual%2520content.%250AAnalysis%2520on%2520EcomMMMU%2520reveals%2520that%2520product%2520images%2520do%2520not%2520consistently%2520improve%250Aperformance%2520and%2520can%252C%2520in%2520some%2520cases%252C%2520degrade%2520it.%2520This%2520indicates%2520that%2520MLLMs%2520may%250Astruggle%2520to%2520effectively%2520leverage%2520rich%2520visual%2520content%2520for%2520e-commerce%2520tasks.%250ABuilding%2520on%2520these%2520insights%252C%2520we%2520propose%2520SUMEI%252C%2520a%2520data-driven%2520method%2520that%250Astrategically%2520utilizes%2520multiple%2520images%2520via%2520predicting%2520visual%2520utilities%2520before%250Ausing%2520them%2520for%2520downstream%2520tasks.%2520Comprehensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520and%2520robustness%2520of%2520SUMEI.%2520The%2520data%2520and%2520code%2520are%2520available%2520through%250Ahttps%253A//anonymous.4open.science/r/submission25.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EcomMMMU%3A%20Strategic%20Utilization%20of%20Visuals%20for%20Robust%20Multimodal%0A%20%20E-Commerce%20Models&entry.906535625=Xinyi%20Ling%20and%20Hanwen%20Du%20and%20Zhihui%20Zhu%20and%20Xia%20Ning&entry.1292438233=%20%20E-commerce%20platforms%20are%20rich%20in%20multimodal%20data%2C%20featuring%20a%20variety%20of%0Aimages%20that%20depict%20product%20details.%20However%2C%20this%20raises%20an%20important%20question%3A%0Ado%20these%20images%20always%20enhance%20product%20understanding%2C%20or%20can%20they%20sometimes%0Aintroduce%20redundancy%20or%20degrade%20performance%3F%20Existing%20datasets%20are%20limited%20in%0Aboth%20scale%20and%20design%2C%20making%20it%20difficult%20to%20systematically%20examine%20this%0Aquestion.%20To%20this%20end%2C%20we%20introduce%20EcomMMMU%2C%20an%20e-commerce%20multimodal%0Amultitask%20understanding%20dataset%20with%20406%2C190%20samples%20and%208%2C989%2C510%20images.%0AEcomMMMU%20is%20comprised%20of%20multi-image%20visual-language%20data%20designed%20with%208%0Aessential%20tasks%20and%20a%20specialized%20VSS%20subset%20to%20benchmark%20the%20capability%20of%0Amultimodal%20large%20language%20models%20%28MLLMs%29%20to%20effectively%20utilize%20visual%20content.%0AAnalysis%20on%20EcomMMMU%20reveals%20that%20product%20images%20do%20not%20consistently%20improve%0Aperformance%20and%20can%2C%20in%20some%20cases%2C%20degrade%20it.%20This%20indicates%20that%20MLLMs%20may%0Astruggle%20to%20effectively%20leverage%20rich%20visual%20content%20for%20e-commerce%20tasks.%0ABuilding%20on%20these%20insights%2C%20we%20propose%20SUMEI%2C%20a%20data-driven%20method%20that%0Astrategically%20utilizes%20multiple%20images%20via%20predicting%20visual%20utilities%20before%0Ausing%20them%20for%20downstream%20tasks.%20Comprehensive%20experiments%20demonstrate%20the%0Aeffectiveness%20and%20robustness%20of%20SUMEI.%20The%20data%20and%20code%20are%20available%20through%0Ahttps%3A//anonymous.4open.science/r/submission25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15721v1&entry.124074799=Read"},
{"title": "An Efficient Open World Environment for Multi-Agent Social Learning", "author": "Eric Ye and Ren Tao and Natasha Jaques", "abstract": "  Many challenges remain before AI agents can be deployed in real-world\nenvironments. However, one virtue of such environments is that they are\ninherently multi-agent and contain human experts. Using advanced social\nintelligence in such an environment can help an AI agent learn adaptive skills\nand behaviors that a known expert exhibits. While social intelligence could\naccelerate training, it is currently difficult to study due to the lack of\nopen-ended multi-agent environments. In this work, we present an environment in\nwhich multiple self-interested agents can pursue complex and independent goals,\nreflective of real world challenges. This environment will enable research into\nthe development of socially intelligent AI agents in open-ended multi-agent\nsettings, where agents may be implicitly incentivized to cooperate to defeat\ncommon enemies, build and share tools, and achieve long horizon goals. In this\nwork, we investigate the impact on agent performance due to social learning in\nthe presence of experts and implicit cooperation such as emergent collaborative\ntool use, and whether agents can benefit from either cooperation or competition\nin this environment.\n", "link": "http://arxiv.org/abs/2508.15679v1", "date": "2025-08-21", "relevancy": 2.2696, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.572}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5685}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Open%20World%20Environment%20for%20Multi-Agent%20Social%20Learning&body=Title%3A%20An%20Efficient%20Open%20World%20Environment%20for%20Multi-Agent%20Social%20Learning%0AAuthor%3A%20Eric%20Ye%20and%20Ren%20Tao%20and%20Natasha%20Jaques%0AAbstract%3A%20%20%20Many%20challenges%20remain%20before%20AI%20agents%20can%20be%20deployed%20in%20real-world%0Aenvironments.%20However%2C%20one%20virtue%20of%20such%20environments%20is%20that%20they%20are%0Ainherently%20multi-agent%20and%20contain%20human%20experts.%20Using%20advanced%20social%0Aintelligence%20in%20such%20an%20environment%20can%20help%20an%20AI%20agent%20learn%20adaptive%20skills%0Aand%20behaviors%20that%20a%20known%20expert%20exhibits.%20While%20social%20intelligence%20could%0Aaccelerate%20training%2C%20it%20is%20currently%20difficult%20to%20study%20due%20to%20the%20lack%20of%0Aopen-ended%20multi-agent%20environments.%20In%20this%20work%2C%20we%20present%20an%20environment%20in%0Awhich%20multiple%20self-interested%20agents%20can%20pursue%20complex%20and%20independent%20goals%2C%0Areflective%20of%20real%20world%20challenges.%20This%20environment%20will%20enable%20research%20into%0Athe%20development%20of%20socially%20intelligent%20AI%20agents%20in%20open-ended%20multi-agent%0Asettings%2C%20where%20agents%20may%20be%20implicitly%20incentivized%20to%20cooperate%20to%20defeat%0Acommon%20enemies%2C%20build%20and%20share%20tools%2C%20and%20achieve%20long%20horizon%20goals.%20In%20this%0Awork%2C%20we%20investigate%20the%20impact%20on%20agent%20performance%20due%20to%20social%20learning%20in%0Athe%20presence%20of%20experts%20and%20implicit%20cooperation%20such%20as%20emergent%20collaborative%0Atool%20use%2C%20and%20whether%20agents%20can%20benefit%20from%20either%20cooperation%20or%20competition%0Ain%20this%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Open%2520World%2520Environment%2520for%2520Multi-Agent%2520Social%2520Learning%26entry.906535625%3DEric%2520Ye%2520and%2520Ren%2520Tao%2520and%2520Natasha%2520Jaques%26entry.1292438233%3D%2520%2520Many%2520challenges%2520remain%2520before%2520AI%2520agents%2520can%2520be%2520deployed%2520in%2520real-world%250Aenvironments.%2520However%252C%2520one%2520virtue%2520of%2520such%2520environments%2520is%2520that%2520they%2520are%250Ainherently%2520multi-agent%2520and%2520contain%2520human%2520experts.%2520Using%2520advanced%2520social%250Aintelligence%2520in%2520such%2520an%2520environment%2520can%2520help%2520an%2520AI%2520agent%2520learn%2520adaptive%2520skills%250Aand%2520behaviors%2520that%2520a%2520known%2520expert%2520exhibits.%2520While%2520social%2520intelligence%2520could%250Aaccelerate%2520training%252C%2520it%2520is%2520currently%2520difficult%2520to%2520study%2520due%2520to%2520the%2520lack%2520of%250Aopen-ended%2520multi-agent%2520environments.%2520In%2520this%2520work%252C%2520we%2520present%2520an%2520environment%2520in%250Awhich%2520multiple%2520self-interested%2520agents%2520can%2520pursue%2520complex%2520and%2520independent%2520goals%252C%250Areflective%2520of%2520real%2520world%2520challenges.%2520This%2520environment%2520will%2520enable%2520research%2520into%250Athe%2520development%2520of%2520socially%2520intelligent%2520AI%2520agents%2520in%2520open-ended%2520multi-agent%250Asettings%252C%2520where%2520agents%2520may%2520be%2520implicitly%2520incentivized%2520to%2520cooperate%2520to%2520defeat%250Acommon%2520enemies%252C%2520build%2520and%2520share%2520tools%252C%2520and%2520achieve%2520long%2520horizon%2520goals.%2520In%2520this%250Awork%252C%2520we%2520investigate%2520the%2520impact%2520on%2520agent%2520performance%2520due%2520to%2520social%2520learning%2520in%250Athe%2520presence%2520of%2520experts%2520and%2520implicit%2520cooperation%2520such%2520as%2520emergent%2520collaborative%250Atool%2520use%252C%2520and%2520whether%2520agents%2520can%2520benefit%2520from%2520either%2520cooperation%2520or%2520competition%250Ain%2520this%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Open%20World%20Environment%20for%20Multi-Agent%20Social%20Learning&entry.906535625=Eric%20Ye%20and%20Ren%20Tao%20and%20Natasha%20Jaques&entry.1292438233=%20%20Many%20challenges%20remain%20before%20AI%20agents%20can%20be%20deployed%20in%20real-world%0Aenvironments.%20However%2C%20one%20virtue%20of%20such%20environments%20is%20that%20they%20are%0Ainherently%20multi-agent%20and%20contain%20human%20experts.%20Using%20advanced%20social%0Aintelligence%20in%20such%20an%20environment%20can%20help%20an%20AI%20agent%20learn%20adaptive%20skills%0Aand%20behaviors%20that%20a%20known%20expert%20exhibits.%20While%20social%20intelligence%20could%0Aaccelerate%20training%2C%20it%20is%20currently%20difficult%20to%20study%20due%20to%20the%20lack%20of%0Aopen-ended%20multi-agent%20environments.%20In%20this%20work%2C%20we%20present%20an%20environment%20in%0Awhich%20multiple%20self-interested%20agents%20can%20pursue%20complex%20and%20independent%20goals%2C%0Areflective%20of%20real%20world%20challenges.%20This%20environment%20will%20enable%20research%20into%0Athe%20development%20of%20socially%20intelligent%20AI%20agents%20in%20open-ended%20multi-agent%0Asettings%2C%20where%20agents%20may%20be%20implicitly%20incentivized%20to%20cooperate%20to%20defeat%0Acommon%20enemies%2C%20build%20and%20share%20tools%2C%20and%20achieve%20long%20horizon%20goals.%20In%20this%0Awork%2C%20we%20investigate%20the%20impact%20on%20agent%20performance%20due%20to%20social%20learning%20in%0Athe%20presence%20of%20experts%20and%20implicit%20cooperation%20such%20as%20emergent%20collaborative%0Atool%20use%2C%20and%20whether%20agents%20can%20benefit%20from%20either%20cooperation%20or%20competition%0Ain%20this%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15679v1&entry.124074799=Read"},
{"title": "AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual\n  Reasoning", "author": "Siminfar Samakoush Galougah and Rishie Raj and Sanjoy Chowdhury and Sayan Nag and Ramani Duraiswami", "abstract": "  Current audio-visual (AV) benchmarks focus on final answer accuracy,\noverlooking the underlying reasoning process. This makes it difficult to\ndistinguish genuine comprehension from correct answers derived through flawed\nreasoning or hallucinations. To address this, we introduce AURA (Audio-visual\nUnderstanding and Reasoning Assessment), a benchmark for evaluating the\ncross-modal reasoning capabilities of Audio-Visual Large Language Models\n(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across\nsix challenging cognitive domains, such as causality, timbre and pitch, tempo\nand AV synchronization, unanswerability, implicit distractions, and skill\nprofiling, explicitly designed to be unanswerable from a single modality. This\nforces models to construct a valid logical path grounded in both audio and\nvideo, setting AURA apart from AV datasets that allow uni-modal shortcuts. To\nassess reasoning traces, we propose a novel metric, AuraScore, which addresses\nthe lack of robust tools for evaluating reasoning fidelity. It decomposes\nreasoning into two aspects: (i) Factual Consistency - whether reasoning is\ngrounded in perceptual evidence, and (ii) Core Inference - the logical validity\nof each reasoning step. Evaluations of SOTA models on AURA reveal a critical\nreasoning gap: although models achieve high accuracy (up to 92% on some tasks),\ntheir Factual Consistency and Core Inference scores fall below 45%. This\ndiscrepancy highlights that models often arrive at correct answers through\nflawed logic, underscoring the need for our benchmark and paving the way for\nmore robust multimodal evaluation.\n", "link": "http://arxiv.org/abs/2508.07470v2", "date": "2025-08-21", "relevancy": 2.2588, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5821}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5821}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AURA%3A%20A%20Fine-Grained%20Benchmark%20and%20Decomposed%20Metric%20for%20Audio-Visual%0A%20%20Reasoning&body=Title%3A%20AURA%3A%20A%20Fine-Grained%20Benchmark%20and%20Decomposed%20Metric%20for%20Audio-Visual%0A%20%20Reasoning%0AAuthor%3A%20Siminfar%20Samakoush%20Galougah%20and%20Rishie%20Raj%20and%20Sanjoy%20Chowdhury%20and%20Sayan%20Nag%20and%20Ramani%20Duraiswami%0AAbstract%3A%20%20%20Current%20audio-visual%20%28AV%29%20benchmarks%20focus%20on%20final%20answer%20accuracy%2C%0Aoverlooking%20the%20underlying%20reasoning%20process.%20This%20makes%20it%20difficult%20to%0Adistinguish%20genuine%20comprehension%20from%20correct%20answers%20derived%20through%20flawed%0Areasoning%20or%20hallucinations.%20To%20address%20this%2C%20we%20introduce%20AURA%20%28Audio-visual%0AUnderstanding%20and%20Reasoning%20Assessment%29%2C%20a%20benchmark%20for%20evaluating%20the%0Across-modal%20reasoning%20capabilities%20of%20Audio-Visual%20Large%20Language%20Models%0A%28AV-LLMs%29%20and%20Omni-modal%20Language%20Models%20%28OLMs%29.%20AURA%20includes%20questions%20across%0Asix%20challenging%20cognitive%20domains%2C%20such%20as%20causality%2C%20timbre%20and%20pitch%2C%20tempo%0Aand%20AV%20synchronization%2C%20unanswerability%2C%20implicit%20distractions%2C%20and%20skill%0Aprofiling%2C%20explicitly%20designed%20to%20be%20unanswerable%20from%20a%20single%20modality.%20This%0Aforces%20models%20to%20construct%20a%20valid%20logical%20path%20grounded%20in%20both%20audio%20and%0Avideo%2C%20setting%20AURA%20apart%20from%20AV%20datasets%20that%20allow%20uni-modal%20shortcuts.%20To%0Aassess%20reasoning%20traces%2C%20we%20propose%20a%20novel%20metric%2C%20AuraScore%2C%20which%20addresses%0Athe%20lack%20of%20robust%20tools%20for%20evaluating%20reasoning%20fidelity.%20It%20decomposes%0Areasoning%20into%20two%20aspects%3A%20%28i%29%20Factual%20Consistency%20-%20whether%20reasoning%20is%0Agrounded%20in%20perceptual%20evidence%2C%20and%20%28ii%29%20Core%20Inference%20-%20the%20logical%20validity%0Aof%20each%20reasoning%20step.%20Evaluations%20of%20SOTA%20models%20on%20AURA%20reveal%20a%20critical%0Areasoning%20gap%3A%20although%20models%20achieve%20high%20accuracy%20%28up%20to%2092%25%20on%20some%20tasks%29%2C%0Atheir%20Factual%20Consistency%20and%20Core%20Inference%20scores%20fall%20below%2045%25.%20This%0Adiscrepancy%20highlights%20that%20models%20often%20arrive%20at%20correct%20answers%20through%0Aflawed%20logic%2C%20underscoring%20the%20need%20for%20our%20benchmark%20and%20paving%20the%20way%20for%0Amore%20robust%20multimodal%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07470v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAURA%253A%2520A%2520Fine-Grained%2520Benchmark%2520and%2520Decomposed%2520Metric%2520for%2520Audio-Visual%250A%2520%2520Reasoning%26entry.906535625%3DSiminfar%2520Samakoush%2520Galougah%2520and%2520Rishie%2520Raj%2520and%2520Sanjoy%2520Chowdhury%2520and%2520Sayan%2520Nag%2520and%2520Ramani%2520Duraiswami%26entry.1292438233%3D%2520%2520Current%2520audio-visual%2520%2528AV%2529%2520benchmarks%2520focus%2520on%2520final%2520answer%2520accuracy%252C%250Aoverlooking%2520the%2520underlying%2520reasoning%2520process.%2520This%2520makes%2520it%2520difficult%2520to%250Adistinguish%2520genuine%2520comprehension%2520from%2520correct%2520answers%2520derived%2520through%2520flawed%250Areasoning%2520or%2520hallucinations.%2520To%2520address%2520this%252C%2520we%2520introduce%2520AURA%2520%2528Audio-visual%250AUnderstanding%2520and%2520Reasoning%2520Assessment%2529%252C%2520a%2520benchmark%2520for%2520evaluating%2520the%250Across-modal%2520reasoning%2520capabilities%2520of%2520Audio-Visual%2520Large%2520Language%2520Models%250A%2528AV-LLMs%2529%2520and%2520Omni-modal%2520Language%2520Models%2520%2528OLMs%2529.%2520AURA%2520includes%2520questions%2520across%250Asix%2520challenging%2520cognitive%2520domains%252C%2520such%2520as%2520causality%252C%2520timbre%2520and%2520pitch%252C%2520tempo%250Aand%2520AV%2520synchronization%252C%2520unanswerability%252C%2520implicit%2520distractions%252C%2520and%2520skill%250Aprofiling%252C%2520explicitly%2520designed%2520to%2520be%2520unanswerable%2520from%2520a%2520single%2520modality.%2520This%250Aforces%2520models%2520to%2520construct%2520a%2520valid%2520logical%2520path%2520grounded%2520in%2520both%2520audio%2520and%250Avideo%252C%2520setting%2520AURA%2520apart%2520from%2520AV%2520datasets%2520that%2520allow%2520uni-modal%2520shortcuts.%2520To%250Aassess%2520reasoning%2520traces%252C%2520we%2520propose%2520a%2520novel%2520metric%252C%2520AuraScore%252C%2520which%2520addresses%250Athe%2520lack%2520of%2520robust%2520tools%2520for%2520evaluating%2520reasoning%2520fidelity.%2520It%2520decomposes%250Areasoning%2520into%2520two%2520aspects%253A%2520%2528i%2529%2520Factual%2520Consistency%2520-%2520whether%2520reasoning%2520is%250Agrounded%2520in%2520perceptual%2520evidence%252C%2520and%2520%2528ii%2529%2520Core%2520Inference%2520-%2520the%2520logical%2520validity%250Aof%2520each%2520reasoning%2520step.%2520Evaluations%2520of%2520SOTA%2520models%2520on%2520AURA%2520reveal%2520a%2520critical%250Areasoning%2520gap%253A%2520although%2520models%2520achieve%2520high%2520accuracy%2520%2528up%2520to%252092%2525%2520on%2520some%2520tasks%2529%252C%250Atheir%2520Factual%2520Consistency%2520and%2520Core%2520Inference%2520scores%2520fall%2520below%252045%2525.%2520This%250Adiscrepancy%2520highlights%2520that%2520models%2520often%2520arrive%2520at%2520correct%2520answers%2520through%250Aflawed%2520logic%252C%2520underscoring%2520the%2520need%2520for%2520our%2520benchmark%2520and%2520paving%2520the%2520way%2520for%250Amore%2520robust%2520multimodal%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07470v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AURA%3A%20A%20Fine-Grained%20Benchmark%20and%20Decomposed%20Metric%20for%20Audio-Visual%0A%20%20Reasoning&entry.906535625=Siminfar%20Samakoush%20Galougah%20and%20Rishie%20Raj%20and%20Sanjoy%20Chowdhury%20and%20Sayan%20Nag%20and%20Ramani%20Duraiswami&entry.1292438233=%20%20Current%20audio-visual%20%28AV%29%20benchmarks%20focus%20on%20final%20answer%20accuracy%2C%0Aoverlooking%20the%20underlying%20reasoning%20process.%20This%20makes%20it%20difficult%20to%0Adistinguish%20genuine%20comprehension%20from%20correct%20answers%20derived%20through%20flawed%0Areasoning%20or%20hallucinations.%20To%20address%20this%2C%20we%20introduce%20AURA%20%28Audio-visual%0AUnderstanding%20and%20Reasoning%20Assessment%29%2C%20a%20benchmark%20for%20evaluating%20the%0Across-modal%20reasoning%20capabilities%20of%20Audio-Visual%20Large%20Language%20Models%0A%28AV-LLMs%29%20and%20Omni-modal%20Language%20Models%20%28OLMs%29.%20AURA%20includes%20questions%20across%0Asix%20challenging%20cognitive%20domains%2C%20such%20as%20causality%2C%20timbre%20and%20pitch%2C%20tempo%0Aand%20AV%20synchronization%2C%20unanswerability%2C%20implicit%20distractions%2C%20and%20skill%0Aprofiling%2C%20explicitly%20designed%20to%20be%20unanswerable%20from%20a%20single%20modality.%20This%0Aforces%20models%20to%20construct%20a%20valid%20logical%20path%20grounded%20in%20both%20audio%20and%0Avideo%2C%20setting%20AURA%20apart%20from%20AV%20datasets%20that%20allow%20uni-modal%20shortcuts.%20To%0Aassess%20reasoning%20traces%2C%20we%20propose%20a%20novel%20metric%2C%20AuraScore%2C%20which%20addresses%0Athe%20lack%20of%20robust%20tools%20for%20evaluating%20reasoning%20fidelity.%20It%20decomposes%0Areasoning%20into%20two%20aspects%3A%20%28i%29%20Factual%20Consistency%20-%20whether%20reasoning%20is%0Agrounded%20in%20perceptual%20evidence%2C%20and%20%28ii%29%20Core%20Inference%20-%20the%20logical%20validity%0Aof%20each%20reasoning%20step.%20Evaluations%20of%20SOTA%20models%20on%20AURA%20reveal%20a%20critical%0Areasoning%20gap%3A%20although%20models%20achieve%20high%20accuracy%20%28up%20to%2092%25%20on%20some%20tasks%29%2C%0Atheir%20Factual%20Consistency%20and%20Core%20Inference%20scores%20fall%20below%2045%25.%20This%0Adiscrepancy%20highlights%20that%20models%20often%20arrive%20at%20correct%20answers%20through%0Aflawed%20logic%2C%20underscoring%20the%20need%20for%20our%20benchmark%20and%20paving%20the%20way%20for%0Amore%20robust%20multimodal%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07470v2&entry.124074799=Read"},
{"title": "Unplug and Play Language Models: Decomposing Experts in Language Models\n  at Inference Time", "author": "Nakyeong Yang and Jiwon Moon and Junseok Kim and Yunah Jang and Kyomin Jung", "abstract": "  Enabled by large-scale text corpora with huge parameters, pre-trained\nlanguage models operate as multi-task experts using a single model\narchitecture. However, recent studies have revealed that certain neurons play\ndisproportionately important roles in solving specific tasks, suggesting that\ntask-relevant substructures can be isolated and selectively activated for each\ntask. Therefore, we introduce Decomposition of Experts (DoE), a novel framework\nthat dynamically identifies and activates task-specific experts within a\nlanguage model to reduce inference cost without sacrificing accuracy. We first\ndefine a task expert as a set of parameters that significantly influence the\nperformance of a specific task and propose a four-step unplug-and-play process:\n(1) receiving a user request, (2) identifying the corresponding task expert,\n(3) performing inference using the expert-localized model, and (4) restoring\nthe original model and waiting for the next task. Using attribution methods and\nprompt tuning, DoE isolates task-relevant neurons, minimizing computational\noverhead while maintaining task performance. We assume a setting where a\nlanguage model receives user requests from five widely used natural language\nunderstanding benchmarks, processing one task at a time. In this setup, we\ndemonstrate that DoE achieves up to a x1.73 inference speed-up with a 65%\npruning rate, without compromising accuracy. Comparisons with various task\nexpert localization methods reveal that DoE effectively identifies task\nexperts, while ablation studies validate the importance of its components.\nAdditionally, we analyze the effects of batch size, token count, and layer\ntypes on inference speed-up, providing practical insights for adopting DoE. The\nproposed framework is both practical and scalable, applicable to any\ntransformer-based architecture, offering a robust solution for efficient\ntask-specific inference.\n", "link": "http://arxiv.org/abs/2404.11916v3", "date": "2025-08-21", "relevancy": 2.2151, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unplug%20and%20Play%20Language%20Models%3A%20Decomposing%20Experts%20in%20Language%20Models%0A%20%20at%20Inference%20Time&body=Title%3A%20Unplug%20and%20Play%20Language%20Models%3A%20Decomposing%20Experts%20in%20Language%20Models%0A%20%20at%20Inference%20Time%0AAuthor%3A%20Nakyeong%20Yang%20and%20Jiwon%20Moon%20and%20Junseok%20Kim%20and%20Yunah%20Jang%20and%20Kyomin%20Jung%0AAbstract%3A%20%20%20Enabled%20by%20large-scale%20text%20corpora%20with%20huge%20parameters%2C%20pre-trained%0Alanguage%20models%20operate%20as%20multi-task%20experts%20using%20a%20single%20model%0Aarchitecture.%20However%2C%20recent%20studies%20have%20revealed%20that%20certain%20neurons%20play%0Adisproportionately%20important%20roles%20in%20solving%20specific%20tasks%2C%20suggesting%20that%0Atask-relevant%20substructures%20can%20be%20isolated%20and%20selectively%20activated%20for%20each%0Atask.%20Therefore%2C%20we%20introduce%20Decomposition%20of%20Experts%20%28DoE%29%2C%20a%20novel%20framework%0Athat%20dynamically%20identifies%20and%20activates%20task-specific%20experts%20within%20a%0Alanguage%20model%20to%20reduce%20inference%20cost%20without%20sacrificing%20accuracy.%20We%20first%0Adefine%20a%20task%20expert%20as%20a%20set%20of%20parameters%20that%20significantly%20influence%20the%0Aperformance%20of%20a%20specific%20task%20and%20propose%20a%20four-step%20unplug-and-play%20process%3A%0A%281%29%20receiving%20a%20user%20request%2C%20%282%29%20identifying%20the%20corresponding%20task%20expert%2C%0A%283%29%20performing%20inference%20using%20the%20expert-localized%20model%2C%20and%20%284%29%20restoring%0Athe%20original%20model%20and%20waiting%20for%20the%20next%20task.%20Using%20attribution%20methods%20and%0Aprompt%20tuning%2C%20DoE%20isolates%20task-relevant%20neurons%2C%20minimizing%20computational%0Aoverhead%20while%20maintaining%20task%20performance.%20We%20assume%20a%20setting%20where%20a%0Alanguage%20model%20receives%20user%20requests%20from%20five%20widely%20used%20natural%20language%0Aunderstanding%20benchmarks%2C%20processing%20one%20task%20at%20a%20time.%20In%20this%20setup%2C%20we%0Ademonstrate%20that%20DoE%20achieves%20up%20to%20a%20x1.73%20inference%20speed-up%20with%20a%2065%25%0Apruning%20rate%2C%20without%20compromising%20accuracy.%20Comparisons%20with%20various%20task%0Aexpert%20localization%20methods%20reveal%20that%20DoE%20effectively%20identifies%20task%0Aexperts%2C%20while%20ablation%20studies%20validate%20the%20importance%20of%20its%20components.%0AAdditionally%2C%20we%20analyze%20the%20effects%20of%20batch%20size%2C%20token%20count%2C%20and%20layer%0Atypes%20on%20inference%20speed-up%2C%20providing%20practical%20insights%20for%20adopting%20DoE.%20The%0Aproposed%20framework%20is%20both%20practical%20and%20scalable%2C%20applicable%20to%20any%0Atransformer-based%20architecture%2C%20offering%20a%20robust%20solution%20for%20efficient%0Atask-specific%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11916v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnplug%2520and%2520Play%2520Language%2520Models%253A%2520Decomposing%2520Experts%2520in%2520Language%2520Models%250A%2520%2520at%2520Inference%2520Time%26entry.906535625%3DNakyeong%2520Yang%2520and%2520Jiwon%2520Moon%2520and%2520Junseok%2520Kim%2520and%2520Yunah%2520Jang%2520and%2520Kyomin%2520Jung%26entry.1292438233%3D%2520%2520Enabled%2520by%2520large-scale%2520text%2520corpora%2520with%2520huge%2520parameters%252C%2520pre-trained%250Alanguage%2520models%2520operate%2520as%2520multi-task%2520experts%2520using%2520a%2520single%2520model%250Aarchitecture.%2520However%252C%2520recent%2520studies%2520have%2520revealed%2520that%2520certain%2520neurons%2520play%250Adisproportionately%2520important%2520roles%2520in%2520solving%2520specific%2520tasks%252C%2520suggesting%2520that%250Atask-relevant%2520substructures%2520can%2520be%2520isolated%2520and%2520selectively%2520activated%2520for%2520each%250Atask.%2520Therefore%252C%2520we%2520introduce%2520Decomposition%2520of%2520Experts%2520%2528DoE%2529%252C%2520a%2520novel%2520framework%250Athat%2520dynamically%2520identifies%2520and%2520activates%2520task-specific%2520experts%2520within%2520a%250Alanguage%2520model%2520to%2520reduce%2520inference%2520cost%2520without%2520sacrificing%2520accuracy.%2520We%2520first%250Adefine%2520a%2520task%2520expert%2520as%2520a%2520set%2520of%2520parameters%2520that%2520significantly%2520influence%2520the%250Aperformance%2520of%2520a%2520specific%2520task%2520and%2520propose%2520a%2520four-step%2520unplug-and-play%2520process%253A%250A%25281%2529%2520receiving%2520a%2520user%2520request%252C%2520%25282%2529%2520identifying%2520the%2520corresponding%2520task%2520expert%252C%250A%25283%2529%2520performing%2520inference%2520using%2520the%2520expert-localized%2520model%252C%2520and%2520%25284%2529%2520restoring%250Athe%2520original%2520model%2520and%2520waiting%2520for%2520the%2520next%2520task.%2520Using%2520attribution%2520methods%2520and%250Aprompt%2520tuning%252C%2520DoE%2520isolates%2520task-relevant%2520neurons%252C%2520minimizing%2520computational%250Aoverhead%2520while%2520maintaining%2520task%2520performance.%2520We%2520assume%2520a%2520setting%2520where%2520a%250Alanguage%2520model%2520receives%2520user%2520requests%2520from%2520five%2520widely%2520used%2520natural%2520language%250Aunderstanding%2520benchmarks%252C%2520processing%2520one%2520task%2520at%2520a%2520time.%2520In%2520this%2520setup%252C%2520we%250Ademonstrate%2520that%2520DoE%2520achieves%2520up%2520to%2520a%2520x1.73%2520inference%2520speed-up%2520with%2520a%252065%2525%250Apruning%2520rate%252C%2520without%2520compromising%2520accuracy.%2520Comparisons%2520with%2520various%2520task%250Aexpert%2520localization%2520methods%2520reveal%2520that%2520DoE%2520effectively%2520identifies%2520task%250Aexperts%252C%2520while%2520ablation%2520studies%2520validate%2520the%2520importance%2520of%2520its%2520components.%250AAdditionally%252C%2520we%2520analyze%2520the%2520effects%2520of%2520batch%2520size%252C%2520token%2520count%252C%2520and%2520layer%250Atypes%2520on%2520inference%2520speed-up%252C%2520providing%2520practical%2520insights%2520for%2520adopting%2520DoE.%2520The%250Aproposed%2520framework%2520is%2520both%2520practical%2520and%2520scalable%252C%2520applicable%2520to%2520any%250Atransformer-based%2520architecture%252C%2520offering%2520a%2520robust%2520solution%2520for%2520efficient%250Atask-specific%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11916v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unplug%20and%20Play%20Language%20Models%3A%20Decomposing%20Experts%20in%20Language%20Models%0A%20%20at%20Inference%20Time&entry.906535625=Nakyeong%20Yang%20and%20Jiwon%20Moon%20and%20Junseok%20Kim%20and%20Yunah%20Jang%20and%20Kyomin%20Jung&entry.1292438233=%20%20Enabled%20by%20large-scale%20text%20corpora%20with%20huge%20parameters%2C%20pre-trained%0Alanguage%20models%20operate%20as%20multi-task%20experts%20using%20a%20single%20model%0Aarchitecture.%20However%2C%20recent%20studies%20have%20revealed%20that%20certain%20neurons%20play%0Adisproportionately%20important%20roles%20in%20solving%20specific%20tasks%2C%20suggesting%20that%0Atask-relevant%20substructures%20can%20be%20isolated%20and%20selectively%20activated%20for%20each%0Atask.%20Therefore%2C%20we%20introduce%20Decomposition%20of%20Experts%20%28DoE%29%2C%20a%20novel%20framework%0Athat%20dynamically%20identifies%20and%20activates%20task-specific%20experts%20within%20a%0Alanguage%20model%20to%20reduce%20inference%20cost%20without%20sacrificing%20accuracy.%20We%20first%0Adefine%20a%20task%20expert%20as%20a%20set%20of%20parameters%20that%20significantly%20influence%20the%0Aperformance%20of%20a%20specific%20task%20and%20propose%20a%20four-step%20unplug-and-play%20process%3A%0A%281%29%20receiving%20a%20user%20request%2C%20%282%29%20identifying%20the%20corresponding%20task%20expert%2C%0A%283%29%20performing%20inference%20using%20the%20expert-localized%20model%2C%20and%20%284%29%20restoring%0Athe%20original%20model%20and%20waiting%20for%20the%20next%20task.%20Using%20attribution%20methods%20and%0Aprompt%20tuning%2C%20DoE%20isolates%20task-relevant%20neurons%2C%20minimizing%20computational%0Aoverhead%20while%20maintaining%20task%20performance.%20We%20assume%20a%20setting%20where%20a%0Alanguage%20model%20receives%20user%20requests%20from%20five%20widely%20used%20natural%20language%0Aunderstanding%20benchmarks%2C%20processing%20one%20task%20at%20a%20time.%20In%20this%20setup%2C%20we%0Ademonstrate%20that%20DoE%20achieves%20up%20to%20a%20x1.73%20inference%20speed-up%20with%20a%2065%25%0Apruning%20rate%2C%20without%20compromising%20accuracy.%20Comparisons%20with%20various%20task%0Aexpert%20localization%20methods%20reveal%20that%20DoE%20effectively%20identifies%20task%0Aexperts%2C%20while%20ablation%20studies%20validate%20the%20importance%20of%20its%20components.%0AAdditionally%2C%20we%20analyze%20the%20effects%20of%20batch%20size%2C%20token%20count%2C%20and%20layer%0Atypes%20on%20inference%20speed-up%2C%20providing%20practical%20insights%20for%20adopting%20DoE.%20The%0Aproposed%20framework%20is%20both%20practical%20and%20scalable%2C%20applicable%20to%20any%0Atransformer-based%20architecture%2C%20offering%20a%20robust%20solution%20for%20efficient%0Atask-specific%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11916v3&entry.124074799=Read"},
{"title": "On the Consistency of GNN Explanations for Malware Detection", "author": "Hossein Shokouhinejad and Griffin Higgins and Roozbeh Razavi-Far and Hesamodin Mohammadian and Ali A. Ghorbani", "abstract": "  Control Flow Graphs (CFGs) are critical for analyzing program execution and\ncharacterizing malware behavior. With the growing adoption of Graph Neural\nNetworks (GNNs), CFG-based representations have proven highly effective for\nmalware detection. This study proposes a novel framework that dynamically\nconstructs CFGs and embeds node features using a hybrid approach combining\nrule-based encoding and autoencoder-based embedding. A GNN-based classifier is\nthen constructed to detect malicious behavior from the resulting graph\nrepresentations. To improve model interpretability, we apply state-of-the-art\nexplainability techniques, including GNNExplainer, PGExplainer, and\nCaptumExplainer, the latter is utilized three attribution methods: Integrated\nGradients, Guided Backpropagation, and Saliency. In addition, we introduce a\nnovel aggregation method, called RankFusion, that integrates the outputs of the\ntop-performing explainers to enhance the explanation quality. We also evaluate\nexplanations using two subgraph extraction strategies, including the proposed\nGreedy Edge-wise Composition (GEC) method for improved structural coherence. A\ncomprehensive evaluation using accuracy, fidelity, and consistency metrics\ndemonstrates the effectiveness of the proposed framework in terms of accurate\nidentification of malware samples and generating reliable and interpretable\nexplanations.\n", "link": "http://arxiv.org/abs/2504.16316v2", "date": "2025-08-21", "relevancy": 2.2108, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4503}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4385}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Consistency%20of%20GNN%20Explanations%20for%20Malware%20Detection&body=Title%3A%20On%20the%20Consistency%20of%20GNN%20Explanations%20for%20Malware%20Detection%0AAuthor%3A%20Hossein%20Shokouhinejad%20and%20Griffin%20Higgins%20and%20Roozbeh%20Razavi-Far%20and%20Hesamodin%20Mohammadian%20and%20Ali%20A.%20Ghorbani%0AAbstract%3A%20%20%20Control%20Flow%20Graphs%20%28CFGs%29%20are%20critical%20for%20analyzing%20program%20execution%20and%0Acharacterizing%20malware%20behavior.%20With%20the%20growing%20adoption%20of%20Graph%20Neural%0ANetworks%20%28GNNs%29%2C%20CFG-based%20representations%20have%20proven%20highly%20effective%20for%0Amalware%20detection.%20This%20study%20proposes%20a%20novel%20framework%20that%20dynamically%0Aconstructs%20CFGs%20and%20embeds%20node%20features%20using%20a%20hybrid%20approach%20combining%0Arule-based%20encoding%20and%20autoencoder-based%20embedding.%20A%20GNN-based%20classifier%20is%0Athen%20constructed%20to%20detect%20malicious%20behavior%20from%20the%20resulting%20graph%0Arepresentations.%20To%20improve%20model%20interpretability%2C%20we%20apply%20state-of-the-art%0Aexplainability%20techniques%2C%20including%20GNNExplainer%2C%20PGExplainer%2C%20and%0ACaptumExplainer%2C%20the%20latter%20is%20utilized%20three%20attribution%20methods%3A%20Integrated%0AGradients%2C%20Guided%20Backpropagation%2C%20and%20Saliency.%20In%20addition%2C%20we%20introduce%20a%0Anovel%20aggregation%20method%2C%20called%20RankFusion%2C%20that%20integrates%20the%20outputs%20of%20the%0Atop-performing%20explainers%20to%20enhance%20the%20explanation%20quality.%20We%20also%20evaluate%0Aexplanations%20using%20two%20subgraph%20extraction%20strategies%2C%20including%20the%20proposed%0AGreedy%20Edge-wise%20Composition%20%28GEC%29%20method%20for%20improved%20structural%20coherence.%20A%0Acomprehensive%20evaluation%20using%20accuracy%2C%20fidelity%2C%20and%20consistency%20metrics%0Ademonstrates%20the%20effectiveness%20of%20the%20proposed%20framework%20in%20terms%20of%20accurate%0Aidentification%20of%20malware%20samples%20and%20generating%20reliable%20and%20interpretable%0Aexplanations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16316v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Consistency%2520of%2520GNN%2520Explanations%2520for%2520Malware%2520Detection%26entry.906535625%3DHossein%2520Shokouhinejad%2520and%2520Griffin%2520Higgins%2520and%2520Roozbeh%2520Razavi-Far%2520and%2520Hesamodin%2520Mohammadian%2520and%2520Ali%2520A.%2520Ghorbani%26entry.1292438233%3D%2520%2520Control%2520Flow%2520Graphs%2520%2528CFGs%2529%2520are%2520critical%2520for%2520analyzing%2520program%2520execution%2520and%250Acharacterizing%2520malware%2520behavior.%2520With%2520the%2520growing%2520adoption%2520of%2520Graph%2520Neural%250ANetworks%2520%2528GNNs%2529%252C%2520CFG-based%2520representations%2520have%2520proven%2520highly%2520effective%2520for%250Amalware%2520detection.%2520This%2520study%2520proposes%2520a%2520novel%2520framework%2520that%2520dynamically%250Aconstructs%2520CFGs%2520and%2520embeds%2520node%2520features%2520using%2520a%2520hybrid%2520approach%2520combining%250Arule-based%2520encoding%2520and%2520autoencoder-based%2520embedding.%2520A%2520GNN-based%2520classifier%2520is%250Athen%2520constructed%2520to%2520detect%2520malicious%2520behavior%2520from%2520the%2520resulting%2520graph%250Arepresentations.%2520To%2520improve%2520model%2520interpretability%252C%2520we%2520apply%2520state-of-the-art%250Aexplainability%2520techniques%252C%2520including%2520GNNExplainer%252C%2520PGExplainer%252C%2520and%250ACaptumExplainer%252C%2520the%2520latter%2520is%2520utilized%2520three%2520attribution%2520methods%253A%2520Integrated%250AGradients%252C%2520Guided%2520Backpropagation%252C%2520and%2520Saliency.%2520In%2520addition%252C%2520we%2520introduce%2520a%250Anovel%2520aggregation%2520method%252C%2520called%2520RankFusion%252C%2520that%2520integrates%2520the%2520outputs%2520of%2520the%250Atop-performing%2520explainers%2520to%2520enhance%2520the%2520explanation%2520quality.%2520We%2520also%2520evaluate%250Aexplanations%2520using%2520two%2520subgraph%2520extraction%2520strategies%252C%2520including%2520the%2520proposed%250AGreedy%2520Edge-wise%2520Composition%2520%2528GEC%2529%2520method%2520for%2520improved%2520structural%2520coherence.%2520A%250Acomprehensive%2520evaluation%2520using%2520accuracy%252C%2520fidelity%252C%2520and%2520consistency%2520metrics%250Ademonstrates%2520the%2520effectiveness%2520of%2520the%2520proposed%2520framework%2520in%2520terms%2520of%2520accurate%250Aidentification%2520of%2520malware%2520samples%2520and%2520generating%2520reliable%2520and%2520interpretable%250Aexplanations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16316v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Consistency%20of%20GNN%20Explanations%20for%20Malware%20Detection&entry.906535625=Hossein%20Shokouhinejad%20and%20Griffin%20Higgins%20and%20Roozbeh%20Razavi-Far%20and%20Hesamodin%20Mohammadian%20and%20Ali%20A.%20Ghorbani&entry.1292438233=%20%20Control%20Flow%20Graphs%20%28CFGs%29%20are%20critical%20for%20analyzing%20program%20execution%20and%0Acharacterizing%20malware%20behavior.%20With%20the%20growing%20adoption%20of%20Graph%20Neural%0ANetworks%20%28GNNs%29%2C%20CFG-based%20representations%20have%20proven%20highly%20effective%20for%0Amalware%20detection.%20This%20study%20proposes%20a%20novel%20framework%20that%20dynamically%0Aconstructs%20CFGs%20and%20embeds%20node%20features%20using%20a%20hybrid%20approach%20combining%0Arule-based%20encoding%20and%20autoencoder-based%20embedding.%20A%20GNN-based%20classifier%20is%0Athen%20constructed%20to%20detect%20malicious%20behavior%20from%20the%20resulting%20graph%0Arepresentations.%20To%20improve%20model%20interpretability%2C%20we%20apply%20state-of-the-art%0Aexplainability%20techniques%2C%20including%20GNNExplainer%2C%20PGExplainer%2C%20and%0ACaptumExplainer%2C%20the%20latter%20is%20utilized%20three%20attribution%20methods%3A%20Integrated%0AGradients%2C%20Guided%20Backpropagation%2C%20and%20Saliency.%20In%20addition%2C%20we%20introduce%20a%0Anovel%20aggregation%20method%2C%20called%20RankFusion%2C%20that%20integrates%20the%20outputs%20of%20the%0Atop-performing%20explainers%20to%20enhance%20the%20explanation%20quality.%20We%20also%20evaluate%0Aexplanations%20using%20two%20subgraph%20extraction%20strategies%2C%20including%20the%20proposed%0AGreedy%20Edge-wise%20Composition%20%28GEC%29%20method%20for%20improved%20structural%20coherence.%20A%0Acomprehensive%20evaluation%20using%20accuracy%2C%20fidelity%2C%20and%20consistency%20metrics%0Ademonstrates%20the%20effectiveness%20of%20the%20proposed%20framework%20in%20terms%20of%20accurate%0Aidentification%20of%20malware%20samples%20and%20generating%20reliable%20and%20interpretable%0Aexplanations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16316v2&entry.124074799=Read"},
{"title": "Towards Comprehensive Cellular Characterisation of H&E slides", "author": "Benjamin Adjadj and Pierre-Antoine Bannier and Guillaume Horent and Sebastien Mandela and Aurore Lyon and Kathryn Schutte and Ulysse Marteau and Valentin Gaury and Laura Dumont and Thomas Mathieu and Reda Belbahri and Beno\u00eet Schmauch and Eric Durand and Katharina Von Loga and Lucie Gillet", "abstract": "  Cell detection, segmentation and classification are essential for analyzing\ntumor microenvironments (TME) on hematoxylin and eosin (H&E) slides. Existing\nmethods suffer from poor performance on understudied cell types (rare or not\npresent in public datasets) and limited cross-domain generalization. To address\nthese shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell\nanalysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei\ncovering 13 cell types. In external validation across 4 independent cohorts,\nHistoPLUS outperforms current state-of-the-art models in detection quality by\n5.2% and overall F1 classification score by 23.7%, while using 5x fewer\nparameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types\nand brings significant improvements on 8 of 13 cell types. Moreover, we show\nthat HistoPLUS robustly transfers to two oncology indications unseen during\ntraining. To support broader TME biomarker research, we release the model\nweights and inference code at https://github.com/owkin/histoplus/.\n", "link": "http://arxiv.org/abs/2508.09926v2", "date": "2025-08-21", "relevancy": 2.1546, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4434}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4247}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Comprehensive%20Cellular%20Characterisation%20of%20H%26E%20slides&body=Title%3A%20Towards%20Comprehensive%20Cellular%20Characterisation%20of%20H%26E%20slides%0AAuthor%3A%20Benjamin%20Adjadj%20and%20Pierre-Antoine%20Bannier%20and%20Guillaume%20Horent%20and%20Sebastien%20Mandela%20and%20Aurore%20Lyon%20and%20Kathryn%20Schutte%20and%20Ulysse%20Marteau%20and%20Valentin%20Gaury%20and%20Laura%20Dumont%20and%20Thomas%20Mathieu%20and%20Reda%20Belbahri%20and%20Beno%C3%AEt%20Schmauch%20and%20Eric%20Durand%20and%20Katharina%20Von%20Loga%20and%20Lucie%20Gillet%0AAbstract%3A%20%20%20Cell%20detection%2C%20segmentation%20and%20classification%20are%20essential%20for%20analyzing%0Atumor%20microenvironments%20%28TME%29%20on%20hematoxylin%20and%20eosin%20%28H%26E%29%20slides.%20Existing%0Amethods%20suffer%20from%20poor%20performance%20on%20understudied%20cell%20types%20%28rare%20or%20not%0Apresent%20in%20public%20datasets%29%20and%20limited%20cross-domain%20generalization.%20To%20address%0Athese%20shortcomings%2C%20we%20introduce%20HistoPLUS%2C%20a%20state-of-the-art%20model%20for%20cell%0Aanalysis%2C%20trained%20on%20a%20novel%20curated%20pan-cancer%20dataset%20of%20108%2C722%20nuclei%0Acovering%2013%20cell%20types.%20In%20external%20validation%20across%204%20independent%20cohorts%2C%0AHistoPLUS%20outperforms%20current%20state-of-the-art%20models%20in%20detection%20quality%20by%0A5.2%25%20and%20overall%20F1%20classification%20score%20by%2023.7%25%2C%20while%20using%205x%20fewer%0Aparameters.%20Notably%2C%20HistoPLUS%20unlocks%20the%20study%20of%207%20understudied%20cell%20types%0Aand%20brings%20significant%20improvements%20on%208%20of%2013%20cell%20types.%20Moreover%2C%20we%20show%0Athat%20HistoPLUS%20robustly%20transfers%20to%20two%20oncology%20indications%20unseen%20during%0Atraining.%20To%20support%20broader%20TME%20biomarker%20research%2C%20we%20release%20the%20model%0Aweights%20and%20inference%20code%20at%20https%3A//github.com/owkin/histoplus/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09926v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Comprehensive%2520Cellular%2520Characterisation%2520of%2520H%2526E%2520slides%26entry.906535625%3DBenjamin%2520Adjadj%2520and%2520Pierre-Antoine%2520Bannier%2520and%2520Guillaume%2520Horent%2520and%2520Sebastien%2520Mandela%2520and%2520Aurore%2520Lyon%2520and%2520Kathryn%2520Schutte%2520and%2520Ulysse%2520Marteau%2520and%2520Valentin%2520Gaury%2520and%2520Laura%2520Dumont%2520and%2520Thomas%2520Mathieu%2520and%2520Reda%2520Belbahri%2520and%2520Beno%25C3%25AEt%2520Schmauch%2520and%2520Eric%2520Durand%2520and%2520Katharina%2520Von%2520Loga%2520and%2520Lucie%2520Gillet%26entry.1292438233%3D%2520%2520Cell%2520detection%252C%2520segmentation%2520and%2520classification%2520are%2520essential%2520for%2520analyzing%250Atumor%2520microenvironments%2520%2528TME%2529%2520on%2520hematoxylin%2520and%2520eosin%2520%2528H%2526E%2529%2520slides.%2520Existing%250Amethods%2520suffer%2520from%2520poor%2520performance%2520on%2520understudied%2520cell%2520types%2520%2528rare%2520or%2520not%250Apresent%2520in%2520public%2520datasets%2529%2520and%2520limited%2520cross-domain%2520generalization.%2520To%2520address%250Athese%2520shortcomings%252C%2520we%2520introduce%2520HistoPLUS%252C%2520a%2520state-of-the-art%2520model%2520for%2520cell%250Aanalysis%252C%2520trained%2520on%2520a%2520novel%2520curated%2520pan-cancer%2520dataset%2520of%2520108%252C722%2520nuclei%250Acovering%252013%2520cell%2520types.%2520In%2520external%2520validation%2520across%25204%2520independent%2520cohorts%252C%250AHistoPLUS%2520outperforms%2520current%2520state-of-the-art%2520models%2520in%2520detection%2520quality%2520by%250A5.2%2525%2520and%2520overall%2520F1%2520classification%2520score%2520by%252023.7%2525%252C%2520while%2520using%25205x%2520fewer%250Aparameters.%2520Notably%252C%2520HistoPLUS%2520unlocks%2520the%2520study%2520of%25207%2520understudied%2520cell%2520types%250Aand%2520brings%2520significant%2520improvements%2520on%25208%2520of%252013%2520cell%2520types.%2520Moreover%252C%2520we%2520show%250Athat%2520HistoPLUS%2520robustly%2520transfers%2520to%2520two%2520oncology%2520indications%2520unseen%2520during%250Atraining.%2520To%2520support%2520broader%2520TME%2520biomarker%2520research%252C%2520we%2520release%2520the%2520model%250Aweights%2520and%2520inference%2520code%2520at%2520https%253A//github.com/owkin/histoplus/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09926v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Comprehensive%20Cellular%20Characterisation%20of%20H%26E%20slides&entry.906535625=Benjamin%20Adjadj%20and%20Pierre-Antoine%20Bannier%20and%20Guillaume%20Horent%20and%20Sebastien%20Mandela%20and%20Aurore%20Lyon%20and%20Kathryn%20Schutte%20and%20Ulysse%20Marteau%20and%20Valentin%20Gaury%20and%20Laura%20Dumont%20and%20Thomas%20Mathieu%20and%20Reda%20Belbahri%20and%20Beno%C3%AEt%20Schmauch%20and%20Eric%20Durand%20and%20Katharina%20Von%20Loga%20and%20Lucie%20Gillet&entry.1292438233=%20%20Cell%20detection%2C%20segmentation%20and%20classification%20are%20essential%20for%20analyzing%0Atumor%20microenvironments%20%28TME%29%20on%20hematoxylin%20and%20eosin%20%28H%26E%29%20slides.%20Existing%0Amethods%20suffer%20from%20poor%20performance%20on%20understudied%20cell%20types%20%28rare%20or%20not%0Apresent%20in%20public%20datasets%29%20and%20limited%20cross-domain%20generalization.%20To%20address%0Athese%20shortcomings%2C%20we%20introduce%20HistoPLUS%2C%20a%20state-of-the-art%20model%20for%20cell%0Aanalysis%2C%20trained%20on%20a%20novel%20curated%20pan-cancer%20dataset%20of%20108%2C722%20nuclei%0Acovering%2013%20cell%20types.%20In%20external%20validation%20across%204%20independent%20cohorts%2C%0AHistoPLUS%20outperforms%20current%20state-of-the-art%20models%20in%20detection%20quality%20by%0A5.2%25%20and%20overall%20F1%20classification%20score%20by%2023.7%25%2C%20while%20using%205x%20fewer%0Aparameters.%20Notably%2C%20HistoPLUS%20unlocks%20the%20study%20of%207%20understudied%20cell%20types%0Aand%20brings%20significant%20improvements%20on%208%20of%2013%20cell%20types.%20Moreover%2C%20we%20show%0Athat%20HistoPLUS%20robustly%20transfers%20to%20two%20oncology%20indications%20unseen%20during%0Atraining.%20To%20support%20broader%20TME%20biomarker%20research%2C%20we%20release%20the%20model%0Aweights%20and%20inference%20code%20at%20https%3A//github.com/owkin/histoplus/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09926v2&entry.124074799=Read"},
{"title": "Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image\n  Denoising", "author": "Jin Ye and Jingran Wang and Fengchao Xiong and Jingzhou Chen and Yuntao Qian", "abstract": "  Hyperspectral images (HSIs) play a crucial role in remote sensing but are\noften degraded by complex noise patterns. Ensuring the physical property of the\ndenoised HSIs is vital for robust HSI denoising, giving the rise of deep\nunfolding-based methods. However, these methods map the optimization of a\nphysical model to a learnable network with a predefined depth, which lacks\nconvergence guarantees. In contrast, Deep Equilibrium (DEQ) models treat the\nhidden layers of deep networks as the solution to a fixed-point problem and\nmodels them as infinite-depth networks, naturally consistent with the\noptimization. Under the framework of DEQ, we propose a Deep Equilibrium\nConvolutional Sparse Coding (DECSC) framework that unifies local\nspatial-spectral correlations, nonlocal spatial self-similarities, and global\nspatial consistency for robust HSI denoising. Within the convolutional sparse\ncoding (CSC) framework, we enforce shared 2D convolutional sparse\nrepresentation to ensure global spatial consistency across bands, while\nunshared 3D convolutional sparse representation captures local spatial-spectral\ndetails. To further exploit nonlocal self-similarities, a transformer block is\nembedded after the 2D CSC. Additionally, a detail enhancement module is\nintegrated with the 3D CSC to promote image detail preservation. We formulate\nthe proximal gradient descent of the CSC model as a fixed-point problem and\ntransform the iterative updates into a learnable network architecture within\nthe framework of DEQ. Experimental results demonstrate that our DECSC method\nachieves superior denoising performance compared to state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2508.15553v1", "date": "2025-08-21", "relevancy": 2.1544, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5689}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5338}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Equilibrium%20Convolutional%20Sparse%20Coding%20for%20Hyperspectral%20Image%0A%20%20Denoising&body=Title%3A%20Deep%20Equilibrium%20Convolutional%20Sparse%20Coding%20for%20Hyperspectral%20Image%0A%20%20Denoising%0AAuthor%3A%20Jin%20Ye%20and%20Jingran%20Wang%20and%20Fengchao%20Xiong%20and%20Jingzhou%20Chen%20and%20Yuntao%20Qian%0AAbstract%3A%20%20%20Hyperspectral%20images%20%28HSIs%29%20play%20a%20crucial%20role%20in%20remote%20sensing%20but%20are%0Aoften%20degraded%20by%20complex%20noise%20patterns.%20Ensuring%20the%20physical%20property%20of%20the%0Adenoised%20HSIs%20is%20vital%20for%20robust%20HSI%20denoising%2C%20giving%20the%20rise%20of%20deep%0Aunfolding-based%20methods.%20However%2C%20these%20methods%20map%20the%20optimization%20of%20a%0Aphysical%20model%20to%20a%20learnable%20network%20with%20a%20predefined%20depth%2C%20which%20lacks%0Aconvergence%20guarantees.%20In%20contrast%2C%20Deep%20Equilibrium%20%28DEQ%29%20models%20treat%20the%0Ahidden%20layers%20of%20deep%20networks%20as%20the%20solution%20to%20a%20fixed-point%20problem%20and%0Amodels%20them%20as%20infinite-depth%20networks%2C%20naturally%20consistent%20with%20the%0Aoptimization.%20Under%20the%20framework%20of%20DEQ%2C%20we%20propose%20a%20Deep%20Equilibrium%0AConvolutional%20Sparse%20Coding%20%28DECSC%29%20framework%20that%20unifies%20local%0Aspatial-spectral%20correlations%2C%20nonlocal%20spatial%20self-similarities%2C%20and%20global%0Aspatial%20consistency%20for%20robust%20HSI%20denoising.%20Within%20the%20convolutional%20sparse%0Acoding%20%28CSC%29%20framework%2C%20we%20enforce%20shared%202D%20convolutional%20sparse%0Arepresentation%20to%20ensure%20global%20spatial%20consistency%20across%20bands%2C%20while%0Aunshared%203D%20convolutional%20sparse%20representation%20captures%20local%20spatial-spectral%0Adetails.%20To%20further%20exploit%20nonlocal%20self-similarities%2C%20a%20transformer%20block%20is%0Aembedded%20after%20the%202D%20CSC.%20Additionally%2C%20a%20detail%20enhancement%20module%20is%0Aintegrated%20with%20the%203D%20CSC%20to%20promote%20image%20detail%20preservation.%20We%20formulate%0Athe%20proximal%20gradient%20descent%20of%20the%20CSC%20model%20as%20a%20fixed-point%20problem%20and%0Atransform%20the%20iterative%20updates%20into%20a%20learnable%20network%20architecture%20within%0Athe%20framework%20of%20DEQ.%20Experimental%20results%20demonstrate%20that%20our%20DECSC%20method%0Aachieves%20superior%20denoising%20performance%20compared%20to%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Equilibrium%2520Convolutional%2520Sparse%2520Coding%2520for%2520Hyperspectral%2520Image%250A%2520%2520Denoising%26entry.906535625%3DJin%2520Ye%2520and%2520Jingran%2520Wang%2520and%2520Fengchao%2520Xiong%2520and%2520Jingzhou%2520Chen%2520and%2520Yuntao%2520Qian%26entry.1292438233%3D%2520%2520Hyperspectral%2520images%2520%2528HSIs%2529%2520play%2520a%2520crucial%2520role%2520in%2520remote%2520sensing%2520but%2520are%250Aoften%2520degraded%2520by%2520complex%2520noise%2520patterns.%2520Ensuring%2520the%2520physical%2520property%2520of%2520the%250Adenoised%2520HSIs%2520is%2520vital%2520for%2520robust%2520HSI%2520denoising%252C%2520giving%2520the%2520rise%2520of%2520deep%250Aunfolding-based%2520methods.%2520However%252C%2520these%2520methods%2520map%2520the%2520optimization%2520of%2520a%250Aphysical%2520model%2520to%2520a%2520learnable%2520network%2520with%2520a%2520predefined%2520depth%252C%2520which%2520lacks%250Aconvergence%2520guarantees.%2520In%2520contrast%252C%2520Deep%2520Equilibrium%2520%2528DEQ%2529%2520models%2520treat%2520the%250Ahidden%2520layers%2520of%2520deep%2520networks%2520as%2520the%2520solution%2520to%2520a%2520fixed-point%2520problem%2520and%250Amodels%2520them%2520as%2520infinite-depth%2520networks%252C%2520naturally%2520consistent%2520with%2520the%250Aoptimization.%2520Under%2520the%2520framework%2520of%2520DEQ%252C%2520we%2520propose%2520a%2520Deep%2520Equilibrium%250AConvolutional%2520Sparse%2520Coding%2520%2528DECSC%2529%2520framework%2520that%2520unifies%2520local%250Aspatial-spectral%2520correlations%252C%2520nonlocal%2520spatial%2520self-similarities%252C%2520and%2520global%250Aspatial%2520consistency%2520for%2520robust%2520HSI%2520denoising.%2520Within%2520the%2520convolutional%2520sparse%250Acoding%2520%2528CSC%2529%2520framework%252C%2520we%2520enforce%2520shared%25202D%2520convolutional%2520sparse%250Arepresentation%2520to%2520ensure%2520global%2520spatial%2520consistency%2520across%2520bands%252C%2520while%250Aunshared%25203D%2520convolutional%2520sparse%2520representation%2520captures%2520local%2520spatial-spectral%250Adetails.%2520To%2520further%2520exploit%2520nonlocal%2520self-similarities%252C%2520a%2520transformer%2520block%2520is%250Aembedded%2520after%2520the%25202D%2520CSC.%2520Additionally%252C%2520a%2520detail%2520enhancement%2520module%2520is%250Aintegrated%2520with%2520the%25203D%2520CSC%2520to%2520promote%2520image%2520detail%2520preservation.%2520We%2520formulate%250Athe%2520proximal%2520gradient%2520descent%2520of%2520the%2520CSC%2520model%2520as%2520a%2520fixed-point%2520problem%2520and%250Atransform%2520the%2520iterative%2520updates%2520into%2520a%2520learnable%2520network%2520architecture%2520within%250Athe%2520framework%2520of%2520DEQ.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520DECSC%2520method%250Aachieves%2520superior%2520denoising%2520performance%2520compared%2520to%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Equilibrium%20Convolutional%20Sparse%20Coding%20for%20Hyperspectral%20Image%0A%20%20Denoising&entry.906535625=Jin%20Ye%20and%20Jingran%20Wang%20and%20Fengchao%20Xiong%20and%20Jingzhou%20Chen%20and%20Yuntao%20Qian&entry.1292438233=%20%20Hyperspectral%20images%20%28HSIs%29%20play%20a%20crucial%20role%20in%20remote%20sensing%20but%20are%0Aoften%20degraded%20by%20complex%20noise%20patterns.%20Ensuring%20the%20physical%20property%20of%20the%0Adenoised%20HSIs%20is%20vital%20for%20robust%20HSI%20denoising%2C%20giving%20the%20rise%20of%20deep%0Aunfolding-based%20methods.%20However%2C%20these%20methods%20map%20the%20optimization%20of%20a%0Aphysical%20model%20to%20a%20learnable%20network%20with%20a%20predefined%20depth%2C%20which%20lacks%0Aconvergence%20guarantees.%20In%20contrast%2C%20Deep%20Equilibrium%20%28DEQ%29%20models%20treat%20the%0Ahidden%20layers%20of%20deep%20networks%20as%20the%20solution%20to%20a%20fixed-point%20problem%20and%0Amodels%20them%20as%20infinite-depth%20networks%2C%20naturally%20consistent%20with%20the%0Aoptimization.%20Under%20the%20framework%20of%20DEQ%2C%20we%20propose%20a%20Deep%20Equilibrium%0AConvolutional%20Sparse%20Coding%20%28DECSC%29%20framework%20that%20unifies%20local%0Aspatial-spectral%20correlations%2C%20nonlocal%20spatial%20self-similarities%2C%20and%20global%0Aspatial%20consistency%20for%20robust%20HSI%20denoising.%20Within%20the%20convolutional%20sparse%0Acoding%20%28CSC%29%20framework%2C%20we%20enforce%20shared%202D%20convolutional%20sparse%0Arepresentation%20to%20ensure%20global%20spatial%20consistency%20across%20bands%2C%20while%0Aunshared%203D%20convolutional%20sparse%20representation%20captures%20local%20spatial-spectral%0Adetails.%20To%20further%20exploit%20nonlocal%20self-similarities%2C%20a%20transformer%20block%20is%0Aembedded%20after%20the%202D%20CSC.%20Additionally%2C%20a%20detail%20enhancement%20module%20is%0Aintegrated%20with%20the%203D%20CSC%20to%20promote%20image%20detail%20preservation.%20We%20formulate%0Athe%20proximal%20gradient%20descent%20of%20the%20CSC%20model%20as%20a%20fixed-point%20problem%20and%0Atransform%20the%20iterative%20updates%20into%20a%20learnable%20network%20architecture%20within%0Athe%20framework%20of%20DEQ.%20Experimental%20results%20demonstrate%20that%20our%20DECSC%20method%0Aachieves%20superior%20denoising%20performance%20compared%20to%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15553v1&entry.124074799=Read"},
{"title": "Cooperative SGD with Dynamic Mixing Matrices", "author": "Soumya Sarkar and Shweta Jain", "abstract": "  One of the most common methods to train machine learning algorithms today is\nthe stochastic gradient descent (SGD). In a distributed setting, SGD-based\nalgorithms have been shown to converge theoretically under specific\ncircumstances. A substantial number of works in the distributed SGD setting\nassume a fixed topology for the edge devices. These papers also assume that the\ncontribution of nodes to the global model is uniform. However, experiments have\nshown that such assumptions are suboptimal and a non uniform aggregation\nstrategy coupled with a dynamically shifting topology and client selection can\nsignificantly improve the performance of such models. This paper details a\nunified framework that covers several Local-Update SGD-based distributed\nalgorithms with dynamic topologies and provides improved or matching\ntheoretical guarantees on convergence compared to existing work.\n", "link": "http://arxiv.org/abs/2508.14565v2", "date": "2025-08-21", "relevancy": 2.1495, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5503}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5315}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cooperative%20SGD%20with%20Dynamic%20Mixing%20Matrices&body=Title%3A%20Cooperative%20SGD%20with%20Dynamic%20Mixing%20Matrices%0AAuthor%3A%20Soumya%20Sarkar%20and%20Shweta%20Jain%0AAbstract%3A%20%20%20One%20of%20the%20most%20common%20methods%20to%20train%20machine%20learning%20algorithms%20today%20is%0Athe%20stochastic%20gradient%20descent%20%28SGD%29.%20In%20a%20distributed%20setting%2C%20SGD-based%0Aalgorithms%20have%20been%20shown%20to%20converge%20theoretically%20under%20specific%0Acircumstances.%20A%20substantial%20number%20of%20works%20in%20the%20distributed%20SGD%20setting%0Aassume%20a%20fixed%20topology%20for%20the%20edge%20devices.%20These%20papers%20also%20assume%20that%20the%0Acontribution%20of%20nodes%20to%20the%20global%20model%20is%20uniform.%20However%2C%20experiments%20have%0Ashown%20that%20such%20assumptions%20are%20suboptimal%20and%20a%20non%20uniform%20aggregation%0Astrategy%20coupled%20with%20a%20dynamically%20shifting%20topology%20and%20client%20selection%20can%0Asignificantly%20improve%20the%20performance%20of%20such%20models.%20This%20paper%20details%20a%0Aunified%20framework%20that%20covers%20several%20Local-Update%20SGD-based%20distributed%0Aalgorithms%20with%20dynamic%20topologies%20and%20provides%20improved%20or%20matching%0Atheoretical%20guarantees%20on%20convergence%20compared%20to%20existing%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14565v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCooperative%2520SGD%2520with%2520Dynamic%2520Mixing%2520Matrices%26entry.906535625%3DSoumya%2520Sarkar%2520and%2520Shweta%2520Jain%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520most%2520common%2520methods%2520to%2520train%2520machine%2520learning%2520algorithms%2520today%2520is%250Athe%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529.%2520In%2520a%2520distributed%2520setting%252C%2520SGD-based%250Aalgorithms%2520have%2520been%2520shown%2520to%2520converge%2520theoretically%2520under%2520specific%250Acircumstances.%2520A%2520substantial%2520number%2520of%2520works%2520in%2520the%2520distributed%2520SGD%2520setting%250Aassume%2520a%2520fixed%2520topology%2520for%2520the%2520edge%2520devices.%2520These%2520papers%2520also%2520assume%2520that%2520the%250Acontribution%2520of%2520nodes%2520to%2520the%2520global%2520model%2520is%2520uniform.%2520However%252C%2520experiments%2520have%250Ashown%2520that%2520such%2520assumptions%2520are%2520suboptimal%2520and%2520a%2520non%2520uniform%2520aggregation%250Astrategy%2520coupled%2520with%2520a%2520dynamically%2520shifting%2520topology%2520and%2520client%2520selection%2520can%250Asignificantly%2520improve%2520the%2520performance%2520of%2520such%2520models.%2520This%2520paper%2520details%2520a%250Aunified%2520framework%2520that%2520covers%2520several%2520Local-Update%2520SGD-based%2520distributed%250Aalgorithms%2520with%2520dynamic%2520topologies%2520and%2520provides%2520improved%2520or%2520matching%250Atheoretical%2520guarantees%2520on%2520convergence%2520compared%2520to%2520existing%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14565v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cooperative%20SGD%20with%20Dynamic%20Mixing%20Matrices&entry.906535625=Soumya%20Sarkar%20and%20Shweta%20Jain&entry.1292438233=%20%20One%20of%20the%20most%20common%20methods%20to%20train%20machine%20learning%20algorithms%20today%20is%0Athe%20stochastic%20gradient%20descent%20%28SGD%29.%20In%20a%20distributed%20setting%2C%20SGD-based%0Aalgorithms%20have%20been%20shown%20to%20converge%20theoretically%20under%20specific%0Acircumstances.%20A%20substantial%20number%20of%20works%20in%20the%20distributed%20SGD%20setting%0Aassume%20a%20fixed%20topology%20for%20the%20edge%20devices.%20These%20papers%20also%20assume%20that%20the%0Acontribution%20of%20nodes%20to%20the%20global%20model%20is%20uniform.%20However%2C%20experiments%20have%0Ashown%20that%20such%20assumptions%20are%20suboptimal%20and%20a%20non%20uniform%20aggregation%0Astrategy%20coupled%20with%20a%20dynamically%20shifting%20topology%20and%20client%20selection%20can%0Asignificantly%20improve%20the%20performance%20of%20such%20models.%20This%20paper%20details%20a%0Aunified%20framework%20that%20covers%20several%20Local-Update%20SGD-based%20distributed%0Aalgorithms%20with%20dynamic%20topologies%20and%20provides%20improved%20or%20matching%0Atheoretical%20guarantees%20on%20convergence%20compared%20to%20existing%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14565v2&entry.124074799=Read"},
{"title": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video\n  Understanding", "author": "Yanlai Yang and Zhuokai Zhao and Satya Narayan Shukla and Aashu Singh and Shlok Kumar Mishra and Lizhu Zhang and Mengye Ren", "abstract": "  Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches.\n", "link": "http://arxiv.org/abs/2508.15717v1", "date": "2025-08-21", "relevancy": 2.1266, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5391}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5391}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StreamMem%3A%20Query-Agnostic%20KV%20Cache%20Memory%20for%20Streaming%20Video%0A%20%20Understanding&body=Title%3A%20StreamMem%3A%20Query-Agnostic%20KV%20Cache%20Memory%20for%20Streaming%20Video%0A%20%20Understanding%0AAuthor%3A%20Yanlai%20Yang%20and%20Zhuokai%20Zhao%20and%20Satya%20Narayan%20Shukla%20and%20Aashu%20Singh%20and%20Shlok%20Kumar%20Mishra%20and%20Lizhu%20Zhang%20and%20Mengye%20Ren%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20made%20significant%20progress%20in%0Avisual-language%20reasoning%2C%20but%20their%20ability%20to%20efficiently%20handle%20long%20videos%0Aremains%20limited.%20Despite%20recent%20advances%20in%20long-context%20MLLMs%2C%20storing%20and%0Aattending%20to%20the%20key-value%20%28KV%29%20cache%20for%20long%20visual%20contexts%20incurs%0Asubstantial%20memory%20and%20computational%20overhead.%20Existing%20visual%20compression%0Amethods%20require%20either%20encoding%20the%20entire%20visual%20context%20before%20compression%20or%0Ahaving%20access%20to%20the%20questions%20in%20advance%2C%20which%20is%20impractical%20for%20long%20video%0Aunderstanding%20and%20multi-turn%20conversational%20settings.%20In%20this%20work%2C%20we%20propose%0AStreamMem%2C%20a%20query-agnostic%20KV%20cache%20memory%20mechanism%20for%20streaming%20video%0Aunderstanding.%20Specifically%2C%20StreamMem%20encodes%20new%20video%20frames%20in%20a%20streaming%0Amanner%2C%20compressing%20the%20KV%20cache%20using%20attention%20scores%20between%20visual%20tokens%0Aand%20generic%20query%20tokens%2C%20while%20maintaining%20a%20fixed-size%20KV%20memory%20to%20enable%0Aefficient%20question%20answering%20%28QA%29%20in%20memory-constrained%2C%20long-video%20scenarios.%0AEvaluation%20on%20three%20long%20video%20understanding%20and%20two%20streaming%20video%20question%0Aanswering%20benchmarks%20shows%20that%20StreamMem%20achieves%20state-of-the-art%20performance%0Ain%20query-agnostic%20KV%20cache%20compression%20and%20is%20competitive%20with%20query-aware%0Acompression%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamMem%253A%2520Query-Agnostic%2520KV%2520Cache%2520Memory%2520for%2520Streaming%2520Video%250A%2520%2520Understanding%26entry.906535625%3DYanlai%2520Yang%2520and%2520Zhuokai%2520Zhao%2520and%2520Satya%2520Narayan%2520Shukla%2520and%2520Aashu%2520Singh%2520and%2520Shlok%2520Kumar%2520Mishra%2520and%2520Lizhu%2520Zhang%2520and%2520Mengye%2520Ren%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520made%2520significant%2520progress%2520in%250Avisual-language%2520reasoning%252C%2520but%2520their%2520ability%2520to%2520efficiently%2520handle%2520long%2520videos%250Aremains%2520limited.%2520Despite%2520recent%2520advances%2520in%2520long-context%2520MLLMs%252C%2520storing%2520and%250Aattending%2520to%2520the%2520key-value%2520%2528KV%2529%2520cache%2520for%2520long%2520visual%2520contexts%2520incurs%250Asubstantial%2520memory%2520and%2520computational%2520overhead.%2520Existing%2520visual%2520compression%250Amethods%2520require%2520either%2520encoding%2520the%2520entire%2520visual%2520context%2520before%2520compression%2520or%250Ahaving%2520access%2520to%2520the%2520questions%2520in%2520advance%252C%2520which%2520is%2520impractical%2520for%2520long%2520video%250Aunderstanding%2520and%2520multi-turn%2520conversational%2520settings.%2520In%2520this%2520work%252C%2520we%2520propose%250AStreamMem%252C%2520a%2520query-agnostic%2520KV%2520cache%2520memory%2520mechanism%2520for%2520streaming%2520video%250Aunderstanding.%2520Specifically%252C%2520StreamMem%2520encodes%2520new%2520video%2520frames%2520in%2520a%2520streaming%250Amanner%252C%2520compressing%2520the%2520KV%2520cache%2520using%2520attention%2520scores%2520between%2520visual%2520tokens%250Aand%2520generic%2520query%2520tokens%252C%2520while%2520maintaining%2520a%2520fixed-size%2520KV%2520memory%2520to%2520enable%250Aefficient%2520question%2520answering%2520%2528QA%2529%2520in%2520memory-constrained%252C%2520long-video%2520scenarios.%250AEvaluation%2520on%2520three%2520long%2520video%2520understanding%2520and%2520two%2520streaming%2520video%2520question%250Aanswering%2520benchmarks%2520shows%2520that%2520StreamMem%2520achieves%2520state-of-the-art%2520performance%250Ain%2520query-agnostic%2520KV%2520cache%2520compression%2520and%2520is%2520competitive%2520with%2520query-aware%250Acompression%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreamMem%3A%20Query-Agnostic%20KV%20Cache%20Memory%20for%20Streaming%20Video%0A%20%20Understanding&entry.906535625=Yanlai%20Yang%20and%20Zhuokai%20Zhao%20and%20Satya%20Narayan%20Shukla%20and%20Aashu%20Singh%20and%20Shlok%20Kumar%20Mishra%20and%20Lizhu%20Zhang%20and%20Mengye%20Ren&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20made%20significant%20progress%20in%0Avisual-language%20reasoning%2C%20but%20their%20ability%20to%20efficiently%20handle%20long%20videos%0Aremains%20limited.%20Despite%20recent%20advances%20in%20long-context%20MLLMs%2C%20storing%20and%0Aattending%20to%20the%20key-value%20%28KV%29%20cache%20for%20long%20visual%20contexts%20incurs%0Asubstantial%20memory%20and%20computational%20overhead.%20Existing%20visual%20compression%0Amethods%20require%20either%20encoding%20the%20entire%20visual%20context%20before%20compression%20or%0Ahaving%20access%20to%20the%20questions%20in%20advance%2C%20which%20is%20impractical%20for%20long%20video%0Aunderstanding%20and%20multi-turn%20conversational%20settings.%20In%20this%20work%2C%20we%20propose%0AStreamMem%2C%20a%20query-agnostic%20KV%20cache%20memory%20mechanism%20for%20streaming%20video%0Aunderstanding.%20Specifically%2C%20StreamMem%20encodes%20new%20video%20frames%20in%20a%20streaming%0Amanner%2C%20compressing%20the%20KV%20cache%20using%20attention%20scores%20between%20visual%20tokens%0Aand%20generic%20query%20tokens%2C%20while%20maintaining%20a%20fixed-size%20KV%20memory%20to%20enable%0Aefficient%20question%20answering%20%28QA%29%20in%20memory-constrained%2C%20long-video%20scenarios.%0AEvaluation%20on%20three%20long%20video%20understanding%20and%20two%20streaming%20video%20question%0Aanswering%20benchmarks%20shows%20that%20StreamMem%20achieves%20state-of-the-art%20performance%0Ain%20query-agnostic%20KV%20cache%20compression%20and%20is%20competitive%20with%20query-aware%0Acompression%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15717v1&entry.124074799=Read"},
{"title": "Automatic Curriculum Design for Zero-Shot Human-AI Coordination", "author": "Won-Sang You and Tae-Gwan Ha and Seo-Young Lee and Kyung-Joong Kim", "abstract": "  Zero-shot human-AI coordination is the training of an ego-agent to coordinate\nwith humans without human data. Most studies on zero-shot human-AI coordination\nhave focused on enhancing the ego-agent's coordination ability in a given\nenvironment without considering the issue of generalization to unseen\nenvironments. Real-world applications of zero-shot human-AI coordination should\nconsider unpredictable environmental changes and the varying coordination\nability of co-players depending on the environment. Previously, the multi-agent\nUED (Unsupervised Environment Design) approach has investigated these\nchallenges by jointly considering environmental changes and co-player policy in\ncompetitive two-player AI-AI scenarios. In this paper, our study extends a\nmulti-agent UED approach to zero-shot human-AI coordination. We propose a\nutility function and co-player sampling for a zero-shot human-AI coordination\nsetting that helps train the ego-agent to coordinate with humans more\neffectively than a previous multi-agent UED approach. The zero-shot human-AI\ncoordination performance was evaluated in the Overcooked-AI environment, using\nhuman proxy agents and real humans. Our method outperforms other baseline\nmodels and achieves high performance in human-AI coordination tasks in unseen\nenvironments. The source code is available at\nhttps://github.com/Uwonsang/ACD_Human-AI\n", "link": "http://arxiv.org/abs/2503.07275v2", "date": "2025-08-21", "relevancy": 2.1236, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5498}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5372}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Curriculum%20Design%20for%20Zero-Shot%20Human-AI%20Coordination&body=Title%3A%20Automatic%20Curriculum%20Design%20for%20Zero-Shot%20Human-AI%20Coordination%0AAuthor%3A%20Won-Sang%20You%20and%20Tae-Gwan%20Ha%20and%20Seo-Young%20Lee%20and%20Kyung-Joong%20Kim%0AAbstract%3A%20%20%20Zero-shot%20human-AI%20coordination%20is%20the%20training%20of%20an%20ego-agent%20to%20coordinate%0Awith%20humans%20without%20human%20data.%20Most%20studies%20on%20zero-shot%20human-AI%20coordination%0Ahave%20focused%20on%20enhancing%20the%20ego-agent%27s%20coordination%20ability%20in%20a%20given%0Aenvironment%20without%20considering%20the%20issue%20of%20generalization%20to%20unseen%0Aenvironments.%20Real-world%20applications%20of%20zero-shot%20human-AI%20coordination%20should%0Aconsider%20unpredictable%20environmental%20changes%20and%20the%20varying%20coordination%0Aability%20of%20co-players%20depending%20on%20the%20environment.%20Previously%2C%20the%20multi-agent%0AUED%20%28Unsupervised%20Environment%20Design%29%20approach%20has%20investigated%20these%0Achallenges%20by%20jointly%20considering%20environmental%20changes%20and%20co-player%20policy%20in%0Acompetitive%20two-player%20AI-AI%20scenarios.%20In%20this%20paper%2C%20our%20study%20extends%20a%0Amulti-agent%20UED%20approach%20to%20zero-shot%20human-AI%20coordination.%20We%20propose%20a%0Autility%20function%20and%20co-player%20sampling%20for%20a%20zero-shot%20human-AI%20coordination%0Asetting%20that%20helps%20train%20the%20ego-agent%20to%20coordinate%20with%20humans%20more%0Aeffectively%20than%20a%20previous%20multi-agent%20UED%20approach.%20The%20zero-shot%20human-AI%0Acoordination%20performance%20was%20evaluated%20in%20the%20Overcooked-AI%20environment%2C%20using%0Ahuman%20proxy%20agents%20and%20real%20humans.%20Our%20method%20outperforms%20other%20baseline%0Amodels%20and%20achieves%20high%20performance%20in%20human-AI%20coordination%20tasks%20in%20unseen%0Aenvironments.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/Uwonsang/ACD_Human-AI%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07275v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Curriculum%2520Design%2520for%2520Zero-Shot%2520Human-AI%2520Coordination%26entry.906535625%3DWon-Sang%2520You%2520and%2520Tae-Gwan%2520Ha%2520and%2520Seo-Young%2520Lee%2520and%2520Kyung-Joong%2520Kim%26entry.1292438233%3D%2520%2520Zero-shot%2520human-AI%2520coordination%2520is%2520the%2520training%2520of%2520an%2520ego-agent%2520to%2520coordinate%250Awith%2520humans%2520without%2520human%2520data.%2520Most%2520studies%2520on%2520zero-shot%2520human-AI%2520coordination%250Ahave%2520focused%2520on%2520enhancing%2520the%2520ego-agent%2527s%2520coordination%2520ability%2520in%2520a%2520given%250Aenvironment%2520without%2520considering%2520the%2520issue%2520of%2520generalization%2520to%2520unseen%250Aenvironments.%2520Real-world%2520applications%2520of%2520zero-shot%2520human-AI%2520coordination%2520should%250Aconsider%2520unpredictable%2520environmental%2520changes%2520and%2520the%2520varying%2520coordination%250Aability%2520of%2520co-players%2520depending%2520on%2520the%2520environment.%2520Previously%252C%2520the%2520multi-agent%250AUED%2520%2528Unsupervised%2520Environment%2520Design%2529%2520approach%2520has%2520investigated%2520these%250Achallenges%2520by%2520jointly%2520considering%2520environmental%2520changes%2520and%2520co-player%2520policy%2520in%250Acompetitive%2520two-player%2520AI-AI%2520scenarios.%2520In%2520this%2520paper%252C%2520our%2520study%2520extends%2520a%250Amulti-agent%2520UED%2520approach%2520to%2520zero-shot%2520human-AI%2520coordination.%2520We%2520propose%2520a%250Autility%2520function%2520and%2520co-player%2520sampling%2520for%2520a%2520zero-shot%2520human-AI%2520coordination%250Asetting%2520that%2520helps%2520train%2520the%2520ego-agent%2520to%2520coordinate%2520with%2520humans%2520more%250Aeffectively%2520than%2520a%2520previous%2520multi-agent%2520UED%2520approach.%2520The%2520zero-shot%2520human-AI%250Acoordination%2520performance%2520was%2520evaluated%2520in%2520the%2520Overcooked-AI%2520environment%252C%2520using%250Ahuman%2520proxy%2520agents%2520and%2520real%2520humans.%2520Our%2520method%2520outperforms%2520other%2520baseline%250Amodels%2520and%2520achieves%2520high%2520performance%2520in%2520human-AI%2520coordination%2520tasks%2520in%2520unseen%250Aenvironments.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Uwonsang/ACD_Human-AI%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07275v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Curriculum%20Design%20for%20Zero-Shot%20Human-AI%20Coordination&entry.906535625=Won-Sang%20You%20and%20Tae-Gwan%20Ha%20and%20Seo-Young%20Lee%20and%20Kyung-Joong%20Kim&entry.1292438233=%20%20Zero-shot%20human-AI%20coordination%20is%20the%20training%20of%20an%20ego-agent%20to%20coordinate%0Awith%20humans%20without%20human%20data.%20Most%20studies%20on%20zero-shot%20human-AI%20coordination%0Ahave%20focused%20on%20enhancing%20the%20ego-agent%27s%20coordination%20ability%20in%20a%20given%0Aenvironment%20without%20considering%20the%20issue%20of%20generalization%20to%20unseen%0Aenvironments.%20Real-world%20applications%20of%20zero-shot%20human-AI%20coordination%20should%0Aconsider%20unpredictable%20environmental%20changes%20and%20the%20varying%20coordination%0Aability%20of%20co-players%20depending%20on%20the%20environment.%20Previously%2C%20the%20multi-agent%0AUED%20%28Unsupervised%20Environment%20Design%29%20approach%20has%20investigated%20these%0Achallenges%20by%20jointly%20considering%20environmental%20changes%20and%20co-player%20policy%20in%0Acompetitive%20two-player%20AI-AI%20scenarios.%20In%20this%20paper%2C%20our%20study%20extends%20a%0Amulti-agent%20UED%20approach%20to%20zero-shot%20human-AI%20coordination.%20We%20propose%20a%0Autility%20function%20and%20co-player%20sampling%20for%20a%20zero-shot%20human-AI%20coordination%0Asetting%20that%20helps%20train%20the%20ego-agent%20to%20coordinate%20with%20humans%20more%0Aeffectively%20than%20a%20previous%20multi-agent%20UED%20approach.%20The%20zero-shot%20human-AI%0Acoordination%20performance%20was%20evaluated%20in%20the%20Overcooked-AI%20environment%2C%20using%0Ahuman%20proxy%20agents%20and%20real%20humans.%20Our%20method%20outperforms%20other%20baseline%0Amodels%20and%20achieves%20high%20performance%20in%20human-AI%20coordination%20tasks%20in%20unseen%0Aenvironments.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/Uwonsang/ACD_Human-AI%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07275v2&entry.124074799=Read"},
{"title": "GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality\n  Tagging, and Management of Synthetic Data for SFT and DPO", "author": "Bidyapati Pradhan and Surajit Dasgupta and Amit Kumar Saha and Omkar Anustoop and Sriram Puttagunta and Vipul Mittal and Gopal Sarda", "abstract": "  The advancement of large language models (LLMs) is critically dependent on\nthe availability of high-quality datasets for Supervised Fine-Tuning (SFT),\nalignment tasks like Direct Preference Optimization (DPO), etc. In this work,\nwe present a comprehensive synthetic data generation framework that facilitates\nscalable, configurable, and high-fidelity generation of synthetic data tailored\nfor these training paradigms. Our approach employs a modular and\nconfiguration-based pipeline capable of modeling complex dialogue flows with\nminimal manual intervention. This framework uses a dual-stage quality tagging\nmechanism, combining heuristic rules and LLM-based evaluations, to\nautomatically filter and score data extracted from OASST-formatted\nconversations, ensuring the curation of high-quality dialogue samples. The\nresulting datasets are structured under a flexible schema supporting both SFT\nand DPO use cases, enabling seamless integration into diverse training\nworkflows. Together, these innovations offer a robust solution for generating\nand managing synthetic conversational data at scale, significantly reducing the\noverhead of data preparation in LLM training pipelines.\n", "link": "http://arxiv.org/abs/2508.15432v1", "date": "2025-08-21", "relevancy": 2.1005, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5318}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5209}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraSP%3A%20A%20Unified%20Graph-Based%20Framework%20for%20Scalable%20Generation%2C%20Quality%0A%20%20Tagging%2C%20and%20Management%20of%20Synthetic%20Data%20for%20SFT%20and%20DPO&body=Title%3A%20GraSP%3A%20A%20Unified%20Graph-Based%20Framework%20for%20Scalable%20Generation%2C%20Quality%0A%20%20Tagging%2C%20and%20Management%20of%20Synthetic%20Data%20for%20SFT%20and%20DPO%0AAuthor%3A%20Bidyapati%20Pradhan%20and%20Surajit%20Dasgupta%20and%20Amit%20Kumar%20Saha%20and%20Omkar%20Anustoop%20and%20Sriram%20Puttagunta%20and%20Vipul%20Mittal%20and%20Gopal%20Sarda%0AAbstract%3A%20%20%20The%20advancement%20of%20large%20language%20models%20%28LLMs%29%20is%20critically%20dependent%20on%0Athe%20availability%20of%20high-quality%20datasets%20for%20Supervised%20Fine-Tuning%20%28SFT%29%2C%0Aalignment%20tasks%20like%20Direct%20Preference%20Optimization%20%28DPO%29%2C%20etc.%20In%20this%20work%2C%0Awe%20present%20a%20comprehensive%20synthetic%20data%20generation%20framework%20that%20facilitates%0Ascalable%2C%20configurable%2C%20and%20high-fidelity%20generation%20of%20synthetic%20data%20tailored%0Afor%20these%20training%20paradigms.%20Our%20approach%20employs%20a%20modular%20and%0Aconfiguration-based%20pipeline%20capable%20of%20modeling%20complex%20dialogue%20flows%20with%0Aminimal%20manual%20intervention.%20This%20framework%20uses%20a%20dual-stage%20quality%20tagging%0Amechanism%2C%20combining%20heuristic%20rules%20and%20LLM-based%20evaluations%2C%20to%0Aautomatically%20filter%20and%20score%20data%20extracted%20from%20OASST-formatted%0Aconversations%2C%20ensuring%20the%20curation%20of%20high-quality%20dialogue%20samples.%20The%0Aresulting%20datasets%20are%20structured%20under%20a%20flexible%20schema%20supporting%20both%20SFT%0Aand%20DPO%20use%20cases%2C%20enabling%20seamless%20integration%20into%20diverse%20training%0Aworkflows.%20Together%2C%20these%20innovations%20offer%20a%20robust%20solution%20for%20generating%0Aand%20managing%20synthetic%20conversational%20data%20at%20scale%2C%20significantly%20reducing%20the%0Aoverhead%20of%20data%20preparation%20in%20LLM%20training%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraSP%253A%2520A%2520Unified%2520Graph-Based%2520Framework%2520for%2520Scalable%2520Generation%252C%2520Quality%250A%2520%2520Tagging%252C%2520and%2520Management%2520of%2520Synthetic%2520Data%2520for%2520SFT%2520and%2520DPO%26entry.906535625%3DBidyapati%2520Pradhan%2520and%2520Surajit%2520Dasgupta%2520and%2520Amit%2520Kumar%2520Saha%2520and%2520Omkar%2520Anustoop%2520and%2520Sriram%2520Puttagunta%2520and%2520Vipul%2520Mittal%2520and%2520Gopal%2520Sarda%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520critically%2520dependent%2520on%250Athe%2520availability%2520of%2520high-quality%2520datasets%2520for%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%252C%250Aalignment%2520tasks%2520like%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%252C%2520etc.%2520In%2520this%2520work%252C%250Awe%2520present%2520a%2520comprehensive%2520synthetic%2520data%2520generation%2520framework%2520that%2520facilitates%250Ascalable%252C%2520configurable%252C%2520and%2520high-fidelity%2520generation%2520of%2520synthetic%2520data%2520tailored%250Afor%2520these%2520training%2520paradigms.%2520Our%2520approach%2520employs%2520a%2520modular%2520and%250Aconfiguration-based%2520pipeline%2520capable%2520of%2520modeling%2520complex%2520dialogue%2520flows%2520with%250Aminimal%2520manual%2520intervention.%2520This%2520framework%2520uses%2520a%2520dual-stage%2520quality%2520tagging%250Amechanism%252C%2520combining%2520heuristic%2520rules%2520and%2520LLM-based%2520evaluations%252C%2520to%250Aautomatically%2520filter%2520and%2520score%2520data%2520extracted%2520from%2520OASST-formatted%250Aconversations%252C%2520ensuring%2520the%2520curation%2520of%2520high-quality%2520dialogue%2520samples.%2520The%250Aresulting%2520datasets%2520are%2520structured%2520under%2520a%2520flexible%2520schema%2520supporting%2520both%2520SFT%250Aand%2520DPO%2520use%2520cases%252C%2520enabling%2520seamless%2520integration%2520into%2520diverse%2520training%250Aworkflows.%2520Together%252C%2520these%2520innovations%2520offer%2520a%2520robust%2520solution%2520for%2520generating%250Aand%2520managing%2520synthetic%2520conversational%2520data%2520at%2520scale%252C%2520significantly%2520reducing%2520the%250Aoverhead%2520of%2520data%2520preparation%2520in%2520LLM%2520training%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraSP%3A%20A%20Unified%20Graph-Based%20Framework%20for%20Scalable%20Generation%2C%20Quality%0A%20%20Tagging%2C%20and%20Management%20of%20Synthetic%20Data%20for%20SFT%20and%20DPO&entry.906535625=Bidyapati%20Pradhan%20and%20Surajit%20Dasgupta%20and%20Amit%20Kumar%20Saha%20and%20Omkar%20Anustoop%20and%20Sriram%20Puttagunta%20and%20Vipul%20Mittal%20and%20Gopal%20Sarda&entry.1292438233=%20%20The%20advancement%20of%20large%20language%20models%20%28LLMs%29%20is%20critically%20dependent%20on%0Athe%20availability%20of%20high-quality%20datasets%20for%20Supervised%20Fine-Tuning%20%28SFT%29%2C%0Aalignment%20tasks%20like%20Direct%20Preference%20Optimization%20%28DPO%29%2C%20etc.%20In%20this%20work%2C%0Awe%20present%20a%20comprehensive%20synthetic%20data%20generation%20framework%20that%20facilitates%0Ascalable%2C%20configurable%2C%20and%20high-fidelity%20generation%20of%20synthetic%20data%20tailored%0Afor%20these%20training%20paradigms.%20Our%20approach%20employs%20a%20modular%20and%0Aconfiguration-based%20pipeline%20capable%20of%20modeling%20complex%20dialogue%20flows%20with%0Aminimal%20manual%20intervention.%20This%20framework%20uses%20a%20dual-stage%20quality%20tagging%0Amechanism%2C%20combining%20heuristic%20rules%20and%20LLM-based%20evaluations%2C%20to%0Aautomatically%20filter%20and%20score%20data%20extracted%20from%20OASST-formatted%0Aconversations%2C%20ensuring%20the%20curation%20of%20high-quality%20dialogue%20samples.%20The%0Aresulting%20datasets%20are%20structured%20under%20a%20flexible%20schema%20supporting%20both%20SFT%0Aand%20DPO%20use%20cases%2C%20enabling%20seamless%20integration%20into%20diverse%20training%0Aworkflows.%20Together%2C%20these%20innovations%20offer%20a%20robust%20solution%20for%20generating%0Aand%20managing%20synthetic%20conversational%20data%20at%20scale%2C%20significantly%20reducing%20the%0Aoverhead%20of%20data%20preparation%20in%20LLM%20training%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15432v1&entry.124074799=Read"},
{"title": "Bridging Generalization and Personalization in Wearable Human Activity\n  Recognition via On-Device Few-Shot Learning", "author": "Pixi Kang and Julian Moosmann and Mengxi Liu and Bo Zhou and Michele Magno and Paul Lukowicz and Sizhen Bian", "abstract": "  Human Activity Recognition (HAR) using wearable devices has advanced\nsignificantly in recent years, yet its generalization remains limited when\nmodels are deployed to new users. This degradation in performance is primarily\ndue to user-induced concept drift (UICD), highlighting the importance of\nefficient personalization. In this paper, we present a hybrid framework that\nfirst generalizes across users and then rapidly adapts to individual users\nusing few-shot learning directly on-device. By updating only the classifier\nlayer with user-specific data, our method achieves robust personalization with\nminimal computational and memory overhead. We implement this framework on the\nenergy-efficient RISC-V-based GAP9 microcontroller and validate it across three\ndiverse HAR scenarios: RecGym, QVAR-Gesture, and Ultrasound-Gesture.\nPost-deployment adaptation yields consistent accuracy improvements of 3.73\\%,\n17.38\\%, and 3.70\\% respectively. These results confirm that fast, lightweight,\nand effective personalization is feasible on embedded platforms, paving the way\nfor scalable and user-aware HAR systems in the wild\n\\footnote{https://github.com/kangpx/onlineTiny2023}.\n", "link": "http://arxiv.org/abs/2508.15413v1", "date": "2025-08-21", "relevancy": 2.0904, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5263}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5239}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Generalization%20and%20Personalization%20in%20Wearable%20Human%20Activity%0A%20%20Recognition%20via%20On-Device%20Few-Shot%20Learning&body=Title%3A%20Bridging%20Generalization%20and%20Personalization%20in%20Wearable%20Human%20Activity%0A%20%20Recognition%20via%20On-Device%20Few-Shot%20Learning%0AAuthor%3A%20Pixi%20Kang%20and%20Julian%20Moosmann%20and%20Mengxi%20Liu%20and%20Bo%20Zhou%20and%20Michele%20Magno%20and%20Paul%20Lukowicz%20and%20Sizhen%20Bian%0AAbstract%3A%20%20%20Human%20Activity%20Recognition%20%28HAR%29%20using%20wearable%20devices%20has%20advanced%0Asignificantly%20in%20recent%20years%2C%20yet%20its%20generalization%20remains%20limited%20when%0Amodels%20are%20deployed%20to%20new%20users.%20This%20degradation%20in%20performance%20is%20primarily%0Adue%20to%20user-induced%20concept%20drift%20%28UICD%29%2C%20highlighting%20the%20importance%20of%0Aefficient%20personalization.%20In%20this%20paper%2C%20we%20present%20a%20hybrid%20framework%20that%0Afirst%20generalizes%20across%20users%20and%20then%20rapidly%20adapts%20to%20individual%20users%0Ausing%20few-shot%20learning%20directly%20on-device.%20By%20updating%20only%20the%20classifier%0Alayer%20with%20user-specific%20data%2C%20our%20method%20achieves%20robust%20personalization%20with%0Aminimal%20computational%20and%20memory%20overhead.%20We%20implement%20this%20framework%20on%20the%0Aenergy-efficient%20RISC-V-based%20GAP9%20microcontroller%20and%20validate%20it%20across%20three%0Adiverse%20HAR%20scenarios%3A%20RecGym%2C%20QVAR-Gesture%2C%20and%20Ultrasound-Gesture.%0APost-deployment%20adaptation%20yields%20consistent%20accuracy%20improvements%20of%203.73%5C%25%2C%0A17.38%5C%25%2C%20and%203.70%5C%25%20respectively.%20These%20results%20confirm%20that%20fast%2C%20lightweight%2C%0Aand%20effective%20personalization%20is%20feasible%20on%20embedded%20platforms%2C%20paving%20the%20way%0Afor%20scalable%20and%20user-aware%20HAR%20systems%20in%20the%20wild%0A%5Cfootnote%7Bhttps%3A//github.com/kangpx/onlineTiny2023%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Generalization%2520and%2520Personalization%2520in%2520Wearable%2520Human%2520Activity%250A%2520%2520Recognition%2520via%2520On-Device%2520Few-Shot%2520Learning%26entry.906535625%3DPixi%2520Kang%2520and%2520Julian%2520Moosmann%2520and%2520Mengxi%2520Liu%2520and%2520Bo%2520Zhou%2520and%2520Michele%2520Magno%2520and%2520Paul%2520Lukowicz%2520and%2520Sizhen%2520Bian%26entry.1292438233%3D%2520%2520Human%2520Activity%2520Recognition%2520%2528HAR%2529%2520using%2520wearable%2520devices%2520has%2520advanced%250Asignificantly%2520in%2520recent%2520years%252C%2520yet%2520its%2520generalization%2520remains%2520limited%2520when%250Amodels%2520are%2520deployed%2520to%2520new%2520users.%2520This%2520degradation%2520in%2520performance%2520is%2520primarily%250Adue%2520to%2520user-induced%2520concept%2520drift%2520%2528UICD%2529%252C%2520highlighting%2520the%2520importance%2520of%250Aefficient%2520personalization.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520hybrid%2520framework%2520that%250Afirst%2520generalizes%2520across%2520users%2520and%2520then%2520rapidly%2520adapts%2520to%2520individual%2520users%250Ausing%2520few-shot%2520learning%2520directly%2520on-device.%2520By%2520updating%2520only%2520the%2520classifier%250Alayer%2520with%2520user-specific%2520data%252C%2520our%2520method%2520achieves%2520robust%2520personalization%2520with%250Aminimal%2520computational%2520and%2520memory%2520overhead.%2520We%2520implement%2520this%2520framework%2520on%2520the%250Aenergy-efficient%2520RISC-V-based%2520GAP9%2520microcontroller%2520and%2520validate%2520it%2520across%2520three%250Adiverse%2520HAR%2520scenarios%253A%2520RecGym%252C%2520QVAR-Gesture%252C%2520and%2520Ultrasound-Gesture.%250APost-deployment%2520adaptation%2520yields%2520consistent%2520accuracy%2520improvements%2520of%25203.73%255C%2525%252C%250A17.38%255C%2525%252C%2520and%25203.70%255C%2525%2520respectively.%2520These%2520results%2520confirm%2520that%2520fast%252C%2520lightweight%252C%250Aand%2520effective%2520personalization%2520is%2520feasible%2520on%2520embedded%2520platforms%252C%2520paving%2520the%2520way%250Afor%2520scalable%2520and%2520user-aware%2520HAR%2520systems%2520in%2520the%2520wild%250A%255Cfootnote%257Bhttps%253A//github.com/kangpx/onlineTiny2023%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Generalization%20and%20Personalization%20in%20Wearable%20Human%20Activity%0A%20%20Recognition%20via%20On-Device%20Few-Shot%20Learning&entry.906535625=Pixi%20Kang%20and%20Julian%20Moosmann%20and%20Mengxi%20Liu%20and%20Bo%20Zhou%20and%20Michele%20Magno%20and%20Paul%20Lukowicz%20and%20Sizhen%20Bian&entry.1292438233=%20%20Human%20Activity%20Recognition%20%28HAR%29%20using%20wearable%20devices%20has%20advanced%0Asignificantly%20in%20recent%20years%2C%20yet%20its%20generalization%20remains%20limited%20when%0Amodels%20are%20deployed%20to%20new%20users.%20This%20degradation%20in%20performance%20is%20primarily%0Adue%20to%20user-induced%20concept%20drift%20%28UICD%29%2C%20highlighting%20the%20importance%20of%0Aefficient%20personalization.%20In%20this%20paper%2C%20we%20present%20a%20hybrid%20framework%20that%0Afirst%20generalizes%20across%20users%20and%20then%20rapidly%20adapts%20to%20individual%20users%0Ausing%20few-shot%20learning%20directly%20on-device.%20By%20updating%20only%20the%20classifier%0Alayer%20with%20user-specific%20data%2C%20our%20method%20achieves%20robust%20personalization%20with%0Aminimal%20computational%20and%20memory%20overhead.%20We%20implement%20this%20framework%20on%20the%0Aenergy-efficient%20RISC-V-based%20GAP9%20microcontroller%20and%20validate%20it%20across%20three%0Adiverse%20HAR%20scenarios%3A%20RecGym%2C%20QVAR-Gesture%2C%20and%20Ultrasound-Gesture.%0APost-deployment%20adaptation%20yields%20consistent%20accuracy%20improvements%20of%203.73%5C%25%2C%0A17.38%5C%25%2C%20and%203.70%5C%25%20respectively.%20These%20results%20confirm%20that%20fast%2C%20lightweight%2C%0Aand%20effective%20personalization%20is%20feasible%20on%20embedded%20platforms%2C%20paving%20the%20way%0Afor%20scalable%20and%20user-aware%20HAR%20systems%20in%20the%20wild%0A%5Cfootnote%7Bhttps%3A//github.com/kangpx/onlineTiny2023%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15413v1&entry.124074799=Read"},
{"title": "Reliable Unlearning Harmful Information in LLMs with Metamorphosis\n  Representation Projection", "author": "Chengcan Wu and Zeming Wei and Huanran Chen and Yinpeng Dong and Meng Sun", "abstract": "  While Large Language Models (LLMs) have demonstrated impressive performance\nin various domains and tasks, concerns about their safety are becoming\nincreasingly severe. In particular, since models may store unsafe knowledge\ninternally, machine unlearning has emerged as a representative paradigm to\nensure model safety. Existing approaches employ various training techniques,\nsuch as gradient ascent and negative preference optimization, in attempts to\neliminate the influence of undesired data on target models. However, these\nmethods merely suppress the activation of undesired data through parametric\ntraining without completely eradicating its informational traces within the\nmodel. This fundamental limitation makes it difficult to achieve effective\ncontinuous unlearning, rendering these methods vulnerable to relearning\nattacks. To overcome these challenges, we propose a Metamorphosis\nRepresentation Projection (MRP) approach that pioneers the application of\nirreversible projection properties to machine unlearning. By implementing\nprojective transformations in the hidden state space of specific network\nlayers, our method effectively eliminates harmful information while preserving\nuseful knowledge. Experimental results demonstrate that our approach enables\neffective continuous unlearning and successfully defends against relearning\nattacks, achieving state-of-the-art performance in unlearning effectiveness\nwhile preserving natural performance. Our code is available in\nhttps://github.com/ChengcanWu/MRP.\n", "link": "http://arxiv.org/abs/2508.15449v1", "date": "2025-08-21", "relevancy": 2.0859, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5374}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5192}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reliable%20Unlearning%20Harmful%20Information%20in%20LLMs%20with%20Metamorphosis%0A%20%20Representation%20Projection&body=Title%3A%20Reliable%20Unlearning%20Harmful%20Information%20in%20LLMs%20with%20Metamorphosis%0A%20%20Representation%20Projection%0AAuthor%3A%20Chengcan%20Wu%20and%20Zeming%20Wei%20and%20Huanran%20Chen%20and%20Yinpeng%20Dong%20and%20Meng%20Sun%0AAbstract%3A%20%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20performance%0Ain%20various%20domains%20and%20tasks%2C%20concerns%20about%20their%20safety%20are%20becoming%0Aincreasingly%20severe.%20In%20particular%2C%20since%20models%20may%20store%20unsafe%20knowledge%0Ainternally%2C%20machine%20unlearning%20has%20emerged%20as%20a%20representative%20paradigm%20to%0Aensure%20model%20safety.%20Existing%20approaches%20employ%20various%20training%20techniques%2C%0Asuch%20as%20gradient%20ascent%20and%20negative%20preference%20optimization%2C%20in%20attempts%20to%0Aeliminate%20the%20influence%20of%20undesired%20data%20on%20target%20models.%20However%2C%20these%0Amethods%20merely%20suppress%20the%20activation%20of%20undesired%20data%20through%20parametric%0Atraining%20without%20completely%20eradicating%20its%20informational%20traces%20within%20the%0Amodel.%20This%20fundamental%20limitation%20makes%20it%20difficult%20to%20achieve%20effective%0Acontinuous%20unlearning%2C%20rendering%20these%20methods%20vulnerable%20to%20relearning%0Aattacks.%20To%20overcome%20these%20challenges%2C%20we%20propose%20a%20Metamorphosis%0ARepresentation%20Projection%20%28MRP%29%20approach%20that%20pioneers%20the%20application%20of%0Airreversible%20projection%20properties%20to%20machine%20unlearning.%20By%20implementing%0Aprojective%20transformations%20in%20the%20hidden%20state%20space%20of%20specific%20network%0Alayers%2C%20our%20method%20effectively%20eliminates%20harmful%20information%20while%20preserving%0Auseful%20knowledge.%20Experimental%20results%20demonstrate%20that%20our%20approach%20enables%0Aeffective%20continuous%20unlearning%20and%20successfully%20defends%20against%20relearning%0Aattacks%2C%20achieving%20state-of-the-art%20performance%20in%20unlearning%20effectiveness%0Awhile%20preserving%20natural%20performance.%20Our%20code%20is%20available%20in%0Ahttps%3A//github.com/ChengcanWu/MRP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReliable%2520Unlearning%2520Harmful%2520Information%2520in%2520LLMs%2520with%2520Metamorphosis%250A%2520%2520Representation%2520Projection%26entry.906535625%3DChengcan%2520Wu%2520and%2520Zeming%2520Wei%2520and%2520Huanran%2520Chen%2520and%2520Yinpeng%2520Dong%2520and%2520Meng%2520Sun%26entry.1292438233%3D%2520%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520performance%250Ain%2520various%2520domains%2520and%2520tasks%252C%2520concerns%2520about%2520their%2520safety%2520are%2520becoming%250Aincreasingly%2520severe.%2520In%2520particular%252C%2520since%2520models%2520may%2520store%2520unsafe%2520knowledge%250Ainternally%252C%2520machine%2520unlearning%2520has%2520emerged%2520as%2520a%2520representative%2520paradigm%2520to%250Aensure%2520model%2520safety.%2520Existing%2520approaches%2520employ%2520various%2520training%2520techniques%252C%250Asuch%2520as%2520gradient%2520ascent%2520and%2520negative%2520preference%2520optimization%252C%2520in%2520attempts%2520to%250Aeliminate%2520the%2520influence%2520of%2520undesired%2520data%2520on%2520target%2520models.%2520However%252C%2520these%250Amethods%2520merely%2520suppress%2520the%2520activation%2520of%2520undesired%2520data%2520through%2520parametric%250Atraining%2520without%2520completely%2520eradicating%2520its%2520informational%2520traces%2520within%2520the%250Amodel.%2520This%2520fundamental%2520limitation%2520makes%2520it%2520difficult%2520to%2520achieve%2520effective%250Acontinuous%2520unlearning%252C%2520rendering%2520these%2520methods%2520vulnerable%2520to%2520relearning%250Aattacks.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520a%2520Metamorphosis%250ARepresentation%2520Projection%2520%2528MRP%2529%2520approach%2520that%2520pioneers%2520the%2520application%2520of%250Airreversible%2520projection%2520properties%2520to%2520machine%2520unlearning.%2520By%2520implementing%250Aprojective%2520transformations%2520in%2520the%2520hidden%2520state%2520space%2520of%2520specific%2520network%250Alayers%252C%2520our%2520method%2520effectively%2520eliminates%2520harmful%2520information%2520while%2520preserving%250Auseful%2520knowledge.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520enables%250Aeffective%2520continuous%2520unlearning%2520and%2520successfully%2520defends%2520against%2520relearning%250Aattacks%252C%2520achieving%2520state-of-the-art%2520performance%2520in%2520unlearning%2520effectiveness%250Awhile%2520preserving%2520natural%2520performance.%2520Our%2520code%2520is%2520available%2520in%250Ahttps%253A//github.com/ChengcanWu/MRP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reliable%20Unlearning%20Harmful%20Information%20in%20LLMs%20with%20Metamorphosis%0A%20%20Representation%20Projection&entry.906535625=Chengcan%20Wu%20and%20Zeming%20Wei%20and%20Huanran%20Chen%20and%20Yinpeng%20Dong%20and%20Meng%20Sun&entry.1292438233=%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20impressive%20performance%0Ain%20various%20domains%20and%20tasks%2C%20concerns%20about%20their%20safety%20are%20becoming%0Aincreasingly%20severe.%20In%20particular%2C%20since%20models%20may%20store%20unsafe%20knowledge%0Ainternally%2C%20machine%20unlearning%20has%20emerged%20as%20a%20representative%20paradigm%20to%0Aensure%20model%20safety.%20Existing%20approaches%20employ%20various%20training%20techniques%2C%0Asuch%20as%20gradient%20ascent%20and%20negative%20preference%20optimization%2C%20in%20attempts%20to%0Aeliminate%20the%20influence%20of%20undesired%20data%20on%20target%20models.%20However%2C%20these%0Amethods%20merely%20suppress%20the%20activation%20of%20undesired%20data%20through%20parametric%0Atraining%20without%20completely%20eradicating%20its%20informational%20traces%20within%20the%0Amodel.%20This%20fundamental%20limitation%20makes%20it%20difficult%20to%20achieve%20effective%0Acontinuous%20unlearning%2C%20rendering%20these%20methods%20vulnerable%20to%20relearning%0Aattacks.%20To%20overcome%20these%20challenges%2C%20we%20propose%20a%20Metamorphosis%0ARepresentation%20Projection%20%28MRP%29%20approach%20that%20pioneers%20the%20application%20of%0Airreversible%20projection%20properties%20to%20machine%20unlearning.%20By%20implementing%0Aprojective%20transformations%20in%20the%20hidden%20state%20space%20of%20specific%20network%0Alayers%2C%20our%20method%20effectively%20eliminates%20harmful%20information%20while%20preserving%0Auseful%20knowledge.%20Experimental%20results%20demonstrate%20that%20our%20approach%20enables%0Aeffective%20continuous%20unlearning%20and%20successfully%20defends%20against%20relearning%0Aattacks%2C%20achieving%20state-of-the-art%20performance%20in%20unlearning%20effectiveness%0Awhile%20preserving%20natural%20performance.%20Our%20code%20is%20available%20in%0Ahttps%3A//github.com/ChengcanWu/MRP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15449v1&entry.124074799=Read"},
{"title": "Are Virtual DES Images a Valid Alternative to the Real Ones?", "author": "Ana C. Perre and Lu\u00eds A. Alexandre and Lu\u00eds C. Freire", "abstract": "  Contrast-enhanced spectral mammography (CESM) is an imaging modality that\nprovides two types of images, commonly known as low-energy (LE) and dual-energy\nsubtracted (DES) images. In many domains, particularly in medicine, the\nemergence of image-to-image translation techniques has enabled the artificial\ngeneration of images using other images as input. Within CESM, applying such\ntechniques to generate DES images from LE images could be highly beneficial,\npotentially reducing patient exposure to radiation associated with high-energy\nimage acquisition. In this study, we investigated three models for the\nartificial generation of DES images (virtual DES): a pre-trained U-Net model, a\nU-Net trained end-to-end model, and a CycleGAN model. We also performed a\nseries of experiments to assess the impact of using virtual DES images on the\nclassification of CESM examinations into malignant and non-malignant\ncategories. To our knowledge, this is the first study to evaluate the impact of\nvirtual DES images on CESM lesion classification. The results demonstrate that\nthe best performance was achieved with the pre-trained U-Net model, yielding an\nF1 score of 85.59% when using the virtual DES images, compared to 90.35% with\nthe real DES images. This discrepancy likely results from the additional\ndiagnostic information in real DES images, which contributes to a higher\nclassification accuracy. Nevertheless, the potential for virtual DES image\ngeneration is considerable and future advancements may narrow this performance\ngap to a level where exclusive reliance on virtual DES images becomes\nclinically viable.\n", "link": "http://arxiv.org/abs/2508.15594v1", "date": "2025-08-21", "relevancy": 2.0796, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5429}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5178}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Virtual%20DES%20Images%20a%20Valid%20Alternative%20to%20the%20Real%20Ones%3F&body=Title%3A%20Are%20Virtual%20DES%20Images%20a%20Valid%20Alternative%20to%20the%20Real%20Ones%3F%0AAuthor%3A%20Ana%20C.%20Perre%20and%20Lu%C3%ADs%20A.%20Alexandre%20and%20Lu%C3%ADs%20C.%20Freire%0AAbstract%3A%20%20%20Contrast-enhanced%20spectral%20mammography%20%28CESM%29%20is%20an%20imaging%20modality%20that%0Aprovides%20two%20types%20of%20images%2C%20commonly%20known%20as%20low-energy%20%28LE%29%20and%20dual-energy%0Asubtracted%20%28DES%29%20images.%20In%20many%20domains%2C%20particularly%20in%20medicine%2C%20the%0Aemergence%20of%20image-to-image%20translation%20techniques%20has%20enabled%20the%20artificial%0Ageneration%20of%20images%20using%20other%20images%20as%20input.%20Within%20CESM%2C%20applying%20such%0Atechniques%20to%20generate%20DES%20images%20from%20LE%20images%20could%20be%20highly%20beneficial%2C%0Apotentially%20reducing%20patient%20exposure%20to%20radiation%20associated%20with%20high-energy%0Aimage%20acquisition.%20In%20this%20study%2C%20we%20investigated%20three%20models%20for%20the%0Aartificial%20generation%20of%20DES%20images%20%28virtual%20DES%29%3A%20a%20pre-trained%20U-Net%20model%2C%20a%0AU-Net%20trained%20end-to-end%20model%2C%20and%20a%20CycleGAN%20model.%20We%20also%20performed%20a%0Aseries%20of%20experiments%20to%20assess%20the%20impact%20of%20using%20virtual%20DES%20images%20on%20the%0Aclassification%20of%20CESM%20examinations%20into%20malignant%20and%20non-malignant%0Acategories.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20study%20to%20evaluate%20the%20impact%20of%0Avirtual%20DES%20images%20on%20CESM%20lesion%20classification.%20The%20results%20demonstrate%20that%0Athe%20best%20performance%20was%20achieved%20with%20the%20pre-trained%20U-Net%20model%2C%20yielding%20an%0AF1%20score%20of%2085.59%25%20when%20using%20the%20virtual%20DES%20images%2C%20compared%20to%2090.35%25%20with%0Athe%20real%20DES%20images.%20This%20discrepancy%20likely%20results%20from%20the%20additional%0Adiagnostic%20information%20in%20real%20DES%20images%2C%20which%20contributes%20to%20a%20higher%0Aclassification%20accuracy.%20Nevertheless%2C%20the%20potential%20for%20virtual%20DES%20image%0Ageneration%20is%20considerable%20and%20future%20advancements%20may%20narrow%20this%20performance%0Agap%20to%20a%20level%20where%20exclusive%20reliance%20on%20virtual%20DES%20images%20becomes%0Aclinically%20viable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Virtual%2520DES%2520Images%2520a%2520Valid%2520Alternative%2520to%2520the%2520Real%2520Ones%253F%26entry.906535625%3DAna%2520C.%2520Perre%2520and%2520Lu%25C3%25ADs%2520A.%2520Alexandre%2520and%2520Lu%25C3%25ADs%2520C.%2520Freire%26entry.1292438233%3D%2520%2520Contrast-enhanced%2520spectral%2520mammography%2520%2528CESM%2529%2520is%2520an%2520imaging%2520modality%2520that%250Aprovides%2520two%2520types%2520of%2520images%252C%2520commonly%2520known%2520as%2520low-energy%2520%2528LE%2529%2520and%2520dual-energy%250Asubtracted%2520%2528DES%2529%2520images.%2520In%2520many%2520domains%252C%2520particularly%2520in%2520medicine%252C%2520the%250Aemergence%2520of%2520image-to-image%2520translation%2520techniques%2520has%2520enabled%2520the%2520artificial%250Ageneration%2520of%2520images%2520using%2520other%2520images%2520as%2520input.%2520Within%2520CESM%252C%2520applying%2520such%250Atechniques%2520to%2520generate%2520DES%2520images%2520from%2520LE%2520images%2520could%2520be%2520highly%2520beneficial%252C%250Apotentially%2520reducing%2520patient%2520exposure%2520to%2520radiation%2520associated%2520with%2520high-energy%250Aimage%2520acquisition.%2520In%2520this%2520study%252C%2520we%2520investigated%2520three%2520models%2520for%2520the%250Aartificial%2520generation%2520of%2520DES%2520images%2520%2528virtual%2520DES%2529%253A%2520a%2520pre-trained%2520U-Net%2520model%252C%2520a%250AU-Net%2520trained%2520end-to-end%2520model%252C%2520and%2520a%2520CycleGAN%2520model.%2520We%2520also%2520performed%2520a%250Aseries%2520of%2520experiments%2520to%2520assess%2520the%2520impact%2520of%2520using%2520virtual%2520DES%2520images%2520on%2520the%250Aclassification%2520of%2520CESM%2520examinations%2520into%2520malignant%2520and%2520non-malignant%250Acategories.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520study%2520to%2520evaluate%2520the%2520impact%2520of%250Avirtual%2520DES%2520images%2520on%2520CESM%2520lesion%2520classification.%2520The%2520results%2520demonstrate%2520that%250Athe%2520best%2520performance%2520was%2520achieved%2520with%2520the%2520pre-trained%2520U-Net%2520model%252C%2520yielding%2520an%250AF1%2520score%2520of%252085.59%2525%2520when%2520using%2520the%2520virtual%2520DES%2520images%252C%2520compared%2520to%252090.35%2525%2520with%250Athe%2520real%2520DES%2520images.%2520This%2520discrepancy%2520likely%2520results%2520from%2520the%2520additional%250Adiagnostic%2520information%2520in%2520real%2520DES%2520images%252C%2520which%2520contributes%2520to%2520a%2520higher%250Aclassification%2520accuracy.%2520Nevertheless%252C%2520the%2520potential%2520for%2520virtual%2520DES%2520image%250Ageneration%2520is%2520considerable%2520and%2520future%2520advancements%2520may%2520narrow%2520this%2520performance%250Agap%2520to%2520a%2520level%2520where%2520exclusive%2520reliance%2520on%2520virtual%2520DES%2520images%2520becomes%250Aclinically%2520viable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Virtual%20DES%20Images%20a%20Valid%20Alternative%20to%20the%20Real%20Ones%3F&entry.906535625=Ana%20C.%20Perre%20and%20Lu%C3%ADs%20A.%20Alexandre%20and%20Lu%C3%ADs%20C.%20Freire&entry.1292438233=%20%20Contrast-enhanced%20spectral%20mammography%20%28CESM%29%20is%20an%20imaging%20modality%20that%0Aprovides%20two%20types%20of%20images%2C%20commonly%20known%20as%20low-energy%20%28LE%29%20and%20dual-energy%0Asubtracted%20%28DES%29%20images.%20In%20many%20domains%2C%20particularly%20in%20medicine%2C%20the%0Aemergence%20of%20image-to-image%20translation%20techniques%20has%20enabled%20the%20artificial%0Ageneration%20of%20images%20using%20other%20images%20as%20input.%20Within%20CESM%2C%20applying%20such%0Atechniques%20to%20generate%20DES%20images%20from%20LE%20images%20could%20be%20highly%20beneficial%2C%0Apotentially%20reducing%20patient%20exposure%20to%20radiation%20associated%20with%20high-energy%0Aimage%20acquisition.%20In%20this%20study%2C%20we%20investigated%20three%20models%20for%20the%0Aartificial%20generation%20of%20DES%20images%20%28virtual%20DES%29%3A%20a%20pre-trained%20U-Net%20model%2C%20a%0AU-Net%20trained%20end-to-end%20model%2C%20and%20a%20CycleGAN%20model.%20We%20also%20performed%20a%0Aseries%20of%20experiments%20to%20assess%20the%20impact%20of%20using%20virtual%20DES%20images%20on%20the%0Aclassification%20of%20CESM%20examinations%20into%20malignant%20and%20non-malignant%0Acategories.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20study%20to%20evaluate%20the%20impact%20of%0Avirtual%20DES%20images%20on%20CESM%20lesion%20classification.%20The%20results%20demonstrate%20that%0Athe%20best%20performance%20was%20achieved%20with%20the%20pre-trained%20U-Net%20model%2C%20yielding%20an%0AF1%20score%20of%2085.59%25%20when%20using%20the%20virtual%20DES%20images%2C%20compared%20to%2090.35%25%20with%0Athe%20real%20DES%20images.%20This%20discrepancy%20likely%20results%20from%20the%20additional%0Adiagnostic%20information%20in%20real%20DES%20images%2C%20which%20contributes%20to%20a%20higher%0Aclassification%20accuracy.%20Nevertheless%2C%20the%20potential%20for%20virtual%20DES%20image%0Ageneration%20is%20considerable%20and%20future%20advancements%20may%20narrow%20this%20performance%0Agap%20to%20a%20level%20where%20exclusive%20reliance%20on%20virtual%20DES%20images%20becomes%0Aclinically%20viable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15594v1&entry.124074799=Read"},
{"title": "Conditionally adaptive augmented Lagrangian method for physics-informed\n  learning of forward and inverse problems using artificial neural networks", "author": "Qifeng Hu and Shamsulhaq Basir and Inanc Senocak", "abstract": "  We present several advances to the physics and equality constrained\nartificial neural networks (PECANN) framework that substantially improve its\ncapability to learn solutions of canonical partial differential equations\n(PDEs). First, we generalize the augmented Lagrangian method (ALM) to support\nmultiple independent penalty parameters, enabling simultaneous enforcement of\nheterogeneous constraints. Second, we reformulate pointwise constraint\nenforcement and Lagrange multipliers as expectations over constraint terms,\nreducing memory overhead and permitting efficient mini-batch training. Third,\nto address PDEs with oscillatory, multi-scale features, we incorporate Fourier\nfeature mappings and show that a single mapping suffices where multiple\nmappings or more costly architectures were required in related methods. Fourth,\nwe introduce a time-windowing strategy for long-time evolution in which the\nterminal state of each window is enforced as an initial-condition constraint\nfor the next, ensuring continuity without discrete time models. Crucially, we\npropose a conditionally adaptive penalty update (CAPU) strategy for ALM, which\npreserves the principle that larger constraint violations incur stronger\npenalties. CAPU accelerates the growth of Lagrange multipliers for selectively\nchallenging constraints, enhancing constraint enforcement during training. We\ndemonstrate the effectiveness of PECANN-CAPU on problems including the\ntransonic rarefaction problem, reversible advection of a passive by a vortex,\nhigh-wavenumber Helmholtz and Poisson equations, and inverse identification of\nspatially varying heat sources. Comparisons with established methods and recent\nKolmogorov-Arnold network approaches show that PECANN-CAPU achieves competitive\naccuracy across all cases. Collectively, these advances improve PECANN's\nrobustness, efficiency, and applicability to demanding problems in scientific\ncomputing.\n", "link": "http://arxiv.org/abs/2508.15695v1", "date": "2025-08-21", "relevancy": 2.0771, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5432}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5298}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditionally%20adaptive%20augmented%20Lagrangian%20method%20for%20physics-informed%0A%20%20learning%20of%20forward%20and%20inverse%20problems%20using%20artificial%20neural%20networks&body=Title%3A%20Conditionally%20adaptive%20augmented%20Lagrangian%20method%20for%20physics-informed%0A%20%20learning%20of%20forward%20and%20inverse%20problems%20using%20artificial%20neural%20networks%0AAuthor%3A%20Qifeng%20Hu%20and%20Shamsulhaq%20Basir%20and%20Inanc%20Senocak%0AAbstract%3A%20%20%20We%20present%20several%20advances%20to%20the%20physics%20and%20equality%20constrained%0Aartificial%20neural%20networks%20%28PECANN%29%20framework%20that%20substantially%20improve%20its%0Acapability%20to%20learn%20solutions%20of%20canonical%20partial%20differential%20equations%0A%28PDEs%29.%20First%2C%20we%20generalize%20the%20augmented%20Lagrangian%20method%20%28ALM%29%20to%20support%0Amultiple%20independent%20penalty%20parameters%2C%20enabling%20simultaneous%20enforcement%20of%0Aheterogeneous%20constraints.%20Second%2C%20we%20reformulate%20pointwise%20constraint%0Aenforcement%20and%20Lagrange%20multipliers%20as%20expectations%20over%20constraint%20terms%2C%0Areducing%20memory%20overhead%20and%20permitting%20efficient%20mini-batch%20training.%20Third%2C%0Ato%20address%20PDEs%20with%20oscillatory%2C%20multi-scale%20features%2C%20we%20incorporate%20Fourier%0Afeature%20mappings%20and%20show%20that%20a%20single%20mapping%20suffices%20where%20multiple%0Amappings%20or%20more%20costly%20architectures%20were%20required%20in%20related%20methods.%20Fourth%2C%0Awe%20introduce%20a%20time-windowing%20strategy%20for%20long-time%20evolution%20in%20which%20the%0Aterminal%20state%20of%20each%20window%20is%20enforced%20as%20an%20initial-condition%20constraint%0Afor%20the%20next%2C%20ensuring%20continuity%20without%20discrete%20time%20models.%20Crucially%2C%20we%0Apropose%20a%20conditionally%20adaptive%20penalty%20update%20%28CAPU%29%20strategy%20for%20ALM%2C%20which%0Apreserves%20the%20principle%20that%20larger%20constraint%20violations%20incur%20stronger%0Apenalties.%20CAPU%20accelerates%20the%20growth%20of%20Lagrange%20multipliers%20for%20selectively%0Achallenging%20constraints%2C%20enhancing%20constraint%20enforcement%20during%20training.%20We%0Ademonstrate%20the%20effectiveness%20of%20PECANN-CAPU%20on%20problems%20including%20the%0Atransonic%20rarefaction%20problem%2C%20reversible%20advection%20of%20a%20passive%20by%20a%20vortex%2C%0Ahigh-wavenumber%20Helmholtz%20and%20Poisson%20equations%2C%20and%20inverse%20identification%20of%0Aspatially%20varying%20heat%20sources.%20Comparisons%20with%20established%20methods%20and%20recent%0AKolmogorov-Arnold%20network%20approaches%20show%20that%20PECANN-CAPU%20achieves%20competitive%0Aaccuracy%20across%20all%20cases.%20Collectively%2C%20these%20advances%20improve%20PECANN%27s%0Arobustness%2C%20efficiency%2C%20and%20applicability%20to%20demanding%20problems%20in%20scientific%0Acomputing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditionally%2520adaptive%2520augmented%2520Lagrangian%2520method%2520for%2520physics-informed%250A%2520%2520learning%2520of%2520forward%2520and%2520inverse%2520problems%2520using%2520artificial%2520neural%2520networks%26entry.906535625%3DQifeng%2520Hu%2520and%2520Shamsulhaq%2520Basir%2520and%2520Inanc%2520Senocak%26entry.1292438233%3D%2520%2520We%2520present%2520several%2520advances%2520to%2520the%2520physics%2520and%2520equality%2520constrained%250Aartificial%2520neural%2520networks%2520%2528PECANN%2529%2520framework%2520that%2520substantially%2520improve%2520its%250Acapability%2520to%2520learn%2520solutions%2520of%2520canonical%2520partial%2520differential%2520equations%250A%2528PDEs%2529.%2520First%252C%2520we%2520generalize%2520the%2520augmented%2520Lagrangian%2520method%2520%2528ALM%2529%2520to%2520support%250Amultiple%2520independent%2520penalty%2520parameters%252C%2520enabling%2520simultaneous%2520enforcement%2520of%250Aheterogeneous%2520constraints.%2520Second%252C%2520we%2520reformulate%2520pointwise%2520constraint%250Aenforcement%2520and%2520Lagrange%2520multipliers%2520as%2520expectations%2520over%2520constraint%2520terms%252C%250Areducing%2520memory%2520overhead%2520and%2520permitting%2520efficient%2520mini-batch%2520training.%2520Third%252C%250Ato%2520address%2520PDEs%2520with%2520oscillatory%252C%2520multi-scale%2520features%252C%2520we%2520incorporate%2520Fourier%250Afeature%2520mappings%2520and%2520show%2520that%2520a%2520single%2520mapping%2520suffices%2520where%2520multiple%250Amappings%2520or%2520more%2520costly%2520architectures%2520were%2520required%2520in%2520related%2520methods.%2520Fourth%252C%250Awe%2520introduce%2520a%2520time-windowing%2520strategy%2520for%2520long-time%2520evolution%2520in%2520which%2520the%250Aterminal%2520state%2520of%2520each%2520window%2520is%2520enforced%2520as%2520an%2520initial-condition%2520constraint%250Afor%2520the%2520next%252C%2520ensuring%2520continuity%2520without%2520discrete%2520time%2520models.%2520Crucially%252C%2520we%250Apropose%2520a%2520conditionally%2520adaptive%2520penalty%2520update%2520%2528CAPU%2529%2520strategy%2520for%2520ALM%252C%2520which%250Apreserves%2520the%2520principle%2520that%2520larger%2520constraint%2520violations%2520incur%2520stronger%250Apenalties.%2520CAPU%2520accelerates%2520the%2520growth%2520of%2520Lagrange%2520multipliers%2520for%2520selectively%250Achallenging%2520constraints%252C%2520enhancing%2520constraint%2520enforcement%2520during%2520training.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520PECANN-CAPU%2520on%2520problems%2520including%2520the%250Atransonic%2520rarefaction%2520problem%252C%2520reversible%2520advection%2520of%2520a%2520passive%2520by%2520a%2520vortex%252C%250Ahigh-wavenumber%2520Helmholtz%2520and%2520Poisson%2520equations%252C%2520and%2520inverse%2520identification%2520of%250Aspatially%2520varying%2520heat%2520sources.%2520Comparisons%2520with%2520established%2520methods%2520and%2520recent%250AKolmogorov-Arnold%2520network%2520approaches%2520show%2520that%2520PECANN-CAPU%2520achieves%2520competitive%250Aaccuracy%2520across%2520all%2520cases.%2520Collectively%252C%2520these%2520advances%2520improve%2520PECANN%2527s%250Arobustness%252C%2520efficiency%252C%2520and%2520applicability%2520to%2520demanding%2520problems%2520in%2520scientific%250Acomputing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditionally%20adaptive%20augmented%20Lagrangian%20method%20for%20physics-informed%0A%20%20learning%20of%20forward%20and%20inverse%20problems%20using%20artificial%20neural%20networks&entry.906535625=Qifeng%20Hu%20and%20Shamsulhaq%20Basir%20and%20Inanc%20Senocak&entry.1292438233=%20%20We%20present%20several%20advances%20to%20the%20physics%20and%20equality%20constrained%0Aartificial%20neural%20networks%20%28PECANN%29%20framework%20that%20substantially%20improve%20its%0Acapability%20to%20learn%20solutions%20of%20canonical%20partial%20differential%20equations%0A%28PDEs%29.%20First%2C%20we%20generalize%20the%20augmented%20Lagrangian%20method%20%28ALM%29%20to%20support%0Amultiple%20independent%20penalty%20parameters%2C%20enabling%20simultaneous%20enforcement%20of%0Aheterogeneous%20constraints.%20Second%2C%20we%20reformulate%20pointwise%20constraint%0Aenforcement%20and%20Lagrange%20multipliers%20as%20expectations%20over%20constraint%20terms%2C%0Areducing%20memory%20overhead%20and%20permitting%20efficient%20mini-batch%20training.%20Third%2C%0Ato%20address%20PDEs%20with%20oscillatory%2C%20multi-scale%20features%2C%20we%20incorporate%20Fourier%0Afeature%20mappings%20and%20show%20that%20a%20single%20mapping%20suffices%20where%20multiple%0Amappings%20or%20more%20costly%20architectures%20were%20required%20in%20related%20methods.%20Fourth%2C%0Awe%20introduce%20a%20time-windowing%20strategy%20for%20long-time%20evolution%20in%20which%20the%0Aterminal%20state%20of%20each%20window%20is%20enforced%20as%20an%20initial-condition%20constraint%0Afor%20the%20next%2C%20ensuring%20continuity%20without%20discrete%20time%20models.%20Crucially%2C%20we%0Apropose%20a%20conditionally%20adaptive%20penalty%20update%20%28CAPU%29%20strategy%20for%20ALM%2C%20which%0Apreserves%20the%20principle%20that%20larger%20constraint%20violations%20incur%20stronger%0Apenalties.%20CAPU%20accelerates%20the%20growth%20of%20Lagrange%20multipliers%20for%20selectively%0Achallenging%20constraints%2C%20enhancing%20constraint%20enforcement%20during%20training.%20We%0Ademonstrate%20the%20effectiveness%20of%20PECANN-CAPU%20on%20problems%20including%20the%0Atransonic%20rarefaction%20problem%2C%20reversible%20advection%20of%20a%20passive%20by%20a%20vortex%2C%0Ahigh-wavenumber%20Helmholtz%20and%20Poisson%20equations%2C%20and%20inverse%20identification%20of%0Aspatially%20varying%20heat%20sources.%20Comparisons%20with%20established%20methods%20and%20recent%0AKolmogorov-Arnold%20network%20approaches%20show%20that%20PECANN-CAPU%20achieves%20competitive%0Aaccuracy%20across%20all%20cases.%20Collectively%2C%20these%20advances%20improve%20PECANN%27s%0Arobustness%2C%20efficiency%2C%20and%20applicability%20to%20demanding%20problems%20in%20scientific%0Acomputing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15695v1&entry.124074799=Read"},
{"title": "LoUQAL: Low-fidelity informed Uncertainty Quantification for Active\n  Learning in the chemical configuration space", "author": "Vivin Vinod and Peter Zaspel", "abstract": "  Uncertainty quantification is an important scheme in active learning\ntechniques, including applications in predicting quantum chemical properties.\nIn quantum chemical calculations, there exists the notion of a fidelity, a less\naccurate computation is accessible at a cheaper computational cost. This work\nproposes a novel low-fidelity informed uncertainty quantification for active\nlearning with applications in predicting diverse quantum chemical properties\nsuch as excitation energies and \\textit{ab initio} potential energy surfaces.\nComputational experiments are carried out in order to assess the proposed\nmethod with results demonstrating that models trained with the novel method\noutperform alternatives in terms of empirical error and number of iterations\nrequired. The effect of the choice of fidelity is also studied to perform a\nthorough benchmark.\n", "link": "http://arxiv.org/abs/2508.15577v1", "date": "2025-08-21", "relevancy": 2.0732, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5434}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5248}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoUQAL%3A%20Low-fidelity%20informed%20Uncertainty%20Quantification%20for%20Active%0A%20%20Learning%20in%20the%20chemical%20configuration%20space&body=Title%3A%20LoUQAL%3A%20Low-fidelity%20informed%20Uncertainty%20Quantification%20for%20Active%0A%20%20Learning%20in%20the%20chemical%20configuration%20space%0AAuthor%3A%20Vivin%20Vinod%20and%20Peter%20Zaspel%0AAbstract%3A%20%20%20Uncertainty%20quantification%20is%20an%20important%20scheme%20in%20active%20learning%0Atechniques%2C%20including%20applications%20in%20predicting%20quantum%20chemical%20properties.%0AIn%20quantum%20chemical%20calculations%2C%20there%20exists%20the%20notion%20of%20a%20fidelity%2C%20a%20less%0Aaccurate%20computation%20is%20accessible%20at%20a%20cheaper%20computational%20cost.%20This%20work%0Aproposes%20a%20novel%20low-fidelity%20informed%20uncertainty%20quantification%20for%20active%0Alearning%20with%20applications%20in%20predicting%20diverse%20quantum%20chemical%20properties%0Asuch%20as%20excitation%20energies%20and%20%5Ctextit%7Bab%20initio%7D%20potential%20energy%20surfaces.%0AComputational%20experiments%20are%20carried%20out%20in%20order%20to%20assess%20the%20proposed%0Amethod%20with%20results%20demonstrating%20that%20models%20trained%20with%20the%20novel%20method%0Aoutperform%20alternatives%20in%20terms%20of%20empirical%20error%20and%20number%20of%20iterations%0Arequired.%20The%20effect%20of%20the%20choice%20of%20fidelity%20is%20also%20studied%20to%20perform%20a%0Athorough%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoUQAL%253A%2520Low-fidelity%2520informed%2520Uncertainty%2520Quantification%2520for%2520Active%250A%2520%2520Learning%2520in%2520the%2520chemical%2520configuration%2520space%26entry.906535625%3DVivin%2520Vinod%2520and%2520Peter%2520Zaspel%26entry.1292438233%3D%2520%2520Uncertainty%2520quantification%2520is%2520an%2520important%2520scheme%2520in%2520active%2520learning%250Atechniques%252C%2520including%2520applications%2520in%2520predicting%2520quantum%2520chemical%2520properties.%250AIn%2520quantum%2520chemical%2520calculations%252C%2520there%2520exists%2520the%2520notion%2520of%2520a%2520fidelity%252C%2520a%2520less%250Aaccurate%2520computation%2520is%2520accessible%2520at%2520a%2520cheaper%2520computational%2520cost.%2520This%2520work%250Aproposes%2520a%2520novel%2520low-fidelity%2520informed%2520uncertainty%2520quantification%2520for%2520active%250Alearning%2520with%2520applications%2520in%2520predicting%2520diverse%2520quantum%2520chemical%2520properties%250Asuch%2520as%2520excitation%2520energies%2520and%2520%255Ctextit%257Bab%2520initio%257D%2520potential%2520energy%2520surfaces.%250AComputational%2520experiments%2520are%2520carried%2520out%2520in%2520order%2520to%2520assess%2520the%2520proposed%250Amethod%2520with%2520results%2520demonstrating%2520that%2520models%2520trained%2520with%2520the%2520novel%2520method%250Aoutperform%2520alternatives%2520in%2520terms%2520of%2520empirical%2520error%2520and%2520number%2520of%2520iterations%250Arequired.%2520The%2520effect%2520of%2520the%2520choice%2520of%2520fidelity%2520is%2520also%2520studied%2520to%2520perform%2520a%250Athorough%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoUQAL%3A%20Low-fidelity%20informed%20Uncertainty%20Quantification%20for%20Active%0A%20%20Learning%20in%20the%20chemical%20configuration%20space&entry.906535625=Vivin%20Vinod%20and%20Peter%20Zaspel&entry.1292438233=%20%20Uncertainty%20quantification%20is%20an%20important%20scheme%20in%20active%20learning%0Atechniques%2C%20including%20applications%20in%20predicting%20quantum%20chemical%20properties.%0AIn%20quantum%20chemical%20calculations%2C%20there%20exists%20the%20notion%20of%20a%20fidelity%2C%20a%20less%0Aaccurate%20computation%20is%20accessible%20at%20a%20cheaper%20computational%20cost.%20This%20work%0Aproposes%20a%20novel%20low-fidelity%20informed%20uncertainty%20quantification%20for%20active%0Alearning%20with%20applications%20in%20predicting%20diverse%20quantum%20chemical%20properties%0Asuch%20as%20excitation%20energies%20and%20%5Ctextit%7Bab%20initio%7D%20potential%20energy%20surfaces.%0AComputational%20experiments%20are%20carried%20out%20in%20order%20to%20assess%20the%20proposed%0Amethod%20with%20results%20demonstrating%20that%20models%20trained%20with%20the%20novel%20method%0Aoutperform%20alternatives%20in%20terms%20of%20empirical%20error%20and%20number%20of%20iterations%0Arequired.%20The%20effect%20of%20the%20choice%20of%20fidelity%20is%20also%20studied%20to%20perform%20a%0Athorough%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15577v1&entry.124074799=Read"},
{"title": "Correct-By-Construction: Certified Individual Fairness through Neural\n  Network Training", "author": "Ruihan Zhang and Jun Sun", "abstract": "  Fairness in machine learning is more important than ever as ethical concerns\ncontinue to grow. Individual fairness demands that individuals differing only\nin sensitive attributes receive the same outcomes. However, commonly used\nmachine learning algorithms often fail to achieve such fairness. To improve\nindividual fairness, various training methods have been developed, such as\nincorporating fairness constraints as optimisation objectives. While these\nmethods have demonstrated empirical effectiveness, they lack formal guarantees\nof fairness. Existing approaches that aim to provide fairness guarantees\nprimarily rely on verification techniques, which can sometimes fail to produce\ndefinitive results. Moreover, verification alone does not actively enhance\nindividual fairness during training. To address this limitation, we propose a\nnovel framework that formally guarantees individual fairness throughout\ntraining. Our approach consists of two parts, i.e., (1) provably fair\ninitialisation that ensures the model starts in a fair state, and (2) a\nfairness-preserving training algorithm that maintains fairness as the model\nlearns. A key element of our method is the use of randomised response\nmechanisms, which protect sensitive attributes while maintaining fairness\nguarantees. We formally prove that this mechanism sustains individual fairness\nthroughout the training process. Experimental evaluations confirm that our\napproach is effective, i.e., producing models that are empirically fair and\naccurate. Furthermore, our approach is much more efficient than the alternative\napproach based on certified training (which requires neural network\nverification during training).\n", "link": "http://arxiv.org/abs/2508.15642v1", "date": "2025-08-21", "relevancy": 2.0559, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5247}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5126}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Correct-By-Construction%3A%20Certified%20Individual%20Fairness%20through%20Neural%0A%20%20Network%20Training&body=Title%3A%20Correct-By-Construction%3A%20Certified%20Individual%20Fairness%20through%20Neural%0A%20%20Network%20Training%0AAuthor%3A%20Ruihan%20Zhang%20and%20Jun%20Sun%0AAbstract%3A%20%20%20Fairness%20in%20machine%20learning%20is%20more%20important%20than%20ever%20as%20ethical%20concerns%0Acontinue%20to%20grow.%20Individual%20fairness%20demands%20that%20individuals%20differing%20only%0Ain%20sensitive%20attributes%20receive%20the%20same%20outcomes.%20However%2C%20commonly%20used%0Amachine%20learning%20algorithms%20often%20fail%20to%20achieve%20such%20fairness.%20To%20improve%0Aindividual%20fairness%2C%20various%20training%20methods%20have%20been%20developed%2C%20such%20as%0Aincorporating%20fairness%20constraints%20as%20optimisation%20objectives.%20While%20these%0Amethods%20have%20demonstrated%20empirical%20effectiveness%2C%20they%20lack%20formal%20guarantees%0Aof%20fairness.%20Existing%20approaches%20that%20aim%20to%20provide%20fairness%20guarantees%0Aprimarily%20rely%20on%20verification%20techniques%2C%20which%20can%20sometimes%20fail%20to%20produce%0Adefinitive%20results.%20Moreover%2C%20verification%20alone%20does%20not%20actively%20enhance%0Aindividual%20fairness%20during%20training.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0Anovel%20framework%20that%20formally%20guarantees%20individual%20fairness%20throughout%0Atraining.%20Our%20approach%20consists%20of%20two%20parts%2C%20i.e.%2C%20%281%29%20provably%20fair%0Ainitialisation%20that%20ensures%20the%20model%20starts%20in%20a%20fair%20state%2C%20and%20%282%29%20a%0Afairness-preserving%20training%20algorithm%20that%20maintains%20fairness%20as%20the%20model%0Alearns.%20A%20key%20element%20of%20our%20method%20is%20the%20use%20of%20randomised%20response%0Amechanisms%2C%20which%20protect%20sensitive%20attributes%20while%20maintaining%20fairness%0Aguarantees.%20We%20formally%20prove%20that%20this%20mechanism%20sustains%20individual%20fairness%0Athroughout%20the%20training%20process.%20Experimental%20evaluations%20confirm%20that%20our%0Aapproach%20is%20effective%2C%20i.e.%2C%20producing%20models%20that%20are%20empirically%20fair%20and%0Aaccurate.%20Furthermore%2C%20our%20approach%20is%20much%20more%20efficient%20than%20the%20alternative%0Aapproach%20based%20on%20certified%20training%20%28which%20requires%20neural%20network%0Averification%20during%20training%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrect-By-Construction%253A%2520Certified%2520Individual%2520Fairness%2520through%2520Neural%250A%2520%2520Network%2520Training%26entry.906535625%3DRuihan%2520Zhang%2520and%2520Jun%2520Sun%26entry.1292438233%3D%2520%2520Fairness%2520in%2520machine%2520learning%2520is%2520more%2520important%2520than%2520ever%2520as%2520ethical%2520concerns%250Acontinue%2520to%2520grow.%2520Individual%2520fairness%2520demands%2520that%2520individuals%2520differing%2520only%250Ain%2520sensitive%2520attributes%2520receive%2520the%2520same%2520outcomes.%2520However%252C%2520commonly%2520used%250Amachine%2520learning%2520algorithms%2520often%2520fail%2520to%2520achieve%2520such%2520fairness.%2520To%2520improve%250Aindividual%2520fairness%252C%2520various%2520training%2520methods%2520have%2520been%2520developed%252C%2520such%2520as%250Aincorporating%2520fairness%2520constraints%2520as%2520optimisation%2520objectives.%2520While%2520these%250Amethods%2520have%2520demonstrated%2520empirical%2520effectiveness%252C%2520they%2520lack%2520formal%2520guarantees%250Aof%2520fairness.%2520Existing%2520approaches%2520that%2520aim%2520to%2520provide%2520fairness%2520guarantees%250Aprimarily%2520rely%2520on%2520verification%2520techniques%252C%2520which%2520can%2520sometimes%2520fail%2520to%2520produce%250Adefinitive%2520results.%2520Moreover%252C%2520verification%2520alone%2520does%2520not%2520actively%2520enhance%250Aindividual%2520fairness%2520during%2520training.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%250Anovel%2520framework%2520that%2520formally%2520guarantees%2520individual%2520fairness%2520throughout%250Atraining.%2520Our%2520approach%2520consists%2520of%2520two%2520parts%252C%2520i.e.%252C%2520%25281%2529%2520provably%2520fair%250Ainitialisation%2520that%2520ensures%2520the%2520model%2520starts%2520in%2520a%2520fair%2520state%252C%2520and%2520%25282%2529%2520a%250Afairness-preserving%2520training%2520algorithm%2520that%2520maintains%2520fairness%2520as%2520the%2520model%250Alearns.%2520A%2520key%2520element%2520of%2520our%2520method%2520is%2520the%2520use%2520of%2520randomised%2520response%250Amechanisms%252C%2520which%2520protect%2520sensitive%2520attributes%2520while%2520maintaining%2520fairness%250Aguarantees.%2520We%2520formally%2520prove%2520that%2520this%2520mechanism%2520sustains%2520individual%2520fairness%250Athroughout%2520the%2520training%2520process.%2520Experimental%2520evaluations%2520confirm%2520that%2520our%250Aapproach%2520is%2520effective%252C%2520i.e.%252C%2520producing%2520models%2520that%2520are%2520empirically%2520fair%2520and%250Aaccurate.%2520Furthermore%252C%2520our%2520approach%2520is%2520much%2520more%2520efficient%2520than%2520the%2520alternative%250Aapproach%2520based%2520on%2520certified%2520training%2520%2528which%2520requires%2520neural%2520network%250Averification%2520during%2520training%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Correct-By-Construction%3A%20Certified%20Individual%20Fairness%20through%20Neural%0A%20%20Network%20Training&entry.906535625=Ruihan%20Zhang%20and%20Jun%20Sun&entry.1292438233=%20%20Fairness%20in%20machine%20learning%20is%20more%20important%20than%20ever%20as%20ethical%20concerns%0Acontinue%20to%20grow.%20Individual%20fairness%20demands%20that%20individuals%20differing%20only%0Ain%20sensitive%20attributes%20receive%20the%20same%20outcomes.%20However%2C%20commonly%20used%0Amachine%20learning%20algorithms%20often%20fail%20to%20achieve%20such%20fairness.%20To%20improve%0Aindividual%20fairness%2C%20various%20training%20methods%20have%20been%20developed%2C%20such%20as%0Aincorporating%20fairness%20constraints%20as%20optimisation%20objectives.%20While%20these%0Amethods%20have%20demonstrated%20empirical%20effectiveness%2C%20they%20lack%20formal%20guarantees%0Aof%20fairness.%20Existing%20approaches%20that%20aim%20to%20provide%20fairness%20guarantees%0Aprimarily%20rely%20on%20verification%20techniques%2C%20which%20can%20sometimes%20fail%20to%20produce%0Adefinitive%20results.%20Moreover%2C%20verification%20alone%20does%20not%20actively%20enhance%0Aindividual%20fairness%20during%20training.%20To%20address%20this%20limitation%2C%20we%20propose%20a%0Anovel%20framework%20that%20formally%20guarantees%20individual%20fairness%20throughout%0Atraining.%20Our%20approach%20consists%20of%20two%20parts%2C%20i.e.%2C%20%281%29%20provably%20fair%0Ainitialisation%20that%20ensures%20the%20model%20starts%20in%20a%20fair%20state%2C%20and%20%282%29%20a%0Afairness-preserving%20training%20algorithm%20that%20maintains%20fairness%20as%20the%20model%0Alearns.%20A%20key%20element%20of%20our%20method%20is%20the%20use%20of%20randomised%20response%0Amechanisms%2C%20which%20protect%20sensitive%20attributes%20while%20maintaining%20fairness%0Aguarantees.%20We%20formally%20prove%20that%20this%20mechanism%20sustains%20individual%20fairness%0Athroughout%20the%20training%20process.%20Experimental%20evaluations%20confirm%20that%20our%0Aapproach%20is%20effective%2C%20i.e.%2C%20producing%20models%20that%20are%20empirically%20fair%20and%0Aaccurate.%20Furthermore%2C%20our%20approach%20is%20much%20more%20efficient%20than%20the%20alternative%0Aapproach%20based%20on%20certified%20training%20%28which%20requires%20neural%20network%0Averification%20during%20training%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15642v1&entry.124074799=Read"},
{"title": "Distributed Detection of Adversarial Attacks in Multi-Agent\n  Reinforcement Learning with Continuous Action Space", "author": "Kiarash Kazari and Ezzeldin Shereen and Gy\u00f6rgy D\u00e1n", "abstract": "  We address the problem of detecting adversarial attacks against cooperative\nmulti-agent reinforcement learning with continuous action space. We propose a\ndecentralized detector that relies solely on the local observations of the\nagents and makes use of a statistical characterization of the normal behavior\nof observable agents. The proposed detector utilizes deep neural networks to\napproximate the normal behavior of agents as parametric multivariate Gaussian\ndistributions. Based on the predicted density functions, we define a normality\nscore and provide a characterization of its mean and variance. This\ncharacterization allows us to employ a two-sided CUSUM procedure for detecting\ndeviations of the normality score from its mean, serving as a detector of\nanomalous behavior in real-time. We evaluate our scheme on various multi-agent\nPettingZoo benchmarks against different state-of-the-art attack methods, and\nour results demonstrate the effectiveness of our method in detecting impactful\nadversarial attacks. Particularly, it outperforms the discrete counterpart by\nachieving AUC-ROC scores of over 0.95 against the most impactful attacks in all\nevaluated environments.\n", "link": "http://arxiv.org/abs/2508.15764v1", "date": "2025-08-21", "relevancy": 2.034, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5311}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5114}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20Detection%20of%20Adversarial%20Attacks%20in%20Multi-Agent%0A%20%20Reinforcement%20Learning%20with%20Continuous%20Action%20Space&body=Title%3A%20Distributed%20Detection%20of%20Adversarial%20Attacks%20in%20Multi-Agent%0A%20%20Reinforcement%20Learning%20with%20Continuous%20Action%20Space%0AAuthor%3A%20Kiarash%20Kazari%20and%20Ezzeldin%20Shereen%20and%20Gy%C3%B6rgy%20D%C3%A1n%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20detecting%20adversarial%20attacks%20against%20cooperative%0Amulti-agent%20reinforcement%20learning%20with%20continuous%20action%20space.%20We%20propose%20a%0Adecentralized%20detector%20that%20relies%20solely%20on%20the%20local%20observations%20of%20the%0Aagents%20and%20makes%20use%20of%20a%20statistical%20characterization%20of%20the%20normal%20behavior%0Aof%20observable%20agents.%20The%20proposed%20detector%20utilizes%20deep%20neural%20networks%20to%0Aapproximate%20the%20normal%20behavior%20of%20agents%20as%20parametric%20multivariate%20Gaussian%0Adistributions.%20Based%20on%20the%20predicted%20density%20functions%2C%20we%20define%20a%20normality%0Ascore%20and%20provide%20a%20characterization%20of%20its%20mean%20and%20variance.%20This%0Acharacterization%20allows%20us%20to%20employ%20a%20two-sided%20CUSUM%20procedure%20for%20detecting%0Adeviations%20of%20the%20normality%20score%20from%20its%20mean%2C%20serving%20as%20a%20detector%20of%0Aanomalous%20behavior%20in%20real-time.%20We%20evaluate%20our%20scheme%20on%20various%20multi-agent%0APettingZoo%20benchmarks%20against%20different%20state-of-the-art%20attack%20methods%2C%20and%0Aour%20results%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20detecting%20impactful%0Aadversarial%20attacks.%20Particularly%2C%20it%20outperforms%20the%20discrete%20counterpart%20by%0Aachieving%20AUC-ROC%20scores%20of%20over%200.95%20against%20the%20most%20impactful%20attacks%20in%20all%0Aevaluated%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520Detection%2520of%2520Adversarial%2520Attacks%2520in%2520Multi-Agent%250A%2520%2520Reinforcement%2520Learning%2520with%2520Continuous%2520Action%2520Space%26entry.906535625%3DKiarash%2520Kazari%2520and%2520Ezzeldin%2520Shereen%2520and%2520Gy%25C3%25B6rgy%2520D%25C3%25A1n%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520detecting%2520adversarial%2520attacks%2520against%2520cooperative%250Amulti-agent%2520reinforcement%2520learning%2520with%2520continuous%2520action%2520space.%2520We%2520propose%2520a%250Adecentralized%2520detector%2520that%2520relies%2520solely%2520on%2520the%2520local%2520observations%2520of%2520the%250Aagents%2520and%2520makes%2520use%2520of%2520a%2520statistical%2520characterization%2520of%2520the%2520normal%2520behavior%250Aof%2520observable%2520agents.%2520The%2520proposed%2520detector%2520utilizes%2520deep%2520neural%2520networks%2520to%250Aapproximate%2520the%2520normal%2520behavior%2520of%2520agents%2520as%2520parametric%2520multivariate%2520Gaussian%250Adistributions.%2520Based%2520on%2520the%2520predicted%2520density%2520functions%252C%2520we%2520define%2520a%2520normality%250Ascore%2520and%2520provide%2520a%2520characterization%2520of%2520its%2520mean%2520and%2520variance.%2520This%250Acharacterization%2520allows%2520us%2520to%2520employ%2520a%2520two-sided%2520CUSUM%2520procedure%2520for%2520detecting%250Adeviations%2520of%2520the%2520normality%2520score%2520from%2520its%2520mean%252C%2520serving%2520as%2520a%2520detector%2520of%250Aanomalous%2520behavior%2520in%2520real-time.%2520We%2520evaluate%2520our%2520scheme%2520on%2520various%2520multi-agent%250APettingZoo%2520benchmarks%2520against%2520different%2520state-of-the-art%2520attack%2520methods%252C%2520and%250Aour%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520detecting%2520impactful%250Aadversarial%2520attacks.%2520Particularly%252C%2520it%2520outperforms%2520the%2520discrete%2520counterpart%2520by%250Aachieving%2520AUC-ROC%2520scores%2520of%2520over%25200.95%2520against%2520the%2520most%2520impactful%2520attacks%2520in%2520all%250Aevaluated%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Detection%20of%20Adversarial%20Attacks%20in%20Multi-Agent%0A%20%20Reinforcement%20Learning%20with%20Continuous%20Action%20Space&entry.906535625=Kiarash%20Kazari%20and%20Ezzeldin%20Shereen%20and%20Gy%C3%B6rgy%20D%C3%A1n&entry.1292438233=%20%20We%20address%20the%20problem%20of%20detecting%20adversarial%20attacks%20against%20cooperative%0Amulti-agent%20reinforcement%20learning%20with%20continuous%20action%20space.%20We%20propose%20a%0Adecentralized%20detector%20that%20relies%20solely%20on%20the%20local%20observations%20of%20the%0Aagents%20and%20makes%20use%20of%20a%20statistical%20characterization%20of%20the%20normal%20behavior%0Aof%20observable%20agents.%20The%20proposed%20detector%20utilizes%20deep%20neural%20networks%20to%0Aapproximate%20the%20normal%20behavior%20of%20agents%20as%20parametric%20multivariate%20Gaussian%0Adistributions.%20Based%20on%20the%20predicted%20density%20functions%2C%20we%20define%20a%20normality%0Ascore%20and%20provide%20a%20characterization%20of%20its%20mean%20and%20variance.%20This%0Acharacterization%20allows%20us%20to%20employ%20a%20two-sided%20CUSUM%20procedure%20for%20detecting%0Adeviations%20of%20the%20normality%20score%20from%20its%20mean%2C%20serving%20as%20a%20detector%20of%0Aanomalous%20behavior%20in%20real-time.%20We%20evaluate%20our%20scheme%20on%20various%20multi-agent%0APettingZoo%20benchmarks%20against%20different%20state-of-the-art%20attack%20methods%2C%20and%0Aour%20results%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20detecting%20impactful%0Aadversarial%20attacks.%20Particularly%2C%20it%20outperforms%20the%20discrete%20counterpart%20by%0Aachieving%20AUC-ROC%20scores%20of%20over%200.95%20against%20the%20most%20impactful%20attacks%20in%20all%0Aevaluated%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15764v1&entry.124074799=Read"},
{"title": "Mini-Batch Robustness Verification of Deep Neural Networks", "author": "Saar Tzour-Shaday and Dana Drachsler Cohen", "abstract": "  Neural network image classifiers are ubiquitous in many safety-critical\napplications. However, they are susceptible to adversarial attacks. To\nunderstand their robustness to attacks, many local robustness verifiers have\nbeen proposed to analyze $\\epsilon$-balls of inputs. Yet, existing verifiers\nintroduce a long analysis time or lose too much precision, making them less\neffective for a large set of inputs. In this work, we propose a new approach to\nlocal robustness: group local robustness verification. The key idea is to\nleverage the similarity of the network computations of certain $\\epsilon$-balls\nto reduce the overall analysis time. We propose BaVerLy, a sound and complete\nverifier that boosts the local robustness verification of a set of\n$\\epsilon$-balls by dynamically constructing and verifying mini-batches.\nBaVerLy adaptively identifies successful mini-batch sizes, accordingly\nconstructs mini-batches of $\\epsilon$-balls that have similar network\ncomputations, and verifies them jointly. If a mini-batch is verified, all\n$\\epsilon$-balls are proven robust. Otherwise, one $\\epsilon$-ball is suspected\nas not being robust, guiding the refinement. In the latter case, BaVerLy\nleverages the analysis results to expedite the analysis of that $\\epsilon$-ball\nas well as the other $\\epsilon$-balls in the batch. We evaluate BaVerLy on\nfully connected and convolutional networks for MNIST and CIFAR-10. Results show\nthat BaVerLy scales the common one by one verification by 2.3x on average and\nup to 4.1x, in which case it reduces the total analysis time from 24 hours to 6\nhours.\n", "link": "http://arxiv.org/abs/2508.15454v1", "date": "2025-08-21", "relevancy": 2.0307, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5126}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5106}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mini-Batch%20Robustness%20Verification%20of%20Deep%20Neural%20Networks&body=Title%3A%20Mini-Batch%20Robustness%20Verification%20of%20Deep%20Neural%20Networks%0AAuthor%3A%20Saar%20Tzour-Shaday%20and%20Dana%20Drachsler%20Cohen%0AAbstract%3A%20%20%20Neural%20network%20image%20classifiers%20are%20ubiquitous%20in%20many%20safety-critical%0Aapplications.%20However%2C%20they%20are%20susceptible%20to%20adversarial%20attacks.%20To%0Aunderstand%20their%20robustness%20to%20attacks%2C%20many%20local%20robustness%20verifiers%20have%0Abeen%20proposed%20to%20analyze%20%24%5Cepsilon%24-balls%20of%20inputs.%20Yet%2C%20existing%20verifiers%0Aintroduce%20a%20long%20analysis%20time%20or%20lose%20too%20much%20precision%2C%20making%20them%20less%0Aeffective%20for%20a%20large%20set%20of%20inputs.%20In%20this%20work%2C%20we%20propose%20a%20new%20approach%20to%0Alocal%20robustness%3A%20group%20local%20robustness%20verification.%20The%20key%20idea%20is%20to%0Aleverage%20the%20similarity%20of%20the%20network%20computations%20of%20certain%20%24%5Cepsilon%24-balls%0Ato%20reduce%20the%20overall%20analysis%20time.%20We%20propose%20BaVerLy%2C%20a%20sound%20and%20complete%0Averifier%20that%20boosts%20the%20local%20robustness%20verification%20of%20a%20set%20of%0A%24%5Cepsilon%24-balls%20by%20dynamically%20constructing%20and%20verifying%20mini-batches.%0ABaVerLy%20adaptively%20identifies%20successful%20mini-batch%20sizes%2C%20accordingly%0Aconstructs%20mini-batches%20of%20%24%5Cepsilon%24-balls%20that%20have%20similar%20network%0Acomputations%2C%20and%20verifies%20them%20jointly.%20If%20a%20mini-batch%20is%20verified%2C%20all%0A%24%5Cepsilon%24-balls%20are%20proven%20robust.%20Otherwise%2C%20one%20%24%5Cepsilon%24-ball%20is%20suspected%0Aas%20not%20being%20robust%2C%20guiding%20the%20refinement.%20In%20the%20latter%20case%2C%20BaVerLy%0Aleverages%20the%20analysis%20results%20to%20expedite%20the%20analysis%20of%20that%20%24%5Cepsilon%24-ball%0Aas%20well%20as%20the%20other%20%24%5Cepsilon%24-balls%20in%20the%20batch.%20We%20evaluate%20BaVerLy%20on%0Afully%20connected%20and%20convolutional%20networks%20for%20MNIST%20and%20CIFAR-10.%20Results%20show%0Athat%20BaVerLy%20scales%20the%20common%20one%20by%20one%20verification%20by%202.3x%20on%20average%20and%0Aup%20to%204.1x%2C%20in%20which%20case%20it%20reduces%20the%20total%20analysis%20time%20from%2024%20hours%20to%206%0Ahours.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMini-Batch%2520Robustness%2520Verification%2520of%2520Deep%2520Neural%2520Networks%26entry.906535625%3DSaar%2520Tzour-Shaday%2520and%2520Dana%2520Drachsler%2520Cohen%26entry.1292438233%3D%2520%2520Neural%2520network%2520image%2520classifiers%2520are%2520ubiquitous%2520in%2520many%2520safety-critical%250Aapplications.%2520However%252C%2520they%2520are%2520susceptible%2520to%2520adversarial%2520attacks.%2520To%250Aunderstand%2520their%2520robustness%2520to%2520attacks%252C%2520many%2520local%2520robustness%2520verifiers%2520have%250Abeen%2520proposed%2520to%2520analyze%2520%2524%255Cepsilon%2524-balls%2520of%2520inputs.%2520Yet%252C%2520existing%2520verifiers%250Aintroduce%2520a%2520long%2520analysis%2520time%2520or%2520lose%2520too%2520much%2520precision%252C%2520making%2520them%2520less%250Aeffective%2520for%2520a%2520large%2520set%2520of%2520inputs.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520approach%2520to%250Alocal%2520robustness%253A%2520group%2520local%2520robustness%2520verification.%2520The%2520key%2520idea%2520is%2520to%250Aleverage%2520the%2520similarity%2520of%2520the%2520network%2520computations%2520of%2520certain%2520%2524%255Cepsilon%2524-balls%250Ato%2520reduce%2520the%2520overall%2520analysis%2520time.%2520We%2520propose%2520BaVerLy%252C%2520a%2520sound%2520and%2520complete%250Averifier%2520that%2520boosts%2520the%2520local%2520robustness%2520verification%2520of%2520a%2520set%2520of%250A%2524%255Cepsilon%2524-balls%2520by%2520dynamically%2520constructing%2520and%2520verifying%2520mini-batches.%250ABaVerLy%2520adaptively%2520identifies%2520successful%2520mini-batch%2520sizes%252C%2520accordingly%250Aconstructs%2520mini-batches%2520of%2520%2524%255Cepsilon%2524-balls%2520that%2520have%2520similar%2520network%250Acomputations%252C%2520and%2520verifies%2520them%2520jointly.%2520If%2520a%2520mini-batch%2520is%2520verified%252C%2520all%250A%2524%255Cepsilon%2524-balls%2520are%2520proven%2520robust.%2520Otherwise%252C%2520one%2520%2524%255Cepsilon%2524-ball%2520is%2520suspected%250Aas%2520not%2520being%2520robust%252C%2520guiding%2520the%2520refinement.%2520In%2520the%2520latter%2520case%252C%2520BaVerLy%250Aleverages%2520the%2520analysis%2520results%2520to%2520expedite%2520the%2520analysis%2520of%2520that%2520%2524%255Cepsilon%2524-ball%250Aas%2520well%2520as%2520the%2520other%2520%2524%255Cepsilon%2524-balls%2520in%2520the%2520batch.%2520We%2520evaluate%2520BaVerLy%2520on%250Afully%2520connected%2520and%2520convolutional%2520networks%2520for%2520MNIST%2520and%2520CIFAR-10.%2520Results%2520show%250Athat%2520BaVerLy%2520scales%2520the%2520common%2520one%2520by%2520one%2520verification%2520by%25202.3x%2520on%2520average%2520and%250Aup%2520to%25204.1x%252C%2520in%2520which%2520case%2520it%2520reduces%2520the%2520total%2520analysis%2520time%2520from%252024%2520hours%2520to%25206%250Ahours.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mini-Batch%20Robustness%20Verification%20of%20Deep%20Neural%20Networks&entry.906535625=Saar%20Tzour-Shaday%20and%20Dana%20Drachsler%20Cohen&entry.1292438233=%20%20Neural%20network%20image%20classifiers%20are%20ubiquitous%20in%20many%20safety-critical%0Aapplications.%20However%2C%20they%20are%20susceptible%20to%20adversarial%20attacks.%20To%0Aunderstand%20their%20robustness%20to%20attacks%2C%20many%20local%20robustness%20verifiers%20have%0Abeen%20proposed%20to%20analyze%20%24%5Cepsilon%24-balls%20of%20inputs.%20Yet%2C%20existing%20verifiers%0Aintroduce%20a%20long%20analysis%20time%20or%20lose%20too%20much%20precision%2C%20making%20them%20less%0Aeffective%20for%20a%20large%20set%20of%20inputs.%20In%20this%20work%2C%20we%20propose%20a%20new%20approach%20to%0Alocal%20robustness%3A%20group%20local%20robustness%20verification.%20The%20key%20idea%20is%20to%0Aleverage%20the%20similarity%20of%20the%20network%20computations%20of%20certain%20%24%5Cepsilon%24-balls%0Ato%20reduce%20the%20overall%20analysis%20time.%20We%20propose%20BaVerLy%2C%20a%20sound%20and%20complete%0Averifier%20that%20boosts%20the%20local%20robustness%20verification%20of%20a%20set%20of%0A%24%5Cepsilon%24-balls%20by%20dynamically%20constructing%20and%20verifying%20mini-batches.%0ABaVerLy%20adaptively%20identifies%20successful%20mini-batch%20sizes%2C%20accordingly%0Aconstructs%20mini-batches%20of%20%24%5Cepsilon%24-balls%20that%20have%20similar%20network%0Acomputations%2C%20and%20verifies%20them%20jointly.%20If%20a%20mini-batch%20is%20verified%2C%20all%0A%24%5Cepsilon%24-balls%20are%20proven%20robust.%20Otherwise%2C%20one%20%24%5Cepsilon%24-ball%20is%20suspected%0Aas%20not%20being%20robust%2C%20guiding%20the%20refinement.%20In%20the%20latter%20case%2C%20BaVerLy%0Aleverages%20the%20analysis%20results%20to%20expedite%20the%20analysis%20of%20that%20%24%5Cepsilon%24-ball%0Aas%20well%20as%20the%20other%20%24%5Cepsilon%24-balls%20in%20the%20batch.%20We%20evaluate%20BaVerLy%20on%0Afully%20connected%20and%20convolutional%20networks%20for%20MNIST%20and%20CIFAR-10.%20Results%20show%0Athat%20BaVerLy%20scales%20the%20common%20one%20by%20one%20verification%20by%202.3x%20on%20average%20and%0Aup%20to%204.1x%2C%20in%20which%20case%20it%20reduces%20the%20total%20analysis%20time%20from%2024%20hours%20to%206%0Ahours.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15454v1&entry.124074799=Read"},
{"title": "LLaSO: A Foundational Framework for Reproducible Research in Large\n  Language and Speech Model", "author": "Yirong Sun and Yizhong Geng and Peidong Wei and Yanjun Chen and Jinghan Yang and Rongfei Chen and Wei Zhang and Xiaoyu Shen", "abstract": "  The development of Large Speech-Language Models (LSLMs) has been slowed by\nfragmented architectures and a lack of transparency, hindering the systematic\ncomparison and reproducibility of research. Unlike in the vision-language\ndomain, the LSLM field suffers from the common practice of releasing model\nweights without their corresponding training data and configurations. To\naddress these critical gaps, we introduce LLaSO, the first fully open,\nend-to-end framework for large-scale speech-language modeling. LLaSO provides\nthe community with three essential resources: (1) LLaSO-Align, a 12M-instance\nspeech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task\ninstruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for\nstandardized evaluation. To validate our framework, we build and release\nLLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public\ndata. It achieves a normalized score of 0.72, establishing a strong,\nreproducible baseline that surpasses comparable models. Our analysis reveals\nthat while broader training coverage enhances performance, significant\ngeneralization gaps persist on unseen tasks, particularly in pure audio\nscenarios. By releasing the complete stack of data, benchmarks, and models,\nLLaSO establishes a foundational open standard to unify research efforts and\naccelerate community-driven progress in LSLMs. We release the code, dataset,\npretrained models, and results in https://github.com/EIT-NLP/LLaSO.\n", "link": "http://arxiv.org/abs/2508.15418v1", "date": "2025-08-21", "relevancy": 2.0255, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaSO%3A%20A%20Foundational%20Framework%20for%20Reproducible%20Research%20in%20Large%0A%20%20Language%20and%20Speech%20Model&body=Title%3A%20LLaSO%3A%20A%20Foundational%20Framework%20for%20Reproducible%20Research%20in%20Large%0A%20%20Language%20and%20Speech%20Model%0AAuthor%3A%20Yirong%20Sun%20and%20Yizhong%20Geng%20and%20Peidong%20Wei%20and%20Yanjun%20Chen%20and%20Jinghan%20Yang%20and%20Rongfei%20Chen%20and%20Wei%20Zhang%20and%20Xiaoyu%20Shen%0AAbstract%3A%20%20%20The%20development%20of%20Large%20Speech-Language%20Models%20%28LSLMs%29%20has%20been%20slowed%20by%0Afragmented%20architectures%20and%20a%20lack%20of%20transparency%2C%20hindering%20the%20systematic%0Acomparison%20and%20reproducibility%20of%20research.%20Unlike%20in%20the%20vision-language%0Adomain%2C%20the%20LSLM%20field%20suffers%20from%20the%20common%20practice%20of%20releasing%20model%0Aweights%20without%20their%20corresponding%20training%20data%20and%20configurations.%20To%0Aaddress%20these%20critical%20gaps%2C%20we%20introduce%20LLaSO%2C%20the%20first%20fully%20open%2C%0Aend-to-end%20framework%20for%20large-scale%20speech-language%20modeling.%20LLaSO%20provides%0Athe%20community%20with%20three%20essential%20resources%3A%20%281%29%20LLaSO-Align%2C%20a%2012M-instance%0Aspeech-text%20alignment%20corpus%3B%20%282%29%20LLaSO-Instruct%2C%20a%2013.5M-instance%20multi-task%0Ainstruction-tuning%20dataset%3B%20and%20%283%29%20LLaSO-Eval%2C%20a%20reproducible%20benchmark%20for%0Astandardized%20evaluation.%20To%20validate%20our%20framework%2C%20we%20build%20and%20release%0ALLaSO-Base%2C%20a%203.8B-parameter%20reference%20model%20trained%20exclusively%20on%20our%20public%0Adata.%20It%20achieves%20a%20normalized%20score%20of%200.72%2C%20establishing%20a%20strong%2C%0Areproducible%20baseline%20that%20surpasses%20comparable%20models.%20Our%20analysis%20reveals%0Athat%20while%20broader%20training%20coverage%20enhances%20performance%2C%20significant%0Ageneralization%20gaps%20persist%20on%20unseen%20tasks%2C%20particularly%20in%20pure%20audio%0Ascenarios.%20By%20releasing%20the%20complete%20stack%20of%20data%2C%20benchmarks%2C%20and%20models%2C%0ALLaSO%20establishes%20a%20foundational%20open%20standard%20to%20unify%20research%20efforts%20and%0Aaccelerate%20community-driven%20progress%20in%20LSLMs.%20We%20release%20the%20code%2C%20dataset%2C%0Apretrained%20models%2C%20and%20results%20in%20https%3A//github.com/EIT-NLP/LLaSO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaSO%253A%2520A%2520Foundational%2520Framework%2520for%2520Reproducible%2520Research%2520in%2520Large%250A%2520%2520Language%2520and%2520Speech%2520Model%26entry.906535625%3DYirong%2520Sun%2520and%2520Yizhong%2520Geng%2520and%2520Peidong%2520Wei%2520and%2520Yanjun%2520Chen%2520and%2520Jinghan%2520Yang%2520and%2520Rongfei%2520Chen%2520and%2520Wei%2520Zhang%2520and%2520Xiaoyu%2520Shen%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520Large%2520Speech-Language%2520Models%2520%2528LSLMs%2529%2520has%2520been%2520slowed%2520by%250Afragmented%2520architectures%2520and%2520a%2520lack%2520of%2520transparency%252C%2520hindering%2520the%2520systematic%250Acomparison%2520and%2520reproducibility%2520of%2520research.%2520Unlike%2520in%2520the%2520vision-language%250Adomain%252C%2520the%2520LSLM%2520field%2520suffers%2520from%2520the%2520common%2520practice%2520of%2520releasing%2520model%250Aweights%2520without%2520their%2520corresponding%2520training%2520data%2520and%2520configurations.%2520To%250Aaddress%2520these%2520critical%2520gaps%252C%2520we%2520introduce%2520LLaSO%252C%2520the%2520first%2520fully%2520open%252C%250Aend-to-end%2520framework%2520for%2520large-scale%2520speech-language%2520modeling.%2520LLaSO%2520provides%250Athe%2520community%2520with%2520three%2520essential%2520resources%253A%2520%25281%2529%2520LLaSO-Align%252C%2520a%252012M-instance%250Aspeech-text%2520alignment%2520corpus%253B%2520%25282%2529%2520LLaSO-Instruct%252C%2520a%252013.5M-instance%2520multi-task%250Ainstruction-tuning%2520dataset%253B%2520and%2520%25283%2529%2520LLaSO-Eval%252C%2520a%2520reproducible%2520benchmark%2520for%250Astandardized%2520evaluation.%2520To%2520validate%2520our%2520framework%252C%2520we%2520build%2520and%2520release%250ALLaSO-Base%252C%2520a%25203.8B-parameter%2520reference%2520model%2520trained%2520exclusively%2520on%2520our%2520public%250Adata.%2520It%2520achieves%2520a%2520normalized%2520score%2520of%25200.72%252C%2520establishing%2520a%2520strong%252C%250Areproducible%2520baseline%2520that%2520surpasses%2520comparable%2520models.%2520Our%2520analysis%2520reveals%250Athat%2520while%2520broader%2520training%2520coverage%2520enhances%2520performance%252C%2520significant%250Ageneralization%2520gaps%2520persist%2520on%2520unseen%2520tasks%252C%2520particularly%2520in%2520pure%2520audio%250Ascenarios.%2520By%2520releasing%2520the%2520complete%2520stack%2520of%2520data%252C%2520benchmarks%252C%2520and%2520models%252C%250ALLaSO%2520establishes%2520a%2520foundational%2520open%2520standard%2520to%2520unify%2520research%2520efforts%2520and%250Aaccelerate%2520community-driven%2520progress%2520in%2520LSLMs.%2520We%2520release%2520the%2520code%252C%2520dataset%252C%250Apretrained%2520models%252C%2520and%2520results%2520in%2520https%253A//github.com/EIT-NLP/LLaSO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaSO%3A%20A%20Foundational%20Framework%20for%20Reproducible%20Research%20in%20Large%0A%20%20Language%20and%20Speech%20Model&entry.906535625=Yirong%20Sun%20and%20Yizhong%20Geng%20and%20Peidong%20Wei%20and%20Yanjun%20Chen%20and%20Jinghan%20Yang%20and%20Rongfei%20Chen%20and%20Wei%20Zhang%20and%20Xiaoyu%20Shen&entry.1292438233=%20%20The%20development%20of%20Large%20Speech-Language%20Models%20%28LSLMs%29%20has%20been%20slowed%20by%0Afragmented%20architectures%20and%20a%20lack%20of%20transparency%2C%20hindering%20the%20systematic%0Acomparison%20and%20reproducibility%20of%20research.%20Unlike%20in%20the%20vision-language%0Adomain%2C%20the%20LSLM%20field%20suffers%20from%20the%20common%20practice%20of%20releasing%20model%0Aweights%20without%20their%20corresponding%20training%20data%20and%20configurations.%20To%0Aaddress%20these%20critical%20gaps%2C%20we%20introduce%20LLaSO%2C%20the%20first%20fully%20open%2C%0Aend-to-end%20framework%20for%20large-scale%20speech-language%20modeling.%20LLaSO%20provides%0Athe%20community%20with%20three%20essential%20resources%3A%20%281%29%20LLaSO-Align%2C%20a%2012M-instance%0Aspeech-text%20alignment%20corpus%3B%20%282%29%20LLaSO-Instruct%2C%20a%2013.5M-instance%20multi-task%0Ainstruction-tuning%20dataset%3B%20and%20%283%29%20LLaSO-Eval%2C%20a%20reproducible%20benchmark%20for%0Astandardized%20evaluation.%20To%20validate%20our%20framework%2C%20we%20build%20and%20release%0ALLaSO-Base%2C%20a%203.8B-parameter%20reference%20model%20trained%20exclusively%20on%20our%20public%0Adata.%20It%20achieves%20a%20normalized%20score%20of%200.72%2C%20establishing%20a%20strong%2C%0Areproducible%20baseline%20that%20surpasses%20comparable%20models.%20Our%20analysis%20reveals%0Athat%20while%20broader%20training%20coverage%20enhances%20performance%2C%20significant%0Ageneralization%20gaps%20persist%20on%20unseen%20tasks%2C%20particularly%20in%20pure%20audio%0Ascenarios.%20By%20releasing%20the%20complete%20stack%20of%20data%2C%20benchmarks%2C%20and%20models%2C%0ALLaSO%20establishes%20a%20foundational%20open%20standard%20to%20unify%20research%20efforts%20and%0Aaccelerate%20community-driven%20progress%20in%20LSLMs.%20We%20release%20the%20code%2C%20dataset%2C%0Apretrained%20models%2C%20and%20results%20in%20https%3A//github.com/EIT-NLP/LLaSO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15418v1&entry.124074799=Read"},
{"title": "Adapting A Vector-Symbolic Memory for Lisp ACT-R", "author": "Meera Ray and Christopher L. Dancy", "abstract": "  Holographic Declarative Memory (HDM) is a vector-symbolic alternative to\nACT-R's Declarative Memory (DM) system that can bring advantages such as\nscalability and architecturally defined similarity between DM chunks. We\nadapted HDM to work with the most comprehensive and widely-used implementation\nof ACT-R (Lisp ACT-R) so extant ACT-R models designed with DM can be run with\nHDM without major changes. With this adaptation of HDM, we have developed\nvector-based versions of common ACT-R functions, set up a text processing\npipeline to add the contents of large documents to ACT-R memory, and most\nsignificantly created a useful and novel mechanism to retrieve an entire chunk\nof memory based on a request using only vector representations of tokens.\nPreliminary results indicate that we can maintain vector-symbolic advantages of\nHDM (e.g., chunk recall without storing the actual chunk and other advantages\nwith scaling) while also extending it so that previous ACT-R models may work\nwith the system with little (or potentially no) modifications within the actual\nprocedural and declarative memory portions of a model. As a part of iterative\nimprovement of this newly translated holographic declarative memory module, we\nwill continue to explore better time-context representations for vectors to\nimprove the module's ability to reconstruct chunks during recall. To more fully\ntest this translated HDM module, we also plan to develop decision-making models\nthat use instance-based learning (IBL) theory, which is a useful application of\nHDM given the advantages of the system.\n", "link": "http://arxiv.org/abs/2508.15630v1", "date": "2025-08-21", "relevancy": 2.0223, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5301}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4966}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20A%20Vector-Symbolic%20Memory%20for%20Lisp%20ACT-R&body=Title%3A%20Adapting%20A%20Vector-Symbolic%20Memory%20for%20Lisp%20ACT-R%0AAuthor%3A%20Meera%20Ray%20and%20Christopher%20L.%20Dancy%0AAbstract%3A%20%20%20Holographic%20Declarative%20Memory%20%28HDM%29%20is%20a%20vector-symbolic%20alternative%20to%0AACT-R%27s%20Declarative%20Memory%20%28DM%29%20system%20that%20can%20bring%20advantages%20such%20as%0Ascalability%20and%20architecturally%20defined%20similarity%20between%20DM%20chunks.%20We%0Aadapted%20HDM%20to%20work%20with%20the%20most%20comprehensive%20and%20widely-used%20implementation%0Aof%20ACT-R%20%28Lisp%20ACT-R%29%20so%20extant%20ACT-R%20models%20designed%20with%20DM%20can%20be%20run%20with%0AHDM%20without%20major%20changes.%20With%20this%20adaptation%20of%20HDM%2C%20we%20have%20developed%0Avector-based%20versions%20of%20common%20ACT-R%20functions%2C%20set%20up%20a%20text%20processing%0Apipeline%20to%20add%20the%20contents%20of%20large%20documents%20to%20ACT-R%20memory%2C%20and%20most%0Asignificantly%20created%20a%20useful%20and%20novel%20mechanism%20to%20retrieve%20an%20entire%20chunk%0Aof%20memory%20based%20on%20a%20request%20using%20only%20vector%20representations%20of%20tokens.%0APreliminary%20results%20indicate%20that%20we%20can%20maintain%20vector-symbolic%20advantages%20of%0AHDM%20%28e.g.%2C%20chunk%20recall%20without%20storing%20the%20actual%20chunk%20and%20other%20advantages%0Awith%20scaling%29%20while%20also%20extending%20it%20so%20that%20previous%20ACT-R%20models%20may%20work%0Awith%20the%20system%20with%20little%20%28or%20potentially%20no%29%20modifications%20within%20the%20actual%0Aprocedural%20and%20declarative%20memory%20portions%20of%20a%20model.%20As%20a%20part%20of%20iterative%0Aimprovement%20of%20this%20newly%20translated%20holographic%20declarative%20memory%20module%2C%20we%0Awill%20continue%20to%20explore%20better%20time-context%20representations%20for%20vectors%20to%0Aimprove%20the%20module%27s%20ability%20to%20reconstruct%20chunks%20during%20recall.%20To%20more%20fully%0Atest%20this%20translated%20HDM%20module%2C%20we%20also%20plan%20to%20develop%20decision-making%20models%0Athat%20use%20instance-based%20learning%20%28IBL%29%20theory%2C%20which%20is%20a%20useful%20application%20of%0AHDM%20given%20the%20advantages%20of%20the%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520A%2520Vector-Symbolic%2520Memory%2520for%2520Lisp%2520ACT-R%26entry.906535625%3DMeera%2520Ray%2520and%2520Christopher%2520L.%2520Dancy%26entry.1292438233%3D%2520%2520Holographic%2520Declarative%2520Memory%2520%2528HDM%2529%2520is%2520a%2520vector-symbolic%2520alternative%2520to%250AACT-R%2527s%2520Declarative%2520Memory%2520%2528DM%2529%2520system%2520that%2520can%2520bring%2520advantages%2520such%2520as%250Ascalability%2520and%2520architecturally%2520defined%2520similarity%2520between%2520DM%2520chunks.%2520We%250Aadapted%2520HDM%2520to%2520work%2520with%2520the%2520most%2520comprehensive%2520and%2520widely-used%2520implementation%250Aof%2520ACT-R%2520%2528Lisp%2520ACT-R%2529%2520so%2520extant%2520ACT-R%2520models%2520designed%2520with%2520DM%2520can%2520be%2520run%2520with%250AHDM%2520without%2520major%2520changes.%2520With%2520this%2520adaptation%2520of%2520HDM%252C%2520we%2520have%2520developed%250Avector-based%2520versions%2520of%2520common%2520ACT-R%2520functions%252C%2520set%2520up%2520a%2520text%2520processing%250Apipeline%2520to%2520add%2520the%2520contents%2520of%2520large%2520documents%2520to%2520ACT-R%2520memory%252C%2520and%2520most%250Asignificantly%2520created%2520a%2520useful%2520and%2520novel%2520mechanism%2520to%2520retrieve%2520an%2520entire%2520chunk%250Aof%2520memory%2520based%2520on%2520a%2520request%2520using%2520only%2520vector%2520representations%2520of%2520tokens.%250APreliminary%2520results%2520indicate%2520that%2520we%2520can%2520maintain%2520vector-symbolic%2520advantages%2520of%250AHDM%2520%2528e.g.%252C%2520chunk%2520recall%2520without%2520storing%2520the%2520actual%2520chunk%2520and%2520other%2520advantages%250Awith%2520scaling%2529%2520while%2520also%2520extending%2520it%2520so%2520that%2520previous%2520ACT-R%2520models%2520may%2520work%250Awith%2520the%2520system%2520with%2520little%2520%2528or%2520potentially%2520no%2529%2520modifications%2520within%2520the%2520actual%250Aprocedural%2520and%2520declarative%2520memory%2520portions%2520of%2520a%2520model.%2520As%2520a%2520part%2520of%2520iterative%250Aimprovement%2520of%2520this%2520newly%2520translated%2520holographic%2520declarative%2520memory%2520module%252C%2520we%250Awill%2520continue%2520to%2520explore%2520better%2520time-context%2520representations%2520for%2520vectors%2520to%250Aimprove%2520the%2520module%2527s%2520ability%2520to%2520reconstruct%2520chunks%2520during%2520recall.%2520To%2520more%2520fully%250Atest%2520this%2520translated%2520HDM%2520module%252C%2520we%2520also%2520plan%2520to%2520develop%2520decision-making%2520models%250Athat%2520use%2520instance-based%2520learning%2520%2528IBL%2529%2520theory%252C%2520which%2520is%2520a%2520useful%2520application%2520of%250AHDM%2520given%2520the%2520advantages%2520of%2520the%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20A%20Vector-Symbolic%20Memory%20for%20Lisp%20ACT-R&entry.906535625=Meera%20Ray%20and%20Christopher%20L.%20Dancy&entry.1292438233=%20%20Holographic%20Declarative%20Memory%20%28HDM%29%20is%20a%20vector-symbolic%20alternative%20to%0AACT-R%27s%20Declarative%20Memory%20%28DM%29%20system%20that%20can%20bring%20advantages%20such%20as%0Ascalability%20and%20architecturally%20defined%20similarity%20between%20DM%20chunks.%20We%0Aadapted%20HDM%20to%20work%20with%20the%20most%20comprehensive%20and%20widely-used%20implementation%0Aof%20ACT-R%20%28Lisp%20ACT-R%29%20so%20extant%20ACT-R%20models%20designed%20with%20DM%20can%20be%20run%20with%0AHDM%20without%20major%20changes.%20With%20this%20adaptation%20of%20HDM%2C%20we%20have%20developed%0Avector-based%20versions%20of%20common%20ACT-R%20functions%2C%20set%20up%20a%20text%20processing%0Apipeline%20to%20add%20the%20contents%20of%20large%20documents%20to%20ACT-R%20memory%2C%20and%20most%0Asignificantly%20created%20a%20useful%20and%20novel%20mechanism%20to%20retrieve%20an%20entire%20chunk%0Aof%20memory%20based%20on%20a%20request%20using%20only%20vector%20representations%20of%20tokens.%0APreliminary%20results%20indicate%20that%20we%20can%20maintain%20vector-symbolic%20advantages%20of%0AHDM%20%28e.g.%2C%20chunk%20recall%20without%20storing%20the%20actual%20chunk%20and%20other%20advantages%0Awith%20scaling%29%20while%20also%20extending%20it%20so%20that%20previous%20ACT-R%20models%20may%20work%0Awith%20the%20system%20with%20little%20%28or%20potentially%20no%29%20modifications%20within%20the%20actual%0Aprocedural%20and%20declarative%20memory%20portions%20of%20a%20model.%20As%20a%20part%20of%20iterative%0Aimprovement%20of%20this%20newly%20translated%20holographic%20declarative%20memory%20module%2C%20we%0Awill%20continue%20to%20explore%20better%20time-context%20representations%20for%20vectors%20to%0Aimprove%20the%20module%27s%20ability%20to%20reconstruct%20chunks%20during%20recall.%20To%20more%20fully%0Atest%20this%20translated%20HDM%20module%2C%20we%20also%20plan%20to%20develop%20decision-making%20models%0Athat%20use%20instance-based%20learning%20%28IBL%29%20theory%2C%20which%20is%20a%20useful%20application%20of%0AHDM%20given%20the%20advantages%20of%20the%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15630v1&entry.124074799=Read"},
{"title": "Mitigating Hallucinations in LM-Based TTS Models via Distribution\n  Alignment Using GFlowNets", "author": "Chenlin Liu and Minghui Fang and Patrick Zhang and Wei Zhou and Jie Gao and Jiqing Han", "abstract": "  Language Model (LM)-based Text-to-Speech (TTS) systems often generate\nhallucinated speech that deviates from input text. Existing mitigation\nstrategies either demand excessive training resources or introduce significant\ninference latency. In this paper, we propose GFlOwNet-guided distribution\nAlignmenT (GOAT) for LM-based TTS, a post-training framework that mitigates\nhallucinations without relying on massive resources or inference cost.\nSpecifically, we first conduct an uncertainty analysis, revealing a strong\npositive correlation between hallucination and model uncertainty. Based on\nthis, we reformulate TTS generation as a trajectory flow optimization problem\nand introduce an enhanced Subtrajectory Balance objective together with a\nsharpened internal reward as target distribution. We further integrate reward\ntemperature decay and learning rate optimization for stability and performance\nbalance. Extensive experiments show that GOAT reduce over 50% character error\nrates on challenging test cases and lowering uncertainty by up to 58%,\ndemonstrating its strong generalization ability and effectiveness.\n", "link": "http://arxiv.org/abs/2508.15442v1", "date": "2025-08-21", "relevancy": 2.0193, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5636}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4984}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Hallucinations%20in%20LM-Based%20TTS%20Models%20via%20Distribution%0A%20%20Alignment%20Using%20GFlowNets&body=Title%3A%20Mitigating%20Hallucinations%20in%20LM-Based%20TTS%20Models%20via%20Distribution%0A%20%20Alignment%20Using%20GFlowNets%0AAuthor%3A%20Chenlin%20Liu%20and%20Minghui%20Fang%20and%20Patrick%20Zhang%20and%20Wei%20Zhou%20and%20Jie%20Gao%20and%20Jiqing%20Han%0AAbstract%3A%20%20%20Language%20Model%20%28LM%29-based%20Text-to-Speech%20%28TTS%29%20systems%20often%20generate%0Ahallucinated%20speech%20that%20deviates%20from%20input%20text.%20Existing%20mitigation%0Astrategies%20either%20demand%20excessive%20training%20resources%20or%20introduce%20significant%0Ainference%20latency.%20In%20this%20paper%2C%20we%20propose%20GFlOwNet-guided%20distribution%0AAlignmenT%20%28GOAT%29%20for%20LM-based%20TTS%2C%20a%20post-training%20framework%20that%20mitigates%0Ahallucinations%20without%20relying%20on%20massive%20resources%20or%20inference%20cost.%0ASpecifically%2C%20we%20first%20conduct%20an%20uncertainty%20analysis%2C%20revealing%20a%20strong%0Apositive%20correlation%20between%20hallucination%20and%20model%20uncertainty.%20Based%20on%0Athis%2C%20we%20reformulate%20TTS%20generation%20as%20a%20trajectory%20flow%20optimization%20problem%0Aand%20introduce%20an%20enhanced%20Subtrajectory%20Balance%20objective%20together%20with%20a%0Asharpened%20internal%20reward%20as%20target%20distribution.%20We%20further%20integrate%20reward%0Atemperature%20decay%20and%20learning%20rate%20optimization%20for%20stability%20and%20performance%0Abalance.%20Extensive%20experiments%20show%20that%20GOAT%20reduce%20over%2050%25%20character%20error%0Arates%20on%20challenging%20test%20cases%20and%20lowering%20uncertainty%20by%20up%20to%2058%25%2C%0Ademonstrating%20its%20strong%20generalization%20ability%20and%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Hallucinations%2520in%2520LM-Based%2520TTS%2520Models%2520via%2520Distribution%250A%2520%2520Alignment%2520Using%2520GFlowNets%26entry.906535625%3DChenlin%2520Liu%2520and%2520Minghui%2520Fang%2520and%2520Patrick%2520Zhang%2520and%2520Wei%2520Zhou%2520and%2520Jie%2520Gao%2520and%2520Jiqing%2520Han%26entry.1292438233%3D%2520%2520Language%2520Model%2520%2528LM%2529-based%2520Text-to-Speech%2520%2528TTS%2529%2520systems%2520often%2520generate%250Ahallucinated%2520speech%2520that%2520deviates%2520from%2520input%2520text.%2520Existing%2520mitigation%250Astrategies%2520either%2520demand%2520excessive%2520training%2520resources%2520or%2520introduce%2520significant%250Ainference%2520latency.%2520In%2520this%2520paper%252C%2520we%2520propose%2520GFlOwNet-guided%2520distribution%250AAlignmenT%2520%2528GOAT%2529%2520for%2520LM-based%2520TTS%252C%2520a%2520post-training%2520framework%2520that%2520mitigates%250Ahallucinations%2520without%2520relying%2520on%2520massive%2520resources%2520or%2520inference%2520cost.%250ASpecifically%252C%2520we%2520first%2520conduct%2520an%2520uncertainty%2520analysis%252C%2520revealing%2520a%2520strong%250Apositive%2520correlation%2520between%2520hallucination%2520and%2520model%2520uncertainty.%2520Based%2520on%250Athis%252C%2520we%2520reformulate%2520TTS%2520generation%2520as%2520a%2520trajectory%2520flow%2520optimization%2520problem%250Aand%2520introduce%2520an%2520enhanced%2520Subtrajectory%2520Balance%2520objective%2520together%2520with%2520a%250Asharpened%2520internal%2520reward%2520as%2520target%2520distribution.%2520We%2520further%2520integrate%2520reward%250Atemperature%2520decay%2520and%2520learning%2520rate%2520optimization%2520for%2520stability%2520and%2520performance%250Abalance.%2520Extensive%2520experiments%2520show%2520that%2520GOAT%2520reduce%2520over%252050%2525%2520character%2520error%250Arates%2520on%2520challenging%2520test%2520cases%2520and%2520lowering%2520uncertainty%2520by%2520up%2520to%252058%2525%252C%250Ademonstrating%2520its%2520strong%2520generalization%2520ability%2520and%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Hallucinations%20in%20LM-Based%20TTS%20Models%20via%20Distribution%0A%20%20Alignment%20Using%20GFlowNets&entry.906535625=Chenlin%20Liu%20and%20Minghui%20Fang%20and%20Patrick%20Zhang%20and%20Wei%20Zhou%20and%20Jie%20Gao%20and%20Jiqing%20Han&entry.1292438233=%20%20Language%20Model%20%28LM%29-based%20Text-to-Speech%20%28TTS%29%20systems%20often%20generate%0Ahallucinated%20speech%20that%20deviates%20from%20input%20text.%20Existing%20mitigation%0Astrategies%20either%20demand%20excessive%20training%20resources%20or%20introduce%20significant%0Ainference%20latency.%20In%20this%20paper%2C%20we%20propose%20GFlOwNet-guided%20distribution%0AAlignmenT%20%28GOAT%29%20for%20LM-based%20TTS%2C%20a%20post-training%20framework%20that%20mitigates%0Ahallucinations%20without%20relying%20on%20massive%20resources%20or%20inference%20cost.%0ASpecifically%2C%20we%20first%20conduct%20an%20uncertainty%20analysis%2C%20revealing%20a%20strong%0Apositive%20correlation%20between%20hallucination%20and%20model%20uncertainty.%20Based%20on%0Athis%2C%20we%20reformulate%20TTS%20generation%20as%20a%20trajectory%20flow%20optimization%20problem%0Aand%20introduce%20an%20enhanced%20Subtrajectory%20Balance%20objective%20together%20with%20a%0Asharpened%20internal%20reward%20as%20target%20distribution.%20We%20further%20integrate%20reward%0Atemperature%20decay%20and%20learning%20rate%20optimization%20for%20stability%20and%20performance%0Abalance.%20Extensive%20experiments%20show%20that%20GOAT%20reduce%20over%2050%25%20character%20error%0Arates%20on%20challenging%20test%20cases%20and%20lowering%20uncertainty%20by%20up%20to%2058%25%2C%0Ademonstrating%20its%20strong%20generalization%20ability%20and%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15442v1&entry.124074799=Read"},
{"title": "Discovering Hidden Algebraic Structures via Transformers with Rank-Aware\n  Beam GRPO", "author": "Jaeha Lee and Gio Huh and Ning Su and Tony Yue YU", "abstract": "  Recent efforts have extended the capabilities of transformers in logical\nreasoning and symbolic computations. In this work, we investigate their\ncapacity for non-linear latent pattern discovery in the context of functional\ndecomposition, focusing on the challenging algebraic task of multivariate\npolynomial decomposition. This problem, with widespread applications in science\nand engineering, is proved to be NP-hard, and demands both precision and\ninsight. Our contributions are threefold: First, we develop a synthetic data\ngeneration pipeline providing fine-grained control over problem complexity.\nSecond, we train transformer models via supervised learning and evaluate them\nacross four key dimensions involving scaling behavior and generalizability.\nThird, we propose Beam Grouped Relative Policy Optimization (BGRPO), a\nrank-aware reinforcement learning method suitable for hard algebraic problems.\nFinetuning with BGRPO improves accuracy while reducing beam width by up to\nhalf, resulting in approximately 75% lower inference compute. Additionally, our\nmodel demonstrates competitive performance in polynomial simplification,\noutperforming Mathematica in various cases.\n", "link": "http://arxiv.org/abs/2508.15766v1", "date": "2025-08-21", "relevancy": 2.0177, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5116}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5094}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20Hidden%20Algebraic%20Structures%20via%20Transformers%20with%20Rank-Aware%0A%20%20Beam%20GRPO&body=Title%3A%20Discovering%20Hidden%20Algebraic%20Structures%20via%20Transformers%20with%20Rank-Aware%0A%20%20Beam%20GRPO%0AAuthor%3A%20Jaeha%20Lee%20and%20Gio%20Huh%20and%20Ning%20Su%20and%20Tony%20Yue%20YU%0AAbstract%3A%20%20%20Recent%20efforts%20have%20extended%20the%20capabilities%20of%20transformers%20in%20logical%0Areasoning%20and%20symbolic%20computations.%20In%20this%20work%2C%20we%20investigate%20their%0Acapacity%20for%20non-linear%20latent%20pattern%20discovery%20in%20the%20context%20of%20functional%0Adecomposition%2C%20focusing%20on%20the%20challenging%20algebraic%20task%20of%20multivariate%0Apolynomial%20decomposition.%20This%20problem%2C%20with%20widespread%20applications%20in%20science%0Aand%20engineering%2C%20is%20proved%20to%20be%20NP-hard%2C%20and%20demands%20both%20precision%20and%0Ainsight.%20Our%20contributions%20are%20threefold%3A%20First%2C%20we%20develop%20a%20synthetic%20data%0Ageneration%20pipeline%20providing%20fine-grained%20control%20over%20problem%20complexity.%0ASecond%2C%20we%20train%20transformer%20models%20via%20supervised%20learning%20and%20evaluate%20them%0Aacross%20four%20key%20dimensions%20involving%20scaling%20behavior%20and%20generalizability.%0AThird%2C%20we%20propose%20Beam%20Grouped%20Relative%20Policy%20Optimization%20%28BGRPO%29%2C%20a%0Arank-aware%20reinforcement%20learning%20method%20suitable%20for%20hard%20algebraic%20problems.%0AFinetuning%20with%20BGRPO%20improves%20accuracy%20while%20reducing%20beam%20width%20by%20up%20to%0Ahalf%2C%20resulting%20in%20approximately%2075%25%20lower%20inference%20compute.%20Additionally%2C%20our%0Amodel%20demonstrates%20competitive%20performance%20in%20polynomial%20simplification%2C%0Aoutperforming%20Mathematica%20in%20various%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520Hidden%2520Algebraic%2520Structures%2520via%2520Transformers%2520with%2520Rank-Aware%250A%2520%2520Beam%2520GRPO%26entry.906535625%3DJaeha%2520Lee%2520and%2520Gio%2520Huh%2520and%2520Ning%2520Su%2520and%2520Tony%2520Yue%2520YU%26entry.1292438233%3D%2520%2520Recent%2520efforts%2520have%2520extended%2520the%2520capabilities%2520of%2520transformers%2520in%2520logical%250Areasoning%2520and%2520symbolic%2520computations.%2520In%2520this%2520work%252C%2520we%2520investigate%2520their%250Acapacity%2520for%2520non-linear%2520latent%2520pattern%2520discovery%2520in%2520the%2520context%2520of%2520functional%250Adecomposition%252C%2520focusing%2520on%2520the%2520challenging%2520algebraic%2520task%2520of%2520multivariate%250Apolynomial%2520decomposition.%2520This%2520problem%252C%2520with%2520widespread%2520applications%2520in%2520science%250Aand%2520engineering%252C%2520is%2520proved%2520to%2520be%2520NP-hard%252C%2520and%2520demands%2520both%2520precision%2520and%250Ainsight.%2520Our%2520contributions%2520are%2520threefold%253A%2520First%252C%2520we%2520develop%2520a%2520synthetic%2520data%250Ageneration%2520pipeline%2520providing%2520fine-grained%2520control%2520over%2520problem%2520complexity.%250ASecond%252C%2520we%2520train%2520transformer%2520models%2520via%2520supervised%2520learning%2520and%2520evaluate%2520them%250Aacross%2520four%2520key%2520dimensions%2520involving%2520scaling%2520behavior%2520and%2520generalizability.%250AThird%252C%2520we%2520propose%2520Beam%2520Grouped%2520Relative%2520Policy%2520Optimization%2520%2528BGRPO%2529%252C%2520a%250Arank-aware%2520reinforcement%2520learning%2520method%2520suitable%2520for%2520hard%2520algebraic%2520problems.%250AFinetuning%2520with%2520BGRPO%2520improves%2520accuracy%2520while%2520reducing%2520beam%2520width%2520by%2520up%2520to%250Ahalf%252C%2520resulting%2520in%2520approximately%252075%2525%2520lower%2520inference%2520compute.%2520Additionally%252C%2520our%250Amodel%2520demonstrates%2520competitive%2520performance%2520in%2520polynomial%2520simplification%252C%250Aoutperforming%2520Mathematica%2520in%2520various%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20Hidden%20Algebraic%20Structures%20via%20Transformers%20with%20Rank-Aware%0A%20%20Beam%20GRPO&entry.906535625=Jaeha%20Lee%20and%20Gio%20Huh%20and%20Ning%20Su%20and%20Tony%20Yue%20YU&entry.1292438233=%20%20Recent%20efforts%20have%20extended%20the%20capabilities%20of%20transformers%20in%20logical%0Areasoning%20and%20symbolic%20computations.%20In%20this%20work%2C%20we%20investigate%20their%0Acapacity%20for%20non-linear%20latent%20pattern%20discovery%20in%20the%20context%20of%20functional%0Adecomposition%2C%20focusing%20on%20the%20challenging%20algebraic%20task%20of%20multivariate%0Apolynomial%20decomposition.%20This%20problem%2C%20with%20widespread%20applications%20in%20science%0Aand%20engineering%2C%20is%20proved%20to%20be%20NP-hard%2C%20and%20demands%20both%20precision%20and%0Ainsight.%20Our%20contributions%20are%20threefold%3A%20First%2C%20we%20develop%20a%20synthetic%20data%0Ageneration%20pipeline%20providing%20fine-grained%20control%20over%20problem%20complexity.%0ASecond%2C%20we%20train%20transformer%20models%20via%20supervised%20learning%20and%20evaluate%20them%0Aacross%20four%20key%20dimensions%20involving%20scaling%20behavior%20and%20generalizability.%0AThird%2C%20we%20propose%20Beam%20Grouped%20Relative%20Policy%20Optimization%20%28BGRPO%29%2C%20a%0Arank-aware%20reinforcement%20learning%20method%20suitable%20for%20hard%20algebraic%20problems.%0AFinetuning%20with%20BGRPO%20improves%20accuracy%20while%20reducing%20beam%20width%20by%20up%20to%0Ahalf%2C%20resulting%20in%20approximately%2075%25%20lower%20inference%20compute.%20Additionally%2C%20our%0Amodel%20demonstrates%20competitive%20performance%20in%20polynomial%20simplification%2C%0Aoutperforming%20Mathematica%20in%20various%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15766v1&entry.124074799=Read"},
{"title": "Multi-perspective monitoring of wildlife and human activities from\n  camera traps and drones with deep learning models", "author": "Hao Chen and Fang Qiu and Li An and Douglas Stow and Eve Bohnett and Haitao Lyu and Shuang Tian", "abstract": "  Wildlife and human activities are key components of landscape systems.\nUnderstanding their spatial distribution is essential for evaluating human\nwildlife interactions and informing effective conservation planning.\nMultiperspective monitoring of wildlife and human activities by combining\ncamera traps and drone imagery. Capturing the spatial patterns of their\ndistributions, which allows the identification of the overlap of their activity\nzones and the assessment of the degree of human wildlife conflict. The study\nwas conducted in Chitwan National Park (CNP), Nepal, and adjacent regions.\nImages collected by visible and nearinfrared camera traps and thermal infrared\ndrones from February to July 2022 were processed to create training and testing\ndatasets, which were used to build deep learning models to automatic identify\nwildlife and human activities. Drone collected thermal imagery was used for\ndetecting targets to provide a multiple monitoring perspective. Spatial pattern\nanalysis was performed to identify animal and resident activity hotspots and\ndelineation potential human wildlife conflict zones. Among the deep learning\nmodels tested, YOLOv11s achieved the highest performance with a precision of\n96.2%, recall of 92.3%, mAP50 of 96.7%, and mAP50 of 81.3%, making it the most\neffective for detecting objects in camera trap imagery. Drone based thermal\nimagery, analyzed with an enhanced Faster RCNN model, added a complementary\naerial viewpoint for camera trap detections. Spatial pattern analysis\nidentified clear hotspots for both wildlife and human activities and their\noverlapping patterns within certain areas in the CNP and buffer zones\nindicating potential conflict. This study reveals human wildlife conflicts\nwithin the conserved landscape. Integrating multiperspective monitoring with\nautomated object detection enhances wildlife surveillance and landscape\nmanagement.\n", "link": "http://arxiv.org/abs/2508.15629v1", "date": "2025-08-21", "relevancy": 2.0161, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5287}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5077}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-perspective%20monitoring%20of%20wildlife%20and%20human%20activities%20from%0A%20%20camera%20traps%20and%20drones%20with%20deep%20learning%20models&body=Title%3A%20Multi-perspective%20monitoring%20of%20wildlife%20and%20human%20activities%20from%0A%20%20camera%20traps%20and%20drones%20with%20deep%20learning%20models%0AAuthor%3A%20Hao%20Chen%20and%20Fang%20Qiu%20and%20Li%20An%20and%20Douglas%20Stow%20and%20Eve%20Bohnett%20and%20Haitao%20Lyu%20and%20Shuang%20Tian%0AAbstract%3A%20%20%20Wildlife%20and%20human%20activities%20are%20key%20components%20of%20landscape%20systems.%0AUnderstanding%20their%20spatial%20distribution%20is%20essential%20for%20evaluating%20human%0Awildlife%20interactions%20and%20informing%20effective%20conservation%20planning.%0AMultiperspective%20monitoring%20of%20wildlife%20and%20human%20activities%20by%20combining%0Acamera%20traps%20and%20drone%20imagery.%20Capturing%20the%20spatial%20patterns%20of%20their%0Adistributions%2C%20which%20allows%20the%20identification%20of%20the%20overlap%20of%20their%20activity%0Azones%20and%20the%20assessment%20of%20the%20degree%20of%20human%20wildlife%20conflict.%20The%20study%0Awas%20conducted%20in%20Chitwan%20National%20Park%20%28CNP%29%2C%20Nepal%2C%20and%20adjacent%20regions.%0AImages%20collected%20by%20visible%20and%20nearinfrared%20camera%20traps%20and%20thermal%20infrared%0Adrones%20from%20February%20to%20July%202022%20were%20processed%20to%20create%20training%20and%20testing%0Adatasets%2C%20which%20were%20used%20to%20build%20deep%20learning%20models%20to%20automatic%20identify%0Awildlife%20and%20human%20activities.%20Drone%20collected%20thermal%20imagery%20was%20used%20for%0Adetecting%20targets%20to%20provide%20a%20multiple%20monitoring%20perspective.%20Spatial%20pattern%0Aanalysis%20was%20performed%20to%20identify%20animal%20and%20resident%20activity%20hotspots%20and%0Adelineation%20potential%20human%20wildlife%20conflict%20zones.%20Among%20the%20deep%20learning%0Amodels%20tested%2C%20YOLOv11s%20achieved%20the%20highest%20performance%20with%20a%20precision%20of%0A96.2%25%2C%20recall%20of%2092.3%25%2C%20mAP50%20of%2096.7%25%2C%20and%20mAP50%20of%2081.3%25%2C%20making%20it%20the%20most%0Aeffective%20for%20detecting%20objects%20in%20camera%20trap%20imagery.%20Drone%20based%20thermal%0Aimagery%2C%20analyzed%20with%20an%20enhanced%20Faster%20RCNN%20model%2C%20added%20a%20complementary%0Aaerial%20viewpoint%20for%20camera%20trap%20detections.%20Spatial%20pattern%20analysis%0Aidentified%20clear%20hotspots%20for%20both%20wildlife%20and%20human%20activities%20and%20their%0Aoverlapping%20patterns%20within%20certain%20areas%20in%20the%20CNP%20and%20buffer%20zones%0Aindicating%20potential%20conflict.%20This%20study%20reveals%20human%20wildlife%20conflicts%0Awithin%20the%20conserved%20landscape.%20Integrating%20multiperspective%20monitoring%20with%0Aautomated%20object%20detection%20enhances%20wildlife%20surveillance%20and%20landscape%0Amanagement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-perspective%2520monitoring%2520of%2520wildlife%2520and%2520human%2520activities%2520from%250A%2520%2520camera%2520traps%2520and%2520drones%2520with%2520deep%2520learning%2520models%26entry.906535625%3DHao%2520Chen%2520and%2520Fang%2520Qiu%2520and%2520Li%2520An%2520and%2520Douglas%2520Stow%2520and%2520Eve%2520Bohnett%2520and%2520Haitao%2520Lyu%2520and%2520Shuang%2520Tian%26entry.1292438233%3D%2520%2520Wildlife%2520and%2520human%2520activities%2520are%2520key%2520components%2520of%2520landscape%2520systems.%250AUnderstanding%2520their%2520spatial%2520distribution%2520is%2520essential%2520for%2520evaluating%2520human%250Awildlife%2520interactions%2520and%2520informing%2520effective%2520conservation%2520planning.%250AMultiperspective%2520monitoring%2520of%2520wildlife%2520and%2520human%2520activities%2520by%2520combining%250Acamera%2520traps%2520and%2520drone%2520imagery.%2520Capturing%2520the%2520spatial%2520patterns%2520of%2520their%250Adistributions%252C%2520which%2520allows%2520the%2520identification%2520of%2520the%2520overlap%2520of%2520their%2520activity%250Azones%2520and%2520the%2520assessment%2520of%2520the%2520degree%2520of%2520human%2520wildlife%2520conflict.%2520The%2520study%250Awas%2520conducted%2520in%2520Chitwan%2520National%2520Park%2520%2528CNP%2529%252C%2520Nepal%252C%2520and%2520adjacent%2520regions.%250AImages%2520collected%2520by%2520visible%2520and%2520nearinfrared%2520camera%2520traps%2520and%2520thermal%2520infrared%250Adrones%2520from%2520February%2520to%2520July%25202022%2520were%2520processed%2520to%2520create%2520training%2520and%2520testing%250Adatasets%252C%2520which%2520were%2520used%2520to%2520build%2520deep%2520learning%2520models%2520to%2520automatic%2520identify%250Awildlife%2520and%2520human%2520activities.%2520Drone%2520collected%2520thermal%2520imagery%2520was%2520used%2520for%250Adetecting%2520targets%2520to%2520provide%2520a%2520multiple%2520monitoring%2520perspective.%2520Spatial%2520pattern%250Aanalysis%2520was%2520performed%2520to%2520identify%2520animal%2520and%2520resident%2520activity%2520hotspots%2520and%250Adelineation%2520potential%2520human%2520wildlife%2520conflict%2520zones.%2520Among%2520the%2520deep%2520learning%250Amodels%2520tested%252C%2520YOLOv11s%2520achieved%2520the%2520highest%2520performance%2520with%2520a%2520precision%2520of%250A96.2%2525%252C%2520recall%2520of%252092.3%2525%252C%2520mAP50%2520of%252096.7%2525%252C%2520and%2520mAP50%2520of%252081.3%2525%252C%2520making%2520it%2520the%2520most%250Aeffective%2520for%2520detecting%2520objects%2520in%2520camera%2520trap%2520imagery.%2520Drone%2520based%2520thermal%250Aimagery%252C%2520analyzed%2520with%2520an%2520enhanced%2520Faster%2520RCNN%2520model%252C%2520added%2520a%2520complementary%250Aaerial%2520viewpoint%2520for%2520camera%2520trap%2520detections.%2520Spatial%2520pattern%2520analysis%250Aidentified%2520clear%2520hotspots%2520for%2520both%2520wildlife%2520and%2520human%2520activities%2520and%2520their%250Aoverlapping%2520patterns%2520within%2520certain%2520areas%2520in%2520the%2520CNP%2520and%2520buffer%2520zones%250Aindicating%2520potential%2520conflict.%2520This%2520study%2520reveals%2520human%2520wildlife%2520conflicts%250Awithin%2520the%2520conserved%2520landscape.%2520Integrating%2520multiperspective%2520monitoring%2520with%250Aautomated%2520object%2520detection%2520enhances%2520wildlife%2520surveillance%2520and%2520landscape%250Amanagement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-perspective%20monitoring%20of%20wildlife%20and%20human%20activities%20from%0A%20%20camera%20traps%20and%20drones%20with%20deep%20learning%20models&entry.906535625=Hao%20Chen%20and%20Fang%20Qiu%20and%20Li%20An%20and%20Douglas%20Stow%20and%20Eve%20Bohnett%20and%20Haitao%20Lyu%20and%20Shuang%20Tian&entry.1292438233=%20%20Wildlife%20and%20human%20activities%20are%20key%20components%20of%20landscape%20systems.%0AUnderstanding%20their%20spatial%20distribution%20is%20essential%20for%20evaluating%20human%0Awildlife%20interactions%20and%20informing%20effective%20conservation%20planning.%0AMultiperspective%20monitoring%20of%20wildlife%20and%20human%20activities%20by%20combining%0Acamera%20traps%20and%20drone%20imagery.%20Capturing%20the%20spatial%20patterns%20of%20their%0Adistributions%2C%20which%20allows%20the%20identification%20of%20the%20overlap%20of%20their%20activity%0Azones%20and%20the%20assessment%20of%20the%20degree%20of%20human%20wildlife%20conflict.%20The%20study%0Awas%20conducted%20in%20Chitwan%20National%20Park%20%28CNP%29%2C%20Nepal%2C%20and%20adjacent%20regions.%0AImages%20collected%20by%20visible%20and%20nearinfrared%20camera%20traps%20and%20thermal%20infrared%0Adrones%20from%20February%20to%20July%202022%20were%20processed%20to%20create%20training%20and%20testing%0Adatasets%2C%20which%20were%20used%20to%20build%20deep%20learning%20models%20to%20automatic%20identify%0Awildlife%20and%20human%20activities.%20Drone%20collected%20thermal%20imagery%20was%20used%20for%0Adetecting%20targets%20to%20provide%20a%20multiple%20monitoring%20perspective.%20Spatial%20pattern%0Aanalysis%20was%20performed%20to%20identify%20animal%20and%20resident%20activity%20hotspots%20and%0Adelineation%20potential%20human%20wildlife%20conflict%20zones.%20Among%20the%20deep%20learning%0Amodels%20tested%2C%20YOLOv11s%20achieved%20the%20highest%20performance%20with%20a%20precision%20of%0A96.2%25%2C%20recall%20of%2092.3%25%2C%20mAP50%20of%2096.7%25%2C%20and%20mAP50%20of%2081.3%25%2C%20making%20it%20the%20most%0Aeffective%20for%20detecting%20objects%20in%20camera%20trap%20imagery.%20Drone%20based%20thermal%0Aimagery%2C%20analyzed%20with%20an%20enhanced%20Faster%20RCNN%20model%2C%20added%20a%20complementary%0Aaerial%20viewpoint%20for%20camera%20trap%20detections.%20Spatial%20pattern%20analysis%0Aidentified%20clear%20hotspots%20for%20both%20wildlife%20and%20human%20activities%20and%20their%0Aoverlapping%20patterns%20within%20certain%20areas%20in%20the%20CNP%20and%20buffer%20zones%0Aindicating%20potential%20conflict.%20This%20study%20reveals%20human%20wildlife%20conflicts%0Awithin%20the%20conserved%20landscape.%20Integrating%20multiperspective%20monitoring%20with%0Aautomated%20object%20detection%20enhances%20wildlife%20surveillance%20and%20landscape%0Amanagement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15629v1&entry.124074799=Read"},
{"title": "OPDR: Order-Preserving Dimension Reduction for Semantic Embedding of\n  Multimodal Scientific Data", "author": "Chengyu Gong and Gefei Shen and Luanzheng Guo and Nathan Tallent and Dongfang Zhao", "abstract": "  One of the most common operations in multimodal scientific data management is\nsearching for the $k$ most similar items (or, $k$-nearest neighbors, KNN) from\nthe database after being provided a new item. Although recent advances of\nmultimodal machine learning models offer a \\textit{semantic} index, the\nso-called \\textit{embedding vectors} mapped from the original multimodal data,\nthe dimension of the resulting embedding vectors are usually on the order of\nhundreds or a thousand, which are impractically high for time-sensitive\nscientific applications.\n  This work proposes to reduce the dimensionality of the output embedding\nvectors such that the set of top-$k$ nearest neighbors do not change in the\nlower-dimensional space, namely Order-Preserving Dimension Reduction (OPDR). In\norder to develop such an OPDR method, our central hypothesis is that by\nanalyzing the intrinsic relationship among key parameters during the\ndimension-reduction map, a quantitative function may be constructed to reveal\nthe correlation between the target (lower) dimensionality and other variables.\nTo demonstrate the hypothesis, this paper first defines a formal measure\nfunction to quantify the KNN similarity for a specific vector, then extends the\nmeasure into an aggregate accuracy of the global metric spaces, and finally\nderives a closed-form function between the target (lower) dimensionality and\nother variables. We incorporate the closed-function into popular\ndimension-reduction methods, various distance metrics, and embedding models.\n", "link": "http://arxiv.org/abs/2408.10264v2", "date": "2025-08-21", "relevancy": 2.0079, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5337}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OPDR%3A%20Order-Preserving%20Dimension%20Reduction%20for%20Semantic%20Embedding%20of%0A%20%20Multimodal%20Scientific%20Data&body=Title%3A%20OPDR%3A%20Order-Preserving%20Dimension%20Reduction%20for%20Semantic%20Embedding%20of%0A%20%20Multimodal%20Scientific%20Data%0AAuthor%3A%20Chengyu%20Gong%20and%20Gefei%20Shen%20and%20Luanzheng%20Guo%20and%20Nathan%20Tallent%20and%20Dongfang%20Zhao%0AAbstract%3A%20%20%20One%20of%20the%20most%20common%20operations%20in%20multimodal%20scientific%20data%20management%20is%0Asearching%20for%20the%20%24k%24%20most%20similar%20items%20%28or%2C%20%24k%24-nearest%20neighbors%2C%20KNN%29%20from%0Athe%20database%20after%20being%20provided%20a%20new%20item.%20Although%20recent%20advances%20of%0Amultimodal%20machine%20learning%20models%20offer%20a%20%5Ctextit%7Bsemantic%7D%20index%2C%20the%0Aso-called%20%5Ctextit%7Bembedding%20vectors%7D%20mapped%20from%20the%20original%20multimodal%20data%2C%0Athe%20dimension%20of%20the%20resulting%20embedding%20vectors%20are%20usually%20on%20the%20order%20of%0Ahundreds%20or%20a%20thousand%2C%20which%20are%20impractically%20high%20for%20time-sensitive%0Ascientific%20applications.%0A%20%20This%20work%20proposes%20to%20reduce%20the%20dimensionality%20of%20the%20output%20embedding%0Avectors%20such%20that%20the%20set%20of%20top-%24k%24%20nearest%20neighbors%20do%20not%20change%20in%20the%0Alower-dimensional%20space%2C%20namely%20Order-Preserving%20Dimension%20Reduction%20%28OPDR%29.%20In%0Aorder%20to%20develop%20such%20an%20OPDR%20method%2C%20our%20central%20hypothesis%20is%20that%20by%0Aanalyzing%20the%20intrinsic%20relationship%20among%20key%20parameters%20during%20the%0Adimension-reduction%20map%2C%20a%20quantitative%20function%20may%20be%20constructed%20to%20reveal%0Athe%20correlation%20between%20the%20target%20%28lower%29%20dimensionality%20and%20other%20variables.%0ATo%20demonstrate%20the%20hypothesis%2C%20this%20paper%20first%20defines%20a%20formal%20measure%0Afunction%20to%20quantify%20the%20KNN%20similarity%20for%20a%20specific%20vector%2C%20then%20extends%20the%0Ameasure%20into%20an%20aggregate%20accuracy%20of%20the%20global%20metric%20spaces%2C%20and%20finally%0Aderives%20a%20closed-form%20function%20between%20the%20target%20%28lower%29%20dimensionality%20and%0Aother%20variables.%20We%20incorporate%20the%20closed-function%20into%20popular%0Adimension-reduction%20methods%2C%20various%20distance%20metrics%2C%20and%20embedding%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10264v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOPDR%253A%2520Order-Preserving%2520Dimension%2520Reduction%2520for%2520Semantic%2520Embedding%2520of%250A%2520%2520Multimodal%2520Scientific%2520Data%26entry.906535625%3DChengyu%2520Gong%2520and%2520Gefei%2520Shen%2520and%2520Luanzheng%2520Guo%2520and%2520Nathan%2520Tallent%2520and%2520Dongfang%2520Zhao%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520most%2520common%2520operations%2520in%2520multimodal%2520scientific%2520data%2520management%2520is%250Asearching%2520for%2520the%2520%2524k%2524%2520most%2520similar%2520items%2520%2528or%252C%2520%2524k%2524-nearest%2520neighbors%252C%2520KNN%2529%2520from%250Athe%2520database%2520after%2520being%2520provided%2520a%2520new%2520item.%2520Although%2520recent%2520advances%2520of%250Amultimodal%2520machine%2520learning%2520models%2520offer%2520a%2520%255Ctextit%257Bsemantic%257D%2520index%252C%2520the%250Aso-called%2520%255Ctextit%257Bembedding%2520vectors%257D%2520mapped%2520from%2520the%2520original%2520multimodal%2520data%252C%250Athe%2520dimension%2520of%2520the%2520resulting%2520embedding%2520vectors%2520are%2520usually%2520on%2520the%2520order%2520of%250Ahundreds%2520or%2520a%2520thousand%252C%2520which%2520are%2520impractically%2520high%2520for%2520time-sensitive%250Ascientific%2520applications.%250A%2520%2520This%2520work%2520proposes%2520to%2520reduce%2520the%2520dimensionality%2520of%2520the%2520output%2520embedding%250Avectors%2520such%2520that%2520the%2520set%2520of%2520top-%2524k%2524%2520nearest%2520neighbors%2520do%2520not%2520change%2520in%2520the%250Alower-dimensional%2520space%252C%2520namely%2520Order-Preserving%2520Dimension%2520Reduction%2520%2528OPDR%2529.%2520In%250Aorder%2520to%2520develop%2520such%2520an%2520OPDR%2520method%252C%2520our%2520central%2520hypothesis%2520is%2520that%2520by%250Aanalyzing%2520the%2520intrinsic%2520relationship%2520among%2520key%2520parameters%2520during%2520the%250Adimension-reduction%2520map%252C%2520a%2520quantitative%2520function%2520may%2520be%2520constructed%2520to%2520reveal%250Athe%2520correlation%2520between%2520the%2520target%2520%2528lower%2529%2520dimensionality%2520and%2520other%2520variables.%250ATo%2520demonstrate%2520the%2520hypothesis%252C%2520this%2520paper%2520first%2520defines%2520a%2520formal%2520measure%250Afunction%2520to%2520quantify%2520the%2520KNN%2520similarity%2520for%2520a%2520specific%2520vector%252C%2520then%2520extends%2520the%250Ameasure%2520into%2520an%2520aggregate%2520accuracy%2520of%2520the%2520global%2520metric%2520spaces%252C%2520and%2520finally%250Aderives%2520a%2520closed-form%2520function%2520between%2520the%2520target%2520%2528lower%2529%2520dimensionality%2520and%250Aother%2520variables.%2520We%2520incorporate%2520the%2520closed-function%2520into%2520popular%250Adimension-reduction%2520methods%252C%2520various%2520distance%2520metrics%252C%2520and%2520embedding%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10264v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OPDR%3A%20Order-Preserving%20Dimension%20Reduction%20for%20Semantic%20Embedding%20of%0A%20%20Multimodal%20Scientific%20Data&entry.906535625=Chengyu%20Gong%20and%20Gefei%20Shen%20and%20Luanzheng%20Guo%20and%20Nathan%20Tallent%20and%20Dongfang%20Zhao&entry.1292438233=%20%20One%20of%20the%20most%20common%20operations%20in%20multimodal%20scientific%20data%20management%20is%0Asearching%20for%20the%20%24k%24%20most%20similar%20items%20%28or%2C%20%24k%24-nearest%20neighbors%2C%20KNN%29%20from%0Athe%20database%20after%20being%20provided%20a%20new%20item.%20Although%20recent%20advances%20of%0Amultimodal%20machine%20learning%20models%20offer%20a%20%5Ctextit%7Bsemantic%7D%20index%2C%20the%0Aso-called%20%5Ctextit%7Bembedding%20vectors%7D%20mapped%20from%20the%20original%20multimodal%20data%2C%0Athe%20dimension%20of%20the%20resulting%20embedding%20vectors%20are%20usually%20on%20the%20order%20of%0Ahundreds%20or%20a%20thousand%2C%20which%20are%20impractically%20high%20for%20time-sensitive%0Ascientific%20applications.%0A%20%20This%20work%20proposes%20to%20reduce%20the%20dimensionality%20of%20the%20output%20embedding%0Avectors%20such%20that%20the%20set%20of%20top-%24k%24%20nearest%20neighbors%20do%20not%20change%20in%20the%0Alower-dimensional%20space%2C%20namely%20Order-Preserving%20Dimension%20Reduction%20%28OPDR%29.%20In%0Aorder%20to%20develop%20such%20an%20OPDR%20method%2C%20our%20central%20hypothesis%20is%20that%20by%0Aanalyzing%20the%20intrinsic%20relationship%20among%20key%20parameters%20during%20the%0Adimension-reduction%20map%2C%20a%20quantitative%20function%20may%20be%20constructed%20to%20reveal%0Athe%20correlation%20between%20the%20target%20%28lower%29%20dimensionality%20and%20other%20variables.%0ATo%20demonstrate%20the%20hypothesis%2C%20this%20paper%20first%20defines%20a%20formal%20measure%0Afunction%20to%20quantify%20the%20KNN%20similarity%20for%20a%20specific%20vector%2C%20then%20extends%20the%0Ameasure%20into%20an%20aggregate%20accuracy%20of%20the%20global%20metric%20spaces%2C%20and%20finally%0Aderives%20a%20closed-form%20function%20between%20the%20target%20%28lower%29%20dimensionality%20and%0Aother%20variables.%20We%20incorporate%20the%20closed-function%20into%20popular%0Adimension-reduction%20methods%2C%20various%20distance%20metrics%2C%20and%20embedding%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10264v2&entry.124074799=Read"},
{"title": "Learning to Generate Unit Tests for Automated Debugging", "author": "Archiki Prasad and Elias Stengel-Eskin and Justin Chih-Yao Chen and Zaid Khan and Mohit Bansal", "abstract": "  Unit tests (UTs) play an instrumental role in assessing code correctness as\nwell as providing feedback to large language models (LLMs), motivating\nautomated test generation. However, we uncover a trade-off between generating\nunit test inputs that reveal errors when given a faulty code and correctly\npredicting the unit test output without access to the gold solution. To address\nthis trade-off, we propose UTGen, which teaches LLMs to generate unit test\ninputs that reveal errors along with their correct expected outputs based on\ntask descriptions. Since model-generated tests can provide noisy signals (e.g.,\nfrom incorrectly predicted outputs), we propose UTDebug that (i) scales UTGen\nvia test-time compute to improve UT output prediction, and (ii) validates and\nbacktracks edits based on multiple generated UTs to avoid overfitting, and\nhelps LLMs debug effectively. We show that UTGen outperforms other LLM-based\nbaselines by 7.59% based on a metric measuring the presence of both\nerror-revealing UT inputs and correct UT outputs. When used with UTDebug, we\nfind that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen2.5\n32B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3.17%\nand 12.35% (respectively) over other LLM-based UT generation baselines.\nMoreover, we observe that feedback from Qwen2.5 32B-based UTGen model can\nenhance debugging with frontier LLMs like GPT-4o by 13.8%. Lastly, we\ndemonstrate that UTGen is a better judge for code correctness, outperforming a\nstate-of-the-art trained 8B reward model by 4.43% on HumanEval+ with best-of-10\nsampling using Qwen2.5 7B.\n", "link": "http://arxiv.org/abs/2502.01619v3", "date": "2025-08-21", "relevancy": 1.993, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5154}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4901}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Generate%20Unit%20Tests%20for%20Automated%20Debugging&body=Title%3A%20Learning%20to%20Generate%20Unit%20Tests%20for%20Automated%20Debugging%0AAuthor%3A%20Archiki%20Prasad%20and%20Elias%20Stengel-Eskin%20and%20Justin%20Chih-Yao%20Chen%20and%20Zaid%20Khan%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Unit%20tests%20%28UTs%29%20play%20an%20instrumental%20role%20in%20assessing%20code%20correctness%20as%0Awell%20as%20providing%20feedback%20to%20large%20language%20models%20%28LLMs%29%2C%20motivating%0Aautomated%20test%20generation.%20However%2C%20we%20uncover%20a%20trade-off%20between%20generating%0Aunit%20test%20inputs%20that%20reveal%20errors%20when%20given%20a%20faulty%20code%20and%20correctly%0Apredicting%20the%20unit%20test%20output%20without%20access%20to%20the%20gold%20solution.%20To%20address%0Athis%20trade-off%2C%20we%20propose%20UTGen%2C%20which%20teaches%20LLMs%20to%20generate%20unit%20test%0Ainputs%20that%20reveal%20errors%20along%20with%20their%20correct%20expected%20outputs%20based%20on%0Atask%20descriptions.%20Since%20model-generated%20tests%20can%20provide%20noisy%20signals%20%28e.g.%2C%0Afrom%20incorrectly%20predicted%20outputs%29%2C%20we%20propose%20UTDebug%20that%20%28i%29%20scales%20UTGen%0Avia%20test-time%20compute%20to%20improve%20UT%20output%20prediction%2C%20and%20%28ii%29%20validates%20and%0Abacktracks%20edits%20based%20on%20multiple%20generated%20UTs%20to%20avoid%20overfitting%2C%20and%0Ahelps%20LLMs%20debug%20effectively.%20We%20show%20that%20UTGen%20outperforms%20other%20LLM-based%0Abaselines%20by%207.59%25%20based%20on%20a%20metric%20measuring%20the%20presence%20of%20both%0Aerror-revealing%20UT%20inputs%20and%20correct%20UT%20outputs.%20When%20used%20with%20UTDebug%2C%20we%0Afind%20that%20feedback%20from%20UTGen%27s%20unit%20tests%20improves%20pass%401%20accuracy%20of%20Qwen2.5%0A32B%20on%20HumanEvalFix%20and%20our%20own%20harder%20debugging%20split%20of%20MBPP%2B%20by%20over%203.17%25%0Aand%2012.35%25%20%28respectively%29%20over%20other%20LLM-based%20UT%20generation%20baselines.%0AMoreover%2C%20we%20observe%20that%20feedback%20from%20Qwen2.5%2032B-based%20UTGen%20model%20can%0Aenhance%20debugging%20with%20frontier%20LLMs%20like%20GPT-4o%20by%2013.8%25.%20Lastly%2C%20we%0Ademonstrate%20that%20UTGen%20is%20a%20better%20judge%20for%20code%20correctness%2C%20outperforming%20a%0Astate-of-the-art%20trained%208B%20reward%20model%20by%204.43%25%20on%20HumanEval%2B%20with%20best-of-10%0Asampling%20using%20Qwen2.5%207B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01619v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Generate%2520Unit%2520Tests%2520for%2520Automated%2520Debugging%26entry.906535625%3DArchiki%2520Prasad%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Justin%2520Chih-Yao%2520Chen%2520and%2520Zaid%2520Khan%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Unit%2520tests%2520%2528UTs%2529%2520play%2520an%2520instrumental%2520role%2520in%2520assessing%2520code%2520correctness%2520as%250Awell%2520as%2520providing%2520feedback%2520to%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520motivating%250Aautomated%2520test%2520generation.%2520However%252C%2520we%2520uncover%2520a%2520trade-off%2520between%2520generating%250Aunit%2520test%2520inputs%2520that%2520reveal%2520errors%2520when%2520given%2520a%2520faulty%2520code%2520and%2520correctly%250Apredicting%2520the%2520unit%2520test%2520output%2520without%2520access%2520to%2520the%2520gold%2520solution.%2520To%2520address%250Athis%2520trade-off%252C%2520we%2520propose%2520UTGen%252C%2520which%2520teaches%2520LLMs%2520to%2520generate%2520unit%2520test%250Ainputs%2520that%2520reveal%2520errors%2520along%2520with%2520their%2520correct%2520expected%2520outputs%2520based%2520on%250Atask%2520descriptions.%2520Since%2520model-generated%2520tests%2520can%2520provide%2520noisy%2520signals%2520%2528e.g.%252C%250Afrom%2520incorrectly%2520predicted%2520outputs%2529%252C%2520we%2520propose%2520UTDebug%2520that%2520%2528i%2529%2520scales%2520UTGen%250Avia%2520test-time%2520compute%2520to%2520improve%2520UT%2520output%2520prediction%252C%2520and%2520%2528ii%2529%2520validates%2520and%250Abacktracks%2520edits%2520based%2520on%2520multiple%2520generated%2520UTs%2520to%2520avoid%2520overfitting%252C%2520and%250Ahelps%2520LLMs%2520debug%2520effectively.%2520We%2520show%2520that%2520UTGen%2520outperforms%2520other%2520LLM-based%250Abaselines%2520by%25207.59%2525%2520based%2520on%2520a%2520metric%2520measuring%2520the%2520presence%2520of%2520both%250Aerror-revealing%2520UT%2520inputs%2520and%2520correct%2520UT%2520outputs.%2520When%2520used%2520with%2520UTDebug%252C%2520we%250Afind%2520that%2520feedback%2520from%2520UTGen%2527s%2520unit%2520tests%2520improves%2520pass%25401%2520accuracy%2520of%2520Qwen2.5%250A32B%2520on%2520HumanEvalFix%2520and%2520our%2520own%2520harder%2520debugging%2520split%2520of%2520MBPP%252B%2520by%2520over%25203.17%2525%250Aand%252012.35%2525%2520%2528respectively%2529%2520over%2520other%2520LLM-based%2520UT%2520generation%2520baselines.%250AMoreover%252C%2520we%2520observe%2520that%2520feedback%2520from%2520Qwen2.5%252032B-based%2520UTGen%2520model%2520can%250Aenhance%2520debugging%2520with%2520frontier%2520LLMs%2520like%2520GPT-4o%2520by%252013.8%2525.%2520Lastly%252C%2520we%250Ademonstrate%2520that%2520UTGen%2520is%2520a%2520better%2520judge%2520for%2520code%2520correctness%252C%2520outperforming%2520a%250Astate-of-the-art%2520trained%25208B%2520reward%2520model%2520by%25204.43%2525%2520on%2520HumanEval%252B%2520with%2520best-of-10%250Asampling%2520using%2520Qwen2.5%25207B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01619v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Generate%20Unit%20Tests%20for%20Automated%20Debugging&entry.906535625=Archiki%20Prasad%20and%20Elias%20Stengel-Eskin%20and%20Justin%20Chih-Yao%20Chen%20and%20Zaid%20Khan%20and%20Mohit%20Bansal&entry.1292438233=%20%20Unit%20tests%20%28UTs%29%20play%20an%20instrumental%20role%20in%20assessing%20code%20correctness%20as%0Awell%20as%20providing%20feedback%20to%20large%20language%20models%20%28LLMs%29%2C%20motivating%0Aautomated%20test%20generation.%20However%2C%20we%20uncover%20a%20trade-off%20between%20generating%0Aunit%20test%20inputs%20that%20reveal%20errors%20when%20given%20a%20faulty%20code%20and%20correctly%0Apredicting%20the%20unit%20test%20output%20without%20access%20to%20the%20gold%20solution.%20To%20address%0Athis%20trade-off%2C%20we%20propose%20UTGen%2C%20which%20teaches%20LLMs%20to%20generate%20unit%20test%0Ainputs%20that%20reveal%20errors%20along%20with%20their%20correct%20expected%20outputs%20based%20on%0Atask%20descriptions.%20Since%20model-generated%20tests%20can%20provide%20noisy%20signals%20%28e.g.%2C%0Afrom%20incorrectly%20predicted%20outputs%29%2C%20we%20propose%20UTDebug%20that%20%28i%29%20scales%20UTGen%0Avia%20test-time%20compute%20to%20improve%20UT%20output%20prediction%2C%20and%20%28ii%29%20validates%20and%0Abacktracks%20edits%20based%20on%20multiple%20generated%20UTs%20to%20avoid%20overfitting%2C%20and%0Ahelps%20LLMs%20debug%20effectively.%20We%20show%20that%20UTGen%20outperforms%20other%20LLM-based%0Abaselines%20by%207.59%25%20based%20on%20a%20metric%20measuring%20the%20presence%20of%20both%0Aerror-revealing%20UT%20inputs%20and%20correct%20UT%20outputs.%20When%20used%20with%20UTDebug%2C%20we%0Afind%20that%20feedback%20from%20UTGen%27s%20unit%20tests%20improves%20pass%401%20accuracy%20of%20Qwen2.5%0A32B%20on%20HumanEvalFix%20and%20our%20own%20harder%20debugging%20split%20of%20MBPP%2B%20by%20over%203.17%25%0Aand%2012.35%25%20%28respectively%29%20over%20other%20LLM-based%20UT%20generation%20baselines.%0AMoreover%2C%20we%20observe%20that%20feedback%20from%20Qwen2.5%2032B-based%20UTGen%20model%20can%0Aenhance%20debugging%20with%20frontier%20LLMs%20like%20GPT-4o%20by%2013.8%25.%20Lastly%2C%20we%0Ademonstrate%20that%20UTGen%20is%20a%20better%20judge%20for%20code%20correctness%2C%20outperforming%20a%0Astate-of-the-art%20trained%208B%20reward%20model%20by%204.43%25%20on%20HumanEval%2B%20with%20best-of-10%0Asampling%20using%20Qwen2.5%207B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01619v3&entry.124074799=Read"},
{"title": "Self-supervised physics-informed generative networks for phase retrieval\n  from a single X-ray hologram", "author": "Xiaogang Yang and Dawit Hailu and Vojt\u011bch Kulvait and Thomas Jentschke and Silja Flenner and Imke Greving and Stuart I. Campbell and Johannes Hagemann and Christian G. Schroer and Tak Ming Wong and Julian Moosmann", "abstract": "  X-ray phase contrast imaging significantly improves the visualization of\nstructures with weak or uniform absorption, broadening its applications across\na wide range of scientific disciplines. Propagation-based phase contrast is\nparticularly suitable for time- or dose-critical in vivo/in situ/operando\n(tomography) experiments because it requires only a single intensity\nmeasurement. However, the phase information of the wave field is lost during\nthe measurement and must be recovered. Conventional algebraic and iterative\nmethods often rely on specific approximations or boundary conditions that may\nnot be met by many samples or experimental setups. In addition, they require\nmanual tuning of reconstruction parameters by experts, making them less\nadaptable for complex or variable conditions. Here we present a self-learning\napproach for solving the inverse problem of phase retrieval in the near-field\nregime of Fresnel theory using a single intensity measurement (hologram). A\nphysics-informed generative adversarial network is employed to reconstruct both\nthe phase and absorbance of the unpropagated wave field in the sample plane\nfrom a single hologram. Unlike most deep learning approaches for phase\nretrieval, our approach does not require paired, unpaired, or simulated\ntraining data. This significantly broadens the applicability of our approach,\nas acquiring or generating suitable training data remains a major challenge due\nto the wide variability in sample types and experimental configurations. The\nalgorithm demonstrates robust and consistent performance across diverse imaging\nconditions and sample types, delivering quantitative, high-quality\nreconstructions for both simulated data and experimental datasets acquired at\nbeamline P05 at PETRA III (DESY, Hamburg), operated by Helmholtz-Zentrum\nHereon. Furthermore, it enables the simultaneous retrieval of both phase and\nabsorption information.\n", "link": "http://arxiv.org/abs/2508.15530v1", "date": "2025-08-21", "relevancy": 1.9881, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5284}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4795}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20physics-informed%20generative%20networks%20for%20phase%20retrieval%0A%20%20from%20a%20single%20X-ray%20hologram&body=Title%3A%20Self-supervised%20physics-informed%20generative%20networks%20for%20phase%20retrieval%0A%20%20from%20a%20single%20X-ray%20hologram%0AAuthor%3A%20Xiaogang%20Yang%20and%20Dawit%20Hailu%20and%20Vojt%C4%9Bch%20Kulvait%20and%20Thomas%20Jentschke%20and%20Silja%20Flenner%20and%20Imke%20Greving%20and%20Stuart%20I.%20Campbell%20and%20Johannes%20Hagemann%20and%20Christian%20G.%20Schroer%20and%20Tak%20Ming%20Wong%20and%20Julian%20Moosmann%0AAbstract%3A%20%20%20X-ray%20phase%20contrast%20imaging%20significantly%20improves%20the%20visualization%20of%0Astructures%20with%20weak%20or%20uniform%20absorption%2C%20broadening%20its%20applications%20across%0Aa%20wide%20range%20of%20scientific%20disciplines.%20Propagation-based%20phase%20contrast%20is%0Aparticularly%20suitable%20for%20time-%20or%20dose-critical%20in%20vivo/in%20situ/operando%0A%28tomography%29%20experiments%20because%20it%20requires%20only%20a%20single%20intensity%0Ameasurement.%20However%2C%20the%20phase%20information%20of%20the%20wave%20field%20is%20lost%20during%0Athe%20measurement%20and%20must%20be%20recovered.%20Conventional%20algebraic%20and%20iterative%0Amethods%20often%20rely%20on%20specific%20approximations%20or%20boundary%20conditions%20that%20may%0Anot%20be%20met%20by%20many%20samples%20or%20experimental%20setups.%20In%20addition%2C%20they%20require%0Amanual%20tuning%20of%20reconstruction%20parameters%20by%20experts%2C%20making%20them%20less%0Aadaptable%20for%20complex%20or%20variable%20conditions.%20Here%20we%20present%20a%20self-learning%0Aapproach%20for%20solving%20the%20inverse%20problem%20of%20phase%20retrieval%20in%20the%20near-field%0Aregime%20of%20Fresnel%20theory%20using%20a%20single%20intensity%20measurement%20%28hologram%29.%20A%0Aphysics-informed%20generative%20adversarial%20network%20is%20employed%20to%20reconstruct%20both%0Athe%20phase%20and%20absorbance%20of%20the%20unpropagated%20wave%20field%20in%20the%20sample%20plane%0Afrom%20a%20single%20hologram.%20Unlike%20most%20deep%20learning%20approaches%20for%20phase%0Aretrieval%2C%20our%20approach%20does%20not%20require%20paired%2C%20unpaired%2C%20or%20simulated%0Atraining%20data.%20This%20significantly%20broadens%20the%20applicability%20of%20our%20approach%2C%0Aas%20acquiring%20or%20generating%20suitable%20training%20data%20remains%20a%20major%20challenge%20due%0Ato%20the%20wide%20variability%20in%20sample%20types%20and%20experimental%20configurations.%20The%0Aalgorithm%20demonstrates%20robust%20and%20consistent%20performance%20across%20diverse%20imaging%0Aconditions%20and%20sample%20types%2C%20delivering%20quantitative%2C%20high-quality%0Areconstructions%20for%20both%20simulated%20data%20and%20experimental%20datasets%20acquired%20at%0Abeamline%20P05%20at%20PETRA%20III%20%28DESY%2C%20Hamburg%29%2C%20operated%20by%20Helmholtz-Zentrum%0AHereon.%20Furthermore%2C%20it%20enables%20the%20simultaneous%20retrieval%20of%20both%20phase%20and%0Aabsorption%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520physics-informed%2520generative%2520networks%2520for%2520phase%2520retrieval%250A%2520%2520from%2520a%2520single%2520X-ray%2520hologram%26entry.906535625%3DXiaogang%2520Yang%2520and%2520Dawit%2520Hailu%2520and%2520Vojt%25C4%259Bch%2520Kulvait%2520and%2520Thomas%2520Jentschke%2520and%2520Silja%2520Flenner%2520and%2520Imke%2520Greving%2520and%2520Stuart%2520I.%2520Campbell%2520and%2520Johannes%2520Hagemann%2520and%2520Christian%2520G.%2520Schroer%2520and%2520Tak%2520Ming%2520Wong%2520and%2520Julian%2520Moosmann%26entry.1292438233%3D%2520%2520X-ray%2520phase%2520contrast%2520imaging%2520significantly%2520improves%2520the%2520visualization%2520of%250Astructures%2520with%2520weak%2520or%2520uniform%2520absorption%252C%2520broadening%2520its%2520applications%2520across%250Aa%2520wide%2520range%2520of%2520scientific%2520disciplines.%2520Propagation-based%2520phase%2520contrast%2520is%250Aparticularly%2520suitable%2520for%2520time-%2520or%2520dose-critical%2520in%2520vivo/in%2520situ/operando%250A%2528tomography%2529%2520experiments%2520because%2520it%2520requires%2520only%2520a%2520single%2520intensity%250Ameasurement.%2520However%252C%2520the%2520phase%2520information%2520of%2520the%2520wave%2520field%2520is%2520lost%2520during%250Athe%2520measurement%2520and%2520must%2520be%2520recovered.%2520Conventional%2520algebraic%2520and%2520iterative%250Amethods%2520often%2520rely%2520on%2520specific%2520approximations%2520or%2520boundary%2520conditions%2520that%2520may%250Anot%2520be%2520met%2520by%2520many%2520samples%2520or%2520experimental%2520setups.%2520In%2520addition%252C%2520they%2520require%250Amanual%2520tuning%2520of%2520reconstruction%2520parameters%2520by%2520experts%252C%2520making%2520them%2520less%250Aadaptable%2520for%2520complex%2520or%2520variable%2520conditions.%2520Here%2520we%2520present%2520a%2520self-learning%250Aapproach%2520for%2520solving%2520the%2520inverse%2520problem%2520of%2520phase%2520retrieval%2520in%2520the%2520near-field%250Aregime%2520of%2520Fresnel%2520theory%2520using%2520a%2520single%2520intensity%2520measurement%2520%2528hologram%2529.%2520A%250Aphysics-informed%2520generative%2520adversarial%2520network%2520is%2520employed%2520to%2520reconstruct%2520both%250Athe%2520phase%2520and%2520absorbance%2520of%2520the%2520unpropagated%2520wave%2520field%2520in%2520the%2520sample%2520plane%250Afrom%2520a%2520single%2520hologram.%2520Unlike%2520most%2520deep%2520learning%2520approaches%2520for%2520phase%250Aretrieval%252C%2520our%2520approach%2520does%2520not%2520require%2520paired%252C%2520unpaired%252C%2520or%2520simulated%250Atraining%2520data.%2520This%2520significantly%2520broadens%2520the%2520applicability%2520of%2520our%2520approach%252C%250Aas%2520acquiring%2520or%2520generating%2520suitable%2520training%2520data%2520remains%2520a%2520major%2520challenge%2520due%250Ato%2520the%2520wide%2520variability%2520in%2520sample%2520types%2520and%2520experimental%2520configurations.%2520The%250Aalgorithm%2520demonstrates%2520robust%2520and%2520consistent%2520performance%2520across%2520diverse%2520imaging%250Aconditions%2520and%2520sample%2520types%252C%2520delivering%2520quantitative%252C%2520high-quality%250Areconstructions%2520for%2520both%2520simulated%2520data%2520and%2520experimental%2520datasets%2520acquired%2520at%250Abeamline%2520P05%2520at%2520PETRA%2520III%2520%2528DESY%252C%2520Hamburg%2529%252C%2520operated%2520by%2520Helmholtz-Zentrum%250AHereon.%2520Furthermore%252C%2520it%2520enables%2520the%2520simultaneous%2520retrieval%2520of%2520both%2520phase%2520and%250Aabsorption%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20physics-informed%20generative%20networks%20for%20phase%20retrieval%0A%20%20from%20a%20single%20X-ray%20hologram&entry.906535625=Xiaogang%20Yang%20and%20Dawit%20Hailu%20and%20Vojt%C4%9Bch%20Kulvait%20and%20Thomas%20Jentschke%20and%20Silja%20Flenner%20and%20Imke%20Greving%20and%20Stuart%20I.%20Campbell%20and%20Johannes%20Hagemann%20and%20Christian%20G.%20Schroer%20and%20Tak%20Ming%20Wong%20and%20Julian%20Moosmann&entry.1292438233=%20%20X-ray%20phase%20contrast%20imaging%20significantly%20improves%20the%20visualization%20of%0Astructures%20with%20weak%20or%20uniform%20absorption%2C%20broadening%20its%20applications%20across%0Aa%20wide%20range%20of%20scientific%20disciplines.%20Propagation-based%20phase%20contrast%20is%0Aparticularly%20suitable%20for%20time-%20or%20dose-critical%20in%20vivo/in%20situ/operando%0A%28tomography%29%20experiments%20because%20it%20requires%20only%20a%20single%20intensity%0Ameasurement.%20However%2C%20the%20phase%20information%20of%20the%20wave%20field%20is%20lost%20during%0Athe%20measurement%20and%20must%20be%20recovered.%20Conventional%20algebraic%20and%20iterative%0Amethods%20often%20rely%20on%20specific%20approximations%20or%20boundary%20conditions%20that%20may%0Anot%20be%20met%20by%20many%20samples%20or%20experimental%20setups.%20In%20addition%2C%20they%20require%0Amanual%20tuning%20of%20reconstruction%20parameters%20by%20experts%2C%20making%20them%20less%0Aadaptable%20for%20complex%20or%20variable%20conditions.%20Here%20we%20present%20a%20self-learning%0Aapproach%20for%20solving%20the%20inverse%20problem%20of%20phase%20retrieval%20in%20the%20near-field%0Aregime%20of%20Fresnel%20theory%20using%20a%20single%20intensity%20measurement%20%28hologram%29.%20A%0Aphysics-informed%20generative%20adversarial%20network%20is%20employed%20to%20reconstruct%20both%0Athe%20phase%20and%20absorbance%20of%20the%20unpropagated%20wave%20field%20in%20the%20sample%20plane%0Afrom%20a%20single%20hologram.%20Unlike%20most%20deep%20learning%20approaches%20for%20phase%0Aretrieval%2C%20our%20approach%20does%20not%20require%20paired%2C%20unpaired%2C%20or%20simulated%0Atraining%20data.%20This%20significantly%20broadens%20the%20applicability%20of%20our%20approach%2C%0Aas%20acquiring%20or%20generating%20suitable%20training%20data%20remains%20a%20major%20challenge%20due%0Ato%20the%20wide%20variability%20in%20sample%20types%20and%20experimental%20configurations.%20The%0Aalgorithm%20demonstrates%20robust%20and%20consistent%20performance%20across%20diverse%20imaging%0Aconditions%20and%20sample%20types%2C%20delivering%20quantitative%2C%20high-quality%0Areconstructions%20for%20both%20simulated%20data%20and%20experimental%20datasets%20acquired%20at%0Abeamline%20P05%20at%20PETRA%20III%20%28DESY%2C%20Hamburg%29%2C%20operated%20by%20Helmholtz-Zentrum%0AHereon.%20Furthermore%2C%20it%20enables%20the%20simultaneous%20retrieval%20of%20both%20phase%20and%0Aabsorption%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15530v1&entry.124074799=Read"},
{"title": "JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on\n  FPGAs", "author": "Zhiqiang Que and Chang Sun and Sudarshan Paramesvaran and Emyr Clement and Katerina Karakoulaki and Christopher Brown and Lauri Laatu and Arianna Cox and Alexander Tapper and Wayne Luk and Maria Spiropulu", "abstract": "  Graph Neural Networks (GNNs), particularly Interaction Networks (INs), have\nshown exceptional performance for jet tagging at the CERN High-Luminosity Large\nHadron Collider (HL-LHC). However, their computational complexity and irregular\nmemory access patterns pose significant challenges for deployment on FPGAs in\nhardware trigger systems, where strict latency and resource constraints apply.\nIn this work, we propose JEDI-linear, a novel GNN architecture with linear\ncomputational complexity that eliminates explicit pairwise interactions by\nleveraging shared transformations and global aggregation. To further enhance\nhardware efficiency, we introduce fine-grained quantization-aware training with\nper-parameter bitwidth optimization and employ multiplier-free\nmultiply-accumulate operations via distributed arithmetic. Evaluation results\nshow that our FPGA-based JEDI-linear achieves 3.7 to 11.5 times lower latency,\nup to 150 times lower initiation interval, and up to 6.2 times lower LUT usage\ncompared to state-of-the-art designs while also delivering higher model\naccuracy and eliminating the need for DSP blocks entirely. In contrast,\nstate-of-the-art solutions consume over 8,700 DSPs. This is the first\ninteraction-based GNN to achieve less than 60~ns latency and currently meets\nthe requirements for use in the HL-LHC CMS Level-1 trigger system. This work\nadvances the next-generation trigger systems by enabling accurate, scalable,\nand resource-efficient GNN inference in real-time environments. Our\nopen-sourced templates will further support reproducibility and broader\nadoption across scientific applications.\n", "link": "http://arxiv.org/abs/2508.15468v1", "date": "2025-08-21", "relevancy": 1.986, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5007}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4945}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JEDI-linear%3A%20Fast%20and%20Efficient%20Graph%20Neural%20Networks%20for%20Jet%20Tagging%20on%0A%20%20FPGAs&body=Title%3A%20JEDI-linear%3A%20Fast%20and%20Efficient%20Graph%20Neural%20Networks%20for%20Jet%20Tagging%20on%0A%20%20FPGAs%0AAuthor%3A%20Zhiqiang%20Que%20and%20Chang%20Sun%20and%20Sudarshan%20Paramesvaran%20and%20Emyr%20Clement%20and%20Katerina%20Karakoulaki%20and%20Christopher%20Brown%20and%20Lauri%20Laatu%20and%20Arianna%20Cox%20and%20Alexander%20Tapper%20and%20Wayne%20Luk%20and%20Maria%20Spiropulu%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20particularly%20Interaction%20Networks%20%28INs%29%2C%20have%0Ashown%20exceptional%20performance%20for%20jet%20tagging%20at%20the%20CERN%20High-Luminosity%20Large%0AHadron%20Collider%20%28HL-LHC%29.%20However%2C%20their%20computational%20complexity%20and%20irregular%0Amemory%20access%20patterns%20pose%20significant%20challenges%20for%20deployment%20on%20FPGAs%20in%0Ahardware%20trigger%20systems%2C%20where%20strict%20latency%20and%20resource%20constraints%20apply.%0AIn%20this%20work%2C%20we%20propose%20JEDI-linear%2C%20a%20novel%20GNN%20architecture%20with%20linear%0Acomputational%20complexity%20that%20eliminates%20explicit%20pairwise%20interactions%20by%0Aleveraging%20shared%20transformations%20and%20global%20aggregation.%20To%20further%20enhance%0Ahardware%20efficiency%2C%20we%20introduce%20fine-grained%20quantization-aware%20training%20with%0Aper-parameter%20bitwidth%20optimization%20and%20employ%20multiplier-free%0Amultiply-accumulate%20operations%20via%20distributed%20arithmetic.%20Evaluation%20results%0Ashow%20that%20our%20FPGA-based%20JEDI-linear%20achieves%203.7%20to%2011.5%20times%20lower%20latency%2C%0Aup%20to%20150%20times%20lower%20initiation%20interval%2C%20and%20up%20to%206.2%20times%20lower%20LUT%20usage%0Acompared%20to%20state-of-the-art%20designs%20while%20also%20delivering%20higher%20model%0Aaccuracy%20and%20eliminating%20the%20need%20for%20DSP%20blocks%20entirely.%20In%20contrast%2C%0Astate-of-the-art%20solutions%20consume%20over%208%2C700%20DSPs.%20This%20is%20the%20first%0Ainteraction-based%20GNN%20to%20achieve%20less%20than%2060~ns%20latency%20and%20currently%20meets%0Athe%20requirements%20for%20use%20in%20the%20HL-LHC%20CMS%20Level-1%20trigger%20system.%20This%20work%0Aadvances%20the%20next-generation%20trigger%20systems%20by%20enabling%20accurate%2C%20scalable%2C%0Aand%20resource-efficient%20GNN%20inference%20in%20real-time%20environments.%20Our%0Aopen-sourced%20templates%20will%20further%20support%20reproducibility%20and%20broader%0Aadoption%20across%20scientific%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJEDI-linear%253A%2520Fast%2520and%2520Efficient%2520Graph%2520Neural%2520Networks%2520for%2520Jet%2520Tagging%2520on%250A%2520%2520FPGAs%26entry.906535625%3DZhiqiang%2520Que%2520and%2520Chang%2520Sun%2520and%2520Sudarshan%2520Paramesvaran%2520and%2520Emyr%2520Clement%2520and%2520Katerina%2520Karakoulaki%2520and%2520Christopher%2520Brown%2520and%2520Lauri%2520Laatu%2520and%2520Arianna%2520Cox%2520and%2520Alexander%2520Tapper%2520and%2520Wayne%2520Luk%2520and%2520Maria%2520Spiropulu%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520particularly%2520Interaction%2520Networks%2520%2528INs%2529%252C%2520have%250Ashown%2520exceptional%2520performance%2520for%2520jet%2520tagging%2520at%2520the%2520CERN%2520High-Luminosity%2520Large%250AHadron%2520Collider%2520%2528HL-LHC%2529.%2520However%252C%2520their%2520computational%2520complexity%2520and%2520irregular%250Amemory%2520access%2520patterns%2520pose%2520significant%2520challenges%2520for%2520deployment%2520on%2520FPGAs%2520in%250Ahardware%2520trigger%2520systems%252C%2520where%2520strict%2520latency%2520and%2520resource%2520constraints%2520apply.%250AIn%2520this%2520work%252C%2520we%2520propose%2520JEDI-linear%252C%2520a%2520novel%2520GNN%2520architecture%2520with%2520linear%250Acomputational%2520complexity%2520that%2520eliminates%2520explicit%2520pairwise%2520interactions%2520by%250Aleveraging%2520shared%2520transformations%2520and%2520global%2520aggregation.%2520To%2520further%2520enhance%250Ahardware%2520efficiency%252C%2520we%2520introduce%2520fine-grained%2520quantization-aware%2520training%2520with%250Aper-parameter%2520bitwidth%2520optimization%2520and%2520employ%2520multiplier-free%250Amultiply-accumulate%2520operations%2520via%2520distributed%2520arithmetic.%2520Evaluation%2520results%250Ashow%2520that%2520our%2520FPGA-based%2520JEDI-linear%2520achieves%25203.7%2520to%252011.5%2520times%2520lower%2520latency%252C%250Aup%2520to%2520150%2520times%2520lower%2520initiation%2520interval%252C%2520and%2520up%2520to%25206.2%2520times%2520lower%2520LUT%2520usage%250Acompared%2520to%2520state-of-the-art%2520designs%2520while%2520also%2520delivering%2520higher%2520model%250Aaccuracy%2520and%2520eliminating%2520the%2520need%2520for%2520DSP%2520blocks%2520entirely.%2520In%2520contrast%252C%250Astate-of-the-art%2520solutions%2520consume%2520over%25208%252C700%2520DSPs.%2520This%2520is%2520the%2520first%250Ainteraction-based%2520GNN%2520to%2520achieve%2520less%2520than%252060~ns%2520latency%2520and%2520currently%2520meets%250Athe%2520requirements%2520for%2520use%2520in%2520the%2520HL-LHC%2520CMS%2520Level-1%2520trigger%2520system.%2520This%2520work%250Aadvances%2520the%2520next-generation%2520trigger%2520systems%2520by%2520enabling%2520accurate%252C%2520scalable%252C%250Aand%2520resource-efficient%2520GNN%2520inference%2520in%2520real-time%2520environments.%2520Our%250Aopen-sourced%2520templates%2520will%2520further%2520support%2520reproducibility%2520and%2520broader%250Aadoption%2520across%2520scientific%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JEDI-linear%3A%20Fast%20and%20Efficient%20Graph%20Neural%20Networks%20for%20Jet%20Tagging%20on%0A%20%20FPGAs&entry.906535625=Zhiqiang%20Que%20and%20Chang%20Sun%20and%20Sudarshan%20Paramesvaran%20and%20Emyr%20Clement%20and%20Katerina%20Karakoulaki%20and%20Christopher%20Brown%20and%20Lauri%20Laatu%20and%20Arianna%20Cox%20and%20Alexander%20Tapper%20and%20Wayne%20Luk%20and%20Maria%20Spiropulu&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20particularly%20Interaction%20Networks%20%28INs%29%2C%20have%0Ashown%20exceptional%20performance%20for%20jet%20tagging%20at%20the%20CERN%20High-Luminosity%20Large%0AHadron%20Collider%20%28HL-LHC%29.%20However%2C%20their%20computational%20complexity%20and%20irregular%0Amemory%20access%20patterns%20pose%20significant%20challenges%20for%20deployment%20on%20FPGAs%20in%0Ahardware%20trigger%20systems%2C%20where%20strict%20latency%20and%20resource%20constraints%20apply.%0AIn%20this%20work%2C%20we%20propose%20JEDI-linear%2C%20a%20novel%20GNN%20architecture%20with%20linear%0Acomputational%20complexity%20that%20eliminates%20explicit%20pairwise%20interactions%20by%0Aleveraging%20shared%20transformations%20and%20global%20aggregation.%20To%20further%20enhance%0Ahardware%20efficiency%2C%20we%20introduce%20fine-grained%20quantization-aware%20training%20with%0Aper-parameter%20bitwidth%20optimization%20and%20employ%20multiplier-free%0Amultiply-accumulate%20operations%20via%20distributed%20arithmetic.%20Evaluation%20results%0Ashow%20that%20our%20FPGA-based%20JEDI-linear%20achieves%203.7%20to%2011.5%20times%20lower%20latency%2C%0Aup%20to%20150%20times%20lower%20initiation%20interval%2C%20and%20up%20to%206.2%20times%20lower%20LUT%20usage%0Acompared%20to%20state-of-the-art%20designs%20while%20also%20delivering%20higher%20model%0Aaccuracy%20and%20eliminating%20the%20need%20for%20DSP%20blocks%20entirely.%20In%20contrast%2C%0Astate-of-the-art%20solutions%20consume%20over%208%2C700%20DSPs.%20This%20is%20the%20first%0Ainteraction-based%20GNN%20to%20achieve%20less%20than%2060~ns%20latency%20and%20currently%20meets%0Athe%20requirements%20for%20use%20in%20the%20HL-LHC%20CMS%20Level-1%20trigger%20system.%20This%20work%0Aadvances%20the%20next-generation%20trigger%20systems%20by%20enabling%20accurate%2C%20scalable%2C%0Aand%20resource-efficient%20GNN%20inference%20in%20real-time%20environments.%20Our%0Aopen-sourced%20templates%20will%20further%20support%20reproducibility%20and%20broader%0Aadoption%20across%20scientific%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15468v1&entry.124074799=Read"},
{"title": "CC-Time: Cross-Model and Cross-Modality Time Series Forecasting", "author": "Peng Chen and Yihang Wang and Yang Shu and Yunyao Cheng and Kai Zhao and Zhongwen Rao and Lujia Pan and Bin Yang and Chenjuan Guo", "abstract": "  With the success of pre-trained language models (PLMs) in various application\nfields beyond natural language processing, language models have raised emerging\nattention in the field of time series forecasting (TSF) and have shown great\nprospects. However, current PLM-based TSF methods still fail to achieve\nsatisfactory prediction accuracy matching the strong sequential modeling power\nof language models. To address this issue, we propose Cross-Model and\nCross-Modality Learning with PLMs for time series forecasting (CC-Time). We\nexplore the potential of PLMs for time series forecasting from two aspects: 1)\nwhat time series features could be modeled by PLMs, and 2) whether relying\nsolely on PLMs is sufficient for building time series models. In the first\naspect, CC-Time incorporates cross-modality learning to model temporal\ndependency and channel correlations in the language model from both time series\nsequences and their corresponding text descriptions. In the second aspect,\nCC-Time further proposes the cross-model fusion block to adaptively integrate\nknowledge from the PLMs and time series model to form a more comprehensive\nmodeling of time series patterns. Extensive experiments on nine real-world\ndatasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy\nin both full-data training and few-shot learning situations.\n", "link": "http://arxiv.org/abs/2508.12235v2", "date": "2025-08-21", "relevancy": 1.9831, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.534}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4702}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CC-Time%3A%20Cross-Model%20and%20Cross-Modality%20Time%20Series%20Forecasting&body=Title%3A%20CC-Time%3A%20Cross-Model%20and%20Cross-Modality%20Time%20Series%20Forecasting%0AAuthor%3A%20Peng%20Chen%20and%20Yihang%20Wang%20and%20Yang%20Shu%20and%20Yunyao%20Cheng%20and%20Kai%20Zhao%20and%20Zhongwen%20Rao%20and%20Lujia%20Pan%20and%20Bin%20Yang%20and%20Chenjuan%20Guo%0AAbstract%3A%20%20%20With%20the%20success%20of%20pre-trained%20language%20models%20%28PLMs%29%20in%20various%20application%0Afields%20beyond%20natural%20language%20processing%2C%20language%20models%20have%20raised%20emerging%0Aattention%20in%20the%20field%20of%20time%20series%20forecasting%20%28TSF%29%20and%20have%20shown%20great%0Aprospects.%20However%2C%20current%20PLM-based%20TSF%20methods%20still%20fail%20to%20achieve%0Asatisfactory%20prediction%20accuracy%20matching%20the%20strong%20sequential%20modeling%20power%0Aof%20language%20models.%20To%20address%20this%20issue%2C%20we%20propose%20Cross-Model%20and%0ACross-Modality%20Learning%20with%20PLMs%20for%20time%20series%20forecasting%20%28CC-Time%29.%20We%0Aexplore%20the%20potential%20of%20PLMs%20for%20time%20series%20forecasting%20from%20two%20aspects%3A%201%29%0Awhat%20time%20series%20features%20could%20be%20modeled%20by%20PLMs%2C%20and%202%29%20whether%20relying%0Asolely%20on%20PLMs%20is%20sufficient%20for%20building%20time%20series%20models.%20In%20the%20first%0Aaspect%2C%20CC-Time%20incorporates%20cross-modality%20learning%20to%20model%20temporal%0Adependency%20and%20channel%20correlations%20in%20the%20language%20model%20from%20both%20time%20series%0Asequences%20and%20their%20corresponding%20text%20descriptions.%20In%20the%20second%20aspect%2C%0ACC-Time%20further%20proposes%20the%20cross-model%20fusion%20block%20to%20adaptively%20integrate%0Aknowledge%20from%20the%20PLMs%20and%20time%20series%20model%20to%20form%20a%20more%20comprehensive%0Amodeling%20of%20time%20series%20patterns.%20Extensive%20experiments%20on%20nine%20real-world%0Adatasets%20demonstrate%20that%20CC-Time%20achieves%20state-of-the-art%20prediction%20accuracy%0Ain%20both%20full-data%20training%20and%20few-shot%20learning%20situations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12235v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCC-Time%253A%2520Cross-Model%2520and%2520Cross-Modality%2520Time%2520Series%2520Forecasting%26entry.906535625%3DPeng%2520Chen%2520and%2520Yihang%2520Wang%2520and%2520Yang%2520Shu%2520and%2520Yunyao%2520Cheng%2520and%2520Kai%2520Zhao%2520and%2520Zhongwen%2520Rao%2520and%2520Lujia%2520Pan%2520and%2520Bin%2520Yang%2520and%2520Chenjuan%2520Guo%26entry.1292438233%3D%2520%2520With%2520the%2520success%2520of%2520pre-trained%2520language%2520models%2520%2528PLMs%2529%2520in%2520various%2520application%250Afields%2520beyond%2520natural%2520language%2520processing%252C%2520language%2520models%2520have%2520raised%2520emerging%250Aattention%2520in%2520the%2520field%2520of%2520time%2520series%2520forecasting%2520%2528TSF%2529%2520and%2520have%2520shown%2520great%250Aprospects.%2520However%252C%2520current%2520PLM-based%2520TSF%2520methods%2520still%2520fail%2520to%2520achieve%250Asatisfactory%2520prediction%2520accuracy%2520matching%2520the%2520strong%2520sequential%2520modeling%2520power%250Aof%2520language%2520models.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Cross-Model%2520and%250ACross-Modality%2520Learning%2520with%2520PLMs%2520for%2520time%2520series%2520forecasting%2520%2528CC-Time%2529.%2520We%250Aexplore%2520the%2520potential%2520of%2520PLMs%2520for%2520time%2520series%2520forecasting%2520from%2520two%2520aspects%253A%25201%2529%250Awhat%2520time%2520series%2520features%2520could%2520be%2520modeled%2520by%2520PLMs%252C%2520and%25202%2529%2520whether%2520relying%250Asolely%2520on%2520PLMs%2520is%2520sufficient%2520for%2520building%2520time%2520series%2520models.%2520In%2520the%2520first%250Aaspect%252C%2520CC-Time%2520incorporates%2520cross-modality%2520learning%2520to%2520model%2520temporal%250Adependency%2520and%2520channel%2520correlations%2520in%2520the%2520language%2520model%2520from%2520both%2520time%2520series%250Asequences%2520and%2520their%2520corresponding%2520text%2520descriptions.%2520In%2520the%2520second%2520aspect%252C%250ACC-Time%2520further%2520proposes%2520the%2520cross-model%2520fusion%2520block%2520to%2520adaptively%2520integrate%250Aknowledge%2520from%2520the%2520PLMs%2520and%2520time%2520series%2520model%2520to%2520form%2520a%2520more%2520comprehensive%250Amodeling%2520of%2520time%2520series%2520patterns.%2520Extensive%2520experiments%2520on%2520nine%2520real-world%250Adatasets%2520demonstrate%2520that%2520CC-Time%2520achieves%2520state-of-the-art%2520prediction%2520accuracy%250Ain%2520both%2520full-data%2520training%2520and%2520few-shot%2520learning%2520situations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12235v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CC-Time%3A%20Cross-Model%20and%20Cross-Modality%20Time%20Series%20Forecasting&entry.906535625=Peng%20Chen%20and%20Yihang%20Wang%20and%20Yang%20Shu%20and%20Yunyao%20Cheng%20and%20Kai%20Zhao%20and%20Zhongwen%20Rao%20and%20Lujia%20Pan%20and%20Bin%20Yang%20and%20Chenjuan%20Guo&entry.1292438233=%20%20With%20the%20success%20of%20pre-trained%20language%20models%20%28PLMs%29%20in%20various%20application%0Afields%20beyond%20natural%20language%20processing%2C%20language%20models%20have%20raised%20emerging%0Aattention%20in%20the%20field%20of%20time%20series%20forecasting%20%28TSF%29%20and%20have%20shown%20great%0Aprospects.%20However%2C%20current%20PLM-based%20TSF%20methods%20still%20fail%20to%20achieve%0Asatisfactory%20prediction%20accuracy%20matching%20the%20strong%20sequential%20modeling%20power%0Aof%20language%20models.%20To%20address%20this%20issue%2C%20we%20propose%20Cross-Model%20and%0ACross-Modality%20Learning%20with%20PLMs%20for%20time%20series%20forecasting%20%28CC-Time%29.%20We%0Aexplore%20the%20potential%20of%20PLMs%20for%20time%20series%20forecasting%20from%20two%20aspects%3A%201%29%0Awhat%20time%20series%20features%20could%20be%20modeled%20by%20PLMs%2C%20and%202%29%20whether%20relying%0Asolely%20on%20PLMs%20is%20sufficient%20for%20building%20time%20series%20models.%20In%20the%20first%0Aaspect%2C%20CC-Time%20incorporates%20cross-modality%20learning%20to%20model%20temporal%0Adependency%20and%20channel%20correlations%20in%20the%20language%20model%20from%20both%20time%20series%0Asequences%20and%20their%20corresponding%20text%20descriptions.%20In%20the%20second%20aspect%2C%0ACC-Time%20further%20proposes%20the%20cross-model%20fusion%20block%20to%20adaptively%20integrate%0Aknowledge%20from%20the%20PLMs%20and%20time%20series%20model%20to%20form%20a%20more%20comprehensive%0Amodeling%20of%20time%20series%20patterns.%20Extensive%20experiments%20on%20nine%20real-world%0Adatasets%20demonstrate%20that%20CC-Time%20achieves%20state-of-the-art%20prediction%20accuracy%0Ain%20both%20full-data%20training%20and%20few-shot%20learning%20situations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12235v2&entry.124074799=Read"},
{"title": "Jointly Computation- and Communication-Efficient Distributed Learning", "author": "Xiaoxing Ren and Nicola Bastianello and Karl H. Johansson and Thomas Parisini", "abstract": "  We address distributed learning problems over undirected networks.\nSpecifically, we focus on designing a novel ADMM-based algorithm that is\njointly computation- and communication-efficient. Our design guarantees\ncomputational efficiency by allowing agents to use stochastic gradients during\nlocal training. Moreover, communication efficiency is achieved as follows: i)\nthe agents perform multiple training epochs between communication rounds, and\nii) compressed transmissions are used. We prove exact linear convergence of the\nalgorithm in the strongly convex setting. We corroborate our theoretical\nresults by numerical comparisons with state of the art techniques on a\nclassification task.\n", "link": "http://arxiv.org/abs/2508.15509v1", "date": "2025-08-21", "relevancy": 1.9829, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4992}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4969}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jointly%20Computation-%20and%20Communication-Efficient%20Distributed%20Learning&body=Title%3A%20Jointly%20Computation-%20and%20Communication-Efficient%20Distributed%20Learning%0AAuthor%3A%20Xiaoxing%20Ren%20and%20Nicola%20Bastianello%20and%20Karl%20H.%20Johansson%20and%20Thomas%20Parisini%0AAbstract%3A%20%20%20We%20address%20distributed%20learning%20problems%20over%20undirected%20networks.%0ASpecifically%2C%20we%20focus%20on%20designing%20a%20novel%20ADMM-based%20algorithm%20that%20is%0Ajointly%20computation-%20and%20communication-efficient.%20Our%20design%20guarantees%0Acomputational%20efficiency%20by%20allowing%20agents%20to%20use%20stochastic%20gradients%20during%0Alocal%20training.%20Moreover%2C%20communication%20efficiency%20is%20achieved%20as%20follows%3A%20i%29%0Athe%20agents%20perform%20multiple%20training%20epochs%20between%20communication%20rounds%2C%20and%0Aii%29%20compressed%20transmissions%20are%20used.%20We%20prove%20exact%20linear%20convergence%20of%20the%0Aalgorithm%20in%20the%20strongly%20convex%20setting.%20We%20corroborate%20our%20theoretical%0Aresults%20by%20numerical%20comparisons%20with%20state%20of%20the%20art%20techniques%20on%20a%0Aclassification%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJointly%2520Computation-%2520and%2520Communication-Efficient%2520Distributed%2520Learning%26entry.906535625%3DXiaoxing%2520Ren%2520and%2520Nicola%2520Bastianello%2520and%2520Karl%2520H.%2520Johansson%2520and%2520Thomas%2520Parisini%26entry.1292438233%3D%2520%2520We%2520address%2520distributed%2520learning%2520problems%2520over%2520undirected%2520networks.%250ASpecifically%252C%2520we%2520focus%2520on%2520designing%2520a%2520novel%2520ADMM-based%2520algorithm%2520that%2520is%250Ajointly%2520computation-%2520and%2520communication-efficient.%2520Our%2520design%2520guarantees%250Acomputational%2520efficiency%2520by%2520allowing%2520agents%2520to%2520use%2520stochastic%2520gradients%2520during%250Alocal%2520training.%2520Moreover%252C%2520communication%2520efficiency%2520is%2520achieved%2520as%2520follows%253A%2520i%2529%250Athe%2520agents%2520perform%2520multiple%2520training%2520epochs%2520between%2520communication%2520rounds%252C%2520and%250Aii%2529%2520compressed%2520transmissions%2520are%2520used.%2520We%2520prove%2520exact%2520linear%2520convergence%2520of%2520the%250Aalgorithm%2520in%2520the%2520strongly%2520convex%2520setting.%2520We%2520corroborate%2520our%2520theoretical%250Aresults%2520by%2520numerical%2520comparisons%2520with%2520state%2520of%2520the%2520art%2520techniques%2520on%2520a%250Aclassification%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jointly%20Computation-%20and%20Communication-Efficient%20Distributed%20Learning&entry.906535625=Xiaoxing%20Ren%20and%20Nicola%20Bastianello%20and%20Karl%20H.%20Johansson%20and%20Thomas%20Parisini&entry.1292438233=%20%20We%20address%20distributed%20learning%20problems%20over%20undirected%20networks.%0ASpecifically%2C%20we%20focus%20on%20designing%20a%20novel%20ADMM-based%20algorithm%20that%20is%0Ajointly%20computation-%20and%20communication-efficient.%20Our%20design%20guarantees%0Acomputational%20efficiency%20by%20allowing%20agents%20to%20use%20stochastic%20gradients%20during%0Alocal%20training.%20Moreover%2C%20communication%20efficiency%20is%20achieved%20as%20follows%3A%20i%29%0Athe%20agents%20perform%20multiple%20training%20epochs%20between%20communication%20rounds%2C%20and%0Aii%29%20compressed%20transmissions%20are%20used.%20We%20prove%20exact%20linear%20convergence%20of%20the%0Aalgorithm%20in%20the%20strongly%20convex%20setting.%20We%20corroborate%20our%20theoretical%0Aresults%20by%20numerical%20comparisons%20with%20state%20of%20the%20art%20techniques%20on%20a%0Aclassification%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15509v1&entry.124074799=Read"},
{"title": "Think in Blocks: Adaptive Reasoning from Direct Response to Deep\n  Reasoning", "author": "Yekun Zhu and Guang Chen and Chengjun Mao", "abstract": "  Large Language Models (LLMs) with chains-of-thought have demonstrated strong\nperformance on an increasing range of tasks, particularly those involving\ncomplex logical reasoning. However, excessively long chains can lead to\noverthinking, causing computational waste and slower responses. This raises a\nquestion: can LLMs dynamically adjust the length of their reasoning processes\nbased on task complexity? To address this, we propose the Think in Blocks\nframework, which enables adaptive reasoning-from zero to deep reasoning-by\npartitioning the reasoning process into a tunable number of blocks. Our main\ncontributions are: (1) Establishing an explicit block-structured paradigm in\nwhich the model first predicts an integer reasoning budget-the number of\nblocks-and then partitions its reasoning accordingly; (2) Training an adaptive\nmodel through a three-stage pipeline-Supervised Fine-Tuning, reward-guided\nDirect Preference Optimization, and Reinforcement Learning-that adjusts its\nreasoning depth to problem difficulty; (3) Exploiting the explicit block count\nto dynamically control reasoning depth at inference time, allowing flexible\nadjustment of chain-of-thought length during deployment.\n", "link": "http://arxiv.org/abs/2508.15507v1", "date": "2025-08-21", "relevancy": 1.9786, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4999}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4999}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think%20in%20Blocks%3A%20Adaptive%20Reasoning%20from%20Direct%20Response%20to%20Deep%0A%20%20Reasoning&body=Title%3A%20Think%20in%20Blocks%3A%20Adaptive%20Reasoning%20from%20Direct%20Response%20to%20Deep%0A%20%20Reasoning%0AAuthor%3A%20Yekun%20Zhu%20and%20Guang%20Chen%20and%20Chengjun%20Mao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20with%20chains-of-thought%20have%20demonstrated%20strong%0Aperformance%20on%20an%20increasing%20range%20of%20tasks%2C%20particularly%20those%20involving%0Acomplex%20logical%20reasoning.%20However%2C%20excessively%20long%20chains%20can%20lead%20to%0Aoverthinking%2C%20causing%20computational%20waste%20and%20slower%20responses.%20This%20raises%20a%0Aquestion%3A%20can%20LLMs%20dynamically%20adjust%20the%20length%20of%20their%20reasoning%20processes%0Abased%20on%20task%20complexity%3F%20To%20address%20this%2C%20we%20propose%20the%20Think%20in%20Blocks%0Aframework%2C%20which%20enables%20adaptive%20reasoning-from%20zero%20to%20deep%20reasoning-by%0Apartitioning%20the%20reasoning%20process%20into%20a%20tunable%20number%20of%20blocks.%20Our%20main%0Acontributions%20are%3A%20%281%29%20Establishing%20an%20explicit%20block-structured%20paradigm%20in%0Awhich%20the%20model%20first%20predicts%20an%20integer%20reasoning%20budget-the%20number%20of%0Ablocks-and%20then%20partitions%20its%20reasoning%20accordingly%3B%20%282%29%20Training%20an%20adaptive%0Amodel%20through%20a%20three-stage%20pipeline-Supervised%20Fine-Tuning%2C%20reward-guided%0ADirect%20Preference%20Optimization%2C%20and%20Reinforcement%20Learning-that%20adjusts%20its%0Areasoning%20depth%20to%20problem%20difficulty%3B%20%283%29%20Exploiting%20the%20explicit%20block%20count%0Ato%20dynamically%20control%20reasoning%20depth%20at%20inference%20time%2C%20allowing%20flexible%0Aadjustment%20of%20chain-of-thought%20length%20during%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink%2520in%2520Blocks%253A%2520Adaptive%2520Reasoning%2520from%2520Direct%2520Response%2520to%2520Deep%250A%2520%2520Reasoning%26entry.906535625%3DYekun%2520Zhu%2520and%2520Guang%2520Chen%2520and%2520Chengjun%2520Mao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520chains-of-thought%2520have%2520demonstrated%2520strong%250Aperformance%2520on%2520an%2520increasing%2520range%2520of%2520tasks%252C%2520particularly%2520those%2520involving%250Acomplex%2520logical%2520reasoning.%2520However%252C%2520excessively%2520long%2520chains%2520can%2520lead%2520to%250Aoverthinking%252C%2520causing%2520computational%2520waste%2520and%2520slower%2520responses.%2520This%2520raises%2520a%250Aquestion%253A%2520can%2520LLMs%2520dynamically%2520adjust%2520the%2520length%2520of%2520their%2520reasoning%2520processes%250Abased%2520on%2520task%2520complexity%253F%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Think%2520in%2520Blocks%250Aframework%252C%2520which%2520enables%2520adaptive%2520reasoning-from%2520zero%2520to%2520deep%2520reasoning-by%250Apartitioning%2520the%2520reasoning%2520process%2520into%2520a%2520tunable%2520number%2520of%2520blocks.%2520Our%2520main%250Acontributions%2520are%253A%2520%25281%2529%2520Establishing%2520an%2520explicit%2520block-structured%2520paradigm%2520in%250Awhich%2520the%2520model%2520first%2520predicts%2520an%2520integer%2520reasoning%2520budget-the%2520number%2520of%250Ablocks-and%2520then%2520partitions%2520its%2520reasoning%2520accordingly%253B%2520%25282%2529%2520Training%2520an%2520adaptive%250Amodel%2520through%2520a%2520three-stage%2520pipeline-Supervised%2520Fine-Tuning%252C%2520reward-guided%250ADirect%2520Preference%2520Optimization%252C%2520and%2520Reinforcement%2520Learning-that%2520adjusts%2520its%250Areasoning%2520depth%2520to%2520problem%2520difficulty%253B%2520%25283%2529%2520Exploiting%2520the%2520explicit%2520block%2520count%250Ato%2520dynamically%2520control%2520reasoning%2520depth%2520at%2520inference%2520time%252C%2520allowing%2520flexible%250Aadjustment%2520of%2520chain-of-thought%2520length%2520during%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%20in%20Blocks%3A%20Adaptive%20Reasoning%20from%20Direct%20Response%20to%20Deep%0A%20%20Reasoning&entry.906535625=Yekun%20Zhu%20and%20Guang%20Chen%20and%20Chengjun%20Mao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20with%20chains-of-thought%20have%20demonstrated%20strong%0Aperformance%20on%20an%20increasing%20range%20of%20tasks%2C%20particularly%20those%20involving%0Acomplex%20logical%20reasoning.%20However%2C%20excessively%20long%20chains%20can%20lead%20to%0Aoverthinking%2C%20causing%20computational%20waste%20and%20slower%20responses.%20This%20raises%20a%0Aquestion%3A%20can%20LLMs%20dynamically%20adjust%20the%20length%20of%20their%20reasoning%20processes%0Abased%20on%20task%20complexity%3F%20To%20address%20this%2C%20we%20propose%20the%20Think%20in%20Blocks%0Aframework%2C%20which%20enables%20adaptive%20reasoning-from%20zero%20to%20deep%20reasoning-by%0Apartitioning%20the%20reasoning%20process%20into%20a%20tunable%20number%20of%20blocks.%20Our%20main%0Acontributions%20are%3A%20%281%29%20Establishing%20an%20explicit%20block-structured%20paradigm%20in%0Awhich%20the%20model%20first%20predicts%20an%20integer%20reasoning%20budget-the%20number%20of%0Ablocks-and%20then%20partitions%20its%20reasoning%20accordingly%3B%20%282%29%20Training%20an%20adaptive%0Amodel%20through%20a%20three-stage%20pipeline-Supervised%20Fine-Tuning%2C%20reward-guided%0ADirect%20Preference%20Optimization%2C%20and%20Reinforcement%20Learning-that%20adjusts%20its%0Areasoning%20depth%20to%20problem%20difficulty%3B%20%283%29%20Exploiting%20the%20explicit%20block%20count%0Ato%20dynamically%20control%20reasoning%20depth%20at%20inference%20time%2C%20allowing%20flexible%0Aadjustment%20of%20chain-of-thought%20length%20during%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15507v1&entry.124074799=Read"},
{"title": "Parallel transport on matrix manifolds and Exponential Action", "author": "Du Nguyen and Stefan Sommer", "abstract": "  We express parallel transport for several common matrix Lie groups with a\nfamily of pseudo-Riemannian metrics in terms of matrix exponential and\nexponential actions. The metrics are constructed from a deformation of a\nbi-invariant metric and are naturally reductive. There is a similar picture for\nhomogeneous spaces when taking quotients satisfying a general condition. In\nparticular, for a Stiefel manifold of orthogonal matrices of size $n\\times d$,\nwe give an expression for parallel transport along a geodesic from time zero to\n$t$, that could be computed with time complexity of $O(n d^2)$ for small $t$,\nand of $O(td^3)$ for large $t$, contributing a step in a long-standing open\nproblem in matrix manifolds. A similar result holds for {\\it flag manifolds}\nwith the canonical metric. We also show the parallel transport formulas for the\n{\\it general linear group} and the {\\it special orthogonal group} under these\nmetrics.\n", "link": "http://arxiv.org/abs/2408.06054v2", "date": "2025-08-21", "relevancy": 1.972, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.404}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.394}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallel%20transport%20on%20matrix%20manifolds%20and%20Exponential%20Action&body=Title%3A%20Parallel%20transport%20on%20matrix%20manifolds%20and%20Exponential%20Action%0AAuthor%3A%20Du%20Nguyen%20and%20Stefan%20Sommer%0AAbstract%3A%20%20%20We%20express%20parallel%20transport%20for%20several%20common%20matrix%20Lie%20groups%20with%20a%0Afamily%20of%20pseudo-Riemannian%20metrics%20in%20terms%20of%20matrix%20exponential%20and%0Aexponential%20actions.%20The%20metrics%20are%20constructed%20from%20a%20deformation%20of%20a%0Abi-invariant%20metric%20and%20are%20naturally%20reductive.%20There%20is%20a%20similar%20picture%20for%0Ahomogeneous%20spaces%20when%20taking%20quotients%20satisfying%20a%20general%20condition.%20In%0Aparticular%2C%20for%20a%20Stiefel%20manifold%20of%20orthogonal%20matrices%20of%20size%20%24n%5Ctimes%20d%24%2C%0Awe%20give%20an%20expression%20for%20parallel%20transport%20along%20a%20geodesic%20from%20time%20zero%20to%0A%24t%24%2C%20that%20could%20be%20computed%20with%20time%20complexity%20of%20%24O%28n%20d%5E2%29%24%20for%20small%20%24t%24%2C%0Aand%20of%20%24O%28td%5E3%29%24%20for%20large%20%24t%24%2C%20contributing%20a%20step%20in%20a%20long-standing%20open%0Aproblem%20in%20matrix%20manifolds.%20A%20similar%20result%20holds%20for%20%7B%5Cit%20flag%20manifolds%7D%0Awith%20the%20canonical%20metric.%20We%20also%20show%20the%20parallel%20transport%20formulas%20for%20the%0A%7B%5Cit%20general%20linear%20group%7D%20and%20the%20%7B%5Cit%20special%20orthogonal%20group%7D%20under%20these%0Ametrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06054v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallel%2520transport%2520on%2520matrix%2520manifolds%2520and%2520Exponential%2520Action%26entry.906535625%3DDu%2520Nguyen%2520and%2520Stefan%2520Sommer%26entry.1292438233%3D%2520%2520We%2520express%2520parallel%2520transport%2520for%2520several%2520common%2520matrix%2520Lie%2520groups%2520with%2520a%250Afamily%2520of%2520pseudo-Riemannian%2520metrics%2520in%2520terms%2520of%2520matrix%2520exponential%2520and%250Aexponential%2520actions.%2520The%2520metrics%2520are%2520constructed%2520from%2520a%2520deformation%2520of%2520a%250Abi-invariant%2520metric%2520and%2520are%2520naturally%2520reductive.%2520There%2520is%2520a%2520similar%2520picture%2520for%250Ahomogeneous%2520spaces%2520when%2520taking%2520quotients%2520satisfying%2520a%2520general%2520condition.%2520In%250Aparticular%252C%2520for%2520a%2520Stiefel%2520manifold%2520of%2520orthogonal%2520matrices%2520of%2520size%2520%2524n%255Ctimes%2520d%2524%252C%250Awe%2520give%2520an%2520expression%2520for%2520parallel%2520transport%2520along%2520a%2520geodesic%2520from%2520time%2520zero%2520to%250A%2524t%2524%252C%2520that%2520could%2520be%2520computed%2520with%2520time%2520complexity%2520of%2520%2524O%2528n%2520d%255E2%2529%2524%2520for%2520small%2520%2524t%2524%252C%250Aand%2520of%2520%2524O%2528td%255E3%2529%2524%2520for%2520large%2520%2524t%2524%252C%2520contributing%2520a%2520step%2520in%2520a%2520long-standing%2520open%250Aproblem%2520in%2520matrix%2520manifolds.%2520A%2520similar%2520result%2520holds%2520for%2520%257B%255Cit%2520flag%2520manifolds%257D%250Awith%2520the%2520canonical%2520metric.%2520We%2520also%2520show%2520the%2520parallel%2520transport%2520formulas%2520for%2520the%250A%257B%255Cit%2520general%2520linear%2520group%257D%2520and%2520the%2520%257B%255Cit%2520special%2520orthogonal%2520group%257D%2520under%2520these%250Ametrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06054v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallel%20transport%20on%20matrix%20manifolds%20and%20Exponential%20Action&entry.906535625=Du%20Nguyen%20and%20Stefan%20Sommer&entry.1292438233=%20%20We%20express%20parallel%20transport%20for%20several%20common%20matrix%20Lie%20groups%20with%20a%0Afamily%20of%20pseudo-Riemannian%20metrics%20in%20terms%20of%20matrix%20exponential%20and%0Aexponential%20actions.%20The%20metrics%20are%20constructed%20from%20a%20deformation%20of%20a%0Abi-invariant%20metric%20and%20are%20naturally%20reductive.%20There%20is%20a%20similar%20picture%20for%0Ahomogeneous%20spaces%20when%20taking%20quotients%20satisfying%20a%20general%20condition.%20In%0Aparticular%2C%20for%20a%20Stiefel%20manifold%20of%20orthogonal%20matrices%20of%20size%20%24n%5Ctimes%20d%24%2C%0Awe%20give%20an%20expression%20for%20parallel%20transport%20along%20a%20geodesic%20from%20time%20zero%20to%0A%24t%24%2C%20that%20could%20be%20computed%20with%20time%20complexity%20of%20%24O%28n%20d%5E2%29%24%20for%20small%20%24t%24%2C%0Aand%20of%20%24O%28td%5E3%29%24%20for%20large%20%24t%24%2C%20contributing%20a%20step%20in%20a%20long-standing%20open%0Aproblem%20in%20matrix%20manifolds.%20A%20similar%20result%20holds%20for%20%7B%5Cit%20flag%20manifolds%7D%0Awith%20the%20canonical%20metric.%20We%20also%20show%20the%20parallel%20transport%20formulas%20for%20the%0A%7B%5Cit%20general%20linear%20group%7D%20and%20the%20%7B%5Cit%20special%20orthogonal%20group%7D%20under%20these%0Ametrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06054v2&entry.124074799=Read"},
{"title": "Exploring Modularity of Agentic Systems for Drug Discovery", "author": "Laura van Weesep and Samuel Genheden and Ola Engkvist and Jens Sj\u00f6lund", "abstract": "  Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery. In this study, we examine the\nmodularity of LLM-based agentic systems for drug discovery, i.e., whether parts\nof the system such as the LLM and type of agent are interchangeable, a topic\nthat has received limited attention in drug discovery. We compare the\nperformance of different LLMs and the effectiveness of tool-calling agents\nversus code-generating agents. Our case study, comparing performance in\norchestrating tools for chemistry and drug discovery using an LLM-as-a-judge\nscore, shows that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform\nalternative language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo,\nand Nova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question- and\nmodel-dependent. Furthermore, the impact of replacing system prompts is\ndependent on the question and model, underscoring that even in this particular\ndomain one cannot just replace components of the system without re-engineering.\nOur study highlights the necessity of further research into the modularity of\nagentic systems to enable the development of reliable and modular solutions for\nreal-world problems.\n", "link": "http://arxiv.org/abs/2506.22189v2", "date": "2025-08-21", "relevancy": 1.9524, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5281}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Modularity%20of%20Agentic%20Systems%20for%20Drug%20Discovery&body=Title%3A%20Exploring%20Modularity%20of%20Agentic%20Systems%20for%20Drug%20Discovery%0AAuthor%3A%20Laura%20van%20Weesep%20and%20Samuel%20Genheden%20and%20Ola%20Engkvist%20and%20Jens%20Sj%C3%B6lund%0AAbstract%3A%20%20%20Large-language%20models%20%28LLMs%29%20and%20agentic%20systems%20present%20exciting%0Aopportunities%20to%20accelerate%20drug%20discovery.%20In%20this%20study%2C%20we%20examine%20the%0Amodularity%20of%20LLM-based%20agentic%20systems%20for%20drug%20discovery%2C%20i.e.%2C%20whether%20parts%0Aof%20the%20system%20such%20as%20the%20LLM%20and%20type%20of%20agent%20are%20interchangeable%2C%20a%20topic%0Athat%20has%20received%20limited%20attention%20in%20drug%20discovery.%20We%20compare%20the%0Aperformance%20of%20different%20LLMs%20and%20the%20effectiveness%20of%20tool-calling%20agents%0Aversus%20code-generating%20agents.%20Our%20case%20study%2C%20comparing%20performance%20in%0Aorchestrating%20tools%20for%20chemistry%20and%20drug%20discovery%20using%20an%20LLM-as-a-judge%0Ascore%2C%20shows%20that%20Claude-3.5-Sonnet%2C%20Claude-3.7-Sonnet%20and%20GPT-4o%20outperform%0Aalternative%20language%20models%20such%20as%20Llama-3.1-8B%2C%20Llama-3.1-70B%2C%20GPT-3.5-Turbo%2C%0Aand%20Nova-Micro.%20Although%20we%20confirm%20that%20code-generating%20agents%20outperform%20the%0Atool-calling%20ones%20on%20average%2C%20we%20show%20that%20this%20is%20highly%20question-%20and%0Amodel-dependent.%20Furthermore%2C%20the%20impact%20of%20replacing%20system%20prompts%20is%0Adependent%20on%20the%20question%20and%20model%2C%20underscoring%20that%20even%20in%20this%20particular%0Adomain%20one%20cannot%20just%20replace%20components%20of%20the%20system%20without%20re-engineering.%0AOur%20study%20highlights%20the%20necessity%20of%20further%20research%20into%20the%20modularity%20of%0Aagentic%20systems%20to%20enable%20the%20development%20of%20reliable%20and%20modular%20solutions%20for%0Areal-world%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.22189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Modularity%2520of%2520Agentic%2520Systems%2520for%2520Drug%2520Discovery%26entry.906535625%3DLaura%2520van%2520Weesep%2520and%2520Samuel%2520Genheden%2520and%2520Ola%2520Engkvist%2520and%2520Jens%2520Sj%25C3%25B6lund%26entry.1292438233%3D%2520%2520Large-language%2520models%2520%2528LLMs%2529%2520and%2520agentic%2520systems%2520present%2520exciting%250Aopportunities%2520to%2520accelerate%2520drug%2520discovery.%2520In%2520this%2520study%252C%2520we%2520examine%2520the%250Amodularity%2520of%2520LLM-based%2520agentic%2520systems%2520for%2520drug%2520discovery%252C%2520i.e.%252C%2520whether%2520parts%250Aof%2520the%2520system%2520such%2520as%2520the%2520LLM%2520and%2520type%2520of%2520agent%2520are%2520interchangeable%252C%2520a%2520topic%250Athat%2520has%2520received%2520limited%2520attention%2520in%2520drug%2520discovery.%2520We%2520compare%2520the%250Aperformance%2520of%2520different%2520LLMs%2520and%2520the%2520effectiveness%2520of%2520tool-calling%2520agents%250Aversus%2520code-generating%2520agents.%2520Our%2520case%2520study%252C%2520comparing%2520performance%2520in%250Aorchestrating%2520tools%2520for%2520chemistry%2520and%2520drug%2520discovery%2520using%2520an%2520LLM-as-a-judge%250Ascore%252C%2520shows%2520that%2520Claude-3.5-Sonnet%252C%2520Claude-3.7-Sonnet%2520and%2520GPT-4o%2520outperform%250Aalternative%2520language%2520models%2520such%2520as%2520Llama-3.1-8B%252C%2520Llama-3.1-70B%252C%2520GPT-3.5-Turbo%252C%250Aand%2520Nova-Micro.%2520Although%2520we%2520confirm%2520that%2520code-generating%2520agents%2520outperform%2520the%250Atool-calling%2520ones%2520on%2520average%252C%2520we%2520show%2520that%2520this%2520is%2520highly%2520question-%2520and%250Amodel-dependent.%2520Furthermore%252C%2520the%2520impact%2520of%2520replacing%2520system%2520prompts%2520is%250Adependent%2520on%2520the%2520question%2520and%2520model%252C%2520underscoring%2520that%2520even%2520in%2520this%2520particular%250Adomain%2520one%2520cannot%2520just%2520replace%2520components%2520of%2520the%2520system%2520without%2520re-engineering.%250AOur%2520study%2520highlights%2520the%2520necessity%2520of%2520further%2520research%2520into%2520the%2520modularity%2520of%250Aagentic%2520systems%2520to%2520enable%2520the%2520development%2520of%2520reliable%2520and%2520modular%2520solutions%2520for%250Areal-world%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.22189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Modularity%20of%20Agentic%20Systems%20for%20Drug%20Discovery&entry.906535625=Laura%20van%20Weesep%20and%20Samuel%20Genheden%20and%20Ola%20Engkvist%20and%20Jens%20Sj%C3%B6lund&entry.1292438233=%20%20Large-language%20models%20%28LLMs%29%20and%20agentic%20systems%20present%20exciting%0Aopportunities%20to%20accelerate%20drug%20discovery.%20In%20this%20study%2C%20we%20examine%20the%0Amodularity%20of%20LLM-based%20agentic%20systems%20for%20drug%20discovery%2C%20i.e.%2C%20whether%20parts%0Aof%20the%20system%20such%20as%20the%20LLM%20and%20type%20of%20agent%20are%20interchangeable%2C%20a%20topic%0Athat%20has%20received%20limited%20attention%20in%20drug%20discovery.%20We%20compare%20the%0Aperformance%20of%20different%20LLMs%20and%20the%20effectiveness%20of%20tool-calling%20agents%0Aversus%20code-generating%20agents.%20Our%20case%20study%2C%20comparing%20performance%20in%0Aorchestrating%20tools%20for%20chemistry%20and%20drug%20discovery%20using%20an%20LLM-as-a-judge%0Ascore%2C%20shows%20that%20Claude-3.5-Sonnet%2C%20Claude-3.7-Sonnet%20and%20GPT-4o%20outperform%0Aalternative%20language%20models%20such%20as%20Llama-3.1-8B%2C%20Llama-3.1-70B%2C%20GPT-3.5-Turbo%2C%0Aand%20Nova-Micro.%20Although%20we%20confirm%20that%20code-generating%20agents%20outperform%20the%0Atool-calling%20ones%20on%20average%2C%20we%20show%20that%20this%20is%20highly%20question-%20and%0Amodel-dependent.%20Furthermore%2C%20the%20impact%20of%20replacing%20system%20prompts%20is%0Adependent%20on%20the%20question%20and%20model%2C%20underscoring%20that%20even%20in%20this%20particular%0Adomain%20one%20cannot%20just%20replace%20components%20of%20the%20system%20without%20re-engineering.%0AOur%20study%20highlights%20the%20necessity%20of%20further%20research%20into%20the%20modularity%20of%0Aagentic%20systems%20to%20enable%20the%20development%20of%20reliable%20and%20modular%20solutions%20for%0Areal-world%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.22189v2&entry.124074799=Read"},
{"title": "Investigation of D-Wave quantum annealing for training Restricted\n  Boltzmann Machines and mitigating catastrophic forgetting", "author": "Abdelmoula El-Yazizi and Yaroslav Koshka", "abstract": "  Modest statistical differences between the sampling performances of the\nD-Wave quantum annealer (QA) and the classical Markov Chain Monte Carlo (MCMC),\nwhen applied to Restricted Boltzmann Machines (RBMs), are explored to explain,\nand possibly address, the absence of significant and consistent improvements in\nRBM trainability when the D-Wave sampling was used in previous investigations.\nA novel hybrid sampling approach, combining the classical and the QA\ncontributions, is investigated as a promising way to benefit from the modest\ndifferences between the two sampling methods. No improvements in the RBM\ntraining are achieved in this work, thereby suggesting that the differences\nbetween the QA-based and MCMC sampling, mainly found in the medium-to-low\nprobability regions of the distribution, which are less important for the\nquality of the sample, are insufficient to benefit the training. Difficulties\nin achieving sufficiently high quality of embedding RBMs into the lattice of\nthe newer generation of D-Wave hardware could be further complicating the task.\nOn the other hand, the ability to generate samples of sufficient variety from\nlower-probability parts of the distribution has a potential to benefit other\nmachine learning applications, such as the mitigation of catastrophic\nforgetting (CF) during incremental learning. The feasibility of using\nQA-generated patterns of desirable classes for CF mitigation by the generative\nreplay is demonstrated in this work for the first time. While the efficiency of\nthe CF mitigation using the D-Wave QA was comparable to that of the classical\nmitigation, both the speed of generating a large number of distinct desirable\npatterns and the potential for further improvement make this approach promising\nfor a variety of challenging machine learning applications.\n", "link": "http://arxiv.org/abs/2508.15697v1", "date": "2025-08-21", "relevancy": 1.9459, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4914}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4839}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigation%20of%20D-Wave%20quantum%20annealing%20for%20training%20Restricted%0A%20%20Boltzmann%20Machines%20and%20mitigating%20catastrophic%20forgetting&body=Title%3A%20Investigation%20of%20D-Wave%20quantum%20annealing%20for%20training%20Restricted%0A%20%20Boltzmann%20Machines%20and%20mitigating%20catastrophic%20forgetting%0AAuthor%3A%20Abdelmoula%20El-Yazizi%20and%20Yaroslav%20Koshka%0AAbstract%3A%20%20%20Modest%20statistical%20differences%20between%20the%20sampling%20performances%20of%20the%0AD-Wave%20quantum%20annealer%20%28QA%29%20and%20the%20classical%20Markov%20Chain%20Monte%20Carlo%20%28MCMC%29%2C%0Awhen%20applied%20to%20Restricted%20Boltzmann%20Machines%20%28RBMs%29%2C%20are%20explored%20to%20explain%2C%0Aand%20possibly%20address%2C%20the%20absence%20of%20significant%20and%20consistent%20improvements%20in%0ARBM%20trainability%20when%20the%20D-Wave%20sampling%20was%20used%20in%20previous%20investigations.%0AA%20novel%20hybrid%20sampling%20approach%2C%20combining%20the%20classical%20and%20the%20QA%0Acontributions%2C%20is%20investigated%20as%20a%20promising%20way%20to%20benefit%20from%20the%20modest%0Adifferences%20between%20the%20two%20sampling%20methods.%20No%20improvements%20in%20the%20RBM%0Atraining%20are%20achieved%20in%20this%20work%2C%20thereby%20suggesting%20that%20the%20differences%0Abetween%20the%20QA-based%20and%20MCMC%20sampling%2C%20mainly%20found%20in%20the%20medium-to-low%0Aprobability%20regions%20of%20the%20distribution%2C%20which%20are%20less%20important%20for%20the%0Aquality%20of%20the%20sample%2C%20are%20insufficient%20to%20benefit%20the%20training.%20Difficulties%0Ain%20achieving%20sufficiently%20high%20quality%20of%20embedding%20RBMs%20into%20the%20lattice%20of%0Athe%20newer%20generation%20of%20D-Wave%20hardware%20could%20be%20further%20complicating%20the%20task.%0AOn%20the%20other%20hand%2C%20the%20ability%20to%20generate%20samples%20of%20sufficient%20variety%20from%0Alower-probability%20parts%20of%20the%20distribution%20has%20a%20potential%20to%20benefit%20other%0Amachine%20learning%20applications%2C%20such%20as%20the%20mitigation%20of%20catastrophic%0Aforgetting%20%28CF%29%20during%20incremental%20learning.%20The%20feasibility%20of%20using%0AQA-generated%20patterns%20of%20desirable%20classes%20for%20CF%20mitigation%20by%20the%20generative%0Areplay%20is%20demonstrated%20in%20this%20work%20for%20the%20first%20time.%20While%20the%20efficiency%20of%0Athe%20CF%20mitigation%20using%20the%20D-Wave%20QA%20was%20comparable%20to%20that%20of%20the%20classical%0Amitigation%2C%20both%20the%20speed%20of%20generating%20a%20large%20number%20of%20distinct%20desirable%0Apatterns%20and%20the%20potential%20for%20further%20improvement%20make%20this%20approach%20promising%0Afor%20a%20variety%20of%20challenging%20machine%20learning%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigation%2520of%2520D-Wave%2520quantum%2520annealing%2520for%2520training%2520Restricted%250A%2520%2520Boltzmann%2520Machines%2520and%2520mitigating%2520catastrophic%2520forgetting%26entry.906535625%3DAbdelmoula%2520El-Yazizi%2520and%2520Yaroslav%2520Koshka%26entry.1292438233%3D%2520%2520Modest%2520statistical%2520differences%2520between%2520the%2520sampling%2520performances%2520of%2520the%250AD-Wave%2520quantum%2520annealer%2520%2528QA%2529%2520and%2520the%2520classical%2520Markov%2520Chain%2520Monte%2520Carlo%2520%2528MCMC%2529%252C%250Awhen%2520applied%2520to%2520Restricted%2520Boltzmann%2520Machines%2520%2528RBMs%2529%252C%2520are%2520explored%2520to%2520explain%252C%250Aand%2520possibly%2520address%252C%2520the%2520absence%2520of%2520significant%2520and%2520consistent%2520improvements%2520in%250ARBM%2520trainability%2520when%2520the%2520D-Wave%2520sampling%2520was%2520used%2520in%2520previous%2520investigations.%250AA%2520novel%2520hybrid%2520sampling%2520approach%252C%2520combining%2520the%2520classical%2520and%2520the%2520QA%250Acontributions%252C%2520is%2520investigated%2520as%2520a%2520promising%2520way%2520to%2520benefit%2520from%2520the%2520modest%250Adifferences%2520between%2520the%2520two%2520sampling%2520methods.%2520No%2520improvements%2520in%2520the%2520RBM%250Atraining%2520are%2520achieved%2520in%2520this%2520work%252C%2520thereby%2520suggesting%2520that%2520the%2520differences%250Abetween%2520the%2520QA-based%2520and%2520MCMC%2520sampling%252C%2520mainly%2520found%2520in%2520the%2520medium-to-low%250Aprobability%2520regions%2520of%2520the%2520distribution%252C%2520which%2520are%2520less%2520important%2520for%2520the%250Aquality%2520of%2520the%2520sample%252C%2520are%2520insufficient%2520to%2520benefit%2520the%2520training.%2520Difficulties%250Ain%2520achieving%2520sufficiently%2520high%2520quality%2520of%2520embedding%2520RBMs%2520into%2520the%2520lattice%2520of%250Athe%2520newer%2520generation%2520of%2520D-Wave%2520hardware%2520could%2520be%2520further%2520complicating%2520the%2520task.%250AOn%2520the%2520other%2520hand%252C%2520the%2520ability%2520to%2520generate%2520samples%2520of%2520sufficient%2520variety%2520from%250Alower-probability%2520parts%2520of%2520the%2520distribution%2520has%2520a%2520potential%2520to%2520benefit%2520other%250Amachine%2520learning%2520applications%252C%2520such%2520as%2520the%2520mitigation%2520of%2520catastrophic%250Aforgetting%2520%2528CF%2529%2520during%2520incremental%2520learning.%2520The%2520feasibility%2520of%2520using%250AQA-generated%2520patterns%2520of%2520desirable%2520classes%2520for%2520CF%2520mitigation%2520by%2520the%2520generative%250Areplay%2520is%2520demonstrated%2520in%2520this%2520work%2520for%2520the%2520first%2520time.%2520While%2520the%2520efficiency%2520of%250Athe%2520CF%2520mitigation%2520using%2520the%2520D-Wave%2520QA%2520was%2520comparable%2520to%2520that%2520of%2520the%2520classical%250Amitigation%252C%2520both%2520the%2520speed%2520of%2520generating%2520a%2520large%2520number%2520of%2520distinct%2520desirable%250Apatterns%2520and%2520the%2520potential%2520for%2520further%2520improvement%2520make%2520this%2520approach%2520promising%250Afor%2520a%2520variety%2520of%2520challenging%2520machine%2520learning%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigation%20of%20D-Wave%20quantum%20annealing%20for%20training%20Restricted%0A%20%20Boltzmann%20Machines%20and%20mitigating%20catastrophic%20forgetting&entry.906535625=Abdelmoula%20El-Yazizi%20and%20Yaroslav%20Koshka&entry.1292438233=%20%20Modest%20statistical%20differences%20between%20the%20sampling%20performances%20of%20the%0AD-Wave%20quantum%20annealer%20%28QA%29%20and%20the%20classical%20Markov%20Chain%20Monte%20Carlo%20%28MCMC%29%2C%0Awhen%20applied%20to%20Restricted%20Boltzmann%20Machines%20%28RBMs%29%2C%20are%20explored%20to%20explain%2C%0Aand%20possibly%20address%2C%20the%20absence%20of%20significant%20and%20consistent%20improvements%20in%0ARBM%20trainability%20when%20the%20D-Wave%20sampling%20was%20used%20in%20previous%20investigations.%0AA%20novel%20hybrid%20sampling%20approach%2C%20combining%20the%20classical%20and%20the%20QA%0Acontributions%2C%20is%20investigated%20as%20a%20promising%20way%20to%20benefit%20from%20the%20modest%0Adifferences%20between%20the%20two%20sampling%20methods.%20No%20improvements%20in%20the%20RBM%0Atraining%20are%20achieved%20in%20this%20work%2C%20thereby%20suggesting%20that%20the%20differences%0Abetween%20the%20QA-based%20and%20MCMC%20sampling%2C%20mainly%20found%20in%20the%20medium-to-low%0Aprobability%20regions%20of%20the%20distribution%2C%20which%20are%20less%20important%20for%20the%0Aquality%20of%20the%20sample%2C%20are%20insufficient%20to%20benefit%20the%20training.%20Difficulties%0Ain%20achieving%20sufficiently%20high%20quality%20of%20embedding%20RBMs%20into%20the%20lattice%20of%0Athe%20newer%20generation%20of%20D-Wave%20hardware%20could%20be%20further%20complicating%20the%20task.%0AOn%20the%20other%20hand%2C%20the%20ability%20to%20generate%20samples%20of%20sufficient%20variety%20from%0Alower-probability%20parts%20of%20the%20distribution%20has%20a%20potential%20to%20benefit%20other%0Amachine%20learning%20applications%2C%20such%20as%20the%20mitigation%20of%20catastrophic%0Aforgetting%20%28CF%29%20during%20incremental%20learning.%20The%20feasibility%20of%20using%0AQA-generated%20patterns%20of%20desirable%20classes%20for%20CF%20mitigation%20by%20the%20generative%0Areplay%20is%20demonstrated%20in%20this%20work%20for%20the%20first%20time.%20While%20the%20efficiency%20of%0Athe%20CF%20mitigation%20using%20the%20D-Wave%20QA%20was%20comparable%20to%20that%20of%20the%20classical%0Amitigation%2C%20both%20the%20speed%20of%20generating%20a%20large%20number%20of%20distinct%20desirable%0Apatterns%20and%20the%20potential%20for%20further%20improvement%20make%20this%20approach%20promising%0Afor%20a%20variety%20of%20challenging%20machine%20learning%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15697v1&entry.124074799=Read"},
{"title": "RefineCoder: Iterative Improving of Large Language Models via Adaptive\n  Critique Refinement for Code Generation", "author": "Changzhi Zhou and Xinyu Zhang and Dandan Song and Xiancai Chen and Wanli Gu and Huipeng Ma and Yuhang Tian and Mengdi Zhang and Linmei Hu", "abstract": "  Code generation has attracted increasing attention with the rise of Large\nLanguage Models (LLMs). Many studies have developed powerful code LLMs by\nsynthesizing code-related instruction data and applying supervised fine-tuning.\nHowever, these methods are limited by teacher model distillation and ignore the\npotential of iterative refinement by self-generated code. In this paper, we\npropose Adaptive Critique Refinement (ACR), which enables the model to refine\nitself by self-generated code and external critique, rather than directly\nimitating the code responses of the teacher model. Concretely, ACR includes a\ncomposite scoring system with LLM-as-a-Judge to evaluate the quality of code\nresponses and a selective critique strategy with LLM-as-a-Critic to critique\nself-generated low-quality code responses. We develop the RefineCoder series by\niteratively applying ACR, achieving continuous performance improvement on\nmultiple code generation benchmarks. Compared to the baselines of the same\nsize, our proposed RefineCoder series can achieve comparable or even superior\nperformance using less data.\n", "link": "http://arxiv.org/abs/2502.09183v2", "date": "2025-08-21", "relevancy": 1.9456, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4868}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4863}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RefineCoder%3A%20Iterative%20Improving%20of%20Large%20Language%20Models%20via%20Adaptive%0A%20%20Critique%20Refinement%20for%20Code%20Generation&body=Title%3A%20RefineCoder%3A%20Iterative%20Improving%20of%20Large%20Language%20Models%20via%20Adaptive%0A%20%20Critique%20Refinement%20for%20Code%20Generation%0AAuthor%3A%20Changzhi%20Zhou%20and%20Xinyu%20Zhang%20and%20Dandan%20Song%20and%20Xiancai%20Chen%20and%20Wanli%20Gu%20and%20Huipeng%20Ma%20and%20Yuhang%20Tian%20and%20Mengdi%20Zhang%20and%20Linmei%20Hu%0AAbstract%3A%20%20%20Code%20generation%20has%20attracted%20increasing%20attention%20with%20the%20rise%20of%20Large%0ALanguage%20Models%20%28LLMs%29.%20Many%20studies%20have%20developed%20powerful%20code%20LLMs%20by%0Asynthesizing%20code-related%20instruction%20data%20and%20applying%20supervised%20fine-tuning.%0AHowever%2C%20these%20methods%20are%20limited%20by%20teacher%20model%20distillation%20and%20ignore%20the%0Apotential%20of%20iterative%20refinement%20by%20self-generated%20code.%20In%20this%20paper%2C%20we%0Apropose%20Adaptive%20Critique%20Refinement%20%28ACR%29%2C%20which%20enables%20the%20model%20to%20refine%0Aitself%20by%20self-generated%20code%20and%20external%20critique%2C%20rather%20than%20directly%0Aimitating%20the%20code%20responses%20of%20the%20teacher%20model.%20Concretely%2C%20ACR%20includes%20a%0Acomposite%20scoring%20system%20with%20LLM-as-a-Judge%20to%20evaluate%20the%20quality%20of%20code%0Aresponses%20and%20a%20selective%20critique%20strategy%20with%20LLM-as-a-Critic%20to%20critique%0Aself-generated%20low-quality%20code%20responses.%20We%20develop%20the%20RefineCoder%20series%20by%0Aiteratively%20applying%20ACR%2C%20achieving%20continuous%20performance%20improvement%20on%0Amultiple%20code%20generation%20benchmarks.%20Compared%20to%20the%20baselines%20of%20the%20same%0Asize%2C%20our%20proposed%20RefineCoder%20series%20can%20achieve%20comparable%20or%20even%20superior%0Aperformance%20using%20less%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09183v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefineCoder%253A%2520Iterative%2520Improving%2520of%2520Large%2520Language%2520Models%2520via%2520Adaptive%250A%2520%2520Critique%2520Refinement%2520for%2520Code%2520Generation%26entry.906535625%3DChangzhi%2520Zhou%2520and%2520Xinyu%2520Zhang%2520and%2520Dandan%2520Song%2520and%2520Xiancai%2520Chen%2520and%2520Wanli%2520Gu%2520and%2520Huipeng%2520Ma%2520and%2520Yuhang%2520Tian%2520and%2520Mengdi%2520Zhang%2520and%2520Linmei%2520Hu%26entry.1292438233%3D%2520%2520Code%2520generation%2520has%2520attracted%2520increasing%2520attention%2520with%2520the%2520rise%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529.%2520Many%2520studies%2520have%2520developed%2520powerful%2520code%2520LLMs%2520by%250Asynthesizing%2520code-related%2520instruction%2520data%2520and%2520applying%2520supervised%2520fine-tuning.%250AHowever%252C%2520these%2520methods%2520are%2520limited%2520by%2520teacher%2520model%2520distillation%2520and%2520ignore%2520the%250Apotential%2520of%2520iterative%2520refinement%2520by%2520self-generated%2520code.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Adaptive%2520Critique%2520Refinement%2520%2528ACR%2529%252C%2520which%2520enables%2520the%2520model%2520to%2520refine%250Aitself%2520by%2520self-generated%2520code%2520and%2520external%2520critique%252C%2520rather%2520than%2520directly%250Aimitating%2520the%2520code%2520responses%2520of%2520the%2520teacher%2520model.%2520Concretely%252C%2520ACR%2520includes%2520a%250Acomposite%2520scoring%2520system%2520with%2520LLM-as-a-Judge%2520to%2520evaluate%2520the%2520quality%2520of%2520code%250Aresponses%2520and%2520a%2520selective%2520critique%2520strategy%2520with%2520LLM-as-a-Critic%2520to%2520critique%250Aself-generated%2520low-quality%2520code%2520responses.%2520We%2520develop%2520the%2520RefineCoder%2520series%2520by%250Aiteratively%2520applying%2520ACR%252C%2520achieving%2520continuous%2520performance%2520improvement%2520on%250Amultiple%2520code%2520generation%2520benchmarks.%2520Compared%2520to%2520the%2520baselines%2520of%2520the%2520same%250Asize%252C%2520our%2520proposed%2520RefineCoder%2520series%2520can%2520achieve%2520comparable%2520or%2520even%2520superior%250Aperformance%2520using%2520less%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09183v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RefineCoder%3A%20Iterative%20Improving%20of%20Large%20Language%20Models%20via%20Adaptive%0A%20%20Critique%20Refinement%20for%20Code%20Generation&entry.906535625=Changzhi%20Zhou%20and%20Xinyu%20Zhang%20and%20Dandan%20Song%20and%20Xiancai%20Chen%20and%20Wanli%20Gu%20and%20Huipeng%20Ma%20and%20Yuhang%20Tian%20and%20Mengdi%20Zhang%20and%20Linmei%20Hu&entry.1292438233=%20%20Code%20generation%20has%20attracted%20increasing%20attention%20with%20the%20rise%20of%20Large%0ALanguage%20Models%20%28LLMs%29.%20Many%20studies%20have%20developed%20powerful%20code%20LLMs%20by%0Asynthesizing%20code-related%20instruction%20data%20and%20applying%20supervised%20fine-tuning.%0AHowever%2C%20these%20methods%20are%20limited%20by%20teacher%20model%20distillation%20and%20ignore%20the%0Apotential%20of%20iterative%20refinement%20by%20self-generated%20code.%20In%20this%20paper%2C%20we%0Apropose%20Adaptive%20Critique%20Refinement%20%28ACR%29%2C%20which%20enables%20the%20model%20to%20refine%0Aitself%20by%20self-generated%20code%20and%20external%20critique%2C%20rather%20than%20directly%0Aimitating%20the%20code%20responses%20of%20the%20teacher%20model.%20Concretely%2C%20ACR%20includes%20a%0Acomposite%20scoring%20system%20with%20LLM-as-a-Judge%20to%20evaluate%20the%20quality%20of%20code%0Aresponses%20and%20a%20selective%20critique%20strategy%20with%20LLM-as-a-Critic%20to%20critique%0Aself-generated%20low-quality%20code%20responses.%20We%20develop%20the%20RefineCoder%20series%20by%0Aiteratively%20applying%20ACR%2C%20achieving%20continuous%20performance%20improvement%20on%0Amultiple%20code%20generation%20benchmarks.%20Compared%20to%20the%20baselines%20of%20the%20same%0Asize%2C%20our%20proposed%20RefineCoder%20series%20can%20achieve%20comparable%20or%20even%20superior%0Aperformance%20using%20less%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09183v2&entry.124074799=Read"},
{"title": "Language-Guided Tuning: Enhancing Numeric Optimization with Textual\n  Feedback", "author": "Yuxing Lu and Yucheng Hu and Nan Sun and Xukai Zhao", "abstract": "  Configuration optimization remains a critical bottleneck in machine learning,\nrequiring coordinated tuning across model architecture, training strategy,\nfeature engineering, and hyperparameters. Traditional approaches treat these\ndimensions independently and lack interpretability, while recent automated\nmethods struggle with dynamic adaptability and semantic reasoning about\noptimization decisions. We introduce Language-Guided Tuning (LGT), a novel\nframework that employs multi-agent Large Language Models to intelligently\noptimize configurations through natural language reasoning. We apply textual\ngradients - qualitative feedback signals that complement numerical optimization\nby providing semantic understanding of training dynamics and configuration\ninterdependencies. LGT coordinates three specialized agents: an Advisor that\nproposes configuration changes, an Evaluator that assesses progress, and an\nOptimizer that refines the decision-making process, creating a self-improving\nfeedback loop. Through comprehensive evaluation on six diverse datasets, LGT\ndemonstrates substantial improvements over traditional optimization methods,\nachieving performance gains while maintaining high interpretability.\n", "link": "http://arxiv.org/abs/2508.15757v1", "date": "2025-08-21", "relevancy": 1.9452, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4871}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4859}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Guided%20Tuning%3A%20Enhancing%20Numeric%20Optimization%20with%20Textual%0A%20%20Feedback&body=Title%3A%20Language-Guided%20Tuning%3A%20Enhancing%20Numeric%20Optimization%20with%20Textual%0A%20%20Feedback%0AAuthor%3A%20Yuxing%20Lu%20and%20Yucheng%20Hu%20and%20Nan%20Sun%20and%20Xukai%20Zhao%0AAbstract%3A%20%20%20Configuration%20optimization%20remains%20a%20critical%20bottleneck%20in%20machine%20learning%2C%0Arequiring%20coordinated%20tuning%20across%20model%20architecture%2C%20training%20strategy%2C%0Afeature%20engineering%2C%20and%20hyperparameters.%20Traditional%20approaches%20treat%20these%0Adimensions%20independently%20and%20lack%20interpretability%2C%20while%20recent%20automated%0Amethods%20struggle%20with%20dynamic%20adaptability%20and%20semantic%20reasoning%20about%0Aoptimization%20decisions.%20We%20introduce%20Language-Guided%20Tuning%20%28LGT%29%2C%20a%20novel%0Aframework%20that%20employs%20multi-agent%20Large%20Language%20Models%20to%20intelligently%0Aoptimize%20configurations%20through%20natural%20language%20reasoning.%20We%20apply%20textual%0Agradients%20-%20qualitative%20feedback%20signals%20that%20complement%20numerical%20optimization%0Aby%20providing%20semantic%20understanding%20of%20training%20dynamics%20and%20configuration%0Ainterdependencies.%20LGT%20coordinates%20three%20specialized%20agents%3A%20an%20Advisor%20that%0Aproposes%20configuration%20changes%2C%20an%20Evaluator%20that%20assesses%20progress%2C%20and%20an%0AOptimizer%20that%20refines%20the%20decision-making%20process%2C%20creating%20a%20self-improving%0Afeedback%20loop.%20Through%20comprehensive%20evaluation%20on%20six%20diverse%20datasets%2C%20LGT%0Ademonstrates%20substantial%20improvements%20over%20traditional%20optimization%20methods%2C%0Aachieving%20performance%20gains%20while%20maintaining%20high%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Guided%2520Tuning%253A%2520Enhancing%2520Numeric%2520Optimization%2520with%2520Textual%250A%2520%2520Feedback%26entry.906535625%3DYuxing%2520Lu%2520and%2520Yucheng%2520Hu%2520and%2520Nan%2520Sun%2520and%2520Xukai%2520Zhao%26entry.1292438233%3D%2520%2520Configuration%2520optimization%2520remains%2520a%2520critical%2520bottleneck%2520in%2520machine%2520learning%252C%250Arequiring%2520coordinated%2520tuning%2520across%2520model%2520architecture%252C%2520training%2520strategy%252C%250Afeature%2520engineering%252C%2520and%2520hyperparameters.%2520Traditional%2520approaches%2520treat%2520these%250Adimensions%2520independently%2520and%2520lack%2520interpretability%252C%2520while%2520recent%2520automated%250Amethods%2520struggle%2520with%2520dynamic%2520adaptability%2520and%2520semantic%2520reasoning%2520about%250Aoptimization%2520decisions.%2520We%2520introduce%2520Language-Guided%2520Tuning%2520%2528LGT%2529%252C%2520a%2520novel%250Aframework%2520that%2520employs%2520multi-agent%2520Large%2520Language%2520Models%2520to%2520intelligently%250Aoptimize%2520configurations%2520through%2520natural%2520language%2520reasoning.%2520We%2520apply%2520textual%250Agradients%2520-%2520qualitative%2520feedback%2520signals%2520that%2520complement%2520numerical%2520optimization%250Aby%2520providing%2520semantic%2520understanding%2520of%2520training%2520dynamics%2520and%2520configuration%250Ainterdependencies.%2520LGT%2520coordinates%2520three%2520specialized%2520agents%253A%2520an%2520Advisor%2520that%250Aproposes%2520configuration%2520changes%252C%2520an%2520Evaluator%2520that%2520assesses%2520progress%252C%2520and%2520an%250AOptimizer%2520that%2520refines%2520the%2520decision-making%2520process%252C%2520creating%2520a%2520self-improving%250Afeedback%2520loop.%2520Through%2520comprehensive%2520evaluation%2520on%2520six%2520diverse%2520datasets%252C%2520LGT%250Ademonstrates%2520substantial%2520improvements%2520over%2520traditional%2520optimization%2520methods%252C%250Aachieving%2520performance%2520gains%2520while%2520maintaining%2520high%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Guided%20Tuning%3A%20Enhancing%20Numeric%20Optimization%20with%20Textual%0A%20%20Feedback&entry.906535625=Yuxing%20Lu%20and%20Yucheng%20Hu%20and%20Nan%20Sun%20and%20Xukai%20Zhao&entry.1292438233=%20%20Configuration%20optimization%20remains%20a%20critical%20bottleneck%20in%20machine%20learning%2C%0Arequiring%20coordinated%20tuning%20across%20model%20architecture%2C%20training%20strategy%2C%0Afeature%20engineering%2C%20and%20hyperparameters.%20Traditional%20approaches%20treat%20these%0Adimensions%20independently%20and%20lack%20interpretability%2C%20while%20recent%20automated%0Amethods%20struggle%20with%20dynamic%20adaptability%20and%20semantic%20reasoning%20about%0Aoptimization%20decisions.%20We%20introduce%20Language-Guided%20Tuning%20%28LGT%29%2C%20a%20novel%0Aframework%20that%20employs%20multi-agent%20Large%20Language%20Models%20to%20intelligently%0Aoptimize%20configurations%20through%20natural%20language%20reasoning.%20We%20apply%20textual%0Agradients%20-%20qualitative%20feedback%20signals%20that%20complement%20numerical%20optimization%0Aby%20providing%20semantic%20understanding%20of%20training%20dynamics%20and%20configuration%0Ainterdependencies.%20LGT%20coordinates%20three%20specialized%20agents%3A%20an%20Advisor%20that%0Aproposes%20configuration%20changes%2C%20an%20Evaluator%20that%20assesses%20progress%2C%20and%20an%0AOptimizer%20that%20refines%20the%20decision-making%20process%2C%20creating%20a%20self-improving%0Afeedback%20loop.%20Through%20comprehensive%20evaluation%20on%20six%20diverse%20datasets%2C%20LGT%0Ademonstrates%20substantial%20improvements%20over%20traditional%20optimization%20methods%2C%0Aachieving%20performance%20gains%20while%20maintaining%20high%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15757v1&entry.124074799=Read"},
{"title": "Learning Protein-Ligand Binding in Hyperbolic Space", "author": "Jianhui Wang and Wenyu Zhu and Bowen Gao and Xin Hong and Ya-Qin Zhang and Wei-Ying Ma and Yanyan Lan", "abstract": "  Protein-ligand binding prediction is central to virtual screening and\naffinity ranking, two fundamental tasks in drug discovery. While recent\nretrieval-based methods embed ligands and protein pockets into Euclidean space\nfor similarity-based search, the geometry of Euclidean embeddings often fails\nto capture the hierarchical structure and fine-grained affinity variations\nintrinsic to molecular interactions. In this work, we propose HypSeek, a\nhyperbolic representation learning framework that embeds ligands, protein\npockets, and sequences into Lorentz-model hyperbolic space. By leveraging the\nexponential geometry and negative curvature of hyperbolic space, HypSeek\nenables expressive, affinity-sensitive embeddings that can effectively model\nboth global activity and subtle functional differences-particularly in\nchallenging cases such as activity cliffs, where structurally similar ligands\nexhibit large affinity gaps. Our mode unifies virtual screening and affinity\nranking in a single framework, introducing a protein-guided three-tower\narchitecture to enhance representational structure. HypSeek improves early\nenrichment in virtual screening on DUD-E from 42.63 to 51.44 (+20.7%) and\naffinity ranking correlation on JACS from 0.5774 to 0.7239 (+25.4%),\ndemonstrating the benefits of hyperbolic geometry across both tasks and\nhighlighting its potential as a powerful inductive bias for protein-ligand\nmodeling.\n", "link": "http://arxiv.org/abs/2508.15480v1", "date": "2025-08-21", "relevancy": 1.933, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4897}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.482}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Protein-Ligand%20Binding%20in%20Hyperbolic%20Space&body=Title%3A%20Learning%20Protein-Ligand%20Binding%20in%20Hyperbolic%20Space%0AAuthor%3A%20Jianhui%20Wang%20and%20Wenyu%20Zhu%20and%20Bowen%20Gao%20and%20Xin%20Hong%20and%20Ya-Qin%20Zhang%20and%20Wei-Ying%20Ma%20and%20Yanyan%20Lan%0AAbstract%3A%20%20%20Protein-ligand%20binding%20prediction%20is%20central%20to%20virtual%20screening%20and%0Aaffinity%20ranking%2C%20two%20fundamental%20tasks%20in%20drug%20discovery.%20While%20recent%0Aretrieval-based%20methods%20embed%20ligands%20and%20protein%20pockets%20into%20Euclidean%20space%0Afor%20similarity-based%20search%2C%20the%20geometry%20of%20Euclidean%20embeddings%20often%20fails%0Ato%20capture%20the%20hierarchical%20structure%20and%20fine-grained%20affinity%20variations%0Aintrinsic%20to%20molecular%20interactions.%20In%20this%20work%2C%20we%20propose%20HypSeek%2C%20a%0Ahyperbolic%20representation%20learning%20framework%20that%20embeds%20ligands%2C%20protein%0Apockets%2C%20and%20sequences%20into%20Lorentz-model%20hyperbolic%20space.%20By%20leveraging%20the%0Aexponential%20geometry%20and%20negative%20curvature%20of%20hyperbolic%20space%2C%20HypSeek%0Aenables%20expressive%2C%20affinity-sensitive%20embeddings%20that%20can%20effectively%20model%0Aboth%20global%20activity%20and%20subtle%20functional%20differences-particularly%20in%0Achallenging%20cases%20such%20as%20activity%20cliffs%2C%20where%20structurally%20similar%20ligands%0Aexhibit%20large%20affinity%20gaps.%20Our%20mode%20unifies%20virtual%20screening%20and%20affinity%0Aranking%20in%20a%20single%20framework%2C%20introducing%20a%20protein-guided%20three-tower%0Aarchitecture%20to%20enhance%20representational%20structure.%20HypSeek%20improves%20early%0Aenrichment%20in%20virtual%20screening%20on%20DUD-E%20from%2042.63%20to%2051.44%20%28%2B20.7%25%29%20and%0Aaffinity%20ranking%20correlation%20on%20JACS%20from%200.5774%20to%200.7239%20%28%2B25.4%25%29%2C%0Ademonstrating%20the%20benefits%20of%20hyperbolic%20geometry%20across%20both%20tasks%20and%0Ahighlighting%20its%20potential%20as%20a%20powerful%20inductive%20bias%20for%20protein-ligand%0Amodeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Protein-Ligand%2520Binding%2520in%2520Hyperbolic%2520Space%26entry.906535625%3DJianhui%2520Wang%2520and%2520Wenyu%2520Zhu%2520and%2520Bowen%2520Gao%2520and%2520Xin%2520Hong%2520and%2520Ya-Qin%2520Zhang%2520and%2520Wei-Ying%2520Ma%2520and%2520Yanyan%2520Lan%26entry.1292438233%3D%2520%2520Protein-ligand%2520binding%2520prediction%2520is%2520central%2520to%2520virtual%2520screening%2520and%250Aaffinity%2520ranking%252C%2520two%2520fundamental%2520tasks%2520in%2520drug%2520discovery.%2520While%2520recent%250Aretrieval-based%2520methods%2520embed%2520ligands%2520and%2520protein%2520pockets%2520into%2520Euclidean%2520space%250Afor%2520similarity-based%2520search%252C%2520the%2520geometry%2520of%2520Euclidean%2520embeddings%2520often%2520fails%250Ato%2520capture%2520the%2520hierarchical%2520structure%2520and%2520fine-grained%2520affinity%2520variations%250Aintrinsic%2520to%2520molecular%2520interactions.%2520In%2520this%2520work%252C%2520we%2520propose%2520HypSeek%252C%2520a%250Ahyperbolic%2520representation%2520learning%2520framework%2520that%2520embeds%2520ligands%252C%2520protein%250Apockets%252C%2520and%2520sequences%2520into%2520Lorentz-model%2520hyperbolic%2520space.%2520By%2520leveraging%2520the%250Aexponential%2520geometry%2520and%2520negative%2520curvature%2520of%2520hyperbolic%2520space%252C%2520HypSeek%250Aenables%2520expressive%252C%2520affinity-sensitive%2520embeddings%2520that%2520can%2520effectively%2520model%250Aboth%2520global%2520activity%2520and%2520subtle%2520functional%2520differences-particularly%2520in%250Achallenging%2520cases%2520such%2520as%2520activity%2520cliffs%252C%2520where%2520structurally%2520similar%2520ligands%250Aexhibit%2520large%2520affinity%2520gaps.%2520Our%2520mode%2520unifies%2520virtual%2520screening%2520and%2520affinity%250Aranking%2520in%2520a%2520single%2520framework%252C%2520introducing%2520a%2520protein-guided%2520three-tower%250Aarchitecture%2520to%2520enhance%2520representational%2520structure.%2520HypSeek%2520improves%2520early%250Aenrichment%2520in%2520virtual%2520screening%2520on%2520DUD-E%2520from%252042.63%2520to%252051.44%2520%2528%252B20.7%2525%2529%2520and%250Aaffinity%2520ranking%2520correlation%2520on%2520JACS%2520from%25200.5774%2520to%25200.7239%2520%2528%252B25.4%2525%2529%252C%250Ademonstrating%2520the%2520benefits%2520of%2520hyperbolic%2520geometry%2520across%2520both%2520tasks%2520and%250Ahighlighting%2520its%2520potential%2520as%2520a%2520powerful%2520inductive%2520bias%2520for%2520protein-ligand%250Amodeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Protein-Ligand%20Binding%20in%20Hyperbolic%20Space&entry.906535625=Jianhui%20Wang%20and%20Wenyu%20Zhu%20and%20Bowen%20Gao%20and%20Xin%20Hong%20and%20Ya-Qin%20Zhang%20and%20Wei-Ying%20Ma%20and%20Yanyan%20Lan&entry.1292438233=%20%20Protein-ligand%20binding%20prediction%20is%20central%20to%20virtual%20screening%20and%0Aaffinity%20ranking%2C%20two%20fundamental%20tasks%20in%20drug%20discovery.%20While%20recent%0Aretrieval-based%20methods%20embed%20ligands%20and%20protein%20pockets%20into%20Euclidean%20space%0Afor%20similarity-based%20search%2C%20the%20geometry%20of%20Euclidean%20embeddings%20often%20fails%0Ato%20capture%20the%20hierarchical%20structure%20and%20fine-grained%20affinity%20variations%0Aintrinsic%20to%20molecular%20interactions.%20In%20this%20work%2C%20we%20propose%20HypSeek%2C%20a%0Ahyperbolic%20representation%20learning%20framework%20that%20embeds%20ligands%2C%20protein%0Apockets%2C%20and%20sequences%20into%20Lorentz-model%20hyperbolic%20space.%20By%20leveraging%20the%0Aexponential%20geometry%20and%20negative%20curvature%20of%20hyperbolic%20space%2C%20HypSeek%0Aenables%20expressive%2C%20affinity-sensitive%20embeddings%20that%20can%20effectively%20model%0Aboth%20global%20activity%20and%20subtle%20functional%20differences-particularly%20in%0Achallenging%20cases%20such%20as%20activity%20cliffs%2C%20where%20structurally%20similar%20ligands%0Aexhibit%20large%20affinity%20gaps.%20Our%20mode%20unifies%20virtual%20screening%20and%20affinity%0Aranking%20in%20a%20single%20framework%2C%20introducing%20a%20protein-guided%20three-tower%0Aarchitecture%20to%20enhance%20representational%20structure.%20HypSeek%20improves%20early%0Aenrichment%20in%20virtual%20screening%20on%20DUD-E%20from%2042.63%20to%2051.44%20%28%2B20.7%25%29%20and%0Aaffinity%20ranking%20correlation%20on%20JACS%20from%200.5774%20to%200.7239%20%28%2B25.4%25%29%2C%0Ademonstrating%20the%20benefits%20of%20hyperbolic%20geometry%20across%20both%20tasks%20and%0Ahighlighting%20its%20potential%20as%20a%20powerful%20inductive%20bias%20for%20protein-ligand%0Amodeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15480v1&entry.124074799=Read"},
{"title": "Pragmatic Inference Chain (PIC) Improving LLMs' Reasoning of Authentic\n  Implicit Toxic Language", "author": "Xi Chen and Shuo Wang", "abstract": "  The rapid development of large language models (LLMs) gives rise to ethical\nconcerns about their performance, while opening new avenues for developing\ntoxic language detection techniques. However, LLMs' unethical output and their\ncapability of detecting toxicity have primarily been tested on language data\nthat do not demand complex meaning inference, such as the biased associations\nof 'he' with programmer and 'she' with household. Nowadays toxic language\nadopts a much more creative range of implicit forms, thanks to advanced\ncensorship. In this study, we collect authentic toxic interactions that evade\nonline censorship and that are verified by human annotators as\ninference-intensive. To evaluate and improve LLMs' reasoning of the authentic\nimplicit toxic language, we propose a new prompting method, Pragmatic Inference\nChain (PIC), drawn on interdisciplinary findings from cognitive science and\nlinguistics. The PIC prompting significantly improves the success rate of\nGPT-4o, Llama-3.1-70B-Instruct, DeepSeek-v2.5, and DeepSeek-v3 in identifying\nimplicit toxic language, compared to five baseline prompts, such as CoT and\nrule-based baselines. In addition, it also facilitates the models to produce\nmore explicit and coherent reasoning processes, hence can potentially be\ngeneralized to other inference-intensive tasks, e.g., understanding humour and\nmetaphors.\n", "link": "http://arxiv.org/abs/2503.01539v2", "date": "2025-08-21", "relevancy": 1.9281, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pragmatic%20Inference%20Chain%20%28PIC%29%20Improving%20LLMs%27%20Reasoning%20of%20Authentic%0A%20%20Implicit%20Toxic%20Language&body=Title%3A%20Pragmatic%20Inference%20Chain%20%28PIC%29%20Improving%20LLMs%27%20Reasoning%20of%20Authentic%0A%20%20Implicit%20Toxic%20Language%0AAuthor%3A%20Xi%20Chen%20and%20Shuo%20Wang%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20gives%20rise%20to%20ethical%0Aconcerns%20about%20their%20performance%2C%20while%20opening%20new%20avenues%20for%20developing%0Atoxic%20language%20detection%20techniques.%20However%2C%20LLMs%27%20unethical%20output%20and%20their%0Acapability%20of%20detecting%20toxicity%20have%20primarily%20been%20tested%20on%20language%20data%0Athat%20do%20not%20demand%20complex%20meaning%20inference%2C%20such%20as%20the%20biased%20associations%0Aof%20%27he%27%20with%20programmer%20and%20%27she%27%20with%20household.%20Nowadays%20toxic%20language%0Aadopts%20a%20much%20more%20creative%20range%20of%20implicit%20forms%2C%20thanks%20to%20advanced%0Acensorship.%20In%20this%20study%2C%20we%20collect%20authentic%20toxic%20interactions%20that%20evade%0Aonline%20censorship%20and%20that%20are%20verified%20by%20human%20annotators%20as%0Ainference-intensive.%20To%20evaluate%20and%20improve%20LLMs%27%20reasoning%20of%20the%20authentic%0Aimplicit%20toxic%20language%2C%20we%20propose%20a%20new%20prompting%20method%2C%20Pragmatic%20Inference%0AChain%20%28PIC%29%2C%20drawn%20on%20interdisciplinary%20findings%20from%20cognitive%20science%20and%0Alinguistics.%20The%20PIC%20prompting%20significantly%20improves%20the%20success%20rate%20of%0AGPT-4o%2C%20Llama-3.1-70B-Instruct%2C%20DeepSeek-v2.5%2C%20and%20DeepSeek-v3%20in%20identifying%0Aimplicit%20toxic%20language%2C%20compared%20to%20five%20baseline%20prompts%2C%20such%20as%20CoT%20and%0Arule-based%20baselines.%20In%20addition%2C%20it%20also%20facilitates%20the%20models%20to%20produce%0Amore%20explicit%20and%20coherent%20reasoning%20processes%2C%20hence%20can%20potentially%20be%0Ageneralized%20to%20other%20inference-intensive%20tasks%2C%20e.g.%2C%20understanding%20humour%20and%0Ametaphors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01539v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPragmatic%2520Inference%2520Chain%2520%2528PIC%2529%2520Improving%2520LLMs%2527%2520Reasoning%2520of%2520Authentic%250A%2520%2520Implicit%2520Toxic%2520Language%26entry.906535625%3DXi%2520Chen%2520and%2520Shuo%2520Wang%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520gives%2520rise%2520to%2520ethical%250Aconcerns%2520about%2520their%2520performance%252C%2520while%2520opening%2520new%2520avenues%2520for%2520developing%250Atoxic%2520language%2520detection%2520techniques.%2520However%252C%2520LLMs%2527%2520unethical%2520output%2520and%2520their%250Acapability%2520of%2520detecting%2520toxicity%2520have%2520primarily%2520been%2520tested%2520on%2520language%2520data%250Athat%2520do%2520not%2520demand%2520complex%2520meaning%2520inference%252C%2520such%2520as%2520the%2520biased%2520associations%250Aof%2520%2527he%2527%2520with%2520programmer%2520and%2520%2527she%2527%2520with%2520household.%2520Nowadays%2520toxic%2520language%250Aadopts%2520a%2520much%2520more%2520creative%2520range%2520of%2520implicit%2520forms%252C%2520thanks%2520to%2520advanced%250Acensorship.%2520In%2520this%2520study%252C%2520we%2520collect%2520authentic%2520toxic%2520interactions%2520that%2520evade%250Aonline%2520censorship%2520and%2520that%2520are%2520verified%2520by%2520human%2520annotators%2520as%250Ainference-intensive.%2520To%2520evaluate%2520and%2520improve%2520LLMs%2527%2520reasoning%2520of%2520the%2520authentic%250Aimplicit%2520toxic%2520language%252C%2520we%2520propose%2520a%2520new%2520prompting%2520method%252C%2520Pragmatic%2520Inference%250AChain%2520%2528PIC%2529%252C%2520drawn%2520on%2520interdisciplinary%2520findings%2520from%2520cognitive%2520science%2520and%250Alinguistics.%2520The%2520PIC%2520prompting%2520significantly%2520improves%2520the%2520success%2520rate%2520of%250AGPT-4o%252C%2520Llama-3.1-70B-Instruct%252C%2520DeepSeek-v2.5%252C%2520and%2520DeepSeek-v3%2520in%2520identifying%250Aimplicit%2520toxic%2520language%252C%2520compared%2520to%2520five%2520baseline%2520prompts%252C%2520such%2520as%2520CoT%2520and%250Arule-based%2520baselines.%2520In%2520addition%252C%2520it%2520also%2520facilitates%2520the%2520models%2520to%2520produce%250Amore%2520explicit%2520and%2520coherent%2520reasoning%2520processes%252C%2520hence%2520can%2520potentially%2520be%250Ageneralized%2520to%2520other%2520inference-intensive%2520tasks%252C%2520e.g.%252C%2520understanding%2520humour%2520and%250Ametaphors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01539v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pragmatic%20Inference%20Chain%20%28PIC%29%20Improving%20LLMs%27%20Reasoning%20of%20Authentic%0A%20%20Implicit%20Toxic%20Language&entry.906535625=Xi%20Chen%20and%20Shuo%20Wang&entry.1292438233=%20%20The%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20gives%20rise%20to%20ethical%0Aconcerns%20about%20their%20performance%2C%20while%20opening%20new%20avenues%20for%20developing%0Atoxic%20language%20detection%20techniques.%20However%2C%20LLMs%27%20unethical%20output%20and%20their%0Acapability%20of%20detecting%20toxicity%20have%20primarily%20been%20tested%20on%20language%20data%0Athat%20do%20not%20demand%20complex%20meaning%20inference%2C%20such%20as%20the%20biased%20associations%0Aof%20%27he%27%20with%20programmer%20and%20%27she%27%20with%20household.%20Nowadays%20toxic%20language%0Aadopts%20a%20much%20more%20creative%20range%20of%20implicit%20forms%2C%20thanks%20to%20advanced%0Acensorship.%20In%20this%20study%2C%20we%20collect%20authentic%20toxic%20interactions%20that%20evade%0Aonline%20censorship%20and%20that%20are%20verified%20by%20human%20annotators%20as%0Ainference-intensive.%20To%20evaluate%20and%20improve%20LLMs%27%20reasoning%20of%20the%20authentic%0Aimplicit%20toxic%20language%2C%20we%20propose%20a%20new%20prompting%20method%2C%20Pragmatic%20Inference%0AChain%20%28PIC%29%2C%20drawn%20on%20interdisciplinary%20findings%20from%20cognitive%20science%20and%0Alinguistics.%20The%20PIC%20prompting%20significantly%20improves%20the%20success%20rate%20of%0AGPT-4o%2C%20Llama-3.1-70B-Instruct%2C%20DeepSeek-v2.5%2C%20and%20DeepSeek-v3%20in%20identifying%0Aimplicit%20toxic%20language%2C%20compared%20to%20five%20baseline%20prompts%2C%20such%20as%20CoT%20and%0Arule-based%20baselines.%20In%20addition%2C%20it%20also%20facilitates%20the%20models%20to%20produce%0Amore%20explicit%20and%20coherent%20reasoning%20processes%2C%20hence%20can%20potentially%20be%0Ageneralized%20to%20other%20inference-intensive%20tasks%2C%20e.g.%2C%20understanding%20humour%20and%0Ametaphors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01539v2&entry.124074799=Read"},
{"title": "Real-Time Beach Litter Detection and Counting: A Comparative Analysis of\n  RT-DETR Model Variants", "author": "Miftahul Huda and Arsyiah Azahra and Putri Maulida Chairani and Dimas Rizky Ramadhani and Nabila Azhari and Ade Lailani", "abstract": "  Coastal pollution is a pressing global environmental issue, necessitating\nscalable and automated solutions for monitoring and management. This study\ninvestigates the efficacy of the Real-Time Detection Transformer (RT-DETR), a\nstate-of-the-art, end-to-end object detection model, for the automated\ndetection and counting of beach litter. A rigorous comparative analysis is\nconducted between two model variants, RT-DETR-Large (RT-DETR-L) and\nRT-DETR-Extra-Large (RT-DETR-X), trained on a publicly available dataset of\ncoastal debris. The evaluation reveals that the RT-DETR-X model achieves\nmarginally superior accuracy, with a mean Average Precision at 50\\% IoU\n(mAP@50) of 0.816 and a mAP@50-95 of 0.612, compared to the RT-DETR-L model's\n0.810 and 0.606, respectively. However, this minor performance gain is realized\nat a significant computational cost; the RT-DETR-L model demonstrates a\nsubstantially faster inference time of 20.1 ms versus 34.5 ms for the\nRT-DETR-X. The findings suggest that the RT-DETR-L model offers a more\npractical and efficient solution for real-time, in-field deployment due to its\nsuperior balance of processing speed and detection accuracy. This research\nprovides valuable insights into the application of advanced Transformer-based\ndetectors for environmental conservation, highlighting the critical trade-offs\nbetween model complexity and operational viability.\n", "link": "http://arxiv.org/abs/2508.13101v2", "date": "2025-08-21", "relevancy": 1.4669, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4906}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4873}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Beach%20Litter%20Detection%20and%20Counting%3A%20A%20Comparative%20Analysis%20of%0A%20%20RT-DETR%20Model%20Variants&body=Title%3A%20Real-Time%20Beach%20Litter%20Detection%20and%20Counting%3A%20A%20Comparative%20Analysis%20of%0A%20%20RT-DETR%20Model%20Variants%0AAuthor%3A%20Miftahul%20Huda%20and%20Arsyiah%20Azahra%20and%20Putri%20Maulida%20Chairani%20and%20Dimas%20Rizky%20Ramadhani%20and%20Nabila%20Azhari%20and%20Ade%20Lailani%0AAbstract%3A%20%20%20Coastal%20pollution%20is%20a%20pressing%20global%20environmental%20issue%2C%20necessitating%0Ascalable%20and%20automated%20solutions%20for%20monitoring%20and%20management.%20This%20study%0Ainvestigates%20the%20efficacy%20of%20the%20Real-Time%20Detection%20Transformer%20%28RT-DETR%29%2C%20a%0Astate-of-the-art%2C%20end-to-end%20object%20detection%20model%2C%20for%20the%20automated%0Adetection%20and%20counting%20of%20beach%20litter.%20A%20rigorous%20comparative%20analysis%20is%0Aconducted%20between%20two%20model%20variants%2C%20RT-DETR-Large%20%28RT-DETR-L%29%20and%0ART-DETR-Extra-Large%20%28RT-DETR-X%29%2C%20trained%20on%20a%20publicly%20available%20dataset%20of%0Acoastal%20debris.%20The%20evaluation%20reveals%20that%20the%20RT-DETR-X%20model%20achieves%0Amarginally%20superior%20accuracy%2C%20with%20a%20mean%20Average%20Precision%20at%2050%5C%25%20IoU%0A%28mAP%4050%29%20of%200.816%20and%20a%20mAP%4050-95%20of%200.612%2C%20compared%20to%20the%20RT-DETR-L%20model%27s%0A0.810%20and%200.606%2C%20respectively.%20However%2C%20this%20minor%20performance%20gain%20is%20realized%0Aat%20a%20significant%20computational%20cost%3B%20the%20RT-DETR-L%20model%20demonstrates%20a%0Asubstantially%20faster%20inference%20time%20of%2020.1%20ms%20versus%2034.5%20ms%20for%20the%0ART-DETR-X.%20The%20findings%20suggest%20that%20the%20RT-DETR-L%20model%20offers%20a%20more%0Apractical%20and%20efficient%20solution%20for%20real-time%2C%20in-field%20deployment%20due%20to%20its%0Asuperior%20balance%20of%20processing%20speed%20and%20detection%20accuracy.%20This%20research%0Aprovides%20valuable%20insights%20into%20the%20application%20of%20advanced%20Transformer-based%0Adetectors%20for%20environmental%20conservation%2C%20highlighting%20the%20critical%20trade-offs%0Abetween%20model%20complexity%20and%20operational%20viability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Beach%2520Litter%2520Detection%2520and%2520Counting%253A%2520A%2520Comparative%2520Analysis%2520of%250A%2520%2520RT-DETR%2520Model%2520Variants%26entry.906535625%3DMiftahul%2520Huda%2520and%2520Arsyiah%2520Azahra%2520and%2520Putri%2520Maulida%2520Chairani%2520and%2520Dimas%2520Rizky%2520Ramadhani%2520and%2520Nabila%2520Azhari%2520and%2520Ade%2520Lailani%26entry.1292438233%3D%2520%2520Coastal%2520pollution%2520is%2520a%2520pressing%2520global%2520environmental%2520issue%252C%2520necessitating%250Ascalable%2520and%2520automated%2520solutions%2520for%2520monitoring%2520and%2520management.%2520This%2520study%250Ainvestigates%2520the%2520efficacy%2520of%2520the%2520Real-Time%2520Detection%2520Transformer%2520%2528RT-DETR%2529%252C%2520a%250Astate-of-the-art%252C%2520end-to-end%2520object%2520detection%2520model%252C%2520for%2520the%2520automated%250Adetection%2520and%2520counting%2520of%2520beach%2520litter.%2520A%2520rigorous%2520comparative%2520analysis%2520is%250Aconducted%2520between%2520two%2520model%2520variants%252C%2520RT-DETR-Large%2520%2528RT-DETR-L%2529%2520and%250ART-DETR-Extra-Large%2520%2528RT-DETR-X%2529%252C%2520trained%2520on%2520a%2520publicly%2520available%2520dataset%2520of%250Acoastal%2520debris.%2520The%2520evaluation%2520reveals%2520that%2520the%2520RT-DETR-X%2520model%2520achieves%250Amarginally%2520superior%2520accuracy%252C%2520with%2520a%2520mean%2520Average%2520Precision%2520at%252050%255C%2525%2520IoU%250A%2528mAP%254050%2529%2520of%25200.816%2520and%2520a%2520mAP%254050-95%2520of%25200.612%252C%2520compared%2520to%2520the%2520RT-DETR-L%2520model%2527s%250A0.810%2520and%25200.606%252C%2520respectively.%2520However%252C%2520this%2520minor%2520performance%2520gain%2520is%2520realized%250Aat%2520a%2520significant%2520computational%2520cost%253B%2520the%2520RT-DETR-L%2520model%2520demonstrates%2520a%250Asubstantially%2520faster%2520inference%2520time%2520of%252020.1%2520ms%2520versus%252034.5%2520ms%2520for%2520the%250ART-DETR-X.%2520The%2520findings%2520suggest%2520that%2520the%2520RT-DETR-L%2520model%2520offers%2520a%2520more%250Apractical%2520and%2520efficient%2520solution%2520for%2520real-time%252C%2520in-field%2520deployment%2520due%2520to%2520its%250Asuperior%2520balance%2520of%2520processing%2520speed%2520and%2520detection%2520accuracy.%2520This%2520research%250Aprovides%2520valuable%2520insights%2520into%2520the%2520application%2520of%2520advanced%2520Transformer-based%250Adetectors%2520for%2520environmental%2520conservation%252C%2520highlighting%2520the%2520critical%2520trade-offs%250Abetween%2520model%2520complexity%2520and%2520operational%2520viability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Beach%20Litter%20Detection%20and%20Counting%3A%20A%20Comparative%20Analysis%20of%0A%20%20RT-DETR%20Model%20Variants&entry.906535625=Miftahul%20Huda%20and%20Arsyiah%20Azahra%20and%20Putri%20Maulida%20Chairani%20and%20Dimas%20Rizky%20Ramadhani%20and%20Nabila%20Azhari%20and%20Ade%20Lailani&entry.1292438233=%20%20Coastal%20pollution%20is%20a%20pressing%20global%20environmental%20issue%2C%20necessitating%0Ascalable%20and%20automated%20solutions%20for%20monitoring%20and%20management.%20This%20study%0Ainvestigates%20the%20efficacy%20of%20the%20Real-Time%20Detection%20Transformer%20%28RT-DETR%29%2C%20a%0Astate-of-the-art%2C%20end-to-end%20object%20detection%20model%2C%20for%20the%20automated%0Adetection%20and%20counting%20of%20beach%20litter.%20A%20rigorous%20comparative%20analysis%20is%0Aconducted%20between%20two%20model%20variants%2C%20RT-DETR-Large%20%28RT-DETR-L%29%20and%0ART-DETR-Extra-Large%20%28RT-DETR-X%29%2C%20trained%20on%20a%20publicly%20available%20dataset%20of%0Acoastal%20debris.%20The%20evaluation%20reveals%20that%20the%20RT-DETR-X%20model%20achieves%0Amarginally%20superior%20accuracy%2C%20with%20a%20mean%20Average%20Precision%20at%2050%5C%25%20IoU%0A%28mAP%4050%29%20of%200.816%20and%20a%20mAP%4050-95%20of%200.612%2C%20compared%20to%20the%20RT-DETR-L%20model%27s%0A0.810%20and%200.606%2C%20respectively.%20However%2C%20this%20minor%20performance%20gain%20is%20realized%0Aat%20a%20significant%20computational%20cost%3B%20the%20RT-DETR-L%20model%20demonstrates%20a%0Asubstantially%20faster%20inference%20time%20of%2020.1%20ms%20versus%2034.5%20ms%20for%20the%0ART-DETR-X.%20The%20findings%20suggest%20that%20the%20RT-DETR-L%20model%20offers%20a%20more%0Apractical%20and%20efficient%20solution%20for%20real-time%2C%20in-field%20deployment%20due%20to%20its%0Asuperior%20balance%20of%20processing%20speed%20and%20detection%20accuracy.%20This%20research%0Aprovides%20valuable%20insights%20into%20the%20application%20of%20advanced%20Transformer-based%0Adetectors%20for%20environmental%20conservation%2C%20highlighting%20the%20critical%20trade-offs%0Abetween%20model%20complexity%20and%20operational%20viability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13101v2&entry.124074799=Read"},
{"title": "Equivariant IMU Preintegration with Biases: a Galilean Group Approach", "author": "Giulio Delama and Alessandro Fornasier and Robert Mahony and Stephan Weiss", "abstract": "  This letter proposes a new approach for Inertial Measurement Unit (IMU)\npreintegration, a fundamental building block that can be leveraged in different\noptimization-based Inertial Navigation System (INS) localization solutions.\nInspired by recent advances in equivariant theory applied to biased INSs, we\nderive a discrete-time formulation of the IMU preintegration on\n${\\mathbf{Gal}(3) \\ltimes \\mathfrak{gal}(3)}$, the left-trivialization of the\ntangent group of the Galilean group $\\mathbf{Gal}(3)$. We define a novel\npreintegration error that geometrically couples the navigation states and the\nbias leading to lower linearization error. Our method improves in consistency\ncompared to existing preintegration approaches which treat IMU biases as a\nseparate state-space. Extensive validation against state-of-the-art methods,\nboth in simulation and with real-world IMU data, implementation in the Lie++\nlibrary, and open-source code are provided.\n", "link": "http://arxiv.org/abs/2411.05548v8", "date": "2025-08-21", "relevancy": 1.8878, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4841}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4691}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20IMU%20Preintegration%20with%20Biases%3A%20a%20Galilean%20Group%20Approach&body=Title%3A%20Equivariant%20IMU%20Preintegration%20with%20Biases%3A%20a%20Galilean%20Group%20Approach%0AAuthor%3A%20Giulio%20Delama%20and%20Alessandro%20Fornasier%20and%20Robert%20Mahony%20and%20Stephan%20Weiss%0AAbstract%3A%20%20%20This%20letter%20proposes%20a%20new%20approach%20for%20Inertial%20Measurement%20Unit%20%28IMU%29%0Apreintegration%2C%20a%20fundamental%20building%20block%20that%20can%20be%20leveraged%20in%20different%0Aoptimization-based%20Inertial%20Navigation%20System%20%28INS%29%20localization%20solutions.%0AInspired%20by%20recent%20advances%20in%20equivariant%20theory%20applied%20to%20biased%20INSs%2C%20we%0Aderive%20a%20discrete-time%20formulation%20of%20the%20IMU%20preintegration%20on%0A%24%7B%5Cmathbf%7BGal%7D%283%29%20%5Cltimes%20%5Cmathfrak%7Bgal%7D%283%29%7D%24%2C%20the%20left-trivialization%20of%20the%0Atangent%20group%20of%20the%20Galilean%20group%20%24%5Cmathbf%7BGal%7D%283%29%24.%20We%20define%20a%20novel%0Apreintegration%20error%20that%20geometrically%20couples%20the%20navigation%20states%20and%20the%0Abias%20leading%20to%20lower%20linearization%20error.%20Our%20method%20improves%20in%20consistency%0Acompared%20to%20existing%20preintegration%20approaches%20which%20treat%20IMU%20biases%20as%20a%0Aseparate%20state-space.%20Extensive%20validation%20against%20state-of-the-art%20methods%2C%0Aboth%20in%20simulation%20and%20with%20real-world%20IMU%20data%2C%20implementation%20in%20the%20Lie%2B%2B%0Alibrary%2C%20and%20open-source%20code%20are%20provided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05548v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520IMU%2520Preintegration%2520with%2520Biases%253A%2520a%2520Galilean%2520Group%2520Approach%26entry.906535625%3DGiulio%2520Delama%2520and%2520Alessandro%2520Fornasier%2520and%2520Robert%2520Mahony%2520and%2520Stephan%2520Weiss%26entry.1292438233%3D%2520%2520This%2520letter%2520proposes%2520a%2520new%2520approach%2520for%2520Inertial%2520Measurement%2520Unit%2520%2528IMU%2529%250Apreintegration%252C%2520a%2520fundamental%2520building%2520block%2520that%2520can%2520be%2520leveraged%2520in%2520different%250Aoptimization-based%2520Inertial%2520Navigation%2520System%2520%2528INS%2529%2520localization%2520solutions.%250AInspired%2520by%2520recent%2520advances%2520in%2520equivariant%2520theory%2520applied%2520to%2520biased%2520INSs%252C%2520we%250Aderive%2520a%2520discrete-time%2520formulation%2520of%2520the%2520IMU%2520preintegration%2520on%250A%2524%257B%255Cmathbf%257BGal%257D%25283%2529%2520%255Cltimes%2520%255Cmathfrak%257Bgal%257D%25283%2529%257D%2524%252C%2520the%2520left-trivialization%2520of%2520the%250Atangent%2520group%2520of%2520the%2520Galilean%2520group%2520%2524%255Cmathbf%257BGal%257D%25283%2529%2524.%2520We%2520define%2520a%2520novel%250Apreintegration%2520error%2520that%2520geometrically%2520couples%2520the%2520navigation%2520states%2520and%2520the%250Abias%2520leading%2520to%2520lower%2520linearization%2520error.%2520Our%2520method%2520improves%2520in%2520consistency%250Acompared%2520to%2520existing%2520preintegration%2520approaches%2520which%2520treat%2520IMU%2520biases%2520as%2520a%250Aseparate%2520state-space.%2520Extensive%2520validation%2520against%2520state-of-the-art%2520methods%252C%250Aboth%2520in%2520simulation%2520and%2520with%2520real-world%2520IMU%2520data%252C%2520implementation%2520in%2520the%2520Lie%252B%252B%250Alibrary%252C%2520and%2520open-source%2520code%2520are%2520provided.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05548v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20IMU%20Preintegration%20with%20Biases%3A%20a%20Galilean%20Group%20Approach&entry.906535625=Giulio%20Delama%20and%20Alessandro%20Fornasier%20and%20Robert%20Mahony%20and%20Stephan%20Weiss&entry.1292438233=%20%20This%20letter%20proposes%20a%20new%20approach%20for%20Inertial%20Measurement%20Unit%20%28IMU%29%0Apreintegration%2C%20a%20fundamental%20building%20block%20that%20can%20be%20leveraged%20in%20different%0Aoptimization-based%20Inertial%20Navigation%20System%20%28INS%29%20localization%20solutions.%0AInspired%20by%20recent%20advances%20in%20equivariant%20theory%20applied%20to%20biased%20INSs%2C%20we%0Aderive%20a%20discrete-time%20formulation%20of%20the%20IMU%20preintegration%20on%0A%24%7B%5Cmathbf%7BGal%7D%283%29%20%5Cltimes%20%5Cmathfrak%7Bgal%7D%283%29%7D%24%2C%20the%20left-trivialization%20of%20the%0Atangent%20group%20of%20the%20Galilean%20group%20%24%5Cmathbf%7BGal%7D%283%29%24.%20We%20define%20a%20novel%0Apreintegration%20error%20that%20geometrically%20couples%20the%20navigation%20states%20and%20the%0Abias%20leading%20to%20lower%20linearization%20error.%20Our%20method%20improves%20in%20consistency%0Acompared%20to%20existing%20preintegration%20approaches%20which%20treat%20IMU%20biases%20as%20a%0Aseparate%20state-space.%20Extensive%20validation%20against%20state-of-the-art%20methods%2C%0Aboth%20in%20simulation%20and%20with%20real-world%20IMU%20data%2C%20implementation%20in%20the%20Lie%2B%2B%0Alibrary%2C%20and%20open-source%20code%20are%20provided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05548v8&entry.124074799=Read"},
{"title": "Numerical models outperform AI weather forecasts of record-breaking\n  extremes", "author": "Zhongwei Zhang and Erich Fischer and Jakob Zscheischler and Sebastian Engelke", "abstract": "  Artificial intelligence (AI)-based models are revolutionizing weather\nforecasting and have surpassed leading numerical weather prediction systems on\nvarious benchmark tasks. However, their ability to extrapolate and reliably\nforecast unprecedented extreme events remains unclear. Here, we show that for\nrecord-breaking weather extremes, the numerical model High RESolution forecast\n(HRES) from the European Centre for Medium-Range Weather Forecasts still\nconsistently outperforms state-of-the-art AI models GraphCast, GraphCast\noperational, Pangu-Weather, Pangu-Weather operational, and Fuxi. We demonstrate\nthat forecast errors in AI models are consistently larger for record-breaking\nheat, cold, and wind than in HRES across nearly all lead times. We further find\nthat the examined AI models tend to underestimate both the frequency and\nintensity of record-breaking events, and they underpredict hot records and\noverestimate cold records with growing errors for larger record exceedance. Our\nfindings underscore the current limitations of AI weather models in\nextrapolating beyond their training domain and in forecasting the potentially\nmost impactful record-breaking weather events that are particularly frequent in\na rapidly warming climate. Further rigorous verification and model development\nis needed before these models can be solely relied upon for high-stakes\napplications such as early warning systems and disaster management.\n", "link": "http://arxiv.org/abs/2508.15724v1", "date": "2025-08-21", "relevancy": 1.1491, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3981}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Numerical%20models%20outperform%20AI%20weather%20forecasts%20of%20record-breaking%0A%20%20extremes&body=Title%3A%20Numerical%20models%20outperform%20AI%20weather%20forecasts%20of%20record-breaking%0A%20%20extremes%0AAuthor%3A%20Zhongwei%20Zhang%20and%20Erich%20Fischer%20and%20Jakob%20Zscheischler%20and%20Sebastian%20Engelke%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29-based%20models%20are%20revolutionizing%20weather%0Aforecasting%20and%20have%20surpassed%20leading%20numerical%20weather%20prediction%20systems%20on%0Avarious%20benchmark%20tasks.%20However%2C%20their%20ability%20to%20extrapolate%20and%20reliably%0Aforecast%20unprecedented%20extreme%20events%20remains%20unclear.%20Here%2C%20we%20show%20that%20for%0Arecord-breaking%20weather%20extremes%2C%20the%20numerical%20model%20High%20RESolution%20forecast%0A%28HRES%29%20from%20the%20European%20Centre%20for%20Medium-Range%20Weather%20Forecasts%20still%0Aconsistently%20outperforms%20state-of-the-art%20AI%20models%20GraphCast%2C%20GraphCast%0Aoperational%2C%20Pangu-Weather%2C%20Pangu-Weather%20operational%2C%20and%20Fuxi.%20We%20demonstrate%0Athat%20forecast%20errors%20in%20AI%20models%20are%20consistently%20larger%20for%20record-breaking%0Aheat%2C%20cold%2C%20and%20wind%20than%20in%20HRES%20across%20nearly%20all%20lead%20times.%20We%20further%20find%0Athat%20the%20examined%20AI%20models%20tend%20to%20underestimate%20both%20the%20frequency%20and%0Aintensity%20of%20record-breaking%20events%2C%20and%20they%20underpredict%20hot%20records%20and%0Aoverestimate%20cold%20records%20with%20growing%20errors%20for%20larger%20record%20exceedance.%20Our%0Afindings%20underscore%20the%20current%20limitations%20of%20AI%20weather%20models%20in%0Aextrapolating%20beyond%20their%20training%20domain%20and%20in%20forecasting%20the%20potentially%0Amost%20impactful%20record-breaking%20weather%20events%20that%20are%20particularly%20frequent%20in%0Aa%20rapidly%20warming%20climate.%20Further%20rigorous%20verification%20and%20model%20development%0Ais%20needed%20before%20these%20models%20can%20be%20solely%20relied%20upon%20for%20high-stakes%0Aapplications%20such%20as%20early%20warning%20systems%20and%20disaster%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNumerical%2520models%2520outperform%2520AI%2520weather%2520forecasts%2520of%2520record-breaking%250A%2520%2520extremes%26entry.906535625%3DZhongwei%2520Zhang%2520and%2520Erich%2520Fischer%2520and%2520Jakob%2520Zscheischler%2520and%2520Sebastian%2520Engelke%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529-based%2520models%2520are%2520revolutionizing%2520weather%250Aforecasting%2520and%2520have%2520surpassed%2520leading%2520numerical%2520weather%2520prediction%2520systems%2520on%250Avarious%2520benchmark%2520tasks.%2520However%252C%2520their%2520ability%2520to%2520extrapolate%2520and%2520reliably%250Aforecast%2520unprecedented%2520extreme%2520events%2520remains%2520unclear.%2520Here%252C%2520we%2520show%2520that%2520for%250Arecord-breaking%2520weather%2520extremes%252C%2520the%2520numerical%2520model%2520High%2520RESolution%2520forecast%250A%2528HRES%2529%2520from%2520the%2520European%2520Centre%2520for%2520Medium-Range%2520Weather%2520Forecasts%2520still%250Aconsistently%2520outperforms%2520state-of-the-art%2520AI%2520models%2520GraphCast%252C%2520GraphCast%250Aoperational%252C%2520Pangu-Weather%252C%2520Pangu-Weather%2520operational%252C%2520and%2520Fuxi.%2520We%2520demonstrate%250Athat%2520forecast%2520errors%2520in%2520AI%2520models%2520are%2520consistently%2520larger%2520for%2520record-breaking%250Aheat%252C%2520cold%252C%2520and%2520wind%2520than%2520in%2520HRES%2520across%2520nearly%2520all%2520lead%2520times.%2520We%2520further%2520find%250Athat%2520the%2520examined%2520AI%2520models%2520tend%2520to%2520underestimate%2520both%2520the%2520frequency%2520and%250Aintensity%2520of%2520record-breaking%2520events%252C%2520and%2520they%2520underpredict%2520hot%2520records%2520and%250Aoverestimate%2520cold%2520records%2520with%2520growing%2520errors%2520for%2520larger%2520record%2520exceedance.%2520Our%250Afindings%2520underscore%2520the%2520current%2520limitations%2520of%2520AI%2520weather%2520models%2520in%250Aextrapolating%2520beyond%2520their%2520training%2520domain%2520and%2520in%2520forecasting%2520the%2520potentially%250Amost%2520impactful%2520record-breaking%2520weather%2520events%2520that%2520are%2520particularly%2520frequent%2520in%250Aa%2520rapidly%2520warming%2520climate.%2520Further%2520rigorous%2520verification%2520and%2520model%2520development%250Ais%2520needed%2520before%2520these%2520models%2520can%2520be%2520solely%2520relied%2520upon%2520for%2520high-stakes%250Aapplications%2520such%2520as%2520early%2520warning%2520systems%2520and%2520disaster%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Numerical%20models%20outperform%20AI%20weather%20forecasts%20of%20record-breaking%0A%20%20extremes&entry.906535625=Zhongwei%20Zhang%20and%20Erich%20Fischer%20and%20Jakob%20Zscheischler%20and%20Sebastian%20Engelke&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29-based%20models%20are%20revolutionizing%20weather%0Aforecasting%20and%20have%20surpassed%20leading%20numerical%20weather%20prediction%20systems%20on%0Avarious%20benchmark%20tasks.%20However%2C%20their%20ability%20to%20extrapolate%20and%20reliably%0Aforecast%20unprecedented%20extreme%20events%20remains%20unclear.%20Here%2C%20we%20show%20that%20for%0Arecord-breaking%20weather%20extremes%2C%20the%20numerical%20model%20High%20RESolution%20forecast%0A%28HRES%29%20from%20the%20European%20Centre%20for%20Medium-Range%20Weather%20Forecasts%20still%0Aconsistently%20outperforms%20state-of-the-art%20AI%20models%20GraphCast%2C%20GraphCast%0Aoperational%2C%20Pangu-Weather%2C%20Pangu-Weather%20operational%2C%20and%20Fuxi.%20We%20demonstrate%0Athat%20forecast%20errors%20in%20AI%20models%20are%20consistently%20larger%20for%20record-breaking%0Aheat%2C%20cold%2C%20and%20wind%20than%20in%20HRES%20across%20nearly%20all%20lead%20times.%20We%20further%20find%0Athat%20the%20examined%20AI%20models%20tend%20to%20underestimate%20both%20the%20frequency%20and%0Aintensity%20of%20record-breaking%20events%2C%20and%20they%20underpredict%20hot%20records%20and%0Aoverestimate%20cold%20records%20with%20growing%20errors%20for%20larger%20record%20exceedance.%20Our%0Afindings%20underscore%20the%20current%20limitations%20of%20AI%20weather%20models%20in%0Aextrapolating%20beyond%20their%20training%20domain%20and%20in%20forecasting%20the%20potentially%0Amost%20impactful%20record-breaking%20weather%20events%20that%20are%20particularly%20frequent%20in%0Aa%20rapidly%20warming%20climate.%20Further%20rigorous%20verification%20and%20model%20development%0Ais%20needed%20before%20these%20models%20can%20be%20solely%20relied%20upon%20for%20high-stakes%0Aapplications%20such%20as%20early%20warning%20systems%20and%20disaster%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15724v1&entry.124074799=Read"},
{"title": "MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media\n  Videos", "author": "Laura De Grazia and Pol Pastells and Mauro V\u00e1zquez Chas and Desmond Elliott and Danae S\u00e1nchez Villegas and Mireia Farr\u00fas and Mariona Taul\u00e9", "abstract": "  Sexism is generally defined as prejudice and discrimination based on sex or\ngender, affecting every sector of society, from social institutions to\nrelationships and individual behavior. Social media platforms amplify the\nimpact of sexism by conveying discriminatory content not only through text but\nalso across multiple modalities, highlighting the critical need for a\nmultimodal approach to the analysis of sexism online. With the rise of social\nmedia platforms where users share short videos, sexism is increasingly\nspreading through video content. Automatically detecting sexism in videos is a\nchallenging task, as it requires analyzing the combination of verbal, audio,\nand visual elements to identify sexist content. In this study, (1) we introduce\nMuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of\n$\\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose\nan innovative annotation framework for analyzing the contributions of textual,\nvocal, and visual modalities to the classification of content as either sexist\nor non-sexist; and (3) we evaluate a range of large language models (LLMs) and\nmultimodal LLMs on the task of sexism detection. We find that visual\ninformation plays a key role in labeling sexist content for both humans and\nmodels. Models effectively detect explicit sexism; however, they struggle with\nimplicit cases, such as stereotypes, instances where annotators also show low\nagreement. This highlights the inherent difficulty of the task, as identifying\nimplicit sexism depends on the social and cultural context.\n", "link": "http://arxiv.org/abs/2504.11169v2", "date": "2025-08-21", "relevancy": 1.489, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5135}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5116}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MuSeD%3A%20A%20Multimodal%20Spanish%20Dataset%20for%20Sexism%20Detection%20in%20Social%20Media%0A%20%20Videos&body=Title%3A%20MuSeD%3A%20A%20Multimodal%20Spanish%20Dataset%20for%20Sexism%20Detection%20in%20Social%20Media%0A%20%20Videos%0AAuthor%3A%20Laura%20De%20Grazia%20and%20Pol%20Pastells%20and%20Mauro%20V%C3%A1zquez%20Chas%20and%20Desmond%20Elliott%20and%20Danae%20S%C3%A1nchez%20Villegas%20and%20Mireia%20Farr%C3%BAs%20and%20Mariona%20Taul%C3%A9%0AAbstract%3A%20%20%20Sexism%20is%20generally%20defined%20as%20prejudice%20and%20discrimination%20based%20on%20sex%20or%0Agender%2C%20affecting%20every%20sector%20of%20society%2C%20from%20social%20institutions%20to%0Arelationships%20and%20individual%20behavior.%20Social%20media%20platforms%20amplify%20the%0Aimpact%20of%20sexism%20by%20conveying%20discriminatory%20content%20not%20only%20through%20text%20but%0Aalso%20across%20multiple%20modalities%2C%20highlighting%20the%20critical%20need%20for%20a%0Amultimodal%20approach%20to%20the%20analysis%20of%20sexism%20online.%20With%20the%20rise%20of%20social%0Amedia%20platforms%20where%20users%20share%20short%20videos%2C%20sexism%20is%20increasingly%0Aspreading%20through%20video%20content.%20Automatically%20detecting%20sexism%20in%20videos%20is%20a%0Achallenging%20task%2C%20as%20it%20requires%20analyzing%20the%20combination%20of%20verbal%2C%20audio%2C%0Aand%20visual%20elements%20to%20identify%20sexist%20content.%20In%20this%20study%2C%20%281%29%20we%20introduce%0AMuSeD%2C%20a%20new%20Multimodal%20Spanish%20dataset%20for%20Sexism%20Detection%20consisting%20of%0A%24%5Capprox%24%2011%20hours%20of%20videos%20extracted%20from%20TikTok%20and%20BitChute%3B%20%282%29%20we%20propose%0Aan%20innovative%20annotation%20framework%20for%20analyzing%20the%20contributions%20of%20textual%2C%0Avocal%2C%20and%20visual%20modalities%20to%20the%20classification%20of%20content%20as%20either%20sexist%0Aor%20non-sexist%3B%20and%20%283%29%20we%20evaluate%20a%20range%20of%20large%20language%20models%20%28LLMs%29%20and%0Amultimodal%20LLMs%20on%20the%20task%20of%20sexism%20detection.%20We%20find%20that%20visual%0Ainformation%20plays%20a%20key%20role%20in%20labeling%20sexist%20content%20for%20both%20humans%20and%0Amodels.%20Models%20effectively%20detect%20explicit%20sexism%3B%20however%2C%20they%20struggle%20with%0Aimplicit%20cases%2C%20such%20as%20stereotypes%2C%20instances%20where%20annotators%20also%20show%20low%0Aagreement.%20This%20highlights%20the%20inherent%20difficulty%20of%20the%20task%2C%20as%20identifying%0Aimplicit%20sexism%20depends%20on%20the%20social%20and%20cultural%20context.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11169v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMuSeD%253A%2520A%2520Multimodal%2520Spanish%2520Dataset%2520for%2520Sexism%2520Detection%2520in%2520Social%2520Media%250A%2520%2520Videos%26entry.906535625%3DLaura%2520De%2520Grazia%2520and%2520Pol%2520Pastells%2520and%2520Mauro%2520V%25C3%25A1zquez%2520Chas%2520and%2520Desmond%2520Elliott%2520and%2520Danae%2520S%25C3%25A1nchez%2520Villegas%2520and%2520Mireia%2520Farr%25C3%25BAs%2520and%2520Mariona%2520Taul%25C3%25A9%26entry.1292438233%3D%2520%2520Sexism%2520is%2520generally%2520defined%2520as%2520prejudice%2520and%2520discrimination%2520based%2520on%2520sex%2520or%250Agender%252C%2520affecting%2520every%2520sector%2520of%2520society%252C%2520from%2520social%2520institutions%2520to%250Arelationships%2520and%2520individual%2520behavior.%2520Social%2520media%2520platforms%2520amplify%2520the%250Aimpact%2520of%2520sexism%2520by%2520conveying%2520discriminatory%2520content%2520not%2520only%2520through%2520text%2520but%250Aalso%2520across%2520multiple%2520modalities%252C%2520highlighting%2520the%2520critical%2520need%2520for%2520a%250Amultimodal%2520approach%2520to%2520the%2520analysis%2520of%2520sexism%2520online.%2520With%2520the%2520rise%2520of%2520social%250Amedia%2520platforms%2520where%2520users%2520share%2520short%2520videos%252C%2520sexism%2520is%2520increasingly%250Aspreading%2520through%2520video%2520content.%2520Automatically%2520detecting%2520sexism%2520in%2520videos%2520is%2520a%250Achallenging%2520task%252C%2520as%2520it%2520requires%2520analyzing%2520the%2520combination%2520of%2520verbal%252C%2520audio%252C%250Aand%2520visual%2520elements%2520to%2520identify%2520sexist%2520content.%2520In%2520this%2520study%252C%2520%25281%2529%2520we%2520introduce%250AMuSeD%252C%2520a%2520new%2520Multimodal%2520Spanish%2520dataset%2520for%2520Sexism%2520Detection%2520consisting%2520of%250A%2524%255Capprox%2524%252011%2520hours%2520of%2520videos%2520extracted%2520from%2520TikTok%2520and%2520BitChute%253B%2520%25282%2529%2520we%2520propose%250Aan%2520innovative%2520annotation%2520framework%2520for%2520analyzing%2520the%2520contributions%2520of%2520textual%252C%250Avocal%252C%2520and%2520visual%2520modalities%2520to%2520the%2520classification%2520of%2520content%2520as%2520either%2520sexist%250Aor%2520non-sexist%253B%2520and%2520%25283%2529%2520we%2520evaluate%2520a%2520range%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%250Amultimodal%2520LLMs%2520on%2520the%2520task%2520of%2520sexism%2520detection.%2520We%2520find%2520that%2520visual%250Ainformation%2520plays%2520a%2520key%2520role%2520in%2520labeling%2520sexist%2520content%2520for%2520both%2520humans%2520and%250Amodels.%2520Models%2520effectively%2520detect%2520explicit%2520sexism%253B%2520however%252C%2520they%2520struggle%2520with%250Aimplicit%2520cases%252C%2520such%2520as%2520stereotypes%252C%2520instances%2520where%2520annotators%2520also%2520show%2520low%250Aagreement.%2520This%2520highlights%2520the%2520inherent%2520difficulty%2520of%2520the%2520task%252C%2520as%2520identifying%250Aimplicit%2520sexism%2520depends%2520on%2520the%2520social%2520and%2520cultural%2520context.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11169v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MuSeD%3A%20A%20Multimodal%20Spanish%20Dataset%20for%20Sexism%20Detection%20in%20Social%20Media%0A%20%20Videos&entry.906535625=Laura%20De%20Grazia%20and%20Pol%20Pastells%20and%20Mauro%20V%C3%A1zquez%20Chas%20and%20Desmond%20Elliott%20and%20Danae%20S%C3%A1nchez%20Villegas%20and%20Mireia%20Farr%C3%BAs%20and%20Mariona%20Taul%C3%A9&entry.1292438233=%20%20Sexism%20is%20generally%20defined%20as%20prejudice%20and%20discrimination%20based%20on%20sex%20or%0Agender%2C%20affecting%20every%20sector%20of%20society%2C%20from%20social%20institutions%20to%0Arelationships%20and%20individual%20behavior.%20Social%20media%20platforms%20amplify%20the%0Aimpact%20of%20sexism%20by%20conveying%20discriminatory%20content%20not%20only%20through%20text%20but%0Aalso%20across%20multiple%20modalities%2C%20highlighting%20the%20critical%20need%20for%20a%0Amultimodal%20approach%20to%20the%20analysis%20of%20sexism%20online.%20With%20the%20rise%20of%20social%0Amedia%20platforms%20where%20users%20share%20short%20videos%2C%20sexism%20is%20increasingly%0Aspreading%20through%20video%20content.%20Automatically%20detecting%20sexism%20in%20videos%20is%20a%0Achallenging%20task%2C%20as%20it%20requires%20analyzing%20the%20combination%20of%20verbal%2C%20audio%2C%0Aand%20visual%20elements%20to%20identify%20sexist%20content.%20In%20this%20study%2C%20%281%29%20we%20introduce%0AMuSeD%2C%20a%20new%20Multimodal%20Spanish%20dataset%20for%20Sexism%20Detection%20consisting%20of%0A%24%5Capprox%24%2011%20hours%20of%20videos%20extracted%20from%20TikTok%20and%20BitChute%3B%20%282%29%20we%20propose%0Aan%20innovative%20annotation%20framework%20for%20analyzing%20the%20contributions%20of%20textual%2C%0Avocal%2C%20and%20visual%20modalities%20to%20the%20classification%20of%20content%20as%20either%20sexist%0Aor%20non-sexist%3B%20and%20%283%29%20we%20evaluate%20a%20range%20of%20large%20language%20models%20%28LLMs%29%20and%0Amultimodal%20LLMs%20on%20the%20task%20of%20sexism%20detection.%20We%20find%20that%20visual%0Ainformation%20plays%20a%20key%20role%20in%20labeling%20sexist%20content%20for%20both%20humans%20and%0Amodels.%20Models%20effectively%20detect%20explicit%20sexism%3B%20however%2C%20they%20struggle%20with%0Aimplicit%20cases%2C%20such%20as%20stereotypes%2C%20instances%20where%20annotators%20also%20show%20low%0Aagreement.%20This%20highlights%20the%20inherent%20difficulty%20of%20the%20task%2C%20as%20identifying%0Aimplicit%20sexism%20depends%20on%20the%20social%20and%20cultural%20context.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11169v2&entry.124074799=Read"},
{"title": "End-to-End Analysis of Charge Stability Diagrams with Transformers", "author": "Rahul Marchand and Lucas Schorling and Cornelius Carlsson and Jonas Schuff and Barnaby van Straaten and Taylor L. Patti and Federico Fedele and Joshua Ziegler and Parth Girdhar and Pranav Vaidhyanathan and Natalia Ares", "abstract": "  Transformer models and end-to-end learning frameworks are rapidly\nrevolutionizing the field of artificial intelligence. In this work, we apply\nobject detection transformers to analyze charge stability diagrams in\nsemiconductor quantum dot arrays, a key task for achieving scalability with\nspin-based quantum computing. Specifically, our model identifies triple points\nand their connectivity, which is crucial for virtual gate calibration, charge\nstate initialization, drift correction, and pulse sequencing. We show that it\nsurpasses convolutional neural networks in performance on three different spin\nqubit architectures, all without the need for retraining. In contrast to\nexisting approaches, our method significantly reduces complexity and runtime,\nwhile enhancing generalizability. The results highlight the potential of\ntransformer-based end-to-end learning frameworks as a foundation for a\nscalable, device- and architecture-agnostic tool for control and tuning of\nquantum dot devices.\n", "link": "http://arxiv.org/abs/2508.15710v1", "date": "2025-08-21", "relevancy": 1.4424, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5442}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.466}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Analysis%20of%20Charge%20Stability%20Diagrams%20with%20Transformers&body=Title%3A%20End-to-End%20Analysis%20of%20Charge%20Stability%20Diagrams%20with%20Transformers%0AAuthor%3A%20Rahul%20Marchand%20and%20Lucas%20Schorling%20and%20Cornelius%20Carlsson%20and%20Jonas%20Schuff%20and%20Barnaby%20van%20Straaten%20and%20Taylor%20L.%20Patti%20and%20Federico%20Fedele%20and%20Joshua%20Ziegler%20and%20Parth%20Girdhar%20and%20Pranav%20Vaidhyanathan%20and%20Natalia%20Ares%0AAbstract%3A%20%20%20Transformer%20models%20and%20end-to-end%20learning%20frameworks%20are%20rapidly%0Arevolutionizing%20the%20field%20of%20artificial%20intelligence.%20In%20this%20work%2C%20we%20apply%0Aobject%20detection%20transformers%20to%20analyze%20charge%20stability%20diagrams%20in%0Asemiconductor%20quantum%20dot%20arrays%2C%20a%20key%20task%20for%20achieving%20scalability%20with%0Aspin-based%20quantum%20computing.%20Specifically%2C%20our%20model%20identifies%20triple%20points%0Aand%20their%20connectivity%2C%20which%20is%20crucial%20for%20virtual%20gate%20calibration%2C%20charge%0Astate%20initialization%2C%20drift%20correction%2C%20and%20pulse%20sequencing.%20We%20show%20that%20it%0Asurpasses%20convolutional%20neural%20networks%20in%20performance%20on%20three%20different%20spin%0Aqubit%20architectures%2C%20all%20without%20the%20need%20for%20retraining.%20In%20contrast%20to%0Aexisting%20approaches%2C%20our%20method%20significantly%20reduces%20complexity%20and%20runtime%2C%0Awhile%20enhancing%20generalizability.%20The%20results%20highlight%20the%20potential%20of%0Atransformer-based%20end-to-end%20learning%20frameworks%20as%20a%20foundation%20for%20a%0Ascalable%2C%20device-%20and%20architecture-agnostic%20tool%20for%20control%20and%20tuning%20of%0Aquantum%20dot%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Analysis%2520of%2520Charge%2520Stability%2520Diagrams%2520with%2520Transformers%26entry.906535625%3DRahul%2520Marchand%2520and%2520Lucas%2520Schorling%2520and%2520Cornelius%2520Carlsson%2520and%2520Jonas%2520Schuff%2520and%2520Barnaby%2520van%2520Straaten%2520and%2520Taylor%2520L.%2520Patti%2520and%2520Federico%2520Fedele%2520and%2520Joshua%2520Ziegler%2520and%2520Parth%2520Girdhar%2520and%2520Pranav%2520Vaidhyanathan%2520and%2520Natalia%2520Ares%26entry.1292438233%3D%2520%2520Transformer%2520models%2520and%2520end-to-end%2520learning%2520frameworks%2520are%2520rapidly%250Arevolutionizing%2520the%2520field%2520of%2520artificial%2520intelligence.%2520In%2520this%2520work%252C%2520we%2520apply%250Aobject%2520detection%2520transformers%2520to%2520analyze%2520charge%2520stability%2520diagrams%2520in%250Asemiconductor%2520quantum%2520dot%2520arrays%252C%2520a%2520key%2520task%2520for%2520achieving%2520scalability%2520with%250Aspin-based%2520quantum%2520computing.%2520Specifically%252C%2520our%2520model%2520identifies%2520triple%2520points%250Aand%2520their%2520connectivity%252C%2520which%2520is%2520crucial%2520for%2520virtual%2520gate%2520calibration%252C%2520charge%250Astate%2520initialization%252C%2520drift%2520correction%252C%2520and%2520pulse%2520sequencing.%2520We%2520show%2520that%2520it%250Asurpasses%2520convolutional%2520neural%2520networks%2520in%2520performance%2520on%2520three%2520different%2520spin%250Aqubit%2520architectures%252C%2520all%2520without%2520the%2520need%2520for%2520retraining.%2520In%2520contrast%2520to%250Aexisting%2520approaches%252C%2520our%2520method%2520significantly%2520reduces%2520complexity%2520and%2520runtime%252C%250Awhile%2520enhancing%2520generalizability.%2520The%2520results%2520highlight%2520the%2520potential%2520of%250Atransformer-based%2520end-to-end%2520learning%2520frameworks%2520as%2520a%2520foundation%2520for%2520a%250Ascalable%252C%2520device-%2520and%2520architecture-agnostic%2520tool%2520for%2520control%2520and%2520tuning%2520of%250Aquantum%2520dot%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Analysis%20of%20Charge%20Stability%20Diagrams%20with%20Transformers&entry.906535625=Rahul%20Marchand%20and%20Lucas%20Schorling%20and%20Cornelius%20Carlsson%20and%20Jonas%20Schuff%20and%20Barnaby%20van%20Straaten%20and%20Taylor%20L.%20Patti%20and%20Federico%20Fedele%20and%20Joshua%20Ziegler%20and%20Parth%20Girdhar%20and%20Pranav%20Vaidhyanathan%20and%20Natalia%20Ares&entry.1292438233=%20%20Transformer%20models%20and%20end-to-end%20learning%20frameworks%20are%20rapidly%0Arevolutionizing%20the%20field%20of%20artificial%20intelligence.%20In%20this%20work%2C%20we%20apply%0Aobject%20detection%20transformers%20to%20analyze%20charge%20stability%20diagrams%20in%0Asemiconductor%20quantum%20dot%20arrays%2C%20a%20key%20task%20for%20achieving%20scalability%20with%0Aspin-based%20quantum%20computing.%20Specifically%2C%20our%20model%20identifies%20triple%20points%0Aand%20their%20connectivity%2C%20which%20is%20crucial%20for%20virtual%20gate%20calibration%2C%20charge%0Astate%20initialization%2C%20drift%20correction%2C%20and%20pulse%20sequencing.%20We%20show%20that%20it%0Asurpasses%20convolutional%20neural%20networks%20in%20performance%20on%20three%20different%20spin%0Aqubit%20architectures%2C%20all%20without%20the%20need%20for%20retraining.%20In%20contrast%20to%0Aexisting%20approaches%2C%20our%20method%20significantly%20reduces%20complexity%20and%20runtime%2C%0Awhile%20enhancing%20generalizability.%20The%20results%20highlight%20the%20potential%20of%0Atransformer-based%20end-to-end%20learning%20frameworks%20as%20a%20foundation%20for%20a%0Ascalable%2C%20device-%20and%20architecture-agnostic%20tool%20for%20control%20and%20tuning%20of%0Aquantum%20dot%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15710v1&entry.124074799=Read"},
{"title": "A Dynamical Systems Framework for Reinforcement Learning Safety and\n  Robustness Verification", "author": "Ahmed Nasir and Abdelhafid Zenati", "abstract": "  The application of reinforcement learning to safety-critical systems is\nlimited by the lack of formal methods for verifying the robustness and safety\nof learned policies. This paper introduces a novel framework that addresses\nthis gap by analyzing the combination of an RL agent and its environment as a\ndiscrete-time autonomous dynamical system. By leveraging tools from dynamical\nsystems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we\nidentify and visualize Lagrangian Coherent Structures (LCS) that act as the\nhidden \"skeleton\" governing the system's behavior. We demonstrate that\nrepelling LCS function as safety barriers around unsafe regions, while\nattracting LCS reveal the system's convergence properties and potential failure\nmodes, such as unintended \"trap\" states. To move beyond qualitative\nvisualization, we introduce a suite of quantitative metrics, Mean Boundary\nRepulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and\nTemporally-Aware Spurious Attractor Strength (TASAS), to formally measure a\npolicy's safety margin and robustness. We further provide a method for deriving\nlocal stability guarantees and extend the analysis to handle model uncertainty.\nThrough experiments in both discrete and continuous control environments, we\nshow that this framework provides a comprehensive and interpretable assessment\nof policy behavior, successfully identifying critical flaws in policies that\nappear successful based on reward alone.\n", "link": "http://arxiv.org/abs/2508.15588v1", "date": "2025-08-21", "relevancy": 1.5691, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5449}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5415}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Dynamical%20Systems%20Framework%20for%20Reinforcement%20Learning%20Safety%20and%0A%20%20Robustness%20Verification&body=Title%3A%20A%20Dynamical%20Systems%20Framework%20for%20Reinforcement%20Learning%20Safety%20and%0A%20%20Robustness%20Verification%0AAuthor%3A%20Ahmed%20Nasir%20and%20Abdelhafid%20Zenati%0AAbstract%3A%20%20%20The%20application%20of%20reinforcement%20learning%20to%20safety-critical%20systems%20is%0Alimited%20by%20the%20lack%20of%20formal%20methods%20for%20verifying%20the%20robustness%20and%20safety%0Aof%20learned%20policies.%20This%20paper%20introduces%20a%20novel%20framework%20that%20addresses%0Athis%20gap%20by%20analyzing%20the%20combination%20of%20an%20RL%20agent%20and%20its%20environment%20as%20a%0Adiscrete-time%20autonomous%20dynamical%20system.%20By%20leveraging%20tools%20from%20dynamical%0Asystems%20theory%2C%20specifically%20the%20Finite-Time%20Lyapunov%20Exponent%20%28FTLE%29%2C%20we%0Aidentify%20and%20visualize%20Lagrangian%20Coherent%20Structures%20%28LCS%29%20that%20act%20as%20the%0Ahidden%20%22skeleton%22%20governing%20the%20system%27s%20behavior.%20We%20demonstrate%20that%0Arepelling%20LCS%20function%20as%20safety%20barriers%20around%20unsafe%20regions%2C%20while%0Aattracting%20LCS%20reveal%20the%20system%27s%20convergence%20properties%20and%20potential%20failure%0Amodes%2C%20such%20as%20unintended%20%22trap%22%20states.%20To%20move%20beyond%20qualitative%0Avisualization%2C%20we%20introduce%20a%20suite%20of%20quantitative%20metrics%2C%20Mean%20Boundary%0ARepulsion%20%28MBR%29%2C%20Aggregated%20Spurious%20Attractor%20Strength%20%28ASAS%29%2C%20and%0ATemporally-Aware%20Spurious%20Attractor%20Strength%20%28TASAS%29%2C%20to%20formally%20measure%20a%0Apolicy%27s%20safety%20margin%20and%20robustness.%20We%20further%20provide%20a%20method%20for%20deriving%0Alocal%20stability%20guarantees%20and%20extend%20the%20analysis%20to%20handle%20model%20uncertainty.%0AThrough%20experiments%20in%20both%20discrete%20and%20continuous%20control%20environments%2C%20we%0Ashow%20that%20this%20framework%20provides%20a%20comprehensive%20and%20interpretable%20assessment%0Aof%20policy%20behavior%2C%20successfully%20identifying%20critical%20flaws%20in%20policies%20that%0Aappear%20successful%20based%20on%20reward%20alone.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Dynamical%2520Systems%2520Framework%2520for%2520Reinforcement%2520Learning%2520Safety%2520and%250A%2520%2520Robustness%2520Verification%26entry.906535625%3DAhmed%2520Nasir%2520and%2520Abdelhafid%2520Zenati%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520reinforcement%2520learning%2520to%2520safety-critical%2520systems%2520is%250Alimited%2520by%2520the%2520lack%2520of%2520formal%2520methods%2520for%2520verifying%2520the%2520robustness%2520and%2520safety%250Aof%2520learned%2520policies.%2520This%2520paper%2520introduces%2520a%2520novel%2520framework%2520that%2520addresses%250Athis%2520gap%2520by%2520analyzing%2520the%2520combination%2520of%2520an%2520RL%2520agent%2520and%2520its%2520environment%2520as%2520a%250Adiscrete-time%2520autonomous%2520dynamical%2520system.%2520By%2520leveraging%2520tools%2520from%2520dynamical%250Asystems%2520theory%252C%2520specifically%2520the%2520Finite-Time%2520Lyapunov%2520Exponent%2520%2528FTLE%2529%252C%2520we%250Aidentify%2520and%2520visualize%2520Lagrangian%2520Coherent%2520Structures%2520%2528LCS%2529%2520that%2520act%2520as%2520the%250Ahidden%2520%2522skeleton%2522%2520governing%2520the%2520system%2527s%2520behavior.%2520We%2520demonstrate%2520that%250Arepelling%2520LCS%2520function%2520as%2520safety%2520barriers%2520around%2520unsafe%2520regions%252C%2520while%250Aattracting%2520LCS%2520reveal%2520the%2520system%2527s%2520convergence%2520properties%2520and%2520potential%2520failure%250Amodes%252C%2520such%2520as%2520unintended%2520%2522trap%2522%2520states.%2520To%2520move%2520beyond%2520qualitative%250Avisualization%252C%2520we%2520introduce%2520a%2520suite%2520of%2520quantitative%2520metrics%252C%2520Mean%2520Boundary%250ARepulsion%2520%2528MBR%2529%252C%2520Aggregated%2520Spurious%2520Attractor%2520Strength%2520%2528ASAS%2529%252C%2520and%250ATemporally-Aware%2520Spurious%2520Attractor%2520Strength%2520%2528TASAS%2529%252C%2520to%2520formally%2520measure%2520a%250Apolicy%2527s%2520safety%2520margin%2520and%2520robustness.%2520We%2520further%2520provide%2520a%2520method%2520for%2520deriving%250Alocal%2520stability%2520guarantees%2520and%2520extend%2520the%2520analysis%2520to%2520handle%2520model%2520uncertainty.%250AThrough%2520experiments%2520in%2520both%2520discrete%2520and%2520continuous%2520control%2520environments%252C%2520we%250Ashow%2520that%2520this%2520framework%2520provides%2520a%2520comprehensive%2520and%2520interpretable%2520assessment%250Aof%2520policy%2520behavior%252C%2520successfully%2520identifying%2520critical%2520flaws%2520in%2520policies%2520that%250Aappear%2520successful%2520based%2520on%2520reward%2520alone.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Dynamical%20Systems%20Framework%20for%20Reinforcement%20Learning%20Safety%20and%0A%20%20Robustness%20Verification&entry.906535625=Ahmed%20Nasir%20and%20Abdelhafid%20Zenati&entry.1292438233=%20%20The%20application%20of%20reinforcement%20learning%20to%20safety-critical%20systems%20is%0Alimited%20by%20the%20lack%20of%20formal%20methods%20for%20verifying%20the%20robustness%20and%20safety%0Aof%20learned%20policies.%20This%20paper%20introduces%20a%20novel%20framework%20that%20addresses%0Athis%20gap%20by%20analyzing%20the%20combination%20of%20an%20RL%20agent%20and%20its%20environment%20as%20a%0Adiscrete-time%20autonomous%20dynamical%20system.%20By%20leveraging%20tools%20from%20dynamical%0Asystems%20theory%2C%20specifically%20the%20Finite-Time%20Lyapunov%20Exponent%20%28FTLE%29%2C%20we%0Aidentify%20and%20visualize%20Lagrangian%20Coherent%20Structures%20%28LCS%29%20that%20act%20as%20the%0Ahidden%20%22skeleton%22%20governing%20the%20system%27s%20behavior.%20We%20demonstrate%20that%0Arepelling%20LCS%20function%20as%20safety%20barriers%20around%20unsafe%20regions%2C%20while%0Aattracting%20LCS%20reveal%20the%20system%27s%20convergence%20properties%20and%20potential%20failure%0Amodes%2C%20such%20as%20unintended%20%22trap%22%20states.%20To%20move%20beyond%20qualitative%0Avisualization%2C%20we%20introduce%20a%20suite%20of%20quantitative%20metrics%2C%20Mean%20Boundary%0ARepulsion%20%28MBR%29%2C%20Aggregated%20Spurious%20Attractor%20Strength%20%28ASAS%29%2C%20and%0ATemporally-Aware%20Spurious%20Attractor%20Strength%20%28TASAS%29%2C%20to%20formally%20measure%20a%0Apolicy%27s%20safety%20margin%20and%20robustness.%20We%20further%20provide%20a%20method%20for%20deriving%0Alocal%20stability%20guarantees%20and%20extend%20the%20analysis%20to%20handle%20model%20uncertainty.%0AThrough%20experiments%20in%20both%20discrete%20and%20continuous%20control%20environments%2C%20we%0Ashow%20that%20this%20framework%20provides%20a%20comprehensive%20and%20interpretable%20assessment%0Aof%20policy%20behavior%2C%20successfully%20identifying%20critical%20flaws%20in%20policies%20that%0Aappear%20successful%20based%20on%20reward%20alone.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15588v1&entry.124074799=Read"},
{"title": "MoCHA-former: Moir\u00e9-Conditioned Hybrid Adaptive Transformer for Video\n  Demoir\u00e9ing", "author": "Jeahun Sung and Changhyun Roh and Chanho Eom and Jihyong Oh", "abstract": "  Recent advances in portable imaging have made camera-based screen capture\nubiquitous. Unfortunately, frequency aliasing between the camera's color filter\narray (CFA) and the display's sub-pixels induces moir\\'e patterns that severely\ndegrade captured photos and videos. Although various demoir\\'eing models have\nbeen proposed to remove such moir\\'e patterns, these approaches still suffer\nfrom several limitations: (i) spatially varying artifact strength within a\nframe, (ii) large-scale and globally spreading structures, (iii)\nchannel-dependent statistics and (iv) rapid temporal fluctuations across\nframes. We address these issues with the Moir\\'e Conditioned Hybrid Adaptive\nTransformer (MoCHA-former), which comprises two key components: Decoupled\nMoir\\'e Adaptive Demoir\\'eing (DMAD) and Spatio-Temporal Adaptive Demoir\\'eing\n(STAD). DMAD separates moir\\'e and content via a Moir\\'e Decoupling Block (MDB)\nand a Detail Decoupling Block (DDB), then produces moir\\'e-adaptive features\nusing a Moir\\'e Conditioning Block (MCB) for targeted restoration. STAD\nintroduces a Spatial Fusion Block (SFB) with window attention to capture\nlarge-scale structures, and a Feature Channel Attention (FCA) to model channel\ndependence in RAW frames. To ensure temporal consistency, MoCHA-former performs\nimplicit frame alignment without any explicit alignment module. We analyze\nmoir\\'e characteristics through qualitative and quantitative studies, and\nevaluate on two video datasets covering RAW and sRGB domains. MoCHA-former\nconsistently surpasses prior methods across PSNR, SSIM, and LPIPS.\n", "link": "http://arxiv.org/abs/2508.14423v2", "date": "2025-08-21", "relevancy": 1.7157, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5788}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5734}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoCHA-former%3A%20Moir%C3%A9-Conditioned%20Hybrid%20Adaptive%20Transformer%20for%20Video%0A%20%20Demoir%C3%A9ing&body=Title%3A%20MoCHA-former%3A%20Moir%C3%A9-Conditioned%20Hybrid%20Adaptive%20Transformer%20for%20Video%0A%20%20Demoir%C3%A9ing%0AAuthor%3A%20Jeahun%20Sung%20and%20Changhyun%20Roh%20and%20Chanho%20Eom%20and%20Jihyong%20Oh%0AAbstract%3A%20%20%20Recent%20advances%20in%20portable%20imaging%20have%20made%20camera-based%20screen%20capture%0Aubiquitous.%20Unfortunately%2C%20frequency%20aliasing%20between%20the%20camera%27s%20color%20filter%0Aarray%20%28CFA%29%20and%20the%20display%27s%20sub-pixels%20induces%20moir%5C%27e%20patterns%20that%20severely%0Adegrade%20captured%20photos%20and%20videos.%20Although%20various%20demoir%5C%27eing%20models%20have%0Abeen%20proposed%20to%20remove%20such%20moir%5C%27e%20patterns%2C%20these%20approaches%20still%20suffer%0Afrom%20several%20limitations%3A%20%28i%29%20spatially%20varying%20artifact%20strength%20within%20a%0Aframe%2C%20%28ii%29%20large-scale%20and%20globally%20spreading%20structures%2C%20%28iii%29%0Achannel-dependent%20statistics%20and%20%28iv%29%20rapid%20temporal%20fluctuations%20across%0Aframes.%20We%20address%20these%20issues%20with%20the%20Moir%5C%27e%20Conditioned%20Hybrid%20Adaptive%0ATransformer%20%28MoCHA-former%29%2C%20which%20comprises%20two%20key%20components%3A%20Decoupled%0AMoir%5C%27e%20Adaptive%20Demoir%5C%27eing%20%28DMAD%29%20and%20Spatio-Temporal%20Adaptive%20Demoir%5C%27eing%0A%28STAD%29.%20DMAD%20separates%20moir%5C%27e%20and%20content%20via%20a%20Moir%5C%27e%20Decoupling%20Block%20%28MDB%29%0Aand%20a%20Detail%20Decoupling%20Block%20%28DDB%29%2C%20then%20produces%20moir%5C%27e-adaptive%20features%0Ausing%20a%20Moir%5C%27e%20Conditioning%20Block%20%28MCB%29%20for%20targeted%20restoration.%20STAD%0Aintroduces%20a%20Spatial%20Fusion%20Block%20%28SFB%29%20with%20window%20attention%20to%20capture%0Alarge-scale%20structures%2C%20and%20a%20Feature%20Channel%20Attention%20%28FCA%29%20to%20model%20channel%0Adependence%20in%20RAW%20frames.%20To%20ensure%20temporal%20consistency%2C%20MoCHA-former%20performs%0Aimplicit%20frame%20alignment%20without%20any%20explicit%20alignment%20module.%20We%20analyze%0Amoir%5C%27e%20characteristics%20through%20qualitative%20and%20quantitative%20studies%2C%20and%0Aevaluate%20on%20two%20video%20datasets%20covering%20RAW%20and%20sRGB%20domains.%20MoCHA-former%0Aconsistently%20surpasses%20prior%20methods%20across%20PSNR%2C%20SSIM%2C%20and%20LPIPS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14423v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoCHA-former%253A%2520Moir%25C3%25A9-Conditioned%2520Hybrid%2520Adaptive%2520Transformer%2520for%2520Video%250A%2520%2520Demoir%25C3%25A9ing%26entry.906535625%3DJeahun%2520Sung%2520and%2520Changhyun%2520Roh%2520and%2520Chanho%2520Eom%2520and%2520Jihyong%2520Oh%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520portable%2520imaging%2520have%2520made%2520camera-based%2520screen%2520capture%250Aubiquitous.%2520Unfortunately%252C%2520frequency%2520aliasing%2520between%2520the%2520camera%2527s%2520color%2520filter%250Aarray%2520%2528CFA%2529%2520and%2520the%2520display%2527s%2520sub-pixels%2520induces%2520moir%255C%2527e%2520patterns%2520that%2520severely%250Adegrade%2520captured%2520photos%2520and%2520videos.%2520Although%2520various%2520demoir%255C%2527eing%2520models%2520have%250Abeen%2520proposed%2520to%2520remove%2520such%2520moir%255C%2527e%2520patterns%252C%2520these%2520approaches%2520still%2520suffer%250Afrom%2520several%2520limitations%253A%2520%2528i%2529%2520spatially%2520varying%2520artifact%2520strength%2520within%2520a%250Aframe%252C%2520%2528ii%2529%2520large-scale%2520and%2520globally%2520spreading%2520structures%252C%2520%2528iii%2529%250Achannel-dependent%2520statistics%2520and%2520%2528iv%2529%2520rapid%2520temporal%2520fluctuations%2520across%250Aframes.%2520We%2520address%2520these%2520issues%2520with%2520the%2520Moir%255C%2527e%2520Conditioned%2520Hybrid%2520Adaptive%250ATransformer%2520%2528MoCHA-former%2529%252C%2520which%2520comprises%2520two%2520key%2520components%253A%2520Decoupled%250AMoir%255C%2527e%2520Adaptive%2520Demoir%255C%2527eing%2520%2528DMAD%2529%2520and%2520Spatio-Temporal%2520Adaptive%2520Demoir%255C%2527eing%250A%2528STAD%2529.%2520DMAD%2520separates%2520moir%255C%2527e%2520and%2520content%2520via%2520a%2520Moir%255C%2527e%2520Decoupling%2520Block%2520%2528MDB%2529%250Aand%2520a%2520Detail%2520Decoupling%2520Block%2520%2528DDB%2529%252C%2520then%2520produces%2520moir%255C%2527e-adaptive%2520features%250Ausing%2520a%2520Moir%255C%2527e%2520Conditioning%2520Block%2520%2528MCB%2529%2520for%2520targeted%2520restoration.%2520STAD%250Aintroduces%2520a%2520Spatial%2520Fusion%2520Block%2520%2528SFB%2529%2520with%2520window%2520attention%2520to%2520capture%250Alarge-scale%2520structures%252C%2520and%2520a%2520Feature%2520Channel%2520Attention%2520%2528FCA%2529%2520to%2520model%2520channel%250Adependence%2520in%2520RAW%2520frames.%2520To%2520ensure%2520temporal%2520consistency%252C%2520MoCHA-former%2520performs%250Aimplicit%2520frame%2520alignment%2520without%2520any%2520explicit%2520alignment%2520module.%2520We%2520analyze%250Amoir%255C%2527e%2520characteristics%2520through%2520qualitative%2520and%2520quantitative%2520studies%252C%2520and%250Aevaluate%2520on%2520two%2520video%2520datasets%2520covering%2520RAW%2520and%2520sRGB%2520domains.%2520MoCHA-former%250Aconsistently%2520surpasses%2520prior%2520methods%2520across%2520PSNR%252C%2520SSIM%252C%2520and%2520LPIPS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14423v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoCHA-former%3A%20Moir%C3%A9-Conditioned%20Hybrid%20Adaptive%20Transformer%20for%20Video%0A%20%20Demoir%C3%A9ing&entry.906535625=Jeahun%20Sung%20and%20Changhyun%20Roh%20and%20Chanho%20Eom%20and%20Jihyong%20Oh&entry.1292438233=%20%20Recent%20advances%20in%20portable%20imaging%20have%20made%20camera-based%20screen%20capture%0Aubiquitous.%20Unfortunately%2C%20frequency%20aliasing%20between%20the%20camera%27s%20color%20filter%0Aarray%20%28CFA%29%20and%20the%20display%27s%20sub-pixels%20induces%20moir%5C%27e%20patterns%20that%20severely%0Adegrade%20captured%20photos%20and%20videos.%20Although%20various%20demoir%5C%27eing%20models%20have%0Abeen%20proposed%20to%20remove%20such%20moir%5C%27e%20patterns%2C%20these%20approaches%20still%20suffer%0Afrom%20several%20limitations%3A%20%28i%29%20spatially%20varying%20artifact%20strength%20within%20a%0Aframe%2C%20%28ii%29%20large-scale%20and%20globally%20spreading%20structures%2C%20%28iii%29%0Achannel-dependent%20statistics%20and%20%28iv%29%20rapid%20temporal%20fluctuations%20across%0Aframes.%20We%20address%20these%20issues%20with%20the%20Moir%5C%27e%20Conditioned%20Hybrid%20Adaptive%0ATransformer%20%28MoCHA-former%29%2C%20which%20comprises%20two%20key%20components%3A%20Decoupled%0AMoir%5C%27e%20Adaptive%20Demoir%5C%27eing%20%28DMAD%29%20and%20Spatio-Temporal%20Adaptive%20Demoir%5C%27eing%0A%28STAD%29.%20DMAD%20separates%20moir%5C%27e%20and%20content%20via%20a%20Moir%5C%27e%20Decoupling%20Block%20%28MDB%29%0Aand%20a%20Detail%20Decoupling%20Block%20%28DDB%29%2C%20then%20produces%20moir%5C%27e-adaptive%20features%0Ausing%20a%20Moir%5C%27e%20Conditioning%20Block%20%28MCB%29%20for%20targeted%20restoration.%20STAD%0Aintroduces%20a%20Spatial%20Fusion%20Block%20%28SFB%29%20with%20window%20attention%20to%20capture%0Alarge-scale%20structures%2C%20and%20a%20Feature%20Channel%20Attention%20%28FCA%29%20to%20model%20channel%0Adependence%20in%20RAW%20frames.%20To%20ensure%20temporal%20consistency%2C%20MoCHA-former%20performs%0Aimplicit%20frame%20alignment%20without%20any%20explicit%20alignment%20module.%20We%20analyze%0Amoir%5C%27e%20characteristics%20through%20qualitative%20and%20quantitative%20studies%2C%20and%0Aevaluate%20on%20two%20video%20datasets%20covering%20RAW%20and%20sRGB%20domains.%20MoCHA-former%0Aconsistently%20surpasses%20prior%20methods%20across%20PSNR%2C%20SSIM%2C%20and%20LPIPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14423v2&entry.124074799=Read"},
{"title": "SycEval: Evaluating LLM Sycophancy", "author": "Aaron Fanous and Jacob Goldberg and Ank A. Agarwal and Joanna Lin and Anson Zhou and Roxana Daneshjou and Sanmi Koyejo", "abstract": "  Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications.\n", "link": "http://arxiv.org/abs/2502.08177v3", "date": "2025-08-21", "relevancy": 1.7835, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.45}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.445}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SycEval%3A%20Evaluating%20LLM%20Sycophancy&body=Title%3A%20SycEval%3A%20Evaluating%20LLM%20Sycophancy%0AAuthor%3A%20Aaron%20Fanous%20and%20Jacob%20Goldberg%20and%20Ank%20A.%20Agarwal%20and%20Joanna%20Lin%20and%20Anson%20Zhou%20and%20Roxana%20Daneshjou%20and%20Sanmi%20Koyejo%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20applied%20in%20educational%2C%0Aclinical%2C%20and%20professional%20settings%2C%20but%20their%20tendency%20for%20sycophancy%20--%0Aprioritizing%20user%20agreement%20over%20independent%20reasoning%20--%20poses%20risks%20to%0Areliability.%20This%20study%20introduces%20a%20framework%20to%20evaluate%20sycophantic%20behavior%0Ain%20ChatGPT-4o%2C%20Claude-Sonnet%2C%20and%20Gemini-1.5-Pro%20across%20AMPS%20%28mathematics%29%20and%0AMedQuad%20%28medical%20advice%29%20datasets.%20Sycophantic%20behavior%20was%20observed%20in%2058.19%25%0Aof%20cases%2C%20with%20Gemini%20exhibiting%20the%20highest%20rate%20%2862.47%25%29%20and%20ChatGPT%20the%0Alowest%20%2856.71%25%29.%20Progressive%20sycophancy%2C%20leading%20to%20correct%20answers%2C%20occurred%0Ain%2043.52%25%20of%20cases%2C%20while%20regressive%20sycophancy%2C%20leading%20to%20incorrect%20answers%2C%0Awas%20observed%20in%2014.66%25.%20Preemptive%20rebuttals%20demonstrated%20significantly%20higher%0Asycophancy%20rates%20than%20in-context%20rebuttals%20%2861.75%25%20vs.%2056.52%25%2C%20%24Z%3D5.87%24%2C%0A%24p%3C0.001%24%29%2C%20particularly%20in%20computational%20tasks%2C%20where%20regressive%20sycophancy%0Aincreased%20significantly%20%28preemptive%3A%208.13%25%2C%20in-context%3A%203.54%25%2C%20%24p%3C0.001%24%29.%0ASimple%20rebuttals%20maximized%20progressive%20sycophancy%20%28%24Z%3D6.59%24%2C%20%24p%3C0.001%24%29%2C%20while%0Acitation-based%20rebuttals%20exhibited%20the%20highest%20regressive%20rates%20%28%24Z%3D6.59%24%2C%0A%24p%3C0.001%24%29.%20Sycophantic%20behavior%20showed%20high%20persistence%20%2878.5%25%2C%2095%25%20CI%3A%0A%5B77.2%25%2C%2079.8%25%5D%29%20regardless%20of%20context%20or%20model.%20These%20findings%20emphasize%20the%0Arisks%20and%20opportunities%20of%20deploying%20LLMs%20in%20structured%20and%20dynamic%20domains%2C%0Aoffering%20insights%20into%20prompt%20programming%20and%20model%20optimization%20for%20safer%20AI%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08177v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSycEval%253A%2520Evaluating%2520LLM%2520Sycophancy%26entry.906535625%3DAaron%2520Fanous%2520and%2520Jacob%2520Goldberg%2520and%2520Ank%2520A.%2520Agarwal%2520and%2520Joanna%2520Lin%2520and%2520Anson%2520Zhou%2520and%2520Roxana%2520Daneshjou%2520and%2520Sanmi%2520Koyejo%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520applied%2520in%2520educational%252C%250Aclinical%252C%2520and%2520professional%2520settings%252C%2520but%2520their%2520tendency%2520for%2520sycophancy%2520--%250Aprioritizing%2520user%2520agreement%2520over%2520independent%2520reasoning%2520--%2520poses%2520risks%2520to%250Areliability.%2520This%2520study%2520introduces%2520a%2520framework%2520to%2520evaluate%2520sycophantic%2520behavior%250Ain%2520ChatGPT-4o%252C%2520Claude-Sonnet%252C%2520and%2520Gemini-1.5-Pro%2520across%2520AMPS%2520%2528mathematics%2529%2520and%250AMedQuad%2520%2528medical%2520advice%2529%2520datasets.%2520Sycophantic%2520behavior%2520was%2520observed%2520in%252058.19%2525%250Aof%2520cases%252C%2520with%2520Gemini%2520exhibiting%2520the%2520highest%2520rate%2520%252862.47%2525%2529%2520and%2520ChatGPT%2520the%250Alowest%2520%252856.71%2525%2529.%2520Progressive%2520sycophancy%252C%2520leading%2520to%2520correct%2520answers%252C%2520occurred%250Ain%252043.52%2525%2520of%2520cases%252C%2520while%2520regressive%2520sycophancy%252C%2520leading%2520to%2520incorrect%2520answers%252C%250Awas%2520observed%2520in%252014.66%2525.%2520Preemptive%2520rebuttals%2520demonstrated%2520significantly%2520higher%250Asycophancy%2520rates%2520than%2520in-context%2520rebuttals%2520%252861.75%2525%2520vs.%252056.52%2525%252C%2520%2524Z%253D5.87%2524%252C%250A%2524p%253C0.001%2524%2529%252C%2520particularly%2520in%2520computational%2520tasks%252C%2520where%2520regressive%2520sycophancy%250Aincreased%2520significantly%2520%2528preemptive%253A%25208.13%2525%252C%2520in-context%253A%25203.54%2525%252C%2520%2524p%253C0.001%2524%2529.%250ASimple%2520rebuttals%2520maximized%2520progressive%2520sycophancy%2520%2528%2524Z%253D6.59%2524%252C%2520%2524p%253C0.001%2524%2529%252C%2520while%250Acitation-based%2520rebuttals%2520exhibited%2520the%2520highest%2520regressive%2520rates%2520%2528%2524Z%253D6.59%2524%252C%250A%2524p%253C0.001%2524%2529.%2520Sycophantic%2520behavior%2520showed%2520high%2520persistence%2520%252878.5%2525%252C%252095%2525%2520CI%253A%250A%255B77.2%2525%252C%252079.8%2525%255D%2529%2520regardless%2520of%2520context%2520or%2520model.%2520These%2520findings%2520emphasize%2520the%250Arisks%2520and%2520opportunities%2520of%2520deploying%2520LLMs%2520in%2520structured%2520and%2520dynamic%2520domains%252C%250Aoffering%2520insights%2520into%2520prompt%2520programming%2520and%2520model%2520optimization%2520for%2520safer%2520AI%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08177v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SycEval%3A%20Evaluating%20LLM%20Sycophancy&entry.906535625=Aaron%20Fanous%20and%20Jacob%20Goldberg%20and%20Ank%20A.%20Agarwal%20and%20Joanna%20Lin%20and%20Anson%20Zhou%20and%20Roxana%20Daneshjou%20and%20Sanmi%20Koyejo&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20applied%20in%20educational%2C%0Aclinical%2C%20and%20professional%20settings%2C%20but%20their%20tendency%20for%20sycophancy%20--%0Aprioritizing%20user%20agreement%20over%20independent%20reasoning%20--%20poses%20risks%20to%0Areliability.%20This%20study%20introduces%20a%20framework%20to%20evaluate%20sycophantic%20behavior%0Ain%20ChatGPT-4o%2C%20Claude-Sonnet%2C%20and%20Gemini-1.5-Pro%20across%20AMPS%20%28mathematics%29%20and%0AMedQuad%20%28medical%20advice%29%20datasets.%20Sycophantic%20behavior%20was%20observed%20in%2058.19%25%0Aof%20cases%2C%20with%20Gemini%20exhibiting%20the%20highest%20rate%20%2862.47%25%29%20and%20ChatGPT%20the%0Alowest%20%2856.71%25%29.%20Progressive%20sycophancy%2C%20leading%20to%20correct%20answers%2C%20occurred%0Ain%2043.52%25%20of%20cases%2C%20while%20regressive%20sycophancy%2C%20leading%20to%20incorrect%20answers%2C%0Awas%20observed%20in%2014.66%25.%20Preemptive%20rebuttals%20demonstrated%20significantly%20higher%0Asycophancy%20rates%20than%20in-context%20rebuttals%20%2861.75%25%20vs.%2056.52%25%2C%20%24Z%3D5.87%24%2C%0A%24p%3C0.001%24%29%2C%20particularly%20in%20computational%20tasks%2C%20where%20regressive%20sycophancy%0Aincreased%20significantly%20%28preemptive%3A%208.13%25%2C%20in-context%3A%203.54%25%2C%20%24p%3C0.001%24%29.%0ASimple%20rebuttals%20maximized%20progressive%20sycophancy%20%28%24Z%3D6.59%24%2C%20%24p%3C0.001%24%29%2C%20while%0Acitation-based%20rebuttals%20exhibited%20the%20highest%20regressive%20rates%20%28%24Z%3D6.59%24%2C%0A%24p%3C0.001%24%29.%20Sycophantic%20behavior%20showed%20high%20persistence%20%2878.5%25%2C%2095%25%20CI%3A%0A%5B77.2%25%2C%2079.8%25%5D%29%20regardless%20of%20context%20or%20model.%20These%20findings%20emphasize%20the%0Arisks%20and%20opportunities%20of%20deploying%20LLMs%20in%20structured%20and%20dynamic%20domains%2C%0Aoffering%20insights%20into%20prompt%20programming%20and%20model%20optimization%20for%20safer%20AI%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08177v3&entry.124074799=Read"},
{"title": "CREMA: A Contrastive Regularized Masked Autoencoder for Robust ECG\n  Diagnostics across Clinical Domains", "author": "Junho Song and Jong-Hwan Jang and DongGyun Hong and Joon-myoung Kwon and Yong-Yeon Jo", "abstract": "  Electrocardiogram (ECG) diagnosis remains challenging due to limited labeled\ndata and the need to capture subtle yet clinically meaningful variations in\nrhythm and morphology. We present CREMA (Contrastive Regularized Masked\nAutoencoder), a foundation model for 12-lead ECGs designed to learn\ngeneralizable representations through self-supervised pretraining. CREMA\ncombines generative learning and contrastive regularization via a Contrastive\nRegularized MAE loss, and employs a Signal Transformer (SiT) architecture to\ncapture both local waveform details and global temporal dependencies. We\nevaluate CREMA on benchmark datasets and real-world clinical environments,\nincluding deployment scenarios with significant distribution shifts. CREMA\noutperforms supervised baselines and existing self-supervised models in both\nlinear probing and fine-tuning evaluations. Notably, it maintains superior\nperformance across diverse clinical domains, such as emergency care,\nhighlighting its robustness under real-world conditions. These results\ndemonstrate that CREMA serves as a scalable and reliable foundation model for\nECG diagnostics, supporting downstream applications across heterogeneous and\nhigh-risk clinical settings.\n", "link": "http://arxiv.org/abs/2407.07110v3", "date": "2025-08-21", "relevancy": 1.861, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4701}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4658}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CREMA%3A%20A%20Contrastive%20Regularized%20Masked%20Autoencoder%20for%20Robust%20ECG%0A%20%20Diagnostics%20across%20Clinical%20Domains&body=Title%3A%20CREMA%3A%20A%20Contrastive%20Regularized%20Masked%20Autoencoder%20for%20Robust%20ECG%0A%20%20Diagnostics%20across%20Clinical%20Domains%0AAuthor%3A%20Junho%20Song%20and%20Jong-Hwan%20Jang%20and%20DongGyun%20Hong%20and%20Joon-myoung%20Kwon%20and%20Yong-Yeon%20Jo%0AAbstract%3A%20%20%20Electrocardiogram%20%28ECG%29%20diagnosis%20remains%20challenging%20due%20to%20limited%20labeled%0Adata%20and%20the%20need%20to%20capture%20subtle%20yet%20clinically%20meaningful%20variations%20in%0Arhythm%20and%20morphology.%20We%20present%20CREMA%20%28Contrastive%20Regularized%20Masked%0AAutoencoder%29%2C%20a%20foundation%20model%20for%2012-lead%20ECGs%20designed%20to%20learn%0Ageneralizable%20representations%20through%20self-supervised%20pretraining.%20CREMA%0Acombines%20generative%20learning%20and%20contrastive%20regularization%20via%20a%20Contrastive%0ARegularized%20MAE%20loss%2C%20and%20employs%20a%20Signal%20Transformer%20%28SiT%29%20architecture%20to%0Acapture%20both%20local%20waveform%20details%20and%20global%20temporal%20dependencies.%20We%0Aevaluate%20CREMA%20on%20benchmark%20datasets%20and%20real-world%20clinical%20environments%2C%0Aincluding%20deployment%20scenarios%20with%20significant%20distribution%20shifts.%20CREMA%0Aoutperforms%20supervised%20baselines%20and%20existing%20self-supervised%20models%20in%20both%0Alinear%20probing%20and%20fine-tuning%20evaluations.%20Notably%2C%20it%20maintains%20superior%0Aperformance%20across%20diverse%20clinical%20domains%2C%20such%20as%20emergency%20care%2C%0Ahighlighting%20its%20robustness%20under%20real-world%20conditions.%20These%20results%0Ademonstrate%20that%20CREMA%20serves%20as%20a%20scalable%20and%20reliable%20foundation%20model%20for%0AECG%20diagnostics%2C%20supporting%20downstream%20applications%20across%20heterogeneous%20and%0Ahigh-risk%20clinical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07110v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCREMA%253A%2520A%2520Contrastive%2520Regularized%2520Masked%2520Autoencoder%2520for%2520Robust%2520ECG%250A%2520%2520Diagnostics%2520across%2520Clinical%2520Domains%26entry.906535625%3DJunho%2520Song%2520and%2520Jong-Hwan%2520Jang%2520and%2520DongGyun%2520Hong%2520and%2520Joon-myoung%2520Kwon%2520and%2520Yong-Yeon%2520Jo%26entry.1292438233%3D%2520%2520Electrocardiogram%2520%2528ECG%2529%2520diagnosis%2520remains%2520challenging%2520due%2520to%2520limited%2520labeled%250Adata%2520and%2520the%2520need%2520to%2520capture%2520subtle%2520yet%2520clinically%2520meaningful%2520variations%2520in%250Arhythm%2520and%2520morphology.%2520We%2520present%2520CREMA%2520%2528Contrastive%2520Regularized%2520Masked%250AAutoencoder%2529%252C%2520a%2520foundation%2520model%2520for%252012-lead%2520ECGs%2520designed%2520to%2520learn%250Ageneralizable%2520representations%2520through%2520self-supervised%2520pretraining.%2520CREMA%250Acombines%2520generative%2520learning%2520and%2520contrastive%2520regularization%2520via%2520a%2520Contrastive%250ARegularized%2520MAE%2520loss%252C%2520and%2520employs%2520a%2520Signal%2520Transformer%2520%2528SiT%2529%2520architecture%2520to%250Acapture%2520both%2520local%2520waveform%2520details%2520and%2520global%2520temporal%2520dependencies.%2520We%250Aevaluate%2520CREMA%2520on%2520benchmark%2520datasets%2520and%2520real-world%2520clinical%2520environments%252C%250Aincluding%2520deployment%2520scenarios%2520with%2520significant%2520distribution%2520shifts.%2520CREMA%250Aoutperforms%2520supervised%2520baselines%2520and%2520existing%2520self-supervised%2520models%2520in%2520both%250Alinear%2520probing%2520and%2520fine-tuning%2520evaluations.%2520Notably%252C%2520it%2520maintains%2520superior%250Aperformance%2520across%2520diverse%2520clinical%2520domains%252C%2520such%2520as%2520emergency%2520care%252C%250Ahighlighting%2520its%2520robustness%2520under%2520real-world%2520conditions.%2520These%2520results%250Ademonstrate%2520that%2520CREMA%2520serves%2520as%2520a%2520scalable%2520and%2520reliable%2520foundation%2520model%2520for%250AECG%2520diagnostics%252C%2520supporting%2520downstream%2520applications%2520across%2520heterogeneous%2520and%250Ahigh-risk%2520clinical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07110v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CREMA%3A%20A%20Contrastive%20Regularized%20Masked%20Autoencoder%20for%20Robust%20ECG%0A%20%20Diagnostics%20across%20Clinical%20Domains&entry.906535625=Junho%20Song%20and%20Jong-Hwan%20Jang%20and%20DongGyun%20Hong%20and%20Joon-myoung%20Kwon%20and%20Yong-Yeon%20Jo&entry.1292438233=%20%20Electrocardiogram%20%28ECG%29%20diagnosis%20remains%20challenging%20due%20to%20limited%20labeled%0Adata%20and%20the%20need%20to%20capture%20subtle%20yet%20clinically%20meaningful%20variations%20in%0Arhythm%20and%20morphology.%20We%20present%20CREMA%20%28Contrastive%20Regularized%20Masked%0AAutoencoder%29%2C%20a%20foundation%20model%20for%2012-lead%20ECGs%20designed%20to%20learn%0Ageneralizable%20representations%20through%20self-supervised%20pretraining.%20CREMA%0Acombines%20generative%20learning%20and%20contrastive%20regularization%20via%20a%20Contrastive%0ARegularized%20MAE%20loss%2C%20and%20employs%20a%20Signal%20Transformer%20%28SiT%29%20architecture%20to%0Acapture%20both%20local%20waveform%20details%20and%20global%20temporal%20dependencies.%20We%0Aevaluate%20CREMA%20on%20benchmark%20datasets%20and%20real-world%20clinical%20environments%2C%0Aincluding%20deployment%20scenarios%20with%20significant%20distribution%20shifts.%20CREMA%0Aoutperforms%20supervised%20baselines%20and%20existing%20self-supervised%20models%20in%20both%0Alinear%20probing%20and%20fine-tuning%20evaluations.%20Notably%2C%20it%20maintains%20superior%0Aperformance%20across%20diverse%20clinical%20domains%2C%20such%20as%20emergency%20care%2C%0Ahighlighting%20its%20robustness%20under%20real-world%20conditions.%20These%20results%0Ademonstrate%20that%20CREMA%20serves%20as%20a%20scalable%20and%20reliable%20foundation%20model%20for%0AECG%20diagnostics%2C%20supporting%20downstream%20applications%20across%20heterogeneous%20and%0Ahigh-risk%20clinical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07110v3&entry.124074799=Read"},
{"title": "Multi-Exit Kolmogorov-Arnold Networks: enhancing accuracy and parsimony", "author": "James Bagrow and Josh Bongard", "abstract": "  Kolmogorov-Arnold Networks (KANs) uniquely combine high accuracy with\ninterpretability, making them valuable for scientific modeling. However, it is\nunclear a priori how deep a network needs to be for any given task, and deeper\nKANs can be difficult to optimize and interpret. Here we introduce multi-exit\nKANs, where each layer includes its own prediction branch, enabling the network\nto make accurate predictions at multiple depths simultaneously. This\narchitecture provides deep supervision that improves training while discovering\nthe right level of model complexity for each task. Multi-exit KANs consistently\noutperform standard, single-exit versions on synthetic functions, dynamical\nsystems, and real-world datasets. Remarkably, the best predictions often come\nfrom earlier, simpler exits, revealing that these networks naturally identify\nsmaller, more parsimonious and interpretable models without sacrificing\naccuracy. To automate this discovery, we develop a differentiable\n\"learning-to-exit\" algorithm that balances contributions from exits during\ntraining. Our approach offers scientists a practical way to achieve both high\nperformance and interpretability, addressing a fundamental challenge in machine\nlearning for scientific discovery.\n", "link": "http://arxiv.org/abs/2506.03302v2", "date": "2025-08-21", "relevancy": 1.5293, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5311}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5028}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Exit%20Kolmogorov-Arnold%20Networks%3A%20enhancing%20accuracy%20and%20parsimony&body=Title%3A%20Multi-Exit%20Kolmogorov-Arnold%20Networks%3A%20enhancing%20accuracy%20and%20parsimony%0AAuthor%3A%20James%20Bagrow%20and%20Josh%20Bongard%0AAbstract%3A%20%20%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20uniquely%20combine%20high%20accuracy%20with%0Ainterpretability%2C%20making%20them%20valuable%20for%20scientific%20modeling.%20However%2C%20it%20is%0Aunclear%20a%20priori%20how%20deep%20a%20network%20needs%20to%20be%20for%20any%20given%20task%2C%20and%20deeper%0AKANs%20can%20be%20difficult%20to%20optimize%20and%20interpret.%20Here%20we%20introduce%20multi-exit%0AKANs%2C%20where%20each%20layer%20includes%20its%20own%20prediction%20branch%2C%20enabling%20the%20network%0Ato%20make%20accurate%20predictions%20at%20multiple%20depths%20simultaneously.%20This%0Aarchitecture%20provides%20deep%20supervision%20that%20improves%20training%20while%20discovering%0Athe%20right%20level%20of%20model%20complexity%20for%20each%20task.%20Multi-exit%20KANs%20consistently%0Aoutperform%20standard%2C%20single-exit%20versions%20on%20synthetic%20functions%2C%20dynamical%0Asystems%2C%20and%20real-world%20datasets.%20Remarkably%2C%20the%20best%20predictions%20often%20come%0Afrom%20earlier%2C%20simpler%20exits%2C%20revealing%20that%20these%20networks%20naturally%20identify%0Asmaller%2C%20more%20parsimonious%20and%20interpretable%20models%20without%20sacrificing%0Aaccuracy.%20To%20automate%20this%20discovery%2C%20we%20develop%20a%20differentiable%0A%22learning-to-exit%22%20algorithm%20that%20balances%20contributions%20from%20exits%20during%0Atraining.%20Our%20approach%20offers%20scientists%20a%20practical%20way%20to%20achieve%20both%20high%0Aperformance%20and%20interpretability%2C%20addressing%20a%20fundamental%20challenge%20in%20machine%0Alearning%20for%20scientific%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03302v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Exit%2520Kolmogorov-Arnold%2520Networks%253A%2520enhancing%2520accuracy%2520and%2520parsimony%26entry.906535625%3DJames%2520Bagrow%2520and%2520Josh%2520Bongard%26entry.1292438233%3D%2520%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520uniquely%2520combine%2520high%2520accuracy%2520with%250Ainterpretability%252C%2520making%2520them%2520valuable%2520for%2520scientific%2520modeling.%2520However%252C%2520it%2520is%250Aunclear%2520a%2520priori%2520how%2520deep%2520a%2520network%2520needs%2520to%2520be%2520for%2520any%2520given%2520task%252C%2520and%2520deeper%250AKANs%2520can%2520be%2520difficult%2520to%2520optimize%2520and%2520interpret.%2520Here%2520we%2520introduce%2520multi-exit%250AKANs%252C%2520where%2520each%2520layer%2520includes%2520its%2520own%2520prediction%2520branch%252C%2520enabling%2520the%2520network%250Ato%2520make%2520accurate%2520predictions%2520at%2520multiple%2520depths%2520simultaneously.%2520This%250Aarchitecture%2520provides%2520deep%2520supervision%2520that%2520improves%2520training%2520while%2520discovering%250Athe%2520right%2520level%2520of%2520model%2520complexity%2520for%2520each%2520task.%2520Multi-exit%2520KANs%2520consistently%250Aoutperform%2520standard%252C%2520single-exit%2520versions%2520on%2520synthetic%2520functions%252C%2520dynamical%250Asystems%252C%2520and%2520real-world%2520datasets.%2520Remarkably%252C%2520the%2520best%2520predictions%2520often%2520come%250Afrom%2520earlier%252C%2520simpler%2520exits%252C%2520revealing%2520that%2520these%2520networks%2520naturally%2520identify%250Asmaller%252C%2520more%2520parsimonious%2520and%2520interpretable%2520models%2520without%2520sacrificing%250Aaccuracy.%2520To%2520automate%2520this%2520discovery%252C%2520we%2520develop%2520a%2520differentiable%250A%2522learning-to-exit%2522%2520algorithm%2520that%2520balances%2520contributions%2520from%2520exits%2520during%250Atraining.%2520Our%2520approach%2520offers%2520scientists%2520a%2520practical%2520way%2520to%2520achieve%2520both%2520high%250Aperformance%2520and%2520interpretability%252C%2520addressing%2520a%2520fundamental%2520challenge%2520in%2520machine%250Alearning%2520for%2520scientific%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03302v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Exit%20Kolmogorov-Arnold%20Networks%3A%20enhancing%20accuracy%20and%20parsimony&entry.906535625=James%20Bagrow%20and%20Josh%20Bongard&entry.1292438233=%20%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20uniquely%20combine%20high%20accuracy%20with%0Ainterpretability%2C%20making%20them%20valuable%20for%20scientific%20modeling.%20However%2C%20it%20is%0Aunclear%20a%20priori%20how%20deep%20a%20network%20needs%20to%20be%20for%20any%20given%20task%2C%20and%20deeper%0AKANs%20can%20be%20difficult%20to%20optimize%20and%20interpret.%20Here%20we%20introduce%20multi-exit%0AKANs%2C%20where%20each%20layer%20includes%20its%20own%20prediction%20branch%2C%20enabling%20the%20network%0Ato%20make%20accurate%20predictions%20at%20multiple%20depths%20simultaneously.%20This%0Aarchitecture%20provides%20deep%20supervision%20that%20improves%20training%20while%20discovering%0Athe%20right%20level%20of%20model%20complexity%20for%20each%20task.%20Multi-exit%20KANs%20consistently%0Aoutperform%20standard%2C%20single-exit%20versions%20on%20synthetic%20functions%2C%20dynamical%0Asystems%2C%20and%20real-world%20datasets.%20Remarkably%2C%20the%20best%20predictions%20often%20come%0Afrom%20earlier%2C%20simpler%20exits%2C%20revealing%20that%20these%20networks%20naturally%20identify%0Asmaller%2C%20more%20parsimonious%20and%20interpretable%20models%20without%20sacrificing%0Aaccuracy.%20To%20automate%20this%20discovery%2C%20we%20develop%20a%20differentiable%0A%22learning-to-exit%22%20algorithm%20that%20balances%20contributions%20from%20exits%20during%0Atraining.%20Our%20approach%20offers%20scientists%20a%20practical%20way%20to%20achieve%20both%20high%0Aperformance%20and%20interpretability%2C%20addressing%20a%20fundamental%20challenge%20in%20machine%0Alearning%20for%20scientific%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03302v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


